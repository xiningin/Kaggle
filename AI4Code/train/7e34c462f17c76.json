{"cell_type":{"f04c4af5":"code","8e4d5d0d":"code","9285cdce":"code","ca1d328f":"code","39a08079":"code","3482c096":"code","9c117b03":"code","a4541b2b":"code","6a7c744f":"code","f8ae4b20":"code","4eff2bfc":"code","08d7c6c1":"code","774d44fc":"code","d3777955":"code","6f560a4f":"code","e7ae414e":"code","cfe31b00":"code","30cdefce":"code","f6be9b21":"code","c16a9fa8":"code","33d9e4db":"code","f1f47a99":"code","13499323":"code","bdc96fb9":"code","5dd34782":"code","392c4636":"markdown","d3905b39":"markdown","96baebdc":"markdown","2deb9cff":"markdown","dbfe8715":"markdown","e9e2bfb1":"markdown","ccfdd70b":"markdown","d2a23e91":"markdown","ec3b6f98":"markdown","532b37e9":"markdown","6c7882e4":"markdown","22cf2593":"markdown","2913db3f":"markdown","fe5c6753":"markdown","650487fc":"markdown","e8818037":"markdown","ebc4387e":"markdown","db32c6de":"markdown","d906ae56":"markdown","9f62bcb5":"markdown","0f91a5f2":"markdown","cc244185":"markdown","014fdf50":"markdown","a6facfc1":"markdown","08e19d76":"markdown","9bf8fa28":"markdown","6f0dffe8":"markdown","947832b1":"markdown","a63800f0":"markdown","306ac1a7":"markdown","2caf5af5":"markdown","a2ee9a5f":"markdown","e4376c6a":"markdown","45807dfd":"markdown","ff8d5721":"markdown","8a09850d":"markdown","83e1b0cb":"markdown","02a4a991":"markdown","4fdcdf1a":"markdown","a10cdc68":"markdown","e708d5d7":"markdown","e15bf2fb":"markdown","70331fac":"markdown","3b5a7d08":"markdown","b2d053e4":"markdown","3d51e864":"markdown","0c87d3fb":"markdown","e9355b2b":"markdown","d920532a":"markdown","83710a47":"markdown","51800e15":"markdown","e711af09":"markdown","eb215347":"markdown"},"source":{"f04c4af5":"#In Python code:\ndef func(x):\n    return 3 * x ** 3 - 5 * x ** 2\n\ndef func_der(x):\n    return 9 * x ** 2 - 10 * x","8e4d5d0d":"%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib import animation, rc\nfrom IPython.display import HTML","9285cdce":"x = np.linspace(0,2,200)\ny = func(x)\nxprime = np.linspace(1.1,1.9)\nyprime = (xprime - 1.5) * func_der(1.5)  + func(1.5)\nplt.text(0.5, 2, \"$y=3x^3-5x^2$\", fontsize=20)\nplt.text(0.5,1, \"$y_{line}=5.25x-9$\", fontsize=16)\nplt.axvline(1.5, color='k', linestyle='--',linewidth=1) \nplt.plot(x,y, xprime, yprime, 'r--', [1.5], [func(1.5)], 'ro')","ca1d328f":"#Initial plot to setup the animation\nfig, ax = plt.subplots()\nax.set_xlim(( 0, 2))\nax.set_ylim((-4, 4))\n_,_,_, point, line = ax.plot(x,y, xprime, yprime, 'r--', [1.5], [func(1.5)], 'ro', [],[], 'ko', [], [], 'k-')\ntext = ax.text(0.5, 1, \"\")\nax.text(0.5, 0.65, \"derivative 5.25\", color=\"r\")","39a08079":"def init():\n    line.set_data([], [])\n    point.set_data([], [])\n    text.set_text(\"\")\n    return (point, line, text)\ndef animate(i):\n    if (i < 45):\n        pt = 1.495 - 0.495 * (60 - i) \/ 60\n    elif (i < 75):\n        pt = 1.495 - 0.495 * (16.25 - (i-45)\/2 ) \/ 75\n    elif (i<80):\n        pt = 1.495\n    elif (i < 125):\n        pt = 1.495 + 0.495 * (140 - i) \/ 60\n    elif (i < 155):\n        pt = 1.495 + 0.495 * (16.25 - (i-125)\/2 ) \/ 75\n    else:\n        pt = 1.505\n    x = np.linspace(0.8, 1.99)\n    text.set_text(\"slope of the arc {0:.4f}\".format((func(1.5) - func(pt))\/(1.5 - pt)))\n    y = (x - 1.5) * (func(1.5) - func(pt))\/(1.5 - pt) + func(1.5)\n    line.set_data(x, y)\n    point.set_data([pt], [func(pt)])\n    return (point, line, text)\nanim = animation.FuncAnimation(fig, animate, init_func=init,\n                               frames=160, repeat=True, blit=True)\nHTML(anim.to_jshtml())","3482c096":"class DualBasic(object):\n    def __init__(self, val, eps):\n        self.val = val\n        self.eps = eps","9c117b03":"class DualBasicEnhanced(object):\n    def __init__(self, *args):\n        if len(args) == 2:\n            value, eps = args\n        elif len(args) == 1:\n            if isinstance(args[0], (float, int)):\n                value, eps = args[0], 0\n            else:\n                value, eps = args[0].value, args[0].eps\n        self.value = value\n        self.eps = eps\n        \n    def __abs__(self):\n        return abs(self.value)\n    \n    def __str__(self):\n        return \"Dual({}, {})\".format(self.value, self.eps)\n\n    def __repr__(self):\n        return str(self)","a4541b2b":"#In code:\nclass DualArith(object):\n    def __add__(self, other):\n        other = Dual(other)\n        return Dual(self.value + other.value, self.eps + other.eps)\n    \n    def __sub__(self, other):\n        other = Dual(other)\n        return Dual(self.value - other.value, self.eps - other.eps)\n    \n    def __mul__(self, other):\n        other = Dual(other)\n        return Dual(self.value * other.value, self.eps * other.value + self.value * other.eps)","6a7c744f":"class DualDiv(object):\n        def __truediv__(self, other):\n            other = Dual(other)\n            if abs(other.value) == 0:\n                raise ZeroDivisionError\n            else:\n                return Dual(self.value \/ other.value, \n                            self.eps \/ other.value - self.value \/ (other.value)**2 * other.eps)","f8ae4b20":"class Dual(DualBasicEnhanced, DualArith, DualDiv):\n    pass","4eff2bfc":"def square(x):\n    return x * x\nsquare(Dual(3,1))","08d7c6c1":"def cube(x):\n    return x * x * x\ncube(Dual(2,1))","774d44fc":"def by2(x):\n    return x * 2\nby2(Dual(5,1))","d3777955":"EPS = 10E-12 # arbitrary accuracy\n\ndef next_iter(xn, ysq):\n    return (xn + ysq \/ xn) * 1\/2\n\ndef custom_sqrt(ysq):\n    xnext, xprev = ysq, 0\n    while abs(xnext * xnext - xprev * xprev) > EPS * abs(ysq):\n        xnext, xprev = next_iter(xnext, ysq), xnext\n    return xnext","6f560a4f":"custom_sqrt(4)","e7ae414e":"custom_sqrt(Dual(4,1))","cfe31b00":"from math import sqrt\nsqrt(Dual(4,1))","30cdefce":"import autograd\ngrad_custom_sqrt = autograd.grad(custom_sqrt)\ngrad_custom_sqrt(4.)","f6be9b21":"import math\ngrad_math_sqrt = autograd.grad(math.sqrt)\ntry:\n    grad_math_sqrt(4.)\nexcept:\n    import traceback\n    traceback.print_exc(limit=1)","c16a9fa8":"import autograd.numpy as np\nautograd.grad(np.sqrt)(4.)","33d9e4db":"import torch\nx = torch.tensor(4., requires_grad=True)\nx","f1f47a99":"graph = custom_sqrt(x)\ngraph.backward()","13499323":"x.grad","bdc96fb9":"from sympy.abc import x\ndef func(var):\n    return var * var * var * 3 - var * var * 5\nfunc(Dual(x,1))","5dd34782":"def two_var_func(x,y):\n    return y\/custom_sqrt(x*x + y*y)","392c4636":"There are two ways to go about this: either compute the directional derivatives or use two-dimensional perturbations correctly. If you don't want to rewrite the ```Dual``` class, you can use ```numpy``` for vector perturbations. Of course at this point our approach will deviate sharply from the deep learning package's way.","d3905b39":"What about other functions that are not computed with polynomial approximation? Example - square root. Square root is computed by Newton formula. To compute the square root of $y$, we seek the fixed point of the function $$f(x) = \\frac12 x + \\frac12 \\frac yx$$. I.e. the square root of $y$ is $x_0$ such that\n$$x_0 = \\frac12 x_0 + \\frac12 \\frac y{x_0}$$\nHow do you compute an approximation to a fixed point? Iterate the function until the results are close to each other. Indeed, if we build a sequence $x_0, x_1, ..., $ by $x_{i+1} = f(x_{i})$ and we have for some $n$ that $x_{n+1}\\approx x_n$ then substituting $f(x_n)\\approx x_n$ which means that $x_n \\approx \\sqrt{y}$ which is what we are trying to achieve.","96baebdc":"6.Another interesting point to think about is many-variable functions. After all, neural networks have many millions of parameters. For concreteness, take the two-variable function below and compute it's gradient.","2deb9cff":"## <a name=\"real\"><\/a> Section 3. How to create an automatic differentiation system?","dbfe8715":"### Prior art\nThere are plenty of resources about automatic differentiation. This notebook comes from my attempt to digest [this great blog post](https:\/\/alexey.radul.name\/ideas\/2013\/introduction-to-automatic-differentiation\/) by Alexey Radul. There've been plenty others, like this [article](http:\/\/www.ams.org\/publicoutreach\/feature-column\/fc-2017-12) from AMS. In terms of code, [this github gist](https:\/\/gist.github.com\/kmizu\/1428717\/b7ccee41e1d8ec62fbd2bd64df50bc8cb097d51c) is very much in the spirit as my code, albeit in Scala. Alexey Radul also teamed up with some autograd luminaries to write [a comprehensive survey](https:\/\/arxiv.org\/abs\/1502.05767) on automatic differentiation.","e9e2bfb1":"Just the result we were expecting!","ccfdd70b":"Of course, there are no miracles here. Trying to differentiate Python's ```math.sqrt``` fill fail:","d2a23e91":"Fantastic!\n\nAlso important, for  a constant $c$ the derivative of $cx$ is just $c$. So for $c=2$:","ec3b6f98":"### So how does one implement automatic differentiation?","532b37e9":"Let's define a very simple function $$\\text{func}(x)=3x^3-5x^2.$$ \nThe derivative of func can be readily computed to be $$\\text{func_der}(x)=9x^2-10x.$$","6c7882e4":"You can use Autograd to compute derivatives of ```custom_sqrt```","22cf2593":"What does derivative really mean? Let's imagine the graph of the function $\\text{func}$. Let's take a point $x=1.5$. We compute the derivative $$\\text{der_func}(1.5)= 9 * (1.5)^2 - 15 = 5.25.$$\nThis informs us that the slope of the tangent line to the graph of $\\text{func}$ is 5.25. Indeed, we can plot this to see.","2913db3f":"We will pay some lip service to the notion of derivative - instead of defining it, we will use a picture and an animation to illustrate it.","fe5c6753":"Great. (Recall that to read off the derivative, you need to look at the second number)\n\nLet's try the derivavive of $x^3$, which at 2 is:","650487fc":"Watch in the animation below how when the other end point approaches $x=1.5$, the slope of the arc becomes closer to the value given by the derivatvive $\\text{der_func}(1.5)=5.25$","e8818037":"One of the way to approach automatic differentiation is to use dual numbers. What are they?","ebc4387e":"### Cauchy's definition of derivative animated ###","db32c6de":"We'll spend the rest of this section to demonstrate this statement.","d906ae56":"### Demonstrate for polynomials ###","9f62bcb5":"__Now to the main point:__\n## Statement: ##\nSuppose a Python function *piece_of_code* approximates a mathematical function *f*. Then\n\n\n<center>*piece_of_code(Dual(x,1))*<\/center>\napproximates\n<center>\n*Dual(f(x), f'(x))*<\/center> ","0f91a5f2":"How should arithmetic work? The arithmetic operations should come from $\\varepsilon^2=0$. So we define:\n\nAddition\n$$(x + a \\varepsilon)+(y + b\\varepsilon)=(x+y) + (a+b)\\varepsilon$$\nMultiplication\n$$(x + a \\varepsilon)*(y + b\\varepsilon)=xy + (xb+ya)\\varepsilon$$","cc244185":"# Gentle Introduction to automatic differentiation","014fdf50":"Instead, we should use the wrapper around ```numpy``` to achieve the desired result:","a6facfc1":"So why do we seek to compute derivatives? There are quite a few uses for that\n\n* Sensitivity analysis\n\nDerivative is an indicator of a rate of change.\n\n* Optimization\n\nCan I find the minimum, maximum value that my code returns? Fastest optimization algorithms are based on computing derivatives.\n* Finding inverses\n\nWhat is the inputs for my code that return 5?\n\nFor example, the function <i>square root<\/i> is the inverse of the function <i>square<\/i>. Indeed, [Newton's method](https:\/\/en.wikipedia.org\/wiki\/Newton%27s_method) to compute the inverse of function $f$ consists of iterating \n$$\nx_{n+1} = x_{n} - \\frac{f(x_n)}{f'(x_n)}\n$$\nuntil convergence and it uses derivatives.\n\n* Machine Learning and Deep Learning\n\nMachine learning training task (mainly supervised) is simply finding parameters that minimize a certain chosen function called loss. In deep learning, the number of parameters can get to billions, hence the need for smart ways to compute derivatives","08e19d76":"### Stepping outside of math for a moment\nSuppose $\\varepsilon$ is so small that $$\\varepsilon^2=0$$ then $$f(x + \\varepsilon) = f(x) + \\varepsilon f'(x).$$\nSo armed object like $x + \\varepsilon$, we can plug it in to computation of $f$ and then read of the derivative from the coefficient of the $\\varepsilon$ term. Simple, isn't it?","9bf8fa28":"For Pytorch, the situation is similar, we could use our ```custom_sqrt``` to build graph in Pytorch (with version 0.4 API improvements):","6f0dffe8":"Let's check that our function works as intended","947832b1":"Automatic differntiation is about computing derivatives of functions encoded as computer programs. In this notebook, we will build a skeleton of a toy autodiff framework in Python, using dual numbers and Python's magic methods.\n\n## Tl;dr summary ##\n* Automatic differentiation is a key component in every deep learning framework.\n* The basics of automatic differentiation are not hard.\n* Autograd package is awesome.","a63800f0":"# <a name=\"build\"><\/a> Section 2. Building toy autodiff","306ac1a7":"This is because many Python function are written in C and don't operate well on custom objects such as our Dual.","2caf5af5":"# <a name=\"picture\"><\/a>Section 1. Derivative in a picture #","a2ee9a5f":"# <a name=\"magic\"><\/a> 2a. Python magic methods","e4376c6a":"Unfortunately, this is as far as our framework can go. If we tried this on a real life Python function:","45807dfd":"## Computation of derivatives - motivation ##","ff8d5721":"### Defining dual numbers\nLet's try to define this intuition. Dual number consists of a pair of real numbers, $(val, eps)$. In math notation one would write $val + eps \\cdot\\varepsilon$. Think of it as a Python object *Dual(val, eps)*. Define this in code:","8a09850d":"What should we expect for the derivative? Recall the computation of the derivative of square root:\n$$ (\\sqrt{x})' =(x^\\frac{1}{2})'=\\frac{1}{2} x^{\\frac{1}{2} - 1} = \\frac{1}{2 \\sqrt{x}}.$$","83e1b0cb":"Very important is the division of two dual numbers. There are two ways to arrive to what should the inverse of a dual number be. I'll give hints in the exercises. For now let's define:\n\nDivision\n$$ \\frac{1}{x+a\\varepsilon} = \\frac{1}{x} - \\varepsilon\\frac{a}{x^2}$$","02a4a991":"## Dual numbers\nSuppose now $f$ is a nice enough function and $\\varepsilon$ is a small number. Then one can write\n$$ f(x + \\varepsilon) = f(x) + \\varepsilon f'(x) + \\varepsilon^2 ... + \\varepsilon^3 ... + ....$$\nThis is essentially Taylor's theorem.","4fdcdf1a":"So a derivative of a monomial $x^k$ where $k$ is an integer number is $$(x^k)'=kx^{k-1}.$$","a10cdc68":"At the next step, we would like manipulations of dual numbers to look as much as possible as manipulation of regular floats. In order to achieve that, we will use Python's `magic methods`. Magic methods  allow overloading\/extending basic operators and functions to new classes by using special names for attributes and methods that serve as hooks for many python standard functions. See this [great tutorial](https:\/\/rszalski.github.io\/magicmethods\/) and the [official documentation](https:\/\/docs.python.org\/3\/reference\/datamodel.html#special-method-names).","e708d5d7":"So derivative of $x^2$ is $2x$ which at $x=3$ is $2*3=6$. With our dual numbers:","e15bf2fb":"What is the precise definition of the derivative? It involves a limit, so instead of writing this down, let's use an animation to illustrate.","70331fac":"The most important class to verify this is for polynomials. Why? Because a lot of other functions can be approximated by polynomials:\n\n\nFor example $$e^x \\approx \\sum_{k=0}^n \\frac{1}{k!} x^k.$$ Take $n=4$: \n$$e^x = 1 + x + \\frac{1}{2} x^2 + \\frac{1}{6} x^3 + O(x^4).$$","3b5a7d08":"Create a pytorch tensor:","b2d053e4":"1. How to establish that $ (Dual(x,1))^{-1}=Dual(\\frac{1}{x}, -\\frac{1}{x^2})$ or in math notation $\\frac{1}{x+\\varepsilon}=\\frac{1}{x}-\\frac{1}{x^2}\\varepsilon$? There are two ways, in fact. One is to use the derivative of the function $f(x)=\\frac{1}{x}$. The more satisfying way is purely algebraic - by solving an equation. We need to find two real numbers $x_1, x_2$ such that: $$ (x+\\varepsilon)(x_1 + x_2\\varepsilon)= 1 + 0 \\cdot \\varepsilon. $$\n    \n1. The approximation $e^x \\approx 1 + x + \\frac{x^2}{2} + \\frac{x^3}{3!}+\\frac{x^4}{4!}$ breaks down pretty quickly when $x$ is large enough. To compute the exponential for large $x$, we can employ the following procedure. Use $x$'s binary expansion to write it as a sum of powers of two and a small remainder:\n$$\nx = \\sum_{b_i> -5} 2^{b_i} + y,\n$$\nwhere $b_i$'s are non-zero bits of the binary expansion of $x$. We chose to terminate our expansion at -5 in an arbitrary fashion. We then use $e^{a+b}=e^{a}e^b$ to write \n$$\ne^x = e^y\\big( e^{2^{b_m}}..e^{2^{b_0}}\\big).\n$$\nWe can then apply the polynomial approximation for $e^{y}$ (in fact $e^{y}\\approx 1+y$ works great). Verify that when implemented like this with say $compute\\_exp$ then $compute\\_exp(Dual(x,1))$ produces the correct derivative.\n\n1. a. Is our toy-autodiff approach a \"forward mode\" or \"backward mode\" differentiation? See [Wikipedia article](https:\/\/en.wikipedia.org\/wiki\/Automatic_differentiation) for defintions.\n\n    b. Why would ```custom_sqrt``` be impossible in Tensorflow (or Tensorflow's default, original mode) and definitely not in Theano.\n    \n    c. If you familiar with either of those rewrite a compromise version of ```custom_sqrt```.\n\n1. Note how we were always using ```Dual(a,b) * c``` for multiplication by a scalar, which looks a little bit awkward. What is Python magic method do we need to implement to make ```c * Dual(a,b)``` work?\n    \n1. Prove that for a polynomial $P(x)=\\sum a_n x^n$ and your favorite method to compute polynomials the identity $P(Dual(x,1)) = Dual(P(x), P'(x))$ holds. \n    This will help to explain why the following happens:\n","3d51e864":"## The end ##\n### Hope you enjoyed, comments and questions are welcome. ###","0c87d3fb":"- Operator overloading\/Templates\/Generify - this works well for statically typed languages. Esp. for Scala with its' powerful `implicits` mechanism.\n- Source to source translation - programmatically inspect the code and replace the computations of floats with computations of derivatives. Google's [Tangent](https:\/\/github.com\/google\/tangent) is one such attempt. It uses `Autograd` and `Tensorflow`'s eager mode of computation to achieve its goals.\n- Write dedicated automatic differentiation framework:\n    - We can add hooks for derivatives that we know. For instance $(e^x)'=e^x$, it is inefficient to recompute it. In fact, one can say that our toy autodiff is such a framework with hooks only for arithmetic operations. That's why we had to go to all that trouble to implement a custom square root. \n    - Can build a function from basic building blocks if our framework is rich enough. Example from `Pytorch` with forward (computation of the function) and backward (computation of the derivative) methods. Also `Tensorflow`, `Theano` and every deep learning framework out there. And of course `Autograd`.\n- `Autograd` goes further than the deep learning frameworks - it swaps numpy with it's own extended version allowing to seamlessly differentiate numpy code without hassle.","e9355b2b":"## Table of Contents \n[Section 1. Derivative in a picture](#picture)\n\n[Section 2. Building toy autodiff](#toy)\n\n  [2a. Python magic methods](#magic)\n \n[Section 3. How to build a real autodiff](#real)\n\n[Section 4. Questions and Exercises.](#qa)","d920532a":"Build the graph with ```custom_sqrt``` and differentiate it:","83710a47":"## <a name=\"qa\"><\/a> Section 4. Questions and Exercises.","51800e15":"And the derivative is...","e711af09":"We have enough to perform some basic computations with our class. Let's bring the all together in one class:","eb215347":"For convenience sake, we will add an automatic coversion of ```float``` or ```int``` number ```x``` to ```Dual(x, 0)```. And also add the absolute value function and string representation."}}