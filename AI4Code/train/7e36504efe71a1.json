{"cell_type":{"59c4f211":"code","8266d108":"code","88ca84d2":"code","faa200ba":"code","6ee3c2cd":"code","9f2412c3":"code","fbbcbccf":"code","fed2be50":"code","398580bd":"code","9ced327f":"code","c6eb32ea":"code","e1eef681":"code","dc6b91be":"code","6587f435":"code","16488fee":"code","2f82386c":"markdown","030b8b2b":"markdown","2262321f":"markdown","b69aa43f":"markdown","6c8ac1b5":"markdown","b2aed3eb":"markdown","31933e8e":"markdown","7a766358":"markdown"},"source":{"59c4f211":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport os\nimport matplotlib.pylab as plt\nprint(os.listdir(\"..\/input\"))","8266d108":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\n\ny_train = train_df['target'].copy()\nid_train = train_df['id'].copy()\nX_train = train_df.drop(['target', 'id'], axis=1)\nid_text = test_df['id'].copy()\nX_test = test_df.drop(['id'], axis=1)","88ca84d2":"train_df['wheezy-copper-turtle-magic'].plot(kind='hist', bins=500, figsize=(15, 5), title='Distribution of Feature wheezy-copper-turtle-magic')\nplt.show()","faa200ba":"# This feature is more categorical than continious\ntrain_df['wheezy-copper-turtle-magic'] = train_df['wheezy-copper-turtle-magic'].astype('category')\ntest_df['wheezy-copper-turtle-magic'] = test_df['wheezy-copper-turtle-magic'].astype('category')\nX_train['wheezy-copper-turtle-magic'] = X_train['wheezy-copper-turtle-magic'].astype('category')\nX_test['wheezy-copper-turtle-magic'] = X_test['wheezy-copper-turtle-magic'].astype('category')","6ee3c2cd":"cmap = cmap=sns.diverging_palette(5, 250, as_cmap=True)\n\ntrain_df.drop(['target', 'wheezy-copper-turtle-magic'], axis=1). \\\n    describe().T\\\n    .sort_values('mean', ascending=False)\\\n    .drop('count', axis=1)\\\n    .T.style.background_gradient(cmap, axis=1)\\\n    .set_precision(2)","9f2412c3":"test_df.drop(['wheezy-copper-turtle-magic'], axis=1). \\\n    describe().T\\\n    .sort_values('mean', ascending=False)\\\n    .drop('count', axis=1)\\\n    .T.style.background_gradient(cmap, axis=1)\\\n    .set_precision(2)","fbbcbccf":"y_train.mean()","fed2be50":"X_train.columns = ['var_{}'.format(x) for x in range(0, 256)]\nX_test.columns = ['var_{}'.format(x) for x in range(0, 256)]","398580bd":"average_of_feat = train_df.groupby('target').agg(['mean']).T.reset_index().rename(columns={'level_0':'feature'}).drop('level_1', axis=1)","9ced327f":"average_of_feat['pos_neg_diff'] = np.abs(average_of_feat[0] - average_of_feat[1])\naverage_of_feat.sort_values('pos_neg_diff', ascending=True) \\\n    .tail(20).set_index('feature')['pos_neg_diff'].plot(kind='barh',\n                                                        title='Top 20 feature with biggest difference in mean between positive and negative class',\n                                                       figsize=(15, 7),\n                                                       color='grey')\nplt.show()","c6eb32ea":"fig, axes = plt.subplots(10, 2, figsize=(20, 30))\ntop20_diff = average_of_feat.sort_values('pos_neg_diff', ascending=True).tail(20)['feature'].values\nax_position = 0\nfor var in top20_diff:\n    if var not in ['target','id']:\n        for i, d in train_df.groupby('target'):\n            d[var].plot(kind='hist', bins=100, alpha=0.5, title=var, label='target={}'.format(i), ax=axes.flat[ax_position])\n        axes.flat[ax_position].legend()\n        ax_position += 1\nplt.show()","e1eef681":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\n\nparam = {\n    'bagging_freq': 3,\n    'bagging_fraction': 0.8,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.9,\n    'learning_rate': 0.01,\n    'max_depth': 8,  \n    'metric':'auc',\n    'min_data_in_leaf': 82,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 10,\n    'objective': 'binary', \n    'verbosity': 1\n}\nN_FOLDS = 5\nfolds = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=529)\noof = np.zeros(len(X_train))\npredictions = np.zeros(len(X_test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(X_train.iloc[trn_idx], label=y_train.iloc[trn_idx])\n    val_data = lgb.Dataset(X_train.iloc[val_idx], label=y_train.iloc[val_idx])\n\n    num_round = 1000000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=500, early_stopping_rounds = 300)\n    oof[val_idx] = clf.predict(X_train.iloc[val_idx], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = X_train.columns\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(X_test, num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(y_train, oof)))","dc6b91be":"ss = pd.read_csv('..\/input\/sample_submission.csv')\nss['target'] = predictions","6587f435":"from datetime import datetime\nrun_id = \"{:%m%d_%H%M}\".format(datetime.now())","16488fee":"# Save Submission\nsubmission_csv = 'submission_{:0.2f}CV_{}Folds_{}.csv'.format(roc_auc_score(y_train, oof), N_FOLDS, run_id)\nprint('Saving submission as {}'.format(submission_csv))\nss.to_csv(submission_csv, index=False)\nss.to_csv('submission.csv', index=False)\n# Save Feature Importance\nfeature_importance_csv = 'fi_{:0.2f}CV_{}Folds_{}.csv'.format(roc_auc_score(y_train, oof), N_FOLDS, run_id)\nprint('Saving feature importance as {}'.format(feature_importance_csv))\nfeature_importance_df.to_csv(feature_importance_csv, index=False)\n\n# Save OOF\noof_df = pd.DataFrame()\noof_df['oof'] = oof\noof_df['id'] = id_train\noof_df['target'] = y_train\noof_csv = 'oof_{:0.2f}CV_{}Folds_{}.csv'.format(roc_auc_score(y_train, oof), N_FOLDS, run_id)\nprint('Saving out-of-fold predictions as {}'.format(oof_csv))\noof_df.to_csv(oof_csv, index=False)","2f82386c":"# Lets look at some summary statistics of features\n- Removing `wheezy-copper-turtle-magic` from this analysis","030b8b2b":"## One of these things is not like the others...\n- When we display summary statistics of each feature we notice that one feature `wheezy-copper-turtle-magic` stands out.","2262321f":"# Baseline LightGBM Model","b69aa43f":"## 257 Features with some interesting names.\n- stealthy-beige-pinscher-golden?\n- nerdy-indigo-wolfhound-sorted?\n\nThis data looks simulated. And the names are funny but will just require a lot of useless typing..\n\nIf we want to we could rename the columns for those of us who worked on santander... :D","6c8ac1b5":"## Target in the training set is almost 50\/50 Split positive\/negative","b2aed3eb":"## Plot positive vs negative feature distributions\n..pretty boring - or is it?","31933e8e":"# lets get some instant gratification\nOverview of competition:\n- First kaggle competition to use \"Synchronous KO\" which allows for running kernels through public and private LB data prior to the deadline.\n- While this may seem like \"instant gratification\" it does require that the kernel runs first for the initial commit, and then again when submitting for a LB score- doubling the usual runtime before you can see a public LB score.\n- This does remove the need for a 2-stage competition and at the compeition deadline the final results can be revealed with no delay.\n- It's also important to note from the rules that \"submissions that result in an error--either within the kernel or within the process of scoring--will count against your daily submission limit and will not return the specific error message.\"","7a766358":"# Save The Results"}}