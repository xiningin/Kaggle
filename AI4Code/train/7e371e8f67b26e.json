{"cell_type":{"1ad35890":"code","cdddd0e5":"code","89998db0":"code","5def1e3f":"code","4070c011":"code","bc5f771c":"code","7d53a3f4":"code","4441a77d":"code","e42276a9":"code","fdf6609b":"code","e97b2cf4":"code","b2ae1f0a":"code","214ad6a3":"code","87d7924f":"code","40ce85ff":"code","5d1779b8":"code","211cb655":"code","bd3ff9f5":"code","76dd0760":"code","71842f77":"code","3478fbc1":"code","e37cc98c":"code","104dfdb8":"code","58c7f085":"code","86293e66":"code","e3ea37f8":"code","74b73e57":"code","7c783abe":"code","e49a538a":"code","e6d945ff":"code","dc1b6f59":"code","5a2aedfe":"code","16441af3":"code","c60d38a6":"code","d48b8b7e":"code","9297fd61":"code","68ea4b1e":"code","eed12816":"code","5ac74092":"code","5d9e9554":"code","bd5c1605":"code","acad8e8d":"code","836aced5":"code","47af7165":"code","0d7052c4":"code","8da648d0":"code","2b2fa564":"code","ce469a31":"code","f0703bf0":"code","b80394a4":"code","0e55b7e5":"code","a57881dc":"code","7686c8d5":"code","e1f12f0a":"code","e7cdb108":"code","c94a8812":"code","7a5d5dc8":"code","735e226e":"code","c5f0a246":"code","ca271991":"code","095c5456":"code","bcc1831e":"code","e4deb974":"code","583a75df":"code","386962a3":"code","d34fb175":"code","0f98e816":"code","0f283d20":"code","03b13494":"code","d3d13d83":"markdown","94fb5715":"markdown","66990898":"markdown","477c94af":"markdown","0c94d5f6":"markdown","f85cb976":"markdown","a25259a2":"markdown","94952507":"markdown","35cba897":"markdown","c214ddd9":"markdown","0a5aa718":"markdown","08ac4946":"markdown","669397f3":"markdown","d9237b24":"markdown","58e74361":"markdown","1d4a0a3b":"markdown","44134c2f":"markdown","6b7ad3b2":"markdown","d8ed2244":"markdown","e3d0bd36":"markdown","f1b0d2c1":"markdown","59b398b4":"markdown","7fb34b30":"markdown","653fe4dd":"markdown"},"source":{"1ad35890":"# data analysis\nimport numpy as np\nimport pandas as pd\nimport math\nimport random\n\n# statistics\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\n\n# data visualization\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# modelling\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\n\n# metrics\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# warnings\nimport warnings\nwarnings.filterwarnings('ignore')","cdddd0e5":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","89998db0":"# dimension of the training data set \ndf_train.shape","5def1e3f":"# dimension of the testing data set \ndf_test.shape","4070c011":"# first few rows of the training data set \ndf_train.head()","bc5f771c":"# first few rows of the testing data set \ndf_test.head()","7d53a3f4":"# there are 81 columns in the training data set and 80 columns in the testing data set\n# want to know the column present in the training data set but not in the testing data set \n# it is the very thing we are supposed to predict\nset(df_train.columns) - set(df_test.columns)","4441a77d":"# column names of the training data set\ndf_train.columns.values","e42276a9":"# non null count and data type of each column of the training data set\ndf_train.info()","fdf6609b":"# numerical variables\nnumerical = df_train.select_dtypes(exclude='object')\nnumerical.head()","e97b2cf4":"numerical.describe()","b2ae1f0a":"# categorical variables\ncategorical = df_train.select_dtypes(include='object')\ncategorical.head()","214ad6a3":"categorical.describe()","87d7924f":"# correlation between 'SalePrice' and other columns of the training data set \n# sorted in ascending order\ndf_train.corr()['SalePrice'].sort_values()","40ce85ff":"# visualizing correlations via heatmap\ncorrelations = df_train.drop('Id', axis=1).corr()\nsns.heatmap(correlations, cmap='YlGnBu')","5d1779b8":"# visualizing the n-most important correlations via heatmap\nn = 10\nimportant_variables = correlations.nlargest(n, 'SalePrice')['SalePrice'].index\nimportant_correlations = np.corrcoef(df_train[important_variables].values.T)\nsns.heatmap(important_correlations, annot=True, xticklabels=important_variables.values, yticklabels=important_variables.values, cmap='YlGnBu')","211cb655":"# visualizing the n-most important correlations via pairwise scatter plots\nn=5\nimportant_variables = correlations.nlargest(n, 'SalePrice')['SalePrice'].index\nsns.pairplot(df_train[important_variables.values])","bd3ff9f5":"# 'OverallQual' has the highest correlation with 'SalePrice'\nsns.scatterplot(data=df_train, x='OverallQual', y='SalePrice')\nplt.axvline(x=9.5, color='black', ls='--')\nplt.axhline(y=200000, color='black', ls='--')\nplt.axhline(y=650000, color='black', ls='--')","76dd0760":"# outliers in the 'SalePrice' vs 'OverallQual' plot\noutlier_table_1 = df_train[(df_train['OverallQual']>9.5) & (df_train['SalePrice']<200000)][['SalePrice', 'OverallQual']]\noutlier_table_2 = df_train[(df_train['SalePrice']>650000)][['SalePrice', 'OverallQual']]\npd.concat([outlier_table_1, outlier_table_2], axis=0)","71842f77":"# 'GrLivArea' has the second highest correlation with 'SalePrice'\nsns.scatterplot(data=df_train, x='GrLivArea', y='SalePrice')\nplt.axvline(x=4000, color='black', ls='--')","3478fbc1":"# outliers in the 'SalePrice' vs 'GrLivArea' plot\ndf_train[(df_train['GrLivArea']>4000)][['SalePrice', 'GrLivArea']]","e37cc98c":"# 'GarageCars' has the third highest correlation with 'SalePrice'\nsns.scatterplot(data=df_train, x='GarageCars', y='SalePrice')\nplt.axhline(y=650000, color='black', ls='--')","104dfdb8":"# outliers in the 'SalePrice' vs 'GarageCars' plot\ndf_train[df_train['SalePrice']>650000][['SalePrice', 'GarageCars']]","58c7f085":"# 'GarageArea' has the forth highest correlation with 'SalePrice'\nsns.scatterplot(data=df_train, x='GarageArea', y='SalePrice')\nplt.axhline(y=650000, color='black', ls='--')\nplt.axhline(y=300000, color='black', ls='--')\nplt.axvline(x=1200, color='black', ls='--')","86293e66":"# outliers in the 'SalePrice' vs 'GarageArea' plot\noutlier_table_1 = df_train[df_train['SalePrice']>650000][['SalePrice', 'GarageArea']]\noutlier_table_2 = df_train[(df_train['GarageArea']>1200) & (df_train['SalePrice']<300000)][['SalePrice', 'GarageArea']]\npd.concat([outlier_table_1, outlier_table_2], axis=0)","e3ea37f8":"# removing the indices that have been identified as outliers \n# note that many indices are outliers in more than one of the above plots\nindex_drop = df_train[(df_train['GrLivArea']>4000)].index\ndf_train = df_train.drop(index_drop, axis=0)\nindex_drop = df_train[(df_train['GarageArea']>1200) & (df_train['SalePrice']<300000)].index\ndf_train = df_train.drop(index_drop, axis=0)","74b73e57":"# stronger correlation between 'SalePrice' and other columns now\n# sorted in ascending order\ndf_train.corr()['SalePrice'].sort_values()","7c783abe":"# removing 'Id' from the training data set \ndf_train = df_train.drop('Id', axis=1)\n\n# defining a function 'non_zero_null_count_percent' \n# finding variables of training data set with non zero null count percentage\ndef non_zero_null_count_percent(df_train):\n    null_count_percent = 100* (df_train.isnull().sum()\/ len(df_train))\n    null_count_percent = null_count_percent[null_count_percent > 0].sort_values()\n    return null_count_percent\n\n# non zero null count percentage for each column of the training data set\nnull_count_percent = non_zero_null_count_percent(df_train)\n\n# plot the non zero null count percentages of the columns of the training data set\nsns.barplot(x=null_count_percent.index, y=null_count_percent)\nplt.xticks(rotation=90)\nplt.grid()","e49a538a":"# columns in the training data set are either numerical or categorical\n# 'MasVnrArea', 'GarageYrBlt', 'LotFrontage' are numerical variables \n# rest of the variables with non zero null count percentage are categorical variables\nnull_count_percent.index[pd.Series(null_count_percent.index).isin(categorical.columns)==False]","e6d945ff":"# remove columns with more than 80% null count percentage from the training data set\ndiscard_variables = null_count_percent[null_count_percent > 80]\ndiscard_variables = discard_variables.index\ndf_train = df_train.drop(discard_variables, axis=1)\n\n# mean and standard deviation of 'LotFrontage'\nLotFrontage_mean = np.mean(df_train['LotFrontage'])\nLotFrontage_std = np.std(df_train['LotFrontage'])\n\n# upper bound and lower bound of (mean - std, mean + std)\n# ceil function and floor function round off to integers \nlower_bound = math.ceil(LotFrontage_mean-LotFrontage_std)\nupper_bound = math.floor(LotFrontage_mean+LotFrontage_std)\n\n# histogram of 'LotFrontage'\nsns.histplot(df_train['LotFrontage'])\nplt.axvline(x=LotFrontage_mean, color='black', ls='--')\nplt.axvline(x=lower_bound, color='black', ls='--')\nplt.axvline(x=upper_bound, color='black', ls='--')\nplt.title('LotFrontage')","dc1b6f59":"# assign random values from (mean - std, mean + std) to missing values\nLotFrontage_null_count = df_train['LotFrontage'].isnull().sum()\nLotFrontage_values = list(df_train['LotFrontage'].values)\nLotFrontage_null_indices = list(df_train.loc[df_train['LotFrontage'].isnull(), 'LotFrontage'].index)\n\nrandom_values = [random.randint(lower_bound, upper_bound) for _ in range(LotFrontage_null_count + 1)]\nrandom_values_idx = 0\n\nfor LotFrontage_null_idx in LotFrontage_null_indices:\n    LotFrontage_values[LotFrontage_null_idx] = random_values[random_values_idx]\n    random_values_idx += 1      \n    \ndf_train['LotFrontage'][df_train['LotFrontage'].isnull()==True]=pd.Series(LotFrontage_values)\n\n# null count percent for 'FireplaceQu' is just below 50% from the graph \n# assign 'null' to missing values\ndf_train['FireplaceQu'] = df_train['FireplaceQu'].fillna('null')\n\n# assign mean to numerical variables with null count percentage < 5%\nmean_variables = ['MasVnrArea', 'GarageYrBlt']\ndf_train[mean_variables] = df_train[mean_variables].fillna(value = df_train[mean_variables].mean())\n\n# assign mode to categorical variables with null count percentage < 10%\nmode_variables = ['Electrical', 'MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtFinType1', 'BsmtExposure', 'BsmtFinType2', 'GarageCond', 'GarageQual', 'GarageFinish', 'GarageType']\ndf_train[mode_variables].mode()","5a2aedfe":"mode = ['SBrkr', 'None', 'TA', 'TA', 'Unf', 'No', 'Unf', 'TA', 'TA', 'Unf', 'Attchd']\nfor i in range(0, 11, 1):\n    df_train[mode_variables[i]] = df_train[mode_variables[i]].fillna(mode[i])\n    \n# null count percent for all the columns of the training data set is zero now\nnull_count_percent = non_zero_null_count_percent(df_train)\nnull_count_percent","16441af3":"# histogram of 'SalePrice'\n# expect the distribution to look normal \nsns.distplot(df_train['SalePrice'], fit=norm)\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nplt.legend(['Gaussian ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\nplt.xlabel('SalePrice')\nplt.ylabel('Frequency')\nplt.title('SalePrice Distribution')\nplt.axvline(x=(df_train['SalePrice'].mean()), color='black', ls='--')","c60d38a6":"# QQ plot check for normality\nstats.probplot(df_train['SalePrice'], plot=plt)","d48b8b7e":"# above plots tell that 'SalePrice' is not normally distributed\n# infact, it is positively skewed\n# transform SalePrice into its logarithm\ndf_train['SalePrice'] = np.log1p(df_train['SalePrice'])\nsns.distplot(df_train['SalePrice'], fit=norm)\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nplt.legend(['Gaussian ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\nplt.xlabel('SalePrice')\nplt.ylabel('Frequency')\nplt.title('SalePrice Distribution')\nplt.axvline(x=(df_train['SalePrice'].mean()), color='black', ls='--')","9297fd61":"# 'SalePrice' is normally distributed now\nstats.probplot(df_train['SalePrice'], plot=plt)","68ea4b1e":"numerical = df_train.select_dtypes(exclude='object')","eed12816":"# computing the skewness of numerical variables\n# call variables with <-1 or > 1 skewness as \"skewed variables\"\nskewed_variables = numerical.apply(lambda x: skew(x))\nskewed_variables = skewed_variables[abs(skewed_variables) > 1]\nskewed_variables = skewed_variables.index\nskewed_variables","5ac74092":"# using box-cox transformations\nlambda_value = 0.15\nfor variable in skewed_variables:\n    df_train[variable] = boxcox1p(df_train[variable], lambda_value)\ndf_train[skewed_variables] = np.log1p(df_train[skewed_variables])","5d9e9554":"# computing the percentage of zero values for each variable\n# call variables with > 80% zero values as \"zero variables\"\n# removing \"zero variables\" from the training data set\nzero_variables = df_train.apply(lambda x: 100* (sum(x==0)\/ 1460))\nzero_variables = zero_variables[zero_variables > 80]\nzero_variables = zero_variables.index\nzero_variables","bd5c1605":"df_train = df_train.drop(zero_variables, axis=1)","acad8e8d":"# unique values 'OverallCond' takes\ndf_train['OverallCond'].unique()","836aced5":"# unique values 'YrSold' takes\ndf_train['YrSold'].unique()","47af7165":"# unique values 'MoSold' takes\ndf_train['MoSold'].unique()","0d7052c4":"# conversion\ndf_train['OverallCond'] = df_train['OverallCond'].apply(str)\ndf_train['YrSold'] = df_train['YrSold'].apply(str)\ndf_train['MoSold'] = df_train['MoSold'].apply(str)","8da648d0":"# re-creating numerical and categorical after pre-processing\nnumerical = df_train.select_dtypes(exclude='object')\ncategorical = df_train.select_dtypes(include='object')\n\n# converting categorical variables into dummy variables\ncategorical = pd.get_dummies(categorical, drop_first=True)\n\n# concatenating the numerical and categorical data sets\ndf_train = pd.concat([numerical, categorical], axis=1)","2b2fa564":"# re-indexing for ease of use\ndf_train.index = range(0, 1453, 1)","ce469a31":"# final pre-processed training data set\ndf_train","f0703bf0":"# pre-processing of testing data set \nId = df_test['Id']\n\ndf_test = df_test.drop('Id', axis=1)\nnull_count_percent_test = non_zero_null_count_percent(df_test)\n\ndiscard_variables_test = null_count_percent_test[null_count_percent_test > 80]\ndiscard_variables_test = discard_variables_test.index\ndf_test = df_test.drop(discard_variables_test, axis=1)\n\nLotFrontage_mean_test = np.mean(df_test['LotFrontage'])\nLotFrontage_std_test = np.std(df_test['LotFrontage'])\n\nlower_bound_test = math.ceil(LotFrontage_mean_test-LotFrontage_std_test)\nupper_bound_test = math.floor(LotFrontage_mean_test+LotFrontage_std_test)\n\nLotFrontage_null_count_test = df_test['LotFrontage'].isnull().sum()\nLotFrontage_values_test = list(df_test['LotFrontage'].values)\nLotFrontage_null_indices_test = list(df_test.loc[df_test['LotFrontage'].isnull(), 'LotFrontage'].index)\n\nrandom_values_test = [random.randint(lower_bound_test, upper_bound_test) for _ in range(LotFrontage_null_count_test + 1)]\nrandom_values_idx_test = 0\n\nfor LotFrontage_null_idx_test in LotFrontage_null_indices_test:\n    LotFrontage_values_test[LotFrontage_null_idx_test] = random_values_test[random_values_idx_test]\n    random_values_idx_test += 1      \n    \ndf_test['LotFrontage'][df_test['LotFrontage'].isnull()==True] = pd.Series(LotFrontage_values_test)\n\ndf_test['FireplaceQu'] = df_test['FireplaceQu'].fillna('null')\n\nmean_variables_test = ['BsmtUnfSF', 'GarageArea', 'GarageCars', 'TotalBsmtSF', 'BsmtFinSF2', 'BsmtFinSF1', 'BsmtHalfBath', 'BsmtFullBath', 'MasVnrArea', 'GarageYrBlt']\ndf_test[mean_variables_test] = df_test[mean_variables_test].fillna(value = df_test[mean_variables_test].mean())\n \nmode_variables_test = ['KitchenQual', 'SaleType', 'Exterior1st', 'Exterior2nd', 'Utilities', 'Functional', 'MSZoning', 'MasVnrType', 'BsmtFinType2', 'BsmtFinType1','BsmtQual', 'BsmtExposure', 'BsmtCond', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\nmode_test = ['TA', 'WD', 'VinylSd', 'VinylSd', 'AllPub', 'Typ', 'RL', 'None', 'Unf', 'GLQ', 'TA', 'No', 'TA', 'Attchd', 'Unf', 'TA', 'TA']\nfor i in range(0, 17, 1):\n    df_test[mode_variables_test[i]] = df_test[mode_variables_test[i]].fillna(mode_test[i])\n\nnumerical_test = df_test.select_dtypes(exclude='object')\nskewed_variables_test = numerical_test.apply(lambda x: skew(x))\nskewed_variables_test = skewed_variables_test[abs(skewed_variables_test) > 1]\nskewed_variables_test = skewed_variables_test.index\n\nlambda_value = 0.15\nfor variable in skewed_variables_test:\n    df_test[variable] = boxcox1p(df_test[variable], lambda_value)\ndf_test[skewed_variables_test] = np.log1p(df_test[skewed_variables_test])\n\nzero_variables_test = df_test.apply(lambda x: 100* (sum(x==0)\/ 1460))\nzero_variables_test = zero_variables_test[zero_variables_test > 80]\nzero_variables_test = zero_variables_test.index\ndf_test = df_test.drop(zero_variables_test, axis=1)\n\ndf_test['OverallCond'] = df_test['OverallCond'].apply(str)\ndf_test['YrSold'] = df_test['YrSold'].apply(str)\ndf_test['MoSold'] = df_test['MoSold'].apply(str)\n \nnumerical_test = df_test.select_dtypes(exclude='object')\ncategorical_test = df_test.select_dtypes(include='object')\ncategorical_test = pd.get_dummies(categorical_test, drop_first=True)\ndf_test = pd.concat([numerical_test, categorical_test], axis=1)\ndf_test.index = range(1453, 2912, 1)\n\n# making the testing data set shape-wise usable\ndf_test = pd.concat([df_train.drop('SalePrice', axis=1), df_test], axis=0)\ndf_test = df_test.loc[df_test.index > 1452]","b80394a4":"# final pre-processed testing data set\ndf_test","0e55b7e5":"# x are the indepedant\/ explanatory\/ regressor variables\n# y is the depedant\/ explained\/ regressed variable\nx = df_train.drop('SalePrice', axis=1)\ny = df_train['SalePrice']","a57881dc":"scaler = StandardScaler()\n\n# scaling the training data \nscaler.fit(x)\nx = scaler.transform(x)\n\n# scaling the testing data \nscaler.fit(df_test)\ndf_test = scaler.transform(df_test)","7686c8d5":"# creating a linear regression model\nlinear_model = LinearRegression()\nlinear_model.fit(x, y)\ny_predicted = linear_model.predict(x)","e1f12f0a":"# finding the score, mean absolute error, mean squared error and the root mean squared error\nscore = linear_model.score(x, y)\nMAE = metrics.mean_absolute_error(y, y_predicted)\nMSE = metrics.mean_squared_error(y, y_predicted)\nRMSE = np.sqrt(MSE)\n\nlinear = pd.DataFrame([score, MAE, MSE, RMSE], index=['score', 'MAE', 'MSE', 'RMSE'], columns=['Linear'])","e7cdb108":"# creating a random forest regression model\nrandom_forest_model = RandomForestRegressor()\nrandom_forest_model.fit(x, y)\ny_predicted = random_forest_model.predict(x)","c94a8812":"# finding the score, mean absolute error, mean squared error and the root mean squared error\nscore = random_forest_model.score(x, y)\nMAE = metrics.mean_absolute_error(y, y_predicted)\nMSE = metrics.mean_squared_error(y, y_predicted)\nRMSE = np.sqrt(MSE)\n\nrandom_forest = pd.DataFrame([score, MAE, MSE, RMSE], index=['score', 'MAE', 'MSE', 'RMSE'], columns=['Random Forest'])","7a5d5dc8":"# creating a ridge regression model\nridge_model = RidgeCV(alphas=(0.1, 1.0, 10.0), scoring='neg_mean_absolute_error')\nridge_model.fit(x, y)\ny_predicted = ridge_model.predict(x)","735e226e":"# finding the score, mean absolute error, mean squared error and the root mean squared error\nscore = ridge_model.score(x, y)\nMAE = metrics.mean_absolute_error(y, y_predicted)\nMSE = metrics.mean_squared_error(y, y_predicted)\nRMSE = np.sqrt(MSE)\n\nridge = pd.DataFrame([score, MAE, MSE, RMSE], index=['score', 'MAE', 'MSE', 'RMSE'], columns=['Ridge'])","c5f0a246":"# creating a lasso regression model\nlasso_model = LassoCV(eps=0.01, n_alphas=100, cv=5)\nlasso_model.fit(x, y)\ny_predicted = lasso_model.predict(x)","ca271991":"# finding the score, mean absolute error, mean squared error and the root mean squared error\nscore = lasso_model.score(x, y)\nMAE = metrics.mean_absolute_error(y, y_predicted)\nMSE = metrics.mean_squared_error(y, y_predicted)\nRMSE = np.sqrt(MSE)\n\nlasso = pd.DataFrame([score, MAE, MSE, RMSE], index=['score', 'MAE', 'MSE', 'RMSE'], columns=['Lasso'])","095c5456":"# creating an elastic net regression model\nelastic_net_model = ElasticNetCV(l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1], cv=5, max_iter=100000)\nelastic_net_model.fit(x, y)\ny_predicted = elastic_net_model.predict(x)","bcc1831e":"# finding the score, mean absolute error, mean squared error and the root mean squared error\nscore = elastic_net_model.score(x, y)\nMAE = metrics.mean_absolute_error(y, y_predicted)\nMSE = metrics.mean_squared_error(y, y_predicted)\nRMSE = np.sqrt(MSE)\n\nelastic_net = pd.DataFrame([score, MAE, MSE, RMSE], index=['score', 'MAE', 'MSE', 'RMSE'], columns=['Elastic Net'])","e4deb974":"# creating a XGB regression model\nXGB_model = xgb.XGBRegressor(objective='reg:linear', n_estimators=300, seed=107)\nXGB_model.fit(x, y)\ny_predicted = XGB_model.predict(x)","583a75df":"# finding the score, mean absolute error, mean squared error and the root mean squared error\nscore = XGB_model.score(x, y)\nMAE = metrics.mean_absolute_error(y, y_predicted)\nMSE = metrics.mean_squared_error(y, y_predicted)\nRMSE = np.sqrt(MSE)\n\nXGB = pd.DataFrame([score, MAE, MSE, RMSE], index=['score', 'MAE', 'MSE', 'RMSE'], columns=['XGB'])","386962a3":"# concatenating the results obtained from all the models\n# observe that XGB regression model is the best\npd.concat([linear, random_forest, ridge, lasso, elastic_net, XGB], axis=1)","d34fb175":"# y vs y_predicted table for the XGB regression model\npd.DataFrame({'y': df_train['SalePrice'],'y_predicted': y_predicted})","0f98e816":"# bias for the XGB regression model\nresiduals = y - y_predicted\nsns.scatterplot(x=y, y=y_predicted, s=100)\nplt.plot([10.5, 14], [10.5, 14], color='black', ls='--')\nplt.xlabel('y')\nplt.ylabel('y_predicted')\nplt.title('Bias')","0f283d20":"# residuals for the XGB regression model\nsns.scatterplot(x=y, y=residuals, s=100)\nplt.axhline(y=0, color='black', ls='--')\nplt.xlabel('y')\nplt.ylabel('residuals')\nplt.title('Residuals')","03b13494":"# predicting 'SalePrice' via XGB model earlier built\nSalePrice_predicted = XGB_model.predict(df_test)\n\n# the SalePrice predicted above are actually the logs of SalePrice\n# due to the transformation done in the normality section of pre-processing\nSalePrice_predicted = np.exp(SalePrice_predicted)\nsubmission = pd.DataFrame({'Id': Id, 'SalePrice': SalePrice_predicted})\nsubmission.to_csv(\"submission.csv\", index=False)","d3d13d83":"**Discarding \"Zero Variables\"**","94fb5715":"**Correlation**","66990898":"**Transforming some numerical variables into categorical variables**","477c94af":"**XGB**","0c94d5f6":"# Reading Data","f85cb976":"**Creating Dummy Variables**","a25259a2":"**Transforming \"Skewed Variables\"**","94952507":"**Numerical and Categorical Variables**","35cba897":"**Results**","c214ddd9":"**Lasso**","0a5aa718":"# Modelling","08ac4946":"**Elastic Net**","669397f3":"**XGB (continued)**","d9237b24":"# Pre-Processing","58e74361":"**Random Forest**","1d4a0a3b":"**Accounting for Missing Values**","44134c2f":"**Linear**","6b7ad3b2":"# Importing Libraries","d8ed2244":"**Scaling**","e3d0bd36":"**Discarding Outliers**","f1b0d2c1":"**Normality**","59b398b4":"**Ridge**","7fb34b30":"# Exploratory Data Analysis","653fe4dd":"# Submission"}}