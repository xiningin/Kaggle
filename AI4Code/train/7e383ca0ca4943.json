{"cell_type":{"bf33a404":"code","5a746582":"code","02c4d938":"code","0913d8b3":"code","cfc9c095":"code","fb9cc623":"code","d1ee9117":"code","ff9e522f":"code","53c37189":"code","9e2835e3":"code","982fb26d":"code","71f11f16":"code","42573cf0":"code","73120e58":"code","cbe9a48f":"code","6b70ebca":"code","2922c404":"code","b7a1fb09":"code","4c1d2b59":"code","3a564a6c":"code","a2220256":"code","af288bc4":"code","934237b7":"code","20b88446":"code","52b981f5":"code","94f49299":"code","7c7fc567":"code","64ef487a":"code","490c0cb4":"code","4382101d":"code","348992c3":"code","a5d7127a":"code","44037031":"code","a23dc563":"code","f6ee7cbf":"code","7ca66592":"code","58389e15":"code","339c672b":"code","a4f3a795":"code","e7e51e00":"code","bb3dc8fd":"code","768cfa41":"code","0bc3aec8":"code","3fdfebb3":"code","61300ebd":"code","4ec6f678":"code","220a1b73":"code","36ccd013":"markdown","2b8c3016":"markdown","ab154aee":"markdown","142b9c40":"markdown","c781e113":"markdown","65361de5":"markdown","630af10e":"markdown","215ac88e":"markdown","5f80de8c":"markdown","1a376e29":"markdown","f3a15e04":"markdown","287c2c0e":"markdown","ce4c80ab":"markdown","d93b852a":"markdown","ff72af4b":"markdown","0c0b7e7b":"markdown","e68fa6b1":"markdown","02065346":"markdown","a72ce7a6":"markdown","7c575edc":"markdown","fe7c4b84":"markdown","0c42ce9e":"markdown","5ad3cc1f":"markdown","38bd7a3c":"markdown","f5c9f39c":"markdown","b11b14be":"markdown","4d5747db":"markdown","701e9310":"markdown","22f603d0":"markdown","dd5c609a":"markdown","988cf545":"markdown","42ff4bf3":"markdown","358b5b3a":"markdown","9b4f2630":"markdown","5851a19c":"markdown","e4600c31":"markdown","03fcbe4b":"markdown","32f8ca20":"markdown","31c31394":"markdown","e2ba5d66":"markdown","e4cad4ff":"markdown","5ebe2706":"markdown","0938053d":"markdown","b37cc0e4":"markdown","e796cb92":"markdown","68c700fb":"markdown","90e5ade8":"markdown","8e3b53ac":"markdown","52329cbb":"markdown"},"source":{"bf33a404":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport torch\nimport helper_functions as hf\nimport meta_cleaning as mc\nimport eda_text as et\nimport config \nimport viz_plot as vp\nimport word_cloud_prep as wcp\nimport covid_clustering as  cc\nimport biobert_embedding as be\nimport spacy\nimport plotly.express as px\nfrom collections import defaultdict\nfrom timeit import default_timer as timer\nfrom IPython.display import HTML, display\nimport tabulate\n\nspacytokenizer = spacy.tokenizer.Tokenizer(be.nlp.vocab)\n\n# Any results you write to the current directory are saved as output.\nROOTDIR = \"..\/input\"\nDATADIR = os.path.join(ROOTDIR, 'CORD-19-research-challenge')","5a746582":"df_meta = pd.read_csv(os.path.join(DATADIR, \"metadata.csv\"))","02c4d938":"biorxiv_path = os.path.join(DATADIR, \"biorxiv_medrxiv\/biorxiv_medrxiv\/pdf_json\/\")\ndf_biorxiv = mc.parse_biorxiv(biorxiv_path)","0913d8b3":"comm_subset_path = os.path.join(\n    DATADIR, \"comm_use_subset\/comm_use_subset\/pdf_json\/\"\n)\ndf_comm = mc.parse_comm(comm_subset_path)\n","cfc9c095":"noncomm_subset_path = os.path.join(\n    DATADIR, \"noncomm_use_subset\/noncomm_use_subset\/pdf_json\/\"\n)\ndf_noncomm = mc.parse_noncomm(noncomm_subset_path)","fb9cc623":"df_meta.columns","d1ee9117":"# Convert publish time to type publish_date \ndf_meta[\"publish_date\"] =  pd.to_datetime(df_meta[\"publish_time\"])\ndf_merge = mc.merge_datasets(df_meta, df_biorxiv, df_comm, df_noncomm)\ndf_merge_impute = mc.impute_columns(df_merge)\ndf_meta_comp = mc.drop_duplicates(df_merge_impute)","ff9e522f":"df_meta_comp.head()\ndf_meta_comp.shape","53c37189":"mc.plot_missing_value_barchart(df_meta_comp)","9e2835e3":"df_authors = df_meta_comp[\"authors\"].apply(mc.author_feats)\nmc.plot_num_author_distrib(df_authors[\"num_authors\"])","982fb26d":"mc.plot_article_sources_distrib(df_meta_comp)","71f11f16":"df_publish_date = mc.groupby_publish_date(df_meta_comp)\nmc.plot_publish_date_distrib(df_publish_date)","42573cf0":"df_date_source = mc.gropuby_date_source(df_meta_comp)\nmc.plot_publish_date_wrt_sources(df_date_source)","73120e58":"df_preprocess_title = df_meta_comp[\"title\"].apply(\n        lambda x: et.nlp_preprocess(x, config.PUNCT_DICT)\n    )\n    \ndf_wc_title = et.corpora_freq(df_preprocess_title)\net.plot_distrib(df_wc_title, \"title\");","cbe9a48f":"df_preprocess_abstract = df_meta_comp[\"abstract\"].apply(\n        lambda x: et.nlp_preprocess(x, config.PUNCT_DICT)\n    )\ndf_wc_abstract = et.corpora_freq(df_preprocess_abstract)\net.plot_distrib(df_wc_abstract, \"abstract\");\n","6b70ebca":"df_preprocess_text = df_meta_comp[\"text\"].apply(\n        lambda x: et.nlp_preprocess_text(x, config.PUNCT_DICT)\n)\ndf_wc_text = et.corpora_freq(df_preprocess_text)\net.plot_distrib(df_wc_text, \"text\");","2922c404":"df_process_affiliation = df_meta_comp[\"affiliations\"].apply(et.process_affiliations)\n\ndf_wc_affiliation = et.corpora_freq(df_process_affiliation, affiliation=True)\nax = et.plot_distrib(df_wc_affiliation.iloc[1:], \"affiliation\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=90);","b7a1fb09":"from transformers import BertTokenizer, BertModel\n\nmodel_version =  os.path.join(ROOTDIR, \"biobert\/biobert\")\ndo_lower_case = True\nmodel = BertModel.from_pretrained(model_version)\ntokenizer = BertTokenizer.from_pretrained(\n    model_version, do_lower_case=do_lower_case\n)\n\n# define some parameters\nnsamples = 2000\nnsize = 2000","4c1d2b59":"title_len = df_meta_comp[\"title\"].apply(lambda x: len(x.split()) if not pd.isnull(x) else np.NaN)\nmean_title_len = np.nanmean(title_len)\nprint(\"Mean title length: %.1f words\" %mean_title_len)","3a564a6c":"df_title = df_meta_comp[\"title\"].copy()","a2220256":"print(\"Processing:%d entries\" % nsize)","af288bc4":"# you can reshuffle\n#shuffle_df_title = df_title#.sample(frac=1)\n\nstart = timer()\ndf_embedding_title = be.embed_loop(df_title, nsize, model, tokenizer, config.STOPWORDS, max_len = 48);\ndt = timer() - start\n","934237b7":"print(\"\\n\\nCalculation done in %f s\" % dt)\n","20b88446":"n_clusters = np.arange(10, 38, 2)\nsubtext = \"_title\"\ndf_clusters = cc.miniBatchClustering(df_embedding_title, None, n_clusters)","52b981f5":"import matplotlib.pylab as plt\nfig, axes = plt.subplots(3, 3, figsize=(12, 16))\n\ncc.plot_silhouette_graph(\n        df_embedding_title,\n        df_clusters,\n        n_clusters,\n        fig,\n        axes,\n    )","94f49299":"df_calinski = cc.evaluate_metric_score(df_embedding_title, df_clusters, metric=\"calinski\")\ndf_calinski[\"n_cluster\"] = n_clusters","7c7fc567":"ax = cc.plot_metric_vs_cluster(df_calinski)\nx = df_calinski['n_cluster'].values\ny = df_calinski['metric'].values\n\na1 = (y[5]-y[0])\/(x[5]-x[0])\nb1 = 1100\n\ndef f(x, a, b):\n    return a*x+b\n\nx_asympt= np.arange(10., 32.)\ny1 = f(x_asympt, a1, b1)\n\nax.axhline(350,  color=\"black\", linestyle=\"dashed\", linewidth=1)\nax.axvline(25, ymax=.6, color=\"red\", linestyle=\"dashed\", linewidth=1)\n\nax.plot(x_asympt, y1, color=\"black\", linestyle=\"dashed\", linewidth=1)\nax.set_ylabel(\"Calinksi-Harabasz score\");\n","64ef487a":"\nX = vp.sphereize(df_embedding_title.values)\nindex = df_embedding_title.index\ncolors = px.colors.qualitative.Alphabet + px.colors.qualitative.Plotly\ndf_pca = vp.compute_pca(X, index)\ndf_resampled = vp.resample(df_pca, df_meta_comp, df_clusters, colors, nsamples=nsamples, n_cluster=24)\ninfo_title = vp.prepare_info(df_resampled)\nvp.plot_tsne(df_resampled, \"X_0\", \"X_1\", info_title, var_z=\"X_2\")\n","490c0cb4":"df_title_cluster = df_clusters.merge(\n        df_title, left_index=True, right_index=True\n    )","4382101d":"df_title_cluster[\"title_process\"] = df_title_cluster[\"title\"].apply(\n        lambda x: et.nlp_preprocess(str(x), config.PUNCT_DICT)\n        if not pd.isnull(x) else \"\")\n\ndf_title_wc = wcp.prepare_word_cloud(df_title_cluster[\"title_process\"])\n# get rid of top 10 words\nextra_stopwords = df_title_wc.head(10).keys().tolist()\n\nextra_stopwords += [\"covid\", \"sars\", \"infectious\", \"19\", \"volume\", \"index\", \"chapter\", \"volume\", \"1\"]","348992c3":"wc_title = defaultdict(int)\nncluster = 24\nnclusters = sorted(\n        df_title_cluster[\"labels_cluster_%d\" % ncluster].unique().tolist()\n    )\n\nfor k in nclusters:\n    temp = df_title_cluster[df_title_cluster[\"labels_cluster_%d\" % ncluster] == k][\n        \"title_process\"\n    ]\n\n    try:\n        wc_title[k] = wcp.prepare_word_cloud(temp, extra_stopwords)\n    except ValueError:\n        pass\n\nn_top = 50\nfig, axes = plt.subplots(6, 4, figsize=(24, 18))\nax = axes.flatten()\nwcp.plot_word_cloud(wc_title, ax)\nplt.tight_layout(w_pad=4.0)","a5d7127a":" df_title_abstract = df_meta_comp[[\"title\", \"abstract\"]].apply(\n            be.join_title_abstract, axis=1\n    )\n    ","44037031":"title_abstract_len = df_title_abstract.apply(lambda x: len(x.split()) if not pd.isnull(x) else np.NaN)\nmean_title_abstract_len = np.nanmean(title_abstract_len)\nprint(\"Mean title abstract length: %.1f words\" %mean_title_abstract_len)","a23dc563":"print(\"Processing:%d entries\" % nsize)\n#shuffle_df_title_abstract = df_title_abstract.sample(frac=1)","f6ee7cbf":"start = timer()\ndf_embedding_title_abstract = be.embed_loop(df_title_abstract, nsize, model, tokenizer, config.STOPWORDS, max_len = 256);\ndt = timer() - start\n","7ca66592":"print(\"\\n\\nCalculation done in %f s\" % dt)","58389e15":"n_clusters = np.arange(4, 30, 2)\ndf_clusters_title_abstract = cc.miniBatchClustering(df_embedding_title_abstract, None, n_clusters)","339c672b":"fig, axes = plt.subplots(3, 3, figsize=(12, 16))\n\ncc.plot_silhouette_graph(\n        df_embedding_title_abstract,\n        df_clusters_title_abstract,\n        n_clusters[-10:],\n        fig,\n        axes,\n    )","a4f3a795":"df_calinski_title_abstract = cc.evaluate_metric_score(df_embedding_title_abstract, df_clusters_title_abstract, metric=\"calinski\")\ndf_calinski_title_abstract[\"n_cluster\"] = n_clusters","e7e51e00":"\nax = cc.plot_metric_vs_cluster(df_calinski_title_abstract)\nx = df_calinski_title_abstract['n_cluster'].values\ny = df_calinski_title_abstract['metric'].values\n\na1 = (y[4]-y[3])\/(x[4]-x[3])\nb1 = 1350\n\ndef f(x, a, b):\n    return a*x+b\n\nx_asympt= np.arange(5., 26.)\ny1 = f(x_asympt, a1, b1)\n\nax.axhline(400,  color=\"black\", linestyle=\"dashed\", linewidth=1)\nax.axvline(21, ymax=.5, color=\"red\", linestyle=\"dashed\", linewidth=1)\nax.plot(x_asympt, y1, color=\"black\", linestyle=\"dashed\", linewidth=1)\n\nax.set_ylabel(\"Calinski-Harabasz score\");","bb3dc8fd":"X_title_abstract = vp.sphereize(df_embedding_title_abstract.values)\nindex_title_abstract = df_embedding_title_abstract.index\ncolors = px.colors.qualitative.Alphabet\ndf_pca_title_abstract = vp.compute_pca(X_title_abstract, index_title_abstract)\ndf_resampled_title_abstract = vp.resample(df_pca_title_abstract, df_meta_comp, \n                                          df_clusters_title_abstract, colors, nsamples=nsamples )\ninfo_title_abstract = vp.prepare_info(df_resampled_title_abstract)\nvp.plot_tsne(df_resampled_title_abstract, \"X_0\", \"X_1\", info_title_abstract, var_z=\"X_2\")","768cfa41":"df_title_abstract.name = \"title_abstract\"\ndf_title_abstract_cluster = df_clusters_title_abstract.merge(\n        df_title_abstract, left_index=True, right_index=True\n    )\ndf_title_abstract_cluster[\"title_abstract_process\"] = df_title_abstract_cluster[\"title_abstract\"].apply(\n        lambda x: et.nlp_preprocess(str(x), config.PUNCT_DICT)\n        if not pd.isnull(x) else \"\")","0bc3aec8":"df_title_abstract_wc = wcp.prepare_word_cloud(df_title_abstract_cluster[\"title_abstract_process\"])\n# since there are more words, we consider the top 20 words as stop words\nextra_stopwords_title_abstract = df_title_abstract_wc.head(20).keys().tolist()\n\nextra_stopwords_title_abstract += [\"covid\", \"sars\", \"infectious\", \"19\", \"may\", \"can\", \"volume\", \"index\", \"chapter\", \"volume\", \"used\", \"also\"]","3fdfebb3":"wc_title_abstract = defaultdict(int)\nncluster = 20\nnclusters = sorted(\n        df_title_abstract_cluster[\"labels_cluster_%d\" % ncluster].unique().tolist()\n    )\n\nfor k in nclusters:\n    temp = df_title_abstract_cluster[df_title_abstract_cluster[\"labels_cluster_%d\" % ncluster] == k][\n        \"title_abstract_process\"\n    ]\n\n    try:\n        wc_title_abstract[k] = wcp.prepare_word_cloud(temp, extra_stopwords_title_abstract)\n    except ValueError:\n        pass\n\nn_top = 50\nfig, axes = plt.subplots(5, 4, figsize=(24, 18))\nax = axes.flatten()\nwcp.plot_word_cloud(wc_title_abstract, ax)\nplt.tight_layout(w_pad=4.0)","61300ebd":"from ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact_manual, Textarea, Layout","4ec6f678":"text_area_layout = Layout(width=\"70%\", height=\"50px\")\ntext_area = widgets.Textarea(value=\"Range of incubation periods for the disease in humans and how long individuals are contagious, even after recovery.\", placeholder=\"Enter your text here.\", layout=text_area_layout)\n\nint_slider_layout = Layout(width=\"50%\")\nint_slider = widgets.IntSlider(description=\"Select number of results to show\",\n                               min=1, \n                               max=40, \n                               value=10, \n                               layout=int_slider_layout,\n                               style={'description_width': 'initial'}\n                              )\n\nradio_buttons_layout = Layout(width=\"50%\")\nradio_buttons = widgets.RadioButtons(description=\"select embeddings\", \n                                     value='title', \n                                     options=['title', 'title+abstract'],\n                                     style={'description_width': 'initial'},\n                                     layout=radio_buttons_layout\n                                    )\n\ntoggle_button = widgets.ToggleButton(value=True)\n\ncheckbox = widgets.Checkbox(value=False, description='Show abstracts', disabled=False, indent=False)","220a1b73":"from sklearn.metrics.pairwise import cosine_similarity\n@interact\ndef plot_search_results(emb=radio_buttons, n=int_slider, show_abstracts=checkbox, text=text_area):\n    if text.split() !=\"\":\n        if emb == \"title\": \n            embs = df_embedding_title\n            max_len = 48\n        elif emb == \"title+abstract\":\n            embs = df_embedding_title_abstract\n            max_len = 256\n        \n        print(f\"Displaying {n} most similar results for \\n{text} ...\\n\")\n        \n        tokenized_text = be.custom_tokenize(text, config.STOPWORDS, tokenizer, spacytokenizer)\n        embedding_text = be.embed_text(tokenized_text, model, tokenizer, max_len=max_len).mean(1).detach().numpy()\n        #row_ids, ordered_dict = be.top_n_closest(embedding_text, embs, df_meta_comp)\n        similarities = cosine_similarity(embs.values, embedding_text).reshape(1,-1)\n        indices = np.argsort(similarities)[0]\n        indices = indices[::-1][:n]\n        row_ids = embs.iloc[indices].index\n      \n        for i, (row_id, index) in enumerate(zip(row_ids, indices)):\n\n            title = df_meta_comp.loc[row_id]['title']\n            abstract = df_meta_comp.loc[row_id]['abstract']\n            print(f'result {i} title : {title}')\n            print(f'similarity : {similarities[0][index]}')\n            \n            if show_abstracts:\n                print('')\n                print(f'result {i} abstract : {abstract}')\n\n            print('----' )\n    else:\n        print('no query, no results baby.')","36ccd013":"The embedding of title is pretty straightforward, as it normally contains a mean of ~12.5 words. ","2b8c3016":"While we see the publication of several articles fairly recently, eg WHO, medrxiv and CZI, PubMed and Elsevier on the other hand have contributions since 2002, and they are the ones which have spikes of publication at the end of the year.\n\nFor publications prior to 2002, topics are mostly about on ILI (Influence-like illnesses) as SARS virus has not known yet.\n\nIn the coming days\/weeks, we'll see an avalanche of papers submitted to biorxiv and medrxic as they don't require any peer reviewing to get published. That implies that the to-be-developed search engine has to be scalable.","ab154aee":"If the abstract has provided us with some uninformative words, we observe that this type of vocab is even more present in the text.","142b9c40":"**Parsing Comm use subset articles**","c781e113":"## On the sources","65361de5":"We observe there are many contributions for PubMed (PMC) and Elsevier, and the rest not as many, it might be dependant on the establishment of these journals. Here is summary of these journals.\n- **PubMed Central\u00ae (PMC)**: free full-text archive of biomedical and life sciences journal literature at the U.S, about 5.9 MILLION Articles are archived in PMC.\n- **Elsevier**: Dutch publishing and analytics company specializing in scientific, technical, and medical content.\n- **WHO**: World Health Organization\n- **biorxiv**: the preprint server for biology. \n- **merdxiv**: the preprint server for medical health science\n- **CZI**: Chan Zuckerberg Initiative","630af10e":"# Conclusions\nIn this kernel, we have used BioBERT embeddings to embed our text data, and using this to calculate the similarity when a user enters a query. We have performed clustering on the datapints and have used PCA to reduce the number of dimensions and visualize them. With word cloud, we have been able to get an idea of the topic of each cluster. And finally, we build a demo to test out two settings, one embedding using title and the other one using both title and abstract. The returned results seem to be a little bit off.\n\n# Next steps\n\nUsing BioBERT embeddings directly to identify the semantic similarity has shown some caveats: it is very sensitive to length and returned results are not necessary very relevant. We have since started exploring Universal Sentence Encoder to compute our embeddings, and it has shown some promising results. Stay tuned for more \ud83d\ude00.","215ac88e":"Now we loop over all clusters and preprocess the text to build the vocab for wordcloud representation.\n","5f80de8c":"## On the affiliation","1a376e29":"### Visualization \nProjection with PCA.","f3a15e04":"By testing with some queries, it is observed that the cosine similarities are very high in both cases. Regarding this, we understood that from this article [Semantic Similarity in Sentences and BERT](https:\/\/medium.com\/analytics-vidhya\/semantic-similarity-in-sentences-and-bert-e8d34f5a4677) that BERT is not trained for semantic sentence similarity directly like the Universal Sentence Encoder or InferSent models. Therefore, BERT embeddings cannot be used directly to apply cosine distance to measure similarity. That constitutes our next step.\n\nAnd when the query is short, using title+abstract, we find that most of the returned responses do not have any abstract, this shows that it is probably sensitive to length.","287c2c0e":"# Embedding\nIn this section, we will proceed with the embeddings of our text data. The idea of embedding is to quantify and categorize semantic similarities between linguistic items based on their distributional properties in large samples of language data. The embedded data are also in a lower dimension as compared to only using classical methods, such as count or tfidf vectorizations Once we have the embeddings, we use them as latent variables for further processing, eg clusterisation, similarity computation with embeded queries... Here, we focus on the aforementioned tasks.\n\nNow comes the question of what embedding to be used. We started of doing a literature research on the most recent NLP embeddings, and BERT [Attention Is All You Need\n](https:\/\/arxiv.org\/abs\/1706.03762) comes into our mind. Further research led us to SciBERT [SciBERT: A Pretrained Language Model for Scientific Text\n](https:\/\/arxiv.org\/abs\/1903.10676) and BioBERT. We have therefore performed a comparison between BERT, sciBERT and bioBERT embeddings. We obtained more relevant results using bioBERT (results not included in this notebook). What we'll be doing here is to perform a comparison between only using title and title with abstract, hence testing the sensitivity BioBERT embeddings to the length of the text.","ce4c80ab":"### Wordcloud\nWe further plot the wordcloud for each cluster to help us with the interpretation. Since the vocab of BioBERT are wordpieces, it doesn't make too much sense to use them for our wordcloud. Therefore, we proceed with the conventional NLP preprocessing pipeline: Lemmatokenization, Stopword removal, Punctuation removal, and Count vectorization. \n\nWe first perform a preprocessing on the general corpus to identify the top words and then consider them as stopwords.","d93b852a":"## BioBERT on title and abstract\nFor this task, we concatenate title and abstract. Since BERT tokenization only allows at most 512 tokens, lengthy text will be truncated. The following procedure ressembles that of embedding on title.","ff72af4b":"## BERT in a nutshell\nThere are some key elements a reader ought to know while using \"BERT\"-like embeddings.\n- BERT has about ~30k words\/subwords (wordpiece embeddings) as vocabulary, an input is mapped to these words\/subwords.\n- Little or no preprocessing is required as the BERT tokenizer will intelligetnly map these words into the vocab, ie common words such as \"the\", or slightly uncommon ones such as \"quantum\", \"constantinopole\" are present in BERT vocabulary, a direct mapping is therefore carried out. However the word \"aerodynamics\" is absent, it is broken down into 4 subwords: electro ##dy ##nami ##cs where these subwords are present in the vocab. \n- Some mandatory tokens must be included such as [CLS] stands for class and often included at the beggining of a sentence and [SEP] stands for separation for the next sentence prediction task, as the underlying model is an attention-based bidirectional RNN structure.\n- The output embeddings have a fixed size of (x, 768), where x is the number of tokens which we set to a maximum length according to the corpus.","0c0b7e7b":"To determine the optimal number of clusters, we use the Silhouette score to measure how similar an object is to its own cluster (cohesion) compared to other clusters (separation), and this metric gives a value between -1 to 1 with -1 meaning the datapoint does not belong to the attributed cluster at all, and 1 indicating good clusterisation.","e68fa6b1":"# Parsing\nWe first would like to include information from the json files to get a complete dataset. The following steps parse documents from Biorxiv, comm use subset and non-comm use subset as indicated the CORD-19-research-challenge.","02065346":"# References\n* [scibert-embeddings](https:\/\/www.kaggle.com\/isaacmg\/scibert-embeddings)\n* [Ipywidgets](https:\/\/ipywidgets.readthedocs.io\/en\/stable\/)\n* [BioBERT](https:\/\/github.com\/dmis-lab\/biobert)\n* [SciBERT](https:\/\/github.com\/allenai\/scibert)\n* [Hugging Face Transformers](https:\/\/github.com\/huggingface\/transformers)","a72ce7a6":"**Parsing Biorxiv articles**","7c575edc":"### Wordcloud","fe7c4b84":"We observe that the clustering is rather coherent. In order to evaluate the pertinence of clusters, our next step is to seek experts.","0c42ce9e":"From the above count plot, we can identify the meaning behind each variable as follows, most of them are self-explanatory:\n\n* ID numbers:\n    * cord_uid,\n    * doi,\n    * pubmed_id (has been exlcuded),\n    * pmcid (has been exlcuded),\n    * WHO #Covidence (has been exlcuded),\n    * Microsoft Academic Paper ID (has been exlcuded),\n    * sha (only this is important to retrieve data from jsons)\n* url: link to the article\n* source_x: source of the article\n* has_full_text: (boolean) indicate whether the article has full text\n* journal\n* authors_list\n* first_author: the first ofauthor of the article\n* last_author: the author who is in the last position, corresponds normally to the chief of the research institute\n* affiliations\n* bibliography\n* raw_bibliography : bibliography in its raw position\n* title\n* abstract\n* text\n* publish_date\n* full_text_file: path to the json file (has been excluded)\n\nA lot of them are related to IDs, here we are only interested in one which is sha which will act as a unique key to retrieve the articles. Among other variables that are useful are url for documentation purpose, source_x, journal, authors, title, abstract and publish_time.\n\nNote that there are several articles (15 of them) that do not contain title. By right, they should be exlcuded from this dataset as they are not informative.","5ad3cc1f":"## On the publish date","38bd7a3c":"**Parsing Non Comm use subset articles**","f5c9f39c":"> **\"Nothing in life is to be feared, it is only to be understood. Now is the time to understand more, so that we may fear less.\" **\n--Marie Curie\n\nIn the midst of this crisis, we are a team of data scientists who would like to put our skills into good use, to shed light by providing a tool to the scientific community using NLP to answer key questions from the scientific literature. \n\nThis tool we are building here is a search engine based on the similarity score calculated on embeddings via BioBERT ([BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https:\/\/github.com\/dmis-lab\/biobert)). We also provide a clustering among all articles so that users can get some insights by navigating between articles that are similar.\n\nWe attempt to write a kernel that is more like an article, instead of drown by codes. If you want to get your hand dirty, please feel free to have a look at the enclosed utility scripts. \n","b11b14be":"We observe that the top words are dominated by influenza-like illnesses and COVID-19 related vocabularies, eg virus, respiratory, coronavirus. It is also noted that the vocab is also related to research terms, such as clinical, study, analysis. There is no doubt that we are dealing with COVID-19 literature.","4d5747db":"Above each word cloud is the top 3 word occurences. Topics are quite distinctive knowing that we have only used title as input. Among the topics, we have syndromes, detection, genetics (rna), control and diagnosis, surveillance...\n","701e9310":"# Introduction","22f603d0":"Nowadays, an article is often an contribution of several people, most likely within a range of 2 to 30.","dd5c609a":"## BioBERT on title\n","988cf545":"**On the abstract**","42ff4bf3":"By plotting the Calinski-Harabasez score wrt number of clusters, we observe an elbow. Though it's not a very precise method of determining the optimal number of clusters, but it at least provides us with some elements of response. The optimal number of clusters determined here is 30. ","358b5b3a":"## Load BioBERT model\nBioBERT model can be obtained from this [GitHub](https:\/\/github.com\/dmis-lab\/biobert) page. We use transformers from HuggingFace to handle our model because of its simplicity, with just a few lines of code, we have the embedding module up and running.\n\n\u26a0\ufe0f **We have previously encountered some RAM issues in generating embeddings. Therefore this part of the code has taken into account batch computing to not overload RAM. **","9b4f2630":"\n### Visualization\nAnother way to check if our datapoints are being correctly clustered, we can visualize in lower dimensions. We choose to perform a PCA with 3 components to have a projection in 3D. Before carrying out the PCA projection, we sphereize our datapoints (the embeddings) by shifting each datapoint by the centroid and make it unit norm. By doing this the datapoints will be better distributed in the spherical space.\n\n**Give it a spin at the 3D plot, and hover over the datapoints to get some information.**","5851a19c":"We observe that there are spikes of publication at the end of the year. Do they only concern certain journals?\n\nLooking at this time plot, we strongly suspect that there is a dependance between the publish date and the sources, let's have a look at that to confirm this hypothesis. We shall only look at the articles from 2002 onwards, as articles before this year are not as frequent.","e4600c31":"Since this work is just a first look of our [website](https:\/\/covid19.ai2prod.com\/), we will randomly sample a handful of articles, feel free to click on the link to test our search engine. ","03fcbe4b":"From the average Silhouette score, we observe that the value is rather low, but again this score should not be the hard and fast rule in determining the optimum number of clusters. Looking closely to the Silhouette plot, we observe that a lot of the datapoints are wrongly attributed but their Silhouette scores are contained between -0.1 and 0 signifying they are not that badly attributed. Of course, looking at the positive side, the Silhouette scores are not that high either. \n\nFor low number of clusters (4-8), we observe that cluster size is rather uneven, and potentially they need to be divided into more sub clusters. Hence higher number of clusters is prioritised.\n\nThe fact that many datapoints are attributed to the wrong cluster might indicate that a lot of articles are cross-discipline, and therefore don't belong to only cluster. \n\nFrom this analysis, we have seen that the Silhouette graph is not very conclusive. We pursue by measuring another metric which is the Calinski-Harabasz score, it is defined as ratio between the within-cluster dispersion and the between-cluster dispersion.\n","32f8ca20":"These Silhouette plots look a little bit different as compared to only using title, the values are higher and the erros in cluster attribution seem to be lower. Let's have a look at the Calinski-Harabasz score.","31c31394":"# Demo\nIn order to have an idea on how the difference between the two studies, we have prepared a demo to have a sense of the performance using ipywidgets.","e2ba5d66":"The top 10 institutitions that have contributed to this dataset are mostly Chinese institutions. This makes sense as China is where the first-wave of COVID-19 has taken place. In summary, the contribution to this dataset is a global effort.","e4cad4ff":"## EDA on text data\n**On the title**\n\nWe perform a light EDA on text data, the NLP preprcocessing pipeline includes the following steps:\n- lowercase\n- remove puncuations\n- remove https\n- remove stopwords","5ebe2706":"The word count plot above shows a richer vocab that does not always seem specified to the medical research literature at hand, eg also, may, can, however, not, abstract ... By right, if we were handling the corpus with the bag-of-words approach, these words should be removed. This plot has been insightful in allowing us to build our strategy in our following studies.","0938053d":"**On the text**","b37cc0e4":"As compared to the previous study using only title, we observe that the vocab is richer. The topics are also very distinctive. We observe that there are topics related to treatment and vaccines, the genes, the outbreak in China, organization and research...","e796cb92":"# EDA\n## Understanding variables \nBefore working on the search engine, we perform some data exploration to get a hint of what data are awaiting us.\n\nWe first have a look at the missing values. This helps us first understand the variables; and subsequently help us in choosing the variables to work with.","68c700fb":"A peek on the completed meta dataset.","90e5ade8":"**Merge dataset**\n\nWe proceed by merging all the dataset using the unique key \"sha\", and check if there are some duplicates in our dataset.","8e3b53ac":"## On the authors","52329cbb":"### Clustering\nWith embeddings done, we can proceed with clustering. We use MiniBatch Kmeans to cluster our datapoints, this again goes with our aim of performing all computations in batch. Then we perform a PCA projection on 3D. "}}