{"cell_type":{"68f4baae":"code","8a59dabc":"code","9a3734f3":"code","dedac287":"code","d0603728":"code","a02f7ee8":"code","06e4e0d2":"code","7e836bbb":"code","7c9571ca":"code","02bce9b4":"code","45b2f7ea":"code","b6726b67":"code","2f1011f1":"code","658714f8":"markdown","11b5780d":"markdown","cc7fd89c":"markdown"},"source":{"68f4baae":"import numpy as np\nimport pandas as pd\nimport scipy\nimport gc\nimport pickle\n\nfrom scipy.stats import ks_2samp, mode\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, ShuffleSplit\nfrom sklearn.model_selection._split import check_cv\nfrom sklearn.base import clone, is_classifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.grid_search import GridSearchCV, RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline, FeatureUnion\n\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras.callbacks import *\nfrom keras.models import Model, Sequential\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nfrom bayes_opt import BayesianOptimization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","8a59dabc":"def read_data():\n    print(\"############# Read the data #############\")\n    \n    train = pd.read_csv(\"..\/input\/train.csv\", index_col = 0)\n    test = pd.read_csv(\"..\/input\/test.csv\", index_col = 0)\n    sub = pd.read_csv(\"..\/input\/sample_submission.csv\", index_col = 0)\n    print(\"\\nTrain shape: {}\\nTest shape: {}\".format(train.shape, test.shape))\n    \n    return train, test, sub","9a3734f3":"train_orig, test_orig, sub_orig = read_data()","dedac287":"train_orig.head()","d0603728":"y_train = train_orig['Target']\nx_train = train_orig.drop('Target', 1)\nx_test = test_orig.copy()\ntest_id = test_orig.index","a02f7ee8":"class MissingValuesImputer(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, impute_zero_columns):\n        self.impute_zero_columns = impute_zero_columns\n        \n    def fit(self, X, y = None):\n        print(\"Mean Values Imputer\")\n        return self\n    \n    def transform(self, X, y = None):\n        \n        # Fill missing values for v18q1, v2a1 and rez_esc\n        for column in self.impute_zero_columns:\n            X[column] = X[column].fillna(0)\n\n        # For meaneduc we use the average schooling of household adults\n        self.X_with_meaneduc_na = X[pd.isnull(X['meaneduc'])]\n        self.mean_escolari_dict = dict(self.X_with_meaneduc_na.groupby('idhogar')['escolari'].apply(np.mean))\n        for row_index in self.X_with_meaneduc_na.index:\n            row_idhogar = X.at[row_index, 'idhogar']\n            X.at[row_index, 'meaneduc'] = self.mean_escolari_dict[row_idhogar]\n            X.at[row_index, 'SQBmeaned'] = np.square(self.mean_escolari_dict[row_idhogar])\n        return X","06e4e0d2":"class CategoricalVariableTransformer(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        self.categorical_columns = ['idhogar']\n        \n    def fit(self, X, y = None):\n        print(\"Categorical Variables Transformer\")\n        return self\n    \n    def transform(self, X, y = None):\n        X['dependency'] = np.sqrt(X['SQBdependency'])\n\n        X.loc[X['edjefe'] == 'no', 'edjefe'] = '0'\n        X.loc[X['edjefe'] == 'yes', 'edjefe'] = '1'\n        \n        X.loc[X['edjefa'] == 'no', 'edjefa'] = '0'\n        X.loc[X['edjefa'] == 'yes', 'edjefa'] = '1'\n\n        X['edjefa'] = X['edjefa'].astype(int)\n        X['edjefe'] = X['edjefe'].astype(int)\n    \n        label_encoder = LabelEncoder()\n        for column in self.categorical_columns:\n            X[column] = label_encoder.fit_transform(X[column])\n\n        return X","7e836bbb":"class UnnecessaryColumnsRemoverTransformer(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, axis = 1):\n        print(\"Unnecessary Columns Remover Transformer\")\n        self.axis = axis\n        self.unnecessary_columns = [\n                                     'r4t3', 'tamhog', 'tamviv', 'hogar_total', 'v18q', 'v14a', 'agesq',\n                                     'mobilephone', 'energcocinar1', 'sanitario6',\n                                     'estadocivil7', 'lugar1', 'area1', 'female'\n                                   ]\n        \n    def fit(self, X, y = None):\n        unnecessary_columns_to_extend = [\n            [col for col in X.columns.tolist() if 'SQB' in col],\n            [col for col in X.columns.tolist() if 'epared' in col],\n            [col for col in X.columns.tolist() if 'etecho' in col],\n            [col for col in X.columns.tolist() if 'eviv' in col],\n            [col for col in X.columns.tolist() if 'instlevel' in col],\n            [col for col in X.columns.tolist() if 'pared' in col],\n            [col for col in X.columns.tolist() if 'piso' in col],\n            [col for col in X.columns.tolist() if 'techo' in col],\n            [col for col in X.columns.tolist() if 'abastagua' in col],\n            [col for col in X.columns.tolist() if 'elimbasu' in col],\n            [col for col in X.columns.tolist() if 'tipoviv' in col]\n        ]\n        \n        for col_list in unnecessary_columns_to_extend:\n            self.unnecessary_columns.extend(col_list)\n        return self\n    \n    def transform(self, X, y = None):\n        X = X.drop(self.unnecessary_columns, axis = self.axis)\n        return X","7c9571ca":"class ReverseOHETransformer(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, axis = 1):\n        self.axis = axis\n        self.ohe_column_prefixes = ['epared', 'etecho', 'eviv', 'instlevel', \n                                    'pared', 'piso', 'techo', 'abastagua']\n                                    \n        \n    def fit(self, X, y = None):\n        print(\"Reverse OHE Transformer\")\n        \n        self.ohe_columns_dict = dict()\n        for column_prefix in self.ohe_column_prefixes:\n            ohe_columns = [col for col in X.columns if col.startswith(column_prefix)]\n            self.ohe_columns_dict[column_prefix] = ohe_columns\n        return self\n    \n    def transform(self, X, y = None):\n        for ohe_column_prefix, ohe_column_list in self.ohe_columns_dict.items():\n            ohe_df = X[ohe_column_list]\n            ohe_df.columns = list(range(ohe_df.shape[1]))\n            ohe_numeric_df = ohe_df.idxmax(axis = self.axis)\n            ohe_numeric_df.name = ohe_column_prefix\n            X = pd.concat([X, ohe_numeric_df], axis = self.axis)\n            \n            # Remove the columns from the data\n            X = X.drop(ohe_column_list, axis = self.axis)\n        return X","02bce9b4":"class FeatureEngineeringTransformer(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, axis = 1):\n        self.axis = axis\n        \n        # individual level boolean features\n        self.individual_boolean_features = ['dis', 'male', 'estadocivil1', 'estadocivil2', \n                                            'estadocivil3', 'estadocivil4', 'estadocivil5', 'estadocivil6', \n                                            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', \n                                            'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8',  \n                                            'parentesco9', 'parentesco10', 'parentesco11']\n\n        # individual level ordered features\n        self.individual_ordered_features = ['escolari', 'age']\n        \n    def fit(self, X, y = None):\n        print(\"Feature Engineering Transformer\")\n        self.more_columns_to_drop = [\n            [col for col in X.columns.tolist() if 'parentesco' in col and 'parentesco1' not in col],\n            ['idhogar']\n        ]\n        \n        f = lambda x: x.std(ddof = 0)\n        f.__name__ = 'std_0'\n        self.aggregate_features = (['mean', 'max', 'min', 'sum', f])\n        return self\n    \n    def transform(self, X, y = None):\n        \n        # Rooms\n        X['rent_per_room'] = X['v2a1']\/X['rooms']\n        X['adults_per_room'] = X['hogar_adul']\/X['rooms']\n        X['males_per_room'] = X['r4h3']\/X['rooms']\n        X['females_per_room'] = X['r4m3']\/X['rooms']\n        X['children_per_room'] = X['hogar_nin']\/X['rooms']\n        X['humans_per_room'] = X['hhsize']\/X['rooms']\n        X['beds_per_room'] = X['bedrooms']\/X['rooms']\n        \n        # Bedroom\n        X['adults_per_bedroom'] = X['hogar_adul']\/X['bedrooms']\n        X['males_per_bedroom'] = X['r4h3']\/X['bedrooms']\n        X['females_per_bedroom'] = X['r4m3']\/X['bedrooms']\n        X['children_per_bedroom'] = X['hogar_nin']\/X['bedrooms']\n        X['humans_per_bedroom'] = X['hhsize']\/X['bedrooms']\n        \n        X['persons12less_fraction'] = (X['r4h1'] + X['r4m1'])\/X['hhsize']\n        X['males12plus_fraction'] = X['r4h2']\/X['hhsize']\n        X['total_males_fraction'] = X['r4h3']\/X['hhsize']\n        X['females12plus_fraction'] = X['r4m2']\/X['hhsize']\n        X['all_females_fraction'] = X['r4m3']\/X['hhsize']\n        X['rent_per_person'] = X['v2a1']\/X['hhsize']\n        X['mobiles_per_person'] = X['qmobilephone']\/X['hhsize']\n        X['tablets_per_person'] = X['v18q1']\/X['hhsize']\n        X['mobiles_per_male'] = X['qmobilephone']\/X['r4h3']\n        X['tablets_per_male'] = X['v18q1']\/X['r4h3']\n        \n#         X['males_per_females'] = X['r4h3']\/X['r4m3']\n#         X['males12plus_per_females12plus'] = X['r4h2']\/X['r4m2']\n#         X['males12less_per_females12less'] = X['r4h1']\/X['r4m1']\n#         X['number_of_non_bedrooms'] = np.abs(X['rooms'] - X['bedrooms'])\n        \n        # Create individual-level features\n        grouped_df = X.groupby('idhogar')[self.individual_boolean_features + self.individual_ordered_features]\n        grouped_df = grouped_df.agg(self.aggregate_features)\n        X = X.join(grouped_df, on = 'idhogar')\n        \n        # Finally remove the other parentesco columns since we are only going to use only heads of\n        # households for our scoring\n        for col in self.more_columns_to_drop:\n            X = X.drop(col, axis = self.axis) \n        \n        return X","45b2f7ea":"class LGBClassifierCV(BaseEstimator, RegressorMixin):\n    \n    def __init__(self, axis = 0, lgb_params = None, fit_params = None, cv = 3, perform_bayes_search = False, perform_random_search = False, use_train_test_split = False, use_kfold_split = True):\n        self.axis = axis\n        self.lgb_params = lgb_params\n        self.fit_params = fit_params\n        self.cv = cv\n        self.perform_random_search = perform_random_search\n        self.perform_bayes_search = perform_bayes_search\n        self.use_train_test_split = use_train_test_split\n        self.use_kfold_split = use_kfold_split\n    \n    @property\n    def feature_importances_(self):\n        feature_importances = []\n        for estimator in self.estimators_:\n            feature_importances.append(\n                estimator.feature_importances_\n            )\n        return np.mean(feature_importances, axis = 0)\n    \n    @property\n    def evals_result_(self):\n        evals_result = []\n        for estimator in self.estimators_:\n            evals_result.append(\n                estimator.evals_result_\n            )\n        return np.array(evals_result)\n    \n    @property\n    def best_scores_(self):\n        best_scores = []\n        for estimator in self.estimators_:\n            best_scores.append(\n                estimator.best_score_['validation']['macroF1']\n            )\n        return np.array(best_scores)\n    \n    @property\n    def cv_scores_(self):\n        return self.best_scores_ \n    \n    @property\n    def cv_score_(self):\n        return np.mean(self.best_scores_)\n    \n    @property\n    def best_iterations_(self):\n        best_iterations = []\n        for estimator in self.estimators_:\n            best_iterations.append(\n                estimator.best_iteration_\n            )\n        return np.array(best_iterations)\n    \n    @property\n    def best_iteration_(self):\n        return np.round(np.mean(self.best_iterations_))\n\n    def find_best_params_(self, X, y):\n        \n        if self.perform_random_search:\n            # Define a search space for the parameters\n            lgb_search_params = {\n                      'num_leaves': sp_randint(20, 100), \n                      'min_child_samples': sp_randint(40, 100), \n                      'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n                      'subsample': sp_uniform(loc = 0.75, scale = 0.25), \n                      'colsample_bytree': sp_uniform(loc = 0.8, scale = 0.15),\n                      'reg_alpha': [0, 1e-3, 1e-1, 1, 10, 50, 100],\n                      'reg_lambda': [0, 1e-3, 1e-1, 1, 10, 50, 100]\n                }\n\n            x_train, x_val, y_train, y_val = train_test_split(X, y, test_size = 0.10, random_state = 42, stratify = y)\n            F1_scorer = make_scorer(f1_score, greater_is_better = True, average = 'macro')\n\n            lgb_model = lgb.LGBMClassifier(**self.lgb_params)\n            self.fit_params[\"eval_set\"] = [(x_train, y_train), (x_val, y_val)]\n            self.fit_params[\"verbose\"] = 200\n\n            rs = RandomizedSearchCV(estimator = lgb_model, \n                                    param_distributions = lgb_search_params, \n                                    n_iter = 100,\n                                    scoring = F1_scorer,\n                                    cv = 5,\n                                    refit = True,\n                                    random_state = 314,\n                                    verbose = False,\n                                    fit_params = self.fit_params)\n\n            # Fit the random search\n            _ = rs.fit(x_train, y_train)\n            optimal_params = rs.best_params_\n        \n        if self.perform_bayes_search:\n            \n            init_round = 10 \n            opt_roun = 10\n            n_folds = 6\n            random_seed = 42\n            n_estimators = 500\n            learning_rate = 0.02\n            colsample_bytree = 0.93\n\n            # prepare data\n            train_data = lgb.Dataset(data = X, label = y)\n\n            # parameters\n            def lgb_eval(num_leaves, feature_fraction, bagging_fraction, max_depth, lambda_l1, lambda_l2, min_split_gain, min_child_weight, colsample_bytree, min_child_samples, subsample):\n                params = {'application': 'multiclass',\n                          'num_iterations': n_estimators, \n                          'learning_rate': learning_rate, \n                          'early_stopping_round': 300, \n                          'metric': 'macroF1'}\n                \n                params[\"num_leaves\"] = int(round(num_leaves))\n                params[\"num_class\"] = 5\n                params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n                params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n                params['max_depth'] = int(round(max_depth))\n                params['lambda_l1'] = max(lambda_l1, 0)\n                params['lambda_l2'] = max(lambda_l2, 0)\n                params['min_split_gain'] = min_split_gain\n                params['min_child_weight'] = min_child_weight\n                params['colsample_bytree'] = 0.93\n                params['min_child_samples'] = 56,\n                params['subsample'] = 0.84\n                cv_result = lgb.cv(params, train_data, nfold = n_folds, seed = random_seed, stratified = True, verbose_eval = 200, metrics = ['auc'])\n                return max(cv_result['auc-mean'])\n\n            # range \n            lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (19, 45),\n                                                    'feature_fraction': (0.1, 0.9),\n                                                    'bagging_fraction': (0.8, 1),\n                                                    'max_depth': (5, 8.99),\n                                                    'lambda_l1': (0, 5),\n                                                    'lambda_l2': (0, 3),\n                                                    'min_split_gain': (0.001, 0.1),\n                                                    'min_child_weight': (5, 50),\n                                                    'colsample_bytree' : (0.7,1.0),\n                                                    'min_child_samples' : (40,65),\n                                                    'subsample' : (0.7,1.0)\n                                                   }, random_state = 0)\n            # optimize\n            lgbBO.maximize(init_points = init_round, n_iter = opt_roun)\n            optimal_params = lgbBO.res['max']['max_params']\n        \n        print(\"Optimal LGB parameters:\")\n        print(optimal_params)\n        with open(\"lgb_best_params.pickle\", \"wb\") as lgb_best_params:\n            pickle.dump(optimal_params, lgb_best_params)\n            \n        return optimal_params\n    \n    def fit(self, X, y, **fit_params):\n        print(\"LGBClassifierCV\")\n        \n        # Use only heads of households for scoring\n        X.insert(0, 'Target', y)\n        X = X.query('parentesco1 == 1')\n        y = X['Target'] - 1\n        X = X.drop(['Target', 'parentesco1'], 1)\n        print(\"Number of columns in train - \" + str(X.shape[1]))\n        \n        self.estimators_ = []\n        \n         # Find the best params using random search\n        if self.perform_bayes_search or self.perform_random_search:\n#             self.lgb_optimal_params = self.find_best_params_(X, y)\n            self.lgb_optimal_params = {'num_leaves': 19, \n                                         'feature_fraction': 0.10676970062446138, \n                                         'bagging_fraction': 0.8, \n                                         'max_depth': 9, \n                                         'lambda_l1': 5.0, \n                                         'lambda_l2': 2.999999999999948, \n                                         'min_split_gain': 0.1, \n                                         'min_child_weight': 49.999999999999986, \n                                         'colsample_bytree': 0.7, \n                                         'min_child_samples': 65, \n                                         'subsample': 1.0}\n            \n        # Use a simple train-test split. I have found that this gives a better local CV score than\n        # K folds.\n        if self.use_train_test_split:\n            x_train, x_val, y_train, y_val = train_test_split(X, y, test_size = 0.1, random_state = 0)\n            \n            lgb_model = lgb.LGBMClassifier(**self.lgb_params)\n            if self.perform_random_search or self.perform_bayes_search:\n                lgb_model.set_params(**self.lgb_optimal_params)\n            \n            lgb_model.fit(\n                    x_train, y_train,\n                    eval_set = [(x_train, y_train), (x_val, y_val)],\n                    **self.fit_params\n            )\n            print(\"Train F1 - \" + str(lgb_model.best_score_['train']['macroF1']) + \"   \" + \"Validation F1 - \" + str(lgb_model.best_score_['validation']['macroF1']))\n            self.estimators_.append(lgb_model)\n            \n        # When not using random search to tune parameters, proceed with a simple Stratified Kfold CV\n        if self.use_kfold_split:\n            kf = StratifiedKFold(n_splits = self.cv, shuffle = True)\n            for fold_index, (train, valid) in enumerate(kf.split(X, y)):\n                print(\"Train Fold Index - \" + str(fold_index))\n\n                lgb_model = lgb.LGBMClassifier(**self.lgb_params)\n                if self.perform_random_search:\n                    lgb_model.set_params(**self.lgb_optimal_params)\n\n                lgb_model.fit(\n                        X.iloc[train], y.iloc[train],\n                        eval_set = [(X.iloc[train], y.iloc[train]), (X.iloc[valid], y.iloc[valid])],\n                        **self.fit_params\n                )\n                print(\"Train F1 - \" + str(lgb_model.best_score_['train']['macroF1']) + \"   \" + \"Validation F1 - \" + str(lgb_model.best_score_['validation']['macroF1']))\n\n                self.estimators_.append(lgb_model)\n        return self\n    \n    def predict(self, X):\n        # Remove this column since we are using only heads of households for scoring\n        X = X.drop('parentesco1', 1)\n        \n        # When not using random search, use voting to get predictions from all CV estimators.\n        y_pred = []\n        for estimator_index, estimator in enumerate(self.estimators_):\n            print(\"Estimator Index - \" + str(estimator_index))\n            y_pred.append(estimator.predict(X))\n        return np.mean(y_pred, axis = self.axis).astype(int)","b6726b67":"def get_lgb_params():\n    \n    def evaluate_macroF1_lgb(truth, predictions):  \n        pred_labels = predictions.reshape(len(np.unique(truth)), -1).argmax(axis = 0)\n        f1 = f1_score(truth, pred_labels, average = 'macro')\n        return ('macroF1', f1, True)\n\n    def learning_rate_power_0997(current_iter):\n            base_learning_rate = 0.1\n            min_learning_rate = 0.02\n            lr = base_learning_rate  * np.power(.995, current_iter)\n            return max(lr, min_learning_rate)\n\n    lgb_params = {'boosting_type': 'dart',\n                  'class_weight': 'balanced',\n                  \"objective\": 'multiclassova',\n                  'metric': None,\n                  'silent': True,\n                  'random_state': 0,\n                  'n_jobs': -1}\n    \n    fit_params={\"early_stopping_rounds\": 400, \n                \"eval_metric\" : evaluate_macroF1_lgb, \n                'eval_names': ['train', 'validation'],\n                'verbose': False,\n                'categorical_feature': 'auto'}\n    \n    return lgb_params, fit_params","2f1011f1":"lgb_params, lgb_fit_params = get_lgb_params()\n\npipeline = Pipeline([\n    ('na_imputer', MissingValuesImputer(impute_zero_columns = ['v18q1', 'v2a1', 'rez_esc'])),\n    ('cat_transformer', CategoricalVariableTransformer()),\n    ('unnecessary_columns_remover_transformer', UnnecessaryColumnsRemoverTransformer()),\n    ('feature_engineering_transformer', FeatureEngineeringTransformer()),\n    ('lgb', LGBClassifierCV(lgb_params = lgb_params,\n                            fit_params = lgb_fit_params,\n                            cv = 5,\n                            perform_random_search = False,\n                            perform_bayes_search = True,\n                            use_train_test_split = True,\n                            use_kfold_split = False)\n    )\n])\n\n\npipeline.fit(x_train.copy(), y_train.copy())\npred = pipeline.predict(x_test.copy())\nprint(\"Local CV Score - \" + str(pipeline.named_steps['lgb'].cv_score_))\nsub_orig['Target'] = pred + 1\nsub_orig.to_csv('Pipeline_Base_LGB_'+ str(pipeline.named_steps['lgb'].cv_score_) + '.csv')\nprint(sub_orig.head())","658714f8":"\n\n## **Build a Pipeline**","11b5780d":"\n## **Stay tuned for Bayes Optimized LGB!! And do upvote if you like this kernel :)**","cc7fd89c":"## **Read the data**"}}