{"cell_type":{"a6bb7125":"code","d61da424":"code","6c34065f":"code","f0147546":"code","595c3616":"code","1ed18613":"code","6b872c5a":"code","9646e753":"code","80619132":"code","72b47018":"code","eb38ce37":"code","8cbb9de5":"code","571013f3":"code","87f0c8d0":"code","36b452ee":"code","e2c42f7d":"code","fa0b941e":"code","84d4a60c":"code","25bfedf1":"code","cf3eacf3":"code","5e05c8d8":"code","8ee21141":"code","d4024889":"code","3f09a36b":"code","faa09239":"code","c4944f95":"code","1122cb30":"code","7390aae9":"code","06f89fa6":"code","5815f602":"code","30d409a8":"code","e0c09b31":"code","3a63a340":"code","e7ec9812":"markdown","71e490e8":"markdown","1892b8db":"markdown","c1fe8d21":"markdown","a1e95f7a":"markdown","eb013f9f":"markdown","d346a2a3":"markdown","9ad4d470":"markdown","435655de":"markdown","7fee2f6a":"markdown","fe2a1a66":"markdown","8b7c7105":"markdown","f6fdd5a8":"markdown","310f6ed8":"markdown","739af7e8":"markdown","1d2c5113":"markdown","40daf3db":"markdown","b47e9319":"markdown","2e2d1ed5":"markdown","a213bf2e":"markdown","f707be41":"markdown","f43c3a15":"markdown","efb3e77c":"markdown","3175bc83":"markdown","006fa8e6":"markdown","d60fdbad":"markdown","c4a209d4":"markdown","10101982":"markdown","648b4fa5":"markdown","adea38ee":"markdown","388b5a2d":"markdown","3da42dd8":"markdown","8e7c573a":"markdown","5bb96d89":"markdown"},"source":{"a6bb7125":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d61da424":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom fbprophet import Prophet\nfrom fbprophet.plot import plot_plotly\nfrom fbprophet.diagnostics import cross_validation\nfrom fbprophet.diagnostics import performance_metrics\nfrom fbprophet.plot import plot_cross_validation_metric\nimport plotly.offline as py\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import \\\nmean_absolute_error,mean_squared_error,r2_score\nfrom math import exp,log\nfrom statsmodels.graphics.gofplots import qqplot\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.tsa.stattools import acf, pacf","6c34065f":"py.init_notebook_mode()\npd.plotting.register_matplotlib_converters\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#delta_positive forces values to be positive in case of \n#log transformations \ndelta_positive=2","f0147546":"data_file = \"\/kaggle\/input\/corona-virus-report\/covid_19_clean_complete.csv\"\ndf_data = pd.read_csv(data_file,na_filter=False)","595c3616":"# Calculate features\n# New_Confirmed,New_Deaths, and New_Recovered are calculated \n# by substracting the values from the previous day per country\n\ndf_data['Active'] = df_data['Confirmed'] - df_data['Deaths'] - df_data['Recovered']\ndf_data['New_Confirmed'] = 0\ndf_data['New_Deaths'] = 0\ndf_data['New_Recovered'] = 0\n\narr_data = df_data[[\"Confirmed\",\"Deaths\",\"Recovered\"]].values\narr_data =  np.concatenate(\\\n                           (df_data.index.values.reshape(-1,1), \\\n                            arr_data), axis=1)\n\narr_new_data = df_data[[\"New_Confirmed\",\"New_Deaths\",\"New_Recovered\"]].values\narr_new_data =  np.concatenate(\\\n                           (df_data.index.values.reshape(-1,1), \\\n                            arr_new_data), axis=1)\n\nnum_days = len(np.unique(df_data[\"Date\"]))\nnum_countries_provinces = int(arr_data.shape[0]\/num_days)\n\nfor ix in range(len(arr_new_data)):\n    if ix < num_countries_provinces:\n        for col in range(1,4):\n            arr_new_data[ix,col]=arr_data[ix,col]\n    else:\n        for col in range(1,4):\n            arr_new_data[ix,col]=\\\n            arr_data[ix,col]-arr_data[ix-num_countries_provinces,col]\n            \ndf_data[\"New_Confirmed\"] = arr_new_data[:,1]\ndf_data[\"New_Deaths\"] = arr_new_data[:,2]\ndf_data[\"New_Recovered\"] = arr_new_data[:,3]","1ed18613":"# case_fatality_rate = deaths\/confirmed\narr_data_Confirmed = df_data[[\"Confirmed\"]].values\narr_data_Deaths = df_data[[\"Deaths\"]].values\ncase_fatality_rate = arr_data_Deaths\/arr_data_Confirmed*100\nwhere_are_NaNs = np.isnan(case_fatality_rate)\ncase_fatality_rate[where_are_NaNs] = 0.\ndf_data[\"CFR\"] = case_fatality_rate","6b872c5a":"df_data['Date'] = pd.to_datetime(df_data['Date'])\ndf_data = df_data.set_index('Date')","9646e753":"df_data","80619132":"country = \"Mexico\"\ndf_country = df_data[df_data[\"Country\/Region\"]==country]\ndf_country","72b47018":"def plot_data(title, data, labels):\n    fig = go.Figure(layout_title_text=title)\n    for (datum, label) in zip(data,labels):\n        fig.add_trace(go.Scatter(x=datum.index,\n                                 y=datum.values,\n                      mode='lines+markers',\n                      name=label))\n    fig.show()\n    return(fig)\n\ndef plot_data_list(title, data, labels):\n    fig = go.Figure(layout_title_text=title)\n    for (datum, label) in zip(data,labels):\n        fig.add_trace(go.Scatter(x=np.array(range(len(datum))),\n                                 y=datum,\n                      mode='lines+markers',\n                      name=label))\n    fig.show()\n\nfig = plot_data(country,\n          [df_country[\"Confirmed\"],df_country[\"Deaths\"],\\\n           df_country[\"Recovered\"],df_country[\"Active\"]], \n          [\"Confirmed\", \"Deaths\", \"Recovered\", \"Active\"])","eb38ce37":"fig = plot_data(country,\n          [df_country[\"Deaths\"],df_country[\"New_Deaths\"]], \n          [\"Deaths\",\"New_Deaths\"])\nfig = plot_data(country,\n          [df_country[\"Confirmed\"],df_country[\"New_Confirmed\"]], \n          [\"Confirmed\",\"New_Confirmed\"])\nfig = plot_data(country + \" Active\",\n          [df_country[\"Active\"]], \n          [\"Active\"])\nfig = plot_data(country + \" Case Fatality Rate\",\n          [df_country[\"CFR\"]], \n          [\"CFR\"])","8cbb9de5":"def get_initial_date(country, type_case):\n    try:\n        initial_date = df_data[(df_data[type_case]>0) & \\\n                               (df_data[\"Country\/Region\"]==country)].index[0]\n    except:\n        initial_date = None\n    return(initial_date)\n\ndef list_of_positive_cases (country, type_case):\n    initial_date = get_initial_date(country, type_case)\n    if initial_date is not None:\n        list_positive_cases = df_data[(df_data.index>=initial_date) & \\\n                (df_data[\"Country\/Region\"]==country)].\\\n        groupby([\"Date\"])[type_case].\\\n        agg('sum').values\n    else:\n        list_positive_cases = []\n    return(list_positive_cases)\n\ndef get_rmse(var1, var2):\n    return np.sqrt(((var1-var2) ** 2).mean())\n\ndef plot_countries_comparision(countries_to_compare,type_case):\n    positive_cases_daily = [list_of_positive_cases(x,type_case) \\\n                             for x in countries_to_compare]\n    plot_data_list(\"Number of \" + type_case + \" since first case in each country\",\n                   positive_cases_daily,\n                   countries_to_compare)\n\n    for i in range(len(countries_to_compare)-1):\n        for j in range(i+1,len(countries_to_compare)):\n            min_days = np.min([len(positive_cases_daily[i]),\\\n                              len(positive_cases_daily[j])])\n            rmse_2 = get_rmse(positive_cases_daily[i][:min_days],\n                positive_cases_daily[j][:min_days])\n            print(\"Mean daily difference (RMSE)\" + \\\n                  countries_to_compare[i] + \"-\" + countries_to_compare[j],\\\n          '{:2.2f}'.format(rmse_2))\n            \ndef get_similar_countries(base_country,type_case, n_countries=3):\n    base_list_of_positive_cases = list_of_positive_cases(base_country,type_case)\n    num_days_base_country = len(base_list_of_positive_cases)\n\n    countries_to_compare = list(np.unique(df_data[\"Country\/Region\"].values))\n    countries_to_compare.remove(base_country)\n\n    positive_cases_daily = [list_of_positive_cases(x,type_case) \\\n                            for x in countries_to_compare]\n\n    list_rmse = []\n    for ix,country in enumerate(countries_to_compare):\n        num_days_second_country = len(positive_cases_daily[ix])\n        if num_days_base_country <= num_days_second_country:\n            min_days = np.min([num_days_base_country,num_days_second_country])\n            rmse_2 = get_rmse(base_list_of_positive_cases[:min_days],\n                    positive_cases_daily[ix][:min_days])\n        else:\n            rmse_2 = np.inf\n        list_rmse.append(rmse_2)\n\n    top_n = [countries_to_compare[x] for x in np.argsort(list_rmse)[:n_countries]]\n    return(top_n)","571013f3":"base_country=\"Mexico\"\ntype_case=\"New_Confirmed\"\nn_countries=3\ncountries_to_compare = get_similar_countries(base_country,type_case, n_countries)\ncountries_to_compare.append(base_country)\nplot_countries_comparision(countries_to_compare,type_case)","87f0c8d0":"countries_to_compare = [\"Belgium\",\"Spain\",\"Italy\",\"Mexico\", \"New Zealand\"]\ntype_case=\"New_Deaths\"\nplot_countries_comparision(countries_to_compare,type_case)","36b452ee":"def create_forecast(country,type_case,periods):\n    ds = df_data[df_data[\"Country\/Region\"]==country].index.date\n    y = df_data[df_data[\"Country\/Region\"]==country][type_case].values\n    df_forecast = pd.DataFrame()\n    df_forecast[\"ds\"] = ds\n    df_forecast[\"y\"] = y\n    \n    model = Prophet(changepoint_prior_scale=.05,\\\n                    interval_width=0.95)\n    \n    model.fit(df_forecast)\n    future = model.make_future_dataframe(periods=periods)\n    forecast = model.predict(future)\n    return(model,df_forecast,future,forecast)\n\ndef plot_forecast(model,forecast):\n    fig = plot_plotly(model,forecast)\n    py.iplot(fig)\n    \ndef plot_forecast_components(model,forecast):\n    fig = model.plot_components(forecast)\n    \ndef cross_val_forecast(model,horizon,period,initial_training_period):\n        df_cv = cross_validation(model=model,\\\n                                 horizon=horizon,\\\n                                period=period,\\\n                                initial=initial_training_period)\n        print('Forecast cross validation ')\n        print(df_cv.tail(5))\n\n        df_p = performance_metrics(df_cv)\n        print('Performance metrics')\n        print(df_p.head(5))\n\n        ufig = plot_cross_validation_metric(df_cv, metric='rmse')\n        return(df_cv)","e2c42f7d":"### Define a country and a metric to forecast\ncountry,type_case,periods = \"Mexico\",\"New_Confirmed\",10\n\nmodel,df_forecast, future, forecast = create_forecast\\\n(country,type_case,periods)\n\nplot_forecast(model,forecast)\nplot_forecast_components(model,forecast)","fa0b941e":"### Show some statistics about the forecast evaluation\nhorizon = \"10 days\"\nperiod = \"1 days\"\ninitial_training_period = \"30 days\"\n\ndf_cv = cross_val_forecast(model,horizon,period,initial_training_period)","84d4a60c":"from statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import kpss\nfrom scipy.stats import boxcox\n\n### Define some helper functions for stationary tests \n### and data transformation\n\n# stationary test\ndef adfuller_test(dataset):  \n    result = adfuller(dataset)\n    print(\"adFuller Test\")\n    print('ADF Statistic: %f' % result[0])\n    print('p-value: %f' % result[1])\n    print('Critical Values:')\n    for key, value in result[4].items():\n        print('\\t%s: %.3f' % (key, value))\n\n# stationary test\ndef kpss_test(dataset):\n    print(\"KPSS test\")\n    kpsstest = kpss(dataset, regression='c')\n    kpss_output = pd.Series(kpsstest[0:3],\\\n                            index=['Test Statistic','p-value',\\\n                                   'Lags Used'])\n    for key,value in kpsstest[3].items():\n        kpss_output['Critical Value (%s)'%key] = value\n    print(kpss_output)\n    \n# differencing transformation\ndef difference_transf(dataset):\n    diff = list()\n    for i in range(1, len(dataset)):\n        value = dataset[i] - dataset[i - 1]\n        diff.append(value)\n    return (np.asarray(diff))\n\n# boxcox transformation\ndef boxcox_transf(dataset):\n    dataset = dataset+delta_positive\n    transformed, lam = boxcox(dataset)\n    if lam < -5:\n        transformed, lam = dataset, 1\n    return(transformed, lam)\n\n# sqrt transformation\ndef sqrt_transf(dataset):\n    return(np.sqrt(dataset))\n\n# two types of moving average transformation\ndef diff_moving_avg(dataset, moving_avg):\n    diff = dataset - moving_avg\n    diff.dropna(inplace=True)\n    return(diff)\ndef moving_average(dataset, n=3) :\n    ret = np.cumsum(dataset, dtype=float)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] \/ n\n\n# ewma transformation\ndef diff_ewma(dataset):\n    expwighted_avg = dataset.ewm(halflife=7).mean()\n    diff = dataset - expwighted_avg\n    diff.dropna(inplace=True)\n    return(diff)","25bfedf1":"### Define helper functions to show time series\n\ndef plot_series(data,initial_date,rolling_days=7):\n    ts = pd.Series(data, index=pd.date_range(initial_date, \n                                         periods=len(data)))\n    rolmean = ts.rolling(window=rolling_days).mean()\n    rolstd = ts.rolling(window=rolling_days).std()\n\n    fig = plt.figure(figsize=(12,6))\n    orig = plt.plot(ts, color='blue',label=type_case)\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation '+country)\n    plt.show(block=False)\n    return(ts,rolmean)\n\n#Source:\n#https:\/\/machinelearningmastery.com\/time-series-forecast-case-study-python-monthly-armed-robberies-boston\/\ndef show_qqplot(dataset):\n    plt.figure(1, figsize=(12,12))\n    # line plot\n    plt.subplot(311)\n    plt.plot(dataset)\n    # histogram\n    plt.subplot(312)\n    plt.hist(dataset)\n    # q-q plot\n    plt.subplot(313)\n    qqplot(dataset, line='r', ax=plt.gca())\n    plt.show()\n    \ndef show_acf_pacf(dataset):\n    max_lags=7\n    plt.figure(figsize=(12,10))\n    plt.subplot(211)\n    plot_acf(dataset, ax=plt.gca(),lags=max_lags)\n    plt.subplot(212)\n    plot_pacf(dataset, ax=plt.gca(),lags=max_lags)\n    plt.show()\n    acf_res = acf(dataset,nlags=max_lags)\n    pacf_res = pacf(dataset,nlags=max_lags)\n    return(acf_res,pacf_res)","cf3eacf3":"country, type_case = \"Mexico\", \"New_Confirmed\"\ndata = (list_of_positive_cases (country, type_case)).astype(\"float\")\n# initial_date that feature type_case started being non-zero\ninitial_date = get_initial_date(country, type_case)\n\nts,rolmean = plot_series(data,initial_date)\nadfuller_test(data)\nkpss_test(data)\nshow_qqplot(data)","5e05c8d8":"dataset = data\nboxcox_t,lam = boxcox_transf(dataset)\nadfuller_test(boxcox_t)\nkpss_test(boxcox_t)\nshow_qqplot(boxcox_t)","8ee21141":"dataset = boxcox_t\nacf_res, pacf_res = show_acf_pacf(dataset)\n# arbitrary set of thresholds for acf in 0.7, and pacf in 0.2\nq_values = [ix for ix,x in enumerate(np.abs(acf_res) > 0.7) if x]\np_values = [ix for ix,x in enumerate(np.abs(pacf_res) > 0.2) if x]","d4024889":"import warnings\n\n# evaluate an ARIMA model for a given order (p,d,q) and return RMSE\ndef evaluate_arima_model(dataset, arima_order):\n    train_size = int(len(dataset) * 0.50)\n    train, test = dataset[0:train_size], dataset[train_size:]\n    history = [x for x in train]\n    # make predictions\n    predictions = list()\n    for t in range(len(test)):\n        model = ARIMA(history, order=arima_order)\n        model_fit = model.fit(disp=False)\n        #yhat = model_fit.forecast()[0]\n        yhat = model_fit.predict(start=0,end=0,typ='levels')\n        predictions.append(yhat)\n        history.append(test[t])\n    # calculate out of sample error\n    rmse = get_rmse(test, predictions)\n    return rmse\n\n# evaluate combinations of p, d and q values for an ARIMA model\ndef evaluate_models(dataset, p_values, d_values, q_values):\n    best_score, best_cfg = float(\"inf\"), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                print(p,d,q)\n                try:\n                    mse = evaluate_arima_model(dataset, order)\n                    if mse < best_score:\n                        best_score, best_cfg = mse, order\n                    print('ARIMA%s RMSE=%.3f' % (order,mse))\n                except:\n                    print('Error', order)\n                    #continue\n    print('Best ARIMA%s RMSE=%.3f' % (best_cfg, best_score))\n    return(best_cfg)","3f09a36b":"%%time\ndataset = boxcox_t\n# evaluate parameters\nd_values = [0,1]\nwarnings.filterwarnings(\"ignore\")\nprint(p_values,d_values,q_values)\nbest_cfg = evaluate_models(dataset, p_values, d_values, q_values)","faa09239":"def profit_plot(model_fit,data,horizon=10):\n    fig, ax = plt.subplots(figsize=(16,8))\n    fig = model_fit.plot_predict(1, len(data)+horizon, ax=ax)\n    plt.show()\n    \ndef mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\ndef show_results(y_true,yhat):\n    mae = mean_absolute_error(y_true, y_pred)    \n    r2 = r2_score(y_true, y_pred)\n    mape = mean_absolute_percentage_error(y_true, y_pred)\n    rmse = get_rmse(y_true, y_pred)\n\n    print(\"Walk-forward validation, Tests = \" + \"{:d}\".format(len(y_true)))\n    print(\"mae = {:4.4f}\".format(mae))\n    print(\"rmse = {:4.4f}\".format(rmse))\n    print(\"r2 = {:4.4f}\".format(r2))\n    print(\"mape = {:4.4f}\".format(mape))\n    return(rmse)\n    \ndef forecast_plot(forecast,actual,title,conf_int=None):\n    fig, ax = plt.subplots(figsize=(16,8))\n    ax.plot(forecast.index, forecast.values, label=\"Forecast\", \\\n            color=\"red\",marker=\".\",linestyle='dashed',linewidth=1,\\\n            alpha=.5)\n    ax.plot(actual.index, actual.values, label=\"Actual\", \\\n            color=\"blue\",marker=\"o\",linestyle=\"\", alpha=.5)\n    if conf_int is not None:\n        fill_label = \"{0:.0%} confidence interval\".format(.95)\n        ax.fill_between(forecast.index[-conf_int.shape[0]:], conf_int[:, 0], conf_int[:, 1],\n                        color='gray', alpha=.5, label=fill_label)\n        \n    ax.legend(loc=2)\n    ax.set_title(title)\n    plt.show()\n    \n# invert box-cox transform\ndef boxcox_inverse(value, lam):\n    if lam == 0:\n        inv = exp(value)\n    else:\n        inv = exp(log(lam * value + 1) \/ lam)\n    return(inv-delta_positive)\n\ndef predict_inverse(data_size,model_fit,lam,horizon,typ=None):\n    if typ is not None:\n        yhat = model_fit.predict(start=data_size,\\\n                                 end=data_size+horizon-1,\n                                 typ='levels')\n    else:\n        yhat = model_fit.predict(start=data_size,\\\n                                end=data_size+horizon-1)\n    return([boxcox_inverse(x,lam) for x in yhat])","c4944f95":"# Arima\ndataset = boxcox_t\nout = 'AIC: {0:0.3f}, BIC: {1:0.3f}'\n# Walk-forward validation after training at least for 30 days\nn_train = 30\nn_records = len(dataset)\ny_pred = []\n\nfor i in range(n_train, n_records):\n    train = dataset[0:i]\n    model = ARIMA(train, order=best_cfg)\n    model_fit = model.fit(disp=False)\n    #print(out.format(model_fit.aic,model_fit.bic))\n    yhat = model_fit.predict(start=i,end=i,typ='levels')\n    y_pred.extend(yhat)\n    #print(i,dataset[i],yhat[0])\ny_true = dataset[n_train:n_records]\nmetric = show_results(y_true,y_pred)","1122cb30":"horizon=10\n# Run the model in the entire transformed dataset using the fine-tuned parameters\ndataset = boxcox_t\nmodel = ARIMA(dataset, order=best_cfg)\nmodel_fit = model.fit(disp=False)\ny_pred = model_fit.predict(start=1,end=len(dataset)+horizon,typ='levels')\n\nforecast, stderr, conf_int = model_fit.forecast(steps=horizon+1)\nforecast=forecast[1:]\nstderr=stderr[1:]\nconf_int=conf_int[1:]\n\ny_pred_inv = [boxcox_inverse(x,lam) for x in y_pred]\nrmse = get_rmse(data, y_pred_inv[:len(data)])\ntitle = type_case + \" \" + country + \" RMSE = {:4.4f}\".format(rmse)\n\nconf_int_inv = np.array([[boxcox_inverse(x,lam),boxcox_inverse(x2,lam)] \\\n                for (x,x2) in conf_int])\n\nts_forecast = pd.Series(y_pred_inv,\\\n                        index=pd.date_range(initial_date,\\\n                                                 periods=len(y_pred_inv)))\n\nforecast_plot(ts_forecast,ts,title,conf_int_inv)\nfig = plot_data(title,\n          [ts,ts_forecast], \n          [\"Actual\", \"Forecast\"])","7390aae9":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\n\ndataset = boxcox_t\nout = 'AIC: {0:0.3f}, BIC: {1:0.3f}'\nn_train = 30\nn_records = len(dataset)\n\n# Define parameters to grid search\nl_trend=[\"add\",\"mul\",None]\nl_damped=[True,False]\nl_seasonal=[\"add\",\"mul\",None]\nseasonal_periods=7\noptimized=True\nuse_boxcox=False\nl_remove_bias=[True,False]\n\nbest_metric = np.inf\nbest_trend, best_damped, best_remove_bias, best_seasonal = \\\nNone,None,None,None\n\nfor trend in l_trend:\n    for damped in l_damped:\n        for remove_bias in l_remove_bias:\n            for seasonal in l_seasonal:\n                print(\"trend:\",trend,\", damped:\",damped,\\\n                \", remove_bias:\",remove_bias, \", seasonal:\",seasonal)\n                y_pred = []\n                try:\n                    for i in range(n_train, n_records):\n                        train = dataset[0:i]\n                        model = ExponentialSmoothing(train,trend=trend,damped=damped,\\\n                                                     seasonal=seasonal,\\\n                                                     seasonal_periods=seasonal_periods)\n                        model_fit = model.fit(optimized=optimized,use_boxcox=use_boxcox,\\\n                                              remove_bias=remove_bias)\n                        #print(out.format(model_fit.aic, model_fit.bic))\n                        yhat = model_fit.predict(start=i,end=i)\n                        y_pred.extend(yhat)\n                    #print(i,dataset[i],yhat[0])\n                    y_true = dataset[n_train:n_records]\n                    metric = show_results(y_true,y_pred)\n                    if metric < best_metric:\n                        best_metric = metric\n                        best_trend = trend\n                        best_damped = damped\n                        best_seasonal = seasonal\n                        best_remove_bias = remove_bias\n                except:\n                    break\nprint(\"\\nBest metric (rmse): \" + \"{0:4f}\".format(best_metric))\nprint(\"trend:\",best_trend,\", damped:\",best_damped,\\\n      \", remove_bias:\",best_remove_bias,\\\n      \", seasonal:\",seasonal)","06f89fa6":"horizon = 10\ndataset = boxcox_t\ntrend=best_trend\ndamped=best_damped\nseasonal=best_seasonal\nremove_bias=best_remove_bias\n\n# Run the model in the entire transformed dataset using the fine-tuned parameters\n\nmodel = ExponentialSmoothing(dataset,trend=trend,damped=damped,\\\n                             seasonal=seasonal,\\\n                             seasonal_periods=seasonal_periods)\nmodel_fit = model.fit(optimized=optimized,use_boxcox=use_boxcox,\\\n                         remove_bias=remove_bias)\ny_pred = model_fit.predict(start=1,end=len(dataset)+horizon)\n\ny_pred_inv = [boxcox_inverse(x,lam) for x in y_pred]\nrmse = get_rmse(data, y_pred_inv[:len(data)])\ntitle = type_case + \" \" + country + \" RMSE = {:4.4f}\".format(rmse)\n\nts_forecast = pd.Series(y_pred_inv,\\\n                        index=pd.date_range(initial_date,\\\n                                                 periods=len(y_pred_inv)))\n\nforecast_plot(ts_forecast,ts,title)\nfig = plot_data(title,\n          [ts,ts_forecast], \n          [\"Actual\", \"Forecast\"])","5815f602":"from keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Bidirectional\nfrom keras.layers import TimeDistributed\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers import ConvLSTM2D","30d409a8":"#https:\/\/machinelearningmastery.com\/how-to-develop-lstm-models-for-time-series-forecasting\/\n    \n# split a univariate sequence into samples\ndef split_sequence(sequence, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the sequence\n        if end_ix > len(sequence)-1:\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return (np.array(X), np.array(y))\n\n# Vainilla LSTM\ndef simple_lstm(num_nodes,n_steps,n_features):\n    model = Sequential()\n    model.add(LSTM(num_nodes, activation='relu', \\\n                   input_shape=(n_steps, n_features)))\n    model.add(Dense(num_nodes))\n    model.add(Dense(num_nodes*2))\n    model.add(Dense(num_nodes))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mse')\n    return(model)\n\n# Stacked LSTM\ndef stacked_lstm(num_nodes,n_steps,n_features):\n    model = Sequential()\n    model.add(LSTM(num_nodes, activation='relu', \\\n                   return_sequences=True, \\\n                   input_shape=(n_steps, n_features)))\n    model.add(LSTM(num_nodes, activation='relu'))\n    model.add(Dense(num_nodes))\n    model.add(Dense(num_nodes*2))\n    model.add(Dense(num_nodes))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mse')\n    return(model)\n\n# Bidirectional LSTM\ndef biderectional_lstm(num_nodes,n_steps,n_features):\n    model = Sequential()\n    model.add(Bidirectional(LSTM(num_nodes, activation='relu'), \\\n                            input_shape=(n_steps, n_features)))\n    model.add(Dense(num_nodes))\n    model.add(Dense(num_nodes*2))\n    model.add(Dense(num_nodes))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mse')\n    return(model)\n\ndef cnn_lstm(num_nodes,n_steps,n_features):\n    model = Sequential()\n    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1,\\\n                                     activation='relu'),\\\n                              input_shape=(None, n_steps, n_features)))\n    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n    model.add(TimeDistributed(Flatten()))\n    model.add(LSTM(num_nodes, activation='relu'))\n    model.add(Dense(num_nodes))\n    model.add(Dense(num_nodes*2))\n    model.add(Dense(num_nodes))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mse')\n    return(model)\n\ndef conv2d_lstm(num_nodes,n_steps,n_features,n_seq):\n    model = Sequential()\n    model.add(ConvLSTM2D(filters=64, \\\n                         kernel_size=(1,2), activation='relu', \\\n                         input_shape=(n_seq, 1, n_steps, n_features)))\n    model.add(Flatten())\n    model.add(Dense(num_nodes))\n    model.add(Dense(num_nodes*2))\n    model.add(Dense(num_nodes))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mse')\n    return(model)","e0c09b31":"# define input layer sequence\nraw_seq = data\nn_features = 1\n\n\nnum_nodes=64\nn_epochs=100\n\nmodels = {\"Simple LSTM\":simple_lstm,\\\n          \"Stacked LSTM\":stacked_lstm,\\\n         \"Bidirectional LSTM\":biderectional_lstm,\\\n         \"CNN LSTM\":cnn_lstm,\\\n         \"Conv2D LSTM\":conv2d_lstm}\n\nbest_rmse = np.inf\nbest_lstm_model=None\n\nfor model_type in models.keys():\n    if (model_type == \"CNN LSTM\"):\n        n_steps = 4\n        X, y = split_sequence(raw_seq, n_steps)\n        # reshape from [samples, timesteps] into :\n        #[samples, subsequences, timesteps, features]\n        n_seq = 2\n        n_steps = 2\n        X = X.reshape((X.shape[0], n_seq, n_steps, n_features))\n        params = (num_nodes,n_steps,n_features)\n    elif (model_type == \"Conv2D LSTM\"):\n        n_steps = 4\n        X, y = split_sequence(raw_seq, n_steps)\n        # reshape from [samples, timesteps] into :\n        #[samples, subsequences, timesteps, features]\n        n_seq = 2\n        n_steps = 2        \n        X = X.reshape((X.shape[0], n_seq, 1, n_steps, n_features))\n        params = (num_nodes,n_steps,n_features,n_seq)\n    else:\n        n_steps = 7\n        X, y = split_sequence(raw_seq, n_steps)\n        # reshape from [samples, timesteps] into :\n        # [samples, timesteps, features]\n        X = X.reshape((X.shape[0], X.shape[1], n_features))\n        params = (num_nodes,n_steps,n_features)\n\n    model = models[model_type](*params)\n    model.fit(X, y, epochs=n_epochs, verbose=0)\n    \n    y_pred=[]\n    for ix in range(X.shape[0]):\n        x_input = X[ix]\n        if (model_type == \"CNN LSTM\"):            \n            x_input = x_input.reshape((1, n_seq, n_steps, n_features))\n        elif (model_type == \"Conv2D LSTM\"):            \n            x_input = x_input.reshape((1, n_seq, 1, n_steps, n_features))\n        else:\n            x_input = x_input.reshape((1, n_steps, n_features))\n        yhat = model.predict(x_input, verbose=0)\n        y_pred.extend(yhat[0])\n    rmse = get_rmse(y, y_pred)\n    print(model_type + \" -  RMSE = {:4.4f}\".format(rmse))\n    if rmse < best_rmse:\n        best_rmse = rmse\n        best_n_steps = n_steps\n        best_lstm_model_name = model_type\n        best_y_pred = y_pred","3a63a340":"ts_forecast = pd.Series(best_y_pred,\\\n                        index=pd.date_range(ts.index[best_n_steps],\\\n                                                 periods=len(best_y_pred)))\n\ntitle = type_case + \" \" + country + \" RMSE = {:4.4f}\".format(best_rmse)\n# Run the model in the entire dataset using the best LSTM network\nforecast_plot(ts_forecast,ts,title)\nfig = plot_data(title,\n          [ts,ts_forecast], \n          [\"Actual\", \"Forecast\"])","e7ec9812":"- After several trials using different types of transformations, box_cox was the only one that provided consistent positive results","71e490e8":"#### Define a range to grid search p and q values in the ARIMA model","1892b8db":"#### Define helper functions to prepare data and different LSTM models","c1fe8d21":"#### Define helper functions","a1e95f7a":"#### Plot data distribution and stationarity tests","eb013f9f":"#### Import libraries","d346a2a3":"#### Load and prepare data","9ad4d470":"#### Perform grid search for p,d,q values","435655de":"#### Identify the best type of LSTM network based on rmse","7fee2f6a":"I would suggest to use Prophet for quick results, then Exponential Smoothing for better accuracy, and finally, ARIMA to maximize the accuracy at the cost of possible transformations and parameters fine-tuning.","fe2a1a66":"#### Using Prophet to forecast a specific metric in a country","8b7c7105":"### FB Prophet","f6fdd5a8":"# Brief comparison between Time Series methods to predict daily confirmed cases of Coronavirus","310f6ed8":"## Forecast Analysis","739af7e8":"#### Compare a country's metric to others with similar behavior ","1d2c5113":"#### Visualization","40daf3db":"#### Create helper functions","b47e9319":"#### Run the winning LSTM  model on regular data and plot it","2e2d1ed5":"#### The rmse is in the 90's for one-day forecast using the best LSTM model  with the regular un-transformed dataset. Most of the runs I have tried rmse in above 130's","a213bf2e":"## Conclusions","f707be41":"#### Run the ARIMA model on transformed data and plot on regular data","f43c3a15":"#### rmse is in the 70's for one-day forecast using a fine-tuned ARIMA model with a box-cox transformed dataset. In previous exercises the mean rmse has been around 30's","efb3e77c":"#### The rmse is in the 80's for one-day forecast using a fine-tuned Holt-Winters Exponential Smoothing model with a box-cox transformed dataset","3175bc83":"#### Open comparision between countries according to one feature","006fa8e6":"### Holt-Winters Exponential Smoothing","d60fdbad":"#### The root mean squared error (rmse) is in the 130's for one-day forecast using Prophet. This means that there is a mean difference of around 130 cases between the one-day forecast and the actual values","c4a209d4":"My personal goal in this notebook was to learn about Time Series and use it to help forecasting the possible evolution of the Coronavirus outbreak.\nThis is not intended to be a comprehensive study, but more a beginner's introduction to apply time series.\n\nThe methods compared are classical time series like Autoregressive Integrated Moving Average (ARIMA)  and Holt-Winters Exponential Smoothing, machine learning  methods like several types of Long Short Term Memory (LSTM) networks, and Facebook Prophet as well.\n\nARIMA requires data sets to be stationary, and most data related to the Coronavirus outbreak has shown an exponential growth, though it has started to reduce the slope in the trend in some places.\nThis exercise demonstrates a comparison between different models using root mean squared error to measure the model performance after using Walk-forward validation testing.\n\nThe dataset used is corona-virus-report\\covid_19_clean_complete.csv from https:\/\/www.kaggle.com.\n\nBefore getting into the forecast analysis, there are some visual comparisons between countries in features like \"Confirmed\" (accumulated number of confirmed cases), \"New_Confirmed\" (daily number of confirmed cases), \"Deaths\" (accumulated number of deaths), \"New_Deaths\" (daily number of deaths), \"Recovered\" (accumulated number of recovered cases), \"New_ Recovered\" (daily number of recovered cases), \"Active\" (difference between the number of confirmed cases and the number of deaths and recovered cases).\nThe last feature is CFR (Case fatality rate) which is the ratio between deaths to confirmed cases. \n\nImportant references:\n\n- Italy space time & spreading of Covid19\nhttps:\/\/www.kaggle.com\/lumierebatalong\/italy-space-time-spreading-of-covid19\n\n- How to Grid Search Triple Exponential Smoothing for Time Series Forecasting in Python\nhttps:\/\/machinelearningmastery.com\/how-to-grid-search-triple-exponential-smoothing-for-time-series-forecasting-in-python\/\n\n- How to Develop LSTM Models for Time Series Forecasting\nhttps:\/\/machinelearningmastery.com\/how-to-develop-lstm-models-for-time-series-forecasting\/\n\n- How To Backtest Machine Learning Models for Time Series Forecasting\nhttps:\/\/machinelearningmastery.com\/backtest-machine-learning-models-time-series-forecasting\/\n\n- Time Series Forecast Case Study with Python: Monthly Armed Robberies in Boston\nhttps:\/\/machinelearningmastery.com\/time-series-forecast-case-study-python-monthly-armed-robberies-boston\/\n","10101982":"#### Create a specific dataframe for a particular country","648b4fa5":"- Data is not stationary as expected. I will use a box_cox transformation to reduce skewness","adea38ee":"- FB Prophet provides a reasonable prediction accuracy without the need to adjust for data stationarity. It also offers some functions to plot and cross-validation testing.\n- Fine-tuned ARIMA model on transformed data provided the best rmse metric on one-day forecasts, but it produces a lot of errors on non-transformed data when it is not stationary.\n- Fine-tuned Holt-Winters Exponential Smoothing on transformed data provided a good rmse. Its performance was still very acceptable even with no fine-tunning configuration and no stationary data.\n- LSTM-based models do not require data to be stationary, and their training was really fast given the few number of records, but their rmse metric results were inconsistent (high variance) and turned-out to be the highest values.","388b5a2d":"### Long Short Term Memory (LSTRM) networks","3da42dd8":"#### Grid search of configuration parameters","8e7c573a":"#### Run the Exponential Smoothing  model on transformed data and plot on regular data","5bb96d89":"### ARIMA"}}