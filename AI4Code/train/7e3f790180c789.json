{"cell_type":{"a2f3d973":"code","7ffd4ba2":"code","edb52053":"code","4b058bf0":"code","7c52c5bd":"code","971c5203":"code","b754cdd8":"code","52e49f34":"code","4db6f52f":"code","0cf03bf1":"code","758c415f":"code","13715a1a":"code","d145d2d4":"code","938f5434":"code","68e8ba97":"code","7ab72724":"code","eaf47fec":"code","43f0bf35":"code","0df2ebcc":"code","199ea279":"code","11b56422":"code","680527d5":"code","397568a2":"code","e1cc3860":"code","4e1dbead":"code","eed00b14":"markdown","8f988c96":"markdown","406b844c":"markdown"},"source":{"a2f3d973":"import os\n\n# \uae30\ubcf8\ud3f4\ub354(base folder)\ndir_ad = '\/kaggle\/input\/tabular-playground-series-nov-2021'\n# dir_ad = os.getcwd()\n\n# \ud559\uc2b5\ub370\uc774\ud130, predict data \ud30c\uc77c\uba85(train, test file name)\ntrain = 'train.csv'\ntest = 'test.csv'\n\n# \uc5b4\ub5a4\uac83\uc744 \ubd84\uc11d \ubc0f \ub370\uc774\ud130 \ucd94\ucd9c\ud560\uc9c0 \uad6c\ubd84\ud560 \uc218 \uc788\ub294 \ud30c\uc77c\uba85 \uc120\ud0dd \n# tag = 'train_MAU_3M_pred_MAUX_4M'\u2190 \uc774\ub7f0 \ubc29\uc2dd\uc73c\ub85c \ub098\uc62c \uac83\uc73c\ub85c \uc608\uc0c1\ntag = 'house_result'\n\n# \ubaa8\ub378 \ubc0f \uac01\uc885 \uc0b0\ucd9c\ubb3c\uc774 \uc800\uc7a5\ub420 \ud3f4\ub354\nmodel_dir = os.path.join(dir_ad, tag + '_v')\n\n# \ub808\uc774\ube14\uc5d0 \ud574\ub2f9\ud558\ub294 \uceec\ub7fc(column corresponding to label: Y value)\ntarget = 'target'\n\n# \ud0c0\uac9f\ud305 \ub300\uc0c1\uc744 \uc2dd\ubcc4\ud558\uae30 \uc704\ud55c \uceec\ub7fc(Column to identify the target)\nindex_col = 'id'\n\n# \ud3c9\uac00\ubc29\ubc95 \"accuracy\", \"roc_auc\",\"f1\" \ub4f1 \uc120\ud0dd\nchoice = \"roc_auc\"","7ffd4ba2":"import seaborn as sns\nimport sys\nimport csv\nimport datetime\nimport operator\nimport joblib\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.svm import SVC\n\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import accuracy_score\nfrom statsmodels.formula.api import ols\nfrom sklearn.metrics import cohen_kappa_score\nfrom collections import OrderedDict\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom scipy.stats import norm, skew, probplot\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom category_encoders.target_encoder import TargetEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier","edb52053":"df_train = pd.read_csv(os.path.join(dir_ad, train))\ndf_test = pd.read_csv(os.path.join(dir_ad, test))","4b058bf0":"num_cols = [col for col in df_test.columns if df_test[col].dtype in [\"float16\",\"float32\",\"float64\", \"int64\", \"int32\"]]\ncat_cols = [col for col in df_test.columns if df_test[col].dtype not in [\"float16\",\"float32\",\"float64\", \"int64\", \"int32\"]]","7c52c5bd":"def detect_outliers(data, col_name):\n    outlier_indices = []\n    # iterate over features(columns)\n    for col in col_name:\n        Q1 = np.percentile(data[col], 25)\n        Q3 = np.percentile(data[col],75)\n        IQR = Q3 - Q1\n        outlier_step = 1.5 * IQR\n        outlier_list_col = data[(data[col] < Q1 - outlier_step) | (data[col] > Q3 + outlier_step )].index\n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","971c5203":"# Drop outliers\ndf_train = df_train.drop(detect_outliers(df_train, num_cols), axis = 0).reset_index(drop=True)","b754cdd8":"def outlier_replace(data, col_name, q1=0.25, q3=0.75):\n    quartile1 = data[col_name].quantile(q1)\n    quartile3 = data[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    data.loc[(data[col_name] < low_limit), col_name] = low_limit\n    data.loc[(data[col_name] > up_limit), col_name] = up_limit","52e49f34":"for i in num_cols:\n    outlier_replace(df_train,i)\n    outlier_replace(df_test, i)","4db6f52f":"df_train = df_train.apply(lambda x: x.fillna(x.median()) if x.dtype != \"O\" else x, axis=0)\ndf_test = df_test.apply(lambda x: x.fillna(x.median()) if x.dtype != \"O\" else x, axis=0)","0cf03bf1":"def count_plot(d, y, x):\n    plt.figure(figsize=(12,6))\n    sns.countplot(x = d[y], hue = x, data=d)\n    plt.ylabel('Number of people')\n    plt.title('Survival count by '+ x)","758c415f":"over_column_name = list()\n\nfor i in num_cols:\n    if (len(df_train[i].value_counts())<20):\n        count_plot(df_train, target, i)\n    elif (len(df_train[i].value_counts())>20):\n        over_column_name.append(i)","13715a1a":"df_train = pd.get_dummies(df_train)\ndf_test = pd.get_dummies(df_test)","d145d2d4":"test_col = []\ndelete_train = []\n\nfor i in df_test.columns:\n    test_col.append(i)\nfor j in df_train.columns:\n    if j not in test_col:\n        delete_train.append(j)\ndelete_train.remove(target)","938f5434":"train_col = []\ndelete_test = []\n\nfor i in df_train.columns:\n    train_col.append(i)\nfor j in df_test.columns:\n    if j not in train_col:\n        delete_test.append(j)","68e8ba97":"for delete in delete_train:\n    df_train = df_train.drop([delete],axis=1)\n    \nfor delete in delete_test:\n    df_test = df_test.drop([delete],axis=1)","7ab72724":"df_train[target] = df_train[target].astype(int)\ntrain_y = df_train[target]\ntrain_x = df_train.drop(labels = [target],axis = 1)","eaf47fec":"# Kfold\ub85c Cross validate\n# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=5)","43f0bf35":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier","0df2ebcc":"# RFC Parameters tunning \nRFC = RandomForestClassifier()\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [3, 10],\n              \"min_samples_split\": [3, 10],\n              \"min_samples_leaf\": [3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring = choice, verbose = 1)\ngsRFC.fit(train_x, train_y)\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_","199ea279":"# XGB Parameters tunning \nXGB = xgb.XGBClassifier()\n\n## Search grid for optimal parameters\nxg_param_grid = {\"max_depth\": [3, 8],\n              \"learning_rate\": [0.0069],\n              \"eval_metric\": ['auc'],\n#               \"n_estimators\": [6000],\n              \"gamma\": [0.64],\n              \"subsample\": [0.2, 0.7],\n              \"max_delta_step\": [3],\n              \"min_child_weight\": [10]}\n\ngsXGB = GridSearchCV(XGB,param_grid = xg_param_grid, cv=kfold, scoring = choice, verbose = 1)\ngsXGB.fit(train_x, train_y)\nXGB_best = gsXGB.best_estimator_\n\n# Best score\ngsXGB.best_score_","11b56422":"# LR Parameters tunning \nLR = LogisticRegression()\n\n## Search grid for optimal parameters\nlr_param_grid = {\"C\": [0.001, 0.01, 10]}\n\n\ngsLR = GridSearchCV(LR,param_grid = lr_param_grid, cv=kfold, scoring = choice, verbose = 1)\ngsLR.fit(train_x, train_y)\nLR_best = gsLR.best_estimator_\n\n# Best score\ngsLR.best_score_","680527d5":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(gsRFC.best_estimator_,\"RF learning curves\",train_x,train_y,cv=kfold)\n# g = plot_learning_curve(gsXGB.best_estimator_,\"XGB learning curves\",train_x,train_y,cv=kfold)\ng = plot_learning_curve(gsLR.best_estimator_,\"LR learning curves\",train_x,train_y,cv=kfold)","397568a2":"# 3\uac1c \ubaa8\ub378 heat map\uc73c\ub85c \uc0c1\uad00\uad00\uacc4 \ube44\uad50\n# Correlation comparison with three model heat maps\n\ntest_RFC = pd.Series(RFC_best.predict(df_test), name=\"RFC\")\ntest_XGB = pd.Series(XGB_best.predict(df_test), name=\"XGB\")\ntest_LR = pd.Series(LR_best.predict(df_test), name=\"LR\")\n\nensemble_results = pd.concat([test_RFC,test_XGB,test_LR],axis=1)\n\ng= sns.heatmap(ensemble_results.corr(),annot=True)","e1cc3860":"votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('xgb', XGB_best),('lr', LR_best)], voting='soft', n_jobs=4)\nvotingC = votingC.fit(train_x, train_y)","4e1dbead":"test_Result = pd.Series(votingC.predict(df_test), name=target)\ndf_test = pd.read_csv(os.path.join(dir_ad, test))\nsubmission = pd.DataFrame({index_col: df_test[index_col], target: test_Result})\nsubmission.to_csv(\"submission.csv\",index=False)\nsubmission.head()","eed00b14":"# Model","8f988c96":"# Config","406b844c":"\uba38\uc2e0\ub7ec\ub2dd\uc744 \ubab0\ub77c\ub3c4 \"Config\"\uc5d0 5\uac00\uc9c0\ub9cc \ub123\uc73c\uba74, \uc2e4\ud589\ud560 \uc218 \uc788\uac8c \ub9cc\ub4e4\uc5c8\uc2b5\ub2c8\ub2e4. \nEven if you don't know machine learning, you can submit only 5 things in \"config\"\n\n\ud68c\uc0ac\uc5d0\uc11c \ub9ce\uc774 \uc0ac\uc6a9\ud558\uac8c \ub9cc\ub4e4\uc5c8\uc5c8\uace0, \ub3c4\uc6c0\uc774 \ub9ce\uc774 \ub420 \uac81\ub2c8\ub2e4. \nI made a lot of use of it in my company, and it will help a lot."}}