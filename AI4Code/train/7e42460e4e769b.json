{"cell_type":{"ce7c71d5":"code","f793fca9":"code","71225896":"code","a714119c":"code","c06a72d6":"code","6683f363":"code","5f859477":"code","80ca73d7":"code","23f4ef6b":"code","407cc29c":"code","7a73e440":"code","0686a666":"code","d896e10f":"code","27b9c12e":"code","b9cc5e1f":"code","da48f77a":"code","17bf5c4f":"code","1e10bbb3":"markdown","46415fb3":"markdown","cf5b0c11":"markdown","77bfac9d":"markdown","04204c52":"markdown"},"source":{"ce7c71d5":"import pandas as pd\nimport numpy as np\nimport re ","f793fca9":"train_data = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_data  =pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntrain_data.head(10)","71225896":"train_data.dtypes","a714119c":"train_data['text'][0]","c06a72d6":"import re\ndef  clean_text(df, text_field, new_text_field_name):\n    df[new_text_field_name] = df[text_field].str.lower()\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)|^rt|http.+?\", \"\", elem))  \n    # remove numbers\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n    \n    return df\ndata_clean = clean_text(train_data, 'text', 'text_clean')\ndata_clean.head()","6683f363":"import nltk.corpus\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\ndata_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ndata_clean.head()","5f859477":"import nltk \nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize, word_tokenize\ndata_clean['text_tokens'] = data_clean['text_clean'].apply(lambda x: word_tokenize(x))\ndata_clean.head()","80ca73d7":"from nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize\ndef word_stemmer(text):\n    stem_text = [PorterStemmer().stem(i) for i in text]\n    return stem_text\ndata_clean['text_clean_tokens'] = data_clean['text_tokens'].apply(lambda x: word_stemmer(x))\ndata_clean.head()","23f4ef6b":"nltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\ndef word_lemmatizer(text):\n    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n    return lem_text\ndata_clean['text_clean_tokens'] = data_clean['text_tokens'].apply(lambda x: word_lemmatizer(x))\ndata_clean.head()","407cc29c":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data_clean['text_clean'],data_clean['target'],random_state = 0)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import LogisticRegression\npipeline_sgd = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf',  TfidfTransformer()),\n    ('lr', LogisticRegression()),\n])\nmodel = pipeline_sgd.fit(X_train, y_train)","7a73e440":"from sklearn.metrics import classification_report\ny_predict = model.predict(X_test)\nprint(classification_report(y_test, y_predict))","0686a666":"#Confusion Matrix Visualisation\nimport matplotlib.pyplot as plt \nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(model, X_test, y_test) \nplt.show()","d896e10f":"test_data.head()","27b9c12e":"submission_test_clean = test_data.copy()\nsubmission_test_clean = clean_text(submission_test_clean, \"text\",\"text_clean\")\nsubmission_test_clean['text_clean'] = submission_test_clean['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\nsubmission_test_clean.head()","b9cc5e1f":"submission_test_pred = model.predict(submission_test_clean['text_clean'])","da48f77a":"id_col = test_data['id']\nsubmission_df_kaggle = pd.DataFrame({\"id\": id_col,\"target\": submission_test_pred})\nsubmission_df_kaggle.head()","17bf5c4f":"submission_df_kaggle.to_csv(\"submission.csv\", index=False)","1e10bbb3":"# Removing stopwords","46415fb3":"# text lemmatisation","cf5b0c11":"# text tokenization","77bfac9d":"# Stemming words with NLTK","04204c52":"# text preprocessing"}}