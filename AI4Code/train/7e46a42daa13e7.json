{"cell_type":{"8ca1d815":"code","8c77b8e4":"code","8ba4ba9f":"code","8318043f":"code","2718e659":"code","907284e2":"code","2b454663":"code","742cf120":"code","38b86c03":"code","c3dcc063":"code","af783fed":"code","40efbca3":"code","551d2d83":"markdown","5dc7ce59":"markdown","a2c646f4":"markdown","08d97d18":"markdown","e3a30ace":"markdown","9db42cf4":"markdown","1dd2b385":"markdown","86ba29a0":"markdown"},"source":{"8ca1d815":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom scipy.stats import randint\nimport matplotlib.pyplot as plt\nimport time\nimport warnings\nwarnings.simplefilter(\"ignore\", category=PendingDeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\", category=FutureWarning)\nwarnings.simplefilter(\"ignore\", category=UserWarning)","8c77b8e4":"train_df = pd.read_csv(\"..\/input\/learn-together\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/learn-together\/test.csv\")","8ba4ba9f":"X_train = train_df.copy()\nX_test = test_df.copy()\n\ny_train = X_train[\"Cover_Type\"]\nX_train.drop(columns=[\"Id\", \"Cover_Type\"], inplace=True, axis=1)\ntest_ids = X_test[\"Id\"]\nX_test.drop(columns=[\"Id\"], inplace=True, axis=1)","8318043f":"SEED = 42","2718e659":"rf = RandomForestClassifier(n_jobs=-1,\n                            random_state=SEED)\nrf.fit(X_train, y_train)","907284e2":"scores = cross_val_score(rf, X_train, y_train, cv=10, scoring=\"accuracy\")\nscores.mean(), scores.std()","2b454663":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)","742cf120":"rf_scaled = RandomForestClassifier(n_jobs=-1,\n                            random_state=SEED)\nrf_scaled.fit(X_train, y_train)\n\nscores = cross_val_score(rf_scaled, X_train, y_train, cv=10, scoring=\"accuracy\")\nscores.mean(), scores.std()","38b86c03":"def hyperparameter_tune(base_model, parameters, n_iter, kfold, X=X_train, y=y_train):\n    start_time = time.time()\n    \n    # Arrange data into folds with approx equal proportion of classes within each fold\n    k = StratifiedKFold(n_splits=kfold, shuffle=False)\n    \n    optimal_model = RandomizedSearchCV(base_model,\n                            param_distributions=parameters,\n                            n_iter=n_iter,\n                            cv=k,\n                            n_jobs=-1,\n                            random_state=SEED)\n    \n    optimal_model.fit(X, y)\n    \n    stop_time = time.time()\n\n    scores = cross_val_score(optimal_model, X, y, cv=k, scoring=\"accuracy\")\n    \n    print(\"Elapsed Time:\", time.strftime(\"%H:%M:%S\", time.gmtime(stop_time - start_time)))\n    print(\"====================\")\n    print(\"Cross Val Mean: {:.3f}, Cross Val Stdev: {:.3f}\".format(scores.mean(), scores.std()))\n    print(\"Best Score: {:.3f}\".format(optimal_model.best_score_))\n    print(\"Best Parameters: {}\".format(optimal_model.best_params_))\n    \n    return optimal_model.best_params_, optimal_model.best_score_","c3dcc063":"base_model = RandomForestClassifier(n_jobs=-1,\n                                   random_state=SEED)\n\nlots_of_parameters = {\n    \"max_depth\": [3, 5, 10, None],\n    \"n_estimators\": [100, 200, 300, 400, 500],\n    \"max_features\": randint(1, 3),\n    \"criterion\": [\"gini\", \"entropy\"],\n    \"bootstrap\": [True, False],\n    \"min_samples_leaf\": randint(1, 4)\n}\n\nparameters = {\n    \"max_depth\": [3, 5, 10, None],\n    \"n_estimators\": [100, 200, 300, 400, 500]\n}\n\nbest_params, best_score = hyperparameter_tune(base_model, parameters, 10, 5, X_train, y_train)","af783fed":"scores = []\nfolds = range(2, 8)\n\nfor i in folds:\n    print(\"\\ncv = \", i)\n    best_params, best_score = hyperparameter_tune(base_model, parameters, 10, i, X_train, y_train)\n    scores.append(best_score)","40efbca3":"plt.plot([x for x in folds], scores)\nplt.xlabel(\"# of Folds\")\nplt.ylabel(\"Best Score\")\nplt.title(\"The Impact of # of Folds on Randomized Search CV Score\")\nplt.show()","551d2d83":"### Tune parameters (a smaller set of parameters for demo purposes)","5dc7ce59":"### Set the starting state for random number generation to ensure reproducibility","a2c646f4":"### Create a baseline random forest model","08d97d18":"### Create a reusable function to tune hyperparameters","e3a30ace":"### The goal of this kernel is to showcase the functionality of RandomizedSearchCV\n\nRandomizedSearchCV is preferred over GridSearchCV for its speed, particularly when tuning many hyperparameters.\n\nComments are welcome :) Please share your expertise - I would love to learn better ways of tuning models.","9db42cf4":"### Read data and split out labels","1dd2b385":"### Observe the impact of scaling features in our base model","86ba29a0":"### As an interesting aside, observe how changing the number of folds impacts the optimal paramaters..."}}