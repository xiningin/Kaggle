{"cell_type":{"c54fd064":"code","9269531e":"code","3ae2c89b":"code","1c73c268":"code","96996643":"code","fc1b0ee3":"code","f674632e":"code","7cc0f9bc":"code","8e5fbd58":"code","aee88bbd":"code","f55d5025":"code","85dd877d":"code","6970ae0d":"code","a2411fcd":"code","9e759441":"code","77b4a329":"code","84a2d3d0":"code","a9acd6b0":"markdown","36f284eb":"markdown"},"source":{"c54fd064":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","9269531e":"#Trained model is loaded here...\n\nimport torch\nfrom transformers import T5ForConditionalGeneration,T5Tokenizer\nimport ast\n\ndef set_seed(seed):\n  torch.manual_seed(seed)\n  if torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\nmodel = T5ForConditionalGeneration.from_pretrained('..\/input\/sentisum-eda-training\/result')\ntokenizer = T5Tokenizer.from_pretrained('t5-base')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print (\"device \",device)\nmodel = model.to(device)","3ae2c89b":"# Topkp decoding function for generating the output\n\ndef topkp_decoding (inp_ids,attn_mask):\n      topkp_output = model.generate(input_ids=inp_ids,\n                                     attention_mask=attn_mask,\n                                     max_length=50,\n                                   do_sample=True,\n                                   top_k=40,\n                                   top_p=0.80,\n                                   num_return_sequences=3,\n                                    no_repeat_ngram_size=2,\n                                    early_stopping=True\n                                   )\n      Questions = [tokenizer.decode(out, skip_special_tokens=True,clean_up_tokenization_spaces=True) for out in topkp_output]\n      return list(Question.strip().capitalize() for Question in Questions)    ","1c73c268":"def t5_answer(review):\n    set_seed(42)\n    con = \"Comment: %s <\/s>\" %(review)\n    encoding = tokenizer.encode_plus(con, return_tensors=\"pt\")\n    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n    output = topkp_decoding(input_ids, attention_masks)\n    output.sort(key=len, reverse=True)\n\n    return output","96996643":"# Negative sentiments must be given more importance since they are the deciding factor for any organization. In this function, all negative sentiments (if any) are pulled in front and best sentiments are refined.\n\ndef refined_out(out):\n    temp1= list(out[0].split(', '))\n    \n    if len(out)==3:\n        temp2= list(out[1].split(', ')) + list(out[2].split(', '))\n        for el in temp2:\n            if el.lower() not in map(str.lower, temp1) and len(temp1)<4:\n                temp1.append(el)\n    i=0\n    j=len(temp1)-1\n    \n    while(j>i):\n        if 'negative' in temp1[i] and 'negative' in temp1[j]:   \n            i+=1\n\n        elif 'positive' in temp1[i] and 'negative' in temp1[j]:\n            temp1[i], temp1[j]= temp1[j], temp1[i]\n            i+=1\n            j-=1\n\n        elif 'positive' in temp1[i] and 'positive' in temp1[j]:\n            j-=1\n        else:\n            j-=1\n    \n    return ' '.join(temp1)","fc1b0ee3":"# Smoothed Bleu Score\n\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.bleu_score import SmoothingFunction\n\ndef bleu_score(ref, pred):\n    smoother = SmoothingFunction()\n    score= sentence_bleu(ref,pred, smoothing_function=smoother.method4, weights= (0.8,0.2))\n    \n    return score","f674632e":"# Jaccard Similarity\n\ndef jaccard_similarity(list1, list2):\n    s1 = set(list1)\n    s2 = set(list2)\n    return float(len(s1.intersection(s2))) \/ float(len(s1.union(s2)))","7cc0f9bc":"# A custom scorer that consider the encoding ID of each word and uses jaccard metrics for calculation \n\ndef custom_scorer(val, out):\n    encoding1 = tokenizer.encode_plus(val, return_tensors=\"pt\")\n    encoding2 = tokenizer.encode_plus(out, return_tensors=\"pt\")\n    ids1, attention_masks = encoding1[\"input_ids\"].to(device), encoding1[\"attention_mask\"].to(device)\n    ids2, attention_masks = encoding2[\"input_ids\"].to(device), encoding2[\"attention_mask\"].to(device)\n    ids1 = [ids1.tolist()[0]]\n    ids2 = ids2.tolist()[0]\n    \n    score= bleu_score(ids1, ids2)\n    \n    return score","8e5fbd58":"# Universal Sentence Encoder for measuring Cosine similarity between validation and test output. USE can maintain the semantic relation in the sentences.\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nimport os, sys\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom numpy import dot\nfrom numpy.linalg import norm\n\nmodule_url = \"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5\"\nembed = hub.load(module_url)","aee88bbd":"def cosine_similarity(val, out):\n\n    val= embed(val).numpy()\n    out= embed(out).numpy()\n    cos_sim = dot(val[0],out[0])\/(norm(val[0])*norm(out[0]))\n    \n    return cos_sim","f55d5025":"ref= pd.read_csv('..\/input\/sentisum-eda-training\/ref.csv')","85dd877d":"ref.head(3)","6970ae0d":"# Evaluation loop\n\nimport statistics\n\nscore=[]\nfor el in ref.index:\n    out= t5_answer(ref['Review0'][el])\n    out1= [refined_out(out)]\n    #out1= list(out1.split(' '))\n    val1= list(ref['Column16'][el].split(', '))\n    val1= [' '.join(val1)]\n    #val1= [list(val1.split(' '))]\n    \n    sc= cosine_similarity(val1, out1)\n    score.append(sc)\n  \navgscore= statistics.mean(score)","a2411fcd":"# Just a test...\n\nout= t5_answer(ref['Review0'][16])","9e759441":"ref['Review0'][16]","77b4a329":"out= refined_out(out)\nout","84a2d3d0":"print('The average Cosine similarity score of unseen validation dataset is: {}%'.format(avgscore*100))","a9acd6b0":"## Different Metrics for evaluation\n\nI performed many iterations through various metrics to end up using Cosine Similarity using Universal Sentence Encoder embeddings since these maintain the semantic meaning of the sentiments.","36f284eb":"# **SentiSum**\n\nThis notebook encapsulates the testing of the model trained in the previous notebook. 'ref' named validation dataset was created in the previous notebook which is unseen for the model. We'll use that dataset to evaluate our model."}}