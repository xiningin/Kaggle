{"cell_type":{"f5aa0fa7":"code","7ebeb9bc":"code","0d2c9954":"code","24740ee7":"code","79cea467":"code","bf29ebd1":"code","67a25daf":"code","c3596682":"code","000ee2cc":"code","96b257c6":"code","79d53579":"code","e317ec9c":"code","16ef0b69":"code","48f2ea52":"code","405d2fd6":"code","dec4caf9":"code","c10cbbc8":"code","1a793dee":"code","3f3872a5":"code","d4248abc":"code","abd69063":"code","cd74dba4":"code","01c33a92":"code","05e30467":"code","1540c3ef":"code","9abb0d3c":"code","f03367d8":"code","fb6f129a":"code","b804cd99":"code","c415eac2":"code","025fbe1f":"code","1142ade7":"code","b92a98b1":"code","f94e2301":"code","cb6fad36":"code","0ee55795":"code","47f4b64e":"code","00bb2e03":"code","b2c83303":"code","7adf2a79":"code","0cd4de2f":"code","c800dcbb":"code","6753042a":"code","8f8ada92":"markdown","9ab7cb6b":"markdown","ff26c093":"markdown","d816ac46":"markdown","841d6be3":"markdown","665b6032":"markdown","f081fe85":"markdown","d65c8b15":"markdown","d44ab1d4":"markdown","cde1e994":"markdown","82247b6f":"markdown","0d8e7fcb":"markdown","b9a8fb36":"markdown","df675c2b":"markdown","c39823c8":"markdown","8362f8a5":"markdown","2c75cefd":"markdown","2815f08f":"markdown","f316cfc4":"markdown","eb9a76c0":"markdown","f377b1d8":"markdown","042fdfde":"markdown","8214938a":"markdown","e6f305d5":"markdown","f1d4892e":"markdown","5b33cf58":"markdown","9016bf73":"markdown","a9833690":"markdown","e5b22004":"markdown","1ff785b3":"markdown","b16baacf":"markdown","cc8e1e29":"markdown","1f0b6ea5":"markdown","d6848027":"markdown"},"source":{"f5aa0fa7":"import numpy as np\nimport pandas as pd\nfrom keras.layers import Input, Lambda, Dense, Flatten\nfrom keras.models import Model\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.preprocessing import image\nfrom keras.models import Sequential\nfrom glob import glob\nimport matplotlib.pyplot as plt\n\nfrom keras.optimizers import Adam, SGD, RMSprop\nimport tensorflow as tf\nimport cv2\nimport glob\nfrom keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\nfrom tensorflow.python.keras import backend as K\nimport plotly.graph_objects as go\nimport plotly.offline as py\nautosize =False\n\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\n%matplotlib inline","7ebeb9bc":"import pandas as pd","0d2c9954":"train_dir='\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'\ntest_dir='\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/test\/'\ntrain=pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/test.csv')\n#sub  = pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/sample_submission.csv')","24740ee7":"train.head()","79cea467":"# as per an ongoing discussion, there are some duplicate images in the training data, these images might adversely impact our model, \n# so, lets remove these images\ndup = pd.read_csv(\"\/kaggle\/input\/siim-list-of-duplicates\/2020_Challenge_duplicates.csv\")\n\ndrop_idx_list = []\nfor dup_image in dup.ISIC_id_paired:\n    for idx,image in enumerate(train.image_name):\n        if image == dup_image:\n            drop_idx_list.append(idx)\n\nprint(\"no. of duplicates in training dataset:\",len(drop_idx_list))\n\ntrain.drop(drop_idx_list,inplace=True)\n\nprint(\"updated dimensions of the training dataset:\",train.shape)","bf29ebd1":"train.target.value_counts()","67a25daf":"# function to draw bar plot\nimport matplotlib.pyplot as plt\ndef draw_bar_plot(category,length,xlabel,ylabel,title,sub):\n    plt.subplot(2,2,sub)\n    plt.bar(category, length)\n    plt.legend()\n    plt.xlabel(xlabel, fontsize=15)\n    plt.ylabel(ylabel, fontsize=15)\n    plt.title(title, fontsize=15)\n    #plt.show()","c3596682":"# lets visualize the class distribution\nplt.figure(figsize = (8,6))\nplt.bar([\"Melanoma\",\"Normal\"],[len(train[train.target==1]), len(train[train.target==0])],color = 'rg')","000ee2cc":"df_benign=train[train['target']==0].sample(2000)\ndf_malignant=train[train['target']==1]","96b257c6":"print('Benign Cases')\nbenign=[]\ndf_b=df_benign.head(30)\ndf_b=df_b.reset_index()\nfor i in range(30):\n    img=cv2.imread(str(train_dir + df_benign['image_name'].iloc[i]+'.jpg'))\n    img = cv2.resize(img, (224,224))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32)\/255.\n    benign.append(img)\nf, ax = plt.subplots(5,6, figsize=(10,6))\nfor i, img in enumerate(benign):\n        ax[i\/\/6, i%6].imshow(img)\n        ax[i\/\/6, i%6].axis('off')\n        \nplt.show()","79d53579":"print('Malignant Cases')\nm=[]\ndf_m=df_malignant.head(30)\ndf_m=df_m.reset_index()\nfor i in range(30):\n    img=cv2.imread(str(train_dir + df_m['image_name'].iloc[i]+'.jpg'))\n    img = cv2.resize(img, (224,224))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32)\/255.\n    m.append(img)\nf, ax = plt.subplots(5,6, figsize=(10,6))\nfor i, img in enumerate(m):\n        ax[i\/\/6, i%6].imshow(img)\n        ax[i\/\/6, i%6].axis('off')\n        \nplt.show()","e317ec9c":"train.info()","16ef0b69":"import plotly.express as px\n\nfig = px.pie(train, train['target'],color_discrete_sequence=px.colors.sequential.RdBu)\nfig.show()","48f2ea52":"train_nona=train.dropna()\n#fig = px.treemap(train_nona, path=['sex', 'age_approx'], values='target', color='target')\n#fig.show()","405d2fd6":"entire=train.append(test)\naffected_areas=pd.value_counts(entire['anatom_site_general_challenge'])\nfig = go.Figure(data=[go.Pie(labels=affected_areas.index, values=affected_areas.values, hole=.3)])\nfig.update_traces(hoverinfo='label+percent', textinfo='value',textfont_size=15,\n                  marker=dict(colors=['#11100b','#ff3560'], line=dict(color='#FFFFFF', width=2.5)))\nfig.update_layout(\n    title='AFFECTED AREAS')\npy.iplot(fig)","dec4caf9":"df_malignant=df_malignant.dropna()","c10cbbc8":"age_counts=pd.value_counts(df_malignant['age_approx'])\ngender_counts=pd.value_counts(df_malignant['sex'])\nanatom_site_counts=pd.value_counts(df_malignant['anatom_site_general_challenge'])\n\nfig = make_subplots(\n    rows=1, cols=3,\n    specs=[[{\"type\": \"xy\"},{\"type\": \"domain\"}, {\"type\": \"xy\"}]])\n\nfig.add_trace(go.Bar(y=age_counts.values, x=age_counts.index),row=1, col=1)\n\n\nfig.add_trace(go.Pie(values=gender_counts.values, labels=gender_counts.index,marker=dict(colors=['#100b','#f00560'], line=dict(color='#FFFFFF', width=2.5))),\n              row=1, col=2)\n\nfig.add_trace(go.Scatter(x=anatom_site_counts.index, y=anatom_site_counts.values),\n              row=1, col=3)\n\nfig.update_layout(height=700, showlegend=False)\n\nfig.update_xaxes(title_text=\"Age\", row=1, col=1)\nfig.update_xaxes(title_text=\"Site\", row=1, col=3)\n\n# Update yaxis properties\nfig.update_yaxes(title_text=\"Count\", row=1, col=1)\nfig.update_yaxes(title_text=\"Count\", row=1, col=3)\n\n# Update title and height\nfig.update_layout(title_text=\"MALIGNANT DATA wrt AGE, GENDER, SITE\",height=600, width=1000)\n\nfig.show()","1a793dee":"agecounts=pd.value_counts(train['age_approx'])\nfig = px.bar(train, x=agecounts.index, y=agecounts.values)\nfig.update_layout(title_text='Age counts of the training data')\nfig.show()","3f3872a5":"agecounts=pd.value_counts(test['age_approx'])\nfig = px.bar(test, x=agecounts.index, y=agecounts.values)\nfig.update_layout(title_text='Age counts of the testing data')\nfig.show()","d4248abc":"fig = make_subplots(\n    rows=1, cols=2,\n    specs=[[{\"type\": \"domain\"}, {\"type\": \"domain\"}]])\n\nsite_train_counts=pd.value_counts(train['anatom_site_general_challenge'])\n\nfig.add_trace(go.Pie(values=site_train_counts.values, labels=site_train_counts.index,title_text='Melanoma regions for training dataset',marker=dict(colors=['#100b','#f00560'], line=dict(color='#FFFFFF', width=2.5))),\n              row=1, col=1)\n\nsite_test_counts=pd.value_counts(test['anatom_site_general_challenge'])\n\nfig.add_trace(go.Pie(values=site_test_counts.values, labels=site_test_counts.index,title_text='Melanoma regions for testing dataset',marker=dict(colors=['#100b','#f00560'], line=dict(color='#FFFFFF', width=2.5))),\n              row=1, col=2)\n\nfig.update_layout(height=700, showlegend=False)\n\nfig.show()","abd69063":"# Since this is a huge dataset, we would take a sample of it for training purpose\n\ndf_0=train[train['target']==0].sample(2000)\ndf_1=train[train['target']==1]\ntrain=pd.concat([df_0,df_1])\ntrain=train.reset_index()","cd74dba4":"# update image names with the whole path\ndef append_ext(fn):\n    return train_dir+fn+\".jpg\"\ntrain[\"image_name\"]=train[\"image_name\"].apply(append_ext)\n\ndef append_ext(fn):\n    return test_dir+fn+\".jpg\"\ntest[\"image_name\"]=test[\"image_name\"].apply(append_ext)","01c33a92":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(train['image_name'],train['target'], test_size=0.2, random_state=1234)\n\ntrain=pd.DataFrame(X_train)\ntrain.columns=['image_name']\ntrain['target']=y_train\n\nvalidation=pd.DataFrame(X_val)\nvalidation.columns=['image_name']\nvalidation['target']=y_val","05e30467":"# resizing the images\nIMG_DIM = (224, 224)\n\n# load images\ntrain_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train.image_name]\ntrain_imgs = np.array(train_imgs)\n\nvalidation_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in validation.image_name]\nvalidation_imgs = np.array(validation_imgs)\n\nprint('Train dataset shape:', train_imgs.shape, \n      '\\tValidation dataset shape:', validation_imgs.shape)","1540c3ef":"# define parameters for model training\nbatch_size = 128\nnum_classes = 2\nepochs = 30\ninput_shape = (224, 224, 3)","9abb0d3c":"# focal loss\ndef focal_loss(alpha=0.25,gamma=2.0):\n    def focal_crossentropy(y_true, y_pred):\n        bce = K.binary_crossentropy(y_true, y_pred)\n        \n        y_pred = K.clip(y_pred, K.epsilon(), 1.- K.epsilon())\n        p_t = (y_true*y_pred) + ((1-y_true)*(1-y_pred))\n        \n        alpha_factor = 1\n        modulating_factor = 1\n\n        alpha_factor = y_true*alpha + ((1-alpha)*(1-y_true))\n        modulating_factor = K.pow((1-p_t), gamma)\n\n        # compute the final loss and return\n        return K.mean(alpha_factor*modulating_factor*bce, axis=-1)\n    return focal_crossentropy","f03367d8":"# we will use Adam optimizer\nopt = Adam(lr=1e-5)\n\n#total number of iterations is always equal to the total number of training samples divided by the batch_size.\nnb_train_steps = train.shape[0]\/\/batch_size\nnb_val_steps=validation.shape[0]\/\/batch_size\n\nprint(\"Number of training and validation steps: {} and {}\".format(nb_train_steps,nb_val_steps))","fb6f129a":"# Pixel Normalization and Image Augmentation\ntrain_datagen = ImageDataGenerator(rescale=1.\/255, zoom_range=0.3, rotation_range=50,\n                                   width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, \n                                   horizontal_flip=True, fill_mode='nearest')\n\n# no need to create augmentation images for validation data, only rescaling the pixels\nval_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow(train_imgs, y_train, batch_size=batch_size)\nval_generator = val_datagen.flow(validation_imgs, y_val, batch_size=batch_size)","b804cd99":"img_id = 100\ngenerator_100 = train_datagen.flow(train_imgs[img_id:img_id+1], train.target[img_id:img_id+1],\n                                   batch_size=1)\naug_img = [next(generator_100) for i in range(0,5)]\nfig, ax = plt.subplots(1,5, figsize=(16, 6))\nprint('Labels:', [item[1][0] for item in aug_img])\nl = [ax[i].imshow(aug_img[i][0][0]) for i in range(0,5)]","c415eac2":"import gc\ndel train\ngc.collect()","025fbe1f":"from keras.applications import vgg16\nfrom keras.models import Model\nimport keras\n\nvgg = vgg16.VGG16(include_top=False, weights='imagenet', \n                                     input_shape=input_shape)\n\noutput = vgg.layers[-1].output\noutput = keras.layers.Flatten()(output)\nvgg_model = Model(vgg.input, output)\n\nvgg_model.trainable = False\nfor layer in vgg_model.layers:\n    layer.trainable = False\n    \nimport pandas as pd\npd.set_option('max_colwidth', -1)\nlayers = [(layer, layer.name, layer.trainable) for layer in vgg_model.layers]\npd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])    ","1142ade7":"vgg_model.trainable = True\n\nset_trainable = False\nfor layer in vgg_model.layers:\n    if layer.name in ['block5_conv1', 'block4_conv1']:\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n    else:\n        layer.trainable = False\n        \nlayers = [(layer, layer.name, layer.trainable) for layer in vgg_model.layers]\npd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])    ","b92a98b1":"from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer\nfrom keras.models import Sequential\nfrom keras import optimizers\n\nmodel = Sequential()\nmodel.add(vgg_model)\nmodel.add(Dense(512, activation='relu', input_dim=input_shape))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\nmodel.compile(loss=focal_loss(), metrics=[tf.keras.metrics.AUC()],optimizer=opt)","f94e2301":"#!pip install livelossplot\n#from livelossplot import PlotLossesKeras","cb6fad36":"from keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='loss', patience=3, verbose=1)","0ee55795":"#cb=[PlotLossesKeras()]\nmodel.fit_generator(train_generator, steps_per_epoch=nb_train_steps, epochs=epochs,callbacks=[es],\n                              validation_data=val_generator, validation_steps=nb_val_steps, \n                              verbose=1)\n","47f4b64e":"x_test = np.load('..\/input\/siimisic-melanoma-resized-images\/x_test_224.npy')\nx_test = x_test.astype('float16')\ntest_imgs_scaled = x_test \/ 255\ndel x_test\ngc.collect()","00bb2e03":"target=[]\ni = 0\nfor img in test_imgs_scaled:\n    img1=np.reshape(img,(1,224,224,3))\n    prediction=model.predict(img1)\n    i = i + 1\n    print(\"predicted image no.\",i)\n    target.append(prediction[0][0])","b2c83303":"# submission file\nsub=pd.read_csv(\"..\/input\/siim-isic-melanoma-classification\/sample_submission.csv\")\nsub['target']=target\n#sub.to_csv('submission.csv', index=False)\nsub.head()","7adf2a79":"#img_csv=sub.copy()\ntab_csv=pd.read_csv('..\/input\/image-and-tab-csv-files\/submission_tab.csv')\n#img_csv.head()","0cd4de2f":"img_csv=pd.read_csv('..\/input\/image-and-tab-csv-files\/submission_img.csv')","c800dcbb":"sub=img_csv.copy()\nsub['target']= (img_csv['target'] + tab_csv['target'])\/2\n#sub['target']= img_csv['target'] * 0.8 + tab_csv['target'] * 0.2\nsub.head()","6753042a":"sub.to_csv('submission.csv',index=False)","8f8ada92":"Naturally, as medical data is bound to have irregularities, about 98.2% data is benign.","9ab7cb6b":"### Optimizer & No. of Iterations","ff26c093":"**To begin with, let us first observe the benign and malignant classified images.**","d816ac46":"**The following terminology is very important with regard to training our model:**\n\n* The batch_size indicates the total number of images passed to the model per iteration.\n\n* The weights of the units in layers are updated after each iteration.\n\n* The total number of iterations is always equal to the total number of training samples divided by the batch_size.\n\n* An epoch is when the complete dataset has passed through the network once, that is, all the iterations are completed based on data batches.","841d6be3":"**Age diversity in training data Vs Age diversity in testing data**","665b6032":"# Modelling - VGG16 (Transfer Learning)","f081fe85":"## What is Melanoma?\n\nMelanoma, also known as malignant melanoma, is a type of skin cancer that develops from the pigment-producing cells known as melanocytes. Melanomas typically occur in the skin but may rarely occur in the mouth, intestines or eye (uveal melanoma).\n\nThe exact cause of all melanomas isn't clear, but exposure to ultraviolet (UV) radiation from sunlight or tanning lamps and beds increases your risk of developing melanoma. Limiting your exposure to UV radiation can help reduce your risk of melanoma.The risk of melanoma seems to be increasing in people under 40, especially women. Knowing the warning signs of skin cancer can help ensure that cancerous changes are detected and treated before the cancer has spread. Melanoma can be treated successfully if it is detected early.\n\n## Causes\n\n![IMG](https:\/\/www.mayoclinic.org\/-\/media\/kcms\/gbs\/patient-consumer\/images\/2013\/11\/15\/17\/40\/ds00190_-ds00439_-ds00924_-ds00925_im02400_c7_skincancerthu_jpg.jpg)\n\nMelanoma occurs when something goes wrong in the melanin-producing cells (melanocytes) that give color to your skin.\n\nNormally, skin cells develop in a controlled and orderly way \u2014 healthy new cells push older cells toward your skin's surface, where they die and eventually fall off. But when some cells develop DNA damage, new cells may begin to grow out of control and can eventually form a mass of cancerous cells.\n\n## Symptoms\n\nThe first melanoma signs and symptoms often are:\n\nA change in an existing mole\nThe development of a new pigmented or unusual-looking growth on your skin\nMelanoma doesn't always begin as a mole. It can also occur on otherwise normal-appearing skin.\n\n## Prevention\n\n\n* Wear sunscreen year-round. \n* Avoid the sun during the middle of the day.\n* Wear protective clothing.\n* Avoid tanning lamps and beds.\n* Become familiar with your skin so that you'll notice changes.\n\n\n## When to see a doctor \n\nMake an appointment with your doctor if you notice any skin changes that seem unusual.\n\n\nFor more information,[ Click here.](https:\/\/www.mayoclinic.org\/diseases-conditions\/melanoma\/symptoms-causes\/syc-20374884)","d65c8b15":"# Introduction","d44ab1d4":"* ### Split into train and validate dataset","cde1e994":"### Class Distribution","82247b6f":"* ### Update Image Names","0d8e7fcb":"We would leverage Transfer Learning Models for image classification, but why Transfer Learning and not Traditional ML\/DL Algorithms?\nLet's find out!","b9a8fb36":"* ### Resize Images\nResizing images is a critical preprocessing step in computer vision. Principally, our machine learning models train faster on smaller images. An input image that is twice as large requires our network to learn from four times as many pixels \u2014 and that time adds up.","df675c2b":"**Percentage of benign cases VS Malignant cases**","c39823c8":"The irregularities in age groups is not as evident. Both the training and testing data have a majority of reports from the 40-50 age group. Although the reports from the age 70 and above are comparatively less in testing data than in training data.","8362f8a5":"# Load required libraries","2c75cefd":"## Data Preparation","2815f08f":"# What is Transfer Learning - An Introduction\n\nIn transfer learning, you can leverage knowledge (features, weights etc) from previously trained models for training newer models and even tackle problems like having less data for the newer task!\n\n* ### Motivation for Transfer Learning\n    We have already briefly discussed that humans don\u2019t learn everything from the ground up and leverage and transfer their knowledge from previously learnt domains to newer domains and tasks. \n\n    Thus, the key motivation, especially considering the context of deep learning is the fact that most models which solve complex problems need a whole lot of data, and getting vast amounts of labeled data for supervised models can be really difficult, considering the time and effort it takes to label data points. \n    A simple example would be the ImageNet dataset, which has millions of images pertaining to different categories, thanks to years of hard work starting at Stanford!\n\n    However, getting such a dataset for every domain is tough. Besides, most deep learning models are very specialized to a particular domain or even a specific task. \n    While these might be state-of-the-art models, with really high accuracy and beating all benchmarks, it would be only on very specific datasets and end up suffering a significant loss in performance when used in a new task which might still be similar to the one it was trained on. \n    This forms the motivation for transfer learning, which goes beyond specific tasks and domains, and tries to see how to leverage knowledge from pre-trained models and use it to solve new problems!\n    \n\n* ### Understanding Transfer Learning\n\n    The first thing to remember here is that, transfer learning, is not a new concept which is very specific to deep learning. There is a stark difference between the traditional approach of building and training machine learning models, and using a methodology following transfer learning principles.\n\n    Below picture shows the difference between traditional ML vs Transfer Learning\n\n![image.png](attachment:image.png)\n\nTraditional learning is isolated and occurs purely based on specific tasks, datasets and training separate isolated models on them.\nNo knowledge is retained which can be transferred from one model to another.  In transfer learning, you can leverage knowledge\n(features, weights etc) from previously trained models for training newer models and even tackle problems like having less data for the newer task!","f316cfc4":"# Problem with conventional ML & DL Algorithms\n\nHumans have an inherent ability to transfer knowledge across tasks. What we acquire as knowledge while learning about one task, we utilize in the same way to solve related tasks. The more related the tasks, the easier it is for us to transfer, or cross-utilize our knowledge. Some simple examples would be,\n    \n*     Know how to ride a motorbike \u2bab Learn how to ride a car\n*     Know how to play classic piano \u2bab Learn how to play jazz piano\n*     Know math and statistics \u2bab Learn machine learning\n\n![image.png](attachment:image.png)\n\nIn each of the above scenarios, we don\u2019t learn everything from scratch when we attempt to learn new aspects or topics. We transfer and leverage our knowledge from what we have learnt in the past!\n\nConventional machine learning and deep learning algorithms, so far, have been traditionally designed to work in isolation. \n\nThese algorithms are trained to solve specific tasks. The models have to be rebuilt from scratch once the feature-space distribution changes. ","eb9a76c0":"### Image Augmentation & Pixels Normalization\n\n**Image Augmentation:**\nThe idea behind image augmentation is that we follow a set process of taking in existing images from our training dataset and applying some image transformation operations to them, such as rotation, shearing, translation, zooming, and so on, to produce new, altered versions of existing images. Due to these random transformations, we don\u2019t get the same images each time, and we will leverage Python generators to feed in these new images to our model during training.\n\nThe Keras framework has an excellent utility called ImageDataGenerator that can help us in doing all the preceding operations. Let\u2019s initialize two of the data generators for our training and validation datasets.\n\n**Pixel Nomralization:**\nNeural networks process inputs using small weight values, and inputs with large integer values can disrupt \nor slow down the learning process. As such it is good practice to normalize the pixel values so that each pixel value has a value \nbetween 0 and 1. Parameter \"rescale\" used below does pixel normalization for us.","f377b1d8":"Also, the samples have recorded that amount of data recorded for males is greater than that of women.","042fdfde":"# Remove duplicate images from the training dataset","8214938a":"### Define VGG16 Model","e6f305d5":"So, if we compare the training and testing data side by side, the area where melanoma signs appear are similar. The most common region is torso followed by lower extremity.","f1d4892e":"## Melanoma skin site appearance in training data vs in testing data","5b33cf58":"There are a lot of options available in ImageDataGenerator and we have just utilized a few of them. Feel free to check out the documentation to get a more detailed perspective. In our training data generator, we take in the raw images and then perform several transformations on them to generate new images. These include the following.\n\n    Zooming the image randomly by a factor of 0.3 using the zoom_range parameter.\n\n    Rotating the image randomly by 50 degrees using the rotation_range parameter.\n\n    Translating the image randomly horizontally or vertically by a 0.2 factor of the image\u2019s width \n    or height using the width_shift_range and the height_shift_range parameters.\n\n    Applying shear-based transformations randomly using the shear_range parameter.\n\n    Randomly flipping half of the images horizontally using the horizontal_flip parameter.\n\n    Leveraging the fill_mode parameter to fill in new pixels for images after we apply any of the preceding operations \n    (especially rotation or translation). In this case, we just fill in the new pixels with their nearest surrounding pixel values.\n\nLet\u2019s see how some of these generated images might look so that you can understand them better. We will take two sample images from our training dataset to illustrate the same. The first image is an image of a cat.\n","9016bf73":"* ### Take Sample Images for training","a9833690":"Some Inferences from the above plots are:\n* Majority of the patients who have malignant reports are in the age range of 50-80 years.\n* About 62% of the patients who tested malignant were males.\n* Torso, Upper extremity and Lower extremity are the most common sites for melanoma appearance.","e5b22004":"We will be using VGG16 model(Transfer Learning) for image classification","1ff785b3":"# Some insights about the training and testing data\n","b16baacf":"### Define loss function\nWe are taking focal loss because the dataset is an imbalanced dataset","cc8e1e29":"**Analysing Malignant data**","1f0b6ea5":"# Exploratory Data Analysis","d6848027":"# Creating Ensemble of Models generated by training image data and by tabular data"}}