{"cell_type":{"dde6a1fa":"code","56231ec4":"code","937bd0d2":"code","1810ce93":"code","dc0c0bbc":"code","e9a42003":"code","7dbb009a":"code","be27da23":"code","710b4937":"code","ca03d873":"code","bd32b71d":"code","1e1fa88c":"code","eb7cdd5c":"code","1b886198":"markdown","a163ac54":"markdown","5ec563e1":"markdown","5f140b13":"markdown","711afee6":"markdown","7b6b88f9":"markdown","98a93e60":"markdown","81dda36f":"markdown","c1c69bcc":"markdown","b23074e3":"markdown"},"source":{"dde6a1fa":"# Import tensorflow and other necessary libraries\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n#file copy library\nfrom shutil import copyfile\n\n# copy our file into the working directory \n#copyfile(src = \"..\/input\/localfunctions\/fncs (1).py\", dst = \"..\/working\/fncspy.py\") \n# import all of our custom functions\n#from fncspy import *","56231ec4":"# Function evaluating the sigmoid\ndef sigmoid(x):\n    return 1\/(1+np.exp(-x))","937bd0d2":"\n#Function evaluating the derivative of the sigmoid\ndef sigmoidPrime(x):\n    return sigmoid(x)*(1-sigmoid(x))\n","1810ce93":"def net_init(net): \n    for i in range(1,len(net['struct'])): \n        net['W'][i] = 0.01*np.random.normal(size=[net['struct'][i],net['struct'][i-1]]) \n        net['b'][i] = np.zeros([net['struct'][i],1])","dc0c0bbc":"def net_create(st):\n\tnet = {'struct': st, 'W': {}, 'b': {}, 'a':{}, 'h':{} }\n\tnet_init(net)\n\treturn net","e9a42003":"# Function used to evaluate the neural network \ndef net_predict(net,X):\n    o = np.ones([1,X.shape[0]])\n    \n    net['h'][0] = np.transpose(X)\n    for k in range(0,len(net['W'])):\n        net['a'][k+1] = np.matmul(net['b'][k+1],o) + np.matmul(net['W'][k+1],net['h'][k])\n        net['h'][k+1] = sigmoid(net['a'][k+1])\n        \n    return np.transpose(net['h'][len(net['W'])])","7dbb009a":"def net_loss(y,yhat):\n    y = y.reshape([len(y),1])\n    return np.sum(-(1-y)*np.log(1-yhat)-y*np.log(yhat))","be27da23":"def net_missed(y,yhat):\n    y = y.reshape([len(y),1])\n    return np.sum(np.abs(y-(yhat>0.5)))\n","710b4937":"def net_backprop(net,X,y):\n    # Performing forward propagation\n    yhat = net_predict(net,X)\n    \n    # Initializing gradients\n    nabla_b = {}\n    nabla_W = {}\n\n\t# Implementation of gradients based on backpropagation algorithm\n    G = yhat-y.reshape([len(y),1])\n    for k in range(len(net['W']),0,-1):\n        # TODO: Implement back propagation here. Make sure that your imple-\n\t\t# mentation aggregates (by adding) the contributions from all the \n\t\t# training samples. You can infere the expected shape of the list of\n\t\t# gradients from how it is used in the 'net_train' function.\n        nabla_b[k] = np.sum(G, axis = 0)\n        nabla_W[k] = np.matmul(net['h'][k-1],G)\n        G = np.dot(np.multiply(np.transpose(sigmoidPrime(net['a'][k])), G),net['W'][k])\n    return nabla_b, nabla_W","ca03d873":"def net_train(net,X_train,y_train,X_val,y_val,epsilon,NIter):\n\t# Initializing arrays holding the history of loss and missed values\n    Loss = np.zeros(NIter)\n    Loss_val = np.zeros(NIter)\n    missed_val = np.zeros(NIter)\n\t\n\t# Simple implementation of gradient descent\n    for n in range(0,NIter):\n\t\t# Computing gradient and updating parameters\n        nabla_b, nabla_W = net_backprop(net,X_train,y_train)\n        for k in range(0,len(net['W'])):\n            net['b'][k+1] = net['b'][k+1] - epsilon*nabla_b[k+1].reshape(net['b'][k+1].shape)\n            net['W'][k+1] = net['W'][k+1] - epsilon*np.transpose(nabla_W[k+1])\n\t\t\t\n\t\t# Computing losses and missed values for the validation set\n        Loss[n] = net_loss(y_train,np.transpose(net['h'][len(net['W'])]))\n        yhat_val = net_predict(net,X_val)\n        Loss_val[n] = net_loss(y_val,yhat_val)\n        missed_val[n] = net_missed(y_val,yhat_val)\n\t\t\n\t\t# Displaying results for the current epoch\n#         print(\"... Epoch {:3d} | Loss_Train: {:.2E} | Loss_Val: {:.2E} | Acc_Val: {:2.2f}\".format(n,Loss[n],Loss_val[n],100-100*(missed_val[n])\/len(yhat_val)))\n\t\n    return Loss, Loss_val, missed_val","bd32b71d":"# Import MNIST data\nfrom sklearn.datasets import fetch_openml\nmnist = fetch_openml('mnist_784',version=1)\n\n# Getting training and testing data:\n# We are setting up just a simple binary classification problem in which we aim to\n# properly classify the number 2.\nX, y_str = mnist[\"data\"], mnist[\"target\"]\ny = np.array([int(int(i)==2) for i in y_str])\nX_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]","1e1fa88c":"# Creating a neural network structure\nnet = net_create([784,100,1])\n\n# Training neural network:\n# Note that since I am not doing any hyper-parameter tuning, I am using the test sets for\n# validation to show how the generalization error changes as the network gets trained. \nLoss,Loss_val,mae_val = net_train(net,X_train,y_train,X_test,y_test,epsilon=1e-6,NIter=300)","eb7cdd5c":"#### Plotting learning curves:\n# Note that we don't observe overfitting here because the model is very simple.\nplt.plot(Loss\/np.max(Loss))\nplt.plot(Loss_val\/np.max(Loss_val))\nplt.legend({'Normalized Training Loss','Normalized Validation Loss'})\nplt.show()","1b886198":"Implementing Sigmoid Prime Function","a163ac54":"Function evaluating the entropy loss","5ec563e1":"Implementing Sigmoid Function","5f140b13":"Function used to initialize the parameters of the neural network","711afee6":"Function used to create the neural network structure. \n\nThe input is a list of parameters corresponding to the length of the input, hidden and output\nThe output is a dictionary that contains all the parameters of the neural network\n","7b6b88f9":"Function used for training of the neural network. It updates the parameters in the neural network and returns the history of the training Loss 'Loss', the validation loss 'Loss_val' and the number of missed prediction in the validation set 'missed_val'.","98a93e60":"Function using backpropagation to compute the gradients of the parameters","81dda36f":"Function to determine the number of mismatches on the prediction","c1c69bcc":"Function used to evaluate the neural network","b23074e3":"# Implementing BackProp and ForwardProp w\/ Numpy\n\nThis script illustrates how backpropagation can be used to train a neural network by setting a simple binary classification problem using the MNIST dataset. You will find the function *net_backprop* in the script *fncs.py*, which requires you to implement the backpropagation algorithm. Note that you will have a few modifications since the gradients need to aggregate (by summing) the contributions from each training sample."}}