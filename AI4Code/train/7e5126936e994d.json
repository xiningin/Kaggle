{"cell_type":{"f436ce19":"code","a8539655":"code","dc071f5b":"code","ffa3bad0":"code","2a4546a4":"code","9bd62327":"code","4bae2cf6":"code","2227371f":"code","275a6d44":"code","2ecb2d2a":"code","5ebb2f52":"code","f17aede7":"code","5067a28f":"code","a5fd9d1e":"code","64f1e22d":"code","6607fabd":"code","c3370a0e":"code","bede949a":"code","a5f6246c":"code","a896b821":"code","10313be5":"code","dfb58332":"code","d68f2430":"code","5e4f052a":"code","e4c0e88d":"code","809380a3":"code","f7f203d2":"code","538013d3":"code","a462907b":"code","e9d9d8ec":"code","c4c1dca9":"code","2ce33673":"code","db3a6f9d":"code","6b96dee9":"code","e543c801":"code","2860bb62":"code","9e29ba50":"code","3eed1f2f":"code","fbe51ee7":"code","d124e1f9":"code","9349dd99":"code","0f55aee1":"code","43503eb4":"code","1bfe4dca":"code","fa0ba8ab":"code","53ed52d1":"code","084de5c0":"code","298a7992":"code","a791924a":"code","e136438f":"code","64a23f54":"code","1ec03056":"code","e5ec79e1":"code","786c2f1b":"code","36a9f750":"code","cf2b5cb0":"code","6f15bc29":"code","a8a202c4":"code","108e156a":"code","5d6fc860":"code","a8c1df36":"markdown","7d58896f":"markdown","29f6ce6e":"markdown","06bfff9e":"markdown","5281aa8d":"markdown","5fd65627":"markdown"},"source":{"f436ce19":"import pandas as pd\nimport numpy as np\nimport dateutil.parser\nimport datetime","a8539655":"### To save from multiple print and display statements, below import\n### will let you see multiple outputs in same cell\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","dc071f5b":"df = pd.read_csv(\"..\/input\/german-credit-data-with-risk\/german_credit_data.csv\", index_col=0)\n\n### below renaming is just to make the data consistent with the one on my local\ndf.rename(columns = {'Checking account': 'Credit History', 'Sex': 'Gender'}, inplace=True)","ffa3bad0":"df.head()","2a4546a4":"df.describe()","9bd62327":"df.shape","4bae2cf6":"df.columns","2227371f":"df.dtypes","275a6d44":"### check NaNs\ndf.isnull().sum()","2ecb2d2a":"feature = 'Gender'\ndf[feature].value_counts()","5ebb2f52":"df[feature].unique()","f17aede7":"df.nunique()","5067a28f":"### show all object type columns\ndf.select_dtypes(include=['object']).columns","a5fd9d1e":"### shallow vs deep copy of a dataframe\n### In deep copy, a new df will be created where any changes to the new df, df_deep will not be reflected in original df\ndf_deep=df.copy(deep=True)\ndf_shallow = df.copy(deep=False)","64f1e22d":"### Let's create a date column\n### Below generates the date for 1000 periods on daily frequency\ndf['date'] = pd.date_range('1\/1\/2000', periods=1000)","6607fabd":"df['date'].dtype","c3370a0e":"df['date'][0]","bede949a":"### As an example, if we want to create a date range at hourly frequency for 5 periods\npd.date_range('1\/1\/2000', freq='H', periods=5)","a5f6246c":"### Passing errors='ignore' will return the input date if the date does not meet the timestamp limitations\n### As per https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/timeseries.html#timeseries-timestamp-limits, \n### the timestamp can be represented using 64 bit in following range: \nprint(\"Minimum date:\", pd.Timestamp.min)\nprint(\"Maximum date:\", pd.Timestamp.max)","a896b821":"### Input date outside the timestamp limitation is outputted as it is, when errors = 'ignore' was passed\npd.to_datetime('2265-04-11', format='%Y-%m-%d', errors = 'ignore')","10313be5":"### Input date outside the timestamp limitation is marked as NaT, when errors = 'coerce' was passed\npd.to_datetime('2265-04-11', format='%Y-%m-%d', errors = 'coerce')","dfb58332":"### takes MM\/DD\/YY format\ndateutil.parser.parse('1\/1\/2000')","d68f2430":"### for cases like below, specify the parameter dayfirst as True\ndateutil.parser.parse('10\/1\/2000', dayfirst=True)","5e4f052a":"### As against default\ndateutil.parser.parse('10\/1\/2000')","e4c0e88d":"### But for case like below with ambiguous date, passing dayfirst is ignored and 13 is assumed as date\ndateutil.parser.parse('10\/13\/2000', dayfirst=True)","809380a3":"### convert a string to datetime\ndatetime.datetime.strptime('18-1-2000','%d-%m-%Y')","f7f203d2":"### https:\/\/strftime.org\/\ntoday =datetime.datetime.today()\nprint(\"today's date:\", today)\ntoday.strftime(\"%d %B %Y\")","538013d3":"### chnaging B (Full month name) to b(abbreviated month name) and Y(in century) to y(without century)\ntoday.strftime(\"%d %b %y\")","a462907b":"df.sort_values(by=['Age'])","e9d9d8ec":"### setting index\nindex_col ='date'\ndf = df.set_index(index_col)","c4c1dca9":"df","2ce33673":"### Now, lets shift the row by 1 \ndf.shift(1)","db3a6f9d":"### Observe the difference by specifying the frequency. Here, the row corresponding to 2000-01-01\n### gets eliminated and data shifts by 1 day, creating the observation at 2002-09-27\ndf.shift(1, freq=\"D\")","6b96dee9":"df.shift(-1)","e543c801":"### filter based on index. Lets index the data based on Age and Duration\ndf_age_duration = df.set_index(['Age', 'Duration'])\ndf_age_duration.head()","2860bb62":"### get level 0 index values, which corresponding to Age and filter the dataframe for Age > 30\ndf_age_duration[df_age_duration.index.get_level_values(0) > 30]","9e29ba50":"### dropping wrt rows and columns\n### how = 'all' drops the rows or columns which have all NaNs","3eed1f2f":"### if any value in a row is NA, then it is dropped\ndf.dropna(axis = 0, how='any')","fbe51ee7":"### if any value in a column are NA, then it is dropped\ndf.dropna(axis = 1, how='any')","d124e1f9":"### lowercase, uppercase, capitalize the column names\n[k.lower() for k in list(df.columns)]\n[k.upper() for k in list(df.columns)]\n[k.capitalize() for k in list(df.columns)]","9349dd99":"df_dup = pd.concat([df, df], axis=0)\ndf_dup","0f55aee1":"### column wise concatenation\ndf_age = df[['Age']]\npd.concat([df, df_age], axis=1)","43503eb4":"df_dup.drop_duplicates()","1bfe4dca":"feature = ['Gender']\ndf.groupby('Gender').size()","fa0ba8ab":"### multiple functions: mean, transform, var, nth(-10), nlargest etc\ndf.groupby(feature).mean()\ndf.groupby(feature).std()","53ed52d1":"df.groupby(feature).first()\ndf.groupby(feature).last()","084de5c0":"df.groupby('Gender').apply(lambda x: x.nlargest(2, 'Credit amount')).reset_index(drop=True)  ","298a7992":"### apply function\ndf.groupby(['Gender'])['Risk'].apply(np.size)","a791924a":"### selecting, dropping, renaming columns","e136438f":"df_dtypes = pd.DataFrame((df.dtypes == 'object'), columns = ['obj_type'])\nselected_cols = list(df_dtypes[(df_dtypes.obj_type == True)].index)\ndf[selected_cols]","64a23f54":"df.columns\ndf.rename(columns = {'Saving accounts': 'richness_quotient'}).columns","1ec03056":"drop_cols = ['Saving accounts']\ndf.drop(columns = drop_cols)","e5ec79e1":"feature = 'Age'\ndf_subset = df[df[feature] >= 30]\nnp.sort(df_subset['Age'].unique())","786c2f1b":"feature = 'Housing'\ndf[df[feature] == 'own'][feature].unique()","36a9f750":"feature = 'Purpose'\ndes_values = ['car', 'education']\ndf[df[feature].isin(des_values)][feature].unique()","cf2b5cb0":"feature = 'Saving accounts'\ndf[~df[feature].isna()]","6f15bc29":"### np.where vs np.select\n### np.where is used where 1 or 2 values are returned, but in case of multiple values , np.select is used","a8a202c4":"criteria = [df['Age'] <30, (df['Age']<45)&(df['Age']>=30), df['Age']>=45]\nassign_vals = [1,2,3]\ndf['Age_binned'] = np.select(criteria, assign_vals)","108e156a":"df.head()","5d6fc860":"### pivot_table, default agg_func is 'mean'\ndf.pivot_table(columns='Housing', values='Credit amount')","a8c1df36":"### Remove duplicates ","7d58896f":"### Preliminary data analysis","29f6ce6e":"### concatenating rows and columns","06bfff9e":"### Key operations","5281aa8d":"### Now, let's groupby and count the instances per 'Gender'","5fd65627":"### datetime conversion"}}