{"cell_type":{"ffb90a21":"code","57e46ca0":"code","0935ca9b":"code","ee81d330":"code","e3baa6c9":"code","29ccb6d8":"code","1f6dba67":"code","eb66f6f5":"code","2ccd05a0":"code","027cc918":"code","141804f4":"code","c204d569":"code","56cacd8e":"code","b9552897":"code","30500f7c":"code","d3d62f41":"code","4a50c82f":"code","217839e6":"code","c01ace66":"code","dca5ee3d":"code","2d653350":"code","2ef4466d":"code","e7a4ae40":"code","079dbd75":"code","c2f8c44c":"code","0fad0e21":"code","ba8fad60":"code","bd3a942e":"code","279a34dd":"code","9fef86ec":"code","db37762f":"code","e37fc88d":"code","81221afd":"code","d1c8c8ee":"code","f22c2dfd":"code","086d0ede":"code","558a0661":"code","79e5766a":"code","1a9a47a9":"code","865d1a57":"code","94a8b6b2":"code","78eb0a9a":"markdown","7600e307":"markdown","0099f1a0":"markdown"},"source":{"ffb90a21":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","57e46ca0":"%matplotlib inline","0935ca9b":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","ee81d330":"t=pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")","e3baa6c9":"t","29ccb6d8":"t.count()","1f6dba67":"plt.subplot(4,2,1)\nsns.kdeplot(t.DiabetesPedigreeFunction)\nplt.subplot(4,2,2)\nsns.kdeplot(t.Pregnancies)\nplt.subplot(4,2,3)\nsns.kdeplot(t.BMI)\nplt.subplot(4,2,4)\nsns.kdeplot(t.Insulin)\nplt.subplot(4,2,5)\nsns.kdeplot(t.Glucose)\nplt.subplot(4,2,6)\nsns.kdeplot(t.BloodPressure)\nplt.subplot(4,2,7)\nsns.kdeplot(t.SkinThickness)\nplt.subplot(4,2,8)\nsns.kdeplot(t.Age)","eb66f6f5":"def replace_0(df,col) :\n    df1 = df.copy()\n    n = df.shape[0]\n    m = df[col].mean()\n    s = df[col].std()\n    for i in range(n) :\n        if df.loc[i,col]==0 :\n            df1.loc[i,col] = np.random.normal(m,s)\n    return df1","2ccd05a0":"t=replace_0(t,'Insulin')","027cc918":"t = replace_0(t,'SkinThickness')","141804f4":"t","c204d569":"plt.subplot(4,2,1)\nsns.kdeplot(t.DiabetesPedigreeFunction)\nplt.subplot(4,2,2)\nsns.kdeplot(t.Pregnancies)\nplt.subplot(4,2,3)\nsns.kdeplot(t.BMI)\nplt.subplot(4,2,4)\nsns.kdeplot(t.Insulin)\nplt.subplot(4,2,5)\nsns.kdeplot(t.Glucose)\nplt.subplot(4,2,6)\nsns.kdeplot(t.BloodPressure)\nplt.subplot(4,2,7)\nsns.kdeplot(t.SkinThickness)\nplt.subplot(4,2,8)\nsns.kdeplot(t.Age)","56cacd8e":"sns.pairplot(t, hue=\"Outcome\")","b9552897":"fig = sns.FacetGrid(t, hue=\"Outcome\", aspect=3) # aspect=3 permet d'allonger le graphique\nfig.map(sns.kdeplot, \"Glucose\", shade=True)\nfig.add_legend()","30500f7c":"from sklearn import preprocessing","d3d62f41":"X = t.drop([\"Outcome\"], axis = 1)\ny= t.Outcome","4a50c82f":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)","217839e6":"print(X_train.shape)\nprint(X_test.shape)","c01ace66":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(max_iter = 300)\nlr.fit(X_train,y_train)\ny_lr=lr.predict(X_test)","dca5ee3d":"from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score","2d653350":"print(confusion_matrix(y_test,y_lr))\n","2ef4466d":"print(accuracy_score(y_test,y_lr))","e7a4ae40":"print(classification_report(y_test, y_lr))","079dbd75":"probas = lr.predict_proba(X_test)\ndfprobas = pd.DataFrame(probas,columns=['proba_0','proba_1'])\ndfprobas['y'] = np.array(y_test)\nplt.figure(figsize=(10,10))\nsns.distplot(1-dfprobas.proba_0[dfprobas.y==0], bins=50)\nsns.distplot(dfprobas.proba_1[dfprobas.y==1], bins=50)","c2f8c44c":"false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,probas[:, 1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint (roc_auc)","0fad0e21":"plt.figure(figsize=(12,12))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')       \nplt.plot([0,0,1],[0,1,1],'g:')     \nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","ba8fad60":"from sklearn import ensemble\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)","bd3a942e":"print(classification_report(y_test,y_rf))","279a34dd":"cm = confusion_matrix(y_test, y_rf)\nprint(cm)","9fef86ec":"rf1 = ensemble.RandomForestClassifier(n_estimators=10, min_samples_leaf=10, max_features=3)\nrf1.fit(X_train, y_train)\ny_rf1 = rf.predict(X_test)\nprint(classification_report(y_test, y_rf1))","db37762f":"from sklearn.model_selection import validation_curve\nparams = np.arange(1, 300,step=30)\ntrain_score, val_score = validation_curve(rf, X, y, 'n_estimators', params, cv=7)\nplt.figure(figsize=(12,12))\nplt.plot(params, np.median(train_score, 1), color='blue', label='training score')\nplt.plot(params, np.median(val_score, 1), color='red', label='validation score')\nplt.legend(loc='best')\nplt.ylim(0, 1)\nplt.xlabel('n_estimators')\nplt.ylabel('score');","e37fc88d":"from sklearn.model_selection import validation_curve\nparams = np.arange(1, 300,step=30)\ntrain_score, val_score = validation_curve(rf, X, y, 'min_samples_leaf', params, cv=7)\nplt.figure(figsize=(12,12))\nplt.plot(params, np.median(train_score, 1), color='blue', label='training score')\nplt.plot(params, np.median(val_score, 1), color='red', label='validation score')\nplt.legend(loc='best')\nplt.ylim(0, 1)\nplt.xlabel('n_estimators')\nplt.ylabel('score');","81221afd":"from sklearn import model_selection\nparam_grid = {\n              'n_estimators': [10, 100, 500],\n              'min_samples_leaf': [1, 20, 50]\n             }\nestimator = ensemble.RandomForestClassifier()\nrf_gs = model_selection.GridSearchCV(estimator, param_grid)","d1c8c8ee":"rf_gs.fit(X_train,y_train)","f22c2dfd":"print(rf_gs.best_params_)","086d0ede":"rf2=rf_gs.best_estimator_","558a0661":"y_rf2=rf2.predict(X_test)","79e5766a":"print(classification_report(y_test,y_rf2))","1a9a47a9":"importances = rf2.feature_importances_\nindices = np.argsort(importances)\nplt.figure(figsize=(8,5))\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), X_train.columns[indices])\nplt.title('Importance des caracteristiques')","865d1a57":"pip install xgboost","94a8b6b2":"import xgboost as XGB\nxgb  = XGB.XGBClassifier()\nxgb.fit(X_train, y_train)\ny_xgb = xgb.predict(X_test)\ncm = confusion_matrix(y_test, y_xgb)\nprint(cm)\nprint(classification_report(y_test, y_xgb))","78eb0a9a":"Toutes les courbes obtenues sont tr\u00e8s similaires pour les deux outcome, la glucose est celle qui pr\u00e9sente la plus grosse diff\u00e9rence","7600e307":"la pr\u00e9cision totale baisse avec la m\u00e9thode xgboost","0099f1a0":"On voit que la colonne insulin contient beaucoup de 0,SkinThickness \u00e9galement, on va donc utiliser la fonction donn\u00e9 en cours pour uniformiser les r\u00e9sultats."}}