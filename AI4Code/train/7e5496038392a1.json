{"cell_type":{"54b1f2a4":"code","29c1cbe8":"code","0f1767e4":"code","fb23d4af":"code","31b6ff47":"code","44639d3a":"code","cab4d5bb":"code","872a91c0":"code","378bc81d":"code","b4e570c0":"code","66ac924e":"code","9358e09e":"code","873dd6a8":"code","1eb6e539":"code","4ae014a0":"code","435d5b58":"code","45120e6c":"code","bbb7b7fb":"code","1b7f4f02":"code","0bd74243":"markdown","0ffb7d4c":"markdown","57fe8f82":"markdown","b15b745c":"markdown","2b8703c5":"markdown","495fad3a":"markdown","adecb41b":"markdown","fc4b5b5c":"markdown"},"source":{"54b1f2a4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","29c1cbe8":"!pip install Sastrawi","0f1767e4":"import pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory","fb23d4af":"# Random seed for consistency\nnp.random.seed(42)\n\nnltk.download('punkt')","31b6ff47":"# Create stemmer\nfactory = StemmerFactory()\nstemmer = factory.create_stemmer()\n\n# Label Encoder use to Encode target labels with value between 0 and n_classes-1\nEncoder = LabelEncoder()\n\n# TfidfVectorizer Convert a collection of raw documents to a matrix of TF-IDF features.\nTfidf_vect = TfidfVectorizer()","44639d3a":"# Read dataset using pandas function read_csv\n\ndata_train = pd.read_csv('..\/input\/indonesiafalsenews\/Data_latih.csv')","cab4d5bb":"# Bentuk data_train\n\ndata_train","872a91c0":"# Jumlah text per-kategori(label)\n\ndata_train['label'].value_counts()","378bc81d":"# Dari cell sebelumnya terlihat jelas bahwa dataset kita sangat tidak balance\n# Untuk membuat datasetnya balance\n\n# Pilih dataset dengan label 1 dan lakukan randomisasi untuk setiap baris datanya\nfalse_news = data_train[data_train['label'] == 1].sample(frac=1)\n\n# Concat dataset berlabel 1 yang telah dipilih dengan dataset berlabel 0\n# dimana jumlah dataset berlabel 1 yang digabungkan sejumlah banyak dataset berlabel 0 + 200\ntrue_fact = data_train[data_train['label'] == 0]\ndf = true_fact.append(false_news[:len(true_fact) + 200])\n\ndf","b4e570c0":"# Kita akan menggunakan fitur narasi saja dalam melakukan prediksi terhadap label\nfeature = df['narasi']\nlabel = df['label']","66ac924e":"# Mengubah semua huruf pada setiap baris menjadi huruf kecil dan\n# melakukan stemming pada setiap baris\nlower = [stemmer.stem(row.lower()) for row in feature]\n\n# Hasil stem dan lower\nlower[:5]","9358e09e":"# Melakukan tokenisasi untuk setiap baris dataset\ntokens = [word_tokenize(element) for element in lower]\n\n# Hasil tokenisasi setiap baris\ntokens[:5]","873dd6a8":"# train_test_split digunakan untuk memecah dataset menjadi 2 bagian\n# X_train dan y_train mewakili data yang akan dilakukan pada fitting model(Training model)\n# X_test dan y_test  mewakili data yang akan dilakukan pada evaluasi model\nX_train, X_test, y_train, y_test = train_test_split(tokens, label, test_size=0.3, stratify=label)","1eb6e539":"# Melihat ukuran data latih dan data uji\nprint('X_train : ', len(X_train))\nprint('X_test : ', len(X_test))","4ae014a0":"# Encoder for Data Label\ny_train = Encoder.fit_transform(y_train)\ny_test = Encoder.fit_transform(y_test)\n\ny_train","435d5b58":"# Fitting dataset terhadap tf-idf\nTfidf_vect.fit([\"\".join(row) for row in X_train])","45120e6c":"# Mentransformasikan hasil fitting terhadap data X_train dan X_test\nX_train_Tfidf = Tfidf_vect.transform([\" \".join(row) for row in X_train])\nX_test_Tfidf = Tfidf_vect.transform([\" \".join(row) for row in X_test])","bbb7b7fb":"# Classifier - Algorithm - SVM\n# fitting\/training datasets pada algoritma SVM(Support Vector Machine)\nSVM = svm.SVC(C=1.0, kernel='linear', degree=1, gamma=\"auto\", verbose=True)\nSVM.fit(X_train_Tfidf, y_train)  # predict the labels on validation dataset\n\n# Menggunakan metrics accuracy untuk melihat performa model\npredictions_SVM = SVM.predict(X_test_Tfidf)\nprint(\"SVM Accuracy Score -> \", accuracy_score(predictions_SVM, y_test)*100)","1b7f4f02":"rf = RandomForestClassifier()\nrf.fit(X_train_Tfidf, y_train)\n\nprediction_rf = rf.predict(X_test_Tfidf)\nprint(\"RandomForest Accuracy Score -> \", accuracy_score(prediction_rf, y_test)*100)","0bd74243":"## Tf-idf (Term Frequency-Inverse document frequency)\n\nTerm Frequency(tf) adalah jumlah kemunculan term","0ffb7d4c":"## Data Preprocessing","57fe8f82":"## Stemmer\nKita dapat menggunakan Stemmer untuk mengurangi jumlah word yang ada pada dataset\n### Contoh Hasil Stem\nPerekonomian -> ekonomi\npertumbuhan -> tumbuh","b15b745c":"## Feature Engineering","2b8703c5":"## Import Dataset\n\n### Feature\nID - ID Data\n\nlabel - 1 for False news, 0 for True fact\n\ntanggal - tanggal narasi ditemukan\n\njudul - judul\/tagline berita yang tersebar\n\nnarasi - isi berita\n\nnama file gambar - file gambar (tidak digunakan pada tutorial ini)\n\n","495fad3a":"#### Dengan Menggunakan Algoritma RandomForest","adecb41b":"## Model Training & Prediction","fc4b5b5c":"#### Dengan menggunakan Algoritma SVM"}}