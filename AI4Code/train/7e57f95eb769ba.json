{"cell_type":{"defdc669":"code","b1727108":"code","9996a5e7":"code","21530f3f":"code","9b1e1582":"code","cd3f2be7":"code","ab9779e1":"code","4c06b1a6":"code","63db074f":"code","b5e32890":"code","d64f1c9b":"code","08d78125":"code","6b642c53":"code","1a693f32":"markdown","65adabf3":"markdown","3c41f627":"markdown"},"source":{"defdc669":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBRegressor\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b1727108":"train = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/sample_submission.csv\")","9996a5e7":"train.head()","21530f3f":"train.columns","9b1e1582":"y = train.target\ntrain.drop(['target', 'id'], axis = 1, inplace = True)\ntest.drop(['id'], axis = 1, inplace = True)\ntrain.head()","cd3f2be7":"test.head()","ab9779e1":"print(\"Train shape: \", train.shape)","4c06b1a6":"print(\"Test shape: \", test.shape)","63db074f":"from sklearn.preprocessing import OrdinalEncoder\ncat_cols = [col for col in train.columns if 'cat' in col]\n\nX = train.copy()\nX_test = test.copy()\nenc = OrdinalEncoder()\nX[cat_cols] = enc.fit_transform(train[cat_cols])\nX_test[cat_cols] = enc.transform(test[cat_cols])\nX.head()","b5e32890":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\n\nmodel_dict = {'n_estimators': 10000,'learning_rate': 0.35,'subsample': 0.926,'colsample_bytree': 0.84,'max_depth': 2,'booster': 'gbtree','reg_lambda': 35.1, 'random_state': 1,'n_jobs': 4}\n                \n# Model hyperparameters\nxgb_params = model_dict","d64f1c9b":"#Setting the kfold parameters\nkf = KFold(n_splits = 10, shuffle = True, random_state = 1)\n\noof_preds = np.zeros((X.shape[0],))\npreds = 0\nmodel_fi = 0\nmean_rmse = 0\n\nfor num, (train_id, valid_id) in enumerate(kf.split(X)):\n    X_train, X_valid = X.loc[train_id], X.loc[valid_id]\n    y_train, y_valid = y.loc[train_id], y.loc[valid_id]\n    \n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_valid, y_valid)],\n             eval_metric = \"rmse\",\n             early_stopping_rounds = 100)\n    \n    #Mean of the predictions\n    preds += model.predict(X_test) \/ 10 # Splits\n    \n    #Mean of feature importance\n    model_fi += model.feature_importances_ \/ 10 #splits\n    \n    #Out of Fold predictions\n    oof_preds[valid_id] = model.predict(X_valid)\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_id]))\n    print(f\"Fold {num} | RMSE: {fold_rmse}\")\n    \n    mean_rmse += fold_rmse \/ 10\n    \nprint(f\"\\noverall RMSE: {mean_rmse}\")","08d78125":"submission.target = preds\nsubmission.head()","6b642c53":"submission.to_csv(\"first_submission.csv\", index = False)\nprint(\"Sent\")","1a693f32":"In this notebook I'm applying what I learned on the lasts days of \"30 Days of ML\" \n# Load and visualize dataset","65adabf3":"# Encoding categorical data","3c41f627":"# Modeling"}}