{"cell_type":{"4c667216":"code","fd9abd18":"code","6e1996bf":"code","fbc15125":"code","b3099187":"code","3d3b4fab":"code","5873157b":"code","ae912f4e":"code","e19ab600":"code","783e5779":"code","ebb73197":"code","b7cb5f00":"code","7d9d9e8a":"code","542f8c5f":"code","646d55f7":"code","b3b6680c":"code","d6500f49":"code","4b309224":"code","86b21ca6":"code","95a21ccf":"code","cd23233a":"code","72a48d31":"code","22a0e205":"code","806fb274":"code","4918e6da":"code","33684763":"code","0951f2ec":"code","8dd6a342":"code","91071352":"code","01a0f089":"code","094762c8":"code","6b4c9ea7":"code","8bb7beae":"code","eda5a8e4":"code","1caf1bfa":"code","c3b8271a":"code","f52067c9":"code","8320a97b":"code","8adda26f":"code","697f7d4b":"code","5de54170":"code","5edb16b6":"code","78d7b57c":"markdown","9515a9c7":"markdown","abf10200":"markdown","eda11042":"markdown","cb5b47f3":"markdown","65be7e37":"markdown","aaf13577":"markdown","35feeca2":"markdown","4a575444":"markdown","430e87de":"markdown","2a62ad71":"markdown","83579d4e":"markdown","98279fa3":"markdown","a3280d1b":"markdown"},"source":{"4c667216":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\n# plotting \nfrom pandas.plotting import lag_plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\ncolorMap = sns.light_palette(\"blue\", as_cmap=True)\n#plt.rcParams.update({'font.size': 12})\nimport os\n#!pip install dabl > \/dev\/null\n# !pip install datatable > \/dev\/null\n#import dabl\nimport datatable as dt\nimport missingno as msno\nimport warnings\nwarnings.filterwarnings('ignore')\n# for the image import\nimport os\nfrom IPython.display import Image\n# garbage collector to keep RAM in check\nimport gc\nfrom tqdm import tqdm\nfrom scipy import stats\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport xgboost as xgb\nimport optuna","fd9abd18":"# List Of Files\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6e1996bf":"%%time\n\ns = !wc -l {'\/kaggle\/input\/jane-street-market-prediction\/train.csv'}\nn_train_rows = int(s[0].split(' ')[0])+1\nprint(\"Total Number Of Rows: \", n_train_rows)\ndel s\ndel n_train_rows","fbc15125":"# %%time\n# Data Load For Analysis\n# For Faster load, use datatable(R) which is faster than pandas\ntrain_data_datatable = dt.fread('..\/input\/jane-street-market-prediction\/train.csv')\ntrain_data = train_data_datatable.to_pandas()\nprint(train_data.info())\n# Delete unnecessary variable in order to free more space from RAM\ndel train_data_datatable","b3099187":"train_data.head(2)","3d3b4fab":"print(train_data.columns.to_list())","5873157b":"print(\"Total Unique Date : \",train_data['date'].nunique())\nprint(\"Row Count\", train_data.shape)\nprint('-'*30)\nprint(\"Column Type : \")\nprint(f\"{train_data.dtypes.value_counts()}\")","ae912f4e":"# # Missing Data Percentage Count\n# n_features = 50\n# missing_values_count = train_data.isnull().sum()[train_data.isna().sum() > 0].sort_values(ascending=False)\n# # print (missing_values_count)\n# total_cells = np.product(train_data.shape)\n# total_missing = missing_values_count.sum()\n# print('NAN Valued Columns: %d' %train_data.isna().any().sum())\n# print (\"Missing data = \",str(round((total_missing\/total_cells) * 100, 2))+'%')\n\n# fig, axs = plt.subplots(figsize=(10, 10))\n\n# sns.barplot(y = missing_values_count.index[0:n_features], \n#             x = missing_values_count.values[0:n_features], \n#             alpha = 1.0\n#            )\n\n# plt.title(f'NaN values of train dataset (Top {n_features})')\n# plt.xlabel('NaN values')\n# fig.savefig(f'nan_values_top_{n_features}_features.png')\n# plt.show()\n\n# # Delete unnecessary variable in order to free more space from RAM\n# del missing_values_count\n# del total_cells\n# del total_missing","e19ab600":"# Check if there are any unnecessary variable or not\n#%whos","783e5779":"# plt.figure(1, figsize = (15 , 10))\n# j=0\n# resp_list = ['resp','resp_1','resp_2','resp_3','resp_4']\n# for i in [[x,y] for x in [0,1,2] for y in [0,1]]:\n    \n#     if i == [0,0]:\n#         train_data['trend'] = pd.Series(train_data[resp_list[j]]).cumsum()                                                              \n#         train_data['weighted_trend'] = pd.Series(train_data['weight'] * train_data[resp_list[j]]).cumsum()\n#         plt.subplot2grid((3 , 2) , (0, 0), colspan=2)\n#         plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n#         plt.plot(train_data['ts_id'], train_data['trend'])\n#         plt.plot(train_data['ts_id'], train_data['weighted_trend'])\n#         gc.collect();\n#         # Delete New Created Columns \n#         train_data = train_data.drop(columns=['trend', 'weighted_trend'], axis=1)\n        \n#         j=j+1\n#     elif i == [0,1]: \n#         pass\n#     else:\n#         train_data['trend'] = pd.Series(train_data[resp_list[j]]).cumsum()                                                              \n#         train_data['weighted_trend'] = pd.Series(train_data['weight'] * train_data[resp_list[j]]).cumsum()\n#         plt.subplot2grid((3 , 2) , (i[0], i[1]), colspan=1)\n#         plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n#         plt.plot(train_data['ts_id'], train_data['trend'])\n#         plt.plot(train_data['ts_id'], train_data['weighted_trend'])\n#         gc.collect();\n#         # Delete New Created Columns \n#         train_data = train_data.drop(columns=['trend', 'weighted_trend'], axis=1)\n#         j = j+1\n# plt.tight_layout()\n# plt.show()","ebb73197":"# fig = plt.figure(figsize=(16,6))\n# ax = plt.subplot(1,1,1)\n# train_data.groupby('date')[['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']].sum().cumsum().plot(ax=ax)\n# plt.title('Cumulative Sum of Different RESP\\'s',fontsize=18)\n# plt.xlabel('Date',fontsize=14)\n# plt.axvspan(0,85,linestyle=':',linewidth=2,label='first 92 days',color='darkorange',alpha=.2)\n# plt.legend(fontsize=12,ncol=3,loc='lower right');","b7cb5f00":"# train_data['weight_resp']   = train_data['weight']*train_data['resp']\n# train_data['weight_resp_1'] = train_data['weight']*train_data['resp_1']\n# train_data['weight_resp_2'] = train_data['weight']*train_data['resp_2']\n# train_data['weight_resp_3'] = train_data['weight']*train_data['resp_3']\n# train_data['weight_resp_4'] = train_data['weight']*train_data['resp_4']\n\n# fig, ax = plt.subplots(figsize=(15, 5))\n# resp    = pd.Series(1+(train_data.groupby('date')['weight_resp'].mean())).cumprod()\n# resp_1  = pd.Series(1+(train_data.groupby('date')['weight_resp_1'].mean())).cumprod()\n# resp_2  = pd.Series(1+(train_data.groupby('date')['weight_resp_2'].mean())).cumprod()\n# resp_3  = pd.Series(1+(train_data.groupby('date')['weight_resp_3'].mean())).cumprod()\n# resp_4  = pd.Series(1+(train_data.groupby('date')['weight_resp_4'].mean())).cumprod()\n# ax.set_xlabel (\"Day\", fontsize=18)\n# ax.set_title (\"Cumulative daily return for resp and time horizons 1, 2, 3, and 4 (500 days)\", fontsize=18)\n# resp.plot(lw=3, label='resp x weight')\n# resp_1.plot(lw=3, label='resp_1 x weight')\n# resp_2.plot(lw=3, label='resp_2 x weight')\n# resp_3.plot(lw=3, label='resp_3 x weight')\n# resp_4.plot(lw=3, label='resp_4 x weight')\n# # day 85 marker\n# ax.axvline(x=85, linestyle='--', alpha=0.3, c='red', lw=1)\n# ax.axvspan(0, 85 , color=sns.xkcd_rgb['grey'], alpha=0.1)\n# plt.legend(loc=\"lower left\");\n# # Delete Unnecessary Variables\n# train_data = train_data.drop(columns=['weight_resp','weight_resp_1','weight_resp_2', 'weight_resp_3','weight_resp_4'], axis=1)\n# del resp, resp_1, resp_2, resp_3, resp_4","7d9d9e8a":"# fig = px.line(train_data.groupby('date')[['resp_1', 'resp_2', 'resp_3', 'resp_4','resp']].mean(),\n#               x= train_data.groupby('date')[['resp_1', 'resp_2', 'resp_3', 'resp_4','resp']].mean().index,\n#               y= ['resp_1', 'resp_2', 'resp_3', 'resp_4','resp'],\n#               title= 'Average Resp per day')\n# fig.layout.xaxis.title = 'Day' \n# fig.layout.yaxis.title = 'Avg Resp'\n# fig.show()","542f8c5f":"def highlight(x):\n    df = x.copy()\n    df.loc[:,:] = '' \n    df.iloc[::,2] = 'background-color: yellow'\n    return df\n\ntemp = train_data[['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']].describe().T\ntemp.style.apply(highlight, axis = None)","646d55f7":"# sns.pairplot(train_data[['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']], corner=True )","b3b6680c":"# fig = px.area(data_frame= train_data.groupby('date')[['resp']].count(),title='Number of operation per day')\n# fig.update_traces(showlegend = False)\n# fig.layout.xaxis.title = 'Day' \n# fig.layout.yaxis.title = 'Number of operations'\n# fig.show()","d6500f49":"# fig = px.area(data_frame= train_data.groupby('date')[['resp']].sum(),title='Resp sum of operation per day')\n# fig.update_traces( showlegend = False)\n# fig.layout.xaxis.title = 'Day' \n# fig.layout.yaxis.title = 'Resp sum'\n# fig.show()","4b309224":"# plt.figure(figsize = (12,5))\n# ax = sns.distplot(train_data['resp'], \n#              bins=3000, \n#              kde_kws={\"clip\":(-0.05,0.05)}, \n#              hist_kws={\"range\":(-0.05,0.05)},\n#              color='darkcyan', \n#              kde=False);\n# values = np.array([rec.get_height() for rec in ax.patches])\n# norm = plt.Normalize(values.min(), values.max())\n# colors = plt.cm.jet(norm(values))\n# for rec, col in zip(ax.patches, colors):\n#     rec.set_color(col)\n# plt.xlabel(\"Histogram of the resp values\", size=14)\n# plt.show();\n# gc.collect();","86b21ca6":"print('The minimum value for resp is: %.5f' % train_data['resp'].min())\nprint('The maximum value for resp is:  %.5f' % train_data['resp'].max())\nprint(\"Skew of resp is: %.2f\" %train_data['resp'].skew() )\nprint(\"Kurtosis of resp is: %.2f\"  %train_data['resp'].kurtosis() )","95a21ccf":"print('Percentage of zero weights is: %.2f' % (train_data[train_data['weight']==0]['weight'].count()\/train_data.shape[0]*100) +\"%\")\nprint('The minimum weight is: %.2f' % train_data['weight'].min())\nprint('The maximum weight is: %.2f' % train_data['weight'].max())","cd23233a":"# date = 0\n# n_features = 130\n\n# cols = [f'feature_{i}' for i in range(1, n_features)]\n# hist = px.histogram(\n#     train_data[train_data[\"date\"] == date], \n#     x=cols, \n#     animation_frame='variable', \n#     range_y=[0, 600], \n#     range_x=[-7, 7]\n# )\n\n# hist.show()","72a48d31":"print(train_data.shape)","22a0e205":"# Replace NaN Value with Mean Value\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputed_train_df = pd.DataFrame(imputer.fit_transform(train_data))\n\nimputed_train_df.columns = train_data.columns\nimputed_train_df.index = train_data.index\n\nprint(f\"Column With NaN Value : {imputed_train_df.isna().any().sum()}\")\ndel train_data, imputer","806fb274":"imputed_train_df.head()","4918e6da":"# # Remove Duplicate Value\n# imputed_train_df.drop_duplicates(keep=False, inplace=True)\n# print(imputed_train_df.shape)","33684763":"# Calculating the Z score : remove outliers\n\nthreshold = 4\n\nz = np.abs(stats.zscore(imputed_train_df, nan_policy='omit'))\nimputed_train_df = imputed_train_df[(z < threshold).all(axis=1)].reset_index(drop=True)\n\ndel z","0951f2ec":"imputed_train_df['action'] = (imputed_train_df['resp'] > 0 ).astype('int')","8dd6a342":"# Check target class is balanced or unbalanced in the training data\n\nsns.countplot(x=\"action\", data = imputed_train_df)","91071352":"X = imputed_train_df.loc[:, imputed_train_df.columns.str.contains('feature')]\ny = imputed_train_df.loc[:, 'action']\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\ndel imputed_train_df\n# del X,y","01a0f089":"dtrain = xgb.DMatrix(x_train, label=y_train)\ndtest  = xgb.DMatrix(x_test, label=y_test)\ndel x_train\ndel x_test","094762c8":"def objective(trial):\n    \n# params specifies the XGBoost hyperparameters to be tuned\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 400, 600),\n        'max_depth': trial.suggest_int('max_depth', 10, 20),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.01, .1),\n        'subsample': trial.suggest_uniform('subsample', 0.50, 1),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.50, 1),\n        'gamma': trial.suggest_int('gamma', 0, 10),\n        'tree_method' : 'gpu_hist',\n        'objective': 'binary:logistic'\n    }\n    \n    bst = xgb.train(params, dtrain)\n    preds = bst.predict(dtest)\n    pred_labels = np.rint(preds)\n    accuracy = sklearn.metrics.accuracy_score(y_test, pred_labels)\n    return accuracy","6b4c9ea7":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective,n_trials=5)\ndel dtrain, dtest","8bb7beae":"fig = optuna.visualization.plot_optimization_history(study)\nfig.show();","eda5a8e4":"fig = optuna.visualization.plot_param_importances(study)\nfig.show();","1caf1bfa":"print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))","c3b8271a":"best_params = study.best_trial.params\n# best_params['gpu_id'] = 0\nbest_params['tree_method'] = 'gpu_hist'\nbest_params['objective'] = 'binary:logistic'\n\ndel study","f52067c9":"# Fit the XGBoost classifier with optimal hyperparameters\nclf = xgb.XGBClassifier(**best_params)","8320a97b":"%time clf.fit(X, y)  #Used the whole training data","8adda26f":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test()","697f7d4b":"# th = 0.5\n# for (test_df, pred_df) in tqdm(env.iter_test()):\n#     if test_df['weight'].item() > 0:\n#         x_tt = test_df.loc[:, test_df.columns.str.contains('feature')].values\n#         if np.isnan(x_tt[:, 1:].sum()):\n#             x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:])\n#         pred = clf.predict(x_tt)\n#         pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n#     else:\n#         pred_df.action = 0\n#     env.predict(pred_df)","5de54170":"th = 0.5\nfor (test_df, pred_df) in tqdm(iter_test):\n    if test_df['weight'].item() > 0:\n        X_test = test_df.loc[:, test_df.columns.str.contains('feature')]\n        y_preds = clf.predict(X_test)\n        pred_df.action = np.where(y_preds >= th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","5edb16b6":"pred_df.head()","78d7b57c":"#### RESP & Weight EDA","9515a9c7":"### Data Load Into Dataframe\n1. Training data is very large in size (almost 2GB+ )\n2. DataTable is more faster than pandas dataframe","abf10200":"* The overall market is going up\n* The weighted returns are trending downwards so you would want to predict fewer ones for this time period.","eda11042":"* more gains in the first 92 days :->  it may be a good idea to drop the observations before this point,\n* resp_4 has the highest cumulative sum.\n* resp_1 has the smallest cumulative sum.","cb5b47f3":"### Investment Time Horizon\n*An Investment Time Horizon is the period where one expects to hold an investment for a specific goal. Investments are generally broken down into two main categories: stocks (riskier) and bonds (less risky). The longer the Time Horizon, the more aggressive, or riskier portfolio, an investor can build. The shorter the Time Horizon, the more conservative, or less risky, the investor may want to adopt.*","65be7e37":"* Training data has 130 attributes\/feature.\n* Total unique date 500.\n* The date seem to contain 2 years of trading data since the trading days of the year are approximately 252 : 253 days","aaf13577":"* from standard deviation we can assume that **resp** is more related to longer time horizon invest as longer time horizon are associated with more return and higher risk","35feeca2":"### Data Cleaning\n##### Missing Value","4a575444":"### Basic Exploratory Data Analysis (EDA)","430e87de":"* **resp** is highly related to **resp_4 **\n* **resp_1** and **resp_2** are highly relted","2a62ad71":"* We can see that the shortest time horizons, resp_1, resp_2 and resp_3, representing a more conservative strategy, result in the lowest return.","83579d4e":"* Lot's of null value(almost 2%), may need to remove or replace with other value","98279fa3":"Leptokartic Kurtosis (as kurtosis > 3)","a3280d1b":"* Each trade has an associated weight and resp, which together represents a return on the trade. \n* Trades with weight = 0 were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation."}}