{"cell_type":{"cc6307f9":"code","1637e85c":"code","c97feb26":"code","babe880d":"code","c0598eb9":"code","f49fe245":"code","c14fbe01":"code","facf113c":"code","51037bc6":"code","028864f3":"code","10d73780":"code","3df3ee3d":"code","2b9ffeb5":"code","d89ae2df":"code","d6a2f4cd":"code","db7e6228":"code","90988f1d":"code","f8d342dd":"code","f2b65cb5":"code","2f4eb224":"code","6927386e":"code","e01d875f":"code","1a2b4ae2":"markdown","bf306403":"markdown","c70755af":"markdown","f504c2fc":"markdown","7f02dfe9":"markdown","f467cfbe":"markdown","a2d02b30":"markdown"},"source":{"cc6307f9":"import pandas as pd\nimport numpy as np\n\n# Deep Learning Imports\nimport tensorflow as tf\nimport random\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import regularizers\n\n# Imports for Pre-processing of Data\nimport re\nimport nltk\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer","1637e85c":"df_train = pd.read_csv('\/kaggle\/input\/student-shopee-code-league-sentiment-analysis\/train.csv', index_col='review_id')\nprint(len(df_train))\ndf_test = pd.read_csv('\/kaggle\/input\/student-shopee-code-league-sentiment-analysis\/test.csv', index_col='review_id')\nprint(len(df_test))","c97feb26":"test_labelled = pd.read_csv('\/kaggle\/input\/test-labelled\/test_labelled.csv', index_col='review_id')\nlen(test_labelled)","babe880d":"comparison_df = df_test.merge(test_labelled, indicator=True, how='outer')\nadd_data = comparison_df[comparison_df['_merge'] == 'right_only']\nadd_data = add_data[['review', 'rating']]\ndf_train = df_train.append(add_data, ignore_index=True)\nlen(df_train)","c0598eb9":"df_train = df_train.sample(frac=1) # shuffle training set\nsentences = list(df_train['review'])\nlabels = list(df_train['rating'])\ndf_train.head()","f49fe245":"def process_sentences(sentences):\n    \n    sentences_clean = []\n    \n    for sentence in sentences:\n\n        stemmer = PorterStemmer()\n        stopwords_english = stopwords.words('english')\n        # tokenize sentence\n        tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                                   reduce_len=True)\n        tokens = tokenizer.tokenize(sentence)\n\n        sentence_clean = []\n        for word in tokens:\n            if (word not in stopwords_english and  # remove stopwords\n                    word not in string.punctuation):  # remove punctuation\n                stem_word = stemmer.stem(word)  # stemming word\n                sentence_clean.append(stem_word)\n                \n        sentences_clean.append(sentence_clean)\n\n    return sentences_clean","c14fbe01":"processed_sentences = process_sentences(sentences)","facf113c":"len(processed_sentences)","51037bc6":"processed_sentences[1]","028864f3":"len(max(processed_sentences))","10d73780":"embedding_dim = 300\nmax_length = 13\noov_tok = \"<OOV>\"\ntraining_size = len(sentences)\ntraining_portion = 0.75","3df3ee3d":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(processed_sentences)\n\nword_index = tokenizer.word_index\nvocab_size = len(word_index)\n\nsequences = tokenizer.texts_to_sequences(processed_sentences)\npadded = pad_sequences(sequences, maxlen = max_length, padding = 'post',\n                       truncating = 'post')\n\n# Split Train \/ (val + test)\nsplit_train = int(training_portion * training_size)\nval_sequences = padded[split_train:]\ntraining_sequences = padded[:split_train]\nval_labels = np.array(labels[split_train:])\ntraining_labels = np.array(labels[:split_train])","2b9ffeb5":"print(len(training_sequences))\nprint(len(val_sequences))","d89ae2df":"embeddings_index = {};\nwith open('\/kaggle\/input\/glove6b300dtxt\/glove.6B.300d.txt', encoding='utf8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector","d6a2f4cd":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim,\n                              input_length=max_length,\n                              weights=[embeddings_matrix],\n                              trainable=False),\n    tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='relu'),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, dropout=0.1, recurrent_dropout=0.2, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(6, activation='softmax')\n                                  ])\n\noptimizer = tf.keras.optimizers.Adam(lr=2e-3)\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])","db7e6228":"model.summary()","90988f1d":"#----- to save your weights if you choose to ----#\n#checkpoint_path = '\/kaggle\/working\/sentiment_weights.ckpt'\n#checkpoint_dir = os.path.dirname(checkpoint_path)\n#cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n#                                                  save_weights_only=True,\n#                                                  verbose=1)","f8d342dd":"# load any existing weights\n#model.load_weights('')","f2b65cb5":"history = model.fit(training_sequences, training_labels, epochs=1,\n                    validation_data=(val_sequences, val_labels), \n                    verbose=1 #callbacks = [cp_callback])","2f4eb224":"test_sentiments = list(df_test.review)\nprocessed_test = process_sentences(test_sentiments)\nlen(max(processed_test))\n\nsequences_test = tokenizer.texts_to_sequences(processed_test)\npadded_test = pad_sequences(sequences_test, maxlen = max_length, padding = 'post',\n                       truncating = 'post')\npadded_test","6927386e":"predictions = model.predict(padded_test)\npreds = predictions.argmax(axis=-1)\npreds","e01d875f":"df_test['rating'] = preds\nsubmission = df_test[['rating']].reset_index()\nsubmission","1a2b4ae2":"# Predicting Test Set + Submission","bf306403":"## Pre-Trained Embeddings\n- Using the gloVe embeddings from Stanford","c70755af":"#### Pre-processing Test set","f504c2fc":"# Additional Data (thanks to Liuhh)","7f02dfe9":"# Pre-processing of reviews","f467cfbe":"#### Predicting Test Set","a2d02b30":"## Simple Model Building + Training"}}