{"cell_type":{"f8268e0e":"code","85d3711c":"code","fdd7d5c0":"code","260db965":"code","587d1f50":"code","3bd9355f":"code","d740235b":"code","6343a23f":"code","b07673d8":"code","a4e5d800":"code","c2fd9357":"code","030e5124":"code","b7597493":"code","7b060f33":"code","6d628afb":"code","cd1c1b9c":"code","c0d756c9":"code","09f91aec":"code","d7085d5d":"code","a46883bd":"code","4a364294":"code","3c5fbda4":"code","a7dbd620":"markdown","a1f9d945":"markdown","14f241f5":"markdown","c6294a14":"markdown","5cb5e59f":"markdown","32ac3edd":"markdown","37e79481":"markdown","de0304d4":"markdown","0839b06f":"markdown","1c78e36a":"markdown","d309ce32":"markdown","36db9147":"markdown","68ebd0da":"markdown","d485f88b":"markdown","f1574832":"markdown"},"source":{"f8268e0e":"# standard Python tools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as ss   # need this for chi-squared function\n\n# special tools for working in Kaggle\nimport joblib   # save and load ML models\nimport gc       # garbage collection\nimport os \n\n# preprocessing steps\nfrom sklearn import preprocessing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n# machine learning models and tools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# surely there will be a lot more packages loaded by the time we are done!","85d3711c":"MainDir = \"..\/input\/..\/input\/home-credit-default-risk\"\nprint(os.listdir(MainDir))\n\n# Main table\ntrain = pd.read_csv(f'{MainDir}\/application_train.csv')\n\n# Supplemental data - we can create additional feature sets by analyzing these.\nbureau = pd.read_csv(f'{MainDir}\/bureau_balance.csv')\ncc = pd.read_csv(f'{MainDir}\/credit_card_balance.csv')\n# and so on - not going to worry about these just yet","fdd7d5c0":"# bureau - separate data set with history for each customer. Let's make a crosstable and then \n# left-join it into the training data.\n\n#bureau.shape           # 27299925, 3\n#bureau.nunique()       # 817,395 unique customers, 8 unique statuses\nbureau_table = pd.crosstab(bureau.SK_ID_BUREAU, bureau.STATUS)\nbureau_table.head()","260db965":"# cc - credit card data. Let's get average balance by customer\n#cc.shape           # 3840312, 23\n#cc.nunique()       \ncc_table = cc.groupby(['SK_ID_CURR']).agg(np.mean)['AMT_BALANCE'].to_frame()\ncc_table.head()\n","587d1f50":"print('Shape of training data:', train.shape)\npd.set_option(\"display.max_columns\", None)            # makes it scrollable horizontally instead of suppressing columns\ntrain.head(5)\n# over 300,000 records in the training set, and 121 features (plus whatever other features we end up importing\n# from the supplemental tables.) Target variable is \"TARGET.\"","3bd9355f":"# Proportion - about 91.9% are zero (not default) and 8.1% are one (default.)\n# Project is being scored as AUC (area under curve) i.e. confidence matters. The best scores of all time are around 80-81%\n# so you can't score high by making a model that naively guesses that everything is a no-default.\n(train['TARGET'].value_counts() \/ len(train)).to_frame()","d740235b":"train.dtypes.value_counts()\n# There are 16 categorical variables in our model.\n# We should also take a look at the 41 integer variables - some could be counting statistics (e.g. family size) but others could be integer-encoded categorical variables","6343a23f":"train.select_dtypes('object').apply(pd.Series.nunique, axis = 0).to_frame()\n# These are the sixteen categorical variables: there would be 140 dummy variables in our model if we one-hot encoded all of these.","b07673d8":"train.isna().sum().to_frame().sort_values(0, ascending = False).head(50)\n# Lots of variables have missing values. We need to come up with a strategy for imputing missing values.","a4e5d800":"# Let's create a sample of 1000 rows from the training data, so that these graphics can render in a reasonable amount of time.\ntrain_1K = train.sample(n=1000, random_state=1)","c2fd9357":"# Income by occupation type\nfig, ax = plt.subplots(figsize=(15, 6))\nax = sns.boxplot(y = \"OCCUPATION_TYPE\", x = \"AMT_INCOME_TOTAL\", orient = \"h\", data = train_1K)\nplt.xlim([0, 1e6])\nplt.show()","030e5124":"# I like two-sided violin plots for categorical classification problems. They can help you see whether different groups\n# have different sensitivities.\n\nfig, ax = plt.subplots(figsize=(12, 6))\nax = sns.violinplot(x=\"WEEKDAY_APPR_PROCESS_START\", y=\"AMT_CREDIT\", hue = \"TARGET\", split = True, data=train_1K)\nplt.show()","b7597493":"# Pairplot - I just picked a few continuous variables to show. Blue = 0 (not default), orange = 1 (default)\nNumRows = train_1K.iloc[:,[1, 7,8,9]]\nax = plt.figure(figsize = (8, 8))\nax=sns.pairplot(NumRows, hue = \"TARGET\", plot_kws={'s':20})\nplt.tight_layout()\nplt.show()","7b060f33":"correl = train.corr()\nfig, ax = plt.subplots(figsize=(24, 24))\nsns.heatmap(correl, annot = False, cmap = \"BuPu\", label = 'small', cbar = False)\nax.set_title('Correlation Matrix'); \nplt.show()\n\n# Big ol' correlation matrix shows that there are some highly correlated variables. This data set could be\n# a candidate for feature reduction using Principal Component Analysis.","6d628afb":"# https:\/\/towardsdatascience.com\/the-search-for-categorical-correlation-a1cf7f1888c9\n\n# Cramer's V (\u03c6) is similar to Pearson's R (correlation coefficient) but it works with categorical data.\n# While R has a range from -1 to +1, V has a range of 0 to +1. We are going to use this to build a\n# heatmap that will help us evaluate whether the categorical variables are independent.\n\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))","cd1c1b9c":"# I don't see a big opportunity for feature reduction here, but it was worth taking a look. And the heatmap looks cool.\n\nCatFeatures = ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_TYPE_SUITE', \n               'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'WEEKDAY_APPR_PROCESS_START', \n               'OCCUPATION_TYPE', 'ORGANIZATION_TYPE', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE']\n\nLL = len(CatFeatures)\ncramers_outputs = []\nfor x in range(0,LL):\n    first = train.loc[:,CatFeatures[x]]\n    for y in range(0,LL):\n        second = train.loc[:,CatFeatures[y]]\n        result = round(cramers_v(first,second),4)\n        cramers_outputs.append(result.tolist())\narray = np.array(cramers_outputs)\nreshaped = array.reshape(LL,LL)\n\nfig = plt.figure(figsize = (8, 8))  # instanciate figure for heat map\nax = sns.heatmap(reshaped, annot = True,  cmap = \"BuPu\", fmt=\".0%\", cbar = False)\nax.set_xticklabels(CatFeatures)\nax.set_yticklabels(CatFeatures)\nax.tick_params(axis = 'x', labelrotation = 90)\nax.tick_params(axis = 'y', labelrotation = 0)\nax.set_title(\"Heatmap of Cramer's V on categorical variables\");","c0d756c9":"import plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\ndef DisplayBreakdown(vars) :\n    df_nb = []\n    df_cols = []\n    for var in vars :\n        df_nb.append(pd.crosstab(train_1K['TARGET'], train_1K[var], normalize='index').reset_index())\n        df_cols.append(train_1K[var].unique())\n\n    for idx in range(len(df_nb)) :\n        fig = px.bar(df_nb[idx], y='TARGET', x=df_cols[idx], orientation='h')\n        fig.update_layout(height=275, width=800, xaxis_tickformat = '.0%', title_text=vars[idx], legend_title='', xaxis_title='', yaxis_type='category', legend=dict(orientation='h'))\n        fig.show()\n\nDisplayBreakdown(['NAME_EDUCATION_TYPE', 'NAME_HOUSING_TYPE', 'NAME_FAMILY_STATUS', 'WEEKDAY_APPR_PROCESS_START'])","09f91aec":"# from sklearn.preprocessing import StandardScaler    <- already loaded\nfrom sklearn.decomposition import PCA                 \n\nPCAFeatures = train_1K.iloc[:,np.r_[41:85]]\nPCAFeatures.replace(np.nan,0, inplace = True)\nPCAFeaturesScaled = preprocessing.scale(PCAFeatures)\n\n# display(round(pd.DataFrame(PCAFeaturesScaled).describe(),4))  # check to see if these columns have mean 0 and std 1: looks good\n\npca = PCA()\npca.fit(PCAFeaturesScaled)\npca_data = pca.transform(PCAFeaturesScaled)\n\n# Plot component importance - maybe keep the top 10 or so?\nfig = plt.figure(figsize = (22, 3))\npcvar = np.round(pca.explained_variance_ratio_ * 100,1)\nlabels = ['PC'+str(x) for x in range(1,len(pcvar)+1)]\nplt.bar(x=range(1,len(pcvar)+1), height = pcvar, tick_label = labels)\nplt.ylabel('%age of explained variance')\nplt.xlabel('Principal Component')\nplt.title('PCA Components')\nplt.show()","d7085d5d":"# PCA data sets are completely uncorrelated\npca_df = pd.DataFrame(pca_data, columns = labels)\nfig = plt.figure(figsize = (26,10))\nsns.heatmap(pca_df.corr(), cmap = 'BuPu', annot = True, fmt=\".0%\")\nplt.title('Correlation Heatmap for principal components')\nplt.show()","a46883bd":"pca_df = pd.DataFrame(pca_data, columns = labels)\npca_df['TARGET'] = train_1K.TARGET\n\npca_df.head(5)\n\n# Scatterplot of PC1 and PC2 components.\n#Yes = pca_df.loc[(pca_df.TARGET == 1),:]\n#No = pca_df.loc[(pca_df.TARGET == 0),:]\n#plt.scatter('PC1','PC2', data = No, s=4, label = '0')\n#plt.scatter('PC1','PC2', data = Yes, s=4, label = '1')\n#plt.xlabel('PC1')\n#plt.ylabel('PC2')\n#plt.title('Scatter Plot - LeftUnion by PC1 and PC2')\n#plt.legend()\n\n#plt.show()\n","4a364294":"# set up table\nresults = pd.DataFrame(columns = ['Model Type','Accuracy','Hyperparameters'])\n\n# each time you run a model, run this code\nresults = results.append({'Model Type' : 'Logistic Regression',                              # logistic regression, random forest, etc\n                          'Accuracy' : 0.7243,                                               # variable that contains best model run\n                          'Hyperparameters' : \"{'max_depth': 9, 'min_samples_leaf': 1}\"},    # variable that contains hyperparameters from best model run\n                        ignore_index=True)    \nresults","3c5fbda4":"# Description table contains characters that are unprintable with UTF8 encoding, so we need to open it this way:\n\nwith open(f'{MainDir}\/HomeCredit_columns_description.csv', 'r', encoding = 'ISO-8859-1') as csvfile:\n    desc = pd.read_csv(csvfile)\npd.set_option(\"display.max_rows\", None)               # print entire thing, not just first and last rows\npd.options.display.max_colwidth = 100                 # description column\ndesc","a7dbd620":"### Some barplots","a1f9d945":"### Some Visualizations","14f241f5":"### Import packages","c6294a14":"# Appendix - data descriptions","5cb5e59f":"# First look at training data set","32ac3edd":"### How many categorical variables do we have, and how many levels in each?","37e79481":"### Read the training data","de0304d4":"### Missing Data","0839b06f":"### Correlations","1c78e36a":"### Proportion Table for target variable","d309ce32":"### View the training data set","36db9147":"### Principal Component Analysis","68ebd0da":"### Categorical association - Cramer's V","d485f88b":"## Home Credit Default Risk - Team 3 (Kahsai, Nichols, Pellerito)","f1574832":"### Build Model Scoreboard"}}