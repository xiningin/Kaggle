{"cell_type":{"0c7c5a1c":"code","a5d2d46e":"code","b965d62e":"code","5099c2ca":"code","6817a67e":"code","03b3ab5f":"code","35694e30":"code","f4552a81":"code","00b0360d":"code","400b85d2":"code","d1f44daf":"code","961004f9":"markdown"},"source":{"0c7c5a1c":"### with no internet\n# !pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ..\/input\/nvidia-apex\/apex \n### with internet\n!git clone https:\/\/github.com\/NVIDIA\/apex\n!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .\/apex","a5d2d46e":"import os\nimport re\nimport string\nimport random\nimport numpy as np\nimport pandas as pd\nimport transformers\nfrom transformers import *\nimport tokenizers\nfrom tqdm.autonotebook import tqdm\nfrom apex import amp\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils import data\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.autograd import Variable\n\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold, train_test_split\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\nSEED = 14\ndef seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False\nseed_all(SEED)","b965d62e":"max_len = 112 # No tweet is longer than 108\ntrain_batch_size = 64\nvalid_batch_size = 16\nepochs = 5\nbert_path = '..\/input\/bert-base-uncased\/vocab.txt'\nmodle_path = \"bertbaseuncased.bin\"\ntrain_file = \"..\/input\/tweet-sentiment-extraction\/train.csv\"\ntest_file = \"..\/input\/tweet-sentiment-extraction\/test.csv\"\ntokenizer = tokenizers.BertWordPieceTokenizer(bert_path,lowercase=True)","5099c2ca":"class AverageMeter:\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\ndef loss_fn(o1, o2, t1, t2):\n    # start_logits, end_logits, target_start, target_end\n    l1 = nn.BCEWithLogitsLoss()(o1, t1)\n    l2 = nn.BCEWithLogitsLoss()(o2, t2)\n    return l1 + l2\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\nclass TweetDataset:\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n\n        tweet = self.tweet[item]\n        selected_text = self.selected_text[item]\n    \n        len_sel_text = len(selected_text)\n        idx0 = -1\n        idx1 = -1\n\n        for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n            if tweet[ind:ind+len_sel_text] == selected_text:\n                idx0 = ind\n                idx1 = ind + len_sel_text - 1\n                break\n        \n        char_targets = [0] * len(tweet)\n        if idx0 != -1 and idx1 != -1:\n            for j in range(idx0, idx1 + 1):\n                if tweet[j] != \" \":\n                    char_targets[j] = 1\n\n        tok_tweet = self.tokenizer.encode(sequence=self.sentiment[item], pair=tweet)\n        tok_tweet_tokens = tok_tweet.tokens\n        tok_tweet_ids = tok_tweet.ids \n        tok_tweet_offsets = tok_tweet.offsets[3:-1] # taking answer part\n\n        targets = [0] * (len(tok_tweet_tokens) - 4) # removing tokens not in answer part\n\n        if self.sentiment[item] == \"positive\" or self.sentiment[item] == \"negative\":\n            sub_minus = 8\n        else:\n            sub_minus = 7\n\n        for j, (offset1, offset2) in enumerate(tok_tweet_offsets):\n            if sum(char_targets[offset1 - sub_minus: offset2 - sub_minus]) > 0:\n                targets[j] = 1\n        \n        targets = [0] + [0] + [0] + targets + [0] # cls, sep and 2 more added here!!\n        targets_start = [0] * len(targets)\n        targets_end = [0] * len(targets)\n\n        non_zero = np.nonzero(targets)[0]\n        if len(non_zero) > 0:\n            targets_start[non_zero[0]] = 1\n            targets_end[non_zero[-1]] = 1\n\n        # attention mask\n        mask = [1] * len(tok_tweet_ids)\n        token_type_ids = [0] * 3 + [1] * (len(tok_tweet_ids) - 3) # this will be all zeros\n        padding_len = self.max_len - len(tok_tweet_ids) # pad everything after the tweet till end\n        tok_tweet_offsets = [(0, 0)]*3 + tok_tweet_offsets + [(0, 0)]*(padding_len+1)\n        # padding for each of mask, tok_tweet_ids, targets, targets_start, targets_end\n        ids = tok_tweet_ids + [0] * padding_len\n        mask += [0] * padding_len\n        token_type_ids += [0] * padding_len\n        targets += [0] * padding_len\n        targets_start += [0] * padding_len\n        targets_end += [0] * padding_len\n\n        # If you have another column called sentiment, use the following preprocessing\n        sentiment = [1,0,0] \n        if self.sentiment[item] == \"positive\":\n            sentiment = [0,0,1]\n        if self.sentiment[item] == \"negative\":\n            sentiment = [0,1,0]\n\n        return {\n            \"ids\": torch.tensor(ids, dtype=torch.long),\n            \"mask\": torch.tensor(mask, dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n            \"targets\": torch.tensor(targets, dtype=torch.long),\n            \"targets_start\": torch.tensor(targets_start, dtype=torch.long),\n            \"targets_end\": torch.tensor(targets_end, dtype=torch.long),\n            \"orig_tweet\": self.tweet[item],\n            \"sentiment\": torch.tensor(sentiment, dtype=torch.long),\n            \"orig_sentiment\": self.sentiment[item],\n            \"orig_selected\": self.selected_text[item],\n            \"tweet_offsets\":torch.tensor(tok_tweet_offsets, dtype=torch.long)\n        }","6817a67e":"# train = pd.read_csv(train_file)\n# dset = TweetDataset(train.text.values, train.sentiment.values, train.selected_text.values)\n# dset[57]","03b3ab5f":"class TweetBertBaseCased(nn.Module):\n    \n    def __init__(self, config):\n        super(TweetBertBaseCased, self).__init__()\n        self.config = config\n        self.num_labels = config.num_labels\n        self.num_recurrent_layers = 1\n        self.units = 768\n\n        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased', config=self.config)\n        self.classifier = nn.Linear(self.units*4, self.num_labels)\n\n    def forward(self, ids, mask, token_type_ids):\n\n        bert_outputs = self.bert(ids,\n                            attention_mask=mask,\n                            token_type_ids=token_type_ids)\n        \n        hidden_outputs = bert_outputs[2]\n\n        sequence_output = torch.cat(tuple([hidden_outputs[i] for i in [-1, -2, -3, -4]]), dim=-1)                \n    \n        logits = self.classifier(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits","35694e30":"def decode_prediction(fin_outputs_start, fin_outputs_end, fin_tweet_offsets,\n                      fin_orig_tweet, fin_orig_sentiment):\n    \n    threshold_start = np.argmax(fin_outputs_start, axis = 1)\n    threshold_end = np.argmax(fin_outputs_end, axis = 1)\n    all_strings = []\n    \n    for j in range(len(fin_orig_tweet)):\n\n        original_tweet = fin_orig_tweet[j]\n        sentiment_val = fin_orig_sentiment[j]\n        tweet_offsets = fin_tweet_offsets[j]\n        \n        idx_start = threshold_start[j]\n        idx_end = threshold_end[j]\n\n        if idx_end < idx_start:\n            idx_end = idx_start\n\n        if sentiment_val == \"neutral\":\n            sub_minus = 7\n        else:\n            sub_minus = 8       \n\n        final_output  = \"\"\n        for i in range(idx_start,idx_end+1):\n            final_output += original_tweet[tweet_offsets[i][0]-sub_minus:tweet_offsets[i][1]-sub_minus]\n            if (i+1) < len(tweet_offsets) and tweet_offsets[i][1] < tweet_offsets[i+1][0]:\n                final_output += \" \"\n\n        if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n            final_output = original_tweet # just select the whole sentence\n        all_strings.append(final_output)\n    \n    return all_strings","f4552a81":"def train_model(model, train_data_loader, optimizer, scheduler, device):\n    \n    model.train()\n    losses = AverageMeter()\n    tk0 = tqdm(train_data_loader, total=len(train_data_loader))\n\n    for bi, batch in enumerate(tk0):\n        ids = batch[\"ids\"]\n        token_type_ids = batch[\"token_type_ids\"]\n        mask = batch[\"mask\"]\n        targets_start = batch[\"targets_start\"]\n        targets_end = batch[\"targets_end\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.float)\n        targets_end = targets_end.to(device, dtype=torch.float)\n\n        optimizer.zero_grad()\n        o1, o2 = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        loss = loss_fn(o1, o2, targets_start, targets_end)\n        with amp.scale_loss(loss, optimizer) as scaled_loss:\n            scaled_loss.backward() \n        optimizer.step()\n        scheduler.step()\n\n        losses.update(loss.item(), ids.size(0))\n        tk0.set_postfix(loss=losses.avg)\n\n\ndef eval_model(model, valid_data_loader, device):\n    \n    model.eval()\n\n    fin_outputs_start = []\n    fin_outputs_end = []\n    fin_orig_tweet = []\n    fin_orig_sentiment = []\n    fin_orig_selected = []\n    fin_tweet_offsets = []\n\n    with torch.no_grad():\n        for bi, batch in enumerate(valid_data_loader):\n            ids = batch[\"ids\"]\n            token_type_ids = batch[\"token_type_ids\"]\n            mask = batch[\"mask\"]\n            tweet_offsets = batch[\"tweet_offsets\"]\n\n            # we need to calculate jaccard, so we need tweet tokens\n            orig_sentiment = batch[\"orig_sentiment\"]\n            orig_selected = batch[\"orig_selected\"]\n            orig_tweet = batch[\"orig_tweet\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n\n            o1, o2 = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n            \n            fin_outputs_start.append(F.softmax(o1, dim=1).cpu().detach().numpy())\n            fin_outputs_end.append(F.softmax(o2, dim=1).cpu().detach().numpy())\n            fin_tweet_offsets.extend(tweet_offsets.cpu().detach().numpy())\n\n            fin_orig_sentiment.extend(orig_sentiment)\n            fin_orig_selected.extend(orig_selected)\n            fin_orig_tweet.extend(orig_tweet)\n\n    # now, construct the final text we will submit to Kaggle\n    fin_outputs_start = np.vstack(fin_outputs_start)\n    fin_outputs_end = np.vstack(fin_outputs_end)\n    \n    all_strings = decode_prediction(fin_outputs_start, fin_outputs_end, fin_tweet_offsets,\n                                    fin_orig_tweet, fin_orig_sentiment)\n    \n    jaccards = []\n    for i in range(len(fin_orig_tweet)):\n        target_string, final_output  = fin_orig_selected[i], all_strings[i]\n        jac = jaccard(target_string, final_output)\n        jaccards.append(jac)\n    mean_jac = np.mean(jaccards)\n    \n    return mean_jac, all_strings","00b0360d":"bert_config = BertConfig.from_pretrained('bert-base-uncased')\nbert_config.num_labels = 2\nbert_config.output_hidden_states = True\n    \ntrain = pd.read_csv(train_file).dropna().reset_index(drop=True)\nall_scores = []\nprediction_bert = ['']*len(train)\n    \nkf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\nfor fold, (tr_ind, val_ind) in enumerate(kf.split(train, train['sentiment'])):\n    \n    if fold == 0:\n        \n        print(f'Fold no {fold+1}:')\n\n        x_train = train.iloc[tr_ind].reset_index(drop=True)\n        x_val = train.iloc[val_ind].reset_index(drop=True)        \n\n        train_dataset = TweetDataset(tweet = x_train.text.values,\n                                     sentiment = x_train.sentiment.values,\n                                     selected_text = x_train.selected_text.values)\n\n        train_data_loader = torch.utils.data.DataLoader(train_dataset,\n                                                        batch_size=train_batch_size,\n                                                        num_workers=4)\n\n        valid_dataset = TweetDataset(tweet = x_val.text.values,\n                                     sentiment = x_val.sentiment.values,\n                                     selected_text = x_val.selected_text.values)\n\n        valid_data_loader = torch.utils.data.DataLoader(valid_dataset,\n                                                        batch_size=valid_batch_size,\n                                                        num_workers=1)\n\n        device = torch.device(\"cuda\")\n        model = TweetBertBaseCased(config=bert_config)\n        model.to(device)\n\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n        ]\n\n        num_train_steps = int(len(x_train) \/ train_batch_size * epochs)\n        optimizer = AdamW(optimizer_parameters, lr=3e-5)\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=0,\n            num_training_steps=num_train_steps\n        )\n\n        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O2\", \n                                          keep_batchnorm_fp32=True, loss_scale=\"dynamic\",\n                                          verbosity = 0)\n        print(\"Training....\")\n\n        best_jaccard = 0\n        for epoch in range(epochs):\n            train_model(model, train_data_loader, optimizer, scheduler, device)\n            jaccard_score, out_strings = eval_model(model, valid_data_loader, device)\n\n            print(f\"Jaccard Score = {jaccard_score}\")\n            if jaccard_score > best_jaccard:\n                torch.save(model.state_dict(), str(fold)+modle_path)\n                best_jaccard = jaccard_score\n                count = 0\n                for i in val_ind:\n                    prediction_bert[i] = out_strings[count]\n                    count += 1\n            break\n        all_scores.append(best_jaccard)","400b85d2":"print(all_scores)","d1f44daf":"train['predictions'] = prediction_bert\ntrain.loc[(train.sentiment == 'negative') | (train.sentiment=='positive')][:10]","961004f9":"# Starter kernel based on Abhishek's and akensert's work in pytorch\n\n## You can train having internet on and make an inference kernel. \n\nSorry for not committing properly since I need GPU hours, running 3 competitions with GPU\/TPU is straining on my resources :)\n"}}