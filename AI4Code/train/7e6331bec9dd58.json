{"cell_type":{"857b0d37":"code","e2f52eab":"code","70acf9a1":"code","aedd9863":"code","0b7a8b90":"code","d66847be":"code","fc9a69b4":"code","88c5d91e":"code","caa60dca":"code","4792d1a2":"code","150c3cd5":"code","5672ea38":"code","7a2d7dc0":"code","9a8b28c7":"code","f4ad90bc":"code","4e29c5d5":"code","f38437cd":"code","6c24bfd0":"code","e6b5930c":"code","de8edcc1":"code","19c94c0f":"code","9e19f824":"code","f1f9b11c":"code","ef0a6ff3":"code","222aca4e":"code","7b8bb051":"code","f4adf994":"code","e895426e":"code","ee6d2a89":"code","c0bed3e5":"code","5a35b58c":"code","ea58cb95":"code","25218a5f":"code","7fe50460":"code","eb36dea3":"code","6c8f9030":"code","9837d4f5":"code","8303164e":"code","0dd45016":"code","e397278e":"code","c79b902a":"code","cfe48279":"code","7b1a8f2c":"code","a4ee5b68":"code","aecdc11b":"code","e7229e21":"code","f2574bb1":"code","2fc405ad":"code","a88203a7":"code","19548f72":"code","20b287ef":"code","c5f30bc7":"code","266d30eb":"code","4320dd41":"code","1bdfc61c":"code","325b94fc":"code","fdc9132b":"code","ab3a0eb7":"code","f0f5f9bb":"code","0c173570":"code","19f8b249":"code","d6a26230":"code","496f576a":"code","849e8df2":"code","b3e9e3a5":"code","b843f182":"code","f3f80a5a":"code","b7a98d0f":"code","18481449":"code","ef4de26b":"code","7ef9ee90":"code","fa265c87":"code","c014d31d":"code","e9a80bdb":"code","0b6467ec":"code","c01e8bdc":"code","f05171f6":"code","75980e38":"code","fd00fe4b":"code","49e5dad8":"code","b01b6099":"code","16597837":"code","09e7a7db":"code","27ffa4c8":"code","aaaf8909":"code","29fef4b9":"code","5192f850":"code","d07b6163":"code","f104e74d":"code","f66c766a":"code","3b1784ce":"code","9da52fa4":"code","a4ff9c6c":"code","5ff4b51e":"code","9f52c5d1":"code","3f1f3f7f":"markdown","2d8d76ac":"markdown","75484a9d":"markdown","a07b4964":"markdown","b8a06829":"markdown","78153993":"markdown","dc807073":"markdown","7a76025d":"markdown","5d1e154b":"markdown","6da03f9e":"markdown","e1b4bc0d":"markdown","7189cd35":"markdown","03e7e100":"markdown","60d05a2e":"markdown","d0921395":"markdown","b50c81ba":"markdown","987d6eb9":"markdown","ec0899b2":"markdown","ca4946fc":"markdown","d610120b":"markdown","c9a3a96b":"markdown","b98cc5bd":"markdown","e23fe06d":"markdown","c3ea55f4":"markdown","817e703f":"markdown","aaa771ba":"markdown","1c29b04e":"markdown","1cdc22aa":"markdown","a5e33ef4":"markdown","8c4b2f12":"markdown","23b77254":"markdown","4453843d":"markdown","48a48491":"markdown","04ef1abe":"markdown","642b7bf6":"markdown","95802072":"markdown","2681ce93":"markdown","f9ba00d7":"markdown","3cd07749":"markdown","1f4f0c73":"markdown","b9216214":"markdown","d9c099bd":"markdown","8203492c":"markdown","dec5f205":"markdown","4eee78f0":"markdown","bb3483c7":"markdown","05feda3b":"markdown","2a02ac8a":"markdown","0e8e9eff":"markdown","6aef26d1":"markdown","33e9b215":"markdown","a2f854de":"markdown"},"source":{"857b0d37":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e2f52eab":"df=pd.read_csv('\/kaggle\/input\/churn-modelling\/Churn_Modelling.csv')","70acf9a1":"df.info()","aedd9863":"df.groupby('Exited').agg({'CustomerId':'count'})","0b7a8b90":"EDA_Data=df.copy()\nEDA_Data['Exited']=EDA_Data['Exited'].apply(lambda x: 'Exit' if x==1 else 'Not Exit')\nEDA_Data['HasCrCard']=EDA_Data['HasCrCard'].apply(lambda x: 'Has Credit Card' if x==1 else 'Do not have Credit Card')\nEDA_Data['IsActiveMember']=EDA_Data['IsActiveMember'].apply(lambda x: 'Active Member' if x==1 else 'Inactive Member')\nEDA_Data['NumOfProducts']=EDA_Data['NumOfProducts'].astype('str')","d66847be":"EDA_Data.drop(['RowNumber','CustomerId'],axis=1).describe()","fc9a69b4":"sns.boxplot(x='Exited',y='Age',data=EDA_Data)","88c5d91e":"sns.boxplot(x='Exited',y='Tenure',data=EDA_Data)","caa60dca":"sns.boxplot(x='Exited',y='Balance',data=EDA_Data)","4792d1a2":"sns.boxplot(x='Exited',y='CreditScore',data=EDA_Data)","150c3cd5":"sns.boxplot(x='Exited',y='EstimatedSalary',data=EDA_Data)","5672ea38":"fig, axis = plt.subplots(3, 2, figsize=(10,15),)\naxis[0,0].set_title(\"Relationship between Gender and Exited\")\naxis[0,1].set_title(\"Relationship between Geography and Exited\")\naxis[1,0].set_title(\"Relationship between Has Credit Card and Exited\")\naxis[1,1].set_title(\"Relationship between Is Active and Exited\")\naxis[2,0].set_title(\"Relationship between No. of Product and Exited\")\n\nsns.countplot(x='Gender',hue='Exited',data=EDA_Data,ax=axis[0,0])\nsns.countplot(x='Geography',hue='Exited',data=EDA_Data,ax=axis[0,1])\nsns.countplot(x='HasCrCard',hue='Exited',data=EDA_Data,ax=axis[1,0])\nsns.countplot(x='IsActiveMember',hue='Exited',data=EDA_Data,ax=axis[1,1])\nsns.countplot(x='NumOfProducts',hue='Exited',data=EDA_Data,ax=axis[2,0])\n\naxis[2,1].remove()","7a2d7dc0":"plt.figure(figsize=(20,10))\nEDA2=df.iloc[:,3:-1]\nEDA2=pd.concat([df.iloc[:,-1],EDA2,],axis=1)\nEDA2=pd.get_dummies(EDA2,columns=['Geography','Gender','NumOfProducts','HasCrCard','IsActiveMember'])\nsns.heatmap(EDA2.corr(),vmin=-1,vmax=1,annot=True)","9a8b28c7":"X0=df.iloc[:,3:-1]\nX0_c=X0.loc[:,['Geography','Gender','NumOfProducts','HasCrCard','IsActiveMember']]","f4ad90bc":"X0_c=pd.get_dummies(X0_c)","4e29c5d5":"from sklearn.preprocessing import StandardScaler\nX0_n=X0.loc[:,['CreditScore','Age','Balance','Tenure','EstimatedSalary']]\nscaler = StandardScaler()\nX0_n1=scaler.fit_transform(X0_n)","f38437cd":"X0_n1=pd.DataFrame(X0_n1,columns=['CreditScore','Age','Balance','Tenure','EstimatedSalary'])","6c24bfd0":"X=pd.concat([X0_c,X0_n1],axis=1)\ny=EDA_Data['Exited'].apply(lambda x: 1 if x=='Exit' else 0)","e6b5930c":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","de8edcc1":"#conda install -c conda-forge imbalanced-learn","19c94c0f":"from imblearn.over_sampling import SMOTE\nsmk = SMOTE()\n# Oversample training  data\nX_train, y_train = smk.fit_sample(X_train, y_train)\n\n# Oversample validation data\nX_test, y_test = smk.fit_sample(X_test, y_test)","9e19f824":"print(y_train.value_counts(),'\\n',y_test.value_counts())","f1f9b11c":"Accuracy_Score=[]\nRecall_Score=[]\nPrecision_Score=[]\nF1_Score=[]","ef0a6ff3":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=42).fit(X_train, y_train)\ny_pred=clf.predict(X_test)\ns1_1_y_pred=y_pred\nclf.score(X_test, y_test)","222aca4e":"print(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))","7b8bb051":"from sklearn.svm import SVC\nclf = SVC(random_state=42)\nsvc=clf.fit(X_train, y_train)\ny_pred=svc.predict(X_test)\ns1_2_y_pred=y_pred\nsvc.score(X_test, y_test)","f4adf994":"print(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))","e895426e":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ny_pred=clf.predict(X_test)\ns1_3_y_pred=y_pred\nclf.score(X_test, y_test)","ee6d2a89":"print(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))","c0bed3e5":"import xgboost as xgb\ndtrain = xgb.DMatrix(data = X_train, label = y_train) \ndtest = xgb.DMatrix(data = X_test, label = y_test) \n# specify parameters via map\nparam = {'max_depth':6, 'eta':0.3, 'objective':'binary:hinge' } #Use default value in the first time\nnum_round = 2\nbst = xgb.train(param, dtrain, num_round)\n# make prediction\ny_pred = bst.predict(dtest)\ns1_4_y_pred=y_pred\naccuracy_score(y_test,y_pred)","5a35b58c":"print(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))","ea58cb95":"# Set 1 Score:\nind_name=['Accuracy_Score','Recall_Score','Precision_Score','F1_Score']\nsummary1=pd.DataFrame(np.vstack((Accuracy_Score,Recall_Score,Precision_Score,F1_Score)),columns=['Logistic Reg.','SVC','Random Forest','XGB'],index=ind_name)\nsummary1","25218a5f":"X1=df.iloc[:,3:-1]\nX1_c=X1.loc[:,['Geography','Gender','NumOfProducts','IsActiveMember']]\nX1_n=X1.loc[:,['Age','Balance']]","7fe50460":"EDA_Data[EDA_Data['Exited']=='Exit'].describe()","eb36dea3":"EDA_Data[EDA_Data['Exited']=='Not Exit'].describe()","6c8f9030":"sns.distplot(a=EDA_Data['Age'],kde=False)","9837d4f5":"Age1=EDA_Data[EDA_Data['Exited']=='Exit']\nsns.distplot(a=Age1['Age'],kde=False)","8303164e":"Age1=EDA_Data[EDA_Data['Exited']=='Not Exit']\nsns.distplot(a=Age1['Age'],kde=False)","0dd45016":"def age_gp(a):\n    if a>=18 and a<30:\n        return 'Gp1'\n    elif a>=30 and a<40:\n        return 'Gp2'\n    elif a>=40 and a<50:\n        return 'Gp3'\n    elif a>=50:\n        return 'Gp4'","e397278e":"X1_c['Age_group']=X1_n['Age'].apply(age_gp)","c79b902a":"X1_c.head(3)","cfe48279":"sns.distplot(a=EDA_Data['Balance'])","7b1a8f2c":"X1_c['Balance_Group']=X1_n['Balance'].apply(lambda x: 'Without Balance' if x<50000 else 'With Balance')","a4ee5b68":"## As All of X's are Catergorical Data, only need transfer them to Binary Dataa","aecdc11b":"X1_c2=pd.get_dummies(X1_c)\ny=EDA_Data['Exited'].apply(lambda x: 1 if x=='Exit' else 0)","e7229e21":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X1_c2, y, test_size=0.2, random_state=42)","f2574bb1":"from imblearn.over_sampling import SMOTE\nsmk = SMOTE()\n# Oversample training  data\nX_train, y_train = smk.fit_sample(X_train, y_train)\n\n# Oversample validation data\nX_test, y_test = smk.fit_sample(X_test, y_test)","2fc405ad":"print(y_train.value_counts(),'\\n',y_test.value_counts())","a88203a7":"Accuracy_Score=[]\nRecall_Score=[]\nPrecision_Score=[]\nF1_Score=[]","19548f72":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=42).fit(X_train, y_train)\ny_pred=clf.predict(X_test)\ns2_1_y_pred=y_pred\nclf.score(X_test, y_test)\n","20b287ef":"print(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))","c5f30bc7":"from sklearn.svm import SVC\nclf = SVC(random_state=42)\nsvc=clf.fit(X_train, y_train)\ny_pred=svc.predict(X_test)\ns2_2_y_pred=y_pred\nsvc.score(X_test, y_test)","266d30eb":"print(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))","4320dd41":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\ny_pred=clf.predict(X_test)\ns2_3_y_pred=y_pred\nclf.score(X_test, y_test)","1bdfc61c":"print(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))","325b94fc":"import xgboost as xgb\ndtrain = xgb.DMatrix(data = X_train, label = y_train) \ndtest = xgb.DMatrix(data = X_test, label = y_test) \n# specify parameters via map\nparam = {'max_depth':6, 'eta':0.3, 'objective':'binary:hinge' }\nnum_round = 2\nbst = xgb.train(param, dtrain, num_round)\n# make prediction\ny_pred = bst.predict(dtest)\ns2_4_y_pred=y_pred\naccuracy_score(y_test,y_pred)","fdc9132b":"print(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))","ab3a0eb7":"# Set 2 Score:\nind_name=['Accuracy_Score','Recall_Score','Precision_Score','F1_Score']\nsummary2=pd.DataFrame(np.vstack((Accuracy_Score,Recall_Score,Precision_Score,F1_Score)),columns=['Logistic Reg.','SVC','Random Forest','XGB'],index=ind_name)\nsummary2","f0f5f9bb":"import sklearn.metrics as metrics\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = clf.predict_proba(X_test)\npreds = probs[:,1]\n\n\nfpr11, tpr11, threshold = metrics.roc_curve(y_test, s1_1_y_pred)\nroc_auc11 = metrics.auc(fpr11, tpr11)\nfpr12, tpr12, threshold = metrics.roc_curve(y_test, s1_2_y_pred)\nroc_auc12 = metrics.auc(fpr12, tpr12)\nfpr13, tpr13, threshold = metrics.roc_curve(y_test, s1_3_y_pred)\nroc_auc13 = metrics.auc(fpr13, tpr13)\nfpr14, tpr14, threshold = metrics.roc_curve(y_test, s1_4_y_pred)\nroc_auc14 = metrics.auc(fpr14, tpr14)\n\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Set 1 Model AUC-ROC')\nplt.plot(fpr11, tpr11, 'b', label = 'AUC = %0.2f logistic' % roc_auc11)\nplt.plot(fpr12, tpr12, 'r', label = 'AUC = %0.2f svc' % roc_auc12)\nplt.plot(fpr13, tpr13, 'y', label = 'AUC = %0.2f RF' % roc_auc13)\nplt.plot(fpr14, tpr14, 'g', label = 'AUC = %0.2f XGB' % roc_auc14)\n\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","0c173570":"import sklearn.metrics as metrics\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = clf.predict_proba(X_test)\npreds = probs[:,1]\n\n\nfpr21, tpr21, threshold = metrics.roc_curve(y_test, s2_1_y_pred)\nroc_auc21 = metrics.auc(fpr21, tpr21)\nfpr22, tpr22, threshold = metrics.roc_curve(y_test, s2_2_y_pred)\nroc_auc22 = metrics.auc(fpr22, tpr22)\nfpr23, tpr23, threshold = metrics.roc_curve(y_test, s2_3_y_pred)\nroc_auc23 = metrics.auc(fpr23, tpr23)\nfpr24, tpr24, threshold = metrics.roc_curve(y_test, s2_4_y_pred)\nroc_auc24 = metrics.auc(fpr24, tpr24)\n\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Set 2 Model AUC-ROC')\nplt.plot(fpr11, tpr21, 'b', label = 'AUC = %0.2f logistic' % roc_auc21)\nplt.plot(fpr12, tpr22, 'r', label = 'AUC = %0.2f svm' % roc_auc22)\nplt.plot(fpr13, tpr23, 'y', label = 'AUC = %0.2f RF' % roc_auc23)\nplt.plot(fpr14, tpr24, 'g', label = 'AUC = %0.2f XGB' % roc_auc24)\n\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","19f8b249":"summary1","d6a26230":"summary2","496f576a":"from sklearn.model_selection import GridSearchCV","849e8df2":"X=pd.concat([X0_c,X0_n1],axis=1)\ny=EDA_Data['Exited'].apply(lambda x: 1 if x=='Exit' else 0)","b3e9e3a5":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","b843f182":"from imblearn.over_sampling import SMOTE\nsmk = SMOTE()\n# Oversample training  data\nX_train, y_train = smk.fit_sample(X_train, y_train)\n\n# Oversample validation data\nX_test, y_test = smk.fit_sample(X_test, y_test)","f3f80a5a":"pd.DataFrame(y_train).groupby('Exited').agg({'Exited':'count'})","b7a98d0f":"Accuracy_Score=[]\nRecall_Score=[]\nPrecision_Score=[]\nF1_Score=[]","18481449":"from sklearn.linear_model import LogisticRegression\n\nclf =  LogisticRegression(random_state=42)\nparam_grid = [\n    {'C':[0.01,0.1,1,10]}]\n     #'max_iter':[100,150,200,1000]}]\n    #{'solver': ['newton-cg','sag','lbfgs' ],'penalty':['l2']}] \n    #{'solver': ['liblinear','saga'],'penalty':['l1']}]\nsearch = GridSearchCV(clf, param_grid,scoring='accuracy',cv=5)\nlr=search.fit(X_train, y_train)\n\nlr=clf.fit(X_train, y_train)\ny_pred=lr.predict(X_test)\nlr.score(X_test, y_test) \n\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n\n#search.cv_results_\n","ef4de26b":"from sklearn.linear_model import LogisticRegression\n\nclf =  LogisticRegression(random_state=42)\n\nparam_grid = [\n    {'C':[0.01,0.1,1,10]}]\n  #{'solver': ['newton-cg','sag','lbfgs' ],'penalty':['l2']}]\n  #{'solver': ['liblinear','saga'],'penalty':['l1']}]\nsearch = GridSearchCV(clf, param_grid,scoring='recall', cv=5)\nlr=search.fit(X_train, y_train)\n\n#lr=clf.fit(X_train, y_train)\ny_pred=lr.predict(X_test)\nlr.score(X_test, y_test) \n\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n\n#search.cv_results_","7ef9ee90":"from sklearn.svm import SVC\nclf = SVC(random_state=42, kernel='rbf')\n\nparam_grid = [\n  {'C': [0.01,0.1,1,10]}]\n\nsearch = GridSearchCV(clf, param_grid,scoring='accuracy', cv=5)\nsvc=search.fit(X_train, y_train)\ny_pred=svc.predict(X_test)\nsvc.score(X_test, y_test) \n\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n#search.cv_results_","fa265c87":"from sklearn.svm import SVC\nclf = SVC(random_state=42, kernel='rbf')\n\nparam_grid = [\n  {'C': [0.01,0.1,1,10],\n   'gamma':['scale', 'auto']}]\n\nsearch = GridSearchCV(clf, param_grid,scoring='recall', cv=5)\nsvc=search.fit(X_train, y_train)\ny_pred=svc.predict(X_test)\nsvc.score(X_test, y_test)\n\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n#search.cv_results_","c014d31d":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nparam_grid = [\n  {'n_estimators' : [140,150,160,170,180,190,200]}]\n\nsearch = GridSearchCV(clf, param_grid,scoring='accuracy', cv=5)\nclf=search.fit(X_train, y_train)\ny_pred=clf.predict(X_test)\n\nclf.score(X_test, y_test)\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n#search.cv_results_  ","e9a80bdb":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nparam_grid = [\n  {'n_estimators' : [140,150,160,170,180,190,200]}]\n\nsearch = GridSearchCV(clf, param_grid, scoring='recall', cv=5)\nclf=search.fit(X_train, y_train)\ny_pred=clf.predict(X_test)\n\nclf.score(X_test, y_test)\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n#search.cv_results_ ","0b6467ec":"import xgboost as xgb\ndtrain = xgb.DMatrix(data = X_train, label = y_train) \ndtest = xgb.DMatrix(data = X_test, label = y_test) \n# specify parameters via map\nparam = {'max_depth':4, 'eta':0.6, 'objective':'binary:hinge'}\nnum_round = 20\nbst = xgb.train(param, dtrain, num_round)\n# make prediction\ny_pred = bst.predict(dtest)\naccuracy_score(y_test,y_pred)\n\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))","c01e8bdc":"# Set 3 Score:\nind_name=['Accuracy_Score','Recall_Score','Precision_Score','F1_Score']\nsummary3=pd.DataFrame(np.vstack((Accuracy_Score,Recall_Score,Precision_Score,F1_Score)),\n                      columns=['Logistic Reg.(Accuracy)','Logistic Reg.(Recall)',\n                               'SVC (Accuracy)','SVC (Recall)',\n                               'Random Forest (Accuracy)','Random Forest (Recall)'\n                               ,'XGB'],index=ind_name)\nsummary3","f05171f6":"summary1","75980e38":"summary2","fd00fe4b":"X1_c2=pd.get_dummies(X1_c)\ny=EDA_Data['Exited'].apply(lambda x: 1 if x=='Exit' else 0)","49e5dad8":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X1_c2, y, test_size=0.2, random_state=42)","b01b6099":"from imblearn.over_sampling import SMOTE\nsmk = SMOTE()\n# Oversample training  data\nX_train, y_train = smk.fit_sample(X_train, y_train)\n\n# Oversample validation data\nX_test, y_test = smk.fit_sample(X_test, y_test)","16597837":"pd.DataFrame(y_train).groupby('Exited').agg({'Exited':'count'})","09e7a7db":"Accuracy_Score=[]\nRecall_Score=[]\nPrecision_Score=[]\nF1_Score=[]","27ffa4c8":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=42)\n\n\nparam_grid = [\n  {'C':[0.1,1,10],'max_iter':[1000,10000]}]\n  #{'solver': ['newton-cg','sag','lbfgs'],'C':[0.1,1,10],'max_iter':[1000,10000]}] \n  #{'solver': ['newton-cg','sag','lbfgs' ],'penalty':['l2'],'C':[0.1,1,10],'max_iter':[1000,10000]}] #1\n  #{'solver': ['liblinear','saga'],'penalty':['l1'],'max_iter':[1000,10000]}]\nsearch = GridSearchCV(clf, param_grid, scoring='accuracy', cv=5)\n\n\nlr=search.fit(X_train, y_train)\ny_pred=lr.predict(X_test)\nlr.score(X_test, y_test) \n\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n#search.cv_results_  (No much Change)","aaaf8909":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=42)\n\nparam_grid = [\n    {'C':[0.1,1,10],'max_iter':[1000,10000]}]\n  #{'solver': ['newton-cg','sag','lbfgs' ],'C':[0.1,1,10],'max_iter':[1000,10000]}] \n  #{'solver': ['newton-cg','sag','lbfgs' ],'penalty':['l2'],'C':[0.1,1,10],'max_iter':[1000,10000]}]\n  #{'solver': ['liblinear','saga'],'penalty':['l1'],'C':[0.1,1,10],'max_iter':[1000,10000]}]\nsearch = GridSearchCV(clf, param_grid, scoring='recall', cv=5)\n\n\nlr=search.fit(X_train, y_train)\ny_pred=lr.predict(X_test)\nlr.score(X_test, y_test) \n\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n#search.cv_results_  (No much Change)","29fef4b9":"from sklearn.svm import SVC\n\nclf = SVC(random_state=42)#,C=1,kernel='rbf',gamma='scale')\n\n\nparam_grid = [\n    {'C': [0.01,0.1,1,10]}]    \n   #{'C': [0.01,0.1,1,10], 'kernel': ['rbf'],'gamma':['scale','auto']}]\n\nsearch = GridSearchCV(clf, param_grid, scoring='accuracy', cv=5)\nsvc=search.fit(X_train, y_train)\ny_pred=svc.predict(X_test)\n\n\n#svc=clf.fit(X_train, y_train)\n#y_pred=svc.predict(X_test)\n#s2_2_y_pred=y_pred\n\nsvc.score(X_test, y_test)\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n#search.cv_results_ ","5192f850":"from sklearn.svm import SVC\n\nclf = SVC(random_state=42)#,C=1,kernel='rbf',gamma='scale')\n\n\nparam_grid = [\n  {'C': [0.01,0.1,1,10]}]    \n  #{'C': [0.01,0.1,1,10], 'kernel': ['rbf'],'gamma':['scale','auto']}]\n\nsearch = GridSearchCV(clf, param_grid, scoring='recall', cv=5)\n\nsvc=search.fit(X_train, y_train)\ny_pred=svc.predict(X_test)\n\n\n#svc=clf.fit(X_train, y_train)\n#y_pred=svc.predict(X_test)\n#s2_2_y_pred=y_pred\n\nsvc.score(X_test, y_test)\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n#search.cv_results_ ","d07b6163":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(random_state=42)\nparam_grid = [\n  #{'n_estimators' : [100,150,200],'max_depth':[5,10,15]}]\n  #{'n_estimators' : [100,110,120,130,140,150],'max_depth':[5,6,7,8,9,10]}]\n  #{'n_estimators' : [100,110,120,130,140,150],'max_depth':[9,10]}]\n  #{'n_estimators' : [110,111,112,113,114,115,116,117,118,119],'max_depth':[5,10,15]}]\n   {'n_estimators' : [117,118,119],'max_depth':[9,10,11],'criterion':['gini','entropy']}]\nsearch = GridSearchCV(clf, param_grid, scoring='accuracy', cv=5)\nclf=search.fit(X_train, y_train)\ny_pred=clf.predict(X_test)\n\n#clf.fit(X_train, y_train)\n#y_pred=clf.predict(X_test)\nclf.score(X_test, y_test)\n\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n\n#search.cv_results_","f104e74d":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(random_state=42)\nparam_grid = [\n  #{'n_estimators' : [100,150,200],'max_depth':[5,10,15]}]\n  #{'n_estimators' : [100,110,120,130,140,150],'max_depth':[5,6,7,8,9,10]}]\n  #{'n_estimators' : [100,110,120,130,140,150],'max_depth':[9,10]}]\n  #{'n_estimators' : [105,106,107,108,109,110,111,112,113,114,115,116,117,118,119],'max_depth':[9,10,11]}]\n   {'n_estimators' : [108,109],'max_depth':[9]}]\nsearch = GridSearchCV(clf, param_grid, scoring='recall', cv=5)\nclf=search.fit(X_train, y_train)\ny_pred=clf.predict(X_test)\n\n#clf.fit(X_train, y_train)\n#y_pred=clf.predict(X_test)\nclf.score(X_test, y_test)\n\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))\n\n#search.cv_results_","f66c766a":"#scale_pos_weight=sum(negative instances) \/ sum(positive instances)\nimport xgboost as xgb\ndtrain = xgb.DMatrix(data = X_train, label = y_train) \ndtest = xgb.DMatrix(data = X_test, label = y_test) \n# specify parameters via map\nparam = {'max_depth':5, 'eta':0.07,'objective':'binary:hinge'}\nnum_round = 500\nbst = xgb.train(param, dtrain, num_round)\n# make prediction\ny_pred = bst.predict(dtest)\naccuracy_score(y_test,y_pred)\n\nprint(' Accuracy Score: %.3f' % accuracy_score(y_test, y_pred) ,'\\n', \n      'Recall Score: %.3f' % recall_score(y_test, y_pred) ,'\\n', #True Postive out of Actual Postive\n      'Precision Score: %.3f' % precision_score(y_test, y_pred) ,'\\n', #True Postive out of Predicted Postive\n      'F1 Score Score: %.3f' % f1_score(y_test, y_pred) ) #Close to 1 is better; Close to 0 is worse\n\nAccuracy_Score.append(accuracy_score(y_test, y_pred))\nRecall_Score.append(recall_score(y_test, y_pred))\nPrecision_Score.append(precision_score(y_test, y_pred))\nF1_Score.append(f1_score(y_test, y_pred))","3b1784ce":"# Set 4 Score:\nind_name=['Accuracy_Score','Recall_Score','Precision_Score','F1_Score']\nsummary4=pd.DataFrame(np.vstack((Accuracy_Score,Recall_Score,Precision_Score,F1_Score)),\n                      columns=['Logistic Reg.(Accuracy)','Logistic Reg.(Recall)',\n                               'SVC (Accuracy)','SVC (Recall)',\n                               'Random Forest (Accuracy)','Random Forest (Recall)'\n                               ,'XGB'],index=ind_name)\nsummary4","9da52fa4":"summary1","a4ff9c6c":"summary2","5ff4b51e":"summary3","9f52c5d1":"summary4","3f1f3f7f":"#### We can see that the best model among Set 1 and Set 2 is 'Random Forest in Set 1'\n#### All Models have higher score in Set 1 than Set 2\n#### However, for building a Churn Model, it is more important to predict customer who will leave correctly rather than the overall accuracy of the model.\n#### Let's think carefully: \n#### For Type I error, which is the error to predict the customer who exit, but actually he\/she doesn't. \n#### For Type II error, which is the error to predict the customer who not exist, but actually he\/she does.\n#### Which error is more serious? It should be Type II error.\n#### For Type I error case, if we predict wrongly, we may waste cost\/resource to retain a customer who actually will stay.\n#### For Type II error case, if we predict wrongly, we may take no action to the customer and the customer will therefore leave.\n#### Therefore, Recall Score is much more important than Accuracy Score.\n#### Before select which model to be used, let's fine tune our model!","2d8d76ac":"## Part 1: Relationship between Numerical Data and Exited","75484a9d":"### XGBoost","a07b4964":"### XGBoosting","b8a06829":"## Train Data without Feature Selected - Set 1","78153993":"### Logistic Regression","dc807073":"## Hyperparameter Tuning - Set 2\nAlso, I will try to fine tune the Hyperparameter and hope to obtain a better Accuracy and Recall Rate:","7a76025d":"#### Generating new data by oversampling\n#### As mentioned before, the data is imbalnce, so, we increase the number of samples by SMOTE technique","5d1e154b":"### XGBoost","6da03f9e":"# Business Question: \n## Based upon customer data provided by a global bank, please provide a Exiting Customer list to Marketing Team.\n## The Exiting Customer list will be used to retain customer who is going to exit the bank.","e1b4bc0d":"### 2) Relationship between Tenure and Exited","7189cd35":"### SVC (Support Vector Classifier)","03e7e100":"After hyperparameter tuning, we can see that:\n1) For Accuracy, All score in set 1 Model have been improved, while in set 2 Model, only Logistic Regression, SVC and XGB have been improved.\n\n2) For Recall rate, Logistic Regression, SVC and Random Forest have been improved in set 1 while only SVC have been improved in set 2.\n\nSo, which model should be used?\nIn my opinion, XGB with hyperparameter tuning in set 1 is recommended to use.\nAs mentioned before, our business question is find out customer who will exit the bank.\nTherefore, Recall Rate is more important than Accuracy Score.\n\nRecap that:\nTP (True Positive): Predict customer will exit while the customer really exit\nTN (True Negative): Predict customer will not exit while the customer really not exit\nFP (False Positive): Predict customer will exit while actually the customer will not exit \nFN (False Negative): Predict customer will not exit while actually the customer will exit \n\nAccuracy Rate = TP\/(TP+TN)\nRecall Rate = TP\/(TP+FN)\nPrecision = TP\/(TP+FP)\nF1 Score = 2TP\/(2TP+FP+FN)\n\nIn our case, we want predict Exit customer for retention.\nTherefore, which ones is more important?\nA) Finding a exit customer from a base with exiting customer and not exiting customer correctly? OR\nB) Finding a exit customer from a base with exiting customer and 'I guess the customer will not leave, but actually the customer will leave'? OR\nC) Finding a exit customer from a base with exiting customer and 'I guess the customer will leave, but actually the customer will not leave'?\n\nThe answer should be B, right? \nWe should minimize the % of 'I guess the customer will not leave, but actually the customer will leave', i.e. Use the highest recall rate.\n\nTherefore XGBoost in set 1 seems the best ones we are going to use.\nWe should the ones with hyperparameter tuning because the F1_Score is higher than the ones without tuning.\nF1 score becomes high only when both precision and recall are high.\n","60d05a2e":"## Part 2: Relationship between Categorical Data and Exited","d0921395":"#### Start to Train Model","b50c81ba":"### Random Forest","987d6eb9":"#### Estimated Salary do not correlated to Exited.","ec0899b2":"#### Generating new data by oversampling\n#### Same as before, as the data is imbalnce, so, we increase the number of samples by SMOTE technique","ca4946fc":"# EDA","d610120b":"### SVC (Support Vector Classifier)","c9a3a96b":"#### Customer whose balance less than $40000 are more likely not exit\n#### The top 25% exit customer has higher balance than those not exit","b98cc5bd":"### Logistic Regression","e23fe06d":"#### Overall, customer who exited is older than those not exit. \n#### The median of exited customer is around 45 years old.\n#### The median of exited customer is already older than 75% not exit customer.","c3ea55f4":"# Import Library","817e703f":"#### 1) There is no missing value\n#### 2) Numerical Data: Age, Estimated Salary, Tenure, Balance, CreditScore, \n#### 3a) Categorical Data: Gender, Geography\n#### 3b) HasCrDard, IsActiveMember and NumofProducts should be Categorical\/ Binary Data rather than int64\n#### 4a) Label: Exited should be Categorical data\n#### 4b) Label: Exited is imbalance, the proportion of Exit:Not Exit around 20%:80%","aaa771ba":"## Hyperparameter Tuning - Set 1\nI will try to fine tune the Hyperparameter and hope to obtain a better Accuracy and Recall Rate:","1c29b04e":"# Data Extraction","1cdc22aa":"### 5) Relationship between EstimatedSalary and Exited","a5e33ef4":"### Correlation between features","8c4b2f12":"#### Compare to Not Exist Customer, Customer who join less than 3 years or above 7 years are more likely to exit.","23b77254":"### 1) Relationship between Age and Exited","4453843d":"### Part 1: Distribution of Numerical Feature","48a48491":"## Checking Missing Value and Data Type","04ef1abe":"### Random Forest","642b7bf6":"### Random Forest","95802072":"## Prepare Dataset for EDA","2681ce93":"#### From the above diagram, we can see that:\n#### 1) Female is easier exit than Male\n#### 2) Customer in Germany more likely to exit\n#### 3) Inactive Member has higher proportion to exit than Active Member\n#### 4) Credit Card is not correlated to Exit\n#### 5) Customer with 2 products have higher proportion not exit","f9ba00d7":"### SVC (Support Vector Classifier)","3cd07749":"### Remove Estimated Salary, Has Credit Card, Credit Score and Tenure refer to EDA","1f4f0c73":"### SVC (Support Vector Classifier)","b9216214":"### Logistic Regression","d9c099bd":"### 3) Relationship between Balance and Exited","8203492c":"### Logistic Regression","dec5f205":"## Model Selection","4eee78f0":"### XGBoost","bb3483c7":"#### Credit Score only slightly correlated to Exited. \n#### Customer with lower credt score slightly exit.\n#### There are some outliers whose score fall around 350. ","05feda3b":"#### Apply OneHotEncoding to Categorical Data: (Geography,Gender,NumOfProducts,HasCrCard,IsActiveMember)","2a02ac8a":"#### Start to Train Model","0e8e9eff":"### 4) Relationship between Credit Score and Exited","6aef26d1":"#### Features with color close to 0 means no correlation, \n#### Features with color close to -1 or 1 means having strong negative or postive relationship to each other respectively. \n#### From the above heat map, we can see that all features only have weak relationship to each other.\n#### Features that have relative slightly relationship with Exited include:\n#### Age, NumOfProduct, Geography, IsActiveMember,Balance,Gender","33e9b215":"## Train Data with Feature Selected (Based on EDA) - Set 2 \n### [Keep all hyperparameter same as Set 1]","a2f854de":"### Random Forest"}}