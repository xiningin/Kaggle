{"cell_type":{"c5f6b75f":"code","1a701c69":"code","761460d1":"code","1dbfc4b4":"code","1d0d8bd0":"code","a6c61ce8":"code","76373c29":"code","a82e974e":"code","8a862d53":"code","fd94f504":"code","fd0d5b49":"code","4f25f2da":"code","7a16a15e":"code","2d2c381f":"code","99987c88":"code","6ba60e90":"code","643fa957":"code","b75a5591":"code","df3982f7":"code","07c64fba":"code","2ca8d602":"code","617c781a":"code","c436e6c6":"code","97452e04":"code","f7770421":"code","ecb7b979":"code","e4ebb1c1":"code","21bf6691":"code","240aec0c":"code","bef04270":"code","bb4c9c7c":"code","7ecbe4b4":"code","ba1d4b3b":"code","1b086511":"code","6c078911":"code","7106f781":"code","59b815f3":"code","eb42ebfe":"code","0241c438":"code","0d44c653":"code","f9eba802":"code","03151054":"code","0fdb44df":"code","03ebf952":"code","293e04c7":"code","2ffa37c8":"code","c8ef6866":"code","e0de2ef9":"code","3e734ef0":"code","eb908272":"code","03352a03":"code","ab9fce34":"code","3e2f45d2":"code","8d4c37b8":"code","70853b0c":"code","7341c155":"code","7ddb9714":"code","659f49d9":"code","d0b8491c":"code","02bd10b3":"code","f65bf926":"code","99abdf30":"code","98a35167":"code","6e2d068f":"code","52b3a15b":"code","dbef8ec5":"code","f4172161":"code","3a016dcd":"code","1258ff0e":"code","d7b79aef":"code","3ce49e18":"code","35d02833":"code","73464af5":"code","d352e947":"code","160af99d":"code","25760f8b":"code","e547170c":"code","cb6f5e94":"code","086d766a":"code","5ceda11f":"code","8b70bfb4":"code","3f5988ac":"code","3d20a034":"code","baab1553":"code","4b3c6ae7":"code","f545b185":"code","802bd622":"code","251886d1":"code","3fcbae19":"code","42cf24fe":"code","e10157ce":"code","2142e235":"code","2084af63":"code","b57de42f":"code","63333660":"code","4bf46fd0":"code","f45f016c":"code","d59d8f1a":"code","2c5c08da":"code","c675eec7":"code","1316da3a":"code","72503172":"code","6cacf135":"code","0ca75d85":"code","888a88ac":"code","4b9e97de":"code","03aa4808":"code","c0bf9791":"code","6f922bf9":"code","25e9e194":"code","9ccde2ac":"code","e8038c97":"code","c5f5fbe2":"code","3eecd50d":"code","280c14c4":"code","91ff34ea":"code","183551ca":"code","8d167ba4":"code","332f2b2b":"code","58caebac":"code","c563f4c0":"code","83bbf1e3":"code","479505d4":"code","a28ca297":"code","6fad658b":"code","370e143d":"code","e7d2eb26":"code","bb910877":"code","7cffcd3f":"code","b8732607":"code","97e53b67":"code","dc9e8c52":"code","0884cabf":"code","2761132c":"code","26915638":"markdown","21090e16":"markdown","a5ebe2f1":"markdown","4c82e4e0":"markdown","724a5b7d":"markdown","1f0f3f12":"markdown","a726de34":"markdown","c745122e":"markdown","20ce9cc6":"markdown","36155dd1":"markdown","cac011f4":"markdown","bc8481e5":"markdown","1e7faf13":"markdown","eb6f5510":"markdown","d56765bf":"markdown","02ae8be1":"markdown","adabae00":"markdown","a9d7f5c7":"markdown","5ab8537d":"markdown","d917293f":"markdown","7b9c63d9":"markdown","423610d9":"markdown","1f0081d8":"markdown","7b00928b":"markdown","8806373d":"markdown","653c5558":"markdown","f4c0893a":"markdown","2eec5b13":"markdown","9676e7e1":"markdown","3a4d1d04":"markdown","b722bb9d":"markdown","1196d996":"markdown","400ecf32":"markdown","cb5421f4":"markdown","fa30e397":"markdown","764f254c":"markdown","57dfc9d8":"markdown","cfb8611a":"markdown","3ddea64a":"markdown","98b79723":"markdown","24ea88ee":"markdown","9ed813dd":"markdown","71cc3161":"markdown","f7124623":"markdown","7679b6a7":"markdown","228fdab3":"markdown","3439d6dc":"markdown","9063b8c3":"markdown","7f0ab2c3":"markdown","3812b1f0":"markdown","bf6c4bab":"markdown","d10acb06":"markdown","0a3b84fb":"markdown","6149f75d":"markdown","4544c892":"markdown","06e51d75":"markdown","708d1833":"markdown","e8ce0e94":"markdown","6ef848e9":"markdown"},"source":{"c5f6b75f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set_style('whitegrid')\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport datetime\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom scipy import sparse\nfrom scipy.sparse import hstack\nfrom scipy.sparse import save_npz","1a701c69":"fake_data=pd.read_csv('\/content\/drive\/My Drive\/news_dataset\/Fake.csv')\nreal_data=pd.read_csv('\/content\/drive\/My Drive\/news_dataset\/True.csv')","761460d1":"fake_data.head()","1dbfc4b4":"real_data.head()","1d0d8bd0":"real_data['True']=1","a6c61ce8":"fake_data['True']=0","76373c29":"data=pd.concat([real_data,fake_data],axis=0)","a82e974e":"data=data.sample(frac=1).reset_index()","8a862d53":"data.reset_index()","fd94f504":"data.head()","fd0d5b49":"data.drop(columns=['index'],inplace=True)","4f25f2da":"data.head()","7a16a15e":"data[data['True']==1].shape","2d2c381f":"data[data['True']==0].shape","99987c88":"data.shape","6ba60e90":"data.to_csv('complete_data.csv')","643fa957":"data['subject'].value_counts()","b75a5591":"nltk.download()","df3982f7":"stop_words = set(stopwords.words('english')) ","07c64fba":"def decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","2ca8d602":"from tqdm import tqdm\ndef preprocess_text(text_data):\n    preprocessed_text = []\n    # tqdm is for printing the status bar\n    for sentance in tqdm(text_data):\n        sent = decontracted(sentance)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        sent = ' '.join(e for e in sent.split() if e.lower() not in stop_words)\n        preprocessed_text.append(sent.lower().strip())\n    return preprocessed_text","617c781a":"preprocessed_title = preprocess_text(data['title'].values)","c436e6c6":"preprocessed_title[:5]","97452e04":"preprocessed_text = preprocess_text(data['text'].values)","f7770421":"preprocessed_text[:5]","ecb7b979":"data['preprocessed_title']=preprocessed_title\ndata['preprocessed_text']=preprocessed_text","e4ebb1c1":"data['length_of_title']=data['preprocessed_title'].apply(lambda x:len(x.split(' ')))","21bf6691":"data['length_of_text']=data['preprocessed_text'].apply(lambda x:len(x.split(' ')))","240aec0c":"data['subject'].value_counts()","bef04270":"data.head()","bb4c9c7c":"# let us see if 'length_of_title' feature can differentiate between fake(0) and true(1) news\nplt.figure(figsize=(6,8))\nsns.boxplot(x='True',y='length_of_title',data=data)\nplt.show()","7ecbe4b4":"# let us see if 'length_of_text' feature can differentiate between fake(0) and true(1) news\nplt.figure(figsize=(6,8))\nsns.boxplot(x='True',y='length_of_text',data=data)\nplt.show()","ba1d4b3b":"type(data['date'][0])","1b086511":"data['date'][0]","6c078911":"# format: %m %d, %y\npd.to_datetime(data['date'])","7106f781":"ind=[]\nfor i,row in data.iterrows():\n  if len(row['date'])>20:\n    ind.append(i)\n","59b815f3":"ind","eb42ebfe":"for i in ind:\n  print(data.loc[i,'date'])","0241c438":"data.drop(index=ind,inplace=True)","0d44c653":"# format: %m %d, %y\ndata['date']=pd.to_datetime(data['date'])","f9eba802":"data.head()","03151054":"data['subject'].unique()","0fdb44df":"plt.figure(figsize=(14,6))\nsns.countplot(x='subject',hue='True',data=data)","03ebf952":"plt.figure(figsize=(8,8))\nsns.scatterplot(x='length_of_title',y='length_of_text',hue='True',data=data)","293e04c7":"def avg_length_of_words(sent):\n  word_list=sent.split(' ')\n  sum=0\n  for word in word_list:\n    sum+=len(word)\n  avg_length=sum\/len(word_list)\n  return np.round(avg_length,2)\n","2ffa37c8":"data['avg_length_title']=data['title'].apply(avg_length_of_words)\ndata['avg_length_text']=data['text'].apply(avg_length_of_words)","c8ef6866":"data.head()","e0de2ef9":"plt.figure(figsize=(8,6))\nsns.boxplot(x='True',y='avg_length_title',data=data)\nplt.show()","3e734ef0":"plt.figure(figsize=(8,6))\nsns.boxplot(x='True',y='avg_length_text',data=data)\nplt.show()","eb908272":"nltk.download('vader_lexicon')","03352a03":"pip install twython","ab9fce34":"from nltk.sentiment.vader import SentimentIntensityAnalyzer","3e2f45d2":"sid = SentimentIntensityAnalyzer()\ntitle_neg_sent=[]\ntitle_pos_sent=[]\ntitle_neu_sent=[]\ntitle_compound_sent=[]\nfor doc in data['title'].values:\n  ss = sid.polarity_scores(doc)\n  title_neg_sent.append(ss['neg'])\n  title_pos_sent.append(ss['pos'])\n  title_neu_sent.append(ss['neu'])\n  title_compound_sent.append(ss['compound'])","8d4c37b8":"title_neg_sent=np.array(title_neg_sent)\ntitle_pos_sent=np.array(title_pos_sent)\ntitle_neu_sent=np.array(title_neu_sent)\ntitle_compound_sent=np.array(title_compound_sent)","70853b0c":"sid = SentimentIntensityAnalyzer()\ntext_neg_sent=[]\ntext_pos_sent=[]\ntext_neu_sent=[]\ntext_compound_sent=[]\nfor doc in data['text'].values:\n  ss = sid.polarity_scores(doc)\n  text_neg_sent.append(ss['neg'])\n  text_pos_sent.append(ss['pos'])\n  text_neu_sent.append(ss['neu'])\n  text_compound_sent.append(ss['compound'])","7341c155":"text_neg_sent=np.array(text_neg_sent)\ntext_pos_sent=np.array(text_pos_sent)\ntext_neu_sent=np.array(text_neu_sent)\ntext_compound_sent=np.array(text_compound_sent)","7ddb9714":"text_sentiment=np.vstack((text_pos_sent,text_neg_sent,text_neu_sent,text_compound_sent)).T","659f49d9":"text_sentiment","d0b8491c":"text_sentiment_df=pd.DataFrame(data=text_sentiment,columns=['text_pos_sent','text_neg_sent','text_neu_sent','text_compound_sent'])","02bd10b3":"data=pd.concat([data,text_sentiment_df],axis=1)","f65bf926":"title_sentiment=np.vstack((title_pos_sent,title_neg_sent,title_neu_sent,title_compound_sent)).T","99abdf30":"title_sentiment_df=pd.DataFrame(data=title_sentiment,columns=['title_pos_sent','title_neg_sent','title_neu_sent','title_compound_sent'])","98a35167":"data=pd.concat([data,title_sentiment_df],axis=1)","6e2d068f":"data.head(1)","52b3a15b":"plt.figure(figsize=(8,6))\nsns.boxplot(x='True',y='title_pos_sent',data=data)\nplt.show()","dbef8ec5":"plt.figure(figsize=(8,6))\nsns.boxplot(x='True',y='title_neg_sent',data=data)\nplt.show()","f4172161":"plt.figure(figsize=(8,6))\nsns.boxplot(x='True',y='title_neu_sent',data=data)\nplt.show()","3a016dcd":"plt.figure(figsize=(8,6))\nsns.boxplot(x='True',y='title_compound_sent',data=data)\nplt.show()","1258ff0e":"plt.figure(figsize=(8,6))\nsns.boxplot(x='True',y='text_pos_sent',data=data)\nplt.show()","d7b79aef":"plt.figure(figsize=(8,6))\nsns.boxplot(x='True',y='text_neg_sent',data=data)\nplt.show()","3ce49e18":"plt.figure(figsize=(8,6))\nsns.boxplot(x='True',y='text_neu_sent',data=data)\nplt.show()","35d02833":"plt.figure(figsize=(8,6))\nsns.boxplot(x='True',y='text_compound_sent',data=data)\nplt.show()","73464af5":"plt.figure(figsize=(8,6))\nsns.boxplot(x=data['True'],y=data['date'].apply(lambda x:x.month))","d352e947":"plt.figure(figsize=(8,6))\nsns.boxplot(x=data['True'],y=data['date'].apply(lambda x:x.weekday()))","160af99d":"plt.figure(figsize=(8,6))\nsns.boxplot(x=data['True'],y=data['date'].apply(lambda x:x.day))","25760f8b":"data.to_csv('data_stage_1.csv')","e547170c":"data=pd.read_csv('\/content\/drive\/My Drive\/news_dataset\/data_stage_1.csv',index_col='date')","cb6f5e94":"data.head()","086d766a":"data.drop(columns=['Unnamed: 0'],inplace=True)","5ceda11f":"data.head(1)","8b70bfb4":"sns.heatmap(data.isna())","3f5988ac":"data.dropna(inplace=True)","3d20a034":"sns.heatmap(data.isnull())","baab1553":"import pickle\nwith open('\/content\/drive\/My Drive\/news_dataset\/glove_vectors', 'rb') as f:\n  model=pickle.load(f)\n  glove_words=set(model.keys())","4b3c6ae7":"Y=data['True'].values","f545b185":"X=data.drop(columns=['True'])","802bd622":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, stratify=Y)","251886d1":"x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, stratify=Y)","3fcbae19":"tfidf_model = TfidfVectorizer()\ntfidf_model.fit(x_train['preprocessed_title'])\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\ntfidf_words = set(tfidf_model.get_feature_names())","42cf24fe":"# computing tfidf word2vec for each title.\nfrom tqdm import tqdm\ntfidf_w2v_vectors_title_tr = []; # the avg-w2v for each title is stored in this list\nfor sentence in tqdm(x_train['preprocessed_title']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if (word in glove_words) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector \/= tf_idf_weight\n    tfidf_w2v_vectors_title_tr.append(vector)\n","e10157ce":"# computing tfidf word2vec for each title.\nfrom tqdm import tqdm\ntfidf_w2v_vectors_title_te = []; # the avg-w2v for each title is stored in this list\nfor sentence in tqdm(x_test['preprocessed_title']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if (word in glove_words) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector \/= tf_idf_weight\n    tfidf_w2v_vectors_title_te.append(vector)\n","2142e235":"len(tfidf_w2v_vectors_title_tr[0])","2084af63":"tfidf_model = TfidfVectorizer()\ntfidf_model.fit(x_train['preprocessed_text'])\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\ntfidf_words = set(tfidf_model.get_feature_names())","b57de42f":"# computing tfidf word2vec for each title.\nfrom tqdm import tqdm\ntfidf_w2v_vectors_text_tr = []; # the avg-w2v for each title is stored in this list\nfor sentence in tqdm(x_train['preprocessed_text']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if (word in glove_words) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector \/= tf_idf_weight\n    tfidf_w2v_vectors_text_tr.append(vector)\n","63333660":"# computing tfidf word2vec for each title.\nfrom tqdm import tqdm\ntfidf_w2v_vectors_text_te = []; # the avg-w2v for each title is stored in this list\nfor sentence in tqdm(x_test['preprocessed_text']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if (word in glove_words) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector \/= tf_idf_weight\n    tfidf_w2v_vectors_text_te.append(vector)\n","4bf46fd0":"x_train.head(2)","f45f016c":"vectorizer = CountVectorizer()\ntext_bow = vectorizer.fit(x_train['subject'])","d59d8f1a":"subject_onehot_train=vectorizer.transform(x_train['subject'])","2c5c08da":"subject_onehot_test=vectorizer.transform(x_test['subject'])","c675eec7":"updated_x_train=x_train.drop(columns=['title','text','subject','preprocessed_title','preprocessed_text'])","1316da3a":"updated_x_train.head(2)","72503172":"updated_x_train=updated_x_train.values","6cacf135":"updated_x_train=np.hstack((updated_x_train,tfidf_w2v_vectors_title_tr,tfidf_w2v_vectors_text_tr))","0ca75d85":"updated_x_train=sparse.csr_matrix(updated_x_train)","888a88ac":"updated_x_train","4b9e97de":"X_train=hstack([updated_x_train,subject_onehot_train])","03aa4808":"updated_x_test=x_test.drop(columns=['title','text','subject','preprocessed_title','preprocessed_text'])","c0bf9791":"updated_x_test.head(2)","6f922bf9":"updated_x_test=updated_x_test.values","25e9e194":"updated_x_test=np.hstack((updated_x_test,tfidf_w2v_vectors_title_te,tfidf_w2v_vectors_text_te))","9ccde2ac":"updated_x_test=sparse.csr_matrix(updated_x_test)","e8038c97":"updated_x_test","c5f5fbe2":"X_test=hstack([updated_x_test,subject_onehot_test])","3eecd50d":"X_test.shape","280c14c4":"X_train.shape","91ff34ea":"y_test.shape","183551ca":"y_train.shape","8d167ba4":"y_test","332f2b2b":"y_train","58caebac":"save_npz('X_train.npz', X_train)\nsave_npz('X_test.npz', X_test)\nnp.save('y_test.npy',y_test)\nnp.save('y_train.npy',y_train)","c563f4c0":"X_train=X_train.toarray()\nX_test=X_test.toarray()","83bbf1e3":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import f1_score,confusion_matrix,classification_report\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.metrics import make_scorer\nscoring = make_scorer(roc_auc_score,f1_score)","479505d4":"params={'var_smoothing':[10**-10,10**-9,10**-8,10**-11,10**-7]}\nnb=GaussianNB()\nclf=RandomizedSearchCV(estimator=nb,param_distributions=params,cv=5,scoring=scoring)\nclf.fit(X_train,y_train)","a28ca297":"clf.best_estimator_","6fad658b":"nb=GaussianNB(priors=None,var_smoothing=10**-10)\nnb.fit(X_train,y_train)","370e143d":"pred=nb.predict(X_test)","e7d2eb26":"cm=confusion_matrix(y_test,pred)","bb910877":"print(cm)","7cffcd3f":"sns.heatmap(cm,cbar=False)","b8732607":"from sklearn.metrics import accuracy_score\n","97e53b67":"accuracy_score(y_test,pred)","dc9e8c52":"import pickle","0884cabf":"Pkl_Filename = \"news_model.pkl\"  \n\nwith open(Pkl_Filename, 'wb') as file:  \n    pickle.dump(nb, file)","2761132c":"with open(\"news_model.pkl\", 'rb') as file:  \n    nb = pickle.load(file)\n\nnb","26915638":"### test data","21090e16":"**there are 21,417 true news data points, 23,481 false news data points, in total there are 44,898 data points**","a5ebe2f1":"## adding columns 'avg_length_title' and 'avg_length_text'","4c82e4e0":"# Pre-processing text data","724a5b7d":"tfidf_w2v_vectors_title_tr, tfidf_w2v_vectors_title_te, \ntfidf_w2v_vectors_text_tr, tfidf_w2v_vectors_text_te, ","1f0f3f12":"loading data\n","a726de34":"# shuffling the data to get True and False news data points in random order","c745122e":"# Concatenating both true and false datasets to create singel dataset consisting of both true and false news","20ce9cc6":"**month might be a little useful but not much**","36155dd1":"## predictions on test data and checking performance using confusion matrix and accuracy score","cac011f4":"## getting sentiment scores","bc8481e5":"**we can clearly see that 'length_of_title' feature can clearly distingush between fake news and true news. this might even prove to be most useful feature. true news has lesser length of title than fake news**","1e7faf13":"#setting label as new column \"True\". value of True will be 1 if the news is real else it qill be fake","eb6f5510":"# doing some exploration","d56765bf":"## RandomSearchCV","02ae8be1":"**'length_of_text' don not seem to be good feature for classification**","adabae00":"## Writing a function to calculate average length of word in a document\/sentence","a9d7f5c7":"## 'text' sentiment scores","5ab8537d":"#### adding title sentiments to 'data' dataframe(main data frame)","d917293f":"Using pretrained global vectors model for getting word embeddings","7b9c63d9":"## let us see if 'date' feature helps us to classify flase and true news","423610d9":"#### adding text sentiments to 'data' dataframe(main data frame)","1f0081d8":"# training GaussianNB ","7b00928b":"## let us explore these sentiment score features a little","8806373d":"**so fake news seem to be appearing for under subjects: 'politics', 'left-news', 'News', 'US_News', 'Government News', 'Middle-east'. true news seem to be appearing under subjects 'politicsNews', worldnews.**\n\n**So we may come up with a feature such that if subject is 'politicsNews' or 'worldnews' then there is a high probability that news is true and if subject is other than these two then there is high probability that the news is false. But we do not want to be deterministic about this.**","653c5558":"# saving the model in a binary file","f4c0893a":"to load model","2eec5b13":"### Adding these to the main data dataframe","9676e7e1":"getting tfidf vocab for 'text' based on train data only","3a4d1d04":"# checking the lengths of final X_train and X_test data matrices","b722bb9d":"## pre-processing 'title' feature","1196d996":"# reading data","400ecf32":"## Train-Test Split","cb5421f4":"**there sure seems to be some kind of clustering but also there is some overlapping of points, fake and true data points seem to be seperable based on lengths of title and text**","fa30e397":"# getting required features for modelling and descarding rest","764f254c":"# saving the model ready data for future use","57dfc9d8":"Vectorizing 'text' of train data","cfb8611a":"### there are some data points for which 'date' feature has some other lengthy non date strings. identifying such rows and removing them","3ddea64a":"We need to get vocabulary based on train data only to avoide data leakage. hence splitting the data into train and test data sets","98b79723":"## pre-processing 'text' feature","24ea88ee":"## let us see if these new features are useful","9ed813dd":"vectorizing 'text' of test data using vocab of train data only","71cc3161":"# Classification of Fake and Real news","f7124623":"## 'title' sentiment scores","7679b6a7":"# getting X_test data ready in sparse matrix form  ","228fdab3":"# All data points in test data correctly classified. 100% accuracy","3439d6dc":"## TF-IDF weighted W2V vectorization of 'text' feature","9063b8c3":"**So basic boxplot analysis shows that these sentiment scores may not be much useful in classification. still we will keep it and train a model with it and see the performance of the model and them again we will train a model without these features and check the performance of the model and see which is better**","7f0ab2c3":"# One hot encoding 'Subject' feature","3812b1f0":"# Featurization of text data","bf6c4bab":"Using vocabulary from train data only ","d10acb06":"## downloading stopwords from nltk","0a3b84fb":"to load sparse matrix: sparse_matrix = scipy.sparse.load_npz('\/tmp\/sparse_matrix.npz')","6149f75d":"# getting X_train data ready in sparse matrix form","4544c892":"### train data","06e51d75":"## training model with best parameters","708d1833":"**not too useful but in case of 'avg_length_text' there seem to be too many outliers i.e. too many false news text have average length of words very high. this might be useful**","e8ce0e94":"## TF-IDF weighted W2V vectorization of 'title' feature","6ef848e9":"# Functions for preprocessing text data"}}