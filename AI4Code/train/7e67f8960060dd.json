{"cell_type":{"914216b8":"code","214e76ed":"code","e87fe43f":"code","8bab9f99":"code","4df6da19":"code","6fa83b38":"code","be2cda74":"code","42299d91":"code","25a521e5":"code","bfa1375d":"code","5a2ed4cc":"code","c967fd48":"code","56450e2a":"code","e5952b91":"code","aa68f260":"code","7dc05600":"code","ff082116":"code","9921c82c":"code","d464a042":"code","0524c950":"code","3b7e34a0":"code","fd9531b2":"code","cd6085ee":"code","3a9e262f":"code","fadea55d":"code","e14d97ea":"code","63cea5d4":"code","15c26e49":"code","81cd3859":"code","9a74eebb":"code","c5e6b4f0":"code","0c012b9f":"code","047f3c29":"code","77262d36":"code","385f1756":"code","d205ff7a":"code","6fb7eb58":"code","e9ea6a0b":"code","3c3e3107":"code","f0eff01e":"code","b74465be":"code","b28375a2":"code","00943018":"code","45ec721d":"markdown","695fb7f9":"markdown","b280386b":"markdown","442f9d40":"markdown"},"source":{"914216b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","214e76ed":"\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","e87fe43f":"train = pd.read_csv(r'\/kaggle\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')\ntest = pd.read_csv(r'\/kaggle\/input\/loan-prediction-problem-dataset\/test_Y3wMUE5_7gLdaTN.csv')","8bab9f99":"##checking the shape of both train and test dataset\nprint(train.shape)\nprint(test.shape)","4df6da19":"##get the column names in the data\n#although its quiet understood that the test data will not contain the label i.e. Loan Status , but still\n#we shall take a look\n\ncols_train = train.columns\ncols_test = test.columns\nprint(\"Train data Column names : \")\nprint(cols_train)\nprint('_____________________________')\nprint(\"Train data Column names : \")\nprint(cols_test)","6fa83b38":"train.head()","be2cda74":"test.head()","42299d91":"## Info about the data using .info and .describe\ntrain.info()\ntest.info()","25a521e5":"##describe\ntrain.describe()","bfa1375d":"test.describe()","5a2ed4cc":"#converting Credit History to Object datatype as its of for 0 and 1\ntrain['Credit_History'] = train['Credit_History'].astype('O')\ntest['Credit_History'] = test['Credit_History'].astype('O')","c967fd48":"print(train.info())\nprint(test.info())","56450e2a":"## describe object type columns\ntrain.describe(include = 'O')","e5952b91":"# we will drop ID because it's not important for our model and it will just mislead the model\n\ntrain.drop('Loan_ID', axis=1, inplace=True)\ntest.drop('Loan_ID', axis=1, inplace=True)","aa68f260":"train.head()","7dc05600":"#check if we have any duplicate rows \nprint(train.duplicated().any())\nprint(test.duplicated().any())","ff082116":"len(test)-len(test.drop_duplicates())\n\n## dropping the duplicates in test data\ntest = test.drop_duplicates()\ntrain['Loan_Status'].value_counts()","9921c82c":"# let's look at the target percentage\n\nplt.figure(figsize=(8,6))\nsns.set(style=\"darkgrid\")\nsns.countplot(train['Loan_Status']);\n\nprint('The percentage of Y class : %.2f' % (train['Loan_Status'].value_counts()[0] \/ len(train)))\nprint('The percentage of N class : %.2f' % (train['Loan_Status'].value_counts()[1] \/ len(train)))\n\n# We can consider it as imbalanced data, we shall use F1 Score, Precision and Recall to evaluate the Prediction","d464a042":"train.head()","0524c950":"## finding the null values and treating them\nheat = sns.heatmap(train.isnull(), cbar=False)\nplt.show()\nNull_percent = train.isna().mean().round(4)*100\n\nNull_percent = pd.DataFrame({'Null_percentage' : Null_percent})\nNull_percent.head()\nNull_percent = Null_percent[Null_percent.Null_percentage > 0].sort_values(by = 'Null_percentage', ascending = False)\nprint(\"Percentage of Null cells : \\n \\n \" , Null_percent)","3b7e34a0":"null_klmns = Null_percent.index\nnull_klmns = list(null_klmns)\ntrain[null_klmns].info()","fd9531b2":"#we shall separate the categorical and numeric columns\ncat_data = []\nnum_data = []\n\nfor i,c in enumerate(train.dtypes):\n    if c == object:\n        cat_data.append(train.iloc[:, i])\n    else :\n        num_data.append(train.iloc[:, i])\n\ncat_data = pd.DataFrame(cat_data).transpose()\nnum_data = pd.DataFrame(num_data).transpose()\ncat_data.head()","cd6085ee":"num_data.head()","3a9e262f":"##EDA for numerical data\nfor i in num_data.columns:\n    print(i)\n    sns.set(style=\"whitegrid\")\n    sns.boxplot(num_data[i])\n    plt.show()\n    \n##EDA for categorical data\nfor i in cat_data.columns:\n    print(i)\n    total = float(len(cat_data))\n    plt.figure(figsize=(8,10))\n    sns.set(style=\"whitegrid\")\n    ax = sns.countplot(cat_data[i])\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,height + 3,'{:1.2f}'.format(height\/total),ha=\"center\") \n    plt.show()","fadea55d":"## EDA on categorical data relative to Loan Status\n##EDA for categorical data\nfor i in cat_data.columns:\n    print(i)\n    total = float(len(cat_data))\n    plt.figure(figsize=(8,10))\n    sns.set(style=\"darkgrid\")\n    ax = sns.countplot(x = i, hue = 'Loan_Status', data = cat_data)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,height + 3,'{:1.2f}'.format(height\/total),ha=\"center\") \n    plt.show()\n    \n#from the corresponding charts one can gain a lot of important info","e14d97ea":"##creating a pair plot\nsns.pairplot(train)\nplt.show()","63cea5d4":"## Data Imputation\n# for cat_data\n\n# If you want to fill every column with its own most frequent value you can use\n\ncat_data = cat_data.apply(lambda x:x.fillna(x.value_counts().index[0]))\ncat_data.isnull().sum().any() # no more missing data","15c26e49":"#num_data\n##as we have many outliers in columns with Numerical data, we shall impute the blank cells with the median of their respective columns\n\nnum_data.fillna(num_data.median(), inplace=True)\nnum_data.isnull().sum().any() # no more missing data \nnum_data.head()","81cd3859":"## num_data has certain columns with some very high valued columns and some very low, thus we \n#should standardize the values of these columns\n\nfor col in num_data.columns:\n    num_data[col] = (num_data[col]-num_data[col].min())\/(num_data[col].max() - num_data[col].min())\n    \nnum_data.head()","9a74eebb":"##Label Encoding\nfrom sklearn.preprocessing import LabelEncoder  \nle = LabelEncoder()\ncat_data.head()","c5e6b4f0":"# transform the target column\n\ntarget_values = {'Y': 1 , 'N' : 0}\n\ntarget = cat_data['Loan_Status']\ncat_data.drop('Loan_Status', axis=1, inplace=True)\n\ntarget = target.map(target_values)","0c012b9f":"# transform other columns\n\nfor i in cat_data:\n    cat_data[i] = le.fit_transform(cat_data[i])","047f3c29":"cat_data.head()","77262d36":"df = pd.concat([cat_data, num_data, target], axis=1)\ndf.head()","385f1756":"#confirming if we have any null values\n\ndf.isna().any()\n\n#so we are good to model","d205ff7a":"## Creating our variable and target dataset\nX = pd.concat([cat_data, num_data], axis=1)\ny = target","6fb7eb58":"# we will use 4 different models for training\n\n## ---------------------------All in one modelling---------------------------\n\nfrom sklearn.model_selection import train_test_split  #to split the dataset for training and testing\nfrom sklearn.linear_model import LogisticRegression  # for Logistic Regression algorithm\nfrom sklearn.tree import DecisionTreeClassifier #for using Decision Tree Algoithm\nfrom sklearn import svm  #for Support Vector Machine (SVM) Algorithm\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours\nfrom sklearn import metrics #for checking the model accuracy\nfrom sklearn.metrics import precision_score, recall_score, f1_score, log_loss, accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import StratifiedShuffleSplit\n","e9ea6a0b":"# we will use StratifiedShuffleSplit to split the data Taking into consideration that we will get the same ratio on the target column\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train, test in sss.split(X, y):\n    X_train, X_test = X.iloc[train], X.iloc[test]\n    y_train, y_test = y.iloc[train], y.iloc[test]\n    \nprint('X_train shape', X_train.shape)\nprint('y_train shape', y_train.shape)\nprint('X_test shape', X_test.shape)\nprint('y_test shape', y_test.shape)\n\n# almost same ratio\nprint('\\nratio of target in y_train :',y_train.value_counts().values\/ len(y_train))\nprint('ratio of target in y_test :',y_test.value_counts().values\/ len(y_test))\nprint('ratio of target in original_data :',df['Loan_Status'].value_counts().values\/ len(df))","3c3e3107":"models = {\n    'LogisticRegression': LogisticRegression(random_state=34),\n    'KNeighborsClassifier': KNeighborsClassifier(n_neighbors= 5),\n    'SVC': SVC(random_state=34),\n    'DecisionTreeClassifier': DecisionTreeClassifier(max_depth=1, random_state=34)\n}","f0eff01e":"# loss\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score, log_loss, accuracy_score\n\ndef loss(y_true, y_pred, retu=False):\n    pre = precision_score(y_true, y_pred)\n    rec = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    loss = log_loss(y_true, y_pred)\n    acc = accuracy_score(y_true, y_pred)\n    \n    if retu:\n        return pre, rec, f1, loss, acc\n    else:\n        print('  pre: %.3f\\n  rec: %.3f\\n  f1: %.3f\\n  loss: %.3f\\n  acc: %.3f' % (pre, rec, f1, loss, acc))","b74465be":"# train_eval_train\n\ndef train_eval_train(models, X, y):\n    for name, model in models.items():\n        print(name,':')\n        model.fit(X, y)\n        loss(y, model.predict(X))\n        print('-'*30)\n        \ntrain_eval_train(models, X_train, y_train)\n\n# we can see that best model is LogisticRegression at least for now, SVC is just memorizing the data so it is overfitting .","b28375a2":"X_train.shape","00943018":"from sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n\ndef train_eval_cross(models, X, y, folds):\n    # we will change X & y to dataframe because we will use iloc (iloc don't work on numpy array)\n    X = pd.DataFrame(X) \n    y = pd.DataFrame(y)\n    idx = [' pre', ' rec', ' f1', ' loss', ' acc']\n    for name, model in models.items():\n        ls = []\n        print(name,':')\n\n        for train, test in folds.split(X, y):\n            model.fit(X.iloc[train], y.iloc[train]) \n            y_pred = model.predict(X.iloc[test]) \n            ls.append(loss(y.iloc[test], y_pred, retu=True))\n        print(pd.DataFrame(np.array(ls).mean(axis=0), index=idx)[0])  #[0] because we don't want to show the name of the column\n        print('-'*30)\n        \ntrain_eval_cross(models, X, y, skf)\n","45ec721d":"# Classification of Loan Status\n#In this notebook, I shall try to show a few ways to create a classification model.\n\nKey Highlights will be :\n\n1. Data Preparation\n2. Visualization and Descriptive analytics (EDA)\n3. Data Imputation\n4. System of multiple models\n5. System of multiple Model quality measure (accuracy score, f1 score, precision, recall)\n6. Cross validation using K folds\n\nDont forget to upvote if you find the notebook useful. Your comments and support will definitely act as a motivator and I shall publish more of my work.","695fb7f9":"Creating a function to calculate various accuracy measures","b280386b":"We shall use 4 different models","442f9d40":"Train cross validation model"}}