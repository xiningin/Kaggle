{"cell_type":{"7d869b4e":"code","8f91ff33":"code","dd50216f":"code","3b998030":"code","e621ab05":"code","dead6f14":"code","e32109c6":"code","02ef1057":"code","592a6be2":"code","641e0680":"code","7caecb12":"code","77752067":"code","18ef7428":"code","112f0078":"code","90eb825e":"code","99b27f8b":"code","4f04409a":"code","d3605b5f":"code","97ae3925":"code","d737304e":"code","baaca05d":"code","d0ed667a":"code","9c444b67":"code","39641247":"code","a877dace":"code","6b83ca98":"code","e7b705b4":"code","2f5535ab":"code","b059bb18":"code","6cfe95fe":"markdown","b70c0ab1":"markdown","decdd57e":"markdown","32a495b0":"markdown","47b03880":"markdown","5169b451":"markdown","5dfef8a4":"markdown"},"source":{"7d869b4e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8f91ff33":"train_data_df = pd.read_csv('..\/input\/training_set.csv')\ntrain_metadata_df = pd.read_csv('..\/input\/training_set_metadata.csv')","dd50216f":"train_data_df['flux_ratio_sq'] = np.power(train_data_df['flux'] \/ train_data_df['flux_err'], 2.0)\ntrain_data_df['flux_by_flux_ratio_sq'] = train_data_df['flux'] * train_data_df['flux_ratio_sq']","3b998030":"data_features = train_data_df.columns[1:]\nmetadata_features = train_metadata_df.columns[1:]","e621ab05":"groupObjects = train_data_df.groupby('object_id')[data_features]\n\nprint(\"Add constant object features\")\nfeatures = train_metadata_df.drop(['target'], axis=1)\n\nprint(\"Add mean of mutable object features\")\nfeatures = pd.merge(features, groupObjects.agg('mean'), how='right', on='object_id', suffixes=['', '_mean'])\n\nprint(\"Add sum of mutable object features\")\nfeatures = pd.merge(features, groupObjects.agg('sum'), how='right', on='object_id', suffixes=['', '_sum'])\n\nprint(\"Add median of mutable features\")\nfeatures = pd.merge(features, groupObjects.agg('median'), how='right', on='object_id', suffixes=['', '_median'])\n\nprint(\"Add minimum of mutable features\")\nfeatures = pd.merge(features, groupObjects.agg('min'), how='right', on='object_id', suffixes=['', '_min'])\n\nprint(\"Add maximum of mutable features\")\nfeatures = pd.merge(features, groupObjects.agg('max'), how='right', on='object_id', suffixes=['', '_max'])\n\nprint(\"Add range of mutable features\")\nfeatures = pd.merge(features, groupObjects.agg(lambda x: max(x) - min(x)), how='right', on='object_id', suffixes=['', '_range'])\n\nprint(\"Add standard deviation of mutable features\")\nfeatures = pd.merge(features, groupObjects.agg('std'), how='right', on='object_id', suffixes=['', '_stddev'])\n\nprint(\"Add skew of mutable features\")\nfeatures = pd.merge(features, groupObjects.agg('skew'), how='right', on='object_id', suffixes=['', '_skew'])","dead6f14":"features = features.fillna(features.mean())","e32109c6":"features","02ef1057":"import tensorflow as tf","592a6be2":"import keras\nfrom keras.utils import to_categorical","641e0680":"train_metadata_df['target'] = train_metadata_df.target.map({6:0, 15:1, 16:2, 42:3, 52:4, 53:5, 62:6, 64:7, 65:8, 67:9, 88:10, 90:11, 92:12, 95:13})\ntargets = train_metadata_df['target']","7caecb12":"import gplearn\nfrom gplearn.genetic import SymbolicTransformer","77752067":"function_set = ['add', 'sub', 'mul', 'div',\n                'sqrt', 'log', 'abs', 'neg', 'inv',\n                'max', 'min']\n\ngp = SymbolicTransformer(generations=100, population_size=2000,\n                         hall_of_fame=100, n_components=10,\n                         function_set=function_set,\n                         parsimony_coefficient=0.0005,\n                         max_samples=0.9, verbose=1,\n                         random_state=0, n_jobs=3)\n\ngp.fit(features.drop('object_id', axis=1).values, targets.values)","18ef7428":"engineered_features = gp._programs\n\nfor i in range(len(engineered_features)):\n    for engineered_feature in engineered_features[i]:\n        if engineered_feature != None:\n            print(engineered_feature)","112f0078":"new_features = pd.DataFrame(gp.transform(features.drop('object_id', axis=1).values))\nfeatures = pd.concat([features, new_features], axis=1, join_axes=[features.index])","90eb825e":"features","99b27f8b":"targets = to_categorical(targets)","4f04409a":"features = features.drop(['object_id'], axis=1).values","d3605b5f":"import sklearn\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(-1, 1)).fit(features)\nfeatures = scaler.transform(features)","97ae3925":"train_features = features [:np.int32(0.8*len(features))]\ntrain_targets = targets [:np.int32(0.8*len(features))]\n\nval_features = features[np.int32(0.8*len(features)):]\nval_targets = targets[np.int32(0.8*len(features)):]","d737304e":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom keras.regularizers import L1L2","baaca05d":"model = Sequential()\n\nmodel.add(Dense(30, activation='relu')) # kernel_regularizer=L1L2(l1=0.00, l2=0.01), bias_regularizer=L1L2(l1=0.00, l2=0.01)\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\n\nmodel.add(Dense(40, activation='relu')) # kernel_regularizer=L1L2(l1=0.00, l2=0.01), bias_regularizer=L1L2(l1=0.00, l2=0.01)\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(50, activation='relu')) # kernel_regularizer=L1L2(l1=0.00, l2=0.01), bias_regularizer=L1L2(l1=0.00, l2=0.01)\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(10, activation='relu')) # kernel_regularizer=L1L2(l1=0.00, l2=0.01), bias_regularizer=L1L2(l1=0.00, l2=0.01)\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(targets.shape[1], activation='softmax')) # kernel_regularizer=L1L2(l1=0.00, l2=0.01), bias_regularizer=L1L2(l1=0.00, l2=0.01)","d0ed667a":"model.compile(loss='categorical_crossentropy', optimizer='rmsprop')","9c444b67":"model.fit(train_features, train_targets, validation_data=(val_features, val_targets), epochs=500)","39641247":"import time\n\ntest_metadata_df = pd.read_csv('..\/input\/test_set_metadata.csv')\n\n# print(\"Add constant object features\")\ntest_metadata_df = test_metadata_df.fillna(test_metadata_df.mean())\n\npredictions = []\nobject_ids = []\n\nchunks = 5000000\ntotal = 0\n\nfor i_c, test_data_df in enumerate(pd.read_csv('..\/input\/test_set.csv', chunksize=chunks, iterator=True)):\n    startTime = time.time()\n    \n    test_data_df['flux_ratio_sq'] = np.power(test_data_df['flux'] \/ test_data_df['flux_err'], 2.0)\n    test_data_df['flux_by_flux_ratio_sq'] = test_data_df['flux'] * test_data_df['flux_ratio_sq']\n    \n    groupObjects = test_data_df.fillna(test_data_df.mean()).groupby('object_id')[data_features]\n\n    # print(\"Add mean of mutable object features\")\n    features = groupObjects.agg('mean')\n    \n    # print(\"Add sum of mutable object features\")\n    features = pd.merge(features, groupObjects.agg('sum'), how='right', on='object_id', suffixes=['', '_sum'])\n\n    # print(\"Add median of mutable features\")\n    features = pd.merge(features, groupObjects.agg('median'), how='right', on='object_id', suffixes=['', '_median'])\n\n    # print(\"Add minimum of mutable features\")\n    features = pd.merge(features, groupObjects.agg('min'), how='right', on='object_id', suffixes=['', '_min'])\n\n    # print(\"Add maximum of mutable features\")\n    features = pd.merge(features, groupObjects.agg('max'), how='right', on='object_id', suffixes=['', '_max'])\n\n    # print(\"Add range of mutable features\")\n    features = pd.merge(features, groupObjects.agg(lambda x: max(x) - min(x)), how='right', on='object_id', suffixes=['', '_range'])\n\n    # print(\"Add standard deviation of mutable features\")\n    features = pd.merge(features, groupObjects.agg('std'), how='right', on='object_id', suffixes=['', '_stddev'])\n    \n    # print(\"Add skew of mutable features\")\n    features = pd.merge(features, groupObjects.agg('skew'), how='right', on='object_id', suffixes=['', '_skew'])\n    \n    test_features = pd.merge(test_metadata_df, features, on='object_id')\n    \n    new_features = pd.DataFrame(gp.transform(test_features.drop('object_id', axis=1).values))\n    test_features = pd.concat([test_features, new_features], axis=1, join_axes=[test_features.index])\n    \n    object_ids.extend(list(test_features['object_id']))\n    test_features = test_features.drop(['object_id'], axis=1).values\n    \n    test_features = scaler.transform(test_features)\n    total = total + len(test_features)\n    \n    predictions.extend(model.predict(test_features))\n\n    endTime = time.time()\n    \n    print(\"Iteration : \" + str(i_c))\n    print(\"Time taken : \" + str(endTime - startTime) + \" s\")\n    print(\"Total objects predicted on : \" + str(total))\n    print(\"\")","a877dace":"predictions = pd.DataFrame(predictions)","6b83ca98":"predictions.columns = ['class_6', 'class_15', 'class_16', 'class_42', 'class_52', 'class_53', 'class_62', 'class_64', 'class_65', 'class_67', 'class_88', 'class_90', 'class_92', 'class_95']","e7b705b4":"predictions['class_99'] = 1 - predictions.max(axis=1)\npredictions['object_id'] = object_ids","2f5535ab":"predictions","b059bb18":"predictions.to_csv('plasticc_submission_file.csv', index=False)","6cfe95fe":"**Compile the model with Categorical Cross Entropy and RMSProp as the loss function and optimizer of the model respectively. **","b70c0ab1":"**Engineer new features using Genetic Programming with the gplearn library.**","decdd57e":"**Train the model and check performance on unseen data with a validation data set.**","32a495b0":"**The class-99 probability is calculated as 1 minus the maximum probability predicted by the model.**","47b03880":"**Iterate over the test data in batches and make predictions on each batch using the trained model. **","5169b451":"**Build a simple Deep Neural Network using Dense, Dropout and BatchNormalization layers in Keras**","5dfef8a4":"**Prepare predictions for submission.**"}}