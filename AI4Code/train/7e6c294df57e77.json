{"cell_type":{"20cb7d9d":"code","85ab8522":"code","dab3dbe2":"code","96d976b6":"code","bcc029de":"code","d8cfcb40":"code","52f8a331":"code","5c61b20c":"code","2eebdade":"code","70aadae9":"code","2be55a1b":"code","80a9fe93":"code","b580d522":"code","877ed59b":"code","afdbad0d":"code","6e9de9bb":"code","b2c62b13":"code","6cd02c1f":"code","3964e53a":"code","a8935c47":"code","5149203d":"code","806d9fcd":"code","231cd87f":"code","83774a75":"code","4028ec08":"code","799c9608":"code","cd0aff45":"code","146effcc":"code","6f8a6969":"code","e88d5e64":"code","41734d7b":"code","523525a7":"code","f9f7df26":"code","cd4742e5":"code","c3250fd1":"code","f7e61e96":"code","1963609f":"code","77d2b41a":"code","2225f148":"code","5fcde859":"code","25963c6f":"code","086cd021":"code","2788d93a":"markdown","b3e6087a":"markdown","f7dc6609":"markdown","37a58b01":"markdown","6ed12cc7":"markdown","c69019e4":"markdown","6dde3328":"markdown","cd17823a":"markdown","5de8f6f4":"markdown","69fdb4ae":"markdown","5fffe24b":"markdown","1f0a7bd6":"markdown","0378fab4":"markdown","9a2a5909":"markdown","0b0735f8":"markdown","74229725":"markdown","4c356e8a":"markdown","9dd7cdde":"markdown","55005caf":"markdown","0fd2293e":"markdown","b9111100":"markdown","eff33411":"markdown","f89d6e3c":"markdown","0df5e4aa":"markdown","aec9d124":"markdown","76d6d9d5":"markdown","f899d4d8":"markdown","fc830f30":"markdown","c67e91b4":"markdown","65bf8de2":"markdown","da8ef5dd":"markdown","d53c3e07":"markdown","68250ff0":"markdown","d07eb6da":"markdown","c53c8d04":"markdown","890ab876":"markdown","6ec03b83":"markdown"},"source":{"20cb7d9d":"import wandb\nimport os\n# os._Environ\u306f\u74b0\u5883\u5909\u6570\u540dkey\u3068\u5024value\u304c\u5bfe\u306b\u306a\u3063\u305f\u30de\u30c3\u30d7\u578b\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\n# print(os.environ)\n# wandb\u3068\u306eAPI\u63a5\u7d9a\u3092\u6697\u53f7\u5316\u3059\u308b\n#!wandb login $api_key","85ab8522":"\"\"\"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"edc8af4e0cd1f3bba30aaea945348675bb6346de\")\n\nos.environ[\"WANDB_SILENT\"] = \"true\"\nCONFIG = {'competition': 'siim-fisabio-rsna', '_wandb_kernel': 'ruch'}\"\"\"","dab3dbe2":"import pandas as pd\nimport pandas_profiling\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pydicom \nimport random\nimport albumentations as A\nfrom sklearn.model_selection import train_test_split\nfrom fastai.vision.all import *\nfrom fastai.medical.imaging import *\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut","96d976b6":"%cd input","bcc029de":"train_study_df = pd.read_csv(\"..\/input\/siim-covid19-detection\/train_study_level.csv\")\ntrain_image_data = pd.read_csv(\"..\/input\/siim-covid19-detection\/train_image_level.csv\")","d8cfcb40":"print(train_study_df.shape)\ntrain_study_df.head()","52f8a331":"study_class = [\"Negative for Pneumonia\", \"Typical Appearance\",\"Indeterminate Appearance\", \"Atypical Appearance\"]\nplt.figure(figsize = (10,5))\nplt.bar([1,2,3,4], train_study_df[study_class].values.sum(axis=0))\nplt.xticks([1,2,3,4],study_class)\nplt.ylabel('Frequency')\nplt.show()","5c61b20c":"train_image_data.head()","2eebdade":"train_image_data['split_label'] = train_image_data.label.apply(lambda x: [x.split()[offs:offs+6] for offs in range(0, len(x.split()), 6)])\n# show the split_label\ntrain_image_data['split_label'][:5]","70aadae9":"train_image_data['split_label'].values[0]","2be55a1b":"classes_freq = []\nfor i in range(len(train_image_data)):\n    for j in train_image_data.iloc[i].split_label: classes_freq.append(j[0])\nplt.hist(classes_freq)\nplt.ylabel('Frequency')","80a9fe93":"image_data_path = \"..\/input\/siim-covid19-detection\/train\"\n\n# show image\n#cv2.imshow(\"image_data\")","b580d522":"# pixel_array\u30d7\u30ed\u30d1\u30c6\u30a3\u3092\u7528\u3044\u308b\u3053\u3068\u3067\u753b\u50cf\u30c7\u30fc\u30bf\u304cNumPy\u306endarray\u3068\u3057\u3066\u53d6\u5f97\u3059\u308b\n\ndef dicom2array(path, voi_lut=True, fix_monochrome=True):\n    dicom = pydicom.read_file(path)\n    # transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return data\n\n\ndef plot_img(img, size=(7, 7),is_rgb=True, title=\"\", cmap='grap'):\n    plt.figure(figsize=size)\n    plt.imshow(img, cmap=camp)\n    plt.suptitle(title)\n    plt.show()\n    \ndef plot_imgs(imgs, cols=4, size=7,is_rgb=True ,title=\"\", cmap='gray', img_size=(500, 500)):\n    rows = len(imgs) \/\/ cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size))\n    for i, img in enumerate(imgs):\n        if img_size is not None:\n            img = cv2.resize(img, img_size)\n        fig.add_subplot(rows, cols, i+1)\n        plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show","877ed59b":"dataset_path = Path('..\/input\/siim-covid19-detection')\ndicom_paths = get_dicom_files(dataset_path\/'train')\nimgs = [dicom2array(path) for path in dicom_paths[:4]]\nplot_imgs(imgs)","afdbad0d":"num_images_per_study = []\nfor i in (dataset_path\/'train').ls():\n    num_images_per_study.append(len(get_dicom_files(i)))\n    if len(get_dicom_files(i)) > 5:\n        print(f'Study {i} had {len(get_dicom_files(i))} images')","6e9de9bb":"plt.hist(num_images_per_study)","b2c62b13":"# \u4e00\u81f4\u3059\u308b\u30d5\u30a1\u30a4\u30eb\u3092\u62bd\u51fa\ndef image_path(row):\n    study_path = dataset_path\/'train'\/row.StudyInstanceUID\n    for i in get_dicom_files(study_path):\n        # \u62e1\u5f35\u5b50\u306a\u3057\u306e\u30d5\u30a1\u30a4\u30eb\u540d\u306e\u6587\u5b57\u5217\u306fstem\u5c5e\u6027\u3067\u53d6\u5f97\n        if row.id.split('_')[0] == i.stem: return i\n    \n\ntrain_image_data['image_path'] = train_image_data.apply(image_path, axis=1)","6cd02c1f":"train_image_data.head()","3964e53a":"imgs = []\nimage_paths = train_image_data['image_path'].values\n# ex ('..\/input\/siim-covid19-detection\/train\/5776db0cec75\/81456c9c5423\/000a312787f2.dcm')\n\nthickness = 10\nscale = 5\n\n\nfor i in range(8):\n    image_path = random.choice(image_paths)\n    print(image_path)\n    img = dicom2array(path=image_path)\n    img = cv2.resize(img, None, fx=1\/scale, fy=1\/scale)\n    img = np.stack([img, img, img], axis=-1)\n    for i in train_image_data.loc[train_image_data['image_path'] == image_path].split_label.values[0]:\n        if i[0] == 'opacity':\n            img = cv2.rectangle(img, (int(float(i[2])\/ scale), int(float(i[3])\/ scale)),\n                                     (int(float(i[4])\/ scale), int(float(i[5])\/ scale)),\n                                     [0, 255, 0], thickness)\n    img = cv2.resize(img, (500, 500))\n    imgs.append(img)\n            \nplot_imgs(imgs, cmap=None)\n\n###\n# split_label.values ex) ['opacity', '1', '789.28836', '582.43035', '1815.94498', '2499.73327']","a8935c47":"%cd ..\/kaggle\n#!mkdir tmp\n%cd tmp","5149203d":"# Download YOLOv5\n!git clone https:\/\/github.com\/ultralytics\/yolov5\n%cd yolov5\n# Install dependecies\n%pip install -qr requirements.txt\n%cd ..\/\nimport torch\n# \u5b66\u7fd2\u56de\u3059\u3068\u304d\u306bGPU\u3092ON\u306b\u3059\u308b\nprint(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")","806d9fcd":"TRAIN_PATH = 'input\/siim-covid19-resized-to-256px-jpg\/train\/'\nIMG_SIZE = 256\nBATCH_SIZE = 16\nEPOCHS = 10","231cd87f":"%cd ..","83774a75":"#%cd kaggle\ndf = pd.read_csv(\"input\/siim-covid19-detection\/train_image_level.csv\")","4028ec08":"df['id'] = df.apply(lambda row: row.id.split('_')[0], axis = 1)\ndf['path'] = df.apply(lambda row: TRAIN_PATH+row.id+'.jpg', axis=1)\ndf['image_level'] = df.apply(lambda row: row.label.split(' ')[0], axis=1)","799c9608":"meta_df = pd.read_csv('input\/siim-covid19-resized-to-256px-jpg\/meta.csv')","cd0aff45":"train_meta_df = meta_df.loc[meta_df.split == 'train'] # select_train\ntrain_meta_df = train_meta_df.drop('split', axis=1) # delete_test\ntrain_meta_df.columns = ['id', 'dim0', 'dim1']","146effcc":"# Merge both the dataframes   why??\ndf = df.merge(train_meta_df, on='id', how=\"left\")\ndf.head()","6f8a6969":"train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df.image_level.values)\n\n# ignore warning\ntrain_df.loc[:, 'split'] = 'train'\nvalid_df.loc[:, 'split'] = 'valid'\n\ndf = pd.concat([train_df, valid_df]).reset_index(drop=True)","e88d5e64":"print(f'Size of dataset: {len(df)}, training images: {len(train_df)}. validation images: {len(valid_df)}')","41734d7b":"os.makedirs('tmp\/covid\/images\/train', exist_ok=True)\nos.makedirs('tmp\/covid\/images\/valid', exist_ok=True)\n! ls tmp\/covid\/images","523525a7":"# Move the images to relevant split folder.  (Need??)\nfrom tqdm import tqdm\nfrom shutil import copyfile\nfor i in tqdm(range(len(df))):\n    row = df.loc[i]\n    if row.split == 'train':\n        copyfile(row.path, f'tmp\/covid\/images\/train\/{row.id}.jpg')\n    else:\n        copyfile(row.path, f'tmp\/covid\/images\/valid\/{row.id}.jpg')","f9f7df26":"%cd tmp","cd4742e5":"%pwd","c3250fd1":"import yaml\n\ndata_yaml = dict(\n    train = '..\/covid\/images\/train',\n    val = '..\/covid\/images\/valid',\n    nc = 2,\n    names = ['none', 'opacity']\n)\n\nwith open('data\/data.yaml', 'w') as outfile:\n    yaml.dump(data_yaml, outfile, default_flow_style=True)\n\n%cat data\/data.yaml","f7e61e96":"# \u5b66\u7fd2\u5b9f\u884c\n\"\"\"\n!python train.py --img {IMG_SIZE} \\\n                 --batch {BATCH_SIZE} \\\n                 --epochs {EPOCHS} \\\n                 --data data.yaml \\\n                 --weights yolov5s.pt \\\n                 --save_period 1\\\n                 --project kaggle-siim-covid\n\"\"\"","1963609f":"TEST_PATH = '\/kaggle\/input\/siim-covid19-resized-to-256px-jpg\/test\/' # absolute path\nMODEL_PATH = 'kaggle-siim-covid\/exp\/weights\/best.pt'\n","77d2b41a":"\"\"\"\n!python detect.py --weights {MODEL_PATH} \\\n                  --source {TEST_PATH} \\\n                  --img {IMG_SIZE} \\\n                  --conf 0.281 \\\n                  --iou-thres 0.5 \\\n                  --max-det 3 \\\n                  --save-txt \\\n                  --save-conf\n\"\"\"","2225f148":"submission_df = pd.read_csv(dataset_path\/'sample_submission.csv')","5fcde859":"submission_df.head()","25963c6f":"submission_df.iloc[2000:2010]","086cd021":"submission_df.to_csv(\"submission.csv\", index=False)","2788d93a":"### Libarary","b3e6087a":"### Look at train_study_level.csv \/ train_image_level.csv","f7dc6609":"Download Yolov% repository in temp directory","37a58b01":"The Process of EDA\n1. Data visualization\n2. Feature select\n3. Feature engineering\n4. fill in missing value","6ed12cc7":"###  model YOLO5","c69019e4":"### look at the images \n","6dde3328":"We have our bounding box labels provided in the label column. The format is as follows:  \n[class ID] [confidence score] [bounding box]","cd17823a":"#### Let's actually look at how many images are available per study:","5de8f6f4":"```\n--weights {MODEL_PATH} \\ # path to the best model.\n--source {TEST_PATH} \\ # absolute path to the test images.\n--img {IMG_SIZE} \\ # Size of image\n--conf 0.281 \\ # Confidence threshold (default is 0.25)\n--iou-thres 0.5 \\ # IOU threshold (default is 0.45)\n--max-det 3 \\ # Number of detections per image (default is 1000) \n--save-txt \\ # Save predicted bounding box coordinates as txt files\n--save-conf # Save the confidence of prediction for each bounding box\n```","69fdb4ae":"## submit","5fffe24b":"### target label distribution","1f0a7bd6":"## Training","0378fab4":"### Prepare Dataset","9a2a5909":"## EDA","0b0735f8":"* train_study_level.csv - the train study-level metadata, with one row for each study, including correct labels.\n* train_image_level.csv - the train image-level metadata, with one row for each image, including both correct labels and any bounding boxes in a dictionary format.  \nSome images in both test and train have multiple bounding boxes.\n* sample_submission.csv - a sample submission file containing all image- and study-level IDs.\n* train folder - comprises 6,334 chest scans in DICOM format, stored in paths with the form study\/series\/image\n* test folder - The hidden test dataset is of roughly the same scale as the training dataset.","74229725":"## Goal \n### Identifying and localizing COVID-19 abnormalities on chest radiographs","4c356e8a":"### Inference","9dd7cdde":"### Create .YANL file","55005caf":"#### Load meta.csv file","0fd2293e":"#### look at image appled boundig box","b9111100":"## Modeling","eff33411":"sample1 https:\/\/propen.dream-target.jp\/blog\/python_albumentations  \nsample2 https:\/\/qiita.com\/Takayoshi_Makabe\/items\/79c8a5ba692aa94043f7","f89d6e3c":"```\n--img {IMG_SIZE} \\ # Input image size.\n--batch {BATCH_SIZE} \\ # Batch size\n--epochs {EPOCHS} \\ # Number of epochs\n--data data.yaml \\ # Configuration file\n--weights yolov5s.pt \\ # Model name\n--save_period 1\\ # Save model after interval\n--project kaggle-siim-covid # W&B project name\n```","0df5e4aa":"# Data","aec9d124":"## Hyperparameters Set","76d6d9d5":"Prepare Folder ","f899d4d8":"#### look at the distribution of opacity vs none:","fc830f30":"### set up W&B\n* save learning parameter\n* vizualiztion image file","c67e91b4":"#### prepare required folder structure","65bf8de2":"# Albumenatations","da8ef5dd":"#### Use W&B","d53c3e07":"### Show Distribution \ntrain_study_level.csv ","68250ff0":"#### Train-Validation split","d07eb6da":"Chose model YOLOv5s\u3001YOLOv5m\u3001YOLOv5l\u3001YOLOv5x (big order \u21c4 GPU memory)  \nuse YOLOv5s first","c53c8d04":"## The Domain Knowledge\n#### \u2605Ground glass opacties\nground glass opacities (GGOs, for short) indicate abnormalities in the lungs. \"Ground glass opacities [are] a pattern that can be seen when the lungs are sick,\" says Dr. Cortopassi. She adds that, while normal lung CT scans appear black, an abnormal chest CT with GGOs will show lighter-colored or gray patches.\n\n#### \u2605Opacity(\u4e0d\u900f\u660e\u5ea6)\nthe degree of transparenet(x-ray image)\n\n#### 1. Typical Appearance  \nCommonly reported imaging features of greater specificity for COVID-19 pneumonia.\n#### 2. Atypical Appearance  \nUncommonly or not reported features of COVID-19 pneumonia.\n#### 3. Indeterminate Appearance(\u4e0d\u78ba\u5b9a)   \nNonspecific imaging features of COVID-19 pneumonia.\n#### 4. Negative for Pneumonia(\u9670\u6027\uff09 \n\n#### boxes\nbounding boxes in easily-readable dictionary format\n\n#### DICOM format\nAny DICOM medical image consists of two parts\u2014a header and the actual image itself. The header consists of data that describes the image, the most important being patient data.This includes the patient\u2019s demographic information such as the patient\u2019s name, age, gender, and date of birth.Hy\n(https:\/\/theaisummer.com\/medical-image-coordinates\/)","890ab876":"## Reference  \n* [YOLO ref1]( https:\/\/www.kaggle.com\/ayuraj\/train-covid-19-detection-using-yolov5)  \n* [YOLO ref2](https:\/\/www.kaggle.com\/h053473666\/siim-cov19-yolov5-train#YOLOv5)  \n* [simple-tutorial](https:\/\/www.kaggle.com\/yujiariyasu\/catch-up-on-positive-samples-plot-submission-csv?scriptVersionId=63394385)  ","6ec03b83":"* required structure for dataset directory\n\n```\n\/parent_folder\n    \/dataset\n         \/images\n             \/train\n             \/val\n         \/labels\n             \/train\n             \/val\n    \/yolov5\n```"}}