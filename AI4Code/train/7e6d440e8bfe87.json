{"cell_type":{"714e42f8":"code","d5d4a73e":"code","117bbc2e":"code","784fc7af":"code","8000f46e":"code","ffcadb72":"code","b7113411":"code","3c1350f5":"code","20d148b3":"code","4522fd68":"code","4ee891a3":"code","c784d0e4":"code","f3480453":"code","21128b93":"code","ba6d119c":"code","54a5303e":"code","441f6136":"code","35b57ce4":"code","5df6d43a":"code","e200824a":"code","0798c7ca":"code","ad6a239c":"code","326481cf":"code","26baf306":"code","e9599871":"markdown","fc62d8b9":"markdown","dff261dd":"markdown","aff354f8":"markdown","1c463bdc":"markdown","94a17b60":"markdown","a5b05909":"markdown","9401aaf8":"markdown","3b1fadbd":"markdown","903e3f81":"markdown","9a5c87be":"markdown","5ad704b2":"markdown","be9fa63a":"markdown"},"source":{"714e42f8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","d5d4a73e":"PATH =  \"\/kaggle\/input\/boston-house-prices\/\"","117bbc2e":"column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', \n                'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n\nhousing_df = pd.read_csv(PATH + 'housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n\nprint(\"Shape of housing dataset: {0}\".format(housing_df.shape))\n\nhousing_df.head(5)","784fc7af":"train_data = housing_df.iloc[:404, :].copy()\ntest_data = housing_df.iloc[404:, :].copy()\n\nX_train = train_data.iloc[:, :-1].copy()\ny_train = train_data.iloc[:, -1:].copy()\n\nX_test = test_data.iloc[:, :-1].copy()\ny_test = test_data.iloc[:, -1:].copy()","8000f46e":"X_train.describe()","ffcadb72":"def feature_normalisation(train_data, test_data):\n    \"\"\" Normalize our dataframe features with zero mean and unit\n        standard deviation \"\"\"\n    \n    std_data = train_data.copy()\n    \n    mean = train_data.mean(axis=0)\n    std_dev = train_data.std(axis=0)\n    \n    # centre data around zero mean and give unit std dev\n    std_data -= mean\n    std_data \/= std_dev\n    \n    # if test data passed to func, convert test data using train mean \/ std dev\n    test_data -= mean\n    test_data \/= std_dev\n        \n    return std_data, test_data","b7113411":"X_train, X_test = feature_normalisation(X_train, X_test)","3c1350f5":"ranf = RandomForestRegressor(random_state=1)\nranf.fit(X_train, y_train.values[:, 0])","20d148b3":"columns = list(X_train.columns)\n\nimportances = ranf.feature_importances_\nindices = np.argsort(importances)[::-1]\ncols_ordered = []\n\nfor feat in range(X_train.shape[1]):\n    print(\"{0:<5} {1:<25} {2:.5f}\".format(feat + 1, columns[indices[feat]], importances[indices[feat]]))\n    cols_ordered.append(columns[indices[feat]])\n    \nplt.figure(figsize=(6,4))\nplt.bar(range(X_train.shape[1]), importances[indices], align='center')\nplt.xticks(range(X_train.shape[1]), cols_ordered, rotation=90)\nplt.xlim([-1, X_train.shape[1]])\nplt.tight_layout()\nplt.title(\"Random Forrest Feature Importances\")\nplt.show()","4522fd68":"from keras import models\nfrom keras import layers","4ee891a3":"def nn_model(dropout=False):\n    \"\"\" Create a basic Deep NN for regression \"\"\"\n    model = models.Sequential()\n    model.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n    if dropout:\n        model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(64, activation='relu'))\n    if dropout:\n        model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(1))\n    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n    return model","c784d0e4":"k = 4\n\nnum_val_samples = len(X_train) \/\/ k\n\nepochs = 100\n\nscores = []\n\n# prepare validation and training partitions\nfor i in range(k):\n    print('Cross-validation fold number {0}'.format(i))\n    val_samples_x = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n    val_samples_y = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n    \n    print(\"X Val: {0}, y Val: {1}\".format(val_samples_x.shape, val_samples_y.shape))\n    \n    train_samples_x = np.concatenate([X_train[:i * num_val_samples],\n                                      X_train[(i + 1) * num_val_samples:]], axis=0)\n    \n    train_samples_y = np.concatenate([y_train[:i * num_val_samples], \n                                      y_train[(i + 1) * num_val_samples:]], axis=0)\n    \n    print(\"X Train: {0}, y Train: {1}\".format(train_samples_x.shape, train_samples_y.shape))\n    \n    # instantiate model and fit training samples, then evaluate on val partition\n    model = nn_model()\n    model.fit(train_samples_x, train_samples_y, epochs=epochs, batch_size=1, verbose=0)\n    val_mse, val_mae = model.evaluate(val_samples_x, val_samples_y, verbose=0)\n    scores.append(val_mae)","f3480453":"print(np.mean(scores))","21128b93":"k = 4\n\nnum_val_samples = len(X_train) \/\/ k\n\nepochs = 100\n\nmae_histories = []\n\n# prepare validation and training partitions\nfor i in range(k):\n    print('Cross-validation fold number {0}'.format(i))\n    val_samples_x = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n    val_samples_y = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n    \n    print(\"X Val: {0}, y Val: {1}\".format(val_samples_x.shape, val_samples_y.shape))\n    \n    train_samples_x = np.concatenate([X_train[:i * num_val_samples],\n                                      X_train[(i + 1) * num_val_samples:]], axis=0)\n    \n    train_samples_y = np.concatenate([y_train[:i * num_val_samples], \n                                      y_train[(i + 1) * num_val_samples:]], axis=0)\n    \n    print(\"X Train: {0}, y Train: {1}\".format(train_samples_x.shape, train_samples_y.shape))\n    \n    # instantiate model and fit training samples, then evaluate on val partition\n    model = nn_model()\n    history = model.fit(train_samples_x, train_samples_y, \n                        epochs=epochs, batch_size=1, \n                        verbose=0, validation_data=(val_samples_x, val_samples_y))\n    \n    val_mae_hist = history.history['val_mae']\n    \n    mae_histories.append(val_mae_hist)","ba6d119c":"average_mae_hist = [np.mean([x[i] for x in mae_histories]) for i in range(epochs)]","54a5303e":"plt.figure(figsize=(10,6))\nplt.plot(range(1, len(average_mae_hist) + 1), average_mae_hist)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Validation MAE\")\nplt.show()","441f6136":"plt.figure(figsize=(10,6))\nplt.plot(range(1, len(average_mae_hist) + 1), average_mae_hist)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Validation MAE\")\nplt.xlim(0.0, 100.0)\nplt.show()","35b57ce4":"k = 4\n\nnum_val_samples = len(X_train) \/\/ k\n\nepochs = 100\n\nreg_mae_histories = []\n\n# prepare validation and training partitions\nfor i in range(k):\n    print('Cross-validation fold number {0}'.format(i))\n    val_samples_x = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n    val_samples_y = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n    \n    print(\"X Val: {0}, y Val: {1}\".format(val_samples_x.shape, val_samples_y.shape))\n    \n    train_samples_x = np.concatenate([X_train[:i * num_val_samples],\n                                      X_train[(i + 1) * num_val_samples:]], axis=0)\n    \n    train_samples_y = np.concatenate([y_train[:i * num_val_samples], \n                                      y_train[(i + 1) * num_val_samples:]], axis=0)\n    \n    print(\"X Train: {0}, y Train: {1}\".format(train_samples_x.shape, train_samples_y.shape))\n    \n    # instantiate dropout regularised model and fit training samples with val data for eval\n    model = nn_model(dropout=True)\n    history = model.fit(train_samples_x, train_samples_y, \n                        epochs=epochs, batch_size=1, \n                        verbose=0, validation_data=(val_samples_x, val_samples_y))\n    \n    val_mae_hist = history.history['val_mae']\n    \n    reg_mae_histories.append(val_mae_hist)\n\naverage_reg_mae_hist = [np.mean([x[i] for x in reg_mae_histories]) for i in range(epochs)]","5df6d43a":"plt.figure(figsize=(10,6))\nplt.plot(range(1, len(average_mae_hist) + 1), average_mae_hist, label='Original Model')\nplt.plot(range(1, len(average_reg_mae_hist) + 1), average_reg_mae_hist, label='Regularised Model')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Validation MAE\")\nplt.xlim(1.0, 100.0)\nplt.legend(loc='best')\nplt.show()","e200824a":"# produce our deep NN model using dropout regularisation, trained on all training data\nfinal_model = nn_model(dropout=True)\nhistory = final_model.fit(X_train, y_train, epochs=200, batch_size=1, verbose=0)","0798c7ca":"hist_dict = history.history\n\ntrg_loss = history.history['loss']\ntrg_acc = history.history['mae']\n\nepochs = range(1, len(trg_acc) + 1)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nax[0].plot(epochs, trg_loss, label='Training Loss')\nax[1].plot(epochs, trg_acc, label='Training MAE')\nax[0].set_ylabel('Training Loss')\nax[1].set_ylabel('Training MAE')\n\nplt.show()","ad6a239c":"test_preds = final_model.predict(X_test)","326481cf":"test_mse, test_mae = final_model.evaluate(X_test, y_test, verbose=0)","26baf306":"print(\"Test set performance: \\n- Test MSE: {0} \\n- Test MAE: {1}\".format(test_mse, test_mae))","e9599871":"# Boston House Price Predictions\n\nRather than applying traditional machine learning techniques on this dataset, as is the norm, in this notebook we'll look at forming a Deep Neural Network (DNN) for regression. This is normally difficult with such a small dataset and a DNN, since the complexity of this type of model tends to significantly overfit the data at hand.\n\nWithin this notebook, I'll exemplify some good practices to counteract overfitting during training, with a particular emphasis on DNNs.","fc62d8b9":"---\n## 4. Formation of Deep Neural Network for regression\n\nForming a deep neural network for regression is relatively simple, especially when armed with high-level libraries like Keras or PyTorch. For regression tasks, we simply need to ensure our final output layer has no activation, unlike in classification tasks where we employ sigmoid or softmax output activations.","dff261dd":"With such a small dataset, we're best employing K-means cross-validation, which makes more from our data than using just one dedicated partition of samples for the validation set.","aff354f8":"---\n## 2. Split our data into training and testing partitions.","1c463bdc":"As we can see, even after a very low number of iterations we begin to over-fit on our data. This is expected with such a small dataset of only 500 samples. To counteract this, we have made use of cross-folds validation. Something that we can apply further to this is regularisation.\n\nBelow we'll apply dropout regularisation in an effort to reduce overfitting.","94a17b60":"It's clear that the only two really important features in our dataset appears to be LSTAT and RM. In comparison to these, the others are almost negigible in terms of their correlation to changing the output house price. However, since we are using a neural network in the following lines of code, we will retain all columns and train our model accordingly, regardless of their importance. If we were using more traditional methods, such as random forests, support vector machine or simple linear regression, we would likely benefit from reduction of some of these less important features.","a5b05909":"Our final test set performance is not bad - only 3.21 Mean Absolute Error (MAE)! Basically the average amount that our predictions of house prices deviated from the actual values was $3210. When we consider how varying houses can be, coupled with their relatively high prices, this is not a bad average error to have in our set of predictions, especially with the extremely low amount of data-preprocessing and feature engineering conducted in this notebook to obtain this model.","9401aaf8":"---\n## 3. Normalization of our features\n\nWith many features that are heterogeneous, we should definitely consider standardising our data through normalisation. For this we should calculate the mean and standard deviation of the features within the training data, and use this to normalise both the training and test sets.\n\nIn the following code, we'll normalize our data with zero mean and unit standard deviation. We'll obtain the mean and standard deviation using the training partition, which we will then use to normalize both the training and test splits.","3b1fadbd":"Prior to producing a neural network model and making subsequent predictions on an evaluated model, lets visualise the relative importance of features using an off the shelf random forrest regressor.","903e3f81":"---\n\n## 1. Import dependencies and dataset","9a5c87be":"Repeat again but obtain a record of our validation performance at each epoch across all folds.","5ad704b2":"---\n## 5. Formation of our evaluated model into a final model\n\n#### Finally, lets make a final model with the entire training set, followed by predictions for our test set using the trained model.","be9fa63a":"Our dropout regularised model is much better in terms of reducing overfitting."}}