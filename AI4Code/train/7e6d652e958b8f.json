{"cell_type":{"ede48039":"code","d1098ab1":"code","05190b3e":"code","590277af":"code","41b540c2":"code","161efe4f":"code","8489f56e":"code","68779f49":"code","b63deb3c":"code","6820d266":"code","be5c0db1":"code","534f5a89":"code","e54fdd6f":"code","a5314508":"code","59338569":"code","e36084a8":"code","7eeb9de7":"code","b8736406":"code","1f132845":"code","7fa1d8fb":"code","5184ccb1":"code","2d4d9c1e":"code","6d1cf41c":"code","f73307d0":"code","e19cf30b":"code","c393e304":"code","1062f968":"code","0878a9e0":"code","fd97da10":"code","20e37b0f":"code","485220b2":"code","c1c4ae00":"code","1e68ac94":"code","1867f154":"code","c9005b4e":"code","2ef0b905":"code","de287d16":"code","7ba10a75":"code","01887ee5":"code","072a0363":"code","a84d095b":"code","493f3040":"code","9a7dfa19":"code","f0800d26":"code","5a794a49":"code","4934f51e":"code","dec27587":"code","1b42cc8a":"code","693db510":"code","eb371ef5":"markdown","2687a32d":"markdown","0c2b65ae":"markdown","b3f0ab73":"markdown","f9d8bf22":"markdown","363f6d9d":"markdown","5f7e565d":"markdown","7ba1e7f4":"markdown","4eb56170":"markdown","23995d94":"markdown","48efa254":"markdown","e4a8e395":"markdown"},"source":{"ede48039":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d1098ab1":"train = pd.read_csv('..\/input\/train.csv', nrows = 1000000)\n# out of the 55million + data, i have picked a sample and will be working with it\n\ntest = pd.read_csv('..\/input\/test.csv')\n\ntrain.head()","05190b3e":"test.head()","590277af":"train.shape","41b540c2":"test.shape","161efe4f":"#We check the datatypes \ntrain.dtypes.value_counts()","8489f56e":"test.dtypes.value_counts()","68779f49":"train.isnull().sum()","b63deb3c":"#I will simply drop the nan columns\ntrain = train.dropna()\ntrain.isnull().sum()","6820d266":"#The we check for zeros in our train and test data\n(train ==0).astype(int).sum()","be5c0db1":"train = train.loc[~(train==0).any(axis =1)]\n#we take a look at what we have done\n#(train ==0).astype(int).sum()","534f5a89":"(train ==0).astype(int).sum()","e54fdd6f":"#we started out with 1 million data point. Lets see what we have now.\ntrain.shape","a5314508":"train.describe()","59338569":"train.describe()","e36084a8":"train.dtypes.value_counts()","7eeb9de7":"#lets take care of the object data first\nobject_data = train.dtypes == np.object\ncategoricals = train.columns[object_data]\ncategoricals\n","b8736406":"#I will drop the key column since i do not really need it \ntrain.drop('key', axis = 1, inplace = True)\ntrain.head()","1f132845":"import datetime as dt\n\ndef date_extraction(data):\n    data['pickup_datetime'] = pd.to_datetime(data['pickup_datetime'])\n    data['year'] = data['pickup_datetime'].dt.year\n    data['month'] = data['pickup_datetime'].dt.month\n    data['weekday'] = data['pickup_datetime'].dt.day\n    data['hour'] = data['pickup_datetime'].dt.hour\n    data = data.drop('pickup_datetime', axis = 1, inplace = True)\n    \n    return data\n    \n#Apply this to both the train and the test data\ndate_extraction(train)\n#test = date_extraction(test)\n    ","7fa1d8fb":"train.head()","5184ccb1":"date_extraction(test)\ntest.head()","2d4d9c1e":"\n\n#First i will define the Haversine function\n#radii of earth in meters = 6371e3 meters\n# def long_lat_distance (x):\n#     x['Longitude_distance'] = x['pickup_longitude'] - x['dropoff_longitude']\n#     x['Latitude_distance'] = x['pickup_latitude'] - x['dropoff_latitude'] \n    \n#     return x   ","6d1cf41c":"def long_lat_distance (x):\n    x['Longitude_distance'] = np.radians(x['pickup_longitude'] - x['dropoff_longitude'])\n    x['Latitude_distance'] = np.radians(x['pickup_latitude'] - x['dropoff_latitude']) \n    x['distance_travelled\/10e3'] = ((x['Longitude_distance']**2 + x['Latitude_distance']**2)**0.5) *1000\n    return x   ","f73307d0":"for x in [train, test]:\n    long_lat_distance(x)\n    \ntrain.head()\n","e19cf30b":"def harvesine(x):\n    #radii of earth in meters = \n    r = 6371000 \n    d = x['distance_travelled\/10e3']\n    theta_1 = np.radians(x['dropoff_latitude'])\n    theta_2 = np.radians(x['pickup_latitude'])\n    lambda_1 = np.radians(x['dropoff_longitude'])\n    lambda_2 = np.radians(x['dropoff_longitude'])\n    theta_diff = x['Longitude_distance']\n    lambda_diff = x['Latitude_distance']\n    \n    a = np.sin(theta_diff\/2)**2 + np.cos(theta_1)*np.cos(theta_2)*np.sin(lambda_diff\/2)**2\n    c = 2 * np.arctan2(a**0.5, (1-a)**0.5)\n    x['harvesine\/km'] = (r * c)\/1000\n","c393e304":"for x in [train, test]:\n    harvesine(x)\n    \ntrain.head()","1062f968":"train.dtypes.value_counts()","0878a9e0":"# #not sure if this is rrally necessary. will have to see\n\n# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()\n# scaler.fit_transform(train)\n","fd97da10":"train.head()","20e37b0f":"test.head()","485220b2":"train.describe()","c1c4ae00":"print('Are there any nulls\\nan in the train data: ')\nprint(train.isnull().sum())\n\nprint('\\nAre there any nulls\\nans in the test data: ')\nprint(test.isnull().sum())\n","1e68ac94":"#as we can see there are 3 nulls in the train data. Lets replace them with the mean\ntrain['harvesine\/km'] = train['harvesine\/km'].fillna(train['harvesine\/km'].median())","1867f154":"from sklearn.ensemble import RandomForestRegressor\n\n#split the train features \nfeature_cols = [x for x in train.columns if x!= 'fare_amount']\nX = train[feature_cols]\ny = train['fare_amount']\n","c9005b4e":"correlations = X.corrwith(y)\ncorrelations = abs(correlations*100)\ncorrelations.sort_values(ascending = False, inplace= True)\n\ncorrelations","2ef0b905":"#lets plot and see what we've got\nax = correlations.plot(kind='bar')\nax.set(ylim=[-1, 1], ylabel='pearson correlation');","de287d16":"train.head()","7ba10a75":"#From the diagram above, i will use the 5 most important features\n\ntrain_1 = train.drop(['pickup_longitude', 'dropoff_longitude','pickup_latitude','dropoff_latitude',\n                    'Longitude_distance', 'Latitude_distance'], axis =1)\n\ntrain_1.head()","01887ee5":"train_1['harvesine\/km'] =train_1['harvesine\/km'].round(2) \ntrain_1['distance_travelled\/10e3'] =train_1['distance_travelled\/10e3'].round(2) \n\ntrain_1.head()","072a0363":"\n# #not sure if this is rrally necessary. will have to see\n\n# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()\n# scaler.fit_transform(train_1)","a84d095b":"train_1.describe()","493f3040":"test_1 = test.drop(['pickup_longitude', 'dropoff_longitude','pickup_latitude','dropoff_latitude',\n                    'Longitude_distance', 'Latitude_distance'], axis =1)\n\ntest_1.head()","9a7dfa19":"from sklearn.model_selection import train_test_split\nfeat_cols = [x for x in train_1.columns if x!= 'fare_amount']\nX_1 = train_1[feat_cols]\ny_1 = train_1['fare_amount']\nX_train, X_test, y_train, y_test = train_test_split(X_1, y_1, test_size = 0.25, random_state = 42)","f0800d26":"#Random forest\nrf = RandomForestRegressor(n_estimators = 100, max_features = 5)\nrf = rf.fit(X_train, y_train)\n","5a794a49":"test.head()","4934f51e":"final_prediction = rf.predict(X_test)","dec27587":"test_1.drop('key', axis = 1, inplace = True)\ntest_1.head()","1b42cc8a":"#random forest\nfinal_prediction = rf.predict(test_1)\n\nNYCtaxiFare_submission = pd.DataFrame({'key': test.key, 'fare_amount': final_prediction})\nNYCtaxiFare_submission.to_csv('NYCtaxiFare_prediction.csv', index = False)","693db510":"NYCtaxiFare_submission.head()","eb371ef5":"With the previous version 14, i used Linear regression and i got a score of 9.38 but with random forest i got 3.89","2687a32d":"As we can see from above, we have 2 columns with the object datatype. We will have to take a closer look at them later. \nFirst we take a look at the data to see if there are any null or NAN values\n","0c2b65ae":"\n\nWHAT we do next is to see how the data points are distributed. By using the the .describe method, we check the min and the max value of the dataset.\n\n","b3f0ab73":"Lets find the absolut distance travelled by each passenger, by taking the difference between their pickup latitudes and dropoff latitudes. We do the same for latitude","f9d8bf22":"Found this on Quora and i am going to use this to find the distance travelled using the longitudes and latitudes given. You can check it out at:\n\nhttps:\/\/www.quora.com\/How-to-measure-the-distance-traveled-using-latitude-and-longitude\nhttps:\/\/community.esri.com\/groups\/coordinate-reference-systems\/blog\/2017\/10\/05\/haversine-formula","363f6d9d":"## Preparing submission file\n","5f7e565d":"## FEATURE SELECTION  and  RANDOM FOREST","7ba1e7f4":" we are simply going to drop the zero rows becasue they may result in wrong outpur down the road'","4eb56170":"Now we take care of the pickup_datetime column, we extract the dates separately and drop the pickup_datetime column\n","23995d94":"Now that we do not have any categorical features left, we are going to use the standard scaler to mitigate the effect ot the outliers on our data set","48efa254":"waouw!!! checkout the difference\/ range between the min and the max . it is ver large. This means we could have a lot of outlier in out dataset, this may lead to wrong out;puts","e4a8e395":"## Modelling and Prediction"}}