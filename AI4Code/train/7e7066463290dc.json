{"cell_type":{"d15d899f":"code","79c0697a":"code","f35a8c64":"code","034c07a1":"code","95154a03":"code","ed2aa052":"code","80619153":"code","99dd2849":"code","d218401a":"code","891e54b0":"code","991c2bab":"code","010d45cf":"code","e1aa1bf5":"code","283efd98":"code","099c3f9d":"code","32371030":"code","de7720e8":"code","4833d7d2":"code","69868bee":"code","12ea085c":"code","5387ec7b":"code","e048b879":"code","de482d4a":"code","c8334962":"code","6acc0c70":"code","6984a199":"code","606d5a41":"code","bed9ea8d":"code","22e51fcd":"code","7f5dc99e":"code","e488b759":"code","a6c88e43":"code","e02d7ef8":"code","b8c729e7":"code","048f8317":"code","65b6977b":"code","a6cae3a8":"code","81a4872b":"code","1ae85d37":"code","4a6b9748":"code","20423d36":"code","825b6754":"code","9125162f":"code","e8c07bc0":"code","7e29df85":"code","d431e39a":"code","cc94bc9d":"code","3152d501":"code","33edc623":"code","70a97452":"code","bbe5dd3d":"code","86add423":"markdown","d1a4f9c3":"markdown","c9598530":"markdown","be1492af":"markdown","11d480c9":"markdown","a782e360":"markdown","4f55a2f4":"markdown","9d863e19":"markdown","d47e2347":"markdown","a5cb686d":"markdown","75c7e6c5":"markdown","f0fa6885":"markdown","018fa1db":"markdown","e83d5d51":"markdown","79539a7f":"markdown","0addd25e":"markdown","e9ac554b":"markdown","0347d5d5":"markdown","168e0abb":"markdown","97c41011":"markdown"},"source":{"d15d899f":"# Run 6042.0s - TPU v3-8 [Public Score 0.95072]\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","79c0697a":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## 0 to silence warning of Weights & Biases","f35a8c64":"import matplotlib.pyplot as plt\nimport tensorflow as tf","034c07a1":"#FC to download the data outside of kaggle\n#!kaggle competitions download -c contradictory-my-dear-watson ","95154a03":"train = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")\ntest = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")","ed2aa052":"train.head()","80619153":"#FC : only French texts\ntrain[train.lang_abv=='fr'].head()","99dd2849":"print(\"Entailment example\",\"\\n\")\nprint(train.premise.values[2])\nprint(train.hypothesis.values[2])\nprint(train.label.values[2])","d218401a":"print(\"Neutral example\",\"\\n\")\nprint(train.premise.values[69])\nprint(train.hypothesis.values[69])\nprint(train.label.values[69])","891e54b0":"print(\"Contradictory example\",\"\\n\")\nprint(train.premise.values[41])\nprint(train.hypothesis.values[41])\nprint(train.label.values[41])","991c2bab":"#!pip3 install datasets version==1\n#!pip3 install sentencepiece","010d45cf":"!pip install nlp\nfrom nlp import load_dataset","e1aa1bf5":"def load_mnli(use_validation=True):\n    result = []\n    dataset = load_dataset('multi_nli')\n    print(dataset['train'])\n    keys = ['train', 'validation_matched','validation_mismatched'] if use_validation else ['train']\n    for k in keys:\n        for record in dataset[k]:\n            c1, c2, c3 = record['premise'], record['hypothesis'], record['label']\n            if c1 and c2 and c3 in {0,1,2}:\n                result.append((c1,c2,c3,'en'))\n    result = pd.DataFrame(result, columns=['premise','hypothesis', 'label','lang_abv'])\n    return result","283efd98":"mnli = load_mnli()","099c3f9d":"total_train = train[['id', 'premise', 'hypothesis','lang_abv', 'language', 'label']]\ntotal_train","32371030":"mnli = mnli[['premise', 'hypothesis', 'lang_abv', 'label']]\nmnli.insert(0, 'language', 'English')\nmnli = mnli[['premise', 'hypothesis', 'lang_abv', 'language', 'label']]\nmnli.insert(0, 'id', 'xxx')\nmnli","de7720e8":"total_train = pd.concat([total_train, mnli], axis = 0)\ntotal_train","4833d7d2":"train.describe(include='all')","69868bee":"test.describe(include='all')","12ea085c":"labels, frequencies = np.unique(train.language.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()","5387ec7b":"import seaborn as sns\n#explore the distribution of classes and languages\nfig, ax = plt.subplots(figsize = (12,5))\n\n#for maximum aesthetics\npalette = sns.cubehelix_palette(8, start=2, rot=0, dark=0, light=.95, reverse=True)\n\ngraph1 = sns.countplot(train['language'], hue = train['label'])#, palette = palette)\n\n#set title\ngraph1.set_title('Distribution of Languages and Labels')\n\nplt.tight_layout()\nplt.show()","e048b879":"from transformers import BertTokenizer, TFBertModel, TFAutoModel,AutoTokenizer\n\n#model_name = \"bert-base-multilingual-cased\"\n#tokenizer = BertTokenizer.from_pretrained(model_name) # FC: this is the tokenizer we will use on our text data to tokenize it\n\nmodel_name = \"joeddav\/xlm-roberta-large-xnli\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)","de482d4a":"list(tokenizer.tokenize(\"I love machine learning\")) # FC: tokenize only create a list of words","c8334962":"# FC we make a function in order to have a list of the id for each word and the separator \ndef encode_sentence(s):\n   tokens = list(tokenizer.tokenize(s)) # FC: split the sentence into tokens that are either words or sub-words\n   tokens.append('[SEP]') # FC: a token called [SEP] (=separator) is added to mark end of each sentence\n   return tokenizer.convert_tokens_to_ids(tokens) # FC: instead of returning the list of tokens, a list of each token ID is returned","6acc0c70":"encode_sentence(\"I love machine learning\") # FC: the output is a number for each word plus the ID for the [SEP] token","6984a199":"def bert_encode(hypotheses, premises, tokenizer): # FC: for RoBERTa we remove the input_type_ids from the inputs of the model\n    \n  num_examples = len(hypotheses)\n  \n  sentence1 = tf.ragged.constant([   # FC: constructs a constant ragged tensor. every entry has a different length\n      encode_sentence(s) for s in np.array(hypotheses)])\n  \n  sentence2 = tf.ragged.constant([\n      encode_sentence(s) for s in np.array(premises)])\n  \n  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0] # FC: list of IDs for the token '[CLS]' to denote each beginning\n  \n  input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1) # FC: put everything together. every row still has a different length.\n  \n  #input_word_ids2 = tf.concat([cls, sentence2, sentence1], axis=-1)\n  \n  #input_word_ids = tf.concat([input_word_ids1, input_word_ids2], axis=0) # we duplicate the dataset inverting sentence 1 and 2\n    \n  input_mask = tf.ones_like(input_word_ids).to_tensor() # FC: first, a tensor with just ones in it is constructed in the same size as input_word_ids. Then, by applying to_tensor the ends of each row are padded with zeros to give every row the same length\n\n  # type is not need for the RoBERTa model it will not be include in the output of this function\n  type_cls = tf.zeros_like(cls) # FC: creates a tensor same shape as cls with only zeros in it\n  \n  type_s1 = tf.zeros_like(sentence1)\n  \n  type_s2 = tf.ones_like(sentence2) # FC: creates a tensor same shape as sentence2 with only ones in it to mark the 2nd sentence\n  \n  input_type_ids = tf.concat(\n      [type_cls, type_s1, type_s2], axis=-1).to_tensor() # FC: concatenates everything and again adds padding \n  \n  inputs = {\n      'input_word_ids': input_word_ids.to_tensor(), # FC: input_word_ids hasn't been padded yet - do it here now\n      'input_mask': input_mask\n      \n      #,'input_type_ids': input_type_ids\n  }\n\n  return inputs","606d5a41":"train_input = bert_encode(train.premise.values, train.hypothesis.values, tokenizer)","bed9ea8d":"train_input","22e51fcd":"total_train_input = bert_encode(total_train.premise.values, total_train.hypothesis.values, tokenizer)","7f5dc99e":"train.label.values.shape","e488b759":"total_train.label.values.shape","a6c88e43":"print(np.count_nonzero(train_input['input_word_ids'], axis=1))","e02d7ef8":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Fixing random state for reproducibility\nnp.random.seed(19680801)\n\n\nx = np.count_nonzero(train_input['input_word_ids'], axis=1)\n\n# the histogram of the data\nn, bins, patches = plt.hist(x, 50, density=True, facecolor='b', alpha=0.75)\n\n\nplt.xlabel('input word lenght')\nplt.ylabel('Probability')\nplt.title('Distribution of word length on the train set')\nplt.text(60, .021, r'max_length=245')\nplt.xlim(0, 250)\n#plt.ylim(0, 0.03)\nplt.grid(True)\nplt.show()","b8c729e7":"test_input = bert_encode(test.premise.values, test.hypothesis.values, tokenizer)","048f8317":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Fixing random state for reproducibility\nnp.random.seed(19680801)\n\n\nx = np.count_nonzero(test_input['input_word_ids'], axis=1)\n\n# the histogram of the data\nn, bins, patches = plt.hist(x, 50, density=True, facecolor='b', alpha=0.75)\n\n\nplt.xlabel('input word lenght')\nplt.ylabel('Probability')\nplt.title('Distribution of word length on the test set')\nplt.text(60, .021, r'max_length=236')\nplt.xlim(0, 250)\n#plt.ylim(0, 0.03)\nplt.grid(True)\nplt.show()","65b6977b":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Fixing random state for reproducibility\nnp.random.seed(19680801)\n\n\nx = np.count_nonzero(total_train_input['input_word_ids'], axis=1)\n\n# the histogram of the data\nn, bins, patches = plt.hist(x, 50, density=True, facecolor='b', alpha=0.75)\n\n\nplt.xlabel('input word lenght')\nplt.ylabel('Probability')\nplt.title('Distribution of word length on the test set')\nplt.text(60, .021, r'max_length=236')\nplt.xlim(0, 250)\n#plt.ylim(0, 0.03)\nplt.grid(True)\nplt.show()","a6cae3a8":"max_len = 236 #: FC 50 in the initial tutorial\n\ndef build_model():\n    #encoder = TFBertModel.from_pretrained(model_name) \n    # FC: constructs a RoBERTa model pre-trained on the above described language model 'xlm-roberta-large-xnli'\n    encoder = TFAutoModel.from_pretrained('joeddav\/xlm-roberta-large-xnli')\n    # FC: now we adjust the model so that it can accept our input by telling the model what the input looks like:\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\") # FC: tf.keras.Input constructs a symbolic tensor object whith certain attributes: \"shape\" tells it that the expected input will be in batches of max_len-dimensional vectors; \"dtype\" tells it that the data type will be int32; \"name\" will be the name string for the input layer\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\") # FC: repeat the same for the other two input variables\n    # FC: the input type is only needed for the BERT model\n    #input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    # FC: now follows, what we want to happen with our input:\n    # FC: first, our input goes into the BERT model bert_encoder. It will return a tuple and the contextualized embeddings that we need are stored in the first element of that tuple\n    embedding = encoder([input_word_ids, input_mask])[0] # FC: add_input_type_ids for the BERT model\n    # FC: we only need the output corresponding to the first token [CLS], which is a 2D-tensor with size (#sentence pairs, 768) and is accessd with embedding[:,0,:]. This will be input for our classifier, which is a regular densely-connected neural network constructed through tf.keras.layers.Dense. The inputs mean: \"3\" is the dimensionality of the output space, which means that the output has shape (#sentence pairs,3). More practically speaking, for each sentence pair that we input, the output will have 3 probability values for each of the 3 possible labels (entailment, neutral, contradiction). They will be in range(0,1) and add up to 1; \"activation\" denotes the activation function, in this case 'softmax', which connects a real vector to a vector of categorical possibilities.\n    \n    # I tried to put another layer put it doesn't help in performance\n    #output = tf.keras.layers.Dense(10, activation='softmax')(embedding[:,0,:]) #FC: no need of a GlobalAveragePooling for BERT\n    \n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    # FC: we also have the posibility of making a globalAveragepooling of all the embeddings, but the resuls are not better \n    #output = tf.keras.layers.GlobalAveragePooling1D()(embedding) \n    #output = tf.keras.layers.Dense(3, activation='softmax')(output) \n    \n       \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output) # FC: based on the code in the lines above, a model is now constructed and passed into the variable model\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy']) # FC: we tell the model how we want it to train and evaluate: \"tf.keras.optimizers.Adam\": use an optimizer that implements the Adam algorithm. \"lr\" denotes the learning rate; \"loss\" denotes the loss function to use; \"metrics\" specifies which kind of metrics to use for training and testing\n    \n    return model ","81a4872b":"try:\n    # detect and init the TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # FC: detect and init the TPU: TPUClusterResolver() locates the TPUs on the network\n    # instantiate a distribution strategy\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    \n    strategy = tf.distribute.experimental.TPUStrategy(tpu) # FC: \"strategy\" contains the necessary distributed training code that will work on the TPUs\nexcept ValueError: # FC: in case Accelerator is not set to TPU in the Notebook Settings\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync) # FC: returns the number of cores","1ae85d37":"try:\n  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\nexcept ValueError:\n  tpu = None\n  gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n\nif tpu:\n  tf.tpu.experimental.initialize_tpu_system(tpu)\n  strategy = tf.distribute.experimental.TPUStrategy(tpu,) # Going back and forth between TPU and host is expensive. Better to run 128 batches on the TPU before reporting back.\n  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\nelif len(gpus) > 1:\n  strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n  print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\nelif len(gpus) == 1:\n  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n  print('Running on single GPU ', gpus[0].name)\nelse:\n  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n  print('Running on CPU')\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","4a6b9748":"# instantiating the model in the strategy scope creates the model on the TPU\n\nwith strategy.scope(): # FC: defines the compute distribution policy for building the model. or in other words: makes sure that the model is created on the TPU\/GPU\/CPU, depending on to what the Accelerator is set in the Notebook Settings\n    model = build_model() # FC: our model is being built\n    model.summary()       # FC: let's look at some of its properties\n\ntf.keras.utils.plot_model(model, \"my_model.png\", show_shapes=True) # FC: I added this line because it gives a nice visualization showing the individual components of our model","20423d36":"# We can freeze the RoBERTa weights in order to save some time\nprint(model.layers[2])\nmodel.layers[2].trainable=True","825b6754":"# We need to put the train set with the same size of the model\nfor key in train_input.keys():\n    train_input[key] = train_input[key][:,:max_len]","9125162f":"# We need to put the train set with the same size of the model\nfor key in total_train_input.keys():\n    total_train_input[key] = total_train_input[key][:,:max_len]","e8c07bc0":"early_stop = tf.keras.callbacks.EarlyStopping(patience=3,restore_best_weights=True)\n# FC: make sure that TPU in Accelerator under Notebook Settings is turned on so that model trains on the TPU. Otherwise this line will crash\nmodel.fit(total_train_input, total_train.label.values, epochs = 30, verbose = 1, validation_split = 0.01,\n         batch_size=16*strategy.num_replicas_in_sync\n          ,callbacks=[early_stop]\n         ) # FC: now we fit the model to our training data that we prepared before. The number of training epochs is 2, verbose = 1 shows progress bar, # of rows in each batch is 64, and 20% of the data is used for validation","7e29df85":"test = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")\ntest_input = bert_encode(test.premise.values, test.hypothesis.values, tokenizer) # FC: finally we prepare our competition data for the model","d431e39a":"# same for the test set we need to put it in the same size of the model\nfor key in test_input.keys():\n    test_input[key] = test_input[key][:,:max_len]","cc94bc9d":"test.head()","3152d501":"predictions = [np.argmax(i) for i in model.predict(test_input)] # FC; ve the model predict three categorical probabilities, choose the highest probability, and save the respective label ID (0,1, or 2)","33edc623":"submission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions","70a97452":"submission.head()","bbe5dd3d":"submission.to_csv(\"submission.csv\", index = False)","86add423":"--**The graph above illustrates in a very detailed way what our model and its inputs look like: input_word_ids, input_mask, and input_type_ids are the 3 input variables for the BERT model, which in turn returns a tuple. The word embeddings that are stored in the first entry of the tuple are then given to the classifier which then returns 3 categorical probabilities. The question marks stand for the number of rows in the input data which are of course unknown.**--","d1a4f9c3":"--**edits by [@Federico CABRERA PAEZ](https:\/\/www.kaggle.com\/federicocabrerapaez): This noteook was originally published by [@anasofiauzsoy](https:\/\/www.kaggle.com\/anasofiauzsoy) under this [link](https:\/\/www.kaggle.com\/anasofiauzsoy\/tutorial-notebook) as a tutorial for the [\"Contradictory, My Dear Watson\" competition](https:\/\/www.kaggle.com\/c\/contradictory-my-dear-watson\/overview). \nI will mark all my edits in the text with bold and with # FC: in the code.**--\n\n\nNatural Language Inferencing (NLI) is a classic NLP (Natural Language Processing) problem that involves taking two sentences (the _premise_ and the _hypothesis_ ), and deciding how they are related- if the premise entails the hypothesis, contradicts it, or neither.\n\nIn this tutorial we'll look at the _Contradictory, My Dear Watson_ competition dataset, build a preliminary model using Tensorflow 2, Keras, and BERT, and prepare a submission file.\n","c9598530":"And now we've created our submission file, which can be submitted to the competition. Good luck!","be1492af":"Let's look at the size of the dataset and the distribution of languages in the training set.","11d480c9":"## Preparing Data for Input","a782e360":"Why do we only need embedding[0][:,0,:] from the BERT's output?**\n\n**[Here](http:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/) is a wonderful blog article (including a notebook) which explains this in great detail (see in particular the section \"Unpacking the BERT output tensor\" for an illustrative visualization). I will try to give a short summary here: embedding[0] consists of a 3D tensor with a number (embedding) for each token in the sentence pair (columns) for each sentence pair (rows) for each hidden unit in BERT (768). Since he output corresponding to the first token [CLS] (a.k.a. embedding[0][:,0,:], this is a 2D tensor with size (#sentence pairs, 768)) can be thought of as an embedding for each individual sentence pair, it is enough to just use that as the input for our classification model.**--","4f55a2f4":"Now, we can incorporate the BERT transformer into a Keras Functional Model. For more information about the Keras Functional API, see: https:\/\/www.tensorflow.org\/guide\/keras\/functional.\n\nThis model was inspired by the model in this notebook: https:\/\/www.kaggle.com\/tanulsingh077\/deep-learning-for-nlp-zero-to-transformers-bert#BERT-and-Its-Implementation-on-this-Competition, which is a wonderful introduction to NLP!\n\n--**Now, we are ready to build the actual model. As mentioned above, the final model will consist of a RoBERTa Large model that performs contextual embedding of the input token IDs which are then passed to a classifier that will return probabilites for each of the possible three labels \"entailment\" (0), \"neutral\" (1), or \"contradiction\" (2). The classifier consists of a regular densely-connected neural network.**--\n\n![image.png](attachment:d8ff3ce5-2ade-4c35-a477-7bcee08bff69.png)","9d863e19":"We are going to import more data to train our model ","d47e2347":"## Creating & Training Model","a5cb686d":"Let's look at one of the pairs of sentences.","75c7e6c5":"The training set contains a premise, a hypothesis, a label (0 = entailment, 1 = neutral, 2 = contradiction), and the language of the text. For more information about what these mean and how the data is structured, check out the data page: https:\/\/www.kaggle.com\/c\/contradictory-my-dear-watson\/data","f0fa6885":"BERT uses three kind of input data- input word IDs, input masks, and input type IDs.\n\nThese allow the model to know that the premise and hypothesis are distinct sentences, and also to ignore any padding from the tokenizer.\n\nWe add a [CLS] token to denote the beginning of the inputs, and a [SEP] token to denote the separation between the premise and the hypothesis. We also need to pad all of the inputs to be the same size. For more information about BERT inputs, see: https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#tfbertmodel\n\n**To prepare our token IDs which we received through our function encode_sentence() to become input into BERT, we first concatenate the token ID list of each sentence in the hypothesis and premise column (remember that each sentence is already separated by the [SEP] token at the end which we added above) into one list and add another token ID at the very beginning (the ID for the token '[CLS]') which denotes the beginning. However, the output list of encode_sentence() will have a different length for each sentence from our data set which means we can't just construct a nice table out of all those concatenated lists to feed them to BERT. Instead, we will have to add zeros at the end of each ID list until it has the length of the longest list in the data set (corresponding to the longest hypothesis\/premise pair). This process is called padding. This way, every ID list will have the same length and BERT will be able to accept them as input. This will be the variable \"input word IDs\" mentioned above and our first input variable for the BERT model.**\n\n**However, we also need to tell BERT which of the IDs in the \"input word IDs\" actually belong to tokens that it should embed and which of them it should ignore because they are just padding. This is where input masks come into play: The input mask variable has the same size as the input word IDs variable but contains a 1 for each entry that is an actual token ID (which BERT should consider) and a 0 for eacht entry that is just padding and which BERT should ignore. This will be our second input argument \"input masks\" for the BERT model.**\n\n**Lastly, BERT (but not RoBERTa because it wasn't train in predicting the likelihood that sentence B belongs after sentence A) also needs to know which of the input word IDs belong to which sentence (i.e., hypthesis or premise). We can explain this to BERT by using the variable 'input type IDs' mentioned above. Again, this variable has the same size as as the input word IDs variable but this time it contains a 1 for each entry that belongs to sentence B (i.e., the premise) and a 0 for each entry that belongs to sentence A (i.e., the hypothesis), including our start token ID for '[CLS]'. \"Input type IDs\" will be our third and last input argument for BERT.**--\n\nNow, we're going to encode all of our premise\/hypothesis pairs for input into BERT.","018fa1db":"--**The tutorial use a pre-trained BERT model combined with a classifier to make the predictions. In order to have better results than the Tutorial we will use a bigger model with the same architecture call RoBERTa (Robust optimization BERT), this model use basically 10 times more data and train more time than the original model** \n![image.png](attachment:bb914455-66cd-428c-9e22-28c4d1ea2f61.png)!\n\n**First, the input text data will be fed into the model (after they have been converted into tokens), which in turn outputs embbeddings of the words. The advantage to other types of embeddings is that the BERT embeddings (or RoBERTa embeddings because is the same architecture) are contextualized. Predictions with contextualized embeddings are more accurate than with non-contextualized embeddings.**\n\n**After we receive the embeddings for the words in our text from RoBERTa, we can input them into the classifier which will then in turn return the prediction labels 0,1, or 2.**--\n\nTo start out, we can use a pretrained model. Here, we'll use a multilingual BERT model from huggingface. For more information about BERT, see: https:\/\/github.com\/google-research\/bert\/blob\/master\/multilingual.md\n\nFirst, we download the tokenizer.\n\n--** We will first break down our text into tokens by using RoBERTa own tokenizer using AutoTokenizer and the model name in order for the tokenizer to know how to tokenize. This will download all the necessary files. This model includes more than 100 languages which is useful since our data also contains multiple languages.**--","e83d5d51":"Dataset from https:\/\/www.kaggle.com\/hugoarmandopazvivas\/contradictory-my-dear-watson-hapv","79539a7f":"Tokenizers turn sequences of words into arrays of numbers. Let's look at an example:","0addd25e":"The submission file will consist of the ID column and a prediction column. We can just copy the ID column from the test file, make it a dataframe, and then add our prediction column.","e9ac554b":"Let's set up our TPU.\n\nMore info about setting up the TPU can be found on this [Kaggle documentation page](https:\/\/www.kaggle.com\/docs\/tpu#tpu1).**--","0347d5d5":"## Generating & Submitting Predictions","168e0abb":"We can use the pandas head() function to take a quick look at the training set.","97c41011":"## Downloading Data"}}