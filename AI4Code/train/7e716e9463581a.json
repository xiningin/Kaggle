{"cell_type":{"87c0e5d2":"code","b0ea4446":"code","6b794949":"code","2f2ff13e":"code","11c57008":"code","727f2c6d":"code","f51b16d1":"code","fddea59c":"code","26e88e15":"code","20d9ffa1":"code","5c1813e3":"code","15b72bf1":"code","7c88da65":"code","fd329f46":"code","cef56e75":"code","1e353386":"code","350ea31b":"code","43d2eb62":"code","762366f9":"code","c59bb405":"code","ea0f2d9a":"code","edb7b57c":"code","b39af609":"code","afae080d":"code","8ad00a4f":"code","9fc4ed42":"code","4806030b":"code","836e3c78":"code","045a8b5c":"code","1b50e9af":"code","a74da56e":"code","65c50380":"code","738f5f2c":"code","a34ce026":"code","7b917f43":"code","ee19df78":"code","b1dd0dc8":"markdown","cfd2508a":"markdown","664f0c06":"markdown","0d9e77bd":"markdown","0966d01a":"markdown","c48289cc":"markdown","53eac314":"markdown","16950739":"markdown","409b33bd":"markdown","b5ba9b97":"markdown","c6cb447e":"markdown","a37b0a6d":"markdown","2d8018d3":"markdown","9fbd3818":"markdown","1d18102c":"markdown","2cfe5531":"markdown","c43357e6":"markdown","e17505ea":"markdown","1ba576d0":"markdown","019075c1":"markdown","85531a3e":"markdown","4281da5b":"markdown","2b1389df":"markdown","da3edc31":"markdown","96b05033":"markdown","32aeb0a0":"markdown","75c41265":"markdown","620a665d":"markdown","81e898e7":"markdown","5898a78c":"markdown","86f32aa4":"markdown","fc1bba34":"markdown","bcd35765":"markdown","8af1d697":"markdown","26a27e95":"markdown","b2349dcd":"markdown"},"source":{"87c0e5d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV \n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b0ea4446":"students = pd.read_csv(\"\/kaggle\/input\/students-performance-in-exams\/StudentsPerformance.csv\")","6b794949":"students.head()","2f2ff13e":"students.columns = \"gender\",\"race\",\"parental_edu\",\"lunch\",\"test_prep\",\"math\",\"reading\",\"writing\"","11c57008":"students.isna().sum()\n# No missing data in this dataset","727f2c6d":"f, axs = plt.subplots(3,3,figsize=(15,15))\nstudents['gender'].value_counts().plot(kind='bar', ax=axs[0,0])\naxs[0,0].title.set_text('Gender')\nstudents['race'].value_counts().plot(kind='bar', ax=axs[0,1])\naxs[0,1].title.set_text('Race')\nstudents['parental_edu'].value_counts().plot(kind='bar', ax=axs[0,2])\naxs[0,2].title.set_text('Parental Education')\nstudents['lunch'].value_counts().plot(kind='bar', ax=axs[1,0])\naxs[1,0].title.set_text('Lunch')\nstudents['test_prep'].value_counts().plot(kind='bar', ax=axs[1,1])\naxs[1,1].title.set_text('Test Prep')\naxs[1,2].hist(students['math'])\naxs[1,2].title.set_text('Math')\naxs[2,0].hist(students['reading'])\naxs[2,0].title.set_text('Readiing')\naxs[2,1].hist(students['writing'])\naxs[2,1].title.set_text('Writing')\n\nf.delaxes(axs[2][2])\nf.tight_layout()\nplt.show()","f51b16d1":"sns.pairplot(students.iloc[:,:])\nstudents.corr()","fddea59c":"dum_gender = pd.get_dummies(students.gender, prefix='gender', prefix_sep='_')\ndum_gender.drop('gender_female', inplace=True, axis=1)\n\ndum_race = pd.get_dummies(students.race, prefix='race', prefix_sep='_')\ndum_race.columns = \"race_A\", \"race_B\", \"race_C\", \"race_D\", \"race_E\"\ndum_race.drop('race_E', inplace=True, axis=1)\n\ndum_parental_edu = pd.get_dummies(students.parental_edu, prefix='parental_edu', prefix_sep='_')\ndum_parental_edu.columns = \"parental_edu_associate\", \"parental_edu_bachelor\", \"parental_edu_hs\", \"parental_edu_masters\", \"parental_edu_somecollege\", \"parental_edu_somehs\"\ndum_parental_edu.drop('parental_edu_somehs', inplace=True, axis=1)\n\ndum_lunch = pd.get_dummies(students.lunch, prefix='lunch', prefix_sep='_')\ndum_lunch.drop('lunch_free\/reduced', inplace=True, axis=1)\n\ndum_test_prep = pd.get_dummies(students.test_prep, prefix='test_prep', prefix_sep='_')\ndum_test_prep.drop('test_prep_none', inplace=True, axis=1)\n\nstudents_d = pd.concat([students, dum_gender, dum_race, dum_parental_edu, dum_lunch, dum_test_prep], axis=1)\nstudents_d.drop(['gender', 'race', 'parental_edu', 'lunch', 'test_prep'], inplace=True, axis=1)\n","26e88e15":"def norm_func(i):\n    x = (i-i.min())\t\/ (i.max()-i.min())\n    return (x)\n\nstudents_d = norm_func(students_d.iloc[:,:]) ","20d9ffa1":"train_data,test_data = train_test_split(students_d, test_size = 0.3) # 30% test data","5c1813e3":"logit_model = sm.logit('gender_male ~ math+reading+writing+race_A+race_B+race_C+race_D+parental_edu_associate+parental_edu_bachelor+parental_edu_hs+parental_edu_masters+parental_edu_somecollege+lunch_standard+test_prep_completed', data = train_data).fit()\n\nlogit_model.summary()","15b72bf1":"predict_train = logit_model.predict(pd.DataFrame(train_data))\npredict_test = logit_model.predict(pd.DataFrame(test_data))\n\ncnf_test_matrix = confusion_matrix(test_data['gender_male'], predict_test > 0.5 )\nprint(\"test set confusion matrix: \\n\", cnf_test_matrix)\n\nprint(\"test set accuracy: \", accuracy_score(test_data.gender_male, predict_test > 0.5), \"\\n\")\n\n# Error on train data\ncnf_train_matrix = confusion_matrix(train_data['gender_male'], predict_train > 0.5 )\nprint(\"train set confusion matrix: \\n\", cnf_train_matrix)\n\nprint(\"train set accuracy: \", accuracy_score(train_data.gender_male, predict_train > 0.5))","7c88da65":"vif = pd.DataFrame()\n\nX = students_d.drop('gender_male', axis=1)\nvif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif[\"features\"] = X.columns\n\nvif","fd329f46":"vif = pd.DataFrame()\n\nX = students_d.drop(['gender_male','reading'], axis=1)\nvif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif[\"features\"] = X.columns\n\nvif","cef56e75":"logit_model = sm.logit('gender_male ~ I(math+reading+writing\/3)+race_A+race_B+race_C+race_D+parental_edu_associate+parental_edu_bachelor+parental_edu_hs+parental_edu_masters+parental_edu_somecollege+lunch_standard+test_prep_completed', data = train_data).fit()\n\nlogit_model.summary()","1e353386":"predict_train = logit_model.predict(pd.DataFrame(train_data))\npredict_test = logit_model.predict(pd.DataFrame(test_data))\n\ncnf_test_matrix = confusion_matrix(test_data['gender_male'], predict_test > 0.5 )\nprint(\"test set confusion matrix: \\n\", cnf_test_matrix)\n\nprint(\"test set accuracy: \", accuracy_score(test_data.gender_male, predict_test > 0.5), \"\\n\")\n\n# Error on train data\ncnf_train_matrix = confusion_matrix(train_data['gender_male'], predict_train > 0.5 )\nprint(\"train set confusion matrix: \\n\", cnf_train_matrix)\n\nprint(\"train set accuracy: \", accuracy_score(train_data.gender_male, predict_train > 0.5))","350ea31b":"logit_model = sm.logit('gender_male ~ I(reading*writing)+math+reading+writing+race_A+race_B+race_C+race_D+parental_edu_associate+parental_edu_bachelor+parental_edu_hs+parental_edu_masters+parental_edu_somecollege+lunch_standard+test_prep_completed', data = train_data).fit()\n\nlogit_model.summary()","43d2eb62":"predict_train = logit_model.predict(pd.DataFrame(train_data))\npredict_test = logit_model.predict(pd.DataFrame(test_data))\n\ncnf_test_matrix = confusion_matrix(test_data['gender_male'], predict_test > 0.5 )\nprint(\"test set confusion matrix: \\n\", cnf_test_matrix)\n\nprint(\"test set accuracy: \", accuracy_score(test_data.gender_male, predict_test > 0.5), \"\\n\")\n\n# Error on train data\ncnf_train_matrix = confusion_matrix(train_data['gender_male'], predict_train > 0.5 )\nprint(\"train set confusion matrix: \\n\", cnf_train_matrix)\n\nprint(\"train set accuracy: \", accuracy_score(train_data.gender_male, predict_train > 0.5))","762366f9":"logit_model = sm.logit('gender_male ~ I(reading*writing)+math+reading+writing+race_A+race_B+race_C+race_D+lunch_standard+test_prep_completed', data = train_data).fit()\n\nlogit_model.summary()","c59bb405":"predict_train = logit_model.predict(pd.DataFrame(train_data))\npredict_test = logit_model.predict(pd.DataFrame(test_data))\n\ncnf_test_matrix = confusion_matrix(test_data['gender_male'], predict_test > 0.5 )\nprint(\"test set confusion matrix: \\n\", cnf_test_matrix)\n\nprint(\"test set accuracy: \", accuracy_score(test_data.gender_male, predict_test > 0.5), \"\\n\")\n\n# Error on train data\ncnf_train_matrix = confusion_matrix(train_data['gender_male'], predict_train > 0.5 )\nprint(\"train set confusion matrix: \\n\", cnf_train_matrix)\n\nprint(\"train set accuracy: \", accuracy_score(train_data.gender_male, predict_train > 0.5))","ea0f2d9a":"log_test_acc = accuracy_score(test_data.gender_male, predict_test > 0.5)\nlog_train_acc = accuracy_score(train_data.gender_male, predict_train > 0.5)","edb7b57c":"train_X = train_data.drop(['gender_male'], axis=1)\ntrain_Y = train_data.loc[:,'gender_male']\ntest_X = test_data.drop(['gender_male'], axis=1)\ntest_Y = test_data.loc[:,'gender_male']","b39af609":"knn = KNeighborsClassifier(n_neighbors=25)\nknn.fit(train_X,train_Y)\n\nprint(\"For test data: \\n\")\npred = knn.predict(test_X)\nprint(pd.crosstab(test_Y, pred, rownames=['Actual'],colnames= ['Predictions']))\nprint(\"Test accuracy:\", accuracy_score(test_Y, pred))\n\nprint(\"\\nFor training data: \\n\")\npred_train = knn.predict(train_X)\nprint(pd.crosstab(train_Y, pred_train, rownames=['Actual'],colnames= ['Predictions']))\nprint(\"Training accuracy:\", accuracy_score(train_Y, pred_train))","afae080d":"knn = KNeighborsClassifier(n_neighbors=11)\nknn.fit(train_X.drop(['parental_edu_associate', 'parental_edu_bachelor', 'parental_edu_hs', 'parental_edu_masters', 'parental_edu_somecollege'], axis=1), train_Y)\n\nprint(\"For test data: \\n\")\npred = knn.predict(test_X.drop(['parental_edu_associate', 'parental_edu_bachelor', 'parental_edu_hs', 'parental_edu_masters', 'parental_edu_somecollege'], axis=1))\nprint(pd.crosstab(test_Y, pred, rownames=['Actual'],colnames= ['Predictions']))\nprint(\"Test accuracy:\", accuracy_score(test_Y, pred))\n\nprint(\"\\nFor training data: \\n\")\npred_train = knn.predict(train_X.drop(['parental_edu_associate', 'parental_edu_bachelor', 'parental_edu_hs', 'parental_edu_masters', 'parental_edu_somecollege'], axis=1))\nprint(pd.crosstab(train_Y, pred_train, rownames=['Actual'],colnames= ['Predictions']))\nprint(\"Training accuracy:\", accuracy_score(train_Y, pred_train))\n","8ad00a4f":"knn = KNeighborsClassifier(n_neighbors=20)\nknn.fit(train_X.drop(['race_A', 'race_B', 'race_C', 'race_D','test_prep_completed','lunch_standard','parental_edu_associate', 'parental_edu_bachelor', 'parental_edu_hs', 'parental_edu_masters', 'parental_edu_somecollege'], axis=1), train_Y)\n\nprint(\"For test data: \\n\")\npred = knn.predict(test_X.drop(['race_A', 'race_B', 'race_C', 'race_D', 'test_prep_completed','lunch_standard','parental_edu_associate', 'parental_edu_bachelor', 'parental_edu_hs', 'parental_edu_masters', 'parental_edu_somecollege'], axis=1))\nprint(pd.crosstab(test_Y, pred, rownames=['Actual'],colnames= ['Predictions']))\nprint(\"Test accuracy:\", accuracy_score(test_Y, pred))\n\nprint(\"\\nFor training data: \\n\")\npred_train = knn.predict(train_X.drop(['race_A', 'race_B', 'race_C', 'race_D','test_prep_completed','lunch_standard','parental_edu_associate', 'parental_edu_bachelor', 'parental_edu_hs', 'parental_edu_masters', 'parental_edu_somecollege'], axis=1))\nprint(pd.crosstab(train_Y, pred_train, rownames=['Actual'],colnames= ['Predictions']))\nprint(\"Training accuracy:\", accuracy_score(train_Y, pred_train))","9fc4ed42":"knn_test_acc = accuracy_score(test_Y, pred)\nknn_train_acc = accuracy_score(train_Y,pred_train)","4806030b":"rf = RandomForestClassifier(n_jobs=2, n_estimators=35, criterion=\"entropy\")\n\nrf.fit(train_X, train_Y)\n\nprint(\"For test data: \\n\")\npred = rf.predict(test_X)\nprint(pd.crosstab(test_Y, pred, rownames = ['Actual'], colnames = ['Predictions']))\nprint(\"Test accuracy:\", accuracy_score(test_Y,pred))\n\nprint(\"\\nFor training data: \\n\")\npred_train = rf.predict(train_X)\nprint(pd.crosstab(train_Y, pred_train, rownames = ['Actual'], colnames = ['Predictions']))\nprint(\"Training accuracy:\", accuracy_score(train_Y,pred_train))\n","836e3c78":"rf = RandomForestClassifier(n_jobs=2, n_estimators=200, criterion=\"entropy\")\n\nrf.fit(train_X.drop(['parental_edu_associate', 'parental_edu_bachelor', 'parental_edu_hs', 'parental_edu_masters', 'parental_edu_somecollege'], axis=1), train_Y)\n\nprint(\"For test data: \\n\")\npred = rf.predict(test_X.drop(['parental_edu_associate', 'parental_edu_bachelor', 'parental_edu_hs', 'parental_edu_masters', 'parental_edu_somecollege'], axis=1))\nprint(pd.crosstab(test_Y, pred, rownames = ['Actual'], colnames = ['Predictions']))\nprint(\"Test accuracy:\", accuracy_score(test_Y,pred))\n\nprint(\"\\nFor training data: \\n\")\npred_train = rf.predict(train_X.drop(['parental_edu_associate', 'parental_edu_bachelor', 'parental_edu_hs', 'parental_edu_masters', 'parental_edu_somecollege'], axis=1))\nprint(pd.crosstab(train_Y, pred_train, rownames = ['Actual'], colnames = ['Predictions']))\nprint(\"Training accuracy:\", accuracy_score(train_Y,pred_train))\n","045a8b5c":"rf_test_acc = accuracy_score(test_Y, pred)\nrf_train_acc = accuracy_score(train_Y,pred_train)","1b50e9af":"# kernel = linear\nmodel_linear = SVC(kernel=\"linear\")\nmodel_linear.fit(train_X, train_Y)\npred_test_linear = model_linear.predict(test_X)\npred_train_linear = model_linear.predict(train_X)\n\n# kernel = poly\nmodel_poly = SVC(kernel=\"poly\")\nmodel_poly.fit(train_X, train_Y)\npred_test_poly = model_poly.predict(test_X)\npred_train_poly = model_poly.predict(train_X)\n\n# kernel = sigmoid\nmodel_sigmoid = SVC(kernel=\"sigmoid\")\nmodel_sigmoid.fit(train_X, train_Y)\npred_test_sigmoid = model_sigmoid.predict(test_X)\npred_train_sigmoid = model_sigmoid.predict(train_X)\n\n# kernel = rbf\nmodel_rbf = SVC(kernel=\"rbf\")\nmodel_rbf.fit(train_X, train_Y)\npred_test_rbf = model_rbf.predict(test_X)\npred_train_rbf = model_rbf.predict(train_X)\n\ndata = {\"kernel\":pd.Series([\"linear\",\"polynomial\",\"sigmoid\",\"rbf\"]),\"Test Accuracy\":pd.Series([accuracy_score(test_Y, pred_test_linear),accuracy_score(test_Y, pred_test_poly),accuracy_score(test_Y, pred_test_sigmoid),accuracy_score(test_Y, pred_test_rbf)])}\ntable_acc=pd.DataFrame(data)\ntable_acc\n","a74da56e":"train_X = train_X.drop(['parental_edu_associate', 'parental_edu_bachelor', 'parental_edu_hs', 'parental_edu_masters', 'parental_edu_somecollege'],axis=1)\ntest_X = test_X.drop(['parental_edu_associate', 'parental_edu_bachelor', 'parental_edu_hs', 'parental_edu_masters', 'parental_edu_somecollege'],axis=1)\n\n# kernel = linear\nmodel_linear = SVC(kernel=\"linear\")\nmodel_linear.fit(train_X, train_Y)\npred_test_linear_dropped = model_linear.predict(test_X)\npred_train_linear_dropped = model_linear.predict(train_X)\n\n# kernel = poly\nmodel_poly = SVC(kernel=\"poly\")\nmodel_poly.fit(train_X, train_Y)\npred_test_poly_dropped = model_poly.predict(test_X)\npred_train_poly_dropped = model_poly.predict(train_X)\n\n# kernel = sigmoid\nmodel_sigmoid = SVC(kernel=\"sigmoid\")\nmodel_sigmoid.fit(train_X, train_Y)\npred_test_sigmoid_dropped = model_sigmoid.predict(test_X)\npred_train_sigmoid_dropped = model_sigmoid.predict(train_X)\n\n# kernel = rbf\nmodel_rbf = SVC(kernel=\"rbf\")\nmodel_rbf.fit(train_X, train_Y)\npred_test_rbf_dropped = model_rbf.predict(test_X)\npred_train_rbf_dropped = model_rbf.predict(train_X)\n\ndata = {\"kernel\":pd.Series([\"linear\",\"polynomial\",\"sigmoid\",\"rbf\"]),\"Test Accuracy\":pd.Series([accuracy_score(test_Y, pred_test_linear_dropped),accuracy_score(test_Y, pred_test_poly_dropped),accuracy_score(test_Y, pred_test_sigmoid_dropped),accuracy_score(test_Y, pred_test_rbf_dropped)]),\"Train Accuracy\":pd.Series([accuracy_score(train_Y, pred_train_linear_dropped),accuracy_score(train_Y, pred_train_poly_dropped),accuracy_score(train_Y, pred_train_sigmoid_dropped),accuracy_score(train_Y, pred_train_rbf_dropped)])}\ntable_acc=pd.DataFrame(data)\ntable_acc\n","65c50380":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import reciprocal, uniform\n\nsvm_clf=SVC()\nparam_distributions = {\"kernel\":('linear','poly','rbf'), \"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}\nrnd_search_cv = RandomizedSearchCV(svm_clf, param_distributions, n_iter=10, verbose=2, cv=3)\nrnd_search_cv.fit(train_X, train_Y)","738f5f2c":"print(\"Best estimator: \",rnd_search_cv.best_estimator_)\n\npredicted = rnd_search_cv.predict(test_X)\nrnd_test_acc = accuracy_score(test_Y,predicted)\nprint(\"test accuracy: \", rnd_test_acc)\n\npredicted_train = rnd_search_cv.predict(train_X)\nrnd_train_acc = accuracy_score(train_Y,predicted_train)\nprint(\"train accuracy: \", rnd_train_acc)","a34ce026":"if (table_acc.iloc[:,1].idxmax() > rnd_test_acc):\n    svm_best = table_acc.iloc[:,1].idxmax()\n    svm_test_acc = table_acc.iloc[svm_best,1]\n    svm_train_acc = table_acc.iloc[svm_best,2]\nelse: \n    svm_test_acc = rnd_test_acc\n    svm_train_acc = rnd_train_acc","7b917f43":"students[\"average_score\"] = students.loc[:,['math','reading','writing']].mean(axis=1).round(1)\nstudents\n\nstudents.loc[:,['gender','math','writing','reading','average_score']].groupby(['gender']).mean().transpose().plot.bar()\nplt.title('Comparison of Student Scores Between Genders')\nplt.xlabel('Subject')\nplt.ylabel('Score')\nplt.legend(loc='lower right')\nplt.show()\n","ee19df78":"data = {\"Model\":pd.Series([\"Logistic Regression\",\"K Nearest Neighbour\",\"Random Forest\",\"SVM\"]),\"Test Accuracy\":pd.Series([log_test_acc,knn_test_acc,rf_test_acc,svm_test_acc]),\"Train Accuracy\":pd.Series([log_train_acc,knn_train_acc,rf_train_acc,svm_train_acc])}\ntable_final=pd.DataFrame(data)\ntable_final\n","b1dd0dc8":"Even after removing the variable 'reading', the VIF scores of math and writing are still high. Thus, rather than removing writing and relying only on math scores, it may be a better approach to average the 3 scores for model building.\n\nWe will attempt to build a model with this approach","cfd2508a":"From this first run, the accuracies of the KNN model is quite low. We will try to remove some variables to try to improve the accuracy of the model. We start by removing parental education, as it was deemed statistically insignificant in the logistic regression model as well","664f0c06":"From this, it can be seen that the linear kernel gives the best test accuracy.\n\nNext, we will try removing variables to further improve model accuracy. We will start by removing the variables 'parental education'","0d9e77bd":"We can see that the accuracy does not differ much from removal of the variable. I have also tried performing pruning on the decision trees to further improve the performance, with not much difference in accuracy.\n\nWe will store the final accuracy values for final tabulation and comparison","0966d01a":"Besides that, we also study the relationship between the quantitative variables","c48289cc":"# Exploratory Data Analytics\n\nWe first take a look at the data","53eac314":"As expected from the pairplot earlier, reading and writing are highly correlated. Thus, we will remove reading and calculate the VIF scores again","16950739":"We also check if there are any missing data in the dataset","409b33bd":"The accuracy has increased quite significantly. In fact, we find that using only the scores for the variables 'math', 'reading', and 'writing', we are able to obtain the best prediction accuracies, as shown below:","b5ba9b97":"Based on the model summary, we can see that the newly added interaction term has a low P-value, meaning that it is statistically significant. Besides that, the R-squared of the model has also increased slightly from before.\n\nBased on the confusion matrix & accuracies, it is clear that this model is more accurate than the previous ones\n\nNote that the variable 'reading' is retained even though it has a high P-value. This is due to the hierarchical principle, stating that if we include interaction terms in the model, we should also include main effects even if their P-value is not significant\n\nWe will try to further improve the model by removing some variables","c6cb447e":"From this, it can be observed that the 3 score variables are quite highly correlated, with highest correlation between reading & writing. As this may affect the model output, we may remove certain variables during modelling or combine them during the data modelling stage","a37b0a6d":"# Gender Prediction\n\nFor gender prediction, we will try and compare several models, namely:\n1. Logistic Regression\n2. K-Nearest Neighbours\n3. Random Forest\n4. SVM","2d8018d3":"# Logistic Regression Model\n\nFor model validation, we will use validation set approach. For this, we first perform a train-test split on the data in the ratio of 70:30","9fbd3818":"# Random Forest\nWe will utilize the same train & test split data to model for random forest\n\nNote that hyperparameter tuning for the number of trees in forest has already been done, and only the best value is displayed","1d18102c":"Splitting traing & test dataset into predictor and target","2cfe5531":"Since we will be using distance based algorithms to model this problem (i.e. K nearest neighbours), we will perform feature scaling to scale the quantitative variables to be between the ranges 0 and 1","c43357e6":"From this, we see that there is not much difference from the initial SVM predictions.\n\nWe will store the highest final test accuracy value for final tabulation and comparison","e17505ea":"We will store the final accuracy values for final tabulation and comparison","1ba576d0":"From the output, it can be seen that the model performs well on the test data, with accuracy of ~0.9. \n\nAdditionally, based on the confusion matrix, we also observe that the predictions of the model is also quite balanced, with approximately equal predictions of both classes\n\nWe will try to further improve the logistic regression model. We do this by first performing VIF on the data to determine any multi-collinearity within the data","019075c1":"# K Nearest Neighbours\n\nWe will utilize the same train & test split data to model for K nearest neighbours. Since all the variables have already been scaled to be between 0 & 1, we can begin modelling immediately\n\nNote that hyperparameter tuning for the value of k has already been done, and only the best value of k is displayed","85531a3e":"After trying out removal of various variables, it is found that removing the variable 'parental education' results in a slight increase in accuracy, with a decrease in complexity. Thus, we will remove this variable.\n\nSeveral transformations were also tried, with no further increase in accuracy. Thus, this will be the final logistic regression model.\n\nWe will store the final accuracy values for final tabulation and comparison","4281da5b":"From this, we can see that some of the P-Values are rather high, meaning they are not statistically significant. Besides that, the R-squared value of the model is also not too great, below 0.85.\n\nEither way, we will use the model for validation on the train & test set to have an idea of the model performance","2b1389df":"# Comparison and Conclusion\n\nWe will tabulate the test & train accuracies for the 4 algorithms","da3edc31":"We then plot bar plots and histograms to visualize the distribution of the data for each variable","96b05033":"The dataframe columns are renamed for easier accessibility","32aeb0a0":"# SVM\n\nWe will utilize the same train & test split data to model for SVM\n\nWe will model the data for the following kernels to select the best one: \n* Linear\n* Polynomial\n* Sigmoid\n* Gaussian (rbf)","75c41265":"It can be observed that removing variables does not result in an increase in accuracies. Removal of other variables have also been tested, similarly with no increase in accuracy\n\nWe will try tuning the hyperparameters for the SVM. We will use a randomized search to find the best estimator\n\nNote that a randomized search is used to have a good estimate of the hyperparameters, while reducing computation time","620a665d":"Next, we will try removing variables to improve model accuracy. We will start by removing the variables 'parental education'","81e898e7":"We then perform logistic regression using all the initial variables from the dataset","5898a78c":"It can be observed from the plot that on average male students score higher in math, whereas female students score higher in writing, reading, and have higher overall scores","86f32aa4":"# Data & Feature Engineering\nAs the columns \"gender\", \"race\", \"parental_edu\", \"lunch\" and \"test_prep\" are qualitative variables, we will create dummy variables for them, removing 1 dummy variable for each variable to prevent dummy trap (multi-collinearity problems)\n\nWe then concatenate the dummy variables with the original dataset, and remove the original variable, we will store this as a new dataframe students_d","fc1bba34":"From this, we observe the following regarding the data:\n* Qualitative variables are distributed rather evenly between the classes, with no sparse classes. \n* Quantitative variables 'Math', 'Reading', and 'Writing' have a relatively normal distribution. Besides that, they have also taken on acceptable values within the range of 0 to 100\n* However, it is worth noting that in the variable \"Parental Education\", the variable has the values \"some college\" and \"some high school\" which may be a repetition of other values in the variable. We will keep them for now, but depending on the model outcome, we may merge some values to see if we get better results","bcd35765":"# Introduction\nIn this notebook, we will try to predict the gender of a student based on their socio economic status, family background, as well as their scores in math, writing and reading. Below are the steps that we will be performing:\n\n1. Exploratory Data Analytics\n    * Checking for missing values\n    * Visualization of variables & correlations\n2. Data Engineering\n    * Converting qualitative variables to dummy variables\n    * Standardizing quantitative variables\n3. Data Modelling with:\n    * Logistic Regression\n    * K Nearest Neighbours\n    * Random Forest\n    * SVM\n4. Additional Findings\n5. Comparison and Conclusion\n\n","8af1d697":"Based on the extremely low R-squared score and test & train accuracies, it is evident that we may have removed some key predictors of gender and oversimplified the relationship between the 3 score variables.\n\nThus, we hypothesize that there may be interaction terms between the score variables","26a27e95":"From this, it can be seen that logistic regression and SVM have the highest accuracies. \n\nIt is proposed that logistic regression be used as the final model, due to advantages in terms of:\n* Interpretability as logistic regression provides a formula, with relative weightage of variables. The probability of each class is also provided\n* Simplicity as the logistic regression model built uses less number of variables","b2349dcd":"# Additional Findings\n\nBased on the models built, it is observed that student scores in math, writing and reading are a significant predictor of their gender. Thus, we are also interested to verify this by directly comparing their scores.\n\nWe will compare male and female students average scores in math, reading, writing and overall score."}}