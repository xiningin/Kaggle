{"cell_type":{"c1f2ae01":"code","2b6f396e":"code","579989de":"code","7c54693d":"code","07874ed9":"code","c8f3f9dc":"code","903e29a3":"code","7a79f63a":"code","a0fd31c9":"code","272abfbe":"code","ec64ba5e":"code","cb00cd23":"code","9fa03406":"code","52088588":"code","eeeaf5fe":"code","4d2bf44c":"markdown","b32f7673":"markdown","54a6b628":"markdown","44227458":"markdown","26270140":"markdown","02840c34":"markdown","24a8ac1a":"markdown","0c596d07":"markdown"},"source":{"c1f2ae01":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2b6f396e":"\n\nimport time \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n","579989de":"df = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv',index_col = 0)  \ndf","7c54693d":"df_test = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv',index_col = 0)  \ndf_test","07874ed9":"y = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv',index_col = 0 )\ny","c8f3f9dc":"y_additional = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv',index_col = 0 )\ny_additional","903e29a3":"y_sum = y.sum(axis = 1)\ny_sum.value_counts()","7a79f63a":"y_additional.sum(axis = 1).value_counts()","a0fd31c9":"mode_which_part_to_process = 'full'\nif mode_which_part_to_process == 'full':\n    # consider only gene expression part \n    list_features_names = [c for c in df.columns if ('c-' in c) or ('g-' in c)]\n    X = df[list_features_names ].values\nif mode_which_part_to_process == 'genes':\n    # consider only gene expression part \n    list_features_names =[c for c in df.columns if 'g-' in c]\n    X = df[list_features_names ].values\nif mode_which_part_to_process == 'c':\n    # consider only gene expression part \n    list_features_names =[c for c in df.columns if 'c-' in c]\n    X = df[list_features_names ].values\n\nprint(len([c for c in df.columns if 'g-' in c] ), 'genes count ')\nX_original_save = X.copy()\nprint(X.shape)","272abfbe":"y_01 = (y_sum > 0 ).astype(float)\ny_01.value_counts()","ec64ba5e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y_01, test_size=0.33, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","cb00cd23":"from sklearn.metrics import roc_auc_score\ndf_features_stat = pd.DataFrame()\nfor i in range(X.shape[1]):\n    v = X[:,i]\n    r = roc_auc_score(y_01, v )\n    df_features_stat.loc[list_features_names[i],'rocauc'] = r\n    df_features_stat.loc[list_features_names[i],'rocauc Abs'] = np.abs(r-0.5) + 0.5\n    \n    \ndf_features_stat.sort_values(by = 'rocauc Abs', ascending=False,  inplace = True) \nplt.figure(figsize = (25,6))\nplt.plot( df_features_stat['rocauc Abs'].values,'o')\nplt.title('Features rocauc Abs')\nplt.grid()\nplt.show()\ndf_features_stat#.head(10)","9fa03406":"from sklearn import linear_model\nfrom sklearn.metrics import roc_auc_score\n\nstart = time.time()\nclf = linear_model.LogisticRegression(penalty='l1', solver='liblinear',\n                                      tol=1e-6, max_iter=int(1e6),\n                                      warm_start=True ) #,\n                                      # intercept_scaling=10000.)\n    \ncoefs_ = []\nfor c in [0.001, .002, 0.003, 0.005, 0.008,  0.01, 0.1 , 1, 1e10]:\n    clf.set_params(C=c)\n    clf.fit(X_train, y_train)\n    coefs_.append(clf.coef_.ravel().copy())\n    print(\"This took %0.3fs\" % (time.time() - start))\n    p = clf.predict_proba(X_train)[:,1]\n    r_train = roc_auc_score(y_train, p )\n    p = clf.predict_proba(X_test)[:,1]\n    r = roc_auc_score(y_test, p )\n    print('c=',c, 'rocauc test:', np.round(r,3) , 'Number of features selected:', (clf.coef_.ravel() !=  0).sum() , 'rocauc train:', np.round(r_train,3)) \n    if (clf.coef_.ravel() !=  0).sum()  < 100:\n        print( np.array(list_features_names)[ (clf.coef_.ravel() !=  0) ] )\n        corr_matr = np.corrcoef(X[:, (clf.coef_.ravel() !=  0) ] .T)\n        print(np.triu(corr_matr,1).max(), np.triu(corr_matr,1).min() )  \n        \n    print()\n# print(coefs_)","52088588":"m1 = ( (coefs_[-1] !=  0) * (~(coefs_[-2] !=  0) ) )#.sum()\nnp.array(list_features_names)[ m1 ] , m1.sum()","eeeaf5fe":"m2 = ( (coefs_[-1] !=  0) * (~(coefs_[-3] !=  0) ) )#.sum()\nnp.array(list_features_names)[ m2 ] , m2.sum()","4d2bf44c":"# Train test split ","b32f7673":"# Load data","54a6b628":"# Create a simplified 0-1 target. \n\nOne - means at least one of all scored targets is non-zero. It is quite rare when several targes simulateneoly are non-zero, thus it might not be too bad to make such a simplification.\n","44227458":"# top 16 somewhat \"useless\" features:","26270140":"# What is about ? \n\n\"Fast and dirty\" feature selection for MoA contest. (\"Dirty\" - because drastical simplification for target is done . Then standard use of Lasso\/L1-logreg for feature selection.)\n\nFeature \"g-100\" seems the most strongest any way. \n\nTop 3 features :  ['g-100' 'g-185' 'c-6'] . Model gives rocauc on test sample: 0.601 \n\nTop 11 features:\n['g-41' 'g-47' 'g-91' 'g-100' 'g-157' 'g-185' 'g-270' 'g-385' 'g-689'\n 'c-6' 'c-98']\nModel gives rocauc on test sample: 0.616 \n\nTop 27 features:\n['g-27' 'g-41' 'g-47' 'g-91' 'g-100' 'g-122' 'g-157' 'g-185' 'g-263'\n 'g-270' 'g-312' 'g-372' 'g-385' 'g-397' 'g-442' 'g-487' 'g-522' 'g-524'\n 'g-563' 'g-620' 'g-635' 'g-644' 'g-689' 'g-701' 'g-707' 'c-6' 'c-98']\nModel gives rocauc on test sample: 0.636 \n\n\nThe \"worst\" features and their rocauc : \n\n| Feature | RocAuc |\n| --- | --- |\n| g-362 |\t \t0.500380 | \n| g-555 |\t \t0.500356 | \n| g-688 |\t \t0.500193 | \n| g-715 |\t \t0.500187 | \n| g-404 |\t \t0.500148 | \n\n\nPlease, pay attention, that analysis is made on simplified target (which is sum of all scored targets binned to 0,1 ) and that importance from the point of view of Lasso may differ from importance   from the other model. \n\n","02840c34":"# top 167 somewhat \"useless\" features:","24a8ac1a":"# Top feauture groups selection by Lasso (L1 logreg )\n\nStronger regularization selects only most relevant features.\nSo relaxing regularization we see tradeoff between number of features and prediction quality. \n\nSmall \"C\" - strongest regularization. \n\nC =  0.001 selects:  ['g-100' 'g-185' 'c-6'] with model rocauc = 0.601\n\nC = 0.01 selects 11 features: ['g-41' 'g-47' 'g-91' 'g-100' 'g-157' 'g-185' 'g-270' 'g-385' 'g-689'\n 'c-6' 'c-98'] with model rocauc =  0.616\n \nC =  0.1 gives TOP rocauc test: 0.66 with 705 features \n\nIncreasing C further leads to dropping rocauc on test - that means to overfitting, so the other features - are not relevant (at least for that lasso model),\ni.e. they either noisy or dependent on the others \n\n","0c596d07":"# Range features by the their individual predictive power "}}