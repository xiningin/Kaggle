{"cell_type":{"2e250a73":"code","eefedf67":"code","9ca951b5":"code","5304d3a5":"code","699f560f":"code","28d7e94f":"code","dae2577b":"code","02896772":"code","767b0ab1":"code","97dc1bc2":"code","b0b3fbb4":"markdown","2d47c676":"markdown","b0ad0835":"markdown","63ee36ae":"markdown","ecf2b3e2":"markdown"},"source":{"2e250a73":"import pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv(\"..\/input\/train.csv\")\nY = data.SalePrice\nX = data.drop([\"SalePrice\"], axis=1)\n\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\nX.describe()","eefedf67":"(X\n .isna()\n .sum()\n .pipe(lambda series: series[series > 0])\n)","9ca951b5":"\"\"\"\nBecause of the nature of this data and the descriptions provided\nthe value of most of the missing data can be inferred without imputation\nLot frontage will be NaN if there is no street, so replace na with 0\nAlley will be NaN if there is no alley entrance, so repalce with \"None\"\nWhile MasVnrType does have a \"none\" value that is different from NaN,\nI think it makes the msot sense to replace these with none as well\nMasVnrArea will be 0 if NaN is present\nSame goes for all the Bsmt columns\nHouses have to have electrical, so replace unknown\nFireplace NaN is None\nsame for all garage entries (YrBlt as 0)\nsame for pool and fence and miscFeature\n\"\"\"\nXClean = X.fillna(value={\"LotFrontage\": 0.0,\n                         \"Alley\": \"None\",\n                         \"MasVnrType\": \"None\",\n                         \"MasVnrArea\": 0.0,\n                         \"BsmtQual\": \"None\",\n                         \"BsmtCond\": \"None\",\n                         \"BsmtExposure\": \"None\",\n                         \"BsmtFinType1\": \"None\",\n                         \"BsmtFinType2\": \"None\",\n                         \"FireplaceQu\": \"None\",\n                         \"GarageType\": \"None\",\n                         \"GarageYrBlt\": 0.0,\n                         \"GarageFinish\": \"None\",\n                         \"GarageQual\": \"None\",\n                         \"GarageCond\": \"None\",\n                         \"PoolQC\": \"None\",\n                         \"Fence\": \"None\",\n                         \"MiscFeature\": \"None\"\n                        }\n                 )\nXClean.isna().sum().pipe(lambda series: series[series > 0])","5304d3a5":"(test\n .isna()\n .sum()\n .pipe(lambda series: series[series > 0])\n)","699f560f":"\"\"\"\nA number of these are the same and can easily be addressed with the same logic as above\nThe bsmt columns new to the test data are all linked to an entry with BsmtQual == NaN\nso can be set to none\/0.0\n\"\"\"\ntestClean = test.fillna(value={\"LotFrontage\": 0.0,\n                         \"Alley\": \"None\",\n                         \"MasVnrType\": \"None\",\n                         \"MasVnrArea\": 0.0,\n                         \"BsmtQual\": \"None\",\n                         \"BsmtCond\": \"None\",\n                         \"BsmtExposure\": \"None\",\n                         \"BsmtFinType1\": \"None\",\n                         \"BsmtFinSF1\": 0.0,\n                         \"BsmtFinType2\": \"None\",\n                         \"BsmtFinSF2\": 0.0,\n                         \"BsmtUnfSF\": 0.0,\n                         \"TotalBsmtSF\" : 0.0,\n                         \"BsmtFullBath\" : 0.0,\n                         \"BsmtHalfBath\" : 0.0,\n                         \"FireplaceQu\": \"None\",\n                         \"GarageType\": \"None\",\n                         \"GarageYrBlt\": 0,\n                         \"GarageFinish\": \"None\",\n                         \"GarageQual\": \"None\",\n                         \"GarageCond\": \"None\",\n                         \"PoolQC\": \"None\",\n                         \"Fence\": \"None\",\n                         \"MiscFeature\": \"None\"\n                        }\n                 )\ntestClean.isna().sum().pipe(lambda series: series[series > 0])","28d7e94f":"\"\"\"\nThe remaining values cannot be deduced from trends, so a simple imputer will be used to fill them in\nFor numeric values, the mean will be used. For objects, the most frequent will be\nFirst we train_test_split for XGBoost\n\"\"\"\ntrainX, evalX, trainY, evalY = train_test_split(XClean, Y, test_size=0.2)\n\ntrainXNumeric = trainX.select_dtypes(include=[\"int64\", \"float64\"])\nevalXNumeric = evalX.select_dtypes(include=[\"int64\", \"float64\"])\nimputeNumeric = SimpleImputer(strategy=\"mean\")\nimputeNumeric.fit_transform(trainXNumeric)\nimputeNumeric.transform(evalXNumeric)\n\ntrainXObject = trainX.select_dtypes(include=[\"object\"])\nevalXObject = evalX.select_dtypes(include=[\"object\"])\nimputeObject = SimpleImputer(strategy=\"most_frequent\")\nimputeObject.fit_transform(trainXObject)\nimputeObject.transform(evalXObject)\ntrainXImputed = trainXNumeric.join(trainXObject)\nevalXImputed = evalXNumeric.join(trainXObject)","dae2577b":"for col in trainXImputed.columns:\n    if XClean[col].dtype == \"O\":\n        print(\"Column: %s %d  \" % (col, len(trainXImputed[col].unique())))","02896772":"#Neighborhood is on the higher side, but will use it to prevent data leakage.\ntrainXCoded = pd.get_dummies(trainXImputed)\nevalXCoded = pd.get_dummies(evalXImputed)\ntrainXFinal, evalXFinal = trainXCoded.align(evalXCoded,\n                                            join=\"inner\",\n                                            axis=1\n                                           )","767b0ab1":"model = XGBRegressor(n_estimators=1000,\n                     learning_rate=0.01,\n                     random_state=0\n                    )\nmodel.fit(trainXFinal, trainY,\n          early_stopping_rounds=5,\n          eval_set=[(evalXFinal, evalY)]\n         )","97dc1bc2":"XNumeric = XClean.select_dtypes(include=[\"int64\", \"float64\"])\ntestNumeric = testClean.select_dtypes(include=[\"int64\", \"float64\"])\nimputeNumeric = SimpleImputer(strategy=\"mean\")\nimputeNumeric.fit_transform(XNumeric)\nimputeNumeric.transform(testNumeric)\n\nXObject = XClean.select_dtypes(include=[\"object\"])\ntestObject = testClean.select_dtypes(include=[\"object\"])\nimputeObject = SimpleImputer(strategy=\"most_frequent\")\nimputeObject.fit_transform(XObject)\nimputeObject.transform(testObject)\n\nXImputed = XNumeric.join(XObject)\ntestImputed = testNumeric.join(testObject)\n\nXCoded = pd.get_dummies(XImputed)\ntestCoded = pd.get_dummies(testImputed)\nXFinal, testFinal = XCoded.align(testCoded,\n                                 join=\"inner\",\n                                 axis=1\n                                )\n\nmodel = XGBRegressor(n_estimators=555,\n                     learning_rate=0.01,\n                     random_state=0\n                    )\nmodel.fit(XFinal, Y)\nmodel.predict(testFinal)\noutput = pd.DataFrame({\"Id\": testFinal.Id,\n                       \"SalePrice\": model.predict(testFinal)\n                      }\n                     ).set_index(\"Id\")\noutput.to_csv(\"submission.csv\")","b0b3fbb4":"All NaNs dealt with. Now on to encoding. Using One-hot. Make sure that no entry has an excessive number of objects.","2d47c676":"First things first, let's deal with NAs.","b0ad0835":"Then, we make and train the model.","63ee36ae":"Finally, we train on the whole dataset and predict on the test data using n_estimators = 555.\nThe question: I know I should train test split before imputing\/encoding. If I use a train test split to find the best n_estimators for training my XGBoost model but then decide to use the whole dataset before my final prediction, should I use the fitted imputer from the train test split on the whole data set or should I refit? Refitting strikes me as correct, so that's what I've done.","ecf2b3e2":"This is my work through of the continually housing market competition on Kaggle.  This represents a basic first start, with intent to add pipelines, automation of encoding\/imputing, and parameter optimisation later"}}