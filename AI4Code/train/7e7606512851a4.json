{"cell_type":{"d040334d":"code","480b2bc1":"code","d8bfee1b":"code","b8d6e863":"code","36b11402":"code","2cb204b1":"code","f74112f7":"code","b0290741":"code","4dca8d4f":"code","52ba2e33":"code","fcc1a21a":"code","3d8a86af":"code","d3e9c27b":"code","b2b8abf2":"code","6029ace5":"code","94e48b9c":"code","06db95fd":"code","b52f00e1":"code","25a50875":"code","25f56cc8":"markdown","2f12500c":"markdown","49f2d5df":"markdown","3b702af2":"markdown","8e9a0515":"markdown","7e564c4a":"markdown","fa22b601":"markdown","76dbc536":"markdown"},"source":{"d040334d":"import tensorflow as tf\nimport numpy as np\nimport random\nimport os\nimport time\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import *\nfrom sklearn.metrics import *\nfrom tensorflow.keras.models import load_model\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # ignore warning :)\n\n# tensorboard = TensorBoard(log_dir='mylog')\n\ngpus = tf.config.experimental.list_physical_devices(device_type='GPU')\nfor gpu in gpus:\n    tf.config.experimental.set_memory_growth(device=gpu, enable=True)\n    \ntrain_tfrecord = 'XRay_train.tfrecords'\ntest_tfrecord = 'XRay_test.tfrecords'\ntrain_percentage = 0.8  # Proportion of training set\n\nrandom.seed(2021)\n\ninput_path='\/kaggle\/input\/covid19-radiography-database\/COVID-19 Radiography Database\/'","480b2bc1":"learning_rate = 0.001\nbuffer_size = 512\nbatch_size = 16\nepochs = 20\n\nimg_size = 224","d8bfee1b":"def read_directory():\n    data_filenames = []\n    data_labels = []\n\n    for filename in os.listdir(input_path + 'COVID-19'):\n        data_filenames.append(input_path + 'COVID-19\/' + filename)\n        data_labels.append(0)\n\n    for filename in os.listdir(input_path + 'NORMAL'):\n        data_filenames.append(input_path + 'NORMAL\/' + filename)\n        data_labels.append(1)\n\n    for filename in os.listdir(input_path + 'Viral Pneumonia'):\n        data_filenames.append(input_path + 'Viral Pneumonia\/' + filename)\n        data_labels.append(2)\n        \n    data_size = len(data_labels)\n\n    tmp_uni = list(zip(data_filenames, data_labels))\n\n    random.shuffle(tmp_uni)\n\n    train_size = int(data_size * train_percentage)\n    print('Size of training set\uff1a', train_size)\n    print('Size of test set\uff1a', data_size - train_size)\n\n    train_list = tmp_uni[0:train_size]\n    test_list = tmp_uni[train_size:]\n\n    train_filenames, train_labels = zip(*train_list)\n    test_filenames, test_labels = zip(*test_list)\n\n    return train_filenames, train_labels, test_filenames, test_labels","b8d6e863":"def build_train_tfrecord(train_filenames, train_labels):  # Generate TFRecord of training set \n    with tf.io.TFRecordWriter(train_tfrecord)as writer:\n        for filename, label in zip(train_filenames, train_labels):\n            image = open(filename, 'rb').read()\n\n            feature = {\n                'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),  # img > Bytes\n                'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))  # label > Int\n            }\n\n            example = tf.train.Example(features=tf.train.Features(feature=feature))\n            writer.write(example.SerializeToString())\n\n\ndef build_test_tfrecord(test_filenames, test_labels):  # Generate TFRecord of test set\n    with tf.io.TFRecordWriter(test_tfrecord)as writer:\n        for filename, label in zip(test_filenames, test_labels):\n            image = open(filename, 'rb').read()\n\n            feature = {\n                'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),\n                'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n            }\n\n            example = tf.train.Example(features=tf.train.Features(feature=feature))\n            writer.write(example.SerializeToString())","36b11402":"def _parse_example(example_string):\n    feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([], tf.int64),\n    }\n\n    feature_dict = tf.io.parse_single_example(example_string, feature_description)\n    feature_dict['image'] = tf.io.decode_png(feature_dict['image'], channels=3)\n    feature_dict['image'] = tf.image.resize(feature_dict['image'], [img_size, img_size]) \/ 255.0\n    return feature_dict['image'], feature_dict['label']\n\n\ndef get_train_dataset(train_tfrecord):  # read TFRecord\n    raw_train_dataset = tf.data.TFRecordDataset(train_tfrecord)\n    train_dataset = raw_train_dataset.map(_parse_example)\n\n    return train_dataset\n\n\ndef get_test_dataset(test_tfrecord):\n    raw_test_dataset = tf.data.TFRecordDataset(test_tfrecord)\n    test_dataset = raw_test_dataset.map(_parse_example)\n\n    return test_dataset\n\n\ndef data_Preprocessing(train_dataset, test_dataset):\n    train_dataset = train_dataset.shuffle(buffer_size)\n    train_dataset = train_dataset.batch(batch_size)\n    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n    test_dataset = test_dataset.batch(batch_size)\n    test_dataset = test_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n    return train_dataset, test_dataset","2cb204b1":"class CNN(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = tf.keras.layers.Conv2D(\n            filters=32,\n            kernel_size=[3, 3],\n            padding='same',\n            activation=tf.nn.relu,\n            input_shape=(img_size, img_size, 3)\n        )\n        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n        self.conv2 = tf.keras.layers.Conv2D(\n            filters=64,\n            kernel_size=[5, 5],\n            padding='same',\n            activation=tf.nn.relu\n        )\n        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n        self.flatten = tf.keras.layers.Reshape(target_shape=(int(img_size\/4) * int(img_size\/4) * 64,))\n        self.dense1 = tf.keras.layers.Dense(units=512, activation=tf.nn.relu)\n        self.drop1 = tf.keras.layers.Dropout(0.5)\n        self.dense2 = tf.keras.layers.Dense(units=3, activation='softmax')\n\n    def call(self, inputs):\n        x = self.conv1(inputs)\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = self.pool2(x)\n        x = self.flatten(x)\n        x = self.dense1(x)\n        x = self.drop1(x)\n        output = self.dense2(x)\n        return output","f74112f7":"models=[]\nhistories=[]","b0290741":"def Xception_model():\n    xception = tf.keras.applications.Xception(weights='imagenet',include_top=False, input_shape=[img_size,img_size,3])\n    xception.trainable= True\n    model = tf.keras.Sequential([\n        xception,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(3, activation='softmax')\n    ])\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\n        metrics=[tf.keras.metrics.sparse_categorical_accuracy]\n    )\n        \n    return model","4dca8d4f":"def VGG16_model():\n    vgg16 = tf.keras.applications.VGG16(weights='imagenet',include_top=False, input_shape=[img_size,img_size,3])\n    vgg16.trainable= True\n    model = tf.keras.Sequential([\n        vgg16,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(3, activation='softmax')\n    ])\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\n        metrics=[tf.keras.metrics.sparse_categorical_accuracy]\n    )\n    \n    return model","52ba2e33":"def ResNet50_model():\n    res50 = tf.keras.applications.ResNet50(weights='imagenet',include_top=False, input_shape=[img_size,img_size,3])\n    res50.trainable= True\n    model = tf.keras.Sequential([\n        res50,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(3, activation='softmax')\n    ])\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\n        metrics=[tf.keras.metrics.sparse_categorical_accuracy]\n    )\n    \n    return model","fcc1a21a":"def InceptionV3_model():\n    incpV3 = tf.keras.applications.InceptionV3(weights='imagenet',include_top=False, input_shape=[img_size,img_size,3])\n    incpV3.trainable= True\n    model = tf.keras.Sequential([\n        incpV3,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(3, activation='softmax')\n    ])\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\n        metrics=[tf.keras.metrics.sparse_categorical_accuracy]\n    )\n    \n    return model","3d8a86af":"def MobileNetV2_model():\n    mobV2 = tf.keras.applications.MobileNetV2(weights='imagenet',include_top=False, input_shape=[img_size,img_size,3])\n    mobV2.trainable= True\n    model = tf.keras.Sequential([\n        mobV2,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(3, activation='softmax')\n    ])\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\n        metrics=[tf.keras.metrics.sparse_categorical_accuracy]\n    )\n    \n    return model","d3e9c27b":"def DenseNet121_model():\n    den121 = tf.keras.applications.DenseNet121(weights='imagenet',include_top=False, input_shape=[img_size,img_size,3])\n    den121.trainable= True\n    model = tf.keras.Sequential([\n        den121,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(3, activation='softmax')\n    ])\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\n        metrics=[tf.keras.metrics.sparse_categorical_accuracy]\n    )\n    \n    return model","b2b8abf2":"def InceptionResNetV2_model():\n    incp_res_v2 = tf.keras.applications.InceptionResNetV2(weights='imagenet',include_top=False, input_shape=[img_size,img_size,3])\n    incp_res_v2.trainable= True\n    model = tf.keras.Sequential([\n        incp_res_v2,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(3, activation='softmax')\n    ])\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\n        metrics=[tf.keras.metrics.sparse_categorical_accuracy]\n    )\n    \n    return model","6029ace5":"def scheduler(epoch,lr):\n    if epoch<7:\n        return lr\n    else:\n        return lr*0.9\n    \ncallback = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose=1)\n\nclass_weight = {0:3., 1:1., 2:1.}","94e48b9c":"def train():\n    #     model = InceptionV3_model()\n    #     model = MobileNetV2_model()\n    #     model = VGG16_model()\n    \n    time_start = time.time()\n    model1 = ResNet50_model()\n    models.append(model1)\n    model1.summary()\n    train_history1 = model1.fit(train_dataset, epochs=epochs, class_weight=class_weight, callbacks=[callback])\n    histories.append(train_history1)\n    time_end = time.time()\n    print('Model1 Training Time:', time_end - time_start)\n    print('\\n')\n    \n    time_start = time.time()\n    model2 = InceptionResNetV2_model()\n    models.append(model2)\n    model2.summary()\n    train_history2 = model2.fit(train_dataset, epochs=epochs, class_weight=class_weight, callbacks=[callback])\n    histories.append(train_history2)\n    time_end = time.time()\n    print('Model2 Training Time:', time_end - time_start)\n    print('\\n')\n    \n    time_start = time.time()\n    model3 = DenseNet121_model()\n    models.append(model3)\n    model3.summary()\n    train_history3 = model3.fit(train_dataset, epochs=epochs, callbacks=[callback])\n    histories.append(train_history3)\n    time_end = time.time()\n    print('Model3 Training Time:', time_end - time_start)\n    print('\\n')\n     \n#     model.save('mymodel.h5')\n    \n#     print('Model saved.')\n\n    return histories","06db95fd":"def show_train_history(train_history, index):\n    plt.plot(train_history.history[index])\n    plt.title('Train History')\n    plt.ylabel(index)\n    plt.xlabel('Epoch')\n    plt.show()","b52f00e1":"def test(test_labels):\n    test_labels = np.array(test_labels)\n#     model = load_model('\/kaggle\/working\/mymodel.h5')\n\n    print('Testing:')\n    \n#     model.evaluate(test_dataset)\n\n    probabilities = (models[0].predict(test_dataset)+models[1].predict(test_dataset)+models[2].predict(test_dataset))\/3\n#     predIdxs = model.predict(test_dataset)\n    predIdxs = np.argmax(probabilities, axis=1) \n\n    target_names = ['COVID-19', 'NORMAL', 'Viral Pneumonia']\n    print('\\n')\n    print(classification_report(test_labels, predIdxs, target_names=target_names, digits=5))","25a50875":"if __name__ == \"__main__\":\n    train_filenames, train_labels, test_filenames, test_labels = read_directory()\n\n    build_train_tfrecord(train_filenames, train_labels)\n    build_test_tfrecord(test_filenames, test_labels)\n\n    train_dataset = get_train_dataset(train_tfrecord)\n    test_dataset = get_test_dataset(test_tfrecord)\n\n    print('Info of train_dataset', type(train_dataset))\n    print('Info of test_dataset', type(test_dataset))\n\n    train_dataset, test_dataset = data_Preprocessing(train_dataset, test_dataset) \n\n    histories = train()\n    \n    test(test_labels)\n    \n    for i in range(3):\n        show_train_history(histories[i], 'sparse_categorical_accuracy') \n    \n    # Avoid filling up the disk, if you want to save the tfrecords, just '#' these lines:\n    for filename in os.listdir('\/kaggle\/working'): \n        if 'X' in filename: # Delete the tfrecord.\n            os.remove('\/kaggle\/working\/' + filename)","25f56cc8":"# Decode TFRecord and get data","2f12500c":"# Load data","49f2d5df":"# Model","3b702af2":"# Initialization","8e9a0515":"# Train&Test","7e564c4a":"# Intro: \nThis script classifies the images to 3 categories: **COVID-19**, **NORMAL** and **Viral Pneumonia**. This script also includes the use of **tfrecord**.  \nThis notebook aims at compareing the performances of different models on this classification problem.","fa22b601":"# Build TFRecord","76dbc536":"# Run Now\uff01"}}