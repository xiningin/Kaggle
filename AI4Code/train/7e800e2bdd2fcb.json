{"cell_type":{"5614023c":"code","774e44b5":"code","ef2b36a0":"code","92c4799c":"code","997b05d2":"code","8785fe1a":"code","2324eef1":"code","313589c7":"code","7b095be5":"code","27784c38":"code","7f18b409":"code","f1c003e0":"code","50bf35c4":"code","c34e95fd":"code","4e479183":"code","423bff5f":"code","94aae06a":"code","e2164570":"code","939df5d4":"code","b71ea435":"code","0c6473a2":"code","317936eb":"code","c921777f":"code","5ed5d1ee":"code","37be48d9":"markdown","19e55bc0":"markdown","a657762f":"markdown","26844df0":"markdown","b205dcfc":"markdown","e7d7b352":"markdown","168f4915":"markdown","c0d33c4a":"markdown","9f38e65c":"markdown","5e77422a":"markdown","d4b6af3b":"markdown","32761df3":"markdown","ddf78312":"markdown","09ca463c":"markdown","07199986":"markdown","95006927":"markdown"},"source":{"5614023c":"!pip install efficientnet-pytorch -qqq","774e44b5":"from fastai.vision.all import *\nimport albumentations as A\nfrom efficientnet_pytorch import EfficientNet \n\nimport warnings\nwarnings.filterwarnings('ignore')","ef2b36a0":"set_seed(42)","92c4799c":"class CFG:\n    size=512\n    bs=32\n    model='efficientnet-b3'","997b05d2":"class AlbumentationsTransform(RandTransform):\n    '''Transform handler for multiple Albumentation transforms'''\n    split_idx, order=None,2\n    def __init__(self, train_aug, valid_aug): store_attr()\n    \n    def before_call(self, b, split_idx):\n        self.idx = split_idx\n    \n    def encodes(self, img: PILImage):\n        if self.idx == 0:\n            aug_img = self.train_aug(image=np.array(img))['image']\n            \n        else:\n            aug_img = self.valid_aug(image=np.array(img))['image']\n            \n        return PILImage.create(aug_img)","8785fe1a":"def get_train_aug(): \n    return A.Compose([\n        A.RandomResizedCrop(CFG.size, CFG.size),\n        A.Transpose(p=0.5),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.ShiftScaleRotate(p=0.5),\n        A.HueSaturationValue(\n            hue_shift_limit=0.2, \n            sat_shift_limit=0.2, \n            val_shift_limit=0.2, \n            p=0.5\n        ),\n        A.RandomBrightnessContrast(\n            brightness_limit=(-0.1,0.1), \n            contrast_limit=(-0.1, 0.1), \n            p=0.5\n        ),\n        A.CoarseDropout(p=0.5),\n        A.Cutout(p=0.5)\n])","2324eef1":"def get_valid_aug(): \n    return A.Compose([\n        A.Resize(CFG.size, CFG.size),\n        A.CenterCrop(CFG.size, CFG.size, p=1.)\n], p=1.)","313589c7":"item_tfms = AlbumentationsTransform(get_train_aug(), get_valid_aug())","7b095be5":"path = Path('\/kaggle\/input\/cassavapreprocessed')","27784c38":"path.ls()","7f18b409":"train_images = path\/'train_images'\/'train_images'\ntest_images  = path\/'test_images'\/'test_images'","f1c003e0":"train_df = pd.read_csv(path\/'new_merged.csv', low_memory=False)","50bf35c4":"train_df.drop('Unnamed: 0', axis=1, inplace=True)","c34e95fd":"train_df.head()","4e479183":"def get_x(row): return train_images\/row['image_id']\ndef get_y(row): return row['label']","423bff5f":"dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                  # Allocate 20% of data to the validation set\n                  splitter=RandomSplitter(valid_pct=0.2, seed=42),\n                  \n                  # Use the functions defined above to get items and labels\n                  get_x=get_x,\n                  get_y=get_y,\n                   \n                  # Use our item_tfms on each image seperately \n                  item_tfms=item_tfms,\n                   \n                  # Normalize a batch of images with imagenet stats\n                  batch_tfms=[Normalize.from_stats(*imagenet_stats)])","94aae06a":"dls = dblock.dataloaders(train_df, bs=CFG.bs)","e2164570":"class CassavaModel(Module):\n    def __init__(self, num_classes):\n        self.effnet = EfficientNet.from_pretrained(CFG.model)\n        self.dropout = nn.Dropout(0.1)\n        self.out = nn.Linear(1536, num_classes)\n        \n    def forward(self, image):\n        batch_size, _, _, _ = image.shape\n        \n        x = self.effnet.extract_features(image)\n        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1)\n        output = self.out(self.dropout(x))\n        \n        return output","939df5d4":"cassava_net = CassavaModel(dls.c)","b71ea435":"from fastai.callback.cutmix import *","0c6473a2":"learn = Learner(dls, cassava_net, loss_func=LabelSmoothingCrossEntropy(),\n               metrics=accuracy, cbs=CutMix()).to_fp16()","317936eb":"learn.lr_find()","c921777f":"learn.fit_flat_cos(20, lr=3e-3, pct_start=0.0,\n                  cbs=[ReduceLROnPlateau(patience=3),\n                      SaveModelCallback()])","5ed5d1ee":"learn.save('cassava_net')\nlearn.export('inference')","37be48d9":"# Save and export the model for inference","19e55bc0":"# Get merged data from the 2019 and current competition","a657762f":"# CFG","26844df0":"# Create the pretrained EfficientNet model","b205dcfc":"## Find the learning rate to fine-tune the head of the model","e7d7b352":"## Validation augmentation","168f4915":"# Training the model","c0d33c4a":"## Fine-tune using the Cosine Annealing approach (by using fit_flat_cos with pct_start=0.0). We use 3e-3 as the learning rate. We add a callback to reduce the learning rates after 3 epochs of no improvement in validation loss and a callback to save the model with the best performance during training.","9f38e65c":"## Get item transforms for both sets","5e77422a":"## Set a seed for reproducibility","d4b6af3b":"We use Cross Entropy with label smoothing as our loss function and Adam as our optimization function. This loss function should be especially helpful here, because the dataset is noisy and it helps with making the model less confident and extreme about it's predictions. The augmentation callback for the model will be CutMix to help with generalization. We have the model train on mixed-precision floating points to speed up the process.","32761df3":"# Creating the DataBlock","ddf78312":"# Setting up the transforms","09ca463c":"## Get DataLoaders","07199986":"## Training augmentation","95006927":"# Library"}}