{"cell_type":{"7bd4b80d":"code","75afe1c8":"code","0f62ce77":"code","d966e418":"code","99c33784":"code","61c08c17":"code","621b6813":"code","5b6e51a1":"code","61ab9d2d":"code","6bcc6c9e":"code","0b5eebf4":"code","be831b1b":"code","1c0d90b0":"code","02040c08":"code","dce1d1a4":"code","da5398fb":"code","92a125b2":"code","ecc31f69":"code","df78673f":"code","717c1d42":"code","9b9e01c4":"code","5eb97fbf":"code","9a84cd3e":"code","58e298fb":"code","790e7a91":"code","d535738e":"code","f905d8ef":"code","d3941ddb":"code","bd517500":"code","af716c56":"code","8af68a34":"code","6ec6b5fa":"code","adf81d2d":"code","809c7e07":"code","0e4a8367":"code","028760d6":"code","9f530795":"code","7f567831":"code","2a5e9224":"code","421d1a7c":"code","a3a0e3b5":"code","2be63116":"code","a3c28497":"markdown","b236db49":"markdown","1607ed70":"markdown","1e657e5f":"markdown","1ab49ce4":"markdown","740e0c4a":"markdown","23f6b837":"markdown","70d82a84":"markdown","33e6c941":"markdown","5825da09":"markdown"},"source":{"7bd4b80d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualisation\nimport tensorflow as tf\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","75afe1c8":"data = pd.read_csv(\"..\/input\/year_prediction.csv\")\ndata = data.rename(index=str, columns={\"label\":\"year\"})","0f62ce77":"nsongs = {}\nfor y in range(1922,2012):\n    nsongs[y] = len(data[data.year==y])\nyrs = range(1922,2011)\nvalues = [nsongs[y] for y in yrs]\nplt.bar(yrs, values, align='center')\nplt.xlabel(\"Year\")\nplt.ylabel(\"Number of songs\")","d966e418":"# separate input attributes and output into different dataframes\nX = data.iloc[:,1:]\nY = data.iloc[:,0]\n\n# Train set\nX_train = X.iloc[0:463715,:]\ny_train = Y.iloc[0:463715]\n\n# Validation set\nX_test = X.iloc[463715:,:]\ny_test = Y.iloc[463715:]\n","99c33784":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\n# Fit on training set only.\nscaler.fit(X_train)\n# Apply transform to both the train set and the test set.\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nX_train = pd.DataFrame(X_train_scaled,columns=X_train.columns)\nX_test = pd.DataFrame(X_test_scaled,columns=X_train.columns)","61c08c17":"X_train.describe()","621b6813":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n# Fit on training set only.\nscaler.fit(X_train)\n# Apply transform to both the train set and the test set.\nX_train_std = scaler.transform(X_train)\nX_test_std = scaler.transform(X_test)","5b6e51a1":"X_train_std = pd.DataFrame(X_train_std,columns=X_train.columns)\nX_train_std.describe()","61ab9d2d":"from sklearn.decomposition import PCA\n# Make an instance of the Model\npca = PCA(.90)\n\n# We fit to only our training set\npca.fit(X_train_std)\n# Print number of components generated\npca.n_components_","6bcc6c9e":"X_train_proc = pca.transform(X_train_std)\nX_test_proc = pca.transform(X_test_std)","0b5eebf4":"y_train_proc = y_train - min(y_train)\ny_test_proc = y_test - min(y_test)\n# y_train_proc","be831b1b":"from tensorflow.python.keras import Sequential\nfrom tensorflow.python.keras.layers import Dense, Lambda, Dropout\n# from tensorflow.python.keras.initializers import Initializer\nfrom tensorflow.python.keras.utils import to_categorical\nfrom tensorflow.python.keras.callbacks import ReduceLROnPlateau, EarlyStopping","1c0d90b0":"y_train_hot = to_categorical(y_train_proc, 90)\ny_test_hot = to_categorical(y_test_proc, 90)","02040c08":"print(X_train_proc.shape)\nprint(y_test_hot.shape)","dce1d1a4":"def plot(history):\n    epochs = range(1, len(history.history['loss']) + 1)\n    plt.plot(epochs, history.history['mean_absolute_error'], label='train');\n    plt.plot(epochs, history.history['val_mean_absolute_error'], label='val');\n    plt.xlabel('epoch');\n    plt.ylabel('mae');\n    plt.legend();\n    plt.show();","da5398fb":"model1 = Sequential()\nmodel1.add(Dense(55, input_shape=(55,)))\nmodel1.add(Dense(110))\nmodel1.add(Dense(90, activation='softmax'))","92a125b2":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=4, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.0001)","ecc31f69":"model1.compile(optimizer='adam'\n             , loss='categorical_crossentropy'\n             , metrics=['accuracy'])","df78673f":"fit1 = model1.fit(x=X_train_proc, y=y_train_hot\n          , epochs=5\n          , batch_size=64\n          , validation_data=(X_test_proc, y_test_hot)\n          , callbacks=[learning_rate_reduction])","717c1d42":"# plot(fit1)","9b9e01c4":"model1.summary()","5eb97fbf":"preds = model1.predict_classes(X_test_proc)","9a84cd3e":"print(np.array(y_test_proc))\nprint(preds)\nnp.mean(np.absolute((preds-np.array(y_test_proc))))","58e298fb":"model2 = Sequential()\nmodel2.add(Dense(55, input_shape=(55,), activation='relu'))\nmodel2.add(Dense(1))","790e7a91":"learning_rate_reduction1 = ReduceLROnPlateau(monitor='mean_absolute_error', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.0001)","d535738e":"model2.compile(optimizer='adam'\n             , loss='mse'\n             , metrics=['mae'])","f905d8ef":"fit2 = model2.fit(x=X_train_proc, y=y_train_proc\n          , epochs=10\n          , batch_size=64\n          , validation_data=(X_test_proc, y_test_proc)\n          , callbacks=[learning_rate_reduction1])","d3941ddb":"preds_model_rms = model2.predict(X_test_proc)\nnp.mean(np.absolute(preds_model_rms.T-np.array(y_test_proc)))","bd517500":"plot(fit2)","af716c56":"# from sklearn.metrics import mean_squared_error, r2_score\n# mean_squared_error(predictions_linearRegr, np.array(y_test_proc))\nes = EarlyStopping(monitor='val_mean_absolute_error', patience=2, restore_best_weights=True)","8af68a34":"model3 = Sequential()\nmodel3.add(Dense(55, input_shape=(55,), activation='relu'))\nmodel3.add(Dense(110, activation='relu'))\nmodel3.add(Dropout(0.2))\nmodel3.add(Dense(1))\n\nmodel3.compile(optimizer='adam'\n             , loss='mse'\n             , metrics=['mae'])","6ec6b5fa":"fit3 = model3.fit(x=X_train_proc, y=y_train_proc\n          , epochs=10\n          , batch_size=64\n          , validation_data=(X_test_proc, y_test_proc)\n          , callbacks=[learning_rate_reduction1, es])","adf81d2d":"preds_model_rms = model3.predict(X_test_proc)\nnp.mean(np.absolute(preds_model_rms.T-np.array(y_test_proc)))","809c7e07":"plot(fit3)","0e4a8367":"model3.compile(optimizer='adam'\n             , loss='mse'\n             , metrics=['mae'])\nfit3 = model3.fit(x=X_train_proc, y=y_train_proc\n          , epochs=10\n          , batch_size=128\n          , validation_data=(X_test_proc, y_test_proc)\n          , callbacks=[learning_rate_reduction1, es])","028760d6":"preds_model_rms = model3.predict(X_test_proc)\nnp.mean(np.absolute(preds_model_rms.T-np.array(y_test_proc)))","9f530795":"plot(fit3)","7f567831":"from keras.optimizers import RMSprop\n# adam = optimizers.Adam()","2a5e9224":"model3.compile(optimizer='RMSprop'\n             , loss='mse'\n             , metrics=['mae'])\nfit4 = model3.fit(x=X_train_proc, y=y_train_proc\n          , epochs=10\n          , batch_size=64\n          , validation_data=(X_test_proc, y_test_proc)\n          , callbacks=[learning_rate_reduction1, es])","421d1a7c":"preds_model_rms = model3.predict(X_test_proc)\nnp.mean(np.absolute(preds_model_rms.T-np.array(y_test_proc)))","a3a0e3b5":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(np.round_(np.array(preds_model_rms), decimals=-1), np.round_(np.array(y_test_proc), decimals=-1))","2be63116":"import seaborn as sns\nind = list(range(1920,2030,10))\ndf_heat = pd.DataFrame(cm, index=ind, columns=ind)\nlen(ind)\n# lab = pd.unique(df_heat[0])\nsns.heatmap(df_heat)\ndf_heat\n# df_plot.transpose().corr()\n# cm.shape","a3c28497":"90 features is lot of features and so we attempt dimensionality reduction by implementing PCA.","b236db49":"batch size = 64","1607ed70":"batch size = 128","1e657e5f":"We plot a histogram to understand how evenly spread the data is by viewing number of songs we have for a given year.","1ab49ce4":"# PCA","740e0c4a":"First we normalise our data using scikits StandardScalar.\nThis is a necessary step for pca:\n> Principle Component Analysis (PCA) as being a prime example of when normalization is important. In PCA we are interested in the components that maximize the variance. If one component (e.g. human height) varies less than another (e.g. weight) because of their respective scales (meters vs. kilos), PCA might determine that the direction of maximal variance more closely corresponds with the \u2018weight\u2019 axis, if those features are not scaled. As a change in height of one meter can be considered much more important than the change in weight of one kilogram, this is clearly incorrect.\n(https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py)","23f6b837":"Our values are normalised and so we now actually apply PCA\n(https:\/\/towardsdatascience.com\/pca-using-python-scikit-learn-e653f8989e60)","70d82a84":"Each of the features takes a wide range of different values and distributions.\n\nWe apply MinMax scaling to our data.","33e6c941":"PCA(.90) means that scikit-learn choose the minimum number of principal components such that 90% of the variance is retained.\n\nIn this case, 90% of the variance amounts to 55 principal components.","5825da09":"# Neural Network"}}