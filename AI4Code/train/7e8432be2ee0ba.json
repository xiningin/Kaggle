{"cell_type":{"f2be207f":"code","02a21e2e":"code","07de15ef":"code","4c6a725a":"code","356897c5":"code","4aaf9e8e":"code","00aff4d2":"code","519d536b":"code","80c5c1b6":"code","8f381a6b":"code","4f58dba1":"code","ade77032":"code","04306a1a":"code","2f224e04":"code","4b183fad":"code","b9e02fe2":"code","f34e91ad":"code","4cfce247":"code","3961e63f":"code","8e09f6e8":"code","83f9c840":"code","240424f4":"code","44176138":"code","2f4d74ff":"code","bc082344":"code","8d029def":"code","508d693c":"code","61713e30":"code","6a68d13d":"code","ede82d05":"code","ffc4ce60":"code","a92540a6":"code","715fd847":"code","168f4a35":"code","9b15eb34":"code","cd4663c7":"code","e78741dc":"code","24c74bf0":"code","66369c40":"code","eb9b1170":"code","af8c14a7":"code","33922558":"code","00117870":"code","b92ecffb":"code","cc4f29f1":"code","e5c7c5ed":"code","a2e1cb0a":"code","11817169":"code","9c23c9a6":"code","e5b04594":"code","a7778dd7":"code","078ebb28":"markdown","cecde486":"markdown","d39c7f37":"markdown","2b2f1ddc":"markdown","c3968733":"markdown","ec26cd32":"markdown","a1993f20":"markdown","1a53f779":"markdown","8b1fdcd0":"markdown","2357e51b":"markdown","d19ad8c1":"markdown","14b1146c":"markdown","a2af5823":"markdown","59b19579":"markdown","163e8ca4":"markdown","4275505b":"markdown","d1044261":"markdown","2a47d74d":"markdown","6982088a":"markdown","cb94600f":"markdown","dec714e3":"markdown","e29a0e93":"markdown","a7789f1d":"markdown","f1b4ea9c":"markdown","7155d441":"markdown","637b6a7e":"markdown","ac5073cb":"markdown","ffcea83e":"markdown","0deaff0b":"markdown","e4f7aa0a":"markdown","42eb7129":"markdown","2d7e866f":"markdown","32164bf5":"markdown","228caa6c":"markdown","04801ce0":"markdown","1f717f7d":"markdown"},"source":{"f2be207f":"from sklearn.metrics import classification_report\ndef get_metrics(x, y, e):\n    z = e.predict(x)\n    print(classification_report(z, y, digits=4))","02a21e2e":"# IMPORTS\nfrom time import time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","07de15ef":"flight = pd.read_csv(\"..\/input\/datavidia2019\/flight.csv\")\ntest = pd.read_csv(\"..\/input\/datavidia2019\/test.csv\")\nhotel = pd.read_csv(\"..\/input\/datavidia2019\/hotel.csv\")","4c6a725a":"def preprocess_city_trx(df):\n    import ast\n    df[\"v_city_proc\"] = df.visited_city.map(lambda x: ast.literal_eval(x.strip(\"''\")))\n    df[\"log_trx_proc\"] = df.log_transaction.map(lambda x: ast.literal_eval(x.strip(\"''\")))\n    return df\npreprocess_city_trx(test)\npreprocess_city_trx(flight).head()","356897c5":"missing_data = pd.DataFrame({'total_missing': flight.isnull().sum(), 'perc_missing': (flight.isnull().sum()\/len(flight.index))*100}) \nmissing_data","4aaf9e8e":"missing_data2 = pd.DataFrame({'total_missing': test.isnull().sum(), 'perc_missing': (test.isnull().sum()\/len(test.index))*100})\nmissing_data2","00aff4d2":"def get_is_cross_sell(df):\n    df[\"is_cross_sell\"] = df.hotel_id.map(lambda x: 0 if x == \"None\" else 1)\n    return df\nget_is_cross_sell(flight).is_cross_sell.value_counts()","519d536b":"flight[\"route\"].value_counts()","80c5c1b6":"flight['order_id'].nunique(), flight.shape[0]","8f381a6b":"print(\"{} of {} account_id from test.csv are identical from flight.csv account_id\".format(len(set(flight['account_id']) & set(test['account_id'])), len(set(test.account_id))))","4f58dba1":"flight.visited_city.value_counts()","ade77032":"y = flight.copy()\ny[\"n_trx\"] = y.log_trx_proc.map(len)\ny[y.n_trx == 2][y.trip == \"trip\"].visited_city.value_counts()","04306a1a":"plt.figure(figsize=(30,10))\nsns.countplot(x=\"is_tx_promo\", hue=\"is_cross_sell\", data=flight)\nplt.show()","2f224e04":"sns.countplot(x=\"trip\", hue=\"is_cross_sell\", data=flight)","4b183fad":"num_columns = [\"sum_transaction\", \"num_transaction\", \"price\", \"member_duration_days\", \"no_of_seats\"]\ncat_columns = [\"trip\", \"is_tx_promo\", \"airlines_name\", \"visited_city\", \"account_id\", \"service_class\", \"gender\"]","b9e02fe2":"num_columns = [\"avg_transaction\", \"price\", \"no_of_seats\", \"member_duration_days\"]\ncat_columns = [\"trip\", \"is_tx_promo\", \"airlines_name\", \"visited_city\", \"account_id\", \"service_class\", \"gender\", \"num_transaction\", \"member_registered_quarter\"]\ndef process_data2(df):    \n    f = df.copy()\n    \n    f[\"num_transaction\"] = f.log_trx_proc.map(len)\n    f[\"avg_transaction\"] = f.log_trx_proc.map(sum) \/ f[\"num_transaction\"]\n    \n#     process num_transaction into a categorical variable\n    f[\"num_transaction\"] = f[\"num_transaction\"].astype(np.int32).map(lambda x: x if x <= 14 else 15)\n\n    f.avg_transaction = f.avg_transaction \/ 10**4\n    f.price = f.price \/ 10**4\n    \n    # something new\n    f[\"member_registered_quarter\"] = ((f.member_duration_days - 329) \/\/ 90).astype(\"int32\")\n    \n    f = f.drop(columns=[\"order_id\", \"hotel_id\", \"route\", \"log_transaction\", \"v_city_proc\", \"log_trx_proc\", \"is_cross_sell\"])\n        \n    return f","f34e91ad":"def cast_columns(f):\n    for c in num_columns:\n        f[c] = f[c].astype(np.int32)\n    # categorical columns\n    for c in cat_columns:\n        f[c] = f[c].astype(\"category\")\n    return f","4cfce247":"# labels \/ target for train dataset\nty = flight[\"hotel_id\"].map(lambda x: 0 if x == \"None\" else 1)","3961e63f":"# join and preprocess whole dataset then split again\ntrain_split = flight.shape[0]\nfull_df = pd.concat((flight, test), sort=True)\np = cast_columns(process_data2(full_df))\n\ntx, tex = p.iloc[:train_split], p.iloc[train_split:]\ntx.shape, ty.shape, tex.shape","8e09f6e8":"# get index of categorical features\ncateg_feat_idx = np.where(tx.dtypes == \"category\")[0]","83f9c840":"# # Use SMOTENC\n# from imblearn.over_sampling import SMOTENC\n# sm = SMOTENC(random_state=17, categorical_features=categ_feat_idx)\n# # trainx, trainy = sm.fit_resample(train_X, train_y)\n# trainx, trainy = sm.fit_resample(tx, ty)","240424f4":"# from sklearn.model_selection import train_test_split\n# train_X, valid_X, train_y, valid_y = train_test_split(trainx, trainy, test_size=0.25, stratify=trainy, random_state=17)","44176138":"from sklearn.model_selection import train_test_split\ntrain_X, valid_X, train_y, valid_y = train_test_split(tx, ty, test_size=0.25, stratify=ty, random_state=17)","2f4d74ff":"# with delta_price\n#               precision    recall  f1-score   support\n\n#            0     0.9546    0.9965    0.9751     27800\n#            1     0.7931    0.2181    0.3422      1687\n\n#     accuracy                         0.9520     29487\n#    macro avg     0.8738    0.6073    0.6586     29487\n# weighted avg     0.9453    0.9520    0.9389     29487\n\n# without\n#               precision    recall  f1-score   support\n\n#            0     0.9549    0.9964    0.9752     27800\n#            1     0.7896    0.2247    0.3498      1687\n\n#     accuracy                         0.9522     29487\n#    macro avg     0.8722    0.6105    0.6625     29487\n# weighted avg     0.9454    0.9522    0.9394     29487\n# without service_class\n#               precision    recall  f1-score   support\n\n#            0     0.9541    0.9964    0.9748     27800\n#            1     0.7802    0.2104    0.3315      1687\n\n#     accuracy                         0.9514     29487\n#    macro avg     0.8672    0.6034    0.6531     29487\n# weighted avg     0.9442    0.9514    0.9380     29487\n# with quarters(numerical)\n#               precision    recall  f1-score   support\n\n#            0     0.9546    0.9966    0.9752     27800\n#            1     0.7957    0.2193    0.3439      1687\n\n#     accuracy                         0.9521     29487\n#    macro avg     0.8752    0.6080    0.6595     29487\n# weighted avg     0.9455    0.9521    0.9390     29487\n# with quarters(categorical)\n#               precision    recall  f1-score   support\n\n#            0     0.9549    0.9963    0.9752     27800\n#            1     0.7879    0.2247    0.3496      1687\n\n#     accuracy                         0.9522     29487\n#    macro avg     0.8714    0.6105    0.6624     29487\n# weighted avg     0.9454    0.9522    0.9394     29487\n# without member_registered_days(numerical)\n#               precision    recall  f1-score   support\n\n#            0     0.9547    0.9963    0.9751     27800\n#            1     0.7841    0.2217    0.3457      1687\n\n#     accuracy                         0.9520     29487\n#    macro avg     0.8694    0.6090    0.6604     29487\n# weighted avg     0.9450    0.9520    0.9391     29487\n# with no_of_seats categorical\n#               precision    recall  f1-score   support\n\n#            0     0.9544    0.9968    0.9751     27800\n#            1     0.8013    0.2152    0.3393      1687\n\n#     accuracy                         0.9520     29487\n#    macro avg     0.8779    0.6060    0.6572     29487\n# weighted avg     0.9456    0.9520    0.9387     29487\n# baseline\n#               precision    recall  f1-score   support\n\n#            0     0.9544    0.9964    0.9750     27800\n#            1     0.7866    0.2164    0.3394      1687\n\n#     accuracy                         0.9518     29487\n#    macro avg     0.8705    0.6064    0.6572     29487\n# weighted avg     0.9448    0.9518    0.9386     29487\n# baseline process-data2\n#               precision    recall  f1-score   support\n\n#            0     0.9549    0.9963    0.9751     27800\n#            1     0.7842    0.2241    0.3485      1687\n\n#     accuracy                         0.9521     29487\n#    macro avg     0.8696    0.6102    0.6618     29487\n# weighted avg     0.9451    0.9521    0.9393     29487\n","bc082344":"# trainx.dtypes","8d029def":"from catboost import CatBoostClassifier\nparameters = {\n    \"random_seed\": 17,\n    \"eval_metric\": \"F1\",\n    \"custom_metric\": [\"F1\", \"Precision\", \"Recall\"],\n    \"loss_function\": \"Logloss\",\n    \"task_type\": \"GPU\",\n    \"random_strength\": 3,\n    \"od_type\": \"Iter\",\n    \"verbose\":100,\n    \"cat_features\":categ_feat_idx\n}\nctb = CatBoostClassifier(**parameters)","508d693c":"ctb.fit(train_X, train_y)","61713e30":"get_metrics(valid_X,valid_y, e=ctb)","6a68d13d":"ctb.get_feature_importance(prettified=True)","ede82d05":" # Workaround for the deprecated skopt library\n!pip uninstall scikit-optimize --yes\n!pip install git+https:\/\/github.com\/darenr\/scikit-optimize","ffc4ce60":"from scipy.stats import randint\nfrom scipy.stats import uniform\n\n# Model selection\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score\n\n# Metrics\nfrom sklearn.metrics import average_precision_score, roc_auc_score, mean_absolute_error, f1_score, make_scorer\n\n# Skopt functions\nfrom skopt import BayesSearchCV\nfrom skopt import gp_minimize # Bayesian optimization using Gaussian Processes\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.utils import use_named_args # decorator to convert a list of parameters to named arguments\nfrom skopt.callbacks import DeadlineStopper # Stop the optimization before running out of a fixed budget of time.\nfrom skopt.callbacks import VerboseCallback # Callback to control the verbosity\nfrom skopt.callbacks import DeltaXStopper # Stop the optimization If the last two positions at which the objective has been evaluated are less than delta","a92540a6":"import pprint\n\"\"\"\nA wrapper for measuring time and performances of different optmizers\n\noptimizer = a sklearn or a skopt optimizer\nX = the training set \ny = our target\ntitle = a string label for the experiment\n\"\"\"\ndef report_perf(optimizer, X, y, title, callbacks=None):\n    start = time()\n    if callbacks:\n        optimizer.fit(X, y, callback=callbacks)\n    else:\n        optimizer.fit(X, y)\n    best_score = optimizer.best_score_\n    best_score_std = optimizer.cv_results_['std_test_score'][optimizer.best_index_]\n    best_params = optimizer.best_params_\n    print((title + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n           +u\"\\u00B1\"+\" %.3f\") % (time() - start, \n                                  len(optimizer.cv_results_['params']),\n                                  best_score,\n                                  best_score_std))    \n    print('Best parameters:')\n    pprint.pprint(best_params)\n    print()\n    return best_params","715fd847":"# 5-fold StratifiedCV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=8)\n\n# Scorer\nscorer = make_scorer(f1_score, average=\"macro\")\n\n# Bayesian Search Optimization Space\nsearch_spaces = {'iterations': Integer(100, 1000),\n                 'depth': Integer(4, 8),\n                 'random_strength': Real(1e-9, 10, 'log-uniform'),\n                 'bagging_temperature': Real(0.0, 1.0),\n                 'border_count': Integer(128, 255),\n                 'l2_leaf_reg': Integer(2, 30),\n                 'scale_pos_weight':Real(1.0, 5.0, 'uniform')}\nopt = BayesSearchCV(ctb,\n                    search_spaces,\n                    scoring=scorer,\n                    cv=skf,\n                    n_jobs=1,  # use just 1 job with CatBoost in order to avoid segmentation fault\n                    return_train_score=False,\n                    refit=True,\n                    optimizer_kwargs={'base_estimator': 'GP'},\n                    fit_params={'cat_features': categ_feat_idx, 'silent': True},\n                    random_state=22,\n                    verbose=True)","168f4a35":"best_params = report_perf(opt, tx, ty,'CatBoost', \n# best_params = report_perf(opt, trainx, trainy,'CatBoost', \n                          callbacks=[VerboseCallback(100),\n                                     DeltaXStopper(0.0001), \n                                     DeadlineStopper(60*5)])","9b15eb34":"best_params = {'bagging_temperature': 0.36426463210895726,\n 'border_count': 206,\n 'depth': 8,\n 'iterations': 342,\n 'l2_leaf_reg': 5,\n 'random_strength': 3.3149120967201682e-06,\n 'scale_pos_weight': 4.210304871687331}","cd4663c7":"tuned_model = CatBoostClassifier(**best_params, cat_features=categ_feat_idx, verbose=100, od_type=\"Iter\")","e78741dc":" train_y.value_counts()[0] \/\/ train_y.value_counts()[1]","24c74bf0":"ctb2 = CatBoostClassifier(**{\"random_seed\": 40,\n    \"eval_metric\": \"F1\",\n    \"custom_metric\": [\"F1\", \"Precision\", \"Recall\"],\n    \"loss_function\": \"Logloss\",\n    \"task_type\": \"GPU\",\n    \"random_strength\": 3,\n    \"od_type\": \"Iter\",\n    \"scale_pos_weight\": 12,\n    \"verbose\":100,\n    \"cat_features\":categ_feat_idx})","66369c40":"flight2 = flight.copy()\nd = flight.duplicated(subset=(\"account_id\", \"airlines_name\", \"trip\", \"is_tx_promo\", \"price\"))\nflight2 = flight[~d]","eb9b1170":"ty2 = flight2.hotel_id.map(lambda x: 0 if x == \"None\" else 1)\ntx2 = cast_columns(process_data2(flight2))[tx.columns]","af8c14a7":"categ_feat_idx2 = np.where(tx2.dtypes == \"category\")[0]","33922558":"ctb4 = CatBoostClassifier(**{\"random_seed\": 40,\n    \"eval_metric\": \"F1\",\n    \"custom_metric\": [\"F1\", \"Precision\", \"Recall\"],\n    \"loss_function\": \"Logloss\",\n    \"task_type\": \"GPU\",\n    \"random_strength\": 3,\n    \"od_type\": \"Iter\",\n    \"scale_pos_weight\": 3.5,\n    \"verbose\":100,\n    \"cat_features\":categ_feat_idx2})","00117870":"train_X2, valid_X2, train_y2, valid_y2 = train_test_split(tx2, ty2, test_size=0.25, stratify=ty2, random_state=17)","b92ecffb":"# from sklearn.ensemble import VotingClassifier\n# vc = VotingClassifier(estimators=[(\"ctb1\", ctb), (\"ctb2\", ctb2), (\"ctb3\", tuned_model)], voting=\"soft\")","cc4f29f1":"tuned_model.fit(tx, ty)","e5c7c5ed":"ctb.fit(tx, ty)\nctb2.fit(tx, ty)\nctb4.fit(tx2, ty2)","a2e1cb0a":"get_metrics(valid_X,valid_y, e=vc)","11817169":"from mlxtend.classifier import EnsembleVoteClassifier\nimport copy\neclf = EnsembleVoteClassifier(clfs=[ctb2, tuned_model], weights=[1,1.1], refit=False, voting=\"soft\")\neclf.fit(tx, ty)","9c23c9a6":"f_pred = pd.Series(eclf.predict(tex.to_numpy())).map(lambda x: \"no\" if x == 0 else \"yes\")\nf_pred.value_counts()","e5b04594":"submission = test[[\"order_id\"]]\nsubmission[\"is_cross_sell\"] = f_pred\nsubmission","a7778dd7":"submission.to_csv('submission.csv', header=True, index=False)","078ebb28":"Recommended value for scale_pos_weight is 16, but after trial and error, 12 proves to be the best value for scoring.","cecde486":"### Flight Dataset","d39c7f37":"## Model Ensembling\nEnsembling is to have not one, but multiple models fitted and then calculate their average to theoretically (if done right) have a more precise model. scikit-learn's Voting Classifier (**Soft Voting\/Majority Rule classifier for unfitted estimators**) is a simple way to implement model ensembling.","2b2f1ddc":"### Test Dataset","c3968733":"# Data Preprocessing","ec26cd32":"# Validation","a1993f20":"## Preprocessing of `visited_city` and `log_transaction`","1a53f779":"# Modelling","8b1fdcd0":"### [Catboost](https:\/\/catboost.ai\/docs\/concepts\/python-reference_catboostclassifier.html)\n\nMade by Yandex, the Russian search engine, Catboost is one of the best performing gradient boosting model along with XGBoost and LightGBM (**In our opinion, it's the best out of the three**). We chose Catboost as our model of choice because of **it's ability to handle categorical variables**, which the features of the dataset provided by this competition contains plenty of. \n\n> The idea used by CatBoost to encode the categorical variables is not new, but it has been a kind of feature engineering used various times, mostly in data science competitions like at Kaggle\u2019s. Mean encoding, also known as likelihood encoding, impact coding, or target coding, is simply a way to transform your labels into a number based on their association with the target variable. If you have a regression, you could transform labels based on the mean target value typical of that level; if it is a classification, it is simply the probability of classification of your target given that label (probability of your target, conditional on each category value). It may appear as a simple and smart feature engineering trick, but actually, it has side effects, mostly in terms of overfitting because you are taking information from the target into your predictors.","2357e51b":"# Feature Engineering","d19ad8c1":"## visited_city","14b1146c":"<center>With Upscaling<center\/>\n    \n|metric     |    0    |    1     | \n| --------- | ------- | ---------|\n|precision  |0.9988   |  0.9884  |     \n|recall     |0.9883   |  0.9988  |     \n|f1-score   |0.9935   |  0.9936  |     \n|support    |2.78e+04 |  2.78e+04|     \n|accuracy   |        0.9936      |   ","a2af5823":"## Making Our Prediction","59b19579":"In this dataset, the positive class(1) is way fewer than the negative class. The positive class makes up only 5.7% of the whole dataset. Therefore this dataset is **imbalanced**.","163e8ca4":"### Fourth Model with slightly different dataset","4275505b":"<center>No Upscaling<center\/>\n\n|metric     |    0    |    1     | \n| --------- | ------- | ---------|\n|precision  |0.9984   |  0.8833  |     \n|recall     |0.9922   |  0.9739  |     \n|f1-score   |0.9953   |  0.9264  |     \n|support    |2.78e+04 |    1687  |     \n|accuracy   |        0.9911      |   ","d1044261":"## Features used in training\n### Old Features\n* account_id              \n* member_duration_days    \n* gender                  \n* trip                    \n* service_class           \n* price                   \n* is_tx_promo             \n* no_of_seats             \n* airlines_name                               \n* visited_city           \n\n### New Features\n* sum_transaction = The total sum of transactions made by each order.\n* num_transaction = The number of transactions made by each order.\n\n### Dropped Features\n* order_id = Each order has a different id, and has no value in helping the prediction.\n* hotel_id = Target value, popped into 'tx' dataframe\n* route = Each individual data contains 'CGK - DPS' value and will have 0 feature importance.\n* log_transaction = Transformed into sum_transaction and num_transaction features.","2a47d74d":"     \n# <center>  Two Epochs Datavidia Submission\n\n#### <center> Author: [Eko J. Salim](https:\/\/morphism.id) & [Jonathan Nicholas](http:\/\/jojonicho.netlify.com)","6982088a":"At this point, there are two types of features that we can divide our dataset into. The first is numerical features, ***Or continuous numeric data***, and categorical features.","cb94600f":"## Data Preprocessing + Feature Engineering","dec714e3":"## is_cross_sell\nA transaction can be considered a cross_sell if its `hotel_id` field is not `None`","e29a0e93":"Their raw values are string of lists, we can preprocess them into an actual list for easier processing.","a7789f1d":"## Missing Values","f1b4ea9c":"We decided to use 12 features, 7 of them being categorical, meaning that it is not a continuous numeric feature such as 2 is greater than 1. 0, 1, and 2 in a categorical concept is to differentiate two or more variables so that category 2 isn't greater in terms of \"value\", it is in fact just a whole different category as 1.","7155d441":"## Route\nThere's only one unique value for route column(CGK-DPS). Therefore, `route` is obviously not a good feature to use.","637b6a7e":"### Tuned Model with best_params from Bayesian Optimization for the second model used in ensembling","ac5073cb":"**Conclusion**: No missing value in our datasets. There is no need for imputing or other such techniques","ffcea83e":"## account_id\naccount_id is a different story, after an analysis from the flight and test dataset, 3905 out of 8724 unique account ids from the test dataset are identical with the flight dataset's account id.","0deaff0b":"## Bayesian Optimization\n> Bayesian Optimization is an approach that uses Bayes Theorem to direct the search in order to find the minimum or maximum of an objective function. It is an approach that is most useful for objective functions that are complex, noisy, and\/or expensive to evaluate.\n\n![](http:\/\/inspirehep.net\/record\/1763292\/files\/step3.png)\n\nWe use Bayesian Optimization to tune these CatBoost parameters:\n* iterations\n* depth\n* random_strength\n* bagging_temperature\n* border_count\n* l2_leaf_reg\n* scale_pos_weight","e4f7aa0a":"> Condorcet's jury theorem (1784) is about an ensemble in some sense. It states that, if each member of the jury makes an independent judgment and the probability of the correct decision by each juror is more than 0.5, then the probability of the correct decision by the whole jury increases with the total number of jurors and tends to one. On the other hand, if the probability of being right is less than 0.5 for each juror, then the probability of the correct decision by the whole jury decreases with the number of jurors and tends to zero.\n\n* \ud835\udc41  is the total number of jurors;\n* \ud835\udc5a  is a minimal number of jurors that would make a majority, that is  \ud835\udc5a=\ud835\udc53\ud835\udc59\ud835\udc5c\ud835\udc5c\ud835\udc5f(\ud835\udc41\/2)+1 ;\n* (\ud835\udc41\ud835\udc56)  is the number of  \ud835\udc56 -combinations from a set with  \ud835\udc41  elements.\n* \ud835\udc5d  is the probability of the correct decision by a juror;\n* \ud835\udf07  is the probability of the correct decision by the whole jury.\n\nThe final classifier will average the outputs from all these individual classifiers. In the case of classification, this technique corresponds to voting:","42eb7129":"## Upscaling The Imbalanced Dataset\n#### Synthetic Minority Over-sampling Technique for Nominal and Continuous (SMOTE-NC).\n\n> Unlike SMOTE, SMOTE-NC for dataset containing continuous and categorical features.\n\nwhich is perfect for the current data that we are preprocessing and engineering, as we are tailoring it for yandex' CatBoost as our machine learning model of choice. But is it better than tuning CatBoost's scale_pos_weight parameter?","2d7e866f":"## Feature Scaling and Normalization\nThey are not needed since we are going to be using a gradient-boosting tree-based algorithms.","32164bf5":"### The third model used for ensembling, CatBoost with adjusted scale_pos_weight\nSince this is an imbalanced dataset, one parameter worth optimizing is CatBoost's [scale_pos_weight](#) parameter","228caa6c":"## order_id\norder_id column also has all unique values, that won't benefit our model at all.","04801ce0":"$$ \\large \\mu = \\sum_{i=m}^{N}{N\\choose i}p^i(1-p)^{N-i} $$","1f717f7d":"# Exploratory Data Analysis\nIn this competition, we are tasked to predict if a given flight transaction is a cross-sell with a hotel transaction."}}