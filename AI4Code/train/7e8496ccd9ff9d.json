{"cell_type":{"c78568a4":"code","944419bf":"code","4ab0b167":"code","9b60eef5":"code","47b23d8c":"code","6d83e635":"code","041c3241":"code","d5870764":"code","d23efd38":"code","83faf447":"code","7341e426":"code","54842bfd":"code","19755cd2":"code","4587ba3e":"code","46c5cd20":"code","2f9b409e":"code","e588f7ea":"code","0436a420":"code","bb5151ce":"code","5608aca1":"code","4a8858be":"code","34ca6a1d":"code","d264ddba":"code","459b5022":"code","fa61d009":"code","a5ec8cda":"code","b8d8949d":"code","fec0818c":"code","fea81de5":"code","4b86fe73":"code","78b94d5f":"code","49ad2fc2":"code","4ff74913":"code","1ccfe43e":"code","1bd306f9":"code","97c5109f":"code","6b085d2a":"code","8d40754c":"code","698e08e7":"code","8a926bbe":"code","31fb79a4":"code","39de9beb":"code","a47d8451":"code","dc065118":"code","5579c756":"code","e74396f2":"code","767f829e":"code","2fa1a1f0":"code","d454837c":"code","67cb27fb":"code","cfa1fe5d":"code","b1db3922":"code","415a509c":"code","8b760073":"code","fdfecf22":"code","db5b10e1":"code","d63fe1e4":"code","bd579710":"code","145f3f70":"code","482ff736":"code","402d1c8f":"code","cc934399":"code","443f1c6d":"code","6eca3396":"code","a9efbb5c":"code","d046d4b6":"code","3f1a28d9":"code","ce3ab478":"code","277ca320":"code","2b499cb2":"code","d4439593":"code","e99f5104":"code","b65f1dcc":"code","c6596070":"code","c53f712e":"code","28e0db34":"code","9c657808":"code","7d8e6865":"code","f246c024":"code","0df1df50":"code","52c7413d":"code","efb2eb4d":"code","cb68dd7d":"markdown","4587722f":"markdown","c9ba7624":"markdown","33215fc1":"markdown","1e4cdedc":"markdown","2d95f0e4":"markdown","9c991c47":"markdown","fb975e6f":"markdown","80e09b06":"markdown","151be8ae":"markdown","21c48d16":"markdown"},"source":{"c78568a4":"image_size = 640\nbatch_size = 32\nnum_workers = 4\nn_batch = 10 # to avoid oom, split 70000+ images into 10 batches\nsim_thresh = 0.8\ntext_sim_thresh = 0.9\nCHANGE_P = False\ntext_filter_threshold = 0.1","944419bf":"import pandas as pd\nimport numpy as np\nimport sys\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')\nimport os\nimport sys\nimport time\nimport cv2\nimport PIL.Image\nimport random\nfrom sklearn.metrics import accuracy_score\nfrom tqdm.notebook import tqdm\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport albumentations\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport gc\nfrom sklearn.metrics import roc_auc_score\n%matplotlib inline\nimport seaborn as sns\nfrom pylab import rcParams\nimport timm\nfrom warnings import filterwarnings\nfrom sklearn.preprocessing import LabelEncoder\nimport math\nimport glob\nfilterwarnings(\"ignore\")\n\ndevice = torch.device('cuda') \nimport pickle\nimport cudf\nimport cuml\nimport cupy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors","4ab0b167":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    print(f'Setting all seeds to be {seed} to reproduce...')\nseed_everything(42)","9b60eef5":"transforms_valid = albumentations.Compose([\n    albumentations.Resize(image_size, image_size),\n    albumentations.Normalize()\n])\n\ntransforms_valid_768 = albumentations.Compose([\n    albumentations.Resize(768, 768),\n    albumentations.Normalize()\n])","47b23d8c":"from transformers import AutoTokenizer\n\nclass SHOPEEDataset(Dataset):\n    def __init__(self, df, mode, tokenizer_path=\"..\/input\/distilbert-base-indonesian\", transform=None, use_image=True, use_text=True):\n        \n        self.df = df.reset_index(drop=True)\n        self.mode = mode\n        self.transform = transform\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n        self.use_image = use_image\n        self.use_text = use_text\n#         self.tokenizer = AutoTokenizer.from_pretrained(\"..\/input\/bert-base-uncased\")\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        row = self.df.loc[index]\n        if self.use_image:\n            img = cv2.imread(row.file_path)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img_768 = img.copy()\n        \n            if self.transform is not None:\n                res = self.transform(image=img)\n                img_512 = res['image']\n                \n            img_512 = img_512.astype(np.float32)\n            img_512 = img_512.transpose(2,0,1)\n            \n            res = transforms_valid_768(image=img_768)\n            img_768 = res['image']\n            img_768 = img_768.astype(np.float32)\n            img_768 = img_768.transpose(2,0,1)\n#             img_768 = [0, 1, 2]\n        else:\n            img_512 = [0, 1, 2] # it's dummy\n            img_768 = [0, 1, 2]\n         \n        text = row.title\n        if self.use_text:\n            text = self.tokenizer(text, padding='max_length', truncation=True, max_length=35, return_tensors=\"pt\")\n            input_ids = text['input_ids'][0]\n            attention_mask = text['attention_mask'][0]\n        else:\n            attention_mask = torch.tensor([0])\n            input_ids = torch.tensor([0])\n#         token_type_ids = text[\"token_type_ids\"][0]\n        \n        if \"token_type_ids\" in text:\n            token_type_ids = text[\"token_type_ids\"][0]\n#             return torch.tensor(img).float(),text['input_ids'][0], text['attention_mask'][0], text[\"token_type_ids\"][0]\n            return torch.tensor(img_512).float(),torch.tensor(img_768).float() ,input_ids, attention_mask,token_type_ids\n        else:\n            return torch.tensor(img_512).float(),torch.tensor(img_768).float(),input_ids, attention_mask","6d83e635":"class SHOPEETextDataset(Dataset):\n    def __init__(self, df, data_dir, vectorizer=None):\n        le = LabelEncoder()\n        self.df = df\n        self.df['file_path'] = self.df.image.apply(lambda x: os.path.join(data_dir, x))\n        self.data_dir = data_dir\n\n        titles = self.df[\"title\"].tolist()\n        if vectorizer is None:\n            self.vectorizer = CountVectorizer(analyzer=\"char\", ngram_range=(1, 2))\n            self.array = self.vectorizer.fit_transform(titles).toarray()\n        else:\n            self.vectorizer = vectorizer\n#             self.titles = titles\n            self.array = self.vectorizer.transform(titles)\n        print(self.array.shape)\n\n    def get_vectorizer(self):\n        return self.vectorizer, len(self.array[0])\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n#         print (self.array[index])\n        arr = self.array[index].toarray()[0]\n#         arr = self.array[index]\n        return torch.tensor(arr).float()\n","041c3241":"from torch.nn.parameter import Parameter\n\nclass ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, s=30, m=0.5):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_normal_(self.weight)\n\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = torch.tensor(math.cos(math.pi - m))\n        self.mm = torch.tensor(math.sin(math.pi - m) * m)\n\n    def forward(self, inputs, labels):\n        cos_th = F.linear(F.normalize(inputs.float()), F.normalize(self.weight.float()))\n        cos_th = cos_th.clamp(-1, 1).float()\n        sin_th = torch.sqrt(1.0 - torch.pow(cos_th, 2)).float()\n        cos_th_m = cos_th * self.cos_m - sin_th * self.sin_m\n        # print(type(cos_th), type(self.th), type(cos_th_m), type(self.mm))\n        cos_th_m = torch.where(cos_th > self.th, cos_th_m, cos_th - self.mm)\n\n        cond_v = cos_th - self.th\n        cond = cond_v <= 0\n        cos_th_m[cond] = (cos_th - self.mm)[cond]\n\n        if labels.dim() == 1:\n            labels = labels.unsqueeze(-1)\n        onehot = torch.zeros(cos_th.size()).cuda()\n        labels = labels.type(torch.LongTensor).cuda()\n        onehot.scatter_(1, labels, 1.0)\n        outputs = onehot * cos_th_m + (1.0 - onehot) * cos_th\n        outputs = outputs * self.s\n        return outputs\n\n\ndef gem(x, p=3, eps=1e-6):\n    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1. \/ p)\n\n\nclass GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6, p_trainable=True):\n        super(GeM, self).__init__()\n        if p_trainable:\n            self.p = Parameter(torch.ones(1) * p)\n        else:\n            self.p = p\n        self.eps = eps\n\n    def forward(self, x):\n        return gem(x, p=self.p, eps=self.eps)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(\n            self.eps) + ')'\n\n\nclass Backbone(nn.Module):\n\n    def __init__(self, name='resnet18', pretrained=True):\n        super(Backbone, self).__init__()\n        self.net = timm.create_model(name, pretrained=pretrained)\n\n        if 'regnet' in name:\n            self.out_features = self.net.head.fc.in_features\n        elif 'csp' in name:\n            self.out_features = self.net.head.fc.in_features\n        elif 'res' in name:  # works also for resnest\n            self.out_features = self.net.fc.in_features\n        elif 'efficientnet' in name:\n            self.out_features = self.net.classifier.in_features\n        elif 'densenet' in name:\n            self.out_features = self.net.classifier.in_features\n        elif 'senet' in name:\n            self.out_features = self.net.fc.in_features\n        elif 'inception' in name:\n            self.out_features = self.net.last_linear.in_features\n\n        else:\n            self.out_features = self.net.classifier.in_features\n\n    def forward(self, x):\n        x = self.net.forward_features(x)\n\n        return x\n    \n    \n\nsigmoid = torch.nn.Sigmoid()\nclass Swish(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i * sigmoid(i)\n        ctx.save_for_backward(i)\n        return result\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n        sigmoid_i = sigmoid(i)\n        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n\nclass Swish_module(nn.Module):\n    def forward(self, x):\n        return Swish.apply(x)","d5870764":"from transformers import AutoModel\nfrom torch.nn.parameter import Parameter\n\nclass TransformerModel(nn.Module):\n    def __init__(self, transformer_type, pooling=\"avg\", p=3., p_trainable=True):\n        super(TransformerModel, self).__init__()\n        self.bert = AutoModel.from_pretrained(transformer_type)\n        self.pooling = pooling\n        self.eps = 1e-6\n        if p_trainable:\n            self.p = Parameter(torch.ones(1) * p)\n        else:\n            self.p = p\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        if token_type_ids is None:\n            out = self.bert(\n                input_ids,\n                attention_mask=attention_mask,\n                # token_type_ids = token_type_ids\n            )[0]\n        else:\n            out = self.bert(\n                input_ids,\n                attention_mask=attention_mask,\n                token_type_ids = token_type_ids\n            )[0]\n        att_mask = input_ids > 0\n        # x_bert = super().forward(ids, att_mask, token_type_ids=seg_ids)[0]\n        att_mask = att_mask.unsqueeze(-1)\n        if self.pooling == \"avg\":\n            return (out * att_mask).sum(dim=1) \/ att_mask.sum(dim=1)\n        else:\n            return((out * att_mask).sum(dim=1).clamp(self.eps).pow(self.p) \/ att_mask.sum(dim=1)).pow(1. \/ self.p)\n        # return out\n\nclass Net(nn.Module):\n    def __init__(self, args, pretrained=True):\n        super(Net, self).__init__()\n\n        self.args = args\n        self.image_features(args, pretrained=pretrained)\n        self.embedding_size = args[\"embedding_size\"]\n\n        # https:\/\/www.groundai.com\/project\/arcface-additive-angular-margin-loss-for-deep-face-recognition\n        if args[\"neck\"] == \"option-D\":\n            self.neck = nn.Sequential(\n                nn.Linear(self.backbone.out_features, self.embedding_size, bias=True),\n                nn.BatchNorm1d(self.embedding_size),\n                torch.nn.PReLU()\n            )\n        elif args[\"neck\"] == \"option-F\":\n            self.neck = nn.Sequential(\n                nn.Dropout(0.3),\n                nn.Linear(self.backbone.out_features, self.embedding_size, bias=True),\n                nn.BatchNorm1d(self.embedding_size),\n                torch.nn.PReLU()\n            )\n        else:\n            self.neck = nn.Sequential(\n                nn.Linear(self.backbone.out_features, self.embedding_size, bias=False),\n                nn.BatchNorm1d(self.embedding_size),\n            )\n\n        if args[\"neck\"] == \"option-D\":\n            self.neckv2 = nn.Sequential(\n                nn.Linear(self.backbone.out_features, self.embedding_size, bias=True),\n                nn.BatchNorm1d(self.embedding_size),\n                torch.nn.PReLU()\n            )\n        elif args[\"neck\"] == \"option-F\":\n            self.neckv2 = nn.Sequential(\n                nn.Dropout(0.3),\n                nn.Linear(self.backbone.out_features, self.embedding_size, bias=True),\n                nn.BatchNorm1d(self.embedding_size),\n                torch.nn.PReLU()\n            )\n        else:\n            self.neckv2 = nn.Sequential(\n                nn.Linear(self.backbone.out_features, self.embedding_size, bias=False),\n                nn.BatchNorm1d(self.embedding_size),\n            )\n        # self.neckv2 = nn.Linear(768 + self.backbone.out_features, self.embedding_size)\n        self.swish = Swish_module()\n        self.dropout = nn.Dropout(0.5)\n\n        self.head = ArcMarginProduct(self.embedding_size, args[\"n_classes\"], s=args[\"s\"], m=args[\"m\"])\n        # self.head = ArcMarginProduct_subcenter(self.embedding_size, args[\"n_classes\"])\n\n        if args[\"pretrained_weights\"] is not None:\n            self.load_state_dict(torch.load(args.pretrained_weights, map_location='cpu'), strict=False)\n            print('weights loaded from', args.pretrained_weights)\n\n    def image_features(self, args, pretrained):\n        self.args = args\n        self.backbone = Backbone(args[\"backbone\"], pretrained=pretrained)\n\n        if args[\"pool\"] == \"gem\":\n            self.global_pool = GeM(p_trainable=3)\n        elif args[\"pool\"] == \"identity\":\n            self.global_pool = torch.nn.Identity()\n        else:\n            self.global_pool = nn.AdaptiveAvgPool2d(1)\n            \n    def change_p(self):\n        self.global_pool.p = Parameter(self.global_pool.p.data + torch.tensor(1.0))\n\n    def forward(self, images, labels,input_ids, attention_mask, token_type_ids, get_embeddings=False, get_attentions=False):\n        x = self.backbone(images)\n        x = self.global_pool(x)\n        x = x[:, :, 0, 0]\n        x = self.neckv2(x)\n        return F.normalize(x)\n        \nclass NetNLP(nn.Module):\n    def __init__(self, args, pretrained=True):\n        super(NetNLP, self).__init__()\n\n        self.args = args\n        self.text_features(args, pretrained)\n        self.embedding_size = args[\"embedding_size\"]\n\n        self.neckv2 = nn.Sequential(\n                nn.Dropout(0.1),\n                nn.Linear(768, self.embedding_size, bias=False),\n                nn.BatchNorm1d(self.embedding_size),\n            )\n\n#         self.head = ArcMarginProduct(self.embedding_size, args[\"n_classes\"], s=args[\"s\"], m=args[\"m\"])\n        # self.head = ArcMarginProduct_subcenter(self.embedding_size, args[\"n_classes\"])\n\n        if args[\"pretrained_weights\"] is not None:\n            self.load_state_dict(torch.load(args.pretrained_weights, map_location='cpu'), strict=False)\n            print('weights loaded from', args.pretrained_weights)\n\n        # for param in self.bert.parameters():\n        #     param.requires_grad = False\n\n    def image_features(self, args, pretrained):\n        self.args = args\n        self.backbone = Backbone(args[\"backbone\"], pretrained=pretrained)\n\n        if args[\"pool\"] == \"gem\":\n            self.global_pool = GeM(p_trainable=True)\n        elif args[\"pool\"] == \"identity\":\n            self.global_pool = torch.nn.Identity()\n        else:\n            self.global_pool = nn.AdaptiveAvgPool2d(1)\n\n\n    def text_features(self, args, pretrained):\n        self.bert = TransformerModel(args[\"transformer_type\"], pooling=\"avg\")\n        \n\n    def forward(self, images, labels,input_ids, attention_mask, token_type_ids, get_embeddings=False, get_attentions=False):\n        out = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        # out = self.neckv2(out)\n        # x = torch.cat([x, out], 1)\n        # print (x.size())\n        x = self.neckv2(out)\n        # logits = self.head(x)\n#         logits = self.head(x, labels)\n\n        # print (logits)\n\n        return F.normalize(x)\n        \n        \nclass NetV2(nn.Module):\n    def __init__(self, args, pretrained=True):\n        super(NetV2, self).__init__()\n\n        self.args = args\n        self.image_features(args, pretrained=pretrained)\n        self.text_features(args, pretrained)\n\n        self.embedding_size = args[\"embedding_size\"]\n\n        if args[\"neck\"] == \"option-D\":\n            self.neckv2 = nn.Sequential(\n                nn.Linear(768 + self.backbone.out_features, self.embedding_size, bias=True),\n                nn.BatchNorm1d(self.embedding_size),\n                torch.nn.PReLU()\n            )\n        elif args[\"neck\"] == \"option-F\":\n            self.neckv2 = nn.Sequential(\n                nn.Dropout(0.3),\n                nn.Linear(768 + self.backbone.out_features, self.embedding_size, bias=True),\n                nn.BatchNorm1d(self.embedding_size),\n                torch.nn.PReLU()\n            )\n        else:\n            self.neckv2 = nn.Sequential(\n                nn.Linear(768 + self.backbone.out_features, self.embedding_size, bias=False),\n                nn.BatchNorm1d(self.embedding_size),\n                Swish_module()\n            )\n        # self.neckv2 = nn.Linear(768 + self.backbone.out_features, self.embedding_size)\n        #\n        # self.dropout = nn.Dropout(0.5)\n#         if args[\"metric_type\"] == \"arc_margin\":\n#             self.head = ArcMarginProduct(self.embedding_size, args[\"n_classes\"], s=args[\"s\"], m=args[\"m\"])\n#         else:\n#             self.head = ArcMarginProduct_subcenter(self.embedding_size, args[\"n_classes\"])\n\n\n        if args[\"pretrained_weights\"] is not None:\n            self.load_state_dict(torch.load(args.pretrained_weights, map_location='cpu'), strict=False)\n            print('weights loaded from', args.pretrained_weights)\n\n    def image_features(self, args, pretrained):\n        self.args = args\n        self.backbone = Backbone(args[\"backbone\"], pretrained=pretrained)\n\n        if args[\"pool\"] == \"gem\":\n            self.global_pool = GeM(p_trainable=False)\n        elif args[\"pool\"] == \"identity\":\n            self.global_pool = torch.nn.Identity()\n        else:\n            self.global_pool = nn.AdaptiveAvgPool2d(1)\n\n\n    def text_features(self, args, pretrained):\n        self.bert = TransformerModel(args[\"transformer_type\"], pooling=\"avg\")\n\n    def forward(self, images, labels,input_ids, attention_mask, token_type_ids, get_embeddings=False, get_attentions=False):\n        x = self.backbone(images)\n        x = self.global_pool(x)\n        x = x[:, :, 0, 0]\n        out = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        # out = self.neckv2(out)\n        x = torch.cat([x, out], 1)\n        # print (x.size())\n        x = self.neckv2(x)\n        # print (logits)\n\n        return F.normalize(x)","d23efd38":"def fix_encoding(x):\n    return normalize(\"NFD\", codecs.escape_decode(x, 'hex')[0].decode(\"utf-8\"))","83faf447":"data_dir = '..\/input\/shopee-product-matching\/train_images'\ndf_train = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\ndf_train['file_path'] = df_train.image.apply(lambda x: os.path.join(data_dir, x))","7341e426":"# torch.load('..\/input\/shopeae-models\/efficientnetb3_baseline.pth', map_location='cuda:0')","54842bfd":"model2 = Net(args={\n            \"backbone\": \"resnet200d\",\n            \"pool\": \"gem\",\n            \"s\": 30,\n            \"m\": 0.5,\n            \"neck\": \"\",\n            \"embedding_size\": 512,\n            \"n_classes\": 11014,\n            \"pretrained_weights\": None,\n            \"transformer_type\": \"bert-base-uncased\"\n        }, pretrained=False)\nmodel2.load_state_dict(torch.load('..\/input\/shopee-models\/resnet200d_images_baseline_optionH_cutout_bh_gnoise_all_img640.pth', map_location='cuda:0')[\"state_dict\"], strict=True)\nmodel2.to(device);\nif CHANGE_P:\n    print (model2.global_pool.p.data)\n    model2.change_p()\n    print (model2.global_pool.p.data)","19755cd2":"model = Net(args={\n            \"backbone\": \"tf_efficientnet_b3_ns\",\n            \"pool\": \"gem\",\n            \"s\": 30,\n            \"m\": 0.5,\n            \"neck\": \"\",\n            \"embedding_size\": 512,\n            \"n_classes\": 11014,\n            \"pretrained_weights\": None,\n            \"transformer_type\": \"bert-base-uncased\"\n        }, pretrained=False)\nmodel.load_state_dict(torch.load('..\/input\/shopee-models\/efficientnetb3_ns_images_baseline_optionH_cutout_bh_v2_768_all.pth', map_location='cuda:0')[\"state_dict\"], strict=False)\nmodel.to(device);\nif CHANGE_P:\n    print (model.global_pool.p.data)\n    model.change_p()\n    print (model.global_pool.p.data)","4587ba3e":"model3 = Net(args={\n            \"backbone\": \"tf_efficientnet_b5_ns\",\n            \"pool\": \"gem\",\n            \"s\": 30,\n            \"m\": 0.5,\n            \"neck\": \"\",\n            \"embedding_size\": 512,\n            \"n_classes\": 11014,\n            \"pretrained_weights\": None,\n            \"transformer_type\": \"bert-base-uncased\"\n        }, pretrained=False)\nmodel3.load_state_dict(torch.load('..\/input\/shopee-models\/efficientnetb5_ns_images_baseline_optionH_cutout_bh_gnoise_all_size640.pth', map_location='cuda:0')[\"state_dict\"], strict=True)\nmodel3.to(device);\nif CHANGE_P:\n    print (model3.global_pool.p.data)\n    model3.change_p()\n    print (model3.global_pool.p.data)","46c5cd20":"nlp_model = NetNLP(args={\n            \"backbone\": \"tf_efficientnet_b3\",\n            \"pool\": \"gem\",\n            \"s\": 30,\n            \"m\": 0.5,\n            \"neck\": \"option-D\",\n            \"embedding_size\": 512,\n            \"n_classes\": 11014,\n            \"pretrained_weights\": None,\n            \"transformer_type\": \"..\/input\/distilbert-base-indonesian\/\",\n#     \"pretrained_weights\": \n})\nnlp_model.load_state_dict(torch.load('..\/input\/shopee-bert\/distilbert_baseline_optionH_count_bs255_all.pth', map_location='cuda:0')[\"state_dict\"], strict=False)\nnlp_model.to(device);","2f9b409e":"# nlp_bert_model = NetNLP(args={\n#             \"backbone\": \"tf_efficientnet_b3\",\n#             \"pool\": \"gem\",\n#             \"s\": 30,\n#             \"m\": 0.5,\n#             \"neck\": \"option-D\",\n#             \"embedding_size\": 512,\n#             \"n_classes\": 11014,\n#             \"pretrained_weights\": None,\n#             \"transformer_type\": \"..\/input\/bertindo15g\/\",\n# #     \"pretrained_weights\": \n# })\n# nlp_bert_model.load_state_dict(torch.load('..\/input\/shopee-bert\/bert_indonesian15G_baseline_optionH_count_bs255_all.pth', map_location='cuda:0')[\"state_dict\"], strict=False)\n# nlp_bert_model.to(device);","e588f7ea":"class NLPMLP(nn.Module):\n    def __init__(self, n_feat, n_class):\n        super(NLPMLP, self).__init__()\n        self.feature_extract = nn.Sequential(\n            nn.Linear(n_feat, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n        )\n\n#         self.neckv2 = ArcMarginProduct(512, n_class, s=32, m=0.5)\n\n    def forward(self, feat1):\n        x = self.feature_extract(feat1)\n#         logits = self.neckv2(x, label)\n\n        return F.normalize(x)\n\nclass NLPMLPV3(nn.Module):\n    def __init__(self, n_feat, n_class):\n        super(NLPMLPV3, self).__init__()\n        self.feature_extract = nn.Sequential(\n            nn.Linear(n_feat, 1024),\n            nn.BatchNorm1d(1024),\n            nn.PReLU(),\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n        )\n\n#         self.neckv2 = ArcMarginProduct(512, n_class, s=32, m=0.5)\n\n    def forward(self, feat1):\n        x = self.feature_extract(feat1)\n#         logits = self.neckv2(x, label)\n\n        return F.normalize(x)","0436a420":"# nlp_model2 = NLPMLP(2716, 11014)\n# vectorizer = pickle.load(open(\"..\/input\/shopee-mlp\/count_vector.pkl\", \"rb\"))\n# nlp_model2.load_state_dict(torch.load('..\/input\/shopee-mlp\/epoch_0099.pth', map_location='cuda:0')[\"state_dict\"], strict=False)\n# nlp_model2.to(device);","bb5151ce":"nlp_model3 = NLPMLPV3(30210, 11014)\nvectorizer2 = pickle.load(open(\"..\/input\/shopee-mlp\/count_vector_1_3_v2.pkl\", \"rb\"))\nnlp_model3.load_state_dict(torch.load('..\/input\/shopee-mlp\/mlpv3_images_cutout_bh_ngram13_v2.pth', map_location='cuda:0')[\"state_dict\"], strict=False)\nnlp_model3.to(device);","5608aca1":"import codecs\nfrom unicodedata import normalize\n\ndef fix_encoding(x):\n    return normalize(\"NFD\", codecs.escape_decode(x, 'hex')[0].decode(\"utf-8\"))","4a8858be":"test = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\n# test = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\n# test = pd.concat([test]*2)\ntest['file_path'] = test.image.apply(lambda x: os.path.join('..\/input\/shopee-product-matching\/test_images',x))\ntest[\"title\"] = test[\"title\"].apply(fix_encoding)","34ca6a1d":"dataset_test = SHOPEEDataset(test, 'test', transform=transforms_valid, use_text=False)\ntest_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)","d264ddba":"distilbert_dataset_test = SHOPEEDataset(test, 'test', transform=transforms_valid,tokenizer_path=\"..\/input\/distilbert-base-indonesian\",use_image=False)\ndistilbert_test_loader = torch.utils.data.DataLoader(distilbert_dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)","459b5022":"# bert_dataset_test = SHOPEEDataset(test, 'test', transform=transforms_valid,tokenizer_path=\"..\/input\/bertindo15g\",use_image=False)\n# bert_test_loader = torch.utils.data.DataLoader(bert_dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)","fa61d009":"# test['file_path'] = test.image.apply(lambda x: os.path.join('..\/input\/shopee-product-matching\/test_images',x))\n# dataset_text_test = SHOPEETextDataset(test, 'test', vectorizer)\n# test_text_loader = torch.utils.data.DataLoader(dataset_text_test, batch_size=128, shuffle=False, num_workers=num_workers, pin_memory=True)","a5ec8cda":"# test['file_path'] = test.image.apply(lambda x: os.path.join('..\/input\/shopee-product-matching\/test_images',x))\ndataset_text_test2 = SHOPEETextDataset(test, 'test', vectorizer2)\ntest_text_loader2 = torch.utils.data.DataLoader(dataset_text_test2, batch_size=32, shuffle=False, num_workers=num_workers, pin_memory=True)","b8d8949d":"train = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\ntrain['file_path'] = train.image.apply(lambda x: os.path.join('..\/input\/shopee-product-matching\/train_images',x))\ntrain[\"title\"] = train[\"title\"].apply(fix_encoding)","fec0818c":"# dataset_text_fp_test2 = SHOPEETextDataset(train, 'test', vectorizer2)\n# test_text_fp_loader2 = torch.utils.data.DataLoader(dataset_text_fp_test2, batch_size=32, shuffle=False, num_workers=num_workers, pin_memory=True)","fea81de5":"distilbert_dataset_fp_test = SHOPEEDataset(train, 'test', transform=transforms_valid,tokenizer_path=\"..\/input\/distilbert-base-indonesian\",use_image=False)\ndistilbert_test_fp_loader = torch.utils.data.DataLoader(distilbert_dataset_fp_test, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)","4b86fe73":"# dataset_text_fp_test = SHOPEETextDataset(test, 'test', vectorizer)\n# test_text_fp_loader = torch.utils.data.DataLoader(dataset_text_fp_test, batch_size=128, shuffle=False, num_workers=num_workers, pin_memory=True)","78b94d5f":"def generate_test_features(test_loader):\n    model.eval()\n    model2.eval()\n    model3.eval()\n    bar = tqdm(test_loader)\n    \n    FEAS = []\n    TARGETS = []\n\n    with torch.no_grad():\n        for batch_idx, (images512, images768,input_ids, attention_mask) in enumerate(bar):\n\n            images768 = images768.to(device)\n            images512 = images512.to(device)\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n#             token_type_ids = token_type_ids.to(device)\n\n            features = model(images768, None, input_ids, attention_mask, None, get_embeddings=True)\n            features2 = model2(images512, None, input_ids, attention_mask, None, get_embeddings=True)\n            features3 = model3(images512, None, input_ids, attention_mask, None, get_embeddings=True)\n            \n            concat_feat = torch.cat([features, features2, features3], axis=1)\n            \n            FEAS += [concat_feat.detach().cpu()]\n\n    FEAS = torch.cat(FEAS).cpu().numpy()\n    \n    return FEAS","49ad2fc2":"%%time\nFEAS = generate_test_features(test_loader)\nFEAS.shape","4ff74913":"n, _ = FEAS.shape\nbs = n \/\/ 10 ","1ccfe43e":"del model\ngc.collect()\ntorch.cuda.empty_cache()","1bd306f9":"def generate_test_features2(test_loader):\n    model2.eval()\n    bar = tqdm(test_loader)\n    \n    FEAS_MODEL2 = []\n    TARGETS = []\n\n    with torch.no_grad():\n        for batch_idx, (images, input_ids, attention_mask) in enumerate(bar):\n            images = images.to(device)\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n#             token_type_ids = token_type_ids.to(device)\n\n            features = model2(images, None, input_ids, attention_mask, None, get_embeddings=True)\n\n            FEAS_MODEL2 += [features.detach().cpu()]\n\n    FEAS_MODEL2 = torch.cat(FEAS_MODEL2).cpu().numpy()\n    \n    return FEAS_MODEL2","97c5109f":"# FEAS_MODEL2 = generate_test_features2(test_loader)","6b085d2a":"del model2\ngc.collect()\ntorch.cuda.empty_cache()","8d40754c":"def generate_test_features3(test_loader):\n    model3.eval()\n    bar = tqdm(test_loader)\n    \n    FEAS_MODEL3 = []\n    TARGETS = []\n\n    with torch.no_grad():\n        for batch_idx, (images, input_ids, attention_mask, _) in enumerate(bar):\n\n            images = images.to(device)\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n#             token_type_ids = token_type_ids.to(device)\n\n            features = model3(images, None, input_ids, attention_mask, None, get_embeddings=True)\n\n            FEAS_MODEL3 += [features.detach().cpu()]\n\n    FEAS_MODEL3 = torch.cat(FEAS_MODEL3).cpu().numpy()\n    \n    return FEAS_MODEL3","698e08e7":"# FEAS_MODEL3  = generate_test_features3(test_loader)","8a926bbe":"del model3\ngc.collect()\ntorch.cuda.empty_cache()","31fb79a4":"def generate_nlp_test_features(test_loader):\n    nlp_model.eval()\n    bar = tqdm(test_loader)\n    \n    TEXT_FEAS_2 = []\n    TARGETS = []\n\n    with torch.no_grad():\n        for batch_idx, (images, images2,input_ids, attention_mask) in enumerate(bar):\n\n            images = images.to(device)\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n#             token_type_ids = token_type_ids.to(device)\n\n            features = nlp_model(images, None, input_ids, attention_mask, None, get_embeddings=True)\n\n            TEXT_FEAS_2 += [features.detach().cpu()]\n\n    TEXT_FEAS_2 = torch.cat(TEXT_FEAS_2).cpu().numpy()\n    \n    return TEXT_FEAS_2","39de9beb":"%%time\nTEXT_FEAS_2 = generate_nlp_test_features(distilbert_test_loader)\nTEXT_FEAS_2.shape\n\nTEXT_FEAS_FP_2 = generate_nlp_test_features(distilbert_test_fp_loader)\nTEXT_FEAS_FP_2.shape","a47d8451":"del nlp_model\ngc.collect()\ntorch.cuda.empty_cache()","dc065118":"# def generate_nlp_test_features(test_loader):\n#     nlp_model2.eval()\n#     bar = tqdm(test_loader)\n    \n#     FEAS = []\n#     TARGETS = []\n\n#     with torch.no_grad():\n#         for batch_idx, text_feat in enumerate(bar):\n\n#             text_feat = text_feat.to(device)\n# #             input_ids = input_ids.to(device)\n# #             attention_mask = attention_mask.to(device)\n# #             token_type_ids = token_type_ids.to(device)\n\n#             features = nlp_model2(text_feat)\n\n#             FEAS += [features.detach().cpu()]\n\n#     FEAS = torch.cat(FEAS).cpu().numpy()\n    \n#     return FEAS","5579c756":"# TEXT_FEAS = generate_nlp_test_features(test_text_loader)\n# TEXT_FEAS_FP = generate_nlp_test_features(test_text_fp_loader)","e74396f2":"# del nlp_model2\n# gc.collect()\n# torch.cuda.empty_cache()","767f829e":"def generate_nlp_test_features3(test_loader):\n    nlp_model3.eval()\n    bar = tqdm(test_loader)\n    \n    FEAS = []\n    TARGETS = []\n\n    with torch.no_grad():\n        for batch_idx, text_feat in enumerate(bar):\n\n            text_feat = text_feat.to(device)\n#             input_ids = input_ids.to(device)\n#             attention_mask = attention_mask.to(device)\n#             token_type_ids = token_type_ids.to(device)\n\n            features = nlp_model3(text_feat)\n\n            FEAS += [features.detach().cpu()]\n\n    FEAS = torch.cat(FEAS).cpu().numpy()\n    \n    return FEAS","2fa1a1f0":"TEXT_FEAS4 = generate_nlp_test_features3(test_text_loader2)\n# TEXT_FEAS4_FP = generate_nlp_test_features3(test_text_fp_loader2)","d454837c":"del nlp_model3\ngc.collect()\ntorch.cuda.empty_cache()","67cb27fb":"# def generate_nlp_test_features(test_loader):\n#     nlp_model2.eval()\n#     bar = tqdm(test_loader)\n    \n#     FEAS = []\n#     TARGETS = []\n\n#     with torch.no_grad():\n#         for batch_idx, text_feat in enumerate(bar):\n\n#             text_feat = text_feat.to(device)\n# #             input_ids = input_ids.to(device)\n# #             attention_mask = attention_mask.to(device)\n# #             token_type_ids = token_type_ids.to(device)\n\n#             features = nlp_model2(text_feat)\n\n#             FEAS += [features.detach().cpu()]\n\n#     FEAS = torch.cat(FEAS).cpu().numpy()\n    \n#     return FEAS","cfa1fe5d":"# TEXT_FEAS = generate_nlp_test_features(test_text_loader)\n# TEXT_FEAS_FP = generate_nlp_test_features(test_text_fp_loader)\n\n# del nlp_model2\n# gc.collect()\n# torch.cuda.empty_cache()","b1db3922":"# def generate_nlp_test_features(test_loader):\n#     nlp_model.eval()\n#     bar = tqdm(test_loader)\n    \n#     TEXT_FEAS_2 = []\n#     TARGETS = []\n\n#     with torch.no_grad():\n#         for batch_idx, (images, images2,input_ids, attention_mask) in enumerate(bar):\n\n#             images = images.to(device)\n#             input_ids = input_ids.to(device)\n#             attention_mask = attention_mask.to(device)\n# #             token_type_ids = token_type_ids.to(device)\n\n#             features = nlp_model(images, None, input_ids, attention_mask, None, get_embeddings=True)\n\n#             TEXT_FEAS_2 += [features.detach().cpu()]\n\n#     TEXT_FEAS_2 = torch.cat(TEXT_FEAS_2).cpu().numpy()\n    \n#     return TEXT_FEAS_2","415a509c":"# TEXT_FEAS_2 = generate_nlp_test_features(distilbert_test_loader)\n# TEXT_FEAS_2.shape","8b760073":"# def generate_nlp_test_features_v2(test_loader):\n#     nlp_bert_model.eval()\n#     bar = tqdm(test_loader)\n    \n#     TEXT_FEAS_3 = []\n#     TARGETS = []\n\n#     with torch.no_grad():\n#         for batch_idx, (images, images2, input_ids, attention_mask, token_type_ids) in enumerate(bar):\n\n#             images = images.to(device)\n#             input_ids = input_ids.to(device)\n#             attention_mask = attention_mask.to(device)\n#             token_type_ids = token_type_ids.to(device)\n\n#             features = nlp_bert_model(images, None, input_ids, attention_mask, token_type_ids, get_embeddings=True)\n\n#             TEXT_FEAS_3 += [features.detach().cpu()]\n\n#     TEXT_FEAS_3 = torch.cat(TEXT_FEAS_3).cpu().numpy()\n    \n#     return TEXT_FEAS_3","fdfecf22":"# TEXT_FEAS_3 = generate_nlp_test_features_v2(bert_test_loader)\n# TEXT_FEAS_3.shape","db5b10e1":"#https:\/\/www.kaggle.com\/cdeotte\/part-2-rapids-tfidfvectorizer-cv-0-700#Use-Text-Embeddings\n\ndef get_text_predictions(df, max_features = 25_000):\n    \n    model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = max_features)\n    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n    preds = []\n    idx_list = []\n    CHUNK = 1024*4\n\n    print('Finding similar titles...')\n    CTS = len(df)\/\/CHUNK\n    if len(df)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(df))\n        print('chunk',a,'to',b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>0.7)[0]\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            idx_list.append(IDX)\n            preds.append(o)\n    \n    del model,text_embeddings\n    gc.collect()\n    return preds, idx_list","d63fe1e4":"# df_cu = cudf.DataFrame(test)\n# text_predictions,text_idx_list = get_text_predictions(test, max_features = 25_000)","bd579710":"def return_dba_feat(feat, thresh, k):\n    feat = torch.tensor(feat).cuda()\n    batches = []\n    for i in range(n_batch):\n        left = bs * i\n        right = bs * (i+1)\n        if i == n_batch - 1:\n            right = n\n        batches.append(feat[left:right,:])\n            \n    matches = []\n    dba_feat = torch.zeros_like(feat)\n    cnt = 0\n    for batch in tqdm(batches):\n        batch = batch.cuda()\n        similarity_matrix = batch@feat.T\n#         print (similarity_matrix.topk(len(test)))\n        selection = ((similarity_matrix > sim_thresh)).cpu().numpy()\n#         selection_indexes = similarity_matrix.topk(len(test))[1].cpu().numpy()[:, k:]\n#         selection = ((batch@feat.T) > sim_thresh).cpu().numpy()\n        \n        for i, row in enumerate(selection):\n#             print (len(row), len(selection_index))\n#             print (feat[row].size())\n            if len(feat[row]) == 1:\n                dba_feat[cnt] = batch[i]\n            else:\n#                 print (feat[row].mean(axis=0))\n                dba_feat[cnt] = feat[row].mean(axis=0)\n            cnt += 1\n#             print (np.mean(feat[row, :], axis=0))\n#             matches.append(' '.join(test.iloc[row].posting_id.tolist()))\n\n    return dba_feat","145f3f70":"def db_aug(V):\n    model = NearestNeighbors(n_neighbors=2, metric=\"cosine\")\n    model.fit(V)\n    distances, indices = model.kneighbors(V)\n    \n    w = np.power(np.clip(2.0 - distances, 0, 2.0), 0.5)\n    \n    V = (w[:, 0, None]*V[indices[:, 0]] + w[:, 1, None]*V[indices[:, 1]])\/w.sum(axis=1)[:, None]\n    \n    return V","482ff736":"def get_neighbors(embeddings, KNN = 2, image = True):\n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    new_embeddings = np.zeros_like(embeddings)\n    for i, idx in enumerate(indices):\n#         print (embeddings[idx, :].shape)\n        new_embeddings[i] = embeddings[idx, :].mean(axis=0)\n    return new_embeddings","402d1c8f":"import collections\ndef combine_for_sub(row):\n#     x = np.concatenate([row.image_matches.split(\" \"),row.nlp_matches.split(\" \"), row.image_matches2.split(\" \")])\n#     x = np.concatenate([row.image_matches.split(\" \"),row.nlp_matches.split(\" \"), row.text_predictions])\n    text_unmatches = row.text_unmatches.split(\" \")\n    nlp_matches = [elem for elem in row.nlp_matches.split(\" \") if elem not in text_unmatches]\n#     text_predictions = [elem for elem in row.text_predictions if elem not in text_unmatches]\n#     x = np.concatenate([row.image_matches.split(\" \"), nlp_matches, text_predictions])\n    x = np.concatenate([row.image_matches.split(\" \"), nlp_matches])\n    \n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    x = np.concatenate([row.image_matches,row.nlp_matches])\n    return np.unique(x)\n\ndef combine_graph(df):\n    posting_id_list = df[\"posting_id\"].tolist()\n    match_list = []\n    for posting_id in posting_id_list:\n        match = \" \".join(df[(df[\"matches\"].str.contains(posting_id)) & (df[\"posting_id\"] != posting_id)][\"matches\"].tolist())\n        orig_match = df[df[\"posting_id\"] != posting_id][\"matches\"].iloc[0]\n        \n        c = collections.Counter(match.split(\" \"))\n        expand_items = [key for key, value in c.items() if value > (len(df[df[\"posting_id\"] != posting_id]) \/ 2)]\n            \n        match_str = ' '.join( np.unique(orig_match.split(\" \") + expand_items))\n        match_list.append(match_str)\n    df[\"matches\"] = match_list\n    return df","cc934399":"%%time\nCONCAT_FEAS = F.normalize(torch.tensor(np.hstack([FEAS, TEXT_FEAS_2])).cuda())\nDBA_FEAS = db_aug(CONCAT_FEAS.cpu().numpy())\nDBA_FEAS = F.normalize(torch.tensor(DBA_FEAS).cuda())\n    \nCONCAT_TEXT_FEAS = torch.tensor(TEXT_FEAS4).cuda()\nCONCAT_TEXT_FEAS = db_aug(CONCAT_TEXT_FEAS.cpu().numpy())\nCONCAT_TEXT_FEAS = F.normalize(torch.tensor(CONCAT_TEXT_FEAS).cuda())","443f1c6d":"# CONCAT_TEXT_FEAS_FP = torch.tensor(np.hstack([TEXT_FEAS4_FP])).cuda()\n# CONCAT_TEXT_FEAS_FP = torch.tensor(TEXT_FEAS4_FP).cuda()\n# CONCAT_TEXT_FEAS_FP = db_aug(CONCAT_TEXT_FEAS_FP.cpu().numpy())\n# CONCAT_TEXT_FEAS_FP = F.normalize(torch.tensor(CONCAT_TEXT_FEAS_FP).cuda())","6eca3396":"# CONCAT_TEXT_FEAS.shape, CONCAT_TEXT_FEAS_FP.shape","a9efbb5c":"FEAS_FP = np.load(\"..\/input\/shopee-precomputed\/FP_FEAS.npy\")","d046d4b6":"def get_min_dist(V, V_fp):\n    d = []\n    bs = 256\n    for begin in tqdm(range(0, V.shape[0], bs)):\n        end = min(V.shape[0], begin + bs)\n        d.append((V[begin:end]@V_fp).max(axis=1)[0])\n    \n    return 1 - torch.cat(d, dim=0)\n\nD_img_fp_min = get_min_dist(torch.tensor(FEAS).cuda(), torch.tensor(FEAS_FP).T.cuda())\nD_img_fp_min = D_img_fp_min.cpu().numpy()\n# D_text_fp_min = get_min_dist(CONCAT_TEXT_FEAS, CONCAT_TEXT_FEAS_FP.T)\n# D_text_fp_min = get_min_dist(torch.tensor(TEXT_FEAS_2).cuda(), torch.tensor(TEXT_FEAS_FP_2.T).cuda())\n# D_text_fp_min = D_text_fp_min.cpu().numpy()","3f1a28d9":"del TEXT_FEAS_FP_2\ngc.collect()\ntorch.cuda.empty_cache()","ce3ab478":"image_batches = []\nfor i in range(n_batch):\n    left = bs * i\n    right = bs * (i+1)\n    if i == n_batch - 1:\n        right = n\n    image_batches.append(DBA_FEAS[left:right,:])\n\ntext_batches = []\nfor i in range(n_batch):\n    left = bs * i\n    right = bs * (i+1)\n    if i == n_batch - 1:\n        right = n\n    text_batches.append(CONCAT_TEXT_FEAS[left:right,:])","277ca320":"posting_ids = test[\"posting_id\"].tolist()\ntest_idx_list = np.arange(len(test))","2b499cb2":"%%time\nmatch_list = []\ncnt = 0\n\nfor image_batch, text_batch in zip(image_batches, text_batches):\n    batch = image_batch.cuda()\n    text_batch = text_batch.cuda()\n    \n    similarity_matrix = batch@DBA_FEAS.T\n    selection = (similarity_matrix > sim_thresh).cpu().numpy()\n    \n    text_similarity_matrix = text_batch@CONCAT_TEXT_FEAS.T\n    text_selection = (text_similarity_matrix > sim_thresh).cpu().numpy()\n    \n    for idx, (img_row, text_row) in enumerate(zip(selection, text_selection)):\n        row = img_row | text_row\n        match_ids = test.iloc[row].posting_id.tolist()\n        text_distances = text_similarity_matrix[idx, row]\n        image_distances = similarity_matrix[idx, row]\n        max_text_dist = text_distances.max().item()\n        max_image_dist = image_distances.max().item()\n        test_idx_tmp_list = test_idx_list[row]\n        posting_id = posting_ids[cnt]\n        \n        for match_id,text_distance, image_distance, test_idx in zip(match_ids, text_distances, image_distances, test_idx_tmp_list):\n            match_list.append({\n                \"posting_id\": posting_id, \n                \"matches\": match_id, \n                \"text_dist\": text_distance.item(), \n                \"image_dist\": image_distance.item(),\n                \"max_text_dist\": max_text_dist,\n                \"max_image_dist\": max_image_dist,\n                \"diff_max_text_dist\": max_text_dist - text_distance.item(),\n                \"diff_max_image_dist\": max_image_dist - image_distance.item(),\n#                 \"fp_text_dist\":  D_text_fp_min[cnt],\n#                 \"fp_text_dist_2\": D_text_fp_min[test_idx],\n                \"fp_img_dist\":  D_img_fp_min[cnt],\n                \"fp_img_dist_2\": D_img_fp_min[test_idx],\n            })\n        cnt += 1\n#         matches.append(' '.join(test.iloc[row].posting_id.tolist()))","d4439593":"res_df = pd.DataFrame(match_list)\nprint(res_df.shape)\nres_df.head()","e99f5104":"res_df[\"multiply_dist\"] = res_df[\"text_dist\"] * res_df[\"image_dist\"]\nres_df[\"total_dist\"] = res_df[\"text_dist\"] + res_df[\"image_dist\"]\nres_df[\"dist_rank\"] = res_df.groupby(\"posting_id\")[\"text_dist\"].rank()\nres_df[\"image_dist_rank\"] = res_df.groupby(\"posting_id\")[\"image_dist\"].rank()\n\n# res_df[\"diff_text_dist_fp\"] = res_df[\"text_dist\"] - res_df[\"fp_text_dist\"]","b65f1dcc":"preds_df = res_df[res_df[\"posting_id\"] < res_df[\"matches\"]]","c6596070":"preds_df = preds_df.merge(test[[\"posting_id\", \"title\"]], on=\"posting_id\", how=\"left\")\npreds_df = preds_df.merge(test[[\"posting_id\", \"title\"]].rename(columns={\"posting_id\": \"matches\"}), on=\"matches\", how=\"left\")","c53f712e":"tfidf = TfidfVectorizer(ngram_range=(1,1), binary=True)\ntfidf.fit(test[\"title\"])\ntfidf2 = TfidfVectorizer(analyzer=\"char\", ngram_range=(5, 5))\ntfidf2.fit(test[\"title\"])\n\npreds_df[\"cos_sim\"] = tfidf.transform(preds_df[\"title_x\"]).multiply(tfidf.transform(preds_df[\"title_y\"])).sum(axis=1)\npreds_df[\"cos_sim2\"] = tfidf2.transform(preds_df[\"title_x\"]).multiply(tfidf2.transform(preds_df[\"title_y\"])).sum(axis=1)","28e0db34":"import xgboost as xgb\n\nfeatures = [\n    \"image_dist\", \"text_dist\", \"dist_rank\", \"image_dist_rank\",\n    \"multiply_dist\", \"total_dist\", \"cos_sim\", \"cos_sim2\",\n    \"max_text_dist\", \"max_image_dist\", \n        \"diff_max_text_dist\", \"diff_max_image_dist\", \n    \"fp_img_dist\", \n#     \"fp_text_dist\", \n#     \"diff_text_dist_fp\",\n#     \"fp_img_dist_2\",\n#     \"fp_text_dist_2\"\n]\n\nxgb_model = xgb.XGBClassifier()\nxgb_model.load_model(\"..\/input\/shopee-2ndstage\/xgb_8135.json\")\n\npreds_df[\"pred\"] = xgb_model.predict_proba(preds_df[features])[:, 1]\npreds_df.head()","9c657808":"%%time\ndef agglomerative_clustering(preds_df, single_link_threshold=0.30, group_link_threshold=0.50, group_merge_threshold=0.60):\n\n    groups = dict()\n    group_members = dict()\n\n    gix = 0\n    for i, row in tqdm(preds_df.sort_values(\"pred\", ascending=False).iterrows(), total=preds_df.shape[0]):\n        if row.pred > single_link_threshold:\n            g1 = groups.get(row.posting_id)\n            g2 = groups.get(row.matches)\n\n            if g1 is None and g2 is None:\n                groups[row.posting_id] = gix\n                groups[row.matches] = gix\n                group_members[gix] = {row.posting_id, row.matches}\n                gix += 1\n            elif g1 is None:\n                if row.pred > group_link_threshold:\n                    groups[row.posting_id] = g2\n                    group_members[g2].add(row.posting_id)\n            elif g2 is None:\n                if row.pred > group_link_threshold:\n                    groups[row.matches] = g1\n                    group_members[g1].add(row.matches)\n            elif (g1 != g2) and (row.pred > group_merge_threshold):\n                groups[row.matches] = g1\n                group_members[g1].update(group_members[g2])\n\n                del group_members[g2]\n\n                for k, v in groups.items():\n                    if v == g2:\n                        groups[k] = g1\n\n\n    print(len(groups))\n\n    out_df = []\n\n    for k, v in groups.items():\n        for k2 in group_members[v]:\n            if k != k2:\n                out_df.append({\"posting_id\": k, \"matches\": k2})\n\n    return pd.DataFrame(out_df)\n\nout_df = agglomerative_clustering(preds_df, group_link_threshold=0.75, group_merge_threshold=0.80)\nout_df.shape\n\nsame_df =test[[\"posting_id\"]].copy()\nsame_df[\"matches\"] = same_df[\"posting_id\"].values\n\nout_df = out_df.append(same_df)\nout_df.shape","7d8e6865":"# THRESHOLD = 0.63\n\n# out_df = preds_df[preds_df[\"pred\"] > THRESHOLD][[\"posting_id\", \"matches\"]]\n# out_df = out_df.append(out_df.rename(columns={\"posting_id\": \"matches\", \"matches\": \"posting_id\"})).drop_duplicates()\n# out_df.shape","f246c024":"# same_df = test[[\"posting_id\"]].copy()\n# same_df[\"matches\"] = same_df[\"posting_id\"].values\n\n# out_df = out_df.append(same_df)\n# out_df.shape","0df1df50":"out_df = out_df.groupby(\"posting_id\")[\"matches\"].agg(list).reset_index()\nout_df[\"matches\"] = out_df[\"matches\"].apply(lambda x: \" \".join(x))\nout_df.head()","52c7413d":"out_df.to_csv('submission.csv', index=False, columns=['posting_id', 'matches'])","efb2eb4d":"# if True:\n# #     FEAS = return_dba_feat(FEAS, 0.95, 3)\n#     FEAS = F.normalize(torch.tensor(np.hstack([FEAS, TEXT_FEAS_2])).cuda())\n# #     FEAS = F.normalize(torch.tensor(np.hstack([FEAS, FEAS_MODEL2])).cuda())\n#     FEAS = db_aug(FEAS.cpu().numpy())\n#     FEAS = F.normalize(torch.tensor(FEAS).cuda())\n# #     FEAS = torch.tensor(FEAS).cuda()\n#     batches = []\n#     for i in range(n_batch):\n#         left = bs * i\n#         right = bs * (i+1)\n#         if i == n_batch - 1:\n#             right = n\n#         batches.append(FEAS[left:right,:])\n    \n#     matches = []\n#     un_matches = []\n#     cnt = 0\n#     for batch in tqdm(batches):\n#         batch = batch.cuda()\n#         similarity_matrix = batch@FEAS.T\n#         selection = (similarity_matrix > sim_thresh).cpu().numpy()\n        \n#         for row in selection:\n#             matches.append(' '.join(test.iloc[row].posting_id.tolist()))\n        \n#         for i in range(len(similarity_matrix)):\n#             un_matches_ids = []\n#             for text_idx in text_idx_list[cnt]:\n#                 if similarity_matrix[i, int(text_idx)] < text_filter_threshold:\n#                     un_matches_ids.append(text_idx)\n#             un_matches.append(' '.join(test.iloc[un_matches_ids].posting_id.tolist()))\n#             cnt += 1\n            \n# #     print (matches)\n\n#     submission = pd.read_csv('..\/input\/shopee-product-matching\/sample_submission.csv')\n#     submission['image_matches'] = matches\n#     submission['text_unmatches'] = un_matches\n            \n# #     FEAS_MODEL2 = torch.tensor(FEAS_MODEL2).cuda()\n# #     batches = []\n# #     for i in range(n_batch):\n# #         left = bs * i\n# #         right = bs * (i+1)\n# #         if i == n_batch - 1:\n# #             right = n\n# #         batches.append(FEAS_MODEL2[left:right,:])\n    \n# #     matches = []\n# #     for batch in tqdm(batches):\n# #         batch = batch.cuda()\n# #         selection = ((batch@FEAS_MODEL2.T) > sim_thresh).cpu().numpy()\n        \n# #         for row in selection:\n# #             matches.append(' '.join(test.iloc[row].posting_id.tolist()))\n    \n# #     submission = pd.read_csv('..\/input\/shopee-product-matching\/sample_submission.csv')\n# #     submission['image_matches2'] = matches\n\n#     TEXT_FEAS = torch.tensor(np.hstack([TEXT_FEAS, TEXT_FEAS4])).cuda()\n#     TEXT_FEAS = db_aug(TEXT_FEAS.cpu().numpy())\n#     TEXT_FEAS = F.normalize(torch.tensor(TEXT_FEAS).cuda())\n#     batches = []\n#     for i in range(n_batch):\n#         left = bs * i\n#         right = bs * (i+1)\n#         if i == n_batch - 1:\n#             right = n\n#         batches.append(TEXT_FEAS[left:right,:])\n    \n#     matches = []\n#     for batch in tqdm(batches):\n#         batch = batch.cuda()\n#         selection = ((batch@TEXT_FEAS.T) > text_sim_thresh).cpu().numpy()\n        \n#         for row in selection:\n#             matches.append(' '.join(test.iloc[row].posting_id.tolist()))\n\n#     submission['nlp_matches'] = matches\n# #     submission['matches'] = matches\n#     submission['text_predictions'] = text_predictions\n#     submission['matches'] = submission.apply(combine_for_sub, axis = 1)\n\n# #     submission = pd.read_csv('..\/input\/shopee-product-matching\/sample_submission.csv')\n# #     TEXT_FEAS = torch.tensor(np.hstack([TEXT_FEAS, FEAS])).cuda()\n# #     batches = []\n# #     for i in range(n_batch):\n# #         left = bs * i\n# #         right = bs * (i+1)\n# #         if i == n_batch - 1:\n# #             right = n\n# #         batches.append(TEXT_FEAS[left:right,:])\n    \n# #     matches = []\n# #     for batch in tqdm(batches):\n# #         batch = batch.cuda()\n# #         selection = ((batch@TEXT_FEAS.T) > text_sim_thresh).cpu().numpy()\n        \n# #         for row in selection:\n# #             matches.append(' '.join(test.iloc[row].posting_id.tolist()))\n# #     submission = pd.read_csv('..\/input\/shopee-product-matching\/sample_submission.csv')\n# #     submission['nlp_matches'] = matches\n# #     submission['matches'] = matches\n# #     submission['matches'] = submission.apply(combine_for_sub, axis = 1)\n# #     submission['matches'] = matches\n# #     submission = combine_graph(submission)\n# submission[['posting_id', 'matches']].to_csv('submission.csv', index=False)","cb68dd7d":"## Dataset","4587722f":"### ForFP","c9ba7624":"## Model","33215fc1":"## Generate Features","1e4cdedc":"## Summary\nThis is the inference notebook for https:\/\/www.kaggle.com\/underwearfitting\/pytorch-densenet-arcface-validation-training. In this notebook, I submitted a single fold trained Arcface Densenet121 with a CV 0.731. I computed the cosine similarities between the feature vectors. To make it faster, I put this process on GPU and compute by batches to avoid OOM issue. Don't hesitate if you have any questions; answering your questions can help me learn as well. ","2d95f0e4":"## Configuration","9c991c47":"### TFIDF","fb975e6f":"## Inference by batches","80e09b06":"## Imports","151be8ae":"## Read in test data","21c48d16":"## Transforms"}}