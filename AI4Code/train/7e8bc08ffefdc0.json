{"cell_type":{"9eec77ee":"code","b2b94fc7":"code","bb74619e":"code","1729ac77":"code","24ddaf60":"code","37b0545d":"code","fa4e2d15":"code","7d5666af":"code","387710ef":"code","48fc758b":"code","36b80390":"code","9dd77fda":"code","28a16bc0":"code","56b25ab1":"code","5c861039":"code","9fdcb12f":"code","65579605":"markdown","44f55f99":"markdown","a2881e3d":"markdown","a5362445":"markdown","32362f39":"markdown","d47ab358":"markdown","e54d0d10":"markdown","f6c6f3aa":"markdown","25d32534":"markdown","61cad5dc":"markdown","f6bd10c7":"markdown"},"source":{"9eec77ee":"#import the basics\nimport numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib as plt\nimport time\n\n#import sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import svm\nfrom sklearn.metrics import classification_report,confusion_matrix, accuracy_score\nfrom sklearn.model_selection import GridSearchCV","b2b94fc7":"# Load the data\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_file = pd.read_csv('..\/input\/titanic\/test.csv')\ndisplay(train.head(),test_file.head())","bb74619e":"hm = sns.heatmap(train[[\"Survived\", \"Pclass\", \"SibSp\", \"Parch\", \"Age\", \"Fare\"]].corr(),annot=True, fmt = \".2f\", cmap = \"cool\")","1729ac77":"SibSurPlt = sns.catplot(x=\"SibSp\",y=\"Survived\",data=train,kind=\"bar\", palette = \"cool\").despine(left=True)\nParSurPlt = sns.catplot(x=\"Parch\",y=\"Survived\",data=train,kind=\"bar\", palette = \"cool\").despine(left=True)\nSurClass = sns.catplot(x=\"Pclass\",y=\"Survived\",data=train,kind=\"bar\", palette = \"cool\").despine(left=True)","24ddaf60":"# Age can be divided into 4 categories - child, young adult, older adult, old person (etc)\n#print(pd.cut(X['Age'], 6)) # - Displays: (-0.08, 13.333] < (13.333, 26.667] < (26.667, 40.0] < (40.0, 53.333] < (53.333, 66.667] < (66.667, 80.0]\n\ndef sortage(x):\n    if x <= 13:\n        return 0\n    elif x <= 26:\n        return 1\n    elif x <= 40:\n        return 2\n    elif x <= 53:\n        return 3\n    elif x <= 65:\n        return 4\n    else:\n        return 5\n    \n# We can categorize fare type instead of broad factor of fare price\n# It seems like people who have higher fare also had higher chance of survival\n# print(pd.cut(X['Fare'], 5))\ndef sortfare(x):\n    if x<=100:\n        return 0\n    if x<=200:\n        return 1\n    if x<=300:\n        return 2\n    if x<=400:\n        return 3\n    else:\n        return 4\n    \n\n\n# This function prepares the data\n# Takes X as input, returns stripped and modified version of it\ndef clean_data(X):\n    X[\"Cabin\"] = X[\"Cabin\"].fillna('N')\n    X[\"CabinLitera\"] = X.Cabin.apply(lambda x: x[:1])\n    X[\"CabinLitera\"] = X[\"CabinLitera\"].map({\"A\":1, \"B\":2, \"C\":3, \"D\":4, \"E\":5, \"F\":6, \"G\":7, \"T\":8, \"N\":0})\n\n    MeanAge = X[\"Age\"].mean()\n    X[\"Age\"] = X[\"Age\"].fillna(MeanAge).astype(int) #consider different value for unknown age\n\n    #This is an additional feature, displaying total number of relatives on board\n    X[\"TotalKin\"] = X[\"SibSp\"] + X[\"Parch\"]\n\n    # Let's replace loading port with digit. Unknown = 0\n    X[\"Embarked\"] = X[\"Embarked\"].fillna('S')\n    X = pd.concat([X,pd.get_dummies(X[\"Embarked\"])], axis = 1)\n    #X[\"Embarked\"] = X[\"Embarked\"].map({\"C\":1, \"Q\":2, \"S\":3})\n\n    # Setting sex as digit\n    X = pd.concat([X,pd.get_dummies(X[\"Sex\"])], axis = 1)\n    X[\"Sex\"] = X[\"Sex\"].map({\"female\":1,\"male\":0})\n    # Title and Lastname as Features:\n    #X[\"Surname\"] = X.Name.apply(lambda x: x.split(' ')[0][:-1])\n    X[\"NamePrefix\"] = X.Name.apply(lambda x: x.split(' ')[1][:-1])\n    # Maybe there is a slight correlation between surname and survival chance. For now we ignore them\n\n    # Name Prefixes describe person's position in society.\n    # Societal status back in the day of Titanic catastrophy definitely had an impact on survival\n    X[\"NamePrefix\"].replace(['Mme', 'Ms', 'Mlle'], ['Mrs', 'Miss', 'Miss'], inplace = True)\n    X[\"NamePrefix\"] = X[\"NamePrefix\"].apply(lambda x: 'Other' if x not in ['Mr', 'Miss', 'Mrs', 'Master', 'Dr', 'Rev'] else x)\n    X[\"NamePrefix\"] = X[\"NamePrefix\"].map({'Other':0, 'Mr':1, 'Miss':2, 'Mrs':3, 'Master':5, 'Dr':6, 'Rev':7})\n\n    # SYNTHETIC FEATURES\n    # Let's consider some artificial features. It seems like being single is a pretty strong feature\n    X[\"IsSingle\"] = X[\"TotalKin\"].apply(lambda x: 1 if x == 0 else 0)\n\n    X['CategoricalAge'] = X['Age'].apply(sortage)\n\n    X['FareType'] = X['Fare'].apply(sortfare)\n    #Drop some unneeded features, \n    X = X.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\", \"Age\", \"Fare\", \"Sex\", \"Embarked\"], axis = 1)\n    return X\n    ","37b0545d":"X = train.copy()\ny = train[\"Survived\"]\nX = X.drop([\"Survived\"], axis=1)\nX = clean_data(X)\ndisplay(X.sample(5))","fa4e2d15":"X1 = test_file.copy()\nX1 = clean_data(X1)\ndisplay(X1.sample(5))","7d5666af":"hm = sns.heatmap(X[[\"Pclass\", \"SibSp\", \"Parch\", \"CabinLitera\", \"TotalKin\", \"female\",\"male\",\"NamePrefix\", \"IsSingle\", \"CategoricalAge\", \"FareType\",\"C\", \"Q\", \"S\"]].corr(),annot=True, fmt = \".2f\", cmap = \"cool\")\n# Hmm... Maybe TotalKin is excessive...","387710ef":"hm = sns.heatmap(X1[[\"Pclass\", \"SibSp\", \"Parch\", \"CabinLitera\", \"TotalKin\", \"female\",\"male\",\"NamePrefix\", \"IsSingle\", \"CategoricalAge\", \"FareType\", \"C\", \"Q\", \"S\"]].corr(),annot=True, fmt = \".2f\", cmap = \"cool\")","48fc758b":"# Let's split test data\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\nX_test, X_cv, y_test, y_cv = train_test_split(X_test, y_test, test_size = 0.2)\nprint('Training set: ', X_train.shape,' \\nTest set:     ', X_test.shape, '\\nCross-validation set: ', X_cv.shape)","36b80390":"# Scaling the entire X\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\nX_cv = scaler.transform(X_cv)\nX1 = scaler.transform(X1)","9dd77fda":"#parameters = {'solver': ['lbfgs', 'sgd', 'adam'], 'alpha': [0.00001,0.0001,0.001,0.1,1], 'activation':['tanh', 'relu'], 'hidden_layer_sizes':[(10,10),50,100,(100,100)]}\n# Good result was\n# 'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 100), 'solver': 'sgd'\nparameters = {'solver': ['lbfgs', 'sgd', 'adam'], 'alpha': [0.000005,0.00001,0.0001,0.001], 'activation':['relu'], 'hidden_layer_sizes':[20, 100,(100,100),(100,100,100)]}\nMLPs = GridSearchCV(MLPClassifier(max_iter=2000), parameters, verbose=1, n_jobs=-1)\nMLPs.fit(X_train, y_train)","28a16bc0":"print(MLPs.score(X_test, y_test))\nprint(\"Best Parameters:\\n\", MLPs.best_params_)\n#print(\"Best Estimators:\\n\", MLPs.best_estimator_)\npredictions_X1_mlp = MLPs.predict(X1)\npredictions_output_mlp = pd.DataFrame({'PassengerId': test_file.PassengerId, 'Survived': predictions_X1_mlp})\ndisplay(predictions_output_mlp.sample(20), predictions_output_mlp.Survived.unique())","56b25ab1":"param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001, 0.00001, 10]}\nSVMs = GridSearchCV(svm.SVC(), param_grid, verbose=1, n_jobs=-1)\nSVMs.fit(X_train, y_train)\nprint(SVMs.score(X_test, y_test))\nprint(SVMs.best_params_)","5c861039":"# Form output\npredictions_X1 = SVMs.predict(X1)\npredictions_output = pd.DataFrame({'PassengerId': test_file.PassengerId, 'Survived': predictions_X1})\ndisplay(predictions_output.sample(20), predictions_output.Survived.unique())","9fdcb12f":"SVMs.fit(X, y)\nMLPs.fit(X, y)\nacc = SVMs.score(X_cv, y_cv)\nacc_mlp = MLPs.score(X_cv, y_cv)\nprint('SVM CV acc: ', acc, ' MLP CV acc: ', acc_mlp)\npredictions_output_mlp.to_csv('survival_titanic_pred_mlp.csv', index=False)\npredictions_output.to_csv('survival_titanic_pred.csv', index=False)\nif acc_mlp > acc:\n    print(\"MLP version seems better...\")\nelse:\n    print(\"SVM version seems better...\")","65579605":"There's a clear correlation between survival by relatives amount and survival by class.\nFare and survival doesn't look like it has direct correlation. Doesn't matter, though. Maybe NN can figure it out.","44f55f99":"# Surviving the Titanic\nMLP-NN and SVM\n\n### Step 1. Import everything","a2881e3d":"# Selecting values for MLP","a5362445":"Let's view some graphs on training data","32362f39":"Thank you for viewing. We are somewhat done here. There is much to improve, but it's decent overall.","d47ab358":"### Step 3. Let's prepare training data","e54d0d10":"## Step 4. Splitting the data\nIt's pretty straightforward","f6c6f3aa":"# Trying SVM\nSupport Vectors seem to be also promising to filter out the data accurately\n\nWith SVM we can automatically play around with parameters to determine better ones.","25d32534":"Let's explore how data looks. Because data is beautiful.","61cad5dc":"### Step 2. Exploring the data","f6bd10c7":"Accuracy for MLP tends to be around 80+% in about a 1000-1500 iterations.\n\nThat is not a terrific result, but it shows right direction."}}