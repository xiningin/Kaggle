{"cell_type":{"0a94de04":"code","a09a2183":"code","092be6a7":"code","7f1a15b7":"code","59016c98":"code","139a2b32":"code","c847e1c1":"code","12cbd611":"code","9e4c2eaa":"code","f50091f7":"code","e3632ab7":"code","5aa2461d":"code","0b91591f":"code","98394a6e":"code","245647de":"code","0d98483e":"code","31025988":"code","5c40bb5f":"code","abf00e30":"code","b945b5d8":"code","9ecd3bf3":"code","456cbe1b":"code","aecd45e1":"code","fff8e916":"code","70c08400":"code","d3361f0f":"code","e259c03f":"code","f16d6f72":"code","ae684aa3":"code","81f27ca3":"code","be92a2fa":"code","4c75c906":"code","7e5fced0":"markdown","9527c9e0":"markdown","3c39fcb8":"markdown","d0617082":"markdown","eb851636":"markdown","bcc37825":"markdown","df7a49a5":"markdown","450bdc3b":"markdown","5b72a0be":"markdown","9809e384":"markdown","19ecb42e":"markdown","436718eb":"markdown","d347dbd2":"markdown","a7831b08":"markdown","8af57618":"markdown","789e0358":"markdown","e1a0b4d0":"markdown","bcd89025":"markdown","d049d094":"markdown"},"source":{"0a94de04":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a09a2183":"import torch\nif torch.cuda.is_available():  \n    device = torch.device(\"cuda\")\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n    \nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","092be6a7":"df_train=pd.read_csv(\"\/kaggle\/input\/turkish-sentiment-analysis-data-beyazperdecom\/train.csv\",index_col=[0],encoding=\"windows-1252\")\ndf_test=pd.read_csv(\"\/kaggle\/input\/turkish-sentiment-analysis-data-beyazperdecom\/test.csv\",index_col=[0],encoding=\"windows-1252\")","7f1a15b7":"df_train.head()","59016c98":"df_train[\"Label\"].value_counts()","139a2b32":"df_train.isna().sum()","c847e1c1":"import string\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nstop_words=stopwords.words(\"turkish\")\nstop_words.extend([\"bir\",\"film\",\"filmi\",\"filme\",\"filmde\",\"filmden\",\"filmin\",\"kadar\",\"bi\",\"ben\"]) #sone extra corpus-related stopwords\n\nexclude = set(string.punctuation)\n\n#for positive sentiments\ndf_pos=df_train[df_train[\"Label\"]==1]\n\n#for only unigrams\ntoken_list=[]\n\nfor i,r in df_pos.iterrows():\n    text=''.join(ch for ch in df_pos[\"comment\"][i] if ch not in exclude and ch != \"\u2019\") #remove punctuations from the text in order not to distort frequencies\n    tokens=word_tokenize(text)\n    tokens=[tok.lower() for tok in tokens if tok not in stop_words] #remove stopwords from the text in order not to distort frequencies\n    token_list.extend(tokens)\n    \nfrequencies=Counter(token_list)\nfrequencies_sorted=sorted(frequencies.items(), key=lambda k: k[1],reverse=True)\ntop_15=dict(frequencies_sorted[0:15])","12cbd611":"import matplotlib.pyplot as plt\nimport numpy as np\n\nplt.rcdefaults()\nfig, ax = plt.subplots()\n\n# Example data\nngram = top_15.keys()\ny_pos = np.arange(len(ngram))\nperformance = top_15.values()\n\n\nax.barh(y_pos, performance, align='center')\nax.set_yticks(y_pos)\nax.set_yticklabels(ngram)\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('Counts')\nax.set_title('Top-15 Most Common Unigrams in Positive Comments')\n\nplt.show()","9e4c2eaa":"#for negative sentiments\ndf_neg=df_train[df_train[\"Label\"]==0]\n\n#for only unigrams\ntoken_list=[]\n\nfor i,r in df_neg.iterrows():\n    text=''.join(ch for ch in df_neg[\"comment\"][i] if ch not in exclude and ch != \"\u2019\") #remove punctuations from the text in order not to distort frequencies\n    tokens=word_tokenize(text)\n    tokens=[tok.lower() for tok in tokens if tok not in stop_words] #remove stopwords from the text in order not to distort frequencies\n    token_list.extend(tokens)\n    \nfrequencies=Counter(token_list)\nfrequencies_sorted=sorted(frequencies.items(), key=lambda k: k[1],reverse=True)\ntop_15=dict(frequencies_sorted[0:15])","f50091f7":"import matplotlib.pyplot as plt\nimport numpy as np\n\nplt.rcdefaults()\nfig, ax = plt.subplots()\n\n# Example data\nngram = top_15.keys()\ny_pos = np.arange(len(ngram))\nperformance = top_15.values()\n\n\nax.barh(y_pos, performance, align='center')\nax.set_yticks(y_pos)\nax.set_yticklabels(ngram)\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('Counts')\nax.set_title('Top-15 Most Common Unigrams in Negative Comments')\n\nplt.show()","e3632ab7":"from nltk.util import ngrams\n\ntoken_list=[]\n\nfor i,r in df_pos.iterrows():\n    text=''.join(ch for ch in df_pos[\"comment\"][i] if ch not in exclude and ch != \"\u2019\") #remove punctuations from the text in order not to distort frequencies\n    tokens=word_tokenize(text)\n    tokens=[tok.lower() for tok in tokens if tok not in stop_words] #remove stopwords from the text in order not to distort frequencies \n    token_list.extend(tokens)\n    \n \nbigrams=list(ngrams(token_list,2))\nfrequencies=Counter(bigrams)\nfrequencies_sorted=sorted(frequencies.items(), key=lambda k: k[1],reverse=True)\ntop_15=dict(frequencies_sorted[0:15])\n","5aa2461d":"plt.rcdefaults()\nfig, ax = plt.subplots()\n\n# Example data\nngram = top_15.keys()\ny_pos = np.arange(len(ngram))\nperformance = top_15.values()\n\n\nax.barh(y_pos, performance, align='center')\nax.set_yticks(y_pos)\nax.set_yticklabels(ngram)\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('Counts')\nax.set_title('Top-15 Most Common Bigrams in Positive Comments')\n\nplt.show()","0b91591f":"token_list=[]\n\nfor i,r in df_neg.iterrows():\n    text=''.join(ch for ch in df_neg[\"comment\"][i] if ch not in exclude and ch != \"\u2019\") #remove punctuations from the text in order not to distort frequencies\n    tokens=word_tokenize(text)\n    tokens=[tok.lower() for tok in tokens if tok not in stop_words] #remove stopwords from the text in order not to distort frequencies \n    token_list.extend(tokens)\n    \n \nbigrams=list(ngrams(token_list,2))\nfrequencies=Counter(bigrams)\nfrequencies_sorted=sorted(frequencies.items(), key=lambda k: k[1],reverse=True)\ntop_15=dict(frequencies_sorted[0:15])","98394a6e":"plt.rcdefaults()\nfig, ax = plt.subplots()\n\n# Example data\nngram = top_15.keys()\ny_pos = np.arange(len(ngram))\nperformance = top_15.values()\n\n\nax.barh(y_pos, performance, align='center')\nax.set_yticks(y_pos)\nax.set_yticklabels(ngram)\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('Counts')\nax.set_title('Top-15 Most Common Bigrams in Negative Comments')\n\nplt.show()","245647de":"# Get the comments and their labels from training data as series.\ncomments = df_train.comment.values\nlabels = df_train.Label.values","0d98483e":"from transformers import AutoTokenizer\n#Load BERT Turkish tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"dbmdz\/bert-base-turkish-cased\",do_lower_case=True)","31025988":"\nimport matplotlib.pyplot as plt\ndef plot_sentence_embeddings_length(text_list, tokenizer):\n    tokenized_texts = list(map(lambda t: tokenizer.tokenize(t), text_list))\n    tokenized_texts_len = list(map(lambda t: len(t), tokenized_texts))\n    fig, ax = plt.subplots(figsize=(8, 5));\n    ax.hist(tokenized_texts_len, bins=40);\n    ax.set_xlabel(\"Length of Comment Embeddings\");\n    ax.set_ylabel(\"Number of Comments\");\n    return","5c40bb5f":"plot_sentence_embeddings_length(comments,tokenizer)","abf00e30":"indices=tokenizer.batch_encode_plus(comments,max_length=128,add_special_tokens=True, return_attention_mask=True,pad_to_max_length=True,truncation=True)\ninput_ids=indices[\"input_ids\"]\nattention_masks=indices[\"attention_mask\"]\nprint(input_ids[0])\nprint(comments[0])","b945b5d8":"from sklearn.model_selection import train_test_split\n\n# Use 99% for training and 1% for validation.\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n                                                            random_state=42, test_size=0.2)\n# Do the same for the masks.\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n                                             random_state=42, test_size=0.2)","9ecd3bf3":"# Convert all of our data into torch tensors, the required datatype for our model\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels, dtype=torch.long)\nvalidation_labels = torch.tensor(validation_labels, dtype=torch.long)\ntrain_masks = torch.tensor(train_masks, dtype=torch.long)\nvalidation_masks = torch.tensor(validation_masks, dtype=torch.long)","456cbe1b":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\nbatch_size = 32\n\n# Create the DataLoader for our training set.\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set.\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)","aecd45e1":"from transformers import AutoModelForSequenceClassification, AdamW, AutoConfig\nconfig = AutoConfig.from_pretrained(\n        \"dbmdz\/bert-base-turkish-cased\",num_labels=2)\n# Load BertForSequenceClassification, the pretrained BERT model with a single \n# linear classification layer on top. \nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"dbmdz\/bert-base-turkish-cased\",config=config)\n\nmodel.cuda()","fff8e916":"# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n# 'W' stands for 'Weight Decay fix\"\noptimizer = AdamW(model.parameters(),\n                  lr = 2e-5, # args.learning_rate - default is 5e-5\n                  betas=[0.9,0.999],\n                  eps = 1e-6 # args.adam_epsilon  - default is 1e-8.\n                )\nfrom transformers import get_linear_schedule_with_warmup\n\n# Number of training epochs (authors recommend between 2 and 4)\nepochs = 5\n\n# Total number of training steps is number of batches * number of epochs.\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","70c08400":"import numpy as np\n\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)","d3361f0f":"import time\nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","e259c03f":"import random\n\n# This training code is based on the `run_glue.py` script here:\n# https:\/\/github.com\/huggingface\/transformers\/blob\/5bfcd0485ece086ebcbed2d008813037968a9e58\/examples\/run_glue.py#L128\n\n\n# Set the seed value all over the place to make this reproducible.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# Store the average loss after each epoch so we can plot them.\nloss_values = []\n\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set.\n\n    print(\"\")\n    print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    total_loss = 0\n\n    # Put the model into training mode. Don't be mislead--the call to \n    # `train` just changes the *mode*, it doesn't *perform* the training.\n    # `dropout` and `batchnorm` layers behave differently during training\n    # vs. test (source: https:\/\/stackoverflow.com\/questions\/51433378\/what-does-model-train-do-in-pytorch)\n    model.train()\n\n    # For each batch of training data...\n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 30 batches.\n        if step % 30 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n        # `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        # Always clear any previously calculated gradients before performing a\n        # backward pass. PyTorch doesn't do this automatically because \n        # accumulating the gradients is \"convenient while training RNNs\". \n        # (source: https:\/\/stackoverflow.com\/questions\/48001598\/why-do-we-need-to-call-zero-grad-in-pytorch)\n        model.zero_grad()        \n\n        # Perform a forward pass (evaluate the model on this training batch).\n        # This will return the loss (rather than the model output) because we\n        # have provided the `labels`.\n        # The documentation for this `model` function is here: \n        # https:\/\/huggingface.co\/transformers\/v2.2.0\/model_doc\/bert.html#transformers.BertForSequenceClassification\n        outputs = model(b_input_ids, \n                    token_type_ids=None, \n                    attention_mask=b_input_mask, \n                    labels=b_labels)\n        \n        # The call to `model` always returns a tuple, so we need to pull the \n        # loss value out of the tuple.\n        loss = outputs[0]\n\n        # Accumulate the training loss over all of the batches so that we can\n        # calculate the average loss at the end. `loss` is a Tensor containing a\n        # single value; the `.item()` function just returns the Python value \n        # from the tensor.\n        total_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are\n        # modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n\n        # Update the learning rate.\n        scheduler.step()\n\n    # Calculate the average loss over the training data.\n    avg_train_loss = total_loss \/ len(train_dataloader)            \n    \n    # Store the loss value for plotting the learning curve.\n    loss_values.append(avg_train_loss)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model.eval()\n\n    # Tracking variables \n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        \n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        \n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        # Telling the model not to compute or store gradients, saving memory and\n        # speeding up validation\n        with torch.no_grad():        \n\n            # Forward pass, calculate logit predictions.\n            # This will return the logits rather than the loss because we have\n            # not provided labels.\n            # token_type_ids is the same as the \"segment ids\", which \n            # differentiates sentence 1 and 2 in 2-sentence tasks.\n            # The documentation for this `model` function is here: \n            # https:\/\/huggingface.co\/transformers\/v2.2.0\/model_doc\/bert.html#transformers.BertForSequenceClassification\n            outputs = model(b_input_ids, \n                            token_type_ids=None, \n                            attention_mask=b_input_mask)\n        \n        # Get the \"logits\" output by the model. The \"logits\" are the output\n        # values prior to applying an activation function like the softmax.\n        logits = outputs[0]\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        # Calculate the accuracy for this batch of test sentences.\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n        \n        # Accumulate the total accuracy.\n        eval_accuracy += tmp_eval_accuracy\n\n        # Track the number of batches\n        nb_eval_steps += 1\n\n    # Report the final accuracy for this validation run.\n    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy\/nb_eval_steps))\n    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n\nprint(\"\")\nprint(\"Training complete!\")","f16d6f72":"# Create sentence and label lists for test data\ncomments1 = df_test.comment.values\nlabels1 = df_test.Label.values\n\nindices1=tokenizer.batch_encode_plus(comments1,max_length=128,add_special_tokens=True, return_attention_mask=True,pad_to_max_length=True,truncation=True)\ninput_ids1=indices1[\"input_ids\"]\nattention_masks1=indices1[\"attention_mask\"]\n\nprediction_inputs1= torch.tensor(input_ids1)\nprediction_masks1 = torch.tensor(attention_masks1)\nprediction_labels1 = torch.tensor(labels1)\n\n# Set the batch size.  \nbatch_size = 32 \n\n# Create the DataLoader.\nprediction_data1 = TensorDataset(prediction_inputs1, prediction_masks1, prediction_labels1)\nprediction_sampler1 = SequentialSampler(prediction_data1)\nprediction_dataloader1 = DataLoader(prediction_data1, sampler=prediction_sampler1, batch_size=batch_size)","ae684aa3":"# Prediction on test set\n\nprint('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs1)))\n\n# Put model in evaluation mode\nmodel.eval()\n\n# Tracking variables \npredictions , true_labels = [], []\n\n# Predict \nfor batch in prediction_dataloader1:\n  # Add batch to GPU\n  batch = tuple(t.to(device) for t in batch)\n  \n  # Unpack the inputs from our dataloader\n  b_input_ids1, b_input_mask1, b_labels1 = batch\n  \n  # Telling the model not to compute or store gradients, saving memory and \n  # speeding up prediction\n  with torch.no_grad():\n      # Forward pass, calculate logit predictions\n      outputs1 = model(b_input_ids1, token_type_ids=None, \n                      attention_mask=b_input_mask1)\n\n  logits1 = outputs1[0]\n\n  # Move logits and labels to CPU\n  logits1 = logits1.detach().cpu().numpy()\n  label_ids1 = b_labels1.to('cpu').numpy()\n  \n  # Store predictions and true labels\n  predictions.append(logits1)\n  true_labels.append(label_ids1)\n\nprint('    DONE.')","81f27ca3":"# Combine the predictions for each batch into a single list of 0s and 1s.\nflat_predictions = [item for sublist in predictions for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n# Combine the correct labels for each batch into a single list.\nflat_true_labels = [item for sublist in true_labels for item in sublist]","be92a2fa":"from sklearn.metrics import accuracy_score\nprint(\"Accuracy of BERT is:\",accuracy_score(flat_true_labels, flat_predictions))","4c75c906":"from sklearn.metrics import classification_report\nprint(classification_report(flat_true_labels, flat_predictions))","7e5fced0":"# 3) BERTurk Model Performance on Test Set ","9527c9e0":"Since BERT takes maximum 512 tokens as input, we need to take care of input lengths. To show length of embeddings in the data will be helpful to determine maximum length of comments and padding threshold.","3c39fcb8":"To understand the dataset, let's have a look at the common top-15 unigrams and bigrams' frequencies for each class. To generate them, we need to tokenize each comments by using **nltk** library.","d0617082":"Even the majority of common words are quite sensible, when only unigrams are explored, some words does not make sense. For example, \"g\u00fczel\" in Turkish has positive meaning (the most common unigram in the positive comments). However, it is one of the most common (5th one) single word in the negative comments. Therefore, the word before and after \"g\u00fczel\" gains more importance. Let's have a look at the bigrams :) ","eb851636":"## Unigram and Bigram Distributions in Each Class","bcc37825":"As stated in https:\/\/stackoverflow.com\/questions\/54938815\/data-preprocessing-for-nlp-pre-training-models-e-g-elmo-bert, BERT has word piece tokenizer and it magicly helps with dirty data. Therefore, there is no so much need to apply preprocessing steps.\n","df7a49a5":"### If you like this notebook, feel free to UPVOTE :) ","450bdc3b":"* The dataset is highly balanced. \n* There is no missing value.","5b72a0be":"With bigram distributions in each class, the dataset seems to have quite good labels. Let's dive into classification model!","9809e384":"# 2) BERTURK Model Fine Tuning","19ecb42e":"### **POSITIVE CLASS BIGRAMS**","436718eb":"*In order to speed up the training process, I recommend to use GPU provided by Kaggle.*","d347dbd2":"It seems almost all comments have less than 100 tokens, therefore instead of 512, we can set maximum length as 128.","a7831b08":"### **NEGATIVE CLASS UNIGRAMS**","8af57618":"# 1) Exploratory Data Analysis","789e0358":"### **POSITIVE CLASS UNIGRAMS**","e1a0b4d0":"Set device type as CUDA to utilize GPU:","bcd89025":"In this notebook, I am going to classify Turkish film comments shared in beyazperde.com with the help of [BERTurk pretrained language model](https:\/\/github.com\/stefan-it\/turkish-bert) using PyTorch and Huggingface Transformers library.","d049d094":"### **We reached 93% accuracy score on this highly balanced dataset even without hyperparameter optimization with BERTURK base model. For comparison, ULMFit pretrained language model based on LSTMs which is develoed by fastai team, has [91% accuracy score](https:\/\/github.com\/fastai\/course-nlp\/blob\/master\/nn-turkish.ipynb) on this dataset. Also, Gezici (2018), *Sentiment Analysis in Turkish* has 75%.**"}}