{"cell_type":{"037787d8":"code","5b14c384":"code","7c3ad473":"code","129d32cd":"code","220d80fe":"code","fdb082c1":"code","c0b15d0e":"code","2dba681f":"code","2cd77522":"code","383e1758":"code","7ddac96e":"code","989084ee":"code","ff3e67a9":"code","bf00024b":"code","b15c4352":"code","9b61a452":"code","145adc14":"code","9a7446da":"code","ee7bd6a0":"code","da0e7b8e":"code","91f66184":"code","9fde1eac":"code","c206770c":"code","2c4c4e20":"code","b17f0f8a":"code","c65fb8ca":"code","71667310":"code","e534088a":"code","10a1a4a6":"code","679e03cf":"code","70d98a20":"code","4c2055f6":"code","eb1bb70c":"code","87273812":"code","6ba32d95":"code","cea37ebd":"code","950caded":"code","bb3c29a3":"code","70202d48":"code","fc2a390c":"code","b597e6dd":"code","d0181d33":"code","e283af35":"code","21a201d2":"code","ad43371f":"code","fdfe512b":"code","a10c6850":"code","96768b4e":"code","91c29eae":"code","fb6555f2":"code","e2d3b59d":"code","1b831359":"code","f949a439":"code","afac5054":"code","d0652b62":"code","163226cb":"code","02d90888":"code","7f821eaa":"code","d6db5273":"code","731f1f88":"code","297ce247":"code","5e77c3c0":"markdown","ef8ff3c6":"markdown","5d599ca3":"markdown","d83e9e24":"markdown","5a46bf51":"markdown","58f171c5":"markdown","537ce7d0":"markdown","4d1214ab":"markdown","1f86c7e9":"markdown","4fcda0e4":"markdown","0faeaf15":"markdown","a8ad02cd":"markdown","5e67ffd9":"markdown","17935178":"markdown","86c942fc":"markdown","83b3881d":"markdown","d54e1670":"markdown","812bb670":"markdown"},"source":{"037787d8":"# Import Dependencies\n%matplotlib inline\n\n# Start Python Imports\nimport math, time, random, datetime\nfrom random import shuffle\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\n# Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize, MinMaxScaler\n\n# Machine learning\nfrom sklearn.metrics import roc_auc_score\nimport catboost\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier, Pool, CatBoostRegressor,cv\nimport xgboost\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.svm import SVC\nfrom sklearn.svm import SVR\n\n\n\n# Let's be rebels and ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')","5b14c384":"df=pd.read_csv('..\/input\/summeranalytics2020\/train.csv')\ndf.shape","7c3ad473":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\ndf.head()","129d32cd":"sns.heatmap(df.isnull(),yticklabels=False,cbar=False)\n\n#df.isnull().sum()","220d80fe":"test_df=pd.read_csv('..\/input\/summeranalytics2020\/test.csv')\nprint(test_df.shape)","fdb082c1":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\ntest_df.head()","c0b15d0e":"sns.heatmap(test_df.isnull(),yticklabels=False,cbar=False)","2dba681f":"### concatenating training and testing dataset horizontally\ndf_combine=pd.concat([df,test_df],axis=0, sort = False,ignore_index = True)\ndf_combine.shape","2cd77522":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\ndf_combine.head()","383e1758":"nominal_catg_col = list(df_combine.select_dtypes(['object']).columns)\nprint(nominal_catg_col)","7ddac96e":"ordinal_catg_col = [\"Education\", \"EnvironmentSatisfaction\", \"JobInvolvement\",\"JobSatisfaction\",\n                    \"PerformanceRating\",\"StockOptionLevel\",\"CommunicationSkill\", \"Behaviour\" ]\nprint(ordinal_catg_col)","989084ee":"# Since Gender and overtime feature have only 2 labels hence using \"drop_first=True\".\n\ndf_BusinessTravel_one_hot = pd.get_dummies(df_combine['BusinessTravel'], prefix='BusinessTravel')\n\ndf_Department_one_hot = pd.get_dummies(df_combine['Department'], prefix='Department')\n\ndf_EducationField_one_hot = pd.get_dummies(df_combine['EducationField'], prefix='EducationField')\n\ndf_Gender_one_hot = pd.get_dummies(df_combine['Gender'], prefix='Gender',drop_first=True)\n\ndf_JobRole_one_hot = pd.get_dummies(df_combine['JobRole'], prefix='JobRole')\n\ndf_MaritalStatus_one_hot = pd.get_dummies(df_combine['MaritalStatus'], prefix='MaritalStatus')\n\ndf_OverTime_one_hot = pd.get_dummies(df_combine['OverTime'], prefix='OverTime',drop_first=True)","ff3e67a9":"# Combine the one hot encoded columns with df_con_enc\ndf_nominal_catg = pd.concat([df_BusinessTravel_one_hot, df_Department_one_hot, df_EducationField_one_hot,\n                             df_Gender_one_hot, df_JobRole_one_hot, df_MaritalStatus_one_hot,\n                             df_OverTime_one_hot], axis=1)\n\nprint(df_nominal_catg.shape)\ndf_nominal_catg.head()","bf00024b":"## Creating ordinal dataframe\n\ndf_ordinal_catg = df_combine[ordinal_catg_col]\nprint(df_ordinal_catg.shape)\ndf_ordinal_catg.head()","b15c4352":"final_catg_col = nominal_catg_col + ordinal_catg_col + [\"Attrition\"]\n\ndf_numeric = df_combine.drop( final_catg_col , axis=1)\ndf_numeric.shape","9b61a452":"# As we separated categorical data from the numeric one, now we can do feature scaling\n\nnumeric_col = list(df_numeric.columns)\n\nscaler = MinMaxScaler(feature_range=(0, 1))\ndf_numeric = scaler.fit_transform(df_numeric)\ndf_numeric = pd.DataFrame(df_numeric, columns= numeric_col)","145adc14":"print(df_numeric.shape)\ndf_numeric.head()","9a7446da":"print(df_numeric.shape)\nprint(df_ordinal_catg.shape)\nprint(df_nominal_catg.shape)","ee7bd6a0":"df_pre_process = pd.concat([df_numeric, df_ordinal_catg.reset_index(drop=True),\n                           df_nominal_catg.reset_index(drop=True)],axis=1,sort = False)\n\nprint(df_pre_process.shape)\ndf_pre_process.head()","da0e7b8e":"X_train_pre = df_pre_process.iloc[:1628,:]\nX_test_pre = df_pre_process.iloc[1628:,:]\ny_train = df['Attrition']\n\nprint(X_train_pre.shape)\nprint(y_train.shape)\nprint(X_test_pre.shape)","91f66184":"def feat_to_binning(str, bin):\n\n    df_bin = pd.DataFrame()\n    df_bin[str] = df_combine[str]\n\n    fig, ax = plt.subplots()\n    df_bin[str].hist(bins = bin, color='#A9C5D3', edgecolor='black',  \n                              grid=False)\n    #ax.set_title('Whole dataset {} Histogram'.format{str}, fontsize=12)\n    ax.set_xlabel(str, fontsize=12)\n    ax.set_ylabel('Frequency', fontsize=12)\n    \n    df_bin[str + '_bin_round'] = np.array(np.int64(\n                              np.array(df_bin[str]) \/ 10.))\n    return df_bin\n","9fde1eac":"df_age = feat_to_binning(\"Age\",5)\n\nprint(df_age['Age_bin_round'].value_counts())\n\ndf_age.head()","c206770c":"X_train_pre['Age'] = df_age.loc[:1628,'Age_bin_round']\nX_test_pre['Age'] = df_age.loc[1628:,'Age_bin_round']\n","2c4c4e20":"df.describe()","b17f0f8a":"X_train_pre.describe()","c65fb8ca":"num_col = numeric_col\nnum_col.remove('Age')\n\nplt.figure(figsize=(20,10))\nsns.boxplot(data= X_train_pre.loc[:,num_col])\n\n# Rotate x-labels\nplt.xticks(rotation=-45)","71667310":"plt.figure(figsize=(20,10))\nsns.boxplot(data= X_test_pre.loc[:,num_col])\n\n# Rotate x-labels\nplt.xticks(rotation=-45)","e534088a":"ord_col = ordinal_catg_col + ['Age']\n\nplt.figure(figsize=(20,10))\nsns.boxplot(data= X_train_pre.loc[:,ord_col])\n\n# Rotate x-labels\nplt.xticks(rotation=-45)","10a1a4a6":"plt.figure(figsize=(20,10))\nsns.boxplot(data= X_test_pre.loc[:,ord_col])\n\n# Rotate x-labels\nplt.xticks(rotation=-45)","679e03cf":"# Let's view the distribution of Attrition\nplt.figure(figsize=(10, 2))\nsns.countplot(y=\"Attrition\", data=df);","70d98a20":"def plot_count_dist(data, bin_df, label_column, target_column, figsize=(20, 5), use_bin_df=False):\n    \"\"\"\n    Function to plot counts and distributions of a label variable and \n    target variable side by side.\n    ::param_data:: = target dataframe\n    ::param_bin_df:: = binned dataframe for countplot\n    ::param_label_column:: = binary labelled column\n    ::param_target_column:: = column you want to view counts and distributions\n    ::param_figsize:: = size of figure (width, height)\n    ::param_use_bin_df:: = whether or not to use the bin_df, default False\n    \"\"\"\n    if use_bin_df: \n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1, 2, 1)                                      # 1 row, 2 col., 1 imamge\n        sns.countplot(y=target_column, data=bin_df);         # here data is not the args one, its inbuilt countplot arg.\n        plt.subplot(1, 2, 2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column], \n                     kde_kws={\"label\": \"Attrition-1\"});\n        sns.distplot(data.loc[data[label_column] == 0][target_column], \n                     kde_kws={\"label\": \"Attrition-0\"});\n    else:\n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1, 2, 1)\n        sns.countplot(y=target_column, data=data);\n        plt.subplot(1, 2, 2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column], \n                     kde_kws={\"label\": \"Attrition-1\"});\n        sns.distplot(data.loc[data[label_column] == 0][target_column], \n                     kde_kws={\"label\": \"Attrition-0\"});","4c2055f6":"df_bin = pd.DataFrame() # for discretised continuous variables\ndf_con = pd.DataFrame() # for continuous variables","eb1bb70c":"# Add Education to subset dataframes\ndf_bin['Education'] = df['Education']\n\n# Visualise the counts of Education and the distribution of the values against Attrition class \nplot_count_dist(df, \n                bin_df=df_bin, \n                label_column='Attrition', \n                target_column='Education', \n                figsize=(12, 8))\n","87273812":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2,f_classif\n\nX = X_train_pre.iloc[:,:21]\ny= y_train\n\nprint(X.shape)\nprint(y.shape)","6ba32d95":"#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k=21)\nfit = bestfeatures.fit(X,y)","cea37ebd":"dfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns","950caded":"featureScores                # higher is the score, more important the feature is ","bb3c29a3":"print(featureScores.nlargest(21,'Score'))  ","70202d48":"#apply SelectKBest class to extract top 10 best features\nbestfeatures_f = SelectKBest(score_func=f_classif, k=39)\nfit_f = bestfeatures.fit(X,y)","fc2a390c":"dfscores_f = pd.DataFrame(fit_f.scores_)\ndfcolumns_f = pd.DataFrame(X.columns)\n\n#concat two dataframes for better visualization \nfeatureScores_f = pd.concat([dfcolumns_f,dfscores_f],axis=1)\nfeatureScores_f.columns = ['Specs','Score']  #naming the dataframe columns","b597e6dd":"print(featureScores_f.nlargest(39,'Score')) ","d0181d33":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\n#model = ExtraTreesClassifier()\n#model = xgboost.XGBClassifier()\n\nX_imp = X_train_pre\ny_imp = y_train\n\nmodel = xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1.0, gamma=0.5, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.03, max_delta_step=0, max_depth=100,\n              min_child_weight=1, missing=None, monotone_constraints='()',\n              n_estimators=500, n_jobs=0, num_parallel_tree=1,\n              objective='binary:logistic', random_state=0, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=1, subsample=0.8,\n              tree_method='exact', validate_parameters=1, verbosity=None)\nmodel.fit(X_imp,y_imp)","e283af35":"print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers","21a201d2":"#plot graph of feature importances for better visualization\nplt.figure(figsize=(20,20))\n\nfeat_importances = pd.Series(model.feature_importances_, index=X_imp.columns)\nfeat_importances.nlargest(X_imp.shape[1]).plot(kind='barh')\nplt.show()","ad43371f":"import seaborn as sns\n\nattr = y\ndf_corr = pd.concat([X, y], axis =1 )\n\ncorrmat = df_corr.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(30,30))\n#plot heat map\ng=sns.heatmap(df_corr[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","fdfe512b":"pearson_corr_col = nominal_catg_col + ['Attrition']\n\ndf_corr =df[pearson_corr_col]\ncorrmat = df[pearson_corr_col].apply(lambda x : pd.factorize(x)[0]).corr(method='pearson', min_periods=1)\n\ntop_corr_features = corrmat.index\nplt.figure(figsize=(10,10))\n#plot heat map\ng=sns.heatmap(corrmat,annot=True,cmap=\"RdYlGn\")","a10c6850":"X_train = X_train_pre.drop(['Id','Behaviour','PerformanceRating',  'PercentSalaryHike',\n                            'YearsAtCompany','TotalWorkingYears', 'EmployeeNumber', \n                            'YearsSinceLastPromotion'], axis=1)\nX_test = X_test_pre.drop(['Id','Behaviour','PerformanceRating',  'PercentSalaryHike',\n                          'YearsAtCompany','TotalWorkingYears', 'EmployeeNumber', \n                          'YearsSinceLastPromotion'], axis=1)\n\nprint(X_train.shape)\nprint(X_test.shape)","96768b4e":"\nX_train_pre_cat = df_combine.iloc[:1628,:]\nX_train_pre_cat = X_train_pre_cat.drop(['Attrition'], axis =1 )\n\n\nX_test_pre_cat = df_combine.iloc[1628:,:]\nX_test_pre_cat = X_test_pre_cat.drop(['Attrition'], axis =1 )\n\nX_train_pre_cat['Age'] = df_age.loc[:1628,'Age_bin_round']\nX_test_pre_cat['Age'] = df_age.loc[1628:,'Age_bin_round']\n\n\nprint(X_train_pre_cat.shape)\nprint(X_test_pre_cat.shape)","91c29eae":"\nX_train_cat = X_train_pre_cat.drop(['Id','Behaviour','PerformanceRating',  'PercentSalaryHike',\n                            'YearsAtCompany','TotalWorkingYears', 'EmployeeNumber', \n                            'YearsSinceLastPromotion'], axis=1)\nX_test_cat = X_test_pre_cat.drop(['Id','Behaviour','PerformanceRating',  'PercentSalaryHike',\n                          'YearsAtCompany','TotalWorkingYears', 'EmployeeNumber', \n                          'YearsSinceLastPromotion'], axis=1)\n\nprint(X_train_cat.shape)\nprint(X_test_cat.shape)\n","fb6555f2":"all_catg_col = ['Age'] + ordinal_catg_col + nominal_catg_col \n\nindices_cat = []\nfor col in all_catg_col:\n    if (col in list(X_train_cat.columns)):\n        indices_cat.append(X_train_cat.columns.get_loc(col))\n\nindices_cat.sort()\nprint(indices_cat)\n\ntrain_pool_cat = Pool(X_train_cat, \n                  y_train,\n                  indices_cat)","e2d3b59d":"### RANDOMIZED SEARCH\n\nmodel_cat = CatBoostClassifier(random_state = 51,eval_metric = 'AUC')\n\nrandom_grid_cat = {'learning_rate': [0.05, 0.08, 0.1, 0.15, 0.2, 0.3],\n        'depth': [4, 6,10,15,20,30,40,50,60,70],\n        'l2_leaf_reg': [1, 3, 5, 7, 9]}\n\nrandomized_search_cat = model_cat.randomized_search(random_grid_cat, train_pool_cat, cv=5,plot=True)\n                    ","1b831359":"# it will print the optimize parameter you get after randomized search...\nrandomized_search_cat","f949a439":"\"\"\" Use the value of parameters you get after performing randomized search \"\"\"\n\n#model_train = CatBoostClassifier(iterations=100, depth =10, learning_rate=0.1, random_state = 51,\n#                                 loss_function ='CrossEntropy', custom_loss = ['Accuracy'], eval_metrics = 'AUC')                              \n\n# These fixed values of hyperparameters got overfitted in the private leaderboard.\nmodel_train = CatBoostClassifier(iterations=170, learning_rate=0.1, random_state = 51,eval_metric = 'AUC',loss_function ='CrossEntropy')                              \n\nmodel_train.fit(train_pool_cat) #, plot=True)\n\n# CatBoost accuracy\nacc_catboost = round(model_train.score(X_train_cat, y_train) * 100, 2)","afac5054":"# Set params for cross-validation as same as initial model\ncv_params = model_train.get_params()\n\n# Run the cross-validation for 10-folds (same as the other models)\ncv_data = cv(train_pool_cat,\n             cv_params,\n             fold_count=5,\n             plot=True)\n\n\n# CatBoost CV results save into a dataframe (cv_data), let's withdraw the maximum accuracy score\nacc_cv_catboost = round(np.max(cv_data['test-AUC-mean']) * 100, 2)\n","d0652b62":"print('train accuracy: ' , acc_catboost)\nprint(\"CV Accuracy: \" ,acc_cv_catboost)\n\ncv_data.head()","163226cb":"y_pred=model_train.predict_proba(X_test_cat)\n\npred=pd.DataFrame(y_pred[:,1])\nsub_df=pd.read_csv('..\/input\/summeranalytics2020\/Sample_submission.csv')\ndatasets=pd.concat([sub_df['Id'],pred],axis=1)\ndatasets.columns=['Id','Attrition']\ndatasets.to_csv('Catboost_submission_temp.csv',index=False)","02d90888":"xbg_classifier = xgboost.XGBClassifier(scoring = 'roc_auc', random_state = 51)\n\nbooster=['gbtree']    \n\n## Hyper Parameter Optimization\n\nhyperparameter_grid = {\n        'n_estimators' : [100, 500, 900, 1100, 1500],      # no. of decision trees used\n        'min_child_weight': [1, 2, 3, 5, 7, 9],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120],\n        'learning_rate' : [0.005,0.01,0.03, 0.05, 0.15,0.3, 0.45, 0.55]\n        }\n\n# Set up the random search with 4-fold cross validation\nfolds = 5\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 51)\n\nrandom_cv = RandomizedSearchCV(estimator=xbg_classifier,\n            param_distributions=hyperparameter_grid,\n            cv=skf.split(X_train,y_train), n_iter=50,\n            scoring = 'roc_auc',n_jobs = 4,\n            verbose = 5, \n            return_train_score = True)","7f821eaa":"random_cv.fit(X_train, y_train)","d6db5273":"classifier_t = random_cv.best_estimator_\nprint(classifier_t)\nclassifier_t.fit(X_train,y_train)     ","731f1f88":"\nacc_xgb = round(classifier_t.score(X_train, y_train) * 100, 2)\n\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\n\ntrain_pred = model_selection.cross_val_predict(classifier_t, \n                                                  X_train, \n                                                  y_train, \n                                                  cv=5, \n                                                  n_jobs = -1)\n\n\n# Cross-validation accuracy metric\nacc_cv = round(metrics.accuracy_score(y_train, train_pred) * 100, 2)\n\nprint(acc_xgb)\nprint(acc_cv)","297ce247":"y_pred=classifier_t.predict_proba(X_test)\n\npred=pd.DataFrame(y_pred[:,1])\nsub_df=pd.read_csv('..\/input\/summeranalytics2020\/Sample_submission.csv')\ndatasets=pd.concat([sub_df['Id'],pred],axis=1)\ndatasets.columns=['Id','Attrition']\ndatasets.to_csv('XBGC_random_submission_temp.csv',index=False)","5e77c3c0":"# using chi-square test","ef8ff3c6":"## Pearson Correlation among Nominal categorical feature","5d599ca3":"### After analyzing all the feature selection methods we will drop some features","d83e9e24":"# Feature Importance","5a46bf51":"### Time to concatenate all features","58f171c5":"# DATA BINNING","537ce7d0":"# XBG classifier","4d1214ab":"# Separating Nominal and Ordinal Categorial features from the data set","1f86c7e9":"# Catboost Classifier","4fcda0e4":"# Performing One hot coding for Nominal feature","0faeaf15":"## Result\n\n1. Using Catboost Classifer, I got a maximum score of 0.86377 in the public leaderboard but it changes drastically in the private leaderboard to 0.78190.\n2. **Using XBG Classifier, I got a maximum score of 0.85863 in the public leaderboard whereas in the private leaderboard it comes to be 0.81921--> this score is Under 20 in private leaderboard i.e. top 2%.**\n","a8ad02cd":"### Converting Age to Binning form","5e67ffd9":"# Feature Engineering - Univariate method","17935178":"## **If you like this kernel please do not forget to share with your friends and kindly upvote it... \ud83d\ude0a\u270c\ud83c\udffb**","86c942fc":"# using ANOVA F-value","83b3881d":"# CORRELATION MATRIX","d54e1670":"### Scaling numeric features using MinMaxscalar","812bb670":"# Visualization of our datset"}}