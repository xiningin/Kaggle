{"cell_type":{"621b283c":"code","ebb37a17":"code","34219542":"code","964e2c36":"code","3f362a27":"code","006f02ed":"code","02b87bf8":"code","a61948a4":"code","5629c349":"code","feaaed09":"code","3ccb6599":"code","7f4e261e":"code","847ec9cf":"code","16de5191":"code","e2bd2b68":"code","e948dc57":"code","1b6bd60d":"code","5edff000":"code","8ee27231":"code","724f8250":"code","80cdd335":"code","babc4546":"markdown","ef89de39":"markdown","787e8087":"markdown","743a885d":"markdown","04f5cbc5":"markdown","10a16dcf":"markdown","af52d783":"markdown","cba8d9f9":"markdown","00a90c15":"markdown","92c1af45":"markdown","f286c7f0":"markdown","f239b175":"markdown","96923801":"markdown","dbdff946":"markdown","80cb4857":"markdown","dd34148a":"markdown","ecd726f7":"markdown","e759b16e":"markdown","eca1d5a5":"markdown","f1a4762b":"markdown","77fd6030":"markdown"},"source":{"621b283c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt  # Matlab-style plotting\n%matplotlib inline\nimport seaborn as sns\nimport warnings \nwarnings.filterwarnings('ignore') #to ignore if any warnings takes place during the run time.\n#import statsmodels.api as sm\n","ebb37a17":"#read the data\ndf=pd.read_csv('..\/input\/train.csv')\ndf.head()","34219542":"#check for missing values in train data\n df.isnull().sum()\n #No missing valuues\n\n","964e2c36":"df['date'] = pd.to_datetime(df['date'], format=\"%Y-%m-%d\") #If need extract year, month and day to new columns:\n\n# per 1 store, 1 item\ntrain_df = df[df['store']==1]\ntrain_df = train_df[df['item']==1]\n\n# train_df = train_df.set_index('date')\ntrain_df['year'] = df['date'].dt.year\ntrain_df['month'] = df['date'].dt.month\ntrain_df['day'] = df['date'].dt.dayofyear\ntrain_df['weekday'] = df['date'].dt.weekday\n\ntrain_df.head()","3f362a27":"sns.lineplot(x=\"date\", y=\"sales\",legend = 'full' , data=train_df)","006f02ed":"sns.lineplot(x=\"date\", y=\"sales\",legend = 'full' , data=train_df[:28])","02b87bf8":"sns.boxplot(x=\"weekday\", y=\"sales\", data=train_df)","a61948a4":"train_df = train_df.set_index('date')\ntrain_df['sales'] = train_df['sales'].astype(float)\ntrain_df.head()","5629c349":"\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nresult = seasonal_decompose(train_df['sales'], model='additive', freq=365)\nfig = plt.figure()  \nfig = result.plot()  \nfig.set_size_inches(15, 12)\n","feaaed09":"from statsmodels.tsa.stattools import adfuller\ndef test_stationarity(timeseries, window = 12, cutoff = 0.01):\n    \n  #Determing rolling statistics\n    rolmean = timeseries.rolling(window).mean()\n    rolstd = timeseries.rolling(window).std()\n    \n    fig= plt.figure(figsize=(12,8))\n    orig = plt.plot(timeseries, color='orange',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='blue', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show()\n    \n    \n      #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC', maxlag = 20 )\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    pvalue = dftest[1]\n    if pvalue < cutoff:\n        print('p-value = %.4f. The series is likely stationary.' % pvalue)\n    else:\n        print('p-value = %.4f. The series is likely non-stationary.' % pvalue)\n    \n    print(dfoutput)\n    \n   \n    \n    ","3ccb6599":"test_stationarity(train_df['sales'])","7f4e261e":"#this is for reducing trend and seasonality\nfirst_diff = train_df.sales - train_df.sales.shift(1)\nfirst_diff = first_diff.dropna(inplace = False)\ntest_stationarity(first_diff, window = 12)","847ec9cf":"import statsmodels.api as sm\n\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(train_df.sales, lags=40, ax=ax1)  \nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(train_df.sales, lags=40, ax=ax2)      #lags=40","16de5191":"fig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(first_diff, lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(first_diff, lags=40, ax=ax2)\n\n# Here we can see the acf and pacf both has a recurring pattern every 7 periods. Indicating a weekly pattern exists. \n# Any time you see a regular pattern like that in one of these plots, you should suspect that there is some sort of \n# significant seasonal thing going on. Then we should start to consider SARIMA to take seasonality into accuont","e2bd2b68":"arima_mod6 = sm.tsa.ARIMA(train_df.sales, (6,1,0)).fit(disp=False)\nprint(arima_mod6.summary())","e948dc57":"from scipy import stats\nfrom scipy.stats import normaltest\n\nresid = arima_mod6.resid\nprint(normaltest(resid))\n# returns a 2-tuple of the chi-squared statistic, and the associated p-value. the p-value is very small, meaning\n# the residual is not a normal distribution\n\nfig = plt.figure(figsize=(12,8))\nax0 = fig.add_subplot(111)\n\nsns.distplot(resid ,fit = stats.norm, ax = ax0) # need to import scipy.stats\n\n# Get the fitted parameters used by the function\n(mu, sigma) = stats.norm.fit(resid)\n\n#Now plot the distribution using \nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\nplt.ylabel('Frequency')\nplt.title('Residual distribution')\n\n\n# ACF and PACF\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(arima_mod6.resid, lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(arima_mod6.resid, lags=40, ax=ax2)","1b6bd60d":"sarima_mod6 = sm.tsa.statespace.SARIMAX(train_df.sales, trend='n', order=(6,1,0)).fit()\nprint(sarima_mod6.summary())","5edff000":"resid = sarima_mod6.resid\nprint(normaltest(resid))\n\nfig = plt.figure(figsize=(12,8))\nax0 = fig.add_subplot(111)\n\nsns.distplot(resid ,fit = stats.norm, ax = ax0) # need to import scipy.stats\n\n# Get the fitted parameters used by the function\n(mu, sigma) = stats.norm.fit(resid)\n\n#Now plot the distribution using \nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\nplt.ylabel('Frequency')\nplt.title('Residual distribution')\n\n\n# ACF and PACF\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(arima_mod6.resid, lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(arima_mod6.resid, lags=40, ax=ax2)","8ee27231":"start_index = 1730\nend_index = 1826\ntrain_df['forecast'] = sarima_mod6.predict(start = start_index, end= end_index, dynamic= True)  \ntrain_df[start_index:end_index][['sales', 'forecast']].plot(figsize=(12, 8))","724f8250":"def smape_kun(y_true, y_pred):\n    mape = np.mean(abs((y_true-y_pred)\/y_true))*100\n    smape = np.mean((np.abs(y_pred - y_true) * 200\/ (np.abs(y_pred) + np.abs(y_true))).fillna(0))\n    print('MAPE: %.2f %% \\nSMAPE: %.2f'% (mape,smape), \"%\")","80cdd335":"smape_kun(train_df[1730:1825]['sales'],train_df[1730:1825]['forecast'])","babc4546":"**Time series decomposition**\n\nThink of the time series ytyt as consisting of three components: a seasonal component, a trend-cycle component (containing both trend and cycle), and a remainder component (containing anything else in the time series).\n1. Additive model\n1. Multiplicative model\n\nThe additive model is most appropriate if the magnitude of the seasonal fluctuations or the variation around the trend-cycle does not vary with the level of the time series.\n\nWhen the variation in the seasonal pattern, or the variation around the trend-cycle, appears to be proportional to the level of the time series, then a multiplicative model is more appropriate.\n\n**play this quiz you will come familiar with additive or multiplicative** https:\/\/kourentzes.com\/forecasting\/2014\/11\/09\/additive-and-multiplicative-seasonality\/ \n\n\n**Should I use an additive model or a multiplicative model?**\n\nChoose the multiplicative model when the magnitude of the seasonal pattern in the data depends on the magnitude of the data. In other words, the magnitude of the seasonal pattern increases as the data values increase, and decreases as the data values decrease.\nChoose the additive model when the magnitude of the seasonal pattern in the data does not depend on the magnitude of the data. In other words, the magnitude of the seasonal pattern does not change as the series goes up or down.\nIf the pattern in the data is not very obvious, and you have trouble choosing between the additive and multiplicative procedures, you can try both and choose the one with smaller accuracy measures.\n\n","ef89de39":"******","787e8087":"By seeing the above plots, there are lots of significant plots so as previously i explained if more lines crossed the blue line, than the model will get complex so go for the first difference. ","743a885d":"**ACF (Auto Corelation Function) and (Partial Auto Corelation Function)**\n\n **What is ACF ?**\n \n For Instance today stock price we predicted based on yesterday stock price the ACF will tell how much strongly they are corelated, If todays value is depended on day before yesterday than ACF will tell how strong they are and how many days required to predict the todays value.\n \n **What is PACF?**\n \n If we want to calculate the corelation between today and yesterday we have to take the corelation of day before yesterday because todays value depends upon the yesterday time spot. So this is the reason we use PACF.\n\n  ** PACF- AR model**\n   \n   **ACF- MA model**\n   \n![image.png](attachment:image.png)\n\nby the above images we can observe that, the lines which crosses the blue dotted lines in PACF and ACF those lines are considered to be that many days are required to predict the todays value. **For example in above PACF plot that has only three lines which crossed the blue dotted lines so last three days values are required to predict the todays value**, similarly ACF plot also but for model we should not consider ACF-MA model because many lines crossed the blue threshold line, so it will create the model complex. So we should select only PACF-AR model to predict\n\n   ***if you want to know more about PACF and ACF go through this link***     https:\/\/www.youtube.com\/watch?v=5Q5p6eVM7zM","04f5cbc5":"**Make prediction and evaluation**\n\nTake the last 30 days in training set as validation data\n\n","10a16dcf":"Here for better understanding of the data, We can eloborate as month and weekday wise.","af52d783":"There are two ways you can check the stationarity of a time series. The first is by looking at the data. By visualizing the data it should be easy to identify a changing mean or variation in the data. For a more accurate assessment there is the **Dickey-Fuller test**. I won\u2019t go into the specifics of this test, but if the **\u2018Test Statistic\u2019 is greater than the \u2018Critical Value\u2019 than the time series is stationary**. Below is code that will help you visualize the time series and test for stationarity.\n","cba8d9f9":"If a time series is stationary and has a particular behaviour over a given time interval, then it is safe to assume that it will have same behaviour at some later point in time. Most statistical modelling methods assume or require the time series to be stationary.\n\n**What is variance?  **\n\nVariance is a measurement of the spread between numbers in a data set. The variance measures how far each number in the set is from the mean. The square root of variance is the standard deviation.\n\n","00a90c15":"Although the graph looks very like a normal distribution. But it failed the test. Also we see a recurring correlation exists in both ACF and PACF. So we need to deal with seasonality.\n\nWhen the plots of ACF and PACF are similar or any sesaonality is present between them than we need to apply **SARIMA** model, which it is extended model of **ARIMA**","92c1af45":"**Conclusion**\n\nThe study concludes with some case studies why specific machine learning methods perform so poorly in practice, given their impressive performance in other areas of artificial intelligence.\nThe challenge leaves it open to evaluate reasons of poor performance for ARIMA\/SARIMA and LSTM models, and devise mechanisms to improve model\u2019s poor performance and accuracy. Some of the areas of application of the models and their performance is listed below:\n\nARIMA yields better results in forecasting short term, whereas LSTM yields better results for long term modeling.\nTraditional time series forecasting methods (ARIMA) focus on univariate data with linear relationships and fixed and manually-diagnosed temporal dependence.\nMachine learning problems with substantial dataset, its found that the average reduction in error rates obtained by LSTM is between 84\u201387 percent when compared to ARIMA indicating the superiority of LSTM to ARIMA.\n\nThe number of training times, known as \u201cepoch\u201d in deep learning, has no effect on the performance of the trained forecast model and it exhibits a truly random behavior.\n\nLSTMs when compared to simpler NNs like RNN and MLP appear to be more suited at fitting or overfitting the training dataset rather than forecasting it.\n\nNeural networks (LSTMs and other deep learning methods) with huge datasets offer ways to divide it into several smaller batches and train the network in multiple stages. \nThe batch size\/each chunk size refers to the total number of training data used. The term iteration is used to represent number of batches needed to complete training a model using the entire dataset.\n\nLSTM is undoubtedly more complicated and difficult to train and in most cases do not exceed the performance of a simple ARIMA model.\n\n\n\n","f286c7f0":"**What is SARIMA and what is the use of it ?**\n\nARIMA, is one of the most widely used forecasting methods for univariate time series data forecasting, but it does not support time series with a seasonal component. The ARIMA model is extended (SARIMA) to support the seasonal component of the series. SARIMA (Seasonal Autoregressive Integrated Moving Average), method for time series forecasting is used on univariate data containing trends and seasonality. SARIMA is composed of trend and seasonal elements of the series.\n\nSome of the parameters that are same as ARIMA model are:\np: Trend autoregression order.\nd: Trend difference order.\nq: Trend moving average order\nThere are four seasonal elements that are not part of ARIMA are:\nP: Seasonal autoregressive order.\nD: Seasonal difference order.\nQ: Seasonal moving average order.\nm: The number of time steps for a single seasonal period.\nThus SARIMA model can be specified as:\nSARIMA (p, d, q) (P,D,Q) m\n\nIf m is 12, it specifies monthly data suggests a yearly seasonal cycle.\nSARIMA time series models can also be combined with spatial and event based models to yield ensemble models that solves multi-dimensional ML problems. Such a ML model can be designed to predict cell load in cellular networks at different times of the day round the year as illustrated below in the sample figure\nAutocorrelation, trend, and seasonality (weekday , weekend effects) from time series analysis can be used to interpret temporal influence.\nRegional and cell wise load distribution can be used to predict sparse and over loaded cells in varying intervals of time.\nEvents (holidays, special mass gatherings and others) can be predicted using decision trees.\n\n**Reference :**\n\nhttps:\/\/towardsdatascience.com\/arima-sarima-vs-lstm-with-ensemble-learning-insights-for-time-series-data-509a5d87f20a\n\n","f239b175":"Import the packages","96923801":"Below plots are for checking the seasonality, trends and outliers.","dbdff946":"\n**How to determin p, d, q**\n\nIt's easy to determin I. In our case, we see the first order differencing make the **ts stationary. I = 1.**\n\nIn our case, it's clearly that within 6 lags the AR is significant. Which means, we can use **AR = 6 (6 lines are crossed the blue lines so 6past days are required to predict)**\n\nTo avoid the potential for incorrectly specifying the MA order (in the case where the MA is first tried then the MA order is being set to 0), it may often make sense to extend the lag observed from the last significant term in the PACF.\n\nWhat is interesting is that when the AR model is appropriately specified, the the residuals from this model can be used to directly observe the uncorrelated error. This residual can be used to further investigate alternative MA and ARMA model specifications directly by regression.\n\nAssuming an AR(s) model were computed, then I would suggest that the next step in identification is to estimate an MA model with s-1 lags in the uncorrelated errors derived from the regression. The parsimonious MA specification might be considered and this might be compared with a more parsimonious AR specification. Then ARMA models might also be analysed.","80cb4857":"**    ARIMA MODEL  **\n                                                            \nI was recently tasked with creating a monthly forecast for the next year for the sales of a product. In my research to learn about time series analysis and forecasting, I came across three sites that helped me to understand time series modeling, as well as how to create a model.\n\nFrom my research, I realized I needed to create a seasonal ARIMA model to forecast the sales. I was able to piece together how to do this from the sites, but none of them gave a full example of how to run a **Seasonal ARIMA model in Python**. So this is a quick tutorial showing that process.\n\n** I hope you find this kernel helpful and some UPVOTES would be very much appreciated\n**\n","dd34148a":"**Evaluations of the model**","ecd726f7":"**How to find whether our data is stationary or not ?**\n\nthe smaller p-value, the more likely it's stationary. Here our p-value is 0.036. It's actually not bad, if we use a 5% Critical Value(CV), this series would be considered stationary. But as we just visually found an upward trend, we want to be more strict, we use 1% CV.\nTo get a stationary data, there's many techiniques. We can use log, differencing etc...\n\n**NOTE**\n\nIf the **p-value** is less than 5%(significance level) or If the **Test Static** value is greater than than the **Critical value** than our data is stationary","e759b16e":"First Need to know about 2 things. \n\n1)\t**Trend:** change in the graph upward or downward with respect to time period. \n\n2)\t**Seasonality:** for instance:  sudden increase in the cost of product like pizza, ice creams etc,.\n\nIn order to apply ARIMA the data need to be stationary, In the sense the data should not be changed with respect to time.\n\n![image.png](attachment:image.png)\n\n\n\n\n","eca1d5a5":"**How to check Whether data is Stationary or not ?**\n","f1a4762b":" The yearly pattern is very obvious. and also we can see a upwards trend. Which means this data is not stationary.\n\n","77fd6030":"**Analyze the result**\n\nTo see how our first model perform, we can plot the residual distribution. See if it's normal dist. And the ACF and PACF. For a good model, we want to see the residual is normal distribution. And ACF, PACF has not significant terms.\n\n"}}