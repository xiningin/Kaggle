{"cell_type":{"bd2fcf2a":"code","14a3b48c":"code","db4660af":"code","f5198c73":"code","062d7b45":"code","83261b4f":"code","399c3b8b":"code","c5e01e07":"code","8eaea5cd":"code","c9eb90d8":"code","d261d823":"code","a745e557":"code","eb099685":"code","0f66525b":"code","7c5b8195":"code","a532a834":"code","84eb6efc":"code","f26da0f8":"code","131deb09":"code","2dbd561c":"code","881df4e3":"code","27e0177f":"code","251d3aca":"code","9fda3793":"code","1228deba":"code","6a064bb9":"code","178340e9":"code","eacf8486":"code","be7d607d":"code","7895f8b4":"code","676b9c78":"markdown","81c0313b":"markdown","d6c09f2f":"markdown","76fc4aac":"markdown","3f6c7db0":"markdown","aee4ab04":"markdown","cd0a4935":"markdown","8eaee976":"markdown","e8d3abd0":"markdown","209662e2":"markdown","873a9b6b":"markdown","e254509f":"markdown","28702bb9":"markdown"},"source":{"bd2fcf2a":"! pip install dataprep | grep -v 'already satisfied'","14a3b48c":"%env WANDB_DISABLED=True","db4660af":"import pandas as pd\nimport numpy as np\nnp.set_printoptions(precision=4)\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom dataprep.eda import plot, plot_diff, plot_correlation, create_report\nfrom dataprep.clean import clean_text\n\n# Warning\nimport warnings\nwarnings.filterwarnings('ignore')","f5198c73":"# Random seeds\nimport random\nimport numpy as np\nimport tensorflow as tf\nrandom.seed(319)\nnp.random.seed(319)\ntf.random.set_seed(319)","062d7b45":"train_full = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_full = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\nprint('Training Set Shape = {}'.format(train_full.shape))\nprint('Training Set Memory Usage = {:.2f}MB'.format(train_full.memory_usage().sum()\/2**20))\n\nprint('Test Set Shape = {}'.format(test_full.shape))\nprint('Test Set Memory Usage = {:.2f}MB'.format(test_full.memory_usage().sum()\/2**20))","83261b4f":"plot(train_full)","399c3b8b":"create_report(train_full)","c5e01e07":"plot(train_full, 'text')","8eaea5cd":"train_full.text","c9eb90d8":"plot(train_full, \"text\", \"target\")","d261d823":"df1 = train_full.text[train_full.target == 0]\ndf2 = train_full.text[train_full.target == 1]\nplot_diff([df1, df2])","a745e557":"# Read commited-dataset\ndf_train = pd.read_csv(\"\/kaggle\/input\/disastertweet-prepared2\/train_prepared.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/disastertweet-prepared2\/test_prepared.csv\")","eb099685":"df_train","0f66525b":"from datasets import Dataset\nfrom transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, default_data_collator, Trainer","7c5b8195":"tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')","a532a834":"model = BertForSequenceClassification.from_pretrained('bert-large-uncased')","84eb6efc":"type(df_train)","f26da0f8":"max_len = 0\n# Find the longest sentence \nfor sentence in pd.concat([df_train.text, df_test.text]):\n    if len(sentence) > max_len: # number of word in a sentence tokenizer is greater max_len\n        max_len = len(sentence)\nmax_len","131deb09":"train = pd.DataFrame()\ntrain['input_ids'] = df_train['text'].apply(lambda x: tokenizer(x, max_length=max_len, padding=\"max_length\",)['input_ids'])\ntrain['attention_mask'] = df_train['text'].apply(lambda x: tokenizer(x, max_length=max_len, padding=\"max_length\",)['attention_mask'])","2dbd561c":"train['labels'] = df_train['target'] ","881df4e3":"train_ds = Dataset.from_pandas(train[:-64].reset_index(drop=True))\nvalid_ds = Dataset.from_pandas(train[-64:].reset_index(drop=True))","27e0177f":"train_ds","251d3aca":"batch_size = 16\n\nargs = TrainingArguments(\n    'nlp-getting-started',\n    evaluation_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate=3e-5,\n    gradient_accumulation_steps=8,\n    num_train_epochs=3,\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n)\n\ndata_collator = default_data_collator\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_ds,\n    eval_dataset=valid_ds,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)","9fda3793":"from transformers import set_seed\nset_seed(42)","1228deba":"trainer.train()","6a064bb9":"test = pd.DataFrame()\ntest['input_ids'] = df_test['text'].apply(lambda x: tokenizer(x, max_length=max_len, padding=\"max_length\",)['input_ids'])\ntest['attention_mask'] = df_test['text'].apply(lambda x: tokenizer(x, max_length=max_len, padding=\"max_length\",)['attention_mask'])","178340e9":"test_ds = Dataset.from_pandas(test)","eacf8486":"test_ds","be7d607d":"sub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\noutputs =  trainer.predict(test_ds)\nsub['target'] = outputs.predictions.argmax(1)\nsub.to_csv('submission.csv', index=False)","7895f8b4":"pd.read_csv('submission.csv')","676b9c78":"# Classifier Model","81c0313b":"<a id=7 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">7. References<\/p>\n\n[Content](#0)","d6c09f2f":"<a id=6 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">6. Make a Submission<\/p>\n\n[Content](#0)","76fc4aac":"<a id=0><\/a>\n## <p style=\"background-color:lightblue; font-family:newtimeroman; font-size:120%; text-align:left; border-radius: 15px 50px;\">Table of Content<\/p>\n* [0. Introduction and updates](#0)\n* [1. Loading Data \ud83d\udc8e](#1)\n* [2. EDA \ud83d\udcca](#2)\n* [3. Data Preprocessing](#3)\n* [6. Make a Submission](#6)\n* [7. References](#7)","3f6c7db0":"### Dataset is balanced","aee4ab04":"<a id=0><\/a>\n<font size=\"+3\" color=\"#5bc0de\"><b>Introduction <\/b><\/font><br>\n[Content](#0)\n\nBeside the general steps working with text data as EDA, preprocessing.. of this data that be described in [kernel](https:\/\/www.kaggle.com\/phanttan\/disastertweet-prepareddata). The workflow in Modelling can divided into 2 main stages:\n1. Defining a Model Architecture with concatenation a keyword column into BERT model\n2. Training Classification Layer Weights.\n\n<a id=1.2 ><\/a>\n<font size=\"+3\" color=\"#5bc0de\"><b>1.2. Update via Versions <\/b><\/font><br>\n[Content](#0)\n\n### Current Version\n* Adding 1 hidden layer in Model to incease accuracy.\n\n\n[Content](#0)","cd0a4935":"# If you like this kernel, please upvote and tell me your thought. Thank you @@","8eaee976":"[Hugging Face Transformers Fine-Tunning DistilBert for Binary Classification Tasks](https:\/\/towardsdatascience.com\/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379)\n\n[Keras functional API](https:\/\/keras.io\/guides\/functional_api\/)\n\n[Distil Bert](https:\/\/huggingface.co\/transformers\/model_doc\/distilbert.html)\n\n[Tensorflow Dataset](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset)\n\n[BERT in TFHub](https:\/\/tfhub.dev\/google\/collections\/bert)\n\n[TensorFlow NLP Modelling Toolkit](https:\/\/github.com\/tensorflow\/models\/tree\/master\/official\/nlp)\n\n[NLP With BERT from Tendorflow](https:\/\/www.tensorflow.org\/text\/tutorials\/fine_tune_bert)\n\n[NLP Optimization](https:\/\/github.com\/tensorflow\/models\/blob\/master\/official\/nlp\/optimization.py)","e8d3abd0":"<a id='1'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">1. Loading Data \ud83d\udc8e<\/p>\n\nJust load the dataset and global variables for colors and so on.\n\n[Content](#0)","209662e2":"<a id='2'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. EDA \ud83d\udcca<\/p>\n\n\n[Content](#0)","873a9b6b":"# Main technics I used in this data\n    * [3.1] Remove 157 duplicated rows\n    * [3.2] Cleaning text\n    * [3.3] Spelling Checker\n    * [3.4] Remove Stemming\n #### Step 3.3 spends a lot time (around 4000s in 4536s in total). \n #### So, I splits Data Preprocessing into [another kernel](https:\/\/www.kaggle.com\/phanttan\/disastertweet-prepareddata). \n #### And the prepared data to save in to [new dataset](https:\/\/www.kaggle.com\/phanttan\/disastertweet-prepared2)\n #### I am so appreciate to you for using\/upvoting it.\n","e254509f":"<a id='3'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">3. Data Pre-processing <\/p>\n\nNow we are going to engineering the data to make it easier for the model to clasiffy.\n\nThis section is very important to reduce the dimensions of the problem.\n\n\n\n\n[Content](#0)","28702bb9":"### Range from 120 to 140 characters is the most common in tweet."}}