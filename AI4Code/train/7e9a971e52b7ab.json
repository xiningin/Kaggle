{"cell_type":{"9bf11d74":"code","696fbc82":"code","1acb1865":"code","e4e81b2a":"code","c8efb4be":"code","6fc16a03":"code","d86bea31":"code","e1d3f7c3":"code","79960624":"code","df9c26d7":"code","12e05964":"code","53c66ba2":"code","59303469":"code","8856f1f7":"code","287b09c3":"code","e8746dcc":"code","88ab9883":"code","7b231653":"code","ccbf9db9":"code","4f6b2097":"code","267bb931":"code","14b65d10":"code","4db9bf52":"code","ce8c5273":"code","033aa7df":"code","54167786":"code","d5e93186":"code","838d11dc":"code","95e9642f":"code","3a491b18":"code","d2fcd66c":"code","9619b888":"code","e76a83ae":"code","3edf5a88":"code","f1013ff9":"code","a53c0910":"code","18fa3423":"code","2efba150":"code","0aab5629":"code","2bad8858":"code","c0d3f9ce":"code","d73d45c0":"code","e5bb5f23":"markdown","eeb59ce0":"markdown","dfa08f2c":"markdown","1c630c80":"markdown","d3bdcb80":"markdown","bc5e8555":"markdown","86308e84":"markdown","0c6d8afb":"markdown","fb0b18ae":"markdown","e7177dcc":"markdown","88cd7161":"markdown","97385d93":"markdown","796fbaad":"markdown","1fb75f8c":"markdown","dd23836b":"markdown","b8ab077f":"markdown","81f1ef30":"markdown","6f96130f":"markdown","5dc92f03":"markdown","554e2cd5":"markdown","dbf4a945":"markdown","64df3824":"markdown","f5ee121d":"markdown","6ecd23fd":"markdown","978f1e7e":"markdown"},"source":{"9bf11d74":"#Supporting libs\n%matplotlib notebook\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nsns.set()","696fbc82":"#Load both trainand test datasets\ntrain_data= pd.read_csv('..\/input\/train.csv')\ntest_data=pd.read_csv('..\/input\/test.csv')","1acb1865":"#Add one column names 'dataType' to mark data types\ntrain_data['dataType']='train'\ntest_data['dataType']='test'","e4e81b2a":"#Combine train and test dataset to do feature engineering all together\nall_data=pd.concat([train_data,test_data],axis=0)\nall_data.set_index('PassengerId',inplace=True)\nall_data.info()","c8efb4be":"# Get a summary of missing value for all fields\nall_data.isnull().sum()","6fc16a03":"#Use 'U' to fill missing value in Cabin\nall_data['Cabin'].fillna('U',inplace =True)\n#Using mean value to fill missing value in Fare\nall_data['Fare'].fillna(all_data['Fare'].mean(),inplace =True)","d86bea31":"all_data['Embarked'].value_counts()","e1d3f7c3":"all_data['Embarked'].fillna('S',inplace = True)","79960624":"#Process Age, while there are 263 missing value, this should impact the the probility of survival\n#Let's use histogram to check how the Age distributed \nall_data['Age'].hist()","df9c26d7":"avg_age=all_data['Age'].mean()\nstd_age=all_data['Age'].std()\nno_nan=all_data['Age'].isnull().sum()\nrand=np.random.randint(avg_age-std_age,avg_age+std_age,size=no_nan)\nall_data['Age'][all_data.Age.isnull()]=rand\nall_data['Age'].hist()","12e05964":"all_data.info()","53c66ba2":"all_data['Pclass']=all_data['Pclass'].astype(str)","59303469":"tobe_dummied_cols = ['Pclass', 'Sex', 'Cabin', 'Embarked']\nobj_df = all_data[tobe_dummied_cols]\nobj_df_dummy = pd.get_dummies(obj_df)","8856f1f7":"obj_df_dummy.shape","287b09c3":"#Procced column Name, transform it into title\ntitles = set()\nfor name in all_data['Name']:\n    titles.add(name.split(',')[1].split('.')[0].strip())\n\ndef status(feature):\n    print('Processing', feature, ': Done')\n    \nTitle_Dictionary = {\n    \"Capt\": \"Officer\",\n    \"Col\": \"Officer\",\n    \"Major\": \"Officer\",\n    \"Jonkheer\": \"Royalty\",\n    \"Don\": \"Royalty\",\n    \"Sir\" : \"Royalty\",\n    \"Dr\": \"Officer\",\n    \"Rev\": \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Mme\": \"Mrs\",\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\",\n    \"Mr\" : \"Mr\",\n    \"Mrs\" : \"Mrs\",\n    \"Miss\" : \"Miss\",\n    \"Master\" : \"Master\",\n    \"Lady\" : \"Royalty\"\n}\n\n# Extract the title from each name, and map names to titles\ndef get_titles(dataset):\n    dataset['Title'] = dataset['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n    dataset['Title'] = dataset.Title.map(Title_Dictionary)\n    status('Title')\n    return dataset","e8746dcc":"all_data = get_titles(all_data)","88ab9883":"all_data['Family']=all_data['Parch']+all_data['SibSp']\nall_data.drop(['Parch','SibSp'],inplace = True, axis=1)","7b231653":"all_data.head()","ccbf9db9":"all_data.drop(['Cabin','Embarked','Name','Pclass','Sex','Ticket'],inplace=True,axis=1)","4f6b2097":"dummy_title=pd.get_dummies(all_data['Title'],prefix='Title')\nall_data.drop('Title',inplace=True,axis=1)","267bb931":"all_data=pd.concat((all_data,dummy_title,obj_df_dummy),axis=1)","14b65d10":"from sklearn.preprocessing import MinMaxScaler\nscaler= MinMaxScaler()\nall_data['Age']=scaler.fit_transform(all_data.filter(['Age']))\nall_data['Fare']=scaler.fit_transform(all_data.filter(['Fare']))","4db9bf52":"# all_data['Family']=scaler.fit_transform(all_data.filter(['Family']))","ce8c5273":"all_data.head()","033aa7df":"from sklearn.decomposition import PCA","54167786":"new_train_data=all_data[all_data['dataType']=='train']\nnew_test_data=all_data[all_data['dataType']=='test']","d5e93186":"new_train_data.drop(['dataType','Survived'],inplace = True,axis=1)\nnew_test_data.drop(['dataType','Survived'],inplace = True,axis=1)","838d11dc":"x_train_reduced = PCA(n_components=0.98).fit_transform(new_train_data)\nx_test_reduced = PCA(n_components=66).fit_transform(new_test_data)","95e9642f":"x_test_reduced.shape","3a491b18":"y_label = train_data['Survived']","d2fcd66c":"y_label.shape","9619b888":"import tensorflow as tf","e76a83ae":"optimizer = tf.keras.optimizers.Adam(0.0001)\nloss_function = \"sparse_categorical_crossentropy\"","3edf5a88":"model = tf.keras.Sequential([\n    tf.keras.layers.Dense(units=64, input_dim=66,\n                          activation='relu'),  #input dims = number of fields\n    tf.keras.layers.Dense(units=32, activation=\"relu\"),\n    tf.keras.layers.Dense(units=2, activation='softmax')\n])\nmodel.compile(optimizer = optimizer, loss = loss_function, metrics=['accuracy'])","f1013ff9":"model.summary()","a53c0910":"history=model.fit(x_train_reduced,y_label,epochs=170,validation_split = 0.2)","18fa3423":"def v_train_history(trainhist, train_metrics, valid_metrics):\n    plt.plot(trainhist.history[train_metrics])\n    plt.plot(trainhist.history[valid_metrics])\n    plt.title('Training metrics')\n    plt.ylabel(train_metrics)\n    plt.xlabel('Epochs')\n    plt.legend(['train','validation'],loc='upper left')\n    plt.show()","2efba150":"v_train_history(history,'loss','val_loss')","0aab5629":"v_train_history(history,'acc','val_acc')","2bad8858":"x_test_reduced.shape","c0d3f9ce":"y_pred=model.predict_classes(x_test_reduced)","d73d45c0":"pred_survied_pd = pd.DataFrame(y_pred,\n                               index=new_test_data.index,\n                               columns=['Survived'])\npred_survied_pd.reset_index()\npred_survied_pd.to_csv('submission.csv')","e5bb5f23":"I tried to set n_components as 10,13, 15, 18 and 20, only with 15, I got the highest score among them","eeb59ce0":"Combine all data with encoded fields","dfa08f2c":"We will do one-hot encoding with Pclass, Sex, Cabin and Embared together","1c630c80":"## Titanic Disaster Survival Prediction with Keras and PCA   \nThis is my first kernal submitted for the playground competition, after trying for more than 17 rounds to get a better score.   \n\nYou may found this kernal is bit different with others' as I did not do much EDA here, because I know you have read enough and got how to do the EDA for this dataset, but may be seeking a solution to reach a better score.  \n\nI can tell you my lowest score is only 0.6, after that I tried find a better way to do the learning and prediction, now I got 0.81 after I applied Keras and PCA together.    \n\nPlease feel free to fork this and vote for me if you like this kernal, that will encourage me to countinue learning and sharing.  \n","d3bdcb80":"Let check how the data set looks like, it seems ready to do learning","bc5e8555":"One-hot encoding the new field Title and drop the Title field in all_data","86308e84":"Process missing values first","0c6d8afb":"Now all missing value filled, moving to process other columns","fb0b18ae":"The reason being I don't remove any field here is because during my previous try outs, I actully used  some ways to find most important features and used them to do the training in RandomForest, XGBoost, and Tensorflow(Keras) as well, but the results were not good, so I decided not to drop any field but use a way to reduced the dimension while keep the most import components within this data sets, so I chose PCA to do this.","e7177dcc":"Change type of Pclass from int64 to object to be prepared for one-hot encoding","88cd7161":"Combine Parch and SibSp to a new column Family","97385d93":"Based on the curve above, the result is good as there's no overfitting, so let's submit it","796fbaad":"Plotting train and validation accuray curve","1fb75f8c":"Check value counts in Embarked, we found most of them are 'S'","dd23836b":"Now, add set 1 input layer with 64 units and 1 hidden layer with 32 units and 2 units in output layer","b8ab077f":"Check the summary of this model","81f1ef30":"Fill missing in Age, but make sure filling new data won't change the original data distribution, so we will use random integer between average value - standard value and average value + standard value","6f96130f":"Because the linear regression is sensitive to the number distribution and scale, we will use MinMaxScaler to scaler Age and Fare","5dc92f03":"So we will use 'S' to fill missing value in Embarked","554e2cd5":"We drop Cabin, Embarked, Name and Pclass, Sex and Tickets","dbf4a945":"Train the model and save training metrics in history","64df3824":"We use Adam with learning rate 0.0001 as optimizer and loss function \"sparse_categorical_crossentropy\" as this will a classsification task","f5ee121d":"Plotting train and validation loss curve","6ecd23fd":"As we found above, there are 263 missing in Age and 1014 missing in Cabin, only 2 missing in Embarked and 1 missing in Fare.","978f1e7e":"Define a function to plot the metrics"}}