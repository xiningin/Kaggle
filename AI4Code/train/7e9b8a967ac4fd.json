{"cell_type":{"a8c97236":"code","3f3df006":"code","1d1e0632":"code","fdc73cfe":"markdown","41358bc3":"markdown","14e4adf2":"markdown"},"source":{"a8c97236":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchtext.data import Field,ReversibleField,TabularDataset,Iterator\n\n","3f3df006":"def tokenize(text):\n    return text.split()\n\ndef proc_float(value):\n    return float(value)\n\ndef proc_int(value):\n    return int(value)\n\nTEXT      = Field(sequential=True, tokenize=tokenize) #might alternatively specify cuda data types to get the dataset to live permanently on the GPU\nFLOAT     = Field(sequential=False, use_vocab=False,dtype=torch.float,preprocessing=proc_float) \nINTEGER   = Field(sequential=False, use_vocab=False,preprocessing=proc_int)\n\ndf         = TabularDataset(\"..\/input\/SICK_train_logistic.txt\",\"tsv\",skip_header=True,\\\n                            fields=[('idx',INTEGER),('sentA',TEXT),('sentB',TEXT),('Relatedness',FLOAT)])\ndf_train,df_dev  = df.split(split_ratio=0.8)\nTEXT.build_vocab(df_train)\n\n#Prints out the first few lines of the dataset\nfor elt in df_dev[:10]: #prints out the ten first examples\n    print(elt.idx,' '.join(elt.sentA),'||',' '.join(elt.sentB),elt.Relatedness)","1d1e0632":"class ParaphraseClassifier(nn.Module):\n    \n    def __init__(self,hidden_dim,embedding_dim):\n        \n        super(ParaphraseClassifier, self).__init__()\n        \n        self.hidden_dim    = hidden_dim\n        self.embedding_dim = embedding_dim\n        self.embedding     = nn.Embedding(len(TEXT.vocab), embedding_dim)\n        self.bilstm        = nn.LSTM(embedding_dim, hidden_dim, num_layers=1,bidirectional=True)\n        self.W             = nn.Linear(hidden_dim*4,1)    \n        \n        \n    def forward(self,xinputA,xinputB):\n        \"\"\"\n        Args: \n            xinputA is a sequence of word indexes\n            xinputB is a sequence of word indexes\n        The forward method also works for batched input.        \n        \"\"\"\n        ##details for dimensionalities \n        #embeddings \n        #  input : batch_size x seq_length\n        #  output: batch-size x seq_length x embedding_dimension\n        #lstm \n        #  input : seq_length x batch_size x embedding_size\n        #  output: seq_length x batch_size x hidden_size  (for the sequence)\n        #  output: batch_size x hidden_size (for the last hidden\/cell state)\n        xembeddedA                       = self.embedding(xinputA)                   #catches embedding vectors\n        lstm_outA, (hiddenA,cellA)       = self.bilstm(xembeddedA.view(len(xinputA), -1, self.embedding_dim), None) #-1 is a wildcard (here we let pytorch guess batch size)\n        \n        xembeddedB                       = self.embedding(xinputB)                   #catches embedding vectors\n        lstm_outB, (hiddenB,cellB)       = self.bilstm(xembeddedB.view(len(xinputB), -1, self.embedding_dim), None)\n        \n        #concat sentence representations\n        hA = hiddenA.view(-1,self.hidden_dim * 2) #-1 is a wildcard (here we let pytorch guess batch size)\n        hB = hiddenB.view(-1,self.hidden_dim * 2) \n        H  = torch.cat((hA,hB),1)\n        return torch.sigmoid(self.W(H)) #sigmoid is the logistic function\n    \n    def train(self,train_set,dev_set,epochs,learning_rate=0.001):\n        \n        loss_func  = nn.BCELoss() \n        optimizer  = optim.Adam(self.parameters(), lr=learning_rate)\n        \n        train_iterator   = Iterator(train_set, batch_size=1, device=-1, sort=False, sort_within_batch=False, repeat=False)\n        \n        for e in range(epochs):\n            idx = 0\n            for batch in train_iterator: \n                xvecA,xvecB,yRelness = batch.sentA,batch.sentB,batch.Relatedness\n                self.zero_grad()\n                prob            = self.forward(xvecA,xvecB).squeeze()\n                loss            = loss_func(prob,yRelness)\n                loss.backward()\n                optimizer.step()\n                print(idx)\n                idx += 1\n                \npc = ParaphraseClassifier(100,100)\npc.train(df_train,df_dev,3)","fdc73cfe":"Lecture des donn\u00e9es\n===================\nPour de plus amples informations, on recommande la consultation de :\nhttps:\/\/mlexplained.com\/2018\/02\/08\/a-comprehensive-tutorial-to-torchtext\/ \net de:\nhttps:\/\/torchtext.readthedocs.io\/en\/latest\/","41358bc3":"Classification\n==============","14e4adf2":"Environnement\n============="}}