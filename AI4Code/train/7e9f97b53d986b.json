{"cell_type":{"7911eec9":"code","2be91df8":"code","7a3830ff":"code","49c32314":"code","e62ef5c1":"code","9dc21eba":"code","4cf6666b":"code","21a47e58":"code","cdb18e71":"code","ecf2811f":"code","5a592ca8":"code","1df4d674":"code","24c81cb5":"code","640f69b2":"code","a8b1a103":"code","50d2aa61":"markdown","d1199307":"markdown","d20a7f67":"markdown","2175a7eb":"markdown","220936ee":"markdown","b47a08d0":"markdown","a87a9966":"markdown","51e21918":"markdown","487826e8":"markdown","ddccafd6":"markdown","b6615ebc":"markdown","bd59d11f":"markdown","f57dc154":"markdown","47a94178":"markdown"},"source":{"7911eec9":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns; sns.set()\nimport gc\n\nfrom sklearn.model_selection import GroupKFold, KFold, TimeSeriesSplit\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM, GRU\nfrom tensorflow.keras.layers import Input, Flatten, Concatenate, BatchNormalization, Embedding\nfrom tensorflow.keras.losses import mse\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM","2be91df8":"DATA_DIR = '\/kaggle\/input\/m5-forecasting-accuracy\/'\n\nDEBUG = False # turning on\/off degugging mode\nCV = False # turning on\/off cross validation\n\nif DEBUG:\n    rows = 100\n    w_size = 15\n    batch_size=32\n    epochs = 3\n    span_lst = [7]\nelse:\n    rows = None\n    w_size = 30 # LSTM window size\n    batch_size=512\n    epochs = 35\n    span_lst = [7, 30, 90] # moving avarage time wiondows","7a3830ff":"d_dtypes = {}\nfor i in range(1914):\n    d_dtypes[f'd_{i}'] = np.int32\n    \nsales = pd.read_csv(DATA_DIR + 'sales_train_validation.csv',\n                    dtype=d_dtypes, nrows=rows)\n\n# categories are used for categorical model input\ncategories = sales[['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']]\nsales['id'] = sales['id'].apply(lambda x: x[:-11])\nids = sales['id'].values\n\nif DEBUG:\n    sales = sales.iloc[:, -150:].T.reset_index()\nelse:\n    # Only last 360 days are used to save run time\n    sales = sales.iloc[:, -360:].T.reset_index()\n    \nsales.columns = ['d'] + list(ids)","49c32314":"calendar = pd.read_csv(DATA_DIR + 'calendar.csv',\n                       dtype={'wm_yr_wk': np.int32, 'wday': np.int32, \n                              'month': np.int32, 'year': np.int32, \n                              'snap_CA': np.int32, 'snap_TX': np.int32,\n                              'snap_WI': np.int32})\n\n# subsetting by starting date in sales\ncalendar = calendar[calendar.d.apply(lambda x: int(x[2:])) >= int(sales.d[0][2:])]","e62ef5c1":"prices = pd.read_csv(DATA_DIR + 'sell_prices.csv',\n                          dtype={'wm_yr_wk': np.int32, \n                                 'sell_price': np.float32})\nprices = prices.loc[prices.wm_yr_wk >= \\\n                    calendar[calendar.d == sales.d[0]]['wm_yr_wk'].values[0]]\n\nprices['id'] = prices.apply(lambda x: x.item_id + '_' + x.store_id, axis=1)\nprices = prices.pivot(index='wm_yr_wk', columns='id', values='sell_price')\n\nprices = calendar[['d','wm_yr_wk']].merge(prices, how='inner', on=['wm_yr_wk'])\nprices.drop('wm_yr_wk', axis=1, inplace=True)\nprices = prices.loc[:, list(sales.columns)]\n\ncalendar.drop(['date','wm_yr_wk', 'weekday', 'd'], axis=1, inplace=True)","9dc21eba":"sales_log = np.log(sales.iloc[:, 1:].values + 1)\nsales_mean = np.mean(sales_log)\nsales_std = np.std(sales_log)\nsales.iloc[:, 1:] = (sales_log - sales_mean) \/ sales_std\n\nprices_log = np.log(prices.iloc[:, 1:].values)\nprices_mean = np.mean(prices_log)\nprices_std = np.std(prices_log)\nprices.iloc[:, 1:] = (prices_log - prices_mean) \/ prices_std\n\nsales.fillna(0, inplace=True)\nprices.fillna(0, inplace=True)","4cf6666b":"cat_ft1 = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\ncat_ft2 = ['wday','month', 'year', 'event_name_1', 'event_type_1',\n           'event_name_2', 'event_type_2']\n\ncategory_counts = {}\nstate_le = None\n\ndef LabelEncoding(df, cat_ft):\n    \n    for col in cat_ft:\n        le = LabelEncoder()\n        df.loc[:, col] = df[col].astype(str)\n        df.loc[:, col] = le.fit_transform(df[col])\n        category_counts[col] = len(list(le.classes_))\n\n    return df\n\ncategories = LabelEncoding(categories, cat_ft1)\ncalendar = LabelEncoding(calendar, cat_ft2)","21a47e58":"def moving_average(a, n):\n    \n    if a.shape[0] >= n:\n        ret = np.cumsum(a, axis=0)\n        ret[n:, :] = ret[n:, :] - ret[:-n, :]\n        ret[:n-1, :] = np.zeros((n-1, ret.shape[1]))\n        return ret \/ n\n    else:\n        return np.zeros((a.shape[0], a.shape[1]))\n    \nclass SequenceGenerator:\n    \n    def __init__(self, inputs, spans=[7], window=30, batch_size=32, infer=False):\n        self.sales = inputs[0]\n        self.prices = inputs[1]\n        self.categories = inputs[2]\n        self.calendar = inputs[3]\n        self.spans = spans\n        self.window = window\n        self.infer = infer\n        self.num_items = self.sales.shape[1]\n        \n        if self.infer:\n            self.batch_size = self.num_items\n            self.num_days = self.sales.shape[0] - self.window + 1\n            self.steps_per_day = 1 \n            self.steps = 1\n        else:\n            self.batch_size = batch_size\n            self.num_days = self.sales.shape[0] - self.window\n            self.steps_per_day = self.num_items \/\/ self.batch_size + 1\n            self.steps = self.steps_per_day * self.num_days\n\n    def generate(self):\n        \n        ## for inference, it starts from the the last starting date (no slides in days)\n        ## for training\/validation, it starts from day 0\n        start_day = self.num_days - 1 if self.infer else 0\n            \n        while True:            \n            \n            for day in range(start_day, self.num_days):\n                    \n                s = self.sales[day:day+self.window, :].reshape(1, self.window, -1)\n                p = self.prices[day:day+self.window, :]\\\n                    .reshape(1, self.window, -1)\n\n                X = np.concatenate((s,p),axis=0)\n\n                for span in self.spans:\n                    \n                    span_ = day if day < span else span\n\n                    ma = moving_average(self.sales[day-span_:day+self.window, :]\n                                        , n=span)[span_:, :]\\\n                        .reshape(1, self.window, -1)\n\n                    X = np.concatenate((X, ma),axis=0)\n\n                ## transposing (features, days, items) into (items, days, features)\n                X = np.transpose(X, (2,1,0)) \n\n                if not self.infer:\n                    y = self.sales[day+self.window, :].reshape(-1,1) \n                    \n                for i in range(self.steps_per_day):\n                    \n                    ## if the batch go over the maxium item number, \n                    ## the batch_size will be truncated\n                    if (i+1)*self.batch_size > self.num_items:\n                        end = self.num_items\n                    else:\n                        end = (i+1)*self.batch_size\n                    \n                    ## categories has (items, features) shape\n                    ## only relevant item rows are fetched\n                    cat = self.categories[i*self.batch_size:end, :]\n                    state_id = cat[:, -1]\n                    # reshaping into (features, items, 1)\n                    cat = cat.T.reshape(cat.shape[1], cat.shape[0], 1)\n                    \n                    ## calender values are taken at prediction target date\n                    calen = self.calendar[day+self.window,:7].reshape(1,-1)\n                    calen = np.repeat(calen, end-i*self.batch_size, axis=0)\n                    calen = calen.T.reshape(calen.shape[1], calen.shape[0], 1)\n                    \n                    ## snap values are taken at prediction target date\n                    snap = self.calendar[day+self.window,7:].reshape(1,-1)\n                    snap = np.repeat(snap, end-i*self.batch_size, axis=0)\n                    # taking only relevant state's snap values for each row\n                    snap = snap[np.arange(len(snap)), state_id].reshape(-1,1)\n                    \n                    if self.infer:\n                        yield [X[i*self.batch_size: end]] + [j for j in cat]\\\n                                + [j for j in calen] + [snap]\n                    else:\n                        yield [X[i*self.batch_size: end]] + [j for j in cat] \\\n                              + [j for j in calen] + [snap],\\\n                              y[i*self.batch_size: end]","cdb18e71":"def define_model(lstm_w_size, lstm_n_fts):\n    \n    ## Categorical embedding\n    cat_inputs = []\n    for cat in cat_ft1+cat_ft2:\n        cat_inputs.append(Input(shape=[1], name=cat))\n        \n    cat_embeddings = []\n    for i, cat in enumerate(cat_ft1+cat_ft2):\n        cat_embeddings.append(Embedding(category_counts[cat], \n                                        min(50, int(category_counts[cat]+1\/ 2)), \n                                        name = cat + \"_embed\")(cat_inputs[i]))\n\n    cat_output = Concatenate()([Flatten()(cat_emb) \\\n                                          for cat_emb in cat_embeddings])\n    cat_output = Dropout(.7)(cat_output)\n    \n    # snap input\n    snap_input = Input(shape=[1])\n\n    ## LSTM\n    lstm_input = Input(shape=(lstm_w_size, lstm_n_fts))\n    lstm_output = CuDNNLSTM(32)(lstm_input)\n    \n    concat = Concatenate()([\n        lstm_output,\n        cat_output,\n        snap_input\n    ])\n        \n    dense_output = Dense(10, activation='relu')(concat)\n    out = Dense(1)(dense_output)\n    model = Model(inputs=[lstm_input] + cat_inputs + [snap_input],\n                  outputs=out)\n\n    model.compile(optimizer='adam', loss='mse')\n    \n    return model","ecf2811f":"def model_training(inputs, cv, w_size=30, batch_size=32, epochs=10,\n                   early_stopping=10, plt_iter=True):\n\n    val_scores=[]\n    train_evals=[]\n    valid_evals=[]\n    best_epoch=[]\n\n    for idx, (train_index, val_index) in enumerate(cv.split(inputs[0])):\n        \n        if idx >= 2: # skipping the first 2 fold to save run time\n\n            #print(\"###### fold %d ######\" % (idx+1))\n            sales_train, sales_val = inputs[0][train_index, :],\\\n                                     inputs[0][val_index, :]\n            prices_train, prices_val = inputs[1][train_index, :],\\\n                                       inputs[1][val_index, :]\n            calendar_train, calendar_val = inputs[3][train_index, :],\\\n                                           inputs[3][val_index, :]\n            inputs_train = [sales_train, prices_train, inputs[2], calendar_train]\n            inputs_val = [sales_val, prices_val, inputs[2], calendar_val]\n\n            train_gen = SequenceGenerator(inputs_train, spans=span_lst,\n                                          window=w_size, batch_size=batch_size)\n            val_gen = SequenceGenerator(inputs_val, spans=span_lst, window=w_size,\n                                        batch_size=batch_size)\n\n            model = define_model(w_size, 2+len(span_lst))\n            early_stop = EarlyStopping(patience=early_stopping,\n                                       verbose=True,\n                                       restore_best_weights=True)\n\n            hist = model.fit_generator(train_gen.generate(),\n                      validation_data=val_gen.generate(),\n                      epochs=epochs,\n                      steps_per_epoch=train_gen.steps, \n                      validation_steps=val_gen.steps, \n                      callbacks=[early_stop],\n                      verbose=0)\n\n            val_scores.append(np.min(hist.history['val_loss']))\n            train_evals.append(hist.history['loss'])\n            valid_evals.append(hist.history['val_loss'])\n\n            best_epoch.append(np.argmin(hist.history['val_loss']) + 1)\n    \n    print('### CV scores by fold ###')\n    for i in range(2, cv.get_n_splits(sales)):\n        print(f'fold {i+1}: {val_scores[i-2]:.4f} at epoch {best_epoch[i-2]}')\n    print('CV mean score: {0:.4f}, std: {1:.4f}'\\\n          .format(np.mean(val_scores), np.std(val_scores)))\n    \n    if plt_iter:\n        \n        fig, axs = plt.subplots(1, 2, figsize=(11,4))\n        \n        for i, ax in enumerate(axs.flatten()):\n            if i < cv.get_n_splits(sales):\n                ax.plot(train_evals[i], label='training')\n                ax.plot(valid_evals[i], label='validation')\n                ax.set(xlabel='epoch', ylabel='loss')\n                ax.set_title(f'fold {i+1+2}', fontsize=12)\n                ax.legend(loc='upper right', prop={'size': 9})\n                          \n        fig.tight_layout()\n        plt.show()\n\n    return best_epoch","5a592ca8":"# %%time\n\nsales = sales.iloc[:,1:].values\nprices = prices.iloc[:,1:].values\ncategories = categories.values\ncalendar = calendar.values\ninputs = [sales, prices, categories, calendar]\n\nif CV:\n    cv = TimeSeriesSplit(n_splits=4)\n    best_epoch = model_training(inputs, cv, w_size=w_size, \n                                batch_size=batch_size, \n                                epochs=epochs, early_stopping=5, \n                                plt_iter=True)","1df4d674":"%%time\n\ntrain_gen = SequenceGenerator(inputs, spans=span_lst, window=w_size,\n                              batch_size=batch_size)\nmodel = define_model(w_size, 2+len(span_lst))\nhist = model.fit_generator(train_gen.generate(),\n                           epochs=best_epoch[-1] if CV else epochs,\n                           steps_per_epoch=train_gen.steps,\n                           verbose=0)","24c81cb5":"# subsetting len-w_size-90: as we need the first 90 days \n# prior to the LSTM window to calculate 90 days moving avarage\nsales_test = sales[sales.shape[0]-w_size-90:, :]\nprices_test = prices[sales.shape[0]-w_size-90:, :]\ncalendar_test = calendar[sales.shape[0]-w_size-90:, :]\ntest_inputs = [sales_test, prices_test, categories, calendar_test]\n\nfor i in range(28):\n    \n    test_gen = SequenceGenerator(test_inputs, spans=span_lst, \n                                 window=w_size, infer=True)\n    test_iter = test_gen.generate()\n    X = next(test_iter)\n    y_pred = model.predict(X)\n\n    # appending predicted sales to the input and shifting it by 1\n    sales_test = np.append(sales_test, y_pred.reshape(1,-1), axis=0)[1:, :]\n    prices_test = prices_test[1:, :]\n    calendar_test = calendar_test[1:, :]\n    test_inputs = [sales_test, prices_test, categories, calendar_test]","640f69b2":"sales_test = np.exp((sales_test * sales_std) + sales_mean) - 1\nsales_test = np.maximum(sales_test, np.zeros(sales_test.shape))","a8b1a103":"submission = pd.read_csv(DATA_DIR + 'sample_submission.csv', nrows=rows)\n\nif DEBUG:\n    submission.iloc[:100, 1:] = sales_test[-28:, :].T\nelse:\n    submission.iloc[:len(submission)\/\/2, 1:] = sales_test[-28:, :].T\n    \nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","50d2aa61":"scaling predicted values back into original values","d1199307":"## Configuration","d20a7f67":"## Making Submission\nWe are only creating \"validation\" (corresponding to the Public leaderboard) submission. The submission for \"evaluation\" (corresponding to the Private leaderboard) is out-of-scope for now.","2175a7eb":"## Predictions for Submission\n* Predicting day by day by looping through 28 days\n* Each prediction will be used for the model input for the next day prediction","220936ee":"## Model Training - Cross Validation","b47a08d0":"Categorical features are label encoded to be used for embeddings.","a87a9966":"### Price Dataset\n* Transposing the long format into (weeks, items) dimension\n* Then it is merged to calendar data to make it daily format","51e21918":"## Defining Model\n\n* Sales and price sequences are put into LSTM\n* Categorical features are going into Embedding layer\n* Snap indicator is put directly into Dense layer","487826e8":"# M5 Forecasting - LSTM w\/ Custom Generator\n\nThis notebook shows LSTM training\/prediction with a custom data generator for Keras LSTM model. The model uses sequences of sales and prices of {w_size} days with categorical features being used with embeddings to predict next one day sales on each item.\nFor the submission, it makes prediction with 28 days loop where each one day prediction is used for an input for the next days' prediction in the loop. ","ddccafd6":"## Model Training without CV split\nThis model will be used to make predictions for the submission file","b6615ebc":"### Calendar Dataset","bd59d11f":"## Defining SequenceGenerator\n\nCustom data generator is used to create input sequences and scalers during the model training.\n\n* Sales:\n  \n  Raw sales values along with its moving averages (7days, 30days, 90days) are used for model input sequences.\n  \n  input shape: (days, items) -> output shape: (items, days, features)\n  \n* Prices:\n  \n  Raw price sequence values are used.\n  \n  input shape: (days, items) -> output shape: (items, days, feature)\n  \n* Categories:\n  \n  item_id, dept_id, cat_id, store_id, state_id are used as a single values for each row. These are fed into embedding layer.\n  \n  input shape: (items, features) -> output shape: (items, 1) * features\n  \n* Calendar:\n\n  wday, month, year, event_name_1, event_type_1, event_name_2, event_type_2 are used as a single values for each row. These are going to embedding layer. Snap indicator corresponding to the item's state are also fetched and it will be directly put into dense layer.\n  \n  input shape: (days, features) -> output shape: (items, 1) * features","f57dc154":"## Training Dataset\n\n### Sales Dataset\n* Transposing (items, days) dimension into (days, items)\n* Only last 360 days are used for now to save run time","47a94178":"## Preprocessing\n\nBoth sales and prices are log scaled and then standardized by global average and std."}}