{"cell_type":{"54ed4119":"code","5532614b":"code","d98f6992":"code","8247a79c":"code","492fbf7a":"code","66b2bd37":"code","3478666e":"code","8bafe498":"code","73fbd03d":"code","aae0e581":"code","691ca300":"code","d0b467ba":"code","7851d8df":"code","5279f0a9":"code","93c1a436":"code","a5b2364f":"code","f2fabfdd":"code","eef674ea":"code","2800beae":"code","9aac7cbd":"code","9d7f8483":"code","8a48540c":"code","80f4bf31":"code","1fa20d84":"code","d049bdd1":"code","2651f1cb":"code","6b456a7b":"code","773d8514":"code","8a5ebfed":"code","d2cab2c7":"code","557ab22e":"code","3b99a251":"code","2b36b4d9":"code","f8992525":"code","71cd89a8":"code","43103bd8":"code","7c2baacf":"code","55f63324":"code","2da09485":"code","20dca4e6":"code","28d96ce2":"code","ed77d502":"code","5d0abd1a":"code","60b3c08a":"code","ac4dcc4a":"code","2a7d5d41":"code","b636dd01":"code","705a05ee":"code","2f0a6e06":"code","0ad5e3ca":"code","f4d0710b":"code","4b78ae54":"code","b46c378d":"code","b8c47372":"code","45cd2158":"code","10d34f3c":"code","e2060300":"code","41ec9312":"code","dad9c83f":"code","13244645":"code","52e9ff9b":"code","4786e5ad":"code","e5a2fdac":"markdown","e29cb867":"markdown","5931f9aa":"markdown","8214ce18":"markdown","bb57b6b7":"markdown","cb6dba2c":"markdown","66794743":"markdown","e9549718":"markdown","9e45ddd5":"markdown","e10a01af":"markdown","b25f3b3e":"markdown","0ed592df":"markdown","eeb7b9c9":"markdown","6906d922":"markdown","b3bfa7c5":"markdown","522fac9e":"markdown","f04fe198":"markdown","3b4d53fd":"markdown","65470079":"markdown","2aa220e2":"markdown","1a9f2308":"markdown","9642d86f":"markdown","e2abd872":"markdown","d1584e31":"markdown","ed9f3874":"markdown","dcd3fa6a":"markdown","e55a63ba":"markdown","feee468c":"markdown","ec6bc20e":"markdown","d85c9af1":"markdown","53899034":"markdown","aa0960cc":"markdown","3861fa57":"markdown","84fdab34":"markdown","920127f3":"markdown","566980cf":"markdown","c1494931":"markdown"},"source":{"54ed4119":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')","5532614b":"import os\nprint(os.listdir(\"..\/input\"))","d98f6992":"Data_train = pd.read_csv(r'..\/input\/house-prices-advanced-regression-techniques\/train.csv')","8247a79c":"Data_train.head()","492fbf7a":"Data_train['SalePrice'].describe()","66b2bd37":"sns.distplot(Data_train['SalePrice'])","3478666e":"var = 'GrLivArea'\ndata = pd.concat([Data_train['SalePrice'], Data_train[var]], axis=1) # axis=0, the columns are connected along\ndata.plot.scatter(x=var, y='SalePrice')","8bafe498":"data = pd.concat([Data_train['SalePrice'], Data_train['TotalBsmtSF']], axis=1)\ndata.plot.scatter(x='TotalBsmtSF', y='SalePrice')","73fbd03d":"Var = ['OverallQual','YearBuilt']\nData_OverallQual = pd.concat([Data_train['SalePrice'], Data_train[Var[0]]], axis=1)\nData_YearBuilt = pd.concat([Data_train['SalePrice'], Data_train[Var[1]]], axis=1)\n# fig, ax = plt.subplots(2,1)\n","aae0e581":"fig = sns.boxplot(x=Var[0], y='SalePrice', data=Data_OverallQual)\n# f, ax = plt.subplots(figsize=(32,8))\nfig = plt.figure(figsize=(32,8))\nfig = sns.boxplot(x=Var[1], y='SalePrice', data=Data_YearBuilt)\nplt.xticks(rotation=90)","691ca300":"# caculate the correlation matrix\ncorrmat = Data_train.corr()\nf, ax = plt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, square=True)","d0b467ba":"# find the most 9 important features in all features.\nk = 10 # number of features to draw heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index # choose the top 10 features\ncm = np.corrcoef(Data_train[cols].values.T)\nhm = sns.heatmap(cm, cbar=True, annot=True, fmt= '.2f', xticklabels=cols.values, yticklabels=cols.values)","7851d8df":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(Data_train[cols], size = 2.5) # draw pairwise figures","5279f0a9":"Total = Data_train.isnull().sum().sort_values(ascending=False)\nPercent = (Data_train.isnull().sum()\/Data_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([Total, Percent], axis=1, keys=['Total','Percent'])\nmissing_data.head(23)","93c1a436":"Data_train = Data_train.drop((missing_data[missing_data['Total']>1]).index, 1)\n","a5b2364f":"Data_train = Data_train.drop(Data_train.loc[Data_train['Electrical'].isnull()].index)\nData_train.isnull().sum().max()","f2fabfdd":"# saleprice outliers\nsaleprice_scaled = StandardScaler().fit_transform(Data_train['SalePrice'][:, np.newaxis])\nsaleprice_scaled_sort = saleprice_scaled[saleprice_scaled[:,0].argsort()] # saleprice_scaled[:,0].argsort() gets the order series","eef674ea":"data = pd.concat([Data_train['SalePrice'], Data_train['GrLivArea']], axis=1)\ndata.plot.scatter(x='GrLivArea', y='SalePrice')","2800beae":"# locate the outliers\nData_train.sort_values(by='GrLivArea', ascending=False)","9aac7cbd":"Data_train = Data_train.drop(Data_train[Data_train['Id'] == 1299].index)\nData_train = Data_train.drop(Data_train[Data_train['Id'] == 524].index)","9d7f8483":"#bivariate analysis saleprice\/TotalBsmtSF\ndata = pd.concat([Data_train['SalePrice'], Data_train['TotalBsmtSF']], axis=1)\ndata.plot.scatter(x='TotalBsmtSF', y='SalePrice')","8a48540c":"Data_train.sort_values(by='TotalBsmtSF',ascending=False)","80f4bf31":"Data_train = Data_train.drop(Data_train[Data_train['Id'] == 333].index)\nData_train = Data_train.drop(Data_train[Data_train['Id'] == 497].index)\nData_train = Data_train.drop(Data_train[Data_train['Id'] == 441].index)","1fa20d84":"sns.distplot(Data_train['SalePrice'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(Data_train['SalePrice'], plot=plt)","d049bdd1":"Data_train.head()","2651f1cb":"Data_train['SalePrice'] = np.log(Data_train['SalePrice'])","6b456a7b":"sns.distplot(Data_train['SalePrice'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(Data_train['SalePrice'], plot=plt)","773d8514":"sns.distplot(Data_train['GrLivArea'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(Data_train['GrLivArea'], plot=plt)","8a5ebfed":"Data_train['GrLivArea'] = np.log(Data_train['GrLivArea'])","d2cab2c7":"sns.distplot(Data_train['GrLivArea'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(Data_train['GrLivArea'], plot=plt)","557ab22e":"sns.distplot(Data_train['TotalBsmtSF'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(Data_train['TotalBsmtSF'], plot=plt)","3b99a251":"Data_train['HasBsmt'] = pd.Series(len(Data_train['TotalBsmtSF']), index=Data_train.index)\nData_train['HasBsmt'] = 0 \nData_train.loc[Data_train['TotalBsmtSF']>0,'HasBsmt'] = 1","2b36b4d9":"Data_train.loc[Data_train['HasBsmt']==1,'TotalBsmtSF'] = np.log(Data_train['TotalBsmtSF'])","f8992525":"sns.distplot(Data_train[Data_train['TotalBsmtSF']>0]['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(Data_train[Data_train['TotalBsmtSF']>0]['TotalBsmtSF'], plot=plt)","71cd89a8":"sns.distplot(Data_train['LotArea'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(Data_train['LotArea'], plot=plt)","43103bd8":"Data_train['LotArea'] = np.log(Data_train['LotArea'])","7c2baacf":"sns.distplot(Data_train['LotArea'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(Data_train['LotArea'], plot=plt)","55f63324":"plt.scatter(Data_train['GrLivArea'], Data_train['SalePrice'])","2da09485":"plt.scatter(Data_train[Data_train['TotalBsmtSF']>0]['TotalBsmtSF'], Data_train[Data_train['TotalBsmtSF']>0]['SalePrice'])","20dca4e6":"Data_train = pd.get_dummies(Data_train)\nData_train.head(10)","28d96ce2":"head_title = ['OverallQual', 'GrLivArea', 'GarageCars', '1stFlrSF','TotalBsmtSF', 'FullBath'\n              , 'TotRmsAbvGrd', 'YearBuilt'\n              , 'LotArea', 'OverallCond','YearRemodAdd', 'Fireplaces', 'WoodDeckSF', 'OpenPorchSF'\n              , 'SalePrice']\nData_in_use = Data_train.loc[:, head_title]\nData_in_use.head()","ed77d502":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgb","5d0abd1a":"Xtrain, Xvali, Ytrain, Yvali = train_test_split(Data_in_use.iloc[:,:14],Data_in_use.iloc[:,14:15]\n                                                ,test_size=0.2,random_state=420,shuffle=False)","60b3c08a":"Xtrain.head()","ac4dcc4a":"# creat data for lightgbm\nlgb_train = lgb.Dataset(Xtrain,Ytrain)\nlgb_vali = lgb.Dataset(Xvali, Yvali)","2a7d5d41":"# parameters setting\nparams = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'regression',\n        'metric': 'auc',\n        'num_leaves': 58,\n        'learning_rate': 0.3,\n        'feature_fraction': 0.8,\n        'bagging_fraction': 0.7,\n        'bagging_freq': 0,\n        'verbose': 1\n        ,'max_depth': 18\n        ,'n_estimators':6\n        ,'max_bin':181\n        ,'min_data_in_leaf':17\n        ,'lambda_l1':0.75\n        ,'lambda_l2':0\n        ,'min_split_gain':0.0\n        }","b636dd01":"# training\ngbm = lgb.train(params, lgb_train, num_boost_round = 100, valid_sets = lgb_vali, verbose_eval=False)\nprint('save model')\n# joblib.dump()\ngbm.save_model('lightgbm_model.txt')\n","705a05ee":"'''\n# Gridsearch for super-parameters\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n# params_test1 = {'min_data_in_leaf':list(np.linspace(0,50,30).astype(int))}\n\nparams_test1 = {'bagging_fraction':list(np.linspace(0.0,1,11))\n                ,'num_leaves': list(np.linspace(0,100,30).astype(int))\n                ,'learning_rate': list(np.linspace(0.0,1,11))\n                ,'feature_fraction': list(np.linspace(0.0,1,11))\n                ,'bagging_fraction': list(np.linspace(0.0,1,11))\n                ,'max_depth': list(np.linspace(0,50,30).astype(int))\n                ,'n_estimators':list(np.linspace(0,100,30).astype(int))\n                ,'lambda_l1':list(np.linspace(0.0,1,5))\n               }\nestimator = lgb.LGBMRegressor(boosting_type='gbdt'\n                              , objective='regression'\n                              , metrics='auc'\n                              , learning_rate=0.1\n                              , n_estimators=1\n                              , max_depth=6\n                              , bagging_fraction = 0.8\n                              , feature_fraction = 0.8)\ngsearch1 = RandomizedSearchCV(estimator\n                              ,param_distributions = params_test1\n                              ,cv=5\n                              ,scoring='neg_log_loss'\n                             )\ngsearch1.fit(Xtrain, Ytrain)\nprint(gsearch1.best_params_)\n'''","2f0a6e06":"# prediction\ny_prediction = gbm.predict(Xvali, num_iteration = gbm.best_iteration)","0ad5e3ca":"# accuracy\nimport math\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nRMSE = math.sqrt(mean_squared_error(Yvali, y_prediction))\nCVRMSE = RMSE\/np.mean(Yvali)*100\nMAE = mean_absolute_error(Yvali, y_prediction)\nMSE = mean_squared_error(Yvali, y_prediction)\nR2 = r2_score(Yvali, y_prediction)\naccuracy = (1-MAE\/np.mean(Yvali))*100\nprint(\"lightgbm's MSE is %.2f, RMSE is %.2f, CVRMSE is %.2f%%, R2 is %f, Accuracy is %.2f%%\" % (MSE, RMSE,CVRMSE,R2, accuracy))","f4d0710b":"Data_test = pd.read_csv(r'..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nData_test.head(10)","4b78ae54":"Data_test = pd.get_dummies(Data_test)\nData_test.head(10)","b46c378d":"# model_lightgbm = lgb.Booster(model_file=r'..\/input\/house-prices-advanced-regression-techniques\/lightgbm_model.txt')\nmodel_lightgbm = gbm","b8c47372":"head_title = ['OverallQual', 'GrLivArea', 'GarageCars', '1stFlrSF','TotalBsmtSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt'\n             ,'LotArea', 'OverallCond','YearRemodAdd', 'Fireplaces', 'WoodDeckSF', 'OpenPorchSF']\nX_test = Data_test.loc[:, head_title]\nX_test.head()","45cd2158":"X_test.isnull().sum()","10d34f3c":"X_test['GrLivArea'] = np.log(X_test['GrLivArea'])\nX_test['TotalBsmtSF'] = np.log(X_test['TotalBsmtSF'])\nX_test['LotArea'] = np.log(X_test['LotArea'])","e2060300":"X_test.head()","41ec9312":"y_prediction_test = model_lightgbm.predict(X_test, num_iteration = gbm.best_iteration)","dad9c83f":"y_prediction_test.shape","13244645":"y_prediction_test = np.exp(y_prediction_test)\ny_prediction_test = y_prediction_test[:, np.newaxis]","52e9ff9b":"y_prediction_test = pd.DataFrame(y_prediction_test)\ny_prediction_test.head()","4786e5ad":"y_prediction_test.to_csv('results.csv')","e5a2fdac":"Outliers ","e29cb867":"dummy all the catagory features","5931f9aa":"Usting the top 9+5 features to do training.\nFirst we used only the first 9 features and the best score is 0.17669, then we used 9+5 features and the best score is 0.17141. The results is improved a lit bit.","8214ce18":"obtain the prediction results","bb57b6b7":"Log-transformation for GrLivArea","cb6dba2c":"getting start for test data!","66794743":"Show the data structure.","e9549718":"drop these three outliers","9e45ddd5":"Load the train data. You could select your route.","e10a01af":"Relationship with catagorical input features","b25f3b3e":"show the statistics summary","0ed592df":"load the developed lightgbm model saved previously.","eeb7b9c9":"Using the 9+5 input features","6906d922":"By this heatmap, we can find the strong relationship between features\n\nwe used the most 9+5 important features to develop the lightgbm model","b3bfa7c5":"How about the other input features? correlation matrix such as heatmap style","522fac9e":"\n<font face=\"\u9ed1\u9ad4\" size=6> Lightgbm model to predict the house prices\n\n<font color=#0099ff size=4> Yongbao Chen <br> September 2021\n\nThis is a first version by using lightgbm model to predict the house price! Comparing with [the stacking models prediction method](https:\/\/www.kaggle.com\/yongbaochen\/ensemble-stacking-to-predict-the-house-prices).\n\nI got the final score of (0.17141, only top 78%), while the stacking models obtained a score of 0.11887 (Top 5% of the leaderboard).\n\nI've refered to [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) by Pedro Marcelino, I really appreciate that.","f04fe198":"Using lightgbm to develop the prediction model. \nsplit to traning set and validation set\ntest set should be pre-processed?\nget the prediciton results and submit it","3b4d53fd":"tansfer back to actual SalePrice values","65470079":"show the histogram","2aa220e2":"Delete the outliers that the two points are at the bottom right corner","1a9f2308":"homoscedasticity analysis","9642d86f":"log-transformation for 'GrLivArea', 'TotalBsmtSF', 'LotArea'","e2abd872":"Analyze the missing data.\nGenerally, we could delete the features when 15% of data is missing","d1584e31":"GrLiveArea and SalePrice have the linear relationship! How about TotalBsmtSF?","ed9f3874":"In the search for normality","dcd3fa6a":"drop these two outliers","e55a63ba":"Data log-transformation.\n\nCause most models presume that the features and lables are normal distribution.","feee468c":"show the features having NA values","ec6bc20e":"save the final results","d85c9af1":"log-transformation for LotArea","53899034":"features relationship between the top features! using pairplot func","aa0960cc":"Log-tranformation for TotalBsmtSF","3861fa57":"Originally, I used the GridsearchCV, while my PC was out of memory (Intel(R) Core(TM) i7-10700 CPU @ 2.90GHz, RAM 16 GB), So I chose RandomizedSearchCV. If you are using GPU or high powerful computer, I suggest you to use GridsearchCV to get an optimal super-parameters.","84fdab34":"Import the required modules","920127f3":"split the train and validation data set","566980cf":"Acctually, all of the input features are also needed to be pre-processed or analized, we have done it in our stacking model kernel. In this kernel, we just do the log-transformation for several features and get_dummies() all the features directly. This may be one reason why the score is not so good.","c1494931":"delete the missing data"}}