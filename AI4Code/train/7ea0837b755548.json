{"cell_type":{"87115395":"code","836b0b89":"code","c728bc46":"code","735e357d":"code","214d5fad":"code","85a1dc8b":"code","d540af4c":"code","8cf472d8":"code","be9362fd":"code","96b8ca64":"code","fa5a2306":"code","333c1e75":"code","afab6e63":"code","e648318e":"code","95d874b4":"code","cfc31749":"code","1de2c184":"code","70b858d7":"code","ab303663":"code","b68d3494":"code","5a8f4f48":"code","52330db5":"code","d36467fa":"code","4030e52c":"code","928ce2a1":"code","90199189":"code","93deb08b":"markdown","f00161d9":"markdown","aafb08d4":"markdown","440e6954":"markdown","a0547a05":"markdown","e85e002e":"markdown","2fa2e1e4":"markdown","1ce20aed":"markdown","5ec91c78":"markdown","cb71b6fa":"markdown","b61f0ad5":"markdown","24c42f44":"markdown","d13994c1":"markdown","fdcc718a":"markdown","73436b42":"markdown","0eca499a":"markdown","cea759ab":"markdown","98916ea7":"markdown","537373be":"markdown","39826832":"markdown","dbd15f15":"markdown","b003451a":"markdown","b63a5591":"markdown","98d7fdc7":"markdown","388e69ab":"markdown","dd61341a":"markdown","20ca13f9":"markdown","a1e3bae9":"markdown","410183e3":"markdown","bb06c727":"markdown","386d03be":"markdown","cc23bcb0":"markdown","a505eaac":"markdown","4a2b4ef7":"markdown","785f22ca":"markdown","3f963504":"markdown","d5c78ab2":"markdown"},"source":{"87115395":"!pip install -q --upgrade wandb","836b0b89":"import numpy as np\nimport pandas as pd\n\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom wordcloud import WordCloud, STOPWORDS\n\n#Text Color\nfrom termcolor import colored\n\n#Data Preprocessing\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n\n#NLP\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#WordCloud\nfrom wordcloud import WordCloud, STOPWORDS\n\n#Text Processing\nimport re\nimport nltk\nnltk.download('popular')\n\n#Language Detection\n!pip install langdetect\nimport langdetect\n\n#Sentiment\nfrom textblob import TextBlob\n\n#ner\nimport spacy\n\n#Vectorizer\nfrom sklearn import feature_extraction, manifold\n\n#Word Embedding\nimport gensim.downloader as gensim_api\n\n#Topic Modeling\nimport gensim\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn.functional as F\n\nimport random\n\nimport wandb\n\nfrom tqdm import tqdm\nimport os\nfrom sklearn.model_selection import StratifiedKFold, TimeSeriesSplit\n\nimport warnings\nwarnings.filterwarnings('ignore')","c728bc46":"# Login to wandb\nwandb.login()","735e357d":"target_stats = pd.read_csv('..\/input\/player-target-stats\/player_target_stats.csv')\n\nclass CFG:\n    nfolds = 5\n    batch_size = 1024\n    val_batch_size = batch_size * 8\n    nepochs = 3\n    lr = 0.001\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    seed = 42\n    targets = ['target1', 'target2', 'target3', 'target4']\n    engineered_cols = ['player_t1_mean', 'player_t2_mean', 'player_t3_mean', 'player_t4_mean']\n    cols = ['month', 'week', 'weekday'] + engineered_cols + [col for col in target_stats.columns if col not in ['playerId']]\n    debug = False","214d5fad":"# wandb config\nWANDB_CONFIG = {\n     'competition': 'MLB', \n              '_wandb_kernel': 'neuracort'\n    }","85a1dc8b":"def set_seed(seed=42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \nset_seed()","d540af4c":"awards = pd.read_csv(\"..\/input\/c\/mlb-player-digital-engagement-forecasting\/awards.csv\")\ntrain = pd.read_csv('..\/input\/unnest-train\/train_nextDayPlayerEngagement.csv')\n\nif CFG.debug:\n    train = train[:10000]","8cf472d8":"awards.head()","be9362fd":"print(f\"Dataset Shape: {colored(awards.shape, 'yellow')}\")","96b8ca64":"awards.info()","fa5a2306":"# Dropping Null Values\nawards.dropna(inplace = True)\n\n# Correcting DataType of `awardDate`\nawards[\"awardDate\"] = pd.to_datetime(awards[\"awardDate\"])","333c1e75":"for col in awards.columns:\n    print(col + \":\" + colored(str(len(awards[col].unique())), 'yellow'))","afab6e63":"def preprocess_text(text, flg_stemm=False, flg_lemm=True):\n\n    lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n    \n    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n            \n    ## Tokenize (convert from string to list)\n    lst_text = text.split()\n    ## remove Stopwords\n    if lst_stopwords is not None:\n        lst_text = [word for word in lst_text if word not in \n                    lst_stopwords]\n                \n    ## Stemming (remove -ing, -ly, ...)\n    if flg_stemm == True:\n        ps = nltk.stem.porter.PorterStemmer()\n        lst_text = [ps.stem(word) for word in lst_text]\n                \n    ## Lemmatisation (convert the word into root word)\n    if flg_lemm == True:\n        lem = nltk.stem.wordnet.WordNetLemmatizer()    \n        lst_text = [lem.lemmatize(word) for word in lst_text]\n            \n    ## back to string from list\n    text = \" \".join(lst_text)\n    return text\n\ndef apply_preprocess_text(df, column):\n\n    #Clean text\n    clean_column = \"clean_\" + column\n    df[clean_column] = df[column].apply(lambda x: preprocess_text(x, flg_stemm=False, flg_lemm=True, ))\n\n    #Length of text\n    clean_column_len = \"clean_\" + column + \"_len\"\n    df[clean_column_len] = df[clean_column].apply(lambda x: len(x))\n\n    #Word Count\n    clean_column_word_count = \"clean_\" + column + \"_word_count\"\n    df[clean_column_word_count] =df[clean_column].apply(lambda x: len(str(x).split(\" \")))\n\n    #Character Count\n    clean_column_char_count = \"clean_\" + column + \"_char_count\"\n    df[clean_column_char_count] = df[clean_column].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\n\n    #Average Word Length\n    clean_column_avg_word_length = \"clean_\" + column + \"_avg_word_length\"\n    df[clean_column_avg_word_length] = df[clean_column_char_count] \/ df[clean_column_word_count]","e648318e":"apply_preprocess_text(awards, \"awardName\")\napply_preprocess_text(awards, \"playerName\")","95d874b4":"import plotly.express as px\ndef plot_distribution(df, x, title):\n\n    fig = px.histogram(\n    df, \n    x = x,\n    width = 800,\n    height = 500,\n    title = title\n    )\n    \n    fig.show()","cfc31749":"plot_distribution(df = awards, x = 'clean_awardName_len', title = 'Award Name Length Distribution')","1de2c184":"plot_distribution(df = awards, x = 'clean_awardName_word_count', title = 'Word Count Distribution')","70b858d7":"plot_distribution(df = awards, x = 'clean_awardName_char_count', title = 'Character Count Distribution')","ab303663":"plot_distribution(df = awards, x = 'clean_awardName_avg_word_length', title = 'Average Word Length Distribution')","b68d3494":"plot_distribution(df = awards, x = 'clean_playerName_len', title = 'Award Name Length Distribution')","5a8f4f48":"plot_distribution(df = awards, x = 'clean_playerName_word_count', title = 'Word Count Distribution')","52330db5":"plot_distribution(df = awards, x = 'clean_playerName_char_count', title = 'Character Count Distribution')","d36467fa":"plot_distribution(df = awards, x = 'clean_playerName_char_count', title = 'Character Count Distribution')","4030e52c":"class MLBDataset(Dataset):\n    def __init__(self, df, targets=None, mode='train'):\n        self.mode = mode\n        self.data = df\n        if mode == 'train':\n            self.targets = targets\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        if self.mode == 'train':\n            x = self.data[idx]\n            y = np.array(self.targets[idx])\n            return torch.from_numpy(x).float(), torch.from_numpy(y).float()\n        elif self.mode == 'test':\n            return torch.from_numpy(self.data[idx]).float()\n        \nclass MLBModel(nn.Module):\n    def __init__(self, num_cols):\n        super(MLBModel, self).__init__()\n        self.dense1 = nn.Linear(num_cols, 100)\n        self.dense2 = nn.Linear(100, 100)\n        self.dense3 = nn.Linear(100, len(CFG.targets))\n\n    def forward(self, x):\n        x = F.relu(self.dense1(x))\n        x = F.relu(self.dense2(x))\n        x = self.dense3(x).squeeze()\n\n        return x","928ce2a1":"def add_player_features(row, player_dict, n_feats, id_col, nan_value=-1):\n    features = np.full((n_feats), nan_value, dtype=np.float32)\n    \n    pid = row[id_col]\n    \n    if pid in player_dict:\n        # overall player means\n        p_count = player_dict[pid]['count']\n        features[0] = player_dict[pid]['mean_t1'] \/ p_count\n        features[1] = player_dict[pid]['mean_t2'] \/ p_count\n        features[2] = player_dict[pid]['mean_t3'] \/ p_count\n        features[3] = player_dict[pid]['mean_t4'] \/ p_count\n    \n    return features\n\ndef update_player_features(row, player_dict, nan_value=-1):\n    assert player_dict is not None\n    \n    pid = row[2]\n    t1, t2, t3, t4 = row[3], row[4], row[5], row[6]\n    \n    if pid in player_dict:\n        # update target lags\n        player_dict[pid]['mean_t1'] += t1\n        player_dict[pid]['mean_t2'] += t2\n        player_dict[pid]['mean_t3'] += t3\n        player_dict[pid]['mean_t4'] += t4\n        player_dict[pid]['count'] += 1\n    else:\n        # init the feature dict for this player\n        player_dict[pid] = {'mean_t1': t1, 'mean_t2': t2, 'mean_t3': t3, 'mean_t4': t4, 'count': 1}\n    \n    return player_dict\n\ndef do_feature_engineering(df, id_col=2):\n    player_features = {}\n    nfeats = len(CFG.engineered_cols)\n    features = np.zeros((df.shape[0], nfeats), dtype=np.float32)\n    \n    for idx, row in enumerate(tqdm(df.values)):\n        row_features = add_player_features(row, player_features, nfeats, id_col=id_col)\n        player_features = update_player_features(row, player_features)\n        features[idx] = row_features\n    \n    features = pd.DataFrame(features, columns=CFG.engineered_cols)\n    df = pd.concat([df, features], axis=1)\n    \n    return df, player_features\n\ndef do_feature_engineering_test(df, player_dict, id_col=2):\n    features = np.zeros((df.shape[0], len(CFG.engineered_cols)), dtype=np.float32)\n    \n    for idx, row in enumerate(df.values):\n        row_features = add_player_features(row, player_dict, len(CFG.engineered_cols), id_col=id_col)\n        features[idx] = row_features\n    \n    features = pd.DataFrame(features, columns=CFG.engineered_cols, index=df.index)\n    df = pd.concat([df, features], axis=1)\n    \n    return df\n\ndef train_epoch(model, loader, optimizer, device, criterion):\n    model.train()\n    train_loss = 0.0\n\n    for i, (sample, target) in enumerate(tqdm(loader)):\n        sample, target = sample.to(device), target.to(device)\n        optimizer.zero_grad()\n\n        preds = model(sample)\n\n        loss = criterion(preds, target)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item() \/ len(loader)\n\n    return model, train_loss\n\ndef validate(model, loader, device, criterion):\n    model.eval()\n    val_loss = 0.0\n    val_preds = []\n    val_targets = []\n\n    with torch.no_grad():\n        for i, (sample, target) in enumerate(tqdm(loader)):\n            sample, target = sample.to(device), target.to(device)\n\n            preds = model(sample)\n            loss = criterion(preds, target)\n            val_loss += loss.item() \/ len(loader)\n\n            val_preds.append(preds.cpu())\n            val_targets.append(target.cpu())\n\n        val_preds = np.concatenate(val_preds)\n        val_targets = np.concatenate(val_targets)\n\n    return val_loss, val_preds\n\nclass ColL1Loss(nn.Module):\n    def __init__(self, num_targets):\n        super().__init__()\n        self.mae = nn.L1Loss()\n        assert num_targets != 0\n        self.num_targets = num_targets\n    \n    def forward(self, preds, targets):\n        l1 = 0.0\n        \n        for i in range(self.num_targets):\n            l1 += self.mae(preds[:, i], targets[:, i])\n        \n        return l1 \/ self.num_targets\n    \ndef train_fold(xtrn, ytrn, xval, yval, fold):\n    print(f\"Train fold {fold}\")\n    train_set = MLBDataset(xtrn, ytrn)\n    val_set = MLBDataset(xval, yval)\n    \n    train_loader = DataLoader(train_set, batch_size=CFG.batch_size, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=CFG.val_batch_size, shuffle=False)\n    \n    model = MLBModel(xtrn.shape[1]).to(CFG.device)\n    criterion = ColL1Loss(len(CFG.targets))\n    optimizer = optim.Adam(model.parameters(), lr=CFG.lr)\n    \n    best_loss = {'train': np.inf, 'val': np.inf}\n    best_val_preds = None\n    \n    # Initialize W&B\n    run = wandb.init(project='MLB', config= WANDB_CONFIG)\n    \n    for epoch in range(1, CFG.nepochs + 1):\n        print(f\"Train epoch {epoch}\")\n        model, loss = train_epoch(model, train_loader, optimizer, CFG.device, criterion)\n        val_loss, val_preds = validate(model, val_loader, CFG.device, criterion)\n        \n        # save best model\n        if val_loss < best_loss['val']:\n            best_loss = {'train': loss, 'val': val_loss}\n            torch.save(model.state_dict(), f'fold{fold}.pt')\n            \n            # save best oof predictions\n            best_val_preds = val_preds\n            np.save(f'oof_fold{fold}.npy', val_preds)\n        \n        print(\"Train loss: {:5.5f}\".format(loss))\n        print(\"Val loss: {:5.5f}\".format(val_loss))\n        \n        wandb.log({\n            'train_loss': loss,\n            'valid_loss': val_loss\n        })\n        \n    # Close W&B run\n    wandb.finish()\n    \n    return best_val_preds\n\ndef extract_date_feats(df):\n    df['year'] = pd.DatetimeIndex(df['engagementMetricsDate']).year\n    df['month'] = pd.DatetimeIndex(df['engagementMetricsDate']).month\n    df['week'] = pd.DatetimeIndex(df['engagementMetricsDate']).week\n    df['weekday'] = pd.DatetimeIndex(df['engagementMetricsDate']).weekday\n    \n    return df\n\ndef train_kfolds(df):\n    set_seed(CFG.seed)\n    ts = TimeSeriesSplit(n_splits=CFG.nfolds)\n    df = extract_date_feats(df)\n    \n    oof_preds = []\n    oof_targets = []\n    \n    for fold_idx, (trn_idx, val_idx) in enumerate(ts.split(df)):\n        trn_df, val_df = df.iloc[trn_idx], df.iloc[val_idx]\n        \n        trn_df, player_dict = do_feature_engineering(trn_df)\n        val_df = do_feature_engineering_test(val_df, player_dict)\n        trn_df = pd.merge(trn_df, target_stats, how='left', on='playerId')\n        val_df = pd.merge(val_df, target_stats, how='left', on='playerId')\n        \n        xtrn, ytrn = trn_df[CFG.cols].values, trn_df[CFG.targets].values\n        xval, yval = val_df[CFG.cols].values, val_df[CFG.targets].values\n        \n        val_preds = train_fold(xtrn, ytrn, xval, yval, fold_idx)\n        \n        oof_preds.append(val_preds)\n        oof_targets.append(yval)\n        del trn_df, val_df, player_dict\n        del xtrn, ytrn, xval, yval\n    \n    oof_preds = torch.from_numpy(np.clip(np.concatenate(oof_preds), 0, 100)).float()\n    oof_targets = torch.from_numpy(np.concatenate(oof_targets)).float()\n    \n    eval_metric = ColL1Loss(len(CFG.targets))\n    oof_loss = eval_metric(oof_preds, oof_targets)\n    df, player_dict = do_feature_engineering(df)\n    del oof_preds, oof_targets\n    del df\n    \n    return oof_loss, player_dict","90199189":"loss, player_dict = train_kfolds(train)\nprint(\"OOF loss: {:5.5f}\".format(loss))\ndel train","93deb08b":"<h1><center>MLB In-Depth EDA<\/center><\/h1>\n<h2><center>One Stop for all your needs!<\/center><\/h2>\n                                                      \n<center><img src = \"https:\/\/upload.wikimedia.org\/wikipedia\/en\/thumb\/a\/a6\/Major_League_Baseball_logo.svg\/1200px-Major_League_Baseball_logo.svg.png\" width = \"750\" height = \"500\"\/><\/center>                                                                                               ","f00161d9":"## Description\n- In this competition, you\u2019ll predict how fans engage with MLB players\u2019 digital content on a daily basis for a future date range. \n- You\u2019ll have access to player performance data, social media data, and team factors like market size. Successful models will provide new insights into what signals most strongly correlate with and influence engagement.\n\n## Evaluation Criteria\n- Submissions are evaluated on the mean column-wise mean absolute error (MCMAE). \n- A mean absolute error is calculated for each of the four target variables and the score is the average of those four MAE values.","aafb08d4":"## **<span style=\"color:orange;\">Cleaning Dataset<\/span>** ","440e6954":"<center><img src = \"https:\/\/olc-wordpress-assets.s3.amazonaws.com\/uploads\/2021\/04\/OLC-Awards-Thumbnail-1200x800.jpg\" width = \"750\" height = \"500\"\/><\/center>                                                                                               ","a0547a05":"## **<span style=\"color:orange;\">Distribution Plot<\/span>** ","e85e002e":"Let's begin by understanding a brief overview of the competition. I like to code more than to write bulky paragraphs, so we will move straight ahead to the code part in the next step!","2fa2e1e4":"Let's take a quick peek into the dataset.","1ce20aed":"Next we will do exploration on each of the files separately and as mentioned \"InDepth\".","5ec91c78":"This file has awards won by players in the training set prior to the beginning of the daily data (i.e. before 2018).\n\n- `awardDate` - Date award was given.\n- `awardSeason` - Season award was from.\n- `awardId`\n- `awardName`\n- `playerId` - Unique identifier for a player.\n- `playerName`\n- `awardPlayerTeamId`","cb71b6fa":"<a id=\"model\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Model<\/center><\/h2>","b61f0ad5":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:purple; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>If you find this notebook useful, do give me an upvote, it helps to keep up my motivation. This notebook will be updated frequently so keep checking for furthur developments.<\/center><\/h3>","24c42f44":"### **You can connect with me on [LinkedIn](https:\/\/www.linkedin.com\/in\/ishandutta0098)**","d13994c1":"<a id=\"global-config\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Global Config<\/center><\/h2>","fdcc718a":"### 2. Player Name Distributions","73436b42":"<center><img src = \"https:\/\/i.imgur.com\/1sm6x8P.png\" width = \"750\" height = \"500\"\/><\/center>        ","0eca499a":"**Weights & Biases** is the machine learning platform for developers to build better models faster.\n\nYou can use W&B's lightweight, interoperable tools to\n\n- quickly track experiments,\n- version and iterate on datasets,\n- evaluate model performance,\n- reproduce models,\n- visualize results and spot regressions,\n- and share findings with colleagues.\n  \nSet up W&B in 5 minutes, then quickly iterate on your machine learning pipeline with the confidence that your datasets and models are tracked and versioned in a reliable system of record.\n\nIn this notebook I will use Weights and Biases's amazing features to perform wonderful visualizations and logging seamlessly.","cea759ab":"--- \n\n## **<span style=\"color:orange;\">Let's have a Talk!<\/span>**\n> ### Reach out to me on [LinkedIn](https:\/\/www.linkedin.com\/in\/ishandutta0098)\n\n---","98916ea7":"## **<span style=\"color:orange;\">Dataset Size<\/span>** ","537373be":"<a id=\"awards-exploration\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Awards Exploration<\/center><\/h2>","39826832":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Contents<\/center><\/h2>","dbd15f15":"Now we shall check if there are any null values in the dataset and if the datatypes are correct.","b003451a":"<a id=\"competition-overview\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Competition Overview<\/center><\/h2>","b63a5591":"<a id=\"weights-and-biases\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Weights and Biases<\/center><\/h2>","98d7fdc7":"Continuing further, we would look for unique values in the data to gain an intuition about repeated values.","388e69ab":"<a id=\"libraries\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Libraries<\/center><\/h2>","dd61341a":"We can infer that all columns have repeated values.","20ca13f9":"1. [Competition Overview](#competition-overview)  \n2. [Libraries](#libraries)  \n3. [Global Config](#global-config)\n4. [Weights and Biases](#weights-and-biases)\n5. [Load Datasets](#load-datasets)  \n6. [Awards Exploration](#awards-exploration)  \n7. [Model](#model)","a1e3bae9":"### 1. Award Name Distributions","410183e3":"<center><img src = \"https:\/\/i.guim.co.uk\/img\/media\/b53e226d3c70e90ddc25b877be945176b0470cc0\/0_187_5616_3370\/master\/5616.jpg?width=1200&height=900&quality=85&auto=format&fit=crop&s=c77f3a62ff97312e0c9e8520ce23508a\" width = \"750\" height = \"500\"\/><\/center>                                                                                               ","bb06c727":"Now that we have imported all we need let's begin by using the toughest Python function `read_csv` ;-)","386d03be":"<a id=\"load-datasets\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Load Datasets<\/center><\/h2>","cc23bcb0":"We will import all the necessary modules at one place to make our life easier :-)","a505eaac":"## **<span style=\"color:orange;\">Dataset Info<\/span>** ","4a2b4ef7":"## **<span style=\"color:orange;\">Text Column Preprocessing<\/span>** ","785f22ca":"**OBSERVATIONS**\n- `awardDate`: No Null Values but data type is incorrect it should be `datetime`\n- `awardSeason`: No Null Values and data type is correct.\n- `awardId`: No Null Values and data type is correct.\n- `awardName`: No Null Values and data type is correct.\n- `playerId`: No Null Values and data type is correct.\n- `playerName`: No Null Values and data type is correct.\n- `awardPlayerTeamId`: 13 Null Values and data type is correct.","3f963504":"We shall preprocess the Text Columns to analyse them in a better way.","d5c78ab2":"Since only 0.1% of the data has Null Values we can drop those rows and analyse with the rest of the dataset."}}