{"cell_type":{"ef20dd2d":"code","36cd6ec3":"code","ad8fa263":"code","b3494c2b":"code","af9921ed":"code","e41f0751":"code","93211158":"code","1424fe12":"code","6f8dba45":"code","726e81e4":"code","8148678f":"code","4c70d486":"code","3385f450":"code","d4d9edf4":"code","2ee703d1":"code","b82c0ff6":"code","1bed7f6e":"code","5de150ff":"markdown"},"source":{"ef20dd2d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nfrom sklearn.metrics import classification_report\n\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","36cd6ec3":"from IPython.display import display, HTML\nimport warnings\nfrom tqdm import tqdm\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import f1_score\ntrain_df = pd.read_csv(\"\/kaggle\/input\/scaling-3\/new_train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/scaling-3\/new_test.csv\")\ntrain_df['batch']=((train_df.time-0.0001)\/\/50).astype(int)\ntest_df['batch']=((test_df.time-0.0001)\/\/50).astype(int)\ntrain_df['mini_batch']=((train_df.time-0.0001)\/\/10).astype(int)\ntest_df['mini_batch']=((test_df.time-0.0001)\/\/10).astype(int)\ntrain_df['mini_mini_batch']=((train_df.time-0.0001)\/\/0.5).astype(int)\ntest_df['mini_mini_batch']=((test_df.time-0.0001)\/\/0.5).astype(int)","ad8fa263":"train_means = train_df.groupby('open_channels').signal.agg('mean').reset_index()\ntrain_means['lb'] = train_means.signal-0.1  #Lower Bound\ntrain_means['ub'] = train_means.signal+0.1  #Upper Bound\ntrain_means","b3494c2b":"train_df['high_ci'] = 0\nfor oc in range(11):\n    train_df.loc[(train_df.signal>train_means.loc[oc,'lb']) & (train_df.signal<train_means.loc[oc,'ub']),'high_ci'] += 1\ntrain_df.high_ci.value_counts()","af9921ed":"cis = train_df.groupby('mini_batch').high_ci.mean().reset_index()\ncis","e41f0751":"stds = train_df.groupby(['mini_batch','open_channels']).signal.agg(['std','count']).reset_index()\nstds = stds.loc[stds.groupby(['mini_batch'], sort=False)[('count')].idxmax().values].drop('open_channels',axis=1)\nstds","93211158":"stds = pd.merge(cis,stds,on='mini_batch')\nstds","1424fe12":"stds.plot.scatter('std','high_ci')","6f8dba45":"test_df['high_ci'] = 0\nfor oc in range(11):\n    test_df.loc[(test_df.signal>train_means.loc[oc,'lb']) & (test_df.signal<train_means.loc[oc,'ub']),'high_ci'] += 1\ntest_df.high_ci.value_counts()","726e81e4":"test_std = test_df.groupby('mini_batch').high_ci.mean().reset_index()\ntest_std","8148678f":"stds[['high_ci2']] = stds[['high_ci']]**2","4c70d486":"from sklearn.linear_model import LinearRegression,Ridge\nmodel = LinearRegression().fit(stds[['high_ci','high_ci2']],stds[['std']])\nmodel.score(stds[['high_ci','high_ci2']],stds[['std']])","3385f450":"test_std[['high_ci2']] = test_std[['high_ci']]**2\ntest_std['pred'] = model.predict(test_std[['high_ci','high_ci2']])\ntest_std","d4d9edf4":"stds['pred'] = model.predict(stds[['high_ci','high_ci2']])\nstds","2ee703d1":"stds.set_index('mini_batch').pred.to_dict()","b82c0ff6":"test_std.set_index('mini_batch').pred.to_dict()","1bed7f6e":"(stds['std'] - stds['pred']).abs().sort_values(ascending=False)","5de150ff":"#### This kernel shows how to estimate standard deviations associated with gaussian distribution of each open channels using the regions of high confidence interval (Mean-0.1) to (Mean+0.1). \n### This feature works great for HMM & Random Forest (My 133rd place solution)."}}