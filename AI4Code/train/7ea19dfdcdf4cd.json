{"cell_type":{"b8006d97":"code","198c0a26":"code","dd3195a9":"code","813939f0":"code","4cbede7b":"code","05221f72":"code","e595d0d9":"code","d51d7a72":"code","27f5aa46":"code","89425f41":"code","0588ca0e":"code","eb437c45":"code","b02485de":"code","b33f62d6":"code","9083a357":"code","f7fbd598":"code","c1a39ca4":"code","4d07b7d1":"code","aec00b58":"code","b9773dfa":"code","b0bb36ba":"code","2496af12":"code","1714efbd":"code","0b6fafef":"code","8cb5e8c8":"code","fefc6486":"code","ba8737e7":"code","119a742d":"code","00a3819f":"code","85b29ab6":"code","9412a007":"code","a7c32199":"code","da93ab22":"code","3c511fee":"code","d29653da":"code","866ada47":"code","abb475ff":"code","168c7420":"code","4a1ffd8f":"code","a1b44a9d":"code","92f423d8":"code","b64191e0":"code","fef0b548":"code","9ddd62e0":"markdown","4294bb8b":"markdown","bbe5ad82":"markdown","33ace0d0":"markdown","ec6171be":"markdown","0fef083b":"markdown","bb41f7aa":"markdown","02ebc9f7":"markdown","784d85d0":"markdown","0c8b1545":"markdown","312a44d0":"markdown","26608c92":"markdown","f8494e12":"markdown","d325a168":"markdown","d4b95f6a":"markdown","e8ecace0":"markdown","ecb62be8":"markdown","d54f43cd":"markdown","7e9ff1ec":"markdown","9628a525":"markdown","77375af9":"markdown","e9a59ee4":"markdown","78bc8ed1":"markdown","955f8823":"markdown","9f0084ea":"markdown","c2388312":"markdown","599b4f50":"markdown","cf39b2d4":"markdown","08d4ca77":"markdown","a67677e3":"markdown","a16c2b93":"markdown"},"source":{"b8006d97":"import os\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.datasets import load_files\nfrom keras.utils import np_utils\nimport matplotlib.pyplot as plt\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\nfrom keras.utils.vis_utils import plot_model\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils import to_categorical\nfrom sklearn.metrics import confusion_matrix\nfrom keras.preprocessing import image                  \nfrom tqdm import tqdm\n\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score","198c0a26":"# Pretty display for notebooks\n%matplotlib inline","dd3195a9":"DATA_DIR = \"..\/input\/state-farm-distracted-driver-detection\/imgs\"\nTEST_DIR  = os.path.join(DATA_DIR,\"test\")\nTRAIN_DIR = os.path.join(DATA_DIR,\"train\")\nRANDOM_DIR = '..\/input\/driverdistractionimages'\n\nMODEL_PATH = os.path.join(os.getcwd(),\"model\",\"trained\")\nPICKLE_DIR = os.path.join(os.getcwd(),\"pickle_files\")\nCSV_DIR = os.path.join(os.getcwd(),\"csv_files\")","813939f0":"if not os.path.exists(TRAIN_DIR):\n    print(\"Training data does not exists\")\nif not os.path.exists(TEST_DIR):\n    print(\"Testing data does not exists\")\nif not os.path.exists(RANDOM_DIR):\n    print(\"Random data does not exists\")\n############################################    \nif not os.path.exists(MODEL_PATH):\n    print(\"Model path does not exists\")\n    os.makedirs(MODEL_PATH)\n    print(\"Model path created\")\n############################################\nif not os.path.exists(PICKLE_DIR):\n    os.makedirs(PICKLE_DIR)\n############################################\nif not os.path.exists(CSV_DIR):\n    os.makedirs(CSV_DIR)","4cbede7b":"def create_csv(DATA_DIR,filename):\n    class_names = os.listdir(DATA_DIR) # train , test\n    data = list()\n    \n    if(os.path.isdir(os.path.join(DATA_DIR,class_names[0]))):            # if in train file\n        for class_name in class_names:                                   # which class in the train file\n            file_names = os.listdir(os.path.join(DATA_DIR,class_name))   # images in this class (c0, c1, ...., c9)\n            for file in file_names:                                      # create a dic of the image path and its class name(c0, c1, ...., c9)\n                data.append({\n                    \"Filename\":os.path.join(DATA_DIR,class_name,file),\n                    \"ClassName\":class_name\n                })\n    else:                                                                 # if in test file\n        class_name = \"test\"\n        file_names = os.listdir(DATA_DIR)                                 # images in the test file\n        for file in file_names:                                           # create a dic of the image path and its class name (test)\n            data.append(({\n                \"FileName\":os.path.join(DATA_DIR,file),\n                \"ClassName\":class_name\n            }))\n    \n    data = pd.DataFrame(data)                                               # create a DataFrame using Pandas\n    data.to_csv(os.path.join(os.getcwd(),\"csv_files\",filename),index=False) # convert the DataFrame to CSV file","05221f72":"create_csv(TRAIN_DIR,\"train.csv\")\ncreate_csv(TEST_DIR,\"test.csv\")","e595d0d9":"data_train = pd.read_csv(os.path.join(os.getcwd(),\"csv_files\",\"train.csv\"))\ndata_train.info()","d51d7a72":"data_train['ClassName'].value_counts()","27f5aa46":"data_train.describe()","89425f41":"sns.set()\n\ncount_img_in_each_class= data_train['ClassName'].value_counts(sort=False)\ncategories = data_train['ClassName'].value_counts(sort=False).index.tolist()\n\ny = np.array(count_img_in_each_class)\nwidth = 1\/1.5\nN = len(y)\nx = range(N)\n\nfig = plt.figure(figsize=(20,15))\nay = fig.add_subplot(211)\n\nplt.xticks(x, categories, size=15)\nplt.yticks(size=15)\n\nay.bar(x, y, width, color=\"#169DE3\")\n\nplt.title('Bar Chart',size=25)\nplt.xlabel('classname',size=15)\nplt.ylabel('Count',size=15)\n\nplt.show()","0588ca0e":"labels_list = list(set(data_train['ClassName'].values.tolist()))\ndata_train","eb437c45":"import re\n\nfor label in labels_list : \n\n    for id in re.findall(r'\\d+',label) : \n        data_train['ClassName'].replace(label,id,inplace=True)\n\ndata_train","b02485de":"with open(os.path.join(os.getcwd(),\"pickle_files\",\"labels_list.pkl\"),\"wb\") as handle:\n    pickle.dump(id,handle)","b33f62d6":"labels = to_categorical(data_train['ClassName'])\nprint(labels.shape)","9083a357":"from sklearn.model_selection import train_test_split\n\nxtrain,xvalid,ytrain,yvalid = train_test_split(data_train.iloc[:,0],labels,test_size = 0.2,random_state=2021)","f7fbd598":"xtrain.shape","c1a39ca4":"from keras.preprocessing.image import img_to_array, array_to_img\nfrom keras.preprocessing import image\n\ndef path_to_tensor(img_path):\n    # loads RGB image as PIL.Image.Image type\n    img = image.load_img(img_path,color_mode = 'grayscale', target_size=(224, 224))\n    # convert PIL.Image.Image type to 3D tensor with shape (1, 224, 224, 1)\n    x = image.img_to_array(img)\n    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 1) and return 4D tensor\n    return np.expand_dims(x, axis=0)\n\ndef paths_to_tensor(img_paths):\n    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n    return np.vstack(list_of_tensors)","4d07b7d1":"\nfrom PIL import ImageFile, Image                            \nImageFile.LOAD_TRUNCATED_IMAGES = True                 \n\n# pre-process the data for Keras\ntrain_tensors = paths_to_tensor(xtrain).astype('float32')\/255 - 0.5","aec00b58":"valid_tensors = paths_to_tensor(xvalid).astype('float32')\/255 - 0.5","b9773dfa":"model = Sequential()\n\nmodel.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(224,224,1), kernel_initializer='glorot_normal'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu', kernel_initializer='glorot_normal'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Conv2D(filters=256, kernel_size=2, padding='same', activation='relu', kernel_initializer='glorot_normal'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Conv2D(filters=512, kernel_size=2, padding='same', activation='relu', kernel_initializer='glorot_normal'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(Dense(500, activation='relu', kernel_initializer='glorot_normal'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax', kernel_initializer='glorot_normal'))\n\n\nmodel.summary()","b0bb36ba":"plot_model(model,to_file=os.path.join(MODEL_PATH,\"model_distracted_driver.png\"),show_shapes=True,show_layer_names=True)","2496af12":"model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])","1714efbd":"filepath = os.path.join(MODEL_PATH,\"CNN-model-{epoch:02d}-{val_accuracy:.2f}.hdf5\")\ncheckpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max',period=1)\ncallbacks_list = [checkpoint]","0b6fafef":"model_history = model.fit(train_tensors,ytrain,validation_data = (valid_tensors, yvalid),epochs=25, batch_size=40, shuffle=True,callbacks=callbacks_list)","8cb5e8c8":"Model_path =os.path.join(MODEL_PATH,\"the-complete-model.hdf5\")\nmodel.save(Model_path)","fefc6486":"# # SAVE THE SESSION\n# import keras\n# sess_path = os.path.join(MODEL_PATH,'session.hdf5')\n# saver = tf.train.Saver()\n# sess = keras.backend.get_session()\n# saver.save(sess, sess_path)","ba8737e7":"def plot_train_history(history = model_history):\n    # Summarize history for accuracy\n    plt.figure(figsize = (8, 5))\n    #plt.xticks(np.arange(0, 10))\n    #plt.yticks(np.arange(0, 100))\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model Accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['train', 'validation'], loc='lower right')\n    plt.show()\n\n    # Summarize history for loss\n    plt.figure(figsize = (8, 5))\n    #plt.xticks(np.arange(0, 10))\n    #plt.yticks(np.arange(0, 100))\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'validation'], loc='upper right')\n    plt.show()\n\nplot_train_history(model_history)","119a742d":"def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n    \n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )    \n    fig = plt.figure(figsize=figsize)\n    \n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")  #Plot rectangular data as a color-encoded matrix.\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n    \n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    fig.savefig(os.path.join(MODEL_PATH,\"confusion_matrix.png\"))\n    \n    return fig","00a3819f":"def print_heatmap(n_labels, n_predictions, class_names):\n    labels = n_labels                            \n    predictions = n_predictions                  \n\n    matrix = confusion_matrix(labels.argmax(axis=1),predictions.argmax(axis=1))\n    row_sum = np.sum(matrix, axis = 1)\n    w, h = matrix.shape\n\n    c_m = np.zeros((w, h))\n\n    for i in range(h):\n        c_m[i] = matrix[i] * 100 \/ row_sum[i]\n    c = c_m.astype(dtype = np.uint8)\n    \n    heatmap = print_confusion_matrix(c, class_names, figsize=(18,10), fontsize=20)\n","85b29ab6":"# class_names = list()\n# for name,idx in labels_list:\n#     class_names.append(name)\n#     print(class_names)\n\nypred_valid = model.predict(valid_tensors)","9412a007":"print_heatmap(yvalid,ypred_valid,labels_list)","a7c32199":"ypred_class = np.argmax(ypred_valid,axis=1)\nyvalid = np.argmax(yvalid,axis=1)","da93ab22":"accuracy = accuracy_score(yvalid,ypred_class)\nprint('Accuracy: %f' % accuracy)\n# precision tp \/ (tp + fp)\nprecision = precision_score(yvalid, ypred_class,average='weighted')\nprint('Precision: %f' % precision)\n# recall: tp \/ (tp + fn)\nrecall = recall_score(yvalid,ypred_class,average='weighted')\nprint('Recall: %f' % recall)\n# f1: 2 tp \/ (2 tp + fp + fn)\nf1 = f1_score(yvalid,ypred_class,average='weighted')\nprint('F1 score: %f' % f1)","3c511fee":"data_test = pd.read_csv(os.path.join(os.getcwd(),\"csv_files\",\"test.csv\"))\nprint('Data Test Shape (n_ imgs, classes) = ',data_test.shape, '\\n\\n')\ndata_test.head()","d29653da":"xtest = data_test['FileName'].tolist()\ntest_tensors = paths_to_tensor(xtest[:20]).astype('float32')\/255 - 0.5","866ada47":"test_tensors.shape","abb475ff":"sns.set_style(\"whitegrid\", {'axes.grid' : False})\n\nimport matplotlib.image as mpimg\n# mapping categotical\n\nclasses = [ 'safe driving', ' texting - right', 'talking on the phone - right',\n           'texting - left', 'talking on the phone - left', 'operating the radio',\n           'drinking', 'reaching behind', 'hair and makeup', 'talking to passenger']\n\n\nPredictions = model.predict(test_tensors)\nPredicted_Labels = np.argmax(Predictions, axis = 1)\nfig = plt.figure(figsize = (20,40))\n\nfor i, LAB in enumerate(Predicted_Labels):\n    \n    fig.add_subplot(10, 5, i+1)\n    plt.imshow(test_tensors[i][:][:])\n    plt.axis('off')\n    plt.title('Predicted Label: \\n C{}->{}'.format(LAB , classes[LAB]))\n    ","168c7420":"import os\nfrom keras.models import load_model\nmodel_saved = load_model(Model_path)","4a1ffd8f":"# create_csv(RANDOM_DIR,\"random.csv\")\n\nclass_name = \"random\"\nfile_names = os.listdir(RANDOM_DIR)\ndata = list() \n\nfor file in file_names:                                           # create a dic of the image path and its class name (test)\n    data.append(({\n                \"FileName\":os.path.join(RANDOM_DIR,file),\n                \"ClassName\":class_name\n            }))\n\ndata = pd.DataFrame(data)                                               # create a DataFrame using Pandas\ndata.to_csv(os.path.join(os.getcwd(),\"csv_files\",'random.csv'),index=False) # convert the DataFrame to CSV file","a1b44a9d":"data_random = pd.read_csv(os.path.join(os.getcwd(),\"csv_files\",\"random.csv\"))\nprint('Data Test Shape (n_ imgs, classes) = ',data_random.shape, '\\n\\n')\ndata_random.head(5)","92f423d8":"xrandom = data_random['FileName'].tolist()\nrandom_tensors = paths_to_tensor(xrandom).astype('float32')\/255 - 0.5","b64191e0":"# import glob\n\n# images_paths = list(glob.glob(os.path.join('..\/input\/driverdistractionimages','*.*')))\n# tensors = paths_to_tensor(images_paths)","fef0b548":"sns.set_style(\"whitegrid\", {'axes.grid' : False})\n\nimport matplotlib.image as mpimg\n# mapping categotical\n\nclasses = [ 'safe driving', ' texting - right', 'talking on the phone - right',\n           'texting - left', 'talking on the phone - left', 'operating the radio',\n           'drinking', 'reaching behind', 'hair and makeup', 'talking to passenger']\n\n\nPredictions = model.predict(random_tensors)\nPredicted_Labels = np.argmax(Predictions, axis = 1)\nfig = plt.figure(figsize = (20,40))\n\nfor i, LAB in enumerate(Predicted_Labels):\n    \n    fig.add_subplot(10, 6, i+1)\n    plt.imshow(random_tensors[i][:][:])\n    plt.axis('off')\n    plt.title('Predicted Label: \\n C{}->{}'.format(LAB , classes[LAB]))\n    ","9ddd62e0":"<a id='1.3'><\/a>\n\n## Tools\n\nThe project utilizes the following dependencies:\n\n- Python 3.5: Tensorflow, Keras, Numpy, Scipy, seaborn, Matplotlib.\n- NVIDIA Geforce GTX1060 GPU, CUDA, CuDNN.","4294bb8b":"<a id='4.1'><\/a>\n\n## Finding the Confusion matrix","bbe5ad82":"<a id='1.4'><\/a>\n\n## Dataset\nThe provided data set has driver images, each taken in a car with a driver doing something in the car (texting, eating, talking on the phone, makeup, reaching behind, etc). This dataset is obtained from [Kaggle](https:\/\/www.kaggle.com\/c\/state-farm-distracted-driver-detection\/data).(State Farm Distracted Driver Detection competition).\n\nFollowing are the file descriptions and URL\u2019s from which the data can be obtained :\n\n* imgs.zip - zipped folder of all (train\/test) images\n* sample_submission.csv - a sample submission file in the correct format\n* driver_imgs_list.csv - a list of training images, their subject (driver) id, and class id","33ace0d0":"<a id='2.4'><\/a>\n## Converting into numerical values","ec6171be":"<a id='2.7'><\/a>\n\n## Splitting datasets","0fef083b":"<a id='3.4'><\/a>\n\n## Callbacks","bb41f7aa":"<a id='3.2'><\/a>\n\n## Plotting the Model architecture","02ebc9f7":"<a id='1.2'><\/a>\n## Problem Statment \n\nGiven a dataset of 2D dashboard camera images, an algorithm needs to be developed to classify each driver's behaviour and determine if they are driving attentively, wearing their seatbelt, or taking a selfie with their friends in the backseat etc..? This can then be used to automatically detect drivers engaging in distracted behaviours from dashboard cameras.\n\nFollowing are needed tasks for the development of the algorithm:\n1. Download and preprocess the driver images.\n1. Build and train the model to classify the driver images.\n1. Test the model and further improve the model using different techniques.","784d85d0":"#### Observation\n1. There are total 22424 training samples\n2. There are total 79726 testing samples\n3. The training dataset is equally balanced to a great extent and hence we need not do any downsampling of the data","0c8b1545":"<a id='1.0'><\/a>\n\n# PROJECT DESCRIBTION","312a44d0":"<a id='4.0'><\/a>\n\n# ANALYZING THE MODEL","26608c92":"<a id='2.6'><\/a>\n\n## Converting into categorical values","f8494e12":"<a id='2.9'><\/a>\n\n## Load training and validation images","d325a168":"<a id='3.0'><\/a>\n\n# MODEL","d4b95f6a":"<a id='3.3'><\/a>\n\n## Compiling the Model","e8ecace0":"<a id='2.2'><\/a>\n\n## Defining the train,test and model directories\nWe will create the directories for train,test and model training paths if not present","ecb62be8":"<a id='2.1'><\/a>\n\n## Import libraries that will be used","d54f43cd":"<a id='3.5'><\/a>\n\n## Training the Model","7e9ff1ec":"<a id='1.5'><\/a>\n\n## Implementation\n\nThis project aims to develop a machine learning system that can detect and classify different distracted states of car drivers. The main approach is to apply deep convolutional neural networks (CNNs). We will explore and experiment various CNN architectures, leveraged pre-trained networks (learning transfer).\n\n#### Transfer Learning\n\nMost of the time you won't want to train a whole convolutional network yourself. Modern ConvNets training on huge datasets like ImageNet take weeks on multiple GPUs. \n\nInstead, most people use a pretrained network either as a fixed feature extractor, or as an initial network to fine tune. \n\nIn this notebook, you'll be using [VGGNet] trained on the [ImageNet dataset] as a feature extractor. Below is a diagram of the VGGNet architecture, with a series of convolutional and maxpooling layers, then three fully-connected layers at the end that classify the 1000 classes found in the ImageNet database.\n\n<img src=\"..\/input\/vgg-pic\/vgg_16_architecture.png\" width=\"700px\">\n\n#!img[VGG Architecture]\nVGGNet is great because it's simple and has great performance, coming in second in the ImageNet competition. The idea here is that we keep all the convolutional layers, but **replace the final fully-connected layer** with our own classifier. This way we can use VGGNet as a *fixed feature extractor* for our images then easily train a simple classifier on top of that. \n\n\n\n","9628a525":"\n## The 10 classes to predict\n\n- c0: safe driving\n- c1: texting - right\n- c2: talking on the phone - right\n- c3: texting - left\n- c4: talking on the phone - left\n- c5: operating the radio\n- c6: drinking\n- c7: reaching behind\n- c8: hair and makeup\n- c9: talking to passenger\n\nThere are 22423 train images and 79725 test images.","77375af9":"<a id='2.8'><\/a>\n\n## Converting into 224*224 images \n**NOTE:**\nYou can substitute 64,64 to 224,224 for better results only if ram is >32gb","e9a59ee4":"<a id='3.1'><\/a>\n\n## Defining the Model","78bc8ed1":"<a id='4.2'><\/a>\n\n## Finding the Precision ,Recall and F1 score","955f8823":"<a id='5.0'><\/a>\n\n# Testing The Un-labedled Dataset","9f0084ea":"<a id='1.1'><\/a>\n\n## Introduction\n\n<p style = \"font:Courier\">According to the World Health Organization (WHO) survey, 1.3 million people worldwide die in traffic accidents each year, making them the eighth leading cause of death and an additional 20-50 millions are injured\/ disabled. As per the report of National Crime Research Bureau (NCRB), Govt. of India, Indian roads account for the highest fatalities in the world. There has been a continuous increase in road crash deaths in India since 2006. The report also states that the total number of deaths have risen to 1.46 lakhs in 2015 and driver error is the most common cause behind these traffic accidents.The number of accidents because of distracted driver has been increasing since few years.<\/p>\n\n<p style = 'Courier'>National Highway Traffic Safety Administrator of United States (NHTSA) reports deaths of 3477 people and injuries to 391000 people in motor vehicle crashes because of distracted drivers in 2015. In the United States, everyday approximately 9 people are killed and more than 1,000 are injured in road crashes that are reported to involve a distracted driver. NTHSA describes distracted driving as \u201cany activity that diverts attention of the driver from the task of driving\u201d which can be classified into Manual, Visual or Cognitive distraction. As per the definitions of Center for Disease Control and Prevention (CDC), cognitive distraction is basically \u201cdriver\u2019s mind is off the driving\u201d. <\/p>\n\n<p style = 'Courier'>n other words, even though the driver is in safe driving posture, he is mentally distracted from the task of driving. He might be lost in thoughts, daydreaming etc. Distraction because of inattention, sleepiness, fatigue or drowsiness falls into visual distraction class where \u201cdrivers\u2019s eyes are off the road\u201d. Manual distractions are concerned with various activities where \u201cdriver\u2019s hands are off the wheel\u201d. Such distractions include talking or texting using mobile phones, eating and drinking, talking to passengers in the vehicle, adjusting the radio, makeup etc.Nowadays, Advanced Driver Assistance Systems (ADAS) are being developed to prevent accidents by offering technologies that alert the driver to potential problems and to keep the car\u2019s driver and occupants safe if an accident does occur. But even today\u2019s latest autonomous vehicles require the driver to be attentive and ready to take the control of the wheel back in case of emergency.<\/p>\n\n<p style = 'Courier'>Tesla autopilot\u2019s crash with the white truck-trailor in Williston, Florida in May 2016 was the first fatal crash in testing of autonomous vehicle. Recently in March 2018, Uber\u2019s self driving car with an emergency backup driver behind the wheel struck and killed a pedestrian in Arizona. In both of these fatalities, the safety driver could have avoided the crashes but evidences reveal that he was clearly distracted. This makes detection of distracted driver an essential part of the self driving cars as well. We believe that distracted driver detection is utmost important for further preventive measures. If the vehicle could detect such distractions and then warn the driver against it, number of road crashes can be reduced. In this projects, we focus on detecting manual distractions where driver is engaged in other activities than safe driving and also identify the cause of distraction. We present a Convolutional Neural Network based approach for this problem. We also attempt to reduce the computational complexity and memory requirement while maintaining good accuracy which is desirable in real time applications.<\/p>","c2388312":"<a id='2.0'><\/a>\n\n# PREPARING","599b4f50":"We will create a csv file having the location of the files present for training and test images and their associated class if present so that it is easily traceable.","cf39b2d4":"<a id='2.3'><\/a>\n## Data Preparation","08d4ca77":"# Driver Distraction Detection (DDD) Project.\n\n# TABLE OF CONTENTS\n1. [PROJECT DESCRIBTION](#1.0)\n   - [Introduction](#1.1)\n   - [Problem Statment ](#1.2)\n   - [Tools](#1.3)\n   - [Dataset](#1.4)\n   - [Implementation](#1.5)\n   \n2. [PREPARING](#2.0) \n   - [Import libraries that will be used](#2.1)\n   - [Defining the train,test and model directories](#2.2)\n   - [Data Preparation](#2.3)\n   - [Converting into numerical values](#2.4)\n   - [Use pickle to save a dictionary](#2.5)\n   - [Converting into categorical values](#2.6)\n   - [Splitting datasets](#2.7)\n   - [Converting into 64*64 images](#2.8)\n\n3. [MODEL](#3.0)\n    - [Defining the model](#3.1)\n    - [Plotting the Model architecture](#3.2)\n    - [Compiling the Model](#3.3)\n    - [Callbacks](#3.4)\n    - [Training the Model](#3.5)\n    - [Visulaizing the Model history](#3.6)\n    \n4. [\u0650ANALYZING THE MODEL](#4.0)\n   - [Finding the Confusion matrix](#4.1)\n   - [Finding the Precision ,Recall and F1 score](#4.2)\n\n5. [Testing The Un-labedled Dataset](#5.0)\n","a67677e3":"<a id='3.6'><\/a>\n\n## Visulaizing the Model history","a16c2b93":"<a id='2.5'><\/a>\n## Use pickle to save a dictionary"}}