{"cell_type":{"88905fce":"code","da9b1137":"code","150ac9b0":"code","1ae5b5c6":"code","fedda253":"code","dd1d886f":"code","56526155":"code","76dbe6dd":"code","c97da3ca":"code","f9253d3a":"code","0ba22923":"code","6b5ff4ef":"code","658f85c3":"code","baf8229b":"code","38535abe":"code","7322082c":"code","a9eeb06c":"code","99c6197a":"code","65b3b4a4":"code","0c418736":"code","8aff1527":"code","ac9b1b30":"code","4948bb4f":"code","fe784de6":"code","9980f14b":"code","c5e671d3":"code","391d9279":"code","6cf25f49":"code","ebbd4750":"code","e098310d":"code","0c8c35b9":"code","55593def":"code","410f5208":"code","efc71a28":"code","8f6a2610":"code","fe151608":"code","ead222ec":"code","7dc0a057":"code","af0fe25b":"code","01374d5c":"code","8439ac39":"code","ddf9048d":"code","bd6a359f":"code","097320a6":"code","d947808f":"code","f974792b":"code","dab956b6":"code","54fc2794":"code","3884a482":"code","a395d3ef":"code","d436c45f":"code","749fc163":"code","19206399":"code","ddd0b9ce":"code","ea1b385b":"code","63c802b4":"code","e5902593":"code","8330d522":"code","bbbf9e37":"code","8ee855ae":"code","1bd92395":"code","aa7dc3e9":"code","a49a3195":"code","57c38f27":"code","49f2ef94":"code","0a631586":"code","c5e26423":"code","075f1ced":"code","1d78065f":"code","1867344c":"markdown","220ab4f9":"markdown","d8ec2bd4":"markdown","da6bd111":"markdown","6787ee37":"markdown","4d33b53d":"markdown","d4470fd7":"markdown","4d42e5e7":"markdown","f1432afa":"markdown","bd58cde7":"markdown","465a2017":"markdown","94a9e5c5":"markdown","9a7c3cc1":"markdown","43b2ae4e":"markdown","62049790":"markdown","02ed724c":"markdown","31eb2a2f":"markdown","37f0f7d9":"markdown","909b91fb":"markdown","44eb2957":"markdown","6734d108":"markdown","b1cc597e":"markdown","e06d70d6":"markdown","a58e21c1":"markdown","39684850":"markdown","c72a74a2":"markdown","7b7854ff":"markdown","f12a4170":"markdown","a89c43e6":"markdown","f331e5ba":"markdown","367b3622":"markdown","a239bd9e":"markdown","74e529bb":"markdown","53c35108":"markdown","cd80e73e":"markdown","347929ed":"markdown","54bf9cad":"markdown","4e69392e":"markdown","e54e4965":"markdown","7405a8c4":"markdown","327daec3":"markdown","1196dbec":"markdown","88a0a9b8":"markdown","8b21dcec":"markdown","8c198900":"markdown","10a1bf45":"markdown","6c56b793":"markdown","b76025d5":"markdown","1339f646":"markdown","5bbbe087":"markdown","f8382dba":"markdown","d3daffe3":"markdown","0355199c":"markdown","59a6af2e":"markdown","8191c35e":"markdown","23eabc4f":"markdown","8d4c04a1":"markdown","4101a957":"markdown","ce40588d":"markdown","a75de8a4":"markdown","90a9ef2a":"markdown","4986e68f":"markdown","eb31dc9a":"markdown","0803397b":"markdown","36e914e0":"markdown","79da529c":"markdown","4bbf6eff":"markdown","61fa01b2":"markdown","702c816f":"markdown","e54f1ee1":"markdown","6ec37fb5":"markdown","e2edf484":"markdown","6b138c7d":"markdown","56aa59bb":"markdown","55190daf":"markdown","fa09fb46":"markdown","8f099c7f":"markdown","819ea03e":"markdown","68ccd495":"markdown","edc31dc4":"markdown","9ad2bfee":"markdown","e767da49":"markdown","61534958":"markdown","2d47004a":"markdown","0b87e905":"markdown"},"source":{"88905fce":"# for data manipulation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', 60)\n\n# for visualization\nfrom IPython.core.pylabtools import figsize\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n# to include graphs inline within the frontends next to code\nimport seaborn as sns\nsns.set_context(font_scale=2)\n\n# to bypass warnings in various dataframe assignments\npd.options.mode.chained_assignment = None\n\n# machine learning models\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# preprocessing functions and evaluation models\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.preprocessing import StandardScaler\n\n# Input data files are available in the \"..\/input\/\" directory.\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","da9b1137":"trees = pd.read_csv(\"\/kaggle\/input\/learn-together\/train.csv\")\nprint(\"Number of rows and columns in the trees dataset are:\", trees.shape)","150ac9b0":"trees.head()","1ae5b5c6":"trees.tail()","fedda253":"display(trees.info())","dd1d886f":"display(trees.describe())","56526155":"def outlier_function(df, col_name):\n    ''' this function detects first and third quartile and interquartile range for a given column of a dataframe\n    then calculates upper and lower limits to determine outliers conservatively\n    returns the number of lower and uper limit and number of outliers respectively\n    '''\n    first_quartile = np.percentile(np.array(df[col_name].tolist()), 25)\n    third_quartile = np.percentile(np.array(df[col_name].tolist()), 75)\n    IQR = third_quartile - first_quartile\n                      \n    upper_limit = third_quartile+(3*IQR)\n    lower_limit = first_quartile-(3*IQR)\n    outlier_count = 0\n                      \n    for value in df[col_name].tolist():\n        if (value < lower_limit) | (value > upper_limit):\n            outlier_count +=1\n    return lower_limit, upper_limit, outlier_count","76dbe6dd":"# loop through all columns to see if there are any outliers\nfor column in trees.columns:\n    if outlier_function(trees, column)[2] > 0:\n        print(\"There are {} outliers in {}\".format(outlier_function(trees, column)[2], column))","c97da3ca":"trees = trees[(trees['Horizontal_Distance_To_Fire_Points'] > outlier_function(trees, 'Horizontal_Distance_To_Fire_Points')[0]) &\n              (trees['Horizontal_Distance_To_Fire_Points'] < outlier_function(trees, 'Horizontal_Distance_To_Fire_Points')[1])]\ntrees.shape","f9253d3a":"# list of columns of wilderness areas and soil types\nis_binary_columns = [column for column in trees.columns if (\"Wilderness\" in column) | (\"Soil\" in column)]\npd.unique(trees[is_binary_columns].values.ravel())","0ba22923":"# sum of all widerness area columns\ntrees[\"w_sum\"] = trees[\"Wilderness_Area1\"] + trees[\"Wilderness_Area2\"] + trees[\"Wilderness_Area3\"] + trees[\"Wilderness_Area4\"]\nprint(trees.w_sum.value_counts())","6b5ff4ef":"# create a list of soil_type columns\nsoil_columns = [c for c in trees.columns if \"Soil\" in c]\ntrees[\"soil_sum\"] = 0\n\n# sum of all soil type columns\nfor c in soil_columns:\n    trees[\"soil_sum\"] += trees[c]\n\nprint(trees.soil_sum.value_counts())","658f85c3":"trees.drop(columns=[\"w_sum\", \"soil_sum\"], inplace=True)","baf8229b":"# set the plot size\nfigsize(14,10)\n\n# set the histogram, mean and median\nsns.distplot(trees[\"Cover_Type\"], kde=False)\nplt.axvline(x=trees.Cover_Type.mean(), linewidth=3, color='g', label=\"mean\", alpha=0.5)\nplt.axvline(x=trees.Cover_Type.median(), linewidth=3, color='y', label=\"median\", alpha=0.5)\n\n# set title, legends and labels\nplt.xlabel(\"Cover_Type\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Trees\/Labels\/Cover_Types\", size=14)\nplt.legend([\"mean\", \"median\"])","38535abe":"# Create one column as Wilderness_Area_Type and represent it as categorical data\ntrees['Wilderness_Area_Type'] = (trees.iloc[:, 11:15] == 1).idxmax(1)\n\n#list of wilderness areas\nwilderness_areas = sorted(trees['Wilderness_Area_Type'].value_counts().index.tolist())\n\n# distribution of the cover type in different wilderness areas\nfigsize(14,10)\n\n# plot cover_type distribution for each wilderness area\nfor area in wilderness_areas:\n    subset = trees[trees['Wilderness_Area_Type'] == area]\n    sns.kdeplot(subset[\"Cover_Type\"], label=area, linewidth=2)\n\n# set title, legends and labels\nplt.ylabel(\"Density\")\nplt.xlabel(\"Cover_Type\")\nplt.title(\"Density of Cover Types Among Different Wilderness Areas\", size=14)","7322082c":"def split_numbers_chars(row):\n    '''This function fetches the numerical characters at the end of a string\n    and returns alphabetical character and numerical chaarcters respectively'''\n    head = row.rstrip('0123456789')\n    tail = row[len(head):]\n    return head, tail\n\ndef reverse_one_hot_encode(dataframe, start_loc, end_loc, numeric_column_name):\n    ''' this function takes the start and end location of the one-hot-encoded column set and numeric column name to be created as arguments\n    1) transforms one-hot-encoded columns into one column consisting of column names with string data type\n    2) splits string column into the alphabetical and numerical characters\n    3) fetches numerical character and creates numeric column in the given dataframe\n    '''\n    dataframe['String_Column'] = (dataframe.iloc[:, start_loc:end_loc] == 1).idxmax(1)\n    dataframe['Tuple_Column'] = dataframe['String_Column'].apply(split_numbers_chars)\n    dataframe[numeric_column_name] = dataframe['Tuple_Column'].apply(lambda x: x[1]).astype('int64')\n    dataframe.drop(columns=['String_Column','Tuple_Column'], inplace=True)","a9eeb06c":"reverse_one_hot_encode(trees, 16, 56, \"Soil_Type\")","99c6197a":"# plot relationship of soil type and cover type among different wilderness areas\ng = sns.FacetGrid(trees, col=\"Wilderness_Area_Type\", \n                  col_wrap=2, height=5, col_order=wilderness_areas)\ng = g.map(plt.scatter,\"Cover_Type\", \"Soil_Type\", edgecolor=\"w\", color=\"g\")","65b3b4a4":"# store continious variables in a list\ncontinuous_variables = trees.columns[1:11].tolist()\n\n# Function to calculate correlation coefficient between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20)","0c418736":"# Create the pairgrid object\ngrid = sns.PairGrid(data = trees[continuous_variables])\n\n# Upper is a correlation and kdeplot\ngrid.map_upper(corr_func);\ngrid.map_upper(sns.kdeplot, cmap = plt.cm.Greens)\n\n# Diagonal is a histogram\ngrid.map_diag(plt.hist, color = 'green', edgecolor = 'white')\n\n# Bottom is scatter plot\ngrid.map_lower(plt.scatter, color = 'green', alpha = 0.1)","8aff1527":"figsize(24,10)\n\n# plot the first subplot\nplt.subplot(1,2,1)\nsns.scatterplot(x=\"Vertical_Distance_To_Hydrology\", y=\"Horizontal_Distance_To_Hydrology\", \n                hue=\"Cover_Type\", data=trees, \n                legend=\"full\", hue_norm=(0,8), palette=\"Set1\")\nplt.title(\"Vertical_Distance_To_Hydrology VS Horizontal_Distance_To_Hydrology\", size=14)\n\n# plot the second subplot\nplt.subplot(1,2,2)\nsns.scatterplot(x=\"Elevation\", y=\"Slope\", \n                hue=\"Cover_Type\", data=trees, \n                legend=\"full\", hue_norm=(0,8), palette=\"Set1\")\nplt.title(\"Elevation VS Slope\", size=14)","ac9b1b30":"figsize(24,10)\n\n# plot the first subplot\nplt.subplot(1,2,1)\nsns.scatterplot(x=\"Hillshade_Noon\", y=\"Hillshade_3pm\", \n                hue=\"Cover_Type\", data=trees, \n                legend=\"full\", hue_norm=(0,8), palette=\"Set1\")\nplt.title(\"Hillshade_Noon VS Hillshade_3pm\", size=14)\n\n# plot the second subplot\nplt.subplot(1,2,2)\nsns.scatterplot(x=\"Hillshade_9am\", y=\"Hillshade_3pm\", \n                hue=\"Cover_Type\", data=trees, \n                legend=\"full\", hue_norm=(0,8), palette=\"Set1\")\nplt.title(\"Hillshade_9am VS Hillshade_3pm\", size=14)","4948bb4f":"plt.figure(figsize=(14,12))\n\n# plot heatmap set the title\ncolormap = plt.cm.RdBu\nsns.heatmap(trees.corr(),linewidths=0.1,vmax=1.0, \n            square=False, cmap=colormap, linecolor='white', annot=False)\nplt.title('Pearson Correlation of All Features', y=1.05, size=14)","fe784de6":"# make a list of numeric features and create a dataframe with them\nall_features_w_label = continuous_variables + wilderness_areas + [\"Soil_Type\"] + [\"Cover_Type\"]\ntrees_w_numeric_soil = trees[all_features_w_label]\n\n# pearson coefficients with numeric soil type column\ncorrelations = pd.DataFrame(trees_w_numeric_soil.corr())\n\nfigsize=(16,14)\n\n# plot the heatmap\ncolormap = plt.cm.RdBu\nsns.heatmap(correlations,linewidths=0.1, \n            square=False, cmap=colormap, linecolor='white', annot=True)\nplt.title('Pearson Correlation of Features with Numeric Soil_Type', size=14)","9980f14b":"# add columns\ntrees_w_numeric_soil['Euclidian_Distance_To_Hydrology'] = (trees_w_numeric_soil['Horizontal_Distance_To_Hydrology']**2 + \n                                                           trees_w_numeric_soil['Vertical_Distance_To_Hydrology']**2)**0.5\ntrees_w_numeric_soil['Mean_Elevation_Vertical_Distance_Hydrology'] = (trees_w_numeric_soil['Elevation'] + \n                                                                      trees_w_numeric_soil['Vertical_Distance_To_Hydrology'])\/2\ntrees_w_numeric_soil['Mean_Distance_Hydrology_Firepoints'] = (trees_w_numeric_soil['Horizontal_Distance_To_Hydrology'] + \n                                                              trees_w_numeric_soil['Horizontal_Distance_To_Fire_Points'])\/2\ntrees_w_numeric_soil['Mean_Distance_Hydrology_Roadways'] = (trees_w_numeric_soil['Horizontal_Distance_To_Hydrology'] + \n                                                            trees_w_numeric_soil['Horizontal_Distance_To_Roadways'])\/2\ntrees_w_numeric_soil['Mean_Distance_Firepoints_Roadways'] = (trees_w_numeric_soil['Horizontal_Distance_To_Fire_Points'] + \n                                                             trees_w_numeric_soil['Horizontal_Distance_To_Roadways'])\/2","c5e671d3":"# add sqrt transformed columns to the trees_w_numeric_soil dataframe\nfor col in trees_w_numeric_soil.columns:\n    if trees_w_numeric_soil[col].min() >= 0:\n        if col == 'Cover_Type':\n            next\n        else:\n            trees_w_numeric_soil['sqrt' + col] = np.sqrt(trees_w_numeric_soil[col])","391d9279":"correlations_transformed = pd.DataFrame(trees_w_numeric_soil.corr())\ncorrelations_transformed = pd.DataFrame(correlations_transformed[\"Cover_Type\"]).reset_index()\n\n# format, and display sorted correlations_transformed\ncorrelations_transformed.columns = [\"Feature\", \"Correlation with Cover_Type\"]\ncorrelations_transformed = (correlations_transformed[correlations_transformed[\"Feature\"] != \"Cover_Type\"]\n                .sort_values(by=\"Correlation with Cover_Type\", ascending=True))\ndisplay(correlations_transformed)","6cf25f49":"# final list of features\ntransformed_features = ['sqrtHorizontal_Distance_To_Hydrology', 'sqrtMean_Distance_Hydrology_Roadways', 'sqrtEuclidian_Distance_To_Hydrology', \n                        'Mean_Elevation_Vertical_Distance_Hydrology', 'Mean_Distance_Firepoints_Roadways', 'Mean_Distance_Hydrology_Firepoints',  ]\n\nall_features =  (['Elevation', 'Aspect', 'Slope', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', \n                  'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points' ] + wilderness_areas +\n                 ['Soil_Type'] + transformed_features)","ebbd4750":"trees_training = trees_w_numeric_soil[all_features]\nlabels_training = trees_w_numeric_soil[\"Cover_Type\"].as_matrix()","e098310d":"X_train, X_valid, y_train, y_valid = train_test_split(trees_training, labels_training, test_size=0.2, random_state=1)","0c8c35b9":"print('Training Data Shape:', X_train.shape)\nprint('Validation Data Shape:', X_valid.shape)","55593def":"print('Training Label Shape:', y_train.shape)\nprint('Validation Label Shape:', y_valid.shape)","410f5208":"# Create dummy classifer\ndummy = DummyClassifier(strategy='stratified', random_state=1)\n\n# train the model\ndummy.fit(X_train, y_train)\n\n# Get accuracy score\nbaseline_accuracy = dummy.score(X_valid, y_valid)\nprint(\"Our dummy algorithm classified {:0.2f} of the of the trees correctly\".format(baseline_accuracy))","efc71a28":"# create scaler\nscaler = StandardScaler()\n\n# apply normalization to training set and transform training set\nX_train_scaled = scaler.fit_transform(X_train, y_train)\n\n# transform validation set\nX_valid_scaled = scaler.transform(X_valid)","8f6a2610":"# function to train a given model, generate predictions, and return accuracy score\ndef fit_evaluate_model(model, X_train, y_train, X_valid, Y_valid):\n    model.fit(X_train, y_train)\n    y_predicted = model.predict(X_valid)\n    return accuracy_score(y_valid, y_predicted)","fe151608":"# create model apply fit_evaluate_model\nknn_classifier = KNeighborsClassifier()\nknn_accuracy = fit_evaluate_model(knn_classifier, X_train_scaled, y_train, X_valid_scaled, y_valid)\nprint(\"Number of correct predictions made out of all predictions are:\", knn_accuracy)","ead222ec":"# create model apply fit_evaluate_model\nlgbm_classifier = LGBMClassifier()\nlgbm_accuracy = fit_evaluate_model(lgbm_classifier, X_train_scaled, y_train, X_valid_scaled, y_valid)\nprint(\"Number of correct predictions made out of all predictions are:\", lgbm_accuracy)","7dc0a057":"# create model apply fit_evaluate_model\nrf_classifier = RandomForestClassifier()\nrf_accuracy = fit_evaluate_model(rf_classifier, X_train, y_train, X_valid, y_valid)\nprint(\"Number of correct predictions made out of all predictions are:\", rf_accuracy)","af0fe25b":"# create model apply fit_evaluate_model\nxrf_classifier = ExtraTreesClassifier()\nxrf_accuracy = fit_evaluate_model(xrf_classifier, X_train, y_train, X_valid, y_valid)\nprint(\"Number of correct predictions made out of all predictions are:\", xrf_accuracy)","01374d5c":"# create model apply fit_evaluate_model\nxgb_classifier = XGBClassifier()\nxgb_accuracy = fit_evaluate_model(xgb_classifier, X_train, y_train, X_valid, y_valid)\nprint(\"Number of correct predictions made out of all predictions are:\", xgb_accuracy)","8439ac39":"# create dataframe of accuracy and model and sort values\nperformance_comparison = pd.DataFrame({\"Model\": [\"K-Nearest Neighbor\", \"LightGBM\", \"Random Forests\", \"Extra Trees\", \"XGBoost\"],\n                                       \"Accuracy\": [knn_accuracy, lgbm_accuracy, rf_accuracy, xrf_accuracy, xgb_accuracy]})\n\nperformance_comparison = performance_comparison.sort_values(by=\"Accuracy\", ascending=True)\n\n# set the plot\nplt.figure(figsize=(10,10))\nax = sns.barplot(x=\"Accuracy\", y=\"Model\", data=performance_comparison, palette=\"Greens_d\")\n\n# set title arrange labels\nplt.yticks(size = 14)\nplt.xticks(size = 14)\nplt.title(\"Accuracy Score of Different Models\", size=14)","ddf9048d":"# The number of trees in the forest algorithm, default value is 10.\nn_estimators = [50, 100, 300, 500, 1000]\n\n# The minimum number of samples required to split an internal node, default value is 2.\nmin_samples_split = [2, 3, 5, 7, 9]\n\n# The minimum number of samples required to be at a leaf node, default value is 1.\nmin_samples_leaf = [1, 2, 4, 6, 8]\n\n# The number of features to consider when looking for the best split, default value is auto.\nmax_features = ['auto', 'sqrt', 'log2', None] \n\n# Define the grid of hyperparameters to search\nhyperparameter_grid = {'n_estimators': n_estimators,\n                       'min_samples_leaf': min_samples_leaf,\n                       'min_samples_split': min_samples_split,\n                       'max_features': max_features}","bd6a359f":"# create model\nbest_model = ExtraTreesClassifier(random_state=42)\n\n# create Randomized search object\nrandom_cv = RandomizedSearchCV(estimator=best_model,\n                               param_distributions=hyperparameter_grid,\n                               cv=5, n_iter=20, \n                               scoring = 'accuracy',\n                               n_jobs = -1, verbose = 1, \n                               return_train_score = True, \n                               random_state=42)","097320a6":"# Fit on the all training data using random search object\nrandom_cv.fit(trees_training, labels_training)","d947808f":"random_cv.best_estimator_","f974792b":"xrf_classifier_w_random_search = ExtraTreesClassifier(n_estimators=300, \n                                                     max_features=None, \n                                                     min_samples_leaf=1, \n                                                     min_samples_split=2,\n                                                     random_state=42)\n\nxrf_accuracy_opt_w_rand_search = fit_evaluate_model(xrf_classifier_w_random_search, X_train, y_train, X_valid, y_valid)","dab956b6":"print(\"Accuracy score in the previous extra random forests model:\", xrf_accuracy)\nprint(\"Accuracy score after hyperparameter tuning:\", xrf_accuracy_opt_w_rand_search)","54fc2794":"# Create a range of trees to evaluate\ntrees_grid = {'n_estimators': [300, 500, 700, 900, 1200, 1500]}\n\n# define all parameters except n_estimators\nxrf_classifier_w_grid_search = ExtraTreesClassifier(max_features=None, \n                                                    min_samples_leaf=1, \n                                                    min_samples_split=2,\n                                                    random_state=42)\n\n# Grid Search Object using the trees range, the model and 5-fold cross validation\ngrid_search = GridSearchCV(estimator = xrf_classifier_w_grid_search, param_grid=trees_grid, \n                           cv = 5, scoring = 'accuracy', verbose = 1,\n                           n_jobs = -1, return_train_score = True)","3884a482":"# fit the dataset to grid search object\ngrid_search.fit(trees_training, labels_training)","a395d3ef":"# Get the results into a dataframe\nxrf_results = pd.DataFrame(grid_search.cv_results_)\n\n# Plot the training and testing error vs number of trees\nplt.figure(figsize=(10,10))\nplt.plot(xrf_results['param_n_estimators'], xrf_results['mean_test_score'], label = 'Testing Accuracy')\nplt.plot(xrf_results['param_n_estimators'], xrf_results['mean_train_score'], label = 'Training Accuracy')\n\n# set title, labels and legend\nplt.xlabel('Number of Estimators(Trees)'); plt.ylabel('Accuracy'); plt.legend();\nplt.title('Performance vs Number of Trees', size=14);","d436c45f":"xrf_results[[\"param_n_estimators\", \"params\", \"mean_test_score\"]].sort_values(by=\"mean_test_score\", ascending=False)","749fc163":"xrf_optimal_model = ExtraTreesClassifier(n_estimators=500, \n                                           max_features=None, \n                                           min_samples_leaf=1, \n                                           min_samples_split=2,\n                                           random_state=42)\n\nxrf_optimal_model_accuracy = fit_evaluate_model(xrf_optimal_model, X_train, y_train, X_valid, y_valid)","19206399":"print(\"Accuracy score with random forests model when n_estimators=300:\", xrf_accuracy_opt_w_rand_search)\nprint(\"Accuracy score with random forests model when n_estimators=500:\", xrf_optimal_model_accuracy)","ddd0b9ce":"# create set of y_predictions\ny_predicted = xrf_optimal_model.predict(X_valid)","ea1b385b":"# make a list of cover_types\ncover_types = sorted(trees['Cover_Type'].value_counts().index.tolist())","63c802b4":"def plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        # print(\"Normalized confusion matrix\")\n    # else:\n        # print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n","e5902593":"plot_confusion_matrix(y_valid, y_predicted, classes=cover_types, normalize=True,\n                      title='Normalized confusion matrix')\nplt.show()","8330d522":"trees_test = pd.read_csv(\"\/kaggle\/input\/learn-together\/test.csv\")","bbbf9e37":"# add numeric soil type column\nreverse_one_hot_encode(trees_test, 16, 56, \"Soil_Type\")","8ee855ae":"# add linear combinations of columns\ntrees_test['Euclidian_Distance_To_Hydrology'] = (trees_test['Horizontal_Distance_To_Hydrology']**2 + \n                                                 trees_test['Vertical_Distance_To_Hydrology']**2)**0.5\ntrees_test['Mean_Elevation_Vertical_Distance_Hydrology'] = (trees_test['Elevation'] + \n                                                            trees_test['Vertical_Distance_To_Hydrology'])\/2\ntrees_test['Mean_Distance_Hydrology_Firepoints'] = (trees_test['Horizontal_Distance_To_Hydrology'] + \n                                                    trees_test['Horizontal_Distance_To_Fire_Points'])\/2\ntrees_test['Mean_Distance_Hydrology_Roadways'] = (trees_test['Horizontal_Distance_To_Hydrology'] + \n                                                  trees_test['Horizontal_Distance_To_Roadways'])\/2\ntrees_test['Mean_Distance_Firepoints_Roadways'] = (trees_test['Horizontal_Distance_To_Fire_Points'] + \n                                                   trees_test['Horizontal_Distance_To_Roadways'])\/2","1bd92395":"# transfrom columns \ntrees_test['sqrt' + 'Horizontal_Distance_To_Hydrology'] = np.sqrt(trees_test['Horizontal_Distance_To_Hydrology'])\ntrees_test['sqrt' + 'Mean_Distance_Hydrology_Roadways'] = np.sqrt(trees_test['Mean_Distance_Hydrology_Roadways'])\ntrees_test['sqrt' + 'Euclidian_Distance_To_Hydrology'] = np.sqrt(trees_test['Euclidian_Distance_To_Hydrology'])","aa7dc3e9":"X_test = trees_test[all_features]\nprint(X_test.columns)","a49a3195":"print('Test Data Shape:', X_test.shape)","57c38f27":"print(X_test.isnull().sum())","49f2ef94":"# generate predictions for test data\ntest_predictions = xrf_optimal_model.predict(X_test)","0a631586":"# write results to the dataframe and create file for submission\noutput = pd.DataFrame({'Id': trees_test[\"Id\"],\n                       'Cover_Type': test_predictions})\noutput.to_csv('submission.csv', index=False)","c5e26423":"#create list of features\nfeatures = list(trees_training.columns)\n\n# Extract the feature importances into a dataframe\nfeature_results = pd.DataFrame({'feature': features, \n                                'importance': xrf_optimal_model.feature_importances_})\n\n# Show the top 10 most important\nfeature_results = feature_results.sort_values('importance', ascending = False).reset_index(drop=True)\nfeature_results.head(10)","075f1ced":"correlations_transformed.head(5)","1d78065f":"correlations_transformed.tail(5)","1867344c":"To find the best combination of the randomly set parameters and apply cross validation, I am going to use the `RandomizedSearchCV` with following arguments:\n* `estimator`: the model\n* `param_distributions`: the distribution of parameters we defined\n* `cv`: K in the K-fold cross validation, number of subsets to create\n* `n_iter`: the number of different combinations to try\n* `scoring`: which metric to use when evaluating candidates\n* `n_jobs`: number of cores to run in parallel (-1 will use all available)\n* `verbose`: how much information to display (1 displays a limited amount)\n* `return_train_score`: return the training score for each cross-validation fold\n* `random_state`: fixes the random number generator used so we get the same results every run","220ab4f9":"So, in addition to the existing features, final features will be:\n* Instead of Horizontal_Distance_To_Hydrology, sqrtHorizontal_Distance_To_Hydrology\n* sqrtMean_Distance_Hydrology_Roadways\n* sqrtEuclidian_Distance_To_Hydrology\n* Mean_Elevation_Vertical_Distance_Hydrology\n* Mean_Distance_Firepoints_Roadways\n* Mean_Distance_Hydrology_Firepoints\n\nAdditionally, I will drop Hillshade_9am column since it is strongly correlated with Hillshadde_3pm.","d8ec2bd4":"I am going to use (with default parameters for now) and without discussing specifics of the models:\n\n1) K-Nearest Neighbors Classifier\n\n2) Light Gradient Boosting Machine (LightGBM) Classifier\n\n3) Random Forest Classifier\n\n4) Extra Trees (Random Forests) Classifier\n\n5) Extra Gradient Boosting (XGBoost) Classifier\n\nand compare the results on accuracy score. Then I will select the best model with the highest accuracy score for use.\n\nSince K-nearest neighbors classifier is using [euclidian distance](https:\/\/en.wikipedia.org\/wiki\/Euclidean_distance) to cluster labels, I am going to use normalized training set for those.","da6bd111":"# 4. Compare Several Machine Learning Models","6787ee37":"## 6.1. Align test set with the training set","4d33b53d":"Now it is time to answer the headline!","d4470fd7":"Nother important finding about Fantastic Trees: Wilderness area is an important feature to determine the cover type:\n* Spruce\/Fir, Lodgepole Pine and Krummholz (Cover_Type 1, 2, 7)  mostly found in Rawah, Neota and Comanche Peak Wilderness Area(1,2 and 3).\n* It is highly likely to find Ponderosa Pine (Cover_Type 3) in Cache la Poudre Wilderness Area (4) rather than other areas.\n* Cottonwood\/Willow (Cover_Type 4) seems to be found only in Cache la Poudre Wilderness Area (4).\n* Aspen (Cover_Type 5) is equally likely to come from wilderness area Rawah and Comanche (1,3).\n* Douglas-fir (Cover_Type 6) can be found in any of the wilderness areas.\n\nNote that, distribution of cover types extend more than the range because of the kernel density estimation.","4d42e5e7":"After the hypermeter parameter tuning I increased the accuracy of the model by 2 to 3 points.","f1432afa":"## 4.1. Z-Score normalization for K-Nearest Neighbors and LightGBM","bd58cde7":"Since different soil types might appear in different wilderness areas, I am going to consider different wilderness areas while examining this relationship.","465a2017":"## 2.9. Pearson coefficients with numeric Soil_Type representation","94a9e5c5":"One of the features from the Hillshade_9am or Hillshade_3pm or Hillshade_Noon will be dropped when determining the training set. Which one to be eliminated will be determined after looking at the Pearson Coeffiecients with the label.","9a7c3cc1":"Distribution of fantastic trees shows perfect uniform distribution.\n\nHere are the 7 types of the fantastic trees, numbered from 1 to 7 in the `Cover_Type` column:\n\n1) Spruce\/Fir\n\n2) Lodgepole Pine\n\n3) Ponderosa Pine\n\n4) Cottonwood\/Willow\n\n5) Aspen\n\n6) Douglas-fir\n\n7) Krummholz","43b2ae4e":"**Distributions:**\n* `Hillshade_9am` and `Hillshade_Noon` has bi-modal and left-skewed distributions.\n* `Horizontal_Distance_To_Firepoints`, `Horizontal_Distance_To_Roadways`, `Horizontal_Distance_To_Hydrology` has bi-modal and right-skewed distributions.\n* `Elevation` (height of a fantastic trees) resembles a uniform distribution.\n* `Slope`, `Vertical_Distance_To_Hydrology`, `Hillshade_3pm` shows a symmetric and bi-modal distribution.\n\n**Some obvious relationships between the continuous features:**\n*  `Elevation` and shows positive trend with following variables:\n   * `Vertical_Distance_To_Hydrology`\n   * `Horizontal_Distance_To_Roadways`\n   * `Horizontal_Distance_To_Firepoints`\n   * `Horizontal_Distance_To_Hydrology`\n* As `Aspect` increases; `Hillshade_Noon` and `Hillshade_3pm` increases.\n* `Slope` has negative trend with:\n  * `Elevation`\n  * `Horizontal_Distance_To_Roadways`\n  * `Hillshade_9am`, `Hillshade_Noon` and `Hillshade_3pm`\n  * `Horizontal_Distance_To_Firepoints`\n* `Horizontal_Distance_To_Hydrology`  has positive trend with:\n  * `Horizontal_Distance_To_Firepoints`\n  * `Horizontal_Distance_To_Roadways`\n  * `Vertical_Distance_To_Hydrology`\n* `Vertical_Distance_To_Hydrology` - `Slope` and `Vertical_Distance_To_Hydrology` - `Horizontal_Distance_To_Hydrology` has obvious collinear relationship.\n* As `Horizontal_Distance_To_Roadways` increases, `Horizontal_Distance_To_Firepoints` increases and `Slope` decreases.\n* `Hillshade_9am` shows negative trend with `Hillshade_3pm` and `Aspect`, as `Hillshade_9am` increases `Elevation` increases.\n* `Hillshade_Noon` has positive trend with:\n  * `Elevation`\n  * `Aspect`\n  * `Horizontal_Distance_To_Roadways`\n  * `Hillshade_3pm` \n  * `Horizontal_Distance_To_Firepoints`\n* `Hillshade_3pm` shows perfect negative relationship with `Hillshade_9am` and perfect positive relationship with `Hillshade_Noon`.\n\n**Some Collinear features:**\n* hillshade noon - hillshade 3 pm\n* hillsahde 3 pm - hillshade 9 am\n* vertical distance to hydrology - horizontal distance to hydrology\n* elevation - slope","62049790":"# 5. Perform Hyperparameter Tuning on the Best Model","02ed724c":"### 4.2.1. K-Nearest Neighbor Classifier","31eb2a2f":"Before diving deep into the ML classification algorithms, I am going to calculate a common sense baseline. A common sense baseline is defined in this [article](https:\/\/towardsdatascience.com\/first-create-a-common-sense-baseline-e66dbf8a8a47) in simple terms, how a person has a knowledge in that field would solve the problem without using any data science tricks. Alternatively, as explained in this [post](https:\/\/machinelearningmastery.com\/implement-baseline-machine-learning-algorithms-scratch-python\/), it can be a dummy or simple algorithm, consisting of few lines of code, to use as a baseline metric.\n\nBaseline metrics can be [different](https:\/\/machinelearningmastery.com\/how-to-get-baseline-results-and-why-they-matter\/) in regression and classification problems. Since fantastic trees will be classified into 7 groups and no expert wizards available around, I am going to use [dummy algorithm](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.dummy.DummyClassifier.html) from scikit-learn library. With that dummy algorithm, I will establish a baseline metric of accuracy which is percentage of correctly predicted trees among the test dataset .\n\nBaseline metrics are important in a way that, if a ML model cannot beat the simple and intuitive prediction of a person's or an algorithm's guess, the original problem needs reconsideration or training data needs reframing.","37f0f7d9":"## 2.3. Distribution of the Fantastic Trees","909b91fb":"To understand how data is structured, I am going to look at:\n* First and last rows\n* Information\n* Descriptive statistics of the dataset.\n\nand apply cleaning and formatting afterwards, if necessary.","44eb2957":"Although it is known that gradient boosting algorithm outperforms others by loss, as plotted below heatmap, extreme (extra) random forests outperformed other algorithms with accuracy performance metric in this case. The reason might be, I did not focus on tuning the parameters of the each algorithm and used defaults values instead.\n\n![image](https:\/\/crossinvalidation.files.wordpress.com\/2017\/08\/olson.jpg?w=900)\n\n[Image Credit](https:\/\/crossinvalidation.com\/2017\/08\/22\/quantitative-comparison-of-scikit-learns-predictive-models-on-a-large-number-of-machine-learning-datasets-a-good-start\/)\n\nDo you remember our baseline metric produced by the dummy algorithm (0.14) ? Well, all 4 models beat that intuitive score and showed that machine learning is applicable to the fantastic tree classification problem!","6734d108":"![image](https:\/\/images.ctfassets.net\/usf1vwtuqyxm\/1SCzmQ07UgSmWegc2KWkmu\/8b8bdf0779bc79769f202415be80fc45\/FB-TRL3-87979.jpg?w=914)\n\n\n[Image Credits](https:\/\/www.wizardingworld.com\/features\/which-fantastic-beast-is-right-for-you) ","b1cc597e":"## 2.5. Understanding the Soil_Type and Cover_Type relationship","e06d70d6":"# 7. Interpret Model Results","a58e21c1":"In the data set, the strongest positive pearson coefficient is 0.22 and -0.11 on the other end. After doing some Google Search about the features, maybe adding some features might help achieving stronger correlations. \n\nI decided to add linear combinations of the horizontal distance columns and  Euclidian distance of Horizontal_Distance_To_Hydrology and Vertical_Distance_To_Hydrology as suggested in this [presentation](https:\/\/www.slideshare.net\/danielgribel\/forest-cover-type-prediction-56288946?next_slideshow=2).\n\n* Elevation and Vertical Distance to Hydrology\n* Horizontal Distance to Hydrology and Horizontal Distance to Firepoints\n* Horizontal Distance to Hydrology and Horizontal Distance to Roadways \n* Horizontal Distance to Firepoints and Horizontal Distance to Roadways\n* Euclidian Distance of Horizontal Distance to Hydrology and Vertical Distance to Hydrology\n\nAfter the addition, I will perform square root transformation to the features with positive data range. Square root transformation might help especially for the highly skewed distributions.\n\nAfter the addition and transformation, I will check pearson coefficients again.","39684850":"## 2.1. Check if Wilderness_Area and Soil_Type columns have only binary values","c72a74a2":"There are no NA values in the test data set, which is ready to be inputted in a ML model.\nIn the training set, there are more than 12000 rows, and the test has much more rows (almost 500.000) than the tarining set. Let's see how the model will deal with a much bigger dataset!","7b7854ff":"### 4.2.2. Light Gradient Boosting Machine (LightGBM) Classifier","f12a4170":"## 3.1. Add & Transform Features","a89c43e6":"To help future ML model to grasp patterns in the data better, I am going to search for outliers. During this search, I will use the logic of [extreme outliers](https:\/\/people.richland.edu\/james\/lecture\/m170\/ch03-pos.html) to keep as much rows I can keep. So following data points will be dropped if they satisfy the following conditions:    \n- x < Q1 - 3 * IQR       \n- x > Q3 + 3 * IQR","f331e5ba":"Training accuracy is very 100% percent, showing that the model studied and learned from the training set very well. \n\nWhen it comes to testing accuracy, accuracy drops 20 points, resulting in accuracy level around 80%. This shows that the model is performing worse in a newly-introduced dataset. This picture also gives a clue about the submission score, I expect it to be around 80%.\n\nAnother important message is, There are slight changes in the accuracy for the test set which means number of estimators trees can be improved further. Let's see what is the best n_estimator value.","367b3622":"Model did pretty good detecting fantastic trees of type 3,4, 5, 6 and 7, and it seems a bit confused to detect types 1 and 2.","a239bd9e":"## 8.2. How to Detect Fantastic Trees?","74e529bb":"## 3.4. Create a baseline metric","53c35108":"I am going to search for the best set of parameters with random search and cross validation.\n\nIn random serach, set of ML model's parameters are defined in a range and inputted to `RandomizedSearchCV`. This algorithm randomly selects some combination of the parameters and compares the defined `score` (accuracy, for this problem) with iterations. Random search runtime and iterations can be controlled with the parameter `n_iter`. This is in contrast to grid search iterations of every single combination of the given parameters. With intuition, one can say that, grid search requires more run-time than random search if a small number of n_iterations is defined. Generally, random search is better when there is a limited knowledge and of the best model hyperparameters and less time. \n\nK-fold Cross validation is the method used to assess the performance of the hyperparameters on the whole dataset. Rather than splitting the dataset set into 2 static subsets of training and test, dataset is divided equally for the given K, and with iterations different K subsets are trained and tested. In other words, divide the dataset into K folds, and follow the iterative process where first traininig is done on K-1 of the folds and then evaluate performance on the Kth fold. Process is repeated K times, so eventually dataset is tested on every example keeping in mind that each iteration is testing on a subset that did not train on before. At the end of K-fold cross validation, average of the performance metric on each of the K iterations substitutes the final performance measure.\n\nA visualization of cross-validation:\n\n![image](https:\/\/scikit-learn.org\/stable\/_images\/grid_search_cross_validation.png)\n","cd80e73e":"Here is the definition from the Scikit-Learn [documentation](https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_scaling_importance.html):\n> Feature scaling through standardization (or Z-score normalization) can be an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one. Many algorithms (such as SVM, K-nearest neighbors, and logistic regression) require features to be normalized.","347929ed":"* Wilderness Area 3 is more diverse in soil type and cover type.\n* Only soil types 1 through 20 is represented in Wilderss Area 4, thus cover types in that area grew with them.\n* Cover type 7 seems to grow with soil types 25 through 40.\n* Cover Type 5 and 6 can grow with most of the soil types.\n* Cover Type 3 loves soil type 0 through 15.\n* Cover Type 1 and 2 can grow with any soil type.","54bf9cad":"## 3.2. Seperate labels from features in the training set","4e69392e":"## 6.2. Make sure of the test data shape and there aren't any missing values","e54e4965":"## 8.1. Where to Find Fantastic Trees?","7405a8c4":"### 5.1.2. Possible further improvements with the GridSearch","327daec3":"## 1.2. Information (how many rows and columns, data types and non-null values) and descriptive statistics of the dataset","1196dbec":"Now, I expect that following ML models beat the accuracy score of 0.14!","88a0a9b8":"## 1.1. First and last rows","8b21dcec":"### 4.2.3. Random Forests Classifier","8c198900":"I am going to use the [function](https:\/\/gist.github.com\/shaypal5\/94c53d765083101efc0240d776a23823) mentioned in the sci-kit learn documentation to print confusion_matrix. Confusion matrix will show the number of predictions made in each category with actual and predicted values, by comparing the actual labels and the prediected labels. \n\nFantastic tree confusion matrix will be a 7x7 matrix. I will use normalized confusion matrix, so percentage of actual tree type correctly guessed out of all guesses in that particular category will appear in the diagonal of the matrix and non-diagonal elements will show misslabeled elements by the model. The higher the diagonal percentages of the confusion matrix the better, indicating many correct predictions.","10a1bf45":"`Wilderness_Area` and `Soil_Type1-40` having only binary values and only one `soil_type` or `wilderness_area` being equal to 1, shows that they are one-hot-encoded columns.\n\nOne important thing about fantastic trees are, they can only belong to one soil type or one wilderness area.","6c56b793":"Number of the rows in the dataset is approximately 15000, after the removal.","b76025d5":"None of the features are significantly different effect on determining the label cover type.\n\nOne interesting finding though, Soil Type 7 and 15 columns are blank in the heatmap, thus zero effect on determining the label Cover_Type. \n\nApproximately 5 (1 percent of all soil types) soil_type columns affects the cover type.\n\nCan we get a better picture if we use soil_type as one numeric column rather than seperate one-hot-encoded columns?","1339f646":"## 5.1. Extra Trees Classifier","5bbbe087":"## 2.2. Can one Fantastic Tree belong to multiple soil types and wilderness areas ?","f8382dba":"### 5.1.1. Hyperparameter Tuning with Random Search and Cross-Validation","d3daffe3":"This notebook aims to classify fantastic trees and give some clues about where to find them in the 4 wilderness areas of the Roosevelt National Forest of Northern Colorado! \n\nOur fantastic tree types are 7 in total and labeled  as `cover_type` in the dataset, using the other columns in the data set such as elevation, aspect, slope and some distance measures I will develop a model to differantiate fantastic trees. \n\nThe notebook will follow the workflow suggested by Will Koehrsen in this [article](https:\/\/towardsdatascience.com\/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420).\n    \n1) Undserstand, Clean and Format Data\n\n2) Exploratory Data Analysis\n\n3) Feature Engineering & Selection\n\n4) Compare Several Machine Learning Models\n\n5) Perform Hyperparameter Tuning on the Best Model\n\n6) Evaluate the Best Model with Test Data\n\n7) Interpret Model Results\n\n8) Summary & Conclusions\n\nIf you are dying from curiosity, you can jump directly to *8. Summary & Conclusions*, but I cannot gurantee that you are not going to miss some beautiful visualizations and interesting insights about data science and machine learning. Enjoy Reading!","0355199c":"## 2.7. Visualize some collinear features with Cover_Type","59a6af2e":"## 1.3. Check for Anomalies & Outliers","8191c35e":"## 4.3. Comparison of model performances","23eabc4f":"### 4.2.5. Extra Gradient Boosting (XGBoost) Classifier","8d4c04a1":"## 5.3. Visualization of the best model predictions","4101a957":"To recognize fantastic trees, I analyzed them first, determined and transformed some of their features (like characteristics). \n\nTo classify them, I implemented a extra random forest classifier model (how funny that fantastic trees are classifed with extra random forests model), fine tuned the model and generated predictions. \n\n**Most importantly: **\n\nWith the current workflow, and selection of features, extra random forests model and parameters I succesfully detected more than 75 percent of the fantastic trees correctly!\n<br>(Previously submitted accuracy score was 72% with random forests model)<\/br>\n\nExtra random forests classification showed that:\n\n* elevation\n* soil type\n* mean distance of elevation and vertical distance hydrology\n* wilderness_Area4\n* mean distance of horizontal distance to firepoints and roadways are the most important characteristics of a fantastic tree.\n\nTop 5 features list stressed the importance of feature engineering and selection. 3 of the Top 5 features are created in the scope of this notebook.\n\n![end credits](https:\/\/i.ytimg.com\/vi\/WfvD-JZGlHs\/maxresdefault.jpg)\n\n[Image Credits](https:\/\/www.youtube.com\/watch?v=WfvD-JZGlHs)","ce40588d":"Spruce\/Fir, Lodgepole Pine and Krummholz loves to hangout in Rawah, Neota and Comanche Peak Wilderness Area.\n\nCache la Poudre Wilderness Area is perfect place for Ponderosa Pine and Cottonwood\/Willow.\n\nIf you see an Aspen suspect that you might be at the Rawah or Comanche.\n\nDouglas-fir is an easy going species, that goes along with any wilderness area.","a75de8a4":"Here is the best combination of parameters:\n* `n_estimators` = 300\n* `max_features` = None\n* `min_samples_leaf`= 1\n* `min_samples_split`= 2\n\nLet's apply those parameters to the extra random forests classifier model and see observe the improvement on the accuracy score.","90a9ef2a":"## 3.3. Split training set as training and validation set","4986e68f":"## 2.4. Check if the Cover_Type shows non-uniform distribution among different Wilderness_Areas","eb31dc9a":"With the random search, I am able to define a best set of parameters (might change for a different case and set of parameters though) as mentioned above. \n\nTo recap, first I used default parameter settings to find which algorithm yields best performance. Then, I improved performance of the best selected algorithm (random forest classifier) by narrowing down to the set of parameters with random search.\n\nNow, I am going to look if there is any room left for further improvement in accuracy score in the algorithm. I am going to look for that improvement in the `n_estimator` parameter, (number of decision trees used in the extra random forests). Having the possibility of long run-times in mind, I will use GridSearch with parameter n_estimators and pass a 6-element list as input, to keep the run-time at reasonable minutes. \n\nLike random search, grid search also performs its search on whole data set with k-fold cross validation. I am going to use 5-fold cross validation as I did for random search.","0803397b":"### 4.2.4. Extra Trees (Random Forests) Classifier","36e914e0":"## 1.4. Findings from Understand, Clean and Format Data\nTraining dataset (trees dataframe) has 15120 entries and 56 columns with headers appropriately named. Dataset is clean and well-formatted, meaning it had no NA values and every column has a numeric (float or integer) data type. \n\n4 columns had outliers, outliers of the `Horizontal_Distance_To_Fire_Points` is removed considering this column has a wider range and has the most number of outliers.\n\n`Cover_Type` is our label\/target column. `Wilderness_Area` and `Soil_Type` columns might have binary values (0,1) if so, they are the one-hot-encoded columns of 4 wilderness areas and 40 soil types respectively. I am going to start exploratory data analysis by seeking answer to that suspicion.","79da529c":"# 1. Understand, Clean and Format Data","4bbf6eff":"Recall the data ranges of those 4 columns:\n* Horizontal_Distance_To_Hydrology: 0, 1343\n* Vertical_Distance_To_Hydrology: -146, 554\n* Horizontal_Distance_To_Roadways: 0, 6890\n* Horizaontal_Distance_To_Firepoints: 0, 6993\n\nConsidering the Horizaontal_Distance_To_Firepoints having the highest number of outliers and widest data range, I am going to remove outliers only from that column.","61fa01b2":"Now, I am going to perform hyperparameter tuning on the best model (extra random forests classifier) and try to improve accuracy of the model. Searching and setting the best and optimal set of parameters for a machine learning model can be defined as hyperparameter tuning.\n\nMore than 80% accuracy can be interpreted as a reasonable score and managed not to fall the areas of underfitting or overfitting. One can call the model as underfit if s\/he gets an accuracy score slightly more than the baseline metric, meaning the model fails to catch and learn from the patterns in the training set.\n\nOn the other hand, an accuracy score of more than 95% might show that the model already in the overfitted area. Meaning the model performed very well on the training data and captured the patterns but it might not show the same performance on the test data set. So one cannot conclude that the higher performance metric is always better.\n\nLet's see if I can improve the accuracy of the model by playing with the parameters of [extra trees classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html) without falling into overfitting area.","702c816f":"## 2.10. Findings From Exploratory Data Analysis\n<p> Data set have balanced labels, resulting in equal number of cover types. This will be an advantage when it comes to apply classification ML models because, the model will have good chance to learn patterns of all labels, eliminating the probability of underfitting. <p\/>\n<p> Different wilderness areas consist of some specific trees. Interestingly, there is one fantastic tree, Cottonwood\/Willow, specifically likes to grow in wilderness area 4. While cover types 1, 2, 5 and 6 can grow in any soil type, other cover types grows more with specific soil types. <p\/>\n<p> Soil types are reverse-one-hot-encoded, meaning they are going to be included as numeric data in the training set and one-hot-encoded soil type columns will be excluded. With that way, there is a stronger correlation between soil type and Cover_Type. Numeric soil type column and other variables have pearson coefficients in the range of [-0.2, 0.1]. <p\/>\n<p> Hillshade columns are collinear within each other and Hillshade_9am has the least importance in determining Cover_Type. Thus this column will be dropped in Part 3 for better interpretability of the future model. <p\/>","e54f1ee1":"## 2.8. Pearson Coefficients of all features","6ec37fb5":"To perform hyperparameter tuning, I am going to define set of parameters and `RandomizedSearchCV` will look for the best combination with cross validation. So, randomly one single element is chosen from the below lists in each iteration. When the iteration is complete on each k-folds best set of parameters can be detected.","e2edf484":"# 8. Summary & Conclusisons","6b138c7d":"# 6. Evaluate the Best Model with Test Data","56aa59bb":"## 7.1. Feature importances","55190daf":"# 2. Exploratory Data Analysis","fa09fb46":"Model performed best when n_estimators are 500. So I am going to update model with that parameter.","8f099c7f":"Training, validation and test set have the same number of columns.","819ea03e":"Top 10 Pearson Correlations before building the model:","68ccd495":"## 2.6. Distribution and relationship of continuous variables (Elevation, Aspect, Slope, Distance and Hillsahde columns)","edc31dc4":"Remember, in the exploratory data analysis I looked at the pearson coeffients of the features. Since final results are generated, I will revisit this picture and observe features with the highest contribution to the model predictions. `feature_importances` attribute will be used for this.","9ad2bfee":"### And many thanks for reading until the end, if I am able to share my knowledge with you or at least created some inspiration in you, I will appreciate your upvote.","e767da49":"# 3. Feature Engineering & Selection","61534958":"Yes, they only have binary values.","2d47004a":"## 4.2. Build models","0b87e905":"I am going to take a close look for the outlier elimination for the following columns:\n* Horizontal_Distance_To_Hydrology\n* Vertical_Distance_To_Hydrology\n* Horizontal_Distance_To_Roadways\n* Horizontal_Distance_To_Fire_Points\n\nI am not going to consider other columns for potential outlier elimination because their data range is already fixed between 0 and 255 (e.g. Hillsahde columns)  or they seem like one-hot-encoded columns (e.g. Soil type and Wilderness areas)."}}