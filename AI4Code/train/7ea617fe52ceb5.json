{"cell_type":{"63e0c464":"code","b2dd82c9":"code","75e3e0bc":"code","a615ee18":"code","63f7872b":"code","6622eae4":"code","fed6d338":"code","83a565c3":"code","4a3c6bbd":"code","5b5c7da9":"code","cb7a89f7":"code","ae7e0849":"code","caa58e98":"code","77610763":"code","89f9dd86":"code","c10c31f5":"code","425b33c8":"code","8b043408":"code","bdc770f8":"code","d71488da":"code","86d7b246":"code","275bc246":"code","1378d4e8":"code","0cbc7062":"code","3e46fcfa":"code","885aab8e":"code","ed586879":"code","bf219f87":"code","ba977f68":"code","1e80435d":"code","691c7410":"code","56440050":"code","6de5ead2":"code","50855fc2":"code","86fc455d":"code","73a738d8":"code","74385c18":"code","10f52d8e":"code","1093868b":"code","eae61818":"code","6de8eeec":"code","099d087b":"code","c0b8395c":"code","12884e05":"code","5f32f12f":"code","cdc6eeba":"code","d3a020c6":"code","da3fd233":"code","89cda325":"code","390f04e8":"code","f1d76426":"code","d666c04e":"code","3d9208a6":"code","e9cc7c67":"code","8b2489db":"code","05c2cf1d":"code","279b0267":"code","a91062b1":"code","88df15fc":"code","da3d1c22":"code","8efc684b":"code","2ac1133f":"code","9dcfe743":"code","f0aef3f2":"code","d4e61833":"code","7bfb76fe":"code","5e6ec0e4":"code","1363cff0":"code","734a4a1a":"code","36a281b8":"code","5cd46561":"code","35b4d82c":"code","f7d9b0eb":"code","96dfd605":"code","1d697ece":"code","1f297529":"code","9792aa80":"code","dd2fc5b9":"code","cd56bdf5":"code","dbd0ce2a":"code","f84d8ea4":"code","496fdfca":"code","e8a83dfe":"code","dc6f85b9":"code","9cd16525":"code","bba20612":"code","d63d265f":"code","2e340e6c":"code","f0577394":"code","ebbba519":"code","53841cdd":"code","07b2f20e":"code","20f910c4":"code","e0ee3a9f":"code","a6c02fe3":"code","316d8ca6":"code","8b850ec6":"code","420f16fa":"code","7e59bfbb":"code","a51d6d19":"code","063c00f4":"markdown","8a7f9aaf":"markdown","afb5fdc0":"markdown","876c2bee":"markdown","43c7dfd0":"markdown","dc96d45c":"markdown","d4305d88":"markdown","22e39df1":"markdown","b37cec35":"markdown","6f8ae8d3":"markdown","d3b641a8":"markdown","4e1e9cf7":"markdown","1527723e":"markdown","ab7cd5e5":"markdown","12dbf7ea":"markdown","4f420568":"markdown","af869d12":"markdown","727bd976":"markdown","6e31c21c":"markdown","e7509e11":"markdown","6868b0fa":"markdown","2e9ca9d8":"markdown","3a1bfe7a":"markdown","2497ba07":"markdown","b1f5fa9e":"markdown","05c3b01f":"markdown","03ee22ad":"markdown","40e4f613":"markdown"},"source":{"63e0c464":"#At each stage we will bring in the library that we need \nimport pandas as pd     # data processing \nimport numpy as np      # linear algebra \nimport seaborn as sns # data visualization\nimport matplotlib.pyplot as plt\n\nimport os","b2dd82c9":"#Importing the data and displaying some rows\ndf= pd.read_csv(\"..\/input\/titanic-extended\/full.csv\")\n\ndf.head()\ndf.tail()","75e3e0bc":"df=df.drop([\"WikiId\",\"Name_wiki\",\"Age_wiki\",\"Hometown\",\"Boarded\",\"Destination\",\"Lifeboat\"\n            ,\"Body\",\"Class\"],axis=1)","a615ee18":"df.head()","63f7872b":"df.shape\n","6622eae4":"df.info()","fed6d338":"df.isnull().sum()","83a565c3":"list(df.columns)","4a3c6bbd":"df.dtypes.value_counts()\n","5b5c7da9":"df.dtypes.value_counts().plot.pie()","cb7a89f7":"#to see the missing values\ndf.isna()","ae7e0849":"#we will look for the missing values on our dataset with simple graph\nsns.heatmap(df.isna())","caa58e98":"#we will look for the missing values on our dataset\ndf.isna().sum()","77610763":"#we will look for the missing values on our dataset in percentage\n(df.isna().sum()\/df.shape[0]).sort_values(ascending=True)","89f9dd86":"#we will identify the columns which have 70% missing values\ndf.isna().sum()\/df.shape[0]<0.7","c10c31f5":"#Elemination of unnecessary columns in our data\ndf.columns[df.isna().sum()\/df.shape[0]<0.7]","425b33c8":"#elimination of unnecessary columns for the train\ndf=df[df.columns[df.isna().sum()\/df.shape[0]<0.7]]","8b043408":"df.head()","bdc770f8":"df.head()","d71488da":"#examining the target column\ndf['Survived'].value_counts()","86d7b246":"df['Survived'].value_counts(normalize=True)","275bc246":"#definition:\n#EDA = is Exploratory Data Analysis.this is the preliminary analysis you help on data to understand your \n#data,your variables,and even how relate to another\n","1378d4e8":"import pandas_profiling # library for automatic EDA","0cbc7062":"report = pandas_profiling.ProfileReport(df)","3e46fcfa":"#let's now visualize the report genereted py pandas_profling\ndisplay(report)\n#there is an option to generate an .HTML file containing all the information generated by the report.\n#report.to_file(output_file='report.html')\n","885aab8e":"#the report can also exported into an interactive HTMLfile with the following code.\n#profile = df.profile_report (title='Pandas Profiling Report')\n#.to_file (output_file=\"Titanic data profiling.html\")","ed586879":"pip install cufflinks","bf219f87":"import cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)","ba977f68":"df.iplot()","1e80435d":"import matplotlib.pyplot as plt \ndf.plot()","691c7410":"#we are going to do another visualization to better understand the variables\nfor col in df.select_dtypes(\"float\"):\n    print(col)\n    plt.figure()\n    sns.distplot(df[col])","56440050":"for col in df.select_dtypes('int'):\n    print(col)\n    plt.figure()\n    sns.distplot(len(df[col]))\n    \n    ","6de5ead2":"#discrete variable analysis\nfor col in df.select_dtypes(\"object\"):\n    print(f'{col:-<50}{df[col].unique()}')\n    ","50855fc2":"for col in df.select_dtypes (\"object\"):\n    plt.figure()\n    df[col].value_counts().plot.pie()","86fc455d":"#we will do some transformation on our dataset\n#according to the previous visualization we notice that we need\n#to do some transformation on the \"Name\" column to improve the perfermance of our model","73a738d8":"train_test_data=[df]","74385c18":"for dataset in train_test_data:\n    dataset['Title'] = dataset['Name'].str.extract('([A-Za-z]+)\\.',expand=False)\n    ","10f52d8e":"df['Title'].value_counts()","1093868b":"#title map\n#Mr = 0\n#Miss = 1\n#Mrs = 2\n#othres = 3","eae61818":"title_mapping = {\"Mr\":0, \"Miss\":1,\"Mrs\":2,\"Master\":3,\"Rev\":3,\"Col\":3,\"Dr\":3,\"Dona\":3,\"Ms\":3,\"Mlle\":3,          \n\"Col\":3,\"Mme\":3,\"Jonkheer\":3,\"Capt\":3,\"Sir\":3,\"Don\":3,\"Lady\":3,\"Countess\":3}\nfor dataset in train_test_data:\n    dataset[\"Title\"]=dataset[\"Title\"].map(title_mapping)","6de8eeec":"df.head()","099d087b":"#delete unnecessary feature from dataset\ndf.drop('Name',axis=1,inplace=True)\n\n","c0b8395c":"df.head()\n","12884e05":"#to do some transformation on the \"Sex\" column to improve the perfermance of our model\n#male = 0\n#female = 1","5f32f12f":"sex_mapping={\"male\":0,\"female\":1}\nfor dataset in train_test_data:\n    dataset[\"Sex\"]=dataset[\"Sex\"].map(sex_mapping)\n","cdc6eeba":"df.head()","d3a020c6":"#fill missing age with median age each title(Mr,Miss,Mrs,others)\ndf[\"Age\"].fillna(df.groupby(\"Title\")[\"Age\"].transform(\"median\"),inplace=True)\n\n","da3fd233":"\ndf[\"Survived\"].fillna(df.groupby(\"Title\")[\"Age\"].transform(\"median\"),inplace=True)","89cda325":"df.head()\ndf.groupby(\"Title\")[\"Age\"].transform(\"median\")","390f04e8":"#now we have all the values of the \"Age\" column we\n#will see its relation with the target \"Survived\"\n#(all the ages that are in the dataset)\n\nhtf = sns.FacetGrid(df, hue=\"Survived\", aspect=2)\nhtf.map(sns.kdeplot, \"Age\", shade=True)\nhtf.set(xlim=(1, df[\"Age\"].max()))\nhtf.add_legend()\nplt.show()\n","f1d76426":"#now we have all the values of the \"Age\" column we\n#will see its relation with the target \"Survived\"\n#(\"Age\"between 0 and 20)\nhtf = sns.FacetGrid(df, hue=\"Survived\", aspect=2)\nhtf.map(sns.kdeplot, \"Age\", shade=True)\nhtf.set(xlim=(1, df[\"Age\"].max()))\nhtf.add_legend()\nplt.xlim(0,20)","d666c04e":"#now we have all the values of the \"Age\" column we\n#will see its relation with the target \"Survived\"\n#(\"Age\"between 20 and 40)\nhtf = sns.FacetGrid(df, hue=\"Survived\", aspect=2)\nhtf.map(sns.kdeplot, \"Age\", shade=True)\nhtf.set(xlim=(1, df[\"Age\"].max()))\nhtf.add_legend()\nplt.xlim(20,40)","3d9208a6":"#now we have all the values of the \"Age\" column we\n#will see its relation with the target \"Survived\"\n#(\"Age\"between 40 and 60)\nhtf = sns.FacetGrid(df, hue=\"Survived\", aspect=2)\nhtf.map(sns.kdeplot, \"Age\", shade=True)\nhtf.set(xlim=(1, df[\"Age\"].max()))\nhtf.add_legend()\nplt.xlim(40,60)","e9cc7c67":"#now we have all the values of the \"Age\" column we\n#will see its relation with the target \"Survived\"\n#(\"Age\"between 60 and 80)\nhtf = sns.FacetGrid(df, hue=\"Survived\", aspect=2)\nhtf.map(sns.kdeplot, \"Age\", shade=True)\nhtf.set(xlim=(1, df[\"Age\"].max()))\nhtf.add_legend()\nplt.xlim(60,80)","8b2489db":"#to do some transformation on the \"Embarked\" column to improve the perfermance of our model\nPclass1 = df[df[\"Pclass\"]==1][\"Embarked\"].value_counts()\nPclass2 = df[df[\"Pclass\"]==2][\"Embarked\"].value_counts()\nPclass3 = df[df[\"Pclass\"]==3][\"Embarked\"].value_counts()\ndf1 = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf1.index = ['1_class','2_class','3_class']\ndf1.plot(kind =\"bar\",stacked =True,figsize=(15,10))","05c2cf1d":"#full out missing embark with \"s\" embark\nfor dataset in train_test_data:\n    dataset[\"Embarked\"]=dataset[\"Embarked\"].fillna(\"S\")","279b0267":"df.head()","a91062b1":"set_mapping={\"S\":0,\"C\":1,\"Q\":2}\nfor dataset in train_test_data:\n    dataset[\"Embarked\"]=dataset[\"Embarked\"].map(set_mapping)","88df15fc":"df.head(50)","da3d1c22":"#to do some transformation on the \"Fare\" column to mprove the perfermance  of our model\n#full missing Fare with median Fare for each pclass\ndf[\"Fare\"].fillna(df.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"),inplace=True)\n","8efc684b":"df.head(50)","2ac1133f":"#now we have all the values of the \"Fare\" column we\n#will see its relation with the target \"Survived\"\n#(all the ages that are in the dataset)\n\nhtf = sns.FacetGrid(df, hue=\"Survived\", aspect=2)\nhtf.map(sns.kdeplot, \"Fare\", shade=True)\nhtf.set(xlim=(1, df[\"Fare\"].max()))\nhtf.add_legend()\nplt.show()","9dcfe743":"#now we have all the values of the \"Fare\" column we\n#will see its relation with the target \"Survived\"\n#(\"Fare\"between 0 and 20)\nhtf = sns.FacetGrid(df, hue=\"Survived\", aspect=2)\nhtf.map(sns.kdeplot, \"Fare\", shade=True)\nhtf.set(xlim=(1, df[\"Fare\"].max()))\nhtf.add_legend()\nplt.xlim(0,20)","f0aef3f2":"#now we have all the values of the \"Fare\" column we\n#will see its relation with the target \"Survived\"\n#(\"Fare\"between 0 and 20)\nhtf = sns.FacetGrid(df, hue=\"Survived\", aspect=2)\nhtf.map(sns.kdeplot, \"Fare\", shade=True)\nhtf.set(xlim=(1, df[\"Fare\"].max()))\nhtf.add_legend()\nplt.xlim(0,30)","d4e61833":"#now we have all the values of the \"Fare\" column we\n#will see its relation with the target \"Survived\"\n#(\"Fare\"between 0 and 20)\nhtf = sns.FacetGrid(df, hue=\"Survived\", aspect=2)\nhtf.map(sns.kdeplot, \"Fare\", shade=True)\nhtf.set(xlim=(1, df[\"Fare\"].max()))\nhtf.add_legend()\nplt.xlim(0)","7bfb76fe":"#we will transform the numerical values into binary form\nfor dataset in train_test_data:\n    dataset.loc[dataset[\"Fare\"] <= 17,'Fare'] == 0\n    dataset.loc[dataset[\"Fare\"] > 17 & (dataset[\"Fare\"] <=30),'Fare'] == 1\n    dataset.loc[dataset[\"Fare\"] > 30 & (dataset[\"Fare\"] <=100),'Fare'] == 2\n    dataset.loc[dataset[\"Fare\"] > 100, 'Fare'] == 3","5e6ec0e4":"df.head()","1363cff0":"#to do some transformation on the \"SibSp\" column to mprove the perfermance  of our model\n#we will create new column \"FamilySize\"\nfor dataset in train_test_data:\n    dataset[\"Title\"]=dataset[\"Title\"].fillna(\"1\")","734a4a1a":"df[\"FamilySize\"]=df[\"SibSp\"]+df[\"Parch\"]+1","36a281b8":"df.head()","5cd46561":"features_drop=[\"PassengerId\",\"SibSp\",\"Parch\",\"Ticket\"]\ndf=df.drop(features_drop,axis=1)\n\n","35b4d82c":"df.head()","f7d9b0eb":"df.info()","96dfd605":"y = df['Survived']\nX = df.drop(['Survived'], axis=1)\nfrom sklearn.model_selection import train_test_split\nX_train, y_train,X_test, y_test = train_test_split(X,y, random_state=100, test_size=0.20, shuffle=False)","1d697ece":"#y.shape\nX_train.shape\n#y_train.shape\n#X_test.shape\n#y_test.shape","1f297529":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import make_column_transformer\ntransformer= make_column_transformer((StandardScaler(),[\"Fare\",\"Age\"]))\ntransformer.fit_transform(df)","9792aa80":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.decomposition import PCA","dd2fc5b9":"model_1 = RandomForestClassifier(random_state=0)","cd56bdf5":"\nmodel_2 = make_pipeline(PolynomialFeatures(2), SelectKBest(f_classif, k=10),\n                      RandomForestClassifier(random_state=0))","dbd0ce2a":"\ndef evaluation(model):\n    \n    model.fit(X_train, y_train)\n    ypred = model.predict(X_test)\n    \n    print(confusion_matrix(y_test, ypred))\n    print(classification_report(y_test, ypred))\n    \n    N, train_score, val_score = learning_curve(model, X_train, y_train,\n                                              cv=4, scoring='f1',\n                                               train_sizes=np.linspace(0.1, 1, 10))\n    \n    \n    plt.figure(figsize=(12, 8))\n    plt.plot(N, train_score.mean(axis=1), label='train score')\n    plt.plot(N, val_score.mean(axis=1), label='validation score')\n    plt.legend()","f84d8ea4":"evaluation(model_1)","496fdfca":"pd.DataFrame(model_1.feature_importances_, index=X_train.columns).plot.bar(figsize=(12, 8))","e8a83dfe":"evaluation(model_2)","dc6f85b9":"\npreprocessor = make_pipeline(PolynomialFeatures(2, include_bias=False), SelectKBest(f_classif, k=10))","9cd16525":"\nRandomForest = make_pipeline(preprocessor, RandomForestClassifier(random_state=0))\nAdaBoost = make_pipeline(preprocessor, AdaBoostClassifier(random_state=0))\nSVM = make_pipeline(preprocessor, StandardScaler(), SVC(random_state=0))\nKNN = make_pipeline(preprocessor, StandardScaler(), KNeighborsClassifier())","bba20612":"\ndict_of_models = {'RandomForest': RandomForest,\n                  'AdaBoost' : AdaBoost,\n                  'SVM': SVM,\n                  'KNN': KNN\n                 }","d63d265f":"for name, model in dict_of_models.items():\n    print(name)\n    evaluation(model)","2e340e6c":"\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV","f0577394":"\n SVM","ebbba519":"hyper_params = {'svc__gamma':[1e-3, 1e-4, 0.0005],\n                'svc__C':[1, 10, 100, 1000, 3000], \n               'pipeline__polynomialfeatures__degree':[2, 3],\n               'pipeline__selectkbest__k': range(45, 60)}","53841cdd":"\ngrid = RandomizedSearchCV(SVM, hyper_params, scoring='recall', cv=4,\n                          n_iter=40)\n\ngrid.fit(X_train, Y_train)\n\nprint(grid.best_params_)\n\ny_pred = grid.predict(X_test)\n\nprint(classification_report(Y_test, y_pred))","07b2f20e":"\nevaluation(grid.best_estimator_)","20f910c4":"\nfrom sklearn.metrics import precision_recall_curve","e0ee3a9f":"\nprecision, recall, threshold = precision_recall_curve(Y_test, grid.best_estimator_.decision_function(X_test))","a6c02fe3":"\nplt.plot(threshold, precision[:-1], label='precision')\nplt.plot(threshold, recall[:-1], label='recall')\nplt.legend()","316d8ca6":"\ndef model_final(model, X, threshold=0):\n    return model.decision_function(X) > threshold","8b850ec6":"\ny_pred = model_final(grid.best_estimator_, X_test, threshold=-1)","420f16fa":"\nfrom sklearn.metrics import recall_score","7e59bfbb":"\nf1_score(y_test, y_pred)","a51d6d19":"\nrecall_score(Y_test, y_pred)","063c00f4":"# we created a dictionary to transform expressions to count into binary form","8a7f9aaf":"# it's a simple code to count the number of words ending in \".\"  the data","afb5fdc0":"# we notice the importance of the \"Fare\" feature on the \"Survived\" target\n# and that the price of the tiket has influenced the death rate in each class","876c2bee":"#  1.1Shape analysis:\n\n#       variable\/target\n     \n#       rows  and columns\n     \n#       missing values analysis","43c7dfd0":"# we notice that there are missing values, they must be treated in another step (the blank represents the missing values)exactly on the columns: \"Age\" and \"Cabin\" and some values on the columns: \"Embarked\"","dc96d45c":"# it is a simple method to count the categorical variable we take into account their deffrente value","d4305d88":"# Titanic Dataset:\n1. ****different data preprocessing and modiling and othres techniques\nand much more","22e39df1":"# 1.section1_Data Exploration","b37cec35":"#  3.data preprosessing","6f8ae8d3":"# cufflinks library binds the power of plotly with the flexibility of pandas for easy plotting.Let's now see this ","d3b641a8":"# we confirm that there are two classes equilibrer which are \"survived == 1\" (342) or not \"survived == 0\" (549)","4e1e9cf7":"# we created a dataset that contains columns to determine","1527723e":"# more than 50% os 1_class are from S embark\n# more than 50% os 2_class are from S embark\n# more than 50% os 3_class are from S embark","ab7cd5e5":"# we have 2 \"float\" type variables which are: \"Age\" and \"Fare\" which are visualized by the previous graphs","12dbf7ea":"# we fill the missing values of the column \"Embarked\" by \"s\"","4f420568":"# according to the previous graph most people died the one who was all alone","af869d12":"# 1.2Bottom analysis:\n#          visualization of the target\n#          meaning of variables\n#          target \/ variable relationship","727bd976":"# combining train and test dataset","6e31c21c":"# we erased the column \"Name\" after the extraction of important information","e7509e11":"# it's a simple code to count the number of words ending in \".\"in all the dataset.We create a new column called \"Title\"","6868b0fa":"#  we have transformed all the values of the column \"Embarked\" with \"0\",\"1\" and \"2\"\n\n","2e9ca9d8":"# now we have nice static graphs and numeric graphs which allow us to see all the following relations: varables \/ variables, variables \/ target.On the other hand these help us to better understand our data","3a1bfe7a":"# the pandas profiling library is useful on helping understand the data we're working on .It saves us some precious time in the EDE process","2497ba07":"# more detailed analysis","b1f5fa9e":"# we have transformed all the values of the column \"Sex\" with \"0\" and \"1\"","05c3b01f":"# we confirm that there are two classes equilibrer which are \"survived == 1\" (0.383838) or not \"survived == 0\" (0.616162)","03ee22ad":"# it's to visualize the categorical variables (but in our model the \"Name\" variable contains a lot of value for this our graph is a little bad).Not worry we will solve the problem in the next part","40e4f613":"# we have 177 missing values for the \"Age\" columns, 687 values for the \"Cabin\" column and 2 values for the \"Embarked\" columns"}}