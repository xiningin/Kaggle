{"cell_type":{"d40ed9ab":"code","e15dcd2e":"code","d1d00f21":"code","81a7a148":"code","5462e9a9":"code","75b534d3":"code","5012e154":"code","e7b29982":"code","52e8f371":"code","31d0e1e6":"code","b3fa35a2":"code","f5bfbac0":"code","31f8088e":"code","f2b2e6a6":"code","d4fa2149":"code","3fed1051":"code","6c605aa7":"code","77faa620":"code","f5884c3d":"code","82d1ae5e":"code","39123fbb":"code","d2266d80":"markdown","b1a8f3bf":"markdown","67c2cb37":"markdown","267997fe":"markdown","aceab3d4":"markdown"},"source":{"d40ed9ab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e15dcd2e":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","d1d00f21":"# Read train and test set\ntrain = pd.read_csv(\"\/kaggle\/input\/bri-data-hackathon-pa\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/bri-data-hackathon-pa\/test.csv\")","81a7a148":"# Drop null values\ntrain = train.dropna()","5462e9a9":"# Split train set into dependent variables and independent variable\ny = train['Best Performance']\nX = train.drop('Best Performance', axis=1)","75b534d3":"y.value_counts()","5012e154":"# Convert to dummy variables\nX = pd.get_dummies(X)\ntest = pd.get_dummies(test)","e7b29982":"# Extract the common features between train and test set and use it to filter the train and test set\ncommon = list(set(X.columns).intersection(set(test.columns)))\nX = X[common]\ntest = test[common]","52e8f371":"# ADASYN, BorderlineSMOTE, KMeansSMOTE, RandomOverSampler, SMOTE, SVMSMOTE\nfrom imblearn.over_sampling import *","31d0e1e6":"over_methods = [\n    ADASYN(random_state=7),\n    BorderlineSMOTE(random_state=7),\n    RandomOverSampler(random_state=7),\n    SMOTE(random_state=7),\n    SVMSMOTE(random_state=7)\n]","b3fa35a2":"classifiers =[\n    RandomForestClassifier(n_estimators=1000),\n    ExtraTreesClassifier(n_estimators=1000)\n]","f5bfbac0":"# Benchmarks\nfor classifier in classifiers:\n        \n    steps = [('model', classifier)]\n\n    pipeline = Pipeline(steps=steps)\n\n    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=7)\n    scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n    \n    print(\"ROC-AUC Score for\", classifier, \"without over-sampling is\", round(np.mean(scores),5))","31f8088e":"# Iterates over classifiers and over_methods\nfor classifier in classifiers:\n    for method in over_methods:\n        \n        steps = [('over', method),\n                 ('model', classifier)]\n\n        pipeline = Pipeline(steps=steps)\n\n        cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=7)\n        scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n    \n        print(\"ROC-AUC Score for\", classifier, \"and\", method, \"is\", round(np.mean(scores),5))","f2b2e6a6":"from imblearn.combine import SMOTEENN, SMOTETomek","d4fa2149":"combinations = [\n    SMOTEENN(random_state=7),\n    SMOTETomek(random_state=7)\n]","3fed1051":"classifiers =[\n    RandomForestClassifier(n_estimators=1000),\n    ExtraTreesClassifier(n_estimators=1000)\n]","6c605aa7":"# Iterates over classifiers and combinations\nfor classifier in classifiers:\n    for combination in combinations:\n        \n        steps = [('comb', combination),\n                 ('model', classifier)]\n\n        pipeline = Pipeline(steps=steps)\n\n        cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=7)\n        scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n    \n        print(\"ROC-AUC Score for\", classifier, \"and\", combination, \"is\", round(np.mean(scores),5))","77faa620":"from imblearn.ensemble import BalancedBaggingClassifier, RUSBoostClassifier, BalancedRandomForestClassifier","f5884c3d":"# Balanced Bagging Classifier\nbbc = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n                                n_estimators = 1000,\n                                sampling_strategy='auto',\n                                replacement=False,\n                                random_state=0)\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=7)\nscores = cross_val_score(bbc, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\nprint(\"ROC-AUC Score for Balanced Bagging Classifier is\", round(np.mean(scores),5))","82d1ae5e":"# Balanced Random Forest Classifier\nbrf = BalancedRandomForestClassifier(n_estimators = 1000,\n                                     sampling_strategy='auto',\n                                     replacement=False,\n                                     random_state=0)\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=7)\nscores = cross_val_score(brf, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\nprint(\"ROC-AUC Score for Balanced Random Forest Classifier is\", round(np.mean(scores),5))","39123fbb":"# RUSBoostClassifier\nrusbc = RUSBoostClassifier(n_estimators = 1000,\n                           random_state = 0)\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=7)\nscores = cross_val_score(rusbc, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\nprint(\"ROC-AUC Score for RUSBoostClassifier is\", round(np.mean(scores),5))","d2266d80":"## Oversampling","b1a8f3bf":"## Combination Sampling\n#### Combination of over- and under-sampling methods\n","67c2cb37":"## Ensemble of Samplers\n#### Classifier including inner balancing samplers\nFor more info: https:\/\/imbalanced-learn.org\/stable\/ensemble.html","267997fe":"Hello everyone!\n\nThis notebook presents an example of how to deal with imbalanced data. I will start with several oversampling methods, combination methods, and balanced classifiers.\n\nIf you have any questions regarding the code, please comment below. I will update the notebook accordingly.\n\n**Please do upvote the notebook if this notebook helps you, as it will be a benchmark for me to do more work in the future. Thank you :)**\n\n**Note: I do not do the feature engineering here, so the result may sub-optimal**","aceab3d4":"## Summary\n\nNow we obtain the highest score are:\n- Balanced Bagging Classifier is 0.58633\n- Balanced Random Forest Classifier is 0.57839\n- RandomForestClassifier(n_estimators=1000) and RandomOverSampler() is 0.574\n- RandomForestClassifier(n_estimators=1000) without over-sampling is 0.57059 [BENCHMARK]\n\n### What's next?\n\n- Try to do feature engineering first or try another encoder method and run all the code to calculate the scores.\n- Tune the classifier with GridSearchCV, RandomizedSearchCV, or Bayesian Optimization. See my other notebook here: https:\/\/www.kaggle.com\/yevonnaelandrew\/starter-xgboost-bayesian-optimization\n- Try other classification algorithms, like XGBoost, CatBoost, LGBM."}}