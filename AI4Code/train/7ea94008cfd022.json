{"cell_type":{"4b754fc1":"code","90f57cce":"code","1e992086":"code","e50f981a":"code","f2341195":"code","3edc92d3":"code","d67386b2":"code","8372c073":"code","e0622f2b":"code","96304cc9":"code","e5ee2d7a":"code","e0cd28c1":"code","264b117c":"code","413762c5":"code","35022431":"code","a112b722":"code","d5471cae":"code","68bfd4d1":"code","2447ceec":"code","1990bd47":"code","0ee699d1":"code","af3ae843":"code","9f8c6e60":"code","e60e6e8e":"code","7d89f7cc":"code","1986f345":"code","18acc6f5":"code","a3e32ac8":"code","69d40b08":"code","bc9f6878":"code","9bb7043e":"code","4a879221":"markdown","4f30fa93":"markdown","f591e7c9":"markdown","3cdc8574":"markdown","10128b39":"markdown","f612d393":"markdown","231c8c1e":"markdown","ac40a819":"markdown"},"source":{"4b754fc1":"import sys \nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\n#sys.path.append('..')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","90f57cce":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sn\n#import mlflow\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n#import mlflow\n\nimport warnings\nwarnings.filterwarnings('ignore')","1e992086":"#from tools.loaders import train_short_form_loader, test_short_form_loader","e50f981a":"exp_name=\"torch_moa_arch_multilabelv5_smoothed_lrplateau_5_folds_continued\"\n#mlflow.set_experiment(exp_name)","f2341195":"def reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            \n            # Print current column type\n            #print(\"******************************\")\n            #print(\"Column: \",col)\n            #print(\"dtype before: \",props[col].dtype)\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(props[col]).all(): \n                NAlist.append(col)\n                props[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n            \n            # Print new column type\n           # print(\"dtype after: \",props[col].dtype)\n           # print(\"******************************\")\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return props, NAlist\n","3edc92d3":"\n\ndef train_short_form_loader(feature_file,target_file,extra_target_file=None):\n    '''takes the original target and features and creates a train dataset \n    in col long format'''\n\n\n    train_features = pd.read_csv(feature_file)\n\n    train_targets = pd.read_csv(target_file)\n    train_features,_= reduce_mem_usage(train_features)\n    train_targets,_ = reduce_mem_usage(train_targets)\n\n\n    if extra_target_file is not None:\n        extra_targets = pd.read_csv(extra_target_file)\n        extra_targets,_ = reduce_mem_usage(extra_targets)\n        train_targets = pd.concat([train_targets,extra_targets])\n        del extra_targets\n\n    targets = train_targets.columns[1:]\n\n    train_melt=train_targets.merge(train_features,how=\"left\",on=\"sig_id\")\n\n\n    del train_features,train_targets\n\n\n    train_melt.set_index(\"sig_id\",inplace=True)\n\n    #train_melt[\"variable\"]= train_melt[\"variable\"].astype('category')\n    train_melt[\"cp_type\"]= train_melt[\"cp_type\"].astype('category')\n    train_melt[\"cp_dose\"]= train_melt[\"cp_dose\"].astype('category')\n\n    return train_melt , targets\n\n\n\ndef test_short_form_loader(feature_file):\n    '''takes the original target and features and creates a train dataset \n    in col long format'''\n\n\n    train_features = pd.read_csv(feature_file)\n\n    #train_targets = pd.read_csv(target_file)\n    train_features,_= reduce_mem_usage(train_features)\n    #train_targets,_ = reduce_mem_usage(train_targets)\n\n    train_melt =  train_features.copy()\n    del train_features\n\n\n    train_melt.set_index(\"sig_id\",inplace=True)\n\n    #train_melt[\"variable\"]= train_melt[\"variable\"].astype('category')\n    train_melt[\"cp_type\"]= train_melt[\"cp_type\"].astype('category')\n    train_melt[\"cp_dose\"]= train_melt[\"cp_dose\"].astype('category')\n\n    return train_melt \n","d67386b2":"#os.listdir('..\/input\/lish-moa') ","8372c073":"train,target=train_short_form_loader('..\/input\/lish-moa\/train_features.csv','..\/input\/lish-moa\/train_targets_scored.csv')","e0622f2b":"train.head()","96304cc9":"target","e5ee2d7a":"def seed_everything(seed=42):\n    random.seed(seed)\n    #os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","e0cd28c1":"def supress_controls(df):\n    \n    df = df[train['cp_type']!='ctl_vehicle']\n    df = df.drop('cp_type', axis=1)\n\n    return df","264b117c":"def map_controls(df):\n    \n    df['cp_type']=df['cp_type'].map({'ctl_vehicle': 0, 'trt_cp': 1})\n    df['cp_type']=df['cp_type'].astype(int)\n    return df\n\ndef map_dose(df):\n    \n    df['cp_dose']=df['cp_dose'].map({'D1': 1, 'D2': 0})\n    df['cp_dose']=df['cp_dose'].astype(int)\n    return df\n\ndef map_time(df):\n    \n    df['cp_time']=df['cp_time'].map({24: 0, 48: 1, 72: 2})\n    df['cp_time']=df['cp_time'].astype(int)\n    return df","413762c5":"def build_preprocess(preprocesses=[map_time,map_dose,map_controls]):\n    \n    def preprocesser(df):\n        for proc in preprocesses:\n            df = proc(df)\n        return df\n    \n    return preprocesser\n    \n    \n    ","35022431":"preprocess_data=build_preprocess()","a112b722":"def multifold_indexer(train,target_columns,n_splits=10,random_state=12347,**kwargs):\n    folds = train.copy()\n\n    mskf = MultilabelStratifiedKFold(n_splits=n_splits,random_state=random_state,**kwargs)\n    folds[ 'kfold']=0\n    for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=train[target_columns])):\n        folds.iloc[v_idx,-1] = int(f)\n\n    folds['kfold'] = folds['kfold'].astype(int)\n    return folds\n","d5471cae":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    ","68bfd4d1":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n#         print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        if not  scheduler.__class__ ==  torch.optim.lr_scheduler.ReduceLROnPlateau:\n            scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, scheduler, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    if scheduler.__class__ ==  torch.optim.lr_scheduler.ReduceLROnPlateau:\n        scheduler.step(final_loss)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n   \n    ","2447ceec":"import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = torch.from_numpy(np.array(smoothing)).float().to(DEVICE)\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n      #  assert np.all(0 <= smoothing) and  np.all(smoothing < 1)\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","1990bd47":"SMOOTHING =[5.01187234e-03, 5.01187234e-03, 3.98107171e-03, 1.00000000e-05,\n       1.58489319e-03, 5.01187234e-04, 1.58489319e-03, 2.51188643e-02,\n       6.30957344e-03, 3.16227766e-04, 1.00000000e-06, 7.94328235e-02,\n       7.94328235e-03, 1.99526231e-03, 6.30957344e-03, 6.30957344e-03,\n       1.58489319e-03, 3.16227766e-04, 5.01187234e-02, 2.51188643e-03,\n       2.51188643e-03, 6.30957344e-04, 7.94328235e-03, 1.58489319e-03,\n       6.30957344e-03, 5.01187234e-03, 6.30957344e-03, 5.01187234e-03,\n       7.94328235e-04, 2.51188643e-03, 5.01187234e-03, 1.58489319e-03,\n       1.58489319e-03, 7.94328235e-03, 7.94328235e-03, 6.30957344e-03,\n       2.51188643e-04, 5.01187234e-03, 2.51188643e-04, 7.94328235e-03,\n       1.00000000e-03, 6.30957344e-04, 2.51188643e-03, 1.00000000e-05,\n       3.16227766e-04, 1.58489319e-04, 6.30957344e-03, 2.51188643e-03,\n       2.51188643e-03, 6.30957344e-04, 3.98107171e-03, 5.01187234e-02,\n       3.16227766e-03, 7.94328235e-03, 2.51188643e-04, 1.99526231e-03,\n       1.25892541e-03, 2.51188643e-03, 2.51188643e-03, 6.30957344e-03,\n       6.30957344e-03, 1.99526231e-04, 5.01187234e-03, 3.98107171e-05,\n       1.25892541e-08, 3.98107171e-03, 1.99526231e-03, 1.58489319e-03,\n       1.25892541e-03, 7.94328235e-03, 3.16227766e-03, 3.98107171e-05,\n       3.16227766e-04, 2.51188643e-03, 3.98107171e-03, 7.94328235e-03,\n       1.58489319e-03, 1.00000000e-08, 6.30957344e-03, 1.58489319e-04,\n       3.16227766e-03, 7.94328235e-03, 7.94328235e-03, 3.98107171e-04,\n       2.51188643e-03, 2.51188643e-03, 5.01187234e-03, 3.98107171e-03,\n       1.00000000e-03, 1.99526231e-02, 5.01187234e-03, 5.01187234e-03,\n       3.98107171e-03, 1.99526231e-02, 2.51188643e-03, 1.00000000e-03,\n       2.51188643e-01, 6.30957344e-03, 6.30957344e-04, 5.01187234e-09,\n       5.01187234e-03, 1.00000000e-03, 6.30957344e-04, 1.58489319e-04,\n       1.00000000e-03, 1.00000000e-06, 3.98107171e-03, 2.51188643e-03,\n       7.94328235e-04, 1.99526231e-03, 3.98107171e-04, 1.99526231e-03,\n       3.16227766e-03, 3.16227766e-03, 7.94328235e-04, 3.16227766e-03,\n       1.58489319e-03, 1.99526231e-03, 3.16227766e-04, 1.99526231e-04,\n       7.94328235e-03, 7.94328235e-03, 1.25892541e-03, 6.30957344e-03,\n       7.94328235e-04, 7.94328235e-03, 3.16227766e-03, 7.94328235e-04,\n       7.94328235e-04, 3.98107171e-03, 6.30957344e-03, 3.16227766e-04,\n       5.01187234e-03, 6.30957344e-05, 1.58489319e-03, 2.51188643e-03,\n       1.25892541e-01, 7.94328235e-03, 3.98107171e-03, 6.30957344e-03,\n       3.16227766e-03, 6.30957344e-03, 5.01187234e-03, 1.00000000e-03,\n       3.98107171e-04, 2.51188643e-03, 7.94328235e-04, 3.98107171e-03,\n       1.00000000e-03, 7.94328235e-05, 5.01187234e-03, 2.51188643e-04,\n       3.98107171e-03, 3.16227766e-05, 3.16227766e-03, 1.00000000e-03,\n       1.99526231e-04, 1.00000000e-04, 3.16227766e-03, 1.58489319e-03,\n       5.01187234e-03, 2.51188643e-03, 3.16227766e-03, 3.98107171e-03,\n       1.58489319e-03, 7.94328235e-03, 1.58489319e-03, 3.98107171e-03,\n       1.58489319e-03, 1.25892541e-03, 6.30957344e-03, 7.94328235e-04,\n       7.94328235e-03, 2.51188643e-03, 1.99526231e-03, 3.16227766e-03,\n       1.00000000e-06, 1.25892541e-04, 1.58489319e-03, 2.51188643e-03,\n       2.51188643e-03, 3.16227766e-03, 3.16227766e-07, 3.16227766e-03,\n       6.30957344e-04, 7.94328235e-03, 5.01187234e-03, 1.00000000e-03,\n       3.16227766e-03, 5.01187234e-03, 3.98107171e-03, 3.16227766e-03,\n       6.30957344e-03, 2.51188643e-03, 1.58489319e-04, 5.01187234e-03,\n       7.94328235e-03, 3.98107171e-03, 1.58489319e-03, 6.30957344e-03,\n       5.01187234e-01, 7.94328235e-03, 7.94328235e-06, 3.98107171e-07,\n       1.58489319e-07, 3.16227766e-07]","0ee699d1":"class Model_multilabel(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size1=388,hidden_size2=512,drop_rate1=0.8,drop_rate2=0.8,drop_rate3=0.8):\n        super(Model_multilabel, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size1))\n\n        self.batch_norm2 = nn.BatchNorm1d(hidden_size1)\n        self.dropout2 = nn.Dropout(drop_rate2)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size1, hidden_size2))\n\n        self.batch_norm3 = nn.BatchNorm1d(hidden_size2)\n        #self.dropout3 = nn.Dropout(drop_rate3)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size2, num_targets))\n\n\n    def forward(self, x):\n        \n        x = self.batch_norm1(x)\n        x = F.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dense3(x)\n        \n        return x\n    ","af3ae843":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 100\nBATCH_SIZE = 512\nLEARNING_RATE = 2e-4\nWEIGHT_DECAY = 2e-7\nNFOLDS = 5\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\n#num_features=len(feature_cols)\n#num_targets=len(target_cols)\nhidden_size=512\n\n","9f8c6e60":"def initialize_from_past_model(model,past_model_file):\n\n   # pretrained_dict = torch.load('FOLD0_.pth')\n    pretrained_dict = torch.load(past_model_file)\n    model_dict = model.state_dict()\n\n    pretrained_dict['dense3.bias']=pretrained_dict['dense3.bias'][:206]\n\n    pretrained_dict['dense3.weight_g']=pretrained_dict['dense3.weight_g'][:206]\n\n    pretrained_dict['dense3.weight_v']=pretrained_dict['dense3.weight_v'][:206]\n\n    # 1. filter out unnecessary keys\n    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n    # 2. overwrite entries in the existing state dict\n    model_dict.update(pretrained_dict) \n    # 3. load the new state dict\n    model.load_state_dict(pretrained_dict)\n    ","e60e6e8e":"#exp_name =  \"test_flow\"","7d89f7cc":"def run_training(X_train,y_train,X_valid,y_valid,X_test,fold, seed,verbose=False,**kwargs):\n    \n    seed_everything(seed)\n    \n   \n    \n    train_dataset = MoADataset(X_train, y_train)\n    valid_dataset = MoADataset(X_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model_multilabel(\n        num_features= X_train.shape[1] ,\n        num_targets=  y_train.shape[1],\n        **kwargs\n    )\n    \n    model.to(DEVICE)\n    \n    initialize_from_past_model(model,f\"..\/results\/torch_moa_arch_multilabelv5_smoothed_lrplateau_5_folds_AUX_SEED{seed}_FOLD{fold}.pth\")#,freeze_first_layer=True)\n    \n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    #scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e2, \n                                          #max_lr=5e-4, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=3)\n    \n    loss_val = nn.BCEWithLogitsLoss()\n    \n    loss_tr = SmoothBCEwLogits(smoothing =SMOOTHING)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    #todo el guardado de los resultados se puede mover a kfold que si tiene info de los indices\n    #oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    \n    \n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        if verbose:\n            print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model,scheduler, loss_val, validloader, DEVICE)\n        if verbose:\n            print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof = valid_preds\n        \n        \n        \n            torch.save(model.state_dict(), f\"..\/results\/{exp_name}_SEED{seed}_FOLD{fold}.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n   \n    testdataset = TestDataset(X_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n#     model = Model(\n#          num_features= X_train.shape[1] ,\n#         num_targets=  y_train.shape[1],\n#         hidden_size=hidden_size,**kwargs\n#     )\n    \n#     model.load_state_dict(torch.load(f\"..\/results\/FOLD{fold}_{exp_name}.pth\"))\n    model.to(DEVICE)\n    \n    #predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions\n","1986f345":"def run_k_fold(folds,target_cols,test,NFOLDS, seed,verbose=False,**kwargs):\n    \n    \n    train = folds\n    test_ = test\n    \n    \n    #oof = np.zeros((len(folds), len(target_cols)))\n    oof = train[target_cols].copy()\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    #print(test_.head())\n    for fold in range(NFOLDS):\n        \n        #trn_idx = train[train['kfold'] != fold].reset_index().index\n        #val_idx = train[train['kfold'] == fold].reset_index().index\n    \n        train_df = train[train['kfold'] != fold]#.reset_index(drop=True)\n        valid_df = train[train['kfold'] == fold]#.reset_index(drop=True)\n        \n       # print(len(train_df))\n        #print(len(valid_df))\n        \n        feature_cols = [col  for col in train_df.columns if not (col in target_cols.to_list()+['kfold'])]\n        \n        #print(feature_cols)\n        \n        X_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n        X_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n        X_test = test_[feature_cols].values\n            \n        oof_, pred_ = run_training(X_train,y_train,X_valid,y_valid,X_test,fold, seed,verbose,**kwargs)\n        \n        oof[train['kfold'] == fold] = oof_\n        \n        \n        \n        predictions += pred_ \/ NFOLDS\n        \n        \n    return oof, predictions","18acc6f5":"params ={'drop_rate1':0.5,'drop_rate2':0.2,'drop_rate3':0.2}","a3e32ac8":"# Averaging on multiple SEEDS\n\nSEED = [0,12347,565657,123123,78591]\n#SEED = [0]\ntrain,target_cols = train_short_form_loader('..\/input\/lish-moa\/train_features.csv','..\/input\/lish-moa\/train_targets_scored.csv')\ntest = test_short_form_loader(\"..\/input\/lish-moa\/test_features.csv\")\n\n\n\ntrain = preprocess_data(train)\ntest = preprocess_data(test)\n    \n\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n   \n    folds = multifold_indexer(train,target_cols,n_splits=NFOLDS)\n    \n    \n    oof_, predictions_ = run_k_fold(folds,target_cols,test,NFOLDS, seed,verbose=True,**params)\n    oof += oof_ \/ len(SEED)\n    predictions += predictions_ \/ len(SEED)\n\n#train[target_cols] = oof\ntest[target_cols] = predictions\n","69d40b08":" folds['kfold'].unique()","bc9f6878":"#valid_results = train.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n#valid_results\n\ny_true = train[target_cols].values\ny_pred = oof\n\nscore = 0\nfor i in range(len(target_cols)):\n   # print(log_loss(y_true[:, i], y_pred[:, i])\/ len(target_cols))\n    score_ = log_loss(y_true[:, i], y_pred.iloc[:, i],labels=[0,1])\n    #if score_ > 0.02:\n     #   print(score_)\n    score +=( score_ \/ len(target_cols))\n    \nprint(\"CV log_loss: \", score)\n    ","9bb7043e":"sample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission.set_index('sig_id',inplace=True)\ntest_features.set_index('sig_id',inplace=True)\ntest_features = test_features.loc[sample_submission.index]\n\nsub = sample_submission.drop(columns=target_cols).merge(test[target_cols], on='sig_id', how='left').fillna(0)\n#sub.set_index('sig_id',inplace=True)\nsub.loc[test_features['cp_type']=='ctl_vehicle', target_cols] =0\nsub.to_csv('.\/submission.csv', index=True)","4a879221":"# Dataset Classes","4f30fa93":"## Update:\n1. Model updated\n2. Changed to reduce LR in plateau\n3. Increased Seeds","f591e7c9":"\nReferences :\n1. @abhishek and @artgor 's Parallel Programming video https:\/\/www.youtube.com\/watch?v=VRVit0-0AXE\n2. @yasufuminakama 's Amazying Notebook https:\/\/www.kaggle.com\/yasufuminakama\/moa-pytorch-nn-starter \n3. @namanj27 mostly  from here https:\/\/www.kaggle.com\/namanj27\/new-baseline-pytorch-moa\n\ntorch BCE smoothing as implemented here https:\/\/gist.github.com\/MrRobot2211\n\n","3cdc8574":"# CV folds","10128b39":"# If you like it, Do Upvote :)","f612d393":"# Model","231c8c1e":"# Single fold training","ac40a819":"# feature Selection using Variance Encoding"}}