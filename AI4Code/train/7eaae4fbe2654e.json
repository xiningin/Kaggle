{"cell_type":{"9e0848f1":"code","4544e9bd":"code","97d7eebd":"code","07ab7175":"code","2fee588c":"code","887e927c":"code","849a0e62":"code","f92dae92":"code","abc6a4c1":"code","cd878b97":"code","2390affc":"code","b52a44a4":"code","21d0646b":"code","3c54e495":"code","8858c805":"code","893edbcc":"code","aacd8749":"code","552e8288":"code","0764c949":"code","33a63474":"code","a6341dcc":"code","9c64acb3":"code","1c4cf112":"code","072141ed":"code","04f6ac8c":"code","451ec23c":"code","53c2bdf4":"code","4bddd3dc":"code","5de2e38c":"code","e14c6b8c":"code","a7299312":"code","1ef469d3":"code","39966747":"code","2474746f":"code","7c92fc1b":"code","70a96ca9":"code","075efa45":"code","369dab86":"code","b0cc9171":"code","8d52c260":"code","ee5bdf9e":"code","9e2e7764":"code","75ea5ba3":"code","9f427429":"code","21e3d8c7":"code","672a4d63":"code","5cc08f8b":"code","ff83b7ad":"code","87327d10":"code","5e8f941e":"code","9c5bd1a4":"code","8d9bf1ae":"code","a99cc514":"code","559b4da5":"code","0ba1d19f":"code","a2fd0f3c":"code","d75eb7d7":"code","3d6fe8c7":"code","9a958826":"code","379f293f":"code","9f5b54c0":"code","0109efa0":"code","f5b98cd4":"code","6944f35c":"code","62fb9a78":"code","7bdee5a2":"code","a207872c":"code","4dd33c6c":"code","9e28056e":"code","66baa6aa":"code","e1fb40b6":"code","ca6e1d02":"code","a6daa767":"code","c019272c":"code","edceb922":"code","2ec4ae7b":"code","ee32df36":"code","23d73309":"code","d5a57549":"code","76217b8e":"code","455752ad":"markdown","3a8ea353":"markdown","5895d784":"markdown","27bcda2b":"markdown","0affea08":"markdown","28fa3b97":"markdown","3b31f619":"markdown","549d231d":"markdown","fc3f3cd8":"markdown","8d4c24e4":"markdown","39898089":"markdown","ea1c3218":"markdown","82c12765":"markdown","a8cc9ae9":"markdown","7490cb6e":"markdown","fe1107d9":"markdown","82740a9d":"markdown","5ca2aa7e":"markdown","d7dd688f":"markdown","2793b583":"markdown","942674ce":"markdown","8fa2697b":"markdown","196ccbbd":"markdown","5f614a37":"markdown","607741c9":"markdown","6c527040":"markdown","95c73031":"markdown","ee61de95":"markdown","d4ac5359":"markdown","f28dd9c3":"markdown","da1e4a7a":"markdown","74842d6b":"markdown","de0fc730":"markdown","404246e7":"markdown","f58fe631":"markdown","74e1e968":"markdown"},"source":{"9e0848f1":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","4544e9bd":"import pandas as pd","97d7eebd":"df = pd.read_csv('\/kaggle\/input\/Banco-PF-DSCOR9\/data_train.csv', index_col='id')\ndf_test = pd.read_csv('\/kaggle\/input\/Banco-PF-DSCOR9\/data_test.csv', index_col='id')","07ab7175":"df.head(3)","2fee588c":"df_test.head(3)","887e927c":"# Cambiamos el target a numeros\ndf.y = df.y.map({'yes': 1, 'no': 0})","849a0e62":"df.job.value_counts()","f92dae92":"def job_transform(df_):\n    # probamos agrupar por activamente trabajando y no\n    hasnt_work = ['retired', 'unemployed', 'student', 'unknown']\n    df_.loc[df_.job.isin(hasnt_work), 'has_work'] = 0\n    df_.loc[~df_.job.isin(hasnt_work), 'has_work'] = 1\n\n    # df.loc[df.job.isin(hasnt_work), 'job'] = 'unemployed'\n\n    # encodeamos las variables\n    df_ = pd.concat([df_.drop(columns=['job']), pd.get_dummies(df_.job, prefix='job')], axis=1)\n    return df_\n\ndf = job_transform(df)\ndf_test = job_transform(df_test)","abc6a4c1":"df.marital.value_counts()","cd878b97":"def marital_transform(df_):\n    # como la mayoria estan casados armamos solo 2 grupos (married y otros)\n    df_.marital = df_.marital.map(lambda x: 1 if x == 'married' else 0)\n    return df_\n\ndf = marital_transform(df)\ndf_test = marital_transform(df_test)","2390affc":"df.education.value_counts()","b52a44a4":"df_test.education.value_counts()","21d0646b":"def education_transform(df_):\n    from sklearn.preprocessing import LabelEncoder\n\n    df_.loc[df_.education.isin(['basic.4y', 'basic.6y', 'unknown']), 'education'] = 'basic.9y'  # porque unknown a basic.9y? no hay porque\n\n    le = LabelEncoder()\n    le.fit(['illiterate', 'basic.9y', 'high.school', 'professional.course', 'university.degree'])  # damos el orden\n    df_.education = le.transform(df_.education)\n    return df_\n\ndf = education_transform(df)\ndf_test = education_transform(df_test)","3c54e495":"def y_las_demas_transforms(df_):\n\n    for col in ['housing', 'loan', 'default', 'contact', 'poutcome']:\n        df_.loc[df_[col] == 'unknown', col] = 'no'\n\n        # encodeamos las variables\n        df_ = pd.concat([df_.drop(columns=[col]), pd.get_dummies(df_[col], prefix=col)], axis=1)\n    return df_\n\ndf = y_las_demas_transforms(df)\ndf_test = y_las_demas_transforms(df_test)","8858c805":"# \n# aca podriamos usar las funciones para fechas del modulo datetime\n# from datetime import datetime as dt\n# pd.to_datetime(['Wed', 'Thu', 'Mon', 'Tue', 'Fri'], format='%a')\n# \n# https:\/\/stackoverflow.com\/questions\/62205571\/pd-to-datetime-doesnt-work-with-a-format\n#\ndf.day_of_week = df.day_of_week.str.title().map({'Sun': 1, 'Mon': 2, 'Tue': 3, 'Wed': 4, 'Thu': 5, 'Fri': 6, 'Sat': 7})\ndf_test.day_of_week = df_test.day_of_week.str.title().map({'Sun': 1, 'Mon': 2, 'Tue': 3, 'Wed': 4, 'Thu': 5, 'Fri': 6, 'Sat': 7})","893edbcc":"df.month = pd.to_datetime(df.month, format='%b').dt.month\ndf_test.month = pd.to_datetime(df_test.month, format='%b').dt.month","aacd8749":"df.head(3)","552e8288":"df_test.head(3)","0764c949":"import numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\n\nfrom sklearn.metrics import classification_report, f1_score\n\nsns.set()","33a63474":"# Separamos las features del targer\nX = df.drop(columns=['y'])\ny = df.y","a6341dcc":"X.shape","9c64acb3":"# Separamos los conjuntos de train y test, usamos stratify=y para que train y test tengan la misma distribucion de y\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=16, stratify=y)","1c4cf112":"X_train.shape, X_test.shape","072141ed":"# Comprobamos que tengan el mismo porcentaje de instancias de cada clase\nsum(y_train)\/len(y_train), sum(y_test)\/len(y_test)","04f6ac8c":"# Creamos un objecto KFold que vamos a usar para cross_validation\nkf = KFold(n_splits=3, shuffle=True)","451ec23c":"from sklearn.metrics import roc_auc_score, roc_curve","53c2bdf4":"kf  # vamos a usar el mismo KFold","4bddd3dc":"param_grid = {\n    'max_depth' : [2, 5, 10],\n    'max_features' : ['sqrt', 'log2', X_test.shape[1]\/\/2],\n    'n_estimators' :[10, 20, 50],\n    'class_weight': [{0:1, 1:5}, {0:1, 1:2}],\n}","5de2e38c":"rf = RandomForestClassifier(min_samples_split=10, oob_score=True, random_state=16)","e14c6b8c":"gs_rf = GridSearchCV(rf, param_grid=param_grid, scoring='f1', cv=kf, verbose=1, n_jobs=-1, return_train_score=True)","a7299312":"%%time\ngs_rf.fit(X_train, y_train)","1ef469d3":"gs_rf.best_estimator_","39966747":"print(classification_report(y_test, gs_rf.predict(X_test)))","2474746f":"gs_rf.best_estimator_.oob_score_","7c92fc1b":"def plot_features_importance(estimator, column_names):\n    # Obtenemos el orden por importance \n    indices = np.argsort(estimator.feature_importances_)\n\n    # ploteamos\n    plt.figure(figsize=(16, 10))\n    sns.barplot(estimator.feature_importances_[indices], column_names[indices])\n    plt.title('Feature Importances')\n    plt.show();","70a96ca9":"plot_features_importance(gs_rf.best_estimator_, X_train.columns)","075efa45":"# Definimos una funcion para plotear curva roc ya que la vamos a volver a usar\n\ndef plot_roc_curve(estimator, X, y, name=''):\n    #Obtaining the ROC score\n    roc_auc = roc_auc_score(y, estimator.predict(X))\n    #Obtaining false and true positives & thresholds\n    fpr, tpr, thresholds = roc_curve(y, estimator.predict_proba(X)[:,1])\n    \n    #Plotting the curve\n    plt.figure(figsize=(10, 6))\n    plt.plot(fpr, tpr, label='roc test')\n\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([-0.01, 1.0]); plt.ylim([0.0, 1.05]); plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n    plt.title(f'ROC curve {name} (area = {roc_auc:0.03f})')\n    plt.legend()\n    plt.show();\n    ","369dab86":"plot_roc_curve(gs_rf, X_test, y_test, name='Random Forest')","b0cc9171":"fig = plt.figure(figsize=(10, 6))\nax = plt.axes(projection='3d')\n\n# Data for a three-dimensional line\nxline = X_train['emp.var.rate']\nyline = X_train['euribor3m']\nzline = X_train['nr.employed']\n\nax.scatter3D(xline, yline, zline, c=y_train)  # c=y_train para diferencias las clases","8d52c260":"param_grid = {\n    'n_estimators': [10, 40, 100],\n    'learning_rate': np.linspace(1e-3, 1, 5)\n}","ee5bdf9e":"ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), algorithm='SAMME.R', random_state=16)","9e2e7764":"gs_ada = GridSearchCV(ada, param_grid=param_grid, scoring='f1', cv=kf, verbose=1, n_jobs=-1, return_train_score=True)","75ea5ba3":"%%time\n# Tener en cuenta que por la cantidad de fit de GridSeach (la combianacion de los hiperametros) estan los fit internos de AdaBoost\ngs_ada.fit(X_train, y_train)","9f427429":"gs_ada.best_estimator_","21e3d8c7":"print(classification_report(y_test, gs_ada.predict(X_test)))","672a4d63":"f1_test = []\nf1_train = []\n\n# Calculamos el accuracy sobre el test set\nfor prediccion_test in gs_ada.best_estimator_.staged_predict(X_test):\n    f1_test.append(f1_score(y_test, prediccion_test))\n\nfor prediccion_train in gs_ada.best_estimator_.staged_predict(X_train):    \n    f1_train.append(f1_score(y_train, prediccion_train))\n\nplt.figure(figsize=(12, 6))\n\n# ploteamos las lineas de train y test\nplt.plot(range(1, len(f1_train) + 1), f1_train, label = 'Train', color=\"b\")\nplt.plot(range(1, len(f1_test) + 1), f1_test, label = 'Test', color=\"r\")\n\nplt.legend()\nplt.title(f'F1-Score {f1_score(gs_ada.predict(X_test), y_test):.2f}')\nplt.ylabel('f1')\nplt.ylim([0.2, 0.7])\nplt.xlabel('Cantidad de estimadores')\nplt.show();","5cc08f8b":"# !pip install xgboost\nimport xgboost as xgb","ff83b7ad":"# dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=X_train.columns, nthread=-1)\n# dtest = xgb.DMatrix(X_test, label=y_test, feature_names=X_test.columns, nthread=-1)","87327d10":"# Definimos nuestra metrica f1 personalizada para xgboost\n# 1 - f1_score es porque queremos que \n\ndef f1_eval(y_pred, dtrain):\n    y_true = dtrain.get_label()\n    err = 1-f1_score(y_true, np.round(y_pred))\n    return 'f1_err', err","5e8f941e":"xgb_clf = xgb.XGBClassifier(objective='binary:logistic', gamma=0.01, min_child_weight=3, verbosity=1, random_state=16)","9c5bd1a4":"sum(y==0)\/sum(y==1)","8d9bf1ae":"# ponemos varios parametros al azar realmente leyendo solo un poco la documentacion https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n\nparam_grid = [\n    { # booster gbtree\n    'booster': ['gbtree'],\n    'max_depth': [2, 4, 5],\n    'learning_rate': np.linspace(1e-5, 1, 4),\n    'n_estimators': [100, 200],\n    'scale_pos_weight': np.arange(1, 15, 4)  # recomendado: sum(y_train==0)\/sum(y_train==1) \n    }\n]","a99cc514":"gs_xgb = GridSearchCV(xgb_clf, param_grid=param_grid, scoring='f1', cv=kf, verbose=1, n_jobs=-1, return_train_score=True)","559b4da5":"%%time\ngs_xgb.fit(X_train, y_train,\n       eval_set=[(X_train, y_train), (X_test, y_test)],\n       eval_metric=f1_eval,\n       early_stopping_rounds=20,\n       verbose=True)","0ba1d19f":"gs_xgb.best_estimator_","a2fd0f3c":"print(classification_report(y_test, gs_xgb.predict(X_test)))","d75eb7d7":"plot_features_importance(gs_xgb.best_estimator_, X_train.columns)","3d6fe8c7":"# pip install catboost\n# pip install ipywidgets  # libreria para plot en jupyter\nfrom catboost import CatBoostClassifier","9a958826":"cat_boost = CatBoostClassifier( random_seed=16)  # tiene mas hiperparametros que ..","379f293f":"grid = {\n    'n_estimators': [60, 100, 150],\n    'learning_rate': [0.03, 0.1],\n    'depth': [4, 6, 10],\n    'l2_leaf_reg': [1, 3, 5, 7, 9]\n}","9f5b54c0":"%%time\ngrid_search_result = cat_boost.grid_search(grid, \n                                           X=X_train,\n                                           y=y_train, \n                                           plot=True,\n                                           cv=kf)","0109efa0":"print(classification_report(y_test, cat_boost.predict(X_test)))","f5b98cd4":"# !pip install lightgbm\nfrom lightgbm import LGBMClassifier","6944f35c":"clf_lgbm = LGBMClassifier(objective='binary', random_state=16)","62fb9a78":"param_grid = {\n    'n_estimators': [60,100,150],\n    'boosting_type': ['gbdt', 'rf'],\n    'max_depth': [2,5,10],\n    'learning_rate': np.linspace(1e-5, 1, 5),\n}","7bdee5a2":"gs_lgbm = GridSearchCV(clf_lgbm, param_grid=param_grid, scoring='f1', cv=kf, verbose=1, n_jobs=-1, return_train_score=True)","a207872c":"%%time\ngs_lgbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric='binary')","4dd33c6c":"print(classification_report(y_test, gs_lgbm.predict(X_test)))","9e28056e":"from sklearn.svm import SVC","66baa6aa":"svm = SVC(probability=True, random_state=16)","e1fb40b6":"grid = [\n#     {\n#         'kernel': ['linear'],\n#         'C': np.linspace(0.01, 2, 5),\n#     }, \n    {\n        'kernel': ['rbf', 'poly'],\n        'C': np.linspace(1e-2, 1, 5),\n        'degree': [2,3,5],\n#         'gamma': ['scale', 'auto'],\n#         'coef0': np.arange(0, 10, 3),\n    }\n]","ca6e1d02":"gs_svm = GridSearchCV(svm, param_grid=grid, scoring='f1', cv=kf, verbose=1, n_jobs=-1, return_train_score=True)","a6daa767":"%%time\ngs_svm.fit(X_train, y_train)","c019272c":"print(classification_report(y_test, gs_svm.predict(X_test)))","edceb922":"# vamos a medir el resultado de f1 para distintos pesos entre xgboost y svm\nfor p in np.linspace(0, 1, 6):\n    predict_probs = gs_svm.predict_proba(X_test)[:,1] * p + gs_xgb.predict_proba(X_test)[:,1] * (1-p)\n\n    predict = np.where(predict_probs >= 0.5, 1, 0)\n\n    print(f'svm * {p:.1f} + xgboost * {1-p:.1f}: {f1_score(y_test, predict, average=\"macro\"):.4f}')","2ec4ae7b":"# vamos a medir el resultado de f1 para distintos pesos entre rf y svm\nfor p in np.linspace(0, 1, 6):\n    predict_probs = gs_rf.predict_proba(X_test)[:,1] * p + gs_xgb.predict_proba(X_test)[:,1] * (1-p)\n\n    predict = np.where(predict_probs >= 0.5, 1, 0)\n\n    print(f'rf * {p:.1f} + xgboost * {1-p:.1f}: {f1_score(y_test, predict, average=\"macro\"):.4f}')","ee32df36":"# Vamos con estos pesos\n\npredict_probs = gs_svm.predict_proba(df_test)[:,1] * 0.2 + gs_xgb.predict_proba(df_test)[:,1] * 0.8\n\npredict = np.where(predict_probs >= 0.5, 1, 0)","23d73309":"sum(predict), sum(predict)\/len(predict) # Total de 1","d5a57549":"pd.Series(predict, name='y').to_csv('sample_submit.csv', index_label='id')","76217b8e":"!head sample_submit.csv","455752ad":"- Marital","3a8ea353":"- Creamos un object DMatrix, lo cual es una especie de dataframe optimizado para xgboost, con esto obtenemos menor tiempo de entrenamiento y menor consumo de memoria ram","5895d784":"### Vamos a ver que hay en las 3 columnas mas importantes de Random Forest","27bcda2b":"---","0affea08":"#### estimators = \\[(gs_rf, .73), (gs_ada, .67), (gs_xgb, .72), (cat_boost, .66), (gs_lgbm, .63), (gs_svm, .64)]","28fa3b97":"### vamos a hacer una ponderacion de las probabilidades de cada algoritmo con el objectivo de hacer \"voting\"   \n( o mejor dicho un resultado ponderado de diferentes algoritmos )","3b31f619":"---","549d231d":"---","fc3f3cd8":"### LightGBM","8d4c24e4":"- **nr.employed** Parece ser la columna mas importante, estaria bueno ver que pasa sin esa columna (puede estar sesgando otras)  \no ver que tiene y que transformaciones de Feature Enginering se le pueden hacer para mantener la info sin que tome todo el control del modelo","39898089":"---","ea1c3218":"### Cargamos un data set para trabajar","82c12765":"- Month, day_of_week","a8cc9ae9":"- Xgboost sklearn API (Vamos a usar Xgboost en modo de compativilidad con sklearn, osea con fit y predict)\n- Xgboost tiene otros metodos propios para entrenar, hacer cross_validation y predecir","7490cb6e":"### Random Forest","fe1107d9":"![alt text][image]\n\n[image]: https:\/\/miro.medium.com\/max\/1088\/1*m2UHkzWWJ0kfQyL5tBFNsQ.png \"Logo Title Text 2\"","82740a9d":"### AdaBoost","5ca2aa7e":"---","d7dd688f":"- Housing, Loan, Default, Contact, Poutcome","2793b583":"### Ensambles:\n#### - Idea principal: ***Construir modelo predictivo en forma de un conjunto de modelos de predicci\u00f3n d\u00e9biles***\nLa predicci\u00f3n final viene dada por una combinaci\u00f3n ponderada de todos los clasificadores\nd\u00e9biles que se han producido anteriormente:","942674ce":"### EDA y Feature Engineer por columnas","8fa2697b":"### XGBoost","196ccbbd":"### Learning rate\n\nControla el ritmo al que \"aprenden\" los modelos. Suelen recomendarse valores de 0.01 o 0.001, aunque la elecci\u00f3n correcta puede variar dependiendo del problema. Cuanto menor sea, m\u00e1s \u00e1rboles se necesitan para alcanzar buenos resultados pero menor es el riesgo de overfitting.","5f614a37":"### Diferencias:\n**Bagging**\n- Se emplean modelos con muy poco bias pero mucha varianza (**DecisionTrees con mucha profundidad**).  \n- Agregando muchos de estos modelos se consigue reducir la varianza sin apenas inflar el bias.  \n- Cada modelo final es distinto de los otros porque se entrenan con diferentes muestras obtenidas por bootstrapping a partir de la muestra original.\n- Todos los modelos son independientes entre si.\n- El n\u00famero de \u00e1rboles creados no es un hiperpar\u00e1metro cr\u00edtico, por mucho que se incremente, no aumenta el riesgo de overfitting. En un determinado n\u00famero de \u00e1rboles, la reducci\u00f3n de test error se estabiliza.( tener en cuenta que  cada \u00e1rbol ocupa memoria ).\n- RandomForest agrega a esta idea la seleccion aleatoria de features en X_train para cada arbol, asi evitar que algunas features muy influyentes dominen todos los arboles ( tendriamos muchos arboles iguales ).\n- Interesante ver [Extremely randomized trees](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html) ( un paso mas de RandomForest )\n\n\n**Boosting**\n- El Boosting se encarga de aplicar secuencialmente un clasificador d\u00e9bil a versiones modificadas de los datos, produciendo una secuencia de clasificadores d\u00e9biles.\n- Se emplean modelos con muy poca varianza pero mucho bias (**DecisionTrees con poca profundidad**), ajustando secuencialmente muchos modelos se reduce el bias.  \n- Los modelos funcionan sobre la misma muesta de datos variando la importancia de cada instancia para cada modelo.\n- Cada modelo es dependiente del modelo anterior, recibiendo de este como input adiccional el error en la prediccion del predictor anterior. Se busca usar esa informacion adicional para mejorar la prediccion de esos errores ( puede generar errores nuevos en instancias predichas bien anteriormente ).\n- El n\u00famero de \u00e1rboles creados es un hiperpar\u00e1metro cr\u00edtico en este tipo de ensamble, puede sufrir **overfitting** si este valor es excesivamente alto.\n","607741c9":"### Gradient Boosting\n- Gradient Boosting es una generalizaci\u00f3n del algoritmo AdaBoost que permite emplear cualquier funci\u00f3n de p\u00e9rdida, siempre que esta sea diferenciable. La funci\u00f3n de p\u00e9rdida es una medida que indica qu\u00e9 tan buenos son los coeficientes del modelo para ajustar los datos. \n- Esta posibilidad de elegir la funcion objetivo a minimizar es la principal diferencia con AdaBoost, y seguramente el motivo por el cual Grandien Boosting tiene mejores resultados en problemas reales.\n- La funcion de perdida podria ser Gini, entrop\u00eda, MSE, etc.\n\nSin embargo, en lugar de cambiar los pesos para cada observaci\u00f3n clasificada incorrecta en cada iteraci\u00f3n como AdaBoost, el m\u00e9todo Gradient Boosting intenta ajustar el nuevo predictor a los errores residuales cometidos por el predictor anterior.","6c527040":"![alt text][image]\n\n[image]: https:\/\/miro.medium.com\/max\/1400\/0*qCcM7uCOqIw6npnJ.png \"Logo Title Text 2\"","95c73031":"# Voting","ee61de95":"- Vemos que comparando XGBoost vs RandomForest, para XGBoost tenemos mas recall sobre la clase 1, esto nos dice que podra capturar mas 1 al predecir a costa de clasficar mal a mas 0 que el RandomForest","d4ac5359":"### AdaBoost\n- Es un meta-algoritmo de boosting.\n- Adaboost utiliza toda la muestra de entrenamiento para entrenar a cada clasificador\n- Utiliza una funcion exponencial de coste.\n\n#### Funcionamiento:\nSe define:  \n1- El modelo debil usado ( usualmente DecisionTree con poca profundidad )  \n2- Un array de importancia para cada instancia en el X_train ( inicialmente todas iguales )  \n3- La cantidad de modelos  \n\nLuego se inicia un proceso iterativo donde se entrena un modelo dandole como input el X_train y el array de importancia,  \ncon el modelo fiteado y almacenado, se predice sobre el mismo conjunto de train y se identifican las instancias mal clasificadas.  \nCon esta informacion se actuliza el array de importancia, aumentando para cada instancia mal clasificada y disminuyendo las bien clasificadas; y se asigna un peso total al modelo, proporcional al total de aciertos.  \nFinalmente se repite esos pasos generando un nuevo modelo con una clasificacion diferente.  \nLa iteraciones totales seran el numero de modelos a crear.  \n\nPara predecir nuevas observaciones, se obtiene la predicci\u00f3n de cada uno de los modelos que forman el ensemble y se promedian sus resultados, ponderando el peso de cada modelo.  \n\n- AdaBoost para clasificacion tiene dos versiones principales **Discrete AdaBoost** y **Real AdaBoost**.  \nLa diferencia entre ambas es el formato de las predicciones que devuelve cada clasificador debil *(predict o predict_proba)*.  \nSe recomienda usar Real Adaboost, en AdaBoostClassifier de sklearn estas variantes se eligen con el parametro *algorithm: {\u2018SAMME\u2019, \u2018SAMME.R\u2019}*\n\n\n- [Learning rate in AdaBoost](https:\/\/stats.stackexchange.com\/questions\/82323\/shrinkage-parameter-in-adaboost)","f28dd9c3":"### CatBoost\n\n-  [CatBoost](https:\/\/catboost.ai\/) es otro algoritmo de boosting que usa descenso por gradiente","da1e4a7a":"- **Nota**: No parece haber mucha info aca, pero lo dejo para que prueben con otras columnas","74842d6b":"---","de0fc730":"- Job","404246e7":"---","f58fe631":"- Education","74e1e968":"- **Nota**:  \n`f'valor de {variable}'` es una forma de armar string en python se llaman **[f-string](https:\/\/realpython.com\/python-f-strings\/)**, \ncuando nuestra variable es tipo float podemos indicar cuantos decimales queremos: `{variable_float : .3f}` indicando que solo use 3 decimales"}}