{"cell_type":{"fe2a42ce":"code","7c6e16a3":"code","571cbc6b":"code","01431237":"code","5f5f93e8":"code","11b975f1":"code","0051ba0f":"code","3262ae29":"code","a515a1e9":"code","921a49f5":"code","f666611a":"code","84b9448d":"code","cc01d615":"code","6b429831":"code","3f67eda9":"code","b5d3db21":"code","1a19b173":"markdown","046e4e11":"markdown","6693b819":"markdown","a45c31ab":"markdown"},"source":{"fe2a42ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7c6e16a3":"train_dataset = '..\/input\/train_V2.csv'\ntest_dataset = '..\/input\/test_V2.csv'","571cbc6b":"train_df = pd.read_csv(train_dataset)\ntrain_df.head()","01431237":"train_df.isnull().sum() #Check if there are any null\ntrain_df = train_df.dropna() #remove nulls from datasets","5f5f93e8":"target = train_df['winPlacePerc'] #target variable to find\nfeatures = train_df.drop(['winPlacePerc'],axis=1) #input features\nfeatures.head()","11b975f1":"refine_features = features.drop(['Id','groupId','matchId'],axis=1) #drop unnecessary features\nrefine_features.info()","0051ba0f":"train_df.corr()['winPlacePerc'].sort_values().plot(kind='bar',figsize=(11,7))","3262ae29":"#Convert categorical variables to numerical by encoding of 0 and 1\nrefine_features = pd.get_dummies(refine_features)\nrefine_features.info()","a515a1e9":"#calculate vif of each column(feature)\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['vif factor'] = [variance_inflation_factor(refine_features.values,i) for i in range(refine_features.shape[1])]\nvif['features'] = refine_features.columns\nvif.sort_values(by=['vif factor'],ascending=False)\n\n","921a49f5":"#dropping columns based on hig VIF\nlist_of_drop_cols = ['maxPlace','numGroups','matchType_squad-fpp','matchType_duo-fpp','winPoints','matchType_solo-fpp','rankPoints','matchType_squad','matchType_duo','matchType_solo']\nrefine_features = refine_features.drop(list_of_drop_cols,axis = 1)\nrefine_features.shape","f666611a":"#Create cross validation test sets to check if model is trained well or not.\nfrom sklearn.model_selection import train_test_split\nXtrain,Xvalidation,Ytrain,Yvalidation = train_test_split(refine_features,target,test_size=0.25)\nprint(len(Xtrain))\nprint(len(Ytrain))\nprint(len(Xvalidation))\nprint(len(Yvalidation))","84b9448d":"#GradientBoostingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor \nfrom sklearn.metrics import r2_score\n\nlinear = GradientBoostingRegressor(learning_rate = 1.0, n_estimators = 100, max_depth = 4)\nlinear.fit(Xtrain,Ytrain)\npred = linear.predict(Xvalidation)\nprint(r2_score(Yvalidation,pred))\n\n","cc01d615":" \ntest_df = pd.read_csv(test_dataset)\ntest_features = test_df.drop(['Id','groupId','matchId'],axis = 1)\ntest_features = pd.get_dummies(test_features)\ntest_features = test_features.drop(['maxPlace','numGroups','matchType_squad-fpp'\n                              ,'matchType_duo-fpp','winPoints','matchType_solo-fpp','rankPoints',\n                              'matchType_squad','matchType_duo','matchType_solo'],axis=1)\n\nXtest = test_features\ntest_features.shape","6b429831":"from sklearn.ensemble import GradientBoostingRegressor\n\n\nlinear = GradientBoostingRegressor()\nlinear.fit(Xtrain,Ytrain)\npred = linear.predict(Xtest)\npred[:10]\n","3f67eda9":"pred_df = pd.DataFrame(pred,test_df['Id'],columns=['WinPlacePerc'])\npred_df.head()","b5d3db21":"\n\npred_df.to_csv('sample_submission.csv')\n\n","1a19b173":"**DATA PREPROCESSING**\n1. Reading the data into the training dataframe\n2. Dropping the NaN values and check if there are any nulls in dataset.\n3. Seperating the features from target variable\n4. Dropping the Unneccessary columns(features) like match id, id and Group id.","046e4e11":"**Visualization**\n\nPlot a bar graph where we see the correlation between the target variable(winPlacePerc) and features to get a clear picture which feature is most correlated with the winning place percentage in a sorted ascending order.","6693b819":"**Variance Inflation Factor(VIF) and Multicollinearity**\n\n**Multicollinearity** - occurs when independent variables in a regression model are correlated. If this happens it can cause accuracy issues while fitting the model because independent variables should be independent and not correlated.\n\n**VIF** - explains the amount of multicollinearity exists between independent variables(predictors) in regression analysis.\nSo high VIF for a feature means highly correlated to other features and vice versa.\nWhen a VIF is above 2.5 then you cannot ignore multicollinearity.","a45c31ab":"**Load the Testing Dataset**\n\n1. Load the testing dataset in the test dataframe.\n2. drop all the unnecessary variables with high VIF and encode the categorical variables into numerical.\n3. Apply the gradient boosting Regressor to the testing dataframe.\n4. Create the pred_df framework as per the submission file and save the file to csv.\n"}}