{"cell_type":{"6175720d":"code","157da806":"code","67e5d52a":"code","175ac0cc":"code","bef8311b":"code","78528e50":"code","85bc688d":"code","2b9257b7":"code","e0c031a7":"code","ddb7debd":"code","86119e3c":"code","8a1f362f":"code","8639db4b":"code","5eb738e3":"code","b7620db1":"code","ff3fb692":"code","c4e05260":"code","2d2cf325":"code","98a4e25a":"markdown","b04977f7":"markdown","1bde2cb2":"markdown","0a6cc039":"markdown","4f4308db":"markdown","1be3eb0c":"markdown","f9c90f84":"markdown","12f3e814":"markdown","e421530f":"markdown","1c10b86c":"markdown","0cdddc89":"markdown"},"source":{"6175720d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","157da806":"import torch\nimport matplotlib.pyplot as plt\nfrom torch.autograd import Variable\nimport torchvision.transforms as transforms\nimport torch.nn as nn \nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")","67e5d52a":"house_price_k = [300,450,500,670,730,858,912,1000,1200]\nhouse_price_k = np.array(house_price_k,dtype=np.float32)\n# Normalize data to prevent inf loss\nhouse_price_k = house_price_k\/house_price_k.max(axis=0)\nhouse_price_k = house_price_k.reshape(-1,1)\nhouse_price_k = Variable(torch.from_numpy(house_price_k))\n\nhouses_sold = [150,70,66,60,55,55,45,30,1]\nhouses_sold = np.array(houses_sold,dtype=np.float32)\n# Normalize data to prevent inf loss\nhouses_sold = houses_sold\/houses_sold.max(axis=0)\nhouses_sold = houses_sold.reshape(-1,1)\nhouses_sold = Variable(torch.from_numpy(houses_sold))\n\nplt.scatter(house_price_k,houses_sold)\nplt.xlabel(\"House prices in \u00a31000 (normalized)\")\nplt.ylabel(\"Number of houses sold at that price\")\nplt.title(\"House Price VS Number of Houses sold\")\nplt.show()","175ac0cc":"class LinearRegression(nn.Module):\n    def __init__(self,input_size,output_size):\n        # super function. It inherits from nn.Module and \n        # we can access everything in nn.Module\n        super(LinearRegression,self).__init__()\n        # Linear function.\n        self.linear = nn.Linear(input_dim,output_dim)\n\n    def forward(self,x):\n        return self.linear(x)\n    \n# define model\ninput_dim = 1\noutput_dim = 1\nmodel = LinearRegression(input_dim,output_dim) # input and output size are 1\n\n# MSE\nmse = nn.MSELoss()\n\n# Optimization (find parameters that minimize error)\nlearning_rate = 0.4  # how fast we reach best parameters\noptimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n\n# train model\nloss_list = []\niteration_number = 10000\nfor iteration in range(iteration_number):\n        \n    # optimization\n    optimizer.zero_grad() \n    \n    # Forward to get output\n    results = model(house_price_k)\n    \n    # Calculate Loss\n    loss = mse(results, houses_sold)\n    \n    # backward propagation\n    loss.backward()\n    \n    # Updating parameters\n    optimizer.step()\n    \n    # store loss\n    loss_list.append(loss.data)\n    \n    # print loss\n    if(iteration %150== 0):\n        print('epoch {}, loss {}'.format(iteration, loss.data))\n\nplt.plot(range(iteration_number),loss_list)\nplt.xlabel(\"Number of Iterations\")\nplt.ylabel(\"Loss\")\nplt.show()","bef8311b":"# predict our car price \npredicted = model(house_price_k).data.numpy()\nplt.scatter(house_price_k,houses_sold,label = \"original data\",color =\"red\")\nplt.scatter(house_price_k,predicted,label = \"predicted data\",color =\"blue\")\nplt.legend()\nplt.xlabel(\"House prices in \u00a31000 (normalized)\")\nplt.ylabel(\"Number of houses sold at that price\")\nplt.title(\"House Price VS Predicted Price\")\nplt.show()","78528e50":"# Prepare Dataset\n# load data\ntrain = pd.read_csv(r\"..\/input\/train.csv\",dtype = np.float32)\n\n# split data into features(pixels) and labels(numbers from 0 to 9)\ntargets_numpy = train.label.values\nfeatures_numpy = train.loc[:,train.columns != \"label\"].values\/255 # normalization\n\n# train test split. Size of train data is 80% and size of test data is 20%. \nfeatures_train, features_test, targets_train, targets_test = train_test_split(features_numpy,\n                                                                             targets_numpy,\n                                                                             test_size = 0.2,\n                                                                             random_state = 42) \n\n# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\nfeaturesTrain = torch.from_numpy(features_train)\ntargetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n\n# create feature and targets tensor for test set.\nfeaturesTest = torch.from_numpy(features_test)\ntargetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long\n\n# batch_size, epoch and iteration\nbatch_size = 100\nn_iters = 50000\nnum_epochs = n_iters \/ (len(features_train) \/ batch_size)\nnum_epochs = int(num_epochs)\n\n# Pytorch train and test sets\ntrain = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\ntest = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n\n# data loader\ntrain_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = False)\n\n# visualize one of the images in data set\nplt.imshow(features_numpy[7].reshape(28,28))\nplt.axis(\"off\")\nplt.title(str(targets_numpy[7]))\nplt.savefig('graph.png')\nplt.show()","85bc688d":"# Create Logistic Regression Model\nclass LogisticRegressionModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LogisticRegressionModel, self).__init__()\n        # Linear part\n        self.linear = nn.Linear(input_dim, output_dim)\n        # There should be logistic function right?\n        # However logistic function in pytorch is in loss function\n        # So actually we do not forget to put it, it is only at next parts\n    \n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\n# Instantiate Model Class\ninput_dim = 28*28 # size of image px*px\noutput_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n\n# create logistic regression model\nmodel = LogisticRegressionModel(input_dim, output_dim)\n\n# Cross Entropy Loss  \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer \nlearning_rate = 0.001\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","2b9257b7":"# Traning the Model\ncount = 0\nloss_list = []\niteration_list = []\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        \n        # Define variables\n        train = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and cross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculate gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        # Prediction\n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Predict test dataset\n            for images, labels in test_loader: \n                test = Variable(images.view(-1, 28*28))\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n                \n                # Total correct predictions\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n        if count % 700 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {}%'.format(count, loss.data, accuracy))","e0c031a7":"# visualization\nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Logistic Regression: Loss vs Number of iteration\")\nplt.show()","ddb7debd":"# Create ANN Model\nclass ANNModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(ANNModel, self).__init__()\n        # Linear function 1: 784 --> 100\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity 1\n        self.relu1 = nn.ReLU()\n        \n        # Linear function 2: 100 --> 100\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 2\n        self.tanh2 = nn.Tanh()\n        \n        # Linear function 3: 100 --> 100\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 3\n        self.elu3 = nn.ELU()\n        \n        # Linear function 4 (readout): 100 --> 10\n        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n    \n    def forward(self, x):\n        # Linear function 1\n        out = self.fc1(x)\n        # Non-linearity 1\n        out = self.relu1(out)\n        \n        # Linear function 2\n        out = self.fc2(out)\n        # Non-linearity 2\n        out = self.tanh2(out)\n        \n        # Linear function 2\n        out = self.fc3(out)\n        # Non-linearity 2\n        out = self.elu3(out)\n        \n        # Linear function 4 (readout)\n        out = self.fc4(out)\n        return out\n\n# instantiate ANN\ninput_dim = 28*28\nhidden_dim = 100 #hidden layer dim is one of the hyper parameter and it should be chosen and tuned. For now I only say 150 there is no reason.\noutput_dim = 10\n\n# Create ANN\nmodel = ANNModel(input_dim, hidden_dim, output_dim)\n\n# Cross Entropy Loss \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlearning_rate = 0.04\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","86119e3c":"# ANN model training\ncount = 0\nloss_list = []\niteration_list = []\naccuracy_list = []\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n\n        train = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and ross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculating gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Predict test dataset\n            for images, labels in test_loader:\n\n                test = Variable(images.view(-1, 28*28))\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n            if count % 500 == 0:\n                # Print Loss\n                print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data.item(), accuracy))","8a1f362f":"# visualization loss \nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"ANN: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.plot(iteration_list,accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"ANN: Accuracy vs Number of iteration\")\nplt.show()","8639db4b":"# Create CNN Model\nclass CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n        \n        # Convolution 1\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n        self.relu1 = nn.ReLU()\n        \n        # Max pool 1\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n     \n        # Convolution 2\n        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n        self.relu2 = nn.ReLU()\n        \n        # Max pool 2\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n        \n        # Fully connected 1\n        self.fc1 = nn.Linear(32 * 4 * 4, 10) \n    \n    def forward(self, x):\n        # Convolution 1\n        out = self.cnn1(x)\n        out = self.relu1(out)\n        \n        # Max pool 1\n        out = self.maxpool1(out)\n        \n        # Convolution 2 \n        out = self.cnn2(out)\n        out = self.relu2(out)\n        \n        # Max pool 2 \n        out = self.maxpool2(out)\n        out = out.view(out.size(0), -1)\n\n        # Linear function (readout)\n        out = self.fc1(out)\n        \n        return out\n\n# batch_size, epoch and iteration\nbatch_size = 100\nn_iters = 2500\nnum_epochs = n_iters \/ (len(features_train) \/ batch_size)\nnum_epochs = int(num_epochs)\n\n# Pytorch train and test sets\ntrain = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\ntest = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n\n# data loader\ntrain_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = False)\n    \n# Create ANN\nmodel = CNNModel()\n\n# Cross Entropy Loss \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlearning_rate = 0.1\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","5eb738e3":"# CNN model training\ncount = 0\nloss_list = []\niteration_list = []\naccuracy_list = []\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        \n        train = Variable(images.view(100,1,28,28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and ross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculating gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        count += 1\n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                \n                test = Variable(images.view(100,1,28,28))\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n                \n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n            if count % 500 == 0:\n                # Print Loss\n                print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data.item, accuracy))","b7620db1":"# visualization loss \nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"CNN: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.plot(iteration_list,accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"CNN: Accuracy vs Number of iteration\")\nplt.show()","ff3fb692":"# Create RNN Model\nclass RNNModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n        super(RNNModel, self).__init__()\n        # Number of hidden dimensions\n        self.hidden_dim = hidden_dim\n        \n        # Number of hidden layers\n        self.layer_dim = layer_dim\n        \n        # RNN\n        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, \n                          nonlinearity='relu')\n        \n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n    \n    def forward(self, x):\n        # Initialize hidden state with zeros\n        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n            \n        # One time step\n        out, hn = self.rnn(x, h0)\n        out = self.fc(out[:, -1, :]) \n        return out\n\n# batch_size, epoch and iteration\nbatch_size = 100\nn_iters = 2500\nnum_epochs = n_iters \/ (len(features_train) \/ batch_size)\nnum_epochs = int(num_epochs)\n\n# Pytorch train and test sets\ntrain = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\ntest = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n\n# data loader\ntrain_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = False)\n    \n# Create RNN\ninput_dim = 28    # input dimension\nhidden_dim = 100  # hidden layer dimension\nlayer_dim = 2     # number of hidden layers\noutput_dim = 10   # output dimension\n\nmodel = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n\n# Cross Entropy Loss \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlearning_rate = 0.05\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","c4e05260":"seq_dim = 28  \nloss_list = []\niteration_list = []\naccuracy_list = []\ncount = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n\n        train  = Variable(images.view(-1, seq_dim, input_dim))\n        labels = Variable(labels )\n            \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and ross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculating gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        if count % 250 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                images = Variable(images.view(-1, seq_dim, input_dim))\n                \n                # Forward propagation\n                outputs = model(images)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += labels.size(0)\n                \n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n            if count % 500 == 0:\n                # Print Loss\n                print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data.item(), accuracy))","2d2cf325":"# visualization loss \nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"RNN: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.plot(iteration_list,accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"RNN: Accuracy vs Number of iteration\")\nplt.savefig('graph.png')\nplt.show()","98a4e25a":"ANNs are able to better fit data containing non-linearities (by adding more non linear functions as hidden layer)","b04977f7":"Simple Linear Regression Example using Pytorch","1bde2cb2":"# Logistic Regression","0a6cc039":"# Convolutional Neural Network (CNN)","4f4308db":"# Pytorch Tutorial","1be3eb0c":"## Linear Regression","f9c90f84":"Following DATAI guide (https:\/\/www.kaggle.com\/kanncaa1\/pytorch-tutorial-for-deep-learning-lovers)","12f3e814":"# Recurrent Neural Network (RNN)","e421530f":"Logistic regression is good at classification but when complexity(non linearity) increases, the accuracy of model decreases.","1c10b86c":"Linear Regression can lead to better results than Linear Regression for Classification porpuses\n\nLogistic Regression = Linear Regression + Logistic Function(softmax)","0cdddc89":"# Artificial Neural Network (ANN)"}}