{"cell_type":{"44e8f952":"code","e91e33d3":"code","d45b0b70":"code","19938223":"code","ec1cf34b":"code","27d3f2a2":"code","dc71db9c":"code","e83b96dc":"code","04c851d5":"code","93116ee7":"code","ae3420d4":"code","36b40520":"code","a77b0fc0":"code","7a956400":"code","a942daf6":"code","75cede85":"code","f7028da8":"code","3848a79d":"code","203f6972":"code","663ee20c":"code","a7916306":"code","279a3bdd":"code","cf286dce":"code","be154805":"code","c4cc2127":"code","6fd6d9c5":"code","9adaad20":"code","c170a001":"code","62c557d2":"code","eb508d42":"code","6bea56ad":"code","71a64e5d":"code","aa0eac57":"code","d9d635ae":"code","03925e27":"code","557f4576":"code","b0cefa09":"code","93b52bde":"code","cde9da2a":"code","19e132b1":"code","78e29977":"code","378ed05f":"code","7636d24f":"code","b1f291d5":"code","d4089a73":"code","8d86ab87":"code","887538cc":"code","c3ef0541":"code","48f3fa1a":"code","4c6929d2":"code","3868c63b":"code","2b8d9dc4":"code","69f45cb0":"code","fc112942":"code","668886ad":"code","f111547b":"code","80f59af9":"code","17b0646f":"code","eede5329":"code","51e63413":"code","e7cb7672":"code","3478a62d":"code","898ec57f":"code","9ef9d140":"code","31142757":"code","1b800dee":"code","aef9f6ac":"code","1174df51":"code","4b9dd3a5":"code","78cdf038":"code","e9307225":"code","23362a9c":"code","07584e7e":"code","9be2269b":"markdown","1438cb24":"markdown","0b84d7cf":"markdown","ce8400a4":"markdown","859d0cc8":"markdown","d4b1d24c":"markdown","3294e07e":"markdown","2e550089":"markdown","3dbda4e0":"markdown","a3fe80a8":"markdown","9029e2b2":"markdown","3da1735d":"markdown","fb8c9be4":"markdown","94e5cb98":"markdown","4ea6837c":"markdown","c6b17977":"markdown","e5f77146":"markdown","a626b01f":"markdown","2cb49dd0":"markdown","dad134a0":"markdown","948476f8":"markdown","ddb30198":"markdown","c6d85df4":"markdown","c93607c9":"markdown","548355d9":"markdown","72a53d72":"markdown","f7704196":"markdown","b520c0b4":"markdown"},"source":{"44e8f952":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e91e33d3":"data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\nprint(data.shape)\ndata.head()","d45b0b70":"data_explore = data.copy()","19938223":"data_explore.info()","ec1cf34b":"data_explore = data_explore.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\ndata_explore.shape","27d3f2a2":"max_embarked = data_explore['Embarked'].value_counts().idxmax()\ndata_explore['Embarked'] = data_explore['Embarked'].fillna(max_embarked)","dc71db9c":"data_explore['Family'] = data_explore['SibSp'] + data_explore['Parch']\ndata_explore['Family'].loc[data_explore['Family'] > 0] = \"Yes\"\ndata_explore['Family'].loc[data_explore['Family'] == 0] = \"No\"","e83b96dc":"data_explore.describe()","04c851d5":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\ndata_explore[\"sex_enc\"] = label_encoder.fit_transform(data_explore[\"Sex\"])\nprint(label_encoder.classes_)\ndata_explore[\"embarked_enc\"] = label_encoder.fit_transform(data_explore['Embarked'])\nprint(label_encoder.classes_)\ndata_explore[\"Family_enc\"] = label_encoder.fit_transform(data_explore[\"Family\"])\nprint(label_encoder.classes_)","93116ee7":"def fill_with_mean(df, num_cols):\n    \"\"\"\n    Instead of replacing null values by mean, this function will replace those null values by \n    any random value within range of (mean + std) & (mean - std).\n    \"\"\"\n    for col in num_cols:\n        total_null = df[col].isna().sum()\n        if total_null>0:\n            mean = df[col].mean()\n            std = df[col].std()\n            lower = mean - std\n            upper = mean + std\n            random_values = np.random.randint(lower, upper, total_null)\n            df[col][np.isnan(df[col])] = random_values.copy()\n    return df","ae3420d4":"data_explore = fill_with_mean(data_explore, ['Age'])","36b40520":"columns = data_explore.columns\nplt.figure(figsize=(14, 14))\nplt.style.use('seaborn')\ni=1\nfor col in columns:\n    plt.subplot(4, 3, i)\n    i+=1\n    ax = plt.gca()\n    counts, _, patches = ax.hist(data_explore[col])\n    for count, patch in zip(counts, patches):\n        if count>0:\n            ax.annotate(str(int(count)), xy=(patch.get_x(), patch.get_height()+5))\n    plt.title(col)","a77b0fc0":"data_explore_survived = data_explore[data_explore['Survived']==1]\ndata_explore_not_survived = data_explore[data_explore['Survived']==0]","7a956400":"plt.figure(figsize=(10, 4))\nplt.subplot(1, 3, 1)\ndata_explore[\"Age\"].hist(alpha=0.7, rwidth=0.85)\nplt.ylabel(\"Total Peoples\")\nplt.title(\"Overall\")\nplt.xticks(ticks=list(range(0,95,15)))\nplt.ylim(0,250)\nplt.subplot(1, 3, 2)\ndata_explore_not_survived[\"Age\"].hist(alpha=0.7, rwidth=0.85)\nplt.title(\"Non-Survivers\")\nplt.xlabel(\"Age Ranges\")\nplt.xticks(ticks=list(range(0,95,15)))\nplt.ylim(0,250)\nplt.subplot(1, 3, 3)\ndata_explore_survived[\"Age\"].hist(alpha=0.7, rwidth=0.85)\nplt.title(\"Survivers\")\nplt.xticks(ticks=list(range(0,95,15)))\nplt.ylim(0,250)\nplt.show()","a942daf6":"def get_person_type(passanger):\n    age, sex = passanger\n    return 'child' if age < 16 else sex\n\ndata_explore[\"Person\"] = data_explore[[\"Age\", \"Sex\"]].apply(get_person_type, axis=1)\ndata_explore_survived = data_explore[data_explore['Survived']==1]\ndata_explore_not_survived = data_explore[data_explore['Survived']==0]\ndata_explore[\"Person_enc\"] = label_encoder.fit_transform(data_explore[\"Person\"])\nprint(label_encoder.classes_)","75cede85":"overall_age = dict(data_explore[\"Person\"].value_counts())\noverall_age = sorted(overall_age.items()) \n# reason for sorting is to make sure labels in overall as well as in survived dict remains in same order.\noverall_age_values = [item[1] for item in overall_age]\noverall_age_label = [item[0] for item in overall_age]\nsurvived_age = dict(data_explore_survived[\"Person\"].value_counts())\nsurvived_age = sorted(survived_age.items())\nsurvived_age_values = [item[1] for item in survived_age]","f7028da8":"x_indexes = np.arange(len(overall_age_label))\nwidth=0.25\n\nplt.bar(x_indexes, overall_age_values, color=\"blue\", label=\"Overall\", width=width)\nfor i in range(3):\n    plt.text(x=x_indexes[i]-0.08, y=overall_age_values[i]+1, s=overall_age_values[i])\n\nplt.bar(x_indexes+width, survived_age_values, color=\"green\", label=\"Survivers\", width=width)\nfor i in range(3):\n    plt.text(x=x_indexes[i]+0.15, y=survived_age_values[i]+1, s=survived_age_values[i])\n\nplt.xticks(ticks=x_indexes, labels=overall_age_label)\nplt.title(\"Survived People \")\nplt.ylabel(\"Total Peoples\")\nplt.legend()\nplt.show()","3848a79d":"overall_embarked = dict(data_explore[\"Embarked\"].value_counts())\noverall_embarked = sorted(overall_embarked.items())\noverall_embarked_values = [item[1] for item in overall_embarked]\noverall_embarked_labels = [item[0] for item in overall_embarked]\nsurvived_embarked = dict(data_explore_survived[\"Embarked\"].value_counts())\nsurvived_embarked = sorted(survived_embarked.items())\nsurvived_embarked_values = [item[1] for item in survived_embarked]","203f6972":"x_indexes = np.arange(len(overall_embarked_labels))\n\nplt.bar(x_indexes, overall_embarked_values, color=\"skyblue\", label=\"Overall\", width=width)\nfor i in range(len(x_indexes)):\n    plt.text(x=x_indexes[i]-0.05, y=overall_embarked_values[i], s=overall_embarked_values[i])\n\nplt.bar(x_indexes+width, survived_embarked_values, color=\"green\", label=\"Survived\", width=width)\nfor i in range(len(x_indexes)):\n    plt.text(x=x_indexes[i]+width-0.05, y=survived_embarked_values[i], s=survived_embarked_values[i])\n\nplt.xticks(ticks=x_indexes, labels=overall_embarked_labels)\nplt.title(\"Embarked\")\nplt.ylabel(\"Total Peoples\")\nplt.legend()\nplt.show()","663ee20c":"overall_pclass = dict(data_explore[\"Pclass\"].value_counts())\noverall_pclass = sorted(overall_pclass.items())\noverall_pclass_labels = [\"Upper Class\", \"Middle Class\", \"Lower Class\"]  #1 ,2, 3\noverall_pclass_values = [item[1] for item in overall_pclass]\nsurvived_pclass = dict(data_explore_survived[\"Pclass\"].value_counts())\nsurvived_pclass = sorted(survived_pclass.items())\nsurvived_pclass_values = [item[1] for item in survived_pclass]","a7916306":"x_indexes = np.arange(len(overall_pclass_labels))\n\nplt.bar(x_indexes, overall_pclass_values, color=\"skyblue\", width=width, label=\"Overall\")\nfor i in range(len(x_indexes)):\n    plt.text(x=x_indexes[i]-0.05, y=overall_pclass_values[i], s=overall_pclass_values[i])\n\nplt.bar(x_indexes+width, survived_pclass_values, color=\"green\", width=width, label=\"Survived\")\nfor i in range(len(x_indexes)):\n    plt.text(x=x_indexes[i]+width-0.05, y=survived_pclass_values[i], s=survived_pclass_values[i])\n\nplt.xticks(ticks=x_indexes, labels=overall_pclass_labels)\nplt.title(\"Ticket Class\")\nplt.ylabel(\"Total Peoples\")\nplt.legend()\nplt.show()","279a3bdd":"n_survived, bins, patches = plt.hist(data_explore_survived[\"Fare\"], bins=list(range(0,51,5)))\n\nplt.hist(data_explore[\"Fare\"], bins=list(range(0,51,5)), label=\"Overall\", edgecolor=\"black\", color=\"skyblue\")\nplt.plot(bins[:-1]+2.5, n_survived, color=\"green\", label=\"Survived\",marker=\"o\")\nplt.title(\"Fares\")\nplt.ylabel(\"Total Peoples\")\nplt.legend()\nplt.show()","cf286dce":"overall_family = dict(data_explore[\"Family\"].value_counts())\noverall_family = sorted(overall_family.items())\noverall_family_labels = [\"No\", \"Yes\"]\noverall_family_values = [item[1] for item in overall_family]\nsurvived_family = dict(data_explore_survived[\"Family\"].value_counts())\nsurvived_family = sorted(survived_family.items())\nsurvived_family_values = [item[1] for item in survived_family]","be154805":"x_indexes = np.arange(len(overall_family_labels))\n\nplt.bar(x_indexes, overall_family_values, color=\"skyblue\", width=width, label=\"Overall\")\nfor i in range(len(x_indexes)):\n    plt.text(x=x_indexes[i]-0.05, y=overall_family_values[i], s=overall_family_values[i])\n\nplt.bar(x_indexes+width, survived_family_values, color=\"green\", width=width, label=\"Survived\")\nfor i in range(len(x_indexes)):\n    plt.text(x=x_indexes[i]+width-0.05, y=survived_family_values[i], s=survived_family_values[i])\n\nplt.xticks(ticks=x_indexes, labels=overall_family_labels)\nplt.title(\"Family\")\nplt.ylabel(\"Total Peoples\")\nplt.legend()\nplt.show()","c4cc2127":"corr_matrix = data_explore.corr()\n\nplt.figure(figsize=(12, 6))\nsns.heatmap(corr_matrix, mask=np.zeros_like(corr_matrix, dtype=np.bool), annot=True, square=True)\nplt.show()","6fd6d9c5":"corr_matrix['Survived'].sort_values(ascending=False)","9adaad20":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin","c170a001":"X = data.drop(columns=['Survived'], axis=1)\ny = data['Survived'].copy()\nX.shape, X.columns","62c557d2":"from sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_index, test_index in split.split(X, y):\n    strat_train_set = data.iloc[train_index]\n    strat_test_set = data.iloc[test_index]\n\nX_train = strat_train_set.drop('Survived', axis=1)\ny_train = strat_train_set['Survived'].copy()\nX_test = strat_test_set.drop('Survived', axis=1)\ny_test = strat_test_set['Survived'].copy()\nX_train.shape, X_test.shape","eb508d42":"def fill_with_mean(df, num_cols):\n    for col in num_cols:\n        total_null = df[col].isna().sum()\n        if total_null>0:\n            mean = df[col].mean()\n            std = df[col].std()\n            lower = mean - std\n            upper = mean + std\n            random_values = np.random.randint(lower, upper, total_null)\n            df[col][np.isnan(df[col])] = random_values.copy()\n    return df","6bea56ad":"class AddCustomAttribute(BaseEstimator, TransformerMixin):\n    \"\"\"\n    New attributes:\n        1. Person: whether passenger is child, female or male.\n        2. Family: does person come with family member or not.\n        \n        These are categorical attributes hence their encoding is handled in this custom transformer.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        - cat_imp is imputer for categorical attributes.\n        - cat_encoder is for encoding newly created attributes.\n        - scaler is for scaling Age attribute.\n        - All fields are initialized with None to make sure fit_transform() method will get call only on fit() \n        method of pipeline. Only transform() method will get call on predict().\n        \"\"\"\n        self.cat_imp=None\n        self.cat_encoder=None\n        self.scaler=None\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        try:\n            # Fill null values\n            X = fill_with_mean(X, ['Age', 'SibSp', 'Parch'])\n            if self.cat_imp==None:\n                self.cat_imp = SimpleImputer(strategy=\"most_frequent\")\n                X['Sex'] = self.cat_imp.fit_transform(X[['Sex']])\n            else:\n                X['Sex'] = self.cat_imp.transform(X[['Sex']])\n            \n            # Create new attributes\n            X['Person'] = X[['Age', 'Sex']].apply(self.get_person_type, axis=1)\n            X['Family'] = X['SibSp'] + X['Parch']\n            X['Family'].loc[X['Family'] > 0] = 1\n            X['Family'].loc[X['Family'] == 0] = 0\n            \n            # Scale numerical attribute\n            if self.scaler==None:\n                self.scaler = StandardScaler()\n                X['Age'] = self.scaler.fit_transform(X[['Age']])\n            else:\n                X['Age'] = self.scaler.transform(X[['Age']])\n            \n            # Encode new categorical attributes\n            if self.cat_encoder==None:\n                self.cat_encoder = OneHotEncoder(handle_unknown='ignore')\n                X_new = self.cat_encoder.fit_transform(X[['Person', 'Family']])\n            else:\n                X_new = self.cat_encoder.transform(X[['Person', 'Family']])\n            \n            X_new = pd.DataFrame(X_new.toarray())\n            X_new.index = X.index\n            X = pd.concat([X, X_new], axis=1)\n            X = X.drop(columns=['Sex', 'Person', 'SibSp', 'Parch', 'Family'], axis=1)\n        except:\n            print(\"Error generated in AddCustomAttribute!!!\")\n        return X\n    \n    @staticmethod\n    def get_person_type(passanger):\n        sex_enc = dict({'child':0, 'female':1, 'male':2})\n        age, sex = passanger\n        return 'child' if age < 16 else sex","71a64e5d":"class FillWithMeanSD(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        try:\n            num_cols = X.columns\n            for col in num_cols:\n                total_null_fare = X[col].isna().sum()\n                if total_null_fare>0:\n                    mean = X[col].mean()\n                    std = X[col].std()\n                    lower = mean - std\n                    upper = mean + std\n                    random_values = np.random.randint(lower, upper, total_null_fare)\n                    X[col][np.isnan(X[col])] = random_values.copy()\n        except:\n            print(\"Error generated in FillWithSD!!!\")\n        return X","aa0eac57":"cat_pipeline = Pipeline([('cat_imputer', SimpleImputer(strategy=\"most_frequent\")),\n                        ('cat_encoder', OneHotEncoder())])\n\nnum_pipeline = Pipeline([('num_imputer', FillWithMeanSD()),\n                        ('scaler', StandardScaler())])\n\n\npre_process = ColumnTransformer([('drop_attrs', 'drop', ['PassengerId', 'Name', 'Ticket', 'Cabin']),\n                                 ('num_pipeline', num_pipeline, ['Pclass', 'Fare']),\n                                 ('cat_pipeline', cat_pipeline, ['Embarked']),\n                                 ('custom_attr', AddCustomAttribute(), ['Age', 'Sex', 'SibSp', 'Parch'])], \n                                remainder='passthrough')","d9d635ae":"X_train_transformed = pre_process.fit_transform(X_train)\nX_test_transformed = pre_process.transform(X_test)\nX_train_transformed.shape, X_test_transformed.shape","03925e27":"X_train.iloc[4,:], X_train_transformed[4]","557f4576":"feature_columns = ['Pclass', 'Fare', 'C', 'Q', 'S', 'Age', 'person_child', 'person_female', 'person_male', \n                   'family_no', 'family_yes']","b0cefa09":"from sklearn.model_selection import GridSearchCV, StratifiedKFold\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)","93b52bde":"from sklearn.linear_model import LogisticRegression\n\nlgr_grid_parm=[{'solver':['liblinear', 'lbfgs'], 'C':list(np.linspace(0.01, 1, 15)), 'penalty':['l1', 'l2'], 'class_weight':[None, 'balanced']}]\nlgr_grid_search = GridSearchCV(LogisticRegression(random_state=42, n_jobs=-1), \n                               lgr_grid_parm, cv=kf, scoring=\"accuracy\", return_train_score=True, n_jobs=-1)\nlgr_grid_search.fit(X_train_transformed, y_train)","cde9da2a":"lgr_grid_search.best_params_, lgr_grid_search.best_score_","19e132b1":"train_models = []\ntrain_models.append(['Logistic Regression', lgr_grid_search.best_params_, lgr_grid_search.best_score_])","78e29977":"best_lgr_clf = lgr_grid_search.best_estimator_\nbest_lgr_clf","378ed05f":"feature_imp = [ col for col in zip(feature_columns, best_lgr_clf.coef_[0])]\nfeature_imp.sort(key=lambda x:x[1], reverse=True)\nfeature_imp","7636d24f":"from sklearn.svm import LinearSVC\n\nsvc_grid_parm=[{'C':list(np.linspace(0.01, 1, 20)), 'penalty':['l1', 'l2'], 'class_weight':[None, 'balanced']}]\nsvc_grid_search = GridSearchCV(LinearSVC(loss=\"hinge\", random_state=42), svc_grid_parm, cv=kf, scoring=\"accuracy\", \n                               return_train_score=True, n_jobs=-1)\nsvc_grid_search.fit(X_train_transformed, y_train)","b1f291d5":"svc_grid_search.best_params_, svc_grid_search.best_score_","d4089a73":"train_models.append(['Linear SVC', svc_grid_search.best_params_, svc_grid_search.best_score_])","8d86ab87":"best_svc_clf = svc_grid_search.best_estimator_\nbest_svc_clf","887538cc":"feature_imp = [ col for col in zip(feature_columns, best_svc_clf.coef_[0])]\nfeature_imp.sort(key=lambda x:x[1], reverse=True)\nfeature_imp","c3ef0541":"from sklearn.ensemble import RandomForestClassifier\n\nrf_grid_parm=[{'n_estimators':[50, 100, 200, 300], 'max_depth':[6, 8, 16], \n               'class_weight':['balanced', 'balanced_subsample', None]}]\nrf_grid_search = GridSearchCV(RandomForestClassifier(random_state=42, n_jobs=-1), rf_grid_parm, cv=kf, \n                              scoring=\"accuracy\", return_train_score=True, n_jobs=-1)\nrf_grid_search.fit(X_train_transformed, y_train)","48f3fa1a":"rf_grid_search.best_params_, rf_grid_search.best_score_","4c6929d2":"train_models.append(['Random Forest', rf_grid_search.best_params_, rf_grid_search.best_score_])","3868c63b":"best_rf_clf = rf_grid_search.best_estimator_\nbest_rf_clf","2b8d9dc4":"feature_imp = [ col for col in zip(feature_columns, best_rf_clf.feature_importances_)]\nfeature_imp.sort(key=lambda x:x[1], reverse=True)\nfeature_imp","69f45cb0":"from xgboost import XGBClassifier\n\nxgb_grid_parm=[{'n_estimators':[50, 100, 200], 'max_depth':[4, 8, 16, 24], 'subsample':[0.5, 0.75, 1.0], \n                'colsample_bytree':[0.5, 0.75, 1.0], 'gamma':[0, 0.25, 0.5, 0.75, 1.0]}]\nxgb_grid_search = GridSearchCV(XGBClassifier(objective='binary:logistic', learning_rate=0.1, random_state=42, \n                                             n_jobs=-1), xgb_grid_parm, cv=kf, scoring=\"accuracy\", \n                               return_train_score=True, n_jobs=-1)\nxgb_grid_search.fit(X_train_transformed, y_train)","fc112942":"xgb_grid_search.best_params_, xgb_grid_search.best_score_","668886ad":"train_models.append(['XGBoost', xgb_grid_search.best_params_, xgb_grid_search.best_score_])","f111547b":"best_xgb_clf = xgb_grid_search.best_estimator_\nbest_xgb_clf","80f59af9":"feature_imp = [ col for col in zip(feature_columns, best_xgb_clf.feature_importances_)]\nfeature_imp.sort(key=lambda x:x[1], reverse=True)\nfeature_imp","17b0646f":"from sklearn.ensemble import StackingClassifier\n\nnamed_estimators = [('logistic', best_lgr_clf), ('linear_svc', best_svc_clf), \n                    ('forest', best_rf_clf), ('xgb', best_xgb_clf)]","eede5329":"stack_clf = StackingClassifier(estimators=named_estimators, cv=kf, passthrough=False, n_jobs=-1)\nstack_clf.fit(X_train_transformed, y_train)","51e63413":"from sklearn.model_selection import cross_val_score\n\nstack_acc = cross_val_score(stack_clf, X_train_transformed, y_train, scoring=\"accuracy\", cv=kf, n_jobs=-1)\nstack_acc = np.round(np.median(stack_acc), 4)\ntrain_models.append(['Stacking Classifier', '', stack_acc])","e7cb7672":"from sklearn.ensemble import VotingClassifier\n\nvoting_clf = VotingClassifier(estimators=named_estimators, n_jobs=-1)\nvoting_clf.fit(X_train_transformed, y_train)","3478a62d":"voting_acc = cross_val_score(voting_clf, X_train_transformed, y_train, scoring=\"accuracy\", cv=kf, n_jobs=-1)\nvoting_acc = np.round(np.mean(voting_acc), 4)\ntrain_models.append(['Voting Classifier', '', voting_acc])","898ec57f":"pd.set_option('display.max_colwidth', -1)\n\ntrain_models_df = pd.DataFrame(train_models, columns=['Model', 'Best Paramas', 'Accuracy'])\ntrain_models_df","9ef9d140":"def plot_results(model_names, model_accuracy):\n        \n    plt.figure(figsize=(12, 5))\n    x_indexes = np.arange(len(model_names))     \n    width = 0.15                            \n    \n    plt.barh(x_indexes, model_accuracy)\n    for i in range(len(x_indexes)):\n        plt.text(x=model_accuracy[i], y=x_indexes[i], s=str(model_accuracy[i]), fontsize=12)\n    \n    plt.xlabel(\"Accuracy Score\", fontsize=14)\n    plt.yticks(ticks=x_indexes, labels=model_names, fontsize=14)\n    plt.title(\"Results on Test Dataset\")\n    plt.show()","31142757":"results = dict()\nbest_models = [best_lgr_clf, best_svc_clf, best_rf_clf, best_xgb_clf, stack_clf, voting_clf]\nmodel_names = []\nmodel_accuracy = []\n\nfor model in best_models:\n    test_accuracy_scores = cross_val_score(model, X_test_transformed, y_test, scoring=\"accuracy\", cv=kf, n_jobs=-1)\n    test_accuracy_scores = np.round(test_accuracy_scores,4)\n    test_accuracy = np.round(np.mean(test_accuracy_scores),4)\n    model_names.append(model.__class__.__name__)\n    model_accuracy.append(test_accuracy)","1b800dee":"plot_results(model_names, model_accuracy)","aef9f6ac":"best_model = best_models[np.argmax(model_accuracy)]\nbest_model","1174df51":"final_model = Pipeline([('pre_process', pre_process),\n                        ('best_model', best_model)])\nfinal_model.fit(X_train, y_train)","4b9dd3a5":"test_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest_data.info()","78cdf038":"predictions = final_model.predict(test_data)","e9307225":"test_predictions = pd.DataFrame(test_data['PassengerId'])\ntest_predictions['Survived'] = predictions.copy()\ntest_predictions.head()","23362a9c":"test_predictions.shape","07584e7e":"test_predictions.to_csv(\".\/submission.csv\", index=False)","9be2269b":"Observations:\n- Most of the peoples on Titanic are in between 15-45 years old.\n- Many childrens, with age less than 15, are survived.\n- There is also a elder peoples with age above 75 are survived.\n- Many peoples with age in range of 15-45, are not survived.","1438cb24":"## Step 4: Modelling","0b84d7cf":"# Titanic: Machine Learning from Disaster","ce8400a4":"### Analysis 5: Is having higher class ticket benifited?","859d0cc8":"- From analysis, we found out that there are high chances of person getting survived if:\n    1. either female or children\n    2. either has upper or middle class ticket\n    3. embarked from Cherbourg\n    4. has family member with him\/her","d4b1d24c":"- Objective: From give passenger information like name, age, gender, ticket class, ticket price etc. we have to predict which passengers will survive the Titanic shipwreck\n\n\t\n- Following are the attributes related to each passenger:\n    <br>PassengerId\t: unique id of passenger\n    <br>Survived \t: Survived (1) or not(0)\n    <br>Pclass\t\t: Ticket class, 1 = 1st(Upper), 2 = 2nd(Middle), 3 = 3rd(Lower) \n    <br>Name\t\t: Name of passenger\n    <br>Sex\t\t\t: male or female\n    <br>Age\t\t\t: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n    <br>SibSp \t\t: # of siblings \/ spouses aboard the Titanic\n    <br>Parch\t\t: # of parents \/ children aboard the Titanic, Some children travelled only with a nanny, therefore parch=0 for them.\n    <br>Ticket\t\t: Ticket number\n    <br>Fare\t\t: Passenger fare\n    <br>Cabin\t\t: Cabin number\n    <br>Embarked \t: Port of Embarkation, C = Cherbourg, Q = Queenstown, S = Southampton","3294e07e":"### Analysis 4: Does place of embarkment matter for getting survived?","2e550089":"### Analysis 3: 'Women and Children First'?","3dbda4e0":"- Lets list down all the data operations we performed in the Exploration step.\n    1. Dropped following columns: PassengerId, Name, Ticket, Cabin.\n    2. For numerical columns, filled null values with random number between range (mean+SD, mean-SD).\n    3. For categorical columns, replaced null values by most frequent category.\n    4. Added new attributes Person and Family.","a3fe80a8":"## Step 5: Model Evaluation","9029e2b2":"- Among all those who were alone on the Titanic, very few of them have survived.\n- Almost 50% of peoples who had family members on Titanic are survived.","3da1735d":"## Step 2: Data Exploration","fb8c9be4":"## Step 3: Data Preprocessing","94e5cb98":"- There are many people with fares less than 50.\n- So observation done only for people with fare less than 50.\n- High % of peoples with fare values in between 15-40 are survived.\n- High % of peoples with fare values less than 15 are not survived.","4ea6837c":"- Following columns contain null value: Age, Cabin, Embarked\n- For analysis and predictions, PassengerId, Name and Ticket columns are not going to usefull, hence we will drop these columns.\n- Columns 'Cabin' has more than 75% null values. So this will have very little to zero impact on predictions.","c6b17977":"Observations:\n- High % of female survivers.\n- Almost 50% childrens are survived.\n- Very less number of males are survived.","e5f77146":"### Analysis 1: Histograms","a626b01f":"### Analysis 7: Correlation Plot","2cb49dd0":"+ Approach:\n    1. Obtain best models for following algorithms using grid search:\n        - Logistic Regression\n        - Linear SVC\n        - Random Forest Classification\n        - XGBoost Classification\n        - Stacking Classification   \n    2. Select the final model by evaluating each model on test data.","dad134a0":"Now lets add a new column which tell whether person is male, female or child.","948476f8":"## Step 6: Make Predictions on Test Dataset","ddb30198":"- Only one-third of all people embarked from S or Q are survived.\n- In case of C, More than half of peoples are survived.","c6d85df4":"In general, columns Parch & SibSp gives information about family. If both column values are zero means person is alone on the ship.\nSo using these two columns I will create another column 'Family' with values 'Yes' if it has either of Parch or SibSp or both, else 'No'.","c93607c9":"## Step 1: Frame the Problem","548355d9":"### Analysis 6: Is there advantage of having family member alongwith you? ","72a53d72":"- Very less number of peoples having lower class tickets are survived.\n- two-third of people having upper class tickets are survived.\n- Around 50% of people having middle class tickets are survived.","f7704196":"Observations\n- Imbalance dataset. There are many non-survivers than survivers.\n- Most number of peoples embarked from Southampton(aroung 72%)\n- Almost 60% peoples are alone on ship. Aroung 25% peoples are with one family member.\n- Almost 80% passenger fare is less than 50.\n- Around 55% passengers with lower class ticket.\n- Almost 30% more male passengers than female passengers.","b520c0b4":"### Analysis 2: Age distribution of Survivers & Non-survivers"}}