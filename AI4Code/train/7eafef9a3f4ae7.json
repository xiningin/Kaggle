{"cell_type":{"e3d26e59":"code","2b7875f5":"code","d463be86":"code","f99d4f07":"code","8f21dda2":"code","9c0dbb78":"code","a7558f8b":"code","3c8135e6":"code","a9d0eddf":"code","fc005396":"code","31314d90":"code","74d9530e":"code","d1c48289":"code","d29f839f":"code","57e038d9":"code","3b42872b":"code","c154b2f8":"code","4fcdcc4c":"code","e8cffeac":"code","d904c903":"code","795bae33":"code","79a457cf":"code","b0194848":"code","d4274d8c":"code","0bdeadf5":"code","bb3633f8":"code","92b7268e":"markdown","a78cf65f":"markdown","8fe31f65":"markdown","c011257f":"markdown","b2702c5d":"markdown","6726e7ce":"markdown","86b3b41f":"markdown"},"source":{"e3d26e59":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2b7875f5":"import warnings\nwarnings.filterwarnings(\"ignore\") #Matching warnings does not display again","d463be86":"import os #for interacting with operating system importing os\nimport gc #importing garbage collection module\nimport cv2 #importing OpenCV cv2 module\nimport glob #importing glob for finding all pathnames matching a specified pattern\nimport h5py #helps store huge amount of numerical data\nimport shutil #enables us to operate with file object easily\nimport itertools #provides various functions to work on iterators to produce complex iterators\nimport random as rn #random number generation\n\nimport imgaug as aug #for image augmentation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns #for stastical graphics in Python\nfrom pathlib import Path #offers classes representing filesystem paths with semantics appropriate for different operating systems\nfrom collections import Counter #container to stores elements as dictionary keys and count as dictionary values\nimport matplotlib.pyplot as plt #collection of work to make matplotlib work like MATLAB\nimport imgaug.augmenters as iaa #import augmenters for aritmetic changes, image color changes, etc.\n\nfrom skimage.io import imread #image reading and writing\nfrom skimage.transform import resize #for image resiszing to a certain size\nfrom sklearn.metrics import confusion_matrix #used to describe the performance of classifier model\nfrom mlxtend.plotting import plot_confusion_matrix #for visualizing confusion matrices via matplotlib\nfrom sklearn.model_selection import train_test_split #for splitting matrices into random train and test subsets\n\nfrom keras.models import Sequential, Model, load_model #sequential for grouping linear stack of layers in Model,and load_model for loading model \nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Dropout #imorting 2D convolution layer, Max pooling operation for 2D data,\n#regular densely neural networks,applies dropout to the input\nfrom keras.layers import Input, Flatten, BatchNormalization, Lambda #flatten the input, normalization of scale inputs, wraps arbitrary expression as layer object\nfrom keras.layers import GRU, LSTM, Bidirectional #Gated recurrent unit, Long Short term memory layer, Bidirectional wrapper for recurrent neural network\nfrom keras.layers import Add, Concatenate, Reshape # layer add, concatenate a list of inputs and reshape input into a given shape\nfrom keras.optimizers import Adam, SGD, RMSprop #compile using Adam algorithm, Gradient descent with momentum  optimizer, RMSprop algoritm optimizer \nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping #to save keras model at some frequency, abstract base class used to build new callbacks, stop training when a monitored metric has stopped working\nfrom keras.utils import to_categorical #for conversion of class vector to binary class matrix\n\nfrom keras import backend as K #import keras backend API for low level operations\nimport tensorflow as tf \n\n\ncolor = sns.color_palette()\n#sets backend of matlplotlib to the inline backend\n%matplotlib inline \n%config InlineBackend.figure_format=\"svg\" #enables scalable graphics inline","f99d4f07":"\n# Set the seed for hash based operations in python\nos.environ['PYTHONHASHSEED'] = '0'\n\nseed=1234\n\n# set the seed for random number generator\nrn.seed(seed)\n\n# Set the numpy seed\nnp.random.seed(seed)\n\n# Set the random seed in tensorflow at graph level\ntf.random.set_seed(seed)\n\n#Using CuDNN implementation of RNNs which already is non-reproducible\nsession_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n                              inter_op_parallelism_threads=1)\nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\ntf.compat.v1.keras.backend.get_session()\n\n# Make the augmentation sequence deterministic\naug.seed(seed)\nprint(\"Seeding Successful\")","8f21dda2":"data_dir = Path(\"..\/input\/captcha6letterspng\/samples\/\")\n\n# getting list of all images\nimages = list(data_dir.glob(\"*.png\"))\nprint(\"Number of images found: \", len(images))","9c0dbb78":"sample_images = images[:4]\n\nf,ax = plt.subplots(2,2, figsize=(5,3))\nfor i in range(4):\n    img = imread(sample_images[i])\n    print(\"Shape of image: \", img.shape)\n    ax[i\/\/2, i%2].imshow(img)\n    ax[i\/\/2, i%2].axis('off')\nplt.show()","a7558f8b":"# make a set of all unique characters. .\nletters = set()\n\n# A list to store the max length for each catcha\nlengths = []\n\n# Iterate over each image. The name of the image is the \n# text contained in it. \nfor image in images:\n    image_name = str(image.name).split(\".\")[0]\n    lengths.append(len(image_name))\n    for ch in image_name:\n        letters.add(ch)\n\n# Sort the letters        \nletters = sorted(letters)\nprint(\"Number of unqiue letters in the whole dataset: \", len(letters))\nprint(\"Maximum length of any captcha: \", max(Counter(lengths).keys()))\nprint(\"\\nAll letters to be considered: \")\nprint(letters)","3c8135e6":"dataset = []\n\nfor image in images:\n    image_path = str(image)\n    label = str(image.name).split(\".\")[0]\n    dataset.append((image_path, label))\n\ndataset = pd.DataFrame(dataset, columns=[\"img_path\", \"label\"], index=None)\ndataset = dataset.sample(frac=1.).reset_index(drop=True)\nprint(\"Total number of samples in the dataset: \", len(dataset))\ndataset.head(10)","a9d0eddf":"# split into train and validation sets\ntraining_data, validation_data = train_test_split(dataset, test_size=0.1, random_state=seed)\n\ntraining_data = training_data.reset_index(drop=True)\nvalidation_data = validation_data.reset_index(drop=True)\n\nprint(\"Number of training samples: \", len(training_data))\nprint(\"Number of validation samples: \", len(validation_data))","fc005396":"# function to create labels from text\ndef text_to_labels(text):\n    return list(map(lambda x: letters.index(x), text))\n\n# function to convert labels back to texts\ndef labels_to_text(label):\n    return ''.join(list(map(lambda x: letters[int(x)], label)))\n\n# sanity-check for letters\ndef is_valid_str(s):\n    for ch in s:\n        if not ch in letters:\n            return False\n    return True","31314d90":"def build_data(df, resize=True, img_height=50, img_width=200):\n    \"\"\"This function reads samples from a dataframe and store\n    the image values and labels in two separate arrays.\n    \n    Args:\n        df        : dataframe from which we want to read the data\n        resize    : whether to resize images or not\n        img_width : width of images to be considered\n        img_height: height of images to be considered\n        \n    Returns:\n        images    : numpy array of images\n        labels    : numpy array of encoded labels\n    \"\"\"\n    n = len(df)\n    images = np.zeros((n, img_height, img_width), dtype=np.float32)\n    labels = [0]*n\n    for i in range(n):\n        img = cv2.imread(df[\"img_path\"][i])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        \n        if resize:\n            img = cv2.resize(img, (img_width, img_height))\n        \n        img = (img\/255.).astype(np.float32)\n        label = df[\"label\"][i]\n        \n        # only add to if all the charaters are valid\n        if is_valid_str(label):\n            images[i, :, :] = img\n            labels[i] = label\n    \n    return images, np.array(labels)","74d9530e":"training_images, training_labels = build_data(training_data)\nprint(\"Number of training images: \", training_images.shape)\nprint(\"Number of training labels: \", training_labels.shape)","d1c48289":"validation_images, validation_labels = build_data(validation_data)\nprint(\"Number of validation images: \", validation_images.shape)\nprint(\"Number of validation labels: \", validation_labels.shape)","d29f839f":"f,ax = plt.subplots(4,2, figsize=(8,5))\nfor i in range(4):\n    ax[i\/\/2, i%2].imshow(training_images[i], cmap='gray')\n    ax[i\/\/2, i%2].set_title(training_labels[i])\n    ax[i\/\/2, i%2].axis('off')\n\nfor i in range(4, 8):\n    ax[i\/\/2, i%2].imshow(validation_images[i], cmap='gray')\n    ax[i\/\/2, i%2].set_title(validation_labels[i])\n    ax[i\/\/2, i%2].axis('off')\n    \nplt.show()","57e038d9":"def data_generator(df, \n                   batch_size, \n                   img_width, \n                   img_height, \n                   downsample_factor, \n                   max_text_len, \n                   is_validation_data=False):\n    \"\"\"This is a data generator which yields batches \n    of (image, label) pairs.\n    \n    Args:\n        df                : training or validation dataframe\n        batch_size        : batch size to be used during training\n        img_width         : width of images to be considered  \n        img_height        : height of images to be considered\n        downsample_factor : by what factor the CNN has downsampled the images\n        max_text_len      : maximum length of the text in your data\n        is_validation_data: is the data being considered a validation data?\n        \n    Returns:\n        inputs: numpy array containg inputs that are required for the final model\n        outputs: a dummy array of zeros \n    \"\"\"\n    n = len(df)\n    indices = np.arange(n)\n    np.random.shuffle(indices)\n    nb_batches = int(np.ceil(n\/batch_size))\n    \n    if not is_validation_data:\n        images, texts = training_images, training_labels\n    else:\n        images, texts = validation_images, validation_labels\n    \n    batch_images = np.ones((batch_size, img_width, img_height, 1), dtype=np.float32)\n    batch_labels = np.ones((batch_size, max_text_len), dtype=np.float32)\n    input_length = np.ones((batch_size, 1), dtype=np.int64) * \\\n                                            (img_width \/\/ downsample_factor - 2)\n    label_length = np.zeros((batch_size, 1), dtype=np.int64)\n    \n    while True:\n        for i in range(nb_batches):\n            idx_to_consider = indices[i*batch_size:(i+1)*batch_size]\n            \n            for j, idx in enumerate(idx_to_consider):\n                img = images[idx].T\n                img = np.expand_dims(img, axis=-1)\n                text = texts[idx]\n                \n                if is_valid_str(text):\n                    label = text_to_labels(texts[idx])\n                    batch_images[j] = img\n                    batch_labels[j] = label\n                    label_length[j] = len(text)\n\n            inputs = {\n            'input_data': batch_images,\n            'input_label': batch_labels,\n            'input_length': input_length,\n            'label_length': label_length,\n            }\n            \n            outputs = {'ctc_loss': np.zeros([batch_size], dtype=np.float32)}\n            yield inputs, outputs","3b42872b":"# batch size to be used for training\nbatch_size = 32\n\n# image dimensions\nimg_width=200\nimg_height=50 \n\n# by what factor the image has been downsampled by the CNN part?\ndownsample_factor=5\n\n# maximum length of any text in the data\nmax_text_len=5","c154b2f8":"# Get a generator object for the training data\ntrain_data_generator = data_generator(training_data, \n                                      batch_size=batch_size, \n                                      img_width=img_width, \n                                      img_height=img_height, \n                                      downsample_factor=downsample_factor, \n                                      max_text_len=max_text_len, \n                                      is_validation_data=False)\n\n# Get a generator object for the validation data \nvalid_data_generator = data_generator(validation_data, \n                                      batch_size=batch_size, \n                                      img_width=img_width, \n                                      img_height=img_height, \n                                      downsample_factor=downsample_factor, \n                                      max_text_len=max_text_len, \n                                      is_validation_data=True)","4fcdcc4c":"# A handy-dandy function for checking the generator output\n# always sanity-check the data before passing it to the model\ndef visualize_data_gen_output(data_gen, samples_to_visualize=2):\n    for i, (inp, out) in enumerate(data_gen):\n        print('Text generator output (data which will be fed into the neutral network):')\n        print('1)the_input (image)')\n        img = (inp['input_data'][i, :, :, 0]*255).astype(np.uint8)\n        plt.imshow(img.T, cmap='gray')\n        plt.show()\n        print(f\"2) the_labels(captcha) {labels_to_text(inp['input_label'][i])} is encoded as {list(map(int, inp['input_label'][i]))}\") \n        print(f\"3) input_length (width of image that is fed to the network after CNN): {inp['input_length'][i][0]} == (200\/5 - 2)\")\n        print(f\"4) label_length (length of captcha): {inp['label_length'][i][0]}\")\n        print(\" \")\n        if i==samples_to_visualize:\n            break","e8cffeac":"visualize_data_gen_output(train_data_generator)","d904c903":"# Using this loss function as the output\n# The loss function in model.compile(..) will be a dummy one\n# This is different from a normal scenario where we pass an actual \n# loss function when compile the model\ndef ctc_lambda_func(args):\n    y_pred, labels, input_length, label_length = args\n    # the 2 is critical here since the first couple outputs of the RNN\n    # tend to be garbage:\n    y_pred = y_pred[:, 2:, :]\n    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)","795bae33":"def build_model():\n    # Inputs to the model\n    input_img = Input(shape=(img_width, img_height, 1), name='input_data', dtype='float32')\n    labels = Input(name='input_label', shape=[max_text_len], dtype='float32')\n    input_length = Input(name='input_length', shape=[1], dtype='int64')\n    label_length = Input(name='label_length', shape=[1], dtype='int64')\n    \n    # Convolution part for feaure extraction\n    x = Conv2D(32, (3,3), activation='relu', kernel_initializer='he_normal', padding='same', name='Conv1')(input_img)\n    x = MaxPooling2D((2,2), name='pool1')(x)\n    x = Conv2D(64, (3,3), activation='relu', kernel_initializer='he_normal', padding='same', name='Conv2')(x)\n    x = MaxPooling2D((2,2), name='pool2')(x)\n    \n    # Reshape the features for passing to RNN\n    # We have used two max pool with pool size and strides of 2. Hence, downsampled is 4x smaller\n    # Also, the number of filters in the last layer is 64.\n    new_shape = ((img_width \/\/ 4), (img_height \/\/ 4)*64)\n    x = Reshape(target_shape=new_shape, name='reshape')(x)\n    x = Dense(64, activation='relu', name='dense1')(x)\n    \n    # RNNs\n    x = Bidirectional(LSTM(128, return_sequences=True,  name='lstm_1'), name='bi_1')(x)\n    x = Bidirectional(LSTM(128, return_sequences=True,  name='lstm_2'), name='bi_2')(x)\n    \n    # final part\n    x = Dense(len(letters)+1, activation='softmax', name='dense2', kernel_initializer='he_normal')(x)\n    \n    # Get the CTC loss and represent it in a layer\n    output = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc_loss')([x, labels, input_length, label_length])\n    \n    # define the final model\n    model = Model([input_img, labels, input_length, label_length], output, name='ocr_model_v1')\n    \n    # optimizer\n    sgd = SGD(lr=0.002, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n    \n    # the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n    # this is the reason we have this ctc_loss array of zeros in our generator\n    model.compile(loss={'ctc_loss': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n    return model","79a457cf":"model = build_model()\nmodel.summary()","b0194848":"# things required for starting the training \nnb_epochs = 50\nnb_train_steps = training_data.shape[0] \/\/ batch_size\nnb_validation_steps = validation_data.shape[0] \/\/ batch_size\nes = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\nckpt = ModelCheckpoint(filepath='ocr_v2.h5', save_best_only=True, monitor='val_loss')","d4274d8c":"# Train the model\nhistory = model.fit_generator(train_data_generator, \n                    epochs=nb_epochs, \n                    steps_per_epoch=nb_train_steps, \n                    validation_data=valid_data_generator, \n                    validation_steps=nb_validation_steps,\n                    callbacks=[es, ckpt])","0bdeadf5":"def decode_batch_predictions(pred):\n    pred = pred[:, 2:]\n    input_len = np.ones(pred.shape[0])*pred.shape[1]\n    \n    # Use greedy search. For complex tasks, you can use beam search\n    results = K.get_value(K.ctc_decode(pred, \n                                   input_length=input_len,\n                                   greedy=True)[0][0])\n    \n    # Iterate over the results and get back the text\n    texts = []\n    for res in results:\n        outstr = ''\n        for c in res:\n            if c < len(letters):\n                outstr += letters[c]\n        texts.append(outstr)\n    \n    # return final text results\n    return texts","bb3633f8":"# Get the input output layer and define a Keras function\n# It is similar to getting layers in tensorflow and \n# passing the information to the session.\noutput_func = K.function([model.get_layer(name='input_data').input],\n                        [model.get_layer(name='dense2').output])\n\n\n#  Let's check results on some validation samples\nfor p, (inp_value, _) in enumerate(valid_data_generator):\n    bs = inp_value['input_data'].shape[0]\n    X_data = inp_value['input_data']\n    labels = inp_value['input_label']\n    \n    preds = output_func([X_data])[0]\n    pred_texts = decode_batch_predictions(preds)\n    \n    \n    orig_texts = []\n    for label in labels:\n        text = ''.join(list(map(lambda x: letters[int(x)], label)))\n        orig_texts.append(text)\n        \n    for i in range(bs):\n        print(f'GT: {orig_texts[i]} \\t Predicted: {pred_texts[i]}')\n    break","92b7268e":"# **TESTING MODEL**","a78cf65f":"**Building Model**","8fe31f65":"**SPLITTING TRAINING AND VALIDATION DATA**","c011257f":"**Listing Input directory**","b2702c5d":"**IMAGE LABELLING**","6726e7ce":"**IMPORTING DATASETS**","86b3b41f":"# **TRAINING MODEL**"}}