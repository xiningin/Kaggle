{"cell_type":{"58ca5e33":"code","8401e505":"code","582c7041":"code","5efa57aa":"code","e553a849":"code","788620e6":"code","8dd70f1f":"code","5d9ad33e":"code","adaff8e7":"code","d0aa9f30":"code","cbe279f8":"code","24ae0342":"code","bdb6dd99":"code","f491f544":"code","4c9e8453":"code","b1a629d5":"code","951ab977":"code","5e609ce1":"code","322a4f21":"code","b7e4c710":"code","d24ce927":"code","97f1ac90":"code","c51bc5b5":"code","72c92614":"code","ff642da0":"code","e9447342":"code","98cf658a":"code","2eba7fce":"code","e7593e77":"code","32ab8d35":"code","0b307716":"code","96e1eb7f":"code","6f462d1d":"code","24beb123":"code","27f13987":"code","9656af32":"code","0c4c921c":"code","a7c83492":"code","ba9c6cbb":"code","2b726c05":"code","9e391bbb":"code","467e82eb":"code","e5d0aafa":"code","353c8dd3":"code","32c9ab77":"code","1f932200":"code","63b53457":"code","41657278":"markdown","6a0d11ec":"markdown","177a3556":"markdown","c1445abd":"markdown","5a7534cc":"markdown","9728748e":"markdown","67e8734f":"markdown","f4737d32":"markdown","91715be7":"markdown","6918daee":"markdown","e836880c":"markdown","08d85f02":"markdown","24a124c7":"markdown","6fa97679":"markdown","69a71ee3":"markdown","3f74f2fa":"markdown","329eb454":"markdown","a447e93b":"markdown","b7e3f737":"markdown","0c539f5b":"markdown","4daca32d":"markdown","0f3d84cb":"markdown","e54b7a33":"markdown","a1af7afa":"markdown","7b9fc436":"markdown","a6f72aa2":"markdown","3860f292":"markdown","1049084b":"markdown","fee112e5":"markdown","7300c02f":"markdown","7ddc1f2d":"markdown","5823f1b9":"markdown","6d517778":"markdown","fb260f6b":"markdown","7170e128":"markdown","0052f386":"markdown","ae48f139":"markdown","0195e106":"markdown","cb4a0de4":"markdown","15114523":"markdown","b6155d05":"markdown","45c04595":"markdown","cd7b4e18":"markdown"},"source":{"58ca5e33":"pip install pywaffle","8401e505":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nimport lightgbm\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import make_scorer\nfrom pywaffle import Waffle","582c7041":"train_data = pd.read_csv(\"..\/input\/black-friday\/train.csv\")\ntest = pd.read_csv(\"..\/input\/black-friday\/test.csv\")","5efa57aa":"train_data.head()","e553a849":"train_data.info()","788620e6":"train_data.describe(include = 'all')","8dd70f1f":"train_data.nunique()","5d9ad33e":"train_data.isna().sum()","adaff8e7":"train_data.hist(figsize=(20,10), color = 'teal')","d0aa9f30":"gender = train_data['Gender'].value_counts()\n\nfig = plt.figure(\n    FigureClass=Waffle, \n    rows=5,\n    columns=10,\n    values=gender,\n    title={'label': 'Gender Distribution', 'loc': 'center','size':20},\n    labels=[\"{}({})\".format(a, b) for a, b in zip(gender.index, gender) ],\n    legend={'loc': 'upper left', 'bbox_to_anchor': (1,1)},\n    font_size=35, \n    icons = ['male','female'],\n    icon_legend=True,\n    figsize=(12, 8)\n)\nMarital_Status = train_data['Marital_Status'].value_counts()\nfig = plt.figure(\n    FigureClass=Waffle, \n    rows=5,\n    columns=10,\n    values=Marital_Status,\n    title={'label': 'Marital Status Distribution', 'loc': 'center','size':20},\n    labels=[\"{}({})\".format(a, b) for a, b in zip(Marital_Status.index, Marital_Status) ],\n    legend={'loc': 'upper left', 'bbox_to_anchor': (1,1)},\n    font_size=35,\n    icons = 'ring',\n    icon_legend=True,\n    figsize=(12, 8)\n)","cbe279f8":"fig,ax = plt.subplots(figsize=(20,6),ncols=2,nrows=1)\nsns.barplot(x=\"Gender\",y=\"Purchase\",hue=\"Marital_Status\",estimator=np.mean,data=train_data,ax=ax[0] , palette=\"Set2\").set_title(label = 'Gender vs Marital Status Average Purchase Distribution', size =15)\nsns.barplot(x=\"Gender\",y=\"Purchase\",hue=\"Marital_Status\",estimator=np.sum,data=train_data,ax=ax[1] , palette=\"Set2\").set_title(label = 'Gender vs Marital Status Purchase Distribution', size =15)","24ae0342":"City = train_data['City_Category'].value_counts()\nfig = plt.figure(\n    FigureClass=Waffle, \n    rows=5,\n    columns=10,\n    values=City,\n    title={'label': 'City Category Distribution', 'loc': 'center','size':20},\n    labels=[\"{}({})\".format(a, b) for a, b in zip(City.index, City) ],\n    legend={'loc': 'upper left', 'bbox_to_anchor': (1,1)},\n    font_size=35,\n    icons = 'star',\n    icon_legend=True,\n    figsize=(12, 8)\n)\n\nStay = train_data['Stay_In_Current_City_Years'].value_counts()\nfig = plt.figure(\n    FigureClass=Waffle, \n    rows=5,\n    columns=10,\n    values=Stay,\n    title={'label': 'Stay in Current City Distribution', 'loc': 'center','size':20},\n    labels=[\"{}({})\".format(a, b) for a, b in zip(Stay.index, Stay) ],\n    legend={'loc': 'upper left', 'bbox_to_anchor': (1,1)},\n    font_size=35,\n    icons = 'circle', \n    icon_legend=True,\n    figsize=(12, 8)\n)","bdb6dd99":"fig,ax = plt.subplots(figsize=(20,8),ncols=1,nrows=1)\nsns.barplot(x=\"City_Category\",y=\"Purchase\",hue=\"Stay_In_Current_City_Years\",estimator=np.mean,data=train_data, palette=\"Set2\").set_title(label = 'City Category vs Stay in Current City Purchases Distribution', size =20)","f491f544":"\nAge = train_data['Age'].value_counts()\nfig = plt.figure(\n    FigureClass=Waffle, \n    rows=5,\n    columns=10,\n    values=Age,\n    title={'label': 'Age Distribution', 'loc': 'center','size':20},\n    labels=[\"{}({})\".format(a, b) for a, b in zip(Age.index, Age) ],\n    legend={'loc': 'upper left', 'bbox_to_anchor': (1,1)},\n    font_size=35,\n    icon_legend=True,\n    figsize=(12, 8)\n)","4c9e8453":"fig,ax = plt.subplots(figsize=(12,8),ncols=1,nrows=1)\nsns.barplot(x=\"Age\",y=\"Purchase\",estimator=np.mean,data=train_data, palette=\"Set2\", order=[\"0-17\", \"18-25\",\"26-35\",\"36-45\",\"46-50\",\"51-55\",\"55+\"]).set_title(label = 'Age bucket vs Average Purchase Distribution', size =20)","b1a629d5":"Occupation_percent =train_data.groupby('Occupation').Purchase.agg(['sum'])\nOccupation_percent=Occupation_percent.apply(lambda x: 100 * x \/ float(x.sum())).reset_index()\nOccupation_percent=Occupation_percent.sort_values(by = ['sum'],ascending=False)\nexplode = (0,0,0,0,0,0,0,0,0,0,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.5,0.5,0.5,0.5)\nplt.figure(figsize=(20,12))\nplt.pie(Occupation_percent['sum'],labels=Occupation_percent['Occupation'], explode= explode,autopct='%1.0f%%', counterclock=True,startangle=45, colors = sns.color_palette('Set2'))\nplt.title(label= 'Average Purchase Amount By Occupation', loc = 'center', size = 20)\nplt.show","951ab977":"fig,ax = plt.subplots(figsize=(12,8),ncols=1,nrows=1)\nsns.barplot(x=\"Occupation\",y=\"Purchase\",estimator=np.mean,data=train_data, palette=\"Set2\").set_title(label = 'Occupation vs Average Purchase Distribution', size =20)","5e609ce1":"fig,ax = plt.subplots(figsize=(20,4),ncols=2,nrows=1)\nsns.barplot(x=\"Product_Category_1\",y=\"Purchase\",estimator=np.mean,data=train_data,ax=ax[0], palette=\"Set2\").set_title(label = 'Product Category 1 vs Average Purchase Distribution', size =20)\nsns.countplot(x=\"Product_Category_1\",data=train_data,ax=ax[1], palette=\"Set2\").set_title(label = 'Total Count Distribution In Product Category 1', size =20)","322a4f21":"fig,ax = plt.subplots(figsize=(20,4),ncols=2,nrows=1)\nsns.barplot(x=\"Product_Category_2\",y=\"Purchase\",estimator=np.mean,data=train_data,ax=ax[0], palette=\"Set2\").set_title(label = 'Product Category 2 vs Average Purchase Distribution', size =20)\nsns.countplot(x=\"Product_Category_2\",data=train_data,ax=ax[1], palette=\"Set2\").set_title(label = 'Total Count Distribution In Product Category 2', size =20)","b7e4c710":"fig,ax = plt.subplots(figsize=(20,4),ncols=2,nrows=1)\nsns.barplot(x=\"Product_Category_3\",y=\"Purchase\",estimator=np.mean,data=train_data,ax=ax[0], palette=\"Set2\").set_title(label = 'Product Category 3 vs Average Purchase Distribution', size =20)\nsns.countplot(x=\"Product_Category_3\",data=train_data,ax=ax[1], palette=\"Set2\").set_title(label = 'Total Count Distribution In Product Category 3', size =20)","d24ce927":"def triple_plot(x, title):\n    fig, ax = plt.subplots(1,3,figsize=(15,4),sharex=True)\n    sns.distplot(x, ax=ax[0], color = 'teal')\n    ax[0].set(xlabel=None)\n    ax[0].set_title('Histogram + KDE')\n    sns.boxplot(x, ax=ax[1], color = 'lightslategrey')\n    ax[1].set(xlabel=None)\n    ax[1].set_title('Boxplot')\n    sns.violinplot(x, ax=ax[2], color = 'springgreen')\n    ax[2].set(xlabel=None)\n    ax[2].set_title('Violin plot')\n    fig.suptitle(title, fontsize=16)\n    plt.tight_layout(pad=3.0)\n    plt.show()\ntriple_plot(train_data['Purchase'],'Distribution of Purchase')","97f1ac90":"submission = test[['User_ID','Product_ID']]","c51bc5b5":"train = train_data.sample(frac=0.999,random_state=0) #random state is a seed value\ndev = train_data.drop(train.index)","72c92614":"train['Age'] = train['Age'].replace(['0-17','18-25','26-35','36-45','46-50','51-55','55+'],[9,22,31,41,48,53,60])\ntrain['Stay_In_Current_City_Years'] = train['Stay_In_Current_City_Years'].replace(['4+'],[5])\n\ndev['Age'] = dev['Age'].replace(['0-17','18-25','26-35','36-45','46-50','51-55','55+'],[9,22,31,41,48,53,60])\ndev['Stay_In_Current_City_Years'] = dev['Stay_In_Current_City_Years'].replace(['4+'],[5])\n\ntest['Age'] = test['Age'].replace(['0-17','18-25','26-35','36-45','46-50','51-55','55+'],[9,22,31,41,48,53,60])\ntest['Stay_In_Current_City_Years'] = test['Stay_In_Current_City_Years'].replace(['4+'],[5])","ff642da0":"train['Gender'] = train['Gender'].replace(['M','F'],[0,1])\ntrain['City_Category'] = train['City_Category'].replace(['A','B','C'],[1,2,3])\n\ndev['Gender'] = dev['Gender'].replace(['M','F'],[0,1])\ndev['City_Category'] = dev['City_Category'].replace(['A','B','C'],[1,2,3])\n\ntest['Gender'] = test['Gender'].replace(['M','F'],[0,1])\ntest['City_Category'] = test['City_Category'].replace(['A','B','C'],[1,2,3])","e9447342":"train = train.fillna(0)\ndev = dev.fillna(0)\ntest = test.fillna(0)","98cf658a":"train['Average_Cost'] = train.groupby(['Product_ID'])['Purchase'].transform('mean')\ntrain['Buying_Power'] =  train.groupby(['User_ID'])['Purchase'].transform('mean')\n\nproduct_price = train[['Average_Cost','Product_ID']].drop_duplicates()\naverage_cost = product_price['Average_Cost'].mean()\nprint(\"Average Cost of Products is \", average_cost)\n\nuser_buying_power =  train[['Buying_Power','User_ID']].drop_duplicates()\nbuying_power = user_buying_power['Buying_Power'].mean()\nprint(\"Average Buying Power of users is \", buying_power)","2eba7fce":"print(\"Dev dimensions before adding features \", dev.shape)\nprint(\"Test dimensions before adding features \", test.shape)\n\ndev = dev.merge(product_price, how = 'left', left_on = 'Product_ID',right_on = 'Product_ID')\ndev = dev.merge(user_buying_power, how = 'left', left_on = 'User_ID',right_on = 'User_ID')\n\ntest = test.merge(product_price, how = 'left', left_on = 'Product_ID',right_on = 'Product_ID')\ntest = test.merge(user_buying_power, how = 'left', left_on = 'User_ID',right_on = 'User_ID')\n\nprint(\"Dev dimensions after adding features \", dev.shape)\nprint(\"Test dimensions after adding features \", test.shape)","e7593e77":"print(\"Nulls in Dev before imputing features \", dev.isna().sum())\nprint(\"Nulls in Test before imputing features \", test.isna().sum())\n\ndev['Average_Cost']  = dev['Average_Cost'].fillna(average_cost)\ndev['Buying_Power']  = dev['Buying_Power'].fillna(buying_power)\n\ntest['Average_Cost']  = test['Average_Cost'].fillna(average_cost)\ntest['Buying_Power']  = test['Buying_Power'].fillna(buying_power)\n\nprint(\"Nulls in Dev after imputing features \", dev.isna().sum())\nprint(\"Nulls in Test after imputing features \", test.isna().sum())","32ab8d35":"categorical_columns = [\"Gender\", \"Occupation\", \"City_Category\", \"Stay_In_Current_City_Years\",\n                       \"Marital_Status\", \"Product_Category_1\", \"Product_Category_2\", \"Product_Category_3\"]","0b307716":"# train = pd.get_dummies(train, columns= categorical_columns)\n# dev = pd.get_dummies(dev, columns= categorical_columns)\n# test = pd.get_dummies(test, columns= categorical_columns)\n\n# for i in list(train.columns):\n#     if i not in list(dev.columns):\n#         dev[i] = 0\n#     if i not in list(test.columns):\n#         test[i] = 0","96e1eb7f":"train = train.drop(columns =['User_ID','Product_ID'])\ndev = dev.drop(columns =['User_ID','Product_ID'])\ntest = test.drop(columns =['User_ID','Product_ID'])","6f462d1d":"train = train[ [ col for col in train.columns if col != 'Purchase' ] + ['Purchase'] ]\n# dev = dev[train.columns]\n# test = test[train.columns]\n# test = test.drop(columns =['Purchase'])","24beb123":"sns.heatmap(train[ [ col for col in train.columns if col not in categorical_columns ]].corr(),annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':10})\n#run this only with LightGBM Columns","27f13987":"sns.pairplot(train[ [ col for col in train.columns if col not in categorical_columns ]], palette = 'Set2') #run this only with LightGBM Columns","9656af32":"X_train = train.iloc[:,:-1].values\nX_dev = dev.iloc[:,:-1].values\ny_train = train.iloc[:,-1].values\ny_actual = dev.iloc[:,-1].values\n\nX_test = test.iloc[:,:].values","0c4c921c":"Scaler = StandardScaler()\n\nScaler.fit(X_train)\nX_train = Scaler.transform(X_train)\nX_dev = Scaler.transform(X_dev)\nX_test = Scaler.transform(X_test)","a7c83492":"def rmse(predictions, targets): \n  return np.sqrt(((predictions - targets) ** 2).mean())\n\nrmse_score = make_scorer(rmse, greater_is_better=False)","ba9c6cbb":"# reg = LinearRegression()\n# reg.fit(X_train,y_train)\n# y_pred = reg.predict(X_dev)\n# print(\"Linear Regression RMSE on 20% data is \", rmse(y_pred,y_actual))","2b726c05":"# reg = DecisionTreeRegressor()\n# reg.fit(X_train,y_train)\n# y_pred = reg.predict(X_dev)\n# print(\"Decision Tree Regressor RMSE on 20% data is \", rmse(y_pred,y_actual))","9e391bbb":"# reg = RandomForestRegressor()\n# reg.fit(X_train,y_train)\n# y_pred = reg.predict(X_dev)\n# print(\"Random Forest Regressor RMSE on 20% data is \", rmse(y_pred,y_actual))","467e82eb":"# reg = LGBMRegressor(metric = 'rsme')\n# reg.fit(X_train,y_train)\n# y_pred = reg.predict(X_dev)\n# print(\"LightGBM RMSE on 20% data is \", rmse(y_pred,y_actual))","e5d0aafa":"lgb = LGBMRegressor(metric = 'rmse', categorical_columns = categorical_columns,subsample = 0.5, num_leaves = 500, num_iterations =200,  random_state=0 )\nparam_test ={'learning_rate' : [0.05,0.1,0.2,0.3]}\n\nTotal_sets = 100\n\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\ngs = RandomizedSearchCV(\n    estimator=lgb, param_distributions=param_test, \n    n_iter=Total_sets,\n    scoring=rmse_score,\n    cv=4,\n    refit=True,\n    random_state=314,\n    n_jobs = 4,\n    verbose=True)\ngs.fit(X_train, y_train)\nprint('Best score reached: {} with params: {} '.format(-1*gs.best_score_, gs.best_params_))\ny_pred  = gs.predict(X_dev)\nscore =  rmse(y_pred,y_actual)\ny_test = gs.predict(X_test)\nsubmission['Purchase'] = pd.DataFrame(y_test) \nsubmission.to_csv(\".\/submission_jupyter.csv\")","353c8dd3":"lgb = LGBMRegressor(metric = 'rmse', categorical_columns = categorical_columns,subsample = 0.5, num_leaves = 500, num_iterations =200,  random_state=0,learning_rate = 0.1)\nlgb.fit(X_train, y_train)\ny_predicted = lgb.predict(X_train)","32c9ab77":"sorted(zip(lgb.feature_importances_, train.columns), reverse=True)\nfeature_imp = pd.DataFrame(sorted(zip(lgb.feature_importances_,train.columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(15, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title(label= 'LightGBM Feature Importance', size = 20)\nplt.tight_layout()\nplt.show()","1f932200":"plt.scatter(y_train, y_predicted, c = 'green')\nplt.xlabel(\"Purchase\")\nplt.ylabel(\"Predicted Purchase\")\nplt.title(label = \"Purchase vs Predicted Purchase\", size = 20)\nplt.show()","63b53457":"plt.scatter(y_train, y_train -y_predicted, c = 'red')\nplt.xlabel(\"Purchase\")\nplt.ylabel(\"Residual\")\nplt.title(label = \"Purchase vs Residual\", size = 20)\nplt.show()","41657278":"# Importing and installing Libraries","6a0d11ec":"Creating initial file for Submission","177a3556":"Bringing the Target Column to the last and commented out the code, which is not required for LightGBM","c1445abd":"A retail company wants to comprehend and interpret the purchase behavior of their customers, with respect to various products belonging to different categories, for the Black Friday sale. The data available for the company is the purchase summary of customers which includes customer demographics, product information, the total purchase amount, for chosen high volume products sold during the Black Friday period of last year.\n\nThe objective is to develop a model to predict the purchasing capacity with respect to various products that would aid them in creating personalized offers for customers against different products along with understanding which areas make more sales during Black Friday","5a7534cc":"Checking correlation of numerical variables","9728748e":"Converting Categories to Numerical type because LightGBM expects the categorical data to be encoded as numbers","67e8734f":"Understanding the summary statistics of all variables in the data set\n1. Most of the variables like occupation, Product categories in the dataset are masked with integers and City_Category is masked with alphabets\n2. Product P00265242 is the most popular product! with 1880 occurences.\n3. Male buyers\u00a0are more frequent in the dataset than female buyers.\n4. Age group with most transactions was 26-35.\n5. Occupation '4' had the most transactions.\n6. City Category with most transactions was B.\n7. Highest number of purchasers had '1 year stay' in the current city.\n8. Data set has more singles (Marital status 0) than married people (Marital status 1).\n","f4737d32":"This solution gave a rank in the Top 10 percentile in the Hackathon","91715be7":"Number 10 in product category 2 & number 3 in product category 2 has premium products with high average purchase value","6918daee":"Checking the Scatter of Purchase vs Predicted Purcahse for the complete dataset","e836880c":"# Data Processing","08d85f02":"Checking the unique values in each of the variables. User ID and product ID have a lot unique categories and hence can't be used directly in the model. We need to explore what features can be extracted from these two columns.\n","24a124c7":"Exploring the frequency distributions of all the columns. There are noticeable variations in the variables and this can be helpful in explaining the variation of the purchase amounts in the data set.","6fa97679":"# Data Visualization and Gathering Insights","69a71ee3":"Number 10 in product category 1 has the maximum mean purchase value followed by Number 7. Number 5 is bought most frequently","3f74f2fa":"Creating a scorer to measure the model performance","329eb454":"Converting Age and Stay in Current City Buckets to numbers so as to cast the data from categorical type to numeric","a447e93b":"52% of sales are shared among 5 occupations while occupation wise average user spend shows some variation","b7e3f737":"Checking the Scatter of Purchase vs Residual for the complete dataset","0c539f5b":"Average purchase value of single men is slightly higher than married people, while the trend is opposite in case of women\u00a0!","4daca32d":"Dropping the User ID and Product ID columns","0f3d84cb":"Checking the number of null values for each of the variables. Product category 2 and 3 have high proportion of nulls and we need to explore what can be done with these null values\n","e54b7a33":"Checking the variable names and the first 5 rows of the data frame","a1af7afa":"Creating a list of categorical columns, which can be used as input for LightGBM","7b9fc436":"Scaling the data based on the traingdata so that all columns are in the same range ","a6f72aa2":"Checking the information and data types of the variable of the data","3860f292":"Ran all the basic models to check RSME and selected the model with highest score to  perform further Hypertuning","1049084b":"There are a lot of missing values in Product Category to 2,3. Instead of imputing all of them, creating a new category '0'. Tried using the mode, but using it as new category gave the best score","fee112e5":"# Introduction","7300c02f":"Creating the datasets for modelling","7ddc1f2d":"The variation in Gender can help us explain the variations in the purchase amount. However, variation in marital status is too little to explain any price variation in the purchase amounts. Average purchase value of single men is slightly higher than married people, while the trend is opposite in case of women\u00a0!","5823f1b9":"Performing a step wise grid search for tuning the model","6d517778":"As we have to drop User ID and Product ID columns, created new features with approximately resembles them. Average Cost of Products for Product ID and Buying Power (total amount spent) for User ID. Test data doesn't have the price data to calculate this, so using the data from train to impute it in Test and Dev. If there are any new users in Test or Dev, we are imputing it with Global averages.","fb260f6b":"50% of the times customer tends to buy a product with a purchase value between (5500,13000) during Black Friday","7170e128":"Commented out the Hot encoding code, as these are required only for Linear Regression, Decision Tree and Random Forest. LightGBM has an internal way of handling categorical data ","0052f386":"Light GBM gave us the best score. We will further tune this model to improve model performance along with Cross Validation over 4 folds","ae48f139":"Plotting the feature importance. As expected, Average Cost and Buying Power are the features with highest importance. As seen in the visualization- City, Stay in city, Marital Status, Gender did not have a lot of variance which led to a lower effect on Purchase Amount. Occupation and Product Categories also had significant effect on Purchase amount.","0195e106":"# Train, Predict and Test Model Performance","cb4a0de4":"Customers from city C buy products of higher purchase value, while stay is current city shows some variation!","15114523":"Checking scatter plots of numerical variables","b6155d05":"Maximum number of purchasers are in the 26-35 age group, while the average purchase vs Age shows positive trend!","45c04595":"# Loading and Understanding Data","cd7b4e18":"Splitting the input train data into Train and Development data, to train the data on train and test it on development data. And then use that model for predictions on the test. Started with 80-20 split and after fitting the model, used 99% of the data for final predictions."}}