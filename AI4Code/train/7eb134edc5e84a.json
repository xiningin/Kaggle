{"cell_type":{"7f2f4881":"code","ea5b0df7":"code","cd28a70a":"code","92801b52":"code","69f5c86f":"code","317628ac":"code","e906f070":"code","d8fb9930":"code","068eb1df":"code","8266dd42":"code","cc3f5350":"code","5fb85e93":"code","c781e135":"code","a15dc686":"code","fa251779":"code","784b9f5f":"code","dd9cf328":"code","28361690":"code","fd9fed8c":"code","0e157816":"code","5dfa16fc":"markdown","08c33984":"markdown","37ce2777":"markdown","78a054d9":"markdown","89ee516f":"markdown","d87c5e16":"markdown","a3921640":"markdown","0ec10380":"markdown","6165f59c":"markdown","91a0bb87":"markdown","dc55ec98":"markdown","a361846b":"markdown","98b0c23d":"markdown","94bffb62":"markdown","09ce6f45":"markdown","48b39ee3":"markdown","4253f6eb":"markdown","f28d2664":"markdown","29ee69b0":"markdown","e97ec241":"markdown","579a7e00":"markdown"},"source":{"7f2f4881":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler,OrdinalEncoder, RobustScaler\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\n","ea5b0df7":"train_file = \"\/kaggle\/input\/30-days-of-ml\/train.csv\"\ntest_file = \"\/kaggle\/input\/30-days-of-ml\/test.csv\"\nX = pd.read_csv(train_file,index_col = 'id')\nX_test_full = pd.read_csv(test_file, index_col = 'id')","cd28a70a":"#training data inspection\nX.head()","92801b52":"print(\"Shape of the training data: \", X.shape)\nprint(\"*\"*70)\nprint(X.info())\nprint(\"*\"*70)\nX.describe(include = \"all\")\n","69f5c86f":"# test data inspection\nX_test_full.head()","317628ac":"print(\"Shape of the test data: \", X_test_full.shape)\nprint(\"*\"*70)\nprint(X_test_full.info())","e906f070":"#using isnull() method\nX.isnull().sum()","d8fb9930":"#using sns heatmap \nsns.heatmap(X.isnull(), cbar = False)","068eb1df":"X.drop_duplicates()","8266dd42":"#first store the numerical features in a seperate dataframe.\nnum_cols  = [col for col in X.columns if X[col].dtype in ['int64', 'float64']]\nnum_cols.remove(\"target\")\n#Now plot a boxplot to identify the outliers in our numerical features.\nsns.boxplot(data = X[num_cols], orient = 'h', palette = 'Set3', linewidth = 2.5 )\nplt.title(\"Numerical Features Box Plot\")\n","cc3f5350":"sns.boxplot(x = X[\"target\"], orient = 'h', linewidth = 2.5 )\nplt.title(\"Target Column Box Plot\")\n","5fb85e93":"from scipy import stats\ndef removeoutliers(df=None, columns=None):\n    for column in columns:\n        Q1 = df[column].quantile(0.25)\n        Q3 = df[column].quantile(0.75)\n        IQR = Q3 - Q1\n        floor, ceil = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n        df[column] = df[column].clip(floor, ceil)\n        print(f\"The columnn: {column}, has been treated for outliers.\\n\")\n    return df\nout_cols = [\"cont0\",\"cont6\",\"cont8\",\"target\"]\nX = removeoutliers(X,out_cols)","c781e135":"sns.boxplot(data = X[num_cols], orient = 'h', palette = 'Set3', linewidth = 2.5 )\nplt.title(\"Numerical Features Box Plot after treating outliers\")","a15dc686":"sns.boxplot(x = X[\"target\"], orient = 'h', linewidth = 2.5 )\nplt.title(\"Target Column Box Plot after treating outliers\")\n","fa251779":"#Remove the rows with missing target\nX.dropna(axis = 0, subset = [\"target\"], inplace = True)\ny = X[\"target\"]\n# Remove the target column from the training dataset\nX.drop([\"target\"],axis=1,inplace = True)\nX.head()","784b9f5f":"# Split the data into train test set\nX_train_full, X_valid_full,y_train, y_valid = train_test_split(X,y, \n                                                               train_size = 0.9, test_size = 0.1, \n                                                               random_state = 0)\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncat_cols = [cname for cname in X_train_full.columns if\n                    X_train_full[cname].dtype == \"object\"]\n\n# Select numeric columns\nnum_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n# Keep selected columns only\nmy_cols = cat_cols + num_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\nX_train.head()\n\n","dd9cf328":"# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='mean')\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')) \n    ,('scaler', OrdinalEncoder())\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, num_cols),\n        ('cat', categorical_transformer, cat_cols)\n    ],\n    remainder=\"passthrough\"\n  )\n","28361690":"model = XGBRegressor(n_estimators = 5000, learning_rate = 0.03)\n#I have tried:\n#n_est = 5000, l_rate = 0.1, rmse = 0.73\n# n_est = 1000, l_rate = 0.3, rmse = 0.722\n#n_est = 2000, l_rate = 0.2, rmse = 0.727\n#n_est = 1500. l_rate = 0.1, rmse = 0.715\n#n_est = 1400, l_rate = 0.1, rmse = 0.714\n#n_est = 1200, l_rate = 0.1, rmse = 0.714\n#n_est = 1000, l_rate = 0.1, rmse = 0.714\n#n_estimators = 1200, learning_rate = 0.03","fd9fed8c":"# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_valid)\n\n# Evaluate the model\nscore = mean_squared_error(y_valid, preds)\nrmse = math.sqrt(score)\nprint('RMSE:', rmse)\n\n","0e157816":"test_predic = my_pipeline.predict(X_test)\n# Save test predictions to csv file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': test_predic})\noutput.to_csv(\"30_Days_ML_Challenge_ver7.csv\", index=False)","5dfa16fc":"From the above boxplot we can see that cont0, cont6, and cont 8 possess outliers.\nLet us check the target column for outliers.","08c33984":"# 30 Days ML Challenge\nThis notebook contains my working on the #30DaysMLChallenge dataset. The dataset consists of total 500000 rows (train = 300000, test = 200000) and 26 columns. The first column is the 'id' columns, the next 10 columns (cat0 - cat9) are categorical columns, which is followed by 14 numerical columns (cont0 - cont13), and the last column is the target column. The goal is to predict the continous target column.\n\nI will be applying the techniques which I have learnt from Kaggle courses, codes from discussion tab, and also what I have learnt from NED University Industrial Data Science Training Program under the mentorship of Sir Waqas Ali and last but not the least, my ongoing MS degree in Data Science.\n","37ce2777":"**4.1.2 Duplicate Data:**\n\nWe will just drop the duplicate data (if any) from our training dataset.","78a054d9":"# 3. Inspecting the Data","89ee516f":"# 5. Preparing the Data for Model Building and pipeline\n","d87c5e16":"# 2. Loading the data","a3921640":"# 1. Importing the important Libraries","0ec10380":"Check out the **XGBoost** lesson and exercise from the kaggle course, **Intermediate Machine Learning**","6165f59c":"**Step 3. Create and Evaluate the pipeline**\n\nFinally, we use the Pipeline class to define a pipeline that bundles the preprocessing and modeling steps.","91a0bb87":"# 6. Predicting the test dataset","dc55ec98":"# 7. Conclusion\n\nIn this notebook, my goal was to predict the target with minimum RMSE (Root Mean Square Error) as possible. I tried to explain it in a very simple but detailed manner so anyone from beginner to advance can easily understand it. Please upvote this if you have liked my work. Feel free to connect with me on my LinkedIn profile. Link given below.\n\nThanks and Regards,\n\nIftikhar Ud Din\n\nData Scientist\n\nLinkedIn: In\/DinIftikhar025\n","a361846b":"**4.1.3 Outliers:**\n\nThe outliers can be identified and dealt with using 3 different methods.\n* Univariate method\n* Multivariate method\n* Minkowski error\n\nWe will use the univariate method i-e box plot to identify and treat the outliers in the numerical columns.\n","98b0c23d":"**Numerical and target column after treating outliers**\n\nNow we plot the numerical features and target column to see whether the outliers have been removed or not.","94bffb62":"**Data Cleaning is done here!!!**\n\nCool! Now since we have dealt with the missing values, duplicate data, and outliers, finally we can move on to model building and preparing piepline.","09ce6f45":"**Step 2. Define the model**\n\nWe are using XGBoost Regressor to train our model. The values for the model are given on trial and error bases.  The optimal hyperparameters can be find using SKlearn GridSearchCV.","48b39ee3":"We will define our own function to identify and remove outliers. We will use the IQR (Inter Quartile Range) method to detect and remove outliers. The other method that we can use is the Z-score method but that doesnt work here quite well.\n\nfrom scipy import stats\n\ndf[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\n\n","4253f6eb":"If there was any missing data we could have seen it in the form of bars as shown in the following figure. The visualization helps in interpreting the missing data as you can see if the data is missing in a specific row or column or it is missing randomly anywhere in the data set.\n\n![image](https:\/\/res.cloudinary.com\/practicaldev\/image\/fetch\/s--a6RI0m_q--\/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\/https:\/\/thepracticaldev.s3.amazonaws.com\/i\/blloy3vrfai3uxf3wf07.png)","f28d2664":"# 4.1 Data Cleaning\n**4.1.1 Missing Data:**\n\nFrom the inspection of data we can see that there is no missing data but still we can confirm it by using different methods.","29ee69b0":"Our target column also contains the outliers. Now let us write a function to treat these outliers.","e97ec241":"# 4. Data Preprocessing\n**Why Data Preprocessing?**\nData Preprocessing ensures\n* Accuracy: correct or wrong data\n* Completeness: no missing or NAN values\n* Consistency: data is same everywhere in the dataset, not like some modified and some not\n* Timeliness:  Timely updated\n* Believability: trustable\n* Interpretable: easily understandable\n\n**Forms of Data Preprocessing**\n* Data Cleaning: Noisy (Outliers), missing, or inconsistent data\n* Data Integration: Data coming from different sources\n* Data Reduction: Dimensionality reduction, Numerosity reduction, Data compression\n* Data Transformation: Normalization, Data Discretization\n\nIn this notebook, we will use only data cleaning techniques.\n\n\n","579a7e00":"# 5. Preparing the Pipeline\n**Step 1. Define the preprocessing steps**\n\nWe use the ColumnTransformer class to bundle together different preprocessing steps. The code below:\n\n* imputes missing values in numerical data\n* imputes missing values and applies a ordinal encoding to categorical data."}}