{"cell_type":{"b74d3e3a":"code","588363ed":"code","15be8b1a":"code","dfab50a1":"code","da9e2498":"code","cd4723a4":"code","83ed6269":"code","ac0bf065":"code","291ad1f3":"code","e63c77a9":"code","f7945dba":"code","3f4d6503":"markdown","282315a7":"markdown","97d320bc":"markdown","0bdcbf8b":"markdown","8165ff41":"markdown","a6cccfc4":"markdown","e176ab72":"markdown","89620497":"markdown","b4acbb9a":"markdown"},"source":{"b74d3e3a":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nmobile_data = pd.read_csv(\"..\/input\/mobile-price-classification\/train.csv\")\n\nX = mobile_data.iloc[:,0:20]  #independent variables\ny = mobile_data.iloc[:,-1]    #target variable i.e price range","588363ed":"mobile_data.head()","15be8b1a":"#apply SelectKBest class to extract top 10 best features\n\nBestFeatures = SelectKBest(score_func=chi2, k=10)\nfit = BestFeatures.fit(X,y)\n","dfab50a1":"df_scores = pd.DataFrame(fit.scores_)\ndf_columns = pd.DataFrame(X.columns)","da9e2498":"#concatenating two dataframes for better visualization\n\nf_Scores = pd.concat([df_columns,df_scores],axis=1)               # feature scores\nf_Scores.columns = ['Specs','Score']  ","cd4723a4":"f_Scores                # Score value is directly proportional to the feature importance","83ed6269":"print(f_Scores.nlargest(10,'Score'))       # print 10 best features in descending order","ac0bf065":"import xgboost\nimport matplotlib.pyplot as plt\n\nmodel = xgboost.XGBClassifier()\nmodel.fit(X,y)","291ad1f3":"print(model.feature_importances_) ","e63c77a9":"# plot the graph of feature importances for better visualization \n\nfeat_imp = pd.Series(model.feature_importances_, index=X.columns)\nfeat_imp.nlargest(10).plot(kind='barh')\n\nplt.figure(figsize=(8,6))\nplt.show()","f7945dba":"import seaborn as sns\n\n#get correlations of each features in dataset\ncorrmat = mobile_data.corr()\ntop_corr_features = corrmat.index\n\nplt.figure(figsize=(20,20))\n\n#plot heat map\ng=sns.heatmap(mobile_data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","3f4d6503":"# Feature Selection\n\nWe all may have faced the problem of identifying the important features from a set of given data and removing the irrelevant or less important features which do not contribute much to our decision making in order to achieve better accuracy for our model.\n\nIn machine learning and statistics, feature selection, also known as **variable selection**, **attribute selection** or **variable subset selection**, is the process of reducing the number of input variables when developing a predictive model. Feature selection techniques are used for several reasons:\n\n* It reduces model complexity by dropping some irrelevant features.\n* Helps ML algorithm to train a model faster.\n* Redcution of dimensionality helps in avoid overfitting.\n\nIn this notebook i will be discussing 3 common techniques used for feature selection which are easy to implement and will give you a good results based on problem. Following are the feature selection techniques:\n\n1. **Univariate Selection**\n2. **Feature Importance** \n3. **Correlation Matrix with Heatmap**\n\n","282315a7":"### Note: Using Above score we can conclude that 'ram' is the most important feature among all the features which is also true in practical scenario.","97d320bc":"# Univariate Method\n\n* Statistical tests can be used to select those features that have the strongest relationship with the output variable.\n\n* The scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.\n\n* The example below uses the chi-squared (chi\u00b2) statistical test for non-negative features to select k (k=10) of the best features from the Mobile Price Range Prediction Dataset.","0bdcbf8b":"**To get the cheat sheet for Feature selection Techniques in ML please follow the below link:**\nhttps:\/\/www.kaggle.com\/getting-started\/186082\n","8165ff41":"# SUMMARY\n\n<p>We can summarize feature selection as follows.<\/p>\n<ul>\n<li><strong>Feature Selection<\/strong>: Select a subset of input features from the dataset.\n<ul>\n<li><strong>Unsupervised<\/strong>: Do not use the target variable for selecting the feature importance of Input variable (e.g. remove redundant variables).\n<ul>\n<li>Correlation<\/li>\n<\/ul>\n<\/li>\n<li><strong>Supervised<\/strong>: Use the target variable (e.g. remove irrelevant I\/P features).\n<ul>\n<li><strong>Wrapper Method<\/strong>: Search for well-performing subsets of features.\n<ul>\n<li>Recursive Feature Elimination (RFE)<\/li>\n<\/ul>\n<\/li>\n<li><strong>Filter Method<\/strong>: Select subsets of features based on their relationship with the target.\n<ul>\n<li>Statistical Methods<\/li>\n<li>Feature Importance Methods<\/li>\n<\/ul>\n<\/li>\n<li><strong>Intrinsic<\/strong>: Algorithms that perform automatic feature selection during training.\n<ul>\n<li>Decision Trees<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<li><strong>Dimensionality Reduction<\/strong>: Project input data into a lower-dimensional feature space.<\/li>\n<\/ul>\n","a6cccfc4":"Before Discussing above three techniques let us go through the basic methodologies used for feature selection.\n\n### 1. Filter Method:\nFilter feature selection methods use statistical techniques to evaluate the relationship **between each input variable and the target variable**, and these scores are used as the basis to choose (filter) those input variables that will be used in the model.\n\nThe statistical measures used in filter-based feature selection are generally calculated one input variable at a time with the target variable. As such, they are referred to as **univariate statistical measures**. This may mean that any interaction between input variables is not considered in the filtering process.\n\n**Note:-** In this case, the existence of correlated predictors makes it possible to select important, but redundant, predictors. The obvious consequences of this issue are that too many predictors are chosen and, as a result, collinearity problems arise. \n\n\n<img src = \"https:\/\/www.analyticsindiamag.com\/wp-content\/uploads\/2019\/04\/filte.jpg\">\n\n\n### 2. Wrapper Method:\nWrapper feature selection methods create many models with different subsets of input features and select those features that result in the best performing model according to a performance metric. These methods are unconcerned with the variable types, although they can be computationally expensive. Recursive Feature Elimination (RFE) is a good example of a wrapper feature selection method.\n\n<img src = \"https:\/\/www.analyticsvidhya.com\/wp-content\/uploads\/2016\/11\/Wrapper_1.png\">\n\n1. __Forward Selection__: Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.\n\n2. __Backward Elimination__: In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.\n\n3. __Recursive Feature elimination__: It is a greedy optimization algorithm which aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination.\n\n### 3. Embedded Method:\nEmbedded methods combine the qualities\u2019 of filter and wrapper methods.\n\n<img src = \"https:\/\/www.analyticsvidhya.com\/wp-content\/uploads\/2016\/11\/Embedded_1.png\">","e176ab72":"### Note: From above correlation plot we can conclude that the feature 'price_range' and 'ram' are highly correlated features which can also be related with the present situation. As ram of your mobile increases price also gets increases. ","89620497":"# Feature Importance\n* You can get the feature importance of each feature of your dataset by using the feature importance property of the model.\n\n* Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable.\n\n* Feature importance is an inbuilt class that comes with Tree Based Classifiers, but here in this example I will be using XGB Classifier for extracting the top 10 features for the dataset.","b4acbb9a":"## Correlation Matrix\n\n* Correlation states how the features are related to each other or the target variable.\n\n* Correlation can be positive (increase in one value of feature increases the value of the target variable) or negative (increase in one value of feature decreases the value of the target variable)\n\n* Heatmap makes it easy to identify which features are most related to the target variable, we will plot heatmap of correlated features using the seaborn library."}}