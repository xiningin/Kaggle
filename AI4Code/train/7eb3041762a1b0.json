{"cell_type":{"fd8f8ded":"code","16578d91":"code","b2b276c2":"code","26c96af3":"code","10e3e833":"code","caf3ada3":"code","996509cb":"code","3af68176":"code","27db4a6e":"code","6070f073":"code","8c33fd1d":"code","c7017875":"code","eec2391b":"markdown"},"source":{"fd8f8ded":"import numpy as np\nimport pandas as pd\nimport os","16578d91":"sub = pd.read_csv('..\/input\/moa-final-inference-gru-b4-save\/submission.csv')\noof = np.load('..\/input\/moa-final-inference-gru-b4-save\/oof_blend.npy')\ntar = np.load('..\/input\/moa-final-inference-gru-b4-save\/oof_targets.npy')","b2b276c2":"pp_threshold = 0.2\npp_factor = 0.0019\ntargets = [col for col in sub.columns if col!='sig_id']","26c96af3":"def mean_logloss(y_pred, y_true):\n    logloss = (1 - y_true) * np.log(1 - y_pred +\n                                    1e-15) + y_true * np.log(y_pred + 1e-15)\n    return np.mean(-logloss)","10e3e833":"mean = np.ones((len(oof), 206)) * pp_threshold\nmean_test = np.ones((len(sub[sub[targets[0]]!=0.0]), 206)) * pp_threshold","caf3ada3":"mean","996509cb":"mean_test","3af68176":"np.round(mean_logloss(oof, tar), 6)","27db4a6e":"diff = oof - mean\nscale = diff * pp_factor\noof += scale\noof = np.clip(oof, 0.0000000000000000001, 0.999999999999999999999999999)","6070f073":"diff_test = sub.loc[sub[targets[0]]!=0.0, targets].values - mean_test\nscale_test = diff_test * pp_factor\nsub.loc[sub[targets[0]]!=0.0, targets] += scale_test\nsub.loc[sub[targets[0]]!=0.0, targets] = np.clip(sub.loc[sub[targets[0]]!=0.0, targets], 0.0000000000000000001, 0.999999999999999999999999999)","8c33fd1d":"np.round(mean_logloss(oof, tar), 6)","c7017875":"sub","eec2391b":"## Strategy:\nThe strategy here is very simple. You pick a threshold somewhere between 0.1 and 0.9, which is determined by the parameter pp_theshold. Then you scale all predictions greater than the threshold towards 1 and all smaller than the threshold towards 0. The degree of scaling is determined by setting the parameter pp_factor.\n\n## Results:\nThis postprocessing is very parameter sensitive and parameter tuning should optimally be done by averaging optimal parameters across multiple folds in order to provide a stable result, but even without doing that this provided us with a 0.0001 boost on local CV and a .00002 boost on private LB. Unfortunately, our solution was overall not good enough for this to be valuable."}}