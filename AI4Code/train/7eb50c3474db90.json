{"cell_type":{"605581a1":"code","0bc8b136":"code","5d82b05c":"code","7323022b":"code","2039ef08":"code","86b87c71":"code","7f96bcc0":"code","a5a17dca":"code","40adef7e":"code","f68105ad":"code","edaa772f":"code","cb842a17":"code","ae71a3da":"code","1641d7c6":"code","bd570709":"code","36818216":"code","cfe36254":"code","98e1753a":"code","f76f1d48":"code","54d4d0bc":"code","589af1cc":"code","514aed32":"code","dc4f379f":"markdown","d3ceb5bf":"markdown","7d59f67a":"markdown","3675bedd":"markdown","74ecdb5c":"markdown","29ad706d":"markdown","b9877749":"markdown","03dd9b33":"markdown","3f91ec24":"markdown","df76568f":"markdown"},"source":{"605581a1":"## imports \n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport random\nimport pathlib\nimport PIL\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential","0bc8b136":"print(tf.__version__)","5d82b05c":"train_dir = pathlib.Path( \"..\/input\/asian-vs-african-elephant-image-classification\/dataset\/train\")\ntest_dir = pathlib.Path(\"..\/input\/asian-vs-african-elephant-image-classification\/dataset\/test\")","7323022b":"list(train_dir.glob('*'))","2039ef08":"train_image_count = len(list(train_dir.glob('*\/*')))\nprint(train_image_count)","86b87c71":"test_image_count = len(list(test_dir.glob('*\/*')))\nprint(test_image_count)","7f96bcc0":"## Class: African elephant \n\nafrican = list(train_dir.glob('African\/*'))\nPIL.Image.open(str(african[0]))","a5a17dca":"## Class: Asian elephant \n\nasian = list(train_dir.glob('Asian\/*'))\nPIL.Image.open(str(asian[0]))","40adef7e":"batch_size = 32\nimg_height = 180\nimg_width = 180","f68105ad":"train_ds = tf.keras.utils.image_dataset_from_directory(\n  train_dir,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","edaa772f":"test_ds = tf.keras.utils.image_dataset_from_directory(\n  test_dir,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","cb842a17":"class_names = train_ds.class_names\nprint(class_names)","ae71a3da":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 15))\nfor images, labels in train_ds.take(1):\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\n    plt.title(class_names[labels[i]])\n    plt.axis(\"off\")","1641d7c6":"for image_batch, labels_batch in train_ds:\n    print(image_batch.shape)\n    print(labels_batch.shape)\n    break","bd570709":"data_augmentation = keras.Sequential(\n  [\n    layers.RandomFlip(\"horizontal\",\n                      input_shape=(img_height,\n                                  img_width,\n                                  3)),\n    layers.RandomCrop(img_height,img_width ),\n    layers.RandomContrast(0.15),\n    layers.RandomRotation(0.1),\n    layers.RandomZoom(0.2),\n  ]\n)","36818216":"plt.figure(figsize=(15, 15))\nfor images, _ in train_ds.take(1):\n  for i in range(9):\n    augmented_images = data_augmentation(images)\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n    plt.axis(\"off\")","cfe36254":"AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = train_ds.cache().shuffle(500).prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)","98e1753a":" \nmodel = Sequential([\n  data_augmentation,\n  layers.Rescaling(1.\/255),\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Dropout(0.2),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(1 , activation = 'sigmoid')\n])","f76f1d48":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=['accuracy'])","54d4d0bc":"model.summary()","589af1cc":"epochs = 40\nhistory = model.fit(\n  train_ds,\n  validation_data=test_ds,\n  epochs=epochs\n)","514aed32":"acc = history.history['accuracy']\ntest_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\ntest_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, test_acc, label='Testing Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Testing Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, test_loss, label='Testing Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Testing Loss')\nplt.show()","dc4f379f":"## Load the dataset ","d3ceb5bf":"## Data Visualizations","7d59f67a":"### Data augmentation\n\n#### when there are a small number of training samples, data augumentation really helps ","3675bedd":"### Configure the dataset for performance\n\n> Dataset.cache keeps the images in memory after they're loaded off disk during the first epoch.  \n\n> Dataset.prefetch overlaps data preprocessing and model execution while training","74ecdb5c":"## Asian vs African Elephants","29ad706d":"## Create the model \n\nhere we will create a simple CNN model ","b9877749":"#### This notebook covers basic EDA on the dataset and a baseline model ","03dd9b33":"## Visualize the dataset ","3f91ec24":"## Dealing with small dataset size \n\n1. #### Data augmentation\n2. #### Adding Droupout ","df76568f":"#### maximum val_acc: 0.7340. "}}