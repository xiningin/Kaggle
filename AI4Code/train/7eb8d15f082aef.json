{"cell_type":{"1b42eb03":"code","fb0d458e":"code","cc29bf1c":"code","e4b6cb5f":"code","f8afc274":"code","4d35ecc5":"code","01addba2":"code","ac42e751":"code","b21e51b3":"code","3e0cf5a9":"code","cbcb5ef8":"code","84d322fd":"code","6bf1e090":"code","5e0f5ad6":"code","dae5b38d":"code","9bb974f1":"code","203dd2ac":"code","e861193e":"code","4ed59655":"code","baeac806":"code","3d7072ed":"code","0a2bc174":"code","22490c59":"code","6aa0635b":"code","90475a9c":"code","cf103742":"code","bc25f3eb":"code","7b35524b":"code","b17e0db0":"code","22bba05e":"code","d0e479c6":"code","2661386f":"code","b6bf5211":"code","5efd3e95":"code","735515d0":"code","3e4f0881":"code","a8788dc0":"code","9a4c32ce":"code","023168f8":"code","bc23c3e4":"code","3db3dff0":"code","22ba1b48":"code","add4f827":"code","e237b196":"code","b8f55da5":"code","a2d06432":"code","f33d8482":"code","1a295e98":"code","095418c7":"code","92016a01":"code","826eddb5":"code","62689ce3":"code","7a6c626a":"code","59a8c09e":"code","b4ad7c2d":"code","b8eacdc2":"code","163c402f":"code","fe70a76b":"code","6396a40e":"code","a778cb9c":"code","dc441f27":"code","fd098451":"code","3b32bb76":"code","66cf32dc":"code","3372d421":"code","6b0c7b27":"code","4f07abd5":"code","0cae2bd0":"code","8089bb14":"code","04367651":"code","a2dcfdfb":"code","9f90ad0d":"code","438f281a":"code","95f553e7":"code","dd921b74":"code","5728191c":"code","5ea89006":"code","2824b9ad":"code","5b334fa3":"code","3466a271":"code","38cd3834":"code","cafcb4e4":"code","ae827e8d":"code","321f4311":"code","3fc0f95b":"markdown","249a9681":"markdown","2a4fb652":"markdown","2be6f96a":"markdown","3b0fb5c6":"markdown","a57b207d":"markdown"},"source":{"1b42eb03":"# Import libraries required for EDA, Data pre-processing\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nimport seaborn as sns\nsns.set(style='white', context='notebook', palette='deep')\n\n# Import all libraries for algorithms\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\n# Import libraries for model evaluation\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import f1_score","fb0d458e":"# Load the train and test datasets\n\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","cc29bf1c":"# Check the dataset to get a hang of it\ndf_train.head()","e4b6cb5f":"# Store PassengerId before dropping it\n\ndf_test_passengers = df_test['PassengerId']\n\n# Drop PassengerId from train and test datasets as it is just a continuous number\n\ndf_train = df_train.drop('PassengerId',axis=1)\ndf_test = df_test.drop('PassengerId',axis=1)","f8afc274":"# Checking all the columns, datatypes\ndf_train.info()","4d35ecc5":"# Function which returns columns having NULLs\ndef missing_vals(dataset):\n    null_columns=dataset.columns[dataset.isnull().any()]\n    print(dataset[null_columns].isnull().sum())","01addba2":"# Check for missing values in columns in train \nmissing_vals(df_train)","ac42e751":"# Check for missing values in columns in test\nmissing_vals(df_test)","b21e51b3":"########## Fill missing values for: Age ##########\nimp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\nimp.fit(df_train[['Age']])\ndf_train['Age'] = imp.transform(df_train[['Age']])\n\ndf_test['Age'] = imp.transform(df_test[['Age']])\n\n# Convert Age to integer\ndata = [df_train, df_test]\n\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)","3e0cf5a9":"########## Fill missing values for: Embarked ##########\n# Check for most frequent value in train dataset\ndf_train.groupby(['Embarked']).size()","cbcb5ef8":"# Replace missing values in train dataset for the column 'Embarked' with the most frequent value of train: 'S'\ndf_train['Embarked'].fillna('S',inplace=True)","84d322fd":"########## Fill missing values for: Fare ##########\n# Replace missing values in test for 'Fare' with 0\ndf_test['Fare'].fillna(0,inplace=True)","6bf1e090":"########## Fill missing values for: Cabin ##########\n# Replace missing values for Cabin with 'U' for 'Unknown'\ndf_train['Cabin'].fillna('U',inplace=True)\ndf_test['Cabin'].fillna('U',inplace=True)","5e0f5ad6":"# Check again if there are any missing values in train\nmissing_vals(df_train)","dae5b38d":"# Check again if there are any missing values in test\nmissing_vals(df_test)","9bb974f1":"#### Analyse columns: SibSp, Parch, Fare\n# Create new column 'FamilySize' which is SibSp + Parch + 1 (1 is added to represent oneself)\n# Drop the columns: SibSp, Parch\n\ndataset = [df_train, df_test]\nfor data in dataset:\n    data['FamilySize'] = ''\n    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n    \ndf_train = df_train.drop(['SibSp', 'Parch'], axis=1)\ndf_test = df_test.drop(['SibSp', 'Parch'], axis=1)        ","203dd2ac":"# Convert Fare to integer\ndata = [df_train, df_test]\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].astype(int)","e861193e":"# Create new column 'FarePP' which is FarePerPerson by dividing Fare by FamilySize to get individual person's fare\ndataset=[df_train, df_test]\nfor data in dataset:\n    data['FarePP'] = ''\n    data['FarePP'] = data['Fare']\/ data['FamilySize']","4ed59655":"# Convert FarePP to integer\ndata = [df_train, df_test]\nfor dataset in data:\n    dataset['FarePP'] = dataset['FarePP'].astype(int)","baeac806":"###### Analyse column: Sex ######\n\nsns.countplot(x=\"Sex\", hue=\"Survived\", data=df_train);","3d7072ed":"# Map 'male' to 0 and 'female' to 1 in both train and test\n# Convert the datatype to int\ndata = [df_train, df_test]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map({'male':0, 'female':1})\n    dataset['Sex'] = dataset['Sex'].astype(int)","0a2bc174":"##### Analyse column: Pclass #####\n\nsns.countplot(x=\"Pclass\", hue=\"Survived\", data=df_train);","22490c59":"##### Analyse column: Embarked #####\n\nsns.countplot(x='Embarked', hue='Survived', data=df_train)","6aa0635b":"# Analyse column: FamilySize\n\nsns.countplot(x='FamilySize', hue='Survived', data=df_train)","90475a9c":"# Create 3 groups of of family and store it in a new column: 'Family'\ndataset = [df_train, df_test]\nfor data in dataset:\n    data['Family'] = ''\n    data.loc[data['FamilySize'] == 1, 'Family'] = 1\n    data.loc[(data['FamilySize'] > 1) & (data['FamilySize'] <= 4), 'Family'] = 2\n    data.loc[data['FamilySize'] > 4, 'Family' ] = 3 ","cf103742":"##### Analyse column: Cabin #####\n\ndf_train['Cabin'].unique()","bc25f3eb":"# Extract Deck from the column Cabin as 'Deck' and drop Cabin\ndf_train['Deck'] = df_train['Cabin'].str.slice(0,1)\ndf_test['Deck'] = df_test['Cabin'].str.slice(0,1)","7b35524b":"# Check how Deck and survival are correlated\nsns.countplot(x='Deck', hue='Survived', data=df_train)","b17e0db0":"# Count number of people per Deck in train\ndf_train.groupby(['Deck']).size()","22bba05e":"# Count number of people per Deck in test\ndf_test.groupby(['Deck']).size()","d0e479c6":"# Replace 'T' with 'U' for Deck in train as there is only one person\ndf_train.loc[df_train['Deck'] == 'T', 'Deck'] = 'U'","2661386f":"# Check again\ndf_train.groupby(['Deck']).size()\n# Now the Decks are same in train and test","b6bf5211":"# Drop the column Cabin\ndf_train = df_train.drop(['Cabin'], axis=1)\ndf_test = df_test.drop(['Cabin'], axis=1)","5efd3e95":"##### Analyse column: Age #####\n\nage_xt = pd.crosstab(df_train['Age'], df_train['Survived'])\nage_xt_pct = age_xt.div(age_xt.sum(1).astype(float), axis=0)\n\nage_xt_pct.plot(kind='bar', \n                  stacked=True, \n                  title='Age & Survival')\nplt.xlabel('Age')\nplt.ylabel('Survival Rate')\nplt.rcParams[\"figure.figsize\"] = (35, 3)","735515d0":"# Binning for the column 'Age' into 4 bins and store in AgeBin\n\ndf_train['AgeBin'] = pd.qcut(df_train['Age'], 4)\ndf_train['AgeBin'].unique()","3e4f0881":"# Binning for the column 'Age' in test dataset by referencing the bins formed in train dataset\nbins = [-0.001, 22.0, 24.0, 35.0, 80.0]\ndf_test['AgeBin'] = pd.cut(df_test['Age'], bins)","a8788dc0":"# Cross check whether the bins formed in train and test are the same\ndf_test['AgeBin'].unique()","9a4c32ce":"# Use LabelEncoder to form codes for the bins formed\nlabel = LabelEncoder()\ndf_train['AgeBinCode'] = label.fit_transform(df_train['AgeBin'])\ndf_test['AgeBinCode'] = label.transform(df_test['AgeBin'])","023168f8":"# Check for the unique age bin codes in train\ndf_train['AgeBinCode'].unique()","bc23c3e4":"# Check for the unique age bin codes in test\ndf_test['AgeBinCode'].unique()","3db3dff0":"# Check how survivors fall under the newly created age bins\nsns.countplot(x='AgeBinCode', hue='Survived', data=df_train)","22ba1b48":"# Drop the column: Age, AgeBin\ndf_train = df_train.drop(['Age', 'AgeBin'], axis=1)\ndf_test = df_test.drop(['Age', 'AgeBin'], axis=1)","add4f827":"##### Analyse column: FarePP (FarePerPerson)#####\n# Sort dataframe by 'FarePP' descending order to see it's relation with 'Ticket'\ndf_train.sort_values(by='FarePP', ascending=False).head(10)\n\n# Those with the same fare have the same Ticket Number. \n# Hence,the column 'Ticket' doesn't seem to give any significant information","e237b196":"# To see how FarePP and Survival are correlated\nfarepp_xt = pd.crosstab(df_train['FarePP'], df_train['Survived'])\nfarepp_xt_pct = farepp_xt.div(farepp_xt.sum(1).astype(float), axis=0)\n\nfarepp_xt_pct.plot(kind='bar', \n                  stacked=True, \n                  title='FarePP & Survival')\nplt.xlabel('FarePP')\nplt.ylabel('Survival Rate')\nplt.rcParams[\"figure.figsize\"] = (28, 5)","b8f55da5":"# Binning for FarePP \ndf_train['FarePPBin'] = pd.qcut(df_train['FarePP'], 4)","a2d06432":"# Check for unique bins created in train dataset\ndf_train['FarePPBin'].unique()","f33d8482":"# Binning for the column 'FarePP' in test dataset by referencing the bins formed in train dataset\nbins = [-0.001, 7.0, 8.0, 23.0, 512.0]\ndf_test['FarePPBin'] = pd.cut(df_test['FarePP'], bins)","1a295e98":"# Check for unique bins created in test to see train and test bins are in sync\ndf_test['FarePPBin'].unique()","095418c7":"# Create fareBinCode using LabelEncoder\nfarelabel = LabelEncoder()\ndf_train['FareBinCode'] = farelabel.fit_transform(df_train['FarePPBin'])\ndf_test['FareBinCode'] = farelabel.transform(df_test['FarePPBin'])","92016a01":"# Crosscheck train and test for FareBinCodes created\ndf_train['FareBinCode'].unique()","826eddb5":"# Crosscheck train and test for FareBinCodes created\ndf_test['FareBinCode'].unique()","62689ce3":"# Check how survivors are falling under the newly created FareBinCodes\nsns.countplot(x='FareBinCode', hue='Survived', data=df_train)","7a6c626a":"# Drop columns: Fare, FamilySize, FarePPBin, FarePP\ndf_train = df_train.drop(['Fare', 'FamilySize', 'FarePPBin', 'FarePP'], axis=1)\ndf_test = df_test.drop(['Fare', 'FamilySize', 'FarePPBin', 'FarePP'], axis=1)","59a8c09e":"###### Analyse column: Ticket ######\ndf_train['Ticket'].unique()","b4ad7c2d":"# Drop the column Ticket as it is not having any significant information\ndf_train = df_train.drop('Ticket', axis=1)\ndf_test = df_test.drop('Ticket', axis=1)","b8eacdc2":" ##### Analyse the column: Name #####\n# Create a new column: Title\ndf_train['Title'] = ''\ndf_test['Title'] = ''\n\n# Extract titles into this column\ndf_train['Title'] = df_train.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ndf_test['Title'] = df_test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)","163c402f":"# Check for NULLs\nprint(df_train['Title'].isnull().sum())\nprint(df_test['Title'].isnull().sum())","fe70a76b":"# Check for NULLs in test\nprint(df_test['Title'].isnull().sum())","6396a40e":"# Check how Title and Survival are correlated\n\ntitle_xt = pd.crosstab(df_train['Title'], df_train['Survived'])\ntitle_xt_pct = title_xt.div(title_xt.sum(1).astype(float), axis=0)\n\ntitle_xt_pct.plot(kind='bar', \n                  stacked=True, \n                  title='Survival Rate by title')\nplt.xlabel('Title')\nplt.ylabel('Survival Rate')\nplt.rcParams[\"figure.figsize\"] = (15,2)","a778cb9c":"# Mapping of each title to specific groups\ndata = [df_train, df_test]\n\nfor dataset in data:\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Mrs')      \n    dataset['Title'] = dataset['Title'].replace('Mrs', 'Mrs')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')     \n    dataset['Title'] = dataset['Title'].replace('Miss', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Master', 'Master')\n    dataset['Title'] = dataset['Title'].replace('Mr', 'Mr')\n    dataset['Title'] = dataset['Title'].replace('Capt', 'Officer')\n    dataset['Title'] = dataset['Title'].replace('Major', 'Officer')\n    dataset['Title'] = dataset['Title'].replace('Dr', 'Officer')\n    dataset['Title'] = dataset['Title'].replace('Col', 'Officer')\n    dataset['Title'] = dataset['Title'].replace('Rev', 'Officer') \n    dataset['Title'] = dataset['Title'].replace('Jonkheer', 'Royalty')    \n    dataset['Title'] = dataset['Title'].replace('Don', 'Royalty')\n    dataset['Title'] = dataset['Title'].replace('Dona', 'Royalty')\n    dataset['Title'] = dataset['Title'].replace('Countess', 'Royalty')\n    dataset['Title'] = dataset['Title'].replace('Lady', 'Royalty')\n    dataset['Title'] = dataset['Title'].replace('Sir', 'Royalty')  ","dc441f27":"# Cross check train and test for the newly created column Title\ndf_train['Title'].unique()","fd098451":"# Cross check train and test for the newly created column Title\ndf_test['Title'].unique()","3b32bb76":"# Check the Title column again after the mapping\n\ntitle_xt = pd.crosstab(df_train['Title'], df_train['Survived'])\ntitle_xt_pct = title_xt.div(title_xt.sum(1).astype(float), axis=0)\n\ntitle_xt_pct.plot(kind='bar', \n                  stacked=True, \n                  title='Survival Rate by title')\nplt.xlabel('Title')\nplt.ylabel('Survival Rate')\nplt.rcParams[\"figure.figsize\"] = (15,2)","66cf32dc":"# Drop the column: Name\ndf_train = df_train.drop(['Name'], axis=1)\ndf_test = df_test.drop(['Name'], axis=1)","3372d421":"# Rename the column 'Survived' to 'Target'\ndf_train = df_train.rename(columns={'Survived': 'Target'})\n\n####### One hot code: Embarked #######\nembarked_one_hot = pd.get_dummies(df_train['Embarked'], prefix='Embarked')\nembarked_one_hot_test = pd.get_dummies(df_test['Embarked'], prefix='Embarked')\n\ndf_train = df_train.join(embarked_one_hot)\ndf_test = df_test.join(embarked_one_hot_test)\n\n# Drop the column Embarked\ndf_train = df_train.drop(['Embarked'], axis=1)\ndf_test = df_test.drop(['Embarked'], axis=1)\n\n####### One hot code: Title #######\ntitle_one_hot = pd.get_dummies(df_train['Title'], prefix='Title')\ntitle_one_hot_test = pd.get_dummies(df_test['Title'], prefix='Title')\n\ndf_train = df_train.join(title_one_hot)\ndf_test = df_test.join(title_one_hot_test)\n\n# Drop the column Title\ndf_train = df_train.drop(['Title'], axis=1)\ndf_test = df_test.drop(['Title'], axis=1)\n\n####### One hot code: Pclass #######\npclass_one_hot = pd.get_dummies(df_train['Pclass'], prefix='Pclass')\npclass_one_hot_test = pd.get_dummies(df_test['Pclass'], prefix='Pclass')\n\ndf_train = df_train.join(pclass_one_hot)\ndf_test = df_test.join(pclass_one_hot_test)\n\n# Drop the column: Pclass\ndf_train = df_train.drop(['Pclass'], axis=1)\ndf_test = df_test.drop(['Pclass'], axis=1)\n\n####### One hot code: Deck #######\ndeck_one_hot = pd.get_dummies(df_train['Deck'], prefix='Deck')\ndeck_one_hot_test = pd.get_dummies(df_test['Deck'], prefix='Deck')\n\ndf_train = df_train.join(deck_one_hot)\ndf_test = df_test.join(deck_one_hot_test)\n\n# Drop the column: Deck\ndf_train = df_train.drop(['Deck'], axis=1)\ndf_test = df_test.drop(['Deck'], axis=1)\n\n####### One hot code: Family #######\nfamily_one_hot = pd.get_dummies(df_train['Family'], prefix='Family')\nfamily_one_hot_test = pd.get_dummies(df_test['Family'], prefix='Family')\n\ndf_train = df_train.join(family_one_hot)\ndf_test = df_test.join(family_one_hot_test)\n\n# Drop the column: Family\ndf_train = df_train.drop(['Family'], axis=1)\ndf_test = df_test.drop(['Family'], axis=1)\n\n####### One hot code: AgeBinCode #######\nage_one_hot = pd.get_dummies(df_train['AgeBinCode'], prefix='AgeBinCode')\nage_one_hot_test = pd.get_dummies(df_test['AgeBinCode'], prefix='AgeBinCode')\n\ndf_train = df_train.join(age_one_hot)\ndf_test = df_test.join(age_one_hot_test)\n\n# Drop the column: AgeBinCode\ndf_train = df_train.drop(['AgeBinCode'], axis=1)\ndf_test = df_test.drop(['AgeBinCode'], axis=1)\n\n####### One hot code: FareBinCode #######\nfare_one_hot = pd.get_dummies(df_train['FareBinCode'], prefix='FareBinCode')\nfare_one_hot_test = pd.get_dummies(df_test['FareBinCode'], prefix='FareBinCode')\n\ndf_train = df_train.join(fare_one_hot)\ndf_test = df_test.join(fare_one_hot_test)\n\n# Drop the column: FareBinCode\ndf_train = df_train.drop(['FareBinCode'], axis=1)\ndf_test = df_test.drop(['FareBinCode'], axis=1)","6b0c7b27":"#### Check all columns once again in train and test ####\ndf_train.head(5)","4f07abd5":"df_test.head(5)","0cae2bd0":"##################### X & y creation #####################\n\ny = df_train['Target']\nX = df_train.drop('Target', axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","8089bb14":"### Cross validate various models ###\n\nnum_folds = 5\nseed = 2\nscoring = 'accuracy'\nmodels = []\nscores = []\nnames = []\n\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('SVC', SVC()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('GBC', GradientBoostingClassifier()))\nmodels.append(('DT', DecisionTreeClassifier()))\nmodels.append(('RF', RandomForestClassifier()))\n\nfor name, model in models:\n    kfold = StratifiedKFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    scores.append(cv_results)\n    names.append(name)\n    msg = \"%s %f %f \" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n    \n# LogisticRegression, GradientBoostingClassifier, and RandomForestClassifier are selected    ","04367651":"##### Logistic Regression with GridSearch #####\n\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {'C': c_space}\n\nLR = LogisticRegression()\n\n# Instantiate the GridSearchCV object: logreg_cv\nlr_gs = GridSearchCV(LR, param_grid=param_grid, cv=5)\n\n# Fit model to the data\nlr_gs.fit(X_train, y_train)\n\nparameters = lr_gs.best_params_\n\n# Print the tuned parameters and score\nprint(\"Tuned Logistic Regression Parameters: {}\".format(lr_gs.best_params_)) \nprint(\"Best score is {}\".format(lr_gs.best_score_))","a2dcfdfb":"y_pred_lr = lr_gs.predict(X_test)\naccuracy_lr = accuracy_score(y_test, y_pred_lr)\nprint(accuracy_lr)","9f90ad0d":"##### Random Forest with Grid Search #####\n\nRF = RandomForestClassifier()\nparam_grid = {\n                 'max_depth' : [4, 6, 8],\n                 'n_estimators': [50, 100],\n                 'min_samples_split': [2, 3, 10],\n                 'min_samples_leaf': [1, 3, 10],\n                 'criterion': ['gini', 'entropy']\n                 }\nrf_gs = GridSearchCV(RF,\n                           scoring='accuracy',\n                           param_grid=param_grid,\n                           cv=5,\n                           verbose=1\n                           )\n\nrf_gs.fit(X_train, y_train)\n\nparameters = rf_gs.best_params_\n\nprint('Best score: {}'.format(rf_gs.best_score_))\nprint('Best parameters: {}'.format(rf_gs.best_params_))","438f281a":"y_pred_rf = rf_gs.predict(X_test)\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\nprint(accuracy_rf)","95f553e7":"##### Gradient Boost Classifier #####\n\nGBC = GradientBoostingClassifier(n_estimators=100)\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngbc_gs = GridSearchCV(GBC, param_grid = gb_param_grid, cv=kfold, scoring='accuracy', n_jobs= 4, verbose = 1)\n","dd921b74":"gbc_gs.fit(X_train, y_train)","5728191c":"y_pred_gbc = gbc_gs.predict(X_test)\naccuracy_gbc = accuracy_score(y_test, y_pred_gbc)\nprint(accuracy_gbc)","5ea89006":"############## Model evaluation ##############","2824b9ad":"confusion_matrix(y_test, y_pred_lr)","5b334fa3":"confusion_matrix(y_test, y_pred_rf)","3466a271":"confusion_matrix(y_test, y_pred_gbc)","38cd3834":"# Scores of Logistic Regression\n\nprint(\"Precision:\", precision_score(y_test, y_pred_lr))\nprint(\"Recall:\",recall_score(y_test, y_pred_lr))\nprint(\"F1 Score:\", f1_score(y_test, y_pred_lr))","cafcb4e4":"# Scores of GBC\n\nprint(\"Precision:\", precision_score(y_test, y_pred_gbc))\nprint(\"Recall:\",recall_score(y_test, y_pred_gbc))\nprint(\"F1 Score:\", f1_score(y_test, y_pred_gbc))","ae827e8d":"# Score of RF\n\nprint(\"Precision:\", precision_score(y_test, y_pred_rf))\nprint(\"Recall:\",recall_score(y_test, y_pred_rf))\nprint(\"F1 Score:\", f1_score(y_test, y_pred_rf))","321f4311":"##################### Final Prediction using GBC #####################\n\ny_pred_final = gbc_gs.predict(df_test)\n\nfinal_submission = pd.DataFrame({\n        \"PassengerId\": df_test_passengers,\n        \"Survived\": y_pred_final\n    })\n\nfilename = 'titanic_submission.csv'\nfinal_submission.to_csv(filename, sep=',', index = False)","3fc0f95b":"### Find and fill missing values","249a9681":"# EDA and Data-Preprocessing","2a4fb652":"### Analyse each column ","2be6f96a":"### Renaming and one hot encoding","3b0fb5c6":"### Model building","a57b207d":"### Train and test data creation "}}