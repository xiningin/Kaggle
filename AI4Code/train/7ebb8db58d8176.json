{"cell_type":{"541ac964":"code","2f6fa640":"code","1a2c5df7":"code","1b505a17":"code","17dfd015":"code","4e85479f":"code","693b5105":"code","796dde3a":"code","14549fd0":"code","d21a6827":"code","2f216457":"code","ccf1d1fb":"code","14000e7d":"code","12e991c2":"code","95506a9c":"code","e0cebc14":"code","0d7f92a4":"code","b5e54872":"markdown","f5f4b8fb":"markdown","34c31a2f":"markdown"},"source":{"541ac964":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","2f6fa640":"import pathlib\nimport random\nimport string\nimport re\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.layers.experimental.preprocessing import TextVectorization","1a2c5df7":"text  = '\/kaggle\/input\/translationtren\/tr-en_cleared.txt'\ntrain = '\/kaggle\/input\/preentr\/train_pairs.txt'\nval = '\/kaggle\/input\/preentr\/val_pairs.txt'\ntest = '\/kaggle\/input\/preentr\/test_pairs.txt'","1b505a17":"with open(text, encoding=\"utf8\") as f:\n    lines = f.read().split(\"\\n\")[:-1]\ntext_pairs = []\nfor line in lines:\n    eng, tr = line.split(\"\\t\")\n    tr = \"[start] \" + tr + \" [end]\"\n    text_pairs.append((eng, tr))\n\nwith open(train, encoding=\"utf8\") as f:\n    lines = f.read().split(\"\\n\")[:-1]\ntrain_pairs = []\nfor line in lines:\n    eng, tr = line.split(\"\\t\")\n    tr = \"[start] \" + tr + \" [end]\"\n    train_pairs.append((eng, tr))\n\nwith open(val, encoding=\"utf8\") as f:\n    lines = f.read().split(\"\\n\")[:-1]\nval_pairs = []\nfor line in lines:\n    eng, tr = line.split(\"\\t\")\n    tr = \"[start] \" + tr + \" [end]\"\n    val_pairs.append((eng, tr))\n\nwith open(test, encoding=\"utf8\") as f:\n    lines = f.read().split(\"\\n\")[:-1]\ntest_pairs = []\nfor line in lines:\n    eng, tr = line.split(\"\\t\")\n    tr = \"[start] \" + tr + \" [end]\"\n    test_pairs.append((eng, tr))","17dfd015":"for _ in range(5):\n    print(random.choice(text_pairs))","4e85479f":"print(f\"{len(text_pairs)} total pairs\")\nprint(f\"{len(train_pairs)} training pairs\")\nprint(f\"{len(val_pairs)} validation pairs\")\nprint(f\"{len(test_pairs)} test pairs\")","693b5105":"strip_chars = string.punctuation\nstrip_chars = strip_chars.replace(\"[\", \"\")\nstrip_chars = strip_chars.replace(\"]\", \"\")\n\nvocab_size = 32000\nsequence_length = 20\nbatch_size = 64\n\n\ndef custom_standardization(input_string):\n    lowercase = tf.strings.lower(input_string)\n    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n\n\neng_vectorization = TextVectorization(\n    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n)\ntr_vectorization = TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length + 1,\n    standardize=custom_standardization,\n)\ntrain_eng_texts = [pair[0] for pair in train_pairs]\ntrain_tr_texts = [pair[1] for pair in train_pairs]\neng_vectorization.adapt(train_eng_texts)\ntr_vectorization.adapt(train_tr_texts)","796dde3a":"def format_dataset(eng, tr):\n    eng = eng_vectorization(eng)\n    tr = tr_vectorization(tr)\n    return ({\"encoder_inputs\": eng, \"decoder_inputs\": tr[:, :-1],}, tr[:, 1:])\n\n\ndef make_dataset(pairs):\n    eng_texts, tr_texts = zip(*pairs)\n    eng_texts = list(eng_texts)\n    tr_texts = list(tr_texts)\n    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, tr_texts))\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(format_dataset)\n    return dataset.prefetch(16).cache()\n\n\ntrain_ds = make_dataset(train_pairs)\nval_ds = make_dataset(val_pairs)","14549fd0":"for inputs, targets in train_ds.take(1):\n    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n    print(f\"targets.shape: {targets.shape}\")","d21a6827":"class TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super(TransformerEncoder, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.dense_proj = keras.Sequential(\n            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.supports_masking = True\n\n    def call(self, inputs, mask=None):\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n        attention_output = self.attention(\n            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n        )\n        proj_input = self.layernorm_1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)\n     \n    def get_config(self):\n        config = super(TransformerEncoder, self).get_config()\n        config.update({\n           # 'attention':self.attention,\n           # 'dense_proj': self.dense_proj,\n            'embed_dim': self.embed_dim,\n            'dense_dim': self.dense_dim,\n            'num_heads': self.num_heads\n           # 'layernorm_1':self.layernorm_1,\n           # 'layernorm_2':self.layernorm_2, \n           # 'supports_masking':self.supports_masking\n            \n        })\n        return config\n\nclass PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super(PositionalEmbedding, self).__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n\n    def get_config(self):\n        config = super(PositionalEmbedding, self).get_config()\n        config.update({\n           # 'token_embeddings': self.token_embeddings,\n           # 'position_embeddings': self.position_embeddings,\n            'sequence_length': self.sequence_length,\n            'vocab_size':self.vocab_size,\n            'embed_dim': self.embed_dim\n            \n        })\n        return config\n\n\nclass TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n        super(TransformerDecoder, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.latent_dim = latent_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.attention_2 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.dense_proj = keras.Sequential(\n            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.layernorm_3 = layers.LayerNormalization()\n        self.supports_masking = True\n\n    def call(self, inputs, encoder_outputs, mask=None):\n        causal_mask = self.get_causal_attention_mask(inputs)\n        if mask is not None:\n            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n            padding_mask = tf.minimum(padding_mask, causal_mask)\n\n        attention_output_1 = self.attention_1(\n            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n        )\n        out_1 = self.layernorm_1(inputs + attention_output_1)\n\n        attention_output_2 = self.attention_2(\n            query=out_1,\n            value=encoder_outputs,\n            key=encoder_outputs,\n            attention_mask=padding_mask,\n        )\n        out_2 = self.layernorm_2(out_1 + attention_output_2)\n\n        proj_output = self.dense_proj(out_2)\n        return self.layernorm_3(out_2 + proj_output)\n\n    def get_causal_attention_mask(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size, sequence_length = input_shape[0], input_shape[1]\n        i = tf.range(sequence_length)[:, tf.newaxis]\n        j = tf.range(sequence_length)\n        mask = tf.cast(i >= j, dtype=\"int32\")\n        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n            axis=0,\n        )\n        return tf.tile(mask, mult)\n\n    def get_config(self):\n        config = super(TransformerDecoder, self).get_config()\n        config.update({\n            'embed_dim': self.embed_dim,\n            'latent_dim': self.latent_dim,\n            'num_heads': self.num_heads\n           # 'attention_1':self.attention_1,\n           # 'attention_2': self.attention_2,\n           # 'dense_proj':self.dense_proj,\n           # 'layernorm_1':self.layernorm_1, \n           # 'layernorm_2':self.layernorm_2,\n           # 'layernorm_3':self.layernorm_3,\n           # 'supports_masking':self.supports_masking\n        })\n        return config","2f216457":"embed_dim = 256\nlatent_dim = 2048\nnum_heads = 8\n\nencoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\nencoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\nencoder = keras.Model(encoder_inputs, encoder_outputs)\n\ndecoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\nencoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\nx = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\nx = layers.Dropout(0.5)(x)\ndecoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\ndecoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n\ndecoder_outputs = decoder([decoder_inputs, encoder_outputs])\ntransformer = keras.Model(\n    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n)","ccf1d1fb":"my_model = tf.keras.models.load_model('\/kaggle\/input\/keras-model-33-epoch\/keras_model_33_epoch.h5',\n                                      custom_objects={'TransformerEncoder': TransformerEncoder,\n                                                      'PositionalEmbedding':PositionalEmbedding,\n                                                      'TransformerDecoder':TransformerDecoder})","14000e7d":"epochs = 25","12e991c2":"my_model.fit(train_ds, epochs=epochs, validation_data=val_ds)","95506a9c":"my_model.save('.\/keras_model_25_epoch.h5')","e0cebc14":"my_model.fit(train_ds, epochs=epochs, validation_data=val_ds)","0d7f92a4":"tr_vocab = tr_vectorization.get_vocabulary()\ntr_index_lookup = dict(zip(range(len(tr_vocab)), tr_vocab))\nmax_decoded_sentence_length = 20\n\n\ndef decode_sequence(input_sentence):\n    tokenized_input_sentence = eng_vectorization([input_sentence])\n    decoded_sentence = \"[start]\"\n    for i in range(max_decoded_sentence_length):\n        tokenized_target_sentence = tr_vectorization([decoded_sentence])[:, :-1]\n        predictions = my_model([tokenized_input_sentence, tokenized_target_sentence])\n\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = tr_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n\n        if sampled_token == \"[end]\":\n            break\n    return decoded_sentence\n\n\ntest_eng_texts = [pair[0] for pair in test_pairs]\nfor a in range(30):\n    input_sentence = test_eng_texts[a]\n    translated = decode_sequence(input_sentence)\n    print(input_sentence,':',translated)","b5e54872":"<a href=\".\/keras_model_25_epoch.h5\"> Download File <\/a>","f5f4b8fb":"# Importing Libraries","34c31a2f":"# Model"}}