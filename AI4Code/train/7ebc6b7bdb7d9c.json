{"cell_type":{"b63eb457":"code","1d59f946":"code","6870e12a":"code","8dbafaf9":"code","73379aab":"code","512934e6":"code","26bd2c2f":"code","d019938b":"code","c9ea4a63":"code","2f90b897":"code","0837788c":"code","95b7aa22":"code","52ca3097":"code","7c36ad25":"code","016a0649":"code","18d8b460":"code","44ded28f":"code","0cc66334":"code","528b8621":"code","7f1b83f5":"code","a4d7a2f3":"code","25c22992":"code","056d1213":"code","49041763":"code","cf2595d8":"code","26c0e47b":"code","ae110d1d":"code","f50b1775":"code","68d631b9":"code","1d9c1a65":"code","acbe47fd":"code","a9157148":"code","a9f29d2e":"code","f8f9eceb":"code","b9cb549b":"code","02d06404":"code","1f143691":"code","a8d0c711":"code","03cb92f6":"code","58ceedcc":"code","16b0d255":"code","d872d338":"code","02583795":"code","8b6b6229":"code","ae929924":"code","0c2cbd88":"code","a685770d":"code","1c052d8a":"code","0e89eef4":"code","5111977a":"code","bacaac82":"code","aad9c6cd":"code","5f96ab79":"code","fe0bf2df":"code","66181dfc":"code","432aa2b7":"code","b2e23d4c":"code","51294547":"code","aba84a63":"code","0e69db5d":"code","c68236d8":"code","608673d0":"code","d2ddd885":"code","9afaeed6":"code","53aae1bd":"markdown","97bc9589":"markdown","f5a269f5":"markdown","7e59f73a":"markdown","669ebf61":"markdown","bb967463":"markdown","43dc1c5f":"markdown","029cdeff":"markdown","46afa135":"markdown","cae46460":"markdown","347411d5":"markdown","97144337":"markdown","509ac039":"markdown","f040aeae":"markdown","72b53039":"markdown"},"source":{"b63eb457":"import numpy as np\nimport pandas as pd\nimport glob\nimport math\n\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.feature_selection import RFE\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\nimport optuna\nfrom optuna.samplers import TPESampler\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Input, Dense\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras.callbacks import EarlyStopping","1d59f946":"pd.set_option('display.max_columns', None)","6870e12a":"train = pd.read_csv(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/train.csv\")\nsample_submission = pd.read_csv(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/sample_submission.csv\")","8dbafaf9":"train","73379aab":"fig = px.histogram(\n    train, \n    x=\"time_to_eruption\",\n    width=800,\n    height=500,\n    nbins=100,\n    title='Time to eruption distribution'\n)\n\nfig.show()","512934e6":"fig = px.line(\n    train, \n    y=\"time_to_eruption\",\n    width=800,\n    height=500,\n    title='Time to eruption for all volcanos'\n)\n\nfig.show()","26bd2c2f":"train['time_to_eruption'].describe()","d019938b":"print('Median:', train['time_to_eruption'].median())\nprint('Skew:', train['time_to_eruption'].skew())\nprint('Std:', train['time_to_eruption'].std())\nprint('Kurtosis:', train['time_to_eruption'].kurtosis())\nprint('Mean:', train['time_to_eruption'].mean())","c9ea4a63":"sample_submission","2f90b897":"train_frags = glob.glob(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/train\/*\")\nlen(train_frags)","0837788c":"test_frags = glob.glob(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/test\/*\")\nlen(test_frags)","95b7aa22":"train_frags[0]","52ca3097":"check = pd.read_csv('..\/input\/predict-volcanic-eruptions-ingv-oe\/train\/2037160701.csv')\ncheck","7c36ad25":"sensors = set()\nobservations = set()\nnan_columns = list()\nmissed_groups = list()\nfor_df = list()\n\nfor item in train_frags:\n    name = int(item.split('.')[-2].split('\/')[-1])\n    at_least_one_missed = 0\n    frag = pd.read_csv(item)\n    missed_group = list()\n    missed_percents = list()\n    for col in frag.columns:\n        missed_percents.append(frag[col].isnull().sum() \/ len(frag))\n        if pd.isnull(frag[col]).all() == True:\n            at_least_one_missed = 1\n            nan_columns.append(col)\n            missed_group.append(col)\n    if len(missed_group) > 0:\n        missed_groups.append(missed_group)\n    sensors.add(len(frag.columns))\n    observations.add(len(frag))\n    for_df.append([name, at_least_one_missed] + missed_percents)","016a0649":"print('Unique number of sensors: ', sensors)\nprint('Unique number of observations: ', observations)","18d8b460":"print('Number of totaly missed sensors:', len(nan_columns))\n\nabsent_sensors = dict()\n\nfor item in nan_columns:\n    if item in absent_sensors:\n        absent_sensors[item] += 1\n    else:\n        absent_sensors[item] = 0","44ded28f":"absent_df = pd.DataFrame(absent_sensors.items(), columns=['Sensor', 'Missed sensors'])\n\nfig = px.bar(\n    absent_df, \n    x=\"Sensor\",\n    y='Missed sensors',\n    width=800,\n    height=500,\n    title='Number of missed sensors in training dataset'\n)\n\nfig.show()","0cc66334":"absent_groups = dict()\n\nfor item in missed_groups:\n    if str(item) in absent_groups:\n        absent_groups[str(item)] += 1\n    else:\n        absent_groups[str(item)] = 0","528b8621":"absent_df = pd.DataFrame(absent_groups.items(), columns=['Group', 'Missed number'])\nabsent_df = absent_df.sort_values('Missed number')\n\nfig = px.bar(\n    absent_df, \n    y=\"Group\",\n    x='Missed number',\n    orientation='h',\n    width=800,\n    height=600,\n    title='Number of missed sensor groups in training dataset'\n)\n\nfig.show()","7f1b83f5":"for_df = pd.DataFrame(\n    for_df, \n    columns=[\n        'segment_id', 'has_missed_sensors', 'missed_percent_sensor1', \n        'missed_percent_sensor2', 'missed_percent_sensor3', 'missed_percent_sensor4', \n        'missed_percent_sensor5', 'missed_percent_sensor6', 'missed_percent_sensor7', \n        'missed_percent_sensor8', 'missed_percent_sensor9', 'missed_percent_sensor10'\n    ]\n)\n\nfor_df","a4d7a2f3":"train = pd.merge(train, for_df)\ntrain","25c22992":"fig = make_subplots(rows=1, cols=2)\ntraces = [\n    go.Histogram(\n        x=train[train['has_missed_sensors']==1]['time_to_eruption'], \n        nbinsx=100, \n        name='Has missed sensors'\n    ),\n    go.Histogram(\n        x=train[train['has_missed_sensors']==0]['time_to_eruption'], \n        nbinsx=100, \n        name=\"Doesn't have missed sensors\"\n    )\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i \/\/ 2) + 1, \n        (i % 2) + 1\n    )\n\nfig.update_layout(\n    title_text='Time to erruption distribution for segments with \/ without missed sensors',\n    height=600,\n    width=1000\n)\nfig.show()","056d1213":"sensors = set()\nobservations = set()\nnan_columns = list()\nmissed_groups = list()\nfor_test_df = list()\n\nfor item in test_frags:\n    name = int(item.split('.')[-2].split('\/')[-1])\n    at_least_one_missed = 0\n    frag = pd.read_csv(item)\n    missed_group = list()\n    missed_percents = list()\n    for col in frag.columns:\n        missed_percents.append(frag[col].isnull().sum() \/ len(frag))\n        if pd.isnull(frag[col]).all() == True:\n            at_least_one_missed = 1\n            nan_columns.append(col)\n            missed_group.append(col)\n    if len(missed_group) > 0:\n        missed_groups.append(missed_group)\n    sensors.add(len(frag.columns))\n    observations.add(len(frag))\n    for_test_df.append([name, at_least_one_missed] + missed_percents)","49041763":"for_test_df = pd.DataFrame(\n    for_test_df, \n    columns=[\n        'segment_id', 'has_missed_sensors', 'missed_percent_sensor1', 'missed_percent_sensor2', 'missed_percent_sensor3', \n        'missed_percent_sensor4', 'missed_percent_sensor5', 'missed_percent_sensor6', 'missed_percent_sensor7', \n        'missed_percent_sensor8', 'missed_percent_sensor9', 'missed_percent_sensor10'\n    ]\n)\n\nfor_test_df","cf2595d8":"print('Unique number of sensors: ', sensors)\nprint('Unique number of observations: ', observations)","26c0e47b":"print('Number of totaly missed sensors:', len(nan_columns))\n\nabsent_sensors = dict()\n\nfor item in nan_columns:\n    if item in absent_sensors:\n        absent_sensors[item] += 1\n    else:\n        absent_sensors[item] = 0","ae110d1d":"absent_df = pd.DataFrame(absent_sensors.items(), columns=['Sensor', 'Missed sensors'])\n\nfig = px.bar(\n    absent_df, \n    x=\"Sensor\",\n    y='Missed sensors',\n    width=800,\n    height=500,\n    title='Number of missed sensors in test dataset'\n)\n\nfig.show()","f50b1775":"absent_groups = dict()\n\nfor item in missed_groups:\n    if str(item) in absent_groups:\n        absent_groups[str(item)] += 1\n    else:\n        absent_groups[str(item)] = 0","68d631b9":"absent_df = pd.DataFrame(absent_groups.items(), columns=['Group', 'Missed number'])\nabsent_df = absent_df.sort_values('Missed number')\n\nfig = px.bar(\n    absent_df, \n    y=\"Group\",\n    x='Missed number',\n    orientation='h',\n    width=800,\n    height=600,\n    title='Number of missed sensor groups in test dataset'\n)\n\nfig.show()","1d9c1a65":"fig = make_subplots(rows=5, cols=2)\ntraces = [\n    go.Histogram(\n        x=check[col], \n        nbinsx=100, \n        name=col\n    ) for col in check.columns\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i \/\/ 2) + 1, \n        (i % 2) + 1\n    )\n\nfig.update_layout(\n    title_text='Data from sensors distribution',\n    height=800,\n    width=1200\n)\n\nfig.show()","acbe47fd":"fig = make_subplots(rows=5, cols=2)\ntraces = [\n    go.Scatter(\n        x=[i for i in range(60002)], \n        y=check[col], \n        mode='lines', \n        name=col\n    ) for col in check.columns\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i \/\/ 2) + 1, \n        (i % 2) + 1\n    )\n\nfig.update_layout(\n    title_text='Data from sensors',\n    height=800,\n    width=1200\n)\n\nfig.show()","a9157148":"def build_features(signal, ts, sensor_id):\n    X = pd.DataFrame()\n    f = np.fft.fft(signal)\n    f_real = np.real(f)\n    X.loc[ts, f'{sensor_id}_sum']       = signal.sum()\n    X.loc[ts, f'{sensor_id}_mean']      = signal.mean()\n    X.loc[ts, f'{sensor_id}_std']       = signal.std()\n    X.loc[ts, f'{sensor_id}_var']       = signal.var() \n    X.loc[ts, f'{sensor_id}_max']       = signal.max()\n    X.loc[ts, f'{sensor_id}_min']       = signal.min()\n    X.loc[ts, f'{sensor_id}_skew']      = signal.skew()\n    X.loc[ts, f'{sensor_id}_mad']       = signal.mad()\n    X.loc[ts, f'{sensor_id}_kurtosis']  = signal.kurtosis()\n    X.loc[ts, f'{sensor_id}_quantile99']= np.quantile(signal, 0.99)\n    X.loc[ts, f'{sensor_id}_quantile95']= np.quantile(signal, 0.95)\n    X.loc[ts, f'{sensor_id}_quantile85']= np.quantile(signal, 0.85)\n    X.loc[ts, f'{sensor_id}_quantile75']= np.quantile(signal, 0.75)\n    X.loc[ts, f'{sensor_id}_quantile55']= np.quantile(signal, 0.55)\n    X.loc[ts, f'{sensor_id}_quantile45']= np.quantile(signal, 0.45) \n    X.loc[ts, f'{sensor_id}_quantile25']= np.quantile(signal, 0.25) \n    X.loc[ts, f'{sensor_id}_quantile15']= np.quantile(signal, 0.15) \n    X.loc[ts, f'{sensor_id}_quantile05']= np.quantile(signal, 0.05)\n    X.loc[ts, f'{sensor_id}_quantile01']= np.quantile(signal, 0.01)\n    X.loc[ts, f'{sensor_id}_fft_real_mean']= f_real.mean()\n    X.loc[ts, f'{sensor_id}_fft_real_std'] = f_real.std()\n    X.loc[ts, f'{sensor_id}_fft_real_max'] = f_real.max()\n    X.loc[ts, f'{sensor_id}_fft_real_min'] = f_real.min()\n\n    return X","a9f29d2e":"train_set = list()\nj=0\nfor seg in train.segment_id:\n    signals = pd.read_csv(f'\/kaggle\/input\/predict-volcanic-eruptions-ingv-oe\/train\/{seg}.csv')\n    train_row = []\n    if j%500 == 0:\n        print(j)\n    for i in range(0, 10):\n        sensor_id = f'sensor_{i+1}'\n        train_row.append(build_features(signals[sensor_id].fillna(0), seg, sensor_id))\n    train_row = pd.concat(train_row, axis=1)\n    train_set.append(train_row)\n    j+=1\n\ntrain_set = pd.concat(train_set)","f8f9eceb":"train_set = train_set.reset_index()\ntrain_set = train_set.rename(columns={'index': 'segment_id'})\ntrain_set = pd.merge(train_set, train, on='segment_id')\ntrain_set","b9cb549b":"drop_cols = list()\nfor col in train_set.columns:\n    if col == 'segment_id':\n        continue\n    if abs(train_set[col].corr(train_set['time_to_eruption'])) < 0.01:\n        drop_cols.append(col)","02d06404":"not_to_drop_cols = list()\n\nfor col1 in train_set.columns:\n    for col2 in train_set.columns:\n        if col1 == col2:\n            continue\n        if col1 == 'segment_id' or col2 == 'segment_id': \n            continue\n        if col1 == 'time_to_eruption' or col2 == 'time_to_eruption':\n            continue\n        if abs(train_set[col1].corr(train_set[col2])) > 0.98:\n            if col2 not in drop_cols and col1 not in not_to_drop_cols:\n                drop_cols.append(col2)\n                not_to_drop_cols.append(col1)\n","1f143691":"train = train_set.drop(['segment_id', 'time_to_eruption'], axis=1)\ny = train_set['time_to_eruption']","a8d0c711":"reduced_y = y.copy()\nreduced_train = train.copy()\nreduced_train = reduced_train.drop(drop_cols, axis=1)\nreduced_train","03cb92f6":"train, val, y, y_val = train_test_split(train, y, random_state=666, test_size=0.2, shuffle=True)\nreduced_train, reduced_val, reduced_y, reduced_y_val = train_test_split(reduced_train, reduced_y, random_state=666, test_size=0.2, shuffle=True)","58ceedcc":"lgb = LGBMRegressor(\n    random_state=666, \n    max_depth=7, \n    n_estimators=250, \n    learning_rate=0.12\n)\n\nlgb.fit(train, y)\npreds = lgb.predict(val)","16b0d255":"def rmse(y_true, y_pred):\n    return math.sqrt(mse(y_true, y_pred))","d872d338":"print('Simple LGB model rmse: ', rmse(y_val, preds))","02583795":"sampler = TPESampler(seed=666)\n\ndef create_model(trial):\n    num_leaves = trial.suggest_int(\"num_leaves\", 2, 31)\n    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300)\n    max_depth = trial.suggest_int('max_depth', 3, 8)\n    min_child_samples = trial.suggest_int('min_child_samples', 100, 1200)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0001, 0.99)\n    min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 5, 90)\n    bagging_fraction = trial.suggest_uniform('bagging_fraction', 0.0001, 1.0)\n    feature_fraction = trial.suggest_uniform('feature_fraction', 0.0001, 1.0)\n    model = LGBMRegressor(\n        num_leaves=num_leaves,\n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        min_child_samples=min_child_samples, \n        min_data_in_leaf=min_data_in_leaf,\n        learning_rate=learning_rate,\n        feature_fraction=feature_fraction,\n        random_state=666\n    )\n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(train, y)\n    preds = model.predict(val)\n    score = rmse(y_val, preds)\n    return score\n\n# To use optuna uncomment it \n# study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n# study.optimize(objective, n_trials=5000)\n# params = study.best_params\n# params['random_state'] = 666\n\nparams = {\n    'num_leaves': 29,\n    'n_estimators': 289,\n    'max_depth': 8,\n    'min_child_samples': 507,\n    'learning_rate': 0.0812634327662599,\n    'min_data_in_leaf': 13,\n    'bagging_fraction': 0.020521665677937423,\n    'feature_fraction': 0.05776459974779927,\n    'random_state': 666\n}\n\nlgb = LGBMRegressor(**params)\nlgb.fit(train, y)","8b6b6229":"preds = lgb.predict(val)\nprint('Optimized LGB model rmse: ', rmse(y_val, preds))","ae929924":"parms = {\n    'num_leaves': 31, \n    'n_estimators': 138, \n    'max_depth': 8, \n    'min_child_samples': 182, \n    'learning_rate': 0.16630987899513125, \n    'min_data_in_leaf': 24, \n    'bagging_fraction': 0.8743237361979733, \n    'feature_fraction': 0.45055692472636766,\n    'random_state': 666\n}\n\nrfe_lgb = RFE(\n    estimator=DecisionTreeRegressor(\n        random_state=666\n    ), \n    n_features_to_select=83\n)\n\npipe_lgb = Pipeline(\n    steps=[\n        ('s', rfe_lgb), \n        ('m', LGBMRegressor(**parms))\n    ]\n)\n\npipe_lgb.fit(train, y)\npreds = pipe_lgb.predict(val)  \nprint('LGB rmse', rmse(y_val, preds))","0c2cbd88":"params = {\n    'max_depth': 11, \n    'n_estimators': 245, \n    'learning_rate': 0.0925872303097654, \n    'gamma': 0.6154687206061559,\n    'random_state': 666\n}\n\nrfe_estimator = RFE(estimator=DecisionTreeRegressor(random_state=666), n_features_to_select=60)\npipe = Pipeline(\n    steps=[\n        ('s', rfe_estimator),\n        ('m', XGBRegressor(**params))\n    ]\n)\n\npipe.fit(train, y)\npreds = pipe.predict(val)  \nprint('XGBoost rmse', rmse(y_val, preds))","a685770d":"params = {\n    'max_depth': 6,\n    'n_estimators': 189,\n    'learning_rate': 0.09910718143795864,\n    'gamma': 0.787986320220815,\n    'random_state': 666\n}\n\nxgb_short = XGBRegressor(\n    **params\n)\nxgb_short.fit(reduced_train, reduced_y)\npreds = xgb_short.predict(reduced_val)  \nprint('XGBoost rmse', rmse(reduced_y_val, preds))","1c052d8a":"def root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=0))\n\ndef create_model():\n    model = tf.keras.Sequential([\n        tf.keras.layers.Input((241,)),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(1000, activation=\"sigmoid\"),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.6),\n        tf.keras.layers.Dense(1, activation='relu')\n    ])\n    \n    model.compile(\n        loss=root_mean_squared_error, \n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n    )\n    return model","0e89eef4":"yy = np.log1p(y)","5111977a":"models = list()\npreds = list()\n\nfor n, (tr, te) in enumerate(KFold(\n    n_splits=3, \n    random_state=666, \n    shuffle=True).split(yy)):\n    \n    early_stopping = EarlyStopping(\n        patience=10, \n        verbose=0\n    )\n    \n    print(f'Fold {n}')\n    \n    model = create_model()\n    \n    model.fit(\n        train.values[tr],\n        yy.values[tr],\n        epochs=4000,\n        batch_size=128,\n        verbose=0,\n        callbacks=[early_stopping]\n    )\n    \n    pred = model.predict(val)\n    pred = np.expm1(pred).reshape((pred.shape[0], ))\n    preds.append(pred)\n    print('Fold rmse', rmse(yy.values[te], model.predict(train.values[te]))) \n    models.append(model)","bacaac82":"predictions = preds[0]\nfor i in range(1, 3):\n    predictions += preds[i]\npredictions \/= 3\n\nprint('NN rmse', rmse(y_val, predictions))","aad9c6cd":"sample_submission","5f96ab79":"test_set = list()\nj=0\nfor seg in sample_submission.segment_id:\n    signals = pd.read_csv(f'\/kaggle\/input\/predict-volcanic-eruptions-ingv-oe\/test\/{seg}.csv')\n    test_row = []\n    if j%500 == 0:\n        print(j)\n    for i in range(0, 10):\n        sensor_id = f'sensor_{i+1}'\n        test_row.append(build_features(signals[sensor_id].fillna(0), seg, sensor_id))\n    test_row = pd.concat(test_row, axis=1)\n    test_set.append(test_row)\n    j+=1\ntest_set = pd.concat(test_set)","fe0bf2df":"test_set = test_set.reset_index()\ntest_set = test_set.rename(columns={'index': 'segment_id'})\ntest_set = pd.merge(test_set, for_test_df, on='segment_id')\ntest = test_set.drop(['segment_id'], axis=1)\ntest","66181dfc":"reduced_test = test.copy()\nreduced_test = reduced_test.drop(drop_cols, axis=1)\nreduced_test","432aa2b7":"preds1 = lgb.predict(test)\npreds1","b2e23d4c":"preds2 = pipe_lgb.predict(test)\npreds2","51294547":"preds3 = pipe.predict(test)\npreds3","aba84a63":"predictions = list()\nfor model in models:\n    pred = model.predict(test)\n    pred = np.expm1(pred).reshape((pred.shape[0], ))\n    predictions.append(pred)\n\npreds4 = predictions[0]\nfor i in range(1, 3):\n    preds4 += predictions[i]\npreds4 \/= 3\n\npreds4","0e69db5d":"preds5 = xgb_short.predict(reduced_test)\npreds5","c68236d8":"test_set['time_to_eruption'] = preds1 * 0.5 + preds2 * 0.05 + preds3 * 0.3 + preds4 * 0.05 + preds5 * 0.1","608673d0":"sample_submission = pd.merge(sample_submission, test_set[['segment_id', 'time_to_eruption']], on='segment_id')","d2ddd885":"sample_submission = sample_submission.drop(['time_to_eruption_x'], axis=1)\nsample_submission.columns = ['segment_id', 'time_to_eruption']\nsample_submission","9afaeed6":"sample_submission.to_csv('submission.csv', index=False)","53aae1bd":"## WORK IN PROGRESS","97bc9589":"Let's check time_to_eruption for segments with and without missed sensors.","f5a269f5":"We can see that we have data from 10 sensors and 60001 observation for each of them.","7e59f73a":"Let's check number of observations and number of sensors for every sample in train directory.","669ebf61":"<h1><center>INGV - Volcanic Eruption Prediction. Data Analysis.<\/center><\/h1>\n\n<center><img src=\"https:\/\/images.ctfassets.net\/81iqaqpfd8fy\/3Wp4SEgzagcICaSqcIMOQM\/5721655abf93a19521dad8a35d747f2d\/Erupting_Volcano.jpg?h=620&w=1440\"><\/center>","bb967463":"<a id=\"1\"><\/a>\n<h2 style='background:black; border:0; color:white'><center>1. Data overview<center><h2>","43dc1c5f":"**[train|test]\/*.csv**: the data files. Each file contains ten minutes of logs from ten different sensors arrayed around a volcano. The readings have been normalized within each segment, in part to ensure that the readings fall within the range of int16 values. If you are using the Pandas library you may find that you still need to load the data as float32 due to the presence of some nulls.","029cdeff":"Let's do the same for test set.","46afa135":"Let's see now missed combinations of sensors.","cae46460":"**train.csv** Metadata for the train files.\n\n* segment_id: ID code for the data segment. Matches the name of the associated data file.\n\n* time_to_eruption: The target value, the time until the next eruption.","347411d5":"<a id=\"2\"><\/a>\n<h2 style='background:black; border:0; color:white'><center>2. Train & test fragments<center><h2>","97144337":"Let's see how many missed sensors we have in training set.","509ac039":"As we can see we don't have missed sensors for all volcanos for sensor_4 and sensor_6 in trainig set.","f040aeae":"Let's check data from all sensors for current volcano.","72b53039":"<a id=\"3\"><\/a>\n<h2 style='background:black; border:0; color:white'><center>3. Modeling<center><h2>"}}