{"cell_type":{"6f3661ca":"code","c803a693":"code","55ab8b14":"code","903e2680":"code","f843dccb":"code","2251bde0":"code","30bbe6a8":"code","92df6267":"code","4abcefa0":"code","345e40fe":"code","d9304fdf":"code","e8358dc3":"code","3db5659f":"code","2f4c13ee":"code","7f322898":"code","f5398837":"code","33978239":"code","45aa20e1":"code","8eabeac8":"code","66d3f63b":"code","fef606c1":"code","79a2d0e1":"code","9ebbc572":"code","cd7285bc":"code","3a14791f":"code","7c7c05e5":"code","fd5aa1d4":"code","3e7ba3d9":"code","17320ba2":"code","c9a93438":"code","e39e468e":"code","097d3821":"code","848b9d36":"code","b29a710c":"code","d9de2fb1":"code","f1594361":"code","858038cd":"code","165a1f85":"code","2bbf3229":"code","90477387":"code","c769efb5":"code","c44c0030":"code","c40c41fc":"code","de4b0c72":"code","67005434":"code","fbc749ca":"code","2a2bf978":"code","a6c58b83":"code","0305246d":"code","cd046c84":"code","90f84b29":"code","323c2971":"code","05894117":"code","fc50884a":"code","4979cb7c":"code","49e96971":"code","0d605165":"code","8b2e74ad":"code","4c33ef86":"code","888b5054":"code","8ca91e8a":"code","d64f00fd":"code","3f6cca8e":"code","a814a46b":"code","7ee2fcfd":"code","86744101":"code","ea9b78c8":"code","e4db29fd":"code","a6283831":"code","5399cf72":"code","19daed8a":"code","efc25577":"code","5a7c56c4":"code","370f7166":"code","c942e001":"code","74748b29":"code","bc655fc0":"code","7ea16a1e":"code","84c1e593":"code","a67d0274":"code","e3442f3d":"code","112b7435":"code","3362b393":"code","999ab5c5":"code","bb2e4d4d":"code","8b9ead3e":"code","0c60ed82":"code","360948ea":"code","9e2cefd6":"code","de0131fc":"code","e6151d7e":"code","6cdf6df3":"code","9a0ebc27":"code","91c6c921":"markdown","1763e70f":"markdown","c7fefe58":"markdown","9ddfe61d":"markdown","dc8f49aa":"markdown","953da466":"markdown","b25e88ef":"markdown","8b76fa8a":"markdown","0aae2106":"markdown","23a0e115":"markdown","fe0b7d4d":"markdown","e5ed4fd3":"markdown","c0a18ad8":"markdown","34106a7f":"markdown","2e9cba62":"markdown","9b2980f6":"markdown","04a93ba5":"markdown","9b5c5655":"markdown","f10601b1":"markdown","3b35d88c":"markdown","93078ffd":"markdown","97c2c9c8":"markdown","bc3dc7f2":"markdown","64cb3fac":"markdown","6d7345fa":"markdown","58bf90cc":"markdown","13726940":"markdown"},"source":{"6f3661ca":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix, precision_score, recall_score\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb\nfrom sklearn.svm import LinearSVC\nimport gc","c803a693":"seed = 1001","55ab8b14":"df = pd.read_csv(\"..\/input\/languagedetectiontrwords\/language_detection.csv\")","903e2680":"df.head()","f843dccb":"df[\"is_turkish\"].value_counts()","2251bde0":"df[\"is_turkish\"].value_counts(normalize = True)","30bbe6a8":"df[\"Words\"].apply(lambda x: len(x)).value_counts()","92df6267":"df['Vowels'] = df.Words.str.lower().str.count(r'[aeiou]')\ndf['Consonant'] = df.Words.str.lower().str.count(r'[a-z]') - df['Vowels']\ndf[\"Len\"] = df.Words.apply(lambda x: len(x))","4abcefa0":"df.head()","345e40fe":"word = False\nword_tfidf = False","d9304fdf":"count_para_word = {\n    \"analyzer\": \"word\",\n    \"dtype\": np.float32,\n}\n\ncount_para_char = {\n    \"analyzer\": \"char_wb\",\n    \"dtype\": np.float32,\n    \"ngram_range\": (1, 6) # max letter count\n}","e8358dc3":"if word:\n    vectorizer = CountVectorizer(\n        max_features=50000, \n        **count_para_word)\nelse:\n    vectorizer = CountVectorizer(\n        max_features=50000, \n        **count_para_char)","3db5659f":"tfidf_para_word = {\n    \"analyzer\": \"word\",\n    \"sublinear_tf\": True,\n    \"dtype\": np.float32,\n    \"norm\": 'l2',\n    \"min_df\": 5,\n    \"max_df\": 0.9,\n    \"smooth_idf\": False\n}\n\ntfidf_para_char = {\n    \"analyzer\": \"char_wb\",\n    \"dtype\": np.float32,\n    \"ngram_range\": (1, 6) # max letter count\n}","2f4c13ee":"\nif word_tfidf:\n    vectorizer_tfidf = TfidfVectorizer(\n                max_features=5000,\n                **tfidf_para_word)\nelse:\n    vectorizer_tfidf = TfidfVectorizer(\n                max_features=50000,\n                **tfidf_para_char)","7f322898":"vectorizer.fit(df[\"Words\"])","f5398837":"vectorizer_tfidf.fit(df[\"Words\"])","33978239":"train_vector, test_vector = train_test_split(df, test_size = 0.2, stratify = df[\"is_turkish\"], random_state = seed)","45aa20e1":"train_vector = train_vector[(train_vector.is_turkish == 1) | (\n    train_vector.is_turkish == 0).sample(frac=0.4, random_state=seed)].sample(\n        frac=1, random_state=seed).reset_index(drop=True)","8eabeac8":"y_train = train_vector[\"is_turkish\"].values\ny_test = test_vector[\"is_turkish\"].values","66d3f63b":"train_vector_ = vectorizer.transform(train_vector[\"Words\"])\ntest_vector_ = vectorizer.transform(test_vector[\"Words\"])","fef606c1":"train_vector_tfidf = vectorizer_tfidf.transform(train_vector[\"Words\"])\ntest_vector_tfidf = vectorizer_tfidf.transform(test_vector[\"Words\"])","79a2d0e1":"from scipy.sparse import hstack, csr_matrix","9ebbc572":"train_vector = hstack([\n    train_vector_,\n    train_vector_tfidf,\n    csr_matrix(train_vector.Vowels.values.reshape(-1, 1)),\n    csr_matrix(train_vector.Consonant.values.reshape(-1, 1)),\n    csr_matrix(train_vector.Len.values.reshape(-1, 1))\n])","cd7285bc":"test_vector = hstack([\n    test_vector_,\n    test_vector_tfidf,\n    csr_matrix(test_vector.Vowels.values.reshape(-1, 1)),\n    csr_matrix(test_vector.Consonant.values.reshape(-1, 1)),\n    csr_matrix(test_vector.Len.values.reshape(-1, 1))\n])","3a14791f":"train_vector = train_vector.tocsr()\ntest_vector = test_vector.tocsr()","7c7c05e5":"train_vector.shape, y_train.shape","fd5aa1d4":" test_vector.shape, y_test.shape","3e7ba3d9":"del train_vector_, test_vector_\ndel train_vector_tfidf, test_vector_tfidf","17320ba2":"gc.collect()","c9a93438":"class SklearnWrapper(object):\n    def __init__(self, clf, seed=0, params=None, seed_bool = True):\n        if(seed_bool == True):\n            params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n        \n    def predict_proba(self, x):\n        return self.clf.predict_proba(x)\n    \n    \ndef get_oof(clf, x_train, y, x_test, prob = False):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf.split(x_train, y)):\n        x_tr = x_train[train_index]\n        y_tr = y[train_index]\n        x_te = x_train[test_index]\n        y_te = y[test_index]\n\n        clf.train(x_tr, y_tr)\n        if prob:\n            oof_train[test_index] = clf.predict_proba(x_te)[:,1]\n            oof_test_skf[i, :] = clf.predict_proba(x_test)[:,1]\n        else:\n            oof_train[test_index] = clf.predict(x_te)\n            oof_test_skf[i, :] = clf.predict(x_test)\n        print(\"Fold:\", i+1)\n        print(\"F1\", np.round(f1_score(y_te, np.round(oof_train[test_index])),6))\n        print(\"\\n\")\n        \n        \n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","e39e468e":"def scorer(y_true, y_pred, is_return = False):\n    if is_return:\n        return [f1_score(np.round(y_true), y_pred), accuracy_score(np.round(y_true), y_pred), recall_score(np.round(y_true), y_pred), precision_score(np.round(y_true), y_pred)]\n    else:\n        print(\"F1: {:.4f}\".format(f1_score(np.round(y_true), y_pred)))\n        print(\"Accuracy: {:.4f}\".format(accuracy_score(np.round(y_true), y_pred)))\n        print(\"Recall: {:.4f}\".format(recall_score(np.round(y_true), y_pred)))\n        print(\"Precision: {:.4f}\".format(precision_score(np.round(y_true), y_pred)))\n        print((confusion_matrix(np.round(y_true), y_pred)))","097d3821":"NFOLDS = 5","848b9d36":"ntrain = train_vector.shape[0]\nntest = test_vector.shape[0]\nkf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed)","b29a710c":"ridge_params = {'alpha':250.0, 'fit_intercept':True, 'normalize':True, 'copy_X':True,\n                'max_iter':None, 'tol':0.001, 'solver':'auto', 'random_state':seed,\n               \"class_weight\": \"balanced\"\n               }","d9de2fb1":"ridge = SklearnWrapper(clf=RidgeClassifier, seed = seed, params = ridge_params)","f1594361":"%%time\nridge_oof_train, ridge_oof_test = get_oof(ridge, train_vector, y_train, test_vector, prob=False)","858038cd":"scorer(ridge_oof_train, y_train)","165a1f85":"scorer(ridge_oof_test, y_test)","2bbf3229":"gc.collect()","90477387":"svm_params = {\"C\": 3, \"max_iter\": 10000, \"class_weight\": \"balanced\"}","c769efb5":"svm = SklearnWrapper(clf=LinearSVC, seed = seed, params = svm_params)","c44c0030":"%%time\nsvm_oof_train, svm_oof_test = get_oof(svm, train_vector.toarray(), y_train, test_vector.toarray(), prob=False)","c40c41fc":"gc.collect()","de4b0c72":"scorer(svm_oof_train, y_train)","67005434":"scorer(svm_oof_test, y_test)","fbc749ca":"gc.collect()","2a2bf978":"lgbm_params =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_error',\n    'num_leaves': 17,\n    \"max_depth\": -1,\n    'feature_fraction': 0.95,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 8,\n    'learning_rate': 0.075,\n    'verbosity': 0,\n    \"num_iterations \": 50,\n    \"pos_bagging_fraction\": 0.8,\n    \"neg_bagging_fraction\": 0.6,\n}","a6c58b83":"%%time\nlgbc = SklearnWrapper(clf=LGBMClassifier, seed = seed, params = lgbm_params)\nlgbc_oof_train, lgbc_oof_test = get_oof(lgbc, train_vector, y_train, test_vector, prob=True)","0305246d":"scorer(lgbc_oof_train, y_train)","cd046c84":"scorer(lgbc_oof_test, y_test)","90f84b29":"gc.collect()","323c2971":"rf_params = {\"n_jobs\":12,\n            \"n_estimators\": 380,\n            \"class_weight\": \"balanced\",\n            \"min_samples_split\": 5}","05894117":"%%time\nrf = SklearnWrapper(clf=RandomForestClassifier, seed = seed, params = rf_params)\nrf_oof_train, rf_oof_test = get_oof(rf, train_vector, y_train, test_vector, prob=True)","fc50884a":"scorer(rf_oof_train, y_train)","4979cb7c":"scorer(rf_oof_test, y_test)","49e96971":"rf_2_params = {\"n_jobs\":12,\n               \"max_depth\": 15,\n               \"n_estimators\": 500,\n               \"criterion\": \"gini\",\n               \"min_samples_split\": 5,\n               \"min_samples_leaf\": 3,\n               \"class_weight\": \"balanced\"}","0d605165":"%%time\nrf_2 = SklearnWrapper(clf=RandomForestClassifier, seed = seed, params = rf_2_params)\nrf_2_oof_train, rf_2_oof_test = get_oof(rf_2, train_vector, y_train, test_vector, prob=True)","8b2e74ad":"scorer(rf_2_oof_train, y_train)","4c33ef86":"scorer(rf_2_oof_test, y_test)","888b5054":"gc.collect()","8ca91e8a":"lr_params = {\"n_jobs\":12,\n            \"solver\": \"saga\",\n             \"C\": 10,\n            \"max_iter\": 1000}","d64f00fd":"%%time\nlr = SklearnWrapper(clf=LogisticRegression, seed = seed, params = lr_params)\nlr_oof_train, lr_oof_test = get_oof(lr, train_vector, y_train, test_vector, prob = True)","3f6cca8e":"scorer(lr_oof_train, y_train)","a814a46b":"scorer(lr_oof_test, y_test)","7ee2fcfd":"lr_2_params = {\n            \"solver\": \"liblinear\",\n    \"class_weight\": \"balanced\",\n            \"max_iter\": 750,\n            \"C\":25,\n              \"penalty\": \"l1\"}","86744101":"%%time\nlr_2 = SklearnWrapper(clf=LogisticRegression, seed = seed, params = lr_2_params)\nlr_2_oof_train, lr_2_oof_test = get_oof(lr_2, train_vector, y_train, test_vector, prob = True)","ea9b78c8":"scorer(lr_2_oof_train, y_train)","e4db29fd":"scorer(lr_2_oof_test, y_test)","a6283831":"gc.collect()","5399cf72":"def threshold_search(y_true, y_proba):\n    best_threshold = 0\n    best_score = 0\n    for threshold in [i * 0.01 for i in range(100)]:\n        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n        if score > best_score:\n            best_threshold = threshold\n            best_score = score\n    search_result = {'threshold': best_threshold, 'f1': best_score}\n    return search_result","19daed8a":"ridge_thr = threshold_search(y_train, ridge_oof_train)\nridge_thr","efc25577":"lgb_thr = threshold_search(y_train, lgbc_oof_train)\nlgb_thr","5a7c56c4":"rf_thr = threshold_search(y_train, rf_oof_train)\nrf_thr","370f7166":"rf2_thr = threshold_search(y_train, rf_2_oof_train)\nrf2_thr","c942e001":"lr_thr = threshold_search(y_train, lr_oof_train)\nlr_thr","74748b29":"lr2_thr = threshold_search(y_train, lr_2_oof_train)\nlr2_thr","bc655fc0":"svm_thr = threshold_search(y_train, svm_oof_train)\nsvm_thr","7ea16a1e":"gc.collect()","84c1e593":"train_df = np.concatenate((\n    ridge_oof_train,\n    (lr_oof_train > lr_thr[\"threshold\"]).astype(int),\n    (lr_2_oof_train > lr2_thr[\"threshold\"]).astype(int),\n    (rf_oof_train > rf_thr[\"threshold\"]).astype(int),\n    (rf_2_oof_train > rf2_thr[\"threshold\"]).astype(int),\n    (lgbc_oof_train > lgb_thr[\"threshold\"]).astype(int),\n    svm_oof_train\n),\n                          axis=1)","a67d0274":"test_df = np.concatenate((\n    ridge_oof_test,\n    (lr_oof_test > lr_thr[\"threshold\"]).astype(int),\n    (lr_2_oof_test > lr2_thr[\"threshold\"]).astype(int),\n    (rf_oof_test > rf_thr[\"threshold\"]).astype(int),\n    (rf_2_oof_test > rf2_thr[\"threshold\"]).astype(int),\n    (lgbc_oof_test > lgb_thr[\"threshold\"]).astype(int),\n    svm_oof_test\n),\n                         axis=1)","e3442f3d":"# Let's check it out!\npd.DataFrame(train_df).head()","112b7435":"y_train.shape, train_df.shape","3362b393":"dtrain = lgb.Dataset(train_df, label=y_train)","999ab5c5":"def lgb_f1_score(y_hat, data):\n    y_true = data.get_label()\n    y_hat = np.round(y_hat)\n    return 'f1', f1_score(y_true, y_hat), True","bb2e4d4d":"lgbm_params =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'auc',\n    'num_leaves': 11,\n    \"max_depth\": 4,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.95,\n    'bagging_freq': 12,\n    'learning_rate': 0.05,\n    \"scale_pos_weight\":  0.9,\n    'verbose': 0,\n    \"pos_bagging_fraction\": 0.8,\n    \"neg_bagging_fraction\": 0.6,\n}","8b9ead3e":"cv = lgb.cv(lgbm_params,\n            dtrain,\n            num_boost_round=1000,\n            folds=kf,\n            verbose_eval=5,\n            early_stopping_rounds=20,\n            feval=lgb_f1_score,\n            eval_train_metric=False)","0c60ed82":"cv[\"f1-mean\"][-1]","360948ea":"best_n = len(cv[\"f1-mean\"])\nbest_n","9e2cefd6":"model = lgb.train(lgbm_params, dtrain, num_boost_round=int(best_n * 1.1))","de0131fc":"test_preds = model.predict(test_df)","e6151d7e":"scorer(y_test, np.round(test_preds))","6cdf6df3":"results_df = pd.DataFrame([scorer(ridge_oof_test, y_test, is_return=True),\nscorer(svm_oof_test, y_test, is_return=True),\nscorer(lgbc_oof_test, y_test, is_return=True),\nscorer(rf_oof_test, y_test, is_return=True),\nscorer(rf_2_oof_test, y_test, is_return=True),\nscorer(lr_oof_test, y_test, is_return=True),\nscorer(lr_2_oof_test, y_test, is_return=True),\nscorer(y_test, np.round(test_preds), is_return=True)], columns=[\"F1\", \"Accuracy\", \"Recall\", \"Precision\"], index = [\"Ridge\", \"SVM\", \"LGB\", \"RF\", \"RF2\", \"LR\", \"LR2\", \"Stacked Model\"])\nresults_df","9a0ebc27":"gc.collect()","91c6c921":"Before transforming text to numbers we need to split the data. That's why I fitted vectorizers on the whole data. We fitted on the whole data and we can use them on splitted data. <br>\nLet's use Pareto! (80-20)","1763e70f":"## Preprocess","c7fefe58":"### Stacking","9ddfe61d":"### Logistic Regression","dc8f49aa":"## Modelling","953da466":"![](https:\/\/qph.fs.quoracdn.net\/main-qimg-773e50b40faed6c9d1ba7df233d32a96-c)\n[Source](https:\/\/www.quora.com\/Why-should-we-use-a-stacking-ensemble-with-a-fixed-k-fold)\n##### Resources for Stacking\n* [Introduction to Ensembling\/Stacking in Python](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python) \n* [Kaggle Ensembling Guide](https:\/\/mlwave.com\/kaggle-ensembling-guide\/)\n* [Stacked generalization, Wolpert DH.](https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0893608005800231)\n* [Stacked Regressions : Top 4% on LeaderBoard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\/notebook)","b25e88ef":"Seems like we have an imbalanced class problem!","8b76fa8a":"### Algorithms\n* Ridge Classifier\n* SVM (Linear Kernel)\n* Logistic Regression\n* Random Forest\n* LightGBM <br>\n\nMost of the parameters are tuned manually. There is a lot of room to improve of course!","0aae2106":"We have a word list which is mixed with Turkish and English words and we want to recognize the language from the words. And we don't want to use Deep Learning based approaches. Back to basics for text preprocessing!","23a0e115":"Also I'm doing some sort of downsampling. Taking the all positive instances and 40% of the negative ones.","fe0b7d4d":"## Language Detection via Words (with Stacking)\n* [Blog Post (in Turkish)](https:\/\/silverstone1903.github.io\/projects\/language-detection-via-words-with-stacking-tr\/)\n* [Deployment with Streamlit (in Turkish)](https:\/\/silverstone1903.github.io\/posts\/2021\/02\/streamlit-ve-heroku-ile-canliya-cikmak\/)","e5ed4fd3":"### Ridge","c0a18ad8":"### LightGBM","34106a7f":"<p align=\"center\">\n  <img src=\"https:\/\/silverstone1903.github.io\/images\/gptmeme.jpg\" \/>\n<\/p>","2e9cba62":"As a result, we achieved the best F1 score with the Stacking model. However, it should not be forgotten that here we are doing a small trade-off between Precision and Recall. For this reason, both preprocessing and modeling stages should be reconsidered, taking into account the content & constraints of the problem and the imbalanced ratio. For example, changes such as changing the downsampling rate, optimizing the parameter or changing the cost function can be done. <br>\n\nFor models selected instead of stacking, blending can be attempted using linear weighting (weighted linear blending).","9b2980f6":"For the modelling I'm going to use Faron's stacking starter with 5 Fold Stratified KFold and get OOF (out of fold predictions). Then use predictions for the stacking. \n\nNow here is the question; What is stacking? Basically our models learn from independent variables (x's) to predict dependent variable (y). For this notebook, our independent variables are TF-IDF values. We got our OOF, test predictions and it's time to train a model with these values as inputs. Model will start to learn from single models predictions. <br> <br>\nWhat are the important things for stacking? One more question! First keep your CV schema same with the single models. We don't want any leakage. And the second one is *diversity*. We want a lot of different models to learn different situations. That's why I'm going to use RF & LR with two different parameters.\n","04a93ba5":"#### Threshold Optimization","9b5c5655":"At this point I round the predictions to maximize F1 metric with cut off value. <br>\nRidge and SVM don't return probability for the prediction. So these are already rounded and that's the reason why threshold is 0. <br>\nOne more thing, I did rounding because of the Ridge and SVM classifiers. Also you can use probability as an input.","f10601b1":"### SVM","3b35d88c":"### Random Forest","93078ffd":"As far as I know it must be Faron's stacking starter code. I just added `predict_proba` method and some printing stuffs. <br>\nCredits goes to Faron: https:\/\/www.kaggle.com\/mmueller\/stacking-starter","97c2c9c8":"Calculating meta features for the words.\n* Number of vowels\n* Number of consonant\n* Total number of characters","bc3dc7f2":"Words have minimum 4 letters and maximum 6 letters.","64cb3fac":"We are going to use TF-IDF and Count vectorizer. Count Vectorizer and TF-IDF are the 2 ways in which text can be converted to numbers. ","6d7345fa":"Here is the our new input array for the both train and test sets.","58bf90cc":"Now we have two sparse matrixes from both CountVec and TF-IDF. Also we calculated some meta-features. Need to combine them together.","13726940":"For the parameter details; <br>\n[Count Vectorizer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html)<br>\n[TF-IDF](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html) <br>\n<br>\nI tried both word analyzer and char analyzer and char analyzer got better results than word. This is because (I think) we just have single words, not the sentences.\n\n"}}