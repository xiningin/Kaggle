{"cell_type":{"d8a3df7a":"code","422cd853":"code","0e006752":"code","86cd5dcd":"code","44a56713":"code","a814a4d6":"code","5e839833":"code","3155106f":"code","ef2d67da":"code","8ee93be7":"code","377ca933":"code","1ccda4a1":"code","db2e8960":"code","c9cabe7d":"code","41ec3f75":"code","86fcff88":"code","735f332b":"code","0a2d4ee8":"code","cd476d55":"code","089da50c":"code","ea1857e0":"code","88cadc7e":"code","28a77c1a":"code","6130f76a":"code","592aaf9c":"code","4afb749b":"markdown"},"source":{"d8a3df7a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","422cd853":"import os\nSEED = 123\nimport warnings\nwarnings.filterwarnings('ignore')\nimport random\n#from sklearn.externals import joblib\nimport qgrid\ndef gview(data):\n    return(qgrid.show_grid(data,show_toolbar=True,grid_options={'forceFitColumns': False,'highlightSelectedCell': True,'highlightSelectedRow': True}))\nfrom time import time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython import display\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_digits\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(123)\n# -----------FUNCTIONS ------------\n#%run start_up.py\n\n\ncurr_dir = os.getcwd()\nprint(\"current directory:\\n\\t\\t\",curr_dir)\n\n# os.chdir(curr_dir)\n\n# folder_list=['input_data','temp_data','submission']\n\n# for iin in folder_list:\n#     if not os.path.exists(f'{iin}'):\n#         os.makedirs(f'{iin}')   \n    \n# data_locn = 'C:\/Users\/nshaikh2\/Desktop\/live\/full_data\/ml_data'\n\nfrom datetime import datetime\nt_stamp = datetime.now().strftime(\"%d-%m-%Y_%I-%M-%S_%p\")\n\n","0e006752":"#!\/usr\/bin\/env python\n# coding: utf-8\n\n# In[ ]:\n\n\n# SETTING BASIC ENVIORMENT :\n\n# LOADING LIBRARY ---\n\nimport qgrid\nimport os\nimport pandas as pd \nimport numpy as np \nimport scipy.stats as stats\n# import qgrid\nfrom scipy.stats import kurtosis, skew\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# import pyarrow.parquet as pq\n# import pyarrow as pa\nimport pandas as pd\nimport numpy as np\nfrom scipy import interp\n\n# from dplython import select, DplyFrame, X, arrange, count, sift, head, summarize, group_by, tail, mutate\n\nfrom sklearn.model_selection import validation_curve\nimport sklearn.metrics as metrics\n\nfrom sklearn.metrics import auc, accuracy_score\nfrom sklearn.metrics  import plot_roc_curve\nfrom sklearn.model_selection import StratifiedShuffleSplit,StratifiedKFold\n\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import ParameterGrid\nfrom sklearn.model_selection import train_test_split\nfrom itertools import product, chain\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.metrics import classification_report\n\n\ndef gview(data):\n    print('DIMENSION',data.shape)\n    return(qgrid.show_grid(data,show_toolbar=True,grid_options={'forceFitColumns': False,'highlightSelectedCell': True,\n        'highlightSelectedRow': True}))\n\ndef trim_all_columns(df):\n        trim_strings = lambda x: x.strip() if isinstance(x, str) else x\n        return df.applymap(trim_strings)\n\n\n# Basic data cleaning\ndef data_basic_clean(fsh):\n        fsh.columns = [c.strip() for c in fsh.columns]\n        fsh.columns = [c.replace(' ', '_') for c in fsh.columns]\n        fsh.columns = map(str.lower, fsh.columns)\n        fsh.replace(['None','nan','Nan',' ','NaT','#REF!'],np.nan,inplace=True)\n        fsh = trim_all_columns(fsh)\n        fsh=fsh.drop_duplicates(keep='last')\n        df = pd.DataFrame(fsh)\n        return(df)\n\ndef call_napercentage(data_train):\n    op = pd.DataFrame(data_train.isnull().sum()\/data_train.shape[0]*100)\n    op = op.reset_index()\n    op.rename(columns={'index':'variable_name'},inplace=True)\n    op.rename(columns={0:'na%'},inplace=True)\n    op=op.sort_values(by='na%',ascending=False)\n    return(op)\n\ndef get_numcolnames(df):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    numdf = df.select_dtypes(include=numerics)\n    cols_nums =numdf.columns\n    cols_nums = cols_nums.tolist()\n    return(cols_nums)\n\ndef get_catcolnames(df):\n    categoric = ['object']\n    catdf = df.select_dtypes(include=categoric)\n    cols_cat = catdf.columns\n    cols_cat = cols_cat.tolist()\n    return(cols_cat)\n    \n    \ndef get_partialcol_match(final_df,txt):\n    date_colist = final_df[final_df.columns[final_df.columns.to_series().str.contains(f'{txt}')]].columns\n    date_colist = date_colist.tolist()\n    return(date_colist)  \n    \n    \ndef summarise_yourdf(df,leveli):\n    print(f\"Dataset Shape: {df.shape}\")    \n\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing_Count'] = df.isnull().sum().values\n    summary['Missing_Perct'] = round(summary['Missing_Count']\/df.shape[0]*100,2)\n    summary['Uniques_Count'] = df.nunique().values\n    summary['Uniques_Perct'] = round(summary['Uniques_Count']\/df.shape[0]*100,2)\n    \n    #summary['First Value'] = df.loc[0].values\n    #summary['Second Value'] = df.loc[1].values\n    #summary['Third Value'] = df.loc[2].values\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n    summary['Zeros_count'] = df[df == 0].count(axis=0).values\n    summary['Zeros_Perct'] = round(summary['Zeros_count']\/df.shape[0]*100,2)\n    \n    \n    summary['Levels']= 'empty'\n    for i, m in enumerate (summary['Name']):\n            #print(i,m)\n            if len(df[f'{m}'].value_counts()) <= leveli:\n                #print(df[f'{m}'].value_counts())\n                tab = df[f'{m}'].unique()\n                summary.ix[i,'Levels']=f'{tab}'\n    summary['N'] = df.shape[0]\n    \n    \n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    numdf = df.select_dtypes(include=numerics)\n    cols_nums =numdf.columns\n    categoric = ['object']\n    catdf = df.select_dtypes(include=categoric)\n    cols_cat = catdf.columns#names of all the columns\n    \n\n    desc=df[cols_nums].describe().T\n    desc = desc.reset_index()\n    desc = pd.DataFrame(desc)\n    desc.rename(columns={f'{desc.columns[0]}':'Name'}, inplace=True)\n    desc.drop(['count'], axis=1,inplace=True)\n    desc = round(desc,2)\n   # desc\n\n    merged_inner=pd.merge(summary, desc, on='Name', how='outer')\n    merged_inner = merged_inner.replace(np.nan, '', regex=True)\n    merged_inner = merged_inner.sort_values('Missing_Perct',ascending=False)\n    merged_inner.to_excel('profiling.xlsx',index=False)\n    return merged_inner\n    \n    \ndef get_dataprofile(data,leveli):\n    print('DATA SHAPE',data.shape)\n    tempi = pd.DataFrame()\n    tempi = pd.DataFrame(data.dtypes,columns=['dtypes'])\n    tempi = tempi.reset_index()\n    tempi['Name'] = tempi['index']\n    tempi = tempi[['Name','dtypes']]\n    tempi['Missing_Count'] = data.isnull().sum().values\n    tempi['Missing_Perct'] = round(tempi['Missing_Count']\/data.shape[0]*100,2)\n    tempi['Uniques_Count'] = data.nunique().values\n    tempi['Uniques_Perct'] = round(tempi['Uniques_Count']\/data.shape[0]*100,2)\n\n    tempi['Zeros_count'] = data[data == 0].count(axis=0).values\n    tempi['Zeros_Perct'] = round(tempi['Zeros_count']\/data.shape[0]*100,2)\n\n    tempi['Ones_count'] = data[data == 1].count(axis=0).values\n    tempi['Ones_Perct'] = round(tempi['Ones_count']\/data.shape[0]*100,2)\n\n    tempi['mcp'] = np.nan\n\n    def mode_perC(data,coli):\n        #i =  'status_6'\n        xi = data[f'{i}'].value_counts(dropna=False)\n        xi = pd.DataFrame(xi)\n        xi.reset_index(inplace=True)\n        xi.rename(columns= {'index':'colanme',0:f'{i}'},inplace=True)\n        xi.sort_values(by=f'{i}')\n        mode_name = xi.iloc[0,0]\n        mode_count = xi.iloc[0,1]\n        mode_perC = round((mode_count\/data.shape[0])*100,3)\n        m = f'{mode_name}\/ {mode_count}\/ %{mode_perC}'\n        return m\n    \n\n# Computing MCp\n    for i in tempi['Name'].unique():\n        #print(mode_perC(data,f'{i}'))\n        idi = tempi[tempi['Name'] == f'{i}'].index\n        tempi.ix[idi,'mcp'] = mode_perC(data,f'{i}')\n        \n# Computing Levels\n    \n    tempi['Levels']= 'empty'\n    for i, m in enumerate (tempi['Name']):\n            #print(i,m)\n            if len(data[f'{m}'].value_counts()) <= leveli:\n                #print(data[f'{m}'].value_counts())\n                tab = data[f'{m}'].unique()\n                tempi.ix[i,'Levels']=f'{tab}'\n    tempi['N'] = data.shape[0]\n    \n    \n    \n    # Numerical describe func\n    \n    num_cols =get_numcolnames(data)\n\n\n    di =data[num_cols].describe().T\n    di.reset_index(inplace=True)\n    di.rename(columns={'index':'Name'},inplace=True)\n    \n    \n    ret_df =pd.merge(tempi, di, on='Name', how='outer')\n    ret_df = ret_df.replace(np.nan, '', regex=True)\n    ret_df = ret_df.sort_values('Missing_Perct',ascending=False)\n    \n    \n    ret_df = ret_df.round(3)\n    print('-'*50)\n    print('DATA TYPES:\\n',tempi['dtypes'].value_counts(normalize=False))\n    \n    \n    a = call_napercentage(data)\n    miss_col_great30 = a[a['na%']>30].shape[0]\n    miss_col_less30 = a[a['na%'] <=30].shape[0]\n    miss_col_equal2_0 = a[a['na%'] ==0].shape[0]\n    miss_col_equal2_100 = a[a['na%'] ==100].shape[0]\n    print('\\nMISSING REPORT:-')\n    print('-'*100,\n           '\\nTotal Observation                       :',tempi.shape[0],\n          '\\nNo of Columns with >30% data missing    :',miss_col_great30,\n          '\\nNo of Columns with <30% data missing    :',miss_col_less30,\n\n          '\\nNo of Columns with =0% data missing     :',miss_col_equal2_0,\n         '\\nNo of Columns with =100% data missing   :',miss_col_equal2_100,'\\n','-'*100)\n    \n    ret_df.to_excel('profile.xlsx',index=False)  \n\n    return(ret_df)\n\n\n# outlier treatment ----\n\ndef get_quatile_df(data):\n    return(pd.DataFrame.quantile(data,[0,0.01,0.02,0.03,0.04,.05,0.25,0.50,0.75,0.85,.95,0.99,0.991,0.992,0.993,0.994,0.995,0.995,0.996,0.997,0.998,0.8999]))\n\ndef get_box_plots(data):\n    for coli in get_numcolnames(data):\n        plt.figure(figsize=(15,9))\n        plt.boxplot(data[f'{coli}'],0,'gD')\n        plt.title(f'{coli}')\n        plt.show()\n        \ndef get_outliers(df,l_band , u_band):\n    Q1 = df.quantile(l_band)\n    Q3 = df.quantile(u_band)\n    IQR = Q3 - Q1\n    df = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]\n    return(df)\n\ndef get_binary_distribution(data,target):\n    f,ax=plt.subplots(1,2,figsize=(15,6))\n    data[f'{target}'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\n    ax[0].set_title(f'{target}')\n    ax[0].set_ylabel('')\n    sns.countplot(f'{target}',data=data,ax=ax[1])\n    ax[1].set_title(f'{target}')\n    plt.show()\n    \ndef get_plot_catVs_target(df,cat_col,y_lab,Top_n,thres,ytag,prt):\n    i = cat_col#\"Age_Of_Vehicle\"\n    y2 = y_lab#\"Renewed2\"\n    Top_n = Top_n #15\n    ytag = ytag\n    col_count  = df[f'{i}'].value_counts()\n    #print(col_count)\n    col_count = col_count[:Top_n,]\n\n    col_count1 = df[f'{i}'].value_counts(normalize=True)*100\n    col_count1 = col_count1[:Top_n,]\n    vol_inperc = col_count1.sum()\n    vol_inperc = round(vol_inperc,2)\n\n    tmp = pd.crosstab(df[f'{i}'], df[f'{y2}'], normalize='index') * 100\n    tmp = pd.merge(col_count, tmp, left_index=True, right_index=True)\n    tmp.rename(columns={0:'NotRenwed%', 1:'Renewed%'}, inplace=True)\n    if 'NotRenwed%' not in tmp.columns:\n        print(\"NotRenwed% is not present in \",i)\n        tmp['NotRenwed%'] = 0\n    if 'Renewed%' not in tmp.columns:\n        print(\"Renewed% is not present in \",i)\n        tmp['Renewed%'] = 0\n\n    tmp1 = pd.crosstab(df[f'{i}'], df[f'{y2}'])\n    tmp1.rename(columns={0:'NR_count', 1:'R_count'}, inplace=True)\n    if 'NR_count' not in tmp1.columns:\n        print(\"NR_count is not present in \",i)\n        tmp1['NR_count'] = 0\n    if 'R_count' not in tmp1.columns:\n        print(\"R_count is not present in \",i)\n        tmp1['R_count'] = 0\n\n    tmpz=pd.merge(tmp,tmp1,\n        left_index=True,\n        right_index=True)\n    tmpz['Tot'] = tmpz['NR_count'] + tmpz['R_count'] \n    tmpz['Renewed%'] =  round(tmpz['Renewed%'],2)\n    tmpz['Mean'] = (tmpz['R_count']\/tmpz['Tot'])*100\n    tmpz['Nperformer'] = np.where(tmpz['Renewed%'] < tmpz['Mean'] ,1,0)\n    #tmpz.sort_index(inplace=True)\n    if prt == 'Y':\n        gview(tmpz)\n        tmpzi = tmpz.reset_index()  \n        #tmpzii = tmpzi .join(pd.DataFrame(tmpzi.index.str.split('-').tolist()))\n        #tmpzi = pd.concat([tmpzi,DataFrame(tmpzi.index.tolist())], axis=1, join='outer')\n        #tmpzi.to_excel(\"tmpz.xlsx\")\n      \n   \n    plt.figure(figsize=(16,7))\n    g=sns.barplot(tmpz.index, tmpz[f'{i}'], alpha=0.8,order = col_count.index)\n    sns.set_style(\"whitegrid\", {'axes.grid' : False})\n    plt.title(f'{i} with {vol_inperc}%', fontsize = 16,color='blue')\n    #g.set_title(f'{i}')\n    plt.ylabel('Volume', fontsize=12)\n    plt.xlabel(f'{i}', fontsize=12)\n    plt.xticks(rotation=90)\n    for p in g.patches:\n        height = p.get_height()\n        g.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{}\\n{:1.2f}%'.format(round(height),height\/len(df)*100),\n            ha=\"center\", fontsize=10, color='blue')\n\n    gt = g.twinx()\n\n    if ytag == 1:\n        values = tmpz['Renewed%'].values # <--- store the values in a variable for easy access\n        gt = sns.pointplot(x=tmpz.index, y='Renewed%', data=tmpz, color='green', legend=True,order=tmpz.index)\n    if thres is np.nan:\n        gt.set_ylim(0,100)\n   \n\n    if ytag == 0:\n        values = tmpz['NotRenwed%'].values # <--- store the values in a variable for easy access\n        gt = sns.pointplot(x=tmpz.index, y='NotRenwed%', data=tmpz, color='red', legend=True,order=tmpz.index)\n        #gt.set_ylim(tmp['NotRenwed%'].min()-1,tmp['NotRenwed%'].max()+5)\n    if thres is np.nan:\n        gt.set_ylim(0,100)\n        \n        \n        \n    if thres is np.nan and ytag ==1: \n        gt.axhline(y=(tmpz['R_count'].sum()\/tmpz['Tot'].sum())*100, xmax=7, color='blue', linestyle='--')\n    else:\n        gt.axhline(y=thres, xmin=0, xmax=7, color='blue', linestyle='--')\n        \n    if thres is np.nan and ytag ==0: \n        gt.axhline(y=(tmpz['NR_count'].sum()\/tmpz['Tot'].sum())*100, xmax=7, color='blue', linestyle='--')\n    else:\n        gt.axhline(y=thres, xmin=0, xmax=7, color='blue', linestyle='--')\n        \n    #values = tmpz['Renewed%'].values # <--- store the values in a variable for easy access\n    j=0\n    for c in gt.collections:\n        for i, of in zip(range(len(c.get_offsets())), c.get_offsets()):\n            gt.annotate(values[j], of, color='brown', fontsize=12, rotation=45)\n            j += 1\n    plt.show()\n    \n    \ndef get_plot_ordVs_target(df,cat_col,y_lab,Top_n,thres,ytag,prt):\n    i = cat_col#\"Age_Of_Vehicle\"\n    y2 = y_lab#\"Renewed2\"\n    Top_n = Top_n #15\n    ytag = ytag\n    \n    #df[f'{i}'] = df[f'{i}'].astype(int)\n    col_count  = df[f'{i}'].value_counts()\n    #print(col_count)\n    col_count.sort_index(inplace=True)\n    col_count = col_count[:Top_n,]\n\n    col_count1 = df[f'{i}'].value_counts(normalize=True)*100\n    col_count1 = col_count1[:Top_n,]\n    vol_inperc = col_count1.sum()\n    vol_inperc = round(vol_inperc,2)\n\n    tmp = pd.crosstab(df[f'{i}'], df[f'{y2}'], normalize='index') * 100\n    tmp = pd.merge(col_count, tmp, left_index=True, right_index=True)\n    tmp.rename(columns={0:'NotRenwed%', 1:'Renewed%'}, inplace=True)\n    if 'NotRenwed%' not in tmp.columns:\n        print(\"NotRenwed% is not present in \",i)\n        tmp['NotRenwed%'] = 0\n    if 'Renewed%' not in tmp.columns:\n        print(\"Renewed% is not present in \",i)\n        tmp['Renewed%'] = 0\n\n    tmp1 = pd.crosstab(df[f'{i}'], df[f'{y2}'])\n    tmp1.rename(columns={0:'NR_count', 1:'R_count'}, inplace=True)\n    if 'NR_count' not in tmp1.columns:\n        print(\"NR_count is not present in \",i)\n        tmp1['NR_count'] = 0\n    if 'R_count' not in tmp1.columns:\n        print(\"R_count is not present in \",i)\n        tmp1['R_count'] = 0\n\n    tmpz=pd.merge(tmp,tmp1,\n        left_index=True,\n        right_index=True)\n    tmpz['Tot'] = tmpz['NR_count'] + tmpz['R_count'] \n    tmpz['Renewed%'] =  round(tmpz['Renewed%'],2)\n    tmpz['Mean'] = tmpz['Renewed%'].mean()\n    tmpz['Nperformer'] = np.where(tmpz['Renewed%'] < tmpz['Mean'] ,1,0)\n    #tmpz.sort_index(inplace=True)\n    \n    \n    if prt == 'Y':\n        print(tmpz)\n        tmpzi = tmpz.reset_index()  \n        #tmpzii = tmpzi .join(pd.DataFrame(tmpzi.index.str.split('-').tolist()))\n        #tmpzi = pd.concat([tmpzi,DataFrame(tmpzi.index.tolist())], axis=1, join='outer')\n        #tmpzi.to_excel(\"tmpz.xlsx\")\n      \n   \n    plt.figure(figsize=(16,7))\n    g=sns.barplot(tmpz.index, tmpz[f'{i}'], alpha=0.8)\n    sns.set_style(\"whitegrid\", {'axes.grid' : False})\n    plt.title(f'{i} with {vol_inperc}%', fontsize = 16,color='blue')\n    #g.set_title(f'{i}')\n    plt.ylabel('Volume', fontsize=12)\n    plt.xlabel(f'{i}', fontsize=12)\n    plt.xticks(rotation=90)\n    for p in g.patches:\n        height = p.get_height()\n        g.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{}\\n{:1.2f}%'.format(round(height),height\/len(df)*100),\n            ha=\"center\", fontsize=10, color='blue')\n\n    gt = g.twinx()\n\n    if ytag == 1:\n        values = tmpz['Renewed%'].values # <--- store the values in a variable for easy access\n        gt = sns.pointplot(x=tmpz.index, y='Renewed%', data=tmpz, color='green', legend=True)\n    \n    if ytag == 0:\n        values = tmpz['NotRenwed%'].values # <--- store the values in a variable for easy access\n        gt = sns.pointplot(x=tmpz.index, y='NotRenwed%', data=tmpz, color='red', legend=True)\n        gt.set_ylim(tmp['NotRenwed%'].min()-1,tmp['NotRenwed%'].max()+5)\n        \n        \n    if thres is np.nan:\n        gt.set_ylim(0,100)\n        gt.axhline(y=(tmpz['R_count'].sum()\/tmpz['Tot'].sum())*100, xmax=7, color='blue', linestyle='--')\n    else:\n        gt.set_ylim(0,100)\n        gt.axhline(y=thres, xmin=0, xmax=7, color='blue', linestyle='--')\n   \n\n\n        \n        \n    if thres is np.nan and ytag ==1: \n        gt.axhline(y=(tmpz['R_count'].sum()\/tmpz['Tot'].sum())*100, xmax=7, color='blue', linestyle='--')\n    else:\n        gt.axhline(y=thres, xmin=0, xmax=7, color='blue', linestyle='--')\n        \n    if thres is np.nan and ytag ==0: \n        gt.axhline(y=(tmpz['NR_count'].sum()\/tmpz['Tot'].sum())*100, xmax=7, color='blue', linestyle='--')\n    else:\n        gt.axhline(y=thres, xmin=0, xmax=7, color='blue', linestyle='--')\n        \n    #values = tmpz['Renewed%'].values # <--- store the values in a variable for easy access\n    j=0\n    for c in gt.collections:\n        for i, of in zip(range(len(c.get_offsets())), c.get_offsets()):\n            gt.annotate(values[j], of, color='brown', fontsize=12, rotation=45)\n            j += 1\n    plt.show()\n    \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor    \n\ndef calculate_vif_(X, thresh=5.0):\n    variables = list(range(X.shape[1]))\n    dropped = True\n    while dropped:\n        dropped = False\n        vif = [variance_inflation_factor(X.iloc[:, variables].values, ix)\n               for ix in range(X.iloc[:, variables].shape[1])]\n\n        maxloc = vif.index(max(vif))\n        if max(vif) > thresh:\n            print('dropping \\'' + X.iloc[:, variables].columns[maxloc] +\n                  '\\' at index: ' + str(maxloc))\n            del variables[maxloc]\n            dropped = True\n\n    print('Remaining variables:')\n    print(X.columns[variables])\n    return X.iloc[:, variables]\n\n# Confusion Matrix function\n\ndef plot_confusion_matrix(cm, classes=None, title='Confusion matrix'):\n    \"\"\"Plots a confusion matrix.\"\"\"\n    if classes is not None:\n        sns.heatmap(cm, cmap=\"YlGnBu\", xticklabels=classes, yticklabels=classes, vmin=0., vmax=1., annot=True, annot_kws={'size':50})\n    else:\n        sns.heatmap(cm, vmin=0., vmax=1.)\n    plt.title(title)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \ndef get_distplot(df,icolumns):\n    plt.figure(figsize=(12,10))\n    sns.distplot(df[f'{icolumns}'], color='g', label = \"close%\") \n    plt.xlabel(f\"{icolumns}\")\n    plt.ylabel(\"Frequency\")\n    plt.title(f\"Distribution {icolumns}\", fontsize=14)\n    plt.legend()\n    plt.show()","86cd5dcd":"data_locn = '\/kaggle\/input\/stock-market-next-day-close-change-prediction\/train_24_june_2020.csv'\ndf=pd.read_csv(f'{data_locn}')","44a56713":"df.head()","a814a4d6":"df.columns","5e839833":"id_col = 'tday_stock_name'\ny_name = 'target'\nx_name = [i for i in df.columns if i not in [f'{id_col}',f'{y_name}','close_diff%','close_diff','close_prev%']]","3155106f":"x =  df[x_name]\ny =  df[y_name]","ef2d67da":"x_tr , x_te , y_tr , y_te = train_test_split(x , y , \n                                               test_size = 0.30,stratify = y,\n                                               random_state =  SEED )","8ee93be7":"cat_features_names = get_catcolnames(x_tr) # here we specify names of categorical features\ncat_features = [x_tr.columns.get_loc(col) for col in cat_features_names]\nprint(cat_features)","377ca933":"params = {'loss_function':'MultiClass', # objective function\n          'eval_metric':'TotalF1', # metric\n          'cat_features': cat_features,\n          'early_stopping_rounds': 500,\n          'verbose': 200, # output to stdout info about training process every 200 iterations\n          'random_seed': SEED,\n          'iterations': 1000\n         }","1ccda4a1":"cbc_1 = CatBoostClassifier(**params)\ncbc_1.fit(x_tr, y_tr, # data to train on (required parameters, unless we provide X as a pool object, will be shown below)\n          eval_set=(x_te, y_te), # data to validate on\n          use_best_model=True, # True if we don't want to save trees created after iteration with the best validation score\n          plot=True # True for visualization of the training process (it is not shown in a published kernel - try executing this code)\n         );","db2e8960":"feat_importances = pd.Series(cbc_1.feature_importances_, index=x_tr.columns)\nfeat_importances.nlargest(30).sort_values(ascending = True).plot(kind='barh',figsize=(15,7))","c9cabe7d":"vi = pd.DataFrame(feat_importances.sort_values(ascending=False)).reset_index()\nvi.rename(columns= {'index':'varname',0:'score'},inplace=True)\nvi","41ec3f75":"ilist = vi[0:20]['varname'].tolist()\nlen(ilist)\nilist","86fcff88":"x =  df[ilist]\ny =  df[y_name]","735f332b":"x_tr , x_te , y_tr , y_te = train_test_split(x , y , \n                                               test_size = 0.30,stratify = y,\n                                               random_state =  SEED )","0a2d4ee8":"cat_features_names = get_catcolnames(x_tr) # here we specify names of categorical features\ncat_features = [x_tr.columns.get_loc(col) for col in cat_features_names]\nprint(cat_features)","cd476d55":"params = {'loss_function':'MultiClass', # objective function\n          'eval_metric':'TotalF1', # metric\n          'cat_features': cat_features,\n          'early_stopping_rounds': 500,\n          'verbose': 200, # output to stdout info about training process every 200 iterations\n          'random_seed': SEED,\n          'iterations': 2000\n         }","089da50c":"cbc_2 = CatBoostClassifier(**params)\ncbc_2.fit(x_tr, y_tr, # data to train on (required parameters, unless we provide X as a pool object, will be shown below)\n          eval_set=(x_te, y_te), # data to validate on\n          use_best_model=True, # True if we don't want to save trees created after iteration with the best validation score\n          plot=True # True for visualization of the training process (it is not shown in a published kernel - try executing this code)\n         );","ea1857e0":"feat_importances = pd.Series(cbc_2.feature_importances_, index=x_tr.columns)\nfeat_importances.nlargest(30).sort_values(ascending = True).plot(kind='barh',figsize=(15,7))","88cadc7e":"y_pred_val = cbc_2.predict(x_te)","28a77c1a":"y_pred_test = cbc_2.predict(x_te)\ny_prob_test = cbc_2.predict_proba(x_te)","6130f76a":"y_prob_test","592aaf9c":"y_pred_test","4afb749b":"TO BE CONTINIUE ...WORKING ON CREATION OF DATA + FEATURE ENGINEERING ................!! COMING SOON"}}