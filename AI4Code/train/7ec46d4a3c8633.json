{"cell_type":{"b9dcd2fd":"code","6283c635":"code","39508cdd":"code","fa7c7630":"code","d4e6739e":"code","9b039e90":"code","e388eae0":"code","043b0eec":"code","51c411e0":"code","a887aeb4":"code","a50d281f":"code","f8be806f":"code","bb103189":"code","9d06d167":"code","f130d524":"code","b26ae7e9":"code","634c1cfe":"code","2f4f6430":"code","6d056e35":"code","d883b9ba":"code","c7160673":"code","4d26ea68":"code","b499bd9c":"code","cd6cfcf8":"code","65906b01":"code","de1de7c4":"code","9c41998b":"code","b8fd5b7f":"markdown","373ed56b":"markdown","38fedf94":"markdown","f7bea787":"markdown","c6b99767":"markdown","7723d547":"markdown","48d74f0a":"markdown","ab1f536d":"markdown","20416d37":"markdown","07d96aec":"markdown","35786e9a":"markdown","f96d7e10":"markdown"},"source":{"b9dcd2fd":"# Libaries for data read\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\n# Libaries for visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Libaries for model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_val_score\n\n# other's\nimport warnings","6283c635":"# Let's hide Warnings\nwarnings.filterwarnings(\"ignore\")","39508cdd":"df = pd.read_csv(\"..\/input\/vehicle-dataset-from-cardekho\/car data.csv\")\ndf.head()","fa7c7630":"# Let's check the missing value \ndf.isnull().sum() # There is no missing value feature","d4e6739e":"# Let's Final Dataset without car name feature\ndataset = df.drop([\"Car_Name\"], axis=1)\ndataset.head()","9b039e90":"# Let's create a current year feature\ncurrent_year = datetime.now().year\ndataset[\"Current_year\"] = current_year\ndataset.head()","e388eae0":"# Let's create Number of years the car uses \ndataset[\"Num_Years\"] = dataset[\"Current_year\"] - dataset[\"Year\"]\ndataset.head()","043b0eec":"# Let's drop unnecessary features like- \"Year\", \"Current_year\"\ndataset.drop([\"Year\", \"Current_year\"], axis=1, inplace=True)\ndataset.head()","51c411e0":"# Let's find categorical features \ncate_features = [feature for feature in dataset.columns if dataset[feature].dtypes == \"O\"]\ncate_features","a887aeb4":"# Let's create a function to do \"One Hot Encoding\" of Categorical Features \nencode = pd.get_dummies(dataset[cate_features], drop_first=True)\n# concatanet with dataset\ndata = pd.concat([dataset, encode], axis=1)\ndata.head()","a50d281f":"# Make final dataset for model\nfinal_dataset = data.drop(cate_features, axis=1)\nfinal_dataset.head()","f8be806f":"final_dataset.corr()","bb103189":"corr = final_dataset.corr()\ncorr_features = corr.index\nplt.figure(figsize=(15,10))\nsns.heatmap(final_dataset[corr_features].corr(), annot=True)","9d06d167":"# Lets divide X & Y dataset\nX = final_dataset.iloc[:, 1:]\ny = final_dataset.iloc[:, 0]","f130d524":"# Features Importance \nfrom sklearn.ensemble import ExtraTreesRegressor\nmodel = ExtraTreesRegressor()\nmodel.fit(X, y)\nprint(model.feature_importances_)","b26ae7e9":"# Let's find best 5 features by using plot\nfeature_importance = pd.Series(model.feature_importances_, index=X.columns)\nfeature_importance.nlargest(5).plot(kind='barh')\nplt.show()","634c1cfe":"# Let's split train and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=40)","2f4f6430":"# create model and train the model with Random Forest Regressor\ncpp_model = RandomForestRegressor()\ncpp_model.fit(X_train, y_train)","6d056e35":"# Let's test the model\ny_pred = cpp_model.predict(X_test)\ny_pred","d883b9ba":"# Let's compare test result with actual\npred_dataset = pd.DataFrame({\"Actual_Data\": y_test, \"Predict_Data\": y_pred})\npred_dataset.head()","c7160673":"# Model score\ncpp_model.score(X_train, y_train)","4d26ea68":"# r2 Score of Model\nR2Score = r2_score(y_test, y_pred)\nR2Score","b499bd9c":"cpp_xgbr_model = XGBRegressor()\ncpp_xgbr_model.fit(X_train, y_train)\ny_pred_xgb = cpp_xgbr_model.predict(X_test)\ny_pred_xgb","cd6cfcf8":"# Let's compare test result with actual\nxgb_pred_dataset = pd.DataFrame({\"Actual_Data\": y_test, \"Predict_Data\": y_pred_xgb})\nxgb_pred_dataset.head()","65906b01":"# Model score\ncpp_xgbr_model.score(X_train, y_train)","de1de7c4":"# r2 Score of Model\nr2_score(y_test, y_pred_xgb)","9c41998b":"cscore = cross_val_score(cpp_xgbr_model, X_train, y_train.ravel(), cv=5)\ncscore.mean()","b8fd5b7f":"## Build a Model","373ed56b":"##### Read Dataset","38fedf94":"##### Handle year feature","f7bea787":"### Features Importance finding\n    If we have so many features, then we can work on Features Importance  ","c6b99767":"##### Handle Categorical features","7723d547":"#### Let's check the model score","48d74f0a":"##### Handle Mising \/ NAN value","ab1f536d":"##### Import necessary Libaries","20416d37":"##### Data Cleaning ","07d96aec":"#### Let's Use XGBoost Regressor","35786e9a":"##### Lets cross validation","f96d7e10":"### Features Correlations\n    If we have so many features then we can check correlation among those features "}}