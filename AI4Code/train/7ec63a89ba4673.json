{"cell_type":{"0c1542f6":"code","3f9cca46":"code","dd8e7a5d":"code","d357f554":"code","b1326999":"code","c4967615":"code","ab32ab40":"code","c68ecc23":"code","670595b9":"code","cc526710":"code","1de0b2fa":"code","bba0e409":"code","d2f15e77":"markdown","c67ffc30":"markdown","fad84569":"markdown","3720d943":"markdown","7263cb42":"markdown","e72d3947":"markdown","21443e38":"markdown","fc87c070":"markdown","d0358255":"markdown","4b228b3b":"markdown","f6b5905a":"markdown","ef15879a":"markdown","b6da39cb":"markdown","1930d0f9":"markdown"},"source":{"0c1542f6":"# standard Python tools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# special tools for working in Kaggle\nimport joblib   # save and load ML models\nimport gc       # garbage collection\nimport os \nimport sklearn\n\n# preprocessing steps\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n# machine learning models and tools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\n\n# cross validation and metrics - remember this competition is scored as area under curve\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n# surely there will be a lot more packages loaded by the time we are done!","3f9cca46":"MainDir = \"..\/input\/..\/input\/home-credit-default-risk\"\nprint(os.listdir(MainDir))\n\n# Main table\ntrain = pd.read_csv(f'{MainDir}\/application_train.csv')\n\ntrain.head(5)","dd8e7a5d":"# Create a stratified sample: reduce number of observations to 10,000, but keep original proportion of 0s and 1s in TARGET\nprint(1 - train['TARGET'].mean())   # full data set is 91.93% zero and 8.07% one.\n\nn = 10000         # set sample size\ntrain10K = train.groupby('TARGET', group_keys=False).apply(lambda x: x.sample(int(np.rint(n*len(x)\/len(train))))).sample(frac=1).reset_index(drop=True)\n\n# did that work? Yes, 9193 out of 10,000 in our sample are zeroes.\n(train10K['TARGET'].value_counts() \/ len(train10K)).to_frame()","d357f554":"# Selected features:\nnum_features = ['CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'OWN_CAR_AGE']\ncat_features = ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE']\nfeatures = num_features + cat_features\n\n#Pipeline:\nPipe_num = Pipeline(\n    steps=[\n    ('imputer', SimpleImputer(strategy = 'median')),\n    ('scaler', StandardScaler())       ])\n\nPipe_cat = Pipeline(\n    steps=[\n    ('imputer', SimpleImputer(strategy = 'constant', fill_value = 'Unknown')),\n    ('onehot', OneHotEncoder())        ])\n\n#ColumnTransformer:\n\npreprocessor = ColumnTransformer(\n    transformers = [\n        ('num', Pipe_num, num_features),\n        ('cat', Pipe_cat, cat_features)])","b1326999":"# set up table\nresults = pd.DataFrame(columns = ['Model Type','AreaUnderCurve', 'Accuracy', 'Hyperparameters'])\nresults","c4967615":"preprocessor.fit(train10K[features])\nX_train = preprocessor.transform(train10K[features])\n\ny_train = train10K.TARGET.values\n\nprint('X_train shape:', X_train.shape)\nprint('y_train shape:', y_train.shape)","ab32ab40":"%%time\n\n# sorted(sklearn.metrics.SCORERS.keys())    <--- need metric roc_auc\n\nlr_clf = LogisticRegression(max_iter=1000, solver='saga', penalty = 'elasticnet')\n\nlr_parameters = {'l1_ratio':[0, 0.3, 0.6, 1], 'C': [0.01, 0.1, 0.3, 1, 3]}\n\nlr_grid = GridSearchCV(lr_clf, lr_parameters, cv=10, refit='True', n_jobs=-1, verbose=10, scoring='roc_auc')\nlr_grid.fit(X_train, y_train)\n\nlr_model = lr_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'Logistic Regression',\n                          'AreaUnderCurve' : lr_grid.best_score_,\n                          'Accuracy' : lr_model.score(X_train, y_train),\n                          'Hyperparameters' : lr_grid.best_params_},\n                        ignore_index=True)\nresults","c68ecc23":"%%time\n\ndt_clf = DecisionTreeClassifier(random_state=1)\n\ndt_parameters = {\n    'max_depth': [4, 8, 12, 16, 20, 24],\n    'min_samples_leaf': [2, 4, 6, 8]\n}\n\ndt_grid = GridSearchCV(dt_clf, dt_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring='roc_auc')\ndt_grid.fit(X_train, y_train)\n\ndt_model = dt_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'Decision Tree',\n                          'AreaUnderCurve' : dt_grid.best_score_,\n                          'Accuracy' : dt_model.score(X_train, y_train),\n                          'Hyperparameters' : dt_grid.best_params_},\n                        ignore_index=True)\nresults","670595b9":"%%time\n\nrf_clf = RandomForestClassifier(random_state=1, n_estimators=100)\n\nrf_parameters = {'max_depth': [4, 8, 12, 16, 20],  'min_samples_leaf': [4, 6, 8, 10, 12]}\n\nrf_grid = GridSearchCV(rf_clf, rf_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring='roc_auc')\nrf_grid.fit(X_train, y_train)\n\nrf_model = rf_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'Random Forest',\n                          'AreaUnderCurve' : rf_grid.best_score_,\n                          'Accuracy' : rf_model.score(X_train, y_train),\n                          'Hyperparameters' : rf_grid.best_params_},\n                        ignore_index=True)\nresults","cc526710":"plt.figure(figsize=[18,4])\nplt.subplot(1,3,1)\nlr_summary = pd.DataFrame(lr_grid.cv_results_['params'])\nlr_summary['cv_score'] = lr_grid.cv_results_['mean_test_score']\n\nfor r in lr_parameters['l1_ratio']:\n    temp = lr_summary.query(f'l1_ratio == {r}')\n    plt.plot(temp.C, temp.cv_score, label=r)\nplt.xscale('log')\nplt.xlabel('Regularization Parameter (C)')\nplt.ylabel('CV Score')\nplt.legend(title='L1 Ratio', loc='lower right')\nplt.grid()\n\nplt.subplot(1,3,2)\ndt_summary = pd.DataFrame(dt_grid.cv_results_['params'])\ndt_summary['cv_score'] = dt_grid.cv_results_['mean_test_score']\n\nfor ms in dt_parameters['min_samples_leaf']:\n    temp = dt_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximum Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\n\nplt.subplot(1,3,3)\nrf_summary = pd.DataFrame(rf_grid.cv_results_['params'])\nrf_summary['cv_score'] = rf_grid.cv_results_['mean_test_score']\n\nfor ms in rf_parameters['min_samples_leaf']:\n    temp = rf_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximum Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\nplt.show()","1de0b2fa":"# The decision tree model shows a handful of people as having a >50% chance of default. But we are scored on AUC, not on accuracy.\n\n# Generate confusion matrix\npredictions = dt_model.predict(X_train)\nmatrix = pd.DataFrame(confusion_matrix(train10K['TARGET'], predictions)) \n\n# plot as seaborn heatmap\nax= plt.subplot()\nsns.heatmap(matrix, annot = True, cmap = \"BuPu\", fmt='g', cbar = False)\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nplt.show()","bba0e409":"final_model = RandomForestClassifier(random_state=1, n_estimators=25, max_depth = 4, min_samples_leaf = 2)\nfinal_model.fit(X_train, y_train)\n\njoblib.dump(preprocessor, 'default_preprocessor.joblib') \njoblib.dump(final_model, 'default_model_01.joblib')","d2f15e77":"### Logistic Regression","c67ffc30":"### Sidebar - confusion matrix","fad84569":"### Model plots","3720d943":"### Final Model Selection - save data","7263cb42":"## Home Credit Default Risk - Team 3 (Kahsai, Nichols, Pellerito)","e72d3947":"### Training data - select smaller sample and limited features for first run","21443e38":"### Random Forest","fc87c070":"### Feature selection (just a few variables for now) and pipeline","d0358255":"### Read the training data","4b228b3b":"### Import packages","f6b5905a":"# Models","ef15879a":"# First look at training data set","b6da39cb":"### Decision Tree","1930d0f9":"### Build Model Scoreboard"}}