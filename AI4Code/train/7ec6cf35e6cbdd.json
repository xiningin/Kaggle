{"cell_type":{"0edba868":"code","e838a59c":"code","5f1d7e74":"code","76945bbb":"code","8499af05":"code","ca5e49cb":"code","4e37bb3e":"code","dfd8266c":"code","1bd93c9e":"code","9354c2e7":"code","b9fe517c":"code","c69c2678":"code","0a4a4ae9":"code","b7a30da8":"code","49087493":"code","d6b24210":"code","2d55aa16":"code","fdd42515":"code","1e34ef98":"code","3775eb99":"code","75043d09":"code","e7ef155d":"code","f30a27a6":"markdown","43c34d2f":"markdown","3b8a1b80":"markdown","6aefbc90":"markdown","52b274ea":"markdown","741feb10":"markdown","602cfcc6":"markdown","c9e6359c":"markdown","ce13fa29":"markdown","c01597ad":"markdown","012b2b92":"markdown","990da2aa":"markdown","b6b974c0":"markdown","dbf52c37":"markdown","25e3fd3c":"markdown","e06fd28d":"markdown","bd23e648":"markdown","a41e6d54":"markdown","37821e08":"markdown","3106b7dd":"markdown","3c29de01":"markdown","db65ef66":"markdown","b8f829ff":"markdown"},"source":{"0edba868":"from IPython.core.display import display, HTML","e838a59c":"%%capture\n\n!wget \"https:\/\/download.java.net\/java\/GA\/jdk11\/9\/GPL\/openjdk-11.0.2_linux-x64_bin.tar.gz\"\n!tar -xvf openjdk-11.0.2_linux-x64_bin.tar.gz\n\n!export JAVA_HOME='\/kaggle\/working\/jdk-11.0.2\/'\n!export PATH='\/kaggle\/working\/jdk-11.0.2\/bin':$PATH","5f1d7e74":"# %%capture\n!mkdir -p \/kaggle\/working\/jdk-11.0.2\/jre\/lib\/amd64\/server\/\n!ln -s \/kaggle\/working\/jdk-11.0.2\/lib\/server\/libjvm.so \/kaggle\/working\/jdk-11.0.2\/jre\/lib\/amd64\/server\/libjvm.so","76945bbb":"# !ls -la \/kaggle\/working\/jdk-11.0.2\/lib\/server\/\n# !ls -la \/kaggle\/working\/jdk-11.0.2\/jre\/lib\/amd64\/server\/","8499af05":"import json\nimport os\n\n# linux\nos.environ[\"JAVA_HOME\"] = \"\/kaggle\/working\/jdk-11.0.2\/\"","ca5e49cb":"%%capture\n!pip install pyserini==0.8.1.0\n!pip install transformers","4e37bb3e":"%%capture\n!wget https:\/\/www.dropbox.com\/s\/j1epbu4ufunbbzv\/lucene-index-covid-2020-03-27.tar.gz\n!tar xvfz lucene-index-covid-2020-03-27.tar.gz","dfd8266c":"!du -h lucene-index-covid-2020-03-27","1bd93c9e":"import torch\nimport numpy\nfrom tqdm import tqdm\nfrom transformers import *\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","9354c2e7":"# tokenizer = AutoTokenizer.from_pretrained('monologg\/biobert_v1.1_pubmed', do_lower_case=False)\n# model = AutoModel.from_pretrained('monologg\/biobert_v1.1_pubmed')","b9fe517c":"tokenizer = AutoTokenizer.from_pretrained('..\/input\/scibertcord19checkpoint950000\/checkpoint-950000', do_lower_case=False)\nmodel = AutoModel.from_pretrained('..\/input\/scibertcord19checkpoint950000\/checkpoint-950000')","c69c2678":"COVID_INDEX = 'lucene-index-covid-2020-03-27\/'\n\ndef show_query(query):\n    \"\"\"HTML print format for the searched query\"\"\"\n    return HTML('<br\/><div style=\"font-family: Times New Roman; font-size: 20px;'\n                'padding-bottom:12px\"><b>Query<\/b>: '+query+'<\/div>')\n\ndef show_document(idx, doc):\n    \"\"\"HTML print format for document fields\"\"\"\n    have_body_text = 'body_text' in json.loads(doc.raw)\n    body_text = ' Full text available.' if have_body_text else ''\n    return HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:10px\">' + \n               f'<b>Document {idx}:<\/b> {doc.docid} ({doc.score:1.2f}) -- ' +\n               f'{doc.lucene_document.get(\"authors\")} et al. ' +\n             # f'{doc.lucene_document.get(\"journal\")}. ' +\n             # f'{doc.lucene_document.get(\"publish_time\")}. ' +\n               f'{doc.lucene_document.get(\"title\")}. ' +\n               f'<a href=\"https:\/\/doi.org\/{doc.lucene_document.get(\"doi\")}\">{doc.lucene_document.get(\"doi\")}<\/a>.'\n               + f'{body_text}<\/div>')\n\ndef show_query_results(query, searcher, top_k=10):\n    \"\"\"HTML print format for the searched query\"\"\"\n    hits = searcher.search(query)\n    display(show_query(query))\n    for i, hit in enumerate(hits[:top_k]):\n        display(show_document(i+1, hit))\n    return hits[:top_k]   ","0a4a4ae9":"from pyserini.search import pysearch\n\nsearcher = pysearch.SimpleSearcher(COVID_INDEX)\nquery = ('these differences reside in the molecular structure of spike proteins and some other factors. Which receptor combination(s) will cause maximum harm')\nhits = show_query_results(query, searcher, top_k=10)","b7a30da8":"def extract_scibert(text, tokenizer, model):\n    text_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n    text_words = tokenizer.convert_ids_to_tokens(text_ids[0])[1:-1]\n\n    n_chunks = int(numpy.ceil(float(text_ids.size(1))\/510))\n    states = []\n    \n    for ci in range(n_chunks):\n        text_ids_ = text_ids[0, 1+ci*510:1+(ci+1)*510]            \n        text_ids_ = torch.cat([text_ids[0, 0].unsqueeze(0), text_ids_])\n        if text_ids[0, -1] != text_ids[0, -1]:\n            text_ids_ = torch.cat([text_ids_, text_ids[0,-1].unsqueeze(0)])\n        \n        with torch.no_grad():\n            state = model(text_ids_.unsqueeze(0))[0]\n            state = state[:, 1:-1, :]\n        states.append(state)\n\n    state = torch.cat(states, axis=1)\n    return text_ids, text_words, state[0]","49087493":"query_ids, query_words, query_state = extract_scibert(query, tokenizer, model)","d6b24210":"ii = 0\ndoc_json = json.loads(hits[ii].raw)\n\nparagraph_states = []\nfor par in tqdm(doc_json['body_text']):\n    state = extract_scibert(par['text'], tokenizer, model)\n    paragraph_states.append(state)","2d55aa16":"def cross_match(state1, state2):\n    state1 = state1 \/ torch.sqrt((state1 ** 2).sum(1, keepdims=True))\n    state2 = state2 \/ torch.sqrt((state2 ** 2).sum(1, keepdims=True))\n    sim = (state1.unsqueeze(1) * state2.unsqueeze(0)).sum(-1)\n    return sim","fdd42515":"sim_matrices = []\nfor pid, par in tqdm(enumerate(doc_json['body_text'])):\n    sim_score = cross_match(query_state, paragraph_states[pid][-1])\n    sim_matrices.append(sim_score)","1e34ef98":"paragraph_relevance = [torch.max(sim).item() for sim in sim_matrices]\n\n# Select the index of top 5 paragraphs with highest relevance\nrel_index = numpy.argsort(paragraph_relevance)[-5:][::-1]","3775eb99":"def show_sections(section, text):\n    \"\"\"HTML print format for document subsections\"\"\"\n    return HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:10px; margin-left: 15px\">' + \n        f'<b>{section}<\/b> -- {text.replace(\" ##\",\"\")} <\/div>')\n\ndisplay(show_query(query))\ndisplay(show_document(ii, hits[ii]))\nfor ri in numpy.sort(rel_index):\n    display(show_sections(doc_json[\"body_text\"][ri]['section'], \" \".join(paragraph_states[ri][1])))","75043d09":"def highlight_paragraph(ptext, rel_words, max_win=10):\n    para = \"\"\n    prev_idx = 0\n    for jj in rel_words:\n        \n        if prev_idx > jj:\n            continue\n        \n        found_start = False\n        for kk in range(jj, prev_idx-1, -1):\n            if ptext[kk] == \".\" and (ptext[kk+1][0].isupper() or ptext[kk+1][0] == '['):\n                sent_start = kk\n                found_start = True\n                break\n        if not found_start:\n            sent_start = prev_idx-1\n            \n        found_end = False\n        for kk in range(jj, len(ptext)-1):\n            if ptext[kk] == \".\" and (ptext[kk+1][0].isupper() or ptext[kk+1][0] == '['):\n                sent_end = kk\n                found_end = True\n                break\n                \n        if not found_end:\n            if kk >= len(ptext) - 2:\n                sent_end = len(ptext)\n            else:\n                sent_end = jj\n        \n        para = para + \" \"\n        para = para + \" \".join(ptext[prev_idx:sent_start+1])\n        para = para + \" <font color='blue'>\"\n        para = para + \" \".join(ptext[sent_start+1:sent_end])\n        para = para + \"<\/font> \"\n        prev_idx = sent_end\n        \n    if prev_idx < len(ptext):\n        para = para + \" \".join(ptext[prev_idx:])\n\n    return para","e7ef155d":"display(show_query(query))\n\ndisplay(show_document(ii, hits[ii]))\n\nfor ri in numpy.sort(rel_index):\n    sim = sim_matrices[ri].data.numpy()\n    \n    # Select the two highest scoring words in the paragraph\n    rel_words = numpy.sort(numpy.argsort(sim.max(0))[-2:][::-1])\n    p_tokens = paragraph_states[ri][1]\n    para = highlight_paragraph(p_tokens, rel_words)\n    display(show_sections(doc_json[\"body_text\"][ri]['section'], para))","f30a27a6":"Let's grab the pre-built index:","43c34d2f":"We want to look at more details by highlighting relevant phrases in each paragraph, where we define relevant phrases for each paragraph as\n\n$$\\arg\\text{top-$M$}_{j=1,\\ldots, |\\text{paragraph}^k|} \\max_{i=1,\\ldots,|\\text{query}|} A_{ij}^k$$\n\nthat is, any word that had a high similarity to each of the query words is considered relevant. given these words, we highlight a window of 10 surrounding each of them.","3b8a1b80":"Let's extract contextualized vectors of queries and abstracts from SciBERT for highlighting relevant paragraphs.","6aefbc90":"Pyserini needs java - it comes with the anserini jar bundled. Try using at least Java 11, otherwise there is a known issue that may cause problems - https:\/\/github.com\/castorini\/pyserini#known-issues.","52b274ea":"Next, install Python dependencies","741feb10":"Let's load BioBERT (https:\/\/arxiv.org\/abs\/1901.08746) from HuggingFace Transformers","602cfcc6":"Second, let's extract contextualized vectors of all the paragraphs from the hit #7:\n\n$$p_1^k, \\ldots, p_{T_k}^k = \\text{SciBERT}(\\text{paragraph}^k)$$","c9e6359c":"Instead, load fine-tuned SciBERT from a local directory. The model was fine-tuned on the dataset from the CORD-19 challenge and saved.","ce13fa29":"# Pyserini Demo on COVID-19 Dataset (Title + Abstract Index) with HuggingFace Transformers based visualization\n","c01597ad":"## Diagram of contribution\n\n<!-- <img src=\"attachment:cord-logo-final.png\" width=\"640\"> -->\n![cord-logo-final.png](attachment:cord-logo-final.png)\n\n**Figure 1.** Our pipeline for tackling the CORD challenge. Our key contributions are as follows: 1) SciBERT that is finetuned on the CORD dataset; 2) QA using TF-IDF and Kendra; 3) IR usingh PySerini (with our fine-tuned SciBERT) and Kendra; 4) Abstractive summarization of Kendra\u2019s IR using a state-of-the-art SciBERT seq2seq summarization model, trained on scientific articles and their corresponding press releases in Science Daily.\n\n## Finetuning SciBERT. \n\nWe adapted Huggingface\u2019s `run_language_modeling.py` script and fine-tuned SciBERT on the CORD dataset for 950,000 steps.\n","012b2b92":"This notebook is a based on the original pyserini notebook but adds a sciBERT model that is fine-tuned on the CORD-19 dataset. The model is used for question answering on top of the retrieved (relevant) documents.","990da2aa":"This notebook provides a demo on how to get started in searching the [COVID-19 Open Research Dataset](https:\/\/pages.semanticscholar.org\/coronavirus-research) (release of 2020\/03\/20) from AI2.\nIn this notebook, we'll be working with the title + abstract index. \nSpecifically, we're not indexing the full text (that'll come later, soon!).\n","b6b974c0":"We then compute the cosine similarity matrix between the query and each paragraph:\n\n$$A^k = [a^k_{ij}] \\in \\mathbb{R}^{|\\text{query}| \\times |\\text{paragraph}^k|},$$\n\nwhere\n\n$$a^k_{ij} = \\frac{q_i^\\top p_j^k}{\\| q_i \\| \\| p_j^k \\|}$$\n","dbf52c37":"You can use `pysearch` to search over an index. Here's the basic usage:","25e3fd3c":"Sanity check of index size (should be 1.3G):","e06fd28d":"First, install a proper java version (11) to make pyserini happy. (https:\/\/github.com\/castorini\/anserini\/issues\/832)","bd23e648":"---","a41e6d54":"Let's retrieve the most relevant paragraphs first, where define the top-$M$ most relevant paragraphs as \n\n$$\\arg\\text{top-$M$}_{k=1}^K \\max_{i=1,\\ldots,|\\text{query}|} \\max_{j=1,\\ldots, |\\text{paragraph}^k|} A_{ij}^k$$\n\nthat is, a paragraph with the highly matched words to the query words is considered relevant.","37821e08":"From the hits array, use `.lucene_document` to access the underlying indexed Lucene `Document`, and from there, call `.get(field)` to fetch specific fields, like \"title\", \"doc\", etc.\nThe complete list of available fields is [here](https:\/\/github.com\/castorini\/anserini\/blob\/master\/src\/main\/java\/io\/anserini\/index\/generator\/CovidGenerator.java#L46).","3106b7dd":"## Credits:\n * This notebook is based on the one provided by the pyserini team.\n * It is part of a collaborative effort with @atanasova, @charlotteloh, @darumen, @preslav","3c29de01":"## Abstractive summarization on Kendra\u2019s IR.\n\nWe used a custom dataset consisting of pairs of scientific articles and their corresponding press releases in the media outlet Science Daily (https:\/\/www.sciencedaily.com\/). We trained a PreSumm model (https:\/\/github.com\/nlpyang\/PreSumm) for abstractive summarization using SciBERT instead of BERT as a base for the seq2seq configuration. Then we performed an out-of-domain experiment, where we took every question from the CORD challenge and appended it to the relevant paragraphs that Kendra extracted. We then used this concatenation as a source for our summarization system to produce a brief abstractive summary for each question. Below we present our initial results: \n\n## Abstractive summaries for the extracted relevant paragraphs from Kendra.\n* Question: what do we know about diagnostics and surveillance ?\n  * Summary: it is commonly thought that the flu virus evolves rapidly and is only a small fraction of the population .<q>but why do we know about the virus that is , on a some point to the possibility of finding a whole new vaccine or diagnostic tool for influenza ?\n* Question: what has been published about medical care ?\n  * Summary: while hospitals and health organizations could focus on helping unravel how sick people feel during their epidemics , there is no ethical benefit , according to a new survey .\n* Question: what do we know about diagnostics and surveillance ?\n  * Summary: an international group of researchers is developing a way to monitor dangerous infectious diseases before they have been able to diagnose them .\n* Question: what is known about transmission , incubation , and environmental stability ?\n  * Summary: as the flu continues to spread seasonally , new research shows that even smallpox outbreaks are most likely to follow seasonally in the winter .\n* Question: what do we know about diagnostics and surveillance ?\n  * Summary: with the help of a handheld diagnostic test , researchers have developed a microfluidic chip that can detect the presence of pathogens in the blood with high resources .<q>the technology could dramatically improve the accuracy of diagnostic tests and help to monitor environmental conditions such as cancer .\n* Question: what do we know about vaccines and therapeutics ?\n  * Summary: in a new report , scientists have developed a way to identify the targets for potential therapeutics and vaccines for deadly mosquito-borne diseases such as sars , ebola and dengue .\n* Question: what has been published about information sharing and inter-sectoral collaboration ?\n  * Summary: a new review examines how fast communication works on the public in response to disease outbreaks .<q>the study is the first to consider risks and benefits of communication between vulnerable populations to the emerging infectious disease and its potential to help the public .\n* Question: what has been published about information sharing and inter-sectoral collaboration ?\n  * Summary: results from a new mathematical model suggest that health insurance coverage in cities is not limited .<q>the study suggests that the majority of cities affected by the 2009-2011 influenza pandemic had good insurance benefits under their health insurance plans or were significantly lower than the average income free .\n* Question: what do we know about non-pharmaceutical interventions ?\n  * Summary: the mosquito-borne virus that causes ebola hemorrhagic fever could spread to animals and humans , too , according to a new analysis .<q>the study examined the factors that lead to ebola-borne viruses and suggests that even communities with limited social and environmental factors play a role .\n* Question: what has been published about information sharing and inter-sectoral collaboration ?\n  * Summary: the recent ebola epidemic epidemic epidemic in the u.s. has received an immediate review of the public health information , according to research .<q>the review is a new issue.\n* Question: what has been published about information sharing and inter-sectoral collaboration ?\n  * Summary: the global action to prevent disease outbreaks in developing countries is vital to public health staff and services , according to a new study .<q>this time-lapse study has assessed how much capitalize on public health education , communication technology , and transmission .\n\nWe observe that the summaries are deviating from the topic, In future work we will work on improving these summaries, and fine-tune the summarization system on the CORD dataset instead of performing an out-of-domain experiment without fine-tuning.","db65ef66":"First, extract the contextualized vectors of the query above:\n\n$$q_1, \\ldots, q_T = \\text{SciBERT}(\\text{query})$$","b8f829ff":"---"}}