{"cell_type":{"6b9cddaf":"code","5c3ddedd":"code","f31b9761":"code","9327dadc":"code","927d8439":"code","4c7e1ba4":"code","c6e84d7b":"code","b3eab1d4":"code","3f6eaf86":"code","a4df3ff0":"code","a79fada3":"code","3a9c4054":"code","48b41cef":"code","1aca13dd":"code","34352efb":"code","246581a0":"code","dcac08d3":"code","1e29af61":"code","9c2273e0":"code","465f197c":"code","f19b219d":"code","415b14fc":"code","4a737f87":"code","d4e76cb9":"code","201fc631":"code","92431868":"code","c24feb55":"code","74988ec2":"code","062b05f8":"code","4db72152":"code","5c84b5ce":"markdown","4a7a03ac":"markdown","a9e73fa0":"markdown","6e7f2050":"markdown","594d199c":"markdown","8b09c2a1":"markdown","033a4846":"markdown","9c51be9e":"markdown","bd0a4529":"markdown","488a60fe":"markdown","d9d45c1d":"markdown","c5cea79e":"markdown","3412ff68":"markdown","c58398e7":"markdown","efffcb49":"markdown","14ebf13e":"markdown","adeacc94":"markdown","eac93e1b":"markdown","c7e8bafc":"markdown"},"source":{"6b9cddaf":"ROOT_DIR = '..\/input\/cassava-leaf-disease-classification\/'\nTRAIN_DIR = ROOT_DIR + '\/train_images\/'\nTEST_DIR = ROOT_DIR + '\/test_images\/'\nIMG_SIZE = 300# IMG_SIZE is determined by EfficientNet model choice. I selected EfficientNetB3 model. therefore input = (300,300,3)\nBATCH_SIZE = 32\nNUM_CLASSES = 5\nEPOCHS = 50\nSEED = 42 \n#why 42 always ? https:\/\/medium.com\/geekculture\/the-story-behind-random-seed-42-in-machine-learning-b838c4ac290a\nTEST_SIZE = 0.2\nVALIDATION_SIZE = 0.1\nLEARNING_RATE =  1e-4\n","5c3ddedd":"!wget https:\/\/storage.googleapis.com\/cloud-tpu-checkpoints\/efficientnet\/noisystudent\/noisy_student_efficientnet-b3.tar.gz","f31b9761":"!tar -xf \/kaggle\/working\/noisy_student_efficientnet-b3.tar.gz","9327dadc":"!wget https:\/\/raw.githubusercontent.com\/9vimu9\/efficientnet_weight_update_util\/main\/efficientnet_weight_update_util.py","927d8439":"!python .\/efficientnet_weight_update_util.py --model b3 --notop --ckpt .\/noisy_student_efficientnet-b3\/model.ckpt --o .\/efficientnetb3_notop.h5","4c7e1ba4":"import matplotlib.pyplot as plt #draw graphs\nimport numpy as np\nimport pandas as pd #create dataframes using csv files\nimport os\nfrom sklearn.model_selection import train_test_split # to create train, test, validation datasets\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator #for data preprocessing, image augmentation\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.applications import EfficientNetB3 #transfer learning model\nfrom tensorflow.keras.utils import plot_model # to draw summary of the model","c6e84d7b":"tf.random.set_seed(SEED) #tensorflow\nos.environ['PYTHONHASHSEED'] = str(SEED) #python\nnp.random.seed(SEED) #numpy\n","b3eab1d4":"all_data_frame = pd.read_csv(ROOT_DIR + 'train.csv')","3f6eaf86":"# read first 10 records\nall_data_frame.sample(10)","a4df3ff0":"all_data_frame = all_data_frame.astype({\"label\": str})","a79fada3":"other_data_frame, test_data_frame = train_test_split(\n                        all_data_frame, \n                        test_size = TEST_SIZE, \n                        random_state = SEED)\nprint(\"test data size : \"+str(test_data_frame.shape[0]))","3a9c4054":"train_data_frame, validation_data_frame = train_test_split(\n                    other_data_frame, \n                    test_size = VALIDATION_SIZE, \n                    random_state = SEED)\n\nprint(\"train data size : \"+str(train_data_frame.shape[0]))\nprint(\"validation data size : \"+str(validation_data_frame.shape[0]))","48b41cef":"train_image_data_generator = ImageDataGenerator(\n                    rotation_range = 270, \n                    zoom_range = [0.05,1.0],\n                    shear_range = 0.3, \n                    brightness_range = [0.5,1.5],\n                    horizontal_flip = True,\n                    vertical_flip = True,\n                    width_shift_range = 0.4, \n                    height_shift_range = 0.4, \n                    fill_mode = 'nearest'\n)","1aca13dd":"\ntrain_data_frame_iterator = train_image_data_generator.flow_from_dataframe(\n                    train_data_frame,\n                    directory = TRAIN_DIR,\n                    x_col = \"image_id\",\n                    y_col = \"label\",\n                    target_size = (IMG_SIZE,IMG_SIZE),\n                    class_mode = \"sparse\",\n                    batch_size = BATCH_SIZE,\n                    shuffle = True,\n                    seed = SEED,\n                    interpolation = \"nearest\"\n)","34352efb":"\nvalidation_data_frame_iterator =  ImageDataGenerator().flow_from_dataframe(\n                    validation_data_frame,\n                    directory = TRAIN_DIR,\n                    x_col = \"image_id\",\n                    y_col = \"label\",\n                    target_size = (IMG_SIZE,IMG_SIZE),\n                    class_mode = \"sparse\",\n                    batch_size = BATCH_SIZE,\n                    shuffle = True,\n                    seed = SEED,\n                    interpolation = \"nearest\"\n)","246581a0":"test_data_frame_iterator =  ImageDataGenerator().flow_from_dataframe(\n                    test_data_frame,\n                    directory = TRAIN_DIR,\n                    x_col = \"image_id\",\n                    y_col = \"label\",\n                    target_size = (IMG_SIZE,IMG_SIZE),\n                    class_mode = \"sparse\",\n                    batch_size = BATCH_SIZE,\n                    shuffle = False,\n                    seed = SEED,\n                    interpolation = \"nearest\"\n)","dcac08d3":"def create_model_unfreezed():\n    #create sequential Model object\n    model = models.Sequential()\n    \n    #create pretrained EfficientNetB3 model and loading weights from downloaded file in previous steps\n    pre_trained_model = EfficientNetB3(input_shape = (IMG_SIZE, IMG_SIZE, 3), include_top = False, weights = \".\/efficientnetb3_notop.h5\",drop_connect_rate=0.2)\n    #make every layer in the pre trained model trainable. that means during the training, weights and biases of the pretrained model can be changed. \n    pre_trained_model.trainable = True\n    #but according to  this documentation BathcNormalization layers need to be kept frozen\n    #https:\/\/keras.io\/examples\/vision\/image_classification_efficientnet_fine_tuning\/\n    for layer in pre_trained_model.layers:\n        if isinstance(layer, layers.BatchNormalization):\n            layer.trainable = False\n            \n    model.add(pre_trained_model)\n    model.add(layers.GlobalAveragePooling2D())\n    model.add(layers.Dense(256, activation = 'relu'))\n    model.add(layers.Dropout(0.7)) # to prevent overfitting\n    model.add(layers.Dense(NUM_CLASSES, activation = 'softmax'))\n    plot_model(model, show_shapes = True)\n    return model","1e29af61":"model = create_model_unfreezed()\n","9c2273e0":"model.compile(\n        loss = SparseCategoricalCrossentropy(name='sparse_categorical_crossentropy'),\n        optimizer = Adam(learning_rate = LEARNING_RATE),\n        metrics = ['accuracy']\n)","465f197c":"# Stop training when a monitored metric has stopped improving. usefull to prevent overfitting\nearly_stopping = EarlyStopping(monitor = 'val_loss',\n                               patience = 4,\n                               mode = 'min',\n                               restore_best_weights = True)\n\n# To save the Keras model in h5 format for future use.\nmodel_checkpoint = ModelCheckpoint('model.h5', \n                             monitor = 'val_accuracy',\n                             mode = 'max', \n                             save_best_only = True)\n\n# When a measure no longer improves, slow down its learning rate.\nreduce_learning_rate = ReduceLROnPlateau(monitor = 'val_loss',\n                              factor = 0.3,\n                              patience = 3,\n                              mode = 'min')","f19b219d":"TRAIN_STEPS_PER_EPOCH = train_data_frame_iterator.n\/\/train_data_frame_iterator.batch_size\nVALIDATION_STEPS_PER_EPOCH = validation_data_frame_iterator.n\/\/validation_data_frame_iterator.batch_size\nTEST_STEPS_PER_EPOCH = test_data_frame_iterator.n\/\/test_data_frame_iterator.batch_size","415b14fc":" train_data_frame_iterator.n","4a737f87":"train_data_frame_iterator.batch_size","d4e76cb9":"model.summary()","201fc631":"plot_model(model, show_shapes = True)","92431868":"history = model.fit(train_data_frame_iterator,\n                    validation_data = validation_data_frame_iterator,\n                    epochs = EPOCHS,\n                    steps_per_epoch = TRAIN_STEPS_PER_EPOCH,\n                    validation_steps = VALIDATION_STEPS_PER_EPOCH,\n                    callbacks = [reduce_learning_rate,model_checkpoint,early_stopping]\n                   )","c24feb55":"validation_loss, validation_accuracy = model.evaluate(validation_data_frame_iterator, steps = VALIDATION_STEPS_PER_EPOCH)\nprint(\"validation accuracy: {:5.2f}%\".format(100 * validation_accuracy))","74988ec2":"test_loss, test_accuracy = model.evaluate(test_data_frame_iterator, steps = TEST_STEPS_PER_EPOCH)\nprint(\"test accuracy: {:5.2f}%\".format(100 * test_accuracy))","062b05f8":"plt.figure(figsize=(15, 5))\nplt.plot(history.history['accuracy'], 'b*-', label=\"train accuracy\")\nplt.plot(history.history['val_accuracy'], 'r*-', label=\"validation accuracy\")\nplt.grid()\nplt.title(\"train accuracy vs validation accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend()\nplt.show()","4db72152":"plt.figure(figsize=(15, 5))\nplt.plot(history.history['loss'], 'b*-', label=\"train loss\")\nplt.plot(history.history['val_loss'], 'r*-', label=\"validation loss\")\nplt.grid()\nplt.title(\"train loss vs validation loss\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\nplt.show()","5c84b5ce":"## Calculating step size ##","4a7a03ac":"change the data type of the column \"label\" to string","a9e73fa0":"creating DataFrame object for validation","6e7f2050":"## data preprocessing ##\nI used ImageDataGenerator object to increase training dataset by doing image augmentation. this is very useful method when the dataset is small","594d199c":"creating DataFrame object for testing","8b09c2a1":"now we created a image generator for training dataset.next, we can select data source for the image generator.in earlier steps, I converted dataset to dataframe. therfore we can use ImageDataGenerator's built in function, flow_from_dataframe.","033a4846":"## constant variables ##","9c51be9e":"## Importing packages ## ","bd0a4529":"## preparing training, validation and test datasets ##\nlabels of the dataset are included in the train.csv.we can use read_csv function in pandas to read the file. it will return a DataFrame. it is very big advantage. because in Tensorflow, we can directly feed DataFrame object to train our model","488a60fe":"## train accuracy vs validation accuracy ##","d9d45c1d":"## Downloading efficientnet weights ##\nin this notebook I will use efficientnet for transfer learning. therfore we need latest EfficientNet weights. \n\nhttps:\/\/keras.io\/examples\/vision\/image_classification_efficientnet_fine_tuning\/","c5cea79e":"creating training and validation dataset using remaining dataset","3412ff68":"## train loss vs validation loss ##","c58398e7":"creating test dataset","efffcb49":"## measrung test accuracy ##","14ebf13e":"## measuring validation accuracy ##","adeacc94":"## Model creation and training ## ","eac93e1b":"## Seeding ##\nWe utilize random seed values when constructing training, validation, and test dataset. The aim is to make sure we receive the same training and validation dataset while we utilize various hyperparameters or methods in order to analyze the performance of different models","c7e8bafc":"## Creating callbacks ##"}}