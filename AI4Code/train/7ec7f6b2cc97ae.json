{"cell_type":{"f1691833":"code","947dbaee":"code","8ed353e8":"code","34428a44":"code","95138c77":"code","3d9b22b6":"code","c0107df6":"code","19cc9a7f":"code","2affafe1":"code","c3db30f6":"code","18801537":"code","6dcca253":"code","b9cc011b":"code","04324041":"code","9b601cb1":"code","8b4218a8":"markdown","359a77d5":"markdown","acc27d71":"markdown","a27d6348":"markdown","cb9dbb78":"markdown","7d9a2499":"markdown","6de9a831":"markdown","82972bbe":"markdown","049ca34a":"markdown","cbc871d1":"markdown","9582ff6c":"markdown","6ee83d7f":"markdown"},"source":{"f1691833":"import numpy as np \nimport pandas as pd\nfrom dateutil.parser import parse\nfrom typing import List\nimport matplotlib.pyplot as plt\nimport seaborn as sns","947dbaee":"df = pd.read_csv(\"..\/input\/ml-olympiad-good-health-and-well-being\/train.csv\")\ndf.dtypes","8ed353e8":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\npltdf = df.copy()\nrename = [cname[0:10] for cname in df.columns]\npltdf.columns = rename\npltdf.iloc[:100, :].plot(subplots=True, layout=(20,3), figsize=(25,20))\n\nplt.show()","34428a44":"sns.countplot(x=df[\"target\"])","95138c77":"# calculate the correlation matrix\ncols = []\ncols_done = []\nfor col_one in df.iloc[:,:].columns:\n    if (df[col_one].corr(df['target']) > 0.1):\n        cols.append(col_one)\n    cols_done.append(col_one)\ncorrdf = df.copy()\ncorrdf = corrdf[cols].corr()\n\nsns.heatmap(corrdf, cmap=\"Blues\")","3d9b22b6":"pd.crosstab(df.GenHlth,df.target,margins=True).style.background_gradient(cmap='Blues')","c0107df6":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\ntree_set = df.select_dtypes(exclude=\"object\").copy()\ntarget = tree_set[\"target\"]\ntree_set.drop([\"target\",\"PatientID\"], axis=1, inplace=True)\n\ntree_clf = DecisionTreeClassifier(max_depth=3, random_state=1)\ntree_clf.fit(tree_set, target)\ntree_clf.score(tree_set, target)","19cc9a7f":"fig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(tree_clf, \n                   feature_names=tree_set.columns,\n                   class_names=[\"0\", \"1\"],\n                   filled=True, \n                   fontsize=18)","2affafe1":"f,ax=plt.subplots(2,2,figsize=(20,10))\n\nsns.countplot(x=df[df['target']==0].GenHlth, ax=ax[0, 0], color='blue')\nax[0, 0].set_title('Health Rating with Heart Disease = 0')\nsns.countplot(x=df[df['target']==1].GenHlth, ax=ax[0, 1], color='darkblue')\nax[0, 1].set_title('Health Rating with Heart Disease = 1')\nsns.countplot(x=df[df['target']==0].Age, ax=ax[1, 0], color='blue')\nax[1, 0].set_title('Age with Heart Disease = 0')\nsns.countplot(x=df[df['target']==1].Age, ax=ax[1, 1], color='darkblue')\nax[1, 1].set_title('Age with Heart Disease = 1')\nplt.show()","c3db30f6":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\nX = df.select_dtypes(exclude=\"object\").copy()\ny = df[\"target\"]\nX.drop([\"target\",\"PatientID\"], axis=1, inplace=True)\nX = X.values\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42) \n\nscaler = preprocessing.StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_val = scaler.transform(X_val)","18801537":"from sklearn.ensemble import ExtraTreesClassifier\n\net_clf = ExtraTreesClassifier(random_state=1)\net_clf.fit(X_train, y_train)\nprint(\"train: \" + str(et_clf.score(X_train, y_train)))\nprint(\"test: \" + str(et_clf.score(X_val, y_val)))","6dcca253":"tree_clf = DecisionTreeClassifier(max_depth=6, random_state=1)\ntree_clf.fit(X_train, y_train)\nprint(\"train: \" + str(tree_clf.score(X_train, y_train)))\nprint(\"test: \" + str(tree_clf.score(X_val, y_val)))","b9cc011b":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(random_state=1)\nrf_clf.fit(X_train, y_train)\nprint(\"train: \" + str(rf_clf.score(X_train, y_train)))\nprint(\"test: \" + str(rf_clf.score(X_val, y_val)))","04324041":"test_X = pd.read_csv(\"..\/input\/ml-olympiad-good-health-and-well-being\/test.csv\")\ntest_X.drop([\"PatientID\"], axis=1, inplace=True)\ntest_X = scaler.transform(test_X.values)\npredicted = tree_clf.predict(test_X)","9b601cb1":"submission = pd.read_csv(\"..\/input\/ml-olympiad-good-health-and-well-being\/sample_submission.csv\")\nsubmission[\"target\"] = np.rint(predicted)\nsubmission = submission.astype(int)\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(submission.head())","8b4218a8":"# Introduction\n\nSeems like an interesting dataset. I'm looking forward to see it in more detail.","359a77d5":"Quite unbalanced that will have to be kept in mind.\n\nNow let's look at important correlations in the data:","acc27d71":"# Exploratory Data Analysis","a27d6348":"# Create Submission","cb9dbb78":"We can see the health rating and age is distributed quite differently depending on the target variable.","7d9a2499":"The limited depth decision tree appears to be the best of these simple approaches.","6de9a831":"Nothing huge. Let's look at the General Health Rating (GenHlth) to see what a cross table tells us.","82972bbe":"Love seeing no object in the datatypes. Great start :). Now a basic visualisation of what we are are looking in the table.","049ca34a":"I find a decision tree is useful at looking how the data is splitting.","cbc871d1":"Let's check the class balance","9582ff6c":"# Conclusion\n\nI'm sure much stronger results can be done than this simple decision tree and I will attempt to do so in another notebook. It was interesting looking through the data, while there are not strong correlations that would make it very easy there are a lot of coefficients available to create strong classifiers. ","6ee83d7f":"# Prediction\n\nI'm going to start with a basic prediction based on Extra Trees since the previous decision tree was so effective."}}