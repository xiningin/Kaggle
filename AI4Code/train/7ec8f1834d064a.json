{"cell_type":{"7405820c":"code","b68b8a4c":"code","3be08c85":"code","0e4eb1f8":"code","9a290792":"code","fa4b0a19":"code","0706fb58":"code","4c808d24":"code","8c1a07e8":"code","6fb682c3":"code","49efc4ad":"code","d133ebb6":"code","7fe16c23":"code","cd8f4586":"code","ab54932d":"code","47251adf":"code","3f2e95a8":"code","e583e1d2":"code","d51e3125":"code","71c00cb5":"code","49bd3eb8":"code","642c1673":"code","956c8443":"code","840cd8d7":"code","30f8a95c":"code","8233d4bf":"code","f36fd495":"code","6daeb33d":"code","2cf2f3f4":"code","2b9477b8":"code","7863213f":"code","7e0e0297":"code","1d8ee2e7":"code","61cdfc43":"code","af8fcf58":"code","db2e4be1":"code","d14bb301":"code","cc8f2dae":"code","183a76bd":"code","e4317f98":"code","a53f153c":"code","fd5a8e80":"markdown","7440916f":"markdown","7d007f25":"markdown","635ff6fd":"markdown","6f25d8a9":"markdown","ab685012":"markdown","9244f172":"markdown","4e435e0d":"markdown","7205c43f":"markdown","6902d091":"markdown","46141707":"markdown","1b4aa6d0":"markdown","a0337809":"markdown","72755434":"markdown","4ab17bc8":"markdown","3166d0db":"markdown","e82d9421":"markdown","83194df5":"markdown"},"source":{"7405820c":"import pandas as pd\nimport numpy as np\nimport os\nimport cv2\nfrom PIL import Image, ImageDraw\nfrom ast import literal_eval\nimport matplotlib.pyplot as plt\nimport urllib\nfrom tqdm.notebook import tqdm","b68b8a4c":"!git clone https:\/\/github.com\/fizyr\/keras-retinanet.git","3be08c85":"%cd keras-retinanet\/\n\n!pip install .","0e4eb1f8":"!python setup.py build_ext --inplace","9a290792":"import tensorflow as tf\nfrom keras_retinanet import models\nfrom keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\nfrom keras_retinanet.utils.visualization import draw_box, draw_caption\nfrom keras_retinanet.utils.colors import label_color","fa4b0a19":"root = \"\/kaggle\/input\/global-wheat-detection\/\"\ntrain_img = root+\"train\"\ntest_img = root+\"test\"\ntrain_csv = root+\"train.csv\"\nsample_submission = root+\"sample_submission.csv\"","0706fb58":"train = pd.read_csv(train_csv)\ntrain.head()","4c808d24":"print(f\"Total Bboxes: {train.shape[0]}\")","8c1a07e8":"train['width'].unique() == train['height'].unique() == [1024]","6fb682c3":"def get_bbox_area(bbox):\n    bbox = literal_eval(bbox)\n    return bbox[2] * bbox[3]","49efc4ad":"train['bbox_area'] = train['bbox'].apply(get_bbox_area)","d133ebb6":"train['bbox_area'].value_counts().hist(bins=10)","7fe16c23":"unique_images = train['image_id'].unique()\nlen(unique_images)","cd8f4586":"num_total = len(os.listdir(train_img))\nnum_annotated = len(unique_images)\n\nprint(f\"There are {num_annotated} annotated images and {num_total - num_annotated} images without annotations.\")","ab54932d":"sources = train['source'].unique()\nprint(f\"There are {len(sources)} sources of data: {sources}\")","47251adf":"train['source'].value_counts()","3f2e95a8":"plt.hist(train['image_id'].value_counts(), bins=10)\nplt.show()","e583e1d2":"def show_images(images, num = 5):\n    \n    images_to_show = np.random.choice(images, num)\n\n    for image_id in images_to_show:\n\n        image_path = os.path.join(train_img, image_id + \".jpg\")\n        image = Image.open(image_path)\n\n        # get all bboxes for given image in [xmin, ymin, width, height]\n        bboxes = [literal_eval(box) for box in train[train['image_id'] == image_id]['bbox']]\n\n        # visualize them\n        draw = ImageDraw.Draw(image)\n        for bbox in bboxes:    \n            draw.rectangle([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]], width=3)\n\n        plt.figure(figsize = (15,15))\n        plt.imshow(image)\n        plt.show()","d51e3125":"show_images(unique_images)","71c00cb5":"bboxs=[ bbox[1:-1].split(', ') for bbox in train['bbox']]\nbboxs=[ f\"{int(float(bbox[0]))},{int(float(bbox[1]))},{int(float(bbox[0]))+int(float(bbox[2]))},{int(float(bbox[1])) + int(float(bbox[3]))},wheat\" for bbox in bboxs]\ntrain['bbox_']=bboxs\ntrain.head()","49bd3eb8":"train_df=train[['image_id','bbox_']]\ntrain_df.head()","642c1673":"train_df=train_df.sample(frac=1).reset_index(drop=True)\ntrain_df.head()","956c8443":"with open(\"annotations.csv\",\"w\") as file:\n    for idx in range(len(train_df)):\n        file.write(train_img+\"\/\"+train_df.iloc[idx,0]+\".jpg\"+\",\"+train_df.iloc[idx,1]+\"\\n\")\n        ","840cd8d7":"with open(\"classes.csv\",\"w\") as file:\n    file.write(\"wheat,0\")","30f8a95c":"PRETRAINED_MODEL = '.\/snapshots\/_pretrained_model.h5'\n\nURL_MODEL = 'https:\/\/github.com\/fizyr\/keras-retinanet\/releases\/download\/0.5.1\/resnet50_coco_best_v2.1.0.h5'\nurllib.request.urlretrieve(URL_MODEL, PRETRAINED_MODEL)\n\nprint('Downloaded pretrained model to ' + PRETRAINED_MODEL)","8233d4bf":"EPOCHS = 1\nBATCH_SIZE=8\nSTEPS = 100 #len(train_df)\/\/BATCH_SIZE #Keeping it small for faster commit\nLR=1e-3","f36fd495":"!keras_retinanet\/bin\/train.py --random-transform --weights {PRETRAINED_MODEL} --lr {LR} --batch-size {BATCH_SIZE} --steps {STEPS} --epochs {EPOCHS} --no-resize csv annotations.csv classes.csv","6daeb33d":"!ls snapshots","2cf2f3f4":"model_path = os.path.join('snapshots', sorted(os.listdir('snapshots'), reverse=True)[0])\n\nmodel = models.load_model(model_path, backbone_name='resnet50')\nmodel = models.convert_model(model)","2b9477b8":"li=os.listdir(test_img)\nli[:5]","7863213f":"def predict(image):\n    image = preprocess_image(image.copy())\n    #image, scale = resize_image(image)\n\n    boxes, scores, labels = model.predict_on_batch(\n    np.expand_dims(image, axis=0)\n  )\n\n    #boxes \/= scale\n\n    return boxes, scores, labels","7e0e0297":"THRES_SCORE = 0.5\n\ndef draw_detections(image, boxes, scores, labels):\n    for box, score, label in zip(boxes[0], scores[0], labels[0]):\n        if score < THRES_SCORE:\n            break\n\n        color = label_color(label)\n\n        b = box.astype(int)\n        draw_box(image, b, color=color)\n\n        caption = \"{:.3f}\".format(score)\n        draw_caption(image, b, caption)","1d8ee2e7":"def show_detected_objects(image_name):\n    img_path = test_img+'\/'+image_name\n  \n    image = read_image_bgr(img_path)\n\n    boxes, scores, labels = predict(image)\n    print(boxes[0,0].shape)\n    draw = image.copy()\n    draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n\n    draw_detections(draw, boxes, scores, labels)\n    plt.figure(figsize=(15,10))\n    plt.axis('off')\n    plt.imshow(draw)\n    plt.show()","61cdfc43":"for img in li:\n    show_detected_objects(img)","af8fcf58":"preds=[]\nimgid=[]\nfor img in tqdm(li,total=len(li)):\n    img_path = test_img+'\/'+img\n    image = read_image_bgr(img_path)\n    boxes, scores, labels = predict(image)\n    boxes=boxes[0]\n    scores=scores[0]\n    for idx in range(boxes.shape[0]):\n        if scores[idx]>THRES_SCORE:\n            box,score=boxes[idx],scores[idx]\n            imgid.append(img.split(\".\")[0])\n            preds.append(\"{} {} {} {} {}\".format(score, int(box[0]), int(box[1]), int(box[2]-box[0]), int(box[3]-box[1])))\n    ","db2e4be1":"preds[0]","d14bb301":"sub={\"image_id\":imgid, \"PredictionString\":preds}\nsub=pd.DataFrame(sub)\nsub.head()","cc8f2dae":"sub_=sub.groupby([\"image_id\"])['PredictionString'].apply(lambda x: ' '.join(x)).reset_index()\nsub_","183a76bd":"samsub=pd.read_csv(\"\/kaggle\/input\/global-wheat-detection\/sample_submission.csv\")\nsamsub.head()","e4317f98":"for idx,imgid in enumerate(samsub['image_id']):\n    samsub.iloc[idx,1]=sub_[sub_['image_id']==imgid].values[0,1]\n    \nsamsub.head()","a53f153c":"samsub.to_csv('\/kaggle\/working\/submission.csv',index=False)","fd5a8e80":"Let's look at how many bounding boxes do we have for each image:","7440916f":"# EDA","7d007f25":" Single Image has multiple bbox","635ff6fd":"Max number of bounding boxes is 116, whereas min (annotated) number is 1 ","6f25d8a9":"Let's Check the Dimensions of images","ab685012":"# Loading the trained model","9244f172":"## Downloading the pretrained model","4e435e0d":"# Installing Keras-RetinaNet ","7205c43f":"# Training Model","6902d091":"## Preparing Files to be given for training\n\n### Annotation file contains all the path of all images and their corresponding bounding boxes\n### Class file contains the number of classes but in our case it is just 1 (Wheat)","46141707":"## Let's look at the data","1b4aa6d0":"## Visualizing images","a0337809":"### Model Parameters","72755434":"What can we tell from visualizations:\n\n* there are plenty of overlappind bounding boxes\n* all photos seem to be taken vertically \n* all plants are can be rotated differently, there is no single orientation. this means that different flip and roration augmentations should probably help\n* colors of wheet heads are quite different and seem to depend a little bit on the source\n* wheet heads themselves are seen from very different angles of view relevant to the observer","4ab17bc8":"<h1 align=center> Global Wheat Detection with Keras RetinaNet <\/h1>\n\n### This Notebook is for Training Purpose of Keras RetinaNet.\n### RetinaNet is very slow as compared to F-RCNN so I've kept epochs and steps per epoch small for fast commiting purpose.\n### I will make another notebook for inference Shortly.\n\n### Credits for the EDA goes to [THIS Notebook](https:\/\/www.kaggle.com\/devvindan\/wheat-detection-eda)\n\n<h3 align=center style=color:red>Upvote If you find this kernel interesting<\/h3>","3166d0db":"# Preprocessing Data for Input to RetinaNet","e82d9421":"# Predictions","83194df5":"### Sources of Data"}}