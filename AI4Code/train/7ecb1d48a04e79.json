{"cell_type":{"4542c345":"code","03036f88":"code","8b181f40":"code","2f67fb4e":"code","2cd5a161":"code","476e71df":"code","6b6b81a8":"code","7952a991":"code","94c91d51":"code","fb7ddb19":"code","cc2fa325":"code","65593d3f":"code","cf4bd496":"code","65b175ad":"code","8c6e7a69":"code","8bf715f6":"code","e52647ed":"code","47f9b4e1":"code","0b96fa52":"code","7fc76c44":"code","4adf738a":"code","d27c9770":"code","2bef9d3d":"code","761f89ac":"code","6f32f52a":"code","617340bc":"code","b5b9be88":"code","3bc6a1d7":"code","bec905cc":"code","6df96555":"code","6f4f9a66":"code","097a8d70":"markdown","feadd707":"markdown","0ba10716":"markdown","2e06dacf":"markdown","76102934":"markdown","184631d4":"markdown","7ca4d233":"markdown","ebb830f0":"markdown","8061d75c":"markdown","4ff272e2":"markdown","d72691eb":"markdown","125fff83":"markdown","e2642b83":"markdown","a945a504":"markdown","5e9cafca":"markdown","da616660":"markdown","29d08655":"markdown"},"source":{"4542c345":"# Data Paths \n\ntrain_path = '..\/input\/tabular-playground-series-may-2021\/train.csv'\n\ntest_path = '..\/input\/tabular-playground-series-may-2021\/test.csv'\n\nsample_submission_path = '..\/input\/tabular-playground-series-may-2021\/sample_submission.csv'\n\n# Importing primary libraries \n\n# Data Manipulation\n#-------------------------------\nimport os\nimport pandas as pd\nimport numpy as np\n\n# Data Visualization\n#--------------------------------\nimport matplotlib.pyplot as plt\nimport seaborn as sns","03036f88":"# Train Data\n\ntrain_data = pd.read_csv( train_path )\n\ntrain_data.head()","8b181f40":"# Test Data\n\ntest_data = pd.read_csv( test_path )\n\ntest_data.head()","2f67fb4e":"# Sample submission Data \n\nsamp_sub = pd.read_csv( sample_submission_path )\n\nsamp_sub.head()","2cd5a161":"# Overall train data structure\n\nprint(train_data.info())\n\ntrain_data.describe()","476e71df":"# Overall test data structure\n\nprint(test_data.info())\n\ntest_data.describe()","6b6b81a8":"for col in train_data.columns:\n    print(col, ' : ', train_data[col].dtype ,end = ' | ')","7952a991":"# Target Value Count Distribution:\n\ntarget_mass = train_data['target'].value_counts()\nvalues = target_mass.values.tolist()\nindexes = target_mass.index.tolist()\n\nax,fig = plt.subplots(1,2,figsize=(15,6))\nplt.subplot(1,2,1)\nplt.pie(values , labels = indexes)\nplt.subplot(1,2,2)\nplt.bar(indexes,values)\nplt.show()","94c91d51":"ax,fig = plt.subplots(10,5,figsize=(15,15))\nfor i in range(50):\n    plt.subplot(10,5,i+1)\n    arr =train_data['feature_'+str(i)].tolist()\n    plt.scatter(range(len(train_data)),arr,s = 0.2)\nplt.show()","fb7ddb19":"def plot_diag_heatmap(data):\n    corr = data.corr()\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n    f, ax = plt.subplots(figsize=(11, 9))\n    sns.heatmap(corr, mask=mask, cmap='YlGnBu', vmax=.01, center=0,square=True, linewidths=.5, cbar_kws={\"shrink\": 1.0})","cc2fa325":"plot_diag_heatmap(train_data.iloc[:,1:])","65593d3f":" color = ['red' , 'green' , 'blue' , 'orange']\nx1 = []\nx2 = []\nx3 = []\nx4 = []\nfor i in range(4):\n    xx = train_data[train_data['target']=='Class_'+str(i+1)]\n    for col in train_data.columns[1:-1]:\n        if i==0:\n            x1.append(np.mean(xx[col]))\n        elif i==1:\n            x2.append(np.mean(xx[col]))\n        elif i==2:\n            x3.append(np.mean(xx[col]))\n        else:\n            x4.append(np.mean(xx[col]))\narr = []\narr.append(x1)\narr.append(x2)\narr.append(x3)\narr.append(x4)\nplt.figure(figsize=(20,7))\n#ax,fig=plt.subplots(4,1,figsize=(20,20))\nfor i in range(4):\n    #plt.subplot(4,1,i+1)\n    plt.plot(arr[i],color=color[i])\nplt.legend()\n#plt.title()","cf4bd496":"ax,fig = plt.subplots(7,7,figsize=(25,20))\nplt.suptitle('Outliers Detection in Train data',size=20)\nfor i in range(7):\n    for j in range(7):\n        plt.subplot(7,7,i*7+j+1)\n        sns.violinplot(x=train_data['target'],y=train_data.iloc[:,i*7+j+1])\n        plt.title(train_data.columns[i*7+j+1])\nplt.show()","65b175ad":"frame_pattern = train_data.iloc[:,1:-1].to_numpy()\nframe_pattern.shape\nax,fig = plt.subplots(5,5,figsize=(15,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.imshow(frame_pattern[i].reshape(5,10)\/255.0)\n    plt.title(train_data['target'][i])\nplt.show()\n","8c6e7a69":"# Dropping the id columns\n\nTrain = train_data.drop('id',1)\n\nTest = test_data.drop('id',1)","8bf715f6":"# Minmaxscaling :\n\ndef minmaxscaler(data, fin):\n    \n    for feature in fin.columns:\n        if data[feature].dtype != 'object':\n            min_value = min(data[feature])\n            max_value = max(data[feature])\n            data[feature] = (data[feature]-min_value) \/ (max_value-min_value)\n            fin[feature] = (fin[feature]-min_value) \/ (max_value-min_value)\n    \n    return data,fin","e52647ed":"Train,Test = minmaxscaler(Train,Test)","47f9b4e1":"Train.head()","0b96fa52":"from scipy.stats import variation as var","7fc76c44":"for col in Test.columns[:-1]:\n    print(col,' : ',var(Train[col]))","4adf738a":"# dropping the features with low variance \ndef drop_low_var_values(data,threshold):\n    labels = []\n    for col in data.columns:\n        if data[col].dtype != 'object':\n            if var(data[col]) >= threshold:\n                labels.append(col)\n        else:\n            labels.append(col)\n    new_data = data[labels]\n    print(data.shape[1],' features ------> ',new_data.shape[1],' features .')\n    return new_data","d27c9770":"Train_data = drop_low_var_values(Train,1.3)\nTest_data = Test[Train_data.columns[:-1]]","2bef9d3d":"from sklearn.model_selection import train_test_split","761f89ac":"def split_data(test_size,data):\n    data = data.sample(frac=1)\n    x_train = data.drop('target',1)\n    y_1 = data['target']\n    x_train = x_train.to_numpy()\n    y_1 = y_1.to_numpy()\n    X_train , X_val , y_1 , y_2 = train_test_split( x_train , y_1 ,\n                                                         test_size = test_size ,\n                                                        random_state =1 ,\n                                                        stratify = y_1)\n    y_train = []\n    y_val = []\n    for value in y_1:\n        y_train.append(int(value[-1])-1)\n    for value in y_2:\n        y_val.append(int(value[-1])-1)\n    return X_train , X_val , np.array(y_train) , np.array(y_val)","6f32f52a":"X_train , X_val , y_train , y_val = split_data(0.2,Train_data)\nX_test = Test_data","617340bc":"X_train.shape , X_val.shape , y_train.shape , y_val.shape , X_test.shape","b5b9be88":"# importing models\n\nfrom sklearn.ensemble import RandomForestClassifier as rfc\nfrom sklearn.ensemble import ExtraTreesClassifier as ext\nfrom xgboost import XGBClassifier as xgb\nfrom lightgbm import LGBMClassifier as lgb","3bc6a1d7":"# Function to train and visualize accuracy and predict\n\ndef train_and_predict(model , x_1  , x_2 , x_3 , y_1 , y_2):\n    labels = ['Class_1' , 'Class_2' , 'Class_3' , 'Class_4' ]\n    model.fit(x_1 , y_1)\n    print('Training Completed..........')\n    print('Train Accuracy : ',model.score(x_1,y_1))\n    print('Validation Accuracy : ',model.score(x_2 , y_2))\n    print('Model Prediction started....')\n    y_pred = model.predict_proba(x_3)\n    final_df = pd.DataFrame(y_pred , columns = labels)\n    #final_df = pd.concat([samp_sub['id'],final_df]  , axis = 1)    uncomment this to find the actual submission files.\n    \n    return final_df\n    \n    \n    ","bec905cc":"clf1 = rfc(random_state = 2)\nclf2 = ext(random_state = 2)\nclf3 = xgb()\nclf4 = lgb()\n\nmodels = [ clf1 , clf2 , clf3 , clf4 ]\nnames = ['rfc' , 'ext' , 'xgb' , 'lgb']","6df96555":"for i in range(4):\n    print(names[i] , models[i])","6f4f9a66":"for i in range(len(models)):\n    model = models[i]\n    print(names[i] , 'model has been opted for training...........')\n    submission = train_and_predict(model , X_train , X_val , X_test , y_train , y_val)\n    print('submission file created................................')\n    submission.to_csv(names[i]+'.csv',index=False)\nprint('Task Completed.............................................')","097a8d70":"Now all the final data has been updated. \n## HURRAH !!!!!! We've reached the end.\n\n### Tou can visit my other works at [github](https:\/\/github.com\/sagnik1511)  or vsiit my kaggle profile [sagnik1511](https:\/\/kaggle.com\/sagnik1511).\n\n# THANK YOU for visiting :)\n\n![](https:\/\/st3.depositphotos.com\/1006899\/12553\/i\/600\/depositphotos_125537970-stock-photo-end-word-hanging-on-ropes.jpg)","feadd707":"We can see the mean values of every single target type has negligible different, so we cannot drop any single row corresponding to this target types.","0ba10716":"## Primary Visualization & Exploratory Data Analysis:\n---\n\nNow we are going to check the basic structure of the data and how we can manipulate in to present the trainable data.","2e06dacf":"## Data Loading :\n---\nAt first we have to gather the data pathas and load them into dataframes for further manipulations.","76102934":"We have found that -\n\n  1.    From this visualization we can see there's one ***ID*** column and all other non-target fearure is integer values  and the target feature is categorical. \n\n 2. Ther are no null values , so we do not have to process the data to fill thos places.\n 3. Some of the features are binary feature where others are numerical.","184631d4":"We have seen that the features are not distributed well. \n\nNow we should check each features' distributions as we ca find any feature which has no significant feature value and  can be omitted in this case .","7ca4d233":"### MinMaxScaler :\n---\n\n We are going to make every single feature's value lie in between 0 to 1 as their equivalence.","ebb830f0":"## Model Generation :\n---\n\nThe approach would be to predict through some famous tuned classifiers ( bagging & boosting both ) and then their prediction probability will be taken for the submission.\n\nWe will use \n\n1) **RandomForestClassifier**\n\n2) **ExtraTreesClassfier**\n\n3) **XGBoostClassifier**\n\n4) **LGBMClassifier**","8061d75c":"So, it looks like the data has no significant pattern. So, leaving this part.","4ff272e2":"## Preprocessing :\n---\nWe're doing some basic data processing to present the trainable data","d72691eb":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/25225\/logos\/header.png?t=2021-01-27-17-34-26)\n\n\n# Tabular PlayGround walkthrough :\n---\n\nIn this notebook we are going to see how to interpret a whole kaggle competition from one end to another.\n\nWe are following some steps which will be guided through other sub-operations \/ manipulations to gather knowledge and process and tune and find good accuracy. \n\nwe are going to use custom Neural Networks using keras and tensorflow to predict.\n\n# UPVOTE if you like this notebook and also to keep the developer sane :)","125fff83":"So, we've found that every single feature has a wide range of data spread.\n\n### Heatmap:\n---\nso, we must check their correlation to find any oher information.","e2642b83":"### Preparing Train and validation :\n---\nWe're going to split prepare the valiation as 20 % of train data.\n ","a945a504":"### Drop Low variation data :\n---\n The features with low variation are no good data for training so we ust drop those features .","5e9cafca":"### Outliers Detection :\n---\n\n Now we should check for the outliers in this data , as we should remove those and find a better trainable data.","da616660":"### Checking for any 2D pattern :\n---\n\nAs the features are too much there might be any 2D pattern that might help to get a good sense of data\n","29d08655":"### Target Distribution:\n---\n\nNow , let's check the target feature mass distribution. As that can share us any any leads towards finding the best trainable data."}}