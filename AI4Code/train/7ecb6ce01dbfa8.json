{"cell_type":{"e4051382":"code","eb45bbb9":"code","38f499dd":"code","20711368":"code","8ae252c2":"code","2ddb4435":"code","9c0177e5":"code","e3c2b1f9":"code","d974ba9a":"code","30e1ed71":"code","dff02942":"code","975ad8c7":"code","a9af5eac":"code","26e0f52a":"code","e5f95ffa":"code","ec55909c":"code","de124354":"code","751659bb":"code","84658940":"code","4994699a":"code","4ff4ef5a":"code","974c5849":"code","2498e47e":"code","9c7f9423":"code","bf87202b":"code","93f9f3a9":"code","c25a514d":"code","14290caa":"code","9a3f7f59":"code","d14a6283":"code","e475d8df":"code","c0157faf":"code","890e62c2":"code","53217ea6":"code","3881cd1e":"code","1244a771":"code","04d423c6":"code","e37af62f":"code","5168feb8":"code","a35668fe":"code","24a51a8d":"code","8d881ede":"markdown","f3ef1320":"markdown","73158ae1":"markdown","e72eb830":"markdown","81b8c648":"markdown","55524ab8":"markdown","e20c9a17":"markdown","f10d11d5":"markdown","6100adac":"markdown","5e8e5e9c":"markdown","345fc939":"markdown","9f2d0a41":"markdown","80eafb39":"markdown","c6d0f2dd":"markdown"},"source":{"e4051382":"#Data Analysis Libraries\nimport pandas as pd\nimport numpy as np\n\n#Visualization Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#Ignore Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","eb45bbb9":"#Import train and test CSV file\ntrain_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')","38f499dd":"#Look training dataset\ntrain_df","20711368":"#Look statistics describe training dataset\ntrain_df.describe(include = \"all\")","8ae252c2":"#To find out the data type\ntrain_df.info()","2ddb4435":"#To know missing value train dataset\ntrain_df.isna().sum()","9c0177e5":"#Get a count of the number of numerical variables\ndef numbernumerical(variable):\n   var = train_df[variable]\n   varValue = var.value_counts()\n\n   print('{} \\n{} \\n'.format(variable, varValue))\n\nnumerical = ['Age', 'Fare', 'SibSp', 'Parch']\nfor i in numerical:\n  numbernumerical(i)","e3c2b1f9":"#Sort the ages into logical categories\ntrain_df[\"Age\"] = train_df[\"Age\"].fillna(-0.5)\ntest_df[\"Age\"] = test_df[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain_df['AgeGroup'] = pd.cut(train_df[\"Age\"], bins, labels = labels)\ntest_df['AgeGroup'] = pd.cut(test_df[\"Age\"], bins, labels = labels)\n\nprint(train_df['AgeGroup'])","d974ba9a":"#Get a count of the number of categorical variables\ndef numbercategorical(variable):\n   var = train_df[variable]\n   varValue = var.value_counts()\n\n   print('{} \\n{} \\n'.format(variable, varValue))\n\ncategory = ['Survived', 'AgeGroup', 'Sex', 'Pclass', 'Embarked']\nfor i in category:\n  numbercategorical(i)","30e1ed71":"#Get a count of the number of numerical variables\ndef numberalphanumeric(variable):\n   var = train_df[variable]\n   varValue = var.value_counts()\n\n   print('{} \\n{} \\n'.format(variable, varValue))\n\nalphanumeric = ['Ticket', 'Cabin']\nfor i in alphanumeric:\n  numberalphanumeric(i)","dff02942":"#Visualize the count of survivors\nsns.countplot(train_df['Survived'])\n\nprint","975ad8c7":"#Visualize the count of survivors for columns 'Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked'\ncols = ['Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked', 'AgeGroup']\n\nn_rows = 2\nn_cols = 3\n\n#The subplot grid and figure size of each graph\nfig, axs = plt.subplots(n_rows, n_cols, figsize = (n_cols * 6, n_rows * 6) )\n\nfor r in range(0, n_rows):\n  for c in range(0, n_cols):\n\n    i = r*n_cols + c #index to go through the number of columns\n    ax = axs[r][c] #show where position each sub plot\n    sns.countplot(train_df[cols[i]], hue= train_df['Survived'], ax=ax)\n    ax.set_title(cols[i])\n    ax.legend(title=\"Survived\", loc = 'upper right')\n\nplt.tight_layout()","a9af5eac":"#Plot the prices paid of each class\nplt.scatter(train_df[\"Fare\"], train_df['Pclass'], color = 'green', label='Passenger Paid')\nplt.ylabel('Class')\nplt.xlabel('Price \/ Fare')\nplt.title('Price of Each Class')\nplt.legend()\nplt.show()","26e0f52a":"#Print percentage of survival by column Sex, Pclass, SibSp, Parch, Embarked, AgeGroup\ndef mean(variable):\n   var = train_df[[variable,'Survived']].groupby(variable, as_index = False)\n   mean = var.mean().sort_values(by='Survived', ascending = False)\n\n   print('Percentage of {} who survived \\n {} \\n'.format(variable, mean))\n\ncols = ['Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked', 'AgeGroup']\nfor i in cols:\n  mean(i)","e5f95ffa":"#Draw a bar plot of survival by column Sex, Pclass, SibSp, Parch, Embarked, AgeGroup\ncols = ['Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked', 'AgeGroup']\n\nn_rows = 2\nn_cols = 3\n\n#The subplot grid and figure size of each graph\nfig, axs = plt.subplots(n_rows, n_cols, figsize = (n_cols * 6, n_rows * 6) )\n\nfor r in range(0, n_rows):\n  for c in range(0, n_cols):\n\n    i = r*n_cols + c #index to go through the number of columns\n    ax = axs[r][c] #show where position each sub plot\n    sns.barplot(x=train_df[cols[i]], y=train_df[\"Survived\"], data=train_df, ax=ax)\n    ax.set_title(cols[i])\n    \nplt.tight_layout()","ec55909c":"#Look testing dataset\ntest_df.describe(include='all')","de124354":"test_df.isna().sum()","751659bb":"train_df.isna().sum()","84658940":"#we'll start off by dropping the Cabin and Ticket feature since not a lot more useful information can be extracted from it.\ntrain_df = train_df.drop(['Cabin', 'Ticket'], axis = 1)\ntest_df = test_df.drop(['Cabin', 'Ticket'], axis = 1)","4994699a":"#Missing value column Embarked\nmodus = train_df['Embarked'].mode()[0]\ntrain_df['Embarked'] = train_df['Embarked'].fillna(modus)\ntrain_df['Embarked'].isna().sum()","4ff4ef5a":"#create a combined group of both datasets\ncombine = [train_df, test_df]\n\n#extract a title for each Name in the train and test datasets\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])","974c5849":"#replace various titles with more common names\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Capt', 'Col',\n    'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","2498e47e":"#map each of the title groups to a numerical value\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royal\": 5, \"Rare\": 6}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()","9c7f9423":"# fill missing age with mode age group for each title\nmr_age = train_df[train_df[\"Title\"] == 1][\"AgeGroup\"].mode() #Young Adult\nmiss_age = train_df[train_df[\"Title\"] == 2][\"AgeGroup\"].mode() #Student\nmrs_age = train_df[train_df[\"Title\"] == 3][\"AgeGroup\"].mode() #Adult\nmaster_age = train_df[train_df[\"Title\"] == 4][\"AgeGroup\"].mode() #Baby\nroyal_age = train_df[train_df[\"Title\"] == 5][\"AgeGroup\"].mode() #Adult\nrare_age = train_df[train_df[\"Title\"] == 6][\"AgeGroup\"].mode() #Adult\n\nage_title_mapping = {1: \"Young Adult\", 2: \"Student\", 3: \"Adult\", 4: \"Baby\", 5: \"Adult\", 6: \"Adult\"}\n\nfor x in range(len(train_df[\"AgeGroup\"])):\n    if train_df[\"AgeGroup\"][x] == \"Unknown\":\n        train_df[\"AgeGroup\"][x] = age_title_mapping[train_df[\"Title\"][x]]\n        \nfor x in range(len(test_df[\"AgeGroup\"])):\n    if test_df[\"AgeGroup\"][x] == \"Unknown\":\n        test_df[\"AgeGroup\"][x] = age_title_mapping[test_df[\"Title\"][x]]","bf87202b":"#map each Age value to a numerical value\nage_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ntrain_df['AgeGroup'] = train_df['AgeGroup'].map(age_mapping)\ntest_df['AgeGroup'] = test_df['AgeGroup'].map(age_mapping)\n\ntrain_df.head()","93f9f3a9":"#drop the name feature since it contains no more useful information.\ntrain_df = train_df.drop(['Name','Age'], axis = 1)\ntest_df = test_df.drop(['Name','Age'], axis = 1)","c25a514d":"#map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain_df['Sex'] = train_df['Sex'].map(sex_mapping)\ntest_df['Sex'] = test_df['Sex'].map(sex_mapping)\n\ntrain_df.head()","14290caa":"#map each Embarked value to a numerical value\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ntrain_df['Embarked'] = train_df['Embarked'].map(embarked_mapping)\ntest_df['Embarked'] = test_df['Embarked'].map(embarked_mapping)\n\ntrain_df.head()","9a3f7f59":"#fill in missing Fare value in test set based on mean fare for that Pclass \nfor x in range(len(test_df[\"Fare\"])):\n    if pd.isnull(test_df[\"Fare\"][x]):\n        pclass = test_df[\"Pclass\"][x] #Pclass = 3\n        test_df[\"Fare\"][x] = round(train_df[train_df[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)\n        \n#map Fare values into groups of numerical values\ntrain_df['FareBand'] = pd.qcut(train_df['Fare'], 4, labels = [1, 2, 3, 4])\ntest_df['FareBand'] = pd.qcut(test_df['Fare'], 4, labels = [1, 2, 3, 4])","d14a6283":"#drop Fare values\ntrain_df = train_df.drop(['Fare'], axis = 1)\ntest_df = test_df.drop(['Fare'], axis = 1)","e475d8df":"#To know training dataset dtypes \ntrain_df.dtypes","c0157faf":"#To know training dataset dtypes \ntest_df.dtypes","890e62c2":"#Convert to numeric\ntrain_df = train_df.apply(pd.to_numeric)","53217ea6":"train_df.dtypes","3881cd1e":"test_df = test_df.apply(pd.to_numeric)","1244a771":"train_df.dtypes","04d423c6":"#Split the dataset into 80% training and 20% testing\nfrom sklearn.model_selection import train_test_split\n\nX = train_df.drop(['Survived', 'PassengerId'], axis = 1)\nY = train_df['Survived']\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.2, random_state = 0)","e37af62f":"#Build Machine Learning Models:\n#1. Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\n\n#2. Support Vector Machines\nfrom sklearn.svm import SVC\nsvc_lin = SVC(kernel = 'linear', random_state = 0)\n\n#3. Support Vector Machines\nsvc_rbf = SVC(kernel = 'rbf', random_state = 0)\n\n#4. Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n\n#5.  KNearest Neighbors Classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n\n#6. Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\n\n#7. Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n\n#8. Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier(n_estimators = 100 , random_state = 1 )\n\n#9. AdaBoost Classifier\nfrom sklearn.ensemble import AdaBoostClassifier\nabc = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n\n#10. Extra Trees Classifier\nfrom sklearn.ensemble import ExtraTreesClassifier\netc = ExtraTreesClassifier(n_estimators=500, random_state=42, max_features=1., min_weight_fraction_leaf=0.5, criterion='gini')\n\n#11. XGB Classifier\nfrom xgboost import XGBClassifier\nxgbc = XGBClassifier(objective=\"binary:logistic\", random_state=42)\n\n#List of all the models with their indices\nmodelNames = [\"Logistic Regression\", \"SVC Linear\", \"SVCRBF\", \"Random Forest Classifier\", \"KNN Classifier\", \"Gaussian NaiveBayes\", \"Decision Tree\",\n              \"Gradien Boosting Classifier\", \"AdaBoost Classifier\", \"Extra Trees Classifier\", \"XGG Classifier\"]\nmodels = [lr, svc_lin, svc_rbf, rf, knn, gnb, dt, gbc, abc, etc, xgbc]\n\n","5168feb8":"#Create a function that returns train accuracy of different models.\ndef calculateTrainAccuracy(model):\n    #Returns training accuracy of a model\n    \n    model.fit(X_train, Y_train)\n    trainAccuracy = model.score(X_train, Y_train)\n    trainAccuracy = round(trainAccuracy*100, 2)\n    return trainAccuracy\n\n# Calculate train accuracy of all the models and store them in a dataframe\nmodelScores = list(map(calculateTrainAccuracy, models))\ntrainAccuracy = pd.DataFrame(modelScores, columns = [\"trainAccuracy\"], index=modelNames)\ntrainAccuracySorted = trainAccuracy.sort_values(by=\"trainAccuracy\", ascending=False)\nprint(\"Training Accuracy of the Classifiers:\")\ndisplay(trainAccuracySorted)","a35668fe":"#Show the confusion matrix and accuracy for all of the models on the valuate data\nfrom sklearn.metrics import confusion_matrix\n\nfor i in range(len(models)):\n  cm = confusion_matrix(Y_val, models[i].predict(X_val))\n\n  #Extract TN, FP, FN, TP\n  TN, FP, FN, TP = confusion_matrix(Y_val, models[i].predict(X_val)).ravel()\n\n  test_score = (TP + TN) \/ (TP + TN + FN +FP)\n\n  print(cm)\n  print('Model {} Validation Accuracy = \"{}\"'.format(modelNames[i], test_score))\n  print()","24a51a8d":"#set ids as PassengerId and predict survival \nids = test_df['PassengerId']\npredictions = dt.predict(test_df.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)","8d881ede":"**2) Read and Explore Dataset**","f3ef1320":"Sources:\n- https:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner\/notebook\n- https:\/\/www.kaggle.com\/eraaz1\/a-comprehensive-guide-to-titanic-machine-learning\n- https:\/\/www.youtube.com\/watch?v=rODWw2_1mCI&t=5675s","73158ae1":"**6. Choosing the Best Model**","e72eb830":"**3) Data Analysis**","81b8c648":"**1) Import Necessary Libraries**","55524ab8":"**6. Cleaning Data**","e20c9a17":"From the information above we know\n*   Categorical Variables: Survived(integer), Sex(string), Pclass(integer), Embarked(string)\n*   Numerical Variables: Age(float), Fare(float), Sibsp(integer), dan Parch(integer)\n*   Unique Variables: PassengerId(string), Name(string), Ticket(string), Cabin(string)\n","f10d11d5":"****TITANIC SURVIVAL PREDICTION****","6100adac":"The Titanic was a British ferry operated by the White Star Line that sank in the North Atlantic Ocean on April 15, 1912, after hitting an iceberg during its maiden voyage from Southampton to New York City. Of the estimated 2,224 ferries and crew members, more than 15000 died, making the sinking at that time one of the deadliest from that time to the present. With much public attention in the aftermath, the disaster became the material of many works of art and became the basis of the film genre.\n\nThis dataset consists of 12 columns, namely:\n1. PassengerId: the unique number of each passenger\n2. Survive: ferry survives (1) or perishes (0)\n3. Class P: ferry class\n4. Name : ferry name\n5. Gender: passenger gender\n6. Age: passenger age\n7. SibSp: number of siblings\n8. Parch: number of parents\/children\n9. Ticket: ticket number\n10. Fares: Amount of money used for tickets\n11. Cabin: cabin category\n12. Starting: The port where passengers depart (C = Cherbourg, Q = Queenstown, S = Southampton)","5e8e5e9c":"Some Observations:\n- There were a total of 891 passengers in our training set and many who did not survive\n- The Age variable lost about 19.8% of its value. I did age grouping so that sufficient missing values would be compared with the title names.\n- The Cabin Variable lost about 77.1% of its value. Because so many variables are missing, it will be difficult to fill in the missing values. We will probably remove these values from our dataset.\n- The Embarked Variable lost 0.22% of its value, which should be relatively harmless. Will be filled with the mode.\n- The Variable Fare will be transformed by labeling the contents of the data","345fc939":"As prediction\n- Female have a much higher chance of survival than men. The Sex feature is very important in our predictions.\n- People with higher socioeconomic class have a higher survival rate\n- In general, it is clear that people with more siblings or partners are less likely to survive. However, contrary to expectations, people without siblings or partners were less likely to survive than those with one or two people.\n- People with less than four parents or children on board are more likely to survive than those with four or more parents. Again, people who travel alone are less likely to survive than those with 1-3 parents or children.\n- Babies are more likely to survive than other age groups.","9f2d0a41":"**4. Visualization**","80eafb39":"I decided to use the Decision Tree Classifier model for the testing data","c6d0f2dd":"**7. Creating Submission File**"}}