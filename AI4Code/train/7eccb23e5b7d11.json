{"cell_type":{"1e1a74b6":"code","8b8d518f":"code","021cae28":"code","774faa04":"code","68d4c7aa":"code","9ac200dd":"code","e5bd8473":"code","9f3ee9e5":"code","a440f3d9":"code","9d69e639":"code","9d8dd3d0":"code","dd0dccf1":"code","6add8d50":"code","ef971dbb":"code","50fe9ae1":"code","b6771fc2":"code","9899e072":"code","8d136e0c":"code","b0bdd29c":"code","214ce71d":"code","67d832ce":"code","dd1314b9":"code","0f02b700":"code","dbc869c6":"code","094dcb83":"markdown","fe190790":"markdown","5f61643d":"markdown","afab5450":"markdown","7c9a7fa8":"markdown","42166f19":"markdown","ac5e4a4b":"markdown","9a1a86e4":"markdown","973a4f42":"markdown","ed0bf3c9":"markdown","bb393cb2":"markdown","3a11f3bb":"markdown","7a6b6818":"markdown","48b35890":"markdown","0756de73":"markdown","72fb6c04":"markdown"},"source":{"1e1a74b6":"import bs4 # beautiful soup for web scraping\nimport requests # request library for making HTTPS\/HTTP requests in Python\nimport time # standard time library\nfrom pprint import pprint # builtin module for pretty printing\nimport sys\nimport pandas as pd # Pandas for dataframes\n","8b8d518f":"# create a user agent for our request\nHEADERS = {\n    'User-Agent': 'Mozilla\/5.0 (X11; Linux x86_64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/67.0.3396.87 Safari\/537.36',\n}\n","021cae28":"# store URL we are going to be using \nurl = \"http:\/\/remote.co\/remote-jobs\/developer\"\n# collect contents of page using requests\nsource = requests.get(url,timeout=5, headers=HEADERS).text\n# create a soup object that contains the parsed HTML\nsoup = bs4.BeautifulSoup(source,'html.parser')\n","774faa04":"# Lets gather just the job listings from the beautiful soup object!\njob_listings = soup.findAll('a',{'class': 'card m-0 border-left-0 border-right-0 border-top-0 border-bottom'} )","68d4c7aa":"print(\"count of listings: \", len(job_listings)) #use the builtin Python method len() to check the size of our list\nprint(job_listings[0]) # lets see what some of this data looks like","9ac200dd":"job_dict = {} # lets create an empty dictionary.\n\n# Using the find() method of BeautifulSoup, we can gather the text contained by the span element we were looking at.\njob_dict['Job Title'] = job_listings[0].find('span',{'class':\n                                        'font-weight-bold larger'}).get_text() \n\n#lets print this dictionary and see what it contains!\njob_dict","e5bd8473":"#  title of company is a little harder...\ncompany = job_listings[0].find('p',{'class':\n                                        'm-0 text-secondary'}).get_text()\nprint(\"RAW TEXT:\\n\\n\", company)\n\n# lets fix this entry up a bit\n# lets try splitting on the \"|\" and picking the first entry \ncompany = job_listings[0].find('p',{'class':'m-0 text-secondary'}).get_text().split(\"|\")[0] \nprint(\"\\nTEXT AFTER SPLITTING ON '|' AND SELECTING THE FIRST INDEX:\\n\\n\",company)\n\n# looks like we still have some leading and trailing whitespace... lets fix that!\ncompany = company.strip()\nprint(\"\\nTEXT AFTER STRIPPING WHITESPACE:\\n\",company)\n\njob_dict['Company'] = company\nprint(\"\\n current state of our dictionary: \",job_dict)\n","9f3ee9e5":"#lets add the date \njob_dict['Posted'] = job_listings[0].find('date').get_text()\n# now lets print the current state of our job_dict\nprint(job_dict)\n","a440f3d9":"# lets write a function that can make a list of dictionaries of job postings \ndef parse_job_listing(job_listing):\n    # empty dictionary\n    job_dict = {}\n    # job title\n    job_dict['Job Title'] = job_listing.find('span',{'class':\n                                        'font-weight-bold larger'}).get_text() \n    \n    # company - notice that we combined some steps from above for this line\n    job_dict['Company'] = job_listing.find('p',{'class':'m-0 text-secondary'}).get_text().split(\"|\")[0].strip()\n    \n    # posted at\n    job_dict['Posted'] = job_listing.find('date').get_text()\n    return job_dict\n\n# lets try the function out\n# prep a list to insert job listings into\njob_listing_dicts = []\n\n# iterate through job listings\nfor job_listing in job_listings:\n    # append new dictionary to list\n    job_listing_dicts.append( parse_job_listing( job_listing)) # <-- using our new function\n\n# lets use pprint to check out the result\npprint(job_listing_dicts)\n    ","9d69e639":"# lets put this in a dataframe, much prettier\njobs_df = pd.DataFrame(job_listing_dicts)\njobs_df\n","9d8dd3d0":"# remember, there is an HREF in the \"a\" tag that leads to more details on the job posting..\njob_listings[0]['href']","dd0dccf1":"# we need to add this to the base URL if we want something that beautifulsoup can parse \ndetail_url = 'https:\/\/remote.co'+ job_listings[0]['href']\nprint(detail_url)","6add8d50":"# lets see what kind of data we can find here\nsource = requests.get(detail_url,timeout=5, headers=HEADERS).text # note we are using the new url\n# create a soup object that contains the parsed HTML\nsoup = bs4.BeautifulSoup(source,'html.parser')\n","ef971dbb":"# lets see what kind of data we can find here\n#looks like we can find the location of the gig\nprint(soup.find('span', {'class': 'location_sm'}).get_text())\n# also we can find the actual date posted!\nprint(soup.find('time')['datetime'])\n\n","50fe9ae1":"# lets add this new information to our old function\ndef parse_job_listing(job_listing):\n    job_dict = {}\n    job_dict['Job Title'] = job_listing.find('span',{'class':\n                                        'font-weight-bold larger'}).get_text() \n    job_dict['Company'] = job_listing.find('p',{'class':'m-0 text-secondary'}).get_text().split(\"|\")[0].strip()\n    #job_dict['Posted'] = job_listing.find('date').get_text() we dont need this anymore!\n\n    #SOME NEW STUFF! \n    detail_url = 'https:\/\/remote.co' + job_listing['href']\n    source = requests.get(detail_url,timeout=5, headers=HEADERS).text # note we are using the new url\n    soup = bs4.BeautifulSoup(source,'html.parser')\n    \n    job_dict['Location'] = soup.find('span', {'class': 'location_sm'}).get_text()\n    job_dict['Date Posted'] = soup.find('time')['datetime']\n    job_dict['URL'] = detail_url\n    return job_dict\n\n# prep a list to insert job listings into\njob_listing_dicts = []\n\n# iterate through job listings\nfor job_listing in job_listings:\n    # append new dictionary to list\n    job_listing_dicts.append( parse_job_listing( job_listing))\n    \njobs_df = pd.DataFrame(job_listing_dicts)\njobs_df\n\n    ","b6771fc2":"# now lets personalize this a bit... I'm going to make a list\n# of keywords that matter to myself for a job...\nkeywords = [\n    \"react\",\n    \"python\",\n    \"api\",\n    \"coffee\",\n    \"javascript\",\n    \"international\"\n]","9899e072":"# lets try to find these keywords in a job listing...\n# first, we need to make sure that we are only checking the \n# important part of the job description\nsource = requests.get('https:\/\/remote.co\/'+job_listings[0]['href'],timeout=5, headers=HEADERS).text # note we are using the new url\n# create a soup object that contains the parsed HTML\nsoup = bs4.BeautifulSoup(source,'html.parser')","8d136e0c":"# lucky for us, the text is all stored in a div called job_description\njob_description = soup.find('div',{'class':'job_description'}).get_text()\njob_description","b0bdd29c":"## lets go ahead and clean this up... for our match to work this needs to be lowercased.\njob_description = job_description.lower().replace(\"\\n\", \"\")\njob_description","214ce71d":"# lets try searching for a keyword\nprint(job_description.count('coffee'))","67d832ce":"# how about a simple for loop that checks for our keywords\nsum = 0\nfor keyword in keywords:\n    sum += job_description.count(keyword)\nprint(sum)","dd1314b9":"# lets add this to our original function that parses job descriptions\n# lets add this new information to our old function\ndef parse_job_listing(job_listing):\n    job_dict = {}\n    job_dict['Job Title'] = job_listing.find('span',{'class':'font-weight-bold larger'}).get_text() \n    job_dict['Company'] = job_listing.find('p',{'class':'m-0 text-secondary'}).get_text().split(\"|\")[0].strip()\n    \n    # job description\n    detail_url = 'https:\/\/remote.co' + job_listing['href']\n    source = requests.get(detail_url,timeout=5, headers=HEADERS).text # note we are using the new url\n    soup = bs4.BeautifulSoup(source,'html.parser')\n    \n    job_dict['Location'] = soup.find('span', {'class': 'location_sm'}).get_text()\n    job_dict['Date Posted'] = soup.find('time')['datetime']\n    job_dict['URL'] = detail_url\n    \n    # new stuff!\n    job_description_text = soup.find('div',{'class':'job_description'}).get_text().lower().replace(\"\\n\", \"\")\n\n    for keyword in keywords:\n        job_dict[keyword] = job_description_text.count(keyword)\n        \n        \n    return job_dict\n\n# prep a list to insert job listings into\njob_listing_dicts = []\n\n# iterate through job listings\nfor job_listing in job_listings:\n    # append new dictionary to list\n    job_listing_dicts.append( parse_job_listing( job_listing))\n    \njobs_df = pd.DataFrame(job_listing_dicts)\njobs_df\n\n    ","0f02b700":"# can we plot this?\njob_df_plotted = jobs_df.plot.bar(rot=90, stacked=True, x=\"Company\",figsize=(20, 12))","dbc869c6":"import bs4 # beautiful soup for web scraping\nimport requests # request library for making HTTPS\/HTTP requests in Python\nimport pandas as pd # Pandas for dataframes\nimport matplotlib.pyplot as plt # display the plot\n\n#~~~~~~~ Global variables ~~~~~~~~~~~~~~~~~#\n# create a user agent header for our requests\nHEADERS = {\n    'User-Agent': 'Mozilla\/5.0 (X11; Linux x86_64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/67.0.3396.87 Safari\/537.36',\n}\n\n#~~~~~~~~~~ job listing parser ~~~~~~~~~~~#\ndef parse_job_listing(job_listing, keywords):\n    job_dict = {}\n    job_dict['Job Title'] = job_listing.find(\n        'span',{'class':'font-weight-bold larger'}\n    ).get_text()\n\n    job_dict['Company'] = job_listing.find(\n        'p',{'class':'m-0 text-secondary'}\n    ).get_text().split(\"|\")[0].strip()\n\n    # prep the url so we can parse the job description page\n    detail_url = 'https:\/\/remote.co' + job_listing['href']\n\n    print(f\"scraping {detail_url}\")\n\n    # note we are using the new url as the source\n    source = requests.get(detail_url,timeout=5, headers=HEADERS).text\n    # create a new beautifulsoup object\n    soup = bs4.BeautifulSoup(source,'html.parser')\n    # gather the new details that we can find from the job description url\n    job_dict['Location'] = soup.find('span', {'class': 'location_sm'}).get_text()\n    job_dict['Date Posted'] = soup.find('time')['datetime']\n    job_dict['URL'] = detail_url\n\n    # use our basic keyword matching to find our desired skills\n    job_description_text = soup.find('div',{'class':'job_description'}).get_text().lower().replace(\"\\n\", \"\")\n\n    for keyword in keywords:\n        job_dict[keyword] = job_description_text.count(keyword)\n\n    # return the wrangled job description\n    return job_dict\n\n\nif __name__ == \"__main__\":\n    # url we are going to scrape\n    url = \"https:\/\/remote.co\/remote-jobs\/developer\"\n    # define some important keywords\n    keywords = [\n        \"react\",\n        \"python\",\n        \"api\",\n        \"coffee\",\n        \"javascript\",\n        \"international\"\n    ]\n    # collect the contents of page\n    source = requests.get(url,timeout=5, headers=HEADERS).text\n\n    # create a beautifulsoup object that contains the parsed HTML\n    soup = bs4.BeautifulSoup(source,'html.parser')\n\n    # find the job listings\n    job_listings = soup.findAll('a',{'class': 'card m-0 border-left-0 border-right-0 border-top-0 border-bottom'} )\n\n    # create a list of dictionaries containing wrangled data\n    job_listing_dicts = [parse_job_listing( job_listing, keywords) for job_listing in job_listings]\n\n    # create a dataframe out of the results\n    jobs_df = pd.DataFrame(job_listing_dicts)\n\n    # plot the dataframe\n    jobs_chart = jobs_df.plot.bar(rot=90, stacked=True, x=\"Company\",figsize=(20, 12))\n    plt.show()\n","094dcb83":"# Lets use our exploratory analysis to create a function. <br>\nNow that we know the atomic elements of our job description, we can create a function that should generally work with all similar listings.\n","fe190790":"Lets find a date posted for our listing. Look at that! There is a `<date>` element. This one should be much easier.<br>\n![image.png](attachment:4ec9504a-5f10-440a-b0b8-e49a391e7924.png)","5f61643d":"## Gathering \/ parsing the HTML with beautifulsoup and requests","afab5450":"# FINAL CODE","7c9a7fa8":"First, we need to set up a user agent. The user agent is what will identify who we are to the server we are scraping.","42166f19":"Obviously the HTML above is not very digestable.<br>\nBrowsing the above HTML, you might notice that job title is contained by a `span` with the class `font-weight-bold larger`. <br>\nWe can use the BeautifulSoup method [find()](https:\/\/www.crummy.com\/software\/BeautifulSoup\/bs4\/doc\/#find-all) to search specifically for this element in the HTML.\n","ac5e4a4b":"## Putting it all together and plotting","9a1a86e4":"We can use some basic text matching to create a primitive custom parser which we can use to \"rank\" the job descriptions.","973a4f42":"After inspect-element browsing the HTML of the page, we find that there is a common HTML style that contains the job listings. <br>\nWe can use the BeautifulSoup method [findall()](https:\/\/www.crummy.com\/software\/BeautifulSoup\/bs4\/doc\/#find-all) to filter the data to just what we are interested in.\n<br><br>\n\n![image.png](attachment:c41aa924-4afb-424d-b13c-cf05c28928c2.png)","ed0bf3c9":"We can print the length of the `job_listings` list to see how many individual \"job postings\" we have gathered.<br>\nAdditionally, we can print `job_listings[0]` to preview the sort of HTML we are dealing with.","bb393cb2":"We can visualize this data much cleaner in a [Pandas Dataframe](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.html)","3a11f3bb":"You may have noticed an `href` inside of the element that contains the job listing. <br>\nthis points to the more detailed job description.\n","7a6b6818":"Looking back at the above HTML, we can see that the company name is also contained inside of the `job_listing`.<br> \n![image.png](attachment:bee7c333-8c5d-400a-81b4-27e4d828633a.png) <br>\nWe're going to have to come up with a creative solution to make this into something easily readable. <br>\nUpon further inspection, you might notice that there is a `|` symbol being used to split the different elements contained in the `<p>` tag that has the company name.<br>\nWe can use the builtin Python string method `.split(character)` to split our string into an array based on the character that we pass to the method.<br> \nAdditionally, we can use the builtin Python string method `.strip()` to remove the leading and trailing whitespace from our `company` string.\n\n\n","48b35890":" ## Adding some keywords","0756de73":"Next, lets go ahead and gather the contents of the URL we are going to scrape using [requests](https:\/\/docs.python-requests.org\/en\/latest\/user\/quickstart\/). <br>\nAfter we gather the contents, we are going to create our [BeautifulSoup](https:\/\/www.crummy.com\/software\/BeautifulSoup\/bs4\/doc\/) object.","72fb6c04":"## Digging deeper into the job description"}}