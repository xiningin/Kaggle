{"cell_type":{"e5dd120e":"code","abca0539":"code","74e0727d":"code","9e891757":"code","78889ac8":"code","77e38983":"code","c7911cc6":"code","4a7b20a8":"code","b17c18ea":"code","ce912cc5":"code","dd3256db":"code","491e99e0":"code","7a376e03":"code","b2e12f2c":"code","12670f97":"code","b76380db":"code","dadb5ef1":"code","46ebaafb":"code","f044e214":"code","f877862a":"code","bfd6ebee":"code","e3479498":"code","e7d29489":"code","e86d0e2b":"code","f84b2ca4":"code","6fc1599e":"code","6f84eb69":"code","a6f62eaf":"code","418ec645":"code","3e21b877":"code","b7628994":"code","da77abc6":"code","885c9c3e":"code","fcd31bce":"code","e517a2a9":"code","69b324dc":"code","57a0e0c8":"code","ba8f3012":"code","ce7b4fb6":"code","43679ff3":"code","aeb4611a":"code","b5cf80fc":"code","30df4204":"code","992f8662":"code","b8024b85":"code","a337541e":"code","a02ee374":"code","da71508e":"code","edde814d":"code","27fc59c8":"code","52cca7a3":"code","4326718c":"code","f9ea60b8":"code","fa2339e5":"code","e54b9c43":"code","3a27aa7c":"code","dadbc75a":"code","01ded19b":"code","9785cf0b":"code","2a071362":"code","1d07d225":"code","ed3d5e03":"code","7db0d646":"code","9e3bb1d1":"code","855600da":"code","db931a00":"code","97bed538":"code","10fd359a":"code","c3d902cc":"code","3e90fdf7":"code","4c6a69cc":"code","217f6271":"code","f0fecfe1":"code","a3db1e30":"markdown","f275f365":"markdown","5e30a2cb":"markdown","b9a510aa":"markdown","ec0df217":"markdown","23661c88":"markdown","2dcc0f89":"markdown","40c6200f":"markdown","ca9571a4":"markdown","ec7acd43":"markdown"},"source":{"e5dd120e":"#Importing all the libraries we need\n\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\nfrom keras.layers import Dense , Dropout\nfrom keras.models import Sequential\n\nfor dir in os.walk(\"\/kaggle\/input\/\"):\n    print(dir[2])","abca0539":"# Getting the training data\n\ntraining_data = pd.read_csv('..\/input\/train.csv')\n\n# Printing first fice instances of training data\ntraining_data.head(10)","74e0727d":"# Printing last five instances of training data\n\ntraining_data.tail(10)","9e891757":"# Getting a total number of data values we have\n\nprint(len(training_data))","78889ac8":"# Checking for null values\n\ntraining_data.isna().sum()","77e38983":"# Droping the columns that are not necessary\n\ntraining_data = training_data.drop(columns = ['Ticket','Name','Cabin','PassengerId'])","c7911cc6":"training_data.isna().sum()","4a7b20a8":"# Replacing Null values in dataset with mean values\n\nmean_value = training_data['Age'].mean()\ntraining_data['Age'] = training_data['Age'].fillna(mean_value) ","b17c18ea":"training_data.isna().sum()","ce912cc5":"# Dropping the values that are null\n\ntraining_data = training_data.dropna()","dd3256db":"print(len(training_data))","491e99e0":"training_data.head()","7a376e03":"training_data.info()","b2e12f2c":"training_data.describe()","12670f97":"# This will give us the count of unique values present in Survived column\n\ntraining_data['Survived'].value_counts()","b76380db":"# Plotting a graph for visualization\n\ntraining_data['Survived'].value_counts().plot.bar()","dadb5ef1":"#Generating Testing data\n\ntesting_data = pd.read_csv(\"..\/input\/test.csv\")","46ebaafb":"# First 10 instances of testing_data \n\ntesting_data.head(10)","f044e214":"# Last 10 instances of testing_data\n\ntesting_data.tail(10)","f877862a":"# Getting the total number of instances in testing_data\n\nprint(len(testing_data))","bfd6ebee":"# Getting count of Na values\n\ntesting_data.isna().sum()","e3479498":"# Droping columns that are not necessary\npassenger_id = pd.DataFrame(testing_data['PassengerId'])\ntesting_data = testing_data.drop(columns = [ 'PassengerId' , 'Cabin' , 'Name' , 'Ticket'])\npassenger_id.head()\nprint(len(passenger_id))","e7d29489":"# Filling the null values with mean values\n\nmean_value = testing_data['Age'].mean()\ntesting_data['Age'] = testing_data['Age'].fillna(mean_value)\nmean_value = testing_data['Fare'].mean()\ntesting_data['Fare'] = testing_data['Fare'].fillna(mean_value)","e86d0e2b":"testing_data = testing_data.dropna()","f84b2ca4":"testing_data.isna().sum()","6fc1599e":"print(len(testing_data))","6f84eb69":"# Reading the actual labels for test data\n\ngender_submission = pd.read_csv(\"..\/input\/gender_submission.csv\")\ngender_submission.head()","a6f62eaf":"len(gender_submission)","418ec645":"gender_submission['Survived'].value_counts().plot.bar()","3e21b877":"training_data.head()","b7628994":"# Encoding the values from column Sex and Embarked\n\nenc = LabelEncoder()\ntraining_data['Sex'] = enc.fit_transform(training_data['Sex'])\ntraining_data['Embarked'] = enc.fit_transform(training_data['Embarked'])","da77abc6":"training_data.head()","885c9c3e":"training_data['Sex'].value_counts().plot.bar()","fcd31bce":"training_data['Embarked'].value_counts().plot.bar()","e517a2a9":"sns.pairplot(training_data,hue=\"Survived\")","69b324dc":"# Generating trianing data\n\nX_train = training_data.iloc[:,1:]\nY_train = np.array(training_data['Survived'])","57a0e0c8":"# Converting it into numpy array\n\nX_train = np.array(X_train)\nprint(X_train.shape)","ba8f3012":"Y_train = np.array(Y_train)\nprint(Y_train.shape)","ce7b4fb6":"print(X_train[0,:])","43679ff3":"print(X_train[0:5])","aeb4611a":"print(Y_train[0:5])","b5cf80fc":"# Splitting training data into train and test, becuase we don't have test data here and the test data in test.csv is for prediction purpose so we will work on training data\n\nX_t , x_test , Y_t , y_test = train_test_split(X_train,Y_train)","30df4204":"Y_t.shape","992f8662":"# Creating our Neural Network\n\nmodel = Sequential()\n\n# First Hidden layer with 256 neurons\nmodel.add(Dense(256 , activation = 'sigmoid' , input_dim = (7)))\n\n# Second Hideen layer with 256 neurons\nmodel.add(Dense(256 , activation = 'relu'))\n\n# Third Hidden layer with 128 neurons\nmodel.add(Dense(128 , activation = 'sigmoid'))\n\n# Fourth Hidden layer with 128 neurons\nmodel.add(Dense(128 , activation = 'relu'))\n\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1 , activation = 'sigmoid'))","b8024b85":"# Defining rules for our Neural Netowrk\n\nmodel.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'])","a337541e":"# Fitting data to our model\n\nmodel.fit( X_t , Y_t , epochs=50 , batch_size=32)","a02ee374":"# Evaluating our model on test data\n\nmodel.evaluate(x_test,y_test , batch_size = 32)","da71508e":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\nclassifier.fit(X_t,Y_t)","edde814d":"# Evaluating on test data\nclassifier.score(x_test,y_test)","27fc59c8":"from sklearn.ensemble import RandomForestClassifier\nclassifier_2 = RandomForestClassifier()\nclassifier_2.fit(X_t,Y_t)","52cca7a3":"# Evaluating on test data\nclassifier_2.score(x_test,y_test)","4326718c":"from sklearn.ensemble import GradientBoostingClassifier\nclassifier_3 = GradientBoostingClassifier()\nclassifier_3.fit(X_t,Y_t)","f9ea60b8":"# Evaluating on test data\nclassifier_3.score(x_test,y_test)","fa2339e5":"# Cross validation on Logistic Regression\nresult = cross_validate(classifier , X_train , Y_train , cv=5)\nprint(result)","e54b9c43":"# Cross validation on Random Forest Classifier\nresult = cross_validate(classifier_2 , X_train , Y_train , cv=5)\nprint(result)","3a27aa7c":"# Cross validation on Gradient Boosting Classifier\nresult = cross_validate(classifier_3 , X_train , Y_train , cv=5)\nprint(result)","dadbc75a":"print(type(testing_data))\nprint(len(testing_data))\nprint(testing_data[0:5])","01ded19b":"# Encoding 'Sex' and 'Embarked' column of testing_data\ntesting_data['Sex'] = enc.fit_transform(testing_data['Sex'])\ntesting_data['Embarked'] = enc.fit_transform(testing_data['Embarked'])","9785cf0b":"# Forst five instances of testing_data\ntesting_data.head()","2a071362":"# X_pred is variable that stores values to be predicted\nX_pred = np.array(testing_data)","1d07d225":"print(X_pred[0:5])","ed3d5e03":"X_pred.shape","7db0d646":"# Predicting values, here Y_pred contains predicted values \nY_pred = model.predict(X_pred).round()","9e3bb1d1":"# Y_test contains the actual labels for our prediction data\nY_test = np.array(gender_submission)\nY_test = Y_test[:,1]","855600da":"print(Y_test)\nprint(Y_test.shape)","db931a00":"Y_pred = Y_pred.reshape(418,)\nprint(Y_pred)\nprint(Y_pred.shape)","97bed538":"cm = confusion_matrix(Y_test , Y_pred)\n\nplt.subplots(figsize = (10,8))\n\nsns.heatmap(cm , xticklabels = ['Survived' , 'Dead'] , yticklabels = ['Survived','Dead'])","10fd359a":"Y_pred = classifier.predict(X_pred).round()\nprint(Y_pred)","c3d902cc":"cm = confusion_matrix(Y_test , Y_pred)\n\nplt.subplots(figsize = (10,8))\n\nsns.heatmap(cm , xticklabels = ['Survived' , 'Dead'] , yticklabels = ['Survived','Dead'])","3e90fdf7":"Y_pred = classifier_2.predict(X_pred).round()\nprint(Y_pred)\n#Y_pred = pd.DataFrame(Y_pred , columns=[\"Survived\"])\n#passenger_id['Survived'] = Y_pred\n#print(len(passenger_id))\n#print(len(Y_pred))\n#passenger_id['Survived'] = Y_pred\n#passenger_id.head(10)\n#passenger_id.head(10)\n#Y_pred.head(10)\n#predictions = pd.concat([passenger_id,Y_pred] , axis=1, join='inner')\n#predictions.head()","4c6a69cc":"cm = confusion_matrix(Y_test , Y_pred)\n\nplt.subplots(figsize = (10,8))\n\nsns.heatmap(cm , xticklabels = ['Survived' , 'Dead'] , yticklabels = ['Survived','Dead'])","217f6271":"Y_pred = classifier_3.predict(X_pred).round()\nprint(Y_pred)\nprint(len(passenger_id))\nprint(len(Y_pred))\npassenger_id['Survived'] = Y_pred\npassenger_id.head(10)\npassenger_id.to_csv(\"predictions.csv\" , index=False)","f0fecfe1":"cm = confusion_matrix(Y_test , Y_pred)\n\nplt.subplots(figsize = (10,8))\n\nsns.heatmap(cm , xticklabels = ['Survived' , 'Dead'] , yticklabels = ['Survived','Dead'])","a3db1e30":"> # 3. Random Forest Classifier","f275f365":"> ## Confusion Matrix for Random Forest","5e30a2cb":"> # 4. Gradient Boosting Classifier","b9a510aa":"> # 2. Logistic Regression","ec0df217":"> # 1. Neural Network","23661c88":"> ## Confustion Matrix for Gradient Boosting Classifier","2dcc0f89":"> # Hi Guys,\n\n## This notebook is on classification algorithms that shows how well different algorithms perform on the same dataset. In this Notebook the classification algorithms I have used are mentioned below:\n\n\n### 1. Neural Network\n### 2. Logistic Regression\n### 3. Random Forest\n### 4. Gradient Boosting Classifier\n\n## And the winner of this Competiton is Logisitc Regression as per confusion matrix, it gave a better accuracy rate at the time of cross_validation, and also when predicting the prediction data that is present in test.csv and gender_submission.csv has the actual labels that should be for test data.\n## So you can predict the testing data and then cross_check it with the actual labels.","40c6200f":"### If you like this notebook, please upvote for this notebook","ca9571a4":"> ## Confusion Matrix for Logistic Regression","ec7acd43":"> ## Confusion Matrix for Neural Network "}}