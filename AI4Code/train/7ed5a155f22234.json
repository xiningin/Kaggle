{"cell_type":{"9eedc5a3":"code","1fa9fb89":"code","ca808738":"code","132ed71b":"code","5f3f5485":"code","a6376bc3":"code","20880954":"code","3c0a3bc6":"code","4fa10b9a":"code","7db01679":"code","09889016":"code","6a8d24fd":"code","8b163fc8":"code","cb81daae":"code","45e82c16":"code","0b40bfb4":"code","21884b83":"code","1a8f66c1":"code","e94c1ff9":"code","8d9ec372":"code","f2e1bc07":"code","6aa8bedb":"code","e5798b18":"code","cc7cbb5e":"code","3caa5d3e":"code","dc0c4857":"code","5c7113d0":"code","06f948eb":"code","7614fa4c":"code","07aea6af":"code","be7f4237":"code","a31baea7":"code","daeb608a":"code","201a2155":"markdown","d1d1a98b":"markdown","ec31c209":"markdown","0ef666e6":"markdown","37ec3189":"markdown","e0b3cb93":"markdown","e810f842":"markdown","f55dc180":"markdown","8b07acfb":"markdown","a18bc2eb":"markdown","11650548":"markdown","1b97c77e":"markdown","84a61244":"markdown","fdbccc37":"markdown","be6988d5":"markdown","6ae45833":"markdown"},"source":{"9eedc5a3":"from IPython.display import clear_output\n!pip install --upgrade imutils\nclear_output()","1fa9fb89":"import numpy as np \nfrom tqdm import tqdm\nimport cv2\nimport os\nimport shutil\nimport itertools\nimport imutils\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras import layers\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import EarlyStopping\n\ninit_notebook_mode(connected=True)\nRANDOM_SEED = 123","ca808738":"!apt-get install tree\nclear_output()\n# create new folders\n!mkdir TRAIN TEST VAL TRAIN\/YES TRAIN\/NO TEST\/YES TEST\/NO VAL\/YES VAL\/NO\n!tree -d","132ed71b":"IMG_PATH = '..\/input\/brain-mri-images-for-brain-tumor-detection\/brain_tumor_dataset\/'\n# split the data by train\/val\/test\nfor CLASS in os.listdir(IMG_PATH):\n    if not CLASS.startswith('.'):\n        IMG_NUM = len(os.listdir(IMG_PATH + CLASS))\n        for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH + CLASS)):\n            img = IMG_PATH + CLASS + '\/' + FILE_NAME\n            if n < 5:\n                shutil.copy(img, 'TEST\/' + CLASS.upper() + '\/' + FILE_NAME)\n            elif n < 0.8*IMG_NUM:\n                shutil.copy(img, 'TRAIN\/'+ CLASS.upper() + '\/' + FILE_NAME)\n            else:\n                shutil.copy(img, 'VAL\/'+ CLASS.upper() + '\/' + FILE_NAME)","5f3f5485":"def load_data(dir_path, img_size=(100,100)):\n    \"\"\"\n    Carrega as imagens redimensionadas como np.arrays para a workspace\n    \"\"\"\n    X = []\n    y = []\n    i = 0\n    labels = dict()\n    for path in tqdm(sorted(os.listdir(dir_path))):\n        if not path.startswith('.'):\n            labels[i] = path\n            for file in os.listdir(dir_path + path):\n                if not file.startswith('.'):\n                    img = cv2.imread(dir_path + path + '\/' + file)\n                    X.append(img)\n                    y.append(i)\n            i += 1\n    X = np.array(X)\n    y = np.array(y)\n    print(f'{len(X)} images loaded from {dir_path} directory.')\n    return X, y, labels\n\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    Esta fun\u00e7\u00e3o imprime e plota a matriz de confus\u00e3o.\n    A normaliza\u00e7\u00e3o pode ser aplicada definindo `normalize = True`. \n    \"\"\"\n    plt.figure(figsize = (6,6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    cm = np.round(cm,2)\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","a6376bc3":"TRAIN_DIR = 'TRAIN\/'\nTEST_DIR = 'TEST\/'\nVAL_DIR = 'VAL\/'\nIMG_SIZE = (224,224)\n\n# Usando a fun\u00e7\u00e3o predefinida para carregar os dados da imagem no espa\u00e7o de trabalho\nX_train, y_train, labels = load_data(TRAIN_DIR, IMG_SIZE)\nX_test, y_test, _ = load_data(TEST_DIR, IMG_SIZE)\nX_val, y_val, _ = load_data(VAL_DIR, IMG_SIZE)","20880954":"y = dict()\ny[0] = []\ny[1] = []\nfor set_name in (y_train, y_val, y_test):\n    y[0].append(np.sum(set_name == 0))\n    y[1].append(np.sum(set_name == 1))\n\ntrace0 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[0],\n    name='No',\n    marker=dict(color='#33cc33'),\n    opacity=0.7\n)\ntrace1 = go.Bar(\n    x=['Train Set', 'Validation Set', 'Test Set'],\n    y=y[1],\n    name='Yes',\n    marker=dict(color='#ff3300'),\n    opacity=0.7\n)\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title='Contagem de classes em cada conjunto',\n    xaxis={'title': 'Set'},\n    yaxis={'title': 'Count'}\n)\nfig = go.Figure(data, layout)\niplot(fig)","3c0a3bc6":"def plot_samples(X, y, labels_dict, n=50):\n    \"\"\"\n    Cria um gr\u00e1fico para o n\u00famero desejado de imagens (n) do conjunto especificado\n    \"\"\"\n    for index in range(len(labels_dict)):\n        imgs = X[np.argwhere(y == index)][:n]\n        j = 10\n        i = int(n\/j)\n\n        plt.figure(figsize=(15,6))\n        c = 1\n        for img in imgs:\n            plt.subplot(i,j,c)\n            plt.imshow(img[0])\n\n            plt.xticks([])\n            plt.yticks([])\n            c += 1\n        plt.suptitle('Tumor: {}'.format(labels_dict[index]))\n        plt.show()","4fa10b9a":"plot_samples(X_train, y_train, labels, 30)","7db01679":"RATIO_LIST = []\nfor set in (X_train, X_test, X_val):\n    for img in set:\n        RATIO_LIST.append(img.shape[1]\/img.shape[0])\n        \nplt.hist(RATIO_LIST)\nplt.title('Distribui\u00e7\u00e3o de propor\u00e7\u00f5es de imagem')\nplt.xlabel('Valor da propor\u00e7\u00e3o')\nplt.ylabel('Contagem')\nplt.show()","09889016":"def crop_imgs(set_name, add_pixels_value=0):\n    \"\"\"\n    Encontra os pontos extremos na imagem e recorta o ret\u00e2ngulo deles\n    \"\"\"\n    set_new = []\n    for img in set_name:\n        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        gray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n        # limite a imagem, em seguida, execute uma s\u00e9rie de eros\u00f5es  +\n        # dilata\u00e7\u00f5es para remover quaisquer pequenas regi\u00f5es de ru\u00eddo \n        thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n        thresh = cv2.erode(thresh, None, iterations=2)\n        thresh = cv2.dilate(thresh, None, iterations=2)\n\n        # encontre contornos na imagem limite e, em seguida, pegue o maior \n        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cnts = imutils.grab_contours(cnts)\n        c = max(cnts, key=cv2.contourArea)\n\n        # encontre os pontos extremos \n        extLeft = tuple(c[c[:, :, 0].argmin()][0])\n        extRight = tuple(c[c[:, :, 0].argmax()][0])\n        extTop = tuple(c[c[:, :, 1].argmin()][0])\n        extBot = tuple(c[c[:, :, 1].argmax()][0])\n\n        \n        # cortar\n        ADD_PIXELS = add_pixels_value\n        new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n        set_new.append(new_img)\n\n    return np.array(set_new)","6a8d24fd":"img = cv2.imread('..\/input\/brain-mri-images-for-brain-tumor-detection\/brain_tumor_dataset\/yes\/Y108.jpg')\nimg = cv2.resize(\n            img,\n            dsize=IMG_SIZE,\n            interpolation=cv2.INTER_CUBIC\n        )\ngray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\ngray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n# limite a imagem, em seguida, execute uma s\u00e9rie de eros\u00f5es  +\n# dilata\u00e7\u00f5es para remover quaisquer pequenas regi\u00f5es de ru\u00eddo \nthresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\nthresh = cv2.erode(thresh, None, iterations=2)\nthresh = cv2.dilate(thresh, None, iterations=2)\n\n# encontre contornos na imagem limite e, em seguida, pegue o maior\ncnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = imutils.grab_contours(cnts)\nc = max(cnts, key=cv2.contourArea)\n\n# encontre os pontos extremos\nextLeft = tuple(c[c[:, :, 0].argmin()][0])\nextRight = tuple(c[c[:, :, 0].argmax()][0])\nextTop = tuple(c[c[:, :, 1].argmin()][0])\nextBot = tuple(c[c[:, :, 1].argmax()][0])\n\n# adicionar contorno na imagem\nimg_cnt = cv2.drawContours(img.copy(), [c], -1, (0, 255, 255), 4)\n\n# adicione pontos extremos\nimg_pnt = cv2.circle(img_cnt.copy(), extLeft, 8, (0, 0, 255), -1)\nimg_pnt = cv2.circle(img_pnt, extRight, 8, (0, 255, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extTop, 8, (255, 0, 0), -1)\nimg_pnt = cv2.circle(img_pnt, extBot, 8, (255, 255, 0), -1)\n\n# cortar\nADD_PIXELS = 0\nnew_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()","8b163fc8":"plt.figure(figsize=(15,6))\nplt.subplot(141)\nplt.imshow(img)\nplt.xticks([])\nplt.yticks([])\nplt.title('1. Obtenha a imagem original')\nplt.subplot(142)\nplt.imshow(img_cnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('2. Encontre o maior contorno')\nplt.subplot(143)\nplt.imshow(img_pnt)\nplt.xticks([])\nplt.yticks([])\nplt.title('3. Encontre os pontos extremos')\nplt.subplot(144)\nplt.imshow(new_img)\nplt.xticks([])\nplt.yticks([])\nplt.title('4. Recorte a imagem')\nplt.show()","cb81daae":"# aplique isso para cada conjunto\nX_train_crop = crop_imgs(set_name=X_train)\nX_val_crop = crop_imgs(set_name=X_val)\nX_test_crop = crop_imgs(set_name=X_test)","45e82c16":"plot_samples(X_train_crop, y_train, labels, 30)","0b40bfb4":"def save_new_images(x_set, y_set, folder_name):\n    i = 0\n    for (img, imclass) in zip(x_set, y_set):\n        if imclass == 0:\n            cv2.imwrite(folder_name+'NO\/'+str(i)+'.jpg', img)\n        else:\n            cv2.imwrite(folder_name+'YES\/'+str(i)+'.jpg', img)\n        i += 1","21884b83":"# Criando as pastas e salvando novas imagens\n!mkdir TRAIN_CROP TEST_CROP VAL_CROP TRAIN_CROP\/YES TRAIN_CROP\/NO TEST_CROP\/YES TEST_CROP\/NO VAL_CROP\/YES VAL_CROP\/NO\n\nsave_new_images(X_train_crop, y_train, folder_name='TRAIN_CROP\/')\nsave_new_images(X_val_crop, y_val, folder_name='VAL_CROP\/')\nsave_new_images(X_test_crop, y_test, folder_name='TEST_CROP\/')","1a8f66c1":"def preprocess_imgs(set_name, img_size):\n    \"\"\"\n    Redimensione e aplique o pr\u00e9-processamento VGG-15\n    \"\"\"\n    set_new = []\n    for img in set_name:\n        img = cv2.resize(\n            img,\n            dsize=img_size,\n            interpolation=cv2.INTER_CUBIC\n        )\n        set_new.append(preprocess_input(img))\n    return np.array(set_new)","e94c1ff9":"X_train_prep = preprocess_imgs(set_name=X_train_crop, img_size=IMG_SIZE)\nX_test_prep = preprocess_imgs(set_name=X_test_crop, img_size=IMG_SIZE)\nX_val_prep = preprocess_imgs(set_name=X_val_crop, img_size=IMG_SIZE)","8d9ec372":"plot_samples(X_train_prep, y_train, labels, 30)","f2e1bc07":"# definindo os par\u00e2metros que queremos alterar aleatoriamente\ndemo_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.05,\n    height_shift_range=0.05,\n    rescale=1.\/255,\n    shear_range=0.05,\n    brightness_range=[0.1, 1.5],\n    horizontal_flip=True,\n    vertical_flip=True\n)","6aa8bedb":"os.mkdir('preview')\nx = X_train_crop[0]  \nx = x.reshape((1,) + x.shape) \n\ni = 0\nfor batch in demo_datagen.flow(x, batch_size=1, save_to_dir='preview', save_prefix='aug_img', save_format='jpg'):\n    i += 1\n    if i > 20:\n        break ","e5798b18":"plt.imshow(X_train_crop[0])\nplt.xticks([])\nplt.yticks([])\nplt.title('Imagem Original')\nplt.show()\n\nplt.figure(figsize=(15,6))\ni = 1\nfor img in os.listdir('preview\/'):\n    img = cv2.cv2.imread('preview\/' + img)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.subplot(3,7,i)\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n    i += 1\n    if i > 3*7:\n        break\nplt.suptitle('Imagens Aumentadas')\nplt.show()","cc7cbb5e":"!rm -rf preview\/","3caa5d3e":"TRAIN_DIR = 'TRAIN_CROP\/'\nVAL_DIR = 'VAL_CROP\/'\n\ntrain_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    brightness_range=[0.5, 1.5],\n    horizontal_flip=True,\n    vertical_flip=True,\n    preprocessing_function=preprocess_input\n)\n\ntest_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input\n)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    TRAIN_DIR,\n    color_mode='rgb',\n    target_size=IMG_SIZE,\n    batch_size=32,\n    class_mode='binary',\n    seed=RANDOM_SEED\n)\n\n\nvalidation_generator = test_datagen.flow_from_directory(\n    VAL_DIR,\n    color_mode='rgb',\n    target_size=IMG_SIZE,\n    batch_size=16,\n    class_mode='binary',\n    seed=RANDOM_SEED\n)","dc0c4857":"# Carregando o modelo base\nvgg16_weight_path = '..\/input\/keras-pretrained-models\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\nbase_model = VGG16(\n    weights=vgg16_weight_path,\n    include_top=False, \n    input_shape=IMG_SIZE + (3,)\n)","5c7113d0":"NUM_CLASSES = 1\n\nmodel = Sequential()\nmodel.add(base_model)\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(NUM_CLASSES, activation='sigmoid'))\n\nmodel.layers[0].trainable = False\n\nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer=RMSprop(lr=1e-4),\n    metrics=['accuracy']\n)\n\nmodel.summary()","06f948eb":"EPOCHS = 30\nes = EarlyStopping(\n    monitor='val_acc', \n    mode='max',\n    patience=6\n)\n\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=50,\n    epochs=EPOCHS,\n    validation_data=validation_generator,\n    validation_steps=25,\n    callbacks=[es]\n)","7614fa4c":"# plotando a performance do model\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs_range = range(1, len(history.epoch) + 1)\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Train Set')\nplt.plot(epochs_range, val_acc, label='Val Set')\nplt.legend(loc=\"best\")\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Train Set')\nplt.plot(epochs_range, val_loss, label='Val Set')\nplt.legend(loc=\"best\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Model Loss')\n\nplt.tight_layout()\nplt.show()","07aea6af":"# validate on val set\npredictions = model.predict(X_val_prep)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\naccuracy = accuracy_score(y_val, predictions)\nprint('Val Accuracy = %.2f' % accuracy)\n\nconfusion_mtx = confusion_matrix(y_val, predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","be7f4237":"# validate on test set\npredictions = model.predict(X_test_prep)\npredictions = [1 if x>0.5 else 0 for x in predictions]\n\naccuracy = accuracy_score(y_test, predictions)\nprint('Test Accuracy = %.2f' % accuracy)\n\nconfusion_mtx = confusion_matrix(y_test, predictions) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(labels.items()), normalize=False)","a31baea7":"ind_list = np.argwhere((y_test == predictions) == False)[:, -1]\nif ind_list.size == 0:\n    print('There are no missclassified images.')\nelse:\n    for i in ind_list:\n        plt.figure()\n        plt.imshow(X_test_crop[i])\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(f'Actual class: {y_val[i]}\\nPredicted class: {predictions[i]}')\n        plt.show()","daeb608a":"# limpe o espa\u00e7o\n!rm -rf TRAIN TEST VAL TRAIN_CROP TEST_CROP VAL_CROP\n\n# save the model\nmodel.save('2019-06-07_VGG_model.h5')","201a2155":"Vamos dar uma olhada na distribui\u00e7\u00e3o de classes entre os conjuntos:","d1d1a98b":"**<center><font size=5>Detec\u00e7\u00e3o de Tumor Cerebral usando VGG-16 Model<\/font><\/center>**\n***\n- <a href='#intro'>1. Vis\u00e3o geral e objetivos do projeto<\/a> \n    - <a href='#dataset'>1.1. Descri\u00e7\u00e3o do conjunto de dados<\/a>\n    - <a href='#tumor'>1.2. O que \u00e9 tumor cerebral?<\/a>\n- <a href='#env'>2. Configurando o Ambiente<\/a>\n\n- <a href='#import'>3. Importa\u00e7\u00e3o e pr\u00e9-processamento de dados<\/a>\n- <a href='#cnn'>4. Modelo CNN<\/a>\n    - <a href='#aug'>4.1. Aumento de dados<\/a>\n        - <a href='#demo'>4.1.1. Demo<\/a>\n        - <a href='#apply'>4.1.2. Aplicar<\/a>\n    - <a href='#build'>4.2. Constru\u00e7\u00e3o de modelo<\/a>\n    - <a href='#perf'>4.3. Desempenho do modelo<\/a>\n- <a href='#concl'>5. Conclus\u00e3o<\/a>","ec31c209":"Todas as imagens est\u00e3o em uma pasta com as subpastas `yes` e` no`. Os dados ser\u00e3o divididos nas pastas `train`,` val` e `test`, o que torna mais f\u00e1cil trabalhar. A nova hierarquia de pasta ser\u00e1 a seguinte: ","0ef666e6":"## <a id='build'>4.2. Constru\u00e7\u00e3o do Modelo<\/a>","37ec3189":"### <a id='apply'>4.1.2. Aplicar<\/a>","e0b3cb93":"Podemos ver as imagens do `conjunto de teste` que foram classificadas incorretamente: ","e810f842":"O primeiro passo da \"normaliza\u00e7\u00e3o\" seria cortar o c\u00e9rebro das imagens. Podemos usar uma t\u00e9cnica que foi descrita em [pyimagesearch](https:\/\/www.pyimagesearch.com\/2016\/04\/11\/finding-extreme-points-in-contours-with-opencv\/)","f55dc180":"# <a id='cnn'>4. Modelo CNN<\/a>\n\nFoi usado [Transfer Learning](https:\/\/towardsdatascience.com\/keras-transfer-learning-for-beginners-6c9b8b7143e) com arquitetura VGG-16 e pesos como modelo b\u00e1sico. \n\n## <a id='aug'>4.1. Aumento de dados<\/a>\n\nComo temos um pequeno conjunto de dados, usamos a t\u00e9cnica chamada [Aumento de Dados (Data Augmentation)](https:\/\/blog.keras.io\/building-powerful-image-classification-models-using-very-little-data.html), que ajuda a \"aumentar\" o tamanho do conjunto de treinamento. \n\n### <a id='demo'>4.1.1. Demo<\/a>\n\nEste \u00e9 o exemplo de uma imagem de como \u00e9 o aumento.","8b07acfb":"# <a id='intro'>1. Vis\u00e3o geral e objetivos do projeto<\/a>\n\nO objetivo principal deste projeto foi construir um modelo de CNN que classificaria se o sujeito tinha tumor ou n\u00e3o com base na resson\u00e2ncia magn\u00e9tica. Usamos a arquitetura e os pesos do modelo [VGG-16](https:\/\/www.kaggle.com\/navoneel\/brain-mri-images-for-brain-tumor-detection) para treinar o modelo para esse problema bin\u00e1rio. Temos a precisao como uma m\u00e9trica para justificar o desempenho do modelo, que pode ser definido como:\n\n$\\textrm{Accuracy} = \\frac{\\textrm{N\u00famero de imagens previstas corretamente}}{\\textrm{N\u00famero total de imagens testadas}} \\times 100\\%$\n\nOs resultados finais s\u00e3o os seguintes:\n\n| Set | Accuracy |\n|:-:|:-:|\n| Validation Set* | ~88% |\n| Test Set* | ~80% |\n<br>\n\n* *`validation set` - \u00e9 o conjunto usado durante o treinamento do modelo para ajustar os hiperpar\u00e2metros . *\n* *`test set` - \u00e9 o pequeno conjunto que foi usado para avalia\u00e7\u00e3o de desempenho do modelo final. *\n\n## <a id='dataset'>1.1. Descri\u00e7\u00e3o do conjunto de dados<\/a>\n\nOs dados da imagem que foram usados para este problema s\u00e3o [Brain MRI Images for Brain Tumor Detection](https:\/\/www.kaggle.com\/navoneel\/brain-mri-images-for-brain-tumor-detection). Consiste em exames de resson\u00e2ncia magn\u00e9tica de duas classes: \n\n* `NO` - nenhum tumor, codificado como `0`\n* `YES` - tumor, codificado como `1`\n\n## <a id='tumor'>1.2. O que \u00e9 tumor cerebral?<\/a>\n\n> Um tumor cerebral ocorre quando c\u00e9lulas anormais se formam dentro do c\u00e9rebro. Existem dois tipos principais de tumores: tumores cancerosos (malignos) e tumores benignos. Os tumores cancerosos podem ser divididos em tumores prim\u00e1rios, que come\u00e7am dentro do c\u00e9rebro, e tumores secund\u00e1rios, que se espalharam de outro lugar, conhecidos como tumores de met\u00e1stase cerebral. Todos os tipos de tumores cerebrais podem produzir sintomas que variam dependendo da parte do c\u00e9rebro envolvida. Esses sintomas podem incluir dores de cabe\u00e7a, convuls\u00f5es, problemas de vis\u00e3o, v\u00f4mitos e altera\u00e7\u00f5es mentais. A dor de cabe\u00e7a \u00e9 classicamente pior pela manh\u00e3 e desaparece com o v\u00f4mito. Outros sintomas podem incluir dificuldade para andar, falar ou com sensa\u00e7\u00f5es. Conforme a doen\u00e7a progride, pode ocorrer inconsci\u00eancia.\n>\n> ![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/5\/5f\/Hirnmetastase_MRT-T1_KM.jpg)\n>\n> *Met\u00e1stases cerebrais no hemisf\u00e9rio cerebral direito de c\u00e2ncer de pulm\u00e3o, mostrado na resson\u00e2ncia magn\u00e9tica*\n\nSource: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Brain_tumor)","a18bc2eb":"A pr\u00f3xima etapa seria redimensionar as imagens para `(224,224)` e aplicar o pr\u00e9-processamento necess\u00e1rio para a entrada do modelo VGG-16. ","11650548":"# <a id='concl'>5. Conclus\u00e3o<\/a>\n\nEste projeto foi uma combina\u00e7\u00e3o de problema de classifica\u00e7\u00e3o de modelo CNN (para prever se o sujeito tinha tumor cerebral ou n\u00e3o) e problema de vis\u00e3o computacional (para automatizar o processo de recorte cerebral a partir de exames de resson\u00e2ncia magn\u00e9tica). A precis\u00e3o final \u00e9 muito superior a 50% da linha de base (estimativa aleat\u00f3ria). No entanto, ele pode ser aumentado por um n\u00famero maior de imagens de trem ou por meio de ajuste de hiperpar\u00e2metros do modelo.","1b97c77e":"# <a id='env'>2. Configurando o Ambiente<\/a>","84a61244":"# <a id='import'>3. Importa\u00e7\u00e3o e pr\u00e9-processamento de dados<\/a>","fdbccc37":"* Veja um exemplo do que esta fun\u00e7\u00e3o far\u00e1 com exames de resson\u00e2ncia magn\u00e9tica: ","be6988d5":"Como podemos ver, as imagens t\u00eam diferentes `width` e` height` e diferentes tamanhos de \"cantos pretos\". Como o tamanho da imagem para a camada de entrada VGG-16 \u00e9 `(224,224)`, algumas imagens largas podem parecer estranhas ap\u00f3s o redimensionamento. Histograma de distribui\u00e7\u00f5es de propor\u00e7\u00e3o (`propor\u00e7\u00e3o = largura \/ altura`):","6ae45833":"## <a id='perf'>4.3. Desempenho do modelo<\/a>"}}