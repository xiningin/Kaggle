{"cell_type":{"831ab1b0":"code","2cfbf278":"code","6a61708d":"code","aaae3c71":"code","62bddc36":"code","b27f85f9":"code","800e9666":"code","d21223a4":"code","69f29fbc":"code","ac9d6f86":"code","9fe5944d":"code","a7cb9ec8":"code","d5c6d9b6":"code","de66492f":"code","00e34262":"code","aafbedb4":"code","7f09bba9":"code","a8936d91":"code","8a6a18e1":"code","31fdf684":"code","47a84e8d":"code","628b5bdc":"code","56a56e6e":"code","f66f395d":"code","f8f1145d":"code","13444647":"code","791a263d":"code","3bb2ee06":"code","3e4f06cf":"markdown","3947f540":"markdown","da5aea99":"markdown","cd2af8da":"markdown","0389d8dd":"markdown","61024f99":"markdown","2e85bc56":"markdown","feec6249":"markdown","addd9449":"markdown","2649842c":"markdown","1d8fc845":"markdown","5fee6a54":"markdown","92b26dab":"markdown","f491fd88":"markdown","da0b5bcb":"markdown","eba1e15e":"markdown","365126d8":"markdown","2ede7567":"markdown","f3f97a4b":"markdown"},"source":{"831ab1b0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","2cfbf278":"#Read the file\ncards = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\n\ncards.head()","6a61708d":"cards.columns","aaae3c71":"#Checking the entire data frame for missing values\ncards.isna().any().any()","62bddc36":"#Creating a correlation dataframe \ncc = cards.corr() \n\n#Identifying the correlation of the other variables to Class\ncc[\"Class\"].sort_values()","b27f85f9":"#Creating a dataframe with all the fraud transactions\nfraud_cards = cards[cards['Class']==1]\n\nfraud_cards['Amount'].describe().round(2)","800e9666":"fig_dims = (10, 6)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.histplot(x=fraud_cards['Amount'], binwidth=50)\nplt.title(\"Distribution of Fraudulent Transaction Amounts\", fontsize=15)\nax.set_xlabel(\"Amount ($)\")","d21223a4":"fig_dims = (12, 4)\nfig, ax = plt.subplots(figsize=fig_dims)\nax.scatter(y='Time',x='Amount', data=cards)\nax.scatter(y='Time',x='Amount', data=fraud_cards,facecolor=\"red\")\nmajor_xticks = np.arange(0, 28000, 2000)\nminor_xticks = np.arange(0, 28000, 500)\nax.set_xticks(major_xticks)\nax.set_xticks(minor_xticks, minor = True)\nax.set_xlabel(\"Amount ($)\")\nax.set_ylabel(\"Transaction Time\")\nax.set_title('Transaction Time vs. Amount',fontsize=20)","69f29fbc":"#Calculating the number of frauds\/legitimate transactions in the dataset\ncards['Class'].value_counts()","ac9d6f86":"#Calculating the percentage of frauds\/legitimate transactions in the dataset\nround((cards['Class'].value_counts()\/len(cards))*100,2)","9fe5944d":"#Creating a new balanced datset with 250 fraudulent transactions\ncards_bal = cards[cards['Class']==1].sample(n=250)\n\n#Adding 250 legitimate transactions\ncards_bal = cards_bal.append(cards[cards['Class']==0].sample(n=250))","a7cb9ec8":"#Calculating the percentage of frauds\/legitimate transactions in the dataset\nround((cards_bal['Class'].value_counts()\/len(cards_bal))*100,2)","d5c6d9b6":"from sklearn.model_selection import train_test_split\n\nX = cards_bal.drop('Class',axis=1)\ny = cards_bal['Class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)","de66492f":"#Import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train,y_train)","00e34262":"decision_trees_predictions = dtree.predict(X_test)","aafbedb4":"#Create an instance of the RandomForestClassifier class and fit it to our training data\nfrom sklearn.ensemble import RandomForestClassifier\n\n#n_estimators is the number of trees in the forest\nrfc = RandomForestClassifier(n_estimators=100)\n\nrfc.fit(X_train,y_train)","7f09bba9":"random_forests_predictions = rfc.predict(X_test)","a8936d91":"#Import KNeighbours Classifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#K represents the Number of neighbors to use for kneighbors queries, we will loop over a range of values to determine which K value is best suited for this model\n#Creating an empty dataframe to store the values\nerror_rate_df = pd.DataFrame(columns=['K', 'Error Rate'])\n\n# Will take some time\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate=np.mean(pred_i != y_test)   \n    df = {'K':i, 'Error Rate':error_rate}\n    error_rate_df = error_rate_df.append(df, ignore_index = True)","8a6a18e1":"plt.figure(figsize=(10,6))\nplt.plot(error_rate_df['K'],error_rate_df['Error Rate'],color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","31fdf684":"k = error_rate_df[error_rate_df['Error Rate'] == min(error_rate_df['Error Rate'])]['K']\n\n#It is possible there may be multiple k values\nif len(k)>1:\n    #In case there are multiple k values we resent the index and select the first k value\n    k = k.reset_index()\n    k = int(k[\"K\"][0])\nelse:\n    k = int(k)\n\nprint(\"The error rate appears to be the lowest when k =\", k,\"Let's train the model with this k value.\")","47a84e8d":"#Create an instance of the KNN classifier and fit it to our training data\nknn = KNeighborsClassifier(n_neighbors=k)\nknn.fit(X_train,y_train)","628b5bdc":"knn_predictions = knn.predict(X_test)","56a56e6e":"#Importing K Means classifier\nfrom sklearn.cluster import KMeans\n\n#Create an instance of the K Means classifier and fit it to our training data\n#We only have two target values 1 and 0 so we set the number of clusters to 2\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(X_train,y_train)","f66f395d":"kmeans_predictions = kmeans.fit_predict(X_test)","f8f1145d":"#Import Classification Report and Confusion Matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\n\n#Converting each of the classification reports into dictionaries\nreport_dt = classification_report(y_test,decision_trees_predictions , output_dict=True)\nreport_rf = classification_report(y_test,random_forests_predictions , output_dict=True)\nreport_knn = classification_report(y_test,knn_predictions, output_dict=True)\nreport_kmeans = classification_report(y_test,kmeans_predictions, output_dict=True)","13444647":"model = {'Model Name':['Decision Trees', 'Random Forests','KNN','K-Means'],\n        'Accuracy':[report_dt['accuracy'], report_rf['accuracy'], report_knn['accuracy'], report_kmeans['accuracy']],\n        'Precision':[report_dt['weighted avg']['precision'],report_rf['weighted avg']['precision'], report_knn['weighted avg']['precision'], report_kmeans['weighted avg']['precision']],\n        'Recall':[report_dt['weighted avg']['recall'],report_rf['weighted avg']['recall'],report_knn['weighted avg']['recall'],report_kmeans['weighted avg']['recall']],\n        'F1-Score':[report_dt['weighted avg']['f1-score'],report_rf['weighted avg']['f1-score'],report_knn['weighted avg']['f1-score'],report_kmeans['weighted avg']['f1-score']]}\n\nmodel = pd.DataFrame(model)\n\nmodel.round(2).sort_values(by=['Accuracy','Precision','Recall','F1-Score'],ascending=False)","791a263d":"X = cards.drop('Class',axis=1)\ny = cards['Class']\n\n#Random Forests predictions\nrandom_forests_predictions = rfc.predict(X)\n\n#Assigning the classification report to a dictionary to call later\ncards_cr = classification_report(y,random_forests_predictions, output_dict=True)\n\nprint(classification_report(y,random_forests_predictions))","3bb2ee06":"cards_accuracy = round(cards_cr['accuracy'],2)\n\nprint('The Random Forests Model has an accuracy of',cards_accuracy)","3e4f06cf":"## Comparing Performance\n\nNow that we have trained four different models we can compare their performance to determine which model should be applied to the original dataset.","3947f540":"We now have a balanced dataset to work with.\n\n## Train Test Split","da5aea99":"## K Means Clustering\n\n### Training the K Means Model","cd2af8da":"# Checking for Missing Values","0389d8dd":"As seen above the <b>Random Forests Model<\/b> yields the best results so we will test the model against the original data from the original dataset to compare its performance.","61024f99":"### Random Forests: Predictions and Evaluation","2e85bc56":"## Decision Trees\n\n### Training a Decision Tree Model","feec6249":"### Training KNN Model","addd9449":"As seen above, only about 0.17% of the data is associated with fraudulent transactions. In order to train and test machine learning models, we need a balanced dataset. Therefore, we will create a balanced dataset from a subset of the data and train and test our model on that data. \n\n## Creating a Balanced Dataset\n\nWe have 492 fraudulent transactions. We will choose 250 transactions at random from these transactions and 250 transactions from the at random from the legitimate transactions to create a data set with 500 transactions. ","2649842c":"### K Means: Predictions and Evaluations","1d8fc845":"# Credit Card Fraud Detection Machine Learning\n\nThe data was downloaded from Kaggle:https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\n\nThe dataset contains transactions made by European credit cardholders credit cards in September 2013. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numeric input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, the original features and more background information about the data are unavailable. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependent cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\nThe dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http:\/\/mlg.ulb.ac.be) of ULB (Universit\u00e9 Libre de Bruxelles) on big data mining and fraud detection.\n\n## Goal\n\nThe objective is to classify the data transactions as either legitimate (class 0) or fradulent (class 1) using a machine learning model. Four machine learning models will be trained and tested to determine which will yield the best results:\n\n- Decision Trees\n- Random Forest\n- K Nearest Neighbours\n- K Means Clustering","5fee6a54":"### Decision Tree: Predictions and Evaluation","92b26dab":"#### Fraud Amount: Key Takeaways\n\n- The average fraud amount is <b>\\$122.21<\/b>\n- The standard deviation is <b>\\$256.68<\/b>, which is quite large which indicates that the fraud amount can vary significantly.\n- The majority of fraudulent transactions are between <b>zero and \\$50<\/b>\n- There frauds occur at various times but overall the majority of fraudulent transactions appear to be <b>under \\$1000.<\/b> \n\n# Train and Test the Model\n\nIn this stage we will separate our dataset into training data that will be used to train the model and testing data that will be used to judge the accuracy of the results. We will now train and test the four machine learning models to determine which will yield the best results:\n\n- Decision Trees\n- Random Forest\n- K Nearest Neighbours\n- K Means Clustering","f491fd88":"Overall, the Random Forests Model does the best job of predicting fraudulent credit card transactions.","da0b5bcb":"## Random Forests\n\n### Training the Random Forest Model","eba1e15e":"## K-Nearest Neighbours\n\n### Selecting a K Value","365126d8":"# Exploratory Data Analysis\n\nIn this stage, we will examine the data to identify any patterns, trends and relationships between the variables. It will help us analyze the data and extract insights that can be used to make decisions.\n\nData Visualization will give us a clear idea of what the data means by giving it visual context.\n\n## Class","2ede7567":"V17 and V14 have a low negative correlation with Class while all the other variables have a negligible correlation with class.\n\nNote: Typically, a correlation value between 0.3-0.5 (either positive or negative) is considered to be a low correlation and anything below that is negligible.\n\n## Amount\n\n### Fraudulent Transaction Amount","f3f97a4b":"### KNN: Predictions and Evaluations"}}