{"cell_type":{"da4ea8dc":"code","6008f8da":"code","389d9c6b":"code","222b3067":"code","8a492187":"code","3d76c9c0":"code","59b4b9ef":"code","8fdf953d":"code","1055d09e":"code","cebda4d5":"code","4745d8ac":"code","178ac353":"code","294e9cc1":"code","a71f74c1":"code","5aba53b2":"code","c2b556a2":"code","a71c9726":"code","6f1fe73f":"code","f41eb2a0":"code","8e98ac2a":"code","cb60a737":"code","deddb3b5":"code","47ec1bb3":"code","94041ac8":"code","834cc53d":"code","901ab980":"code","8715aed4":"code","618a7364":"code","4c5f38ba":"code","7bc07d7b":"code","fbff34bb":"code","6c2d1110":"code","1d3505c6":"code","2b130d5e":"code","fe4ec8ec":"code","4c73bd7d":"code","691b2e62":"code","3c1a8822":"code","fb853d5b":"code","a4651201":"code","28223f7e":"code","8e0e6d12":"code","04a095fb":"code","9b40a478":"code","c56b0baf":"code","1f9b2d15":"code","7ce80521":"code","277ca4b0":"code","9cf38ee9":"code","3c9c15ca":"code","5d13bf87":"code","8ed6fc64":"code","9b0b1093":"code","3e5dc378":"code","ce40be7d":"code","616f11b4":"code","9ed8796f":"code","498c5c72":"code","803a7483":"code","d64c34fe":"markdown","e8aebe12":"markdown","b00d71f5":"markdown","e279ad02":"markdown","a85c1fcb":"markdown","689bd148":"markdown","48e54237":"markdown","e245d935":"markdown","f7235b60":"markdown","25af0e89":"markdown","f8787e46":"markdown","74c72f03":"markdown","f54cfe2d":"markdown","5b2688a3":"markdown","c129d752":"markdown","7c3700d1":"markdown","3a5b2bd1":"markdown"},"source":{"da4ea8dc":"import numpy as np # linear algebra\nnp.random.seed(1000)\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\n#import cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nimport os\n","6008f8da":"import glob\nfrom tqdm import tqdm\nfrom skimage.io import imread,imshow\nfrom skimage.transform import resize,rotate","389d9c6b":"infected_PATH='..\/input\/cell-images-for-detecting-malaria\/cell_images\/Parasitized\/'\nuninfected_PATH='..\/input\/cell-images-for-detecting-malaria\/cell_images\/Uninfected\/'\n\n#read files in the path and filter png files\ninfected_train_ids=glob.glob(f\"{infected_PATH}*.png\")\nuninfected_train_ids=glob.glob(f\"{uninfected_PATH}*.png\")","222b3067":"IMG_HEIGHT=100\nIMG_WIDTH=100\nIMG_CHANNELS=3\n\n#datasize=len(infected_train_ids)*3+len(uninfected_train_ids)*3\ndatasize=len(infected_train_ids)+len(uninfected_train_ids)\n\nX_train = np.zeros((datasize, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nY_train = np.zeros((datasize, 1), dtype=np.bool)","8a492187":"path = infected_train_ids[9211]\nimg = imread(path )[:,:,:IMG_CHANNELS]\nimshow(img)","3d76c9c0":"try:\n    print('Resizing infected training images')\n    i=0\n    for n, id_ in tqdm(enumerate(infected_train_ids), total=len(infected_train_ids)):   \n        img = imread(id_ )[:,:,:IMG_CHANNELS]  \n        resize_img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n        #rotated45 = rotate(resize_img,45)\n        #rotated75 = rotate(resize_img,75)\n        #blur = cv2.blur(np.array(resize_img) ,(10,10))\n        X_train[i]=resize_img\n        #X_train[i+1]=rotated45\n        #X_train[i+2]=rotated75\n        #data.append(np.array(blur))\n        Y_train[i]=1\n        #Y_train[i+1]=1\n        #Y_train[i+2]=1\n        #labels.append(1)\n        #i=i+3    \n        i=i+1\n\n    print('Resizing uninfected training images')\n    for n, id_ in tqdm(enumerate(uninfected_train_ids), total=len(uninfected_train_ids)):   \n        img = imread(id_ )[:,:,:IMG_CHANNELS]  \n        resize_img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n        #rotated45 = rotate(resize_img,45)\n        #rotated75 = rotate(resize_img,75)\n        X_train[i]=resize_img\n        #X_train[i+1]=rotated45\n        #X_train[i+2]=rotated75\n        Y_train[i]=0\n        #Y_train[i+1]=0\n        #Y_train[i+2]=0\n        #i=i+3    \n        i=i+1\n\n    print('done')\nexcept AttributeError:\n    print(f'error {AttributeError} ')","59b4b9ef":"X_train = X_train.astype(np.float32)\nX_train = X_train\/255\nY_train = Y_train.astype(np.int32)","8fdf953d":"#np.savez('train' , X_train=X_train,Y_train=Y_train)\n","1055d09e":"'''\nwith np.load('..\/input\/malariacellclassification\/train.npz') as data:\n    X_train = data['X_train']    \n    Y_train = data['Y_train']\n'''","cebda4d5":"print(f'X_train : {X_train.shape} | labels : {Y_train.shape}')\n","4745d8ac":"plt.figure(1 , figsize = (15 , 9))\nn = 0 \nfor i in range(49):\n    n += 1 \n    r = np.random.randint(0 , X_train.shape[0] , 1)\n    plt.subplot(7 , 7 , n)\n    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n    plt.imshow(X_train[r[0]])\n    plt.title('{} : {}'.format('Infected' if Y_train[r[0]] == 1 else 'Unifected' ,\n                               Y_train[r[0]]) )\n    plt.xticks([]) , plt.yticks([])\n    \nplt.show()","178ac353":"plt.figure(1, figsize = (15 , 7))\nplt.subplot(1 , 2 , 1)\nplt.imshow(X_train[0])\nplt.title('Infected Cell')\nplt.xticks([]) , plt.yticks([])\n\nplt.subplot(1 , 2 , 2)\nplt.imshow(X_train[26000])\nplt.title('Uninfected Cell')\nplt.xticks([]) , plt.yticks([])\n\nplt.show()","294e9cc1":"n = np.arange(X_train.shape[0])\nnp.random.shuffle(n)\nX_train = X_train[n]\nY_train = Y_train[n]","a71f74c1":"from sklearn.model_selection import train_test_split\n\ntrain_x , x , train_y , y = train_test_split(X_train , Y_train , test_size = 0.2 , random_state = 111)\n\neval_x , test_x , eval_y , test_y = train_test_split(x , y ,test_size = 0.5 , random_state = 111)","5aba53b2":"'''\nplt.figure(1 , figsize = (15 ,5))\nn = 0 \nfor z , j in zip([train_y , eval_y , test_y] , ['train labels','eval labels','test labels']):\n    n += 1\n    plt.subplot(1 , 3  , n)\n    sns.countplot(x = z )\n    plt.title(j)\nplt.show()\n'''","c2b556a2":"print('train data shape {} ,eval data shape {} , test data shape {}'.format(train_x.shape,\n                                                                           eval_x.shape ,\n                                                                           test_x.shape))","a71c9726":"models_train=[]","6f1fe73f":"#model_0\ninput_layers_0 = tf.keras.layers.Input((IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS))    \n\nconv1_0 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(input_layers_0)\npool1_0 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1_0)\nnorm1_0 = tf.keras.layers.BatchNormalization(axis = -1)(pool1_0)\n\nconv2_0 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  activation='relu', padding='same')(norm1_0)\npool2_0 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2_0)\nnorm2_0 = tf.keras.layers.BatchNormalization(axis = -1)(pool2_0)\n\nconv3_0 = tf.keras.layers.Conv2D(128, kernel_size=(3, 3),  activation='relu', padding='same')(norm2_0)\npool3_0 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3_0)\nnorm3_0 = tf.keras.layers.BatchNormalization(axis = -1)(pool3_0)\n\nconv4_0 = tf.keras.layers.Conv2D(256, kernel_size=(3, 3),  activation='relu', padding='same')(norm3_0)\npool4_0 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv4_0)\nnorm4_0 = tf.keras.layers.BatchNormalization(axis = -1)(pool4_0)\n\nflat_0 = tf.keras.layers.Flatten()(norm4_0)  #Flatten the matrix to get it ready for dense.\n\nhidden1_0 = tf.keras.layers.Dense(512, activation='relu')(flat_0)\nnorm5_0 = tf.keras.layers.BatchNormalization(axis = -1)(hidden1_0)\n\nhidden2_0 = tf.keras.layers.Dense(256, activation='relu')(norm5_0)\nnorm6_0 = tf.keras.layers.BatchNormalization(axis = -1)(hidden2_0)\n\nout_0 = tf.keras.layers.Dense(1, activation='sigmoid')(norm6_0)   #units=1 gives error\n\nmodel_0 = tf.keras.Model(inputs=input_layers_0, outputs=out_0)","f41eb2a0":"model_0.compile(optimizer='adam',\n                loss='categorical_crossentropy',   #Check between binary_crossentropy and categorical_crossentropy\n                metrics=['accuracy'])\n#print(model_0.summary())\n#Fit the model\nprint('start Model train')\ncall_backs = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',mode='max',patience=3,verbose=1)\n\nhistory = model_0.fit(train_x, \n                         train_y, \n                         batch_size = 64, \n                         verbose = 1, \n                         epochs = 10,      #Changed to 3 from 50 for testing purposes.\n                         validation_data=(eval_x, eval_y),\n                         callbacks=[call_backs]\n                     )","8e98ac2a":"test_accuracy=model_0.evaluate(test_x, test_y)[1]*100\nprint(\"Test_Accuracy: {:.2f}%\".format(test_accuracy))","cb60a737":"models_train.append(['sigmoid activation function',test_accuracy,history.history['accuracy'],history.history['val_accuracy'],history.history['loss'],history.history['val_loss']])","deddb3b5":"out = tf.keras.layers.Dense(2, activation='softmax')(norm6_0)   \n\nmodel = tf.keras.Model(inputs=input_layers_0, outputs=out)","47ec1bb3":"from tensorflow.keras.utils import to_categorical\ntrain_y = to_categorical(train_y)\neval_y = to_categorical(eval_y)\ntest_y = to_categorical(test_y)","94041ac8":"print(train_y.shape)","834cc53d":"model.compile(optimizer='adam',\n                loss='categorical_crossentropy',   #Check between binary_crossentropy and categorical_crossentropy\n                metrics=['accuracy'])\n#print(model.summary())\n#Fit the model\nprint('start Model train')\ncall_backs = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',mode='max',patience=3,verbose=1)\n\nhistory = model.fit(train_x, \n                         train_y, \n                         batch_size = 64, \n                         verbose = 1, \n                         epochs = 10,      #Changed to 3 from 50 for testing purposes.\n                         validation_data=(eval_x, eval_y),\n                         callbacks=[call_backs]\n                     )","901ab980":"test_accuracy=model.evaluate(test_x, test_y)[1]*100\nprint(\"Test_Accuracy: {:.2f}%\".format(test_accuracy))","8715aed4":"models_train.append(['softmax activation function',test_accuracy,history.history['accuracy'],history.history['val_accuracy'],history.history['loss'],history.history['val_loss']])","618a7364":"#model_1\ninput_layers_1 = tf.keras.layers.Input((IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS))    \n\nconv1_1 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(input_layers_1)\npool1_1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1_1)\nnorm1_1 = tf.keras.layers.BatchNormalization(axis = -1)(pool1_1)\n\nconv2_1 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  activation='relu', padding='same')(norm1_1)\npool2_1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2_1)\nnorm2_1 = tf.keras.layers.BatchNormalization(axis = -1)(pool2_1)\ndrop1_1 = tf.keras.layers.Dropout(rate=0.2)(norm2_1)\n\nconv3_1 = tf.keras.layers.Conv2D(128, kernel_size=(3, 3),  activation='relu', padding='same')(drop1_1)\npool3_1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3_1)\nnorm3_1 = tf.keras.layers.BatchNormalization(axis = -1)(pool3_1)\n\nconv4_1 = tf.keras.layers.Conv2D(256, kernel_size=(3, 3),  activation='relu', padding='same')(norm3_1)\npool4_1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv4_1)\nnorm4_1 = tf.keras.layers.BatchNormalization(axis = -1)(pool4_1)\ndrop2_1 = tf.keras.layers.Dropout(rate=0.2)(norm4_1)\n\nflat_1 = tf.keras.layers.Flatten()(drop2_1)  #Flatten the matrix to get it ready for dense.\n\nhidden1_1 = tf.keras.layers.Dense(512, activation='relu')(flat_1)\nnorm5_1 = tf.keras.layers.BatchNormalization(axis = -1)(hidden1_1)\n\nhidden2_1 = tf.keras.layers.Dense(256, activation='relu')(norm5_1)\nnorm6_1 = tf.keras.layers.BatchNormalization(axis = -1)(hidden2_1)\ndrop3_1 = tf.keras.layers.Dropout(rate=0.2)(norm6_1)\n\nout_1 = tf.keras.layers.Dense(2, activation='softmax')(drop3_1)   #units=1 gives error\n\nmodel_1 = tf.keras.Model(inputs=input_layers_1, outputs=out_1)","4c5f38ba":"model_1.compile(optimizer='adam',\n                loss='categorical_crossentropy',   #Check between binary_crossentropy and categorical_crossentropy\n                metrics=['accuracy'])\n#print(model_1.summary())\n#Fit the model\nprint('start Model train')\ncall_backs = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',mode='max',patience=3,verbose=1)\n\nhistory = model_1.fit(train_x, \n                         train_y, \n                         batch_size = 64, \n                         verbose = 1, \n                         epochs = 10,      #Changed to 3 from 50 for testing purposes.\n                         validation_data=(eval_x, eval_y),\n                         callbacks=[call_backs]\n                     )","7bc07d7b":"test_accuracy=model_1.evaluate(test_x, test_y)[1]*100\nprint(\"Test_Accuracy: {:.2f}%\".format(test_accuracy))","fbff34bb":"models_train.append(['add dropout after 2 conv layers',test_accuracy,history.history['accuracy'],history.history['val_accuracy'],history.history['loss'],history.history['val_loss']])","6c2d1110":"#model_2\ninput_layers_2 = tf.keras.layers.Input((IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS))    \n\nconv1_2 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(input_layers_2)\npool1_2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1_2)\nnorm1_2 = tf.keras.layers.BatchNormalization(axis = -1)(pool1_2)\ndrop1_2 = tf.keras.layers.Dropout(rate=0.2)(norm1_2)\n\nconv2_2 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  activation='relu', padding='same')(drop1_2)\npool2_2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2_2)\nnorm2_2 = tf.keras.layers.BatchNormalization(axis = -1)(pool2_2)\ndrop2_2 = tf.keras.layers.Dropout(rate=0.2)(norm2_2)\n\nconv3_2 = tf.keras.layers.Conv2D(128, kernel_size=(3, 3),  activation='relu', padding='same')(drop2_2\npool3_2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3_2)\nnorm3_2 = tf.keras.layers.BatchNormalization(axis = -1)(pool3_2)\ndrop3_2 = tf.keras.layers.Dropout(rate=0.2)(norm3_2)\n\nconv4_2 = tf.keras.layers.Conv2D(256, kernel_size=(3, 3),  activation='relu', padding='same')(drop3_2)\npool4_2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv4_2)\nnorm4_2 = tf.keras.layers.BatchNormalization(axis = -1)(pool4_2)\ndrop4_2 = tf.keras.layers.Dropout(rate=0.2)(norm4_2)\n\nflat_2 = tf.keras.layers.Flatten()(drop4_2)  #Flatten the matrix to get it ready for dense.\n\nhidden1_2 = tf.keras.layers.Dense(512, activation='relu')(flat_2)\nnorm5_2 = tf.keras.layers.BatchNormalization(axis = -1)(hidden1_2)\ndrop5_2 = tf.keras.layers.Dropout(rate=0.2)(norm5_2)\n\nhidden2_2 = tf.keras.layers.Dense(256, activation='relu')(drop5_2)\nnorm6_2 = tf.keras.layers.BatchNormalization(axis = -1)(hidden2_2)\ndrop6_2 = tf.keras.layers.Dropout(rate=0.2)(norm6_2)\n\nout_2 = tf.keras.layers.Dense(2, activation='softmax')(drop6_2)   #units=1 gives error\n\nmodel_2 = tf.keras.Model(inputs=input_layers_2, outputs=out_2)","1d3505c6":"\nmodel_2.compile(optimizer='adam',\n                loss='categorical_crossentropy',   #Check between binary_crossentropy and categorical_crossentropy\n                metrics=['accuracy'])\n#print(model_2.summary())\n\n#Fit the model\nprint('start Model train')\ncall_backs = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',mode='max',patience=3,verbose=1)\n\nhistory = model_2.fit(train_x, \n                         train_y, \n                         batch_size = 64, \n                         verbose = 1, \n                         epochs = 10,      #Changed to 3 from 50 for testing purposes.\n                         validation_data=(eval_x, eval_y),\n                         callbacks=[call_backs]\n                     )","2b130d5e":"test_accuracy=model_2.evaluate(test_x, test_y)[1]*100\nprint(\"Test_Accuracy: {:.2f}%\".format(test_accuracy))","fe4ec8ec":"models_train.append(['all dropout',test_accuracy,history.history['accuracy'],history.history['val_accuracy'],history.history['loss'],history.history['val_loss']])","4c73bd7d":"#model_3\ninput_layers_3 = tf.keras.layers.Input((IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS))    \n\nconv1_3 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(input_layers_3)\npool1_3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1_3)\nnorm1_3 = tf.keras.layers.BatchNormalization(axis = -1)(pool1_3)\n#drop1_3 = tf.keras.layers.Dropout(rate=0.2)(norm1_3)\n\nconv2_3 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  activation='relu', padding='same')(norm1_3)\npool2_3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2_3)\nnorm2_3 = tf.keras.layers.BatchNormalization(axis = -1)(pool2_3)\n#drop2_3 = tf.keras.layers.Dropout(rate=0.2)(norm2_3)\n\nconv3_3 = tf.keras.layers.Conv2D(128, kernel_size=(3, 3),  activation='relu', padding='same')(norm2_3)\npool3_3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3_3)\nnorm3_3 = tf.keras.layers.BatchNormalization(axis = -1)(pool3_3)\n#drop3_3 = tf.keras.layers.Dropout(rate=0.2)(norm3_3)\n\nconv4_3 = tf.keras.layers.Conv2D(256, kernel_size=(3, 3),  activation='relu', padding='same')(norm3_3)\npool4_3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv4_3)\nnorm4_3 = tf.keras.layers.BatchNormalization(axis = -1)(pool4_3)\n#drop4_3 = tf.keras.layers.Dropout(rate=0.2)(norm4_3)\n\nflat_3 = tf.keras.layers.Flatten()(norm4_3)  #Flatten the matrix to get it ready for dense.\n\nhidden1_3 = tf.keras.layers.Dense(300, activation='relu')(flat_3)\nnorm5_3 = tf.keras.layers.BatchNormalization(axis = -1)(hidden1_3)\n#drop5_3 = tf.keras.layers.Dropout(rate=0.2)(norm5_3)\n\nhidden2_3 = tf.keras.layers.Dense(100, activation='relu')(norm5_3)\nnorm6_3 = tf.keras.layers.BatchNormalization(axis = -1)(hidden2_3)\n#drop6_3 = tf.keras.layers.Dropout(rate=0.2)(norm6_3)\n\nout_3 = tf.keras.layers.Dense(2, activation='softmax')(norm6_3)   #units=1 gives error\n\nmodel_3 = tf.keras.Model(inputs=input_layers_3, outputs=out_3)","691b2e62":"\nmodel_3.compile(optimizer='adam',\n                loss='categorical_crossentropy',   #Check between binary_crossentropy and categorical_crossentropy\n                metrics=['accuracy'])\n#print(model_3.summary())\n\n#Fit the model\nprint('start Model train')\ncall_backs = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',mode='max',patience=3,verbose=1)\n\nhistory = model_3.fit(train_x, \n                         train_y, \n                         batch_size = 64, \n                         verbose = 1, \n                         epochs = 10,      #Changed to 3 from 50 for testing purposes.\n                         validation_data=(eval_x, eval_y),\n                         callbacks=[call_backs]\n                     )","3c1a8822":"test_accuracy=model_3.evaluate(test_x, test_y)[1]*100\nprint(\"Test_Accuracy: {:.2f}%\".format(test_accuracy))","fb853d5b":"models_train.append(['change dense layer size',test_accuracy,history.history['accuracy'],history.history['val_accuracy'],history.history['loss'],history.history['val_loss']])","a4651201":"#model_4\ninput_layers_4 = tf.keras.layers.Input((IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS))    \n\nconv1_4 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(input_layers_4)\npool1_4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1_4)\nnorm1_4 = tf.keras.layers.BatchNormalization(axis = -1)(pool1_4)\n#drop1_4 = tf.keras.layers.Dropout(rate=0.2)(norm1_4)\n\nconv2_4 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  activation='relu', padding='same')(norm1_4)\npool2_4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2_4)\nnorm2_4 = tf.keras.layers.BatchNormalization(axis = -1)(pool2_4)\n#drop2_4 = tf.keras.layers.Dropout(rate=0.2)(norm2_4)\n\nconv3_4 = tf.keras.layers.Conv2D(128, kernel_size=(3, 3),  activation='relu', padding='same')(norm2_4)\npool3_4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3_4)\nnorm3_4 = tf.keras.layers.BatchNormalization(axis = -1)(pool3_4)\n#drop3_4 = tf.keras.layers.Dropout(rate=0.2)(norm3_4)\n\ndil_conv1_4 = tf.keras.layers.Conv2D(256, kernel_size=(3, 3),dilation_rate=(2,2),  activation='relu', padding='same')(norm3_4)\nconv4_4 = tf.keras.layers.Conv2D(256, kernel_size=(3, 3),  activation='relu', padding='same')(dil_conv1_4)\n\npool4_4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv4_4)\nnorm4_4 = tf.keras.layers.BatchNormalization(axis = -1)(pool4_4)\n#drop4_4 = tf.keras.layers.Dropout(rate=0.2)(norm4_4)\n\nflat_4 = tf.keras.layers.Flatten()(norm4_4)  #Flatten the matrix to get it ready for dense.\n\nhidden1_4 = tf.keras.layers.Dense(300, activation='relu')(flat_4)\nnorm5_4 = tf.keras.layers.BatchNormalization(axis = -1)(hidden1_4)\n#drop5_4 = tf.keras.layers.Dropout(rate=0.2)(norm5_4)\n\nhidden2_4 = tf.keras.layers.Dense(100, activation='relu')(norm5_4)\nnorm6_4 = tf.keras.layers.BatchNormalization(axis = -1)(hidden2_4)\n#drop6_4 = tf.keras.layers.Dropout(rate=0.2)(norm6_4)\n\nout_4 = tf.keras.layers.Dense(2, activation='softmax')(norm6_4)   #units=1 gives error\n\nmodel_4 = tf.keras.Model(inputs=input_layers_4, outputs=out_4)","28223f7e":"\nmodel_4.compile(optimizer='adam',\n                loss='categorical_crossentropy',   #Check between binary_crossentropy and categorical_crossentropy\n                metrics=['accuracy'])\n#print(model_4.summary())\n\n#Fit the model\nprint('start Model train')\ncall_backs = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',mode='max',patience=3,verbose=1)\n\nhistory = model_4.fit(train_x, \n                         train_y, \n                         batch_size = 64, \n                         verbose = 1, \n                         epochs = 10,      #Changed to 3 from 50 for testing purposes.\n                         validation_data=(eval_x, eval_y),\n                         callbacks=[call_backs]\n                     )","8e0e6d12":"test_accuracy=model_4.evaluate(test_x, test_y)[1]*100\nprint(\"Test_Accuracy: {:.2f}%\".format(test_accuracy))","04a095fb":"models_train.append(['add dilated conv',test_accuracy,history.history['accuracy'],history.history['val_accuracy'],history.history['loss'],history.history['val_loss']])","9b40a478":"#model_5\ninput_layers_5 = tf.keras.layers.Input((IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS))    \n\nconv1_5 = tf.keras.layers.Conv2D(32, kernel_size=(5, 5), activation='relu', padding='same')(input_layers_5)\npool1_5 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1_5)\nnorm1_5 = tf.keras.layers.BatchNormalization(axis = -1)(pool1_5)\n#drop1_5 = tf.keras.layers.Dropout(rate=0.2)(norm1_5)\n\nconv2_5 = tf.keras.layers.Conv2D(64, kernel_size=(5, 5),  activation='relu', padding='same')(norm1_5)\npool2_5 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2_5)\nnorm2_5 = tf.keras.layers.BatchNormalization(axis = -1)(pool2_5)\n#drop2_5 = tf.keras.layers.Dropout(rate=0.2)(norm2_5)\n\nconv3_5 = tf.keras.layers.Conv2D(128, kernel_size=(3, 3),  activation='relu', padding='same')(norm2_5)\npool3_5 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3_5)\nnorm3_5 = tf.keras.layers.BatchNormalization(axis = -1)(pool3_5)\n#drop3_5 = tf.keras.layers.Dropout(rate=0.2)(norm3_5)\n\ndil_conv1_5 = tf.keras.layers.Conv2D(256, kernel_size=(3, 3),dilation_rate=(2,2),  activation='relu', padding='same')(norm3_5)\nconv4_5 = tf.keras.layers.Conv2D(256, kernel_size=(3, 3),  activation='relu', padding='same')(dil_conv1_5)\n\npool4_5 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv4_5)\nnorm4_5 = tf.keras.layers.BatchNormalization(axis = -1)(pool4_5)\n#drop4_5 = tf.keras.layers.Dropout(rate=0.2)(norm4_5)\n\nflat_5 = tf.keras.layers.Flatten()(norm4_5)  #Flatten the matrix to get it ready for dense.\n\nhidden1_5 = tf.keras.layers.Dense(300, activation='relu')(flat_5)\nnorm5_5 = tf.keras.layers.BatchNormalization(axis = -1)(hidden1_5)\n#drop5_5 = tf.keras.layers.Dropout(rate=0.2)(norm5_5)\n\nhidden2_5 = tf.keras.layers.Dense(100, activation='relu')(norm5_5)\nnorm6_5 = tf.keras.layers.BatchNormalization(axis = -1)(hidden2_5)\n#drop6_5 = tf.keras.layers.Dropout(rate=0.2)(norm6_5)\n\nout_5 = tf.keras.layers.Dense(2, activation='softmax')(norm6_5)   #units=1 gives error\n\nmodel_5 = tf.keras.Model(inputs=input_layers_5, outputs=out_5)","c56b0baf":"model_5.compile(optimizer='adam',\n                loss='categorical_crossentropy',   #Check between binary_crossentropy and categorical_crossentropy\n                metrics=['accuracy'])\n#print(model_5.summary())\n\n#Fit the model\nprint('start Model train')\ncall_backs = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',mode='max',patience=3,verbose=1)\n\nhistory = model_5.fit(train_x, \n                         train_y, \n                         batch_size = 64, \n                         verbose = 1, \n                         epochs = 10,      #Changed to 3 from 50 for testing purposes.\n                         validation_data=(eval_x, eval_y),\n                         callbacks=[call_backs]\n                     )","1f9b2d15":"test_accuracy=model_5.evaluate(test_x, test_y)[1]*100\nprint(\"Test_Accuracy: {:.2f}%\".format(test_accuracy))","7ce80521":"models_train.append(['change kernel size',test_accuracy,history.history['accuracy'],history.history['val_accuracy'],history.history['loss'],history.history['val_loss']])","277ca4b0":"#model_6\ninput_layers_6 = tf.keras.layers.Input((IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS))    \n\nconv1_6 = tf.keras.layers.Conv2D(32, kernel_size=(5, 5), activation='relu', padding='same')(input_layers_6)\npool1_6 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=4)(conv1_6)\nnorm1_6 = tf.keras.layers.BatchNormalization(axis = -1)(pool1_6)\n#drop1_6 = tf.keras.layers.Dropout(rate=0.2)(norm1_6)\n\nconv2_6 = tf.keras.layers.Conv2D(64, kernel_size=(5, 5),  activation='relu', padding='same')(norm1_6)\npool2_6 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2_6)\nnorm2_6 = tf.keras.layers.BatchNormalization(axis = -1)(pool2_6)\n#drop2_6 = tf.keras.layers.Dropout(rate=0.2)(norm2_6)\n\nconv3_6 = tf.keras.layers.Conv2D(128, kernel_size=(3, 3),  activation='relu', padding='same')(norm2_6)\npool3_6 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3_6)\nnorm3_6 = tf.keras.layers.BatchNormalization(axis = -1)(pool3_6)\n#drop3_6 = tf.keras.layers.Dropout(rate=0.2)(norm3_6)\n\ndil_conv1_6 = tf.keras.layers.Conv2D(256, kernel_size=(3, 3),dilation_rate=(2,2),  activation='relu', padding='same')(norm3_6)\nconv4_6 = tf.keras.layers.Conv2D(256, kernel_size=(3, 3),  activation='relu', padding='same')(dil_conv1_6)\n\npool4_6 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv4_6)\nnorm4_6 = tf.keras.layers.BatchNormalization(axis = -1)(pool4_6)\n#drop4_6 = tf.keras.layers.Dropout(rate=0.2)(norm4_6)\n\nflat_6 = tf.keras.layers.Flatten()(norm4_6)  #Flatten the matrix to get it ready for dense.\n\nhidden1_6 = tf.keras.layers.Dense(300, activation='relu')(flat_6)\nnorm5_6 = tf.keras.layers.BatchNormalization(axis = -1)(hidden1_6)\n#drop5_6 = tf.keras.layers.Dropout(rate=0.2)(norm5_6)\n\nhidden2_6 = tf.keras.layers.Dense(100, activation='relu')(norm5_6)\nnorm6_6 = tf.keras.layers.BatchNormalization(axis = -1)(hidden2_6)\n#drop6_6 = tf.keras.layers.Dropout(rate=0.2)(norm6_6)\n\nout_6 = tf.keras.layers.Dense(2, activation='softmax')(norm6_6)   #units=1 gives error\n\nmodel_6 = tf.keras.Model(inputs=input_layers_6, outputs=out_6)","9cf38ee9":"model_6.compile(optimizer='adam',\n                loss='categorical_crossentropy',   #Check between binary_crossentropy and categorical_crossentropy\n                metrics=['accuracy'])\n#print(model_6.summary())\n\n#Fit the model\nprint('start Model train')\ncall_backs = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',mode='max',patience=3,verbose=1)\n\nhistory = model_6.fit(train_x, \n                         train_y, \n                         batch_size = 64, \n                         verbose = 1, \n                         epochs = 10,      #Changed to 3 from 50 for testing purposes.\n                         validation_data=(eval_x, eval_y),\n                         callbacks=[call_backs]\n                     )","3c9c15ca":"test_accuracy=model_6.evaluate(test_x, test_y)[1]*100\nprint(\"Test_Accuracy: {:.2f}%\".format(test_accuracy))","5d13bf87":"models_train.append(['change stride',test_accuracy,history.history['accuracy'],history.history['val_accuracy'],history.history['loss'],history.history['val_loss']])","8ed6fc64":"#model_7\ninput_layers_7 = tf.keras.layers.Input((IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS))    \n\nconv1_7 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(input_layers_7)\npool1_7 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=4)(conv1_7)\nnorm1_7 = tf.keras.layers.BatchNormalization(axis = -1)(pool1_7)\n#drop1_7 = tf.keras.layers.Dropout(rate=0.2)(norm1_7)\n\nconv2_7 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),  activation='relu', padding='same')(norm1_7)\npool2_7 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2_7)\nnorm2_7 = tf.keras.layers.BatchNormalization(axis = -1)(pool2_7)\n#drop2_7 = tf.keras.layers.Dropout(rate=0.2)(norm2_7)\n\nconv3_7 = tf.keras.layers.Conv2D(128, kernel_size=(5, 5),  activation='relu', padding='same')(norm2_7)\npool3_7 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3_7)\nnorm3_7 = tf.keras.layers.BatchNormalization(axis = -1)(pool3_7)\n#drop3_7 = tf.keras.layers.Dropout(rate=0.2)(norm3_7)\n\ndil_conv1_7 = tf.keras.layers.Conv2D(256, kernel_size=(5, 5),dilation_rate=(2,2),  activation='relu', padding='same')(norm3_7)\nconv4_7 = tf.keras.layers.Conv2D(256, kernel_size=(5, 5),  activation='relu', padding='same')(dil_conv1_7)\n\npool4_7 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv4_7)\nnorm4_7 = tf.keras.layers.BatchNormalization(axis = -1)(pool4_7)\n#drop4_7 = tf.keras.layers.Dropout(rate=0.2)(norm4_7)\n\nflat_7 = tf.keras.layers.Flatten()(norm4_7)  #Flatten the matrix to get it ready for dense.\n\nhidden1_7 = tf.keras.layers.Dense(300, activation='relu')(flat_7)\nnorm5_7 = tf.keras.layers.BatchNormalization(axis = -1)(hidden1_7)\n#drop5_7 = tf.keras.layers.Dropout(rate=0.2)(norm5_7)\n\nhidden2_7 = tf.keras.layers.Dense(100, activation='relu')(norm5_7)\nnorm6_7 = tf.keras.layers.BatchNormalization(axis = -1)(hidden2_7)\n#drop6_7 = tf.keras.layers.Dropout(rate=0.2)(norm6_7)\n\nout_7 = tf.keras.layers.Dense(2, activation='softmax')(norm6_7)   #units=1 gives error\n\nmodel_7 = tf.keras.Model(inputs=input_layers_7, outputs=out_7)","9b0b1093":"model_7.compile(optimizer='adam',\n                loss='categorical_crossentropy',   #Check between binary_crossentropy and categorical_crossentropy\n                metrics=['accuracy'])\n#print(model_7.summary())\n\n#Fit the model\nprint('start Model train')\ncall_backs = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',mode='max',patience=3,verbose=1)\n\nhistory = model_7.fit(train_x, \n                         train_y, \n                         batch_size = 64, \n                         verbose = 1, \n                         epochs = 10,      #Changed to 3 from 50 for testing purposes.\n                         validation_data=(eval_x, eval_y),\n                         callbacks=[call_backs]\n                     )","3e5dc378":"test_accuracy=model_7.evaluate(test_x, test_y)[1]*100\nprint(\"Test_Accuracy: {:.2f}%\".format(test_accuracy))","ce40be7d":"models_train.append(['start with small kernel size then increase',test_accuracy,history.history['accuracy'],history.history['val_accuracy'],history.history['loss'],history.history['val_loss']])","616f11b4":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('CNN Performance', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nmax_epoch = len(history.history['accuracy'])+1\nepoch_list = list(range(1,max_epoch))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(1, max_epoch, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(1, max_epoch, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")\n","9ed8796f":"columns=['model','Test Accuracy','Train Accuracy','Validation Accuracy','Train Loss','Validation Loss']\ndf=pd.DataFrame(models_train,columns=columns)","498c5c72":"df","803a7483":"#Save the model\n#model_2.save('malaria_cnn.h5')","d64c34fe":"# change stride","e8aebe12":"# change kernel size","b00d71f5":"# Show Training data shape(size)","e279ad02":"# Data PrePocessing","a85c1fcb":"# influence of adding dilated convolution","689bd148":"# change dense layer size","48e54237":"change train data to categorical data for softmax activation function","e245d935":"# add dropout after 2 conv layers","f7235b60":"# use sigmoid activation function","25af0e89":"# Data Load","f8787e46":"# imports","74c72f03":"# start with kernel size small then increase the kernel size","f54cfe2d":"# change activation function to softmax","5b2688a3":"# add all dropout","c129d752":"# Show Data Samples","7c3700d1":"# Shuffle and split dataset\n1. Train       80%\n2. Validation  10%\n3. Test        10%","3a5b2bd1":"# Save & Reload Variables"}}