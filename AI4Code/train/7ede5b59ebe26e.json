{"cell_type":{"bb8c26ba":"code","d8d5a0ad":"code","29a01630":"code","14d4b25a":"code","8ababa07":"code","88326a1a":"code","6b7ce01b":"code","4e91f1a5":"code","a9ae757c":"code","7e79eb9c":"markdown"},"source":{"bb8c26ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# 3D plot\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d8d5a0ad":"#Load Data\ndata = pd.read_csv(\"..\/input\/iris-flower-dataset\/IRIS.csv\")\ndata","29a01630":"data.describe()","14d4b25a":"# split data\nX = data.iloc[:,:4].values \ny = data.iloc[:,4:].values ","8ababa07":"# label encoding\nspecies = data.iloc[:,4:].values\nfrom sklearn import preprocessing\n#\u00f6nce tr, fr ve us yi 0 1 ve 2 haline \u00e7eviriyoruz\nle=preprocessing.LabelEncoder()\nspecies[:,0] = le.fit_transform(data.iloc[:,4:])\ny = species\ny=y.astype('int')","88326a1a":"# plots\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n\nplt.figure(1, figsize=(8,6))\nplt.clf()\n\n# 2D plot\nplt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.Set1, edgecolor=\"k\")\nplt.xlabel(\"Sepal Length\")\nplt.ylabel(\"Sepal Width\")\n\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\n\n# 3D plot the training points -- it's better to see this data in 3D\nfig = plt.figure(2, figsize=(8,6))\nax = Axes3D(fig, elev=-150, azim=110)\nax.scatter(X[:,0], X[:,1], X[:,2], c=y, cmap=plt.cm.Set1, edgecolor='k', s=40)\nax.set_title(\"Iris Dataset\")\nax.set_xlabel(\"First Spec\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"Second Spec\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"Third Spec\")\nax.w_zaxis.set_ticklabels([])\n\nplt.show()","6b7ce01b":"# train-test-split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.33, random_state=0)\n\n# scaling dataset\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(x_train)\nX_test = sc.transform(x_test)\n#y_test = y_test.reshape(-1,1)\n","4e91f1a5":"# Algorithms\nfrom sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression(random_state = 0)\nlog_reg.fit(X_train, y_train)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=6, metric='minkowski')\nknn.fit(X_train,y_train)\n\nfrom sklearn.svm import SVC\nsvc = SVC(kernel='rbf')\nsvc.fit(X_train,y_train)\ny_pred = svc.predict(X_test)\n\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n\nfrom sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(criterion = 'entropy')\ndtc.fit(X_train,y_train)\n\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=10, criterion = 'entropy')\nrfc.fit(X_train,y_train)","a9ae757c":"# confusion matrices\ny_pred = log_reg.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test,y_pred)\nprint(\"Logistic Regression\")\nprint(cm)\n\ny_pred = knn.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nprint(\"KNN\")\nprint(cm)\n\ny_pred = gnb.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nprint('SVC')\nprint(cm)\n\ny_pred = dtc.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nprint('GNB')\nprint(cm)\n\ny_pred = rfc.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nprint('DTC')\nprint(cm)\n\ncm = confusion_matrix(y_test,y_pred)\nprint('RFC')\nprint(cm)","7e79eb9c":"In this notebook, I compare some algorithms' results. Changing some parameters may give better results.  \n\n*Note to myself: using a function to change parameters will be easier than doing this manually.*"}}