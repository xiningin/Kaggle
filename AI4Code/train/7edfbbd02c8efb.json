{"cell_type":{"73b77025":"code","7927bdef":"code","cb1b504c":"code","3e5466b4":"code","75b31221":"code","3dd51dbe":"code","986b9bb8":"code","be57ffbd":"code","2ab374e2":"code","ccb530bd":"code","d1734831":"code","fdbd86ee":"code","1ccbbfee":"code","17f495b8":"code","15506f40":"code","4fe58c5a":"code","027f31bc":"code","18e031d5":"code","6ba34f24":"code","5ae58831":"code","0547667e":"code","d164fc04":"code","5cd88f6b":"code","c281cbee":"code","9ba449e2":"code","be314acb":"code","d02dcefe":"code","5ed88f2c":"code","d69e11c0":"code","e8abc89f":"code","93242fb2":"code","91ad2927":"code","c542375a":"code","5d453eb6":"code","3f00f219":"code","62a22ed0":"code","4507bc8b":"code","427067b3":"code","a87ca644":"code","c4edcaeb":"code","e25d42d7":"code","57e64c83":"code","e6d8c4da":"code","49ed7639":"code","c39fa37c":"code","b92b318e":"code","32061626":"code","9e9178ed":"code","94bde4e7":"markdown","cf9eb006":"markdown","8669c0c9":"markdown","2de504ed":"markdown","c2734001":"markdown","3e0ffc2d":"markdown"},"source":{"73b77025":"#first import libraries\n# data analysis and wrangling\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier","7927bdef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cb1b504c":"# 1- Load the dataset\ntrain_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ncombine = [train_df, test_df]","3e5466b4":"#Analyze by describing data\n#Which features are available in the dataset?\n\nprint(train_df.columns.values)","75b31221":"#Which features are categorical?\n#Which features are numerical?\ntrain_df.head()","3dd51dbe":"# Which features contain blank, null or empty values?\ntrain_df.info()\nprint('*'*40)\ntest_df.info()","986b9bb8":"#the distribution of numerical feature values across the samples\ntrain_df.describe()","be57ffbd":"#the distribution of categorical features\ntrain_df.describe(include=['O'])\n\n# Names are unique across the dataset (count=unique=891)\n\n# Sex variable as two possible values with 65% male (top=male, freq=577\/count=891).\n\n# Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.\n\n# Embarked takes three possible values. S port used by most passengers (top=S).\n\n# Ticket feature has high ratio (22%) of duplicate values (unique=681).\n","2ab374e2":"train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","ccb530bd":"train_df[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","d1734831":"train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","fdbd86ee":"train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","1ccbbfee":"print('Before', train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\n\nprint('After', train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)","17f495b8":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract('([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])    ","15506f40":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n    'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","4fe58c5a":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()","027f31bc":"train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","18e031d5":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()","6ba34f24":"guess_ages = np.zeros((2,3))\nguess_ages","5ae58831":"for dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df.head()","0547667e":"train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","d164fc04":"#Let us replace Age with ordinals based on these bands.\n\nfor dataset in combine:\n    dataset.loc[dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_df.head()","5cd88f6b":"train_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()","c281cbee":"#We can create a new feature for FamilySize which combines Parch and SibSp. This will enable us to drop Parch and SibSp from our datasets.\nfor dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","9ba449e2":"#We can create another feature called IsAlone.\nfor dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","be314acb":"#Let us drop Parch, SibSp, and FamilySize features in favor of IsAlone.\ntrain_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","d02dcefe":"#We can also create feature combining Pclass and Age.\nfor dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntrain_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)","5ed88f2c":"freq_port = train_df.Embarked.dropna().mode()[0]\nfreq_port","d69e11c0":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e8abc89f":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\n    \ntrain_df.head()\n    ","93242fb2":"test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()","91ad2927":"train_df['FareBand'] = pd.cut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index= False).mean().sort_values(by='FareBand', ascending=True) ","c542375a":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head(10)","5d453eb6":"test_df.head(10)","3f00f219":"X_train = train_df.drop('Survived', axis=1)\nY_train = train_df['Survived']\nX_test = test_df.drop('PassengerId', axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape\n","62a22ed0":"lr = LogisticRegression()\nlr.fit(X_train, Y_train)\nY_pred = lr.predict(X_test)\nacc_log = round(lr.score(X_train, Y_train) * 100, 2)\nacc_log","4507bc8b":"coeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(lr.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","427067b3":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","a87ca644":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","c4edcaeb":"gaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","e25d42d7":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","57e64c83":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","e6d8c4da":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","49ed7639":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","c39fa37c":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","b92b318e":"xgb = XGBClassifier()\nxgb.fit(X_train, Y_train)\nY_pred = xgb.predict(X_test)\nacc_xgb = round(xgb.score(X_train, Y_train) * 100, 2)\nacc_xgb","32061626":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'XGBoost'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree, acc_xgb]})\nmodels.sort_values(by='Score', ascending=False)","9e9178ed":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n","94bde4e7":"**Analyze by pivoting features**","cf9eb006":"**Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations.\n**\n","8669c0c9":"**We can convert the categorical titles to ordinal.\n**\n","2de504ed":"**retain the new Title feature for model training.**","c2734001":"**Model evaluation**","3e0ffc2d":"**Workflow stages\n\nThe competition solution workflow goes through six stages.\n\n1- Acquire training and testing data.\n\n2- Wrangle, prepare, cleanse the data.\n\n3- Analyze, identify patterns, and explore the data.\n\n4- Model, predict and solve the problem.\n\n5- Visualize, report, and present the problem solving steps and final solution.\n\n6- Supply or submit the results.\n\n"}}