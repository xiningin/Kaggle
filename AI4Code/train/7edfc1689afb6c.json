{"cell_type":{"7fbff234":"code","12ef95a4":"code","db5dcc72":"code","9a5f8d48":"code","5ae631b8":"code","1b137c25":"code","b240f7a7":"code","552ee7ff":"code","5ca767d1":"code","ca244d84":"code","c30c850c":"code","f5b39f57":"code","4873b5b5":"code","60417288":"code","15d197fb":"code","df079180":"code","8cdf40df":"code","9744afbd":"code","4a2d1f9f":"code","1971f37d":"code","bd38c06e":"code","69096161":"code","0f167120":"code","2194755a":"code","572504fb":"code","d0903416":"code","d0939c3c":"code","9e013b03":"code","f916ffe9":"code","b72bc014":"code","1fa0ec8c":"code","afa7f608":"code","70c207ea":"code","9d3566c7":"code","4d7b5745":"code","e5c9114d":"code","f3134cb0":"code","5bb32060":"code","b3d2b575":"code","e14aabc7":"code","de34bfc1":"code","c3427e21":"code","393610b9":"code","2ebed4c6":"code","add386fc":"code","4dd97409":"code","4a639e0c":"code","7d4af275":"code","a40d9453":"code","30640eb0":"code","6236ab68":"code","06f45cd8":"code","809425aa":"code","2314bf59":"code","5e5d039c":"code","cc6ed520":"code","ce0510c7":"code","fe7f7931":"code","975fb204":"code","763d552b":"code","d8d758c5":"code","56413bae":"code","766edc3e":"code","e52cc168":"code","23868cf6":"code","90358b1b":"code","b06fd121":"code","341486b1":"code","ec0a73ff":"code","53dba2e1":"code","597d0f5c":"code","ea34846d":"code","4bbc1c9e":"code","52a3f603":"code","334e6005":"code","896aee76":"code","3b98a0ac":"code","4b6b4264":"code","4035fde6":"code","02fa61aa":"code","13f501df":"code","98ca3440":"code","556190b9":"code","226a7110":"code","13ca42bf":"code","b6074de0":"code","f4e88a97":"code","adfd2204":"code","30333772":"code","484f4989":"code","51011f95":"code","af05c529":"code","16836754":"code","6647c0d8":"code","28d94910":"code","4931763a":"code","dd9f9514":"code","629e2733":"code","3651adc7":"code","7e945bf8":"code","d9c4ef8f":"code","b21cf923":"code","4a340fbe":"code","188ff4eb":"code","71a8901c":"code","284a21e4":"code","8c5e5cd7":"code","123fccc4":"code","9dcfc4a1":"code","d04107d3":"code","05316483":"code","8ddc3378":"code","7f4ec71a":"code","603971bf":"code","a3afd46c":"code","c0bdad4b":"code","a4005232":"code","6c79495e":"code","034efa10":"code","55ba20b7":"code","992e3f1f":"code","5e15e6ab":"code","2bcc4eeb":"code","98d3dad6":"code","c2c27815":"code","5529ddc4":"code","04760a57":"code","54e2533e":"code","16a1a890":"code","ebd341b9":"code","9fd2559a":"code","ff424732":"code","1ba9c6ee":"code","b7774490":"code","3f13123e":"code","5192fd69":"code","9b497ac2":"code","4a010725":"code","fd100b4a":"code","a354ce49":"code","0416449e":"code","b3c905b4":"markdown"},"source":{"7fbff234":"!pip install simdkalman","12ef95a4":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport numpy as np # linear algebra\nfrom pathlib import Path\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy import sparse\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport simdkalman\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tqdm.notebook import tqdm\nfrom warnings import simplefilter\n\nsimplefilter('ignore')\nplt.style.use('fivethirtyeight')\npd.set_option('max_columns', 100)\npd.set_option('max_rows', 100)","db5dcc72":"model_name = 'nn_v2'\n\ndata_dir = Path('..\/input\/google-smartphone-decimeter-challenge')\ntrain_file = data_dir \/ 'baseline_locations_train.csv'\ntest_file = data_dir \/ 'baseline_locations_test.csv'\nsample_file = data_dir \/ 'sample_submission.csv'\n\nbuild_dir = Path('.\/build')\nbuild_dir.mkdir(parents=True, exist_ok=True)\npredict_val_file = build_dir \/ f'{model_name}.val.txt'\npredict_tst_file = build_dir \/ f'{model_name}.tst.txt'\nsubmission_file = 'submission.csv'\n\ncname_col = 'collectionName'\npname_col = 'phoneName'\nphone_col = 'phone'\nts_col = 'millisSinceGpsEpoch'\ndt_col = 'datetime'\nlat_col = 'latDeg'\nlon_col = 'lngDeg'\n\nlrate = .01 #.001\nbatch_size = 32 #1024\nepochs = 2000#100\nn_stop = 10\nn_fold = 5\nseed = 42#77","9a5f8d48":"train = pd.read_csv(train_file)\ntest = pd.read_csv(test_file)","5ae631b8":"train.groupby('collectionName').apply(lambda x: x['phoneName'].unique())","1b137c25":"test.groupby('collectionName').apply(lambda x: x['phoneName'].unique())","b240f7a7":"train.phoneName.unique()","552ee7ff":"f = open('..\/input\/google-smartphone-decimeter-challenge\/train\/2020-05-14-US-MTV-1\/Pixel4\/supplemental\/Pixel4_GnssLog.20o', 'r')\ndata = f.readlines()\nf.close()\ndata[:20]","5ca767d1":"f = open('..\/input\/google-smartphone-decimeter-challenge\/train\/2020-05-14-US-MTV-1\/Pixel4\/supplemental\/SPAN_Pixel4_10Hz.nmea', 'r')\ndata = f.readlines()\nf.close()\ndata[:10]","ca244d84":"ground = pd.read_csv('..\/input\/google-smartphone-decimeter-challenge\/train\/2020-05-14-US-MTV-1\/Pixel4\/ground_truth.csv')\nground.head(5)","c30c850c":"f = open('..\/input\/google-smartphone-decimeter-challenge\/train\/2020-05-14-US-MTV-1\/Pixel4\/Pixel4_GnssLog.txt', 'r')\ndata = f.readlines()\nf.close()\ndata[:10]","f5b39f57":"derived = pd.read_csv('..\/input\/google-smartphone-decimeter-challenge\/train\/2020-05-14-US-MTV-1\/Pixel4\/Pixel4_derived.csv')\nderived","4873b5b5":"derived_unique = list(derived.millisSinceGpsEpoch.unique())\nlen(derived_unique)","60417288":"ground_unique = list(ground.millisSinceGpsEpoch.unique())\nlen(ground_unique)","15d197fb":"import json\njson_open = open('..\/input\/google-smartphone-decimeter-challenge\/metadata\/accumulated_delta_range_state_bit_map.json', 'r')\njson.load(json_open)","df079180":"import json\njson_open = open('..\/input\/google-smartphone-decimeter-challenge\/metadata\/raw_state_bit_map.json', 'r')\njson.load(json_open)","8cdf40df":"pd.read_csv('..\/input\/google-smartphone-decimeter-challenge\/metadata\/constellation_type_mapping.csv').head(5)","9744afbd":"# from https:\/\/www.kaggle.com\/sohier\/loading-gnss-logs\ndef gnss_log_to_dataframes(path):\n    print('Loading ' + path, flush=True)\n    gnss_section_names = {'Raw','UncalAccel', 'UncalGyro', 'UncalMag', 'Fix', 'Status', 'OrientationDeg'} #\u3053\u308c\u306f\u3069\u3053\u304b\u3089\u3067\u3066\u304d\u305f\u306e\u304b\uff1f\n    with open(path) as f_open:\n        datalines = f_open.readlines()\n\n    datas = {k: [] for k in gnss_section_names}\n    gnss_map = {k: [] for k in gnss_section_names}\n    for dataline in datalines:\n        is_header = dataline.startswith('#')\n        dataline = dataline.strip('#').strip().split(',')\n        # skip over notes, version numbers, etc\n        if is_header and dataline[0] in gnss_section_names:\n            gnss_map[dataline[0]] = dataline[1:]\n        elif not is_header:\n            datas[dataline[0]].append(dataline[1:])\n\n    results = dict()\n    for k, v in datas.items():\n        results[k] = pd.DataFrame(v, columns=gnss_map[k])\n    # pandas doesn't properly infer types from these lists by default\n    for k, df in results.items():\n        for col in df.columns:\n            if col == 'CodeType':\n                continue\n            results[k][col] = pd.to_numeric(results[k][col])\n\n    return results","4a2d1f9f":"gnss_section_names = {'Raw','UncalAccel', 'UncalGyro', 'UncalMag', 'Fix', 'Status', 'OrientationDeg'}\ndatas = {k: [] for k in gnss_section_names}\ngnss_map = {k: [] for k in gnss_section_names}\ndatas","1971f37d":"results = dict()\nfor k, v in datas.items():\n     results[k] = pd.DataFrame(v, columns=gnss_map[k])\nresults","bd38c06e":"# from https:\/\/www.kaggle.com\/dannellyz\/start-here-simple-folium-heatmap-for-geo-data\nimport folium\nfrom folium import plugins\n\n\ndef simple_folium(df:pd.DataFrame, lat_col:str, lon_col:str):\n    #Preprocess\n    #Drop rows that do not have lat\/lon\n    df = df[df[lat_col].notnull() & df[lon_col].notnull()]\n\n    # Convert lat\/lon to (n, 2) nd-array format for heatmap\n    # Then send to list\n    df_locs = list(df[[lat_col, lon_col]].values)\n\n    ##folium.Map\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u4f5c\u6210\n    fol_map = folium.Map([df[lat_col].median(), df[lon_col].median()])\n\n    # plot heatmap\n    heat_map = plugins.HeatMap(df_locs)\n    print(heat_map)\n    fol_map.add_child(heat_map)\n\n    # plot markers\n    markers = plugins.MarkerCluster(locations = df_locs)\n    fol_map.add_child(markers)\n\n    #Add Layer Control\n    folium.LayerControl().add_to(fol_map)\n\n    return fol_map","69096161":"# from https:\/\/www.kaggle.com\/jpmiller\/baseline-from-host-data\n# simplified haversine distance\ndef calc_haversine(lat1, lon1, lat2, lon2):\n    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat\/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon\/2.0)**2\n\n    c = 2 * np.arcsin(a**0.5)\n    dist = 6_367_000 * c\n    return dist","0f167120":"# from https:\/\/www.kaggle.com\/emaerthin\/demonstration-of-the-kalman-filter\nT = 1.0 \nstate_transition = np.array([[1, 0, T, 0, 0.5 * T ** 2, 0], [0, 1, 0, T, 0, 0.5 * T ** 2], [0, 0, 1, 0, T, 0],\n                             [0, 0, 0, 1, 0, T], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]])\nprocess_noise = np.diag([1e-5, 1e-5, 5e-6, 5e-6, 1e-6, 1e-6]) + np.ones((6, 6)) * 1e-9\nobservation_model = np.array([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0]])\nobservation_noise = np.diag([5e-5, 5e-5]) + np.ones((2, 2)) * 1e-9\n\nkf = simdkalman.KalmanFilter(\n        state_transition = state_transition,\n        process_noise = process_noise,\n        observation_model = observation_model,\n        observation_noise = observation_noise)\n\ndef apply_kf_smoothing(df, kf_=kf):\n    unique_paths = df[phone_col].unique()\n    for phone in tqdm(unique_paths):\n        data = df.loc[df[phone_col] == phone][[lat_col, lon_col]].values\n        data = data.reshape(1, len(data), 2)\n        smoothed = kf_.smooth(data)\n        df.loc[df[phone_col] == phone, lat_col] = smoothed.states.mean[0, :, 0]\n        df.loc[df[phone_col] == phone, lon_col] = smoothed.states.mean[0, :, 1]\n    return df","2194755a":"trn = pd.read_csv(train_file)\nprint(trn.shape)\ntrn.head()","572504fb":"tst = pd.read_csv(test_file)\nprint(tst.shape)\ntst.head()","d0903416":"sub = pd.read_csv(sample_file)\nprint(sub.shape)\nsub.head()","d0939c3c":"cname = trn[cname_col][0]\ncname","9e013b03":"pname = trn[pname_col][0]\npname","f916ffe9":"path =str(data_dir \/ 'train' \/ cname \/ pname \/ f'{pname}_GnssLog.txt')\nwith open(path) as f_open:\n        datalines = f_open.readlines()","b72bc014":"    for dataline in datalines:\n        is_header = dataline.startswith('#')\n        dataline = dataline.strip('#').strip().split(',')\n        break","1fa0ec8c":"datalines[:10]","afa7f608":"for col in [cname_col, pname_col]:\n    print(f'# of unique {col:>14s} in training: {trn[col].nunique():4d}')\n    print(f'# of unique {col:>14s}     in test: {tst[col].nunique():4d}')","70c207ea":"trn[pname_col].value_counts()","9d3566c7":"tst[pname_col].value_counts()","4d7b5745":"print(f'# of unique phone in training: {trn[phone_col].nunique():4d}')\nprint(f'    # of unique phone in test: {tst[phone_col].nunique():4d}')","e5c9114d":"trn[phone_col].value_counts()[:10]","f3134cb0":"tst[phone_col].value_counts()[:10]","5bb32060":"overlapping_phones = [x for x in tst[phone_col] if x in trn[phone_col]]\nprint(len(overlapping_phones))","b3d2b575":"tst[ts_col].min(), tst[ts_col].max()","e14aabc7":"dt_offset = pd.to_datetime('1980-01-06 00:00:00')\nprint(dt_offset)\ndt_offset_in_ms = int(dt_offset.value \/ 1e6)","de34bfc1":"trn[dt_col] = pd.to_datetime(trn[ts_col] + dt_offset_in_ms, unit='ms')\ntst[dt_col] = pd.to_datetime(tst[ts_col] + dt_offset_in_ms, unit='ms')\nprint(f'Training data range: {trn[dt_col].min()} - {trn[dt_col].max()}')\nprint(f'    Test data range: {tst[dt_col].min()} - {tst[dt_col].max()}')","c3427e21":"latlon_trn = trn[[lat_col, lon_col]].round(3)\nlatlon_trn['counts'] = 1\nlatlon_trn = latlon_trn.groupby([lat_col, lon_col]).sum().reset_index()\nlatlon_trn.head()","393610b9":"    #def simple_folium(df:pd.DataFrame, lat_col:str, lon_col:str):\n    simple_folium(latlon_trn, lat_col, lon_col)\n    df = pd.DataFrame(latlon_trn)\n\n    #Preprocess\n    #Drop rows that do not have lat\/lon\n    df = df[df[lat_col].notnull() & df[lon_col].notnull()]\n    df\n","2ebed4c6":"\n    # Convert lat\/lon to (n, 2) nd-array format for heatmap\n    # Then send to list\n    df_locs = list(df[[lat_col, lon_col]].values)\n    ","add386fc":"df[lat_col].median() #\u4e2d\u592e\u5024","4dd97409":"    fol_map = folium.Map([df[lat_col].median(), df[lon_col].median()])\n    fol_map","4a639e0c":"    # plot heatmap\n    heat_map = plugins.HeatMap(df_locs)\n    print(heat_map)","7d4af275":"    fol_map.add_child(heat_map)\n    fol_map","a40d9453":"\n    # plot markers\n    markers = plugins.MarkerCluster(locations = df_locs)\n    ","30640eb0":"fol_map.add_child(markers)","6236ab68":"    #Add Layer Control\n    folium.LayerControl().add_to(fol_map)","06f45cd8":"simple_folium(latlon_trn, lat_col, lon_col)","809425aa":"latlon_tst = tst[[lat_col, lon_col]].round(3)\nlatlon_tst","2314bf59":"latlon_tst['counts'] = 1\nlatlon_tst = latlon_tst.groupby([lat_col, lon_col]).sum().reset_index()\nlatlon_tst","5e5d039c":"simple_folium(latlon_tst, lat_col, lon_col)","cc6ed520":"cname = trn[cname_col][0]\ncname","ce0510c7":"pname = trn[pname_col][0]\npname","fe7f7931":"dfs = gnss = gnss_log_to_dataframes(str(data_dir \/ 'train' \/ cname \/ pname \/ f'{pname}_GnssLog.txt'))\nprint(dfs.keys())","975fb204":"df_raw = dfs['Raw']\nprint(df_raw.shape)\ndf_raw.head()","763d552b":"df_raw.info()","d8d758c5":"df_raw['ArrivalTime'] = df_raw['TimeNanos'] - df_raw['FullBiasNanos'] - df_raw['BiasNanos']\nprint(df_raw['ArrivalTime'].describe())\ndf_raw['ArrivalTime'].hist(bins=20)","56413bae":"print(df_raw['BiasUncertaintyNanos'].describe())\ndf_raw['BiasUncertaintyNanos'].hist(bins=20)","766edc3e":"print(df_raw['ReceivedSvTimeUncertaintyNanos'].describe())\ndf_raw['ReceivedSvTimeUncertaintyNanos'].hist(bins=20)","e52cc168":"print(df_raw.AccumulatedDeltaRangeUncertaintyMeters.describe())\ndf_raw.AccumulatedDeltaRangeUncertaintyMeters.hist(bins=20)","23868cf6":"print(df_raw.Cn0DbHz.describe())\ndf_raw.Cn0DbHz.hist(bins=20)","90358b1b":"df_raw = df_raw.loc[\n    ~pd.isnull(df_raw.FullBiasNanos) &\n    (df_raw.BiasUncertaintyNanos < 100) &\n    (df_raw.ArrivalTime > 0) &\n    (df_raw.ConstellationType != 0) &\n    ~pd.isnull(df_raw.TimeNanos) &\n    (df_raw.State != 3) & (df_raw.State != 14) & (df_raw.State != 7) & (df_raw.State != 15) &\n    (df_raw.ReceivedSvTimeUncertaintyNanos < 100) &\n    (df_raw.AccumulatedDeltaRangeUncertaintyMeters < 0.3) &\n    (df_raw.Cn0DbHz > 20)\n]\nprint(df_raw.shape)","b06fd121":"df_raw","341486b1":"derived = pd.read_csv(data_dir \/ 'train' \/ cname \/ pname \/ f'{pname}_derived.csv')\nprint(derived.shape)\nderived.head()","ec0a73ff":"derived.info()","53dba2e1":"derived = derived.loc[derived.constellationType != 0]\nprint(derived.shape)","597d0f5c":"derived","ea34846d":"derived['correctedPrM'] = (derived['rawPrM'] + derived['satClkBiasM'] - derived['isrbM'] - \n                           derived['ionoDelayM'] - derived['tropoDelayM'])\nsns.pairplot(data=derived, vars=['correctedPrM', 'rawPrM'], size=3)","4bbc1c9e":"derived[dt_col] = pd.to_datetime(derived[ts_col] + dt_offset_in_ms, unit='ms')\nprint(f'Data range for {cname}\/{pname}: {derived[dt_col].min()} - {derived[dt_col].max()}')","52a3f603":"derived[['constellationType', 'svid', 'signalType']].value_counts()","334e6005":"derived[[ts_col, 'constellationType', 'correctedPrM']].groupby([ts_col, 'constellationType']).agg(['mean', 'std', 'count']).describe()","896aee76":"derived.loc[derived.constellationType == 1][[ts_col, 'svid', 'correctedPrM']].groupby([ts_col, 'svid']).agg(['mean', 'std', 'count']).describe()","3b98a0ac":"pd.read_csv('..\/input\/google-smartphone-decimeter-challenge\/metadata\/constellation_type_mapping.csv')","4b6b4264":"derived.loc[derived.signalType == 'GPS_L1'][[ts_col, 'svid', 'correctedPrM']].groupby([ts_col, 'svid']).agg(['mean', 'std', 'count'])","4035fde6":"derived.loc[derived.signalType == 'GPS_L1'][[ts_col, 'svid', 'correctedPrM']].groupby([ts_col, 'svid']).agg(['mean', 'std', 'count']).describe()","02fa61aa":"derived.loc[derived.signalType == 'GPS_L1'][[ts_col, 'svid']].drop_duplicates().groupby([ts_col]).agg(['mean', 'std', 'count']).describe()","13f501df":"gps_l1 = derived.loc[derived.signalType == 'GPS_L1'][[ts_col, 'svid', 'correctedPrM']].drop_duplicates([ts_col, 'svid'])\nprint(gps_l1.shape)\ngps_l1.head()","98ca3440":"label = pd.read_csv(data_dir \/ 'train' \/ cname \/ pname \/ 'ground_truth.csv')\nprint(label.shape)\nlabel.head()","556190b9":"label[dt_col] = pd.to_datetime(label[ts_col] + dt_offset_in_ms, unit='ms')\nprint(f'Labels range for {cname}\/{pname}: {label[dt_col].min()} - {label[dt_col].max()}')","226a7110":"cname = trn[cname_col][10]\npname = trn[pname_col][10]\nderived2 = pd.read_csv(data_dir \/ 'train' \/ cname \/ pname \/ f'{pname}_derived.csv')\nlabel2 = pd.read_csv(data_dir \/ 'train' \/ cname \/ pname \/ 'ground_truth.csv')\nprint(f\"Derived data starts at: {pd.to_datetime(derived2[ts_col].min() + dt_offset_in_ms, unit='ms')}\")\nprint(f\"  Label data starts at: {pd.to_datetime(label2[ts_col].min() + dt_offset_in_ms, unit='ms')}\")","13ca42bf":"trn.sort_values([phone_col, ts_col], inplace=True)","b6074de0":"trn[['prev_lat']] = trn[lat_col].shift().where(trn[phone_col].eq(trn[phone_col].shift()))\ntrn[['prev_lat']] ","f4e88a97":"trn[['prev_lon']] = trn[lon_col].shift().where(trn[phone_col].eq(trn[phone_col].shift()))\ntrn[['prev_lon']]","adfd2204":"tst.sort_values([phone_col, ts_col], inplace=True)","30333772":"tst[['prev_lat']] = tst[lat_col].shift().where(tst[phone_col].eq(tst[phone_col].shift()))\ntst[['prev_lat']] ","484f4989":"tst[['prev_lon']] = tst[lon_col].shift().where(tst[phone_col].eq(tst[phone_col].shift()))\ntrn.head()","51011f95":"# from https:\/\/www.kaggle.com\/jpmiller\/baseline-from-host-data\nlabel_files = (data_dir \/ 'train').rglob('ground_truth.csv')\nlabel_files","af05c529":"cols = [phone_col, ts_col, lat_col, lon_col]\n\ndf_list = []\nfor t in tqdm(label_files, total=73):\n    label = pd.read_csv(t, usecols=[cname_col, pname_col, ts_col, lat_col, lon_col])\n    df_list.append(label)\n   ","16836754":"df_label = pd.concat(df_list, ignore_index=True)\ndf_label","6647c0d8":"pd.DataFrame(df_list)[:5]","28d94910":"df_label[phone_col] = df_label[cname_col] + '_' + df_label[pname_col]\ndf_label","4931763a":"df = df_label.merge(trn[cols + ['prev_lat', 'prev_lon']], how='inner', on=[phone_col, ts_col], \n                    suffixes=('_gt', '')).drop([cname_col, pname_col], axis=1) #\u5217\u540d\u304c\u91cd\u8907\u3057\u3066\u3044\u308b\u5834\u5408\u306e\u30b5\u30d5\u30a3\u30c3\u30af\u30b9\u3092\u6307\u5b9a: \u5f15\u6570suffixes\ndf","dd9f9514":"df['sSinceGpsEpoch'] = df[ts_col] \/\/ 1000 ## \u5207\u308a\u6368\u3066\u9664\u7b97\nprint(df.shape)\ndf.head()","629e2733":"df_tst = sub[[phone_col, ts_col]].merge(tst[[phone_col, ts_col, lat_col, lon_col, 'prev_lat', 'prev_lon']], \n                                        how='left', on=[phone_col, ts_col], suffixes=('', '_basepred'))\ndf_tst","3651adc7":"df_tst['sSinceGpsEpoch'] = df_tst[ts_col] \/\/ 1000\nprint(df_tst.shape)\ndf_tst.head()","7e945bf8":"derived_files = (data_dir \/ 'train').rglob('*_derived.csv')\ncols = [ts_col, 'svid', 'correctedPrM']\nderived_files","d9c4ef8f":"df_list = []\nfor t in tqdm(derived_files, total=73):\n    derived = pd.read_csv(t).drop_duplicates([ts_col, 'svid'])\n    derived['correctedPrM'] = (derived['rawPrM'] + derived['satClkBiasM'] - derived['isrbM'] - \n                               derived['ionoDelayM'] - derived['tropoDelayM'])\n    df_list.append(derived[[cname_col, pname_col, ts_col, 'svid', 'correctedPrM']])","b21cf923":"df_derived = pd.concat(df_list, ignore_index=True)\ndf_derived","4a340fbe":"df_derived[phone_col] = df_derived[cname_col] + '_' + df_derived[pname_col]\ndf_derived[phone_col] ","188ff4eb":"df_derived.drop([cname_col, pname_col], axis=1, inplace=True)\n\nprint(df_derived.shape)\ndf_derived.head()","71a8901c":"df_derived_pivot = pd.pivot_table(df_derived, \n                                  values='correctedPrM', \n                                  index=[phone_col, ts_col],\n                                  columns=['svid'],\n                                  aggfunc=np.mean)\ndf_derived_pivot ","284a21e4":"df_derived_pivot.columns = [f'svid_{x}' for x in df_derived_pivot.columns]\ndf_derived_pivot.columns","8c5e5cd7":"df_derived_pivot.reset_index(inplace=True)\ndf_derived_pivot","123fccc4":"df_derived_pivot['sSinceGpsEpoch'] = df_derived_pivot[ts_col] \/\/ 1000\n\nprint(df_derived_pivot.shape)\ndf_derived_pivot.head()","9dcfc4a1":"df = df.merge(df_derived_pivot, how='left', on=[phone_col, 'sSinceGpsEpoch'], suffixes=['', '_2'])\ndf.drop(['sSinceGpsEpoch', ts_col + '_2'], axis=1, inplace=True)\nprint(df.shape)\ndf.head()","d04107d3":"df['d_lat'] = df['latDeg_gt'] - df[lat_col]\ndf['d_lon'] = df['lngDeg_gt'] - df[lon_col]\ndf[['d_lat', 'd_lon']].describe()","05316483":"derived_files = (data_dir \/ 'test').rglob('*_derived.csv')\ncols = [ts_col, 'svid', 'correctedPrM']\nderived_files ","8ddc3378":"df_list = []\nfor t in tqdm(derived_files, total=48):\n    derived = pd.read_csv(t)\n    derived['sSinceGpsEpoch'] = derived[ts_col] \/\/ 1000\n    derived.drop_duplicates(['sSinceGpsEpoch', 'svid'], inplace=True)\n    derived['correctedPrM'] = (derived['rawPrM'] + derived['satClkBiasM'] - derived['isrbM'] - \n                               derived['ionoDelayM'] - derived['tropoDelayM'])\n    df_list.append(derived[[cname_col, pname_col, 'sSinceGpsEpoch', 'svid', 'correctedPrM']])\n    ","7f4ec71a":"df_derived = pd.concat(df_list, ignore_index=True)\ndf_derived","603971bf":"df_derived[phone_col] = df_derived[cname_col] + '_' + df_derived[pname_col]\ndf_derived.drop([cname_col, pname_col], axis=1, inplace=True)\ndf_derived","a3afd46c":"df_derived_pivot = pd.pivot_table(df_derived, \n                                  values='correctedPrM', \n                                  index=[phone_col, 'sSinceGpsEpoch'],\n                                  columns=['svid'],\n                                  aggfunc=np.mean)\ndf_derived_pivot","c0bdad4b":"df_derived_pivot.columns = [f'svid_{x}' for x in df_derived_pivot.columns]\ndf_derived_pivot.reset_index(inplace=True)\ndf_derived_pivot","a4005232":"df_tst = df_tst.merge(df_derived_pivot, how='left', \n                      on=[phone_col, 'sSinceGpsEpoch']).drop(['sSinceGpsEpoch'], axis=1)\nprint(df_tst.shape)\ndf_tst.head()","6c79495e":"df_tst.describe()","034efa10":"feature_cols = [x for x in df_tst.columns if x not in [phone_col, ts_col]]\ntarget_cols = ['d_lat', 'd_lon']\ninput_dim = len(feature_cols)\noutput_dim = len(target_cols)","55ba20b7":"feature_cols ","992e3f1f":"scaler = StandardScaler()\nlabel_scaler = StandardScaler()\nscaler.fit(pd.concat([df[feature_cols], df_tst[feature_cols]], axis=0).fillna(0).values)\nX = scaler.transform(df[feature_cols].fillna(0).values)\nX_tst = scaler.transform(df_tst[feature_cols].fillna(0).values)\nY = label_scaler.fit_transform(df[target_cols].values)\nprint(X.shape, Y.shape, X_tst.shape)","5e15e6ab":"def scheduler(epoch, lr, warmup=5):\n    if epoch < warmup:\n        return lr * 1.5\n    else:\n        return lr * tf.math.exp(-.1) #epoch\u6bce\u306b\u6e1b\u8870\u3055\u305b\u3066\u3044\u308b\u3002","2bcc4eeb":"import optuna \nimport optuna.integration.lightgbm as lgbo\nimport lightgbm as lgb\n'''\nparams = { 'objective': 'mse', 'metric': 'mse' }\nY = pd.DataFrame(Y,columns={'data','data2'})\n\nlgb_train1 = lgb.Dataset(X, Y.data)\nlgb_valid1 = lgb.Dataset(X, Y.data)\nmodel1 = lgbo.train(params, lgb_train1, valid_sets=[lgb_valid1], verbose_eval=False, num_boost_round=100, early_stopping_rounds=5) \nmodel1.params[\"learning_rate\"] = 0.01\nmodel1.params[\"early_stopping_round\"] = 100\nmodel1.params[\"num_iterations\"] = 8000\nmodel1.params\n'''","98d3dad6":"params1= {'objective': 'mse',\n 'metric': 'l2',\n 'feature_pre_filter': False,\n 'lambda_l1': 0.0,\n 'lambda_l2': 0.0,\n 'num_leaves': 253,\n 'feature_fraction': 0.8999999999999999,\n 'bagging_fraction': 0.8540227553324429,\n 'bagging_freq': 2,\n 'min_child_samples': 5,\n 'num_iterations': 8000,\n 'early_stopping_round': 100,\n 'learning_rate': 0.01}","c2c27815":"'''\nlgb_train2 = lgb.Dataset(X, Y.data2)\nlgb_valid2 = lgb.Dataset(X, Y.data2)\nmodel2 = lgbo.train(params, lgb_train2, valid_sets=[lgb_valid2], verbose_eval=False, num_boost_round=100, early_stopping_rounds=5) \nmodel2.params[\"learning_rate\"] = 0.01\nmodel2.params[\"early_stopping_round\"] = 100\nmodel2.params[\"num_iterations\"] = 8000\nmodel2.params\n'''","5529ddc4":"params2 = {'objective': 'mse',\n 'metric': 'l2',\n 'feature_pre_filter': False,\n 'lambda_l1': 0.0,\n 'lambda_l2': 0.0,\n 'num_leaves': 242,\n 'feature_fraction': 0.8839999999999999,\n 'bagging_fraction': 0.9480235884535055,\n 'bagging_freq': 3,\n 'min_child_samples': 5,\n 'num_iterations': 8000,\n 'early_stopping_round': 100,\n 'learning_rate': 0.01}","04760a57":"params = {'objective': 'mse',\n 'metric': 'mse',\n 'num_iterations': 8000,\n 'early_stopping_round': 100,\n 'learning_rate': 0.001}","54e2533e":"\nfrom sklearn.multioutput import MultiOutputRegressor\nimport lightgbm as lgb\n\nparams={'learning_rate': 0.02, #0.01\n        'objective':'mae', \n        'metric':'mae',\n        'num_leaves': 9, #9 @@\n        'verbose': 0,\n        'bagging_fraction': 0.8, #0.7\n        'feature_fraction': 0.8 #0.7\n       }\nreg = MultiOutputRegressor(lgb.LGBMRegressor(**params, n_estimators=2000))\n\ncv = KFold(n_splits=n_fold, shuffle=True, random_state=seed)\n\nP = np.zeros_like(Y, dtype=float)\nP_tst = np.zeros((X_tst.shape[0], output_dim), dtype=float)\n#Y = pd.DataFrame(Y,columns={'data','data2'})\nfor i, (i_trn, i_val) in enumerate(cv.split(X), 1):\n    print(f'Training for CV #{i}')\n    #lgb_train1 = lgb.Dataset(X[i_trn], Y.data[i_trn])\n    #lgb_valid1 = lgb.Dataset(X[i_val], Y.data[i_val])\n    \n    #lgb_train2 = lgb.Dataset(X[i_trn], Y.data2[i_trn])\n    #lgb_valid2 = lgb.Dataset(X[i_val], Y.data2[i_val])\n    \n    reg.fit(X[i_trn], Y[i_trn])\n    #model1 = lgb.train(params1, lgb_train1, valid_sets=[lgb_valid1], verbose_eval=100)\n    #model2 = lgb.train(params2, lgb_train2, valid_sets=[lgb_valid2], verbose_eval=100)\n    \n    #a =model1.predict(X[i_val])\n    #b =model2.predict(X[i_val])\n    #tt = pd.DataFrame(columns=['a','b'],index=range(len(a)))\n    #tt.a = a\n    #tt.b = b\n   \n    tt = reg.predict(X[i_val])\n    P[i_val] = label_scaler.inverse_transform(tt)\n    \n    \n    #a =model1.predict(X_tst)\n    #b =model2.predict(X_tst)\n    #tt = pd.DataFrame(columns=['a','b'],index=range(len(a)))\n    #tt.a = a\n    #tt.b = b\n    tt = reg.predict(X_tst)\n\n    P_tst += label_scaler.inverse_transform(tt) \/ n_fold\n    \n    distance_i = calc_haversine(df.latDeg_gt.values[i_val], \n                                df.lngDeg_gt.values[i_val], \n                                P[i_val, 0] + df.latDeg.values[i_val], \n                                P[i_val, 1] + df.lngDeg.values[i_val]).mean()\n    print(f'CV #{i}: {np.percentile(distance_i, [50, 95])}')\n","16a1a890":"'''\ncv = KFold(n_splits=n_fold, shuffle=True, random_state=seed)\n\nP = np.zeros_like(Y, dtype=float)\nP_tst = np.zeros((X_tst.shape[0], output_dim), dtype=float)\nY = pd.DataFrame(Y,columns={'data','data2'})\nfor i, (i_trn, i_val) in enumerate(cv.split(X), 1):\n    print(f'Training for CV #{i}')\n    lgb_train1 = lgb.Dataset(X[i_trn], Y.data[i_trn])\n    lgb_valid1 = lgb.Dataset(X[i_val], Y.data[i_val])\n    \n    lgb_train2 = lgb.Dataset(X[i_trn], Y.data2[i_trn])\n    lgb_valid2 = lgb.Dataset(X[i_val], Y.data2[i_val])\n    \n\n    model1 = lgb.train(params1, lgb_train1, valid_sets=[lgb_valid1], verbose_eval=100)\n    model2 = lgb.train(params2, lgb_train2, valid_sets=[lgb_valid2], verbose_eval=100)\n    \n    a =model1.predict(X[i_val])\n    b =model2.predict(X[i_val])\n    tt = pd.DataFrame(columns=['a','b'],index=range(len(a)))\n    tt.a = a\n    tt.b = b\n   \n    P[i_val] = label_scaler.inverse_transform(tt)\n    \n    \n    a =model1.predict(X_tst)\n    b =model2.predict(X_tst)\n    tt = pd.DataFrame(columns=['a','b'],index=range(len(a)))\n    tt.a = a\n    tt.b = b\n\n    P_tst += label_scaler.inverse_transform(tt) \/ n_fold\n    \n    distance_i = calc_haversine(df.latDeg_gt.values[i_val], \n                                df.lngDeg_gt.values[i_val], \n                                P[i_val, 0] + df.latDeg.values[i_val], \n                                P[i_val, 1] + df.lngDeg.values[i_val]).mean()\n    print(f'CV #{i}: {np.percentile(distance_i, [50, 95])}')\n    '''","ebd341b9":"print(P.mean(axis=0), P_tst.mean(axis=0))\nnp.savetxt(predict_val_file, P, delimiter=',', fmt='%.6f')\nnp.savetxt(predict_tst_file, P_tst, delimiter=',', fmt='%.6f')","9fd2559a":"distance = calc_haversine(df.latDeg_gt, df.lngDeg_gt, P[:, 0] + df.latDeg, P[:, 1] + df.lngDeg)\nprint(f'CV All: {np.percentile(distance, [50, 95])}')","ff424732":"df.sort_values([phone_col, ts_col], inplace=True)\ndf_smoothed = df.copy()\ndf_smoothed[lat_col] = df[lat_col] + P[:, 0]\ndf_smoothed[lon_col] = df[lon_col] + P[:, 1]\ndf_smoothed = apply_kf_smoothing(df_smoothed)\ndistance = calc_haversine(df_smoothed.latDeg_gt, df_smoothed.lngDeg_gt, df_smoothed.latDeg, df_smoothed.lngDeg)\nprint(f'CV All (smoothed): {np.percentile(distance, [50, 95])}')","1ba9c6ee":"distance_tst = calc_haversine(df_tst.latDeg, df_tst.lngDeg, P_tst[:, 0] + df_tst.latDeg, P_tst[:, 1] + df_tst.lngDeg)\nprint(f'CV All: {np.percentile(distance_tst, [50, 95])}')","b7774490":"distance_tst","3f13123e":"df_tst.sort_values([phone_col, ts_col], inplace=True)\ndf_tst_smoothed = df_tst.copy()\ndf_tst_smoothed[lat_col] = df_tst_smoothed[lat_col] + P_tst[:, 0]\ndf_tst_smoothed[lon_col] = df_tst_smoothed[lon_col] + P_tst[:, 1]\ndf_tst_smoothed","5192fd69":"df_tst_smoothed = apply_kf_smoothing(df_tst_smoothed)\ndf_tst_smoothed","9b497ac2":"distance_tst = calc_haversine(df_tst.latDeg, df_tst.lngDeg, df_tst_smoothed.latDeg, df_tst_smoothed.lngDeg)\nprint(f'CV All (smoothed): {np.percentile(distance_tst, [50, 95])}')","4a010725":"df_tst_smoothed[[phone_col, ts_col, lat_col, lon_col]].to_csv(submission_file, index=False)","fd100b4a":"submission = df_tst_smoothed[[phone_col, ts_col, lat_col, lon_col]]","a354ce49":"def get_removedevice(input_df: pd.DataFrame, divece: str) -> pd.DataFrame:\n    input_df['index'] = input_df.index\n    input_df = input_df.sort_values('millisSinceGpsEpoch')\n    input_df.index = input_df['millisSinceGpsEpoch'].values\n\n    output_df = pd.DataFrame() \n    for _, subdf in input_df.groupby('collectionName'):\n\n        phones = subdf['phoneName'].unique()\n\n        if (len(phones) == 1) or (not divece in phones):\n            output_df = pd.concat([output_df, subdf])\n            continue\n\n        origin_df = subdf.copy()\n        \n        _index = subdf['phoneName']==divece\n        subdf.loc[_index, 'latDeg'] = np.nan\n        subdf.loc[_index, 'lngDeg'] = np.nan\n        subdf = subdf.interpolate(method='index', limit_area='inside')\n\n        _index = subdf['latDeg'].isnull()\n        subdf.loc[_index, 'latDeg'] = origin_df.loc[_index, 'latDeg'].values\n        subdf.loc[_index, 'lngDeg'] = origin_df.loc[_index, 'lngDeg'].values\n\n        output_df = pd.concat([output_df, subdf])\n\n    output_df.index = output_df['index'].values\n    output_df = output_df.sort_index()\n\n    del output_df['index']\n    \n    return output_df","0416449e":"submission['collectionName'] = submission['phone'].map(lambda x: x.split('_')[0])\nsubmission['phoneName'] = submission['phone'].map(lambda x: x.split('_')[1])\nsubmission = get_removedevice(submission, 'SamsungS20Ultra')\n\nsubmission = submission.drop(columns=['collectionName', 'phoneName'], axis=1)\nsubmission.to_csv('submission.csv', index=False)","b3c905b4":"# Google Smartphone Decimeter Challenge by LightGBM\n\nSolution is based on referring to various notebooks posted at Kaggle, along with my changes. Seeing this as a learning experience.\n\n## Notes\n\n If you\u2019re outside, with open sky, the GPS accuracy from your phone is about five meters, and that\u2019s been constant for a while. With raw GNSS measurements from the phones, this can now improve dramatically.\n \n### The GNSS problem description\nhttps:\/\/github.com\/commaai\/laika\n\nGNSS satellites orbit the earth broadcasting signals that allow the receiver to determine the distance to each satellite. These satellites have known orbits and so their positions are known. This makes determining the receiver's position a basic 3-dimensional trilateration problem. In practice observed distances to each satellite will be measured with some offset that is caused by the receiver's clock error. This offset also needs to be determined, making it a 4-dimensional trilateration problem. \n\n<img src= \"https:\/\/camo.githubusercontent.com\/0d85f5131c63442f8e7b46de7dab8040a7d693effd5e611ebed25be0b7600a32\/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f632f63332f33737068657265732e7376672f36323270782d33737068657265732e7376672e706e67\"  alt =\"GNSS\" style=\"width:400px;height:400px;\">\n\nSince this problem is generally overdetermined (more than 4 satellites to solve the 4d problem) there is a variety of methods to compute a position estimate from the measurements. One can use a basic weighted least squares solver for experimental purposes. This is far from optimal due to the dynamic nature of the system, this makes a Bayesian estimator like a Kalman filter the preferred estimator.\n\nHowever, the above description is over-simplified. Getting accurate distance estimates to satellites and the satellite's position from the receiver observations is not trivial. This is what we call processing of the GNSS observables and it is this procedure laika is designed to make easy.\n \n ### How positioning works?\n - Send a burst of these transactions and, as a consequence, the system can calculate ranging statistics, such as the mean and the variance.\n \n <img src= \"https:\/\/www.gpsworld.com\/wp-content\/uploads\/2018\/07\/Android-Figure-3.jpg\" alt =\"Wifi distance\">\n \n Wi-Fi RTT principles, basic concept. Image by Frank van Diggelen, Roy Want and Wei Wang\n \n - Take these ranges of separate access points; if those ranges were accurate, they would define four circles that would intersect at a single point. In practice, because of error in each range, a maximum likelihood position is calculated using a least squares multilateration algorithm.\n \n - Further refine this position by repeating the process, particularly as the phone moves, and then calculate trajectory using filtering techniques, such as Kalman filtering, to optimize the estimate.\n \n  <img src= \"https:\/\/www.gpsworld.com\/wp-content\/uploads\/2018\/07\/Android-Figure-4.jpg\" alt =\"Workflow\">\n  \n  Wi-Fi Workflow. Image by: Frank van Diggelen, Roy Want and Wei Wang.\n\n \n \n### Steps for the solution from Sohier Dane\n- Smoothing out the baseline estimates\n- Integrating readings from other phone instruments, like the accelerometer.\n- Satellite triangulation using the *derived.csv files.\n- Building triangulations directly from the raw gnss logs. \n- Incorporating external data for controls like satellite readings from base stations in the area.\n\n### References: \n- https:\/\/www.kaggle.com\/tensorchoko\/google-smartphone-lightgbm\n- https:\/\/www.kaggle.com\/jeongyoonlee\/google-smartphone-decimeter-eda-keras-tpu\n- Discussions topics from Sohier Dane\n- Google I\/O https:\/\/www.gpsworld.com\/how-to-achieve-1-meter-accuracy-in-android\/\n- GPS Survey Workshop video https:\/\/www.youtube.com\/watch?v=vOJ3u7Zd_i0\n- Hardware: Centimeter Positioning with a Smartphone-Quality GNSS Antenna https:\/\/www.youtube.com\/watch?v=rCOvklUB5vQ"}}