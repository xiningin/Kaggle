{"cell_type":{"715687c0":"code","c0c2390e":"code","a1a79b05":"code","1d299a45":"code","134f3a05":"code","d1e80a7a":"code","fad85061":"code","ff77db05":"code","5fc5cd6a":"code","cc35d8d5":"markdown","fff2a6d7":"markdown","2c924e74":"markdown","d0d0dbc0":"markdown","775ba272":"markdown"},"source":{"715687c0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c0c2390e":"from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nprint(\"cancer.keys():\", cancer.keys())","a1a79b05":"print(\"Shape of cancer data:\", cancer.data.shape)","1d299a45":"print(\"Sample counts per class:\", {n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))})","134f3a05":"cancer.feature_names","d1e80a7a":"cancer.DESCR","fad85061":"from sklearn.model_selection import train_test_split \nfrom sklearn.ensemble import GradientBoostingClassifier\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state = 0)\n\ngbrt = GradientBoostingClassifier(random_state = 0)\ngbrt.fit(X_train, y_train)\n\nprint(\"Accuracy on training set:\", gbrt.score(X_train, y_train))\nprint(\"Accuracy on test set:\", gbrt.score(X_test, y_test))","ff77db05":"gbrt = GradientBoostingClassifier(random_state = 0, max_depth = 1)\ngbrt.fit(X_train, y_train)\n\nprint(\"Accuracy on training set:\", gbrt.score(X_train, y_train))\nprint(\"Accuracy on test set:\", gbrt.score(X_test, y_test))","5fc5cd6a":"gbrt = GradientBoostingClassifier(random_state = 0, learning_rate =0.01)\ngbrt.fit(X_train, y_train)\n\nprint(\"Accuracy on training set:\", gbrt.score(X_train, y_train))\nprint(\"Accuracy on test set:\", gbrt.score(X_test, y_test))","cc35d8d5":"The data can be loaded using load_breast_cancer function from scikit-learn:","fff2a6d7":"As the training accuracy is 100 percent we are likely overfitting. Let us apply a stronger pre-pruning by limiting the maximum depth or lowering the learning rate:","2c924e74":"Of those 569 data points, 212 are labeled as malignant and 357 as benign:","d0d0dbc0":"The dataset consists of 569 data points, with 30 features each:","775ba272":"Gradient Boosted Regression Trees"}}