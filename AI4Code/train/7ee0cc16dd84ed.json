{"cell_type":{"85dad98f":"code","1cb634b5":"code","a89a7b94":"code","cd8e3c68":"code","0fecde19":"code","7d6a4704":"code","5902e23b":"code","dc4c35cd":"code","ce37dc93":"code","bbe59a2b":"code","7a8b0b48":"code","e079dd1e":"code","bee0e9ad":"code","c6c95bd1":"code","677d3e7d":"code","033b3f1d":"code","d6602cb1":"code","39a48e18":"code","61d6a1e7":"code","de701610":"code","375e4d30":"code","096c18ec":"code","a1b674d2":"code","b27b509c":"code","7693fed7":"code","27ecef3c":"code","817269d7":"code","55dde7bd":"code","bc7c7643":"code","0b084624":"code","bcfa13f2":"code","d730739e":"code","3dec25c6":"code","32debaf2":"code","37578495":"code","9af32546":"code","5c6f7396":"code","591f84a5":"code","8adb56a2":"code","7c377c45":"code","ca4cb8f1":"code","a6febc8d":"code","91de6c0a":"code","88195cd7":"code","8f551bff":"code","93cf6eab":"code","81f04335":"code","f9856a3e":"code","468a66f4":"code","ff6de60a":"code","c59bf6ca":"code","385860d5":"code","1d5de711":"code","59c76982":"code","6171a2b5":"code","e9d0668b":"code","b971e65b":"code","19801114":"code","74d95b7d":"code","a8c53c2f":"code","78cb84b9":"code","c5e648b9":"code","368d7e1a":"code","1c1be474":"code","04f22857":"code","efd516ab":"code","1774c852":"code","abf474a7":"code","fbc0e8b1":"code","f3dc327e":"code","26037acc":"code","ce069ad7":"code","d885bbf9":"code","dd50040a":"code","3f937604":"code","0a14ce96":"code","994fe380":"markdown","88884eee":"markdown","2048050e":"markdown","1b92a58a":"markdown","7596c591":"markdown","07f827d6":"markdown","742d7596":"markdown","5ec0d017":"markdown","2bb2f0a3":"markdown","dcee4a6e":"markdown","8403df81":"markdown","470a516b":"markdown","4e5ab44e":"markdown","a156656c":"markdown","746395e8":"markdown","1b053d7f":"markdown","ff71168e":"markdown","7867250e":"markdown","c07eabc6":"markdown","86c997c4":"markdown","0e3504e8":"markdown"},"source":{"85dad98f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","1cb634b5":"import seaborn as sns","a89a7b94":"from sklearn.model_selection import train_test_split","cd8e3c68":"from sklearn.ensemble import IsolationForest","0fecde19":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer","7d6a4704":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler","5902e23b":"from sklearn.model_selection import GridSearchCV","dc4c35cd":"from sklearn.linear_model import LogisticRegression","ce37dc93":"from sklearn.ensemble import RandomForestClassifier","bbe59a2b":"from collections import Counter","7a8b0b48":"from sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier","e079dd1e":"from sklearn.metrics import accuracy_score, recall_score, precision_score","bee0e9ad":"from sklearn.metrics import roc_auc_score, plot_roc_curve, precision_recall_curve, auc, f1_score, plot_confusion_matrix","c6c95bd1":"from sklearn.metrics import classification_report","677d3e7d":"from imblearn.over_sampling import SMOTE","033b3f1d":"from imblearn.pipeline import Pipeline as ImbalancedPipeline","d6602cb1":"from imblearn.over_sampling import RandomOverSampler","39a48e18":"from xgboost import XGBClassifier","61d6a1e7":"test_df = pd.read_csv(\"..\/input\/iba-ml1-mid-project\/test.csv\")","de701610":"test_id = test_df['Id']","375e4d30":"test_df = test_df.iloc[:, 1:]\ntest_df.head()","096c18ec":"df = pd.read_csv(\"..\/input\/iba-ml1-mid-project\/train.csv\")\ndf.head()","a1b674d2":"df = df.iloc[:,1:]\ndf.head(10)","b27b509c":"df['credit_line_utilization'] = pd.to_numeric(df['credit_line_utilization'], errors='coerce')","7693fed7":"test_df['credit_line_utilization'] = pd.to_numeric(test_df['credit_line_utilization'], errors='coerce')","27ecef3c":"df.isnull().sum().sort_values(ascending=False)","817269d7":"df['defaulted_on_loan'].value_counts()","55dde7bd":"df.describe()","bc7c7643":"%matplotlib inline\n\nsns.histplot(df['monthly_income'])","0b084624":"(df['monthly_income'] == 0).value_counts()","bcfa13f2":"df['log_monthly_income'] = np.log(df['monthly_income'])\ndf.head()","d730739e":"(df['log_monthly_income'] == np.NINF).value_counts()","3dec25c6":"df.loc[:, 'log_monthly_income'].replace(np.NINF, 0,inplace=True)\ndf.head()","32debaf2":"test_df['log_monthly_income'] = np.log(test_df['monthly_income'])\ntest_df.loc[:, 'log_monthly_income'].replace(np.NINF, 0,inplace=True)","37578495":"(test_df['log_monthly_income'] == np.NINF).value_counts()","9af32546":"fig, ax = plt.subplots(figsize=(15, 6))\n\nsns.histplot(x='log_monthly_income', hue='defaulted_on_loan',\n             data=df, ax=ax)","5c6f7396":"df['log_number_of_credit_lines'] = np.log(df['number_of_credit_lines'])\ndf.loc[:, 'log_number_of_credit_lines'].replace(np.NINF, 0,inplace=True)","591f84a5":"test_df['log_number_of_credit_lines'] = np.log(test_df['number_of_credit_lines'])\ntest_df.loc[:, 'log_number_of_credit_lines'].replace(np.NINF, 0,inplace=True)","8adb56a2":"# A copy of our data with the necessary features\n\ndf_cpy = df[['age', 'number_dependent_family_members', 'log_monthly_income', 'log_number_of_credit_lines', 'real_estate_loans',\n             'ratio_debt_payment_to_income', 'credit_line_utilization', 'number_of_previous_late_payments_up_to_59_days',\n            'number_of_previous_late_payments_up_to_89_days', 'number_of_previous_late_payments_90_days_or_more', 'defaulted_on_loan']]","7c377c45":"X = df_cpy.iloc[:, :-1]\nX.head()","ca4cb8f1":"y = df_cpy.iloc[:, -1]\ny.head()","a6febc8d":"over_sampler = RandomOverSampler(sampling_strategy='minority', random_state=0)","91de6c0a":"# With Outlier Detection\n\nforest_pipeline_OD = Pipeline(steps=[\n    ('classifier', RandomForestClassifier(random_state=0))\n])","88195cd7":"forest_pipeline.get_params().keys()","8f551bff":"OD_imputer = SimpleImputer(strategy='most_frequent')","93cf6eab":"def forest_splits_OD(X, y, split_ratios, OD_contamination):\n    \n    params = {'model' : {},\n              'accuracy' : {},\n              'recall' : {},\n              'precision' : {},\n              'f1_score' : {},\n              'roc_auc_score' : {},\n              'pr_auc' : {}}\n    \n    for split_ratio in split_ratios:\n\n        fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(15,8))       \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = split_ratio,\n                                                            random_state = 0)\n        \n        X_train = OD_imputer.fit_transform(X_train)\n        X_test = OD_imputer.fit_transform(X_test)\n        \n        iso = IsolationForest(contamination=OD_contamination, random_state=0)\n        y_hat = iso.fit_predict(X_train)\n        \n        mask = y_hat != -1\n        \n        X_train, y_train = X_train[mask], y_train[mask]\n        \n#         X_train, y_train = over_sampler.fit_resample(X_train, y_train)\n#         X_test, y_test = over_sampler.fit_resample(X_test, y_test)\n        \n#         X_train, X_test = pd.DataFrame(X_train), pd.DataFrame(X_test)\n        \n#         X_train.columns = list(X.columns)\n#         X_test.columns = list(X.columns)\n\n        X_train, y_train = over_sampler.fit_resample(X_train, y_train)\n        \n        X_train = pd.DataFrame(X_train)\n        \n        X_train.columns = list(X.columns)\n        \n        gridsearch = GridSearchCV(forest_pipeline_OD, forest_param_space_OD,\n                                  cv=5, )\n        gridsearch.fit(X_train, y_train)\n        \n        params['model'][split_ratio] = gridsearch\n        \n        y_pred = gridsearch.predict(X_test)\n        \n        precision = precision_score(y_test, y_pred)\n        params['precision'][split_ratio] = precision\n        \n        recall = recall_score(y_test, y_pred)\n        params['recall'][split_ratio] = recall\n                \n        accuracy = accuracy_score(y_test, y_pred)\n        params['accuracy'][split_ratio] = accuracy\n        \n        f1 = f1_score(y_test, y_pred)\n        params['f1_score'][split_ratio] = f1\n\n        roc_score = roc_auc_score(y_test, y_pred)\n        params['roc_auc_score'][split_ratio] = roc_score\n        \n        curve_precision, curve_recall, res = precision_recall_curve(y_test, y_pred)\n        \n        pr_auc = auc(curve_recall, curve_precision)\n        params['pr_auc'][split_ratio] = pr_auc\n        \n        \n        print(\"\\n>>> At split ratio \", split_ratio)\n        print(\"accuracy: \", accuracy)\n        print(\"ROC_AUC: \", roc_score)\n        print(\"Precision: \", precision)\n        print(\"Recall: \", recall)\n        print(\"PR AUC: \", pr_auc)\n        print(\"F1 score: \", f1)\n        print(\"Detected best params: \", gridsearch.best_params_)\n        \n        plot_confusion_matrix(gridsearch, X_test, y_test, ax=ax1)\n        plot_roc_curve(gridsearch, X_test, y_test, ax=ax2)\n        ax2.plot(np.linspace(0, 1, 10), np.linspace(0, 1, 10), '--', label = 'no skill classifier')\n        ax2.legend()\n        ax2.set_title(\"Split Ratio = {}\".format(split_ratio))\n        \n    plt.show()\n    \n    params = pd.DataFrame(params)\n    \n    return params","81f04335":"# With Outlier Detection\n\nforest_param_space_OD = {\n    'classifier__max_depth': [10],\n    'classifier__n_estimators': [200],\n    'classifier__criterion': ['gini'],\n    'classifier__max_leaf_nodes': [25],\n    'classifier__n_jobs': [5]\n}","f9856a3e":"%%time\n\nforests_OD_10 = forest_splits_OD(X, y, [0.1, 0.2, 0.3], 0.1)","468a66f4":"forests_OD_10","ff6de60a":"fig, ax = plt.subplots(figsize=(12,6))\n\nforests_OD_10.iloc[:, 1:].plot(kind='barh', ax=ax)\nax.grid()\nplt.show()","c59bf6ca":"feature_trfm = Pipeline(steps=[\n    ('imputer', SimpleImputer()),\n    ('scaler', MinMaxScaler())\n])\n\ncol_trfm = ColumnTransformer(transformers=[\n    ('numeric', feature_trfm, list(X.columns))\n])","385860d5":"HistGradientBoost_pipeline = Pipeline(steps=[\n    ('classifier', HistGradientBoostingClassifier(random_state=0))\n])","1d5de711":"HistGradientBoost_pipeline.get_params().keys()","59c76982":"HistGradientBoost_param_space = {\n    'classifier__max_depth': np.arange(5,21, 5),\n    'classifier__max_leaf_nodes': np.arange(2, 12, 3)\n}","6171a2b5":"HistBoost_imputer = SimpleImputer(strategy='median')","e9d0668b":"def HistGradientBoost_splits(X, y, split_ratios, OD_contamination):\n    \n    params = {'model' : {},\n              'accuracy' : {},\n              'recall' : {},\n              'precision' : {},\n              'f1_score' : {},\n              'roc_auc_score' : {},\n              'pr_auc' : {}}\n    \n    for split_ratio in split_ratios:\n\n        fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(15,8))       \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = split_ratio,\n                                                            random_state = 0)\n        \n        X_train = HistBoost_imputer.fit_transform(X_train)\n        X_test = HistBoost_imputer.fit_transform(X_test)\n        \n        iso = IsolationForest(contamination=OD_contamination, random_state=0)\n        y_hat = iso.fit_predict(X_train)\n        \n        mask = y_hat != -1\n        \n        X_train, y_train = X_train[mask], y_train[mask]\n        \n#         X_train, y_train = over_sampler.fit_resample(X_train, y_train)\n#         X_test, y_test = over_sampler.fit_resample(X_test, y_test)\n        \n#         X_train, X_test = pd.DataFrame(X_train), pd.DataFrame(X_test)\n        \n#         X_train.columns = list(X.columns)\n#         X_test.columns = list(X.columns)\n\n        X_train, y_train = over_sampler.fit_resample(X_train, y_train)\n        \n        X_train = pd.DataFrame(X_train)\n        \n        X_train.columns = list(X.columns)\n        \n        gridsearch = GridSearchCV(HistGradientBoost_pipeline, HistGradientBoost_param_space,\n                                  cv=5, )\n        gridsearch.fit(X_train, y_train)\n        \n        params['model'][split_ratio] = gridsearch\n        \n        y_pred = gridsearch.predict(X_test)\n        \n        precision = precision_score(y_test, y_pred)\n        params['precision'][split_ratio] = precision\n        \n        recall = recall_score(y_test, y_pred)\n        params['recall'][split_ratio] = recall\n                \n        accuracy = accuracy_score(y_test, y_pred)\n        params['accuracy'][split_ratio] = accuracy\n        \n        f1 = f1_score(y_test, y_pred)\n        params['f1_score'][split_ratio] = f1\n\n        roc_score = roc_auc_score(y_test, y_pred)\n        params['roc_auc_score'][split_ratio] = roc_score\n        \n        curve_precision, curve_recall, res = precision_recall_curve(y_test, y_pred)\n        \n        pr_auc = auc(curve_recall, curve_precision)\n        params['pr_auc'][split_ratio] = pr_auc\n        \n        \n        print(\"\\n>>> At split ratio \", split_ratio)\n        print(\"accuracy: \", accuracy)\n        print(\"ROC_AUC: \", roc_score)\n        print(\"Precision: \", precision)\n        print(\"Recall: \", recall)\n        print(\"PR AUC: \", pr_auc)\n        print(\"F1 score: \", f1)\n        print(\"Detected best params: \", gridsearch.best_params_)\n        \n        plot_confusion_matrix(gridsearch, X_test, y_test, ax=ax1)\n        plot_roc_curve(gridsearch, X_test, y_test, ax=ax2)\n        ax2.plot(np.linspace(0, 1, 10), np.linspace(0, 1, 10), '--', label = 'no skill classifier')\n        ax2.legend()\n        ax2.set_title(\"Split Ratio = {}\".format(split_ratio))\n        \n    plt.show()\n    \n    params = pd.DataFrame(params)\n    \n    return params","b971e65b":"%%time\n\nHistGradientBoost_1 = HistGradientBoost_splits(X, y, [0.1, 0.2, 0.3], 0.1)","19801114":"%%time\n\nHistGradientBoost_3 = HistGradientBoost_splits(X, y, [0.1, 0.2, 0.3], 0.05)","74d95b7d":"HistGradientBoost_param_space = {\n#     'preprocessing__numeric__imputer__strategy': ['median'],\n    'classifier__max_depth': np.arange(5, 11, 5),\n    'classifier__max_leaf_nodes': np.arange(11, 12, 3),\n    'classifier__max_iter': [100, 200, 300]\n}","a8c53c2f":"%%time\n\nHistGradientBoost_4 = HistGradientBoost_splits(X, y, [0.1, 0.2, 0.3], 0.05)","78cb84b9":"HistGradientBoost_4","c5e648b9":"HistGradientBoost_3","368d7e1a":"HistGradientBoost_2","1c1be474":"HistGradientBoost_1","04f22857":"test_df_processed = OD_imputer.fit_transform(test_df_cpy)\ntest_df_processed = pd.DataFrame(test_df_processed)\ntest_df_processed.columns = test_df_cpy.columns","efd516ab":"iso = IsolationForest(contamination=0.05, random_state=0)\nmask = iso.fit_predict(test_df_processed) != -1","1774c852":"print(test_df_processed.shape, ' vs ', test_df_processed[mask].shape)","abf474a7":"test_df_processed.loc[~mask, :] = np.NaN\ntest_df_processed.head()","fbc0e8b1":"test_df_processed = OD_imputer.transform(test_df_processed)","f3dc327e":"test_df_processed = pd.DataFrame(test_df_processed)\ntest_df_processed.columns = test_df_cpy.columns\ntest_df_processed.head()","26037acc":"RF_test_pred_OD10 = pd.concat([test_id, pd.Series(forests_OD_10.loc[0.2, 'model'].predict_proba(test_df_processed)[:, -1])], axis=1)\nRF_test_pred_OD10.columns = ['Id', 'Predicted']\nRF_test_pred_OD10.head()","ce069ad7":"HistGradientBoost_OS_OD_1 = pd.concat([test_id, pd.Series(HistGradientBoost_1.loc[0.2, 'model'].predict_proba(test_df_processed)[:, -1])], axis=1)\nHistGradientBoost_OS_OD_1.columns = ['Id', 'Predicted']\nHistGradientBoost_OS_OD_1.head()","d885bbf9":"HistGradientBoost_OS_OD_1.to_csv('HistGradientBoost_OS_OD_1.csv', index=False)","dd50040a":"RF_test_pred_OD10.to_csv('RF_OD_Oversampled10.csv', index=False)","3f937604":"HistGradientBoost_OS_OD_1.iloc[:, -1].hist()","0a14ce96":"RF_test_pred_OD10.iloc[:, -1].hist()","994fe380":"## Random Forests","88884eee":"# Fetching the Data","2048050e":"## Train Data","1b92a58a":"From the above it is lucid that ther are severe outliers in feautres 'ratio_debt_payment_to_income' and 'credit_line_utilization'. We will take care of the later.","7596c591":"We can see that are some values that do not quite fit the distribution. These values may not actually be considered by our models, so we better check for outliers as well. But before this, we need to remember that in the distributions of all the features we saw at the beginning of our EDA, we saw similar patterns emerge in some other features as well","07f827d6":"# Feature Engineering","742d7596":"From the above distribution plot of the \"monthly income\" feature we can see that our data is right skewed and this is not something that our ML models are fond of. So let's deal with this issue before moving on any further and see what we get. For this we'll engineer a new feature which will be the log of the feature in question. As we are using this approach, let's first see if there are any instances of zero income as this will result in negative inf values if we apply the mentioned method (log(0) = -inf). If there are, we will replace them with 0 instead","5ec0d017":"The credit_line_utilization feature is in encoded as string so we need to deal with that first  ","2bb2f0a3":"Now this looks like a better feature for our models, but we will still need to deal with the outliers","dcee4a6e":"## Test Data","8403df81":"#### With Outlier Detection","470a516b":"This will create a few instances of negative inf values as can be seen below","4e5ab44e":"It seems like we can deal with the skewness of the 'number_of_credit_lines' feature like we did in the case of 'monthly_income'. Let's see how it goes","a156656c":"# Importing the Libraries","746395e8":"## HistGradientBoostingClassifier","1b053d7f":"# Model Fitting","ff71168e":"We do have 0 monthly_income cases so we will proceed as mentioned","7867250e":"Doing the same for the test data and the rest of the skewed features","c07eabc6":"### Testing our model on the test set and saving the results","86c997c4":"#### Visual Inspection","0e3504e8":"Seems like among the Random Forests algorithms, we have a winner. The approach taken was to increase the generalization capabilities of our models as much as possible while keeping them accurate enough. We can see this from the fact that ROC_AUC score of 0.7583 on the training data becomes 0.84317 in the test data"}}