{"cell_type":{"1a20b1bc":"code","72521efb":"code","350df0f0":"code","e0c10de6":"code","42b0147d":"code","29231cc3":"code","2caad9fc":"code","ff665b70":"code","1b77f04d":"code","7f17c774":"code","e47840e5":"code","ebe7ac7d":"code","55a18146":"code","7cac4a2c":"code","27bb49db":"markdown","a79e0fea":"markdown","e03b1f66":"markdown"},"source":{"1a20b1bc":"import warnings\nwarnings.filterwarnings('ignore')","72521efb":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","350df0f0":"dfb_data = pd.read_csv('..\/input\/time-series-data\/daily-total-female-births.csv')\ndfb_data.head()","e0c10de6":"dfb_data.info()","42b0147d":"dfb_data.Date = pd.to_datetime(dfb_data.Date)","29231cc3":"dfb_data.set_index('Date',drop=True,inplace=True)","2caad9fc":"dfb_data.head()","ff665b70":"dfb_data.Births = dfb_data.Births.astype('float')","1b77f04d":"dfb_data.plot(figsize=(15,6));","7f17c774":"from statsmodels.tsa.stattools import adfuller\n\nresults = adfuller(dfb_data.Births)\n\nprint(f'ADF statistics : {results[0]}')\nprint(f'p-value : {results[1]}')\nprint(f'Critical Values : {results[4]}')","e47840e5":"results[1] <= 0.05 #The data is stationary","ebe7ac7d":"from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n\nfig = plt.figure()\nfig.subplots_adjust(hspace=0.6)\nplt.subplot(211)\nplot_acf(dfb_data.Births, ax=plt.gca()) # gca -> \"GetCurrentAxis\"\n# Plots lags on the horizontal and the correlations on vertical axis.\n\nplt.subplot(212)\nplot_pacf(dfb_data.Births, ax=plt.gca())\nplt.show();","55a18146":"from statsmodels.tsa.arima_model import ARIMA\n\ntrain_size = int(len(dfb_data.Births) * 0.80)\ntrain, test = dfb_data.Births[0:train_size], dfb_data.Births[train_size:]\n\n# walk-forward validation\nhistory = [x for x in train]\npredictions = list()\n\nfor i in range(len(test)):\n  model = ARIMA(history, order=(1,0,1))\n  model_fit = model.fit(disp=0)\n  yhat = model_fit.forecast()\n  predictions.append(yhat[0])\n  obs = test[i]\n  history.append(obs)\n","7cac4a2c":"from sklearn.metrics import mean_squared_error\n\n\nrmse = np.sqrt(mean_squared_error(test, predictions))\nprint(f'RMSE : {rmse}')","27bb49db":"# How to Read the above ACF and PACF plots ?\n--\nand \n**`also how to deduce p and q values for ARIMA`**\n\n**plot_acf(series)** \nRunning the example creates a 2D plot showing the `lag value` along the **x-axis** and the `correlation` on the y-axis between **-1 and 1**.\n\n`Confidence intervals` are drawn as a cone. By default, this is set to a `95% confidence interval`, suggesting that correlation values outside of this cone are very likely a correlation and not a statistical fluke.\n\n> `Tip` : By default, all lag values are printed, which makes the plot noisy.\nWe can limit the number of lags on the x-axis like this :\n`plot_acf(series, lags=50)`\nto make the plot easier to read. In our case yearly-water-usage.csv has fewer values.\n\n**Now let\u2019s check out on how we can figure out what value of p and q to use**. We have used two popular plotting techniques above; they are:\n\n\u2022 `Autocorrelation Function (ACF)`: It just measures the correlation between two consecutive (lagged version). example at lag 4, ACF will compare series at time instance t1\u2026t2 with series at instance t1\u20134\u2026t2\u20134\n\n\u2022 `Partial Autocorrelation Function (PACF)`: is used to measure the degree of association between y(t) and y(t-p).\n\n\u2022 p: The point where the PACF crosses the upper confidence interval and is the highest, here its 0 ( *look at x = 0* ). hence p = 0. This means at lag = 0, we get the highest correlation. \n\n\u2022 q: The point where the ACF crosses the upper confidence interval and is the highest. Here its close to 0. hence q = 0.\n\n**`This quick analysis suggests an ARIMA(0,1,0) on the raw data may be a good starting point`**","a79e0fea":"Ad-fuller technique helps to determine whether the data is stationary or not.<br>\nWe can acheive this by either comparing ADF statistics value with critical values or if p-value is less than equal to 0.05.","e03b1f66":"# ARIMA PART 1\n\nARIMA model is used for time series forecasting.\n\n\n![](https:\/\/drive.google.com\/uc?export=view&id=1By82n1gsOACXQsaauFq9rkAN34FzQ2JD)\n\n\n**ARIMA stands for Autoregressive Integrated Moving Average models**. Univariate (single vector) ARIMA is a forecasting technique that projects the future values of a series based entirely on its own inertia. `Its main application is in the area of short term forecasting requiring at least 40 historical data points`. It works best when your data exhibits a stable or consistent pattern over time with a minimum amount of outliers. Sometimes called `Box-Jenkins` (after the original authors), ARIMA is usually superior to exponential smoothing techniques when the data is reasonably long and the correlation between past observations is stable. If the data is short or highly volatile, then some smoothing method may perform better. If you do not have at least 38 data points, you should consider some other method than ARIMA.\n\n**`Basic Concepts:`**\n\nThe first step in applying ARIMA methodology **is to check for stationarity**. \"Stationarity\" implies that the series remains at a fairly constant level over time. If a trend exists, as in most economic or business applications, then your data is NOT stationary. The data should also show a constant variance in its fluctuations over time. This is easily seen with a series that is heavily seasonal and growing at a faster rate. In such a case, the ups and downs in the seasonality will become more dramatic over time. Without these stationarity conditions being met, many of the calculations associated with the process cannot be computed.\n\n\n**`Differencing`**:\n\nIf a graphical plot of the data indicates nonstationarity, then you should \"difference\" the series. Differencing is an excellent way of transforming a nonstationary series to a stationary one. This is done by subtracting the observation in the current period from the previous one. If this transformation is done only once to a series, you say that the data has been \"first differenced\"( means d=1 ). This process essentially eliminates the trend if your series is growing at a fairly constant rate. If it is growing at an increasing rate, you can apply the same procedure and difference the data again. Your data would then be \"second differenced\"( means d=2 ). \n\n**`Autocorrelations`**\n\n\"Autocorrelations\" are numerical values that indicate how a data series is related to itself over time. More precisely, it measures how strongly data values at a specified number of periods apart are correlated to each other over time. The number of periods apart is usually called the \"lag\". For example, an autocorrelation at lag 1 measures how values 1 period apart are correlated to one another throughout the series. An autocorrelation at lag 2 measures how the data two periods apart are correlated throughout the series. Autocorrelations may range from +1 to -1. A value close to +1 indicates a high positive correlation while a value close to -1 implies a high negative correlation.\n\n\n**`Autoregressive Models`**:\n\nARIMA methodology attempts to describe the movements in a stationary time series as a function of what are called \"autoregressive and moving average\" parameters. These are referred to as AR parameters (autoregessive) and MA parameters (moving averages). An AR model with only 1 parameter may be written as...\n\nX(t) = A(1) * X(t-1) + E(t)\n\nwhere X(t) = time series under investigation\n\nA(1) = the autoregressive parameter of order 1\n\nX(t-1) = the time series lagged 1 period\n\nE(t) = the error term of the model\n\nThis simply means that any given value X(t) can be explained by some function of its previous value, X(t-1), plus some unexplainable random error, E(t). If the estimated value of A(1) was .30, then the current value of the series would be related to 30% of its value 1 period ago. Of course, the series could be related to more than just one past value. For example,\n\nX(t) = A(1) * X(t-1) + A(2) * X(t-2) + E(t)\n\nThis indicates that the current value of the series is a combination of the two immediately preceding values, X(t-1) and X(t-2), plus some random error E(t). Our model is now an autoregressive model of order 2.\n\n**`Moving Average Models`**:\n\nA second type of Box-Jenkins model is called a \"moving average\" model. Although these models look very similar to the AR model, the concept behind them is quite different. Moving average parameters relate what happens in period t only to the random errors that occurred in past time periods, i.e. E(t-1), E(t-2), etc. rather than to X(t-1), X(t-2), (Xt-3) as in the autoregressive approaches. A moving average model with one MA term may be written as follows...\n\nX(t) = -B(1) * E(t-1) + E(t)\n\nThe term B(1) is called an MA of order 1. The negative sign in front of the parameter is used for convention only and is usually printed out auto- matically by most computer programs. The above model simply says that any given value of X(t) is directly related only to the random error in the previous period, E(t-1), and to the current error term, E(t). As in the case of autoregressive models, the moving average models can be extended to higher order structures covering different combinations and moving average lengths.\n\n**`Mixed Models`**:\n\nARIMA methodology also allows models to be built that incorporate both autoregressive and moving average parameters together. These models are often referred to as \"mixed models\". Although this makes for a more complicated forecasting tool, the structure may indeed simulate the series better and produce a more accurate forecast. Pure models imply that the structure consists only of AR or MA parameters - not both.\n\nThe models developed by this approach are usually called `ARIMA models because they use a combination of autoregressive (AR), integration (I) - referring to the reverse process of differencing to produce the forecast, and moving average (MA) operations`. An ARIMA model is usually stated as **`ARIMA(p,d,q)`**. This represents the order of the autoregressive components (p), the number of differencing operators (d), and the highest order of the moving average term(q). For example, ARIMA(2,1,1) means that you have a second order autoregressive model with a first order moving average component whose series has been differenced once to induce stationarity.\n\n\n**`Important`** : Parameters (p,d,q) of the ARIMA model.\n\nLet me explain these dependent parameters:\n\n\u2022 p : This is the number of AR (Auto-Regressive) terms or The number of lag observations included in the model, also called the lag order.\n\n**`Example`** \u2014 if p is 3 the predictor for y(t) will be y(t-1),y(t-2),y(t-3).\n\n\n\u2022 d :This is the number of differences or the number of non-seasonal differences. \n\n**`Example`** - \nd=0: no differencing (no trends)\n\nd=1: perform differencing once (linear trend)\n\nd=2: double differencing\n\n\n\u2022 q : The size of the moving average window. q is the number of lagged forecast errors in the prediction equation (MA part). This allows us to set the error of our model as a linear combination of the error values observed at previous time points in the past. \n\n**`Example`** - an MA of order 1 means that X(t) is directly related only to the random error in the previous period, E(t-1), and to the current error term, E(t)."}}