{"cell_type":{"59d5e24a":"code","44e0a15c":"code","9135e774":"code","bac34278":"code","928ad7e9":"code","22661f3e":"code","0889d642":"code","65f43ba3":"code","59bef8c4":"code","929014aa":"code","855e9ab7":"code","89ab447c":"code","0ead9393":"code","da1fc1de":"code","8a0129c5":"code","15b93082":"code","82e2b3b9":"code","16d53c51":"code","ae1ff1a9":"code","d48726eb":"code","704a1c8c":"code","1e2cf807":"code","32459946":"code","4b42402b":"code","9daedbc0":"code","c7d6ed8d":"code","9bd9c98d":"code","50fc9ea3":"code","d96853cd":"code","6b648e2c":"code","890a3027":"code","ec50c52f":"code","7d15e432":"code","87439826":"code","c73353db":"code","7b817daf":"code","95428bad":"code","f358ce13":"code","cb3b5cc2":"code","33c842dd":"code","546aa9ef":"code","f0845927":"code","105ddf06":"code","f56dc8d8":"code","3e0aca02":"code","d85016fd":"code","57755f8b":"code","6d152fa1":"code","b8f951a2":"code","21d1a001":"code","bb97cec1":"code","e24ec6cc":"code","a3a4bf33":"code","6ab92b74":"code","1d5169cc":"code","c462671d":"code","a9b55e10":"code","02cbb025":"code","e3b48ddd":"code","17a8bbde":"code","d94d4a91":"code","70605119":"code","0a0472f1":"code","710d3a31":"markdown","3702c415":"markdown","6f105ce6":"markdown","755c0795":"markdown","18fb7963":"markdown","7224ff39":"markdown","729ce1c7":"markdown","9a1e51ae":"markdown","5c5a513a":"markdown","55d9e082":"markdown","267c0344":"markdown","228f2222":"markdown","57aea328":"markdown","01ee2305":"markdown","5aeb3f14":"markdown","0cb35cfd":"markdown","e1d1e5aa":"markdown","13cbf350":"markdown","2a623a49":"markdown","ab72d6e9":"markdown","de4ac255":"markdown","e6b58d64":"markdown","68add8d7":"markdown"},"source":{"59d5e24a":"import pandas as pd\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nimport os\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nimport seaborn as sns","44e0a15c":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline","9135e774":"data_white = pd.read_csv(r'..\/input\/usi-nlp-practicum-2\/winequality-white.csv', delimiter= ';')\ndata_red = pd.read_csv(r'..\/input\/usi-nlp-practicum-2\/winequality-red.csv', delimiter= ';')\ndata_y = data_white.pop('quality')\ndata_y = pd.concat([data_y, data_red.pop('quality')])\ndata_white['white'] = 1\ndata_red['white'] = 0\ndata_x = pd.concat([data_white,data_red])","bac34278":"n, bins, patches = plt.hist(x=data_y, bins='auto', color='#0504aa',\n                            alpha=0.7, rwidth=1)\nplt.grid(axis='y', alpha=0.75)\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Wine Ratings')","928ad7e9":"data_y = 1*(data_y >= 7)\ndata_y.mean()","22661f3e":"enc = OneHotEncoder()\nY = enc.fit_transform(data_y[:, np.newaxis]).toarray()\nY","0889d642":"# Scale data to have mean 0 and variance 1 \n# which is importance for convergence of the neural network\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(data_x)\n\n# Split the data set into training and testing\nX_train, X_test, Y_train, Y_test = train_test_split(\n    X_scaled, Y, test_size=0.3, random_state=42)","65f43ba3":"from keras.models import Sequential\nfrom keras.layers import Dense\n\ninput_dim = X_scaled.shape[1]\noutput_dim = Y.shape[1]\n\nhidden_nodes = input_dim*2\n\nmodel = Sequential()\nmodel.add(Dense(hidden_nodes, input_dim = input_dim, activation = 'sigmoid'))\nmodel.add(Dense(output_dim, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', \n                      metrics=['accuracy'])\n\nmodel.summary()","59bef8c4":"24*12","929014aa":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom keras.callbacks import TensorBoard\n\nhistory_dict = {}\n\n# TensorBoard Callback\ncb = TensorBoard()\n\nhistory_callback = model.fit(X_train, Y_train,\n                                 batch_size=10,\n                                 epochs=10,\n                                 verbose=1,\n                                 validation_data=(X_test, Y_test),\n                                 callbacks=[cb])\n\nprint('Accuracy: '+str(model.evaluate(X_test, Y_test, verbose=0)[1]))","855e9ab7":"history_callback.history","89ab447c":"fig, (ax1, ax2) = plt.subplots(2, figsize=(8, 6))\n\n\naccuracy = history_callback.history['accuracy']\nloss = history_callback.history['loss']\nval_acc = history_callback.history['val_accuracy']\nval_loss = history_callback.history['val_loss']\nax1.plot(val_acc, label=\"validation accuracy\")\nax1.plot(accuracy, label = 'training accuracy')\nax2.plot(val_loss, label=\"validation loss\")\nax2.plot(loss, label=\"training loss\")\n\n\nax1.set_ylabel('validation accuracy')\nax2.set_ylabel('validation loss')\nax2.set_xlabel('epochs')\nax1.legend()\nax2.legend()\nplt.show()","0ead9393":"from sklearn.metrics import classification_report, confusion_matrix\nY_pred = model.predict_classes(X_test)\nY_test_list = [a[1] for a in Y_test]\n\nconfusion_matrix(Y_test_list,Y_pred)","da1fc1de":"print(classification_report(Y_test_list, Y_pred))","8a0129c5":"input_dim = X_scaled.shape[1]\noutput_dim = Y.shape[1]\n\nhidden_nodes = 24\n\nmodel_weighted = Sequential()\nmodel_weighted.add(Dense(hidden_nodes, input_dim = input_dim, activation = 'sigmoid'))\nmodel_weighted.add(Dense(output_dim, activation='softmax'))\n\nmodel_weighted.compile(loss='categorical_crossentropy', optimizer='adam', \n                      metrics=['accuracy'])\n\nmodel_weighted.summary()\n\nclass_weight = {0: 1,\n                1: 1\/data_y.mean()}","15b93082":"cb = TensorBoard()\n\nhistory_callback_weighted = model_weighted.fit(X_train, Y_train,\n                                 batch_size=10,\n                                 epochs=10,\n                                 verbose=1,\n                                 validation_data=(X_test, Y_test),\n                                 callbacks=[cb],\n                                 class_weight = class_weight)\n\nprint('Accuracy: '+str(model_weighted.evaluate(X_test, Y_test, verbose=0)[1]))","82e2b3b9":"fig, (ax1, ax2) = plt.subplots(2, figsize=(8, 6))\n\n\naccuracy = history_callback_weighted.history['accuracy']\nloss = history_callback_weighted.history['loss']\nval_acc = history_callback_weighted.history['val_accuracy']\nval_loss = history_callback_weighted.history['val_loss']\nax1.plot(val_acc, label=\"validation accuracy\")\nax1.plot(accuracy, label = 'training accuracy')\nax2.plot(val_loss, label=\"validation loss\")\nax2.plot(loss, label=\"training loss\")\n\n\nax1.set_ylabel('validation accuracy')\nax2.set_ylabel('validation loss')\nax2.set_xlabel('epochs')\nax1.legend()\nax2.legend()\nplt.show()","16d53c51":"Y_pred_w = model_weighted.predict_classes(X_test)\nconfusion_matrix(Y_test_list,Y_pred_w)","ae1ff1a9":"print(classification_report(Y_test_list, Y_pred_w))","d48726eb":"def create_custom_model(input_dim, output_dim, nodes, activation = 'sigmoid', name='model', lr = .001):\n    def create_model():\n        # Create model\n        model = Sequential(name=name)\n        model.add(Dense(nodes, input_dim=input_dim, activation=activation))\n        model.add(Dense(output_dim, activation='softmax'))\n\n        # Compile model\n        \n        optim = Adam(lr)\n        \n        model.compile(loss='categorical_crossentropy', \n                      optimizer=optim,\n                      metrics=['accuracy'])\n        return model\n    return create_model\n\n\ndef train_custom_models(model_list, batch = 10, epochs = 10, verbose = 0, class_weight = class_weight):\n    history_dict = {}\n\n    # TensorBoard Callback\n    cb = TensorBoard()\n\n    for create_model in model_list:\n        model = create_model()\n        print('Model name:', model.name)\n        history_callback = model.fit(X_train, Y_train,\n                                     batch_size=batch,\n                                     epochs=epochs,\n                                     verbose=verbose,\n                                     validation_data=(X_test, Y_test),\n                                     callbacks=[cb],\n                                    class_weight = class_weight)\n        score = model.evaluate(X_test, Y_test, verbose=0)\n        print('Test loss:', score[0])\n        print('Test accuracy:', score[1])\n\n        history_dict[model.name] = [history_callback, model]\n    return history_dict\n    \n    \ndef plot_models(history_dict):\n    fig, (ax1, ax2) = plt.subplots(2, figsize=(8, 6))\n\n    for model_name in history_dict:\n        val_acc = history_dict[model_name][0].history['val_accuracy']\n        val_loss = history_dict[model_name][0].history['val_loss']\n        ax1.plot(val_acc, label=model_name)\n        ax2.plot(val_loss, label=model_name)\n    \n    ax1.set_ylabel('validation accuracy')\n    ax2.set_ylabel('validation loss')\n    ax2.set_xlabel('epochs')\n    ax1.legend()\n    ax2.legend()\n    plt.show\n    \n    from sklearn.metrics import roc_curve, auc\n\n    plt.figure(figsize=(10, 10))\n    plt.plot([0, 1], [0, 1], 'k--')\n\n    for model_name in history_dict:\n        model = history_dict[model_name][1]\n\n        Y_pred = model.predict(X_test)\n        fpr, tpr, threshold = roc_curve(Y_test.ravel(), Y_pred.ravel())\n\n        plt.plot(fpr, tpr, label='{}, AUC = {:.3f}'.format(model_name, auc(fpr, tpr)))\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve')\n    plt.legend();\n","704a1c8c":"input_node_options = [12, 24, 48, 100, 200, 400]\n\nmodels_nodes = [create_custom_model(input_dim, output_dim, i, name='model_{}'.format(i)) \n          for i in input_node_options]\n\nfor create_model in models_nodes:\n    create_model().summary()","1e2cf807":"history_dict_nodes = train_custom_models(models_nodes, verbose = 1)","32459946":"plot_models(history_dict_nodes)","4b42402b":"activation_options = ['sigmoid', 'tanh', 'relu', 'exponential','elu']\n\nmodels_activation = [create_custom_model(input_dim, output_dim, 100, i, 'model_{}'.format(i), lr = .001) \n          for i in activation_options]\n\nfor create_model in models_activation:\n    create_model().summary()","9daedbc0":"history_dict_activation = train_custom_models(models_activation, epochs = 10, batch = 100)","c7d6ed8d":"plot_models(history_dict_activation)","9bd9c98d":"lr_options = [1e-3, 1e-3\/3, 1e-4, 1e-4\/3, 1e-5, 1e-5\/3]\n\nmodels_lr = [create_custom_model(input_dim, output_dim, 100, 'relu', 'model_{}'.format(i), lr = i) \n          for i in lr_options]\n\nfor create_model in models_lr:\n    create_model().summary()","50fc9ea3":"history_dict_activation = train_custom_models(models_lr, epochs = 10, batch = 100)","d96853cd":"plot_models(history_dict_activation)","6b648e2c":"model_mlp = Sequential()\nmodel_mlp.add(Dense(100, input_dim = input_dim, activation = 'relu'))\nmodel_mlp.add(Dense(100, input_dim = input_dim, activation = 'relu'))\nmodel_mlp.add(Dense(output_dim, activation='softmax'))\n\noptim = Adam()                    \n\nmodel_mlp.compile(loss='categorical_crossentropy', optimizer=Adam(.0001), \n                      metrics=['accuracy'])\n\nmodel_mlp.summary()","890a3027":"cb = TensorBoard()\n\nhistory_callback_weighted = model_mlp.fit(X_train, Y_train,\n                                 batch_size=20,\n                                 epochs=15,\n                                 verbose=1,\n                                 validation_data=(X_test, Y_test),\n                                 callbacks=[cb])\n\nprint('Accuracy: '+str(model_mlp.evaluate(X_test, Y_test, verbose=0)[1]))","ec50c52f":"from keras.layers import Dropout\n\nmodel_mlp_d = Sequential()\nmodel_mlp_d.add(Dense(100, input_dim = input_dim, activation = 'relu'))\nmodel_mlp_d.add(Dropout(.25))\nmodel_mlp_d.add(Dense(100, input_dim = input_dim, activation = 'relu'))\nmodel_mlp_d.add(Dropout(.25))\nmodel_mlp_d.add(Dense(output_dim, activation='softmax'))\n\noptim = Adam()                    \n\nmodel_mlp_d.compile(loss='categorical_crossentropy', optimizer=Adam(.0001), \n                      metrics=['accuracy'])\n\nmodel_mlp_d.summary()","7d15e432":"cb = TensorBoard()\n\nhistory_callback_mlp_d = model_mlp_d.fit(X_train, Y_train,\n                                 batch_size=20,\n                                 epochs=15,\n                                 verbose=1,\n                                 validation_data=(X_test, Y_test),\n                                 callbacks=[cb],\n                                 class_weight = class_weight)\n\nprint('Accuracy: '+str(model_mlp_d.evaluate(X_test, Y_test, verbose=0)[1]))","87439826":"model_mlp_d = Sequential()\nmodel_mlp_d.add(Dense(100, input_dim = input_dim, activation = 'relu'))\nmodel_mlp_d.add(Dropout(.25))\nmodel_mlp_d.add(Dense(100, input_dim = input_dim, activation = 'relu'))\nmodel_mlp_d.add(Dropout(.25))\nmodel_mlp_d.add(Dense(output_dim, activation='softmax'))\n\noptim = Adam()                    \n\nmodel_mlp_d.compile(loss='categorical_crossentropy', optimizer=Adam(1e-4, decay = 1e-5), \n                      metrics=['accuracy'])\n\nmodel_mlp_d.summary()","c73353db":"cb = TensorBoard()\n\nhistory_callback_mlp_d = model_mlp_d.fit(X_train, Y_train,\n                                 batch_size=20,\n                                 epochs=15,\n                                 verbose=1,\n                                 validation_data=(X_test, Y_test),\n                                 callbacks=[cb],\n                                 class_weight = class_weight)\n\nprint('Accuracy: '+str(model_mlp_d.evaluate(X_test, Y_test, verbose=0)[1]))","7b817daf":"fig, (ax1, ax2) = plt.subplots(2, figsize=(8, 6))\n\n\nval_acc = history_callback_mlp_d.history['val_accuracy']\nval_loss = history_callback_mlp_d.history['val_loss']\nax1.plot(val_acc, label=\"our first net\")\nax2.plot(val_loss, label=\"our first net\")\n    \nax1.set_ylabel('validation accuracy')\nax2.set_ylabel('validation loss')\nax2.set_xlabel('epochs')\nax1.legend()\nax2.legend()\nplt.show()","95428bad":"from keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)","f358ce13":"model_mlp_es = Sequential()\nmodel_mlp_es.add(Dense(100, input_dim = input_dim, activation = 'relu'))\nmodel_mlp_es.add(Dropout(.25))\nmodel_mlp_es.add(Dense(100, input_dim = input_dim, activation = 'relu'))\nmodel_mlp_es.add(Dropout(.25))\nmodel_mlp_es.add(Dense(output_dim, activation='softmax'))\n\noptim = Adam()                    \n\nmodel_mlp_es.compile(loss='categorical_crossentropy', optimizer=Adam(.0001), \n                      metrics=['accuracy'])\n\nmodel_mlp_es.summary()","cb3b5cc2":"cb = TensorBoard()\n\nhistory_callback_mlp_es = model_mlp_es.fit(X_train, Y_train,\n                                 batch_size=100,\n                                 epochs=100,\n                                 verbose=1,\n                                 validation_data=(X_test, Y_test),\n                                 callbacks=[cb, es],\n                                 class_weight = class_weight)\n\nprint('Accuracy: '+str(model_mlp_es.evaluate(X_test, Y_test, verbose=0)[1]))","33c842dd":"import tensorflow as tf\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nimport os\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","546aa9ef":"from keras.datasets import mnist  # get the dataset\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nplt.imshow(255-X_train[120], cmap='gray', vmin=0, vmax=255)","f0845927":"X_train[120]","105ddf06":"print('Training Size:'+str(X_train.shape))\nprint('Testing Size:'+str(X_test.shape))","f56dc8d8":"X_train = X_train\/255.\nX_test = X_test\/255.\nX_train_flat = X_train.reshape(60000, 784)\nX_test_flat = X_test.reshape(10000, 784)\n\nenc = OneHotEncoder(categories = 'auto')\ny_train = enc.fit_transform(y_train[:, np.newaxis]).toarray()\ny_test = enc.fit_transform(y_test[:, np.newaxis]).toarray()\ny_train","3e0aca02":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\n\ninput_dim = (X_train_flat.shape[1])\noutput_dim = y_train.shape[1]\n\nhidden_nodes = input_dim\n\nmodel = Sequential()\nmodel.add(Dense(hidden_nodes, input_dim = input_dim, activation = 'sigmoid'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(hidden_nodes, activation = 'sigmoid'))\nmodel.add(Dropout(.1))\nmodel.add(Dense(output_dim, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', \n                      metrics=['accuracy'])\n\nmodel.summary()","d85016fd":"from keras.callbacks import TensorBoard\n\nhistory_dict = {}\n\n# TensorBoard Callback\ncb = TensorBoard()\n\nhistory_callback = model.fit(X_train_flat, y_train,\n                                 batch_size=100,\n                                 epochs=5,\n                                 verbose=1,\n                                 validation_data=(X_test_flat, y_test),\n                                 callbacks=[cb])\n\nprint('Accuracy: '+str(model.evaluate(X_test_flat, y_test, verbose=0)[1]))","57755f8b":"from keras.layers import Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\n\nX_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\nX_test = X_test.reshape(X_test.shape[0], 28, 28, 1)","6d152fa1":"input_dim = (28,28,1)\noutput_dim = y_train.shape[1]\n\nmodel_cnn = Sequential()\nmodel_cnn.add(Conv2D(16, kernel_size=(3, 3),\n                 activation='sigmoid',\n                 input_shape=input_dim))\nmodel_cnn.add(Conv2D(32, (3, 3), activation='sigmoid'))\nmodel_cnn.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_cnn.add(Dropout(0.1))\nmodel_cnn.add(Flatten())\nmodel_cnn.add(Dense(64, activation='sigmoid'))\nmodel_cnn.add(Dropout(0.2))\nmodel_cnn.add(Dense(output_dim, activation='softmax'))\n\nmodel_cnn.compile(loss='categorical_crossentropy', optimizer='adam', \n                      metrics=['accuracy'])\n\nmodel_cnn.summary()\n","b8f951a2":"model_cnn.fit(X_train, y_train,\n          batch_size=100,\n          epochs=5,\n          verbose=1,\n          validation_data=(X_test, y_test))\n\nscore = model_cnn.evaluate(X_test, y_test, verbose=1)\nprint('Test accuracy:', score[1])","21d1a001":"input_dim = (28,28,1)\noutput_dim = y_train.shape[1]\n\nmodel_cnn_relu = Sequential()\nmodel_cnn_relu.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 padding ='same',\n                 input_shape=input_dim))\nmodel_cnn_relu.add(Conv2D(64, (3, 3), activation='relu', padding ='same'))\nmodel_cnn_relu.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_cnn_relu.add(Dropout(0.25))\nmodel_cnn_relu.add(Flatten())\nmodel_cnn_relu.add(Dense(128, activation='relu'))\nmodel_cnn_relu.add(Dropout(0.5))\nmodel_cnn_relu.add(Dense(output_dim, activation='softmax'))\n\nmodel_cnn_relu.compile(loss='categorical_crossentropy', optimizer='adam', \n                      metrics=['accuracy'])\n\nmodel_cnn_relu.summary()","bb97cec1":"model_cnn_relu.fit(X_train, y_train,\n          batch_size=100,\n          epochs=5,\n          verbose=1,\n          validation_data=(X_test, y_test))\n\nscore = model_cnn_relu.evaluate(X_test, y_test, verbose=0)\nprint('Test accuracy:', score[1])","e24ec6cc":"from keras.optimizers import Adam\n\nmodel_cnn_relu.compile(loss='categorical_crossentropy', optimizer=Adam(1e-5\/3), \n                      metrics=['accuracy'])\n\nmodel_cnn_relu.fit(X_train, y_train,\n          batch_size=100,\n          epochs=10,\n          verbose=1,\n          validation_data=(X_test, y_test))\n\nscore = model_cnn_relu.evaluate(X_test, y_test, verbose=0)\nprint('Test accuracy:', score[1])","a3a4bf33":"input_dim = (28,28,1)\noutput_dim = y_train.shape[1]\n\nfrom keras.optimizers import Adam\nfrom keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n\ncb = TensorBoard()\nearly = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=15, verbose=1, mode='auto')\ncheckpoint = ModelCheckpoint(\"mnist.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n\nopt = Adam(lr=(1e-4), beta_1 = .85, beta_2 = .99)\n\n\nmodel_cnn_practical = Sequential()\nmodel_cnn_practical.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 padding ='same',\n                 input_shape=input_dim))\nmodel_cnn_practical.add(Conv2D(64, (3, 3), activation='relu', padding ='same'))\nmodel_cnn_practical.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_cnn_practical.add(Dropout(0.25))\nmodel_cnn_practical.add(Flatten())\nmodel_cnn_practical.add(Dense(128, activation='relu'))\nmodel_cnn_practical.add(Dropout(0.5))\nmodel_cnn_practical.add(Dense(output_dim, activation='softmax'))\n\nmodel_cnn_practical.compile(loss='categorical_crossentropy', optimizer=opt, \n                      metrics=['accuracy'])\n\nmodel_cnn_practical.summary()","6ab92b74":"model_cnn_practical.fit(X_train, y_train,\n          batch_size=100,\n          epochs=100,\n          verbose=1,\n          callbacks = [cb, early, checkpoint],\n          validation_data=(X_test, y_test))\n\nscore = model_cnn_practical.evaluate(X_test, y_test, verbose=0)\nprint('Test accuracy:', score[1])","1d5169cc":"!pip install nltk gensim","c462671d":"import pandas as pd\nimport numpy as np\nimport nltk\nimport seaborn as sns\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nimport re\nimport os\nimport warnings\n\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Input, Dense, Embedding, Lambda,SimpleRNN, GRU, LSTM, Bidirectional, TimeDistributed\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.callbacks import TensorBoard\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom gensim.models import Word2Vec\n  \nwarnings.filterwarnings(action = 'ignore') \n\nnltk.download('popular')","a9b55e10":"book_list = [f'\/notebooks\/storage\/nn_practicum\/class_3\/data\/HP\/book_{x}.txt' for x in list(range(1,8))]\n#book_raw = [open(x,'r').read().replace(\"\\n\", \" \") for x in book_list]\n\nbook_num = 0\ndf = pd.DataFrame(columns = ['book', 'sentence'])\nfor book in book_list:\n    book_num += 1\n    sentences = nltk.sent_tokenize(open(book).read())\n    dfBook = pd.DataFrame({'book': book_num, 'sentence': sentences})\n    df = pd.concat([dfBook, df])\n    \n# Remove most of the punctuation, lower all letters, divide into tokens\ndef simple_tokens(text):\n    cleantext = re.sub(r'[^\\w\\s]', '', text)\n    cleantext = cleantext.lower()\n    return cleantext.split(' ')\n    \ndf['tokenized'] = df['sentence'].apply(simple_tokens)\n\nenc = OneHotEncoder()\nY = enc.fit_transform(df['book'][:, np.newaxis]).toarray()\nY\n\n# Split the data set into training and testing\nX_train, X_test, Y_train, Y_test = train_test_split(\n    df['sentence'], Y, test_size=0.2, random_state=42)\n\n# Tokenize text data as required by and using keras\ntokenizer = Tokenizer(num_words=5000, filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n')\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\n# Save total number of words in vocabulary to be used later\nvocab_size = len(tokenizer.word_index) + 1\n\n# Set max length of text vector for each sentence\nmaxlen = 30\n\n# Pad articles with fewer than maxlen tokens\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n\ny_integers = np.argmax(Y, axis=1)\nclass_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\nd_class_weights = dict(enumerate(class_weights))\n\nembed_size = 100","02cbb025":"model_rnn = Sequential()\nmodel_rnn.add(Embedding(input_dim = vocab_size, output_dim = embed_size, input_length = X_train.shape[1]))\nmodel_rnn.add(SimpleRNN(25))\nmodel_rnn.add(Dense(7, activation='softmax'))\n\nmodel_rnn.compile(loss='categorical_crossentropy', optimizer='adam', \n                      metrics=['accuracy'])\n\nmodel_rnn.summary() ","e3b48ddd":"# TensorBoard Callback\ncb = TensorBoard()\n\nhistory_callback = model_rnn.fit(X_train, Y_train,\n                                 batch_size=512,\n                                 epochs=5,\n                                 verbose=1,\n                                 class_weight = d_class_weights,\n                                 validation_data=(X_test, Y_test),\n                                 callbacks=[cb])\n\nprint('Accuracy: '+str(model_rnn.evaluate(X_test, Y_test, verbose=0)[1]))","17a8bbde":"model_lstm = Sequential()\nmodel_lstm.add(Embedding(input_dim = vocab_size, output_dim = embed_size, input_length = X_train.shape[1]))\nmodel_lstm.add(LSTM(25))\nmodel_lstm.add(Dense(7, activation='softmax'))\n\nmodel_lstm.compile(loss='categorical_crossentropy', optimizer='adam', \n                      metrics=['accuracy'])\n\nmodel_lstm.summary() ","d94d4a91":"# TensorBoard Callback\ncb = TensorBoard()\n\nhistory_callback = model_lstm.fit(X_train, Y_train,\n                                 batch_size=512,\n                                 epochs=5,\n                                 verbose=1,\n                                 validation_data=(X_test, Y_test),\n                                 callbacks=[cb])\n\nprint('Accuracy: '+str(model_lstm.evaluate(X_test, Y_test, verbose=0)[1]))","70605119":"model_bidir = Sequential()\nmodel_bidir.add(Embedding(input_dim = vocab_size, output_dim = embed_size, input_length = X_train.shape[1]))\nmodel_bidir.add(Bidirectional(LSTM(12,return_sequences=True), merge_mode='concat'))\nmodel_bidir.add(TimeDistributed(Dense(12,activation='relu')))\nmodel_bidir.add(Flatten())\nmodel_bidir.add(Dense(7, activation='softmax'))\n\nmodel_bidir.compile(loss='categorical_crossentropy', optimizer='adam', \n                      metrics=['accuracy'])\nmodel_bidir.summary()","0a0472f1":"# TensorBoard Callback\ncb = TensorBoard()\n\nhistory_callback = model_bidir.fit(X_train, Y_train,\n                                 batch_size = 512,\n                                 epochs=5,\n                                 verbose=1,\n                                 validation_data=(X_test, Y_test),\n                                 callbacks=[cb])\n\nprint('Accuracy: '+str(model_bidir.evaluate(X_test, Y_test, verbose=0)[1]))","710d3a31":"## The First Network\n### Design","3702c415":"## Data Normalization","6f105ce6":"### Visualizing Loss","755c0795":"### How I'd (honestly) start this off","18fb7963":"### Fine tuning","7224ff39":"### Training","729ce1c7":"## Learning Rate Decay","9a1e51ae":"### Now with padding and ReLU","5c5a513a":"## CNNs","55d9e082":"### The Metrics","267c0344":"## Data Import","228f2222":"### Trying Hidden Layer Sizes","57aea328":"### A CNN","01ee2305":"## Our \"Deep\" Learning Model","5aeb3f14":"# Dense Net Walkthrough","0cb35cfd":"## RNNs","e1d1e5aa":"### Visualizing Accuracy & Loss","13cbf350":"## Data Prep","2a623a49":"### Trying Activation Functions","ab72d6e9":"### Balancing Our Data","de4ac255":"## Regularizing our MLP","e6b58d64":"### Trying Learning Rates","68add8d7":"### Early Stopping"}}