{"cell_type":{"df67a092":"code","d6b9caf2":"code","2503b2d0":"code","d092bae6":"code","b82fd60a":"code","adfc3dde":"code","77dc5103":"code","7a01d10e":"code","581b25a8":"code","5e5ad360":"code","b8f02325":"code","207dd959":"code","568dcdf4":"code","f1deb11e":"code","af8efdd7":"code","5ecc3db8":"code","da21ab6c":"code","02d0a0c2":"code","9defa42c":"code","96cf5642":"code","11183715":"code","a12e79ae":"code","e57f612e":"code","c7e6f0cf":"code","91dc1455":"code","086c4be3":"code","3aea474d":"code","170c2d4c":"code","97ba9442":"code","abef44f8":"markdown","f1864ca3":"markdown","5a255a10":"markdown","02cba1ac":"markdown","5ba3b5cb":"markdown","4f6c441c":"markdown","205ab52a":"markdown","1fea7682":"markdown","669bf2fd":"markdown","6455ae6e":"markdown","eb5fde25":"markdown","6baf8d1c":"markdown","c611319e":"markdown","8fa9aedf":"markdown","f3f437bf":"markdown","20e4fb33":"markdown","04e1d964":"markdown","86dd7f1c":"markdown","7489176f":"markdown"},"source":{"df67a092":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import preprocessing\n\nfrom sklearn.preprocessing import MinMaxScaler, Normalizer, StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import KFold, train_test_split\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom scipy.stats import pearsonr\n\nfrom sklearn.linear_model import LinearRegression, SGDRegressor\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Boosting algorithm\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n","d6b9caf2":"# Explore the input dir\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2503b2d0":"train_data = pd.read_csv('\/kaggle\/input\/ccpp-data\/Train.csv', header=0)\ntest_data = pd.read_csv('\/kaggle\/input\/ccpp-data\/Test.csv', header=0)\nsubmit_data = pd.read_csv('\/kaggle\/input\/ccpp-data\/sample_submission.csv', header=0)","d092bae6":"# View basic details of the dataset\ndef view_details(dataframe):\n    print()\n    dataframe.info()\n    print(\"--\"*30)\n    print(dataframe.head())\n\n    \nview_details(train_data)\nview_details(test_data)\nview_details(submit_data)","b82fd60a":"# Statistic describtion of the dataset\ntrain_data.describe()","adfc3dde":"plt.figure(figsize=(12,6))\n\nsns.pairplot(train_data, diag_kind='hist')\n\nplt.tight_layout()\nplt.show()","77dc5103":"print(\"Is there a null value in Train data? {}\".format(train_data.isnull().sum().any()))\nprint(\"Is there a null value in Test data? {}\".format(test_data.isnull().sum().any()))","7a01d10e":"# Compute correlation between the variables\ncorr = train_data.corr()\n\n\nplt.figure(figsize=(10,5))\nsns.heatmap(corr, annot=True)\nplt.show()","581b25a8":"# Peason correlation calculation\nfor i in range(train_data.shape[1]-1):\n    feature = train_data.columns[i]\n    X = train_data[feature].values\n    y = train_data['PE']\n    r = np.round(pearsonr(X, y),2)\n    print(f\"Feature PE vs {feature}: r-value : {r}\")","5e5ad360":"train_data['train_or_test'] = 1\ntest_data['train_or_test'] = 0\n\n# Combining train and test data\ncombined_set = pd.concat([train_data, test_data])\n\nfeature_cols = ['AT', 'V', 'AP', 'RH']\npoly_cols = ['Coff','AT', 'V', 'AP', 'RH', 'AT2', 'AT*V', 'AT*AP', 'AT*RH', 'V2', 'V*AP', 'V*RH', 'AP2', 'AP*RH', 'RH2']\nX = combined_set[feature_cols]\n# Invoking polynomial feature transform method with degree of 2 for two variables\npoly_reg = PolynomialFeatures(degree=2, interaction_only=False)\nX_poly = poly_reg.fit_transform(X)\n\n# Converting array into a Dataframe\nX_poly = pd.DataFrame(X_poly, columns=poly_cols, index=combined_set.index)\n\nX_poly['train_or_test'] = combined_set['train_or_test']","b8f02325":"X_poly.head()","207dd959":"train_enc, test_enc = X_poly.loc[X_poly['train_or_test'] == 1], X_poly.loc[X_poly['train_or_test'] == 0]","568dcdf4":"X_train, X_valid, y_train, y_valid = train_test_split(train_enc, train_data['PE'], test_size=0.2, random_state=42)","f1deb11e":"# Random forest regressor model to fit the split data\nrnf = RandomForestRegressor(n_estimators=300)\n\nrnf.fit(X_train, y_train)","af8efdd7":"y_preds = rnf.predict(X_valid)\n\nprint(\"RMSE : {:.3f} \".format(mean_squared_error(y_valid, y_preds, squared=False)))\n# print(\"RMSLE : {np.sqrt(mean_squared_log_error(y_valid, y_preds))}\")","5ecc3db8":"X_train.columns","da21ab6c":"rnf.feature_importances_","02d0a0c2":"imp_features = list(zip(X_train.columns, rnf.feature_importances_))\nsorted(imp_features, key=lambda x: x[1], reverse=True)","9defa42c":"# Checking the target value skewness\ny_target = train_data['PE'].values\n\ny_data = y_target.reshape(len(y_target), 1)\n\n# QuantileTransformer to make bimodel to uniform\nquant_trans = QuantileTransformer(n_quantiles=300, output_distribution='uniform')\n\ny_trans = quant_trans.fit_transform(y_data)","96cf5642":"\nfig, axes = plt.subplots(1,2,figsize=(10,6))\n\nsns.distplot(y_target, kde=True, ax=axes[0])\nsns.distplot(y_trans, kde=True, ax=axes[1])\n\nplt.tight_layout()\nplt.show()","11183715":"imp_features = ['AT','V', 'AP','AT*V', 'AT*AP', 'AT*RH']\n# feature_cols = ['AT', 'AP', 'RH']\nX = train_enc[imp_features]\n# y = train_enc['y_trans']\ny = train_data['PE']\nX_test = test_enc[imp_features]","a12e79ae":"# StandardScaler to scale the variable values\nscaler = StandardScaler().fit(X)\n\nX_scaled = scaler.transform(X)\nX_test_scaled = scaler.transform(X_test)\n\nX_scaled_df = pd.DataFrame(X_scaled, columns=imp_features)","e57f612e":"lgb_params={\n    'learning_rate': 0.1,\n    'objective':'regression',\n    'importance_type':'split',\n    'metric':'rmse',\n    'num_leaves': 31,\n    \"random_state\":42,\n    'max_depth': 8,\n    \"bagging_seed\" : 42,\n    \"bagging_frequency\" : 5,\n    \"reg_alpha\":0.0,\n    \"reg_lambda\":0.0,\n    'min_child_samples': 20,\n    'min_child_weight':1\n}\n\nxgb_params = {\n    'objective': 'reg:squarederror',\n    'booster': 'gbtree',\n    'learning_rate': 0.1,\n    'max_depth': 8,\n    'min_child_weight': 1\n}","c7e6f0cf":"# Boosting algorithms\n\nlgbm = LGBMRegressor(**lgb_params, n_estimators=900)\n\nxgb = XGBRegressor(**xgb_params, n_estimators=700)","91dc1455":"K = 5\nkfolds = KFold(n_splits=K, shuffle=True, random_state=42)\n\nfor i, (train_idx, test_idx) in enumerate(kfolds.split(X_scaled_df)):\n    \n    y_train, y_valid = y.iloc[train_idx], y.iloc[test_idx]\n    X_train, X_valid = X_scaled_df.iloc[train_idx, :], X_scaled_df.iloc[test_idx, :]\n    \n    print( \"\\nFold \", i)\n    print(\"-\"* 20 + \"LGBM Regression\" + \"-\"* 20)\n    lgbm.fit(X_train, y_train, \n             eval_metric='rmse',\n             eval_set=[(X_valid,  y_valid)],\n            early_stopping_rounds=20,\n            verbose=100)\n    \n    print(\"-\"* 20 + \"XGB Regression\" + \"-\"* 20)\n    xgb.fit(X_train, y_train, \n             eval_metric='rmse',\n             eval_set=[(X_valid, y_valid)],\n            early_stopping_rounds=20,\n            verbose=100)","086c4be3":"X_test_df = pd.DataFrame(X_test_scaled, columns=imp_features)\n\n# Invoking model predict method\nlgb_preds = lgbm.predict(X_test_df)\nxgb_preds = xgb.predict(X_test_df)","3aea474d":"boost_1, boost_2 = 0.3, 0.7 \n\npreds_1 = boost_1 * lgb_preds\npreds_2 = boost_2 * xgb_preds\n\npredictions = preds_1 + preds_2","170c2d4c":"predictions","97ba9442":"final_df = pd.DataFrame(np.round(predictions, 6), columns=['PE'])\n\nfinal_df.to_csv('powerplant_linear_01.csv', index=False)","abef44f8":"## Feature Scaling","f1864ca3":"**Feature Selection using RandomForest Regressor**","5a255a10":"One of the primary observations on the dataset was that the dataset had only numerical variables and there are variables which directly involve in calculation of Energy output(PE). \n\nThe estimation of Electrical Energy output (PE) is dependent on all the given factors(variables). This involves certain scientific methods and formula to obtain PE value. That is one of the key aspects to be considered in the feature engineering. We should include the required feature, which can be derived from the existing features. \n\n### Learnings:\n1. We should check the domain objective of the Target variable data. In this case, PE can be calculated with the given feature data and derived data.\n2. Is there a way we can derive a new feature. For ex, Dew Point is one of the critical feature for estimating PE.\n3. Interactive features play a major role in reducing bias.\n4. Bimodel distribution of data can be transformed to uniform or normal distribution using Quantile transformation.\n\nThe final score of this submission are as follows, \n\nPrivate score : **2.45**\n\nPublic Score: **2.51**\n\n![13#MachineHack.png](attachment:13#MachineHack.png)","02cba1ac":"# Regression Model on Multicollinary Dataset\n\n## MachineHack Weekend Hackathon #13\n\n### Power Plant Energy Output Prediction\n\n### Overview\n\nThe dataset was collected from a Combined Cycle Power Plant over 6 years (2006-2011) when the power plant was set to work with a full load. Features consist of hourly average ambient variables *Temperature (T), Ambient Pressure (AP), Relative Humidity (RH)*, and *Exhaust Vacuum (V)* to predict the net hourly electrical energy output (PE) of the plant.\nA combined-cycle power plant (CCPP) is composed of gas turbines (GT), steam turbines (ST), and heat recovery steam generators.\n\nIn a CCPP, the electricity is generated by gas and steam turbines, which are combined in one cycle, and is transferred from one turbine to another. While the Vacuum is collected from and has an effect on the Steam Turbine, the other three of the ambient variables affect the GT performance.","5ba3b5cb":"## Target Variable Transformation\n\nThe target variable **PE** shows the Bimodel distribution. We need to transform this to uniform distribution to improve the model performance. ","4f6c441c":"## Conclusion","205ab52a":"## 3. Feature Engineering","1fea7682":"## 0. Import Libraries","669bf2fd":"**Interactive Features by Polynomial function**","6455ae6e":"**Weighted Average Ensemble**","eb5fde25":"## 2. Exploratory Data Analysis","6baf8d1c":"**Null or missing value check**","c611319e":"**Visualize the dataset in one-shot using Pair plot**","8fa9aedf":"## 1. Load dataset","f3f437bf":"## Model Building","20e4fb33":"**Correlation between attributes**","04e1d964":"**Split the data into Train and Validation set**","86dd7f1c":"## Final Submission","7489176f":"**Predicting Test Data**"}}