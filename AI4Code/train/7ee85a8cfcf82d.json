{"cell_type":{"7361770f":"code","b6d3a391":"code","f9c6c8dc":"code","44daf712":"code","916bd4e6":"code","8faabb35":"code","5d58a745":"code","0f6cf4a7":"code","7ab60336":"code","ec3641ec":"code","62a1f889":"code","38e64167":"code","37f10fc0":"code","d91a3390":"code","43c1e11d":"code","7b7c45f2":"code","42048ea0":"code","1d73c10b":"code","238e51ca":"code","cd1bb859":"code","860e2e69":"code","7c47e41c":"code","048a0cd9":"code","6daaf837":"markdown","1a7c3a72":"markdown","f5e13c38":"markdown","0b52f7b4":"markdown","8c313773":"markdown","a9ba9c65":"markdown","38a40819":"markdown","8b3b8db2":"markdown","72ed494a":"markdown","a93aad07":"markdown","2dac2bac":"markdown","5d3c6d6d":"markdown","abd27273":"markdown","6f899e43":"markdown","1fd85cd8":"markdown","c1010f4b":"markdown","ebf3215e":"markdown","1fc52b69":"markdown","99e060d0":"markdown","35366acc":"markdown","8cc177a3":"markdown","ed57820f":"markdown"},"source":{"7361770f":"class config():\n    CORPUS_FN = '\/kaggle\/input\/cord-19-step2-corpus\/corpus.pkl'\n    ENM_FN = '\/kaggle\/input\/cord-19-step3-enm\/ranker_enm.pickle'\n    TOC2_FN='\/kaggle\/input\/toc2js\/toc2.js'\n    \n    n_relevant = 150\n    rm1_lambda = 0.6\n    rm3_lambda = 0.7\n    \n    n_display = 15\n    \n    query_txt = 'Immune response and immunity' \n","b6d3a391":"import cord_19_container as container\nimport cord_19_rankers as rankers\nimport cord_19_lm as lm\nimport cord_19_vis as vis\n\nfrom cord_19_container import Sentence, Document, Paper, Corpus\n\nfrom cord_19_metrics import compute_queries_perf\n\nfrom cord_19_helpers import load, save\nfrom cord_19_text_cleaner import Cleaner\nfrom cord_19_wn_phrases import wn_phrases","f9c6c8dc":"from gensim import matutils\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport copy\nfrom collections import defaultdict\nimport re\n\nimport numpy as np\nimport pandas as pd","44daf712":"%matplotlib inline\n\nfrom IPython.display import display, HTML, Markdown, Latex\n\nimport wordcloud\n\nimport matplotlib.pyplot as plt\nimport bokeh\nimport holoviews as hv\n\nhv.extension('bokeh', logo=False)\nhv.output(size=260)\n\nHTML(\"\"\"\n<style>\n.output_png {\n    text-align: center;\n    vertical-align: middle;\n}\n\n.rendered_html table{\n    display: table;\n}\n<\/style>\n\"\"\")","916bd4e6":"corpus = load(config.CORPUS_FN)\ndictionary = corpus.dictionary\n\n# Rebuild id2token from token2id, only token2id is saved\nfor k,v in dictionary.token2id.items():\n    dictionary.id2token[v]=k\n\n# Set the dictionary as global, we have to find better way\ncontainer.dictionary = dictionary\nrankers.dictionary = dictionary\nvis.dictionary = dictionary\n\nprint(f'#Papers {len(corpus)}, #Tokens {len(dictionary)}')","8faabb35":"ranker_enm = load(config.ENM_FN)\nranker_nmf = ranker_enm.models['NMF']\nranker_ldi = ranker_enm.models['LDI']\n\nranker_ql = rankers.ranker_QL(corpus, config.rm1_lambda)","5d58a745":"query = container.Document([Cleaner(True).clean(config.query_txt)])\nquery.tokenize()\nwn_phrases(query)\n\ndisplay(HTML(f'We are searching for:<br><br>'))\nq_original_text = '<br>'.join([s.original_text for s in query.sentences])\ndisplay(HTML(f'<p style=\"font-size: 18pt;\">{q_original_text}<\/p>'))\n\n#When debuging print query.text","0f6cf4a7":"def plot_topics_dist(q):\n    score = ranker_nmf[q]\n    _, R = lm.get_relevant(corpus, score, config.n_relevant)\n    \n    fig_1, (ax1_nmf, ax1_ldi) = plt.subplots(1, 2, figsize=(14,6), sharey=True)\n    fig_2, (ax2_nmf, ax2_ldi) = plt.subplots(1, 2, figsize=(14,6), sharey=True)\n\n    vis.plot_topics_dist(ranker_nmf, R, ax1_nmf, ax2_nmf, \"NMF\", set_y_label=True)\n    fig_1.suptitle(f'Number of Documents by Dominant Topic.')\n\n    vis.plot_topics_dist(ranker_ldi, R, ax1_ldi, ax2_ldi, \"LDI\", set_y_label=False)\n    fig_2.suptitle(f'Mean topic probability over corpus.')\n\n    plt.show()\n    \nplot_topics_dist(query)","7ab60336":"# https:\/\/notes.mikejarrett.ca\/connecting-neighbourhoods\/\ndef rotate_label(plot, element):\n    text_cds = plot.handles['text_1_source']\n    length = len(text_cds.data['angle'])\n    text_cds.data['angle'] = [0]*length\n    xs = text_cds.data['x']\n    text = np.array(text_cds.data['text'])\n    xs[xs<0] -= np.array([len(t)*0.03 for t in text[xs<0]])\n\ndef display_coi(ranker, q, n_words=100):\n    \"\"\"Cooccurrences importance\n    \"\"\"\n    \n    score = ranker[q]\n    query_likelihood = ranker_ql[q]\n    \n    I, R = lm.get_relevant(corpus, score, config.n_relevant)\n    query_likelihood = query_likelihood[I]\n    query_likelihood = query_likelihood \/ sum(query_likelihood)\n    \n    rm1 = lm.compute_rm1(R.TRF, corpus.p_coll, query_likelihood, lambda_=config.rm1_lambda)\n    \n    # NOTE: Here we are using RM1 and not RM3, we haven't to emphasize query terms\n    lda_tm_rm = lm.compute_tm_rm(ranker_ldi, R, q, query_likelihood, rm1, lambda_=0.7)\n    nmf_tm_rm = lm.compute_tm_rm(ranker_nmf, R, q, query_likelihood, rm1, lambda_=0.7)\n    tm_rm = 0.7*nmf_tm_rm + 0.3*lda_tm_rm\n    \n    top_tokens = np.argsort(tm_rm)[::-1][:n_words]\n    top_tokens_set = set(top_tokens)\n    tokens_subset_id = list(range(len(top_tokens_set)))\n    \n    map_to_new = dict(zip(top_tokens_set, tokens_subset_id))\n    map_to_old = dict(zip(tokens_subset_id, top_tokens_set))\n\n    def to_new_id(X):\n        return [map_to_new[x] for x in X]\n    \n    def to_old_id(X):\n        return [map_to_old[x] for x in X]\n    \n    def to_word(X):\n        return [dictionary.id2token[x] for x in X]\n    \n    # Cooccurrences matrix\n    CoM = np.zeros([n_words, n_words])\n    \n    for paper in R:\n        for sent in paper:\n            sent_words = list(sent.tokensid_set.intersection(top_tokens_set))\n            words_p = [tm_rm[w] for w in sent_words]\n            sent_words = to_new_id(sent_words)\n            \n            pseudo_corpus = list(zip(sent_words, words_p))\n            vec = matutils.corpus2dense([pseudo_corpus], n_words)\n            \n            # [n_words,1]*[1,n_words]=[n_words,n_words]\n            CoM += vec@vec.T\n    \n    I,J = np.triu_indices(n_words, 1)\n    co_score = CoM[I,J]\n    \n    df = pd.DataFrame({'source':to_word(to_old_id(I)),\n                       'target':to_word(to_old_id(J)),\n                       'value':co_score})\n    df.sort_values(by=['value'], inplace=True, ascending=False)\n    \n    links = 0\n    words_set = set()\n    max_words = 20 # It can be 21 or 22\n    for row_i, row in df.iterrows():\n        words_set.update([row.source, row.target])\n        if len(words_set) < max_words:\n            links += 1\n        else:\n            break\n    \n    # print(f'#links {links}')\n    df = df.head(links)\n    \n    #display(df)\n    \n    chord = hv.Chord(df)\n    chord.opts(\n        hv.opts.Chord(cmap='Category20', edge_cmap='Category20', \n                      node_color='index', labels='index', edge_color='source',\n                      edge_line_width=3,\n                      hooks=[rotate_label])\n    )\n    \n    display(chord)\n","ec3641ec":"display_coi(ranker_enm, query)","62a1f889":"def display_wi(ranker, q):\n    \"\"\"\n    Word importance\n    \"\"\"\n\n    score = ranker[q]\n    query_likelihood = ranker_ql[q]\n    \n    I, R = lm.get_relevant(corpus, score, config.n_relevant)\n    query_likelihood = query_likelihood[I]\n    query_likelihood = query_likelihood \/ sum(query_likelihood)\n    \n    rm1 = lm.compute_rm1(R.TRF, corpus.p_coll, query_likelihood, lambda_=config.rm1_lambda)\n    \n    # NOTE: Here we are using RM1 and not RM3, we haven't to emphasize query terms\n    lda_tm_rm = lm.compute_tm_rm(ranker_ldi, R, q, query_likelihood, rm1, lambda_=0.7)\n    nmf_tm_rm = lm.compute_tm_rm(ranker_nmf, R, q, query_likelihood, rm1, lambda_=0.7)\n    tm_rm = 0.7*nmf_tm_rm + 0.3*lda_tm_rm\n    \n    freq = {}\n    for i,p in enumerate(tm_rm):\n        token = dictionary.id2token[i]\n        freq[token] = p\n    \n    tm_rm = np.sort(tm_rm)\n    \n    wc = wordcloud.WordCloud(width=800, height=400, max_words=100).generate_from_frequencies(freq)\n    plt.figure(figsize=[7,3], dpi=120)\n    plt.imshow(wc, interpolation='bilinear')\n    plt.title(f'Top100 Word\/Relevants probality [{tm_rm[-100]:.3f}, {tm_rm[-1]:.3f}]')\n    plt.axis(\"off\")\n    plt.tight_layout(pad=0)\n    plt.show()\n    ","38e64167":"display_wi(ranker_enm, query)","37f10fc0":"def display_results(ranker, q):\n    \n    scores = ranker[q]\n    \n    I, R = lm.get_relevant(corpus, scores, config.n_display)\n    \n    for i, paper in enumerate(R):\n        paper_id = I[i]\n        enm = scores[paper_id]\n        \n        table_html = \"<table>\"\n        \n        # Please add the link (Kaggle kernel) from where you got the 'doi link'\n        link='<a href=\"https:\/\/doi.org\/'+paper.doi+'\" target=blank>'+paper.title+'<\/a>'\n        table_html += f\"<tr><th style='text-align:left; width: 5%;'>Title:<\/th><td style='text-align:left;'><b>{link}<\/b><\/td><\/tr>\"\n        table_html += f\"<tr><th style='text-align:left;'>Score:<\/th><td style='text-align:left;'>{enm:.3f}<\/td><\/tr>\"\n        \n        sentences = [sent for sent in paper if len(sent.bow)>1]\n        \n        a = ranker_nmf.project(sentences)\n        b = ranker_nmf.project(q)\n        \n        sim = cosine_similarity(a, b)\n        sim = sim[:,0]\n        \n        for j,sent in enumerate(sentences):\n            jaccard_sim = 1-matutils.jaccard(q.bow, sent.bow)\n            sim[j] = (sim[j] + jaccard_sim)*0.5\n        \n        sI = np.argsort(sim)[::-1]\n        top5_tbl = \"<table>\"\n        top5_tbl += \"<tr><th>Score<\/th><th style='text-align:left;'>Sentence<\/th><\/tr>\"\n        for j in sI[:5]:\n            top5_tbl += f\"<tr><td>{sim[j]:.3f}<\/td><td style='text-align:left;'>{sentences[j].original_text}<\/td><\/tr>\"\n        top5_tbl += \"<\/table>\"\n        \n        table_html += \"<tr><td colspan='2' style='text-align:left;'><b>Top5 sentences:<\/b><\/td><\/tr>\"\n        table_html += \"<tr><td colspan='2'>\"+top5_tbl+\"<\/td><\/tr>\"\n        \n        table_html += \"<\/table>\"\n        \n        display(HTML(table_html))\n        #display(HTML('<hr>'))","d91a3390":"%%time\n\ndisplay_results(ranker_enm, query)","43c1e11d":"def get_top_terms(q, score, query_likelihood, lambda_ = 0.6, topk=20):\n    I, R = lm.get_relevant(corpus, score, config.n_relevant)\n    \n    query_likelihood = query_likelihood[I]\n    query_likelihood = query_likelihood \/ sum(query_likelihood)\n    \n    rm1 = lm.compute_rm1(R.TRF, corpus.p_coll,\n                         query_likelihood,\n                         lambda_=config.rm1_lambda)\n    \n    TF = R.TF.copy()\n    # Set to 0 words not in query\n    mask = np.isin(np.arange(len(corpus.dictionary)), list(q.tokensid_set), invert=True)\n    TF[:,mask] = 0\n    p_w_q = lm.mle(TF) # P_mle(w|Q)\n    rm3 = lm.compute_rm3(rm1, p_w_q, lambda_=config.rm3_lambda)\n\n    # Combine topics models with RM\n    lda_tm_rm = lm.compute_tm_rm(ranker_ldi, R, q, query_likelihood, rm3, lambda_=lambda_)\n    nmf_tm_rm = lm.compute_tm_rm(ranker_nmf, R, q, query_likelihood, rm3, lambda_=lambda_)\n    tm_rm = 0.7*nmf_tm_rm + 0.3*lda_tm_rm\n    \n    return np.argsort(tm_rm)[::-1][:topk]\n\ndef expand_query(q):\n    oq = copy.deepcopy(q)\n    best_q = oq\n    \n    n_expanded = 0\n\n    best_clarity_score = 0\n    max_iter = 3\n\n    while max_iter:\n        max_iter -= 1\n        expanded = False\n\n        q_score = ranker_enm[q]\n        query_likelihood = ranker_ql[q]\n        \n        prev_clarity_score = compute_queries_perf(corpus, q, q_score, query_likelihood,\n                                                  kind='uef', n_relevant=config.n_relevant,\n                                                  rm1_lambda=config.rm1_lambda, rm3_lambda=config.rm3_lambda).item()\n\n        best_clarity_score = prev_clarity_score\n        best_lambda = 0\n        best_topk = 0\n        min_k = len(q.text)+1\n        for topk in np.arange(min_k, 20):\n            for lambda_ in np.linspace(0.2,0.8,7):\n                top_terms = get_top_terms(q, q_score, query_likelihood, lambda_=lambda_, topk=topk)\n                new_q = defaultdict(int, q.bow)\n                for tokenid in top_terms:\n                    new_q[tokenid] += 1\n                new_q_bow = list(new_q.items())\n                new_q = copy.deepcopy(q)\n                new_q._bow = new_q_bow\n\n                new_q_score = ranker_enm[new_q]\n                new_query_likelihood = ranker_ql[new_q]\n\n                clarity_score = compute_queries_perf(corpus, new_q, new_q_score, query_likelihood,\n                                                     kind='uef', n_relevant=config.n_relevant,\n                                                     rm1_lambda=config.rm1_lambda,\n                                                     rm3_lambda=config.rm3_lambda).item()\n                if clarity_score>best_clarity_score:\n                    best_clarity_score = clarity_score\n                    best_q = new_q\n                    best_lambda = lambda_\n                    best_topk = topk\n                    break\n\n        if best_topk != 0:\n            q = best_q\n            delta = best_clarity_score - prev_clarity_score\n        else:\n            delta = 0\n            best_lambda = 0\n        print(f'CS: {best_clarity_score:.6f} \\u0394: +{delta:.6f} Lambda: {best_lambda:.2f}')\n\n        if best_topk == 0:\n            break\n                \n    return best_q","7b7c45f2":"%%time\n\nexpanded_query = expand_query(query)","42048ea0":"df_o = pd.DataFrame([(dictionary.id2token[k], v) for k,v in query.bow], columns=['word', 'original'])\ndf_e = pd.DataFrame([(dictionary.id2token[k], v) for k,v in expanded_query.bow], columns=['word', 'expanded'])\n\ndf = pd.merge(df_e, df_o, how='left', on='word')\ndf.fillna(0, inplace=True)\ndf.sort_values(by=['expanded', 'word'], ascending=[True, False], inplace=True)\ndf['expanded'] -= df['original']\n\ndf.plot.barh(x='word', y=['original', 'expanded'], colormap='bwr', stacked=True,\n             figsize=[12,8], rot=0, title=\"Expanded\" )\nplt.tight_layout(pad=0)\nplt.show()","1d73c10b":"def plot_topics_dist(q):\n    score = ranker_nmf[q]\n    _, R = lm.get_relevant(corpus, score, config.n_relevant)\n    \n    fig_1, (ax1_nmf, ax1_ldi) = plt.subplots(1, 2, figsize=(14,6), sharey=True)\n    fig_2, (ax2_nmf, ax2_ldi) = plt.subplots(1, 2, figsize=(14,6), sharey=True)\n\n    vis.plot_topics_dist(ranker_nmf, R, ax1_nmf, ax2_nmf, \"NMF\", set_y_label=True)\n    fig_1.suptitle(f'Number of Documents by Dominant Topic.')\n\n    vis.plot_topics_dist(ranker_ldi, R, ax1_ldi, ax2_ldi, \"LDI\", set_y_label=False)\n    fig_2.suptitle(f'Mean topic probability over corpus.')\n\n    plt.show()\n    \nplot_topics_dist(query)","238e51ca":"display_coi(ranker_enm, expanded_query)","cd1bb859":"display_wi(ranker_enm, expanded_query)","860e2e69":"display_results(ranker_enm, expanded_query)","7c47e41c":"from IPython.display import HTML\n\nwith open(config.TOC2_FN, 'r') as file:\n    js = file.read()\n\n    display(HTML('<script type=\"text\/Javascript\">'+js+'<\/script>'))\n    \n    del js","048a0cd9":"%%javascript\n\n\/\/ Autonumbering & Table of Contents\n\/\/ Using: https:\/\/github.com\/ipython-contrib\/jupyter_contrib_nbextensions\/tree\/master\/src\/jupyter_contrib_nbextensions\/nbextensions\/toc2\ntable_of_contents(default_cfg);","6daaf837":"# Technical","1a7c3a72":"**<center style=\"font-size: 16pt;\"><a href=\"https:\/\/www.kaggle.com\/atmarouane\/covid-19-search-engine-indexing-by-lda-enm\">Ensemble Model (EnM) for document retrieval results<\/a><\/center>**","f5e13c38":"<h1><span class=\"tocSkip\"><\/span>Table of Contents<\/h1>\n<div id=\"toc-wrapper\"><\/div>\n<div id=\"toc\"><\/div>","0b52f7b4":"## Cooccurrences importance","8c313773":"## Words importance","a9ba9c65":"Load the corpus, papers talking about COVID-19\/SARS-CoV-2, done in our previous kernel.","38a40819":"# Expanded query","8b3b8db2":"Loading our model.","72ed494a":"## Load data","a93aad07":"## Topics importance","2dac2bac":"# Original query","5d3c6d6d":"## Load model","abd27273":"## Libraries","6f899e43":"## Results","1fd85cd8":"## Cooccurrences importance","c1010f4b":"## Topics importance","ebf3215e":"## Configuration class\n\nWe set variables like from where we load, where to store and some parameters (Explained later).","1fc52b69":"### Commun libraries","99e060d0":"### Our libraries\n\nAll our libraries are made public under open source.","35366acc":"### Visualization libraries","8cc177a3":"## Results","ed57820f":"## Words importance"}}