{"cell_type":{"2855e4a4":"code","2f7a8b9d":"code","967dd063":"code","8bb8082d":"code","f460ef78":"code","261094e7":"code","5be76faa":"code","c2cb458e":"code","a12146dd":"code","78971733":"code","0aad1288":"code","e361ba93":"code","b3cdbd9d":"code","50f81b42":"code","b2acfff1":"code","24a7c8de":"code","776d0f77":"code","9586ef4e":"code","55a1aad7":"code","276ff9aa":"code","a3cd8ad7":"code","865957dc":"code","b2615868":"code","932aa4d1":"code","d5a2b3ae":"code","619c7dd3":"markdown","da945ac7":"markdown"},"source":{"2855e4a4":"import re\nimport librosa\nimport torch\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tqdm.notebook import tqdm","2f7a8b9d":"\n\n#### Specify duration of audio in samples ##########\nrequiredAudioLen = 160000 #220500\n####################################################\n\n############ Read the label file ###############\nlabelFile = pd.read_csv('..\/input\/iemocap-save\/dict.csv', index_col='Unnamed: 0')\n#################################################\n\n########### Create list of wav files ############\nfile1 = open(\"test.txt\",\"a\") \n#################################################\n\nlabels = []\naudioData = []\nfilenames = []","967dd063":"labelFile.shape","8bb8082d":"# labelFile[labelFile.FileName.str.contains('Ses\\d{2}\\w_impro\\d{1,3}_\\w\\d{2}', regex=True)]","f460ef78":"# labelFile = labelFile[labelFile.FileName.str.contains('impro')]","261094e7":"# labelFile.reset_index(inplace=True, drop=True)\n# labelFile.head()","5be76faa":"# labelFile['Class'].iloc[0]","c2cb458e":"############# Extract the audio and clip\/append it to suit the length #################\nshort_count = 0\nfor i in range(1,6):\n    path = f'..\/input\/iemocap-full\/IEMOCAP_full_release\/Session{i}\/sentences\/wav\/'\n    file_info = []\n    labels = []\n    audioData = []\n    file_num = 1 # take file counters for every session\n    for root, dirs, files in tqdm(os.walk(path)):\n  #     print(root)\n        for name in files:\n#             print(root+'\/'+name)\n#             if 'script' in name:\n#                 continue\n            if ('wav' in name):\n                one_sam = 16000*8\n                audio,sr = librosa.load(root+'\/'+name, sr=None)\n                n_samples = len(audio)\/\/one_sam\n                if len(audio)>=(n_samples*one_sam):\n                    extractedAudio = audio[:n_samples*one_sam]\n                else:\n                    short_count += 1\n                    extractedAudio = np.zeros(one_sam)\n                    extractedAudio[:len(audio)] = audio[:]\n              \n              ######## Extract the label ########\n                label = labelFile.loc[labelFile['FileName'] == name[:-4]]['Class']\n                label = label.iloc[0]\n                if(label == 'fru'):\n                    clss = 0\n                    filenames.append(root+'\/'+name)\n  #                 file1.write(root+'\/'+name+'\\n')\n                elif(label == 'neu'):\n                    clss = 1\n                    filenames.append(root+'\/'+name)\n  #                 file1.write(root+'\/'+name+'\\n')\n                elif(label == 'ang'):\n                    clss = 2\n                    filenames.append(root+'\/'+name)\n  #                 file1.write(root+'\/'+name+'\\n')\n                elif(label == 'sad'):\n                    clss = 3\n                    filenames.append(root+'\/'+name)\n  #                 file1.write(root+'\/'+name+'\\n')\n                elif(label == 'hap') or (label == 'exc'):\n                    clss = 4\n                    filenames.append(root+'\/'+name)\n  #                 file1.write(root+'\/'+name+'\\n') \n                extractedAudio = extractedAudio.reshape(-1, one_sam)\n                labels += [clss]*n_samples\n                audioData.append(extractedAudio)\n                file_info += [file_num]*n_samples\n                file_num += 1\n              #################################\n    audioData = np.array(audioData)\n    audioData = np.vstack(audioData)\n    np.save(f'.\/sess{i}Data.npy',audioData)\n    np.save(f'.\/sess{i}Labels.npy',np.array(labels))\n    np.save(f'.\/sess{i}info.npy', np.array(file_info))\n#     break\n\nres = dict(zip(filenames, labels))\n# np.save('sess1_dict.npy',res)","a12146dd":"short_count","78971733":"sr","0aad1288":"# sess = np.load('.\/sess1Data.npy')\n# lab = np.load('.\/sess1Labels.npy')\n# inf = np.load('.\/sess1info.npy')\n# sess.shape, lab.shape, inf.shape","e361ba93":"! du -sh .\/sess1Data.npy\n! du -sh .\/sess2Data.npy\n! du -sh .\/sess3Data.npy\n! du -sh .\/sess4Data.npy\n! du -sh .\/sess5Data.npy","b3cdbd9d":"# ! pip install soundfile git+git:\/\/github.com\/pytorch\/fairseq.git@b8ea8a9b72c82192da07e3377adf4ebbde16716d\n    \n# ! wget https:\/\/dl.fbaipublicfiles.com\/fairseq\/wav2vec\/wav2vec_large.pt","50f81b42":"# import torch\n# import fairseq","b2acfff1":"# cp_path = '.\/wav2vec_large.pt'\n# model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\n# model = model[0]\n# model.eval()\n# device = torch.device(\"cuda\")\n# model.to(device);","24a7c8de":"# z = model.feature_extractor(torch.cuda.FloatTensor(50,32000))\n# c = model.feature_aggregator(z)\n# temp = c.cpu().detach().numpy()","776d0f77":"# for i in range(1,6):\n#     sess_data = np.load(f'sess{i}Data.npy')\n#   # sess_label = np.load(f'sess{i}Labels.npy')\n#   # print(sess_data.shape, sess_label.shape)\n#     features_audio = []\n#     for j in tqdm(range(sess_data.shape[0])):\n#         wav_input_16khz = (np.expand_dims(sess_data[j],0)) #torch.randn(1,220500)\n#         z = model.feature_extractor(torch.cuda.FloatTensor(wav_input_16khz))\n# #         c = model.feature_aggregator(z)\n#         temp = z.cpu().detach().numpy().flatten()\n#         features_audio.append(temp)\n# #     os.remove(f'.\/sess{i}Data.npy')\n#     np.save(f'.\/sess{i}feat.npy',np.array(features_audio))\n# #   break","9586ef4e":"# ! du -sh .\/sess1feat.npy\n# ! du -sh .\/sess2feat.npy\n# ! du -sh .\/sess3feat.npy\n# ! du -sh .\/sess4feat.npy\n# ! du -sh .\/sess5feat.npy","55a1aad7":"# ! pip install -U transformers\n# ! pip install datasets\n","276ff9aa":"# from transformers import Wav2Vec2Model, Wav2Vec2Config\n# # Initializing a Wav2Vec2 facebook\/wav2vec2-base-960h style configuration\n# configuration = Wav2Vec2Config()\n# # Initializing a model from the facebook\/wav2vec2-base-960h style configuration\n# model = Wav2Vec2Model(configuration)\n# # Accessing the model configuration\n# configuration = model.config","a3cd8ad7":"# from transformers import Wav2Vec2Tokenizer, Wav2Vec2Model\n# from datasets import load_dataset\n# import soundfile as sf\n# tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook\/wav2vec2-base-960h\")\n# model = Wav2Vec2Model.from_pretrained(\"facebook\/wav2vec2-base-960h\")\n# device = torch.device(\"cuda\")\n# model.to(device)\n\n\n\n\n\n# # def map_to_array(batch):\n# #     speech, _ = sf.read(batch[\"file\"])\n# #     batch[\"speech\"] = speech\n# #     return batch\n# # ds = load_dataset(\"patrickvonplaten\/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n# # ds = ds.map(map_to_array)\n# # input_values = tokenizer(ds[\"speech\"][0], return_tensors=\"pt\").input_values  # Batch size 1\n# # hidden_states = model(input_values).last_hidden_state","865957dc":"# def get_batch(data, bs=32):\n#     length = len(data)\n#     num_batches = length\/\/bs\n#     print(f'Total batches: {num_batches+1}')\n#     for batch in range(num_batches+1):\n#         start = batch*bs\n#         end = start+bs\n        \n#         yield data[start:end]\n    ","b2615868":"# bs = 8\n# for i in range(1,6):\n#     sess_data = np.load(f'sess{i}Data.npy')\n#     print(f'sess{i}Data.npy', end='   ')\n#     features_audio = []\n#     for data in tqdm(get_batch(sess_data.tolist(),bs=bs)):\n# #         wav_input_16khz = data\n#         input_values = tokenizer(data, return_tensors=\"pt\").input_values\n# #         print(f'input_values.shape: {input_values.shape}')\n#         hidden_states = model(input_values.to(device)).last_hidden_state\n# #         print(f'hidden_states.shape:  {hidden_states.shape}')\n#         temp = hidden_states.cpu().detach().numpy().reshape(input_values.shape[0], -1)\n#         features_audio.append(temp)\n# #         break\n# #     os.remove(f'.\/sess{i}Data.npy')\n#     features_audio = np.array(features_audio)\n#     features_audio = np.vstack(features_audio)\n#     np.save(f'.\/sess{i}feats2.npy',(features_audio))\n#     print(f'features_audio.shape:  {features_audio.shape}')\n# #     break","932aa4d1":"# ! du -sh .\/sess1feats2.npy\n# ! du -sh .\/sess2feats2.npy\n# ! du -sh .\/sess3feats2.npy\n# ! du -sh .\/sess4feats2.npy\n# ! du -sh .\/sess5feats2.npy","d5a2b3ae":"# (input_values).shape, hidden_states.shape, temp.shape","619c7dd3":"## Fairseq Wav2vec","da945ac7":"## HF Wav2Vec2.0"}}