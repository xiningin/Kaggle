{"cell_type":{"bbd21373":"code","830a94cf":"code","fb7ca2a1":"code","029f3153":"code","4fccbedc":"code","b4e560b2":"code","5b653658":"code","77aea053":"code","4c014466":"code","2c4343c1":"code","a1acee00":"code","11f3f080":"code","52b803e5":"code","a3db96b0":"markdown"},"source":{"bbd21373":"import gc\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import TimeSeriesSplit\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.keras import backend as K\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\nprint('Running on TPU ', tpu.master())\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","830a94cf":"class GCF:\n    INPUT_ROOT = \"\/kaggle\/input\/ump-npy-dataset\/\"\n    #ADD_FEATURES = \"\/kaggle\/input\/ump-norm-lag-1-features\/lag_1_features_std_scaled.npy\"\n    ADD_FEATURES = \"\/kaggle\/input\/ump-agg-average-value-features\/agg_avg_features_std_scaled.npy\"\n    N_TRAIN = 700_000  # 705086\n    N_FOLDS = 5\n    SEED = 0\n    \n    N_EPOCHS = 1000\n    BATCH_SIZE = 4096\n    EARLY_STOPPING_PATIENCE = 10\n    EARLY_STOPPING_MIN_DELTA = 1e-3\n    ALL_TRAIN_ADD_EPOCH = 3","fb7ca2a1":"def seed_everything(seed=GCF.SEED):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)","029f3153":"%%time\n\nX = np.load(f\"{GCF.INPUT_ROOT}\/features_std_scaled.npy\")\ny = np.load(f\"{GCF.INPUT_ROOT}\/targets.npy\")\ntime_id = np.load(f\"{GCF.INPUT_ROOT}\/time_id.npy\")\n\naddf = np.load(GCF.ADD_FEATURES)\nX = np.hstack([X, addf])\n\ndel addf\ngc.collect()","4fccbedc":"# https:\/\/www.kaggle.com\/c\/ubiquant-market-prediction\/discussion\/302977\n\ndef correlationMetric(x, y, axis=-2):\n    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum \/ n\n    ymean = ysum \/ n\n    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov \/ tf.sqrt(xvar * yvar)\n    return tf.constant(1.0, dtype=x.dtype) - corr\n\ndef correlationLoss(x,y, axis=-2):\n    \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n    while trying to have the same mean and variance\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum \/ n\n    ymean = ysum \/ n\n    xsqsum = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    ysqsum = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov \/ tf.sqrt(xsqsum * ysqsum)\n    sqdif = tf.reduce_sum(tf.math.squared_difference(x, y), axis=axis) \/ n \/ tf.sqrt(ysqsum \/ n)\n    return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr + (0.01 * sqdif)) , dtype=tf.float32 )\n\n\n#\u3000https:\/\/www.kaggle.com\/c\/ubiquant-market-prediction\/discussion\/301987\ndef pearson_coef(data):\n    return data.corr()['target']['preds']\n\ndef comp_metric(time_id, y, pred):\n    return np.mean(\n        pd.DataFrame(np.stack([time_id, y, pred]).T, columns=['time_id', 'target', 'preds']\n    ).groupby('time_id').apply(pearson_coef))","b4e560b2":"# https:\/\/www.kaggle.com\/sishihara\/1dcnn-for-tabular-from-moa-2nd-place\ndef create_model():\n    model = keras.Sequential([\n        layers.Dense(4096\/\/4, activation='relu', input_shape=(300+300,)),\n        layers.Reshape((256\/\/4, 16)),\n        layers.Dropout(0.75),\n        layers.Conv1D(filters=16, kernel_size=5, strides=1, activation='relu'),\n        layers.MaxPooling1D(pool_size=2),\n        layers.Flatten(),\n        layers.Dense(16, activation='relu'),\n        layers.Dense(1, activation='linear'),\n    ])\n    \n    model.compile(\n        optimizer=tf.optimizers.Adam(1e-4),\n        #loss='mse',\n        loss=correlationLoss,\n        metrics=[keras.metrics.RootMeanSquaredError(), correlationMetric]\n    )\n    \n    return model","5b653658":"is_train = np.where((time_id <= 1066) & (time_id > 1015), True, False)\n\nis_test = time_id > 1066\n\nsum(is_train), sum(is_test)","77aea053":"seed_everything()\n\nwith strategy.scope():\n    model = create_model()\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=GCF.EARLY_STOPPING_PATIENCE,\n    min_delta=GCF.EARLY_STOPPING_MIN_DELTA,\n    restore_best_weights=True,\n)\nreduce_lr = ReduceLROnPlateau(\n                    monitor='val_loss',\n                    factor=0.5,\n                    patience=3,\n                    min_lr=1e-5,\n                    verbose=1\n)\n\nhistory = model.fit(\n    X[is_train, :], y[is_train],\n    validation_data=(X[is_test, :], y[is_test]),\n    batch_size=GCF.BATCH_SIZE,\n    epochs=GCF.N_EPOCHS,\n    callbacks=[early_stopping, reduce_lr],\n)\n\nmodel.save(f\"ump_1dcnn_holdout.h5\")","4c014466":"valid_pred = model.predict(X[is_test, :]).reshape(1, -1)[0]\n    \nrmse = mean_squared_error(y[is_test], valid_pred, squared=False)\nscore = comp_metric(time_id[is_test], y[is_test], valid_pred)\nprint(f'RMSR={rmse}, SCORE={score}')\n\npd.DataFrame(history.history)[['loss', 'val_loss']].plot()\nplt.title(\"loss\")\nplt.show()\n\npd.DataFrame(history.history)[['root_mean_squared_error', 'val_root_mean_squared_error']].plot()\nplt.title(\"rmse\")\nplt.show()\n\npd.DataFrame(history.history)[['correlationMetric', 'val_correlationMetric']].plot()\nplt.title(\"correlation\")\nplt.show()","2c4343c1":"run_epoch = len(history.history['loss'])\nbest_epoch = run_epoch - GCF.EARLY_STOPPING_PATIENCE\nprint(f\"best epoch is {best_epoch}\")","a1acee00":"seed_everything()\n\nwith strategy.scope():\n    model = create_model()\n\nreduce_lr = ReduceLROnPlateau(\n                    monitor='val_loss',\n                    factor=0.5,\n                    patience=3,\n                    min_lr=1e-5,\n                    verbose=1\n)\n\nhistory = model.fit(\n    X[is_train + is_test, :], y[is_train + is_test],\n    batch_size=GCF.BATCH_SIZE,\n    epochs=best_epoch + GCF.ALL_TRAIN_ADD_EPOCH,\n    callbacks=[reduce_lr],\n)\n\nmodel.save(f\"ump_1dcnn_all_train.h5\")","11f3f080":"pd.DataFrame(history.history)[['loss']].plot()\nplt.title(\"loss\")\nplt.show()\n\npd.DataFrame(history.history)[['root_mean_squared_error']].plot()\nplt.title(\"rmse\")\nplt.show()\n\npd.DataFrame(history.history)[['correlationMetric']].plot()\nplt.title(\"correlation\")\nplt.show()","52b803e5":"!ls ","a3db96b0":"# Train 1DCNN by using lag features\n\nThis code train lag features by 1DCNN.\n\nFor more information on the lag feature, please see [this discussion](https:\/\/www.kaggle.com\/c\/ubiquant-market-prediction\/discussion\/303147).\n\nTo save time and memory, I converted train.csv to a numpy array beforehand.\n- [original features](https:\/\/www.kaggle.com\/takamichitoda\/ump-npy-dataset)\n- [lag features](https:\/\/www.kaggle.com\/takamichitoda\/ump-norm-lag-1-features)\n- [agg past avg features](https:\/\/www.kaggle.com\/takamichitoda\/ump-agg-average-value-features)\n\nupdate\n- Version 9: TimeSeriesSplit, use past average value features\n- Version 11: hold out & train all, normalize lag features\n- Version 12: hold out & train all\n"}}