{"cell_type":{"da3b49ea":"code","a14eb5d0":"code","382773f0":"code","b92d48bb":"code","c5b9ec73":"code","39e82d20":"code","d8b87c8f":"code","3f10d29a":"code","59fb0f3a":"code","ffb60de1":"code","2b6a726d":"code","87ddf9bb":"code","e0cd2a05":"code","3c714d53":"code","4cb5cef8":"code","a8866b4c":"code","8c0ff517":"code","02d0a1a0":"code","ecc76c7f":"code","eca42a6a":"code","50ac9592":"code","1d593919":"code","43df80ba":"code","4144eba3":"code","ead93f95":"code","8d4fc6ff":"code","bd4e7429":"code","ba820fec":"code","70d381f5":"code","94cd08bd":"code","820a7a76":"code","b36a0c5c":"code","7986063b":"code","72d99555":"code","2fa2e785":"code","feb94a40":"code","e1978acb":"code","a127c288":"code","cb3f2181":"code","667b29d7":"code","8fc92318":"code","1185d217":"code","e9e92508":"code","9b787460":"code","cb568aa4":"code","3d5226b1":"code","09ae64ed":"code","85c46533":"code","81f55752":"code","496ef40d":"code","a2e3770d":"code","af6454be":"code","c11c1af0":"code","9d8db4f0":"code","fb49ac04":"code","1f66a808":"code","08005bf9":"code","36277fe5":"code","753f9b0a":"code","41fe4d59":"code","fcceb2b6":"code","481b999c":"code","04c51ba9":"code","178b5f1d":"code","d137d29f":"code","ef60b195":"code","d69f5c82":"code","50161087":"code","835b178d":"code","8c278f25":"code","96e8f8f9":"code","2c5da999":"code","f1567d67":"code","16fec128":"code","5dcf9e11":"code","bbb5cc3c":"code","374cb023":"code","1567c322":"code","dd5d7754":"markdown","9cc6bd1f":"markdown","252649fb":"markdown","5d0b290d":"markdown","0cea6c71":"markdown","8d47685f":"markdown","87969ffd":"markdown","d8961106":"markdown","200fdb2a":"markdown","6db59ce4":"markdown","7083cedb":"markdown","d6fb0944":"markdown","968cf96d":"markdown","f8f77c74":"markdown","07e31188":"markdown","54983236":"markdown","a55639eb":"markdown","52023cbb":"markdown","28f5f78b":"markdown","0d7dc205":"markdown","b787ef33":"markdown","61eb9d5c":"markdown","fc4f9afd":"markdown","3dbc0b7b":"markdown","7258d849":"markdown","876b60e4":"markdown","9ff32b23":"markdown","1982f403":"markdown","b9bafce6":"markdown","e64eaa3e":"markdown","5883cac4":"markdown","308e21d4":"markdown","a36b786f":"markdown","634ddc22":"markdown","8cba0c51":"markdown","5fde0bee":"markdown","e44abc1c":"markdown","151405ab":"markdown","96f23017":"markdown","f0c21cf6":"markdown","0bb21b09":"markdown","e2c61b96":"markdown","ea5b851d":"markdown","36877bb6":"markdown"},"source":{"da3b49ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, skew #for some statistics\nfrom scipy import stats\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a14eb5d0":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n# See the first few rows of train.csv\nprint(train.head())\n\n# Check out the shapes of train.csv and test.csv\nprint(train.shape)\nprint(test.shape)","382773f0":"## LotArea\n\nplt.subplots(figsize=(10,7))\nplt.scatter(train[\"LotArea\"], train['SalePrice'], s = 8, alpha = 0.5)\nplt.xlabel('LotArea')\nplt.ylabel('SalePrice')\nplt.title(\"SalePrice vs LotArea\")\nplt.show()","b92d48bb":"## GrLivArea\n\nplt.subplots(figsize=(12,9))\nplt.scatter(train[\"GrLivArea\"], train['SalePrice'], s = 8, alpha = 0.5)\nplt.xlim()\nplt.xlabel('GrLivArea')\nplt.ylabel('SalePrice')\nplt.title(\"SalePrice vs GrLivArea\")\nplt.show()","c5b9ec73":"## Remove the 2 outliers\n## Descending order of GrLivArea values\n#GrLivArea_threshold = train['GrLivArea'].sort_values(ascending = False).iloc[1]\n\n#train = train.drop(train[(train['GrLivArea'] >= GrLivArea_threshold)].index)\n\n## AFTER removing outliers - GrLivArea\n\n#plt.subplots(figsize=(12,9))\n#plt.scatter(train[\"GrLivArea\"], y, s = 8, alpha = 0.5)\n#plt.xlim()\n#plt.xlabel('GrLivArea')\n#plt.ylabel('SalePrice')\n#plt.title(\"SalePrice vs GrLivArea\")\n#plt.show()","39e82d20":"### YearBuilt\n\nplt.subplots(figsize=(12,9))\nsns.scatterplot(x = 'YearBuilt', y = 'SalePrice', data = train, s = 100, alpha = 0.7)\nplt.xlabel('YearBuilt')\nplt.ylabel('SalePrice')\nplt.title(\"SalePrice vs YearBuilt\")\nplt.show()","d8b87c8f":"### OverallCond\n## Categorical column - let's look at the possible values\n## According to the data_description.txt, OverallCond ranges from 1 to 10 - 1 being the worst score, and 10 the best.\n\ntrain['OverallCond'].value_counts()","3f10d29a":"### OverallCond\n## Categorical column\n\nplt.subplots(figsize=(12,9))\nsns.boxplot(x = 'OverallCond', y = 'SalePrice', data = train)\nplt.xlabel('OverallCond')\nplt.ylabel('SalePrice')\nplt.title(\"SalePrice vs OverallCond\")\nplt.show()","59fb0f3a":"### OverallQual\n## Categorical column - let's look at the possible values\n## According to the data_description.txt, OverallQual ranges from 1 to 10 - 1 being the worst score, and 10 the best.\n\ntrain['OverallQual'].value_counts()","ffb60de1":"### OverallQual\n## Categorical column\n\nplt.subplots(figsize=(12,9))\nsns.boxplot(x = 'OverallQual', y = 'SalePrice', data = train)\nplt.xlabel('OverallQual')\nplt.ylabel('SalePrice')\nplt.title(\"SalePrice vs OverallQual\")\nplt.show()","2b6a726d":"### HouseStyle\n## Categorical column - let's look at the possible values\n## According to the data_description.txt, HouseStyle is the style of dwelling\n\ntrain['HouseStyle'].value_counts()","87ddf9bb":"### HouseStyle\n## Categorical column\n\nplt.subplots(figsize=(12,9))\nsns.boxplot(x = 'HouseStyle', y = 'SalePrice', data = train)\nplt.xlabel('HouseStyle')\nplt.ylabel('SalePrice')\nplt.title(\"SalePrice vs HouseStyle\")\nplt.show()","e0cd2a05":"### BldgType\n## Categorical column\n\nplt.subplots(figsize=(12,9))\nsns.boxplot(x = 'BldgType', y = 'SalePrice', data = train)\nplt.xlabel('BldgType')\nplt.ylabel('SalePrice')\nplt.title(\"SalePrice vs BldgType\")\nplt.show()","3c714d53":"### Neighborhood\n## Categorical column\n\nplt.subplots(figsize=(25,9))\nsns.boxplot(x = 'Neighborhood', y = 'SalePrice', data = train)\nplt.xlabel('Neighborhood')\nplt.ylabel('SalePrice')\nplt.title(\"SalePrice vs Neighborhood\")\nplt.show()","4cb5cef8":"### MSZoning\n## Categorical column\n\nplt.subplots(figsize=(15,9))\nsns.boxplot(x = 'MSZoning', y = 'SalePrice', data = train)\nplt.xlabel('MSZoning')\nplt.ylabel('SalePrice')\nplt.title(\"SalePrice vs MSZoning\")\nplt.show()","a8866b4c":"### Summary Statistics\n\ntrain.info()","8c0ff517":"X = train.drop(['SalePrice'], axis = 1)\ny = train['SalePrice']\n\ncomb_df = pd.concat([X, test], axis = 0, sort = False )\ncomb_df.reset_index(inplace = True)\ncomb_df.drop(['index'], axis = 1, inplace = True)\ncomb_df.head()\ncomb_df.tail()","02d0a1a0":"# Shape of new dataframe (Without labels)\nprint(comb_df.shape)","ecc76c7f":"## Before Standardization\n\nsns.distplot(y , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(y)\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(y, plot=plt)\nplt.show()","eca42a6a":"# Log-transformation of SalePrice - y\n# Use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ny = np.log1p(y)\n\n# Check the new distribution \nsns.distplot(y , fit = norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(y)\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(y, plot=plt)\nplt.show()","50ac9592":"### Since the first column 'Id' is not important and useful in predicting the SalePrice, we will drop it from the dataset\ncomb_df = comb_df.drop(['Id'], axis = 1)\ncomb_df.head()","1d593919":"# nan_columns = list of columns with missing values\nnan_columns = []\n\nfor i in range(0, comb_df.shape[1]):\n    # Total number of non-null values in a column must be 2919\n    # Count the sum of the series from the value_counts() of each column in comb_df\n    col_value_counts = comb_df.iloc[:, i].value_counts()\n    value_counts_sum = sum(col_value_counts)\n    \n    if value_counts_sum != comb_df.shape[0]:\n        # Append column name to nan_columns\n        nan_columns.append(comb_df.columns[i])","43df80ba":"# Count the number of such columns \nprint('Number of columns with missing values is', len(nan_columns))","4144eba3":"temp = comb_df.isnull().sum().sort_values(ascending = False)\nprint(temp[temp > 0 ])\nprint(\"Number of columns with missing values is\", len(temp[temp > 0]))\n\n# Focus on columns with missing values\ntemp = temp[temp > 0]","ead93f95":"# For columns with less than 5 missing values\ntemp = temp[(temp > 0) & (temp < 5)]\n\n# Index of temp_col - Column names\ntemp_index = temp.index\n\n# Check out datatype of these columns first:\ncomb_df.loc[:, temp_index].info()","8d4fc6ff":"for col in comb_df.loc[:, temp_index].select_dtypes(include=np.number):\n    comb_df[col] = comb_df[col].fillna(comb_df[col].median())\n\n# Check that the missing values of the numeric columns have been filled up\ntemp = comb_df.isnull().sum().sort_values(ascending = False)\ntemp = temp[(temp > 0) & (temp < 5)]\nprint(temp)\n\n# Create a list containing the names of the columns:\ncol_name_nan = list(temp.index)\nprint(col_name_nan)","bd4e7429":"def replace_nan(df, col):\n    # The arguments are df and col, where\n    # df is the dataframe, and col (list) are the columns that we want to replace missing values from.\n    \n    for c in col:\n        # Calculate distribution of values in column 'c'\n        valueCounts_c = df[c].value_counts()\n        \n        # Obtain the value that appears the most\n        majorityVal_c = valueCounts_c.index[0]\n        \n        # Replace the missing value with the majority value\n        df[c].fillna(majorityVal_c, inplace = True)\n        \n    return df","ba820fec":"comb_df = replace_nan(comb_df, col_name_nan)\n\n# Check that the missing values of the non-numeric columns have been filled up (Columns have less than 5 missing values)\ntemp = comb_df.isnull().sum().sort_values(ascending = False)\ntemp = temp[(temp > 0)]\nprint(temp)\n\nprint(\"Number of columns with missing values is:\", len(temp[temp > 0]))","70d381f5":"comb_df.Alley.fillna('NA', inplace = True)\ncomb_df.BsmtQual.fillna('NA', inplace = True)\ncomb_df.BsmtCond.fillna('NA', inplace = True)\ncomb_df.BsmtExposure.fillna('NA', inplace = True)\ncomb_df.BsmtFinType1.fillna('NA', inplace = True)\ncomb_df.BsmtFinType2.fillna('NA', inplace = True)\ncomb_df.FireplaceQu.fillna('NA', inplace = True)\ncomb_df.GarageType.fillna('NA', inplace = True)\ncomb_df.GarageFinish.fillna('NA', inplace = True)\ncomb_df.GarageQual.fillna('NA', inplace = True)\ncomb_df.GarageCond.fillna('NA', inplace = True)\ncomb_df.PoolQC.fillna('NA', inplace = True)\ncomb_df.Fence.fillna('NA', inplace = True)\ncomb_df.MiscFeature.fillna('NA', inplace = True)\n\ncomb_df.MasVnrType.fillna('None', inplace = True)","94cd08bd":"temp = comb_df.isnull().sum().sort_values(ascending = False)\ntemp = temp[temp > 0]\n\n# Index of temp_col - Column names\ntemp_index = temp.index\n\n# Check out datatype of these columns first:\ncomb_df.loc[:, temp_index].info()","820a7a76":"# Replace missing values with zero for numeric columns\ncomb_df.LotFrontage.fillna(0, inplace = True)\ncomb_df.GarageYrBlt.fillna(0, inplace = True)\ncomb_df.MasVnrArea.fillna(0, inplace = True)\n\n# Check that the missing values of the numeric columns have been filled up\ntemp = comb_df.isnull().sum().sort_values(ascending = False)\ntemp = temp[(temp > 0)]\nprint(temp)\n\n# Create a list containing the names of the columns:\ncol_name_nan = list(temp.index)\nprint(col_name_nan)","b36a0c5c":"# def replace_nan_random(df, col):\n#     # df - dataframe\n#     # col - affected columns\n    \n#     for c in col:\n#         ### print('For column:', c)\n#         unique_values = list(df[c].value_counts().index)\n#         df[c] = df[c].astype('category')\n#         d = dict(enumerate(df[c].cat.categories))\n#         ### print(d)\n#         temp_df_nan = df[c].cat.codes.to_frame()   # with nan entries\n#         temp_df = temp_df_nan[(temp_df_nan > -1)]\n#         stats = temp_df.describe()\n        \n#         # New column\n#         new_col = []\n#         for i in range(0, len(temp_df_nan)):\n#             if (temp_df_nan.iloc[i][0] == -1): # if True --> entry is -1 (-1 instead of Nan)\n#                 # replace with a normal distribution based on values of the temporary 'temp_df'\n#                 random_replacement = np.int(np.random.normal(loc = stats.loc['mean'], scale = stats.loc['std']))\n                \n#                 while random_replacement < 0 or random_replacement > len(d) - 1: # if negative, continue while loop to generate new random value\n#                     random_replacement = np.int(np.random.normal(loc = stats.loc['mean'], scale = stats.loc['std']))\n#                 new_col.append(random_replacement)\n#             else:\n#                 new_col.append(temp_df_nan.iloc[i][0])\n\n#         # Map back to categorical values\n#         new_col = list(map(d.get, new_col))\n#         ### print(new_col)\n#         df[c] = new_col\n        \n#     return df","7986063b":"# comb_df = replace_nan_random(comb_df, col_name_nan)\n\n# # Check that the missing values of all of the columns have been filled up\n# temp = comb_df.isnull().sum().sort_values(ascending = False)\n# temp = temp[(temp > 0)]\n# print(temp)","72d99555":"comb_df.head()","2fa2e785":"comb_df.MSSubClass = comb_df.MSSubClass.map({20 : 'onestorey', \n                                             30 : 'onestorey', \n                                             40 : 'onestorey', \n                                             45 : 'onehalfstorey', \n                                             50 : 'onehalfstorey', \n                                             60 : 'twostorey', \n                                             70 : 'twostorey', \n                                             75 : 'twohalfstorey', \n                                             80 : 'split', \n                                             85 : 'split',\n                                             90 : 'duplex',\n                                             120 : 'onestorey', \n                                             150 : 'onehalfstorey',\n                                            160 : 'twostorey',\n                                            180 : 'split',\n                                            190 : 'duplex'})\n\n### Check the updated value_counts()\ncomb_df.MSSubClass.value_counts()","feb94a40":"## Create new feature - AgeSold\ncomb_df['AgeSold'] = comb_df.YrSold - comb_df.YearBuilt","e1978acb":"## Visualize the ages of houses when they were sold - allows us to decide how many bins to group these values\nsns.distplot(comb_df['AgeSold'], bins = 50)","a127c288":"### Split the AgeSold into 4 categories:\n### 0 <= AgeSold <= 7\n### 7 < AgeSold <= 35\n### 35 < AgeSold <= 54\n### AgeSold > 54\n\n### Sort the 'Age' into 6 different categories\ndataset = [comb_df]\nfor row in dataset:\n    row['AgeSold'] = row['AgeSold'].astype(int)\n    row.loc[(row['AgeSold'] <= 7), 'AgeSold'] = 0\n    row.loc[(row['AgeSold'] > 7) & (row['AgeSold'] <= 35), 'AgeSold'] = 1\n    row.loc[(row['AgeSold'] > 35) & (row['AgeSold'] <= 54), 'AgeSold'] = 2\n    row.loc[ row['AgeSold'] > 54, 'AgeSold'] = 3\n    \n### View value_counts() of AgeSold column\ncomb_df.AgeSold.value_counts()\n\n### View distribution of AgeSold again\n## If needed, re-split the AgeSold into appropriate bins, so that the distribution of data in each category is approximately the same\nsns.distplot(comb_df['AgeSold'])\n\n## Delete YrSold, MoSold, and YearBuilt from the dataset - Not needed anymore\ncomb_df.drop(['YrSold', 'MoSold', 'YearBuilt'], axis = 1, inplace = True)","cb3f2181":"### Create a map to map the neighborhoods to the locations in Ames City\nneighborhood_map = {'Blmngtn':'N', 'Blueste':'W', 'BrDale':'N', 'BrkSide':'C', 'ClearCr':'N', 'CollgCr':'C', 'Crawfor':'E', 'Edwards':'W', 'Gilbert':'N',\n                   'IODTRR':'E', 'MeadowV':'S', 'Mitchel':'S', 'Names':'N', 'NoRidge':'N', 'NPkVill':'N', 'NridgHt':'N', 'NWAmes':'C', 'OldTown':'E',\n                   'SWISU':'S', 'Sawyer':'W', 'SawyerW':'W', 'Somerst':'N', 'StoneBr':'N', 'Timber':'S', 'Veenker':'N'}\n\n### Map the values\ncomb_df.Neighborhood = comb_df.Neighborhood.map(neighborhood_map)\n\n### Check out the mapped values in the column\ncomb_df.Neighborhood.value_counts()","667b29d7":"# Get column names of numeric columns\nnumeric_feats = comb_df.dtypes[comb_df.dtypes != \"object\"].index\n\n# Check the skewness of all numerical features\nskewed_feats = comb_df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nprint(skewness.head(10))","8fc92318":"## Box Cox Transformation\n\nskewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlambd = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    comb_df[feat] = boxcox1p(comb_df[feat], lambd)\n\n# AFTER: check the skewness of all numerical features\nskewed_feats = comb_df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nprint(skewness.head(10))","1185d217":"### Columns with categorical values\ncomb_df.info()","e9e92508":"# Categorical columns\ncategorical_cols = comb_df.select_dtypes('object').columns\nprint(categorical_cols)\n      \n# Let's create another function perform Pandas dummy encoding on these categorical columns\ndef encode_cat(df, cols):\n    # df - target dataframe\n    # cols - columns that are categorical\n      \n    ## Steps to dummy encode:\n    # 1. create dummy_encode df \n    # 2. concat to main df\n    # 3. drop original cabin column\n    for c in cols:\n        df_dummy_c = pd.get_dummies(df[c], prefix = c)\n        df = pd.concat([df, df_dummy_c], axis = 1)\n        df.drop([c], axis = 1, inplace = True)\n    \n    # return dataframe\n    return df\n\n# Call the function encode_cat(df, cols) to create dummy columns\ncomb_df = encode_cat(comb_df, categorical_cols)\nprint(comb_df.shape)","9b787460":"print(comb_df.head())\nprint('No. of columns in final dataset is', comb_df.shape[1])","cb568aa4":"### Split comb_df back into train and test sets\n# Recall that the index of the train set ranges from 0 to 1459, while the index of the test set ranges from 1460 onwards.\n\n### Train set ###\ntrain = comb_df.iloc[0:1460,:]\nprint('Shape of train set is:', train.shape)\n\n### Test set ###\ntest = comb_df.iloc[1460:, :]\nprint('Shape of test set is:', test.shape)","3d5226b1":"# Use train_test_split to create train and test datasets\n# We will create 20% test data and 80% train data out of 'train'\n# train = features\/predictors\n# y = labels\n#from sklearn.model_selection import train_test_split\n#X_train, X_test, y_train, y_test = train_test_split(train, y, test_size = 0.2, random_state = 123)\n#print(X_train.shape)\n#print(X_test.shape)\n#print(y_train.shape)\n#print(y_test.shape)","09ae64ed":"# We will be importing the following libraries to implement the algorithms to predict housing prices.\nfrom sklearn.linear_model import ElasticNet, LinearRegression, Lasso, BayesianRidge, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import metrics\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","85c46533":"# We use k-fold cross validation\nk_folds = 10\n\n# We need to define a function that will calculate the RMSE between the true SalePrice and predicted SalePrice\ndef rmse_model(model):\n    kf_cv = KFold(k_folds, shuffle = True, random_state = 123).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model, train.values, y, scoring = \"neg_mean_squared_error\", cv = kf_cv))\n    return(rmse)","81f55752":"### We will implement a function to plot the graph for the RMSE for different cross-validated sets:\n\ndef eval_rmse(model, model_name, result_table):\n    \n    import time\n    start = time.time()\n    \n    model_rmse = rmse_model(model)\n    \n    end = time.time()\n    duration = (end - start) \/ 60\n    print('{} took {:.4f} minutes'.format(model, duration))\n    \n    # Plot the graph of RMSE for different trainings\n    plt.plot(model_rmse)\n    plt.xlabel('ith Iteration of K-Folds')\n    plt.ylabel('RMSE')\n    plt.title('RMSE for different iteraiton of K-folds')\n    plt.show()\n    \n    # Print out the Mean (Average) and Standard Deviation of the RMSE values\n    rmse_mean = model_rmse.mean()\n    rmse_std = model_rmse.std()\n    print('The mean RMSE is: {:.5f}'.format(rmse_mean))\n    print('The standard deviation of RMSE is: {:.5f}'.format(rmse_std))\n    \n    # Append the results to a table for consolidation\n    new_row = [model_name, rmse_mean, rmse_std]\n    result_table.loc[len(result_table)] = new_row\n    result_table.sort_values(by = ['Mean_RMSE'])\n    print(result_table)\n    \n    return None","496ef40d":"### Initialize the 'Result_Table' first\nresult_table = pd.DataFrame(columns = ['Model', 'Mean_RMSE', 'Std_RMSE'])","a2e3770d":"# Linear Regression\n# For a start, train a simple linear regression model\nlm = LinearRegression()\n\n# Evaluate the Linear Regression Model:\neval_rmse(lm, 'LinearRegression', result_table)","af6454be":"# Lasso Regression\n# Define the Lasso Regression model using make_pipeline to include RobustScaler()\n# Another way of implementing:\n# lasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds))\n# score = rmse_model(lasso)\n\nlasso = make_pipeline(RobustScaler(), Lasso(max_iter = 1e7, alpha = 0.001, random_state = 123))\n\n# Evaluate the Lasso Regression\neval_rmse(lasso, 'Lasso', result_table)","c11c1af0":"# KernelRidge\n# Define the model:\nkrr = KernelRidge(alpha = 0.6, kernel='polynomial', degree = 2, coef0 = 2.5)\n\n# Evaluate the model\neval_rmse(krr, 'KernelRidge', result_table)","9d8db4f0":"# Gradient Boosting\n# Define the model:\ngradboost = GradientBoostingRegressor(n_estimators = 3000,\n                                      learning_rate = 0.001,\n                                      max_depth = 4,\n                                      max_features = 'sqrt',\n                                      # min_samples_leaf = 15,\n                                      # min_samples_split = 10,\n                                      loss = 'huber',\n                                      random_state = 123,\n                                      criterion = 'friedman_mse')\n\n# Evaluate the model:\neval_rmse(gradboost, 'Gradient Boosting', result_table)","fb49ac04":"# Define the model\nelasticnet = make_pipeline(RobustScaler(), ElasticNet(max_iter=1e7, alpha = 0.0005, l1_ratio= 0.9, random_state = 123))                                \n\n# Evaluate the model\neval_rmse(elasticnet, 'Elastic Net', result_table)","1f66a808":"# XGBoost\n# Define the model\nxgb = XGBRegressor(colsample_bytree=0.2,\n                       gamma=0.0,\n                       learning_rate=0.01,\n                       max_depth=5, \n                       min_child_weight=1.5,\n                       n_estimators=4000,\n                       reg_alpha=0.9,\n                       reg_lambda=0.6,\n                       subsample=0.8,\n                       verbosity=0,\n                       random_state = 7,\n                       objective='reg:squarederror',\n                  n_jobs = -1)\n\n\n# Evaluate the model\neval_rmse(xgb, 'XGBoost', result_table)","08005bf9":"# LightGBM\n# Define the model\nlightGBM = lgb.LGBMRegressor(objective='regression',\n                             num_leaves=10,\n                             learning_rate=0.05,\n                             n_estimators=1000,\n                             max_bin = 55,\n                             bagging_fraction = 0.8,\n                             bagging_freq = 5,\n                             feature_fraction = 0.2319,\n                             feature_fraction_seed=9,\n                             bagging_seed=9,\n                             min_data_in_leaf =10,\n                             min_sum_hessian_in_leaf = 11)\n\n# Evaluate the model\neval_rmse(lightGBM, 'LightGBM', result_table)","36277fe5":"from sklearn.feature_selection import SelectFromModel\nfrom xgboost import plot_importance\nfrom numpy import sort\n\n# Define function to calculate RMSE:\ndef model_rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n# Use train_test_split to split train into X_train, Y_train, X_test, Y_test\nX_train, X_test, y_train, y_test = train_test_split(train, y, test_size = 0.2, random_state = 123)\n\n# Define XGB\nxgb = XGBRegressor(colsample_bytree=0.2,\n                       gamma=0.0,\n                       learning_rate=0.001,\n                       max_depth=5, \n                       min_child_weight=1.5,\n                       n_estimators=4000,\n                       reg_alpha=0.9,\n                       reg_lambda=0.6,\n                       subsample=0.5,\n                       verbosity=0,\n                       random_state = 123,\n                       objective='reg:squarederror',\n                  n_jobs = -1)\n\n# fit XGB on all training data - X_train\nxgb.fit(X_train, y_train)\n\n# Plot features according to their importance\n# Only the top 50 most important features will be plotted due to space constraints\nfig, ax = plt.subplots(figsize=(20, 15))\nplot_importance(xgb, max_num_features = 50, height = 0.8, ax = ax)\nplt.show()","753f9b0a":"# Fit xgb using each importance as a threshold\nthresholds = sort(xgb.feature_importances_)\n\n# Do a quick plot to see the feature importance\nplt.subplots(figsize=(10, 5))\nplt.scatter(x = range(0, len(thresholds)), y = thresholds, s= 1)\nplt.xlabel('n-th feature')\nplt.ylabel('Feature_Importance')\nplt.show()","41fe4d59":"# Store the threshold, no. of features, and corresponding RMSE for purpose of visualisation\nrmse_feat_importance = pd.DataFrame(columns = ['thresholds', 'no_features', 'threshold_rmse'])\n\nimport time\nstart = time.time()\n\n# For thresh values in interval of 5 units:\nfor i in range(0, len(thresholds)):\n    if i % 5 == 0: # multiples of 5\n        print('Current index is:', i)\n        \n        thresh = thresholds[i]\n        # For thresh values in interval of 5 units:\n        # select features using threshold\n        selection = SelectFromModel(xgb, threshold = thresh, prefit = True)\n        select_X_train = selection.transform(X_train)\n            \n        # define model\n        selection_model = XGBRegressor(colsample_bytree=0.2,\n                       gamma=0.0,\n                       learning_rate=0.01,\n                       max_depth=5, \n                       min_child_weight=1.5,\n                       n_estimators=4000,\n                       reg_alpha=0.9,\n                       reg_lambda=0.6,\n                       subsample=0.8,\n                       verbosity=0,\n                       random_state = 7,\n                       objective='reg:squarederror',\n                  n_jobs = -1)\n        \n        # train model\n        selection_model.fit(select_X_train, y_train)\n\n        # eval model\n        select_X_test = selection.transform(X_test)\n        y_pred = selection_model.predict(select_X_test)\n        selection_model_rmse = model_rmse(y_test, y_pred)\n        print(\"Thresh = {:.7f}, n = {}, RMSE = {:.5f}\".format(thresh, select_X_train.shape[1], selection_model_rmse))\n\n        # Append the results to a rmse_feat_importance for consolidation            \n        new_entry = [thresh, select_X_train.shape[1], selection_model_rmse]\n        rmse_feat_importance.loc[len(rmse_feat_importance)] = new_entry\n    else:\n        continue\n                \nend = time.time()\nprint('Time taken to run:', (end-start)\/60)\n# Show final 'rmse_feat_importance' table\nprint(rmse_feat_importance)","fcceb2b6":"# Plot a graph to see the performance of XGB for different number of features\nplt.subplots(figsize=(15, 10))\nplt.scatter(x = rmse_feat_importance['no_features'], y = rmse_feat_importance['threshold_rmse'], s = 5)\nplt.xlabel('No. of Features in XGB')\nplt.ylabel('RMSE - Performance of XGB')\nplt.show()","481b999c":"# From 35 onwards, we can pick the number of features that corresponds to the lower RMSE\nrow_min_rmse = rmse_feat_importance[rmse_feat_importance['threshold_rmse'] == rmse_feat_importance['threshold_rmse'].min()]\nprint(row_min_rmse)\n\n# Number of features for min rmse\nno_features_min_rmse = row_min_rmse['no_features'].values[0]\nprint(no_features_min_rmse)\n\n# Corresponding threshold\nthreshold_min_rmse = row_min_rmse['thresholds'].values[0]\nprint(threshold_min_rmse)","04c51ba9":"### Use KFolds CV to retrain with X features\n\n#We use k-fold cross validation\nk_folds = 10\n\n### Retrain the XGB model using X features only on FULL TRAINING DATA\n\n# Modify the function for calculating mean RMSE\ndef rmse_model_feat_impt(model):\n    kf_cv = KFold(k_folds, shuffle = True, random_state = 123).get_n_splits(select_train)\n    rmse = np.sqrt(-cross_val_score(model, select_train, y, scoring = \"neg_mean_squared_error\", cv = kf_cv))\n    return(rmse)\n\nimport time\nstart = time.time()\n\nselection = SelectFromModel(xgb, threshold = threshold_min_rmse, prefit = True)\nselect_train = selection.transform(train)\nselect_test = selection.transform(test)\n            \n# Define model\nselection_model = XGBRegressor(colsample_bytree=0.2,\n                       gamma=0.0,\n                       learning_rate=0.01,\n                       max_depth=5, \n                       min_child_weight=1.5,\n                       n_estimators=4000,\n                       reg_alpha=0.9,\n                       reg_lambda=0.6,\n                       subsample=0.8,\n                       verbosity=0,\n                       random_state = 7,\n                       objective='reg:squarederror',\n                  n_jobs = -1)\n\n# KFolds CV:\nCV_rmse = rmse_model_feat_impt(selection_model)\n\n# Print result\nprint('Mean RMSE of XGB training using {} features is {:.6f}'.format(no_features_min_rmse, CV_rmse.mean()))\n\nend = time.time()\nprint('Time taken to complete {:.2f} mins'.format((end-start)\/60))","178b5f1d":"# Define a function to calculate RMSE\ndef rmse(y_true, y_pred):\n    return np.sqrt(np.mean((y_true-y_pred)**2))\n\n# Define a function to calculate negative RMSE (as a score)\ndef nrmse(y_true, y_pred):\n    return -1.0*rmse(y_true, y_pred)\n\nfrom sklearn.metrics import make_scorer \nneg_rmse = make_scorer(nrmse)\n\nestimator = XGBRegressor(colsample_bytree=0.2,\n                       gamma=0.0,\n                       learning_rate=0.01,\n                       max_depth=5, \n                       min_child_weight=1.5,\n                       n_estimators=4000,\n                       reg_alpha=0.9,\n                       reg_lambda=0.6,\n                       subsample=0.8,\n                       verbosity=0,\n                       random_state = 7,\n                       objective='reg:squarederror',\n                  n_jobs = -1)\n\nfrom sklearn.feature_selection import RFECV\nimport time\nstart = time.time()\nselector = RFECV(estimator, cv = 5, n_jobs = -1, scoring = neg_rmse)\nselector = selector.fit(X_train, y_train)\nend = time.time()\nprint(\"Time taken for RFECV to complete:{} mins\".format((end-start)\/60))\nprint(\"The number of selected features is: {}\".format(selector.n_features_))\n\nfeatures_kept = X_train.columns.values[selector.support_] \n\n# Select selected features from X_train and X_test (This is for retraining the XGB using selected features and calculating the RMSE based on train_test_split datasets - this step might not be really needed)\nX_train = selector.transform(X_train)  \nX_test = selector.transform(X_test)\n\n# Select selected features from train and test to be used for Ensemble Learning\ntest = selector.transform(test)\ntrain = selector.transform(train)\n\n# transform the labels to a numpy array so later we can feed it to a neural network\ny_train = y_train.values \ny_test = y_test.values","d137d29f":"## Retrain XGB using selected features from RFECV\n\n# Modify the function for calculating mean RMSE\ndef rmse_model_feat_impt(model):\n    kf_cv = KFold(k_folds, shuffle = True, random_state = 123).get_n_splits(X_train)\n    rmse = np.sqrt(-cross_val_score(model, X_train, y_train, scoring = \"neg_mean_squared_error\", cv = kf_cv))\n    return(rmse)\n\nimport time\nstart = time.time()\n           \n# Define XGB model again\nRFECV_xgb_model = XGBRegressor(colsample_bytree=0.2,\n                       gamma=0.0,\n                       learning_rate=0.01,\n                       max_depth=5, \n                       min_child_weight=1.5,\n                       n_estimators=4000,\n                       reg_alpha=0.9,\n                       reg_lambda=0.6,\n                       subsample=0.8,\n                       verbosity=0,\n                       random_state = 7,\n                       objective='reg:squarederror',\n                  n_jobs = -1)\n\n# KFolds CV:\nRFECV_CV_rmse = rmse_model_feat_impt(RFECV_xgb_model)\n\n# Print result\nprint('Mean RMSE of XGB training using {} features is {:.6f}'.format(selector.n_features_, RFECV_CV_rmse.mean()))\n\nend = time.time()\nprint('Time taken to complete {:.2f} mins'.format((end-start)\/60))","ef60b195":"## Evaluate the RFECV_xgb_model on the test dataset from train_test_split\n\n# Fit on X_train, y_train\nRFECV_xgb_model.fit(X_train, y_train)\n\n# Predict with X_test\nRFECV_xgb_pred = RFECV_xgb_model.predict(X_test)\n\n# Evaluate RMSE score\nRFECV_xgb_rmse = rmse(y_test, RFECV_xgb_pred)\nprint(\"The RMSE for REFCV_xgb_model is: {:.5f}\".format(RFECV_xgb_rmse))","d69f5c82":"# ### Perform ensemble learning by using X features selected from Method 1: SelectFromModel\n# print('shape of full train set (features) after feature selection', select_train.shape)\n# print('shape of full test set (features) after feature selection', select_test.shape)\n# print('shape of y_train (labels)', y.shape)\n\n### Perform Ensemble Learning by using X features selected from Method 2: REFCV\nprint(\"Shape of full train set (features) after Method 2:\", train.shape)\nprint(\"Shape of full test set (features) after Method 2:\", test.shape)\nprint('shape of y_train (training labels)', y.shape)","50161087":"### Define StackingCVRegressor model\n\nfrom mlxtend.regressor import StackingCVRegressor\n\nstackingreg = StackingCVRegressor(\n    regressors=(krr, elasticnet, lightGBM),\n    meta_regressor= xgb,\n    use_features_in_secondary=True,\n    random_state = 123,\n    n_jobs = -1\n)","835b178d":"import time\nstart = time.time()\n\n# Begin fitting\nstack_model = stackingreg.fit(np.array(train), np.array(y))\n\n# krr_model = krr.fit(train, y)\n# elasticnet_model = elasticnet.fit(train, y)\n# lightGBM_model = lightGBM.fit(train, y)\n# xgb_model = xgb.fit(train, y)\n\nend = time.time()\nprint('The fitting is completed. Time taken is: {:.2f} minutes'.format((end-start)\/60))","8c278f25":"### Using stack_gen_model to predict using train set\nstack_train_pred = stack_model.predict(train)\n\n# Calculate RMSE on stacked models:\nstack_train_rmse = model_rmse(y, stack_train_pred)\nprint('RMSE of stacked models is: {:.5f}'.format(stack_train_rmse))","96e8f8f9":"## Define GridSearchCV for the stacked models\n\nstart = time.time()\n\nnp.random.seed(123)\n\ngrid_stacked_models = GridSearchCV(\n    estimator = stackingreg, \n    param_grid = {\n        #'kernelridge__alpha': [0.1, 0.5, 1.0],\n        #'pipeline-2__elasticnet__alpha': [0.001, 0.01, 0.1],\n        #'pipeline-1__lasso__alpha': [0.001, 0.01, 0.1],\n        'lgbmregressor__n_estimators': [500, 3000],\n        'meta_regressor__n_estimators': [500, 3000]\n    }, \n    cv=3,\n    refit=True\n)\n\ngrid_stacked_models.fit(np.array(train), y)\n\nend = time.time()\nprint('Time taken to run the GridSearchCV for the Stacked Models Ensemble is: {:.2f} minutes'.format((end-start)\/60))\n\nprint(\"Best: %f using %s\" % (grid_stacked_models.best_score_, grid_stacked_models.best_params_))","2c5da999":"### Using stack_gen_model to predict using train set\nstack_tuned_pred = grid_stacked_models.predict(train)\n\n# Calculate RMSE on stacked models:\nstack_tuned_rmse = model_rmse(y, stack_tuned_pred)\nprint('RMSE of stacked models is: {:.5f}'.format(stack_tuned_rmse))","f1567d67":"# Compute final predictions\nfinal_predictions = np.expm1(grid_stacked_models.predict(test))\n    \n# Store final_predictions in a csv file\nsaleprice_submission = pd.DataFrame(\n    {'Id': range(1461, 2920),\n    'SalePrice': final_predictions}\n)\n    \nprint('Submission:')\nprint(saleprice_submission.head())\n    \n# Save to CSV file\nsaleprice_submission.to_csv('submission.csv', index = False)","16fec128":"# import time\n# start = time.time()\n# grid_params_1 = {'learning_rate':[0.001, 0.01], 'n_estimators':[500, 400], 'max_depth': [2, 4, 6]}\n\n# GS_gradboost_1 = GridSearchCV(\n#     estimator = GradientBoostingRegressor(min_samples_split = 2, min_samples_leaf = 2, subsample = 1, max_features = 'auto', random_state = 123, verbose = True, loss = 'huber', criterion = 'friedman_mse'),\n#     param_grid = grid_params_1,\n#     scoring='neg_mean_squared_error',\n#     n_jobs=-1,\n#     iid=False,\n#     cv=5,\n#     verbose = 1\n#     )\n\n# #GS_gradboost_1.fit(train,y)","5dcf9e11":"# #GS_gradboost_1.grid_scores_,\n# bestparam = GS_gradboost_1.best_params_\n# bestscore = GS_gradboost_1.best_score_\n\n# print('bestscore is:', bestscore)\n# print('bestparam is:', bestparam)\n# end = time.time()\n# print('Time taken to run is:', end - start)","bbb5cc3c":"# ### Retrain model again with best parameters from GridSearchCV\n\n# final_gradboost = GradientBoostingRegressor(\n#     learning_rate = bestparam['learning_rate'],\n#     max_depth = bestparam['max_depth'],\n#     n_estimators = bestparam['n_estimators'],\n#     min_samples_split = 2,\n#     min_samples_leaf = 2,\n#     subsample = 1,\n#     max_features = 'auto',\n#     random_state = 123,\n#     verbose = True,\n#     loss = 'huber'\n#     )\n\n# final_gradboost.fit(train, y)","374cb023":"# # Churn out the predictions based on this final model\n# final_gb_pred = final_gradboost.predict(train)\n\n# # Calculate RMSE \n# final_gb_rmse = model_rmse(y, final_gb_pred)\n# print('RMSE of final GradBoost is: {:.5f}'.format(final_gb_rmse))","1567c322":"# # Decide to use tuned Gradient Boosting or the Stack Regressor\n# based on RMSE\n\n# if (stack_tuned_rmse < final_gb_rmse):\n#     Use Stack Regressor\n#     print('Stack Regressor performs better')\n#     Scale back the predicted values\n#     final_predictions = np.expm1(grid_stacked_models.predict(test.values))\n    \n#     Store final_predictions in a csv file\n#     saleprice_submission = pd.DataFrame(\n#     {'Id': range(1461, 2920),\n#     'SalePrice': final_predictions})\n    \n#     print('Submission:')\n#     print(saleprice_submission.head())\n    \n#     Save to CSV file\n#     saleprice_submission.to_csv('submission.csv', index = False)\n    \n# else:\n#     Use tuned Gradient Boosting\n#     print('Tuned Gradient Boosting performs better')\n#     Scale back the predicted values\n#     final_predictions = np.expm1(final_gradboost.predict(test))\n\n#     Store final_prediction in a csv file\n#     saleprice_submission = pd.DataFrame(\n#        {'Id': range(1461, 2920),\n#        'SalePrice': final_predictions})\n\n#     print('Submission:')\n#     print(saleprice_submission.head())\n\n#     Save to CSV file\n#     saleprice_submission.to_csv('submission.csv', index = False)","dd5d7754":"#### MSSubClass\n\nMSSubClass: Identifies the type of dwelling involved in the sale.\t\n\n        20\t1-STORY 1946 & NEWER ALL STYLES\n        30\t1-STORY 1945 & OLDER\n        40\t1-STORY W\/FINISHED ATTIC ALL AGES\n        45\t1-1\/2 STORY - UNFINISHED ALL AGES\n        50\t1-1\/2 STORY FINISHED ALL AGES\n        60\t2-STORY 1946 & NEWER\n        70\t2-STORY 1945 & OLDER\n        75\t2-1\/2 STORY ALL AGES\n        80\tSPLIT OR MULTI-LEVEL\n        85\tSPLIT FOYER\n        90\tDUPLEX - ALL STYLES AND AGES\n       120\t1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n       150\t1-1\/2 STORY PUD - ALL AGES\n       160\t2-STORY PUD - 1946 & NEWER\n       180\tPUD - MULTILEVEL - INCL SPLIT LEV\/FOYER\n       190\t2 FAMILY CONVERSION - ALL STYLES AND AGES\n\nGroup some of these SubClass together","9cc6bd1f":"### Number of Columns with Missing Values","252649fb":"Quite expected, newer houses are able to fetch better prices because of newer (and highly likely better) housing conditions. These conditions can be better cleanliness, utilities such as piping system and cables are still new, or even less accumulation of dirt and dust, etc.","5d0b290d":"### Replace missing values with Zeros\n\nSimilar to above, we replace missing values with zero values since these houses might not have the features in the first place.","0cea6c71":"### Some Data Visualization to gain a 'feel' of our data at hand\n\nGiven that there are many features in this dataset, we will only focus on a handful for EDA. These features are:\n1. YearBuilt\n2. OverallCond\n3. OverallQual\n4. HouseStyle\n5. BldgType\n6. Neighborhood\n7. MSZoning\n\nWe will make use of the Seaborn library for quick data visualization.","8d47685f":"From the above graph, we see that there are 2 prominent outliers located at the extreme right. The trend is also expected since the SalePrice of a house directly proportional to the amount of available living space.\n\nGiven that there are many features that we can plot against SalePrice in order to reveal outliers, we only restrict ourselves to 2. The concern with outlier is not too much since the regression algorithms, when coupled with RobustScaler(), are quite robust to outliers.","87969ffd":"### Skewed Features (?)\nWe will examine out of all the features, which are the highly skewed ones. We will perform Box Cox Transformation on these features.\n\nWe use the scipy function boxcox1p which computes the Box-Cox transformation of  1+x .\n\nNote that setting  \u03bb=0  is equivalent to log1p used above for the target variable.","d8961106":"### Feature Transformation\n\nLet's try to generate new features and reduce number of features","200fdb2a":"## Lasso Regression","6db59ce4":"### [Deprecated] - Filling up NaN entries with random values based on a normal distribution of the column\n\nFor the remaining columns, we will allocate random values to the missing values according to a normal distribution based on the column values. Similar to what we did above, we will define a new function.","7083cedb":"### Hyperparamter Tuning using GridSearchCV on StackingCVRegression\n\nWe can also implement GridSearchCV on StackingCVRegression by tuning the hyperparameters for the following models:\n1. KernelRidge\n2. ElasticNet\n3. XGBoost\n4. LightGBM\n\nFor Reference:\nstackingreg = StackingCVRegressor(\n    regressors=(krr, lasso, elasticnet, gradboost, xgb, lightGBM),\n    meta_regressor= xgb,\n    use_features_in_secondary=True,\n    random_state = 123\n)","d6fb0944":"Similarly, ased on the boxplot, there is a stronger linear trend that SalePrice increases with better OverallQual (Overall Quality as opposed to Overall Condition) of the house, which is expected. Note that there are numerous outliers for various OverallCond from 5 to 8, albeit fewer than that for OverallCond.","968cf96d":"**Method 2: Using Recursive Feature Elimination with Cross-Validation (RFECV)**\n\nUse sklearn.RFECV to remove redundant features and using cross-validation to select the best features.\nWe will see if there is any improvement in the RMSE score as compared to Method 1: SelectFromModel","f8f77c74":"### Hyperparameter Tuning using GridSearchCV for Gradient Boosting\n\nOut of the 7 models that we have trained, the best performing ones are Gradient Boosting and XGBoost.\nFor this section, we will perform hyperparameter tuning using GridSearch to try and optimize the models.","07e31188":"# 3. Training of ML Algorithms\nWe will use the train set to train the machine-learning regression algorithms. After training is done, we will feed the best-performing algorithm the test set to predict SalePrice for the test set.","54983236":"## KernelRidge Regression","a55639eb":"## ElasticNet","52023cbb":"Based on the above, there are 16 columns with less than 5 missing values each, while the rest have more than 20 missing values.\nFor columns with less than 5 missing values, we perform manual examination and replace the Nan entries with the median value of each column.\nFor now, we will drop columns with more than 1400 missing values since it would be hard to extrapolate.\nFor the remaining columns, we can randomly fill up the values using a random distribution.","28f5f78b":"We see from the summary statistics that there are some missing values from the columns in train.csv\nLet's see what are the columns with missing values e.g. NaN.\n\nBut first, let's concate the train.csv and test.csv together for feature engineering:","0d7dc205":"### Checking for Outliers\nBefore we continue to training the algorithms, we should check for any outliers in the train dataset.","b787ef33":"### Categorical Encoding\nNow that we have taken care of the missing and skewed values, we need to encode the categorical columns before we feed them into our machine learning algorithms.","61eb9d5c":"Let's call our function on comb_df:","fc4f9afd":"#### Neighborhood\n\nThe feature 'Neighborhood' contains the following categories that are physical locations within Ames city limits in Iowa.\nBy referencing the physical locations of these neighborhood, we shall group them into 5 major locations in the city, namely Central, North, South, East, and West.\n\n       Blmngtn\tBloomington Heights \n       Blueste\tBluestem\n       BrDale\tBriardale\n       BrkSide\tBrookside\n       ClearCr\tClear Creek\n       CollgCr\tCollege Creek\n       Crawfor\tCrawford\n       Edwards\tEdwards\n       Gilbert\tGilbert\n       IDOTRR\tIowa DOT and Rail Road\n       MeadowV\tMeadow Village\n       Mitchel\tMitchell\n       Names\tNorth Ames\n       NoRidge\tNorthridge\n       NPkVill\tNorthpark Villa\n       NridgHt\tNorthridge Heights\n       NWAmes\tNorthwest Ames\n       OldTown\tOld Town\n       SWISU\tSouth & West of Iowa State University\n       Sawyer\tSawyer\n       SawyerW\tSawyer West\n       Somerst\tSomerset\n       StoneBr\tStone Brook\n       Timber\tTimberland\n       Veenker\tVeenker","3dbc0b7b":"OR we could have simply done the following to see the breakdown:","7258d849":"#### Method 1: SelectFromModel\nLet's try to use SelectFromModel to select the number of features to consider based on the threshold level.\n1. The feature importance of each feature in the XGB are obtained from xgb.feature_importances_\n2. SelectFromModel will take a pre-trained model - e.g. a model that is trained on the entire training dataset - and then use a **Threshold** to decide which features to select\n3. Each feature has a relative importance as stored in xgb.feature_importances_. For a certain threshold value, there will be X number of features once a features' relative importance passes the threshold value.\n4. This threshold is used when the transform() method is called to select the same features on the training and test datatsets.\n\nIn the example below we first train and then evaluate an XGBoost model on the entire training dataset and test datasets respectively.\n\nUsing the feature importances calculated from the training dataset, we then wrap the model in a SelectFromModel instance. We use this to select features on the training dataset, train a model from the selected subset of features, then evaluate the model on the testset, subject to the same feature selection scheme.","876b60e4":"## Gradient Boosting","9ff32b23":"Based on the above graph, it seems that only by including a number of features that is fewer than 35 will result in poorer performance, in terms of RMSE. \n","1982f403":"The boxplot above reveals the disparity amongst neighborhoods, where some neighborhoods, judging by the selling prices of the houses, are more affluent than others. NoRidge, NridgHt, and StoneBr are the top 3 neighborhoods.","b9bafce6":"#### YearBuilt and YrSold\n\nWe will use the features YearBuilt and YrSold to determine the age of a house when it was sold - let's call this new feature AgeSold.","e64eaa3e":"### Filling NaN Entries of Numeric Columns with Median Values\nBased on the above, for numeric columns, we can easily perform the steps:","5883cac4":"## XGBoost","308e21d4":"## LightGBM","a36b786f":"### Replacing NaN Entries with Majority Value for Categorical Columns\n\nFor the remaining columns listed above, we will define a function that will replace the missing value with the majority value.\nAffected columns are: [MSZoning, Functional, Utilities, Exterior1st, KitchenQual, SaleType, Electrical, Exterior2nd]","634ddc22":"We will use KFolds CV to retrain the XGB with X number of features selected based on the threshold value. Observe if there is any improvement to the RMSE score.","8cba0c51":"# Part 2: Feature Engineering","5fde0bee":"## Linear Regression","e44abc1c":"From the above graph, in addition to seeing at least 4 outliers, there is an expected trend of SalePrice increasing with the LotArea of the houses.","151405ab":"## Ensemble Learning by Stacking Models\n\n> Ensemble learning is a technique that combines the skills of different algorithms in predicting for a given sample of data. A technique to combine the best of multiple algorithms which can give more stable predictions with very less variance than what we get with a single regressor.  The StackingCVRegressor is one such algorithm that allows us to collectively use multiple regressors to predict. -https:\/\/www.analyticsindiamag.com\/stackingcvregressor-in-python\/\n\nWe will be using StackingCVRegressor to stack the following models with relatively low RMSE:\n1. XGBoost\n2. LightGBM\n3. Elastic Net\n4. KernelRidge\n\nMore info on StackingCVRegressor: http:\/\/rasbt.github.io\/mlxtend\/user_guide\/regressor\/StackingCVRegressor\/","96f23017":"Based on the boxplot, there is a slight linear trend that SalePrice increases with better OverallCond (Overall Condition) of the house, which is expected. Note that there are numerous outliers for various OverallCond from 5 to 8.","f0c21cf6":"### Feature Importance and Selection on XGBoost\n\nBased on the mean RMSE from the 7 regressors, we can focus on the best performing algorithm - XGBoost.\nWe will use the XGBoost as an estimator to rank and select features according to their importance. As of now, we have 275 features\/columns to select from.","0bb21b09":"### Dealing missing values","e2c61b96":"### Replace missing values with NA for some of the columns\n\nFor the following columns, NA is one of the possible values. It likely means that a house did not contain the feature at all. Hence, we will fill these missing values with NA\n\n    Alley\n    BsmtQual\n    BsmtCond\n    BsmtExposure\n    BsmtFinType1\n    BsmtFinType2\n    FireplaceQu\n    GarageType\n    GaradeFinish\n    GarageQual\n    GarageCond\n    PoolQC\n    Fence\n    MiscFeature","ea5b851d":"### Log-transformation of SalePrice - Standardization","36877bb6":"# Part 1: Simple Exploratory Data Analysis"}}