{"cell_type":{"eb60e002":"code","c2efd19b":"code","5c0614ef":"code","46028835":"code","918d0639":"code","562fa6f2":"code","d70ff58a":"code","d774d9ba":"code","12a65991":"code","bb96e5e0":"code","12373be4":"code","9d0dcc13":"code","79982b8f":"code","2178f069":"code","06ea9d95":"code","b5b8b7c7":"code","9d5c3716":"code","ed940d8f":"code","750395e4":"code","8ff134a9":"code","c05b0bb2":"code","da21b4ae":"code","2296ed24":"code","5f9c9dad":"code","927ca254":"code","670a5e3d":"code","9d24933e":"code","42a4c436":"code","80e38cdd":"code","8de45534":"code","18b8d215":"code","75b6c493":"code","3ce17976":"code","21dab599":"code","6f8f61ab":"code","525c1a0e":"code","bd762e2b":"code","7e0e9487":"code","d1a7c5ad":"code","5cfd93eb":"code","956f21c9":"code","6c1365a4":"code","3f72c7ec":"code","b1c687e5":"code","9308f03a":"code","03a6501d":"code","c8bfdcbc":"code","5a95d6e2":"code","1728846b":"code","4c1bd7c8":"code","e199df21":"code","8bf58c58":"code","a8aaf6ca":"code","877a4956":"code","c73f215b":"code","9165fd78":"code","eb6dfd0c":"code","d67937a9":"code","731139b4":"markdown","56bdad56":"markdown","20156155":"markdown","1755fc46":"markdown","8d059bd9":"markdown","dbe5c275":"markdown","810b4dd2":"markdown","2fb2b583":"markdown","39de5adf":"markdown","8df86e10":"markdown","0e91bea1":"markdown","30015591":"markdown","850ce87a":"markdown","949b434e":"markdown","59e827b1":"markdown","dee5158b":"markdown","5f51273c":"markdown","52c9804a":"markdown","bbf58658":"markdown","9a4aebd2":"markdown","33728c5e":"markdown","42552d83":"markdown","ac909df6":"markdown","3bee3c43":"markdown","e9a00e7f":"markdown","7cdc0347":"markdown"},"source":{"eb60e002":"# Importing the pandas library\nimport pandas as pd\n\n# Reading the file and showing top 5 records\nbike_sharing = pd.read_csv('..\/input\/bike-sharing-mlr-ds\/day.csv')\nbike_sharing.head()","c2efd19b":"# Optionally checking the rows and columns\nbike_sharing.shape","5c0614ef":"# Checking the datatypes and number of records\nbike_sharing.info()","46028835":"# Checking whether there are any missing values\nbike_sharing.isnull().sum()","918d0639":"# Imputing the values of 'season' as categorical variable\nbike_sharing_modified = bike_sharing.copy()\nbike_sharing_modified.season = bike_sharing_modified.season.map({1:'spring', 2:'summer', 3:'fall', 4:'winter'})\nbike_sharing_modified.head()","562fa6f2":"# Imputing the values of 'weathersit' as categorical variable\nbike_sharing_modified.weathersit = bike_sharing_modified.weathersit.map({1:'Clear', 2:'Mist', 3:'Light Snow', 4:'Heavy Rain'})\nbike_sharing_modified.head()","d70ff58a":"# Imputing the values of 'weekday' as categorical variable\nweek_days = {1:'mon', 2:'tue', 3:'wed', 4:'thu', 5:'fri', 6:'sat', 0:'sun'}\nbike_sharing_modified.weekday = bike_sharing_modified.weekday.map(week_days)\nbike_sharing_modified.head()","d774d9ba":"# Imputing the values of 'mnth' as categorical variable\nmonth = {1:'jan', 2:'feb', 3:'mar', 4:'apr', 5:'may', 6:'jun', 7:'jul', 8:'aug', 9:'sep', 10:'oct', 11:'nov', 12:'dec'}\nbike_sharing_modified.mnth = bike_sharing_modified.mnth.map(month)\nbike_sharing_modified.head()","12a65991":"# Dropping the 'dteday' & 'instant' column\nbike_sharing_modified.drop(\"dteday\", axis=1, inplace=True)\nbike_sharing_modified.drop('instant', axis=1, inplace=True)\nbike_sharing_modified.head()","bb96e5e0":"# Checking the continuous variables\nbike_sharing_modified.describe()","12373be4":"# Visualizing the continous variables - Part I\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.pairplot(bike_sharing_modified, vars=[\"cnt\", \"temp\", \"atemp\", \"hum\", \"windspeed\"])\nplt.show()","9d0dcc13":"# Visualizing the continous variables - Part II\nsns.pairplot(bike_sharing_modified, vars=[\"cnt\", \"casual\", \"registered\"])\nplt.show()","79982b8f":"# Visualizing the categorical variables - Part I\nplt.figure(figsize=(15, 8))\n\nplt.subplot(2, 2, 1)\nsns.boxplot(x=bike_sharing_modified.season, y=bike_sharing_modified.cnt, data=bike_sharing_modified)\n\nplt.subplot(2, 2, 2)\nsns.boxplot(x=bike_sharing_modified.season, y=bike_sharing_modified.cnt, hue=bike_sharing_modified.yr, data=bike_sharing_modified)\n\nplt.subplot(2, 2, 3)\nsns.boxplot(x=bike_sharing_modified.weathersit, y=bike_sharing_modified.cnt, data=bike_sharing_modified)\n\nplt.subplot(2, 2, 4)\nsns.boxplot(x=bike_sharing_modified.weathersit, y=bike_sharing_modified.cnt, hue=bike_sharing_modified.yr, data=bike_sharing_modified)\n\nplt.show()","2178f069":"# Visualizing the categorical variables - Part II\nplt.figure(figsize=(15, 10))\n\nplt.subplot(3, 2, 1)\nsns.boxplot(x=bike_sharing_modified.holiday, y=bike_sharing_modified.cnt, data=bike_sharing_modified)\n\nplt.subplot(3, 2, 2)\nsns.boxplot(x=bike_sharing_modified.holiday, y=bike_sharing_modified.cnt, hue=bike_sharing_modified.yr, data=bike_sharing_modified)\n\nplt.subplot(3, 2, 3)\nsns.boxplot(x=bike_sharing_modified.weekday, y=bike_sharing_modified.cnt, data=bike_sharing_modified)\n\nplt.subplot(3, 2, 4)\nsns.boxplot(x=bike_sharing_modified.weekday, y=bike_sharing_modified.cnt, hue=bike_sharing_modified.yr, data=bike_sharing_modified)\n\nplt.subplot(3, 2, 5)\nsns.boxplot(x=bike_sharing_modified.workingday, y=bike_sharing_modified.cnt, data=bike_sharing_modified)\n\nplt.subplot(3, 2, 6)\nsns.boxplot(x=bike_sharing_modified.workingday, y=bike_sharing_modified.cnt, hue=bike_sharing_modified.yr, data=bike_sharing_modified)\n\nplt.show()","06ea9d95":"# Visualizing the categorical variables - Part III\nplt.figure(figsize=(15, 8))\n\nplt.subplot(2, 2, 1)\nsns.boxplot(x=bike_sharing_modified.mnth, y=bike_sharing_modified.cnt, data=bike_sharing_modified)\n\nplt.subplot(2, 2, 2)\nsns.boxplot(x=bike_sharing_modified.mnth, y=bike_sharing_modified.cnt, hue=bike_sharing_modified.yr, data=bike_sharing_modified)\n\nplt.subplot(2, 2, 3)\nsns.boxplot(x=bike_sharing_modified.yr, y=bike_sharing_modified.cnt, data=bike_sharing_modified)\n\nplt.show()","b5b8b7c7":"# Imputing season as categorical 'season' values\nseason = pd.get_dummies(bike_sharing_modified.season)\nseason.drop('summer', axis=1, inplace=True)\nseason.head()","9d5c3716":"# Removing the 'season' column\nbike_sharing_modified = pd.concat([bike_sharing_modified, season], axis=1)\nbike_sharing_modified.drop('season', axis=1, inplace=True)\nbike_sharing_modified.head()","ed940d8f":"# Imputing season as categorical 'mnth' values\nmonths = pd.get_dummies(bike_sharing_modified.mnth)\nmonths.drop('jan', axis=1, inplace=True)\nmonths.head()","750395e4":"# Removing the 'mnth' column\nbike_sharing_modified = pd.concat([bike_sharing_modified, months], axis=1)\nbike_sharing_modified.drop('mnth', axis=1, inplace=True)\nbike_sharing_modified.head()","8ff134a9":"# Imputing season as categorical 'weekday' values\nweekdays = pd.get_dummies(bike_sharing_modified.weekday)\nweekdays.drop('mon', axis=1, inplace=True)\nweekdays.head()","c05b0bb2":"# Removing the 'weekday' column\nbike_sharing_modified = pd.concat([bike_sharing_modified, weekdays], axis=1)\nbike_sharing_modified.drop('weekday', axis=1, inplace=True)\nbike_sharing_modified.head()","da21b4ae":"# Imputing season as categorical 'weathersit' values\nweathers = pd.get_dummies(bike_sharing_modified.weathersit)\nweathers.drop('Mist', axis=1, inplace=True)\nweathers.head()","2296ed24":"# Removing the 'weathersit' column\nbike_sharing_modified = pd.concat([bike_sharing_modified, weathers], axis=1)\nbike_sharing_modified.drop('weathersit', axis=1, inplace=True)\nbike_sharing_modified.head()","5f9c9dad":"# Dropping the irrelevant columns\nbike_sharing_modified.drop(['registered', 'casual'], axis=1, inplace=True)","927ca254":"# Importing the scikit learn library\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n\n# Splitting the data\ndf_train, df_test = train_test_split(bike_sharing_modified, train_size=0.7, test_size=0.3, random_state=100)\nprint(\"Train Dataset Size: \", df_train.shape)\nprint(\"Test Dataset Size: \", df_test.shape)","670a5e3d":"# Listing out the columns\nbike_sharing_modified.columns","9d24933e":"# Importing 'warnings' library\nimport warnings as warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Importing the MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Instantiating an object\nscaler = MinMaxScaler()\n\n# Creating a list of numeric vars\nnum_vars = ['temp', 'atemp', 'hum', 'windspeed', 'cnt']\n\n# Fitting on object\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])\ndf_train.head()","42a4c436":"# Checking various parameters of continuous variables\ndf_train.describe()","80e38cdd":"# Checking the heatmap of correlation\nplt.figure(figsize=(35, 25))\nsns.heatmap(df_train.corr(), annot=True, cmap='YlGnBu')\nplt.show()","8de45534":"# Selecting X_train, y_train without 'cnt'\ny_train = df_train.pop('cnt')\nX_train = df_train","18b8d215":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","75b6c493":"# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\n# Running RFE\nrfe = RFE(lm, 15) \nrfe = rfe.fit(X_train, y_train)","3ce17976":"# Listing the columns with ranking\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","21dab599":"# Columns to be considered\ncol = X_train.columns[rfe.support_]\ncol","6f8f61ab":"# Columns not to be considered\nX_train.columns[~rfe.support_]","525c1a0e":"# Importing the statsmodels library\nimport statsmodels.api as sm","bd762e2b":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col]","7e0e9487":"# Building a model with X_test dataframe with RFE selected variables\n\n# Adding a constant\nX_train_sm = sm.add_constant(X_train_rfe)\n\n# Creating the first model\nlr = sm.OLS(y_train, X_train_sm)\n\n# Fitting the model\nlr_model = lr.fit()\n\n# Checking the summary\nlr_model.summary()","d1a7c5ad":"# Building a model with X_test dataframe with RFE selected variables\n# And without 'may'\nX_train_rfe.drop([\"may\"], axis = 1, inplace=True)\n\n# Adding a constant\nX_train_sm = sm.add_constant(X_train_rfe)\n\n# Creating the subsequent model\nlr = sm.OLS(y_train, X_train_sm)\n\n# Fitting the model\nlr_model = lr.fit()\n\n# Checking the summary\nlr_model.summary()","5cfd93eb":"# Importing the library to calculate the VIFi\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","956f21c9":"# Calculating the VIF\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]\nvif.VIF = round(vif.VIF, 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","6c1365a4":"# Building a model with X_test dataframe with RFE selected variables\n# And without 'winter'\nX_train_rfe.drop([\"winter\"], axis = 1, inplace=True)\n\n# Adding a constant\nX_train_sm = sm.add_constant(X_train_rfe)\n\n# Creating the subsequent model\nlr = sm.OLS(y_train, X_train_sm)\n\n# Fitting the model\nlr_model = lr.fit()\n\n# Checking the summary\nlr_model.summary()","3f72c7ec":"# Building a model with X_test dataframe with RFE selected variables\n# And without 'mar'\nX_train_rfe.drop([\"mar\"], axis = 1, inplace=True)\n\n# Adding a constant\nX_train_sm = sm.add_constant(X_train_rfe)\n\n# Creating the subsequent model\nlr = sm.OLS(y_train, X_train_sm)\n\n# Fitting the model\nlr_model = lr.fit()\n\n# Checking the summary\nlr_model.summary()","b1c687e5":"# Calculating the VIF\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]\nvif.VIF = round(vif.VIF, 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","9308f03a":"# Building a model with X_test dataframe with RFE selected variables\n# And without 'hum'\nX_train_rfe.drop([\"hum\"], axis = 1, inplace=True)\n\n# Adding a constant\nX_train_sm = sm.add_constant(X_train_rfe)\n\n# Creating the subsequent model\nlr = sm.OLS(y_train, X_train_sm)\n\n# Fitting the model\nlr_model = lr.fit()\n\n# Checking the summary\nlr_model.summary()","03a6501d":"# Calculating the VIF\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]\nvif.VIF = round(vif.VIF, 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","c8bfdcbc":"# Building a model with X_test dataframe with RFE selected variables\n# And without 'fall'\nX_train_rfe.drop([\"fall\"], axis = 1, inplace=True)\n\n# Adding a constant\nX_train_sm = sm.add_constant(X_train_rfe)\n\n# Creating the subsequent model\nlr = sm.OLS(y_train, X_train_sm)\n\n# Fitting the model\nlr_model = lr.fit()\n\n# Checking the summary\nlr_model.summary()","5a95d6e2":"# Calculating the VIF\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]\nvif.VIF = round(vif.VIF, 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","1728846b":"# Building a model with X_test dataframe with RFE selected variables\n# And without 'aug'\nX_train_rfe.drop([\"aug\"], axis = 1, inplace=True)\n\n# Adding a constant\nX_train_sm = sm.add_constant(X_train_rfe)\n\n# Creating the subsequent model\nlr = sm.OLS(y_train, X_train_sm)\n\n# Fitting the model\nlr_model = lr.fit()\n\n# Checking the summary\nlr_model.summary()","4c1bd7c8":"# Calculating the VIF\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]\nvif.VIF = round(vif.VIF, 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","e199df21":"# Calculating the predicted values \ny_train_pred = lr_model.predict(X_train_sm)\n\n# Calculating the residual\nres = y_train - y_train_pred\n\n# Plotting the graph\nfig = plt.figure()\nsns.distplot(res, bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label\nplt.show()","8bf58c58":"# Fitting on data\ndf_test[num_vars] = scaler.transform(df_test[num_vars])\ndf_test.head()","a8aaf6ca":"# Checking various parameters of continuous variables\ndf_test.describe()","877a4956":"# Preparing the test data\ny_test = df_test.pop('cnt')\nX_test = df_test","c73f215b":"# Adding a constant\nX_test_sm = sm.add_constant(X_test)\nX_test_sm.head()","9165fd78":"# Predicting\ny_test_pred = lr_model.predict(X_test_sm[X_train_sm.columns])","eb6dfd0c":"# Importing r2_score library\nfrom sklearn.metrics import r2_score\n\n# Evaluating\nr2_score(y_true=y_test, y_pred=y_test_pred)","d67937a9":"# Plotting y_test and y_pred to understand the spread.\nimport numpy as np\n\nfig = plt.figure()\nplt.scatter(y_test,y_test_pred)\nfig.suptitle('Actual vs Predictions', fontsize=20)              # Plot heading \nplt.xlabel('Actual', fontsize=18)                               # X-label\nplt.ylabel('Predictions', fontsize=16)\n\n# Adding 45 degree line\nxp = np.linspace(y_test.min(), y_test.max(), 100)\nplt.plot(xp, xp, 'k', alpha = 0.9, linewidth = 2, color = 'red')\nplt.show()","731139b4":"#### IV.iii.i - Scaling the Training dataset","56bdad56":"# IV - Building Model\n\nAfter having sufficient information about the data, what should be our next step?\n* We will try to create dummies for necessary categorical variables\n* We will then try to split the data into training and test set.\n* We will then try to build a model using RFE.\n* We will then verify the model using manual model building step.\n* We will test the model finally.","20156155":"#### IV.v.i - Scaling the Test dataset","1755fc46":"# I - Data Sourcing\n\nWhat do we do in the first place?\n* We will first import the necessary libraries.\n* We will then import the files containing the information.\n* We will then try to gather basic information from the dataframe.","8d059bd9":"#### I.i - Importing the necessary libraries & file","dbe5c275":"#### Note : The linear equation for the model can be given as : 0.2366 (yr) - 0.0918 (holiday) + 0.3571 (temp) - 0.1450 (windspeed)  - 0.1398 (spring) + 0.0624 (oct) + 0.0762 (sep) + 0.0804 (clear) - 0.2148 (lightsnow)","810b4dd2":"#### III.i - Plotting the graphs for continous variables","2fb2b583":"#### I.ii - Fetching the basic information","39de5adf":"#### Note : There are no records for 'Heavy Rain' which means noone use 'boombike' app.","8df86e10":"#### II.i - Replacing the values of categorical columns","0e91bea1":"#### IV.iv.i - Building model using statsmodel for the detailed statistics","30015591":"#### Note: 'drop_first=True' can also be used to drop first column implicitly.","850ce87a":"# II - Data Cleaning\n\nNow we have the data in our memory. What will we do next?\n* We will change the categorical columns numerical values to categorical values.\n* We will then drop irrelevant columns.","949b434e":"#### II.ii - Removal of two columns","59e827b1":"#### IV.v.ii - Testing the model","dee5158b":"#### IV.ii - Splitting the Data into Training and Testing Sets","5f51273c":"#### IV.i - Creating Dummies ","52c9804a":"#### IV.iv.ii - Residual Analysis of the train data","bbf58658":"# VI - Conclusion\n\nThere are a few points we have observed. Let's go through them here.\n* #### Data Sourcing\n    - After sourcing data and checking the basic information, we could find that there are no missing values in the data.\n* #### Data Cleaning\n    - In data cleaning section, we could observe that the values of some categorical columns were giving an impression of continuous columns. We replaced values with original labels.\n    - We removed two columns('dteday' and 'instance') which were not making any sense for the data analysis.\n* #### Data Analysis\n    - From pairplotting the 'temp' and 'atemp' against 'cnt', we could get a sense that they are linearly dependents.\n    - We could also conclude that 'registered' and 'casual' are the subset values of 'cnt'. So it can be ignored while preparing final model.\n    - Plotting the categorical variables against 'cnt' for the year '2018' & '2019', we could clearly see the values increase in later year.\n    - We could also observe one peculiar thing on plotting 'workingday' against 'cnt' and it contains almost same values for 25th, 50th & 75th percentile.\n* #### Model Building\n    - To build a model we had distributed our dataset in training and test dataset in the ratio of 7:3.\n    - We chose to build a model using RFE and manual steps.\n    - Finally, the model was found to be dependent on 9 features.\n* #### Model Evaluation\n    - r2 score of test data set comes out to cover 81.4% variance while the same for our model is 82.8%.","9a4aebd2":"# V - Model Evaluation\n\nFinally, we should try to check a few things. And what are they?\n* We should check r2_score.\n* We should try to plot y_test vs y_pred.","33728c5e":"#### V.i - Checking r2_score","42552d83":"# III - Data Analysis\n\nNext we should try to understand the data. How?\n* We will try to plot graphs for continuous variables.\n* We will try to plot graphs for categorical variables.","ac909df6":"#### IV.iii.ii - Building the model using RFE","3bee3c43":"#### Note : We could have removed 'temp' here which would have led to build a model \n* not depending on any continuous variables. \n* accounts for merely 77.7% of variance.\n* with errors not normally distrbuted.\n\n#### We have not chosen that path and decided to remove another variable 'hum'.","e9a00e7f":"#### III.ii - Plotting the graphs for categorical variables","7cdc0347":"#### V.ii - Plotting the graph"}}