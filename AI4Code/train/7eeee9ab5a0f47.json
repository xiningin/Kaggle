{"cell_type":{"7f62f11c":"code","165165c4":"code","800307c2":"code","8ddd9da4":"code","238cf09b":"code","46802786":"code","2bdfc996":"code","69cdd4c7":"code","a3f40496":"code","8992ce26":"code","cc19c8ab":"code","915bdcf1":"code","64819680":"code","0d16fda7":"code","2f23ecb0":"code","15b05af8":"code","759c23d4":"code","91447542":"code","1d369cba":"code","bb6e619b":"code","91b29139":"code","3a0b5608":"code","d89478e7":"code","33dc51c3":"code","ad140d34":"code","f26523af":"code","ea1835ed":"code","74fb27da":"code","2d949f2c":"code","5a5d2ece":"code","01823dd0":"code","2f1f2c63":"code","eeae20b2":"code","f5d6c39f":"code","811e087b":"code","54832181":"code","18c6dc15":"code","dbd9a991":"code","1a5d6e5f":"code","fd9bdb50":"code","df34aecf":"code","447aa09d":"code","8254d00f":"code","cffccc8c":"code","bf6ce322":"code","39c748de":"code","d96f2e6b":"code","a563ef1a":"markdown","b358bef7":"markdown","0184c822":"markdown","826fcb79":"markdown","d1ac3411":"markdown","6c372d2e":"markdown","b6be1a7f":"markdown","cf98dedf":"markdown","e8d0a3f1":"markdown","c4e4c796":"markdown","aeb151fe":"markdown","1b75d9de":"markdown","4611b3ff":"markdown","9762b8ff":"markdown","1a78e5f2":"markdown","78fe0f6d":"markdown","3d8f5970":"markdown","42112bd6":"markdown","9701ae0b":"markdown","54952717":"markdown","ad91ba86":"markdown","cdd5ea55":"markdown","02f1ab3c":"markdown"},"source":{"7f62f11c":"#T\u00e9l\u00e9charger la biblioth\u00e8que n\u00e9cessaire\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport pylab as pl\nfrom IPython import display\nimport seaborn as sns\nsns.set()\nimport re\nimport pydicom\nimport random\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nimport pandas as pd\n\n#import pytorch_lightning as pl\nfrom scipy.special import softmax\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import roc_auc_score, auc\n\nfrom skimage.io import imread\nfrom PIL import Image\nimport plotly.express as px\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\n\nimport os\nimport copy\n\nfrom albumentations import Compose, RandomCrop, Normalize,HorizontalFlip, Resize\nfrom albumentations import VerticalFlip, RGBShift, RandomBrightness\nfrom albumentations.core.transforms_interface import ImageOnlyTransform\nfrom albumentations.pytorch import ToTensor\n\nfrom tqdm.notebook import tqdm","165165c4":"os.listdir(\"..\/input\/\")","800307c2":"basepath = '\/kaggle\/input\/siim-isic-melanoma-classification\/'\ntrain_img_path = '\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'\ntest_img_path = '\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/test\/'\nos.listdir(basepath)","8ddd9da4":"# Setting color palette.\ncolor_black = ['#333300','#666600','#999900','#CCCC00','#FFFF00','#FFFF33','#FFFF66','#FFFF99','#FFFFCC']\ncolor_black_2 = ['#FFD700','#BDB76B','#F0E68C','#EEE8AA','#FFEFD5','#FFDAB9','#330C73']\n\n# Setting color palette.\norange_black = ['#fdc029', '#df861d', '#FF6347', '#aa3d01', '#a30e15', '#800000', '#171820']\ncolor_black_3=['#318ce7','#ff66cc']\nB = ['#C3073F','#1A1A1D']","238cf09b":"train_info = pd.read_csv(basepath + \"train.csv\")\ntrain_info.head()","46802786":"test_info = pd.read_csv(basepath + \"test.csv\")\ntest_info.head()","2bdfc996":"train_info.shape[0] \/ test_info.shape[0]","69cdd4c7":"# Image names\n\n# train: image_name count\ntrain_info.image_name.value_counts().max()\n","a3f40496":"# test: image_name count\ntest_info.image_name.value_counts().max()","8992ce26":"# Patient id counts\n\n# train: patient_id count\ntrain_info.patient_id.value_counts().max()","cc19c8ab":"# test: patient_id count\ntest_info.patient_id.value_counts().max()","915bdcf1":"patient_counts_train = train_info.patient_id.value_counts()\npatient_counts_test = test_info.patient_id.value_counts()\n\nfig, ax = plt.subplots(2,2,figsize=(20,12))\n\nsns.distplot(patient_counts_train, ax=ax[0,0], color=\"orangered\", kde=True);\nax[0,0].set_xlabel(\"Counts\")\nax[0,0].set_ylabel(\"Frequency\")\nax[0,0].set_title(\"Patient id value counts in train\");\n\nsns.distplot(patient_counts_test, ax=ax[0,1], color=\"lightseagreen\", kde=True);\nax[0,1].set_xlabel(\"Counts\")\nax[0,1].set_ylabel(\"Frequency\")\nax[0,1].set_title(\"Patient id value counts in test\");\n\nsns.boxplot(patient_counts_train, ax=ax[1,0], color=\"orangered\");\nax[1,0].set_xlim(0, 250)\nsns.boxplot(patient_counts_test, ax=ax[1,1], color=\"lightseagreen\");\nax[1,1].set_xlim(0, 250);","64819680":"fig, ax = plt.subplots(2,2,figsize=(20,15))\n\nsns.boxplot(train_info.sex, train_info.age_approx, ax=ax[0,0], palette=\"Reds_r\");\nax[0,0].set_title(\"Age per gender in train\");\n\nsns.boxplot(test_info.sex, test_info.age_approx, ax=ax[0,1], palette=\"Blues_r\");\nax[0,1].set_title(\"Age per gender in test\");\n\nsns.countplot(train_info.age_approx, hue=train_info.sex, ax=ax[1,0], palette=\"Reds_r\");\nsns.countplot(test_info.age_approx, hue=test_info.sex, ax=ax[1,1], palette=\"Blues_r\");","0d16fda7":"# Plotting interactive sunburst:\n\n\nfig = px.sunburst(data_frame=train_info,\n                  path=['benign_malignant', 'sex', 'anatom_site_general_challenge'],\n                  color='sex',\n                  color_discrete_sequence=orange_black,\n                  maxdepth=-1,\n                  title='Sunburst Chart Benign\/Malignant > Sex > Location')\n\nfig.update_traces(textinfo='label+percent parent')\nfig.update_layout(margin=dict(t=0, l=0, r=0, b=0))\nfig.show()","2f23ecb0":"patient_gender_train = train_info.groupby(\"patient_id\").sex.unique().apply(lambda l: l[0])\npatient_gender_test = test_info.groupby(\"patient_id\").sex.unique().apply(lambda l: l[0])\n\ntrain_patients = pd.DataFrame(index=patient_gender_train.index.values, data=patient_gender_train.values, columns=[\"sex\"])\ntest_patients = pd.DataFrame(index=patient_gender_test.index.values, data=patient_gender_test.values, columns=[\"sex\"])\n\ntrain_patients.loc[:, \"num_images\"] = train_info.groupby(\"patient_id\").size()\ntest_patients.loc[:, \"num_images\"] = test_info.groupby(\"patient_id\").size()\n\ntrain_patients.loc[:, \"min_age\"] = train_info.groupby(\"patient_id\").age_approx.min()\ntrain_patients.loc[:, \"max_age\"] = train_info.groupby(\"patient_id\").age_approx.max()\ntest_patients.loc[:, \"min_age\"] = test_info.groupby(\"patient_id\").age_approx.min()\ntest_patients.loc[:, \"max_age\"] = test_info.groupby(\"patient_id\").age_approx.max()\n\ntrain_patients.loc[:, \"age_span\"] = train_patients[\"max_age\"] - train_patients[\"min_age\"]\ntest_patients.loc[:, \"age_span\"] = test_patients[\"max_age\"] - test_patients[\"min_age\"]\n\ntrain_patients.loc[:, \"benign_cases\"] = train_info.groupby([\"patient_id\", \"benign_malignant\"]).size().loc[:, \"benign\"]\ntrain_patients.loc[:, \"malignant_cases\"] = train_info.groupby([\"patient_id\", \"benign_malignant\"]).size().loc[:, \"malignant\"]\ntrain_patients[\"min_age_malignant\"] = train_info.groupby([\"patient_id\", \"benign_malignant\"]).age_approx.min().loc[:, \"malignant\"]\ntrain_patients[\"max_age_malignant\"] = train_info.groupby([\"patient_id\", \"benign_malignant\"]).age_approx.max().loc[:, \"malignant\"]","15b05af8":"train_patients.sort_values(by=\"malignant_cases\", ascending=False).head()","759c23d4":"import cv2, pandas as pd, matplotlib.pyplot as plt\ntrain = pd.read_csv('..\/input\/siim-isic-melanoma-classification\/train.csv')\nprint('Examples WITH Melanoma')\nimgs = train.loc[train.target==1].sample(10).image_name.values\nplt.figure(figsize=(20,8))\nfor i,k in enumerate(imgs):\n    img = cv2.imread(train_img_path+'\/%s.jpg'%k)\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    plt.subplot(2,5,i+1); plt.axis('off')\n    plt.imshow(img)\nplt.show()\nprint('Examples WITHOUT Melanoma')\nimgs = train.loc[train.target==0].sample(10).image_name.values\nplt.figure(figsize=(20,8))\nfor i,k in enumerate(imgs):\n    img = cv2.imread(train_img_path+'\/%s.jpg'%k)\n    #img = cv2.imread('..\/input\/siim-isic-melanoma-classification\/jpeg\/train\/%s.jpg'%k)\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    plt.subplot(2,5,i+1); plt.axis('off')\n    plt.imshow(img)\nplt.show()","91447542":"# Massing values\n\n# Missing Train Values\nmissing_vals_train = train_info.isnull().sum() \/ train_info.shape[0]\nmissing_vals_train[missing_vals_train > 0].sort_values(ascending=False)\n","1d369cba":"# Missing Test Values\nmissing_vals_test = test_info.isnull().sum() \/ test_info.shape[0]\nmissing_vals_test[missing_vals_test > 0].sort_values(ascending=False)","bb6e619b":"sex_dummies = pd.get_dummies(train_info['sex'], prefix='sex')\ntrain = pd.concat([train_info, sex_dummies], axis=1)\n\n# getting dummy variables for gender on test set\n\nsex_dummies = pd.get_dummies(test_info['sex'], prefix='sex')\ntest = pd.concat([test_info, sex_dummies], axis=1)\n\n# dropping not useful columns\n\ntrain.drop(['sex','image_name','patient_id','diagnosis','benign_malignant'], axis=1, inplace=True)\ntest.drop(['sex','image_name','patient_id'], axis=1, inplace=True)","91b29139":"# getting dummy variables for location on train set\n\nanatom_dummies = pd.get_dummies(train['anatom_site_general_challenge'], prefix='anatom')\ntrain = pd.concat([train, anatom_dummies], axis=1)\n\n# getting dummy variables for location on test set\n\nanatom_dummies = pd.get_dummies(test['anatom_site_general_challenge'], prefix='anatom')\ntest = pd.concat([test, anatom_dummies], axis=1)\n\n# dropping not useful columns\n\ntrain.drop('anatom_site_general_challenge', axis=1, inplace=True)\ntest.drop('anatom_site_general_challenge', axis=1, inplace=True)\n","3a0b5608":"import xgboost as xgb\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score, cross_validate\nfrom sklearn.metrics import roc_auc_score, roc_curve","d89478e7":"# dividing train set and labels for modelling\n\nX = train.drop('target', axis=1)\ny = train.target","33dc51c3":"X.head()","ad140d34":"# taking holdout set for validating with stratified y\n\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.2,\n                                                    stratify=y,\n                                                    random_state=42)\n\n# 5 fold stratify for cv\n\ncv = StratifiedKFold(5, shuffle=True, random_state=42)","f26523af":"# setting model hyperparameters, didn't include fine tuning here because of timing reasons...\n\nxg = xgb.XGBClassifier(\n    n_estimators=750,\n    min_child_weight=0.81,\n    learning_rate=0.025,\n    max_depth=2,\n    subsample=0.80,\n    colsample_bytree=0.42,\n    gamma=0.10,\n    random_state=42,\n    n_jobs=-1,\n)","ea1835ed":"estimators = [xg]","74fb27da":"# cross validation scheme\n\ndef model_check(X_train, y_train, estimators, cv):\n    model_table = pd.DataFrame()\n\n    row_index = 0\n    for est in estimators:\n\n        MLA_name = est.__class__.__name__\n        model_table.loc[row_index, 'Model Name'] = MLA_name\n\n        cv_results = cross_validate(est,\n                                    X_train,\n                                    y_train,\n                                    cv=cv,\n                                    scoring='roc_auc',\n                                    return_train_score=True,\n                                    n_jobs=-1)\n\n        model_table.loc[row_index,\n                        'Train roc Mean'] = cv_results['train_score'].mean()\n        model_table.loc[row_index,\n                        'Test roc Mean'] = cv_results['test_score'].mean()\n        model_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        model_table.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n    model_table.sort_values(by=['Test roc Mean'],\n                            ascending=False,\n                            inplace=True)\n\n    return model_table","2d949f2c":"# display cv results\n\nraw_models = model_check(X_train, y_train, estimators, cv)\nprint(raw_models)","5a5d2ece":"# fitting train data\n\nxg.fit(X_train, y_train)\n\n# predicting on holdout set\nvalidation = xg.predict_proba(X_test)[:, 1]\n\n# checking results on validation set\nroc_auc_score(y_test, validation)","01823dd0":"# predicting on test set\n\npredictions = xg.predict_proba(test)[:, 1]","2f1f2c63":"sample = pd.read_csv(os.path.join(base_path, 'sample_submission.csv'))","eeae20b2":"# creating submission df\n\nmeta_df = pd.DataFrame(columns=['image_name', 'target'])\n\n# assigning predictions on submission df\n\nmeta_df['image_name'] = sample['image_name']\nmeta_df['target'] = predictions","f5d6c39f":"# creating submission csv file\n\nmeta_df.to_csv('meta_with_img_data.csv', header=True, index=False)","811e087b":"pip install -U segmentation-models","54832181":"import os\nimport random\nimport re\nimport math\nimport time\n\nfrom tqdm import tqdm\nfrom tqdm.keras import TqdmCallback\n\n\nfrom pandas_summary import DataFrameSummary\n\nimport warnings\n\nwarnings.filterwarnings('ignore') # Disabling warnings for clearer outputs\n\n\n\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)","18c6dc15":"# Importing packages\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport efficientnet.tfkeras as efn\nfrom kaggle_datasets import KaggleDatasets\n\nseed_val = 42\n\ntf.random.set_seed(seed_val)","dbd9a991":"# Loading image storage buckets\n\n# Data access\nGCS_PATH = KaggleDatasets().get_gcs_path('siim-isic-melanoma-classification')\nfilenames_train = np.array(tf.io.gfile.glob(GCS_PATH + '\/tfrecords\/train*.tfrec')\nfilenames_test = np.array(tf.io.gfile.glob(GCS_PATH + '\/tfrecords\/test*.tfrec')","1a5d6e5f":"# Setting TPU as main device for training, if you get warnings while working with tpu's ignore them.\n\nDEVICE = 'TPU'\nif DEVICE == 'TPU':\n    print('connecting to TPU...')\n    try:        \n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print('Could not connect to TPU')\n        tpu = None\n\n    if tpu:\n        try:\n            print('Initializing  TPU...')\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print('TPU initialized')\n        except _:\n            print('Failed to initialize TPU!')\n    else:\n        DEVICE = 'GPU'\n\nif DEVICE != 'TPU':\n    print('Using default strategy for CPU and single GPU')\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == 'GPU':\n    print('Num GPUs Available: ',\n          len(tf.config.experimental.list_physical_devices('GPU')))\n\nprint('REPLICAS: ', strategy.num_replicas_in_sync)\nAUTO = tf.data.experimental.AUTOTUNE","fd9bdb50":"#you can edit these settings.\n\ncfg = dict(\n           batch_size=32,\n           img_size=384,\n    \n           lr_start=0.000005,\n           lr_max=0.00000125,\n           lr_min=0.000001,\n           lr_rampup=5,\n           lr_sustain=0,\n           lr_decay=0.8,\n           epochs=12,\n    \n           transform_prob=1.0,\n           rot=180.0,\n           shr=2.0,\n           hzoom=8.0,\n           wzoom=8.0,\n           hshift=8.0,\n           wshift=8.0,\n    \n           optimizer='adam',\n           label_smooth_fac=0.05,\n           tta_steps=20\n            \n        )","df34aecf":"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift,\n            width_shift):\n    \n    ''' Settings for image preparations '''\n\n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation \/ 180.\n    shear = math.pi * shear \/ 180.\n\n    # ROTATION MATRIX\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1], dtype='float32')\n    zero = tf.constant([0], dtype='float32')\n    rotation_matrix = tf.reshape(\n        tf.concat([c1, s1, zero, -s1, c1, zero, zero, zero, one], axis=0),\n        [3, 3])\n\n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    shear_matrix = tf.reshape(\n        tf.concat([one, s2, zero, zero, c2, zero, zero, zero, one], axis=0),\n        [3, 3])\n\n    # ZOOM MATRIX\n    zoom_matrix = tf.reshape(\n        tf.concat([\n            one \/ height_zoom, zero, zero, zero, one \/ width_zoom, zero, zero,\n            zero, one\n        ],\n                  axis=0), [3, 3])\n\n    # SHIFT MATRIX\n    shift_matrix = tf.reshape(\n        tf.concat(\n            [one, zero, height_shift, zero, one, width_shift, zero, zero, one],\n            axis=0), [3, 3])\n\n    return K.dot(K.dot(rotation_matrix, shear_matrix),\n                 K.dot(zoom_matrix, shift_matrix))\n\n\ndef transform(image, cfg):\n    \n    ''' This function takes input images of [: , :, 3] sizes and returns them as randomly rotated, sheared, shifted and zoomed. '''\n\n    DIM = cfg['img_size']\n    XDIM = DIM % 2  # fix for size 331\n\n    rot = cfg['rot'] * tf.random.normal([1], dtype='float32')\n    shr = cfg['shr'] * tf.random.normal([1], dtype='float32')\n    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ cfg['hzoom']\n    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ cfg['wzoom']\n    h_shift = cfg['hshift'] * tf.random.normal([1], dtype='float32')\n    w_shift = cfg['wshift'] * tf.random.normal([1], dtype='float32')\n\n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot, shr, h_zoom, w_zoom, h_shift, w_shift)\n\n    # LIST DESTINATION PIXEL INDICES\n    x = tf.repeat(tf.range(DIM \/\/ 2, -DIM \/\/ 2, -1), DIM)\n    y = tf.tile(tf.range(-DIM \/\/ 2, DIM \/\/ 2), [DIM])\n    z = tf.ones([DIM * DIM], dtype='int32')\n    idx = tf.stack([x, y, z])\n\n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n    idx2 = K.cast(idx2, dtype='int32')\n    idx2 = K.clip(idx2, -DIM \/\/ 2 + XDIM + 1, DIM \/\/ 2)\n\n    # FIND ORIGIN PIXEL VALUES\n    idx3 = tf.stack([DIM \/\/ 2 - idx2[0, ], DIM \/\/ 2 - 1 + idx2[1, ]])\n    d = tf.gather_nd(image, tf.transpose(idx3))\n\n    return tf.reshape(d, [DIM, DIM, 3])\n\ndef prepare_image(img, cfg=None, augment=True):\n    \n    ''' This function loads the image, resizes it, casts a tensor to a new type float32 in our case, transforms it using the function just above, then applies the augmentations.'''\n    \n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [cfg['img_size'], cfg['img_size']],\n                          antialias=True)\n    img = tf.cast(img, tf.float32) \/ 255.0\n\n    if augment:\n        if cfg['transform_prob'] > tf.random.uniform([1], minval=0, maxval=1):\n            img = transform(img, cfg)\n\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_saturation(img, 0.7, 1.3)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n        img = tf.image.random_brightness(img, 0.1)\n\n    return img","447aa09d":"def read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'image_name': tf.io.FixedLenFeature([], tf.string),\n        'patient_id': tf.io.FixedLenFeature([], tf.int64),\n        'sex': tf.io.FixedLenFeature([], tf.int64),\n        'age_approx': tf.io.FixedLenFeature([], tf.int64),\n        'anatom_site_general_challenge': tf.io.FixedLenFeature([], tf.int64),\n        'diagnosis': tf.io.FixedLenFeature([], tf.int64),\n        'target': tf.io.FixedLenFeature([], tf.int64),\n        #'width': tf.io.FixedLenFeature([], tf.int64),\n        #'height': tf.io.FixedLenFeature([], tf.int64)\n    }\n\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    return example['image'], example['target']\n\n\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'image_name': tf.io.FixedLenFeature([], tf.string),\n        'patient_id': tf.io.FixedLenFeature([], tf.int64),\n        'sex': tf.io.FixedLenFeature([], tf.int64),\n        'age_approx': tf.io.FixedLenFeature([], tf.int64),\n        'anatom_site_general_challenge': tf.io.FixedLenFeature([], tf.int64),\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    return example['image'], example['image_name']\n\ndef count_data_items(filenames):\n    n = [\n        int(re.compile(r'-([0-9]*)\\.').search(filename).group(1))\n        for filename in filenames\n    ]\n    return np.sum(n)","8254d00f":"def getTrainDataset(files, cfg, augment=True, shuffle=True):\n    \n    ''' This function reads the tfrecord train images, shuffles them, apply augmentations to them and prepares the data for training. '''\n    \n    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n    ds = ds.cache()\n\n    if shuffle:\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False\n        ds = ds.with_options(opt)\n\n    ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n    ds = ds.repeat()\n    if shuffle:\n        ds = ds.shuffle(2048)\n    ds = ds.map(lambda img, label:\n                (prepare_image(img, augment=augment, cfg=cfg), label),\n                num_parallel_calls=AUTO)\n    ds = ds.batch(cfg['batch_size'] * strategy.num_replicas_in_sync)\n    ds = ds.prefetch(AUTO)\n    return ds\n\ndef getTestDataset(files, cfg, augment=False, repeat=False):\n    \n    ''' This function reads the tfrecord test images and prepares the data for predicting. '''\n    \n    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n    ds = ds.cache()\n    if repeat:\n        ds = ds.repeat()\n    ds = ds.map(read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n    ds = ds.map(lambda img, idnum:\n                (prepare_image(img, augment=augment, cfg=cfg), idnum),\n                num_parallel_calls=AUTO)\n    ds = ds.batch(cfg['batch_size'] * strategy.num_replicas_in_sync)\n    ds = ds.prefetch(AUTO)\n    return ds\n\ndef get_model():\n    \n    ''' This function gets the layers inclunding efficientnet ones. '''\n    \n    model_input = tf.keras.Input(shape=(cfg['img_size'], cfg['img_size'], 3),\n                                 name='img_input')\n\n    dummy = tf.keras.layers.Lambda(lambda x: x)(model_input)\n\n    outputs = []\n\n    x = efn.EfficientNetB3(include_top=False,\n                           weights='noisy-student',\n                           input_shape=(cfg['img_size'], cfg['img_size'], 3),\n                           pooling='avg')(dummy)\n    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    outputs.append(x)\n\n    x = efn.EfficientNetB4(include_top=False,\n                           weights='noisy-student',\n                           input_shape=(cfg['img_size'], cfg['img_size'], 3),\n                           pooling='avg')(dummy)\n    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    outputs.append(x)\n\n    x = efn.EfficientNetB5(include_top=False,\n                           weights='noisy-student',\n                           input_shape=(cfg['img_size'], cfg['img_size'], 3),\n                           pooling='avg')(dummy)\n    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    outputs.append(x)\n\n    model = tf.keras.Model(model_input, outputs, name='aNetwork')\n    model.summary()\n    return model","cffccc8c":"def compileNewModel(cfg):\n    \n    ''' Configuring the model with losses and metrics. '''    \n    \n    with strategy.scope():\n        model = get_model()\n\n    with strategy.scope():\n        model.compile(optimizer=cfg['optimizer'],\n                      loss=[\n                          tf.keras.losses.BinaryCrossentropy(\n                              label_smoothing=cfg['label_smooth_fac']),\n                          tf.keras.losses.BinaryCrossentropy(\n                              label_smoothing=cfg['label_smooth_fac']),\n                          tf.keras.losses.BinaryCrossentropy(\n                              label_smoothing=cfg['label_smooth_fac'])\n                      ],\n                      metrics=[tf.keras.metrics.AUC(name='auc')])\n    return model\n","bf6ce322":"def getLearnRateCallback(cfg):\n    \n    ''' Using callbacks for learning rate adjustments. '''\n    \n    lr_start = cfg['lr_start']\n    lr_max = cfg['lr_max'] * strategy.num_replicas_in_sync * cfg['batch_size']\n    lr_min = cfg['lr_min']\n    lr_rampup = cfg['lr_rampup']\n    lr_sustain = cfg['lr_sustain']\n    lr_decay = cfg['lr_decay']\n\n    def lrfn(epoch):\n        if epoch < lr_rampup:\n            lr = (lr_max - lr_start) \/ lr_rampup * epoch + lr_start\n        elif epoch < lr_rampup + lr_sustain:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_rampup -\n                                                lr_sustain) + lr_min\n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback\n\ndef learnModel(model, ds_train, stepsTrain, cfg, ds_val=None, stepsVal=0):\n    \n    ''' Fitting things together for training '''\n    \n    callbacks = [getLearnRateCallback(cfg)]\n\n    history = model.fit(ds_train,\n                        validation_data=ds_val,\n                        verbose=1,\n                        steps_per_epoch=stepsTrain,\n                        validation_steps=stepsVal,\n                        epochs=cfg['epochs'],\n                        callbacks=callbacks)\n\n    return history","39c748de":"!pip install git+https:\/\/github.com\/qubvel\/efficientnet","d96f2e6b":"# getting train data\n\nds_train = getTrainDataset(\n    filenames_train, cfg).map(lambda img, label: (img, (label, label, label)))\nstepsTrain = count_data_items(filenames_train) \/ \\\n    (cfg['batch_size'] * strategy.num_replicas_in_sync)\n\n# compiling and training model\n\nmodel = compileNewModel(cfg)\nlearnModel(model, ds_train, stepsTrain, cfg)","a563ef1a":"### Comprendre les donn\u00e9es\nDans cette section, nous allons r\u00e9cup\u00e9rer les donn\u00e9es et \u00e9tudier leurs distributions. Fait int\u00e9ressant que les donn\u00e9es contiennent \u00e0 la fois des donn\u00e9es tabulaires et des donn\u00e9es d'image.\n\nNous continuerons en chargeant les m\u00e9tadonn\u00e9es qui nous sont fournies. Les donn\u00e9es de train ont 8 caract\u00e9ristiques et 33126 observations,les donn\u00e9es de test ont 5 caract\u00e9ristiques et 10982 observations.\n\nL'ensemble de donn\u00e9es de train se compose de:\n\n1. nom de l'image -> le nom de fichier de l'image sp\u00e9cifique pour la rame\n2. patient_id -> identifie le patient unique\n3. sexe -> sexe du patient\n4. age_approx -> \u00e2ge approximatif du patient au moment de la num\u00e9risation\n5. anatom_site_general_challenge -> emplacement du site d'analyse\n6. diagnostic -> informations sur le diagnostic\n7. benign_malignant - indique le r\u00e9sultat du scan s'il est malin ou b\u00e9nin\n8. target -> m\u00eame que ci-dessus mais mieux pour la mod\u00e9lisation car c'est binaire","b358bef7":"#### Creating Meta Submission\n","0184c822":"Three times more entries in train than in test.","826fcb79":"**A General Look With Sunburst Chart**\n\nSunburst chart is pretty cool looking fella I'd say. It also giving lots of basic information to us. Let's see...\n\n* Only 2% of our targets are malignant\n* On malignant images males are dominant with 62%\n* Gender wise benign images are more balance 52-48% male female ratio\n* Malignant image scan locations differs based on the patients gender:\n* Meanwhile the torso is most common location in males it's almost half of the scans meanwhile in females it's 39%\n* Lower extremity is more common with female scans than males 18% males vs 26% females\n* Again upper extremity malignant scans is common with females than males (23- 17%)\n* Benign image scan locations more similar between male and female patients.","d1ac3411":"Il s'agit d'un boxplot qui d\u00e9crit en jaune les males et en gris les femmes , comme il est bien clair la cat\u00e9gorie d'age entre 40ans et 60 ans des males ont plus de chances de ne pas avoir la maladie ,par contre les femmes non atteinte ont une categorie entre 35ans jusqu'a 50 ans. En contre partie les males atteint varie leur age entre 60 ans et 70 ans et les femmes atteintes entre 40 et 60 ans.","6c372d2e":"### Apprentissage (Modeling)","b6be1a7f":"### Pr\u00e9paration des donn\u00e9es","cf98dedf":"## Acquisition des donn\u00e9es","e8d0a3f1":"###  CNN Neural Networks","c4e4c796":"### Analyser les donn\u00e9es","aeb151fe":"**Patient Information Management Individually**","1b75d9de":"# **Le m\u00e9lanome de la peau**","4611b3ff":"**Insights**\n* For most of the patients we only have a few images ranging fom 1 to roughly 20.\n* More than 45 images per patient is very seldom!\n* Nonetheless we have patients with more than 100 images.\n* There is one heavy outlier patient in the test set with close to 250 images.**","9762b8ff":"**Data Visualization Age Vs. Gender**","1a78e5f2":"<figure>\n<center>\n<img src='https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/6\/6c\/Melanoma.jpg\/260px-Melanoma.jpg' \/>\n<figcaption>La m\u00e9lanome de la peau<\/figcaption><\/center>\n<\/figure>\n<\/center>","78fe0f6d":"## Formalisation des objectifs","3d8f5970":"1. The anatomy shows most missing values.\n\n","42112bd6":"**Setting Cross-Validation and Hold-out Set**\nCross validation might be enough but I wanted to test our model on data which it never seen before.\n","9701ae0b":"Our test set misses three columns: diagnosis, benign_malignant & target.","54952717":"## Introduction\n   Un m\u00e9lanome de la peau est une maladie des cellules de la peau appel\u00e9es m\u00e9lanocytes. Il se d\u00e9veloppe \u00e0 partir d\u2019une cellule initialement normale qui se transforme et se multiplie de fa\u00e7on anarchique pour former une l\u00e9sion appel\u00e9e tumeur maligne.\n\n   Il existe quatre principaux types de m\u00e9lanome de la peau : le m\u00e9lanome superficiel extensif, le m\u00e9lanome de Dubreuilh, le m\u00e9lanome nodulaire et le m\u00e9lanome acrolentigineux. Le traitement de ces diff\u00e9rents types de m\u00e9lanome repose essentiellement sur la chirurgie qui sera adapt\u00e9e \u00e0 la topographie (c\u2019est-\u00e0-dire \u00e0 l\u2019endroit o\u00f9 est situ\u00e9 le m\u00e9lanome) et \u00e0 la profondeur de la l\u00e9sion.","ad91ba86":"Here we train our model, takes a while but at the end we'll have strong model to make predictions!","cdd5ea55":"#### **Model Results Based on Meta Features**\nResults are encouraging! It seems little bit overfitting but we might fix that in future by fine tuning. Let's leave it like that for now...","02f1ab3c":"**Images show**"}}