{"cell_type":{"8821f07c":"code","9b7c3f21":"code","8b795187":"code","d92e99f1":"code","a39d0710":"code","4a59ebae":"code","c309f680":"code","38370c19":"code","0e31d8c6":"code","5ba02c83":"code","c16270b3":"code","2134ea9c":"code","6743d1bd":"code","e20ef288":"code","428c9f6d":"code","8e9f2ebe":"code","dbde43bc":"code","b8858363":"code","fb45be87":"code","faba80f7":"code","62f9550d":"code","8155e86c":"code","e902d86b":"code","26f9d782":"code","e0c7a60c":"code","a4002ac1":"code","62ac8002":"code","9766b2b0":"code","f4e48a6a":"markdown","a6309e68":"markdown","8e354535":"markdown","f824fc50":"markdown","8e17f5e9":"markdown","70d4e37f":"markdown","66b989cc":"markdown","59dc24f5":"markdown","9b6714f7":"markdown"},"source":{"8821f07c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport time\nfrom sklearn.metrics import accuracy_score\nfrom kaggle.competitions import twosigmanews\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Input, Dense, Embedding, Concatenate, Flatten, BatchNormalization\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.losses import binary_crossentropy, mse\nfrom keras import backend as K\nimport tensorflow as tf\nimport os\n\nsession_conf = tf.ConfigProto()\ntf.set_random_seed(42)\nsession_conf.gpu_options.allow_growth = True\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)\n\nos.environ['PYTHONHASHSEED'] = '0'","9b7c3f21":"# comment these out for random individuals each time\n#np.random.seed(1)\n#random.seed(1)\n\n# randomly initialize data\nnum_samples = 20\ndata = np.concatenate((\n    np.array([random.randrange(2, 10, 2) for _ in range(num_samples)]).reshape(-1,1), \n    np.random.randint(64,128,size=(num_samples, 1)),\n    np.random.randint(0,3,size=(num_samples, 1)), # There are 7 different optimization functions\n    np.random.randint(0,4,size=(num_samples, 1)), # There are 11 different activation functions\n    np.random.randint(0,1,size=(num_samples, 1))), axis=1)\n# create dataframe\ndf = pd.DataFrame(data, columns = ['n_layers','n_nodes','optimization','activation','score'])\ndf.sort_values('score',ascending=True)","8b795187":"# define mutations\ndef create_mutations(df):    \n    t = 1000 * time.time() # current time in milliseconds\n    np.random.seed(int(t) % 2**32)\n    # select random chromo to mutate\n    df_mut = df\n    # select random column to mutate\n    param_to_mut = df_mut.columns.values[np.random.choice(len(df.columns.values)-1)]\n    # identify which param is being mutated\n    if param_to_mut == 'activation':\n        df.iloc[0][param_to_mut]=random.randint(0, 2) # Can give same value\n    if param_to_mut == 'optimization':\n        df.iloc[0][param_to_mut]=random.randint(0, 2) # Can give same value\n    if param_to_mut == 'n_layers':\n        flip = random.randint(0,1)\n        if flip == 0:\n            df.iloc[0][param_to_mut] += 1\n        elif flip == 1 and df.iloc[0][param_to_mut] > 1:\n            df.iloc[0][param_to_mut] -= 1\n    if param_to_mut == 'n_nodes':\n        flip = random.randint(0,1)\n        if flip == 0:\n            df.iloc[0][param_to_mut] += 2\n        elif flip == 1 and df.iloc[0][param_to_mut] > 2:\n            df.iloc[0][param_to_mut] -= 2\n    return df","d92e99f1":"# define Gaussian mutations\ndef gaussian_mutations(df, dfc, generation, n_generations):    \n    t = 1000 * time.time() # current time in milliseconds\n    np.random.seed(int(t) % 2**32)\n    # select random chromo to mutate\n    df_mut = df\n    # select random column to mutate\n    param_to_mut = df_mut.columns.values[np.random.choice(len(df.columns.values)-1)]\n    print(param_to_mut)\n    print('before: ', df.iloc[0][param_to_mut])\n    \n    # determine stepsize for mutation\n    p_success = float(generation)\/float(n_generations)\n    tau = 0.5 # using formula tau = 1\/n^0.5, Rechenberg's 1\/5 rule\n    if p_success > 0.2:\n        tau = 2 #stdev\/tau\n    elif p_success < 0.2:\n        tau = 0.5 #stdev*tau\n    elif p_success == 0.2:\n        tau = 1 #stdev*1\n    \n    # identify which param is being mutated\n    if param_to_mut == 'activation':\n        act_stepsize = dfc[param_to_mut].std() * tau\n        act_mean = dfc[param_to_mut].mean()\n        act_val = random.gauss(act_mean,act_stepsize)\n        act_current = df.iloc[0][param_to_mut]\n        act_mod = act_val - act_mean #modifier value\n        \n        df.iloc[0][param_to_mut] = round(act_current + act_mod) #final val mutated\n        \n        if df.iloc[0][param_to_mut] < 0:\n            df.iloc[0][param_to_mut] = 0\n        elif df.iloc[0][param_to_mut] > 3:\n            df.iloc[0][param_to_mut] = 3\n    if param_to_mut == 'optimization':\n        opt_stepsize = dfc[param_to_mut].std() * tau\n        opt_mean = dfc[param_to_mut].mean()\n        opt_val = random.gauss(opt_mean,opt_stepsize)\n        opt_current = df.iloc[0][param_to_mut]\n        opt_mod = opt_val - opt_mean #modifier value\n        \n        df.iloc[0][param_to_mut] = round(opt_current + opt_mod)\n        \n        if df.iloc[0][param_to_mut] < 0:\n            df.iloc[0][param_to_mut] = 0\n        elif df.iloc[0][param_to_mut] > 2:\n            df.iloc[0][param_to_mut] = 2\n    if param_to_mut == 'n_layers':\n        lay_stepsize = dfc[param_to_mut].std() * tau\n        lay_mean = dfc[param_to_mut].mean()\n        lay_val = random.gauss(lay_mean,lay_stepsize)\n        lay_current = df.iloc[0][param_to_mut]\n        lay_mod = lay_val - lay_mean #modifier value\n        \n        df.iloc[0][param_to_mut] = round(lay_current + lay_mod)\n        \n        if df.iloc[0][param_to_mut] < 0:\n            df.iloc[0][param_to_mut] = 1\n    if param_to_mut == 'n_nodes':\n        nod_stepsize = dfc[param_to_mut].std() * tau\n        nod_mean = dfc[param_to_mut].mean()\n        nod_val = random.gauss(nod_mean,nod_stepsize)\n        nod_current = df.iloc[0][param_to_mut]\n        nod_mod = nod_val - nod_mean #modifier value\n        \n        df.iloc[0][param_to_mut] = round(nod_current + nod_mod)\n        \n        if df.iloc[0][param_to_mut] < 0:\n            df.iloc[0][param_to_mut] = 0\n            \n    print('after: ', df.iloc[0][param_to_mut])\n    return df\n#display(df)\n#test = gaussian_mutations(df,0,5)\n#display(test)","a39d0710":"def uniform_crossover(df):\n    t = 1000 * time.time() # current time in milliseconds\n    np.random.seed(int(t) % 2**32)\n    # select two random chromosomes to perform crossover on\n    df_cross = df.sample(n=2).copy()\n    # create copy of first individual to be used as offspring\n    offspring = df_cross.iloc[[0]]\n    offspring.iat[0, df.columns.get_loc(\"score\")] = 0\n    # create binary array representing crossover points\n    split_bits = np.random.randint(0,2,4)\n    \n    #no duplicates\n    while np.sum(split_bits) == 0 or np.sum(split_bits) == 4: #[0,0,0,0] or [1,1,1,1]\n        split_bits = np.random.randint(0,2,4)\n        \n    # iterate through bit array to perform uniform crossover\n    for index, val in enumerate(split_bits):\n        if split_bits[index] == 1:\n            offspring.iat[0,index] = df_cross.iat[1,index]\n    return offspring","4a59ebae":"def breed(df, n_individuals=15, mut_percent=5): \n    for i in range(n_individuals):\n        offspring_df = uniform_crossover(df.head())\n        if random.randint(1, 100) <= mut_percent:\n            create_mutations(offspring_df)\n        new_df = [df, offspring_df]\n        df = pd.concat(new_df, ignore_index=True)\n    print(\"\\n------------- Updated DataFrame -------------\\n\")\n    \n    return df","c309f680":"def breed_es(df, generation, n_generations, n_individuals=15, mut_percent=20): \n    for i in range(n_individuals):\n        offspring_df = uniform_crossover(df)\n        display(offspring_df)\n        if random.randint(1, 100) <= mut_percent:\n            gaussian_mutations(offspring_df, df, generation, n_generations) \n        new_df = [df, offspring_df]\n        df = pd.concat(new_df, ignore_index=True)\n    print(\"\\n------------- Updated DataFrame -------------\\n\")\n    return df\n\n#test = breed_es(df.head(), 0, 5, mut_percent=20)\n#display(df)\n#display(test)","38370c19":"env = twosigmanews.make_env()\n(market_train, _) = env.get_training_data()","0e31d8c6":"market_train.head()","5ba02c83":"market_train_subset = market_train.sample(n=30000).copy()\nmarket_train_subset = market_train_subset.reset_index(drop=True)\n\ncat_cols = ['assetCode']\nnum_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                    'returnsOpenPrevMktres10']\n\nmarket_train_subset.head(20)","c16270b3":"from sklearn.model_selection import train_test_split\n\ntrain_indices, val_indices = train_test_split(market_train.index.values,test_size=0.25, random_state=23)\nsubset_indices, subset_val_indices = train_test_split(market_train_subset.index.values, test_size=0.25, random_state=42)\n\nprint(subset_indices)","2134ea9c":"def encode(encoder, x):\n    len_encoder = len(encoder)\n    try:\n        id = encoder[x]\n    except KeyError:\n        id = len_encoder\n    return id\n\nencoders = [{} for cat in cat_cols]\nsubset_encoders = [{} for cat in cat_cols]\n\n\nfor i, cat in enumerate(cat_cols):\n    print('encoding %s ...' % cat, end=' ')\n    encoders[i] = {l: id for id, l in enumerate(market_train.loc[train_indices, cat].astype(str).unique())}\n    subset_encoders[i] = {l: id for id, l in enumerate(market_train_subset.loc[subset_indices, cat].astype(str).unique())}\n    market_train[cat] = market_train[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n    market_train_subset[cat] = market_train_subset[cat].astype(str).apply(lambda x: encode(subset_encoders[i], x))\n    print('Done')\n\nembed_sizes = [len(encoder) + 1 for encoder in encoders] #+1 for possible unknown assets\n","6743d1bd":"from sklearn.preprocessing import StandardScaler\n \nmarket_train[num_cols] = market_train[num_cols].fillna(0)\nmarket_train_subset[num_cols] = market_train_subset[num_cols].fillna(0)\nprint('scaling numerical columns')\n\nscaler = StandardScaler()\n\nscaler = StandardScaler()\nmarket_train[num_cols] = scaler.fit_transform(market_train[num_cols])\nmarket_train_subset[num_cols] = scaler.fit_transform(market_train_subset[num_cols])\n\nmarket_train_subset.tail()","e20ef288":"    def get_input(market_train, indices):\n        X_num = market_train.loc[indices, num_cols].values\n        X = {'num':X_num}\n        for cat in cat_cols:\n            X[cat] = market_train.loc[indices, cat_cols].values\n        y = (market_train.loc[indices,'returnsOpenNextMktres10'] >= 0).values\n        r = market_train.loc[indices,'returnsOpenNextMktres10'].values\n        u = market_train.loc[indices, 'universe']\n        d = market_train.loc[indices, 'time'].dt.date\n        return X,y,r,u,d","428c9f6d":"def load_neural_net(df):    \n    num_layers = df.iloc[0]['n_layers']\n    num_nodes = df.iloc[0]['n_nodes']\n    activation = df.iloc[0]['activation']\n    optimization = df.iloc[0]['optimization']\n    \n    if activation == 0:\n        activation = 'relu'\n    elif activation == 1:\n        activation = 'selu'\n    elif activation == 2:\n        activation = 'sigmoid'\n    else:\n        activation = 'elu'\n    \n    if optimization == 0:\n        optimization = 'adam'\n    elif optimization == 1:\n        optimization = 'SGD'\n    else:\n        optimization = 'adadelta'\n    \n    print(num_layers)\n    print(num_nodes)\n    print(activation)\n    print(optimization)\n    categorical_inputs = []\n    for cat in cat_cols:\n        categorical_inputs.append(Input(shape=[1], name=cat))\n            \n    categorical_embeddings = []\n    for i, cat in enumerate(cat_cols):\n        categorical_embeddings.append(Embedding(embed_sizes[i], 10)(categorical_inputs[i]))\n\n    categorical_logits = Flatten()(categorical_embeddings[0])\n    categorical_logits = Dense(32,activation='relu')(categorical_logits)\n\n    numerical_inputs = Input(shape=(11,), name='num')\n    numerical_logits = numerical_inputs\n    numerical_logits = BatchNormalization()(numerical_logits)\n    \n    # ADD LOOP HERE\n    for i in range(num_layers):\n        numerical_logits = Dense(num_nodes,activation=activation)(numerical_logits)\n\n    logits = Concatenate()([numerical_logits,categorical_logits])\n    logits = Dense(64,activation='relu')(logits)\n    out = Dense(1, activation='sigmoid')(logits)\n\n    model = Model(inputs = categorical_inputs + [numerical_inputs], outputs=out)\n    model.compile(optimizer=optimization,loss=binary_crossentropy)\n    \n    # r, u and d are used to calculate the scoring metric\n    X_train,y_train,r_train,u_train,d_train = get_input(market_train_subset, subset_indices)\n    X_valid,y_valid,r_valid,u_valid,d_valid = get_input(market_train_subset, subset_val_indices)\n\n    neural_net = model.fit(X_train,y_train.astype(int),\n              validation_data=(X_valid,y_valid.astype(int)),\n              epochs=1,\n              verbose=True)\n    \n    new_score = min(neural_net.history['val_loss']) * 100000\n    #if new_score < df.iloc[0]['score'] or df.iloc[0]['score'] == 0:\n    df.iloc[0]['score'] = new_score\n        \n    print(neural_net.history['val_loss'])\n    print(min(neural_net.history['val_loss']))\n    display(df)\n    return df","8e9f2ebe":"# Used for testing\n#initial_weights = \"\"\n#for generation in range(0,2):\n#    for index, row in df.head(1).iterrows():\n#        np.random.seed(1)\n#        df.loc[[index]], initial_weights = load_neural_net(df.loc[[index]], initial_weights)\n#display(df.head(3))","dbde43bc":"gen_results = pd.DataFrame()\nwhole_population = pd.DataFrame()\ndisplay(df)\n\ndef evaluate(df, n_generations=5):\n    for generation in range(0,n_generations):\n        for index, row in df.iterrows(): \n            #np.random.seed(1)\n            df.loc[[index]] = load_neural_net(df.loc[[index]])\n        display(df)\n        whole_population[generation] = df['score']\n        df = df.sort_values('score',ascending=True).head()\n        df = df.reset_index(drop=True)\n        df = breed(df)\n        gen_results[generation] = df['score'].head()\n        display(df)\n    return df\n\n#df = evaluate(df)\n#gen_results = pd.concat([gen_results, df.head()], axis=1)\n#display(gen_results)\n#display(whole_population)","b8858363":"gen_results = pd.DataFrame()\nwhole_population = pd.DataFrame()\ndisplay(gen_results)\n\ndisplay(df)\ndef evaluate_es(df, n_generations=5):\n    for generation in range(0,n_generations):\n        for index, row in df.iterrows(): \n            #np.random.seed(1)\n            df.loc[[index]] = load_neural_net(df.loc[[index]])\n        display(df)\n        whole_population[generation] = df['score']\n        df = df.sort_values('score',ascending=True).head()\n        df = df.reset_index(drop=True)\n        df = breed_es(df,generation, n_generations)\n        gen_results[generation] = df['score'].head()\n        display(df)\n    return df\n\n#df = evaluate_es(df)\n#gen_results = pd.concat([gen_results, df.head()], axis = 1)\n#display(gen_results)\n#display(whole_population)","fb45be87":"file_name = \"..\/working\/gen_scores_es.csv\" # increment this per run that you save\ngen_results.to_csv(file_name)","faba80f7":"categorical_inputs = []\nfor cat in cat_cols:\n    categorical_inputs.append(Input(shape=[1], name=cat))\n\ncategorical_embeddings = []\nfor i, cat in enumerate(cat_cols):\n    categorical_embeddings.append(Embedding(embed_sizes[i], 10)(categorical_inputs[i]))\n\n#categorical_logits = Concatenate()([Flatten()(cat_emb) for cat_emb in categorical_embeddings])\ncategorical_logits = Flatten()(categorical_embeddings[0])\nprint(categorical_logits)\ncategorical_logits = Dense(32,activation='relu')(categorical_logits)\n\nnumerical_inputs = Input(shape=(11,), name='num')\nnumerical_logits = numerical_inputs\nnumerical_logits = BatchNormalization()(numerical_logits)\n\n#for i in range(6):\nnumerical_logits = Dense(128,activation=\"relu\")(numerical_logits)\nnumerical_logits = Dense(128,activation=\"relu\")(numerical_logits)\n\nlogits = Concatenate()([numerical_logits,categorical_logits])\nlogits = Dense(64,activation='relu')(logits)\nout = Dense(1, activation='sigmoid')(logits)\n\nmodel = Model(inputs = categorical_inputs + [numerical_inputs], outputs=out)\nmodel.compile(optimizer='adam',loss=binary_crossentropy)","62f9550d":"# Lets print our model\nmodel.summary()","8155e86c":"def get_input(market_train, indices):\n    X_num = market_train.loc[indices, num_cols].values\n    X = {'num':X_num}\n    for cat in cat_cols:\n        X[cat] = market_train.loc[indices, cat_cols].values\n    y = (market_train.loc[indices,'returnsOpenNextMktres10'] >= 0).values\n    r = market_train.loc[indices,'returnsOpenNextMktres10'].values\n    u = market_train.loc[indices, 'universe']\n    d = market_train.loc[indices, 'time'].dt.date\n    return X,y,r,u,d\n\n# r, u and d are used to calculate the scoring metric\nX_train,y_train,r_train,u_train,d_train = get_input(market_train, train_indices)\nX_valid,y_valid,r_valid,u_valid,d_valid = get_input(market_train, val_indices)","e902d86b":"\"\"\"\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\ncheck_point = ModelCheckpoint('model.hdf5',verbose=True, save_best_only=True)\nneural_net = model.fit(X_train,y_train.astype(int),\n          validation_data=(X_valid,y_valid.astype(int)),\n          epochs=3,\n          verbose=True,\n          callbacks=[check_point]) \nprint(neural_net.history['val_loss'])\n\"\"\"","26f9d782":"\"\"\"\n# distribution of confidence that will be used as submission\nmodel.load_weights('model.hdf5')\nconfidence_valid = model.predict(X_valid)[:,0]*2 -1\nprint(accuracy_score(confidence_valid>0,y_valid))\nplt.hist(confidence_valid, bins='auto')\nplt.title(\"predicted confidence\")\nplt.show()\n\"\"\"","e0c7a60c":"\"\"\"\n# calculation of actual metric that is used to calculate final score\nr_valid = r_valid.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = confidence_valid * r_valid * u_valid\ndata = {'day' : d_valid, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean \/ std\nprint(score_valid)\n\"\"\"","a4002ac1":"#days = env.get_prediction_days()","62ac8002":"\"\"\"\nn_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\npredicted_confidences = np.array([])\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    print(n_days,end=' ')\n    \n    t = time.time()\n\n    market_obs_df['assetCode_encoded'] = market_obs_df[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n\n    market_obs_df[num_cols] = market_obs_df[num_cols].fillna(0)\n    market_obs_df[num_cols] = scaler.transform(market_obs_df[num_cols])\n    X_num_test = market_obs_df[num_cols].values\n    X_test = {'num':X_num_test}\n    X_test['assetCode'] = market_obs_df['assetCode_encoded'].values\n    \n    prep_time += time.time() - t\n    \n    t = time.time()\n    market_prediction = model.predict(X_test)[:,0]*2 -1\n    predicted_confidences = np.concatenate((predicted_confidences, market_prediction))\n    prediction_time += time.time() -t\n    \n    t = time.time()\n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':market_prediction})\n    # insert predictions to template\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t\n\nenv.write_submission_file()\ntotal = prep_time + prediction_time + packaging_time\nprint(f'Preparing Data: {prep_time:.2f}s')\nprint(f'Making Predictions: {prediction_time:.2f}s')\nprint(f'Packing: {packaging_time:.2f}s')\nprint(f'Total: {total:.2f}s')\n\"\"\"","9766b2b0":"\"\"\"\n# distribution of confidence as a sanity check: they should be distributed as above\nplt.hist(predicted_confidences, bins='auto')\nplt.title(\"predicted confidence\")\nplt.show()\n\"\"\"","f4e48a6a":"# Market Data Only Baseline\n\nMarket data (2007 to present)\n\nNews data (2007 to present)\n\nvolume(float64) - trading volume in shares for the day\n\nclose(float64) - the close price for the day (not adjusted for splits or dividends)\n\nopen(float64) - the open price for the day (not adjusted for splits or dividends)\n\nreturnsClosePrevRaw1(float64) - return from previous day based on close prices\n\nreturnsOpenPrevRaw1(float64) - return form previous day based on open prices\n\nreturnsClosePrevMktres1(float64) - return from previous day based on close prices, adjusted to market movements\n\nreturnsOpenPrevMktres1(float64) - return from previous day based on open prices, adjusted to market movements\n\nreturnsClosePrevRaw10(float64) - return from previous 10 day based on close prices\n\nreturnsOpenPrevRaw10(float64) -  return from previous 10 day based on open prices\n\nreturnsClosePrevMktres10(float64) -  return from previous 10 day based on close prices, adjusted to market movements\n\nreturnsOpenPrevMktres10(float64) -  return from previous 10 day based on open prices, adjusted to market movements\n\nreturnsOpenNextMktres10(float64) - 10 day, market-residualized return. This is the target variable used in competition scoring. The market data has been filtered such that returnsOpenNextMktres10 is always not null.","a6309e68":"# Make sure you don't overwrite previous csv files!","8e354535":"# Handling categorical variables","f824fc50":"# Handling numerical variables","8e17f5e9":"# Define NN Architecture","70d4e37f":"Todo: add explanation of architecture","66b989cc":"# Evaluation of Validation Set","59dc24f5":"# Train NN model","9b6714f7":"# Prediction"}}