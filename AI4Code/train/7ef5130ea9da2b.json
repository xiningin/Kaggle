{"cell_type":{"bed2ea80":"code","dd7488a2":"code","e2d7d7fa":"code","97ad7cdd":"code","b2379ac9":"code","ffebb12c":"code","b1047cff":"code","8b9cf448":"code","fe9c6465":"code","ff12f105":"code","d0ba9d19":"code","670593bd":"code","af63b8e7":"code","35549423":"code","6f1904e9":"code","c89b13f1":"markdown","ec260413":"markdown","b98467af":"markdown","3f6aa570":"markdown","6b21223e":"markdown","480f4094":"markdown","6231cc80":"markdown","88db308f":"markdown","e5e64d3b":"markdown","9553b48e":"markdown","e850801f":"markdown","5ba6a128":"markdown","53b303f6":"markdown","20340f8c":"markdown"},"source":{"bed2ea80":"import pandas as pd\nimport numpy as np\n#importing data \ntrain=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n#creating a combined dataset \ntest['SalePrice']=999\ntrain['SalePrice']=np.log(train.SalePrice)\ntrain['train']=1\ntest['train']=0\ncombined=pd.concat((train,test),axis=0)","dd7488a2":"# checking for columns with missing values\nmissing_bool=combined.isnull().any()\nmissing_cols=missing_bool[missing_bool==True]\n\n#collecting data type of columns with missing values \ndtypes=train[missing_cols.index.tolist()].dtypes\n#collecting % of missing values for missing columns \nmissing_per=train[missing_cols.index.tolist()].isnull().sum()\/train.shape[0]\n#merging both\nmissing_table=pd.concat((dtypes,missing_per),axis=1)\nmissing_table","e2d7d7fa":"#replacing with median\nfloat_missing=dtypes[dtypes=='float64'].index.tolist()\nfor col in float_missing:\n    median_val=train[col].median()\n    combined[col]=combined[col].fillna(median_val)\n\n#replacing with mode and creating indicator for missing rows\nobj_missing=dtypes[dtypes=='object'].index.tolist()\nfor col in obj_missing:\n    mode_val=train[col].mode()[0]\n    combined[col+'_missingind']=np.where(combined[col].isnull(),1,0)\n    combined[col]=combined[col].fillna(mode_val)\n","97ad7cdd":"#converting character variabels to category codes   \ndtypes_all=combined.dtypes[combined.dtypes=='object'].index.tolist()\nfor col in dtypes_all:\n    combined[col]=combined[col].astype('category').cat.codes","b2379ac9":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nmodel=RandomForestRegressor(n_estimators=500,max_samples=0.8,min_samples_leaf=30,max_features='sqrt',oob_score=True)\ndep_var=[col for col in combined.columns.tolist() if col not in ['Id','SalePrice','train']]\nparams={'n_estimators':[100,300,400],'max_samples':[0.5,0.8],'max_features':['sqrt'],'min_samples_leaf':[30,60],'max_depth':[5,8,10]}\n#scoring function\nimport math\ndef rmse(x,y): return math.sqrt(((x-y)**2).mean())\nfrom sklearn.metrics import make_scorer\nnegative_rmse=make_scorer(rmse,greater_is_better=False)\n\ngrid=GridSearchCV(model,param_grid=params,scoring='r2')\ngrid.fit(combined[combined.train==1][dep_var],combined[combined.train==1]['SalePrice'])","ffebb12c":"#variable reduction using feature importance\ngrid.best_params_","b1047cff":"# variable reduction using only feature importance and dropping one element every step\nbase_var=dep_var.copy()\nperformance={}\nmax_perf=-1\nwhile(len(base_var)>=5):\n    model=RandomForestRegressor(max_depth= 10,max_features='sqrt',max_samples=0.8,min_samples_leaf=30,n_estimators=400,oob_score=True)\n    model.fit(combined[combined.train==1][base_var],combined[combined.train==1]['SalePrice'])\n    sorted_idx=np.argsort(-model.feature_importances_)\n    #storing oob score for diagonstic purposes\n    performance[len(base_var)]=model.oob_score_\n    if model.oob_score_>max_perf:\n        best_model=model\n        max_perf=model.oob_score_\n        best_var=base_var.copy()\n    #removing list important variable from the predictors list\n    base_var.pop(sorted_idx[-1])","8b9cf448":"from matplotlib import pyplot as plt\nfig=plt.figure()\nfig.canvas.draw()\n# first plot -dropping one variable at a time\nax1=fig.add_subplot(111, label=\"1\")\nxvalues1=[i for i in range(0,98)]\nyvalues1=[performance[102-i] for i in range(0,98)]\nax1.plot(xvalues1,yvalues1)\nax1.set_xlabel(\"#variables in model\")\nax1.set_ylabel(\"R-square on OOB sample\")\nnew_labels = [103-(i-1)*20 for i in range(6)]\n\nax1.set_xticklabels(labels=new_labels)\nplt.show()","fe9c6465":"base_model=RandomForestRegressor(max_depth= 10,max_features='sqrt',max_samples=0.8,min_samples_leaf=30,n_estimators=400,oob_score=True)\nbase_model.fit(combined[combined.train==1][dep_var],combined[combined.train==1]['SalePrice'])\nimportance=base_model.feature_importances_\nsorted_idx=np.argsort(-importance)\nimportance_dict={}\nfor idx in sorted_idx:\n    importance_dict[dep_var[idx]]=importance[idx]","ff12f105":"#variable reduction using feature importance and correlation\n#cycle1\ncurr_var=list(importance_dict.keys())\nthreshold_list=[0.8-i*0.1 for i in range(6)]\nmetrics=[]\nfor threshold in threshold_list:\n    for idx,var1 in enumerate(curr_var):\n        for var2 in curr_var[idx+1:]:\n            corr=combined[combined.train==1][var1].corr(combined[combined.train==1][var2])\n            if corr>threshold:\n                curr_var.remove(var2)\n    model=RandomForestRegressor(max_depth= 10,max_features='sqrt',max_samples=0.8,min_samples_leaf=30,n_estimators=400,oob_score=True)\n    model.fit(combined[combined.train==1][curr_var],combined[combined.train==1]['SalePrice'])\n    metrics.append( (threshold,len(curr_var),model.oob_score_) )","d0ba9d19":"metrics","670593bd":"#dendrogram \nimport scipy\nfrom scipy.cluster import hierarchy as hc\ncorr = np.round(scipy.stats.spearmanr(combined[combined.train==1][best_var]).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,10))\ndendrogram = hc.dendrogram(z, labels=best_var, \n      orientation='left', leaf_font_size=16)\nplt.show()","af63b8e7":"def get_oob(drop_var):\n    m = RandomForestRegressor(max_depth= 10,max_features='sqrt',\n                              max_samples=0.8,min_samples_leaf=30,n_estimators=400,oob_score=True)\n    pred_var=[var for var in best_var if var is not drop_var]\n    m.fit(combined[combined.train==1][pred_var],combined[combined.train==1]['SalePrice'])\n    return m.oob_score_\n\nget_oob('KitchenQual')","35549423":"from sklearn.decomposition import PCA\n\ndef pca_get_oob(n):\n    pca=PCA(n_components=n)\n    pca_data=pca.fit_transform(combined[combined.train==1][dep_var])\n    pca_df=pd.DataFrame(data=pca_data,columns=['pca'+str(i) for i in range(n)])\n    m = RandomForestRegressor(max_depth= 10,max_features='sqrt',\n                              max_samples=0.8,min_samples_leaf=30,n_estimators=400,oob_score=True)\n    m.fit(pca_df,combined[combined.train==1]['SalePrice'])\n    return m.oob_score_\n\npca_rsquare=[]\nfor i in range(30,90):\n    pca_rsquare.append(pca_get_oob(i))","6f1904e9":"from matplotlib import pyplot as plt\nplt.plot(range(30,90),pca_rsquare)\nplt.xlabel('n components')\nplt.ylabel('Rsquare on OOB sample')\nplt.show()","c89b13f1":"# <a id='1'> 1. Importing Data <a> ","ec260413":"# Random Forest : Variable reduction techniques","b98467af":"# <a id='5'>5.Variable reduction by sequentially dropping least important feature <a><br>","3f6aa570":"# <a id='7'>7.Using Dendrogram to drop redundant variables<a>","6b21223e":"# <a id ='8'>8. Performance using PCA <a><br>","480f4094":"# <a id='3'> 3. Converting Character columns to numeric <a> <br>","6231cc80":"Principal component analysis is although mathematically robust dimensionality reduction technique, the biggest disadvantage is the loss of interpretability. We can't interpret the variables in the model as PCA original variables aren't present in the model. The following graph shows the performance against number of components in PCA.","88db308f":"In this methodology, we use correlation in conjunction with feature importance to drop variables. Computationally this methodology is faster than the earlier method. Algorithm is as below:","e5e64d3b":"This Technique is taught by Jeremy Howard in the Introduction to Machine learning course as a part of fastai. This methodology is based on hierarchical clustering, in which we look at every pair of features and say which two are closest. We then take the closest pair, delete them, and replace them with the midpoint of the two. Then repeat that again and again. we can plot a dendrogram based on this","9553b48e":"# <a id='2'>2. Missing Value Treatment<a>","e850801f":"# <a id='4'> 4. Baseline Model <a>","5ba6a128":"# <a id ='6'>6. Variable reduciton using feature importance in conjuction with correlation <a><br>","53b303f6":"we develop a model on all variables and sort variables by feature importance. Then, iteratively drop the least important variable and record performance until only one variable is present. If you feel dropping a single variable at a time is computationally expensive, we can drop multiple variables in one step to save time. Later, we can plot performance vs number of features and decide which model to finalize.","20340f8c":"<a href='#1'>1.Importing data<a> <br>\n<a href='#2'>2.Missing Value treatment<a><br>\n<a href='#3'>3. Converting Character columns into numeric type <a><br>\n<a href='#4'>4. Baseline Model <a><br>\n<a href='#5'>5.Variable reduction by sequentially dropping least important feature <a><br>\n<a href ='#6'>6. Variable reduciton using feature importance in conjuction with correlation <a><br>\n<a href='#7'>7.Using Dendrogram to drop redundant variables<a><br>\n<a href='#8'>8. Performance using PCA <a><br>"}}