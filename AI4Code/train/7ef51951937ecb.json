{"cell_type":{"57847c0e":"code","fd0f92f4":"code","ce83d6d3":"code","f4fcb0e0":"code","04c3fba2":"code","3f7c5ad4":"code","a1222c20":"code","befbbe5f":"code","9e1b10c9":"code","8d58419f":"code","5f499b94":"code","5b70ceb1":"code","f57e363f":"code","2baef874":"code","16c2df36":"code","4a561ad6":"code","14b72c12":"code","ec05d1c0":"code","8e7266ca":"code","cfeba9c9":"code","ebfe8d97":"code","fb494778":"code","48759149":"code","ddba94d6":"code","964afa71":"code","b2db62a2":"code","e883adee":"code","c2d0845d":"code","cb5a7c21":"code","d4ae3760":"code","5d83532f":"code","9ff26e06":"code","76515eb3":"code","3e482779":"code","1ec89099":"code","866f33aa":"code","30591e4a":"markdown","b04cc678":"markdown","2913199b":"markdown","6907c215":"markdown","d3570238":"markdown","a945a0ce":"markdown","831b2ec2":"markdown","0b1801cb":"markdown","292b0eb6":"markdown","3507505f":"markdown","0c1fbd56":"markdown","bcde6452":"markdown","ef54b5de":"markdown","767a96c5":"markdown","e3cf000d":"markdown","cfb31d26":"markdown","d195f893":"markdown"},"source":{"57847c0e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fd0f92f4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pandas import plotting\n\n#plotly \nimport plotly.offline as py\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\ninit_notebook_mode(connected=True)\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn import neighbors\nfrom sklearn.metrics import confusion_matrix,classification_report,precision_score\nfrom sklearn.model_selection import train_test_split\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nsns.set(style=\"whitegrid\")","ce83d6d3":"df=pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.head()","f4fcb0e0":"df=df.drop('Unnamed: 32', axis=1)\n","04c3fba2":"from scipy import stats\n","3f7c5ad4":"r, p=stats.pearsonr(df.loc[:,'concavity_worst'], df.loc[:,'concave points_worst'])\ngraph=sns.jointplot(df.loc[:,'concavity_worst'], df.loc[:,'concave points_worst'], kind=\"reg\", color=\"#ce1414\",)\nphantom, =graph.ax_joint.plot([],[], linestyle=\"\", alpha=0)\ngraph.ax_joint.legend([phantom],['r={:f}, p={:f}'.format(r,p)])\nplt.show()","a1222c20":"diagnosis={'M':1, 'B':0}\ndf['diagnosis']=[diagnosis[x] for x in df['diagnosis']]","befbbe5f":"col=['id', 'diagnosis']\nX=df.drop(col, axis=1)\ny=df['diagnosis']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","9e1b10c9":"X.shape, y.shape","8d58419f":"X.info()","5f499b94":"y.dtype","5b70ceb1":"from sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix,ConfusionMatrixDisplay\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf","f57e363f":"X_train.shape, y_train.shape","2baef874":"lr=LogisticRegression(solver=\"liblinear\")\nlr.fit(X, y)\n","16c2df36":"col=['radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']","4a561ad6":"importance = lr.coef_[0]\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.figure(figsize=(10,10))\nplt.bar(X.columns, importance)\nplt.xticks(rotation=90)\nplt.show()","14b72c12":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)","ec05d1c0":"lr=LogisticRegression(solver=\"liblinear\")\nlr.fit(X, y)","8e7266ca":"importance = lr.coef_[0]\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.figure(figsize=(10,10))\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","cfeba9c9":"col=['id', 'diagnosis']\nX=df.drop(col, axis=1)\ny=df['diagnosis']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","ebfe8d97":"from sklearn.tree import DecisionTreeClassifier\n","fb494778":"model = DecisionTreeClassifier()\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.figure(figsize=(15,15))\nplt.bar(X.columns, importance)\nplt.xticks(rotation=90)\n\nplt.show()","48759149":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)","ddba94d6":"model = DecisionTreeClassifier()\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.figure(figsize=(15,15))\nplt.bar([x for x in range(len(importance))], importance)\nplt.xticks(rotation=90)\n\nplt.show()","964afa71":"col=['id', 'diagnosis']\nX=df.drop(col, axis=1)\ny=df['diagnosis']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","b2db62a2":"from sklearn.ensemble import RandomForestClassifier\n","e883adee":"model = RandomForestClassifier()\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.figure(figsize=(15,15))\nplt.bar(X.columns, importance)\nplt.xticks(rotation=90)\n\nplt.show()","c2d0845d":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)","cb5a7c21":"model=RandomForestClassifier()\nmodel.fit(X, y)","d4ae3760":"importance = model.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.figure(figsize=(10,10))\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","5d83532f":"col=['id', 'diagnosis']\nX=df.drop(col, axis=1)\ny=df['diagnosis']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","9ff26e06":"from xgboost import XGBClassifier\n","76515eb3":"model = XGBClassifier()\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.figure(figsize=(15,15))\nplt.bar(X.columns, importance)\nplt.xticks(rotation=90)\n\nplt.show()","3e482779":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import permutation_importance","1ec89099":"col=['id', 'diagnosis']\nX=df.drop(col, axis=1)\ny=df['diagnosis']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","866f33aa":"model = KNeighborsClassifier()\n# fit the model\nmodel.fit(X, y)\n# perform permutation importance\nresults = permutation_importance(model, X, y, scoring='neg_mean_squared_error')\n# get importance\nimportance = results.importances_mean\nfor i,v in enumerate(importance):\n    print('Feature: %d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.figure(figsize=(15,15))\nplt.bar(X.columns, importance)\nplt.xticks(rotation=90)\n\nplt.show()","30591e4a":"# Decision Tree","b04cc678":"# Logistic Regresion","2913199b":"# Conclusion\n* Standarization affect the features importance of the dataset\n* As in above graph we can see that there wwre few features whose values were either too negative or positive. Rest of the features had almost 0\n* After standarization we can see that a lot of the features wholse coefficeint values are much better. \n* Here negative values indicate that it tries to push the model towards the negative side\n* Same case with the positive value which tends to push the model in positive side.","6907c215":"# Joint PLot with Pearson Coefficent","d3570238":"# Conclusion\n* It is same as done above \n* Without and with standarization has less impact or almost no impact on feature importance","a945a0ce":"# Feature imporatnce after Standarization","831b2ec2":"# Conclusion\n* The most important features are \"radius_worst\"\n* The lkist of the important featuires are --Radius_worst, texture_worst, concave point worst, texture mean, concavity features, etc\n* Those features wholse value is almost equal to 0 , are least important features. \n* Least features can be removed form the dataset and the dataset will be used for prediction. ","0b1801cb":"# Feature importance after the standarization ","292b0eb6":"# Feature importance before Standarization","3507505f":"# Permutation Feature Importance\n* technique for calculating relative importance scores that is independent of the model used.\n* First, a model is fit on the dataset, such as a model that does not support native feature importance scores. \n* This approach can be used for regression or classification and requires that a performance metric be chosen as the basis of the importance score, such as the mean squared error for regression and accuracy for classification.\n","0c1fbd56":"# Random Forest","bcde6452":"# XGBoost Feature Importance","ef54b5de":"# Conlusion\n* Since we are getting same number of the important features.\n","767a96c5":"# Conclusion\n* The most important features is \"Concave Point Wrost\"\n* The list of the most imporatnt feature that we obtained using Random Forest Classifier are----Perimetr Wrost, Concave point worst, radius worst, concave point mean, area worst, texture mean etc.\n* SInce we can see that there are llarge number of important features ae available\n* Those feature which have least value can be removed from the dataset. ","e3cf000d":"# Feature Importance\n*  techniques that assign a score to input features based on how useful they are at predicting a target variable\n* There are many types and sources of feature importance scores, although popular examples include statistical correlation scores, coefficients calculated as part of linear models, decision trees, and permutation importance scores\n* Feature importance scores play an important role in a predictive modeling project, including providing insight into the data, insight into the model\n* Feature importance refers to a class of techniques for assigning scores to input features to a predictive model that indicates the relative importance of each feature when making a prediction\n* Feature importance scores can be calculated for problems that involve predicting a numerical value, called regression, and those problems that involve predicting a class label, called classification.","cfb31d26":"# Conclusion\n* The most imprtant features is Area worst\n* The 2nd most important feature is Area Mean\n* as we can see that there is only 2 features which is important","d195f893":"# Classification\n* The most important features is Radius Worst\n* The number of important features decreaseas.\n* As we can see that Perimeter Worst is 2nd important features."}}