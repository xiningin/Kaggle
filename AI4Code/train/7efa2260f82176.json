{"cell_type":{"3f00713d":"code","622b3004":"code","e4b4152e":"code","58cbb4cf":"code","092b42b2":"code","61e8d2d4":"code","5833489f":"code","d2f40cca":"code","73ae408e":"code","23eb70a0":"code","f29ee205":"code","dd13b2f8":"code","a5fa4723":"code","313c0c0a":"code","e018b47d":"code","07a55e74":"code","0f9c0ce8":"code","755b1c6a":"code","b68acca1":"code","85dfad09":"code","23200bd9":"code","3cad9faf":"code","f136c454":"code","c04bce71":"code","d99cbf4d":"code","793eb899":"code","990914c2":"code","cbd32975":"code","d1227a07":"code","24d6612c":"code","e9759b1f":"code","e095eb7e":"code","589e5691":"code","ac4e2b32":"code","0a5ffb9a":"code","01a7c195":"code","6bf4be14":"code","65ea77a6":"code","b9e6a36a":"markdown","d0877f66":"markdown","6b52ed64":"markdown","74bc4a7e":"markdown","ab428879":"markdown","8f9f73cd":"markdown","23c9d06d":"markdown","90fef25f":"markdown","ad71dc8c":"markdown","4023deaa":"markdown","2b3ac47b":"markdown","71639f14":"markdown","8f9cc42b":"markdown","a6fb739d":"markdown","098f632b":"markdown","26676706":"markdown","fe74737e":"markdown","f3ec723d":"markdown","84039003":"markdown","3e252a8d":"markdown","8cefce4c":"markdown","07892367":"markdown","2a1ba3a0":"markdown","518986ba":"markdown","aec932e1":"markdown","883f7c87":"markdown","346a507b":"markdown","33f31347":"markdown","56a853b3":"markdown","7be1c58e":"markdown"},"source":{"3f00713d":"import pandas as pd\n\nimport json\nfrom tqdm.notebook import tqdm\n\n%matplotlib inline\n\n%load_ext autoreload\n%autoreload 2","622b3004":"# PARAMETERS\n\nmode = 0 # 0 to run in kaggle, 1 to run locally\n\ncorpus_paths = ['\/kaggle\/input\/CORD-19-research-challenge\/', '..\/data\/CORD-19-research-challenge\/']\nresults_paths = ['\/kaggle\/input\/covid19-ie-from-literature-precomputed-results\/', '..\/results\/']\ncovid_master_df_filtered_paths = ['\/kaggle\/input\/covid19-ie-from-literature-processed-triplets\/', '..\/results\/']\n\ncorpus_path = corpus_paths[mode]\nresults_path = results_paths[mode]\ncovid_master_df_file = results_path+'covid_master_df.csv\/covid_master_df.csv'\ncovid_master_df_filtered_file = covid_master_df_filtered_paths[mode]+'covid_master_df_filtered.csv'\n\n# SAVING LITERATURE REVIEW TABLES\ndef save_to_csv(df, filename): \n    foldername = [\n        '\/kaggle\/working\/',\n        '..\/results\/'\n    ]\n    foldername = foldername[mode]\n    \n    df.to_csv(foldername+filename)","e4b4152e":"# from create_corpus_dataframe import create_dataframes\n\n# df = create_dataframes(corpus_path, results_path)","58cbb4cf":"# Loading pre-computed dataframe\ndf = pd.read_csv(results_path+'all_articles_content.csv\/all_articles_content.csv', index_col=False)\ndf.sort_index().head()","092b42b2":"def create_entity_df_dict(entity_list, corpus_df):\n    entity_dfs = {}\n    for entity in entity_list:\n        entity_dfs[entity] = corpus_df[\\\n                                       (corpus_df['content'].str.lower().str.contains(entity))|\\\n                                       (corpus_df['section'].str.lower().str.contains(entity))|\\\n                                       (corpus_df['title'].str.lower().str.contains(entity))\n                                      ].copy()\n        print('{}: {} articles found'.format(entity, len(entity_dfs[entity])))\n    return entity_dfs\n\ndef combine_df_dict(entity_dfs):\n    combined_df = pd.DataFrame()\n    for df in entity_dfs.values():\n        combined_df = pd.concat([combined_df, df])\n    combined_df.drop_duplicates(inplace=True)\n    print('Total articles: {}'.format(len(combined_df)))\n    return combined_df","61e8d2d4":"covid_ent_list = ['coronavirus disease', 'covid-19', 'severe acute respiratory syndrome coronavirus 2', 'sars-cov-2']","5833489f":"# Storing the dataframes for each concept of interest\ncovid_dfs = create_entity_df_dict(covid_ent_list, df)","d2f40cca":"# Combining the dataframes\ncovid_combined_df = combine_df_dict(covid_dfs)\ncovid_combined_df.sort_index().head()","73ae408e":"# from openie import StanfordOpenIE\n\n# def trim(annotations):\n#     for sent in annotations['sentences']:\n#         del sent['basicDependencies']\n#         del sent['enhancedDependencies']\n#         del sent['enhancedPlusPlusDependencies']\n#     return annotations\n\n# def create_annotations_list(text_list):\n#     annotations_list = []\n#     with StanfordOpenIE() as client:\n#         for text in tqdm(text_list):\n#             if isinstance(text, str):\n#                 annotations = client.annotate(text, simple_format=False)\n#                 annotations = trim(annotations)\n#                 annotations_list.append(annotations)\n#             elif isinstance(text, list):\n#                 body_annots = []\n#                 for paragraph in text:\n#                     annotations = client.annotate(paragraph, simple_format=False)\n#                     annotations = trim(annotations)\n#                     body_annots.append(annotations)\n#                 annotations_list.append(body_annots)\n#             else:\n#                 print('Wrong object passed to Information Extractor')\n#     return annotations_list\n\n\n# def create_covid19_triplets(text_list, covid_combined_df, covid_full_triplet_file):\n\n#     annotations_list = create_annotations_list(text_list)\n#     print(len(annotations_list))\n#     with open(covid_full_triplet_file, 'w') as f:\n#         json.dump(annotations_list, f)\n\n#     # Creating a lite version of the dictionary including just triplets as a column for the dataframe\n#     annotations_column = []\n#     for row in annotations_list:\n#         row_ann_list = []\n#         for sentence in row['sentences']:\n#             for triplet_d in sentence['openie']:\n#                 row_ann_list.append({k: triplet_d[k] for k in ('subject', 'relation', 'object')})\n#         annotations_column.append(row_ann_list)\n#     # Adding to the dataframe\n#     covid_combined_df['triplets'] = annotations_column\n#     # Exporting\n#     covid_combined_df.to_csv(covid_master_df_file)\n\n#     return covid_combined_df\n\n\n# df = create_covid19_triplets(covid_combined_df['content'], covid_combined_df, covid_full_triplet_file)","23eb70a0":"# Loading precomputed dataframe including all triplets for each section of the journal articles\ncovid_master_df = pd.read_csv(covid_master_df_file)\ncovid_master_df.head()","f29ee205":"import covid as cv","dd13b2f8":"# Create `ArticleFilter` instance and showing some relevant information\naf = cv.ArticleFilter(covid_master_df_file)\naf.info()","a5fa4723":"# af.removeDuplicateTriplets()\n# af.info()\n# af.cdf.to_csv(covid_master_df_filtered_file)","313c0c0a":"# Importing dataframe with filtered triplets\naf = cv.ArticleFilter(covid_master_df_filtered_file)\naf.info()","e018b47d":"af.cdf.sort_index().head()","07a55e74":"af.generateTripletsDB()","0f9c0ce8":"af.triplets.sort_index().head()","755b1c6a":"filter = {'section': 'history', 'triplet':'disease|care|public|prevention|transmission|infection|trials'}\naf.search(filter, html=True, max_items=7)","b68acca1":"filter = {'content': 'replication', 'subject':'naproxen|clarithromycin|minocycline|viral inhibitor'}\nst_df = af.search(filter, html=True)","85dfad09":"filename = 'Effectiveness of drugs being developed and tried to treat COVID-19 patients.csv'\nsave_to_csv(st_df,filename)\nst_df","23200bd9":"filter = {'content': 'ADE' , 'subject':'antibody-dependent enhancement'}\nst_df = af.search(filter, html=True)","3cad9faf":"filename = 'Methods evaluating potential complication of Antibody-Dependent Enhancement in vaccine recipients.csv'\nsave_to_csv(st_df,filename)\nst_df","f136c454":"filter = {'title':'vaccine', 'triplet':'animal|dog|mouse|cat|vamp|bird'}\nst_df = af.search(filter, html=True)","c04bce71":"filename = 'Exploration of use of best animal models and their predictive value for a human vaccine.csv'\nsave_to_csv(st_df,filename)\nst_df","d99cbf4d":"filter = {'content': 'therapeutic|antiviral', 'subject':'therapeutic'}\nst_df = af.search(filter, html=True)","793eb899":"filename = 'Capabilities to discover a therapeutic (not vaccine) for the disease.csv'\nsave_to_csv(st_df,filename)\nst_df","990914c2":"filter = {'content': 'therapeutic|antiviral|distribute|production', 'subject':'new therapeutic|scarce therapeutic'}\nst_df = af.search(filter, html=True)","cbd32975":"filename = 'Alternative models to aid decision makers in determining how to prioritize and distribute scarce newly proven therapeutics.csv'\nsave_to_csv(st_df,filename)\nst_df","d1227a07":"filter = {'content': 'vaccination|vaccine|universal vaccine',\\\n          'subject':'vaccination|vaccine|universal vaccine'}\nst_df = af.search(filter, html=True)","24d6612c":"filename = 'Efforts targeted at a universal coronavirus vaccine.csv'\nsave_to_csv(st_df,filename)\nst_df","e9759b1f":"filter = {'content': 'animal model|challenge study|challenge studies|standardize|standardise',\\\n          'subject':'animal models|challenge study|challenge studies'}\nst_df = af.search(filter, html=True)","e095eb7e":"filename = 'Efforts to develop animal models and standardize challenge studies.csv'\nsave_to_csv(st_df,filename)\nst_df","589e5691":"filter = {'content': '', 'subject':'healthcare worker|doctor|nurse|front-line|prophylaxis'}\nst_df = af.search(filter, html=True)","ac4e2b32":"filename = 'Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers.csv'\nsave_to_csv(st_df,filename)\nst_df","0a5ffb9a":"filter = {'content': 'risk after vaccination|enhanced disease', 'subject':'risk|vaccine|enhanced disease'}\nst_df = af.search(filter, html=True)","01a7c195":"filename = 'Approaches to evaluate risk for enhanced disease after vaccination.csv'\nsave_to_csv(st_df,filename)\nst_df","6bf4be14":"filter = {'content': 'vaccine', 'subject':'assay'}\nst_df = af.search(filter, html=True)","65ea77a6":"filename = 'Assays to evaluate vaccine immune response and process development for vaccines.csv'\nsave_to_csv(st_df,filename)\nst_df","b9e6a36a":"## Generating Triplet Database to answer questions\n\nWe will finally create a concise dataframe with triplet elements that will be used by the subroutine that filters through the literature.\n\nGenerating this will take a minute or two.","d0877f66":"# What has been published about vaccines and therapeutics against COVID-19?\n\n\u00a9 2020 Nokia.\nLicensed under the BSD 3-Clause License.\nSPDX-License-Identifier: BSD-3-Clause.\n\n### Goal\nThere is a growing urgency to facilitate the assimilation of the rapidly increasing literature on COVID-19 by the scientific community.\nWe aim to address this need by automatically extracting insights about key medical entities from the provided literature corpus using Information Extraction techniques and Knowledge Graphs representations.\nOur approach can be applied to any medical entity, as it builds a knowledge graph around the concepts of interest.\nHowever, we submit it as an answer to \"Task 4: What do we know about vaccines and therapeutics?\" as we apply our methodology to answer the 10 questions of said task.\n\n### Information Extraction Methodology\n0. Structure the literature corpus as a dataframe of article sections\n1. Keyword search to identify articles that contain entities of interest: COVID-19\n2. Information Extraction using Stanford's OpenIE to extract IE triplets: (`object`, `relation`, `subject`)\n3. Removing redundant triplets in each article section\n4. Generating Triplet Database to answer questions\n5. Report creation by building a Knowledge Graph and providing structured insights\n\n### Acknowledgements\n* Ivan Ega Pratama. [Dataset Parsing Code | Kaggle, COVID EDA: Initial Exploration Tool](https:\/\/www.kaggle.com\/ivanegapratama\/covid-eda-initial-exploration-tool).\n* Manning, Christopher D., Surdeanu, Mihai, Bauer, John, Finkel, Jenny, Bethard, Steven J., and McClosky, David. 2014. The Stanford CoreNLP Natural Language Processing Toolkit In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60. [Open IE](https:\/\/stanfordnlp.github.io\/CoreNLP\/openie.html)\n* jezrael. [Pandas - Check if a value in a column is a substring of another value in the same column](https:\/\/stackoverflow.com\/a\/58951442)\n* Ishamael. [Finding subsequence (nonconsecutive)](https:\/\/stackoverflow.com\/a\/29954829)\n* Marius Borcan: [A Python implementation of a basic Knowledge Graph](https:\/\/github.com\/bdmarius\/python-knowledge-graph)\n* Shahules786: [CORD: Tools and Knowledge graphs\ud83d\udee0\ufe0f \ud83d\udee0\ufe0f](https:\/\/www.kaggle.com\/shahules\/cord-tools-and-knowledge-graphs)\n* Rathachai (used in the external version of the tool only): [D3RDF](https:\/\/github.com\/Rathachai\/d3rdf)\n\n### Challenge Submissions\nThe present notebook consititutes our submission to the Task \"What do we know about vaccines and therapeutics?\".\n\nWe also present [Kaggle community contributions](https:\/\/www.kaggle.com\/covid-19-contributions) for each of the three categories:\n* **Literature Review**: This notebook outputs answers to the questions in the task in a standard CSV format as it is being done in the Kaglle community\n* **Tools**: We offer our search capability that creates a knowledge graph and associated report as an interactive tool: http:\/\/covid19search.net\/\n* **Datasets**: We offer our structured dataset of extracted triplets (`object`, `relation`, `subject`) for each relevant section in the literature corpus: https:\/\/www.kaggle.com\/enriquemartinlopez\/covid19-ie-from-literature-processed-triplets","6b52ed64":"We have implemented a general solution that provides an end-to-end methodology to extract information of interest from a text corpus.\nBy applying it to articles related to Covid-19, our tool can provide any information of interest in a concise, structured manner (Knowledge Graph representation and CSV table with relevant subject-relation-object triplets) given a keyword search by the user.\nWe apply our solution on the Covid-19 subset of articles to extract information of interest for the Challenge Task \"What do we know about vaccines and therapeutics?\".\n\nWe would like to emphasize the flexibility of our approach, which is based on dividing the corpus of articles in article sections, and within those, extracting and indentifying relevant triplets.\n\nOur solution provides:\n- A flexible framework for information retrieval from a corpus of scientific articles\n- Extraction of triplets per article section, providing full traceability back to the source article(s)\/section(s)\n- Keyword search based on both title\/section\/content of the articles and extracted triplets (object, relation, subject)\n- Knowledge Graph representation of the results\n- Graphical front-end as implemented in our externally hosted tool http:\/\/covid19search.net\n\n\nWe also identify some current weaknesses that we can improve and iterate in next revisions:\n- Expand the search to articles in the corpus that do not explicitly mention the Covid-19 keywords\n- Improve the flexibility of the search approach by adding more logical operators, at the moment the different keys of the filter dictionary are combined in an AND fashion, and the values only allow OR opperations\n- Usage of NLP techniques from lemmatization, stemming and even embedding feature representations to obtain better results than using keyword searches and being able to normalize triplet entities\n- Exploit the Knowledge Graph representation by leveraging transitive relations between the nodes, this would require good normalization of the triplet entities in the fashion indicated above\n\nThe usage of more sophisticated NLP would help combat the main limitation of the current approach: much of the ability to find relevant answers to these questions rely on a skilful selection of the keywords. In cases where the keywords are nouns that are highly specific medical terms (e.g. naproxen, ADE) it is much more straightforward to extract relevant information. However, when the questions are asked around more general nouns (e.g. production, healthcare workers) it is more challenging to find specific results. The sub-graph produced by the search algorithm becomes non-sparse due to the use of these general nouns in a wide-variety of contexts, that may not necessarily relate to the question being answered. Moreover, a large number of synonyms for one given keyword can present time-constraint challenges to performing a full search on the knowledge graph.\n\nDue to these limitations we were not able to find direct answers to the posed questions. Here are some hand-picked examples of triplets related to the questions, although they do not directly provide the answer. However, they can be traced back to the corresponding article section in the HTML report or using the provided triplet tables.","74bc4a7e":"Let us inspect the resulting dataframe","ab428879":"# Q1\/10: Effectiveness of drugs being developed and tried to treat COVID-19 patients.\n\n### Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin and minocycline that may exert effects on viral replication.","8f9f73cd":"## Keyword search to identify articles that contain entities of interest: COVID-19\n\nWe provide the function `create_entity_df_dict`,which takes a list of entities, together with the dataframe created above, to produce a dictionary that maps each entity to a dataframe of articles that contain said entity.\n\nThe function `combine_df_dict` combines the dataframes for each entities of the entities to return a single dataframe with the same shape as the one above.\n\n**This method can be applied to filter the corpus by any list of concepts of interest.**\nHowever, for this notebook submission we will focus on the literature on COVID-19.","23c9d06d":"We create the `covid` library including the `ArticleFilter` class that we will use to remove redundant triplets and create reports that answer the questions in this task.","90fef25f":"#### Q1\/10: Effectiveness of drugs being developed and tried to treat COVID-19 patients.\n\n* New broad-spectrum anti-coronavirai\n* Is with \n* Minimum side-effects\n\n#### Q2\/10 Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\n\n* Antibody-dependent enhancement  \n* be cause of \n* Vaccine failure\n\n#### Q3\/10 Exploration of use of best animal models and their predictive value for a human vaccine.\n\n* Clinical manifestations\n* Can vary widely between\n* Animal models\n\n#### Q4\/10 Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\n\n* Therapeutics\n* Specific against\n* COVID-19\n\n#### Q5\/10 Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.\n\n* New therapeutic measures\n* Thus are needed for\n* Treatment of ICU patients\n\n#### Q6\/10 Efforts targeted at a universal coronavirus vaccine.\n\n* Inactivated vaccine\n* Is available for\n* Control of canine coronavirus infection\n\n#### Q7\/10 Efforts to develop animal models and standardize challenge studies\n\n* Animale models \n* Uncover\n* Mechanism of viral pathogenicity from entrance to transmission\n\n#### Q8\/10 Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers\n\n* Front-line healthcare workers\n* Are vulnerable to \n* emotional impact of coronavirus Maunder et.\n\n#### Q9\/10 Approaches to evaluate risk for enhanced disease after vaccination\n\n* Vaccines \n* Attenuated\n* Parainfluenza virus type 3\n\n#### Q10\/10 Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics]\n\n* Synthetic peptide vaccine approach\n* Sometimes be more effective than\n* Traditional vaccine methods","ad71dc8c":"## Report creation by building a Knowledge Graph and providing structured insights\n\nWe will create a knowledge graph with directed edges from `subject` to `object` that are labelled by the corresponding `relation`.\n\nThe user will provide a set of search keywords that are relevant to the question.\nSuch keywords are structured to filter the Covid-19 literature dataframe based on the string content of the `title`, `section`, and `content`; and also the string content of the `subject`, `relation` , and `object` of the corresponding triplets.\n\nWe acknowledge more sophisticated approaches to achieve this with greater accuracy, but this serves our purpose of building the methodology end-to-end to then improve certain parts with more sophisticated NLP methods.\n\nThe search function performs the following steps:\n\n1) create a new DataFrame object that combines together the article sections and triplets, as obtained from steps outlined in the previous sections .\n\n2) search through the specifically targeted columns of this new DataFrame to find the mentions of the keywords passed on to the function.\n\n3) extract only the contents where the keywords are present to create a graph with relevant information.\n\nLet us see an example, taken somehow arbitrarily from a discussion from this challenge: \"Epidemiologist available to hand-code records for training sets\".\nThis somehow illustrates the generality of the method. In particular, an epidemiologist [proposes the following question](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/137027#778986) as interesting:","4023deaa":"# Discussion: Pros and Cons of this approach","2b3ac47b":"# Q8\/10: Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers","71639f14":"We obtain a dataframe that organises the literature corpus with one article **section** per row, storing its **content** and keeping track of both its **title** and **paper_id**.","8f9cc42b":"# Q2\/10 Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients","a6fb739d":"### \"Efforts to define the natural history of disease to inform clinical care, public health interventions, infection prevention control, transmission, and clinical trials\"","098f632b":"## Removing redundant triplets in each article section\n\nStanford's OpenIE tool attempts to extract all possible triplets present in a piece of text. However, this may lead to situations in which some of the extracted triplets are redundant.\nFor instance, take the following sentence:\n> Several studies have reported the typical lung features of the disease on chest CT.\n\nOpenIE will extract, amongst others, the following triplets for this sentence:\n> {\n    \"subject\": \"several studies\",\n    \"relation\": \"have reported\",\n    \"object\": \"lung features of disease\"\n  }\n\n> {\n    \"subject\": \"studies\",\n    \"relation\": \"have reported\",\n    \"object\": \"typical lung features of disease\"\n  }\n  \n> {\n    \"subject\": \"several studies\",\n    \"relation\": \"have reported\",\n    \"object\": \"typical lung features of disease\"\n  }\n  \n> {\n    \"subject\": \"several studies\",\n    \"relation\": \"have reported\",\n    \"object\": \"lung features on chest CT.\"\n  }\n  \n> {\n    \"subject\": \"studies\",\n    \"relation\": \"have reported\",\n    \"object\": \"lung features of disease on chest CT.\"\n  }\n  \nTo avoid cluttering our information extraction task, we will apply a simple filter that keeps the longest `relation` between the same `subject` and `object`, and other things equal, the longest `subject` and `object`.\nTherefore, our function to remove \"Duplicate\" triplets will return just the following ones in this example:\n> {'subject': 'several studies',\n  'relation': 'have reported',\n  'object': 'lung features on chest CT.'}\n  \n> {'subject': 'several studies',\n  'relation': 'have reported',\n  'object': 'typical lung features of disease'}\n  \n> {'subject': 'studies',\n  'relation': 'have reported',\n  'object': 'lung features of disease on chest CT.'}\n  \nWe acknoledge that this part of the methodology can be improved, by doing some lemmatization, stemming, and even clustering of concepts.\nHowever due to time constrains we will leave this improvement as future work.","26676706":"We will now attempt to answer all the questions in this task following the same reporting structure:\n* Building a graph visualization based on the selected triplets\n* Producing a literature extract using the first items from the dataframe (the `max_items` argument limits the number of HTML items to render)\n* Producing a CSV table with the relevant triplets","fe74737e":"## Information Extraction using Stanford's OpenIE\n\nInformation Extraction is a Natural Langua Processing taks that extracts concept triplets from a given piece of text. Such triplets are formed by two concepts and the relationship between them: (`object`, `relation`, `subject`).\n\nFor instance, given \"Washington is the capital city of US\" we could extract the triplet annotation (`Washington`, `to be the capital city`, `US`).\n\nWe will use a widespread tool for this task: Stanford's Open IE.\nThe information for installing a Python wrapper for Stanford's OpenIE can be found [here](https:\/\/pypi.org\/project\/stanford-openie\/).\n\nIt is however not possible to install this module in the Kaggle's kernel, so for the sake of reproducibility, let us copy below the corresponding code that can be run locally to extract all triplet annotations from the different sections in the dataframe.\n\nFor this notebook submission you can just load the pre-computed results in the corresponding cell.","f3ec723d":"# Q9\/10: Approaches to evaluate risk for enhanced disease after vaccination","84039003":"# Information Extraction Methodology","3e252a8d":"# Q10\/10: Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models in conjuntion with therapeutics","8cefce4c":"# Q6\/10: Efforts targeted at a universal coronavirus vaccine.","07892367":"## Structure the literature corpus as a dataframe of article sections\n\nWe adapt the code from the notebook by Ivan Ega Pratama, from Kaggle.\n[Dataset Parsing Code | Kaggle, COVID EDA: Initial Exploration Tool](https:\/\/www.kaggle.com\/ivanegapratama\/covid-eda-initial-exploration-tool)\n\nThe `create_dataframes` function below depends on the specific version of the dataset at the time of writing the code, so we recommend to simply load the pre-computed dataframe by running the corresponding (uncommented) cell. ","2a1ba3a0":"# Q7\/10: Efforts to develop animal models and standardize challenge studies","518986ba":"# Q3\/10: Exploration of use of best animal models and their predictive value for a human vaccine.","aec932e1":"# Q4\/10: Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeuticss, to include antiviral agents","883f7c87":"# Q5\/10: Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up.\n\n### This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.","346a507b":"Removing redundant triplets takes some time, so we provide the code below but recommend importing the results using the corresponding cell.","33f31347":"To do so, we will create a list with all the official names of the coronavirus disease (COVID-19) and the virus that causes it.\n\nThis list is taken from the WHO website:\nhttps:\/\/www.who.int\/emergencies\/diseases\/novel-coronavirus-2019\/technical-guidance\/naming-the-coronavirus-disease-(covid-2019)-and-the-virus-that-causes-it\n\nAt the time of our search, it includes the following terms:","56a853b3":"We can inspect this dataframe, which we contribute as a [Kaggle community contributions](https:\/\/www.kaggle.com\/covid-19-contributions), since we believe is one of the main outputs of our work, and we hope it can be reused by other researchers.","7be1c58e":"The authors acknowledge the listed shortcomings of the extractions but believe their approach of information extraction from relevant sections, graph representations and more sophisticated extraction algorithms from the Knowledge Graph can provide a general framework for extracting insights from the corpus of scientific literature."}}