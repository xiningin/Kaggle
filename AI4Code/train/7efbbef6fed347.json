{"cell_type":{"29e543b6":"code","e27b353e":"code","c028a047":"code","d27f8ae2":"code","6120191c":"code","ffcf2c42":"code","203f9e9d":"code","1daa1a7c":"code","98d6c661":"code","d433f8d3":"code","d910d622":"code","73587c1e":"code","07a0fe0b":"code","3a15eb39":"code","dd52c9bc":"code","03461bd3":"code","7011dd69":"code","f2310a67":"code","e2693ad0":"code","8ca96096":"code","8d0aa2d1":"code","f3c98f28":"code","412addb2":"code","9afb707c":"code","5201bedb":"code","6d2b7a5e":"code","abd6f043":"code","aa2931b9":"code","fcc0ed67":"code","ddd5799e":"markdown","66ac9796":"markdown","e12a033f":"markdown"},"source":{"29e543b6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e27b353e":"# Standard ML Models for comparison\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\n\n# Splitting data into training\/testing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n\n# Distributions\nimport scipy","c028a047":"import matplotlib.pyplot as plt\nimport seaborn as sns","d27f8ae2":"df = pd.read_csv(\"..\/input\/runners-dataset-to-predict-finish-time\/data_fiver_1.csv\")","6120191c":"df.head(2)","ffcf2c42":"df.dtypes","203f9e9d":"# no null values and missing values are present in the datase\ndf.apply(lambda x: x.isnull().sum())","1daa1a7c":"df['PREV_W_TIME_1'].describe() # Y","98d6c661":"discrete = df.dtypes.loc[df.dtypes == 'int64'].index\nprint(discrete)","d433f8d3":"Continious = df.dtypes.loc[df.dtypes == 'float64'].index\nprint(Continious)","d910d622":"#Find corrleation with Prev_win_time_1 variable\ndf.corr()['PREV_W_TIME_1'].sort_values()","73587c1e":"# Find correlations with the prev_time variable and convert corr values into abs values\nmost_correlated = df.corr().abs()['PREV_W_TIME_1'].sort_values(ascending=False)\n\n# Maintain the top 8 most correlation features with Win variable\nmost_correlated = most_correlated[:9]\nmost_correlated","07a0fe0b":"#creating new dataset dff and adding the most correlated variables in it\ndff = df.loc[:, most_correlated.index]\ndff.head()","3a15eb39":"b = sns.scatterplot(x=dff['PREV_REQ_VELOCITY_1'],y=dff['PREV_W_TIME_1'])\nb.axes.set_title('REQ Velocity VS Prev Win Time', fontsize = 30)\nb.set_xlabel('Previous required Velocity_1', fontsize = 20)\nb.set_ylabel('Previous win time_1', fontsize = 20)\nplt.show()","dd52c9bc":"b = sns.scatterplot(x=dff['PREV_VELOCITY_RATE_1'],y=dff['PREV_W_TIME_1'])\nb.axes.set_title('Velocity VS Prev Win Time', fontsize = 30)\nb.set_xlabel('Previous Velocity Rate 1', fontsize = 20)\nb.set_ylabel('Previous win time_1', fontsize = 20)\nplt.show()","03461bd3":"b = sns.scatterplot(x=dff['PREV_REQ_ACCELERATION_1'],y=dff['PREV_W_TIME_1'])\nb.axes.set_title('REQ Acceleration VS Prev Win Time', fontsize = 30)\nb.set_xlabel('Previous required Acceleration Rate 1', fontsize = 20)\nb.set_ylabel('Previous win time_1', fontsize = 20)\nplt.show()","7011dd69":"b = sns.scatterplot(x=dff['PREV_ACC_RATE_1'],y=dff['PREV_W_TIME_1'])\nb.axes.set_title('Acceleration VS Prev Win Time', fontsize = 30)\nb.set_xlabel('Previous Acceleration Rate 1', fontsize = 20)\nb.set_ylabel('Previous win time_1', fontsize = 20)\nplt.show()","f2310a67":"b = sns.scatterplot(x=dff['PREV_ACCELERATION_1'],y=dff['PREV_W_TIME_1'])\nb.axes.set_title('Previous Acceleration VS Prev Win Time', fontsize = 30)\nb.set_xlabel('Previous Acceleration 1', fontsize = 20)\nb.set_ylabel('Previous win time_1', fontsize = 20)\nplt.show()","e2693ad0":"##","8ca96096":"#creating a Y variable\nlabels = dff['PREV_W_TIME_1']","8d0aa2d1":"# splitting the data into training and testing data (75% and 25%)\n# we mention the random state to achieve the same split everytime we run the code\nX_train, X_test, y_train, y_test = train_test_split(dff, labels, test_size = 0.25, random_state=42)","f3c98f28":"X_train.head()","412addb2":"labels.head()","9afb707c":"# Calculate mae and rmse\ndef evaluate_predictions(predictions, true):\n    mae = np.mean(abs(predictions - true))\n    rmse = np.sqrt(np.mean((predictions - true) ** 2))\n    \n    return mae, rmse","5201bedb":"# find the median\nmedian_pred = X_train['PREV_W_TIME_1'].median()\n\n# create a list with all values as median\nmedian_preds = [median_pred for _ in range(len(X_test))]\n\n# store the true G3 values for passing into the function\ntrue = X_test['PREV_W_TIME_1']","6d2b7a5e":"# Display the naive baseline metrics\nmb_mae, mb_rmse = evaluate_predictions(median_preds, true)\nprint('Median Baseline  MAE: {:.2f}'.format(mb_mae))\nprint('Median Baseline RMSE: {:.2f}'.format(mb_rmse))","abd6f043":"# Evaluate several ml models by training on training set and testing on testing set\ndef evaluate(X_train, X_test, y_train, y_test):\n    # Names of models\n    model_name_list = ['Linear Regression', 'ElasticNet Regression',\n                      'Random Forest', 'Extra Trees', 'SVM',\n                       'Gradient Boosted', 'Baseline']\n    X_train = X_train.drop('PREV_W_TIME_1', axis='columns')\n    X_test = X_test.drop('PREV_W_TIME_1', axis='columns')\n    \n    # Instantiate the models\n    model1 = LinearRegression()\n    model2 = ElasticNet(alpha=1.0, l1_ratio=0.5)\n    model3 = RandomForestRegressor(n_estimators=100)\n    model4 = ExtraTreesRegressor(n_estimators=100)\n    model5 = SVR(kernel='rbf', degree=3, C=1.0, gamma='auto')\n    model6 = GradientBoostingRegressor(n_estimators=50)\n    \n    # Dataframe for results\n    results = pd.DataFrame(columns=['mae', 'rmse'], index = model_name_list)\n    \n    # Train and predict with each model\n    for i, model in enumerate([model1, model2, model3, model4, model5, model6]):\n        model.fit(X_train, y_train)\n        predictions = model.predict(X_test)\n        \n        # Metrics\n        mae = np.mean(abs(predictions - y_test))\n        rmse = np.sqrt(np.mean((predictions - y_test) ** 2))\n        \n        # Insert results into the dataframe\n        model_name = model_name_list[i]\n        results.loc[model_name, :] = [mae, rmse]\n    \n    # Median Value Baseline Metrics\n    baseline = np.median(y_train)\n    baseline_mae = np.mean(abs(baseline - y_test))\n    baseline_rmse = np.sqrt(np.mean((baseline - y_test) ** 2))\n    \n    results.loc['Baseline', :] = [baseline_mae, baseline_rmse]\n    \n    return results","aa2931b9":"results = evaluate(X_train, X_test, y_train, y_test)\nresults","fcc0ed67":"plt.figure(figsize=(12, 8))\n\n# Root mean squared error\nax =  plt.subplot(1, 2, 1)\nresults.sort_values('mae', ascending = True).plot.bar(y = 'mae', color = 'b', ax = ax, fontsize=20)\nplt.title('Model Mean Absolute Error', fontsize=20) \nplt.ylabel('MAE', fontsize=20)\n\n# Median absolute percentage error\nax = plt.subplot(1, 2, 2)\nresults.sort_values('rmse', ascending = True).plot.bar(y = 'rmse', color = 'r', ax = ax, fontsize=20)\nplt.title('Model Root Mean Squared Error', fontsize=20) \nplt.ylabel('RMSE',fontsize=20)\n\nplt.show()","ddd5799e":"# For Prev_Win_Time variable","66ac9796":"## Modelling and ML","e12a033f":"# Performing EDA with most correlated variables \n"}}