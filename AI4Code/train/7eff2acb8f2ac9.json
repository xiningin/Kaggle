{"cell_type":{"9fa7bc76":"code","e86ad7d4":"code","17e0f00d":"code","bdf7362a":"code","d4276798":"code","a6c29261":"code","eb7a71a2":"code","8135d685":"code","41b20faa":"code","ebec8069":"code","a5e5faf8":"code","e43934c8":"code","411dda87":"code","1b651d9f":"code","562b9bd8":"code","1b890e9b":"code","9c0b1afb":"code","966716a0":"code","56153711":"code","1874a090":"code","408948bf":"code","940f9eb8":"code","058b4a08":"code","9d6668ae":"code","a8ca2518":"code","4de467c0":"code","7df9cb21":"code","33e78721":"code","3a601549":"code","7b6f09b9":"code","348f5450":"code","25e52de6":"code","f67ebc38":"code","8b2fed5c":"code","6956525e":"code","c4263c6a":"code","3bcbe585":"code","2918fa89":"code","ca95c95f":"code","43815959":"code","030e1ec9":"code","4634630a":"code","129d0fcb":"markdown","91971254":"markdown","3292f0b1":"markdown","f1b04c97":"markdown","24869b59":"markdown","a87db76f":"markdown","559c820e":"markdown","21a25575":"markdown","99bebded":"markdown","e0ac819e":"markdown","6491c613":"markdown"},"source":{"9fa7bc76":"import psutil\nimport gc\n\nimport typing\nfrom typing import List, Union, Optional\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import QuantileTransformer, StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\n\ngc.enable()","e86ad7d4":"machine_config = {\n    \"cpu_count (n)\": psutil.cpu_count(),\n    \"cpu_freq (Hz)\": psutil.cpu_freq().current,\n    \"virtual_memory (GB)\": psutil.virtual_memory().total\/\/1024**3,\n}\n\npd.DataFrame(machine_config, index=[0]).T","17e0f00d":"TEST_RUN = True\n\nSEED = 1291\n\nMINS_in_WEEK = 60*24*7\nMINS_in_HOUR = 60\nHOURS_in_WEEK = 24*7\nTIME_STEPS = MINS_in_WEEK\n\n\nTEST_SIZE = 58_678\nDST_MIN_MAX = (-2000, 500)\n\nmain_cfg = {\n    \"n_epochs\": 150,\n    \"early_stopping\": 15,\n}\n\ntest_cfg = {\n    \"n_epochs\": 300,\n    \"early_stopping\": 15,\n}\n\nif TEST_RUN:\n    cfg = test_cfg\nelse:\n    cfg = main_cfg\n    \nweight_decay = keras.regularizers.l2(l2=1e-4)","bdf7362a":"\n\"\"\" Transform \"\"\"\n\ndef transfrom_all_data(transformer, train: Union[pd.DataFrame, None], test: Optional[pd.DataFrame]=None, feature_list=None):\n    \"\"\"\n    Apply transformer to train and test features\n    \n    :Example:\n        >>> logTrans = FunctionTransformer(np.log1p)\n        >>> train_trans, test_trans = transfrom_all_data(transformer, train, test, feature_list)\n    \"\"\"\n    train_trans = transformer.fit_transform(train[feature_list])\n    if len(test):\n        test_trans = transformer.transform(test[feature_list])\n    else:\n        test_trans = None\n    \n    if type(train_trans) != np.ndarray:\n        train_trans = np.array(train_trans)\n        if len(test):\n            test_trans = np.array(test_trans)\n    \n    return train_trans, test_trans\n\n\ndef make_features(transformer, train: Union[pd.DataFrame, None], test: Union[pd.DataFrame, None], feature_list: List[str], name: str, \n                  normalize: bool=False, scaler=None):\n    \"\"\"\n    Add newly generated transformed features to train and test dataframe\n    \n    :Example:\n        >>> scaler = StandardScaler()\n        >>> logTrans = FunctionTransformer(np.log1p)\n        >>> train_X, val_X = make_features(qTrans, train_X, val_X, feature_list=range(10), name=\"qTrans\", normalize=False, scaler=scaler)\n    \"\"\"\n    train, test = train.copy(), test.copy()\n    train_trans, test_trans = transfrom_all_data(transformer, train, test, feature_list)\n    \n    if normalize and scaler is not None:\n        train_trans = scaler.fit_transform(train_trans).astype(np.float32)\n        test_trans = scaler.transform(test_trans).astype(np.float32)\n    \n    for i in range(train_trans.shape[1]):\n        train['{0}_{1}'.format(name, i)] = train_trans[:, i]\n        if len(test):\n            test['{0}_{1}'.format(name, i)] = test_trans[:, i]\n        \n    return train, test\n","d4276798":"# Generated training sequences for use in the model.\ndef create_sequences(values, time_steps=32, skip_steps=1, ignore_last=0):\n    output = []\n    for i in range(0, len(values) - time_steps, skip_steps):\n        output.append(values[i : (i + time_steps - ignore_last)])\n    return np.stack(output)","a6c29261":"def get_model(input_shape):\n    K.clear_session()\n    \n    inputs = keras.layers.Input(shape=input_shape)    \n    x = keras.layers.BatchNormalization()(inputs)\n    x = keras.layers.Dropout(0.25)(x)\n    \n    x = keras.layers.Conv1D(filters=32, kernel_size=5)(x)\n    x = keras.layers.Conv1D(filters=64, kernel_size=5, activation=\"relu\", kernel_regularizer=weight_decay)(x)\n    x = keras.layers.MaxPool1D(pool_size=2, strides=1, padding='valid')(x)\n    \n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.25)(x)\n    \n    x = keras.layers.Conv1D(filters=64, kernel_size=5, activation=\"relu\", kernel_regularizer=weight_decay)(x)\n    x = keras.layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\", kernel_regularizer=weight_decay)(x)\n    x = keras.layers.MaxPool1D(pool_size=2)(x)\n    \n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.25)(x)\n    \n    x = keras.layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\", kernel_regularizer=weight_decay)(x)\n    x = keras.layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\", kernel_regularizer=weight_decay)(x)\n    x = keras.layers.MaxPool1D(pool_size=2)(x)\n    \n    x = keras.layers.Flatten()(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.25)(x)\n    x = keras.layers.RepeatVector(2)(x)\n    \n    x = keras.layers.LSTM(128, return_sequences=True)(x)\n    \n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.25)(x)\n    x = keras.layers.Dense(128, kernel_regularizer=weight_decay)(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.25)(x)\n    x = keras.layers.Dense(64, kernel_regularizer=weight_decay)(x)\n\n    x = keras.layers.Dense(1)(x)\n    \n    # x = keras.layers.TimeDistributed(keras.layers.Dense(16, kernel_regularizer=weight_decay))(x)\n    # x = keras.layers.TimeDistributed(keras.layers.Dense(1, kernel_regularizer=weight_decay))(x)\n    \n    model = keras.models.Model(inputs,x)\n    return model\n\nmodel = get_model(input_shape=(360, 28))\nmodel.summary()","eb7a71a2":"class lr_callback(keras.callbacks.Callback):\n    def on_batch_end(self, batch, logs=None):\n        K.set_value(self.model.optimizer.lr, 0.54321)\n\n# def scheduler(epoch, lr): return max(1e-25, lr * 0.96 ** (epoch \/\/ 50))\ndef scheduler(epoch, lr): \n    if (epoch % 3) == 0:\n        new_lr = max(1e-5, lr * 0.97)\n    else:\n        new_lr = lr\n    # print(f\"{lr:0.5f} ---> {new_lr:0.5f}\")\n    return new_lr \n\ndef get_callbacks(name=\"default\"):\n\n    callbacks=[\n            keras.callbacks.EarlyStopping(monitor=\"val_loss\", #val_loss \n                                             patience=cfg[\"early_stopping\"], \n                                             mode=\"min\",\n                                             min_delta=1e-5,\n                                             verbose=1, \n                                             restore_best_weights=True),\n\n            keras.callbacks.ModelCheckpoint(filepath=f\"{name}_best_model.hdf5\", \n                                               verbose=1, \n                                               save_best_only=True),\n\n#             keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n#                                                  factor=0.95,\n#                                                  patience=5,\n#                                                  verbose=1,\n#                                                  mode='min',\n#                                                  min_delta=1e-4,\n#                                                  cooldown=0,\n#                                                  min_lr=1e-5),\n        \n#             keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n        ]\n    return callbacks","8135d685":"usecols = [\n    \"bx_gse\",\"by_gse\",\"bz_gse\",\n    \"bx_gsm\",\"by_gsm\",\"bz_gsm\",\n    \"theta_gse\", \"phi_gsm\",\n    \"theta_gsm\", \"phi_gse\",\n    \"bt\", \n    \"speed\", \"density\", \"temperature\",\n]\nusecols_dtype=dict(zip(usecols,[np.float32]*len(usecols)))\n\n\nsolar_wind = pd.read_csv(\"\/kaggle\/input\/magnet\/noaa-runtime\/data\/solar_wind.csv\",\n                         nrows=8_500_000\/\/3,\n                         usecols=list(usecols_dtype.keys())+[\"timedelta\"],\n                         dtype=usecols_dtype, #parse_dates=[\"timedelta\"], date_parser=pd.to_timedelta\n                        )\nsolar_wind.shape, solar_wind.info()","41b20faa":"dst_labels = pd.read_csv(\"\/kaggle\/input\/magnet\/noaa-runtime\/data\/dst_labels.csv\")\ndst_labels.shape, TEST_SIZE,  TEST_SIZE\/dst_labels.shape[0]\n\nqt_target = QuantileTransformer(n_quantiles=5000, output_distribution=\"normal\")\nqt_target = StandardScaler()\ndst_labels[[\"dst\"]] = qt_target.fit_transform(dst_labels[[\"dst\"]])","ebec8069":"pd.read_csv(\"\/kaggle\/input\/magnet\/noaa-runtime\/data\/satellite_positions.csv\").info()","a5e5faf8":"dst_labels.index = dst_labels.index*60\ndst_labels.index","e43934c8":"show_steps = 60*24*7*1 # minutes * hours * days * weeks\nsmoothing_window_size = 60\n\nfig, axs = plt.subplots(10, figsize=(24,14), sharex=True)\nsolar_wind[\"bx_gse\"].iloc[:show_steps].interpolate().rolling(smoothing_window_size).mean().fillna(method=\"ffill\").plot(ax=axs[0], style=\".--\", title=\"bx_gse\")\naxs[0].plot(solar_wind[\"bx_gse\"][:show_steps], \".\", alpha=0.05)\naxs[0].set_ylim(-7,7)\n\nsolar_wind[\"by_gse\"][:show_steps].rolling(smoothing_window_size).median().fillna(method=\"ffill\").plot(ax=axs[1], style=\".--\", title=\"by_gse\")\nsolar_wind[\"bz_gse\"][:show_steps].rolling(smoothing_window_size).median().fillna(method=\"ffill\").plot(ax=axs[2], style=\".--\", title=\"bz_gse\")\n\nsolar_wind[\"theta_gse\"][:show_steps].rolling(smoothing_window_size).median().fillna(method=\"ffill\").plot(ax=axs[3], style=\".--\", title=\"theta_gse\", color=\"C1\")\nsolar_wind[\"phi_gse\"][:show_steps].rolling(smoothing_window_size).median().fillna(method=\"ffill\").plot(ax=axs[4], style=\".--\", title=\"phi_gse\")\n\nsolar_wind[\"bt\"][:show_steps].rolling(smoothing_window_size).median().fillna(method=\"ffill\").plot(ax=axs[5], style=\".--\", title=\"bt\")\ndst_labels[\"dst\"][:show_steps\/\/60].plot(ax=axs[6], style=\".--\", title=\"dst\", color=\"C3\")\n\nsolar_wind[\"speed\"][:show_steps].rolling(smoothing_window_size).median().fillna(method=\"ffill\").plot(ax=axs[7], style=\".--\", title=\"speed\")\nsolar_wind[\"density\"][:show_steps].rolling(smoothing_window_size).median().fillna(method=\"ffill\").plot(ax=axs[8], style=\".--\", title=\"density\")\nsolar_wind[\"temperature\"][:show_steps].rolling(smoothing_window_size).median().fillna(method=\"ffill\").plot(ax=axs[9], style=\".--\", title=\"temperature\")\n\n\nplt.tight_layout()\nplt.show()","411dda87":"features_touse = usecols #[\"timedelta\", \"bx_gse\", \"by_gse\", \"bz_gse\", \"theta_gse\", \"phi_gsm\"][1:]","1b651d9f":"solar_wind[features_touse] = solar_wind[features_touse].interpolate()","562b9bd8":"qt = QuantileTransformer(n_quantiles=200, output_distribution=\"normal\")\n\ntrain_feature, test_feature = make_features(\n                                transformer=qt,\n                                train=solar_wind[features_touse],\n                                test=solar_wind.loc[[]],\n                                feature_list=features_touse,\n                                name=\"qt\",\n                                normalize=False,\n                                scaler=None,)\n\ntrain_feature = train_feature.drop(features_touse,axis=1)","1b890e9b":"new_feature = train_feature.rolling(60*3).median().fillna(method=\"bfill\")\ntrain_feature = pd.concat([train_feature, new_feature],1)","9c0b1afb":"# train_feature.reindex([1,2,3,5])\ntrain_feature = train_feature.reindex(range(0,2833333,30))","966716a0":"train_feature.shape","56153711":"gc.collect()","1874a090":"TIME_STEPS = 60*6 # Minutes * Hours * Days\nx_train = create_sequences(train_feature.values, time_steps=TIME_STEPS, skip_steps=MINS_in_HOUR\/\/30, ignore_last=0)\nprint(\"Training input shape: \", x_train.shape)","408948bf":"skip_first = TIME_STEPS\/\/60\ntarget_cols = [\"timedelta\",\"dst\"][1:]\n\ny_train = np.concatenate([\n    dst_labels.iloc[skip_first:(skip_first+x_train.shape[0])][target_cols],\n    dst_labels.iloc[skip_first+1:(skip_first+1+x_train.shape[0])][target_cols]],\n    axis=1)\ny_train.shape","940f9eb8":"# check for data leakage using timestamps\nx_train[0,-1,0], y_train[0]","058b4a08":"x_train[:2].nbytes \/ (1024*1024) # MBs","9d6668ae":"x_train[np.isnan(x_train)]=0\nnp.isnan(x_train).sum() ","a8ca2518":"class LrRangeFinder(tf.keras.callbacks.Callback):\n  def __init__(self, start_lr, end_lr):\n    super().__init__()\n    self.start_lr = start_lr\n    self.end_lr = end_lr \n       \n  def on_train_begin(self, logs={}):\n    self.lrs = []\n    self.losses = []\n    tf.keras.backend.set_value(self.model.optimizer.lr, self.start_lr)\n    \n    n_steps = self.params['steps'] if self.params['steps'] is not None else round(self.params['samples'] \/ self.params['batch_size'])\n    n_steps *= self.params['epochs']\n    self.by = (self.end_lr - self.start_lr) \/ n_steps\n      \n\n  def on_batch_end(self, batch, logs={}):\n    lr = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n    self.lrs.append(lr)\n    self.losses.append(logs.get('loss'))\n    lr += self.by\n    tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n    \n\nclass SGDRScheduler(tf.keras.callbacks.Callback):\n    '''Cosine annealing learning rate scheduler with periodic restarts.\n\n    # Usage\n        ```python\n            schedule = SGDRScheduler(min_lr=1e-5,\n                                     max_lr=1e-2,\n                                     lr_decay=0.9,\n                                     cycle_length=5,\n                                     mult_factor=1.5)\n            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n        ```\n\n    # Arguments\n        min_lr: The lower bound of the learning rate range for the experiment.\n        max_lr: The upper bound of the learning rate range for the experiment. \n        lr_decay: Reduce the max_lr after the completion of each cycle.\n                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n        cycle_length: Initial number of epochs in a cycle.\n        mult_factor: Scale epochs_to_restart after each full cycle completion.\n\n    # References\n        Original paper: http:\/\/arxiv.org\/abs\/1608.03983\n    '''\n    def __init__(self,\n                 min_lr,\n                 max_lr,\n                 lr_decay=1,\n                 cycle_length=10,\n                 mult_factor=2):\n\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.lr_decay = lr_decay\n\n        self.batch_since_restart = 0\n        self.next_restart = cycle_length\n\n        self.cycle_length = cycle_length\n        self.mult_factor = mult_factor\n\n        self.history = {}\n\n    def clr(self):\n        '''Calculate the learning rate.'''\n        fraction_to_restart = self.batch_since_restart \/ (self.steps_per_epoch * self.cycle_length)\n        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n        return lr\n\n    def on_train_begin(self, logs={}):\n        '''Initialize the learning rate to the minimum value at the start of training.'''\n        self.steps_per_epoch = self.params['steps'] if self.params['steps'] is not None else round(self.params['samples'] \/ self.params['batch_size'])\n        logs = logs or {}\n        tf.keras.backend.set_value(self.model.optimizer.lr, self.max_lr)\n\n    def on_batch_end(self, batch, logs={}):\n        '''Record previous batch statistics and update the learning rate.'''\n        logs = logs or {}\n        self.history.setdefault('lr', []).append(tf.keras.backend.get_value(self.model.optimizer.lr))\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n        self.batch_since_restart += 1\n        tf.keras.backend.set_value(self.model.optimizer.lr, self.clr())\n\n    def on_epoch_end(self, epoch, logs={}):\n        '''Check for end of current cycle, apply restarts when necessary.'''\n        if epoch + 1 == self.next_restart:\n            self.batch_since_restart = 0\n            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n            self.next_restart += self.cycle_length\n            self.max_lr *= self.lr_decay\n            self.best_weights = self.model.get_weights()\n\n    def on_train_end(self, logs={}):\n        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n        self.model.set_weights(self.best_weights)\n    \n    \ndef smooth(y, box_pts):\n  \"\"\"smoothes an array by taking the average of the `box_pts` point around each point\"\"\"\n  box = np.ones(box_pts)\/box_pts\n  y_smooth = np.convolve(y, box, mode='same')\n  return y_smooth","4de467c0":"lrRangeFinder = LrRangeFinder(start_lr=1e-5, end_lr=0.005)\nmodel = get_model(input_shape=x_train.shape[1:])\nmodel.compile(optimizer=\"adam\",loss=keras.losses.MeanSquaredError())\nnp.random.seed(SEED)\n\nmodel.fit(x_train, y_train, batch_size=64, \n          verbose=1, validation_split=0.3, epochs=1,\n         callbacks=[lrRangeFinder])","7df9cb21":"plt.plot(lrRangeFinder.lrs, lrRangeFinder.losses)\nplt.title('Model Losses Batch after Batch')\nplt.ylabel('loss')\nplt.xlabel('learning rate')\nplt.show()","33e78721":"smoothed_losses = smooth(lrRangeFinder.losses, 20)\n\nplt.plot(lrRangeFinder.lrs, smoothed_losses)\nplt.title('Smoothed Model Losses Batch after Batch')\nplt.ylabel('loss')\nplt.xlabel('learning rate')\nplt.show()\n\nmin_ = np.argmin(smoothed_losses)\nmax_ = np.argmax(smoothed_losses)\n\nsmoothed_losses_ = smoothed_losses[min_: max_]","3a601549":"smoothed_diffs = smooth(np.diff(smoothed_losses), 20)\n\nplt.plot(lrRangeFinder.lrs[:-1], smoothed_diffs)\nplt.title('Differences')\nplt.ylabel('loss difference')\nplt.xlabel('learning rate')\nplt.show()","7b6f09b9":"min_ = np.argmax(smoothed_diffs <= 0)  # where the (smoothed) loss starts to decrease\nmax_ = np.argmax(smoothed_diffs >= 0)  # where the (smoothed) loss restarts to increase\nmax_ = max_ if max_ > 0 else smoothed_diffs.shape[0]  # because max_ == 0 if it never restarts to increase\n\nsmoothed_losses_ = smoothed_losses[min_: max_]  # restrain the window to the min_, max_ interval\n# Take min and max loss in this restrained window\nmin_smoothed_loss_ = min(smoothed_losses_[:-1])\nmax_smoothed_loss_ = max(smoothed_losses_[:-1])\ndelta = max_smoothed_loss_ - min_smoothed_loss_\n\n\nlr_arg_max = np.argmax(smoothed_losses_ <= min_smoothed_loss_ + .05 * delta)\nlr_arg_min = np.argmax(smoothed_losses_ <= min_smoothed_loss_ + .5 * delta)\n\nlr_arg_min += min_\nlr_arg_max += min_\n\nlrs = lrRangeFinder.lrs[lr_arg_min: lr_arg_max]\nlr_min, lr_max = min(lrs), max(lrs)\n\nprint('lr range: [{}, {}]'.format(lr_min, lr_max))","348f5450":"\n\n\nax = plt.axes()\nax.plot(lrRangeFinder.lrs, smoothed_losses)\nax.set_title('Smoothed Model Losses Batch after Batch')\nax.set_ylabel('loss')\nax.set_xlabel('learning rate')\nax.set_ylim(0, 1.05 * np.max(smoothed_losses))\nax.set_xlim(0, max(lrRangeFinder.lrs))\nax.vlines(x=[lr_min, lr_max], ymin=[0, 0], ymax=[smoothed_losses[lr_arg_min], smoothed_losses[lr_arg_max]], color='r', linestyle='--', linewidth=.8)\nax.plot(lrs, smoothed_losses[lr_arg_min: lr_arg_max], linewidth=2)\nx_arrow_arg = int((lr_arg_min + lr_arg_max) \/ 2)\nx_arrow = lrRangeFinder.lrs[x_arrow_arg]\ny_arrow = smoothed_losses[x_arrow_arg]\nax.annotate('best piece of slope', xy=(x_arrow, y_arrow), xytext=(lr_max, smoothed_losses[lr_arg_min]), arrowprops=dict(facecolor='black', shrink=0.05))\nax.annotate ('', (lr_min, smoothed_losses[lr_arg_max] \/ 5), (lr_max, smoothed_losses[lr_arg_max] \/ 5), arrowprops={'arrowstyle':'<->'})\nax.text((lr_min + lr_max) \/ 2, 3 * smoothed_losses[lr_arg_max] \/ 5, 'lr range', horizontalalignment='center', verticalalignment='center', weight='bold')\n\nplt.show()","25e52de6":"scheduler = SGDRScheduler(min_lr=lr_min, max_lr=lr_max, lr_decay=.9, cycle_length=6, mult_factor=1.5)\ncallbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True),\n             tf.keras.callbacks.ModelCheckpoint(filepath='best_model_with_cycling.h5', monitor='val_loss', verbose=1, save_best_only=True),\n             scheduler]","f67ebc38":"weight_decay = keras.regularizers.l2(l2=1e-6)\n\nmodel = get_model(input_shape=x_train.shape[1:])\nadam=keras.optimizers.Adam(lr=0.007, decay=1e-6)\nmodel.compile(optimizer=adam,loss=keras.losses.MeanSquaredError())\nnp.random.seed(SEED)\n\nmodel.fit(x_train, y_train, batch_size=265, \n          verbose=1, validation_split=0.3,\n          epochs=cfg[\"n_epochs\"],\n          callbacks=callbacks)","8b2fed5c":"model.history.history","6956525e":"def plot_lcurve(model, metrics=[\"loss\"], name=\"default.jpg\"):\n\n    fig, ax = plt.subplots(len(metrics),1, figsize=(8, 6))\n    if len(metrics)==1:\n        ax=[ax]\n    for metric, axi in zip(metrics, ax):\n        axi.set_title(metric)\n        axi.plot(model.history.history[metric], \".--\", label=\"tr\")\n        axi.plot(model.history.history[f'val_{metric}'], \".--\", label=\"val\")\n        axi.legend()\n    plt.tight_layout()\n    plt.savefig(name)\n    plt.show()\n\nplot_lcurve(model)","c4263c6a":"# solar_wind_2nd = pd.read_csv(\"..\/input\/magnet\/noaa-runtime\/data\/solar_wind.csv\", skiprows=8_500_000\/\/3)\n# solar_wind_2nd.info()","3bcbe585":"test_size = 60_000\ny_pred = model.predict(x_train[:test_size]).reshape(-1,2)\ny_pred.shape","2918fa89":"def mse(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred,squared=True)\n\ndef rmse(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred,squared=False)\n\nrmse(y_train[:test_size],y_pred), mse(y_train[:test_size],y_pred)","ca95c95f":"\nshow_steps = 500\n\nfig, axs = plt.subplots(2,figsize=(15,5))\naxs[0].plot(y_train[:test_size][:show_steps,0],\".--\", label=\"tr-t0\")\naxs[0].plot(y_pred[:show_steps,0],\".--\", label=\"pr-t0\")\n\naxs[1].plot(y_train[:test_size][:show_steps,1],\".--\", label=\"tr-t1\")\naxs[1].plot(y_pred[:show_steps,1],\".--\", label=\"pr-t1\");\n\n[ax.legend() for ax in axs]\n\nplt.tight_layout()\nplt.show()","43815959":"def plot_true_vs_pred(y_true, y_pred, series_names=[\"t0\",\"t1\"]):\n    lowess = sm.nonparametric.lowess\n    \n    xmin, xmax = (min(y_true.min(), y_pred.min()), \n                  max(y_true.max(), y_pred.max()))\n\n    fig, axs = plt.subplots(1,2,figsize=(22,8))\n\n    for i, series_i in enumerate(series_names):\n        rmse_i = rmse(y_true[:,i], y_pred[:,i])\n        w = lowess(y_pred[:,i],y_true[:,i], frac=1.\/3)\n        axs[i].plot(y_true[:,i], y_pred[:,i], \".\")\n        axs[i].plot([xmin,xmax], [xmin,xmax], \"--\")\n        axs[i].plot(*w.transpose(), \"--\")\n        axs[i].set_xlim(xmin,xmax)\n        axs[i].set_ylim(xmin,xmax)\n        axs[i].text(xmin,xmax+0,f\"RMSE_{series_i}: {rmse_i:.4f}\")\n\n    \n    plt.tight_layout()\n    plt.show()\n    \nplot_true_vs_pred(y_train[:test_size], y_pred)","030e1ec9":"y_true = np.apply_along_axis(lambda x: qt_target.inverse_transform(x.reshape(-1,1)).reshape(-1), 0, y_train[:test_size])\ny_pred_inv = np.apply_along_axis(lambda x: qt_target.inverse_transform(x.reshape(-1,1)).reshape(-1), 0, y_pred)\n\nplot_true_vs_pred(y_true, y_pred_inv)","4634630a":"\n\nres = y_pred[:,1]-y_train[:test_size][:,1]\nabs_max = np.abs([res.min(),res.max()]).max()\n\nfig, axs = plt.subplots(2,2,figsize=(12*2,5*2))\n\nfig.suptitle(\"Regression Residual Analysis\")\naxs[0,0].set_title(\"Residual Plot\")\nsns.residplot(y_train[:test_size][:,1], y_pred[:,1], lowess=True, color=\"C0\", ax=axs[0,0])\naxs[0,0].set_ylim(-abs_max,abs_max)\naxs[0,0].set_ylabel(\"Residues\")\naxs[0,0].set_xlabel(\"Predicted Value\")\n\naxs[0,1].set_title(\"Residuals Distribution\")\nsns.distplot(res, ax=axs[0,1], bins=300, color=\"C0\")\n\naxs[1,1].set_title(\"Residuals Q-Q Plot\")\nsm.qqplot(res, ax=axs[1,1], line=\"s\", color=\"C0\")\n\nplt.show()\nres.std()","129d0fcb":"# Importing libraries","91971254":"# Down Sampling","3292f0b1":"# Visualize Predictions","f1b04c97":"# Prepare Train Set","24869b59":"# Functions","a87db76f":"# Train Model","559c820e":"## MagNet: Model the Geomagnetic Field\n\n[Competition Page](https:\/\/www.drivendata.org\/competitions\/73\/noaa-magnetic-forecasting\/page\/278\/)\n<img src=\"https:\/\/drivendata-public-assets.s3.amazonaws.com\/noaa-cover-img.png\" width=\"400\">\n\n### Overview\nHelp NOAA better forecast changes in Earth\u2019s magnetic field!\n\nThe efficient transfer of energy from solar wind into the Earth\u2019s magnetic field causes geomagnetic storms. The resulting variations in the magnetic field increase errors in magnetic navigation. The disturbance-storm-time index, or Dst, is a measure of the severity of the geomagnetic storm.\n\n\n### Training Data\n\nThree different time-series datasets are provided as features, in addition to the Dst labels:\n\n| filename\t  |           description\t                            |                    frequency   | \n| ----------- | --------------------------------------------------- | ----------------------------- | \n| solar_wind.csv          | Solar wind data collected from ACE and DSCOVR satellites | \tminutely\n| sunspots.csv            | Smoothed sunspot counts\t                               |      monthly\n| satellite_positions.csv | Coordinate positions for ACE and DSCOVR\t               |      daily\n| dst_labels.csv          | Dst values averaged across the four stations\t           |      hourly\n\n\n### Labels\nThe labels are hourly Dst values, indexed using the same period and timedelta multi-index. Your task is to predict the current timestep (t0) and the following timestep (t+1). Remember that you are not allowed to use historical Dst values as an input for prediction.\n\n### Performance metric\nPerformance is evaluated according to Root Mean Squared Error (RMSE). RMSE will be calculated on **t0** and **t+1** simultaneously.","21a25575":"# LrRangeFinder","99bebded":"# Load Data ","e0ac819e":"# Preprocessing Data","6491c613":"# Domain Knowledge\n\n1. 3-4 days\n   - It affects it by the intense clouds of high energy particles that it often contains which are produced by solar storms. When these clouds, called coronal mass ejections, make their way to the Earth in 3-4 days, they collide with the magnetic field of the Earth and cause it to change its shape."}}