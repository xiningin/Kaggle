{"cell_type":{"6b62a618":"code","ae9edefe":"code","ce24c578":"code","4490d7ec":"code","215b93b5":"code","a02db7e0":"code","f0c99c15":"code","f3b17202":"code","c0b17dd7":"code","8549fda6":"code","a1f7faa2":"code","e209b77d":"code","f056a06f":"markdown","db3178b1":"markdown","8fc95eb4":"markdown","396a04b2":"markdown","bf0335ea":"markdown","55c4cf9f":"markdown","8fbc73ac":"markdown"},"source":{"6b62a618":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ae9edefe":"\n# Data manipulation libraries\nimport pandas as pd\nimport numpy as np\n\n##### Scikit Learn modules needed for Logistic Regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder,MinMaxScaler , StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# Plotting libraries\nfrom IPython.display import SVG\nfrom graphviz import Source\nfrom IPython.display import display\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(color_codes = True)\n%matplotlib inline","ce24c578":"df = pd.read_csv(\"..\/input\/train.csv\")\ndf.head()\nprint(df.describe())","4490d7ec":"# Select only numerical columns for data analysis\ndf_numeric = df._get_numeric_data()\nprint(df_numeric.columns)\nexclude_dates = ['Id','YearBuilt','YearRemodAdd','MoSold', 'YrSold','SalePrice']\ndf_numeric = df_numeric.drop(exclude_dates,axis=1)\nprint(df_numeric.columns)","215b93b5":"# Explore data visually\n# Build Correlation Matrix to study multi collinearity\ncorrelation = df_numeric.corr()\n#print(correlation)\n\nfig , ax = plt.subplots()\nfig.set_figwidth(18)\nfig.set_figheight(18)\nsns.heatmap(correlation,annot=True,cmap=\"YlGnBu\")","a02db7e0":"# We create the preprocessing pipelines for both numeric and categorical data.\n\nnumeric_features = [x for x in df_numeric.columns]\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler()),\n    ('pca', PCA(n_components= 2))])\n\nall_numeric_columns = exclude_dates + numeric_features\ncategorical_features = [x for x in df.columns if x not in all_numeric_columns ]\n# categorical_features = [x for x in df.columns if x not in df_numeric + exclude_dates]\n#print(categorical_features)\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])\n\n# Append classifier to preprocessing pipeline.\n# Now we have a full prediction pipeline.\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', RandomForestRegressor())])","f0c99c15":"X_train, X_test, y_train, y_test = train_test_split(df[numeric_features + categorical_features], \n                                                    df[\"SalePrice\"], test_size=0.2,random_state =42)","f3b17202":"param_grid = {\n    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n    'classifier__max_features': [\"auto\",\"sqrt\", \"log2\"],\n    #'classifier__max_iter' :[100,150,200],\n    'classifier__n_estimators': [10,50,100,200],\n    'classifier__max_depth':[2,4,8]\n}\n\ngrid_search_rfr = GridSearchCV(clf, param_grid, cv=10, iid=False,verbose= 2 , n_jobs = -1)\ngrid_search_rfr.fit(X_train, y_train)\n\nprint((\"best Linear Regression from grid search: %.3f\"\n       % grid_search_rfr.score(X_test, y_test)))\nprint(\"Best Parameter Setting is {}\".format(grid_search_rfr.best_params_))","c0b17dd7":"test_df = pd.read_csv(\"..\/input\/test.csv\")\ntest_df_columns = [x for x in test_df if x not in exclude_dates]\n\n# Load Submission File\nsample_submission = pd.read_csv(\"..\/input\/sample_submission.csv\")","8549fda6":"y_prediction = grid_search_rfr.predict(test_df[test_df_columns])","a1f7faa2":"submission = pd.DataFrame({\"Id\":sample_submission[\"Id\"].values, \"SalePrice\":y_prediction.tolist()})","e209b77d":"submission.to_csv(\"submission_randomfr_V1.csv\",index=False)","f056a06f":"### Load Training Data","db3178b1":"### Explore Training Data","8fc95eb4":"### Visual Observation\n- Several numerical variables are strongly correlated viz Garage Cars with Garage Area or Ground Levl Area with Total rooms above\n- We could either remove of of the correlated values but can also engineer a metric as ratio of two quantities\n- In my current example I have kept all the correlated values and opted for reducing dimensions of numerical variables by using PCA","396a04b2":"### Build Preprocessing Pipeline -\n- created separate strategy to handle numerical and categorical variables\n\n#### Preprocessing of Numerical Features - \n- Imputation using Mean (however added median as part of grid search in below code)\n- opted for standard scaling of numerical values\n- Dimentionality Reduction using PCA\n\n#### Categorical Variables\n- Imputed missing values with word _'missing'_\n- Tranformation using One hot encoding","bf0335ea":"### Load Libraries","55c4cf9f":"### Thats all for the day folks !! Oh yes and I havent touched upon time based variables, kept them for next iterations to come :)","8fbc73ac":"### Split Data in Training & test sets (80\/20 ratio)"}}