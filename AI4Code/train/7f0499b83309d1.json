{"cell_type":{"59eb56a5":"code","21de07cb":"code","8e95525a":"code","edc0c325":"code","5d1aeea0":"code","0af6f174":"code","77000fd2":"code","67d98673":"code","bef15c03":"code","c6807406":"code","869e90ca":"code","37ea5eb4":"code","9bf645ec":"code","9a10be2a":"code","7c1bb77d":"code","f8325fb0":"code","7799bb1e":"code","ab8ed35d":"code","62151fe2":"code","2b45dd5b":"code","69e438cc":"code","27fcd747":"code","0fccacd6":"code","c31ca507":"code","5ad076b7":"code","db86df78":"code","2e2aa017":"code","ea1c4a90":"code","7edbc5eb":"code","08e4d688":"code","a2eeeead":"code","631803ca":"code","91ce28b4":"code","7e259d8e":"code","071eba2d":"code","c7c4d0cd":"code","b05aeee2":"code","74091474":"code","09209e4a":"code","ed3cb386":"markdown"},"source":{"59eb56a5":"#Libraries\n!pip install tweepy\nimport tweepy\nfrom tweepy import Stream\nfrom tweepy import OAuthHandler\nfrom tweepy.streaming import StreamListener\nimport json\nimport pandas as pd\nimport csv\nimport re #regular expression\nfrom textblob import TextBlob\nimport string\nimport preprocessing as p\nimport os\nfrom nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re,string,unicodedata\n!pip install contractions\nimport contractions #import contractions_dict\nfrom bs4 import BeautifulSoup\n%matplotlib inline\n\n\n#Importing text processing libraries\nimport spacy\nimport spacy.cli\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.lancaster import LancasterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\n#downloading wordnet\/punkt dictionary\nnltk.download('wordnet')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option('display.max_columns', 100)","21de07cb":"!pip install preprocessor\n!pip install tweet-preprocessor\nfrom preprocessor.api import clean","8e95525a":"#Twitter credentials \nconsumer_key = '*****'\nconsumer_secret = '****'\naccess_key= '***-****'\naccess_secret = '*******'","edc0c325":"#pass twitter credentials to tweepy\n\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_key, access_secret)\napi = tweepy.API(auth)","5d1aeea0":"#file location\nCrude_Oil_Twitter_Analysis = \"\/Users\/dhruvarora\/Downloads\/Twitter Analysis - Oil\/TwitterData-US Oil Prices.csv\"","0af6f174":"COLS = ['id', 'created_at', 'source', 'original_text','clean_text', 'sentiment','polarity','subjectivity', 'lang',\n'favorite_count', 'retweet_count', 'original_author',   'possibly_sensitive', 'hashtags',\n'user_mentions', 'place', 'place_coord_boundaries']","77000fd2":"#set two date variables for date range\nstart_date = '2020-01-01'\nend_date = '2020-01-14'","67d98673":"#HappyEmoticons\nemoticons_happy = set([\n    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n    '<3'\n    ])","bef15c03":"# Sad Emoticons\nemoticons_sad = set([\n    ':L', ':-\/', '>:\/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n    ':-[', ':-<', '=\\\\', '=\/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n    ':c', ':{', '>:\\\\', ';('\n    ])","c6807406":"#Emoji patterns\nemoji_pattern = re.compile(\"[\"\n         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n         u\"\\U00002702-\\U000027B0\"\n         u\"\\U000024C2-\\U0001F251\"\n         \"]+\", flags=re.UNICODE)","869e90ca":"#combine sad and happy emoticons\nemoticons = emoticons_happy.union(emoticons_sad)","37ea5eb4":"#Clean tweet\ndef clean_tweets(tweet):\n \n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(tweet)\n#after tweepy preprocessing the colon symbol left remain after      #removing mentions\n    tweet = re.sub(r':', '', tweet)\n    tweet = re.sub(r'\u201a\u00c4\u00b6', '', tweet)\n#replace consecutive non-ASCII characters with a space\n    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n#remove emojis from tweet\n    tweet = emoji_pattern.sub(r'', tweet)\n#filter using NLTK library append it to a string\n    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n    filtered_tweet = []\n#looping through conditions\n    for w in word_tokens:\n#check tokens against stop words , emoticons and punctuations\n        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n            filtered_tweet.append(w)\n    return ' '.join(filtered_tweet)\n    #print(word_tokens)\n    #print(filtered_sentence)return tweet","9bf645ec":"def write_tweets(keyword, file):\n    # If the file exists, then read the existing data from the CSV file.\n    if os.path.exists(file):\n        df = pd.read_csv(file, header=0)\n    else:\n        df = pd.DataFrame(columns=COLS)\n    #page attribute in tweepy.cursor and iteration\n    for page in tweepy.Cursor(api.search, q=keyword,\n                              count=1000, include_rts=False, since=start_date).pages(200):\n        for status in page:\n            new_entry = []\n            status = status._json\n\n            ## check whether the tweet is in english or skip to the next tweet\n            if status['lang'] != 'en':\n                continue\n\n            #when run the code, below code replaces the retweet amount and\n            #no of favorires that are changed since last download.\n            if status['created_at'] in df['created_at'].values:\n                i = df.loc[df['created_at'] == status['created_at']].index[0]\n                if status['favorite_count'] != df.at[i, 'favorite_count'] or \\\n                   status['retweet_count'] != df.at[i, 'retweet_count']:\n                    df.at[i, 'favorite_count'] = status['favorite_count']\n                    df.at[i, 'retweet_count'] = status['retweet_count']\n                continue\n\n\n           #tweepy preprocessing called for basic preprocessing\n            clean_text = clean(status['text'])\n\n            #call clean_tweet method for extra preprocessing\n            filtered_tweet=clean_tweets(clean_text)\n\n            #pass textBlob method for sentiment calculations\n            blob = TextBlob(filtered_tweet)\n            Sentiment = blob.sentiment\n\n            #seperate polarity and subjectivity in to two variables\n            polarity = Sentiment.polarity\n            subjectivity = Sentiment.subjectivity\n\n            #new entry append\n            new_entry += [status['id'], status['created_at'],\n                          status['source'], status['text'],filtered_tweet, Sentiment,polarity,subjectivity, status['lang'],\n                          status['favorite_count'], status['retweet_count']]\n\n            #to append original author of the tweet\n            new_entry.append(status['user']['screen_name'])\n\n            try:\n                is_sensitive = status['possibly_sensitive']\n            except KeyError:\n                is_sensitive = None\n            new_entry.append(is_sensitive)\n\n            # hashtagas and mentiones are saved using comma separted\n            hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n            new_entry.append(hashtags)\n            mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n            new_entry.append(mentions)\n\n            #get location of the tweet if possible\n            try:\n                location = status['user']['location']\n            except TypeError:\n                location = ''\n            new_entry.append(location)\n\n            try:\n                coordinates = [coord for loc in status['place']['bounding_box']['coordinates'] for coord in loc]\n            except TypeError:\n                coordinates = None\n            new_entry.append(coordinates)\n\n            single_tweet_df = pd.DataFrame([new_entry], columns=COLS)\n            df = df.append(single_tweet_df, ignore_index=True)\n            csvFile = open(file, 'a' ,encoding='utf-8')\n    df.to_csv(csvFile, mode='a', columns=COLS, index=False, encoding=\"utf-8\")","9a10be2a":"#declare keywords as a query for three categories\nCrudeOil_keywords = 'US Oil Prices'","7c1bb77d":"#write_tweets(CrudeOil_keywords,  Crude_Oil_Twitter_Analysis)#","f8325fb0":"#Loading Dataset\nurl = '..\/input\/twitterdataus-oil-pricescsv\/TwitterData-US Oil Prices.csv'\nraw_data = pd.read_csv(url, header='infer')","7799bb1e":"data = raw_data[['original_text','original_author','retweet_count','polarity','subjectivity']]\ndata = data.rename(columns={'original_text': 'text', 'original_author': 'screenName', 'retweet_count':'retweetCount'})","ab8ed35d":"#Resetting Index\ndata.reset_index(drop=True, inplace=True)","62151fe2":"#Backup of the newly created dataset\ndata_backup = data.copy()","2b45dd5b":"#lowering cases\ndata['text'] = data['text'].str.lower()","69e438cc":"#stripping leading spaces (if any)\ndata['text'] = data['text'].str.strip()","27fcd747":"# Removing HTML tags\ndef strip_html_tags(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    stripped_text = soup.get_text()\n    return stripped_text\n\n#apply to the dataset\ndata['text'] = data['text'].apply(strip_html_tags)","0fccacd6":"# Remove URL and links\ndef strip_url(text):\n    strip_url_text = re.sub(r'http\\S+', '', text)\n    return strip_url_text\n\n#Applying the dataset\ndata['text'] = data['text'].apply(strip_url)\n\n","c31ca507":"#removing punctuations\nfrom string import punctuation\n\ndef remove_punct(text):\n    for punctuations in punctuation:\n        text = text.replace(punctuations, '')\n        return text\n\n#apply to the dataset\ndata['text'] = data['text'].apply(remove_punct)","5ad076b7":"#function to remove special characters\ndef remove_special_chars(text, remove_digits=True):\n    pattern = r'[^a-zA-z0-9\\s]'\n    text = re.sub(pattern, '', text)\n    return text\n\n#applying the function on the clean dataset\ndata['text'] = data['text'].apply(remove_special_chars)","db86df78":"#function to remove macrons & accented characters\ndef remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text\n\n#applying the function on the clean dataset\ndata['text'] = data['text'].apply(remove_accented_chars) ","2e2aa017":"#Function to expand contractions\ndef expand_contractions(con_text):\n    con_text = contractions.fix(con_text)\n    return con_text\n\n#applying the function on the clean dataset\ndata['text'] = data['text'].apply(expand_contractions)","ea1c4a90":"#creating a new column in the dataset for word count\ndata ['word_count'] = data['text'].apply(lambda x:len(str(x).split(\" \")))","7edbc5eb":"#Taking Backup\ndata_clean = data.copy()","08e4d688":"#function to remove stopwords\ndef remove_stopwords(text, is_lower_case=False):\n    stopword_list = set(stopwords.words('english'))\n    tokenizer = ToktokTokenizer()\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text\n\n#applying the function\ndata ['text'] = data['text'].apply(remove_stopwords) ","a2eeeead":"#Function for stemming\ndef simple_stemmer(text):\n    ps = nltk.porter.PorterStemmer()\n    text = ' '.join([ps.stem(word) for word in text.split()])\n    return text\n\n#applying the function\ndata['Stemd_text'] = data['text'].apply(simple_stemmer)","631803ca":"#rearranging columns\ndata = data[['screenName','text','Stemd_text','retweetCount','word_count','polarity','subjectivity']]","91ce28b4":"#Taking Backup\ndata_preproc = data.copy()","7e259d8e":"from textblob import TextBlob","071eba2d":"#function to perform Textblob Sentiment Analyis\ndef sentiment_analysis(text):\n    polarity = round(TextBlob(text).sentiment.polarity, 3)\n    sentiment_categories = ['positive','negative','neutral']\n    if polarity > 0:\n        return sentiment_categories[0]\n    elif polarity < 0:\n        return sentiment_categories[1]\n    else:\n        return sentiment_categories[2]  \n        \n#Apply to the Stemd_Text\ndata['Sentiments'] = [sentiment_analysis(txt) for txt in data['Stemd_text']]","c7c4d0cd":"num_bins = 50\nplt.figure(figsize=(10,6))\nn, bins, patches = plt.hist(data.word_count, num_bins, facecolor='blue', alpha=0.5)\nplt.xlabel('Word Count')\nplt.ylabel('Tweet Count')\nplt.title('Histogram of Word Count')\nplt.show();","b05aeee2":"#Creating a Count Plot\nsns.set(style=\"darkgrid\")\nfig, ax = plt.subplots(figsize=(8,8))\nax = sns.countplot(x=\"Sentiments\", data=data)\nplt.title('Sentiments Count')\nplt.ylabel('Count')\nplt.xlabel('Sentiments')","74091474":"data.head()","09209e4a":"#Displaying Negative Tweets with a high retweet count\ndata[(data.Sentiments == 'negative') & (data.retweetCount > 40)]","ed3cb386":"# Twitter Sentiment Analysis using \"US Oil Prices\" as a keyword"}}