{"cell_type":{"94fbc937":"code","8a354267":"code","18d9d9a7":"code","d9a689e7":"code","8576b940":"code","b71684e0":"code","7fb73d54":"code","de8450bf":"code","c34867e7":"code","d147911b":"code","22e0ae5a":"code","80e36e5a":"code","53fcb03b":"code","3b45c43d":"code","be417c81":"code","84ab31ac":"code","67841fc4":"code","771de03a":"code","3d1ecabd":"code","e8f53e25":"code","73f7923a":"code","27b29ba8":"code","3bcfe5ad":"code","e1c3cea3":"markdown","4858ba0e":"markdown","c58f4af8":"markdown"},"source":{"94fbc937":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8a354267":"df=pd.read_csv('\/kaggle\/input\/weather.csv')","18d9d9a7":"#Checking Dataset before applying Encoding\ndf.head()","d9a689e7":"#To Convert Categorical vaariables into Numerical Variables using Pandas\ndf=pd.get_dummies(df,drop_first=True)","8576b940":"#Data after Encoding\ndf.head()","b71684e0":"#Checking dataset for Datatypes and other information \ndf.info()","7fb73d54":"#Checking missing values\ndf.isnull().sum()","de8450bf":"df_1=df[['Temperature_c','Humidity','Wind_Speed_kmh','Wind_Bearing_degrees','Visibility_km','Pressure_millibars']]\ndf_1","c34867e7":"df.describe()","d147911b":"def outliers(dataset,column_name):\n    IQR=dataset[column_name].quantile(0.75)-dataset[column_name].quantile(0.25)\n    \n    upper_boundary=dataset[column_name].quantile(0.75)+(IQR*1.5)\n    lower_boundary=dataset[column_name].quantile(0.25)+(IQR*1.5)\n    \n    return upper_boundary, lower_boundary","22e0ae5a":"print('Upper and Lower Boundaries of Temperature_c ',outliers(df,'Temperature_c'))\nprint('Upper and Lower Boundaries of Humidity ',outliers(df,'Humidity'))\nprint('Upper and Lower Boundaries of Wind_Speed_kmh ',outliers(df,'Wind_Speed_kmh'))\nprint('Upper and Lower Boundaries of Wind_Bearing_degrees ',outliers(df,'Wind_Bearing_degrees'))\nprint('Upper and Lower Boundaries of Visibility_km ',outliers(df,'Visibility_km'))\nprint('Upper and Lower Boundaries of Pressure_millibars ',outliers(df,'Pressure_millibars'))","80e36e5a":"fig, axes = plt.subplots(nrows=3,ncols=2,figsize=(8,8))\nplt.tight_layout()\naxes[0,0].set_title('Outliers For Temperature_c')\naxes[0,1].set_title('Outliers For Humidity')\naxes[1,0].set_title('Outliers For Wind_Speed_kmh')\naxes[1,1].set_title('Outliers For Wind_Bearing_degrees')\naxes[2,0].set_title('Outliers For Visibility_km')\naxes[2,1].set_title('Outliers For Pressure_millibars')\n\naxes[0,0].boxplot(df['Temperature_c'])\naxes[0,1].boxplot(df['Humidity'])\naxes[1,0].boxplot(df['Wind_Speed_kmh'])\naxes[1,1].boxplot(df['Wind_Bearing_degrees'])\naxes[2,0].boxplot(df['Visibility_km'])\naxes[2,1].boxplot(df['Pressure_millibars'])\n\nfig.savefig('Six_Subplots')","53fcb03b":"X=df.iloc[:,[0,1,2,3,4,5,7,8]]\ny=df['Rain']","3b45c43d":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33,random_state=0)","be417c81":"from sklearn.preprocessing import StandardScaler\nscaler =StandardScaler().fit(X_train)\nscaler.transform(X_train)","84ab31ac":"grid={\"C\":np.logspace(1,10,10), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\nlogreg=LogisticRegression()\nlogreg_cv=GridSearchCV(logreg,grid,cv=10)\nlogreg_cv.fit(X_train,y_train)","67841fc4":"print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\nprint(\"accuracy :\",logreg_cv.best_score_)","771de03a":"regressor=LogisticRegression(C=10000,penalty= 'l2')\nregressor.fit(X_train,y_train)","3d1ecabd":"intercept = regressor.intercept_\ncoefficients = regressor.coef_","e8f53e25":"coef_list = list(coefficients[0,:])\ncoef_df = pd.DataFrame({'Feature': list(X_train.columns),'Coefficient': coef_list})\nprint(coef_df)","73f7923a":"predicted_prob = regressor.predict_proba(X_test)[:,1]\ny_pred=regressor.predict(X_test)","27b29ba8":"from sklearn.metrics import confusion_matrix\ncm = pd.DataFrame(confusion_matrix(y_test, y_pred))\ncm['Total'] = np.sum(cm, axis=1)\ncm = cm.append(np.sum(cm, axis=0), ignore_index=True)\ncm.columns = ['Predicted No', 'Predicted Yes', 'Total']\ncm = cm.set_index([['Actual No', 'Actual Yes', 'Total']])\nprint(cm)","3bcfe5ad":"from sklearn.metrics import classification_report\nclf_report=classification_report(y_test,y_pred)\nprint(clf_report)","e1c3cea3":"Our model predicted very well as shown from above.","4858ba0e":"**Checking for Ouliers**","c58f4af8":"\nThe best method to remove ouliers in our dataset is Discretization, which is applied below."}}