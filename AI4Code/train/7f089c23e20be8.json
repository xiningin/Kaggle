{"cell_type":{"2ab43aca":"code","e79b33b9":"code","42b48423":"code","2daa7a2c":"code","89cee812":"code","c40b6bc0":"code","8440d99f":"code","3063a7c4":"code","1d89b270":"code","55726f02":"markdown","5460acd4":"markdown","a1c3cbd2":"markdown","e62ab99d":"markdown","2a5b9572":"markdown","d19d3e63":"markdown","d37a7636":"markdown","79cd96d3":"markdown","ffab7d56":"markdown"},"source":{"2ab43aca":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","e79b33b9":"sphist = pd.read_csv('..\/input\/sp500-index-data\/sphist.csv')\nprint(sphist.describe())\nprint(\"\\ndf shape: \", sphist.shape)\nsphist.head()","42b48423":"# Convert 'Date' column to Pandas date type\nsphist['Date'] = pd.to_datetime(sphist['Date'])\n\n# Sort df by that column\nsphist.sort_values(by=['Date'], inplace=True)\nsphist.head()","2daa7a2c":"# Add new indicators to each observation:\n# 1 \nsphist['avg_price_5'] = sphist['Close'].rolling(5).mean()\nsphist['avg_price_5'] = sphist['avg_price_5'].shift() # Avoid using current day's price by reindexing\n\n# 2\nsphist['avg_price_365'] = sphist['Close'].rolling(365).mean()\nsphist['avg_price_365'] = sphist['avg_price_365'].shift() # Avoid using current day's price by reindexing\n\n# 3\nsphist['avg_price_5_365'] = sphist['avg_price_5'] \/ sphist['avg_price_365']\n\n# 4\nsphist['std_price_5'] = sphist['Close'].rolling(5).std()\nsphist['std_price_5'] = sphist['std_price_5'].shift() # Avoid using current day's price by reindexing\n\n# 5\nsphist['std_price_365'] = sphist['Close'].rolling(365).std()\nsphist['std_price_365'] = sphist['std_price_365'].shift() # Avoid using current day's price by reindexing\n\n# 6\nsphist['std_price_5_365'] = sphist['std_price_5'] \/ sphist['std_price_365']\n\n# 7 \nsphist['avg_volume_5'] = sphist['Volume'].rolling(5).mean()\nsphist['avg_volume_5'] = sphist['avg_volume_5'].shift() # Avoid using current day's price by reindexing\n\n# 8\nsphist['avg_volume_365'] = sphist['Volume'].rolling(365).mean()\nsphist['avg_volume_365'] = sphist['avg_volume_365'].shift() # Avoid using current day's price by reindexing\n\n# 9\nsphist['avg_volume_5_365'] = sphist['avg_volume_5'] \/ sphist['avg_volume_365']\n\n# 10\nmin_last_year = sphist['Close'].rolling(365).min()\nsphist['last_min_current_ratio'] = min_last_year \/ sphist['Close']\nsphist['last_min_current_ratio'] = sphist['last_min_current_ratio'].shift()","89cee812":"print(\"# of observations before: \", sphist.shape[0])\nprint(\"NaN values before: \\n\\n\", sphist.isnull().sum())\n\nsphist = sphist[sphist['Date'] > datetime(year=1951, month=1, day=2)]\nsphist.dropna(axis=0, inplace=True)\n\nprint(\"\\n# of observations after: \", sphist.shape[0])\nprint(\"NaN values after: \\n\\n\", sphist.isnull().sum())","c40b6bc0":"train = sphist[sphist[\"Date\"] < datetime(year=2013, month=1, day=1)]\ntest = sphist[sphist[\"Date\"] >= datetime(year=2013, month=1, day=1)]\n\nprint(\"Train: \", train.shape)\nprint(\"Test: \", test.shape)","8440d99f":"# Sorted correlations with target column 'Close'\nsorted_corrs = sphist.corr()['Close'].sort_values()\n\nprint(sorted_corrs)\nfig, ax = plt.subplots(figsize=(15,10))\nsns.heatmap(sphist[sorted_corrs.index].corr())","3063a7c4":"features = ['avg_price_5', 'avg_price_365', 'avg_price_5_365', 'std_price_5', \n            'std_price_365', 'std_price_5_365', 'avg_volume_5', 'avg_volume_365', \n            'avg_volume_5_365', 'last_min_current_ratio']\n\nX_train = train[features]\ny_train = train['Close']\n\nX_test = test[features]\ny_test = test['Close']\n\n# Train\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\n# Predict\nclosing_price_pred_lr = lr.predict(X_test)\n\n# --------------------------------------------------\n# Performance metrics\n# --------------------------------------------------\n\n# Calculate MSE\nmse_lr = mean_squared_error(y_test, closing_price_pred_lr)\n\n# Calculate the absolute errors and MAPE\nerrors_lr = abs(closing_price_pred_lr - y_test)\nmape_lr = 100 * (errors_lr \/ y_test)\n\n# MAE\nmae_lr = round(np.mean(errors_lr), 2)\n\n# Accuracy\naccuracy_lr = 100 - np.mean(mape_lr)\n\nprint(\"-----------------\\nLinear regression\\n-----------------\")\nprint(\"MSE: \", mse_lr)\nprint(\"MAE: \", mae_lr, \"degrees\")\nprint('Accuracy:', round(accuracy_lr, 2), '%.')","1d89b270":"rf = RandomForestRegressor(n_estimators=150, random_state=1, min_samples_leaf=2)\n\n# Train \nrf.fit(X_train, y_train)\n\n# Predict\nclosing_price_pred_rf = rf.predict(X_test)\n\n# --------------------------------------------------\n# Performance metrics\n# --------------------------------------------------\n\n# Calculate the absolute errors and MAPE\nerrors_rf = abs(closing_price_pred_rf - y_test)\nmape_rf = 100 * (errors_rf \/ y_test)\n\n# MAE\nmae_rf = round(np.mean(errors_rf), 2)\n\n#\u00a0Accuracy\naccuracy_rf = 100 - np.mean(mape_rf)\n\nprint(\"-----------------\\nRandom Forest\\n-----------------\")\nprint(\"MAE: \", mae_rf, \"degrees\")\nprint('Accuracy:', round(accuracy_rf, 2), '%.')","55726f02":"<a id='section9'><\/a>\n# Random Forest","5460acd4":"<a id='section2'><\/a>\n# Read in the data\n\nThe dataset contains historical data on the price of the S&P500 Index. The columns are:\n\n- Date: The date of the record.\n- Open: The opening price of the day (when trading starts).\n- High: The highest trade price during the day.\n- Low: The lowest trade price during the day.\n- Close: The closing price for the day (when trading is finished).\n- Volume: The number of shares traded.\n- Adj Close: The daily closing price, adjusted retroactively to include any corporate actions.","a1c3cbd2":"---\n<a id='section1'><\/a>\n# Introduction\nThis project aims to predict the S&P500 Index using feature engineering, creating new features based on the historical data, and building both a linear regression and Random Forest model. ","e62ab99d":"<a id='section6'><\/a>\n## NaN values\nSince the new indicators require data from the previous 5 and 365 days, after adding them, considering the first observation corresponds to 1950-01-03, we need to:\n\n- Remove any rows from the DataFrame that fall before 1951-01-03\n- Remove any rows with NaN values","2a5b9572":"<a id='section7'><\/a>\n# Train\/Test split\n\n- Training set: Observations up to 2013-01-01\n- Test set: Observations after 2013-01-01","d19d3e63":"<a id='section8'><\/a>\n# Linear model","d37a7636":"<a id='section3'><\/a>\n# Feature engineering\n<a id='section4'><\/a>\n## Pandas date type","79cd96d3":"# Table of contents\n*  [Introduction](#section1) \n*  [Read in the data](#section2)\n*  [Feature engineering](#section3)\n    - [Pandas date type](#section4)\n    - [New indicators](#section5)\n    - [NaN values](#section6)\n*  [Train\/Test split](#section7)\n*  [Linear model](#section8)\n*  [Random Forest](#section9)\n\nby @samaxtech","ffab7d56":"<a id='section5'><\/a>\n## New indicators\n\nGiven the nature of the stock market, in order to prevent injecting future knowledge into the model, let's create indicators based on the past.\n\n- 1) Average price for the last 5 days\n- 2) Average price for the last 365 days\n- 3) Ratio between the average price for the past 5 days, and the average price for the past 365 days.\n- 4) Standard deviation of the price for the last 5 days\n- 5) Standard deviation of the price for the last 365 days\n- 6) Ratio between the standard deviation for the past 5 days, and the standard deviation for the past 365 days.\n- 7) The average volume over the past five days.\n- 8) The average volume over the past year.\n- 9) The ratio between the average volume for the past five days, and the average volume for the past year.\n- 10) The ratio between the lowest price in the past year and the current price."}}