{"cell_type":{"791b2bd9":"code","a9d44d50":"code","6d7961aa":"code","82a8e698":"code","8bae372a":"code","368a1392":"code","040b5029":"code","46da8612":"code","c09e2bcf":"code","1488a7ea":"code","fe3735e7":"code","e7a3fbce":"code","fb9b8fbd":"code","4d14be52":"code","47ceab0f":"code","becefbeb":"code","3bb3b734":"code","4a52f0f3":"code","6142604f":"code","fcccc64e":"markdown","1b4f88a0":"markdown","6f697225":"markdown","d3506bf3":"markdown","5db13951":"markdown","2488960e":"markdown","da6b1963":"markdown","aa0f15e7":"markdown","4da3ffab":"markdown","68230726":"markdown","47227d72":"markdown","bd6fbc9d":"markdown","a7137e35":"markdown","e7810eba":"markdown","2899038e":"markdown","f2c0fe2c":"markdown"},"source":{"791b2bd9":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix,precision_score\nfrom lightgbm import LGBMClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n#Matplotlib Config\nplt.style.use('fivethirtyeight')\n\nplt.rcParams['font.size'] = 16\nplt.rcParams['axes.labelsize'] = 20\n# plt.rcParams['axes.labelweight'] = 'bold'\nplt.rcParams['axes.titlesize'] = 24\nplt.rcParams['xtick.labelsize'] = 14\nplt.rcParams['ytick.labelsize'] = 14\nplt.rcParams['legend.fontsize'] = 12\nplt.rcParams['figure.titlesize'] = 20\nwidth, height = plt.figaspect(1.68)\nfig = plt.figure(figsize=(width,height), dpi=400)","a9d44d50":"df = pd.read_csv('\/kaggle\/input\/employee-future-prediction\/Employee.csv')","6d7961aa":"df.shape","82a8e698":"df.info()","8bae372a":"df.isna().sum()","368a1392":"duplicates = df.duplicated().sum()\ndf = df.drop_duplicates()\n\nprint('No. of duplicate records :',duplicates)\nprint('Shape after dropping duplicate records :',df.shape)","040b5029":"df.describe()","46da8612":"f, ax = plt.subplots(figsize=(10, 8))\ncorr = df.corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool),\n            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax ,annot=True)","c09e2bcf":"df['JoiningYear'] = df['JoiningYear'].astype('object')\nsns.countplot(data = df ,x='JoiningYear',hue='LeaveOrNot')","1488a7ea":"sns.countplot(data = df ,x='EverBenched',hue='LeaveOrNot')","fe3735e7":"sns.countplot(data = df ,x='ExperienceInCurrentDomain',hue='LeaveOrNot')","e7a3fbce":"sns.countplot(data = df ,x='Gender',hue='LeaveOrNot')","fb9b8fbd":"df['PaymentTier'] = df['PaymentTier'].astype('category')\nsns.countplot(data = df ,x='PaymentTier',hue='LeaveOrNot')","4d14be52":"sns.countplot(data = df ,x='City',hue='LeaveOrNot')","47ceab0f":"sns.countplot(data = df ,x='Education',hue='LeaveOrNot')","becefbeb":"groups = ['Young', 'MiddleAged', 'Adulthood']\ndf['AgeGroup'] = pd.qcut(df['Age'], q=3, labels=groups)\nsns.countplot(data = df ,x='AgeGroup',hue='LeaveOrNot')","3bb3b734":"X = df.drop('LeaveOrNot',axis=1)\ny= df.LeaveOrNot.values\n\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)","4a52f0f3":"#new feature engineering code which got  me lgbm's acc : 83% and precision 97%\n\nmulti_categories = ['AgeGroup','EverBenched','City','JoiningYear']\nordinal_cat = [['PHD','Masters','Bachelors'],[1,2,3]]\nbinary_categories=['Gender','EverBenched']\ntransformer = ColumnTransformer(transformers=[('ohe1', OneHotEncoder(sparse='False'), multi_categories),\n                                             ('oe',OrdinalEncoder(categories=ordinal_cat),['Education','PaymentTier']),\n                                             ('ohe2', OneHotEncoder(drop='first',sparse='False'), binary_categories)],remainder='passthrough')\n\n# old feat engineering code which got me lgbm's acc : 83% and precision 93% \n\n# df1= df.copy()\n# df1['PaymentTier'] = df['PaymentTier'].astype(np.uint8)\n# df1['LeaveOrNot'] = df['LeaveOrNot'].astype('category')\n# dummy_col = ['JoiningYear','City','AgeGroup']\n# df1=pd.get_dummies(df, columns=dummy_col,prefix = ['year','city','age'])\n# df1[\"is_masters\"] = df1[\"Education\"].map(lambda is_masters: 1 if is_masters == \"Masters\" else 0).astype(np.uint8)\n# df1[\"is_male\"] = df1[\"Gender\"].map(lambda is_male: 1 if is_male == \"Male\" else 0).astype(np.uint8)\n# df1[\"EverBenched\"] = df1['EverBenched'].map(lambda is_benched: 1 if is_benched =='Yes' else 0).astype(np.uint8)\n# df1['Senority'] = df1[['ExperienceInCurrentDomain','Age']].apply(lambda x: 1 if x.ExperienceInCurrentDomain >= 3 else 0, axis=1).astype(np.uint8) # 0 : junior , 1: senior\n# # df1['isRecentEmployee']= df['JoiningYear'].map(lambda x : 1 if (x == '2018'or'2017') else 0).astype(np.uint8)\n# df1=df1.drop(['Education','Gender'],axis=1)\n# df1.head()\n\nX_train = transformer.fit_transform(X_train)\nX_train.shape\nX_test = transformer.transform(X_test)","6142604f":"knc = KNeighborsClassifier()\nmnb = MultinomialNB()\ndtc = DecisionTreeClassifier(max_depth=7,random_state=2)\nlrc = LogisticRegression(solver='liblinear', penalty='l1')\nrfc = RandomForestClassifier(n_estimators=17, random_state=2,max_depth=5)\nabc = AdaBoostClassifier(n_estimators=17, random_state=2,learning_rate=0.2)\nbc = BaggingClassifier(n_estimators=17, random_state=2)\netc = ExtraTreesClassifier(n_estimators=50, random_state=2)\ngbdt = GradientBoostingClassifier(n_estimators=18,random_state=2)\nxgb = XGBClassifier(n_estimators=17,random_state=2,use_label_encoder=False,eval_metric='mlogloss')\nlgbm= LGBMClassifier(verbose=-1,\n                          learning_rate=0.1,\n                          max_depth=6,\n                          num_leaves=10, \n                          n_estimators=17,\n                          max_bin=500,random_state=2)\n\n\nclfs = {\n    'KN' : knc, \n    'NB': mnb, \n    'DT': dtc, \n    'LR': lrc, \n    'RF': rfc, \n    'AdaBoost': abc, \n    'BgC': bc, \n    'ETC': etc,\n    'GBDT':gbdt,\n    'xgb':xgb,\n    'lgbm':lgbm\n}\n\ndef train_classifier(clf,X_train,y_train,X_test,y_test):\n    clf.fit(X_train,y_train)\n    y_pred = clf.predict(X_test)\n    accuracy = accuracy_score(y_test,y_pred)\n    precision = precision_score(y_test,y_pred,zero_division=0)\n    \n    return accuracy,precision\n\naccuracy_scores = []\nprecision_scores = []\n\nfor name,clf in clfs.items():\n    \n    current_accuracy,current_precision = train_classifier(clf, X_train,y_train,X_test,y_test)\n    \n#     print(\"For \",name)\n#     print(\"Accuracy - \",current_accuracy)\n#     print(\"Precision - \",current_precision)\n    \n    accuracy_scores.append(current_accuracy)\n    precision_scores.append(current_precision)\n    \n\nperformance_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy':accuracy_scores,'Precision':precision_scores}).sort_values('Precision',ascending=False)\nperformance_df","fcccc64e":"<h1 id=\"model\" style=\"color:#E36149;\">Building Models..<\/h1>","1b4f88a0":"**No missing values**","6f697225":"<h1 id =\"libraries\" style=\"color:#E36149;\">Libraries<\/h1>","d3506bf3":"* **Majority of the Employees who pursued Bachelors are working for these companies**\n* **Education can be considered as Ordinal variable where  PHD > Masters > Bachelors**","5db13951":"* **From the bar chart , it is clear that Young Employees left their jobs more than the MiddleAged & Adulthood Employees.**\n* **Maybe young employees who leave want to pursue their dreams**","2488960e":"<h1 id=\"eda\" style=\"color:#E36149;\">EDA<\/h1>","da6b1963":"**Its surprising that Employees being Benched on working projects have not left the company \ud83d\ude02 jokes apart ..Back to EDA**","aa0f15e7":"**Surprising ! Majority of the employees joined in 2018 (recently) have left the company**","4da3ffab":"* **'PaymentTier' & 'Age' have a very weak negative correlation with Our Target variable ('LeaveOrNot')**\n* **'JoiningYear' has a very weak positive correlation with Target variable ('LeaveOrNot')**","68230726":"* **Most of the Employees are having payment Tier 3**\n* **By observing the trend it seems that 'PaymentTier' Catergory is an Ordinal Variable**\n* **Where, Tier 3 > Tier 2 > Tier 1**","47227d72":"<h1 id=\"feature\"style=\"color:#E36149;\">Feature Engineering<\/h1>","bd6fbc9d":"* **There is not much data available where Employees having Experience in their Current Domain Greater than 5 Years**\n* **As the experience of Employees increases they chose to work\/stay with company**","a7137e35":"<h2 style=\"color:#E36149;\">\n<br><br>\n Feedbacks and Suggestions for improving models are welcome \ud83d\ude03<\/h2>\n\n","e7810eba":"*  **Majority of the Employees residing \/ Working in Bangalore & New Delhi chose not to leave the company**\n* **Maybe the work culture in Bangalore & New Delhi is pretty good when compared to Pune**\n","2899038e":"* **Majority of the Male Employees choose not to leave the company**\n* **While the ratio ( Leave \/ Not Leave ) of the Female Employees is  almost equal to 1**","f2c0fe2c":"**Best Working Model is LGBM\n with Accuracy of 83% and Precision of 97%**"}}