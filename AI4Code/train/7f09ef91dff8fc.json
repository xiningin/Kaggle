{"cell_type":{"b6784b7c":"code","3492f0aa":"code","06613600":"code","861e8195":"code","2c463281":"code","557b3c35":"code","fdd318b1":"code","62733593":"code","fbfa8a61":"code","23ced9a9":"code","8c8a77b2":"code","f488d6a2":"code","e91be7ae":"code","31eefd2c":"code","4516ee70":"code","8aa8b0fb":"code","cfc19bf3":"code","2bd037e6":"code","22031a76":"code","86c1a8ad":"code","7afafcd0":"code","697a4981":"code","2170cc8d":"code","e5ee51e6":"code","69bfb23b":"markdown","5f9ee887":"markdown","56d5b77f":"markdown","27724a55":"markdown","019fb274":"markdown","1329c562":"markdown"},"source":{"b6784b7c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3492f0aa":"import missingno as msno\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss\n\nfrom lightgbm import LGBMClassifier\n\n#import lightgbm as lgb\n#import optuna.integration.lightgbm as lgb\n\nimport optuna\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","06613600":"pd.set_option('display.max_columns', 100)","861e8195":"sample_submission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/test.csv\")","2c463281":"sample_submission","557b3c35":"train","fdd318b1":"test","62733593":"train = train.drop(columns=['id'],axis=1)\ntest = test.drop(columns=['id'],axis=1)","fbfa8a61":"# Search for missing data\n\nmsno.matrix(df=train, figsize=(10,6), color=(0,.3,.3))","23ced9a9":"# Search for missing data\n\nmsno.matrix(df=test, figsize=(10,6), color=(0,.3,.3))","8c8a77b2":"plt.figure(figsize=(10,6))\n#sns.countplot(x='target', data=train, order=train['target'].value_counts().index)\nsns.countplot(x='target', data=train, order=sorted(train['target'].unique()))","f488d6a2":"train.drop(columns=['target']).describe().T\\\n        .style.bar(subset=['mean'], color=px.colors.qualitative.G10[0])\\\n        .background_gradient(subset=['std'], cmap='Greens')\\\n        .background_gradient(subset=['50%'], cmap='BuGn')","e91be7ae":"test.describe().T\\\n        .style.bar(subset=['mean'], color=px.colors.qualitative.G10[0])\\\n        .background_gradient(subset=['std'], cmap='Greens')\\\n        .background_gradient(subset=['50%'], cmap='BuGn')","31eefd2c":"le = LabelEncoder()\ntrain['target'] = le.fit_transform(train['target'])","4516ee70":"train","8aa8b0fb":"train_corr = train.corr()\ntrain_corr","cfc19bf3":"plt.figure(figsize=(20,10))\nsns.heatmap(train_corr, vmin=0, vmax=0.12, center=0, square=False, annot=False, cmap='coolwarm');","2bd037e6":"X = train.drop('target',axis=1)\ny = train['target']","22031a76":"def cross_val(X, y, model, params, folds=10):\n    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=3)#(3,21)1\uff5e30\n    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n        print(f\"Fold: {fold}\")\n        x_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        x_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n\n        alg = model(**params)\n        alg.fit(x_train, y_train,\n                eval_set=[(x_test, y_test)],\n                early_stopping_rounds=100,\n                verbose=100)\n\n        pred = alg.predict_proba(x_test)\n        loss = log_loss(y_test, pred)\n        print(f\"Log loss: {loss}\")\n        print(\"-\"*62)\n    \n    return alg","86c1a8ad":"lgb_params= {'learning_rate': 0.04, #0.045\n             'n_estimators': 20000, \n             'max_bin': 94,\n             'num_leaves': 10,\n             'max_depth': 8,\n             'reg_alpha': 8.457,\n             'reg_lambda': 6.853,\n             'subsample': 0.749\n             }","7afafcd0":"lgb_model = cross_val(X, y, LGBMClassifier, lgb_params)","697a4981":"result = lgb_model.predict_proba(test)","2170cc8d":"sample_submission[['Class_1','Class_2', 'Class_3', 'Class_4','Class_5','Class_6', 'Class_7', 'Class_8', 'Class_9']] = result\nsample_submission.to_csv(f'lgb.csv',index=False)","e5ee51e6":"sample_submission","69bfb23b":"# 4. Modeling","5f9ee887":"# 3. Check the correlation between each item","56d5b77f":"# 2. Preprocessing","27724a55":"# 6.Make submission file","019fb274":"# 1.Import data","1329c562":"# 5.Prediction"}}