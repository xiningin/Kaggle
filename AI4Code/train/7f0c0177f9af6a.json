{"cell_type":{"6b53e058":"code","e3b5e313":"code","be993192":"code","f26728a7":"code","801f7fcd":"code","76a3a742":"code","f4427a4c":"code","9550e124":"code","a0b529ea":"code","9feb7814":"code","2c88a727":"code","79d3ca95":"code","4f67026d":"code","39153327":"code","8c50f5b9":"code","0a29e7b3":"code","3b2546f1":"code","5dbff3db":"code","1b934030":"code","553fe8da":"code","870f65c9":"code","d4accd1f":"code","3a500c46":"code","cf7453ad":"code","5618b6e2":"code","033dae81":"code","ff483089":"code","fb28fbb1":"code","f4b16ce4":"code","c606fc60":"code","eb492d98":"code","276e517e":"code","b7f46b4f":"code","6c8fbc40":"code","57f30559":"code","c59a9697":"code","849b9f7c":"code","12f4f32d":"code","6401ced7":"code","d6dc6e66":"code","837ddb71":"code","41accac3":"code","9bf29d45":"code","ba9ab5dd":"code","89777d5f":"code","2a868abf":"code","0790597c":"code","793971b2":"code","90ffb7ea":"code","a29d9858":"code","02af4cdc":"code","c50870d9":"markdown","e149e35b":"markdown","cb603162":"markdown","e9f35d93":"markdown","7efd1aeb":"markdown","f96be911":"markdown","43d8fffd":"markdown","9068f7b8":"markdown","68e940b0":"markdown","bb43503b":"markdown","4f718d0f":"markdown","54412b6c":"markdown","3796ab68":"markdown"},"source":{"6b53e058":"!pip install whoosh\n!pip install ipywidgets\n!pip install sentence-transformers","e3b5e313":"import os\nimport json\nimport re\nimport logging\nfrom tqdm import tqdm\nimport itertools\nfrom itertools import chain","be993192":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy import spatial\nfrom scipy.special import softmax\nfrom scipy.spatial.distance import cosine","f26728a7":"from spacy.lang.en.stop_words import STOP_WORDS as stop_words\nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem import WordNetLemmatizer\nfrom collections import Counter\nfrom sklearn.metrics import confusion_matrix\nfrom gensim.models import Word2Vec","801f7fcd":"from whoosh.fields import Schema, TEXT, ID, BOOLEAN\nfrom whoosh.index import create_in\nfrom whoosh.qparser import MultifieldParser\nimport whoosh.index as search_index\nfrom whoosh import highlight\nimport math","76a3a742":"import torch\nfrom transformers import BertTokenizer\nfrom transformers import BertForQuestionAnswering\nfrom sentence_transformers import SentenceTransformer\nimport random\nimport time\nimport datetime","f4427a4c":"import matplotlib.pyplot as plt\nimport ipywidgets as widgets\nfrom IPython.core.display import display, HTML","9550e124":"class DataLoader:\n    \"\"\"\n    Data loader for build the dataset from metadata.csv, 4 data folders containing the json files for the articles\n    \"\"\"\n    data_folder = '..\/input\/CORD-19-research-challenge\/'\n    metadata_path = os.path.join(data_folder, 'metadata.csv')\n    default_dataset_path = 'merged_dataset.pickle'\n    key_words = ['ncov',\n        'covid19',\n        'covid-19',\n        'sars cov2',\n        'sars cov-2',\n        'sars-cov-2',\n        'sars coronavirus 2',\n        '2019-ncov',\n        '2019 novel coronavirus',\n        '2019-ncov sars',\n        'cov-2',\n        'cov2',\n        'novel coronvirus',\n        'coronavirus 2019-ncov']\n    \n    def __init__(self, dataset_path):\n        \n        ## If dataset_path is None, set dataset_path to self.default_dataset_path\n        if dataset_path is None:\n            dataset_path = self.default_dataset_path\n        \n        try:\n            self.dataset = pd.read_pickle(dataset_path)\n            self.dataset_path = dataset_path\n            self.build = False\n        except FileNotFoundError:\n            self.dataset_path = self.default_dataset_path\n            self.build = True\n    \n    def build_if_not_exist(self):\n        \"\"\"\n        The method builds the dataset that merges the information in metadata.csv and json files\n        \n        \"\"\"\n        if self.build and not os.path.exists(self.dataset_path):\n            \n            print(f'Building the dataset at {self.dataset_path}')\n            \n            dataset = self._load_articles_from_folder()\n            metadata = self._load_metadata()\n            \n            print(f'There are {len(pd.concat([metadata[\"title\"], dataset[\"title\"]]).drop_duplicates())} articles in the dataset')\n\n            dataset = self._merge_with_metadata(metadata, dataset)\n            dataset = dataset[(dataset['title'] != '') & dataset['title'].notna()] \\\n                .reset_index().drop(['index'], axis=1).reset_index().rename(columns={'index': 'id'})\n\n            dataset['full_text'] = dataset['full_text'].astype(str)\n            dataset['abstract'] = dataset['abstract'].astype(str)\n            dataset['is_covid_19'] = (dataset['title'] + dataset['abstract'].astype(str)).str.lower() \\\n                .apply(lambda t: sum([key in t for key in self.key_words]) > 0)\n            \n            dataset.to_pickle(self.dataset_path)\n            \n            print(f'Total records in the merged dataset {len(dataset)}')\n        \n        return self\n    \n    def get_dataset(self):\n        \"\"\"\n        Get the dataset from the dataloader\n        \"\"\"\n        if hasattr(self, 'dataset'):\n            return self.dataset\n        else:\n            try:\n                self.build_if_not_exist()\n                self.dataset = pd.read_pickle(self.dataset_path)\n                return self.dataset\n            except FileNotFoundError:\n                print(f'dataset doesn`t exist at {self.dataset_path}')\n        \n        return None\n    \n    def _load_metadata(self):\n        \n        metadata = pd.read_csv(self.metadata_path)\n        metadata = metadata[['sha', 'title', 'abstract']].drop_duplicates()\n        print(f'Total records in metadata {len(metadata)}')\n        print(f'Total records in metadata after dropping the duplicates {len(metadata)}')\n        return metadata\n    \n    def _load_articles_from_folder(self):\n        \n        articles = []\n\n        for (dirpath, dirnames, filenames) in os.walk(self.data_folder):\n            if filenames:\n                for filename in tqdm([f for f in filenames if re.search('.+\\.json$', f) is not None]):\n                    file_path = os.path.join(dirpath, filename)\n                    paper_id, title, abstract, full_text, reference = self._load_article_from_json(file_path)\n                    articles.append((paper_id, title, abstract, full_text, reference))\n        \n        return pd.DataFrame(articles, columns=['sha', 'title', 'abstract', 'full_text', 'reference'])\n    \n    def _load_article_from_json(self, file_path):\n        \"\"\"\n        Load the article json file\n        \"\"\"\n        with open(file_path) as json_file: \n\n            data = json.load(json_file)\n            paper_id = data['paper_id']\n            title = data['metadata']['title'].replace('\\n', '')\n            abstract = '\\n'.join([a['text'] for a in data.get('abstract', [])])\n            full_text = '\\n'.join([body_text.get('text', '') for body_text in data.get('body_text', [])])\n            reference = '\\n'.join([v['title'] for k, v in data.get('bib_entries', dict()).items()])\n\n        return (paper_id, title, abstract, full_text, reference)\n    \n    def _merge_with_metadata(self, metadata, dataset):\n    \n        def _coalease(_df, _col):\n            _col_x = _col + '_x'\n            _col_y = _col + '_y'\n            _df[_col] = _df[_col_x].combine_first(_df[_col_y])\n            _df.drop(_col_x, axis=1, inplace=True)\n            _df.drop(_col_y, axis=1, inplace=True)\n\n            return _df\n\n        if 'full_text' not in metadata.columns:\n            metadata['full_text'] = np.nan\n\n        if 'reference' not in metadata.columns:\n            metadata['reference'] = np.nan\n\n        _combined = metadata.merge(dataset, on='sha', how='outer')\n        _combined = _coalease(_combined, 'title')\n        _combined = _coalease(_combined, 'abstract')\n        _combined = _coalease(_combined, 'full_text')\n        _combined = _coalease(_combined, 'reference')\n\n        _combined = _combined.merge(dataset, on='title', how='outer')\n        _combined = _coalease(_combined, 'sha')\n        _combined = _coalease(_combined, 'abstract')\n        _combined = _coalease(_combined, 'full_text')\n        _combined = _coalease(_combined, 'reference')\n\n        return _combined.sort_values(['title', 'sha', 'full_text'], na_position='last') \\\n            .drop_duplicates(['title'], keep='first')\n","a0b529ea":"class SearchEngine:\n    \n    lemmatizer = WordNetLemmatizer() \n    default_index_folder = 'index'\n    batch_size = 10000\n    \n    def __init__(self, index_folder, dataset):\n        \"\"\"\n        Initialize SearchEngine with an index folder and the pandas dataset that contains the article raw data\n        \"\"\"\n        if index_folder is None or not os.path.exists(index_folder):\n            if not os.path.exists(self.default_index_folder):\n                os.mkdir(self.default_index_folder)\n                self.built = True\n            else:\n                self.built = False\n            self.index_folder = self.default_index_folder\n        else:\n            self.index_folder = index_folder\n            self.built = False\n        \n        self.dataset = dataset\n    \n    def search_articles(self, query, num):\n        \"\"\"\n        Search for the article in the index\n        \"\"\"\n        #query = ' '.join(self._tokenize_text(query))\n        ix = search_index.open_dir(self.index_folder)\n        with ix.searcher() as searcher:\n            parser = MultifieldParser(['title', 'abstract', 'full_text'], ix.schema)\n            processed_query = query + ' AND (is_covid_19:TRUE)'\n            parsed_query = parser.parse(processed_query)\n            results = searcher.search(parsed_query, limit=num)\n            results.fragmenter = highlight.ContextFragmenter(surround=200)\n            return [self._process_hit(result) for result in results]\n        \n        return []\n        \n    def _process_hit(self, hit):\n        \"\"\"\n        Extract the article data from the dataset based on the paper_id in the search hit\n        \"\"\"\n        paper_id = int(hit['paper_auto_id'])\n        record = self.dataset[self.dataset['id'] == paper_id]\n        title = record['title'].iloc[0]\n        abstract = record['abstract'].iloc[0]\n        full_text = record['full_text'].iloc[0]\n        reference = record['reference'].iloc[0]\n        score = hit.score\n        abstract_highlight = hit.highlights('abstract', text=abstract)        \n        full_text_highlight = hit.highlights('full_text', text=full_text)        \n        reference_highlight = hit.highlights('reference', text=full_text)        \n\n        return {'title': title, \n                'abstract': abstract, \n                'full_text': full_text, \n                'score': score, \n                'abstract_highlight': abstract_highlight, \n                'full_text_highlight': full_text_highlight, \n                'reference_highlight': reference_highlight}\n\n    def build_index_if_not_exist(self):\n        \"\"\"\n        Build the index in the default folder if the provided index_folder does not have index\n        \"\"\"\n        if self.built:\n            print(f'Building the index at {self.index_folder}')\n            num_of_batches = math.ceil(len(self.dataset) \/ self.batch_size)\n            \n            for i in range(num_of_batches):\n                print(f'Building index for batch no. {i+1} for {self.batch_size} records')\n                self._build_index_batch(self.dataset[i * self.batch_size: (1 + i) * self.batch_size])\n        else:\n            print(f'The index exists at {self.index_folder}')\n        \n        return self\n    \n    def _build_index_batch(self, batch):\n        \n        ix = create_in(self.index_folder, self._get_schema())\n        writer = ix.writer(limitmb=2048)\n        \n        for _, row in tqdm(batch.replace(np.nan, '', regex=True).iterrows()):\n    \n            paper_auto_id = str(row[0])\n            title = row[1]\n            abstract = row[3]\n            full_text = row[4]\n            reference = row[5]\n            is_covid_19 = row[6]\n            writer.add_document(paper_auto_id=paper_auto_id,\n                                title=title,\n                                abstract=abstract,\n                                full_text=full_text,\n                                reference=reference, \n                                is_covid_19=is_covid_19)\n\n        writer.commit()\n        \n    def _get_schema(self):\n        schema = Schema(\n            paper_auto_id=ID(stored=True), \n            title=TEXT(stored=True, phrase=True), \n            abstract=TEXT(stored=False, phrase=True),\n            full_text=TEXT(stored=False, phrase=True),\n            reference=TEXT(stored=False, phrase=True),\n            is_covid_19=BOOLEAN(stored=False)\n        )\n        return schema\n    \n    def _tokenize_text(self, text):\n        return [self.lemmatizer.lemmatize(w) for w in word_tokenize(text.lower()) if not w in stop_words and w.isalpha()]","9feb7814":"class BertQA:\n    \n    def __init__(self):\n        BERT_SQUAD = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n        self.model = BertForQuestionAnswering.from_pretrained(BERT_SQUAD)\n        self.tokenizer = BertTokenizer.from_pretrained(BERT_SQUAD)\n        self.torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.model.to(self.torch_device)\n        self.model.eval()\n        \n    def answer_question(self, question, context):\n        # anser question given question and context\n        encoded_dict = self.tokenizer.encode_plus(\n                            question, context,\n                            add_special_tokens = True,\n                            max_length = 512,\n                            pad_to_max_length = True,\n                            return_tensors = 'pt'\n                       )\n\n        input_ids = encoded_dict['input_ids'].to(self.torch_device)\n        token_type_ids = encoded_dict['token_type_ids'].to(self.torch_device)\n\n        start_scores, end_scores = self.model(input_ids, token_type_ids=token_type_ids)\n\n        all_tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])\n        start_index = torch.argmax(start_scores)\n        end_index = torch.argmax(end_scores)\n\n        answer = self.tokenizer.convert_tokens_to_string(all_tokens[start_index:end_index+1])\n        answer = answer.replace('[CLS]', '').replace('[SEP]', '')\n        return answer\n    \n#Full credit goes to https:\/\/www.kaggle.com\/jonathanbesomi\/a-qa-model-to-answer-them-all. That's where I got this code snippet.","2c88a727":"#Load the dataset from the specified location, if the dataset is missing, it builds the dataset from scratch. \ndata_loader = DataLoader(None)\ndataset = data_loader.build_if_not_exist().get_dataset()\n\n#Initialize the search engine and build the index from scratch if index folder doesn't exist in the input folder. \nsearch_engine = SearchEngine(None, dataset).build_index_if_not_exist()\n\n#Initalize the sentence transformer model downloaded from https:\/\/github.com\/UKPLab\/sentence-transformers. \nsentence_transformers_model = SentenceTransformer('bert-large-nli-mean-tokens')","79d3ca95":"def perform_task(main_query, subtask_list, num):\n    \n    def get_top_hits(sen_ques_dataframe, sent_i, num):\n        return sen_ques_dataframe[sen_ques_dataframe['question_index'] == sent_i] \\\n            .sort_values(['cos'], ascending=False)['sentence'].iloc[0:num].to_list()\n    \n    def tokenize_text(text):\n            return [lemmatizer.lemmatize(w) for w in word_tokenize(text.lower()) if not w in stop_words and w.isalpha()]\n        \n    lemmatizer = WordNetLemmatizer()\n    \n    question_embeddings = sentence_transformers_model.encode(subtask_list)\n    results = search_engine.search_articles(main_query, num)\n    \n    html_string = ''\n\n    for index, result in enumerate(results):\n\n        title = result['title']\n        abstract = result['abstract']\n        score = result['score']\n        full_text = result['full_text']\n        full_text = abstract if full_text == '' or full_text is None else full_text\n        original_sentences = re.split('[\\.?!]\\s+', full_text)\n        original_sentences = [sent for sent in original_sentences if len(tokenize_text(sent)) >= 5]\n\n        if len(original_sentences) > 0:\n\n            sentences = [' '.join(tokenize_text(sent)) for sent in original_sentences] \n            sentence_embeddings = sentence_transformers_model.encode(sentences)\n            sentence_zip = enumerate(zip(original_sentences, sentence_embeddings))\n            task_zip = enumerate(zip(subtask_list, question_embeddings))\n            sen_ques = [(sen_t[0], sen_t[1][0], sen_t[1][1], ques_t[0], ques_t[1][0], ques_t[1][1]) \\\n                 for sen_t, ques_t in itertools.product(sentence_zip, task_zip)]\n\n            sen_ques_dataframe = pd.DataFrame(sen_ques, columns=['sentence_index', \n                                                                 'sentence', \n                                                                 'sentence_embeddings', \n                                                                 'question_index', \n                                                                 'question', \n                                                                 'question_embeddings'])\n\n            sen_ques_dataframe['cos'] = sen_ques_dataframe.apply(lambda x: 1 - cosine(x.sentence_embeddings, x.question_embeddings), axis=1)\n            sen_ques_dataframe['rank'] = sen_ques_dataframe.groupby('sentence_index')['cos'].rank(\"dense\", ascending=False)\n            confidences = softmax(sen_ques_dataframe.groupby('question_index')['cos'].sum())\n            \n            confidences = confidences.sort_values(ascending=False).reset_index().to_records(index=False)\n            cosine_sum = sen_ques_dataframe[sen_ques_dataframe['rank'].astype(int) == 1]['cos'].sum()\n\n            html_string += f'<h2>{index+1}. {title}<\/h2><br><b>Abstract<\/b>: {abstract}<br><br>'\n            for conf in confidences:\n                if conf[1] < 0.01: \n                    break\n                html_string += f'<b>Subtask:<\/b> {subtask_list[conf[0]]}<br>'\n                html_string += f'<b>Confidence:<\/b> {round(conf[1], 2)}<br>'\n                html_string += f'<b>Top matchin sentences:<\/b><br>'\n                html_string += '<br>'.join(get_top_hits(sen_ques_dataframe, conf[0], 3))\n                html_string += '<br><br>'\n            html_string += '<br>'\n\n    return html_string","4f67026d":"task_1_search_query = '(transmission OR (incubation AND period) OR (environmental AND factor))'","39153327":"task1 = [\"Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\",\n\"Prevalence of asymptomatic shedding and transmission (e.g., particularly children).\",\n\"Seasonality of transmission.\",\n\"Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).\",\n\"Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).\",\n\"Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\",\n\"Natural history of the virus and shedding of it from an infected person\",\n\"Implementation of diagnostics and products to improve clinical processes\",\n\"Disease models, including animal models for infection, disease and transmission\",\n\"Tools and studies to monitor phenotypic change and potential adaptation of the virus\",\n\"Immune response and immunity\",\n\"Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\",\n \"Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\",\n\"Role of the environment in transmission\"]","8c50f5b9":"html_string = perform_task(task_1_search_query, task1, 50)","0a29e7b3":"display(HTML(html_string))","3b2546f1":"task_2_searh_query = '(risk AND factors)'","5dbff3db":"task2 = ['Data on potential risks factors',\n'Smoking, pre-existing pulmonary disease',\n'Co-infections (determine whether co-existing respiratory\/viral infections make the virus more transmissible or virulent) and other co-morbidities',\n'Neonates and pregnant women',\n'Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.',\n'Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors', \n'Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups',\n'Susceptibility of populations',\n'Public health mitigation measures that could be effective for control']","1b934030":"html_string = perform_task(task_2_searh_query, task2, 50)","553fe8da":"display(HTML(html_string))","870f65c9":"task_3_search_query = '((virus AND genetics) OR origin OR evolution)'","d4accd1f":"task3 = ['Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the virus over time.',\n'Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences, and determine whether there is more than one strain in circulation. Multi-lateral agreements such as the Nagoya Protocol could be leveraged.',\n'Evidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over.',\n'Evidence of whether farmers are infected, and whether farmers could have played a role in the origin.',\n'Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia.',\n'Experimental infections to test host range for this pathogen.',\n'Animal host(s) and any evidence of continued spill-over to humans',\n'Socioeconomic and behavioral risk factors for this spill-over',\n'Sustainable risk reduction strategies']","3a500c46":"html_string = perform_task(task_3_search_query, task3, 50)","cf7453ad":"display(HTML(html_string))","5618b6e2":"task_4_search_query = '((non-pharmaceutical AND interventions) OR (tranditional AND medicine) OR (alternative AND medicine) OR (herbal AND medicine))'","033dae81":"task4 = [\"Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases.\",\n\"Rapid design and execution of experiments to examine and compare NPIs currently being implemented. DHS Centers for Excellence could potentially be leveraged to conduct these experiments.\",\n\"Rapid assessment of the likely efficacy of school closures, travel bans, bans on mass gatherings of various sizes, and other social distancing approaches.\",\n\"Methods to control the spread in communities, barriers to compliance and how these vary among different populations..\",\n\"Models of potential interventions to predict costs and benefits that take account of such factors as race, income, disability, age, geographic location, immigration status, housing status, employment status, and health insurance status.\",\n\"Policy changes necessary to enable the compliance of individuals with limited resources and the underserved with NPIs.\",\n\"Research on why people fail to comply with public health advice, even if they want to do so (e.g., social or financial costs may be too high).\",\n\"Research on the economic impact of this or any pandemic. This would include identifying policy and programmatic alternatives that lessen\/mitigate risks to critical government services, food distribution and supplies, access to critical household supplies, and access to health diagnoses, treatment, and needed care, regardless of ability to pay.\"]","ff483089":"html_string = perform_task(task_4_search_query, task4, 50)","fb28fbb1":"display(HTML(html_string))","f4b16ce4":"task_5_search_query = '(vaccines OR therapeutics)'","c606fc60":"task5 = [\"Effectiveness of drugs being developed and tried to treat COVID-19 patients. Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.\",\n\"Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\",\n\"Exploration of use of best animal models and their predictive value for a human vaccine.\",\n\"Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\",\n\"Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.\",\n\"Efforts targeted at a universal coronavirus vaccine.\",\n\"Efforts to develop animal models and standardize challenge studies\",\n\"Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers\",\n\"Approaches to evaluate risk for enhanced disease after vaccination\",\n\"Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics]\"]","eb492d98":"html_string = perform_task(task_5_search_query, task5, 50)","276e517e":"display(HTML(html_string))","b7f46b4f":"task_6_sbearch_query = '(ethical AND considerationsand) OR (social OR science OR considerations))'","6c8fbc40":"task6 = [\"Efforts to articulate and translate existing ethical principles and standards to salient issues in COVID-2019\", \n\"Efforts to embed ethics across all thematic areas, engage with novel ethical issues that arise and coordinate to minimize duplication of oversight\",\n\"Efforts to support sustained education, access, and capacity building in the area of ethics\",\n\"Efforts to establish a team at WHO that will be integrated within multidisciplinary research and operational platforms and that will connect with existing and expanded global networks of social sciences.\",\n\"Efforts to develop qualitative assessment frameworks to systematically collect information related to local barriers and enablers for the uptake and adherence to public health measures for prevention and control. This includes the rapid identification of the secondary impacts of these measures. (e.g. use of surgical masks, modification of health seeking behaviors for SRH, school closures)\",\n\"Efforts to identify how the burden of responding to the outbreak and implementing public health measures affects the physical and psychological health of those providing care for Covid-19 patients and identify the immediate needs that must be addressed.\",\n\"Efforts to identify the underlying drivers of fear, anxiety and stigma that fuel misinformation and rumor, particularly through social media.\"]","57f30559":"html_string = perform_task(task_6_sbearch_query, task6, 50)","c59a9697":"display(HTML(html_string))","849b9f7c":"task_7_search_query = '(diagnostics OR surveillance)'","12f4f32d":"task7 = [\"How widespread current exposure is to be able to make immediate policy recommendations on mitigation measures. Denominators for testing and a mechanism for rapidly sharing that information, including demographics, to the extent possible. Sampling methods to determine asymptomatic disease (e.g., use of serosurveys (such as convalescent samples) and early detection of disease (e.g., use of screening of neutralizing antibodies such as ELISAs).\",\n\"Efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms.\",\n\"Recruitment, support, and coordination of local expertise and capacity (public, private\u2014commercial, and non-profit, including academic), including legal, ethical, communications, and operational issues.\",\n\"National guidance and guidelines about best practices to states (e.g., how states might leverage universities and private laboratories for testing purposes, communications to public health officials and the public).\",\n\"Development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy.\",\n\"Rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity. These experiments could aid in collecting longitudinal samples, which are critical to understanding the impact of ad hoc local interventions (which also need to be recorded).\",\n\"Separation of assay development issues from instruments, and the role of the private sector to help quickly migrate assays onto those devices.\",\n\"Efforts to track the evolution of the virus (i.e., genetic drift or mutations) and avoid locking into specific reagents and surveillance\/detection schemes.\",\n\"Latency issues and when there is sufficient viral load to detect the pathogen, and understanding of what is needed in terms of biological and environmental sampling.\",\n\"Use of diagnostics such as host response markers (e.g., cytokines) to detect early disease or predict severe disease progression, which would be important to understanding best clinical practice and efficacy of therapeutic interventions.\",\n\"Policies and protocols for screening and testing.\",\n\"Policies to mitigate the effects on supplies associated with mass testing, including swabs and reagents.\",\n\"Technology roadmap for diagnostics.\",\n\"Barriers to developing and scaling up new diagnostic tests (e.g., market forces), how future coalition and accelerator models (e.g., Coalition for Epidemic Preparedness Innovations) could provide critical funding for diagnostics, and opportunities for a streamlined regulatory environment.\",\n\"New platforms and technology (e.g., CRISPR) to improve response times and employ more holistic approaches to COVID-19 and future diseases.\",\n\"Coupling genomics and diagnostic testing on a large scale.\",\n\"Enhance capabilities for rapid sequencing and bioinformatics to target regions of the genome that will allow specificity for a particular variant.\",\n\"Enhance capacity (people, technology, data) for sequencing with advanced analytics for unknown pathogens, and explore capabilities for distinguishing naturally-occurring pathogens from intentional.\",\n\"One Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism and future pathogens, including both evolutionary hosts (e.g., bats) and transmission hosts (e.g., heavily trafficked and farmed wildlife and domestic food and companion species), inclusive of environmental, demographic, and occupational risk factors.\"]","6401ced7":"html_string = perform_task(task_7_search_query, task7, 50)","d6dc6e66":"display(HTML(html_string))","837ddb71":"task_8_search_query = '(medical AND care)'","41accac3":"task8 = [\"Resources to support skilled nursing facilities and long term care facilities.\",\n\"Mobilization of surge medical staff to address shortages in overwhelmed communities\",\n\"Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with\/without other organ failure \u2013 particularly for viral etiologies\",\n\"Extracorporeal membrane oxygenation (ECMO) outcomes data of COVID-19 patients\",\n\"Outcomes data for COVID-19 after mechanical ventilation adjusted for age.\",\n\"Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest.\",\n\"Application of regulatory standards (e.g., EUA, CLIA) and ability to adapt care to crisis standards of care level.\",\n\"Approaches for encouraging and facilitating the production of elastomeric respirators, which can save thousands of N95 masks.\",\n\"Best telemedicine practices, barriers and faciitators, and specific actions to remove\/expand them within and across state boundaries.\",\n\"Guidance on the simple things people can do at home to take care of sick people and manage disease.\",\n\"Oral medications that might potentially work.\",\n\"Use of AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in a way that could not be done manually.\",\n\"Best practices and critical challenges and innovative solutions and technologies in hospital flow and organization, workforce protection, workforce allocation, community-based support resources, payment, and supply chain management to enhance capacity, efficiency, and outcomes.\",\n\"Efforts to define the natural history of disease to inform clinical care, public health interventions, infection prevention control, transmission, and clinical trials\",\n\"Efforts to develop a core clinical outcome set to maximize usability of data across a range of trials\",\n\"Efforts to determine adjunctive and supportive interventions that can improve the clinical outcomes of infected patients (e.g. steroids, high flow oxygen)\"]","9bf29d45":"html_string = perform_task(task_8_search_query, task8, 50)","ba9ab5dd":"display(HTML(html_string))","89777d5f":"task_9_search_query = '((information AND sharing) OR (inter-sectoral AND collaboration))'","2a868abf":"task9 = [\"Methods for coordinating data-gathering with standardized nomenclature.\",\n\"Sharing response information among planners, providers, and others.\",\n\"Understanding and mitigating barriers to information-sharing.\",\n\"How to recruit, support, and coordinate local (non-Federal) expertise and capacity relevant to public health emergency response (public, private, commercial and non-profit, including academic).\",\n\"Integration of federal\/state\/local public health surveillance systems.\",\n\"Value of investments in baseline public health response infrastructure preparedness\",\n\"Modes of communicating with target high-risk populations (elderly, health care workers).\",\n\"Risk communication and guidelines that are easy to understand and follow (include targeting at risk populations\u2019 families too).\",\n\"Communication that indicates potential risk of disease to all population groups.\",\n\"Misunderstanding around containment and mitigation.\",\n\"Action plan to mitigate gaps and problems of inequity in the Nation\u2019s public health capability, capacity, and funding to ensure all citizens in need are supported and can access information, surveillance, and treatment.\",\n\"Measures to reach marginalized and disadvantaged populations.\",\n\"Data systems and research priorities and agendas incorporate attention to the needs and circumstances of disadvantaged populations and underrepresented minorities.\",\n\"Mitigating threats to incarcerated people from COVID-19, assuring access to information, prevention, diagnosis, and treatment.\",\n\"Understanding coverage policies (barriers and opportunities) related to testing, treatment, and care\"]","0790597c":"html_string = perform_task(task_9_search_query, task9, 50)","793971b2":"display(HTML(html_string))","90ffb7ea":"bert_qa = BertQA()","a29d9858":"def render_search_results(query, num=10):\n    \n    results = search_engine.search_articles(query, num)\n    html_string = f'<h2>Query: {query}<\/h2><\/br><\/br>'\n    if len(results) == 0:\n        html_string='There is no match. '\n        \n    for i, result in enumerate(results):\n        title = result['title']\n        abstract = result['abstract']\n        full_text = result['full_text']\n        abstract_highlight = result['abstract_highlight']\n        full_text_highlight = result['full_text_highlight']\n        reference_highlight = result['reference_highlight']\n        html_string += f'<h2>{i+1}. {title}<\/h2><\/br>'\n\n        if abstract_highlight != '':\n            html_string += f'<h3>Abstract evidence<\/h3>'\n            for i, sent in enumerate(abstract_highlight.split(\"...\")):\n                html_string += f'{i+1}. ...{sent}...<\/br>'\n            bert_answer = bert_qa.answer_question(query, abstract)\n            if (bert_answer.lower() not in query.lower()) and (bert_answer != ''):\n                html_string += f'<\/br>Bert answer: {bert_answer}<\/br>'\n            html_string += f'<\/br>'\n\n        if full_text_highlight != '':\n            html_string += f'<h3>Full Text evidence<\/h3>'\n            for i, sent in enumerate(full_text_highlight.split(\"...\")):\n                html_string += f'{i+1}. ...{sent}...<\/br>'\n            bert_answer = bert_qa.answer_question(query, full_text)\n            if (bert_answer.lower() not in query.lower()) and (bert_answer != ''):\n                html_string += f'<\/br>Bert answer: {bert_answer}<\/br>'\n            html_string += f'<\/br>'\n        \n        \n        \n    display(HTML(html_string))","02af4cdc":"text = widgets.Text(\n    value='Seasonality of transmission.',\n    placeholder='Paste ticket description here!',\n    disabled=False,\n    layout=widgets.Layout(width='60%', overflow_y='auto')\n)\noutput = widgets.Output()\nhbox = widgets.HBox([widgets.Label('Search articles: '), text])\nvbox = widgets.VBox([hbox, output])\n\ndisplay(vbox)\n\ndef callback(wdgt):\n    with output:\n        output.clear_output()\n        render_search_results(wdgt.value)\n\ntext.on_submit(callback)","c50870d9":"# What do we know about non-pharmaceutical interventions?","e149e35b":"## Discussion","cb603162":"# What do we know about COVID-19 risk factors?","e9f35d93":"# What has been published about medical care?","7efd1aeb":"# What do we know about virus genetics, origin, and evolution?","f96be911":"# What do we know about diagnostics and surveillance?","43d8fffd":"# Interactive search\nYou could search any keywords. The articles will be retrieved from the index and the pretrained BertQuestionAnswering model will be used to find the answer. BertQuestionAnswering doesn't always find the answer, the question needs to be phrased in simple questions e.g. Is hypertension a risk factor for COVID-19?","9068f7b8":"# What do we know about vaccines and therapeutics?","68e940b0":"# Install python libraries","bb43503b":"# What is known about transmission, incubation, and environmental stability?","4f718d0f":"# The summary of the approach\n\nThis notebook is created as an attempt to answer questions in all 10 tasks except for **Sample task with sample submission** because the main task is phrased too broadly and may not fit our approach. Our approach combines the classic search algorithm BM25 with the state of the art deep learning model BERT for NLP. The strategy can be summarized in the following sections. \n\n### Retrieve relevant articles for COVID-19 (BM25) \nWe use broad terms defined in the main task question as the query to retrieve relevant articles in a BM25 search engine. For example, the main question for task 1 is **What is known about transmission, incubation, and environmental stability?**. We extract 3 terms MANUALLY including **transmission**, **incubation** and **environment stability**. We use a library called whoosh as the indexing engine to enable fast search in title, abstract, and full_text across all documents. https:\/\/whoosh.readthedocs.io\/en\/latest\/index.html. \n\n### Define COVID-19 key words\nTo find COVID-19 related articles, we have defined a list of key words, the article is considered COVID-19 related if, any of these fields (title, abstract and full text) has any of the key word mentions. To make sure we have included all the key words for COVID-19, we trained a word2vec model on all full texts for phrase embeddings, then we tried to find all synonyms for COVID-19 from the word2vec model. We used an iterative approach, where we start looking for synonyms with one key word and add new phrases or words to the key word list, then use the newly found key word to repeat the same process until there is no new key word found anymore. Here is the list of synonyms fr COVID-19.\n\n['ncov',\n 'covid19',\n 'covid-19',\n 'sars cov2',\n 'sars cov-2',\n 'sars-cov-2',\n 'sars coronavirus 2',\n '2019-ncov',\n '2019 novel coronavirus',\n '2019-ncov sars',\n 'cov-2',\n 'cov2',\n 'novel coronvirus',\n 'coronavirus 2019-ncov']\n\n### Assign documents to subtasks\nWe retrieve the top 50 articles using the broad terms extracted from the main task. Then for each article, we take the following steps. \n1. we split the full text into sentences, then use a pre-trained deep learning model called sentence-transformers to encode sentences into embeddings (vectors).\n2. we apply sentence-transformers to all subtasks and turn them into a set of sentence embeddings as well.\n3. we compute the sentence-subtask pairwise cosine similarities based on their sentence embeddings.\n4. we then sum up the cosine similarities for each subtask. These summed up cosine similarity scores indicate relatedness between each subtask and this article. \n5. we normalize the cosine similarities by applying softmax. The output of it could be interpreted as how strong each subtask is related to the article, which will be referred to as confidence in the rest of the notebook.\n6. we print out the articles in the order generated by the BM25 search algorithm. In addition, we print out the confidence associated with each subtask and the top matching sentences except the subtasks with a weak confidence defined as conf < 0.5. \n\n**What is Bert and what is sentence-transformers?** \nBert is the state of the art deep learning language model for NLP tasks, it uses a self-attention mechanism to create context-dependent word embeddings, and sentence transformers leverage these context-aware embeddings to create sentence embeddings https:\/\/github.com\/UKPLab\/sentence-transformers. \n\n### Interactive mode\nIn addition to the search strategy, we introduced an interactive mode to enable users to do the free text search and incorporated the Bert Question-Answer model to pinpoint the answer (sentence(s)) given a subtask question. \n\n\n## Discussion\nWe did notice that sentence-transformers did not work for sentences especially given a very short sentence or an incomplete sentence such as subtask **Seasonality of Transmission** in Task 1. sentence-transformers couldn't encode such sentences into a useful sentence embeddings due to the limited context in the short sentence, as a consequence the cosine similarities computed for these tend to be low, and the matching sentences for **Seasonality of Transmission** are not very useful. Therefore, we proposed a complementary approach **Interactive mode** to solve such cases. \n\n## Future work\nCurrently, we use straight-up search for finding relevant articles and haven't implemented the query expansion in this notebook yet. We do plan to incorporate it if we get to the next round of the submission. One of the ways for finding related terms or synonyms is to use word\/phrase embeddings, which could be easily done by training a word2vec model on all abstracts or full texts extracted from the dataset. \n\nCurrently, the queries are defined manually by extracting the main concepts from the tasks, we want to use NLP tools such spaCy to automatically extract the important concepts (primarily noun chunks) from the tasks. \n\nWe would like to explore the citation connections between articles. The hypothesis behind it is that similar articles tend to reference the same set of articles, by exploring those relationships, we could generate the article embeddings and cluster similar articles together. \n\n\n## Acknowledgement\nThis notebook is inspired by some other people's work. \n\nhttps:\/\/www.kaggle.com\/jonathanbesomi\/a-qa-model-to-answer-them-all\n\nhttps:\/\/www.kaggle.com\/danielwolffram\/topic-modeling-finding-related-articles\n\nhttps:\/\/www.kaggle.com\/davidmezzetti\/cord-19-analysis-with-sentence-embeddings\n","54412b6c":"# What has been published about information sharing and inter-sectoral collaboration?","3796ab68":"# What has been published about ethical and social science considerations?"}}