{"cell_type":{"1ca4045d":"code","67a68a3a":"code","2b796fa4":"code","203f08bd":"code","467ffd0c":"code","08ca16cd":"code","43e3be05":"code","cdf21a80":"code","6038ca6f":"code","37c0c8cc":"code","834fbfe5":"code","9a501736":"code","56965f64":"code","55020595":"code","0880759a":"code","e7a719da":"code","ec45fef8":"code","01e78524":"code","ef90bf82":"code","d350ce5c":"code","fbcdf6aa":"code","d4a0432c":"code","e9b4e7dc":"code","48af2770":"code","6b9814d2":"code","c5c1a5e5":"code","e72b07c0":"code","079f337f":"code","101d8632":"code","bbb49866":"code","2ffc7a0a":"code","eecfede5":"code","cd70aec7":"code","bcad866f":"code","c06189ef":"code","958d66bb":"code","27e0cf3d":"code","ab3c9edb":"code","1f051010":"code","a78aa016":"code","8552fda2":"code","ff9e7e4c":"code","a6383855":"code","f3055826":"code","b1652c42":"code","58a7b6e4":"code","847d6445":"code","4b4f3af1":"code","24336345":"code","f3a975e4":"code","17d77e70":"code","ca03a502":"code","afc084c8":"code","2ce0ce98":"code","debfadd5":"code","c852c6c2":"code","7bd0edc6":"code","70f19404":"code","46361b96":"code","2f5d9567":"code","126526ea":"code","47fff4f3":"code","80fcf76b":"code","324613e8":"code","0d053638":"code","cbfa1021":"code","7a2ba0a1":"code","05353ed3":"code","590fc288":"code","fc37f765":"markdown","3511dece":"markdown","1cf7d56a":"markdown","e4c8c939":"markdown","2a9e60de":"markdown","cd1c065d":"markdown","59f6285f":"markdown","e88ef36e":"markdown","3ac60332":"markdown","6cbb873b":"markdown","e38809e4":"markdown","952c8d91":"markdown","d87e4bf0":"markdown","abcc2227":"markdown","b01d97ea":"markdown","7a92a2c8":"markdown","1d11e309":"markdown","c37d4605":"markdown","b07c489c":"markdown","3c900506":"markdown","77c08b2e":"markdown","ea1bd860":"markdown","f7e56572":"markdown","acefd5c4":"markdown","e007f539":"markdown","dbd199eb":"markdown","16557a47":"markdown","19fee55b":"markdown","913e7f99":"markdown","119d5ebb":"markdown","f947f843":"markdown","aaa13aa1":"markdown","f659d4d9":"markdown","f062f4c7":"markdown","70609047":"markdown","df70c92a":"markdown","8fde5737":"markdown","ceaa11c5":"markdown","aba79ffe":"markdown","77e7d977":"markdown","fb94b144":"markdown","9db71d6f":"markdown","d07383f9":"markdown","8e53dee3":"markdown","e381fd74":"markdown","963cf269":"markdown","d82c7f26":"markdown","9c71e698":"markdown","40afc23b":"markdown","be5693e8":"markdown","8911fdc0":"markdown","e5852159":"markdown"},"source":{"1ca4045d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","67a68a3a":"# Numerical libraries\nimport numpy as np   \n\n# Import Linear Regression machine learning library\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.metrics import r2_score\n\n# to handle data in form of rows and columns \nimport pandas as pd    \n\n# importing ploting libraries\nimport matplotlib.pyplot as plt   \n\nimport statsmodels.formula.api as sm\n\n#importing seaborn for statistical plots\nimport seaborn as sns\n\nimport datetime\nimport time\nfrom time import strftime, gmtime\n\nimport statsmodels.formula.api as smf\n#maschine learning libraries\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix \n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.svm import SVC\nfrom random import sample","2b796fa4":"df_flights=pd.read_csv(\"\/kaggle\/input\/flight-delays\/flights.csv\")","203f08bd":"df_flights.head()","467ffd0c":"df_flights.info()","08ca16cd":"# converting input time value to datetime.\ndef conv_time(time_val):\n    if pd.isnull(time_val):\n        return np.nan\n    else:\n            # replace 24:00 o'clock with 00:00 o'clock:\n        if time_val == 2400: time_val = 0\n            # creating a 4 digit value out of input value:\n        time_val = \"{0:04d}\".format(int(time_val))\n            # creating a time datatype out of input value: \n        time_formatted = datetime.time(int(time_val[0:2]), int(time_val[2:4]))\n    return time_formatted","43e3be05":"df_flights['ARRIVAL_TIME'] = df_flights['ARRIVAL_TIME'].apply(conv_time)\ndf_flights['DEPARTURE_TIME'] = df_flights['DEPARTURE_TIME'].apply(conv_time)\ndf_flights['SCHEDULED_DEPARTURE'] = df_flights['SCHEDULED_DEPARTURE'].apply(conv_time)\ndf_flights['WHEELS_OFF'] = df_flights['WHEELS_OFF'].apply(conv_time)\ndf_flights['WHEELS_ON'] = df_flights['WHEELS_ON'].apply(conv_time)\ndf_flights['SCHEDULED_ARRIVAL'] = df_flights['SCHEDULED_ARRIVAL'].apply(conv_time)","cdf21a80":"df_flights.isnull().sum()","6038ca6f":"df_flights['AIRLINE_DELAY'] = df_flights['AIRLINE_DELAY'].fillna(0)\ndf_flights['AIR_SYSTEM_DELAY'] = df_flights['AIR_SYSTEM_DELAY'].fillna(0)\ndf_flights['SECURITY_DELAY'] = df_flights['SECURITY_DELAY'].fillna(0)\ndf_flights['LATE_AIRCRAFT_DELAY'] = df_flights['LATE_AIRCRAFT_DELAY'].fillna(0)\ndf_flights['WEATHER_DELAY'] = df_flights['WEATHER_DELAY'].fillna(0)","37c0c8cc":"df_flights.isnull().sum()","834fbfe5":"# group by CANCELLATION_REASON to see the ration\ndf_flights['CANCELLATION_REASON'].value_counts()","9a501736":"# -------------------------------------\n# converting categoric value to numeric\ndf_flights.loc[df_flights['CANCELLATION_REASON'] == 'A', 'CANCELLATION_REASON'] = 1\ndf_flights.loc[df_flights['CANCELLATION_REASON'] == 'B', 'CANCELLATION_REASON'] = 2\ndf_flights.loc[df_flights['CANCELLATION_REASON'] == 'C', 'CANCELLATION_REASON'] = 3\ndf_flights.loc[df_flights['CANCELLATION_REASON'] == 'D', 'CANCELLATION_REASON'] = 4\n\n# -----------------------------------\n# converting NaN data to numeric zero\ndf_flights['CANCELLATION_REASON'] = df_flights['CANCELLATION_REASON'].fillna(0)","56965f64":"df_flights.isnull().sum()","55020595":"# drop the last 1% of missing data rows.\ndf_flights = df_flights.dropna(axis=0)","0880759a":"df_flights.isnull().sum()","e7a719da":"df_airlines = pd.read_csv('\/kaggle\/input\/flightdelay\/airlines.csv')\ndf_airlines","ec45fef8":"# joining airlines\ndf_flights = df_flights.merge(df_airlines, left_on='AIRLINE', right_on='IATA_CODE', how='inner')","01e78524":"# dropping old column and rename new one\ndf_flights = df_flights.drop(['AIRLINE_x','IATA_CODE'], axis=1)\ndf_flights = df_flights.rename(columns={\"AIRLINE_y\":\"AIRLINE\"})","ef90bf82":"fig_dim = (14,18)\nf, ax = plt.subplots(figsize=fig_dim)\nquality=df_flights[\"AIRLINE\"].unique()\nsize=df_flights[\"AIRLINE\"].value_counts()\n\nplt.pie(size,labels=quality,autopct='%1.0f%%')\nplt.show()","d350ce5c":"sns.set(style=\"whitegrid\")\n\n# initialize the figure\nfig_dim = (10,12)\nf, ax = plt.subplots(figsize=fig_dim)\nsns.despine(bottom=True, left=True)\n\n# Show each observation with a scatterplot\nsns.stripplot(x=\"ARRIVAL_DELAY\", y=\"AIRLINE\",\n              data=df_flights, dodge=True, jitter=True\n            )\nplt.show()","fbcdf6aa":"# Group by airline and sum up \/ count the values\ndf_flights_grouped_sum = df_flights.groupby('AIRLINE', as_index= False)['ARRIVAL_DELAY'].agg('sum').rename(columns={\"ARRIVAL_DELAY\":\"ARRIVAL_DELAY_SUM\"})\ndf_flights_grouped_cnt = df_flights.groupby('AIRLINE', as_index= False)['ARRIVAL_DELAY'].agg('count').rename(columns={\"ARRIVAL_DELAY\":\"ARRIVAL_DELAY_CNT\"})\n\n# Merge the two groups together\ndf_flights_grouped_delay = df_flights_grouped_sum.merge(df_flights_grouped_cnt, left_on='AIRLINE', right_on='AIRLINE', how='inner')\n# Calculate the average delay per airline\ndf_flights_grouped_delay.loc[:,'AVG_DELAY_AIRLINE'] = df_flights_grouped_delay['ARRIVAL_DELAY_SUM'] \/ df_flights_grouped_delay['ARRIVAL_DELAY_CNT']\n\ndf_flights_grouped_delay.sort_values('ARRIVAL_DELAY_SUM', ascending=False)","d4a0432c":"# Dataframe correlation\ndel_corr = df_flights.corr()\n\n# Draw the figure\nf, ax = plt.subplots(figsize=(14, 12))\n\n# Draw the heatmap\nsns.heatmap(del_corr,annot=True,cmap='inferno')\nplt.show()","e9b4e7dc":"# Marking the delayed flights\ndf_flights['DELAYED'] = df_flights.loc[:,'ARRIVAL_DELAY'].values > 0","48af2770":"figsize=plt.subplots(figsize=(10,12))\nsns.countplot(x='DELAYED',hue='AIRLINE',data=df_flights)\nplt.show()","6b9814d2":"# Label definition\ny = df_flights.DELAYED\n\n# Choosing the predictors\nfeature_list_s = [\n    'LATE_AIRCRAFT_DELAY'\n    ,'AIRLINE_DELAY'\n    ,'AIR_SYSTEM_DELAY'\n    ,'WEATHER_DELAY'\n    ,'ELAPSED_TIME']\n\n# New dataframe based on a small feature list\nX_small = df_flights[feature_list_s]","c5c1a5e5":"# RandomForestClassifier with 10 trees and fitted on the small feature set \nclf = RandomForestClassifier(n_estimators = 10, random_state=32) \nclf.fit(X_small, y)\n","e72b07c0":"importances=clf.feature_importances_\nimportances=pd.DataFrame([X_small.columns,importances]).transpose()\nimportances.columns=[['Variables','Importance']]\nimportances","079f337f":"# choosing the predictors\nfeature_list = [\n    'YEAR'\n    ,'MONTH'\n    ,'DAY'\n    ,'LATE_AIRCRAFT_DELAY'\n    ,'AIRLINE_DELAY'\n    ,'AIR_SYSTEM_DELAY'\n    ,'WEATHER_DELAY'\n    ,'ELAPSED_TIME'\n    ,'DEPARTURE_DELAY'\n    ,'SCHEDULED_TIME'\n    ,'AIR_TIME'\n    ,'DISTANCE'\n    ,'TAXI_IN'\n    ,'TAXI_OUT'\n    ,'DAY_OF_WEEK'\n    ,'SECURITY_DELAY'\n]\n# Any number can be used in place of '0'. \nimport random\nrandom.seed(0)\n    \ndf_flights_1=df_flights.sample(n=50000)\nX = df_flights_1[feature_list]\n","101d8632":"X.info()","bbb49866":"y = df_flights_1.DELAYED","2ffc7a0a":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\nfrom sklearn.preprocessing import scale\nX_train=scale(X_train)\nX_test=scale(X_test)","eecfede5":"model=LinearRegression()\nmodel=model.fit(X_train,y_train)\nslope=model.coef_\ncoef=model.intercept_\nprint(slope.flatten())\nprint(coef)","cd70aec7":"y_pred=model.predict(X_train)","bcad866f":"r2_score(y_train,y_pred)","c06189ef":"from mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nfrom sklearn.linear_model import LinearRegression","958d66bb":"lr = LinearRegression()\nsfs = SFS(lr, k_features='best', forward=True, floating=False, \n          scoring='neg_mean_squared_error', cv=10)\nmodel = sfs.fit(X_train, y_train)\n\nfig = plot_sfs(sfs.get_metric_dict(), kind='std_err')\n\nplt.title('Sequential Forward Selection (w. StdErr)')\nplt.grid()\nplt.show()","27e0cf3d":"print('Selected features:', sfs.k_feature_idx_)","ab3c9edb":"lr = LinearRegression()\nsfs2 = SFS(lr, k_features='best', forward=False, floating=False, \n          scoring='neg_mean_squared_error', cv=10)\nmodel = sfs2.fit(X_train, y_train)\n\nfig = plot_sfs(sfs2.get_metric_dict(), kind='std_err')\n\nplt.title('Backward Selection (w. StdErr)')\nplt.grid()\nplt.show()","1f051010":"print('Selected features:', sfs2.k_feature_idx_)","a78aa016":"from sklearn import ensemble,gaussian_process,linear_model,naive_bayes,neighbors,svm,tree","8552fda2":"MLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostRegressor(),\n    ensemble.BaggingRegressor(),\n    ensemble.ExtraTreesRegressor(),\n    ensemble.GradientBoostingRegressor(),\n    ensemble.RandomForestRegressor(),\n    #Nearest Neighbor\n    neighbors.KNeighborsRegressor(),\n    #Trees    \n    tree.DecisionTreeRegressor(),\n    tree.ExtraTreeRegressor()\n    ]","ff9e7e4c":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_curve, roc_auc_score,precision_score,recall_score,auc","a6383855":"MLA_columns = []\nMLA_compare = pd.DataFrame(columns = MLA_columns)\nresults=[]\n\nrow_index = 0\nfor alg in MLA:\n    \n    cv_results = cross_val_score(alg, X_train, y_train, cv=10)\n    results.append(cv_results)\n    predicted = alg.fit(X_train, y_train).predict(X_test)\n    fp, tp, th = roc_curve(y_test, predicted)\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index,'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Train Accuracy'] = round(alg.score(X_train, y_train), 4)\n    MLA_compare.loc[row_index, 'MLA Test Accuracy'] = round(alg.score(X_test, y_test), 4)\n    MLA_compare.loc[row_index, 'MLA AUC'] = auc(fp, tp)\n    \n    \n    row_index+=1\n    \nMLA_compare.sort_values(by = ['MLA Test Accuracy'], ascending = False, inplace = True)    \nMLA_compare","f3055826":"plt.subplots(figsize=(15,6))\nsns.lineplot(x=\"MLA Name\", y=\"MLA Train Accuracy\",data=MLA_compare,palette='hot',label='Train Accuracy')\nsns.lineplot(x=\"MLA Name\", y=\"MLA Test Accuracy\",data=MLA_compare,palette='hot',label='Test Accuracy')\nplt.xticks(rotation=90)\nplt.title('MLA Accuracy Comparison')\nplt.legend()\nplt.show()","b1652c42":"plt.subplots(figsize=(15,6))\nsns.lineplot(x=\"MLA Name\", y=\"MLA AUC\",data=MLA_compare,palette='hot',label='Accuracy')\n\nplt.xticks(rotation=90)\nplt.title('MLA Accuracy Comparison')\nplt.legend()\nplt.show()","58a7b6e4":"#sns.boxplot(MLA_compare[\"MLA AUC\"])# boxplot algorithm comparison\nfig = plt.figure(figsize=(10,10))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results,labels=MLA_compare['MLA Name'])\nplt.xticks(rotation=45)\nplt.show()","847d6445":"# RandomForest with 100 trees\nforest_model = RandomForestRegressor(n_estimators = 100, random_state=42)","4b4f3af1":"y = df_flights_1.ARRIVAL_DELAY\ny = np.array(y)","24336345":"X = np.array(X)","f3a975e4":"# split data into training and validation data, for both predictors and target\n# The split is based on a random number generator. Supplying a numeric value to\n# the random_state argument guarantees we get the same split every time we\n# run this script.\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, test_size = 0.30, random_state = 42)","17d77e70":"#The Shape of Train- and Testdata\nprint('Training Features Shape:', train_X.shape)\nprint('Training Labels Shape:', train_y.shape)\nprint('Testing Features Shape:', val_X.shape)\nprint('Testing Labels Shape:', val_y.shape)","ca03a502":"# Average arrival delay for our dataset\nbaseline_preds = df_flights['ARRIVAL_DELAY'].agg('sum') \/ df_flights['ARRIVAL_DELAY'].agg('count') \n\n# Baseline error by average arrival delay \nbaseline_errors = abs(baseline_preds - val_y)\nprint('Average baseline error: ', round(np.mean(baseline_errors),2))","afc084c8":"# Fit the model\nforest_model.fit(train_X, train_y)","2ce0ce98":"# Predict the target based on testdata \nflightdelay_pred= forest_model.predict(val_X)","debfadd5":"#Calculate the absolute errors\nerrors_random1 = abs(flightdelay_pred - val_y)","c852c6c2":"print('Mean Absolute Error: ', round(np.mean(errors_random1),3), 'minutes.')","7bd0edc6":"X=pd.DataFrame(X)","70f19404":"importances=forest_model.feature_importances_\nimportances=pd.DataFrame([X.columns,importances]).transpose()\nimportances.columns=[['Variables','Importance']]\nimportances","46361b96":"# Count of DEPARTURE_DELAYs that are not zero and could influence our prediction.\nprint(\"DEPARTURE_DELAY count: \")\nprint(df_flights_1[df_flights_1['DEPARTURE_DELAY'] != 0]['DEPARTURE_DELAY'].count())\nprint(\"-------------------------------\")\nprint(\"All datarow count:\")\nprint((df_flights_1)['DEPARTURE_DELAY'].count())\nprint(\"-------------------------------\")\nprint(\"-------------------------------\")\nprint(\"Percentag of DEPARTURE_DELAY that is not zero:\")\nprint(df_flights_1[df_flights_1['DEPARTURE_DELAY'] != 0]['DEPARTURE_DELAY'].count() \/ df_flights_1['DEPARTURE_DELAY'].count())","2f5d9567":"print(\"----------------- TRAINING ------------------------\")\nprint(\"r-squared score: \",forest_model.score(train_X, train_y))\nprint(\"------------------- TEST --------------------------\")\nprint(\"r-squared score: \", forest_model.score(val_X, val_y))","126526ea":"random.seed(1)\ndf_flights__2=df_flights.sample(n=50000)\nX2 = df_flights__2[feature_list]\ny2 = df_flights__2.ARRIVAL_DELAY","47fff4f3":"# Predict the new data based on the old model (forest_model)\nflightdelay_pred_ = forest_model.predict(X2)\n\n#Calculate the absolute errors\nerrors_random_2 = abs(flightdelay_pred_ - y2)\n","80fcf76b":"# Mean Absolute Error im comparison\nprint('Mean Absolute Error Random Sample 1: ', round(np.mean(errors_random1),3), 'minutes.')\nprint('---------------------------------------------------------------')\nprint('Mean Absolute Error Random Sample 1: ', round(np.mean(errors_random_2),3), 'minutes.')","324613e8":"print(\"r-squared score Random Sample 1: \",forest_model.score(val_X, val_y))\nprint(\"------------------- TEST --------------------------\")\nprint(\"r-squared score Random Sample 2: \", forest_model.score(X2, y2))","0d053638":"# Searching for a flight that fits our needs\na=df_flights__2[(df_flights__2.loc[:,'DEPARTURE_DELAY'] < 0) & (df_flights__2.loc[:,'ARRIVAL_DELAY'] > 60)].head(10)\n","cbfa1021":"a","7a2ba0a1":"# Look into the flight with Arrival Delay but no Departure Delay\na.iloc[1]","05353ed3":"# Retrieving the flight with index 3221210 (delayed flight without departure delay).\nX3 = a.loc[:,feature_list]\nX3 = X3.iloc[0]\n# Setting the target for our flight index 3221210\ny3 =a.iloc[0]['ARRIVAL_DELAY']\nprint(y3)\nX3\n","590fc288":"# Printing the important stuff\nflight_pred_s = forest_model.predict([X3])\nprint(\"Predicted Delay of the Flight (Minutes): \", flight_pred_s)\nprint(\"-------------------------------------------------\")\nprint(\"Original Delay of the Flight (Minutes):  \", y3)\nprint(\"_________________________________________________\")\nprint(\"_________________________________________________\")\nprint(\"Difference (Minutes)                   : \",  y3-flight_pred_s)","fc37f765":"As I already mentioned, the mean absolute error, as well as the r-squared equation both look that the model would not fit that well, because they seem too accurate. The model is highly based on the DEPARTURE_DELAY feature and makes its decisions by that. If there is a flight that has been delayed but not according to the DEPARTURE_DELAY, the model would probably don't give a prediction that has that accuracy.\n\nI will test this in the following.","3511dece":"    **Model Training and Prediction**\nEstablish Baseline","1cf7d56a":"    **Conclusion:**\nThe gap between the predicted and the original delay is 0.26 minutes. Here we can see how the model behavior changes according to the missing main feature impact (the DEPARTURE_DELAY). The original delay is much lower than the mean absolute error of 2.118 minutes from the previous calculations. The conjecture about the risk of one high rated feature has confirmed. Nevertheless, this difference is in a range that has not be bad at all. It seems this model has a good accuracy to predict the flight delay.\n\nSome kind of pruning for the DEPARTURE_DELAY would definitely improve the model more. I will keep that in mind for a later version of this model, this notebook, right now I'm happy with the result of the model and will leave it as it is.","e4c8c939":"These delayed flights seems to be a good one. It has the following properties:","2a9e60de":"Stepwise Regression  is where we select the features to be used in a model on the basis of variable importance.\n\nVariables are sequentially selected and a model is built starting with one variable. In the next model another variable is added and the adjusted R2 for both the models are compared. If the adjusted R2 increases the next variable is added. This process is repeated till there is a decrease in the adjusted R2 The above process is known as forward selection.\n\nFor this purpose we can use the mlextend library","cd1c065d":"Nearly 88% of the feature importance is based by the DEPARTURE_DELAY feature which is a lot. The next eight features not even have individually more than 10% of importance, they are all lower. AIR_SYSTEM_DELAY, SCHEDULED_TIME and ELAPSED_TIME have at least values that are greater than 1.0%. The remaining features all having an importance that is lower 1.0%.\n\nI will take a closer look into the single features in the next section.","59f6285f":"    **Choosing the Prediction Target**\nThis time I choose the ARRIVAL_DELAY as the target and change the model to the Random Forest Regressor (seen above) to predict the exact minutes delayed or arrived in time.","e88ef36e":"    **Model Check without DEPARTURE_DELAY Impact**\nFor this test, I will search for a special flight that is not delayed by the DEPARTURE_DELAY and is at least a delayed flight of 60 minutes (ARRIVAL_DELAY > 60).","3ac60332":"We can see almost all the features are importants, as they are interlinked.\n\nWe then applied the train on all the models to check which model is giving us the best accuracy.","6cbb873b":"The required data has now the correct format.","e38809e4":"**Predict and Validate the Result**","952c8d91":"**Analyzing the proportion of flights with respect to the companies.**","d87e4bf0":"The reason for cancellation of flights splits into the following occurrences:\n\n    A - Airline\/Carrier\n    B - Weather\n    C - National Air System\n    D - Security\n    ... and has the following ratio:","abcc2227":"The distribution above shows the airlines in comparison to their ARRIVAL_DELAYs. It clearly shows that American Airlines has a wide spread of delays. By contrast, the airline with the most entries is Southwest Airlines and their delays look pretty low compared to the American Airlines delays. I will elaborate on this in the following:","b01d97ea":"Similarly backward elimination can also be performed on the same data","7a92a2c8":"This here seems to be as well pretty accurate. The training dataset is a known dataset by the model why the test dataset is used as well here. As we know due to the previous analysis, the model is highly based on the DEPARTURE_DELAY feature. All the model's decision is based on what the DEPARTURE_DELAY does, which afterward leads to that accuracy.\n\nI will test the model with another new dataset and calculate the necessary key figures.","1d11e309":"The Above plot shows the test train accuracy with respect to each models.","c37d4605":"We can see that Bagging Regressor and Random Forest Regressor models are giving the accuracy. So we choose Random Forest Model to predict the tune is more.","b07c489c":"    **The Coefficient of Determination - The Model Fitness**\nIn this chapter, I will calculate the coefficient of determination or \"R-squared\" for the model. It will show how good the inputs fit the output of the model, or how good the model represents the underlying data. That means if the regressions of our features have an R-squared close to 1, it means that the independent variables (the features) are well-suited to predict the dependent variable (our target, the ARRIVAL_DELAY).\n\nI will now calculate the R-squared for the built model based on the training and test dataset:","3c900506":"    **Choosing the Predictors**\nTo predict our prediction target (ARRIVAL_DELAY), we need some features. I will select the same features as in the chapter before.","77c08b2e":"    **Data Analysis and Preprocessing**\nThe following is the first overview of all attributes:","ea1bd860":"    **Feature Correlation**\nSo let us look at the correlation between each of the features ( and the label as well). This might be the first step into a closer feature selection. The main goal is to identify the features that affect the ARRIVAL_DELAY in a positive or negative way.","f7e56572":"This is our average baseline error of 21.39 minutes of delays we want to beat with our regression model.","acefd5c4":"**Return the Absolute Error**","e007f539":"The Above plot shows the Accuracy score of each model, where clearly Random Forest and Bagging Models are giving the best accuracy.","dbd199eb":"There is a need to convert them all to datetime. In addition, it seems to be helpful to write\/use a function for this conversion. (Thanks to fabiendaniel and her great tutorial here ):","16557a47":"Nearly 94% of the values from DEPARTURE_DELAY are set with a value that is not zero. The nearly 100% fulfillment and the effects from the DEPARTURE_DELAY on the ARRIVAL_DELAY leads to that feature importance for the built model. So it seems to be not unusual to have such an accuracy in that case. Still, this seems too accurate, we are talking here about a minute difference to the real arrival delay of a flight. There is a need to check the accuracy of the model in a much better way.\n\nIn the next chapter, I will analyze the coefficient of determination to get a better overview of how good the model fits the dataset.","19fee55b":"    **Separating into Test and Train Datasets**\nIt is necessary to separate the data into train and test dataset.","913e7f99":"    **Analyzing the Delays by Airline**\nGetting an overview of delays by airlines companies.","119d5ebb":"This looks quite after overfitting. The mean absolute error is pretty small which means the model predicts the arrival delay nearly accurate or over accurate. I will validate and visualize the model in the next chapter.","f947f843":"    **Thank You!**","aaa13aa1":"    **Insights:**\nWe can see that departure delay is the main problem which is creating Delay in the aviation industry. Departure Delay can be caused by many circumstances, that is airline haven't reached the origin,its still on its way from other location,because of some weather delay,crew delay. These delays are basically causing the flight to be on air for more time,thus more fuel is being consumed.feul Consumption negatively effects the revene of the airline company. So in order to increse there revenue, we can think of reducing the delay such that fue consumption is reduced.\n\nAnother important insight is customer satisfaction, people flight boarders generally get irritated if flights are delayed for long hours, so complimentary foods should be given in order no to churn the customers.","f659d4d9":"We clearly see that the DEPARTURE_DELAY is not the reason for the delay this time, moreover the airplane departed early than scheduled. So let's use this flight for the model check. Preparations in the following step:","f062f4c7":"**Analyzing Distribution after Cleansing, Conversion and Preprocessing**\n**Feature and Label Selection**\nFor our prediction, I now need to identify the features that are most likely to impact on the flight delays.\n\nFirst I want to include the airports and try to figure out whether there is an impact on the delay regarding the departure airport or not. For this, I will include the airports from another file in this evaluation. With the included information about the location of the airport, I could identify regions on the map that support a delay.\n\nFirst, I will include the airlines in the evaluation to get a distribution of the delays per airline. Later I will add the airports and their location data to the evaluation to get a closer view of the map and some location-based delays.\n\nMerging the Airline Codes (IATA-Codes) I am going to merge the IATA-Airline codes from the other .csv-file.","70609047":"So it's ok to transform the NAN-data to the value \"0.0\" because there was no impact on the flight by these data that causes a delay.\n\nNull values have now decreased significantly. There are only a few attributes left. Particular striking, however, is the CANCELLATION_REASON that hits the high mark with around 98% null values. We need to take a closer look at the cancellation data.","df70c92a":"    **Reconstruct Data Manually**\nOur null analysis above shows the following features with a lot of null values:\n\n* CANCELLATION_REASON\n* AIR_SYSTEM_DELAY\n* SECURITY_DELAY\n* AIRLINE_DELAY\n* LATE_AIRCRAFT_DELAY\n* WEATHER_DELAY\nIn this case here, I try to determine or \"calculate\" the data by deriving the situation. Look at the values that are mostly empty according to the coherent afford of an airline to not be the reason for a delay. Therefore the missing data (or Not-a-Number data) is not based on a bad data quality, it is more the fact that it didn't happen any action by these delay features. You can prove it by looking at a tuple of one of that features when there is at least one feature triggered, all the other features are \"initialized\" with \"0.0\":","8fde5737":"Good to know how to calculate this values, bad is the fact that the values to calculate these times are also NaN - data. That is probably the reason for its initial NaN - data value. I have no choice but to declare the data as outliers.","ceaa11c5":"A little bit has changed. Now the AIR_SYSTEM__DELAY has got the most influences on a flight that has been delayed. This feature had a less positive correlation in our correlation resume above. All the other features have remained in the same order of importance as we have found out. Let us try a wider range with the same model. And please keep in mind that we use a classification here. We have classified the data into delayed and not delayed data and want to find out now which of these features effects a delay of a flight the most. There could be and there probably will be different features for a flight that arrives just in time, but this will be part of a later section where we try to determine the actual arrival at an airport.\n\nIn following, I want to proof the above written down feature correlation count with a machine learning algorithms. Do they really correlate as good as I think with the ARRIVAL_DELAY? I will classify the data into delayed and not delayed data and define a label (DELAYED) for that in the dataframe. Afterward I will show the feature importance for the given attributes.\n\nIn the beginning, I need to reduce computation time by reducing the data ,so choose a Random Sample of 50000 records. Otherwise, this whole prediction will execute too long.","aba79ffe":"    **Validate and Visualize the Model**\nIn this chapter, I will validate and visualize the prediction model. The previous mentioned mean absolute error of 0.857 minute seems to be a quite good prediction of the arrival delay. The predictions are on average around 0.857 minutes away from the real value. This is a really exact prediction. It is mandatory to check the model whether it is an overfitted one or not.\n\nThe previously shown feature importance of the model looks like this:","77e7d977":"So this is basiccally the Test accuracy result of each models, where we can see Random Forest is giving us the best results,whose data is not varying much as well as it is not having any outliyers like Extra Tree ,Extra Trees and Ada Boost models.\n\nWe can see, Bagging Regressor and Randon Forrest Regressor are the two models,which gives approximately same accuracy and we can see, if we can imporve those accuracy.","fb94b144":"In conclusion, Southwest Airlines has a lot of mostly smaller delays which are in total the high mark of delays in this evaluation. On the other side and with a hint on our distribution chart above, American Airlines has a lot of huge delays in single flights which effects the total delay of the airline. They are in the upper thirds of the delays but their mean delay per airline is one of the lowest of all airlines.","9db71d6f":"      **Handling the Null Values**\nAfter I converted the necessary time values to a DateTime datatype, I need to check our data according to its integrity. Null values or missing data are often occurring data states that need to be handled.\n\nIn addition to several other methods, I will focus on two or three methods in this notebook to deal with null value data or missing data.\n\nOne option is to delete the corresponding rows.\n\nAnother case of handling missing or null value data is to reconstruct the missing data according to information from other columns. Imagine there is a start and an end time and only the duration is missing. You could calculate the missing values simply by the difference between end time and start time. Accordingly, you do not have to delete the data column but you can continue to use the information contained in it.\n\nOne of the best ways to handle missing or null value data is the imputation. The imputation will fill the missing gaps with some numbers that are based on existing data columns. The numbers are not as accurate as the real data but fits the needs for the most prediction models and lead to a better resolution of the model.","d07383f9":"**Train Model**","8e53dee3":"    **Test with Unknown Data Again**\nI will use data from February now, to test the model against total new, unknown data. After all the necessary model preparations I will print out the Mean Absolute Error as well as the r-squared score of the new test data.","e381fd74":"    **Results from Correlation Matrix**\nI am dividing the different correlations into two parts, the positive correlations (higher than 0.6 ) and the less positive correlations (less than 0.6 but higher than 0.2). The results are listed in the list below:\n\n    Positive correlations between:\n        DEPARTURE_DELAY and\n        ARRIVAL_DELAY\n        LATE_AIRCRAFT_DELAY\n        AIRLINE_DELAY\n        ARRIVAL_DELAY and\n        DEPARTURE_DELAY\n        LATE_AIRCRAFT_DELAY\n        AIRLINE_DELAY\n    Less positive correlations between:\n        ARRIVAL_DELAY and\n        AIR_SYSTEM_DELAY\n        WEATHER_DELAY\n        DEPARTURE_DELAY and\n        AIR_SYSTEM_DELAY\n        WEATHER_DELAY\n        TAXI_OUT and\n        AIR_SYSTEM_DELAY\n        ELAPSED_TIME","963cf269":"CSV-File Import","d82c7f26":"    **Feature Selection with Machine Learning Algorithms**","9c71e698":"    **Flight Delay Prediction without DEPARTURE_DELAY**\nNext step will be the prediction and the validation of the result. Therefore I will use the already trained model and give them the information from the special flight above.","40afc23b":"    **Data Prediction**\n    Preparing the Prediction\n    Building the Model First\n    I am building the model first. Here I am choosing 100 trees for the model to not overexert the computation time in later purpose.","be5693e8":"The difference between the two datasets (Random Sample 1 and Random Sample 2 ) is not that big, it's even very small. The model even fits on total new data. What about the R-squared calculation?","8911fdc0":"As you can see the main reason for cancelation is B the weather. It is well known that the weather is often the cause of delays and cancelations. In the case of this attribute, we look at the weather as a cancelation reason, not a delay reason. Now there is the following question: If we want to predict delay times from departure flights, is it necessary to include flight cancellation reasons in our calculation? Don't we want to focus only on not canceled flights, on flights with a departure and a (late) arrival time? The answer is: No, we want them all! We don't want to lose data for our prediction. Every information, in this case, is important. For example cancellation reason \"Weather\" for a canceled flight. The flight themselves did not take place, that's right, but what about the consequences of the canceled flights? All the passengers need to get to their destinations, therefore they will be booked on the next flight or moreover the canceled flight will start in another timeshift and will probably block another's plains flight slot. That all leads to a knock-on effect on other flights.\n\n\"Manuell\" Conversion of categories to numeric values Most models don't work pretty good with categorical values. They need to be converted into numeric values to use a prediction model. There is a way to convert all categorical data into numeric values, its called One-Hot Encoding. This approach will line in all categorical values in separate columns, creates a new column and matches every occurrence of the categorical value with 1 or 0 for non-occurrences. You should take a deeper look into it\n\nAnother approach that is similar to the One-Hot encoding approach is included in the Pandas library and is called get_dummies(). This function converts categorical values into dummy\/indicator values as well. In this case, all null value data are also converted and filled with 0 values. Nevertheless, in this case and according to the small amount of four categorical values, I will convert the CANCELLATION_REASON manually. The final result will be the following:\n\nNaN = 0 A = 1 B = 2 C = 3 D = 4","e5852159":"**Dealing with Null Values in Categorical Data**\n"}}