{"cell_type":{"0b898e4e":"code","e1b1a960":"code","02705968":"code","e123218d":"code","53aa0fa2":"code","3ff1cd3b":"code","b4240cd3":"code","17ad3363":"code","412fcf31":"code","1cee2297":"code","beb19a62":"code","c6a579a5":"markdown","d0f1be6b":"markdown","abc1782c":"markdown"},"source":{"0b898e4e":"import numpy as np # linear algebra\nimport pandas as pd # CSV file I\/O (e.g. pd.read_csv)\nimport os # reading the input files we have access to\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.core import Activation\nfrom keras.layers.core import Flatten\nfrom keras.layers.core import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom keras.layers import Dense\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import explained_variance_score\nimport matplotlib.pyplot as plt\nfrom keras.callbacks import ModelCheckpoint\nimport numpy\n%matplotlib inline\nsize = 10_000_000\npercent = 25\nsizeSplit = int(size * (100 - percent) \/ 100)\nprint('sizeSplit : ' + str(sizeSplit))\nprint(os.listdir('..\/input'))","e1b1a960":"#Red data from csv file for training and validation data\ntrain_df =  pd.read_csv('..\/input\/new-york-city-taxi-fare-prediction\/train.csv', nrows = size)\nprint(train_df.dtypes)\n# Given a dataframe, add two new features 'abs_diff_longitude' and\n# 'abs_diff_latitude' reprensenting the \"Manhattan vector\" from\n# the pickup location to the dropoff location.\ndef add_travel_vector_features(df):\n    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\nadd_travel_vector_features(train_df)\nprint(train_df.isnull().sum())\ntrain_df = train_df.dropna(how = 'any', axis = 'rows')","02705968":"plot = train_df.iloc[:2000].plot.scatter('abs_diff_longitude', 'abs_diff_latitude')\nprint(plot)","e123218d":"train_df = train_df[(train_df.abs_diff_longitude < 5.0) & (train_df.abs_diff_latitude < 5.0)]","53aa0fa2":"plot = train_df.iloc[:2000].plot.scatter('abs_diff_longitude', 'abs_diff_latitude')\nprint(plot)","3ff1cd3b":"def get_input_matrix(df):\n    return np.column_stack((df.abs_diff_longitude, df.abs_diff_latitude, np.ones(len(df))))\ntrain_X = get_input_matrix(train_df)\ntrain_y = np.array(train_df['fare_amount'])\n\nprint(train_X.shape)\nprint(train_y.shape)","b4240cd3":"X1 = train_X[0:sizeSplit]\nX2 = train_X[sizeSplit:size]\nY1 = train_y[0:sizeSplit]\nY2 = train_y[sizeSplit:size]\n#numpy.savetxt(\"testSplit.csv\", X2, delimiter=\",\")\n#numpy.savetxt(\"valTtestSplit.csv\", Y2, delimiter=\",\")","17ad3363":"# Fit the model\nmodel = keras.models.load_model(\"..\/input\/modelsfile\/predictedModel.model\")\n\n# Calculate predictions\nPredTestSet = model.predict(X1)\nPredValSet = model.predict(X2)\n\n# Save predictions\n#numpy.savetxt(\"trainresults.csv\", PredTestSet, delimiter=\",\")\n#numpy.savetxt(\"valresults.csv\", PredValSet, delimiter=\",\")\n","412fcf31":"##### Plot actual vs predition for training set\n#TestResults = numpy.genfromtxt(\"trainresults.csv\", delimiter=\",\")\n#plt.plot(Y1,TestResults,'ro')\nplt.plot(Y1,PredTestSet,'ro')\nplt.title('Training Set')\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\n\n#Compute R-Square value for training set\nTestR2Value = r2_score(Y1,PredTestSet)\nprint(\"Training Set R-Square=\", TestR2Value)\n#Compute explained_variance_score value for training set\nTestR2Value = explained_variance_score(Y1,PredTestSet)\nprint(\"Training Set explained_variance_score=\", TestR2Value)","1cee2297":"#Plot actual vs predition for validation set\n#ValResults = numpy.genfromtxt(\"valresults.csv\", delimiter=\",\")\nplt.plot(Y2,PredValSet,'ro')\nplt.title('Validation Set')\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\n\n#Compute R-Square value for validation set\nValR2Value = r2_score(Y2,PredValSet)\nprint(\"Validation Set R-Square=\",ValR2Value)\n#Compute explained_variance_score value for validation set\nValR2Value = explained_variance_score(Y2,PredValSet)\nprint(\"Validation Set explained_variance_score=\",ValR2Value)","beb19a62":"import decimal\nrealValues = pd.read_csv('..\/input\/new-york-city-taxi-fare-prediction\/sample_submission.csv')\ntest_df = pd.read_csv('..\/input\/new-york-city-taxi-fare-prediction\/test.csv')\nkeys = test_df.key\nadd_travel_vector_features(test_df)\ntest_X = get_input_matrix(test_df)\nPredTestX = model.predict(test_X)\nPredTestX = PredTestX.round(decimals=2)\nPredTestXList = map(lambda x: x[0], PredTestX)\nserPredTest = pd.Series(PredTestXList)\nrealValues.fare_amount.update(serPredTest)\n# Write the predictions to a CSV file which we can submit to the competition.\nsubmission = pd.DataFrame(\n    {'key': keys, 'fare_amount': serPredTest},\n    columns = ['key', 'fare_amount'])\nsubmission.to_csv('sample_submission_prediction.csv', index = False)\n#realValues.to_csv('sample_submission_prediction.csv')\n#numpy.savetxt(\"sample_submission_prediction.csv\", PredTestX, delimiter=\",\")","c6a579a5":"Initial Python environment setup...","d0f1be6b":"### ------------------------------End of Code------------------------------","abc1782c":"**Regression Analysis: How Do I Interpret R-squared and Assess the Goodness-of-Fit?**\n  \nAfter you have fit a linear model using regression analysis, ANOVA, or design of experiments (DOE), you need to determine how well the model fits the data. To help you out, Minitab statistical software presents a variety of goodness-of-fit statistics. In this post, we\u2019ll explore the R-squared (R2 ) statistic, some of its limitations, and uncover some surprises along the way. For instance, low R-squared values are not always bad and high R-squared values are not always good!\n\n**What Is Goodness-of-Fit for a Linear Model?**\nIllustration of regression residuals Definition: Residual = Observed value - Fitted value\nLinear regression calculates an equation that minimizes the distance between the fitted line and all of the data points. Technically, ordinary least squares (OLS) regression minimizes the sum of the squared residuals.\n\nIn general, a model fits the data well if the differences between the observed values and the model's predicted values are small and unbiased.\n\nBefore you look at the statistical measures for goodness-of-fit, you should check the residual plots. Residual plots can reveal unwanted residual patterns that indicate biased results more effectively than numbers. When your residual plots pass muster, you can trust your numerical results and check the goodness-of-fit statistics.\n\n**What Is R-squared?**\nR-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression.\n\nThe definition of R-squared is fairly straight-forward; it is the percentage of the response variable variation that is explained by a linear model. Or:\n\nR-squared = Explained variation \/ Total variation\n\nR-squared is always between 0 and 100%:\n\n0% indicates that the model explains none of the variability of the response data around its mean.\n100% indicates that the model explains all the variability of the response data around its mean.\n\n**In general,**** the higher the R-squared, the better the model fits your data. However, there are important conditions for this guideline that I\u2019ll talk about both in this post and my next post.\n\n**Graphical Representation of R-squared**\n\nPlotting fitted values by observed values graphically illustrates different R-squared values for regression models.\n![image.png](attachment:image.png)\n\nRegression plots of fitted by observed responses to illustrate R-squared\n\nThe regression model on the left accounts for 38.0% of the variance while the one on the right accounts for 87.4%. The more variance that is accounted for by the regression model the closer the data points will fall to the fitted regression line. Theoretically, if a model could explain 100% of the variance, the fitted values would always equal the observed values and, therefore, all the data points would fall on the fitted regression line."}}