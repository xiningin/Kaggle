{"cell_type":{"d4c32b31":"code","3dc59188":"code","e067e1e5":"code","858b9893":"code","78d0f183":"code","bb3801a7":"code","7ca7ce77":"code","efcd235b":"code","2add5553":"code","8a4dec0b":"code","0fb4d92c":"code","6b1cb120":"code","2238d385":"code","1ef4321a":"code","171135f3":"markdown","ed50a8cf":"markdown","10b778dd":"markdown","6cb8624f":"markdown","ac3b297c":"markdown","05ba3142":"markdown","f42bb1c0":"markdown","36e456a4":"markdown","f2bb8677":"markdown","c58862a9":"markdown","49b09420":"markdown","773992a4":"markdown","c5b352a9":"markdown"},"source":{"d4c32b31":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy\nimport seaborn as sns \nimport plotly.express as px \nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.preprocessing import PolynomialFeatures\n","3dc59188":"# lets say that x in this range \nX=np.linspace(1,50,100)\n# the target ( dependant variable ) is defined by the folloeing function \nY=X**2+ 5*X +10\n# set the style for the plot\nplt.style.use(\"seaborn\")\n# lets plot the data \nplt.figure(figsize=(6,6))\nplt.plot(X,Y,c=\"r\",label=\"perfect data\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.title(\"Line plot\",size=15,color=\"r\",pad=10)\nplt.legend()\nplt.show()","e067e1e5":"noise = np.random.random(100) *200\n# add the noise to the original Y\nYfit= Y+noise\n# plot the effect\nplt.figure(figsize=(6,6))\nplt.scatter(X,Yfit,c=\"b\",label=\"Noise data\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.title(\"Line plot\",size=15,color=\"r\",pad=10)\nplt.legend()\nplt.show()","858b9893":"# defining the function of the second degree\ndef poly(x,a,b,c):\n    y= a*(x**2)+b*x+c\n    return y\n# applying to the model \nfrom scipy.optimize import curve_fit\nresult= curve_fit(poly,X,Yfit)\nprint(result)","78d0f183":"def scipy(x):\n    y=result[0][0]*(x**2)+ result[0][1] * x + result[0][2]\n    return y\nplt.figure(figsize=(6,6))\nplt.scatter(X,Yfit,c=\"r\")\nplt.plot(X,scipy(X),c=\"b\",label=\"fit_line\")\nplt.title(\"Fitting using the Scipy\",size=13,pad=10)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.legend()\nplt.show() ","bb3801a7":"from sklearn.linear_model import LinearRegression\nmodel= LinearRegression()\nmodel.fit(X.reshape(-1,1),Yfit.reshape(-1,1))\ncof=model.coef_\nintercept= model.intercept_","7ca7ce77":"cof, intercept","efcd235b":"# tring to fit the curve\ndef linear(x,a,b):\n    return a*x+b\nplt.figure(figsize=(10,10))\nax1= plt.subplot(1,2,1)\nax1.scatter(X,Yfit,c=\"r\")\nax1.plot(X,scipy(X),c=\"b\")\nax1.set_title(\"Futting using the Scipy\",size=13)\nax1.set_xlabel(\"X\")\nax1.set_ylabel(\"Y\")\nax2= plt.subplot(1,2,2)\nax2.scatter(X,Yfit,c=\"r\")\nax2.plot(X.reshape(-1,),linear(X,cof,intercept).reshape(-1,),c=\"b\")\nax2.set_title(\"Futting using the linear_model\",size=13)\nax2.set_xlabel(\"X\")\nax2.set_ylabel(\"Y\")","2add5553":"# applying polynomial preprocessingn to the features to apply higher degree\nfrom sklearn.preprocessing import PolynomialFeatures\npolyf=PolynomialFeatures(2,include_bias=False) # 2 indicated the digree of the polynomial\nnew_x=polyf.fit_transform(X.reshape(-1,1))\nnew_x[:5]\n","8a4dec0b":"poly_model= LinearRegression()\npoly_model.fit(new_x,Yfit.reshape(-1,1))\npoly_coef=poly_model.coef_\npoly_intercept= poly_model.intercept_\npoly_coef,poly_intercept","0fb4d92c":"poly_coef","6b1cb120":"def poly_model(X,coef,intercept):\n    return coef[0][1]*(X**2)+ coef[0][0]* X + intercept","2238d385":"# getting the r2 of the each one\nfrom sklearn.metrics import r2_score, mean_squared_error\nr2_linear_model= r2_score(Yfit.reshape(-1,1),linear(X,cof,intercept).reshape(-1,1))\nRMS_linear_model= np.sqrt(mean_squared_error(Yfit.reshape(-1,1),linear(X,cof,intercept).reshape(-1,1)))\nr2_poly_model= r2_score(Yfit, poly_model(X,poly_coef,poly_intercept))\nRMS_poly_model= np.sqrt(mean_squared_error(Yfit.reshape(-1,1),poly_model(X,poly_coef,poly_intercept)))\nr2_scipy= r2_score(Yfit, scipy(X))\nRMS_scipy= np.sqrt(mean_squared_error(Yfit.reshape(-1,1),scipy(X)))","1ef4321a":"plt.figure(figsize=(15,8))\nax1= plt.subplot(1,3,1)\nax1.scatter(X,Yfit,c=\"r\")\nax1.plot(X,scipy(X),c=\"b\")\nax1.set_title(\"Fitting using the Scipy\",size=13,pad=10)\nax1.set_xlabel(\"X\")\nax1.set_ylabel(\"Y\")\nax1.annotate(f\"R2 : { r2_scipy}\\n\\nRMS : {RMS_scipy}\",xy=(10,2500),color=\"r\",fontsize=12)\nax2= plt.subplot(1,3,2)\nax2.scatter(X,Yfit,c=\"r\")\nax2.plot(X.reshape(-1,),poly_model(X,poly_coef,poly_intercept).reshape(-1,),c=\"b\")\nax2.set_title(\"Fitting using the Poly_model\",size=13,pad=10)\nax2.set_xlabel(\"X\")\nax2.set_ylabel(\"Y\")\nax2.annotate(f\"R2 : { r2_poly_model}\\n\\nRMS : {RMS_poly_model}\",xy=(10,2500),color=\"r\",fontsize=12)\nax3= plt.subplot(1,3,3)\nax3.scatter(X,Yfit,c=\"r\")\nax3.plot(X.reshape(-1,),linear(X,cof,intercept).reshape(-1,),c=\"b\")\nax3.set_title(\"Fitting using the linear_model\",size=13,pad=10)\nax3.set_xlabel(\"X\")\nax3.set_ylabel(\"Y\")\nax3.annotate(f\"R2 : { r2_linear_model} \\n\\nRMS : {RMS_linear_model}\",xy=(10,2500),color=\"r\",fontsize=12)\nplt.tight_layout()","171135f3":"- we only want the first array that return the value of the parameters i defined in the function \n","ed50a8cf":"##### acording to the output, the function can be written like this\n  ####  y = ax^2 +b X + c\n #### which is simillar to the output of the scipy curve_fit method","10b778dd":"### 5.1 - Polynomial features of second degree\n- sklearn.preprocessing.PolynomialFeatures : get all possible combination of the features up to the degree you want \n- you can check the documentation better <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.PolynomialFeatures.html\" > Here <\/a>","6cb8624f":"### note:\n- the fist column -> X\n- the second column -> X^2","ac3b297c":"## 2- inialize the data ","05ba3142":"## 5- Using sklearn","f42bb1c0":"## 4-show the reaults","36e456a4":"## 3-curve fitting using scipy\n### what to do :\n- define the function used to get the parameters\n- use the optimize.curve_fit to get the parameter ","f2bb8677":"## curve fitting of only one feature using \n- curve_fit in (scipy)\n- linearRegrression in sklearn ( linear_model , polynimial_model )","c58862a9":"## 2- adding the noise to the data","49b09420":"### 5.1 - linear model of first degree ","773992a4":"### it is very simple notebook ,but if there is any mistakes or auggestions ... please,write a cooment \n### Thanks ","c5b352a9":"## 1- Importing the libiraries "}}