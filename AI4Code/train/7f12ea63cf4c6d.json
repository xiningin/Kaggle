{"cell_type":{"6ceebcf4":"code","78234046":"code","9357288a":"code","64608e66":"code","99da35eb":"code","45649018":"code","e2cd7826":"code","86b5ad39":"code","d8ce86ae":"code","843b70e0":"code","2da1f089":"code","7c23ee76":"code","ef95a03a":"code","cca8c74d":"code","f824e04f":"code","352aa114":"code","acd30bec":"code","d40170f5":"code","683c59e2":"code","ce42936c":"code","4c6882f5":"code","02ed95dc":"code","0a8c0c09":"code","47989545":"code","72b41b68":"code","b45c0f43":"code","5dbb7fcf":"code","787e3d1a":"code","2f685037":"code","a5a7d38e":"code","b7096ede":"code","9ca485ab":"code","43083bb6":"code","6ef77a48":"code","097646c3":"code","577451ac":"code","6970af1d":"code","a0f37876":"code","b20632d4":"code","3aed23ad":"code","26099799":"code","b5afa448":"markdown","b9d51cd7":"markdown","de3d93b7":"markdown","3b8fc58f":"markdown","d45bf43e":"markdown","e27648a5":"markdown","dc199d48":"markdown","44162e97":"markdown","644624a2":"markdown","f7c570fb":"markdown","a425c6c0":"markdown","daa2ed72":"markdown","913c6f33":"markdown"},"source":{"6ceebcf4":"import time\nAcc_Start = time.time()\n\n# Viz\nimport matplotlib.pyplot as plt # basic plotting\nimport seaborn as sns # for prettier plots\n\n\nfrom fbprophet import Prophet\n\nplt.rcParams['figure.figsize'] = (16, 9)\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\n## Load Data\n\nPath=\"..\/input\/\"\n#os.listdir(f'{Path}')\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# TIME SERIES\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\n# Basic packages\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.options.display.max_columns = 99\nimport random as rd # generating random numbers\nimport datetime # manipulating date formats\nimport time\nimport os\nimport shutil\n%matplotlib inline\n\n#Data Preparation\n  #loading data, don't load sample submission\ndata = {\n    'item_cat': pd.read_csv(f'{Path}item_categories.csv'),\n    'items': pd.read_csv(f'{Path}items.csv'),\n    'sales_train': pd.read_csv(f'{Path}sales_train.csv'),\n    'shops': pd.read_csv(f'{Path}shops.csv'),\n    'test': pd.read_csv(f'{Path}test.csv'),\n    }\n\nBlock_End=time.time()-Acc_Start\nprint(\"Execute block use \" + \"%0.2f\" % round(Block_End,2) +\"secs\\n\")\nprint(\"Accumulate use \" + \"%0.2f\" % round(Block_End,2) +\"secs\")","78234046":"Block_Start=time.time()\ndata['sales_train']['date'] = pd.to_datetime(data['sales_train']['date'])\ndata['sales_train']['date'] = data['sales_train']['date'].dt.date\n\nBlock_End=time.time()-Block_Start\nAcc_End=time.time()-Acc_Start\nprint(\"Execute block use \" + \"%0.2f\" % round(Block_End,2) +\"secs\\n\")\nprint(\"Accumulate use \" + \"%0.2f\" % round(Acc_End,2) +\"secs\")","9357288a":"Block_Start=time.time()\ndata['sales_train']['date'] = pd.to_datetime(data['sales_train']['date'])\ndata['sales_train']['dow'] = data['sales_train']['date'].dt.dayofweek\ndata['sales_train']['year'] = data['sales_train']['date'].dt.year\ndata['sales_train']['month'] = data['sales_train']['date'].dt.month\n\ndata['sales_train'] = pd.merge(data['sales_train'], data['items'] , how='left', on=['item_id'])\ndata['sales_train'].drop(['item_name'], axis=1, inplace=True)\n\nBlock_End=time.time()-Block_Start\nAcc_End=time.time()-Acc_Start\nprint(\"Execute block use \" + \"%0.2f\" % round(Block_End,2) +\"secs\\n\")\nprint(\"Accumulate use \" + \"%0.2f\" % round(Acc_End,2) +\"secs\")","64608e66":"Block_Start=time.time()\n\n# Lable encoder for categorical variables\nlbl = LabelEncoder()\ndata['sales_train']['item_id'] = lbl.fit_transform(data['sales_train']['item_id'])\ndata['sales_train']['item_category_id'] = lbl.fit_transform(data['sales_train']['item_category_id'])\n\nBlock_End=time.time()-Block_Start\nAcc_End=time.time()-Acc_Start\nprint(\"Execute block use \" + \"%0.2f\" % round(Block_End,2) +\"secs\\n\")\nprint(\"Accumulate use \" + \"%0.2f\" % round(Acc_End,2) +\"secs\")","99da35eb":"data['sales_train'].head(20)","45649018":"cols = data['sales_train'].columns.tolist()","e2cd7826":"cols","86b5ad39":"cols=['date',\n     'date_block_num',\n     'year',\n     'month',\n     'dow',\n     'shop_id',\n     'item_category_id',\n     'item_id',\n     'item_price',\n     'item_cnt_day',\n    ]","d8ce86ae":"data['sales_train']=data['sales_train'][cols]","843b70e0":"data['sales_train'].head()","2da1f089":"Block_Start=time.time()\ndata['test'] = pd.merge(data['test'], data['items'], how='left', on=['item_id'])\ndata['test'].drop(['item_name'], axis=1, inplace=True)\ndata['test'].head(20)\nBlock_End=time.time()-Block_Start\nAcc_End=time.time()-Acc_Start\nprint(\"Execute block use \" + \"%0.2f\" % round(Block_End,2) +\"secs\\n\")\nprint(\"Accumulate use \" + \"%0.2f\" % round(Acc_End,2) +\"secs\")","7c23ee76":"Block_Start=time.time()\n#Split the test set not in train set\ntrain_items = data['sales_train'].item_id.unique()\ntest_old= data['test'][~data['test'] .item_id.isin(train_items)]\ntest_items_not_in_train = data['test'][~data['test'].item_id.isin(train_items)].item_id.unique()\nprint('%d items in test data not found in train data' % len(test_items_not_in_train))\nprint(\"\\n\")\nBlock_End=time.time()-Block_Start\nAcc_End=time.time()-Acc_Start\nprint(\"Execute block use \" + \"%0.2f\" % round(Block_End,2) +\"secs\\n\")\nprint(\"Accumulate use \" + \"%0.2f\" % round(Acc_End,2) +\"secs\")","ef95a03a":"test_new=data['test'][~data['test'].item_id.isin(train_items)]\ntest_old= data['test'][data['test'].item_id.isin(train_items)]\n\ny=len(data['test'])\nx=len(test_old)+len(test_new)\nx==y #True means that we have succesfully splitted the test_set with \n","cca8c74d":"#Block_Start=time.time()\nimport pandas_profiling\npandas_profiling.ProfileReport(data['sales_train'])\n#Block_End=time.time()-Block_Start\n\n","f824e04f":"Acc_End=time.time()-Acc_Start\n#print(\"Execute block use \" + \"%0.2f\" % round(Block_End,2) +\"secs\\n\")\nprint(\"Accumulate use \" + \"%0.2f\" % round(Acc_End,2) +\"secs\")","352aa114":"ts=data['sales_train'].groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts.astype('float')\nplt.figure(figsize=(16,8))\nplt.title('Total Sales of the company')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(ts);","acd30bec":"plt.figure(figsize=(16,6))\nplt.plot(ts.rolling(window=12,center=False).mean(),label='Rolling Mean');\nplt.plot(ts.rolling(window=12,center=False).std(),label='Rolling sd');\nplt.legend();","d40170f5":"import statsmodels.api as sm\n# multiplicative\nres = sm.tsa.seasonal_decompose(ts.values,freq=12,model=\"multiplicative\")\n#plt.figure(figsize=(16,12))\nfig = res.plot()\n#fig.show()","683c59e2":"# Additive model\nres = sm.tsa.seasonal_decompose(ts.values,freq=12,model=\"additive\")\n#plt.figure(figsize=(16,12))\nfig = res.plot()\n#fig.show()","ce42936c":"# Stationarity tests\ndef test_stationarity(timeseries):\n    \n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n\ntest_stationarity(ts)","4c6882f5":"# to remove trend\nfrom pandas import Series as Series\n# create a differenced series\ndef difference(dataset, interval=1):\n    diff = list()\n    for i in range(interval, len(dataset)):\n        value = dataset[i] - dataset[i - interval]\n        diff.append(value)\n    return Series(diff)\n\n# invert differenced forecast\ndef inverse_difference(last_ob, value):\n    return value + last_ob\n\n","02ed95dc":"ts=data['sales_train'].groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts.astype('float')\nplt.figure(figsize=(16,16))\nplt.subplot(311)\nplt.title('Original')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(ts)\nplt.subplot(312)\nplt.title('After De-trend')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nnew_ts=difference(ts)\nplt.plot(new_ts)\nplt.plot()\n\nplt.subplot(313)\nplt.title('After De-seasonalization')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nnew_ts=difference(ts,12)       # assuming the seasonality is 12 months long\nplt.plot(new_ts)\nplt.plot()","0a8c0c09":"test_stationarity(new_ts)","47989545":"proph_results = test_old.reset_index()\nproph_results['item_cnt_day'] = 0","72b41b68":"test_old.drop(['item_category_id'], axis=1, inplace=True)\ntest_old.head()","b45c0f43":"cols = test_old.columns.tolist()","5dbb7fcf":"cols","787e3d1a":"cols=['date','ID', 'shop_id', 'item_id']\n","2f685037":"test_old=test_old[cols]\n","a5a7d38e":"test_old.head()","b7096ede":"test_old['date'] = pd.to_datetime(test_old['date'], format=\"%m\/%d\/%Y\")","9ca485ab":"test_old.set_index('date', inplace=True)","43083bb6":"train.drop(['date_block_num','item_price'], axis=1, inplace=True)","6ef77a48":"train.head()","097646c3":"train.dropna(axis=1, how='all') ","577451ac":"test_old.dropna(axis=1, how='all') ","6970af1d":"train['item_cnt_day'].describe()","a0f37876":"test_old.head()","b20632d4":"tic = time.time()\n\nfor s in proph_results['shop_id'].unique():\n    for i in proph_results['item_id'].unique():\n        proph_train = train.loc[(train['shop_id'] == s) & (train['item_id'] == i)].reset_index()\n        proph_train.rename(columns={'date': 'ds', 'item_cnt_day': 'y'}, inplace=True)\n        \n        m = Prophet()\n        m.fit(proph_train[['ds', 'y']])\n        future = m.make_future_dataframe(periods=len(test_old.index.unique()), include_history=False)\n        fcst = m.predict(future)\n        \n        proph_results.loc[(proph_results['shop_id'] == s) & (proph_results['item_id'] == i), 'sales'] = fcst['yhat'].values\n        \n        toc = time.time()\n        if i % 10 == 0:\n            print(\"Completed store {} item {}. Cumulative time: {:.1f}s\".format(s, i, toc-tic))","3aed23ad":"proph_results.drop(['date', 'store', 'item'], axis=1, inplace=True)\nproph_results.head()\nproph_results = np.clip(proph_results,0.,20.)","26099799":"proph_results.to_csv('proph_results.csv', index=False)","b5afa448":"train = train[train['date'].notnull()]","b9d51cd7":"We will focus on the **test_old** first which the items are covered in the training set.","de3d93b7":"## Prophet","3b8fc58f":"## Prophet","d45bf43e":"Clean Above","e27648a5":"**Stationarity Testing**\n","dc199d48":"<a href=\"https:\/\/www.kaggle.com\/c\/demand-forecasting-kernels-only\">Link to competition on Kaggle.<\/a>\n\n<a href=\"https:\/\/facebook.github.io\/prophet\/\">Prophet<\/a> is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.\n\nProphet is open source software released by Facebook\u2019s Core Data Science team. It is available for download on CRAN and PyPI.","44162e97":"lt_date1,test=Predict_Range(test,'11.1.2015','11.30.2015')    #m.d.y","644624a2":"train_copy=train.copy()","f7c570fb":"# Store Item Demand Forecasting Challenge","a425c6c0":"for pd in data:\n    print(data[pd].head())","daa2ed72":"\nprint (test[test.ID==200])","913c6f33":"#helper function to set my predict range\n\ndef Predict_Range(pd_data,begin,end) :\n    dt_begin=datetime.datetime.strptime(begin, \"%m.%d.%Y\").date()\n    dt_end=datetime.datetime.strptime(end, \"%m.%d.%Y\").date()\n    delta = dt_end - dt_begin\n    periods_in_days=delta.days+1\n    lt_date=pd.date_range(begin, periods= periods_in_days, freq='d').tolist()\n    \n    i=0\n    pd_filled=pd.DataFrame()\n    pd_temp=pd_data[:]\n    \n    for i in range(len(lt_date)):\n        pd_temp['date']= lt_date[i].strftime('%m\/%d\/%Y')\n        pd_filled = pd.concat([pd_filled, pd_temp]) \n        pd_temp=pd_data[:]\n        \n    return lt_date,pd_filled\n \n\n"}}