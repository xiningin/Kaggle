{"cell_type":{"9015620c":"code","8452601f":"code","f4e09e32":"code","f4f18a48":"code","87925de4":"code","41db7193":"code","18168ca2":"code","c091189c":"code","1da14680":"code","66ab207b":"code","07a44a28":"code","50ec7b14":"code","eb96c4e4":"code","7e39ac4f":"code","70f86445":"code","7351d361":"code","2a90fab2":"code","167c6b7b":"code","b6b80143":"code","d803c7af":"code","a2fe29c4":"code","874ddc30":"code","f40d158e":"code","04e13090":"code","cc7e14a4":"code","86909348":"code","eb403ac5":"code","b550d0aa":"code","9ed3180b":"code","edf6daa3":"code","bd5161c4":"code","1b76c033":"code","8a942b8d":"code","af343b54":"code","774d6aa8":"code","bb2a80a1":"code","5e5e6e4e":"code","e3cedfd6":"code","7069f82f":"code","ae28d528":"markdown","16b65a3e":"markdown","201c8777":"markdown","b982767c":"markdown","a08b92eb":"markdown","1a74dd88":"markdown","01c77dcb":"markdown","3105cdde":"markdown","2b8397a9":"markdown","34bcc6e5":"markdown","7658fb85":"markdown","7ee6c711":"markdown","e834beea":"markdown"},"source":{"9015620c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename));\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8452601f":"!pip install librosa","f4e09e32":"import os\nimport pathlib\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nimport IPython.display as ipd\n\nimport pandas as pd\nimport librosa\nplt.rcParams['pcolor.shading'] = 'nearest'","f4f18a48":"audio_data = '..\/input\/cat-meow-classification\/dataset\/dataset\/B_ANI01_MC_FN_SIM01_303.wav'\nx , sr = librosa.load(audio_data)\nprint(type(x), type(sr))\nprint(x.shape, sr)","87925de4":"librosa.load(audio_data, sr=44100)\nipd.Audio(audio_data)","41db7193":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport librosa.display\nplt.figure(figsize=(14, 5))\nlibrosa.display.waveplot(x, sr=sr);","18168ca2":"# matplotlib.pyplot.pcolormesh\nX = librosa.stft(x)\nXdb = librosa.amplitude_to_db(abs(X))\nplt.figure(figsize=(14,5))\nlibrosa.display.specshow(Xdb,sr=sr, x_axis='time', y_axis='hz')\nplt.colorbar();","c091189c":"librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\nplt.colorbar();","1da14680":"plt.figure(figsize=(14, 5))\nlibrosa.display.waveplot(x, sr=sr)\n# Zooming in\nn0 = 21000\nn1 = 21500\nplt.figure(figsize=(14, 5))\nplt.plot(x[n0:n1])\nplt.grid()","66ab207b":"zero_crossings = librosa.zero_crossings(x[n0:n1])\nprint('Zero-Crossing value is:', sum(zero_crossings))","07a44a28":"# fs=10\nmfccs = librosa.feature.mfcc(x,sr=sr)\nprint(mfccs.shape)\nplt.figure(figsize=(14,10));\nplt.title('MEOW')\nlibrosa.display.specshow(mfccs,sr=sr, x_axis='time');\nplt.colorbar();","50ec7b14":"hop_length=12\nchromagram = librosa.feature.chroma_stft(x, sr=sr, hop_length=hop_length)\nplt.figure(figsize=(15, 5))\nlibrosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm')\nplt.colorbar();","eb96c4e4":"hop_length=12\nchromagram = librosa.feature.melspectrogram(x, sr=sr, hop_length=hop_length)\nplt.figure(figsize=(15, 15))\nlibrosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm')\nplt.colorbar();","7e39ac4f":"# hop_length=12\nchromagram = librosa.feature.chroma_cens(x)\nplt.figure(figsize=(15, 15))\nlibrosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm')\nplt.colorbar();","70f86445":"DATASET_PATH = '..\/input\/cat-meow-classification\/dataset\/dataset'\n\ndata_dir = pathlib.Path(DATASET_PATH)\n\nnames = np.array(tf.io.gfile.listdir(str(data_dir)))","7351d361":"names_dict = {'B':'brushing','F':'waiting for food','I':'isolation in an unfamiliar environment'}","2a90fab2":"names_2 = []\nfor x in names:\n    names_1 = x[0].replace(x[0], names_dict.get(x[0]))\n    names_2.append(names_1)","167c6b7b":"names = names_2","b6b80143":"commands = np.array(tf.io.gfile.listdir(str(data_dir)))\nprint('Commands:', commands[:5])","d803c7af":"filenames = tf.io.gfile.glob(str(data_dir) + '\/*')","a2fe29c4":"len(filenames)","874ddc30":"filenames[0]","f40d158e":"train_files = filenames[:352]\nval_files = filenames[352: 352 + 44]\ntest_files = filenames[-44:]\n\nprint('Training set size', len(train_files))\nprint('Validation set size', len(val_files))\nprint('Test set size', len(test_files))","04e13090":"test_file = tf.io.read_file(DATASET_PATH+'\/F_BRA01_MC_MN_SIM01_301.wav')\ntest_audio, _ = tf.audio.decode_wav(contents=test_file)\ntest_audio.shape","cc7e14a4":"def decode_audio(audio_binary):\n    # Decode WAV-encoded audio files to `float32` tensors, normalized\n    # to the [-1.0, 1.0] range. Return `float32` audio and a sample rate.\n    audio, _ = tf.audio.decode_wav(contents=audio_binary)\n    # Since all the data is single channel (mono), drop the `channels`\n    # axis from the array.\n    return tf.squeeze(audio, axis=-1)","86909348":"def get_label(file_path):\n    parts = tf.strings.split(\n        input=file_path,\n        sep=os.path.sep)\n    # Note: You'll use indexing here instead of tuple unpacking to enable this\n    # to work in a TensorFlow graph.\n    return parts[-1]","eb403ac5":"def get_waveform_and_label(file_path):\n    label = get_label(file_path)\n    audio_binary = tf.io.read_file(file_path)\n    waveform = decode_audio(audio_binary)\n    return waveform, label","b550d0aa":"AUTOTUNE = tf.data.AUTOTUNE\n\nfiles_ds = tf.data.Dataset.from_tensor_slices(train_files)\n\nwaveform_ds = files_ds.map(\n    map_func=get_waveform_and_label,\n    num_parallel_calls=AUTOTUNE)","9ed3180b":"waveform_ds","edf6daa3":"for (audio,label) in waveform_ds.take(1):\n    print(label)\n    pd.DataFrame(audio).plot()","bd5161c4":"rows = 3\ncols = 3\nn = rows * cols\nfig, axes = plt.subplots(rows, cols, figsize=(20, 12))\nplt.subplots_adjust(wspace=0.3, hspace=0.2)\n\nfor i, (audio, label) in enumerate(waveform_ds.take(n)):\n    r = i \/\/ cols\n    c = i % cols\n    ax = axes[r][c]\n    ax.plot(audio.numpy())\n    ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n    ax.grid()\n    label = label.numpy().decode('utf-8')\n    ax.set_title(label)\n\nplt.show()","1b76c033":"def get_spectrogram(waveform):\n    # Zero-padding for an audio waveform with less than 16,000 samples.\n    input_len = 16000\n    waveform = waveform[:input_len]\n    zero_padding = tf.zeros(\n        [16000] - tf.shape(waveform),\n        dtype=tf.float32)\n    # Cast the waveform tensors' dtype to float32.\n    waveform = tf.cast(waveform, dtype=tf.float32)\n    # Concatenate the waveform with `zero_padding`, which ensures all audio\n    # clips are of the same length.\n    equal_length = tf.concat([waveform, zero_padding], 0)\n    # Convert the waveform to a spectrogram via a STFT.\n    spectrogram = tf.signal.stft(\n        equal_length, frame_length=255, frame_step=128)\n    # Obtain the magnitude of the STFT.\n    spectrogram = tf.abs(spectrogram)\n    # Add a `channels` dimension, so that the spectrogram can be used\n    # as image-like input data with convolution layers (which expect\n    # shape (`batch_size`, `height`, `width`, `channels`).\n    spectrogram = spectrogram[..., tf.newaxis]\n    return spectrogram","8a942b8d":"for waveform, label in waveform_ds.take(1):\n    label = label.numpy().decode('utf-8')\n    spectrogram = get_spectrogram(waveform)\n\nprint('Label:', label)\nprint('Waveform shape:', waveform.shape)\nprint('Spectrogram shape:', spectrogram.shape)\nprint('Audio playback')\nipd.display(ipd.Audio(waveform, rate=16000))","af343b54":"def plot_spectrogram(spectrogram, ax):\n    if len(spectrogram.shape) > 2:\n        assert len(spectrogram.shape) == 3\n        spectrogram = np.squeeze(spectrogram, axis=-1)\n    # Convert the frequencies to log scale and transpose, so that the time is\n    # represented on the x-axis (columns).\n    # Add an epsilon to avoid taking a log of zero.\n    log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n    height = log_spec.shape[0]\n    width = log_spec.shape[1]\n    X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n    Y = range(height)\n    ax.pcolormesh(X, Y, log_spec)","774d6aa8":"fig, axes = plt.subplots(2, figsize=(12, 8))\ntimescale = np.arange(waveform.shape[0])\naxes[0].plot(timescale, waveform.numpy())\naxes[0].set_title('Waveform')\naxes[0].set_xlim([0, 16000])\n\nplot_spectrogram(spectrogram.numpy(), axes[1])\naxes[1].set_title('Spectrogram')\nplt.show();","bb2a80a1":"def get_spectrogram_and_label_id(audio, label):\n    spectrogram = get_spectrogram(audio)\n    label_id = tf.argmax(label == commands)\n    return spectrogram, label_id","5e5e6e4e":"spectrogram_ds = waveform_ds.map(\n  map_func=get_spectrogram_and_label_id,\n  num_parallel_calls=AUTOTUNE)","e3cedfd6":"rows = 3\ncols = 3\nn = rows*cols\nfig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n\nfor i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):\n    r = i \/\/ cols\n    c = i % cols\n    ax = axes[r][c]\n    plot_spectrogram(spectrogram.numpy(), ax)\n    ax.set_title(commands[label_id.numpy()])\n    ax.axis('off')\n    \nplt.show()","7069f82f":"# to be continued...","ae28d528":"\nWe'll use The legendary Librosa library for exploring audio data","16b65a3e":"### Melspectrogram","201c8777":"<p style=\"background-color:#252629;font-family:avenir next;color:#F1F3F4;font-size:200%;text-align:center;border-radius:25px 25px;\">MEOW CLASSIFICATION<\/p>","b982767c":"<p style=\"background-color:#252629;font-family:avenir next;color:#F1F3F4;font-size:200%;text-align:center;border-radius:25px 25px;\">Converting audio to spectrogram<\/p>","a08b92eb":"### Zooming","1a74dd88":"<p style=\"background-color:#252629;font-family:avenir next;color:#F1F3F4;font-size:200%;text-align:center;border-radius:25px 25px;\">Extract audio to dataset<\/p>","01c77dcb":"### Zero-Crossing","3105cdde":"#### About this directory\n#### Data directory.\n#### Naming convention for files -> CNNNNNBBSSOOOOO_RXX, where:\n#### C = emission context (values: B = brushing; F = waiting for food; I: isolation in an unfamiliar environment);\n#### NNNNN = cat\u2019s unique ID;\n#### BB = breed (values: MC = Maine Coon; EU: European Shorthair);\n#### SS = sex (values: FI = female, intact; FN: female, neutered; MI: male, intact; MN: male, neutered);\n#### OOOOO = cat owner\u2019s unique ID;\n#### R = recording session (values: 1, 2 or 3)\n#### XX = vocalization counter (values: 01..99)","2b8397a9":"### Mel-Frequency Cepstral Coefficients(MFCCs)","34bcc6e5":"### Chroma feature","7658fb85":"<p style=\"background-color:#252629;font-family:avenir next;color:#F1F3F4;font-size:200%;text-align:center;border-radius:25px 25px;\">Features extraction<\/p>","7ee6c711":"### Chroma cens","e834beea":"B = brushing; F = waiting for food; I: isolation in an unfamiliar environment"}}