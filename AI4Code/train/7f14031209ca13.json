{"cell_type":{"c451c927":"code","0fb3370c":"code","dbea86a2":"code","8bb1ba09":"code","2187f8d1":"code","b7cd95af":"code","f122d2e2":"code","89012712":"code","381c5b87":"code","95a8ac23":"markdown","fd841475":"markdown","be52eb5f":"markdown","6e965d19":"markdown","5fa9c680":"markdown"},"source":{"c451c927":"import random, os, warnings, math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import optimizers, losses, metrics, Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom transformers import TFAutoModelForSequenceClassification, TFAutoModel, AutoTokenizer","0fb3370c":"\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nseed = 0\nseed_everything(seed)\nsns.set(style='whitegrid')\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_colwidth', 150)","dbea86a2":"# TPU or GPU detection\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU {tpu.master()}')\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","8bb1ba09":"train_filepath = '\/kaggle\/input\/commonlitreadabilityprize\/train.csv'\ntest_filepath = '\/kaggle\/input\/commonlitreadabilityprize\/test.csv'\n\ntrain = pd.read_csv(train_filepath)\ntest = pd.read_csv(test_filepath)\n\n# removing unused columns\ntrain.drop(['url_legal', 'license'], axis=1, inplace=True)\ntest.drop(['url_legal', 'license'], axis=1, inplace=True)","2187f8d1":"BATCH_SIZE = 8 * REPLICAS\nLEARNING_RATE = 1e-5 * REPLICAS\nEPOCHS = 35\nES_PATIENCE = 7\nPATIENCE = 2\nN_FOLDS = 5\nSEQ_LEN = 256\nBASE_MODEL = '\/kaggle\/input\/huggingface-roberta\/roberta-base\/'","b7cd95af":"\n# Datasets utility functions\ndef custom_standardization(text):\n    text = text.lower()\n    text = text.strip()\n    return text\n\n\ndef sample_target(features, target):\n    mean, stddev = target\n    sampled_target = tf.random.normal([], mean=tf.cast(mean, dtype=tf.float32), \n                                      stddev=tf.cast(stddev, dtype=tf.float32), dtype=tf.float32)\n    \n    return (features, sampled_target)\n    \n\n# Get a dataset ready\ndef get_dataset(pandas_df, tokenizer, labeled=True, ordered=False, repeated=False, \n                is_sampled=False, batch_size=32, seq_len=128):\n    text = [custom_standardization(text) for text in pandas_df['excerpt']]\n    \n    # Tokenize inputs\n    tokenized_inputs = tokenizer(text, max_length=seq_len, truncation=True, \n                                 padding='max_length', return_tensors='tf')\n    \n    if labeled:\n        dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs['input_ids'], \n                                                      'attention_mask': tokenized_inputs['attention_mask']}, \n                                                      (pandas_df['target'], pandas_df['standard_error'])))\n        if is_sampled:\n            dataset = dataset.map(sample_target, num_parallel_calls=tf.data.AUTOTUNE)\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices({'input_ids': tokenized_inputs['input_ids'], \n                                                      'attention_mask': tokenized_inputs['attention_mask']})\n        \n    if repeated:\n        dataset = dataset.repeat()\n    if not ordered:\n        dataset = dataset.shuffle(1024)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    \n    return dataset\n","f122d2e2":"\ndef model_fn(encoder, seq_len=256):\n    input_ids = L.Input(shape=(seq_len,), dtype=tf.int32, name='input_ids')\n    input_attention_mask = L.Input(shape=(seq_len,), dtype=tf.int32, name='attention_mask')\n    \n    outputs = encoder({'input_ids': input_ids, \n                       'attention_mask': input_attention_mask})\n    \n    model = Model(inputs=[input_ids, input_attention_mask], outputs=outputs)\n\n    optimizer = optimizers.Adam(lr=LEARNING_RATE)\n    model.compile(optimizer=optimizer, \n                  loss=losses.MeanSquaredError(), \n                  metrics=[metrics.RootMeanSquaredError()])\n    \n    return model\n\n\nwith strategy.scope():\n    encoder = TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)\n    model = model_fn(encoder, SEQ_LEN)\n    \nmodel.summary()","89012712":"tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\nskf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\noof_pred = []; oof_labels = []; history_list = []; test_pred = []\n\nfor fold,(idxT, idxV) in enumerate(skf.split(train)):\n    if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\n    print(f'\\nFOLD: {fold+1}')\n    print(f'TRAIN: {len(idxT)} VALID: {len(idxV)}')\n\n    # Model\n    K.clear_session()\n    with strategy.scope():\n        encoder = TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)\n        model = model_fn(encoder, SEQ_LEN)\n        \n    model_path = f'model_{fold}.h5'\n    es = EarlyStopping(monitor='val_root_mean_squared_error', mode='min', \n                       patience=ES_PATIENCE, restore_best_weights=True, verbose=1)\n    checkpoint = ModelCheckpoint(model_path, monitor='val_root_mean_squared_error', mode='min', \n                                 save_best_only=True, save_weights_only=True)\n\n    # Train\n    history = model.fit(x=get_dataset(train.loc[idxT], tokenizer, repeated=True, is_sampled=True, \n                                      batch_size=BATCH_SIZE, seq_len=SEQ_LEN), \n                        validation_data=get_dataset(train.loc[idxV], tokenizer, ordered=True, \n                                                    batch_size=BATCH_SIZE, seq_len=SEQ_LEN), \n                        steps_per_epoch=50, \n                        callbacks=[es, checkpoint], \n                        epochs=EPOCHS,  \n                        verbose=2).history\n      \n    history_list.append(history)\n    # Save last model weights\n    model.load_weights(model_path)\n    \n    # Results\n    print(f\"#### FOLD {fold+1} OOF RMSE = {np.min(history['val_root_mean_squared_error']):.4f}\")\n\n    # OOF predictions\n    valid_ds = get_dataset(train.loc[idxV], tokenizer, ordered=True, batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n    oof_labels.append([target[0].numpy() for sample, target in iter(valid_ds.unbatch())])\n    tempdebugv = valid_ds.map(lambda sample, target: model(sample,training=False)).unbatch().batch(len(idxV))\n    \n    for yv in tempdebugv.take(1):\n        pass\n    oof_pred.append(yv[0].numpy().flatten())\n\n    # Test predictions\n    test_ds = get_dataset(test, tokenizer, labeled=False, ordered=True, batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n    x_test = test_ds.map(lambda sample: sample)\n    test_pred.append(model.predict(x_test)['logits'])","381c5b87":"submission = test[['id']]\nsubmission['target'] = np.mean(test_pred, axis=0)\nsubmission.to_csv('submission.csv', index=False)\ndisplay(submission.head(10))","95a8ac23":"## \ud83c\udf7d ***Import Packages***","fd841475":"## \ud83c\udf73 ***Pre-processing and Training Model***","be52eb5f":"K-fold corss validation","6e965d19":"![](https:\/\/t1.daumcdn.net\/cfile\/tistory\/99D6E7505E24EFF221)","5fa9c680":"## \ud83c\udf73 ***Loading Test & Train set***"}}