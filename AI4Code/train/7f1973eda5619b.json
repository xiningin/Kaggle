{"cell_type":{"edb0d4d2":"code","9cbb29e4":"code","ee217d40":"code","c570d9cd":"code","26607a87":"code","7e30da0a":"code","c3ed384e":"code","5c66f5cd":"code","798beb62":"code","1797efcc":"code","1b616508":"code","feeffcb5":"code","a23b79aa":"code","51eb0830":"code","24a30302":"code","a610bf85":"code","3fdbc937":"code","87270ae8":"code","1f5d5eef":"code","3f3ddad5":"code","8ad1f56c":"code","99792600":"code","623ee9d8":"code","1295be77":"code","5fb81476":"code","e69f0b3a":"code","d376ddee":"code","5f75caac":"code","749c7a00":"code","f1694adf":"code","3812c743":"code","d727bce1":"code","a8fb764d":"code","6dd48a59":"code","4560056e":"code","98527af9":"code","5563ccbd":"code","1acb347d":"code","4cfa6a48":"code","1bc60a9f":"code","d496a74d":"code","0bae9b99":"code","c601c7e8":"markdown","887544e3":"markdown","9b09e398":"markdown","c30f8721":"markdown","3fa10afb":"markdown","1e43294a":"markdown"},"source":{"edb0d4d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9cbb29e4":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, plot_roc_curve, classification_report\nfrom  sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt","ee217d40":"main_df = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ndf = main_df.copy()\ndf.head()","c570d9cd":"# Information about the dataset\ndf.info()","26607a87":"df.isnull().sum()","7e30da0a":"df['Class'].value_counts()","c3ed384e":"# 0 Normal Transaction\n# 1 Fraud Transaction\n\nlegit = df[df.Class == 0]\nfraud = df[df.Class == 1]","5c66f5cd":"print(legit.shape)\nprint(fraud.shape)","798beb62":"# Statistical Measure of the data\nlegit.Amount.describe()","1797efcc":"fraud.Amount.describe()","1b616508":"# compare the values for both transactions\ndf.groupby('Class').mean()","feeffcb5":"# Undersampling\n# Build a sample dataset containing the similar distribution of the normal transactions and Fraudulent Transactions\n\nlegit_sample = legit.sample(n=492)","a23b79aa":"# concatenating 2 dataset\nnew_df= pd.concat([legit_sample, fraud], axis = 0)","51eb0830":"new_df.shape","24a30302":"new_df.head()","a610bf85":"new_df['Class'].value_counts()","3fdbc937":"# Mean of each column is still approx. same which means sample are still same\n\nnew_df.groupby('Class').mean()","87270ae8":"# Splitting the data into features & target\nX = new_df.drop(columns = 'Class', axis=1)\nY = new_df['Class']","1f5d5eef":"X.shape","3f3ddad5":"Y.shape","8ad1f56c":"# Split the data into training and test data\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n","99792600":"model_lr = LogisticRegression(max_iter=120,random_state=0, n_jobs=20, solver='liblinear')\n","623ee9d8":"model_lr.fit(X_train, y_train)","1295be77":"trn_lr_pred = model_lr.predict(X_train)\ntrn_lr_acc = accuracy_score(trn_lr_pred, y_train)\nprint(round(trn_lr_acc*100, 2))","5fb81476":"tst_lr_pred = model_lr.predict(X_test)\ntst_lr_acc = accuracy_score(tst_lr_pred, y_test)\nprint(round(tst_lr_acc*100, 2))","e69f0b3a":"cm1 = confusion_matrix(y_test, tst_lr_pred)\nsns.heatmap(cm1\/np.sum(cm1), annot = True, fmt=  '0.2%', cmap = 'Reds')","d376ddee":"fig, ax = plt.subplots(figsize=(12, 8))\nplot_roc_curve(model_lr, X_test, y_test, color='darkgreen', ax=ax)","5f75caac":"print(classification_report(y_test,tst_lr_pred))","749c7a00":"from imblearn.over_sampling import SMOTE\nfrom sklearn.pipeline import Pipeline","f1694adf":"df.shape","3812c743":"X1 = df.drop(columns='Class', axis=1)\ny1 = df['Class']","d727bce1":"X1.shape, y1.shape","a8fb764d":"sm = SMOTE(sampling_strategy='auto', random_state=42, k_neighbors=4)\nX_oversampled , y_oversampled = sm.fit_resample(X1, y1)\n","6dd48a59":"pipeline = Pipeline([('model', LogisticRegression(solver='liblinear'))])\npipeline.fit(X_oversampled, y_oversampled)","4560056e":"# Summarize the fraud class distribution of the new SMOTE-transformed dataset\nunique_original, counts_original = np.unique(y1, return_counts=True)\nunique_oversampled, counts_oversampled = np.unique(y_oversampled, return_counts=True)\n\nprint('Original fraud class distribution:', dict(zip(unique_original, counts_original)))\nprint('New transformed fraud class distribution:',dict(zip(unique_oversampled, counts_oversampled)))","98527af9":"X_train, X_test, y_train, y_test = train_test_split(X_oversampled, y_oversampled, test_size=0.33, random_state=42)","5563ccbd":"model_lr_smt = LogisticRegression(solver='liblinear')\nmodel_lr_smt.fit(X_train, y_train)","1acb347d":"trn_lr_smt_pred = model_lr.predict(X_train)\ntrn_lr_smt_acc = accuracy_score(trn_lr_smt_pred, y_train)\nprint(round(trn_lr_smt_acc*100, 2))","4cfa6a48":"tst_lr_smt_pred = model_lr.predict(X_test)\ntst_lr_smt_acc = accuracy_score(tst_lr_smt_pred, y_test)\nprint(round(tst_lr_smt_acc*100, 2))","1bc60a9f":"cm2 = confusion_matrix(y_test, tst_lr_smt_pred)\nsns.heatmap(cm2\/np.sum(cm2), annot = True, fmt=  '0.2%', cmap = 'Reds')","d496a74d":"fig, ax = plt.subplots(figsize=(12, 8))\nplot_roc_curve(model_lr_smt, X_test, y_test, color='darkgreen', ax=ax)","0bae9b99":"print(classification_report(y_test,tst_lr_smt_pred))","c601c7e8":"* dataset is highly imbalance","887544e3":"#####  Conclusion : Model have achieved an accuracy of approx 97% .","9b09e398":"### Using SMOTE for Unbalance dataset","c30f8721":"<center> <img src=\"https:\/\/miro.medium.com\/max\/800\/0*_6WEDnZubsQfTMlY.png\"> <\/center>","3fa10afb":"* Here accuracy on the train and the test set is almost same which means that model is neither overfitting nor underfitting.","1e43294a":"## About Dataset\n\n**Context**\n\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n\n**Content**\n\nThe dataset contains transactions made by credit cards in September 2013 by European cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\nGiven the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\n\n"}}