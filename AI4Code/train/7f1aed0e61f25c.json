{"cell_type":{"30a0145c":"code","9015bf11":"code","6c859d95":"code","089c0fa7":"code","a47e8f35":"code","5d800806":"code","bccccc55":"code","829384b0":"code","da80d089":"code","ee50db50":"code","18bec718":"code","c9d2d000":"code","210fc0cf":"code","3afa3bc1":"code","3fcf3958":"code","c880568e":"code","efb09e4f":"code","90b397ba":"code","1222fea4":"code","dc210b02":"code","16c24b52":"code","fcfea3aa":"code","cbc525fe":"code","88f11e5b":"markdown","456edcf8":"markdown","76e03b78":"markdown","f87c0b21":"markdown"},"source":{"30a0145c":"import gc\nfrom functools import partial\nfrom pathlib import Path\n\nfrom fastai.text import *\nfrom fastai.callbacks import *\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\npd.set_option('display.max_colwidth', 200)\npd.set_option('display.max_columns', None)\npd.set_option('display.min_rows', 100)\npd.set_option('display.max_rows', 100)\n\nhome = Path(\".\")\ninput_dir = Path(\"\/kaggle\/input\/google-quest-challenge\/\")\n\n!mkdir -p ~\/.fastai\/models\/\n!cp -R ..\/input\/fastai-wt103-models\/* ~\/.fastai\/models\/","9015bf11":"BS = 256","6c859d95":"pd.read_csv(input_dir\/\"sample_submission.csv\").head(5)","089c0fa7":"raw_test = pd.read_csv(input_dir\/\"test.csv\"); raw_test.tail(3)","a47e8f35":"raw_train = pd.read_csv(input_dir\/\"train.csv\"); raw_train.tail(3)","5d800806":"lm_df = raw_train.append(raw_test, ignore_index=True, sort=False)","bccccc55":"np.random.seed(42)","829384b0":"lm_df = lm_df.iloc[np.random.permutation(len(raw_train))]\ncut = int(0.2 * len(lm_df)) + 1\ntrain_lm_df, valid_lm_df = lm_df[cut:], lm_df[:cut]","da80d089":"data_lm = TextLMDataBunch.from_df(home, train_lm_df, valid_lm_df,\n                                  text_cols=[\"question_title\", \"question_body\", \"answer\"],\n                                  mark_fields=True,\n                                  bs=BS)","ee50db50":"data_lm.show_batch()","18bec718":"data_lm.save('.\/data_lm_export.pkl')","c9d2d000":"labels = raw_train.columns[(raw_train.columns.str.startswith(\"question_\")) |\n                           (raw_train.columns.str.startswith(\"answer_\"))].to_list()\nlabels = list(filter(lambda x: x not in ['question_title',\n                                         'question_body',\n                                         'question_user_name',\n                                         'question_user_page',\n                                         'answer_user_name',\n                                         'answer_user_page',], labels))\nassert len(labels) == 30","210fc0cf":"data_clas = TextClasDataBunch.from_csv(home, input_dir\/\"train.csv\", test=input_dir\/\"test.csv\",\n                                       vocab=data_lm.train_ds.vocab, bs=BS,\n                                       text_cols=[\"question_title\", \"question_body\", \"answer\"],\n                                       mark_fields=True,\n                                       label_cols=labels)","3afa3bc1":"data_clas.show_batch(reverse=True)","3fcf3958":"data_clas.save('.\/data_clas.pkl')","c880568e":"data_lm = load_data(home, 'data_lm_export.pkl', bs=BS)","efb09e4f":"learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5,\n                               metrics=[accuracy, Perplexity()],\n                               callback_fns=[partial(EarlyStoppingCallback, monitor=\"perplexity\", mode=\"min\", patience=3),\n                                             partial(SaveModelCallback, monitor=\"perplexity\", mode=\"min\", name=\"best_model\")])\nlearn = learn.to_fp16()","90b397ba":"learn.lr_find()\nlearn.recorder.plot(skip_end=5)","1222fea4":"lr = 5e-02\nmoms = (0.8, 0.7)\nwd=0.1","dc210b02":"learn.fit_one_cycle(5, slice(lr), moms=moms, wd=wd)","16c24b52":"learn.unfreeze()\nlearn.fit_one_cycle(100, slice(lr\/2), moms=moms, wd=wd,\n                    callbacks=[SaveModelCallback(learn, monitor=\"perplexity\", name=\"best_model\"),\n                               ReduceLROnPlateauCallback(learn, monitor=\"perplexity\", patience=5,\n                                                         min_delta=0.1, min_lr=1e-6)])","fcfea3aa":"learn.save_encoder('ft_enc')","cbc525fe":"learn.predict(\"As a non-mathematician, I am somewhat\", n_words=10)","88f11e5b":"# Preprocess","456edcf8":"Using all the text data to fine tune LM","76e03b78":"This notebook is to show how one can fine tune the language model for transfer learning and pre-process data.\n\nThe whole workflow:\n1. Fine tune fastai wt103 lm on the competition text\n2. Train a classification model using encodings from the previous step\n\nThe language model uses public test set, which is onle 13% of the whole test set. To improve results, you want to train LM on the private test set also. In other words, include the LM fine tuning in the submission notebook.","f87c0b21":"# Fine tune LM"}}