{"cell_type":{"0f2a1be7":"code","dec8351f":"code","7ee0c29d":"code","1fc51c8d":"code","406c1f37":"code","bad0d961":"code","689e77f8":"code","6abb9658":"code","3a4c0df0":"code","0650b819":"code","54ff92b7":"code","47715d20":"code","4a37886b":"code","adf08937":"code","231a1265":"code","e821c288":"code","91050f71":"code","f99e03ac":"code","23b2951e":"code","291d7395":"code","2b0564a8":"code","56483941":"code","d08eeec8":"code","85c8b65d":"code","7c990d60":"code","f50af26c":"code","19f273e6":"code","4ef41c2f":"code","60de7a8d":"code","e25f39d3":"code","a64daf37":"code","e369e921":"code","b18866a6":"code","845f5436":"code","bd17e1ca":"code","7f758c55":"code","291b8a6b":"code","e37d0dae":"code","bba91bc2":"markdown","cbaf23d6":"markdown","7a872d65":"markdown","f1adfc0d":"markdown","1c671401":"markdown","6075853e":"markdown","3459fbb5":"markdown","ce5023c5":"markdown","3de6abeb":"markdown","2b95d3e8":"markdown","95f70fe1":"markdown","05f6a5b8":"markdown","ddfa414f":"markdown","70d730e3":"markdown","c5ced8f9":"markdown","31594087":"markdown","5e62045d":"markdown","4a768698":"markdown"},"source":{"0f2a1be7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom transformers import LongformerTokenizer\nfrom tqdm.notebook import tqdm\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom sklearn.model_selection import train_test_split\n\nimport glob\nimport re\n\ntqdm.pandas()","dec8351f":"# In debug mode a subset of the training dataset is used\nDEBUG = False","7ee0c29d":"# Column Data Types\ndtype = {\n    'id': 'string',\n    'discourse_id': np.uint64,\n    'discourse_start': np.uint16,\n    'discourse_end': np.uint16,\n    'discourse_text': 'string',\n    'discourse_type': 'category',\n    'discourse_type_num': 'category',\n    'predictionstring': 'string',\n}\n\nif DEBUG:\n    train = pd.read_csv('\/kaggle\/input\/feedback-prize-2021\/train.csv', dtype=dtype).head(int(10e3))\nelse:\n    train = pd.read_csv('\/kaggle\/input\/feedback-prize-2021\/train.csv', dtype=dtype)\n\ndisplay(train.head())\n\ndisplay(train.info())","1fc51c8d":"# Number of Annotated Words, Ignoring Double Annotated Words\ndef sample_ann_word_count(predictionstrings):\n    s = set()\n    \n    for l in predictionstrings.str.split():\n        for e in l:\n            s.add(int(e))\n        \n    return [len(s)] * len(predictionstrings)\n\n# Text Word Count\ntrain['word_count'] = train['discourse_text'].apply(word_tokenize).apply(len).astype(np.uint16)\n\n# Text Word Count\ntrain['ann_word_count'] = train['predictionstring'].str.split(' ').apply(len).astype(np.uint16)\n\n# Sample Word Count\ntrain['sample_ann_word_count'] = train.groupby('id')['predictionstring'].transform(sample_ann_word_count).astype(np.uint16)\n\n# Text Sentence Count\ntrain['sentence_count'] = train['discourse_text'].apply(sent_tokenize).apply(len).astype(np.uint16)\n\n# Maximum Word Index\ntrain['max_word_index'] = train['predictionstring'].str.split(' ').apply(lambda l: int(l[-1])).astype(np.uint16)\n\n# Max Word Index of Text ID\ntrain['sample_max_word_index'] = train.groupby('id')['max_word_index'].transform('max')","406c1f37":"# Text ID to Word Count\ndef id2sample_word_count(text_ids):\n    text_id = text_ids.values[0]\n    # Read Text File\n    with open(f'\/kaggle\/input\/feedback-prize-2021\/train\/{text_id}.txt', 'r') as f:\n        text = f.read().split()\n        word_count = len(text)\n        \n    return [word_count] * len(text_ids)","bad0d961":"# Sample Word Count\ntrain['sample_word_count'] = train.groupby('id')['id'].transform(id2sample_word_count).astype(np.uint16)\n\n# Ratio of Annotated Words\ntrain['ann_ratio'] = (train['sample_ann_word_count'] \/ train['sample_word_count']).astype(np.float32)","689e77f8":"display(train.head(10))","6abb9658":"# Discourse Type Distribution\nplt.figure(figsize=(10,10))\ntrain.groupby('discourse_type')['discourse_type'].count().plot(kind='pie', autopct='%1.1f%%', textprops={'fontsize': 16}, startangle=0)\nplt.title('Discourse Type Distribution', size=24)\nplt.ylabel('')\npass","3a4c0df0":"# Discourse Type Distribution\nplt.figure(figsize=(10,10))\ntrain.groupby('discourse_type')['ann_word_count'].sum().plot(kind='pie', autopct='%1.1f%%', textprops={'fontsize': 16}, startangle=0)\nplt.title('Discourse Type Annotated Words Distribution', size=24)\nplt.ylabel('')\npass","0650b819":"# The annotated word count inbalance can be explained by the difference in discourse type size\ndisplay(train.groupby(['discourse_type'])['ann_word_count'].describe())","54ff92b7":"# Word Count Distribution\nplt.figure(figsize=(15, 8))\ntrain.groupby('id')['word_count'].sum().plot(kind='hist', bins=32)\nplt.title('Word Count Distribution', size=24)\nplt.xlabel('Word Count', size=18)\nplt.ylabel('Frequency', size=18)\nplt.xticks(size=16)\nplt.yticks(size=16)\nplt.grid()\npass","47715d20":"# Word Count Distribution\nplt.figure(figsize=(15, 8))\ntrain.groupby('id')['ann_word_count'].sum().plot(kind='hist', bins=32)\nplt.title('Text Annotated Word Count Distribution', size=24)\nplt.xlabel('Text Annotated Word Count', size=18)\nplt.ylabel('Frequency', size=18)\nplt.xticks(size=16)\nplt.yticks(size=16)\nplt.grid()\npass","4a37886b":"# Word Count Distribution\nplt.figure(figsize=(15, 8))\ntrain.groupby('id')['sample_word_count'].first().plot(kind='hist', bins=32)\nplt.title('Sample Word Count Distribution', size=24)\nplt.xlabel('Sample Word Count', size=18)\nplt.ylabel('Frequency', size=18)\nplt.xticks(size=16)\nplt.yticks(size=16)\nplt.grid()\npass","adf08937":"# Word Count Distribution\nplt.figure(figsize=(15, 8))\ntrain.groupby('id')['sentence_count'].sum().plot(kind='hist', bins=32)\nplt.title('Sentence Count Distribution', size=24)\nplt.xlabel('Sentence Count', size=18)\nplt.ylabel('Frequency', size=18)\nplt.xticks(size=16)\nplt.yticks(size=16)\nplt.grid()\npass","231a1265":"# Annotation Ratio Distribution\nplt.figure(figsize=(20, 8))\ntrain.groupby('id')['ann_ratio'].first().plot(kind='hist', bins=32)\nplt.title('Annotation Ratio', size=24)\nplt.xlabel('Annotation Ration', size=18)\nplt.ylabel('Frequency', size=18)\nplt.xticks([i * 0.05 for i in range(21)], size=14)\nplt.yticks(size=16)\nplt.grid()\npass","e821c288":"# Input Sequence Length\nSEQ_LENGTH= 4096","91050f71":"# Load the Tokenizer\ntokenizer = LongformerTokenizer.from_pretrained('allenai\/longformer-large-4096')\n\n# Save Tokenizer\ntokenizer.save_pretrained('.\/tokenizer\/')","f99e03ac":"# This function tokenize the text according to a transformers model tokenizer\ndef tokenize(excerpt, padding='max_length', max_length=SEQ_LENGTH):\n    enc_di = tokenizer.encode_plus(\n        excerpt,\n        padding = padding,\n        truncation = True,\n        max_length = max_length,\n    )\n    \n    return np.array(enc_di['input_ids'], dtype=np.int32)","23b2951e":"def get_token_lengths(text_ids):\n    text_id = text_ids.values[0]\n    # Read Text File\n    with open(f'\/kaggle\/input\/feedback-prize-2021\/train\/{text_id}.txt', 'r') as f:\n        text = f.read().split(' ')\n        \n    # Tokenize Text\n    text_encoded = tokenize(text, padding='do_not_pad', max_length=np.PINF)\n    \n    return [len(text_encoded)] * len(text_ids)\n\n# Get Token Length\ntrain['token_len'] = train.groupby('id')['id'].progress_transform(get_token_lengths).astype(np.uint16)","291d7395":"# Token Length Distribution\nplt.figure(figsize=(15, 8))\ntrain.groupby('id')['token_len'].first().plot(kind='hist', bins=64)\nplt.title('Token Length Distribution', size=24)\nplt.xlabel('Token Length', size=18)\nplt.ylabel('Frequency', size=18)\nplt.xticks(size=16)\nplt.yticks(size=16)\nplt.grid()\npass","2b0564a8":"# Minimum Annotation Ratio\nANN_RATIO_THRESHOLD = 0.70\n\n# All Text Ids\nALL_TEXT_IDS = train['id'].nunique()\n\n# Drop Sample with Word Count Above Threshold and Annotation Ratio Below Threshold\ndrop_idxs = train[\n    (train['token_len'] > SEQ_LENGTH) | # Token Length Larger than Max Sequence Length\n    (train['ann_ratio'] < ANN_RATIO_THRESHOLD) | # Annotation Ratio Below Threshold\n    (train['max_word_index'] >= SEQ_LENGTH) # Max Word Index Larger than Max Sequence Length\n].index\ntrain = train.drop(drop_idxs, axis=0).reset_index(drop=True)\n\n# Valid Text Ids\nVALID_TEXT_IDS = list(train['id'].unique())\nN_VALID_TEXT_IDS = len(VALID_TEXT_IDS)\nprint(f'{N_VALID_TEXT_IDS} valid text ids out of {ALL_TEXT_IDS} text ids')","56483941":"# We will be using 1024 Validation Samples\nN_VAL = 1024\nX_train_ids, X_val_ids = train_test_split(train['id'].unique(), test_size=N_VAL, random_state=42)\nprint(f'X_train_ids shape: {X_train_ids.shape}, X_val_ids shape: {X_val_ids.shape}')","d08eeec8":"X_train_ids = list(X_train_ids)\nX_val_ids = list(X_val_ids)\n\nDISCOURSE_TYPES = train['discourse_type'].unique().tolist()\n\nid2discourse_types_dict = train.groupby('id')['discourse_type'].unique().to_dict()","85c8b65d":"# Dicsourse Type Count\ndef train_ids2discourse_type_counts():\n    discourse_type_counts_train_ids = dict([(dt, 0) for dt  in DISCOURSE_TYPES])\n    \n    for train_id in X_train_ids:\n        for dt in id2discourse_types_dict[train_id]:\n            discourse_type_counts_train_ids[dt] += 1\n            \n    return pd.Series(discourse_type_counts_train_ids).sort_values()\n\ndiscourse_type_counts0 = train_ids2discourse_type_counts()\n\n# Discour Type Count\ndisplay(discourse_type_counts0.sort_index())","7c990d60":"# Fill All Classes to the Majority Class\nDISCOURSE_TYPES = discourse_type_counts0.index.tolist()\nFILL_TO = max(discourse_type_counts0)\n\nprint(f'DISCOURSE_TYPES: {DISCOURSE_TYPES}, FILL_TO: {FILL_TO}')","f50af26c":"# Oversample to Maximum Sample Count\nfor dt in tqdm(DISCOURSE_TYPES):\n    # Get current Discourse Type Count\n    discourse_type_counts = train_ids2discourse_type_counts()\n    samples_discourse_type = discourse_type_counts[dt]\n    if samples_discourse_type < FILL_TO:\n        while samples_discourse_type < FILL_TO:\n            # Take Random ID\n            random_id = str(np.random.choice(X_train_ids, 1).squeeze())\n            if dt in id2discourse_types_dict[random_id]:\n                X_train_ids.append(random_id)\n                samples_discourse_type += 1","19f273e6":"print('=== BEFORE ===')\ndisplay(discourse_type_counts0.to_frame().sort_index())\nprint('=== AFTER ===')\ndisplay(discourse_type_counts.to_frame().sort_index())\nprint('=== DIFFERENCE PERCENTAGE ===')\npercentual_increase = ((discourse_type_counts - discourse_type_counts0) \/ discourse_type_counts0 * 100)\npercentual_increase = percentual_increase.apply(lambda i: f'{int(i)}%')\npercentual_increase = percentual_increase.to_frame(name='Percentage Increase')\ndisplay(percentual_increase.sort_index())","4ef41c2f":"N_TRAIN_SAMPLES = len(X_train_ids)\nN_VAL_SAMPLES = len(X_val_ids)\n\nprint(f'N_TRAIN_SAMPLES: {N_TRAIN_SAMPLES}, N_VAL_SAMPLES: {N_VAL_SAMPLES}')","60de7a8d":"# Create Train Directory\n!rm -rf train val\n!mkdir train val","e25f39d3":"train_tokens = np.empty(shape=(N_TRAIN_SAMPLES, SEQ_LENGTH), dtype=np.uint16)\n\n# === TRAIN ===\nfor idx, text_id in enumerate(tqdm(X_train_ids)):\n    \n    # Read Text File\n    with open(f'\/kaggle\/input\/feedback-prize-2021\/train\/{text_id}.txt', 'r') as f:\n        text = f.read()\n        \n    # Tokenize Text\n    text_encoded = tokenize(text)\n    \n    # Add to Train Tokens Array\n    train_tokens[idx] = text_encoded\n    \n# Save Train Tokens as Numpy Array\nnp.save('train\/train_tokens.npy', train_tokens)","a64daf37":"val_tokens = np.empty(shape=(N_VAL_SAMPLES, SEQ_LENGTH), dtype=np.uint16)\n\n# === VALIDATION ===\nfor idx, text_id in enumerate(tqdm(X_val_ids)):\n    \n    # Read Text File\n    with open(f'\/kaggle\/input\/feedback-prize-2021\/train\/{text_id}.txt', 'r') as f:\n        text = f.read()\n        \n    # Tokenize Text\n    text_encoded = tokenize(text)\n    \n    # Add to Val Tokens Array\n    val_tokens[idx] = text_encoded\n    \n# Save Val Tokens as Numpy Array\nnp.save('val\/val_tokens.npy', val_tokens)","e369e921":"# Text Id to Token Length Mapping\nID2TOKEN_LEN = train[['id', 'token_len']].set_index('id').squeeze().to_dict()","b18866a6":"# Get all labels sorted for reproducibility\nLABELS = train['discourse_type'].unique().sort_values().tolist()\n# Add extra non-annotated and padding label\nN_LABELS = len(LABELS) + 2\n# Not Annotated Class\nNA_CLASS = len(LABELS)\n# Padding Class\nPAD_CLASS = len(LABELS) + 1\n\nprint(f'N_LABELS: {N_LABELS}, NA_CLASS: {NA_CLASS}, PAD_CLASS: {PAD_CLASS}')\nprint(f'LABELS: {LABELS}')","845f5436":"# Text Id to Label\ndef id2label(text_id):\n    group = train[train['id'] == text_id]\n    \n    labels = np.full(fill_value=NA_CLASS, shape=SEQ_LENGTH, dtype=np.int8)\n    # Set with set indices\n    idxs_set = set()\n    \n    # Set Labels\n    for _, row in group.iterrows():\n        # Discourse Type\n        discourse_type = row['discourse_type']\n        # Discourse Label\n        discourse_type_int = LABELS.index(discourse_type)\n        idxs = np.array(row['predictionstring'].split(' '), dtype=np.int16)\n        # filter on indices that are already set\n        idxs = idxs[[e not in idxs_set for e in idxs]]\n        # Set Discourse Labels to 1\n        labels[idxs] = discourse_type_int\n        # Update Indices Seen\n        idxs_set.update(idxs)\n        \n    # Set Padding Class\n    token_len = ID2TOKEN_LEN[text_id]\n    labels[token_len:] = PAD_CLASS\n        \n    return labels","bd17e1ca":"# === TRAIN ===\ntrain_labels = np.zeros(shape=(N_TRAIN_SAMPLES, SEQ_LENGTH), dtype=np.int8)\nprint(f'train_labels shape: {train_labels.shape}')\n\n# Generate Labels\nfor idx, text_id in enumerate(tqdm(X_train_ids)):\n    train_labels[idx] = id2label(text_id)\n    \n# Save Train Labels as Numpy Array\nnp.save('train\/train_labels.npy', train_labels)","7f758c55":"# === VALIDATION ===\nval_labels = np.zeros(shape=(N_VAL_SAMPLES, SEQ_LENGTH), dtype=np.int8)\nprint(f'val_labels shape: {val_labels.shape}')\n\n# Generate Labels\nfor idx, text_id in enumerate(tqdm(X_val_ids)):\n    val_labels[idx] = id2label(text_id)\n    \n# Save Val Labels as Numpy Array\nnp.save('val\/val_labels.npy', val_labels)","291b8a6b":"# === TRAIN ===\ntrain_attention_masks = (train_labels != PAD_CLASS).astype(np.int8)\nprint(f'train_attention_masks shape: {train_attention_masks.shape}')\n    \n# Save as Numpy Array\nnp.save('train\/train_attention_masks.npy', train_attention_masks)","e37d0dae":"# # === VALIDATION ===\nval_attention_masks = (val_labels != PAD_CLASS).astype(np.int8)\nprint(f'val_attention_masks shape: {val_attention_masks.shape}')\n\n# Save as Numpy Array\nnp.save('val\/val_attention_masks.npy', val_attention_masks)","bba91bc2":"# Annotation Count","cbaf23d6":"# Attention Mask ","7a872d65":"# Oversample Statistics","f1adfc0d":"# Annotation Ratio\n\nMost texts have an annotation ratio, which is the ratio of words which are annotated in a text, close to 100%. There are however a handful of texts which have less than 70% of words annotated which will be filtered out.","1c671401":"# Token Length","6075853e":"# Filter Samples","3459fbb5":"# Word and Sentence Count","ce5023c5":"# Train Tokens","3de6abeb":"# Read Train DataFrame","2b95d3e8":"# Longformer Tokenizer","95f70fe1":"# Validation Tokens","05f6a5b8":"# Train\/Validation Split","ddfa414f":"Hello Fellow Kagglers,\n\nThis notebook demonstrates the preprocessing of the data for the Feedback Price competition by tokenizing the excerpts and oversampling the minority classes.\n\nThis oversampling process reduces the class inbalance and should make the model less biased towards the majority class.\n\n[Training Notebook](https:\/\/www.kaggle.com\/markwijkhuizen\/training-longformer-gradient-accumulation)\n\nInference Notebook Coming Soon","70d730e3":"# Discourse Class Distribution\n\nThe discourse types are unbalances, which occurances the range of 2.7% to 33.2%.","c5ced8f9":"# Oversampling","31594087":"# Create Tokens","5e62045d":"# Labels","4a768698":"When looking at the number of annotated words per discourse type the inbalance becomes even more severe, with occurances ranging from 1.7% to 57.5%."}}