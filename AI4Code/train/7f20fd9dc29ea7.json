{"cell_type":{"701c2743":"code","16f7ed1c":"code","0e675e1f":"code","6fde52ee":"code","3aaf179a":"code","421e10d9":"code","134b8539":"code","b4664c19":"code","4dd36cd8":"code","bab5639b":"code","a282470d":"code","cd803868":"code","91b4c163":"code","c5ef8ed2":"code","3ced0af2":"code","792ce06b":"code","5d2e33b4":"code","d506e32b":"code","7a8123fc":"code","310482d9":"code","71662636":"code","0f213616":"code","f1e537e7":"code","3be70c3d":"code","78c8f1ae":"code","5dbc45f6":"code","dc8c3ef1":"code","791f8d0b":"code","a32d2c05":"code","c53f0df3":"code","9d12d130":"code","c50ee6a2":"code","0544aa93":"code","4c4a4b9f":"code","2962a360":"code","9ef8a69f":"code","27af3a5c":"code","46d38875":"markdown","14ca27e6":"markdown","1a6c760d":"markdown","31206bc0":"markdown","e9596997":"markdown","b9a36183":"markdown","a585e05c":"markdown","c3193046":"markdown","4da243ad":"markdown","1a7f9b92":"markdown","705561b6":"markdown","f29e42db":"markdown","4bdcef43":"markdown","8b37b913":"markdown","1c412e51":"markdown","942db42b":"markdown","d42acd6e":"markdown","4170e2aa":"markdown","b9bdcdaf":"markdown","cfc0ca22":"markdown","c5373550":"markdown","079c7fb2":"markdown","b3f9c51e":"markdown","70783479":"markdown","5cea7ea9":"markdown","3f8d48d1":"markdown","5cc5108a":"markdown","4eb7744a":"markdown","13e40a24":"markdown","7ac3d615":"markdown"},"source":{"701c2743":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import f1_score, confusion_matrix\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# settings\nsns.set_style('whitegrid')\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.max_columns', None)","16f7ed1c":"# Below function implements above steps.\ndef run_kfold(model, X_train, y_train, N_SPLITS = 10):\n    f1_list = []\n    oofs = np.zeros(len(X_train))\n    folds = StratifiedKFold(n_splits=N_SPLITS)\n    for i, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n        \n        print(f'\\n------------- Fold {i + 1} -------------')\n        X_trn, y_trn = X_train.iloc[trn_idx], y_train.iloc[trn_idx]\n        X_val, y_val = X_train.iloc[val_idx], y_train.iloc[val_idx]\n        \n        model.fit(X_trn, y_trn)\n        # Instead of directly predicting the classes we will obtain the probability of positive class.\n        preds_val = model.predict_proba(X_val)[:,1]\n        \n        fold_f1 = f1_score(y_val, preds_val.round())\n        f1_list.append(fold_f1)\n        \n        print(f'\\nf1 score for validation set is {fold_f1}') \n        \n        oofs[val_idx] = preds_val\n        \n    mean_f1 = sum(f1_list)\/N_SPLITS\n    print(\"\\nMean validation f1 score :\", mean_f1)\n    \n    oofs_score = f1_score(y_train, oofs.round())\n    print(f'\\nF1 score for oofs is {oofs_score}')\n    return oofs","0e675e1f":"# Load data into memory\ndata = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\n\nprint(\"No of columns in the data : \", len(data.columns))\nprint(\"No of rows in the data : \", len(data))","6fde52ee":"# random sample of data\ndata.sample(5)","3aaf179a":"# statistical summary of the data\ndata.describe()","421e10d9":"# null values\ndata.isna().sum().to_frame(name=\"Null count\")","134b8539":"# features\nfeatures = ['gender', 'age', 'hypertension', 'heart_disease',\n            'ever_married','work_type','Residence_type','avg_glucose_level',\n            'bmi','smoking_status']\n\n#target\ntarget = 'stroke'\n\nnumerical_features = ['age', 'avg_glucose_level', 'bmi']\n\ncategorical_features = ['gender', 'hypertension', 'heart_disease',\n                        'ever_married', 'work_type', 'Residence_type', \n                        'smoking_status']","b4664c19":"# Converting features into required datatypes\ndata[numerical_features] = data[numerical_features].astype(np.float64)\n\ndata[categorical_features] = data[categorical_features].astype('category')\n\n# Replace Other label in gender with Female\ndata.gender.replace({'Other':\"Female\"}, inplace=True)\n\n# Remove id column\ndata.drop('id', axis=1, inplace=True)","4dd36cd8":"# data types\ndata[features+[target]].dtypes.to_frame(name=\"Data type\")","bab5639b":"train, test = train_test_split(data, random_state=1,\n                               test_size=0.25,\n                               stratify=data.stroke)\n\nprint(\"No. of data points in training set : \", len(train))\nprint(\"No. of data points in testing set : \", len(test))","a282470d":"imputer = KNNImputer(n_neighbors = 5)\n\ntrain[numerical_features] = imputer.fit_transform(train[numerical_features])\ntest[numerical_features] = imputer.transform(test[numerical_features])","cd803868":"fig, axes = plt.subplots(ncols=2,figsize=(12, 4))\ntrain[target].value_counts(normalize=True).plot \\\n.bar(width=0.2, color=('red','green'), ax=axes[0], title=\"Train\")\n\ntest[target].value_counts(normalize=True).plot \\\n.bar(width=0.2, color=('red','green'), ax=axes[1], title=\"Test\")\n\nplt.tight_layout()\nplt.show()","91b4c163":"fig, axes = plt.subplots(nrows=3,figsize=(8, 8))\nfor i, c in enumerate(numerical_features):\n    hist = train[c].plot(kind = 'hist', ax=axes[i], \n                         title=c, color='blue', bins=30)\nplt.tight_layout()\nplt.show()","c5ef8ed2":"fig, axes = plt.subplots(nrows=3, figsize=(8, 7))\nfor i, c in enumerate(numerical_features):\n    box = train[c].plot(kind = 'box', ax=axes[i],\n                        vert=False, color='blue')\nplt.tight_layout()\nplt.show()","3ced0af2":"fig, axes = plt.subplots(nrows=3, figsize=(8, 7))\nfor i, c in enumerate(numerical_features):\n    plot = sns.kdeplot(data=train, x=c, ax=axes[i],\n                       fill=True, color='blue')\nplt.tight_layout()\nplt.show()","792ce06b":"fig, axes = plt.subplots(4, 2, figsize=(12,16))\naxes = [ax for axes_row in axes for ax in axes_row]\n\nfor i,c in enumerate(categorical_features):\n    train[c].value_counts() \\\n    .plot(kind='pie', ax=axes[i], title=c, autopct=\"%.2f\", fontsize=14)\n    axes[i].set_ylabel('')\nplt.tight_layout()\nplt.show()","5d2e33b4":"fig, axes = plt.subplots(nrows=3, figsize=(8, 8))\nfor i, c in enumerate(numerical_features): \n    plot = sns.boxplot(x=train[target], y=train[c], ax=axes[i])\n    axes[i].set_ylabel(c, fontsize=13)\n    axes[i].set_xlabel(target, fontsize=13)\nplt.tight_layout()\nplt.show()","d506e32b":"fig, axes = plt.subplots(ncols=3, figsize=(20, 5))\nfor i, c in enumerate(numerical_features):\n    train.groupby(target)[c].mean().plot(kind = 'bar', ax=axes[i], color=('red','green'))\n    axes[i].set_ylabel(f'Mean_{c}', fontsize=14)\n    axes[i].set_xlabel('stroke', fontsize=14)\nplt.tight_layout()","7a8123fc":"fig, axes = plt.subplots(2, 4, figsize=(20,10))\naxes = [ax for axes_row in axes for ax in axes_row]\n\nfor i, c in enumerate(categorical_features):\n    df = train[[c,target]].groupby(c).mean().reset_index()\n    sns.barplot(df[c], df[target], ax=axes[i])\n    axes[i].set_ylabel('Target mean', fontsize=14)\n    axes[i].set_xlabel(c, fontsize=14)\n    \nplt.tight_layout()\nplt.show()","310482d9":"def age_group(x):\n    if x<13: return \"Child\"\n    elif 13<x<20: return \"Teenager\"\n    elif 20<x<=60: return \"Adult\"\n    else: return \"Elder\"\n    \ntrain[\"age_group\"] = train.age.apply(age_group)\ntest['age_group'] = test.age.apply(age_group)\n\ndef bmi_group(x):\n    if x<18.5 : return \"UnderWeight\"\n    elif 18.5<x<25: return \"Healthy\"\n    elif 25<x<30: return \"OverWeight\"\n    else: return \"Obese\"\n\ntrain[\"bmi_group\"] = train.bmi.apply(bmi_group)\ntest['bmi_group'] = test.bmi.apply(bmi_group)","71662636":"# add new features\ncategorical_features.extend([\"age_group\", \"bmi_group\"])\n\nencoder = OneHotEncoder(drop='first', sparse=False)\nencoder.fit(train[categorical_features])\n\ncols = encoder.get_feature_names(categorical_features)\n\ntrain.loc[:, cols] = encoder.transform(train[categorical_features])\ntest.loc[:, cols] = encoder.transform(test[categorical_features])\n\n# Drop categorical features\ntrain.drop(categorical_features, axis=1, inplace=True)\ntest.drop(categorical_features, axis=1, inplace=True)","0f213616":"scaler = StandardScaler()\nscaler.fit(train[numerical_features])\n\ntrain.loc[:, numerical_features] = scaler.transform(train[numerical_features])\ntest.loc[:, numerical_features] = scaler.transform(test[numerical_features])","f1e537e7":"# Correlation with Target\n\ncorr = train.corr()[target].sort_values(ascending=False).to_frame()\nplt.figure(figsize=(2,8))\nsns.heatmap(corr, cmap='Blues', cbar=False, annot=True)\nplt.show()","3be70c3d":"train.head()","78c8f1ae":"# Inputs and Target \nX_train = train.drop(target, axis=1)\ny_train = train[target]\n\nX_test = test.drop(target, axis=1)\ny_test = test[target]","5dbc45f6":"# Base model\nclf = DecisionTreeClassifier(random_state=1)\nclf.fit(X_train, y_train)\ntrain_preds = clf.predict(X_train)\ntest_preds = clf.predict(X_test)\nprint(\"Train f1 Score :\", f1_score(y_train, train_preds))\nprint(\"Test f1 Score :\", f1_score(y_test, test_preds))","dc8c3ef1":"# Hyperparameter tuning\nparams = {\n    'max_depth': [4, 6, 8, 10, 12, 14, 16, 20],\n    'criterion': ['gini', 'entropy'],\n    'min_samples_split': [5, 10, 20, 30, 40, 50],\n    'max_features': [0.2, 0.4, 0.6, 0.8, 1],\n    'max_leaf_nodes': [8, 16, 32, 64, 128,256],\n    'class_weight': [{0: 1, 1: 9}, {0: 1, 1: 4},\n                     {0: 1, 1: 5}, {0: 1, 1: 6}, \n                     {0: 1, 1: 7}, {0: 1, 1: 8}]\n}\n\nclf = RandomizedSearchCV(DecisionTreeClassifier(random_state=1),\n                         params,\n                         scoring='f1',\n                         verbose=1,\n                         random_state=1,\n                         cv=5,\n                         n_iter=50)\n\nsearch = clf.fit(X_train, y_train)\n\nprint(\"\\nBest f1-score:\",search.best_score_)\nprint(\"\\nBest params:\",search.best_params_)","791f8d0b":"# Cross validation\nclf = DecisionTreeClassifier(random_state = 1,\n                             **search.best_params_)\noofs = run_kfold(clf, X_train, y_train, N_SPLITS=5)","a32d2c05":"# Final Decision tree classifier\nclf = DecisionTreeClassifier(random_state = 1, \n                             **search.best_params_)\nclf.fit(X_train, y_train)\n\npreds_test = clf.predict_proba(X_test)[:, 1]\n    \ncm = confusion_matrix(y_test,preds_test.round(),normalize='true')\nplt.figure(figsize=(5,5))\nsns.heatmap(cm, annot=True, cmap='Blues', cbar=False,fmt='.2f')\nplt.show()","c53f0df3":"# Base model\nclf = LogisticRegression(random_state=1, \n                         class_weight='balanced')\n\nclf.fit(X_train, y_train)\ntrain_preds = clf.predict(X_train)\ntest_preds = clf.predict(X_test)\nprint(\"Train f1 Score :\", f1_score(y_train, train_preds))\nprint(\"Test f1 Score :\", f1_score(y_test, test_preds))","9d12d130":"# Hyperparameter tuning\nparams = {\n    'penalty': ['l1', 'l2','elasticnet'],\n    'C':[0.0001, 0.001, 0.1, 1, 10, 100,1000],\n    'fit_intercept':[True, False],\n    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n}\n\nclf = RandomizedSearchCV(LogisticRegression(random_state=1,\n                                            class_weight='balanced'),\n                         params,\n                         scoring='f1',\n                         verbose=1,\n                         random_state=1,\n                         cv=5,\n                         n_iter=20)\n\nsearch = clf.fit(X_train, y_train)\n\nprint(\"\\nBest f1-score:\",search.best_score_)\nprint(\"\\nBest params:\",search.best_params_)","c50ee6a2":"# Cross validation\nclf = LogisticRegression(random_state = 1,\n                         class_weight='balanced', \n                         **search.best_params_)\noofs = run_kfold(clf, X_train, y_train, N_SPLITS=5)","0544aa93":"# Final Logistic regression\n\nclf = LogisticRegression(random_state = 1,\n                         class_weight='balanced',\n                         **search.best_params_)\nclf.fit(X_train, y_train)\n\npreds_test = clf.predict_proba(X_test)[:, 1]\n\ncm = confusion_matrix(y_test, preds_test.round(), normalize='true')\nplt.figure(figsize=(5,5))\nsns.heatmap(cm, annot=True, cmap='Blues', cbar=False, fmt='.2f')\nplt.show()","4c4a4b9f":"imp = pd.DataFrame([X_train.columns, \n                    clf.coef_[0]]).T.sort_values(1, ascending=False).reset_index(drop=True)\nimp.columns=['feature', 'coeff']\nimp","2962a360":"with open(\"onehotencoder.pkl\", 'wb') as f:\n    pickle.dump(encoder, f)\n\nwith open(\"scaler.pkl\", 'wb') as f:\n    pickle.dump(scaler, f)\n\nwith open(\"model.pkl\", 'wb') as f:\n    pickle.dump(clf, f)","9ef8a69f":"def predict(x):\n    X = pd.DataFrame(x, columns=features)\n    # converting numerical features as float dtype\n    X.loc[:, numerical_features] = X.loc[:, numerical_features].astype('float64')\n    # add new features\n    X[\"age_group\"] = X.age.apply(age_group)\n    X[\"bmi_group\"] = X.age.apply(bmi_group)\n    \n    # converting categorical features as category dtype\n    X.loc[:, categorical_features] = X.loc[:, categorical_features].astype('category')\n    # Categorical encoding\n    cols = encoder.get_feature_names(categorical_features)\n\n    X.loc[:, cols] = encoder.transform(X[categorical_features])\n\n    # Drop categorical features\n    X.drop(categorical_features, axis=1, inplace=True)\n\n    # Feature scaling\n    X.loc[:, numerical_features] = scaler.transform(X[numerical_features])\n    return clf.predict(X)[0]","27af3a5c":"# Random data point\nx = [['Male', 67.0, 0, 1, 'Yes', 'Private', 'Urban', 228.69, 36.6, 'formerly smoked']]\ny_true = 1\nprint(\"y_true :\", y_true)\ny_pred = predict(x)\nprint(\"y_pred :\",y_pred)","46d38875":"#  Bivariate analysis\nIt involves the analysis of two variables, for the purpose of determining the empirical relationship between them.\n\nWe perform bivariate analysis of features with respect to target.\n\n### Box plots","14ca27e6":"### K-Fold Cross-Validation\nStep 1: Randomly divide a dataset into k groups, or \u201cfolds\u201d, of roughly equal size.<br>\nStep 2: Choose one of the folds to be the holdout set. Fit the model on the remaining k-1 folds.<br>\nStep 3: Calculate the test F1-score on the observations in the fold that was held out.<br>\nStep 4: Repeat this process k times, using a different set each time as the holdout set.<br>\nStep 5: Calculate the average of the k test F1-scores to get the overall test F1-score.","1a6c760d":"# Data preprocessing","31206bc0":"# Save all the transformers\nOne hot encoder<br>\nStandard scaler<br>\nLogistic regression<br>\n<b> Logistic regression is giving the high true positive rate i.e., performing better at predicting the likelihood of Stroke. Which is what we want.!! ","e9596997":"## Prediction on single data point","b9a36183":"## Logistic Regression","a585e05c":"### Boxplot (Outliers)\nAn outlier is a data point that differs significantly from other observations.","c3193046":"## Variable separation","4da243ad":"### Target vs Mean of Numerical features","1a7f9b92":"### KDE Plot","705561b6":"## Feature Scaling\nStandardize the numerical features","f29e42db":"## New Features with Age and bmi","4bdcef43":"## Weigths or Coefficents learnt by Logistic regression for each feature","8b37b913":"### Pie-Charts\nPercentage of labels in categorical features","1c412e51":"# Machine Learning\n## Decision tree classifier","942db42b":"## Preprocessed data","d42acd6e":"## OneHot encoding\n- Replaces categorical column(s) with the binary value for each category.","4170e2aa":"# Feature Engineering\nFeature engineering is the process of using domain knowledge to extract features from raw data. These features can be used to improve the performance of machine learning algorithms.","b9bdcdaf":"## About Dataset\nSource : https:\/\/www.kaggle.com\/fedesoriano\/stroke-prediction-dataset\n\n### Attribute Information\n<b>id<\/b>: unique identifier<br>\n<b>gender<\/b>: Male, Female or Other<br>\n<b>age<\/b>: age of the patient<br>\n<b>hypertension<\/b>: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension<br>\n<b>heart_disease<\/b>: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease<br>\n<b>ever_married<\/b>: No or Yes<br>\n<b>work_type<\/b>: children, Govt_jov, Never_worked, Private or Self-employed<br>\n<b>Residence_type<\/b>: Rural or Urban<br>\n<b>avg_glucose_level<\/b>: average glucose level in blood<br>\n<b>bmi<\/b>: body mass index<br>\n<b>smoking_status<\/b>: formerly smoked, never smoked, smokes or Unknown<br>\n<b>stroke<\/b>: 1 if the patient had a stroke or 0 if not (target)<br>","cfc0ca22":"<img src=\"img.jpg\" width=\"60%\">","c5373550":"# Exploratory Data Analysis\nExploratory data analysis (EDA) is used to analyze and investigate data sets and summarize their main characteristics, often employing data visualization methods.\n\nPerforming EDA on Training set only (best practice to avoid overfitting)\n## Univariate analysis\n- Univariate analysis refers to the analysis of one variable.\n- The purpose of univariate analysis is to understand the distribution of values for a single variable.\n\n### A. Target distribution","079c7fb2":"### Target vs categorical features","b3f9c51e":"## Train Test Split\n- Dividing the total dataset into training and testing sets\n- For Training 75% of data\n- For Testing 25% of data","70783479":"### B. Histogram","5cea7ea9":"# Libraries\nImporting all the necessary python modules","3f8d48d1":"# Problem statement\nStroke is sometimes termed as brain attack or a cardiovascular accident (CVA). It is much like a heart attack, only it occurs in the brain.<br>\n\nIt occurs when the supply of blood to the brain is reduced or blocked completely, which prevents brain tissue from getting oxygen and nutrients.<br>\n\nAccording to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.<br>\nEarly identification of stroke can help doctors to give necessary medication to the patient.\n\n\n## Machine Learning problem\nPredict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status.<br>\n<b>Type<\/b> : Supervised Learning<br>\n<b>Task<\/b> : Binary classification<br>\n<b>Performance metric<\/b> : F1 score (since imbalanced classes)<br>","5cc5108a":"<h1> <u>Stroke prediction<\/u><\/h1>","4eb7744a":" ## Correlation\n Correalation between features and target","13e40a24":"## Next steps\n\n#### Building a web application for this problem!! Updating soon.\ud83d\udc4d\n## Thanks for reading!! Please upvote if you like it.\ud83d\ude00","7ac3d615":"## Fill Missing values\nUsing K-nearest neighbors of numerical features to fill the missing values in bmi"}}