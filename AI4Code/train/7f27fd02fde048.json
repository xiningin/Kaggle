{"cell_type":{"23d85ed9":"code","51685185":"code","6eab13cf":"code","f270cfaa":"code","749f9415":"code","6acd1d06":"code","40187f81":"code","70422d65":"code","39c1478a":"code","567883e4":"code","f65be901":"code","173aa36d":"markdown","1fa7d9a9":"markdown","b3bd7d20":"markdown","bd70850a":"markdown","0ae8ff40":"markdown","f8cf9396":"markdown","bf748b80":"markdown","e519032c":"markdown"},"source":{"23d85ed9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot # visualization library\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","51685185":"dataset = pd.read_csv('..\/input\/Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\ndataset.head()","6eab13cf":"#cleaning the text\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\ncorpus = []\nfor i in range(0,1000):\n    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n    review = ' '.join(review)\n    corpus.append(review)","f270cfaa":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1500)\nX = cv.fit_transform(corpus).todense()\ny = dataset.iloc[:,1].values","749f9415":"#splitting the dataset into the training set and testset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X , y, test_size = 0.20, random_state = 0)","6acd1d06":"#fitting the naive bayes model to the training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)","40187f81":"#predicitng the Test set results\ny_pred = classifier.predict(X_test)","70422d65":"#creating the confusion matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nConfusion_Matrix = confusion_matrix(y_test, y_pred)\nAccuracy_Score = accuracy_score(y_test, y_pred)","39c1478a":"#visualizing the results\nimport seaborn as sn\nimport pandas as pd\nimport matplotlib.pyplot as plt\n      \ndf_cm = pd.DataFrame(Confusion_Matrix, range(2),\n                  range(2))\n#plt.figure(figsize = (10,7))\nsn.set(font_scale=1.4)#for label size\nsn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16})# font size\nprint(\"Accuracy Score is :\", Accuracy_Score)","567883e4":"feedback = \"\"\n\nnewReview = \"\"\n\nnewReview = \"The food was Amazing\"\n\ndef predict(new_review):   \n\n        new_review = re.sub(\"[^a-zA-Z]\", \" \", new_review)   \n\n        new_review = new_review.lower().split()\n\n        new_review = [ps.stem(word) for word in new_review if word not in set(stopwords.words(\"english\"))]   \n\n        new_review = \" \".join(new_review)   \n\n        new_review = [new_review]   \n\n        new_review = cv.transform(new_review).toarray()   \n\n        if classifier.predict(new_review)[0] == 1:\n\n            return \"Positive\"   \n\n        else:       \n\n            return \"Negative\"\n\n       \n\nfeedback = predict(newReview)\n\nprint(\"This review is: \", feedback) ","f65be901":"feedback = \"\"\n\nnewReview = \"\"\n\nnewReview = \"The food was terrible\"\n\ndef predict(new_review):   \n\n        new_review = re.sub(\"[^a-zA-Z]\", \" \", new_review)   \n\n        new_review = new_review.lower().split()\n\n        new_review = [ps.stem(word) for word in new_review if word not in set(stopwords.words(\"english\"))]   \n\n        new_review = \" \".join(new_review)   \n\n        new_review = [new_review]   \n\n        new_review = cv.transform(new_review).toarray()   \n\n        if classifier.predict(new_review)[0] == 1:\n\n            return \"Positive\"   \n\n        else:       \n\n            return \"Negative\"\n\n       \n\nfeedback = predict(newReview)\n\nprint(\"This review is: \", feedback) ","173aa36d":"\n# Challenge of Natural Language\n\nWorking with natural language data is not solved.\n\nIt has been studied for half a century, and it is really hard.\n\n*It is hard from the standpoint of the child, who must spend many years acquiring a language \u2026 it is hard for the adult language learner, it is hard for the scientist who attempts to model the relevant phenomena, and it is hard for the engineer who attempts to build systems that deal with natural language input or output. These tasks are so hard that Turing could rightly make fluent conversation in natural language the centerpiece of his test for intelligence.*\n\n\u2014 Page 248, Mathematical Linguistics, 2010.\n\nNatural language is primarily hard because it is messy. There are few rules.\n\nAnd yet we can easily understand each other most of the time.\n\n*Human language is highly ambiguous \u2026 It is also ever changing and evolving. People are great at producing language and understanding language, and are capable of expressing, perceiving, and interpreting very elaborate and nuanced meanings. At the same time, while we humans are great users of language, we are also very poor at formally understanding and describing the rules that govern language.*\n\n\u2014 Page 1, Neural Network Methods in Natural Language Processing, 2017.\n\n# From Linguistics to Natural Language Processing\n\n**Linguistics**\n\nLinguistics is the scientific study of language, including its grammar, semantics, and phonetics.\n\nClassical linguistics involved devising and evaluating rules of language. Great progress was made on formal methods for syntax and semantics, but for the most part, the interesting problems in natural language understanding resist clean mathematical formalisms.\n\nBroadly, a linguist is anyone who studies language, but perhaps more colloquially, a self-defining linguist may be more focused on being out in the field.\n\nMathematics is the tool of science. Mathematicians working on natural language may refer to their study as mathematical linguistics, focusing exclusively on the use of discrete mathematical formalisms and theory for natural language (e.g. formal languages and automata theory).\n\n**Computational Linguistics**\n\n*Computational linguistics is the modern study of linguistics using the tools of computer science. Yesterday\u2019s linguistics may be today\u2019s computational linguist as the use of computational tools and thinking has overtaken most fields of study.\n\nComputational linguistics is the study of computer systems for understanding and generating natural language. \u2026 One natural function for computational linguistics would be the testing of grammars proposed by theoretical linguists.*\n\n\u2014 Pages 4-5, Computational Linguistics: An Introduction, 1986.\n\nLarge data and fast computers mean that new and different things can be discovered from large datasets of text by writing and running software.\n\nIn the 1990s, statistical methods and statistical machine learning began to and eventually replaced the classical top-down rule-based approaches to language, primarily because of their better results, speed, and robustness. The statistical approach to studying natural language now dominates the field; it may define the field.\n\n*Data-Drive methods for natural language processing have now become so popular that they must be considered mainstream approaches to computational linguistics. \u2026 A strong contributing factor to this development is undoubtedly the increase amount of available electronically stored data to which these methods can be applied; another factor might be a certain disenchantment with approaches relying exclusively on hand-crafted rules, due to their observed brittleness.\n*\n\u2014 Page 358, The Oxford Handbook of Computational Linguistics, 2005.\n\nThe statistical approach to natural language is not limited to statistics per-se, but also to advanced inference methods like those used in applied machine learning.\n\n*\u2026 understanding natural language require large amounts of knowledge about morphology, syntax, semantics and pragmatics as well as general knowledge about the world. Acquiring and encoding all of this knowledge is one of the fundamental impediments to developing effective and robust language systems. Like the statistical methods \u2026 machine learning methods off the promise of automatic the acquisition of this knowledge from annotated or unannotated language corpora.*\n\n\u2014 Page 377, The Oxford Handbook of Computational Linguistics, 2005.\n\n*Statistical Natural Language Processing*\n\nComputational linguistics also became known by the name of natural language process, or NLP, to reflect the more engineer-based or empirical approach of the statistical methods.\n\nThe statistical dominance of the field also often leads to NLP being described as Statistical Natural Language Processing, perhaps to distance it from the classical computational linguistics methods.\n\n*I view computational linguistics as having both a scientific and an engineering side. The engineering side of computational linguistics, often called natural language processing (NLP), is largely concerned with building computational tools that do useful things with language, e.g., machine translation, summarization, question-answering, etc. Like any engineering discipline, natural language processing draws on a variety of different scientific disciplines.\n*\n\u2014 How the statistical revolution changes (computational) linguistics, 2009.\n\nLinguistics is a large topic of study, and, although the statistical approach to NLP has shown great success in some areas, there is still room and great benefit from the classical top-down methods.\n\n*Roughly speaking, statistical NLP associates probabilities with the alternatives encountered in the course of analyzing an utterance or a text and accepts the most probable outcome as the correct one. \u2026 Not surprisingly, words that name phenomena that are closely related in the world, or our perception of it, frequently occur close to one another so that crisp facts about the world are reflected in somewhat fuzzier facts about texts. There is much room for debate in this view.\n*\n\u2014 Page xix, The Oxford Handbook of Computational Linguistics, 2005.\n","1fa7d9a9":"**Bag of words is created using the CountVectorizer which converts the words in the dataset into 0 and 1**","b3bd7d20":"# **To Predict New Review:** (for Negative review)","bd70850a":"# Introduction \n\nLet\u2019s make this simpler with an example.\n\nHave you ever tried to communicate with someone who didn\u2019t speak your language, and they couldn\u2019t understand you? The classic example of this is when someone goes out to a restaurant in a Foreign country and they think they ordered steak, but when the food shows up, it turns out they actually asked for liver stew.\n\nThis is the kind of relationship that most of us have with our own unconscious mind. We might think we are \u201cordering up\u201d more money, a happy, healthy relationship, peace with our family members, and being able to stick to a healthy diet\u2026but unless that\u2019s what showing up, then something is probably getting lost in translation. In NLP, we have a saying: the conscious mind is the goal setter, and the unconscious mind is the goal getter. Your unconscious mind is not out to get you\u2013rather, it\u2019s out TO GET FOR YOU whatever you want in life. However, if you don\u2019t know how to communicate what you want properly, it will keep bringing steaming bowls of liver stew out of the kitchen.\n\nIn fact, go ahead right now and think of, if there was one thing you could change, one habit you could break, what would it be?\n\nWould you remain calm during work presentations?\nQuit procrastinating and spending so much time on Facebook?\nNot devour a whole bag of potato chips or tub of ice cream in one sitting?\nWhatever it is, realize that your unconscious mind only does that because it thinks that\u2019s what you want. (\u201cSir, here is your procrastination along with a side of anxiety. I\u2019ve also told the valet to bring up your emotional baggage as per your request. Will you be needing anything else?\u201d)\n\nNeuro-Linguistic Programming is like a user\u2019s manual for the brain, and taking an NLP training is like learning how to become fluent in the language of your mind so that the ever-so-helpful \u201cserver\u201d that is your unconscious will finally understand what you actually want out of life.\n\nNLP is the study of excellent communication\u2013both with yourself, and with others. It was developed by modeling excellent communicators and therapists who got results with their clients. NLP is a set of tools and techniques, but it is so much more than that. It is an attitude and a methodology of knowing how to achieve your goals and get results.\n\nHere at NLP.com, presented by The Empowerment Partnership, our mission, put simply is to empower the planet. How does teaching NLP help us do that? Because after being in business for 30 years, we\u2019ve have spent that time gathering all of the best concepts and techniques to help you get in control of your mind, your emotional state, and your life.\n\nThat is what NLP is.  \u2014*(nlp.com)*","0ae8ff40":"**The Dataset contains two colums Review and Liked. The value of the liked column in 1 if the review is positive and 0 if negative.**","f8cf9396":"**To clean the dataset, all the stopwords like is,was, the which are not relevant should be removed.\nStemming is applied to the dataset. Stemming is applied to generalize the words in the reviews which makes an efficient dataset.**","bf748b80":"# **To Predict new Review:** (for Positive review)","e519032c":"**Any calssifiction can be used for Natural Language Processing. But For this dataset Naive Bayes Classification Model yieds better accuracy than other models.**"}}