{"cell_type":{"663baf52":"code","2b6454a1":"code","f75b87fc":"code","b72fe407":"code","68e82a2d":"code","b655ba8c":"code","15eeb6df":"code","338b6a6d":"code","78675291":"code","45d9f602":"code","2a113c31":"code","d9a78918":"code","10230060":"code","5a4c4ebe":"code","43726baa":"code","22823106":"code","af3321e9":"code","702a08c2":"code","d8fcf00f":"code","039fc5aa":"code","210b2261":"code","ff4870f1":"code","d738be22":"code","bdce508f":"code","71695523":"code","30d08790":"code","f8526010":"code","1fd6d4e9":"code","d96b0291":"code","6c133af4":"markdown","a91b0b7e":"markdown","4040ba11":"markdown","715e0179":"markdown","8117362f":"markdown","afdc98bb":"markdown","4489dcce":"markdown","4036d958":"markdown","6de4d943":"markdown","76fb5e9e":"markdown","dd1f7d4f":"markdown","76be5c28":"markdown","5fefe5cf":"markdown","fc0b9783":"markdown","26df19ad":"markdown","93f8ec6a":"markdown","eae5ac25":"markdown","5e27778c":"markdown","c4dfdfb4":"markdown","a8706d36":"markdown","0669b69a":"markdown","f4967031":"markdown","880b044d":"markdown","14acee98":"markdown","08ad9cce":"markdown"},"source":{"663baf52":"# installing pyspark\n!pip install pyspark","2b6454a1":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","f75b87fc":"from pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n# Creating spark session containing spark context\n#sc = SparkContext(appName = \"Santander_Customer_Satisfaction\")\nspark = SparkSession.Builder().getOrCreate()","b72fe407":"# Loading data\n\n\ntrain = pd.read_csv('..\/input\/santander-customer-satisfaction\/train.csv')\n\ntest = pd.read_csv('..\/input\/santander-customer-satisfaction\/test.csv')\n","68e82a2d":"# Loading data to spark session\ntrain_spark = spark.read.format(\"csv\").option(\"header\", \"true\").load('..\/input\/santander-customer-satisfaction\/train.csv')\ntest_spark = spark.read.format(\"csv\").option(\"header\", \"true\").load('..\/input\/santander-customer-satisfaction\/test.csv')","b655ba8c":"# displaying first 5 rows\ntrain_spark.toPandas().head(5)","15eeb6df":"# Looking at the distribution of the target column \ntotal_datapoints = len(train[\"TARGET\"])\nprint(train[\"TARGET\"].value_counts()\/total_datapoints)\nplt.figure(figsize=(6,4))\nsns.barplot(y=train[\"TARGET\"].value_counts()\/total_datapoints, x=[\"0\",\"1\"])\nsns.despine()\nplt.suptitle(\"Distribution of TARGET column in percentage\",fontsize=18)\nplt.title(\"blue=0 (satisfied), orange=1 (unsatisfied)\")\n\n# 0 = Satisfied, 1= unsatisfied\n# Alot more satisfied customers, about 96%","338b6a6d":"# Getting stats on each column\ntrain_spark.describe().toPandas()","78675291":"train.var3.value_counts()","45d9f602":"# Checking distribution of rows with feature \"var3\" = -999999\nplt.figure(figsize=(6,4))\nsns.barplot(y=train.loc[train.var3 == -999999].TARGET.value_counts()\/len(train.loc[train.var3 == -999999]),x=[\"0\",\"1\"])\nsns.despine()\nplt.suptitle(\"Distribution for var3=-999999\")\nplt.title(\"count of TARGET=0 (blue) and TARGET=1 (orange)\")","2a113c31":"print(\"Checking for nan-values:\")\nprint(train.isnull().values.any())","d9a78918":"# Assuming ID is not correlated with customer satisfaction\n#train = train.drop([\"ID\"], axis=1)","10230060":"# Assuming ID is not correlated with customer satisfaction so i drop it\ntrain_spark_drop_id = train_spark.drop('ID')\n#train_spark_drop_id.toPandas()","5a4c4ebe":"# Creating one data frame for each class\ntrain_spark_target_0 = train_spark_drop_id.filter(\"TARGET=0\")\ntrain_spark_target_1 = train_spark_drop_id.filter(\"TARGET=1\")\n\n# Counting the number of samples for each of them\nnum_target_0 = train_spark_target_0.count()\nnum_target_1 = train_spark_target_1.count()\n\n\n# Downsampling the dataset of TARGET=0 to same about amount of rows as TARGET=1\n# OBS. This function does not sample exact amount, subject for improvement\ntrain_spark_target_0_under = train_spark_target_0.sample(True, num_target_1\/num_target_0)\n\n# Concatenating the undersampled with TARGET=0 and the ordinary TARGET=1\ntrain_under_spark = train_spark_target_0_under.union(train_spark_target_1)\n\nprint(\"Precentage of each class after under sampling\")\nprint(train_under_spark.toPandas()[\"TARGET\"].value_counts()\/train_under_spark.count())\n\n","43726baa":"#count_class_0, count_class_1 = train.TARGET.value_counts()\n#train_target_0 = train[train['TARGET'] == 0]\n\n#train_target_1 = train[train['TARGET'] == 1]\n\n#train_target_0_under = train_target_0.sample(count_class_1)\n#train_under =  pd.concat([train_target_0_under, train_target_1], axis=0, ignore_index=True)\n\n#total_datapoints_under = len(train_under[\"TARGET\"])\n#print(\"Precentage of each class after under sampling\")\n#print(train_under[\"TARGET\"].value_counts()\/total_datapoints_under)\n#train_under","22823106":"# Calculating the amount of unique values for each column\n## This could maybe be solved in a better way than to cast as panda. Casting seems quite heavy\nconstant_in_train_spark= train_under_spark.toPandas().apply(lambda x: x.nunique(), axis=0)\n\n# Extracting the list of the columns with only one unique value\nc_train_ind_spark = list(constant_in_train_spark[constant_in_train_spark == 1].index.values)\n\n# Dropping all the columns \n## below '*'' is used to sen the list as arguments to drop since it could not take a list as input\ntrain_drop_1_spark = train_under_spark.drop(*c_train_ind_spark)\nprint('Number of cols dropped: ', len(c_train_ind_spark))\nprint(c_train_ind_spark)\n\n","af3321e9":"# checking if some column is constant\n# axis=0 : applies to all columns\n#constant_in_train = train_under.apply(lambda x: x.nunique(), axis=0)\n#c_train_ind = constant_in_train[constant_in_train == 1].index.values\n#print('Number of cols dropped: ', len(c_train_ind)))\n#print(list(c_train_ind))\n\n#train_drop_1 = train_under\n\n# Dropping constant columns\n#train_drop_1.drop(list(c_train_ind), axis=1)\n\n","702a08c2":"# Spark\n#Calculating the correlation matrix . From https:\/\/stackoverflow.com\/questions\/51831874\/how-to-get-correlation-matrix-values-pyspark\/51834729\nfrom pyspark.mllib.stat import Statistics\n\n# copy df except TARGET\ndf_corr = train_drop_1_spark.drop(\"TARGET\")\n# copying columns\ncol_names = df_corr.columns\n# Creatting an rdd of all features\nfeatures = df_corr.rdd.map(lambda row: row[0:])\n#Calculating correlation\ncorr_mat=Statistics.corr(features, method=\"pearson\")\n# Creating a data frame from result\ncorr_matrix_spark = pd.DataFrame(corr_mat)\n# Setting column names of datafram\ncorr_matrix_spark.index, corr_matrix_spark.columns = col_names, col_names\n\ncorr_matrix_spark","d8fcf00f":"# Calculating correlation matrix for all features\n#corr_matrix = train_drop_1.corr()\n#corr_matrix","039fc5aa":"cols_to_remove_spark = []\n\n# Looping through \nfor col in range(len(corr_matrix_spark.columns)):\n    for row in range(col):\n        if (corr_matrix_spark.iloc[row,col] >0.5 \\\n            or corr_matrix_spark.iloc[row,col] < -0.5) \\\n            and (corr_matrix_spark.columns[row] not in cols_to_remove_spark):\n                \n            cols_to_remove_spark.append(corr_matrix_spark.columns[col])\n\ntrain_drop_2_spark = train_drop_1_spark.drop(*cols_to_remove_spark)\n\nprint(\"Columns removed:\")\nprint(len(cols_to_remove_spark))\n","210b2261":"#cols_to_remove = []\n#for col in range(len(corr_matrix.columns)):\n#    for row in range(col):\n#        if (corr_matrix.iloc[row,col] >0.9 or corr_matrix.iloc[row,col] < -0.9) and (corr_matrix.columns[row] not in cols_to_remove):\n#            cols_to_remove.append(corr_matrix.columns[col])\n\n#train_drop_2 = train_drop_1.drop(cols_to_remove, axis=1)\n\n#print(\"Columns removed:\")\n#print(len(cols_to_remove))\n","ff4870f1":"train_no_target_cols_spark = train_drop_2_spark.columns[0:-1]\ntest_ids = test_spark.toPandas()[\"ID\"].values\ntest_remove_spark = test_spark.select(*train_no_target_cols_spark)\n#test_remove_spark.toPandas()","d738be22":"#Removing same columns from test\n\n#train_no_target_cols = train_drop_2.columns[0:-1]\n\n#test_remove = test[train_no_target_cols]\n#test_remove","bdce508f":"# Creating RDD from panda\n#from pyspark.mllib.regression import LabeledPoint\n\n#training set\n#s_df_train = spark.createDataFrame(train_drop_2.sample(300))\n#RDD_train = s_df_train.rdd.map(lambda x: LabeledPoint(x[\"TARGET\"], x[:-1]))\n\n\n#test set\n#s_df_test = spark.createDataFrame(test_remove)\n#RDD_test = s_df_test.rdd.map(lambda x: x[:])","71695523":"from pyspark.mllib.regression import LabeledPoint\n\nRDD_train_spark = train_drop_2_spark.rdd.map(lambda x: LabeledPoint(x[\"TARGET\"], x[:-1]))\nRDD_test_spark = test_remove_spark.rdd.map(lambda x: x[:])","30d08790":"from pyspark.mllib.tree import RandomForest, RandomForestModel\nfrom pyspark.mllib.util import MLUtils\n\n# Creating random forest model\nmodel = RandomForest.trainClassifier(RDD_train_spark, numClasses=2, categoricalFeaturesInfo={},\n                                     numTrees=200, featureSubsetStrategy=\"auto\",\n                                     impurity='gini', maxDepth=15, maxBins=32, seed=12345)\n#print(model.toDebugString())","f8526010":"# Predicting test values\npredictions = model.predict(RDD_test_spark).collect()\nprint(predictions[0:100])","1fd6d4e9":"# Creating a datafram to submit results on test set\nsubmission_df = pd.DataFrame(columns=[\"ID\", \"TARGET\"])\nsubmission_df[\"TARGET\"] = predictions\nsubmission_df[\"ID\"] = test_ids\nsubmission_df","d96b0291":"# WRiting results to csv-files\nsubmission_df.to_csv('santandersubmission_corma_test.cvs', index=False)","6c133af4":"# Loading spark context","a91b0b7e":"#### Python","4040ba11":"The var3=-999999 seems to have a different distribution than the overall dataset\nwhen looking at the TARGET variable -> may contain information. So we keep those rows.\nOBS, subject of improvement, should be investigated further.\n","715e0179":"# Data exploration","8117362f":"# Transforming from spark DF to RDD","afdc98bb":"# Undersampling of data due to imbalanced target distribution","4489dcce":"The dataset is heavily skew towards satisfied customers.","4036d958":"Min value: -999999 for var3 as value seems a bit wierd!","6de4d943":"#### Spark","76fb5e9e":"# Data cleaning\n## Removing constant columns","dd1f7d4f":"## Removing same columns for test data","76be5c28":"#### Spark","5fefe5cf":"#### Spark","fc0b9783":"# Reading data from files","26df19ad":"## Dropping all features with correlation higher than 0.9","93f8ec6a":"#### Python","eae5ac25":"#### Spark mixed with some python","5e27778c":"#### Spark","c4dfdfb4":"#### Python","a8706d36":"# Final Score on test data : 0.75 ","0669b69a":"# ML-part Random forest","f4967031":"#### Spark","880b044d":"## Checking for highly correlated features","14acee98":"#### Python","08ad9cce":"#### Python"}}