{"cell_type":{"8f694459":"code","8241d4a2":"code","5ae658db":"code","a2abd2ec":"code","ab028a7f":"code","4dc200fa":"code","fecf1947":"code","7c08e7e9":"code","d39f6bc1":"code","fdb1ea37":"code","1a14170a":"code","d155589d":"code","72931e94":"code","33473df7":"code","c603f4d6":"code","5b1ad5b1":"code","d4a8ac6e":"code","c5ca757a":"code","a745f383":"code","cb7b1c9b":"code","4a052b5d":"code","f4e46d9d":"code","ea2fb953":"code","7fc5b10c":"code","dd3cf84a":"code","00eea892":"code","f63eafa5":"code","04088be7":"code","f7cf8b04":"code","a315a044":"code","015d2a8d":"code","47eb13fb":"code","31af5fae":"code","57626966":"code","ad169b0c":"code","d0b78fc7":"code","36d22251":"code","efe4e1aa":"code","e0a19eb6":"code","d7112e1f":"code","76e8d70e":"code","41408f3c":"code","a5d97d5b":"code","d15e3c4a":"markdown","a115e9d5":"markdown","156b45d6":"markdown","4726e37e":"markdown","5a6a23f3":"markdown","34d16ba5":"markdown","2d0018aa":"markdown","5a12372c":"markdown","bc2dd834":"markdown","080d4acd":"markdown","561942a8":"markdown"},"source":{"8f694459":"import requests\nimport numpy as np\nimport pandas as pd\nimport csv\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.metrics import accuracy_score\nfrom bs4 import BeautifulSoup\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom yellowbrick.text import FreqDistVisualizer\nfrom sklearn.model_selection import train_test_split,cross_validate\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.model_selection import cross_val_score\nimport seaborn as sns\nimport xgboost\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","8241d4a2":"tweets = pd.read_csv(r'..\/input\/market-volatility\/TrumpTweets.csv') #MB removed dash in market-volatility\ntweets['Date']  = pd.to_datetime(tweets['created_at']).dt.date\ntweets.head()","5ae658db":"REPLACE_BY_SPACE_RE = re.compile('[\/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\nSTOPWORDS.add('tco') #additional manual word removal\nSTOPWORDS.add('https') #additional manual word removal\n\ndef clean_text(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n    \"\"\"\n    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n    text = text.lower() # lowercase text\n    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n    return text\n\ntweets['text'] = tweets['text'].apply(clean_text)\n\n#Source: https:\/\/stackoverflow.com\/questions\/58603042\/easier-way-to-clean-up-text","a2abd2ec":"trade_words = r'\\A(trade|xi|tariff|billion|vietnam|war|deal|talk|dollar|export|farm|agri\\w*|producer|duty|global|china|chinese|market|deal|soy|economy)'\ntrade_tweets = tweets[tweets['text'].str.contains(trade_words)]\nprint('total of {} trade-related tweets'.format(trade_tweets.shape[0]))\ntrade_tweets.head(5)","ab028a7f":"from textblob import TextBlob \ntrade_tweets.loc[:, 'sentiment'] = trade_tweets['text'].apply(lambda tweet: TextBlob(tweet).sentiment.polarity)\ndaily_tweets = trade_tweets[['Date', 'sentiment']].groupby(['Date'], as_index=False).mean()","4dc200fa":"options_df = pd                       \\\n    .read_csv(r'\/kaggle\/input\/market-volatility\/options.csv') \\\n    .query(\"commodity == 'soybeans'\") \\\n    .query(\"ticker == 'CZO'\")         \\\n    .query('volatility > 0')          \\\n    .query('month_remaining == 2 or month_remaining == 4')   \\\n    .query(\"dt > '2016-12-31'\")       \\\n    .drop_duplicates(subset=['dt', 'ticker', 'call', 'month_remaining'], keep = 'first')\n\noptions_df.head()","fecf1947":"options_df['Date'] = pd.to_datetime(options_df.dt)\ndaily_tweets['Date'] = pd.to_datetime(daily_tweets.Date)\nfig, [ax1, ax2] = plt.subplots(2, figsize=(16, 8), sharex=True)\nsns.lineplot(data=options_df, x='Date', y='volatility', hue = 'month_remaining', ax=ax1)\nsns.scatterplot(data=daily_tweets, x='Date', y='sentiment', ax=ax2)\nax1.set(xlim=('2017-01-01', '2019-10-01'))\nplt.show()","7c08e7e9":"def days_since_last_tweet(row, tweets):\n    options_date = row['Date']\n    date_row = tweets[tweets.Date < options_date].tail(1)\n    return (options_date -  date_row.iloc[0, 0]).days\n\ndef last_tweet_sentiment(row, tweets):\n    options_date = row['Date']\n    date_row = tweets[tweets.Date < options_date].tail(1)\n    return date_row.iloc[0, 1]\n\noptions_df['days_since_last_tweet'] = None\noptions_df['last_tweet_sentiment'] = None\noptions_df['latest_volatility'] = None\noptions_df['days_since_last_tweet'] = options_df.apply(lambda row: days_since_last_tweet(row, daily_tweets), axis=1)\noptions_df['last_tweet_sentiment'] = options_df.apply(lambda row: last_tweet_sentiment(row, daily_tweets), axis=1)","d39f6bc1":"options_df.head()","fdb1ea37":"def latest_volatility(row, options):\n    date_row = options[(options.Date < row['Date']) & \n                       (options.call == row['call']) & \n                       (options.month_remaining == row['month_remaining']) & \n                       (options.commodity == row['commodity'])].tail(1)\n    return row['volatility'] if date_row.empty else date_row.iloc[0, 4]\n \noptions_df['latest_volatility'] = options_df.apply(lambda row: latest_volatility(row, options_df), axis=1)","1a14170a":"sns.pairplot(options_df)","d155589d":"sns.scatterplot(data=options_df, x='latest_volatility', y='volatility')","72931e94":"fig, axs = plt.subplots(2, figsize=(10, 10))\nsns.scatterplot(data=options_df, x='future_open_interest', y='volume', hue='month_remaining', alpha=0.5, ax=axs[0])\nsns.scatterplot(data=options_df, x='future_open_interest', y='volatility', hue='month_remaining', alpha=0.5, ax=axs[1])","33473df7":"X = options_df[['call', 'month_remaining', 'volume', \n                   'underlying_price', 'future_open_interest', \n                   'tnote_rate', 'days_since_last_tweet', \n                   'last_tweet_sentiment']]\ny = options_df['volatility']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.2,\n                                                    random_state = 42) ","c603f4d6":"def output_accurancy(y_hat, y):\n    errors = np.mean(abs(y_hat - y))\n    accuracy = round((1-np.mean(errors \/ y)) * 100, 1) \n    print('Mean Absolute Error: {} with accurancy of {}%.'.format(errors, accuracy))","5b1ad5b1":"model = LinearRegression()\nmodel.fit(X_train, y_train)\n\ny_train_hat = model.predict(X_train)\ny_test_hat = model.predict(X_test)\n\nprint('---- Train --- ')\noutput_accurancy(y_train_hat, y_train)\nprint('---- Test --- ')\noutput_accurancy(y_test_hat, y_test)","d4a8ac6e":"best_depth = 5\nmodel = RandomForestRegressor(max_depth=best_depth)\nmodel.fit(X_train, y_train)","c5ca757a":"y_train_hat = model.predict(X_train)\ny_test_hat = model.predict(X_test)\n\nprint('---- Train --- ')\noutput_accurancy(y_train_hat, y_train)\nprint('---- Test --- ')\noutput_accurancy(y_test_hat, y_test)","a745f383":"#def output_accurancy2(y_hat, y):\n#    errors = np.mean(abs(y_hat - y))\n#    accuracy = round((1-np.mean(errors \/ y)) * 100, 3) \n#    print('Mean Absolute Error: {} with accurancy of {}%.'.format(errors, accuracy))\n    \ndef output_accurancy2(y_hat, y):\n    errors = np.mean(abs(y_hat - y))\n    accuracy = round((1-np.mean(errors \/ y)) * 100, 3) \n    return accuracy\n\nacc_train = []\nacc_test = []\nfor i in range(1,50):\n    model = RandomForestRegressor(max_depth=i)\n    model.fit(X_train, y_train)\n\n    y_train_hat = model.predict(X_train)\n    y_test_hat = model.predict(X_test)\n\n    acc_train.append(output_accurancy2(y_train_hat, y_train))\n    acc_test.append(output_accurancy2(y_test_hat, y_test))\n\nplt.plot(acc_train,c='r',label='Train')\nplt.plot(acc_test,c='b',label='Test')\nplt.ylabel('Accuracy')\nplt.xlabel('Max Depth')\nplt.title('Random Forest Depth Accuracy')\nplt.legend()\nplt.show()","cb7b1c9b":"best_depth = 18\nmodel = RandomForestRegressor(max_depth=best_depth)\nmodel.fit(X_train, y_train)\n\ny_train_hat = model.predict(X_train)\ny_test_hat = model.predict(X_test)\n\nprint('---- Train --- ')\noutput_accurancy(y_train_hat, y_train)\nprint('---- Test --- ')\noutput_accurancy(y_test_hat, y_test)","4a052b5d":"sorted_feature_importance = sorted(zip(model.feature_importances_, X_train.columns), reverse=True)\nprint (*sorted_feature_importance, sep = \"\\n\")","f4e46d9d":"np.array(sorted_feature_importance)[:,0].astype(float)\nfig, ax = plt.subplots(figsize=(8, 6))\nax.set_title('important features for predicting volatility')\nb = sns.barplot(y=np.array(sorted_feature_importance)[:,1], \n                x=np.array(sorted_feature_importance)[:,0].astype(float))\nplt.xticks(rotation=45)\n\nplt.show()","ea2fb953":"options_df['delta_volatility'] = options_df['volatility'] - options_df['latest_volatility']","7fc5b10c":"delta_options_df = options_df.query('delta_volatility != 0')\nX_delta = delta_options_df[['call', 'month_remaining', 'volume', \n                   'underlying_price', 'future_open_interest', \n                   'tnote_rate', 'days_since_last_tweet', \n                   'last_tweet_sentiment']]\n    \ny_delta = delta_options_df['delta_volatility']\n\nX_train_delta, X_test_delta, y_train_delta, y_test_delta = train_test_split(X_delta, y_delta, \n                                                    test_size=0.2,\n                                                    random_state = 42) ","dd3cf84a":"X_train_delta.head()","00eea892":"y_train_delta.head()","f63eafa5":"best_depth = 20\nmodel_delta = RandomForestRegressor(max_depth=best_depth)\nmodel_delta.fit(X_train_delta, y_train_delta)","04088be7":"fig, [ax1, ax2] = plt.subplots(2, figsize=(16, 8), sharex=True)\nsns.lineplot(data=delta_options_df, x='Date', y='delta_volatility', ax=ax1, legend=\"full\", alpha=0.7)\nsns.lineplot(x=delta_options_df['Date'], y=model_delta.predict(X_delta), ax=ax1, legend=\"full\", alpha=0.7)\nax1.legend(labels=[\"actual vol changes\",\"predicted vol changes\"])\nsns.scatterplot(data=daily_tweets, x='Date', y='sentiment', ax=ax2)\nax1.set_title('volatility delta actual vs predicted')\nplt.setp(ax1.get_legend().get_texts(), fontsize=22)  # for legend text\nax1.set(xlim=('2017-01-01', '2019-10-01'))\nplt.show()","f7cf8b04":"y_train_hat_delta = model_delta.predict(X_train_delta)\ny_test_hat_delta = model_delta.predict(X_test_delta)\n\ndef output_accurancy(y_hat, y):\n    errors = np.mean(abs(y_hat - y))\n    print(np.mean(errors\/ abs(y)))\n    accuracy = round((1-np.mean(errors \/ abs(y))) * 100, 1) \n    print('Mean Absolute Error: {} with accurancy of {}%.'.format(errors, accuracy))\n\nprint('---- Train --- ')\noutput_accurancy(y_train_hat_delta, y_train_delta)\nprint('---- Test --- ')\noutput_accurancy(y_test_hat_delta, y_test_delta)","a315a044":"from sklearn import metrics\ndef output_accurancy2(y_hat, y):\n    errors = np.mean(abs(y_hat - y))\n    #print(np.mean(errors\/ abs(y)))\n    accuracy = round((1-np.mean(errors \/ abs(y))) * 100, 1) \n    #print('Mean Absolute Error: {} with accurancy of {}%.'.format(errors, accuracy))\n    return errors\n\nmodel_delta = RandomForestRegressor(max_depth=10)\nmodel_delta.fit(X_train_delta, y_train_delta)\ny_train_hat_delta = model_delta.predict(X_train_delta)\noutput_accurancy2(y_train_hat_delta, y_train_delta)*100\n#df = pd.DataFrame({'Actual': y_train_delta, 'Predicted': y_train_hat_delta})\n#df\nprint('Mean Squared Error:', metrics.mean_squared_error(y_train_delta, y_train_hat_delta))  ","015d2a8d":"from sklearn import metrics\ndef output_accurancy2(y_hat, y):\n    #errors = np.mean(abs(y_hat - y))\n    #print(np.mean(errors\/ abs(y)))\n    #accuracy = round((1-np.mean(errors \/ abs(y))) * 100, 1) \n    #print('Mean Absolute Error: {} with accurancy of {}%.'.format(errors, accuracy))\n    return metrics.mean_squared_error(y,y_hat)\n\nmse_train_delta = []\nmse_test_delta = []\nfor i in range(1,50):\n    model_delta = RandomForestRegressor(max_depth=i)\n    model_delta.fit(X_train_delta, y_train_delta)\n\n    y_train_hat_delta = model_delta.predict(X_train_delta)\n    y_test_hat_delta = model_delta.predict(X_test_delta)\n\n    mse_train = output_accurancy2(y_train_hat_delta, y_train_delta)*100\n    mse_test = output_accurancy2(y_test_hat_delta, y_test_delta)*100\n\n    mse_train_delta.append(mse_train)\n    mse_test_delta.append(mse_test)\n\nplt.plot(mse_train_delta,c='r',label='Train')\nplt.plot(mse_test_delta,c='b',label='Test')\nplt.ylabel('MSE')\nplt.xlabel('Max Depth')\nplt.title('Random Forest Regressor Error')\nplt.legend()\nplt.show()","47eb13fb":"model_delta = RandomForestRegressor(max_depth=20)\nmodel_delta.fit(X_train_delta, y_train_delta)\nsorted_feature_importance_delta = sorted(zip(model_delta.feature_importances_, X_train_delta.columns), reverse=True)\nprint (*sorted_feature_importance_delta, sep = \"\\n\")","31af5fae":"np.array(sorted_feature_importance_delta)[:,0].astype(float)\nfig, ax = plt.subplots(figsize=(8, 6))\nax.set_title('important features for predicting changes in volatility')\nb = sns.barplot(y=np.array(sorted_feature_importance_delta)[:,1], \n                x=np.array(sorted_feature_importance_delta)[:,0].astype(float))\nplt.xticks(rotation=45)\nplt.show()","57626966":"delta_options_df['delta_volatility_up'] = 1*(delta_options_df['volatility'] > delta_options_df['latest_volatility'])","ad169b0c":"delta_options_df.head()","d0b78fc7":"X_direction = delta_options_df[['call', 'month_remaining', 'volume', \n                   'underlying_price', 'future_open_interest', \n                   'tnote_rate', 'days_since_last_tweet', \n                   'last_tweet_sentiment']]\ny_direction = delta_options_df['delta_volatility_up']\n\nX_train_direction, X_test_direction, y_train_direction, y_test_direction = train_test_split(X_direction, y_direction, \n                                                    test_size=0.2,\n                                                    random_state = 42,\n                                                    stratify = y_direction) ","36d22251":"best_depth = 8\nmodel_direction = RandomForestClassifier(max_depth=best_depth)\nmodel_direction.fit(X_train_direction, y_train_direction)\ny_train_hat_direction = model_direction.predict(X_train_direction)\ny_test_hat_direction = model_direction.predict(X_test_direction)\n\n\n#Perfromance Evaluation\nacc_random_forest_training = accuracy_score(y_train_direction, y_train_hat_direction)*100\nacc_random_forest_testing = accuracy_score(y_test_direction, y_test_hat_direction)*100\n\nprint(\"Random Forest: Direction Prediction Accuracy, Training Set : {:0.2f}%\".format(acc_random_forest_training))\nprint(\"Random Forest: Direction Prediction Accuracy, Testing Set :  {:0.2f}%\".format(acc_random_forest_testing))","efe4e1aa":"sorted_feature_importance_delta = sorted(zip(model_delta.feature_importances_, X_train_delta.columns), reverse=True)\n\nnp.array(sorted_feature_importance_delta)[:,0].astype(float)\nfig, ax = plt.subplots(figsize=(8, 6))\nax.set_title('important features for predicting change direction in volatility')\nb = sns.barplot(y=np.array(sorted_feature_importance_delta)[:,1], \n                x=np.array(sorted_feature_importance_delta)[:,0].astype(float))\nplt.xticks(rotation=45)\nplt.show()","e0a19eb6":"#    RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0),\n#    LinearSVC(),\n#    MultinomialNB(),\n#    LogisticRegression(random_state=0),\n\nacc_train_direction = []\nacc_test_direction = []\nfor i in range(1,50):\n    model_direction = RandomForestClassifier(max_depth=i)\n    model_direction.fit(X_train_direction, y_train_direction)\n\n    y_train_hat_direction = model_direction.predict(X_train_direction)\n    y_test_hat_direction = model_direction.predict(X_test_direction)\n\n    acc_random_forest_training = accuracy_score(y_train_direction, y_train_hat_direction)*100\n    acc_random_forest_testing = accuracy_score(y_test_direction, y_test_hat_direction)*100\n\n    acc_train_direction.append(acc_random_forest_training)\n    acc_test_direction.append(acc_random_forest_testing)\n    \nplt.plot(acc_train_direction,c='r',label='Train')\nplt.plot(acc_test_direction,c='b',label='Test')\nplt.ylabel('Accuracy')\nplt.xlabel('Max Depth')\nplt.title('Random Forest Depth Accuracy')\nplt.legend()\nplt.show()","d7112e1f":"#plt.hist(X_train_direction)\n#X_train_direction\nX_train_direction_nonnegative = X_train_direction[(X_train_direction > 0).all(1)]","76e8d70e":"#delete\nfrom sklearn.model_selection import cross_val_score\n\nmodels = [\n    RandomForestClassifier(max_depth=15, random_state=0),\n    LinearSVC(),\n    MultinomialNB(),\n    xgboost.XGBClassifier(),\n    LogisticRegression(random_state=0),\n]\n\nCV = 20\ncv_df = pd.DataFrame(index=range(CV * len(models)))\n\nfeatures = X_train_direction+1\nlabels = y_train_direction\n\nentries = []\nfor model in models:\n  model_name = model.__class__.__name__\n  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n  for fold_idx, accuracy in enumerate(accuracies):\n    entries.append((model_name, fold_idx, accuracy))\n    \ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n\nmean_accuracy = cv_df.groupby('model_name').accuracy.mean()\nstd_accuracy = cv_df.groupby('model_name').accuracy.std()\n\nacc = pd.concat([mean_accuracy, std_accuracy], axis= 1, \n          ignore_index=True)\nacc.columns = ['Mean Accuracy', 'Standard deviation']\nacc","41408f3c":"plt.figure(figsize=(8,5))\nsns.boxplot(x='model_name', y='accuracy', \n            data=cv_df, \n            color='lightblue', \n            showmeans=True)\nplt.title(\"MEAN ACCURACY (cv = 20)\\n\", size=14);","a5d97d5b":"#import pandas as pd\n#TrumpTweets = pd.read_csv(\"..\/input\/TrumpTweets.csv\")\n#options = pd.read_csv(\"..\/input\/options.csv\")","d15e3c4a":"## plot these options' vol data against the backdrop of the tweets","a115e9d5":"### cleanup the tweets text","156b45d6":"## filter to only consider tweets with certain topic, in this case, trade.","4726e37e":"## augment options dataset with tweet context","5a6a23f3":"## fit a regression model to predict the market volatility","34d16ba5":"## read in the options data and narrow down to only certain commodity options of certain types","2d0018aa":"## fit a classifier to determine if the volatility is going up or down","5a12372c":"> ## fit a regression model on volatility changes (a.k.a. volality of volatility)","bc2dd834":"## read in the tweet data (since 2017-01-01)","080d4acd":"## extract the sentiment of the tweets","561942a8":"## evaluate the model"}}