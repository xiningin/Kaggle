{"cell_type":{"0e4c9724":"code","6e8d5af0":"code","43261e05":"code","ca3ec6cc":"code","a8fc7aa8":"code","31ed741a":"code","2564835a":"code","22fd1c5d":"code","a644d9ad":"code","4c460383":"code","623655aa":"code","f56c9482":"code","738c14a1":"code","2d0aa492":"code","7f7744b0":"code","eac5b514":"code","e6621e75":"code","a3759611":"code","caff8c2b":"code","0f2fa95b":"code","c0732389":"code","1f0dd8a6":"code","64a87b16":"code","14341375":"code","23519dd0":"code","4f23810e":"code","e4743b65":"code","972aee00":"code","2c54f37f":"code","3a191d03":"code","a660e3ef":"code","dd036495":"code","b5c48cd4":"code","28d87fe0":"code","a9f19f76":"markdown","111dc9ee":"markdown","2cadf80d":"markdown","fc453b09":"markdown","e97ee3cd":"markdown","02995539":"markdown","38117198":"markdown","54cb4a4e":"markdown","8fcc4243":"markdown","e03c37d3":"markdown","8707fd2a":"markdown","551203ed":"markdown","93c536c7":"markdown","2bae23bb":"markdown","af619b48":"markdown","9b0b47df":"markdown","cb60ba74":"markdown","6a2365ac":"markdown","e44bb57f":"markdown","4fe0aa16":"markdown","f47cd4f9":"markdown","ceec71c5":"markdown","cb4b2185":"markdown","ce64c9ca":"markdown","f2f848a7":"markdown","bbce3b77":"markdown","e09d809f":"markdown","08652f40":"markdown"},"source":{"0e4c9724":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6e8d5af0":"df_train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ndf_train.describe()\n#df_train.head()","43261e05":"#closer look to target variable\n\ndf_train['SalePrice'].describe()","ca3ec6cc":"#histogram to see the distribution of the target value\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew, norm\n\n\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\n\nf, ax = plt.subplots(figsize=(16, 6))\nplt.subplot(1,2,1)\nsns.distplot(df_train['SalePrice'] , fit=norm, color=\"b\");\n(mu, sigma) = norm.fit(df_train['SalePrice'])\n\nplt.legend(['Distribution ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\n#ax.set(title=\"SalePrice distribution\")\nplt.xticks(rotation=90);\n#sns.despine(trim=True, left=True)\nplt.show()\n\nplt.figure(figsize=(16,6))\nplt.subplot(1,2,2)\nsns.boxplot(x = df_train['SalePrice'])\nplt.xticks(rotation=90);\nplt.show()","a8fc7aa8":"correlation_matrix = df_train.corr().round(2)\nsns.set(rc={'figure.figsize':(12,8)})\nsns.heatmap(data=correlation_matrix)\nplt.show()","31ed741a":"n = 10 #top number of variables for heatmap\ncols = correlation_matrix.nlargest(n, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","2564835a":"sns.pairplot(df_train[cols])","22fd1c5d":"df_train.groupby('OverallQual')['SalePrice'].count().sort_values(ascending = False)\nsns.violinplot(x = df_train.OverallQual, y = df_train.SalePrice)","a644d9ad":"plt.figure(figsize=(10,10))\nplt.subplot(2,2,2)\nplt.scatter(x = df_train.SalePrice, y = df_train.YearBuilt)\nplt.title('Joint distribution of sale price and year built')\n\nplt.subplot(2,2,4)\nplt.hist(x=df_train.SalePrice)\nplt.title(\"Distribution of sale price\")\n\nplt.subplot(2,2,1)\nplt.hist(x=df_train.YearBuilt)\nplt.title(\"Distribution of year built\")\n\nplt.show()","4c460383":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\n\nplt.figure(figsize=(8, 6))\n\nsns.distplot(df_train['GrLivArea'], fit=norm, color=\"b\");\n(mu, sigma) = norm.fit(df_train['GrLivArea'])\n\nplt.legend(['Distribution ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"GrLivArea\")\n\nplt.show()\n","623655aa":"from sklearn import preprocessing\nimport statsmodels.api as sm\n\n#to normalize the predictor variable \ndf_train[\"GrLivArea_pre\"] = preprocessing.scale(df_train.GrLivArea.values)\n\nmodel = sm.OLS.from_formula(\"SalePrice ~ GrLivArea_pre\", data=df_train)\nresult = model.fit()\nresult.summary()","f56c9482":"df_train['OverallQualx'] = df_train.OverallQual.replace({1: 'Very Poor', 2: 'Poor', 3: 'Fair', 4: 'Below Average', \n                                                    5: 'Average', 6:'Above Average', 7: 'Good', 8: 'Very Good',\n                                                       9: 'Excellent', 10: 'Very Excellent'})\n\nmodel = sm.OLS.from_formula(\"SalePrice ~ GrLivArea + OverallQualx\", data=df_train)\nresult = model.fit()\nresult.summary()","738c14a1":"df_train[\"GarageArea_pre\"] = preprocessing.scale(df_train.GarageArea.values)\n\n# 4 variables\nmodel = sm.OLS.from_formula(\"SalePrice ~ GrLivArea_pre + OverallQualx + GarageArea_pre\", data=df_train)\nresult = model.fit()\nresult.summary()","2d0aa492":"df_train[[\"OverallQual\", \"GrLivArea_pre\", \"GarageArea_pre\"]].corr()","7f7744b0":"# plot the residuals against the fitted values\n\npp = sns.scatterplot(result.fittedvalues, result.resid)\npp.set_xlabel(\"Fitted values\")\n_ = pp.set_ylabel(\"Residuals\")","eac5b514":"y = df_train['SalePrice']\nX = df_train.drop(['SalePrice'], axis=1)\nX.shape","e6621e75":"X = X.drop([\"GrLivArea_pre\", \"GarageArea_pre\"], axis=1)","a3759611":"X['group'] = 'train'\ndf_test['group'] = 'test'\n\nall_features = pd.concat([X, df_test])\nall_features.shape","caff8c2b":"all_features['HasPool'] = np.where(pd.isnull(all_features['PoolQC']), 0, 1)\nall_features['HasGarage'] = np.where(pd.isnull(all_features['GarageYrBlt']), 0, 1)\nall_features['HasBsmt'] = np.where(pd.isnull(all_features['BsmtQual']), 0, 1)\nall_features['LotFrontage'] = all_features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\nall_features['AgeTillSold'] = all_features['YrSold'] - all_features['YearBuilt']\nall_features['MSSubClass'] = all_features['MSSubClass'].apply(str)\nall_features['YrSold'] = all_features['YrSold'].astype(str)\nall_features['MoSold'] = all_features['MoSold'].astype(str)\n","0f2fa95b":"X = all_features[all_features['group'] == 'train']\ndf_test = all_features[all_features['group'] == 'test']\nX = X.drop('group', axis=1)\ndf_test = df_test.drop('group', axis=1)\nX.shape, df_test.shape","c0732389":"total = X.isnull().sum().sort_values(ascending=False)\npercent = (X.isnull().sum()\/ X.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","1f0dd8a6":"cols_encode = ['ExterQual', 'BsmtQual', 'ExterCond', 'BsmtCond','HeatingQC', 'KitchenQual',  'GarageQual', \n               'GarageCond' ]\ndata_object = [i for i in X.columns if X[i].dtype == object]\nother_encode = [i for i in data_object if i not in cols_encode]\nother_encode","64a87b16":"numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_cols = [i for i in X.columns if X[i].dtype in numeric_dtypes]","14341375":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\n\n\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer_ord = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('ordinal', OrdinalEncoder(handle_unknown='ignore'))\n])\ncategorical_transformer_onehot = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat_or', categorical_transformer_ord, cols_encode),\n        ('cat_one', categorical_transformer_onehot, other_encode)\n    ])","23519dd0":"from sklearn.svm import LinearSVR \nfrom xgboost import XGBRegressor \nfrom xgboost import plot_importance \nfrom sklearn.metrics import auc \nfrom sklearn.model_selection import GridSearchCV","4f23810e":"my_model = XGBRegressor()\nmy_model.get_params()","e4743b65":"xgbreg = XGBRegressor(learning_rate = 0.05, max_depth = 2, n_estimators=500, subsample= 0.9, gamma= 0.01, colsample_bytree=0.5)","972aee00":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1, shuffle=True)","2c54f37f":"X_train.shape","3a191d03":"my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),('model', xgbreg)])\n\nmy_pipeline.fit(X_train, y_train)\n#my_pipeline.fit(X, y)\n#prediction = my_pipeline.predict(df_test)","a660e3ef":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nprediction = my_pipeline.predict(X_val)\nmse = mean_squared_error(y_val, prediction)\nrmse = np.sqrt(mse)\nrmse","dd036495":"from yellowbrick.regressor import residuals_plot\nvisualizer = residuals_plot(my_pipeline, X_train, y_train, X_val,y_val)\nvisualizer.show()","b5c48cd4":"R_squared = r2_score(y_val, prediction)\nR_squared","28d87fe0":"#output = pd.DataFrame({'Id': df_test.Id, 'SalePrice': prediction})\n#print(output.head())\n#output.to_csv('house_submission.csv', index=False)","a9f19f76":"Let's check the distribution of GrLivArea which is one of the highly correlated numerical variables.","111dc9ee":"Now let's fit a simple regression model with only one independent variable GrLivArea by using OLS Regression model.","2cadf80d":"For the demonstration purposes, we can split the df_train dataset to train \/ validation datasets.","fc453b09":"### Checking the missing data in the data set\nWe are going to apply the imputation as part of the preprocessing step of the pipeline, but first let's have a look to the missing data percentages of the variables.","e97ee3cd":"Let's review the pairplot for the top correlated features to see the distribution of each variable with each other to have better understanding.","02995539":"## Part I: Data Analysis","38117198":"We need to check the distribution of the residuals for the fitted values, expected to be scattered around 0 symmetrically for a good predictibility.\n\nThis does not look too bad. However, it appears that we have a modestly increasing mean\/variance relationship. That is, the scatter around the mean price is smaller compared to high predicted values where we have some outliers.\n\nAlthough, we observe from the results of the last model that all three variables are significant for the SalePrice prediction, we need more inputs. \n\n","54cb4a4e":"As the predictive model, we are going to use XGBRegressor which is a quite powerful model. ","8fcc4243":"We can add a second variable \"OverallQual\" to the model and check the impact on the predictibility. First, we relabel the values to have a better understanding.","e03c37d3":"Some of the categorical variables are ordinal values. So, let's separate the ordinal categorical variables from the others to have a proper encoding.","8707fd2a":"### Creating new features\nFor creating new features, we can concatenate both data sets X and df_test for easiness. This is not recommended for the imputation or categorical values encoding since this can cause the data leakage. ","551203ed":"### Visualization of the fitted model\nWe can visualize the residuals from the fitted model to evaluate the result from the model.","93c536c7":"First thing that we notice is that the mean > median (50%), which points to a right-skewed data. Also, we see that we have outliers on the higher end (high prices range). Let's also visualize the distribution.","2bae23bb":"Let's also create a list for the numerical values since we are going to impute them differently.","af619b48":"Let's also drop the preprocessed variables since we are going to scale the data as part of the pipeline.","9b0b47df":"We can apply separately GridSearchCV after cleaning and preprocessing the data for the hyperparameter optimization, but it takes a while to compute the best parameters for the model. Following is a sample code snippet for this purpose.\n\n> optimization_dict = {'max_depth': [2,4],'n_estimators': [200,500],'learning_rate': [0.05, 0.1],'gamma': [0.01, 0.1]}\n\n\n> model = GridSearchCV(my_model, optimization_dict, verbose=1, n_jobs=2)\n\n> model.fit(X, y)\n\n> means = model.cv_results_['mean_test_score']\n\n> stds = model.cv_results_['std_test_score']\n\n> for mean, std, params in zip(means, stds, model.cv_results_['params']):\n        print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))\n              \n> print()\n\n> print(model.best_params_)","cb60ba74":"The model that was fit above uses both GrLivArea and OverallQual to explain the variation in price. It finds that two houses with the same quality level whose GrLivArea differ by one square feet tend to have price values differing by 0.0003 units, which is less than the GrLivArea parameter that we found above in the model based on age alone. This model also shows us that comparing two houses with the same size, the price slope changes based on the quality level.\nWe also observe that the 76% of the variation in price is explained by both variables.\n\nWe can go ahead and add one more variables with high correlation to the model.","6a2365ac":"# House Sale Price Prediction\n\nThis notebook has been prepared to walkthrough the steps of a machine learning problem. It includes data analysis of both the target variable and the independent variables, simple approach for the new feature generation, handling of the missing values and categorical variable encoding.\n\nWe don't have much background information regarding the survey method applied (how the data has been collected). So, in the first part, the different statistical analyses have been applied to have a closer look. In the second part a simple data cleaning and a prediction process have been applied.\n\n### Data:\nThe data consist of 80 independent variables to predict the sale price of a house in Iowa Ames city. Every independent variable describes different parts and properties of the houses, some of which with a high correlation, others with low correlation with the target variable. For the details of the variables, you can check https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data.\n\nLet's get started to this excited journey!","e44bb57f":"GarageArea is positively associated with price. Given two subjects with the same quality and size, and whose GarageArea differs by 1 unit, the house with greater GarageArea will have, on average, 0.0001 units greater price. Also note that after adding GarageArea to the model, the coefficient for quality became somewhat greater for the negative values and smaller for the positive values. \nNext we can check the correlation of the variables with each other to have a better view.","4fe0aa16":"## Part II: Data Cleaning\/Preprocessing and Predictions\n\nNext, we are going to continue with data cleaning and prediction processes with more complex machine learning models. Please note that the above data analysis can be extended with the cluster analysis, such as marginal and \/ or multilevel linear models, based on the Neighborhood variable to find more hints regarding the SalePrice.","f47cd4f9":"We can check the parameters of the model for the hyperparameter optimization.","ceec71c5":"Now, let's check the correlation of the independent variables with the target variable by using heat map.","cb4b2185":"We have completed all the steps for our pipeline till here. Let's build our pipeline as the final step and fit it to our train dataset.","ce64c9ca":"### Preprocessing for imputation and encoding","f2f848a7":"Now all set! As the next step, we are going to build the preprocessing step for our our pipeline.","bbce3b77":"This fitted model implies that when comparing two houses whose above grade (ground) living area differ by one square feet, the bigger house will on average have 0.0005 units higher price than the smaller house. This difference is statistically significant, based on the p-value shown under the column labeled P>|t|. This means that there is strong evidence that there is a real association between between GrLivArea and price in this population.\nAdditionally, R-squared, which is the primary summary statistic for assessing the strength of a predictive relationship in a regression model, is shown to be 0.502 in the regression output above. This means that 50% of the variation in price is explained by above grade (ground) living area. ","e09d809f":"We can also visualize the SalePrice distribution for the top variables in different ways independently","08652f40":"After the above hyperparameter optimization we have done separately, we apply the best parameters found for our XGBRegressor model."}}