{"cell_type":{"74b2e164":"code","0ab588c1":"code","586ca194":"code","269eea2e":"code","805785d4":"code","c4744cbd":"code","db4e9095":"code","b653bf00":"code","96459c4d":"code","3d6fcc06":"code","a802044d":"code","ba64b2da":"code","db60a205":"code","0a370c10":"code","f4b0729e":"code","90a31744":"code","ff5a5ae6":"code","eb61f738":"code","16135adf":"code","e55e11e8":"code","6a3ce655":"code","399d6738":"markdown","ab3a0167":"markdown","cf27bae4":"markdown","4af9c2b8":"markdown","96cfd77a":"markdown","ff11009d":"markdown","f6b88b84":"markdown","fa8a0d33":"markdown","82dabda7":"markdown","b092f6dd":"markdown"},"source":{"74b2e164":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression, Ridge, ElasticNet\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import model_selection\nimport spacy","0ab588c1":"# config params\nclass CFG:\n    nfolds =  5","586ca194":"def syllable_count(word):\n    count = 0\n    vowels = \"aeiouAEIOU\"\n    if word[0] in vowels:\n        count += 1\n    for index in range(1, len(word)):\n        if word[index] in vowels and word[index - 1] not in vowels:\n            count += 1\n            if word.endswith(\"e\"):\n                count -= 1\n    if count == 0:\n        count += 1\n    return count","269eea2e":"xtrain = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\nxtest = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","805785d4":"palette = sns.diverging_palette(120, 220, n=20)\nfig, ax = plt.subplots(1,2,figsize=(20,10))\nsns.kdeplot(xtrain['target'], color=palette[-1], shade=True,ax=ax[0])\nsns.kdeplot(xtrain['standard_error'], color=palette[5], shade=True,ax=ax[1])\nax[0].set_title(\"Target Distribution\",font=\"Serif\")\nax[1].set_title(\"Standard Error Distribution\",font=\"Serif\")\nplt.show()","c4744cbd":"plt.figure(figsize=(16, 8))\nsns.countplot(y=\"license\",data=xtrain,palette=palette)\nplt.title(\"License Distribution\",font=\"Serif\")\nplt.show()","db4e9095":"# count the characters\nxtrain['nof_char'] = xtrain['excerpt'].apply(len)\nxtest['nof_char'] = xtest['excerpt'].apply(len)","b653bf00":"# count the words\nxtrain['nof_words'] = xtrain['excerpt'].apply(lambda s: len(s.split(' ')))\nxtest['nof_words'] = xtest['excerpt'].apply(lambda s: len(s.split(' ')))","96459c4d":"# words to characters\nxtrain['w2c'] = xtrain['nof_words'] \/ xtrain['nof_char']\nxtest['w2c'] = xtest['nof_words'] \/ xtest['nof_char']","3d6fcc06":"# nof sentences\nxtrain['nof_sentences'] =  xtrain['excerpt'].apply(lambda s: s.count('.'))\nxtest['nof_sentences'] =  xtest['excerpt'].apply(lambda s: s.count('.'))","a802044d":"# nof syllables\nxtrain['nof_syllables'] =  xtrain['excerpt'].apply(lambda s: syllable_count(s))\nxtest['nof_syllables'] =  xtest['excerpt'].apply(lambda s: syllable_count(s))","ba64b2da":"# Fleisch score\na = 206.835 - 1.015 * (xtrain['nof_words'] \/ xtrain['nof_sentences'])\nb = -84.6 * (xtrain['nof_syllables'] \/ xtrain['nof_words'])\nxtrain['fleisch_score'] = a + b\n\na = 206.835 - 1.015 * (xtest['nof_words'] \/ xtest['nof_sentences'])\nb = -84.6 * (xtest['nof_syllables'] \/ xtest['nof_words'])\nxtest['fleisch_score'] = a + b","db60a205":"# Fleisch score 2\na = (xtrain['nof_words'] \/ xtrain['nof_sentences'])\nb = (xtrain['nof_syllables'] \/ xtrain['nof_words'])\nxtrain['fleisch_score2'] = 0.39 * a + 11.8 * b - 15.59\n\na = (xtest['nof_words'] \/ xtest['nof_sentences'])\nb = (xtest['nof_syllables'] \/ xtest['nof_words'])\nxtest['fleisch_score2'] = 0.39 * a + 11.8 * b - 15.59\n \n    \ndel a,b","0a370c10":"# count the unique words\nxtrain['nof_unique_words'] = xtrain['excerpt'].apply(lambda s: len(set( s.split(' ') )))\nxtest['nof_unique_words'] = xtest['excerpt'].apply(lambda s: len(set( s.split(' ') )))","f4b0729e":"# text diversity\nxtrain['txt_diversity'] = xtrain['nof_unique_words'] \/ xtrain['nof_words']\nxtest['txt_diversity'] = xtest['nof_unique_words'] \/ xtest['nof_words']","90a31744":"# word lengths\nwords = xtrain['excerpt'].apply(lambda s: s.split(' '))\nword_lengths = words.apply(lambda s: [len(f) for f in s ])\nxtrain['longest_word'] = word_lengths.apply(max)\nxtrain['avg_word'] = word_lengths.apply(np.mean)\n\nwords = xtest['excerpt'].apply(lambda s: s.split(' '))\nword_lengths = words.apply(lambda s: [len(f) for f in s ])\nxtest['longest_word'] = word_lengths.apply(max)\nxtest['avg_word'] = word_lengths.apply(np.mean)","ff5a5ae6":"nlp = spacy.load('en_core_web_lg')\nwith nlp.disable_pipes():\n    train_vectors = np.array([nlp(text).vector for text in xtrain.excerpt])\n    test_vectors = np.array([nlp(text).vector for text in xtest.excerpt])\n        \nnamelist = ['f' + str(ii) for ii in range(train_vectors.shape[1])]\n\ntrain_vectors = pd.DataFrame(train_vectors)\ntest_vectors = pd.DataFrame(test_vectors)\ntrain_vectors.columns = namelist\ntest_vectors.columns = namelist","eb61f738":"xtrain = pd.concat([xtrain, train_vectors], axis = 1)\nxtest = pd.concat([xtest, test_vectors], axis = 1)\n\n\nfeatures = ['nof_words', 'nof_sentences', 'nof_syllables', 'fleisch_score',\n           'txt_diversity', 'nof_unique_words', 'nof_char', 'w2c', \n            'fleisch_score2'] + namelist","16135adf":"kf = KFold(n_splits = CFG.nfolds)\n\nprval = np.zeros((xtrain.shape[0],1))\nprfull = np.zeros((xtest.shape[0],1))","e55e11e8":"for id0, id1 in kf.split(xtrain):\n    x0, x1 = xtrain[features].loc[id0], xtrain[features].loc[id1]\n    y0, y1 = xtrain['target'][id0], xtrain['target'][id1]\n    \n    model = Ridge(alpha = 1)\n\n    model.fit(x0,y0)\n    \n    ypred = model.predict(x1)\n    prval[id1,0] =  model.predict(x1)\n    prfull[:,0] += model.predict(xtest[features])\/CFG.nfolds\n    \n    print(np.round(np.sqrt(mse(prval[id1,0], y1)),2 ))\n    \n# score\nprint('--')\nprint(np.round( np.sqrt(mse(prval, xtrain['target'])) , 3))","6a3ce655":"xsub = xtest[[\"id\"]].copy()\nxsub[\"target\"] = prfull\nxsub.to_csv('submission.csv', index = False)\nxsub","399d6738":"# Functions","ab3a0167":"## Combine","cf27bae4":"## Summary statistics features\n","4af9c2b8":"# Cross Validation ","96cfd77a":"## EDA","ff11009d":"# Submission","f6b88b84":"## Spacy features","fa8a0d33":"Import all the required libraries","82dabda7":"# Introduction\n\nI am predicting the reading ease of excerpts from literature. We've provided excerpts from several time periods and a wide range of reading ease scores.","b092f6dd":"# Data and FE"}}