{"cell_type":{"195fe976":"code","a2e89269":"code","06307048":"code","b6d5e0c6":"code","cd234058":"code","f755d5c1":"code","c22455cd":"code","fe00650a":"code","4251a39a":"code","78312c08":"code","febf6845":"code","f751c24b":"code","1901e6f4":"code","9f65a1a8":"code","935753be":"code","523701c0":"code","f86e7109":"code","dc66bc0a":"code","1572345f":"code","5592b0c3":"code","72aa5df0":"code","ca20661d":"code","32697c95":"code","139c307d":"code","f3e444e2":"code","996d96e8":"code","4a003aea":"code","3efeb0be":"code","514af9b6":"code","ae749833":"code","94c7cc9f":"code","fa61254a":"code","7bfc559c":"code","a6b20a63":"code","e20db490":"code","9c4cf217":"code","247d7cbe":"code","d854b41c":"code","e458739d":"code","807c830a":"code","8ffbcdd7":"code","f9194438":"code","e60a9310":"code","735fee0d":"code","d2d9b771":"code","dc6ccd00":"code","7e12b203":"code","ea8383e7":"code","a0cd1bf6":"markdown","8bb6fe20":"markdown","79b1038e":"markdown","f9adf238":"markdown","f79e93c9":"markdown","81bb6c13":"markdown","f98613d3":"markdown","91b862e6":"markdown","56e7422d":"markdown","5459102a":"markdown","78364c3a":"markdown","6f522ad5":"markdown","0b094f4b":"markdown","a40ded5a":"markdown","1cc83406":"markdown","86385e93":"markdown","8d6e7eef":"markdown","6ef56816":"markdown","718c2ed2":"markdown","08726dc6":"markdown","a4eafdbd":"markdown","34a83455":"markdown","63f334e1":"markdown","4c2d7e9c":"markdown","a28fb68f":"markdown","5a0a0bf5":"markdown","759b472d":"markdown","cc949b14":"markdown","27326793":"markdown","eec46eef":"markdown","64c6c463":"markdown","4b84c63d":"markdown","92882a8a":"markdown","768d0a2d":"markdown","a44b1ff7":"markdown","855e7def":"markdown","da080300":"markdown"},"source":{"195fe976":"#\nfrom scipy import stats\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight');\nplt.rcParams['font.size'] = 14;\nplt.figure(figsize=(12,5));\npalette = sns.color_palette('Paired', 10);\n\n# map\nimport folium\nfrom folium.plugins import HeatMap\nfrom folium.plugins import HeatMapWithTime\n\n# Sci-kit learn\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.base import BaseEstimator, TransformerMixin \nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn import set_config; set_config(display='diagram')","a2e89269":"data = pd.read_csv(\"\/kaggle\/input\/new-york-city-taxi-fare-prediction\/train.csv\", nrows=1000000)\ndata.head()","06307048":"data = data.drop([\"key\"], axis=1)","b6d5e0c6":"size_before = len(data)\ndata = data.drop_duplicates()\nsize_after = len(data)\nprint(str(size_before - size_after) + \" duplicates were removed.\")","cd234058":"100 * data.isnull().sum().sort_values(ascending=False)\/len(data)","f755d5c1":"def plot_dist(series=data[\"fare_amount\"], title=\"Fare Distribution\"):\n    sns.histplot(series, kde=True, stat='density', discrete=True)\n    sns.despine()\n    plt.title(title);\n    plt.show()\nplot_dist()","c22455cd":"data = data[data.fare_amount.between(0, 60)]\nplot_dist(data.fare_amount)","fe00650a":"data['fare-bin'] = pd.cut(data['fare_amount'], bins = list(range(0, 50, 5)), include_lowest=True).astype('str')\n\n# Uppermost bin\ndata['fare-bin'] = data['fare-bin'].replace(np.nan, '[45+]')\n\n# apply this to clean up the first bin's label\ndata['fare-bin'] = data['fare-bin'].apply(lambda x: x.replace('-0.001', '0'))\n\n# sort by fare the correct look in the chart\ndata = data.sort_values(by='fare_amount')","4251a39a":"sns.catplot(x=\"fare-bin\", kind=\"count\", palette=palette, data=data, height=5, aspect=3);\nsns.despine()\nplt.show()","78312c08":"data.passenger_count.describe()","febf6845":"sns.catplot(x=\"passenger_count\", kind=\"count\", palette=palette, data=data, height=5, aspect=3);\nsns.despine()\nplt.title('Passenger Count');\nplt.show()","f751c24b":"def extract_time_features(df):\n    timezone_name = 'America\/New_York'\n    time_column = \"pickup_datetime\"\n    df.index = pd.to_datetime(df[time_column])\n    df.index = df.index.tz_convert(timezone_name)\n    df[\"dow\"] = df.index.weekday\n    df[\"hour\"] = df.index.hour\n    df[\"month\"] = df.index.month\n    df[\"year\"] = df.index.year\n    return df.reset_index(drop=True)","1901e6f4":"data = extract_time_features(data.drop([\"fare-bin\"], axis=1))\ndata.head()","9f65a1a8":"sns.catplot(x=\"hour\", kind=\"count\", palette=palette, data=data, height=5, aspect=3);\nsns.despine()\nplt.title('Hour of Day');\nplt.show()","935753be":"sns.catplot(x=\"dow\", kind=\"count\", palette=palette, data=data, height=5, aspect=3);\nsns.despine()\nplt.title('Day of Week');\nplt.show()","523701c0":"data_test = pd.read_csv(\"\/kaggle\/input\/new-york-city-taxi-fare-prediction\/test.csv\")","f86e7109":"for col in [\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"]:\n    MIN = data_test[col].min()\n    MAX = data_test[col].max()\n    print(col, MIN, MAX)","dc66bc0a":"data = data[data[\"pickup_latitude\"].between(left = 40, right = 42 )]\ndata = data[data[\"pickup_longitude\"].between(left = -74.3, right = -72.9 )]\ndata = data[data[\"dropoff_latitude\"].between(left = 40, right = 42 )]\ndata = data[data[\"dropoff_longitude\"].between(left = -74, right = -72.9 )]","1572345f":"center_location = [40.758896, -73.985130]\nm = folium.Map(location=center_location, control_scale=True, zoom_start=11)","5592b0c3":"data[\"count\"] =1\nheatmap_data = data.head(10000)[['pickup_latitude', 'pickup_longitude', 'count']].groupby(['pickup_latitude', 'pickup_longitude']).sum().reset_index().values.tolist()\ngradient = {0.2: 'blue', 0.4: 'lime', 0.6: 'orange', 1: 'red'}\nHeatMap(data=heatmap_data, radius=5, gradient=gradient, max_zoom=13).add_to(m)\nm","72aa5df0":"heatmap_data_by_hour = []\n__data__ = data.head(10000)\nfor hour in data.hour.sort_values().unique():\n    _data = __data__[__data__.hour == hour][['pickup_latitude', 'pickup_longitude', 'count']].groupby(['pickup_latitude', 'pickup_longitude']).sum().reset_index().values.tolist()\n    heatmap_data_by_hour.append(_data)","ca20661d":"m2 = folium.Map(location=center_location, control_scale=True, zoom_start=11)\nHeatMapWithTime(heatmap_data_by_hour, radius=5, \n                gradient=gradient, \n                min_opacity=0.5, max_opacity=0.8, \n                use_local_extrema=False).add_to(m2)\nm2","32697c95":"def haversine_distance(df,\n                       start_lat=\"start_lat\",\n                       start_lon=\"start_lon\",\n                       end_lat=\"end_lat\",\n                       end_lon=\"end_lon\"):\n    \"\"\"\n        Calculate the great circle distance between two points \n        on the earth (specified in decimal degrees).\n        Vectorized version of the haversine distance for pandas df\n        Computes distance in kms\n    \"\"\"\n\n    lat_1_rad, lon_1_rad = np.radians(df[start_lat].astype(float)), np.radians(df[start_lon].astype(float))\n    lat_2_rad, lon_2_rad = np.radians(df[end_lat].astype(float)), np.radians(df[end_lon].astype(float))\n    dlon = lon_2_rad - lon_1_rad\n    dlat = lat_2_rad - lat_1_rad\n\n    a = np.sin(dlat \/ 2.0) ** 2 + np.cos(lat_1_rad) * np.cos(lat_2_rad) * np.sin(dlon \/ 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    haversine_distance = 6371 * c\n    return haversine_distance\n\ndata[\"distance\"] = haversine_distance(data, \n                                      start_lat=\"pickup_latitude\", start_lon=\"pickup_longitude\",\n                                      end_lat=\"dropoff_latitude\", end_lon=\"dropoff_longitude\"\n                                     )","139c307d":"data.distance.describe()","f3e444e2":"%matplotlib inline\nplot_dist(series=data[data.distance < 50].distance, title='Distance distribution')","996d96e8":"sns.catplot(x=\"passenger_count\", y=\"fare_amount\", palette=palette, data=data, kind=\"bar\", aspect=3)\nsns.despine()\nplt.show()","4a003aea":"sns.catplot(x=\"hour\", y=\"fare_amount\", palette=palette, data=data, kind=\"bar\", aspect=3)\nsns.despine()\nplt.show()","3efeb0be":"sns.catplot(x=\"dow\", y=\"fare_amount\", palette=palette, data=data, kind=\"bar\", aspect=3)\nsns.despine()\nplt.show()","514af9b6":"sns.scatterplot(x=\"distance\", y=\"fare_amount\", data=data[data.distance < 80].sample(100000))\nplt.show()","ae749833":"data.head()","94c7cc9f":"data_train = pd.read_csv(\"\/kaggle\/input\/new-york-city-taxi-fare-prediction\/train.csv\", nrows=1000)","fa61254a":"def clean_data(df, test=False, predict=False):\n    df = df.drop([\"key\"], axis=1)\n    df = df.dropna(how='any', axis='rows')\n    df = df[(df.dropoff_latitude != 0) | (df.dropoff_longitude != 0)]\n    df = df[(df.pickup_latitude != 0) | (df.pickup_longitude != 0)]\n    if \"fare_amount\" in list(df):\n        df = df[df.fare_amount.between(0, 4000)]\n    df = df[df.passenger_count < 8]\n    df = df[df.passenger_count >= 0]\n    df = df[df[\"pickup_latitude\"].between(left=40, right=42)]\n    df = df[df[\"pickup_longitude\"].between(left=-74.3, right=-72.9)]\n    df = df[df[\"dropoff_latitude\"].between(left=40, right=42)]\n    df = df[df[\"dropoff_longitude\"].between(left=-74, right=-72.9)]\n    return df\ndata_train = clean_data(data_train)\ndata_train.head()","7bfc559c":"X_train = data_train.drop([\"fare_amount\"], axis=1)\ny_train = data_train[\"fare_amount\"]","a6b20a63":"class TimeFeaturesEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"Extract the day of week (dow), the hour, the month and the year from a time column.\"\"\"\n\n    def __init__(self, time_column, time_zone_name='America\/New_York'):\n        self.time_column = time_column\n        self.time_zone_name = time_zone_name\n\n    def extract_time_features(self, X):\n        timezone_name = self.time_zone_name\n        time_column = self.time_column\n        df = X.copy()\n        df.index = pd.to_datetime(df[time_column])\n        df.index = df.index.tz_convert(timezone_name)\n        df[\"dow\"] = df.index.weekday\n        df[\"hour\"] = df.index.hour\n        df[\"month\"] = df.index.month\n        df[\"year\"] = df.index.year        \n        return df\n        \n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        \"\"\"Returns a copy of the DataFrame X with only four columns: 'dow', 'hour', 'month', 'year'\"\"\"\n        return self.extract_time_features(X)[['dow', 'hour', 'month', 'year']].reset_index(drop=True)","e20db490":"# test the TimeFeaturesEncoder\ntime_enc = TimeFeaturesEncoder('pickup_datetime')\ntime_features = time_enc.fit_transform(X_train, y_train)\ntime_features.head()","9c4cf217":"# TIME PIPELINE\npipe_time = Pipeline([\n    ('time_features_create', TimeFeaturesEncoder('pickup_datetime')),\n    ('time_features_ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\npipe_time","247d7cbe":"def haversine_vectorized(df, \n         start_lat=\"pickup_latitude\",\n         start_lon=\"pickup_longitude\",\n         end_lat=\"dropoff_latitude\",\n         end_lon=\"dropoff_longitude\"):\n\n    \"\"\" \n        Calculate the great circle distance between two points \n        on the earth (specified in decimal degrees).\n        Vectorized version of the haversine distance for pandas df\n        Computes distance in kms\n    \"\"\"\n\n    lat_1_rad, lon_1_rad = np.radians(df[start_lat].astype(float)), np.radians(df[start_lon].astype(float))\n    lat_2_rad, lon_2_rad = np.radians(df[end_lat].astype(float)), np.radians(df[end_lon].astype(float))\n    dlon = lon_2_rad - lon_1_rad\n    dlat = lat_2_rad - lat_1_rad\n\n    a = np.sin(dlat \/ 2.0) ** 2 + np.cos(lat_1_rad) * np.cos(lat_2_rad) * np.sin(dlon \/ 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return 6371 * c","d854b41c":"class DistanceTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Compute the haversine distance between two GPS points.\"\"\"\n\n    def __init__(self, \n                 start_lat=\"pickup_latitude\",\n                 start_lon=\"pickup_longitude\", \n                 end_lat=\"dropoff_latitude\", \n                 end_lon=\"dropoff_longitude\"):\n        self.start_lat = start_lat\n        self.start_lon = start_lon\n        self.end_lat = end_lat\n        self.end_lon = end_lon\n\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        \"\"\"Returns a copy of the DataFrame X with only one column: 'distance'\"\"\"\n        return pd.DataFrame(haversine_vectorized(X)).rename(columns={0: \"course distance [km]\"}).copy()","e458739d":"dist_trans = DistanceTransformer()\ndistance = dist_trans.fit_transform(X_train, y_train)\ndistance.head()","807c830a":"preprocessor = ColumnTransformer([\n    ('distance', DistanceTransformer(), ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']),\n    ('time', pipe_time, ['pickup_datetime'])\n], remainder='passthrough')\npreprocessor","8ffbcdd7":"pipe_prepro = Pipeline([\n    ('dist_and_time', preprocessor),\n    ('scaler', MinMaxScaler())\n])\npipe_prepro","f9194438":"final_pipe = Pipeline([\n    ('preprocessor', pipe_prepro),\n    ('model', RandomForestRegressor())\n])\nfinal_pipe","e60a9310":"def custom_rmse(y_true, y_pred):\n    return np.sqrt(np.mean(np.square(y_true - y_pred)))","735fee0d":"rmse = make_scorer(custom_rmse, greater_is_better=False)","d2d9b771":"baseline = cross_validate(final_pipe,\n                          X_train,\n                          y_train,\n                          scoring=rmse,\n                          cv=5)\nbaseline_rmse = -round(baseline[\"test_score\"].mean(), 3)\nbaseline_rmse","dc6ccd00":"grid_RFR = {'model__n_estimators': stats.randint(1, 300),\n            'model__max_depth': stats.randint(1, 300),\n            'model__max_samples': stats.randint(1, 300),\n            \"preprocessor__scaler\": [StandardScaler(), RobustScaler(), MinMaxScaler()]\n            }\n\nsearch_RFR = RandomizedSearchCV(final_pipe,\n                                grid_RFR,\n                                scoring=rmse,\n                                n_iter=100,\n                                cv=5,\n                                n_jobs=-1,\n                                verbose=True)\nsearch_RFR.fit(X_train, y_train);","7e12b203":"search_RFR.best_params_","ea8383e7":"print(\"Tuned RandomForestRegressor model rmse: \" + str(-round(search_RFR.best_score_, 2)))","a0cd1bf6":"### 6.3.3. Combination of distance and time features pipeline","8bb6fe20":"<div style=\"font-weight:700\">Let's have a look at the corelation between <i style=\"color:royalblue\">fare_amount<\/i> and <i style=\"color:royalblue\">distance<\/i> features<\/div>","79b1038e":"# 4. Missing\n<div style=\"font-weight:700\">Droping features that have too many missing values<\/div>","f9adf238":"<div style=\"font-weight:700\">Taxi trip repartition by hour of the day<\/div>","f79e93c9":"<div style=\"font-weight:700\">Let's have a look at <i style=\"color:royalblue\">passenger_count<\/i> feature<\/div>","81bb6c13":"#### 6.3.1.3. Pipeline for time features","f98613d3":"### 6.3.1. Time features","91b862e6":"<div style=\"font-weight:700\">Let's have a look at <span style=\"color:royalblue;font-variant:small-caps\">geospatial<\/span> features<\/div>\n<br\/>\n<div style=\"font-weight:700\">Find boudaries from test set and remove outliers from training set<\/div>","56e7422d":"<div style=\"font-weight:700\">Taxi trip repartition by day of week<\/div>","5459102a":"#### 6.3.2.1 Class for distance encoding","78364c3a":"# 2. Dropping useless features","6f522ad5":"# 5. Explore Data","0b094f4b":"## 6.1. Cleaning dataset","a40ded5a":"#### 6.3.1.2. Test of the TimeFeaturesEncoder","1cc83406":"# 6. Training\n<div style=\"font-weight:700\">Starting on a fresh dataset to prepare for training<\/div>","86385e93":"### 6.3.2. Distances","8d6e7eef":"<div style=\"font-weight:700\">We can also visualise binned fare_amount variable:<\/div>","6ef56816":"# 1. Import","718c2ed2":"## 6.4. Definition of custom score for RMSE","08726dc6":"### 6.3.5. Finally, Full pipeline. We choose RandomForestRegressor","a4eafdbd":"<div style=\"font-weight:700\">Let's have a look at <span style=\"color:royalblue;font-variant:small-caps\">distance<\/span> features<\/div>\n<br\/>\n<div style=\"font-weight:700\">Here it is a method to compute distance between two point from their geospatial coordinates<\/div>","34a83455":"## 6.3. Pipelines","63f334e1":"This workbook is the New York City Taxi Fare Prediction dataset analysis to predict the fare of a taxi trip in New York City.\n\nIt is a Kaggle competition available <a href=\"https:\/\/www.kaggle.com\/c\/new-york-city-taxi-fare-prediction\" target=\"_blank\">HERE<\/a>.\n\n<i><blockquote><b>The Challenge<\/b><\/blockquote><\/i>\n    \n<blockquote>In this playground competition, hosted in partnership with Google Cloud and Coursera, you are tasked with predicting the fare amount (inclusive of tolls) for a taxi ride in New York City given the pickup and dropoff locations. While you can get a basic estimate based on just the distance between the two points, this will result in an RMSE of $5-$8, depending on the model used (see the starter code for an example of this approach in Kernels). Your challenge is to do better than this using Machine Learning techniques!\n\nTo learn how to handle large datasets with ease and solve this problem using TensorFlow, consider taking the Machine Learning with TensorFlow on Google Cloud Platform specialization on Coursera -- the taxi fare problem is one of several real-world problems that are used as case studies in the series of courses. To make this easier, head to Coursera.org\/NEXTextended to claim this specialization for free for the first month!<\/blockquote>\n<figcaption>Kaggle<\/figcaption>","4c2d7e9c":"#### 6.3.2.2. Test of the DistanceTransformer","a28fb68f":"<div style=\"font-size:30pt; font-weight:700;margin-top:50px;margin-bottom:50px;color:royalblue; text-align:center;width:800px;line-height:20pt\">Kaggle New York City Taxi Fare Prediction<\/div>","5a0a0bf5":"## 6.6. RandomizedSearchCV for an optimized model","759b472d":"<div style=\"font-weight:700\">Seems there is nothing to drop.<\/div>","cc949b14":"<div style=\"font-weight:700\">Let's have a look at <i style=\"color:royalblue\">fare_amount<\/i> feature by day of week<\/div>","27326793":"### 6.3.4. Full preprocessor pipeline","eec46eef":"# 3. Duplicates","64c6c463":"<div style=\"font-weight:700\">Let's have a closer look at <i style=\"color:royalblue\">passenger_count<\/i> feature<\/div>","4b84c63d":"<div style=\"font-weight:700\">Let's drop absurd values.<\/div>","92882a8a":"## 6.2. Preparing model inputs","768d0a2d":"## 6.5. Baseline RMSE for RandomForestRegressor","a44b1ff7":"#### 6.3.1.1 Class for time features encoding","855e7def":"<div style=\"font-weight:700\">Let's have a look at <i style=\"color:royalblue\">fare_amount<\/i> feature by hour<\/div>","da080300":"<div style=\"font-weight:700\">Let's have a look at <i style=\"color:royalblue\">pickup_datetime<\/i> feature<\/div>"}}