{"cell_type":{"564aed18":"code","7099263c":"code","47f2dd65":"code","64b92e92":"code","46084dc9":"code","bd7e1cc0":"code","977ce211":"code","e9a584b5":"code","373f10f8":"code","73ce976b":"code","8d30ab65":"code","f6c54971":"code","9aaf0c5d":"code","52fccb4e":"code","0ee2a397":"code","ca1b1438":"code","cf00d655":"code","0827be34":"code","9ff12fe5":"code","9482646c":"code","cdd710a7":"code","f3529ae0":"code","07dcd755":"markdown","4b4bdf86":"markdown","4c9ac1c3":"markdown","64e296d8":"markdown","83d5a8f0":"markdown","f0413b21":"markdown","129e2a46":"markdown","509ad1c8":"markdown","fb059e5a":"markdown","a5cfffaa":"markdown","8e65c80a":"markdown","39734fee":"markdown","91673502":"markdown","333a9e03":"markdown","932cd402":"markdown","44c48d23":"markdown","bc32e625":"markdown"},"source":{"564aed18":"%%capture\n# install tensorflow 2.0 alpha\n!pip install -q tensorflow-gpu==2.0.0-alpha0\n\n#install GapCV\n!pip install -q gapcv","7099263c":"import os\nimport time\nimport gc\nimport shutil\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras import regularizers\n\nimport gapcv\nfrom gapcv.vision import Images\nfrom gapcv.utils.img_tools import ImgUtils\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nprint('tensorflow version: ', tf.__version__)\nprint('keras version: ', tf.keras.__version__)\nprint('gapcv version: ', gapcv.__version__)","47f2dd65":"os.makedirs('model', exist_ok=True)\nprint(os.listdir('..\/input'))\nprint(os.listdir('.\/'))","64b92e92":"def plot_sample(imgs_set, labels_set, img_size=(12,12), columns=4, rows=4, random=False):\n    \"\"\"\n    Plot a sample of images\n    \"\"\"\n    \n    fig=plt.figure(figsize=img_size)\n    \n    for i in range(1, columns*rows + 1):\n        \n        if random:\n            img_x = np.random.randint(0, len(imgs_set))\n        else:\n            img_x = i-1\n        \n        img = imgs_set[img_x]\n        ax = fig.add_subplot(rows, columns, i)\n        ax.set_title(str(labels_set[img_x]))\n        plt.axis('off')\n        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    plt.show()","46084dc9":"wildlife_filter = ['black_bear', 'bald_eagle', 'cougar', 'elk', 'gray_wolf']\n\nfor folder in os.scandir('..\/input\/oregon_wildlife\/oregon_wildlife'):\n    if folder.name in wildlife_filter:\n        shutil.copytree(folder.path, '.\/oregon_wildlife\/{}'.format(folder.name))\n        print('{} copied from main data set'.format(folder.name))","bd7e1cc0":"!ls -l oregon_wildlife","977ce211":"data_set = 'wildlife'\ndata_set_folder = '.\/oregon_wildlife'\nimg_height = 128 \nimg_width = 128\nbatch_size = 32\nnb_epochs = 50","e9a584b5":"def model_def(img_height, img_width):\n    return Sequential([\n        layers.Conv2D(filters=128, kernel_size=(4, 4), activation='tanh', input_shape=(img_height, img_width, 3)),\n        layers.MaxPool2D(pool_size=(2,2)),\n        layers.Dropout(0.22018745727040784),\n        layers.Conv2D(filters=64, kernel_size=(4, 4), activation='relu'),\n        layers.MaxPool2D(pool_size=(2,2)),\n        layers.Dropout(0.02990527559235584),\n        layers.Conv2D(filters=32, kernel_size=(4, 4), activation='tanh'),\n        layers.MaxPool2D(pool_size=(2,2)),\n        layers.Dropout(0.0015225556862044631),\n        layers.Conv2D(filters=32, kernel_size=(4, 4), activation='tanh'),\n        layers.MaxPool2D(pool_size=(2,2)),\n        layers.Dropout(0.1207251417283281),\n        layers.Flatten(),\n        layers.Dense(256, activation='relu'),\n        layers.Dropout(0.4724418446300173),\n        layers.Dense(len(wildlife_filter), activation='softmax')\n    ])","373f10f8":"image_datagen = ImageDataGenerator(\n    rescale=1.\/255,\n    validation_split=0.2 # set validation split\n)\n\ntrain_generator = image_datagen.flow_from_directory(\n    data_set_folder,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training' # set as training data\n)\n\nvalidation_generator = image_datagen.flow_from_directory(\n    data_set_folder, # same directory as training data\n    target_size=(img_height, img_width),\n    batch_size=batch_size, # tried to set 703 manually to compare apples with apples but performance was way worse\n    class_mode='categorical',\n    subset='validation' # set as validation data\n)","73ce976b":"model = model_def(img_height, img_width)\nmodel.summary()","8d30ab65":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","f6c54971":"!free -m","9aaf0c5d":"%%time\nmodel.fit_generator(\n    train_generator,\n    steps_per_epoch=train_generator.samples \/\/ batch_size,\n    validation_data=validation_generator, \n    validation_steps=validation_generator.samples \/\/ batch_size,\n    epochs = nb_epochs\n)","52fccb4e":"images = Images(data_set, data_set_folder, config=['resize=({},{})'.format(img_height, img_width), 'store', 'stream'])\n# explore directory to see if the h5 file is there\n!ls","0ee2a397":"# stream from h5 file\nimages = Images(config=['stream'])\nimages.load(data_set, '..\/input')","ca1b1438":"# split data set\nimages.split = 0.2\nX_test, Y_test = images.test\n\n# generator\nimages.minibatch = batch_size\ngap_generator = images.minibatch","cf00d655":"total_train_images = images.count - len(X_test)\nn_classes = len(images.classes)\n\nprint('content:', os.listdir(\".\/\"))\nprint('time to preprocess the data set:', images.elapsed)\nprint('number of images in data set:', images.count)\nprint('classes:', images.classes)\nprint('data type:', images.dtype)","0827be34":"shutil.rmtree(data_set_folder) # delete images folder for kaggle kernel limitations\nK.clear_session() # clean previous model\ngc.collect()","9ff12fe5":"model = model_def(img_height, img_width)\nmodel.summary()","9482646c":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","cdd710a7":"!free -m","f3529ae0":"%%time\nmodel.fit_generator(\n    generator=gap_generator,\n    validation_data=(X_test, Y_test),\n    steps_per_epoch=total_train_images \/\/ batch_size,\n    epochs=nb_epochs,\n    verbose=1\n)","07dcd755":"## data sample\n\nLet's get a sample of 5 classes from the data to reduce preprocesing and training times instead of using the whole data set.","4b4bdf86":"Here get can get some information about our data set:","4c9ac1c3":"As we can see the elapsed time using `flow_from_directory` from keras. We got over an hour to complete training of 50 epochs, with 88 steps to load each batch of 32 images per step. It's interesting to see how the time was between 75 to 88 sec per epoch. Out of the scope of this notebook but notice the accuracy and val_accuracy at the final epoch.","64e296d8":"# OREGON WILDLIFE - TENSORFLOW 2.0 + KERAS vs GAPCV\n\nIf you are wondering why I'm using [gapcv](https:\/\/gapml.github.io\/CV\/) instead of [flow_from_directory](https:\/\/keras.io\/preprocessing\/image\/) from `tf.keras`. Let me show you in this notebook why.\n\nI'm going to compare both libraries and their performance when we need to preprocess images to fit a model.","83d5a8f0":"Woah! What a difference almost 3 minutes to complete the whole traing of 50 epochs, with 88 steps to load each batch of 32 images per step. As we can see the time per epoch was constant 3 sec each (after the first one). Plus check the accuracy!\n\nFor `gapcv` was 2:40 minutes plus 15 minutes to preprocess the data but once we have the `h5` file. It is done, we don't have to create it again and it's way easy to share with the team.\n\nNow if we are using some library for hyperparameters optimization such as hyperas or grid search from `sklearn`. Where we have to run several evaluations and train several models with diferent hyperparameters. We don't want to wait so long to get our results plus we can add callbacks and stop unusefull trainings. Even worse if we are paying cloud computing! We want to expend as low as posible and don't ruin our budgets.","f0413b21":"## Keras Image Preprocessing\n\nLet's start with keras defining `ImageDataGenerator` where we will get:\n\n* normalization,\n* augmentation:\n    * zoom = 0.3\n    * horizontal flip = True\n* split the data set between train and validation by 20%\n\nand get a couple generators for training and validation data to fit the model later","129e2a46":"OK! Let's define and train our model!","509ad1c8":"## install tensorboard and gapcv","fb059e5a":"## utils functions","a5cfffaa":"## Metadata\n\nLet's compare apples with apples for this I'm going to have the same metadata for both trainings. ","8e65c80a":"## GapCV image preprocessing\n\nNow let's test `gapcv` `mini_batch` generator. The first thing that gap does is create a preprocessed `h5` file where we will find our data set already transform using numpy arrays to normalize the data.\n\nWith this line of code we will use three configurations:\n\n1. image resize\n2. store: to save the information in a `h5` file\n3. stream: allows the flow of preprocessed image directly to the `h5` file without save it in-memory (ideal if we have limitated CPU, GPU, or TPU resources)","39734fee":"## Model definition\n\nWe are going to use the same model definition I used on a previous notebook into a function to call it later.","91673502":"## import libraries","333a9e03":"## training","932cd402":"Let's split the data set as well as we did with keras 20% for validation and the rest for training. Then define the generator:","44c48d23":"## Keras model definition\n\nNow let's clean the previous session to start the `keras` model all over again","bc32e625":"Once the `h5` file is created we can call it again in `stream` mode and also apply image augmentation while the images are getting fit into the model. If we have a `mini_batch` of 32. `gapcv` does is split the data (1\/2) and fit the model with half of the images augmented and the other half without augmentation. Even shuffles the data on each epoch so we have a complete new set of images to fit our model."}}