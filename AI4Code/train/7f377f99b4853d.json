{"cell_type":{"4421abf3":"code","2691f541":"code","3610f5db":"code","56f86aa0":"code","567e939f":"code","c0dfff08":"code","d2436d44":"code","3bbd64ec":"code","3894e082":"code","58621f8c":"code","7164d33e":"code","98c3f706":"code","293b7677":"code","1812bbec":"code","629f6c54":"code","84900419":"code","1a4ee3f4":"code","58ba729c":"code","008105a3":"code","9c6bf35a":"code","00b20047":"code","cf249605":"markdown","d3aba7a1":"markdown","7e926170":"markdown","53356153":"markdown","bb7d98a1":"markdown","71a702a1":"markdown","ba0756bb":"markdown","852c6d5e":"markdown","a667c203":"markdown","dbc1e740":"markdown","5fda588a":"markdown","be66c489":"markdown","f97017c5":"markdown","d0cc4f24":"markdown","cacb86de":"markdown","2b9eb99a":"markdown","7f085e77":"markdown","19badfdb":"markdown"},"source":{"4421abf3":"# Pick any one of the following Twi models\n#MODEL = \"Ghana-NLP\/abena-base-akuapem-twi-cased\" # (Akuapem ABENA) mBERT fine-tuned on JW300 Akuapem Twi, cased\n#MODEL = \"Ghana-NLP\/abena-base-asante-twi-uncased\" # (Asante ABENA) Akuapem ABENA fine-tuned on Asante Twi Bible, uncased\nMODEL = \"Ghana-NLP\/distilabena-base-akuapem-twi-cased\" # (Akuapem DistilABENA) DistilmBERT fine-tuned on JW300 Akuapem Twi, cased\n#MODEL = \"Ghana-NLP\/distilabena-base-v2-akuapem-twi-cased\" # (Akuapem DistilABENA V2) DistilmBERT fine-tuned on JW300 Akuapem Twi with Twi-only tokenizer trained from scratch, cased\n#MODEL = \"Ghana-NLP\/distilabena-base-asante-twi-uncased\" # (Asante DistilABENA) Akuapem DistilABENA fine-tuned on Asante Bible, uncased\n#MODEL = \"Ghana-NLP\/distilabena-base-v2-asante-twi-uncased\" # (Asante DistilABENA V2) Akuapem DistilABENA V2 fine-tuned on Asante Bible, uncased\n#MODEL = \"Ghana-NLP\/robako-base-akuapem-twi-cased\" # (Akuapem RoBAKO) RoBERTa trained from scratch on JW300 Akuapem Twi, cased [note - use <mask> not [MASK] to represent blank in sentence]\n#MODEL = \"Ghana-NLP\/robako-base-asante-twi-uncased\" # (Asante RoBAKO) Akuapem RoBAKO fine-tuned on Asante Twi Bible, uncased [note - use <mask> not [MASK] to represent blank in sentence]","2691f541":"from transformers import pipeline\n\nfill_mask = pipeline(\n    \"fill-mask\",\n    model=MODEL,\n    tokenizer=MODEL\n)\n\nprint(fill_mask(\"Saa tebea yi maa me papa [MASK].\")) # if using ABENA\n\n#print(fill_mask(\"Saa tebea yi maa me papa <mask>.\")) # if using BAKO\n","3610f5db":"print(fill_mask(\"Eyi de \u0254haw k\u025bse baa [MASK] h\u0254.\")) # if using ABENA\n\n#print(fill_mask(\"Eyi de \u0254haw k\u025bse baa <mask> h\u0254.\")) # if using BAKO","56f86aa0":"from transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nmodel = AutoModelForMaskedLM.from_pretrained(MODEL)","567e939f":"input_ids = tokenizer(\"Eyi de \u0254haw k\u025bse baa fie h\u0254\", return_tensors=\"pt\")[\"input_ids\"] # these are indices of tokens in the vocabulary\nprint(input_ids)","c0dfff08":"decoded_tokens = [tokenizer.decode(int(el)) for el in input_ids[0]]\nprint(decoded_tokens)","d2436d44":"import torch\nimport numpy as np\n\ndef get_embedding(in_str,model):\n    input_ids = torch.tensor(tokenizer.encode(in_str)).unsqueeze(0)  # Batch has size 1\n    outputs = model(input_ids)\n    hidden_states = outputs[1]  # The embedding vectors are a tuple of length equal to number of layers\n    embedding_vecs = hidden_states[-1].detach().numpy()[0] # these vectors are n_tokens by 768 in size\n    CLS_embedding_vec = embedding_vecs[0] # the CLS token is usually used as the average representation for classification\n    average_vec = np.average(embedding_vecs[1:-1],axis=0) # averaging remaining vectors instead for similarity task yields slightly better results\n    \n    return average_vec\n","3bbd64ec":"'''in_str = \"Eyi de \u0254haw k\u025bse baa fie h\u0254\"\ntorch.tensor(tokenizer.encode(in_str)).unsqueeze(0)\noutputs = model(input_ids)'''","3894e082":"#outputs[1][-1].detach().numpy()[0][1:-1].shape","58621f8c":"from transformers import AutoConfig\nconfig = AutoConfig.from_pretrained(MODEL)\nconfig.output_hidden_states=True\nmodel = AutoModelForMaskedLM.from_pretrained(MODEL,config=config)\nmodel.eval()","7164d33e":"vec = get_embedding(\"Eyi de \u0254haw k\u025bse baa fie h\u0254\",model)\nprint(\"The vector representation of the sentence is:\")\nprint(vec)","98c3f706":"print(\"The shape of the vector is:\")\nprint(vec.shape)","293b7677":"import pandas\n\ndata_df = pandas.read_csv(\"..\/input\/twi-sentiment-analysis-unit-dataset\/sentiment_analysis_unit_dataset.csv\")\ndata_df = data_df.sample(frac=1) # shuffle\nprint(data_df)","1812bbec":"train_data = data_df[:14][\"Sentence\"].values # use 14 out of the 20 as training, i.e., val ratio of 30%\ntrain_labels = data_df[:14][\"Label (1 is +ve)\"].values\ntest_data = data_df[14:][\"Sentence\"].values # use 6 out of the 20 as testing\ntest_labels = data_df[14:][\"Label (1 is +ve)\"].values","629f6c54":"print(test_data)","84900419":"print(\"Checking testing data:\")\nprint(test_data)\nprint(test_labels)","1a4ee3f4":"X_train_list = [get_embedding(sent,model) for sent in train_data] # vectorize\/generate features for training\nX_train = np.asarray(X_train_list)\ny_train = train_labels\nprint(\"Training data shape is:\")\nprint(X_train.shape)","58ba729c":"X_test_list = [get_embedding(sent,model) for sent in test_data] # vectorize\/generate features for testing\nX_test = np.asarray(X_test_list)\ny_test = test_labels\nprint(\"Testing data shape is:\")\nprint(X_test.shape)","008105a3":"from sklearn.neighbors import KNeighborsClassifier # use a simple sklearn nearest neighbor classifier\n\nCLF = KNeighborsClassifier(n_neighbors=1)\nCLF.fit(X_train, y_train)\n","9c6bf35a":"y_pred = CLF.predict(X_test)\nprint(y_pred)","00b20047":"np.average(y_test==y_pred)","cf249605":"This is how you would load the model and tokenizer for further use by [transformers](https:\/\/github.com\/huggingface\/transformers):","d3aba7a1":"Now, vectorize the dataset, fit and test a simple nearest neighbour classifier","7e926170":"## Extract Vector Representation For Sentence","53356153":"## Fill-in-the-blanks","bb7d98a1":"Then test:","71a702a1":"# [GhanaNLP](https:\/\/ghananlp.org) - ABENA Usage DEMO\n\nThis is a short demo on how to use GhanaNLP's ABENA (and BAKO) family of transformer-based language models for Akuapem and Asante Twi. Models are described in the following [paper](https:\/\/arxiv.org\/abs\/2103.15963) presented at the [2nd AfricaNLP Workshop collocated with EACL 2021](https:\/\/sites.google.com\/view\/africanlp-workshop)","ba0756bb":"## Classification Example\n\nWe built a simple sentiment analysis dataset of just 20 samples and want to see if we can use the features shown above to classify them.\n\nFirst, let's load our dataset and display it.","852c6d5e":"Now, we are ready to train the classifier and test it... First train:","a667c203":"This function will extract an average of pretrained vectors of all tokens in a sentence (variable `average_vec`). You can modify this function to get embeddings for all individual tokens before the average back (`embedding_vecs`) or the CLS (always first in BERT-type models) token used in BERT for average representation of the entire sequence (`CLS_embedding_vec`). Depending on the application, one of these might work better than others. ","dbc1e740":"We can clearly see the subword nature of the tokenization from this.","5fda588a":"To use this function, you need to make sure the model outputs the hidden states, which are the representation vectors we are looking for","be66c489":"To see what exactly these tokens are, decode them:","f97017c5":"For instance, let us tokenize and encode a sentence:","d0cc4f24":"First, Use the [Hugging Face](https:\/\/huggingface.co\/) transformers *pipelines* API to fill-in-the blanks for a sample sentence using the Akan\/Twi BERT Model ABENA","cacb86de":"Another sentence:","2b9eb99a":"Compute Accuracy:","7f085e77":"## Encode a Sentence With Tokenizer","19badfdb":"So now, let's actually use this function to get an average vector representation for a twi sentence."}}