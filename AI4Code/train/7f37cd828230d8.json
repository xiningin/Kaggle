{"cell_type":{"2d5286fb":"code","2198cdd6":"code","539d93e8":"code","3f34714b":"code","e5b18636":"code","6b012e5a":"code","2fe25edd":"code","cbd1ab65":"code","4077d94c":"code","0cde0f0c":"code","c3babaa8":"code","a02cb050":"code","2d91aa64":"code","4c287648":"code","6f361964":"code","f23ea214":"code","9f3be446":"code","4fb0cc1b":"code","6dae8819":"code","fafd8ac8":"code","493d2879":"code","f7156016":"code","45dd53f5":"code","ba4a11ca":"code","c6262f06":"code","3b76b6d2":"code","c9970af9":"code","7370e961":"code","d96c64b6":"code","4df6f143":"code","543108b3":"code","abd3c4fb":"code","ed45cde0":"code","01222c97":"code","e494ef7d":"code","78aae36f":"code","59e7c1ab":"code","0f915b4a":"code","f658f432":"code","592440a7":"code","7e2f853f":"code","390f9c73":"code","95c4a5f0":"code","5c898d9a":"code","ff81d96b":"code","7b89f757":"code","cc9d117a":"code","a0fcbab3":"code","a1b49ee4":"code","9bfec148":"code","adafd74d":"code","18a7a265":"code","9d95b453":"code","f833d0bd":"code","48af410f":"code","8db7b94e":"code","9b551f7b":"code","40b91e85":"markdown","aaf44711":"markdown","bdabf15f":"markdown","e0d72c21":"markdown","5955f4d0":"markdown","da3a3e09":"markdown","62044809":"markdown","c39430e0":"markdown","1f1dcd1b":"markdown","e7a5aaf0":"markdown","1dbfc925":"markdown","4bc16645":"markdown","9d826e9d":"markdown","d2ccf50c":"markdown","81f28cc0":"markdown","5bcd2862":"markdown","18ae6f7c":"markdown","2c2bf5ef":"markdown","ed473bac":"markdown"},"source":{"2d5286fb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom fancyimpute import KNN\nimport pyodbc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2198cdd6":"df = pd.read_csv(\"\/kaggle\/input\/clv-prediction-data\/CLV_Train.csv\")\ndf.head()","539d93e8":"class Attribute_Information():\n    \n\n    def __init__(self):\n        \n        print(\"Attribute Information object created\")\n        \n        \n        \n    def Column_information(self,df):\n        \n        \"\"\"\n        This method will give us a basic\n        information of the dataframe like\n        Count of Attributes,Count of rows,\n        Numerical Attributes, Categorical \n        Attributes, Factor Attributes etc..\n        \"\"\"\n    \n        data_info = pd.DataFrame(\n                                columns=['No of observation',\n                                        'No of Variables',\n                                        'No of Numerical Variables',\n                                        'No of Factor Variables',\n                                        'No of Categorical Variables',\n                                        'No of Logical Variables',\n                                        'No of Date Variables',\n                                        'No of zero variance variables'])\n\n\n        data_info.loc[0,'No of observation'] = df.shape[0]\n        data_info.loc[0,'No of Variables'] = df.shape[1]\n        data_info.loc[0,'No of Numerical Variables'] = df._get_numeric_data().shape[1]\n        data_info.loc[0,'No of Factor Variables'] = df.select_dtypes(include='category').shape[1]\n        data_info.loc[0,'No of Logical Variables'] = df.select_dtypes(include='bool').shape[1]\n        data_info.loc[0,'No of Categorical Variables'] = df.select_dtypes(include='object').shape[1]\n        data_info.loc[0,'No of Date Variables'] = df.select_dtypes(include='datetime64').shape[1]\n        data_info.loc[0,'No of zero variance variables'] = df.loc[:,df.apply(pd.Series.nunique)==1].shape[1]\n\n        data_info =data_info.transpose()\n        data_info.columns=['value']\n        data_info['value'] = data_info['value'].astype(int)\n\n\n        return data_info\n\n    def __get_missing_values(self,data):\n        \n        \"\"\"\n        It is a Private method, so it cannot \n        be accessed by object outside the \n        class. This function will give us \n        a basic information like count \n        of missing values\n        \"\"\"\n        \n        #Getting sum of missing values for each feature\n        missing_values = data.isnull().sum()\n        #Feature missing values are sorted from few to many\n        missing_values.sort_values(ascending=False, inplace=True)\n        \n        #Returning missing values\n        return missing_values\n\n    def Generate_Schema(self,data):\n        \n        \n        \"\"\"\n        This method will gives you a \n        overall schema of dataframe with\n        Attributes names,types,missing \n        values and sample observations\n        \"\"\"\n        \n        feature_dtypes=data.dtypes\n        self.missing_values=self.__get_missing_values(data)\n\n        print(\"=\" * 110)\n\n        print(\"{:16} {:16} {:20} {:16}\".format(\"Feature Name\".upper(),\n                                            \"Data Type\".upper(),\n                                            \"# of Missing Values\".upper(),\n                                            \"Samples\".upper()))\n        for feature_name, dtype, missing_value in zip(self.missing_values.index.values,\n                                                      feature_dtypes[self.missing_values.index.values],\n                                                      self.missing_values.values):\n            print(\"{:18} {:19} {:19} \".format(feature_name, str(dtype), str(missing_value)), end=\"\")\n            for v in data[feature_name].values[:5]:\n                print(v, end=\",\")\n            print()\n\n        print(\"=\"*110)\n        \n    def Agg_Tabulation(self,data):\n        \n        \n        \"\"\"\n        This method is a extension of \n        schema will gives the aditional \n        information about the data\n        like Entropy value, Missing \n        Value Percentage and some observations\n        \"\"\"\n        \n        print(\"=\" * 110)\n        print(\"Aggregation of Table\")\n        print(\"=\" * 110)\n        table = pd.DataFrame(data.dtypes,columns=['dtypes'])\n        table1 =pd.DataFrame(data.columns,columns=['Names'])\n        table = table.reset_index()\n        table= table.rename(columns={'index':'Name'})\n        table['No of Missing'] = data.isnull().sum().values    \n        table['No of Uniques'] = data.nunique().values\n        table['Percent of Missing'] = ((data.isnull().sum().values)\/ (data.shape[0])) *100\n        table['First Observation'] = data.loc[0].values\n        table['Second Observation'] = data.loc[1].values\n        table['Third Observation'] = data.loc[2].values\n        for name in table['Name'].value_counts().index:\n            table.loc[table['Name'] == name, 'Entropy'] = round(stats.entropy(data[name].value_counts(normalize=True), base=2),2)\n        return table\n    \n        print(\"=\" * 110)\n        \n    def __iqr(self,x):\n        \n        \n        \"\"\"\n        It is a private method which \n        returns you interquartile Range\n        \"\"\"\n        return x.quantile(q=0.75) - x.quantile(q=0.25)\n\n    def __outlier_count(self,x):\n        \n        \n        \"\"\"\n        It is a private method which \n        returns you outlier present\n        in the interquartile Range\n        \"\"\"\n        upper_out = x.quantile(q=0.75) + 1.5 * self.__iqr(x)\n        lower_out = x.quantile(q=0.25) - 1.5 * self.__iqr(x)\n        return len(x[x > upper_out]) + len(x[x < lower_out])\n\n    def num_count_summary(self,df):\n        \n        \n        \"\"\"\n        This method will returns \n        you the information about\n        numerical attributes like\n        Positive values,Negative Values\n        Unique count, Zero count \n        positive and negative inf-\n        nity count and count of outliers\n        etc \n        \n        \"\"\"\n        \n        df_num = df._get_numeric_data()\n        data_info_num = pd.DataFrame()\n        i=0\n        for c in  df_num.columns:\n            data_info_num.loc[c,'Negative values count']= df_num[df_num[c]<0].shape[0]\n            data_info_num.loc[c,'Positive values count']= df_num[df_num[c]>0].shape[0]\n            data_info_num.loc[c,'Zero count']= df_num[df_num[c]==0].shape[0]\n            data_info_num.loc[c,'Unique count']= len(df_num[c].unique())\n            data_info_num.loc[c,'Negative Infinity count']= df_num[df_num[c]== -np.inf].shape[0]\n            data_info_num.loc[c,'Positive Infinity count']= df_num[df_num[c]== np.inf].shape[0]\n            data_info_num.loc[c,'Missing Percentage']= df_num[df_num[c].isnull()].shape[0]\/ df_num.shape[0]\n            data_info_num.loc[c,'Count of outliers']= self.__outlier_count(df_num[c])\n            i = i+1\n        return data_info_num\n    \n    def statistical_summary(self,df):\n        \n        \n        \"\"\"\n        This method will returns \n        you the varoius percentile\n        of the data including count \n        and mean\n        \"\"\"\n    \n        df_num = df._get_numeric_data()\n\n        data_stat_num = pd.DataFrame()\n\n        try:\n            data_stat_num = pd.concat([df_num.describe().transpose(),\n                                       pd.DataFrame(df_num.quantile(q=0.10)),\n                                       pd.DataFrame(df_num.quantile(q=0.90)),\n                                       pd.DataFrame(df_num.quantile(q=0.95))],axis=1)\n            data_stat_num.columns = ['count','mean','std','min','25%','50%','75%','max','10%','90%','95%']\n        except:\n            pass\n\n        return data_stat_num","3f34714b":"Info = Attribute_Information()","e5b18636":"Info.Column_information(df)","6b012e5a":"Info.Generate_Schema(df)","2fe25edd":"Info.Agg_Tabulation(df)","cbd1ab65":"Info.num_count_summary(df)","4077d94c":"Info.statistical_summary(df)","0cde0f0c":"import math\n\nclass Compute_Haversine_Distance():\n    \n\n    def __init__(self):\n        print(\"Distance object created\")\n        \n    \n    def convert_income_tonumeric(self,df):\n        df['Income']= pd.to_numeric(df['Income'],errors='coerce')\n\n        \n    def Split_Location_geo(self,Location_Geo):\n        \n        \"\"\"\n        This method is created to split\n        the Location Geo variable into\n        Lati and Longi by comma seperated\n        values to compute distance\n        \"\"\"\n        \n        df['Lati'], df['Longi'] = \\\n        df['Location.Geo'].str.split(',', 1).str\n        \n\n    def harvesine_distance(self,lati,longi):\n        \n        \"\"\"\n        This method helps to compute the \n        distance between the points of \n        latitude and longitude by replicating\n        the Haversine formula\n        \"\"\"\n        \n        df['Lati']= pd.to_numeric(df['Lati'],errors='coerce')\n        df['Longi']= pd.to_numeric(df['Longi'],errors='coerce')\n        df['LAT_rad'],df['LON_rad'] = np.radians(df['Lati']), \\\n        np.radians(df['Longi'])\n        df['dLON'] = df['LON_rad'] - math.radians(-56.7213600)\n        df['dLAT'] = df['LAT_rad'] - math.radians(37.2175900)\n        df['distance'] = 6367 * 2 * np.arcsin(np.sqrt(np.sin(df['dLAT']\/2)**2 +\\\n        math.cos(math.radians(37.2175900)) * np.cos(df['LAT_rad'])*\\\n                                        np.sin(df['dLON']\/2)**2))","c3babaa8":"distance = Compute_Haversine_Distance()","a02cb050":"distance.convert_income_tonumeric(df)","2d91aa64":"distance.Split_Location_geo(df)","4c287648":"distance.harvesine_distance(df['Lati'],df['Longi'])","6f361964":"class DataFrame_Categorical_Imputer():\n    \n\n    def __init__(self):\n        \n        \n        print(\"Imputation object created\")\n        \n        \n        \n    def fit(self, data):\n        \n        \n        \"\"\"\n        This method will fit \n        impute mode value for \n        all missing categoriical \n        variables\n        \"\"\"\n        \n\n        self.fill = pd.Series([data[column].\\\n                        value_counts().index[0]\n            if data[column].dtype == np.dtype('O') else \\\n                 data[column].mode() for column in data],\n            index=data.columns)\n\n        return self\n    \n    \n\n    def transform(self, data):\n        \n        \"\"\"\n        This method will transform\n        the fitted function and \n        return the DataFrame\n        \"\"\"\n        \n        return data.fillna(self.fill)","f23ea214":"impute = DataFrame_Categorical_Imputer()","9f3be446":"impute.fit(df)","4fb0cc1b":"df=impute.transform(df)","6dae8819":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing\nclass Base_Feature_Engineering():\n\n    def __init__(self):\n        print(\"Feature Engineering object created\")\n        \n        \n    \"\"\"\n    This method hepls\n    to encode all the\n    categorical variables\n    with Labelencoder\n    \"\"\"\n    \n    def _Label_Encoding(self,data):\n        category_col =[var for var in data.columns if \\\n                       data[var].dtypes ==\"object\"] \n        labelEncoder = preprocessing.LabelEncoder()\n        mapping_dict={}\n        for col in category_col:\n            data[col] = labelEncoder.fit_transform(data[col])\n            le_name_mapping = dict(zip(labelEncoder.classes_,\\\n                labelEncoder.transform(labelEncoder.classes_)))\n            mapping_dict[col]=le_name_mapping\n            return mapping_dict\n        \n        \n    def Process_Income(self,df):\n        df['Income'] = df['Income'].replace('?', '')\n        \n        \n    def get_dummies(self,data):\n        \n        for i in data.columns:\n            \n            if data[i].dtypes == \"object\":\n                \n                data[i] = pd.get_dummies(data[i])\n                \n        return data","fafd8ac8":"FE = Base_Feature_Engineering()","493d2879":"FE.Process_Income(df)","f7156016":"df = FE.get_dummies(df)\ndf.head()","45dd53f5":"class DataFrame_numerical_Imputer():\n    \n\n    def __init__(self):\n        print(\"numerical_Imputer object created\")\n        \n        \n    \"\"\"\n    This method hepls\n    to remove the special\n    charactors in the\n    income variable\n    \"\"\"\n        \n   \n    def KNN_Imputer(self,df):\n        \n        \"\"\"\n        This method is for\n        imputation, behalf\n        of all methods KNN\n        imputation performs\n        well, hence this method\n        will helps to impute\n        missing values in \n        dataset\n        \"\"\"\n        \n        knn_imputer = KNN()\n        df.iloc[:, :] = knn_imputer.fit_transform(df)\n        return df","ba4a11ca":"knn = DataFrame_numerical_Imputer()","c6262f06":"df = knn.KNN_Imputer(df)","3b76b6d2":"# The elements present in the list will be dropped by Dropper \n\ncol_list=['CustomerID','Location.Geo','Lati', 'Longi', \n          'LAT_rad', 'LON_rad', 'dLON', 'dLAT']\n\n\nclass Column_Dopper():\n\n    def __init__(self):\n        print(\"Column Dopper object created\")\n    \n    def dropper(self,x):\n        \n        \"\"\"\n        This method helps\n        to drop the columns\n        in our original \n        dataframe which is \n        available in the \n        col_list and return \n        us final dataset\n        \"\"\"\n        \n        data=[]\n        for i in x.columns:\n            if i not in col_list:\n                data.append(i)\n        return df[data]\n\n    def remove_outliers(self,data):\n        \n        \"\"\"\n        This method helps\n        to remove the outliers\n        from the target \n        variable, hence it \n        removes the influencial\n        values\n        \"\"\"\n        \n        q1 =df['Customer.Lifetime.Value'].quantile(.25)\n        q3 = df['Customer.Lifetime.Value'].quantile(.75)\n        iqr = q3-q1\n        df_out = df[~((df['Customer.Lifetime.Value'] < \\\n        (q1 - 1.5 *iqr))  |  (df['Customer.Lifetime.Value'] > \\\n                            (q3+ 1.5 * iqr)))]\n        return df_out","c9970af9":"drop = Column_Dopper()","7370e961":"df=drop.dropper(df)","d96c64b6":"df=drop.remove_outliers(df)","4df6f143":"from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nx = df.drop(['Customer.Lifetime.Value'],axis=1)\ny = df['Customer.Lifetime.Value']\nx_train,x_test,y_train,y_test=train_test_split(x\\\n                ,y,test_size=0.30,random_state=42)\n\nclass Model_Selector():\n    \n    \n\n    def __init__(self,n_estimators=100,\\\n            random_state=42,max_depth=10):\n        print(\"Model Selector object created\")\n        \n    \"\"\"\n    This method helps to select\n    the best machine learning \n    model to compute the relationship\n    betweem i\/p and d\/p variable\n    \n    \"\"\"    \n        \n        \n    def Regression_Model_Selector(self,df):\n        seed = 42\n        models = []\n        models.append((\"LR\", LinearRegression()))\n        models.append((\"RF\", RandomForestRegressor()))\n        models.append((\"KNN\", KNeighborsRegressor()))\n        models.append((\"CART\", DecisionTreeRegressor()))\n        models.append((\"XGB\", XGBRegressor()))\n        result = []\n        names = []\n        scoring = 'r2'\n        seed = 42\n        \n        \n\n        for name, model in models:\n            kfold = KFold(n_splits = 5, random_state =seed)\n            cv_results = cross_val_score(model, x_train,\\\n                    y_train, cv = kfold, scoring = scoring)\n            result.append(cv_results)\n            names.append(name)\n            msg = (name, cv_results.mean(), cv_results.std())\n            print(msg)\n            \n            \n            \n        fig = plt.figure(figsize = (8,4))\n        fig.suptitle('Algorithm Comparison')\n        ax = fig.add_subplot(1,1,1)\n        plt.boxplot(result)\n        ax.set_xticklabels(names)\n        plt.show()","543108b3":"MS = Model_Selector()","abd3c4fb":"MS.Regression_Model_Selector(df)","ed45cde0":"import statsmodels.api as sm\n\nclass Data_Modelling():\n    \n\n    def __init__(self,n_estimators=100,random_state=42,max_depth=10):\n        \n        print(\"Data Modelling object created\")\n        \n        \n    def OLS_Summary(self,data):\n        model2 =sm.OLS(y_train,x_train).fit()\n        return model2.summary()\n    \n        \n    def Linear_Regression_Model(self,df):\n        regressor = LinearRegression()\n        reg=regressor.fit(x_train,y_train)\n        LR_pred=regressor.predict(x_test)\n        LR_RMSE = np.sqrt(metrics.mean_squared_error(y_test,LR_pred))\n        LR_r2_score = r2_score(y_test,LR_pred)\n        return LR_RMSE,LR_r2_score\n    \n\n    def Decision_Tree_Model(self,df):\n        regressor = DecisionTreeRegressor(random_state=29)\n        reg=regressor.fit(x_train,y_train)\n        DT_pred=regressor.predict(x_test)\n        DT_RMSE = np.sqrt(metrics.mean_squared_error(y_test,DT_pred))\n        DT_r2_score = r2_score(y_test,DT_pred)\n        return DT_RMSE,DT_r2_score\n    \n\n    def Random_Forest_Model(self,df):\n        regressor = RandomForestRegressor(n_estimators=100,random_state=29,max_depth=12)\n        reg=regressor.fit(x_train,y_train)\n        RF_pred=regressor.predict(x_test)\n        RF_RMSE = np.sqrt(metrics.mean_squared_error(y_test,RF_pred))\n        RF_r2_score = r2_score(y_test,RF_pred)\n        return RF_RMSE,RF_r2_score\n    \n\n    def Extreme_Gradient_Boosting_Model(self,df):\n        regressor = XGBRegressor(n_estimators=100,random_state=29,max_depth=9,learning_rate=0.07)\n        reg=regressor.fit(x_train,y_train)\n        XGB_pred=regressor.predict(x_test)\n        XGB_RMSE = np.sqrt(metrics.mean_squared_error(y_test,XGB_pred))\n        XGB_r2_score = r2_score(y_test,XGB_pred)\n        return XGB_RMSE,XGB_r2_score","01222c97":"model = Data_Modelling()","e494ef7d":"model.OLS_Summary(df)","78aae36f":"model.Linear_Regression_Model(df)","59e7c1ab":"model.Decision_Tree_Model(df)","0f915b4a":"model.Random_Forest_Model(df)","f658f432":"model.Extreme_Gradient_Boosting_Model(df)","592440a7":"from sklearn.feature_selection import RFE\nfrom catboost import CatBoostRegressor\n\nclass Feature_Selection():\n\n    def __init__(self,n_estimators=100,\\\n            random_state=42,max_depth=10):\n        print(\"Feature Selection object created\")\n        \n    def Regression_Feature_Selector(self,data):\n        x = df.drop(['Customer.Lifetime.Value'],axis=1)\n        y = df['Customer.Lifetime.Value']\n        x_train,x_test,y_train,y_test=\\\n        train_test_split(x,y,test_size=0.30,random_state=42)\n        estimator = RandomForestRegressor()\n        selector = RFE(estimator,6,step=1)\n        selector = selector.fit(x_train,y_train)\n        rank =pd.DataFrame(selector.ranking_,\\\n                        columns=['Importance'])\n        Columns = pd.DataFrame(x_train.columns,\\\n                            columns=['Columns'])\n        Var = pd.concat([rank,Columns],axis=1)\n        Var.sort_values([\"Importance\"], axis=0,\\\n                    ascending=True, inplace=True) \n        return Var\n    \n    def Feature_visualizer(self,data):\n        RF_Selector = RandomForestRegressor()\n        RF_Selector = RF_Selector.fit(x_train,y_train)\n        importances = RF_Selector.feature_importances_\n        std = np.std([tree.feature_importances_ for tree \\\n                          in RF_Selector.estimators_],\n                         axis=0)\n        indices = np.argsort(importances)[::-1]\n\n            # Print the feature ranking\n        print(\"Feature ranking:\")\n        for f in range(x_train.shape[1]):\n            print(\"%d. feature %d (%f)\" % (f + 1, indices[f],\\\n                                        importances[indices[f]]))\n\n            # Plot the feature importances of the forest\n\n        plt.figure(1, figsize=(14, 13))\n        plt.title(\"Feature importances\")\n        plt.bar(range(x_train.shape[1]), importances[indices],\n                   color=\"g\", yerr=std[indices], align=\"center\")\n        plt.xticks(range(x_train.shape[1]), \\\n                    x_train.columns[indices],rotation=90)\n        plt.xlim([-1, x_train.shape[1]])\n        plt.show()    ","7e2f853f":"FS = Feature_Selection()","390f9c73":"FS.Regression_Feature_Selector(df)","95c4a5f0":"FS.Feature_visualizer(df)","5c898d9a":"feature_list=[['Months.Since.Policy.Inception',\\\n'Total.Claim.Amount','Number.of.Policies',\\\n'Monthly.Premium.Auto','distance','Income',\\\n'Months.Since.Last.Claim']]\n\n\nclass Feature_Dopper():\n\n    def __init__(self):\n        \n        \n        print(\"Column Dopper object created\")\n        \n        \n    \n    def drop_Feature(self,x):\n        \n        \"\"\"\n        This method helps\n        to removes all the \n        variable in dataset \n        except variables \n        availabe in the \n        feature list\n        \"\"\"\n        for i in feature_list:\n            data = x[i]\n        return data","ff81d96b":"FD =Feature_Dopper()","7b89f757":"x = FD.drop_Feature(df)\nx.head()","cc9d117a":"y = df['Customer.Lifetime.Value']\nx_train,x_test,y_train,y_test=train_test_split(x\\\n                ,y,test_size=0.30,random_state=42)","a0fcbab3":"model.OLS_Summary(df)","a1b49ee4":"model.Linear_Regression_Model(df)","9bfec148":"model.Decision_Tree_Model(df)","adafd74d":"model.Random_Forest_Model(df)","18a7a265":"model.Extreme_Gradient_Boosting_Model(df)","9d95b453":"import pickle\npickle.dump(regressor,open('RF_KModel.pkl','wb'))","f833d0bd":"from sklearn.model_selection import GridSearchCV\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nclass Regression_Cross_Validator():\n    \n\n    def __init__(self,n_estimators=100,random_state=42,max_depth=10):\n        \n        print(\"Cross Validation object created\")\n\n    def Cross_Validated_Random_Forest_Model(self,data):\n        \n        \n        \n        param_grid = [\n        {'n_estimators': [10, 100], 'max_features': [5, 60], \n         'max_depth': [10, 200, None], 'bootstrap': [True, False]}\n        ]\n        regressor = RandomForestRegressor(n_estimators=100,random_state=29,max_depth=12)\n        reg=regressor.fit(x_train,y_train)\n        grid_search_forest = GridSearchCV(regressor, param_grid, cv=10, scoring='neg_mean_squared_error')\n        grid_search_forest.fit(x_train, y_train)\n\n        grid_best= grid_search_forest.best_estimator_.predict(x_test)\n        grid_mse = metrics.mean_squared_error(y_test, grid_best)\n        grid_rScore = r2_score(y_test, grid_best)\n        grid_rmse = np.sqrt(grid_mse)\n        return grid_rmse,grid_rScore\n    \n    def Cross_Validated_Extreme_Gradient_Boosting_Model(self,data):\n        \n        \n        params={\n         \"learning_rate\"    : [0.05, 0.1] ,\n         \"max_depth\"        : [ 10,100],\n         \"n_estimators\"     : [ 10, 100],\n         \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n\n        }\n        XGBregressor = XGBRegressor(n_estimators=100,random_state=29,max_depth=9,learning_rate=0.07)\n        reg=XGBregressor.fit(x_train,y_train)\n        grid_search_forest = GridSearchCV(XGBregressor, params, cv=10, scoring='neg_mean_squared_error')\n        grid_search_forest.fit(x_train, y_train)\n\n        grid_best= grid_search_forest.best_estimator_.predict(x_test)\n        grid_mse = metrics.mean_squared_error(y_test, grid_best)\n        grid_rScore = r2_score(y_test, grid_best)\n        grid_rmse = np.sqrt(grid_mse)\n        return grid_rmse,grid_rScore","48af410f":"cv = Regression_Cross_Validator()","8db7b94e":"cv.Cross_Validated_Random_Forest_Model(df)","9b551f7b":"cv.Cross_Validated_Extreme_Gradient_Boosting_Model(df)","40b91e85":"![clv.jpg](attachment:clv.jpg)","aaf44711":"# FEATURES DETAILS\n\n\n<br>\n<html>\n<head>\n<style>\ntable {\n  font-family: arial, sans-serif;\n  border-collapse: collapse;\n  width: 100%;\n}\n\ntd, th {\n  border: 1px solid #dddddd;\n  text-align: left;\n  padding: 8px;\n}\n\ntr:nth-child(even) {\n  background-color: #dddddd;\n}\n<\/style>\n<\/head>\n<body>\n\n<h2><\/h2>\n\n<table>\n  <tr>\n    <th>FEATURE NAMES<\/th>\n    <th>EXPLANATION<\/th>\n\n  <\/tr>\n  <tr>\n    <td><b>CustomerID<\/b><\/td>\n    <td>ID of the Customer<\/td>\n\n  <\/tr>\n  <tr>\n      <td><b>Customer Lifetime Value<\/b><\/td>\n    <td>Life Time Profit to the owner by Customer<\/b><\/td>\n  \n  <\/tr>\n  <tr>\n    <td><b>Coverage<\/b><\/td>\n    <td>Insurance Coverge to vehicle<b>[Categorical Variable]<\/td>\n \n  <\/tr>\n  <tr>\n    <td><b>Education<\/b><\/td>\n    <td>Customer Education<b>[Categorical Variable]<\/b><\/td>\n \n  <\/tr>\n  <tr>\n    <td><b>Employment Status<\/b><\/td>\n    <td>Employment Status of Customer<b>[Categorical Variable]<\/td>\n\n  <\/tr>\n  <tr>\n    <td><b>Gender<\/b><\/td>\n    <td>Gender of Customer <b>[Categorical Variable]<\/b><\/td>\n\n  <\/tr>\n    <tr>\n        <td><b>Income<\/b><\/td>\n    <td>Income of Customer <\/b><\/td>\n\n  <\/tr>\n    <tr>\n    <td><b>Location Geo<\/b><\/td>\n    <td>Latitude and Longitude of Customer <b>[Categorical Variable]<\/b><\/td>\n\n  <\/tr>\n    <tr>\n    <td><b>Location Code<\/b><\/td>\n    <td>Location Code of Customer <b>[Categorical Variable]<\/b><\/td>\n\n  <\/tr>\n    <tr>\n    <td><b>Marital Status<\/b><\/td>\n    <td>Marital Status of Customer <b>[Categorical Variable]<\/b><\/td>\n\n  <\/tr>\n    <tr>\n    <td><b>Monthly Premium Auto<\/b><\/td>\n    <td>Monthly Premium Amount of Customers<\/td>\n\n  <\/tr>\n    <tr>\n    <td><b>Months Since Last Claim<\/b><\/td>\n    <td>Last Insurance pay of Customer<\/td>\n\n  <\/tr>\n    <tr>\n    <td><b>Months Since Policy Inception<\/b><\/td>\n    <td>Policy Inception of a Customer<\/td>\n\n  <\/tr>\n    <tr>\n    <td><b>Number of Open Complaints<\/b><\/td>\n    <td>Open Complaints of a Customer<\/b><\/td>\n\n  <\/tr>\n    <tr>\n    <td><b>Number of Policies<\/b><\/td>\n    <td>Number of Policies of Customer <\/b><\/td>\n\n  <\/tr>\n  <tr>\n    <td><b>Policy Type<\/b><\/td>\n    <td>Policy Type of Customer <b>[Categorical Variable]<\/b><\/td>\n\n  <\/tr>\n  <tr>\n    <td><b>Policy<\/b><\/td>\n    <td>Policy taken by Customer <b>[Categorical Variable]<\/b><\/td>\n\n  <\/tr>\n  <tr>\n    <td><b>Renew Offer Type<\/b><\/td>\n    <td>Renew Offers type for Customer <b>[Categorical Variable]<\/b><\/td>\n\n  <\/tr>\n  <tr>\n    <td><b>Sales Channel<\/b><\/td>\n    <td>Sales Channel of Customer<b>[Categorical Variable] <\/b><\/td>\n\n  <\/tr>\n  <tr>\n    <td><b>Total Claim Amount<\/b><\/td>\n    <td>Total Claim Amount of Customers <\/b><\/td>\n\n  <\/tr>\n  <tr>\n    <td><b>Vehicle Class<\/b><\/td>\n    <td>Vehicle Class of Customers <b>[Categorical Variable]<\/b><\/td>\n\n  <\/tr>\n  <tr>\n    <td><b>Vehicle Size<\/b><\/td>\n    <td>Vehicle Size of Customers <\/b><\/td>\n\n  <\/tr>\n<\/table>\n\n<\/body>\n<\/html>\n","bdabf15f":"# Haversine distance Computaion\n\n\n**Haversine Distance can be defined as the angular distance between two locations on the Earth\u2019s surface.**\n\n![image.png](attachment:image.png)\n\nHaversine formula determines the great-circle distance between two points on a sphere given their longitudes and latitudes.\n\n**Euclidean Distance works for the flat surface** like a Cartesian plain however, **Earth is not flat. So we have to use a special type of formula known as Haversine Distance**.","e0d72c21":"# Four Wheeler Insurance Customer LifeTime Value Prediction <html><font size =\"2\"> &nbsp;&nbsp;&nbsp;&nbsp;BY DHEERAJ KUMAR<\/html>","5955f4d0":"# What's the Challenge\n\n\nI wouldn't say that knowing your data is the most difficult thing in data science, but it is time-consuming. Therefore, it's easy to overlook this initial step and jump too soon into the water.\n\n\n**Understand the problem :-**\n\n          We'll look at each variable and do a philosophical analysis about their meaning and importance for this problem.\n**Univariable study      :-**\n\n          We'll just focus on the dependent variable ('CLV') and try to know a little bit more about it.\n**Multivariate study     :-**\n\n          We'll try to understand how the dependent variable and independent variables relate.\n**Basic cleaning         :-**\n\n          We'll clean the dataset and handle the missing data, outliers and categorical variables.\n**Doing All with Function       :-**\n\n          We'll check some of the satistical Analysis by creating some Customized function, to make the analysis what we want.\n","da3a3e09":"# Model Selection using KFold CrossValidation\n\nCross-validation is a statistical method used to estimate the skill of machine learning models.\n\nIt is commonly used in applied machine learning to compare and select a model for a given predictive modeling problem because it is easy to understand, easy to implement, and results in skill estimates that generally have a lower bias than other methods.\n\n        \n        (i)    That k-fold cross validation is a procedure used to estimate the skill of the model on new data.\n\n        (ii)   There are common tactics that you can use to select the value of k for your dataset.\n\n        (iii)  There are commonly used variations on cross-validation such as stratified and repeated that are available in scikit-                 learn.\n","62044809":"# Model Building\n\nThe Model Building process is used to estimate or compute  the relationship between independent and dependant variables.\n\nModel Building  involves setting up ways of collecting data, understanding and paying attention to what is important in the data to answer the questions you are asking, finding a statistical, mathematical or a simulation model to gain understanding and make predictions.","c39430e0":"# Import Libraries\n\nnumpy        =>  Numerical Python\n\npandas       =>  Panel Data Analysis\n\nmatplotlib   =>  Mathematical Plottig Library\n\nscipy        =>  Scientific Python\n\npyodbc       =>  Python Open DataBase Connectivity","1f1dcd1b":"# Loader Function to Load Dataframe","e7a5aaf0":"# Model Creation After Feature Selection","1dbfc925":"# PROJECT GOAL\n<br>\n\nThe **Requirement** of a Business Problem **is to develop a predictive model to Analyse and to Predit the LifeTime Value of customer** in a Four wheeler insurance company using **Regression Analysis** with **Python**.","4bc16645":"# Dropper Function to drop unwanted variables","9d826e9d":"# Feature Selection","d2ccf50c":"# Exploratory Data Analysis","81f28cc0":"# Data Preprocessing\n### Imputing Categorical Variable\n\nThis method is applicable for categorical variables, where you have a list of finite values. We can impute with the most frequent value. It is possible, if values are **Nominal and Ordinal categorical values**.\n\nUnfortunately this method doesn\u2019t handle correlation between features and there is a possibility of introducing bias in the data. If the category values are not balanced  than you are likely to introduce bias in the data. So make sure that our independent variables are balanced, if it is balanced we can impute with most frequent value.","5bcd2862":"# What is Customer Lifetime Value\n\n\n**Customer Lifetime Value** is the **total worth to a business of a customer over the whole period of their relationship**. It\u2019s an important metric as it costs less to keep existing customers than it does to acquire new ones, so increasing the value of your existing customers is a great way to drive growth.\n\nThis is an important figure to know because it helps you **make decisions about how much money to invest in acquiring new customers and retaining existing ones**.\n\n\n## Calculating CLV\n\n**The simplest way to calculate CLV is:**\n\nCLV = **average value of a Insurance taken X number of times the customer will take insurance each year X average length of the customer relationship (in years)**\n\n\n## The Value of Knowing Your CLV\n\nCalculating the CLV for different customers helps in a number of ways, mainly regarding business decision-making. Knowing your CLV you can determine, among other things:\n\n    (i)   How much you can spend to acquire a similar customer and still have a profitable relationship\n    \n    (ii)  What kinds of insurance policies or coverages customers with the highest CLV want\n    \n    (iii) Which products have the highest profitability\n    \n    (iv)  Who your most profitable types of clients are\n","18ae6f7c":"# Data Preprocessing\n### Imputing Numerical Variable\n\nWe will use the KNNImputer function from the impute module of the sklearn. KNNImputer helps to impute missing values present in the observations by finding the nearest neighbors with the Euclidean distance matrix.\n\nThe idea in kNN methods is to identify 'k' samples in the dataset that are similar or close in the space. Then we use these 'k' samples to estimate the value of the missing data points. Each sample's missing values are imputed using the mean value of the 'k'-neighbors found in the dataset.","2c2bf5ef":" # Model HyperParameter Tuning\n \n \n The parameters, called hyperparameters, that define the performance of the machine learning algorithm (model), depends on the problem we are trying to solve. Thus, they need to be configured accordingly. This process of finding the best set of parameters is called hyperparameter optimization.\n \n \n The grid search is an exhaustive search through a set of manually specified set of values of hyperparameters. It means you have a set of models (which differ from each other in their parameter values, which lie on a grid). What you do is you then train each of the models and evaluate it using cross-validation. You then select the one that performed best.","ed473bac":"# Feature Engineering\n\n\n**Feature Engineering** is a **crucial part** part **in data science projects**, in our roject we have done feature engineering **on Location Geo valiable to create Distance variable**, **which is core variable** for this dataset **in terms of contribution**.\n\nMostly in categorical variable, Nonminal variables will be Dummied or OneHot Encoded and ordinal variable will be Label Encoded, but in this case considering the deployment part, beacuse of containing 13 nommial variables, i have performed Label Encoding to handle features and to avoid risk dimentionality factor"}}