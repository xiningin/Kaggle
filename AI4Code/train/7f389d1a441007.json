{"cell_type":{"15dbf904":"code","dcc5bc21":"code","43df47c3":"code","04a6c3c1":"code","e7b130a2":"code","e270307f":"code","fb24b2fb":"code","ffe20e42":"code","2e255465":"code","266e10a1":"code","e3d095be":"code","97ba65c6":"code","7a83c42e":"code","0ac4f33a":"code","3024622b":"code","aeccb276":"code","17ed27de":"code","9889d351":"code","c5945f96":"code","48c99e00":"code","50163d2b":"code","1f14be2a":"code","71014fc2":"code","61fc05af":"code","60d8d4c1":"code","9f612b18":"code","9a0a3cb1":"code","2366be63":"code","9e91b294":"code","7f251738":"code","73e155e9":"code","0478c0a0":"code","b46e81fa":"code","411350f5":"markdown","36021476":"markdown","138f1f91":"markdown","5b68fdec":"markdown","b35316fb":"markdown","3561f3ea":"markdown","b340921b":"markdown","ee597fc5":"markdown","9164613f":"markdown","86a033c4":"markdown","93742c3e":"markdown","96c83c7f":"markdown","3c7f12b2":"markdown","2f5c66d0":"markdown","09e06602":"markdown","033eb471":"markdown","cd3ffb03":"markdown","2b97afa3":"markdown","555886da":"markdown","413b6a5b":"markdown","78cc3439":"markdown"},"source":{"15dbf904":"!pip install texthero","dcc5bc21":"import pandas as pd\nimport numpy as np\nimport texthero as hero\nfrom texthero import preprocessing as ppe\nfrom texthero import visualization as viz\nimport spacy\nfrom spacy import displacy\nimport re\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, cross_val_score, cross_val_predict\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom xgboost import XGBClassifier","43df47c3":"train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntrain_df.head()","04a6c3c1":"test_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntest_df.head()","e7b130a2":"train_df.info()","e270307f":"test_df.info()","fb24b2fb":"sns.countplot(x = 'target', data = train_df, facecolor=(0, 0, 0, 0),\n                   linewidth=5,\n                   edgecolor=sns.color_palette(\"dark\", 3))\nplt.xlabel('Class Names')\nplt.ylabel('Count')\nplt.title('Distribution of classes in the training dataset')\nplt.show()","ffe20e42":"plt.figure(figsize=(10,5))\nsns.barplot(y=train_df['location'].value_counts()[:10].index,x=train_df['location'].value_counts()[:10],orient='h',\n            facecolor=(0, 0, 0, 0), linewidth = 3,\n           edgecolor=sns.color_palette(\"dark\", 3))\nplt.xlabel('Location Count')\nplt.title('Top 10 locations with the maximum occurence in the training dataset')\nplt.show()","2e255465":"plt.figure(figsize=(10,5))\nsns.barplot(y=train_df['keyword'].value_counts()[:10].index,x=train_df['keyword'].value_counts()[:10],orient='h',\n            facecolor=(0, 0, 0, 0), linewidth = 2,\n           edgecolor=sns.color_palette(\"dark\", 3))\nplt.xlabel('Keyword Count')\nplt.title('Top 10 keywords with the maximum occurence in the training dataset')\nplt.show()","266e10a1":"test_df.keyword.value_counts()[:10]","e3d095be":"test_df.location.value_counts()[:10]","97ba65c6":"train_df.drop(['keyword', 'location'], axis = 1, inplace = True)\ntrain_df.head()","7a83c42e":"test_df.drop(['keyword', 'location'], axis = 1, inplace = True)\ntest_df.head()","0ac4f33a":"train_df['word count'] = train_df.text.apply(len)\ntrain_df.head()","3024622b":"train_df['word count'].describe()","aeccb276":"#most common words\nfreq = pd.Series(''.join(train_df['text']).split()).value_counts()[:10]\nfreq","17ed27de":"#uncommon words\nnot_freq = pd.Series(''.join(train_df['text']).split()).value_counts()[-10:]\nnot_freq","9889d351":"custom_pipeline = [ppe.fillna, ppe.lowercase, ppe.remove_punctuation, ppe.remove_whitespace, \n                  ppe.remove_stopwords, ppe.remove_urls, ppe.remove_digits]\n\ntrain_df['cleaned_text'] = hero.clean(train_df['text'], custom_pipeline)\ntest_df['cleaned_text'] = hero.clean(test_df['text'], custom_pipeline)","c5945f96":"train_df.head()","48c99e00":"test_df.head()","50163d2b":"def lemmatizer(r):\n    wnl = WordNetLemmatizer()\n    words = nltk.word_tokenize(r)\n    lemmatized_words = [wnl.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n    return \" \".join(lemmatized_words)\n\ntrain_df['lemma_cleaned_text'] = train_df['cleaned_text'].apply(lemmatizer)\ntest_df['lemma_cleaned_text'] = test_df['cleaned_text'].apply(lemmatizer)","1f14be2a":"def remove_special_characters(text):\n    pattern = r'[^a-zA-Z]'\n    text = re.sub(pattern, ' ', text)\n    return text\n\ntrain_df['special_char_cleaned_text'] = train_df['lemma_cleaned_text'].apply(remove_special_characters)\ntest_df['special_char_cleaned_text'] = test_df['lemma_cleaned_text'].apply(remove_special_characters)","71014fc2":"train_df.head()","61fc05af":"test_df.head()","60d8d4c1":"wordcloud = WordCloud().generate(' '.join(train_df['special_char_cleaned_text']))","9f612b18":"print(wordcloud)\nfig = plt.figure(1)\nplt.figure(figsize=(10,5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","9a0a3cb1":"nlp = spacy.load('en_core_web_sm')\nl = []\nfor i in train_df['special_char_cleaned_text']:\n    doc = nlp(i)\n    if doc.ents:\n        for ent in doc.ents:\n            ner = {\n                    'Text' : [ent.text],\n                    'NER Label' : [ent.label_],\n                    'Label explaination' : [str(spacy.explain(ent.label_))]\n                }\n            l.append(ner)   \n            df1 = pd.DataFrame(data = l)\n            df1['Text'] = df1['Text'].str.get(0)\n            df1['NER Label'] = df1['NER Label'].str.get(0)\n            df1['Label explaination'] = df1['Label explaination'].str.get(0)\n            ","2366be63":"plt.figure(figsize=(10,5))\nsns.barplot(y=df1['NER Label'].value_counts().index,x=df1['NER Label'].value_counts(),orient='h',\n            facecolor=(0, 0, 0, 0), linewidth = 2,\n           edgecolor=sns.color_palette(\"dark\", 11))\nplt.xlabel('NER Label Count')\nplt.title('NER Labels and frequency of their occurence')\nplt.show()","9e91b294":"tfidf_vectorizer = TfidfVectorizer()\ntrain_tfidf = tfidf_vectorizer.fit_transform(train_df['special_char_cleaned_text'])\ntest_tfidf = tfidf_vectorizer.transform(test_df['special_char_cleaned_text'])","7f251738":"model = XGBClassifier()\n# define grid\nweights = [1, 10, 15, 20]\nparam_grid = dict(scale_pos_weight=weights)\n# define evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n# define grid search\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='roc_auc')\n# execute the grid search\ngrid_result = grid.fit(train_tfidf, train_df.target)\n# report the best configuration\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# report all configurations\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","73e155e9":"pred = grid_result.predict(test_tfidf)","0478c0a0":"sample = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\ndata={\"id\":[],\"target\":[]}\nfor id,pred_1 in zip(sample['id'].unique(),pred): \n    data[\"id\"].append(id) \n    data[\"target\"].append(pred_1)\n\n    \noutput=pd.DataFrame(data,columns=[\"id\",\"target\"])\noutput.to_csv('submission.csv', index=False)","b46e81fa":"output","411350f5":"# Class Distribution","36021476":"**Visualing the NER labels and the frequency of their occurence**","138f1f91":"**Dropping the Keyword and location columns from both training and test set.**","5b68fdec":"# Most common and uncommon words in the text column","b35316fb":"# Keyword column value distribution","3561f3ea":"# Location column value distribution","b340921b":"The **describe()** method is used for calculating statistical data like percentile, mean and std of the numerical values of the Series or DataFrame. We're using this method below on the word count column.","ee597fc5":"# If you like my work,  don't forget to upvote ;)","9164613f":"# Data Preprocessing\nFor data pre-processing, we're mainly gonna use the awesome **TextHero** library. Under the hoods, Texthero utilizes various NLP and AI tool compartments like **Gensim, NLTK, SpaCy** and **scikit-learn**. We're gonna use mainly the pre-processing toolkit of this library. \n\nSo, with this library, we can create custom pipeline where we can mention various textual data cleaning techniques like **removing whitespaces, stop words, punctuations**, etc. and then apply this on the text column.","86a033c4":"Getting the word count of the text column","93742c3e":"# WordCloud \nThis Wordcloud displays the most frequent words in the training dataset.","96c83c7f":"**Predicting on the test set**","3c7f12b2":"# Named Entity Recognition (NER)\nNamed-entity recognition is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. We're using the **SpaCy** library for that.","2f5c66d0":"**Importing the required libraries.**","09e06602":"# Lemmatization\nWe use lemmatizer to convert the words into their root words. So, suppose if the data contains 2 words where one of them is in past tense (**believed**) and another one in future tense (**believing**), that particular word will be converted to its root word (**believe**). \n\nThis particularly helps us while training our model, as the model doesn't need to learn 2 different words which basically have the same meaning.","033eb471":"# Special Character removal\nWe're gonna use the below code to remove special characters like @, #, $ , etc. \nThis step is really important as these characters can really hamper our model's performance while training.\n","cd3ffb03":"**Reading the training and test dataset**","2b97afa3":"**Final Output :)**","555886da":"# Vectorization\nIn basic terms, Vectorization is the **process of converting text into numerical representation** which are also called **embeddings**. \nSince, the computers are not as intelligent as us (till now atleast ;), they can't understand textual data, so to make our data understandable to a computer, we first convert it to a numerical format.\n\nThere are various techniques for text vectorization like:-\n* **Bag of Words**\n* **Count Vectorizer**\n* **TF-IDF Vectorizer**\n\nHere, we're gonna use the **TF-IDF Vectorizer** approach. \nSo, TF-IDF is an acronym for **Term Frequency - Inverse Document Frequency**. \n* TF makes sure to give high score to the word that appears frequently.\n* IDF makes sure to give low score to the word if it appears pretty frequently in documents (not a unique identifier).\n\nSo, the amalgamation of **TF * IDF** is how the score is calculated for this vectorizer.","413b6a5b":"# References:-\nSome of the literature and learnings are borrowed from below sources. Feel free to check them out :)\n* https:\/\/machinelearningmastery.com\/xgboost-for-imbalanced-classification\/\n* https:\/\/towardsdatascience.com\/a-beginners-guide-to-xgboost-87f5d4c30ed7\n* https:\/\/texthero.org\/docs\/getting-started#preprocessing\n* https:\/\/towardsdatascience.com\/named-entity-recognition-ner-using-spacy-nlp-part-4-28da2ece57c6","78cc3439":"# Training our model\nSo, here we're gonna use the** XGBClasifier** which basically harnesses the power of **boosting trees**. \nSo, boosting trees are a little different than your normal decision trees. In decision trees, we're ensembling a model on top of another but at the end of the day we're using a single model for our prediction. But boosting trees take a smarter approach when it comes to training a model efficiently. So, rather than training all of the models in isolation of one another, boosting trains models in succession, with each new model being trained to correct the errors made by the previous ones. \nModels are added sequentially until no further improvements can be made.\n\nThe main advantage of this iterative approach is that the new models being added are focused on correcting the mistakes which were caused by other models. In a standard ensemble method, like random forest,  where models are trained in isolation, all of the models might simply end up making the same mistakes.\n\nSince we have imbalanced data in our classes, we're gonna use **scale_pos_weight**. So, here we have defined a definite set of weights which we're gonna use in this hyper parameter.\n\nThen, we'll be using **RepeatedStratifiedKFold** method which is going to repeat Stratified K-Fold 3 times with different randomization in each repetition. We're putting number of splits as 5, so it'll be a 5 fold cross validation.\n\nAfter that, we're using **GridSearchCV** to feed and iterate through our mentioned weights and other hyper parameters. We're using **ROC AUC** curve for scoring our model's performance. So, the ROC AUC curve is the measure of the ability of a classifier to distinguish between classes. The higher the AUC, the better the performance of the model at distinguishing one class from another.\n\nLastly, we're summarizing the best configuration and printing them out."}}