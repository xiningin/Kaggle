{"cell_type":{"e2c6cbfd":"code","a2e9b35b":"code","21b29918":"code","1023bd64":"code","64ae9a75":"code","9ea2747e":"code","44d52356":"code","edffe3d4":"code","4411ada7":"code","10352067":"code","05423ab3":"code","a97f5336":"code","750392ae":"code","8122f549":"code","39b262ef":"code","dad5d4d4":"code","67e1c195":"code","6793003a":"code","9e41fd4d":"code","f884a8ad":"code","0209918d":"code","a2d6cc94":"code","c538697c":"code","db67e041":"code","c2205849":"code","9b2449ba":"code","f9a7e48c":"code","3fa00ba3":"code","78407fee":"code","d80001d3":"code","4e17fbd3":"code","d2456c8a":"code","2906f7c5":"code","f8cf6eac":"code","b263af19":"code","4f119cd6":"code","fb2b2447":"code","52a297ad":"code","108a8235":"code","e583dd63":"code","5278e6f9":"code","e9fe0b4f":"code","8f4e33ad":"code","82636ab8":"code","82f3803a":"code","6984aff6":"code","be27f694":"code","e1f48272":"code","1ee40825":"code","b8263312":"code","32299bd4":"code","4c3c252f":"code","ca0e5601":"code","f97da75f":"code","6e5ed0c1":"code","dbe68784":"code","c7e124bb":"code","e7ed65ed":"code","7991b540":"code","087e3724":"code","9eb35be5":"code","105a55a6":"code","ab78fda5":"code","2f8df0ab":"code","398a03e9":"code","a1454706":"code","a8af8d77":"code","f74b099c":"code","3eb1bc18":"code","0c4cb1a6":"code","49fbd0a3":"code","2430d60e":"code","225b78ef":"code","46b95535":"code","77ea2638":"code","167dedff":"code","d6a2f7ef":"code","04f1d590":"code","37b07235":"code","c362e717":"code","ab44a39d":"code","ee51bb26":"code","5ef5994c":"code","abfa9cf5":"code","8a2a8530":"code","d44f4e9a":"code","1b96536a":"code","5e7f797e":"code","69bdbd65":"code","a8c618c6":"code","b838b4ba":"code","c716a2fb":"code","94f33428":"code","8c4f1219":"code","46587c5f":"code","d4d3cfeb":"code","572eb983":"code","a5860ce6":"code","c7ab894e":"code","358d31f8":"code","abb8377f":"code","8f4b8acd":"code","af976121":"code","1dcca9ce":"code","5ccfa521":"code","998ff047":"code","17ef6d9b":"code","4a8f8dc8":"code","7e9e1bad":"code","f52b45a8":"code","5c021d7c":"code","5f701ad3":"code","a820a0e4":"code","a08830ff":"code","154f7a36":"code","e7880928":"code","4dc14ec0":"code","f60ee729":"code","3428d693":"code","19d95c8f":"code","2e032d35":"code","454bfe1c":"code","23a5ef26":"code","44f0d712":"code","0a2ca16f":"code","d5e75679":"code","e6ae07f5":"code","ad314853":"code","f49e0fa3":"code","292a700a":"code","9e45de3f":"code","89cbc1a9":"code","72b4b74b":"code","606d54b5":"code","5bf41991":"code","6ec7faf9":"code","31c44a82":"code","e4fed11a":"code","61ca7f83":"code","ebb3e213":"code","1375e6b5":"code","a9a215ca":"code","97050bd2":"code","aadad4b4":"code","77e7d89b":"code","fd8e5a84":"code","3bc92735":"code","9d519657":"code","2fc99628":"code","a2f6cdfa":"code","b62604da":"code","df2b4157":"code","3f5401a4":"markdown","e2cdd86b":"markdown","6265b5fa":"markdown","cf8bdcb6":"markdown","8c7d535b":"markdown","4403c8f2":"markdown","961e5669":"markdown","2de0d0dc":"markdown","6a0d0ed0":"markdown","0fa51187":"markdown","0f36c021":"markdown","8dec1b60":"markdown","fd795d40":"markdown","0f10986a":"markdown","cbe664d4":"markdown","7103d897":"markdown","c6efeeac":"markdown","7432e32d":"markdown","d16af076":"markdown","479c4f26":"markdown","aa33c556":"markdown","97038270":"markdown","cd9ee349":"markdown","06196ab4":"markdown","4cd243aa":"markdown","86914e7d":"markdown","58e48123":"markdown","dac335ab":"markdown","92d15b0e":"markdown","3c29c823":"markdown","13ff7d0d":"markdown","0aedf358":"markdown","5273849c":"markdown","2a3b0d4b":"markdown","dfea4257":"markdown","a801d56f":"markdown","372938d3":"markdown","3e7abe24":"markdown","67ab10a9":"markdown","24f037ab":"markdown","5c0f4528":"markdown","da63dd56":"markdown","9a589462":"markdown","1e04f109":"markdown","936ec7a8":"markdown","912f21c0":"markdown","dd4ec521":"markdown","9cd55231":"markdown","216cdcbf":"markdown","81a57570":"markdown","d168e833":"markdown","107fc2de":"markdown","d279c9e1":"markdown","9f904939":"markdown","81730698":"markdown","db21aeec":"markdown","d14e5a31":"markdown","20f587a2":"markdown","f6b0d04e":"markdown","a3ff0dbd":"markdown","4468ed15":"markdown","37efb21f":"markdown","fd855d8d":"markdown"},"source":{"e2c6cbfd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a2e9b35b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport datetime as dt\nimport seaborn as sns\nimport pickle\nfrom collections import defaultdict\nfrom mpl_toolkits.mplot3d import Axes3D\nimport gc\n\nsns.set()\ngc.enable()\n%matplotlib inline","21b29918":"#df_train = pd.read_csv(\"..\/input\/prepaired-data-of-customer-revenue-prediction\/train_flat.csv\", converters={'fullVisitorId': str})\n#df_test = pd.read_csv(\"..\/input\/prepaired-data-of-customer-revenue-prediction\/test_flat.csv\", converters={'fullVisitorId': str})","1023bd64":"#df_train.head()","64ae9a75":"#df_train.shape, df_test.shape","9ea2747e":"#df_train[\"totals_transactionRevenue\"] = df_train[\"totals_transactionRevenue\"].fillna(0)","44d52356":"#train_col = np.array(df_train.columns)\n#test_col = np.array(df_test.columns)\n#print(set(train_col) - set(test_col))","edffe3d4":"#df_train = df_train.drop(columns=[\"trafficSource_campaignCode\"])","4411ada7":"#print(np.unique(df_train[\"socialEngagementType\"], return_counts=True))\n#print(np.unique(df_test[\"socialEngagementType\"], return_counts=True))","10352067":"#df_train = df_train.drop(columns=[\"socialEngagementType\"])\n#df_test = df_test.drop(columns=[\"socialEngagementType\"])","05423ab3":"#df_train_eq_nan = df_train.fillna(-1543)","a97f5336":"\"\"\"for col_name in np.array(df_train_eq_nan.columns):\n    print(col_name)\n    try:\n        print(np.unique(np.array(df_train_eq_nan[col_name]), return_counts=True))\n    except Exception:\n        print(np.unique(np.array(df_train_eq_nan[col_name]).astype(str), return_counts=True))\n    print(\"-\" * 43)\n\"\"\"","750392ae":"# numerical = [\"visitNumber\", \"visitStartTime\", \"totals_bounces\", \"totals_hits\", \"totals_newVisits\", \"totals_pageviews\", \n#             \"trafficSource_adwordsClickInfo.isVideoAd\", \"trafficSource_adwordsClickInfo.page\", \n#             \"trafficSource_isTrueDirect\", \"device_isMobile\"]\n\n# categorial = [\"channelGrouping\", \"date\", \"device_browser\", \"device_deviceCategory\", \"device_operatingSystem\",\n#                \"geoNetwork_city\", \"geoNetwork_continent\",\"geoNetwork_metro\", \"geoNetwork_country\",\n#               \"geoNetwork_networkDomain\", \"geoNetwork_region\",\n#                \"geoNetwork_subContinent\", \"trafficSource_adContent\", \"trafficSource_adwordsClickInfo.adNetworkType\",\n#                \"trafficSource_adwordsClickInfo.slot\", \"trafficSource_campaign\", \"trafficSource_keyword\", \"trafficSource_medium\",\n#                \"trafficSource_source\"]\n\n# saved_trash = [\"fullVisitorId\"]\n\n# trash_trash = [\"totals_visits\", \"device_browserSize\", \"device_browserVersion\",\n#                 \"device_flashVersion\", \"device_language\", \"device_mobileDeviceBranding\", \"device_mobileDeviceInfo\",\n#                 \"device_mobileDeviceMarketingName\", \"device_mobileDeviceModel\", \"device_mobileInputSelector\",\n#                 \"device_screenColors\", \"device_screenResolution\", \"geoNetwork_cityId\", \"geoNetwork_latitude\",\n#                 \"geoNetwork_longitude\", \"geoNetwork_networkLocation\", \"trafficSource_adwordsClickInfo.criteriaParameters\",\n#                 \"device_operatingSystemVersion\"]\n                \n# wanted_to_trash = [\"sessionId\", \"visitId\", \"trafficSource_adwordsClickInfo.gclId\", \"trafficSource_referralPath\"]\n\n# in_future_wanted_to_trash = [\"visitStartTime\", \"date\", \"device_isMobile\", \"geoNetwork_city\", \"geoNetwork_metro\",\n#                 \"geoNetwork_networkDomain\", \"trafficSource_adContent\", \"trafficSource_keyword\"]\n\n# to_heal = [\"totals_bounces\", \"totals_newVisits\", \"totals_pageviews\", \"trafficSource_adwordsClickInfo.adNetworkType\",\n#             \"device_browser\", \"trafficSource_adwordsClickInfo.isVideoAd\", \"trafficSource_adwordsClickInfo.slot\", \n#             \"trafficSource_adwordsClickInfo.page\", \"trafficSource_campaign\", \"trafficSource_isTrueDirect\",\n#             \"trafficSource_medium\", \"trafficSource_source\", \"device_operatingSystem\", \"geoNetwork_city\",\n#             \"geoNetwork_region\",\n#             \"geoNetwork_continent\", \"geoNetwork_country\", \"geoNetwork_metro\", \"geoNetwork_networkDomain\",\n#             \"geoNetwork_subContinent\", \"trafficSource_adContent\", \"trafficSource_keyword\"]\n\n# answer_feature = [\"totals_transactionRevenue\"]","8122f549":"# df_train = df_train.drop(columns=trash_trash)\n# df_test = df_test.drop(columns=trash_trash)","39b262ef":"# df_train_eq_nan = df_train.fillna(-1543)\n# for col_name in wanted_to_trash:\n#     print(col_name)\n#     un = None\n#     try:\n#         un = np.unique(np.array(df_train_eq_nan[col_name]), return_counts=True)\n#     except Exception:\n#         un = np.unique(np.array(df_train_eq_nan[col_name]).astype(str), return_counts=True)\n#     print(un)\n#     print(\"DIFFERENT COUNT: \", un[0].shape[0])\n#     print(\"-\" * 43)","dad5d4d4":"# un = np.unique(df_train[\"sessionId\"], return_counts=True)\n# idx = np.where(un[1] != 1)[0]\n# repeat_session_id = un[0][idx]\n\n# repeated_df = df_train[df_train[\"sessionId\"].isin(repeat_session_id)].sort_values([\"sessionId\"]).iloc[:, :20]\n# repeated_df","67e1c195":"# np.all(\n#     df_train[df_train[\"sessionId\"].isin(repeat_session_id)][[\"date\", \"sessionId\"]].groupby(\"sessionId\").count() == 2)","6793003a":"# def parse_datetime(strdate):\n#     year, month, day = list(map(lambda x: int(x), [strdate[:4], strdate[4:6], strdate[6:8]]))\n#     return dt.datetime(year=year, month=month, day=day)","9e41fd4d":"# bad_idxs = []\n# for session_id in repeat_session_id:\n#     part_df = repeated_df[repeated_df[\"sessionId\"] == session_id]\n#     dt1, dt2 = parse_datetime(str(part_df[\"date\"].iloc[0])), parse_datetime(str(part_df[\"date\"].iloc[1]))\n#     if dt1 < dt2:\n#         bad_idxs.append(part_df.iloc[0].name)\n#     else:\n#         bad_idxs.append(part_df.iloc[1].name)","f884a8ad":"# len(bad_idxs)","0209918d":"# df_train = df_train.drop(bad_idxs)\n# df_train.shape","a2d6cc94":"# df_train.index = np.arange(df_train.shape[0])","c538697c":"# df_train_eq_nan = df_train.fillna(-1543)\n# for col_name in wanted_to_trash:\n#     print(col_name)\n#     un = None\n#     try:\n#         un = np.unique(np.array(df_train_eq_nan[col_name]), return_counts=True)\n#     except Exception:\n#         un = np.unique(np.array(df_train_eq_nan[col_name]).astype(str), return_counts=True)\n#     print(un)\n#     print(\"DIFFERENT COUNT: \", un[0].shape[0])\n#     print(\"-\" * 43)","db67e041":"# df_train = df_train.drop(columns=[\"sessionId\"])\n# df_test = df_test.drop(columns=[\"sessionId\"])","c2205849":"# un = np.unique(df_train[\"visitId\"], return_counts=True)\n# idx = np.where(un[1] > 1)[0]\n# repeat_session_id = un[0][idx]\n# repeated_df = df_train[df_train[\"visitId\"].isin(repeat_session_id)].sort_values([\"visitId\"]).iloc[:, :20]\n# repeated_df","9b2449ba":"# len(repeat_session_id)","f9a7e48c":"# df_train = df_train.drop(columns=[\"visitId\"])\n# df_test = df_test.drop(columns=[\"visitId\"])","3fa00ba3":"# df_train_eq_nan = df_train.fillna(-1543)\n# un = np.unique(np.array(df_train_eq_nan[\"trafficSource_adwordsClickInfo.gclId\"]).astype(str), return_counts=True)\n# idx = np.where(un[0] != \"-1543\")[0]\n# not_null_CI_train = un[0][idx]\n# not_null_CI_train, not_null_CI_train.shape","78407fee":"# df_test_eq_nan = df_test.fillna(-1543)\n# un = np.unique(np.array(df_test_eq_nan[\"trafficSource_adwordsClickInfo.gclId\"]).astype(str), return_counts=True)\n# idx = np.where(un[0] != \"-1543\")[0]\n# not_null_CI_test = un[0][idx]\n# not_null_CI_test, not_null_CI_test.shape","d80001d3":"# CI_intersect = set(not_null_CI_train) & set(not_null_CI_test)\n# len(CI_intersect)","4e17fbd3":"# df_train = df_train.drop(columns=[\"trafficSource_adwordsClickInfo.gclId\"])\n# df_test = df_test.drop(columns=[\"trafficSource_adwordsClickInfo.gclId\"])","d2456c8a":"# df_train_eq_nan = df_train.fillna(-1543)\n# un = np.unique(np.array(df_train_eq_nan[\"trafficSource_referralPath\"]).astype(str), return_counts=True)\n# idx = np.where(un[0] != \"-1543\")[0]\n# not_null_RP_train = un[0][idx]\n# not_null_RP_train, not_null_RP_train.shape","2906f7c5":"# df_test_eq_nan = df_test.fillna(-1543)\n# un = np.unique(np.array(df_test_eq_nan[\"trafficSource_referralPath\"]).astype(str), return_counts=True)\n# idx = np.where(un[0] != \"-1543\")[0]\n# not_null_RP_test = un[0][idx]\n# not_null_RP_test, not_null_RP_test.shape","f8cf6eac":"# del df_train_eq_nan, df_test_eq_nan\n# gc.collect()","b263af19":"# revenues = df_train[df_train[\"trafficSource_referralPath\"].isin(not_null_RP_train)][\"totals_transactionRevenue\"]\n# revenues[revenues != 0].shape","4f119cd6":"# df_train[df_train[\"totals_transactionRevenue\"] != 0].shape","fb2b2447":"# categorial.append(\"trafficSource_referralPath\")","52a297ad":"# zero_filling = [\"totals_bounces\", \"totals_newVisits\", \"totals_pageviews\", \"trafficSource_adwordsClickInfo.isVideoAd\",\n#                 \"trafficSource_adwordsClickInfo.page\", \"trafficSource_isTrueDirect\"]\n# empty_filling = [\"device_browser\", \"trafficSource_adwordsClickInfo.adNetworkType\",\n#                  \"trafficSource_adwordsClickInfo.slot\", \"geoNetwork_city\", \"geoNetwork_continent\", \n#                  \"geoNetwork_country\", \"geoNetwork_metro\", \"geoNetwork_networkDomain\", \"geoNetwork_region\",\n#                  \"geoNetwork_subContinent\", \"trafficSource_adContent\", \"trafficSource_keyword\",\n#                  \"trafficSource_campaign\", \"trafficSource_medium\", \"trafficSource_source\",\n#                  \"device_operatingSystem\", \"trafficSource_referralPath\"]","108a8235":"# df_train[zero_filling] = df_train[zero_filling].fillna(0)\n# df_train[\"trafficSource_adwordsClickInfo.isVideoAd\"] = df_train[\"trafficSource_adwordsClickInfo.isVideoAd\"].apply(lambda x: 1 if x == 0 else 0)\n# df_train[\"trafficSource_isTrueDirect\"] = df_train[\"trafficSource_isTrueDirect\"].apply(lambda x: 1 if x else 0)","e583dd63":"# df_test[zero_filling] = df_test[zero_filling].fillna(0)\n# df_test[\"trafficSource_adwordsClickInfo.isVideoAd\"] = df_test[\"trafficSource_adwordsClickInfo.isVideoAd\"].apply(lambda x: 1 if x == 0 else 0)\n# df_test[\"trafficSource_isTrueDirect\"] = df_test[\"trafficSource_isTrueDirect\"].apply(lambda x: 1 if x else 0)","5278e6f9":"# df_train[empty_filling] = df_train[empty_filling].fillna(\"@\")\n# df_test[empty_filling] = df_test[empty_filling].fillna(\"@\")","e9fe0b4f":"# df_train[\"device_isMobile\"] = df_train[\"device_isMobile\"].apply(lambda x: 1 if x else 0)\n# df_test[\"device_isMobile\"] = df_test[\"device_isMobile\"].apply(lambda x: 1 if x else 0)","8f4e33ad":"# set(df_train.columns) - (set(numerical) | set(categorial))","82636ab8":"# len(numerical) + len(categorial), df_train.shape","82f3803a":"#df_train.to_csv(\".\/kernel\/data\/train_filtered.csv\", sep=\",\", index=False)\n#df_test.to_csv(\".\/kernel\/data\/test_filtered.csv\", sep=\",\", index=False)","6984aff6":"#df_train = pd.read_csv(\"..\/input\/prepaired-data-of-customer-revenue-prediction\/train_filtered.csv\", converters={'fullVisitorId': str}, sep=\",\")\n#df_test = pd.read_csv(\"..\/input\/prepaired-data-of-customer-revenue-prediction\/test_filtered.csv\", converters={'fullVisitorId': str}, sep=\",\")","be27f694":"# for col_name in categorial:\n#     print(col_name)\n#     un_train = np.unique(np.array(df_train[col_name]).astype(str), return_counts=True)\n#     un_test = np.unique(np.array(df_test[col_name]).astype(str), return_counts=True)\n#     print(\"TRAIN: \", un_train[0])\n#     print(\"TEST: \", un_test[0])\n#     print(\"DIFFERENT TRAIN COUNT: \", un_train[0].shape[0])\n#     print(\"DIFFERENT TEST COUNT: \", un_test[0].shape[0])\n#     print(\"-\" * 43)","e1f48272":"# easy_OHE = [\"channelGrouping\", \"device_deviceCategory\", \"geoNetwork_continent\", \"geoNetwork_subContinent\", \"trafficSource_medium\"]\n# easy_OHE_but_prepare = [\"trafficSource_adwordsClickInfo.adNetworkType\", \"trafficSource_adwordsClickInfo.slot\"]\n\n# bad_categorial = [\"device_browser\", \"date\", \"device_operatingSystem\", \"geoNetwork_city\", \"geoNetwork_metro\", \"geoNetwork_country\",\n#                   \"geoNetwork_networkDomain\", \"geoNetwork_region\", \"trafficSource_adContent\",\n#                   \"trafficSource_campaign\", \"trafficSource_keyword\", \"trafficSource_source\",\n#                  \"trafficSource_referralPath\"]","1ee40825":"# from sklearn.preprocessing import OneHotEncoder as OHE\n# from sklearn.preprocessing import LabelEncoder as OE","b8263312":"# y_train = np.array(df_train[answer_feature])\n# id_numeration_train = np.array(df_train[saved_trash])\n# X_train_numerical = np.array(df_train[numerical])\n\n# id_numeration_test = np.array(df_test[saved_trash])\n# X_test_numerical = np.array(df_test[numerical])","32299bd4":"# train_easy_OHE = np.array(df_train[easy_OHE])\n# test_easy_OHE = np.array(df_test[easy_OHE])\n\n# easy_OEs = [OE() for i in range(train_easy_OHE.shape[1])]\n# for f in range(train_easy_OHE.shape[1]):\n#     easy_OEs[f].fit(train_easy_OHE[:, f])\n#     train_easy_OHE[:, f] = easy_OEs[f].transform(train_easy_OHE[:, f])\n#     test_easy_OHE[:, f] = easy_OEs[f].transform(test_easy_OHE[:, f])","4c3c252f":"# easy_enc = OHE(sparse=False)\n# easy_enc.fit(train_easy_OHE)\n# train_easy_OHE_conv = easy_enc.transform(train_easy_OHE)\n# test_easy_OHE_conv = easy_enc.transform(test_easy_OHE)","ca0e5601":"# errors = np.where(\n#     np.array(train_easy_OHE_conv[:, 9]).astype(bool)\n#     != np.array(df_train[\"device_isMobile\"]))[0]\n# errors.shape","f97da75f":"# errors = np.where(\n#     ((np.array(train_easy_OHE_conv[:, 10]).astype(bool)) | (np.array(train_easy_OHE_conv[:, 9]).astype(bool)))\n#     != np.array(df_train[\"device_isMobile\"]))[0]\n# errors.shape","6e5ed0c1":"# display(df_train.iloc[errors, 10:])","dbe68784":"# deviceCategory_idx = 11\n# is_mobile_idx = 12","c7e124bb":"# df_train.iloc[errors, deviceCategory_idx] = (\n#     df_train.iloc[errors, is_mobile_idx].apply(lambda x: \"desktop\" if x == 0 else \"mobile\"))","e7ed65ed":"# errors_te = np.where(\n#     ((np.array(test_easy_OHE_conv[:, 10]).astype(bool)) | (np.array(test_easy_OHE_conv[:, 9]).astype(bool)))\n#     != np.array(df_test[\"device_isMobile\"]))[0]\n# errors_te.shape","7991b540":"# df_test.iloc[errors_te, deviceCategory_idx - 1] = (\n#     df_test.iloc[errors_te, is_mobile_idx - 1].apply(lambda x: \"desktop\" if x == 0 else \"mobile\"))","087e3724":"# train_easy_OHE = np.array(df_train[easy_OHE])\n# test_easy_OHE = np.array(df_test[easy_OHE])\n# easy_OEs = [OE() for i in range(train_easy_OHE.shape[1])]\n# for f in range(train_easy_OHE.shape[1]):\n#     easy_OEs[f].fit(train_easy_OHE[:, f])\n#     train_easy_OHE[:, f] = easy_OEs[f].transform(train_easy_OHE[:, f])\n#     test_easy_OHE[:, f] = easy_OEs[f].transform(test_easy_OHE[:, f])\n# easy_enc = OHE(sparse=False)\n# easy_enc.fit(train_easy_OHE)\n# train_easy_OHE_conv = easy_enc.transform(train_easy_OHE)\n# test_easy_OHE_conv = easy_enc.transform(test_easy_OHE)","9eb35be5":"# errors2 = np.where(\n#     ((np.array(train_easy_OHE_conv[:, 10]).astype(bool)) | (np.array(train_easy_OHE_conv[:, 9]).astype(bool)))\n#     != np.array(df_train[\"device_isMobile\"]))[0]\n# print(errors2.shape)\n# errors_te2 = np.where(\n#     ((np.array(test_easy_OHE_conv[:, 10]).astype(bool)) | (np.array(test_easy_OHE_conv[:, 9]).astype(bool)))\n#     != np.array(df_test[\"device_isMobile\"]))[0]\n# print(errors_te2.shape)","105a55a6":"# easy_OHE_names = ['channelGrouping_(Other)',\n#  'channelGrouping_Affiliates',\n#  'channelGrouping_Direct',\n#  'channelGrouping_Display',\n#  'channelGrouping_Organic Search',\n#  'channelGrouping_Paid Search',\n#  'channelGrouping_Referral',\n#  'channelGrouping_Social',\n#  'device_deviceCategory_desktop',\n#  'device_deviceCategory_mobile',\n#  'device_deviceCategory_tablet',\n#  'geoNetwork_continent_@',\n#  'geoNetwork_continent_Africa',\n#  'geoNetwork_continent_Americas',\n#  'geoNetwork_continent_Asia',\n#  'geoNetwork_continent_Europe',\n#  'geoNetwork_continent_Oceania',\n#  'geoNetwork_subContinent_@',\n#  'geoNetwork_subContinent_Australasia',\n#  'geoNetwork_subContinent_Caribbean',\n#  'geoNetwork_subContinent_Central America',\n#  'geoNetwork_subContinent_Central Asia',\n#  'geoNetwork_subContinent_Eastern Africa',\n#  'geoNetwork_subContinent_Eastern Asia',\n#  'geoNetwork_subContinent_Eastern Europe',\n#  'geoNetwork_subContinent_Melanesia',\n#  'geoNetwork_subContinent_Micronesian Region',\n#  'geoNetwork_subContinent_Middle Africa',\n#  'geoNetwork_subContinent_Northern Africa',\n#  'geoNetwork_subContinent_Northern America',\n#  'geoNetwork_subContinent_Northern Europe',\n#  'geoNetwork_subContinent_Polynesia',\n#  'geoNetwork_subContinent_South America',\n#  'geoNetwork_subContinent_Southeast Asia',\n#  'geoNetwork_subContinent_Southern Africa',\n#  'geoNetwork_subContinent_Southern Asia',\n#  'geoNetwork_subContinent_Southern Europe',\n#  'geoNetwork_subContinent_Western Africa',\n#  'geoNetwork_subContinent_Western Asia',\n#  'geoNetwork_subContinent_Western Europe',\n#  'trafficSource_medium_@',\n#  'trafficSource_medium_affiliate',\n#  'trafficSource_medium_cpc',\n#  'trafficSource_medium_cpm',\n#  'trafficSource_medium_organic',\n#  'trafficSource_medium_referral']","ab78fda5":"# df_test[\"trafficSource_adwordsClickInfo.adNetworkType\"] = (\n#     df_test[\"trafficSource_adwordsClickInfo.adNetworkType\"].apply(lambda x: \"@\" if x == 'Content' else x))\n# df_test[\"trafficSource_adwordsClickInfo.slot\"] = (\n#     df_test[\"trafficSource_adwordsClickInfo.slot\"].apply(lambda x: \"@\" if x == 'Google Display Network' else x))","2f8df0ab":"# for col_name in easy_OHE_but_prepare:\n#     print(col_name)\n#     un_train = np.unique(np.array(df_train[col_name]).astype(str), return_counts=True)\n#     un_test = np.unique(np.array(df_test[col_name]).astype(str), return_counts=True)\n#     print(\"TRAIN: \", un_train[0])\n#     print(\"TEST: \", un_test[0])\n#     print(\"DIFFERENT TRAIN COUNT: \", un_train[0].shape[0])\n#     print(\"DIFFERENT TEST COUNT: \", un_test[0].shape[0])\n#     print(\"-\" * 43)","398a03e9":"# train_easy_OHE_prep = np.array(df_train[easy_OHE_but_prepare])\n# test_easy_OHE_prep = np.array(df_test[easy_OHE_but_prepare])\n# easy_OEs_prep = [OE() for i in range(train_easy_OHE_prep.shape[1])]\n# for f in range(train_easy_OHE_prep.shape[1]):\n#     easy_OEs_prep[f].fit(train_easy_OHE_prep[:, f])\n#     train_easy_OHE_prep[:, f] = easy_OEs_prep[f].transform(train_easy_OHE_prep[:, f])\n#     test_easy_OHE_prep[:, f] = easy_OEs_prep[f].transform(test_easy_OHE_prep[:, f])","a1454706":"# easy_enc_prep = OHE(sparse=False)\n# easy_enc_prep.fit(train_easy_OHE_prep)\n# train_easy_OHE_prep_conv = easy_enc_prep.transform(train_easy_OHE_prep)\n# test_easy_OHE_prep_conv = easy_enc_prep.transform(test_easy_OHE_prep)","a8af8d77":"# easy_OHE_prep_names = ['trafficSource_adwordsClickInfo.adNetworkType_@',\n#  'trafficSource_adwordsClickInfo.adNetworkType_Google Search',\n#  'trafficSource_adwordsClickInfo.adNetworkType_Search partners',\n#  'trafficSource_adwordsClickInfo.slot_@',\n#  'trafficSource_adwordsClickInfo.slot_RHS',\n#  'trafficSource_adwordsClickInfo.slot_Top']","f74b099c":"# train_bad_cat = df_train[bad_categorial].copy()\n# test_bad_cat = df_test[bad_categorial].copy()\n# for f in bad_categorial:\n#     train_bad_cat[f], indexer = pd.factorize(train_bad_cat[f])\n#     test_bad_cat[f] = indexer.get_indexer(test_bad_cat[f])","3eb1bc18":"# y_train_clf = (y_train.ravel() > 0).astype(int)","0c4cb1a6":"# with_rev = df_train.iloc[y_train_clf == True, :][bad_categorial]\n# no_rev = df_train.iloc[y_train_clf == False, :][bad_categorial]","49fbd0a3":"# for feature in bad_categorial:\n#     vals = with_rev[feature]\n#     un, cnt = np.unique(vals, return_counts=True)\n#     data = np.hstack((un.reshape((-1, 1)), cnt.reshape((-1, 1))))\n\n#     top = 20\n#     df = pd.DataFrame(data, columns=[\"feature\", \"count features\"])\n#     plt.figure(figsize=(15, 8))\n#     sns.barplot(x=\"count features\", y='feature', data=df.sort_values(\"count features\", ascending=False).iloc[:top])\n#     plt.title(\"FEATURE {} HAVE REVENUE\".format(feature), fontsize=18)\n#     plt.show()\n    \n#     vals = no_rev[feature]\n#     un, cnt = np.unique(vals, return_counts=True)\n#     data = np.hstack((un.reshape((-1, 1)), cnt.reshape((-1, 1))))\n#     top = 20\n#     df = pd.DataFrame(data, columns=[\"feature\", \"count features\"])\n#     plt.figure(figsize=(15, 8))\n#     sns.barplot(x=\"count features\", y='feature', data=df.sort_values(\"count features\", ascending=False).iloc[:top])\n#     plt.title(\"FEATURE {} HAVEN'T REVENUE\".format(feature), fontsize=18)\n#     plt.show()","2430d60e":"# top_rev =    [5, 0, 6, 20, 6, 4, 7, 10, 2, 5, 5, 16, 5]\n# top_no_rev = [11, 0, 6, 10, 5, 10, 7, 4, 3, 4, 5, 10, 7] ","225b78ef":"# bad_cat_to_OHE = defaultdict(set)\n\n# for feature, t_r, t_n_r in zip(bad_categorial, top_rev, top_no_rev):\n#     vals = with_rev[feature]\n#     un, cnt = np.unique(vals, return_counts=True)\n#     sortidx = np.argsort(cnt)\n#     bad_cat_to_OHE[feature] |= set(un[sortidx][::-1][:t_r])\n    \n#     vals = no_rev[feature]\n#     un, cnt = np.unique(vals, return_counts=True)\n#     sortidx = np.argsort(cnt)\n#     bad_cat_to_OHE[feature] |= set(un[sortidx][::-1][:t_n_r])\n#     if \"@\" in bad_cat_to_OHE[feature]:\n#         bad_cat_to_OHE[feature].remove(\"@\")\n        \n#     test_vals = set(df_test[feature])\n#     bad_cat_to_OHE[feature] &= test_vals","46b95535":"# ordered_dict = dict()\n# for (name, value) in bad_cat_to_OHE.items():\n#     ordered_dict[name] = list(value)\n    \n# bad_cat_features_OHE_names = []\n# for (name, value) in ordered_dict.items():\n#     if len(value) > 0:\n#         for val in value:\n#             bad_cat_features_OHE_names.append(\"{}_{}\".format(name, val))","77ea2638":"# add_len = len(bad_cat_features_OHE_names)\n# add_len","167dedff":"# train_bad_cat_to_OHE = np.zeros((df_train.shape[0], add_len))\n# test_bad_cat_to_OHE = np.zeros((df_test.shape[0], add_len))\n# for num, feature_value in enumerate(bad_cat_features_OHE_names):\n#     idx = feature_value.rfind(\"_\")\n#     feature, value = feature_value[:idx], feature_value[idx + 1:]\n#     train_bad_cat_to_OHE[:, num] = np.array(df_train[feature] == value).astype(int)\n#     test_bad_cat_to_OHE[:, num] = np.array(df_test[feature] == value).astype(int)","d6a2f7ef":"# bad_categorial += [\"weekday\"]","04f1d590":"# tr_wd = np.array(pd.to_datetime(df_train[\"date\"], format=\"%Y%m%d\").apply(lambda x: x.weekday())).reshape((-1, 1))\n# te_wd = np.array(pd.to_datetime(df_test[\"date\"], format=\"%Y%m%d\").apply(lambda x: x.weekday())).reshape((-1, 1))","37b07235":"# X_train = np.hstack((X_train_numerical, train_easy_OHE_conv,\n#                       train_easy_OHE_prep_conv, train_bad_cat_to_OHE, train_bad_cat, tr_wd))\n# X_test = np.hstack((X_test_numerical, test_easy_OHE_conv,\n#                      test_easy_OHE_prep_conv, test_bad_cat_to_OHE, test_bad_cat, te_wd))","c362e717":"# feature_names = numerical + easy_OHE_names + easy_OHE_prep_names + bad_cat_features_OHE_names + bad_categorial\n# len(feature_names), X_train.shape","ab44a39d":"from sklearn.decomposition import TruncatedSVD, PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\nimport scipy\nimport pickle\nimport re\nimport time\nfrom sklearn.utils import shuffle as skshuffle","ee51bb26":"# def visualize_data(data_train, y_train, data_test, amount_train=None, amount_test=None,\n#                    scale=True, threeD=False, shuffle=False, alpha=1.0):\n    \n#     def visualize_2d(data_train, y_train, data_test):\n#         tsvd = PCA(n_components=2)\n#         tsvd.fit(data_train)\n#         Z_train = tsvd.fit_transform(data_train)\n#         Z_test = tsvd.transform(data_test)\n\n#         classes_amount = np.unique(y_train).shape[0]\n#         cm = plt.get_cmap('jet')\n#         plt.figure(figsize=(18, 15))\n#         plt.scatter(Z_train[:, 0], Z_train[:, 1],\n#                     c=y_train.ravel(), cmap='RdYlGn_r', alpha=alpha) # green - 1 blue 0\n#         plt.title(\"Train data\")\n#         plt.figure(figsize=(18, 15))\n#         plt.scatter(Z_test[:, 0], Z_test[:, 1],\n#                     c=[\"plum\"] * Z_test.shape[0], alpha=alpha)\n#         plt.title(\"Test data\")\n#         plt.show()\n        \n#     def visualize_3d(data_train, y_train, data_test):\n#         tsvd = PCA(n_components=3)\n#         tsvd.fit(data_train)\n#         Z_train = tsvd.fit_transform(data_train)\n#         Z_test = tsvd.transform(data_test)\n\n#         classes_amount = np.unique(y_train).shape[0]\n#         cm = plt.get_cmap('jet')\n        \n#         fig = plt.figure(figsize=(18, 15))\n#         ax = fig.add_subplot(111, projection='3d')\n#         sc = ax.scatter(Z_train[:, 0], Z_train[:, 1], Z_train[:, 2],\n#                 c=y_train.ravel(), cmap='RdYlGn_r', alpha=alpha)\n#         plt.title(\"Train data\")\n#         fig = plt.figure(figsize=(18, 15))\n#         ax = fig.add_subplot(111, projection='3d')\n#         sc = ax.scatter(Z_test[:, 0], Z_test[:, 1], Z_test[:, 2],\n#                 c=[\"plum\"] * Z_test.shape[0], alpha=alpha)\n#         plt.title(\"Test data\")\n#         plt.show()\n        \n#     if amount_train is None:\n#         amount_train = data_train.shape[0]\n#     else:\n#         amount_train = int(amount_train * data_train.shape[0])\n        \n#     if amount_test is None:\n#         amount_test = data_test.shape[0]\n#     else:\n#         amount_test = int(amount_test * data_test.shape[0])\n        \n#     if shuffle:\n#         data_train, y_train = skshuffle(data_train, y_train)\n#         data_test = skshuffle(data_test)\n    \n#     if scale:\n#         scaler = StandardScaler(copy=True)\n#         scaler.fit(data_train)\n#         X_train = scaler.transform(data_train)\n#         X_test = scaler.transform(data_test)\n#     else:\n#         X_train = data_train\n#         X_test = data_test\n        \n#     start = time.time()\n#     if threeD:\n#         visualize_3d(X_train[:amount_train, :], y_train[:amount_train], X_test[:amount_train, :])\n#     else:\n#         visualize_2d(X_train[:amount_train, :], y_train[:amount_train], X_test[:amount_train, :])\n#     print(time.time() - start)","5ef5994c":"# y_train_clf = np.copy(y_train)\n# y_train_clf[y_train_clf > 0] = 1\n# y_train_clf[y_train_clf == 0] = 0","abfa9cf5":"# visualize_data(X_train, y_train_clf, X_test, amount_train=0.2,\n#                amount_test=0.2, threeD=False, alpha=0.5, scale=True, shuffle=True)","8a2a8530":"# visualize_data(X_train, y_train_clf, X_test, amount_train=0.2,\n#                amount_test=0.2, threeD=True, alpha=0.5, scale=True, shuffle=True)","d44f4e9a":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold, GroupKFold, KFold\nfrom sklearn.metrics import mean_squared_error, roc_auc_score, log_loss\nfrom sklearn.preprocessing import StandardScaler as SS","1b96536a":"def tune_threshold(base_cv, preds, y_true):\n    diffs = []\n    MIN_VALS = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n    for MIN_VAL in MIN_VALS:\n        print(\"TH: \", MIN_VAL)\n        print(\"RMSE CV {}\".format(base_cv))\n        y2 = np.expm1(preds)\n        y2[y2 < MIN_VAL] = 0\n        neo = mean_squared_error(np.log1p(y_true), np.log1p(y2)) ** .5\n        print(\"RMSE NEW CV: \", neo)\n        print(\"DIFF: \", base_cv - neo)\n        diffs.append(base_cv - neo)\n        print(\"-\" * 43)\n    idx = np.argmax(np.array(diffs))\n    return MIN_VALS[idx]","5e7f797e":"def get_folds(visitor_ids_in_train, n_splits=5):\n    \"\"\"Returns dataframe indices corresponding to Visitors Group KFold\"\"\"\n    # Get sorted unique visitors\n    unique_vis = np.array(sorted(np.unique(visitor_ids_in_train)))\n\n    # Get folds\n    folds = GroupKFold(n_splits=n_splits)\n    fold_ids = []\n    ids = np.arange(visitor_ids_in_train.shape[0])\n    df = pd.DataFrame()\n    df[\"fullVisitorId\"] = visitor_ids_in_train\n    for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n        fold_ids.append(\n            [\n                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],\n                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]\n            ]\n        )\n\n    return fold_ids","69bdbd65":"# bad_features = [\n#        'trafficSource_adwordsClickInfo.isVideoAd',\n#        'channelGrouping_(Other)', 'geoNetwork_subContinent_Australasia',\n#        'geoNetwork_subContinent_Central Asia',\n#        'geoNetwork_subContinent_Eastern Africa',\n#        'geoNetwork_subContinent_Eastern Europe',\n#        'geoNetwork_subContinent_Melanesia',\n#        'geoNetwork_subContinent_Micronesian Region',\n#        'geoNetwork_subContinent_Middle Africa',\n#        'geoNetwork_subContinent_Northern Africa',\n#        'geoNetwork_subContinent_Polynesia',\n#        'geoNetwork_subContinent_Southern Africa',\n#        'geoNetwork_subContinent_Southern Asia',\n#        'geoNetwork_subContinent_Western Africa',\n#        'trafficSource_adwordsClickInfo.adNetworkType_Search partners',\n#        'trafficSource_adwordsClickInfo.slot_@',\n#        'trafficSource_adwordsClickInfo.slot_RHS'\n# ]\n\n# bad_features2 = [\n#        'device_browser_Android Webview', 'device_browser_Opera',\n#        'device_browser_Opera Mini', 'device_browser_Safari (in-app)',\n#        'device_browser_UC Browser', 'device_browser_YaBrowser',\n#        'geoNetwork_city_Bangkok', 'geoNetwork_city_Ho Chi Minh City',\n#        'geoNetwork_city_London', 'geoNetwork_continent_Africa',\n#        'geoNetwork_continent_Oceania', 'geoNetwork_country_Germany',\n#        'geoNetwork_country_India', 'geoNetwork_country_Japan',\n#        'geoNetwork_country_Mexico', 'geoNetwork_country_Thailand',\n#        'geoNetwork_country_Turkey', 'geoNetwork_country_United Kingdom',\n#        'geoNetwork_country_Vietnam', 'geoNetwork_metro_London',\n#        'geoNetwork_networkDomain_hinet.net',\n#        'geoNetwork_networkDomain_ttnet.com.tr',\n#        'geoNetwork_region_England',\n#        'geoNetwork_subContinent_Southeast Asia',\n#        'geoNetwork_subContinent_Southern Europe',\n#        'geoNetwork_subContinent_Western Europe',\n#        'trafficSource_adContent_Google Online Store',\n#        'trafficSource_adwordsClickInfo.adNetworkType_Google Search',\n#        'trafficSource_campaign_Data Share Promo',\n#        'trafficSource_medium_affiliate', 'trafficSource_medium_cpc',\n#        'trafficSource_source_Partners', 'trafficSource_source_baidu',\n#        'trafficSource_source_facebook.com',\n#        'trafficSource_source_gdeals.googleplex.com',\n#        'trafficSource_source_google.com',\n#        'trafficSource_source_m.facebook.com'\n# ]\n\n# bad_features2 += [\"visitStartTime\", \"date\"]","a8c618c6":"# len(feature_names)","b838b4ba":"# categorial = bad_categorial\n# names_indices = dict()\n# for idx, name in enumerate(feature_names):\n#     names_indices[name] = idx\n    \n# good_indices = []\n# new_feature_names = []\n# for (name, idx) in names_indices.items():\n#     if name not in bad_features and name not in bad_features2:\n#         good_indices.append(idx)\n#         new_feature_names.append(name)\n\n# X_train = X_train[:, good_indices]\n# X_test = X_test[:, good_indices]\n\n# idx = numerical.index(\"visitStartTime\")\n# numerical = numerical[:idx] + numerical[idx + 1:]\n# idx = categorial.index(\"date\")\n# categorial = categorial[:idx] + categorial[idx + 1:]\n\n# cat_indices = np.arange(X_train.shape[1] - len(categorial), X_train.shape[1])","c716a2fb":"# y_train = y_train.ravel()\n# X_train.shape","94f33428":"\"\"\"data_pkl = {\n    \"X_train\" : X_train,\n    \"y_train\" : y_train,\n    \"X_test\" : X_test,\n    \"feature_names\" : new_feature_names,\n    \"categorial\" : categorial,\n    \"numerical\" : numerical,\n    \"id_numeration_train\" : id_numeration_train,\n    \"id_numeration_test\" : id_numeration_test,\n}\nwith open(\".\/kernel\/prepaired_data\/data_all_features.pkl\", \"wb\") as fout:\n    pickle.dump(data_pkl, fout)\n\"\"\"","8c4f1219":"gc.collect()","46587c5f":"with open(\"..\/input\/prepaired-data-of-customer-revenue-prediction\/data_all_features.pkl\", \"rb\") as fin:\n    data = pickle.load(fin)\nX_train, y_train = data[\"X_train\"], data[\"y_train\"]\nX_test = data[\"X_test\"]\nid_numeration_train = data[\"id_numeration_train\"]\nid_numeration_test = data[\"id_numeration_test\"]\nfeature_names = data[\"feature_names\"]\nnumerical = data[\"numerical\"]\ncategorial = data[\"categorial\"]","d4d3cfeb":"X_train.shape","572eb983":"scaler = SS()\nscaler.fit(X_train[:, :len(numerical)])\nX_train_scaled = np.copy(X_train)\nX_test_scaled = np.copy(X_test)","a5860ce6":"X_train_scaled[:, :len(numerical)] = scaler.transform(X_train[:, :len(numerical)])\nX_test_scaled[:, :len(numerical)] = scaler.transform(X_test[:, :len(numerical)])","c7ab894e":"cat_indices = np.arange(X_train.shape[1] - len(categorial), X_train.shape[1])","358d31f8":"cat_indices","abb8377f":"X_train_new = np.hstack((X_train_scaled[:, cat_indices], X_train_scaled[:, :cat_indices[0]]))\nX_test_new = np.hstack((X_test_scaled[:, cat_indices], X_test_scaled[:, :cat_indices[0]]))","8f4b8acd":"new_feature_names = feature_names","af976121":"cat_ind_new = [int(x) for x in np.arange(len(cat_indices)).astype(np.int32)]\nnew_new_feature_names = new_feature_names[-len(cat_ind_new):] + new_feature_names[:-len(cat_ind_new)]","1dcca9ce":"n_splits = 5\n\nfolds = get_folds(id_numeration_train.ravel(), n_splits=n_splits)\n\noof_reg_preds = np.zeros(X_train.shape[0])\nsub_reg_preds_cv = np.zeros(X_test.shape[0])\nimportances = pd.DataFrame()\n\nnames = new_new_feature_names\n\nfor fold_, (trn_, val_) in enumerate(folds):\n    trn_x, trn_y = X_train_new[trn_], y_train[trn_]\n    val_x, val_y = X_train_new[val_], y_train[val_]\n    \n    reg = lgb.LGBMRegressor(\n        num_leaves=31,\n        learning_rate=0.03,\n        n_estimators=1543 + 1543,\n        subsample=.9,\n        max_depth=15,\n        colsample_bytree=.9,\n        random_state=1543\n    )\n    start = time.time()\n    reg.fit(\n        trn_x, np.log1p(trn_y),\n        eval_set=[(trn_x, np.log1p(trn_y)), (val_x, np.log1p(val_y))],\n        eval_names=['TRAIN', 'VALID'],\n        early_stopping_rounds=50,\n        verbose=50,\n        eval_metric=\"rmse\",\n        categorical_feature=cat_ind_new\n    )\n    print(\"REGR TIME: \", time.time() - start)\n    imp_df = pd.DataFrame()\n    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n    imp_df['feature'] = names\n    imp_df['fold'] = fold_ + 1\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n    \n    oof_reg_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    oof_reg_preds[oof_reg_preds < 0] = 0\n    _preds = reg.predict(X_test_new, num_iteration=reg.best_iteration_)\n    _preds[_preds < 0] = 0\n    sub_reg_preds_cv += _preds \/ n_splits # maybe fix\n    \nprint(\"CV RMSE: \", mean_squared_error(np.log1p(y_train), oof_reg_preds) ** .5)","5ccfa521":"importances['gain_log'] = np.log1p(importances['gain'])\nmean_gain = importances[['gain', 'feature']].groupby('feature').mean()\nimportances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(10, 30))\nsns.barplot(x='gain_log', y='feature', data=importances.sort_values('mean_gain', ascending=False))","998ff047":"base = mean_squared_error(np.log1p(y_train), oof_reg_preds) ** .5\nMIN_VAL = tune_threshold(base, oof_reg_preds, y_train)","17ef6d9b":"MIN_VAL","4a8f8dc8":"y_p1 = np.expm1(sub_reg_preds_cv)\ny_p1[y_p1 < MIN_VAL] = 0\nsub_reg_preds_cv_correted = np.log1p(y_p1)","7e9e1bad":"del X_train_scaled, X_test_scaled, X_train_new, X_test_new\ngc.collect()","f52b45a8":"gc.collect()","5c021d7c":"train_num = pd.DataFrame(X_train[:, :-len(categorial)], columns=new_feature_names[:-len(categorial)], index=None)\ntrain_num[\"fullVisitorId\"] = id_numeration_train\ntrn_data_num = train_num.groupby('fullVisitorId').mean()\n#trn_data_num1.columns = [\"{}_mean\".format(x) for x in trn_data_num1.columns]\n#trn_data_num2 = train_num.groupby(\"fullVisitorId\").std().fillna(0)\n#trn_data_num2.columns = [\"{}_std\".format(x) for x in trn_data_num2.columns]","5f701ad3":"#trn_data_num = pd.concat([trn_data_num1, trn_data_num2], axis=1)\n#trn_data_num.shape","a820a0e4":"#del trn_data_num1, trn_data_num2\ngc.collect()","a08830ff":"# it's works about 2 hours\n\"\"\"\ntrain_cat = pd.DataFrame(X_train[:, -len(categorial):], columns=new_feature_names[-len(categorial):], index=None)\ntrain_cat[\"fullVisitorId\"] = id_numeration_train\ntrn_data_cat = train_cat.groupby(\"fullVisitorId\").aggregate(lambda x: x.value_counts().index[0])\ntrn_data_cat.to_csv(\".\/kernel\/prepaired_data\/train_categorial_features_moda.csv\", sep=\",\")\n\"\"\"","154f7a36":"trn_data_cat = pd.read_csv(\"..\/input\/prepaired-data-of-customer-revenue-prediction\/train_categorial_features_moda.csv\", sep=\",\",\n                           converters={\"fullVisitorId\" : str}, index_col=\"fullVisitorId\")","e7880928":"np.all(trn_data_num.index == trn_data_cat.index)","4dc14ec0":"trn_data = pd.concat([trn_data_num, trn_data_cat], axis=1)","f60ee729":"train_num[\"pred\"] = np.expm1(oof_reg_preds)\ntrn_pred_list = train_num[['fullVisitorId', 'pred']].groupby('fullVisitorId')\\\n    .apply(lambda df: list(df.pred))\\\n    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})\ntrn_all_predictions = pd.DataFrame(list(trn_pred_list.values), index=trn_data.index)\ntrn_feats = trn_all_predictions.columns","3428d693":"del train_num, trn_data_num, trn_data_cat\ngc.collect()","19d95c8f":"trn_all_predictions['t_mean'] = np.log1p(trn_all_predictions[trn_feats].mean(axis=1))\ntrn_all_predictions['t_median'] = np.log1p(trn_all_predictions[trn_feats].median(axis=1))\ntrn_all_predictions['t_std'] = np.log1p(trn_all_predictions[trn_feats].std(axis=1).fillna(0))\ntrn_all_predictions['t_min'] = np.log1p(trn_all_predictions[trn_feats].min(axis=1))\ntrn_all_predictions['t_var_coef'] = np.log1p(trn_all_predictions[trn_feats].std(axis=1).fillna(0) \/ (trn_all_predictions[trn_feats].mean(axis=1) + 1e-6))\ntrn_all_predictions['t_max'] = np.log1p(trn_all_predictions[trn_feats].max(axis=1))\ntrn_all_predictions['t_sum_log'] = np.log1p(trn_all_predictions[trn_feats]).sum(axis=1)\ntrn_all_predictions['t_sum_act'] = np.log1p(trn_all_predictions[trn_feats].fillna(0).sum(axis=1))\ntrn_all_predictions['t_nb_sess'] = trn_all_predictions[trn_feats].isnull().sum(axis=1)","2e032d35":"trn_all_predictions['t_std'] = trn_all_predictions['t_std'].fillna(0)\ntrn_all_predictions['t_var_coef'] = trn_all_predictions['t_var_coef'].fillna(0)","454bfe1c":"full_data = pd.concat([trn_data, trn_all_predictions], axis=1)","23a5ef26":"del trn_data, trn_all_predictions\ngc.collect()","44f0d712":"test_num = pd.DataFrame(X_test[:, :-len(categorial)], columns=new_feature_names[:-len(categorial)], index=None)\ntest_num[\"fullVisitorId\"] = id_numeration_test\nsub_data_num = test_num.groupby('fullVisitorId').mean()\n#sub_data_num1.columns = [\"{}_mean\".format(x) for x in sub_data_num1.columns]\n#sub_data_num2 = test_num.groupby('fullVisitorId').std().fillna(0)\n#sub_data_num2.columns = [\"{}_std\".format(x) for x in sub_data_num2.columns]","0a2ca16f":"#sub_data_num = pd.concat([sub_data_num1, sub_data_num2], axis=1)\n#sub_data_num.shape","d5e75679":"#del sub_data_num1, sub_data_num2\ngc.collect()","e6ae07f5":"# it's works about 2 hours\n\"\"\"\ntest_cat = pd.DataFrame(X_test[:, -len(categorial):], columns=new_feature_names[-len(categorial):], index=None)\ntest_cat[\"fullVisitorId\"] = id_numeration_test\ntest_data_cat = test_cat.groupby(\"fullVisitorId\").aggregate(lambda x: x.value_counts().index[0])\ntest_data_cat.to_csv(\".\/kernel\/prepaired_data\/test_categorial_features_moda.csv\", sep=\",\")\n\"\"\"","ad314853":"test_data_cat = pd.read_csv(\"..\/input\/prepaired-data-of-customer-revenue-prediction\/test_categorial_features_moda.csv\", sep=\",\",\n                           converters={\"fullVisitorId\" : str}, index_col=\"fullVisitorId\")","f49e0fa3":"np.all(sub_data_num.index == test_data_cat.index)","292a700a":"sub_data = pd.concat([sub_data_num, test_data_cat], axis=1)","9e45de3f":"del sub_data_num, test_data_cat\ngc.collect()","89cbc1a9":"test_num[\"pred\"] = np.expm1(sub_reg_preds_cv_correted)\nsub_pred_list = test_num[['fullVisitorId', 'pred']].groupby('fullVisitorId')\\\n    .apply(lambda df: list(df.pred))\\\n    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})","72b4b74b":"del test_num\ngc.collect()","606d54b5":"sub_all_predictions = pd.DataFrame(list(sub_pred_list.values), index=sub_data.index)\nfor f in trn_feats:\n    if f not in sub_all_predictions.columns:\n        sub_all_predictions[f] = np.nan\nsub_all_predictions['t_mean'] = np.log1p(sub_all_predictions[trn_feats].mean(axis=1))\nsub_all_predictions['t_median'] = np.log1p(sub_all_predictions[trn_feats].median(axis=1))\nsub_all_predictions['t_std'] = np.log1p(sub_all_predictions[trn_feats].std(axis=1).fillna(0))\nsub_all_predictions['t_min'] = np.log1p(sub_all_predictions[trn_feats].min(axis=1))\nsub_all_predictions['t_var_coef'] = np.log1p(sub_all_predictions[trn_feats].std(axis=1).fillna(0) \/ (sub_all_predictions[trn_feats].mean(axis=1) + 1e-6))\nsub_all_predictions['t_max'] = np.log1p(sub_all_predictions[trn_feats].max(axis=1))\nsub_all_predictions['t_sum_log'] = np.log1p(sub_all_predictions[trn_feats]).sum(axis=1)\nsub_all_predictions['t_sum_act'] = np.log1p(sub_all_predictions[trn_feats].fillna(0).sum(axis=1))\nsub_all_predictions['t_nb_sess'] = sub_all_predictions[trn_feats].isnull().sum(axis=1)\n","5bf41991":"sub_all_predictions['t_std'] = sub_all_predictions['t_std'].fillna(0)\nsub_all_predictions['t_var_coef'] = sub_all_predictions['t_var_coef'].fillna(0)","6ec7faf9":"sub_full_data = pd.concat([sub_data, sub_all_predictions], axis=1)","31c44a82":"del sub_data, sub_all_predictions\ngc.collect()","e4fed11a":"full_data.shape, sub_full_data.shape","61ca7f83":"tmp = pd.DataFrame()\ntmp[\"fullVisitorId\"] = id_numeration_train.ravel()\ntmp['target'] = y_train\ntrn_user_target = tmp[['fullVisitorId', 'target']].groupby('fullVisitorId').sum()","ebb3e213":"feat = categorial + list(set(full_data.columns) - set(categorial))\ncat_ind = [int(x) for x in np.arange(len(categorial))]","1375e6b5":"full_data = full_data[feat]\nsub_full_data = sub_full_data[feat]","a9a215ca":"features_to_scale = list(set(full_data.columns) - set(categorial) - set(trn_feats))","97050bd2":"scaler2 = SS()\nscaler2.fit(np.array(full_data[features_to_scale]))","aadad4b4":"full_data[features_to_scale] = scaler2.transform(np.array(full_data[features_to_scale]))\nsub_full_data[features_to_scale] = scaler2.transform(np.array(sub_full_data[features_to_scale]))","77e7d89b":"params={'learning_rate': 0.05,\n        'objective':'regression',\n        'metric':'rmse',\n        'num_leaves': 200,\n        'verbose': 1,\n        \"subsample\": 0.99,\n        \"colsample_bytree\": 0.99,\n        \"random_state\":1543,\n        'max_depth': 14,\n        'lambda_l2': 0.02085548700474218,\n        'lambda_l1': 0.004107624022751344,\n        'bagging_fraction': 0.7934712636944741,\n        'feature_fraction': 0.686612409641711,\n}","fd8e5a84":"gc.collect()","3bc92735":"n_splits=5\nfolds2 = get_folds(np.array(trn_user_target.index), n_splits=n_splits)\n\noof_preds2 = np.zeros(full_data.shape[0])\nsub_preds2 = np.zeros(sub_full_data.shape[0])\nvis_importances = pd.DataFrame()\n\nfeat = categorial + list(set(full_data.columns) - set(categorial))\n\nfor fold_, (trn_, val_) in enumerate(folds2):\n    trn_x, trn_y = np.array(full_data.iloc[trn_][feat]), np.array(trn_user_target['target'].iloc[trn_])\n    val_x, val_y = np.array(full_data.iloc[val_][feat]), np.array(trn_user_target['target'].iloc[val_])\n    \n    reg = lgb.LGBMRegressor(**params,\n        #num_leaves=31,\n        # learning_rate=0.03,\n        n_estimators=1543,\n        # subsample=.93,\n        # colsample_bytree=.94,\n        # random_state=1543\n    )\n    start = time.time()\n    reg.fit(\n        trn_x, np.log1p(trn_y),\n        eval_set=[(trn_x, np.log1p(trn_y)), (val_x, np.log1p(val_y))],\n        eval_names=['TRAIN', 'VALID'],\n        early_stopping_rounds=50,\n        eval_metric='rmse',\n        verbose=50,\n        categorical_feature=cat_ind\n    )\n    print(\"FIT TIME: {}\".format(time.time() - start))\n    imp_df = pd.DataFrame()\n    imp_df['feature'] = feat\n    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n    \n    imp_df['fold'] = fold_ + 1\n    vis_importances = pd.concat([vis_importances, imp_df], axis=0, sort=False)\n    \n    oof_preds2[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    oof_preds2[oof_preds2 < 0] = 0\n    \n    # Make sure features are in the same order\n    _preds2 = reg.predict(np.array(sub_full_data[feat]), num_iteration=reg.best_iteration_)\n    _preds2[_preds2 < 0] = 0\n    sub_preds2 += _preds2 \/ n_splits # ensembling\n    \nprint(\"RMSE CV: \", mean_squared_error(np.log1p(trn_user_target['target']), oof_preds2) ** .5)","9d519657":"oof_preds2.max(), _preds2.max()","2fc99628":"MIN_VAL2 = tune_threshold(\n    mean_squared_error(np.log1p(trn_user_target['target']), oof_preds2) ** .5,\n    oof_preds2, trn_user_target['target'])","a2f6cdfa":"vis_importances['gain_log'] = np.log1p(vis_importances['gain'])\nmean_gain = vis_importances[['gain', 'feature']].groupby('feature').mean()\nvis_importances['mean_gain'] = vis_importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(8, 200))\nsns.barplot(x='gain_log', y='feature', data=vis_importances.sort_values('mean_gain', ascending=False))","b62604da":"y_pred = np.expm1(sub_preds2)\ny_pred[y_pred < MIN_VAL2] = 0","df2b4157":"sub_full_data['PredictedLogRevenue'] = np.log1p(y_pred)\nsub_full_data[['PredictedLogRevenue']].to_csv('kernel_submit_1.csv', index=True) ","3f5401a4":"**based on ideas of https:\/\/www.kaggle.com\/ogrellier\/using-classification-for-predictions\/notebook and\nhttps:\/\/www.kaggle.com\/nikitpatel\/lightgbm-with-best-hyperparameter-tuning\/notebook**","e2cdd86b":"**Scale features and train-predict:**","6265b5fa":"**Fill nans:**","cf8bdcb6":"**Apparently, there are errors in deviceCategory rather than in isMobie. Moreover, in erroneous examples in favor of the error says feature OS. Let's make this correction: where isMobile in erroneous examples = 0 -- set desktop, otherwise -- mobile.**","8c7d535b":"**Let's visualize train and test:**","4403c8f2":"**Submit prediction**","961e5669":"**After watching this values I aggreagated the following groups of featues**","2de0d0dc":"**Explore the objects with same SessionId**","6a0d0ed0":"### 2. Data exploring and filtering. Part 1","0fa51187":"**Let's factorize all bad_categorial features**","0f36c021":"**next cell based on https:\/\/www.kaggle.com\/nikitpatel\/lightgbm-with-best-hyperparameter-tuning\/notebook**","8dec1b60":"**Save data:**","fd795d40":"**Explore features from wanted_to_trash**","0f10986a":"**Collect all together:**","cbe664d4":"**Remove meaningless features:**","7103d897":"**Use this kernel https:\/\/www.kaggle.com\/ravann\/1-step-by-step-format-data-to-columnar-format\/notebook\n**","c6efeeac":"**Check if is that true that there are only two record for such \"bad\" equal SessionId:**","7432e32d":"### First part of train-prediction logic","d16af076":"**Load dataset**","479c4f26":"### 1. Convert data to json format","aa33c556":"**View dataset**","97038270":"**Add moda for _categorial_ features**","cd9ee349":"**Add statistic features for _numerical_ features**","06196ab4":"**There is a little intersect and it seems to be such king of information coding. I suppose we should remove this feature**","4cd243aa":"**Let's see on VisitId:**","86914e7d":"**So delete objects and SessionId:**","58e48123":"**Process test data:**","dac335ab":"**Check whether columns in train and in test are equal**","92d15b0e":"**No, it's not a truth. Suppose that isMobile means that device_deviceCategory equals \"mobile\" or \"tablet\":**","3c29c823":"**Save result**","13ff7d0d":"**Let's see on features date and SessionId. Obviously if SessionIds are the same then fullVisitodIds are also the same. It seems that difference between dates for equal SessinId is 1 day. Perhaps the session was started at one day and ended at the next day, but there are two records about this one session.  So we can not only delete feature SessionId but also delete duplicated objects with the same SessionId (delete more earlier objects because there is much more information about later objects).**","0aedf358":"**Let's encode easy_OHE_but_prepare:**","5273849c":"**There is no errors!**","2a3b0d4b":"**Explore feature values in train**","dfea4257":"### 3. Data exploring and filtering. Part 2","a801d56f":"**Remove this \"usefull\" feature from train and test**","372938d3":"**View at \"socialEngagementType\"**","3e7abe24":"**Process train data:**","67ab10a9":"**Add to categorial features feature weekday:**","24f037ab":"**Think about feature trafficSource_adwordsClickInfo.gclId: it has a lot of nans**","5c0f4528":"**Remove features from trash_trash**","da63dd56":"* numerical -- _numerical features_\n* categorial -- _categorial features_\n* saved_trash -- _actually fullVisitorId_\n* trash_trash -- _unusefull features. Has a lot of nans or the same value for all objects_\n* wanted_to_trash -- _explore this featuers now but wants to delete them (e.g. all values are different for all objects)_\n* in_future_wanted_to_trash -- <i>features that can be dublicated by others (e.g. divice_isMobile may be duplicated by One-Hot-encoding of device_deviceCategory) or there is not any dependence between them and answer. But now don't remove them<\/i>\n* to_heal -- _fill nans by other value_\n* answer_feature -- _feature with answer_","9a589462":"**Seems that SessionId and visitId should be removed. Let see at them:**","1e04f109":"**Remove strange column trafficSource_campaignCode from test**","936ec7a8":"**Add some new OHE features from categorial features**","912f21c0":"### 0. Intro\n\nSorry for commenting alomost all code, but kaggle servers can't calculate all at commit.\n\nIt's simple for report in university course","dd4ec521":"**We can select the following groups of categorial features:**","9cd55231":"**Suppose that isMobile means that device_deviceCategory equals \"mobile\":**","216cdcbf":"**Red points -- records with some revenue**","81a57570":"**So it seems that this feature is important**","d168e833":"* easy_OHE -- _easy to do One-Hot-Encoding_\n* easy_OHE_but_prepare -- _easy to do One-Hot-Encoding, but feature values in train and in test are not the same_\n* bad_categorial -- _other categorial features_","107fc2de":"**But some of values of features let's convert via OHE. Explore how feature values are distributed for record with some revenue and with no revenue:**","d279c9e1":"**It's truth, but there is some errors that are needed to be fixed**","9f904939":"### Second part of train-prediction logic","81730698":"**Based on some several attempts to fit lgbm I selected the following groups of meaningless features:**","db21aeec":"**Let explore how many objects has revenue when trafficSource_referralPath isn't a nan**","d14e5a31":"**Assumption: since we will encode the device_deviceCategory, the isMobile feature can be thrown out, as it will be a duplicate. So we will check it**","20f587a2":"**Let's make a top of values for each feature:**","f6b0d04e":"**We can see that records with same visitId has different fullVisitorId but the same visitStartTime. Because of simultaneous sessions data was received concurrently and this sessions has the same visidId. So we can't remove records with same visitId but can remove feature visitId**","a3ff0dbd":"**Scale numerical features:**","4468ed15":"### 4. Train and predict","37efb21f":"**Think about feature trafficSource_referralPath:**","fd855d8d":"**Fill missing values in totals_transactionRevenues by zeros**"}}