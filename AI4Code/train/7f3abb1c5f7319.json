{"cell_type":{"13538355":"code","e13689ac":"code","9ae45c08":"code","e85b6f9a":"code","76323eb5":"code","d10502db":"code","0f957e72":"code","523fa8b0":"code","72e68c7d":"code","e1de17d1":"code","61d51358":"code","9b93555b":"code","7252704a":"code","7de9365b":"code","19db7ade":"code","457cae2f":"code","fdaaca8e":"code","819fe5f3":"code","b85d9b53":"code","99019df7":"code","b391b0e8":"code","1397b225":"code","fbcaa28f":"code","d8f3fc06":"code","c5487536":"code","b48c8a05":"code","7b87e371":"code","071944ee":"markdown","9918138a":"markdown","25710aaf":"markdown","90736b0c":"markdown","7ed8547e":"markdown","e6662f16":"markdown","cff0e503":"markdown"},"source":{"13538355":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e13689ac":"df_train= pd.read_csv('\/kaggle\/input\/leaf-classification\/train.csv.zip')\ndf_test = pd.read_csv('\/kaggle\/input\/leaf-classification\/test.csv.zip')","9ae45c08":"df_train.info()","e85b6f9a":"df_test.info()","76323eb5":"df_train.head(10)","d10502db":"df_train.describe()","0f957e72":"df_test.describe()","523fa8b0":"df_train.isnull().sum()","72e68c7d":"df_test.isnull().sum()","e1de17d1":"print(df_train.shape)\nprint(df_test.shape)","61d51358":"df_train.duplicated().sum()","9b93555b":"df_train['species'].nunique()","7252704a":"df_train.columns.values","7de9365b":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedShuffleSplit # we will know about that package while using it","19db7ade":"# Label Encoding\ndef encode(df_train,df_test):\n    le = LabelEncoder().fit(df_train.species)\n    labels = le.transform(df_train.species) # Species are in stings \n    \n    classes = list(le.classes_) #creating list of column names for submission\n    \n    test_ids = df_test.id # creating variable for IDs\n    \n    df_train = df_train.drop(['species','id'],axis = 1) #droping columns \n    df_test = df_test.drop(['id'],axis = 1)\n\n    return df_train, labels, classes, test_ids, df_test\n\ndf_train, labels, classes, test_ids, df_test = encode(df_train,df_test)","457cae2f":"df_train.head()","fdaaca8e":"df_train.shape","819fe5f3":"# labels is our traget columns which is we created by transforming of species colums(LabelEncoding)\nlabels","b85d9b53":"X = df_train.values\ny = labels","99019df7":"sss = StratifiedShuffleSplit(n_splits=10, test_size=0.25, random_state=5)\n>>> sss.get_n_splits(X, y)","b391b0e8":"sss","1397b225":"for train_index, test_index in sss.split(X, y):\n#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]","fbcaa28f":"# importing libraries\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC,LinearSVC,NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import log_loss\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel='rbf',C = 0.025, probability= True),\n    NuSVC(probability = True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis()\n]","d8f3fc06":"# Logging for Visual Comparison\nlog_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"] # created list \nlog = pd.DataFrame(columns=log_cols)\n\nfor clf in classifiers:\n    clf.fit(X_train, y_train) # fit method for applying algorithm \n    name = clf.__class__.__name__\n    \n    print(\"=\"*30) ### printing = in 30 times\n    print(name) ### printing name of algorithm\n    \n    print('****Results****') \n    train_predictions = clf.predict(X_test)   ### predicting y_predict\n    acc = accuracy_score(y_test, train_predictions) # now compairing y predict(train_prediction) with y_test\n    print(\"Accuracy: {:.4%}\".format(acc)) ### printing accuracy  with 4 decimal with pecentage mark \n    \n    train_predictions = clf.predict_proba(X_test) ### Probability estimates y_predict_probability(train_predictions)\n    ll = log_loss(y_test, train_predictions) ### appllying log loss on y_test and y_predict_probability(train_predictions)\n    print(\"Log Loss: {}\".format(ll)) ### printing log loss  \n    \n    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols) ## creating new dataframe with \n                                                                        ## name of log_entry considering name ,\n                                                                          ## accuracy score(this score not in pecentage form hence it multiply by 100)\n                                                                            ##logloss stored each iteratin value in log_cols\n    log = log.append(log_entry) ### append those entry in log \n    \nprint(\"=\"*30)\n    ","c5487536":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_color_codes(\"muted\")\nsns.barplot(y='Classifier', x='Accuracy', data=log, color=\"b\")\n\nplt.xlabel('Accuracy %')\nplt.title('Classifier Accuracy')\nplt.show()\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Log Loss', y='Classifier', data=log, color=\"g\")\n\nplt.xlabel('Log Loss')\nplt.title('Classifier Log Loss')\nplt.show()","b48c8a05":"log.head(10)","7b87e371":"# now predicting test values \nchosen_clf = RandomForestClassifier()\nchosen_clf.fit(X_train,y_train)\ntest_predictions = chosen_clf.predict_proba(df_test)\n\n# Format DataFrame\nsubmission = pd.DataFrame(test_predictions, columns=classes)\nsubmission.insert(0, 'id', test_ids)\nsubmission.reset_index()\n\n# saving submission\nsubmission.to_csv('submission.csv', index = False)\nsubmission.tail()","071944ee":"Here we use swiss army knife function to orgnize the data\n<br>Also we use label encoding ","9918138a":"Creating interation for these 10 algorithm and printing result then we select higher algorithm which having high accuracy and less log loss","25710aaf":"## Stratified TrainTest Split\n### why stratified train test split?\nHere are relatively large no of classes(columns) available (192 classes\/columns for 990 samples\/rows).This will ensure we have all classes represented in both the train and test indices.","90736b0c":"Random Forest Classifier has the highest accuracy  also it has less log loss so we choosing Random forest Classifier as our model ","7ed8547e":"code Reffered from other notebook for learning perpose(thanks )","e6662f16":"## Data Preparation ","cff0e503":"## Classification\nwe will use 10 classification technique and printing their results. These will perform much better after tuning their Hyperparameters, this gives you a decent ballpark idea.\n1. KNeighborsClassifier\n2. SVC(Support Vector Classifier)\n3. NuSVC(Nu-Support Vector Classification)Similar to SVC but uses a parameter to control the number of support vectors. The implementation is based on libsvm.\n4. DecisionTreeClassifier\n5. RandomForestClassifier\n6. AdaBoostClassifier\n7. GradientBoostingClassifier\n8. GaussianNB\n9. LinearDiscriminantAnalysis\n10. QuadraticDiscriminantAnalysis"}}