{"cell_type":{"a6886826":"code","ea197eea":"code","bb38d137":"code","cddf9d61":"code","6f0a7417":"code","2bb6cf9b":"code","88cda7a1":"code","1f2377c9":"code","ed2ffb5f":"code","2d672e66":"code","85248d2c":"code","f0d2082e":"code","eb05ede5":"code","86451023":"code","8b9c14c0":"code","836a5e73":"code","5eda196d":"code","be546f5f":"code","0b76705f":"code","4c5fff68":"code","f6e8ec76":"code","ee7b0e9a":"code","c1838adf":"code","4679f0e0":"code","984dc282":"code","20f68ae0":"code","6323c0e8":"code","f253abcf":"code","01ddbbb8":"code","eeb0395c":"code","89a78eba":"code","f61e34e0":"code","b963f474":"code","50308858":"code","c58825f0":"code","f0810641":"code","36fcdd84":"code","9156be9c":"code","b75baa11":"code","f3164caa":"code","faeefa59":"code","35fe69a8":"code","567440a0":"code","d96c06ef":"code","26b043cb":"code","d62bb32a":"code","ef768d4c":"code","5d5c11a5":"code","bfab4317":"code","0e48c5f0":"code","d843a00b":"code","0a7d2f5e":"code","3d40eb11":"code","da3f8ac3":"code","df996ac9":"code","81d5538d":"code","0813c338":"code","6675fec1":"code","604fab3d":"code","321af0fb":"code","3d0ac196":"code","8e5f8b85":"code","b1cee50d":"code","fe3854fb":"code","fd1585b1":"code","b9deeb89":"code","15a5ab2f":"code","a3cb1865":"code","fe56301a":"code","304da22f":"code","f623ccae":"code","b467134b":"code","240ef5d9":"code","42db203b":"code","e06d68cd":"code","62acda71":"code","556452cb":"code","d41f699e":"code","6266d755":"code","13a4ca94":"code","9992999b":"code","a3226099":"code","3ec2f612":"code","79e115f3":"code","5d2edd10":"code","3a1ad1ca":"code","66651410":"code","483bdfc6":"code","24ebe95c":"code","4a6f8754":"code","30ae0170":"code","1f24d0c6":"code","dcdea7ac":"code","d439c919":"code","41b9b190":"code","7561a1a3":"code","1dc60696":"code","914a3d5c":"code","cf1f3e3b":"code","bfad5c2e":"code","127196be":"code","bceb305b":"code","b005bb57":"markdown","4fc19f2e":"markdown","1fbc4eb1":"markdown","e5dc5b0d":"markdown","3442cd3a":"markdown","dd16d287":"markdown","43d40896":"markdown","9caf5ec6":"markdown","9f109833":"markdown","59ceeec6":"markdown","6af1525f":"markdown","30f88041":"markdown","cb196572":"markdown","a06def69":"markdown","04082955":"markdown","25d80885":"markdown","2a980ddf":"markdown","234d741f":"markdown","08ae5138":"markdown","9d327181":"markdown","92534620":"markdown","cc8250d2":"markdown","783cd1c2":"markdown","6947310e":"markdown","cbba94e9":"markdown","0b6c147e":"markdown","389ea449":"markdown","1d86094e":"markdown","8ae9a870":"markdown","9151712e":"markdown","8721b667":"markdown","bb5c09d9":"markdown","d92a99e8":"markdown","9836285e":"markdown","0f5a9fbf":"markdown","1c9c8f0f":"markdown","63e6af38":"markdown","99174b8a":"markdown","207c432e":"markdown","e796845b":"markdown","b087871a":"markdown","c59e8cfb":"markdown","60fea1a0":"markdown","05df8ed4":"markdown","1dc753c5":"markdown","0e7f52c5":"markdown"},"source":{"a6886826":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ea197eea":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder,StandardScaler,PowerTransformer, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_predict, train_test_split,GridSearchCV,cross_val_score, cross_validate\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge,ElasticNet\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.metrics import classification_report, confusion_matrix, log_loss, recall_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVR\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.feature_selection import SelectKBest,SelectPercentile,f_classif,f_regression,mutual_info_regression\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nfrom sklearn.impute import SimpleImputer\nfrom yellowbrick.classifier import ConfusionMatrix\nfrom yellowbrick.cluster import KElbowVisualizer\n\nimport missingno as msno\n\n#importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams[\"figure.figsize\"] = (10,6)\npd.set_option('max_colwidth',200)\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 200)\npd.set_option('display.float_format', lambda x: '%.7f' % x)","bb38d137":"# For checking missing values and missing percent\n\ndef missing (df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values","cddf9d61":"# To view summary information about the column\n\ndef first_looking(col):\n    print(\"column name    : \", col)\n    print(\"--------------------------------\")\n    print(\"per_of_nulls   : \", \"%\", round(df[col].isnull().sum()\/df.shape[0]*100, 2))\n    print(\"num_of_nulls   : \", df[col].isnull().sum())\n    print(\"num_of_uniques : \", df[col].nunique())\n    print(df[col].value_counts(dropna = False))","6f0a7417":"df = pd.read_csv(\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")","2bb6cf9b":"#df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")","88cda7a1":"df.head()","1f2377c9":"df.columns = df.columns.str.lower().str.replace('&', '_').str.replace(' ', '_')","ed2ffb5f":"df.columns","2d672e66":"df.shape","85248d2c":"print (f' We have {df.shape[0]} instances with the {df.shape[1]-1} features and 1 target variable.')","f0d2082e":"df.info()","eb05ede5":"df.describe().T","86451023":"df.describe(include=object).T","8b9c14c0":"df.nunique()","836a5e73":"df.duplicated().value_counts()","5eda196d":"missing (df)","be546f5f":"df.drop('id', axis=1, inplace=True)","0b76705f":"df.columns","4c5fff68":"sns.heatmap(df.corr(), annot=True);","f6e8ec76":"numerical= df.drop(['stroke'], axis=1).select_dtypes('number').columns\n\ncategorical = df.select_dtypes('object').columns\n\nprint(f'Numerical Columns:  {df[numerical].columns}')\nprint('\\n')\nprint(f'Categorical Columns: {df[categorical].columns}')","ee7b0e9a":"first_looking(\"stroke\")","c1838adf":"df['stroke'].describe()","4679f0e0":"print( f\"Skewness: {df['stroke'].skew()}\")","984dc282":"df['stroke'].iplot(kind='hist')","20f68ae0":"df[numerical].head().T","6323c0e8":"df[numerical].describe().T","f253abcf":"df[numerical].iplot(kind='hist');","01ddbbb8":"df[numerical].iplot(kind='histogram',subplots=True,bins=50)","eeb0395c":"index = 0\nplt.figure(figsize=(20,20))\nfor feature in numerical:\n    if feature != \"stroke\":\n        index += 1\n        plt.subplot(4,3,index)\n        sns.boxplot(x='stroke',y=feature,data=df)","89a78eba":"skew_limit = 0.75 # This is our threshold-limit to evaluate skewness. Overall below abs(1) seems acceptable for the linear models. \nskew_vals = df[numerical].skew()\nskew_cols = skew_vals[abs(skew_vals)> skew_limit].sort_values(ascending=False)\nskew_cols ","f61e34e0":"df[skew_cols.index].iplot(kind='hist');","b963f474":"df[skew_cols.index].iplot(kind='histogram',subplots=True,bins=50)","50308858":"skew_limit = 0.75 # This is our threshold-limit to evaluate skewness. Overall below abs(1) seems acceptable for the linear models.\nskew_vals = df[numerical].skew()\nskew_cols = skew_vals[abs(skew_vals)> skew_limit].sort_values(ascending=False)\nskew_cols = skew_cols.drop(['heart_disease', 'hypertension'])\nskew_cols","c58825f0":"df_try = df.copy()\ndf_try = df[skew_cols.index].copy()\nfor col in skew_cols.index.values:\n    df_try[col] = df_try[col].apply(np.log1p)\nprint(df_try[skew_cols.index].skew())\nprint()\ndf_try[skew_cols.index].iplot(kind='histogram',subplots=True,bins=50);","f0810641":"df_trans = df[skew_cols.index].copy()\npt = PowerTransformer(method='yeo-johnson')\ntrans= pt.fit_transform(df_trans)\ndf_trans = pd.DataFrame(trans, columns =skew_cols.index )\nprint(df_trans.skew())\nprint()\ndf_trans.iplot(kind='histogram',subplots=True,bins=50);","36fcdd84":"df_trans.columns","9156be9c":"df.shape","b75baa11":"df.drop(['avg_glucose_level', 'bmi'], axis=1, inplace=True)","f3164caa":"df.shape","faeefa59":"df = pd.concat([df, df_trans], axis=1)","35fe69a8":"df.shape","567440a0":"df.columns","d96c06ef":"skew_limit = 0.75 # This is our threshold-limit to evaluate skewness. \n                  # Overall below abs(1) seems acceptable for the linear models.\nskew_vals = df[numerical].skew()\nskew_cols= skew_vals[abs(skew_vals)> skew_limit].sort_values(ascending=False)\nskew_cols","26b043cb":"numerical= df.select_dtypes('number').columns\n\nmatrix = np.triu(df[numerical].corr())\nfig, ax = plt.subplots(figsize=(14,10)) \nsns.heatmap (df[numerical].corr(), annot=True, fmt= '.2f', vmin=-1, vmax=1, center=0, cmap='coolwarm',mask=matrix, ax=ax);","d62bb32a":"df.head()","ef768d4c":"first_looking(\"age\")","5d5c11a5":"first_looking(\"hypertension\")","bfab4317":"first_looking(\"heart_disease\")","0e48c5f0":"first_looking(\"avg_glucose_level\")","d843a00b":"first_looking(\"bmi\")","0a7d2f5e":"df[categorical].head().T","3d40eb11":"df[categorical].describe().T","da3f8ac3":"first_looking(\"gender\")","df996ac9":"df.drop(df[df['gender'] == 'Other'].index, inplace = True)","81d5538d":"df['gender'].value_counts()","0813c338":"print(df.groupby('gender')['stroke'].mean().sort_values())\nprint()\ndf.groupby('gender')['stroke'].mean().iplot(kind='histogram',subplots=True,bins=50)","6675fec1":"first_looking(\"ever_married\")","604fab3d":"print(df.groupby('ever_married')['stroke'].mean().sort_values())\nprint()\ndf.groupby('ever_married')['stroke'].mean().iplot(kind='histogram',subplots=True,bins=50)","321af0fb":"first_looking(\"work_type\")","3d0ac196":"print(df.groupby('work_type')['stroke'].mean().sort_values())\nprint()\ndf.groupby('work_type')['stroke'].mean().iplot(kind='histogram',subplots=True,bins=50)","8e5f8b85":"first_looking(\"residence_type\")","b1cee50d":"print(df.groupby('residence_type')['stroke'].mean().sort_values())\nprint()\ndf.groupby('residence_type')['stroke'].mean().iplot(kind='histogram',subplots=True,bins=50)","fe3854fb":"first_looking(\"smoking_status\")","fd1585b1":"print(df.groupby('smoking_status')['stroke'].mean().sort_values())\nprint()\ndf.groupby('smoking_status')['stroke'].mean().iplot(kind='histogram',subplots=True,bins=50)","b9deeb89":"sns.pairplot(df, hue=\"stroke\");","15a5ab2f":"sns.scatterplot(x = 'avg_glucose_level', y = 'age', hue = 'stroke', data = df);","a3cb1865":"sns.scatterplot(x = 'bmi', y = 'age', hue = 'stroke', data = df);","fe56301a":"sns.scatterplot(x = 'avg_glucose_level', y = 'bmi', hue = 'stroke', data = df);","304da22f":"df = pd.get_dummies(df, columns=['gender', 'ever_married',\n       'work_type', 'residence_type', 'smoking_status',], drop_first=True)","f623ccae":"df.head()","b467134b":"sns.heatmap(df.corr(), annot=True)","240ef5d9":"X = df.drop('stroke',axis=1)\ny = df['stroke']","42db203b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)","e06d68cd":"X_train.bmi.value_counts(dropna=False)","62acda71":"imputer = SimpleImputer(missing_values=np.nan, strategy=\"median\")\n\nX_train['bmi'] = imputer.fit_transform(X_train['bmi'].values.reshape(-1,1))[:,0]","556452cb":"X_test['bmi'] = imputer.fit_transform(X_test['bmi'].values.reshape(-1,1))[:,0]","d41f699e":"print(X_train.isnull().sum())","6266d755":"print(X_test.isnull().sum())","13a4ca94":"scaler = MinMaxScaler()\nscaler","9992999b":"X_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","a3226099":"knn = KNeighborsClassifier()","3ec2f612":"knn.fit(X_train_scaled, y_train)","79e115f3":"knn_pred = knn.predict(X_test_scaled)","5d2edd10":"print(confusion_matrix(y_test, knn_pred))\nprint(classification_report(y_test, knn_pred))","3a1ad1ca":"plt.figure(figsize=(10, 6))\ncm = ConfusionMatrix(knn)\ncm.fit(X_train_scaled, y_train)\ncm.score(X_test_scaled, y_test)\ncm.show()","66651410":"error_rate = []\nfor i in range(1,30):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train_scaled,y_train)\n    pred_i = knn.predict(X_test_scaled)\n    error_rate.append(1 - recall_score(y_test, pred_i))","483bdfc6":"plt.figure(figsize=(10,6))\nplt.plot(range(1,30),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","24ebe95c":"# FIRST A QUICK COMPARISON TO OUR DEFAULT VALUE K=5\nknn = KNeighborsClassifier()\n\nknn.fit(X_train_scaled,y_train)\npred = knn.predict(X_test_scaled)\n\nprint('WITH K=5')\nprint('\\n')\nprint(confusion_matrix(y_test, pred))\nprint('\\n')\nprint(classification_report(y_test, pred))","4a6f8754":"# WITH K=1\nknn = KNeighborsClassifier(n_neighbors=1)\n\nknn.fit(X_train_scaled,y_train)\npred = knn.predict(X_test_scaled)\n\nprint('WITH K=1')\nprint('\\n')\nprint(confusion_matrix(y_test, pred))\nprint('\\n')\nprint(classification_report(y_test, pred))","30ae0170":"# WITH K=2\nknn = KNeighborsClassifier(n_neighbors=2)\n\nknn.fit(X_train_scaled,y_train)\npred = knn.predict(X_test_scaled)\n\nprint('WITH K=2')\nprint('\\n')\nprint(confusion_matrix(y_test, pred))\nprint('\\n')\nprint(classification_report(y_test, pred))","1f24d0c6":"# WITH K=3\nknn = KNeighborsClassifier(n_neighbors=3)\n\nknn.fit(X_train_scaled,y_train)\npred = knn.predict(X_test_scaled)\n\nprint('WITH K=3')\nprint('\\n')\nprint(confusion_matrix(y_test, pred))\nprint('\\n')\nprint(classification_report(y_test, pred))","dcdea7ac":"# WITH K=4\nknn = KNeighborsClassifier(n_neighbors=4)\n\nknn.fit(X_train_scaled,y_train)\npred = knn.predict(X_test_scaled)\n\nprint('WITH K=4')\nprint('\\n')\nprint(confusion_matrix(y_test, pred))\nprint('\\n')\nprint(classification_report(y_test, pred))","d439c919":"model = KNeighborsClassifier(n_neighbors=1)\n\nscores = cross_validate(model, X_train_scaled, y_train, scoring = ['accuracy', 'precision','recall',\n                                                                   'f1'], cv = 10)\ndf_scores = pd.DataFrame(scores, index = range(1, 11))\ndf_scores","41b9b190":"df_scores.mean()[2:]","7561a1a3":"knn_grid = KNeighborsClassifier()","1dc60696":"k_values= range(1,30)","914a3d5c":"param_grid = {\"n_neighbors\":k_values, \"p\": [1,2], \"weights\": ['uniform', \"distance\"]}","cf1f3e3b":"knn_grid_model = GridSearchCV(knn_grid, param_grid, cv=10, scoring= 'recall')","bfad5c2e":"knn_grid_model.fit(X_train_scaled, y_train)","127196be":"knn_grid_model.best_params_","bceb305b":"# NOW WITH K=1\nknn = KNeighborsClassifier(n_neighbors=1, p=2, weights='uniform')\n\nknn.fit(X_train_scaled,y_train)\npred = knn.predict(X_test_scaled)\n\nprint('WITH K=1')\nprint('\\n')\nprint(confusion_matrix(y_test, pred))\nprint('\\n')\nprint(classification_report(y_test, pred))","b005bb57":"- \"stroke\" column is coded as **1** for positive cases (has a stroke) and **0** for negative cases (does not have a stroke).\n- Approximately % 95 of our target variable is 'No stroke' (4861)\n- %5 of the instances of our target variable is 'Stroke' (249)","4fc19f2e":"### Train \/ Test and Split","1fbc4eb1":"# Data Gathering\n- Read the csv","e5dc5b0d":"# Problem definition","3442cd3a":"# User Defined Functions","dd16d287":"### Scores by Various K Values","43d40896":"### Scaling","9caf5ec6":"- An overview of the values each column contains. Above we can see some basic descriptive statistics for all numeric columns.","9f109833":"- Our results with the KNN model for this dataset are not very good.\n\n- Although we did cross validation and found the most suitable parameters with GridSearch, we could not achieve the desired improvement in our results.\n\n- After this point let's go on with other models.","59ceeec6":"- We have 3 unique values. It seems that the \"other\" value is entered incorrectly. So let's drop it.","6af1525f":"- Only the \"bmi\" column has 201 null values. We will fill in these values while defining X and y before modelling.","30f88041":"- We have both numerical and categorical variables (8 numerical, 4 categorical).\n- The most important column is our target variable \"stroke\".\n- Target variable is coded as **1** for positive cases and **0** for negative cases.\n- \"Hypertension\" and \"heart disease\" columns are coded like \"stroke\" column.","cb196572":"### Filling the Missing Values","a06def69":"**With Power Transformer**","04082955":"# Stroke Prediction with KNN","25d80885":"In \"id\" column, all the values are unique. Let's drop the \"id\" column.","2a980ddf":"### work_type & stroke","234d741f":"- Some features have skewness. Let's try to handle with their skewness by \"np.log\" and \"PowerTransformer\".","08ae5138":"### ever_married & stroke","9d327181":"# Conclusion","92534620":"### Gridsearch Method for Choosing Optimal K Values","cc8250d2":"### Handling with Skewness","783cd1c2":"- Before deeping into the analysis it would be benefical to examine the correlation among variables using heatmap.","6947310e":"# Data Preparation\n- Get basic information from the dataset","cbba94e9":"Let us import necessary libraries.","0b6c147e":"### The Examination of Target Variable","389ea449":"### residence_type & stroke","1d86094e":"### gender & stroke","8ae9a870":"- There is no dublicated value in the dataset","9151712e":"- Number of unique values in each column","8721b667":"- We can also analyze other data types. But we can not calculate a mean or a standard deviation for the object columns. However, it will still display some descriptive statistics","bb5c09d9":"### smoking_status & stroke","d92a99e8":"Context\n\nAccording to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.\n\nThis dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.\n\nAttribute Information\n\n1) id: unique identifier\n\n2) gender: \"Male\", \"Female\" or \"Other\"\n\n3) age: age of the patient\n\n4) hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n\n5) heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n\n6) ever_married: \"No\" or \"Yes\"\n\n7) work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n\n8) Residence_type: \"Rural\" or \"Urban\"\n\n9) avg_glucose_level: average glucose level in blood\n\n10) bmi: body mass index\n\n11) smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n\n12) stroke: 1 if the patient had a stroke or 0 if not\n\n*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient\n\nAcknowledgements (Confidential Source) - Use only for educational purposes If you use this dataset in your research, please credit the author.","9836285e":"# Data Preprocessing","0f5a9fbf":"Let's start by standardizing all column names.","1c9c8f0f":"### Data Cleaning","63e6af38":"### Cross Validate For Optimal K Value","99174b8a":"### Categorical Features","207c432e":"# Dummy Operations","e796845b":"- Let's split our features into two part, numerical and categorical, for easing our further examination.","b087871a":"**With np.log**","c59e8cfb":"### Numerical Features","60fea1a0":"# Implement KNN and Evaluate","05df8ed4":"- We imputed train and test sets seperately so that we did not have data leakage.","1dc753c5":"### Elbow Method for Choosing Reasonable K Values","0e7f52c5":"# Libraries"}}