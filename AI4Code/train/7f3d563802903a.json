{"cell_type":{"4157ddd2":"code","9edb29ce":"code","3477dab0":"code","e409a9f2":"code","ead1b7e7":"code","5564e9fc":"code","7e879fe9":"code","093b6868":"code","e364204b":"code","4f0e1b49":"code","e65a016b":"code","1399919b":"code","ac6d32a2":"code","82f47e4b":"code","1b40e1a0":"code","f4058a15":"code","aa69e23d":"code","05ef2f53":"code","82d7f21c":"code","75eb53f4":"code","c4806897":"code","008d916b":"code","11eff9ec":"code","76061818":"code","9bb9bf08":"code","ffd931c4":"code","49fdb655":"code","141540a8":"code","d439fbcb":"code","03e3e274":"code","99a73164":"code","e6ec3d82":"code","d5a39f03":"code","bb94fa38":"code","1a9b12f9":"code","0a40bb66":"code","cd5fe393":"code","367ab936":"code","4c5b3692":"code","d6a1c076":"code","f823bc0b":"code","e838f454":"markdown","83944b73":"markdown","d9e2f68e":"markdown","d278cce0":"markdown","e4b13c31":"markdown","9456c5c8":"markdown","6a8c6d99":"markdown","f9bb1250":"markdown"},"source":{"4157ddd2":"import warnings\nwarnings.filterwarnings('ignore')","9edb29ce":"ls \/kaggle\/input\/credit-default-prediction-ai-big-data\/","3477dab0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","e409a9f2":"df = pd.read_csv('\/kaggle\/input\/credit-default-prediction-ai-big-data\/train.csv', \n                 index_col='Id')","ead1b7e7":"df.columns = ['_'.join(col.split(' ')).lower() for col in df.columns]","5564e9fc":"df.T","7e879fe9":"years_dict = {'-1': -1, '10+ years': 10, '8 years': 8, '6 years': 6, \n              '7 years': 7, '5 years': 5, '1 year': 1, '< 1 year': 0, \n              '4 years': 4, '3 years': 3, '2 years': 2, '9 years': 9}\n\ndf['years_in_current_job'] = (df['years_in_current_job']\n                              .fillna('-1')\n                              .map(years_dict))","093b6868":"df['credit_default'].value_counts()","e364204b":"round(1e2 * df['credit_default'].value_counts()\/len(df), 2)","4f0e1b49":"df.shape","e65a016b":"df.info()","1399919b":"df.nunique()","ac6d32a2":"(1e2 * df.isnull().sum()\/len(df)).plot(kind='barh')\nplt.xlim(0, 10**2)\nplt.grid();","82f47e4b":"df.describe()","1b40e1a0":"plt.figure(figsize=(6, 5))\nsns.heatmap(df.corr());","f4058a15":"from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.preprocessing import (StandardScaler, MinMaxScaler, \n                                   OneHotEncoder, LabelEncoder)\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.decomposition import PCA\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score","aa69e23d":"num_feat = df.drop('credit_default', axis=1).select_dtypes(include=np.number).columns\ncat_feat = df.drop('credit_default', axis=1).select_dtypes(include=['object']).columns\nX = df.drop('credit_default', axis=1)\ny = df['credit_default']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)","05ef2f53":"numeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)),\n    # ('imputer', KNNImputer()),\n    ('scaler', StandardScaler()),\n    # ('poly', PolynomialFeatures(2, include_bias=False)), \n    # ('scaler1', StandardScaler()),\n    # ('pca', PCA(n_components=100))\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('one_hot', OneHotEncoder(handle_unknown='ignore')),\n])","82d7f21c":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, num_feat),\n        ('cat', categorical_transformer, cat_feat)\n    ])","75eb53f4":"pipe = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier',  LogisticRegression())\n])\n\nmodel = pipe.fit(X_train, y_train)\ntarget_names = y_test.unique().astype(str)\ny_pred = model.predict(X_test)","c4806897":"print(classification_report(y_test, y_pred, target_names=target_names))","008d916b":"print(round(pd.DataFrame(confusion_matrix(y_test, y_pred)\/len(y_test)*1e2)))","11eff9ec":"f1_score(y_true=y_test, y_pred=y_pred)","76061818":"from yellowbrick.classifier import DiscriminationThreshold\n\nfig, ax = plt.subplots(figsize=(6, 6))\nmodel_viz = DiscriminationThreshold(pipe)\nmodel_viz.fit(X_train, y_train)\nmodel_viz.poof();","9bb9bf08":"from time import time\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import (LogisticRegression, RidgeClassifier, \n                                  SGDClassifier, PassiveAggressiveClassifier)\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n                              ExtraTreesClassifier, VotingClassifier)\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score","ffd931c4":"results = pd.DataFrame(columns=['Name', 'f1', 'f1_test', 'StdDev(%)', 'Time(s)'])\n\nfor model in [\n    DummyClassifier,\n    LinearDiscriminantAnalysis,\n    LogisticRegression, \n    RidgeClassifier,\n    SGDClassifier,\n    PassiveAggressiveClassifier,\n    GaussianNB,\n    KNeighborsClassifier,\n#     SVC,\n    DecisionTreeClassifier,\n    RandomForestClassifier, \n#     GradientBoostingClassifier,\n    ExtraTreesClassifier,\n#     MLPClassifier,\n    XGBClassifier,\n    LGBMClassifier\n]:\n    pipe = make_pipeline(preprocessor, model())\n    start_time = time()\n    kfold = StratifiedKFold(n_splits=4, random_state=1)\n    scores = cross_val_score(pipe, X_train, y_train, scoring='f1', cv=kfold)\n    pipe.fit(X_train, y_train)\n    scores_test = f1_score(y_true=y_test, y_pred=pipe.predict(X_test))\n    time_mod = time() - start_time\n    results = results.append({\n        'Name' : model.__name__, \n        'f1' : round(scores.mean(), 4), \n        'f1_test' : round(scores_test, 4), \n        'StdDev(%)' : round(1e2*scores.std(), 2), \n        'Time(s)': round(time_mod, 2)\n    }, ignore_index=True)\n    del pipe\n    print('Analyzed {}.'.format(model.__name__))\nprint('Done!')\n\nresults = results.sort_values('f1', ascending=False)","49fdb655":"results","141540a8":"X_train_prepared = preprocessor.fit_transform(X_train)\nX_test_prepared = preprocessor.transform(X_test)","d439fbcb":"from mlxtend.classifier import StackingClassifier\n\nclfs = [x for x in [GaussianNB(), \n                    XGBClassifier(), \n                    LGBMClassifier(),                     \n                    DecisionTreeClassifier(), \n                    #RandomForestClassifier(), \n                    #ExtraTreesClassifier(), \n                    #SVC(probability=True), \n                    #LogisticRegression(), \n                    #KNeighborsClassifier()\n                   ]]\n\nstack = StackingClassifier(classifiers=clfs, meta_classifier=KNeighborsClassifier())\n\nkfold = StratifiedKFold(n_splits=10, random_state=0)\n\ncross_val_score(stack, X_train_prepared, y_train, scoring='f1', cv=kfold).mean()","03e3e274":"from yellowbrick.classifier import DiscriminationThreshold\n\nfig, ax = plt.subplots(figsize=(6, 6))\nmodel_viz = DiscriminationThreshold(stack)\nmodel_viz.fit(X_train_prepared, y_train)\nmodel_viz.poof();","99a73164":"def threshold_optimizer(X_train, X_test, y_train, y_test):\n    scores, thresholds = [], []\n    for threshold in np.linspace(0, 1, 21):\n        y_pred = np.zeros(len(y_test))\n\n        all_models = [\n            GaussianNB(), XGBClassifier(), \n            DecisionTreeClassifier(), LGBMClassifier(), \n            RandomForestClassifier(), ExtraTreesClassifier(), \n            SVC(probability=True), LogisticRegression(), \n            KNeighborsClassifier()\n        ]\n\n        for model in all_models:\n            model.fit(X_train, y_train)\n            y_pred += model.predict_proba(X_test)[:, 1]\n#             y_pred += model.predict(X_test)\n\n\n        y_pred \/= len(all_models)\n        y_pred = (y_pred > threshold) * 1\n\n        thresholds.append(threshold)\n        scores.append(f1_score(y_test, y_pred))\n    return thresholds, scores","e6ec3d82":"thresholds, scores = threshold_optimizer(X_train_prepared, X_test_prepared, y_train, y_test)","d5a39f03":"plt.plot(thresholds, scores)\nplt.scatter(thresholds, scores)\nplt.legend(['f1 Scores'])\nplt.xlabel('Threshold')\nplt.ylabel('f1 Score');\n#plt.grid();","bb94fa38":"df_test = pd.read_csv('\/kaggle\/input\/credit-default-prediction-ai-big-data\/test.csv', \n                      index_col='Id')\n\ndf_test.columns = ['_'.join(col.split(' ')).lower() for col in df_test.columns]\ndf_test['years_in_current_job'] = (df_test['years_in_current_job']\n                                   .fillna('-1')\n                                   .map(years_dict))\n\nX_prepared = preprocessor.fit_transform(X)\nX_sub_prepared = preprocessor.transform(df_test)\n\nall_models = [\n    GaussianNB(), XGBClassifier(), \n    DecisionTreeClassifier(), LGBMClassifier(), \n    RandomForestClassifier(), ExtraTreesClassifier(), \n    SVC(probability=True), LogisticRegression(), \n    KNeighborsClassifier()\n]\n\ny_pred = np.zeros(len(df_test))\n\nfor model in all_models:\n    model.fit(X_prepared, y)\n    y_pred += model.predict_proba(X_sub_prepared)[:, 1]\n\ny_pred \/= len(all_models)\ny_pred = (y_pred > .35) * 1\n\ndf_sub = pd.read_csv('\/kaggle\/input\/credit-default-prediction-ai-big-data\/sampleSubmission.csv', \n                     index_col='Id')\n\ndf_sub['Credit Default'] = y_pred","1a9b12f9":"# X_train_prepared = preprocessor.fit_transform(X_train)\n# X_test_prepared = preprocessor.transform(X_test)","0a40bb66":"# dtc = DecisionTreeClassifier()\n# param_grid = [\n#     {\n#         'max_depth': [None, 2, 3, 5], \n#         'min_samples_split': [128,512, 2048], \n#         'min_samples_leaf': [2, 4, 6, 8], \n#         'max_features': ['auto', 'sqrt', 'log2', None]}\n# ]\n# grid_search = GridSearchCV(dtc, param_grid=param_grid, cv=5, scoring='f1')\n# grid_search.fit(X_train_prepared, y_train)\n# print(grid_search.best_estimator_)\n# grid_search.score(X_test_prepared, y_test) # f1_score(y_test, grid_search.predict(X_test_prepared))","cd5fe393":"# dtc = XGBClassifier()\n# param_grid = [\n#     {\n#         'eta': [0, .25, .5, .75, 1],\n#         'gamma': [0, 10, 100, 1000], \n#         'max_depth': [2, 4, 6, 8, 10], \n#     }\n# ]\n# grid_search = GridSearchCV(dtc, param_grid=param_grid, cv=5, scoring='f1')\n# grid_search.fit(X_train_prepared, y_train)\n# print(grid_search.best_estimator_)\n# grid_search.score(X_test_prepared, y_test)","367ab936":"# gnb = GaussianNB() \n# dtc = DecisionTreeClassifier(min_samples_leaf=6, min_samples_split=128)\n# xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n#               colsample_bynode=1, colsample_bytree=1, eta=1, gamma=0, gpu_id=-1,\n#               importance_type='gain', interaction_constraints='',\n#               learning_rate=1, max_delta_step=0, max_depth=2,\n#               min_child_weight=1, monotone_constraints='()',\n#               n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n#               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n#               tree_method='exact', validate_parameters=1, verbosity=None)\n# lgb = LGBMClassifier()\n# # svm = SVC(probability=True)\n# # log = LogisticRegression()\n\n# voting_clf = VotingClassifier(\n#     estimators=[\n#         ('gnb', gnb), \n#         ('dtc', dtc), \n#         ('xgb', xgb), \n# #         ('lgb', lgb)\n#     ], \n#     voting='soft'\n# )\n\n# voting_clf.fit(X_train_prepared, y_train)\n\n# for clf in (gnb, dtc, xgb, lgb, voting_clf):\n#     clf.fit(X_train_prepared, y_train)\n#     y_pred = clf.predict(X_test_prepared)\n#     print(clf.__class__.__name__, f1_score(y_test, y_pred))","4c5b3692":"# for threshold in np.linspace(0, 1, 21):\n#     y_pred = np.zeros(len(y_test))\n#     y_pred = voting_clf.predict_proba(X_test_prepared)[:, 1]\n#     y_pred = (y_pred > threshold) * 1\n#     print('{:.4f}: {:.2f}'.format(f1_score(y_test, y_pred), threshold))","d6a1c076":"# df_test = pd.read_csv('\/kaggle\/input\/credit-default-prediction-ai-big-data\/test.csv', \n#                       index_col='Id')\n\n# df_test.columns = ['_'.join(col.split(' ')).lower() for col in df_test.columns]\n# df_test['years_in_current_job'] = (df_test['years_in_current_job']\n#                                    .fillna('-99')\n#                                    .map(years_dict))\n\n# voting_clf = VotingClassifier(\n#     estimators=[\n#         ('gnb', gnb), \n#         ('dtc', dtc), \n#         ('xgb', xgb), \n# #        ('lgb', lgb)\n#     ], \n#     voting='soft'\n# )\n\n# X_prepared = preprocessor.transform(X)\n# voting_clf.fit(X_prepared, y)\n\n# X_sub_test = df_test\n# X_sub_test_prepared = preprocessor.transform(X_sub_test)\n\n# df_sub = pd.read_csv('\/kaggle\/input\/credit-default-prediction-ai-big-data\/sampleSubmission.csv', \n#                      index_col='Id')\n\n# df_sub['Credit Default'] = voting_clf.predict(X_sub_test_prepared)","f823bc0b":"df_sub.to_csv('submission.csv')","e838f454":"# Grid Search","83944b73":"### Note: Grid Search models are commented out, before submission.","d9e2f68e":"# Method 1","d278cce0":"# Multiple Models","e4b13c31":"# Probability of Default modeling\n\nWe are going to create a model that estimates a probability for a borrower to default her loan.","9456c5c8":"## Pipeline","6a8c6d99":"## Working Example","f9bb1250":"# Final Submission"}}