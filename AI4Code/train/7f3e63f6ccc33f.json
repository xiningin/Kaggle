{"cell_type":{"12ff5044":"code","c83fa5dd":"code","164eaeb2":"code","cd1e8083":"code","d67d7b41":"code","e798a315":"code","b92ac928":"code","a3ed13bf":"code","87fbc824":"code","fb9fc9f0":"code","f296819a":"code","e4eb262f":"code","8904d892":"code","e76ccb55":"code","ab27e199":"code","2c816eb4":"code","328f56b4":"code","42a511d3":"code","fdcb98dd":"code","9a348c26":"code","9d12a7fd":"code","5ea29630":"code","e4f2aaac":"code","c00566ca":"markdown"},"source":{"12ff5044":"!pip install -U catalyst transformers","c83fa5dd":"# Python \nimport os\nimport warnings\nimport logging\nfrom typing import Mapping, List\nfrom pprint import pprint\n\n# Numpy and Pandas \nimport numpy as np\nimport pandas as pd\n\n# PyTorch \nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Transformers \nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\n\n# Catalyst\nfrom catalyst.dl import SupervisedRunner\nfrom catalyst.dl.callbacks import AccuracyCallback, F1ScoreCallback, OptimizerCallback\nfrom catalyst.dl.callbacks import CheckpointCallback, InferCallback\nfrom catalyst.utils import set_global_seed, prepare_cudnn","164eaeb2":"MODEL_NAME = 'distilbert-base-uncased' # pretrained model from Transformers\nLOG_DIR = \".\/logdir\"                   # for training logs and tensorboard visualizations\nNUM_EPOCHS = 5                         # smth around 2-6 epochs is typically fine when finetuning transformers\nBATCH_SIZE = 128                       # depends on your available GPU memory (in combination with max seq length)\nMAX_SEQ_LENGTH = 64                    # depends on your available GPU memory (in combination with batch size)\nLEARN_RATE = 5e-5                      # learning rate is typically ~1e-5 for transformers\nACCUM_STEPS = 1                        # one optimization step for that many backward passes\nSEED = 42                              # random seed for reproducibility","cd1e8083":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","d67d7b41":"train.head()","e798a315":"train['text'] = train['text'].apply(lambda x: x.lower())\ntrain['len'] = train['text'].apply(lambda x: len(x.split()))","b92ac928":"train['len'].hist()","a3ed13bf":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train, train.target.values, random_state = 42, test_size = 0.2, stratify=train.target.values)","87fbc824":"class TextClassificationDataset(Dataset):\n    \"\"\"\n    Wrapper around Torch Dataset to perform text classification\n    \"\"\"\n    def __init__(self,\n                 texts: List[str],\n                 labels: List[str] = None,\n                 label_dict: Mapping[str, int] = None,\n                 max_seq_length: int = 512,\n                 model_name: str = 'distilbert-base-uncased'):\n        \"\"\"\n        Args:\n            texts (List[str]): a list with texts to classify or to train the\n                classifier on\n            labels List[str]: a list with classification labels (optional)\n            label_dict (dict): a dictionary mapping class names to class ids,\n                to be passed to the validation data (optional)\n            max_seq_length (int): maximal sequence length in tokens,\n                texts will be stripped to this length\n            model_name (str): transformer model name, needed to perform\n                appropriate tokenization\n\n        \"\"\"\n\n        self.texts = texts#([word for word in texts if word not in stopwords.words('english')])#texts\n        self.labels = labels\n        self.label_dict = label_dict\n        self.max_seq_length = max_seq_length\n\n        if self.label_dict is None and labels is not None:\n            # {'class1': 0, 'class2': 1, 'class3': 2, ...}\n            # using this instead of `sklearn.preprocessing.LabelEncoder`\n            # no easily handle unknown target values\n            self.label_dict = dict(zip(sorted(set(labels)),\n                                       range(len(set(labels)))))\n\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        # suppresses tokenizer warnings\n        logging.getLogger(\n            \"transformers.tokenization_utils\").setLevel(logging.FATAL)\n\n        # special tokens for transformers\n        # in the simplest case a [CLS] token is added in the beginning\n        # and [SEP] token is added in the end of a piece of text\n        # [CLS] <indexes text tokens> [SEP] .. <[PAD]>\n        self.sep_vid = self.tokenizer.vocab[\"[SEP]\"]\n        self.cls_vid = self.tokenizer.vocab[\"[CLS]\"]\n        self.pad_vid = self.tokenizer.vocab[\"[PAD]\"]\n\n    def __len__(self):\n        \"\"\"\n        Returns:\n            int: length of the dataset\n        \"\"\"\n        return len(self.texts)\n\n    def __getitem__(self, index) -> Mapping[str, torch.Tensor]:\n        \"\"\"Gets element of the dataset\n\n        Args:\n            index (int): index of the element in the dataset\n        Returns:\n            Single element by index\n        \"\"\"\n\n        # encoding the text\n        x = self.texts[index]\n        x_encoded = self.tokenizer.encode(\n            x,\n            add_special_tokens=True,\n            max_length=self.max_seq_length,\n            return_tensors=\"pt\",\n        ).squeeze(0)\n\n        # padding short texts\n        true_seq_length = x_encoded.size(0)\n        pad_size = self.max_seq_length - true_seq_length\n        pad_ids = torch.Tensor([self.pad_vid] * pad_size).long()\n        x_tensor = torch.cat((x_encoded, pad_ids))\n\n        # dealing with attention masks - there's a 1 for each input token and\n        # if the sequence is shorter that `max_seq_length` then the rest is\n        # padded with zeroes. Attention mask will be passed to the model in\n        # order to compute attention scores only with input data\n        # ignoring padding\n        mask = torch.ones_like(x_encoded, dtype=torch.int8)\n        mask_pad = torch.zeros_like(pad_ids, dtype=torch.int8)\n        mask = torch.cat((mask, mask_pad))\n\n        output_dict = {\n            \"features\": x_tensor,\n            'attention_mask': mask\n        }\n\n        # encoding target\n        if self.labels is not None:\n            y = self.labels[index]\n            y_encoded = torch.Tensor(\n                [self.label_dict.get(y, -1)]\n            ).long().squeeze(0)\n            output_dict[\"targets\"] = y_encoded\n\n        return output_dict","fb9fc9f0":"train_dataset = TextClassificationDataset(\n    texts=X_train['text'].values.tolist(),\n    labels=y_train.tolist(),\n    label_dict=None,\n    max_seq_length=MAX_SEQ_LENGTH,\n    model_name=MODEL_NAME\n)\n\nvalid_dataset = TextClassificationDataset(\n    texts=X_test['text'].values.tolist(),\n    labels=y_test.tolist(),\n    label_dict=train_dataset.label_dict,\n    max_seq_length=MAX_SEQ_LENGTH,\n    model_name=MODEL_NAME\n)\n\ntest_dataset = TextClassificationDataset(\n    texts=test['text'].values.tolist(),\n    labels=None,\n    label_dict=None,\n    max_seq_length=MAX_SEQ_LENGTH,\n    model_name=MODEL_NAME\n)","f296819a":"NUM_CLASSES = len(train_dataset.label_dict)","e4eb262f":"train_val_loaders = {\n    \"train\": DataLoader(dataset=train_dataset,\n                        batch_size=BATCH_SIZE, \n                        shuffle=True),\n    \"valid\": DataLoader(dataset=valid_dataset,\n                        batch_size=BATCH_SIZE, \n                        shuffle=False)    \n}","8904d892":"class DistilBertForSequenceClassification(nn.Module):\n    \"\"\"\n    Simplified version of the same class by HuggingFace.\n    See transformers\/modeling_distilbert.py in the transformers repository.\n    \"\"\"\n\n    def __init__(self, pretrained_model_name: str, num_classes: int = None):\n        \"\"\"\n        Args:\n            pretrained_model_name (str): HuggingFace model name.\n                See transformers\/modeling_auto.py\n            num_classes (int): the number of class labels\n                in the classification task\n        \"\"\"\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(\n            pretrained_model_name, num_labels=num_classes)\n\n        self.distilbert = AutoModel.from_pretrained(pretrained_model_name,\n                                                    config=config)\n        self.pre_classifier = nn.Linear(config.dim, config.dim)\n        self.classifier = nn.Linear(config.dim, num_classes)\n        self.dropout = nn.Dropout(config.seq_classif_dropout)\n\n    def forward(self, features, attention_mask=None, head_mask=None):\n        \"\"\"Compute class probabilities for the input sequence.\n\n        Args:\n            features (torch.Tensor): ids of each token,\n                size ([bs, seq_length]\n            attention_mask (torch.Tensor): binary tensor, used to select\n                tokens which are used to compute attention scores\n                in the self-attention heads, size [bs, seq_length]\n            head_mask (torch.Tensor): 1.0 in head_mask indicates that\n                we keep the head, size: [num_heads]\n                or [num_hidden_layers x num_heads]\n        Returns:\n            PyTorch Tensor with predicted class probabilities\n        \"\"\"\n        assert attention_mask is not None, \"attention mask is none\"\n        distilbert_output = self.distilbert(input_ids=features,\n                                            attention_mask=attention_mask,\n                                            head_mask=head_mask)\n        # we only need the hidden state here and don't need\n        # transformer output, so index 0\n        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n        # we take embeddings from the [CLS] token, so again index 0\n        pooled_output = hidden_state[:, 0]  # (bs, dim)\n        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n        logits = self.classifier(pooled_output)  # (bs, dim)\n\n        return logits","e76ccb55":"model = DistilBertForSequenceClassification(pretrained_model_name=MODEL_NAME,\n                                            num_classes=NUM_CLASSES)","ab27e199":"criterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARN_RATE)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)","2c816eb4":"os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"    # can be changed in case of multiple GPUs onboard\nset_global_seed(SEED)                       # reproducibility\nprepare_cudnn(deterministic=True)           # reproducibility","328f56b4":"%%time\n# here we specify that we pass masks to the runner. So model's forward method will be called with\n# these arguments passed to it. \nrunner = SupervisedRunner(\n    input_key=(\n        \"features\",\n        \"attention_mask\"\n    )\n)\n\n\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=train_val_loaders,\n    callbacks=[\n        AccuracyCallback(num_classes=NUM_CLASSES),\n#         F1ScoreCallback(activation='Softmax'), # throws a tensor shape mismatch error\n        OptimizerCallback(accumulation_steps=ACCUM_STEPS)\n    ],\n    fp16=None,\n    logdir=LOG_DIR,\n    num_epochs=NUM_EPOCHS,\n    verbose=True\n)","42a511d3":"torch.cuda.empty_cache()","fdcb98dd":"test_loaders = {\n    \"test\": DataLoader(dataset=test_dataset,\n                        batch_size=BATCH_SIZE, \n                        shuffle=False) \n}","9a348c26":"runner.infer(\n    model=model,\n    loaders=test_loaders,\n    callbacks=[\n        CheckpointCallback(\n            resume=f\"{LOG_DIR}\/checkpoints\/best.pth\"\n        ),\n        InferCallback(),\n    ],   \n    verbose=True\n)","9d12a7fd":"predicted_probs = runner.callbacks[0].predictions['logits']","5ea29630":"submission['target'] = predicted_probs.argmax(axis=1)\nsubmission.head()","e4f2aaac":"submission.to_csv('submission.csv', index=False)","c00566ca":"**From https:\/\/www.kaggle.com\/kashnitsky\/distillbert-catalyst-amazon-product-reviews**\nTODO:\n1. Remove stopwords\n1. Parameters tuning\n1. Text processing"}}