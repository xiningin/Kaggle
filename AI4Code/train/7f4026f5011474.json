{"cell_type":{"0033456b":"code","a69d0561":"code","924bdb57":"code","40fa9e10":"code","2d75c880":"code","10b2f3ed":"code","4743cfc7":"code","378ac7ce":"code","89d0ac38":"code","9f026fab":"code","b3ca19e2":"code","75c88ebb":"code","c868c3b3":"code","9ad71ddd":"code","591ee46b":"code","6e12f816":"code","ee10fded":"code","d8c0ab08":"code","f1e91704":"code","b20fea36":"code","d093c6ee":"code","50dfb462":"code","1a781385":"code","07ba72ee":"code","a7bbdf32":"code","8660a4e5":"code","6e9f12ac":"code","d88b05a2":"code","974064d6":"markdown","d79abcb0":"markdown","f8322cf6":"markdown","58ef827a":"markdown","9468e42e":"markdown","f02dc356":"markdown","52e56564":"markdown","4c1b9ede":"markdown","477c2988":"markdown","94b77853":"markdown","657a6952":"markdown","7a42560f":"markdown","33ec5a7f":"markdown","9c64c018":"markdown","22c8ab85":"markdown","eb206bf4":"markdown","249f694a":"markdown","136462c5":"markdown","e657e622":"markdown","9844919a":"markdown","2428d426":"markdown","5eede875":"markdown","a9c479c0":"markdown","d583edc1":"markdown","9ef29c89":"markdown","23b20288":"markdown"},"source":{"0033456b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport scipy.stats as stats\n\nimport seaborn as sns\n#sns.set_style('whitegrid')\nsns.set_style('white')\n\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.float_format = '{:.2f}'.format","a69d0561":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")","924bdb57":"print('<train>')\nprint(\"Number of rows:\",train.shape[0],\"Number of colums:\",train.shape[1])\ndisplay(train.head(3))\nprint('<test>')\nprint(\"Number of rows:\",test.shape[0],\"Number of colums:\",test.shape[1])\ndisplay(test.head(3))","40fa9e10":"train = train.drop(['Id'],axis=1)\ntest = test.drop(['Id'],axis=1)\ntrain['flag'] = 'train'\ntest['flag'] = 'test'","2d75c880":"alldata = pd.concat([train, test],axis=0).reset_index(drop=True)\nalldata.shape","10b2f3ed":"df = alldata\nplt.figure(figsize=(30,16)) \nplt.title(\"Missing Value\") \nsns.heatmap(df.isnull(), cbar=False)","4743cfc7":"df=alldata\ntotal = df.isnull().sum()\npercent = round(df.isnull().sum()\/df.isnull().count()*100,2)\n\nmissing_data = pd.concat([total,percent],axis =1, keys=['Total','Ratio_of_NA(%)'])\ntypes=pd.DataFrame(df[missing_data.index].dtypes, columns=['Types'])\nmissing_data=pd.concat([missing_data,types],axis=1)\nmissing_data=missing_data.sort_values('Total',ascending=False)\n\ncategorical_cols=missing_data[missing_data['Types']==\"object\"].index\nnumerical_cols=missing_data[missing_data['Types']!=\"object\"].index\n\nprint(\"Top 20 columns for Missing value\")\nprint(missing_data.head(20))\nprint()\nprint(\"---Types---\")\nprint(set(missing_data['Types']))\nprint()\nprint(\"---Categorical col---\")\nprint(categorical_cols)\nprint()\nprint(\"---Numerical col---\")\nprint(numerical_cols)\nprint()\nprint(\"---Categorical col with NaN---\")\nprint(missing_data[(missing_data['Types']==\"object\")&(missing_data['Ratio_of_NA(%)']>0)].index)\nprint()\nprint(\"---Numerical col with NaN---\")\nprint(missing_data[(missing_data['Types']!=\"object\")&(missing_data['Ratio_of_NA(%)']>0)].index)","378ac7ce":"from scipy.stats import norm, skew \n\nplt.figure(figsize = (12, 6))\nsns.distplot(train['SalePrice'] , fit=norm)\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint(\"NormalDistribution: Mean\", int(mu))\nprint(\"NormalDistribution: Sigma\", int(sigma))\n\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')","89d0ac38":"train['log_SalePrice']= np.log1p(train['SalePrice'])","9f026fab":"stats.probplot(train['SalePrice'], dist=\"norm\", plot=plt)\nplt.show()","b3ca19e2":"stats.probplot(train['log_SalePrice'], dist=\"norm\", plot=plt)\nplt.show()","75c88ebb":"alldata['log_SalePrice']=np.log1p(alldata['SalePrice'])\nalldata=alldata.drop(['SalePrice'],axis=1)\nalldata.head()","c868c3b3":"k = 10 #number\u00a0of variables for heatmap\nkeyword = 'log_SalePrice'\ndf = train.drop(['SalePrice'],axis=1)\nplt.subplots(figsize = (10,10))\ncorrmat = df.corr()\ncols = corrmat.nlargest(k, keyword)[keyword].index\ncm = np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=False, annot=True, square=True,cmap = 'coolwarm', fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.title(\"Heatmap\u00a0of all the Features\", fontsize = 20);\n#plt.ylim(0,\u00a0corr.shape[0])\nplt.show()","9ad71ddd":"sns.pairplot(alldata[['log_SalePrice','OverallQual','GrLivArea','GarageCars','GarageArea','TotalBsmtSF','flag']],hue=\"flag\")","591ee46b":"train = alldata.iloc[:train.shape[0],:]\ntest = alldata.iloc[train.shape[0]:,:]","6e12f816":"X_train = train[['OverallQual','GrLivArea','GarageCars','TotalBsmtSF']]\ny_train = train['log_SalePrice']\nX_test  = test[['OverallQual','GrLivArea','GarageCars','TotalBsmtSF']]","ee10fded":"X_test['GarageCars']=X_test['GarageCars'].fillna(np.mean(X_test['GarageCars']))\nX_test['TotalBsmtSF']=X_test['TotalBsmtSF'].fillna(np.mean(X_test['TotalBsmtSF']))","d8c0ab08":"%%time\nfrom sklearn.preprocessing import StandardScaler\n\nX_train = StandardScaler().fit_transform(X_train)\nX_test = StandardScaler().fit_transform(X_test)","f1e91704":"pd.DataFrame(X_train,columns=['OverallQual','GrLivArea','GarageCars','TotalBsmtSF']).describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","b20fea36":"from sklearn.model_selection import train_test_split\nX_tra, X_val, y_tra, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)","d093c6ee":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nLR = LinearRegression()\nLR.fit(X_tra, y_tra)\ny_pred=LR.predict(X_val)\nprint(np.sqrt(mean_squared_error(y_val,y_pred)))","50dfb462":"plt.subplots(figsize = (10,10))\nplt.scatter(y_val,y_pred)\nplt.title(\"Predict vs Actual of First Linear Regression Model\")\nplt.xlim([10.5,14.0])\nplt.ylim([10.5,14.0])\nplt.show()","1a781385":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\n\n# kf = KFold(n_splits=10,shuffle=True)\n\nscores_LR=[]\n# Modeling\nfolds = KFold(n_splits=20, shuffle=True, random_state=42)\n\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(X_train)):\n    trn_x, trn_y = X_train[trn_idx], y_train[trn_idx]\n    val_x, val_y = X_train[val_idx], y_train[val_idx]\n    model = LinearRegression()\n    model.fit(trn_x, trn_y)\n    y_pred = model.predict(val_x)\n    score=np.sqrt(mean_squared_error(val_y, y_pred))\n    scores_LR.append(score)\nscores_LR = np.array(scores_LR)\nprint(\"mean:\",scores_LR.mean(),\"\u00b1\",scores_LR.std())","07ba72ee":"from sklearn import linear_model\n\nscores_lasso=[]\n# Modeling\nfolds = KFold(n_splits=20, shuffle=True, random_state=42)\n\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(X_train)):\n    trn_x, trn_y = X_train[trn_idx], y_train[trn_idx]\n    val_x, val_y = X_train[val_idx], y_train[val_idx]\n    model = linear_model.Lasso(alpha=0.1)\n    model.fit(trn_x, trn_y)\n    y_pred = model.predict(val_x)\n    score=np.sqrt(mean_squared_error(val_y, y_pred))\n    scores_lasso.append(score)\nscores_lasso = np.array(scores_lasso)\nprint(\"mean:\",scores_lasso.mean(),\"\u00b1\",scores_lasso.std())","a7bbdf32":"from sklearn import linear_model\n\n# kf = KFold(n_splits=20,shuffle=True)\n\nscores_ridge=[]\n# Modeling\nfolds = KFold(n_splits=20, shuffle=True, random_state=123)\n\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(X_train)):\n    trn_x, trn_y = X_train[trn_idx], y_train[trn_idx]\n    val_x, val_y = X_train[val_idx], y_train[val_idx]\n    model = linear_model.Ridge(alpha=0.1)\n    model.fit(trn_x, trn_y)\n    y_pred = model.predict(val_x)\n    score=np.sqrt(mean_squared_error(val_y, y_pred))\n    scores_ridge.append(score)\nscores_ridge = np.array(scores_ridge)\nprint(\"mean:\",scores_ridge.mean(),\"\u00b1\",scores_ridge.std())","8660a4e5":"df=pd.DataFrame({\"Linear\":scores_LR,\"Lasso\":scores_lasso,\"Ridge\":scores_ridge})\ndf=df.stack().reset_index()\ndf.columns = ['id','model','RMSE']\n\nplt.subplots(figsize = (10,10))\nplt.title(\"RMSE by models\")\nsns.violinplot(x=\"model\", y=\"RMSE\", data=df)\n#sns.boxplot(x=\"model\", y=\"RMSE\", data=df)","6e9f12ac":"LR = LinearRegression()\nLR.fit(X_train,y_train)\ny_submission=LR.predict(X_test)","d88b05a2":"sample_submission['SalePrice']=np.exp(y_submission)\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission","974064d6":"# Correlation Map\n<font size=4><span style=\"font-family:calibri;\">\nTo create LinearRegression model, we should consider which feature affects to target a lot<br>\nFeatures which have high correlation with target must be important.<br>\nTo see that, I would like to use Correlation map here. <br>\nThese are features which are higly correlated with log_SalePrice.<br>\nYou can notice GarageCars and GarageArea, TotalBsmtSF and 1stFlrSF are highly correlated.<br>\nTo avoid Multicollinearity, I will not use GarageArea and 1stFlrSF to create model.\nIf you would like to know about Multicollinearity,<\/font> [this site](https:\/\/statisticsbyjim.com\/regression\/multicollinearity-in-regression-analysis\/) <font size=4><span style=\"font-family:calibri;\">is good.<\/font>\n","d79abcb0":"# Pairplot\n<font size=4><span style=\"font-family:calibri;\">\nLet's see pairplot and compare these relationship.<br>\nSince it is too many columns to see at the same time, I picked up some columns.<br>\nBlue dot represents train data and orange dot represents test data.\n<\/font>","f8322cf6":"<font size=4><span style=\"font-family:calibri;\">\nSince I would like to use LinearRegression, I will use log-transformation and strage the data to the colum 'log_SalePrice'\n<\/font>","58ef827a":"<font size=4><span style=\"font-family:calibri;\">\nLet's see the insight of dataset. There are 4 files. You can check what the data is and columns of these data [here](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data)<br>\n* train.csv - the training set<br>\n* test.csv - the test set<br>\n* data_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here<br>\n* sample_submission.csv - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms\n<\/span><\/font>","9468e42e":"# Observe Missing values\n<font size=4><span style=\"font-family:calibri;\">\nLet's check Missing values.Now you can visualize alldata by using Seaborn library.<br>\nWhite part shows NAN and black prat shows non-NaN data.\n<\/font>","f02dc356":"# Introduction\n<font size=4><span style=\"font-family:calibri;\">\nThis competition is for Kaggle beginner. Titanic is the most famoust Clasification competition for beginner and<br>\nthis House Prices - Advanced Regression Techniques is the mose famous Regression competition.<br>\nIn this competition, you are required creating regression model and predict each house price.<br> \n<\/span><\/font>","52e56564":"<font size=4><span style=\"font-family:calibri;\"><font size=4><span style=\"font-family:calibri;\">\nLet's see predict vs actual in graph below.<br>\n<br>\nIf the graph follows y=x perfectly, it means that the model is perfectly predictable. The graph below is scattered around y=x, and we can imagine that we have a reasonably good prediction.If the graph follows y=x perfectly, it means that the model is perfectly predictable. The graph below is scattered around y=x, and we can imagine that we have a reasonably good prediction.\n<\/font>","4c1b9ede":"# QQplot\n<font size=4><span style=\"font-family:calibri;\">\nQQplot is a way to check if it follows a normal distribution.<br>\nIf the data follow a normal distribution, then each data will appear on a red line<br>\nIt shows SalePrice doesn't follow normal distribution. However, log_SalePrice follows normal distribution.<br>\nSo I will use log_SalePrice to create LinearRegression model instead of SalePrice.\n<\/font>","477c2988":"# Closing Words\n<font size=4><span style=\"font-family:calibri;\"><font size=4><span style=\"font-family:calibri;\"><font size=4><span style=\"font-family:calibri;\"><font size=4><span style=\"font-family:calibri;\">\nThank you very much for reading this article.<br>\nI will update this article reguraly.<br>\nIf you find this article practical, please upvote!!\n<\/font>","94b77853":"Treat NaN in Test data","657a6952":"<font size=4><span style=\"font-family:calibri;\">\nTo handle missing values(NaN) and many categorical columns, I will merge both train and test<br>\nNow alldata has 2919 rows and 81 columns(80 original columns + added flag)\n<\/span><\/font>","7a42560f":"![](https:\/\/images.unsplash.com\/photo-1522735555435-a8fe18da2089?ixid=MnwxMjA3fDB8MHxzZWFyY2h8N3x8c2FuZnJhbmNpc2NvJTIwaG91c2VzfGVufDB8fDB8fA%3D%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=500&q=60)","33ec5a7f":"# Dataset\n<font size=4><span style=\"font-family:calibri;\">\nA lot of people just start to create model without reading the explanation of dataset.<br>\nYou have to know the detail of dataset for first.<br>\nThis is dataset of houseprice in Ames, Iowa. You can see the detailed information <\/span><\/font>[here](chrome-extension:\/\/efaidnbmnnnibpcajpcglclefindmkaj\/viewer.html?pdfurl=http%3A%2F%2Fjse.amstat.org%2Fv19n3%2Fdecock.pdf&clen=377943&chunk=true) .<br>\n<font size=4><span style=\"font-family:calibri;\">\nWhen you want to know the overview of dataset, you can find the information on <\/span><\/font>[Overview page](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques).","9c64c018":"<font size=4><span style=\"font-family:calibri;\"><font size=4><span style=\"font-family:calibri;\">\nLet's see these RMSE scores.These are RMSE of log_SalePrice.<br>\nIn this case, the Linear's score is the best.<br>\n<\/font>","22c8ab85":"# Exploratory Data Analysis\n<font size=4><span style=\"font-family:calibri;\">\nLet's see the insight of data more. Firstly, let's check SalePrice.\n<\/font>","eb206bf4":"<font size=4><span style=\"font-family:calibri;\">\nOK. The column which has NaN most is PoolQC. To create Regression model, we have to hundle NaN properly.<BR>\nBut, what is the meaning of NaN in PoolQC? Let's check data fields [here](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data) <br>\nIt says thata PoolQC: Pool quality. In this case, <span style=\"color:red;\">NaN represent the house doesn't have Pool<\/span>.<br>\nLike this column, we should consider how we should fill in NaN every time.\n<\/font>","249f694a":"<font size=4><span style=\"font-family:calibri;\">\nThese are the nine columns that are highly correlated with log_SalePrice.<br>\n<br>\nOverallQual: Rates the overall material and finish of the house<br>\nGrLivArea: Above grade (ground) living area square feet<br>\nGarageCars: Size of garage in car capacity<br>\nGarageArea: Size of garage in square feet<br>\nTotalBsmtSF: Total square feet of basement area<br>\n1stFlrSF: First Floor square feet<br>\nFullBath: Full bathrooms above grade<br>\nYearBuilt: Original construction date<br>\nYearRemodAdd: Remodel date<br>\n<\/font>","136462c5":"# Create Model\n<font size=4><span style=\"font-family:calibri;\">\nLet's create the first model.<br>\nI would like to use 'OverallQual','GrLivArea','GarageCars','TotalBsmtSF' as features<br>\n<\/font>","e657e622":"# Lasso Model","9844919a":"# Submission\n<font size=4><span style=\"font-family:calibri;\"><font size=4><span style=\"font-family:calibri;\"><font size=4><span style=\"font-family:calibri;\"><font size=4><span style=\"font-family:calibri;\">\nI will make submission file with using LinearRegression model.\n<\/font>","2428d426":"<font size=4><span style=\"font-family:calibri;\">Since Id is nothing related to SalePrice, I will drop Id.<br>\nAlso I would like to add flag<\/span><\/font>","5eede875":"# LinearRegression","a9c479c0":"# Ridge Model","d583edc1":"<font size=4><span style=\"font-family:calibri;\">\nYou can also check the Ratio of NaN in each column.<br>\nAs people can imagin, column which has NaN a lot might not have any information and you can drop the colum.\n<\/font>","9ef29c89":"<font size=4><span style=\"font-family:calibri;\"><font size=4><span style=\"font-family:calibri;\">\nUse StandardScaler to normaraize numerical data.\n<\/font>","23b20288":"<font size=4><span style=\"font-family:calibri;\">Let's see train and test data.There are very similar format.\nHowever, test data doesn't have colum <b><span style=\"color:red;\">\"SalePrice\"<\/span><\/b>.<br>\nThis is because you have to predict SalePrice of test data.<br>\nTrain data has 1460 rows and 81 columns and test data has 1459 rows and 80 columns\n<\/span><\/font>"}}