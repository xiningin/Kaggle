{"cell_type":{"16329176":"code","f698f4b7":"code","885a5f95":"code","b8b1f566":"code","13799487":"code","c1aaefb1":"code","eba60241":"code","a0c06045":"code","df879265":"code","a359469d":"code","bef6ba1f":"code","6bf8db1e":"code","770d0798":"code","42955d96":"code","9c3fbcc7":"code","80c22a2f":"code","c35dd7b4":"code","3c45d035":"code","25cbb2e2":"code","e0150f5d":"code","639afc80":"code","48e54537":"code","f4c9ae9f":"code","e62e2b6f":"code","064d74f8":"code","4a8b8b3f":"code","097e02ec":"code","519b4c7c":"code","b5861f33":"code","1f646fdf":"markdown","39a96263":"markdown","121a306b":"markdown","8932cb8e":"markdown","db08c0e6":"markdown","93d10669":"markdown","f0cb6c65":"markdown","4fdfb26f":"markdown","0b50443f":"markdown","a14cbafc":"markdown","5c312c0e":"markdown","a5f68f7a":"markdown","398de6e2":"markdown","5f22b593":"markdown","789d3eca":"markdown","c88e932f":"markdown","c1955a50":"markdown","b9fc78a8":"markdown","025d4e2b":"markdown","804cd03b":"markdown","dd0ec7b2":"markdown","8cf45b31":"markdown","990d9245":"markdown","eee4cf2a":"markdown","c1b252f8":"markdown","2a593d34":"markdown","a819b7ca":"markdown","2d5c53de":"markdown","dfaca8d4":"markdown"},"source":{"16329176":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f698f4b7":"telco_data = pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')","885a5f95":"telco_data.info()","b8b1f566":"telco_data.describe()","13799487":"telco_data.head()","c1aaefb1":"# Drop the customerID column\ntelco_data.drop(['customerID'],axis=1,inplace=True)\n\n# Change type of TotalCharges\ntelco_data['TotalCharges'] = telco_data['TotalCharges'].apply(pd.to_numeric,errors='coerce')","eba60241":"# Check if there is missing data or NaN values\ntelco_data.isnull().sum()","a0c06045":"# replace the NaN values in TotalCharges with its average value\ntelco_data['TotalCharges'] = telco_data['TotalCharges'].fillna(telco_data['TotalCharges'].mean())","df879265":"sns.countplot(telco_data['Churn'])\nplt.title('Customer Churn Count')","a359469d":"labels = telco_data['Churn'].value_counts().index\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=telco_data['Churn'].value_counts())])\nfig.show()","bef6ba1f":"sns.pairplot(telco_data,hue='Churn')","6bf8db1e":"cat_features = telco_data.select_dtypes(include='object')","770d0798":"plt.figure(figsize=(15,20))\n\ni=1\nfor col_name in cat_features.columns[0:15]: #specify this because Churn is included in the cat_features at this moment\n    plt.subplot(5,3,i)\n    sns.countplot(telco_data[col_name],hue=telco_data['Churn'])\n    i +=1\n    plt.tight_layout()","42955d96":"sns.set_style('whitegrid')\ntelco_data['tenure'].hist(bins=35,alpha=0.7)\n\nplt.title(\"Tenure Distribution\")\nplt.xlabel(\"Tenure\")\nplt.ylabel(\"Count\")","9c3fbcc7":"sns.distplot(telco_data['MonthlyCharges'],hist=True)\n\nplt.title(\"Customer Monthly Charges\")\nplt.xlabel(\"Dollars\")\nplt.ylabel(\"Count\")","80c22a2f":"sns.distplot(telco_data['TotalCharges'],hist=True,bins=35)\n\nplt.title(\"Customer Total Charges\")\nplt.xlabel(\"Dollars\")\nplt.ylabel(\"Count\")","c35dd7b4":"num_features = telco_data[['tenure','MonthlyCharges','TotalCharges']]\nskew_features = num_features.skew().sort_values(ascending=False)\nsknewness= pd.DataFrame({'Skew':skew_features})\nsknewness","3c45d035":"# Simplify the \"No internet service\" response to \"No\"\nInternet_cat = ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\n\nfor cat in Internet_cat:\n    telco_data[cat] = telco_data[cat].replace({\"No internet service\":\"No\"})\n    \n# MulitpleLines\ntelco_data['MultipleLines'] = telco_data['MultipleLines'].replace({\"No phone service\":\"No\"})","25cbb2e2":"# Convert the target column Churn to a binary feature either using LabelEncoder or LabelBinarizer\n# Yes (churn) is 1\n# No Churn is 0\n\nlabel_encd = LabelEncoder()\ntelco_data['Churn'] = label_encd.fit_transform(telco_data['Churn'])","e0150f5d":"# Convert the categorical features to dummy variables\n\ncat_col = cat_features.drop('Churn',axis=1).columns.tolist()\n\ntelco_data_encd = pd.get_dummies(telco_data,prefix_sep=\"__\",columns=cat_col,drop_first=True)","639afc80":"# Quick look at the current data set to gather some preliminary insights\n\ndata = [go.Heatmap(\n        z= telco_data_encd.corr().values,\n        x=telco_data_encd.columns.values,\n        y=telco_data_encd.columns.values,\n        colorscale='RdBu_r',\n        opacity = 1.0 )]\n\nlayout = go.Layout(\n    title='Pearson Correlation of Input Features',\n    xaxis = dict(ticks='', nticks=36),\n    yaxis = dict(ticks='' ),\n    width = 900, height = 700)\n\nfig = go.Figure(data=data, layout=layout)\nfig.show()","48e54537":"X = telco_data_encd.drop('Churn',axis=1)\ny = telco_data_encd['Churn']","f4c9ae9f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","e62e2b6f":"k_range = range(1,41)\nparam_grid_knn = dict(n_neighbors=k_range)\n\nfor k in k_range:\n    grid_knn = GridSearchCV(KNeighborsClassifier(n_neighbors=k), param_grid_knn, cv=10,scoring='accuracy')\n\ngrid_knn.fit(X_train,y_train)\ngrid_knn.best_params_","064d74f8":"param_grid={'C':[0.1, 1, 10, 100], 'gamma':[1, 0.1, 0.01, 0.001]}\n\ngrid_svm = GridSearchCV(SVC(), param_grid, cv=10,scoring='accuracy')\n\ngrid_svm.fit(X_train,y_train)\ngrid_svm.best_params_","4a8b8b3f":"# Instantiate the models\nlog = LogisticRegression().fit(X_train, y_train)\nknn = KNeighborsClassifier(n_neighbors=29).fit(X_train, y_train)\ntree = DecisionTreeClassifier().fit(X_train, y_train)\nrfc = RandomForestClassifier().fit(X_train, y_train)\nsvc = SVC(C=1,gamma=0.001).fit(X_train, y_train)\n\nmodels = [log, knn, tree, rfc, svc]\nmodels_names = ['Logistic Regression', 'KNN', 'Decision Tree', 'Random Forest', 'Support Vector Machine']","097e02ec":"scoring = ['accuracy','precision','recall']\ntrain_accuracy = []\ntrain_precision = []\ntrain_recall = []\ntrain_std = []\n\ntrain_scoring = {}\nfor i,model in enumerate (models):\n    scores = cross_validate(model, X_train, y_train, cv=10, scoring=scoring)\n    \n    # ignore the first two columns from scoring which are fit_time and score_time\n    # pay attention to the breakdown in the Cross validation section  \n    train_accuracy.append(scores['test_accuracy'].mean())\n    train_precision.append(scores['test_precision'].mean())\n    train_recall.append(scores['test_recall'].mean())\n    train_std.append(scores['test_accuracy'].std())\n    \n    train_scoring[i] = scores['test_accuracy']\n    \n\ntrain_scores = pd.DataFrame(list(zip(train_accuracy,train_precision,train_recall,train_std)),\n                            index=models_names,columns=['Accuracy','Precision','Recall','Standard Deviation'])\nprint('Models Training Scores')\ntrain_scores","519b4c7c":"fig, ax = plt.subplots(figsize=(10,6))\nax.boxplot(train_scoring.values())\nax.set_xticklabels(['Log','KNN','DTree','RFC','SVM'])\n\nplt.title('Models Accuracy Comparison')\nplt.ylabel('Accuracy rate')","b5861f33":"accuracy_scores = []\nprecision_scores = []\nrecall_scores = []\nerror_rate = []\n\nfor i,model in enumerate (models):\n    y_pred = model.predict(X_test)\n    conf_matrix = confusion_matrix(y_test,y_pred)\n    \n    print('\\n')\n    print(models_names[i])\n    print(classification_report(y_test,y_pred))\n    '\\n'\n    print(conf_matrix)\n    \n    tn = conf_matrix[0,0]\n    fp = conf_matrix[0,1]\n    tp = conf_matrix[1,1]\n    fn = conf_matrix[1,0]\n    \n    total = tn + fp + tp + fn\n\n    accuracy  = (tp + tn) \/ total # Accuracy Rate\n    precision = tp \/ (tp + fp) # Positive Predictive Value\n    recall    = tp \/ (tp + fn) # True Positive Rate\n    error = (fp + fn) \/ total # Missclassification Rate\n \n    accuracy_scores.append(accuracy)\n    precision_scores.append(precision)\n    recall_scores.append(recall)\n    error_rate.append(error)\n    \nscores_df = pd.DataFrame(list(zip(accuracy_scores,precision_scores,recall_scores,error_rate)),index=models_names,columns=['Accuracy','Precision','Recall','Error Rate'])\nprint('\\n')\nprint('Models Evaluation from Test Set')\nscores_df","1f646fdf":"## Tuning Parameters for KNN and SVM\nFor these two models, I will perform a grid search to find the optimal parameters (k for KNN; C and gamma for SVM)","39a96263":"Take a look at the distribution of the 3 numerical data: tenure, monthly charges and total charges","121a306b":"### For KNN","8932cb8e":"## Handling Categorical Data ","db08c0e6":"### For SVM","93d10669":"# Goal: Choose a Classification Model that Predicts the Customer Churn rate","f0cb6c65":"* The customerID is not important\n* TotalCharges is not numerical","4fdfb26f":"## Train the Models","0b50443f":"### Insights\nBy far, we see that Logistic Regression model achieves the high scores in all the metrics","a14cbafc":"=> Total Charges skewwess is moderately postive skewed: 0.5 < 0.96 < 1\n\n=> It is fine to train the model with these features as they are","5c312c0e":"### Insights:\nLogistic Regression model still achieves high scores on all the chosen metrics when it applies on the test set.","a5f68f7a":"## Evaluate the important metrics for this analysis\nFirstly, we want to have a model with high accuracy predicting the customer churn rate.\n\nCurrently, 73.5% of customers is keeping the service. As I want to retain existing customers, I want to focus on correctly predicting customers who are likely to churn. Precision is the second important metric.\n\nAssume that it is inexpensive to lose a customer, I want to give out promotions to those that are predicted to churn. I will try to minimize the wrongly predicited no-churn customers (FN), a high Recall value is also a good metric. \n","398de6e2":"## Insights:\nFeatures that are highly correlated to Churn rate:\n* Payment Method - Electronic Check, Internet Service - Fiber optic, Monthly Charges, Paperless Billing, Seniror Citizen\n\nFeatures that contribute to high montly charges\/total charges:\n* Streaming TV and movies, Fiber optic, Online Backup, Device Protection","5f22b593":"## Test the Models","789d3eca":"There are only 4 numerical features to plot. We will look at count plots on all the categorical features to see their relation to churn rate","c88e932f":"A look at the customer churn rate","c1955a50":"# Import Packages","b9fc78a8":"# EDA","025d4e2b":"### Scope: \nIn this notebook, I'm selecting a suitable classification model to predict customer churn.\n\n### Out of scope:\nThis work *not yet* includes the feature importances that identifies the key features affecting the churn.","804cd03b":"=> The Total charges distribuiton is skewed to the right.","dd0ec7b2":"# Import the Data Set","8cf45b31":"## EDA Insights\n* Churn rate is low for No-interenet-service feature\n* Churn rate is significantly high for Month-to-month contract and Electronic-check payment method","990d9245":"## Compare the Models\nComparing the models accuracy scores from cross-validation","eee4cf2a":"# Conclusion\nLogistic Regression is chosen as the predictive model for Customer Churn due to its good performance and simplicity","c1b252f8":"The following models are used:\n\n* Logistic Regression\n* KNN\n* Decision Tree\n* Random Forest\n* Support Vector Machine","2a593d34":"## Train - Test Split","a819b7ca":"# Data Preprocessing","2d5c53de":"# ML Classification Models","dfaca8d4":"## Skewness "}}