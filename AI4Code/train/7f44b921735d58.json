{"cell_type":{"fdf00ef3":"code","74779a7c":"code","1d3e0e4e":"code","7df5a828":"code","cf2ea40f":"code","3b7eeec1":"code","4166dcf8":"code","aa19a5e0":"markdown","3d7e5fa7":"markdown","0ffe50ea":"markdown","bcd1c640":"markdown","3cf5bf65":"markdown"},"source":{"fdf00ef3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","74779a7c":"!pip install skorch","1d3e0e4e":"import numpy as np\nfrom sklearn.datasets import make_classification\nfrom torch import nn\n\nfrom skorch import NeuralNetClassifier\n","7df5a828":"X, y = make_classification(1000, 20, n_informative=10, random_state=0)\nX = X.astype(np.float32)\ny = y.astype(np.int64)","cf2ea40f":"MAX_EPOCHS = 50\n\nclass MyModule(nn.Module):\n    def __init__(self, num_units=10, nonlin=nn.ReLU()):\n        super(MyModule, self).__init__()\n\n        self.dense0 = nn.Linear(20, num_units)\n        self.nonlin = nonlin\n        self.dropout = nn.Dropout(0.5)\n        self.dense1 = nn.Linear(num_units, num_units)\n        self.output = nn.Linear(num_units, 2)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, X, **kwargs):\n        X = self.nonlin(self.dense0(X))\n        X = self.dropout(X)\n        X = self.nonlin(self.dense1(X))\n        X = self.softmax(self.output(X))\n        return X\n\n\nnet = NeuralNetClassifier(\n    MyModule,\n    max_epochs=MAX_EPOCHS,\n    lr=0.1,\n    # Shuffle training data on each epoch\n    iterator_train__shuffle=True,\n)\n\nnet.fit(X, y)\ny_proba = net.predict_proba(X)","3b7eeec1":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\npipe = Pipeline([\n    ('scale', StandardScaler()),\n    ('net', net),\n])\n\npipe.fit(X, y)\ny_proba = pipe.predict_proba(X)","4166dcf8":"from sklearn.model_selection import GridSearchCV\n\n\n# deactivate skorch-internal train-valid split and verbose logging\nnet.set_params(train_split=False, verbose=0)\nparams = {\n    'lr': [0.01, 0.02],\n    'max_epochs': [10, 20],\n    'module__num_units': [10, 20],\n}\ngs = GridSearchCV(net, params, refit=False, cv=3, scoring='accuracy', verbose=2)\n\ngs.fit(X, y)\nprint(\"best score: {:.3f}, best params: {}\".format(gs.best_score_, gs.best_params_))","aa19a5e0":"### Example 2: with Pipeline","3d7e5fa7":"Disclaimer:\n\nI have just used the same codes (with a little or no changes) which I have seen in the Skorch documentation. I will have to play around with the code and Kaggle seems easier for me to play.\n\nIf you want to check the original code, check here:\nhttps:\/\/github.com\/skorch-dev\/skorch","0ffe50ea":"**Example 1**","bcd1c640":"### Example 3: with GridSearchCV","3cf5bf65":"To Do:\n\n* Play with different datasets\n* Come up with different approaches"}}