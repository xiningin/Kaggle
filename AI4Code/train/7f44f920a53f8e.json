{"cell_type":{"314496ea":"code","2b2a752e":"code","96074e2c":"code","b9cd248a":"code","cff7d9c0":"code","b80aa8bc":"code","09fda7e2":"code","2d6f809b":"code","a2cf8b0f":"code","cfc686bb":"code","cc82b0b6":"code","cb33910d":"code","73292a4f":"code","f8dc9972":"code","6c49303f":"code","1830a9fb":"code","bf156cb1":"code","4092107e":"code","1585c0e3":"markdown","40fe090f":"markdown","3e1d14e0":"markdown","ab447de4":"markdown","c3306a96":"markdown","40dcea1c":"markdown","31070f77":"markdown"},"source":{"314496ea":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os.path\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display, Markdown\nimport matplotlib.cm as cm\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\ndef printmd(string):\n    # Print with Markdowns    \n    display(Markdown(string))","2b2a752e":"image_dir = Path('..\/input\/v2-plant-seedlings-dataset')\n\n# Get filepaths and labels\nfilepaths = list(image_dir.glob(r'**\/*.png'))\nlabels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))","96074e2c":"filepaths = pd.Series(filepaths, name='Filepath').astype(str)\nlabels = pd.Series(labels, name='Label')\n\n# Concatenate filepaths and labels\nimage_df = pd.concat([filepaths, labels], axis=1)\n\n# Shuffle the DataFrame and reset index\nimage_df = image_df.sample(frac=1).reset_index(drop = True)\n\n# Show the result\nimage_df.head(3)","b9cd248a":"# Display some pictures of the dataset with their labels\nfig, axes = plt.subplots(nrows=3, ncols=5, figsize=(15, 10),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(image_df.Filepath[i]))\n    ax.set_title(image_df.Label[i])\nplt.tight_layout()\nplt.show()","cff7d9c0":"def create_gen():\n    # Load the Images with a generator and Data Augmentation\n    train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n        validation_split=0.1\n    )\n\n    test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n    )\n\n    train_images = train_generator.flow_from_dataframe(\n        dataframe=train_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=True,\n        seed=0,\n        subset='training',\n        rotation_range=30, # Uncomment to use data augmentation\n        zoom_range=0.15,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.15,\n        horizontal_flip=True,\n        fill_mode=\"nearest\"\n    )\n\n    val_images = train_generator.flow_from_dataframe(\n        dataframe=train_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=True,\n        seed=0,\n        subset='validation',\n        rotation_range=30, # Uncomment to use data augmentation\n        zoom_range=0.15,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.15,\n        horizontal_flip=True,\n        fill_mode=\"nearest\"\n    )\n\n    test_images = test_generator.flow_from_dataframe(\n        dataframe=test_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=False\n    )\n    \n    return train_generator,test_generator,train_images,val_images,test_images","b80aa8bc":"# Load the pretained model\npretrained_model = tf.keras.applications.MobileNetV2(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights='imagenet',\n    pooling='avg'\n)\n\npretrained_model.trainable = False","09fda7e2":"# Separate in train and test data\ntrain_df, test_df = train_test_split(image_df, train_size=0.9, shuffle=True, random_state=1)\n\n# Create the generators\ntrain_generator,test_generator,train_images,val_images,test_images = create_gen()","2d6f809b":"inputs = pretrained_model.input\n\nx = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\n\noutputs = tf.keras.layers.Dense(12, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    train_images,\n    validation_data=val_images,\n    batch_size = 32,\n    epochs=10,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=2,\n            restore_best_weights=True\n        )\n    ]\n)","a2cf8b0f":"pd.DataFrame(history.history)[['accuracy','val_accuracy']].plot()\nplt.title(\"Accuracy\")\nplt.show()","cfc686bb":"pd.DataFrame(history.history)[['loss','val_loss']].plot()\nplt.title(\"Loss\")\nplt.show()","cc82b0b6":"results = model.evaluate(test_images, verbose=0)","cb33910d":"printmd(\" ## Test Loss: {:.5f}\".format(results[0]))\nprintmd(\"## Accuracy on the test set: {:.2f}%\".format(results[1] * 100))","73292a4f":"# Predict the label of the test_images\npred = model.predict(test_images)\npred = np.argmax(pred,axis=1)\n\n# Map the label\nlabels = (train_images.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred = [labels[k] for k in pred]\n\n# Display the result\nprint(f'The first 5 predictions: {pred[:5]}')","f8dc9972":"from sklearn.metrics import classification_report\ny_test = list(test_df.Label)\nprint(classification_report(y_test, pred))","6c49303f":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncf_matrix = confusion_matrix(y_test, pred, normalize='true')\nplt.figure(figsize = (10,6))\nsns.heatmap(cf_matrix, annot=True, xticklabels = sorted(set(y_test)), yticklabels = sorted(set(y_test)))\nplt.title('Normalized Confusion Matrix')\nplt.show()","1830a9fb":"# Display some pictures of the dataset with their labels and the predictions\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(test_df.Filepath.iloc[i]))\n    ax.set_title(f\"True: {test_df.Label.iloc[i]}\\nPredicted: {pred[i]}\")\nplt.tight_layout()\nplt.show()","bf156cb1":"def get_img_array(img_path, size):\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=size)\n    array = tf.keras.preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size \"size\"\n    array = np.expand_dims(array, axis=0)\n    return array\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer as well as the output predictions\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    # This is the gradient of the output neuron (top predicted or chosen)\n    # with regard to the output feature map of the last conv layer\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    # then sum all the channels to obtain the heatmap class activation\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = tf.maximum(heatmap, 0) \/ tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n\ndef save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n    # Load the original image\n    img = tf.keras.preprocessing.image.load_img(img_path)\n    img = tf.keras.preprocessing.image.img_to_array(img)\n\n    # Rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # Use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # Use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # Create an image with RGB colorized heatmap\n    jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * alpha + img\n    superimposed_img = tf.keras.preprocessing.image.array_to_img(superimposed_img)\n\n    # Save the superimposed image\n    superimposed_img.save(cam_path)\n\n    # Display Grad CAM\n#     display(Image(cam_path))\n    \n    return cam_path\n    \npreprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\ndecode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n\nlast_conv_layer_name = \"Conv_1\"\nimg_size = (224,224)\n\n# Remove last layer's softmax\nmodel.layers[-1].activation = None","4092107e":"# Display the part of the pictures used by the neural network to classify the pictures\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    img_path = test_df.Filepath.iloc[i]\n    img_array = preprocess_input(get_img_array(img_path, size=img_size))\n    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n    cam_path = save_and_display_gradcam(img_path, heatmap)\n    ax.imshow(plt.imread(cam_path))\n    ax.set_title(f\"True: {test_df.Label.iloc[i]}\\nPredicted: {pred[i]}\")\nplt.tight_layout()\nplt.show()","1585c0e3":"# 4. Visualize the result<a class=\"anchor\" id=\"4\"><\/a>","40fe090f":"# 3. Train the model<a class=\"anchor\" id=\"3\"><\/a>","3e1d14e0":"# Plant seedlings classifier\n## \\# Class activation heatmap for image classification\n## \\# Grad-CAM class activation visualization\n\nHaving around 5.000 pictures of 12 different crop and weed seedlings\n\n![seedlings](https:\/\/i.imgur.com\/6oTep8r.png)","ab447de4":"# 1. Load and transform the dataset<a class=\"anchor\" id=\"1\"><\/a>","c3306a96":"# 2. Load the Images with a generator<a class=\"anchor\" id=\"2\"><\/a>","40dcea1c":"# 5. Class activation heatmap for image classification<a class=\"anchor\" id=\"5\"><\/a>\n## Grad-CAM class activation visualization\n*Code adapted from keras.io*","31070f77":"# Table of contents\n\n[<h3>1. Load and transform the dataset<\/h3>](#1)\n\n[<h3>2. Load the Images with a generator<\/h3>](#2)\n\n[<h3>3. Train the model<\/h3>](#3)\n\n[<h3>4. Visualize the result<\/h3>](#4)\n\n[<h3>5. Class activation heatmap for image classification<\/h3>](#5)\n\n## Context\nWhy is it important to detect weeds while they are still seedlings?\n\nThis is what the University of Pretoria has to say regarding maize (corn) farming in southern Africa:\n\n> Successful cultivation of maize depends largely on the efficacy of weed control. Weed control during the first six to eight weeks after planting is crucial, because weeds compete vigorously with the crop for nutrients and water during this period. Annual yield losses occur as a result of weed infestations in cultivated crops. Crop yield losses that are attributable to weeds vary with type of weed, type of crop, and the environmental conditions involved. Generally, depending on the level of weed control practiced yield losses can vary from 10 to 100 %. Rarely does one experience zero yield loss due to weeds... Yield losses occur as a result of weed interference with the crop's growth and development....This explains why effective weed control is imperative. In order to do effective control the first critical requirement is correct weed identification.\n\n## Content\n\nThis dataset contains 5,539 images of crop and weed seedlings. The images are grouped into 12 classes as shown in the above pictures. These classes represent common plant species in Danish agriculture. Each class contains rgb images that show plants at different growth stages. The images are in various sizes and are in png format.\n\nThe V1 version of this dataset was used in the Plant Seedling Classification playground competition here on Kaggle. This is the V2 version. Some samples in V1 contained multiple plants. The dataset\u2019s creators have now removed those samples.\n\n## Citation\nPaper: A Public Image Database for Benchmark of Plant Seedling Classification Algorithms\n\n\n## Acknowledgements\nMany thanks to the Computer Vision and Signal Processing Group, Department of Engineering \u2013 Aarhus University, for making this dataset publicly available."}}