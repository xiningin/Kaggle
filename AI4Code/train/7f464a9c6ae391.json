{"cell_type":{"97e93d3e":"code","ade93b8f":"code","cac3598d":"code","a34510ab":"code","e328dbd3":"code","70a8f7ba":"code","d6ce7c63":"code","a260217f":"code","a0acb22b":"code","a4abe9cb":"code","bae3896c":"code","3758bc0b":"code","7397edab":"code","34511f9e":"code","8c00b84e":"code","ad4d343b":"code","9923e0a5":"code","eb105eac":"code","2903a3e0":"code","91d031fc":"code","80f7b2d6":"code","f838c1b9":"code","76281643":"code","649e55d9":"code","3e1c51f3":"code","93de532c":"markdown","5d05f6bf":"markdown","430fd853":"markdown","f23e1ca0":"markdown","5a0339c3":"markdown","8e308a8d":"markdown","3a307ba8":"markdown","90b4ef0e":"markdown","a7f779ba":"markdown","a0653307":"markdown","af66e7aa":"markdown","5396e8c6":"markdown","79d311c0":"markdown","ba83ccb9":"markdown","178c44dc":"markdown","e43d7bb8":"markdown","5c43a20d":"markdown","79a0f0d0":"markdown","37ac0a6f":"markdown","a4ae069f":"markdown","a4bb66ea":"markdown","d4df439c":"markdown","82724714":"markdown"},"source":{"97e93d3e":"pip install pandas-profiling","ade93b8f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt # visualization\nimport seaborn as sns # visualization\n%matplotlib inline\n\nfrom pandas_profiling import ProfileReport # pandas-profiling\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cac3598d":"df = pd.read_csv('\/kaggle\/input\/usa-housing\/USA_Housing.csv')","a34510ab":"df.info()","e328dbd3":"df.head()","70a8f7ba":"df.tail()","d6ce7c63":"profile = ProfileReport(df, title=\"Pandas Profiling Report\")\nprofile","a260217f":"sns.pairplot(df)","a0acb22b":"df.hist(bins=60, figsize=(15,9),color='darkviolet')\nplt.show()","a4abe9cb":"df = df.drop('Address', 1)","bae3896c":"df.columns","3758bc0b":"# 80% data s\u1eed d\u1ee5ng training\ntrain_len = int(len(df) * 0.8)\n\n# 20% data \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh\neval_len = len(df) - train_len\n\n# 1. C\u1eaft 80% cho training v\u00e0 20% cho validation\ntrain_df = df.iloc[:train_len].sample(frac=1, random_state=48).reset_index(drop=True)\neval_df = df.iloc[train_len:].sample(frac=1, random_state=48).reset_index(drop=True)\n\n# 2. C\u1eaft c\u1ed9t median_house_value kh\u1ecfi features v\u00e0 t\u1ea1o nh\u00e3n\nX_train = train_df.drop('Price', axis = 1)\ny_train = train_df['Price']\n\n# 3. C\u1eaft c\u1ed9t median_house_value kh\u1ecfi features v\u00e0 t\u1ea1o nh\u00e3n\nX_val = eval_df.drop('Price', axis = 1)\ny_val = eval_df['Price']","7397edab":"# 4. Hi\u1ec3n th\u1ecb chi\u1ec1u\n# Train, Dev\n# Train, Val\nX_train.shape, X_val.shape","34511f9e":"df.columns","8c00b84e":"from sklearn.preprocessing import StandardScaler\nstandard_scaler = StandardScaler()\n\ncolumn_names_to_standardize = ['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms', 'Avg. Area Number of Bedrooms', 'Area Population']\n\n# >>>>> Train <<<<<<<\n\n# 13. Chu\u1ea9n h\u00f3a c\u00e1c c\u1ed9t v\u1ec1 ph\u00e2n ph\u1ed1i chu\u1ea9n \u0111\u01a1n v\u1ecb\nx_train_cols = X_train[column_names_to_standardize].values\nx_train_cols_scaled = standard_scaler.fit_transform(x_train_cols)\nstandardized_train_features = pd.DataFrame(x_train_cols_scaled, columns=column_names_to_standardize, index = X_train.index)\nX_train[column_names_to_standardize] = standardized_train_features\n\n# >>>>> Validation <<<<<<<\n\n# 14. Chu\u1ea9n h\u00f3a c\u00e1c c\u1ed9t v\u1ec1 ph\u00e2n ph\u1ed1i chu\u1ea9n \u0111\u01a1n v\u1ecb\nx_val_cols = X_val[column_names_to_standardize].values\n\n# Chu\u1ea9n h\u00f3a X_val theo c\u00e1c tham s\u1ed1 c\u1ee7a X_train: c\u1ee5 th\u1ec3 l\u00e0 mu v\u00e0 sigma\n# S\u1ebd \u0111\u01b0\u1ee3c \u0111\u1ec1 c\u1eadp chi ti\u1ebft t\u1ea1i b\u00e0i ph\u00e2n ph\u1ed1i chu\u1ea9n\n# standard_scaler n\u00f3 \u0111\u00e3 l\u01b0u c\u00e1c tham s\u1ed1 c\u1ee7a X_train\n# Thay v\u00ec s\u1eed d\u1ee5ng fit_transform th\u00ec s\u1eed d\u1ee5ng transform\n\nx_val_cols_scaled = standard_scaler.transform(x_val_cols)\nstandardized_val_features = pd.DataFrame(x_val_cols_scaled, columns=column_names_to_standardize, index = X_val.index)\nX_val[column_names_to_standardize] = standardized_val_features","ad4d343b":"X_train.head()","9923e0a5":"X_val.head()","eb105eac":"profile_after_scale = ProfileReport(X_train, title=\"Pandas Profiling Report\")\nprofile_after_scale","2903a3e0":"def init_theta(n):\n  theta = np.random.normal(size=n).reshape((n, 1))\n  return theta","91d031fc":"def hypo(X, theta):\n  y_hat = X.dot(theta)\n  return y_hat","80f7b2d6":"def loss(X, y, theta, m):  \n  loss_value = 1 \/ (2 * m) * np.sum((hypo(X, theta) - y) ** 2)\n\n  return loss_value","f838c1b9":"def gradient(X, y, theta, m):\n  # L\u1eadp tr\u00ecnh t\u1ea1i \u0111\u00e2y\n  y_hat = hypo(X, theta) \n  gradient = 1 \/ m * X.T.dot(y_hat - y)\n  return gradient","76281643":"def train(X_train, y_train, X_val, y_val, epochs, learning_rate):\n  # L\u1eadp tr\u00ecnh t\u1ea1i \u0111\u00e2y\n\n  # L\u1ea5y ma tr\u1eadn t\u1eeb Dataframe c\u1ee7a pandas\n  X_train = X_train.values\n  X_val = X_val.values\n\n  # L\u1ea5y chi\u1ec1u c\u1ee7a X_train bao g\u1ed3m s\u1ed1 d\u00f2ng v\u00e0 s\u1ed1 c\u1ed9t\n  m_train, n_train = X_train.shape[0], X_train.shape[1]\n\n  # L\u1ea5y chi\u1ec1u c\u1ee7a X_val bao g\u1ed3m s\u1ed1 d\u00f2ng v\u00e0 s\u1ed1 c\u1ed9t\n  m_val, n_val = X_val.shape[0], X_val.shape[1]\n\n  # \u0110i\u1ec1u ch\u1ec9nh chi\u1ec1u cho y_train, y_val th\u00e0nh ma tr\u1eadn 1 c\u1ed9t\n  y_train = y_train.values.reshape(-1, 1)\n  y_val = y_val.values.reshape(-1, 1)\n\n\n  # Kh\u1edfi t\u1ea1o c\u00e1c gi\u00e1 tr\u1ecb c\u1ea7n t\u00ednh to\u00e1n bao g\u1ed3m\n    # - train_loss: Gi\u00e1 tr\u1ecb m\u1ea5t m\u00e1t tr\u00ean t\u1eadp Train\n    # - validation_loss: Gi\u00e1 tr\u1ecb m\u1ea5t m\u00e1t tr\u00ean t\u1eadp Validation\n    # - theta: Tham s\u1ed1 c\u1ee7a m\u00f4 h\u00ecnh\n  train_loss = 0\n  validation_loss = 0\n  theta = init_theta(n_train)\n\n  # L\u1eb7p qua c\u00e1c epoch:\n  for i in range(epochs):\n    print('Epoch {}\/{}'.format(i+1, epochs))\n\n    # T\u00ednh Gradient c\u1ee7a h\u00e0m m\u1ea5t m\u00e1t tr\u00ean theta\n    grad = gradient(X_train, y_train, theta, m_train)\n    \n    # C\u1eadp nh\u1eadt theta theo gradient nh\u01b0 c\u00f4ng th\u1ee9c b\u00ean tr\u00ean\n    theta = theta - learning_rate * grad\n    \n    # T\u00ednh gi\u00e1 tr\u1ecb Loss tr\u00ean t\u1eadp train\n    train_loss = loss(X_train, y_train, theta, m_train)\n\n    # T\u00ednh gi\u00e1 tr\u1ecb Loss tr\u00ean t\u1eadp validation\n    validation_loss = loss(X_val, y_val, theta, m_val)\n\n    # In 2 gi\u00e1 tr\u1ecb loss n\u00e0y\n    print('Loss: {:.6f} Validation loss: {:.6f}'.format(train_loss, validation_loss))\n    print('----------------------------------------')\n\n  return theta, train_loss, validation_loss\n","649e55d9":"epochs = 40000\nlearning_rate = 0.01\n\ntheta, train_loss, validation_loss = train(X_train, y_train, X_val, y_val, epochs, learning_rate)","3e1c51f3":"theta.shape","93de532c":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#FF5733;\n       font-size:220%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 20px;\n          color:white;\">\n        <b>4 |<\/b> BUILD MODEL\n    <\/p>\n<\/div>\n\n\u0110\u1ec3 b\u00e0i to\u00e1n \u0111\u01a1n gi\u1ea3n trong tr\u01b0\u1eddng h\u1ee3p n\u00e0y ta s\u1ebd kh\u00f4ng s\u1eed d\u1ee5ng **bias**.\n\nM\u1ed9t s\u1ed1 k\u00fd hi\u1ec7u c\u1ea7n ch\u00fa \u00fd: \n- $m$: S\u1ed1 d\u00f2ng c\u1ee7a t\u1eadp d\u1eef li\u1ec7u\n- $n$: S\u1ed1 c\u1ed9t c\u1ee7a t\u1eadp d\u1eef li\u1ec7u\n- $\\theta$: Tham s\u1ed1 c\u1ee7a m\u00f4 h\u00ecnh h\u1ed3i quy tuy\u1ebfn t\u00ednh. \n  - Chi\u1ec1u $\\theta \\in \\mathbb{R}^{n \\times 1}$\n- $\\theta_{\\text{new}}$: Tham s\u1ed1 c\u1ee7a m\u00f4 h\u00ecnh h\u1ed3i quy tuy\u1ebfn t\u00ednh \u1edf th\u1eddi \u0111i\u1ec3m sau. \n  - Chi\u1ec1u $\\theta_{\\text{new}} \\in \\mathbb{R}^{n \\times 1}$\n- $\\theta_{\\text{old}}$: Tham s\u1ed1 c\u1ee7a m\u00f4 h\u00ecnh h\u1ed3i quy tuy\u1ebfn t\u00ednh \u1edf th\u1eddi \u0111i\u1ec3m tr\u01b0\u1edbc. \n  - Chi\u1ec1u $\\theta_{\\text{old}} \\in \\mathbb{R}^{n \\times 1}$\n- $X$: Ma tr\u1eadn features d\u1eef li\u1ec7u hi\u1ec7n t\u1ea1i.\n  - Chi\u1ec1u $X \\in \\mathbb{R}^{m \\times n}$\n- $\\textbf{y}$: Vector nh\u00e3n. (Ta s\u1ebd d\u00f9ng ma tr\u1eadn 1 c\u1ed9t \u0111\u1ec3 t\u00ednh to\u00e1n tr\u00e1nh l\u1ed7i)\n  - Chi\u1ec1u $\\textbf{y} \\in \\mathbb{R}^{m \\times 1}$\n- $\\hat{\\textbf{y}}$: K\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n c\u1ee7a m\u00f4 h\u00ecnh.\n  - Chi\u1ec1u $\\hat{\\textbf{y}} \\in \\mathbb{R}^{m \\times 1}$\n- $\\nabla{J(\\theta)}$: Gradient c\u1ee7a h\u00e0m m\u1ea5t m\u00e1t tr\u00ean b\u1ed9 tham s\u1ed1\n  - Chi\u1ec1u $\\nabla{J(\\theta)} \\in \\mathbb{R}^{n \\times 1}$","5d05f6bf":"L\u1eadp tr\u00ecnh h\u00e0m t\u00ednh gi\u00e1 tr\u1ecb m\u1ea5t m\u00e1t tr\u00ean b\u1ed9 d\u1eef li\u1ec7u b\u1ea5t k\u1ef3. C\u00f4ng th\u1ee9c Mean Squared Error\n\n$$\nJ(\\theta) =  \\frac{1}{2m} \\sum{(\\hat{\\textbf{y}} - \\textbf{y})^2} = \\frac{1}{2m} \\sum{(X\\theta - \\textbf{y})^2} \n$$","430fd853":"### <b><span style='color:#FF5733'>3.2<\/span> | Standard Scaler<\/b>","f23e1ca0":"### <b><span style='color:#FF5733'>4.7<\/span> | Training<\/b>\n","5a0339c3":"### <b><span style='color:#FF5733'>4.4<\/span> | Gradient<\/b>\n","8e308a8d":"### <b><span style='color:#FF5733'>2.4<\/span> | Detail Exploratory Data Analysis (EDA)<\/b>\n\nCreate some simple plots to check out the data\n\nLets look at our data distribution, using `univariate analysis` (analysis of 1 variable).\nWhat we might look for in histograms:\n    \n- Data distribution ( certain models prefer less skewed distributions ) <br>\n- Outliers ( Low Noise Assumption can be detremental to model performance ) <br>\n- Odd patterns in data ( Data abnormalities also affect model performance ) <br>\n- Axis Scale ( Feature scale values can affect a models performance )","3a307ba8":"C\u00f3 th\u1ec3 s\u1eed d\u1ee5ng ph\u00e2n ph\u1ed1i chu\u1ea9n: [t\u1ea1i \u0111\u00e2y](https:\/\/numpy.org\/doc\/stable\/reference\/random\/generated\/numpy.random.rand.html)","90b4ef0e":"![](https:\/\/images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com\/f\/8cc1eeaa-4046-4c4a-ae93-93d656f68688\/deogm3w-009101b0-ea70-4b7c-8ba5-5e62f12f751d.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcLzhjYzFlZWFhLTQwNDYtNGM0YS1hZTkzLTkzZDY1NmY2ODY4OFwvZGVvZ20zdy0wMDkxMDFiMC1lYTcwLTRiN2MtOGJhNS01ZTYyZjEyZjc1MWQuanBnIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.PBuOz6sTEJRUCBKXnLz20kScs7x7q3usy4uQQ5x5N_c)\nPhoto of the Golden Gate Bridge in California by <a href=\"https:\/\/unsplash.com\/@jbcreate_?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Joseph Barrientos<\/a> on <a href=\"https:\/\/unsplash.com\/s\/photos\/california?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash<\/a>\n  \n\n<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#FF5733;\n       font-size:220%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 20px;\n          color:white;\">\n        <b>1 |<\/b> INTRODUCTION\n    <\/p>\n<\/div>\n\n### <b><span style='color:#FF5733'>1.1<\/span> | Notebook Goal<\/b>\n\n- The aim is to model is to predict the `Price`, <code>target variable<\/code>\n- Understand how to code from scratch <b>Gradient Descent<\/b> to optimize <b>Linear Regression<\/b> problems with many variables.\n\n### <b><span style='color:#FF5733'>1.2<\/span> | What is covered<\/b>\n\n- Introduction to basic ML principles such as `scaling`, `feature engineering`\n- A basic introduction to `sklearn` compatible classes,`model exploration` & `model modification` in an attempt to  improve our model\n\n### <b><span style='color:#FF5733'>1.3<\/span> | Algorithm<\/b>\n- Optimizer Algorithm: <b>Gradient Descent<\/b>\n- Learning Algorithm: <b>Linear Regression<\/b>\n<br> \n\n<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#FF5733;\n       font-size:220%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 20px;\n          color:white;\">\n        <b>2 |<\/b> DATA PREPARATION\n    <\/p>\n<\/div>\n\ndf.info(),describe(),head() are probably one of the first things we might want to inspect having a pandas dataframe; showing feature names, limits\/stats and and a few first columns respectively, to get a some initial impression of the data. \n\n### <b><span style='color:#FF5733'>2.1<\/span> | Loading the Dataset<\/b>\nI have made some minor adjustment to the [original dataset](https:\/\/www.kaggle.com\/vedavyasv\/usa-housing) and pulled out some random data from all but two coordinate features, so we can do some data imputation, the dataset is located below, even though its not made public.","a7f779ba":"### <b><span style='color:#FF5733'>4.5<\/span> | Gradient Descent<\/b>\n","a0653307":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#FF5733;\n       font-size:220%;\n       font-family:Nexa;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 20px;\n          color:white;\">\n        <b>3 |<\/b> NORMALIZE DATA\n    <\/p>\n<\/div>\n\n### <b><span style='color:#FF5733'>3.1<\/span> | Prepare Data<\/b>\n\nUse Standard Scaling follows distribution of data","af66e7aa":"### <b><span style='color:#FF5733'>2.3<\/span> | Pandas Profiling<\/b>\n\nGenerates profile reports from a pandas DataFrame.","5396e8c6":"### <b><span style='color:#FF5733'>4.6<\/span> | Train Function<\/b>\n","79d311c0":"### <b><span style='color:#FF5733'>4.2<\/span> | Hypothesis<\/b>\n\nL\u1eadp tr\u00ecnh h\u00e0m gi\u1ea3 thi\u1ebft c\u1ee7a m\u00f4 h\u00ecnh h\u1ed3i quy tuy\u1ebfn t\u00ednh\n\n$$\n\\hat{\\textbf{y}} = h_{\\theta}(X) = X\\theta\n$$","ba83ccb9":"L\u1eadp tr\u00ecnh c\u00f4ng th\u1ee9c t\u00ednh Gradient c\u1ee7a h\u00e0m m\u1ea5t m\u00e1t tr\u00ean $\\theta$\n\n$$ \\nabla{J(\\theta)} = \\frac{1}{m} X^T(\\hat{\\textbf{y}} - \\textbf{y})   = \\frac{1}{m} X^T(X\\theta - \\textbf{y}) $$","178c44dc":"### <b><span style='color:#FF5733'>2.2<\/span> | Data Explorer<\/b>\n\n> We are going to use the `USA_Housing` dataset. Since house price is a continues variable, this is a regression problem. The data contains the following columns:\n\n> * `Avg. Area Income`: Avg. Income of residents of the city house is located in.\n> * `Avg. Area House Age`: Avg Age of Houses in same city\n> * `Avg. Area Number of Rooms`: Avg Number of Rooms for Houses in same city\n> * `Avg. Area Number of Bedrooms`: Avg Number of Bedrooms for Houses in same city\n> * `Area Population`: Population of city hou  se is located in\n> * `Price`: Price that the house sold at\n> * `Address`: Address for the house","e43d7bb8":"### <b><span style='color:#FF5733'>4.3<\/span> | Mean Squared Error<\/b>\n","5c43a20d":"Kh\u1edfi t\u1ea1o tham s\u1ed1 c\u1ee7a m\u00f4 h\u00ecnh ng\u1eabu nhi\u00ean v\u1edbi chi\u1ec1u nh\u01b0 sau:\n\n$$\\theta \\in \\mathbb{R}^{n \\times 1} $$","79a0f0d0":"K\u00fd hi\u1ec7u $\\sum$ \u1edf \u0111\u00e2y th\u1ec3 hi\u1ec7n t\u1ed5ng c\u00e1c gi\u00e1 tr\u1ecb trong vector.","37ac0a6f":"### <b><span style='color:#FF5733'>4.1<\/span> | Theta<\/b>\n","a4ae069f":"Display shape","a4bb66ea":"C\u00f4ng th\u1ee9c Gradient Descent: C\u1eadp nh\u1eadt tham s\u1ed1 theo Gradient.\n\n$$\\theta_{\\textbf{new}} = \\theta_{\\textbf{old}} - \\alpha \\nabla{J(\\theta)} = \\theta_{\\textbf{old}} - \\alpha \\frac{1}{m} X^T(X\\theta - \\textbf{y}) $$","d4df439c":"After normalize","82724714":"Ti\u1ebfn h\u00e0nh training. \n\nB\u1ea1n c\u1ea7n \u0111i\u1ec1u ch\u1ec9nh 2 gi\u00e1 tr\u1ecb epochs v\u00e0 t\u1ed1c \u0111\u1ed9 h\u1ecdc \u0111\u1ec3 c\u00f3 \u0111\u01b0\u1ee3c k\u1ebft qu\u1ea3 t\u1ed1t."}}