{"cell_type":{"bac568ee":"code","edb1c639":"code","90d83db1":"code","f9d8d9d6":"code","95603314":"code","6a5f3a19":"markdown","664d254a":"markdown","06a3b2ee":"markdown","5498ada0":"markdown","ab40d0de":"markdown","b666ac57":"markdown","bd50063c":"markdown","0eaa8e49":"markdown","e3919212":"markdown","4ca4ff42":"markdown","5a70469a":"markdown","d65b9138":"markdown"},"source":{"bac568ee":"import numpy as np\nimport pandas as pd\n\n# generate a random dataframe with @\n# 2 columns and 100 rows\ndf = pd.DataFrame(\n    np.random.rand(200, 2),\n    columns=[f\"f_{i}\" for i in range(1, 3)]\n)\ndf.head()","edb1c639":"from sklearn import preprocessing\n\n# initialize polynomial features class object\n# for two-degree polynomial features\npf = preprocessing.PolynomialFeatures(\n    degree=2,\n    interaction_only=False,\n    include_bias=False\n)\n\n# fit to the features\npf.fit(df)\n\n# create polynomial features\npoly_feats = pf.transform(df)\n\n# create a dataframe with all the features\nnum_feats = poly_feats.shape[1]\ndf_transformed = pd.DataFrame(\n    poly_feats,\n    columns=[f'f_{i}' for i in range(1, num_feats + 1)]\n)\n\ndf_transformed.head()\n\n","90d83db1":"# create bins of the numerical columns\n# 10 bins\ndf[\"f_bin_10\"] = pd.cut(df[\"f_1\"], bins=10, labels=False)\n# 100 bins\ndf[\"f_bin_100\"] = pd.cut(df[\"f_1\"], bins=10, labels=False)\n\ndf","f9d8d9d6":"import numpy as np\nfrom sklearn import impute\n\n# create a random numpy array with 10 samples\n# and 6 features and values ranging from 1 to 15\nX = np.random.randint(1, 15, (10, 6))\n\n# convert the array to float\nX = X.astype(float)\n\n# randomly assign 10 elements to NaN ( missing)\nX.ravel()[np.random.choice(X.size, 10, replace=False)] = np.nan\n\nX","95603314":"# use 2 nearest neighbours to fill NaN values\nknn_imputer = impute.KNNImputer(n_neighbors=2)\nknn_imputer.fit_transform(X)","6a5f3a19":"<h2>Binning<\/h2><\/br>\nwe use ten bins adn we see that we can divied the data into\nten parts. This is accomplished using the pandas' <i>cut<\/i>function.","664d254a":"So, now we have created some polynomical features.<\/br><\/br>\nThe more the number of features, the more the no of polynomial features and you must also remember that if you have a lot of samples in the dataset, it is going to take a while creating thses kinds of features.","06a3b2ee":"A fancy way of filling in the missing values would be to use a k-nearest neighbour method. You can select a sample with missing values and find the nearest neighbours utilising some kind of distance metric, for example, Euclidean distance. Then you can take the mean of all nearest neighbours and fill up the missing value. You can use the KNN imputer implementation for filling missing values","5498ada0":"One way to fill missing values in numerical data would be to choose a value that does not appear in the specific feature and fill using that. For example, let\u2019s say 0 is not seen in the feature. So, we fill all the missing values using 0. This is one of the ways but might not be the most effective. One of the methods that works better than filling 0s for numerical data is to fill with mean instead. You can also try to fill with the median of all the values for that feature, or you can use the most common value to fill the missing values. There are just so many ways to do this.","ab40d0de":"as you can see in our matrix we have some missing values.","b666ac57":"Yet another interesting type of feature that you can create from numerical features is log transformation.<\/br> we would want to reduce the variance of this column, and that can be done by taking a log transformation.And we can apply log(1 + x) to this column to reduce its variance. ","bd50063c":"*Always remember that imputing values for tree-based models is unnecessary as they can handle it themselves.*","0eaa8e49":"When you bin, you can use both the bin and the original feature.<i>Binning also enables you to treat numerical features as categorical.<\/i>","e3919212":"And we can create two-degree polynomial features using PolynomialFeatures","4ca4ff42":"\nAnother way of imputing missing values in a column would be to train a regression model that tries to predict missing values in a column based on other columns. So, you start with one column that has a missing value and treat this column as the target column for regression model without the missing values. Using all the other columns, you now train a model on samples for which there is no missing value in the concerned column and then try to predict target (the same column) for the samples that were removed earlier. This way, you have a more robust model based imputation.","5a70469a":"# ***If you ever encounter missing values in categorical features, treat is as a new category! As simple as this is, it (almost) always works!***","d65b9138":"Sometimes, instead of log, you can also take exponential. A very interesting case is when you use a log-based evaluation metric, for example, RMSLE. In that case, you can train on log-transformed targets and convert back to original using exponential on the prediction. That would help optimize the model for the metric.\n"}}