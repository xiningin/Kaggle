{"cell_type":{"cf8d84ff":"code","ae15f6e2":"code","4a85209d":"code","ca13b65a":"code","1b204e85":"code","5bffa8a6":"code","aa3bd25a":"code","6d3f7cfd":"code","6ddc6ccd":"code","5ff1da79":"code","36d0866e":"code","2d7280bd":"code","e5857014":"code","37498231":"code","541a1214":"code","f77fd180":"code","2fea7de5":"code","334041b3":"code","5660f585":"code","0553222f":"code","1d870fea":"code","6b087b7f":"code","66c3dd99":"code","c22c2653":"code","66bcfef5":"code","cae6995d":"code","966ba2c5":"code","1539a35f":"code","11b9d20b":"code","7067e589":"code","177fbbcd":"code","314e4f4c":"code","fe35684b":"code","1493ae26":"code","6a58965b":"code","805df97f":"code","58ecb297":"code","9e0f266e":"code","4ecd05e8":"code","83b31806":"markdown","b89c78ae":"markdown","d9c703aa":"markdown","09adb521":"markdown","8c122edb":"markdown","703709fe":"markdown","3ac928e7":"markdown","570b9994":"markdown","7386c3fb":"markdown","6ecf0ad1":"markdown","3aba6e52":"markdown","afd4774c":"markdown","5014d7d0":"markdown","54e252b1":"markdown","bdbfb59f":"markdown","638d2f8e":"markdown","a7a41865":"markdown","6bbea3ae":"markdown","4f0473cb":"markdown","8aabc2ea":"markdown","f28e1ddf":"markdown","3bcd6fad":"markdown","ce4352b8":"markdown","0625df0e":"markdown","ee4b6d3b":"markdown","bbfce9d1":"markdown","d2f96512":"markdown","b28419e2":"markdown","51b081c9":"markdown","5d71a3e3":"markdown","d6e177b5":"markdown","faa70289":"markdown","3dc11253":"markdown","2e2bfc91":"markdown","dcd1597c":"markdown","8cb3055c":"markdown","1a84917c":"markdown","af290762":"markdown","214a637d":"markdown","46060bec":"markdown","a8a58b5c":"markdown","d9d26380":"markdown","68116ffe":"markdown","0a543203":"markdown","88ad61c6":"markdown","7b2a9130":"markdown","9845c751":"markdown","a27ee9d9":"markdown","c6c0dd27":"markdown","5c78a520":"markdown","dea1a6e7":"markdown","e54861f2":"markdown","a3892ff5":"markdown","47e5f8aa":"markdown","c189c84a":"markdown","4c5755ec":"markdown","9dc85fcf":"markdown","75bea8a6":"markdown","e91203d0":"markdown","3fe4fc58":"markdown","df2698ec":"markdown"},"source":{"cf8d84ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\n#load packages\nimport sys #access to system parameters https:\/\/docs.python.org\/3\/library\/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport matplotlib #collection of functions for scientific and publication-ready visualization\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\n\nimport numpy as np #foundational package for scientific computing\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport scipy as sp #collection of functions for scientific computing and advance mathematics\nprint(\"SciPy version: {}\". format(sp.__version__)) \n\nimport IPython\nfrom IPython import display #pretty printing of dataframes in Jupyter notebook\nprint(\"IPython version: {}\". format(IPython.__version__)) \n\nimport sklearn #collection of machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\n#misc libraries\nimport random\nimport time\n\n\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('-'*25)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","ae15f6e2":"#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.plotting import autocorrelation_plot\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","4a85209d":"#Loading the single csv file to a variable named 'airbnb'\nairbnb=pd.read_csv(\"..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv\")","ca13b65a":"#Lets look at a glimpse of table\nairbnb.head()","1b204e85":"print (\"The shape of the  data is (row, column):\"+ str(airbnb.shape))\nprint (airbnb.info())","5bffa8a6":"#Looking at the datatypes of each factor\nairbnb.dtypes","aa3bd25a":"import missingno as msno \nmsno.matrix(airbnb)","6d3f7cfd":"print('Data columns with null values:',airbnb.isnull().sum(), sep = '\\n')","6ddc6ccd":"airbnb['reviews_per_month'].fillna(value=0, inplace=True)\nprint('Reviews_per_month column with null values:',airbnb['reviews_per_month'].isnull().sum(), sep = '\\n')","5ff1da79":"airbnb.drop(['id','host_name','last_review'], axis = 1,inplace=True) \nairbnb.head()","36d0866e":"plt.figure(figsize = (15, 10))\nplt.style.use('seaborn-white')\nax=plt.subplot(221)\nplt.boxplot(airbnb['number_of_reviews'])\nax.set_title('Numer of Reviews')\nax=plt.subplot(222)\nplt.boxplot(airbnb['price'])\nax.set_title('Price')\nax=plt.subplot(223)\nplt.boxplot(airbnb['availability_365'])\nax.set_title('availability_365')\nax=plt.subplot(224)\nplt.boxplot(airbnb['reviews_per_month'])\nax.set_title('reviews_per_month')","2d7280bd":"Q1 = airbnb['price'].quantile(0.25)\nQ3 = airbnb['price'].quantile(0.75)\nIQR = Q3 - Q1    #IQR is interquartile range. \n\nfilter = (airbnb['price'] >= Q1 - 1.5 * IQR) & (airbnb['price'] <= Q3 + 1.5 *IQR)\nairbnb1=airbnb.loc[filter]\n\nQ1 = airbnb1['number_of_reviews'].quantile(0.25)\nQ3 = airbnb1['number_of_reviews'].quantile(0.75)\nIQR = Q3 - Q1    #IQR is interquartile range. \n\nfilter = (airbnb1['number_of_reviews'] >= Q1 - 1.5 * IQR) & (airbnb1['number_of_reviews'] <= Q3 + 1.5 *IQR)\nairbnb2=airbnb1.loc[filter]\n\n\nQ1 = airbnb2['reviews_per_month'].quantile(0.25)\nQ3 = airbnb2['reviews_per_month'].quantile(0.75)\nIQR = Q3 - Q1    #IQR is interquartile range. \n\nfilter = (airbnb2['reviews_per_month'] >= Q1 - 1.5 * IQR) & (airbnb2['reviews_per_month'] <= Q3 + 1.5 *IQR)\nairbnb_new=airbnb2.loc[filter]","e5857014":"plt.figure(figsize = (15, 10))\nplt.style.use('seaborn-white')\nax=plt.subplot(221)\nplt.boxplot(airbnb_new['number_of_reviews'])\nax.set_title('Numer of Reviews')\nax=plt.subplot(222)\nplt.boxplot(airbnb_new['price'])\nax.set_title('Price')\nax=plt.subplot(223)\nplt.boxplot(airbnb_new['availability_365'])\nax.set_title('availability_365')\nax=plt.subplot(224)\nplt.boxplot(airbnb_new['reviews_per_month'])\nax.set_title('reviews_per_month')","37498231":"plt.figure(figsize = (15, 7))\nplt.style.use('seaborn-white')\n#Neighbourhood group\nplt.subplot(221)\nsns.countplot(x=\"neighbourhood_group\", data=airbnb_new, palette=\"Greens_d\",\n              order=airbnb_new.neighbourhood_group.value_counts().index)\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\n#Top 10 Neighbourhood\nplt.subplot(222)\nax=sns.countplot(x=\"neighbourhood\", data=airbnb_new, palette=\"Greens_d\",\n              order=airbnb_new.neighbourhood.value_counts().iloc[:10].index)\nfig = plt.gcf()\nfig.set_size_inches(10,10)\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nax.set_xticklabels(ax.get_xticklabels(), fontsize=9)\nplt.tight_layout()\nplt.show()\n\n#Room type\nplt.subplot(223)\nsns.countplot(x=\"room_type\", data=airbnb_new, palette=\"Greens_d\",\n              order=airbnb_new.room_type.value_counts().index)\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\n#Top 10 hosts\nplt.subplot(224)\nax=sns.countplot(x=\"host_id\", data=airbnb_new, palette=\"Greens_d\",\n              order=airbnb_new.host_id.value_counts().iloc[:10].index)\nfig = plt.gcf()\nfig.set_size_inches(10,10)\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nax.set_xticklabels(ax.get_xticklabels(), fontsize=9)\nplt.tight_layout()\nplt.show()","541a1214":"plt.figure(figsize = (15, 7))\nplt.style.use('seaborn-white')\nplt.subplot(221)\nsns.distplot(airbnb_new['price'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\nplt.subplot(222)\nsns.distplot(airbnb_new['reviews_per_month'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\nplt.subplot(223)\nsns.distplot(airbnb_new['number_of_reviews'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\nplt.subplot(224)\nsns.distplot(airbnb_new['availability_365'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n","f77fd180":"plt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = (16, 8)\nax = sns.boxplot(x = airbnb_new['neighbourhood_group'], y =airbnb_new['price'], data = airbnb_new, palette = 'Set3')\nax.set_xlabel(xlabel = 'Location', fontsize = 20)\nax.set_ylabel(ylabel = 'Price', fontsize = 20)\nax.set_title(label = 'Distribution of prices acros location', fontsize = 30)\nplt.xticks(rotation = 90)\nplt.show()","2fea7de5":"#Code forked from-https:\/\/www.kaggle.com\/biphili\/hospitality-in-era-of-airbnb\nplt.style.use('seaborn-white')\nf,ax=plt.subplots(1,2,figsize=(18,8))\nairbnb_new['room_type'].value_counts().plot.pie(explode=[0,0.05,0],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Share of Room Type')\nax[0].set_ylabel('Room Type Share')\nsns.countplot(x = 'room_type',hue = \"neighbourhood_group\",data = airbnb_new)\nax[1].set_title('Room types occupied by the neighbourhood_group')\nplt.show()","334041b3":"f,ax=plt.subplots(figsize=(10,10))\nsns.heatmap(airbnb_new.corr(),annot=True,linewidths=0.5,linecolor=\"black\",fmt=\".1f\",ax=ax)\nplt.show()","5660f585":"plt.figure(figsize = (15, 15))\nplt.style.use('seaborn-white')\nplt.subplot(221)\nsns.scatterplot(x=\"latitude\", y=\"longitude\",hue=\"neighbourhood_group\", data=airbnb_new)\nplt.subplot(222)\nsns.scatterplot(x=\"latitude\", y=\"longitude\",hue=\"room_type\", data=airbnb_new)\nplt.subplot(223)\nsns.scatterplot(x=\"latitude\", y=\"longitude\",hue=\"price\", data=airbnb_new)\nplt.subplot(224)\nsns.scatterplot(x=\"latitude\", y=\"longitude\",hue=\"availability_365\", data=airbnb_new)","0553222f":"import pandas as pd\nimport geopandas as gpd\nimport math\nimport folium\nfrom folium import Choropleth, Circle, Marker\nfrom folium.plugins import HeatMap, MarkerCluster\nm_1 = folium.Map(location=[40.7128,-74.0060], tiles='cartodbpositron', zoom_start=12)\n\n# Adding a heatmap to the base map\nHeatMap(data=airbnb_new[['latitude', 'longitude']], radius=10).add_to(m_1)\n\n# Displaying the map\nm_1","1d870fea":"number_of_reviews = airbnb_new[(airbnb_new.number_of_reviews.isin(range(50,58)))]\n# Creating a map\nm_2 = folium.Map(location=[40.7128,-74.0060], tiles='cartodbpositron', zoom_start=13)\n\n# Adding points to the map\nfor idx, row in number_of_reviews.iterrows():\n    Marker([row['latitude'], row['longitude']]).add_to(m_2)\n\n# Displaying the map\nm_2","6b087b7f":"# Creating the map\nm_3 = folium.Map(location=[40.7128,-74.0060], tiles='cartodbpositron', zoom_start=13)\n\n# Adding points to the map\nmc = MarkerCluster()\nfor idx, row in number_of_reviews.iterrows():\n    if not math.isnan(row['longitude']) and not math.isnan(row['latitude']):\n        mc.add_child(Marker([row['latitude'], row['longitude']]))\nm_3.add_child(mc)\n\n# Displaying the map\nm_3","66c3dd99":"airbnb_features=airbnb_new[['neighbourhood_group','room_type','availability_365','minimum_nights','calculated_host_listings_count','reviews_per_month','number_of_reviews']]\nairbnb_features.head()","c22c2653":"dummy_neighbourhood=pd.get_dummies(airbnb_features['neighbourhood_group'], prefix='dummy')\ndummy_roomtype=pd.get_dummies(airbnb_features['room_type'], prefix='dummy')\nairbnb_features = pd.concat([airbnb_features,dummy_neighbourhood,dummy_roomtype],axis=1)\nairbnb_features.drop(['neighbourhood_group','room_type'],axis=1, inplace=True)\nairbnb_features","66bcfef5":"from sklearn import preprocessing\nX=preprocessing.scale(airbnb_features)\ny=airbnb_new.price\nprint(X)\nprint(y)","cae6995d":"X = pd.DataFrame(X)\nX=X.rename(index=str, columns={0:'availability_365',1:'minimum_nights',2:'calculated_host_listings_count',3:'reviews_per_month',\n                             4:'number_of_reviews',5:'dummy_Bronx',6:'dummy_Brooklyn',7:'dummy_Manhattan',8:'dummy_Queens',9:'dummy_Staten Island',\n                             10:'dummy_Entire home\/apt',11:'dummy_Private room',12:'dummy_Shared room'})\nX.head()","966ba2c5":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\n\nmy_pipeline = Pipeline(steps=[('model', RandomForestRegressor(n_estimators=50,\n                                                              random_state=0))])","1539a35f":"from sklearn.model_selection import cross_val_score\n\n# Multiply by -1 since sklearn calculates *negative* MAE\nscores = -1 * cross_val_score(my_pipeline, X, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n\nprint(\"MAE scores:\\n\", scores)\nprint(\"Average MAE score (across experiments):\",scores.mean())","11b9d20b":"from sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# Since there is no preprocessing, we don't need a pipeline (used anyway as best practice!)\nmy_pipeline = make_pipeline(RandomForestClassifier(n_estimators=100))\ncv_scores = cross_val_score(my_pipeline, X, y, \n                            cv=5,\n                            scoring='accuracy')\n\nprint(\"Cross-validation accuracy: %f\" % cv_scores.mean())","7067e589":"# Drop leaky predictors from dataset\npotential_leaks = ['reviews_per_month','number_of_reviews']\nX2 = X.drop(potential_leaks, axis=1)\n\n# Evaluate the model with leaky predictors removed\ncv_scores = cross_val_score(my_pipeline, X2, y, \n                            cv=5,\n                            scoring='accuracy')\n\nprint(\"Cross-val accuracy: %f\" % cv_scores.mean())","177fbbcd":"from sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X2, y, random_state=1)","314e4f4c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogreg = LogisticRegression()\nlogreg.fit(train_X, train_y)","fe35684b":"y_pred = logreg.predict(val_X)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(val_X, val_y)))","1493ae26":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)","6a58965b":"for max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","805df97f":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# function for comparing different approaches\ndef score_dataset(train_X, val_X, train_y, val_y):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(train_X, train_y)\n    preds = model.predict(val_X)\n    return mean_absolute_error(val_y, preds)","58ecb297":"print(\"Mean Absolute error of the Model:\")\nprint(score_dataset(train_X, val_X, train_y, val_y))","9e0f266e":"from xgboost import XGBRegressor\n\nmy_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\nmy_model.fit(train_X, train_y, \n             early_stopping_rounds=5, \n             eval_set=[(val_X, val_y)], \n             verbose=False)\npredictions = my_model.predict(val_X)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, val_y)))","4ecd05e8":"#Code forked from: https:\/\/www.kaggle.com\/biphili\/hospitality-in-era-of-airbnb\nerror_airbnb = pd.DataFrame({\n        'Actual Values': np.array(val_y).round(),\n        'Predicted Values': predictions.round()}).head(20)\n\nerror_airbnb.head(5)","83b31806":"**Inference**\n* All the numerical factors are having **right skewed distribution**\n* **Price factor** has an unstable distribution \n* Both the **review factors** distribution are similar as they are dependent\n\nWe need to preprocess these data before sending into the model","b89c78ae":"**Inference**\n* There are a **total of 557 reviews by top customers**\n* **322 ** are from Brooklyn followed by **178** in Queens\n* Manhattan has **44** top customer reviews","d9c703aa":"## 5c. Distribution of prices across location- Box Plot","09adb521":"### Top customers(Reviews-50 plus)- Marker location plot\n\nThe top customers are the one who engages with airbnb. We have consider number of reviews given by the customers should be **above 50 as a criteria**","8c122edb":"**Inference**\n* The average prices of Brooklyn,Queens and Statent Island is more are less the same which varies around **75-100 dollars **\n* The prices are really high in Manhattan as the average price comes around **150 dollars** and maximum reaches near **350 dollars**\n* The prices in Bronx are comparatively low as it average price is around **60 dollars.**","703709fe":"## 5e. Best Customers-Geopatial analysis\n\nNow let's dive into the map of New York with a better visualization feature using **folium librar**y. **Thanks to Kaggle course** on [Geospatial analysis](https:\/\/www.kaggle.com\/alexisbcook\/interactive-maps#Choropleth-maps) for helping me get these amazing geographical visuals","3ac928e7":"This looks neat with all the **standardized scale**. Now let's jump into **Machine Learning !!**","570b9994":"## 7h. XGBoost\nXGBoost is an algorithm that has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data. XGBoost is an implementation of gradient boosted decision trees designed for speed and performance","7386c3fb":"## 7f. Decision Tree Regressor\nDecision Tree is a decision-making tool that uses a flowchart-like tree structure or is a model of decisions and all of their possible results, including outcomes, input costs and utility.Decision-tree algorithm falls under the category of supervised learning algorithms. It works for both continuous as well as categorical output variables. Now we are creating the function directly to get Mean Absolute error to fasten up our coding","6ecf0ad1":"**Inference**\n* There are very less number of people who prefers shared room,and in **Staten Island many prefers Entire home\/apartment**\n* The price range in **Manhattan are pretty high** whereas the prices in **Staten Island are low** even most of them prefer Entire home\/ apartment \n* **Brooklyn,Queens and Bronx** are being listed many number of days for airbnb space in a year than rest of the location","3aba6e52":"### Removing outliers\nLet's remove those outliers using **IQR** method","afd4774c":"## 5b. Standardizing our dataset + Setting Feature(X) and Target(y)","5014d7d0":"## 7g. Random Forest Regressor\nA Random Forest is an ensemble technique capable of performing both regression and classification tasks with the use of multiple decision trees and a technique called Bootstrap Aggregation, commonly known as bagging. The basic idea behind this is to combine multiple decision trees in determining the final output rather than relying on individual decision trees.","54e252b1":"## 3b.Importing Visualization and ML Libraries\nIt is important for an analysis to have data visualization and develop machine learning models to get accurate prediction. Here we are going use sklearn and matplotlib for machine learning and plotting respectively","bdbfb59f":"## Before we Begin:\nIf you liked my work, please upvote this kernel since it will keep me motivated to perform more in-depth reserach towards further datasets and produce more accurate models","638d2f8e":"Sweeet! let's standardize our dataframe ","a7a41865":"## 7c.Removing Data Leakage factors\nNow let us try to identify possible Data Leakage feature and remove it. If we look into reviews per month and number of reviews, it **does not help us predict the price as this feature will occur only after the purchase(stay) gets over**. So, let's remove this feature and check for accuracy","6bbea3ae":"**Inference:**\n* Number of reviews and reviews per month have strong positive correlation and it is very obvious\n* Availability_365 and minimum nights have a small positive correlation(0.2) which shows that whenever the list was displayed people stayed there, which is also obvious\n* Price and Calculated list hosting have a small positive correlation(0.2) which shows that price has been increased when more number of lists have been shown to the host\n\n","4f0473cb":"## 5d. Geographical analysis- Scatter Plot\nNow let's visualise our data according to the geographical locations.We use latitude and longitude data for this plots. The first plot shows the places in New York which can be used as our reference","8aabc2ea":"**8% accuracy ** for our Logistic Regression model. Pretty bad","f28e1ddf":"# 8. Actual Vs Predicted\nLet's look how our model has predicted,by comparing actual and predicted values","3bcd6fad":"# 2.Kernel Goals\nThere are three primary goals of this kernel.\n\n1. Do a **exploratory analysis** of the Airbnb dataset\n2. Do an **visualization analysis** of the Airbnb dataset\n3. **Prediction: ** To predict the price of NYC Airbnb rentals based on the data provided","ce4352b8":"The majority of the outliers has been removed ! Great","0625df0e":"# 4. Data Wrangling\n## 4a. Handling mis_ing values\nFirst lets focus on the missing data in review features,if we drop the rows which has null values we might sabotage some  potential information from the dataset. I guess we have to **impute values **into the NaN records which leads us to accurate models. If you see the **last_review column** has dates, I don't think it adds much value to our analysis so we can drop that column as well and we will impute the null values in reviews_per_month with **zero**","ee4b6d3b":"**MAE of 37.49** . Now lets boost our model using XGBoost","bbfce9d1":"**MAE of 35.59** ! That's an improvement","d2f96512":"**Inference**\n* There are lot of top customers who have given 50-58 reviews (**max no. of reviews is 58 in this dataset**)\n* The top customers located in** Staten Island and near seaside are low**","b28419e2":"As you see the white lines, there are lot many missing\/unentered values in two of the **review features**. It could be due to the host neglecting to provide review for the stay. Since this is a large record of data we can't completely see how many missing values we have visually, it's better to count the null values for all features","51b081c9":"## 5b. Numerical factors-Distribution plot","5d71a3e3":"After loading the dataset we can see a number of things.Our dataset is a mixture of integer, float and object\/categorical. We have **7 integer data,6 categorical data,3 float data**","d6e177b5":"Yayy ! we have cleared that reviews_per_month with **zero null values**. Now it's time to drop unwanted features !","faa70289":"**From airbnb dataset we can infer**\n* We have a **good data on location** of space (lat long, area name and neighbourhood)\n* We can except **missing values** in this dataset\n* **Price feature** should be our target variable for prediction\n* **Id, name and host name** features plays a little role in our analysis, we can drop it later ","3dc11253":"### Heatmap of our data:\nLet's look at which location is our data more collected from. Its shows the number of customers in each location as per our dataset","2e2bfc91":"## 3d. Examining the dataset","dcd1597c":"**OOPS!**, our target dataframe **turned into an array**, let's turn it into a dataframe again ","8cb3055c":"## 6a. One hot encoding- Neighbourhood and Room type\nWe can't preprocess the dataframe which has categorical data, so let's get some dummies instead of them","1a84917c":"The error dips and then rises when the leaf nodes increases. This model is getting pretty bad ","af290762":"### Clustering Markers- better view\n\n**Please do zoom in and out and touch those markers to understand the review splitup based on location**","214a637d":"# 5.Data Visualization\n## 5a.Count of categorical Variable-Bar Chart","46060bec":"# 1. Introduction\n![airbnb_logo_detail.jpg](attachment:airbnb_logo_detail.jpg)\n**Airbnb Inc**. is an online marketplace for arranging or offering lodging, primarily homestays, or tourism experiences. The company does not own any of the real estate listings, nor does it host events; it acts as a broker, receiving commissions from each booking.The company is based in San Francisco, California, United States.\n\nThe company was conceived after its founders put an air mattress in their living room, effectively turning their apartment into a bed and breakfast, in order to offset the high cost of rent in San Francisco; Airbnb is a shortened version of its original name, AirBedandBreakfast.com. Source: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Airbnb)\n\n**Context of our Dataset:**\nSince 2008, guests and hosts have used Airbnb to expand on traveling possibilities and present more unique, personalized way of experiencing the world. This dataset describes the listing activity and metrics in NYC, NY for 2019.","a8a58b5c":"## 7b.Cross Validation-MAE + Accuracy\nNow let's send our target and features for **cross validation**","d9d26380":"## Hit upvote if you like my work and also check out my [other notebooks](https:\/\/www.kaggle.com\/benroshan\/notebooks)","68116ffe":"# 6.Preprocessing data for ML model prediction \nFirst let's consider the feature inputs we will be using for the prediction,Let's consider **neighbourhood group, room type,minimum nights,availability_365, number of reviews,reviews per month and calculated_host_listings_count**. Let's process these feature inputs!","0a543203":"Woof! We have nearly **48895** records and **16** factors","88ad61c6":"## 3e.Checking for missing data\nDatasets in the real world are often messy, However, this dataset is almost clean and simple. Lets analyze and see what we have here.","7b2a9130":"# 3. Importing libraries and exploring Data\n## 3a.Importing Libraries\nPython is a fantastic language with a vibrant community that produces many amazing libraries. I am not a big fan of importing everything at once for the newcomers. So, I am going to introduce a few necessary libraries for now, and as we go on, we will keep unboxing new libraries when it seems appropriate","9845c751":"## 7d. Splitting into training and test dataset\nNow let's split our dataset into training and test\/validation data for both feature and label variables","a27ee9d9":"# Conclusion\n### For now, I can conclude that our model accuracy is pretty bad. I guess it is because of overfitting. I have checked with other[ top performers notebook of this dataset](https:\/\/www.kaggle.com\/dgomonov\/new-york-city-airbnb-open-data\/kernels) and they too have high MAE and low accuracy in their models. If I can do anything to improve my model, Feel free to drop your suggestions in comments. ","c6c0dd27":"**4.7 % accuracy!!** That's pretty bad. ","5c78a520":"# 7.Machine Learning models\n## 7a.Pipeline\nNow before splitting into training and test data, let's run through some **cross validation using pipeline **","dea1a6e7":"## 7e. Logistic Regression model and accuracy\nLogistic regression is a supervised learning classification algorithm used to predict the probability of a target variable. The nature of target or dependent variable is dichotomous, which means there would be only two possible classes. Mathematically, a logistic regression model predicts P(Y=1) as a function of X.","e54861f2":"**Inference**\n* In **neighbourhood group(location)**,Manhattan and Brooklyn stands in the top 2 positions respectively owing to the size and number of people who booked a room and it is followed by Queens, Bronx and Staten Island\n* The top 10 of **neighbourhood(area)** is displayed second and Williamsburg stands top with most no. of books of nearly 3000\n* Most of the **Entire home** were booked than private and shared room type\n* host id **219517861** has the most number of books and he\/she is the best customer of Airbnb in the NY","a3892ff5":"## 3c.Extracting data","47e5f8aa":"**Inference**\n* Most of our data we got is from most of Queens and some parts of Brooklyn shown in red\n* The purple areas are the lighter region where the data is available less, which means there are less customers\/space in those regions","c189c84a":"**MAE=39.04!!!** Thats a huge error.Now, let's try with **pipepline ** and measure accuracy","4c5755ec":"oof! except **availability_365** all other numerical factors have huge amount of outliers,as from my experience it's better to remove these outliers because it will significantly disrupt our statistical analysis.","9dc85fcf":"**Inference**\n* We have null data in **name feature** which we less likely to use for our data\n* There are nearly **10052 null records** in last_review and reviews_per_month column\n","75bea8a6":"### For different leaf nodes\nLet's check MAE at each leaf nodes of our decision tree","e91203d0":"**5.9% accuracy**. That's an improvement but that isn't close to being called as a good model.Do you have any suggestions on improving this model accuracy ? Should I have considered some other features? Drop some comments and help me","3fe4fc58":"## 4b. ....... Outliers\nOutliers are unusual values in your dataset, and they can distort statistical analyses and violate their assumptions. Unfortunately, all analysts will confront outliers and be forced to make decisions about what to do with them. Given the problems they can cause, you might think that it\u2019s best to remove them from your data. But, that\u2019s not always the case. Removing outliers is legitimate only for specific reasons.Outliers can be very informative about the subject-area and data collection process. It\u2019s essential to understand how outliers occur and whether they might happen again as a normal part of the process or study area. Unfortunately, resisting the temptation to remove outliers inappropriately can be difficult. Outliers increase the variability in your data, which decreases statistical power. Consequently, excluding outliers can cause your results to become statistically significant. **In our case, let's first visualize our data and decide on what to do with the outliers**","df2698ec":"**Inference**\n* Very less(**2.7%**) people prefer shared room, may be people are not comfortable to stay with strangers in NY\n* More than 8000 people in Manhattan prefer **Entire home\/ apartment **\n* Nearly around 8000 people in Brooklyn prefer **private room**\n* People in **Staten island** seldom book through airbnb\n"}}