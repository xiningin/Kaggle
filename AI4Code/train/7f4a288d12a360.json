{"cell_type":{"aeb4b757":"code","a04bb9be":"code","3cd2fb31":"code","2e8a32a9":"code","c5287501":"code","c1b9f93b":"code","ec0da6c5":"code","a335442d":"code","d435c0bd":"code","ded28226":"code","8bcec0d2":"code","5c00512e":"code","87cfb5da":"code","4555006e":"code","5f519e49":"code","9eeb5004":"code","3abd0c66":"code","6b8c846e":"code","645436d4":"code","01c60672":"code","50e52c78":"code","276eaa9d":"code","b2274c50":"code","235d8da9":"markdown","01fe9bf3":"markdown","17cd5402":"markdown","11ad644e":"markdown","a1e42eb4":"markdown","c40a595f":"markdown","aafb2295":"markdown","133a5dcc":"markdown","cd8c7567":"markdown"},"source":{"aeb4b757":"# Importing Libraries\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.regularizers import l2, l1\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nimport random\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.metrics import confusion_matrix, roc_curve","a04bb9be":"# Importing Data\ndata = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\n\n# Printing Data Info\nprint(data.info())","3cd2fb31":"# Co-relation Matrix\nfig, ax = plt.subplots(figsize=(15,10))\nsns.heatmap(data.corr(), cmap='Blues', annot=True, ax=ax)","2e8a32a9":"# Since 'caa', 'cp', 'restecg' are categories rather than just integers\n# We divide them into categories\ndummies = pd.get_dummies(pd.DataFrame(data[['caa', 'cp', 'restecg']], dtype='object'))\ndummies.head()","c5287501":"# Considering all the features except 'caa', 'cp', and 'restecg'\nfeatures = ['age', 'sex', 'trtbps','thalachh', 'exng', 'oldpeak', 'slp', 'thall', 'output']\ndata = data[features]\ndata.head()","c1b9f93b":"# Joining Number Data with the Categorical Data\ndata = pd.concat([data, dummies], axis=1)\ndata.head()","ec0da6c5":"# We can observe outliers on 'oldpeak' column. \n# Outliers must be handled to avoid disruption in prediction accuracy.\ndef outliers(dataCol):\n    # sorting column\n    sorted(dataCol)\n    \n    # Interquartile Range\n    Q1,Q3 = np.percentile(dataCol,[25,75])    \n    IQR = Q3-Q1\n    \n    # Lower Range Error\n    LowerRange = Q1-(1.5 * IQR)\n    \n    # Upper Range Error\n    UpperRange = Q3+(1.5 * IQR)\n    \n    return LowerRange,UpperRange","a335442d":"# Outliers in OldPeak, Denoted by dots\nsns.boxplot(data=data, x='oldpeak')","d435c0bd":"# We get the limits of the column\nlwoldpeak,upoldpeak = outliers(data['oldpeak'])\n\n# Limiting the column values between lwoldpeak and upoldpeak\ndata['oldpeak'].replace(list(data[data['oldpeak'] < lwoldpeak].oldpeak) ,lwoldpeak,inplace=True)\ndata['oldpeak'].replace(list(data[data['oldpeak'] > upoldpeak].oldpeak) ,upoldpeak,inplace=True)","ded28226":"# Outliers Managed \nsns.boxplot(data=data, x='oldpeak')","8bcec0d2":"# Splitting Data into Postive and Negative Heart Attack cases.\n# The dataset has 'output'= 1 for all the top rows and 'output'= 0 for bottom rows\n\ndata_1 = data[data['output']==0]\ndata_2 = data[data['output']==1]\ndata_1.shape, data_2.shape","5c00512e":"# Adding rows from both the true and false predictions with good 1:0 split \ntrain_data = pd.concat([data_1.iloc[:125,:], data_2.iloc[:152, :]], ignore_index=True)\ntest_data = pd.concat([data_1.iloc[125:,:], data_2.iloc[152:, :]], ignore_index=True)\n\ntrain_data.shape, test_data.shape","87cfb5da":"# Shuffling the data to mix up the features\ntrain_data = shuffle(train_data)\ntest_data = shuffle(test_data)","4555006e":"sns.countplot(data = train_data, x='output')","5f519e49":"sns.countplot(data = test_data, x='output')","9eeb5004":"# Splitting Labels into Training and Validation\ntrain_labels = np.array(train_data['output'])\ntest_labels = np.array(test_data['output'])","3abd0c66":"# Splitting Data into Training and Validation\ntrain_features = np.array(train_data.iloc[:, :-1])\ntest_features = np.array(test_data.iloc[:, :-1])","6b8c846e":"# Printing out data shapes\ntrain_features.shape, train_labels.shape, test_features.shape, test_labels.shape","645436d4":"def initialize_weights(shape, dtype=None):\n    \n    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)\n\ndef initialize_bias(shape, dtype=None):\n    \n    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)\n\ndef DeepLearningModel(input_shape):\n    model = Sequential()\n    \n    model.add(Input(input_shape))\n    \n    model.add(Dense(16, activation='relu', kernel_initializer=initialize_weights, bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n    \n    model.add(Dense(32, activation='relu', kernel_initializer=initialize_weights, bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n    \n    model.add(Dense(64, activation='relu', kernel_initializer=initialize_weights, bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n    \n    model.add(Dense(1, activation='sigmoid', kernel_initializer=initialize_weights, bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))    \n    \n    return model","01c60672":"# Hyperparameters\n\n# These hyperparameters can be modified to get different output.\n# I found these to work well for the data. Play around and let me know in comments\n\nmodel = DeepLearningModel((20))\nprint(model.summary())\nlr = 0.003\nepochs = 40\nbatch_size = 100\noptimizer = Adam(lr)\n\n# You can try different Keras Error for Different Prediction Values.\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\nhistory = model.fit(train_features, train_labels, epochs=epochs, batch_size=batch_size)","50e52c78":"# Plotting Accuracy and Loss against Number of Epochs\nsns.lineplot(x=history.epoch, y=history.history['accuracy'])\nsns.lineplot(x=history.epoch, y=history.history['loss'])","276eaa9d":"# Evaluating on Validation Dataset\nmodel.evaluate(test_features, test_labels)","b2274c50":"# Plotting Confusion Matrix for better understanding of Model Performance on Validation\nroc_act = test_labels\nroc_pred = list(map(int, np.round(model.predict(test_features))))\nlabels = ['True Neg','False Pos','False Neg','True Pos']\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(confusion_matrix(roc_pred, roc_act), cmap='Blues', annot=True, fmt='')","235d8da9":"## Feature Engineering","01fe9bf3":"## Model Development","17cd5402":"## Training and Testing Dataset Spilt","11ad644e":"## Model Evaluation using Test Dataset","a1e42eb4":"## Confusion Matrix for Test Dataset","c40a595f":"## Data Handling","aafb2295":"## About this dataset\n\n### Stuff we know:\n\n* Age : Age of the patient\n\n* Sex : Sex of the patient\n\n* exng: exercise induced angina (1 = yes; 0 = no)\n\n* caa: number of major vessels (0-3)\n\n* cp : Chest Pain type chest pain type\n\n    * Value 1: typical angina\n    * Value 2: atypical angina\n    * Value 3: non-anginal pain\n    * Value 4: asymptomatic\n\n* trtbps : resting blood pressure (in mm Hg)\n\n* chol : cholestoral in mg\/dl fetched via BMI sensor\n\n* fbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\n* restecg : resting electrocardiographic results\n\n    * Value 0: normal\n    * Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n    * Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n\n* thalach : maximum heart rate achieved\n\n### Stuff we should predict\n* target : \n    * 0 = less chance of heart attack \n    * 1 = more chance of heart attack","133a5dcc":"## 96% Test Accuracy using DNN Model for Heart Attack Prediction.","cd8c7567":"## Plotting Performance over Training Set"}}