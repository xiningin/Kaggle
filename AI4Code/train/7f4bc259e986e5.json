{"cell_type":{"8c16d2de":"code","1c04db84":"code","7ea6e3e0":"code","d4202bc8":"code","5cdbd785":"code","1fce6b04":"code","5ac3c99f":"code","18169c31":"code","9614ad37":"code","ef3a5d20":"code","84d03586":"code","16162dd3":"code","f2db8217":"code","dbfafce5":"code","3cb718f5":"code","ad7b0164":"code","e2bc115e":"code","755d7cb1":"code","3780b688":"code","685be28d":"code","c0ae42c5":"code","2235f879":"code","af65aa2a":"code","798ea478":"code","6ee44562":"code","edc23df8":"code","04f1b402":"code","5573334e":"code","3df33294":"code","e2afa411":"code","6e683db1":"code","4693799d":"code","168af331":"code","32610075":"code","430194cc":"code","6cb559df":"code","9220ab21":"code","ae1c3fc0":"code","886be41f":"code","b22f1189":"code","b2c80fd8":"code","263e4cf3":"code","aec845cb":"code","cf4177e0":"code","b3d3de3d":"code","12c821f8":"code","3ac00496":"code","f5e652da":"code","1e88332f":"code","f8209a83":"code","f9222866":"code","00e73d67":"code","6f424dcf":"code","762bd8ff":"code","4c12f3ee":"markdown","d1300217":"markdown","dde185c6":"markdown","e028806f":"markdown","52b566fd":"markdown","74412929":"markdown","08b18708":"markdown","4e8b084f":"markdown","375b9422":"markdown","22740234":"markdown","0d742c2e":"markdown","923f313d":"markdown","8476e2fd":"markdown","b1aaeec9":"markdown","6f8fd17a":"markdown","9d1f7dd1":"markdown","94fa5636":"markdown","8cb0d694":"markdown","2f1d489f":"markdown","5f090638":"markdown"},"source":{"8c16d2de":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, recall_score, precision_score, f1_score, roc_auc_score, roc_curve\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense","1c04db84":"df = pd.read_csv(\"..\/input\/adult-census-income\/adult.csv\")","7ea6e3e0":"df.head(3)","d4202bc8":"df.info()","5cdbd785":"df.describe()","1fce6b04":"df.columns","5ac3c99f":"# Definition of the columns that will be features (note that the column 'clientid' is not present)\nfeatures = [\n    'age', 'workclass', 'fnlwgt', 'education', 'education.num',\n    'marital.status', 'occupation', 'relationship', 'race', 'sex',\n    'capital.gain', 'capital.loss', 'hours.per.week', 'native.country'\n]\n\n# Preparation of arguments for ``scikit-learn`` library methods\nX = df[features].values","18169c31":"from sklearn.preprocessing import LabelEncoder\n\nlbp = LabelEncoder()","9614ad37":"# Part of transforming categorical to integer\n\nX[:, 1] = lbp.fit_transform(X[:, 1])\n\nX[:, 3] = lbp.fit_transform(X[:, 3])\n\nX[:, 5] = lbp.fit_transform(X[:, 5])\n\nX[:, 6] = lbp.fit_transform(X[:, 6])\n\nX[:, 7] = lbp.fit_transform(X[:, 7])\nX\nX[:, 8] = lbp.fit_transform(X[:, 8])\n\nX[:, 9] = lbp.fit_transform(X[:, 9])\n\nX[:, 13] = lbp.fit_transform(X[:, 13])","ef3a5d20":"df.head(3)","84d03586":"X[0:3]","16162dd3":"# converting the Label to a numeric format for testing later...\nLE = LabelEncoder()\n\ny = LE.fit_transform(df[\"income\"])","f2db8217":"scaler = StandardScaler()","dbfafce5":"X = scaler.fit_transform(X)\nX","3cb718f5":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","ad7b0164":"nb = GaussianNB()","e2bc115e":"nb.fit(X_train, y_train)\n\ny_pred_nb = nb.predict(X_test)","755d7cb1":"y_pred_nb = nb.predict(X_test)\n\naccuracy_nb = accuracy_score(y_test, y_pred_nb)\nrecall_nb = recall_score(y_test, y_pred_nb)\nprecision_nb = precision_score(y_test, y_pred_nb)\nf1_nb = f1_score(y_test, y_pred_nb)\nroc_nb = roc_auc_score(y_test, y_pred_nb)","3780b688":"print(classification_report(y_test, y_pred_nb))","685be28d":"sns.heatmap(confusion_matrix(y_test, y_pred_nb),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","c0ae42c5":"dt = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)","2235f879":"dt.fit(X_train, y_train)","af65aa2a":"y_pred_dt = dt.predict(X_test)\n\naccuracy_dt = accuracy_score(y_test, y_pred_dt)\nrecall_dt = recall_score(y_test, y_pred_dt)\nprecision_dt = precision_score(y_test, y_pred_dt)\nf1_dt = f1_score(y_test, y_pred_dt)\nroc_dt = roc_auc_score(y_test, y_pred_dt)","798ea478":"print(classification_report(y_test, y_pred_dt))","6ee44562":"sns.heatmap(confusion_matrix(y_test, y_pred_dt),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","edc23df8":"rf = RandomForestClassifier(n_estimators = 40, criterion= 'entropy', random_state= 0)","04f1b402":"rf.fit(X_train, y_train)","5573334e":"y_pred_rf = rf.predict(X_test)\n\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\nrecall_rf = recall_score(y_test, y_pred_rf)\nprecision_rf = precision_score(y_test, y_pred_rf)\nf1_rf = f1_score(y_test, y_pred_rf)\nroc_rf = roc_auc_score(y_test, y_pred_rf)","3df33294":"print(classification_report(y_test, y_pred_rf))","e2afa411":"sns.heatmap(confusion_matrix(y_test, y_pred_rf),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","6e683db1":"knn = KNeighborsClassifier(n_neighbors=5, metric = 'minkowski', p = 2)","4693799d":"knn.fit(X_train, y_train)","168af331":"y_pred_knn = knn.predict(X_test)\n\naccuracy_knn = accuracy_score(y_test, y_pred_knn)\nrecall_knn = recall_score(y_test, y_pred_knn)\nprecision_knn = precision_score(y_test, y_pred_knn)\nf1_knn = f1_score(y_test, y_pred_knn)\nroc_knn = roc_auc_score(y_test, y_pred_knn)","32610075":"print(classification_report(y_test, y_pred_knn))","430194cc":"sns.heatmap(confusion_matrix(y_test, y_pred_dt),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","6cb559df":"rl = LogisticRegression(random_state=0)","9220ab21":"rl.fit(X_test, y_test)","ae1c3fc0":"y_pred_rl = rl.predict(X_test)\n\naccuracy_rl = accuracy_score(y_test, y_pred_rl)\nrecall_rl = recall_score(y_test, y_pred_rl)\nprecision_rl = precision_score(y_test, y_pred_rl)\nf1_rl = f1_score(y_test, y_pred_rl)\nroc_rl = roc_auc_score(y_test, y_pred_rl)","886be41f":"print(classification_report(y_test, y_pred_rl))","b22f1189":"sns.heatmap(confusion_matrix(y_test, y_pred_rl),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","b2c80fd8":"svm = SVC(kernel = 'linear', random_state=0)","263e4cf3":"svm.fit(X_train, y_train)","aec845cb":"y_pred_svm = svm.predict(X_test)\n\naccuracy_svm = accuracy_score(y_test, y_pred_svm)\nrecall_svm = recall_score(y_test, y_pred_svm)\nprecision_svm = precision_score(y_test, y_pred_svm)\nf1_svm = f1_score(y_test, y_pred_svm)\nroc_svm = roc_auc_score(y_test, y_pred_svm)","cf4177e0":"print(classification_report(y_test, y_pred_svm))","b3d3de3d":"sns.heatmap(confusion_matrix(y_test, y_pred_svm),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","12c821f8":"rn = MLPClassifier(verbose = True, max_iter= 250, tol = 0.000010)","3ac00496":"rn.fit(X_train, y_train)","f5e652da":"y_pred_rn = rn.predict(X_test)\n\naccuracy_rn = accuracy_score(y_test, y_pred_rn)\nrecall_rn = recall_score(y_test, y_pred_rn)\nprecision_rn = precision_score(y_test, y_pred_rn)\nf1_rn = f1_score(y_test, y_pred_rn)\nroc_rn = roc_auc_score(y_test, y_pred_rn)","1e88332f":"print(classification_report(y_test, y_pred_rn))","f8209a83":"sns.heatmap(confusion_matrix(y_test, y_pred_rn),annot = True, fmt = 'd')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\nplt.show()","f9222866":"models = [('Naive Bayes', accuracy_nb, recall_nb, precision_nb, f1_nb, roc_nb),\n          ('Decision Tree', accuracy_dt, recall_dt, precision_dt, f1_dt, roc_dt),\n          ('Random Forest', accuracy_rf, recall_rf, precision_rf, f1_rf, roc_rf),\n          ('kNN', accuracy_knn, recall_knn, precision_knn, f1_knn, roc_knn),\n          ('Logistic Regression', accuracy_rl, recall_rl, precision_rl, f1_rl, roc_rl),\n          ('SVM', accuracy_svm, recall_svm, precision_svm, f1_svm, roc_svm),\n          ('Neural Networks', accuracy_rn, recall_rn, precision_rn, f1_rn, roc_rn)]\n\ndf_all_models = pd.DataFrame(models, columns = ['Model', 'Accuracy (%)', 'Recall (%)', 'Precision (%)', 'F1 (%)', 'AUC'])\n\ndf_all_models","00e73d67":"plt.style.use(\"dark_background\")\n\nplt.subplots(figsize=(12, 10))\nsns.barplot(y = df_all_models['Accuracy (%)'], x = df_all_models['Model'], palette = 'icefire')\nplt.xlabel(\"Models\")\nplt.title('Accuracy')\nplt.show()","6f424dcf":"r_probs = [0 for _ in range(len(y_test))]\nr_auc = roc_auc_score(y_test, r_probs)\nr_fpr, r_tpr, _ = roc_curve(y_test, r_probs)\n\nfpr_nb, tpr_nb, _ = roc_curve(y_test, y_pred_nb)\nfpr_dt, tpr_dt, _ = roc_curve(y_test, y_pred_dt)\nfpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)\nfpr_knn, tpr_knn, _ = roc_curve(y_test, y_pred_knn)\nfpr_rl, tpr_rl, _ = roc_curve(y_test, y_pred_rl)\nfpr_svm, tpr_svm, _ = roc_curve(y_test, y_pred_svm)\nfpr_rn, tpr_rn, _ = roc_curve(y_test, y_pred_rn)","762bd8ff":"sns.set_style('darkgrid')\n\nplt.plot(r_fpr, r_tpr, linestyle='--', label='Random prediction (AUROC = %0.3f)' % r_auc)\n\nplt.plot(fpr_nb, tpr_nb, marker='.', label='Naive Bayes (AUROC = %0.3f)' % roc_nb)\nplt.plot(fpr_dt, tpr_dt, marker='.', label='Decision Tree (AUROC = %0.3f)' % roc_dt)\nplt.plot(fpr_rf, tpr_rf, marker='.', label='Random Forest (AUROC = %0.3f)' % roc_rf)\nplt.plot(fpr_knn, tpr_knn, marker='.', label='kNN (AUROC = %0.3f)' % roc_knn)\nplt.plot(fpr_rl, tpr_rl, marker='.', label='Logistic Regression (AUROC = %0.3f)' % roc_rl)\nplt.plot(fpr_svm, tpr_svm, marker='.', label='SVM (AUROC = %0.3f)' % roc_svm)\nplt.plot(fpr_rn, tpr_rn, marker='.', label='Neural Networks (AUROC = %0.3f)' % roc_rn)\n\nplt.title('ROC Plot')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend() \nplt.show()","4c12f3ee":"###### Defining the features of our model\nFor this, we will create the variable X that will receive the characteristic variables of our model, and the variable y that will receive the target variable of our model.\n\nWe will also remove the 'clientid' columns that will not be relevant in our model.","d1300217":"## 3. Dividing into training and testing sets\nNow we need to convert our data into training and testing sets. We will use 75% as our training data and test our model on the remaining 25% with Scikit-learn's train_test_split function.","dde185c6":"###### 5. Logistic Regression\nLogistic regression algorithm is used where a discrete output is expected, (eg Predict whether a user is a good or bad payer). Typically, logistic regression uses some function to squeeze values into a given range.","e028806f":"###### 4. kNN\nThe KNN or k-nearest neighbor algorithm is a very simple machine learning algorithm. It uses some sort of similarity measure to tell which class the new data falls into, in which case we'll use 5 nearest neighbors.","52b566fd":"## Detection of Adult Census Income with Machine Learning & Deep Learning\n![census.PNG](attachment:536303b8-cb98-46d7-af88-50ab092c88ca.PNG)\n###### Dataset information:\n\n- This data was extracted from the [1994 Census bureau database](https:\/\/www.census.gov\/en.html) by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over $50K a year.\n\nThe dataset can be found on the `` Kaggle`` platform at the link below:\n\nhttps:\/\/www.kaggle.com\/uciml\/adult-census-income","74412929":"###### Checking if the data has changed from the first three lines","08b18708":"Note that there are variables of type ``float64`` (\"decimal\" numbers), variables of type ``int64`` (integers) and variables of type ``object`` (in this case they are *strings*, or text) .\n\nSince most supervised statistical learning algorithms only accept numerical values as input, it is necessary then to preprocess variables of type \"object\" before using this dataset as input for training a model.","4e8b084f":"###### Categorical variable handling\n\nAs mentioned before, computers aren't good with \"categorical\" variables (or strings).\n\nGiven a column with categorical variable, what we can do is encoding that column into multiple columns containing binary variables. This process is called \"one-hot-encoding\" or \"dummy encoding\".","375b9422":"###### 6. SVM (Support Vector Machines)\nThe SVM algorithm separates data points using a line. This line is chosen in such a way that it will be the most important of the closest data points in 2 categories.","22740234":"We have 14 columns present in the dataset provided, 13 of which are characteristic variables (input data) and one of them is a target variable (which we want our model to be able to predict).\n\nThe characteristic variables are:\n\n     age - The age of the user\n     Workclass - User Profession\n     final-weight - Final user income\n     education - user education\n     education-num - user education ID\n     marital-status - user's civil status\n     occupation - User occupation\n     relationship - User relationship\n     race - user race\n     Fri - User Gender\n     capital-gain - Capital gained\n     capital-loss - lost capital\n     hour-per-week - Hours per week\n     native-country - hometown\n\nThe target variable is:\n\n     income - a *binary* type that indicates the user's income:\n             <=50k - User with income less than or equal to 50000\n              >50k - User with income over 50000","0d742c2e":"###### Scaling of numerical data\nAs we can see in the data there is a big difference between high numbers and low numbers, so we must scale the data to keep them on the same scale.","923f313d":"## 1. Imports from libraries","8476e2fd":"## 5. Viewing the results of all models","b1aaeec9":"## 4. Creation of models","6f8fd17a":"The ``describe()`` function generates a lot of information about numeric variables that can also be useful:","9d1f7dd1":"###### 2. Decision Tree\nThe Decision Tree algorithm are statistical models that use supervised training for data classification and prediction. These models use the divide-and-conquer strategy: a complex problem is decomposed into simpler sub-problems and recursively this technique is applied to each sub. -problem","94fa5636":"###### 1. Naive Bayes\nThe Naive Bayes algorithm is a simple classification algorithm that uses historical data to predict the classification of new data. It works by calculating the probability of an event occurring given that another event has already occurred.","8cb0d694":"###### 3. Random Forest\nThe Random Forest algorithm creates a forest in a random way, creating several decision trees and combining them, each tree tries to estimate a ranking and this is called as \u201cvote\u201d, thus to obtain a more accurate and more stable prediction.","2f1d489f":"###### 7. Neural networks\nThe purpose of the Neural Networks algorithm is to imitate the nervous system of humans in the learning process, it is inspired by biological neural networks","5f090638":"## 2. Starting..."}}