{"cell_type":{"42155bd0":"code","0925de6e":"code","63d718cb":"code","47fe3c7f":"code","2ae35987":"code","442901c4":"code","ccf01569":"code","611fe186":"markdown","a3183507":"markdown","2b68dff9":"markdown"},"source":{"42155bd0":"# Importing required Packages\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stat\n%matplotlib notebook\nimport matplotlib.pyplot as plt\nimport random\nimport time","0925de6e":"data = pd.read_csv(\"..\/input\/SIMPLEPENDULUMOSCILLATIONDATA.txt\", sep=\" \", header=None, names=['l', 't'])\nprint(data.head())\nprint(data.tail())","63d718cb":"l = data['l'].values\nt = data['t'].values\ntsq = t * t","47fe3c7f":"def next_step(x, y, m, c, eta):\n    ycalc = m * x + c\n    error = (y - ycalc) ** 2\n    delta_m = -(y - ycalc) * x\n    delta_c = -(y - ycalc)\n    m = m - delta_m * eta\n    c = c - delta_c * eta\n    return m, c, error\n\ndef one_loop_random(x, y, m, c, eta):\n    # Making random idx\n    random_idx = np.arange(len(y))\n    np.random.shuffle(random_idx)\n    # Training with random idx\n    for idx in random_idx:\n        m, c, e = next_step(x[idx], y[idx], m, c, eta)\n        #print(m, c, e)\n    return m,c,e\n\ndef train_stochastic(x, y, m, c, eta, iterations=1000):\n    for iteration in range(iterations):\n        m, c, err = one_loop_random(x, y, m, c, eta)\n    return m, c, err","2ae35987":"# Init m, c\nm, c = 0, 0\n\n# Learning rate\nlr = 0.001","442901c4":"# Training for 1000 iterations, plotting after every 100 iterations:\nfig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot(111)\nplt.ion()\nfig.show()\nfig.canvas.draw()\n\nfor num in range(10):\n    m, c, error = train_stochastic(l, tsq, m, c, lr, iterations=100)\n    print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m, c, error))\n    y = m * l + c\n    ax.clear()\n    ax.plot(l, tsq, '.k')\n    ax.plot(l, y)\n    fig.canvas.draw()\n    time.sleep(1)","ccf01569":"ms, cs,errs = [], [], []\nm, c = 0, 0\neta = 0.001\nfor times in range(200):\n    m, c, error = train_stochastic(l, tsq, m, c, eta, iterations=100) # We will plot the value of for every 100 iterations\n    ms.append(m)\n    cs.append(c)\n    errs.append(error)\nepochs = range(0, 20000,100)\nplt.figure(figsize=(8,5))\nplt.plot(epochs, errs)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Error\")\nplt.title(\"Stochastic Gradient Descent\")\nplt.show()","611fe186":"**Stochastic gradient descent (Single sample)**\n\nInstead of computing the sum of all gradients, stochastic gradient descent selects an observation uniformly at random.\n\n$y = mx + c$\n\n$E$ = $(y_i - y)^2$\n\n$\\frac{\\partial E }{\\partial m}$ = $ -(y_i - (mx_i + c)) * x_i$\n\n$\\frac{\\partial E }{\\partial c}$ = $ -(y_i - (mx_i + c))$","a3183507":"\n## Plotting error vs iterations","2b68dff9":"## PROBLEM\n\nProblem with Sequential Gradient Descent is it does not scale well - it makes the same calculation of gradient descent on each sample sequentially. So the time taken will increase linearly with the number of samples. Many datasets have samples in the range of millions. Hence, even though it gives good results, it is not ideal.\n\nWe need a gradient descent formulation that gives the speed of vanilla gradient descent and the accuracy of sequential\/stochastic gradient descent.\n\nNext up - **Minibatch Gradient Descent!**"}}