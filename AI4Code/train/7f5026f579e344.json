{"cell_type":{"1e9a2eac":"code","3646a0b3":"code","5ae9a3f9":"code","9b455bf1":"code","0c554403":"code","06437f24":"code","e1262332":"code","e2b9a040":"code","f8b69ace":"code","c3e3fdc7":"code","bcba318b":"code","ffbab1f6":"code","0c83f857":"code","e13297cd":"code","5191fa03":"code","29ffc8f7":"code","64f0dea5":"code","aa062e54":"code","00386ba0":"code","4725d639":"code","66e11071":"code","5e472c96":"code","56107932":"code","b5d2347c":"code","2a6a38a5":"code","68abbcb4":"code","316152a4":"code","3131a626":"code","86cc64a0":"code","78c16d74":"code","ba594ea8":"code","5d6ece0b":"code","81fb18a8":"code","028e2f96":"code","31436c76":"code","c444c056":"code","ab98d573":"code","affc5933":"code","9076c28d":"markdown","97570cc5":"markdown","653d116b":"markdown","fe04c007":"markdown","dfa8e596":"markdown","01cb6228":"markdown","a937e451":"markdown","2d16e3c5":"markdown","78e1f353":"markdown","8e33c4d3":"markdown","337fa2f6":"markdown","220a42c1":"markdown","a464edf9":"markdown","6201d94a":"markdown","b8e4a846":"markdown","8f0e526f":"markdown","0e066538":"markdown","abbc9aa8":"markdown","53307592":"markdown","e51f9f98":"markdown","6f47ace4":"markdown","93ee6514":"markdown","295b3bf0":"markdown","c232ad1c":"markdown","5aef9346":"markdown","90c6c442":"markdown","cf49cf97":"markdown","54f3b5a6":"markdown","4d76708a":"markdown","15f8543e":"markdown","c9ec3f11":"markdown","777ae919":"markdown","9b6ef487":"markdown","14603378":"markdown"},"source":{"1e9a2eac":"import tensorflow_datasets as tfds\nimport tensorflow as tf\n\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt","3646a0b3":"code_path='..\/input\/deepcom\/camel_code.txt'\nnl_path='..\/input\/deepcom\/comment.txt'","5ae9a3f9":"def creat_dataset(a,b):\n    # a : code\n    # b: nl\n    with open(a,encoding='utf-8') as tc:\n        lines1=tc.readlines()\n        for i in range(len(lines1)):\n             lines1[i]=\"<start> \"+lines1[i].strip('\\n')+\" <end>\"\n    with open(b,encoding='utf-8') as ts:\n        lines2=ts.readlines()\n        for i in range(len(lines2)):\n            lines2[i]=\"<start> \"+lines2[i].strip('\\n')+\" <end>\"\n    if(len(lines1)!=len(lines2) ):\n        print(\"\u6570\u636e\u91cf\u4e0d\u5339\u914d\")\n    # else:\n        # print(len(lines1))\n    return lines1,lines2\n\ncode,nl=creat_dataset(code_path,nl_path)","9b455bf1":"code_maxlen=200\nnl_maxlen=30\n#vocab_size=30000","0c554403":"def tokenize(lang,maxlen=code_maxlen):\n    # lang:\u6587\u672c\n    # maxlen: \u6700\u5927\u957f\u5ea6  code\u662f200\uff0cnl\u662f30\n    \n    # 1. \u521b\u5efa\u5206\u8bcd\u5668\n    lang_tokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=None,filters='',oov_token=\"<unk>\")\n    # 2. \u5c06\u6587\u672c\u5185\u5bb9\u6dfb\u52a0\u8fdb\u6765\n    lang_tokenizer.fit_on_texts(lang)\n    # 3. \u5c06\u8bcd\u5217\u8868\u8f6c\u6362\u4e3a\u8bcd\u7d22\u5f15\u8868\n    tensor=lang_tokenizer.texts_to_sequences(lang)\n    # \u8fdb\u884c\u672b\u5c3epad,\u5728\u672b\u5c3e\u8fdb\u884c\u622a\u65ad\uff08\u4fdd\u7559\u524d\u9762\u7684\uff09\n    tensor=tf.keras.preprocessing.sequence.pad_sequences(tensor,maxlen=maxlen,padding='post',truncating='post')\n    \n    return tensor,lang_tokenizer","06437f24":"code_tensor,code_lang=tokenize(code,code_maxlen)\nnl_tensor,nl_lang=tokenize(nl,nl_maxlen)","e1262332":"test_code_tensor=code_tensor[:20000]\nval_code_tensor=code_tensor[20000:40000]\ntrain_code_tensor=code_tensor[40000:]\n\ntest_nl_tensor=nl_tensor[:20000]\nval_nl_tensor=nl_tensor[20000:40000]\ntrain_nl_tensor=nl_tensor[40000:]","e2b9a040":"# \u8bcd\u6c47\u6570\u91cf\u7edf\u8ba1 ,\u56e0\u4e3a\u75280\u586b\u5145\uff0c\u6240\u4ee5\u8981+1\nvocab_code_size = len(code_lang.word_index)+1\nvocab_nl_size = len(nl_lang.word_index)+1\nprint(vocab_code_size)\nprint(vocab_nl_size)\n\n# shuffle()\u51fd\u6570\u7684\u53c2\u6570\uff0c\u5b8c\u7f8e\u6d17\u724c\ntrain_buffer_size=len(train_nl_tensor)\ntest_buffer_size=len(test_nl_tensor)\nval_buffer_size=len(val_nl_tensor)\n\nbatch_size=128\n\ntrain_steps_per_epoch=train_buffer_size\/\/batch_size\ntest_steps_per_epoch=test_buffer_size\/\/batch_size\nval_steps_per_epoh=val_buffer_size\/\/batch_size","f8b69ace":"train_dataset=tf.data.Dataset.from_tensor_slices((train_code_tensor,train_nl_tensor)).shuffle(train_buffer_size)\n\ntrain_dataset=train_dataset.batch(batch_size,drop_remainder=True)\n\nprint(train_dataset)","c3e3fdc7":"val_dataset=tf.data.Dataset.from_tensor_slices((val_code_tensor,val_nl_tensor)).shuffle(val_buffer_size)\n\nval_dataset=val_dataset.batch(batch_size,drop_remainder=True)\n\nprint(val_dataset)","bcba318b":"def get_angles(pos, i, d_model):\n    angle_rates = 1 \/ np.power(10000, (2 * (i\/\/2)) \/ np.float32(d_model))\n    return pos * angle_rates\n\ndef positional_encoding(position, d_model):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                          np.arange(d_model)[np.newaxis, :],\n                          d_model)\n  \n  # \u5c06 sin \u5e94\u7528\u4e8e\u6570\u7ec4\u4e2d\u7684\u5076\u6570\u7d22\u5f15\uff08indices\uff09\uff1b2i\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n  \n  # \u5c06 cos \u5e94\u7528\u4e8e\u6570\u7ec4\u4e2d\u7684\u5947\u6570\u7d22\u5f15\uff1b2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    \n    pos_encoding = angle_rads[np.newaxis, ...]\n    \n    return tf.cast(pos_encoding, dtype=tf.float32)","ffbab1f6":"def create_padding_mask(seq):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n  \n  # \u6dfb\u52a0\u989d\u5916\u7684\u7ef4\u5ea6\u6765\u5c06\u586b\u5145\u52a0\u5230\n  # \u6ce8\u610f\u529b\u5bf9\u6570\uff08logits\uff09\u3002\n    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)","0c83f857":"def create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask  # (seq_len, seq_len)","e13297cd":"def scaled_dot_product_attention(q, k, v, mask):\n\n\n    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk \/ tf.math.sqrt(dk)\n\n  # \u5c06 mask \u52a0\u5165\u5230\u7f29\u653e\u7684\u5f20\u91cf\u4e0a\u3002\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)  \n\n  # softmax \u5728\u6700\u540e\u4e00\u4e2a\u8f74\uff08seq_len_k\uff09\u4e0a\u5f52\u4e00\u5316\uff0c\u56e0\u6b64\u5206\u6570\n  # \u76f8\u52a0\u7b49\u4e8e1\u3002\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n    return output","5191fa03":"class MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        assert d_model % self.num_heads == 0\n\n        self.depth = d_model \/\/ self.num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n        \n    def split_heads(self, x, batch_size):\n    \n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\n        k = self.wk(k)  # (batch_size, seq_len, d_model)\n        v = self.wv(v)  # (batch_size, seq_len, d_model)\n\n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n        scaled_attention = scaled_dot_product_attention(\n            q, k, v, mask)\n\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n        concat_attention = tf.reshape(scaled_attention, \n                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n        return output","29ffc8f7":"def point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n    ])","64f0dea5":"class EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n    \n    def call(self, x, training, mask):\n\n        attn_output = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n\n        return out2","aa062e54":"class DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n    \n    \n    def call(self, x, enc_output, training, \n           look_ahead_mask, padding_mask):\n        # enc_output.shape == (batch_size, input_seq_len, d_model)\n\n        attn1= self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x)\n\n        attn2 = self.mha2(\n            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n\n        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n\n        return out3","00386ba0":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n               maximum_position_encoding, rate=0.1):\n        super(Encoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, \n                                                self.d_model)\n\n\n        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n                           for _ in range(num_layers)]\n\n        self.dropout = tf.keras.layers.Dropout(rate)\n        \n    def call(self, x, training, mask):\n\n        seq_len = tf.shape(x)[1]\n\n        # \u5c06\u5d4c\u5165\u548c\u4f4d\u7f6e\u7f16\u7801\u76f8\u52a0\u3002\n        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n\n        return x  # (batch_size, input_seq_len, d_model)","4725d639":"class Decoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n               maximum_position_encoding, rate=0.1):\n        super(Decoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n\n        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n                           for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n    \n    def call(self, x, enc_output, training, \n           look_ahead_mask, padding_mask):\n\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n\n        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x = self.dec_layers[i](x, enc_output, training,look_ahead_mask, padding_mask)\n\n        # x.shape == (batch_size, target_seq_len, d_model)\n\n        return x","66e11071":"class Transformer(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n               target_vocab_size, pe_input, pe_target, rate=0.1):\n        super(Transformer, self).__init__()\n\n        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n                               input_vocab_size, pe_input, rate)\n\n        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n                               target_vocab_size, pe_target, rate)\n\n        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n    \n    def call(self, inp, tar, training, enc_padding_mask, \n           look_ahead_mask, dec_padding_mask):\n\n        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n\n        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n        dec_output= self.decoder(\n            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n\n        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n\n        return final_output","5e472c96":"num_layers = 4\nd_model = 128\ndff = 512\nnum_heads = 8\n\ninput_vocab_size = vocab_code_size\ntarget_vocab_size = vocab_nl_size\ndropout_rate = 0.2","56107932":"class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n    \n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n\n        self.warmup_steps = warmup_steps\n    \n    def __call__(self, step):\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)","b5d2347c":"learning_rate = CustomSchedule(d_model)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n                                     epsilon=1e-9)","2a6a38a5":"loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')","68abbcb4":"def loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n  \n    return tf.reduce_mean(loss_)","316152a4":"train_loss = tf.keras.metrics.Mean(name='train_loss')\nval_loss=tf.keras.metrics.Mean(name='val_loss')","3131a626":"transformer = Transformer(num_layers, d_model, num_heads, dff,\n                          input_vocab_size, target_vocab_size, \n                          pe_input=input_vocab_size, \n                          pe_target=target_vocab_size,\n                          rate=dropout_rate)","86cc64a0":"def create_masks(inp, tar):\n    \n    # \u7f16\u7801\u5668\u586b\u5145\u906e\u6321\n    enc_padding_mask = create_padding_mask(inp)\n  \n    # \u5728\u89e3\u7801\u5668\u7684\u7b2c\u4e8c\u4e2a\u6ce8\u610f\u529b\u6a21\u5757\u4f7f\u7528\u3002\n    # \u8be5\u586b\u5145\u906e\u6321\u7528\u4e8e\u906e\u6321\u7f16\u7801\u5668\u7684\u8f93\u51fa\u3002\n    dec_padding_mask = create_padding_mask(inp)\n  \n  # \u5728\u89e3\u7801\u5668\u7684\u7b2c\u4e00\u4e2a\u6ce8\u610f\u529b\u6a21\u5757\u4f7f\u7528\u3002\n  # \u7528\u4e8e\u586b\u5145\uff08pad\uff09\u548c\u906e\u6321\uff08mask\uff09\u89e3\u7801\u5668\u83b7\u53d6\u5230\u7684\u8f93\u5165\u7684\u540e\u7eed\u6807\u8bb0\uff08future tokens\uff09\u3002\n    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n    dec_target_padding_mask = create_padding_mask(tar)\n    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n  \n    return enc_padding_mask, combined_mask, dec_padding_mask","78c16d74":"checkpoint_path = \"\/kaggle\/working\/checkpoints\/train\"\n\nckpt = tf.train.Checkpoint(transformer=transformer)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\n# \u5982\u679c\u68c0\u67e5\u70b9\u5b58\u5728\uff0c\u5219\u6062\u590d\u6700\u65b0\u7684\u68c0\u67e5\u70b9\u3002\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print ('Latest checkpoint restored!!')","ba594ea8":"EPOCHS = 10","5d6ece0b":"# \u8be5 @tf.function \u5c06\u8ffd\u8e2a-\u7f16\u8bd1 train_step \u5230 TF \u56fe\u4e2d\uff0c\u4ee5\u4fbf\u66f4\u5feb\u5730\n# \u6267\u884c\u3002\u8be5\u51fd\u6570\u4e13\u7528\u4e8e\u53c2\u6570\u5f20\u91cf\u7684\u7cbe\u786e\u5f62\u72b6\u3002\u4e3a\u4e86\u907f\u514d\u7531\u4e8e\u53ef\u53d8\u5e8f\u5217\u957f\u5ea6\u6216\u53ef\u53d8\n# \u6279\u6b21\u5927\u5c0f\uff08\u6700\u540e\u4e00\u6279\u6b21\u8f83\u5c0f\uff09\u5bfc\u81f4\u7684\u518d\u8ffd\u8e2a\uff0c\u4f7f\u7528 input_signature \u6307\u5b9a\n# \u66f4\u591a\u7684\u901a\u7528\u5f62\u72b6\u3002\n\ntrain_step_signature = [\n    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n]\n\n@tf.function(input_signature=train_step_signature)\n\ndef train_step(inp, tar):\n    tar_inp = tar[:, :-1]\n    tar_real = tar[:, 1:]\n  \n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n  \n    with tf.GradientTape() as tape:\n        predictions= transformer(inp, tar_inp, \n                                 True, \n                                 enc_padding_mask, \n                                 combined_mask, \n                                 dec_padding_mask)\n        loss = loss_function(tar_real, predictions)\n\n    gradients = tape.gradient(loss, transformer.trainable_variables)    \n    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n   \n    train_loss(loss)\n  ","81fb18a8":"# \u8be5 @tf.function \u5c06\u8ffd\u8e2a-\u7f16\u8bd1 train_step \u5230 TF \u56fe\u4e2d\uff0c\u4ee5\u4fbf\u66f4\u5feb\u5730\n# \u6267\u884c\u3002\u8be5\u51fd\u6570\u4e13\u7528\u4e8e\u53c2\u6570\u5f20\u91cf\u7684\u7cbe\u786e\u5f62\u72b6\u3002\u4e3a\u4e86\u907f\u514d\u7531\u4e8e\u53ef\u53d8\u5e8f\u5217\u957f\u5ea6\u6216\u53ef\u53d8\n# \u6279\u6b21\u5927\u5c0f\uff08\u6700\u540e\u4e00\u6279\u6b21\u8f83\u5c0f\uff09\u5bfc\u81f4\u7684\u518d\u8ffd\u8e2a\uff0c\u4f7f\u7528 input_signature \u6307\u5b9a\n# \u66f4\u591a\u7684\u901a\u7528\u5f62\u72b6\u3002\n\nval_step_signature = [\n    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n]\n\n@tf.function(input_signature=val_step_signature)\n\ndef val_step(inp, tar):\n    tar_inp = tar[:, :-1]\n    tar_real = tar[:, 1:]\n  \n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n  \n    with tf.GradientTape() as tape:\n        predictions= transformer(inp, tar_inp, \n                                 False, \n                                 enc_padding_mask, \n                                 combined_mask, \n                                 dec_padding_mask)\n        loss = loss_function(tar_real, predictions)\n\n    val_loss(loss)\n  ","028e2f96":"train_loss_list=[]\nval_loss_list=[]\nfor epoch in range(EPOCHS):\n    start = time.time()\n\n    # start train\n    train_loss.reset_states()\n    for (batch, (inp, tar)) in enumerate(train_dataset):\n        \n        train_step(inp, tar)\n\n        if batch % 100 == 0:\n            print ('Epoch {} Batch {} Loss {:.4f} '.format(\n            epoch + 1, batch, train_loss.result()))\n\n    if (epoch + 1) == EPOCHS:\n        ckpt_save_path = ckpt_manager.save()\n        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,ckpt_save_path))\n                                                         \n    print ('train Epoch {} Loss {:.4f} '.format(epoch + 1, train_loss.result(), ))\n                                                \n                                                \n    train_loss_list.append(train_loss.result().numpy())\n\n    \n    \n    # start val\n    val_loss.reset_states()\n    for (batch, (inp, tar)) in enumerate(val_dataset):\n        \n        val_step(inp, tar)\n\n        if batch % 50 == 0:\n            print ('Epoch {} Batch {} Loss {:.4f} '.format(epoch + 1, batch, val_loss.result()))\n            \n    print ('val Epoch {} Loss {:.4f} '.format(epoch + 1, val_loss.result(),))\n                                                \n                                                \n    val_loss_list.append(val_loss.result().numpy()) \n\n    print ('Time taken for 1 epoch: {} secs == {:.2f} h\\n'.format(time.time() - start,(time.time() - start)\/3600))","31436c76":"print(train_loss_list)\nprint(val_loss_list)","c444c056":"def draw_loss(a,b):\n    x_list=[]\n    for i in range(len(a)):\n        x_list.append(i)\n    plt.title(\"LOSS\")\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.plot(x_list,a,marker='s',label=\"train\")\n    plt.plot(x_list,b,marker='s',label=\"val\")\n    plt.legend()\n    plt.savefig('\/kaggle\/working\/LOSS.png')\n    plt.show()\n    \ndraw_loss(train_loss_list,val_loss_list)","ab98d573":"def evaluate(code):\n    \n  \n    result=''\n    # code.shape(1,500)\n    encoder_input=tf.expand_dims(code,axis=0)\n  \n    output = tf.expand_dims([nl_lang.word_index['<start>']],axis=0)\n    \n    for i in range(nl_maxlen):\n    \n        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n\n        # predictions.shape == (batch_size, seq_len, vocab_size)\n        predictions = transformer(encoder_input, output,False,enc_padding_mask,combined_mask,dec_padding_mask)\n\n        # \u4ece seq_len \u7ef4\u5ea6\u9009\u62e9\u6700\u540e\u4e00\u4e2a\u8bcd\n        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n\n        pre=tf.argmax(predictions, axis=-1)\n        predicted_id = tf.cast(pre, tf.int32)\n        pre=pre.numpy()[0][0]\n        \n\n        # \u5982\u679c predicted_id \u7b49\u4e8e\u7ed3\u675f\u6807\u8bb0\uff0c\u5c31\u8fd4\u56de\u7ed3\u679c\n        if nl_lang.index_word[pre]=='<end>':\n            return result\n        \n        result+=nl_lang.index_word[pre]+' '\n\n        # \u8fde\u63a5 predicted_id \u4e0e\u8f93\u51fa\uff0c\u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u8f93\u5165\u4f20\u9012\u5230\u89e3\u7801\u5668\u3002\n        output = tf.concat([output, predicted_id], axis=-1)\n\n    return result","affc5933":"def translate():\n    with open('\/kaggle\/working\/result.txt','w+') as re:\n        for i in range(len(test_code_tensor)):\n            result=evaluate(test_code_tensor[i])\n            #print(result)\n            re.write(result+'\\n')   \n\nst=time.time()            \ntranslate()\nprint(\"translate time: {:.2f}h\".format((time.time() - st)\/3600))","9076c28d":"### \u7f16\u7801\u5668\uff08Encoder\uff09\n\n`\u7f16\u7801\u5668` \u5305\u62ec\uff1a\n1.   \u8f93\u5165\u5d4c\u5165\uff08Input Embedding\uff09\n2.   \u4f4d\u7f6e\u7f16\u7801\uff08Positional Encoding\uff09\n3.   N \u4e2a\u7f16\u7801\u5668\u5c42\uff08encoder layers\uff09\n\n\u8f93\u5165\u7ecf\u8fc7\u5d4c\u5165\uff08embedding\uff09\u540e\uff0c\u8be5\u5d4c\u5165\u4e0e\u4f4d\u7f6e\u7f16\u7801\u76f8\u52a0\u3002\u8be5\u52a0\u6cd5\u7ed3\u679c\u7684\u8f93\u51fa\u662f\u7f16\u7801\u5668\u5c42\u7684\u8f93\u5165\u3002\u7f16\u7801\u5668\u7684\u8f93\u51fa\u662f\u89e3\u7801\u5668\u7684\u8f93\u5165\u3002","97570cc5":"## \u906e\u6321\uff08Masking\uff09","653d116b":"### \u89e3\u7801\u5668\u5c42\uff08Decoder layer\uff09\n\n\u6bcf\u4e2a\u89e3\u7801\u5668\u5c42\u5305\u62ec\u4ee5\u4e0b\u5b50\u5c42\uff1a\n\n1.   \u906e\u6321\u7684\u591a\u5934\u6ce8\u610f\u529b\uff08\u524d\u77bb\u906e\u6321\u548c\u586b\u5145\u906e\u6321\uff09\n2.   \u591a\u5934\u6ce8\u610f\u529b\uff08\u7528\u586b\u5145\u906e\u6321\uff09\u3002V\uff08\u6570\u503c\uff09\u548c K\uff08\u4e3b\u952e\uff09\u63a5\u6536*\u7f16\u7801\u5668\u8f93\u51fa*\u4f5c\u4e3a\u8f93\u5165\u3002Q\uff08\u8bf7\u6c42\uff09\u63a5\u6536*\u906e\u6321\u7684\u591a\u5934\u6ce8\u610f\u529b\u5b50\u5c42\u7684\u8f93\u51fa*\u3002\n3.   \u70b9\u5f0f\u524d\u9988\u7f51\u7edc\n\n\u6bcf\u4e2a\u5b50\u5c42\u5728\u5176\u5468\u56f4\u6709\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5\uff0c\u7136\u540e\u8fdb\u884c\u5c42\u5f52\u4e00\u5316\u3002\u6bcf\u4e2a\u5b50\u5c42\u7684\u8f93\u51fa\u662f `LayerNorm(x + Sublayer(x))`\u3002\u5f52\u4e00\u5316\u662f\u5728 `d_model`\uff08\u6700\u540e\u4e00\u4e2a\uff09\u7ef4\u5ea6\u5b8c\u6210\u7684\u3002\n\nTransformer \u4e2d\u5171\u6709 N \u4e2a\u89e3\u7801\u5668\u5c42\u3002\n\n\u5f53 Q \u63a5\u6536\u5230\u89e3\u7801\u5668\u7684\u7b2c\u4e00\u4e2a\u6ce8\u610f\u529b\u5757\u7684\u8f93\u51fa\uff0c\u5e76\u4e14 K \u63a5\u6536\u5230\u7f16\u7801\u5668\u7684\u8f93\u51fa\u65f6\uff0c\u6ce8\u610f\u529b\u6743\u91cd\u8868\u793a\u6839\u636e\u7f16\u7801\u5668\u7684\u8f93\u51fa\u8d4b\u4e88\u89e3\u7801\u5668\u8f93\u5165\u7684\u91cd\u8981\u6027\u3002\u6362\u4e00\u79cd\u8bf4\u6cd5\uff0c\u89e3\u7801\u5668\u901a\u8fc7\u67e5\u770b\u7f16\u7801\u5668\u8f93\u51fa\u548c\u5bf9\u5176\u81ea\u8eab\u8f93\u51fa\u7684\u81ea\u6ce8\u610f\u529b\uff0c\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u3002\u53c2\u770b\u6309\u6bd4\u7f29\u653e\u7684\u70b9\u79ef\u6ce8\u610f\u529b\u90e8\u5206\u7684\u6f14\u793a\u3002","fe04c007":"## \u521b\u5efa Transformer","dfa8e596":"## \u7f16\u7801\u4e0e\u89e3\u7801\uff08Encoder and decoder\uff09","01cb6228":"\u524d\u77bb\u906e\u6321\uff08look-ahead mask\uff09\u7528\u4e8e\u906e\u6321\u4e00\u4e2a\u5e8f\u5217\u4e2d\u7684\u540e\u7eed\u6807\u8bb0\uff08future tokens\uff09\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u8be5 mask \u8868\u660e\u4e86\u4e0d\u5e94\u8be5\u4f7f\u7528\u7684\u6761\u76ee\u3002\n\n\u8fd9\u610f\u5473\u7740\u8981\u9884\u6d4b\u7b2c\u4e09\u4e2a\u8bcd\uff0c\u5c06\u4ec5\u4f7f\u7528\u7b2c\u4e00\u4e2a\u548c\u7b2c\u4e8c\u4e2a\u8bcd\u3002\u4e0e\u6b64\u7c7b\u4f3c\uff0c\u9884\u6d4b\u7b2c\u56db\u4e2a\u8bcd\uff0c\u4ec5\u4f7f\u7528\u7b2c\u4e00\u4e2a\uff0c\u7b2c\u4e8c\u4e2a\u548c\u7b2c\u4e09\u4e2a\u8bcd\uff0c\u4f9d\u6b64\u7c7b\u63a8\u3002 ","a937e451":"<img src=\"https:\/\/tensorflow.google.cn\/images\/tutorials\/transformer\/transformer.png\" width=\"600\" alt=\"transformer\">","2d16e3c5":"## \u4f18\u5316\u5668\uff08Optimizer\uff09","78e1f353":"## \u8bc4\u4f30\uff08Evaluate\uff09","8e33c4d3":"`\u89e3\u7801\u5668`\u5305\u62ec\uff1a\n1.   \u8f93\u51fa\u5d4c\u5165\uff08Output Embedding\uff09\n2.   \u4f4d\u7f6e\u7f16\u7801\uff08Positional Encoding\uff09\n3.   N \u4e2a\u89e3\u7801\u5668\u5c42\uff08decoder layers\uff09\n\n\u76ee\u6807\uff08target\uff09\u7ecf\u8fc7\u4e00\u4e2a\u5d4c\u5165\u540e\uff0c\u8be5\u5d4c\u5165\u548c\u4f4d\u7f6e\u7f16\u7801\u76f8\u52a0\u3002\u8be5\u52a0\u6cd5\u7ed3\u679c\u662f\u89e3\u7801\u5668\u5c42\u7684\u8f93\u5165\u3002\u89e3\u7801\u5668\u7684\u8f93\u51fa\u662f\u6700\u540e\u7684\u7ebf\u6027\u5c42\u7684\u8f93\u5165\u3002","337fa2f6":"## \u591a\u5934\u6ce8\u610f\u529b\uff08Multi-head attention\uff09","220a42c1":"### \u89e3\u7801\u5668\uff08Decoder\uff09","a464edf9":"\u7531\u4e8e\u76ee\u6807\u5e8f\u5217\u662f\u586b\u5145\uff08padded\uff09\u8fc7\u7684\uff0c\u56e0\u6b64\u5728\u8ba1\u7b97\u635f\u5931\u51fd\u6570\u65f6\uff0c\u5e94\u7528\u586b\u5145\u906e\u6321\u975e\u5e38\u91cd\u8981\u3002","6201d94a":"\u76ee\u6807\uff08target\uff09\u88ab\u5206\u6210\u4e86 tar_inp \u548c tar_real\u3002tar_inp \u4f5c\u4e3a\u8f93\u5165\u4f20\u9012\u5230\u89e3\u7801\u5668\u3002`tar_real` \u662f\u4f4d\u79fb\u4e86 1 \u7684\u540c\u4e00\u4e2a\u8f93\u5165\uff1a\u5728 `tar_inp` \u4e2d\u7684\u6bcf\u4e2a\u4f4d\u7f6e\uff0c`tar_real` \u5305\u542b\u4e86\u5e94\u8be5\u88ab\u9884\u6d4b\u5230\u7684\u4e0b\u4e00\u4e2a\u6807\u8bb0\uff08token\uff09\u3002\n\n\u4f8b\u5982\uff0c`sentence` = \"SOS A lion in the jungle is sleeping EOS\"\n\n`tar_inp` =  \"SOS A lion in the jungle is sleeping\"\n\n`tar_real` = \"A lion in the jungle is sleeping EOS\"\n\nTransformer \u662f\u4e00\u4e2a\u81ea\u56de\u5f52\uff08auto-regressive\uff09\u6a21\u578b\uff1a\u5b83\u4e00\u6b21\u4f5c\u4e00\u4e2a\u90e8\u5206\u7684\u9884\u6d4b\uff0c\u7136\u540e\u4f7f\u7528\u5230\u76ee\u524d\u4e3a\u6b62\u7684\u81ea\u8eab\u7684\u8f93\u51fa\u6765\u51b3\u5b9a\u4e0b\u4e00\u6b65\u8981\u505a\u4ec0\u4e48\u3002\n\n\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u672c\u793a\u4f8b\u4f7f\u7528\u4e86 teacher-forcing \u7684\u65b9\u6cd5\uff08\u5c31\u50cf[\u6587\u672c\u751f\u6210\u6559\u7a0b](.\/text_generation.ipynb)\u4e2d\u4e00\u6837\uff09\u3002\u65e0\u8bba\u6a21\u578b\u5728\u5f53\u524d\u65f6\u95f4\u6b65\u9aa4\u4e0b\u9884\u6d4b\u51fa\u4ec0\u4e48\uff0cteacher-forcing \u65b9\u6cd5\u90fd\u4f1a\u5c06\u771f\u5b9e\u7684\u8f93\u51fa\u4f20\u9012\u5230\u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65\u9aa4\u4e0a\u3002\n\n\u5f53 transformer \u9884\u6d4b\u6bcf\u4e2a\u8bcd\u65f6\uff0c*\u81ea\u6ce8\u610f\u529b\uff08self-attention\uff09*\u529f\u80fd\u4f7f\u5b83\u80fd\u591f\u67e5\u770b\u8f93\u5165\u5e8f\u5217\u4e2d\u524d\u9762\u7684\u5355\u8bcd\uff0c\u4ece\u800c\u66f4\u597d\u5730\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5355\u8bcd\u3002\n\n\u4e3a\u4e86\u9632\u6b62\u6a21\u578b\u5728\u671f\u671b\u7684\u8f93\u51fa\u4e0a\u8fbe\u5230\u5cf0\u503c\uff0c\u6a21\u578b\u4f7f\u7528\u4e86\u524d\u77bb\u906e\u6321\uff08look-ahead mask\uff09\u3002","b8e4a846":"\u6bcf\u4e2a\u591a\u5934\u6ce8\u610f\u529b\u5757\u6709\u4e09\u4e2a\u8f93\u5165\uff1aQ\uff08\u8bf7\u6c42\uff09\u3001K\uff08\u4e3b\u952e\uff09\u3001V\uff08\u6570\u503c\uff09\u3002\u8fd9\u4e9b\u8f93\u5165\u7ecf\u8fc7\u7ebf\u6027\uff08Dense\uff09\u5c42\uff0c\u5e76\u5206\u62c6\u6210\u591a\u5934\u3002 \n\n\u5c06\u4e0a\u9762\u5b9a\u4e49\u7684 `scaled_dot_product_attention` \u51fd\u6570\u5e94\u7528\u4e8e\u6bcf\u4e2a\u5934\uff08\u8fdb\u884c\u4e86\u5e7f\u64ad\uff08broadcasted\uff09\u4ee5\u63d0\u9ad8\u6548\u7387\uff09\u3002\u6ce8\u610f\u529b\u8fd9\u6b65\u5fc5\u987b\u4f7f\u7528\u4e00\u4e2a\u6070\u5f53\u7684 mask\u3002\u7136\u540e\u5c06\u6bcf\u4e2a\u5934\u7684\u6ce8\u610f\u529b\u8f93\u51fa\u8fde\u63a5\u8d77\u6765\uff08\u7528`tf.transpose` \u548c `tf.reshape`\uff09\uff0c\u5e76\u653e\u5165\u6700\u540e\u7684 `Dense` \u5c42\u3002\n\nQ\u3001K\u3001\u548c V \u88ab\u62c6\u5206\u5230\u4e86\u591a\u4e2a\u5934\uff0c\u800c\u975e\u5355\u4e2a\u7684\u6ce8\u610f\u529b\u5934\uff0c\u56e0\u4e3a\u591a\u5934\u5141\u8bb8\u6a21\u578b\u5171\u540c\u6ce8\u610f\u6765\u81ea\u4e0d\u540c\u8868\u793a\u7a7a\u95f4\u7684\u4e0d\u540c\u4f4d\u7f6e\u7684\u4fe1\u606f\u3002\u5728\u5206\u62c6\u540e\uff0c\u6bcf\u4e2a\u5934\u90e8\u7684\u7ef4\u5ea6\u51cf\u5c11\uff0c\u56e0\u6b64\u603b\u7684\u8ba1\u7b97\u6210\u672c\u4e0e\u6709\u7740\u5168\u90e8\u7ef4\u5ea6\u7684\u5355\u4e2a\u6ce8\u610f\u529b\u5934\u76f8\u540c\u3002","8f0e526f":"<img src=\"https:\/\/tensorflow.google.cn\/images\/tutorials\/transformer\/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n\n\n\u591a\u5934\u6ce8\u610f\u529b\u7531\u56db\u90e8\u5206\u7ec4\u6210\uff1a\n*    \u7ebf\u6027\u5c42\u5e76\u5206\u62c6\u6210\u591a\u5934\u3002\n*    \u6309\u6bd4\u7f29\u653e\u7684\u70b9\u79ef\u6ce8\u610f\u529b\u3002\n*    \u591a\u5934\u53ca\u8054\u3002\n*    \u6700\u540e\u4e00\u5c42\u7ebf\u6027\u5c42\u3002","0e066538":"<img src=\"https:\/\/tensorflow.google.cn\/images\/tutorials\/transformer\/scaled_attention.png\" width=\"500\" alt=\"scaled_dot_product_attention\">\n\nTransformer \u4f7f\u7528\u7684\u6ce8\u610f\u529b\u51fd\u6570\u6709\u4e09\u4e2a\u8f93\u5165\uff1aQ\uff08\u8bf7\u6c42\uff08query\uff09\uff09\u3001K\uff08\u4e3b\u952e\uff08key\uff09\uff09\u3001V\uff08\u6570\u503c\uff08value\uff09\uff09\u3002\u7528\u4e8e\u8ba1\u7b97\u6ce8\u610f\u529b\u6743\u91cd\u7684\u7b49\u5f0f\u4e3a\uff1a\n\n$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n\n\u70b9\u79ef\u6ce8\u610f\u529b\u88ab\u7f29\u5c0f\u4e86\u6df1\u5ea6\u7684\u5e73\u65b9\u6839\u500d\u3002\u8fd9\u6837\u505a\u662f\u56e0\u4e3a\u5bf9\u4e8e\u8f83\u5927\u7684\u6df1\u5ea6\u503c\uff0c\u70b9\u79ef\u7684\u5927\u5c0f\u4f1a\u589e\u5927\uff0c\u4ece\u800c\u63a8\u52a8 softmax \u51fd\u6570\u5f80\u4ec5\u6709\u5f88\u5c0f\u7684\u68af\u5ea6\u7684\u65b9\u5411\u9760\u62e2\uff0c\u5bfc\u81f4\u4e86\u4e00\u79cd\u5f88\u786c\u7684\uff08hard\uff09softmax\u3002\n\n\u4f8b\u5982\uff0c\u5047\u8bbe `Q` \u548c `K` \u7684\u5747\u503c\u4e3a0\uff0c\u65b9\u5dee\u4e3a1\u3002\u5b83\u4eec\u7684\u77e9\u9635\u4e58\u79ef\u5c06\u6709\u5747\u503c\u4e3a0\uff0c\u65b9\u5dee\u4e3a `dk`\u3002\u56e0\u6b64\uff0c*`dk` \u7684\u5e73\u65b9\u6839*\u88ab\u7528\u4e8e\u7f29\u653e\uff08\u800c\u975e\u5176\u4ed6\u6570\u503c\uff09\uff0c\u56e0\u4e3a\uff0c`Q` \u548c `K` \u7684\u77e9\u9635\u4e58\u79ef\u7684\u5747\u503c\u672c\u5e94\u8be5\u4e3a 0\uff0c\u65b9\u5dee\u672c\u5e94\u8be5\u4e3a1\uff0c\u8fd9\u6837\u4f1a\u83b7\u5f97\u4e00\u4e2a\u66f4\u5e73\u7f13\u7684 softmax\u3002\n\n\u906e\u6321\uff08mask\uff09\u4e0e -1e9\uff08\u63a5\u8fd1\u4e8e\u8d1f\u65e0\u7a77\uff09\u76f8\u4e58\u3002\u8fd9\u6837\u505a\u662f\u56e0\u4e3a\u906e\u6321\u4e0e\u7f29\u653e\u7684 Q \u548c K \u7684\u77e9\u9635\u4e58\u79ef\u76f8\u52a0\uff0c\u5e76\u5728 softmax \u4e4b\u524d\u7acb\u5373\u5e94\u7528\u3002\u76ee\u6807\u662f\u5c06\u8fd9\u4e9b\u5355\u5143\u5f52\u96f6\uff0c\u56e0\u4e3a softmax \u7684\u8f83\u5927\u8d1f\u6570\u8f93\u5165\u5728\u8f93\u51fa\u4e2d\u63a5\u8fd1\u4e8e\u96f6\u3002","abbc9aa8":"num_words: the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.","53307592":"\u5f53 softmax \u5728 K \u4e0a\u8fdb\u884c\u5f52\u4e00\u5316\u540e\uff0c\u5b83\u7684\u503c\u51b3\u5b9a\u4e86\u5206\u914d\u5230 Q \u7684\u91cd\u8981\u7a0b\u5ea6\u3002\n\n\u8f93\u51fa\u8868\u793a\u6ce8\u610f\u529b\u6743\u91cd\u548c V\uff08\u6570\u503c\uff09\u5411\u91cf\u7684\u4e58\u79ef\u3002\u8fd9\u786e\u4fdd\u4e86\u8981\u5173\u6ce8\u7684\u8bcd\u4fdd\u6301\u539f\u6837\uff0c\u800c\u65e0\u5173\u7684\u8bcd\u5c06\u88ab\u6e05\u9664\u6389\u3002","e51f9f98":"\u521b\u5efa\u68c0\u67e5\u70b9\u7684\u8def\u5f84\u548c\u68c0\u67e5\u70b9\u7ba1\u7406\u5668\uff08manager\uff09\u3002\u8fd9\u5c06\u7528\u4e8e\u5728\u6bcf `n` \u4e2a\u5468\u671f\uff08epochs\uff09\u4fdd\u5b58\u68c0\u67e5\u70b9\u3002","6f47ace4":"Transformer \u5305\u62ec\u7f16\u7801\u5668\uff0c\u89e3\u7801\u5668\u548c\u6700\u540e\u7684\u7ebf\u6027\u5c42\u3002\u89e3\u7801\u5668\u7684\u8f93\u51fa\u662f\u7ebf\u6027\u5c42\u7684\u8f93\u5165\uff0c\u8fd4\u56de\u7ebf\u6027\u5c42\u7684\u8f93\u51fa\u3002","93ee6514":"## \u8bad\u7ec3\u4e0e\u68c0\u67e5\u70b9\uff08Training and checkpointing\uff09","295b3bf0":"## \u914d\u7f6e\u8d85\u53c2\u6570\uff08hyperparameters\uff09","c232ad1c":"## \u4f4d\u7f6e\u7f16\u7801\uff08Positional encoding\uff09\n\n\u56e0\u4e3a\u8be5\u6a21\u578b\u5e76\u4e0d\u5305\u62ec\u4efb\u4f55\u7684\u5faa\u73af\uff08recurrence\uff09\u6216\u5377\u79ef\uff0c\u6240\u4ee5\u6a21\u578b\u6dfb\u52a0\u4e86\u4f4d\u7f6e\u7f16\u7801\uff0c\u4e3a\u6a21\u578b\u63d0\u4f9b\u4e00\u4e9b\u5173\u4e8e\u5355\u8bcd\u5728\u53e5\u5b50\u4e2d\u76f8\u5bf9\u4f4d\u7f6e\u7684\u4fe1\u606f\u3002\n\n\u4f4d\u7f6e\u7f16\u7801\u5411\u91cf\u88ab\u52a0\u5230\u5d4c\u5165\uff08embedding\uff09\u5411\u91cf\u4e2d\u3002\u5d4c\u5165\u8868\u793a\u4e00\u4e2a d \u7ef4\u7a7a\u95f4\u7684\u6807\u8bb0\uff0c\u5728 d \u7ef4\u7a7a\u95f4\u4e2d\u6709\u7740\u76f8\u4f3c\u542b\u4e49\u7684\u6807\u8bb0\u4f1a\u79bb\u5f7c\u6b64\u66f4\u8fd1\u3002\u4f46\u662f\uff0c\u5d4c\u5165\u5e76\u6ca1\u6709\u5bf9\u5728\u4e00\u53e5\u8bdd\u4e2d\u7684\u8bcd\u7684\u76f8\u5bf9\u4f4d\u7f6e\u8fdb\u884c\u7f16\u7801\u3002\u56e0\u6b64\uff0c\u5f53\u52a0\u4e0a\u4f4d\u7f6e\u7f16\u7801\u540e\uff0c\u8bcd\u5c06\u57fa\u4e8e*\u5b83\u4eec\u542b\u4e49\u7684\u76f8\u4f3c\u5ea6\u4ee5\u53ca\u5b83\u4eec\u5728\u53e5\u5b50\u4e2d\u7684\u4f4d\u7f6e*\uff0c\u5728 d \u7ef4\u7a7a\u95f4\u4e2d\u79bb\u5f7c\u6b64\u66f4\u8fd1\u3002\n\n\u53c2\u770b [\u4f4d\u7f6e\u7f16\u7801](https:\/\/github.com\/tensorflow\/examples\/blob\/master\/community\/en\/position_encoding.ipynb) \u7684 notebook \u4e86\u89e3\u66f4\u591a\u4fe1\u606f\u3002\u8ba1\u7b97\u4f4d\u7f6e\u7f16\u7801\u7684\u516c\u5f0f\u5982\u4e0b\uff1a\n\n$$\\Large{PE_{(pos, 2i)} = sin(pos \/ 10000^{2i \/ d_{model}})} $$\n$$\\Large{PE_{(pos, 2i+1)} = cos(pos \/ 10000^{2i \/ d_{model}})} $$","5aef9346":"Transformer \u6a21\u578b\u4e0e\u6807\u51c6\u7684[\u5177\u6709\u6ce8\u610f\u529b\u673a\u5236\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b\uff08sequence to sequence with attention model\uff09](nmt_with_attention.ipynb)\uff0c\u9075\u5faa\u76f8\u540c\u7684\u4e00\u822c\u6a21\u5f0f\u3002\n\n* \u8f93\u5165\u8bed\u53e5\u7ecf\u8fc7 `N` \u4e2a\u7f16\u7801\u5668\u5c42\uff0c\u4e3a\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u8bcd\/\u6807\u8bb0\u751f\u6210\u4e00\u4e2a\u8f93\u51fa\u3002\n* \u89e3\u7801\u5668\u5173\u6ce8\u7f16\u7801\u5668\u7684\u8f93\u51fa\u4ee5\u53ca\u5b83\u81ea\u8eab\u7684\u8f93\u5165\uff08\u81ea\u6ce8\u610f\u529b\uff09\u6765\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u3002","90c6c442":"\u906e\u6321\u4e00\u6279\u5e8f\u5217\u4e2d\u6240\u6709\u7684\u586b\u5145\u6807\u8bb0\uff08pad tokens\uff09\u3002\u8fd9\u786e\u4fdd\u4e86\u6a21\u578b\u4e0d\u4f1a\u5c06\u586b\u5145\u4f5c\u4e3a\u8f93\u5165\u3002\u8be5 mask \u8868\u660e\u586b\u5145\u503c `0` \u51fa\u73b0\u7684\u4f4d\u7f6e\uff1a\u5728\u8fd9\u4e9b\u4f4d\u7f6e mask \u8f93\u51fa `1`\uff0c\u5426\u5219\u8f93\u51fa `0`\u3002","cf49cf97":"## \u70b9\u5f0f\u524d\u9988\u7f51\u7edc\uff08Point wise feed forward network\uff09","54f3b5a6":"\u70b9\u5f0f\u524d\u9988\u7f51\u7edc\u7531\u4e24\u5c42\u5168\u8054\u63a5\u5c42\u7ec4\u6210\uff0c\u4e24\u5c42\u4e4b\u95f4\u6709\u4e00\u4e2a ReLU \u6fc0\u6d3b\u51fd\u6570\u3002","4d76708a":"### \u7f16\u7801\u5668\u5c42\uff08Encoder layer\uff09\n\n\u6bcf\u4e2a\u7f16\u7801\u5668\u5c42\u5305\u62ec\u4ee5\u4e0b\u5b50\u5c42\uff1a\n\n1.   \u591a\u5934\u6ce8\u610f\u529b\uff08\u6709\u586b\u5145\u906e\u6321\uff09\n2.   \u70b9\u5f0f\u524d\u9988\u7f51\u7edc\uff08Point wise feed forward networks\uff09\u3002\n\n\u6bcf\u4e2a\u5b50\u5c42\u5728\u5176\u5468\u56f4\u6709\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5\uff0c\u7136\u540e\u8fdb\u884c\u5c42\u5f52\u4e00\u5316\u3002\u6b8b\u5dee\u8fde\u63a5\u6709\u52a9\u4e8e\u907f\u514d\u6df1\u5ea6\u7f51\u7edc\u4e2d\u7684\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u3002\n\n\u6bcf\u4e2a\u5b50\u5c42\u7684\u8f93\u51fa\u662f `LayerNorm(x + Sublayer(x))`\u3002\u5f52\u4e00\u5316\u662f\u5728 `d_model`\uff08\u6700\u540e\u4e00\u4e2a\uff09\u7ef4\u5ea6\u5b8c\u6210\u7684\u3002Transformer \u4e2d\u6709 N \u4e2a\u7f16\u7801\u5668\u5c42\u3002","15f8543e":"## \u635f\u5931\u51fd\u6570\u4e0e\u6307\u6807\uff08Loss and metrics\uff09","c9ec3f11":"\u4e3a\u4e86\u8ba9\u672c\u793a\u4f8b\u5c0f\u4e14\u76f8\u5bf9\u8f83\u5feb\uff0c\u5df2\u7ecf\u51cf\u5c0f\u4e86*num_layers\u3001 d_model \u548c  dff* \u7684\u503c\u3002 \n\nTransformer \u7684\u57fa\u7840\u6a21\u578b\u4f7f\u7528\u7684\u6570\u503c\u4e3a\uff1a*num_layers=6*\uff0c*d_model = 512*\uff0c*dff = 2048*\u3002\u5173\u4e8e\u6240\u6709\u5176\u4ed6\u7248\u672c\u7684 Transformer\uff0c\u8bf7\u67e5\u9605[\u8bba\u6587](https:\/\/arxiv.org\/abs\/1706.03762)\u3002\n\nNote\uff1a\u901a\u8fc7\u6539\u53d8\u4ee5\u4e0b\u6570\u503c\uff0c\u60a8\u53ef\u4ee5\u83b7\u5f97\u5728\u8bb8\u591a\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u7684\u6a21\u578b\u3002","777ae919":"## \u6309\u6bd4\u7f29\u653e\u7684\u70b9\u79ef\u6ce8\u610f\u529b\uff08Scaled dot product attention\uff09","9b6ef487":"    \n  \"\"\"\u8ba1\u7b97\u6ce8\u610f\u529b\u6743\u91cd\u3002\n  q, k, v \u5fc5\u987b\u5177\u6709\u5339\u914d\u7684\u524d\u7f6e\u7ef4\u5ea6\u3002\n  k, v \u5fc5\u987b\u6709\u5339\u914d\u7684\u5012\u6570\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\uff0c\u4f8b\u5982\uff1aseq_len_k = seq_len_v\u3002\n  \u867d\u7136 mask \u6839\u636e\u5176\u7c7b\u578b\uff08\u586b\u5145\u6216\u524d\u77bb\uff09\u6709\u4e0d\u540c\u7684\u5f62\u72b6\uff0c\n  \u4f46\u662f mask \u5fc5\u987b\u80fd\u8fdb\u884c\u5e7f\u64ad\u8f6c\u6362\u4ee5\u4fbf\u6c42\u548c\u3002\n  \n  \u53c2\u6570:\n    q: \u8bf7\u6c42\u7684\u5f62\u72b6 == (..., seq_len_q, depth)\n    k: \u4e3b\u952e\u7684\u5f62\u72b6 == (..., seq_len_k, depth)\n    v: \u6570\u503c\u7684\u5f62\u72b6 == (..., seq_len_v, depth_v)\n    mask: Float \u5f20\u91cf\uff0c\u5176\u5f62\u72b6\u80fd\u8f6c\u6362\u6210\n          (..., seq_len_q, seq_len_k)\u3002\u9ed8\u8ba4\u4e3aNone\u3002\n    \n  \u8fd4\u56de\u503c:\n    \u8f93\u51fa\uff0c\u6ce8\u610f\u529b\u6743\u91cd\n  \"\"\"","14603378":"\u6839\u636e[\u8bba\u6587](https:\/\/arxiv.org\/abs\/1706.03762)\u4e2d\u7684\u516c\u5f0f\uff0c\u5c06 Adam \u4f18\u5316\u5668\u4e0e\u81ea\u5b9a\u4e49\u7684\u5b66\u4e60\u901f\u7387\u8c03\u5ea6\u7a0b\u5e8f\uff08scheduler\uff09\u914d\u5408\u4f7f\u7528\u3002\n\n$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$\n"}}