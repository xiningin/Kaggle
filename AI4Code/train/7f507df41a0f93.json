{"cell_type":{"e7f45e8f":"code","609588a7":"code","b17e66db":"code","27a17799":"code","a84f1df2":"code","e56a5201":"code","65aa4f62":"code","25acdfaf":"code","85a7082a":"code","3969c4d1":"code","8a6c4791":"code","a8cd0193":"code","991ae99b":"code","310be72a":"code","d4be833c":"code","bca47935":"markdown","b8a8da59":"markdown","2b079a8f":"markdown","6020a79e":"markdown","4b1e94ee":"markdown","0c7c7d22":"markdown","2bcdac3d":"markdown","db45d0ca":"markdown","2cf4dc04":"markdown"},"source":{"e7f45e8f":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split","609588a7":"df = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\ndf.head()","b17e66db":"import string\nfrom nltk.corpus import stopwords\nimport plotly.graph_objects as go\nimport plotly_express as px\nfrom plotly.subplots import make_subplots\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom PIL import Image","27a17799":"def remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            final_text.append(i.strip())\n    return \" \".join(final_text)","a84f1df2":"stop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)","e56a5201":"df['headline']=df['headline'].apply(remove_stopwords)\nwc_0 = WordCloud(max_words = 2000, background_color=\"white\", width = 1000 , height = 800).generate(\" \".join(df[df.is_sarcastic == 0].headline))\nwc_1 = WordCloud(max_words = 2000, background_color=\"white\", width = 1000 , height = 800).generate(\" \".join(df[df.is_sarcastic == 1].headline))\nfig = make_subplots(1, 2, subplot_titles=(\"Non-Sarcastic\", \"Sarcastic\"))\nfig.update_layout(\n    title=\"Word Clouds\")\nfig.add_trace(go.Image(z = wc_0), 1, 1)\nfig.add_trace(go.Image(z = wc_1), 1, 2)\nfig.show()","65aa4f62":"vocab_size = 10000\nembedding_dim = 16\nmax_length = 100\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\ntraining_size = 20000\nepochs = 5","25acdfaf":"x_train, x_test, y_train, y_test = train_test_split(df['headline'], df['is_sarcastic'], train_size = 0.9, random_state = 73)\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(x_train)\nword_index = tokenizer.word_index\n\ntrain_sequences = tokenizer.texts_to_sequences(x_train)\nx_train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\ntest_sequences = tokenizer.texts_to_sequences(x_test)\nx_test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","85a7082a":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,recurrent_dropout = 0.3 , dropout = 0.3, return_sequences = True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,recurrent_dropout = 0.1 , dropout = 0.1)),\n    tf.keras.layers.Dense(512, activation = \"relu\"),\n    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n])\n\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics=['accuracy'])","3969c4d1":"history = model.fit(x_train_padded,y_train, batch_size = 128, epochs = epochs, validation_data = (x_test_padded, y_test))","8a6c4791":"train_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\n\nfig_model = make_subplots(1, 2, subplot_titles=(\"Accuracy over Time\", \"Loss over Time\"))\nfig_model.add_trace(go.Scatter(x = np.arange(epochs),\n                                 y = train_acc,\n                                 mode = \"lines+markers\",\n                                 name = \"Training Accuracy\"),\n                   row = 1, col = 1)\nfig_model.add_trace(go.Scatter(x = np.arange(epochs),\n                                 y = val_acc,\n                                 mode = \"lines+markers\",\n                                 name = \"Validation Accuracy\"),\n                   row = 1, col = 1)\nfig_model.add_trace(go.Scatter(x = np.arange(epochs),\n                                 y = train_loss,\n                                 mode = \"lines+markers\",\n                                 name = \"Training Loss\"),\n                   row = 1, col = 2)\nfig_model.add_trace(go.Scatter(x = np.arange(epochs),\n                                 y = val_loss,\n                                 mode = \"lines+markers\",\n                                 name = \"Validation Loss\"),\n                   row = 1, col = 2)\nfig_model.show()","a8cd0193":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report","991ae99b":"pred = model.predict_classes(x_test_padded)","310be72a":"print(classification_report(y_test, pred, target_names = ['Non-Sarcastic','Sarcastic']))","d4be833c":"cm = confusion_matrix(y_test, pred)\ncm = pd.DataFrame(cm , index = ['Non-Sarcastic','Sarcastic'] , columns = ['Predicted Non-Sarcastic','Predicted Sarcastic'])\nplt.figure(figsize = (10,8))\nsns.heatmap(cm,cmap = \"BuGn\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Predicted Non-Sarcastic','Predicted Sarcastic'] , yticklabels = ['Non-Sarcastic','Sarcastic'])\nplt.show()","bca47935":"Thank you for going through my notebook. Hopefully you have gained valuable insights into the dataset.\nI would appreciate if you Upvote this notebook as well! Kindly let me know if I can improve my work in any aspect","b8a8da59":"<center><h1> The End <\/h1><\/center>","2b079a8f":"<h2>Inspiration<\/h2>","6020a79e":"<center><h1>Using Tensorflow for Sarcasm Detection in News Headlines<\/h1><\/center>","4b1e94ee":"## Modelling a Sarcasm Classifier","0c7c7d22":"Past studies in Sarcasm Detection mostly make use of Twitter datasets collected using hashtag based supervision but such datasets are noisy in terms of labels and language. Furthermore, many tweets are replies to other tweets and detecting sarcasm in these requires the availability of contextual tweets.\n\nTo overcome the limitations related to noise in Twitter datasets, this News Headlines dataset for Sarcasm Detection is collected from two news website. TheOnion aims at producing sarcastic versions of current events and we collected all the headlines from News in Brief and News in Photos categories (which are sarcastic). We collect real (and non-sarcastic) news headlines from HuffPost.\n\nThis new dataset has following advantages over the existing Twitter datasets:\n\n* Since news headlines are written by professionals in a formal manner, there are no spelling mistakes and informal usage. This reduces the sparsity and also increases the chance of finding pre-trained embeddings.\n\n* Furthermore, since the sole purpose of TheOnion is to publish sarcastic news, we get high-quality labels with much less noise as compared to Twitter datasets.\n\n* Unlike tweets which are replies to other tweets, the news headlines we obtained are self-contained. This would help us in teasing apart the real sarcastic elements.","2bcdac3d":"## Visualize the Data","db45d0ca":"<h2>Context of the Dataset<\/h2>","2cf4dc04":"To identify sarcastic sentences and to distinguish between fake news and legitimate news?"}}