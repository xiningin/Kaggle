{"cell_type":{"1b0c7126":"code","6d9d72c2":"code","dede30ec":"code","874921d2":"code","d0a64bf7":"code","8337db27":"code","1b4e1f3c":"code","f6f4965b":"code","de74f625":"code","37b8b683":"code","f9913730":"code","f1633bbe":"code","dd5d5196":"code","efed96fd":"code","2cbc0c33":"code","3b85a4bd":"code","97e39ecd":"code","246f9096":"code","9c55b9c9":"code","a719f4cd":"code","5d839ef1":"code","efacf0bb":"code","c0fed758":"code","a06de2d4":"code","c81d0223":"code","e97690d2":"code","f6824bfa":"code","8b51f3d4":"code","024eac83":"code","e36231ff":"code","89baaec7":"markdown","39862677":"markdown","55aa9c6c":"markdown","d439ef53":"markdown","fbc743fd":"markdown","bd4881c3":"markdown","50ef82a1":"markdown","94d07231":"markdown","68421ce5":"markdown"},"source":{"1b0c7126":"%matplotlib inline\n\nimport pandas as pd\npd.set_option('display.float_format', '{:.3f}'.format)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score, classification_report\nfrom pandas import Series\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport eli5 \nfrom eli5.sklearn import PermutationImportance\n\nimport itertools","6d9d72c2":"def plot_cf_matrix_and_roc(model, \n                           X_train, \n                           y_train,\n                           X_test, \n                           y_test,\n                           y_pred, \n                           classes=[0,1],\n                           normalize=False,\n                           cmap=plt.cm.Blues):\n    metrics_list = []\n    \n    # the main plot\n    plt.figure(figsize=(15,5))\n\n    # the confusion matrix\n    plt.subplot(1,2,1)\n    cm = confusion_matrix(y_test, y_pred)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    \n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        plt.title(\"Normalized confusion matrix\")\n    else:\n        plt.title('Confusion matrix')\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, format(cm[i, j]),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n    # the ROC curve\n    plt.subplot(1,2,2)\n    tmp = model.fit(X_train, y_train.ravel())\n    y_pred_sample_score = tmp.decision_function(X_test)\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_sample_score)\n    roc_auc = auc(fpr,tpr)\n\n    # Plot ROC\n    plt.title('Receiver Operating Characteristic (ROC)')\n    plt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)\n    plt.legend(loc='lower right')\n    plt.plot([0,1],[0,1],'r--')\n    plt.xlim([-0.01,1.0])\n    plt.ylim([-0.01,1.01])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.tight_layout()\n    \n    # the result metrix\n    summary_df = pd.DataFrame([[str(np.unique( y_pred )),\n                               str(round(metrics.precision_score(y_test, y_pred.round()),3)),\n                               str(round(metrics.accuracy_score(y_test, y_pred.round()),3)),\n                               str(round(metrics.recall_score(y_test, y_pred.round(), average='binary'),3)),\n                               str(round(metrics.roc_auc_score(y_test, y_pred.round()),3)),\n                               str(round(roc_auc,3)),\n                               str(round(metrics.f1_score(y_test, y_pred.round(), average='binary'),3))]], \n                              columns=['Class', 'Precision', 'Accuracy', 'Recall', 'ROC-AUC', 'AUC', 'F1-score'])\n    # print the metrics\n    print(\"\\n\");\n    print(summary_df);\n    print(\"\\n\");\n    \n    plt.show()","dede30ec":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:1\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","874921d2":"def summary(df):\n    summary = pd.DataFrame(df.dtypes, columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    return summary","d0a64bf7":"from scipy import stats\nfrom scipy.stats import norm, skew\n\ndef numeric_transform(df, var, transform_type = 'log'):\n    \"\"\" possible values: 'log', 'log1p', 'log10', 'sqrt' \"\"\"\n    print('Transform with: '+ transform_type+\"\\n\");\n    \n    # Plot histogram and probability\n    fig = plt.figure(figsize=(15,5))\n    plt.subplot(1,2,1)\n    sns.distplot(df[var] , fit=norm);\n    (mu, sigma) = norm.fit(df[var])\n    print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n                loc='best')\n    plt.ylabel('Frequency')\n    plt.title(var+' distribution')\n    plt.subplot(1,2,2)\n    res = stats.probplot(df[var], plot=plt)\n    plt.suptitle('Before transformation')\n    \n    # Apply transformation\n    if transform_type == 'log':\n        df[var] = np.log(df[var])\n    elif transform_type == 'log10':\n        df[var] = np.log10(df[var] )\n    elif transform_type == 'sqrt':\n        df[var] = np.sqrt(df[var] )\n        \n    # New prediction\n    y_train = df[var].values\n    y_train_orig = df[var]\n\n    # Plot histogram and probability after transformation\n    fig = plt.figure(figsize=(15,5))\n    plt.subplot(1,2,1)\n    sns.distplot(df[var] , fit=norm);\n    (mu, sigma) = norm.fit(df[var])\n    print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n                loc='best')\n    plt.ylabel('Frequency')\n    plt.title(var+' distribution')\n    plt.subplot(1,2,2)\n    res = stats.probplot(df[var], plot=plt)\n    plt.suptitle('After transformation')","8337db27":"def to_numeric_mean(df, cols):\n    for col in cols:\n        data.loc[data[col].isnull(), col] = 0\n        data[col] = pd.to_numeric(data[col])\n        data[col].fillna((data[col].mean()), inplace=True)\n    return df","1b4e1f3c":"def to_numeric_most_frequent(df, cols):\n    for col in cols:\n        data.loc[data[col].isnull(), col] = 0\n        data[col] = pd.to_numeric(data[col])\n        data[col].fillna((data[col].value_counts().index[0]), inplace=True)\n    return df","f6f4965b":"data = pd.read_csv(\"..\/input\/heart-attack-prediction\/data.csv\")","de74f625":"data.shape","37b8b683":"summary(data)","f9913730":"data.head()","f1633bbe":"data.replace('?', np.nan, inplace=True)","dd5d5196":"data.rename({'num       ':'num'},axis=1,inplace=True)","efed96fd":"summary(data)","2cbc0c33":"data = data.drop(columns=['slope', 'ca', 'thal'])","3b85a4bd":"data = to_numeric_mean(data, ['chol', 'trestbps', 'thalach'])","97e39ecd":"data = to_numeric_mean(data, ['restecg', 'exang', 'fbs'])","246f9096":"summary(data)","9c55b9c9":"sns.countplot(data['num'])","a719f4cd":"count_no_lead = len(data[data['num']==0])\ncount_lead = len(data[data['num']==1])\npct_of_no_sub = count_no_lead\/(count_no_lead+count_lead)*100\npct_of_sub = count_lead\/(count_no_lead + count_lead)*100\nprint('{} {} % Sales '.format(count_lead, pct_of_sub))\nprint('{} {} % No-Sales '.format(count_no_lead, pct_of_no_sub))","5d839ef1":"data.corrwith(data.num).plot.bar(figsize=(20,15), \n                                              title=\"Correlation with the response Variable\", \n                                              fontsize=15, rot=90, grid=True)","efacf0bb":"plt.figure(figsize=(25, 15))\ncorr = data.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, mask=mask, square=True, cmap='RdBu_r', vmin=-1, vmax=1, annot=True, fmt='.2f');","c0fed758":"from sklearn.preprocessing import MinMaxScaler\ncol_names = data.columns\nfeatures = data[col_names]\nscaler = MinMaxScaler(feature_range = (0,1)).fit(features.values)\nfeatures = scaler.transform(features.values)\ndata[col_names] = features","a06de2d4":"data.head()","c81d0223":"X = data.drop(columns=['num'])\ny = data['num']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n\nprint('Shape of X: {}'.format(X.shape))\nprint('Shape of y: {}'.format(y.shape))\nprint (\"\\n\")\nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Counts of label '0': {}\".format(sum(y_train==0)))\nprint(\"Counts of label '1': {}\".format(sum(y_train==1)))\nprint (\"\\n\")\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)\nprint(\"Counts of label '0': {}\".format(sum(y_test==0)))\nprint(\"Counts of label '1': {}\".format(sum(y_test==1)))","e97690d2":"from sklearn.linear_model import LogisticRegression\n\n# Logistic Regression with class weight 1:3 for the minority class\nclf = LogisticRegression(\n    class_weight={0:1,1:3},\n    n_jobs=-1 # Use all CPU\n)\n\n# train the classifier\nclf.fit(X_train, y_train)\n\n# predict on the test data\npred_y = clf.predict(X_test)","f6824bfa":"plot_cf_matrix_and_roc(clf, X_train, y_train, X_test, y_test, pred_y , classes=['NO','YES'])","8b51f3d4":"# set different parameters\nparams = [{\n    'solver': ['newton-cg', 'lbfgs', 'sag'],\n    'C': [0.3, 0.5, 0.7, 1, 1.3],\n    'penalty': ['l2']\n    },{\n    'solver': ['liblinear','saga'],\n    'C': [0.3, 0.5, 0.7, 1, 1.3],\n    'penalty': ['l1','l2']\n}]\n\n# Logistic Regression with class weight 1:3 for the minority class\nclf = LogisticRegression(\n    class_weight={0:1,1:3},\n    n_jobs=-1 # Use all CPU\n)\n\n# load GridSearchCV for the best parameter evaluation\nsearch = GridSearchCV(\n    estimator=clf,\n    param_grid=params,\n    n_jobs=-1,\n    scoring='recall'\n)\n\n# train search object\nsearch.fit(X_train, y_train)\n\n# predict the on the test data\npred_y_p = search.predict(X_test)","024eac83":"plot_cf_matrix_and_roc(search, X_train, y_train, X_test, y_test, pred_y_p , classes=['NO','YES'])","e36231ff":"# Cross-validate the recall score on the complete data with Kfold\nfrom sklearn.model_selection import StratifiedKFold\nkf = StratifiedKFold(shuffle=True, n_splits=5)\ncv_results = cross_val_score(X=X,y=y,estimator=search, cv=kf, scoring='recall')\nprint(\"KFold: Cross-validation recall scores:\",cv_results)\nprint(\"Mean recall score:\",cv_results.mean())","89baaec7":"## Correlation with the target variable","39862677":"## Drop the columns with more than 5% of NaN values","55aa9c6c":"## Replace the nan values and convert to numeric","d439ef53":"## Optimize the parameters for better classifier performance","fbc743fd":"## Classifier: Logistic Regression with class_weight 1:3","bd4881c3":"## Split the data to train (70%) and test (30%)","50ef82a1":"## Standardisation for all variables","94d07231":"## Crossvalidate the model","68421ce5":"# Heart Attack Prediction"}}