{"cell_type":{"fd78129e":"code","17ad1732":"code","2bbffb4a":"code","79f70e98":"code","6df7f6fc":"code","a01e006a":"code","943761df":"code","753bf08f":"code","437dac09":"code","a040c7c8":"code","f763245c":"code","e68fd735":"code","824d9ed4":"code","504b49a3":"code","5d692bb2":"code","a1dd5666":"code","6b63e848":"code","216d0bf0":"code","b62136d9":"code","dd4f1506":"markdown","b71e5b7f":"markdown","4c7a4ccb":"markdown","363d1885":"markdown","bc2bb067":"markdown","50b3334d":"markdown","76d62ff1":"markdown","86e60064":"markdown","bddc1a4a":"markdown","b33c68da":"markdown","24cfc569":"markdown","85380c57":"markdown","996847af":"markdown"},"source":{"fd78129e":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='darkgrid')","17ad1732":"iris=pd.read_csv('..\/input\/iris\/Iris.csv')","2bbffb4a":"iris.columns","79f70e98":"iris.shape","6df7f6fc":"iris.info()\n\n# We don't have null values in our data, all seems fine","a01e006a":"iris.rename(columns={'SepalLengthCm':'sepal_length','SepalWidthCm':'sepal_width',\n                     'PetalLengthCm':'petal_length','PetalWidthCm':'petal_width'},inplace=True)","943761df":"iris.head()","753bf08f":"iris.describe()  ","437dac09":"# We can see the descriptive statistics grouped by species\niris.groupby('Species').describe()","a040c7c8":"sns.heatmap(iris.drop('Id',axis=1).corr(),annot=True)","f763245c":"sns.pairplot(iris.drop('Id',axis=1),hue='Species',palette='bright')","e68fd735":"# For visualizing the summary statistics for each species, we use boxplot \nspecies=iris.Species.unique()\nk=1\nplt.figure(figsize=(21,8))\nfor i in species:\n    plt.subplot(1,3,k)\n    k=k+1\n    sns.boxplot(data=iris.drop('Id',axis=1)[iris.Species==i],width=0.5,fliersize=5)\n    plt.title(str('Boxplot'+i.upper()))","824d9ed4":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\nX=iris.drop(['Species','Id'],axis=1) # This is also called the independent value (training data)\ny=iris.Species # This is also called the dependent value (the target), the value obtained through the independent values","504b49a3":"def prediction(k,train,target):\n    knn=KNeighborsClassifier(n_neighbors=k)\n    knn.fit(train,target) \n    new=pd.DataFrame([[4.5,2.3,3.1,2],[6,2.1,3,4.8]]) # I added 2 random observations \n    new_obsv=knn.predict(new)\n    X_train, X_test, y_train, y_test= train_test_split(train,target,test_size=0.3,random_state=21,stratify=target)\n    knn.fit(X_train,y_train)\n    knn.predict(X_test)\n    print('This observations belong to :', new_obsv,'with an accuracy of:',knn.score(X_test,y_test))\n    ","5d692bb2":"prediction(6,X,y)\nprediction(9,X,y)\n\n# The accuracy for these 2 are different, so a plot will be useful for visualizing the influence of the neighbors ","a1dd5666":"X_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.3,random_state=21,stratify=y)\nneighbors=np.arange(1,100)\naccuracy_list=np.empty(len(neighbors))\nfor i,k in enumerate(neighbors):\n    knn=KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train,y_train)\n    knn.predict(X_test)\n    accuracy_list[i]=knn.score(X_test,y_test)","6b63e848":"from bokeh.io import output_notebook, output_file, show\nfrom bokeh.plotting import figure\noutput_notebook()\n\np1=figure(plot_height=500,plot_width=900,title='The influence of neighbors numbers on accuracy',\n          x_axis_label='Number of neighbors',y_axis_label='Accuracy of KNN model')\np1.line(x=neighbors,y=accuracy_list)\np1.circle(x=neighbors,y=accuracy_list)\nshow(p1)","216d0bf0":"neigh = [i for i, x in enumerate(accuracy_list) if x == max(accuracy_list)]\naccuracy_list=list(accuracy_list)","b62136d9":"for i in neigh:\n    print('The maxim accuracy is :',max(accuracy_list),' number of neighbours:',i+1)","dd4f1506":"![](https:\/\/s3.amazonaws.com\/assets.datacamp.com\/blog_assets\/Machine+Learning+R\/iris-machinelearning.png)","b71e5b7f":"# 2. Prepare Data \n**a) Cleaning Data **","4c7a4ccb":" From these graphics we can see that the highest correlation is between petal_length and petal_width, which are POSITIVE correlated, meaning that an increasing in petal's length\nwill determine an increasing in petal's width. Also , a high correlation is between petal_length - sepal_length, followed by petal_width - sepal_length. A negative correlation, meaning that  an increasing will determine a decreasing, or inverse, is present between petal_length - sepal_width and petal_width - sepal_width. Also, Iris-virginica has a large petal's width & length, compared with the others 2 which are similar regarding petal's measurements. \n    ","363d1885":"I want to rename the columns for an easier work:","bc2bb067":"So these are the perfect numbers of neighbours for KNN.","50b3334d":"# Supervised learning \n****K-Nearest Neighbors****\n\nThrough this model we want to label a new set of measurements of petals and sepals. Because we want to see if these observations are setosa, virginica or versicolor (3 categories), this model is called *Classification* . Also, it's called *Supervised* learning because we know before the prediction which labels will be applied to the unlabeled data.","76d62ff1":"**Preparing the model and testing the accuracy of it **","86e60064":"**So, that's it. Feel free to ask questions or correct me if I mistaken anything. I will really apreciate it! Thank you.**","bddc1a4a":"**b) Data visualizations**\n","b33c68da":" Hello everyone! This is my first project into the world of Data Scientists, so it's not something too complicated.\n\n# 1.Prepare the problem\n **a) Loading the libraries**","24cfc569":"**b) Loading the dataset **","85380c57":" # 3.Explore Data\n**  a) Descriptive statistics**","996847af":"We can observe how the accuracy is dropping when the number of neighbors is increasing"}}