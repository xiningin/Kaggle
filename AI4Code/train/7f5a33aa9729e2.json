{"cell_type":{"2af63981":"code","47ad3ba0":"code","a2868cfb":"code","17d2b4cf":"code","dcd91503":"code","da9c0433":"code","bb843cdc":"code","d3bb516c":"code","b8689acb":"code","01589684":"code","850f7e9a":"code","6614628d":"code","e85cee31":"code","5ec0d498":"code","301715b0":"code","c61a6735":"code","1ee9f1a5":"code","5790c58f":"code","f539a20e":"code","faa94599":"code","082b5c0f":"code","75e9b053":"code","328fa040":"code","92f7da4a":"code","a38c3c45":"code","66b714e9":"code","5f9413ad":"code","64422fe1":"code","dbad5840":"code","f800452b":"code","a21c93b6":"code","5baedb20":"code","0b2cf9e9":"code","8ac8c55c":"code","06a41c3e":"code","9e2df75b":"code","4e3fa9db":"code","83ce242d":"code","9a00118c":"code","53411402":"code","416b4121":"code","de08f276":"code","782a8253":"code","ea00bee6":"code","7263ac5b":"code","2ba7d0cb":"code","b9364d0b":"code","efebd22f":"code","087a58a8":"code","a1725eba":"code","04268f10":"code","0bff2d9f":"code","af84c32f":"code","c5e08cd7":"code","ab180e31":"code","f9ecebdb":"code","4958f2cd":"code","e8981de8":"code","28ec27c3":"code","b39f72e7":"code","74e183b0":"code","6a6ba819":"code","6b1ecb3f":"code","01c7d61a":"code","b273f079":"code","5ae7a50d":"code","6dc34b49":"code","bc9df998":"code","ab48f72d":"code","23622870":"code","641f13a6":"code","a94a33cd":"code","e21c98d3":"code","7d698504":"code","ff535d3a":"code","c4ce482a":"code","4992c994":"code","b3cf15ee":"code","535736b5":"code","96626606":"code","48295ccf":"code","340e5b06":"code","b6a9e5f1":"code","30b2e287":"code","cbe084d1":"code","fc2beb91":"code","62543aba":"code","58673be1":"code","12ac3ab4":"code","845bfae8":"code","2af409c1":"code","7e1b8ab5":"code","a9e92bb9":"code","5dc2cc18":"code","267f8bf9":"code","6185ab41":"code","420b0b88":"code","624979c2":"code","f625a566":"code","c2adf21b":"code","905cd507":"code","26a5f362":"code","20f1afc7":"code","98f227df":"code","839dea64":"code","76b7d013":"code","52628e18":"code","bcc2fa25":"code","fe23ae7b":"code","4182395d":"code","b1873a3c":"code","95292573":"code","f906d699":"code","496e63a6":"code","0ed6cfea":"code","849c4e08":"code","44c41444":"code","fe8435be":"code","eb417e00":"code","294f52f5":"code","6ce2f97c":"code","155fe1ca":"code","0990bad1":"code","cafc3874":"code","99a758ef":"code","d9b2cc3b":"code","cb887776":"code","2d0a6778":"code","54f8f661":"code","4775bda5":"code","3f6c1c51":"code","0954756b":"code","e2872adb":"code","6b056507":"code","3c066872":"code","61a9fa5d":"code","2e2f565b":"code","ddf85f58":"code","82cdcae7":"code","2cfa604f":"markdown","0738da73":"markdown","858d8751":"markdown","37998604":"markdown","c56f58ff":"markdown","162a35ee":"markdown","db2702d2":"markdown","4097bc2a":"markdown","d5f0654f":"markdown","6fd9f300":"markdown","fab8d1a9":"markdown","f621f35a":"markdown","a1b0b01c":"markdown","10723bec":"markdown","85d6dac6":"markdown","ffa69794":"markdown","bb170e1f":"markdown","fba3ac7e":"markdown","2765fab2":"markdown","e71a0e50":"markdown","a2900b28":"markdown","ebf1b016":"markdown","c364bf1c":"markdown","ab7326d4":"markdown","e97f68fa":"markdown","cad9823e":"markdown","943421dd":"markdown","e3d7a50c":"markdown","b01cca64":"markdown","79785049":"markdown","3ecab0d7":"markdown","abe9c790":"markdown","2ea862f4":"markdown","e658da2d":"markdown","d4b56a46":"markdown","7dbc20c7":"markdown","88fd8b45":"markdown","fcc0a691":"markdown","134905ff":"markdown","5eec0f37":"markdown","f617af46":"markdown","8d55f918":"markdown","068cf04e":"markdown","efdab1be":"markdown","478da0c0":"markdown","07438bd5":"markdown","b1985d2c":"markdown","128a5731":"markdown","105860b0":"markdown","6b103480":"markdown","9ef3b420":"markdown","bcf06cbf":"markdown","250ed6cf":"markdown","8457e25c":"markdown","b335ccaf":"markdown","45770f90":"markdown","a6cf0f3c":"markdown","466f07b1":"markdown","c56c8ef0":"markdown","c58a93b3":"markdown","f342ba9e":"markdown","a890abd6":"markdown","23803f5e":"markdown","78f7d2da":"markdown","02f799df":"markdown","1c2c115e":"markdown","1acaf125":"markdown","a3e042fa":"markdown","255ea1b2":"markdown","68614117":"markdown","82071b4d":"markdown","2c307539":"markdown","01e63d88":"markdown","51645623":"markdown","8d73afbb":"markdown","2e21a83a":"markdown","0c940bec":"markdown","eb397a1a":"markdown","9a332121":"markdown","3b17c6f0":"markdown","1d7115f3":"markdown","19c515bd":"markdown","d9cfe068":"markdown","43630ca5":"markdown","aac35bc0":"markdown","b402064f":"markdown","5aadb239":"markdown","922c8a77":"markdown","ee4f6d4a":"markdown","f631e459":"markdown","444e64e9":"markdown","fd014a28":"markdown","b8d88f83":"markdown","f36cd801":"markdown","65ad64ee":"markdown","dc4ac517":"markdown","960cdbe4":"markdown","6be23a2b":"markdown","720f9b07":"markdown"},"source":{"2af63981":"!pip install zipcodes # installing Zipcodes library .","47ad3ba0":"### IMPORT: ------------------------------------\nimport scipy.stats as stats \nimport pandas as pd\nimport numpy as np\nimport zipcodes as zcode # to get zipcodes\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\n\nimport statsmodels.api as sm\n#--Sklearn library--\n# Sklearn package's randomized data splitting function\nfrom sklearn.model_selection import train_test_split \n\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn import metrics\n#AUC ROC curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import precision_recall_curve\n\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay #to plot confusion matric\nfrom sklearn.metrics import plot_confusion_matrix\n\nfrom sklearn.linear_model import LogisticRegression #to build the model\nfrom sklearn.tree import DecisionTreeClassifier#to build the model\n\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.max_rows', 300)\npd.set_option('display.max_colwidth',400)\npd.set_option('display.float_format', lambda x: '%.5f' % x) \n# To supress numerical display in scientific notations\nwarnings.filterwarnings('ignore') # To supress warnings\n # set the background for the graphs\nplt.style.use('ggplot')\n","a2868cfb":"#Reading the csv file  used car data.csv \ndata_path='..\/input\/personal-loan-modeling\/Bank_Personal_Loan_Modelling.csv'\ndf=pd.read_csv(data_path)\ndf_loan=df.copy()\nprint(f'There are {df_loan.shape[0]} rows and {df_loan.shape[1]} columns') # fstring ","17d2b4cf":"df_loan.head()","dcd91503":"df_loan.tail()","da9c0433":"#get the size of dataframe\nprint (\"Rows     : \" , df_loan.shape[0])  #get number of rows\/observations\nprint (\"Columns  : \" , df_loan.shape[1]) #get number of columns\nprint (\"#\"*40,\"\\n\",\"Features : \\n\\n\", df_loan.columns.tolist()) #get name of columns\/features\nprint (\"#\"*40,\"\\nMissing values :\\n\\n\", df_loan.isnull().sum().sort_values(ascending=False))\nprint( \"#\"*40,\"\\nPercent of missing :\\n\\n\", round(df_loan.isna().sum() \/ df_loan.isna().count() * 100, 2)) # looking at columns with most Missing Values\nprint (\"#\"*40,\"\\nUnique values :  \\n\\n\", df_loan.nunique())  #  count of unique values\n","bb843cdc":"df_loan.info()","d3bb516c":"df_loan.sample(10)","b8689acb":"df_loan.drop(['ID'],axis=1,inplace=True) #droping id","01589684":"df_loan.rename(columns={\"ZIP Code\":\"ZIPCode\",\"Personal Loan\":\"PersonalLoan\",\"Securities Account\":\"SecuritiesAccount\",\"CD Account\":'CDAccount'},inplace=True)","850f7e9a":"df_loan.ZIPCode.nunique()","6614628d":"# get unique zipcodes\nlist_zipcode=df_loan.ZIPCode.unique()\n\n","e85cee31":"#here i am creating a dictionary of county by using library zipcode and matching method.\ndict_zip={}\nfor zipcode in list_zipcode:\n    my_city_county = zcode.matching(zipcode.astype('str'))\n    if len(my_city_county)==1: # if  zipcode is present then get county else, assign zipcode to county\n        county=my_city_county[0].get('county')\n    else:\n        county=zipcode\n    \n    dict_zip.update({zipcode:county})\n      ","5ec0d498":"dict_zip","301715b0":"dict_zip.update({92717:'Orange County'})\ndict_zip.update({92634:'Orange County'})\n","c61a6735":"df_loan['County']=df_loan['ZIPCode'].map(dict_zip)\n","1ee9f1a5":"df_loan.County.nunique()","5790c58f":"df_loan.info()","f539a20e":"# converting categorical varaible to category type\ncategory_col = ['PersonalLoan', 'SecuritiesAccount','Family', 'CDAccount', 'Online', 'CreditCard', 'ZIPCode', 'Education','County']\ndf_loan[category_col] = df_loan[category_col].astype('category')\n    ","faa94599":"df_loan.info()","082b5c0f":"# checking negative and zero values for experience. \ndf_loan[df_loan['Experience']<0]['Age'].describe()","75e9b053":"df_loan[df_loan['Experience']<0].sort_values(by='Experience',ascending=True)","328fa040":"df_loan.groupby(['Age','Education'])['Experience'].describe().T","92f7da4a":"df_loan.loc[df_loan['Experience']<0,'Experience']=np.abs(df_loan['Experience'])","a38c3c45":"df_loan[df_loan['Experience']==0]['Age'].describe()","66b714e9":"df_loan.describe().T","5f9413ad":"for column in category_col:\n    print(df_loan[column].value_counts())\n    print(\"#\" * 40)\n","64422fe1":"def dist_box(data):\n # function plots a combined graph for univariate analysis of continous variable \n #to check spread, central tendency , dispersion and outliers  \n    Name=data.name.upper()\n    fig,(ax_box,ax_dis)  =plt.subplots(nrows=2,sharex=True,gridspec_kw = {\"height_ratios\": (.25, .75)},figsize=(8, 5))\n    mean=data.mean()\n    median=data.median()\n    mode=data.mode().tolist()[0]\n    sns.set_theme(style=\"white\")\n    fig.suptitle(\"SPREAD OF DATA FOR \"+ Name  , fontsize=18, fontweight='bold')\n    sns.boxplot(x=data,showmeans=True, orient='h',color=\"violet\",ax=ax_box)\n    ax_box.set(xlabel='')\n     # just trying to make visualisation better. This will set background to white\n    sns.despine(top=True,right=True,left=True) # to remove side line from graph\n    sns.distplot(data,kde=False,color='blue',ax=ax_dis)\n    ax_dis.axvline(mean, color='r', linestyle='--',linewidth=2)\n    ax_dis.axvline(median, color='g', linestyle='-',linewidth=2)\n    ax_dis.axvline(mode, color='y', linestyle='-',linewidth=2)\n    plt.legend({'Mean':mean,'Median':median,'Mode':mode})\n                    ","dbad5840":"#select all quantitative columns for checking the spread\nlist_col=  ['Age','Experience','Income','CCAvg','Mortgage']\nfor i in range(len(list_col)):\n    dist_box(df_loan[list_col[i]])","f800452b":"df_loan['Agebin'] = pd.cut(df_loan['Age'], bins = [0, 30, 40, 50, 60, 100], labels = ['18-30', '31-40', '41-50', '51-60', '60-100'])","a21c93b6":"# Create a new variable - Income group\n\ndf_loan[\"Income_group\"] = pd.cut(\n    x=df[\"Income\"],\n    bins=[0, 50, 140, 224],\n    labels=[\"Lower\", \"Middle\", \"High\"],\n)\n","5baedb20":"df_loan.CCAvg.describe()","0b2cf9e9":"df_loan[\"Spending_group\"] = pd.cut( x=df_loan[\"CCAvg\"], bins=[0.00000, 0.70000, 2.50000, 10.00000],\n    labels=[\"Low\", \"Medium\", \"High\"],include_lowest=True ,\n)\n","8ac8c55c":"# Making a list of all categorical variables\ncat_columns = ['Family','Education','PersonalLoan','SecuritiesAccount',\n               'CDAccount','Online','CreditCard','Agebin','Income_group','Spending_group']\ntitle=['Number of Family','Education','Customers who took Personal Loan',\n       ' Customer has Securities Account','Customers has a CD Account',\n       'Customers  who transcat  Online',' Customers who has  Credit Card','Agebins',\"Income group\",'Spending group']\nplt.figure(figsize=(14,20))\n\nsns.set_theme(style=\"white\") # just trying to make visualisation better. This will set background to white\n#list_palette=['Blues_r','Greens_r','Purples_r','Reds_r','Blues_r','Greens_r','Purples_r','Reds_r','Blues_r']\n\nfor i, variable in enumerate(cat_columns):\n                     plt.subplot(5,2,i+1)\n                     order = df_loan[variable].value_counts(ascending=False).index   \n                     #sns.set_palette(list_palette[i]) # to set the palette\n                     sns.set_palette('Set2')\n                     ax=sns.countplot(x=df_loan[variable], data=df_loan )\n                     sns.despine(top=True,right=True,left=True) # to remove side line from graph\n                     for p in ax.patches:\n                           percentage = '{:.1f}%'.format(100 * p.get_height()\/len(df_loan[variable]))\n                           x = p.get_x() + p.get_width() \/ 2 - 0.05\n                           y = p.get_y() + p.get_height()\n                           plt.annotate(percentage, (x, y),ha='center')\n                     plt.tight_layout()\n                     plt.title(title[i].upper())\n                                     \n","06a41c3e":"df_loan.groupby(['County','PersonalLoan'])['PersonalLoan'].agg({'size'}).unstack()\n\n","9e2df75b":"plt.figure(figsize=(15,24))\n\npd.crosstab(index=df_loan['County'],columns=df_loan['PersonalLoan'].sort_values(ascending=False)).plot(kind='barh',stacked=True,figsize=(15,24))","4e3fa9db":"counties = {\n'Los Angeles County':'Los Angeles Region',\n'San Diego County':'Southern',\n'Santa Clara County':'Bay Area',\n'Alameda County':'Bay Area',\n'Orange County':'Southern',\n'San Francisco County':'Bay Area',\n'San Mateo County':'Bay Area',\n'Sacramento County':'Central',\n'Santa Barbara County':'Southern',\n'Yolo County':'Central',\n'Monterey County':'Bay Area',            \n'Ventura County':'Southern',             \n'San Bernardino County':'Southern',       \n'Contra Costa County':'Bay Area',        \n'Santa Cruz County':'Bay Area',           \n'Riverside County':'Southern',            \n'Kern County':'Southern',                 \n'Marin County':'Bay Area',                \n'San Luis Obispo County':'Southern',     \n'Solano County':'Bay Area',              \n'Humboldt County':'Superior',            \n'Sonoma County':'Bay Area',                \n'Fresno County':'Central',               \n'Placer County':'Central',                \n'Butte County':'Superior',               \n'Shasta County':'Superior',                \n'El Dorado County':'Central',             \n'Stanislaus County':'Central',            \n'San Benito County':'Bay Area',          \n'San Joaquin County':'Central',           \n'Mendocino County':'Superior',             \n'Tuolumne County':'Central',                \n'Siskiyou County':'Superior',              \n'Trinity County':'Superior',                \n'Merced County':'Central',                  \n'Lake County':'Superior',                 \n'Napa County':'Bay Area',                   \n'Imperial County':'Southern',\n93077:'Southern',\n96651:'Bay Area'\n}","83ce242d":"\ndf_loan['Regions'] = df_loan['County'].map(counties)","9a00118c":"df_loan['Regions'].unique()","53411402":"df_loan.isnull().sum()","416b4121":"df_loan.dropna(inplace=True)","de08f276":"plt.figure(figsize=(9,7))\nsns.countplot(data=df_loan,x=df_loan['Regions'])\nsns.despine(top=True,right=True,left=True) # to remove side line from graph","782a8253":"df_loan.info()","ea00bee6":"sns.set_palette(sns.color_palette(\"Set2\", 8))\nplt.figure(figsize=(15,10))\nsns.heatmap(df_loan.corr(),annot=True)\nplt.show()","7263ac5b":"sns.set_palette(sns.color_palette(\"Set2\", 8))\nsns.pairplot(df_loan, hue=\"PersonalLoan\",corner=True)\nplt.show()","2ba7d0cb":"numeric_columns = ['Age','Experience','Income','CCAvg','Mortgage']\nplt.figure(figsize=(15,25))\n\nsns.set_palette(sns.color_palette(\"Set2\", 8))\nfor i, variable in enumerate(numeric_columns):\n        plt.subplot(10,3,i+1)\n        \n        sns.boxplot(x='PersonalLoan',y= df_loan[variable], data=df_loan)     \n        sns.despine(top=True,right=True,left=True) # to remove side line from graph\n        plt.tight_layout()\n        plt.title(variable.upper())","b9364d0b":"\nsns.distplot( df_loan[df_loan['PersonalLoan'] == 0]['CCAvg'], color = 'g')\nsns.distplot( df_loan[df_loan['PersonalLoan'] == 1]['CCAvg'], color = 'r')\nsns.despine(top=True,right=True,left=True) # to remove side line from graph","efebd22f":"sns.distplot( df_loan[df_loan['PersonalLoan'] == 0]['Income'], color = 'g')\nsns.distplot( df_loan[df_loan['PersonalLoan'] == 1]['Income'], color = 'r')\nsns.despine(top=True,right=True,left=True) # to remove side line from graph\n","087a58a8":"sns.relplot(x='Income_group',y='CCAvg',hue='PersonalLoan',data=df_loan)\nsns.despine(top=True,right=True,left=True) # to remove side line from graph\n\n","a1725eba":"sns.distplot( df_loan[df_loan['PersonalLoan'] == 0]['Mortgage'], color = 'g')\nsns.distplot( df_loan[df_loan['PersonalLoan'] == 1]['Mortgage'], color = 'r')\nsns.despine(top=True,right=True,left=True) # to remove side line from graph","04268f10":"sns.swarmplot(x='Income_group',y='Mortgage',hue='PersonalLoan',data=df_loan)\nsns.despine(top=True,right=True,left=True) # to remove side line from graph","0bff2d9f":"sns.swarmplot(x='Education',y='Income',hue='PersonalLoan',data=df_loan)\nsns.despine(top=True,right=True,left=True) # to remove side line from graph\nlabels=[\"No\",\"Yes\"]\nplt.legend(loc='lower left', frameon=False,)\nplt.legend(loc=\"upper left\", labels=labels,title=\"Borrowed Loan\",bbox_to_anchor=(1,1))","af84c32f":"sns.countplot(x='Income_group',hue='CDAccount',data=df_loan)\nsns.despine(top=True,right=True,left=True) # to remove side line from graph","c5e08cd7":"sns.distplot( df_loan[df_loan['PersonalLoan'] == 0]['Age'], color = 'g')\nsns.distplot( df_loan[df_loan['PersonalLoan'] == 1]['Age'], color = 'r')\nsns.despine(top=True,right=True,left=True) # to remove side line from graph\n","ab180e31":"\nsns.countplot(x='Spending_group',hue='PersonalLoan',data=df_loan)\n\nsns.despine(top=True,right=True,left=True) # to remove side line from graph\n","f9ecebdb":"sns.catplot(y='Income',x='Agebin',hue='Education',kind='bar',col=\"PersonalLoan\", data=df_loan)\nsns.despine(top=True,right=True,left=True) # to remove side line from graph","4958f2cd":"sns.set_palette(sns.color_palette(\"Set2\", 8))\nsns.barplot(y='CCAvg',x='Education', hue='PersonalLoan',data=df_loan)\nlabels=[\"No\",\"Yes\"]\nplt.legend(loc=\"upper left\", title=\"Borrowed Loan\",bbox_to_anchor=(1,1))\nsns.despine(top=True,right=True,left=True) # to remove side line from graph","e8981de8":"## Function to plot stacked bar chart\ndef stacked_plot(x):\n    sns.set_palette(sns.color_palette(\"Set2\", 8))\n    tab1 = pd.crosstab(x,df_loan['PersonalLoan'],margins=True)\n    print(tab1)\n    print('-'*120)\n    tab = pd.crosstab(x,df_loan['PersonalLoan'],normalize='index')\n    tab.plot(kind='bar',stacked=True,figsize=(7,4))\n    plt.xticks(rotation=360)\n    labels=[\"No\",\"Yes\"]\n    plt.legend(loc='lower left', frameon=False,)\n    plt.legend(loc=\"upper left\", labels=labels,title=\"Borrowed Loan\",bbox_to_anchor=(1,1))\n    sns.despine(top=True,right=True,left=True) # to remove side line from graph\n    #plt.legend(labels)\n    plt.show()","28ec27c3":"\ncat_columns=['Family','Education','SecuritiesAccount','CDAccount','CreditCard','Online','Regions','Agebin','Income_group','Spending_group']\nfor i, variable in enumerate(cat_columns):\n       stacked_plot(df_loan[variable])","b39f72e7":"plt.figure(figsize=(10,5))\nnumerical=['Income','Age','Experience','CCAvg','Mortgage']\n\n# A better color to see the positive or negative correlation of each variable\nheatmap = sns.heatmap(df_loan[numerical].corr(), annot=True, cmap='YlGnBu',linewidths=0.5)\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=2);","74e183b0":"\nplt.pie(data=df_loan,x=df_loan[\"PersonalLoan\"].value_counts(),autopct='%1.1f%%')","6a6ba819":"df_loan.head()","6b1ecb3f":"\n# Saving dataset before treating outliers for logistic regression.\ndf_Decision = df_loan.copy()\n","01c7d61a":"numeric_columns =['Income','CCAvg','Mortgage','Age']\n# outlier detection using boxplot\nplt.figure(figsize=(20,30))\n\nfor i, variable in enumerate(numeric_columns):\n                     plt.subplot(4,4,i+1)\n                     plt.boxplot(df_loan[variable],whis=1.5)\n                     plt.tight_layout()\n                     plt.title(variable)\n\nplt.show()","b273f079":"# Check Income extreme values\ndf_loan.sort_values(by=[\"Income\"],ascending = False).head(5)","5ae7a50d":"df_loan.loc[(df_loan['Age']==48) & (df_loan['Experience']==24)].sort_values(by=['Income'],ascending=False).head(5)","6dc34b49":"# Check Mortgage extreme values\ndf_loan.sort_values(by=[\"Mortgage\"],ascending = False).head(5)","bc9df998":"# Check CCAVg extreme values\ndf_loan.sort_values(by=[\"CCAvg\"],ascending = False).head(5)","ab48f72d":"df_loan","23622870":"df_loan.info()","641f13a6":"#drop column which we don't need for modelling\ndf_loan.drop(columns=[\"Agebin\", \"ZIPCode\",\"County\",'Experience','Income_group','Spending_group'], inplace=True)","a94a33cd":"df_loan.info()","e21c98d3":"X = df_loan.drop(['PersonalLoan'], axis=1)\nY = df_loan['PersonalLoan']\n\noneHotCols=['Regions','Education']\nX=pd.get_dummies(X,columns=oneHotCols,drop_first=True)","7d698504":"#Splitting data in train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.30, random_state = 1,stratify=Y)","ff535d3a":"from sklearn.preprocessing import StandardScaler\n# Creating StandardScaler instance\nscaler = StandardScaler()\n\n# Fitting Standard Scaller\nX_scaler = scaler.fit(X_train)\n\n# Scaling data\nX_train_scaled = X_scaler.transform(X_train)\nX_test_scaled = X_scaler.transform(X_test)\n\nX_train_scaled_df = pd.DataFrame(X_train_scaled,columns=X_train.columns)\nX_test_scaled_df = pd.DataFrame(X_test_scaled,columns=X_test.columns)\n\n\nX_train_scaled_df.index=np.arange(len(X_train_scaled_df))\nX_test_scaled_df.index=np.arange(len(X_test_scaled_df))\ny_train.index=np.arange(len(y_train))\ny_test.index=np.arange(len(y_test))\n","c4ce482a":"def make_confusion_matrix(y_actual,y_predict,title):\n    fig, ax = plt.subplots(1, 1)\n    \n    cm = confusion_matrix(y_actual, y_predict, labels=[0,1])\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                               display_labels=[\"No\",\"Yes\"])\n    disp.plot(cmap='Greens',colorbar=True,ax=ax)\n    ax.set_title(title)\n    plt.tick_params(axis=u'both', which=u'both',length=0)\n    plt.grid(b=None,axis='both',which='both',visible=False)\n    plt.show()\n   ","4992c994":"def get_metrics_score(model,X_train_df,X_test_df,y_train_pass,y_test_pass,statsklearn,threshold=0.5,flag=True,roc=False):\n    '''\n    Function to calculate different metric scores of the model - Accuracy, Recall, Precision, and F1 score\n    model: classifier to predict values of X\n    X_train_df, X_test_df: Independent features\n    y_train_pass,y_test_pass: Dependent variable\n    statsklearn : 0 if calling for Sklearn model else 1\n    threshold: thresold for classifiying the observation as 1\n    flag: If the flag is set to True then only the print statements showing different will be displayed. The default value is set to True.\n    roc: If the roc is set to True then only roc score will be displayed. The default value is set to False.\n    '''\n    # defining an empty list to store train and test results\n    \n    score_list=[] \n    if statsklearn==0:\n        pred_train = model.predict(X_train_df)\n        pred_test = model.predict(X_test_df)\n    else:\n        pred_train = (model.predict(X_train_df)>threshold)\n        pred_test = (model.predict(X_test_df)>threshold)\n    \n    \n    pred_train = np.round(pred_train)\n    pred_test = np.round(pred_test)\n    \n    train_acc = accuracy_score(y_train_pass,pred_train)\n    test_acc = accuracy_score(y_test_pass,pred_test)\n    \n    train_recall = recall_score(y_train_pass,pred_train)\n    test_recall = recall_score(y_test_pass,pred_test)\n    \n    train_precision = precision_score(y_train_pass,pred_train)\n    test_precision = precision_score(y_test_pass,pred_test)\n    \n    train_f1 = f1_score(y_train_pass,pred_train)\n    test_f1 = f1_score(y_test_pass,pred_test)\n    \n    \n    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision,train_f1,test_f1))\n      \n    if flag == True: \n        print(\"\\x1b[0;30;47m \\033[1mMODEL PERFORMANCE\\x1b[0m\")\n        print(\"\\x1b[0;30;47m \\033[1mAccuracy   : Train:\\x1b[0m\",\n              round(accuracy_score(y_train_pass,pred_train),3),\n              \"\\x1b[0;30;47m \\033[1mTest:\\x1b[0m \",\n              round(accuracy_score(y_test_pass,pred_test),3))\n        print(\"\\x1b[0;30;47m \\033[1mRecall     : Train:\\x1b[0m\"\n              ,round(recall_score(y_train_pass,pred_train),3),\n              \"\\x1b[0;30;47m \\033[1mTest:\\x1b[0m\" ,\n              round(recall_score(y_test_pass,pred_test),3))\n        \n        print(\"\\x1b[0;30;47m \\033[1mPrecision  : Train:\\x1b[0m\",\n              round(precision_score(y_train_pass,pred_train),3),\n              \"\\x1b[0;30;47m \\033[1mTest:\\x1b[0m \",\n              round(precision_score(y_test_pass,pred_test),3))\n        print(\"\\x1b[0;30;47m \\033[1mF1         : Train:\\x1b[0m\",\n              round(f1_score(y_train_pass,pred_train),3),\n              \"\\x1b[0;30;47m \\033[1mTest:\\x1b[0m\",\n              round(f1_score(y_test_pass,pred_test),3))\n        make_confusion_matrix(y_train_pass,pred_train,\"Confusion Matrix for Train\")     \n        make_confusion_matrix(y_test_pass,pred_test,\"Confusion Matrix for Test\") \n   \n    if roc == True:\n        \n        print(\"\\x1b[0;30;47m \\033[1mROC-AUC Score  :Train:\\x1b[0m: \",\n              round(roc_auc_score(y_train_pass,pred_train),3),\n              \"\\x1b[0;30;47m \\033[1mTest:\\x1b[0m: \",\n              round(roc_auc_score(y_test_pass,pred_test),3))\n    \n    return score_list # returning the list with train and test scores","b3cf15ee":"# # defining empty lists to add train and test results\nacc_train = []\nacc_test = []\nrecall_train = []\nrecall_test = []\nprecision_train = []\nprecision_test = []\nf1_train = []\nf1_test = []\n\ndef add_score_model(score):\n     '''Add scores to list so that we can compare all models score together'''   \n     acc_train.append(score[0])\n     acc_test.append(score[1])\n     recall_train.append(score[2])\n     recall_test.append(score[3])\n     precision_train.append(score[4])\n     precision_test.append(score[5])\n     f1_train.append(score[6])\n     f1_test.append(score[7])","535736b5":"lr = LogisticRegression(solver='newton-cg',random_state=1,fit_intercept=False,class_weight={0:0.15,1:0.85})\nmodel  = lr.fit(X_train_scaled_df,y_train)\n\nstatmodel=0  #0 for sklearn and 1 for statmodel\n\n# Let's check model performances for this model\nscores_Sklearn = get_metrics_score(model,X_train_scaled_df,X_test_scaled_df,y_train,y_test,statmodel)\n","96626606":"add_score_model(scores_Sklearn)","48295ccf":"# adding constant to training and test set\nX_train_stat = sm.add_constant(X_train_scaled_df)\nX_test_stat = sm.add_constant(X_test_scaled_df)\nstatmodel=1  #0 for sklearn and 1 for statmodel\nlogit = sm.Logit( y_train, X_train_stat.astype(float) )\nlg = logit.fit(warn_convergence=False)\n\n# Let's check model performances for this model\nscores_statmodel = get_metrics_score(lg,X_train_stat,X_test_stat,y_train,y_test,statmodel)\nlg.summary() ","340e5b06":"# changing datatype of colums to numeric for checking vif\nX_train_num = X_train_stat.astype(float).copy()","b6a9e5f1":"vif_series1 = pd.Series([variance_inflation_factor(X_train_num.values,i) for i in range(X_train_num.shape[1])],index=X_train_num.columns, dtype = float)\nprint('Series before feature selection: \\n\\n{}\\n'.format(vif_series1))","30b2e287":"lg.summary()","cbe084d1":"X_train1 = X_train_stat.drop(['Regions_Central', 'Regions_Los Angeles Region', 'Regions_Southern', 'Regions_Superior'], axis = 1)\nX_test1= X_test_stat.drop(['Regions_Central', 'Regions_Los Angeles Region', 'Regions_Southern', 'Regions_Superior'], axis = 1)\nlogit1 = sm.Logit(y_train, X_train1.astype(float))\nlg1 = logit1.fit(warn_convergence =False)\n\nlg1.summary()","fc2beb91":" X_train2 = X_train1.drop(['Mortgage'], axis = 1)\n X_test2= X_test1.drop(['Mortgage'], axis = 1)\n logit2 = sm.Logit(y_train, X_train2.astype(float))\n lg2 = logit2.fit()\n lg2.summary()","62543aba":" X_train3 = X_train2.drop(['Age'], axis = 1)\n X_test3= X_test2.drop(['Age'], axis = 1)\n logit3 = sm.Logit(y_train, X_train3.astype(float))\n lg3 = logit3.fit()\n lg3.summary()","58673be1":"# Let's check model performances for this model\n\nscores_statmodel = get_metrics_score(lg3,X_train3,X_test3,y_train,y_test,statmodel)\nadd_score_model(scores_statmodel)","12ac3ab4":"logit_roc_auc_train = roc_auc_score(y_train, lg3.predict(X_train3))\nfpr, tpr, thresholds = roc_curve(y_train, lg3.predict(X_train3))\nplt.figure(figsize=(7,5))\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc_train)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic Train data')\nplt.legend(loc=\"lower right\")\nplt.show()","845bfae8":"logit_roc_auc_test = roc_auc_score(y_test, lg3.predict(X_test3))\nfpr, tpr, thresholds = roc_curve(y_test, lg3.predict(X_test3))\nplt.figure(figsize=(7,5))\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc_test)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic- Test data')\nplt.legend(loc=\"lower right\")\nplt.show()","2af409c1":"#Calculate Odds Ratio, probability\n##create a data frame to collate Odds ratio, probability and p-value of the coef\nlgcoef = pd.DataFrame(lg3.params, columns=['coef'])\nlgcoef.loc[:, \"Odds Ratio\"] = np.exp(lgcoef.coef)\nlgcoef['Probability'] = lgcoef['Odds Ratio']\/(1+lgcoef['Odds Ratio'])\nlgcoef['Percentage Change of Odds']=(np.exp(lg3.params)-1)*100\nlgcoef['pval']=lg3.pvalues\npd.options.display.float_format = '{:.2f}'.format\nlgcoef = lgcoef.sort_values(by=\"Odds Ratio\", ascending=False)\nlgcoef","7e1b8ab5":"# Let's check model performances for this model\nscores_LR = get_metrics_score(lg3,X_train3,X_test3,y_train,y_test,statmodel)","a9e92bb9":"# Optimal threshold as per AUC-ROC curve\n# The optimal cut off would be where tpr is high and fpr is low\n#fpr, tpr, thresholds = metrics.roc_curve(y_test, lg2.predict(X_test2))\n\noptimal_idx = np.argmax(tpr - fpr)\noptimal_threshold_auc_roc = thresholds[optimal_idx]\nprint(optimal_threshold_auc_roc)","5dc2cc18":"scores_statmodel = get_metrics_score(lg3,X_train3,X_test3,y_train,y_test,statmodel,threshold=optimal_threshold_auc_roc,roc=True)\nadd_score_model(scores_statmodel)","267f8bf9":"y_scores=lg3.predict(X_train3)\nprec, rec, tre = precision_recall_curve(y_train, y_scores,)\n\ndef plot_prec_recall_vs_tresh(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], 'b--', label='precision')\n    plt.plot(thresholds, recalls[:-1], 'g--', label = 'recall')\n    plt.xlabel('Threshold')\n    plt.legend(loc='upper left')\n    plt.ylim([0,1])\nplt.figure(figsize=(10,7))\nplot_prec_recall_vs_tresh(prec, rec, tre)\nplt.show()","6185ab41":"optimal_threshold_curve = 0.3\n\nscores_opt_curve = get_metrics_score(lg3,X_train3,X_test3,y_train,y_test,statmodel,threshold=optimal_threshold_curve,roc=True)\nadd_score_model(scores_opt_curve)","420b0b88":"X_train_seq=X_train_stat\nX_test_seq=X_test_stat","624979c2":"from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n#from sklearn.lin\nX_train_seq.shape","f625a566":"\nstatmodel=0  #0 for sklearn and 1 for statmodel\n\nclf = LogisticRegression(solver='newton-cg',random_state=1,fit_intercept=False)\n# Build step forward feature selection\nsfs1 = sfs(clf,k_features = 16,forward=True,\n           floating=False, scoring= 'recall',\n           verbose=2,\n           cv=5)\n\n # Perform SFFS\nsfs1 = sfs1.fit(X_train_seq, y_train)","c2adf21b":"statmodel=0  #0 for sklearn and 1 for statmodel\n\nclf = LogisticRegression(solver='newton-cg',random_state=1,fit_intercept=False)\n# Build step forward feature selection\nsfs1 = sfs(clf,k_features = 11,forward=True,\n           floating=False, scoring= 'recall',\n           verbose=2,\n           cv=5)\n\n # Perform SFFS\nsfs1 = sfs1.fit(X_train_seq, y_train)\n","905cd507":"# Now Which features are important?\nfeat_cols = list(sfs1.k_feature_idx_)\nprint(feat_cols)","26a5f362":"X_train_seq.columns[feat_cols]","20f1afc7":"X_train_final = X_train_seq[X_train_seq.columns[feat_cols]]\nX_test_final = X_test_seq[X_train_final.columns]","98f227df":"lr = LogisticRegression(solver='newton-cg',random_state=1,fit_intercept=False)\nmodel  = lr.fit(X_train_final,y_train)\n\nstatmodel=0  #0 for sklearn and 1 for statmodel\n\n# Let's check model performances for this model\nscores_sfs = get_metrics_score(model,X_train_final,X_test_final,y_train,y_test,statmodel)\nadd_score_model(scores_sfs)","839dea64":"comparison_frame = pd.DataFrame({'Model':['Logistic Regression Model- Sklearn',\n                                          'Logistic Regression Model - Statsmodels',\n                                          'Logistic Regression - Optimal threshold = 0.092',\n                                          'Logistic Regression - Optimal threshold = 0.3',\n                                          'Logistic Regression - Sequential feature selection'\n                                          ],\n                                          'Train_Accuracy':acc_train, \n                                          'Test_Accuracy':acc_test,\n                                          'Train Recall':recall_train,\n                                          'Test Recall':recall_test, \n                                          'Train Precision':precision_train,\n                                          'Test Precision':precision_test,\n                                          'Train F1':f1_train,\n                                          'Test F1':f1_test\n                                })\n                                    \n    \n                                       \n\ncomparison_frame","76b7d013":"#drop column which we don't need for modelling\ndf_Decision.drop(columns=[\"Agebin\", \"ZIPCode\",\"County\",'Experience','Income_group','Spending_group'], inplace=True)","52628e18":"X_dt = df_Decision.drop('PersonalLoan', axis=1)\ny_dt = df_Decision['PersonalLoan']","bcc2fa25":"#oneHotCols=['Regions']\noneHotCols=X_dt.select_dtypes(exclude='number').columns.to_list()\nX_dt=pd.get_dummies(X_dt,columns=oneHotCols,drop_first=True)\n# Spliting data set\nX_train_dt, X_test_dt, y_train_dt, y_test_dt = train_test_split(X_dt, y_dt, test_size=0.3, random_state=1, stratify=y_dt)","fe23ae7b":"##  Function to calculate recall score\ndef get_recall_score(model):\n    '''\n    model : classifier to predict values of X\n\n    '''\n    ytrain_predict = model.predict(X_train_dt)\n    ytest_predict = model.predict(X_test_dt)\n    # accuracy on training set\n    print(\"\\x1b[0;30;47m \\033[1mAccuracy : Train :\\033[0m\", \n          model.score(X_train_dt,y_train_dt),\n          \"\\x1b[0;30;47m \\033[1mTest:\\033[0m\", \n          model.score(X_test_dt,y_test_dt))\n# accuracy on training set\n    print(\"\\x1b[0;30;47m \\033[1mRecall   : Train :\\033[0m\", \n          metrics.recall_score(y_train_dt,ytrain_predict),\n          \"\\x1b[0;30;47m \\033[1mTest:\\033[0m\", \n          metrics.recall_score(y_test_dt,ytest_predict))\n    make_confusion_matrix(y_train_dt,ytrain_predict,\"Confusion Matric on Train Data\")\n    make_confusion_matrix(y_test_dt,ytest_predict,\"Confusion Matric on Test Data\")","4182395d":"#since data is imbalanced adding weights\nmodel = DecisionTreeClassifier(criterion = 'gini',class_weight={0:0.15,1:0.85}, random_state=1)\nmodel.fit(X_train_dt, y_train_dt)\nget_recall_score(model)\n","b1873a3c":"column_names = list(X_dt.columns)\nfeature_names = column_names\nprint(feature_names)","95292573":"plt.figure(figsize=(20,30))\nfrom sklearn import tree\nfrom sklearn.model_selection import GridSearchCV\nout = tree.plot_tree(model,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)\nfor o in out:\n     arrow = o.arrow_patch\n     if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(1)\nplt.show()","f906d699":"# Text report showing the rules of a decision tree -\n\nprint(tree.export_text(model,feature_names=feature_names,show_weights=True))","496e63a6":"importances = model.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='purple', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()\n","0ed6cfea":"# Choose the type of classifier. \nestimator = DecisionTreeClassifier(random_state=1)\n\n# Grid of parameters to choose from\n\nparameters = {'max_depth': np.arange(1,10), \n              'min_samples_leaf': [1, 2, 5, 7, 10,15,20],\n              'max_leaf_nodes' : [5, 10,15,20,25,30],\n              }\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = metrics.make_scorer(metrics.recall_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer,cv=5)\ngrid_obj = grid_obj.fit(X_train_dt, y_train_dt)\n\n# Set the clf to the best combination of parameters\nestimator = grid_obj.best_estimator_\nestimator","849c4e08":"# Fit the best algorithm to the data. \nestimator.fit(X_train_dt, y_train_dt)\nytrain_predict=estimator.predict(X_train_dt)\nytest_predict=estimator.predict(X_test_dt)","44c41444":"plt.figure(figsize=(15,10))\n\nout = tree.plot_tree(estimator,feature_names=feature_names,filled=True,fontsize=9,node_ids=False,class_names=True)\nfor o in out:\n    arrow = o.arrow_patch\n    if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(1)\nplt.show()","fe8435be":"importances = estimator.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","eb417e00":"get_recall_score(estimator)","294f52f5":"clf = DecisionTreeClassifier(random_state=1)\npath = clf.cost_complexity_pruning_path(X_train_dt, y_train_dt)\nccp_alphas, impurities = path.ccp_alphas, path.impurities","6ce2f97c":"fig, ax = plt.subplots(figsize=(15,5))\nax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\nax.set_xlabel(\"effective alpha\")\nax.set_ylabel(\"total impurity of leaves\")\nax.set_title(\"Total Impurity vs effective alpha for training set\")\nplt.show()","155fe1ca":"clfs = []\naccuracy_train=[]\naccuracy_test=[]\nrecall_train=[]\nrecall_test=[]\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=1, ccp_alpha=ccp_alpha,class_weight = {0:0.15,1:0.85})\n    clf.fit(X_train_dt, y_train_dt)\n    y_train_pred=clf.predict(X_train_dt)\n    y_test_pred=clf.predict(X_test_dt)\n    accuracy_train.append(clf.score(X_train_dt,y_train_dt))\n    accuracy_test.append(clf.score(X_test_dt,y_test_dt))\n    recall_train.append(metrics.recall_score(y_train_dt,y_train_pred))\n    recall_test.append(metrics.recall_score(y_test_dt,y_test_pred))\n    clfs.append(clf)\n    \n","0990bad1":"fig, ax = plt.subplots(figsize=(10,5))\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"Recall\")\nax.set_title(\"Recall vs alpha for training and testing sets\")\nax.plot(ccp_alphas, recall_train, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, recall_test, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()\n   ","cafc3874":"best_model = DecisionTreeClassifier(ccp_alpha=0.002,\n                       class_weight={0: 0.15, 1: 0.85}, random_state=1)\nbest_model.fit(X_train_dt, y_train_dt)","99a758ef":"get_recall_score(best_model)\n","d9b2cc3b":"plt.figure(figsize=(15,10))\n\nout = tree.plot_tree(best_model,feature_names=feature_names,filled=True,fontsize=9,node_ids=False,class_names=True)\nfor o in out:\n    arrow = o.arrow_patch\n    if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(1)\nplt.show()","cb887776":"# Text report showing the rules of a decision tree -\n\nprint(tree.export_text(best_model,feature_names=feature_names,show_weights=True))","2d0a6778":"importances = best_model.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","54f8f661":"comparison_frame = pd.DataFrame({'Model':['Logisitic Regression with Optimal Threshold 0.104',\n                                          'Initial decision tree model',\n                                          'Decision treee with hyperparameter tuning',\n                                          'Decision tree with post-pruning'], \n                                          'Train_accuracy':[0.92,1,0.99,0.98],\n                                          'Test_accuracy':[0.91,0.98,0.98,0.97],\n                                          'Train_Recall':[0.90,1,0.92,0.98], \n                                          'Test_Recall':[0.88,0.86,0.84,0.96]})  \n\ncomparison_frame","4775bda5":"y_pred = best_model.predict(X_test_dt)\nprint(classification_report(y_test_dt,y_pred))\nmake_confusion_matrix(y_test,y_pred,\"confusion matrix on test\")","3f6c1c51":"misclass_df = X_test_dt.copy()","0954756b":"misclass_df['Actual']=y_test_dt\nmisclass_df['Predicted'] = y_pred\n","e2872adb":"plt.pie(data=misclass_df,x=misclass_df[\"Actual\"].value_counts(),autopct='%1.1f%%')\n","6b056507":"plt.pie(data=misclass_df,x=misclass_df[\"Predicted\"].value_counts(),autopct='%1.1f%%')","3c066872":"pd.crosstab(misclass_df['Predicted'],misclass_df['Actual'],margins=True)","61a9fa5d":"pd.crosstab(misclass_df['Predicted'],misclass_df['Actual'],normalize='index').plot(kind='bar',stacked=True)","2e2f565b":"# Rows that were classified as Negative when they were actually positive\nfn_rows = misclass_df[(misclass_df['Actual'] == 1) & (misclass_df['Predicted'] == 0)]\n","ddf85f58":"False_negative= df_Decision[df_Decision.index.isin(fn_rows.index.values)].copy()\nFalse_negative","82cdcae7":"# Rows that were classified as postive when they were actually negative\nfp_rows = misclass_df[(misclass_df['Actual'] == 0) & (misclass_df['Predicted'] == 1)]\nfp_rows\nFalse_Positive= df_Decision[df_Decision.index.isin(fp_rows.index.values)].copy()\nFalse_Positive","2cfa604f":"With `0.092` Threshold the Recall score has improved  from `68%` to `87%` on test data with 89% accuracy. \nAlso False negative  values has decreased to 18 from 46 for testdat. ROC-AUC score is 88 which is good.","0738da73":"<font size=2 color=\"blue\" style=\"font-family:TimesNewRoman\">","858d8751":"#### Cost Complexity Pruning","37998604":"### Univariate Analysis","c56f58ff":"Since we want higher Recall with higher accuracy  Optimal Threshold 0.3 seems to be a good choice. Lets explore a model with decison tree if this score can be improved further.","162a35ee":"### Now we will fit a sklearn model using these features only","db2702d2":" **we can see that the memory usage has decreased  from `547 to 266.`**","4097bc2a":"**It can be seen the percentage of loan taken from various country differ.There are so many county converting them to regions will help in our model**","d5f0654f":"**Checking age and income for customers with 0 experience. We will see how to impute this columns more after EDA.**","6fd9f300":"### Misclassification Analysis","fab8d1a9":"The target variable personal_loan is highly imbalanced where only 9.6% of the customers have previously opted for a personal loan in the dataset. This can be handled using weight or SMOTE.But for now we will carry with on without SMOTE","f621f35a":"### <a id='link1'>Summary of EDA<\/a>\n**Data Description:**\n\n* Dependent variable is the Personal_loan which is of categorical data type.\n* Age, Experience, Income,mortage ,CCavg are of integer type while other variables are of categorical type\n* There were no missing values in the dataset.\n\n**Data Cleaning:**\n\n* We observed that some observations where experience = -ve but since there was a strong correlation with age , we dropped experience.\n* There are 450 unique zipcode, we mapped it to counties. They were further mapped to regions to reduce the dimension of data and we now have only 5 distinct values in the data.\n* We also created Age bin ,Spending group and Incomegroup to analyse in there is any pattern in buying loan based on these.\n\n**Observations from EDA:**\n- People with higher income had opted for personal loan before.\n- People with high mortgages opted for loan.\n- Customers will higher average monthly credit usage have opted for loan.  \n- Customers with Family of 3 members had  borrowed the loans with the bank.\n- Education level 2: Graduate and 3: Advanced\/Professional  have borrowed loans with the bank.\n- Customers who had certificate of deposit  with the bank had previously borrowed loan\n- Majority of customers who did have Personal loan with the bank used Online facilities.\n- Majority customers who had take personal loan before are from LosAngeles region.\n- Ratio of borrowing loan is high in 30 and below and 60 and above customers.\n- The more income you get the more you spend and have a \"large than life\" lifestyle.\n\n##### Customer segmentation for borrowing  loan based on EDA\n - Customer with Higher income have higher mortages and higher monthly average spending.They also have certificate of deposit with the bank.They are our high profile clients.\n - Few Customer in medium income  group don't have higher mortages and have less average monthly credit card spending .They are average profile clients.\n - Customer in lower income group  have less mortages( few outliers are there) ,less monthly spending. They are our low profile clients.\n  \n   \n**Actions for data pre-processing:**\n\n* Many variables have outliers that need to be treated.\n* We can drop Experience, Country,Zipcode and Agebin,Income_group,Spending_group.","a1b0b01c":"Next, we train a decision tree using the effective alphas. We will set these values of alpha and pass it to the ccp_alpha parameter of our DecisionTreeClassifier. By looping over the alphas array, we will find the accuracy on both Train and Test parts of our dataset.\n\n","10723bec":"  \n- [Context](#Context) \n- [Data Set](#Data-Set)\n- [Problem](#Problem)\n- [Libraries](#Libraries)\n- [Read and Understand Data](#Read-and-Understand-data)\n- [Data Preprocessing](#Data-Preprocessing)\n- [Exploratory Data Analysis](#Exploratory-Data-Analysis) \n    - [Univariate Analysis](#Univariate-Analysis) \n    \n    - [Bivariate and Multivariate Analysis](#Bivariate-&-Multivariate-Analysis) \n- [Insights based on EDA](#Insights-based-on-EDA)\n- [Model Building Logistic Regression](#Model-building-Logistic-Regression)\n- [Model Building Decision Tree](#Model-building-Decision-Tree)\n- [Conclusion](#Conclusion) \n- [Actionable Insights & Recommendations](#Actionable-Insights-&-Recommendations)","85d6dac6":"# Data Preprocessing","ffa69794":"#### ROC-AUC curve \n","bb170e1f":"### Model evaluation criterion\n\n### Model can make wrong predictions as:\n1. Predicting a person will buy a loan but he actually doesn't.(Loss of Resource)\n2. Predicting a person will not buy a loan but he actually does.(Loss of Opportunity)\n\n### Which case is more important? \n* The whole purpose of the campagin is to bring in more customers. 2nd case is more important to us .A potential customer is missed by the sales\/marketing team .It's lost of opportunity.So we want to minimize this loss.\n\n### How to reduce losses?i.e need to reduce False Negatives ?\n* In this case, not being able to identify a potential customer is the biggest loss we can face. Hence, recall is the right metric to check the performance of the model.Banks wants Recall to be maximized, greater the recall lesser the chances of false negatives.\n* We can use accuracy but since the data is imbalanced it would not be the right metric to check the model performance.\n*  Therefore, `Recall` should be maximized, the greater the Recall higher the chances of identifying both the classes correctly.","fba3ac7e":"### **Right Metric to use:**\n**Here not able to identify a potential customer is the biggest loss we can face. Hence, Recall is the right metric to check the performance of the model .We have  recall as `68` on train and `67` on test. False negative are `107` and `47` on train and test. We can further improve this score  using Optimal threshold for ROC AUC curve and precision recall curve**","2765fab2":"#### Optimal threshold using AUC-ROC curve","e71a0e50":"# Exploratory Data Analysis","a2900b28":"# Model building Logistic Regression","ebf1b016":"**Insights:**\n\n**True Positives:**\n\nReality: A customer wanted to take personal Loan.\nModel Prediction: The customer will take personal loan.\nOutcome: The model is good.\n    \n**True Negatives**:\n\nReality: A customer didn't wanted to take personal loan.\nModel Prediction: The customer will not take personal loan.\nOutcome: The business is unaffected .\n\n**False Positives** :\n\nReality: A customer didn't want to take personal loan.\nModel Prediction: The customer will take personal loan.\nOutcome: The team which is targeting the potential customers would waste their resources on the customers who will not be buying a personal loan.\n\n**False Negatives**:\n\nReality: A customer wanted to take personal Loan.\nModel Prediction: The customer will not take personal loan.\nOutcome: The potential customer is missed by the salesteam. This is loss of oppurtunity. The purpose of campaign was to target such customers. If team knew about this customers, they could have offered some good APR \/interest rates.","c364bf1c":"**Observations**\n   \n- People with higher income had opted for personal loan before.\n    \n- People with high mortgages opted for loan.\n    \n- Customers with higher average monthly credit usage have opted for loan.    \n   \n- Customers with higher income had higher average credit card usage and mortgage.\n\n- Graduate and Advanced\/Professional have higher monhtly  credit card usage and have borrowed loans with the bank.\n\n","ab7326d4":"CCavg is important parameter as per EDA so not dropping it","e97f68fa":"#### Creating model with 0.002 ccp_alpha","cad9823e":"# Libraries","943421dd":"### Using Sequential Feature Selection","e3d7a50c":"# **Table of Contents**\n  ","b01cca64":"Percentage of value predicted by our model has been very close to the actual values. Lets find out False Negative and False Positive observations","79785049":"# Model building Decision Tree\n- Data preparation\n- Partition the data into train and test set.\n- Built a CART model on the train data.\n- Tune the model and prune the tree, if required.\n- Test the data on test set.","3ecab0d7":"### Income\nTo understand customers segments derving new columns which will help us identify if customer belongs to Upper , middle or lower income group ","abe9c790":"- Decision trees doesn't require to much data preparation or handling of outliers like logistic regression. They are easy to understand. Decision tress can easily overfit , so we have to be careful using decision tree.\n- Based on EDA, logistic Regression , Decision tree , Income ,Educatoin,Family,CCavg are most  important factor.\n- Customers who have income above 98k dollars , Advance\/graduate level education, a family of more than 2, such customers have higher chances of taking personal loans.\n- So for this campaign we can have different profiles for customers.\n- `High Profile Clients` :-Higher income,Advanced\/Graduate level education, 3 \/4 Family members,high spending\n- `Average Profile` :- Medium income  group,Graduate level education.3\/4Family members,medium spending\n- `Low Profile`:-Lower income group,undergrads ,3\/4Family Member,low spending\n-  Customer Average Spending and Mortages can also be looked upon as based on EDA and logistic Regression this parameters also play some role in likelihood of buy loan.\n- We can 1st  target high profile customers , by providing them with a personal relationship managers who can address there concerns and  can pursue them to buy loan from the bank with completive interest rates.\n- Prequalifying for Loan can also attract more customers.\n- Our 2nd target would be Medium profile customers. \n- The model cannot identify  well if there are some exceptional cases when low profile customer is ready to buy a personal  loan.\n","2ea862f4":"### Comparing all the models based on Model Performance","e658da2d":"**Observations**\n- Number of Customers  with Family size of  3 who had  borrowed loans from the bank is greatet than other family size\n- 60 of those who had Personal loan with the bank also had Securities_Account.\n- Customers who had certificate of deposit  with the bank had previously borrowed loan\n- Customers using Online facilities has no impact on personal loan\n- Majority customers who did have Personal loan with the bank did not used CrediCard from other banks.\n- Majority customers who had take personal loan before are from LosAngeles and Bay region.\n- Ratio of borrowing loan is high in 30 and below and 60 and above customers.\n- Customer with high average Monthly spending have bought personal loan before\n- As expected Age and experience are highly correlated and one of them can be dropped. Since experience had negative values dropping experience would be better option.\n","d4b56a46":"* The coefficients of the logistic regression model are in terms of log(odd), to find the odds we have to take the exponential of the coefficients. \n* Therefore, **odds =  exp(b)**\n* Calculate the probability from the odds ratio using the formula probability = odds \/ (1+odds)\n* The percentage change in odds is given as odds = (exp(b) - 1) * 100","7dbc20c7":"#### Using GridSearch for Hyperparameter tuning of our tree model \n* Grid search is a tuning technique that attempts to compute the optimum values of hyperparameters. \n* It is an exhaustive search that is performed on a the specific parameter values of a model.\n* The parameters of the estimator\/model used to apply these methods are optimized by cross-validated grid-search over a parameter grid.\n* Let's see if we can improve our model performance even more.\n","88fd8b45":"## Data Set\n<br>\n<font size=3 color=\"black\" style=\"font-family:TimesNewRoman\">\n    ","fcc0a691":"### Recommendation ","134905ff":"<p style = \"font-size:40px; font-family:Garamond ; font-weight : strong; color :blue   ; text-align: center;\"> Personal Loan Modelling <\/p>\n <p style = \"font-size:30px; font-family:Garamond ; font-weight : normal; color :blue   ; text-align: center;\">  Using Logistic Regression & Decision Tree<\/p> \n<center><img src=https:\/\/cdn.wallethub.com\/wallethub\/posts\/79708\/best-place-to-get-a-personal-loan.png   width=\"500\" height=\"300\"><\/center>\n<br>","5eec0f37":"**Observations:**\nThere is no correlation between predicator variables\n","f617af46":"# Actionable Insights & Recommendations","8d55f918":"- We are getting  a higher recall on test data between 0.002  to 0.005. Will choosed alpha as 0.002.\n- The Recall on train  and  test indicate we have created a generalized model. with 96 % accuracy and reduced False  negatives.\n- Important features : Income, Graduate education, Family member  3 and 4, Ccavg, Advanced education, Age.\n- This is the best model as false negative is only 6 on Testdata.\n","068cf04e":"**There are no missing values in the dataset.  All the columns are numerical here. `Personal loan` is target variable.  Zipcode,Family,Education,Securities Account,CD_account,online,Credit card are all categorical variables.**","efdab1be":"This is some really extreme values in income 224K USD compared to same age group and experience. Values for Credit card and Mortages looks fine.After identifying outliers, we can decide whether to remove\/treat them or not. It depends,here I am  not going to treat them as there will be outliers in real case scenario (in Income, Mortgage value, Average spending on the credit card, etc) and we would want our model to learn the underlying pattern for such customers.","478da0c0":"- With HyperParameter `max_depth=6, max_leaf_nodes=20, min_samples_leaf=7` the overfitting on  train has reduced, but the recall for test has not improved.\n- Important features are Income,Education 2 and Education 3, Family 4, Family 3, CCavg & Age.\n- But the recall metric is still 91 and false negatives are 12.We don't want to loose opportunity in predicting this customers. so  Let see if instead of pre pruning , post pruning helps in reducing false negative.","07438bd5":"### Check  distrubution in target column","b1985d2c":"**In this case  'Regions' all the attributes have a high p-value which means it is not significant therefore we can drop the complete variable.**\n\n","128a5731":"We got almost all county expect for `96651,92634,93077,92717`. We can fix this zip code by searching internet. Couldn't find for other zipcodes.","105860b0":"We are gettingt a higher recall on test data between 0.002  to 0.005. Will choose alpha as **0.002.**","6b103480":"**Observations**\n- As expected Age and experience are highly correlated and one of them can be dropped.Since we had to handle 0, will drop experience.\n- Income and Average spending on  credit card are  positively corrleated.\n- Mortgage has very little correlation with income.\n","9ef3b420":"**Observations**\n- `~29.4 %` customers are single.\n- `~41.9%` customers are undergrad.\n- `~9.6%` bought  a personal loan from the bank.\n- `10.4 %` customers have a securities account with the bank\n- `6 %` customer have a CD account.\n- `60%` customers  transact online.\n- `29.4%` customers have  credit cards.  \n- `~ 75 %` of customers  are in range of 31- 60.\n- `~ 50 %` Most of bank customers belong to middle income group.\n- `~48 %` of customers has medium Average spending","bcf06cbf":"### Processing Zipcode\nZipcode is a categorical feature and can be a good predictor of target variable. We can analyse if there is any pattern on  location  for customers who had borrowed loaned during previous campaign. Trying to see if we can reduce the category","250ed6cf":"### Bivariate & Multivariate Analysis","8457e25c":"#### Roc -Auc curve on Test data","b335ccaf":"**Observation**\n\n- After Post Pruning ,the false negative has reduced to 6.The accuracy on test data is 97% & Recall is 97% after choosing optimal cc-alpha. \n","45770f90":"**The Recall on train  and  test indicate we have created a generalized model. with 96 % accuracy and reduced False negatives.**","a6cf0f3c":"On analyzing the  Education , we can see most of them  have  education as Advance or Graduate . These cases are some exceptions.","466f07b1":"**The Pvalue for Mortgage is 0.264 So droping Mortage**","c56c8ef0":"### Outliers detection","c58a93b3":" We have 6 categorical independent variables but 4 of them are binary, so we'll have the same results with them even after creating dummies So we will only make dummies for Regions and Education.","f342ba9e":"**we will have to  check and remove multicollinearity from the data to get reliable coefficients and p-values.** \nThere are different ways of detecting (or testing) multi-collinearity, one such way is the Variation Inflation Factor.* General Rule of thumb: If VIF is 1 then there is no correlation among the predictor and the remaining predictor variables. Whereas if VIF exceeds 5, we say it shows signs of high multi-collinearity. But the purpose of the analysis should dictate which threshold to use. ","a890abd6":"## Insights based on EDA","23803f5e":"**Observations**\n- Customers age is in range of `23 - 67 `, with mean and median  of `~45`.\n- Maximum experience is `43` years. where as mean and median are `~20`.\n- Income are in range  `8k to 224k` USD. Mean is `73k` USD and median is `64k` USD. 224 Max salary need to be verified\n- Maximum mortgage taken is `635k` USD.Need to verify this\n- Average spending on credit card per month  ranges from `1- 10k` with mean of `1.9kUSD` and median of `1.5k USD`\n- `1095` customers are from `Los Angeles County`.\n- `480` customers had borrowed loan before.\n","78f7d2da":"#### Build Model\n* We are using  'gini' criteria to split. \n* If the frequency of class A is 10% and the frequency of class B is 90%, then class B will become the dominant class and the decision tree will become biased toward the dominant classes.\n\n* To handle this imbalanced data set,we can pass a dictionary {0:0.15,1:0.85} to the model to specify the weight of each class and the decision tree will give more weightage to class 1.\n\n* class_weight is a hyperparameter for the decision tree classifier.\n* Since not being able to identify a potential customer is the biggest loss as mentioned  earlier with logistic regression. Hence, recall is the right metric to check the performance of the model.","02f799df":"# Conclusion","1c2c115e":"#### Roc -Auc curve on Train data","1acaf125":"**Observations**\n- Age and experience both has same distrubtion with spike at 5 points.\n- Income is right skewed and has some outlier on higher side which can be clipped.\n- Average montly credit is right skewed and has lot of outliers on higher side which can be clipped.\n- Mortgage is  mostly 0 . but is  right skewed and has lot of outlier on higher side which can be clipped.\n\n","a3e042fa":"### Age\nAge can be a vital factor in borrowinng loan, converting ages to bin to explore if there is any pattern ","255ea1b2":"* Logistic Regression model is giving a generalized performance on training and test set.\n* ROC-AUC score of 0.96 on training and test set is quite good.","68614117":"<p1 size=3 color=\"black\" style=\"font-family:TimesNewRoman\">\nThera Bankis a US bank that has a growing customer base. The majority of these customers are liability customers (depositors) with varying sizes of deposits. The number of customers who are also borrowers (asset customers) is quite small, and the bank is interested in expanding this base rapidly to bring in more loan business and in the process, earn more through the interest on loans. In particular, the management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors).\n\nA campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio.\n\nAs a Data scientist at Thera Bank have to build a model that will help the marketing department to identify the potential customers who have a higher probability of purchasing the loan.<\/p>\n  \n    \n**Motivation : To understand Logistic regression  and Decision tree and explore this algorthims using Sklearn, Statmodel,  Roc-Auc Curve,Precision Curve,Sequential feature selection, hyperparamter tuning Decision tress  and post pruning Decision trees**\n  ","82071b4d":"##### Let's use Precision-Recall curve and see if we can find a better threshold","2c307539":"Here are my other notebooks....Do checkout if you find my work helpful, happy learning.\n\n1.[Predicting diabetes ](https:\/\/www.kaggle.com\/yogidsba\/diabetes-prediction-eda-model)\n\n2.[Predict Prices of Used cars](https:\/\/www.kaggle.com\/yogidsba\/predict-used-car-prices-linearregression)\n\n3.[Predict Travel package using Ensemble techniques](https:\/\/www.kaggle.com\/yogidsba\/travelpackageprediction-ensemble-techniques)\n\n4 [Insurance Claim Hypothesis Testing](http:\/\/www.kaggle.com\/yogidsba\/insurance-claims-eda-hypothesis-testing)\n\n5.[Basic EDA on Covid vaccination](http:\/\/www.kaggle.com\/yogidsba\/basic-eda-on-covid-vaccination)\n\n6.[Pandas Tutorial](http:\/\/www.kaggle.com\/yogidsba\/pandas-function-and-data-analysis)\n\n7.[Case study EDA on cardio good fitness](http:\/\/www.kaggle.com\/yogidsba\/casestudy-eda-for-cardio-good-fitness)\n\n","01e63d88":"\n    \n* ID: Customer ID\n* Age: Customer\u2019s age in completed years\n* Experience: #years of professional experience\n* Income: Annual income of the customer (in thousand dollars)\n* ZIP Code: Home Address ZIP code.\n* Family: the Family size of the customer\n* CCAvg: Average spending on credit cards per month (in thousand dollars)\n* Education: Education Level. 1: Undergrad; 2: Graduate;3: Advanced\/Professional\n* Mortgage: Value of house mortgage if any. (in thousand dollars)\n* Personal_Loan: Did this customer accept the personal loan offered in the last campaign?\n* Securities_Account: Does the customer have securities account with the bank?\n* CD_Account: Does the customer have a certificate of deposit (CD) account with the bank?\n* Online: Do customers use internet banking facilities?\n* CreditCard: Does the customer use a credit card issued by any other Bank (excluding All life Bank)?\n   ","51645623":"With this model the False negative  cases have gone up and recall for test is 72 with 95 % accuracy.\nModel is performing well on training and test set.\nModel has given a balanced performance, if the bank wishes to maintain a balance between recall and precision this model can be used.\nArea under the curve has decreased as compared to the initial model but the performance is generalized on training and test set.","8d73afbb":"#### Coefficient interpretations","2e21a83a":"**Decision tree model post pruning has given us best recall scores on data with 97% accuracy . Exploratory data analysis  also suggested income and education were important features in deciding if person will borrow personal loan.\nso choosing  Decision Tree with post-pruning for our prediction.**","0c940bec":"**Converting the county to regions based on  https:\/\/www.calbhbc.org\/region-map-and-listing.html**","eb397a1a":"### Visualizing the Decision Tree","9a332121":"- Income: Holding all other features constant a 1 unit change in Income will increase the odds of a customer taking a personal loan by 20 times or a 95% chance of a customer taking personal loan.\n- Family: Holding all other features constant a 1 unit change in Family will increase the odds of a customer taking a personal loan by 2.16 times increase in the odds of a customer taking personal loan.\n- CCAvg: Holding all other features constant a 1 unit change in CCAvg will increase the odds of a customer taking a personal loan by 1.22 times or a 22.16% increase in the odds of a customer taking personal loan.\n- Education Advance has  7 times higher chances of taking a personal loan than undergraduate\nInterpretation for other attributes can be done similarly.\n\n***Most overall significant varaibles  are Income,Education, CD account ,Family and CCAvg***","3b17c6f0":"Decision tree tends to Overfit and the disparity between the Recall on Train and Test suggest that the model is overfitted","1d7115f3":"- We analyzed the Personal Loan campaign data using EDA and by using different models like Logistic Regression and Decision Tree Classifier to build a likelihood of Customer buying Loan.\n- First we built model using Logistic Regression and performance metric used was Recall. The most important features for classification were Income,Education, CD account ,Family and CCAvg .\n- Coefficient of Income, Graduate and Advanced Education, Family_3,Family 4,CCavg,CD account,Age, are positive , ie a one unit increase in these will lead to increase in chances of a person borrowing loan\n- Coefficient of Securities account,online ,Family_2 credit card are negative increase in these will lead to decrease in chances of a person borrowing a loan.\n- We also improved the performance using  ROC-AUC curve and optimal threshold .This was best model with high recall and accuracy .\n- Decision tree can easily overfit. They require less datapreprocessing  compared to logistic Regression and are easy to understand.\n- We used decision trees  with prepruning and post pruning. The Post pruning model gave 96 % recall with 97% accuracy.\n- Income, Customers with graduate degree, customers having 3 family members are some of the most  important variables in predicting if the customers will purchase a personal loan.\n","19c515bd":"## Problem\n\n    \n- To predict whether a liability customer will buy a personal loan or not.\n- Which variables are most significant.\n- Which segment of customers should be targeted more.\n- Does Age have any impact of customer buying loan?\n- Do people with less income borrow loans .?\n","d9cfe068":"#### MultiCollinearity","43630ca5":"52 customers from age group 23 - 30 have negative values in experience","aac35bc0":"# Read and Understand data","b402064f":"## Context\n    \n<font size=3 color=\"black\" style=\"font-family:TimesNewRoman\">\n    ","5aadb239":"* Coefficient of  Income, Education, Family,CCavg,CD account,Age,  are positive , ie a one unit  increase in these will lead to increase in chances of a person borrowing loan\n* Coefficient of Securities account,online ,Credit card are negative, increase in these will lead to decrease in chances of a person borrowing a loan.","922c8a77":"**Logistic Regression (with Sklearn library)**","ee4f6d4a":"**`Id` column is not needed and can be dropped.**\n","f631e459":"### Fixing the data types\nPersonal_Loan, Securities_Account, CD_Account, 'Online', 'CreditCard' ,Education are of int\/object type, we can change them to category type  to reduce the dataspace required.\n","444e64e9":"#### **Test Assumption**","fd014a28":"**Dropping Age as pvalue is greater than 0.05**","b8d88f83":"### Processing Experience","f36cd801":"**Our model predicted 6 customers wrongly. On analyzing the Income , Education,Family , we can see the income is not in range of High income group and education is undergrad for most of them and there average spending is also low.  These cases are some exceptions.**","65ad64ee":"### Model performance evaluation and improvement","dc4ac517":"**Logistic Regression (with Statmodel)**","960cdbe4":"### Spending\nTo understand customers spending derving new columns which will say if customer belongs to Upper , middle or lower spending","6be23a2b":"**Its seems experience is negative for all observation with for age 23,24, and few others . Based on there age and education it seems values were negative by mistake. so changing it to absoulte values of experience.**","720f9b07":"####  Converting coefficients to odds"}}