{"cell_type":{"2851aa60":"code","17440d41":"code","f6ac5122":"code","c55a6edc":"code","57ff39fe":"code","50c7c400":"code","02dd4340":"code","73521cb5":"code","eb652e5d":"code","aa09d8ef":"markdown","53192e12":"markdown","02c59c48":"markdown","92193bb5":"markdown","2d8491c7":"markdown","751c1be9":"markdown","cc7120e1":"markdown"},"source":{"2851aa60":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom os import listdir\nfrom os.path import isfile, join\nimport os\n\nimport tensorflow as tf # google's library for deep learning\nfrom tensorflow.keras.utils import Sequence, to_categorical\nimport cv2 # opencv - for computer vision\nfrom tensorflow.keras.preprocessing.image import (ImageDataGenerator, load_img,\n                                                  save_img)\n\nfrom tensorflow.keras.layers import (AveragePooling2D, Conv2D, Dense, Dropout,\n                                     Flatten, Input, Lambda, LeakyReLU,\n                                     MaxPooling2D, UpSampling2D)\nfrom tensorflow.keras import optimizers\nimport tensorflow.keras.backend as K\nfrom sklearn.utils import shuffle\nimport math\nfrom typing import AnyStr, Callable\n","17440d41":"df = pd.DataFrame(columns=['image','file','level'])\nmypath = '\/kaggle\/input\/diabetic-retinopathy-2015-data-colored-resized\/colored_images\/colored_images\/No_DR\/'\ndf['image'] = [f for f in listdir(mypath) if isfile(join(mypath, f))]\ndf['level'] = 0\ndf['file'] = mypath + df['image']\n\ndf2 = pd.DataFrame(columns=['image','file','level'])\nmypath = '\/kaggle\/input\/diabetic-retinopathy-2015-data-colored-resized\/colored_images\/colored_images\/Mild\/'\ndf2['image'] = [f for f in listdir(mypath) if isfile(join(mypath, f))]\ndf2['level'] = 1\ndf2['file'] = mypath + df2['image']\ndf = df.append(df2,ignore_index=True)\n\ndf2 = pd.DataFrame(columns=['image','file','level'])\nmypath = '\/kaggle\/input\/diabetic-retinopathy-2015-data-colored-resized\/colored_images\/colored_images\/Moderate\/'\ndf2['image'] = [f for f in listdir(mypath) if isfile(join(mypath, f))]\ndf2['level'] = 2\ndf2['file'] = mypath + df2['image']\ndf = df.append(df2,ignore_index=True)\n\ndf2 = pd.DataFrame(columns=['image','file','level'])\nmypath = '\/kaggle\/input\/diabetic-retinopathy-2015-data-colored-resized\/colored_images\/colored_images\/Severe\/'\ndf2['image'] = [f for f in listdir(mypath) if isfile(join(mypath, f))]\ndf2['level'] = 3\ndf2['file'] = mypath + df2['image']\ndf = df.append(df2,ignore_index=True)\n\ndf2 = pd.DataFrame(columns=['image','file','level'])\nmypath = '\/kaggle\/input\/diabetic-retinopathy-2015-data-colored-resized\/colored_images\/colored_images\/Proliferate_DR\/'\ndf2['image'] = [f for f in listdir(mypath) if isfile(join(mypath, f))]\ndf2['level'] = 4\ndf2['file'] = mypath + df2['image']\ndf = df.append(df2,ignore_index=True)\n\ndf.head()","f6ac5122":"class RetinaSequence(Sequence):\n\n    def __init__(self, x_set: list, y_set: list, batch_size: int = 32, augmentate: bool = True, \n                 shuffle: bool = True, input_shape: tuple = (512, 512), preprocessing_fcn: callable = None):\n        \"\"\" \n        This is called when the object is initialized.\n        \n        Parameters\n        ----------\n        x_set : list\n            The array with image files paths.\n        y_set : list\n            The array with DR levels.  \n        batch_size : int\n            The batch size, which is how many samples are delivered at each update of the training algorithm.\n        augmentate : bool\n            If True, will use data augmentation.\n        shuffle: bool\n            If True, will shuffle the data at the end of each epoch.\n        input_shape: tuple\n            The size of the input images. Larger input images needs GPUs with more memory.\n        preprocessing_fcn: callable\n            Method for the preprocessing function that will be applied to the input images.\n            \n        print_cols : bool, optional\n            A flag used to print the columns to the console (default is False)\n\n        \n        \"\"\"\n        \n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n        self.augmentate = augmentate\n        self.shuffle = shuffle\n        self.preprocessing_fcn = preprocessing_fcn\n        self.input_shape = input_shape\n\n        self.datagen = ImageDataGenerator(featurewise_center=False,\n                                          samplewise_center=False,\n                                          featurewise_std_normalization=False,\n                                          samplewise_std_normalization=False,\n                                          rotation_range=360.,\n                                          width_shift_range=0.0,\n                                          height_shift_range=0.0,\n                                          shear_range=0.,\n                                          zoom_range=0.0,\n                                          channel_shift_range=0.,\n                                          fill_mode='constant',\n                                          cval=0.,\n                                          horizontal_flip=True,\n                                          vertical_flip=True,\n                                          rescale=None,\n                                          preprocessing_function=None,\n                                          data_format=K.image_data_format())\n\n    def __len__(self):\n        return math.ceil(len(self.x) \/ self.batch_size)\n\n    def __getitem__(self, idx):\n        \"\"\"This is the method called to actually give data to the model.\n\n        Args:\n            idx (int): index.\n\n        Returns:\n            [ndarray]: array with a batch of data and its labels.\n        \"\"\"        \n        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        return np.array([self.preprocess(file_name)\n                         for file_name in batch_x]), np.array(batch_y)\n\n    def preprocess(self, file_name):\n        \"\"\"The preprocessing method. It does the contrast enhancement. Data augmentation if asked and transform to float32.\n\n        Args:\n            file_name (str): the file path.\n\n        Returns:\n            ndarray: the preprocessed image.\n        \"\"\"        \n        img = load_img(file_name, target_size=self.input_shape)\n        x = np.array(img, dtype=np.uint8)\n        alfa = 4.0\n        tal = -4.0\n        xf = cv2.GaussianBlur(\n            x, ksize=(0, 0), sigmaX=img.size[0] \/\/ 30, sigmaY=0)\n        xf = xf.astype(np.float32)\n        x = x.astype(np.float32)\n        x = alfa * x + tal * xf + 128.0\n        x[x < 0] = 0\n        x[x > 255] = 255\n\n        if self.augmentate:\n            x = self.datagen.random_transform(x)\n            factor = np.random.uniform(low=0.6, high=1.67)\n            x = np.array(x, dtype=K.floatx())\n            x = 128 + factor * (x - 128)\n            x[x > 255] = 255\n            x[x < 0] = 0\n        else:\n            x = x.astype(np.float32)\n        if self.preprocessing_fcn is None:\n            x -= 127.0\n            x \/= 127.0\n        else:\n            x = x.astype(np.uint8)\n            x = self.preprocessing_fcn(x)\n        return x\n\n    def on_epoch_end(self):\n        \"\"\"This is called on each epoch end and shuffles the data if self.shuffe is True.\n        \"\"\"\n        if self.shuffle:\n            self.x, self.y = shuffle(self.x, self.y)    ","c55a6edc":"test_ratio = 0.5 # this is the percentage of data that will be used as test set, where we'll verify the model's performance.\nvalidation_ratio = 0.2 # this is the percentage of data that will be used as validation set, used for early stopping.\n\ndf['class'] = df.level > 1\nn = df.shape[0]\nweights = {False: n \/ df.loc[df['class'] == False].shape[0],\n           True: n \/ df.loc[df['class'] == True].shape[0]}\ndf['weights'] = df['class'].map(weights) # higher weights for class with less examples.\n\nimages = df.image.unique()\n\nmsk = np.random.rand(len(df)) < (1.0 - test_ratio)\ntrain = df.loc[df.image.isin(images[msk])]\ntest = df.loc[df.image.isin(images[~msk])]\n\nimages = train.image.unique()\nmsk = np.random.rand(len(train)) < (1.0 - validation_ratio)\nval = train.loc[train.image.isin(images[~msk])]\ntrain = train.loc[train.image.isin(images[msk])]\n\ntrain = train.sample(n=4 * len(train.index),\n                     replace=True, weights='weights') # balancing the classes in the set used for training.\n\nval = val.sample(n=4 * len(val.index),\n                     replace=True, weights='weights') # balancing the classes in the set used for validation.\n\nnb_train = len(train.index)\nnb_val = len(val.index)","57ff39fe":"preprocessing_fcn = tf.keras.applications.nasnet.preprocess_input # the preprocessing function is the default for the used model.\n\n# we are going to use NASNetMobile because it is small and fast to train.\nbase_model = tf.keras.applications.nasnet.NASNetMobile(include_top=False, weights='imagenet',\n                                                    input_tensor=None,\n                                                    pooling='avg')\ninput_shape = base_model.input_shape[1:3]\n\n# Freeze the pretrained weights\nbase_model.trainable = False\n\n# adding the final part of the model.\nx = base_model.output\nx = Dense(units=512, activation='relu', use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(0.005),\n          bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = Dense(units=256, activation='relu', use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(0.005),\n          bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = Dense(units=2, activation='softmax', use_bias=True, kernel_regularizer=tf.keras.regularizers.l2(0.005),\n          bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(x)\nmodel = tf.keras.Model(inputs=base_model.input,\n                    outputs=x, name='NASNetMobile')","50c7c400":"# Instantiate the sequences.\ntrain_seq = RetinaSequence(x_set=train['file'],\n                           y_set=to_categorical(\n                               train['class'], num_classes=2),\n                           batch_size=16, augmentate=True, shuffle=True,\n                           input_shape=input_shape,\n                           preprocessing_fcn=preprocessing_fcn)\nval_seq = RetinaSequence(x_set=val['file'], y_set=to_categorical(val['class'], num_classes=2),\n                         batch_size=16, augmentate=False, shuffle=False,\n                         input_shape=input_shape,\n                         preprocessing_fcn=preprocessing_fcn)","02dd4340":"# those are the metrics we are going to check.\nMETRICS = [\n      tf.keras.metrics.TruePositives(name='tp'),\n      tf.keras.metrics.FalsePositives(name='fp'),\n      tf.keras.metrics.TrueNegatives(name='tn'),\n      tf.keras.metrics.FalseNegatives(name='fn'), \n      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.Recall(name='recall'),\n      tf.keras.metrics.AUC(name='auc'),\n      tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n]\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=METRICS)\n\nepochs = 5\nmodel.fit(\n    x=train_seq,\n    steps_per_epoch=nb_train \/\/ (4 * 16),\n    epochs=epochs,\n    verbose=1,\n    validation_data=val_seq,\n    validation_steps=nb_val \/\/ (4*16),\n    max_queue_size=10,\n    workers=20,\n    use_multiprocessing=False,\n    shuffle=False)","73521cb5":"patience_early_stop = 5\nfor layer in model.layers:\n    if not isinstance(layer, tf.keras.layers.BatchNormalization):\n        layer.trainable = True\n\n# compile the model using Stochastic Gradient Descent optimization algorithm.\nmodel.compile(optimizer=optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0, nesterov=True),\n              loss='binary_crossentropy', metrics=METRICS)\n\n# The callbacks are the methods called during the training process.\n# EarlyStopping will stop the training process if the loss stops getting better.\n\n# ModelCheckpoint will save the model at the best epoch.\ncallbacks = [tf.keras.callbacks.EarlyStopping(\n                 monitor='val_loss',\n                 patience=patience_early_stop),\n                 tf.keras.callbacks.ModelCheckpoint(filepath='\/kaggle\/working\/model_DR.h5', monitor=\"val_loss\", verbose=1, save_best_only=True, save_weights_only=False, mode=\"auto\", save_freq=\"epoch\")\n                     ]\n\nmodel.fit(\n    x=train_seq,\n    steps_per_epoch=nb_train \/\/ (4 * 16),\n    epochs=50,\n    verbose=1,\n    callbacks=callbacks,\n    validation_data=val_seq,\n    validation_steps= nb_val \/\/ 16,\n    max_queue_size=10,\n    workers=20,\n    use_multiprocessing=False,\n    shuffle=False)","eb652e5d":"test_seq = RetinaSequence(x_set=test['file'], y_set=to_categorical(test['class'], num_classes=2),\n                         batch_size=16, augmentate=False, shuffle=False,\n                         input_shape=input_shape,\n                         preprocessing_fcn=preprocessing_fcn)\n# Evaluate the model on the test data using `evaluate`\nprint(\"Evaluate on test data\")\nresults = model.evaluate(test_seq)\nprint(\"test loss, test acc:\", results)","aa09d8ef":"# Diabetic retinopathy detection using Convolutional Neural Networks\n\nIn this notebook I'll show how to use CNNs and transfer learning to train a diabetic retinopathy detection model using fundus images.\n\n---\n\n## *Detec\u00e7\u00e3o de retinopatia diab\u00e9tica utilizando redes neurais convolucionais*\n\n*Neste notebook irei demonstrar como utilizar redes neurais convolucionais e transfer\u00eancia de aprendizado para treinar um modelo de detec\u00e7\u00e3o de retinopatia diab\u00e9tica a partir de imagens de retina.*","53192e12":"## List all image files and its DR levels on a DataFrame\n\nThe used dataset can be found [here](https:\/\/www.kaggle.com\/sovitrath\/diabetic-retinopathy-2015-data-colored-resized).\n\n---\n\n### *Lista todos arquivos de imagem e seus n\u00edveis de RD e os coloca num DataFrame*\n\nO banco de dados utilizado pode ser encontrado [aqui](https:\/\/www.kaggle.com\/sovitrath\/diabetic-retinopathy-2015-data-colored-resized).","02c59c48":"Then, we unfreeze the layers of the pre-trained model and train it all.","92193bb5":"The DataFrame df contains 3 columns:\n- image: the name of the each image.\n- file: the complete path of each image file.\n- level: the DR level.\n---\n\n*O DataFrame df cont\u00e9m 3 colunas:*\n- *image: o nome de cada imagem.*\n- *file: o caminho completo de cada arquivo de imagem.*\n- *level: o n\u00edvel de RD de cada imagem.*","2d8491c7":"First we will train the recently added layers for a couple of epochs.","751c1be9":"## Now let's evaluate our model.\n---\n## Agora vamos avaliar nosso modelo.","cc7120e1":"### Define the Sequence, structure responsible for providing the model with preprocessed data\n\n---\n\n### *Define a estrutura Sequence, respons\u00e1vel por prover as amostras ao model durante o treinamento*"}}