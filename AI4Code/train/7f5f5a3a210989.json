{"cell_type":{"b069b1fc":"code","a77124f2":"code","21768735":"code","92ac687a":"code","383b0a83":"code","5fa6ccb9":"code","a891e430":"code","0b7f189c":"code","2d417a0a":"code","38c5f788":"code","227b6d2e":"code","37178b35":"code","782ec905":"code","14ae8b08":"code","3e7cd16b":"code","57aa3c4e":"code","f199be80":"code","239e2000":"code","dc747e3f":"code","ea67d992":"code","c72a0945":"code","fd71e2f0":"code","d7030f79":"code","aabd8ca5":"markdown","103fca64":"markdown","e51a98c1":"markdown","b27c04c5":"markdown","cca09cd3":"markdown","9ac973a6":"markdown","1d017550":"markdown"},"source":{"b069b1fc":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# # For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"..\/input\/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a77124f2":"from sklearn.preprocessing import StandardScaler\n\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor","21768735":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","92ac687a":"train.SalePrice.describe()","383b0a83":"# #Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n#drop \"id\" axis=1 means drop column, axis=0 means drop label\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\nprint(train.shape)\nprint(test.shape)","5fa6ccb9":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","a891e430":"#Combine two dataset into one\nntrain = train.shape[0]\nntest = test.shape[0]\nprint(ntrain)\nprint(ntest)\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","0b7f189c":"#find out missing data\nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","2d417a0a":"# 1.PoolQC\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\n# 2.MiscFeature\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\n# 3.Alley\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\n# 4.Fence \nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n# 5.FireplaceQu\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","38c5f788":"all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","227b6d2e":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n    \nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n    \nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n    \nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\n    \nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n\nall_data = all_data.drop(['Utilities'], axis=1)\n\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n\nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","37178b35":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","782ec905":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","14ae8b08":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","3e7cd16b":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","57aa3c4e":"# Setup cross validation folds\nkf = KFold(n_splits=12, random_state=42, shuffle=True)","f199be80":"# Define error metrics\nX = train.values\nn_folds = 5\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, X , y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","239e2000":"# # Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                min_samples_split=10,\n                                random_state=42)  \n\n# XGBoost Regressor\n# xgb = XGBRegressor(learning_rate=0.01,\n#                        n_estimators=6000,\n#                        max_depth=4,\n#                        random_state=42)\n\n# Stack up all the models above, optimized using xgboost\n# stack_gen = StackingCVRegressor(regressors=(xgb, gbr),\n#                                 meta_regressor=xgb,\n#                                 use_features_in_secondary=True)","dc747e3f":"scores = {}\n# score = rmsle_cv(xgb)\n# print(\"xgb: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n# scores['xgb'] = (score.mean(), score.std())","ea67d992":"score = rmsle_cv(gbr)\nprint(\"gbr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['gbr'] = (score.mean(), score.std())","c72a0945":"# print('xgboost')\n# xgb_model_full_data = xgb.fit(X, y_train)","fd71e2f0":"gbr.fit(train.values, y_train)\ngbr_train_pred = gbr.predict(train.values) \ngbr_pred = np.exp(gbr.predict(test.values))\nprint('Gradient Boosting')","d7030f79":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = gbr_pred\nsub.to_csv('submission.csv',index=False)","aabd8ca5":"# Label Encoding","103fca64":"# Submitting the predictions","e51a98c1":"# Modelling","b27c04c5":"# Imputing the missing values","cca09cd3":"# Feature Engineering","9ac973a6":"# Importing the dataset","1d017550":"# Transforming some numerical values into categorical values"}}