{"cell_type":{"20df60d8":"code","0f13cba1":"code","1c3a924a":"code","a788de94":"code","048025b1":"code","d537a32d":"code","7a6c3dd1":"code","58c48cfd":"code","24706119":"code","34bf4a9b":"code","9f4f4683":"code","104fb6c9":"code","300cc715":"code","f9596389":"code","a606a5d6":"code","e73b2d6b":"code","f0c3ec80":"code","b34e3959":"code","3d5c1cc2":"code","fcb54917":"code","7c89df73":"code","f8a8dcb5":"code","5d5e00a9":"code","26d8485e":"code","fe0b2044":"code","074ebfc0":"code","b9497eea":"code","4798fdaf":"code","c5020f3f":"code","52b2994f":"code","d3b2bef1":"code","16b61a87":"code","0e5b0dd3":"code","6b172b47":"code","974ef10a":"code","38e6bec6":"code","8fa056c3":"code","5b6589bf":"code","d2b636bf":"code","d1413c59":"code","bdfe334d":"code","10ce3ea5":"code","6636e329":"code","267c2176":"code","b700d21c":"code","1106cc1e":"code","e5ca6f72":"code","026269ad":"code","7f2129be":"code","ddb86b36":"code","9ab90b68":"code","b1d7f74b":"code","9972622d":"code","29baebf4":"code","5e0ce8af":"code","aeb88c9d":"code","212c28c7":"code","f20dfce9":"code","749d3364":"code","d8f60f13":"code","88866171":"code","17b2f182":"code","f8e89495":"code","5d191166":"code","5cabd356":"code","061f5027":"code","be9a315d":"code","14d1bedb":"code","f36e8a75":"code","fcc66857":"code","7bf1c581":"code","04304cb7":"code","6dabd4e8":"code","1da105d3":"code","f98ba0f1":"code","520cac12":"code","7bd779e8":"code","537bc126":"code","ed463a31":"code","b95d828b":"code","c6e3dd45":"markdown","96c4e2c7":"markdown","ecc434f5":"markdown","fa6eccd6":"markdown","f8e6bd19":"markdown","149c13b9":"markdown","783c1fd9":"markdown","92a0e63d":"markdown","f07c44b5":"markdown","c1f6110d":"markdown","800d5528":"markdown","128895bd":"markdown","fad23420":"markdown","82e82e18":"markdown","9fa95bab":"markdown","7e95fa39":"markdown","f7ed7a83":"markdown","c08fd42a":"markdown","b7b7a30d":"markdown","fc007148":"markdown","c6c1d89e":"markdown","40311004":"markdown","0b74a56d":"markdown","9212725d":"markdown","53128881":"markdown","c0d8360f":"markdown","891cdcd5":"markdown","a61666d4":"markdown","0a2d0a58":"markdown","65ff8a25":"markdown","fd87749a":"markdown","ce4a8b60":"markdown","ae80b194":"markdown","d4d459df":"markdown","1d2aceba":"markdown","13490757":"markdown","b85266e9":"markdown","f931ee6e":"markdown","ada94afa":"markdown","57eee5ce":"markdown","c302a99c":"markdown"},"source":{"20df60d8":"!pip install ..\/input\/kerasapplications\/keras-team-keras-applications-3b180cb -f .\/ --no-index\n!pip install ..\/input\/efficientnet\/efficientnet-1.1.0\/ -f .\/ --no-index","0f13cba1":"import os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport tensorflow as tf \nimport matplotlib.pyplot as plt \nimport random\nfrom tqdm.notebook import tqdm \nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom tensorflow_addons.optimizers import RectifiedAdam\nfrom tensorflow.keras import Model\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nfrom tensorflow.keras.optimizers import Nadam\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom PIL import Image","1c3a924a":"def seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)","a788de94":"seed_everything(42)","048025b1":"config = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = tf.compat.v1.Session(config=config)","d537a32d":"# From the best on Private LB - commit 17\nDropout_model = 0.25\nFVC_weight = 0.5\nConfidence_weight = 0.5\nGaussianNoise_stddev = 0.2","7a6c3dd1":"commits_df = pd.DataFrame(columns = ['commit_num', 'FVC_weight', 'Dropout_model', 'LB_score', 'seed', 'GaussianNoise_stddev'])","58c48cfd":"n=0\ncommits_df.loc[n,'commit_num'] = 1\ncommits_df.loc[n,'Dropout_model'] = 0.4\ncommits_df.loc[n,'FVC_weight'] = 0.25\ncommits_df.loc[n,'LB_score'] = -6.8111","24706119":"n=1\ncommits_df.loc[n,'commit_num'] = 4\ncommits_df.loc[n,'Dropout_model'] = 0.36\ncommits_df.loc[n,'FVC_weight'] = 0.25\ncommits_df.loc[n,'LB_score'] = -6.8105","34bf4a9b":"n=2\ncommits_df.loc[n,'commit_num'] = 6\ncommits_df.loc[n,'Dropout_model'] = 0.36\ncommits_df.loc[n,'FVC_weight'] = 0.35\ncommits_df.loc[n,'LB_score'] = -6.8158","9f4f4683":"n=3\ncommits_df.loc[n,'commit_num'] = 8\ncommits_df.loc[n,'Dropout_model'] = 0.35\ncommits_df.loc[n,'FVC_weight'] = 0.25\ncommits_df.loc[n,'LB_score'] = -6.8107","104fb6c9":"n=4\ncommits_df.loc[n,'commit_num'] = 9\ncommits_df.loc[n,'Dropout_model'] = 0.35\ncommits_df.loc[n,'FVC_weight'] = 0.3\ncommits_df.loc[n,'LB_score'] = -6.8125","300cc715":"n=5\ncommits_df.loc[n,'commit_num'] = 10\ncommits_df.loc[n,'Dropout_model'] = 0.36\ncommits_df.loc[n,'FVC_weight'] = 0.2\ncommits_df.loc[n,'LB_score'] = -6.8089","f9596389":"n=6\ncommits_df.loc[n,'commit_num'] = 11\ncommits_df.loc[n,'Dropout_model'] = 0.36\ncommits_df.loc[n,'FVC_weight'] = 0.15\ncommits_df.loc[n,'LB_score'] = -6.8100","a606a5d6":"n=7\ncommits_df.loc[n,'commit_num'] = 13\ncommits_df.loc[n,'Dropout_model'] = 0.36\ncommits_df.loc[n,'FVC_weight'] = 0.175\ncommits_df.loc[n,'LB_score'] = -6.8096","e73b2d6b":"n=8\ncommits_df.loc[n,'commit_num'] = 14\ncommits_df.loc[n,'Dropout_model'] = 0.36\ncommits_df.loc[n,'FVC_weight'] = 0.225\ncommits_df.loc[n,'LB_score'] = -6.8100","f0c3ec80":"n=9\ncommits_df.loc[n,'commit_num'] = 15\ncommits_df.loc[n,'Dropout_model'] = 0.32\ncommits_df.loc[n,'FVC_weight'] = 0.2\ncommits_df.loc[n,'LB_score'] = -6.8092","b34e3959":"n=10\ncommits_df.loc[n,'commit_num'] = 16\ncommits_df.loc[n,'Dropout_model'] = 0.25\ncommits_df.loc[n,'FVC_weight'] = 0.2\ncommits_df.loc[n,'LB_score'] = -6.8093","3d5c1cc2":"n=11\ncommits_df.loc[n,'commit_num'] = 17\ncommits_df.loc[n,'Dropout_model'] = 0.25\ncommits_df.loc[n,'FVC_weight'] = 0.5\ncommits_df.loc[n,'LB_score'] = -6.8283","fcb54917":"n=12\ncommits_df.loc[n,'commit_num'] = 18\ncommits_df.loc[n,'Dropout_model'] = 0.38\ncommits_df.loc[n,'FVC_weight'] = 0.2\ncommits_df.loc[n,'LB_score'] = -6.8087","7c89df73":"n=13\ncommits_df.loc[n,'commit_num'] = 19\ncommits_df.loc[n,'Dropout_model'] = 0.39\ncommits_df.loc[n,'FVC_weight'] = 0.2\ncommits_df.loc[n,'LB_score'] = -6.8090","f8a8dcb5":"n=14\ncommits_df.loc[n,'commit_num'] = 20\ncommits_df.loc[n,'Dropout_model'] = 0.37\ncommits_df.loc[n,'FVC_weight'] = 0.2\ncommits_df.loc[n,'LB_score'] = -6.8092","5d5e00a9":"n=15\ncommits_df.loc[n,'commit_num'] = 21\ncommits_df.loc[n,'Dropout_model'] = 0.38\ncommits_df.loc[n,'FVC_weight'] = 0.19\ncommits_df.loc[n,'LB_score'] = -6.8093","26d8485e":"n=16\ncommits_df.loc[n,'commit_num'] = 23\ncommits_df.loc[n,'Dropout_model'] = 0.38\ncommits_df.loc[n,'FVC_weight'] = 0.2\ncommits_df.loc[n,'LB_score'] = -6.8402","fe0b2044":"n=17\ncommits_df.loc[n,'commit_num'] = 26\ncommits_df.loc[n,'Dropout_model'] = 0.385\ncommits_df.loc[n,'FVC_weight'] = 0.2\ncommits_df.loc[n,'LB_score'] = -6.8092","074ebfc0":"n=18\ncommits_df.loc[n,'commit_num'] = 27\ncommits_df.loc[n,'Dropout_model'] = 0.38\ncommits_df.loc[n,'FVC_weight'] = 0.21\ncommits_df.loc[n,'LB_score'] = -6.8091","b9497eea":"n=19\ncommits_df.loc[n,'commit_num'] = 28\ncommits_df.loc[n,'Dropout_model'] = 0.38\ncommits_df.loc[n,'FVC_weight'] = 0.2\ncommits_df.loc[n,'LB_score'] = -6.8090","4798fdaf":"n=20\ncommits_df.loc[n,'commit_num'] = 29\ncommits_df.loc[n,'Dropout_model'] = 0.38\ncommits_df.loc[n,'FVC_weight'] = 0.2\ncommits_df.loc[n,'GaussianNoise_stddev'] = 0.15\ncommits_df.loc[n,'LB_score'] = -6.8092","c5020f3f":"n=21\ncommits_df.loc[n,'commit_num'] = 31\ncommits_df.loc[n,'Dropout_model'] = 0.38\ncommits_df.loc[n,'FVC_weight'] = 0.2\ncommits_df.loc[n,'GaussianNoise_stddev'] = 0.21\ncommits_df.loc[n,'LB_score'] = -6.8093","52b2994f":"n=22\ncommits_df.loc[n,'commit_num'] = 32\ncommits_df.loc[n,'Dropout_model'] = 0.24\ncommits_df.loc[n,'FVC_weight'] = 0.14\ncommits_df.loc[n,'GaussianNoise_stddev'] = 0.2\ncommits_df.loc[n,'LB_score'] = -6.8106","d3b2bef1":"# Find and mark maximum value of LB score\ncommits_df['LB_score'] = pd.to_numeric(commits_df['LB_score'])\ncommits_df['max'] = 0\ncommits_df.loc[commits_df['LB_score'].idxmax(), 'max'] = 1","16b61a87":"# Seed\ncommits_df['seed'] = 42\ncommits_df.loc[commits_df['commit_num'] == 23, 'seed'] = 0","0e5b0dd3":"# GaussianNoise_stddev\ncommits_df['GaussianNoise_stddev'] = 0.2\ncommits_df.loc[commits_df['commit_num'] == 28, 'GaussianNoise_stddev'] = 0.25","6b172b47":"commits_df.sort_values(by=['LB_score'], ascending = False)","974ef10a":"sorted(list(commits_df['FVC_weight'].unique()))","38e6bec6":"# Interactive plot with results of parameters tuning\nfig = px.scatter_3d(commits_df, x='FVC_weight', y='Dropout_model', z='LB_score', color = 'max', \n                    symbol = 'GaussianNoise_stddev',\n                    title='Parameters and LB score visualization of OSIC PFP solutions')\nfig.update(layout=dict(title=dict(x=0.1)))","8fa056c3":"# Interactive plot with results of parameters tuning\nfig = px.scatter_3d(commits_df, x='FVC_weight', y='Dropout_model', z='LB_score', color = 'max', \n                    symbol = 'seed',\n                    title='Parameters and LB score visualization of OSIC PFP solutions')\nfig.update(layout=dict(title=dict(x=0.1)))","5b6589bf":"train = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/train.csv') ","d2b636bf":"def get_tab(df):\n    vector = [(df.Age.values[0] - 30) \/ 30] \n    \n    if df.Sex.values[0] == 'male':\n       vector.append(0)\n    else:\n       vector.append(1)\n    \n    if df.SmokingStatus.values[0] == 'Never smoked':\n        vector.extend([0,0])\n    elif df.SmokingStatus.values[0] == 'Ex-smoker':\n        vector.extend([1,1])\n    elif df.SmokingStatus.values[0] == 'Currently smokes':\n        vector.extend([0,1])\n    else:\n        vector.extend([1,0])\n    return np.array(vector) ","d1413c59":"A = {} \nTAB = {} \nP = [] \nfor i, p in tqdm(enumerate(train.Patient.unique())):\n    sub = train.loc[train.Patient == p, :] \n    fvc = sub.FVC.values\n    weeks = sub.Weeks.values\n    c = np.vstack([weeks, np.ones(len(weeks))]).T\n    a, b = np.linalg.lstsq(c, fvc)[0]\n    \n    A[p] = a\n    TAB[p] = get_tab(sub)\n    P.append(p)","bdfe334d":"def get_img(path):\n    d = pydicom.dcmread(path)\n    return cv2.resize(d.pixel_array \/ 2**11, (512, 512))","10ce3ea5":"from tensorflow.keras.utils import Sequence\n\nclass IGenerator(Sequence):\n    BAD_ID = ['ID00011637202177653955184', 'ID00052637202186188008618']\n    def __init__(self, keys, a, tab, batch_size=32):\n        self.keys = [k for k in keys if k not in self.BAD_ID]\n        self.a = a\n        self.tab = tab\n        self.batch_size = batch_size\n        \n        self.train_data = {}\n        for p in train.Patient.values:\n            self.train_data[p] = os.listdir(f'..\/input\/osic-pulmonary-fibrosis-progression\/train\/{p}\/')\n    \n    def __len__(self):\n        return 1000\n    \n    def __getitem__(self, idx):\n        x = []\n        a, tab = [], [] \n        keys = np.random.choice(self.keys, size = self.batch_size)\n        for k in keys:\n            try:\n                i = np.random.choice(self.train_data[k], size=1)[0]\n                img = get_img(f'..\/input\/osic-pulmonary-fibrosis-progression\/train\/{k}\/{i}')\n                x.append(img)\n                a.append(self.a[k])\n                tab.append(self.tab[k])\n            except:\n                print(k, i)\n       \n        x,a,tab = np.array(x), np.array(a), np.array(tab)\n        x = np.expand_dims(x, axis=-1)\n        return [x, tab] , a","6636e329":"from tensorflow.keras.layers import (\n    Dense, Dropout, Activation, Flatten, Input, BatchNormalization, GlobalAveragePooling2D, Add, Conv2D, AveragePooling2D, \n    LeakyReLU, Concatenate \n)\nimport efficientnet.tfkeras as efn\n\ndef get_efficientnet(model, shape):\n    models_dict = {\n        'b0': efn.EfficientNetB0(input_shape=shape,weights=None,include_top=False),\n        'b1': efn.EfficientNetB1(input_shape=shape,weights=None,include_top=False),\n        'b2': efn.EfficientNetB2(input_shape=shape,weights=None,include_top=False),\n        'b3': efn.EfficientNetB3(input_shape=shape,weights=None,include_top=False),\n        'b4': efn.EfficientNetB4(input_shape=shape,weights=None,include_top=False),\n        'b5': efn.EfficientNetB5(input_shape=shape,weights=None,include_top=False),\n        'b6': efn.EfficientNetB6(input_shape=shape,weights=None,include_top=False),\n        'b7': efn.EfficientNetB7(input_shape=shape,weights=None,include_top=False)\n    }\n    return models_dict[model]\n\ndef build_model(shape=(512, 512, 1), model_class=None):\n    inp = Input(shape=shape)\n    base = get_efficientnet(model_class, shape)\n    x = base(inp)\n    x = GlobalAveragePooling2D()(x)\n    inp2 = Input(shape=(4,))\n    x2 = tf.keras.layers.GaussianNoise(GaussianNoise_stddev)(inp2)\n    x = Concatenate()([x, x2]) \n    x = Dropout(Dropout_model)(x)\n    x = Dense(1)(x)\n    model = Model([inp, inp2] , x)\n    \n    weights = [w for w in os.listdir('..\/input\/osic-model-weights') if model_class in w][0]\n    model.load_weights('..\/input\/osic-model-weights\/' + weights)\n    return model\n\nmodel_classes = ['b5'] #['b0','b1','b2','b3',b4','b5','b6','b7']\nmodels = [build_model(shape=(512, 512, 1), model_class=m) for m in model_classes]\nprint('Number of models: ' + str(len(models)))","267c2176":"tr_p, vl_p = train_test_split(P, shuffle=True, train_size = 0.8) ","b700d21c":"def score(fvc_true, fvc_pred, sigma):\n    sigma_clip = np.maximum(sigma, 70) # changed from 70, trie 66.7 too\n    delta = np.abs(fvc_true - fvc_pred)\n    delta = np.minimum(delta, 1000)\n    sq2 = np.sqrt(2)\n    metric = (delta \/ sigma_clip)*sq2 + np.log(sigma_clip* sq2)\n    return np.mean(metric)","1106cc1e":"subs = []\nfor model in models:\n    metric = []\n    for q in tqdm(range(1, 10)):\n        m = []\n        for p in vl_p:\n            x = [] \n            tab = [] \n\n            if p in ['ID00011637202177653955184', 'ID00052637202186188008618']:\n                continue\n\n            ldir = os.listdir(f'..\/input\/osic-pulmonary-fibrosis-progression\/train\/{p}\/')\n            for i in ldir:\n                if int(i[:-4]) \/ len(ldir) < 0.8 and int(i[:-4]) \/ len(ldir) > 0.15:\n                    x.append(get_img(f'..\/input\/osic-pulmonary-fibrosis-progression\/train\/{p}\/{i}')) \n                    tab.append(get_tab(train.loc[train.Patient == p, :])) \n            if len(x) < 1:\n                continue\n            tab = np.array(tab) \n\n            x = np.expand_dims(x, axis=-1) \n            _a = model.predict([x, tab]) \n            a = np.quantile(_a, q \/ 10)\n\n            percent_true = train.Percent.values[train.Patient == p]\n            fvc_true = train.FVC.values[train.Patient == p]\n            weeks_true = train.Weeks.values[train.Patient == p]\n\n            fvc = a * (weeks_true - weeks_true[0]) + fvc_true[0]\n            percent = percent_true[0] - a * abs(weeks_true - weeks_true[0])\n            m.append(score(fvc_true, fvc, percent))\n        print(np.mean(m))\n        metric.append(np.mean(m))\n\n    q = (np.argmin(metric) + 1)\/ 10\n\n    sub = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/sample_submission.csv') \n    test = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/test.csv') \n    A_test, B_test, P_test,W, FVC= {}, {}, {},{},{} \n    STD, WEEK = {}, {} \n    for p in test.Patient.unique():\n        x = [] \n        tab = [] \n        ldir = os.listdir(f'..\/input\/osic-pulmonary-fibrosis-progression\/test\/{p}\/')\n        for i in ldir:\n            if int(i[:-4]) \/ len(ldir) < 0.8 and int(i[:-4]) \/ len(ldir) > 0.15:\n                x.append(get_img(f'..\/input\/osic-pulmonary-fibrosis-progression\/test\/{p}\/{i}')) \n                tab.append(get_tab(test.loc[test.Patient == p, :])) \n        if len(x) <= 1:\n            continue\n        tab = np.array(tab) \n\n        x = np.expand_dims(x, axis=-1) \n        _a = model.predict([x, tab]) \n        a = np.quantile(_a, q)\n        A_test[p] = a\n        B_test[p] = test.FVC.values[test.Patient == p] - a*test.Weeks.values[test.Patient == p]\n        P_test[p] = test.Percent.values[test.Patient == p] \n        WEEK[p] = test.Weeks.values[test.Patient == p]\n\n    for k in sub.Patient_Week.values:\n        p, w = k.split('_')\n        w = int(w) \n\n        fvc = A_test[p] * w + B_test[p]\n        sub.loc[sub.Patient_Week == k, 'FVC'] = fvc\n        sub.loc[sub.Patient_Week == k, 'Confidence'] = (\n            P_test[p] - A_test[p] * abs(WEEK[p] - w) \n    ) \n\n    _sub = sub[[\"Patient_Week\",\"FVC\",\"Confidence\"]].copy()\n    subs.append(_sub)","e5ca6f72":"N = len(subs)\nsub = subs[0].copy() # ref\nsub[\"FVC\"] = 0\nsub[\"Confidence\"] = 0\nfor i in range(N):\n    sub[\"FVC\"] += subs[0][\"FVC\"] * (1\/N)\n    sub[\"Confidence\"] += subs[0][\"Confidence\"] * (1\/N)","026269ad":"sub.head()","7f2129be":"sub[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission_img.csv\", index=False)","ddb86b36":"img_sub = sub[[\"Patient_Week\",\"FVC\",\"Confidence\"]].copy()","9ab90b68":"ROOT = \"..\/input\/osic-pulmonary-fibrosis-progression\"\nBATCH_SIZE=128\n\ntr = pd.read_csv(f\"{ROOT}\/train.csv\")\ntr.drop_duplicates(keep=False, inplace=True, subset=['Patient','Weeks'])\nchunk = pd.read_csv(f\"{ROOT}\/test.csv\")\n\nprint(\"add infos\")\nsub = pd.read_csv(f\"{ROOT}\/sample_submission.csv\")\nsub['Patient'] = sub['Patient_Week'].apply(lambda x:x.split('_')[0])\nsub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\nsub =  sub[['Patient','Weeks','Confidence','Patient_Week']]\nsub = sub.merge(chunk.drop('Weeks', axis=1), on=\"Patient\")","b1d7f74b":"tr['WHERE'] = 'train'\nchunk['WHERE'] = 'val'\nsub['WHERE'] = 'test'\ndata = tr.append([chunk, sub])","9972622d":"print(tr.shape, chunk.shape, sub.shape, data.shape)\nprint(tr.Patient.nunique(), chunk.Patient.nunique(), sub.Patient.nunique(), \n      data.Patient.nunique())","29baebf4":"data['min_week'] = data['Weeks']\ndata.loc[data.WHERE=='test','min_week'] = np.nan\ndata['min_week'] = data.groupby('Patient')['min_week'].transform('min')","5e0ce8af":"base = data.loc[data.Weeks == data.min_week]\nbase = base[['Patient','FVC']].copy()\nbase.columns = ['Patient','min_FVC']\nbase['nb'] = 1\nbase['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\nbase = base[base.nb==1]\nbase.drop('nb', axis=1, inplace=True)","aeb88c9d":"data = data.merge(base, on='Patient', how='left')\ndata['base_week'] = data['Weeks'] - data['min_week']\ndel base","212c28c7":"COLS = ['Sex','SmokingStatus'] #,'Age'\nFE = []\nfor col in COLS:\n    for mod in data[col].unique():\n        FE.append(mod)\n        data[mod] = (data[col] == mod).astype(int)","f20dfce9":"#\ndata['age'] = (data['Age'] - data['Age'].min() ) \/ ( data['Age'].max() - data['Age'].min() )\ndata['BASE'] = (data['min_FVC'] - data['min_FVC'].min() ) \/ ( data['min_FVC'].max() - data['min_FVC'].min() )\ndata['week'] = (data['base_week'] - data['base_week'].min() ) \/ ( data['base_week'].max() - data['base_week'].min() )\ndata['percent'] = (data['Percent'] - data['Percent'].min() ) \/ ( data['Percent'].max() - data['Percent'].min() )\nFE += ['age','percent','week','BASE']","749d3364":"tr = data.loc[data.WHERE=='train']\nchunk = data.loc[data.WHERE=='val']\nsub = data.loc[data.WHERE=='test']\ndel data","d8f60f13":"tr.shape, chunk.shape, sub.shape","88866171":"C1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\n\ndef score(y_true, y_pred):\n    tf.dtypes.cast(y_true, tf.float32)\n    tf.dtypes.cast(y_pred, tf.float32)\n    sigma = y_pred[:, 2] - y_pred[:, 0]\n    fvc_pred = y_pred[:, 1]\n    \n    #sigma_clip = sigma + C1\n    sigma_clip = tf.maximum(sigma, C1)\n    delta = tf.abs(y_true[:, 0] - fvc_pred)\n    delta = tf.minimum(delta, C2)\n    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n    metric = (delta \/ sigma_clip)*sq2 + tf.math.log(sigma_clip* sq2)\n    return K.mean(metric)\n\ndef qloss(y_true, y_pred):\n    # Pinball loss for multiple quantiles\n    qs = [0.2, 0.50, 0.8]\n    q = tf.constant(np.array([qs]), dtype=tf.float32)\n    e = y_true - y_pred\n    v = tf.maximum(q*e, (q-1)*e)\n    return K.mean(v)\n\ndef mloss(_lambda):\n    def loss(y_true, y_pred):\n        return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred)\n    return loss\n\ndef make_model(nh):\n    z = L.Input((nh,), name=\"Patient\")\n    x = L.Dense(100, activation=\"relu\", name=\"d1\")(z)\n    x = L.Dense(100, activation=\"relu\", name=\"d2\")(x)\n    p1 = L.Dense(3, activation=\"linear\", name=\"p1\")(x)\n    p2 = L.Dense(3, activation=\"relu\", name=\"p2\")(x)\n    preds = L.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis=1), \n                     name=\"preds\")([p1, p2])\n    \n    model = M.Model(z, preds, name=\"CNN\")\n    model.compile(loss=mloss(0.65), optimizer=tf.keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False), metrics=[score])\n    return model","17b2f182":"y = tr['FVC'].values\nz = tr[FE].values\nze = sub[FE].values\nnh = z.shape[1]\npe = np.zeros((ze.shape[0], 3))\npred = np.zeros((z.shape[0], 3))","f8e89495":"net = make_model(nh)\nprint(net.summary())\nprint(net.count_params())","5d191166":"NFOLD = 5 # originally 5\nkf = KFold(n_splits=NFOLD)","5cabd356":"%%time\ncnt = 0\nEPOCHS = 800\nfor tr_idx, val_idx in kf.split(z):\n    cnt += 1\n    print(f\"FOLD {cnt}\")\n    net = make_model(nh)\n    net.fit(z[tr_idx], y[tr_idx], batch_size=BATCH_SIZE, epochs=EPOCHS, \n            validation_data=(z[val_idx], y[val_idx]), verbose=0) #\n    print(\"train\", net.evaluate(z[tr_idx], y[tr_idx], verbose=0, batch_size=BATCH_SIZE))\n    print(\"val\", net.evaluate(z[val_idx], y[val_idx], verbose=0, batch_size=BATCH_SIZE))\n    print(\"predict val...\")\n    pred[val_idx] = net.predict(z[val_idx], batch_size=BATCH_SIZE, verbose=0)\n    print(\"predict test...\")\n    pe += net.predict(ze, batch_size=BATCH_SIZE, verbose=0) \/ NFOLD","061f5027":"sigma_opt = mean_absolute_error(y, pred[:, 1])\nunc = pred[:,2] - pred[:, 0]\nsigma_mean = np.mean(unc)\nprint(sigma_opt, sigma_mean)","be9a315d":"idxs = np.random.randint(0, y.shape[0], 100)\nplt.plot(y[idxs], label=\"ground truth\")\nplt.plot(pred[idxs, 0], label=\"q25\")\nplt.plot(pred[idxs, 1], label=\"q50\")\nplt.plot(pred[idxs, 2], label=\"q75\")\nplt.legend(loc=\"best\")\nplt.show()","14d1bedb":"print(unc.min(), unc.mean(), unc.max(), (unc>=0).mean())","f36e8a75":"plt.hist(unc)\nplt.title(\"uncertainty in prediction\")\nplt.show()","fcc66857":"sub.head()","7bf1c581":"# PREDICTION\nsub['FVC1'] = 1.*pe[:, 1]\nsub['Confidence1'] = pe[:, 2] - pe[:, 0]\nsubm = sub[['Patient_Week','FVC','Confidence','FVC1','Confidence1']].copy()\nsubm.loc[~subm.FVC1.isnull()].head(10)","04304cb7":"subm.loc[~subm.FVC1.isnull(),'FVC'] = subm.loc[~subm.FVC1.isnull(),'FVC1']\nif sigma_mean<70:\n    subm['Confidence'] = sigma_opt\nelse:\n    subm.loc[~subm.FVC1.isnull(),'Confidence'] = subm.loc[~subm.FVC1.isnull(),'Confidence1']","6dabd4e8":"subm.head()","1da105d3":"subm.describe().T","f98ba0f1":"otest = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/test.csv')\nfor i in range(len(otest)):\n    subm.loc[subm['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'FVC'] = otest.FVC[i]\n    subm.loc[subm['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'Confidence'] = 0.1","520cac12":"subm[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission_regression.csv\", index=False)","7bd779e8":"reg_sub = subm[[\"Patient_Week\",\"FVC\",\"Confidence\"]].copy()","537bc126":"df1 = img_sub.sort_values(by=['Patient_Week'], ascending=True).reset_index(drop=True)\ndf2 = reg_sub.sort_values(by=['Patient_Week'], ascending=True).reset_index(drop=True)","ed463a31":"df = df1[['Patient_Week']].copy()\ndf['FVC'] = FVC_weight*df1['FVC'] + (1-FVC_weight)*df2['FVC']\ndf['Confidence'] = Confidence_weight*df1['Confidence'] + (1-Confidence_weight)*df2['Confidence']\ndf.head()","b95d828b":"df.to_csv('submission.csv', index=False)","c6e3dd45":"Other parameters should be changed to increase accuracy.","96c4e2c7":"### Commit 11\n\n* Dropout_model = 0.36\n* FVC_weight = 0.15\n* Confidence_weight = 0.15\n\nLB = -6.8100","ecc434f5":"### Commit 9\n\n* Dropout_model = 0.35\n* FVC_weight = 0.3\n* Confidence_weight = 0.3\n\nLB = -6.8125","fa6eccd6":"### Commit 21\n\n* Dropout_model = 0.38\n* FVC_weight = 0.19\n* Confidence_weight = 0.19\n\nLB = -6.8093","f8e6bd19":"## 3. Download data, auxiliary functions and model tuning <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","149c13b9":"[Go to Top](#0)","783c1fd9":"### Commit 29\n\n* Dropout_model = 0.38\n* FVC_weight = 0.2\n* Confidence_weight = 0.2\n* GaussianNoise_stddev = 0.15\n\nLB = -6.8092","92a0e63d":"## 1. Import libraries <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","f07c44b5":"<a class=\"anchor\" id=\"0\"><\/a>\n# [OSIC Pulmonary Fibrosis Progression](https:\/\/www.kaggle.com\/c\/osic-pulmonary-fibrosis-progression)","c1f6110d":"### Commit 32\n\n* Dropout_model = 0.24\n* FVC_weight = 0.14\n* Confidence_weight = 0.14\n* GaussianNoise_stddev = 0.2\n\nLB = -6.8106","800d5528":"## 4.1 Average prediction <a class=\"anchor\" id=\"4.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","128895bd":"### Commit 16\n\n* Dropout_model = 0.25\n* FVC_weight = 0.2\n* Confidence_weight = 0.2\n\nLB = -6.8093","fad23420":"## 2.2 Previous commits <a class=\"anchor\" id=\"2.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","82e82e18":"### Commit 15\n\n* Dropout_model = 0.32\n* FVC_weight = 0.2\n* Confidence_weight = 0.2\n\nLB = -6.8092","9fa95bab":"### Commit 19\n\n* Dropout_model = 0.39\n* FVC_weight = 0.2\n* Confidence_weight = 0.2\n\nLB = -6.8090","7e95fa39":"## 4. Prediction and submission <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","f7ed7a83":"### Commit 8\n\n* Dropout_model = 0.35\n* FVC_weight = 0.25\n* Confidence_weight = 0.26\n\nLB = -6.8107","c08fd42a":"### I use the notebook [Higher LB score by tuning mloss (around -6.811)](https:\/\/www.kaggle.com\/reighns\/higher-lb-score-by-tuning-mloss-around-6-811) from [Hongnan Gao](https:\/\/www.kaggle.com\/reighns) as a basis and will try to tune its various parameters. \n\n#### From the first attempt, it was possible to improve the LB score","b7b7a30d":"### Commit 1 (parameters from https:\/\/www.kaggle.com\/reighns\/higher-lb-score-by-tuning-mloss)\n\n* Dropout_model = 0.4\n* FVC_weight = 0.25\n* Confidence_weight = 0.26\n\nLB = -6.8111","fc007148":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [My upgrade](#2)\n    -  [Commit now](#2.1)\n    -  [Previous commits](#2.2)\n    -  [Parameters and LB score visualization](#2.3)\n1. [Download data, auxiliary functions and model tuning](#3)\n1. [Prediction and submission](#4)\n    -  [Average prediction](#4.1)\n    -  [Osic-Multiple-Quantile-Regression](#4.2)\n    -  [The change of mloss](#4.3) \n    -  [Ensemble and blending](#4.4)","c6c1d89e":"### Commit 20\n\n* Dropout_model = 0.37\n* FVC_weight = 0.2\n* Confidence_weight = 0.2\n\nLB = -6.8092","40311004":"### Commit 10\n\n* Dropout_model = 0.36\n* FVC_weight = 0.2\n* Confidence_weight = 0.2\n\nLB = -6.8089","0b74a56d":"### Commit 4\n\n* Dropout_model = 0.36\n* FVC_weight = 0.25\n* Confidence_weight = 0.26\n\nLB = -6.8105","9212725d":"### Commit 26\n\n* Dropout_model = 0.385\n* FVC_weight = 0.2\n* Confidence_weight = 0.2\n\nLB = -6.8092","53128881":"## 2.1. Commit now <a class=\"anchor\" id=\"2.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","c0d8360f":"### Commit 27\n\n* Dropout_model = 0.38\n* FVC_weight = 0.21\n* Confidence_weight = 0.21\n\nLB = -6.8091","891cdcd5":"### Commit 14\n\n* Dropout_model = 0.36\n* FVC_weight = 0.225\n* Confidence_weight = 0.225\n\nLB = -6.8100","a61666d4":"## 2. My upgrade <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","0a2d0a58":"### Commit 18\n\n* Dropout_model = 0.38\n* FVC_weight = 0.2\n* Confidence_weight = 0.2\n\nLB = -6.8087","65ff8a25":"## 2.3 Parameters and LB score visualization <a class=\"anchor\" id=\"2.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","fd87749a":"## 4.4 Ensemble and blending <a class=\"anchor\" id=\"4.4\"><\/a>\n\n[Back to Table of Contents](#0.1)","ce4a8b60":"# Acknowledgements\n\n* Ulrich GOUE's Osic-Multiple-Quantile-Regression-Starter\n    - Model that uses images can be found at: https:\/\/www.kaggle.com\/miklgr500\/linear-decay-based-on-resnet-cnn\n* Michael Kazachok's Linear Decay (based on ResNet CNN)\n    - Model that uses tabular data can be found at: https:\/\/www.kaggle.com\/ulrich07\/osic-multiple-quantile-regression-starter\n* Replaced Michael's model with EfficientNets B0, B2, B4\n* Tuning the parameters of the models from https:\/\/www.kaggle.com\/reighns\/higher-lb-score-by-tuning-mloss \n* https:\/\/www.kaggle.com\/leoisleo1\/efficientnets-quantile-regression-inference\n* https:\/\/www.kaggle.com\/khoongweihao\/efficientnets-quantile-regression-inference","ae80b194":"### Commit 31\n\n* Dropout_model = 0.38\n* FVC_weight = 0.2\n* Confidence_weight = 0.2\n* GaussianNoise_stddev = 0.21\n\nLB = -6.8093","d4d459df":"### Commit 28\n\n* Dropout_model = 0.38\n* FVC_weight = 0.2\n* Confidence_weight = 0.2\n* GaussianNoise_stddev = 0.25\n\nLB = -6.8090","1d2aceba":"### Commit 23\n\n* Dropout_model = 0.38\n* FVC_weight = 0.2\n* Confidence_weight = 0.2\n\nLB = -6.8402","13490757":"### seed_everything\n* Commits 1-21, 24...: seed_everything = 42\n* Commit 23: seed_everything = 0","b85266e9":"### Commit 17\n\n* Dropout_model = 0.25\n* FVC_weight = 0.5\n* Confidence_weight = 0.5\n\nLB = -6.8283","f931ee6e":"### Commit 13\n\n* Dropout_model = 0.36\n* FVC_weight = 0.175\n* Confidence_weight = 0.175\n\nLB = -6.8096","ada94afa":"## 4.2 Osic-Multiple-Quantile-Regression <a class=\"anchor\" id=\"4.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","57eee5ce":"## 4.3 The change of mloss <a class=\"anchor\" id=\"4.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","c302a99c":"### Commit 6\n\n* Dropout_model = 0.36\n* FVC_weight = 0.35\n* Confidence_weight = 0.36\n\nLB = -6.8158"}}