{"cell_type":{"828bd62d":"code","60f467db":"code","0ffdd16d":"code","aa850efc":"code","b590fb02":"code","24af22dc":"code","79905e26":"code","98346ff8":"markdown","8b8cfb4f":"markdown","dff8fe94":"markdown","d06b58de":"markdown","5729e420":"markdown","fda25f63":"markdown","3f68b460":"markdown"},"source":{"828bd62d":"import numpy as np\nimport gym\nimport random","60f467db":"env = gym.make('FrozenLake-v0')\nenv.render()","0ffdd16d":"state_space = env.observation_space.n\naction_space = env.action_space.n\n\nQ = np.zeros((state_space, action_space))\nQ.size","aa850efc":"total_episodes = 25000 # Training episodes\ntotal_test_episodes = 10 # Testing episodes\nmax_steps = 100\n\nalpha = 0.1 # Learning rate\ngamma = 0.99 # Discount rate\n\n# Exploration parameters\nepsilon = 1.0\nmax_epsilon = 1.0\nmin_epsilon = 0.001\ndecay_rate = 0.01","b590fb02":"def epsilon_greedy_policy(Q, state):\n    if (random.uniform(0,1) > epsilon):\n        # If we're outside of (higher than) the epsilon range, use our Q value\n        action = np.argmax(Q[state])\n    else:\n        # Otherwise, pick a random action.\n        action = env.action_space.sample()\n    \n    return action","24af22dc":"for episode in range(total_episodes):\n    state = env.reset()\n    step = 0\n    done = False\n\n    # Exponentially decay epsilon. This is so that as we train over and over we \"exploit\" \n    # our Q value table more and more (instead of just exploring).\n    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode )\n    \n    for step in range(max_steps):\n        # Pick an action\n        action = epsilon_greedy_policy(Q, state)\n    \n        # Take the action\n        new_state, reward, done, info = env.step(action)\n\n        # Update our Q\n        Q[state][action] = Q[state][action] + alpha*(reward + gamma*np.max(Q[new_state]) - Q[state][action])\n\n        if done == True:\n            break\n            \n        # We're in the new state now\n        state = new_state","79905e26":"import time\nrewards = []\n\nframes = []\nfor episode in range(total_test_episodes):\n    state = env.reset()\n    step = 0\n    done = False\n    total_rewards = 0\n    print(\"****************************************************\")\n    print(\"EPISODE \", episode)\n    for step in range(max_steps):\n        env.render()     \n        # Take the action (index) that have the maximum expected future reward given that state\n        action = np.argmax(Q[state][:])\n        new_state, reward, done, info = env.step(action)\n        total_rewards += reward\n        \n        if done:\n            rewards.append(total_rewards)\n            #print (\"Score\", total_rewards)\n            break\n        state = new_state\nenv.close()\nprint (\"Score over time: \" +  str(sum(rewards)\/total_test_episodes))","98346ff8":"# Make Enviornment\nThis is using the ","8b8cfb4f":"This is from the Deep Reinforcement course by Thomas Simonini. This particular notebook is from his second chapter where he uses the Q-Learning algorithm to train a taxi. In working through his example myself I changed the example to a the frozen lake gym, just for a chance to work through it myself.\n\nReference:\n* https:\/\/thomassimonini.medium.com\/q-learning-lets-create-an-autonomous-taxi-part-2-2-8cbafa19d7f5","dff8fe94":"The state space is the collection of all possible locations on a frozen lake you can be in. The action space is all possible directions you can move in. ","d06b58de":"# Policy","5729e420":"# Training","fda25f63":"# Hyperparameters","3f68b460":"# Output"}}