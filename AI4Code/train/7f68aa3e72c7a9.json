{"cell_type":{"9ae9fa75":"code","e2e691a4":"code","2fd0a98a":"code","5d83b4ac":"code","71f676ef":"code","20160019":"code","f787ac0f":"code","e11bb685":"code","8ab0b905":"code","ab25398d":"code","ab0a2fb1":"markdown","e22bd991":"markdown","a7d63ca4":"markdown","68ed9569":"markdown","4c92aabb":"markdown","9b9575b6":"markdown","e4f077db":"markdown","69e1d4e4":"markdown"},"source":{"9ae9fa75":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt76de7\nimport tensorflow as tf\nimport json","e2e691a4":"%%time\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\ntrain = pd.read_pickle('..\/input\/ubiquant-market-prediction-half-precision-pickle\/train.pkl')\ntrain.head()","2fd0a98a":"investment_id = train.pop(\"investment_id\")\ninvestment_id.head()","5d83b4ac":"time_id = train.pop(\"time_id\")","71f676ef":"y = train.pop(\"target\")\ny.head()","20160019":"def create_record(i):\n    dic = {}\n    dic[f\"features\"] = tf.train.Feature(float_list=tf.train.FloatList(value=list(train.iloc[i])))\n    dic[\"time_id\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[time_id.iloc[i]]))\n    dic[\"investment_id\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[investment_id.iloc[i]]))\n    dic[\"target\"] = tf.train.Feature(float_list=tf.train.FloatList(value=[y.iloc[i]]))\n    record_bytes = tf.train.Example(features=tf.train.Features(feature=dic)).SerializeToString()\n    return record_bytes\n    \ndef decode_function(record_bytes):\n  return tf.io.parse_single_example(\n      # Data\n      record_bytes,\n      # Schema\n      {\n          \"features\": tf.io.FixedLenFeature([300], dtype=tf.float32),\n          \"time_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"investment_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"target\": tf.io.FixedLenFeature([], dtype=tf.float32)\n      }\n  )","f787ac0f":"folds = 10\ntime_id_value_counts = dict(time_id.value_counts())\ntime_ids = sorted(time_id.unique())\nsample_per_fold = train.shape[0] \/\/ folds\nfold = 0\nsample_count_for_fold = 0\ntime_ids_for_fold = []\nfold_info = []\ntotal_count = 0\nfor i in range(len(time_ids)):\n    identifier = time_ids[i]\n    sample_count_for_fold += time_id_value_counts[identifier]\n    time_ids_for_fold.append(identifier)\n    if sample_count_for_fold >= sample_per_fold or i == len(time_ids) - 1:\n        print(f\"Sample Count for Fold {fold}\", sample_count_for_fold)\n        fold_info.append({\"sample_count\": sample_count_for_fold, \"time_ids\": time_ids_for_fold, \"start_position\": total_count, \"end_position\": total_count + sample_count_for_fold - 1, \"file_name\": f\"fold_{fold}.tfrecords\"})\n        total_count += sample_count_for_fold\n        sample_count_for_fold = 0\n        time_ids_for_current_fold = []\n        fold += 1\ninfo = pd.DataFrame(fold_info)\ninfo.to_csv(\"info.csv\", index=False)","e11bb685":"%%time\nsave_path = \"sample.tfrecords\"\nwith tf.io.TFRecordWriter(save_path) as file_writer:\n    for i in range(10000):\n        record_bytes = create_record(i)\n        file_writer.write(record_bytes)\ndataset = tf.data.TFRecordDataset([save_path])\ndataset = dataset.map(decode_function).batch(32)\nfor item in dataset.take(1):\n    print(item)","8ab0b905":"%%time\nimport time\nfor i, info in enumerate(fold_info):\n    begin = time.time()\n    save_path = f\"fold_{i}.tfrecords\"\n    print(f\"Create {save_path}\")\n    print(f\"Begin position: %d, End Position: %d\"%(info[\"start_position\"], info[\"end_position\"]))\n    save_path = f\"fold_{i}.tfrecords\"\n    with tf.io.TFRecordWriter(save_path) as file_writer:\n        for i in range(info[\"start_position\"], info[\"end_position\"]):\n            record_bytes = create_record(i)\n            file_writer.write(record_bytes)\n    print(\"Elapsed time: %.2f\"%(time.time() - begin))","ab25398d":"investment_ids = investment_id.unique()\ninvestment_id_df = pd.DataFrame({\"investment_id\": investment_ids})\ninvestment_id_df.to_csv(\"investment_ids.csv\", index=False)","ab0a2fb1":"Now separate dataset to 10 folds and make sure samples with same time_id appear only in the same fold. So the sample count for each fold are slightly different. In the same time, I also output some useful fold information for convenience.","e22bd991":"Let's see a simple sample of creating and reading TF-Record.","a7d63ca4":"#  Create TF-Record for UMP dataset\n\nIn this notebook, I am going to create TF-Record for UMP dataset. I am going to divide the dataset to 10 folds by using Time Series Split. You may experiment other types of Data Spliting method.","68ed9569":"Now create the whole dataset, it will take more than 1 hour.","4c92aabb":"## Write unique Investment Ids","9b9575b6":"## Import Packages","e4f077db":"## Import dataset","69e1d4e4":"## Create TF-Record"}}