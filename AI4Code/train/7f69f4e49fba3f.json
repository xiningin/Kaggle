{"cell_type":{"4d16c14f":"code","1f3c887a":"code","a27e3562":"code","c52d2267":"code","03cbb968":"code","e3bc99c3":"code","26729d8a":"code","53b4829d":"code","950577dd":"code","2e412eaa":"code","217c9968":"code","8b7f77a5":"code","0bbb2bb9":"code","c241814b":"code","dacf3f5e":"code","de9153a4":"code","730f719a":"code","c8bd975e":"code","9bd27dfd":"code","13044514":"code","22a82e5d":"code","9750b695":"code","11a1e061":"code","100a9956":"code","4b28f1f5":"markdown","6a47b90c":"markdown","3f755a91":"markdown","b5081ebf":"markdown","9ddea67c":"markdown","29dbfef1":"markdown","283a82f8":"markdown","6e3078a4":"markdown","0293b59c":"markdown","a5deb369":"markdown","14a6f20d":"markdown","0aa85fc5":"markdown","5807a6ee":"markdown","2b1cdb3b":"markdown","56810207":"markdown","878c226b":"markdown","007fc3f0":"markdown"},"source":{"4d16c14f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt","1f3c887a":"train = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')","a27e3562":"train.head()","c52d2267":"test.head()","03cbb968":"# identify feature columns and label column\nX = train.copy().drop(columns=['id'])\ny = X.pop('target')\nX","e3bc99c3":"y.unique()","26729d8a":"y.value_counts() ","53b4829d":"for i in range(len(y)):\n    y[i]=(y[i][-1])","950577dd":"y.describe()","2e412eaa":"y.value_counts() ","217c9968":"# All discrete features should now have integer dtypes (double-check this before using MI!)\ndiscrete_features = X.dtypes == int","8b7f77a5":"# calculate MI scores for our features\nfrom sklearn.feature_selection import mutual_info_regression\n\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X, y, discrete_features)\nmi_scores[::3]  # show a few features with their MI scores","0bbb2bb9":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\n\nplt.figure(dpi=100, figsize=(8, 12))\nplot_mi_scores(mi_scores)","c241814b":"# Standardize and normalize the data.\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\n\n# Standardization\nX = StandardScaler().fit_transform(X)\n\n# Normalization\nX = MinMaxScaler().fit_transform(X)\n","dacf3f5e":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42)","de9153a4":"from imblearn.over_sampling import SMOTE\nX_train, y_train = SMOTE().fit_resample(X_train, y_train)","730f719a":"import xgboost as xgb\n\nxgb_model = xgb.XGBClassifier(objective='multi:softprob', num_class=4, n_estimators = 500)\n\nxgb_model.fit(X_train, y_train,\n              early_stopping_rounds=50,\n              eval_metric=['mlogloss'],\n              eval_set=[(X_train, y_train),(X_test, y_test)]\n             ) \n","c8bd975e":"from sklearn.metrics import accuracy_score\n# Get predictions for train data\npredictions_train = xgb_model.predict(X_train)\n\nprint('Accuracy - Train:', accuracy_score(y_train, predictions_train))\n","9bd27dfd":"from sklearn.metrics import multilabel_confusion_matrix\ny_unique = y.unique()\nmcm_train = multilabel_confusion_matrix(y_train, predictions_train, labels = y_unique)\nprint(mcm_train)","13044514":"\n# Get predictions from test data\npredictions_test = xgb_model.predict(X_test)\n\nprint('Accuracy - Test:', accuracy_score(y_test, predictions_test))\n","22a82e5d":"mcm_test = multilabel_confusion_matrix(y_test, predictions_test, labels = y_unique)\nmcm_test","9750b695":"test","11a1e061":"test = test.drop(columns=['id'])\n\n# Output test results\npredictions = xgb_model.predict_proba(test)\nclass_labels = ['Class_1','Class_2','Class_3','Class_4']\n\n\n#sample_submission.drop(columns=class_labels, inplace=True)\nsubmission =pd.DataFrame(data=predictions, columns=class_labels)\nid = list(range(100000, 150000))\nsubmission.insert(0,'id',id)\nsubmission.to_csv(\"my_submission.csv\", index=False)","100a9956":"submission","4b28f1f5":"#### Strip the last character from the label - so that our labels will just be (1,2,3,4)","6a47b90c":"#### Plot mutual information","3f755a91":"#### Look at the unique list of labels","b5081ebf":"#### Model - XGBoost","9ddea67c":"### Split data for train and test","29dbfef1":"#### Apply the model to test data set - Save as submission file","283a82f8":"#### Confusion Matrix for the train data set","6e3078a4":"#### Get predictions and calculate accuracy score","0293b59c":"#### How many in each label group","a5deb369":"#### Use SMOTE for imbalanced data","14a6f20d":"#### Test dataset","0aa85fc5":"#### Standarize and Normalize data","5807a6ee":"#### Separate features and label - remove the id column","2b1cdb3b":"### Import data and see what it looks like","56810207":"##### The counts per label should remain the same","878c226b":"#### Train dataset","007fc3f0":"#### Calculate mutual information"}}