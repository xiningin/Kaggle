{"cell_type":{"e1397e56":"code","515376ec":"code","c164c5ae":"code","eef69c71":"code","01ec8bae":"code","8fd776df":"code","e2fffb9c":"code","407e4ecb":"code","65973805":"code","a10ba422":"code","9eac0e46":"code","455030ec":"code","ab5890a3":"code","9b88ba86":"code","847560b8":"code","6d35a367":"code","19f7fbbd":"markdown","81ab7325":"markdown","7624d4e7":"markdown","7c31c38c":"markdown","aae8dca6":"markdown","ff09bd60":"markdown","27980e0b":"markdown","8aa6bd04":"markdown","49d760b1":"markdown","4a94271f":"markdown","68a17bff":"markdown","ee04c5bb":"markdown","7c0c377c":"markdown","7c023425":"markdown","39c4e185":"markdown","84d4046a":"markdown","f8a81173":"markdown","9fb37bc2":"markdown","c284c394":"markdown"},"source":{"e1397e56":"# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Suppress matplotlib user warnings\nimport warnings   \nwarnings.filterwarnings(\"ignore\", category = UserWarning, module = \"matplotlib\")\n\n# Allows the use of display() for DataFrames\nfrom IPython.display import display \n\n# Display inline matplotlib plots with IPython\nfrom IPython import get_ipython\nget_ipython().run_line_magic('matplotlib', 'inline')\n\n# Pretty display for notebooks\n%matplotlib inline\n\n# Load the dataset\n\nin_file = '..\/input\/titanic-data\/titanic_data.csv'\n\nfull_data = pd.read_csv(in_file)\n\n# Print the first few entries of the RMS Titanic data\ndisplay(full_data.head())\n#display(full_data)","515376ec":"# Store the 'Survived' feature in a new variable and remove it from the dataset\n\noutcomes = full_data['Survived']\ndata = full_data.drop('Survived', axis = 1)\n\n# Show the new dataset with 'Survived' removed\ndisplay(data.head())","c164c5ae":"def accuracy_score(truth, pred):\n    \"\"\" Returns accuracy score for input truth and predictions. \"\"\"\n    \n    # Ensure that the number of predictions matches number of outcomes\n    if len(truth) == len(pred): \n        \n        # Calculate and return the accuracy as a percent\n        return \"Predictions have an accuracy of {:.2f}%.\".format((truth == pred).mean()*100)\n    \n    else:\n        return \"Number of predictions does not match number of outcomes!\"\n    ","eef69c71":"# Test the 'accuracy_score' function\npredictions = pd.Series(np.ones(5, dtype = int))\nprint(accuracy_score(outcomes[:5], predictions))","01ec8bae":"def predictions_0(data):\n    \"\"\" Model with no features. Always predicts a passenger did not survive. \"\"\"\n\n    predictions = []\n    for _, passenger in data.iterrows():\n        \n        # Predict the survival of 'passenger'\n        predictions.append(0)\n    \n    # Return our predictions\n    return pd.Series(predictions)\n\n# Make the predictions\npredictions = predictions_0(data)","8fd776df":"print(accuracy_score(outcomes, predictions))","e2fffb9c":"def filter_data(data, condition):\n    \"\"\"\n    Remove elements that do not match the condition provided.\n    Takes a data list as input and returns a filtered list.\n    Conditions should be a list of strings of the following format:\n      '<field> <op> <value>'\n    where the following operations are valid: >, <, >=, <=, ==, !=\n    \n    Example: [\"Sex == 'male'\", 'Age < 18']\n    \"\"\"\n\n    field, op, value = condition.split(\" \")\n    \n    # convert value into number or strip excess quotes if string\n    try:\n        value = float(value)\n    except:\n        value = value.strip(\"\\'\\\"\")\n    \n    # get booleans for filtering\n    if op == \">\":\n        matches = data[field] > value\n    elif op == \"<\":\n        matches = data[field] < value\n    elif op == \">=\":\n        matches = data[field] >= value\n    elif op == \"<=\":\n        matches = data[field] <= value\n    elif op == \"==\":\n        matches = data[field] == value\n    elif op == \"!=\":\n        matches = data[field] != value\n    else: # catch invalid operation codes\n        raise Exception(\"Invalid comparison operator. Only >, <, >=, <=, ==, != allowed.\")\n    \n    # filter data and outcomes\n    data = data[matches].reset_index(drop = True)\n    return data","407e4ecb":"def survival_stats(data, outcomes, key, filters = []):\n    \"\"\"\n    Print out selected statistics regarding survival, given a feature of\n    interest and any number of filters (including no filters)\n    \"\"\"\n    \n    # Check that the key exists\n    if key not in data.columns.values :\n        print(\"'{}' is not a feature of the Titanic data. Did you spell something wrong?\".format(key))\n        return False\n\n    # Return the function before visualizing if 'Cabin' or 'Ticket'\n    # is selected: too many unique categories to display\n    if(key == 'Cabin' or key == 'PassengerId' or key == 'Ticket'):\n        print(\"'{}' has too many unique categories to display! Try a different feature.\".format(key))\n        return False\n\n    # Merge data and outcomes into single dataframe\n    all_data = pd.concat([data, outcomes.to_frame()], axis = 1)\n    \n    # Apply filters to data\n    for condition in filters:\n        all_data = filter_data(all_data, condition)\n\n    # Create outcomes DataFrame\n    all_data = all_data[[key, 'Survived']]\n    \n    # Create plotting figure\n    plt.figure(figsize=(8,6))\n\n    # 'Numerical' features\n    if(key == 'Age' or key == 'Fare'):\n        \n        # Remove NaN values from Age data\n        all_data = all_data[~np.isnan(all_data[key])]\n        \n        # Divide the range of data into bins and count survival rates\n        min_value = all_data[key].min()\n        max_value = all_data[key].max()\n        value_range = max_value - min_value\n\n        # 'Fares' has larger range of values than 'Age' so create more bins\n        if(key == 'Fare'):\n            bins = np.arange(0, all_data['Fare'].max() + 20, 20)\n        if(key == 'Age'):\n            bins = np.arange(0, all_data['Age'].max() + 10, 10)\n        \n        # Overlay each bin's survival rates\n        nonsurv_vals = all_data[all_data['Survived'] == 0][key].reset_index(drop = True)\n        surv_vals = all_data[all_data['Survived'] == 1][key].reset_index(drop = True)\n        plt.hist(nonsurv_vals, bins = bins, alpha = 0.6,\n                 color = 'red', label = 'Did not survive')\n        plt.hist(surv_vals, bins = bins, alpha = 0.6,\n                 color = 'green', label = 'Survived')\n    \n        # Add legend to plot\n        plt.xlim(0, bins.max())\n        plt.legend(framealpha = 0.8)\n    \n    # 'Categorical' features\n    else:\n       \n        # Set the various categories\n        if(key == 'Pclass'):\n            values = np.arange(1,4)\n        if(key == 'Parch' or key == 'SibSp'):\n            values = np.arange(0,np.max(data[key]) + 1)\n        if(key == 'Embarked'):\n            values = ['C', 'Q', 'S']\n        if(key == 'Sex'):\n            values = ['male', 'female']\n\n        # Create DataFrame containing categories and count of each\n        frame = pd.DataFrame(index = np.arange(len(values)), columns=(key,'Survived','NSurvived'))\n        for i, value in enumerate(values):\n            frame.loc[i] = [value, \\\n                   len(all_data[(all_data['Survived'] == 1) & (all_data[key] == value)]), \\\n                   len(all_data[(all_data['Survived'] == 0) & (all_data[key] == value)])]\n\n        # Set the width of each bar\n        bar_width = 0.4\n\n        # Display each category's survival rates\n        for i in np.arange(len(frame)):\n            nonsurv_bar = plt.bar(i-bar_width, frame.loc[i]['NSurvived'], width = bar_width, color = 'r')\n            surv_bar = plt.bar(i, frame.loc[i]['Survived'], width = bar_width, color = 'g')\n\n            plt.xticks(np.arange(len(frame)), values)\n            plt.legend((nonsurv_bar[0], surv_bar[0]),('Did not survive', 'Survived'), framealpha = 0.8)\n\n    # Common attributes for plot formatting\n    plt.xlabel(key)\n    plt.ylabel('Number of Passengers')\n    plt.title('Passenger Survival Statistics With \\'%s\\' Feature'%(key))\n    plt.show()\n\n    # Report number of passengers with missing values\n    if sum(pd.isnull(all_data[key])):\n        nan_outcomes = all_data[pd.isnull(all_data[key])]['Survived']\n        print(\"Passengers with missing '{}' values: {} ({} survived, {} did not survive)\".format( \\\n              key, len(nan_outcomes), sum(nan_outcomes == 1), sum(nan_outcomes == 0)))","65973805":"survival_stats(data, outcomes, 'Sex')","a10ba422":"def predictions_1(data):\n    \"\"\" Model with one feature: \n            - Predict a passenger survived if they are female. \"\"\"\n    \n    predictions = []\n    for _, passenger in data.iterrows():\n        \n        # Remove the 'pass' statement below \n        # and write your prediction conditions here\n        #pass\n        if passenger['Sex'] == 'female':\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    \n    # Return our predictions\n    return pd.Series(predictions)\n\n# Make the predictions\npredictions = predictions_1(data)","9eac0e46":"print(accuracy_score(outcomes, predictions))","455030ec":"survival_stats(data, outcomes, 'Age', [\"Sex == 'male'\"])","ab5890a3":"def predictions_2(data):\n    \"\"\" Model with two features: \n            - Predict a passenger survived if they are female.\n            - Predict a passenger survived if they are male and younger than 10. \"\"\"\n    \n    predictions = []\n    for _, passenger in data.iterrows():\n        \n        # Remove the 'pass' statement below \n        # and write your prediction conditions here\n        #pass\n        if passenger['Sex'] == 'female':\n            predictions.append(1)\n        else:\n            if passenger['Age'] < 10:\n                predictions.append(1)\n            else:\n                predictions.append(0)\n    \n    # Return our predictions\n    return pd.Series(predictions)\n\n# Make the predictions\npredictions = predictions_2(data)","9b88ba86":"print(accuracy_score(outcomes, predictions))","847560b8":"#vs.survival_stats(data, outcomes, 'Age', [\"Sex == 'male'\", \"Age < 18\"])\nsurvival_stats(data, outcomes, 'SibSp', [\"Sex == 'female'\"])","6d35a367":"print(accuracy_score(outcomes, predictions))","19f7fbbd":"**Example:** *Out of the first five passengers, if we predict that all of them survived, what would you expect the accuracy of our predictions to be?*","81ab7325":"After exploring the survival statistics visualization, the function below will make our prediction in which we keep track of the various features and conditions we tried before arriving at our final prediction model.","7624d4e7":"***\nAnd also creating  the `survival_stats` function. This function will take three arguments. The first two parameters passed to the function are the RMS Titanic data and passenger survival outcomes, respectively. The third parameter indicates which feature we want to plot survival statistics across.  ","7c31c38c":"The code below shows the accuracy of our  predictions.","aae8dca6":"# Making Predictions\n\nIf we were asked to make a prediction about any passenger aboard the RMS Titanic whom we knew nothing about, then the best prediction we could make would be that they did not survive. This is because we can assume that a majority of the passengers (more than 50%) did not survive the ship sinking.  \nThe `predictions_0` function below will always predict that a passenger did not survive.","ff09bd60":"# Data Exploration\n## By Yosry Negm\n\n<br><hr><br>\nThis Project introduces how to explore data and build simple prediction model, to do so let us back to the year 1912, when the ship RMS Titanic struck an iceberg on its maiden voyage and sank, resulting in the deaths of most of its passengers and crew. In this project, I will explore a subset of the RMS Titanic passenger manifest to determine which features best predict whether someone survived or did not survive. To begin working with the RMS Titanic passenger data, we'll first need to `import` the functionality we need, and load our data into a `pandas` DataFrame. Hence, the code cell below will load our data and display the first few entries (passengers) for examination using the `.head()` function.<br>","27980e0b":"* To know how accurate would a prediction be that all female passengers survived and the remaining passengers did not survive?\n  The code below shows the accuracy of this prediction.","8aa6bd04":"Examining the survival statistics, a large majority of males did not survive the ship sinking. However, a majority of females *did* survive the ship sinking. Let's build on our previous prediction: If a passenger was female, then we will predict that they survived. Otherwise, we will predict the passenger did not survive. The code below shows the function will make this prediction.","49d760b1":"* To Know how accurate would a prediction be that none of the passengers survived? The code below shows the accuracy of this\n  prediction.","4a94271f":"***\nAdding the feature **Age** as a condition in conjunction with **Sex** improves the accuracy by a small margin more than with simply using the feature **Sex** alone. <br>\nWe always try to find a series of features and conditions to split the data on to obtain an outcome with higher prediction accuracy, in our case it should be  of at least 80%. This may require multiple features and multiple levels of conditional statements to succeed. we can use the same feature multiple times with different conditions. **Pclass**, **Sex**, **Age**, **SibSp**, and **Parch** are some suggested features to try.\n\nI will use the `survival_stats` function below to to examine various survival statistics and to use mulitple filter conditions, I put each condition in the list passed as the last argument. e.g  `[\"Sex == 'male'\", \"Age < 18\"]`.","68a17bff":"Examining the survival statistics, the majority of males younger than 10 survived the ship sinking, whereas most males age 10 or older *did not survive* the ship sinking. Let's continue to build on our previous prediction: If a passenger was female, then we will predict they survive. If a passenger was male and younger than 10, then we will also predict they survive. Otherwise, we will predict they do not survive. Therefore, we will modify our code so that the function will make this prediction.  ","ee04c5bb":"***\nTo build our model we will firstly design a function that filter the input data by Removing elements that do not match the condition provided. Takes a data list as input and returns a filtered list as shown in the code below.","7c0c377c":"def predictions_3(data):\n    \"\"\" Model with multiple features. Makes a prediction with an accuracy of at least 80%. \"\"\"\n    \n    predictions = []\n    for _, passenger in data.iterrows():\n        \n        # Remove the 'pass' statement below \n        # and write your prediction conditions here\n        #pass\n        if passenger['Sex'] == 'female':\n            if passenger['SibSp'] > 2:\n                predictions.append(0)\n            else:\n                predictions.append(1)\n        else:\n            if passenger['Age'] < 10:\n                predictions.append(1)\n            else:\n                predictions.append(0)\n    \n    # Return our predictions\n    return pd.Series(predictions)\n\n# Make the predictions\npredictions = predictions_3(data)","7c023425":"The **Survived** feature removed from the DataFrame. Note that `data` (the passenger data) and `outcomes` (the outcomes of survival) are now *paired*. That means for any passenger `data.loc[i]`, they have the survival outcome `outcomes[i]`.\n\nTo measure the performance of our predictions, we need a metric to score our predictions against the true outcomes of survival. Since we are interested in how *accurate* our predictions are, we will calculate the proportion of passengers where our prediction of their survival is correct. The code below will create our `accuracy_score` function and test a prediction on the first five passengers.","39c4e185":"***\nUsing just the **Sex** feature for each passenger, we are able to increase the accuracy of our predictions by a significant margin. Now, let's consider using an additional feature to see if we can further improve our predictions. For example, consider all of the male passengers aboard the RMS Titanic: Can we find a subset of those passengers that had a higher rate of survival? Let's start by looking at the **Age** of each male, by again using the `survival_stats` function. This time, we'll use a fourth parameter to filter out the data so that only passengers with the **Sex** 'male' will be included.  \nThe code below plots the survival outcomes of male passengers based on their age.","84d4046a":"***\nLet's take a look at whether the feature **Sex** has any indication of survival rates among passengers.  \nThe code below plots the survival outcomes of passengers based on their sex.","f8a81173":"From a sample of the RMS Titanic data, we can see the various features present for each passenger on the ship:\n- **Survived**: Outcome of survival (0 = No; 1 = Yes)\n- **Pclass**: Socio-economic class (1 = Upper class; 2 = Middle class; 3 = Lower class)\n- **Name**: Name of passenger\n- **Sex**: Sex of the passenger\n- **Age**: Age of the passenger (Some entries contain `NaN`)\n- **SibSp**: Number of siblings and spouses of the passenger aboard\n- **Parch**: Number of parents and children of the passenger aboard\n- **Ticket**: Ticket number of the passenger\n- **Fare**: Fare paid by the passenger\n- **Cabin** Cabin number of the passenger (Some entries contain `NaN`)\n- **Embarked**: Port of embarkation of the passenger (C = Cherbourg; Q = Queenstown; S = Southampton)\n\nSince we're interested in the outcome of survival for each passenger or crew member, we can remove the **Survived** feature from this dataset and store it as its own separate variable `outcomes`. We will use these outcomes as our prediction targets.  \nthe code below will remove **Survived** as a feature of the dataset and store it in `outcomes`.","9fb37bc2":"* To know how accurate would a prediction be that all female passengers and all male passengers younger than 10 survived? <br>\n  The code below shows the accuracy of this prediction.","c284c394":"# Conclusion\n\nAfter several iterations of exploring and conditioning on the data, we have built a useful algorithm for predicting the survival of each passenger aboard the RMS Titanic. The technique applied in this project is a manual implementation of a simple machine learning model, the *decision tree*. A decision tree splits a set of data into smaller and smaller groups (called *nodes*), by one feature at a time. Each time a subset of the data is split, our predictions become more accurate if each of the resulting subgroups are more homogeneous (contain similar labels) than before. The advantage of having a computer do things for us is that it will be more exhaustive and more precise than our manual exploration above. A decision tree is just one of many models that come from *supervised learning*. In supervised learning, we attempt to use features of the data to predict or model things with objective outcome labels. That is to say, each of our data points has a known outcome value, such as a categorical, discrete label like `'Survived'`, or a numerical, continuous value like predicting the price of a house.\n"}}