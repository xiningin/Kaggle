{"cell_type":{"5dcb47f7":"code","6a568ed9":"code","c1ad4996":"code","9eda9500":"code","557f259a":"code","4665520a":"code","be616180":"markdown","31b93bdf":"markdown","5d100dfa":"markdown"},"source":{"5dcb47f7":"import numpy as np \nimport pandas as pd \n\nfrom sklearn import ensemble\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_validate,GridSearchCV\nimport lightgbm as lgbm\nfrom collections import Counter","6a568ed9":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","c1ad4996":"# Outlier detection \n\ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n\n# 1.detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\ntrain = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)\n\n# 2.get train row size \ntrain_len = len(train)\n\n# 3.concat train + test \ndataset =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)\n\n# 4. empty data => np.nan\ndataset = dataset.fillna(np.nan)\n\n# 5.column encoding and create new features\n# 5.1.Fare\ndataset[\"Fare\"] = dataset[\"Fare\"].fillna(dataset[\"Fare\"].mean())\n# dataset[\"Fare\"] = dataset[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n\n# 5.2.Embarked\ndataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(dataset[\"Fare\"].mode()[0])\ndataset = pd.get_dummies(dataset, columns = [\"Embarked\"], prefix=\"Em\")\n\n# 5.3.Sex\ndataset[\"Sex\"] = dataset[\"Sex\"].map({\"male\": 0, \"female\":1})\n# dataset = pd.get_dummies(dataset, columns = [\"Sex\"])\n\n# 5.4.Age\nindex_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\nfor i in index_NaN_age :\n    age_med = dataset[\"Age\"].mean()\n    dataset['Age'].iloc[i] = age_med\n#     age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n#     if not np.isnan(age_pred) :\n#         dataset['Age'].iloc[i] = age_pred\n#     else :\n#         dataset['Age'].iloc[i] = age_med\n        \n        \n# 5.5.Name         \n# dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\n# dataset[\"Title\"] = pd.Series(dataset_title)\n# dataset[\"Title\"] = dataset[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n# dataset[\"Title\"] = dataset[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\n# dataset[\"Title\"] = dataset[\"Title\"].astype(int)\n# dataset.drop(labels = [\"Name\"], axis = 1, inplace = True)\n# dataset = pd.get_dummies(dataset, columns = [\"Title\"])\n\n# 5.6.SibSp(Number of Siblings\/Spouses Aboard) + Parch(Number of Parents\/Children Aboard) =>Fsize\ndataset[\"FamilySize\"] = dataset[\"SibSp\"] + dataset[\"Parch\"] + 1\ndataset['IsSingle'] = dataset['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n# dataset['IsSmallFamily'] = dataset['FamilySize'].map(lambda s: 1 if  s == 2  else 0)\n# dataset['IsMedFamily'] = dataset['FamilySize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n# dataset['IsLargeFamily'] = dataset['FamilySize'].map(lambda s: 1 if s >= 5 else 0)\n\n# #5.7.Cabin\ndataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin'] ])\ndataset = pd.get_dummies(dataset, columns = [\"Cabin\"],prefix=\"Cabin\")\n\n# #5.8.Ticket \nTicket = []\nfor i in list(dataset.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket.append(\"X\")\n        \ndataset[\"Ticket\"] = Ticket\ndataset = pd.get_dummies(dataset, columns = [\"Ticket\"], prefix=\"T\")\n\n# 5.9.Pclass\ndataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\ndataset = pd.get_dummies(dataset, columns = [\"Pclass\"],prefix=\"Pc\")\n\n# 5.10.PassengerId\ndataset.drop(labels = [\"Name\",\"PassengerId\"], axis = 1, inplace = True)\n\n# 6.devide train test \ntrain = dataset[:train_len]\ntest = dataset[train_len:]\ntest.drop(labels=[\"Survived\"],axis = 1,inplace=True)","9eda9500":"train.info()","557f259a":"import matplotlib.pyplot as plt \n\ndef plot_hist(variable,df):\n    print(\"min {} : {} \".format(variable, min(df[variable])))\n    print(\"max {} : {}\".format(variable, max(df[variable])))\n    \n    plt.figure(figsize=(9,5))\n    plt.hist(df[variable], color=\"darkred\")\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist \".format(variable))\n    plt.show()\n\ndef plot_all_hist(df):\n    \n    contiuousColumns = []\n    for colname, colvalue in df.iteritems():\n        if type(colvalue[1]) != str and colvalue.nunique() >= 5:\n            contiuousColumns.append(colname)\n            \n    for col in contiuousColumns:\n        plot_hist(col,df)","4665520a":"plot_all_hist(train)","be616180":"# load data ","31b93bdf":"# preprocessing","5d100dfa":"# import libraries"}}