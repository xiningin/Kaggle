{"cell_type":{"035f8f9b":"code","828f6347":"code","dded7c8e":"code","bdc4e109":"code","221f544c":"code","61c76143":"code","806cf532":"code","46b7b041":"code","f174e277":"code","f2006113":"code","db0fd3eb":"code","489193bd":"code","5be668ee":"code","9ce3f2f6":"code","22d0f46f":"code","c7d935d7":"code","6c15a304":"code","b12a5dda":"code","e953c401":"code","99259084":"code","7ada754d":"code","fa9399f0":"code","3cb99f30":"code","81869149":"code","fd8a0858":"code","9da04848":"code","07451481":"code","8fa0df8b":"code","0ca4c220":"code","4d3857e8":"code","db1796cf":"code","dd2c296f":"code","ae06911c":"code","9721c4e2":"code","25a8f92b":"markdown","db4b2d1f":"markdown","87561148":"markdown","164e898e":"markdown","ae9f2620":"markdown","a2f053e7":"markdown","e98494d6":"markdown","f35a61ad":"markdown","a44a73e7":"markdown","3e28fdc0":"markdown","f50fffe8":"markdown","5f4979a7":"markdown","48a837e9":"markdown","9cd84e50":"markdown","1d671182":"markdown"},"source":{"035f8f9b":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom random import randint\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\nimport keras\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout, BatchNormalization,UpSampling2D,Concatenate, SpatialDropout2D\n\nfrom tqdm import tqdm_notebook\nimport datetime","828f6347":"img_size_ori = 101\nimg_size_target = 128\n\ndef upsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n    #res[:img_size_ori, :img_size_ori] = img\n    #return res\n    \ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n    #return img[:img_size_ori, :img_size_ori]","dded7c8e":"from keras.callbacks import *\n\nclass CyclicLR(Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency, as detailed in this paper (https:\/\/arxiv.org\/abs\/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or \n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w\/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n    For more detail, please see paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi\/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1\/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary\/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations\/(2*self.step_size))\n        x = np.abs(self.clr_iterations\/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())\n","bdc4e109":"train_df = pd.read_csv(\"..\/input\/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"..\/input\/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]","221f544c":"train_df[\"images\"] = [np.array(load_img(\"..\/input\/train\/images\/{}.png\".format(idx), grayscale=True)) \/ 255 for idx in tqdm_notebook(train_df.index)]","61c76143":"train_df[\"masks\"] = [np.array(load_img(\"..\/input\/train\/masks\/{}.png\".format(idx), grayscale=True)) \/ 255 for idx in tqdm_notebook(train_df.index)]","806cf532":"train_df[\"coverage\"] = train_df.masks.map(np.sum) \/ pow(img_size_ori, 2)","46b7b041":"def cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)","f174e277":"fig, axs = plt.subplots(1, 2, figsize=(15,5))\nsns.distplot(train_df.coverage, kde=False, ax=axs[0])\nsns.distplot(train_df.coverage_class, bins=10, kde=False, ax=axs[1])\nplt.suptitle(\"Salt coverage\")\naxs[0].set_xlabel(\"Coverage\")\naxs[1].set_xlabel(\"Coverage class\")","f2006113":"sns.distplot(train_df.z, label=\"Train\")\nsns.distplot(test_df.z, label=\"Test\")\nplt.legend()\nplt.title(\"Depth distribution\")","db0fd3eb":"max_images = 25\ngrid_width = 5\ngrid_height = int(max_images \/ grid_width)*2\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*4, grid_height*4))\nfor i, idx in enumerate(train_df.index[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    ax_image = axs[int(i \/ grid_width)*2, i % grid_width]\n    ax_image.imshow(img, cmap=\"Greys\")\n    ax_image.set_title(\"Image {0}\\nDepth: {1}\".format(i, train_df.loc[idx].z))\n    ax_image.set_yticklabels([])\n    ax_image.set_xticklabels([])\n    ax_mask = axs[int(i \/ grid_width)*2+1, i % grid_width]\n    ax_mask.imshow(img, cmap=\"Greys\")\n    ax_mask.imshow(mask, alpha=0.2, cmap=\"Greens\")\n    ax_mask.set_title(\"Mask {0}\\nCoverage: {1}\".format(i,  round(train_df.loc[idx].coverage, 2)))\n    ax_mask.set_yticklabels([])\n    ax_mask.set_xticklabels([])","489193bd":"ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)","5be668ee":"'''\ndef build_model(input_layer, start_neurons):\n    # 128 -> 64\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n    pool1 = Dropout(0.25)(pool1)\n\n    # 64 -> 32\n    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n    pool2 = MaxPooling2D((2, 2))(conv2)\n    pool2 = Dropout(0.5)(pool2)\n\n    # 32 -> 16\n    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(conv3)\n    pool3 = MaxPooling2D((2, 2))(conv3)\n    pool3 = Dropout(0.5)(pool3)\n\n    # 16 -> 8\n    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(pool3)\n    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(conv4)\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    pool4 = Dropout(0.5)(pool4)\n\n    # Middle\n    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(convm)\n\n    # 8 -> 16\n    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n    uconv4 = concatenate([deconv4, conv4])\n    uconv4 = Dropout(0.5)(uconv4)\n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n\n    # 16 -> 32\n    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n    uconv3 = concatenate([deconv3, conv3])\n    uconv3 = Dropout(0.5)(uconv3)\n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n\n    # 32 -> 64\n    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n    uconv2 = concatenate([deconv2, conv2])\n    uconv2 = Dropout(0.5)(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n\n    # 64 -> 128\n    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n    uconv1 = concatenate([deconv1, conv1])\n    uconv1 = Dropout(0.5)(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n\n    ucov1 = Dropout(0.5)(uconv1)\n    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n    \n    return output_layer\n\ninput_layer = Input((img_size_target, img_size_target, 1))\noutput_layer = build_model(input_layer, 16)\n'''","9ce3f2f6":"def conv_block(m, dim, acti, bn, res, do=0.5):\n    n = Conv2D(dim, 3, activation=acti, padding='same')(m)\n    n = BatchNormalization()(n) if bn else n\n    n = SpatialDropout2D(do)(n) if do else n\n    n = Conv2D(dim, 3, activation=acti, padding='same')(n)\n    n = BatchNormalization()(n) if bn else n\n    return Concatenate()([m, n]) if res else n\n\ndef level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n    if depth > 0:\n        n = conv_block(m, dim, acti, bn, res)\n        m = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n        m = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n        if up:\n            m = UpSampling2D()(m)\n            m = Conv2D(dim, 2, activation=acti, padding='same')(m)\n        else:\n            m = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding='same')(m)\n        n = Concatenate()([n, m])\n        m = conv_block(n, dim, acti, bn, res)\n    else:\n        m = conv_block(m, dim, acti, bn, res, do=do)\n    return m\n\ndef UNet(img_shape, out_ch=1, start_ch=64, depth=5, inc_rate=2., activation='relu', \n         dropout=0.5, batchnorm=False, maxpool=True, upconv=True, residual=True):\n    i = Input(shape=img_shape)\n    o = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n    o = Conv2D(out_ch, 1, activation='sigmoid')(o)\n    return Model(inputs=i, outputs=o)","22d0f46f":"#model = Model(input_layer, output_layer)\nmodel = UNet((img_size_target,img_size_target,1),start_ch=16,depth=5,batchnorm=True)","c7d935d7":"from keras import backend as K\n\n# define iou or jaccard loss function\ndef iou_loss(y_true, y_pred):\n    y_true = tf.reshape(y_true, [-1])\n    y_pred = tf.reshape(y_pred, [-1])\n    intersection = tf.reduce_sum(y_true * y_pred)\n    score = (intersection + 1.) \/ (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + 1.)\n    return 1 - score\n\n# combine bce loss and iou loss\ndef iou_bce_loss(y_true, y_pred):\n    return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * iou_loss(y_true, y_pred)\n\n# mean iou as a metric\ndef mean_iou(y_true, y_pred):\n    y_pred = tf.round(y_pred)\n    intersect = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])\n    smooth = tf.ones(tf.shape(intersect))\n    return tf.reduce_mean((intersect + smooth) \/ (union - intersect + smooth))\n\nmodel.compile(loss=iou_bce_loss, optimizer=\"adam\", metrics=['accuracy', mean_iou])","6c15a304":"model.summary()","b12a5dda":"#x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n#y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)\n\n## Image Augmentation\n# Vertical Image\nVx = [np.flip(x, axis=0) for x in x_train]\nVy = [np.flip(x, axis=0) for x in y_train]\n\n# Horizontal Image\nHx = [np.flip(x, axis=1) for x in x_train]\nHy = [np.flip(x, axis=1) for x in y_train]\n\n# Horizontal Vertical Image\nHVx = [np.flip(x, axis=1) for x in Vx]\nHVy = [np.flip(x, axis=1) for x in Vy]\n\n# Appending the augmented image and mask to the main dataset.\nx_train = np.append(x_train, Vx, axis=0)\ny_train = np.append(y_train, Vy, axis=0)\n\nx_train = np.append(x_train, Hx, axis=0)\ny_train = np.append(y_train, Hy, axis=0)\n\nx_train = np.append(x_train, HVx, axis=0)\ny_train = np.append(y_train, HVy, axis=0)","e953c401":"fig, axs = plt.subplots(2, 10, figsize=(15,3))\nfor i in range(10):\n    axs[0][i].imshow(x_train[i].squeeze(), cmap=\"Greys\")\n    axs[0][i].imshow(y_train[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n    axs[1][i].imshow(x_train[int(len(x_train)\/2 + i)].squeeze(), cmap=\"Greys\")\n    axs[1][i].imshow(y_train[int(len(y_train)\/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\nfig.suptitle(\"Top row: original images, bottom row: augmented images\")","99259084":"early_stopping = EarlyStopping(patience=10, verbose=1)\nmodel_checkpoint = ModelCheckpoint(\".\/keras.model\", save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.0000005, verbose=1)\nclr = CyclicLR(base_lr=0.001, max_lr=0.0001, step_size=200, mode='triangular')\n\n# cosine learning rate annealing\ndef cosine_annealing(x):\n    lr = 0.001\n    epochs = 200\n    return lr*(np.cos(np.pi*x\/epochs)+1.)\/2\nlearning_rate = tf.keras.callbacks.LearningRateScheduler(cosine_annealing)\n\nepochs = 200\nbatch_size = 32\n\nhistory = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[early_stopping, model_checkpoint, learning_rate])","7ada754d":"fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\nax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\nax_loss.legend()\nax_acc.plot(history.epoch, history.history[\"mean_iou\"], label=\"Train IOU\")\nax_acc.plot(history.epoch, history.history[\"val_mean_iou\"], label=\"Validation IOU\")\nax_acc.legend()","fa9399f0":"#model = load_model(\".\/keras.model\")","3cb99f30":"preds_valid = model.predict(x_valid).reshape(-1, img_size_target, img_size_target)\npreds_valid = np.array([downsample(x) for x in preds_valid])\nmask_valid = np.array([downsample(x) for x in y_valid])","81869149":"# src: https:\/\/www.kaggle.com\/aglotero\/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection \/ union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp \/ (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)","fd8a0858":"thresholds = np.linspace(0, 1, 50)\nious = np.array([iou_metric_batch(mask_valid, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])","9da04848":"threshold_best_index = np.argmax(ious[9:-10]) + 9\niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]","07451481":"plt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","8fa0df8b":"max_images = 30\ngrid_width = 5\ngrid_height = int(max_images \/ grid_width)*3\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*4, grid_height*4))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    #print(idx)\n    img = downsample(np.squeeze(x_valid[i]))\n    mask = np.squeeze(mask_valid[i])\n    pred = np.squeeze(preds_valid[i]>threshold_best)\n    ax_image = axs[int(i \/ grid_width)*3, i % grid_width]\n    ax_image.imshow(img, cmap=\"Greys\")\n    ax_image.set_title(\"Image {0}\\nDepth: {1}\".format(idx, train_df.loc[idx].z))\n    ax_image.set_yticklabels([])\n    ax_image.set_xticklabels([])\n    ax_mask = axs[int(i \/ grid_width)*3+1, i % grid_width]\n    ax_mask.imshow(img, cmap=\"Greys\")\n    ax_mask.imshow(mask, alpha=0.9, cmap=\"Greens\")\n    ax_mask.set_title(\"Mask {0}\\nCoverage: {1}\".format(idx,  round(train_df.loc[idx].coverage, 2)))\n    ax_mask.set_yticklabels([])\n    ax_mask.set_xticklabels([])\n    ax_pred = axs[int(i \/ grid_width)*3+2, i % grid_width]\n    ax_pred.imshow(img, cmap=\"Greys\")\n    ax_pred.imshow(pred, alpha=0.9, cmap=\"Blues\")\n    coverage_pred = np.sum(pred) \/ pow(img_size_ori, 2)\n    ax_pred.set_title(\"Predict {0}\\nCoverage: {1}\".format(idx,  round(coverage_pred, 2)))\n    ax_pred.set_yticklabels([])\n    ax_pred.set_xticklabels([])","0ca4c220":"# plot small charts\nmax_images = 24\ngrid_width = 12\ngrid_height = int(max_images \/ grid_width)*3\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width*2, grid_height*2))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    #print(idx)\n    img = downsample(np.squeeze(x_valid[i]))\n    mask = np.squeeze(mask_valid[i])\n    pred = np.squeeze(preds_valid[i]>threshold_best)\n    ax_image = axs[int(i \/ grid_width)*3, i % grid_width]\n    ax_image.imshow(img, cmap=\"Greys\")\n    ax_image.set_title(\"Image\")\n    ax_image.set_yticklabels([])\n    ax_image.set_xticklabels([])\n    ax_mask = axs[int(i \/ grid_width)*3+1, i % grid_width]\n    ax_mask.imshow(img, cmap=\"Greys\")\n    ax_mask.imshow(mask, alpha=0.9, cmap=\"Greens\")\n    ax_mask.set_title(\"Mask\")\n    ax_mask.set_yticklabels([])\n    ax_mask.set_xticklabels([])\n    ax_pred = axs[int(i \/ grid_width)*3+2, i % grid_width]\n    ax_pred.imshow(img, cmap=\"Greys\")\n    ax_pred.imshow(pred, alpha=0.9, cmap=\"Blues\")\n    coverage_pred = np.sum(pred) \/ pow(img_size_ori, 2)\n    ax_pred.set_title(\"Predict\")\n    ax_pred.set_yticklabels([])\n    ax_pred.set_xticklabels([])","4d3857e8":"# Source https:\/\/www.kaggle.com\/bguberfain\/unet-with-depth\ndef RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs","db1796cf":"#del x_train, y_train\nx_test = np.array([upsample(np.array(load_img(\"..\/input\/test\/images\/{}.png\".format(idx), grayscale=True))) \/ 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)","dd2c296f":"preds_test = model.predict(x_test, verbose=True)","ae06911c":"pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}","9721c4e2":"import base64\nimport pandas as pd\nfrom IPython.display import HTML\n\ndef create_download_link( df, title = \"Download CSV file\", filename = \"sub.csv\"):\n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\nsub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('submission.csv')\ntimestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\nprint('Submission output to: sub-{}.csv'.format(timestamp))\nsub.to_csv(\"sub-{}.csv\".format(timestamp))\n\ncreate_download_link(sub)\n","25a8f92b":"# Params and helpers","db4b2d1f":"# Predict the validation set to do a sanity check\nAgain plot some sample images including the predictions.","87561148":"# Show some example images","164e898e":"# Loading of training\/testing ids and depths\nReading the training data and the depths, store them in a DataFrame. Also create a test DataFrame with entries from depth not in train.","ae9f2620":"# Create train\/validation split stratified by salt coverage\nUsing the salt coverage as a stratification criterion. Also show an image to check for correct upsampling.","a2f053e7":"# Plotting the depth distributions\nSeparatelty plotting the depth distributions for the training and the testing data.","e98494d6":"# About\nI learned a lot from Peter's kernal: [U-net, dropout, augmentation, stratification](https:\/\/www.kaggle.com\/phoenigs\/u-net-dropout-augmentation-stratification). \n\nI added more visualization to understand the analysis process.","f35a61ad":"# Build model","a44a73e7":"# Sanity check with adjusted threshold\nAgain some sample images with the adjusted threshold.","3e28fdc0":"# Read images and masks\nLoad the images and masks into the DataFrame and divide the pixel values by 255.","f50fffe8":"# Data augmentation","5f4979a7":"# Calculating the salt coverage and salt coverage classes\nCounting the number of salt pixels in the masks and dividing them by the image size. Also create 11 coverage classes, -0.1 having no salt at all to 1.0 being salt only.\nPlotting the distribution of coverages and coverage classes, and the class against the raw coverage.","48a837e9":"# Scoring\nScore the model and do a threshold optimization by the best IoU.","9cd84e50":"# Submission\nLoad, predict and submit the test image predictions.","1d671182":"# Training"}}