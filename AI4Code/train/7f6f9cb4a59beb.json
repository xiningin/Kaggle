{"cell_type":{"b39716c3":"code","8b176aac":"code","cb6b4cb0":"code","c3550f1e":"code","18ab3076":"code","1ce36ad0":"code","5689f36a":"code","58aa688b":"code","f28725ca":"code","1d3623d3":"code","da27622b":"code","82e28843":"code","68b3fbb1":"code","58a47aa6":"code","ca01b22b":"code","fd18d086":"code","c2850c90":"code","81de2801":"code","32f8f845":"code","b8680df5":"code","53a0d873":"code","977cb3c4":"code","ffdfc691":"code","8be27398":"code","876ace68":"code","6b13ac56":"code","2d864e0c":"code","4ebbbb8a":"code","1d725cf3":"code","9305e2d4":"code","dc662805":"code","ead39672":"code","8206e18b":"code","7e1fec83":"code","61274df0":"code","d00cc111":"code","76b9b193":"code","dda2858a":"code","9b982072":"code","699076b7":"code","ddb796b5":"code","5feb08dc":"code","6ac24fb2":"code","e3b7b0ef":"code","5492eece":"code","f3e75163":"code","031dd001":"code","22c9d187":"code","84385c18":"code","d6bb0dc5":"code","243484cf":"code","bff7cda8":"code","e62639ac":"code","d43c559c":"markdown","25a6f3f1":"markdown","0bc84449":"markdown","7ec0642a":"markdown","88b995e0":"markdown","4c853589":"markdown","853f53b0":"markdown","7c2d300a":"markdown","c306e964":"markdown","a4ccbce6":"markdown","d488f5f2":"markdown","54588658":"markdown","8813fde3":"markdown","d771eed9":"markdown","917ad2cc":"markdown","4639f346":"markdown","055f7ca5":"markdown","c38c0af4":"markdown","e9a12af9":"markdown","24adaf4d":"markdown","155b7a39":"markdown","2fbc6aea":"markdown","c4654445":"markdown","626dc5f9":"markdown","76416472":"markdown","699a8c93":"markdown","c5dd5fe8":"markdown","d8235a3a":"markdown","2a644d60":"markdown","9d7ea5eb":"markdown","8b86a345":"markdown","69d84fe8":"markdown","e1ea71cd":"markdown","f7077264":"markdown","e67855a7":"markdown","12389e9e":"markdown","724f2d02":"markdown","850fe960":"markdown","10efd946":"markdown","2d309a1c":"markdown","21bc6a80":"markdown","3b0790e9":"markdown","05c48b00":"markdown","bec8c893":"markdown","8e2712cb":"markdown","e394b16c":"markdown","23717146":"markdown","f882d1c2":"markdown","298f781d":"markdown","de6ebe00":"markdown","3fcdc2cf":"markdown","e120802e":"markdown","39c3931f":"markdown","0ceae0a9":"markdown","7f29c0f7":"markdown","56df5ad4":"markdown"},"source":{"b39716c3":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8b176aac":"plt.style.use('ggplot')","cb6b4cb0":"path = '\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Training Data.csv'\npd.set_option('display.max_columns', None)\ndata = pd.read_csv(path)\ndata.head()","c3550f1e":"data.describe(include='all')","18ab3076":"data.info()","1ce36ad0":"data.groupby('Risk_Flag').describe()['Age']","5689f36a":"data[data['Risk_Flag'] == 1]['Age'].value_counts(sort=True)[:20]","58aa688b":"data.groupby('Risk_Flag')['Married\/Single'].value_counts()","f28725ca":"data['Married\/Single'].hist()","1d3623d3":"sns.distplot(data['Income'], bins=20)\nplt.show()","da27622b":"data['Status'] = np.where(data['Income']>=data['Income'].mean(), 'Above Average', 'Under Average')\ndata['Status'].hist()","82e28843":"data.groupby('Risk_Flag')['Status'].hist()","68b3fbb1":"data.groupby(\"Risk_Flag\")['Income'].describe()","58a47aa6":"data.groupby(\"Risk_Flag\")['STATE'].value_counts(sort=True)","ca01b22b":"sns.countplot(y='STATE', data=data)\nplt.show()","fd18d086":"data[data['STATE'] == 'West_Bengal'].describe()","c2850c90":"data.describe()","81de2801":"data.groupby(\"Risk_Flag\")['CURRENT_JOB_YRS'].value_counts(sort=True)","32f8f845":"sns.countplot(x='CURRENT_JOB_YRS', hue='Risk_Flag', data=data)\nplt.show()","b8680df5":"data['Profession'].value_counts()","53a0d873":"data.groupby('Profession')['Income'].mean().sort_values(ascending=False)","977cb3c4":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ndata[\"Married\/Single\"] = label_encoder.fit_transform(data[\"Married\/Single\"])\ndata[\"House_Ownership\"] = label_encoder.fit_transform(data[\"House_Ownership\"])\ndata[\"Car_Ownership\"] = label_encoder.fit_transform(data[\"Car_Ownership\"])\ndata[\"Profession\"] = label_encoder.fit_transform(data[\"Profession\"])\ndata[\"CITY\"] = label_encoder.fit_transform(data[\"CITY\"])\ndata[\"STATE\"] = label_encoder.fit_transform(data[\"STATE\"])\ndata[\"Status\"] = label_encoder.fit_transform(data[\"Status\"])","ffdfc691":"data['STATE'].value_counts()","8be27398":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.ensemble import GradientBoostingClassifier","876ace68":"from sklearn import metrics\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split","6b13ac56":"pred = data['Risk_Flag']\ndata.drop(columns=['Risk_Flag'], inplace=True)","2d864e0c":"'''\nKFold_Score = pd.DataFrame()\nclassifiers = ['LogisticRegression', 'GradientBoostingClassifier']\nmodels = [\n          LogisticRegression(max_iter = 1000),\n          GradientBoostingClassifier(random_state=0)\n         ]\n'''","4ebbbb8a":"'''\nj = 0\nfor i in models:\n    model = i\n    cv = KFold(n_splits=5, random_state=0, shuffle=True)\n    KFold_Score[classifiers[j]] = (cross_val_score(model, data, np.ravel(pred), scoring = 'accuracy', cv=cv))\n    j = j+1\n'''","1d725cf3":"'''\nmean = pd.DataFrame(KFold_Score.mean(), index= classifiers)\nKFold_Score = pd.concat([KFold_Score,mean.T])\nKFold_Score.index=['Fold 1','Fold 2','Fold 3','Fold 4','Fold 5','Mean']\nKFold_Score.T.sort_values(by=['Mean'], ascending = False)\n'''","9305e2d4":"'''\nKFold_Score2 = pd.DataFrame()\nclassifiers = ['RandomForestClassifier', 'XGBoostClassifier']\nmodels = [\n          RandomForestClassifier(n_estimators=200, random_state=0),\n          xgb.XGBClassifier(n_estimators=100),\n         ]\nj = 0\nfor i in models:\n    model = i\n    cv = KFold(n_splits=5, random_state=0, shuffle=True)\n    KFold_Score2[classifiers[j]] = (cross_val_score(model, data, np.ravel(pred), scoring = 'accuracy', cv=cv))\n    j = j+1\n'''","dc662805":"'''\nmean = pd.DataFrame(KFold_Score2.mean(), index= classifiers)\nKFold_Score2 = pd.concat([KFold_Score2,mean.T])\nKFold_Score2.index=['Fold 1','Fold 2','Fold 3','Fold 4','Fold 5','Mean']\nKFold_Score2.T.sort_values(by=['Mean'], ascending = False)\n'''","ead39672":"X_train, X_test, y_train, y_test = train_test_split(data, pred, test_size=0.2, random_state=42)","8206e18b":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)","7e1fec83":"params = {\n    'max_depth':6,\n    'min_child_weight': 1,\n    'eta':.3,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    'objective':'reg:linear',\n}","61274df0":"params['eval_metric'] = 'logloss'","d00cc111":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=999,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=10\n)","76b9b193":"print(\"Best Log Loss: {:.2f} with {} rounds\".format(\n                 model.best_score,\n                 model.best_iteration+1))","dda2858a":"cv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=999,\n    seed=42,\n    nfold=5,\n    metrics={'logloss'},\n    early_stopping_rounds=10\n)\ncv_results","9b982072":"cv_results['test-logloss-mean'].min()","699076b7":"gridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(9,12)\n    for min_child_weight in range(5,8)\n]","ddb796b5":"min_log = float(\"Inf\")\nbest_params = None\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(\n                             max_depth,\n                             min_child_weight))\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=999,\n        seed=42,\n        nfold=5,\n        metrics={'logloss'},\n        early_stopping_rounds=10)\n    \n    mean_log = cv_results['test-logloss-mean'].min()\n    boost_rounds = cv_results['test-logloss-mean'].argmin()\n    print(\"\\logloss {} for {} rounds\".format(mean_log, boost_rounds))\n    if mean_log < min_log:\n        min_log = mean_log\n        best_params = (max_depth,min_child_weight)\nprint(\"Best params: {}, {}, logloss: {}\".format(best_params[0], best_params[1], min_log))","5feb08dc":"params['max_depth'] = 9\nparams['min_child_weight'] = 7","6ac24fb2":"gridsearch_params = [\n    (subsample, colsample)\n    for subsample in [i\/10. for i in range(7,11)]\n    for colsample in [i\/10. for i in range(7,11)]\n]","e3b7b0ef":"min_log = float(\"Inf\")\nbest_params = None\nfor subsample, colsample in reversed(gridsearch_params):\n    print(\"CV with subsample={}, colsample={}\".format(\n                             subsample,\n                             colsample))\n    params['subsample'] = subsample\n    params['colsample_bytree'] = colsample\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=500,\n        seed=42,\n        nfold=5,\n        metrics={'logloss'},\n        early_stopping_rounds=10\n    )\n    mean_log = cv_results['test-logloss-mean'].min()\n    boost_rounds = cv_results['test-logloss-mean'].argmin()\n    print(\"\\log {} for {} rounds\".format(mean_log, boost_rounds))\n    if mean_log < min_log:\n        min_log = mean_log\n        best_params = (subsample,colsample)\nprint(\"Best params: {}, {}, log: {}\".format(best_params[0], best_params[1], min_log))","5492eece":"params['subsample'] = 1.0\nparams['colsample_bytree'] = .7","f3e75163":"min_log = float(\"Inf\")\nbest_params = None\nfor eta in [.3, .2, .1, .05, .01, .005]:\n    print(\"CV with eta={}\".format(eta))\n    params['eta'] = eta\n    cv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=500,\n    seed=42,\n    nfold=5,\n    metrics=['logloss'],\n    early_stopping_rounds=10)\n    mean_log = cv_results['test-logloss-mean'].min()\n    boost_rounds = cv_results['test-logloss-mean'].argmin()\n    print(\"\\Log Loss {} for {} rounds\\n\".format(mean_log, boost_rounds))\n    if mean_log < min_log:\n        min_log = mean_log\n        best_params = eta\nprint(\"Best params: {}, Log Loss: {}\".format(best_params, min_log))","031dd001":"params['eta'] = .1","22c9d187":"params","84385c18":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=900,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=10\n)\n\nprint(\"Best Log: {:.2f} in {} rounds\".format(model.best_score, model.best_iteration+1))","d6bb0dc5":"num_boost_round = model.best_iteration + 1\nbest_model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")]\n)","243484cf":"predictions = best_model.predict(dtest)","bff7cda8":"metrics.roc_auc_score(y_test, predictions)","e62639ac":"best_model.save_model(\"my_model.model\")","d43c559c":"# REFERENCES","25a6f3f1":"Before diving into model testing, we have to get familiar with the data and understand what it represents.","0bc84449":"The average salary of Risk Flag group 1 is 0.3 e+06 smaller than the non risky group. Meanwhile other metrics for both groups are similar.","7ec0642a":"I would be using ggplot theme for my plots","88b995e0":"Import the data","4c853589":"The best learning rate is 0.1","853f53b0":"# Loan Prediction\n# XGBoost \/ Hyperparameter tunning","7c2d300a":"## Marital status","c306e964":"We can see that the minimum age, maximum age, and even average age does not affect the risk flag variable. <br>\nAlso we can see that both (0 and 1) have the same difference between the age groups and the mean: standard deviation of 17.","a4ccbce6":"As required, we will calculate the roc auc score of the model on the test set","d488f5f2":"Here we can see that single people are more risky, 91% of risky flag people are single. But actually this can't tell much since most of people that apply are single.","54588658":"Train the model","8813fde3":"The optimal results are 1 and 0.7","d771eed9":"We have got quite interesting result, 0.94. Of course it could be improved but we will it here for the moment.\nSave the model:","917ad2cc":"We will start by defining the initial parameters of the model:","4639f346":"After testig four classifiers, we can see that XGBoostClassifier returns the best mean cross validation score.","055f7ca5":"## State","c38c0af4":"The income does not affect the risk flag variable of a person.","e9a12af9":"We have multiple categorical data, since we will test various models and not only tree based models, we should econde these variables. <br>\nHowever we can see we do not have any missing values in our data.","24adaf4d":"# Data analysis","155b7a39":"## Income","2fbc6aea":"Uttar Pradesh ranks first in both groups which is normal since Uttar has the most number of applicants, ","c4654445":"This plot doesn't tell much.","626dc5f9":"Since this is a classification problem, we will set the evaluation metric to log loss","76416472":"Moving to the next step, we will start by importing the classifiers that we will test:\n* Logistic Regression\n* XGBoost\n* Random Forest \n* Gradient Boosting\n\nWe would test other classifiers but it takes so much time to run a cross validation test on many classifiers.","699a8c93":"Let us check the initial log loss","c5dd5fe8":"## Profession","d8235a3a":"Income and age of applicants from West Bangali does not differ much from other states.","2a644d60":"# Hyperparameter tunning:","9d7ea5eb":"Tada!","8b86a345":"We will start by searching two parameters: max_depth and min_child_weight","69d84fe8":"Train the model with the parameters selected, and specify early stopping at ten rounds","e1ea71cd":"First, we have to drop the target column from the data.","f7077264":"Moving to the subsample and colsample paramters","e67855a7":"## Current Job","12389e9e":"The data does not need much cleaning, we only need to encode the categorical data so it could be used by our algorithms. For this step I will use LabelEncode of the sklearn pre processing library.","724f2d02":"## Plan\n1. Data Analysis \n2. Data Pre processing\n3. Models testing\n4. Hyperparameter tunning","850fe960":"# Data Encoding:","10efd946":"Let us have a look at the parameters we have got so far","2d309a1c":"Instead of numpy arrays or pandas dataFrame, XGBoost uses DMatrices.","21bc6a80":"Tuning the learning rate might take some time, if you clone the code prepare to wait up to 30mins for this cell to be executed","3b0790e9":"Most applicants have well paid jobs.","05c48b00":"And of course evaluation metrics are important, alongside the splitting function.","bec8c893":"Well, we can clearly see we have got nearly the same mean cross validation score. Moving on to the next test. `I have commented the code because of the limited computing power.`","8e2712cb":"West Bengal ranks second with the most risky flags, even though it is the fourth city in the number of applicants.","e394b16c":"Let us first start by importing the necessary packages:","23717146":"# ROC_AUC score on the test set","f882d1c2":"The data is distributed equally between people with under average and above average income.","298f781d":"` This notebook is work in progress, I will add comments later`","de6ebe00":"Save these two parameters to the params dictionary","3fcdc2cf":"For more details of the hyperparameter tuning techniques I have used in this notebook, refer to the following blog, it explains hyperparameter tuning in xgboost in detail:","e120802e":"To avoid wasting too much time running one cell of code, first we will test logistic regression and gradient boosting, and then move to XGBoost and Random Forest.","39c3931f":"There is not a big difference between the top 5 of the Risk Flag list, however starting from the sixth position the difference grow, but the distribution of age groups is normal, since we don't have a certain age group repeating. As we can see the first place is for people aged 22, second place for people aged 66.","0ceae0a9":"XGBoost Hyperparameter tunning: https:\/\/blog.cambridgespark.com\/hyperparameter-tuning-in-xgboost-4ff9100a3b2f","7f29c0f7":"## Age","56df5ad4":"# Testing Classifiers"}}