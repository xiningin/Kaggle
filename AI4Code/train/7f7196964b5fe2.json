{"cell_type":{"0a64385a":"code","9a090d3a":"code","94b35c66":"code","8bf19158":"code","96ad24f6":"code","8a2bf2c3":"code","ccaef29a":"code","bb499701":"code","fa22383d":"code","0d166903":"code","bd34992f":"code","19b8c83b":"code","edde2986":"code","674056a0":"code","bdddef93":"code","24a027c8":"code","c286505c":"code","a077a184":"code","ac6f29d6":"code","eb7f7e59":"code","7fd1a782":"code","acf17d2f":"code","f42adc44":"code","80f23533":"code","e05844db":"code","3b349c7b":"markdown","76a57a12":"markdown","b18ff462":"markdown","a72cc981":"markdown","d61613af":"markdown","52a0650f":"markdown","9af598c7":"markdown","f4373ca4":"markdown","8cf8c06c":"markdown","09338bfa":"markdown","df47d737":"markdown"},"source":{"0a64385a":"import json\nimport tensorflow as tf\nimport csv\nimport random\nimport numpy as np\nimport re\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\nimport tqdm\n\n\nimport nltk \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\nimport pandas as pd\n\nLR = 1e-1\nBATCH_SIZE = 1024\nembedding_dim = 100\nmax_length = 30\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\ntraining_size=1600000\ntest_portion=.2\n\ncorpus = []\n","9a090d3a":"!wget --no-check-certificate \\https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/training_cleaned.csv \\\/input\/training_cleaned.csv\n!wget --no-check-certificate \\https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/glove.6B.100d.txt \\\/input\/100d.txt","94b35c66":"df  = pd.read_csv(\"training_cleaned.csv\", names=['label','id', 'day', 'querytype', 'user', 'comment'])\nprint(len(df))\ndf.tail()","8bf19158":"toxiclen = len([d for d in df['label'] if d])\nnontoxiclen = len(df) - toxiclen\nprint(f\"Number of toxic comments: {toxiclen}\")\nprint(f\"Number of non toxic comments: {nontoxiclen}\")\nprint(\"The dataset is balanced\")","96ad24f6":"df[\"label_corrected\"] = [1 if l else 0 for l in df[\"label\"]]\ncorpus = df[[\"comment\", \"label_corrected\"]]\ncorpus = np.array(corpus)","8a2bf2c3":"\nprint(len(corpus))\nprint(corpus[-1])\n\n# Expected Output:\n# 1600000\n# [\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", 0]","ccaef29a":"stop_words = stopwords.words('english')\nstemmer = SnowballStemmer('english')\n\ndef preprocess(text, stem=False):\n  text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n  tokens = []\n  for token in text.split():\n    if token not in stop_words:\n      if stem:\n        tokens.append(stemmer.stem(token))\n      else:\n        tokens.append(token)\n  return \" \".join(tokens)","bb499701":"def remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)        \n    return input_txt \n\n\ntrain_labels=[]\n\ntrain_sentence=[]\nrandom.shuffle(corpus)\nfor x in tqdm.tqdm(range(training_size), desc='Preprocessing...'):\n    sentence=corpus[x][0]\n    train_labels.append(corpus[x][1])\n    for word in stop_words:\n        token=\" \"+word+\" \"\n        sentence=sentence.replace(token,\" \")\n    train_sentence.append(sentence)\n    \n","fa22383d":"print(len(train_sentence))","0d166903":"sentences1=[]\nfor sentence in tqdm.tqdm(train_sentence, desc=\"Changing...\"):\n    for word in sentence:\n        word1=stemmer.stem(word)\n        token=\" \"+word+\" \"\n        sentence=sentence.replace(token,word1)\n    sentence=sentence.replace(\"[^a-zA-Z#]\", \" \")\n    sentences1.append(sentence)\ntrain_sentence=sentences1","bd34992f":"print(len(train_sentence))\nprint(train_sentence[0])","19b8c83b":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_sentence)\n\nword_index = tokenizer.word_index\nvocab_size = len(word_index)\n\nsequences = tokenizer.texts_to_sequences(train_sentence)\npad = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n","edde2986":"from sklearn.model_selection import train_test_split\nx, testx, y, testy = train_test_split(pad, train_labels, test_size=0.2)","674056a0":"x = np.array(x)\ny = np.array(y)\n\ntestx = np.array(testx)\ntesty = np.array(testy)\nprint(x.shape)\nprint(testx.shape)\nprint(y[2])\nx[2]","bdddef93":"np.save(\"testx.npy\", testx)\nnp.save(\"testy.npy\", testy)","24a027c8":"print(vocab_size)\n#print(word_index['bad'])\n","c286505c":"# Note this is the 100 dimension version of GloVe from Stanford\n# I unzipped and hosted it on my site to make this notebook easier\n\n# Note2: using 50d version of GloVe as well to compare which one does better\n# !wget --no-check-certificate \\ https:\/\/www.kaggle.com\/watts2\/glove6b50dtxt  \\\/content\/glove.6B.50d.txt","a077a184":"embeddings_index = {};\nwith open('..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec') as f:\n    for line in tqdm.tqdm(f, desc='loading embeddings....'):\n        values = line.split()\n        word = values[0];\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\nembedding_dim = 300\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim))\nfor word, i in tqdm.tqdm(word_index.items(), desc='making embedding matrix....'):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector\n\nprint('Found %s word vectors.' %len(embeddings_index))","ac6f29d6":"print(len(embeddings_matrix))\n# Expected Output\n# 138859","eb7f7e59":"def lrs(epoch, lr):\n    if epoch < 4:\n        return lr\n    else:\n        return lr\/(0.5 * epoch)\n\ncb = tf.keras.callbacks.LearningRateScheduler(lrs)","7fd1a782":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.SpatialDropout1D(0.2),\n   \n    #tf.keras.layers.Conv1D(64, 5, activation='relu'),\n    #tf.keras.layers.Conv1D(128, 5, activation='relu'),\n    #tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,dropout=0.2,recurrent_dropout=0.2)),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(learning_rate=0.01),metrics=['accuracy'])\nRlr = ReduceLROnPlateau(             factor=0.1,\n                                     min_lr = 0.001,\n                                     monitor = 'val_loss',\n                                     verbose = 1)\nmodel.summary()\n\nnum_epochs = 30\nhistory = model.fit(x, y, batch_size=int(x.shape[0]\/200), epochs=num_epochs, validation_data=(testx, testy), callbacks=[Rlr])\n\nprint(\"Training Complete\")","acf17d2f":"import matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt\n\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nacc=history.history['accuracy']\nval_acc=history.history['val_accuracy']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs=range(len(acc)) # Get number of epochs\n\n#------------------------------------------------\n# Plot training and validation accuracy per epoch\n#------------------------------------------------\nplt.plot(epochs, acc, 'r')\nplt.plot(epochs, val_acc, 'b')\nplt.title('Training and validation accuracy')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend([\"Accuracy\", \"Validation Accuracy\"])\n\nplt.figure()\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(epochs, loss, 'r')\nplt.plot(epochs, val_loss, 'b')\nplt.title('Training and validation loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Loss\", \"Validation Loss\"])\n\nplt.figure()\n\n\n# Expected Output\n# A chart where the validation loss does not increase sharply!","f42adc44":"model.save(\"100dLSTM-FastText.h5\")","80f23533":"np.save(\"wordidx.npy\", tokenizer.word_index)","e05844db":"\nfile.remove(\".\/100dLSTMCNN-nothing.h5\")","3b349c7b":"## Import libraries and set hyperparameters","76a57a12":"## Read and preprocess the data\n#### Read the csv dataset and add the column names","b18ff462":"#### Check for number of toxic and non toxic comments to look for any dataset imbalance","a72cc981":"# Sentiment Analysis Comparisions","d61613af":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.LSTM(64),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n\nnum_epochs = 50\nhistory = model.fit(trainx, trainy, epochs=num_epochs, validation_data=(testx, testy), verbose=1)\n\nprint(\"Training Complete\")","52a0650f":"## Tokenize the sentences and prepare for training\n#### Use the tokenizer to tokenize the sentences and then apply padding. Also split the data into training and testing sets","9af598c7":"## Load the pretrained word embeddings\n#### Load the embeddings and make a matrix to use as weights for embedding layer","f4373ca4":"* ","8cf8c06c":"## Download the dataset and the embeddings\n#### The dataset is a cleaned version of toxic twitter comments and the embeddings are 100d GloVe embeddings","09338bfa":"#### Preprocess data by removing stop words and replacing other words using stemming","df47d737":"#### Seperate the dataset's comment and label columns as they are the only field which matter in the classification"}}