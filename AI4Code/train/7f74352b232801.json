{"cell_type":{"08be8f4c":"code","82525534":"code","c53ab345":"code","a8463675":"code","66b0c1cd":"code","c5f1d751":"code","a165d9cf":"code","ffc9f56b":"code","838fd223":"code","62522860":"code","78785a1f":"code","eb46ba3c":"code","5395e1a4":"code","b22e8374":"code","a96cb496":"code","dc0342eb":"code","670f8879":"code","201e4597":"code","ae80ab41":"code","53dae3a7":"markdown","48901fde":"markdown","12f0aea2":"markdown","2437931c":"markdown","3489b581":"markdown","508ed215":"markdown"},"source":{"08be8f4c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# hide warnings\nimport warnings\nwarnings.simplefilter('ignore')","82525534":"import pandas as pd\nimport numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point, Polygon","c53ab345":"train = pd.read_csv('\/kaggle\/input\/bigquery-geotab-intersection-congestion\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/bigquery-geotab-intersection-congestion\/test.csv')","a8463675":"train_df = train.copy()\ntest_df = test.copy()","66b0c1cd":"train_df.isnull().sum()","c5f1d751":"train_df = train.drop(['EntryStreetName','ExitStreetName'],axis=1)","a165d9cf":"train_df.isnull().sum()","ffc9f56b":"def QreIndex(dataset,name):\n  \n  index = {'Percentile':[],'Distance':[]}\n  \n  list = [name+'_p{}'.format(s) for s in [20,40,50,60,80]]\n  \n  data_df = pd.DataFrame(data=index)\n  \n  for i in list:\n    \n    data = pd.DataFrame(data=index)\n    data['Distance'] = dataset[i].T\n    data['Percentile'] = i\n    data_df = pd.concat([data_df,data],ignore_index=True)\n    data_df0 = data_df[data_df['Distance']>0]\n    \n  return data_df ,data_df0\n\n\npercentile_df,percentile_df0 = QreIndex(train_df,'DistanceToFirstStop')\n#quantile_df0 = quantile_df[quantile_df['Distance']>0]","838fd223":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nPercentile_list = ['DistanceToFirstStop','TotalTimeStopped','TimeFromFirstStop']\n\ndef Qdistplot(dataset,title):\n  \n  index = ['Percentile','Distance']\n  \n  list = dataset[index[0]].unique()\n  fig = plt.figure()\n  \n  for i in list:\n    \n    x = dataset[index[1]].loc[dataset[index[0]]==i]\n    x = np.log(x+1)\n    ax = sns.distplot(x,kde=False)\n    ax.set(xlim=(min(x), max(x)))\n    ax.set_ylabel('Counts')\n    ax.set_xlabel('Natural Log')\n    list = [j[-3:] for j in list]\n    ax.legend(labels=list,loc='upper right',fontsize='small')\n  ax.set_title(title)\n  plt.show()\n  \nfor i in Percentile_list:\n  \n  percentile_df,percentile_df0=QreIndex(train_df,i)\n  Qdistplot(percentile_df,i)","62522860":"percentile_df0.head()","78785a1f":"train_df['City'].unique()","eb46ba3c":"cities  = {'Atlanta':1, 'Boston':2, 'Chicago':3, 'Philadelphia':4}\n\n  \ndef encode(x):\n  \n  if pd.isna(x):\n    return 0\n  \n  for city in cities.keys():\n    \n    return cities[city]\n  \n  return 0\n\ntrain_df['City_num'] = train_df['City'].apply(encode)\n","5395e1a4":"def Hours(dataset,hours):\n  \n  k=0\n  \n  for i in hours:\n    \n    idx = dataset['Hour']==i\n    if k==0:\n      idx2 = idx.copy()\n      k = k+1\n    else:\n      idx2 = (idx2|idx)\n  data = dataset.loc[idx2]\n  \n  return data\n      ","b22e8374":"Timeline = ['Hour','Weekend','Month']\npercentile = [20,40,50,60,80]\npercentile2 = [20,50,80]\ndef TimeStopPlot(dataset,Percentile,types):\n  \n  Percentile_list = ['DistanceToFirstStop','TotalTimeStopped','TimeFromFirstStop']\n  width=3\n  fig,ax =plt.subplots(1,3)\n  fig.set_size_inches(15, 3)\n  \n  for i in range(width):\n    if types=='Weekend':\n      dataset.groupby([types,'City'])[Percentile_list[i]+'_p{}'.format(Percentile)].mean().unstack().plot(kind='bar',ax=ax[i])\n      ax[i].set_title(Percentile_list[i]+'_p{}'.format(Percentile))\n    else:\n      dataset.groupby([types,'City'])[Percentile_list[i]+'_p{}'.format(Percentile)].mean().unstack().plot(ax=ax[i])\n      ax[i].set_title(Percentile_list[i]+'_p{}'.format(Percentile))\n  plt.show()\n\ndef SelectiveTimeplot(dataset,Percentile,Timeline):\n  \n  for j in Percentile:\n    \n    [TimeStopPlot(train_df,j,i) for i in Timeline]\n      \nSelectiveTimeplot(train_df,percentile2,Timeline)","a96cb496":"def Category(datasets,name):\n  \n  length = datasets[name].nunique()\n  dataset = [0]*length\n  list = datasets[name].unique()\n  \n  for i in range(length):\n    \n    dataset[i] = datasets[datasets[name]==list[i]]\n    \n  return dataset\n\n\ntrain_city_df = Category(train_df,'City')\n","dc0342eb":"def trafficMap(dataset):\n  \n  fig,ax = plt.subplots(figsize = (15,15))\n  crs = {'init' :'epsg:4326'}\n  city = dataset['City'].unique()[0]\n  geometry = [Point(xy) for xy in zip(dataset['Longitude'],dataset['Latitude'])]\n  geo_df = gpd.GeoDataFrame(dataset, crs = crs\n                          , geometry = geometry)\n  minx, miny, maxx, maxy = geo_df.total_bounds\n  ax.set_xlim(minx-0.01, maxx+0.01)\n  ax.set_ylim(miny-0.01, maxy+0.01)\n  #geo_df[geo_df['Hour']==4].plot(ax = ax, markersize = 5,color='r', marker='o', label='1')\n  geo_df[geo_df['Hour']==8].plot(ax = ax, markersize = 5, color='r', marker='v', label='2')\n  #geo_df[geo_df['Hour']==12].plot(ax = ax, markersize = 0.5, color='b', marker='*', label='3')\n  #geo_df[geo_df['Hour']==16].plot(ax = ax, markersize = 0.3, color='g', marker='o', label='4')\n  geo_df[geo_df['Hour']==24].plot(ax = ax, markersize = 1, color='b', marker='*', label='5')\n  ax.set_title(city)\n  plt.show()\n\ntrafficMap(train_city_df[0])\ntrafficMap(train_city_df[1])\ntrafficMap(train_city_df[2])\ntrafficMap(train_city_df[3])","670f8879":"#From : https:\/\/www.machinelearningplus.com\/statistics\/mahalanobis-distance\/\nimport scipy as sp\ndef mahalanobis(x=None, data=None, cov=None):\n    \"\"\"Compute the Mahalanobis Distance between each row of x and the data  \n    x    : vector or matrix of data with, say, p columns.\n    data : ndarray of the distribution from which Mahalanobis distance of each observation of x is to be computed.\n    cov  : covariance matrix (p x p) of the distribution. If None, will be computed from data.\n    \"\"\"\n    x_minus_mu = x - np.mean(data)\n    if not cov:\n        cov = np.cov(data.values.T)\n    inv_covmat = sp.linalg.inv(cov)\n    left_term = np.dot(x_minus_mu, inv_covmat)\n    mahal = np.dot(left_term, x_minus_mu.T)\n    return mahal.diagonal()","201e4597":"def Compute_Mahala(dataset):\n  distance = dataset[['Longitude','Latitude']]\n  distance = distance.round(5)\n  distance = distance.drop_duplicates()\n  distance['mahala'] = mahalanobis(x=distance, data=distance[['Latitude','Longitude']])\n    \n  return distance\n  \ndef Mahalanobis_dist(dataset):\n  \n  fig = plt.figure(figsize=(20,3))\n  \n  for i in range(len(dataset)):\n    plt.subplot(1, 5, i+1)\n    distance = Compute_Mahala(dataset[i])\n    city = dataset[i]['City'].unique()[0]\n    ax = sns.distplot(distance['mahala'],kde=False)\n    ax.set_title(city)\n    ax.set_xlabel('Mahalanobis_distance')\n    dataset[i]['mahala'] = distance['mahala']\n  return dataset\n\ntrain_city_df = Mahalanobis_dist(train_city_df)","ae80ab41":"distance = train_city_df[0][['Longitude','Latitude']]\ndistance = distance.round(5)\ndistance = distance.drop_duplicates()\ndistance['mahala'] = mahalanobis(x=distance, data=distance[['Latitude','Longitude']])","53dae3a7":"I just dive into this competition from yesterday, and I want to share some EDA with you. It might not be new to you but slightly different. that's a valuable points! :)","48901fde":"Mahalanobis distance is calculated by $\\sqrt{(\\textbf{x}-\\bar{\\textbf{x}})^\\top S^{-1}(\\textbf{x}-\\bar{\\textbf{x}})}$, where $S^{1}$ is inverse of covariance matrix and $\\textbf{x}$. You might check that Atlanda(A) and Philadelphia(P) is very skewed compared to Boston(B) and Chicage(C). and also checked that points drawn by Longitude and Latitude are more dense in A and P. You can get some information from here :)","12f0aea2":"Keep working! :)","2437931c":"I put natural log on raw values with +1(because of zero valeus)","3489b581":" You can see some patterns in these charts and you might get some good insight here and there :)\n","508ed215":"This is a simplified geomap without any background but just points drawn by Latitude and Longitude. You can roughly see Urban structure from the points."}}