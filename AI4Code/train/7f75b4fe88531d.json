{"cell_type":{"16d5e62c":"code","b0f9be27":"code","d7ec5420":"code","f793f1ff":"code","fa762888":"code","7050b6b2":"code","511d8bd7":"code","eb367027":"code","b1a379a6":"code","fff867a0":"code","b121cd35":"code","f3b69dc6":"code","3687821b":"code","55952b9f":"code","f2f4044c":"code","de79847b":"code","b9362fa7":"code","4ff00b66":"code","5b0971a9":"code","045c9b41":"code","98b2bdd6":"code","c494c62d":"code","94e55fc3":"markdown","deb9f34d":"markdown","fbcde4c6":"markdown","4afdbb06":"markdown","49697be3":"markdown","365f1698":"markdown","38e0a35a":"markdown","ea6db8da":"markdown","f6ed5f06":"markdown","b7e30124":"markdown","28b3b943":"markdown","48586608":"markdown","b65cae09":"markdown","dcbc6883":"markdown","2bfc3e60":"markdown"},"source":{"16d5e62c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b0f9be27":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n\ndatasets = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndatasets.head()\n\n# Make only the columns you need.\ndatasets = datasets[['text', 'target']]\n\n# shuffle data\ndatasets = datasets.sample(frac=1).reset_index(drop=True)\n\ntrain_df, test_df = train_test_split(datasets, train_size=0.95)\nprint('train data', train_df.shape)\nprint('test data', test_df.shape)\nprint(train_df.head())\n\n# save as tsv file\ntrain_df.to_csv('.\/train.tsv', sep='\\t', index=False, header=None)\ntest_df.to_csv('.\/test.tsv', sep='\\t', index=False, header=None)","d7ec5420":"import seaborn as sns\nsns.distplot(datasets['target'])","f793f1ff":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport torchtext\nfrom transformers import BertTokenizer\nfrom transformers.modeling_bert import BertModel","fa762888":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\n# Check Bert Network\nprint(model)","7050b6b2":"text = list(train_df['text'])[0]\nprint(\"sample text: \", text)\ntokens = tokenizer.encode(text, return_tensors='pt')\nprint(tokens)\nprint(tokenizer.convert_ids_to_tokens(tokens[0].tolist()))","511d8bd7":"length = datasets['text'].map(tokenizer.encode).map(len)\nprint(max(length))\nsns.distplot(length)\n#The maximum length of the series that BERT can handle is 512, but the data in this case is fine.","eb367027":"def bert_tokenizer(text):\n    return tokenizer.encode(text, return_tensors='pt')[0]\n\nTEXT = torchtext.data.Field(sequential=True, tokenize=bert_tokenizer, use_vocab=False, lower=False,\n                            include_lengths=True, batch_first=True, pad_token=0)\nLABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n\ntrain_data, test_data = torchtext.data.TabularDataset.splits(\n    path='.\/', train='train.tsv', test='test.tsv', format='tsv', fields=[('Text', TEXT), ('Label', LABEL)])\n\nBATCH_SIZE = 32\ntrain_iter, test_iter = torchtext.data.Iterator.splits((train_data, test_data),\n                                                       batch_sizes=(BATCH_SIZE, BATCH_SIZE), repeat=False, sort=False)","b1a379a6":"batch = next(iter(train_iter))\nprint(batch.Text)\nprint(batch.Label)","fff867a0":"class BertClassifier(nn.Module):\n    def __init__(self):\n        super(BertClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        # BERT hidden state size is 768, class number is 2\n        self.linear = nn.Linear(768, 2)\n        # initialing weights and bias\n        nn.init.normal_(self.linear.weight, std=0.02)\n        nn.init.normal_(self.linear.bias, 0)\n\n    def forward(self, input_ids):\n        # get last_hidden_state\n        vec, _ = self.bert(input_ids)\n        # only get first token 'cls'\n        vec = vec[:,0,:]\n        vec = vec.view(-1, 768)\n\n        out = self.linear(vec)\n        return F.log_softmax(out)\n\nclassifier = BertClassifier()","b121cd35":"# First, turn off the gradient for all parameters.\nfor param in classifier.parameters():\n    param.requires_grad = False\n\n# Second, turn on only last BERT layer.\nfor param in classifier.bert.encoder.layer[-1].parameters():\n    param.requires_grad = True\n\n# Finally, turn on classifier layer.\nfor param in classifier.linear.parameters():\n    param.requires_grad = True\n\nimport torch.optim as optim\n\n# The pre-learned sections should have a smaller learning rate, and the last total combined layer should be larger.\noptimizer = optim.Adam([\n    {'params': classifier.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n    {'params': classifier.linear.parameters(), 'lr': 1e-4}\n])\n\n# loss function\nloss_function = nn.NLLLoss()","f3b69dc6":"# set GPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# send network to GPU\nclassifier.to(device)\nlosses = []\n\nfor epoch in range(10):\n    all_loss = 0\n    for idx, batch in enumerate(train_iter):\n        batch_loss = 0\n        classifier.zero_grad()\n        input_ids = batch.Text[0].to(device)\n        label_ids = batch.Label.to(device)\n        out = classifier(input_ids)\n        batch_loss = loss_function(out, label_ids)\n        batch_loss.backward()\n        optimizer.step()\n        all_loss += batch_loss.item()\n    print(\"epoch\", epoch, \"\\t\" , \"loss\", all_loss)","3687821b":"from sklearn.metrics import classification_report\n\nanswer = []\nprediction = []\nwith torch.no_grad():\n    for batch in test_iter:\n\n        text_tensor = batch.Text[0].to(device)\n        label_tensor = batch.Label.to(device)\n\n        score = classifier(text_tensor)\n        _, pred = torch.max(score, 1)\n\n        prediction += list(pred.cpu().numpy())\n        answer += list(label_tensor.cpu().numpy())\nprint(classification_report(prediction, answer))","55952b9f":"test_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nids = list(test_df['id'])\ntest_df = test_df[['text']]\ntest_df['target'] = [0 for _ in range(test_df.shape[0])]\ntest_df.head()\ntest_df.to_csv('.\/s_test.tsv', sep='\\t', index=False, header=None)\n\ntrain_data, test_data = torchtext.data.TabularDataset.splits(\n    path='.\/', train='train.tsv', test='s_test.tsv', format='tsv', fields=[('Text', TEXT), ('Label', LABEL)])\n\nBATCH_SIZE = 32\n_, test_iter = torchtext.data.Iterator.splits((train_data, test_data),\n                                                       batch_sizes=(BATCH_SIZE, BATCH_SIZE), repeat=False, sort=False)","f2f4044c":"prediction = []\nwith torch.no_grad():\n    for batch in test_iter:\n\n        text_tensor = batch.Text[0].to(device)\n        label_tensor = batch.Label.to(device)\n\n        score = classifier(text_tensor)\n        _, pred = torch.max(score, 1)\n\n        prediction += list(pred.cpu().numpy())","de79847b":"sub_dict = {'id':ids, 'target':prediction}\n\nsub = pd.DataFrame.from_dict(sub_dict)\nsub.to_csv(\"submission.csv\", index = False)","b9362fa7":"from transformers import * \n\nroberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nroberta_model = RobertaModel.from_pretrained('roberta-base')","4ff00b66":"def roberta_tokenizer(text):\n    return tokenizer.encode(text, return_tensors='pt')[0]\n\nTEXT = torchtext.data.Field(sequential=True, tokenize=roberta_tokenizer, use_vocab=False, lower=False,\n                            include_lengths=True, batch_first=True, pad_token=0)\nLABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n\ntrain_data, test_data = torchtext.data.TabularDataset.splits(\n    path='.\/', train='train.tsv', test='test.tsv', format='tsv', fields=[('Text', TEXT), ('Label', LABEL)])\n\nBATCH_SIZE = 32\ntrain_iter, test_iter = torchtext.data.Iterator.splits((train_data, test_data),\n                                                       batch_sizes=(BATCH_SIZE, BATCH_SIZE), repeat=False, sort=False)","5b0971a9":"class RobertaClassifier(nn.Module):\n    def __init__(self):\n        super(RobertaClassifier, self).__init__()\n        self.roberta = RobertaModel.from_pretrained('roberta-base')\n        # ROBERTA hidden state size is 768, class number is 2\n        self.linear = nn.Linear(768, 2)\n        # initialing weights and bias\n        nn.init.normal_(self.linear.weight, std=0.02)\n        nn.init.normal_(self.linear.bias, 0)\n\n    def forward(self, input_ids):\n        # get last_hidden_state\n        vec, _ = self.roberta(input_ids)\n        # only get first token 'cls'\n        vec = vec[:,0,:]\n        vec = vec.view(-1, 768)\n\n        out = self.linear(vec)\n        return F.log_softmax(out)\n\nclassifier = RobertaClassifier()","045c9b41":"# First, turn off the gradient for all parameters.\nfor param in classifier.parameters():\n    param.requires_grad = False\n\n# Second, turn on only last BERT layer.\nfor param in classifier.roberta.encoder.layer[-1].parameters():\n    param.requires_grad = True\n\n# Finally, turn on classifier layer.\nfor param in classifier.linear.parameters():\n    param.requires_grad = True\n\nimport torch.optim as optim\n\n# The pre-learned sections should have a smaller learning rate, and the last total combined layer should be larger.\noptimizer = optim.Adam([\n    {'params': classifier.roberta.encoder.layer[-1].parameters(), 'lr': 5e-5},\n    {'params': classifier.linear.parameters(), 'lr': 1e-4}\n])\n\n# loss function\nloss_function = nn.NLLLoss()","98b2bdd6":"# set GPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# send network to GPU\nclassifier.to(device)\nlosses = []\n\nfor epoch in range(10):\n    all_loss = 0\n    for idx, batch in enumerate(train_iter):\n        batch_loss = 0\n        classifier.zero_grad()\n        input_ids = batch.Text[0].to(device)\n        label_ids = batch.Label.to(device)\n        out = classifier(input_ids)\n        batch_loss = loss_function(out, label_ids)\n        batch_loss.backward()\n        optimizer.step()\n        all_loss += batch_loss.item()\n    print(\"epoch\", epoch, \"\\t\" , \"loss\", all_loss)","c494c62d":"answer = []\nprediction = []\nwith torch.no_grad():\n    for batch in test_iter:\n\n        text_tensor = batch.Text[0].to(device)\n        label_tensor = batch.Label.to(device)\n\n        score = classifier(text_tensor)\n        _, pred = torch.max(score, 1)\n\n        prediction += list(pred.cpu().numpy())\n        answer += list(label_tensor.cpu().numpy())\nprint(classification_report(prediction, answer))","94e55fc3":"## Import Library","deb9f34d":"## Check the distribution of the data.","fbcde4c6":"## Declare BertModel","4afdbb06":"## Create DataLoader using torchtext","49697be3":"If you want to use another BERT model, just change the model declaration in the source code above.  \nSee also https:\/\/github.com\/huggingface\/transformers","365f1698":"### Check dataloader","38e0a35a":"## Learning","ea6db8da":"## Setting fine tuning","f6ed5f06":"## Check max sentence(tokens) length","b7e30124":"## For submission","28b3b943":"## Check accuracy","48586608":"## Declare Classifier Model","b65cae09":"## For example, use ROBERTA model","dcbc6883":"## Test Tokenizer","2bfc3e60":"## Prepare training\/test data"}}