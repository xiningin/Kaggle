{"cell_type":{"70c9a4a8":"code","a15adfd8":"code","faf94cdc":"code","421e54dd":"code","18b6b02a":"markdown","68443814":"markdown","49df8769":"markdown","70f6462a":"markdown"},"source":{"70c9a4a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a15adfd8":"#Load data and preprocess\nfrom keras.datasets import imdb\nfrom keras.preprocessing import sequence\nimport numpy as np\n\nimdb_path = r'\/kaggle\/input\/imdb-dataset-for-keras-imdbnpz\/imdb.npz'\n\nmax_features = 10000\nmax_len = 500\n\n#Load data\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(path=imdb_path, num_words=max_features)\n\n#One hot code\ndef vectorize_samples(samples, features=max_features):\n    results = np.zeros((len(samples), features))\n    for i, sample in enumerate(samples):\n        results[i, sample] = 1.\n    return results\none_hot_train_data = vectorize_samples(train_data)\none_hot_test_data = vectorize_samples(test_data)\none_hot_train_labels = np.asarray(train_labels).astype('float32')\none_hot_test_labels = np.asarray(test_labels).astype('float32')\n\n#Pad sequence\nx_train = sequence.pad_sequences(train_data, maxlen=max_len)\nx_test = sequence.pad_sequences(test_data, maxlen=max_len)\ny_train = np.array(train_labels)\ny_test = np.array(test_labels)\n\nprint('Data prepared.')","faf94cdc":"#Common function\nimport matplotlib.pyplot as plt\n\ndef show_loss_and_acc(history):\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n    plt.show()\n    plt.figure()\n    plt.plot(epochs, acc, 'ro', label='Training acc')\n    plt.plot(epochs, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation acc')\n    plt.legend()\n    plt.show()\n\ndef test_model(model, data, labels):\n    print('Test result:')\n    results = model.evaluate(data, labels)\n    predictions = model.predict(data)\n    print(results)\n    print(predictions)\n    \nprint('Common function prepared.')","421e54dd":"#Use functional programming\nimport keras\nfrom keras import models\nfrom keras import layers\nfrom keras import Input\nfrom keras.models import Model\nfrom keras.utils import plot_model\n\n#Define the model\nimdb_input = Input(shape=(None,), dtype='int32', name='imdb')\nimdb_embedded = layers.Embedding(max_features, 64)(imdb_input)\nbranch_a = layers.GRU(128)(imdb_embedded)\nbranch_b = layers.Conv1D(128, 5, activation='relu')(imdb_embedded)\nbranch_b = layers.GlobalMaxPooling1D()(branch_b)\nconcatenated = layers.concatenate([branch_a, branch_b], axis=-1)\nimdb_output = layers.Dense(1, activation='sigmoid')(concatenated)\nmodel = Model(imdb_input, imdb_output)\nmodel.summary()\n\n#Compile the model\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n\n#monitor the model\ncallbacks_list = [\n    keras.callbacks.EarlyStopping(monitor='val_acc', patience=5)\n]\nhistory = model.fit(x_train, y_train, epochs=20, batch_size=32, validation_split=0.2, callbacks=callbacks_list)\n\n#Show and test\nshow_loss_and_acc(history)\ntest_model(model, x_test, y_test)","18b6b02a":"2. Define some functions","68443814":"**IMDB reviews sentiment classification using Keras**\n\nHi, It's a \"hello, world\" problem in DL area. Does anyone know what the highest val_acc is for this problem using the same Keras built-in imdb dataset? As a noob, It's really not easy for me to get val_acc over 0.9 till I tried functional pragramming to combine RNN(GRU) & CNN(Conv1D) together. Maybe next I should continue improving my model and\/or trying something like AutoML for getting a better val_acc? What's your suggestion? Please don't hesitate to tell me, I will be very much appreciated.","49df8769":"1. Load data and preprocess","70f6462a":"3. Try my model"}}