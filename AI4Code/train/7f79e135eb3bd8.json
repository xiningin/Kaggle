{"cell_type":{"ab2d0605":"code","3b9fb563":"code","af9aa2a4":"code","6bc91d22":"code","b7bb73e9":"code","41c32072":"code","716e408d":"code","5548e128":"code","db336d83":"code","adb97276":"code","38b273b7":"code","106a12fe":"code","4fe64678":"code","7430f26c":"code","454d1fb8":"code","adebdca0":"code","db89f7fe":"code","84962da7":"code","f099f58c":"code","184772fa":"code","32003edd":"code","3f741864":"code","fcac23ee":"code","3586dc8e":"code","263ed0c1":"markdown","afbd148d":"markdown"},"source":{"ab2d0605":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b9fb563":"dataset = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndataset.head()\n","af9aa2a4":"print(dataset.columns)","6bc91d22":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()\n","b7bb73e9":"print(test_data.columns)","41c32072":"#\n# Add Title (Mr, Mrs, etc) and FamilyS (family size) to datasets\n#\n\ndataset_title = [i.split(',')[1].split('.')[0].strip() for i in dataset['Name']]\ndataset['Title'] = pd.Series(dataset_title)\ndataset['Title'].value_counts()\ndataset['Title'] = dataset['Title'].replace(['Lady', 'the Countess', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona', 'Ms', 'Mme', 'Mlle'], 'Rare')\n\ndataset_title = [i.split(',')[1].split('.')[0].strip() for i in test_data['Name']]\ntest_data['Title'] = pd.Series(dataset_title)\ntest_data['Title'].value_counts()\ntest_data['Title'] = test_data['Title'].replace(['Lady', 'the Countess', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona', 'Ms', 'Mme', 'Mlle'], 'Rare')\n\ndataset['FamilyS'] = dataset['SibSp'] + dataset['Parch'] + 1\ntest_data['FamilyS'] = test_data['SibSp'] + test_data['Parch'] + 1","716e408d":"#\n# Convert FamilyS to factor\n#\ndef family(x):\n    if x < 2:\n        return 'Single'\n    elif x == 2:\n        return 'Couple'\n    elif x <= 4:\n        return 'InterM'\n    else:\n        return 'Large'\n    \ndataset['FamilyS'] = dataset['FamilyS'].apply(family)\ntest_data['FamilyS'] = test_data['FamilyS'].apply(family)","5548e128":"test_data.head()","db336d83":"#\n# Replace NA's in Embarked, Age and Fare columns\n#\ndataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace=True)\ntest_data['Embarked'].fillna(test_data['Embarked'].mode()[0], inplace=True)\ndataset['Age'].fillna(dataset['Age'].median(), inplace=True)\ntest_data['Age'].fillna(test_data['Age'].median(), inplace=True)\ntest_data['Fare'].fillna(test_data['Fare'].median(), inplace=True)","adb97276":"#\n# Drop unused columns\n#\ndataset = dataset.drop(['PassengerId', 'Cabin', 'Name', 'SibSp', 'Parch', 'Ticket'], axis=1)\ntest_passengers = test_data['PassengerId']\ntest_data = test_data.drop(['PassengerId', 'Cabin', 'Name', 'SibSp', 'Parch', 'Ticket'], axis=1)","38b273b7":"test_data.head()","106a12fe":"#\n# Split to train, dev, test\n#\nX_train = dataset.iloc[:, 1:9].values\nY_train = dataset.iloc[:, 0].values\nX_test = test_data.values","4fe64678":"#\n# Convert the remaining labels to numbers\n#\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nlabelencoder_X_1 = LabelEncoder()\nX_train[:, 1] = labelencoder_X_1.fit_transform(X_train[:, 1])\nX_train[:, 4] = labelencoder_X_1.fit_transform(X_train[:, 4])\nX_train[:, 5] = labelencoder_X_1.fit_transform(X_train[:, 5])\nX_train[:, 6] = labelencoder_X_1.fit_transform(X_train[:, 6])\n\nlabelencoder_X_2 = LabelEncoder()\nX_test[:, 1] = labelencoder_X_2.fit_transform(X_test[:, 1])\nX_test[:, 4] = labelencoder_X_2.fit_transform(X_test[:, 4])\nX_test[:, 5] = labelencoder_X_2.fit_transform(X_test[:, 5])\nX_test[:, 6] = labelencoder_X_2.fit_transform(X_test[:, 6])","7430f26c":"#\n# Convert categorical values to one-hot representation\n#\ncolumn_transformer_1 = ColumnTransformer([('one_hot', OneHotEncoder(), [0, 1, 4, 5, 6])], remainder='passthrough')\ncolumn_transformer_2 = ColumnTransformer([('one_hot', OneHotEncoder(), [0, 1, 4, 5, 6])], remainder='passthrough')\n\nX_train = column_transformer_1.fit_transform(X_train).astype(np.float64)\nX_test = column_transformer_2.fit_transform(X_test).astype(np.float64)\n\n","454d1fb8":"print(X_test.shape)","adebdca0":"#\n# Split train to train, val\n#\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size = 0.1)","db89f7fe":"#\n# Build NN using PyTorch with 3 layers.\n# Layer 1 is linear, has 270 neurons, with Dropout and ReLU activation\n# Layer 2 is linear, has 16 neurons, with ReLU activation\n# Layer 3 is linear with 2 neurons and uses Sigmoid activation\n#\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass NeuralNet(nn.Module):\n    \n    def __init__(self):\n        super(NeuralNet, self).__init__()\n        self.fc1 = nn.Linear(19, 270)\n        self.fc2 = nn.Linear(270, 16)\n        self.fc3 = nn.Linear(16, 2)\n\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.dropout(x, p=0.1)\n        x = F.relu(x)\n        x = self.fc2(x)\n        x = F.dropout(x, p=0.1)\n        x = F.relu(x)        \n        x = self.fc3(x)\n        x = torch.sigmoid(x)\n        \n        return x\n    \nnet = NeuralNet()","84962da7":"#\n# Set learning meta-parameters\n#\nbatch_size = 50\nnum_epochs = 200\nlearning_rate = 0.01\nbatch_no = len(x_train) \/\/ batch_size","f099f58c":"#\n# Set more learning meta-parameters \n#\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)","184772fa":"#\n# Run the learning loop\n#\nfrom sklearn.utils import shuffle\nfrom torch.autograd import Variable\n\n# make reproducible\nnp.random.seed(1)\ntorch.manual_seed(2)\nshuffle(random_state=3)\n\nloss_values = []\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    if epoch % 5 == 0:\n        print('Epoch {}'.format(epoch+1))\n    x_train, y_train = shuffle(x_train, y_train)\n    # Mini batch learning\n    for i in range(batch_no):\n        start = i * batch_size\n        end = start + batch_size\n        x_var = Variable(torch.FloatTensor(x_train[start:end]))\n        y_var = Variable(torch.LongTensor(y_train[start:end]))\n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        ypred_var = net(x_var)\n        loss = criterion(ypred_var, y_var)\n        loss_values.append(loss.item())\n        loss.backward()\n        optimizer.step()","32003edd":"#\n# Plot the loss\n#\n%matplotlib inline\nimport matplotlib.pyplot as plt\n_ = plt.plot(np.array(loss_values), 'r')","3f741864":"#\n# Check the accuracy using test\n#\ntest_var = Variable(torch.FloatTensor(x_val), requires_grad=True)\nwith torch.no_grad():\n    result = net(test_var)\nvalues, labels = torch.max(result, 1)\nnum_right = np.sum(labels.data.numpy() == y_val)\nprint('Accuracy {:.2f}'.format(num_right \/ len(y_val)))","fcac23ee":"#\n# Applying model on the test data\n#\nX_test_var = Variable(torch.FloatTensor(X_test), requires_grad=True) \nwith torch.no_grad():\n    test_result = net(X_test_var)\nvalues, labels = torch.max(test_result, 1)\npredictions = labels.data.numpy()","3586dc8e":"#\n# Create output\n#\noutput = pd.DataFrame({'PassengerId': test_passengers, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","263ed0c1":"# Getting Started with Titanic using PyTorch","afbd148d":"## Introduction\n\nMy goal in creating this notebook is to learn how to build a neural network using PyTorch, and use that as a basis to build one using Neural Ordinary Differential Equations [1]. The authors have implemented Neural ODE's for PyTorch in the the `torchdiffeq` library found at [https:\/\/github.com\/rtqichen\/torchdiffeq](https:\/\/github.com\/rtqichen\/torchdiffeq). \n\nI made use of several other good Notebooks, including [Getting Started with Titanic](https:\/\/www.kaggle.com\/tanvibhandarkar\/getting-started-with-titanic) which I used for loading the *training* and *test* data.\n\nNext, I manipulated the data into suitable columns, and did the train\/dev split like from [Titanic - PyTorch](https:\/\/www.kaggle.com\/kiranscaria\/titanic-pytorch) notebook, although I had to update the code a bit to new versions of the libraries.\n\n## References\n[1]: Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud (2018):  _Neural ordinary differential equations_.\n"}}