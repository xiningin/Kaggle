{"cell_type":{"ce0310a6":"code","99668c90":"code","649be7f0":"code","241a1860":"code","91af880a":"code","6e6754bd":"code","dc271ecb":"code","36327445":"code","f445913c":"code","9c24bee7":"code","3d7aeec6":"code","8b508f44":"code","d566b5bc":"code","833aab73":"code","48e585f1":"code","e782dead":"code","8cdf0a3d":"code","c3c66fd2":"code","3f696875":"code","c586258b":"code","21df5056":"code","b767456d":"code","a057de92":"code","a3a891d2":"code","80073eae":"code","b2b833dd":"code","9d7929a8":"code","c554639b":"code","5406ccfc":"code","585b904c":"code","a705033a":"code","1e832f80":"code","51587b13":"code","ffc826b7":"code","b7848368":"markdown","9110796a":"markdown","ce18d4fd":"markdown","9d857fa3":"markdown","03134985":"markdown","e64a3d8f":"markdown","552b3b7e":"markdown","d2435ce2":"markdown","fce75b28":"markdown","f69113e5":"markdown","672dbe5b":"markdown"},"source":{"ce0310a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","99668c90":"train_df = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')","649be7f0":"train_df.head()","241a1860":"train_df.drop(['id'], axis=1, inplace=True)\n","91af880a":"train_df","6e6754bd":"#Examine target value\ntrain_df['target'].value_counts()","dc271ecb":"import matplotlib.pyplot as plt\nimport seaborn as sns\ncolors=['#5e5e5e','#919191','#c6c6c6','#ffffff']\nsns.set(palette=colors, style='white', rc={'axes.facecolor':'#1c465f', 'figure.facecolor':'#1c465f','text.color':'white','axes.labelcolor':'white'\n                                                       , 'xtick.color':'white','ytick.color':'white', 'axes.edgecolor':'white',\n                                          'font.family': ['serif'],'font.sans-serif': ['Liberation Sans']})\nsns.palplot(colors)\n","36327445":"fig=plt.figure(figsize=(15,8))\n\nax=sns.countplot(train_df['target'], linewidth=2);\nplt.title(\"Target variable counts\", size='20', weight='bold')\n\nfor p in ax.patches:\n        ax.annotate(p.get_height(), (p.get_x()+0.25, p.get_height()+1000), ha='left', size=15)\n        \n","f445913c":"train_df.describe().T.style.bar(subset=['std'])","9c24bee7":"feat=train_df.std().nlargest(6).index.to_list()\nfeat","3d7aeec6":"from scipy.stats import norm\nfig,ax=plt.subplots(nrows=2, ncols=3, figsize=(20,8), sharey=True)\nplt.suptitle(\"Distribution of Feature with high Std.Dev\", size=20, weight=20)\nax=ax.flatten()\nfor i,j in enumerate(feat):\n    sns.kdeplot(train_df[j], ax=ax[i], fill=True, color='white', hue=train_df['target'])\n    ax[i].axvline(train_df[j].mean(), ls='--', c='white')\n    ax[i].set_xlim(ax[i].get_xlim()[0],20)","8b508f44":"from scipy.stats import norm\nfig,ax=plt.subplots(nrows=2, ncols=3, figsize=(20,8), sharey=True)\nplt.suptitle(\"Distribution of Feature with high Std.Dev\", size=20, weight=20)\nax=ax.flatten()\nfor i,j in enumerate(feat):\n    sns.scatterplot(x=train_df[j],y=train_df['feature_35'], ax=ax[i], hue=train_df['target'])\n    ","d566b5bc":"from scipy.stats import norm\nfig,ax=plt.subplots(nrows=2, ncols=3, figsize=(20,8), sharey=True)\nplt.suptitle(\"Distribution of Feature with high Std.Dev\", size=20, weight=20)\nax=ax.flatten()\nfor i,j in enumerate(feat):\n    sns.boxplot(x=train_df[j], ax=ax[i], y=train_df['target'])","833aab73":"feat.append('target')\n","48e585f1":"sns.pairplot(train_df[feat], hue='target')","e782dead":"sns.jointplot(x=train_df['feature_38'],y=train_df['feature_39'], kind='kde', hue=train_df['target'])","8cdf0a3d":"#univariated analysis\nclass_v={'Class_1':0,'Class_2':1,'Class_3':2,'Class_4':3}\ntrain_df['target']=train_df['target'].map(class_v)","c3c66fd2":"#train_df.drop('id', axis=1, inplace=True)\nX=train_df.drop('target', axis=1)\ny=train_df['target']\ncols=[]\n#X=X.apply(lambda x: np.log(x+1))\nfor i in X.columns:\n    if X[i].nunique()<10:\n        cols.append(i)\n\n#from imblearn import over_sampling\n#samp = over_sampling.SMOTE()\n#X, y=samp.fit_resample(X,y)\n\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\ndep_cols = train_df.columns\n\n\n\nfrom sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, QuantileTransformer, PowerTransformer\nscale = QuantileTransformer().fit(X_train.drop(cols, axis=1))\nX_train_scale=scale.transform(X_train.drop(cols,axis=1))\nX_test_scale=scale.transform(X_test.drop(cols, axis=1))\n\n","3f696875":"cols","c586258b":"import seaborn as sns\nsns.kdeplot(X_train_scale[:,10])","21df5056":"sns.scatterplot(x=X_train_decom[:,0],y=X_train_decom[:,1], hue=y_train)","b767456d":"from sklearn.linear_model import LogisticRegressionCV\n#from sklearn.svm import SVC\nreg= LogisticRegressionCV(cv=5, scoring='accuracy')\n#reg=SVC()\nreg.fit(X_train_scale, y_train)\nprint(reg.score(X_train_scale, y_train))\npred=reg.predict(X_test_scale)","a057de92":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nprint(accuracy_score(y_test, pred))\nprint(classification_report(y_test, pred))","a3a891d2":"from xgboost import XGBClassifier, plot_importance, XGBRFClassifier\n\nX= train_df.drop(['target'], axis=1)\ny=train_df['target']\n\n#from imblearn import over_sampling\n#samp = over_sampling.SMOTE(k_neighbors=40)\n#X, y=samp.fit_resample(X,y)\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\ndep_cols = train_df.columns\n\nfrom sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, QuantileTransformer\nscale = QuantileTransformer().fit(X_train.drop(cols, axis=1))\nX_train_scale=scale.transform(X_train.drop(cols,axis=1))\nX_test_scale=scale.transform(X_test.drop(cols, axis=1))\n\n\nmodel1 = XGBClassifier(use_label_encoder=False)\nmodel1.fit(X_train_scale,y_train)\nprint(model1.score(X_train_scale,y_train))\npred=model1.predict(X_test_scale)\n","80073eae":"print(accuracy_score(y_test,pred))\nprint(classification_report(y_test, pred))\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nsns.heatmap(confusion_matrix(y_test, pred), annot=True, fmt='2d')","b2b833dd":"fig=plt.figure(figsize=(15,8))\nplot_importance(model1)","9d7929a8":"X1=scale.transform(X.drop(cols,axis=1))\nmodel1.fit(X1,y)","c554639b":"test_x=test_df.drop('id',axis=1)\ntest_x_scale=scale.transform(test_x.drop(cols,axis=1))\n#test_x_scale.shape\n#test_x_best=seq.transform(test_x_scale)\n#print(test_x_best.shape)\npred_test=model1.predict_proba(test_x_scale)\npred_test = pd.DataFrame(pred_test, columns=['Class_1','Class_2','Class_3','Class_4'])\npred_test=pred_test.set_index(test_df.index)\ndf=pd.concat([test_df['id'],pred_test], ignore_index=False, axis=1)\ndf.to_csv('submission1.csv', index=False)","5406ccfc":"import h2o\nfrom h2o.automl import H2OAutoML\nh2o.init()","585b904c":"train = h2o.H2OFrame(train_df)\ntrain['target']=train['target'].asfactor()\nfeat = train.drop(['target'], axis=1).columns\ntarget='target'","a705033a":"model = H2OAutoML(seed=13,stopping_metric='logloss',preprocessing=['target_encoding'],exploitation_ratio=0.1, sort_metric='logloss')\nmodel.train(x=feat, y=target, training_frame=train)","1e832f80":"model.leaderboard","51587b13":"test=h2o.H2OFrame(test_df)\npred = model.leader.predict(test.drop('id', axis=1))","ffc826b7":"#df = pd.DataFrame(columns=['id','Class_1','Class_2','Class_3','Class_4'])\ndf=h2o.as_list(pred, use_pandas=True)\n\ndf['id']=test_df['id']\n\ndf.drop(['predict'], axis=1, inplace=True)\ndf=df[['id','Class_1','Class_2','Class_3','Class_4']]\ndf.to_csv('submission_automl.csv', index=False)","b7848368":"# Read Data","9110796a":"# Tabular Playground Analysis","ce18d4fd":"# Lets try to decompose the data so find patern","9d857fa3":"# Model creation","03134985":"***There are lot of outliers in the data***","e64a3d8f":"*** couldn't find cluster or relationship in the data.***","552b3b7e":"***Simple logisitc regression give 57% accuracy but it always predicts Class 2. so we need to go for alternative method***","d2435ce2":"***In all the above high density features, Class 2 has last data points compared to other 3 classes. So, there is imbalance in data.***","fce75b28":"I will keep working to improve the score.","f69113e5":"# Try with H2O.ai","672dbe5b":"***decomposition technique also doesn't provide pattern in data***"}}