{"cell_type":{"885b0f2c":"code","5abdf214":"code","702b58fe":"code","c0c5d3dd":"code","ff8c973a":"code","a1924095":"code","97a42124":"code","70a16f4b":"code","6ee235c5":"code","978b2601":"code","9455685d":"code","55269722":"code","e633846d":"code","99be62f4":"code","4f1106c7":"code","f1378c90":"code","d48e0eea":"code","bf78c263":"code","0052c769":"code","a60382bb":"code","2b32c5f5":"code","f884d82e":"code","70bd34cc":"code","1041972e":"code","dac8b218":"code","6d1949ce":"code","980dab5e":"code","59a7a1ec":"code","6fc729c8":"code","56767d77":"code","5bdf960d":"code","ca451adc":"code","c6dadfa1":"code","4f0a90f5":"code","a89cce37":"code","fd28cb3c":"code","4e32d5ac":"code","4b2b56ca":"code","7c03119e":"code","f24332e8":"code","21d8ee08":"code","a018a20e":"code","dd6eec91":"code","119a810d":"code","cc775f24":"code","7a0f6bc6":"code","18e139ff":"code","c53fefd5":"code","05fd6b45":"code","e9d5b993":"code","54cde947":"code","88921261":"code","c82fff87":"code","b909c97b":"code","c1fcb4a7":"code","180f0dad":"code","13aa2d33":"code","615743fb":"code","f982cd7a":"code","d3b44a8c":"code","08859365":"code","b41b862a":"code","6b096058":"code","bf5acdf0":"code","d4c5e9a0":"code","528cf6d6":"code","911c7790":"code","a3504d78":"code","53bc2b5e":"code","f9e68dca":"code","6916ec9d":"code","bc4df065":"code","91c08fa6":"code","d3e141ac":"code","8c81ffaa":"code","d62b8606":"code","975a85ea":"code","d6608300":"code","d23a8660":"code","552b92d5":"code","48850c4c":"code","5daf07af":"code","1cd8dce4":"code","1701edc3":"code","1e9f57ca":"code","fb3038b2":"code","5be92eb4":"code","d281a7cd":"code","fda3233c":"code","d723143c":"code","f1df4fcd":"code","747fd2c9":"code","5efd9401":"code","cfc7fc09":"code","8e414a24":"code","0599f41c":"code","501fa024":"code","8253e018":"code","9166c7c4":"code","af95c7a2":"code","b731dec0":"code","bed41c2a":"code","a68334a9":"code","63efa7ae":"code","f36a60e7":"code","844af2ee":"code","61cdef07":"code","7ca5c906":"code","731faff1":"code","018c1d1f":"code","0bbb837c":"code","10d9abcf":"code","1db0fe75":"code","0a1f2d87":"markdown","2f0e1cd3":"markdown","adba5b10":"markdown","edd2306f":"markdown","1197c9ac":"markdown","7010cf58":"markdown","764ca483":"markdown","32091b15":"markdown","b8208698":"markdown","a389b874":"markdown","cc47c1c7":"markdown","03596710":"markdown","5d69c487":"markdown","5ddd118b":"markdown","34e32193":"markdown","3629a02b":"markdown","a7dbcb45":"markdown","2d507d7e":"markdown","247fc50a":"markdown","2544b8e7":"markdown","821ea9b4":"markdown","2f314627":"markdown","344331ba":"markdown","a9db67f8":"markdown","7c6f4808":"markdown","28e9bb67":"markdown","b97df588":"markdown","b5f99c8f":"markdown","4b541961":"markdown","3db387ec":"markdown","4daa659e":"markdown","355be2e2":"markdown","49400a51":"markdown","99d78a93":"markdown","e93c2f4e":"markdown","a8eb1e4f":"markdown","cb3838a9":"markdown","2f62e026":"markdown","6f2d906a":"markdown","7f2c3dd2":"markdown","431dfdc1":"markdown","65fe1f55":"markdown","9f6ae410":"markdown","164fc12c":"markdown","f45d9671":"markdown","95aefc21":"markdown","8db9bcec":"markdown","e4153e54":"markdown","5f5b4359":"markdown","b35e2e5f":"markdown","bb67a39e":"markdown","6abb7275":"markdown","7cb42e7e":"markdown","9e3515eb":"markdown","329a8c32":"markdown","5932fb78":"markdown","f6481ef8":"markdown","9dbaabb4":"markdown","60b932ce":"markdown","cccdf22e":"markdown","c77ddce5":"markdown","6c5b7226":"markdown","32b2f52a":"markdown","ee5b3e68":"markdown","92496a2a":"markdown","69b8e363":"markdown","8954c2c6":"markdown","7b2d1e04":"markdown","25a8f21c":"markdown","d7bd7912":"markdown","3969d6e8":"markdown","7706cc11":"markdown","cfd41e17":"markdown","ef4b8692":"markdown","ebb34f4a":"markdown","6c8d47f5":"markdown","0d8c1a5f":"markdown","0dae682b":"markdown","b57153b1":"markdown","98623bcc":"markdown"},"source":{"885b0f2c":"#! pip install comet-ml\n#! pip install wordcloud\n#! pip install tsne\n#! pip install textblob\n#! pip install gensim\n#! pip install scikitplot","5abdf214":"#Deploying the model\n#from comet_ml import Experiment","702b58fe":"# Api key\n#experiment = Experiment(api_key=\"NMdrE2Fvv00bzfhwE99pCjGSq\",\n                        #project_name=\"team-4-climate-change\", workspace=\"primmk\", log_code=True)","c0c5d3dd":"\n#Standard Imports\nimport numpy as np\nimport pandas as pd\nimport re\n\n#Visualisations \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport scikitplot as skplt\n\n#Data Cleaning\nfrom nltk.stem import PorterStemmer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom textblob import Word\n\n\n#import dependencies\nfrom gensim.models import word2vec\nfrom sklearn.manifold import TSNE\n\n#import for imbalance\nfrom sklearn.utils import resample\n\n\n# imports for N- Grams\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n#Modeling\nfrom sklearn import preprocessing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics \nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\n","ff8c973a":"df_train=pd.read_csv(\"..\/input\/climate-change-belief-analysis\/train.csv\")\ndf_test =pd.read_csv(\"..\/input\/climate-change-belief-analysis\/test.csv\")\n","a1924095":"df_train.head()","97a42124":"df_test.head()","70a16f4b":"# check null values in the train dataframe\ndf_train.isnull().sum()","6ee235c5":"# check null values in the test dataframe\ndf_test.isnull().sum()","978b2601":"blanks = []  # start with an empty list\n\nfor i,sen,mes,twe in df_train.itertuples():  # iterate over the DataFrame\n    if type(mes)==str:            # avoid NaN values\n        if mes.isspace():         # test 'review' for whitespace\n            blanks.append(i)     # add matching index numbers to the list\n        \nprint(len(blanks), 'blanks: ', blanks)","9455685d":"dist_class = df_train['sentiment'].value_counts()\nlabels = ['1', '2','0','-1']\n\nfig, (ax1 )= plt.subplots(1, figsize=(8,4))\n\nsns.barplot(x=dist_class.index, y=dist_class, ax=ax1).set_title(\"Tweet message distribution over the sentiments\")\nplt.show()","55269722":"df_train.info()","e633846d":"def clean_text(df):\n    \"\"\"\n    This function cleans tweets on the 'messages' column.\n\n    Parameters: \n    df (obj): Data frame.\n\n    Returns:\n    Dataframe with cleaned tweets.\n\n    \"\"\"\n    # Lowering all the text\n    df.message = df.message.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n    # Removing mentions\n    df.message = df.message.apply(lambda x: re.sub(\"(@[A-Za-z0-9]+)\",\"\",x))\n    # Removing short words\n    df.message = df.message.apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n    # Removing https\/http links\n    df.message = df.message.apply(lambda x: re.sub('http[s]?:\/\/\\S+', '', x))\n    # Removing punctuation, with the exception of hashtags\n    df.message = df.message.str.replace(\"[^a-zA-Z#]\", \" \")\n    # Removing numbers\n    df.message = df.message.apply(lambda x: re.sub('\\d+','',x.lower()))\n    return df","99be62f4":"clean_text(df_train)\ndf_train.head()","4f1106c7":"clean_text(df_test)\ndf_test.head()","f1378c90":"# Load library\nfrom nltk.corpus import stopwords\n\n#download the set of stop words the first time\nimport nltk\nnltk.download('stopwords')\n\nstop = stopwords.words('english')\ndf_train['message'] = df_train['message'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ndf_test['message'] = df_test['message'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n","d48e0eea":"#Code for lemmatize\nimport nltk\n\nnltk.download('wordnet')\n\ndf_train['message'] = df_train['message'].apply(lambda x: \" \".join([Word(word).\nlemmatize() for word in x.split()]))\ndf_train['message']","bf78c263":"df_test['message'] = df_test['message'].apply(lambda x: \" \".join([Word(word).\nlemmatize() for word in x.split()]))\ndf_test['message']","0052c769":"def word_cloud(df,class_no,class_name):\n  \"\"\"\n  This function generates word cloud visualizations across different classes.\n\n  Parameters: \n    df (obj): Data frame.\n    class_no (int): Class number\n    class_name (obj): Class name\n\n   Returns:\n    word cloud visual\n  \"\"\"\n  # create list of words per class\n  sentiment_class = ' '.join([text for text in df['message'][df['sentiment'] == class_no]])\n  #initialize wordcolud\n  from wordcloud import WordCloud\n  wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110,\n                        background_color=\"white\").generate(sentiment_class)\n\n  plt.figure(figsize=(10, 7))\n  plt.imshow(wordcloud, interpolation=\"bilinear\")\n  plt.title('WordCloud for' + \" \" + class_name)\n  plt.axis('off')\n  return plt.show()\n","a60382bb":"word_cloud(df_train,1,'Pro Tweets')","2b32c5f5":"word_cloud(df_train,-1,'Anti Tweets')","f884d82e":"word_cloud(df_train,0,'Neutral Tweets')","70bd34cc":"word_cloud(df_train,1,'News Articles')","1041972e":"def common_words(df, class_no, class_name):\n  \"\"\"\n  This is a function to extract top 20 comon words per class.\n  \n    Parameters: \n    df (obj): Data frame.\n    class_no (int): Class number\n    class_name (obj): Class name\n  \n    Returns: \n    Bar plot for the 20 most used words in the tweets.\n    \"\"\"\n  # create new dataframe with top 20 common words\n  name =[text for text in df['message'][df['sentiment'] == class_no]]\n  series=pd.Series(' '.join(name).split()).value_counts()[:20]\n  new_df=pd.DataFrame(data=series, columns=['count']).reset_index()\n\n  #plot barplot\n  plt.figure(figsize=(10, 7))\n  ax=sns.barplot(x=new_df['count'],y=new_df['index'],data=new_df)\n  plt.xlabel('value counts')\n  plt.ylabel('common words')\n  plt.title('Top 20 Common words for'+ ' '+class_name)\n  return plt.show()","dac8b218":"common_words(df_train,1, 'Pro Tweets')","6d1949ce":"common_words(df_train,-1, 'Anti Tweets')","980dab5e":"common_words(df_train,0, 'Neutral Tweets')","59a7a1ec":"# function to collect hashtags\ndef hashtag_extract(data):\n  \"\"\"\n  Function to extact hashtags.\n\n  Parameter(s):\n    data (obj): a dataframe object\n  \n  Returns:\n  List of hashtags\n  \"\"\"\n  hashtags = []\n    # Loop over the words in the tweet\n  for i in data:\n    ht = re.findall(r\"#(\\w+)\", i)\n    hashtags.append(ht)\n  return hashtags","6fc729c8":"# extracting hashtags from pro climate change tweets\n\nHT_pro = hashtag_extract(df_train['message'][df_train['sentiment'] == 1])\n\n# extracting hashtags from anti climate change tweets\nHT_anti = hashtag_extract(df_train['message'][df_train['sentiment'] == -1])\n\n#extracting hashtags from neutral tweets\nHT_neutral = hashtag_extract(df_train['message'][df_train['sentiment'] == 0])\n# unnesting list\nHT_pro = sum(HT_pro,[])\nHT_anti = sum(HT_anti,[])\nHT_neutral= sum(HT_neutral,[])","56767d77":"def common_tags(class_list, name):\n  \"\"\"\n  Function to plot top 10 common hashtags.\n\n  Returns:\n  Bar plot of common hashtags.\n  \"\"\"\n  a = nltk.FreqDist(class_list)\n  d = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 10 most frequent hashtags    \n  d = d.nlargest(columns=\"Count\", n = 10)\n  plt.figure(figsize=(10,12))\n  ax=sns.barplot(data=d, x='Count',y = 'Hashtag')\n  plt.xlabel('counts')\n  plt.ylabel('Hashtags')\n  plt.title('Top 10 Common Hashtags for'+ ' '+ name)\n  return plt.show()","5bdf960d":"common_tags(HT_pro, 'Pro Climate Change Tweets')","ca451adc":"common_tags(HT_anti, 'Anti Climate Change Tweets')","c6dadfa1":"common_tags(HT_neutral, 'Neutral Climate Change Tweets')","4f0a90f5":"# Removing hashtags\ndf_train.message = df_train.message.apply(lambda x: re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\",\"\",x))","a89cce37":"def map_labels(df):\n  \"\"\"\n  This function finds maps labels onto new column\n\n  Parameters:\n    df (obj) : a dataframe\n\n  Returns:\n  A dataframe object.\n  \"\"\"\n  # tokenize tweets\n  df['split_tokens']= df['message'].apply(lambda x: x.split())\n  # create a labels column on the dataframe\n  Bdict = {-1: 'anti tweet', 1: 'pro tweet', 0:'neutral tweet', 2:'news article'}\n  df['labels'] = df['sentiment'].map(Bdict)\n  return df","fd28cb3c":"# Find politics related tweets, using political hashtags\npolitical_tags = ['maga','trump','parisagreement','imvotingbecause', 'cop']\ndf_pol=df_train[map_labels(df_train).split_tokens.apply(lambda x:any(set(x).intersection(political_tags)))]\n\n# Find social related tweets, using social related hashtags\nsocial_tags = ['beforetheflood','opchemtrails','qanda','amreading', 'actonclimate','husband']\ndf_soc=df_train[map_labels(df_train).split_tokens.apply(lambda x:any(set(x).intersection(social_tags)))]","4e32d5ac":"def plot_pie(df,sentiment_name):\n\n  \"\"\"\n  Function to plot pie chart, show distribution of classes between \n  political and social related tweets.\n\n  Parameters:\n    df (obj): a dataframe\n    sentiment_name : Political or Social relation\n     \n  Returns:\n  A pie chart plot, and a value counts table\n  \"\"\"\n  # Define labels\n  labels = 'pro tweet', 'news article', 'neutral tweet', 'anti tweet'\n  sizes= df['labels'].value_counts()\n  \n  # Plot pie chart\n  plt.figure(figsize=(18,16))\n  fig1, (ax1, ax2) = plt.subplots(1,2)\n  ax1.pie(sizes, labels=labels, autopct='%1.1f%%', pctdistance= 1.5, labeldistance=1.8,\n        shadow=False, startangle=90)\n  plt.title('Tweet Classes with a'+' '+ sentiment_name)\n  \n  # Plot Value counts table\n  table = pd.DataFrame(df['labels'].value_counts())\n  table.reset_index(inplace=True)\n  table.columns=['Class Name','Count']\n\n\n  cell_text = []\n  for row in range(len(table)):\n    cell_text.append(table.iloc[row])\n\n  ax2.table(cellText=cell_text, colLabels=table.columns)\n  plt.axis('off')\n  return plt.show()","4b2b56ca":"plot_pie(df_pol, 'Political View')","7c03119e":"plot_pie(df_soc, 'Social View')","f24332e8":"#function to build a library of words from the cleaned tweets\ndef build_corpus(data):\n    corpus = []\n    for column in ['message']:\n        for sentence in df_train[column].iteritems():\n            word_list = sentence[1].split(\" \")\n            corpus.append(word_list)\n            \n    return corpus\n\ncorpus = build_corpus(df_train)","21d8ee08":"model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=100, workers=4)\nmodel.wv['beforetheflood']","a018a20e":"#function to create TSNE model, and plot the word vectors\ndef tsne_plot(model):\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(16, 16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","dd6eec91":"tsne_plot(model)","119a810d":"words= model.most_similar('beforetheflood')","cc775f24":"df = pd.DataFrame(words, columns =['word', 'Similarity_score']) \n  \nprint(df)","7a0f6bc6":"def get_top_n_words(corpus, n=None):\n  \"\"\"\n  Function to get top n words.\n\n  Parameters:\n    corpus (obj): a library of word vectors\n  \n  Returns:\n  list of frequent words\n  \"\"\"\n  # Vectorize words and count frequency\n  vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n  bag_of_words = vec.transform(corpus)\n  sum_words = bag_of_words.sum(axis=0) \n  words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n  words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n  return words_freq[:n]\n# create dataframe\nfrequent_words = get_top_n_words(df_train['message'][df_train['sentiment'] == 1], 20)\nfor word, freq in frequent_words:\n  df2 = pd.DataFrame(frequent_words, columns = ['text' , 'count'])\n  grouped= df2.groupby('text').sum()['count'].sort_values(ascending=False)\n  df3=pd.DataFrame(data=grouped, columns=['count']).reset_index()\n","18e139ff":"#plot barplot\nplt.figure(figsize=(10, 7))\nax=sns.barplot(x=df3['count'],y=df3['text'],data=df3)\nplt.xlabel('Word count')\nplt.ylabel('Word Pairs')\nplt.title('Top 20 Common word Pairs for Pro Climate Change Tweets')\nplt.show()","c53fefd5":"# Create dataframe\nfrequent_words = get_top_n_words(df_train['message'][df_train['sentiment'] == -1], 20)\nfor word, freq in frequent_words:\n  df2 = pd.DataFrame(frequent_words, columns = ['text' , 'count'])\n  grouped=df2.groupby('text').sum()['count'].sort_values(ascending=False)\n  df4=pd.DataFrame(data=grouped, columns=['count']).reset_index()","05fd6b45":"plt.figure(figsize=(10, 7))\nax=sns.barplot(x=df4['count'],y=df4['text'],data=df4)\nplt.xlabel('Word count')\nplt.ylabel('Word Pairs')\nplt.title('Top 20 Common word Pairs for Anti Climate Change Tweets')\nplt.show()","e9d5b993":"X=df_train['message']\ny= df_train['sentiment']\nunseen_data = df_test['message']","54cde947":"X","88921261":"y","c82fff87":"unseen_data","b909c97b":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size =0.2, random_state=42)\n","c1fcb4a7":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\ntext_clf = Pipeline([('tfidf', TfidfVectorizer(stop_words='english', \n                             min_df=1, \n                             max_df=0.9, \n                             ngram_range=(1, 2))),\n                     ('clf',LogisticRegression()),\n])\n# Feed the training data through the pipeline\ntext_clf.fit(X_train, y_train)  ","180f0dad":"predictions = text_clf.predict(X_test)","13aa2d33":"#from unseen data\ny_pred = text_clf.predict(unseen_data)","615743fb":"from sklearn import metrics \nprint(metrics.confusion_matrix(y_test,predictions))","f982cd7a":"#Print a classification report\nprint(metrics.classification_report(y_test,predictions))","d3b44a8c":"#print the overall accuracy\nprint(metrics.accuracy_score(y_test,predictions))","08859365":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\n\ntext_clf = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('clf', LinearSVC()),\n])\n\n# Feed the training data through the pipeline\ntext_clf.fit(X_train, y_train)  ","b41b862a":"predictions = text_clf.predict(X_test)","6b096058":"#from unseen data\ny_pred = text_clf.predict(unseen_data)","bf5acdf0":"from sklearn import metrics \nprint(metrics.confusion_matrix(y_test,predictions))","d4c5e9a0":"#Print a classification report\nprint(metrics.classification_report(y_test,predictions))","528cf6d6":"#print the overall accuracy\nprint(metrics.accuracy_score(y_test,predictions))","911c7790":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import svm\ntext_clf = Pipeline([('tfidf', TfidfVectorizer(stop_words='english', \n                             min_df=1, \n                             max_df=0.9, \n                             ngram_range=(1, 2))),\n                     ('clf', svm.SVC(decision_function_shape='ovo')),\n])\n# Feed the training data through the pipeline\ntext_clf.fit(X_train, y_train)  ","a3504d78":"predictions = text_clf.predict(X_test)","53bc2b5e":"#from unseen data\ny_pred = text_clf.predict(unseen_data)","f9e68dca":"from sklearn import metrics \nprint(metrics.confusion_matrix(y_test,predictions))","6916ec9d":"#Print a classification report\nprint(metrics.classification_report(y_test,predictions))","bc4df065":"#print the overall accuracy\nprint(metrics.accuracy_score(y_test,predictions))","91c08fa6":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\ntext_clf = Pipeline([('tfidf', TfidfVectorizer(stop_words='english', \n                             min_df=1, \n                             max_df=0.9, \n                             ngram_range=(1, 2))),\n                     ('clf', SVC(kernel='rbf')),\n])\n# Feed the training data through the pipeline\ntext_clf.fit(X_train, y_train) ","d3e141ac":"predictions = text_clf.predict(X_test)","8c81ffaa":"#from unseen data\ny_pred = text_clf.predict(unseen_data)","d62b8606":"from sklearn import metrics \nprint(metrics.confusion_matrix(y_test,predictions))","975a85ea":"#Print a classification report\nprint(metrics.classification_report(y_test,predictions))","d6608300":"#print the overall accuracy\nprint(metrics.accuracy_score(y_test,predictions))","d23a8660":"#Separate minority and majority classes\ndf_majority = df_train[(df_train.sentiment==1) |\n                      (df_train.sentiment==0) | \n                      (df_train.sentiment ==2)]\ndf_minority = df_train[df_train.sentiment == -1]\n\n#Upsample minority class\ndf_minority_upsampled= resample(df_minority,replace= True,\n                            n_samples= 4000, random_state =42) #sample with replacement\n\n#Combine majority class with upsampled minority class\ndf_upsampled = pd.concat ([df_majority,\n                          df_minority_upsampled])\n#Display new class counts\ndf_upsampled.sentiment.value_counts()","552b92d5":"# message  Distribution ove the classes\ndist_class = df_upsampled['sentiment'].value_counts()\nlabels = ['1', '2','0','-1']\n\nfig, (ax1 )= plt.subplots(1, figsize=(12,6))\n\nsns.barplot(x=dist_class.index, y=dist_class, ax=ax1).set_title(\"Tweet message distribution over the sentiments\")","48850c4c":"#Independent Feature of the train dataframe\nX=df_upsampled['message']\n#Dependent feature of the train dataframe\ny= df_upsampled['sentiment']\n#Independent feature of the test dataframe\nunseen_data = df_test['message']","5daf07af":"#Splitting the train dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)","1cd8dce4":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\ntext_clf = Pipeline([('tfidf', TfidfVectorizer(stop_words='english', \n                             min_df=1, \n                             max_df=0.9, \n                             ngram_range=(1, 2))),\n                     ('clf',LogisticRegression()),\n])\n# Feed the training data through the pipeline\ntext_clf.fit(X_train, y_train)  ","1701edc3":"predictions = text_clf.predict(X_test)","1e9f57ca":"#from unseen data\ny_pred = text_clf.predict(unseen_data)","fb3038b2":"from sklearn import metrics \nprint(metrics.confusion_matrix(y_test,predictions))","5be92eb4":"#Print a classification report\nprint(metrics.classification_report(y_test,predictions))","d281a7cd":"#print the overall accuracy\nprint(metrics.accuracy_score(y_test,predictions))","fda3233c":"from sklearn.svm import LinearSVC\n\ntext_clf = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('clf', LinearSVC()),\n])\n\n# Feed the training data through the pipeline\ntext_clf.fit(X_train, y_train)  ","d723143c":"predictions = text_clf.predict(X_test)","f1df4fcd":"#from unseen data\ny_pred = text_clf.predict(unseen_data)","747fd2c9":"from sklearn import metrics \nprint(metrics.confusion_matrix(y_test,predictions))","5efd9401":"#Print a classification report\nprint(metrics.classification_report(y_test,predictions))","cfc7fc09":"#print the overall accuracy\nprint(metrics.accuracy_score(y_test,predictions))","8e414a24":"from sklearn import svm\ntext_clf = Pipeline([('tfidf', TfidfVectorizer(stop_words='english', \n                             min_df=1, \n                             max_df=0.9, \n                             ngram_range=(1, 2))),\n                     ('clf', svm.SVC(decision_function_shape='ovo')),\n])\n# Feed the training data through the pipeline\ntext_clf.fit(X_train, y_train)  ","0599f41c":"predictions = text_clf.predict(X_test)","501fa024":"#from unseen data\ny_pred = text_clf.predict(unseen_data)","8253e018":"from sklearn import metrics \nprint(metrics.confusion_matrix(y_test,predictions))","9166c7c4":"#Print a classification report\nprint(metrics.classification_report(y_test,predictions))","af95c7a2":"#print the overall accuracy\nprint(metrics.accuracy_score(y_test,predictions))","b731dec0":"A=df_test.set_index('tweetid')   \nB= A.index","bed41c2a":"Final_Submission ={'tweetid': B,'sentiment': np.round(y_pred,0)}","a68334a9":"Submission =pd.DataFrame(data=Final_Submission)","63efa7ae":"Submission =Submission[['tweetid','sentiment']]","f36a60e7":"Submission.set_index('tweetid')","844af2ee":"Submission.to_csv('ClimateChange.csv', index=False)","61cdef07":"from sklearn.svm import SVC\ntext_clf = Pipeline([('tfidf', TfidfVectorizer(stop_words='english', \n                             min_df=1, \n                             max_df=0.9, \n                             ngram_range=(1, 2))),\n                     ('clf', SVC(kernel='rbf')),\n])\n# Feed the training data through the pipeline\ntext_clf.fit(X_train, y_train) ","7ca5c906":"predictions = text_clf.predict(X_test)","731faff1":"#from unseen data\ny_pred = text_clf.predict(unseen_data)","018c1d1f":"from sklearn import metrics \nprint(metrics.confusion_matrix(y_test,predictions))","0bbb837c":"#Print a classification report\nprint(metrics.classification_report(y_test,predictions))","10d9abcf":"#print the overall accuracy\nprint(metrics.accuracy_score(y_test,predictions))","1db0fe75":"#experiment.display()","0a1f2d87":"#### 3.1 Extracting info from the datasets","2f0e1cd3":"#### 7.4.4 SVC","adba5b10":"* **TF-IDF vectorizer** converts raw texts to a matrix of TD-IDF features that could be understood by a computational language\n\n* **ngram_range** is the number of words in a sequence\n\n* **max_df** Removing the data values that appearing most frequently\n\n* **min_df**  Removing terms that are appearing less frequently\n\n\n","edd2306f":"Lemmatization is a process of extrating a root word by considering the vocabulary. Lemmatization was used instead of stemming because it gives better results ","1197c9ac":"## 1. Introduction","7010cf58":"* Investigating top 20 most common words didn't paint a clear picture since we saw quite a significant overlap in the common words among the different classes.\n* We then opted to investigate pairs of common words using N-grams. While pairs of common words do make a bit more sense as compared to investigating single common words, it is not enough to deduce whether it is in line with believing or disbelief in climate change.\n\n* We can then conclude that investigating common words or pairs of common words may not be enough to solely build a model on. In light of this, we then went on to train our model on the whole word corpus as we had seen with the word2vec visualization that indeed our word library can provide context into whether a tweet with certain words is related to another positively or negatively.\n\n","764ca483":"## Working Environment Preparation","32091b15":"Comet provides a self-hosted and cloud-based meta machine learning platform allowing data scientists and teams to track, compare, explain and optimize experiments and models.","b8208698":"#### 6.1 Defining Variables","a389b874":"**1.1 Background**","cc47c1c7":"**Observations**","03596710":"**Observations for Unbalanced data**","5d69c487":"Climate change is refered to as the global phenomenon of climate transformation characterized by the changes in the usual climate of the planet . Such Changes could be manifested in form of change in precipitation , temperature and wind magnitudes. These changes are mostly said to be a result of antropogenic activities, and they have brought nothing but misery to both human and natural environment, for instance lot of natural habitant has been destructed from its normal form, wherelse, human complain about rising temperature and cooling of temperatures in other form of the globe.\n\nDue to the misfortune brought by climate change, both governments and comapnies are working together as a collective to try and minimize any activities that can worsen the level of climate change. Hence lot of environmental sustaonability projects are introduced. In this project, we will be evaluating tweets using Classification Machine learning model.","5ddd118b":"**1.2 Problem Statement**","34e32193":"##### 7.4. 3.SVM","3629a02b":"Relevant procedures were followed before machine models were built, and all these procedure have helped in having the best predictive model. The procedures, followed includes data cleaning and data preprocessing which are essential in making sure that the relevant information is conserved without unnecessary noises which could lead to unrealiable results. \n\nHaving perfomed both data cleaning and pre-processing have lead us to a conclusion that  the Logistic regression model has perfomed best when the unbalance data was used,and the accuracy as well as the F1 score of the other models were close to the best performing model.\n\nFuthermore, the data was the balanced using the upsampling method, the results changed slightly from what was obtained from the unalanced data. However the best model in this case was the Support Vector Classifer, that was ran on an rbf kernel. and the worst perfoming model was the logistic regression model.But the difference was nothing beyond 0.2 accuracy.  We can conclude that the upsampled data gave the best results compared to the unbalanced data set.\n\n**RECOMMEDATION**\nGiven more time,the hyperparameter and the grid search would have been used and possibly we could have gotten better results.\n","a7dbcb45":"## 9. References","2d507d7e":"###### 7.4 Splitting the dataset","247fc50a":"##### 6.3 Brief explanation of parameters and the TF-IDF vectorizer","2544b8e7":"##### 7.2  Distribution of classes after Upsampling ","821ea9b4":"**1.4 Deliveries**","2f314627":"#### 5.4 Text Feature Extraction","344331ba":"##### check for null values, blanks and distribution of classes","a9db67f8":"Wordcloud is the pictorial representation of the most frequently repeated words representing the size of the word,(i.e, the bigger the word, means the Word appeared the most )","7c6f4808":"## Install Packages","28e9bb67":"Many companies are built around lessening one\u2019s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product\/service may be received.","b97df588":"# Team 4 Climate Change Belief Analysis\n\n\n\n","b5f99c8f":"## 3. Importing Datasets","4b541961":"###### 5.4.1 Word2Vec","3db387ec":"The aim of this project is to find out if people believe in climate change or not . And this will be done by view people's previous sentiments when it comes to Climate Change. This will give an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic catergories. Thus increasing their insights and informing future marketing strategies.","4daa659e":"**Bi-Grams**","355be2e2":"* Though climate change still is the most tweeted hashtag,extracting hashtags does show a difference\/distinction between the tweet classes. Pro climate change tweets have a high number of 'before the flood' hashtags which is for a documentary that is about teaching about the effects of global warming. This further confirms our theory that there is a social aspect to how people perceive and formulate their opinions around climate change.\n\n* Furthermore, we see even more politically inclined words among the common hashtags. Anti climate change tweets are frequented by the 'maga' hashtags, which stands for Make America Great Again, a slogan used by President Donald Trump in his election campaign. President Trump has been quoted saying that climate change is a hoax. We can now confirm that politics does play a big role also in the perception of climate change and matters related to it.\n\n* But to what extent does the social aspect and the political aspect play a role in forming opinions on twitter? Are people most influenced by political developments or views to tweet about climate either for or against it? Or are people more influenced by documentaries, tv shows, family, friends, social media? \nWe explore this further below.\n\n","49400a51":"## 7. Building a model with balanced data","99d78a93":"## 4. Data Pre-processing","e93c2f4e":"\n\n*   Kaggle.com. 2020. Visualizing Word Vectors With T-SNE. [online] Available at: <https:\/\/www.kaggle.com\/jeffd23\/visualizing-word-vectors-with-t-sne> [Accessed 27 June 2020].\n*   KDnuggets. 2020. A Complete Exploratory Data Analysis And Visualization For Text Data: Combine Visualization And NLP To Generate Insights - Kdnuggets. [online] Available at: <https:\/\/www.kdnuggets.com\/2019\/05\/complete-exploratory-data-analysis-visualization-text-data.html> [Accessed 27 June 2020].\n* Kulkarni A., Shivananda A. 2019. Deep Learning for NLP. In: Natural Language Processing Recipes. Apress, Berkeley, CA.\n* Medium daily digest. 2020.Data Preprocessing steps in Python for any Machine Learning Algorithm. [online] Available at:<https:\/\/towardsdatascience.com\/> .[Accessed 29 June 2020].\n","a8eb1e4f":"##### 7.4.1 Logistic Regression Model (Upsampled data)","cb3838a9":"## 8. Conclusion","2f62e026":"##### 7.3 Defining the upsampled variables ","6f2d906a":"#### 4.1 Cleaning Tweets ","7f2c3dd2":"* Four different models were trained, namely the logistic regression, SVC linear  and SVM.\n* All these models were fitted into a pipe line with a TfidfVectorizer, and they are behaved differently looking at the accuracy score that they have given us.\n* Based on the accuracy score that was obtained, the model that perfomed best is the logistics regression classifier with an accuracy of 0.783.\n","431dfdc1":"**Check distribution of classes**","65fe1f55":"##### 6.4.3.SVM\n* Support Vector Machine (SVM) is a classification method that was developed in the computer scince community in the 1990s. \n* The SVM have shown its capabilities of performing best in different settings and in most times it is considered to be one of the best classifiers. \n* The objective of the SVM in this case is to find the a hyperplane in an N-dimensional space(N- the number of features) that distinctly classifies the data points.The ovo and rbf kernel was used , because it allows the SVC to fit a non-linear decision boundary ","9f6ae410":"## 2. Importing Libraries ","164fc12c":"#### 5.1 Wordcloud Visualization(s)","f45d9671":"## Contents ","95aefc21":"## 5. Exploratory Data Analysis","8db9bcec":"\n*   There are 15 819 tweets, all tweets have a unique tweet ID.\n*   The sentiment and tweet id columns are of int64 data type and the message column is an object datatype.\n*   No null values and no blanks exist within the train dataset.\n*   The classes are imbalanced, with the tweeters believing in climate change (pro tweets) having the most count (53% of our data), and the tweeters who do not believe in climate change (anti tweets) having the least count.\n*   The imbalance of classes may train our model to only be good at detecting only the pro tweets, therefore having an inaccurate accuracy.\n*   Due to the imbalancing of classes we are going to have to either downsample or upsample our minority or majority classes or deploy a model that can work to counteract the imbalancing of classes.","e4153e54":"* The Linear SVC which is also regarded as the soft margin classifier is know for separating hyperplanes that will necessarily give a perfect classification of the training observations, however this could lead to sentivity to individual observations. \n* The reason behind , why this classifier is used is because it is more flexiable in the choise of penalities and loss functions and it scales large samples better.","5f5b4359":"*   Train dataset - Training the model will be done on this dataset.\n*   Test dataset - The model will be tested on this dataset ","b35e2e5f":"Pre-processing is the most vital step in classification modelling ,as we all know that at least 90% of the text data of the world data is unstructured, for instance Text can come in a variety of forms from a list of individual words, to sentences to multiple paragraphs with spacial characters.All in all this data is never clean and consists of a lot of noise, therefore,it needs to be treated and then perfomr a few of the pre-processing functions to make sure we have the right input data for the feature enginerring and modeling. Preprocessing involves transforming raw text data into an understable format and this process has proven to have resolved such problems.","bb67a39e":"We have seen that hashtags provide more insights into whether a tweet is pro or anti climate change. Next, we want to explore this further by using word2vec to investigate words that go together with the most frequent hashtags and the relatedness thereof, in terms of the words being negative or positive in this climate change belief context.\n\n**word2vec**\nWord2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located close to one another in the space.\n\n**TSNE**\nTSNE Works by taking a group of high-dimensional (# of dimensions via Word2Vec) vocabulary word feature vectors, then compresses them down to 2-dimensional x,y coordinate pairs. In order to group similar words (as per corpus model) located close to one another on the plane, while maximizing the distance between dissimilar words as per trained model corpus.","6abb7275":"## Connecting to Comet","7cb42e7e":"* Investigating individual words still shows that there is an overlap of most used words between the classes. \n* However, it does become very apparent that there are themes that help formulate or form tweeters opinions on twitter. Seeing words such as Trump, Obama would lead one to believe that there is a political connection to what people tweet about climate change. We can also see the word 'husband' appearing as most common under the pro tweets, this shows that the climate change topic is being discussed amongst families as well, or that people do think about climate change in relation to people close to them. \n* We can then also assume that there is perhaps a social aspect to how people form their opinion on climate change. We will investigate this observation further by investigating the most common hashtags. Hashtags will provide more context, as people will most usually tweet under a certain hashtag as a means of making it easier to find information with a theme or specific context.\n","9e3515eb":"##### 7.4.2.Linear SVC","329a8c32":"**Observation**\n\n* Climate change and global warming seem to be the most common words among the tweets, there also seems to an overlap of common words among the tweet classes. \n* However, due to word cloud not showing how frequent a word is used and in what context, not much distinct insights can be drawn from the word cloud visualizations. \n* We did, however, notice that some of the most commonly used words are of a political context. We will investigate this further below.\n","5932fb78":"#### 4.2 Removing stopwords","f6481ef8":"Exploratory Data Analysis is a way of visualizing, summarizing, and interpreting the information that is not obvious or hidden in rows and columns format. The EDA is one of the most essential steps whilst building a model, as it gives insights and statistical measure that is essentila for whatever one is trying to achieve. In this case, this process will help in terms of coming with concepts that will help business in adopting a business strategy that will allow them to be more thoughtful when it comes to sustainable development. And also having a clear understanding with regards to how people think about climate change.","9dbaabb4":"## Displaying the experiment","60b932ce":"This notebook was designed using the packages below. If you do not have the packages already installed in your environment, to install, un-comment the cell below and run.","cccdf22e":"##### 6.4. 1.Logistic Regression \n* Logistic regression is a powerful machine learning algorithm that utilizes a sigmoid function and works best on binary and it can also be performed in multi class function.\n* In this case the Logistic Regression classifer is  used in a multi-class problem. The Logistic Regression classifier is used within a pipeline with different parameters.","c77ddce5":"\n * From the tsne model depiction, we can see that the word2vec model has succesfully grouped closer together words with the same context, before the flood is a documentary presented by Leonardo DiCaprio which aims to educate about climate change and the effects of global warming. Similar words to before the flood are words such as take, action, cop (a world leaders conference to address climate change).\n\n* We can also see that words such as hoax, whic is a term a non believer would use are much farther from the word before the flood. However, the tsne model can be difficult to read as the word vectors become clumped together. We then created a similarity score dataframe to show the similarity scores of the related words.\n\n* This shows that the words contained within the tweets we are going to use to train our model really do have a pattern amongst themselves which is either synonymous with believing or a disbelief in climate change.","6c5b7226":"#### 5.3 Extracting hashtags","32b2f52a":"**Check for blanks**","ee5b3e68":"\nIn this stage, the data was lowered inorder to have all the words in the same format and maintain consistency. This method will be applied to both the train and test dataset \n\nRemoving Punctuation will be done, to make sure there are no added extra information or values. The removal of puncuation will help reduce the size of the data and increase computational eeficiency. This method will be applied to both the train and the test dataset .","92496a2a":"##### 6.4. 4 svc \n* The Linear SVC which is also regarded as the soft margin classifier is know for separating hyperplanes that will necessarily give a perfect classification of the training observations, however this could lead to sentivity to individual observations. \n* The reason behind , why this classifier is used is because it is more flexiable in the choise of penalities and loss functions and it scales large samples better. \n* The rbf kernel which allows the SVC to fit a non-linear decision boundary.","69b8e363":"#### 5.3  Political vs Social Aspect ","8954c2c6":"* From the plots above we can see that from a political context, there are more pro tweets and news article tweets. This is to be expected as news outlets are most likely to tweet of stories relating to the political climate of the country, in the context of climate change than they are likely to tweet about movies on climate change. Whether the sentiment of the tweets is in agreement with the political context remains unseen. \n\n* However, from a social standpoint we can see that there are a whole lot more pro tweets. This is also an expected observation as humans are more likely to relate from a 'human\/social' aspect. For instance if Leonardo Di Caprio is one of my favourite actors, I am most likely to watch his documentary and take the message being relayed seriously. \n\n* The question now becomes, what do we do with this information. In the next section we  explore the library of words from the tweets and we explore if we can see a connection in how the words are spatially arranged in a vector space.","7b2d1e04":"## 6 . Building the model Via  pipeline (Unbalanced Data)","25a8f21c":"Before the project could begin , the environment was prepared which means all the  packages that one might needed for statstical evaluation and model building Packages  were installed  and the libraries were also imported. The model was also deployed to comet_ml, which will be furthered explained\/elaborated when approached.","d7bd7912":"As it was stated in one of the previous sections that the data is unbalanced and there is also the possiblity that it could affect the quality of the results, it was then decided that the process of of building the data will be perfomed with the balanced data to solve that unbalanced issue.\n\nInorder to solve that problem, the method of resampling the data was then implemented, inorder to have a better a well informed decision.\n\nResampling method is an indispensable recipe in modern statistics, this recipe includees repeatedly drawing samples from training sets and refitting the model of the fitted model.In this instance  the upscaling the minority class method will be implemented, to make sure that the Anti-climate change tweets match up with the Pro-climate change.\n\nUpscaling method is the process where the minority classes (Anti) is repetedly randmonly sampled  until the number of minority class is matching the number of majority class.","3969d6e8":"###### 6.2  Splitting the dataset","7706cc11":"##### 5.4.2  N-Grams","cfd41e17":"Stopwords are common in most texts and in most cases they dont carry any meaning or they have less meaning compared to the other keywords. For instance , if these words are removed, more focus will be on impontant keywords.","ef4b8692":"The first step that was taken into consideration was to study the data and have a full understanding, before anything could be done with the data. Extraction of the infomation from the datasets includes looking for missing data, ( Which is  the first step that will be taken) and secondly we we had to look if there are any blank spaces or empty strings within the data.Then we learn more about the distribution of the data, where the dependent variable is divided into 4 classes(pro_tweets, anti_tweets, neutral_tweets and news).","ebb34f4a":"1. Introduction\n2. Import Libraries \n3. Import Datasets\n4. Data cleaning and preprocessing\n5. Exploratory data analysis\n6. Model building unbalanced data \n7. Model Building balanced data\n8. Conclusion and Recommedation\n9. Reference","6c8d47f5":"####  5.2 Top 20 Common words","0d8c1a5f":" **1.3 Objectives**","0dae682b":"#### 6.4. 2.Linear SVC","b57153b1":"#### 4.3 Lemmetization","98623bcc":"Building a Machine learning model that is able to classify whether or not a person believes in climate change , based on their novel tweet data"}}