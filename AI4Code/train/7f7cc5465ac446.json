{"cell_type":{"3acd07c5":"code","495f74ca":"code","c97d1bbb":"code","400e5a1e":"code","df54b247":"code","7be4c3c5":"code","a76156fb":"code","eb1a6923":"code","6167f98c":"code","a4c5df4c":"code","5a975d01":"code","139cc5d0":"code","f9c7334d":"code","a070219d":"code","af3174c4":"code","99f22682":"code","c78c8b5c":"code","49b154b6":"code","cdeb6b4f":"code","dde0253c":"code","1e288a3f":"code","0857727a":"code","940925d9":"code","e3602a40":"code","c36add94":"code","8fbc1f28":"code","8fb349c3":"code","6ef53fae":"code","5d6b0a9b":"code","79e963fb":"code","67ff2430":"code","5f1f6c06":"code","42eaa875":"code","9b9d1819":"code","6bb8ee32":"code","544d6539":"code","5fb503fa":"code","8cac01ea":"code","827f0d42":"code","36e4780d":"code","d24ee149":"code","c489c71d":"code","be295a84":"code","6dc31612":"code","72ba5fac":"code","6da5ee06":"code","345d2f1c":"code","93d5c766":"code","9ca79d8e":"code","907855e7":"code","96377a8f":"code","0779b5c4":"code","b79583d1":"code","ddf03992":"markdown","803bceb7":"markdown","feebcc9e":"markdown","ad5ce6e2":"markdown","423fb277":"markdown","d3e9d2d3":"markdown","222c674b":"markdown","58feed2a":"markdown","c77d7686":"markdown","c4279448":"markdown","ed5d17eb":"markdown","1b4ba564":"markdown","71e9a1c3":"markdown","74f20266":"markdown","0e808d88":"markdown","7278227f":"markdown","c101d2ae":"markdown","1760b3ec":"markdown","065bd2ef":"markdown","d6daebfb":"markdown","876a0ac4":"markdown","aabd9d8d":"markdown","e1e7ae92":"markdown","f48f66dc":"markdown","eb70e969":"markdown","f0c4e557":"markdown","38a60163":"markdown","1ff74c72":"markdown","104d678e":"markdown","b0d0f1bb":"markdown","fd1fc6f4":"markdown","05b97ccf":"markdown","55df7261":"markdown","612087b2":"markdown","9bd9a89c":"markdown","71c4cc54":"markdown","73b0a831":"markdown","493761fa":"markdown"},"source":{"3acd07c5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom wordcloud import WordCloud","495f74ca":"path = '\/kaggle\/input\/titanic\/'\n\ntrain = pd.read_csv(path + 'train.csv', sep=\",\")\ntest = pd.read_csv(path + \"test.csv\", sep=\",\")\ntest_sub = test.copy()","c97d1bbb":"print('Shape:', train.shape)\ntrain.info()","400e5a1e":"print('Shape:', test.shape)\ntest.info()","df54b247":"train['Survived'].value_counts()","7be4c3c5":"class Plot_class():\n\n    def __init__(self,feature, my_dataframe, my_table):\n        self.feature = feature\n        self.my_dataframe = my_dataframe\n        \n        \n    def plot_bar(feature, my_dataframe):\n        fig, axes = plt.subplots(nrows=1,ncols=1, figsize=(12,4))\n        my_dataframe.groupby([feature,'Survived'])[feature].count().unstack().plot(kind='bar',stacked=True, ax=axes)\n        plt.title('Frequency of {} feature vs  survived (target)'.format(feature))\n\n    def plot_bar_table(feature, my_dataframe, my_table):\n        fig = plt.figure()\n        # definitions for the axes\n        left, width = 0.10, 1.5\n        bottom, height = 0.1, .8\n        bottom_h = left_h = left + width + 0.02\n\n        rect_cones = [left, bottom, width, height]\n        rect_box = [left_h, bottom, 0.17, height]\n        \n        # plot\n        ax1 = plt.axes(rect_cones)\n        my_dataframe.groupby([feature,'Survived'])[feature].count().unstack().plot(kind='bar',stacked=True, ax=ax1)\n        plt.title('Frequency of {} feature vs  survived (target)'.format(feature))\n        \n        # Table\n        ax2 = plt.axes(rect_box)\n        my_table = ax2.table(cellText = table_data, loc ='right')\n        my_table.set_fontsize(40)\n        my_table.scale(4,4)\n        ax2.axis('off')\n        plt.show()\n    \n    def distri(feature, my_dataframe):\n        fig, axes = plt.subplots(nrows=1,ncols=1, figsize=(12,5))\n        ax = sns.distplot(train.loc[(train['Survived'] == 0),feature].dropna(),color='orange',bins=40)\n        ax = sns.distplot(train.loc[(train['Survived'] == 1),feature].dropna(),color='blue',bins=40)\n        plt.legend(labels=['not survived','survived'])\n        plt.title('{} Distribution Survived vs Non Survived'.format(feature))\n        plt.ylabel(\"Frequency of Passenger Survived\")\n        plt.xlabel(feature)\n        plt.show()\n   ","a76156fb":"#values\nTotal = train.Survived.count()\nFemale = train[train['Sex'] == 'female'].Survived.count()\nMale = train[train['Sex'] == 'male'].Survived.count()\nP_Female = round(Female \/ Total,2)\nP_Male = round(Male \/ Total,2)\nP_Female_and_Survived = round(train[((train['Sex']=='female') & (train['Survived']==1))].Survived.count() \/ Total, 2)\nP_Male_and_Survived = round(train[((train['Sex']=='male') & (train['Survived']==1))].Survived.count() \/ Total, 2)\nP_Survived = round(train[train['Survived'] == 1].Survived.count()\/Total, 2)\nP_Survived_Female = round(P_Female_and_Survived \/ P_Female,2) #P(survived|female)\nP_Survived_Male = round(P_Male_and_Survived \/ P_Male, 2) #P(survived|male)\nP_Female_Survived = round(P_Female_and_Survived \/ P_Survived,2) #P(Female | Survived)\nP_Male_Survived = round(P_Male_and_Survived \/ P_Survived,2) #P(Male | Survived)\n\ntable_data=[\n    [\"P(Female)\", P_Female],\n    [\"P(Male)\", P_Male],\n    [\"P(Survived | Female)\", P_Survived_Female ],\n    [\"P(Survived | Male)\", P_Survived_Male ],\n    [\"P(Female | Survived)\", P_Female_Survived],\n    [\"P(Male | Survived)\", P_Male_Survived ]\n]","eb1a6923":"Plot_class.plot_bar_table('Sex',train,table_data)","6167f98c":"#values\nP_First = round(train[train['Pclass'] == 1].Survived.count() \/ Total, 2)\nP_Middle = round(train[train['Pclass'] == 2].Survived.count() \/ Total, 2)\nP_Third = round(train[train['Pclass'] == 3].Survived.count() \/ Total, 2)\nP_First_and_Survived = round(train[((train['Pclass']==1) & (train['Survived']==1))].Survived.count() \/ Total, 2)#P(first and survived)\nP_Middle_and_Survived = round(train[((train['Pclass']==2) & (train['Survived']==1))].Survived.count() \/ Total, 2)#P(middle and survived)\nP_Third_and_Survived = round(train[((train['Pclass']==3) & (train['Survived']==1))].Survived.count() \/ Total, 2)#P(third and survived)\nP_First_Survived = round(P_First_and_Survived \/ P_Survived, 2) #P(first | survived)\nP_Middle_Survived = round(P_Middle_and_Survived \/ P_Survived, 2) #P(middle | survived)\nP_Third_Survived = round(P_Third_and_Survived \/ P_Survived, 2) #P(first | survived)\nP_Survived_First = round(P_First_and_Survived \/ P_First, 2) #P(Survived | First)\nP_Survived_Middle = round(P_First_and_Survived \/ P_Middle, 2) #P(survived | middle)\nP_Survived_Third = round(P_First_and_Survived \/ P_Third, 2) #P(survived | third)\n\ntable_data = [\n    [\"P(First)\", P_First],\n    [\"P(Middle)\", P_Middle],\n    [\"P(Third)\", P_Third],\n    [\"P(First | Survived)\", P_First_Survived],\n    [\"P(Middle | Survived)\", P_Middle_Survived],\n    [\"P(Third | Survived)\", P_Third_Survived],\n    [\"P(Survived | First)\", P_Survived_First],\n    [\"P(Survived | Middle)\", P_Survived_Middle],\n    [\"P(Survived | Third)\", P_Survived_Third]\n]","a4c5df4c":"Plot_class.plot_bar_table('Pclass', train, table_data)","5a975d01":"#values\nP_C = round(train[train['Embarked'] == 'C'].Survived.count() \/ Total, 2)\nP_Q = round(train[train['Embarked'] == 'Q'].Survived.count() \/ Total, 2)\nP_S = round(train[train['Embarked'] == 'S'].Survived.count() \/ Total, 2)\nP_C_and_Survived = round(train[((train['Embarked']=='C') & (train['Survived']==1))].Survived.count() \/ Total, 2)#P(first and survived)\nP_Q_and_Survived = round(train[((train['Embarked']=='Q') & (train['Survived']==1))].Survived.count() \/ Total, 2)#P(middle and survived)\nP_S_and_Survived = round(train[((train['Embarked']=='S') & (train['Survived']==1))].Survived.count() \/ Total, 2)#P(third and survived)\nP_C_Survived = round(P_C_and_Survived \/ P_Survived, 2) #P(first | survived)\nP_Q_Survived = round(P_Q_and_Survived \/ P_Survived, 2) #P(middle | survived)\nP_S_Survived = round(P_S_and_Survived \/ P_Survived, 2) #P(first | survived)\nP_Survived_C = round(P_C_and_Survived \/ P_C, 2) #P(Survived | First)\nP_Survived_Q = round(P_Q_and_Survived \/ P_Q, 2) #P(survived | middle)\nP_Survived_S = round(P_S_and_Survived \/ P_S, 2) #P(survived | third)\n\ntable_data = [\n    [\"P(C)\", P_C],\n    [\"P(Q)\", P_Q],\n    [\"P(S)\", P_S],\n    [\"P(C | Survived)\", P_C_Survived],\n    [\"P(Q | Survived)\", P_Q_Survived],\n    [\"P(S | Survived)\", P_S_Survived],\n    [\"P(Survived | C)\", P_Survived_C],\n    [\"P(Survived | Q)\", P_Survived_Q],\n    [\"P(Survived | S)\", P_Survived_S]\n]","139cc5d0":"Plot_class.plot_bar_table('Embarked', train, table_data)","f9c7334d":"train['Cabin_'] = train['Cabin'].astype(str).str[0]\ntrain['Cabin_'] = train['Cabin_'].replace({'n':'No_value'})\nPlot_class.plot_bar('Cabin_', train)","a070219d":"Plot_class.plot_bar('SibSp', train)","af3174c4":"Plot_class.plot_bar('Parch',train)","99f22682":"train['Title'] = train['Name'].str.replace('(.*, )|(\\..*)',\"\")\nPlot_class.plot_bar('Title', train)\n# although test is not plotting we create the featue\ntest['Title'] = test['Name'].str.replace('(.*, )|(\\..*)',\"\")","c78c8b5c":"Plot_class.distri('Fare',train)","49b154b6":"Plot_class.distri('Age',train)","cdeb6b4f":"#f, ax = plt.subplots(figsize=(10, 8))\n#Firstly, sex feature change numerical 0 for male and 1 for female\ntrain_corr = train.replace({'Sex':{'male': 0, 'female':1}})\ntrain_corr = train_corr.replace({'Embarked':{'C': 0, 'Q': 1 ,'S':2}})\ncorr=train_corr[['Survived','Sex', 'Pclass','Embarked','Age', 'SibSp', 'Parch', 'Fare']].corr()\n#train_corr.corr()\ncorr.style.background_gradient().set_precision(2)","dde0253c":"d = {'color': ['orange', 'b']}\ng = sns.FacetGrid(train, col='Embarked')\ng.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ng.add_legend()","1e288a3f":"d = {'color': ['orange', 'b']}\ng = sns.FacetGrid(train, row='Sex', col='Survived', hue_kws=d, hue='Survived')\ng.map(plt.hist, 'Age')\ng.add_legend()","0857727a":"d = {'color': ['orange', 'b']}\ng = sns.FacetGrid(train, row='Sex', col='Survived', hue_kws=d, hue='Survived')\ng.map(plt.hist, 'Fare', bins=20)\ng.add_legend()","940925d9":"# Filling empty and NaNs values with NaN\ntrain = train.fillna(np.nan)\n# Checking for Null values\ntrain.isnull().sum()","e3602a40":"# Filling empty and NaNs values with NaN\ntest = test.fillna(np.nan)\n# Checking for Null values\ntest.isnull().sum()","c36add94":"train['Embarked'].fillna(train.Embarked.mode()[0], inplace = True)","8fbc1f28":"test['Fare'].fillna(test.Fare.mean(), inplace = True)","8fb349c3":"train = train.drop(['Cabin','Cabin_'], axis=1)\ntest = test.drop(['Cabin'], axis=1)","6ef53fae":"test.Title.unique()","5d6b0a9b":"def transform_title(dataset):\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n                        'Don', 'Major', 'Rev', 'Sir', 'Jonkheer','the Countess', 'Dona'], 'Other')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","79e963fb":"transform_title(train)\ntransform_title(test)","67ff2430":"fig, axes = plt.subplots(nrows=2,ncols=3, figsize=(12,4))\nsns.boxplot(data=train, x = 'Title', y = 'Age', ax=axes[0,0])\nsns.boxplot(data=train, x = 'SibSp', y = 'Age', ax=axes[0,1])\nsns.boxplot(data=train, x = 'Pclass', y = 'Age', ax=axes[0,2])\nsns.boxplot(data=test, x = 'Title', y = 'Age', ax=axes[1,0])\nsns.boxplot(data=test, x = 'SibSp', y = 'Age', ax=axes[1,1])\nsns.boxplot(data=test, x = 'Pclass', y = 'Age', ax=axes[1,2])\nfig.suptitle('Boxplot before filling missing values')\nplt.show()","5f1f6c06":"def filling_Age(dataset):\n    dataset_aux = dataset.dropna()\n    dataset_aux = dataset_aux.reset_index(drop=True)\n    dataset_aux = dataset_aux.groupby(['Title','Pclass','SibSp'])['Age'].apply(lambda g: g.mean(skipna=True)).to_frame()\n    \n    aux = []\n\n    for idx,row in dataset.iterrows():\n        if row.isnull().sum() == 0:\n            aux.append(dataset.loc[idx]['Age'])\n        else:\n            val_1 = dataset.loc[idx]['Title']\n            val_2 = dataset.loc[idx]['Pclass']\n            val_3 = dataset.loc[idx]['SibSp']\n            if (val_1, val_2, val_3) in list(dataset_aux.index):\n                val_sus = dataset_aux.loc[val_1, val_2, val_3][0]\n                aux.append(val_sus)\n            else:\n                aux.append(dataset.Age.mean())\n    \n    dataset['Age']=aux","42eaa875":"filling_Age(train)\nfilling_Age(test)","9b9d1819":"fig, axes = plt.subplots(nrows=2,ncols=3, figsize=(12,4))\nplt.subplots_adjust(hspace = 0.8)\n\n\nsns.boxplot(data=train, x = 'Title', y = 'Age', ax=axes[0,0])\nsns.boxplot(data=train, x = 'SibSp', y = 'Age', ax=axes[0,1])\nsns.boxplot(data=train, x = 'Pclass', y = 'Age', ax=axes[0,2])\nsns.boxplot(data=test, x = 'Title', y = 'Age', ax=axes[1,0])\nsns.boxplot(data=test, x = 'SibSp', y = 'Age', ax=axes[1,1])\nsns.boxplot(data=test, x = 'Pclass', y = 'Age', ax=axes[1,2])\nfig.suptitle('Boxplot after filling missing values', fontsize=14)\n\nplt.show()","6bb8ee32":"train = train.drop(['PassengerId', 'Name', 'Ticket'], axis=1)\ntest = test.drop(['PassengerId', 'Name', 'Ticket'], axis=1)","544d6539":"train['Sex'] = train['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\ntest['Sex'] = test['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","5fb503fa":"train['Embarked'] = train['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2 } ).astype(int)\ntest['Embarked'] = test['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2 } ).astype(int)","8cac01ea":"titles_numerics={\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Other\":6}\ntrain['Title'] = train['Title'].map(titles_numerics).astype(int)\ntest['Title'] = test['Title'].map(titles_numerics).astype(int)","827f0d42":"def Ages_to_category(dataset):\n    dataset_aux = dataset.copy()\n    bins = [0, 3, 16, 25, 45, 60, dataset_aux.Age.max()]\n    labels = ['Baby', 'Child', 'Young', 'Adult', 'Older Adult','Senior']\n    dataset_aux['Age'] = pd.cut(dataset_aux['Age'], bins, labels = labels)\n    Ages_numerics = {\"Baby\": 1, \"Child\": 2, \"Young\": 3, \"Adult\": 4, \"Older Adult\": 5, \"Senior\":6}\n    dataset_aux['Age'] = dataset_aux['Age'].map(Ages_numerics).astype(int)\n    return(dataset_aux)\n\ndef Fare_to_category(dataset):\n    dataset_aux = dataset.copy()\n    bins = [0, 50, 100, 150 , 200, 250, dataset_aux.Fare.max()]\n    labels = [1, 2, 3, 4, 5, 6]\n    dataset_aux['Fare'] = pd.cut(dataset_aux['Fare'], bins, labels = labels)\n    dataset_aux['Fare'] = dataset_aux['Fare'].astype(int)\n    return(dataset_aux)\n  ","36e4780d":"train['FamilySize'] = train.SibSp + train.Parch + 1\ntest['FamilySize'] = test.SibSp + test.Parch + 1\n\ntrain = train.drop(['SibSp', 'Parch'], axis=1)\ntest = test.drop(['SibSp', 'Parch'], axis=1)","d24ee149":"######Prueba como funciona el modelo\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\n#making the dummy varaible of catagorical data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics","c489c71d":"class Models:\n    def __init__(self,dataset, Model_name, X_train, X_test, y_train, y_test):\n        self.dataset = dataset\n        self.Model_name = Model_name\n        self.X_train = X_train\n        self.X_test = X_test\n        self.y_train = y_train\n        self.y_test = y_test\n       \n    def preparing_data(dataset):\n        X = dataset.drop(\"Survived\", axis=1)\n        y = dataset[\"Survived\"]\n        X = preprocessing.StandardScaler().fit(X).transform(X)\n        X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=5)\n        return(X_train, X_test, y_train, y_test)\n    \n    def one_hot_encoding(dataset):\n        #OneHotEncoder\n        OH_encoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n        object_cols  = ['Pclass', 'Embarked', 'Title', 'FamilySize']\n        OH_cols_dataset = pd.DataFrame(OH_encoder.fit_transform(dataset[object_cols]))\n        #Remove categorical columns(will replace with one hot encoding)\n        dataset = dataset.drop(object_cols, axis = 1)\n        dataset = pd.concat([dataset, OH_cols_dataset], axis=1)\n        return(dataset)\n    \n    def fitting_Model(Model_name, X_train, y_train):\n        if Model_name == DecisionTreeClassifier:\n            decision_tree = DecisionTreeClassifier(max_depth=4)\n            return(decision_tree.fit(X_train, y_train))\n        elif Model_name == KNeighborsClassifier:\n            knn = KNeighborsClassifier(n_neighbors = 4)\n            return(knn.fit(X_train, y_train))\n        elif Model_name == RandomForestClassifier:\n            random_forest = RandomForestClassifier(n_estimators=100, max_depth=4)\n            return(random_forest.fit(X_train, y_train))\n        elif Model_name == LogisticRegression:\n            lg = LogisticRegression(solver='lbfgs')\n            return(lg.fit(X_train, y_train))\n        else:\n            model = Model_name()\n            return(model.fit(X_train, y_train))\n    \n    def predicting_Model(Model_name, X_train, y_train, X_test):\n        model = Models.fitting_Model(Model_name, X_train, y_train)\n        return(model.predict(X_test))\n    \n    def score_model(Model_name, X_train, y_train, X_test, y_test):\n        model = Models.fitting_Model(Model_name, X_train, y_train)\n        y_pred = model.predict(X_test)    \n        acc_model = round(model.score(X_train, y_train) * 100, 2)\n        acc_test = round(metrics.accuracy_score(y_test, y_pred)*100,2)\n        return(acc_model, acc_test)","be295a84":"X_train, X_test, y_train, y_test = Models.preparing_data(train)","6dc31612":"acc_train, acc_test = Models.score_model(DecisionTreeClassifier, X_train, y_train, X_test, y_test)\nprint('- Decision_tree:\\n \\t accuracy_train: {}\\n \\t accuracy_test:{}'.format(acc_train, acc_test))\nacc_train, acc_test = Models.score_model(RandomForestClassifier, X_train, y_train, X_test, y_test)\nprint('- Random Forest:\\n \\t accuracy_train: {}\\n \\t accuracy_test:{}'.format(acc_train, acc_test))\nacc_train, acc_test = Models.score_model(SGDClassifier, X_train, y_train, X_test, y_test)\nprint('- SGD_classifier:\\n \\t accuracy_train: {}\\n \\t accuracy_test:{}'.format(acc_train, acc_test))\nacc_train, acc_test = Models.score_model(KNeighborsClassifier, X_train, y_train, X_test, y_test)\nprint('- KNN:\\n \\t accuracy_train: {}\\n \\t accuracy_test:{}'.format(acc_train, acc_test))\nacc_train, acc_test = Models.score_model(GaussianNB, X_train, y_train, X_test, y_test)\nprint('- Gaussian:\\n \\t accuracy_train: {}\\n \\t accuracy_test:{}'.format(acc_train, acc_test))","72ba5fac":"#converting Age and Fare features, from continuous to categorical.\ntrain_aux = Ages_to_category(train)\ntrain_aux = Fare_to_category(train_aux)","6da5ee06":"X_train_aux, X_test_aux, y_train_aux, y_test_aux = Models.preparing_data(train_aux)","345d2f1c":"acc_train, acc_test = Models.score_model(DecisionTreeClassifier, X_train_aux, y_train_aux, X_test_aux, y_test_aux)\nprint('- Decision_tree:\\n \\t accuracy_train: {}\\n \\t accuracy_test:{}'.format(acc_train, acc_test))\nacc_train, acc_test = Models.score_model(RandomForestClassifier, X_train_aux, y_train_aux, X_test_aux, y_test_aux)\nprint('- Random Forest:\\n \\t accuracy_train: {}\\n \\t accuracy_test:{}'.format(acc_train, acc_test))\nacc_train, acc_test = Models.score_model(SGDClassifier, X_train_aux, y_train_aux, X_test_aux, y_test_aux)\nprint('- SGD_classifier:\\n \\t accuracy_train: {}\\n \\t accuracy_test:{}'.format(acc_train, acc_test))\nacc_train, acc_test = Models.score_model(KNeighborsClassifier, X_train_aux, y_train_aux, X_test_aux, y_test_aux)\nprint('- KNN:\\n \\t accuracy_train: {}\\n \\t accuracy_test:{}'.format(acc_train, acc_test))\nacc_train, acc_test = Models.score_model(GaussianNB, X_train_aux, y_train_aux, X_test_aux, y_test_aux)\nprint('- Gaussian:\\n \\t accuracy_train: {}\\n \\t accuracy_test:{}'.format(acc_train, acc_test))","93d5c766":"# In order to use logistic regression, we applys hot encoding to Pclass, Embarked, Title, FamilySize features \n# We keep Age and Fare features as continuous features.\ntrain_hot = Models.one_hot_encoding(train)\ntest_hot = Models.one_hot_encoding(test)","9ca79d8e":"X_train_hot, X_test_hot, y_train_hot, y_test_hot = Models.preparing_data(train_hot)","907855e7":"acc_train, acc_test = Models.score_model(LogisticRegression, X_train_hot, y_train_hot,X_test_hot, y_test_hot)\nprint('- Logistic Regression:\\n \\t accuracy_train: {}\\n \\t accuracy_test:{}'.format(acc_train, acc_test))","96377a8f":"#taking the LogisticRegression we have the follow prediction\ntest_hot = preprocessing.StandardScaler().fit(test_hot).transform(test_hot)","0779b5c4":"y_pred = Models.predicting_Model(LogisticRegression, X_train_hot, y_train_hot, test_hot)","b79583d1":"gender_submission_LG = pd.DataFrame({\n        \"PassengerId\": test_sub[\"PassengerId\"],\n        \"Survived\": y_pred\n    })","ddf03992":"- Title\n\nMr = 1, Miss = 2, Mrs = 3, Master = 4, Dr = 5 and Other = 6","803bceb7":"### 4.3 Creating new features\n\n- FamilySyze\n\nWe join two features SibSp and Parch. We are going to consider a person who travelled alone as 1.","feebcc9e":"- Age\n\nIn order to fill the Nan, we are going to obrserve the relationship between age and three features (SibSp, Title, Pclass) becasue it can give us a hint.\n\nFirstly, we would like to group the Title feature in Mr, Mrs, Miss, Master, Dr, and Other.","ad5ce6e2":"Looking at the information above, we need to fill in three features: _Age_ , _Cabin_ and _Embarked_ .\n\n- Embarked\n\nWe start for the easiest feature because there are only two missing values and we will fill in with the mode value, which is _S_ , how is represented by the plot of frequency of _Embarked_ feature.","423fb277":"- Sex","d3e9d2d3":"- test set","222c674b":"- Fare","58feed2a":"- Cabin","c77d7686":"### 2.2  Visiualising some correlations ","c4279448":"For modelling we are going to use two dataframes in order to see if the Age and Fare work better as categorical features or continuous features. Therefore train contains continuous and train_aux contains categorical variables.","ed5d17eb":"- Parch","1b4ba564":"- train dataset","71e9a1c3":"# Titanic Data","74f20266":"### 2.1 Features vs target\nFollowing with the analysis, we are going to try to find some relationship between the target and the others features.","0e808d88":"## 2. EDA<a id='section2'><\/a>\nWe are going to study two dataframes _train_ and _test_.\n\nFirst, we apply info to both of dataframes, in order to see the features and missing values.","7278227f":"- test dataset","c101d2ae":"## Index\n- [1. Import libraries and download data](#section1)\n- [2. EDA](#section2)\n- [3. Cleaning Data](#section3)\n- [4. Data Engineering](#section4)\n- [5. Modelling](#section5)\n","1760b3ec":"#### 4.2.2. Numerical continues to categorical.\nWe are going to create two functions, that they convert from continuous variable to categorical. ","065bd2ef":"- Cabin \n\nWe are goint to eleminate this feature in both sets, because the percentage of missing values is really high and the train plot also does not show difference survive or not between different cabins.\n","d6daebfb":"### 4.2 Converting features\n\n#### 4.2.1.There are some features are string, and they have to become in numerical.\n\n- Sex \n\nfemale = 1 and male = 0","876a0ac4":"- Embarked\n\nS = 0 , C = 1, Q = 2 ","aabd9d8d":"Secondly, we plot different boxplot for the features SibSp, Title, Pclass.","e1e7ae92":"- Age","f48f66dc":"- People's title","eb70e969":"- Class ","f0c4e557":"## 3. Cleaning Data<a id='section3'><\/a>\nIn this section, we are going to work with the features that contain missing values, in both datasets.","38a60163":"### 3.1 Checking missing values","1ff74c72":"# 5. Modelling<a id='section5'><\/a>","104d678e":"- train set","b0d0f1bb":"Preparing the datasets in order to apply the models.\n\n### 4.1. Dropping features.\n\nPassengerId, Name, Ticket","fd1fc6f4":"- Fare\n\nThere is only one missing value in test dataframe, so we are going to fill with the mean value of Fare.","05b97ccf":"Observing the data before, we can say the following:\n- the __train__ set is composed by 12 features and 891 cases. These features are different type of data, such as integer, float, string, therefore we must homogeneous them, in order to build the model. Some of these features contain missing values: _Age, Cabin_ and _Embarked_.\n- the __test__ set is formed by 11 features (not contains the target) and 418 cases. And there are also some features with missing values: _Age, Fare_ and _Cabin_. ","55df7261":"## 1. Import libraries and download data<a id='section1'><\/a>","612087b2":"- Embarked","9bd9a89c":"## 4. Data Engineering<a id='section4'><\/a>","71c4cc54":"- SibSp","73b0a831":"Analysing the target of this data, which is _Survived_ feature, we can say that the majority of people did not survived. Talking with probabilities,\n\nP(survived) = 0.384\n\nP(no\\_survived) =0.616\n","493761fa":"- Correlations between features"}}