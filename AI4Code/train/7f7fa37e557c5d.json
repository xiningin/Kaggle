{"cell_type":{"ed753066":"code","ff355140":"code","8ded3d4c":"code","f3c6f4ad":"code","7466b3ab":"code","6fcff729":"code","f368c51d":"code","b67e5450":"code","d244d7d8":"code","57041731":"code","e42675aa":"code","7252a86f":"code","2899be96":"code","7df57b14":"code","f2771682":"code","94a646ce":"code","88db37c2":"code","0c843b2c":"code","53e86de6":"code","381914d3":"code","7acfc6d5":"code","6e53ba7c":"code","d43ead48":"code","75b4c8a0":"code","e8a1e070":"code","1a2f0927":"code","fa7ed071":"code","5ca99505":"code","e03287d2":"markdown","a6c877fa":"markdown","3b1659c6":"markdown","f1e9ef15":"markdown","ff3b6767":"markdown","a98e04fc":"markdown","7234f750":"markdown","e33e528c":"markdown","e6edc9e2":"markdown","f9bf8674":"markdown","64a0a3c4":"markdown","8dcade4c":"markdown","487b7a85":"markdown","753a7ee0":"markdown","131f6f4b":"markdown"},"source":{"ed753066":"from fastai.conv_learner import *\nfrom fastai.dataset import *\n\nimport pandas as pd\nimport numpy as np\nimport os\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tnrange, tqdm_notebook\nfrom scipy import ndimage","ff355140":"PATH = '.\/'\nTRAIN = '..\/input\/airbus-ship-detection\/train\/'\nTEST = '..\/input\/airbus-ship-detection\/test\/'\nSEGMENTATION = '..\/input\/airbus-ship-detection\/train_ship_segmentations.csv'\nPRETRAINED_DETECTION_PATH = '..\/input\/fine-tuning-resnet34-on-ship-detection\/models\/'\nPRETRAINED_SEGMENTATION_PATH = '..\/input\/unet34-dice-0-87\/models\/'\nDETECTION_TEST_PRED = '..\/input\/fine-tuning-resnet34-on-ship-detection\/ship_detection.csv'\nexclude_list = ['6384c3e78.jpg','13703f040.jpg', '14715c06d.jpg',  '33e0ff2d5.jpg',\n                '4d4e09f2a.jpg', '877691df8.jpg', '8b909bb20.jpg', 'a8d99130e.jpg', \n                'ad55c3143.jpg', 'c8260c541.jpg', 'd6c7f17c7.jpg', 'dc3e7c901.jpg',\n                'e44dffe88.jpg', 'ef87bad36.jpg', 'f083256d8.jpg'] #corrupted images","8ded3d4c":"nw = 2   #number of workers for data loader\narch = resnet34 #specify target architecture","f3c6f4ad":"train_names = [f for f in os.listdir(TRAIN)]\ntest_names = [f for f in os.listdir(TEST)]\nfor el in exclude_list:\n    if(el in train_names): train_names.remove(el)\n    if(el in test_names): test_names.remove(el)\n#5% of data in the validation set is sufficient for model evaluation\ntr_n, val_n = train_test_split(train_names, test_size=0.05, random_state=42)\nsegmentation_df = pd.read_csv(os.path.join(PATH, SEGMENTATION)).set_index('ImageId')","7466b3ab":"def cut_empty(names):\n    return [name for name in names \n            if(type(segmentation_df.loc[name]['EncodedPixels']) != float)]\n\ntr_n_cut = cut_empty(tr_n)\nval_n_cut = cut_empty(val_n)","6fcff729":"def get_mask(img_id, df):\n    shape = (768,768)\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    masks = df.loc[img_id]['EncodedPixels']\n    if(type(masks) == float): return img.reshape(shape)\n    if(type(masks) == str): masks = [masks]\n    for mask in masks:\n        s = mask.split()\n        for i in range(len(s)\/\/2):\n            start = int(s[2*i]) - 1\n            length = int(s[2*i+1])\n            img[start:start+length] = 1\n    return img.reshape(shape).T","f368c51d":"class pdFilesDataset(FilesDataset):\n    def __init__(self, fnames, path, transform):\n        self.segmentation_df = pd.read_csv(SEGMENTATION).set_index('ImageId')\n        super().__init__(fnames, transform, path)\n    \n    def get_x(self, i):\n        img = open_image(os.path.join(self.path, self.fnames[i]))\n        if self.sz == 768: return img \n        else: return cv2.resize(img, (self.sz, self.sz))\n    \n    def get_y(self, i):\n        mask = np.zeros((768,768), dtype=np.uint8) if (self.path == TEST) \\\n            else get_mask(self.fnames[i], self.segmentation_df)\n        img = Image.fromarray(mask).resize((self.sz, self.sz)).convert('RGB')\n        return np.array(img).astype(np.float32)\n    \n    def get_c(self): return 0","b67e5450":"def get_data(sz,bs):\n    tfms = tfms_from_model(arch, sz, crop_type=CropType.NO, tfm_y=TfmType.CLASS)\n    tr_names = tr_n if (len(tr_n_cut)%bs == 0) else tr_n[:-(len(tr_n_cut)%bs)] #cut incomplete batch\n    ds = ImageData.get_ds(pdFilesDataset, (tr_names,TRAIN), \n                (val_n_cut,TRAIN), tfms, test=(test_names,TEST))\n    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n    md.is_multi = False\n    return md","d244d7d8":"cut,lr_cut = model_meta[arch]","57041731":"def get_base():                   #load ResNet34 model\n    layers = cut_model(arch(True), cut)\n    return nn.Sequential(*layers)","e42675aa":"class UnetBlock(nn.Module):\n    def __init__(self, up_in, x_in, n_out):\n        super().__init__()\n        up_out = x_out = n_out\/\/2\n        self.x_conv  = nn.Conv2d(x_in,  x_out,  1)\n        self.tr_conv = nn.ConvTranspose2d(up_in, up_out, 2, stride=2)\n        self.bn = nn.BatchNorm2d(n_out)\n        \n    def forward(self, up_p, x_p):\n        up_p = self.tr_conv(up_p)\n        x_p = self.x_conv(x_p)\n        cat_p = torch.cat([up_p,x_p], dim=1)\n        return self.bn(F.relu(cat_p))\n\nclass SaveFeatures():\n    features=None\n    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n    def hook_fn(self, module, input, output): self.features = output\n    def remove(self): self.hook.remove()\n    \nclass Unet34(nn.Module):\n    def __init__(self, rn):\n        super().__init__()\n        self.rn = rn\n        self.sfs = [SaveFeatures(rn[i]) for i in [2,4,5,6]]\n        self.up1 = UnetBlock(512,256,256)\n        self.up2 = UnetBlock(256,128,256)\n        self.up3 = UnetBlock(256,64,256)\n        self.up4 = UnetBlock(256,64,256)\n        self.up5 = nn.ConvTranspose2d(256, 1, 2, stride=2)\n        \n    def forward(self,x):\n        x = F.relu(self.rn(x))\n        x = self.up1(x, self.sfs[3].features)\n        x = self.up2(x, self.sfs[2].features)\n        x = self.up3(x, self.sfs[1].features)\n        x = self.up4(x, self.sfs[0].features)\n        x = self.up5(x)\n        return x[:,0]\n    \n    def close(self):\n        for sf in self.sfs: sf.remove()\n            \nclass UnetModel():\n    def __init__(self,model,name='Unet'):\n        self.model,self.name = model,name\n\n    def get_layer_groups(self, precompute):\n        lgs = list(split_by_idxs(children(self.model.rn), [lr_cut]))\n        return lgs + [children(self.model)[1:]]","7252a86f":"def IoU(pred, targs):\n    pred = (pred > 0.5).astype(float)\n    intersection = (pred*targs).sum()\n    return intersection \/ ((pred+targs).sum() - intersection + 1.0)","2899be96":"def get_score(pred, true):\n    n_th = 10\n    b = 4\n    thresholds = [0.5 + 0.05*i for i in range(n_th)]\n    n_masks = len(true)\n    n_pred = len(pred)\n    ious = []\n    score = 0\n    for mask in true:\n        buf = []\n        for p in pred: buf.append(IoU(p,mask))\n        ious.append(buf)\n    for t in thresholds:   \n        tp, fp, fn = 0, 0, 0\n        for i in range(n_masks):\n            match = False\n            for j in range(n_pred):\n                if ious[i][j] > t: match = True\n            if not match: fn += 1\n        \n        for j in range(n_pred):\n            match = False\n            for i in range(n_masks):\n                if ious[i][j] > t: match = True\n            if match: tp += 1\n            else: fp += 1\n        score += ((b+1)*tp)\/((b+1)*tp + b*fn + fp)       \n    return score\/n_th","7df57b14":"def split_mask(mask):\n    threshold = 0.5\n    threshold_obj = 8 #ignor predictions composed of \"threshold_obj\" pixels or less\n    labled,n_objs = ndimage.label(mask > threshold)\n    result = []\n    for i in range(n_objs):\n        obj = (labled == i + 1).astype(int)\n        if(obj.sum() > threshold_obj): result.append(obj)\n    return result","f2771682":"def get_mask_ind(img_id, df, shape = (768,768)): #return mask for each ship\n    masks = df.loc[img_id]['EncodedPixels']\n    if(type(masks) == float): return []\n    if(type(masks) == str): masks = [masks]\n    result = []\n    for mask in masks:\n        img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n        s = mask.split()\n        for i in range(len(s)\/\/2):\n            start = int(s[2*i]) - 1\n            length = int(s[2*i+1])\n            img[start:start+length] = 1\n        result.append(img.reshape(shape).T)\n    return result","94a646ce":"class Score_eval():\n    def __init__(self):\n        self.segmentation_df = pd.read_csv(SEGMENTATION).set_index('ImageId')\n        self.score, self.count = 0.0, 0\n        \n    def put(self,pred,name):\n        true = get_mask_ind(name, self.segmentation_df)\n        self.score += get_score(pred,true)\n        self.count += 1\n        \n    def evaluate(self):\n        return self.score\/self.count","88db37c2":"m = to_gpu(Unet34(get_base()))\nmodels = UnetModel(m)","0c843b2c":"sz = 768 #image size\nbs = 8  #batch size\nmd = get_data(sz,bs)","53e86de6":"learn = ConvLearner(md, models)\nlearn.models_path = PRETRAINED_SEGMENTATION_PATH\nlearn.load('Unet34_768_1')\nlearn.models_path = PATH","381914d3":"def model_pred(learner, dl, F_save): #if use train dl, disable shuffling\n    learner.model.eval();\n    name_list = dl.dataset.fnames\n    num_batchs = len(dl)\n    t = tqdm(iter(dl), leave=False, total=num_batchs)\n    count = 0\n    for x,y in t:\n        py = to_np(F.sigmoid(learn.model(V(x))))\n        batch_size = len(py)\n        for i in range(batch_size):\n            F_save(py[i],to_np(y[i]),name_list[count])\n            count += 1","7acfc6d5":"score = Score_eval()\nprocess_pred = lambda yp, y, name : score.put(split_mask(yp),name)\nmodel_pred(learn, md.val_dl, process_pred)\nprint('\\n',score.evaluate())","6e53ba7c":"ship_detection = pd.read_csv(DETECTION_TEST_PRED)\nship_detection.head()","d43ead48":"test_names = ship_detection.loc[ship_detection['p_ship'] > 0.5, ['id']]['id'].values.tolist()\ntest_names_nothing = ship_detection.loc[ship_detection['p_ship'] <= 0.5, ['id']]['id'].values.tolist()\nlen(test_names), len(test_names_nothing)","75b4c8a0":"md = get_data(sz,bs)\nlearn.set_data(md)","e8a1e070":"def decode_mask(mask, shape=(768, 768)):\n    pixels = mask.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","1a2f0927":"ship_list_dict = []\nfor name in test_names_nothing:\n    ship_list_dict.append({'ImageId':name,'EncodedPixels':np.nan})","fa7ed071":"def enc_test(yp, y, name):\n    masks = split_mask(yp)\n    if(len(masks) == 0): \n        ship_list_dict.append({'ImageId':name,'EncodedPixels':np.nan})\n    for mask in masks:\n        ship_list_dict.append({'ImageId':name,'EncodedPixels':decode_mask(mask)})","5ca99505":"model_pred(learn, md.test_dl, enc_test)\npred_df = pd.DataFrame(ship_list_dict)\npred_df.to_csv('submission.csv', index=False)","e03287d2":"Identify images with ships and run Unet34 model only for them (~15%).","a6c877fa":"The function for mask decoding is borrowed from https:\/\/www.kaggle.com\/kmader\/from-trained-u-net-to-submission-part-2\/notebook .","3b1659c6":"Since the model predicts pixel masks, which are quite large, running standard functions for making a prediction will fail due to memory issue, especially for the test set, where about 100k 786x786 pixel masks should be created. Therefore, I wrote a function that does prediction batch by batch and applies F_save function for each generated mask.","f1e9ef15":"### Submission","ff3b6767":"Load the prediction of ship detection model (https:\/\/www.kaggle.com\/iafoss\/fine-tuning-resnet34-on-ship-detection\/notebook) for the test set.","a98e04fc":"As explained in https:\/\/www.kaggle.com\/iafoss\/unet34-dice-0-87\/notebook, I drop all images without ships. The model responsible for ship detection will take care of them.","7234f750":"It is the **score based only on images with ships**, a model responsible for ship detection (accuracy ~98%) takes care of images without ships. Since the fraction of empty images in the test set is ~0.85, the expected score of the model stacked with ship detection one (https:\/\/www.kaggle.com\/iafoss\/fine-tuning-resnet34-on-ship-detection\/notebook) is approximately 0.85 + 0.34 * 0.15 = 0.90. However, you should keep in mind that the evaluated model has been trained only for one epoch on full resolution images (the dice is only ~0.80 for 784x784 images). I tried to do similar testing for a model with dice 0.895 and got 0.44 based on images only with ships that would result in 0.92 for the model evaluation score. Continuing training the model, TTA, and mask postprocessing can further boost it.","e33e528c":"### Score evaluation","e6edc9e2":"### Prediction","f9bf8674":"It is a follow-up notebook to \"Fine-tuning ResNet34 on ship detection\" (https:\/\/www.kaggle.com\/iafoss\/fine-tuning-resnet34-on-ship-detection\/notebook) and \"Unet34 (dice 0.87+)\" (https:\/\/www.kaggle.com\/iafoss\/unet34-dice-0-87\/notebook) that shows how to evaluate the solution and submit predictions. Please check these notebooks for additional details.","64a0a3c4":"## Overview","8dcade4c":"In this competition we should submit and individual mask for each identified ship. The simplest way to do it is splitting the total mask into individual ones based on the connectivity of detected objects.","487b7a85":"Running the model evaluation on the validation set.","753a7ee0":"### Data","131f6f4b":"### Model"}}