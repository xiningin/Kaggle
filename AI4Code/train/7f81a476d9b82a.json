{"cell_type":{"ed133366":"code","adf5186b":"code","7d616d44":"code","357a8982":"code","ba385308":"code","65d16045":"code","5f2b4949":"code","7c978ff1":"code","aa2ea292":"code","4ede142e":"code","dda32b28":"code","8486cd8e":"code","e4668f85":"code","a8095314":"code","9cb07f27":"code","447f6e37":"code","46642160":"code","a7d30482":"code","c55bc55a":"code","93a5961a":"code","d8be631c":"code","fad29e94":"code","71177089":"code","5956eebf":"code","a6eb6a1a":"code","4b5c7905":"code","fd6be7aa":"code","3a7d38ad":"code","fe11b3e7":"code","9a415de3":"code","77600ccc":"markdown","af317ca3":"markdown","e3ef49e3":"markdown","a37a04a6":"markdown","083c4474":"markdown","8a0b81d5":"markdown","7bbe9e6f":"markdown","12d93c73":"markdown"},"source":{"ed133366":"import os\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nfrom tqdm.notebook import tqdm","adf5186b":"device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ndevice = torch.device(device)\nprint(device)","7d616d44":"data_dir = os.path.join(\"\/kaggle\", \"input\", \"cityscapes-image-pairs\", \"cityscapes_data\")\ntrain_dir = os.path.join(data_dir, \"train\") \nval_dir = os.path.join(data_dir, \"val\")\ntrain_fns = os.listdir(train_dir)\nval_fns = os.listdir(val_dir)\nprint(len(train_fns), len(val_fns))","357a8982":"sample_image_fp = os.path.join(train_dir, train_fns[0])\nsample_image = Image.open(sample_image_fp).convert(\"RGB\")\nplt.imshow(sample_image)\nprint(sample_image_fp)","ba385308":"def split_image(image):\n    image = np.array(image)\n    cityscape, label = image[:, :256, :], image[:, 256:, :]\n    return cityscape, label","65d16045":"sample_image = np.array(sample_image)\nprint(sample_image.shape)\ncityscape, label = split_image(sample_image)\nprint(cityscape.min(), cityscape.max(), label.min(), label.max())\ncityscape, label = Image.fromarray(cityscape), Image.fromarray(label)\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[0].imshow(cityscape)\naxes[1].imshow(label)","5f2b4949":"\"\"\"\ncolor_set = set()\nfor train_fn in tqdm(train_fns[:10]):\n    train_fp = os.path.join(train_dir, train_fn)\n    image = np.array(Image.open(train_fp))\n    cityscape, label = split_image(sample_image)\n    label = label.reshape(-1, 3)\n    local_color_set = set([tuple(c) for c in list(label)])\n    color_set.update(local_color_set)\ncolor_array = np.array(list(color_set))\n\"\"\"\n\nnum_items = 1000\ncolor_array = np.random.choice(range(256), 3*num_items).reshape(-1, 3)\nprint(color_array.shape)\nprint(color_array[:5, :])","7c978ff1":"num_classes = 10\nlabel_model = KMeans(n_clusters=num_classes)\nlabel_model.fit(color_array)","aa2ea292":"label_model.predict(color_array[:5, :])","4ede142e":"cityscape, label = split_image(sample_image)\nlabel_class = label_model.predict(label.reshape(-1, 3)).reshape(256, 256)\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\naxes[0].imshow(cityscape)\naxes[1].imshow(label)\naxes[2].imshow(label_class)","dda32b28":"label_class","8486cd8e":"class CityscapeDataset(Dataset):\n    \n    def __init__(self, image_dir, label_model):\n        self.image_dir = image_dir\n        self.image_fns = os.listdir(image_dir)\n        self.label_model = label_model\n        \n    def __len__(self):\n        return len(self.image_fns)\n    \n    def __getitem__(self, index):\n        image_fn = self.image_fns[index]\n        image_fp = os.path.join(self.image_dir, image_fn)\n        image = Image.open(image_fp).convert('RGB')\n        image = np.array(image)\n        cityscape, label = self.split_image(image)\n        label_class = self.label_model.predict(label.reshape(-1, 3)).reshape(256, 256)\n        cityscape = self.transform(cityscape)\n        label_class = torch.Tensor(label_class).long()\n        return cityscape, label_class\n    \n    def split_image(self, image):\n        image = np.array(image)\n        cityscape, label = image[:, :256, :], image[:, 256:, :]\n        return cityscape, label\n    \n    def transform(self, image):\n        transform_ops = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n        ])\n        return transform_ops(image)","e4668f85":"dataset = CityscapeDataset(train_dir, label_model)\nprint(len(dataset))","a8095314":"cityscape, label_class = dataset[0]\nprint(cityscape.shape, label_class.shape)","9cb07f27":"class UNet(nn.Module):\n    \n    def __init__(self, num_classes):\n        super(UNet, self).__init__()\n        self.num_classes = num_classes\n        self.contracting_11 = self.conv_block(in_channels=3, out_channels=64)\n        self.contracting_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.contracting_21 = self.conv_block(in_channels=64, out_channels=128)\n        self.contracting_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.contracting_31 = self.conv_block(in_channels=128, out_channels=256)\n        self.contracting_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.contracting_41 = self.conv_block(in_channels=256, out_channels=512)\n        self.contracting_42 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.middle = self.conv_block(in_channels=512, out_channels=1024)\n        self.expansive_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_12 = self.conv_block(in_channels=1024, out_channels=512)\n        self.expansive_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_22 = self.conv_block(in_channels=512, out_channels=256)\n        self.expansive_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_32 = self.conv_block(in_channels=256, out_channels=128)\n        self.expansive_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_42 = self.conv_block(in_channels=128, out_channels=64)\n        self.output = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=3, stride=1, padding=1)\n        \n    def conv_block(self, in_channels, out_channels):\n        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n                                    nn.ReLU(),\n                                    nn.BatchNorm2d(num_features=out_channels),\n                                    nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n                                    nn.ReLU(),\n                                    nn.BatchNorm2d(num_features=out_channels))\n        return block\n    \n    def forward(self, X):\n        contracting_11_out = self.contracting_11(X) # [-1, 64, 256, 256]\n        contracting_12_out = self.contracting_12(contracting_11_out) # [-1, 64, 128, 128]\n        contracting_21_out = self.contracting_21(contracting_12_out) # [-1, 128, 128, 128]\n        contracting_22_out = self.contracting_22(contracting_21_out) # [-1, 128, 64, 64]\n        contracting_31_out = self.contracting_31(contracting_22_out) # [-1, 256, 64, 64]\n        contracting_32_out = self.contracting_32(contracting_31_out) # [-1, 256, 32, 32]\n        contracting_41_out = self.contracting_41(contracting_32_out) # [-1, 512, 32, 32]\n        contracting_42_out = self.contracting_42(contracting_41_out) # [-1, 512, 16, 16]\n        middle_out = self.middle(contracting_42_out) # [-1, 1024, 16, 16]\n        expansive_11_out = self.expansive_11(middle_out) # [-1, 512, 32, 32]\n        expansive_12_out = self.expansive_12(torch.cat((expansive_11_out, contracting_41_out), dim=1)) # [-1, 1024, 32, 32] -> [-1, 512, 32, 32]\n        expansive_21_out = self.expansive_21(expansive_12_out) # [-1, 256, 64, 64]\n        expansive_22_out = self.expansive_22(torch.cat((expansive_21_out, contracting_31_out), dim=1)) # [-1, 512, 64, 64] -> [-1, 256, 64, 64]\n        expansive_31_out = self.expansive_31(expansive_22_out) # [-1, 128, 128, 128]\n        expansive_32_out = self.expansive_32(torch.cat((expansive_31_out, contracting_21_out), dim=1)) # [-1, 256, 128, 128] -> [-1, 128, 128, 128]\n        expansive_41_out = self.expansive_41(expansive_32_out) # [-1, 64, 256, 256]\n        expansive_42_out = self.expansive_42(torch.cat((expansive_41_out, contracting_11_out), dim=1)) # [-1, 128, 256, 256] -> [-1, 64, 256, 256]\n        output_out = self.output(expansive_42_out) # [-1, num_classes, 256, 256]\n        return output_out","447f6e37":"model = UNet(num_classes=num_classes)","46642160":"data_loader = DataLoader(dataset, batch_size=4)\nprint(len(dataset), len(data_loader))\n\nX, Y = iter(data_loader).next()\nprint(X.shape, Y.shape)","a7d30482":"Y_pred = model(X)\nprint(Y_pred.shape)","c55bc55a":"batch_size = 16\n\nepochs = 10\nlr = 0.01","93a5961a":"dataset = CityscapeDataset(train_dir, label_model)\ndata_loader = DataLoader(dataset, batch_size=batch_size)","d8be631c":"model = UNet(num_classes=num_classes).to(device)","fad29e94":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=lr)","71177089":"step_losses = []\nepoch_losses = []\nfor epoch in tqdm(range(epochs)):\n    epoch_loss = 0\n    for X, Y in tqdm(data_loader, total=len(data_loader), leave=False):\n        X, Y = X.to(device), Y.to(device)\n        optimizer.zero_grad()\n        Y_pred = model(X)\n        loss = criterion(Y_pred, Y)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        step_losses.append(loss.item())\n    epoch_losses.append(epoch_loss\/len(data_loader))","5956eebf":"fig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[0].plot(step_losses)\naxes[1].plot(epoch_losses)","a6eb6a1a":"model_name = \"U-Net.pth\"\ntorch.save(model.state_dict(), model_name)","4b5c7905":"model_path = \"\/kaggle\/working\/U-Net.pth\"\nmodel_ = UNet(num_classes=num_classes).to(device)\nmodel_.load_state_dict(torch.load(model_path))","fd6be7aa":"test_batch_size = 8\ndataset = CityscapeDataset(val_dir, label_model)\ndata_loader = DataLoader(dataset, batch_size=test_batch_size)","3a7d38ad":"X, Y = next(iter(data_loader))\nX, Y = X.to(device), Y.to(device)\nY_pred = model_(X)\nprint(Y_pred.shape)\nY_pred = torch.argmax(Y_pred, dim=1)\nprint(Y_pred.shape)","fe11b3e7":"inverse_transform = transforms.Compose([\n    transforms.Normalize((-0.485\/0.229, -0.456\/0.224, -0.406\/0.225), (1\/0.229, 1\/0.224, 1\/0.225))\n])","9a415de3":"fig, axes = plt.subplots(test_batch_size, 3, figsize=(3*5, test_batch_size*5))\n\nfor i in range(test_batch_size):\n    \n    landscape = inverse_transform(X[i]).permute(1, 2, 0).cpu().detach().numpy()\n    label_class = Y[i].cpu().detach().numpy()\n    label_class_predicted = Y_pred[i].cpu().detach().numpy()\n    \n    axes[i, 0].imshow(landscape)\n    axes[i, 0].set_title(\"Landscape\")\n    axes[i, 1].imshow(label_class)\n    axes[i, 1].set_title(\"Label Class\")\n    axes[i, 2].imshow(label_class_predicted)\n    axes[i, 2].set_title(\"Label Class - Predicted\")","77600ccc":"## 7. Check model predictions","af317ca3":"## 5. Define Model","e3ef49e3":"#### Dataset:\nhttps:\/\/www.kaggle.com\/dansbecker\/cityscapes-image-pairs\n\n#### Kaggle Link:\nhttps:\/\/www.kaggle.com\/gokulkarthik\/image-segmentation-with-unet-pytorch\n\n#### References:\n* https:\/\/arxiv.org\/pdf\/1603.07285v1.pdf\n* https:\/\/towardsdatascience.com\/u-net-b229b32b4a71","a37a04a6":"## 2. Analyze data","083c4474":"## 6. Train the model","8a0b81d5":"## 1. Setup","7bbe9e6f":"## 3. Define Labels","12d93c73":"## 4. Define Dataset"}}