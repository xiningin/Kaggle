{"cell_type":{"43e0665a":"code","8c2ea99e":"code","ca0b43be":"code","10adc925":"code","828c4d92":"code","538e59bb":"code","f14d97df":"code","3c3cbfaa":"code","415ff15e":"code","fc61b361":"code","e3cedce7":"code","f6d0fe3b":"code","7072e02a":"code","89c298ac":"code","0a8abf8c":"code","a5051b47":"code","e5e11862":"code","4c990b92":"code","fa53b77e":"code","7bf60a3a":"code","32d72c0c":"code","bd8acffd":"code","c8eb8f65":"code","9539eff9":"code","cb4284a7":"code","245b310e":"code","7a1104e8":"code","82e4f13f":"code","3fef2ae6":"code","0a50ae28":"code","8d94deba":"code","7798be87":"code","97bc4d65":"code","8f8c3811":"code","281f2f41":"code","f7fb9cdf":"code","45d650c2":"code","b8a34c49":"code","2cf04540":"code","203b39e7":"code","9d4a3f61":"code","4bdfb069":"code","4358c750":"code","805ffe06":"code","c4e98f48":"markdown","4fdc36ed":"markdown","262b914c":"markdown","70acb997":"markdown","ffd38447":"markdown","dd50ff1c":"markdown","cdb6c744":"markdown","af59d327":"markdown","a4125f1d":"markdown","3d5a03b4":"markdown","991d590b":"markdown","553657c9":"markdown","cf16b138":"markdown","87ff6218":"markdown","70d0a55e":"markdown","de7c1392":"markdown","f2d7068d":"markdown","d36bd00a":"markdown","e30b4c71":"markdown","f71e7e97":"markdown","7fa5413b":"markdown","eb6dce1f":"markdown","b89c5637":"markdown","c80a9cf9":"markdown","166ca171":"markdown","446cd31b":"markdown","4dc8f2e0":"markdown","78da3fbc":"markdown","ef8d3b53":"markdown","73944962":"markdown","d3be2cb1":"markdown","094278e7":"markdown","e58f230c":"markdown","f64a2baf":"markdown","fcbb37e5":"markdown","e080057c":"markdown","a072bc6a":"markdown","08dae8c6":"markdown","f3a6dc23":"markdown","5470d59d":"markdown","9030a04a":"markdown","0461c0f3":"markdown","42bff913":"markdown","c0853168":"markdown"},"source":{"43e0665a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('white')\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8c2ea99e":"from scipy import sparse\nfrom lightfm import LightFM\nfrom lightfm.evaluation import precision_at_k\nfrom lightfm.evaluation import auc_score\n\nfrom lightfm.datasets import fetch_movielens\n\nmovielens = fetch_movielens()","ca0b43be":"from tabulate import tabulate\nfrom surprise import Dataset\nfrom surprise import Reader\nfrom surprise import BaselineOnly\n\nimport random\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\n\nfrom surprise.model_selection import train_test_split\n\nfrom surprise import KNNWithMeans  \nfrom surprise import SVD \n\nfrom surprise.model_selection import GridSearchCV\n\nfrom surprise import accuracy","10adc925":"column_names = ['User_ID', 'MovieID', 'Rating', 'Timestamp']\nratings = pd.read_csv(\"..\/input\/movielens-1m-dataset\/ratings.dat\", sep = \"::\", names = column_names, engine='python')\nratings.head()","828c4d92":"column_names = ['User_ID', 'Gender', 'Age', 'Occupation', 'Zip-code']\nusers = pd.read_csv(\"..\/input\/movielens-1m-dataset\/users.dat\", sep = \"::\", names = column_names, engine='python')\nusers.head()","538e59bb":"column_names = ['MovieID', 'Title', 'Genres']\nmovies = pd.read_csv(\"..\/input\/movielens-1m-dataset\/movies.dat\", sep = \"::\", names = column_names, encoding='latin-1', engine='python')\nmovies.head() ","f14d97df":"movies[['Genre 1', 'Genre 2', 'Genre 3', 'Genre 4', 'Genre 5', 'Genre 6']] = movies.Genres.str.split(\"|\",expand=True)\nmovies.head()","3c3cbfaa":"movieset = pd.merge(movies, ratings, on = 'MovieID')\nmovieset.head()","415ff15e":"most_rated_movies = movieset.groupby('Title').size().sort_values(ascending = False)\nmost_rated_movies[:5]","fc61b361":"popular_movies = movieset.groupby('Title')['Rating'].mean().sort_values(ascending = False)\npopular_movies.head()","e3cedce7":"popular_movies_count = movieset.groupby('Title')['Rating'].count().sort_values(ascending = False)\npopular_movies_count.head()","f6d0fe3b":"movie_ratings = pd.DataFrame(movieset.groupby('Title')['Rating'].mean())\nmovie_ratings['Number of Ratings'] = movieset.groupby('Title')['Rating'].count()\nmovie_ratings.head()","7072e02a":"plt.figure(figsize = (10, 4))\nmovie_ratings['Number of Ratings'].hist(bins = 50)","89c298ac":"plt.figure(figsize = (10, 4))\nmovie_ratings['Rating'].hist(bins = 50)","0a8abf8c":"def topgenremovies(movieset, selectedgenre, Genre_1, Genre_2, Genre_3, Genre_4,\n                    Genre_5, Genre_6, Title, Rating, Num_Ratings,min_num, top_n):\n    moviegenre1 = movieset[movieset[Genre_1] == selectedgenre]\n    moviegenre2 = movieset[movieset[Genre_1] == selectedgenre]\n    moviegenre3 = movieset[movieset[Genre_1] == selectedgenre]\n    moviegenre4 = movieset[movieset[Genre_1] == selectedgenre]\n    moviegenre5 = movieset[movieset[Genre_1] == selectedgenre]\n    moviegenre6 = movieset[movieset[Genre_1] == selectedgenre]\n    moviegenre = pd.concat([moviegenre1, moviegenre2, moviegenre3, moviegenre4, moviegenre5, moviegenre6])\n    movieonegenrerating = pd.DataFrame(moviegenre.groupby(Title)[Rating].mean())\n    movieonegenrerating[Num_Ratings] = moviegenre.groupby(Title)[Rating].count()\n    highlyrated_onegenre_movies = movieonegenrerating[movieonegenrerating[Num_Ratings] > min_num].sort_values(Num_Ratings, ascending = False).reset_index()\n    top_movies = list(highlyrated_onegenre_movies[Title][0:top_n])\n    \n    return(top_movies)","a5051b47":"genre_list = [\"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\", \"Film-Noir\",\n              \"Horror\", \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"]\n\nallgenres_top = pd.DataFrame(columns = genre_list)\nallgenres_top.head()\n\nfor genre in allgenres_top.columns:\n    allgenres_top[genre] = topgenremovies(movieset, genre, 'Genre 1', 'Genre 2', 'Genre 3', 'Genre 4', 'Genre 5', 'Genre 6',\n                                          'Title', 'Rating', 'Number of Ratings',70, 10)\n    \nallgenres_top.head()","e5e11862":"allgenres_top.to_csv('allgenres_top.csv')","4c990b92":"def populargenre(movieset, selectedgenre, Genre_1, Genre_2, Genre_3, Genre_4,\n                    Genre_5, Genre_6, Title, Rating, Num_Ratings, min_num, top_n):\n    moviegenre1 = movieset[movieset[Genre_1] == selectedgenre]\n    moviegenre2 = movieset[movieset[Genre_1] == selectedgenre]\n    moviegenre3 = movieset[movieset[Genre_1] == selectedgenre]\n    moviegenre4 = movieset[movieset[Genre_1] == selectedgenre]\n    moviegenre5 = movieset[movieset[Genre_1] == selectedgenre]\n    moviegenre6 = movieset[movieset[Genre_1] == selectedgenre]\n    moviegenre = pd.concat([moviegenre1, moviegenre2, moviegenre3, moviegenre4, moviegenre5, moviegenre6])\n    movieonegenrerating = pd.DataFrame(moviegenre.groupby(Title)[Rating].mean())\n    movieonegenrerating[Num_Ratings] = moviegenre.groupby(Title)[Rating].count()\n    highlyrated_onegenre_movies = movieonegenrerating[movieonegenrerating[Num_Ratings] > min_num].sort_values(Rating, ascending = False).reset_index()\n    top_movies = list(highlyrated_onegenre_movies[Title][0:top_n])\n    \n    return(top_movies)","fa53b77e":"genre_list = [\"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\", \"Film-Noir\",\n              \"Horror\", \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"]\n\nallgenres_popular = pd.DataFrame(columns = genre_list)\nallgenres_popular.head()\n\nfor genre in allgenres_popular.columns:\n    allgenres_popular[genre] = populargenre(movieset, genre, 'Genre 1', 'Genre 2', 'Genre 3', 'Genre 4', 'Genre 5', 'Genre 6',\n                                          'Title', 'Rating', 'Number of Ratings',70, 10)\n    \nallgenres_popular.head()","7bf60a3a":"def interactionmatrix(dataset, User_ID, Title, Rating):\n    movie_user_interact = dataset.pivot_table(index = User_ID, columns = Title, values = Rating).fillna(0)\n    return movie_user_interact","32d72c0c":"movie_user = interactionmatrix(movieset, 'User_ID', 'Title', 'Rating')\nmovie_user.head()","bd8acffd":"def similarmovierecom(movie_user_interact, movie, Title, Num_Ratings, n):\n    selectedmovie_user_rating = movie_user_interact[movie]\n    selectedmovie_corr = movie_user.corrwith(selectedmovie_user_rating)\n    selectedmovie_corr = pd.DataFrame(selectedmovie_corr, columns = ['Correlation'])\n    selectedmovie_corr.dropna(inplace = True)\n    selectedmovie_corr = selectedmovie_corr.join(movie_ratings[Num_Ratings])\n    selectedmovie_corr_higlyrated = selectedmovie_corr[selectedmovie_corr[Num_Ratings] > 70].sort_values('Correlation', ascending = False).reset_index()\n    top_movies = list(selectedmovie_corr_higlyrated[Title][1:n])\n    \n    return(top_movies)","c8eb8f65":"americanbeauty_similar = similarmovierecom(movie_user, 'American Beauty (1999)', 'Title', 'Number of Ratings', 10)\nprint(americanbeauty_similar)","9539eff9":"surprisedata = ratings.drop('Timestamp', axis = 1)\nsurprisedata = surprisedata.rename({'User_ID': 'userID', 'MovieID': 'itemID', 'Rating': 'rating'}, axis=1)","cb4284a7":"newuser = {'userID': np.repeat((surprisedata.shape[0] + 1), 5), 'itemID': [1, 4, 6, 3950, 3951], 'rating': [2, 3, 5, 5, 4]}\nnewuser = pd.DataFrame(data = newuser)\nnewuser.head()","245b310e":"surprisedata = pd.concat([surprisedata, newuser], axis = 0).reset_index()\n\nsurprisedata.tail(10)","7a1104e8":"train=surprisedata.iloc[:int(surprisedata.shape[0]*0.80)]\ntest=surprisedata.iloc[int(surprisedata.shape[0]*0.80):]\nreader = Reader(rating_scale = (1,5))\n\nfull_data = Dataset.load_from_df(surprisedata[['userID', 'itemID', 'rating']], reader)\nfulldata = full_data.build_full_trainset()\ntrain_set = Dataset.load_from_df(train[['userID', 'itemID', 'rating']], reader)\ntrainset = train_set.build_full_trainset()\n\ntest_set = Dataset.load_from_df(test[['userID', 'itemID', 'rating']], reader)\ntestset = test_set.build_full_trainset()","82e4f13f":"sim_options = { \"name\": [\"cosine\"], \"min_support\": [3, 4, 5], \"user_based\": [False, True]}\n\nparam_grid = {\"sim_options\": sim_options}\n\nknnmean_gridsearch = GridSearchCV(KNNWithMeans, param_grid, measures=[\"rmse\", \"mae\"], cv=5)\nknnmean_gridsearch.fit(full_data)","3fef2ae6":"print(knnmean_gridsearch.best_score[\"rmse\"])\nprint(knnmean_gridsearch.best_params[\"rmse\"])","0a50ae28":"knn_rmses = {}\nfor i in range(10):\n    knnmean = KNNWithMeans(n_epochs = i+1, sim_options={'name': 'cosine', 'min_support': 5, 'user_based': False})\n    knnmean.fit(trainset)\n    predictions = knnmean.test(testset.build_testset())\n    knn_rmses[i+1] = accuracy.rmse(predictions)","8d94deba":"print(\"Iterations\", \"RMSE\")\nfor iterations, rmse in knn_rmses.items():\n    print('{} {}'.format(iterations, rmse))","7798be87":"param_grid = {\"lr_all\": [0.001, 0.003, 0.005], \"reg_all\": [0.2, 0.5, 0.7]}\n\nsvd_gridsearch = GridSearchCV(SVD, param_grid, measures=[\"rmse\"], cv=5)\nsvd_gridsearch.fit(full_data)\n\nprint(svd_gridsearch.best_params[\"rmse\"])","97bc4d65":"print(svd_gridsearch.best_score[\"rmse\"])\nprint(svd_gridsearch.best_params[\"rmse\"])","8f8c3811":"svd_rmses = {}\nfor i in range(10):\n    svd_alg = SVD(n_epochs = i+1, lr_all = 0.003, reg_all = 0.2)\n    svd_alg.fit(trainset)\n    predictions = svd_alg.test(testset.build_testset())\n    svd_rmses[i+1] = accuracy.rmse(predictions)","281f2f41":"print(\"Iterations\", \"RMSE\")\nfor iterations, rmse in svd_rmses.items():\n    print('{} {}'.format(iterations, rmse))","f7fb9cdf":"def get_top_n(predictions, n=10):\n    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n\n    Args:\n        predictions(list of Prediction objects): The list of predictions, as\n            returned by the test method of an algorithm.\n        n(int): The number of recommendation to output for each user. Default\n            is 10.\n\n    Returns:\n    A dict where keys are user (raw) ids and values are lists of tuples:\n        [(raw item id, rating estimation), ...] of size n.\n    \"\"\"\n\n    # First map the predictions to each user.\n    top_n = defaultdict(list)\n    for uid, iid, true_r, est, _ in predictions:\n        top_n[uid].append((iid, est))\n\n    # Then sort the predictions for each user and retrieve the k highest ones.\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[uid] = user_ratings[:n]\n\n    return top_n\n","45d650c2":"svd_alg.fit(fulldata)\n\n# Predict ratings for all pairs (u, i) that are NOT in the training set.\ntestset_full = fulldata.build_anti_testset()\npredictions = svd_alg.test(testset_full)\n\ntop_n = get_top_n(predictions, n=10)\n","b8a34c49":"# Print the recommended items for each user\n# for uid, user_ratings in top_n{1}.items():\n#     print(uid, [iid for (iid, _) in user_ratings])","2cf04540":"newuserlikes = pd.DataFrame(np.array([[2, 3, 5, 5, 4]]).astype(int),\n                   columns=[\"$1,000,000 Duck (1971)\", \"'Night Mother (1986)\", \"All the King's Men (1949)\",\n                           \"Heaven & Earth (1993)\", \"Young Sherlock Holmes (1985)\" ])\n\nnewuserlikes = pd.DataFrame(data = newuserlikes)","203b39e7":"movie_user = movie_user.append(pd.Series(name= (movie_user.shape[0] + 1))).fillna(0)\nmovie_user.tail()","9d4a3f61":"new_user_index = movie_user.shape[0]\n\nfor movie in newuserlikes.columns:\n    movie_user.at[new_user_index,movie] = newuserlikes.iloc[0][movie]\n\nmovie_user.tail()","4bdfb069":"lfdata = sparse.csr_matrix(movie_user.values)\nlfmodel = LightFM(no_components = 30, loss = 'warp', k = 15)\nlfmodel.fit(lfdata, epochs = 10)","4358c750":"def sample_recommendation_user(model, interactions, user_id,\n                               threshold = 0,nrec_items = 10, show = True):\n    '''\n    Function to produce user recommendations\n    Required Input - \n        - model = Trained matrix factorization model\n        - interactions = dataset used for training the model\n        - user_id = user ID for which we need to generate recommendation\n        - user_dict = Dictionary type input containing interaction_index as key and user_id as value\n        - item_dict = Dictionary type input containing item_id as key and item_name as value\n        - threshold = value above which the rating is favorable in new interaction matrix\n        - nrec_items = Number of output recommendation needed\n    Expected Output - \n        - Prints list of items the given user has already bought\n        - Prints list of N recommended items  which user hopefully will be interested in\n    '''\n    n_users, n_items = interactions.shape\n    scores = pd.Series(model.predict(user_id,np.arange(n_items)))\n    scores.index = interactions.columns\n    scores = list(pd.Series(scores.sort_values(ascending=False).index))\n    \n    known_items = list(pd.Series(interactions.loc[user_id,:] \\\n                                 [interactions.loc[user_id,:] > threshold].index) \\\n\t\t\t\t\t\t\t\t .sort_values(ascending=False))\n    \n    scores = [x for x in scores if x not in known_items]\n    return_score_list = scores[0:nrec_items]\n    scores = list(pd.Series(return_score_list))\n    if show == True:\n        print(\"\\n Recommended Movies:\")\n        counter = 1\n        for i in scores:\n            print(counter,' ',i)\n            counter+=1\n    return return_score_list","805ffe06":"recommended_movies =sample_recommendation_user(model = lfmodel, interactions = movie_user, user_id = 6040,\n                               threshold = 0,nrec_items = 10, show = True)","c4e98f48":"# Data Visualization","4fdc36ed":"# 1. Movie Recommendation With Correlation Matrix","262b914c":"In this segment we will find the most rated movies in the given genre.","70acb997":"* KNNWithMeans is a KNN model that takes the mean rating of each user into account. This will solve the issue of the missing values in the ratings by not associating zero for missing values with a zero rating. Bascially centering the average of the user's rating at zero.\n* Normalizing the rating matrix will solve the issue of the tough\/easy rater. Some users are pickier when rating and will give thougher ratings in general and some other users give more generous ratings to all movies. KNNwithMeans algorithm normalizes the data by default.\n* The data is divided into training (80%) and test(20%).\n* Parameters for the model are selected with Gridsearch CV. Tuned sim_options parameters are:\n    *   'K': The (max) number of neighbors to take into account for aggregation (see this note). Default: 40. Note: The actual number of neighbors that are aggregated to compute an estimation is necessarily less than or equal to k. First, there might just not exist enough neighbors and second the algorithm only includes neighbors for which the similarity measure is positive. It would make no sense to aggregate ratings from users (or items) that are negatively correlated.\n    *   'name': The name of the similarity to use. Cosine similarity is used here.\n    *   'min_support': The minimum number of common items (when 'user_based' is 'True') or minimum number of common users (when 'user_based' is 'False') for the similarity not to be zero.\n    *   'user_based': Whether similarities will be computed between users or between items. This has a huge impact on the performance of a prediction algorithm.","ffd38447":"# Data Preparation and Analysis","dd50ff1c":"Another approach for recommender systems is model based. Model based approach uses matrix factorization for the user-item interactions matrix. It transforms user-item matrix into a low-dimensional representation in terms of latent factors. These latent factors provide hidden characteristics about users and items. A user\u2019s interaction with an item is modelled as the product of their latent vectors. Singular Vector Decomposition (SVD) is a popular matrix factorization algorithm.  The prediction r^ui is set as: r^ui=\u03bc+bu+bi+qiTpu\n\nThe objective for this algorithm is to minimize the the regularized squared error here: \u2211(rui\u2212r^ui)^2 + \u03bb(bi^2+bu^+||qi||^2+||pu||^2)    (rui\u2208Rtrain)\n\nThe algoithm improves its objective (minimizing the regularized squared error) with the stochastic gradient descent. GridSearchCV is used to find the best combination of the learning rate and the regularization term.\n* \"lr_all\": The learning rate for all parameters.\n* \"reg_all\": The regularization term for all parameters\n\n\n","cdb6c744":"In this step, the new user's ratings of 5 movies will be given to the code as input. (This is an interactive step in the developed app, here I have generated a fake input to be able to use user's input in the remaining parts of the code).","af59d327":"To fix that issue, we add a column named \"Number of Ratings\" to get the number of ratings for each movie along with their average ranking","a4125f1d":"# System I The Most Rated Movies Per Genre","3d5a03b4":"# **Finding Top Movies Per Genre**","991d590b":"Running KNNWithMeans model over 10 iterations","553657c9":"Finding Top n recommendations (that are not already in the training set) for the users ","cf16b138":"Plotting number of ratings per movie","87ff6218":"LightFM is another library like Surprise with implemented algorithms for recommender systems. LightFM incorporates both item and user metadata into the traditional matrix factorization algorithms, making it possible to generalize to new items (via item features) and new users (via user features). LightFM is less computational heavy compared to Surprise models as surprise models need to go through each pair of user and item in a for loop to make predictions for top n recommendations to users, while LightFM can make this prediction in one line getting the array of the items as the input.\n\nSo, we also implemented a model with LightFM to make predictions as it is much faster than Surprise top_n prediction model and a better option to use for the application building.","70d0a55e":"In the second part, we will implement 4 different algorithms to make more personalized movie recommendations to user with both item-based and user-based recommendations.\nWe start with a very simple algorithm and make the algorithms more complicated for each model.","de7c1392":"Belwo is some basic data exploratory steps to get more familiarize with the movielens 1 m dataset","f2d7068d":"Here we do basic data analysis, preparing the dataframes for our input type.","d36bd00a":"# ****System II - Collaborative Recommendation Systems****","e30b4c71":"In this part, we first create a sparse matrix of each user and their rating for each movie. Then will find their correlation matrix with pearson correlation and use that to recommend movies to other users. This model will find the most similar movies to the given movie","f71e7e97":"And what are top 5 with highest ranking rates in the dataframe","7fa5413b":"Plotting ratings distribution","eb6dce1f":"# ****Personalized Movie Recommendation****","b89c5637":"Some movies have multiple gerne tags in the \"Genres\" columns. We need to split this column into different genres. ","c80a9cf9":"# System I - Algorithm II: Finding Popular Movies in Each Genre","166ca171":"Surprise library gets a specific data format as an input. The input dataset for Surprise models must have three columns named \"userID\", \"itemID\", \"rating\". In the following parts the new user is added to the dataset and the dataset is prepared for Surprise KNN model input format. ","446cd31b":"Here we will implement two collaborative models: KNN (memory based) and SVD (model based)","4dc8f2e0":"The first model is derived from a basic nearest neighbors approach. The model predicts ratings of user-item combination based on its neighborhood. Neighborhoods can be defined either as user based or item based.\n\nSurprise library is used for KNN modeling. To find the best similarity measures parameters (sim_options) GridSearchCV with 5 - fold was used. GridsearchCV finds the best combination of sim_options that optimizes the accuracy metrics with cross-validation.\nThe model and the chosen parameters are explained in more details further.\n","78da3fbc":"The code below prints top 10 movie recommendations for all users. This part has been commented due to the length of the output.","ef8d3b53":"Below we make recommendations to the user based on their the movies they have rated in the input","73944962":"For system | recommendation we need to provide two algorithms to recommend top 10 movies in a genre selected by the user.\n* Algorithm one implements \"top\" as the most viewed movies in that genre.\n* Algorithm two implements \"top\" as the most popular movies in that genre. We later explain on the process for determining the most popular","d3be2cb1":"The issue with the above approach is that although some movies might have a very highly ranked (5, 4 stars) rating, but might have not been watched with many users. These movies should not be listed as popular if only a small group of users have watched them.\nBelow we can see that this is actually the case and therefore the issue with just sorting the ratings in the datasset to get the most popular movies.","094278e7":"# **Recommender Systems Movie Recommendation**","e58f230c":"Printing RMSE results for each iteration","f64a2baf":"\nIn this project we are going to build six different recommender systems (sorted from basic to more complicated) for Movielens 1M dataset. The dataset contains about 1 million anonymous ratings of approximately 3,900 movies made by 6,040 MovieLens users who joined MovieLens in 2000. We then create an interactive web application for the user to choose their favorite movies and get recommendations. Below is a link to the web application:\n\n[abishekjs.pythonanywhere.com](http:\/abishekjs.pythonanywhere.com\/)\n\nTasks Responsibilities:\n* Abishek Samuel (asamuel4): Application Development\n* Donia Zaheri (DoniaZ2): Recommender Systems Modeling\n\nThe first two recommender algorithms make recommendation based on genre. The last four algorithms make more personilzied recommendations by collaborative recommendation, asking the user to rate a number of movies and using that input to make the movies recommendation to the user.","fcbb37e5":"Importing Movielens dataset into Pandas data frames","e080057c":"# Data Exploratory","a072bc6a":"# **4. Building a Recommender System with LightFM**","08dae8c6":"# 3. Movie Recommendation With Building a SVD Model","f3a6dc23":"# 2. Movie Recommendation With Building a KNN Model","5470d59d":"Below we get input from the \"New User\" that has ranked n movies","9030a04a":"# References for this code:\nA few parts of this code were used\/modified from the resources below:\n* https:\/\/surprise.readthedocs.io\/en\/stable\/getting_started.html\n* https:\/\/github.com\/aayushmnit\/cookbook\n","0461c0f3":"Let's see what are the top five most rated movies","42bff913":"Merging movies and ratings dataframes to have both the ratings and movies information in one data frame for analyzing","c0853168":"Now we propose another approach for recommending top movies to users for each genre: Popular movies\nWe define popular as movies which have more than average rating score and more than 70 people have reviewed it."}}