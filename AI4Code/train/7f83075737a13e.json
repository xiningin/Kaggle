{"cell_type":{"ed5851f3":"code","b334d350":"code","bc53e4d3":"code","fd31193c":"code","e0fd0ff0":"code","5280eeda":"code","342d655b":"code","6169c9a7":"code","5fff3c47":"code","71980d97":"code","7c33f576":"code","9b6164cf":"code","9ed86a79":"code","8270c51f":"code","46aa0820":"code","4bcae8dc":"code","adc144f6":"code","dcabae8b":"code","4d8673c8":"code","dc324ebf":"code","68af3876":"code","e397d87a":"code","793b23af":"code","f49d818b":"code","cac49a74":"code","e7a51853":"code","f74e55f7":"code","5e9d0c94":"code","7afd648b":"code","3fc5ea7d":"code","24b9a337":"code","117adc6a":"code","559197b8":"code","b1390237":"code","fb8778d1":"markdown","67c3d70d":"markdown","9599efca":"markdown","ceac0f25":"markdown","38c806d0":"markdown","b7325054":"markdown","5297c779":"markdown","0234a2ec":"markdown","7d36ca26":"markdown","e58b2d27":"markdown","749ec9eb":"markdown","63e9c47d":"markdown","c601e189":"markdown","525bd383":"markdown","15a19b77":"markdown","4ce4ee0b":"markdown","7c085571":"markdown","57a1a587":"markdown","3457c1ef":"markdown","15796109":"markdown","291873e7":"markdown","6a43ba43":"markdown","2ee23123":"markdown","377c125c":"markdown","5a9dae94":"markdown","4d559bde":"markdown","a6f5bee7":"markdown","c2ec7f46":"markdown","f9abb836":"markdown"},"source":{"ed5851f3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b334d350":"import tensorflow as tf\nprint(tf.__version__)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, BatchNormalization, Dropout, Flatten, AveragePooling2D, Embedding, Masking\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n\nimport os\nfrom os import path, getcwd, chdir\nimport random\nimport pickle\n\nfrom shutil import copyfile\nfrom zipfile import ZipFile\n","bc53e4d3":"DataDirForest = '\/kaggle\/input\/d\/kutaykutlu\/forest-fire'\nDataDirFire = '\/kaggle\/input\/fire-dataset\/fire_dataset'\nDataDirFireExt = '\/kaggle\/input\/forest-fire\/FIRE_DATASET'\n\n\nprint(os.listdir(DataDirForest))\nprint(os.listdir(DataDirFire))\nprint(os.listdir(DataDirFireExt))","fd31193c":"# Use os.mkdir to create your directories\n# You will need a directory for cats-v-dogs, and subdirectories for training\n# and testing. These in turn will need subdirectories for 'cats' and 'dogs'\nto_making_dir = [\n    '\/tmp\/smoke-fire',\n    '\/tmp\/smoke-fire\/fire',\n    '\/tmp\/smoke-fire\/smoke',\n    '\/tmp\/smoke-fire\/natural'\n]\nfor directory in to_making_dir:\n    try:\n        os.mkdir(directory)\n    except OSError:\n        pass\n    \n","e0fd0ff0":"print(os.listdir('\/tmp\/smoke-fire\/'))\n# os.rmdir('tmp\/smoke-fire\/testing')\n# os.rmdir('tmp\/smoke-fire\/training')","5280eeda":"FIRE_DIR = '\/tmp\/smoke-fire\/fire'\n\nSMOKE_DIR = '\/tmp\/smoke-fire\/smoke'\n\nNATURAL_DIR = '\/tmp\/smoke-fire\/natural'\n\n","342d655b":"def copy_file(src, dst):\n    for f in os.listdir(src):\n        file_dir = src +f\n        dest_dir = dst +'\/'+f\n        copyfile(file_dir, dest_dir)\n        \ncopy_file(DataDirForest + '\/train_fire\/', FIRE_DIR)\ncopy_file(DataDirFire + '\/fire_images\/', FIRE_DIR)\ncopy_file(DataDirFireExt + '\/fire_images\/', FIRE_DIR)\n\ncopy_file(DataDirForest + '\/train-smoke\/', SMOKE_DIR)\n\ncopy_file(DataDirFire + '\/non_fire_images\/', NATURAL_DIR)\ncopy_file(DataDirFireExt + '\/non_fire_images\/', NATURAL_DIR)","6169c9a7":"fire = len(os.listdir(FIRE_DIR))\nsmoke = len(os.listdir(SMOKE_DIR))\nnatural = len(os.listdir(NATURAL_DIR))\n\nlen(os.listdir(SMOKE_DIR))\nlen(os.listdir(FIRE_DIR))\n\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.set_title('Distribution Dataset')\nlabel = ['Fire', 'Smoke', 'Natural']\ndata = [fire, smoke, natural]\nax.bar(label,data)\n\nplt.show()","5fff3c47":"\n## Visualisasi Image Augmentation\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n\nex_img = DataDirForest + '\/train_fire\/20130611_100618_Black-Forest-Fire-engulfs-house.jpg'\n\nimg = load_img(ex_img)\nimg_arr = img_to_array(img)\nplt.title('Real Image')\nplt.imshow(img)\nnew_img = np.expand_dims(img_arr, 0)\nimageDataGen = ImageDataGenerator(horizontal_flip=True,\n                                  zoom_range = 0.4,\n                                  rotation_range=20,    \n                                  featurewise_center=True,\n                                  featurewise_std_normalization=True,\n                                  width_shift_range=0.2,\n                                  height_shift_range=0.2,\n                                  validation_split=0.2,\n                                  shear_range = 0.2,\n                                  fill_mode = 'nearest'            \n)\niterate = imageDataGen.flow(new_img, batch_size=1)\n\nplt.figure(figsize=(10, 8))\n\nfor i in range(9):\n  plt.subplot(330  + 1 + i)\n  batch = iterate.next()\n  image = batch[0].astype('uint8')\n  plt.imshow(image)\nplt.show()","71980d97":"BASE_DIR = '\/tmp\/smoke-fire'\ntrain_datagen = ImageDataGenerator(\n    rescale=1.\/255, #membuat datanya menjadi 'normalizing'\n    rotation_range=20,\n    featurewise_center=True,\n    featurewise_std_normalization=True,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2,\n    shear_range = 0.2,\n    fill_mode = 'nearest'\n)","7c33f576":"train_generator = train_datagen.flow_from_directory(\n    BASE_DIR,\n    target_size=(150, 150),\n    batch_size=16,\n    save_format=\"png\",\n    shuffle=True,\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    BASE_DIR,\n    target_size=(150, 150),\n    batch_size=16,\n    save_format=\"png\",\n    subset='validation'\n)\n","9b6164cf":"class myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if (logs.get('accuracy') > 0.98):\n            print(\"\\nReached 98% accuracy so cancelling training\")\n            self.model.stop_training = True","9ed86a79":"def lr_scheduler(epochs, lr):\n    if epochs > 50 and epochs < 75:\n        lr = 0.001\n    if epochs > 75:\n        lr = 0.0001\n    return lr","8270c51f":"callbacks = myCallback()","46aa0820":"scheduler = LearningRateScheduler(lr_scheduler, verbose=1)","4bcae8dc":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)), #harus memperhatikan input_shape dan '16, (3, 3)' adalah filter 6 dan kernelnya 3\n    tf.keras.layers.MaxPooling2D(2,2), #membuat model lebih minimalis lagi dengan pengurangan sejumlah parameternya\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(3, activation='softmax')\n]) ","adc144f6":"model.compile(optimizer = tf.optimizers.Adam(), # optimizenya adalah yang akan kita pakai\n              loss = 'categorical_crossentropy',# harus memperhatikan apa yang kita akan pakai untuk pelabelannya\n              metrics=['accuracy'])","dcabae8b":"model.summary()","4d8673c8":"%load_ext tensorboard\nfrom datetime import datetime\n\n\ndef plot_confusion_matrix(cm, class_names):\n    \"\"\"\n    Returns a matplotlib figure containing the plotted confusion matrix.\n    \n    Args:\n       cm (array, shape = [n, n]): a confusion matrix of integer classes\n       class_names (array, shape = [n]): String names of the integer classes\n    \"\"\"\n    \n    figure = plt.figure(figsize=(8, 8))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(\"Confusion matrix\")\n    plt.colorbar()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names, rotation=45)\n    plt.yticks(tick_marks, class_names)\n    \n    # Normalize the confusion matrix.\n    cm = np.around(cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis], decimals=2)\n    \n    # Use white text if squares are dark; otherwise black.\n    threshold = cm.max() \/ 2.\n    \n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        color = \"white\" if cm[i, j] > threshold else \"black\"\n        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n        \n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    return figure\n\n# Clear logs prior to logging data.\n!rm -rf logs\/image\n\n# Create log directory\nlogdir = \"logs\/image\/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\n# EXERCISE: Define a TensorBoard callback. Use the log_dir parameter\n# to specify the path to the directory where you want to save the\n# log files to be parsed by TensorBoard.\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)# YOUR CODE HERE\n\nfile_writer_cm = tf.summary.create_file_writer(logdir + '\/cm')\n\ndef plot_to_image(figure):\n    \"\"\"\n    Converts the matplotlib plot specified by 'figure' to a PNG image and\n    returns it. The supplied figure is closed and inaccessible after this call.\n    \"\"\"\n    \n    buf = io.BytesIO()\n    \n    # EXERCISE: Use plt.savefig to save the plot to a PNG in memory.\n    # YOUR CODE HERE\n    plt.savefig(buf, format='png')\n    \n    # Closing the figure prevents it from being displayed directly inside\n    # the notebook.\n    plt.close(figure)\n    buf.seek(0)\n    \n    # EXERCISE: Use tf.image.decode_png to convert the PNG buffer\n    # to a TF image. Make sure you use 4 channels.\n    image = tf.image.decode_png(buf.getvalue(), channels=4)# YOUR CODE HERE\n    \n    # EXERCISE: Use tf.expand_dims to add the batch dimension\n    image = tf.expand_dims(image, 0)# YOUR CODE HERE\n    \n    return image\n\ndef log_confusion_matrix(epoch, logs):\n    \n    # EXERCISE: Use the model to predict the values from the test_images.\n    test_pred_raw = model.predict(test_images)# YOUR CODE HERE\n    \n    test_pred = np.argmax(test_pred_raw, axis=1)\n    \n    # EXERCISE: Calculate the confusion matrix using sklearn.metrics\n    cm = sklearn.metrics.confusion_matrix(test_labels, test_pred)# YOUR CODE HERE\n    \n    figure = plot_confusion_matrix(cm, class_names=class_names)\n    cm_image = plot_to_image(figure)\n    \n    # Log the confusion matrix as an image summary.\n    with file_writer_cm.as_default():\n        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)\n\n# Define the per-epoch callback.\ncm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\n\n# Start TensorBoard.\n%tensorboard --logdir logs\/image\n\n","dc324ebf":"  %reload_ext tensorboard","68af3876":"history = model.fit(\n    train_generator,\n    steps_per_epoch=16,\n    epochs=40,\n    validation_data=validation_generator,\n    validation_steps=4,\n    verbose=2,\n    callbacks=[callbacks]#tensorboard_callback, cm_callback. kamu bisa menambahkan hal disamping namun harus dirapihkan lagi.\n)\n\n# model fitting\nreturn history.epoch, history.history['accuracy'][-1]","e397d87a":"\nacc=history.history['accuracy']\nval_acc=history.history['val_accuracy']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs=range(len(acc))\nplt.figure(figsize=(16,9))\nplt.subplot(221)\n\nplt.grid(True)\nplt.plot(epochs, acc, 'g')\nplt.plot(epochs, val_acc, 'b')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend(['acc', 'val_acc'], loc='best')\nplt.title('Training and validation accuracy')\n\nplt.subplot(222)\nplt.grid(True)\nplt.plot(epochs, loss, 'r')\nplt.plot(epochs, val_loss, 'b')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend(['loss', 'val_loss'], loc='upper right')\nplt.title('Training and validation loss')\n\nplt.show()","793b23af":"# EXERCISE: Use the tf.saved_model API to save your model in the SavedModel format. \nexport_dir = BASE_DIR + 'saved_model\/1'\n\ntf.saved_model.save(model, export_dir)# YOUR CODE HERE","f49d818b":"# Select mode of optimization\nmode = \"Speed\" \n\nif mode == 'Storage':\n    optimization = tf.lite.Optimize.OPTIMIZE_FOR_SIZE\nelif mode == 'Speed':\n    optimization = tf.lite.Optimize.OPTIMIZE_FOR_LATENCY\nelse:\n    optimization = tf.lite.Optimize.DEFAULT","cac49a74":"# EXERCISE: Use the TFLiteConverter SavedModel API to initialize the converter\n\nconverter = tf.lite.TFLiteConverter.from_saved_model(export_dir)# YOUR CODE HERE\n\n# Set the optimzations\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]# YOUR CODE HERE\n\n# Invoke the converter to finally generate the TFLite model\ntflite_model = converter.convert()# YOUR CODE HERE","e7a51853":"with open('model.tflite', 'wb') as f:\n  f.write(tflite_model)","f74e55f7":"with open('model_transfer.tflite', 'wb') as f:\n  f.write(tflite_model)","5e9d0c94":"model.save_weights(\"model_transfer.h5\")","7afd648b":"\ndef detect_image(image_dir, name_file):\n\n    img = image.load_img(image_dir, target_size=(150,150))\n    imgplot = plt.imshow(img)\n    x = image.img_to_array(img) \n    x = np.expand_dims(x, axis=0)\n    x = x\/255.0\n    y = np.expand_dims(x, axis=0)\n    \n    class_detect = 'None'\n    \n    threshold = 0.5\n    images = np.vstack([x])\n    classes = model.predict(images, batch_size=10)\n    result = np.argmax(classes)\n    confident = classes[0][result]\n    \n    print(classes)\n    \n    if threshold < confident:\n        if result == 0:\n            class_detect = 'fire'\n        else:\n            class_detect = 'Smoke'\n            \n            \n#     if result > classes[0][1] and classes[0][2]:\n#         class_detect = \"natural\"\n#     elif classes[0][1] > classes[0][0] and classes[0][2]:\n#         class_detect = \"fire\"\n#     elif classes[0][2] > classes[0][0] and classes[0][1]:\n#         class_detect = \"smoke\"\n\n    print('{} detected as {} with confident score = {}'.format(name_file, class_detect, confident))","3fc5ea7d":"list_test = os.listdir('..\/input\/d\/kutaykutlu\/forest-fire\/test_small')\nfor file in list_test:\n    print(file)\n    detect_image('..\/input\/d\/kutaykutlu\/forest-fire\/test_small\/'+file,file)\n# detect_image('..\/input\/d\/kutaykutlu\/forest-fire\/train-smoke\/000006.jpg', '000001.jpg')","24b9a337":"!pip install opencv-python","117adc6a":"import cv2\ncv2.__version__","559197b8":"src = cv2.VideoCapture('..\/input\/videofire')\ncolor_dict={0:(0,255,0),1:(0,0,255)}\nclass_detect = 'None'\nthreshold = 0.7\nfont = cv2.FONT_HERSHEY_SIMPLEX\nwhile(True):\n    ret,frame = src.read()\n    \n    img = cv2.resize(frame,(224,224), fx=0, fy=0, interpolation = cv2.INTER_CUBIC)     \n    img = img.reshape(1,224,224,3)\n    \n    images = np.vstack([img])\n    \n    classes = model.predict(images, batch_size=10)\n    result = np.argmax(classes)\n    confident = classes[0][result]\n    \n    if threshold < confident:\n        if result == 0:\n            class_detect = 'fire'\n        else:\n            class_detect = 'Smoke'\n    else:\n        class_detect = ''\n        \n    cv2.putText(frame, \n                class_detect, \n                (50, 50), \n                font, 1, \n                (0, 255, 255), \n                2, \n                cv2.LINE_4)\n\n    cv2.imshow(\"LIVE\", frame)\n    key =cv2.waitKey(1)\n\n    if(key==27):\n        break\n\ncv2.destroyAllWindows()\nsrc.release()","b1390237":"import os, signal\nos.kill(os.getpid(), signal.SIGKILL)","fb8778d1":"### Convert a model","67c3d70d":"## Import all what we need","9599efca":"### Evaluating Accuracy and Loss for the Model","ceac0f25":"### Checkpoint callback options","38c806d0":"### load the weights from the checkpoint and re-evaluate:","b7325054":"### Clean Up","5297c779":"### SavedModel format\n","0234a2ec":"### Exporting to TFLite","7d36ca26":"### rebuild a fresh\nuntrained model and evaluate it on the test set. An untrained model will perform at chance levels (~10% accuracy):","e58b2d27":"# **Overview**\n* Import all what we need \n* Load data with os,zipfile\n* Split data\n* Preprocessing data - augmented, etc\n* Learning not with transfer learning\n* evaluate model performance\n* dump model into file\n* posttest","749ec9eb":"### Standardize the data","63e9c47d":"Apa aja yang harus diperhatikan dari dataset?\n\n**[DeepLearning.AI TensorFlow Developer - Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning - week 2]**\n\n1. bentuk imagenya seperti (28, 28, 1) bahwa ukuran imagenya 28 * 28 dan greyscale\n2. Berapa banyak datanya disini, ex 70.000 dataset\n3. diperhatikan juga banyak label nya misalkan ada 10 label maka ada 10 output neuron\n4. kalau 'relu' buat apasih? itu hanya akan mereturn nilai jika nilai itu lebih dari 0\n5. kenapa ada training dan test sets? untuk mentest network sebelumnya data yg tidak terlihat\n6. kalau softmax itu mengambil banyak set value, dan secara efektif mengambil nilai yang paling terbesar, ex [0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05] maka hasilnya akan [0,0,0,0,1,0,0,0,0]\n7. loss='sparse_categorical_crossentropy' untuk apa ini? **tolong segera dicari**\n8. optimizer='adam' untuk apa ini? **tolong segera dicari**\n\n**[DeepLearning.AI TensorFlow Developer - Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning - week 3]**\n1. Conv, teknik dalam menisolasi feature pada gambar\n2. Pooling, teknik untuk me-reduce informasi pada gambar bersamaan juga menjaga feature2 didalamnya\n3. apa yang terjadi pada gambar yang di-dilter dari 28x28 dengan 3x3 filter, itu akan menjadi nilai 26x26\n4. setelah melakukan maxpooling gambar 26x26 dengan 2x2, itu akan menjadi 13x13\n5. megunakan Conv pada DNN akan membuat training, menjadi memiliki banyak factor training menjadi lebih cepat dan lambat, jeleknya mendesign dengan Conv layer mungkin walaupun lebih kurang efficient dari pada DNN\n\n**[DeepLearning.AI TensorFlow Developer - Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning - week 4]**\n1. bagaimana Image Generator akan memberikan gambar label? itu akan sesuia dir image termasuk didalamnya.\n2. methide untuk normalize pada Image Generator? .rescale\n3. bagaimana kita memberikan training_size pada gambar? dengan target_size pada training generator\n4. ketika kita memasukan input_shape(300, 300, 3), apa maksudnya? setiap gambar akan memiliki ukuran pixel 300X300 dan memiliki 3 byte mendefinisikan warna(RGB)\n5. jika kamu training data mencapai 1.000 accuracy, tapi validation data tidak seperti itu, apa masalahnya disini? kamu berada pada overfitting pada training data mu\n6. Conv Neural Networks itu lebih bagus dari pada image yg klasifikasi karena? karena dalam setiap gambar, feature mungkin bisa berbeda disetiap bagian frame-nya, banyak macam dari gambar a, banyak variasi dari gambar b\n7. setelah mereduksi ukuran dari gambar, result training akan berbeda, kenapa? kita menghapus beberapa Conv untuk menghandle gambar yang lebih kecil\n\n**[DeepLearning.AI TensorFlow Developer - Convolutional Neural Networks in TensorFlowg - week 1]**\n1. apa yang dilakukan flow_from_directory sebab yang kamu rasakan pada ImageGenerator? kemampuan untuk lebih mudah untuk me-load gambar2 pada trining, kemampuan untuk meng-pick ukuran pada gambar training, kemampuan untuk automatis pada pe-label-an gambar sesuai pada nama directory -nya\n2. jika gambar-mu memiliki ukuran 150x150 dan melakukan 3x3 conv padanya, apa hasinya? 148x148\n3. jika data mu memiliki ukuran 150x150 dan melakukan pool 2x2, apa hasilnya? 75x75\n4. jika kamu ingin untuk melihat history pada training mu, bagaimana mengaksesnya? buatlah variable 'history' dan assign itu pada return dari model.fit_generator\n5. apa nama API yang bisa kamu gunakan untuk me-inspect dari hasil conv pada gambar? model.layers API\n6. ketika meng-explore pada graph, loss menunjukan level 0.75 pada epochs ke-2, tapi akurasi sampai pada mendekati 1.0 setelah 15 epochs, apa yang terjadi secara significant? tidak ada titik training setelah 2 epochs, seperti kita telah overfit pada validation data\n7. kenapa validation accuracy menjadi indicator yg lebih baik dari pada training accuracy terhadap performa model?\n8. kenapa overfitting itu disebabkan pada datasets yang terlalu kecil? Karena kemungkinan semua fitur yang mungkin ditemukan dalam proses pelatihan lebih kecil.\n\n**[DeepLearning.AI TensorFlow Developer - Convolutional Neural Networks in TensorFlowg - week 2]**\n1. bagaimana kamu menggunakan image augmentation dalam TF? menggunakan parameters pada ImageDataGenerator\n2. jika training data mu memiliki gambar muka pada sebelah kiri saja, tapi kamu ingin untuk mengklasifikasi muka dari sebelah kanan, bagaimana kamu menghindari overfitting? menggunakan 'horizontal_flip' parameter\n3. ketika training dengan augmentation, kamu akan tahu bahwa training nya akan lebeh sedikit lambat, kenapa? karena gambar yg diproses mengambil banyak cycles(tahapan)\n4. apa yang dilakukan fill_do parameter? itu akan mencoba untuk menciptakan ulang informasi yang hilang setelah transformasi yang terpotong\n5. ketika menggunakan image augmentation dengan ImageDataGenerator, apa yang akan terjadi pada kumpulan gambar data on-disk? tidak terjadi apa2, semua augmentasi terjadi dan selesai pada in-memory\n6. bagaimana Image Augmentation membantu menyelesaikan overfitting? memanipulasi validation set untuk meng-generate lebih banyak skenario pada feature dalam gambar\n7. ketika menggunakan image augmentation trainig ku akan menjadi lebih lambat\n8. menggunakan image augmentation memberi efek pada simulasi memiliki data yang besar untuk training[benar]\n\n**[DeepLearning.AI TensorFlow Developer - Convolutional Neural Networks in TensorFlowg - week 3]**\n1. jika kamu memberikan dropout parameter menjadi 0.2, berapa banyak node yang akan kamu hilangkan? 20 % dari seluruhnya\n2. kenapa transfer learning itu sangat berguna? karena kamu dapat menggunakan feature2 yang sudah learned dari datasets yang besar berasal kamu yang tidak dapat akses kepadanya\n3. bagaimana kamu me-lock atau me-freeze layer untuk retraining di kemudian? layer.trainable = false\n4. bagaimana kamu menganti angka dari kelas, model yang dapat menklasifikasi ketika mengunakan transfer learning?(ex. original model dapat menghandle 1000 kelas, tapi kamu hanya membutuhkan 2 kelas) ketika kamu menambahkan pada DNN di bawah dari network, kamu menspesifikasi output layer - mu dengan angka dari kelas yang kamu mau\n5. apakah kamu dapat mengunakan image augmentation dengan Transfer Learning models? ya bisa, karena kamu menambahkan layer baru pada bagian bawah dari network, dan kamu dapat mengunakan Image Augmentation ketika men-training semua itu\n6. kenapa dropout membantu dari overfitting? karena neighbor neurons bisa dapat similar weight, dan kemudian kemghasilkan skew pada final training\n7. apakah gejala dari droupout me-rate terlalu tinggi? network akan banyak kehilangan specilization untuk meng-effect yang itu akan menghasilkan ketidakefficiensi pada learning, kemudian accuracy akan menurun\n8. bagaimana kode yang benar untuk menambah Dropout 20% dari neurons dengan TF? tf.keras.layers.Dropout(0,2),\n\n**[DeepLearning.AI TensorFlow Developer - Convolutional Neural Networks in TensorFlowg - week 4]**\n1. diagram dari traditional programing memiliki Rules dan data dimasukan, tapi mengeluarkan? answers\n2. kenapa DNN pada fashion MNIST memiliki keluaran 10 neurons? karena datasets memiliki 10 kelas\n3. apa conv itu? teknik untuk mengekstrak feature dari gambar\n4. menggunakan Conv pada atas dari DNN akan menghasilkan impact pada training? itu bergantung pada banyak factor. itu bisa membuat training semakin cepat atau lambat, dan design yang jelek Conv layer mungkin lebih kurang eficient dari pada DNN\n5. apa methode pada ImageGenerator dapat mengunakan normalize image? rescale\n6. ketika mengunakan image augmentation dengan ImageDataGenerator, apa yang akan terjadi pada sebagian besar data on-disk? tidak terjadi apa2\n7. apakah bisa kamu mengunakan Image Augmentation dengan Transfer Learning? iya bisa, itu harus dengan me-pre trained layer yang mana frozen. maka kemudian kamu dapat meng-augment gambarmu seperti yang kamu train pada layer bawah dari DNN dengan semuanya\n8. ketika training untuk multiple kelas, apa kelas mide untuk Image Augmentation? class_mode='categorical'\n\n**[TensorFlow: Data and Deployment - \nDevice-based Models with TensorFlow Lite - week 1]**\n1. apa platform yang support dengan TFLite? Rasbery Pi, IOS, Android, Some Microcontroller\n2. Apa itu Quantization? teknik me-reduce presisi dan ukuran model untuk dapat bekerja lebih baik pada Mobile\n3. TFLite file format adalah contoh dari? flatbuffer\n4. apa type dari input yang TFLite Convertoer API terima? a set of concrete funtion, savedmodel, keras HDF5 file\n5. benar atau salah, savedmodel format mensupport model versioning? betul\n6. jika kamu mau untuk mensave keras model eksiting, apa APInya? tf.saved_model.save(model, path)\n7. jika kamu ingin untuk menggunakan TFLite Converter untuk meng-convert model yang telah disimpan menjadi TF Lite, apa API-nya?\n* converter = tf.lite.TFLiteConverter.from_saved_model(path)\n* newModel = converter.convert()\n8. jika kamu ingin membuat model keras dan meng-convertnya, apa TFLiteConverter methodenya? from_keras_model(model)\n9. jika kamu ingin meng-convert menggunakan command line tool, apa nama toolnya? tflite_convert\n10. jika kamu ingin melakukan post training quantization, apa option yang mungkin optimization? OPTIMIZE_FOR_LATENCY, OPTIMIZE_FOR_SIZE","c601e189":"## Preprocessing data","525bd383":"### Visualizing the Convolutions and Pooling","15a19b77":"### Test the Model with TFLite Interpreter","4ce4ee0b":"# Split data","7c085571":"### Pre trained model or Transfer learning","57a1a587":"## Load data with os,zipfile","3457c1ef":"## POSTTEST ","15796109":"### Visualizing Intermediate Representations","291873e7":"### Image data preprocessing","6a43ba43":"## Dump model into file","2ee23123":"Load images from documentation tf","377c125c":"## Learning","5a9dae94":"### Configure the dataset for performance","4d559bde":"mengecek labelnya","a6f5bee7":"### Save checkpoints during training","c2ec7f46":"melihat datanya","f9abb836":"## Evaluate Model"}}