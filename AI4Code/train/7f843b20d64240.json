{"cell_type":{"dd9fb662":"code","948e3ec4":"code","f42504e7":"code","7fe10100":"code","4c6f5d0b":"code","e2d809a9":"code","dc93cbc4":"code","62e91dc1":"code","7c593596":"code","bb336a53":"code","d4f6f94a":"code","b2089f21":"code","8703c49c":"code","214f2801":"code","e3670d76":"code","a5fd7366":"code","4cf0656b":"code","2a3f9acc":"code","7eff2ca3":"code","7226a977":"code","13a9f5f8":"code","19bfeac9":"code","fcf91c3d":"code","a9d755a9":"code","3e7d4399":"code","1f82528a":"code","7b0e93ce":"markdown","eb53474d":"markdown","dfbdc81b":"markdown","cfdec1e5":"markdown","e9a0b3dd":"markdown","e74d9a89":"markdown","d6ebf735":"markdown","49bd2bcd":"markdown","7fde5e64":"markdown","268a0461":"markdown","d32dfbf3":"markdown","489dd53c":"markdown","a5660b0f":"markdown","7423e01a":"markdown","afadca95":"markdown","57e8a998":"markdown","e6a9f828":"markdown","8a739eee":"markdown","13295161":"markdown","fde44c60":"markdown","21ff9664":"markdown","056109cb":"markdown","3cbe3bf1":"markdown"},"source":{"dd9fb662":"#Usual suspects\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('fivethirtyeight')\n\n#To ignore Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","948e3ec4":"#Get the data\niris = pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\")","f42504e7":"#To display top rows\niris.head()","7fe10100":"#Shape of iris dataset\nprint (\"Number of Observations :\",iris.shape[0])\nprint (\"Number of Features\/Columns  :\",iris.shape[1])","4c6f5d0b":"#Infos \niris.info()","e2d809a9":"#Basic descriptive stats\niris.describe()","dc93cbc4":"#Checking for any missing values\niris.isna().sum()","62e91dc1":"#Remove unnecessary feature\niris =iris.drop('Id',axis=1)","7c593596":"#Lets see final dataset shape\niris.shape","bb336a53":"# Visualizing relationship between features :. \nsns.pairplot(iris,hue='Species',palette='Dark2');","d4f6f94a":"#Dataset balance checking\nsns.countplot(iris.Species);","b2089f21":"#Import required ML models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Standard scaler\nfrom sklearn.preprocessing import StandardScaler","8703c49c":"#Separate X and Y varaibles\nX = iris.iloc[:, 0:4].values\ny = iris.iloc[:, 4].values\n\n#train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","214f2801":"#Feature scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test) ","e3670d76":"#Implementing Lda\nlda = LDA(n_components=1)\nX_train_lda = lda.fit_transform(X_train, y_train)\nX_test_lda = lda.transform(X_test)","a5fd7366":"#Lets check our dataset shape\nprint(\"Training data shape\",X_train_lda.shape)","4cf0656b":"#Explaned variance ratio\nlda.explained_variance_ratio_ * 100","2a3f9acc":"#Random forest classifer\nclassifier = RandomForestClassifier(max_depth=2, random_state=0)\n\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)","7eff2ca3":"from sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import accuracy_score\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","7226a977":"#Seaborn heatmap for confusion matrix\nsns.heatmap(cm,cmap='viridis',annot=True);\nprint('Accuracy of test data' ,accuracy_score(y_test, y_pred))","13a9f5f8":"#Classification report\nprint(f\"classification report :\")\nprint(\"\\n\")\nprint(classification_report(y_test, y_pred))","19bfeac9":"from sklearn.decomposition import PCA\npca = PCA(n_components=1).fit(X_train)\n\n#Fitting our model\nX_train_pca = pca.transform(X_train)\nx_test_pca = pca.transform(X_test)","fcf91c3d":"#Lets check variance\nprint(f\"Pca explained varaince ratio (n_comp=1) :{pca.explained_variance_ratio_ *100}\")","a9d755a9":"#Make a model with RF\nclf2 = RandomForestClassifier(max_depth=2,random_state=42)\nclf2.fit(X_train,y_train)\npca_pred = clf2.predict(X_test)","3e7d4399":"##Confusion Matrix\ncm_pca = confusion_matrix(y_test,pca_pred)\nprint(cm_pca)","1f82528a":"##Let's visualize our CM\nsns.heatmap(cm_pca,cmap='viridis',annot=True);\nprint('Accuracy of test data (PCA)' ,accuracy_score(y_test, pca_pred)*100)","7b0e93ce":"## Training and Making Predictions\nSince we want to compare the performance of LDA with one linear discriminant to the performance of PCA with one principal component, we will use the same Random Forest classifier that we used to evaluate performance of PCA-reduced algorithms.","eb53474d":"**We will use Lda ,dimensionality reduction technique to reduce no of features --LDA is supervised machine learning **","dfbdc81b":"Well,Lda outperformed PCA in this case(BUT not all the scenarios).Please Do upvote if you liked this kernel !!\nIf you have any doubts or suggestions pls make a comment..","cfdec1e5":"Let's visualize the data and get you started!\n\n## Data understanding\n\n***The better we understand about our data ,the better the result will be!!***","e9a0b3dd":"## Data Preprocessing\n  The first step is to divide dataset into features and corresponding labels and then divide the resultant dataset into training and test sets. The following code divides data into labels and feature set:","e74d9a89":"As we can see, the Random forest classifier correctly classified everything in the test set\n\n***You can see that with one linear discriminant, the algorithm achieved an accuracy of 100%, which is greater than the accuracy achieved with one principal component***","d6ebf735":"##Lets evaluate our model with confusion matrix","49bd2bcd":"The iris dataset contains measurements for 150 iris flowers from three different species.\n\nThe three classes in the Iris dataset:\n\n    Iris-setosa (n=50)\n    Iris-versicolor (n=50)\n    Iris-virginica (n=50)\n\nThe four features of the Iris dataset:\n\n    sepal length in cm\n    sepal width in cm\n    petal length in cm\n    petal width in cm\n\n## Get the data","7fde5e64":"*There are no missing vslues in our dataset!!!*","268a0461":"## Performing PCA\n\n*PCA is a technique for feature extraction. So it combines our input variables in a specific way, then we can drop the \u201cleast important\u201d variables while still retaining the most valuable parts of all the variables.*","d32dfbf3":"## Evaluating the Performance\nAs always, the last step is to evaluate performance of the algorithm with the help of a confusion matrix and find the accuracy of the prediction. ","489dd53c":"**Looks like LDA is performing better than PCA!!It's evident that after tranforming variables to one LDA explained 99% of the varaiance !!***\n\nLet's Make predictions with pca performed variables","a5660b0f":"### Performing LDA","7423e01a":"## The IRIS Data\nFor this series of lectures, we will be using the famous [Iris flower data set](http:\/\/en.wikipedia.org\/wiki\/Iris_flower_data_set). \n\nThe Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by Sir Ronald Fisher in the 1936 as an example of discriminant analysis. \n\nThe data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor), so 150 total samples. Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n","afadca95":" setosa flower can be easily identified by any ML algos!!Because,it's easy to separte from other flowers.","57e8a998":"## Feature Scaling\nAs was the case with PCA, we need to perform feature scaling for LDA too.","e6a9f828":"***AT this stage,we have 5 features including target feature::***","8a739eee":"***Well,our data is really good balanced !!!***","13295161":"We reduced # of features from 4 to 1..\n\n****Like PCA, we have to pass the value for the **n_components** parameter of the LDA, which refers to the number of linear discriminates that we want to retrieve. In this case we set the** n_components to 1,**** since we first want to check the performance of our classifier with a single linear discriminant. Finally we execute the fit and transform methods to actually retrieve the linear discriminants.****\n\n**Notice, in case of LDA, the transform method takes two parameters: the **X_train** and the **y_train**. However in the case of PCA, the transform method only requires one parameter i.e. X_train. This reflects the fact that LDA takes the output class labels into account while selecting the linear discriminants, while PCA doesn't depend upon the output labels.**","fde44c60":"**We can access the following property to obtain the variance explained by each component.**","21ff9664":"## PCA vs LDA: What to Choose for Dimensionality Reduction?\n\nIn case of uniformly distributed data, LDA almost always performs better than PCA. However if the data is highly skewed (irregularly distributed) then it is advised to use PCA since LDA can be biased towards the majority class.\n\nFinally, it is beneficial that PCA can be applied to labeled as well as unlabeled data since it doesn't rely on the output labels. On the other hand, LDA requires output classes for finding linear discriminants and hence requires labeled data","056109cb":"Let's visualize the data and get you started!\n\n## Exploratory Data Analysis\n\nTime to put your data viz skills to the test! Try to recreate the following plots, make sure to import the libraries you'll need!","3cbe3bf1":"## PCA vs LDA: What's the Difference?\nBoth PCA and LDA are linear transformation techniques. However, PCA is an unsupervised while LDA is a supervised dimensionality reduction technique.\n\nUnlike PCA, LDA tries to reduce dimensions of the feature set while retaining the information that discriminates output classes. LDA tries to find a decision boundary around each cluster of a class\n\n### Import usual libraries:"}}