{"cell_type":{"0c192627":"code","8bd9e649":"code","f5f410dc":"code","d163aa02":"code","e166f863":"code","4fce9091":"code","10441892":"code","431f4115":"code","6ce4305f":"code","055deed3":"code","1bd47976":"code","4d905ad6":"code","e1315438":"code","67cb88e9":"code","ced90503":"code","22f1461d":"code","c4c079a1":"code","b4fb4745":"code","e06a2958":"code","d8ab6341":"code","f566773d":"code","2ea942e2":"markdown","ca15ee52":"markdown","8c4d1758":"markdown","ff70cbf6":"markdown","a68fe6a6":"markdown","41a829c9":"markdown","ae7e0985":"markdown"},"source":{"0c192627":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer, KNNImputer\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.feature_selection import SelectKBest, f_regression,f_classif,chi2\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8bd9e649":"data=pd.read_csv(\"..\/input\/iba-ml1-mid-project\/train.csv\")\n","f5f410dc":"print(data.shape)\nprint(data.columns.values)","d163aa02":"data.head(15)","e166f863":"data.tail(15)","4fce9091":"data.describe()","10441892":"data.dtypes","431f4115":"data=pd.read_csv(\"..\/input\/iba-ml1-mid-project\/train.csv\")\ndata=data.drop(['Id'], axis=1)\ndata[\"credit_line_utilization\"] = data[\"credit_line_utilization\"].str.replace(',', '.')\ndata[\"credit_line_utilization\"] = data[\"credit_line_utilization\"].astype(float)","6ce4305f":"for i in data.columns:\n   \n    x = data[i].value_counts()\n    print(\"Column name is:\\n\",i,\"\\nand it value is:\\n\",x)\n    print() ","055deed3":"comumns=data.columns\ndata.hist(comumns, figsize=(20,15))\n","1bd47976":"X=data.drop(['defaulted_on_loan'], axis=1)\ny=data['defaulted_on_loan']\nrus = RandomUnderSampler(sampling_strategy=1)\nX_rus, y_rus = rus.fit_resample(X, y)\nprint(X_rus.shape) \nprint(y_rus.shape) \ndf_concat = pd.concat([X_rus, y_rus], axis=1)\n","4d905ad6":"X_train, X_test, y_train, y_test = train_test_split(X_rus, y_rus ,shuffle=True,stratify=y_rus)","e1315438":"X_train.isnull().sum()","67cb88e9":"imputer=KNNImputer(missing_values=np.nan, n_neighbors=5)\nX_train_i = imputer.fit_transform(X_train)\nX_train = pd.DataFrame(X_train_i, columns = X_train.columns)","ced90503":"model = DecisionTreeClassifier()\n# fit the model\nmodel.fit(X_train, y_train)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n\tprint('Feature: %s, Score: %.5f' % (X_train.columns[i],v))\n# plot feature importance\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","22f1461d":"fig, axes = plt.subplots(nrows=3, ncols=2,figsize=(5,5))\n\nX_train.boxplot(column=['age'], ax=axes[0,0])\nX_train.boxplot(column=['monthly_income'], ax=axes[0,1]) \nX_train.boxplot(column=['ratio_debt_payment_to_income'], ax=axes[1,0])\nX_train.boxplot(column=['number_dependent_family_members'], ax=axes[1,1]) \nX_train.boxplot(column=['real_estate_loans'], ax=axes[2,0])\nX_train.boxplot(column=['number_of_credit_lines'], ax=axes[2,1]) \nfig.tight_layout()\nplt.show()","c4c079a1":"def detect_outliers(var):\n    q_1, q_3 = np.percentile(var, [25, 75])\n    IQR = q_3 - q_1\n    lower_bound = q_1 - (IQR * 1.5)\n    upper_bound = q_3 + (IQR * 1.5)\n    return np.asarray((var > upper_bound) | (var < lower_bound)).nonzero()","b4fb4745":"X_train = X_train.reset_index(drop=True)\ny_train = y_train.reset_index(drop=True)\n\n\nage_outliers = detect_outliers(X_train['age'])[0]\nndfm_outliers = detect_outliers(X_train['number_dependent_family_members'])[0]\nmonthly_income_outliers = detect_outliers(X_train['monthly_income'])[0]\nnocl_outliers = detect_outliers(X_train['number_of_credit_lines'])[0]\nrel_outliers = detect_outliers(X_train['real_estate_loans'])[0]\nrdpti_outliers = detect_outliers(X_train['ratio_debt_payment_to_income'])[0]\n\noverall_outliers = np.unique(np.concatenate((age_outliers, ndfm_outliers, monthly_income_outliers, nocl_outliers, rel_outliers, rdpti_outliers),axis=None))\noverall_outliers\nX_train = X_train.drop(overall_outliers)\ny_train = y_train.drop(overall_outliers)\n","e06a2958":"fig, axes = plt.subplots(nrows=3, ncols=2,figsize=(5,5))\n\nX_train.boxplot(column=['age'], ax=axes[0,0])\nX_train.boxplot(column=['monthly_income'], ax=axes[0,1]) \nX_train.boxplot(column=['ratio_debt_payment_to_income'], ax=axes[1,0])\nX_train.boxplot(column=['number_dependent_family_members'], ax=axes[1,1]) \nX_train.boxplot(column=['real_estate_loans'], ax=axes[2,0])\nX_train.boxplot(column=['number_of_credit_lines'], ax=axes[2,1]) \nfig.tight_layout()\nplt.show()","d8ab6341":"plt.figure(figsize = (15,15))\nsns.heatmap(df_concat.corr(), annot = True, vmax = 1, vmin = -1, square = True)","f566773d":"from sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nimport umap\nreducer = umap.UMAP()\nembedding = reducer.fit_transform(X_train.values)\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14,8))\nax1.scatter(embedding[:,0], embedding[:,1], c=(y_train == 0))\nax1.set_title('U_Map', fontsize=15)\nembedding = TSNE(n_components=2).fit_transform(X_train.values)\nax2.scatter(embedding[:,0], embedding[:,1], c=(y_train == 0))\nax2.set_title('TNSE', fontsize=15)\nembedding = PCA(n_components=2).fit_transform(X_train.values)\nax3.scatter(embedding[:,0], embedding[:,1], c=(y_train == 0))\nax3.set_title('PCA', fontsize=15)\nplt.show()\n","2ea942e2":"## It is obvious that we dont need [ID] column during EDA and model building,so we will get rid of it and [credit_line_utilization] is actually an object, even though it consists of Floating point values.","ca15ee52":"## Understanding about consistency of each column","8c4d1758":"# Now we are going to look at some joint distributions","ff70cbf6":"# Looking on how Dimentionality reduction looks with our data","a68fe6a6":"# Univariate outlier detection. We will use IQR for detecting outliers and getting rid of them. We will trace this process by looking at boxplots before and after this code block. This process will give us a huge benefits while scaling features and fitting the model.","41a829c9":"## Firstly we will deal with imbalanced distribution and use for this case random oversampler,we will lose considerable amout of data but it will be faster and easier to work with it. Secondly we will do train_test_split, try to impute all missing values by KNN(becouse it is more suitable in this case) and understand feature importance order. ","ae7e0985":"# Uploading dataset, trying to understand general distribution and doing basic analysis. I will do whole EDA only on data that is going to be our training set, but when it comes to test set I will treat it like \"black box\" to have less bias and avoid issues while facing unseen data."}}