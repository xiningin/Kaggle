{"cell_type":{"f4c44181":"code","adf73c11":"code","b1f1c4fd":"code","d9db4475":"code","e161c5d5":"code","7d3cf442":"code","f19dccbd":"code","b0713441":"code","e12643aa":"code","40a15bad":"code","19343640":"code","156754cc":"code","9ec55bcb":"code","4bf10b86":"code","41bff45f":"code","d303ab9f":"code","2da3db88":"code","9aad3501":"code","b40eb27e":"code","b5b47e64":"code","9b153bef":"code","3e49147e":"code","eeab9909":"code","43adff6d":"code","66101c97":"code","22381c4d":"code","8bd880c0":"code","1333b986":"code","3a08b6fd":"code","3db16783":"code","1ec2e875":"code","af9d32ad":"code","560310fa":"code","f5d2a6e0":"code","8af72a58":"code","9fb84142":"code","1c2488bd":"markdown","9d8deb20":"markdown","6f759ed7":"markdown","52d850e1":"markdown","e05462bd":"markdown","64172e2c":"markdown","ea7a8c48":"markdown","c3bc4694":"markdown","fc389b78":"markdown","4b54e925":"markdown","88edd9b5":"markdown","25bd1964":"markdown","fa152911":"markdown","21f4bb18":"markdown","50102915":"markdown","b5726049":"markdown"},"source":{"f4c44181":"!pip install tweepy\nimport tweepy","adf73c11":"from bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\nimport numpy as np\nimport datetime","b1f1c4fd":"CONSUMER_KEY = 'uWgt6k1Mzl6IN1IiEKvHAHaNm'\nCONSUMER_SECRET = 'PNsb7eZpt8AUdN0TzUNZOsvAaxajMmw094nvcTIVlRd2ne64yx'\nACCESS_KEY = '1061225034407206912-v5VBoAVQddfuZdSABSDZH8azyQXmLm'\nACCESS_SECRET = 'eiwbyRCFz1TeKXVFyIVAJ3RGqoJ5HLoMeqX4NaFs9lfrU'","d9db4475":"auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\nauth.set_access_token(ACCESS_KEY, ACCESS_SECRET)\napi = tweepy.API(auth)","e161c5d5":"# Lucknow, Patna, Ranchi, Amritsar, Ahmedabad, Mumbai, Hyderabad, Bangalore, Chennai\n# Chicago, Detroit, NYC, Washington\n# London, Manchester, Newcastle, Liverpool\n# Paris\n# Berlin\n\nwoeid = {\"Worldwide\": [1],\n        \"India\": [2295377, 2295381, 2295383, 2295388, 2295402, 2295411, 2295414, 2295420, 2295424],\n         \"US\": [2379574, 2391585, 2459115, 2514815],\n         \"UK\": [44418, 28218, 30079, 26734],\n         \"France\": [615702],\n         \"Germany\": [638242]\n        }","7d3cf442":"twitter_trends = []\n\nfor country in woeid:\n    for state_code in woeid[country]:\n        trends1 = api.trends_place(state_code)\n        trends = [trend['name'][1:] for trend in trends1[0]['trends']]\n        twitter_trends.extend(trends)","f19dccbd":"twitter_tags = []\n\nfor trend in twitter_trends:\n    if trend not in twitter_tags:\n        twitter_tags.append(trend)","b0713441":"print(len(twitter_tags), twitter_tags)","e12643aa":"def all_hashtag(keyword):\n    try:\n        url = \"https:\/\/www.all-hashtag.com\/library\/contents\/ajax_generator.php\"\n        payload = {'keyword': keyword,\n                   'filter': 'top'}\n        files = []\n        headers = {}\n        response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n        text = str(response.text)\n        soup = BeautifulSoup(text, features=\"html.parser\")\n        tags_str = \"\"\n        for val in soup.find_all(\"div\", class_=\"copy-hashtags\"):\n            tags_str = val.get_text()\n        tags_str = tags_str.split(\" \")\n        tags = list()\n        for tag in tags_str:\n            if tag:\n                tags.append(tag.replace(\"\\n\", \"\"))\n        return tags\n    except Exception as ex:\n        print(ex)\n        return False","40a15bad":"def best_hashtag_handler(keyword):\n    try:\n        url = \"http:\/\/best-hashtags.com\/hashtag\/{}\/\".format(keyword)\n        response = requests.request(\"POST\", url)\n        soup = BeautifulSoup(response.text, features=\"html.parser\")\n        hashtags = list()\n        all_text = ''\n        for val in soup.find_all(\"table\", class_=\"table\"):\n            all_text = val.get_text()\n        all_text = all_text.split(\"\\n\")\n        for val in all_text:\n            if val.startswith(\"#\") and val != '#':\n                hashtags.append(val)\n        return hashtags\n    except Exception as ex:\n        print(ex)\n        return False","19343640":"def top_hashtags(keyword):\n    tags_with_popularity = {}\n    try:\n        pages = 4\n        for i in range(1, pages + 1):\n            url = 'https:\/\/top-hashtags.com\/search\/?q={}&opt=top&sp={}'.format(keyword, i)\n            req = requests.get(url)\n            html_doc = req.text\n            soup = BeautifulSoup(html_doc, \"html5lib\")\n            for litag in soup.find_all('li', {'class': 'i-row'}):\n                currentKey = None\n                for div in litag.find_all('div', {'class': 'i-tag'}):\n                    currentKey = div.text\n                for div in litag.find_all('div', {'class': 'i-total'}):\n                    tags_with_popularity[currentKey] = div.text\n        return tags_with_popularity\n    except Exception as ex:\n        print(ex)\n        return False","156754cc":"import string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport re","9ec55bcb":"lemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer()\n\nstop_words = set(stopwords.words('english')) \n\npunctuation_to_remove = string.punctuation","4bf10b86":"EMOTICONS = {\n    u\":\u2011\\)\":\"Happy face or smiley\",\n    u\":\\)\":\"Happy face or smiley\",\n    u\":-\\]\":\"Happy face or smiley\",\n    u\":\\]\":\"Happy face or smiley\",\n    u\":-3\":\"Happy face smiley\",\n    u\":3\":\"Happy face smiley\",\n    u\":->\":\"Happy face smiley\",\n    u\":>\":\"Happy face smiley\",\n    u\"8-\\)\":\"Happy face smiley\",\n    u\":o\\)\":\"Happy face smiley\",\n    u\":-\\}\":\"Happy face smiley\",\n    u\":\\}\":\"Happy face smiley\",\n    u\":-\\)\":\"Happy face smiley\",\n    u\":c\\)\":\"Happy face smiley\",\n    u\":\\^\\)\":\"Happy face smiley\",\n    u\"=\\]\":\"Happy face smiley\",\n    u\"=\\)\":\"Happy face smiley\",\n    u\":\u2011D\":\"Laughing, big grin or laugh with glasses\",\n    u\":D\":\"Laughing, big grin or laugh with glasses\",\n    u\"8\u2011D\":\"Laughing, big grin or laugh with glasses\",\n    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n    u\"X\u2011D\":\"Laughing, big grin or laugh with glasses\",\n    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n    u\":-\\)\\)\":\"Very happy\",\n    u\":\u2011\\(\":\"Frown, sad, andry or pouting\",\n    u\":-\\(\":\"Frown, sad, andry or pouting\",\n    u\":\\(\":\"Frown, sad, andry or pouting\",\n    u\":\u2011c\":\"Frown, sad, andry or pouting\",\n    u\":c\":\"Frown, sad, andry or pouting\",\n    u\":\u2011<\":\"Frown, sad, andry or pouting\",\n    u\":<\":\"Frown, sad, andry or pouting\",\n    u\":\u2011\\[\":\"Frown, sad, andry or pouting\",\n    u\":\\[\":\"Frown, sad, andry or pouting\",\n    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n    u\">:\\[\":\"Frown, sad, andry or pouting\",\n    u\":\\{\":\"Frown, sad, andry or pouting\",\n    u\":@\":\"Frown, sad, andry or pouting\",\n    u\">:\\(\":\"Frown, sad, andry or pouting\",\n    u\":'\u2011\\(\":\"Crying\",\n    u\":'\\(\":\"Crying\",\n    u\":'\u2011\\)\":\"Tears of happiness\",\n    u\":'\\)\":\"Tears of happiness\",\n    u\"D\u2011':\":\"Horror\",\n    u\"D:<\":\"Disgust\",\n    u\"D:\":\"Sadness\",\n    u\"D8\":\"Great dismay\",\n    u\"D;\":\"Great dismay\",\n    u\"D=\":\"Great dismay\",\n    u\"DX\":\"Great dismay\",\n    u\":\u2011O\":\"Surprise\",\n    u\":O\":\"Surprise\",\n    u\":\u2011o\":\"Surprise\",\n    u\":o\":\"Surprise\",\n    u\":-0\":\"Shock\",\n    u\"8\u20110\":\"Yawn\",\n    u\">:O\":\"Yawn\",\n    u\":-\\*\":\"Kiss\",\n    u\":\\*\":\"Kiss\",\n    u\":X\":\"Kiss\",\n    u\";\u2011\\)\":\"Wink or smirk\",\n    u\";\\)\":\"Wink or smirk\",\n    u\"\\*-\\)\":\"Wink or smirk\",\n    u\"\\*\\)\":\"Wink or smirk\",\n    u\";\u2011\\]\":\"Wink or smirk\",\n    u\";\\]\":\"Wink or smirk\",\n    u\";\\^\\)\":\"Wink or smirk\",\n    u\":\u2011,\":\"Wink or smirk\",\n    u\";D\":\"Wink or smirk\",\n    u\":\u2011P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"X\u2011P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":\u2011\u00de\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":\u00de\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":\u2011\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":\u2011\\|\":\"Straight face\",\n    u\":\\|\":\"Straight face\",\n    u\":$\":\"Embarrassed or blushing\",\n    u\":\u2011x\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":\u2011#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":\u2011&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\"O:\u2011\\)\":\"Angel, saint or innocent\",\n    u\"O:\\)\":\"Angel, saint or innocent\",\n    u\"0:\u20113\":\"Angel, saint or innocent\",\n    u\"0:3\":\"Angel, saint or innocent\",\n    u\"0:\u2011\\)\":\"Angel, saint or innocent\",\n    u\"0:\\)\":\"Angel, saint or innocent\",\n    u\":\u2011b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n    u\">:\u2011\\)\":\"Evil or devilish\",\n    u\">:\\)\":\"Evil or devilish\",\n    u\"\\}:\u2011\\)\":\"Evil or devilish\",\n    u\"\\}:\\)\":\"Evil or devilish\",\n    u\"3:\u2011\\)\":\"Evil or devilish\",\n    u\"3:\\)\":\"Evil or devilish\",\n    u\">;\\)\":\"Evil or devilish\",\n    u\"\\|;\u2011\\)\":\"Cool\",\n    u\"\\|\u2011O\":\"Bored\",\n    u\":\u2011J\":\"Tongue-in-cheek\",\n    u\"#\u2011\\)\":\"Party all night\",\n    u\"%\u2011\\)\":\"Drunk or confused\",\n    u\"%\\)\":\"Drunk or confused\",\n    u\":-###..\":\"Being sick\",\n    u\":###..\":\"Being sick\",\n    u\"<:\u2011\\|\":\"Dump\",\n    u\"\\(>_<\\)\":\"Troubled\",\n    u\"\\(>_<\\)>\":\"Troubled\",\n    u\"\\(';'\\)\":\"Baby\",\n    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(~_~;\\) \\(\u30fb\\.\u30fb;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-\\)zzz\":\"Sleeping\",\n    u\"\\(\\^_-\\)\":\"Wink\",\n    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n    u\"\\(\\+o\\+\\)\":\"Confused\",\n    u\"\\(o\\|o\\)\":\"Ultraman\",\n    u\"\\^_\\^\":\"Joyful\",\n    u\"\\(\\^_\\^\\)\/\":\"Joyful\",\n    u\"\\(\\^O\\^\\)\uff0f\":\"Joyful\",\n    u\"\\(\\^o\\^\\)\uff0f\":\"Joyful\",\n    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"\\('_'\\)\":\"Sad or Crying\",\n    u\"\\(\/_;\\)\":\"Sad or Crying\",\n    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n    u\"\\(;_;\":\"Sad of Crying\",\n    u\"\\(;_:\\)\":\"Sad or Crying\",\n    u\"\\(;O;\\)\":\"Sad or Crying\",\n    u\"\\(:_;\\)\":\"Sad or Crying\",\n    u\"\\(ToT\\)\":\"Sad or Crying\",\n    u\";_;\":\"Sad or Crying\",\n    u\";-;\":\"Sad or Crying\",\n    u\";n;\":\"Sad or Crying\",\n    u\";;\":\"Sad or Crying\",\n    u\"Q\\.Q\":\"Sad or Crying\",\n    u\"T\\.T\":\"Sad or Crying\",\n    u\"QQ\":\"Sad or Crying\",\n    u\"Q_Q\":\"Sad or Crying\",\n    u\"\\(-\\.-\\)\":\"Shame\",\n    u\"\\(-_-\\)\":\"Shame\",\n    u\"\\(\u4e00\u4e00\\)\":\"Shame\",\n    u\"\\(\uff1b\u4e00_\u4e00\\)\":\"Shame\",\n    u\"\\(=_=\\)\":\"Tired\",\n    u\"\\(=\\^\\\u00b7\\^=\\)\":\"cat\",\n    u\"\\(=\\^\\\u00b7\\\u00b7\\^=\\)\":\"cat\",\n    u\"=_\\^=\t\":\"cat\",\n    u\"\\(\\.\\.\\)\":\"Looking down\",\n    u\"\\(\\._\\.\\)\":\"Looking down\",\n    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n    u\"\\(\\\u30fb\\\u30fb?\":\"Confusion\",\n    u\"\\(?_?\\)\":\"Confusion\",\n    u\">\\^_\\^<\":\"Normal Laugh\",\n    u\"<\\^!\\^>\":\"Normal Laugh\",\n    u\"\\^\/\\^\":\"Normal Laugh\",\n    u\"\\\uff08\\*\\^_\\^\\*\uff09\" :\"Normal Laugh\",\n    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n    u\"\\(^\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n    u\"\\(\\^\u2014\\^\\\uff09\":\"Normal Laugh\",\n    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n    u\"\\\uff08\\^\u2014\\^\\\uff09\":\"Waving\",\n    u\"\\(;_;\\)\/~~~\":\"Waving\",\n    u\"\\(\\^\\.\\^\\)\/~~~\":\"Waving\",\n    u\"\\(-_-\\)\/~~~ \\($\\\u00b7\\\u00b7\\)\/~~~\":\"Waving\",\n    u\"\\(T_T\\)\/~~~\":\"Waving\",\n    u\"\\(ToT\\)\/~~~\":\"Waving\",\n    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n    u\"\\(\\*_\\*\\)\":\"Amazed\",\n    u\"\\(\\*_\\*;\":\"Amazed\",\n    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n    u'\\(-\"-\\)':\"Worried\",\n    u\"\\(\u30fc\u30fc;\\)\":\"Worried\",\n    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n    u\"\\(\\\uff3e\uff56\\\uff3e\\)\":\"Happy\",\n    u\"\\(\\\uff3e\uff55\\\uff3e\\)\":\"Happy\",\n    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n    u\"\\(\\^O\\^\\)\":\"Happy\",\n    u\"\\(\\^o\\^\\)\":\"Happy\",\n    u\"\\)\\^o\\^\\(\":\"Happy\",\n    u\":O o_O\":\"Surprised\",\n    u\"o_0\":\"Surprised\",\n    u\"o\\.O\":\"Surpised\",\n    u\"\\(o\\.o\\)\":\"Surprised\",\n    u\"oO\":\"Surprised\",\n    u\"\\(\\*\uffe3m\uffe3\\)\":\"Dissatisfied\",\n    u\"\\(\u2018A`\\)\":\"Snubbed or Deflated\"\n}\nemoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')","41bff45f":"emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)","d303ab9f":"def preprocess_text(text):\n    text = text.lower()\n    text = \" \".join([stemmer.stem(word) for word in text.split()])\n    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n    text = emoji_pattern.sub(r'', text)\n    text = emoticon_pattern.sub(r'', text)\n    return text","2da3db88":"text = \"We want love and support \u2764\ufe0f\u2764\ufe0f\u2764\ufe0f\"","9aad3501":"text = \" \".join([word for word in str(text).split() if '@' not in word])\ntext = \" \".join([word for word in str(text).split() if word not in stop_words])\ntext = text.translate(str.maketrans('', '', punctuation_to_remove))","b40eb27e":"twitter_df = pd.DataFrame({\"hashtag\": twitter_tags})\ntwitter_df['hashTagLower'] = twitter_df['hashtag'].apply(lambda x: x.lower())","b5b47e64":"trending_on_twitter = []\n\nfor word in text.split():\n    word = word.lower()\n    if word[0] == '#':\n        word = word[1:]\n    for i in range(twitter_df.shape[0]):\n        if word in twitter_df.iloc[i][\"hashTagLower\"]:\n            hashtag_to_add = '#' + twitter_df.iloc[i][\"hashtag\"]\n            if hashtag_to_add not in trending_on_twitter:\n                trending_on_twitter.append(hashtag_to_add)\n            \nprint(f'Trending on twitter: {trending_on_twitter}')","9b153bef":"preprocess_text = preprocess_text(text)\nunique_text = []\n\nfor txt in preprocess_text.split():\n    if txt not in unique_text:\n        unique_text.append(txt)\n        \nunique_text = \" \".join(unique_text)","3e49147e":"unique_text","eeab9909":"pos = nltk.pos_tag(unique_text.split()) \n\ntags = []\nfilter_pos = [\"VB\", \"VBN\", \"VBP\", \"VBZ\", \"RB\", \"RBS\", \"RBR\", \"JJ\", \"JJR\", \"JJS\", \"NN\", \"NNS\"]\n\nfor word in pos:\n    if word[1] in filter_pos:\n        tags.append(word[0])","43adff6d":"tags","66101c97":"hashTags = []\nhashTagFor = []\npopularity = []\ncommon = []","22381c4d":"for tag in tags:\n    ht = []\n    ht.extend(all_hashtag(tag))\n    ht.extend(best_hashtag_handler(tag))\n    hashTags.extend(ht)\n    hashTagFor.extend([tag] * len(ht))\n    popularity.extend([0] * len(ht))\n    common.extend([1] * len(ht))","8bd880c0":"for tag in tags:\n    tags_with_popularity = top_hashtags(tag)\n    for key in tags_with_popularity:\n        hashTags.append(key)\n        hashTagFor.append(tag) \n        common.append(1)\n        val = tags_with_popularity[key]\n        if 'K' in val:\n            popularity.append(round(float(val[:-1]) * 1000, 2))\n        elif 'M' in val:\n            popularity.append(round(float(val[:-1]) * 1000000, 2))\n        else:\n             popularity.append(0)","1333b986":"df = pd.DataFrame({\"word\": hashTagFor, \"hashTag\": hashTags, \"popularity\": popularity, \"common\": common})\ndf['hashTagLower'] = df['hashTag'].apply(lambda x: x.lower()[1:])\ndf[\"popularity\"] = pd.to_numeric(df[\"popularity\"], errors='coerce')\ndf = df.drop_duplicates(subset='hashTag', keep='first')\ndf = df.sort_values(by=\"popularity\", ascending=False)","3a08b6fd":"df.head()","3db16783":"# convert common to %\ndf[\"common\"] = df[\"common\"] \/ 3 ","1ec2e875":"df.head()","af9d32ad":"# This part of code is unused\n\n\n# unique_hashtags = list(df[\"word\"].unique())\n# # suggest_hashtags = {\"hashtag\": [], \"popularity\": [], \"trending\": []}\n# suggest_hashtags = {\"hashtag\": [], \"popularity\": []}\n\n# # add unique most popular hashtags of each word\n# for i in range(df.shape[0]):\n#     current_word = df.iloc[i][\"word\"]\n#     if current_word in unique_hashtags:\n#         suggest_hashtags[\"hashtag\"].append(df.iloc[i][\"hashTag\"])\n#         suggest_hashtags[\"popularity\"].append(df.iloc[i][\"popularity\"])\n# #         if df.iloc[i][\"hashTag\"] in trending:\n# #              suggest_hashtags[\"trending\"].append(\"Trending\")\n# #         else:\n# #             suggest_hashtags[\"trending\"].append(\"Not Trending\")\n#         unique_hashtags.remove(current_word)\n\n# #  add insta\/gram hashtags\n# for i in range(df.shape[0]):\n#     current_hashtag = df.iloc[i][\"hashTagLower\"]\n#     if \"insta\" in current_hashtag or \"gram\" in current_hashtag:\n#         if df.iloc[i][\"popularity\"] > 0:\n#             suggest_hashtags[\"hashtag\"].append(df.iloc[i][\"hashTag\"])\n#             suggest_hashtags[\"popularity\"].append(df.iloc[i][\"popularity\"])\n# #             if df.iloc[i][\"hashTag\"] in trending:\n# #                  suggest_hashtags[\"trending\"].append(\"Trending\")\n# #             else:\n# #                 suggest_hashtags[\"trending\"].append(\"Not Trending\")","560310fa":"insta_hashtag_suggest = {\"hashtag\": [], \"score\": [], \"popularity\": []}\n\ncount = 0\ntargetCount = 26\n\nwhile count < targetCount and count < df.shape[0]:\n    score = 0\n    current_hashtag = df.iloc[count][\"hashTag\"]\n    current_popularity = df.iloc[count][\"popularity\"]\n    if current_popularity >= 100000 and current_popularity <= 1000000:\n        score = 7\n    if current_popularity >= 1000000 and current_popularity <= 5000000:\n        score = 8\n    if current_popularity >= 5000000:  \n        score = 9\n    insta_hashtag_suggest[\"score\"].append(score)\n    insta_hashtag_suggest[\"hashtag\"].append(current_hashtag)\n    insta_hashtag_suggest[\"popularity\"].append(current_popularity)\n    count += 1","f5d2a6e0":"print(\"Trending hashtags on Instagram with popularity scores: \")\nprint(\"_________________________________________________________\")\nprint(insta_hashtag_suggest)","8af72a58":"print(f'Trending hashtags on twitter: {trending_on_twitter}')","9fb84142":"# Banned hashtags\nbanned_tags = [\n        \"alone\", \"always\", \"armparty\", \"adulting\", \"assday\", \"ass\", \"abdl\", \"assworship\", \"addmysc\", \"asiangirl\",\n        \"beautyblogger\", \"brain\", \"boho\", \"besties\", \"bikinibody\", \"costumes\", \"curvygirls\",\n        \"date\", \"dating\", \"desk\", \"dm\", \"direct\", \"elevator\", \"eggplant\", \"edm\", \n        \"fuck\", \"girlsonly\", \"gloves\", \"graffitiigers\", \"happythanksgiving\", \n        \"hawks\", \"hotweather\", \"humpday\", \"hustler\", \"ilovemyinstagram\", \"instababy\", \"instasport\", \"iphonegraphy\", \"italiano\", \"ice\", \n        \"killingit\", \"kansas\", \"kissing\", \"kickoff\", \"leaves\", \"like\", \"lulu\", \"lean\", \n        \"master\", \"milf\", \"mileycyrus\", \"models\", \"mustfollow\", \"nasty\", \"newyearsday\", \"nude\", \"nudism\", \"nudity\", \n        \"overnight\", \"orderweedonline\", \"parties\", \"petite\", \"pornfood\", \"pushups\", \"prettygirl\", \n        \"rate\", \"ravens\", \"samelove\", \"selfharm\", \"skateboarding\", \"skype\", \"snap\", \"snapchat\", \"single\", \"singlelife\", \"stranger\",\n        \"saltwater\", \"shower\", \"shit\", \"sopretty\", \"sunbathing\", \"streetphoto\", \"swole\", \"snowstorm\", \"sun\", \"sexy\", \n        \"tanlines\", \"todayimwearing\", \"teens\", \"teen\", \"thought\", \"tag4like\", \"tagsforlikes\", \"thighs\", \"undies\", \"valentinesday\",\n        \"workflow\", \"wtf\", \"xanax\", \"youngmodel\"\n]","1c2488bd":"Filter out the duplicate hashtags","9d8deb20":"# Provide the text","6f759ed7":"Theses are the tags","52d850e1":"Start pre-processing the text to get instagram hashtags","e05462bd":"# Now the hashtags are extracted and we to create a dataframe for further computations","64172e2c":"# Setting up the instagram hashtags with some customized scores based on popularity","ea7a8c48":"Provide the woeid codes of the regions from which you want to get the hashtags","c3bc4694":"Check if any hashtag is trending on the twitter trending hashtag list we created earlier","fc389b78":"# The preprocessing methods for extracting meaningful text","4b54e925":"Take top 25 hashtags based on popularity","88edd9b5":"# The twitter hashtags are fetched now we will start with instagram hashtags","25bd1964":"# These are the banned hashtags","fa152911":"# Set up developer twitter auths","21f4bb18":"See all the trending hashtags on twitter","50102915":"# Search and use webscraping on these websites for getting instagram hashtags\n* https:\/\/www.all-hashtag.com\n* http:\/\/best-hashtags.com\n* https:\/\/top-hashtags.com","b5726049":"# Search and extract all the hashtags from the above 3 websites. (Use the tags list)"}}