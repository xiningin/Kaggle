{"cell_type":{"22d880e6":"code","a05f5cd5":"code","77f02a92":"code","96d3a7c3":"code","802ac00c":"code","c4f71bfe":"code","4fc700fc":"code","e958f80c":"code","3d845f7c":"code","d2871642":"code","f30be400":"code","59d7724d":"code","6109fce8":"code","e9f6b232":"code","c008c110":"code","d3e917e1":"code","ad507d00":"code","b5e04800":"code","260553d4":"code","2dfa79bf":"code","268f486f":"code","332ab762":"code","d7d2e5b6":"code","34537aa0":"code","dd859e2a":"code","96a0a025":"code","f8ab3657":"code","e4f4915a":"code","58988faf":"code","934e5352":"code","56762285":"code","e37d8d39":"code","6abc18f3":"code","48f21587":"code","7b4e24ed":"code","cbe60313":"code","137ab496":"code","2c6a1bf5":"code","2a7ec9d8":"code","1ea1527e":"code","eac90490":"code","1965efa7":"code","acb493d3":"code","78b68d42":"code","5d340016":"code","de083b80":"code","7b52a27a":"code","2b48d765":"code","ca720273":"code","dc5b897e":"code","12b92587":"code","24f2360e":"code","78d67dca":"code","10b86cf2":"code","ae3b3313":"code","9ecf4601":"code","0dfd27a3":"code","0428ec31":"code","76154b9d":"code","28b642fe":"code","0e36289a":"code","31c46bfd":"code","efaadae5":"code","68b890b8":"code","eafb8d77":"code","8add19ff":"code","449996af":"code","a2f52d8a":"code","c340a791":"code","a8efd86b":"code","74dfc825":"code","1bd14e8f":"code","fe6d0608":"code","44c29ccf":"code","3fbcf993":"code","a0cb4e10":"code","19f37d1e":"code","0bb68401":"code","1a401572":"code","ea242905":"code","a67f22fd":"code","f2a1faa8":"code","ae48117c":"code","e0ee9db8":"code","87683663":"code","dcb0314e":"code","d295b768":"code","de002e77":"code","ef0c7e09":"code","ff9ead01":"code","9b6d91e0":"code","b0ef8c27":"code","c26fb994":"code","387f19c8":"code","459698aa":"code","786b3f6b":"code","ec9739b6":"code","06109047":"code","9ce28468":"code","29e80e92":"code","fca713b5":"code","4b5e1a18":"code","dc224052":"code","a6912c83":"code","10756a5c":"code","3f7e3783":"code","87f4d552":"code","f3215af4":"code","ccc90f87":"code","8164c54c":"code","e0d6baae":"code","5761c7f9":"code","90faa0d4":"code","952349a1":"code","296a58d5":"code","f976ac6a":"code","ef1d0d66":"code","20a2d4ee":"code","d98b2604":"code","933de4cc":"code","9b3a4900":"code","69e0c524":"code","9c867723":"code","e46a8fee":"code","85d79c77":"code","7b9a633e":"code","fcf5ddab":"code","c3601c81":"code","b3f31a4e":"code","3a45a424":"code","0c67c90f":"code","1c828475":"code","ac2c56c6":"code","3b1ed6cd":"code","139d3706":"code","9cd8fd1c":"code","cb600153":"code","2186bef0":"code","845ce5c9":"code","bdcaf56d":"code","c41574a5":"code","b0cf75cf":"code","ee4cf85a":"code","46eecd98":"code","e5de366b":"code","33a155dd":"code","0e1a26fb":"code","cfcd746a":"code","f5dd0395":"code","ebd20a4c":"code","776ec144":"code","a6332e00":"code","ee560b81":"code","b8bfd6ae":"code","39a1d7c5":"code","f83435ec":"code","476966ca":"code","6d0c5a53":"code","0af710fe":"code","b3ef7f95":"code","2a846ba0":"code","0620c3d5":"code","fb9ea601":"code","1fada2d6":"code","c76660ca":"code","5091c780":"code","a21c1877":"code","2337e15f":"code","b4c36bfe":"code","4dbb7f59":"markdown","6306b1a8":"markdown","120cb635":"markdown","614171c6":"markdown","aa68f38e":"markdown","2027b269":"markdown","c8931338":"markdown","5f341b95":"markdown","9ccd4efd":"markdown","f18d4f73":"markdown","e32c24e7":"markdown","fa261307":"markdown","dcd0e643":"markdown","e54c53ae":"markdown","bf5ada66":"markdown","d83bc012":"markdown","1a490847":"markdown","3671322d":"markdown","875d5ce8":"markdown","447d586f":"markdown","00e1122a":"markdown","26651580":"markdown","f283de58":"markdown","3d09dc84":"markdown","5e5db0bc":"markdown","56c21413":"markdown","fc1799d0":"markdown","25cabd1d":"markdown","3c75703d":"markdown","914016d8":"markdown","68f9e215":"markdown","4d39f348":"markdown","bb24f660":"markdown","d0c5874d":"markdown","d221918a":"markdown","48ea0fad":"markdown","545a1aaf":"markdown","2907e926":"markdown","2a742209":"markdown","171f380e":"markdown","d44b8b4a":"markdown","932569fc":"markdown","2b161173":"markdown","8badd73b":"markdown","7a69894e":"markdown","c2d7b6ed":"markdown","8de67e5f":"markdown","c982c689":"markdown","dc9101ae":"markdown","5cf7a4fd":"markdown","d935ea6d":"markdown","975f97c9":"markdown","273b6819":"markdown","739586de":"markdown","ae095046":"markdown","08242c4a":"markdown","95b61cdc":"markdown","94c0cffe":"markdown","cafc186e":"markdown","bd923370":"markdown","d3ddc259":"markdown","d284069c":"markdown","483dd17b":"markdown","5efabd81":"markdown","5d0f1dc4":"markdown","e5a591a2":"markdown","bf40c3d7":"markdown","51b90022":"markdown","5de7401f":"markdown","8d9eb582":"markdown","606c56d1":"markdown","909d08a6":"markdown","8f250b75":"markdown","f38e5e30":"markdown","0241e412":"markdown","15a551f1":"markdown","fcbcf06c":"markdown","ba9b44e4":"markdown","0239dc5f":"markdown","d3e35417":"markdown","0baacd3c":"markdown","da455eb1":"markdown","0accccc7":"markdown","77ef6d9c":"markdown","68447a62":"markdown","265ad245":"markdown","25e056b3":"markdown","c184ca7d":"markdown","1b6f84b8":"markdown","01ac637a":"markdown","d140e43b":"markdown","6578c8a8":"markdown","d99b4996":"markdown","7a10339a":"markdown","86c7e4bd":"markdown","0959ffc9":"markdown","1b762eb9":"markdown","f3e520bc":"markdown","cc28d56f":"markdown","e37bc547":"markdown","8875bb6f":"markdown","ea54357b":"markdown","1f9549c7":"markdown","29964568":"markdown","c8e9407b":"markdown","2902fc8e":"markdown","2cffc365":"markdown","0dad62bc":"markdown","801db6e5":"markdown","e4631b2e":"markdown","d30c3c5d":"markdown","20a7ab9e":"markdown","9d333268":"markdown","fe4d814f":"markdown","1d32a6b3":"markdown","ff9d6e46":"markdown","08d4de5a":"markdown","4c2a8d56":"markdown","5fd67f5a":"markdown","07abd887":"markdown","7377a5d6":"markdown","f0fe5a7a":"markdown","eecd8f9c":"markdown","0e1b14a9":"markdown","2fe087c7":"markdown","28e4a8b9":"markdown","0c52e142":"markdown","1c393be6":"markdown","5fbe5669":"markdown","e4f58212":"markdown","d77cf032":"markdown","4c15de83":"markdown","8aaf1b02":"markdown","c52b25bd":"markdown","ef801431":"markdown","07f22c2b":"markdown","5454b782":"markdown","36dbb636":"markdown","df188974":"markdown","1def2cfa":"markdown","2e998e13":"markdown","9fbd23cc":"markdown","7da912ba":"markdown","76b14fc3":"markdown","5ef68022":"markdown","d3a11e53":"markdown","15e90db8":"markdown","47b5f288":"markdown","c195f9e2":"markdown","5548813f":"markdown","ce3e7a5b":"markdown","032317e4":"markdown","c80ac3c5":"markdown","f23c6a8f":"markdown"},"source":{"22d880e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a05f5cd5":"import pickle\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nsns.set(font_scale=1.5)\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\npd.set_option('display.max_rows',None)\npd.set_option('display.max_column',None)\nimport warnings\nwarnings.filterwarnings(\"ignore\")","77f02a92":"from sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, RandomForestRegressor, BaggingRegressor, ExtraTreesRegressor\nfrom sklearn.model_selection import cross_val_score, GridSearchCV","96d3a7c3":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","802ac00c":"train.head()","c4f71bfe":"test.head()","4fc700fc":"# Find the columns which have missing value\nnan_columns = train.columns[train.isna().any()]\n\n# Print the precentage of missing value in each column\n(train[nan_columns].isna().sum().sort_values(ascending=False)\/len(train))*100","e958f80c":"# Find the columns which have missing value\nnan_columns = test.columns[test.isna().any()]\n\n# Print the precentage of missing value in each column\n(test[nan_columns].isna().sum().sort_values(ascending=False)\/len(test))*100","3d845f7c":"# Look at the information of the columns in train dataset:\ntrain.info()","d2871642":"# Look at the information of the columns in test dataset:\ntest.info()","f30be400":"# Make copy of train and test dataset for cleaning process\ntrain_a = train.copy()\ntest_a = test.copy()\n\n# Drop the unnecessary columns \ntrain_a = train_a.drop(columns=['Id','PoolQC','MiscFeature','Alley','Fence'])\ntest_a = test_a.drop(columns=['Id','PoolQC','MiscFeature','Alley','Fence'])","59d7724d":"def subplot_histograms(column_name):\n    # Define two subplots\n    fig, ax = plt.subplots(nrows=1,ncols=2,figsize=(14,5))\n\n    # Assign the histogram of selected column in train dataset to the first subplot\n    train[column_name].hist(ax=ax[0],grid=False,legend=True);\n\n    # Assign the histogram of selected column in test dataset to the second subplot\n    test[column_name].hist(ax=ax[1],grid=False,legend=True);\n\n    # Assign the titles to subplots\n    ax[0].set_title('Train dataset')\n    ax[1].set_title('Test dataset')\n\n    # Assign the ylabel to subplots\n    ax[0].set_ylabel('Count')\n    ax[1].set_ylabel('Count')\n    plt.show()","6109fce8":"# Call function to draw histgram of LotFrontage\nsubplot_histograms('LotFrontage')","e9f6b232":"# Show records of the missing values in lot frontage of train dataset\ntrain[train.LotFrontage.isna()].head()","c008c110":"# Show records of the missing values in lot frontage of test dataset\ntest[test.LotFrontage.isna()].head()","d3e917e1":"# Replace the missing value in lot frontage in both dataset with their median\ntrain_a.LotFrontage.fillna(train_a.LotFrontage.median(),inplace=True)\ntest_a.LotFrontage.fillna(test_a.LotFrontage.median(),inplace=True);","ad507d00":"# Call function to draw histgram of MasVnrArea\nsubplot_histograms('MasVnrArea')","b5e04800":"# Show records of the missing values in MasVnrArea of train dataset\ntrain[train.MasVnrArea.isna()].head()","260553d4":"# Show records of the missing values in MasVnrArea of test dataset\ntest[test.MasVnrArea.isna()].head()","2dfa79bf":"# Replace the missing values in MasVnrArea in both dataset with 0.0 since their type not defined\ntrain_a.MasVnrArea.fillna(0.0,inplace=True)\ntest_a.MasVnrArea.fillna(0.0,inplace=True)\n\n# While the MasVnrType is missing and related with MasVnrArea, we relpace the missing with NA\ntrain_a.MasVnrType.fillna('Na',inplace=True)\ntest_a.MasVnrType.fillna('Na',inplace=True);","268f486f":"# Call function to draw histgram of GarageType\nsubplot_histograms('GarageType')","332ab762":"# Show records of the missing values in GarageType of train dataset\ntrain[train.GarageType.isna()].head()","d7d2e5b6":"# Show records of the missing values in GarageType of test dataset\ntest[test.GarageType.isna()].head()","34537aa0":"# Replace the missing values in GarageType and related columns in both dataset with 0.0 for numerical and NA for non-numerical\n# Because the missing values of related columns have same records of GarageType missing values\ntrain_a.GarageYrBlt.fillna(0.0,inplace=True)\ntest_a.GarageYrBlt.fillna(0.0,inplace=True)\ntrain_a.GarageType.fillna('NA',inplace=True)\ntest_a.GarageType.fillna('NA',inplace=True)\ntrain_a.GarageFinish.fillna('NA',inplace=True)\ntest_a.GarageFinish.fillna('NA',inplace=True)\ntrain_a.GarageQual.fillna('NA',inplace=True)\ntest_a.GarageQual.fillna('NA',inplace=True)\ntrain_a.GarageCond.fillna('NA',inplace=True)\ntest_a.GarageCond.fillna('NA',inplace=True)\ntest_a.GarageArea.fillna(0.0,inplace=True)\ntest_a.GarageCars.fillna(0.0,inplace=True);","dd859e2a":"# Call function to draw histgram of BsmtCond\nsubplot_histograms('BsmtCond')","96a0a025":"# Show records of the missing values in BsmtCond of train dataset\ntrain[train.BsmtCond.isna()].head()","f8ab3657":"# Show records of the missing values in BsmtCond of test dataset\ntest[test.BsmtCond.isna()].head()","e4f4915a":"# Replace the missing values in BsmtCond and related columns in both dataset with NA\n# Because the missing values of related columns have same records of BsmtCond missing values\ntrain_a.BsmtFinType1.fillna('NA',inplace=True)\ntest_a.BsmtFinType1.fillna('NA',inplace=True)\n\ntrain_a.BsmtFinType2.fillna('NA',inplace=True)\ntest_a.BsmtFinType2.fillna('NA',inplace=True)\n\ntrain_a.BsmtExposure.fillna('NA',inplace=True)\ntest_a.BsmtExposure.fillna('NA',inplace=True)\n\ntrain_a.BsmtCond.fillna('NA',inplace=True)\ntest_a.BsmtCond.fillna('NA',inplace=True)\n\ntrain_a.BsmtQual.fillna('NA',inplace=True)\ntest_a.BsmtQual.fillna('NA',inplace=True);  ","58988faf":"# Call function to draw histgram of BsmtCond\nsubplot_histograms('FireplaceQu')","934e5352":"# Define the dataframe that contains numercial columns\nnumeric_columns = train_a.select_dtypes(include=np.number)\n\n# Drop the correlated columns as shown in part 7 of this section\nnumeric_columns.drop(columns=['GrLivArea','GarageYrBlt','GarageCars','OverallQual'],inplace=True)\n\n# Merge the numercial columns with target column\ntrain_firep = pd.concat([numeric_columns,train_a.FireplaceQu],axis=1)\n\n# Assign the records of missing value of FireplaceQu to variable\nfireplace_null = train_firep.loc[train_firep.FireplaceQu.isna()]\n\n# Assign the records of non-missing value of FireplaceQu to variable\nfireplace_true = train_firep.dropna(subset=['FireplaceQu'])\n\n# Define the train set without target for model\nX_train_firep = fireplace_true.drop('FireplaceQu',axis=1)\n\n# Define the target values of train set for model\ny_train_firep = fireplace_true.FireplaceQu\n\n# Define the test set for model\nX_test_firep = fireplace_null.drop('FireplaceQu',axis=1)","56762285":"# Do Scaling for data\nss = StandardScaler()\nXs_train_firep = ss.fit_transform(X_train_firep)\nXs_test_firep = ss.transform(X_test_firep)","e37d8d39":"# Initialize the model\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# Fitting the model\nknn.fit(Xs_train_firep, y_train_firep);","6abc18f3":"# Predicting for k = 3\ny_pred_firep = knn.predict(X_test_firep)\n\n# Define the dataframe that contain the filled FireplaceQu values\nfireplace_filled = pd.DataFrame(y_pred_firep,columns=['FireplaceQu'],index=fireplace_null.index)\n\n# Merge the filled FireplaceQu with missing FireplaceQu dataframe as column\nfireplace_null = pd.concat([fireplace_null.drop('FireplaceQu',axis=1),fireplace_filled],axis=1)","48f21587":"# Merge the filled FireplaceQu dataframe with non-missing FireplaceQu dataframe by rows\nfireplace_full = pd.concat([fireplace_null,fireplace_true],axis=0)\n\n# Sorting the index of rows in dataframe\nfireplace_full = fireplace_full.sort_index()\n\n# Merge the completed FireplaceQu column with train datset as column\ntrain_b = pd.concat([train_a.drop('FireplaceQu',axis=1),fireplace_full.FireplaceQu],axis=1)","7b4e24ed":"# Plot the histogram of Electrical in train dataset\ntrain.Electrical.hist(grid=False,legend=True)\n\n# Set ylable for histogram\nplt.ylabel('Count');","cbe60313":"# Show records of the missing values in Electrical of train dataset\ntrain[train.Electrical.isna()]","137ab496":"# Replace the missing value in Electrical with most frequent value\ntrain_a.Electrical.fillna('SBrkr',inplace=True);","2c6a1bf5":"# Plot the count plot of MSZoning by BldgType in test dataset\nsns.countplot(x=\"BldgType\", hue=\"MSZoning\", data=test)\n\n# Assign location of lengend to the right corner\nplt.legend(loc='upper right');","2a7ec9d8":"# Show records of the missing values in MSZoning of test dataset\ntest[test.MSZoning.isna()]","1ea1527e":"# Replace the missing value in MSZoning with the most frequent value\ntest_a.MSZoning.fillna('RL',inplace=True);","eac90490":"# Plot the histogram of BsmtFullBath in test dataset\ntest.BsmtFullBath.hist(grid=False,legend=True)\n\n# Set ylable for histogram\nplt.ylabel('Count');","1965efa7":"# Show records of the missing values in BsmtFullBath of test dataset\ntest[test.BsmtFullBath.isna()]","acb493d3":"# Replace the missing value in BsmtFullBath and BsmtHalfBath with 0.0\ntest_a.BsmtFullBath.fillna(0.0,inplace=True)\ntest_a.BsmtHalfBath.fillna(0.0,inplace=True);","78b68d42":"# Plot the histogram of Utilities in test dataset\ntest.Utilities.hist(grid=False,legend=True)\n\n# Set ylable for histogram\nplt.ylabel('Count');","5d340016":"# Show records of the missing values in Utilities of test dataset\ntest[test.Utilities.isna()]","de083b80":"# Replace the missing value in Utilities with the most frequent value\ntest_a.Utilities.fillna('AllPub',inplace=True);","7b52a27a":"# Plot the histogram of Functional in test dataset\ntest.Functional.hist(grid=False,legend=True)\n\n# Set ylable for histogram\nplt.ylabel('Count');","2b48d765":"# Show records of the missing values in Functional of test dataset\ntest[test.Functional.isna()]","ca720273":"# Replace the missing value in Functional with the most frequent value\ntest_a.Functional.fillna('Typ',inplace=True);","dc5b897e":"# Define the subplots with figsize\nplt.subplots(figsize=(14,6))\n\n# Plot the count plot of Exterior1st by Street in test dataset\nsns.countplot(x=\"Street\", hue=\"Exterior1st\", data=test)\n\n# Assign location of lengend to the right corner\nplt.legend(loc='upper right');","12b92587":"# Show records of the missing values in Exterior1st of test dataset\ntest[test.Exterior1st.isna()]","24f2360e":"# Replace the missing value in Exterior1st with the most frequent value\ntest_a.Exterior1st.fillna('VinylSd',inplace=True);","78d67dca":"# Define the subplots with figsize\nplt.subplots(figsize=(14,6))\n\n# Plot the count plot of Exterior2nd by Street in test dataset\nsns.countplot(x=\"Street\", hue=\"Exterior2nd\", data=test)\n\n# Assign location of lengend to the right corner\nplt.legend(loc='upper right');","10b86cf2":"# Replace the missing value in VinylSd with the most frequent value\ntest_a.Exterior2nd.fillna('VinylSd',inplace=True);","ae3b3313":"# Define the subplots with figsize\nplt.subplots(figsize=(12,6))\n\n# Plot the histogram of SaleType in test dataset\ntest.SaleType.hist(grid=False,legend=True)\n\n# Set ylable for histogram\nplt.ylabel('Count');","9ecf4601":"# Show records of the missing values in SaleType of test dataset\ntest[test.SaleType.isna()]","0dfd27a3":"# Replace the missing value in SaleType with the most frequent value\ntest_a.SaleType.fillna('WD',inplace=True)","0428ec31":"# Plot the histogram of BsmtFinSF1 in test dataset\ntest.BsmtFinSF1.hist(grid=False,legend=True)\n\n# Set ylable for histogram\nplt.ylabel('Count');","76154b9d":"# Show records of the missing values in BsmtFinSF1 of test dataset\ntest[test.BsmtFinSF1.isna()]","28b642fe":"# Replace the missing value in BsmtFinSF1 and related columns with 0.0\ntest_a.BsmtFinSF1.fillna(0.0,inplace=True)\ntest_a.BsmtFinSF2.fillna(0.0,inplace=True)\ntest_a.BsmtUnfSF.fillna(0.0,inplace=True)\ntest_a.TotalBsmtSF.fillna(0.0,inplace=True);","0e36289a":"# Define the subplots with specific figsize\nplt.subplots(figsize=(14,6))\n\n# Plot the count plot of KitchenQual by BldgType in test dataset\nsns.countplot(x=\"BldgType\", hue=\"KitchenQual\", data=test)\n\n# Assign location of lengend to the right corner\nplt.legend(loc='upper right');","31c46bfd":"# Show records of the missing values in KitchenQual of test dataset\ntest[test.KitchenQual.isna()]","efaadae5":"# Replace the missing value in KitchenQual with the most frequent value\ntest_a.KitchenQual.fillna('TA',inplace=True);","68b890b8":"# Define the dataframe that contains required columns\n# Here we used the columns that used for filling FireplaceQu in train dataset\ntest_firep = test_a[train_firep.drop('SalePrice',axis=1).columns]\n\n# Assign the records of missing value of FireplaceQu to variable\nfireplace_null = test_firep.loc[test_firep.FireplaceQu.isna()]\n\n# Assign the records of non-missing value of FireplaceQu to variable\nfireplace_true = test_firep.dropna(subset=['FireplaceQu'])\n\n# Define the train set without target for model\nX_train_firep = fireplace_true.drop('FireplaceQu',axis=1)\n\n# Define the target values of train set for model\ny_train_firep = fireplace_true.FireplaceQu\n\n# Define the test set for model\nX_test_firep = fireplace_null.drop('FireplaceQu',axis=1)","eafb8d77":"# Do Scaling for data\nss = StandardScaler()\nXs_train_firep = ss.fit_transform(X_train_firep)\nXs_test_firep = ss.transform(X_test_firep)","8add19ff":"# Initialize the model\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# Fitting the model\nknn.fit(Xs_train_firep, y_train_firep);","449996af":"# Predicting for k = 3\ny_pred_firep = knn.predict(X_test_firep)\n\n# Define the dataframe that contain the filled FireplaceQu values\nfireplace_filled = pd.DataFrame(y_pred_firep,columns=['FireplaceQu'],index=fireplace_null.index)\n\n# Merge the filled FireplaceQu with missing FireplaceQu dataframe as column\nfireplace_null = pd.concat([fireplace_null.drop('FireplaceQu',axis=1),fireplace_filled],axis=1)","a2f52d8a":"# Merge the filled FireplaceQu dataframe with non-missing FireplaceQu dataframe by rows\nfireplace_full = pd.concat([fireplace_null,fireplace_true],axis=0)\n\n# Sorting the index of rows in dataframe\nfireplace_full = fireplace_full.sort_index()\n\n# Merge the completed FireplaceQu column with train datset as column\ntest_b = pd.concat([test_a.drop('FireplaceQu',axis=1),fireplace_full.FireplaceQu],axis=1)","c340a791":"# Use corr() function to calculate the correlation between numerical columns\ncorr_matrix = train_a.corr()\n\n# Change the format of correlation matrix from wide to long format\ncorr_matrix_unstack = corr_matrix.unstack()\n\n# Sort the rows by correlation coefficient\ncorr_matrix_sort = corr_matrix_unstack.sort_values().reset_index()\n\n# Show the strong positively correlated columns except the correlation with target\ncorr_matrix_sort[(corr_matrix_sort[0]>0.65) & (corr_matrix_sort[0]<1.0)&(corr_matrix_sort.level_0 != 'SalePrice')&(corr_matrix_sort.level_1 != 'SalePrice')]","a8efd86b":"# Show the strong negatively correlated columns except the correlation with target\ncorr_matrix_sort[(corr_matrix_sort[0]<-0.65) & (corr_matrix_sort[0]>-1.0)&(corr_matrix_sort.level_0 != 'SalePrice')&(corr_matrix_sort.level_1 != 'SalePrice')]","74dfc825":"# Define the dataframe which contains the categorical columns\ncateg_columns = train_b.select_dtypes(include=np.object).columns\n\n# Define the list which contains the percentage of the most frequent value in each column\nprecent_list = [round(train_b[column].value_counts()[0]\/len(train_b),3)*100 for column in categ_columns]\n\n# Define the list which contains the most frequent value in each column\nvalue_list = [train_b[column].value_counts().index[0] for column in categ_columns]\n\n# Define the dataframe which contains the categorical columns and their most frequent value and percentage\nhigh_frequent_df = pd.DataFrame({'Value':value_list,'Precentage' : precent_list},index=categ_columns)\n\n# Show the columns which has repetition precentage of value more than %50\nhigh_frequent_df[high_frequent_df.Precentage >= 50]","1bd14e8f":"# Define the list contains the columns name which have repetition precentage of value more than %50\nbiased_columns = high_frequent_df[high_frequent_df.Precentage >= 50].index.tolist()\n\n# Define the list contains the columns name which have strong correlation\ncorrelated_columns = ['GarageCars', 'GrLivArea', 'TotalBsmtSF', 'TotRmsAbvGrd']\n\n# Combine the selected columns into one list\nunwanted_columns = biased_columns+correlated_columns\n\n# Remove the columns from both datasets\ntrain_b.drop(columns=unwanted_columns,inplace=True)\ntest_b.drop(columns=unwanted_columns,inplace=True)","fe6d0608":"# Define subplots for categorical columns\nfig, ax = plt.subplots(nrows=int(len(train_b.select_dtypes([np.object]).columns)\/2),ncols=2,figsize=(20,30))\n\n# Set padding between figures edge\nfig.tight_layout(pad=7)\n\n# Ravel turns a matrix into a vector, which is easier to iterate\nax = ax.ravel() \n\nfor index,column in enumerate(train_b.select_dtypes([np.object]).columns):\n    plot = sns.countplot(x=column,data=train_b,ax=ax[index]);\n    # Rotate the x tick labels by 90 degree\n    plt.setp(plot.get_xticklabels(), rotation=90)","44c29ccf":"# Do One Hot Encoding on Neighborhood, BsmtFinType1 and GarageFinish\nNeighborhood_ = pd.get_dummies(train_b.Neighborhood,prefix='Neighborhood')\nGarageFinish_ = pd.get_dummies(train_b.GarageFinish,prefix='GarageFinish')\nBsmtFinType1_ = pd.get_dummies(train_b.BsmtFinType1,prefix='BsmtFinType1')","3fbcf993":"# Merge the numerical columns with categorical dummy columns in train dataset\ntrain_c = pd.concat([train_b.select_dtypes(include=np.number),BsmtFinType1_,Neighborhood_,GarageFinish_],axis=1)\n\n# Define X_train for modeling\nX_train = train_c.drop(columns=['SalePrice'])","a0cb4e10":"# selected_columns = ['Neighborhood','GarageFinish','BsmtFinType1']\nNeighborhood_ = pd.get_dummies(test_b.Neighborhood,prefix='Neighborhood')\nGarageFinish_ = pd.get_dummies(test_b.GarageFinish,prefix='GarageFinish')\nBsmtFinType1_ = pd.get_dummies(test_b.BsmtFinType1,prefix='BsmtFinType1')","19f37d1e":"# Merge the numerical columns with categorical dummy columns in test dataset\ntest_c = pd.concat([test_b.select_dtypes(include=np.number),Neighborhood_,GarageFinish_,BsmtFinType1_],axis=1)\n\n# Define X_test for modeling\nX_test = test_c","0bb68401":"# Do Scaling for data\nss = StandardScaler()\nXs_train = ss.fit_transform(X_train)\n\n# Define y_test for modeling\ny_train = train_c.SalePrice","1a401572":"# Initialize the model\ndtr = DecisionTreeRegressor(random_state=1)\n\n# Fitting the model\ndtr.fit(Xs_train, y_train)\n\n# Get importance\nimportance = dtr.feature_importances_\n\n# Define the dataframe of column importance\ndf_feature_selected = pd.DataFrame({'Column Name':X_train.columns.tolist(),\"Importance\":importance.tolist()})\n\n# Show the columns which have importance is more than 0.001\ndf_feature_selected[df_feature_selected.Importance >= 0.001].sort_values('Importance',ascending=False)","ea242905":"# Assign the columns which have importance is more than 0.001 as predictors of X_train_a and X_test_a\nX_train_a = X_train[df_feature_selected[df_feature_selected.Importance >= 0.001]['Column Name'].values.tolist()]\nX_test_a = X_test[df_feature_selected[df_feature_selected.Importance >= 0.001]['Column Name'].values.tolist()]","a67f22fd":"train_b.describe()","f2a1faa8":"# Plot the scatter plot of OverallQual and SalePrice in train dataset\nsns.scatterplot(x='OverallQual',y='SalePrice',data=train_b)\n\n# Hide the grid in the plot\nplt.grid(False)\nplt.show();","ae48117c":"plt.subplots(figsize=(8,6))\n\n# Plot the box plot of HouseStyle by SalePrice in train dataset\nsns.boxplot(x= 'HouseStyle', y= 'SalePrice',data= train_b,palette='Set3')\nplt.show()","e0ee9db8":"# Plot the count plot of Foundation with grouping by GarageFinish in train dataset\nsns.countplot(x='Foundation',hue='GarageFinish',data=test_b,palette='coolwarm_r')\nplt.legend(loc='upper right');","87683663":"# Group data by YearBuilt then calculate the mean of lotarea \ntrain_grouped = train_b.groupby(['YearBuilt']).agg({'LotArea': 'mean'}).sort_values(by=['LotArea'],ascending=False).reset_index()\ntrain_grouped.head(1)","dcb0314e":"# Use corr() function to calculate the correlation between numerical columns\ncorr_matrix = train_b.corr()\n\n# Change the format of correlation matrix from wide to long format\ncorr_matrix_unstack = corr_matrix.unstack()\n\n# Sort the rows by correlation coefficient\ncorr_matrix_sort = corr_matrix_unstack.sort_values().reset_index()\n\n# Show the strong positively correlated columns with the target 'SalePrice'\ncorr_matrix_sort[(corr_matrix_sort[0]>0.6) & (corr_matrix_sort[0]<1.0)&(corr_matrix_sort.level_0 == 'SalePrice')]","d295b768":"train_b.SalePrice.mean()","de002e77":"# Do Scaling for data\nXs_train_a = ss.fit_transform(X_train_a)\nXs_test_a = ss.transform(X_test_a)","ef0c7e09":"# Build Linear Regression model \nlr = LinearRegression()\n\n# Fitting the model\nlr.fit(Xs_train_a,y_train);","ff9ead01":"lr.score(Xs_train_a,y_train)","9b6d91e0":"# Perform 5-fold cross validation\nscores = cross_val_score(lr,Xs_train_a,y_train,cv=5)\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean cross-validated scores:\", scores.mean())","b0ef8c27":"# Build AdaBoost Regression model by linear regression with its best parameters\nlr_boost = AdaBoostRegressor(base_estimator=lr)\n\n# Define the parameters of GridSearchCV \nparams = {\n 'n_estimators': [50, 60, 70, 80 ,90 ,100],\n 'learning_rate' : [0.01, 0.05, 0.1, 0.5],\n 'loss' : ['linear', 'square', 'exponential']\n }\n\ngs_lr = GridSearchCV(lr_boost,\n                  params, \n                  scoring='r2',\n                  verbose=1,\n                 n_jobs=4)\n\n# Fitting the model\ngs_lr.fit(Xs_train_a, y_train)","c26fb994":"# Getting the best parameters of estimator\ngs_lr.best_params_","387f19c8":"score = gs_lr.score(Xs_train_a, y_train)\nscore","459698aa":"# Getting the best cross validation score of estimator\ncross_value_score = gs_lr.best_score_\ncross_value_score","786b3f6b":"# Define the list which have the best score and cross validation score of each model\nbest_scores = []\n\n# Add the score and cross validation score of linear regression with AdaBoost and GS_CV to the list\nbest_scores.append(['LR with AdaBoost and GS_CV',score,cross_value_score])","ec9739b6":"# Build Lasso regression model \nlasso = Lasso()\n\n# Fitting the model\nlasso.fit(Xs_train_a, y_train)\nlasso.score(Xs_train_a,y_train)","06109047":"# Perform 5-fold cross validation\nscores = cross_val_score(lasso,Xs_train_a,y_train,cv=5)\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean cross-validated scores:\", scores.mean())","9ce28468":"# Build LassoCV regression model \nlassocv = LassoCV(alphas=np.logspace(.1, 10, 30),random_state=1,n_jobs=5,verbose=1,cv=5)\n\n# Fitting the model\nlassocv.fit(Xs_train_a,y_train)\nprint('The optimal value for lasso regression alpha is ',lassocv.alpha_)","29e80e92":"# Build Lasso regression model with best alpha \nlr_lassocv = Lasso(alpha=lassocv.alpha_)\n\n# Fitting the model\nlr_lassocv.fit(Xs_train_a,y_train)\nlr_lassocv.score(Xs_train_a,y_train)","fca713b5":"# Perform 5-fold cross validation\nscores = cross_val_score(lr_lassocv,Xs_train_a,y_train,cv=5)\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean cross-validated scores:\", scores.mean())","4b5e1a18":"# Build AdaBoost Regression model by lasso with its best parameters \nlr_lassocv_boost = AdaBoostRegressor(base_estimator=lr_lassocv,random_state=1)\n\n# Define the parameters of GridSearchCV \nparams = {\n 'n_estimators': [50, 60, 70, 80 ,90 ,100],\n 'learning_rate' : [0.01, 0.05, 0.1, 0.5],\n 'loss' : ['linear', 'square', 'exponential']\n }\n\ngs_lr_lassocv = GridSearchCV(lr_lassocv_boost,\n                  params, \n                  scoring='r2',\n                  verbose=1,\n                 n_jobs=5)\n\n# Fitting the model\ngs_lr_lassocv.fit(Xs_train_a, y_train)","dc224052":"# Getting the best parameters of estimator\ngs_lr_lassocv.best_params_","a6912c83":"score = gs_lr_lassocv.score(Xs_train_a, y_train)\nscore","10756a5c":"# Getting the best cross validation score of estimator\ncross_value_score = gs_lr_lassocv.best_score_\ncross_value_score","3f7e3783":"# Add the score and cross validation score of lassoCV with AdaBoost and GS_CV to the list\nbest_scores.append(['LassoCV with AdaBoost and GS_CV',score,cross_value_score])","87f4d552":"# Build Ridge regression model \nridge = Ridge()\n\n# Fitting the model\nridge.fit(Xs_train_a, y_train)\nridge.score(Xs_train_a,y_train)","f3215af4":"# Perform 5-fold cross validation\nscores = cross_val_score(ridge,Xs_train_a,y_train,cv=5)\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean cross-validated scores:\", scores.mean())","ccc90f87":"# Build RidgeCV regression model \nridgecv = RidgeCV(alphas=np.logspace(.1, 10, 30),scoring='r2',cv=5)\nridgecv.fit(Xs_train_a,y_train)\nprint('The optimal value for lasso regression alpha is ',ridgecv.alpha_)","8164c54c":"# Build Ridge regression model with best alpha \nlr_ridgecv = Ridge(alpha=ridgecv.alpha_)\nlr_ridgecv.fit(Xs_train_a,y_train)\nlr_ridgecv.score(Xs_train_a,y_train)","e0d6baae":"# Perform 5-fold cross validation\nscores = cross_val_score(lr_ridgecv,Xs_train_a,y_train,cv=5)\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean cross-validated scores:\", scores.mean())","5761c7f9":"# Build AdaBoost Regression model by Ridge with its best parameters\nlr_ridgecv_boost = AdaBoostRegressor(base_estimator=lr_ridgecv,random_state=1)\n\n# Define the parameters of GridSearchCV \nparams = {\n 'n_estimators': [50, 60, 70, 80 ,90 ,100],\n 'learning_rate' : [0.01, 0.05, 0.1, 0.5],\n 'loss' : ['linear', 'square', 'exponential']\n }\n\ngs_lr_ridgecv = GridSearchCV(lr_ridgecv_boost,\n                  params, \n                  scoring='r2',\n                  verbose=1,\n                 n_jobs=5)\n\n# Fitting the model\ngs_lr_ridgecv.fit(Xs_train_a, y_train)","90faa0d4":"# Getting the best parameters of estimator\ngs_lr_ridgecv.best_params_","952349a1":"score = gs_lr_ridgecv.score(Xs_train_a, y_train)\nscore","296a58d5":"# Getting the best cross validation score of estimator\ncross_value_score = gs_lr_ridgecv.best_score_\ncross_value_score","f976ac6a":"# Add the score and cross validation score of lassoCV with AdaBoost and GS_CV to the list\nbest_scores.append(['RidgeCV with AdaBoost and GS_CV',score,cross_value_score])","ef1d0d66":"# Build ElasticNet regression model \nelastic_net = ElasticNet()\n\n# Fitting the model\nelastic_net.fit(Xs_train_a,y_train)\nelastic_net.score(Xs_train_a,y_train)","20a2d4ee":"# Perform 5-fold cross validation\nscores = cross_val_score(elastic_net,Xs_train_a,y_train,cv=5)\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean cross-validated scores:\", scores.mean())","d98b2604":"# Build ElasticNetCV regression model \nelastic_netcv = ElasticNetCV(cv=5, random_state=1,l1_ratio=[.3,.6,.9],alphas=np.linspace(0.01, .99, 100),tol=0.1)\nelastic_netcv.fit(Xs_train_a,y_train)\nprint('The optimal value for elastic net regression alpha is %.5f and L1 ratio is %.2f' % (elastic_netcv.alpha_,elastic_netcv.l1_ratio_))","933de4cc":"# Build ElasticNetCV regression model \nlr_elastic_netcv = ElasticNet(alpha=elastic_netcv.alpha_,l1_ratio=elastic_netcv.l1_ratio_)\n\n# Fitting the model\nlr_elastic_netcv.fit(Xs_train_a,y_train)\nlr_elastic_netcv.score(Xs_train_a,y_train)","9b3a4900":"# Perform 5-fold cross validation\nscores = cross_val_score(lr_elastic_netcv,Xs_train_a,y_train,cv=5)\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean cross-validated scores:\", scores.mean())","69e0c524":"# Build AdaBoost Regression model by ElasticNet with its best parameters\nlr_elastic_netcv_boost = AdaBoostRegressor(base_estimator=lr_elastic_netcv,random_state=1)\n\n# Define the parameters of GridSearchCV \nparams = {\n 'n_estimators': [50, 60, 70, 80 ,90 ,100],\n 'learning_rate' : [0.01, 0.05, 0.1, 0.5],\n 'loss' : ['linear', 'square', 'exponential']\n }\n\ngs_lr_elastic_netcv = GridSearchCV(lr_elastic_netcv_boost,\n                  params, \n                  scoring='r2',\n                  verbose=1,\n                 n_jobs=5)\n\n# Fitting the model\ngs_lr_elastic_netcv.fit(Xs_train_a, y_train)","9c867723":"# Getting the best parameters of estimator\ngs_lr_elastic_netcv.best_params_","e46a8fee":"score = gs_lr_elastic_netcv.score(Xs_train_a, y_train)\nscore","85d79c77":"# Getting the best cross validation score of estimator\ncross_value_score = gs_lr_elastic_netcv.best_score_\ncross_value_score","7b9a633e":"# Add the score and cross validation score of ElasticNetCV with AdaBoost and GS_CV to the list\nbest_scores.append(['ElasticNetCV with AdaBoost and GS_CV',score,cross_value_score])","fcf5ddab":"# Build KNN regression model \nknn = KNeighborsRegressor()\n\n# Fitting the model\nknn.fit(Xs_train_a, y_train)\nknn.score(Xs_train_a,y_train)","c3601c81":"# Perform 5-fold cross validation\nscores = cross_val_score(knn,Xs_train_a,y_train,cv=5)\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean cross-validated scores:\", scores.mean())","b3f31a4e":"# Define the parameters of GridSearchCV \nparam_grid = {\n    'n_neighbors': np.linspace(2,12,num=12,dtype=int),\n    'metric': ['minkowski','euclidean']\n}\n\ngs_knn = GridSearchCV(knn, \n                  param_grid, \n                  cv=5,\n                  scoring='r2',\n                  verbose=1,\n                 n_jobs=3)\n\n# Fitting the model\ngs_knn.fit(Xs_train_a, y_train)","3a45a424":"# Getting the best parameters of estimator\ngs_knn.best_params_","0c67c90f":"gs_knn.score(Xs_train_a,y_train)","1c828475":"# Getting the best cross validation score of estimator\ngs_knn.best_score_","ac2c56c6":"# Build K-Nearest Neighbors Regression model with its best parameters \nknn_best = KNeighborsRegressor(metric='minkowski', n_neighbors=8)\nknn_best.fit(Xs_train_a,y_train)\n\n# Build AdaBoost Regression model by optimezed K-Nearest Neighbors Regression\nknn_boost = AdaBoostRegressor(base_estimator=knn_best,random_state=1)\n\n# Define the parameters of GridSearchCV \nparams = {\n 'n_estimators': [50, 75 ,100],\n 'learning_rate' : [0.01, 0.05, 0.1, 0.5],\n 'loss' : ['linear', 'square', 'exponential']\n }\n\ngs_knn = GridSearchCV(knn_boost,\n                  params, \n                  scoring='r2',\n                  verbose=1,\n                 n_jobs=5)\n\n# Fitting the model\ngs_knn.fit(Xs_train_a, y_train)","3b1ed6cd":"# Getting the best parameters of estimator\ngs_knn.best_params_","139d3706":"score = gs_knn.score(Xs_train_a,y_train)\nscore","9cd8fd1c":"# Getting the best cross validation score of estimator\ncross_value_score = gs_knn.best_score_\ncross_value_score","cb600153":"# Add the score and cross validation score of KNN with AdaBoost and GS_CV to the list\nbest_scores.append(['KNN with AdaBoost and GS_CV',score,cross_value_score])","2186bef0":"# Build Decision Tree regression model \ndtr = DecisionTreeRegressor()\n\n# Fitting the model\ndtr.fit(Xs_train_a, y_train)\ndtr.score(Xs_train_a,y_train)","845ce5c9":"# Perform 5-fold cross validation\nscores = cross_val_score(dtr,Xs_train_a,y_train,cv=5)\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean cross-validated scores:\", scores.mean())","bdcaf56d":"dtr_bagging = BaggingRegressor(base_estimator=DecisionTreeRegressor(),random_state=1)\n\n# Define the parameters of GridSearchCV \nparam_grid = {\n    'n_estimators': np.linspace(10,100,num=10,dtype=int),\n    'max_features' : np.linspace(0.1,0.9,num=9),\n    'max_samples' : [0.3,0.4,0.5,0.6, 0.7, 0.8, 0.9],\n}\n\ngs_dtr = GridSearchCV(dtr_bagging, \n                  param_grid, \n                  cv=5,\n                  scoring='r2',\n                  verbose=1,\n                 n_jobs=5)\n\n# Fitting the model\ngs_dtr.fit(Xs_train_a, y_train)","c41574a5":"# Getting the best parameters of estimator\ngs_dtr.best_params_","b0cf75cf":"score = gs_dtr.score(Xs_train_a,y_train)\nscore","ee4cf85a":"# Getting the best cross validation score of estimator\ncross_value_score = gs_dtr.best_score_\ncross_value_score","46eecd98":"# Add the score and cross validation score of KNN with AdaBoost and GS_CV to the list\nbest_scores.append(['DT with Bagging and GS_CV',score,cross_value_score])","e5de366b":"# Build Random Forest regression model \nrf = RandomForestRegressor(random_state=1)\n\n# Fitting the model\nrf.fit(Xs_train_a,y_train)\nrf.score(Xs_train_a,y_train)","33a155dd":"# Perform 5-fold cross validation\nscores = cross_val_score(rf,Xs_train_a,y_train,cv=5)\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean cross-validated scores:\", scores.mean())","0e1a26fb":"# Define the parameters of GridSearchCV \nparam_grid = {\n    'n_estimators': np.linspace(80, 130, num=6,dtype=int),\n    'max_features' : [0.6,0.7,0.8],\n    'max_depth': np.linspace(1,5,num=5,dtype=int),\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf' : [1,2,4]\n}\n\ngs_rf = GridSearchCV(rf, \n                  param_grid, \n                  cv=5,\n                  scoring='r2',\n                  verbose=1,\n                 n_jobs=5)\n\n# Fitting the model\ngs_rf.fit(Xs_train_a, y_train)","cfcd746a":"gs_rf.score(Xs_train_a,y_train)","f5dd0395":"# Getting the best parameters of estimator\ngs_rf.best_params_","ebd20a4c":"# Getting the best cross validation score of estimator\ngs_rf.best_score_","776ec144":"score = rf.score(Xs_train_a,y_train)\ncross_value_score = scores.mean()\n\n# Add the score and cross validation score of KNN with AdaBoost and GS_CV to the list\nbest_scores.append(['RF with Default Parameters',score,cross_value_score])","a6332e00":"# Build ExtraTree regression model \netr = ExtraTreesRegressor(n_estimators=5,random_state=1)\netr.fit(Xs_train_a,y_train)\netr.score(Xs_train_a,y_train)","ee560b81":"# Perform 5-fold cross validation\nscores = cross_val_score(etr,Xs_train_a,y_train,cv=5)\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean cross-validated scores:\", scores.mean())","b8bfd6ae":"# Define the parameters of GridSearchCV \nparam_grid = {\n    'n_estimators': np.linspace(10,100,num=10,dtype=int),\n    'max_features' : np.linspace(0.1,0.9,num=9),\n    'max_samples' : np.linspace(0.3, 0.9,num=7),\n}\n\ngs_etr = GridSearchCV(etr, \n                  param_grid, \n                  cv=5,\n                  scoring='r2',\n                  verbose=1,\n                 n_jobs=5)\n\n# Fitting the model\ngs_etr.fit(Xs_train_a, y_train)","39a1d7c5":"# Getting the best parameters of estimator\ngs_etr.best_params_","f83435ec":"score = gs_etr.score(Xs_train_a,y_train)\nscore","476966ca":"# Getting the best cross validation score of estimator\ncross_value_score = gs_etr.best_score_\ncross_value_score","6d0c5a53":"# Add the score and cross validation score of ET with GS_CV to the list\nbest_scores.append(['ET with Bagging',score,cross_value_score])","0af710fe":"# Build Gradient Boosting regression model \ngbr = GradientBoostingRegressor()\n\n# Fitting the model\ngbr.fit(Xs_train_a,y_train)\ngbr.score(Xs_train_a,y_train)","b3ef7f95":"# Perform 5-fold cross validation\nscores = cross_val_score(gbr,Xs_train_a,y_train,cv=5)\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean cross-validated scores:\", scores.mean())","2a846ba0":"gbr = GradientBoostingRegressor(random_state=1)\n\n# Define the parameters of GridSearchCV \nparam_grid = {\n    'learning_rate': np.linspace(0.01,0.1,10),\n    'subsample'    : np.linspace(0.5,0.9,4),\n    'n_estimators' : np.linspace(50,200,4,dtype=int),\n    'max_depth'    : np.linspace(1,5,5,dtype=int)\n}\n\ngs_gbr = GridSearchCV(gbr,\n                  param_grid, \n                  cv=5,\n                  scoring='r2',\n                  verbose=1,\n                 n_jobs=5)\n\n# Fitting the model\ngs_gbr.fit(Xs_train_a, y_train)","0620c3d5":"# Getting the best parameters of estimator\ngs_gbr.best_params_","fb9ea601":"score = gs_gbr.score(Xs_train_a,y_train)\nscore","1fada2d6":"# Getting the best cross validation of estimator\ncross_value_score = gs_gbr.best_score_\ncross_value_score","c76660ca":"# Add the score and cross validation score of Gradient Boosting with GS_CV to the list\nbest_scores.append(['Gradient Boosting with GS_CV',score,cross_value_score])","5091c780":"# Define the dataframe contains the best score and cross validation score in each model\nbest_scores_df = pd.DataFrame(best_scores, columns=['Model','Score','Cross Validation Score'])\nbest_scores_df","a21c1877":"# Define the subplots with specific figsize\nfig, ax = plt.subplots(figsize=(15,4))\n\n# Sort the best scores dataframe by Cross Validation Score\nsort_best_score_df = best_scores_df.sort_values('Cross Validation Score',ascending=False)\n\n# Plot the Cross Validation Score and Score per model by bar plot\nplot = sort_best_score_df.plot(kind=\"bar\",x='Model',y=['Cross Validation Score', 'Score'],sort_columns=True,ax=ax)\n\n# Rotate the x tick labels by 90 degree\nplt.setp(plot.get_xticklabels(), rotation=80)\n\n# Assign location of lengend to the right corner\nplt.legend(loc='upper right',bbox_to_anchor=(1.30, 1), ncol=1);","2337e15f":"filename = 'finalized_gs_gbr_model.sav'\n\n# Save the optimal model\npickle.dump(gs_gbr, open(filename, 'wb'))","b4c36bfe":"# Predict the sale price of test dataset\npred_prices = gs_gbr.predict(Xs_test_a)\n\n# Define the dataframe to save the result as .CSV file\nfinal_outcome = pd.DataFrame(pred_prices,columns=['SalePrice'],index=test.Id)\nfinal_outcome.to_csv('predicted_SalePrice.csv')","4dbb7f59":"As a result, we try to improve the model with RidgeCV.","6306b1a8":"<!-- ### MetaData -->\n<!-- |Feature | Description|\n|--------|-----------|\n|MSSubClass |Identifies the type of dwelling involved in the sale.|\n|MSZoning| Identifies the general zoning classification of the sale.|\n|LotFrontage: | Linear feet of street connected to property|\n|LotArea: |Lot size in square feet|\n|Street: |Type of road access to property|\n|Alley: |Type of alley access to property|\n|LotShape: |General shape of property|\n|LandContour: |Flatness of the property|\n|Utilities: |Type of utilities available| -->","120cb635":"As a result, we try to improve the model with LassoCV.","614171c6":"<br>","aa68f38e":"##### 3.b Linear Regression with AdaBoost and GridSearchCV","2027b269":"In this section, we develop models to predict sale price and evalute them by cross validation to choose the optimal model for prediction. Then, predict the sale price of house with GridSearchCV to find the best parameters for model. The R Square used as regression evaluation metric.","c8931338":"<br>","5f341b95":"<br>","9ccd4efd":"We have finished the part of preparing the datasets. The dummy variable has been used to convert categorical values to numerical as binary 1 or 0 for each group value.","f18d4f73":"#### 13. Predict The house Prices","e32c24e7":"Also, we try to improve the model with the its best parameters by AdaBoost and GridSearchCV.","fa261307":"As a result, we try to improve the model by AdaBoost and GridSearchCV.","dcd0e643":"<p style=\"text-align:justify\"> The house price is an important part to decide whether you live in or choose another one. Many businesses have worked in this domain. Also, the house prices have been affected by many factors like foundation and lot area. The aim of this project is to develop a model to predict the house price based on the historical data provided by Kaggle. Also, the model predicts the sale prices as a submission for Kaggle Competition.<\/p>","e54c53ae":"<br>","bf5ada66":"As a plot has shown, we find that more overall quality leads to more sale price.","d83bc012":"## Load packages","1a490847":"Based on the distribution of features and the nature of missing values, we have filled by the global known (0.0 and Na). The feature are realted with each other.","3671322d":"After feature selection, we have used the selected features as predictors for modeling part.","875d5ce8":"<br>","447d586f":"<br>","00e1122a":"According to the plot, we have choosen the `Gradient Boosting with GridSearchCV` due to the cross validation score which is the highest score.","26651580":"The overall quality of house has postivly correlated with the sale price so, the higher quality leads to higher sale price.","f283de58":"<br>","3d09dc84":"##### 9.a Random Forest with Default Parameters","5e5db0bc":"Also, we try to improve the model with the best alpha by AdaBoost and GridSearchCV.","56c21413":"##### 6.a Define a custom function to subplot histograms ","fc1799d0":"##### 6.c The missing values in train datasets","25cabd1d":"#### 6. Fill the missing values","3c75703d":"##### 4.c LassoCV with AdaBoost and GridSearchCV","914016d8":"##### 4.b LassoCV","68f9e215":"##### 10.b ExtraTree with GridSearchCV","4d39f348":"##### 6.c ElasticNetCV with AdaBoost and GridSearchCV","bb24f660":"<br>","d0c5874d":"Based on the distribution of features and the nature of missing values, we have filled by the global known (0.0). The feature are realted with each other.","d221918a":"As a result, we try to improve the model with GridSearchCV.","48ea0fad":"##### 5.c RidgeCV with AdaBoost and GridSearchCV","545a1aaf":"Based on the distribution of feature and the nature of missing values, we have filled by the median.","2907e926":"We decied to remove the most missing value columns and the worthless column `Id`.","2a742209":"## Data Load and Cleaning\n---\n#### 1. Load House Prices Data\nLoad House Prices `train.csv` and `test.csv` from Kaggle into `DataFrames`.","171f380e":"In this part, we select the best of feature set for modeling part by using decision tree algorithm. As a result, we will be reflacted in result of house price prediction.","d44b8b4a":"As a result, we try to reduce the overfit of model by GridSearchCV.","932569fc":"Also, the top 4 columns of missing values like in train dataset so, we will remove as well. The test dataset has columns more than train dataset which reqiured more work in data cleaning part.","2b161173":"##### 6.b The common missing values in datasets","8badd73b":"<br>","7a69894e":"#### 1. Summary Statistics (train dataset)","c2d7b6ed":"Based on the distribution of feature and the nature of missing value, we have filled by the most frequent value.","8de67e5f":"<br>","c982c689":"#### 5. Ridge","dc9101ae":"#### 7. K-Nearest Neighbors","5cf7a4fd":"---","d935ea6d":"Also, The test dataset seems good like the previous dataset which there is no unexcepted data type in any columns.","975f97c9":"<h3>Conclusions and Recommendations<\/h3>","273b6819":"#### 3. Linear Regression","739586de":"#### 3. How the sale price distributed among the house styles. (train dataset)","ae095046":"##### 8.b Decision Tree with Bagging and GridSearchCV","08242c4a":"Here we have used the mean of target as a baseline because the problem is regression.","95b61cdc":"Based on the distribution of feature and the nature of missing value, we have filled by the most frequent value.","94c0cffe":"#### 12. Choose The Optimal Model","cafc186e":"<br>","bd923370":"Based on the distribution of feature and the nature of missing value, we have filled by the most frequent value.","d3ddc259":"##### 3.a Linear Regression with Default Parameters","d284069c":"<br>","483dd17b":"<br>","5efabd81":"##### 7.b K-Nearest Neighbors with GridSearchCV","5d0f1dc4":"### Model Instantiation & Prediction","e5a591a2":"<br>","bf40c3d7":"As a result, we try to reduce the overfit of model by GridSearchCV.","51b90022":"#### 11. Gradient Boosting","5de7401f":"Also, we try to improve the model with the best alpha and L1 ratio by AdaBoost and GridSearchCV.","8d9eb582":"In this section, we discover the findings in both datasets in order to know how the sale price affected and features distributed.","606c56d1":"#### 3. How complete is the data?\n\nInvestigate missing values etc.","909d08a6":"Also, we try to improve the model with the best alpha by AdaBoost and GridSearchCV.","8f250b75":"<br>","f38e5e30":"## Problem Statment","0241e412":"We decied to remove the high correlated columns which are `GarageCars`, `GrLivArea`, `TotalBsmtSF` and `TotRmsAbvGrd` .","15a551f1":"Based on the distribution of features and the nature of missing values, we have filled by the global known (NA). The feature are realted with each other.","fcbcf06c":"Based on the distribution of feature and the nature of missing value, we have filled by the most frequent value.","ba9b44e4":"Based on the distribution of feature and the nature of missing value, we have filled by using K-NN algorithm since the half of values are missed.","0239dc5f":"#### 4. The status of interior finish of the garage per foundation type. (test dataset)","d3e35417":"Based on the distribution of features and the nature of missing values, we have filled by the global known (0.0 and Na). The feature are realted with each other.","0baacd3c":"#### 8. Decision Tree","da455eb1":"<br>","0accccc7":"##### 6.d The missing values in test dataset","77ef6d9c":"<i>note:<\/i>\n- LR : Linear Regression\n- GS_CV : GridSearchCV\n- KNN : K-Nearest Neighbors\n- DT : Decision Tree\n- RF : Random Forest\n- ET : ExtraTree","68447a62":"### Contents:\n- [Data Load & Cleaning](#Data-Load-and-Cleaning)\n- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n- [Model Instantiation & Prediction](#Model-Instantiation-&-Prediction)\n- [Conclusions and Recommendations](#Conclusions-and-Recommendations)","265ad245":"##### 6.b ElasticNetCV","25e056b3":"##### 4.a Lasso with Default Parameters","c184ca7d":"By : Ibrahim Al-zahrani , Mohammed Al-Ali and Rawan MohammedEid","1b6f84b8":"#### 9. Feature Selection","01ac637a":"<p style=\"text-align:justify\">The various of factors have a huge impact on house prices. The overall quality impact has appeared in datasets. Also, the overall quality and floor area are the top 3 important factors in the prediction. So, the house price will be increased by increasing the overall quality. The findings in the training dataset prove the previous fact since the correlation coefficient is strong positively by 0.79. The recommendations are focusing on lot, floor, garage area and frontage lot within the overall quality when buying the house. Therefore, the house prices will be more predictable without considering every single factor that might not affect the price. Those recommendations will provide a clear view of the house and how will be estimated while the developed model automates those process to save time on deciding and predicts the sale price.<\/p>","d140e43b":"The train dataset seems good in data type which there is no unexcepted data type in any columns.","6578c8a8":"<br>","d99b4996":"<br>","7a10339a":"We decied to choose those columns which are `Neighborhood`, `BsmtFinType1` and `GarageFinish` .","86c7e4bd":"## Load required sklearn packages.","0959ffc9":"## House Pricing Prediction","1b762eb9":"##### 7.c K-Nearest Neighbors with AdaBoost and GridSearchCV","f3e520bc":"This section shows the baseline of regression to determine if the model has improvement on prediction.","cc28d56f":"<br>","e37bc547":"The 1908 year has the highest average of lot area due to life style and economical factor like many companies founded such as <a href=\"https:\/\/en.wikipedia.org\/wiki\/Chuck_Taylor_All-Stars\">Converse, Inc.<\/a>","8875bb6f":"<br>","ea54357b":"<br>","1f9549c7":"As a result, we try to reduce the overfit of model by GridSearchCV.","29964568":"In this part, we investigate the data distribution by plotting. For the cleaning data, removes the skewed features to uncover the optimisation opportunity.","c8e9407b":"As we have did it in train dataset, we have filled by using K-NN algorithm since the half of values are missed.","2902fc8e":"<br>","2cffc365":"<br>","0dad62bc":"As a result, we see the top 4 columns of missing value have more than 80% which most of values are missing so, we will reomve them from train dataset.","801db6e5":"#### 4. The data types\nDisplay the data types of each feature. ","e4631b2e":"Based on the distribution of feature and the nature of missing value, we have filled by the most frequent value.","d30c3c5d":"<p style=\"text-align:justify\">This project focuses on data on house prices provided by Kaggle, which provides information about the house like year built, lot area, and house style. Currently, the quality of the building house has been increased in the last years. As a result, the house price did not rely on the building quality only that makes the house price estimation harder. So, the objective of this project is to predict the house price by considering the main factors. The project conducts on the data science process (Regression).<\/p>\nExploratory data analysis produces the findings:\n<ol>\n    <li>The 1904 has the highest avgerage of lot area by 26519 ft<sup>2<\/sup>.<\/li>\n    <li>The sale price and overall quality are positively correlated by 0.79 which is strong.<\/li>\n    <li>The split level of house style has sale prices between <span>&#36;<\/span>100,000 and &#36;200,000.<\/li>\n<\/ol>","20a7ab9e":"##### 5.a Ridge with Default Parameters","9d333268":"#### 5. Which year has the highest average of lot area ? (train dataset)","fe4d814f":"#### 8. Distribution of Categorical Columns","1d32a6b3":"<br>","ff9d6e46":"##### 11.a Gradient Boosting with Default Parameters","08d4de5a":"##### 6.b The common missing values in datasets","4c2a8d56":"##### 8.a Decision Tree with Default Parameters","5fd67f5a":"#### 9. Random Forest","07abd887":"#### 10. ExtraTree","7377a5d6":"The houses have huge difference in lot area since the range is more than 21,000 and the most of overall quality between 5-7. Also, the years sold of house are between 2006 and 2010, and months in the summer. The most of second floor areas are less than the first floor areas. The sale prices are positively skewed due to the raised prices compared with the rest.","f0fe5a7a":"In this section, we scaled data to remove the effect of coefficient varinace in datasets.","eecd8f9c":"As a result, we try to improve the model with ElasticNetCV.","0e1b14a9":"<br>","2fe087c7":"##### 9.b Random Forest with GridSearchCV","28e4a8b9":"##### 10.a ExtraTree with Customized Parameters","0c52e142":"<br>","1c393be6":"##### 11.b Gradient Boosting with GridSearchCV","5fbe5669":"<h3>Exploratory Data Analysis<\/h3>","e4f58212":"#### 1. Baseline Prediction (train data)","d77cf032":"##### 7.a K-Nearest Neighbors with Default Parameters","4c15de83":"#### 6. Correlation (train dataset)","8aaf1b02":"#### 4. Lasso","c52b25bd":"According to the plot, `CBlock` has many garages are not finished while garages of `PConc` are almost finished and the rest foundation types like `CBlock`.","ef801431":"<br>","07f22c2b":"#### 7.Multicollinearity","5454b782":"<br>","36dbb636":"Based on the distribution of feature and the nature of missing value, we have filled by the most frequent value.","df188974":"##### 5.b RidgeCV","1def2cfa":"## Executive Summary","2e998e13":"In this part, we investigate the correlation between the features by using `corr()` function in `pandas`. For the cleaning data, removes the strong correlated features to prevent the fact masking.","9fbd23cc":"Based on the distribution of feature and the nature of missing value, we have filled by the most frequent value.","7da912ba":"<br>","76b14fc3":"#### 2. Scaled Data","5ef68022":"According to the plot, `2Story` and `1Story` have huge variance in sale prices so, those house styles are not stable for prediction.","d3a11e53":"#### 5. Drop unnecessary columns","15e90db8":"##### 6.a ElasticNet with Default Parameters","47b5f288":"Based on the distribution of features and the nature of missing values, we have filled by the global known (0.0). The feature are realted with each other.","c195f9e2":"Based on the distribution of feature and the nature of missing value, we have filled by the most frequent value.","5548813f":"#### 2. Display data\n\nPrint the first 5 rows of each dataframe in jupyter notebook","ce3e7a5b":"As a result, we try to reduce the overfit of model by Bagging and GridSearchCV.","032317e4":"#### 2. Find the realtion between Overall Quality and Sale Price. (train dataset)","c80ac3c5":"In this part, we investigate the missing values by plot the disturbtion and see some missing record in both datasets. For the deciding, which is the best option to fill the missing values.","f23c6a8f":"#### 6. ElasticNet"}}