{"cell_type":{"4bbc5cec":"code","938d0b88":"code","2df3de18":"code","f925cb67":"code","e7573df9":"code","ab60c027":"code","3164659f":"code","610508d8":"code","01bf26a1":"code","5668f09a":"code","0ab248cb":"code","fd6b6a1d":"code","3901a566":"code","ccc75da7":"code","a0468b6e":"markdown","554bf55f":"markdown","1433ec7d":"markdown","daba9e44":"markdown","9c46aa9f":"markdown","e4d53a8c":"markdown","c5f7b679":"markdown","92ce04f8":"markdown","72c1e5d5":"markdown","0245ad5c":"markdown","64d072db":"markdown","18b01300":"markdown","f70ce7c7":"markdown","ae8ce1bc":"markdown","76c9e5be":"markdown","057d6529":"markdown","4ccf30f8":"markdown"},"source":{"4bbc5cec":"! pip install nlpaug fairseq >> \/dev\/null","938d0b88":"import nlpaug.augmenter.char as nac\nimport nlpaug.augmenter.word as naw\n\ntest_sentence = \"B33F My Food Shop Shopping Trolley - ST. 003 troli mainan edukasi anak keranjang belanja\"","2df3de18":"aug = nac.KeyboardAug(name='Keyboard_Aug', aug_char_min=1, aug_char_max=10, aug_char_p=0.3, aug_word_p=0.3, \n                      aug_word_min=1, aug_word_max=10, stopwords=None, tokenizer=None, reverse_tokenizer=None, \n                      include_special_char=True, include_numeric=True, include_upper_case=True, lang='en', verbose=0, \n                      stopwords_regex=None, model_path=None, min_char=4)\n\ntest_sentence_aug = aug.augment(test_sentence)\nprint(test_sentence)\nprint(test_sentence_aug)","f925cb67":"aug = nac.OcrAug(name='OCR_Aug', aug_char_min=1, aug_char_max=10, aug_char_p=0.3, aug_word_p=0.3, aug_word_min=1, \n                 aug_word_max=10, stopwords=None, tokenizer=None, reverse_tokenizer=None, verbose=0, stopwords_regex=None, \n                 min_char=1)\n\ntest_sentence_aug = aug.augment(test_sentence)\nprint(test_sentence)\nprint(test_sentence_aug)","e7573df9":"aug = nac.RandomCharAug(action='substitute', name='RandomChar_Aug', aug_char_min=1, aug_char_max=10, aug_char_p=0.3, \n                        aug_word_p=0.3, aug_word_min=1, aug_word_max=10, include_upper_case=True, include_lower_case=True, \n                        include_numeric=True, min_char=4, swap_mode='adjacent', spec_char='!@#$%^&*()_+', stopwords=None, \n                        tokenizer=None, reverse_tokenizer=None, verbose=0, stopwords_regex=None, candidiates=None)\n\ntest_sentence_aug = aug.augment(test_sentence)\nprint(test_sentence)\nprint(test_sentence_aug)","ab60c027":"aug = naw.AntonymAug(name='Antonym_Aug', aug_min=1, aug_max=10, aug_p=0.3, lang='eng', stopwords=None, tokenizer=None, \n                     reverse_tokenizer=None, stopwords_regex=None, verbose=0)\n\ntest_sentence_aug = aug.augment(\"very interesting\")\nprint(\"very interesting\")\nprint(test_sentence_aug)","3164659f":"# aug = naw.BackTranslationAug(from_model_name='transformer.wmt19.en-de', to_model_name='transformer.wmt19.de-en', \n#                              from_model_checkpt='model1.pt', to_model_checkpt='model1.pt', tokenizer='moses', \n#                              bpe='fastbpe', is_load_from_github=True, name='BackTranslationAug', device='cpu', \n#                              force_reload=False, verbose=0)\n\n# test_sentence_aug = aug.augment(test_sentence)\n# print(test_sentence)\n# print(test_sentence_aug)","610508d8":"aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', model_type='', action='substitute', temperature=1.0, \n                                top_k=100, top_p=None, name='ContextualWordEmbs_Aug', aug_min=1, aug_max=10, aug_p=0.3, \n                                stopwords=None, device='cpu', force_reload=False, optimize=None, stopwords_regex=None, \n                                verbose=0, silence=True)\n\ntest_sentence_aug = aug.augment(test_sentence)\nprint(test_sentence)\nprint(test_sentence_aug)","01bf26a1":"aug = naw.RandomWordAug(action='delete', name='RandomWord_Aug', aug_min=1, aug_max=10, aug_p=0.3, stopwords=None, \n                        target_words=None, tokenizer=None, reverse_tokenizer=None, stopwords_regex=None, verbose=0)\n\ntest_sentence_aug = aug.augment(test_sentence)\nprint(test_sentence)\nprint(test_sentence_aug)","5668f09a":"aug = naw.SpellingAug(dict_path=None, name='Spelling_Aug', aug_min=1, aug_max=10, aug_p=0.3, stopwords=None, \n                      tokenizer=None, reverse_tokenizer=None, include_reverse=True, stopwords_regex=None, verbose=0)\n\ntest_sentence_aug = aug.augment(test_sentence)\nprint(test_sentence)\nprint(test_sentence_aug)","0ab248cb":"aug = naw.SplitAug(name='Split_Aug', aug_min=1, aug_max=10, aug_p=0.3, min_char=4, stopwords=None, tokenizer=None, \n                   reverse_tokenizer=None, stopwords_regex=None, verbose=0)\n\ntest_sentence_aug = aug.augment(test_sentence)\nprint(test_sentence)\nprint(test_sentence_aug)","fd6b6a1d":"aug = naw.SynonymAug(aug_src='wordnet', model_path=None, name='Synonym_Aug', aug_min=1, aug_max=10, aug_p=0.3, lang='eng', \n                     stopwords=None, tokenizer=None, reverse_tokenizer=None, stopwords_regex=None, force_reload=False, \n                     verbose=0)\n\ntest_sentence_aug = aug.augment(test_sentence)\nprint(test_sentence)\nprint(test_sentence_aug)","3901a566":"# aug = naw.TfIdfAug(model_path='.', action='substitute', name='TfIdf_Aug', aug_min=1, aug_max=10, aug_p=0.3, top_k=5, \n#                    stopwords=None, tokenizer=None, reverse_tokenizer=None, stopwords_regex=None, verbose=0)\n# \n# test_sentence_aug = aug.augment(test_sentence)\n# print(test_sentence)\n# print(test_sentence_aug)","ccc75da7":"# aug = naw.WordEmbsAug(model_type='word2vec', model_path='.', model=None, action='substitute', name='WordEmbs_Aug', \n#                       aug_min=1, aug_max=10, aug_p=0.3, top_k=100, n_gram_separator='_', stopwords=None, tokenizer=None, \n#                       reverse_tokenizer=None, force_reload=False, stopwords_regex=None, verbose=0)\n\n# test_sentence_aug = aug.augment(test_sentence)\n# print(test_sentence)\n# print(test_sentence_aug)","a0468b6e":"### Word Augmenter","554bf55f":"9. word_embs : Augmenter that apply operation to textual input based on word embeddings.","1433ec7d":"5. spelling : Augmenter that apply spelling error simulation to textual input.","daba9e44":"2. ocr : Augmenter that apply ocr error simulation to textual input.","9c46aa9f":"2. back_translation : Augmenter that apply operation (word level) to textual input based on back translation.\n\n**Need about 15mn for models downloading**","e4d53a8c":"# References :\n* [GitHub](https:\/\/github.com\/makcedward\/nlpaug)\n* [Augmenting the Data](https:\/\/www.kaggle.com\/jpmiller\/augmenting-the-data)","c5f7b679":"3. context_word_embedding : Augmenter that apply operation (word level) to textual input based on contextual word embeddings.","92ce04f8":"4. random : Augmenter that apply random word operation to textual input.","72c1e5d5":"8. tfidf : Augmenter that apply TF-IDF based to textual input.\n\n**TfIdfAug have to been trained based on your data. You can refer to this notebook for step of training.** see : https:\/\/github.com\/makcedward\/nlpaug\/blob\/master\/example\/tfidf-train_model.ipynb","0245ad5c":"6. split : Augmenter that apply word splitting operation to textual input.","64d072db":"### Character Augmenter","18b01300":"7. synonym : Augmenter that apply semantic meaning based to textual input.","f70ce7c7":"This notebook is a demo of [nlpaug](https:\/\/github.com\/makcedward\/nlpaug) package, which contains a variety of augmentations to supplement text data and introduce noise that may help your model generalize.","ae8ce1bc":"### Installation","76c9e5be":"1. antonym : Augmenter that apply semantic meaning based to textual input.","057d6529":"1. keyboard : Augmenter that apply typo error simulation to textual input.","4ccf30f8":"3. random : Augmenter that apply random character error to textual input."}}