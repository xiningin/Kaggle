{"cell_type":{"0f2630c4":"code","4964dbb8":"code","01aacdad":"code","d74a45d6":"code","56dade7f":"code","5e33c4ba":"code","416131e3":"code","344f36a0":"code","ebe09508":"code","88bf7512":"code","90ae37b3":"code","0fbb7d85":"code","71ff8d4e":"code","898a771e":"code","18451ff5":"code","a31a1f31":"code","aca96083":"code","be6bb8f7":"code","a2fe78f1":"code","246e8bfa":"code","b9c8bdab":"markdown","4e6ff2cf":"markdown","2d7d0f11":"markdown","7c76e25c":"markdown","e83e7709":"markdown","e0f887e7":"markdown","d525b28d":"markdown"},"source":{"0f2630c4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import OrderedDict\nimport cv2\nimport os\nfrom PIL import Image\n\n\nimport torch\nfrom torch import optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import random_split, DataLoader\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision.utils import make_grid\nfrom torchvision import transforms, models, datasets\n","4964dbb8":"train_dir = \"..\/input\/intel-image-classification\/seg_train\/seg_train\/\"\ntest_dir = \"..\/input\/intel-image-classification\/seg_test\/seg_test\/\"\npred_dir =\"..\/input\/intel-image-classification\/seg_pred\/seg_pred\/\"\n\n\npred_files = [os.path.join(pred_dir, f) for f in os.listdir(pred_dir)]","01aacdad":"cat_counts = {}\nfor cat in os.listdir(train_dir):\n    counts = len(os.listdir(os.path.join(train_dir, cat)))\n    cat_counts[cat] =counts\nprint(cat_counts)","d74a45d6":"cat_counts = {}\nfor cat in os.listdir(test_dir):\n    counts = len(os.listdir(os.path.join(test_dir, cat)))\n    cat_counts[cat] =counts\nprint(cat_counts)","56dade7f":"outcomes = os.listdir(train_dir)\nclasses = {k:v for k , v in enumerate(sorted(outcomes))}\n\nclasses","5e33c4ba":"## Temporary transforms, dataset, and loader to compute the mean and std of the set\n# tmp_transforms = transforms.Compose([transforms.Resize((150, 150)),\n#                                      transforms.ToTensor()])\n\n# tmp_dataset = datasets.ImageFolder(root = train_dir, transform = tmp_transforms)\n\n# tmp_loader = torch.utils.data.DataLoader(dataset = tmp_dataset, batch_size=512, shuffle=False)","416131e3":"# # Compute mean and standard deviation\n# def get_mean_and_std(loader):\n#   # Tracking variable\n#   mean = 0.0\n#   std = 0.0\n#   total_images_count = 0\n#   # Extract image data from the loader\n#   for i, batch in enumerate(loader):\n#     images, _ = batch\n#     # Store the size of each deimension (b, c, w, h)\n#     image_count_in_batch = images.size(0)\n#     # Reshape to sum the image pixels\n#     images = images.view(image_count_in_batch, images.size(1), -1)\n\n#     # Accumulate the mean and std\n#     mean += images.mean(2).sum(0)\n#     std += images.std(2).sum(0)\n\n#     # Accumiate total image count\n#     total_images_count += image_count_in_batch\n#     # Print batch number and total image count\n#     # print(f'Batch: {i} \\tTotal Images Processed: {total_images_count} of 14034')\n\n#   # Divide accumilated mean and std by total count \n#   mean \/= total_images_count\n#   std \/= total_images_count\n#   # Return values\n#   return mean, std\n\n\n# mean, std = get_mean_and_std(tmp_loader)\n#(tensor([0.4302, 0.4575, 0.4539]), tensor([0.2361, 0.2347, 0.2432]))","344f36a0":"# Actually don't need to compute mean and std of the dataset as we're \n# using mean and std for which Resnet was trained on\nmean = [0.485, 0.456, 0.406] \nstd = [0.229, 0.224, 0.225]","ebe09508":"# Torchvision train and test transforms \ntrain_transforms = transforms.Compose([transforms.Resize((150, 150)), # Resize all images \n                                       transforms.RandomResizedCrop(150),# Crop\n                                       transforms.RandomRotation(30), # Rotate \n                                       transforms.RandomHorizontalFlip(), # Flip\n                                       transforms.ToTensor(), # Convert\n                                       transforms.Normalize(torch.Tensor(mean), torch.Tensor(std)) # Normalize\n                                       ])\n\n\n\ntest_transforms = transforms.Compose([transforms.Resize((150, 150)),\n                                     transforms.CenterCrop(150),\n                                     transforms.ToTensor(),\n                                     transforms.Normalize(torch.Tensor(mean),torch.Tensor(std))\n                                     ])\n\n# Tmp torchvision datasets.Image folder to split into train and validation sets\ntmp_data = datasets.ImageFolder(train_dir, transform=train_transforms)\n# len(tmp_data): 14034\n\n# Randomsplit tmp data based on length of dataset and set seed for reproducable split\ntrain_data, val_data = random_split(tmp_data, [10000, 4034], generator=torch.Generator().manual_seed(42))\n# Test set with with test transforms \ntest_data = datasets.ImageFolder(test_dir, transform=test_transforms)\n\n\n# Set Pytorch dataloaders, batch_size, training set shuffle\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=64)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=64)","88bf7512":"# Plot transformed images \ndef show_transformed(loader):\n    plt.figure(figsize=(60,60))\n    # Interate single batch (64)\n    batch = next(iter(loader))\n    # for images, labels in single batch:\n    images, labels = batch\n    # torchvision make_grid\n    grid = make_grid(images, nrow = 10)\n    print(('Labels', labels))\n    plt.imshow(np.transpose(grid, (1 ,2, 0)))\n    plt.show()\n\n\nshow_transformed(train_loader)","90ae37b3":"# check if cuda is available\ntrain_on_gpu = torch.cuda.is_available()\n# if yes set device as cuda\ndevice =  torch.device('cuda' if torch.cuda.is_available else 'cpu')","0fbb7d85":"# # Capture installation text\n# %%capture\n# Import resnet50\nresnet = models.resnet50(pretrained=True)\n# Freeze model params \nfor param in resnet.parameters():\n    param.required_grad = False\n# Pull final fc layer feature dimensions\nfeatures = resnet.fc.in_features\n\n\n# Build custom classifier which reduces Resnets 1000 out_features to 6\nclassifier = nn.Sequential(OrderedDict([('fc1', nn.Linear(features, 512)),\n                                        ('relu', nn.ReLU()),\n                                        ('drop', nn.Dropout(0.05)),\n                                        ('fc2', nn.Linear(512, 6)),\n                                        ]))\n\n# ('output', nn.LogSoftmax(dim=1)) - NLLLoss\n# Appending classifier layer to Resnet\nresnet.classifier = classifier\n# Pushing the model to cuda\nresnet.to(device)","71ff8d4e":"# Define criterion and optimizer\ncriterion = nn.CrossEntropyLoss()\n# Pass the optimizer to the appended classifier layer\noptimizer = torch.optim.SGD(resnet.parameters(), lr=0.001)\n# Set scheduler\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 15], gamma=0.05)\n# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=10, epochs=20)","898a771e":"epochs = 20\n\n\ntr_losses = []\navg_epoch_tr_loss = []\ntr_accuracy = []\n\n\nval_losses = []\navg_epoch_val_loss = []\nval_accuracy = []\nval_loss_min = np.Inf\n\n\nresnet.train()\nfor epoch in range(epochs):\n  for i, batch in enumerate(train_loader):\n    # Pull the data and labels from the batch\n    data, label = batch\n    # If available push data and label to GPU\n    if train_on_gpu:\n      data, label = data.to(device), label.to(device)\n    # Clear the gradient\n    optimizer.zero_grad()\n    # Compute the logit\n    logit = resnet(data)\n    # Compte loss\n    loss = criterion(logit, label)\n    # Backpropagate the gradients (accumulte the partial derivatives of loss)\n    loss.backward()\n    # Apply the updates to the optimizer step in the opposite direction to the gradient\n    optimizer.step()\n    # Store the losses of each batch\n    # loss.item() seperates the loss from comp graph\n    tr_losses.append(loss.item())\n    # Detach and store the average accuracy of each batch\n    tr_accuracy.append(label.eq(logit.argmax(dim=1)).float().mean())\n    # Print the rolling batch training loss every 20 batches\n    if i % 40 == 0:\n      print(f'Batch No: {i} \\tAverage Training Batch Loss: {torch.tensor(tr_losses).mean():.2f}')\n  # Print the average loss for each epoch\n  print(f'\\nEpoch No: {epoch + 1},Training Loss: {torch.tensor(tr_losses).mean():.2f}')\n  # Print the average accuracy for each epoch\n  print(f'Epoch No: {epoch + 1}, Training Accuracy: {torch.tensor(tr_accuracy).mean():.2f}\\n')\n  # Store the avg epoch loss for plotting\n  avg_epoch_tr_loss.append(torch.tensor(tr_losses).mean())\n\n\n  resnet.eval()\n  for i, batch in enumerate(val_loader):\n    # Pull the data and labels from the batch\n    data, label = batch\n    # If available push data and label to GPU\n    if train_on_gpu:\n      data, label = data.to(device), label.to(device)\n    # Compute the logits without computing the gradients\n    with torch.no_grad():\n      logit = resnet(data)\n    # Compte loss\n    loss = criterion(logit, label)\n    # Store test loss\n    val_losses.append(loss.item())\n    # Store the accuracy for each batch\n    val_accuracy.append(label.eq(logit.argmax(dim=1)).float().mean())\n    if i % 40 == 0:\n      print(f'Batch No: {i} \\tAverage Val Batch Loss: {torch.tensor(val_losses).mean():.2f}')\n  # Print the average loss for each epoch\n  print(f'\\nEpoch No: {epoch + 1}, Epoch Val Loss: {torch.tensor(val_losses).mean():.2f}')\n  # Print the average accuracy for each epoch    \n  print(f'Epoch No: {epoch + 1}, Epoch Val Accuracy: {torch.tensor(val_accuracy).mean():.2f}\\n')\n  # Store the avg epoch loss for plotting\n  avg_epoch_val_loss.append(torch.tensor(val_losses).mean())\n\n  # Checpoininting the model using val loss threshold\n  if torch.tensor(val_losses).float().mean() <= val_loss_min:\n    print(\"Val Loss Decreased... Saving model\")\n    # save current model\n    torch.save(resnet.state_dict(), '.\/model_state.pt')\n    val_loss_min = torch.tensor(val_losses).mean()\n  # Step the scheduler for the next epoch\n  scheduler.step()\n  # Print the updated learning rate\n  print('Learning Rate Set To: {:.10f}'.format(optimizer.state_dict()['param_groups'][0]['lr']),'\\n')","18451ff5":"# Plot the average epoch losses \nplt.plot(avg_epoch_tr_loss, label='Training loss')\nplt.plot(avg_epoch_val_loss, label='Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(frameon=False)\nplt.show()","a31a1f31":"# Load resnet state dict\nresnet.load_state_dict(torch.load('.\/model_state.pt'))\n# Set model to evaluate \nresnet.eval()\n# Push to cuda\nresnet = resnet.cuda()","aca96083":"def test_model(model, data):\n  te_accuracy =[]\n  for i, batch in enumerate(test_loader):\n      # Pull the data and labels from the batch\n      data, label = batch\n      # If available push data and label to GPU\n      if train_on_gpu:\n        data, label = data.to(device), label.to(device)\n      # Compute the logits without computing the gradients\n      with torch.no_grad():\n        logit = resnet(data)\n      # Store the accuracy for each batch\n      te_accuracy.append(label.eq(logit.argmax(dim=1)).float().mean())\n      if i % 20 == 0 and not i == 1:\n        print(f'Batch No. {i}\\tAverage Test Batch Accuracy: {torch.tensor(te_accuracy).mean():.3f}')\n\n  print(f'Test Accuracy: {torch.tensor(te_accuracy).mean():.3f}\\n')\n","be6bb8f7":"# Test model\ntest_model(resnet, test_loader)","a2fe78f1":"# Function to pull the label\ndef predictions(image):\n    # pil_img = Image.open(image)\n    # transform images\n    transform = test_transforms(image)\n    # add extra dimension in place of batch size\n    img = transform.unsqueeze(0).cuda() \n    # push to gpu\n    gpu_img = img.to(device)\n    # make prediction\n    output = resnet(gpu_img)\n    # convert image to numpy format in cpu and snatching max prediction score class index\n    # _, index = torch.max(output.data, 1)\n    index = output.data.cpu().numpy().argmax()    \n    return index","246e8bfa":"# use the class dict created earlier to pull the label as title\nresnet.eval()\nplt.figure(figsize=(20,20))\nfor i, images in enumerate(pred_files):\n    # just want 25 images to print\n    if i > 24:\n      break\n    # Using pil to open the image\n    img = Image.open(images)\n    # Calling prediction function on image\n    index = predictions(img)\n    # Plotting\n    plt.subplot(5,5,i+1)\n    plt.title(classes[index])\n    plt.axis('off')\n    plt.imshow(img)","b9c8bdab":"Download and modify the model (Resnet50)\n    \n* Check for GPU\n* Download model\n* Append classifier layer\n* Set criterion, optimizer, scheduler \n* Define train, val loop with model checkpoints\n* Save model\n* Plot loss\n","4e6ff2cf":"\nThanks for coming so far, consider and upvote...\n\n\nOn to the next project...\n","2d7d0f11":"<img src=\"https:\/\/media.giphy.com\/media\/BpGWitbFZflfSUYuZ9\/giphy.gif\">\n","7c76e25c":"<font size=\"4\"> \nThis is my first swaray into image classification so if you spot any errors or suggestions for improvement please leave a comment. \n<br \/>\n<br \/>\nShoutout to Sudhanshu Singh his notebook really helped me figure some of this out especially the final two cells.\n    <\/font>","e83e7709":"\n\nTest the model\n    \n* Test set\n* Define Inference function\n* Predict for the pred files\n","e0f887e7":"Invesitaging the data\n    \n* Set directory paths\n* Get train\/test category counts \n* Establish class labels\n* Compute mean and std of dataset for normalization","d525b28d":"\nTransform the data\n    \n* Transforms Images\n* Create ImageFolder\n* Create DataLoader\n* Preview Images"}}