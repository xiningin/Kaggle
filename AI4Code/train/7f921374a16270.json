{"cell_type":{"3615614e":"code","f98ee01c":"code","0b059e7a":"code","17df92ed":"code","1dd2cd9b":"code","2c4cd37c":"code","0a944f5f":"code","8341fc2d":"code","b9b3e810":"code","4fbfb249":"code","dddfbb5f":"code","22b5f057":"code","54047ec3":"code","bb04ff05":"code","07f6478a":"code","a2e74e68":"code","fbba37fd":"code","d7fb0424":"code","2411f089":"code","01a3000e":"code","77cd6735":"code","fcf9c1c5":"markdown","08bc1b35":"markdown","f15d1b75":"markdown","7819a7c3":"markdown","3d50d9fb":"markdown","6ff776f4":"markdown","1b822300":"markdown","86f63878":"markdown","c35790cd":"markdown"},"source":{"3615614e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f98ee01c":"#import data\ndata = pd.read_csv(\"..\/input\/data.csv\")","0b059e7a":"data.head()","17df92ed":"data.info()","1dd2cd9b":"data.drop([\"id\",\"Unnamed: 32\"],axis = 1,inplace = True)","2c4cd37c":"data.head()","0a944f5f":"# binary classification\ndata.diagnosis.unique()","8341fc2d":"# list comprehention\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]","b9b3e810":"data.diagnosis.unique()","4fbfb249":"y = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"],axis = 1)","dddfbb5f":"# normalization\nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","22b5f057":"# train test split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3 , random_state = 1)","54047ec3":"# svm\nfrom sklearn.svm import SVC\nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train)","bb04ff05":"# accuracy score\nprint(\"accuracy of svm algo:\",svm.score(x_test,y_test))","07f6478a":"# naive bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)","a2e74e68":"# accuracy score\nprint(\"accuracy of naive bayes algo:\",nb.score(x_test,y_test))","fbba37fd":"# decision tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)","d7fb0424":"# accuracy score\nprint(\"accuracy of decision tree algo:\",dt.score(x_test,y_test))","2411f089":"# random forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 100,random_state = 1)\nrf.fit(x_train,y_train)","01a3000e":"# accuracy score\nprint(\"accuracy of random forest algo:\",rf.score(x_test,y_test))","77cd6735":"# confusion matrix\ny_pred = rf.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\nimport seaborn as sns\nsns.heatmap(cm, annot = True, linewidths = 0.5 , linecolor = \"yellow\", fmt =\".0f\")\nplt.xlabel(\"y_true\")\nplt.ylabel(\"y_pred\")\nplt.show()","fcf9c1c5":"**Confusion Matrix**\n\nA confusion matrix is a table that is often used to describe the performance of a classification model  on a set of test data for which the true values are known.\n!![image.png](attachment:image.png)[](http:\/\/)\n\nTP:  true positives\n\nTN:  true negatives\n\nFP : false positives\n\n\nFN : false negatives \n","08bc1b35":"We can compare the success rates of different algorithms in this kernel.","f15d1b75":"**Support Vector Machine**\n\nThe learning of the hyperplane in linear SVM is done by transforming the problem using some linear algebra, which is out of the scope of this introduction to SVM.\n\n!![image.png](attachment:image.png)[](http:\/\/)","7819a7c3":"We always start with analysis to work.","3d50d9fb":"**Decision Tree**\n\nDecision Trees are a type of Supervised Machine Learning, where data is constantly divided according to a certain parameter. The tree can be explained by two entities, decision nodes and leaves. Leaves are decisions or final results. And the decision nodes show where the data is divided.\n\n!![image.png](attachment:image.png)[](http:\/\/)","6ff776f4":"Thank you for reviewing. Please remember to comment.","1b822300":"**Hi. I will mention some machine learning algorithms (SVM,Naive Bayes,Decision Tree,Random Forest) in kernel. I hope you like.**","86f63878":"**Random Forest**\n\nRandom forest builds multiple decision trees and merges them together to get a more accurate and stable prediction.\n\n!![image.png](attachment:image.png)[](http:\/\/)","c35790cd":"**Naive Bayes**\n\nNaive Bayes methods are a set of supervised learning algorithms based on applying Bayes\u2019 theorem with the \u201cnaive\u201d assumption of independence between every pair of features.\n\n!![image.png](attachment:image.png)[](http:\/\/)"}}