{"cell_type":{"bf7bb287":"code","153ca2d8":"code","7ea1ec89":"code","3a29ccd1":"code","42be7ee1":"code","af956560":"code","1cb63b14":"code","d2ecd9e7":"code","d245eae0":"code","b92691ba":"code","128df1cf":"code","d979943d":"code","ff5f53a8":"code","9f275b5b":"code","3a4e66cb":"code","50e130c7":"code","10827273":"code","4152bb0d":"code","c1882a78":"code","3f9005df":"markdown","3ffb6b71":"markdown","22e11b02":"markdown","fb0da128":"markdown","36deb0e3":"markdown","303b5ab8":"markdown","bfed7692":"markdown","1bb2d187":"markdown","002ea9eb":"markdown","99457a5c":"markdown","c5435026":"markdown","cca2d5b2":"markdown"},"source":{"bf7bb287":"import pandas as pd\nimport numpy as np\nfrom catboost import CatBoostRegressor, Pool\nfrom sklearn.model_selection import train_test_split\nimport gc\nimport matplotlib.pyplot as plt\n%matplotlib inline","153ca2d8":"print('Loading Properties...')\nproperties2016 = pd.read_csv('..\/input\/properties_2016.csv', low_memory = False)\nproperties2017 = pd.read_csv('..\/input\/properties_2017.csv', low_memory = False)\n\nprint('Loading Train...')\ntrain2016 = pd.read_csv('..\/input\/train_2016_v2.csv', parse_dates=['transactiondate'], low_memory=False)\ntrain2017 = pd.read_csv('..\/input\/train_2017.csv', parse_dates=['transactiondate'], low_memory=False)\n\nprint('Loading Sample ...')\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv', low_memory=False)","7ea1ec89":"def add_date_features(df):\n    df[\"transaction_year\"] = df[\"transactiondate\"].dt.year\n    df[\"transaction_month\"] = df[\"transactiondate\"].dt.month\n    df[\"transaction_day\"] = df[\"transactiondate\"].dt.day\n    df[\"transaction_quarter\"] = df[\"transactiondate\"].dt.quarter\n    df.drop([\"transactiondate\"], inplace=True, axis=1)\n    return df","3a29ccd1":"train2016 = add_date_features(train2016)\ntrain2017 = add_date_features(train2017)\n\nsample_submission['parcelid'] = sample_submission['ParcelId']\n\nprint('Merge Train & Test with Properties...')\ntrain2016 = pd.merge(train2016, properties2016, how='left', on='parcelid')\ntrain2017 = pd.merge(train2017, properties2017, how='left', on='parcelid')\ntest_df = pd.merge(sample_submission, properties2016, how='left', on='parcelid')\n\nprint('Concat Train 2016 & 2017...')\ntrain_df = pd.concat([train2016, train2017], axis=0)\n\ndel properties2016, properties2017, train2016, train2017\ngc.collect();\n\nprint(\"Train: \", train_df.shape)\nprint(\"Test: \", test_df.shape)","42be7ee1":"# print (\"Replacing NaN values by -999 !!\")\n# train_df.fillna(-999, inplace=True)\n# test_df.fillna(-999, inplace=True)","af956560":"# print(train_df['hashottuborspa'])\n# for c in train_df.columns:\n#     if c not in ['parcelid', 'logerror']:\n#         print(c)\n#         print(sum(np.isnan(train_df[c])), len(train_df[c]))\n#         #plt.plot(train_df[c][np.isfinite(train_df[c])], train_df['logerror'], marker='o', linestyle = 'None',)\n# #         plt.scatter(train_df['logerror'], train_df[c][np.isfinite(train_df[c])])\n# #         plt.ylabel(c)\n# #         plt.xlabel('logerror')\n# #         plt.show()","1cb63b14":"# 98% \u043d\u044c \u0445\u043e\u043e\u0441\u043e\u043d \u0431\u043e\u043b \u0445\u0430\u0441\u043d\u0430\nmissing_perc_thresh = 0.98\nexclude_missing = []\nnum_rows = train_df.shape[0]\nfor c in train_df.columns:\n    num_missing = train_df[c].isnull().sum()\n    if num_missing == 0:\n        continue\n    missing_frac = num_missing \/ float(num_rows)\n    if missing_frac > missing_perc_thresh:\n        exclude_missing.append(c)\nprint(\"We exclude: %s\" % exclude_missing)\nprint(len(exclude_missing))","d2ecd9e7":"exclude_unique = []\nfor c in train_df.columns:\n    num_uniques = len(train_df[c].unique())\n    if train_df[c].isnull().sum() != 0:\n        num_uniques -= 1\n    if num_uniques == 1:\n        exclude_unique.append(c)\nprint(\"We exclude: %s\" % exclude_unique)\nprint(len(exclude_unique))","d245eae0":"exclude_other = ['parcelid', 'logerror','propertyzoningdesc']\ntrain_features = []\nfor c in train_df.columns:\n    if c not in exclude_missing \\\n       and c not in exclude_other and c not in exclude_unique:\n        train_features.append(c)\nprint(\"We use these for training: %s\" % train_features)\nprint(len(train_features))","b92691ba":"cat_feature_inds = []\ncat_unique_thresh = 1000\nfor i, c in enumerate(train_features):\n    num_uniques = len(train_df[c].unique())\n    if num_uniques < cat_unique_thresh \\\n       and not 'sqft' in c \\\n       and not 'cnt' in c \\\n       and not 'nbr' in c \\\n       and not 'number' in c:\n        cat_feature_inds.append(i)\n        \nprint(\"Cat features are: %s\" % [train_features[ind] for ind in cat_feature_inds])","128df1cf":"print (\"Replacing NaN values by -999 !!\")\ntrain_df.fillna(-999, inplace=True)\ntest_df.fillna(-999, inplace=True)","d979943d":"def print_feature_importance(model, pool, X_train):\n    feature_importances = model.get_feature_importance(pool)\n    feature_names = X_train.columns\n    for score, name in sorted(zip(feature_importances, feature_names), reverse=True):\n        print('{}\\t{}'.format(name, score))","ff5f53a8":"X_train, X_test, y_train, y_test = train_test_split(train_df[train_features], train_df.logerror, test_size=0.2, random_state=99)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)\n\nall_pool = Pool(train_df[train_features], train_df.logerror, cat_feature_inds)\ntrain_pool = Pool(X_train, y_train, cat_feature_inds)\ntest_pool = Pool(X_test, y_test, cat_feature_inds)","9f275b5b":"catboost_parameters = {\n    'iterations': 400,\n    'learning_rate': 0.035,\n    'depth': 7,\n    'verbose': 20,\n#     'l2_leaf_reg': 1000,\n    'task_type': 'GPU',\n    'loss_function': 'MAE',\n    'eval_metric': 'MAE',\n    'random_seed': 0,\n}","3a4e66cb":"model = CatBoostRegressor(**catboost_parameters)\nmodel.fit(train_pool, eval_set=test_pool)","50e130c7":"print_feature_importance(model, train_pool, X_train)","10827273":"# submission = pd.DataFrame({\n#     'ParcelId': test_df['parcelid'],\n# })\n\n# test_dates = {\n#     '201610': pd.Timestamp('2016-09-30'),\n#     '201611': pd.Timestamp('2016-10-31'),\n#     '201612': pd.Timestamp('2016-11-30'),\n#     '201710': pd.Timestamp('2017-09-30'),\n#     '201711': pd.Timestamp('2017-10-31'),\n#     '201712': pd.Timestamp('2017-11-30')\n# }\n\n# for label, test_date in test_dates.items():\n#     print(\"Predicting for: %s ... \" % (label))\n#     test_df['transactiondate'] = test_date\n#     test_df = add_date_features(test_df)\n#     y_pred = model.predict(test_df[train_features])\n#     submission[label] = y_pred\n\n# submission_major = 1\n# print(\"Creating submission: submission_%03d.csv ...\" % (submission_major))\n# submission.to_csv(\n#     'submission_%03d.csv' % (submission_major),\n#     float_format='%.4f',\n#     index=False)\n# print(\"Finished.\")","4152bb0d":"num_ensembles = 5\n# ensemble models\nmodels = [None] * num_ensembles\nfor i in range(num_ensembles):\n    print(\"\\nTraining (ensemble): %d ...\" % (i))\n    catboost_parameters['random_seed'] = i\n    models[i] = CatBoostRegressor(**catboost_parameters)\n    models[i].fit(train_pool, eval_set=test_pool)\n    print('-- Feature Importance --')\n    print_feature_importance(models[i], train_pool, X_train)","c1882a78":"submission = pd.DataFrame({\n    'ParcelId': test_df['parcelid'],\n})\n\ntest_dates = {\n    '201610': pd.Timestamp('2016-09-30'),\n    '201611': pd.Timestamp('2016-10-31'),\n    '201612': pd.Timestamp('2016-11-30'),\n    '201710': pd.Timestamp('2017-09-30'),\n    '201711': pd.Timestamp('2017-10-31'),\n    '201712': pd.Timestamp('2017-11-30')\n}\n\nfor label, test_date in test_dates.items():\n    print(\"Predicting for: %s ... \" % (label))\n    test_df['transactiondate'] = test_date\n    test_df = add_date_features(test_df)\n    y_pred = 0.0\n    for i in range(num_ensembles):\n        print(\"Ensemble:\", i)\n        y_pred += models[i].predict(test_df[train_features])\n    y_pred \/= num_ensembles\n    submission[label] = y_pred\n\nsubmission_major = 2\nprint(\"Creating submission: submission_%03d.csv ...\" % (submission_major))\nsubmission.to_csv(\n    'submission_%03d.csv' % (submission_major),\n    float_format='%.4f',\n    index=False)\nprint(\"Finished.\")","3f9005df":"## 1.1a \u0411\u0430\u0440\u0430\u0433 \u0445\u043e\u043e\u0441\u043e\u043d \u0431\u0430\u0433\u0430\u043d\u0443\u0443\u0434\u044b\u0433 \u0445\u0430\u0441\u0430\u0445","3ffb6b71":"## 0.1 \u04e8\u0433\u04e9\u0433\u0434\u04e9\u043b \u0443\u043d\u0448\u0438\u0445","22e11b02":"## 1.1b \u0411\u04af\u0433\u0434 \u0430\u0434\u0438\u043b\u0445\u0430\u043d \u0443\u0442\u0433\u0430\u0442\u0430\u0439 \u0431\u0430\u0433\u0430\u043d\u0443\u0443\u0434\u044b\u0433 \u0445\u0430\u0441\u0430\u0445","fb0da128":"## 0.2 \u0421\u0443\u0440\u0433\u0430\u043b\u0442 \u0431\u043e\u043b\u043e\u043d \u0448\u0430\u043b\u0433\u0430\u043b\u0442\u044b\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u04e9\u04e9 \u0431\u044d\u043b\u0442\u0433\u044d\u0445","36deb0e3":"## 1.2a \u0421\u0443\u0440\u0433\u0430\u043b\u0442\u0430\u043d\u0434 \u0430\u0448\u0438\u0433\u043b\u0430\u0445 \u0448\u0438\u043d\u0436\u04af\u04af\u0434 \u0431\u0443\u044e\u0443 \u0431\u0430\u0433\u0430\u043d\u0443\u0443\u0434\u044b\u0433 \u0441\u043e\u043d\u0433\u043e\u0445","303b5ab8":"## 2.1a \u042d\u043d\u0438\u0433\u0438\u0439\u043d CatBoostRegressor","bfed7692":"# 1. \u0421\u0443\u0440\u0433\u0430\u043b\u0442\u0430\u043d\u0434 \u0430\u0448\u0438\u0433\u043b\u0430\u0445 \u0448\u0438\u043d\u0436\u04af\u04af\u0434\u0438\u0439\u0433 \u0441\u043e\u043d\u0433\u043e\u0445","1bb2d187":"## 2.1b Ensemble - CatBoostRegressor","002ea9eb":"# 0. \u04e8\u0433\u04e9\u0433\u0434\u043b\u04e9\u04e9 \u0443\u043d\u0448\u0438\u0445, \u0431\u044d\u043b\u0442\u0433\u044d\u0445","99457a5c":"# 2 \u0421\u0443\u0440\u0433\u0430\u043b\u0442","c5435026":"## 1.3 \u0425\u043e\u043e\u0441\u043e\u043d \u0443\u0442\u0433\u0443\u0443\u0434\u044b\u0433 \u0431\u04e9\u0433\u043b\u04e9\u0445","cca2d5b2":"## 1.2b \u0410\u043d\u0433\u0438\u043b\u0430\u0445 \u0431\u043e\u043b\u043e\u043c\u0436\u0442\u043e\u0439 \u0448\u0438\u043d\u0436\u04af\u04af\u0434"}}