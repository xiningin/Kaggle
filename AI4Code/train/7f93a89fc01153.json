{"cell_type":{"0de2b495":"code","b954a09e":"code","372b1df4":"code","a87eac42":"code","c528286e":"code","ba8084a5":"code","2f0ffee3":"code","ce031c62":"code","eff28919":"code","a7f9a0b1":"code","06b7aff8":"code","af435d45":"code","fa13dce4":"code","b638be0c":"code","44d64375":"markdown"},"source":{"0de2b495":"!pip install utils\n!pip install tensorflow --upgrade\n#!pip install  unicode","b954a09e":"#Import libraries\nimport os\nfrom keras.models import Model\nfrom keras.utils import np_utils\nfrom keras.callbacks import Callback\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom time import time\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom utils import *\nfrom keras.layers import Embedding, Dense, Dropout, Input#, LSTM, Bidirectional\nfrom keras.layers import MaxPooling1D, Conv1D, Flatten, LSTM\nfrom keras.preprocessing import sequence#, text\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing import text\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    confusion_matrix as confmat,\n    classification_report as creport\n)\nimport matplotlib.pyplot as plt\nimport os\nimport re\nimport shutil\nimport string\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import preprocessing\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n","372b1df4":"print(tf.__version__)","a87eac42":"#Load dataset using the new tensorflow text_dataset_from_directory \n\nseed=42\ndata_paths = '..\/input\/sanad-dataset'\nlabels=os.listdir(data_paths) \nraw_data_train = tf.keras.preprocessing.text_dataset_from_directory(\n    data_paths,\n    labels=\"inferred\",\n    label_mode=\"int\",\n    #class_names=classes,\n    #batch_size=1,\n    max_length=None,\n    shuffle=True,\n    seed=seed,\n    validation_split=None,\n    subset=None,\n    follow_links=False,\n)","c528286e":"raw_data_test = tf.keras.preprocessing.text_dataset_from_directory(\n    data_paths, \n    batch_size=batch_size, \n    validation_split=0.2, \n    subset='validation', \n    seed=seed)","ba8084a5":"x_train=[]\ny_train=[]\nfor text_batch, label_batch in raw_data_train:\n    for i in range(len(text_batch)):\n        s=text_batch.numpy()[i].decode(\"utf-8\") \n        x_train.append(s)\n        y_train.append(raw_data.class_names[label_batch.numpy()[i]])\n        #print(label_batch.numpy()[i])\nprint(len(x_train))\nprint(len(y_train))","2f0ffee3":"x_test=[]\ny_test=[]\nfor text_batch, label_batch in raw_data_test:\n    for i in range(len(text_batch)):\n        s=text_batch.numpy()[i].decode(\"utf-8\") \n        x_test.append(s)\n        y_test.append(raw_data.class_names[label_batch.numpy()[i]])\n        #print(label_batch.numpy()[i])\nprint(len(x_test))\nprint(len(y_test))","ce031c62":"#To prevent train\/test skew (also know as train\/serving skew), it is important to preprocess the data identically\ndef custom_standardization(input_data):\n  lowercase = tf.strings.lower(input_data)\n  stripped_html = tf.strings.regex_replace(lowercase, '<br \/>', ' ')\n  return tf.strings.regex_replace(stripped_html,\n                                  '[%s]' % re.escape(string.punctuation),\n                                  '')\nmax_features = 10000\nsequence_length = 500\n\nvectorize_layer = TextVectorization(\n    standardize=custom_standardization,\n    max_tokens=max_features,\n    output_mode='int',\n    output_sequence_length=sequence_length)\n# Make a text-only dataset (without labels), then call adapt\ntrain_text = raw_data_train.map(lambda x, y: x)\nvectorize_layer.adapt(train_text)","eff28919":"# Standardize, tokenize, and vectorize our data\ndef vectorize_text(text, label):\n  text = tf.expand_dims(text, -1)\n  return vectorize_layer(text), label\n# retrieve a batch (of 32 reviews and labels) from the dataset\ntext_batch, label_batch = next(iter(raw_data_train))\nfirst_review, first_label = text_batch[0], label_batch[0]\nprint(\"Text\", first_review)\nprint(\"Label\", raw_data_train.class_names[first_label])\nprint(\"Vectorized review\", vectorize_text(first_review, first_label))","a7f9a0b1":"\ntrain_ds = raw_data_train.map(vectorize_text)\nval_ds = raw_data_test.map(vectorize_text)\n","06b7aff8":"embedding_dim = 100\nmax_length = 16\nmodel = tf.keras.Sequential([\n  layers.Embedding(max_features + 1, embedding_dim,  trainable=False),\n  layers.Dropout(0.2),\n  layers.Conv1D(64, 5, activation='relu'),\n  layers.MaxPooling1D(pool_size=4),\n  layers.LSTM(64),\n  layers.Dense(1, activation='softmax')])\n\nmodel.summary()","af435d45":"model.compile(loss=losses.categorical_crossentropy, optimizer='SGD', metrics=['accuracy'])","fa13dce4":"epochs = 10\nhistory = model.fit(\n    train_ds,\n    batch_size=batch_size,\n    validation_data=val_ds,\n    epochs=epochs)","b638be0c":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","44d64375":"This notebook implements an Arabic text classification\/categorization using a subset of the dataset SANAD. The subset contains **45500** Arabic news articles organized into 7 balanced categories from www.alkhaleej.ae\nLabels are categorized in: **Culture**, **Finance**, **Medical**, **Politics**,**Religion**,**Sports**,**Tech**.\n"}}