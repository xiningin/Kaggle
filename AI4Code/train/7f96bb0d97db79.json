{"cell_type":{"f5bbed0f":"code","b3fd4aad":"code","93681d22":"code","bdde5bbc":"code","c3bf964a":"code","5b296155":"code","42a3c366":"code","d9cb0a96":"code","f81b9240":"code","d62339eb":"code","e7db95ea":"code","b5778094":"code","1e135bce":"code","2260cb04":"code","f2ea854d":"code","017141be":"code","1d8d439f":"code","0bab078f":"code","7b9fbc9e":"code","6f0db01c":"code","443a25ca":"code","5eeb31e3":"code","5543f222":"code","be64b974":"code","86369118":"code","b23a6d07":"code","69c4e38e":"code","78d633f0":"code","3f2e01ad":"code","131127b3":"code","8ba6ddab":"code","5c2a125d":"code","81fa21b8":"code","8e1ea333":"markdown","5728c684":"markdown","7bc58a05":"markdown","981fe783":"markdown","0afec06f":"markdown","89910b52":"markdown","f045bd05":"markdown","6163988b":"markdown","175abe31":"markdown","245b89e1":"markdown","7601d0aa":"markdown","bd4d8a9f":"markdown"},"source":{"f5bbed0f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b3fd4aad":"# Fetching training data\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntrain.head()","93681d22":"# Fetching testing data\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntest.head()","bdde5bbc":"# Checking the dimensions of datasets\nprint(train.shape)\nprint(test.shape)","c3bf964a":"# Checking missing values in both train and test set\nprint(train.isnull().sum())\nprint(test.isnull().sum())","5b296155":"# Filling the missing values of \"keyword\" as Unknown so that those can be counted as well\ntrain.keyword.fillna(\"Unknown\", inplace=True)\ntest.keyword.fillna(\"Unknown\", inplace=True)","42a3c366":"# Dropping the non-required fields i.e. location and id from the dataset\ntrain.drop(\"location\", axis=1, inplace=True)\ntest.drop(\"location\", axis=1, inplace=True)\ntrain.drop(\"id\", axis=1, inplace=True)\ntest.drop(\"id\", axis=1, inplace=True)","d9cb0a96":"# Final check to verify the missing values\nprint(train.isnull().sum())\nprint(test.isnull().sum())","f81b9240":"# Importing stopwords from NLTK\nimport nltk\nnltk.download('stopwords')","d62339eb":"# Pre-processing the text with the removal of irrelevant characters, symbols and stopwords\nimport re\nimport string\nfrom nltk.corpus import stopwords\nREPLACE_BY_SPACE_RE = re.compile('[\/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\ndef text_prepare(text):\n    \"\"\"\n        text: a string   \n        return: modified initial string\n    \"\"\"\n    text = text.lower() #lowercase text  \n    text = REPLACE_BY_SPACE_RE.sub(' ',text) #replace REPLACE_BY_SPACE_RE symbols by space in text    \n    text = BAD_SYMBOLS_RE.sub('',text) #delete symbols which are in BAD_SYMBOLS_RE from text \n    text = re.sub('https','',text)\n    text = re.sub('http','',text)\n    text = re.sub('tco','',text)\n    temp = [s.strip() for s in text.split() if s not in STOPWORDS] #delete stopwords from text\n    new_text = ''\n    for i in temp:\n        new_text +=i+' '\n    text = new_text\n    return text.strip()","e7db95ea":"# Applying the above preprocessing to the train set\ntrain['text'] = train['text'].map(text_prepare)\ntrain['text'].head()","b5778094":"# Applying the above preprocessing to the test set\ntest['text'] = test['text'].map(text_prepare)\ntest['text'].head()","1e135bce":"# Cleaning the text using tokens and stemming\nstopword =  nltk.corpus.stopwords.words('english')\nps = nltk.PorterStemmer()\ndef clean(text):\n    no_punct = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens = re.split('\\W+',no_punct) #tokenization\n    text_stem = ([ps.stem(word) for word in tokens if word not in stopword]) #stemming\n    return text_stem","2260cb04":"import seaborn as sns\nsns.countplot(train.target)","f2ea854d":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Common words in the tweets\nplt.figure(figsize = (16,24))\nwordcloud = WordCloud(min_font_size = 5,  max_words = 500 , width = 1800 , height = 1000, stopwords= STOPWORDS).generate(\" \".join(train['text']))\nplt.imshow(wordcloud,interpolation = 'bilinear')","017141be":"# Common words in the disaster tweets\ndisaster = train.text[train.target[train.target==1].index]\nplt.figure(figsize = (16,24))\nwordcloud = WordCloud(min_font_size = 5,  max_words = 500 , width = 1800 , height = 1000, stopwords= STOPWORDS).generate(\" \".join(disaster))\nplt.imshow(wordcloud,interpolation = 'bilinear')","1d8d439f":"# Common words in the non-disaster tweets\nndisaster = train.text[train.target[train.target==0].index]\nplt.figure(figsize = (16,24))\nwordcloud = WordCloud(min_font_size = 5,  max_words = 500 , width = 1800 , height = 1000, stopwords= STOPWORDS).generate(\" \".join(ndisaster))\nplt.imshow(wordcloud,interpolation = 'bilinear')","0bab078f":"# Tweet words frequency plot\ntrain_new = train.copy(deep=True)\ntrain_new['words'] = train_new['text'].apply(lambda x: clean(x))\nAll_words = []\nfor words in train_new['words']:\n    for word in words:\n            All_words.append(word)\nAll_words_freq = nltk.FreqDist(All_words)\nFreq_word_DF = pd.DataFrame({\"Data\":All_words_freq.most_common(15)})\nFreq_word_DF['Words'] = Freq_word_DF['Data'].apply(lambda x : x[0])\nFreq_word_DF['freq'] = Freq_word_DF['Data'].apply(lambda x : x[1])\nsns.set()\nsns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\nsns.set_context(\"paper\", font_scale=2)\nfig=plt.figure(figsize =(20,8),dpi=50)\nsns.barplot('Words','freq',data = Freq_word_DF)","7b9fbc9e":"# Number of characters in tweets\ndef length(text):    \n    '''a function which returns the length of text'''\n    return len(text)\ntrain_new['length'] = train_new['text'].apply(length)\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nbins = 150\nplt.hist(train_new[train_new['target'] == 0]['length'], alpha = 0.6, bins=bins, label='Non-Disaster')\nplt.hist(train_new[train_new['target'] == 1]['length'], alpha = 0.8, bins=bins, label='Disaster')\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,150)\nplt.grid()\nplt.show()","6f0db01c":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train_new[train_new['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='blue')\nax1.set_title('disaster tweets')\ntweet_len=train_new[train_new['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='red')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","443a25ca":"# Number of words in a tweet\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train_new[train_new['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='blue')\nax1.set_title('disaster tweets')\ntweet_len=train_new[train_new['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='red')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()","5eeb31e3":"# Average word length in a tweet\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=train_new[train_new['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='blue')\nax1.set_title('disaster')\nword=train_new[train_new['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet')","5543f222":"# Performing text vectorization so that data can be fed to the model\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntf_idf = TfidfVectorizer(analyzer= clean) #Using the above text cleaning to clean the data\nXtf_idfVector = tf_idf.fit_transform(train['text'])\nXtest_idfVector = tf_idf.transform(test['text'])","be64b974":"# Performing one hot encoding for categorical variables\ntrain_mod = pd.get_dummies(data=train, columns=['keyword'])\ntest_mod = pd.get_dummies(data=test, columns=['keyword'])","86369118":"# Dropping irrelevant columns from the modified data\ntrain_mod.drop('text', axis=1, inplace=True)\ntrain_mod.drop('target', axis=1, inplace=True)\ntest_mod.drop('text', axis=1, inplace=True)","b23a6d07":"# Creating the final dataframe consisting of keywords and text i.e. X Variable\nXfeatures_data = pd.concat([train_mod, \n                            pd.DataFrame(Xtf_idfVector.toarray())], axis = 1)\nX_test = pd.concat([test_mod, \n                    pd.DataFrame(Xtest_idfVector.toarray())], axis = 1)\n\n# Target variable i.e. Y Variable\ny_data = train.target","69c4e38e":"# Using randomforest classifier to train the data\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators= 50, max_depth= 20, n_jobs= -1)\nmodel1 = rf.fit(Xfeatures_data,y_data)\n\n#Predict Output using RF\npredicted_RF= rf.predict(X_test) \npredicted_RF","78d633f0":"# Using naive_bayes classifier to train the data\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nmodel2 = nb.fit(Xfeatures_data,y_data)\n\n#Predict Output using NB\npredicted_NB= nb.predict(X_test) \npredicted_NB","3f2e01ad":"# Using xgboost classifier to train the data\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier()\nmodel3 = xgb.fit(Xfeatures_data,y_data)\n\n#Predict Output using XGB\npredicted_XG= xgb.predict(X_test) \npredicted_XG","131127b3":"# Using SVC classifier to train the data\nfrom sklearn.svm import SVC\nsvc = SVC()\nmodel4 = svc.fit(Xfeatures_data,y_data)\n\n#Predict Output using SVC\npredicted_SC= svc.predict(X_test) \npredicted_SC","8ba6ddab":"# Using Voting classifier to train the data\nfrom sklearn.ensemble import VotingClassifier\nvoting_clf = VotingClassifier(estimators=[('SVC', svc), ('XGB', xgb), ('NB', nb), ('RF', rf)], voting='hard')\nmodel5 = voting_clf.fit(Xfeatures_data,y_data)\n\n#Predict Output using Voting\npredicted_VC= voting_clf.predict(X_test) \npredicted_VC","5c2a125d":"# Using Neural_networks to train the data\nfrom keras.layers import Dense, Conv1D, MaxPool1D, Flatten, Dropout, Embedding, LSTM\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nmodel = Sequential()\nmodel.add(Dense(units = 512 , activation = 'relu' , input_dim = Xfeatures_data.shape[1]))\nmodel.add(Dense(units = 256 , activation = 'relu'))\nmodel.add(Dense(units = 100 , activation = 'relu'))\nmodel.add(Dense(units = 10 , activation = 'relu'))\nmodel.add(Dense(units = 1 , activation = 'sigmoid'))\nmodel.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\nmodel6 = model.fit(Xfeatures_data,y_data, batch_size=128, epochs=10)\n\n#Predict Output using NN\npredicted_NN= model.predict_classes(X_test) \npredicted_NN","81fa21b8":"#Saving output results of NB to csv file\npredicted_df = pd.DataFrame(predicted_NB)\npredicted_df.to_csv('out.csv')","8e1ea333":"# Don't Forget to Upvote, It's Free","5728c684":"NB Score : 0.79550","7bc58a05":"Voting Score : 0.78016","981fe783":"NN Score : 0.78936","0afec06f":"XGB Score : 0.78732","89910b52":"RF Score : 0.71165","f045bd05":"**Input Data and Pre-processing**","6163988b":"**Text Vectorization and Finalizaing Independent and dependent variables**","175abe31":"**Text Pre-processing**","245b89e1":"**EDA and Visualizations**","7601d0aa":"**Predictions with classifiers : RF, NB, XGB, SVC, Voting, NN**","bd4d8a9f":"SVC Score : 0.79447"}}