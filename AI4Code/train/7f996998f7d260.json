{"cell_type":{"f007d298":"code","35d266ad":"code","bed31afc":"code","b82c763d":"code","d369c05d":"code","691c8ddb":"code","3c7627a2":"code","4b2352a9":"code","28c52b41":"code","bb9aef3c":"code","34e1c02f":"code","caab9edf":"code","fae78b58":"code","3ce7d1a1":"code","8f88c1a7":"code","1f1b711f":"code","8599b5c7":"code","a2056851":"code","594120ee":"code","cedceb56":"code","2824379b":"code","cf6d5499":"code","dd564500":"code","f5d67e8c":"code","94b9b983":"code","a25739e2":"code","af793ee4":"code","4deb9abc":"code","2f3f6233":"code","f717b252":"code","de92b363":"code","98cfe800":"code","44a83605":"code","9a7216ec":"code","9e45eaa0":"code","de8c7aed":"code","5d274c53":"code","68db85a4":"markdown","037f5ca4":"markdown","ba21f3eb":"markdown","1763dc68":"markdown","037161cd":"markdown","97869187":"markdown","65f37c39":"markdown","d30f3857":"markdown","bf857fb2":"markdown","5eeaa2e3":"markdown","e56f4e4d":"markdown","7f8cb766":"markdown","88476fa9":"markdown","95d7c71e":"markdown","4d48696e":"markdown"},"source":{"f007d298":"import tensorflow as tf\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nimport re\nimport numpy as np\nimport os\nimport time\nimport json\nimport gc\nfrom glob import glob\nfrom PIL import Image\nimport pickle\nimport pandas as pd\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications import efficientnet\n\nimport string \nfrom numpy import array\n\n\nimport sys,warnings \nwarnings.filterwarnings(\"ignore\")\n\nfrom tqdm import tqdm\n\nfrom nltk.translate.bleu_score import sentence_bleu\n\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications import EfficientNetB7\nfrom tensorflow.keras.applications import InceptionResNetV2\nfrom tensorflow.keras.applications import ResNet50\nimport cv2 \nfrom glob import glob\n\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Dense, BatchNormalization\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import add\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Dense, GRU, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate\nfrom tensorflow.keras.optimizers import RMSprop, Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom kaggle_datasets import KaggleDatasets\n","35d266ad":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","bed31afc":"LOCAL_FLICKR_PATH = '\/kaggle\/input\/flickr-image-dataset\/flickr30k_images\/'\nannotation_file = LOCAL_FLICKR_PATH + 'results.csv'\nLOCAL_IMG_PATH = LOCAL_FLICKR_PATH + 'flickr30k_images\/'\n\n!ls {LOCAL_IMG_PATH} | wc","b82c763d":"%%time\n## This steps will take around 25 minutes offline ...\nif strategy.num_replicas_in_sync == 8:\n#     GCS_DS_PATH_FLICKR = KaggleDatasets().get_gcs_path('flickr8k-sau') # 2gb # 5 mins\n    GCS_DS_PATH = KaggleDatasets().get_gcs_path('flickr-image-dataset') # 8gb # 20-25 mins\n    print('yeah')","d369c05d":"BATCH_SIZE = 64 * strategy.num_replicas_in_sync","691c8ddb":"#Image Path\n# Image path given ------ used glob to get list of all images\nimage_path = \"..\/input\/flickr-image-dataset\/flickr30k_images\/flickr30k_images\/\"\njpgs = os.listdir(image_path)\nimages = glob(image_path+'*.jpg')\n\n\n\n#Caption Path\n# Read the captions with Pandas ---- changed coloumn names\ncaption_path = '..\/input\/flickr-image-dataset\/flickr30k_images\/results.csv'\ncaptions = pd.read_csv(caption_path, sep = '|')\ndf = pd.DataFrame(captions)\ndf = df.rename(columns={'image_name' : 'filename',' comment_number': 'index', ' comment': 'caption'}, inplace = False)\ndf = df.reindex(columns =['index','filename','caption'])\ndata = df[df['caption'].notnull()]\n\n\n\n#Printing DataFrame\n# DataFrame name is Data\nprint(\"Total Images in Dataset = {}\".format(len(jpgs)))\ndata\n","3c7627a2":"# Made a list of images, above list had sets of images, this also has replciated images \n# ( 5 same image paths of 5 captions)\nall_img_name_vector = []\nfor annot in data[\"filename\"]:\n   full_image_path = image_path + annot\n   all_img_name_vector.append(full_image_path)","4b2352a9":"#To know lenght of list\nlen(all_img_name_vector)","28c52b41":"\n# from tqdm import tqdm, tqdm_notebook\n# tqdm.pandas()\n\n# def out_path_fn(text):\n#     return '.\/image_features\/' + str(text)\n\n# data['out_path'] = data['filename'].progress_apply(out_path_fn)\n# #data.out_path.values[:6]\n\n# #-----------------------------------------#\n\n# out_path = []\n# for path in data[\"out_path\"].astype(str):\n#    out_path.append(path)\n# out_path = sorted(set(out_path))\n# print(out_path[:6])","bb9aef3c":"#tf.strings.split(images[0], sep='\/')[-1]\n#'.\/image_features\/' + tf.strings.split(images[0], sep='\/')[-1]\n#os.makedirs('.\/image_features')","34e1c02f":"# def load_image(image_path):\n#    img = tf.io.read_file(image_path)\n#    img = tf.image.decode_jpeg(img, channels=3)\n#    img = tf.image.resize(img, (224, 224))\n#    img = tf.keras.applications.resnet.preprocess_input(img)\n#    image_path = '.\/image_features\/' + tf.strings.split(image_path, sep='\/')[-1]\n#    return img, image_path\n\n#extract_model = tf.keras.applications.ResNet50(include_top=True)\n#last = extract_model.layers[-2].output\n#image_features_extract_model = Model(inputs = extract_model.input,outputs = last)\n##image_features_extract_model.summary()","caab9edf":"# images = sorted(images)\n# image_dataset = tf.data.Dataset.from_tensor_slices(images)\n# image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(1)","fae78b58":"# for img,path in image_dataset.take(3):\n#    print(path)","3ce7d1a1":"# %%time\n# for img, path in tqdm(image_dataset):\n#     batch_features = image_features_extract_model(img)\n#     batch_features = tf.reshape(batch_features,\n#                               (2048, )) # batch_features.shape[0], -1, batch_features.shape[3]\n#     #print(batch_features)\n    \n#     for bf, p in zip([batch_features], path):\n#         #print(bf, p)\n#         path_of_feature = p.numpy().decode(\"utf-8\")\n#         np.save(path_of_feature, bf.numpy())\n#         #print(path_of_feature, bf.numpy())","8f88c1a7":"# images_features = {}\n# count = 0\n# for i in out_path:\n#     img_feature = np.load(i + '.npy')\n#     img_name = i.split('\/')[-1]\n    \n#     images_features[img_name] = img_feature\n# len(images_features)    ","1f1b711f":"# np.save('images_features.npy',images_features)","8599b5c7":"# URL : https:\/\/www.kaggle.com\/syedumairhassankazmi\/flickr30k-to-resnet50-dictionary\n# Inputted the data from the above URL\n\nimages_features = np.load('..\/input\/flickr30k-to-resnet50-dictionary\/images_features.npy' , allow_pickle = True)","a2056851":"##  get image features dictionary out of numpy ndarray\n\nimages_features = np.reshape(images_features, (-1))\nimages_features = images_features[0]\n\n### get image features list\n\nimages_features_list  = list(images_features.values())","594120ee":"## some code for code checking\n\nprint(images_features['10002456.jpg'])\nprint(len(images_features['10002456.jpg']))\nprint(len(images_features))\n","cedceb56":"#Some functions to make cleaner captions\n\n# To remove punctuations\ndef remove_punctuation(text_original):\n    text_no_punctuation = text_original.translate(string.punctuation)\n    return(text_no_punctuation)\n\n# To remove single characters\ndef remove_single_character(text):\n    text_len_more_than1 = \"\"\n    for word in text.split():\n        if len(word) > 1:\n            text_len_more_than1 += \" \" + word\n    return(text_len_more_than1)\n\n# To remove numeric values\ndef remove_numeric(text):\n    text_no_numeric = \"\"\n    for word in text.split():\n        isalpha = word.isalpha()\n        if isalpha:\n            text_no_numeric += \" \" + word\n    return(text_no_numeric)\n","2824379b":"#Applying those functions\n\ndef text_clean(text_original):\n    text = remove_punctuation(text_original)\n    text = remove_single_character(text)\n    text = remove_numeric(text)\n    return(text)\n    \nfor i, caption in enumerate(data.caption.values):\n    newcaption = text_clean(caption)\n    data[\"caption\"].iloc[i] = newcaption","cf6d5499":"#Adding start and end token\nfrom tqdm import tqdm, tqdm_notebook\ntqdm.pandas()\nSTART_TOKEN = '<start> '\nEND_TOKEN = ' <end>'\n\ndef add_start_end(text):\n    return START_TOKEN + str(text) + END_TOKEN\n\ndata['caption'] = data['caption'].progress_apply(add_start_end)\ndata.caption.values[:6]","dd564500":"#Making a list of captions out of the pandas dict\ncaptions_list = data['caption']\n#captions_list[-20:-10]","f5d67e8c":"len(captions_list)\n#print(captions_dict)","94b9b983":"#Tokenizing the captions list and padding them\n\ntop_k = 18000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n                                                 oov_token=\"<unk>\",\n                                                 filters='!\"#$%&()*+.,-\/:;=?@[\\]^_`{|}~ ')\n\ntokenizer.fit_on_texts(captions_list)\ntokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'\n\ncaptions_list_seq = tokenizer.texts_to_sequences(captions_list)\ncaptions_list_seq = tf.keras.preprocessing.sequence.pad_sequences(captions_list_seq, padding='post')\n\n#Will be used in prediction step\ntoken_start = tokenizer.word_index[START_TOKEN.strip()]\ntoken_end = tokenizer.word_index[END_TOKEN.strip()]\n","a25739e2":"#A function to get the maximum length of the biggest sentence in the captions list\n\ndef calc_max_length(tensor):\n    return max(len(t) for t in tensor)\n\nmax_length = calc_max_length(captions_list_seq) \nprint(max_length)\n","af793ee4":"#captions_list_seq[:5]","4deb9abc":"# Importing some libraries for splitting data\n# Since each image has 5 captions so if we randomly shuffle them then some train data\n# will leak into the Validation data\n# hence we will use kfold groupfold method to make sure each caption of the same image stays in the\n# same set\n\nfrom sklearn.model_selection import KFold, GroupKFold\n\n# 40 splits = 2.5 Val Data\nkf = GroupKFold(n_splits=40).split(X=all_img_name_vector, groups=all_img_name_vector)\n\nfor ind, (tr, val) in enumerate(kf):\n    img_train = np.array(all_img_name_vector)[tr] # np.array make indexing possible\n    img_val = np.array(all_img_name_vector)[val]\n    \n    cap_train =  captions_list_seq[tr]\n    cap_val =  captions_list_seq[val]\n    \n    break\n\n# Image and captions train and validation sets created","2f3f6233":"#len(img_val)","f717b252":"# Function to create a list of image features by \n# mapping image path to it's features using the image_features dictionary\n\ndef get_image_encoding(image_ids):\n   encoding=[]\n   for idx in image_ids:\n     encoding.append(images_features[idx.split('\/')[-1]])\n   return np.array(encoding)\n\nencode_train=get_image_encoding(img_train)\nencode_val=get_image_encoding(img_val)","de92b363":"# Function to create dataset that is to be fed in the model\n# Using tensor.data to apply inner function to all of the dataset easily \n\ndef create_dataset(data,labels,batch_size):\n  def map_func(img_encode, cap):\n    x = {'decoder_input': cap[0:-1],'encoder_input': img_encode}\n    y = {'decoder_output': cap[1:]}\n    \n    return x,y\n  \n  \n  dataset = tf.data.Dataset.from_tensor_slices((data,labels))\n  dataset = dataset.map(map_func)\n  dataset = dataset.repeat()\n  dataset = dataset.shuffle(data.shape[0]).batch(batch_size)\n  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n  return dataset","98cfe800":"# Some Input values, Batch size according to TPU ---- go to top\n\nBATCH_SIZE = BATCH_SIZE\nembedding_dim = 512\nembedding_size = 512\nunits = 512\nvocab_size = len(tokenizer.word_index) \nmax_len = max_length -1\nencoder_shape=2048\nsteps_per_epoch = int(len(img_train) \/ BATCH_SIZE)\n\n\n# To train using TPU\n\nwith strategy.scope():\n        \n    #Layers applied to the CNN block of the model\n    x_input = Input(shape=(encoder_shape,),name='encoder_input')\n    x = Dense(512,activation='relu')(x_input)\n    x = RepeatVector(max_len,name='encoder_dense')(x)\n    \n    #LAyer applied to the RNN block of the model\n    y1_input = Input(shape=(None, ), name='decoder_input')\n    y1 = Embedding(input_dim=vocab_size,\n                                  output_dim=embedding_dim,input_length=max_len,\n                                  name='decoder_embedding')(y1_input)\n    y1 = LSTM(units, name='decoder_gru1',\n                       return_sequences=True)(y1)\n    y1 = LSTM(units, name='decoder_gru2',\n                       return_sequences=True)(y1)\n    y1 = LSTM(units, name='decoder_gru3',\n                       return_sequences=True)(y1)\n    y1 = Dropout(0.8)(y1)\n    \n    #Concatenating the CNN and RNN block and then adding more layers \n    conca = Concatenate()([x, y1])\n    y2 = LSTM(1024, return_sequences=True)(conca)\n    y2 = Dropout(0.8)(y2)\n    y2 = LSTM(512, return_sequences=True)(y2)\n    y2 = LSTM(512, return_sequences=True)(y2)\n    y2 = Dropout(0.8)(y2)\n    y2_output = Dense(vocab_size,\n                         activation='linear',\n                         name='decoder_output')(y2)\n\n    model = Model(inputs=[x_input, y1_input],\n                          outputs=[y2_output])\n\n    model.summary()\n    \n\n    model.compile(optimizer=Adam(lr=1e-3),\n                  loss= SparseCategoricalCrossentropy(from_logits=True), \n                  metrics=['accuracy']) \n    \n    \n    #to stop if validation stops increasing, to increase LR on platue, to save the best model\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_loss'),\n        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.2, patience=3, min_lr=0.001),\n    ]\n","44a83605":"#actually creating the train and val set using the function created above\n\ntrain_dataset=create_dataset(encode_train,cap_train,BATCH_SIZE)\nval_dataset=create_dataset(encode_val,cap_val,BATCH_SIZE)","9a7216ec":"model.fit(train_dataset, epochs=50,\n          validation_data=val_dataset,\n          steps_per_epoch=steps_per_epoch,\n          validation_steps=3,\n      callbacks=callbacks)","9e45eaa0":"#Saving stuff for later use\nmodel.save('model_ImageCap_Umair.h5')\nmodel.save_weights('model_ImageCap_Umair_weights.h5')\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)","de8c7aed":"#Function to load an image from flickr database and predict its caption\n\ndef generate_caption(image_id,true_caption,max_tokens=max_len):\n    \n    encoder_input = images_features[image_id.split('\/')[-1]]\n    encoder_input = np.expand_dims(encoder_input, axis=0)\n\n    shape = (1, max_tokens)\n    decoder_input = np.zeros(shape=shape, dtype=np.int)\n\n    token_id = token_start\n  \n    output=[]\n\n    count_tokens = 0\n\n    while token_id != token_end and count_tokens < max_tokens:\n        \n        decoder_input[0, count_tokens] = token_id\n\n        input_data ={'encoder_input':encoder_input ,'decoder_input': decoder_input}\n        \n        predict = model.predict(input_data)\n        \n        token_id = np.argmax(predict[0, count_tokens, :])\n        \n        output.append(token_id)\n        \n        count_tokens += 1\n    \n    print('Predicted caption',tokenizer.sequences_to_texts([output]))\n    print('True captions',tokenizer.sequences_to_texts([true_caption]))\n    \n    from tensorflow.keras.preprocessing import image\n\n    def preprocess(image_path):\n        img = image.load_img(image_path, target_size=(299, 299))\n        x = image.img_to_array(img)\n        x = np.expand_dims(x, axis=0)\n\n        x \/= 255.\n        return x\n\n    plt.imshow(np.squeeze(preprocess(image_id)))\n    \n\n# Change the number to see the results\ngenerate_caption(img_val[1465],cap_val[1465])\n","5d274c53":"#Import Librarires\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pickle\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\n\n\n\n#Import resnet model for feature extraction\nextract_model = ResNet50(include_top=True)\nlast = extract_model.layers[-2].output\nimage_features_extract_model = Model(inputs = extract_model.input,outputs = last)\n\n#Import the CNN RNN model  \nreconstructed_model = load_model(\".\/model_ImageCap_Umair.h5\")\n\n#Import tokenizor from pickle file\nwith open('.\/tokenizer.pickle', 'rb') as handle:\n    tokenizer = pickle.load(handle)\n    \n\n    \n#State vocab size, max length(max tokens)\nvocab_size = len(tokenizer.word_index) \nmax_tokens = 73\nSTART_TOKEN = '<start> '\nEND_TOKEN = ' <end>'\ntoken_start = tokenizer.word_index[START_TOKEN.strip()]\ntoken_end = tokenizer.word_index[END_TOKEN.strip()]\n\n\n#Import the file, incase of an application the user will upload the file here\nimage_path = '..\/input\/flickr-image-dataset\/flickr30k_images\/flickr30k_images\/1000366164.jpg'\n\n#Presprocess the image\nimg = image.load_img(image_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nimage = tf.keras.applications.resnet.preprocess_input(x)\n\n#Use the prediction coding,extarct image features from ResNet50 model and RNN features from ------>\n#Reconstructed model, use that model to predict\nencoder_input = image_features_extract_model.predict(image)\nencoder_input = tf.reshape(encoder_input,\n                             (2048, ))\n\nencoder_input = np.expand_dims(encoder_input, axis=0)\n\nshape = (1, max_tokens)\ndecoder_input = np.zeros(shape=shape, dtype=np.int)\n\ntoken_id = token_start\n\noutput=[]\n\ncount_tokens = 0\n\nwhile token_id != token_end and count_tokens < max_tokens:\n    \n    decoder_input[0, count_tokens] = token_id\n\n    input_data ={'encoder_input':encoder_input ,'decoder_input': decoder_input}\n    \n    predict = reconstructed_model.predict(input_data)\n    \n    token_id = np.argmax(predict[0, count_tokens, :])\n    \n    output.append(token_id)\n    \n    count_tokens += 1\n\n#Print prediction    \nfinal = tokenizer.sequences_to_texts([output])\nprint(final)\n#Print Image\nplt.imshow(np.squeeze(x \/255.))","68db85a4":"Made a fn and a model ---- passed them through a pipeline -------- saved all values ----- made a dictionary --------- saved the dictionary","037f5ca4":"# Model","ba21f3eb":"**Importing**","1763dc68":"# Importing Libraries and Turning on TPU","037161cd":"# Train\/Val Split","97869187":"Made a new list which has the filnames for output destination","65f37c39":"# Prediction","d30f3857":"### Captions Preprocessing","bf857fb2":"# Tokenizing","5eeaa2e3":"some code to check code and make directory","e56f4e4d":"#  Data Preprocessing","7f8cb766":"**Code to run on TPU**","88476fa9":"### IMAGE FEATURES PREPROCESSING\n##### Used all these codes to get images features, made a dictionary out of them \n##### and then saved them\n##### commiting all code lines, uploading the file on kaggle and then just loading the dictionary back\n##### to save time for multiple runs\n##### also for some path reason i ran the image feature code on gpu, will run the rest on tpu","95d7c71e":"**Code below can be used inside an application or a web browser**\n**To predict caption of any image uploaded**","4d48696e":"Now, reloading the dictionary "}}