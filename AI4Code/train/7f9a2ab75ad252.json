{"cell_type":{"16cafb09":"code","51d77bf1":"code","17a7fc5b":"code","ee733233":"code","4f009793":"code","29771137":"code","96f75c69":"markdown","b92d8e2c":"markdown","5c2dc7dc":"markdown"},"source":{"16cafb09":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","51d77bf1":"# Import Libraries\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset","17a7fc5b":"# load data\ntrain = pd.read_csv(r\"\/kaggle\/input\/digit-recognizer\/train.csv\",dtype = np.float32)\n\n# split data into features(pixels) and labels(numbers from 0 to 9)\ntargets_numpy = train.label.values\nfeatures_numpy = train.loc[:,train.columns != \"label\"].values\/255 # normalization\n\n# train test split. Size of train data is 80% and size of test data is 20%. \nfeatures_train, features_test, targets_train, targets_test = train_test_split(features_numpy,\n                                                                             targets_numpy,\n                                                                             test_size = 0.2,\n                                                                             random_state = 42) \n\n# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\nfeaturesTrain = torch.from_numpy(features_train)\ntargetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n\n# create feature and targets tensor for test set.\nfeaturesTest = torch.from_numpy(features_test)\ntargetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long\n\n# batch_size, epoch and iteration\nbatch_size = 100\nn_iters = 6000\nnum_epochs = n_iters \/ (len(features_train) \/ batch_size)\nnum_epochs = int(num_epochs)\nprint(\"Epoch Number: \",num_epochs)\n\n# Pytorch train and test sets\ntrain = TensorDataset(featuresTrain,targetsTrain)\ntest = TensorDataset(featuresTest,targetsTest)\n\n# data loader\ntrain_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n\n# visualize one of the images in data set\nplt.imshow(features_numpy[10].reshape(28,28))\nplt.axis(\"off\")\nplt.title(str(targets_numpy[10]))\nplt.savefig('graph.png')\nplt.show()\n\nprint(len(train_loader.dataset))\nprint(len(test_loader.dataset))\n","ee733233":"class LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n        super(LSTMModel, self).__init__()\n        \n        # Hidden dimensions\n        self.hidden_dim = hidden_dim\n\n        # Number of hidden layers\n        self.layer_dim = layer_dim\n\n        # LSTM\n        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True) # batch_first=True (batch_dim, seq_dim, feature_dim)\n\n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # Initialize hidden state with zeros\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n\n        # Initialize cell state\n        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n\n        # 28 time steps\n        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n        # If we don't, we'll backprop all the way to the start even after going through another batch\n        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n\n        # Index hidden state of last time step\n        # out.size() --> 100, 28, 100\n        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n        out = self.fc(out[:, -1, :]) \n        # out.size() --> 100, 10\n        return out\n    \ninput_dim = 28\nhidden_dim = 100\nlayer_dim = 1\noutput_dim = 10\nmodel = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n\nerror = nn.CrossEntropyLoss()\n\nlearning_rate = 0.1\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) ","4f009793":"# Number of steps to unroll\nseq_dim = 28  \nloss_list = []\niteration_list = []\naccuracy_list = []\ncount = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as a torch tensor with gradient accumulation abilities\n        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n\n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n\n        # Forward pass to get output\/logits\n        # outputs.size 100, 10\n        outputs = model(images)\n\n        # Calculate Loss: softmax --> cross entropy loss\n        loss = error(outputs, labels)\n\n        # Getting gradients\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n        count += 1\n\n        if count % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            for images, labels in test_loader:\n                \n                images = images.view(-1, seq_dim, input_dim)\n\n                # Forward pass only to get logits\/output\n                outputs = model(images)\n\n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n\n                # Total number of labels\n                total += labels.size(0)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n\n            accuracy = 100 * correct \/ total\n            \n            loss_list.append(loss.data.item())\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n            \n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(count, loss.data.item(), accuracy))","29771137":"# visualization loss \nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"LSTM: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.plot(iteration_list,accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"LSTM: Accuracy vs Number of iteration\")\nplt.savefig('graph.png')\nplt.show()","96f75c69":"## Conclusion\nIn this tutorial, we learn:\n\n1. Basics of pytorch\n1. Linear regression with pytorch\n1. Logistic regression with pytorch\n1. Artificial neural network with with pytorch\n1. Convolutional neural network with pytorch\n    1. https:\/\/www.kaggle.com\/kanncaa1\/pytorch-tutorial-for-deep-learning-lovers\/code\n1. Recurrent neural network with pytorch\n1. Long-Short Term Memory (LSTM)\n    1. https:\/\/www.kaggle.com\/kanncaa1\/long-short-term-memory-with-pytorch\n\nIf you have any question or suggest, I will be happy to hear it","b92d8e2c":"Long-Short Term Memory (LSTM)\n<br>\n<br>**Steps of LSTM:**\n1. Import Libraries\n1. Prepare Dataset\n1. Create LSTM Model\n    1. hidden layer dimension is 100\n    1. number of hidden layer is 1\n1. Instantiate Model\n1. Instantiate Loss\n    1. Cross entropy loss\n    1. It also has softmax(logistic function) in it.\n1. Instantiate Optimizer\n    1. SGD Optimizer\n1. Traning the Model\n1. Prediction","5c2dc7dc":"# INTRODUCTION\n* It\u2019s a Python based scientific computing package targeted at two sets of audiences:\n    * A replacement for NumPy to use the power of GPUs\n    * Deep learning research platform that provides maximum flexibility and speed\n* pros:\n    * Iinteractively debugging PyTorch. Many users who have used both frameworks would argue that makes pytorch significantly easier to debug and visualize.\n    * Clean support for dynamic graphs\n    * Organizational backing from Facebook\n    * Blend of high level and low level APIs\n* cons:\n    * Much less mature than alternatives\n    * Limited references \/ resources outside of the official documentation\n* I accept you know neural network basics. If you do not know check my tutorial. Because I will not explain neural network concepts detailed, I only explain how to use pytorch for neural network\n* Neural Network tutorial: https:\/\/www.kaggle.com\/kanncaa1\/deep-learning-tutorial-for-beginners\n* The most important parts of this tutorial from matrices to ANN. If you learn these parts very well, implementing remaining parts like CNN or RNN will be very easy.\n\nContent:\n1. Basics of Pytorch, Linear Regression, Logistic Regression, Artificial Neural Network (ANN), Concolutional Neural Network (CNN)\n    * https:\/\/www.kaggle.com\/kanncaa1\/pytorch-tutorial-for-deep-learning-lovers\/code\n1. Recurrent Neural Network (RNN)\n1. Long-Short Term Memory (LSTM)\n    * https:\/\/www.kaggle.com\/kanncaa1\/long-short-term-memory-with-pytorch"}}