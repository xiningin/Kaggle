{"cell_type":{"d01d1bf3":"code","9d84c923":"code","d7cc476d":"code","53a6f603":"code","8cfb9345":"code","169b84c5":"code","5a411dd0":"code","ea5ce9e0":"code","d3c70e93":"code","7ff2a906":"code","add9d149":"code","ddec2b1d":"code","61622c77":"code","ddb92985":"code","14450aca":"code","7562da73":"code","3ad9ef69":"code","f957972b":"code","40deee75":"code","8255662e":"code","2f1a8f6e":"code","8dc6ff55":"code","97b787c6":"code","30856b72":"code","9eaf18bf":"code","c2066a20":"code","f8d52ad4":"code","7788524a":"code","cbfaaae5":"code","53ebb31e":"code","48905019":"code","9c3ddaa2":"code","06079c78":"code","a2993021":"code","e46475ec":"code","df865481":"code","a352376e":"code","78bfbfd2":"code","6bda27e1":"code","873661a2":"code","3455ae0f":"code","53ecf1cf":"code","685923a0":"code","6a9ea60c":"code","bb7ae5ac":"code","8f1703ef":"code","4950b670":"code","b14064e0":"code","8d905d55":"code","6e429679":"code","ffb2e07b":"code","868ce394":"code","2450d07f":"code","d9356f75":"code","64a8d20f":"markdown","f351cac4":"markdown","94974f88":"markdown","07d7a343":"markdown","98532a34":"markdown","39f43e7a":"markdown","87df9166":"markdown","691711f8":"markdown","ec786142":"markdown","08f7f865":"markdown","632f845f":"markdown","8ceb1603":"markdown","1ba4debf":"markdown","c0ff1760":"markdown","a99639d0":"markdown","fae6fc32":"markdown","f75b40ac":"markdown","04a08f23":"markdown","b5b425ef":"markdown","fdf09193":"markdown","6963dd36":"markdown","867fb095":"markdown","fbaa054a":"markdown","fdb36b6d":"markdown","9536ed9a":"markdown","bfde69a0":"markdown","ced5b53e":"markdown","03e44c72":"markdown","ec374209":"markdown","0cd374b8":"markdown","89cdb21a":"markdown","399f5930":"markdown","16717e7e":"markdown","8d44c951":"markdown","2c47b436":"markdown","55a30bdb":"markdown","d7fb3e16":"markdown","15cd795e":"markdown","df9c0231":"markdown","d345700d":"markdown","2d9cd254":"markdown","ca3b23f6":"markdown","c059fdd8":"markdown","e367f3d9":"markdown","1c52f9a0":"markdown"},"source":{"d01d1bf3":"import numpy as np\nimport pandas as pd\nfrom pandas import Series\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn')\nsns.set(font_scale=2.5) \nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n%matplotlib inline\n\ndf_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\ndf_train['FamilySize'] = df_train['SibSp'] + df_train['Parch'] + 1 # \uc790\uc2e0\uc744 \ud3ec\ud568\ud574\uc57c\ud558\ub2c8 1\uc744 \ub354\ud569\ub2c8\ub2e4\ndf_test['FamilySize'] = df_test['SibSp'] + df_test['Parch'] + 1 # \uc790\uc2e0\uc744 \ud3ec\ud568\ud574\uc57c\ud558\ub2c8 1\uc744 \ub354\ud569\ub2c8\ub2e4\n\ndf_test.loc[df_test.Fare.isnull(), 'Fare'] = df_test['Fare'].mean()\n\ndf_train['Fare'] = df_train['Fare'].map(lambda i: np.log(i) if i > 0 else 0)\ndf_test['Fare'] = df_test['Fare'].map(lambda i: np.log(i) if i > 0 else 0)","9d84c923":"df_train['Ticket'].value_counts()","d7cc476d":"df_train['Initial']= df_train.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n    \ndf_test['Initial']= df_test.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations","53a6f603":"#Checking the Initials with the Sex\npd.crosstab(df_train['Initial'], df_train['Sex']).T.style.background_gradient(cmap='summer_r') ","8cfb9345":"#It replaces a certain value in the series \"Initials\" with a different value. \n\n\ndf_train['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don', 'Dona'],\n                        ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr', 'Mr'],inplace=True)\n\ndf_test['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don', 'Dona'],\n                        ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr', 'Mr'],inplace=True)","169b84c5":"df_train.groupby('Initial').mean()\n","5a411dd0":"df_train.groupby('Initial')['Survived'].mean().plot.bar(color=['black', 'red', 'green', 'blue', 'cyan'])","ea5ce9e0":"#Combine 'Train' and 'Test' data to populate the null values at once.\ndf_all = pd.concat([df_train, df_test])","d3c70e93":"df_all.reset_index(drop=True)","7ff2a906":"df_all.groupby('Initial').mean()","add9d149":"#Put below code into 'loc'\ndf_train['Survived'] == 1","ddec2b1d":"#So just return the survived one like this!! You can set it like this. It's useful just to look at it'\ndf_train.loc[df_train['Survived'] == 1]","61622c77":"#\uc6b0\ub9ac\ub294 \uc774\uc81c \uc5ec\uae30\uc11c null \uac12\uc744 \ucc44\uc6cc\uc57c \ud558\ub294\uac83. \ndf_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Mr'),'Age'] = 33\ndf_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Mrs'),'Age'] = 36\ndf_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Master'),'Age'] = 5\ndf_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Miss'),'Age'] = 22\ndf_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Other'),'Age'] = 46\n\ndf_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Mr'),'Age'] = 33\ndf_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Mrs'),'Age'] = 36\ndf_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Master'),'Age'] = 5\ndf_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Miss'),'Age'] = 22\ndf_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Other'),'Age'] = 46","ddb92985":"df_train['Age'].isnull().sum()","14450aca":"df_test['Age'].isnull().sum()","7562da73":"df_train['Embarked']","3ad9ef69":"df_train['Embarked'].isnull().sum()","f957972b":"#null\uac12\uc744 \ucd5c\ub300\uac12\uc73c\ub85c \ucc44\uc6b0\ub294 \ubc29\ubc95 \ndf_train['Embarked'].fillna('S', inplace=True)","40deee75":"df_train['Embarked'].isnull().sum()","8255662e":"df_train['Age_cat'] = 0","2f1a8f6e":"df_train.head()","8dc6ff55":"df_train.loc[df_train['Age'] <10, 'Age_cat'] = 0\ndf_train.loc[(10 <= df_train['Age'])&(df_train['Age']<20), 'Age_cat'] =1\ndf_train.loc[(20 <= df_train['Age'])&(df_train['Age']<30), 'Age_cat'] =2\ndf_train.loc[(30 <= df_train['Age'])&(df_train['Age']<40), 'Age_cat'] =3\ndf_train.loc[(40 <= df_train['Age'])&(df_train['Age']<50), 'Age_cat'] =4\ndf_train.loc[(50 <= df_train['Age'])&(df_train['Age']<60), 'Age_cat'] =5\ndf_train.loc[(60 <= df_train['Age'])&(df_train['Age']<70), 'Age_cat'] =6\ndf_train.loc[(70 <= df_train['Age']), 'Age_cat'] = 7","97b787c6":"df_train.head()","30856b72":"df_test.loc[df_train['Age'] <10, 'Age_cat'] = 0\ndf_test.loc[(10 <= df_train['Age'])&(df_train['Age']<20), 'Age_cat'] =1\ndf_test.loc[(20 <= df_train['Age'])&(df_train['Age']<30), 'Age_cat'] =2\ndf_test.loc[(30 <= df_train['Age'])&(df_train['Age']<40), 'Age_cat'] =3\ndf_test.loc[(40 <= df_train['Age'])&(df_train['Age']<50), 'Age_cat'] =4\ndf_test.loc[(50 <= df_train['Age'])&(df_train['Age']<60), 'Age_cat'] =5\ndf_test.loc[(60 <= df_train['Age'])&(df_train['Age']<70), 'Age_cat'] =6\ndf_test.loc[(70 <= df_train['Age']), 'Age_cat'] = 7","9eaf18bf":"df_test.head()","c2066a20":"def category_age(x):\n    if x < 10:\n        return 0\n    elif x < 20:\n        return 1\n    elif x < 30:\n        return 2\n    elif x < 40:\n        return 3\n    elif x < 50:\n        return 4\n    elif x < 60:\n        return 5\n    elif x < 70:\n        return 6\n    else:\n        return 7    \n    \n","f8d52ad4":"df_train['Age_cat_2'] = df_train['Age'].apply(category_age)","7788524a":"#\ub450\uac1c\ub97c \ud569\uccd0\uc11c \ube44\uad50\ud574\ubcf4\ub294 \uac83\uc774\ub2e4 .all \ud568\uc218\ub294 \uc774\uc81c \ubaa8\ub4e0\uac12\uc774 true\uba74 true\uac12\uc744 \uc8fc\ub294 \uac83\n(df_train['Age_cat'] == df_train['Age_cat_2']).any()","cbfaaae5":"df_train.drop(['Age', 'Age_cat_2'], axis=1, inplace=True)\ndf_test.drop(['Age'], axis=1, inplace=True)","53ebb31e":"df_train['Initial'] = df_train['Initial'].map({'Master': 0, 'Miss': 1, 'Mr': 2, 'Mrs': 3, 'Other': 4})\ndf_test['Initial'] = df_test['Initial'].map({'Master': 0, 'Miss': 1, 'Mr': 2, 'Mrs': 3, 'Other': 4})","48905019":"df_train['Embarked'].unique()","9c3ddaa2":"df_train['Embarked'].value_counts()","06079c78":"df_train['Embarked'] = df_train['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\ndf_test['Embarked'] = df_test['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})","a2993021":"df_train['Embarked']","e46475ec":"df_train['Embarked'].isnull().any()","df865481":"df_train['Sex'] = df_train['Sex'].map({'female': 0, 'male': 1})\ndf_test['Sex'] = df_test['Sex'].map({'female': 0, 'male': 1})","a352376e":"heatmap_data = df_train[['Survived', 'Pclass', 'Sex', 'Fare', 'Embarked', 'FamilySize', 'Initial', 'Age_cat']] \n\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14, 12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(heatmap_data.astype(float).corr(), linewidths=0.1, vmax=1.0,\n           square=True, cmap=colormap, linecolor='white', annot=True, annot_kws={\"size\": 16})\n\ndel heatmap_data","78bfbfd2":"df_train = pd.get_dummies(df_train, columns=['Initial'], prefix='Initial')\ndf_test = pd.get_dummies(df_test, columns=['Initial'], prefix='Initial')","6bda27e1":"df_train.head()","873661a2":"df_train = pd.get_dummies(df_train, columns=['Embarked'], prefix='Embarked')\ndf_test = pd.get_dummies(df_test, columns=['Embarked'], prefix='Embarked')","3455ae0f":"df_train.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1, inplace=True)\ndf_test.drop(['PassengerId', 'Name',  'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1, inplace=True)","53ecf1cf":"df_train.head()","685923a0":"df_test.head()","6a9ea60c":"#importing all the required ML packages\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \nfrom sklearn.model_selection import train_test_split ","bb7ae5ac":"X_train = df_train.drop('Survived', axis=1).values\ntarget_label = df_train['Survived'].values\nX_test = df_test.values","8f1703ef":"X_tr, X_vld, y_tr, y_vld = train_test_split(X_train, target_label, test_size=0.3, random_state=2018)","4950b670":"model = RandomForestClassifier()\nmodel.fit(X_tr, y_tr)\nprediction = model.predict(X_vld)","b14064e0":"print('{:.2f}% accuracy'.format(y_vld.shape[0], 100 * metrics.accuracy_score(prediction, y_vld)))","8d905d55":"from pandas import Series\n\nfeature_importance = model.feature_importances_\nSeries_feat_imp = Series(feature_importance, index=df_test.columns)","6e429679":"plt.figure(figsize=(8, 8))\nSeries_feat_imp.sort_values(ascending=True).plot.barh()\nplt.xlabel('Feature importance')\nplt.ylabel('Feature')\nplt.show()","ffb2e07b":"submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","868ce394":"submission.head()","2450d07f":"prediction = model.predict(X_test)\nsubmission['Survived'] = prediction","d9356f75":"submission.to_csv('.\/my_first_submission.csv', index=False)","64a8d20f":"* Let's take a look at the count between the Initial and Sex we extracted using the crossstab of Pandas.","f351cac4":"* Fare has the greatest influence on the models we have obtained, followed by Initial_2, Age_cat, and Pclass.\n* In fact, feature importance represents the importance in the current model. If you use a different model, the feature importance may come out differently.\n* You can look at this feature importance to determine that Fare can actually be an important feature, but this is one conclusion that ultimately attributes to the model, so you should look at it statistically more.\n* With feature importance, you can perform feature selection to obtain a more accurate model, or remove feature for a faster model.","94974f88":"* ##### Now you want to look at the correlation between each feature. You can obtain a value between (-1, 1) by obtaining Pearson correction between two variables. Negative correlation by -1; positive correlation by 1, and 0 means no correlation. The formula you are looking for is as follows.","07d7a343":"* You can see that Embarked consists of three methods: S, C, and Q. Now, let's use the map.","98532a34":"* Now let's make a prediction for the testset and save the results as a csv file.","39f43e7a":"# 4.2 Model generation and prediction","87df9166":"* If the two methods are applied well, both should produce the same results.\n* To verify this, use the all() method after comparing Boolean between Series. The all() method gives True if all values are True, False if any are False.","691711f8":"# 3. 1 Fill Null","ec786142":"* As you can see, if you take out the Survived Feature (target class) of the train, you can see that both train and test have the same columns.","08f7f865":"\n* We applied one-hot encoding very easily.\n* One-hot encoding is also possible using Labelencoder + OneHotencoder with sklearn.\n* Sometimes there are more than 100 categories. If you use one-hot encoding, you can get 100 columns, which can be very hard to learn. In this case, a different method is used.","632f845f":"* Sklearn supports several machine learning algorithms.\n* We will use the Random Forest model in this tutorial.\n* Random Forest is a crystal tree-based model and an ensemble of different crystal trees.\n* Each machine learning algorithm has several parameters. The random forest classifier also has several parameters: n_estimators, max_features, max_depth, min_samples_split, min_samples_leaf, and so on. Depending on how these are set up, the performance of the model depends on the same dataset.\n* Parameter tuning requires time, experience, and understanding of algorithms. In the end, you have to use it a lot to build a good model.\n* Since this is a tutorial, let's set aside the parameter tuning for a while and proceed with the default setting.\n* Create a model object and train it with the fit method.\n* Then insert the valid set input to obtain the predicted value (whether or not the passenger is alive).","8ceb1603":"# 3.4 One-hot encoding on Initial and Embarked\n\n* You can put the numericalized category data as it is, but you can do one-hot encoding to increase the performance of the model.\n* Numericalization simply refers to mapping to Master == 0, Miss == 1, Mr == 2, Mrs == 3, Other == 4.\n* One-hot encoding refers to the representation of the above category as a vector of five dimensions (0, 1) as shown below.\n* You can also code the above tasks directly, but you can easily solve them using get_dummy in Pandas.\n* A total of 5 categories, and after one-hot encoding, 5 new columns are created.\n* The initial is prefixed to make it easier to distinguish.","1ba4debf":"* Usually, only train and test are mentioned, but to make a good model, we make a separate set and evaluate the model.\n* It's not like the soccer team is going to the World Cup right after the team training, but rather to go to the World Cup after the team training, checking the team's training level (learning level) after the evaluation match (valid).\n* Train_test_split makes it easy to detach train sets.","c0ff1760":"* Here we simply filled in the null, but there are examples of more diverse methods in other kernels. I will show you a more efficient way below.","a99639d0":"* Sex is also composed of Female and Male. Let's change it using map.","fae6fc32":"* The second way is to create a simple function and put it into the apply method.\n* It's much easier.","f75b40ac":"**In two different way! Second one is much simpler **","04a08f23":"* As you can see, on the right you can see the one-hot encoded columns that we were trying to create.\n* I'll apply it to Embarked as well. I will express it using one-hot encoding just like the initial.","b5b425ef":"# 4.3 Feature importance\n* The learned model has a feature importance, which we can check to see which feature the model we have created has been affected a lot.\n* Simply put, when we think of 10 = 4x1 + 2x2 + 1*x3, we can think that x1 has a big impact on the result (10). Feature importance refers to 4, 2, and 1, and since x1 has the largest value (4), it can be said that it has the greatest impact on this model.\n* The learned model basically has feature imports, so you can easily get that figure.\n* Using the pandas series, you can easily sort and draw graphs.","fdf09193":"* Let's take a look at the count between the Initial and Sex we extracted using the crossstab of Pandas.","6963dd36":"# 4.4 Prediction on Test set\n* Now, let's give the model a set of tests that the model didn't learn (didn't see) and predict its survival.\n* The results are actually submission, so you can find them on the leaderboard.\n* I will read the file given by the Cagle, the gender_submission.csv file and prepare for submission.","867fb095":"* Since Embarked has two null values and has the largest number of passengers in S, we will simply fill the null with S.\n* You can fill it easily by using the fillna method in the data frame. If you do inplace=True here, you will actually apply fillna to df_train.","fbaa054a":"* As you can see, it's true. You can choose between the two.\n* Now we will remove the duplicate Age_cat column and the original Column Age.","fdb36b6d":"### 3. 1. 1 Fill Null in Age using title\n\n* Age has 177 null data. There will be a lot of ideas to fill this up, and we'll use the title + statistics here.\n* In English, there is a title like Miss, Mr. and Mrs. Each passenger's name will have a title like this, so I'll try it.\n* In pandas series, there is a str method that changes data to string, and an extract method that allows normal expression to be applied to it. You can use this to easily extract the title. I will save the table in the initial column.","9536ed9a":"* Sklearn has machine learning from beginning to end. All tasks related to machine learning, such as feature engineering, preprocessing, supervised learning algorithms, unsupervised learning algorithms, model evaluation, and pipeline, are implemented as easy interfaces. If you want to do data analysis + machine learning, you must be familiar with this library.\n* \n* The Titanic problem is a binary classification problem because the target class (survived) is made up of 0, 1.\n* We optimize the model with the input except the survived of the train set that we have now, and we create a model to determine the survival of each sample (passenger).\n* Then, give the test set that the model did not learn as input to predict the survival of each sample (passenger) of the test set.","bfde69a0":"* Currently, the initial consists of 5 pieces, Mr. Mrs, Miss, Master, and Other. When data expressed in these categories is inputted to the model, what we need to do is digitize it so that the computer can recognize it.\n* You can do it simply with a map method.\n* I'll organize it in advance and do the mapping.","ced5b53e":"# 3. Feature Engineering","03e44c72":"# 4 Building machine learning model and prediction using the trained model\n\n* Now that we're ready, let's use sklearn to create a machine learning model.","ec374209":"# Exploratory data analysis, visualization, machine learning\n\n","0cd374b8":"* Now, we're going to fill in the null. There are so many ways to populate null data. There are ways to utilize statistics, and there are ways to create and predict and populate new machine learning algorithms based on data without null data. Here we will use how to leverage statistics.\n* where statistics means train data. We should always leave the test as unseen and fill the null data of the test based on statistics obtained from the train.","89cdb21a":"* Miss, Mr. and Mrs. related to women have a high survival rate.\n","399f5930":"# 3.3 Change Initial, Embarked and Sex (string to numerical)","16717e7e":"* With just three lines, you've built a model and even predicted it.\n* Now, let's take a look at the performance of the model.","8d44c951":"* We will fill the null value using the average of the Age.\n* When handling pandas dataframe, it is very convenient to index using Boolean array.\n* To interpret the first line of code below, replace the value of 'Age' of the row that is null() and meets the condition that the initial is Mr.\n* The method of replacing values using loc + Boolean + column is often used, so let's get used to it.","2c47b436":"* As we have seen in the EDA, we can see that Sex and Pclass are somewhat correlated with Survived.\n* You can see that there is a correlation between fair and embarked than you think.\n* And the information we can get from this is that there are no features that are strongly correlated with each other.\n* This means that when we train a model, there is no redundant (superfluous feature. If there's a feature A or B that correlates with one or one, there's actually only one piece of information we can get.\n* Now, before we actually train the model, let's do data preprocessing. We're almost there. Let's go hip!","55a30bdb":"* Age is currently continuous feature. You can build a model even if you use it as it is, but you can divide the Age into several groups and category them. Changing continuous to casual may lead to information loss, but the purpose of this tutorial is to introduce various methods, so we will proceed.\n* There are many ways. You can do it yourself using loc, the indexing method of dataframe, or you can add a function using apply.\n* The first method is using loc. Loc is used frequently, so it's good to know how to use it.\n* I will divide the age into 10 years apart.","d7fb3e16":"* We have many features, so it would be convenient to see them in a form of a maxtrix, which is called a heatmap plot, and you can draw them comfortably with the corr() method of dataframe and seaborn.","15cd795e":"# 3.1.2 Fill Null in Embarked","df9c0231":"* Let's see if Null is gone. Since importing only Embarked Columns is a Series object in one pandas, you can use the innull() method to obtain Boolean values for whether or not the values in the Series are null. And using any(), if there is a single true (if there is one null), it will return true. We got False because we changed null to S.","d345700d":"# 4.1 Preparation - Split dataset into train, valid, test set\n* First, separate the target label (Survived) from the data that will be used for the learning. You can do it simply by using drop.","2d9cd254":"* Let's start the feature engineering.\n* First of all, you want to populate the null data that exists in the dataset.\n* You can't fill it with any number, you can refer to statistics in the feature that contains null data, or you can squeeze other ideas to populate it.\n* This is something you should pay attention to because the performance of the model can depend on how you fill in the null data.\n* Feature engineering is intended to be used for the learning of real models, so you should apply the same test as well as train. Let's not forget.","ca3b23f6":"* Embarked also consists of C, Q, and S. Let's change it using map.\n* Before we do that, let's take a quick look at how to see what values are in a particular column. You can simply write the unique() method or use value_counts() to view the count.","c059fdd8":"# 3.5 Drop columns\n* It's time to clean up the desk. Let's erase all the columns we need.","e367f3d9":"We didn't tune any parameters, but we got 82% accuracy.","1c52f9a0":"# 3.2 Change Age(continuous to categorical)"}}