{"cell_type":{"3f9f8538":"code","8751cacf":"code","6595f999":"code","454a0f0f":"code","c04a6e0c":"code","c5d86947":"code","9c9e7276":"code","2ee671d6":"code","e25d7ade":"code","09792712":"code","b79b6c3f":"code","878e422b":"code","11da12cb":"code","18c71149":"code","91414ee8":"code","fcdbf1a5":"code","bd470757":"code","3199057e":"code","2f8b9b42":"code","7b84ceb9":"code","b9f73719":"code","537d78cd":"code","74751274":"code","0a3c7406":"code","0c47058c":"code","21d042ea":"code","7d1edb9b":"code","6d79cc72":"code","fc2f3f9a":"code","d86251a6":"code","d6b00425":"code","21f9166e":"markdown","03bef049":"markdown","ab6343b9":"markdown","fef9351d":"markdown","afad76c3":"markdown","005f4f8a":"markdown","afa59360":"markdown","1d5ab7dd":"markdown","330c783f":"markdown","b3fc0304":"markdown","1278673e":"markdown","ab19cc25":"markdown","0a5813d0":"markdown","ea74dce6":"markdown","6fefccec":"markdown","883300e5":"markdown","5aa06ef3":"markdown","ebab64e3":"markdown","3d138d39":"markdown","6dde33ee":"markdown"},"source":{"3f9f8538":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport warnings\nwarnings.filterwarnings('ignore')","8751cacf":"df = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","6595f999":"df.head()","454a0f0f":"df.dtypes","c04a6e0c":"df.hist(figsize=(15,15))","c5d86947":"plt.figure(figsize=(15,15))\nsns.heatmap(df.corr(),annot=True,cmap='RdYlGn')","9c9e7276":"plt.bar(df.target.unique(),df.target.value_counts(),color=['red','green'])\nplt.xticks([0,1])\nprint('No disease:{}%\\nDisease:{}%'.format(round(df.target.value_counts(normalize=True)[0],2)*100,\n                                           round(df.target.value_counts(normalize=True)[1],2)*100))","2ee671d6":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler\ncolumn_trans = make_column_transformer(\n                (OneHotEncoder(),['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']),\n                (StandardScaler(),['age', 'trestbps', 'chol', 'thalach', 'oldpeak']),\n                remainder = 'passthrough')","e25d7ade":"from sklearn.model_selection import train_test_split\nX = df.drop(['target'], axis = 1)\ny = df['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)","09792712":"column_trans.fit_transform(X_train)","b79b6c3f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nlogreg = LogisticRegression(solver='lbfgs')\npipe = make_pipeline(column_trans,logreg)","878e422b":"from sklearn.model_selection import cross_val_score\ncross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean()","11da12cb":"from sklearn.neighbors import KNeighborsClassifier\nknn_scores = []\nfor k in range(1,31):\n    knn_classifier = KNeighborsClassifier(n_neighbors = k)\n    pipe = make_pipeline(column_trans,knn_classifier)\n    knn_scores.append(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean())","18c71149":"plt.figure(figsize=(12,12))\nplt.plot([k for k in range(1, 31)], knn_scores, color = 'red')\nfor i in range(1,31):\n    plt.text(i, knn_scores[i-1], (i, round(knn_scores[i-1]*100,2)))\nplt.xticks([i for i in range(1, 31)])\nplt.xlabel('Number of Neighbors (K)')\nplt.ylabel('Scores')\nplt.title('K Neighbors Classifier scores for different K values')","91414ee8":"knn_scores[25]","fcdbf1a5":"from sklearn.svm import SVC\nsvc_scores = []\nkernels = ['linear', 'poly', 'rbf', 'sigmoid']\nfor i in range(len(kernels)):\n    svc_classifier = SVC(kernel = kernels[i])\n    pipe = make_pipeline(column_trans,svc_classifier)\n    svc_scores.append(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean())","bd470757":"from matplotlib.cm import rainbow\ncolors = rainbow(np.linspace(0, 1, len(kernels)))\nplt.figure(figsize=(10,10))\nplt.bar(kernels, svc_scores, color = colors)\nfor i in range(len(kernels)):\n    plt.text(i, svc_scores[i], svc_scores[i])\nplt.xlabel('Kernels')\nplt.ylabel('Scores')\nplt.title('Support Vector Classifier scores for different kernels')","3199057e":"svc_scores[0] #linear","2f8b9b42":"from sklearn.tree import DecisionTreeClassifier\ndt_scores = []\nfor i in range(1, len(X.columns) + 1):\n    dt_classifier = DecisionTreeClassifier(max_features = i, random_state = 0)\n    pipe = make_pipeline(column_trans,dt_classifier)\n    dt_scores.append(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean())","7b84ceb9":"plt.figure(figsize=(10,10))\nplt.plot([i for i in range(1, len(X.columns) + 1)], dt_scores, color = 'green')\nfor i in range(1, len(X.columns) + 1):\n    plt.text(i, dt_scores[i-1], (i, dt_scores[i-1]))\nplt.xticks([i for i in range(1, len(X.columns) + 1)])\nplt.xlabel('Max features')\nplt.ylabel('Scores')\nplt.title('Decision Tree Classifier scores for different number of maximum features')","b9f73719":"dt_scores[4]","537d78cd":"from sklearn.ensemble import RandomForestClassifier\nrf_scores = []\nestimators = [10, 100, 200, 500, 1000]\nfor i in estimators:\n    rf_classifier = RandomForestClassifier(n_estimators = i, random_state = 0)\n    pipe = make_pipeline(column_trans,rf_classifier)\n    rf_scores.append(cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean())","74751274":"plt.figure(figsize=(10,10))\ncolors = rainbow(np.linspace(0, 1, len(estimators)))\nplt.bar([i for i in range(len(estimators))], rf_scores, color = colors, width = 0.8)\nfor i in range(len(estimators)):\n    plt.text(i, rf_scores[i], round(rf_scores[i],5))\nplt.xticks(ticks = [i for i in range(len(estimators))], labels = [str(estimator) for estimator in estimators])\nplt.xlabel('Number of estimators')\nplt.ylabel('Scores')\nplt.title('Random Forest Classifier scores for different number of estimators')","0a3c7406":"rf_scores[1]","0c47058c":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\npipe = make_pipeline(column_trans,nb)\ncross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy').mean()","21d042ea":"pipe = make_pipeline(column_trans,logreg)\npipe.fit(X_train, y_train)","7d1edb9b":"y_pred = pipe.predict(X_test)","6d79cc72":"from sklearn import metrics\nmetrics.accuracy_score(y_test,y_pred)*100","fc2f3f9a":"svc_classifier = SVC(kernel = 'linear')\npipe = make_pipeline(column_trans,svc_classifier)\npipe.fit(X_train, y_train)","d86251a6":"y_pred = pipe.predict(X_test)","d6b00425":"from sklearn import metrics\nmetrics.accuracy_score(y_test,y_pred)*100","21f9166e":"## Train Test Split","03bef049":"## Make predictions on \"unseen\" data","ab6343b9":"### 3. Support Vector Classifier (SVC)","fef9351d":"There are few features that have negative correlation with target and few with positive correlation","afad76c3":"### 6. Gaussian NB","005f4f8a":"Suggestions are Welcome. Thank you!","afa59360":"## Classification Algorithms","1d5ab7dd":"### 2. SVC with linear kernel","330c783f":"### 2. K Nearest Neighbors","b3fc0304":"### 4. Decision Tree Classifier","1278673e":"The two classes are not exactly 50% each but the ratio is good enough to continue without dropping\/increasing our data.","ab19cc25":"It is worth to test our model on 'testing dataset' using two classification models that are having high accuracy score than other classification models. They are (i) Logistic Regression and (ii) SVC using linear kernel.","0a5813d0":"### 1. Logistic Regression","ea74dce6":"### 5. Random Forest Classifier","6fefccec":"### 1. Logistic Regression","883300e5":"We can see that the dataset contains few categorical features and few continuous features","5aa06ef3":"## Getting some insight about our data","ebab64e3":"## Data Preprocessing","3d138d39":"Applying OneHotEncoder on the categorical features and StandardScalar on continuous features","6dde33ee":"There are several kernels for Support Vector Classifier. I'll test some of them and check which has the best score."}}