{"cell_type":{"9294e296":"code","f756b568":"code","548a8d92":"code","69616419":"code","081e385f":"code","4f2eae0f":"code","09bd349f":"code","e86efa86":"code","4f1ac383":"code","1ec0a704":"code","73c07624":"code","30ca70de":"code","fbd34aed":"code","4a9c2360":"code","5e067120":"code","2658d750":"code","9e02dbc0":"code","2c6b7ff1":"code","a06a84ec":"code","9e5c9b41":"code","93e7ed37":"code","3f7fe6c4":"code","794bf270":"code","e9cae1e9":"code","1680b82c":"code","b7b243b4":"code","1ed2c9f5":"code","82a709cb":"code","214fcc4a":"code","6fdb0466":"code","386c7866":"code","397680dc":"code","71ac1463":"code","127db008":"code","3940bb0c":"code","374df5ae":"code","64b40b5a":"code","6fddcd7c":"code","d15a8d2f":"code","c40d804b":"code","472c1fb4":"markdown","22563759":"markdown","e02b67b6":"markdown","95566d27":"markdown","5c406377":"markdown","babade55":"markdown","a50e39eb":"markdown","a9e00a9d":"markdown","cd4143ee":"markdown","bd35c550":"markdown","9e136083":"markdown","a1db5266":"markdown","81efcb9b":"markdown","7b6f45da":"markdown","30247577":"markdown","c55737ee":"markdown","902df314":"markdown","53827a67":"markdown","16cd985e":"markdown","459cac8d":"markdown","5550354f":"markdown","5fb6669e":"markdown"},"source":{"9294e296":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f756b568":"import seaborn as sns\nimport matplotlib.pyplot as plt","548a8d92":"df = pd.read_csv('https:\/\/cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud\/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork\/labs\/Data%20files\/automobileEDA.csv')\ndf.head()","69616419":"df.info()","081e385f":"df.isnull().sum()\n#There are few missing values in the data set ","4f2eae0f":"df.dropna(inplace=True) # we just drop these rows with missing values\ndf.isnull().sum()\n#Now there is no  missing values","09bd349f":"df.corr()[\"price\"].sort_values(ascending=False)\n# here we can see the correlations between the target and the other features","e86efa86":"plt.figure(figsize=(15,10))\nsns.heatmap(df.corr(), annot=True, linewidths=0.1, cmap=\"viridis\")\n#Here we visualize all of the correlations between the columns","4f1ac383":"plt.figure(figsize=(15,10))\nsns.set_style(\"whitegrid\")\nsns.jointplot(x=\"engine-size\", y=\"price\", data=df)\n#Here is the plot between the target variable and the feature with the highest correlation value","1ec0a704":"# here we will make a simple linear regression between highway-mpg and price\nx=df[['highway-mpg']]\ny=df[\"price\"]","73c07624":"sns.jointplot(x,y)\n# we see that there is a neagtive correlation between the two","30ca70de":"df.corr()[\"price\"][\"highway-mpg\"]\n# the correlation r is -0.7","fbd34aed":"from sklearn.linear_model import LinearRegression","4a9c2360":"model=LinearRegression()","5e067120":"from sklearn.model_selection import train_test_split","2658d750":"X_train, X_test, y_train, y_test=train_test_split(x,y, test_size=0.3)","9e02dbc0":"model.fit(X_train,y_train)\n#Here we fit our modl into the training set","2c6b7ff1":"predictions=model.predict(X_test)","a06a84ec":"model.intercept_\n#This is the intercept of the function for predictions","9e5c9b41":"model.coef_","93e7ed37":"model.score(x_train,y_train)","3f7fe6c4":"plt.figure(figsize=(15,10))\nsns.regplot(x=predictions, y=y_test, data=df)","794bf270":"X = df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']]\ny=df[\"price\"]","e9cae1e9":"model2=LinearRegression()","1680b82c":"X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.3)","b7b243b4":"model2.fit(X_train, y_train)","1ed2c9f5":"model.intercept_\n#The intecept of our model","82a709cb":"model2.coef_\n#These are coeeficients of our model with respecto four feature we used ","214fcc4a":"predictions2= model2.predict(X_test)","6fdb0466":"plt.figure(figsize=(15,10))\nsns.regplot(x=predictions, y=y_test, data=df,label=\"Simple Linear Regression\")\nsns.regplot(x=predictions2, y=y_test, data=df, label=\"Multiple Linear Regression\")\n# It is obvius that multiple regression predicts better than simple linear regression\n","386c7866":"plt.figure(figsize=(15,10))\nsns.residplot(x=predictions, y=y_test, data=df,label=\"Simple Linear Regression\")","397680dc":"plt.figure(figsize=(15,10))\nsns.residplot(x=predictions, y=y_test, data=df,label=\"Simple Linear Regression\", color=\"green\")","71ac1463":"plt.figure(figsize=(15,10))\nsns.distplot(df['price'], hist=False, color=\"r\", label=\"Actual Value\")\nsns.distplot(predictions, hist=False, color=\"b\", label=\"Fitted Values\")\n\n\nplt.title('Actual vs Fitted Values for Price in Simple Linear Regression')\n","127db008":"plt.figure(figsize=(15,10))\nsns.distplot(df['price'], hist=False, color=\"r\", label=\"Actual Value\")\nsns.distplot(predictions2, hist=False, color=\"b\", label=\"Fitted Values\" )\n\n\nplt.title('Actual vs Fitted Values for Price in Multiple Linear Regression')","3940bb0c":"model.score(X,y)","374df5ae":"model2.score(X,y)","64b40b5a":"# I will evaluate our models' performance by calculating the residual sum of squares and the explained variance score\nfrom sklearn import metrics\nprint(\"MAE:\",metrics.mean_absolute_error(y_test,predictions))\nprint (\"MSE:\",metrics.mean_squared_error(y_test,predictions))\nprint(\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test,predictions)))","6fddcd7c":"print(\"MAE:\",metrics.mean_absolute_error(y_test,predictions2))\nprint (\"MSE:\",metrics.mean_squared_error(y_test,predictions2))\nprint(\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test,predictions2)))","d15a8d2f":"metrics.explained_variance_score(y_test,predictions)","c40d804b":"metrics.explained_variance_score(y_test,predictions2)","472c1fb4":"<font color='blue'>\nIn th ethe two latets distribution plot, we can easily see that multiple linear regression performs better than simple linear regression. According to the distribution of the simple linear regression, the model can not explain the data after 30 000, but the multiple linear regression can explain more prices to 45 000 dolar and predicts very close to the actual values.","22563759":"# 4. Model Evaluation:","e02b67b6":"<font color='blue'>\n<li>Simple Linear Regression is an algorithm to show the relationship between two variables  and to predict the dependent(target) variable as a function of the feature (independent) variable. <\/li>\n\n\n<b>Simple Linear Regression Function:<\/b>\n$$\nYhat = a + b  X\n$$\n\n<ul>\n    <li>a represents the <b>intercept<\/b> of the regression line <\/li>\n    <li>b represents the <b>slope<\/b> of the regression line, which means how much Y changes when X increases by 1 unit<\/li>\n<\/ul>\n","95566d27":"<font color='blue'>\nExplained variance score also higher in in multiple linear regression than simple linear regression","5c406377":"<font color='blue'>\n**In this dataset we will predict the prices of the cars by using a supervised machine learning algorithms with respect to given features. **\n\n","babade55":"4.1. Visual Evaluation","a50e39eb":"<font color='red'>\nOur predicition simple linear regression function \n$$\nYhat = a + b  X\n$$\n\n$$\nYhat = 38423.31 - 821.73 X \n$$\n","a9e00a9d":"4.2. Quantitative Evaluation","cd4143ee":"<font color='blue'>\n<p> We think  that the more variables you have, the better your model is at predicting, but this is not always true. Sometimes we may not have enough data, we may run into numerical problems, or many of the variables may not be useful and or even act as noise. As a result, we should always check the MSE and R^2.<\/p>\n\n<p>So to be able to compare the results of the MLR vs SLR models, we look at a combination of both the R-squared and MSE to make the best conclusion about the fit of the model.\n<ul>","bd35c550":"<font color='blue'>\nResidual plot, returns the difference between the observed value (y) and the predicted value (Yhat) is called the residual (e). When we look at a regression plot, the residual is the distance from the data point to the fitted regression line.","9e136083":" # 3. Multiple Linear Regression:","a1db5266":"<font color='blue'>\nWe can say that ~%81.2 of the variation of the price is explained by this simple linear model and ~%81.6 of the variation of the price is explained by multiple linear regression which we used just 4 features. The variance will increase when we take into account other features.","81efcb9b":"# Decision Making and Determining a Good Model Fit","7b6f45da":"# 2. Simple Linear Regression:","30247577":" <font color='blue'>\n we have evaluated the different models, and generated the R-squared and MSE values for the fits, how do we determine a good model fit?\n<ul>\n    <li><i>What is a good R-squared value?<\/i><\/li>\n<\/ul>\n<\/p>\n\n<p>When comparing models, <b>the model with the higher R-squared value is a better fit<\/b> for the data.Multiple Linear regression outperformed the Simple Linear Regression.\n<ul>\n    <li><i>What is a good MSE?<\/i><\/li>\n<\/ul>\n<\/p>\n\n<p>When comparing models, <b>the model with the smallest MSE value is a better fit<\/b> for the data.Multiple Linear regression lower error scores than the Simple Linear Regression.<\/p>\n","c55737ee":"<font color='blue'>\nWe can easily see that errors are lower in multiple linear regression than simple linear regression","902df314":"<font color='blue'>\n**Simple linear regression use a single predictor variable X to model the response variable Y. In many applications, there\nis more than one factor that influences the response. Multiple regression models thus describe how a single response variable Y depends linearly on a number of predictor variables.**","53827a67":"<font color='blue'>\nIn order to evaluate quantitatively, there are two very important measures that are often used in Statistics to determine the accuracy of a model are:\n\nR^2 \/ R-squared\nMean Squared Error (MSE)\nR-squared\n\nR squared, also known as the coefficient of determination, is a measure to indicate how close the data is to the fitted regression line.\n\nThe value of the R-squared is the percentage of variation of the response variable (y) that is explained by a linear model.\n\nMean Squared Error (MSE)\n\nThe Mean Squared Error measures the average of the squares of errors, that is, the difference between actual value (y) and the estimated value (\u0177).","16cd985e":"<font color='blue'>\nThe points in a residual plot are <b>randomly spread out around the x-axis<\/b> in both simple linear and multiple linear regression, so a <b>linear model is appropriate<\/b> for the data because randomly spread out residuals means that the variance is constant, and thus the linear model is a good fit for this data.","459cac8d":"# 1. Exploratory Data Analysis:","5550354f":"<font color='red'>\n$$\nYhat = a + b_1 X_1 + b_2 X_2 + b_3 X_3 + b_4 X_4\n$$\n\n\n$$\nPrice = -16367.19 + 28.30211432 x Horsepower + 3.33859201 x Curb-weight + 131.57409552 x Engine-size + 49.82157175 x Highway-mpg\n$$","5fb6669e":"<font color='red'>\nFormula and Calcualtion of Multiple Linear Regression\n\\begin{aligned} &y_i = \\beta_0 + \\beta _1 x_{i1} + \\beta _2 x_{i2} + ... + \\beta _p x_{ip} + \\epsilon\\\\ &\\textbf{where, for } i = n \\textbf{ observations:}\\\\ &y_i=\\text{dependent variable}\\\\ &x_i=\\text{expanatory variables}\\\\ &\\beta_0=\\text{y-intercept (constant term)}\\\\ &\\beta_p=\\text{slope coefficients for each explanatory variable}\\\\ &\\epsilon=\\text{the model's error term (also known as the residuals)}\\\\ \\end{aligned} \n\u200b\t  \ny \ni\n\u200b\t =\u03b2 \n0\n\u200b\t +\u03b2 \n1\n\u200b\t x \ni1\n\u200b\t +\u03b2 \n2\n\u200b\t x \ni2\n\u200b\t +...+\u03b2 \np\n\u200b\t x \nip\n\u200b\t +\u03f5\nwhere, for i=n observations:\ny \ni\n\u200b\t =dependent variable\nx \ni\n\u200b\t =expanatory variables\n\u03b2 \n0\n\u200b\t =y-intercept (constant term)\n\u03b2 \np\n\u200b\t =slope coefficients for each explanatory variable\n\u03f5=the model\u2019s error term (also known as the residuals)\n\u200b\t"}}