{"cell_type":{"e5817dbb":"code","44d99078":"code","674b89cd":"code","5e467d14":"code","a076a297":"code","3bb8a87a":"code","22005e30":"code","34678eb8":"code","25b23c9e":"code","2b785273":"code","1f73566d":"code","cd97a23a":"code","3468dede":"code","0fe069a5":"code","e00bf2e0":"code","1314f488":"code","c72b1e7a":"markdown","2c06ba31":"markdown","07e7dfb4":"markdown","11107280":"markdown","f61e7df8":"markdown","387070f4":"markdown","6217823f":"markdown","b8b69462":"markdown","85cb96fc":"markdown","abeafcb6":"markdown","7bcc96cf":"markdown"},"source":{"e5817dbb":"import json\nimport random\nimport numpy as np\nimport pandas as pd\n\nall_lines = []\nfor line in open(\"..\/input\/gutenberg-poetry-v001.ndjson\"):\n    all_lines.append(json.loads(line.strip()))","44d99078":"corpus = \"\\n\".join([line['s'] for line in random.sample(all_lines, 1000)])","674b89cd":"import markovify","5e467d14":"model = markovify.NewlineText(corpus)","a076a297":"for i in range(5):\n    print()\n    for i in range(random.randrange(1, 4)):\n        print(model.make_short_sentence(30))","3bb8a87a":"from keras.layers import LSTM, Dense, Dropout, Flatten\nfrom keras.callbacks import LambdaCallback\nfrom keras.models import Sequential\nfrom keras.optimizers import RMSprop\nfrom keras.utils import np_utils","22005e30":"# Lowercase all text\ntext = corpus.lower()\n\nchars = list(set(text))\nchar_indices = dict((c, i) for i, c in enumerate(chars))\nindices_char = dict((i, c) for i, c in enumerate(chars))\n\nvocab_size = len(chars)\nprint('Vocabulary size: {}'.format(vocab_size))","34678eb8":"# Data preparation\nX = [] # training array\nY = [] # target array\n\nlength = len(text)\nseq_length = 100 # number of characters to consider before predicting a character\n\n# Iterate over length of text and create sequences stored in X, true values stored in Y\n# true values being which character would actually come after sequence stored in X\nfor i in range(0, length - seq_length, 1):\n    sequence = text[i:i + seq_length]\n    label = text[i + seq_length]\n    X.append([char_indices[char] for char in sequence])\n    Y.append(char_indices[label])\n\nprint('Number of sequences: {}'.format(len(X)))","25b23c9e":"# Reshape dimensions\nX_new = np.reshape(X, (len(X), seq_length, 1))\n# Scale values\nX_new = X_new\/float(len(chars))\n# One-hot encode Y to remove ordinal relationships\nY_new = np_utils.to_categorical(Y)\n\nX_new.shape, Y_new.shape","2b785273":"model = Sequential()\n# Add LSTM layer to compute output using 150 LSTM units\nmodel.add(LSTM(150, input_shape = (X_new.shape[1], X_new.shape[2]), return_sequences = True))\n\n# Add regularization layer to prevent overfitting.\n# Dropout ignores randomly selected neurons during training (\"dropped out\").\n# Ultimately, network becomes less sensitive to specific weights of neurons --> network is better at generalization.\nmodel.add(Dropout(0.1))\n\nmodel.add(Flatten())\n# Dense layer with softmax activation function to approximate probability distribution of next best word\nmodel.add(Dense(Y_new.shape[1], activation = 'softmax'))\n\n# Compile model to configure learning process\n# Categorical crossentropy: an example can only belong to one class\n# Adam optimization algorithm updates a learning rate for each network weight iteratively as learning unfolds\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n\n# Use 1 epoch for sake of computational time\nmodel.fit(X_new, Y_new, epochs = 1, verbose = 1)","1f73566d":"# Random start\nstart = np.random.randint(0, len(X)-1)\nstring_mapped = list(X[start])\nfull_string = [indices_char[value] for value in string_mapped]\n\n# Generate text\nfor i in range(400):\n    x = np.reshape(string_mapped, (1, len(string_mapped), 1))\n    x = x \/ float(len(chars))\n    \n    pred_index = np.argmax(model.predict(x, verbose = 0))\n    seq = [indices_char[value] for value in string_mapped]\n    full_string.append(indices_char[pred_index])\n    \n    string_mapped.append(pred_index)\n    string_mapped = string_mapped[1:len(string_mapped)]\n    \n# Combine text\nnewtext = ''\nfor char in full_string:\n    newtext = newtext + char\n\nprint(newtext)","cd97a23a":"import keras.utils as ku\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding\n\n# Lowercase all text\ntext = corpus.lower()\ntext = text.split('\\n')\n\n# Create Tokenizer object to convert words to sequences of integers\ntokenizer = Tokenizer(num_words = None, filters = '#$%&()*+-<=>@[\\\\]^_`{|}~\\t\\n', lower = False)\n\n# Train tokenizer to the texts\ntokenizer.fit_on_texts(text)\ntotal_words = len(tokenizer.word_index) + 1\n\n# Convert list of strings into flat dataset of sequences of tokens\nsequences = []\nfor line in text:\n    token_list = tokenizer.texts_to_sequences([line])[0]\n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n        sequences.append(n_gram_sequence)\n\n# Pad sequences to ensure equal lengths\nmax_seq_len = max([len(x) for x in sequences])\nsequences = np.array(pad_sequences(sequences, maxlen = max_seq_len, padding = 'pre'))\n\n# Create n-grams sequence predictors and labels\npredictors, label = sequences[:, :-1], sequences[:, -1]\nlabel = ku.to_categorical(label, num_classes = total_words)","3468dede":"# Input layer takes sequence of words as input\ninput_len = max_seq_len - 1\nmodel = Sequential()\nmodel.add(Embedding(total_words, 10, input_length = input_len))\nmodel.add(LSTM(150))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(total_words, activation = 'softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n\n# Use 100 epoch for efficacy\nmodel.fit(predictors, label, epochs = 100, verbose = 1)","0fe069a5":"# Function to generate line\ndef generate_line(text, next_words, max_seq_len, model):\n    for j in range(next_words):\n        token_list = tokenizer.texts_to_sequences([text])[0]\n        token_list = pad_sequences([token_list], maxlen = max_seq_len - 1, padding = 'pre')\n        predicted = model.predict_classes(token_list, verbose = 0)\n        \n        output_word = ''\n        for word, index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        text += ' ' + output_word\n    return text","e00bf2e0":"generate_line(\"gone were the\", 5, max_seq_len, model)","1314f488":"generate_line(\"oh what sweet\", 5, max_seq_len, model)","c72b1e7a":"#### Prepare Data","2c06ba31":"##### Thank you!","07e7dfb4":"#### Prepare Text","11107280":"## Prepare Data","f61e7df8":"#### Create Model","387070f4":"## Aside: Markov chain text generation\n\nMarkov chains generate sentences based on recombination of elements of known sentences: it analyzes the words and the probability of occurence of two consecutive words. The generation is randomized and based on the probability of each words.\n\nLet's generate a poem with this method, and then continue with our RNN method to see how they compare.","6217823f":"## Predicting Next Words","b8b69462":"#### Create Model","85cb96fc":"## Character-Level LSTM Text Generation\n\n#### Import Libraries","abeafcb6":"# Poetry with RNN\n\nToday, we will generate poems. I love reading and writing poetry, so I thought it would be fun to see what a machine can come up with rather than my own mind. I'm using the [Gutenberg Poetry Corpus by Allison Parris](https:\/\/github.com\/aparrish\/gutenberg-poetry-corpus).\n\nThis is mostly a learning exercise and practice document for me. Hope you enjoy the read.\n\n#### Recurrent Neural Network (RNN)\nA recurrent neural network is a network with loops in it, which allows previous outputs to be used as inputs to the current step while having a hidden state. The hidden state remembers some information about a sequence. It can be understood as multiple copies of the same network that each pass a message to the next network.\n\nFor more details, [visit this Medium article by Suvro Banerjee](https:\/\/medium.com\/explore-artificial-intelligence\/an-introduction-to-recurrent-neural-networks-72c97bf0912).\n\n#### Long Short-Term Memory (LSTM) Networks\nRNNs fail to understand the context behind input because they only remember things for short durations of time. Therefore once a lot of words are fed in, information gets lost and predictions are less reliable. This is because of the Vanishing Gradient problem: as more layers using the sigmoid activation function (or others) are added to neural networks, the gradients of the loss function approach 0. It becomes difficult to train layers. Read more about it [on Towards Data Science](https:\/\/towardsdatascience.com\/the-vanishing-gradient-problem-69bf08b15484).\n\nWe can use LSTM Networks, a version of a RNN, to fix this issue. With LSTMs, information flows through cell states, allowing for slight modification of information -- LSTMs can therefore selectively remember or forget things.","7bcc96cf":"#### Generate Text"}}