{"cell_type":{"0edad084":"code","425f0ea3":"code","d8558db3":"code","544ba28a":"code","5238d9c0":"code","57d6ee8a":"code","99906ca4":"code","f886fb9a":"code","8b4f4510":"code","196d828a":"code","4fd6ab9b":"code","0e7ca6ab":"code","c72dac3e":"code","09aa2e18":"code","d14a5b32":"code","6646ff83":"code","88bdfcd7":"code","93efbd15":"code","969abda9":"code","f55fa7ee":"code","c7b9fc81":"code","05057668":"code","d197db7e":"markdown"},"source":{"0edad084":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","425f0ea3":"import numpy as np # linear algebra\nimport matplotlib.pylab as plt\n\nimport pandas as pd # data processing\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dropout\nimport tensorflow as tf \nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom keras.layers import BatchNormalization, Dropout\nfrom keras.layers import Dense, Activation\nfrom tensorflow import keras\n\n\nfrom sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import GaussianNB\n\n\nplt.rcParams[\"figure.figsize\"] = (16,10) #Make the plots bigger by default\nplt.rcParams[\"lines.linewidth\"] = 2 #Setting the default line width\nplt.style.use(\"ggplot\")","d8558db3":"df0 = pd.read_csv(\"..\/input\/emg-4\/0.csv\", header=None )\ndf1 = pd.read_csv(\"..\/input\/emg-4\/1.csv\", header=None )\ndf2 = pd.read_csv(\"..\/input\/emg-4\/2.csv\", header=None )\ndf3 = pd.read_csv(\"..\/input\/emg-4\/3.csv\", header=None )\ndf = pd.concat([df0,df1,df2,df3], axis = 0)\ndf.head()","544ba28a":"x = df.loc[:,0:63]\ny = df[64]","5238d9c0":"y.unique()","57d6ee8a":"x = np.array(x)\ny= np.array(y)","99906ca4":"x = x.reshape(x.shape[0]*x.shape[1], 1)\n","f886fb9a":"sc = StandardScaler()\nx = sc.fit_transform(x)\n","8b4f4510":"x = x.reshape((-1, 8, 8))\n","196d828a":"print(\"All Data size X and y\")\nprint(x.shape)\nprint(y.shape)","4fd6ab9b":"y = np.eye(np.max(y) + 1)[y]\n","0e7ca6ab":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42,stratify=y)","c72dac3e":"print(\"All Data size X and y\")\nprint(x_train.shape)\nprint(y_train.shape)","09aa2e18":"print(\"Test Data size X and y\")\nprint(x_test.shape)\nprint(y_test.shape)","d14a5b32":"model = Sequential()\n\nmodel.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1], 8)))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units = 50))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units = 64))\nmodel.add(Dense(units = 128))\n\nmodel.add(Dense(units = 4, activation=\"softmax\"))\nmodel.compile(optimizer = \"adam\" , loss = \"binary_crossentropy\", metrics=[\"accuracy\"])\n\n\nmodel.summary()\n","6646ff83":"callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n","88bdfcd7":"history=model.fit(x_train, y_train, epochs = 250, batch_size = 32, verbose=2 , callbacks=[callback],validation_split=0.2,)\n","93efbd15":"plt.figure(0)\n\nplt.plot(history.history['accuracy'], label='training accuracy')\nplt.plot(history.history['val_accuracy'], label='val accuracy')\nplt.title('Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.figure(1)\nplt.plot(history.history['loss'], label='training loss')\nplt.plot(history.history['val_loss'], label='val loss')\nplt.title('Loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()","969abda9":"# confusion matrix\nimport seaborn as sns\n# Predict the values from the validation dataset\nY_pred = model.predict(x_test)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_test,axis = 1) ","f55fa7ee":"confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix","c7b9fc81":"f,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","05057668":"from sklearn.metrics import classification_report\n\nprint(classification_report(Y_true, Y_pred_classes))","d197db7e":"# rock = 0,\n# scissors = 1\n# paper = 2,\n# ok =3. "}}