{"cell_type":{"fa7b61a6":"code","dc0a0a5e":"code","c6336293":"code","15e432dd":"code","766d4258":"code","acef5c05":"code","ad5d9624":"code","ecbe075a":"code","0d00bed6":"code","0cba17a8":"markdown"},"source":{"fa7b61a6":"import numpy as np\nimport pandas as pd\nimport re\nimport os\nimport string\nfrom matplotlib import pyplot as plt\nfrom tqdm.auto import tqdm\nfrom transformers import ElectraTokenizer, ElectraForSequenceClassification,AdamW,get_linear_schedule_with_warmup\nimport torch\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom sklearn.model_selection import StratifiedKFold","dc0a0a5e":"class CFG:\n    ROOT_DIR = '..\/input\/nlp-getting-started'\n    BATCH_SIZE = 32\n    ELECTRA_MODEL = 'google\/electra-base-discriminator'\n    EPOCHS = 30\n    DEVICE = 'cuda'","c6336293":"def preprocess(text):\n    text=text.lower()\n    # remove hyperlinks\n    text = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', text)\n    text = re.sub(r'http?:\\\/\\\/.*[\\r\\n]*', '', text)\n    #Replace &amp, &lt, &gt with &,<,> respectively\n    text=text.replace(r'&amp;?',r'and')\n    text=text.replace(r'&lt;',r'<')\n    text=text.replace(r'&gt;',r'>') \n    #remove mentions\n    text = re.sub(r\"(?:\\@)\\w+\", '', text)\n    #remove non ascii chars\n    text=text.encode(\"ascii\",errors=\"ignore\").decode()\n    #remove some puncts (except . ! ?)\n    text=re.sub(r'[:\"#$%&\\*+,-\/:;<=>@\\\\^_`{|}~]+','',text)\n    text=re.sub(r'[!]+','!',text)\n    text=re.sub(r'[?]+','?',text)\n    text=re.sub(r'[.]+','.',text)\n    text=re.sub(r\"'\",\"\",text)\n    text=re.sub(r\"\\(\",\"\",text)\n    text=re.sub(r\"\\)\",\"\",text)\n\n    text=\" \".join(text.split())\n    return text","15e432dd":"class LitDataNLP(pl.LightningDataModule):\n    def __init__(self, fold, tokenizer, data_dir:str = '.\/', batch_size: int = 32):\n        super().__init__()\n        self.fold = fold\n        self.tokenizer = tokenizer\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        \n    def setup(self, stage=None):\n        train_df = pd.read_csv(os.path.join(self.data_dir, 'train.csv'))\n        #train_df['text'] = train_df['text'].apply(preprocess)\n        train_df = train_df[train_df[\"text\"]!='']\n        train_df = train_df[[\"text\",\"target\"]]\n        texts = train_df.text.values\n        labels = train_df.target.values\n        indices=self.tokenizer.batch_encode_plus(texts,max_length=64,add_special_tokens=True, \n                                            return_attention_mask=True,pad_to_max_length=True,\n                                            truncation=True)\n\n        input_ids=np.array(indices[\"input_ids\"])\n        attention_masks=np.array(indices[\"attention_mask\"])\n        skf = StratifiedKFold(5, shuffle=True, random_state=42)\n        #train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n        #                                                    random_state=42, test_size=0.2)\n        #train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n        #                                             random_state=42, test_size=0.2)\n        for fold, (tr_idx, val_idx) in enumerate(skf.split(input_ids, labels)):\n            train_inputs = input_ids[tr_idx]\n            train_labels = labels[tr_idx]\n            validation_inputs = input_ids[val_idx]\n            validation_labels = labels[val_idx]\n            if fold == self.fold:\n                break\n                \n        for fold, (tr_idx, val_idx) in enumerate(skf.split(attention_masks, labels)):\n            train_masks = attention_masks[tr_idx]\n            validation_masks = attention_masks[val_idx]\n            if fold == self.fold:\n                break\n                \n        self.train_inputs = torch.tensor(train_inputs)\n        self.validation_inputs = torch.tensor(validation_inputs)\n        self.train_labels = torch.tensor(train_labels, dtype=torch.long)\n        self.validation_labels = torch.tensor(validation_labels, dtype=torch.long)\n        self.train_masks = torch.tensor(train_masks, dtype=torch.long)\n        self.validation_masks = torch.tensor(validation_masks, dtype=torch.long)\n        \n    def train_dataloader(self):\n        train_data = TensorDataset(self.train_inputs, self.train_masks, self.train_labels)\n        train_sampler = RandomSampler(train_data)\n        return DataLoader(train_data, sampler=train_sampler, batch_size=self.batch_size)\n    \n    def val_dataloader(self):\n        validation_data = TensorDataset(self.validation_inputs, self.validation_masks, self.validation_labels)\n        validation_sampler = SequentialSampler(validation_data)\n        return DataLoader(validation_data, sampler=validation_sampler, batch_size=self.batch_size)","766d4258":"class LitNLPModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = ElectraForSequenceClassification.from_pretrained(CFG.ELECTRA_MODEL, num_labels=2)\n        self.f1_score = pl.metrics.F1(num_classes=2)\n        \n    def forward(self, b_input_ids, b_input_mask, b_labels):\n        output = self.model(b_input_ids, \n                        token_type_ids=None, \n                        attention_mask=b_input_mask,\n                        labels=b_labels)\n        return output\n    \n    def training_step(self, batch, batch_idx):\n        b_input_ids = batch[0]\n        b_input_mask = batch[1]\n        b_labels = batch[2]\n        z = self(b_input_ids, b_input_mask, b_labels)\n        loss = z[0]\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        b_input_ids = batch[0]\n        b_input_mask = batch[1]\n        b_labels = batch[2]\n        z = self(b_input_ids, b_input_mask, b_labels)\n        val_loss = z[0]\n        logits = z[1]\n        #logits = logits.detach().cpu().numpy()\n        #label_ids = b_labels.to('cpu').numpy()\n        self.log('val_loss', val_loss, prog_bar=True)\n        self.log('val_f1_score', self.f1_score(logits, b_labels), prog_bar=True)\n        return val_loss\n    \n    def configure_optimizers(self):\n        optimizer = AdamW(model.parameters(), lr=6e-6)\n        scheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=0, \n                                            num_training_steps=189*CFG.EPOCHS)\n        return [optimizer], [scheduler]\n    \n    def flat_accuracy(self, preds, labels):\n        pred_flat = np.argmax(preds, axis=1).flatten()\n        labels_flat = labels.flatten()\n        return np.sum(pred_flat == labels_flat) \/ len(labels_flat)","acef5c05":"tokenizer = ElectraTokenizer.from_pretrained(CFG.ELECTRA_MODEL)\nfor fold in range(5):\n    dm = LitDataNLP(fold=fold, tokenizer=tokenizer, data_dir=CFG.ROOT_DIR, batch_size=CFG.BATCH_SIZE)\n    chk_callback = ModelCheckpoint(\n        monitor='val_f1_score',\n        filename='model_best',\n        save_top_k=1,\n        mode='max',\n    )\n    es_callback = EarlyStopping(\n       monitor='val_f1_score',\n       min_delta=0.001,\n       patience=5,\n       verbose=False,\n       mode='max'\n    )\n    model = LitNLPModel()\n\n    trainer = pl.Trainer(\n        gpus=1,\n        max_epochs=CFG.EPOCHS,\n        callbacks=[chk_callback, es_callback]\n    )\n\n    trainer.fit(model, dm)","ad5d9624":"def run_inference(data_dir, model, device, batch_size:int = 32):\n    test_df = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n    #test_df['text'] = test_df['text'].apply(preprocess)\n    comments = test_df.text.values\n\n    indices = tokenizer.batch_encode_plus(comments, max_length=128, add_special_tokens=True, \n                                           return_attention_mask=True, pad_to_max_length=True,\n                                           truncation=True)\n    input_ids = indices[\"input_ids\"]\n    attention_masks = indices[\"attention_mask\"]\n\n    test_inputs = torch.tensor(input_ids)\n    test_masks = torch.tensor(attention_masks)\n\n    # Create the DataLoader.\n    test_data = TensorDataset(test_inputs, test_masks)\n    test_sampler = SequentialSampler(test_data)\n    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n\n    print('Predicting labels...')\n    \n    preds = []\n    for fold in range(5):\n        model.load_state_dict(torch.load(f'.\/lightning_logs\/version_{fold}\/checkpoints\/model_best.ckpt')['state_dict'])\n        model.eval()\n        model.to(device)\n\n        # Tracking variables \n        predictions = []\n\n        # Predict \n        for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n            batch = tuple(t.to(device) for t in batch)\n            b_input_ids, b_input_mask = batch\n\n            with torch.no_grad():\n                outputs = model(b_input_ids, b_input_mask, None)\n\n            logits = outputs[0]\n\n            logits = logits.detach().cpu().numpy()\n\n            # Store predictions and true labels\n            predictions.append(logits)\n\n        flat_predictions = [item for sublist in predictions for item in sublist]\n        flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n        preds.append(flat_predictions)\n    return np.round(np.mean(preds, axis=0), 0)","ecbe075a":"preds = run_inference(CFG.ROOT_DIR, model, CFG.DEVICE, batch_size=CFG.BATCH_SIZE)","0d00bed6":"sample_sub = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nsub = pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':preds.astype(int)})\nsub.to_csv('submission.csv',index=False)\nsub","0cba17a8":"## Submission"}}