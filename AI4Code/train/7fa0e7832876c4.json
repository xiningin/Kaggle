{"cell_type":{"2f22496e":"code","0521e8e2":"code","5a2a84a1":"code","02aba096":"code","0cc721a1":"code","62d01d73":"code","71f3e943":"code","e6c7ec0d":"code","40b3e916":"code","269bbd81":"code","9003de26":"code","7eb678f6":"code","ffb7da18":"code","45a6a9ee":"code","3e66fe30":"code","e24b25e7":"markdown","571e6ea2":"markdown","5324480f":"markdown","272a5b50":"markdown","dd85570a":"markdown","0f3ba59a":"markdown"},"source":{"2f22496e":"import numpy as np\nimport pandas as pd\n\nimport torch\nfrom torch import nn\n\nimport matplotlib.pyplot as plt\n\nfrom fastai.dataset import ImageClassifierData\nfrom sklearn.model_selection import train_test_split","0521e8e2":"# carregando os training set\ntrain = pd.read_csv('..\/input\/train.csv')","5a2a84a1":"# separando a label das features\nX, y = train.iloc[:,1:], train.iloc[:,0]\n# os pixels variam de 0 a 255\n# vamos dividir por 255 para que range fique entre 0 e 1\nX = np.array(X)\/255.\nprint(X.shape, y.shape)","02aba096":"# as imagens cont\u00eam n\u00fameros de 0 a 9\nf, axes = plt.subplots(2, 5)\nplt.subplots_adjust(hspace=0, wspace=0)\n\nix = 0\nfor rows in axes:\n    for ax in rows:\n        ax.imshow(X[ix].reshape(-1, 28), 'gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ix += 1","0cc721a1":"# fazendo o one hot encoding\ny_onehot = np.zeros((42000,10))\ny_onehot[range(42000), y] = 1.\ny_onehot","62d01d73":"# TODO\n# Utilize a funcao train_test_split para dividir o dataset\nx_train, x_valid, y_train, y_valid = train_test_split(X, y_onehot, test_size=0.33, random_state=42)\nprint(x_train.shape, x_valid.shape)\nprint(y_train.shape, y_valid.shape)\n\nassert len(y_train.shape) != 1, \"Utilize o 'y_onehot' na fun\u00e7\u00e3o\"\nassert y_train.shape[1] == 10, \"Utilize o 'y_onehot' na fun\u00e7\u00e3o\"","71f3e943":"# TODO\n# calcule a media e o desvio padrao\nmean = x_train.mean()\nstd = x_train.std()","e6c7ec0d":"# TODO\n# normalize o training set\nx_new = (x_train-mean)\/std\n\nassert np.abs(x_new.mean()) < 1e-8, 'A media precisa estar proximo de zero'\nassert np.abs(1. - x_new.std()) < 1e-8, 'O desvio padrao precisa estar proximo de 1'\n\nx_train = x_new","40b3e916":"# TODO\n# normalize o validation set usando a media e o desvio padrao calculado anteriormente\nx_new = (x_valid-mean)\/std\n\nx_valid = x_new","269bbd81":"md = ImageClassifierData.from_arrays('..\/input\/', (x_train, y_train), (x_valid, y_valid))","9003de26":"def get_weights(*dims):\n    return nn.Parameter(torch.randn(dims)\/dims[0])\n\ndef softmax(x):\n    return torch.exp(x)\/(torch.exp(x).sum(dim=1)[:,None])","7eb678f6":"class NeuralNet(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        # TODO\n        # use a fun\u00e7\u00e3o `get_weights` para inicializar a matriz\n        # essa matriz deve refletir o est\u00edmulo do input layer para a hidden layer\n        self.l1_w = get_weights(28*28,15)\n        self.l1_b = get_weights(15)\n        \n        # TODO\n        # use a fun\u00e7\u00e3o `get_weights` para inicializar a matriz\n        # essa matriz deve refletir o est\u00edmulo da hidden para o output layer\n        self.l2_w = get_weights(15,10)\n        self.l2_b = get_weights(10)\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = x @ self.l1_w + self.l1_b\n        x = x @ self.l2_w + self.l2_b\n        x = torch.log(softmax(x))\n        return x\n\nnet = NeuralNet()\n\nassert net.l1_w.shape[0] == 28**2\nassert net.l1_w.shape[1] == 15\nassert net.l1_b.shape[0] == 15\n\nassert net.l2_w.shape[0] == 15\nassert net.l2_w.shape[1] == 10\nassert net.l2_b.shape[0] == 10","ffb7da18":"arr = np.arange(.01, 1., .01)\nplt.plot(arr, -np.log(arr))\nplt.xlabel('p(x)')\nplt.ylabel('-log p(x)')\nplt.show()","45a6a9ee":"# treinando o modelo\nloss = nn.NLLLoss()\nlearning_rate = 1e-2\nopt = torch.optim.SGD(net.parameters(), learning_rate)\n\nepochs = 50\n\ntrain_size = len(md.trn_dl)\n\nall_val_loss = []\nall_trn_loss = []\nfor epoch in range(epochs):\n    losses = []\n    dl = iter(md.trn_dl)\n    for _ in range(train_size):\n        x_batch, y_batch = next(dl)\n        \n        y_pred = net(x_batch)\n        l = loss(y_pred, np.argmax(y_batch.long(), 1))\n        losses.append(l.data)\n        \n        opt.zero_grad()\n        l.backward()\n        opt.step()\n\n    val_losses = []\n    val_scores = []\n    for x_batch, y_batch in iter(md.val_dl):\n        y_pred = net(x_batch)\n        l = loss(y_pred, np.argmax(y_batch.long(), 1))\n        val_losses.append(l.data)\n        val_scores.extend(torch.argmax(y_pred, 1) == torch.argmax(y_batch, 1))\n\n    all_trn_loss.append(np.mean(losses))\n    all_val_loss.append(np.mean(val_losses))\n    print(f'\\rEpoch: {epoch+1:3}\/{epochs} | Training loss: {np.mean(losses):.4f} | ' + \\\n            f'Validation Loss: {np.mean(val_losses):.4f} - Validation Accuracy: {np.mean(val_scores):.4f}', end='')\n    if (epoch+1) % 5 == 0:\n        print('')","3e66fe30":"plt.plot(range(len(all_trn_loss)), all_trn_loss, label='training loss')\nplt.plot(range(len(all_val_loss)), all_val_loss, label='validation loss')\nplt.legend()","e24b25e7":"Queremos que a rede neural seja capaz de dizer qual n\u00famero est\u00e1 desenhado em cada imagem. <br \/>\nIsso se trata de uma classifica\u00e7\u00e3o!\n\nPara facilitar o desempenho da rede neural, precisamos fazer o *one hot encoding*. <br \/>\nAssim, ela ser\u00e1 capaz de atribuir uma probabilidade para cada label.","571e6ea2":"# Implementando a Rede Neural\nAs redes neurais artificias foram inspiradas nas redes neurais biol\u00f3gicas<sup>1<\/sup> que comp\u00f5em o sistema nervoso. <br \/>\nAs redes neurais biol\u00f3gicas recebem est\u00edmulos e os propagam por toda a rede at\u00e9 o c\u00e9rebro. O c\u00e9rebro, ent\u00e3o, processa e reage a esses est\u00edmulos.\n\nAnalogamente, as redes neurais artificais recebem est\u00edmulos atrav\u00e9s da camada de entrada (*input layer*) e os propagam atrav\u00e9s das camadas intermedi\u00e1rias (*hidden layers*). <br \/>\nPor fim, a camada de sa\u00edda (*output layer*) nos devolve um resultado.\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/4\/46\/Colored_neural_network.svg\">\n\nQuando n\u00f3s propagamos esse est\u00edmulo at\u00e9 o *output layer*, n\u00f3s estamos fazendo um *prediction*. <br \/>\nN\u00f3s chamamos esse fluxo de *feed forward*.\n\nPara indicar a rede neural o qu\u00e3o bem est\u00e1 o desempenho dela, utilizamos uma fun\u00e7\u00e3o objetivo (*loss function*). <br \/>\nA fun\u00e7\u00e3o objetivo consegue mensurar o erro, e ent\u00e3o, um est\u00edmulo do erro \u00e9 propagado de volta at\u00e9 a camada de entrada a fim de ajustar os pesos de cada camada e minimizar o erro. <br \/>\nEsse fluxo se chama de *back propagation*.\n\nAgora, vamos implementar uma rede neural que reflete a imagem a acima. <br \/>\n\n* O *input layer* ter\u00e1 784 n\u00f3s\n* A *hidden layer* ter\u00e1 15 n\u00f3s\n* A *output layer* ter\u00e1 10 n\u00f3s\n\nSeguindo a configura\u00e7\u00e3o descrita acima, vamos precisar:\n* do *input layer* at\u00e9 o *hidden layer*, uma matriz 784 por 15\n* da *hidden layer* at\u00e9 o *output layer*, uma matriz 15 por 10","5324480f":"Antes de montar a rede neural, vamos precisar dividir o dataset em training\/validation set. <br \/>\nFazemos isso para ter um meio de mensurar a performance do nosso modelo.","272a5b50":"# Rede Neural do Zero\nEsse caderno tem a finalidade de introduzir redes neurais para os alunos do curso de fast.ai em Bras\u00edlia.\n\n# Conhecendo o dataset\n## MNIST dataset\nPara treinar nossa rede neural, vamos utilizar o MNIST dataset. <br \/>\n* O dataset cont\u00e9m imagens com n\u00fameros de 0 a 9.\n* Cada imagem \u00e9 um vetor que cont\u00e9m 784 pixels que variam de 0 a 255.\n    * O pixel pr\u00f3ximo de 0 tem cor preta e pr\u00f3ximo de 255 tem cor branca.","dd85570a":"Por fim,  vamos treinar nossa rede neural. <br \/>\n\nPara orientar nossa rede neural, vamos usar a fun\u00e7\u00e3o objetivo `nn.NLLLoss`. <br \/>\nEssa fun\u00e7\u00e3o \u00e9 chamada de *Negative Log Likelihood Loss* ou *Categorical Cross Entropy*. <br \/>\nEla \u00e9 utilizada para classifica\u00e7\u00e3o de N classes.\n\nA intui\u00e7\u00e3o sobre essa fun\u00e7\u00e3o \u00e9 simples. <br \/>\nO objetivo da rede neural \u00e9 minimizar o erro das *predictions* em rela\u00e7\u00e3o \u00e0s *targets*, ou seja, n\u00f3s queremos maximizar a probabilidade de que vamos acertar nossas *targets*.\n\nNo gr\u00e1fico abaixo, n\u00f3s temos a fun\u00e7\u00e3o objetivo. <br \/>\nNote que quando a probabilidade est\u00e1 pr\u00f3xima de 1.0, o erro est\u00e1 pr\u00f3ximo de zero. <br \/>\nE da mesma forma, quanto mais pr\u00f3ximo de zero, maior ser\u00e1 o erro.","0f3ba59a":"## Normalizando os dados\nDiferente das \u00e1rvores de decis\u00e3o, h\u00e1 a necessidade de normalizar os inputs para facilitar o desempenho das redes neurais.\n\nPara normalizar, precisamos subtrair do input a m\u00e9dia e dividir pelo desvio padr\u00e3o.\n$$ X_{new} = \\frac{(X_{old} - \\mu)}{\\sigma}$$"}}