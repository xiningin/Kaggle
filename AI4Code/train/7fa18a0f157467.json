{"cell_type":{"e6970691":"code","656303ce":"code","4e3b4332":"code","069707a3":"code","a6b43392":"code","a2621468":"code","87fee92f":"code","1e11d9dd":"code","cf4a72d0":"code","364d550a":"code","e4c5aaa3":"code","a5adee2a":"code","bc1508b4":"code","819121c8":"code","5c2822c6":"code","f1711cf3":"code","d6cfa0b5":"code","a73d3bc0":"code","d1f65c5d":"code","a5fd7ddf":"code","1c3aafee":"code","66514eda":"code","31f30150":"code","00229bf8":"code","3d7a8684":"code","0ad4c5df":"code","b53daafd":"code","fa94c0f4":"code","ce99c6f5":"code","6d8272a1":"code","8183af2f":"code","edcb5da2":"code","206b046f":"code","5a356f0b":"code","8fd43463":"code","eda66efe":"code","4df181ec":"code","3ed19adf":"code","46243629":"code","4a6b89e5":"code","86c969e5":"code","b4b10b9d":"code","480190e4":"code","74df9b43":"code","21a80907":"code","7e12626a":"code","bfdad80c":"code","2f5e9e5e":"code","309a14af":"markdown","268d3495":"markdown","6761d05e":"markdown","81565648":"markdown","0e8aadb5":"markdown","1d8064ab":"markdown","f6b00dd7":"markdown","6ace1926":"markdown","10960682":"markdown","be418c29":"markdown","8533a8f9":"markdown","84c395d1":"markdown","5e4146e5":"markdown","1e93fcc0":"markdown","3648342a":"markdown","8d0d32b0":"markdown","01ec873c":"markdown","7ab00a44":"markdown","70e1f229":"markdown","a885d783":"markdown","6fce5f94":"markdown","e17c3481":"markdown","8544760f":"markdown","57e123bd":"markdown","69c25553":"markdown","27534b4c":"markdown","a6b2749c":"markdown","c632c937":"markdown","93275148":"markdown","ba0aca0e":"markdown","71723d80":"markdown","14cae7fa":"markdown","89306d9e":"markdown"},"source":{"e6970691":"# import and all essentials for the report\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom IPython.display import display","656303ce":"# this section contains the filenames of the data and columns mappings\n\n\nlist_of_files = ['PAMAP2_Dataset\/Protocol\/subject101.dat',\n                 'PAMAP2_Dataset\/Protocol\/subject102.dat',\n                 'PAMAP2_Dataset\/Protocol\/subject103.dat',\n                 'PAMAP2_Dataset\/Protocol\/subject104.dat',\n                 'PAMAP2_Dataset\/Protocol\/subject105.dat',\n                 'PAMAP2_Dataset\/Protocol\/subject106.dat',\n                 'PAMAP2_Dataset\/Protocol\/subject107.dat',\n                 'PAMAP2_Dataset\/Protocol\/subject108.dat',\n                 'PAMAP2_Dataset\/Protocol\/subject109.dat' ]\n\nsubjectID = [1,2,3,4,5,6,7,8,9]\n\nactivityIDdict = {0: 'transient',\n              1: 'lying',\n              2: 'sitting',\n              3: 'standing',\n              4: 'walking',\n              5: 'running',\n              6: 'cycling',\n              7: 'Nordic_walking',\n              9: 'watching_TV',\n              10: 'computer_work',\n              11: 'car driving',\n              12: 'ascending_stairs',\n              13: 'descending_stairs',\n              16: 'vacuum_cleaning',\n              17: 'ironing',\n              18: 'folding_laundry',\n              19: 'house_cleaning',\n              20: 'playing_soccer',\n              24: 'rope_jumping' }\n\ncolNames = [\"timestamp\", \"activityID\",\"heartrate\"]\n\nIMUhand = ['handTemperature', \n           'handAcc16_1', 'handAcc16_2', 'handAcc16_3', \n           'handAcc6_1', 'handAcc6_2', 'handAcc6_3', \n           'handGyro1', 'handGyro2', 'handGyro3', \n           'handMagne1', 'handMagne2', 'handMagne3',\n           'handOrientation1', 'handOrientation2', 'handOrientation3', 'handOrientation4']\n\nIMUchest = ['chestTemperature', \n           'chestAcc16_1', 'chestAcc16_2', 'chestAcc16_3', \n           'chestAcc6_1', 'chestAcc6_2', 'chestAcc6_3', \n           'chestGyro1', 'chestGyro2', 'chestGyro3', \n           'chestMagne1', 'chestMagne2', 'chestMagne3',\n           'chestOrientation1', 'chestOrientation2', 'chestOrientation3', 'chestOrientation4']\n\nIMUankle = ['ankleTemperature', \n           'ankleAcc16_1', 'ankleAcc16_2', 'ankleAcc16_3', \n           'ankleAcc6_1', 'ankleAcc6_2', 'ankleAcc6_3', \n           'ankleGyro1', 'ankleGyro2', 'ankleGyro3', \n           'ankleMagne1', 'ankleMagne2', 'ankleMagne3',\n           'ankleOrientation1', 'ankleOrientation2', 'ankleOrientation3', 'ankleOrientation4']\n\ncolumns = colNames + IMUhand + IMUchest + IMUankle","4e3b4332":"# loading the data into pandas DataFrame\n\ndata_pamap2 = pd.DataFrame()\ninp_prefix = \"\/kaggle\/input\/\"\n\nfor file in list_of_files:\n    file = inp_prefix + file\n    subject = pd.read_table(file, header=None, sep='\\s+')\n    subject.columns = columns\n    subject['subject_id'] = int(file[-5])\n    data_pamap2 = data_pamap2.append(subject, ignore_index=True)\n\ndata_pamap2.reset_index(drop=True, inplace=True)","069707a3":"display(data_pamap2.head())","a6b43392":"def clean_data(data):\n    # removing data with transient activity\n    data = data.drop(data[data['activityID']==0].index)\n    # remove non-numeric data cells\n    data = data.apply(pd.to_numeric, errors = 'coerce')\n    # removing NaN values using iterpolation\n    data = data.interpolate()\n    return data\n    ","a2621468":"data_clean = clean_data(data_pamap2)\ndata_clean.reset_index(drop=True,inplace=True)\ndisplay(data_clean.head(15))","87fee92f":"print(data_clean.isnull().sum())","1e11d9dd":"for i in range(4):\n    data_clean[\"heartrate\"].iloc[i]=100\nprint(data_clean.isnull().sum())","cf4a72d0":"data_clean_copy = data_clean\nplt.figure()\n# activity distribution\nN = len(np.unique(data_clean_copy['activityID']))\nxticks = np.arange(12)\nxticks_lbl = [activityIDdict[x] for x in  np.unique(data_clean_copy['activityID']).tolist()]\ndata_clean_copy['activityID'].value_counts().plot(kind=\"bar\", figsize=(20,10), color=plt.cm.Paired(np.arange(N)))\nplt.xticks(ticks=xticks,labels=xticks_lbl)\nplt.title(\"Activity Distribution\")\nplt.xlabel('Activity')\nplt.ylabel(\"time interval for activity(0.01sec)\")\nplt.show()","364d550a":"display(data_clean.describe())","e4c5aaa3":"data_clean['heartrate'].groupby(data_clean['activityID']).mean()","a5adee2a":"plt.figure()\ndf_heartrate = data_clean['heartrate'].groupby(data_clean['activityID']).mean()\ndf_heartrate.index = df_heartrate.index.map(activityIDdict)\nplt.title(\"Heart Rate to Activity Mean\")\nplt.xlabel(\"Activity Name\")\nplt.ylabel(\"Heart Rate\")\ndf_heartrate.plot(kind='bar',figsize=(20,10), color=plt.cm.Paired(np.arange(N)))\nplt.show()","bc1508b4":"# data scaling helper functions\n\ndef scale_data(train_data,test_data,features):\n    from sklearn.preprocessing import RobustScaler\n    \n    scaler = RobustScaler()\n    train_data = train_data.copy()\n    test_data = test_data.copy()\n    \n    scaler = scaler.fit(train_data[features])\n    train_data.loc[:,features] = scaler.transform(train_data[features].to_numpy())\n    test_data.loc[:,features] = scaler.transform(test_data[features].to_numpy())\n    return train_data, test_data\n\ndef get_features_for_scale(test_list, remove_list):\n    res = [i for i in test_list if i not in remove_list]\n    return res","819121c8":"def get_test_data(data_clean):\n    sub_7 = data_clean[data_clean[\"subject_id\"] == 7]\n    sub_8 = data_clean[data_clean[\"subject_id\"] == 8]\n    return sub_7.append(sub_8)\n\ndef get_train_data(data_clean):\n    not_sub_7 = data_clean[data_clean[\"subject_id\"] != 7]\n    not_sub_8 = not_sub_7[not_sub_7[\"subject_id\"] != 8]\n    return not_sub_8","5c2822c6":"print(\"data_clean shape:\" + str(data_clean.shape))\n\ntest_data = get_test_data(data_clean)\nprint(\"test_data shape:\" + str(test_data.shape))\ntrain_data = get_train_data(data_clean)\nprint(\"train_data shape:\" + str(train_data.shape))","f1711cf3":"test_data = test_data.drop([\"subject_id\",\"timestamp\"], axis=1)\ntrain_data = train_data.drop([\"subject_id\", \"timestamp\"], axis=1)","d6cfa0b5":"features_for_scale = get_features_for_scale(list(train_data.columns),['activityID'])","a73d3bc0":"train_sc, test_sc = scale_data(train_data,test_data,features_for_scale)\nprint(\"scaled train data\")\ndisplay(train_sc.head(5))\nprint(\"scaled test data\")\ndisplay(test_sc.head(5))","d1f65c5d":"# getting X,y values for train and test set\n\nX_train = train_sc.drop('activityID', axis=1).values\ny_train = train_sc['activityID'].values\n\nX_test = test_sc.drop('activityID', axis=1).values\ny_test = test_sc['activityID'].values","a5fd7ddf":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Input\nfrom keras.utils import to_categorical\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.linear_model import LogisticRegression","1c3aafee":"# visualizations helper functions\ndef plot_accuracy_vs_loss(history):\n  # from lecture no.2\n\n  fig, ax = plt.subplots(1,2,figsize=(12,4))\n  ax[0].plot(history.history['accuracy'])\n  ax[0].plot(history.history['val_accuracy'])\n  ax[0].set_title('Model accuracy')\n  ax[0].set_ylabel('Accuracy')\n  ax[0].set_xlabel('Epoch')\n  ax[0].legend(['Train', 'Test'], loc='upper left')\n\n  # Plot training & validation loss values\n  ax[1].plot(history.history['loss'])\n  ax[1].plot(history.history['val_loss'])\n  ax[1].set_title('Model loss')\n  ax[1].set_ylabel('Loss')\n  ax[1].set_xlabel('Epoch')\n  ax[1].legend(['Train', 'Test'], loc='upper left')\n  plt.show()\n\ndef plot_confusion_matrix(model,X_test,y_test,title='',is_ml=False, labels={}):\n    \n  ticks = list(map(lambda x : activityIDdict[x], np.unique(y_test).tolist())) if labels=={} else labels.values()\n  preds = model.predict(X_test)\n  pred_cat = preds if is_ml else np.argmax(preds,axis=1)\n  print('model accuracy on test set is: {0:.2f}%'.format(accuracy_score(y_test,pred_cat)*100))\n  plt.figure(figsize=(15,8),dpi=120)\n  sns.heatmap(confusion_matrix(y_test,pred_cat),cmap='Blues',annot=True, fmt='d',xticklabels=ticks,yticklabels=ticks)\n  plt.xlabel('Prediction')\n  plt.ylabel('True label')\n  plt.title(title)\n  plt.show()","66514eda":"train_act = train_sc.groupby(data_clean['activityID'])\nX_train_base = train_act.std().drop('activityID', axis=1).values\ny_train_base = np.array(train_act['activityID'].unique().explode().values).astype('float64')","31f30150":"# using logistic regression as our estimator for the naive model\nlogreg = LogisticRegression()\nlogreg.fit(X_train_base, y_train_base)","00229bf8":"y_pred_train = logreg.predict(X_train)\nprint('model accuracy on train set is: {0:.2f}%'.format(accuracy_score(y_train,y_pred_train)*100))\ny_pred_test = logreg.predict(X_test)\nprint('model accuracy on test set is: {0:.2f}%'.format(accuracy_score(y_test,y_pred_test)*100))","3d7a8684":"from sklearn.tree import DecisionTreeClassifier\n\nfeatures_used = ['handAcc16_1', 'handAcc16_2', 'handAcc16_3', 'handAcc6_1', 'handAcc6_2', 'handAcc6_3',\n                'chestAcc16_1', 'chestAcc16_2', 'chestAcc16_3', 'chestAcc6_1', 'chestAcc6_2', 'chestAcc6_3',\n                'ankleAcc16_1', 'ankleAcc16_2', 'ankleAcc16_3', 'ankleAcc6_1', 'ankleAcc6_2', 'ankleAcc6_3',\n                'handGyro1', 'handGyro2', 'handGyro3',\n                'chestGyro1', 'chestGyro2', 'chestGyro3',\n                'ankleGyro1', 'ankleGyro2', 'ankleGyro3','activityID']\n\ntrain_dt = train_sc.loc[:,features_used]\ntest_dt =  test_sc.loc[:, features_used]\n\n\n# getting X,y values for train and test set\n\nX_train_dt = train_dt.drop('activityID', axis=1).values\ny_train_dt = train_dt['activityID'].values\n\nX_test_dt = test_dt.drop('activityID', axis=1).values\ny_test_dt = test_dt['activityID'].values","0ad4c5df":"tclf = DecisionTreeClassifier()\ntclf.fit(X_train_dt,y_train_dt)","b53daafd":"plot_confusion_matrix(tclf,X_test_dt,y_test_dt,\"Decision Tree Classification Using Accelerometer And Gyroscope Data\",is_ml=True)","fa94c0f4":"from scipy import stats\n\ndef create_dataset(X, y, time_steps=1, step=1):\n    Xs, ys = [], []\n    for i in range(0, len(X) - time_steps, step):\n        v = X.iloc[i:(i + time_steps)].values\n        labels = y.iloc[i: i + time_steps]\n        Xs.append(v)\n        ys.append(stats.mode(labels)[0][0])\n    return np.array(Xs), np.array(ys).reshape(-1, 1)","ce99c6f5":"TIME_STEPS = 6\nSTEPS = 2\n\ndf_train_X = train_sc.drop('activityID', axis=1)\ndf_train_y = train_sc['activityID']\n\ndf_test_X = test_sc.drop('activityID', axis=1)\ndf_test_y = test_sc['activityID']\n\nX_train_lstm, y_train_lstm = create_dataset(df_train_X,df_train_y,TIME_STEPS,STEPS)\nX_test_lstm, y_test_lstm = create_dataset(df_test_X,df_test_y,TIME_STEPS,STEPS)","6d8272a1":"from sklearn.preprocessing import OneHotEncoder\n\n# use of OneHotEncoder for the labels(classifaction task)\nenc = OneHotEncoder(handle_unknown='ignore', sparse=False)\nenc.fit(y_train_lstm)\n\ny_train_lstm = enc.transform(y_train_lstm)\ny_test_lstm = enc.transform(y_test_lstm)","8183af2f":"lstm_model = Sequential()\nlstm_model.add(LSTM(16,input_shape=(X_train_lstm.shape[1],X_train_lstm.shape[2])))\nlstm_model.add(Dense(16,activation='relu'))\nlstm_model.add(Dense(y_train_lstm.shape[1], activation='softmax'))\n\nlstm_model.summary()\nlstm_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","edcb5da2":"np.random.seed(2018)\nhist_lstm = lstm_model.fit(X_train_lstm,y_train_lstm,validation_split=0.2,epochs=6, verbose=1)","206b046f":"plot_accuracy_vs_loss(hist_lstm)","5a356f0b":"def map_one_hot_enc_positions_and_labels(model,X_test,y_test):\n    preds = model.predict(X_test)\n    preds = np.argmax(preds,axis=1)\n    indicies = np.unique(preds).tolist()\n    actual = np.unique(y_test).tolist()\n    \n    zipped = list(zip(actual, indicies))\n    idx_dict = dict(zipped)\n    y_test_e = []\n    labelsDict = dict()\n    for act in y_test.tolist():\n        y_test_e.append(idx_dict[act[0]])\n    \n    for (act,idx) in zipped:\n        labelsDict[idx] = activityIDdict[act]\n    return np.array(y_test_e), labelsDict\n    ","8fd43463":"y = enc.inverse_transform(y_test_lstm)\ny,labels = map_one_hot_enc_positions_and_labels(lstm_model,X_test_lstm,y)\nplot_confusion_matrix(lstm_model,X_test_lstm,y,\"LSTM Model\",labels=labels)","eda66efe":"# Note we are using the same TIME_STEPS and STEPS of the model we want to fine tune\n\nTIME_STEPS = 6\nSTEPS = 2\n\n# creating the relevant dataset\n\nfeatures_for_scale_hr = get_features_for_scale(list(train_data.columns),[])\ntrain_sc_hr, test_sc_hr = scale_data(train_data,test_data,features_for_scale_hr)\n\ndf_train_X_hr = train_sc_hr.drop('heartrate', axis=1)\ndf_train_y_hr = train_sc_hr['heartrate']\n\ndf_test_X_hr = test_sc_hr.drop('heartrate', axis=1)\ndf_test_y_hr = test_sc_hr['heartrate']\n\nX_train_hr, y_train_hr = create_dataset(df_train_X_hr,df_train_y_hr,TIME_STEPS,STEPS)\nX_test_hr, y_test_hr = create_dataset(df_test_X_hr,df_test_y_hr,TIME_STEPS,STEPS)","4df181ec":"# creating our regression model - predicting heartrate\nfrom keras.layers import Dropout\n\nhr_model = Sequential()\nhr_model.add(LSTM(128,input_shape=(X_train_hr.shape[1],X_train_hr.shape[2])))\nhr_model.add(Dropout(0.25))\nhr_model.add(Dense(128,activation='relu'))\nhr_model.add(Dense(y_train_hr.shape[1],activation='linear'))\nhr_model.summary()\nhr_model.compile(loss='mse', optimizer='adam', metrics=['mae'])","3ed19adf":"np.random.seed(2018)\nhist_hr = hr_model.fit(X_train_hr,y_train_hr,validation_split=0.2,epochs=6, verbose=1)","46243629":"ypred_hr = hr_model.predict(X_test_hr)","4a6b89e5":"from sklearn.metrics import mean_absolute_error as mae\naccuracy = mae(y_test_hr, ypred_hr)\nprint(\"heart rate regressor accuracy:\" +str(accuracy))","86c969e5":"# fine tuning our previous model\nfrom keras.layers import Reshape\n\nlstm_hr_model = Sequential()\n\nfor layer in hr_model.layers[:-1]:\n    lstm_hr_model.add(layer)    \n\n# Freeze the layers \nfor layer in lstm_hr_model.layers:\n    layer.trainable = False\n\nlstm_hr_model.add(Reshape((128,1), input_shape=(128,)))\nlstm_hr_model.add(LSTM(128))\nlstm_hr_model.add(Dense(128,activation='relu'))\nlstm_hr_model.add(Dense(y_train_lstm.shape[1], activation='softmax'))\n\nlstm_hr_model.summary()\nlstm_hr_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","b4b10b9d":"np.random.seed(2018)\nhist_lstm_hr = lstm_hr_model.fit(X_train_lstm,y_train_lstm,validation_split=0.2,epochs=3, verbose=1)","480190e4":"plot_accuracy_vs_loss(hist_lstm_hr)\ny_hr = enc.inverse_transform(y_test_lstm)\ny_hr,labels_hr = map_one_hot_enc_positions_and_labels(lstm_hr_model,X_test_lstm,y_hr)\nplot_confusion_matrix(lstm_hr_model,X_test_lstm,y,\"HR-LSTM Model\",labels=labels_hr)","74df9b43":"TIME_STEPS = 1000\nSTEPS = 200\n\ndf_train_X = train_sc.drop('activityID', axis=1)\ndf_train_y = train_sc['activityID']\n\ndf_test_X = test_sc.drop('activityID', axis=1)\ndf_test_y = test_sc['activityID']\n\nX_train_lstm, y_train_lstm = create_dataset(df_train_X,df_train_y,TIME_STEPS,STEPS)\nX_test_lstm, y_test_lstm = create_dataset(df_test_X,df_test_y,TIME_STEPS,STEPS)","21a80907":"from sklearn.preprocessing import OneHotEncoder\n\n# use of OneHotEncoder for the labels(classifaction task)\nenc = OneHotEncoder(handle_unknown='ignore', sparse=False)\nenc.fit(y_train_lstm)\n\ny_train_lstm = enc.transform(y_train_lstm)\ny_test_lstm = enc.transform(y_test_lstm)","7e12626a":"from keras.layers import Dropout\n\nlstm_model = Sequential()\nlstm_model.add(LSTM(128,input_shape=(X_train_lstm.shape[1],X_train_lstm.shape[2])))\nlstm_model.add(Dense(128,activation='relu'))\nlstm_model.add(Dropout(0.25))\nlstm_model.add(Dense(128,activation='relu'))\nlstm_model.add(Dense(y_train_lstm.shape[1], activation='softmax'))\n\nlstm_model.summary()\nlstm_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","bfdad80c":"np.random.seed(2018)\nhist_lstm = lstm_model.fit(X_train_lstm,y_train_lstm,validation_split=0.2,epochs=6, verbose=1)","2f5e9e5e":"plot_accuracy_vs_loss(hist_lstm)\ny = enc.inverse_transform(y_test_lstm)\ny,labels = map_one_hot_enc_positions_and_labels(lstm_model,X_test_lstm,y)\nplot_confusion_matrix(lstm_model,X_test_lstm,y,\"LSTM Model\",labels=labels)","309a14af":"![image.png](attachment:image.png)\n\nAs it seen from the picture above, most of the data is going be balanced,<br>i.e. most of the task were performed in the same amount of time and with mostly even distribution across the range of subjects.<br><br>Note that **subject 109** performed the least amount of activities, this is a notion we need to refer while testing the model's, we don't want only him to represant our testing data.","268d3495":"We can clearly see that activities that requires more phisical endurance, such as running or rope jumping leads to a higher heart rate.<br>\nwhile activities likes sitting or lying leads to a lower heart rate.","6761d05e":" # Practical Deep Learning Workshop - Assignment 2\n #### by: Roy Levy 313577611 & Yuval Sabag 205712151 \n ---","81565648":"## Creating test and train data\n---\n### We decided to go with the following testing strategy: since subject 7 and 8 performed most of the tasks they will be a good subjects to use as our test data.<br> So our data will be splitted in the following manner:\n* #### train_data: subjects 1-6,9\n* #### test_data: subjects 7-8","0e8aadb5":"### Data Distribution\n---","1d8064ab":"### As it can be seen very blantly our model is overfitting. <br> Also note that although his overfitting it is generalizing a bit better than our previous classic ML model, reaching about 54% accuracy on the test data.><br><br> It seems that this model is classifying 'walking', 'cycling' and 'lying' very well, but activities like 'running' is still hard for him to classify. ","f6b00dd7":"### Data preprocessing\n----\n#### We will start by creating a dataset based on sliding window as shown in class.<br><br>The *timesteps* represant the samples we want to sample in a batch.<br> The *steps* represant the stride of the window.","6ace1926":"### We can that this model is worse than the previous model, also it takes a lot of time to run.<br>We will consider of dropping or tweeking it in order to reach a higher accuracy in short amount of time.","10960682":"# Data Exploration","be418c29":"## Additional preprocessing of the data\n---\n\nAs it can be seen from the description above the data is vastly compromized on big numbers, for example heart rate of 202.<br> In order to reduce computation time we will use a scaler to scale the data, from various attemps we concluded that the best scaler is **RobustScaler**, probably due to the outliers which is the disconnections in the wireless sensors.<br><br>\nAlso note that there is no need for the subjectID or the timestamps since we preprocessed our data to perform as a data for a supervised learning model, we are only interested in predicting the activityID","8533a8f9":"# Building Our First Model\n---","84c395d1":"![image.png](attachment:image.png)\n\nWe can see clearly from the picture above that the age of the subjects varies between 24-32.<br> also note that there are 8 males and only 1 female tested.","5e4146e5":"### Note that we will not suffule our data since each entry position in the dataset has a time meaning.<br>For example: entry 444 was measured 0.01[seconds] before entry 445.","1e93fcc0":"# Loading the Data\nWe will start first by loading and preparing our data\n---","3648342a":"We can see for example that max 'heartrate' is 202 and min is '57', this raises a question about how the activity impacts the heart rate.<br>\nThe following graph will help us to answer that question.","8d0d32b0":"### As we can see the result are very low on both the training and test set, next we will try to better our model using another classic ML algorithm.","01ec873c":"As we can see there are still 4 more values of NaN for the heart rate for the first subject,<br>this is due the notion that the first 4 values were NaN so the interpolation uses these values.<br><br>As it can be seen the heart rate for this subject is 100.0 for at least 10 more timelapses, so we will assumes that the 4 values are also 100.0 since heart rate don't change rapidly, also note that the activity performed by the subject is 'lying' which makes our assumption more realistic.","7ab00a44":"## Building our second model - LSTM\n---\n#### since our data compromized on time-series we will use the notion of Deep Neural Network with LSTM embedding.<br> We will start with a simple LSTM model and based on the result will lean toward better models with a greater accuracy.","70e1f229":"# Conclusions\n---\nIn our process of building an human activity classifier we used a methods and reached to the following conclusions:\n\n1.   We built a naive model that based on the standat deviation of the measurements given in the dataset.<br> This model didn't performed well but showed us a very basic benchmark for improving our results.\n2.   We built a more complex model using decisions trees. This model yielded a better results and gave us an idea of a solid benchmark we want to improve.\n3.   We built a basic LSTM model which splits the dataset into sequences and yielded a slightly better results than our classic ML model.\n4.   We tried to improve our LSTM model by pretraining our model on a built heartrate regressor. This model didn't improve the results and took a long time to run.\n5.   We dropped the heartrate regressor and splitted our data to larger sequences in order to take full advantage of our LSTM model. We also made our network even deeper. This notion improved our model, reaching about 66% accuracy(pervious model was about 50%).<br><br>\n---\n#### The Following table summarizes our accuracy scores during the entire process:\n![image.png](attachment:image.png)\n\n\n\n","a885d783":"Before we start to examine the loaded data we will the start by presanting an overview on the data.<br> if you want a full overview you can check out the provided PDF's.","6fce5f94":"#### Final Note: few suggestions for next is maybe to decrease our STEPS so we can get more data, increase our training time and we can continue and try another pertraining concept,<br> such as predicting hand temperature or even improve our heart rate regressor","e17c3481":"# Building Our Fourth Model - LSTM\n---\n### Since our previous model did not yield the result we wanted we will try a new model.<br>In order to reach a greater accuracy we suggest the following:<br><br> \n1. #### Enlarge TIME_STEPS and STEPS - thus creating smaller dataset but large enough sequences of timestaps for the LSTM embedding\n2. #### Making our network deeper and with more neurons, also dropping the fine-tuning from previous step\n3. #### Trying to do another fine-tuning for the model\n\n### We will implement the first two suggestions in the following code","8544760f":"It can be seen from the sample of the loaded data that some **data cleaning** is required.\nFor example the activityID 0 must be removed completely from the dataset because it is represanting a transient state(see the readme.pdf for more information).","57e123bd":"# Building or Third Model - Using Fine Tuning\n---\n### In this section we are going to try to use fine tuning in order to improve our model.<br> We will start by creating a regression model that predicts the heart rate of a subject based on the other measurements,<br> then use the weights of this model and apply it to our previous model.","69c25553":"# Using DecisionTreeClassifier as ML solid benchmark\n---\nIn this section we will use a DecisionTreeClassifier in order to get a better understanding of the results we are trying to improve.<br> The result given by this classifier will act as a benchmark with which his results we will try improve during our process.<br><br>\nAfter a few tries we concluded that it is best for us to use only the Accelerometer and the Gyroscope data as features for this model.","27534b4c":"#### looks like we managed to improve the model, reaching about 60% accuracy on the test set.<br> we can see that out validation accuracy reached 75% but our loss got higher.<br><br> We can see that our predictions are much more balanced and also that our number of samples is smaller,<br> this is due to the choice of TIMESTEPS and STEPS that define our sequences in our LSTM model.(we concluded that this parameter has the biggest impact on the results).<br><br> It seems that our model is classifing most of the activities in a much more even way, but still it's hard for it to classify 'running' and 'rope_jumping'.<br>\n----\n##### *Side note: after running the model several times we concluded that our accuracy on the test set is about 62%.*","a6b2749c":"## Data Cleaning\n---","c632c937":"## building a naive model\n---\n### We will start by building a first naive model.<br>We will the standart deviation of the features and then fit a simple classic ML model.<br>We will then predict on the real train and test set in order to get a simple base results","93275148":"### Creating our model","ba0aca0e":"### In this notebook we are going to fucus on the *PAMAP2* dataset.\n### The dataset, provided in: https:\/\/archive.ics.uci.edu\/ml\/datasets\/PAMAP2+Physical+Activity+Monitoring<br> consists on measurements that were monitored on 9 subject while performing certain phsical activities over time such as: running, standing or cycle.<br>The measurements were colected by various of wireless sensors that the subjects wore on their body. \n### Our goal for this report is to build a model that can predict the given task a subject is performing based on a given measurements.\n\n---","71723d80":"From the given file **PerformedActivitiesSummary** we can see that few of the data is missing, due to wireless disconnections for example.<br>\nWe chose also to remove the body orientation, because it seems to us irrelavnat and will take the model more time to train.<br><br> The easiest way to remove the NaN values is to use interpolation, we will start by removing the irrelavant values and then interpolate between the unkown values.\n\n\n","14cae7fa":"#### We can see that the model accuracy on the test set is around 50% this is much better than our naive model.<br> This gives us an idea on the starting point from which we will try to improve!<br><br> Notice that the model is misclassifing activities like 'running' and 'rope_jumping' which have similar measurements,<br> and how it is generalizing the activity of 'lying' very well.","89306d9e":"As we can see the data is mostly balanced so no need of data augmentation.<br>\nNext we will see some stats about the data."}}