{"cell_type":{"f37bb5e4":"code","47351219":"code","5f3d0ab0":"code","d9a81886":"code","c9cb0017":"code","c7fa13fe":"code","f7d1a5d8":"code","0982419e":"code","fc0936e6":"code","ce767209":"code","c434ce6f":"code","e4623bdc":"code","bade9127":"code","4cbfbade":"code","7ad85eaf":"code","de2b2948":"code","a402ccb6":"code","d7437639":"code","42d630b5":"code","2d4156cc":"code","970998fa":"code","d76fa641":"code","6514abee":"code","fa5c97ee":"code","c6f3383e":"code","8ec7bb6b":"code","fee63a6a":"code","00ba5f66":"code","13caaf81":"code","8eb208f4":"code","d7bdb28b":"code","658d51b0":"code","3dd08774":"code","1ec8a5d0":"code","b50dd1f7":"code","7c92205d":"code","23b85ac7":"code","9d1fd422":"code","241cbf6a":"code","a1ba95a0":"code","f036cb5f":"code","11e6811e":"code","7655267a":"code","e1140faf":"code","c83308fb":"code","e6109590":"code","d0c50f49":"code","0241e7c9":"code","b1d8ac01":"code","0ed4fc39":"markdown","3ac87a37":"markdown","2ad60eb6":"markdown","c5408863":"markdown","8f2bb20e":"markdown","d0f9e844":"markdown","78a5bf66":"markdown","2d8535b9":"markdown","7d9cbb39":"markdown","70bc940b":"markdown","d481adf9":"markdown","93aec49d":"markdown","462e5381":"markdown","2efe8b9c":"markdown","36345111":"markdown","3d152d89":"markdown","513fbc02":"markdown","f9b8fb04":"markdown","31fd7b5e":"markdown","764956be":"markdown","d4a046bc":"markdown","7c052908":"markdown","34a8aba9":"markdown","c116a641":"markdown","5dc73101":"markdown","7fb15fcf":"markdown","a36f075a":"markdown","18e86958":"markdown","a23a0310":"markdown","514283c3":"markdown","85ca4da8":"markdown","0285951a":"markdown","6cc648a7":"markdown","1fda9104":"markdown","9fa956e2":"markdown","682905a0":"markdown","1b76a387":"markdown"},"source":{"f37bb5e4":"%reload_ext autoreload\n%autoreload 2\n%reload_ext line_profiler\n%matplotlib inline\n\nimport gc\nimport os\nimport time\nimport math\nimport random\nimport numexpr\nimport itertools\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, ExtraTreesClassifier\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, confusion_matrix\n\nimport torch\nfrom torch import nn, cuda\nfrom torch.nn import functional as F\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom torch.optim import Adam, SGD, Optimizer\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nplt.style.use('fivethirtyeight')\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n\nimport warnings\nwarnings.filterwarnings('ignore')","47351219":"# \uc2e4\ud5d8\uc758 \uc7ac\uc0dd\uc0b0\uc744 \uc704\ud55c seed\uac12 \uace0\uc815 \ud568\uc218\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","5f3d0ab0":"# \ub77c\ubca8 \uc778\ucf54\ub529 (train\uacfc test\uc758 \uc885\ubaa9 \ud569\uccd0\uc11c \uc9c4\ud589)\ndef encode_LE(col, train, test, verbose=True):\n    df_comb = pd.concat([train[col],train[col]],axis=0)\n    df_comb,_ = df_comb.factorize(sort=True)\n    nm = col + '_encoded'\n    if df_comb.max() > 32000: \n        train[nm] = df_comb[:len(train)].astype('int32')\n        test[nm] = df_comb[len(train):].astype('int32')\n    else:\n        train[nm] = df_comb[:len(train)].astype('int16')\n        test[nm] = df_comb[len(train):].astype('int16')\n    del df_comb; x = gc.collect()\n    if verbose: print(nm)","d9a81886":"# Merge Column\uc744 \uae30\uc900\uc73c\ub85c agg\ud558\ub294 \ud568\uc218 - ex) code (\uc885\ubaa9)\ubcc4\ub85c \ud3c9\uade0 F \ud53c\uccd0 \uac12\uc744 \uacc4\uc0b0\uc73c\ub85c\uc368 \ud30c\uc0dd \ubcc0\uc218\ub97c \uc0dd\uc131\ud574 \ubaa8\ub378\uc5d0 \uc885\ubaa9 \uc815\ubcf4\ub97c \uc81c\uacf5\ud574\uc904 \uc218 \uc788\ub2e4\ndef code_agg(train_df, test_df, merge_columns, columns, aggs=['mean']):\n    tr, te = df_copy(train_df, test_df)\n    for merge_column in merge_columns:\n        for col in columns:\n            for agg in aggs:\n                valid = pd.concat([tr[[merge_column, col]], te[[merge_column, col]]])\n                new_cn = merge_column + '_' + agg + '_' + col\n                if agg=='quantile':\n                    valid = valid.groupby(merge_column)[col].quantile(0.8).reset_index().rename(columns={col:new_cn})\n                else:\n                    valid = valid.groupby(merge_column)[col].agg([agg]).reset_index().rename(columns={agg:new_cn})\n                valid.index = valid[merge_column].tolist()\n                valid = valid[new_cn].to_dict()\n            \n                tr[new_cn] = tr[merge_column].map(valid)\n                te[new_cn] = te[merge_column].map(valid)\n    return tr, te","c9cb0017":"def df_copy(tr_df, te_df):\n    tr = tr_df.copy()\n    te = te_df.copy()\n    return tr, te","c7fa13fe":"# use numexpr library for improved performance\n# Simple Moving Average (code \ubcc4\ub85c \uacc4\uc0b0)\ndef SMA(df, target, num_windows=3):\n    arr = np.array([])\n    x = df['code_encoded'].values\n    for code in df['code_encoded'].unique():\n        temp_df = df[numexpr.evaluate(f'(x == code)')]\n        arr = np.concatenate((arr, temp_df[target].rolling(window=num_windows, min_periods=1).mean().values))\n    return arr\n\n# Exponential Moving Average (code \ubcc4\ub85c \uacc4\uc0b0)\ndef EMA(df, target, span_num=3):\n    arr = np.array([])\n    x = df['code_encoded'].values\n    for code in df['code_encoded'].unique():\n        temp_df = df[numexpr.evaluate(f'(x == code)')]\n        arr = np.concatenate((arr, temp_df[target].ewm(span=span_num, min_periods=1).mean().values))\n    return arr","f7d1a5d8":"# \ub370\uc774\ud130\uc14b NaN \uc804\ucc98\ub9ac\ndef preprocess_nan(df, feat_cols):\n\n    preprocessed_df = pd.DataFrame()\n\n    # code(\uc885\ubaa9)\ubcc4, \ud53c\uccd0\ubcc4\ub85c NaN \uac12\uc744 \ucc44\uc6b0\ub418, \ucd5c\uadfc \uc2dc\uac04\uc5d0 \ub354 \uac00\uc911\uce58\ub97c \ub450\uace0 \uacc4\uc0b0\ud55c\ub2e4.\n    for code in df['code'].unique():\n        code_df = df[df['code'] == code].copy()\n\n        for i, feat_col in enumerate(code_df[feat_cols]):\n\n            temp_df = code_df[feat_col].dropna().to_frame()\n\n            if len(temp_df) == len(code_df):\n                continue\n\n            temp_df['weight'] = [i for i in range(len(temp_df), 0, -1)]\n\n            try:\n                fill_value = np.average(temp_df[feat_col], weights=temp_df['weight'])\n                code_df[feat_col].fillna(fill_value, inplace=True)\n\n            except:\n                continue\n\n        preprocessed_df = preprocessed_df.append(code_df)\n        \n    del code_df, temp_df; gc.collect()\n    \n    # \ud574\ub2f9 code(\uc885\ubaa9)\uc758 feature\uc5d0 \ubaa8\ub4e0 \uac12\uc774 NaN\uc77c \uacbd\uc6b0 \ubaa8\ub4e0 \uc885\ubaa9\uc758 \ud53c\uccd0 \ud3c9\uade0\uac12\uc73c\ub85c NaN\uc744 \ucc44\uc6b4\ub2e4.  \n    for feat_col in feat_cols:\n        mean_val = preprocessed_df[feat_col].mean()\n        preprocessed_df[feat_col].fillna(mean_val, inplace=True)\n\n    return preprocessed_df","0982419e":"# \uc608\uce21\uac12\uc774 0.5 \uc774\uc0c1\uc77c \uacbd\uc6b0 1, 0.5 \uc774\ud558\uc77c \uacbd\uc6b0 0\uc73c\ub85c mapping\ud55c\ub2e4.\n# threshold\ub294 \uc218\uc815 \uac00\ub2a5\ndef to_binary(preds, threshold=0.5):\n    return np.where(preds >= threshold, 1, 0)","fc0936e6":"# \uc608\uce21 \uacb0\uacfc\ub97c \uae30\ubc18\uc73c\ub85c Confusion Matrix \uc2dc\uac01\ud654\ud558\ub294 \ud568\uc218\ndef plot_confusion_matrix(cm, target_names, title='Validation Confusion matrix', cmap=None, normalize=True):\n\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('OrRd_r')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"black\" if cm[i, j] > thresh else \"white\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"black\" if cm[i, j] > thresh else \"white\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","ce767209":"# \ub2e8\uc704 \uc2dc\uac04 td\ub97c \uae30\uc900\uc73c\ub85c validation set\uc744 \uc0dd\uc131\ud558\ub294 \uc704\ud55c \ud568\uc218\ndef make_dict(train_df, test_df, feat_cols, target):\n\n    dataset_dict = {}\n\n    # dataset 1\n    X_train1 = train_df.query(\"td <= 172\")[feat_cols].values\n    X_valid1 = train_df.query(\"td > 172 & td <= 206\")[feat_cols].values\n    \n    scaler = StandardScaler()\n    scaler.fit(X_train1)\n    \n    dataset_dict['X_train1'] = scaler.transform(X_train1)\n    dataset_dict['X_valid1'] = scaler.transform(X_valid1)\n    dataset_dict['y_train1'] = train_df.query(\"td <= 172\")[target].values\n    dataset_dict['y_valid1'] = train_df.query(\"td > 172 & td <= 206\")[target].values\n    del scaler\n\n    # dataset 2\n    X_train2 = train_df.query(\"td <= 206\")[feat_cols].values\n    X_valid2 = train_df.query(\"td > 206 & td <= 240\")[feat_cols].values\n    \n    scaler = StandardScaler()\n    scaler.fit(X_train2)\n    \n    dataset_dict['X_train2'] = scaler.transform(X_train2)\n    dataset_dict['X_valid2'] = scaler.transform(X_valid2)\n    dataset_dict['y_train2'] = train_df.query(\"td <= 206\")[target].values\n    dataset_dict['y_valid2'] = train_df.query(\"td > 206 & td <= 240\")[target].values\n    del scaler\n\n    # dataset 3\n    X_train3 = train_df.query(\"td <= 240\")[feat_cols].values\n    X_valid3 = train_df.query(\"td > 240 & td <= 274\")[feat_cols].values\n    \n    scaler = StandardScaler()\n    scaler.fit(X_train3)\n    \n    dataset_dict['X_train3'] = scaler.transform(X_train3)\n    dataset_dict['X_valid3'] = scaler.transform(X_valid3)\n    dataset_dict['y_train3'] = train_df.query(\"td <= 240\")[target].values\n    dataset_dict['y_valid3'] = train_df.query(\"td > 240 & td <= 274\")[target].values\n\n    x_test = test_df[feat_cols].values\n    dataset_dict['x_test'] = scaler.transform(x_test)\n\n    return dataset_dict","c434ce6f":"# \ub370\uc774\ud130\ub97c \ubc1b\uc544 \ud559\uc2b5\uc744 \ud558\ub294 \ud568\uc218. validation \ubc29\ubc95\uc5d0 \ub530\ub77c \ub2e4\ub974\uba70, feature importance\uc640 confusion matrix\ub3c4 \uc2dc\uac01\ud654\ud574 \ubcfc \uc218 \uc788\ub2e4.\ndef make_predictions(dataset_dict, df, feat_cols, lgb_params, valid_type='hold_out', plot_importance=True, plot_confusion=True):\n\n    x_test = dataset_dict['x_test']\n    if valid_type == 'hold_out':\n        X_train = dataset_dict['X_train3']\n        y_train = dataset_dict['y_train3']\n        X_valid = dataset_dict['X_valid3']\n        y_valid = dataset_dict['y_valid3']\n        \n        trn_data = lgb.Dataset(X_train, label=y_train)\n        val_data = lgb.Dataset(X_valid, label=y_valid) \n\n        clf = lgb.train(\n            lgb_params,\n            trn_data,\n            valid_sets = [trn_data, val_data],\n            verbose_eval = 100 ,\n        )   \n        \n        valid_preds = clf.predict(X_valid)\n        preds = clf.predict(x_test)\n        \n        print()\n        print(\"roc_auc_score: {:.4f}\".format(roc_auc_score(y_valid, valid_preds)))\n        print(\"accuracy_score: {:.4f}\".format(accuracy_score(y_valid, to_binary(valid_preds, 0.5))))\n\n        if plot_importance:\n            \n            feature_importance_df = pd.DataFrame()\n            feature_importance_df[\"Feature\"] = feat_cols\n            feature_importance_df[\"Importance\"] = clf.feature_importance()\n            \n            cols = (feature_importance_df[[\"Feature\", \"Importance\"]]\n                .groupby(\"Feature\")\n                .mean()\n                .sort_values(by=\"Importance\", ascending=False).index)\n\n            best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\n            plt.figure(figsize=(14,10))\n            sns.barplot(x=\"Importance\",\n                        y=\"Feature\",\n                        data=best_features.sort_values(by=\"Importance\",\n                                                       ascending=False)[:20], ci=None)\n            plt.title('LightGBM Feature Importance', fontsize=20)\n            plt.tight_layout()\n            \n        if plot_confusion:\n            plot_confusion_matrix(confusion_matrix(y_valid, to_binary(valid_preds, 0.5)), \n                          normalize    = False,\n                          target_names = ['pos', 'neg'],\n                          title        = \"Confusion Matrix\")\n    \n    elif valid_type == 'sliding_window':\n        \n        window_num = 3\n        acc = 0\n        auc = 0\n        for num in range(1, window_num+1):    \n            print(f\"num {num} dataset training starts\")\n            \n            preds = np.zeros(len(x_test))\n            X_train = dataset_dict[f'X_train{num}']\n            y_train = dataset_dict[f'y_train{num}']\n            X_valid = dataset_dict[f'X_valid{num}']\n            y_valid = dataset_dict[f'y_valid{num}']\n            trn_data = lgb.Dataset(X_train, label=y_train)\n            val_data = lgb.Dataset(X_valid, label=y_valid)\n\n            clf = lgb.train(lgb_params, trn_data, valid_sets = [trn_data, val_data], verbose_eval=100)\n            valid_preds = clf.predict(X_valid)\n            preds += clf.predict(x_test) \/ window_num\n            acc += accuracy_score(y_valid, to_binary(valid_preds, 0.5)) \/ window_num\n            auc += roc_auc_score(y_valid, valid_preds) \/ window_num\n            print()            \n        \n        print(\"mean roc_auc_score: {:.4f}\".format(auc))\n        print(\"mean acc_score: {:.4f}\".format(acc))\n\n    return preds","e4623bdc":"# improved version of adam optimizer\nclass AdamW(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay)\n        super(AdamW, self).__init__(params, defaults)\n\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError('AdamW does not support sparse gradients, please consider SparseAdam instead')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                # according to the paper, this penalty should come after the bias correction\n                # if group['weight_decay'] != 0:\n                #     grad = grad.add(group['weight_decay'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                denom = exp_avg_sq.sqrt().add_(group['eps'])\n\n                bias_correction1 = 1 - beta1 ** state['step']\n                bias_correction2 = 1 - beta2 ** state['step']\n                step_size = group['lr'] * math.sqrt(bias_correction2) \/ bias_correction1\n\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n\n                if group['weight_decay'] != 0:\n                    p.data.add_(-group['weight_decay'], p.data)\n\n        return loss","bade9127":"# Multi-Layer Perceptron baseline model (further improvements needed)\nclass MLP(nn.Module):\n    def __init__(self, num_classes=1, num_feats=47):\n        super(MLP, self).__init__()\n        self.num_classes = num_classes         \n        self.mlp_layers = nn.Sequential(\n            nn.Linear(num_feats, 1024),\n            nn.PReLU(),\n            nn.BatchNorm1d(1024),\n            nn.Linear(1024, 512),\n            nn.PReLU(),\n            nn.BatchNorm1d(512),\n            nn.Linear(512, 256),\n            nn.PReLU(),\n            nn.BatchNorm1d(256),\n            nn.Dropout(0.3),\n            nn.Linear(256, 128),\n            nn.PReLU(),\n            nn.BatchNorm1d(128),\n            nn.Dropout(0.2),\n            nn.Linear(128, self.num_classes)\n        )\n        self.sigmoid = nn.Sigmoid()\n        self._initialize_weights()\n\n    def forward(self, x):\n        out = self.mlp_layers(x)\n        return self.sigmoid(out)\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)","4cbfbade":"# numpy array\ub97c \ubc1b\uc544 dataset\uc73c\ub85c\nclass Stock_mlp_dataset(Dataset):\n    def __init__(self, X, Y):\n        self.X = X\n        self.Y = Y\n        self.X_dataset = []\n        self.Y_dataset = []\n        for x in X:\n            self.X_dataset.append(torch.FloatTensor(x))\n        try:\n            for y in Y:\n                self.Y_dataset.append(torch.tensor(y))\n        \n        # test set\uc758 \uacbd\uc6b0\uc5d4 \ub77c\ubca8\uc774 \uc5c6\uc74c\n        except:\n#             print(\"no_label\")\n            pass\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, index):\n        inputs = self.X_dataset[index]\n        # train, valid set\n        try:\n            target = self.Y_dataset[index]\n            return inputs, target\n        # test set\n        except:\n            return inputs","7ad85eaf":"# torch dataset load \ud568\uc218 (dataset => dataloader \ubc18\ud658)\ndef build_dataloader(X, Y, batch_size, shuffle=False):\n    \n    dataset = Stock_mlp_dataset(X, Y)\n    dataloader = DataLoader(\n                            dataset,\n                            batch_size=batch_size,\n                            shuffle=shuffle,\n                            num_workers=2\n                            )\n    return dataloader","de2b2948":"# model build \ud568\uc218 \ndef build_model(device, model_name='MLP', num_classes=1, num_feats=47):\n    if model_name == 'MLP':\n        model = MLP(num_classes, num_feats)\n#     \ubaa8\ub378 \ucd94\uac00 \uac00\ub2a5\n#     elif model_name == '':\n#         model = _\n    else:\n        raise NotImplementedError\n    model.to(device)\n    return model","a402ccb6":"# \ub9e4 epoch\ub9c8\ub2e4 validation\uc744 \uc9c4\ud589, \uac01\uc885 metric\uc758 score\ub97c \ubc18\ud658\ndef validation(model, criterion, valid_loader, use_cuda):\n    \n    model.eval()\n    \n    valid_preds = []\n    valid_targets = []\n    val_loss = 0.\n    \n    with torch.no_grad():\n        for batch_idx, (inputs, target) in enumerate(train_loader):\n\n            target = target.reshape(-1, 1).float()\n            valid_targets.append(target.numpy().copy())\n\n            if use_cuda:\n                inputs = inputs.to(device)\n                target = target.to(device)       \n                    \n            output = model(inputs)\n            \n#             print(output[:10])\n#             print(target[:10])\n\n            loss = criterion(output, target)\n            valid_preds.append(output.detach().cpu().numpy())\n            \n            val_loss += loss.item() \/ len(valid_loader)\n     \n    # to_binary \ud568\uc218\ub97c \ud1b5\ud574 0\uacfc 1 \uc0ac\uc774\uc758 \uc544\uc6c3\ud48b\uc744 0\uacfc 1\ub85c \ub9e4\ud551\n    valid_preds = np.concatenate(valid_preds)\n    valid_targets = np.concatenate(valid_targets)\n    acc = accuracy_score(valid_targets, to_binary(valid_preds))\n    roc_auc = roc_auc_score(valid_targets, valid_preds)\n    recall = recall_score(valid_targets, to_binary(valid_preds)) \n    precision = precision_score(valid_targets, to_binary(valid_preds)) \n    \n    return val_loss, acc, roc_auc, recall, precision","d7437639":"# MLP \ubaa8\ub378\uc744 \ud559\uc2b5\ud558\ub294 \ud568\uc218\ndef train_mlp_model(model, train_loader, valid_loader, criterion, optimizer, scheduler, use_cuda, verbose_epoch=20, path='best_model.pt'):\n    \n    best_valid_acc = 0.0\n    best_epoch = 0\n    count = 0\n    start_time = time.time()\n    \n    for epoch in range(1, num_epochs+1):\n\n        model.train()\n        optimizer.zero_grad()\n        train_loss = 0.0\n\n        for batch_idx, (inputs, target) in enumerate(train_loader):\n\n            target = target.reshape(-1, 1).float()\n\n            if use_cuda:\n                inputs = inputs.to(device)\n                target = target.to(device)\n\n            output = model(inputs)\n            loss = criterion(output, target)\n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            train_loss += loss.item() \/ len(train_loader)\n\n        # validation \uc9c4\ud589\n        val_loss, acc_score, auc_score, recall, precision = validation(model, criterion, valid_loader, use_cuda)\n\n        elapsed = time.time() - start_time\n\n        lr = [_['lr'] for _ in optimizer.param_groups]\n\n        if epoch % verbose_epoch == 0:\n            print('\\nEpoch [{}\/{}]  train Loss: {:.3f}  val_loss: {:.3f}  accuracy: {:.3f}  roc_auc: {:.3f}  recall: {:.3f}  precision: {:.4f}  lr: {:.7f}  elapsed: {:.0f}m {:.0f}s' \\\n                  .format(epoch,  num_epochs, train_loss, val_loss, acc_score, auc_score, recall, precision, lr[0], elapsed \/\/ 60, elapsed % 60))\n        \n        model_path = output_dir \/ path\n\n        # validation accuracy\uac00 \ucd5c\ub300\uc77c \ub54c \ubaa8\ub378 \uc800\uc7a5\n        if acc_score > best_valid_acc:\n            best_epoch = epoch\n            best_valid_acc = acc_score\n            roc_auc_score = auc_score\n            recall_ = recall\n            precision_ = precision\n        \n            torch.save(model.state_dict(), model_path)\n\n        # 50 epoch \ub3d9\uc548 \uac1c\uc120\uc774 \uc5c6\uc73c\uba74 \ud559\uc2b5 \uac15\uc81c \uc885\ub8cc\n        if count == 50:\n            print(\"not improved for 50 epochs\")\n            break\n        if acc_score < best_valid_acc:\n            count += 1\n        else:\n            count = 0\n            \n        # learning_rate scheduling based on accuracy score\n        scheduler.step(acc_score)\n            \n    print(\"\\n- training report\")\n    print(\"best epoch: {}\".format(best_epoch))\n    print(\"accuracy: {:.4f}\".format(best_valid_acc))\n    print(\"roc_auc_score: {:.4f}\".format(roc_auc_score))\n    print(\"recall score: {:.4f}\".format(recall_))\n    print(\"precision score: {:.4f}\\n\".format(precision_))","42d630b5":"def mlp_inference(model, test_loader, batch_size, use_cuda):\n    \n    test_preds = []\n    model.eval()\n    \n    with torch.no_grad():\n        for batch_idx, data in enumerate(test_loader):\n            if use_cuda:\n                data = data.to(device)\n            outputs = model(data)\n            test_preds.append(outputs.detach().cpu().numpy())\n            \n    test_preds = np.concatenate(test_preds)\n    return test_preds","2d4156cc":"# lstm \ubaa8\ub378 \ud559\uc2b5 (\ud558\ub098\uc758 code\ub9cc)\ndef train_lstm_model(model, data_loader, criterion, num_epochs, verbose_eval, model_path):\n    \n    optimizer = Adam(model.parameters(), lr=0.0001)\n    best_loss = float('inf')\n    best_epoch = 0\n    \n    for epoch in range(1, num_epochs+1):\n\n        model.train()\n        optimizer.zero_grad()\n        train_loss = 0\n\n        for i, (X, Y) in enumerate(data_loader):\n            X = X.float()\n            Y = Y.float()\n            if use_cuda:\n                X = X.to(device)\n                Y = Y.to(device)\n            output = model(X) \n\n            loss = 0\n            preds = []\n            for i, y_t in enumerate(Y.chunk(Y.size(1), dim=1)):\n                loss += criterion(output[i], y_t)\n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            train_loss += loss.item() \/ len(data_loader)\n\n        # train loss\uac00 \ub0ae\uc744 \ub54c \ubaa8\ub378 \uc800\uc7a5\n        if train_loss < best_loss:\n            best_epoch = epoch\n            best_loss = train_loss\n            torch.save(model.state_dict(), model_path)\n\n        if epoch % (verbose_eval) == 0:\n            print(\"Epoch [{}\/{}]  train_loss: {:.5f}\".format(epoch, num_epochs, train_loss))    \n\n    print(\"\\nBest epoch: {}  Best Loss: {:.5f}\".format(best_epoch, best_loss))","970998fa":"# \ud2b9\uc815 \ucf54\ub4dc\uc758 \ub370\uc774\ud130\ub97c \ubd88\ub7ec\uc640 scaling => \uc774\ud6c4 Stock_lstm_dataset \ud568\uc218\uc5d0 \uc778\ud48b\uc73c\ub85c \uc804\ub2ec\ndef prepared_code_data(df, code, feat_cols, seq_len=12):\n    \n    temp_df = df[df['code']==code][feat_cols+['target']].reset_index(drop=True)\n\n    X = temp_df[feat_cols].values\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    Y = temp_df['target'].values\n    \n    return X, Y","d76fa641":"# lstm\uc744 \uc704\ud55c \ub370\uc774\ud130\uc14b \uc0dd\uc131 \ud568\uc218\nclass Stock_lstm_dataset(Dataset):\n    def __init__(self, X, Y, seq_len):\n        self.X = []\n        self.Y = []\n        for i in range(len(X) - seq_len):\n            self.X.append(X[i : i + seq_len])\n            self.Y.append(Y[i : i + seq_len])\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, index):\n        inputs = torch.tensor(self.X[index])\n        targets = torch.tensor(self.Y[index])\n        return inputs, targets","6514abee":"# simple rnn based model (further improvements needed)\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNN, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.lstm = nn.LSTM(self.input_size, self.hidden_size, bidirectional=False, batch_first=True)\n        self.lstm1 = nn.LSTM(self.hidden_size, self.hidden_size, bidirectional=False, batch_first=True)\n        self.linear = nn.Linear(self.hidden_size, self.output_size)\n\n    def forward(self, x):\n        outputs = []\n        for i, x_t in enumerate(x.chunk(x.size(1), dim=1)):\n            h_lstm1, _ = self.lstm(x_t)\n            h_lstm2, _ = self.lstm1(h_lstm1)\n            output = self.linear(h_lstm2)\n            outputs += [output.squeeze(-1)]\n        return outputs","fa5c97ee":"# lstm \ubaa8\ub378\uc758 inference \ud568\uc218\ndef lstm_predict(model, X, seq_len):\n    i = 0\n    result = []\n    while (i < X.shape[0]):\n\n        batch_end = i + seq_len\n\n        if batch_end > X.shape[0]:\n            batch_end = X.shape[0]\n        x_input = torch.tensor(X[i: batch_end])\n\n        if x_input.dim() == 2:\n            x_input = x_input.unsqueeze(0)\n\n        x_input = x_input.float()\n        if use_cuda:\n            x_input = x_input.to(device)\n\n        output = model(x_input)\n        for value in output:\n            result.append(value.item())\n\n        i = batch_end\n    return result","c6f3383e":"# \ud558\ub098\uc758 \uc885\ubaa9\uc744 \ud559\uc2b5\ud558\uace0 \uc774\ub97c \uc2dc\uac01\ud654\uae4c\uc9c0 \ud574\uc8fc\ub294 \ud568\uc218\ndef lstm_train_visualize(train_df, code, feat_cols, num_epochs, seq_len, verbose_eval=20, plot_prediction=True):\n    X, Y = prepared_code_data(train_df, code, feat_cols, seq_len)\n\n    dataset = Stock_lstm_dataset(X, Y, seq_len=seq_len)\n    data_loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=2)\n\n    # model setting\n    INPUT_SIZE = len(feat_cols)\n    HIDDEN_SIZE = 100\n    OUTPUT_SIZE = 1\n\n    model = RNN(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n    model.to(device)\n\n    criterion = nn.MSELoss()\n    optimizer = Adam(model.parameters(), lr=0.0001)\n\n    model_path = output_dir \/ 'rnn_best_model.pt'\n\n    train_lstm_model(model, data_loader, criterion, num_epochs, verbose_eval, model_path)\n\n    model = RNN(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n    model.load_state_dict(torch.load(model_path))\n    model.to(device)\n\n    model.eval()\n\n    result = lstm_predict(model, X, seq_len)\n    result_df = pd.DataFrame({'td':[i for i in range(X.shape[0])], 'predicted':result, 'target':Y})\n    \n    if plot_prediction:\n        \n        data = []\n        \n        data.append(go.Scatter(\n            x = result_df['td'].values,\n            y = result_df['target'].values,\n            name = \"target\"\n        ))\n        \n        data.append(go.Scatter(\n            x = result_df['td'].values,\n            y = result_df['predicted'].values,\n            name = \"predicted\"\n        ))\n        layout = go.Layout(dict(title = f\"code: {code}\",\n                          xaxis = dict(title = 'Time'),\n                          yaxis = dict(title = 'Earning Ratio (target)'),\n                          ),legend=dict(orientation=\"h\"))\n\n        py.iplot(dict(data=data, layout=layout), filename='basic-line')\n","8ec7bb6b":"# \uacb0\uacfc \uc7ac\uc0dd\uc0b0\uc744 \uc704\ud55c seed\uac12 \uace0\uc815\nseed = 42\nseed_everything(seed)","fee63a6a":"DATASET_PATH = '..\/input\/stock-price'\n\nX_train = pd.read_csv(os.path.join(DATASET_PATH, 'train_data.csv')) #\ud6c8\ub828 \ub370\uc774\ud130\nY_train = pd.read_csv(os.path.join(DATASET_PATH,'train_target.csv')) # \ud6c8\ub828 \ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc815\ub2f5\ub370\uc774\ud130 for regression\nY2_train = pd.read_csv(os.path.join(DATASET_PATH,'train_target2.csv')) # \ud6c8\ub828 \ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc815\ub2f5\ub370\uc774\ud130 for classification\ntest_df = pd.read_csv(os.path.join(DATASET_PATH,'test_data.csv')) # \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130","00ba5f66":"X_train = X_train.set_index(['td', 'code'])\nY_train = Y_train.set_index(['td', 'code'])\nY2_train = Y2_train.set_index(['td', 'code'])","13caaf81":"# \uc2dc\uac01\ud654 \ubc0f \uc804\ucc98\ub9ac\ubd80\ud130 \ubaa8\ub378\ub9c1\uae4c\uc9c0 \ubcf4\ub2e4 \ud3b8\ud558\uac8c \uc218\ud589\ud558\uae30 \uc704\ud574 \uc0c8\ub85c\uc6b4 \ub370\uc774\ud130\uc14b\uc744 \uc0dd\uc131\nY2_train = Y2_train.rename(columns={'target':'binned_target'})\n\ntrain_df = pd.merge(X_train, Y_train['target'], how='left', on=['td', 'code'])\ntrain_df = pd.merge(train_df, Y2_train['binned_target'], how='left', on=['td', 'code'])\ntrain_df['binary_target'] = train_df['target'].apply(lambda x: 1 if x >= 0 else 0)\n\ntrain_df = train_df.reset_index()\n\ntrain_df['td'] = train_df['td'].str[1:].astype('int')\ntest_df['td'] = test_df['td'].str[1:].astype('int')","8eb208f4":"train_df.head()","d7bdb28b":"# 3\ubc88 \uc774\ud558 \ub4f1\uc7a5\ud558\ub294 code (\uc885\ubaa9) \uc81c\uac70\ntemp_dict = Counter(train_df['code'])\noutlier_codes = [k for k, v in set(temp_dict.items()) if v <= 3]\ntrain_df = train_df.loc[~train_df['code'].isin(outlier_codes)]\n\n# F feature\ub9cc \ucd94\ucd9c\nF_cols = [col for col in train_df.columns if col.startswith('F')]\n\ntrain_df = preprocess_nan(train_df, F_cols)\ntest_df = preprocess_nan(test_df, F_cols)\n\n# code (\uc885\ubaa9) \ub77c\ubca8 \uc778\ucf54\ub529, group \ud1b5\uacc4 \ud30c\uc0dd\ubcc0\uc218 \uc0dd\uc131\uc744 \uc704\ud574\uc11c\nle = LabelEncoder().fit(pd.concat([train_df['code'], test_df['code']]))\nfor df in [train_df, test_df]:\n    df['code_encoded'] = le.transform(df['code'])\n\n# EMA (Exponential Moving Average)\ub97c \uac01\uac01\uc758 F \ud53c\uccd0\uc5d0 \uc801\uc6a9\ud558\uc5ec \uc0c8\ub85c\uc6b4 \ud30c\uc0dd\ubcc0\uc218\ub97c \uc0dd\uc131\ntrain_df = train_df.sort_values(by=['code', 'td']).reset_index(drop=True)\ntest_df = test_df.sort_values(by=['code', 'td']).reset_index(drop=True)\nfor feat_col in F_cols:\n    train_df[f'{feat_col}_EMA_3'] = EMA(train_df, feat_col, 3)\n    test_df[f'{feat_col}_EMA_3'] = EMA(test_df, feat_col, 3)\n\n# code (\uc885\ubaa9)\ubcc4\ub85c \ud1b5\uacc4 \uae30\ubc18 aggregation \ud30c\uc0dd\ubcc0\uc218 \uc0dd\uc131 (mean)\ntrain_df, test_df = code_agg(train_df, test_df, merge_columns=['code'], columns=F_cols, aggs=['mean'])","658d51b0":"# column \uad6c\ubd84\ntarget_cols = [col for col in train_df.columns if col in ['target', 'binned_target', 'binary_target']]\nremove_cols = ['td', 'code']\nremove_cols.append('code_encoded') # label encoding \ud53c\uccd0\ub294 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\ub294\ub2e4.\nfeat_cols = [col for col in train_df.columns if col not in target_cols+remove_cols]","3dd08774":"# check if NaN exists\nprint(\"number of NaNs in the Train Dataset: {}\".format(train_df[feat_cols].isnull().sum().sum()))\nprint(\"number of NaNs in the Test Dataset: {}\".format(test_df[feat_cols].isnull().sum().sum()))","1ec8a5d0":"# \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ud558\ub294 feature \uce7c\ub7fc\ub4e4\nfor col in feat_cols:\n    print(col, end=' ')\nprint(\"\\n\\ntotal features used for training: {}\".format(len(feat_cols)))","b50dd1f7":"# \uc804\ucc98\ub9ac\ub41c tree\uacc4\uc5f4\uacfc mlp \ubaa8\ub378 \ud559\uc2b5\uc744 \uc704\ud55c \ucd5c\uc885 \ub370\uc774\ud130\uc14b (train, validation split + feature normalizing)\ndataset_dict = make_dict(train_df, test_df, feat_cols, 'binary_target')","7c92205d":"# Light GBM parameters\nlgb_params = {\n                'objective':'binary',\n                'boosting_type':'gbdt',\n                'metric':'auc',\n                'n_jobs':-1,\n                'learning_rate':0.01,\n                'num_leaves': 2**8,\n                'max_depth':-1,\n                'tree_learner':'serial',\n                'colsample_bytree': 0.7,\n                'subsample_freq':1,\n                'subsample':0.7,\n                'n_estimators':3000,\n                'max_bin':255,\n                'verbose':-1,\n                'seed': 42,\n                'early_stopping_rounds':100, \n                } ","23b85ac7":"X_train, X_valid, y_train, y_valid = train_test_split(train_df[feat_cols], train_df[target_cols[2]], test_size=0.1, shuffle=True, random_state=42)\n\ntr_data = lgb.Dataset(X_train, label=y_train)\nvl_data = lgb.Dataset(X_valid, label=y_valid) \n\nclf = lgb.train(\n    lgb_params,\n    tr_data,\n    valid_sets = [tr_data, vl_data],\n    verbose_eval = 100 ,\n)   \n\npreds = clf.predict(X_valid)\nprint(\"\\naccuracy score: {:.4f}\".format(accuracy_score(y_valid, to_binary(preds))))\nprint(\"roc_auc score: {:.4f}\".format(roc_auc_score(y_valid, preds)))","9d1fd422":"# dataset 1: train (1 <= td <= 172) valid (172 < td <= 206)\npreds = make_predictions(dataset_dict, train_df, feat_cols, lgb_params, valid_type='hold_out')","241cbf6a":"# \uc815\ud655\ud55c \uad6c\ud604\uc740 \uc544\ub2c8\uc9c0\ub9cc, \uc2dc\uac04 \ub2e8\uc704\uac00 \ube44\uc2dd\ubcc4\ud654 \ub418\uc5b4\uc788\uc5b4 \ub098\ub204\uae30\uac00 \ubaa8\ud638\ud558\uae30 \ub54c\ubb38\uc5d0 \ub2e4\uc74c\uacfc \uac19\uc774 \uc9c4\ud589\ud55c\ub2e4.\n# dataset 1: train (1 <= td <= 172) valid (172 < td <= 206)\n# dataset 2: train (1 <= td <= 206) valid (206 < td <= 240)\n# dataset 3: train (1 <= td <= 240) valid (240 < td <= 276)\npreds = make_predictions(dataset_dict, train_df, feat_cols, lgb_params, valid_type='sliding_window')","a1ba95a0":"# check if using cuda\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nuse_cuda = True if device.type == 'cuda' else False\nuse_cuda","f036cb5f":"# output path \noutput_dir = Path('.\/', 'output')\noutput_dir.mkdir(exist_ok=True, parents=True)","11e6811e":"# train setting (hyper parameters)\nvalid_type = 'sliding_window' # in ['hold_out', 'sliding_window']\n# \uc124\uba85\uc744 \ub367\ubd99\uc774\uc790\uba74, hold_out \ubc29\ubc95\uc740 \uc5b4\ub5a4 \ud53c\uccd0\uac00 \uc88b\uc740\uc9c0 \uc2e4\ud5d8\ud574\ubcf4\uace0\uc790 \ud560 \ub54c, \uc9e7\uc740 \uc2dc\uac04 \uc548\uc5d0 \uc218\ud589\ud560 \uc218 \uc788\uae30\uc5d0 \uc774\uc810\uc774 \uc788\ub294 \ubc18\uba74\uc5d0\n# sliding_window \ubc29\ubc95\uc740 \ubcf8\uaca9\uc801\uc73c\ub85c \ud559\uc2b5\uc744 \uc9c4\ud589\ud558\uace0\uc790 \ud560 \ub54c \uc0ac\uc6a9\ud558\ub3c4\ub85d \ud55c\ub2e4.\nnum_epochs = 120\nverbose_epoch = 20\nlr = 0.00025\nbatch_size = 1024\nnum_classes = 1 # \uc774\uc9c4 \ubd84\ub958\nnum_feats = len(feat_cols)\n\ncriterion = nn.BCELoss()\n# criterion = nn.BCEWithLogitsLoss()","7655267a":"# validation \uc885\ub958\uc5d0 \ub530\ub77c \ub098\ub208\ub2e4 (hold_out, sliding_window \ub450 \uc885\ub958)\nif valid_type == 'hold_out':\n    print(f\"training starts (hold-out)\")\n    \n    model = build_model(device, 'MLP', num_classes, num_feats)\n    optimizer = AdamW(model.parameters(), lr, weight_decay=0.000025)\n    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n\n    X_train = dataset_dict['X_train3']\n    y_train = dataset_dict['y_train3']\n    X_valid = dataset_dict['X_valid3']\n    y_valid = dataset_dict['y_valid3']\n    \n    train_loader = build_dataloader(X_train, y_train, batch_size, shuffle=True)\n    valid_loader = build_dataloader(X_valid, y_valid, batch_size, shuffle=False)\n    train_mlp_model(model, train_loader, valid_loader, criterion, optimizer, scheduler, use_cuda, verbose_epoch)\n    \n# total prediction (using 3 models)\nelif valid_type == 'sliding_window':\n    window_num = 3\n    for num in range(1, window_num+1):    \n        \n        print(f\"num {num} dataset training starts (sliding_window)\")\n        '''\n        dataset 1: train (1 <= td <= 172) valid (172 < td <= 206)\n        dataset 2: train (1 <= td <= 206) valid (206 < td <= 240)\n        dataset 3: train (1 <= td <= 240) valid (240 < td <= 276)\n        '''\n        model = build_model(device, 'MLP', num_classes, num_feats)\n        optimizer = AdamW(model.parameters(), lr, weight_decay=0.000025)\n        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10)\n\n        X_train = dataset_dict[f'X_train{num}']\n        y_train = dataset_dict[f'y_train{num}']\n        X_valid = dataset_dict[f'X_valid{num}']\n        y_valid = dataset_dict[f'y_valid{num}']\n        train_loader = build_dataloader(X_train, y_train, batch_size, shuffle=True)\n        valid_loader = build_dataloader(X_valid, y_valid, batch_size, shuffle=False)\n        path = f'best_model_{num}.pt'\n        train_mlp_model(model, train_loader, valid_loader, criterion, optimizer, scheduler, use_cuda, verbose_epoch, path)\n        del model; gc.collect()\n        print(\"#\"*150)\nelse:\n    raise NotImplementedError","e1140faf":"# MLP inference [hold_out or sliding_window]\nif valid_type == 'hold_out':\n\n    test_loader = build_dataloader(dataset_dict['x_test'], Y=None, batch_size=batch_size, shuffle=False)\n\n    model_path = os.listdir('..\/working\/output')[0]\n    model = build_model(device, 'MLP', num_classes, num_feats)\n    model.load_state_dict(torch.load(os.path.join('..\/working\/output\/', model_path)))\n    model.to(device)\n    total_preds = mlp_inference(model, test_loader, batch_size, use_cuda)\n    total_preds = np.where(total_preds >= 0.5, 1, 0)\n    \nelif valid_type == 'sliding_window':\n    \n    model_list = os.listdir('..\/working\/output')\n    total_preds = []\n    window_num = 3\n    \n    for i in range(window_num):\n        \n        batch_size = 1024\n        test_loader = build_dataloader(dataset_dict['x_test'], Y=None, batch_size=batch_size, shuffle=False)\n\n        model = build_model(device, 'MLP', num_classes, num_feats)\n        model_path = model_list[i]\n        model.load_state_dict(torch.load(os.path.join('..\/working\/output\/', model_path)))\n        model.to(device)\n\n        test_preds = mlp_inference(model, test_loader, batch_size, use_cuda)\n        total_preds.append(test_preds)\n        \n    # logit \ub2e8\uc5d0\uc11c \ubaa8\ub378 \uc138 \uac1c\uc758 \uacb0\uacfc\uac12\uc744 \ud3c9\uade0 (3\uac1c\uc758 window \uae30\ubc18\uc73c\ub85c \uac80\uc99d\ub41c)\n    total_preds = np.mean(total_preds, axis=0)\n    total_preds = np.where(total_preds >= 0.5, 1, 0)\n\nelse:\n    raise NotImplementedError","c83308fb":"test_df['prediction'] = total_preds\nsubmission = test_df[['td', 'code', 'prediction']].set_index(['td', 'code'])\nsubmission.to_csv('submission.csv', index=False)","e6109590":"submission['prediction'].value_counts()","d0c50f49":"# code 'A507' \ud559\uc2b5 (40 epoch)\nlstm_train_visualize(train_df, 'A507', feat_cols, num_epochs=40, seq_len=12)","0241e7c9":"# code 'A507' \ud559\uc2b5 (100 epoch)\nlstm_train_visualize(train_df, 'A507', feat_cols, num_epochs=100, seq_len=12)","b1d8ac01":"# code 'A507' \ud559\uc2b5 (300 epoch)\nlstm_train_visualize(train_df, 'A507', feat_cols, num_epochs=300, seq_len=12)","0ed4fc39":"# 5. Dataset Preprocess","3ac87a37":"\ub2e4\uc74c\uacfc \uac19\uc774 \ucd5c\uc18c\ud55c\uc758 \uac83\ub4e4\ub9cc \uc801\uc6a9\ud558\uace0\uc790 \ud55c\ub2e4.\n- 3\ubc88 \uc774\ud558 \ub4f1\uc7a5\ud558\ub294 code (\uc885\ubaa9) \uc81c\uac70\n- EDA part5.12\uc774 NaN preprocess \uc801\uc6a9\n- train\uacfc test\uc5d0 code (\uc885\ubaa9)\uc5d0 label encoding \uc801\uc6a9\n- EMA\ub97c \ud65c\uc6a9\ud55c F \ud30c\uc0dd\ubcc0\uc218 \uc0dd\uc131\n- code (\uc885\ubaa9) \uae30\ubc18\uc758 aggregation \ud30c\uc0dd\ubcc0\uc218 \uc0dd\uc131m","2ad60eb6":"# 7. Tree based Model","c5408863":"test set \uc608\uce21\uac12\uc758 \ubd84\ud3ec\uac00 \ub300\ub7b5 \ubc18\ubc18\uc73c\ub85c \uc798 \ub098\uc628\uac83\uc73c\ub85c \ud655\uc778\ub41c\ub2e4.","8f2bb20e":"\uc704\uc640 \uac19\uc774 \ub525\ub7ec\ub2dd \uacc4\uc5f4 \ubaa8\ub378\uc740 90\ud504\ub85c\uac00 \ub118\ub294 \uc0c1\ub2f9\ud788 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud558\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \uc815\ud655\ub3c4 \ubfd0\ub9cc \uc544\ub2c8\ub77c \ub2e4\ub978 \uc9c0\ud45c\ub3c4 \ubcfc \uc218 \uc788\uac8c\ub054 \ud574\ub1a8\uc9c0\ub9cc, \uc560\ucd08\uc5d0 \uc815\ud655\ub3c4\uac00 \ub108\ubb34 \ub192\uc544\uc11c \uc758\ubbf8\uc5c6\uc5b4 \ubcf4\uc778\ub2e4.\n<br>\n\ubb3c\ub860 \uc704\uc640 \uac19\uc740 validation\uc758 \uc815\ud655\ub3c4\uac00 \uc2dc\uacc4\uc5f4\uc801 \uc694\uc18c\ub97c \uc644\uc804\ud558\uac8c \ubc18\uc601\ud558\uae30\ub294 \ud798\ub4e4\uae30 \ub54c\ubb38\uc5d0 test set\uc5d0\uc11c\uc758 \uc131\ub2a5\uc744 \ubcf4\uc7a5\ud558\uc9c0\ub294 \uc54a\uc744 \uac83\uc774\ub77c \uc608\uc0c1\ub41c\ub2e4.\n<br>\n\ub525\ub7ec\ub2dd\uc740 **weight \ucd08\uae30\uac12(initialization)** \uc5d0 \ub530\ub77c\uc11c\ub3c4 \uacb0\uacfc\uac00 \ub2ec\ub77c\uc9c0\uae30 \ub54c\ubb38\uc5d0, \ub2e4\uc591\ud55c seed\uac12\uc744 \uae30\ubc18\uc73c\ub85c \ud559\uc2b5\ud55c\ub2e4\uba74 test set\uc5d0\uc11c\uc758 \uc131\ub2a5\uc744 \uc880 \ub354 robust\ud558\uac8c \ub9cc\ub4e4\uc5b4\uc904 \uc218 \uc788\uc744 \uac83\uc73c\ub85c \uae30\ub300\ub41c\ub2e4.\n<br>\n<br>\n\ud558\uc9c0\ub9cc \uadf8\ub9cc\ud07c \ud559\uc2b5\uc774 \uc624\ub798\uac78\ub9b0\ub2e4\ub294 \ub2e8\uc810\uc774 \uc788\uae30 \ub54c\ubb38\uc5d0, \ud559\uc2b5 \uc2dc\uac04\uc744 \uace0\ub824\ud558\uc5ec \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uc88b\uc73c\ub9ac\ub77c \uc0dd\uac01\ud55c\ub2e4. \n<br>\n\uc774 \uacfc\uc81c\uc5d0\uc11c\ub294 seed \uc559\uc0c1\ube14\uc740 \ub118\uc5b4\uac00\ub3c4\ub85d \ud55c\ub2e4 (\uc131\ub2a5 \ub300\uacb0\uc774 \uc544\ub2d8\uc73c\ub85c)\n<br>\n<br>\nmetric\uc5d0 \ub300\ud55c \uace0\ubbfc\ub3c4 \uc870\uae08 \ud574\ubcfc \uc218 \uc788\ub2e4. \ud06c\uac8c\ub294 \uc815\ud655\ub3c4\ub97c \uc0b4\ud3b4\ubcfc \uc218 \uc788\uaca0\uc9c0\ub9cc \uc870\uae08 \ub354 \uad6c\uccb4\uc801\uc73c\ub85c \ub4e4\uc5b4\uac00\uc57c \ud55c\ub2e4.\n<br>\n\uc6b0\ub9ac\uac00 \uc6d0\ud558\ub294 \uacb0\uacfc\ub294 \ucd5c\uc885\uc801\uc73c\ub85c \uc5bc\ub9c8\ub098 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud588\ub290\ub0d0\uac00 \uc544\ub2c8\ub77c, \uc774 \uc608\uce21\uc774 \"\uc5bc\ub9c8\ub098 \ud22c\uc790 \uc218\uc775\uc744 \uadf9\ub300\ud654\uc2dc\ud0ac \uc218 \uc788\ub290\ub0d0\"\uc774\ub2e4.\n<br>\n\ub530\ub77c\uc11c \uc11c\ub85c \ub2e4\ub978 code (\uc885\ubaa9) \uac04\uc758 \ud2b9\uc131\uae4c\uc9c0 \uace0\ub824\ud558\uc5ec, \ud559\uc2b5\uc5d0 \ubc18\uc601\uc744 \ud558\ub4e0\uc9c0 \uc544\ub2c8\uba74 \ucd5c\uc885 \ubaa8\ub378 \uc120\ud0dd\uc5d0 \uc788\uc5b4\uc11c \ubc18\uc601\ud560 \uc218\ub3c4 \uc788\ub2e4.","d0f9e844":"![valid1](https:\/\/user-images.githubusercontent.com\/40786348\/73640980-97048400-46b2-11ea-8f81-b0d606dd916e.png)\n\ucd9c\ucc98: Predicting the direction of stock market prices using random forest (https:\/\/arxiv.org\/abs\/1605.00003v1)","78a5bf66":"\uc9c0\uae08\uae4c\uc9c0 \uc8fc\uc2dd \ub370\uc774\ud130\uc14b\uc744 \ud65c\uc6a9\ud558\uc5ec \ub2e4\uc74c \uc2dc\uac04 \ub2e8\uc704\uc758 \uc218\uc775\ub960\uc744 \uc608\uce21\ud558\ub294 \ubaa8\ub378\ub9c1 \uacfc\uc815\uc744 \uc0b4\ud3b4\ubcf4\uc558\ub2e4. \ucd1d \ud559\uc2b5 \uc2dc\uac04\uc740 1\uc2dc\uac04\ub3c4 \uc548\uac78\ub9b4 \uc815\ub3c4\ub85c \ub9e4\uc6b0 \uc9e7\ub2e4.\n<br>\n\ubcf8\uc778\uc774 \uc774\ubbf8\uc9c0, \uc790\uc5f0\uc5b4, \uc74c\uc131 \ub4f1\uacfc \uac19\uc740 \ud0c0 \ubb38\uc81c\ub97c \uae30\ubc18\uc73c\ub85c AI \ubaa8\ub378\ub9c1\uc744 \ubaa8\ub450 \uc218\ud589\ud574 \ubcf8 \uacb0\uacfc, \ubaa8\ub4e0 \ubd80\ubd84\uc5d0\uc11c \uc815\uad50\ud568\uc774 \uc694\uad6c\ub418\uae30 \ub54c\ubb38\uc5d0 \uc8fc\uac00 \uc608\uce21 \ubaa8\ub378\ub9c1\uc774 \uadf8 \uc911\uc5d0\uc11c \uac00\uc7a5 \uc5b4\ub824\uc6b4 \ucd95\uc5d0 \uc18d\ud558\uc9c0 \uc54a\uc744\uae4c \ud558\ub294 \uc0dd\uac01\uc774 \ub4e0\ub2e4.\n<br> \n\uac00\uc7a5 \uc544\uc26c\uc6b4 \uc810\uc774 \ubaa8\ub4e0 \uc885\ubaa9\uc744 \uae30\ubc18\uc73c\ub85c \ud559\uc2b5\uc744 \ud574\ubcf4\uc9c0 \ubabb\ud588\ub2e4\ub294 \uac83\uc774\uba70, \uadf8 \ub2e4\uc74c\uc740 RNN \uae30\ubc18\uc758 \uc774\uc9c4\ubd84\ub958 \ubaa8\ub378\uc744 \uc2dc\ub3c4\ud574\ubcf4\uc9c0 \ubabb\ud588\ub2e4\ub294 \uac83\uc774\ub2e4.\n<br>\n\uae30\ud68c\uac00 \uc8fc\uc5b4\uc9c4\ub2e4\uba74 encoder\/decoder, attention \uae30\ubc18\uc758 \ubaa8\ub378 \uad6c\uc870, GAN\uc744 \ud65c\uc6a9\ud55c unsupervised \uae30\ubc18\uc758 \ubaa8\ub378\ub9c1 \ub4f1 \ub2e4\uc591\ud55c \uad00\uc810\uc5d0\uc11c \ubb38\uc81c\ub97c \ud574\uacb0\ud574\ubcf4\uace0 \uc2f6\ub2e4.","2d8535b9":"# Contents\n### 1.  [Introduction](#1.-Introduction)\n### 2.  [Import Libraries](#2.-Import-Libraries)\n### 3.  [Helper Functions](#3.-Helper-Functions)\n### 4.  [Load Dataset](#4.-Load-Dataset)\n### 5.  [Dataset Preprocess](#5.-Dataset-Preprocess)\n### 6.  [Validation Strategy](#6.-Validation-Strategy)\n### 7.  [Tree based Model](#7.-Tree-based-Model)\n### 8.  [MLP based Model](#8.-MLP-based-Model)\n### 9.  [RNN based Model](#9.-RNN-based-Model)","7d9cbb39":"# 2. Import Libraries","70bc940b":"### 1. Simple Random Hold-out-set","d481adf9":"# 8. MLP based Model","93aec49d":"# Conclusion","462e5381":"![validation](https:\/\/user-images.githubusercontent.com\/40786348\/73620023-58030e00-4673-11ea-8f95-e51503d3174f.png)\n\ucd9c\ucc98: Predicting the direction of stock market prices using random forest (https:\/\/arxiv.org\/abs\/1605.00003v1)","2efe8b9c":"# Helper Functions","36345111":"\ubcf5\uc7a1\ud55c \uad6c\uc870\uc758 \ubaa8\ub378\uc774 \uc544\ub2cc RNN layer \ub450 \uac1c\uc640 fc layer \ud55c \uac1c\ub9cc\uc744 \ud65c\uc6a9 \ud588\uc74c\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \uc798 \ud559\uc2b5\uc774 \ub41c\ub2e4.\n<br>\n\ubaa8\ub4e0 \uc885\ubaa9\uc744 \ud65c\uc6a9\ud560 \uacbd\uc6b0, \uac04\ub2e8\ud788 \uc0dd\uac01\ud574 \ubcfc \uc218 \uc788\ub294 \uac83\uc740 \uc720\uc0ac\ud55c \uc885\ubaa9\ub07c\ub9ac \uad70\uc9d1\ud654\ud558\uc5ec \ud559\uc2b5\uc744 \uc9c4\ud589\ud558\ub294 \uac83\uc774\ub2e4.","3d152d89":"![111111](https:\/\/user-images.githubusercontent.com\/40786348\/73930754-6c1c6900-491a-11ea-8e6e-bf38c6ba9e02.png)\n","513fbc02":"**2. Last Time Hold-Out-Set**","f9b8fb04":"# 1. Introduction","31fd7b5e":"### 2. Last Time Hold-Out-Set","764956be":"\uc704\uc640 \uac19\uc740 \ubc29\ubc95\uc73c\ub85c \ubcf4\ub2e4 \ub354 \uc815\ud655\ud558\uac8c test set\uc744 \uc608\uce21\ud560 \uc218 \uc788\uc744 \uac83\uc774\ub2e4.\n<br>\n\ub354 \ub9ce\uc740 \ubaa8\ub378\uc744 \uc0dd\uc131\ud558\uc5ec \ub2e4\uc591\ud55c \uc2dc\uac04 \ub2e8\uc704\ub97c \uae30\ubc18\uc73c\ub85c validation \ud55c\ub2e4\uba74 test set \uc608\uce21 \uc131\ub2a5\uc774 \uc880 \ub354 robust\ud574\uc9c8 \uac83\uc774\ub2e4.\n<br>\n\ubb3c\ub860 \uc815\ud655\ub3c4\uc640 \ubaa8\ub378 \ud559\uc2b5 \uc2dc\uac04\uc740 trade-off \uad00\uacc4\uc5d0 \uc788\uc73c\ub2c8 \uc774 \ubd80\ubd84\uc744 \uc798 \uace0\ub824\ud558\uc5ec \ubaa8\ub378\ub9c1\uc744 \ud55c\ub2e4\uba74 \uc88b\uc73c\ub9ac\ub77c \uc0dd\uac01\ud55c\ub2e4.\n<br>","d4a046bc":"\uc2dc\uacc4\uc5f4 \ud2b9\uc131\uc744 \uace0\ub824\ud588\uc744\uc2dc Validation Split \ubc29\ubc95 \uc911\uc5d0\uc11c \uac00\uc7a5 \uac04\ub2e8\ud55c \ubc29\ubc95\uc774\ub2e4.\n<br>\ntrain set\uc744 \uc2dc\uac04\uc73c\ub85c \uc815\ub82c\ud558\uc5ec \ub9c8\uc9c0\ub9c9 \ubd80\ubd84\uc744 \ub5bc\uc5b4\ub0b4 validation set\uc73c\ub85c \ud65c\uc6a9\ud560 \uc218 \uc788\ub2e4.","7c052908":"baseline\uc774\uae30 \ub54c\ubb38\uc5d0 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud558\ub294\ub370\ub294 \ubb34\ub9ac\uac00 \uc788\ub2e4. \ub354 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \uc704\ud574\uc11c\ub294 \uc815\uad50\ud55c hyper parameter \ud29c\ub2dd\uacfc \ud30c\uc0dd\ubcc0\uc218 \uc0dd\uc131\uc774 \uc694\uad6c\ub420 \uac83\uc73c\ub85c \ubcf4\uc778\ub2e4.\n<br>\n\uc9c0\uae08\ubd80\ud130\ub294 \ub525\ub7ec\ub2dd \uae30\ubc18\uc758 \ubaa8\ub378\ub9c1\uc744 \uc218\ud589\ud574\ubcf4\uc790.","34a8aba9":"# 4. Load Dataset","c116a641":"# 6. Validation Strategy","5dc73101":"## 3. Sliding Window","7fb15fcf":"\uc704\uc5d0\uc11c \uc0b4\ud3b4\ubcf8 \uac80\uc99d \uc804\ub7b5\ub4e4\uc744 tree \uae30\ubc18 \ubaa8\ub378\uc5d0 \uc801\uc6a9\uc2dc\ucf1c\ubcf4\uc790.","a36f075a":"\uc9c0\uae08\uae4c\uc9c0\ub294 \ud2b8\ub9ac \uae30\ubc18\uc758 \ubaa8\ub378\uacfc MLP \uae30\ubc18\uc758 \ubaa8\ub378\ub9c1\uc744 \uc0b4\ud3b4\ubd24\ub2e4\uba74, \uc774\ubc88 section\uc5d0\uc11c\ub294 RNN\uc744 \uc0b4\ud3b4\ubcf4\uace0\uc790 \ud55c\ub2e4.\n<br>\nRNN\uc740 \ud68c\uadc0 \ubb38\uc81c\ub85c \uc811\uadfc\ud558\uc5ec \uc704 \uc774\ubbf8\uc9c0\uc758 \uac00\uc7a5 \uc6b0\uce21\uc5d0 \ud574\ub2f9\ud558\ub294 many-to-many \uad6c\uc870\uc758 RNN\uc744 \ucc44\ud0dd\ud558\uc600\ub2e4. \n<br>\n\ud68c\uadc0 \ubb38\uc81c\ub85c \uc811\uadfc\ud588\uc744 \ub54c\uc758 \uc774\uc810\uc740 \uc608\uce21 \uacb0\uacfc\ub97c \uc2dc\uac01\ud654\ud560 \uc218 \uc788\ub2e4\ub294 \uc810\uc774\ub2e4.\n<br>\n\uc774\ub97c \ud1b5\ud574 \ud559\uc2b5\uc774 \uc815\uc0c1\uc801\uc73c\ub85c \uc9c4\ud589\ub418\uace0 \uc788\ub294\uc9c0 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \uc798 \uc124\uacc4\ub41c \ubaa8\ub378 \uad6c\uc870\uc640 \uc798 \uc0dd\uc131\ub41c \ud53c\uccd0\ub4e4\uc740 \ud68c\uadc0\/\ubd84\ub958 \ubb38\uc81c \uc0c1\uad00\uc5c6\uc774 \uc131\ub2a5\uc774 \uc88b\uac8c \ub098\uc624\uae30 \ub9c8\ub828\uc774\ub2e4.\n<br>\n<br>\n\ub610\ud55c \uc624\uc9c1 \ud558\ub098\uc758 \uc885\ubaa9\uc744 \uae30\ubc18\uc73c\ub85c \ud559\uc2b5\ud558\ub294 \ucf54\ub4dc\ub97c \uc791\uc131\ud558\uace0\uc790 \ud55c\ub2e4. \uc548\ud0c0\uae5d\uac8c\ub3c4 \uc81c\ucd9c \uae30\ud55c\uc774 \uc788\uc5b4 \ubaa8\ub4e0 \uc885\ubaa9\uc744 \ud65c\uc6a9\ud558\uc5ec \ud559\uc2b5\ud558\ub294 \uc644\uc131\ub41c \ucf54\ub4dc\ub97c \uc791\uc131\ud558\uc9c0\ub294 \ubabb\ud588\ub2e4.\n<br>\n\uc544\ub798 \ucf54\ub4dc\ub97c \ud1b5\ud574 \ud559\uc2b5 \uc9c4\ud589 \uacfc\uc815\uc5d0 \ub530\ub77c \uc608\uce21\uac12\uc774 \uc5b4\ub5a4\uc2dd\uc73c\ub85c \ubcc0\ud654\ud558\ub294\uc9c0 \ud655\uc778\ud574\ubcf4\uc790.","18e86958":"\ud2b8\ub9ac \ubaa8\ub378\uc758 \uc7a5\uc810\uc740 feature importance\ub97c \uc2dc\uac01\ud654\ud568\uc73c\ub85c\uc368 \uc5b4\ub5a4 \ud53c\uccd0\uac00 \ubaa8\ub378 \uc608\uce21\uc5d0 \uc788\uc5b4\uc11c \uc911\uc694\ud558\uac8c \uc791\uc6a9\ud588\ub294\uc9c0 \ub208\uc73c\ub85c \ud30c\uc545\ud560 \uc218 \uc788\ub2e4\ub294 \uc810\uc774\ub2e4.\n<br>\n\ucd94\ud6c4 \ud30c\uc0dd\ubcc0\uc218 \uc0dd\uc131\uc744 \ud1b5\ud558\uc5ec tree \uae30\ubc18\uc758 \ubaa8\ub378\ub9c1\uc744 \uc218\ud589\ud560 \uacbd\uc6b0 \uc774 \uc810\uc744 \ucc38\uace0\ud558\uc5ec feature engineering\uc744 \uc9c4\ud589\ud560 \uc218 \uc788\ub2e4.","a23a0310":"# 9. RNN based Model","514283c3":"**3. Sliding Window**","85ca4da8":"\uadf8\ub7fc \uc9c0\uae08\ubd80\ud130\ub294 \ub274\ub7f4\ub137 \uae30\ubc18\uc758 \ubaa8\ub378\ub9c1\uc744 \uc9c4\ud589\ud574\ubcf4\uace0\uc790 \ud55c\ub2e4.\n<br>\n\uc55e\uc120 EDA\uc5d0\uc11c \uc0b4\ud3b4\ubd24\ub4ef\uc774, \ub3c5\ub9bd\ubcc0\uc218\uc640 \uc885\uc18d\ubcc0\uc218 \uac04\uc5d0 \uc0c1\uad00\uad00\uacc4\uac00 \uc804\ud600 \uc874\uc7ac\ud558\uc9c0 \uc54a\ub294 \ub9e4\uc6b0 \uc5b4\ub824\uc6b4 \ubb38\uc81c\uc774\ub2e4.\n<br>\n\ub530\ub77c\uc11c \ub2e4\uc218\uc758 acitvation \ud568\uc218 \uc313\uc544 \ud559\uc2b5\uc744 \ud55c\ub2e4\uba74 \ub354 \ub098\uc740 \uc608\uce21\uc744 \ud560 \uc218 \uc788\uc73c\ub9ac\ub77c \uc0dd\uac01\ud55c\ub2e4.\n<br>\n\uccab \ubc88\uc9f8 \ubaa8\ub378\uc740, **MLP(multi-layer-perceptron)** \uae30\ubc18\uc758 \ubaa8\ub378\uc774\uba70, \ub450 \ubc88\uc9f8 \ubaa8\ub378\uc740 **RNN** \uae30\ubc18\uc758 \ubaa8\ub378\ub9c1\uc774\ub2e4.","0285951a":"**1. Simple Random Hold-out-set**","6cc648a7":"![stock](https:\/\/user-images.githubusercontent.com\/40786348\/73641895-0e86e300-46b4-11ea-9b36-f651a549f28f.png)\n\n[paper link](https:\/\/arxiv.org\/abs\/1605.00003v1)\n<br>\n\uc704 \ub17c\ubb38\uc5d0 \uc758\ud558\uba74 \uc5f0\uc18d\uac12\uc744 \uc608\uce21\ud558\ub294 regression \ubb38\uc81c\ubcf4\ub2e4, \ubd84\ub958 \ubb38\uc81c\ub85c \uc811\uadfc\ud588\uc744 \uc2dc\uc5d0 \ub354 \ub098\uc740 performance\ub97c \ubcf4\uc600\uae30 \ub54c\ubb38\uc5d0 \ud544\uc790\ub3c4 **\uc774\uc9c4 \ubd84\ub958 \ubb38\uc81c**\ub85c \uc811\uadfc\ud558\uace0\uc790 \ud55c\ub2e4.\n\ubaa8\ub378\uc5d0\uc11c \ub098\uc628 \uc608\uce21 \uacb0\uacfc\ub294 \uc8fc\uc2dd \uc2dc\uc7a5\uc5d0\uc11c \ud22c\uc790\ub97c \ud558\ub294 \uc758\uc0ac \uacb0\uc815\uad8c\uc790\ub97c support \ud560 \uc218 \uc788\uae30\ub97c \uae30\ub300\ud55c\ub2e4.\n<br>\n<br>\n\uc704 \ub17c\ubb38\uc5d0\uc11c\ub294 \uc608\uce21\uc744 \uc704\ud55c \uc9c0\ud45c\ub85c\uc11c \ub9ce\uc774 \ud65c\uc6a9\ub418\ub294 Relative Strength Index, Stochastic Oscillator\uc640 \uac19\uc740 \n<br>\nTechinical Indicators\ub4e4\uc744 exponential smoothing\uc744 \ud1b5\ud574 \uc804\ucc98\ub9ac\ub97c \ud558\uc600\uace0 Random Forest \ubaa8\ub378\uc744 \ud65c\uc6a9\ud558\uc5ec \uc608\uce21\uc744 \ud558\uc600\ub2e4.\n<br>\n\ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ud55c Random Forest\ub294 Bootstrapping\uc744 \ud1b5\ud55c \uc624\ubc84\ud53c\ud305\uc744 \ubc29\uc9c0\ud55c\ub2e4\ub294 \uc7a5\uc810\uc774 \uc788\uc9c0\ub9cc \ud559\uc2b5\uc774 \uc624\ub798\uac78\ub9ac\uae30 \ub54c\ubb38\uc5d0 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc558\ub2e4.\n<br>\n\ub300\uc2e0 \ub2e4\uc218\uc758 \uacbd\uc9c4\ub300\ud68c\uc5d0\uc11c \ube44\uad50\uc801 \ube60\ub978 \uc2dc\uac04\uc5d0 \ub192\uc740 \uc131\ub2a5\uc744 \uc790\ub791\ud558\ub294 **[Light GBM \ubaa8\ub378](https:\/\/lightgbm.readthedocs.io\/en\/latest\/)**\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud559\uc2b5\uc744 \uc9c4\ud589\ud558\uc600\ub2e4.\n\ub610\ud55c \ub2e4\uc218\uc758 \ud53c\uccd0\ub4e4\uc774 \ube44\uc2dd\ubcc4\ud654\ub418\uc5b4 \uc788\uae30 \ub54c\ubb38\uc5d0, validation \uc131\ub2a5\uc744 \uae30\uc900\uc73c\ub85c \ud30c\uc0dd \ubcc0\uc218\ub97c \uc0dd\uc131\ud558\uc5ec \uc608\uce21 \uc131\ub2a5\uc744 \ub192\uc774\uace0\uc790 \ud588\ub2e4.\n","1fda9104":"\ub2e8\uc21c\ud558\uac8c \ub370\uc774\ud130\uc14b\uc758 \uc77c\ubd80\ub97c validation set\uc73c\ub85c \ud65c\uc6a9\ud558\ub294 \ubc29\ubc95\uc774\ub2e4. \uc2dc\uacc4\uc5f4\uc744 \ubb34\uc2dc\ud558\uc9c0\ub9cc \ucc98\uc74c \ubaa8\ub378\ub9c1\uc744 \uc2dc\uc791\ud558\uae30 \uc804\uc5d0 baseline\uc73c\ub85c \uc7a1\uae30 \uc88b\ub2e4.","9fa956e2":"- \uc774\ubc88 \ub178\ud2b8\ubd81\uc744 \ud1b5\ud574\uc11c\ub294 \uc2dc\uacc4\uc5f4 \ub370\uc774\ud130\ub97c \uc5b4\ub5bb\uac8c \ubaa8\ub378\ub9c1\ud560 \uc218 \uc788\ub294\uac00\uc5d0 \ub300\ud574\uc11c \uc0b4\ud3b4\ubcf4\uace0\uc790 \ud55c\ub2e4.\n<br>\n- \ub178\ud2b8\ubd81\uc744 \ud1b5\ud574 tree \uacc4\uc5f4\uc758 \ubaa8\ub378\ub9c1\ubfd0\ub9cc \uc544\ub2c8\ub77c MLP, RNN\uacfc \uac19\uc740 \ub525\ub7ec\ub2dd \uacc4\uc5f4\uc758 \ubaa8\ub378\ub9c1 \ud30c\uc774\ud504\ub77c\uc778\uc744 \uc791\uc131\ud558\uc600\ub2e4.\n<br>\n- \ub370\uc774\ud130 \uc804\ucc98\ub9ac, \ud30c\uc0dd \ubcc0\uc218 \uc0dd\uc131, \uac80\uc99d \uc804\ub7b5\ubd80\ud130 \ubaa8\ub378\ub9c1\uae4c\uc9c0 \uc21c\ucc28\uc801\uc73c\ub85c \uc791\uc131\ud558\uc600\uace0 \uc608\uce21 \uacb0\uacfc\ub97c \uc9c1\uc811 \ub208\uc73c\ub85c \ud655\uc778\ud560 \uc218 \uc788\ub3c4\ub85d \ud588\uc2b5\ub2c8\ub2e4.\n<br>\n- tree \uacc4\uc5f4\uacfc mlp \uae30\ubc18\uc758 \ubaa8\ub378\ub9c1\uc740 classification\uc73c\ub85c, RNN\uc740 regression \ubb38\uc81c\ub85c \ucf54\ub4dc\ub97c \uc791\uc131\ud558\uc600\ub2e4.","682905a0":"![haha](https:\/\/user-images.githubusercontent.com\/40786348\/73935061-15675d00-4923-11ea-98a4-74cd8aa81e22.png)\n","1b76a387":"56% \uc815\ub3c4\uc758 \uc815\ud655\ub3c4\ub97c \ub2ec\uc131\ud55c\ub2e4.\n<br>\n\ud558\uc9c0\ub9cc \uc704\uc640 \uac19\uc740 validation \ubc29\ubc95\uc740 \uc633\uc9c0 \ubabb\ud558\ub2e4. \uc6b0\ub9ac\uac00 \uc608\uce21\ud558\uace0\uc790 \ud558\ub294 \ub370\uc774\ud130\ub294 \ubbf8\ub798\uc758 \ud2b9\uc815 \uae30\uac04\uc774\ub2e4.\n<br>\n\ub530\ub77c\uc11c \uc774 \ud2b9\uc815 \uae30\uac04\uc744 \ub300\ubcc0\ud574 \uc904 \uc218 \uc788\ub294 validation set\uc744 \uc798 \uace0\ub974\ub294 \uac83\uc774 \uc911\uc694\ud558\ub2e4."}}