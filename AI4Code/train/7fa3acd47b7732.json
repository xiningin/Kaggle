{"cell_type":{"641ab22d":"code","1f2a9aa8":"code","4e9cc584":"code","5f0404f1":"code","b4ef19c3":"code","419b7d01":"code","73289357":"code","b50b8a49":"code","aa4cec19":"code","6f23684a":"code","501205e5":"code","25fc2ad8":"code","e2d088f8":"code","4b59137d":"code","9175469c":"code","4fae7c53":"markdown","03354d92":"markdown","b2be7fcf":"markdown","1f6d7cae":"markdown","667b7afa":"markdown","50a15614":"markdown","16ce1e63":"markdown","3b7a3941":"markdown","36e4f057":"markdown"},"source":{"641ab22d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nimport seaborn as sns\nimport sklearn\nimport imblearn\nimport matplotlib.pyplot as plt\nimport time\nimport sklearn.metrics as m\nimport xgboost as xgb\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Settings\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n#Probably can`t be finished because of huge amount of data with kaggle hardware, add nrows parameter to run here\n#Load Data\n\ncols = [' Bwd Packet Length Std',' PSH Flag Count',' min_seg_size_forward',' Min Packet Length',' ACK Flag Count',' Bwd Packet Length Min',' Fwd IAT Std','Init_Win_bytes_forward',' Flow IAT Max',' Bwd Packets\/s',' URG Flag Count','Bwd IAT Total',' Label']\ndf1=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\", usecols = cols)#,nrows = 50000\ndf2=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\", usecols = cols)\ndf3=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Friday-WorkingHours-Morning.pcap_ISCX.csv\", usecols = cols)\ndf5=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\", usecols = cols)\ndf6=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\", usecols = cols)\n\n# df4, df7 and df8 are being left out as they only have the benign samples","1f2a9aa8":"df = pd.concat([df1,df2])\ndel df1,df2\ndf = pd.concat([df,df3])\ndel df3\ndf = pd.concat([df,df5])\ndel df5\ndf = pd.concat([df,df6])\ndel df6\n\ndata = df.copy()\n\nfor column in data.columns:\n    if data[column].dtype == np.int64:\n        maxVal = data[column].max()\n        if maxVal < 120:\n            data[column] = data[column].astype(np.int8)\n        elif maxVal < 32767:\n            data[column] = data[column].astype(np.int16)\n        else:\n            data[column] = data[column].astype(np.int32)\n            \n    if data[column].dtype == np.float64:\n        maxVal = data[column].max()\n        minVal = data[data[column]>0][column]\n        if maxVal < 120 and minVal>0.01 :\n            data[column] = data[column].astype(np.float16)\n        else:\n            data[column] = data[column].astype(np.float32)\n            \n            \n\nattackType = data[' Label'].unique()\ndata[' Label'] = data[' Label'].astype('category')\ndata[' Label'] = data[' Label'].astype(\"category\").cat.codes","4e9cc584":"y = data[' Label'].copy()\nX = data.drop([' Label'],axis=1)","5f0404f1":"from imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler('majority')\nX_rus, y_rus = rus.fit_sample(X, y)","b4ef19c3":"y_rus.value_counts()","419b7d01":"df = X_rus\ndf[' Label'] = y_rus\nminor = pd.DataFrame(df[(df[' Label']!=4) & (df[' Label']!=2)])\nmajor = pd.DataFrame(df[(df[' Label']==4) | (df[' Label']==2)])\nminor[' Label'].value_counts()","73289357":"from imblearn.over_sampling import SMOTE\ny_rus_ =  minor[' Label']\nX_rus_ =  minor.drop([' Label'],axis=1)\nstrategy = {1:2000, 5:1600, 7:800, 3:300, 6:200, 0:200}\nsm = SMOTE(sampling_strategy=strategy)\nX_sm, y_sm = sm.fit_sample(X_rus_, y_rus_)\nX_min,y_min = X_sm, y_sm ","b50b8a49":"major[' Label'].value_counts()","aa4cec19":"from imblearn.under_sampling import RandomUnderSampler\ny_rus_ =  major[' Label']\nX_rus_ =  major.drop([' Label'],axis=1)\nstrategy = {4:10000, 2:6000}\ntom = RandomUnderSampler(sampling_strategy=strategy)\nX_tom, y_tom = tom.fit_sample(X_rus_, y_rus_)\ny_tom.value_counts()","6f23684a":"X_maj,y_maj = X_tom, y_tom\nX,y = pd.concat([X_maj,X_min]), pd.concat([y_maj,y_min])\nX.info()","501205e5":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# extract numerical attributes and scale it to have zero mean and unit variance  \ncols = X.select_dtypes(include=['float32','float16','int32','int16','int8']).columns\ntrain_X = scaler.fit_transform(X.select_dtypes(include=['float32','float16','int32','int16','int8']))\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(train_X,y,train_size=0.70, random_state=2)\n\n\nfrom sklearn import tree\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n# Train Random Forest\nRFC_Classifier = RandomForestClassifier(max_depth=40)\nRFC_Classifier.fit(X_train, Y_train)\nprint ('RF Classifier run')\n\n# Train SVC\nSVM_Classifier = SVC()\nSVM_Classifier.fit(X_train, Y_train)\nprint ('SV Classifier run')\n# Train Decision Tree Model\nDTC_Classifier = tree.DecisionTreeClassifier(criterion='gini', max_depth=33, random_state=20, max_features=12, splitter='random')\nDTC_Classifier.fit(X_train, Y_train)\nprint ('DTC Classifier run')","25fc2ad8":"from sklearn import metrics\n\nmodels = []\nmodels.append(('Random Forest Classifier', RFC_Classifier))\nmodels.append(('Decision Tree Classifier', DTC_Classifier))\nmodels.append(('Support Vector Classifier',SVM_Classifier))\n\n\nfor i, v in models:\n    Xpred =  v.predict(X_train)\n    scores = cross_val_score(v, X_train, Y_train, cv=10)\n    accuracy = metrics.accuracy_score(Y_train, Xpred)\n    confusion_matrix = metrics.confusion_matrix(Y_train, Xpred)\n    classification = metrics.classification_report(Y_train, Xpred)\n    print()\n    print('============================== {} Model Evaluation =============================='.format(i))\n    print()\n    print (\"Cross Validation Mean Score:\" \"\\n\", scores.mean())\n    print()\n    print (\"Model Accuracy:\" \"\\n\", accuracy)\n    print()\n    print(\"Confusion matrix:\" \"\\n\", confusion_matrix)\n    print()\n    print(\"Classification report:\" \"\\n\", classification) \n    print()","e2d088f8":"for i, v in models:\n    pred = v.predict(X_test)\n    accuracy = metrics.accuracy_score(Y_test,pred)\n    confusion_matrix = metrics.confusion_matrix(Y_test, pred)\n    classification = metrics.classification_report(Y_test, pred)\n    print()\n    print('============================== {} Model Test Results =============================='.format(i))\n    print()\n    print (\"Model Accuracy:\" \"\\n\", accuracy)\n    print()\n    print(\"Confusion matrix:\" \"\\n\", confusion_matrix)\n    print()\n    print(\"Classification report:\" \"\\n\", classification) \n    print()    ","4b59137d":"from sklearn.ensemble import VotingClassifier\n\nclf1 = tree.DecisionTreeClassifier(criterion='gini', max_depth=33, random_state=20, max_features=12, splitter='random')\nclf2 = RandomForestClassifier(criterion='gini', max_depth=40, random_state=20)\nclf3 = SVC()\n\nvotingC = VotingClassifier(estimators=[('dc',clf1), ('rf', clf2),('svc',clf3)],voting='hard', weights=[2,2,1],flatten_transform=True)\nvotingC.fit(X_train,Y_train)","9175469c":"pred = votingC.predict(X_test)\naccuracy = metrics.accuracy_score(Y_test,pred)\nconfusion_matrix = metrics.confusion_matrix(Y_test, pred)\nclassification = metrics.classification_report(Y_test, pred)\nprint()\nprint('============================== {} Model Test Results =============================='.format('Voting Classifier'))\nprint()\nprint (\"Model Accuracy:\" \"\\n\", accuracy)\nprint()\nprint(\"Confusion matrix:\" \"\\n\", confusion_matrix)\nprint()\nprint(\"Classification report:\" \"\\n\", classification) \nprint()    ","4fae7c53":"Since this data is very imbalanced, F1-score is given more priority over accuracy of the model. This model has been made to optimize the f1-macro score of the model. Steps are as following - \n1. Data Loading\n2. Data Preprocessing\n3. Balancing Imbalanced Dataset\n4. Machine Learning Models\n5. Ensemble Model ","03354d92":"# Hence we have been able to achive an accuracy of 96% and F1-score of 89%. ","b2be7fcf":"## Balancing The Imbalanced Data","1f6d7cae":"## Ensemble Model","667b7afa":"## Data Preprocessing","50a15614":"# CICIDS ML PipeLine - 90% F1-score","16ce1e63":"DecisionTreeClassifier(max_depth=35, random_state=10, splitter='random') - 0.85\n\nDecisionTreeClassifier(max_depth=33, random_state=20, splitter='random') - 0.88","3b7a3941":"## Data Loading","36e4f057":"## Machine Learning Models"}}