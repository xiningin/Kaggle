{"cell_type":{"f8d33fe6":"code","a77eb3e2":"code","9723c5ca":"code","a2f33297":"code","32f5698d":"code","906ab5d6":"code","d206ff2a":"code","34bfd3c8":"code","599bdf97":"code","314ffefc":"code","443f5911":"code","0f164bc8":"code","c48bb71a":"code","ca228d01":"code","cceb4f69":"code","d3db2ccf":"code","f855608c":"code","e56703e2":"code","b91fa3b8":"code","d41b175d":"code","1bf0fb6d":"code","1f909450":"code","b00682e7":"code","f27bf61f":"code","656b9f4f":"code","7348e15e":"code","c32a9fc5":"code","30d781c5":"code","8dfa93d6":"code","f0740cc7":"code","13ba6875":"code","60637ebe":"code","f3d29add":"code","a31bb881":"code","3fd37168":"code","8ba9bf48":"code","11fa9450":"code","b002b27a":"code","9af9ce60":"markdown","bb1f1561":"markdown","9037202d":"markdown","61d70dd1":"markdown","98e0d4f2":"markdown","b6325923":"markdown","69dbe339":"markdown","cfb747f7":"markdown","cc4375a1":"markdown","efffd271":"markdown","2f7ac1f8":"markdown","b6c1e418":"markdown","f3f8f2d2":"markdown","462ca9f2":"markdown","23499f91":"markdown","808c48fd":"markdown","5f63cb26":"markdown","b1afdc8f":"markdown","9f165568":"markdown","b4850702":"markdown","bafe37df":"markdown","f52b7026":"markdown","95548dc7":"markdown","4f24a9f9":"markdown","68ae1a59":"markdown","b3ba6f65":"markdown","a0cb51d8":"markdown","5dd8935d":"markdown","7e8af59b":"markdown","b54a1723":"markdown","1de9a93d":"markdown","62aaa6be":"markdown","9ca195ae":"markdown","ad0ff88c":"markdown","88a2ad38":"markdown","a3996498":"markdown"},"source":{"f8d33fe6":"%%time\n\nimport os, psutil\nimport gc\n\nimport numpy as np \nimport pandas as pd \nfrom statsmodels.graphics.mosaicplot import mosaic\nfrom scipy.stats import randint\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn import ensemble, linear_model,metrics,model_selection,neighbors,preprocessing, svm, tree\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import (AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier,GradientBoostingClassifier,RandomForestClassifier,VotingClassifier)\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier, LogisticRegression, PassiveAggressiveClassifier,RidgeClassifierCV\nfrom sklearn.metrics import classification_report, accuracy_score, log_loss, roc_auc_score\nfrom sklearn.model_selection import cross_validate,cross_val_score, GridSearchCV,KFold,train_test_split\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import KBinsDiscretizer, OrdinalEncoder, OneHotEncoder,MinMaxScaler,StandardScaler, RobustScaler\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import ensemble, linear_model,neighbors, svm, tree\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a77eb3e2":"%%time\n# taken from https:\/\/www.kaggle.com\/ryanholbrook\/getting-started-september-2021-tabular-playground\n\ndef cpu_stats():\n    pid = os.getpid()\n    py = psutil.Process(pid)\n    memory_use = py.memory_info()[0] \/ 2. ** 30\n    return 'memory GB:' + str(np.round(memory_use, 2))\n\ndef score(X, y, model, cv):\n    scoring = [\"roc_auc\"]\n    scores = cross_validate(\n        model, X_train, y_train, scoring=scoring, cv=cv, return_train_score=True\n    )\n    scores = pd.DataFrame(scores).T\n    return scores.assign(\n        mean = lambda x: x.mean(axis=1),\n        std = lambda x: x.std(axis=1),\n    )\n\n## from: https:\/\/www.kaggle.com\/bextuychiev\/how-to-work-w-million-row-datasets-like-a-pro\ndef reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df\n\nprint('Function built')","9723c5ca":"%%time\n# Get data\ntrain=pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv')\ntest=pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')\nprint(\"Data imported\")\n\ntrain = reduce_memory_usage(train, verbose=True)\ntest = reduce_memory_usage(test, verbose=True)\nprint(cpu_stats())\nprint('Memory reduced')\n\nplt.pie([len(train), len(test)], \n        labels=['train', 'test'],\n        colors=['skyblue', 'blue'],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\nplt.show()","a2f33297":"train=train.drop(columns=['f34', 'f96', 'f107','f151', 'f152','f154', 'f155', 'f159', 'f160', 'f162', 'f166', 'f168', 'f170'])\ntest=test.drop(columns=['f34', 'f96', 'f107','f151', 'f152','f154', 'f155', 'f159', 'f160', 'f162', 'f166', 'f168', 'f170'])\nprint('columns dropped')","32f5698d":"%%time\nfeatures=[]\ncat_features=[]\ncont_features=[]\nfor feature in test.columns:\n    features.append(feature)\n    if test.dtypes[feature]=='int8':\n        cat_features.append(feature)\n    if test.dtypes[feature]=='float16':\n        cont_features.append(feature)\n    #print(test.dtypes[feature])\nprint('features obtained')\n\nplt.pie([len(cat_features), len(cont_features)], \n        labels=['Categorical', 'Continuous'],\n        colors=['skyblue', 'blue'],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\nplt.show()\n\n# get list of columns with high correlations\ncorr = train[cont_features].corr().abs()\nhigh_corr=np.where(corr>0.02)\nhigh_corr=[(corr.columns[x],corr.columns[y]) for x,y in zip(*high_corr) if x!=y and x<y]\nprint('initial correlations calculated')\n#print(\"high correlation \\n\",high_corr)\nhigh_corr_features=[]\nfor x in high_corr:\n    for item in x:\n        if item not in high_corr_features:\n            high_corr_features.append(item)\ncorr_matrix = train[high_corr_features].corr()\nprint('high correlations calculated')","906ab5d6":"%%time\n# select scaler (remark out the options you don't want to use)\n#scale = StandardScaler()\n#scale = RobustScaler()\nscale = MinMaxScaler()\ntrain[cont_features]=scale.fit_transform(train[cont_features])\ntest[cont_features]= scale.transform(test[cont_features])  \n\nprint('Data scaled using : ', scale)\nprint(train.f10.head())","d206ff2a":"#converted_features=['f10','f23','f68','f70','f94','f98','f104','f120','f175','f191','f195','f198','f200','f207','f214','f219','f222','f227','f239']\n# convert continuous to categorical\n#add a new column category next to the f10. \ncategory = pd.cut(train.f10,bins=[0,0.1,0.2,0.3,1],labels=['A','B','C','D'])\ntrain.insert(0,'f10_cat',category)\n#add a new column category next to the f10. \ncategory = pd.cut(test.f10,bins=[0,0.1,0.2,0.3,1],labels=['A','B','C','D'])\ntest.insert(0,'f10_cat',category)\n\n#add a new column category next to the f68. \ncategory = pd.cut(train.f68,bins=[0,0.1,0.3,1],labels=['A','B','C'])\ntrain.insert(0,'f68_cat',category)\n#add a new column category next to the f68. \ncategory = pd.cut(test.f68,bins=[0,0.1,0.3,1],labels=['A','B','C'])\ntest.insert(0,'f68_cat',category)\n\n#drop columns\ntrain.drop(['f10','f68'], axis=1, inplace=True)","34bfd3c8":"train.head()","599bdf97":"# convert categorical to binary\ncategorical_features=['f10_cat','f68_cat']\ntrain[categorical_features]=train[categorical_features].fillna('A')\ntest[categorical_features]=test[categorical_features].fillna('A')\n\n# one-hot_encoding\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train[categorical_features]))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(test[categorical_features]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = train.index\nOH_cols_test.index = test.index\n\n# Relabel columns\nall_cols=OH_cols_train.columns\nnew_cols = [i for i in all_cols if isinstance(i, (int, float))]\nOH_cols_train=OH_cols_train[new_cols].add_prefix('new_')\nOH_cols_test=OH_cols_test[new_cols].add_prefix('new_')\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = train.drop(categorical_features, axis=1)\nnum_X_test = test.drop(categorical_features, axis=1)\n\n# Add one-hot encoded columns to numerical features\ntrain= pd.concat([num_X_train, OH_cols_train], axis=1)\ntest = pd.concat([num_X_test, OH_cols_test], axis=1)\n\nprint('data encoded')\nprint(new_cols)\n\n#adjust lists\nremove_features=['f68','f10']\nfor feature in remove_features:\n    cont_features.remove(feature)\nprint('list updated')","314ffefc":"print(train.head())\n","443f5911":"# add std field\n\nif \"Std\" in train.columns:\n    print('Std training feature exists')\nelse:\n    train['std'] = train[cont_features].std(axis=1)\n    print('Std training feature added')\n    \nif \"Std\" in test.columns:\n    print('Std test feature exists')\nelse:\n    test['std'] = test[cont_features].std(axis=1)\n    print('Std test feature added')\n    cont_features.insert(0,'std')\n    \n# add abs_sum field\nif \"abs_sum\" in train.columns:\n    print('Abs_sum training feature exists')\nelse:\n    train['abs_sum'] = train[cont_features].abs().sum(axis=1)\n    print('Abs_sum training feature added')\n    \nif \"abs_sum\" in test.columns:\n    print('Abs_sum test feature exists')\nelse:\n    test['abs_sum'] = test[cont_features].abs().sum(axis=1)\n    print('Abs_sum test feature added')\n    cont_features.insert(0,'abs_sum')","0f164bc8":"# Create an empty dataframe for performance results\nBasicModelPerformanced_df = pd.DataFrame(columns=['Score'])\nprint('Dataframe created')","c48bb71a":"%%time\ntry:\n    y = train.target\n    train.drop(['target'], axis=1, inplace=True)\nexcept:\n    print('target already separated')\nX=train#[cat_features] #slice training here to explore feature reduction\nprint('Target data separated')\n# Create data sets for training (10%) and validation (5%)\nX_train, X_val, y_train, y_val = train_test_split(X, y,train_size=0.1,test_size = 0.05,random_state = 0)\nprint('Model data split')\n\n#plt.pie([len(X_train), len(X_val),len(X-(len(X_train)+len(X_val)))], \n        #labels=['X-train', 'y-train','usused data'],\n        #colors=['skyblue', 'blue','cornflowerblue'],\n        #textprops={'fontsize': 13},\n        #autopct='%1.1f%%')\n#plt.show()","ca228d01":"print(cpu_stats())","cceb4f69":"%%time\n# Adaboost\nmodelname='Adaboost'\n\nadaboost=AdaBoostClassifier()\nadaboost.fit(X_train, y_train)\ny_pred = adaboost.predict(X_val)\nacc_adaboost = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_adaboost)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(adaboost, X_val, y_val)\nplt.title('Confusion matrix for Adaboost model')\nplt.grid(False)\nplt.show()\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_adaboost\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_adaboost})","d3db2ccf":"%%time\n## bagging\nmodelname='Bagging'\nbagging=BaggingClassifier(verbose=0)\nbagging.fit(X_train, y_train)\ny_pred = bagging.predict(X_val)\nacc_bagging = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_bagging)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(bagging, X_val, y_val)\nplt.title('Confusion matrix for Bagging Model')\nplt.grid(False)\nplt.show()\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_bagging\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_bagging})","f855608c":"%%time\n## catboost\nmodelname = \"Catboost\"\ncatboost=CatBoostClassifier(verbose=0)\ncatboost.fit(X_train, y_train)\ny_pred = catboost.predict(X_val)\nacc_catboost = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_catboost)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(catboost, X_val, y_val)\nplt.title('Confusion matrix for catboost model')\nplt.grid(False)\nplt.show()\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_catboost\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_bagging})","e56703e2":"%%time\n#Decision Tree\nmodelname = \"Decision Tree\"\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(X_train, y_train)\ny_pred = decisiontree.predict(X_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(decisiontree, X_val, y_val)\nplt.title('Confusion matrix for decision tree')\nplt.grid(False)\nplt.show()\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_decisiontree\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_decisiontree})","b91fa3b8":"%%time\n# ExtraTreesClassifier\nmodelname =\"Extra Trees\"\net = ExtraTreesClassifier()\net.fit(X_train, y_train)\ny_pred = et.predict(X_val)\nacc_et = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_et)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(et, X_val, y_val)\nplt.title('Confusion matrix for Extra trees')\nplt.grid(False)\nplt.show()\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_et\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_et})","d41b175d":"%%time\n# Gaussian Naive Bayes\nmodelname =\"Gaussian Naive Bayes\"\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ny_pred = gaussian.predict(X_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(gaussian, X_val, y_val)\nplt.title('Confusion matrix for gaussian')\nplt.grid(False)\nplt.show()\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_gaussian\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_gaussian})","1bf0fb6d":"%%time\n# Gradient Boosting Classifier\nmodelname =\"Gradient Boosting\"\ngbk = GradientBoostingClassifier()\ngbk.fit(X_train, y_train)\ny_pred = gbk.predict(X_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(gbk, X_val, y_val)\nplt.title('Confusion matrix for gradient boosting')\nplt.grid(False)\nplt.show()\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_gbk\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_gbk})","1f909450":"%%time\n# KNN or k-Nearest Neighbors\nmodelname =\"K-nearest neighbors\"\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(knn, X_val, y_val)\nplt.title('Confusion matrix for k Nearest neighbors')\nplt.grid(False)\nplt.show()\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_knn\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_knn})","b00682e7":"%%time\n## lgb\nmodelname =\"Light Gradient Boosting\"\nlgbmmodel = LGBMClassifier()\nlgbmmodel.fit(X_train, y_train)\ny_pred = lgbmmodel.predict(X_val)\nacc_lgbmmodel = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_lgbmmodel)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(lgbmmodel, X_val, y_val)\nplt.title('Confusion matrix for light gradient boosting')\nplt.grid(False)\nplt.show()\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_lgbmmodel\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_lgbmmodel})","f27bf61f":"%%time\n## Linear discriminant\nmodelname =\"Linear discriminant\"\nlinear_da=LinearDiscriminantAnalysis()\nlinear_da.fit(X_train, y_train)\ny_pred = linear_da.predict(X_val)\nacc_linear_da = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_da)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(linear_da, X_val, y_val)\nplt.title('Confusion matrix for Linear Discriminant Analysis')\nplt.grid(False)\nplt.show()\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_linear_da\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_linear_da})","656b9f4f":"%%time\n# Linear SVC\nmodelname =\"Linear SVC\"\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\ny_pred = linear_svc.predict(X_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(linear_svc, X_val, y_val)\nplt.title('Confusion matrix for Linear svc')\nplt.grid(False)\nplt.show()\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_linear_svc\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_linear_svc})","7348e15e":"%%time\n# Logistic Regression\nmodelname =\"Logistic Regression\"\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(logreg, X_val, y_val)\nplt.title('Confusion matrix for Linear svc')\nplt.grid(False)\nplt.show()\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_linear_svc\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_linear_svc})","c32a9fc5":"%%time\n## MLP\nmodelname =\"MLP\"\nMLP = MLPClassifier()\nMLP.fit(X_train, y_train)\ny_pred = MLP.predict(X_val)\nacc_MLP= round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_MLP)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(MLP, X_val, y_val)\nplt.title('Confusion matrix for MLP')\nplt.grid(False)\nplt.show()\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_MLP\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_MLP})","30d781c5":"%%time\n## passive aggressive\nmodelname =\"Passive Aggressive\"\npassiveaggressive = PassiveAggressiveClassifier()\npassiveaggressive.fit(X_train, y_train)\ny_pred = passiveaggressive.predict(X_val)\nacc_passiveaggressive = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_passiveaggressive)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(passiveaggressive, X_val, y_val)\nplt.title('Confusion matrix for Passive Aggressive')\nplt.grid(False)\nplt.show()\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_passiveaggressive\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_passiveaggressive})","8dfa93d6":"%%time\n# Perceptron\nmodelname =\"Perceptron\"\nperceptron = Perceptron()\nperceptron.fit(X_train, y_train)\ny_pred = perceptron.predict(X_val)\nacc_perceptron = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_perceptron)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(perceptron, X_val, y_val)\nplt.title('Confusion matrix for Perceptron')\nplt.grid(False)\nplt.show()\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_perceptron\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_perceptron})","f0740cc7":"%%time\n# Random Forest\nmodelname =\"Random Forest\"\nrandomforest = RandomForestClassifier(random_state = 0)\nrandomforest.fit(X_train, y_train)\ny_pred = randomforest.predict(X_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(randomforest, X_val, y_val)\nplt.title('Confusion matrix for Random Forest')\nplt.grid(False)\nplt.show()\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_randomforest\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_randomforest})","13ba6875":"%%time\n## ridge\nmodelname =\"Ridge\"\nridge = RidgeClassifierCV()\nridge.fit(X_train, y_train)\ny_pred = ridge.predict(X_val)\nacc_ridge = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_ridge)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(ridge, X_val, y_val)\nplt.title('Confusion matrix for Ridge')\nplt.grid(False)\nplt.show()\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_ridge\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_ridge})","60637ebe":"%%time\n# Stochastic Gradient Descent\nmodelname =\"Stochastic Gradient Descent\"\nsgd = SGDClassifier()\nsgd.fit(X_train, y_train)\ny_pred = sgd.predict(X_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(sgd, X_val, y_val)\nplt.title('Confusion matrix for Stochastic Gradient Descent')\nplt.grid(False)\nplt.show()\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_sgd\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_sgd})","f3d29add":"%%time\n# instanciate model\nmodelname =\"Support Vector Machines\"\nsvc = SVC()\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(svc, X_val, y_val)\nplt.title('Confusion matrix for Support Vector Machines')\nplt.grid(False)\nplt.show()\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_svc\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_svc})","a31bb881":"%%time\n# xgboost\nmodelname =\"XGBoost\"\nxgb = XGBClassifier(n_estimators=10)\nxgb.fit(X_train, y_train)\ny_pred = xgb.predict(X_val)\nacc_xgb = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_xgb)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(xgb, X_val, y_val)\nplt.title('Confusion matrix for XGBoost')\nplt.grid(False)\nplt.show()\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_xgb\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_xgb})","3fd37168":"print(cpu_stats())","8ba9bf48":"#%%time\n\n# tuned xgboost\nmodelname =\"tuned XGBoost\"\nxgb = XGBClassifier(\n    n_estimators=1000,\n    learning_rate=0.1,\n    max_depth=3,\n    min_child_weight=3,\n    subsample=0.5,\n    colsample_bytree=0.5,\n    n_jobs=-1,\n    objective='binary:logistic',\n    eval_metric='auc', \n    # Uncomment if you want to use GPU. Recommended for whole training set.\n    #tree_method='gpu_hist',\n    random_state=38)\nxgb.fit(X_train, y_train)\ny_pred = xgb.predict(X_val)\nacc_xgb = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_xgb)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(xgb, X_val, y_val)\nplt.title('Confusion matrix for xgb model')\nplt.grid(False)\nplt.show()\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_catboost\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_bagging})\n    \n# Use the model to generate predictions\nxgbpredictions = xgb.predict(test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.id,\n                       'target': xgbpredictions})\noutput.to_csv('tuned_xgb_submission.csv', index=False)\nprint('tuned xgb submission completed')\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_xgb\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_xgb})","11fa9450":"# tuned catboost model\nmodelname =\"tuned CatBoost\"\n# set parameters\ncat_params = {\n    'iterations': 15585, \n    'objective': 'CrossEntropy', \n    'bootstrap_type': 'Bernoulli',\n    'learning_rate': 0.023575206684596582, \n    'reg_lambda': 36.30433203563295, \n    'random_strength': 43.75597655616195, \n    'depth': 8, \n    'min_data_in_leaf': 11, \n    'leaf_estimation_iterations': 1, \n    'subsample': 0.8227911142845009,\n    'eval_metric' : 'AUC',\n    'verbose' : 1000,\n    'early_stopping_rounds' : 500,\n}\n# instanciate model\ncat = CatBoostClassifier(**cat_params )\n# fit model\ncat.fit(X_train, y_train)\n\n# evaluate performance\ny_pred = cat.predict(X_val)\ny_pred_proba = cat.predict_proba(X_val)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_val,  y_pred_proba)\nauc = metrics.roc_auc_score(y_val, y_pred_proba)\nprint(auc)\nmetrics.plot_confusion_matrix(cat, X_val, y_val)\nplt.title('Confusion matrix for catboost model')\nplt.grid(False)\nplt.show()\n\nplt.plot(fpr,tpr,label=\"Catboost, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()\n\n# Use the model to generate predictions\npredictions = cat.predict(test)\n\n# Save the predictions to a CSV file\ntest=pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')\noutput = pd.DataFrame({'Id': test.id,\n                       'target': predictions})\noutput.to_csv('tuned_cat_submission.csv', index=False)\nprint('tuned cat submission completed')\n\ntry:\n    BasicModelPerformanced_df.at[modelname,'Score']=acc_xgb\nexcept:\n    BasicModelPerformanced_df = BasicModelPerformanced_df.append({index:modelname,'Score':acc_xgb})","b002b27a":"%%time\n#order and print model comparison\nBasicModelPerformanced_df=BasicModelPerformanced_df.sort_values(by='Score', ascending=False)\nprint(BasicModelPerformanced_df)\n\n# plot results\nx_pos = [i for i, _ in enumerate(BasicModelPerformanced_df.index)]\nplt.bar(x_pos,BasicModelPerformanced_df.Score)\nplt.xlabel(\"Model\")\nplt.ylabel(\"Score\")\nplt.title(\"Basic Model Comparison\")\nplt.xticks(x_pos, BasicModelPerformanced_df.index,rotation='vertical')\nplt.show()\n\nBasicModelPerformanced_df.to_csv(\"basic_model_comparison.csv\")\nprint('Comparison saved as csv')","9af9ce60":"## About Tabular Playground Series - Oct 2021\n\nThe dataset used for this competition is synthetic, but based on a real dataset and generated using a CTGAN.\n\nThe dataset deals with predicting the biological response of molecules given various chemical properties. Although the features are anonymized, they have properties relating to real-world features.\n\n## Previous notebooks\n\nMy first notebook on this competition explored the data in detail. At this point that notebook has had 38 upvotes and 15 comments, so might be worth a look before moving on to look at this notebook if you have not yet explored the data fully.\nhttps:\/\/www.kaggle.com\/davidcoxon\/first-look-at-october-data\n\n## About this notebook\n\nThis notebook is a work in progress and will be regularly updated. It is my second notebook of this competition and will concentrate on evaluating the performance of a number of basic models creating a baseline for future notebooks that will concentrate on parameter tuning the models the best performing models. This is a beginner level notebook meant for my own use and not indended to be a training aid or tutorial. Some of the code will be taken from other public notebooks, sources will be creditted at the bottom of the notebook.\n\n## About this notebook\nhttps:\/\/www.kaggle.com\/davidcoxon\/first-look-at-october-data\n\n## First thoughts on this months project\n\n* This months Tabular Playground Dataset is once again quite large, so managing both cpu usage and ram is going to be an important element of the project.\n* It looks like another classification problem.\n* There is no missing data, so imputing values will not be required.\n* There a both categorical and continuous features. The categorical data is all binary and some of the continuous data appears to be category like. It may be possible to reduce the memory requirements by redefining data types in order to minimize memory use without lossing any meaningful information.\n* Data engineering and feature importance may be important.\n* Its likely that model selection and hyper parameter tuning will be important.\n* Staking, blending and ensambles are likely to be important to get higher scores.\n\n## Exploring the data\n\nYou can find a complete exploration of the data this notebook: https:\/\/www.kaggle.com\/davidcoxon\/first-look-at-october-data\/notebook\n\nThe summary of the data exploration is:\n\n* The test dataset is approx 1\/2 the size of the training dataset\n* The training dataset is highly representative of the test dataset\n* There is no missing data\n* Approx 1\/6th of features are binary features\n* Approx 5\/6th of features are continuous features\n* There is relatively low correlation between features\n* There appears to be a relatively high correlation between f22 and target value.\n* The majority of categorical features have a negative correlation to target classification.\n* Continuous feature have show both positive and negative correlations to target classification.\n* feature importance indicates that there are a number of both categorical and continuous features of importance.\n* feature importance doesn't indicate f22 as an important feature.","bb1f1561":"# Convert category like features","9037202d":"# Optimized Models ","61d70dd1":"## Set up environment","98e0d4f2":"## Perceptron","b6325923":"## xgboost","69dbe339":"## Linear Discriminant Analysis","cfb747f7":"## Passive Aggressive","cc4375a1":"## Stochastic Gradient Descent\u00b6","efffd271":"## Extra Trees\n\nThe Extra Trees classifier is based on the decision trees classifier but implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.","2f7ac1f8":"## Catboost\nCatboost is a categorizer that used gradient boosting on decision trees.\n\nWe can use CatBoost without any explicit pre-processing to convert categories into numbers. CatBoost converts categorical values into numbers using various statistics on combinations of categorical features and combinations of categorical and numerical features.\n\nIt reduces the need for extensive hyper-parameter tuning and lowers the chances of overfitting also which leads to more generalized models. ","b6c1e418":"## Ridge Classifier\u00b6","f3f8f2d2":"# Basic modelling\n\nBefore spending time tuning models we are going to quickly take a look at how a range of basic models perform on a subset of the training data (20%) without any parameter tuning. We can then retrain and evaluate the top performing models with a bigger dataset  (80%) and tuned parameters.\n\nI could have created a list of models and simply iterated through the list fitting and scoring and generating a confusion matrix for each but i wanted to add a description of how each model worked so i've just done it long hand which makes for a longer notebook but it also allows you to run individual model independantly if you want to.","462ca9f2":"## Get Data","23499f91":"## Adaboost\n\nAn AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.","808c48fd":"## drop columns\nIn my previous notebook exploring the data i found several columns with only 1 value (which can be dropped).","5f63cb26":"## Decision Tree\n\nA decision tree is a flowchart-like tree structure where each node represents a feature or attribute, each branch represents a decision rule, and each leaf node represents the outcome. \n\nDecision trees are considered 'white box' classifiers in that they are easy to explain\/understand.","b1afdc8f":"## Get features","9f165568":"## Logistic Regression","b4850702":"## Normalize data","bafe37df":"## Create comparison dataframe","f52b7026":"## MLP","95548dc7":"## Gradient Boosting","4f24a9f9":"## Random Forest","68ae1a59":"## Gaussian Naive Bayes\n\nThe naive Bayes or idiot Bayes classified works with binary (two-class) and multi-class classification problems. It's calculations for the probabilities for each hypothesis are simplified to make their calculation tractable. Rather than attempting to calculate the values of each attribute value they are assumed to be conditionally independent given the target value.\n\nGaussian Naive Bayes is an extension of the Naive Bayes classifier which can be extended to real-valued attributes, most commonly by assuming a Gaussian distribution.","b3ba6f65":"# Model Comparison","a0cb51d8":"## Add statistical features","5dd8935d":"## Observations on Model comparison\n\nWe fitted and scored 20 models using the default perameters, we used only 15% of the available training data and we didn't use cross fold validation in any form. (The observations below were made based on the robust scaler).\n\nWe used ROC AUC or Area Under the Receiver Operating Characteristic Curve to score the results. \n\nThe ROC AUC scores fell into 2 groups clustered around 76 and 50. The Boosted classifiers generally produced the best results with Catboost coming top by a small margin. Ridge, Linear Discriminant Analysis and Random forest were also in the higher scoring model. Low scoring models included Gausian naive bayes, k nearest neighbour, linear svc, logistic regression, Stochastic Gradient Descent and passive aggressive models.\n\nMany of the models that scored only around 50% prediced almost all of their vales as 0, Stochastic Gradient Descent did the opoosite and predicting all 1's.\n\nIf we take just features with higher correlations (+\/-) 0.2 and run the same models agian, we get a range af ROC_AUC scores of between 49.95 and 55. The boosting models still perform best but this time only the Gaussian Naive Bayes predicts all 0's and the models at the lower end of the scale scored modertly better.\n\nIf we take just the categorical features and run the same models we get a range af ROC_AUC scores of between 61.93 and 75.77 in fact 12 of the models produce a score of 75.77.  \n\nIf we take just the continuous features and run the models we get a range of ROC_AUC scores of between 49.5 and 63.4, meaning that despite 85% of the features being contimuous on only 15% binary categogical features, the categorical features seem to produce better results than the continuous and the best categogical is only 1.5 benind the best result for all features. Once again boosters worked best.\n\nMoving forward to data engineering and parameter tuning it would be worth initially looking at the top 8 models all of which scored around 76. It would also be worth looking at the continuous data and seeing how it could be made to produce better results. \n\nNow that we know which models are the most likely candidates for tuning we could rerun those models with a higher split of the available training data. We could also filter the features and try just the categorical features or just the continuous features to see how that affects the results.\n","7e8af59b":"## Credit were credits due\n\nFirst up thanks to the Kaggle team for the tireless work putting the tabular plaground together. Many thanks to the kaggle and stackoverflow communities and the folks that contitbutor to the various documents for the various python modules, without whom finding solutions to these problems would be so much rougher.\n\nhttps:\/\/www.kaggle.com\/shenurisumanasekara\/tabular-october-catboost for parameters for Catboost tuning\n\nIf you found this notebook useful or you have comments please upvote \/ comment here. Also please do upvote any of the notebooks above if you use them or find them helpful. ","b54a1723":"## Prepare data for modelling\n\nIn this notebook we are really only exploring how different models might perform to identify some models that we'll go on to look at in more detail later. I have therefore split only 10% of the data out for training the models and 5% for testing. This allows the full notebook to run in about an hour. You can increase this to 80% \/ 20% if you want to test individual models more robustly but its a relatively large dataset and the session will most likely time out if you try and run all cell with 80% of the data.","1de9a93d":"## Light Gradient Boosting","62aaa6be":"## K Nearest Neighbors","9ca195ae":"## Bagging\nA Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction.\n\nSuch a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree).","ad0ff88c":"## Create functions","88a2ad38":"## LinearSVC","a3996498":"## Support Vector Machines"}}