{"cell_type":{"6e423490":"code","ca254286":"code","b216a650":"code","f7cddda6":"code","896f1c49":"code","e2ca03ed":"code","929509bf":"code","82680969":"code","95262ab8":"code","9117c43f":"code","e185ec19":"code","1bd46463":"code","da0478d7":"code","7bbcf410":"code","c26ce25f":"code","0ed78f7c":"code","096e2606":"code","6f751b7c":"code","36d8bb85":"code","020620d6":"code","f6d43124":"code","0c088897":"code","7c1b2e79":"code","b84c0350":"code","e0c21995":"code","523887d3":"code","f28e37c8":"code","f4902108":"code","b3bd5a97":"code","6b13d7f3":"code","83c2b2a7":"code","0e9f972f":"code","4cf03925":"code","e12e6ae5":"code","e6914ab2":"code","f373d6cc":"code","b6b85cd1":"code","df116fa6":"code","e5d7179c":"code","9e44a12d":"code","268bb2ac":"code","b0446221":"code","cc45da6a":"markdown","e2853901":"markdown","08fe3d93":"markdown","5546dfd6":"markdown","3c82971c":"markdown","6c15b9b8":"markdown","6f019e48":"markdown","23c94e3b":"markdown","0b6ef6b8":"markdown","8148a5ae":"markdown","579a0fcb":"markdown","d4b09add":"markdown","018044b9":"markdown","9261dc96":"markdown","b87336fb":"markdown","4f4d578d":"markdown","02d1e2a2":"markdown","cf1645d9":"markdown","2d055bf8":"markdown","e8dd74be":"markdown","3105c62c":"markdown","05eb4c78":"markdown","7607f756":"markdown","a3d270de":"markdown","cfa8a31c":"markdown","9387ed07":"markdown","822d2c4d":"markdown","cae1d9de":"markdown","7aa92b10":"markdown","53a1760e":"markdown","9f7fcff6":"markdown","bdf0c7bc":"markdown","fb8b2df4":"markdown","9593f806":"markdown","74927f2d":"markdown","6b4c0bc3":"markdown","0062db43":"markdown","5c076f45":"markdown","aee6d3d6":"markdown","5df91d83":"markdown","53024630":"markdown"},"source":{"6e423490":"!pip install ekphrasis","ca254286":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS \nfrom wordcloud import WordCloud\nfrom ekphrasis.classes.segmenter import Segmenter\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nimport gensim\nfrom collections import Counter\n\npd.set_option('display.max_columns', 80)\n%matplotlib inline","b216a650":"nltk.download(\"punkt\")\nfrom nltk.tokenize import word_tokenize\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nfrom nltk.stem.snowball import SnowballStemmer\nnltk.download(\"brown\")\nnltk.download('averaged_perceptron_tagger')","f7cddda6":"questions = pd.read_csv(\"..\/input\/data-science-for-good-careervillage\/questions.csv\")\n# Questions get posted by students. Sometimes they're very advanced. Sometimes they're just getting started.\n# It's all fair game, as long as it'\"s relevant to the student's future professional success.\ntag_questions = pd.read_csv(\"..\/input\/data-science-for-good-careervillage\/tag_questions.csv\")\n# Every question can be hashtagged. We track the hashtag-to-question pairings, and put them into this file.\ntags = pd.read_csv(\"..\/input\/data-science-for-good-careervillage\/tags.csv\")\n# Each tag gets a name.\nquestion_scores = pd.read_csv(\"..\/input\/data-science-for-good-careervillage\/question_scores.csv\")\n# \"Hearts\" scores for each question.\n\n# Data is merged.\nquestions_all = pd.merge(questions, tag_questions, left_on=\"questions_id\", right_on=\"tag_questions_question_id\").drop(\"tag_questions_question_id\", axis=\"columns\")\nquestions_all = pd.merge(questions_all, tags, left_on=\"tag_questions_tag_id\", right_on=\"tags_tag_id\").drop([\"tag_questions_tag_id\", \"tags_tag_id\"], axis=\"columns\")\nquestions_all = pd.merge(questions_all, question_scores, left_on=\"questions_id\", right_on=\"id\").drop(\"questions_id\", axis=\"columns\")\n\nquestion_tags= questions_all.groupby(\"id\")[\"tags_tag_name\"].unique()\nquestions_all= pd.merge(questions_all, question_tags.to_frame(), left_on=\"id\", right_index=True)\nquestions_all.drop_duplicates(subset=[\"id\"], inplace=True)\nquestions_all.drop(\"tags_tag_name_x\", axis=\"columns\", inplace=True)\nquestions_all.rename(columns={\"tags_tag_name_y\": \"tag_name\"}, inplace=True)\nprint(questions_all.shape)\nquestions_all.head()","896f1c49":"seg = Segmenter(corpus=\"english\")","e2ca03ed":"def tag_extender(tags):\n    from_hyphen = [y for x in tags for y in x.split(\"-\")]\n    from_underscore = [y for x in from_hyphen for y in x.split(\"_\")]\n    from_hashtag = [seg.segment(tag) for tag in from_underscore]\n    remove_numbers = re.sub(r'[0-9]+', '', \" \".join(from_hashtag))\n    create_list = [x for x in remove_numbers.split(\" \")]\n    final_list = [x for x in create_list if len(x)>0]\n    return final_list\n\nquestions_all[\"extended_tags\"] = questions_all[\"tag_name\"].apply(tag_extender)\nquestions_all.tail().iloc[:,[6,7]]","929509bf":"# Unwanted characters are removed from question bodies.\nquestions_all.reset_index(drop=True, inplace=True)\nquestions_all[\"questions_body\"] = questions_all[\"questions_body\"].apply(lambda x: re.compile(r\"[\\n\\r\\t]\").sub(\" \", x))\nquestions_all[\"questions_body\"] = questions_all[\"questions_body\"].apply(lambda x: re.sub(r\"(#\\S*)\", \"\", x))","82680969":"# Merging stopwords\nnltk_stopwords= set(stopwords.words('english'))\nspacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\ntotal_stopwords=nltk_stopwords|spacy_stopwords\nlen(total_stopwords)","95262ab8":"all_words_body = (' '.join(questions_all[\"questions_body\"]).lower().split())\nno_sw_body = [word for word in all_words_body if not word in total_stopwords]\nmost_common_body_sw = Counter(no_sw_body).most_common(30)\n\nadd_stopwords=[]\n\nfor i in range(30):\n    add_stopwords.append(most_common_body_sw[i][0])\n    \nadd_stopwords","9117c43f":"wikipedia_stopwords_nouns = [\"time\", \"person\", \"year\", \"way\", \"day\", \"thing\", \"man\", \"world\", \"life\", \"hand\", \"part\", \"child\",\n                             \"eye\", \"woman\", \"place\", \"work\", \"week\", \"case\", \"point\", \"government\", \"company\", \"number\",\n                             \"group\", \"problem\", \"fact\"]\n\n# https:\/\/en.wikipedia.org\/wiki\/Most_common_words_in_English#Nouns\n# 25 most common nouns in English language listed on Wikipedia.","e185ec19":"wikipedia_stopwords_verbs = [\"be\", \"have\", \"do\", \"say\", \"get\", \"make\", \"go\", \"know\", \"take\", \"see\", \"come\", \"think\", \"look\",\n                             \"want\", \"give\", \"use\", \"find\", \"tell\", \"ask\", \"work\", \"seem\", \"feel\", \"try\", \"leave\", \"call\"]\n\n# https:\/\/en.wikipedia.org\/wiki\/Most_common_words_in_English#Verbs\n# 25 most common verbs in English language listed on Wikipedia.","1bd46463":"manual_stopwords = [\"question\", \"answer\", \"class\", \"study\", \"graduate\", \"grad\", \"interest\", \"university\", \"undecided\", \"decide\", \"decision\", \"manage\"]\n\n# Generic words used frequently in the texts due to the nature of the topic.","da0478d7":"# Combining all the stopwords\n\nadded_total_stopwords=total_stopwords|set(add_stopwords)|set(wikipedia_stopwords_nouns)|set(wikipedia_stopwords_verbs)|set(manual_stopwords)\nlen(added_total_stopwords)","7bbcf410":"# Parameters\nmin_len = 3\nstemmer = SnowballStemmer(\"english\")\nnot_to_stem = [\"animation\", \"animator\"]\n###\n\ndef process_text(text):\n    # Make all the strings lowercase and remove non alphabetic characters\n    text = re.sub('[^A-Za-z]', ' ', text.lower())\n\n    # Tokenizing the text\n    tokenized_text = word_tokenize(text)\n    \n    # Taking only nouns, adjectives and verbs\n    is_noun_adj_verb = lambda pos: pos[:2] == \"NN\" or pos[:2] == \"JJ\" or pos[:2] == \"VB\"\n    noun_adj_verb = [(word, pos) for (word, pos) in nltk.pos_tag(tokenized_text) if is_noun_adj_verb(pos)]\n    \n    # Lemmatizing the tokens according to their PoS tags\n    lemmatized_text = []\n    for word, tag in noun_adj_verb:\n        if tag.startswith(\"NN\"):\n            lemmatized_text.append(lemmatizer.lemmatize(word, pos='n'))\n        elif tag.startswith('VB'):\n            lemmatized_text.append(lemmatizer.lemmatize(word, pos='v'))\n        elif tag.startswith('JJ'):\n            lemmatized_text.append(lemmatizer.lemmatize(word, pos='a'))\n        else:\n            None\n    \n    # Removing tokens that are shorter than 3 characters\n    longer_words = [word for word in lemmatized_text if len(word)>=min_len]\n    \n    # Removing stopwords\n    remove_stopwords = [word for word in longer_words if not word in added_total_stopwords]\n    \n    # Stemming with Snowball English\n    stemmed_words = [stemmer.stem(word) if word not in not_to_stem else word for word in remove_stopwords]\n\n    return stemmed_words\n\n# Processing question titles and questions\n\nquestions_all[\"title_tokens\"]=questions_all[\"questions_title\"].apply(process_text)\nquestions_all[\"body_tokens\"]=questions_all[\"questions_body\"].apply(process_text)","c26ce25f":"# Parameters\nmin_len = 3\nstemmer = SnowballStemmer(\"english\")\nnot_to_stem = [\"animation\", \"animator\"]\n###\n\ndef process_tags(text):\n    is_noun_adj_verb = lambda pos: pos[:2] == \"NN\" or pos[:2] == \"JJ\" or pos[:2] == \"VB\"\n    noun_adj_verb = [(word, pos) for (word, pos) in nltk.pos_tag(text) if is_noun_adj_verb(pos)]\n    \n    # Lemmatizing the tokens according to their PoS tags\n    lemmatized_text = []\n    for word, tag in noun_adj_verb:\n        if tag.startswith(\"NN\"):\n            lemmatized_text.append(lemmatizer.lemmatize(word, pos='n'))\n        elif tag.startswith('VB'):\n            lemmatized_text.append(lemmatizer.lemmatize(word, pos='v'))\n        elif tag.startswith('JJ'):\n            lemmatized_text.append(lemmatizer.lemmatize(word, pos='a'))\n        else:\n            None\n    longer_words = [word for word in lemmatized_text if len(word)>=min_len]\n    remove_stopwords = [word for word in longer_words if not word in added_total_stopwords]\n    stemmed_words = [stemmer.stem(word) if word not in not_to_stem else word for word in remove_stopwords]\n    return stemmed_words\n\n# Processing tags\n\nquestions_all[\"tag_tokens\"] = questions_all[\"extended_tags\"].apply(process_tags)","0ed78f7c":"questions_all[\"tag_len\"] = questions_all[\"tag_tokens\"].apply(lambda x: len(x))\nquestions_all.sort_values(\"tag_len\", ascending=False).head().iloc[:, [2,3, -6, -1]]","096e2606":"questions_all[\"tag_len\"].plot(kind=\"box\", figsize=(10,10), grid=True);","6f751b7c":"questions_all[questions_all[\"tag_len\"]>30][\"tag_tokens\"].apply(lambda x: x.clear())","36d8bb85":"questions_all.drop(\"tag_len\", axis=1, inplace=True)","020620d6":"questions_all[\"question_tokens\"]=questions_all[\"title_tokens\"]+questions_all[\"body_tokens\"]+questions_all[\"tag_tokens\"]\nquestions_all.head()","f6d43124":"def as_it_is(word):\n    return word\n\ntfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, tokenizer=as_it_is, preprocessor=as_it_is)\ntfidf = tfidf_vectorizer.fit_transform(questions_all[\"question_tokens\"])\ntfidf_feature_names = tfidf_vectorizer.get_feature_names()","0c088897":"len(tfidf_feature_names)","7c1b2e79":"kmin, kmax = 15, 40\n\ntopic_models = []\n\nfor k in range(kmin,kmax+1):\n    model = NMF( init=\"nndsvd\", n_components=k, random_state=10, alpha=0.1, l1_ratio=.5 ).fit(tfidf)\n    W = model.fit_transform(tfidf)\n    H = model.components_    \n    topic_models.append( (k,W,H) )","b84c0350":"import gensim\nall_tokens = questions_all[\"question_tokens\"].apply(lambda x: \" \".join(x))\nall_tokens = ((\"\").join(all_tokens).split())\nw2v_model = gensim.models.Word2Vec([all_tokens], min_count=2, sg=1)","e0c21995":"# https:\/\/github.com\/derekgreene\/topic-model-tutorial\/blob\/master\/3%20-%20Parameter%20Selection%20for%20NMF.ipynb\n\ndef calculate_coherence( w2v_model, term_rankings ):\n    overall_coherence = 0.0\n    for topic_index in range(len(term_rankings)):\n        # check each pair of terms\n        pair_scores = []\n        for pair in combinations( term_rankings[topic_index], 2 ):\n            pair_scores.append( w2v_model.similarity(pair[0], pair[1]) )\n        # get the mean for all pairs in this topic\n        topic_score = sum(pair_scores) \/ len(pair_scores)\n        overall_coherence += topic_score\n    # get the mean score across all topics\n    return overall_coherence \/ len(term_rankings)\n\ndef get_descriptor( all_terms, H, topic_index, top ):\n    # reverse sort the values to sort the indices\n    top_indices = np.argsort( H[topic_index,:] )[::-1]\n    # now get the terms corresponding to the top-ranked indices\n    top_terms = []\n    for term_index in top_indices[0:top]:\n        top_terms.append( all_terms[term_index] )\n    return top_terms\n\nfrom itertools import combinations\nk_values = []\ncoherences = []\nfor (k,W,H) in topic_models:\n    # Get all of the topic descriptors - the term_rankings, based on top 10 terms\n    term_rankings = []\n    for topic_index in range(k):\n        term_rankings.append( get_descriptor( tfidf_feature_names, H, topic_index, 10 ) )\n    # Now calculate the coherence based on our Word2vec model\n    k_values.append( k )\n    coherences.append( calculate_coherence( w2v_model, term_rankings ) )","523887d3":"plt.plot(k_values, coherences)\nplt.grid(True)\nplt.xlabel(\"Number of topics\", fontsize=14, weight=\"bold\", labelpad = 20)\nplt.ylabel(\"Coherence score\", fontsize = 14, weight=\"bold\", labelpad=15)\nplt.title(\"Change in coherence score with number of topics\", fontsize = 17, weight=\"bold\", pad=10)\nplt.xticks(np.arange(kmin,kmax+1,1))\nfig = plt.gcf()\nfig.set_size_inches(17,4)\nplt.show()","f28e37c8":"model = NMF(n_components=26, random_state=10, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n\nW = model.fit_transform(tfidf)\nH = model.components_ ","f4902108":"def display_topics(model, feature_names, no_top_words):\n    topic_dict = {}\n    for topic_idx, topic in enumerate(model.components_):\n        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n    return pd.DataFrame(topic_dict)","b3bd5a97":"no_top_words = 15\ntopic_weight = display_topics(model, tfidf_feature_names, no_top_words)\ntopic_weight","6b13d7f3":"topic_dict_wc = {\n    0: \"Engineering\",\n    1: \"Nursing\",\n    2: \"Business adm. and management\",\n    3: \"Computer science\",\n    4: \"Medicine\",\n    5: \"Financing studies\",\n    6: \"Psychology and psychiatry\",\n    7: \"Law\",\n    8: \"Applied arts\",\n    9: \"Teaching\",\n    10: \"Gaining work experience\",\n    11: \"Physical therapy\",\n    12: \"Accounting and finance\",\n    13: \"Biosciences\",\n    14: \"Sports\",\n    15: \"Performing arts\",\n    16: \"Budgetary issues\",\n    17: \"Budgetary issues\",\n    18: \"Medicine\",\n    19: \"Game development\",\n    20: \"Career path\",\n    21: \"Veterinary med. and zoology\",\n    22: \"Arts\",\n    23: \"Science\",\n    24: \"Social work\",\n    25: \"Computer science\"\n}\n\nfig = plt.figure(figsize=(20,25))\na=1\nfor col in np.arange(0, 52, 2):\n    col_w = col + 1\n    words=list()\n    weights=list()\n    for i in range(15):\n        words.append(topic_weight.iloc[i, col])\n        weights.append(float(topic_weight.iloc[i, col_w]))\n    temp = zip(words, weights)\n    dictWords = dict(temp)\n    wordcloud = WordCloud(width=500,height=300, background_color=\"white\", min_font_size=8).generate_from_frequencies(dictWords)\n    ax = fig.add_subplot(7,4,a)\n    ax.imshow(wordcloud, interpolation='bilinear')\n    ax.set_title(\"Topic Nr: \"+str(int(col\/2))+\", \" + str(topic_dict_wc[col\/2]), fontsize=14, weight=\"bold\")\n\n    for spine in ax.spines.values():\n        spine.set_edgecolor('#666666')\n    ax.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n    ax.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\n\n    a=a+1\n\nplt.show();","83c2b2a7":"topic_dict = {\n    0: \"Engineering\",\n    1: \"Nursing\",\n    2: \"Business administration and management\",\n    3: \"Computer science\",\n    4: \"Medicine\",\n    5: \"Financing studies\",\n    6: \"Psychology and psychiatry\",\n    7: \"Law\",\n    8: \"Applied arts\",\n    9: \"Teaching\",\n    10: \"Gaining work experience\",\n    11: \"Physical therapy\",\n    12: \"Accounting and finance\",\n    13: \"Biosciences\",\n    14: \"Sports\",\n    15: \"Performing arts\",\n    16: \"Budgetary issues\",\n    17: \"Budgetary issues\",\n    18: \"Medicine\",\n    19: \"Game development\",\n    20: \"Career path\",\n    21: \"Veterinary medicine and zoology\",\n    22: \"Arts\",\n    23: \"Science\",\n    24: \"Social work\",\n    25: \"Computer science\"\n}","0e9f972f":"#Labeling the questions with their dominant topics\n\nquestions_all[\"topic\"] = questions_all.reset_index()[\"index\"].apply(lambda x: topic_dict[W[x,:].argmax()])\nquestions_all.iloc[:, [2,3,6,-1]].head()","4cf03925":"topic_shortened = {\n    \"Engineering\": \"Engineering\",\n    \"Nursing\": \"Nursing\",\n    \"Business administration and management\": \"Buss. adm. & mng\",\n    \"Computer science\": \"Comp. science\",\n    \"Medicine\": \"Medicine\",\n    \"Financing studies\": \"Financing stud.\",\n    \"Psychology and psychiatry\": \"Psych. & Psychiatry\",\n    \"Law\": \"Law\",\n    \"Applied arts\": \"Appl. arts\",\n    \"Teaching\": \"Teaching\",\n    \"Gaining work experience\": \"Gain. work exp.\",\n    \"Physical therapy\": \"Phys. therapy\",\n    \"Accounting and finance\": \"Acc. & finance\",\n    \"Biosciences\": \"Biosciences\",\n    \"Sports\": \"Sports\",\n    \"Performing arts\": \"Perf. arts\",\n    \"Budgetary issues\": \"Budget. iss.\",\n    \"Game development\": \"Game dev.\",\n    \"Career path\": \"Career path\",\n    \"Veterinary medicine and zoology\": \"Vet. med. & zool.\",\n    \"Arts\": \"Arts\",\n    \"Science\": \"Science\",\n    \"Social work\": \"Social work\"\n}","e12e6ae5":"topic_question_count = questions_all.groupby(\"topic\").size().sort_values(ascending=False).tolist()\ntopic_question_count_x = questions_all.groupby(\"topic\").size().sort_values(ascending=False).index.tolist()\n\ntopic_perc = []\na=0\nfor count in topic_question_count:\n    topic_perc.append(count\/sum(topic_question_count)*100 + a)\n    a = count\/sum(topic_question_count)*100 + a\n    \nshort_topics = [topic_shortened[topic] for topic in topic_question_count_x]\n\nfig, ax1 = plt.subplots()\nax1.bar(short_topics, topic_question_count, color=\"#7aa428\")\nax1.set_xlabel(\"Topics\", labelpad=15, weight=\"bold\", fontsize=19)\nax1.set_ylabel(\"Number of questions\", labelpad=17, weight=\"bold\", fontsize=17)\nax1.tick_params(\"x\", rotation = 50, labelsize=14)\nplt.setp(ax1.get_xticklabels(), ha=\"right\")\nax1.tick_params(\"y\", labelsize=15)\nax1.set_yticks(np.arange(0,6000, 1000).tolist())\n\nax2 = ax1.twinx()\nax2.plot(topic_perc, marker=\"o\")\nax2.set_ylabel(\"% of questions\", labelpad = 25, weight=\"bold\", fontsize=17, rotation=270)\nax2.tick_params(\"y\", labelsize=15)\nax2.set_yticks(np.arange(0,110,10).tolist())\n\nplt.title(\"Question count per topic\", fontsize=20, weight=\"bold\", pad =10)\nplt.grid(True, axis=\"y\")\nfig=plt.gcf()\nfig.set_size_inches(20, 10)\nplt.show()","e6914ab2":"answers = pd.read_csv(\"..\/input\/data-science-for-good-careervillage\/answers.csv\")\nanswers.dropna(axis=0, inplace=True)\npf_answer_topic = answers[[\"answers_id\", \"answers_question_id\"]].merge(questions_all[[\"id\", \"topic\"]], left_on=\"answers_question_id\", right_on =\"id\").drop(\"answers_question_id\", axis=1)\ntopic_answer_count = pf_answer_topic.groupby(\"topic\").size().sort_values(ascending=False).tolist()\ntopic_answer_count_x = pf_answer_topic.groupby(\"topic\").size().sort_values(ascending=False).index.tolist()\n\ntopic_perc = []\na=0\nfor count in topic_answer_count:\n    topic_perc.append(count\/sum(topic_answer_count)*100 + a)\n    a = count\/sum(topic_answer_count)*100 + a\n    \nshort_topics = [topic_shortened[topic] for topic in topic_answer_count_x]\n\nfig, ax1 = plt.subplots()\nax1.bar(short_topics, topic_answer_count, color=\"#ffdb0d\")\nax1.set_xlabel(\"Topics\", labelpad=15, weight=\"bold\", fontsize=19)\nax1.set_ylabel(\"Number of answers\", labelpad=17, weight=\"bold\", fontsize=17)\nax1.tick_params(\"x\", rotation = 50, labelsize=14)\nplt.setp(ax1.get_xticklabels(), ha=\"right\")\nax1.tick_params(\"y\", labelsize=15)\n#ax1.set_yticks(np.arange(0,6000, 1000).tolist())\n\nax2 = ax1.twinx()\nax2.plot(topic_perc, marker=\"o\")\nax2.set_ylabel(\"% of answers\", labelpad = 25, weight=\"bold\", fontsize=17, rotation=270)\nax2.tick_params(\"y\", labelsize=15)\nax2.set_yticks(np.arange(0,110,10).tolist())\n\nplt.title(\"Answer count per topic\", fontsize=20, weight=\"bold\", pad =10)\nplt.grid(True, axis=\"y\")\nfig=plt.gcf()\nfig.set_size_inches(20, 10)\nplt.show()","f373d6cc":"topic_mean_score = questions_all.groupby(\"topic\")[\"score\"].mean().sort_values(ascending=False)\nshort_topics = [topic_shortened[x] for x in topic_mean_score.index.tolist()]\n\nplt.bar(short_topics, topic_mean_score)\nplt.title(\"Average question scores per topic\", pad=15, fontsize=17, weight=\"bold\")\nplt.xlabel(\"Topics\", fontsize = 15, weight=\"bold\", labelpad=10)\nplt.ylabel(\"Average score\", fontsize=15, weight=\"bold\", labelpad=10)\nplt.xticks(short_topics, rotation=50, ha=\"right\", fontsize=12)\nplt.yticks(fontsize=12)\nplt.grid(True, axis=\"y\")\nmean_line=questions_all[\"score\"].mean()\nplt.axhline(mean_line, label=\"mean\", color=\"#ff279d\")\nplt.legend(loc=\"best\")\nfig=plt.gcf()\nfig.set_size_inches(15,7)","b6b85cd1":"answers = pd.read_csv(\"..\/input\/data-science-for-good-careervillage\/answers.csv\")\nanswers_col = [\"answer_id\", \"author_id\", \"question_id\", \"a_date\", \"answer\"]\nanswers.columns = answers_col\npf = pd.read_csv(\"..\/input\/data-science-for-good-careervillage\/professionals.csv\")\npf_col = [\"pf_id\", \"pf_loc\", \"pf_ind\", \"pf_hl\", \"pf_date\"]\npf.columns = pf_col\n\npf_answers = pd.merge(answers, pf, left_on=\"author_id\", right_on=\"pf_id\", how=\"inner\").drop(\"author_id\", axis=1)\npf_answers_questions = pd.merge(pf_answers, questions_all, left_on=\"question_id\", right_on=\"id\").drop(\"id\", axis=1)\n\npf_answers = pf_answers_questions[[ \"topic\", \"questions_title\", \"questions_body\", \"extended_tags\", \"a_date\", \"answer\", \"pf_id\", \"pf_loc\", \"pf_ind\", \"pf_hl\", \"answer_id\", \"question_id\"]]\npf_answers.columns = [\"topic\", \"title\", \"question\", \"tags\", \"answer_date\", \"answer\", \"pf_id\", \"pf_loc\", \"pf_ind\", \"pf_hl\", \"a_id\", \"question_id\"]\nprint(\"Out of %d professionals listed in the dataset, %d of them answered questions.\" % (pf.shape[0], pf_answers[\"pf_id\"].unique().shape[0]))\nprint(\"There are %d answers in total, written by professionals listed in tha dataset.\" % (pf_answers.shape[0]))","df116fa6":"#Most answered topics of professionals are extracted.\n\npf_id_list = pf_answers[\"pf_id\"].unique().tolist()\npf_answered_topics = pd.DataFrame()\npf_group = pf_answers.groupby([\"pf_id\", \"topic\"]).size().sort_values(ascending=False)\n\nfor id_nr in pf_id_list:\n    topic_count = pf_group.loc[id_nr]\n    topic_first = topic_count.index[0]\n    pf_answered_topics = pf_answered_topics.append(pd.Series([id_nr, topic_first]), ignore_index=True)\n    \npf_answered_topics.columns = [\"pf_id\", \"topic_first\"]\npf_topics = pf.merge(pf_answered_topics, left_on=\"pf_id\", right_on=\"pf_id\")","e5d7179c":"topics_top_ind = pd.DataFrame(pf_topics.groupby(\"topic_first\")[\"pf_ind\"].agg(lambda x:x.value_counts().index[0])).reset_index()\ntopics_top_ind.columns = [\"Most answered topic\", \"Industry of Professionals\"]\ntopics_top_ind","9e44a12d":"topics_top_hl = pd.DataFrame(pf_topics.groupby(\"topic_first\")[\"pf_hl\"].agg(lambda x:x.value_counts().index[0])).reset_index()\ntopics_top_hl.columns = [\"Most answered topic\", \"Headline of Professionals\"]\ntopics_top_hl","268bb2ac":"pf_topics.groupby(\"pf_ind\").size().sort_values(ascending=False).head()","b0446221":"pf_topics.groupby(\"pf_hl\").size().sort_values(ascending=False).head()","cc45da6a":"<h2>7) Topic Modeling<\/h2>","e2853901":"<b>1) What is the distribution of number of questions per topics?<\/b>","08fe3d93":"<h3>7.2) Selecting the optimum number of topics<\/h3>","5546dfd6":"Data from the datasets <i>questions<\/i>, <i>tag_questions<\/i>, <i>tags<\/i> and <i>question_scores<\/i> are merged and named as <i>questions_all<\/i>.","3c82971c":"Some students wanted to ask generic career questions, which is addressed to professionals of any occupation. Hoping to reach them, those students added all different tags to their questions. When such irrelevant tags are used together, the model gets confused and assigns those irrelevant topics to same topics.<br>\n\nIn order to avoid a confusion with the model, some questions might need some alteration. Before that, the number of tags used in questions should be examined.","6c15b9b8":"The same reason also applies to \"Solutions Manager\" as a headline.","6f019e48":"Judging by this fact, the minimum number of topics is set to 15.","23c94e3b":"Tags are split from hyphens (\"-\") and underscores (\"_\"). <i>Segmenter<\/i> from <i>\"Ekphrasis\"<\/i> is used to split tags, in which words are appended after one another without a white space.","0b6ef6b8":"Now that the confusing tag issue has been overcome, a corpus can be generated from the tokens extracted from question titles, question body and tags.","8148a5ae":"As can be seen above, <b>\"Accounting and finance\"<\/b> has the highest mean score of approximately 4, which means that related questions include highly \"popular\" issues.<br>\n<b>\"Medicine\"<\/b>, the second most asked topic, has a below average mean score, which could mean that its questions are from various aspects of the topic and issues are disperse.<hr>","579a0fcb":"<h2>6) Generating token list<\/h2>","d4b09add":"As can be seen above, there are outliers in terms of number of tags used in questions, especially increasing after 30. Following the manual examination, it is seen that questions with more than 30 tags are meant as generic career path questions, as explained above. In order to avoid a confusion with the model, the tags of questions, which have more than 30 tags, are cleared.","018044b9":"When tag sets are interpreted as topics, there can be observed at least 15 topics.","9261dc96":"<hr>","b87336fb":"For selecting the optimum number of topics, first, the minimum and maximum values of expected topics are defined. Then, for each number of topics, a model is generated and coherence score is calculated. Resulting scores are plotted with corresponding number of topics and the model resulting in the highest coherence score is chosen as the main model.<br><br>\n\nFor comparing the goodness of fit of models with different number of topics, coherence score is calculated with the TC-W2V measure, which is proposed by O'Callaghan et al. (2015). This measure is calculates the average similarity of word pairs in corpus, where similarity is the cosine difference.<br><br>\n\nThe code for calculating and comparing the coherence score is written by Dr. Derek Greene, who is one of the contributors of the purposed TC-W2V measure (2017).\n\n<hr>\nGreene, D. (2017). topic-model-tutorial. GitHub repository, https:\/\/github.com\/derekgreene\/topic-model-tutorial <br><br>\nO\u2019Callaghan, D., Greene, D., Carthy, J. and Cunningham, P. (2015). An analysis of the coherence of descriptors in topic modeling. Expert Systems with Applications, 42(13), pp.5645-5657.\n<hr>","4f4d578d":"<h2>4) Text processing<\/h2>","02d1e2a2":"<h2>1) Generating the main dataset <\/h2>","cf1645d9":"As can be seen above in the graphic, the model with 26 topics gave the highest coherence score. However, before deciding on the model, the interpretability should also be considered. Therefore, top 15 tokens and their weight coefficients are listed for the corresponding model.","2d055bf8":"<h2>3) Formatting questions <\/h2>","e8dd74be":"Topics above can be interpreted as below:\n\n0. Engineering\n1. Nursing\n2. Business administration and management\n3. Computer science\n4. Medicine\n5. Financing studies\n6. Psychology and psychiatry\n7. Law\n8. Applied arts (Graphic design, fashion design, architectural design)\n9. Teaching\n10. Gaining work experience\n11. Physical therapy\n12. Accounting and finance\n13. Biosciences\n14. Sports\n15. Performing arts (Music, theater, film)\n16. Earnings\n17. Debts (Student loans)\n18. Medical doctor\n19. Game development\n20. Career path\n21. Veterinary medicine and zoology\n22. Arts (Fine arts, culinary arts)\n23. Science\n24. Social work\n25. Software development","3105c62c":"<img src=\"https:\/\/i.imgur.com\/lDUCj8o.jpg\" width=\"700\">","05eb4c78":"In CareerVillage.org, there are questions regarding many career branches, including veterinary medicine and graphic design. Although these fields are very far from each other, they share a similar word in spelling, which is \"animal\" for veterinary medicine and \"animator\" for graphic desing. When relevant texts are processed, the stemming process reduces words \"animal\", \"animate\", \"animation\" and \"animator\" to the same stem of \"anim\", which makes those two very different branches to be assigned to the same topics. In order to avoid this and to emphasize that they are different words, <b>\"animation\"<\/b> and <b>\"animator\"<\/b> are added as words not to be stemmed during the text processing.","7607f756":"<h2>Steps<\/h2>\n<br>\n<b>1)<\/b> Relevant datasets are merged and columns are renamed for usability purposes.<br>\n\n<b>2)<\/b> Tags are formatted in a proper way to be processed.<br>\n<ul>\n    <li> Some tags consist of more than one words, but are written without a white space, by using a hyphen (\"-\") or an underscore (\"_\") or by appending after one another. In order to tokenize tags, they are split in such cases.<\/li>\n    <li> Numbers are removed.<\/li>\n<\/ul>\n\n<b>3)<\/b> Questions are formatted in a proper way to be processed.<br>\n<ul>\n    <li> HTML tags (\\n, \\r) are removed.<\/li>\n    <li> Hashtags are removed from the text, as they are stored in another dataset <i>tags<\/i>, and are to be merged into the master dataset through it.<\/li>\n<\/ul>\n\n<b>4)<\/b> Text processing is executed on titles, questions and tags.<br>\n<ul>\n    <li> <b>Lowercase: <\/b>Characters are converted into lowercase in order to be grouped better.<\/li>\n    <li> <b>Tokenization: <\/b>Text is tokenized.<\/li>\n    <li> <b>PoS Tagging: <\/b>A PoS (Part-of-Speech) tagging process is executed to filter out tokens of certain types. In this task, token types of nouns, adjectives and verbs are considered.<\/li>\n    <li> <b>Lemmatization: <\/b>Tokens are lemmatized according to their PoS tags, which brings the tokens to their dictionary form. For example, postfixes (e.g. plural forms) are stripped out.<\/li>\n    <li> <b>Removing short tokens: <\/b>Tokens which are shorter than 3 characters are filtered out.<\/li>\n    <li> <b>Removing stopwords: <\/b>Stopwords are removed. For this step, stopwords from SpaCy and NLTK are merged. NLTK removes modal verbs and articles successfully, however SpaCy includes pronouns, such as \"something\" and \"someone\", as well. Moreover, there are certain words, which appear in most of the questions, regardless of the topic, such as \"college\", \"school\", \"career\" and \"interested\". They do not imply any special topic and confuse the model, as they appear often in questions of diverse topics. Therefore, 30 most commen tokens are found and added as stopwords. Moreover, 25 most common nouns and adjectives, as listed on Wikipedia) are also added to the stopwords set.<\/li>\n    <li> <b>Stemming: <\/b>As the last step, stemming with Snowball English will be performed, in order to group related words together. For example, as lemmatization is done before, \"nurse\" and \"nursing\" are considered as two different words. This might lower the accuracy of the model, as those two words might be assigned to differet topics. After stemming, they both are converted into their stem \"nurs\" and are grouped together.<br><br>\n        <b>Note 1: <\/b>The reason why stemming is done in the last step is, so that stopwords would be removed properly. For example, the most common words added to the stopwords set include \"college\", whose stemmed version is \"colleg\". If stemming had been done before, then \"colleg\" would not be removed. Therefore lemmatization is made for a proper stopwords removal and stemming is made for a proper grouping of tokens.<br><br>\n        <b>Note 2: <\/b>Tags are not tokenized, as they are already in token forms.<\/li>\n<\/ul>\n\n<b>5)<\/b> In some generic career questions, students used tags from various categories, with the purpose of increasing the reach of their questions, which results in tokens of very different topics to be used in the same text. In order to avoid a confusion in the model due to this issue, for questions with more than 30 tags, tags are cleared.<br>\n\n<b>6)<\/b> A token set is generated, which is a combination of tokens from questions, titles and tags, and the data set is saved.<br>\n\n<b>7)<\/b> Topic modeling is executed.\n<ul>\n    <li> A TF-IDF vectorizer is generated with the tokens from the general token set.<\/li>\n    <li> In order to find the optimum number of topics, coherence scores are plotted.<\/li>\n    <li> The number of topics with the highest coherence score is used to generate the main model.<\/li>\n    <li> The resulting topics are named and combined if necessary. <\/li>\n    <li> The topic with the highest coefficient is assigned as the (main) topic of the corresponsing question and added to the main dataset.\n<\/ul>","a3d270de":"HTML tags and hashtags are removed from the questions text.","cfa8a31c":"<hr>","9387ed07":"There results show that extracted topics are consistent. The reason why \"Telecommunitions\" appear very often in the industry table is because there are many professionals in this industry.","822d2c4d":"<h1>Topic Modeling<\/h1><br>\nIn this notebook, questions, as well as their titles and tags will be analyzed by text analysis and a topic modeling will be performed with NMF (Non-negative Matrix factorization).<br><br>\nDatasets related to this task are: <i>questions<\/i>, <i>tag_questions<\/i>, <i>tags<\/i> and <i>question_scores<\/i>.","cae1d9de":"The graph above showes that <b>\"Career path\"<\/b> is the most common topic, which makes almost one fourth of all the questions asked. Moreover 50% of all questions are regarding the topics <b>\"Career path\"<\/b>, <b>\"Medicine\"<\/b>, <b>\"Engineering\"<\/b>, <b>\"Budgetary issues\"<\/b> and <b>\"Computer science\"<\/b>.<hr>","7aa92b10":"<b>2) What is the average question score per topic?<\/b><br>\nScores are the votes that questions received. It shows the prevalence of the issue, which the question is about, and it can be interpreted as how well the questions (and the answers) helped the users. For example, if a question has a score of 5, then it can be said that it concerns 5 people and its answers can help 5 people. Moreover, scores also show the popularity of the issue.","53a1760e":"<h3>7.1) Generating TF-IDF Vectorizer <\/h3>","9f7fcff6":"<h2>2) Formatting tags <\/h2>","bdf0c7bc":"Tags used together in questions are analysed with Network Analysis on Gephi. For simplicity and interpretability purposes, nodes are filtered accourding to their weighted degrees. Below is the resulting network graph of the tags frequently used together.","fb8b2df4":"<h3>4.2) Processing text from questions, titles and tags<\/h3>","9593f806":"<b>3) What are the most frequent industry and headlines of professionals per topics?<\/b>","74927f2d":"The resulting topics are interpretable and are consistent with their respective keywords. Therefore, the model will be accepted.<br>\nHowever, some topics represent similar fields and might concern same group of professionals. Therefore, such topics are merged and are labeled as same.<br>\nChanges are listed above:\n<ul>\n    <li> Topic #3 (Computer science) & Topic #25 (Software development) &#8658; <b>Computer science<\/b><\/li>\n    <li> Topic #4 (Medicine) & Topic #18 (Medical doctor) &#8658; <b>Medicine<\/b>\n    <li> Topic #16 (Earnings) & Topic #17 (Debts) &#8658; <b>Budgetary issues<\/b><\/li>\n<\/ul>","6b4c0bc3":"<h2>5) Examining number of tags<\/h2>","0062db43":"<ul>\n    <li>The words <b>\"question\"<\/b> and <b>\"answer\"<\/b> are found in many questions, as they are referring to the texts.<\/li><br>\n    <li>The words <b>\"study\"<\/b>, <b>\"class\"<\/b>, <b>\"graduate\"<\/b>, <b>\"grad\"<\/b> and <b>\"university\"<\/b> are used mostly when students introduce themselves.<\/li><br>\n    <li>The words <b>\"interest\"<\/b>, <b>\"undecided\"<\/b>, <b>\"decide\"<\/b> and <b>\"decision\"<\/b> are frequently used when students explain their career questions. For example, they mostly describe their favorable career paths with \"interest\" and use \"undecided\" as a tag to specify their situation.<\/li><br>\n    <li>The word <b>\"manage\"<\/b> could be controversial as a stopword, as it carries valuable information. It indeed does. However, the model cannot differentiate between the uses of word \"manage\" in e.g. \"Managing people is my passion\" and \"How do you manage two jobs at the same time?\". In first use, it is meant as a career path, yet in the second it is meant as a verb with less information. This mostly happens with the verb form of the word, therefore only \"manage\" is added to the stopwords list and e.g. \"management\" continues to appear as an input.","5c076f45":"<img src=\"https:\/\/i.imgur.com\/yVu99su.jpg\" width=\"700\">","aee6d3d6":"Tokens extracted from questions, titles and tags are appended and main token list is generated.","5df91d83":"<h3>4.1) Preparing Stopwords<\/h3>","53024630":"Pre-defined stopwords from NLTK and SpaCy are used initially, however, further inspection showed that more stopwords are needed in order to achieve less overlapping topics. Therefore, following word sets are added to the stopwords set:\n<ul>\n    <li> 30 most common tokens in questions text<\/li>\n    <li> 25 most common nouns, according to Wikipedia[1] <\/li>\n    <li> 25 most common verbs, according to Wikipedia[1] <\/li>\n    <li> Words, which are used frequently due to the nature of the platform, such as \"question\", \"answer\" and \"interest\"<\/li>\n<\/ul><br>\n[1] : https:\/\/en.wikipedia.org\/wiki\/Most_common_words_in_English"}}