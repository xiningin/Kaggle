{"cell_type":{"d4c8a75c":"code","da6c7df3":"code","2f826808":"code","dd05026c":"code","d92d1fcf":"code","3974d587":"code","6aac1ea0":"code","7afb0a0b":"code","10831282":"code","1d7308a6":"code","5145bdaf":"code","3f203057":"code","234472c6":"code","997a3194":"code","723ae54b":"code","aed3ed3c":"code","8fe6c4e9":"code","91f838e2":"markdown","6a2a5b98":"markdown","8dc20d0c":"markdown","e2281906":"markdown","53c50343":"markdown","edd71f28":"markdown","e6add700":"markdown","0d5da085":"markdown","b051f81e":"markdown","878d70b9":"markdown","002b1970":"markdown","65c3e288":"markdown","978cf0f4":"markdown","63e2d306":"markdown","a9ccb0e9":"markdown"},"source":{"d4c8a75c":"#This project was part of the Microsoft Data Science Capstone, that was completed recently.\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport category_encoders as ce\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\n%matplotlib inline \npd.options.display.max_columns = None\n\nX_train = pd.read_csv(\"..\/input\/train_values.csv\")\ny_train = pd.read_csv(\"..\/input\/train_labels.csv\")\ntrain = pd.concat([X_train, y_train], axis=1)","da6c7df3":"train.count()","2f826808":"train.drop(\"row_id\", axis=1, inplace=True)\n\nreplace_dict = {\n    'msa_md': -1,\n    'state_code': -1,\n    'county_code': -1,\n    'occupancy': 3,\n    'preapproval': 3,\n    'applicant_ethnicity': [3, 4, 5],\n    \"applicant_race\": [6, 7, 8],\n    \"applicant_sex\": [3, 4, 5]\n}\n\ncat_cols_few = [\"loan_type\", \"property_type\", \"loan_purpose\", \"occupancy\", \"preapproval\",\n                \"applicant_ethnicity\", \"applicant_race\", \"applicant_sex\", \"co_applicant\"] \n\ncat_cols_many = [\"msa_md\", \"state_code\", \"county_code\", \"lender\"]\n\nnumerical_cols = [\"loan_amount\", \"applicant_income\", \"population\", \"minority_population_pct\",\n                 \"ffiecmedian_family_income\", \"tract_to_msa_md_income_pct\",\n                 \"number_of_owner-occupied_units\", \"number_of_1_to_4_family_units\"]\n\ntrain.replace(replace_dict, np.nan, inplace = True)\ntrain.count()","dd05026c":"train[\"accepted\"].value_counts().plot(kind='bar')\nplt.title('Accepted loan applications')\nplt.show()\ntrain[\"accepted\"].value_counts(normalize = 'index')\n","d92d1fcf":"train[numerical_cols].hist(figsize=(12,10), bins=20)\nplt.suptitle(\"Histograms of numerical values\")\nplt.show()\n\nprint(\"Skewness of numerical columns:\")\ntrain[numerical_cols].skew()","3974d587":"import math\nto_log = [\"loan_amount\", \"applicant_income\", \"number_of_owner-occupied_units\", \"number_of_1_to_4_family_units\"]\ntrain[to_log] = train[to_log].applymap(math.log)\n\ntrain[numerical_cols].hist(figsize=(12,10), bins=20)\nplt.suptitle(\"Histograms of numerical values\")\nplt.show()\n\nprint(\"Skewness of numerical columns after applying log function:\")\ntrain[numerical_cols].skew()","6aac1ea0":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfig, axes = plt.subplots(ncols = 2, nrows = 4, figsize = (12,14))\nfig.subplots_adjust(hspace = 0.4, wspace = 0.2)\nfig.suptitle(\"KDE plots of numerical features\")\n\nfor ax, col in zip(axes.flatten(), numerical_cols) :\n    sns.kdeplot(train[train[\"accepted\"] == 0][col], shade=\"True\", label=\"Not accepted\", ax = ax)\n    sns.kdeplot(train[train[\"accepted\"] == 1][col], shade=\"True\", label=\"Accepted\", ax = ax)\n    ax.set_xlabel(col)\n","7afb0a0b":"fig, axes = plt.subplots(ncols = 3, nrows = 3, figsize = (14,14))\nfig.subplots_adjust(hspace = 0.3, wspace = 0.3)\nfig.suptitle(\"Categorical features with low cardinality\")\n\nfor ax, col in zip(axes.flatten(), cat_cols_few) :\n    pd.crosstab(train[col], train[\"accepted\"]).plot(kind=\"bar\", ax = ax)\n    ax.set_xlabel(col)","10831282":"fig, axes = plt.subplots(ncols = 2, nrows = 2, figsize = (14,10))\nfig.subplots_adjust(hspace = 0.3, wspace = 0.3)\nfig.suptitle(\"Categorical features with high cardinality\")\n\nfor ax, col in zip(axes.flatten(), cat_cols_many) :\n    sns.kdeplot(train[train[\"accepted\"] == 0][col], shade=\"True\", label=\"Not accepted\", ax = ax)\n    sns.kdeplot(train[train[\"accepted\"] == 1][col], shade=\"True\", label=\"Accepted\", ax = ax)\n    ax.set_xlabel(col)\n","1d7308a6":"train[\"minority_population\"] = (train[\"minority_population_pct\"] \/ 100) * (train[\"population\"])\ntrain[\"tract_family_income\"] = (train[\"tract_to_msa_md_income_pct\"] \/100) * (train[\"ffiecmedian_family_income\"])\n\ntrain[\"minority_population\"] = train[\"minority_population\"].apply(math.log)","5145bdaf":"new_cols = [\"minority_population\", \"tract_family_income\"]\n\nfig, axes = plt.subplots(ncols = 2, nrows = 1, figsize = (14,5))\nfig.subplots_adjust(hspace = 0.5, wspace = 0.3)\nfig.suptitle(\"New features\")\n\nfor ax, col in zip(axes.flatten(), new_cols) :\n    sns.kdeplot(train[train[\"accepted\"] == 0][col], shade=\"True\", label=\"Not accepted\", ax = ax)\n    sns.kdeplot(train[train[\"accepted\"] == 1][col], shade=\"True\", label=\"Accepted\", ax = ax)\n    ax.set_xlabel(col)","3f203057":"plt.figure(figsize=(16,12))\nsns.heatmap(train.corr().round(decimals=2), annot=True)\nplt.title(\"Correlation heatmap\")\nplt.show()","234472c6":"to_log = [\"loan_amount\", \"applicant_income\", \"number_of_owner-occupied_units\",\n          \"number_of_1_to_4_family_units\", \"minority_population\"]\n\nto_drop = [\"row_id\", \"number_of_1_to_4_family_units\",\n           \"occupancy\", \"preapproval\", \"county_code\"]\n\nnum_cols = [\"loan_amount\", \"applicant_income\", \"population\", \"minority_population_pct\",\n            \"ffiecmedian_family_income\", \"tract_to_msa_md_income_pct\",\n            \"number_of_owner-occupied_units\"]\n\ncat_cols_few = [\"loan_type\", \"property_type\", \"loan_purpose\",\n            \"applicant_ethnicity\", \"applicant_race\",\n            \"applicant_sex\", \"co_applicant\"]\n\ndef prepare_data(df):\n    \n    df[\"co_applicant\"] = df[\"co_applicant\"].astype(\"int8\")\n    df.replace(replace_dict, np.nan, inplace = True)\n    \n    for col in num_cols:\n        df[col].fillna(df[col].median(), inplace=True)\n        \n    for col in cat_cols_few:\n        df[col].fillna(df[col].mode()[0], inplace=True)\n        \n    df[\"minority_population\"] = (df[\"minority_population_pct\"] \/ 100) * (df[\"population\"])\n    df[\"tract_family_income\"] = (df[\"tract_to_msa_md_income_pct\"] \/ 100) * (df[\"ffiecmedian_family_income\"])\n\n    df[to_log] = df[to_log].applymap(math.log)\n    \n    to_drop.extend([\"minority_population_pct\", \"population\",\n                    \"ffiecmedian_family_income\", \"tract_to_msa_md_income_pct\"])\n    df.drop(to_drop, axis=1, inplace=True)\n    \n    df = pd.get_dummies(df, columns = cat_cols_few)\n    \n    return df","997a3194":"X_train = prepare_data(X_train)\n\nce_target = ce.TargetEncoder(cols = [\"lender\", \"msa_md\", \"state_code\"], smoothing = 5, return_df = True)\nX_train = ce_target.fit_transform(X_train, y_train[\"accepted\"])","723ae54b":"X_train, X_test, y_train, y_test = train_test_split(X_train.values, y_train[\"accepted\"].values, test_size=0.3, random_state=0)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","aed3ed3c":"model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=0.6, gamma=0, learning_rate=0.02, max_delta_step=0,\n       max_depth=8, min_child_weight=8, missing=None, n_estimators=600,\n       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n       reg_alpha=0.2, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=True, subsample=0.7)\n\nmodel.fit(X_train, y_train)\nprediction = model.predict(X_test)","8fe6c4e9":"print('The accuracy is:', metrics.accuracy_score(y_test, prediction))\nprint(\"\\n\")\nprint(\"Classification Report:\")\nprint(metrics.classification_report(y_test, prediction))\n\nsns.heatmap(confusion_matrix(y_test, prediction), annot=True)\nplt.title(\"Confusion Matrix\")\nplt.show()","91f838e2":"We are now going to visually examine class separation on the categorical features. I divided those features in two groups, depending on the number of categories, i.e. low and high cardinality. \u0399 decided to do this for a number of reasons, including the fact that features with low cardinality can be visualized using bar plots, while this isn't appropriate for those with high cardinality. As we can see, most of the low-cardinality features provide information about class separation, except for *occupancy*. It must be noted that there are some evident signs of discrimination, specifically based on sex and race. The mortgage applications of white citizens had a higher approval rate compared to african americans. Furthermore, women had a lower approval rate compared to men. Making inferences and trying to identify any causes for those differences, is beyond the scope of this analysis. Regardless of that, using data that is influenced by prejudice and discrimination, raises ethical concerns that will be examined later in further detail.","6a2a5b98":"# Exploratory Data Analysis\nFirst of all, we are going to examine the total number of non-null values for each feature.","8dc20d0c":"The classes of our dataset are almost perfectly balanced. This is helpful, as it simplifies the creation of the machine learning model.","e2281906":"We can see that after applying the logarithmic function, the numerical features are significantly less skewed, and their distribution is closer to normal as expected.\n","53c50343":"As we can see, some of the features have a fair number of missing values. Furthermore, we can read on the dataset description that some categorical features include categories that correspond to missing values, e.g. the -1 category of *msa_md* column indicates a missing value. We are going to remove those, so we have a clearer view of our dataset.","edd71f28":"After removing the aforementioned categories, we see that certain features have significantly fewer values. This has to be dealt with before we create the binary classifier, as machine learning models can't accept null values on a dataset.","e6add700":"The correlation heatmap helps us visually identify any features that are highly correlated. Including those in the binary classifier is redundant, because the information they provide is highly overlapping, so they will be discarded from the final dataset. We can see that the new features we created earlier, are highly correlated with those that comprise them. This is fairly self-evident and reasonable, so I decided to drop the original features.","0d5da085":"As we can see on the classification report, as well as the confusion matrix, the classifier is fairly accurate, but outputs a number of false positive and false  negative values as well.","b051f81e":"# Machine Learning Modelling\nFor the creation of the binary classifier, I experimented with various algorithms and techniques. First of all I tested the accuracy of some typical classification algorithms, such as Logistic Regression, K-Nearest Neighbors and Support Vector Machines. The accuracy I got with them was below 70%, and that was unacceptable. I also tried more sophisticated algorithms and machine learning libraries, like LightGBM and Keras\/Tensorflow. Eventually, I got the highest accuracy (about 72.5%) with the XGBoost library. I used the XGBoost Classifier for the main modelling, as well as various functions of the scikit-earn library for data preprocessing, metrics, and model selection.\nFirst of all, I created a *prepare_data* function that dealt with missing values by imputing them with the median\/mode value of each feature, depending on whether it was a numerical or categorical one. It also applied the logarithmic function to the skewed numerical features, as mentioned earlier. Afterwards, I applied one-hot encoding to the categorical features, as it is typical in machine learning. I ran into a problem though, as the encoded high-cardinality features resulted in the dramatic increase of the dataset size.\nThis was normal, as thousands of features were added to it, but led to memory problems and increased the time needed to train the machine learning models. After doing some research, I decided to apply target encoding to the high-cardinality features, i.e. *lender*, *msa_md* and *state_code*, instead of one-hot encoding, thus solving the problem. After that, I used the StandardScaler class of scikit-learn to scale my dataset, as well as the train_test_split function to split the dataset and evaluate the classifier's performance","878d70b9":"# Ethical Concerns And Conclusion\nThe accuracy of our binary classifier is satisfactory. It could possibly be improved if more useful features were added on the dataset, or by exploring the performance of other algorithms and techniques. As it was mentioned before though, our data analysis indicates that there is discrimination against specific groups of people in the approval of mortgages, i.e. based on race, nationality and sex. Including that information in a real-world data science project, would possibly reinforce and exacerbate that problem. I would personally be reluctant to do it, as it is ethically questionable. Of course in a business environment, that decision might not be mine to make, so I would discuss that issue with a manager, or the head of my department. Regardless, our goal in this project was simply to maximize the accuracy of the classifier in the context of an online course, so all of the relevant features were included in the machine learning model.","002b1970":"I decided to create two new features, i.e. *minority_population* and *tract_family_income*. The first feature is the product of *minority_population_pct* and *population*, and contains the actual number of people that belong in a minority group. The second is the product of *tract_to_msa_md_income_pct* and *ffiecmedian_family_income*, and contains the tract median family income in dollars. It is evident from the KDE plots of those new features, that they both provide useful information about class separation, so they will be used in the binary classifier.","65c3e288":"Next, we're going to examine the KDE plots of the numerical features. I created different plots for each class of the label, so we can visually determine which features provide more information about the separation of those classes. As we can see *loan_amount*, *applicant_income*, *minority_population_pct* and *ffiecmedian_family_income* are the most important numerical features, and will be used for the creation of the binary classifier.","978cf0f4":"# Executive Summary\nIn this report I am going to present my analysis of the HMDA mortgage application dataset, as well as the machine learning model I created for the aforementioned data. The dataset includes information about 500000 mortage applications. After doing basic exploratory analysis, visualization, as well as some data cleaning and processing, I managed to identify the most essential features of the dataset, i.e. those that provided the largest amount of information about the separation of the two label classes (mortgage accepted\/rejected). Those features were subsequentely used for the creation of a binary classifier capable of predicting whether an application will be accepted or not, with an accuracy of 72%. The most important features of the dataset are the following:\n\n* *lender* - A categorical with no ordering indicating which of the lenders was the authority in approving or denying this loan\n* *loan_amount* - Size of the requested loan in thousands of dollars\n* *msa_md* - A categorical with no ordering indicating Metropolitan Statistical Area\/Metropolitan Division \n* *state_code* - A categorical with no ordering indicating the U.S. state\n* *applicant_income* - In thousands of dollars\n* *number_of_owner-occupied_units* - Number of dwellings, including individual condominiums, that are lived in by the owner\n* *minority_population* - Number of people that belong in a minority group, a new feature that was created for the purposes of this analysis\n* *tract_family_income* - The tract median family income in dollars, another feature that was created for the purposes of this analysis","63e2d306":"For the high-cardinality categorical features, I decided to use KDE plots. We can see that *lender* and *state_code* provide adequate information about the separation of the target classes.","a9ccb0e9":"As we can see on the histograms, as well as the list of skewness values, a few of the numerical features, especially *loan_amount* and *applicant_income* are highly skewed. We are going to fix that by applying the logarithmic function on them, as it is helpful to have features with low skewness and a distribution that is close to normal. This will enable us to create more meaningful visualizations, as well as a better machine learning model."}}