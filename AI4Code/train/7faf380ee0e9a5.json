{"cell_type":{"c78cb9e4":"code","8a9bf0b7":"code","61e9cb40":"code","9e837b07":"code","0d5899bb":"code","86288115":"code","16458afe":"code","9a4767b2":"code","ac96dd10":"code","723a6e2c":"code","dc58dd51":"code","e9753206":"code","45ca27f7":"markdown","7a11220e":"markdown","dfd0251f":"markdown","a18d10d2":"markdown","07c67250":"markdown","c7e1702a":"markdown","e873e187":"markdown"},"source":{"c78cb9e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8a9bf0b7":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\n\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier, Pool\nfrom xgboost import XGBClassifier\n\nimport optuna\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","61e9cb40":"df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv\")\nss = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")","9e837b07":"df[\"kfold\"] = -1\ndf = df.sample(frac=1).reset_index(drop=True)\ny = df.target\nkf = StratifiedKFold(n_splits=5)\nfor f, (t_,v_) in enumerate(kf.split(X=df,y=y)):\n  df.loc[v_,\"kfold\"] = f","0d5899bb":"df.shape,test.shape","86288115":"df.columns","16458afe":"cat_features = [f\"feature_{i}\" for i in range(0,50)]","9a4767b2":"cat = CatBoostClassifier(task_type='GPU',\n                         iterations=3000,\n                         loss_function='MultiClass',\n                         random_state = 42,\n                         early_stopping_rounds=500,\n                         verbose=100)","ac96dd10":"logloss = []\ncat_pred = 0\nfor f in range(5): # Looping around 5 folds\n    \n    #Splitting the data into train and validation set\n    train = df[df.kfold!= f].reset_index(drop=True) \n    valid = df[df.kfold== f].reset_index(drop=True)\n    \n    #Creating X_train and y_train\n    X_train = train.drop([\"id\",\"target\", \"kfold\"], axis=1)\n    y_train = train.target\n    X_valid = valid.drop([\"id\",\"target\", \"kfold\"], axis=1)\n    y_valid = valid.target\n    X_test = test.drop([\"id\"], axis=1)\n    \n    #Creating pool\n    train_pool = Pool(data=X_train,label=y_train,cat_features=cat_features)\n    valid_pool = Pool(data=X_valid,label=y_valid,cat_features=cat_features)\n    \n    #Fitting the model\n    cat.fit(train_pool, eval_set=valid_pool,verbose=100)\n    \n    #Predicting for valid and test datasets\n    valid_preds = cat.predict_proba(X_valid)\n    cat_pred += cat.predict_proba(X_test)\/5\n    \n    #Calculating log loss\n    logloss.append(log_loss(y_valid,valid_preds))\n    \nprint(logloss)\nprint(sum(logloss)\/len(logloss))","723a6e2c":"lgbm = LGBMClassifier(random_state=42)\nlogloss = []\nlgbm_pred = 0\nfor f in range(5): # Looping around 5 folds\n    \n    #Splitting the data into train and validation set\n    train = df[df.kfold!= f].reset_index(drop=True) \n    valid = df[df.kfold== f].reset_index(drop=True)\n    \n    #Creating X_train and y_train\n    X_train = train.drop([\"id\",\"target\", \"kfold\"], axis=1)\n    y_train = train.target\n    X_valid = valid.drop([\"id\",\"target\", \"kfold\"], axis=1)\n    y_valid = valid.target\n    X_test = test.drop([\"id\"], axis=1)\n    \n    #Creating pool\n    #train_pool = Pool(data=X_train,label=y_train,cat_features=cat_features)\n    #valid_pool = Pool(data=X_valid,label=y_valid,cat_features=cat_features)\n    \n    #Fitting the model\n    lgbm.fit(X_train,y_train)\n    \n    #Predicting for valid and test datasets\n    valid_preds = lgbm.predict_proba(X_valid)\n    lgbm_pred += lgbm.predict_proba(X_test)\/5\n    \n    #Calculating log loss\n    logloss.append(log_loss(y_valid,valid_preds))\n    \nprint(logloss)\nprint(sum(logloss)\/len(logloss))","dc58dd51":"avg_pred = []\navg_pred.append((cat_pred[:,0] + lgbm_pred[:,0]) \/ 2)\navg_pred.append((cat_pred[:,1] + lgbm_pred[:,1]) \/ 2)\navg_pred.append((cat_pred[:,2] + lgbm_pred[:,2]) \/ 2)\navg_pred.append((cat_pred[:,3] + lgbm_pred[:,3]) \/ 2)","e9753206":"ss[\"Class_1\"] = avg_pred[0]\nss[\"Class_2\"] = avg_pred[1]\nss[\"Class_3\"] = avg_pred[2]\nss[\"Class_4\"] = avg_pred[3]\nss.to_csv(\"\/kaggle\/working\/cat_lgbm_pred.csv\", index=False)","45ca27f7":"# LGBM","7a11220e":"Thank you","dfd0251f":"Taking the average of both the predictions","a18d10d2":"# CATBOOST","07c67250":"# Blending","c7e1702a":"Read all the required datasets","e873e187":"Creating folds using Startified kfold"}}