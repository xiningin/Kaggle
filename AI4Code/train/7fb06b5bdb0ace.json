{"cell_type":{"76e95948":"code","8b1bca41":"code","0343c4fb":"code","3391466e":"code","119594a1":"code","1b45b75e":"code","96778bae":"code","b67b1eb5":"code","eb7d476c":"code","f5fee13a":"code","0355ff10":"code","33c048b4":"code","61725f7b":"code","8a49cb40":"markdown","3ee00aba":"markdown","cb37184d":"markdown","0fb1e8e2":"markdown","021fecb3":"markdown","a6c423d2":"markdown","1f0278af":"markdown","c3072ff0":"markdown","a3de2a67":"markdown","73721a2d":"markdown","f7aef1cb":"markdown","1cc93c94":"markdown","e8013a97":"markdown","6953de7f":"markdown","cbf7cbc3":"markdown","e4dbfcd4":"markdown","ca176f96":"markdown","387d728d":"markdown","caf54a5a":"markdown","d61d40b6":"markdown","b72a1b34":"markdown","9db86bc0":"markdown"},"source":{"76e95948":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nnp.random.seed(0)\n\ndef cat_to_num(x):\n    try:\n        return int(x.replace(',000', '').replace('+', '').replace('< 1', '0').replace('%', '-').replace(' year', '-').strip().split('-')[0])\n    except:\n        return -1\n\ndf = pd.read_csv('..\/input\/multipleChoiceResponses.csv', low_memory=False, header=[0,1])\ncolumn_descriptions = [col[1] for col in df.columns if col[0] != 'Q9']\ndf.columns = [col[0] for col in df.columns]\n\ndef salary_stats(data, title):\n    dfg = data.groupby('Q9')['Q1'].count().reset_index()\n    dfg.columns = ['Salary range','# of respondents']\n    dfg['sort'] = dfg['Salary range'].apply(cat_to_num)\n    dfg['Salary range'] = dfg['Salary range'].apply(lambda x: 'Unknown' if 'not' in x else x)\n    dfg = dfg.sort_values('sort')\n    plt.figure(figsize=(12, 5))\n    ax = sns.barplot(x='Salary range', y='# of respondents', data=dfg)\n    for item in ax.get_xticklabels():\n        item.set_rotation(90)\n    plt.title(title)\n    plt.tight_layout()\n    plt.show()\nsalary_stats(df, 'Number of respondents by salary ranges (USD per year)')","8b1bca41":"salary_stats(df[df['Q6'] == 'Data Scientist'], 'Number of respondents by salary ranges (USD per year) - Job title: Data Scientist')","0343c4fb":"import lightgbm as lgb\nfrom sklearn.preprocessing import LabelEncoder\n\ndff = df.copy() # copy version to use in ML model\nfor col in ['Q2','Q8','Q9','Q23','Q25','Q43','Q46']: # convert ranges to numerical\n    dff[col] = dff[col].apply(cat_to_num).replace(-1, np.nan)\ndff = dff[~dff['Q9'].isnull()] # leave only entries where salary is defined\n\ncat_vars = [f for f in dff.columns if dff[f].dtype == 'object']\nfor col in cat_vars: # label-encode categorical variables\n    lbl = LabelEncoder()\n    dff[col] = lbl.fit_transform(dff[col].values.astype('str'))\n\nparams = {\n    'objective':'regression',\n    'metric':'rmse',\n    'nthread':4,\n    'learning_rate':0.08,\n    'num_leaves':31,\n    'colsample_bytree':0.9,\n    'subsample':0.8,\n    'max_depth':5,\n    'verbose':-1\n}\n    \ncols = [f for f in dff.columns if f != 'Q9']\ntrain_df = dff[cols]\ntrain_x = lgb.Dataset(train_df, dff['Q9'].values)\nclf = lgb.train(params, train_x, 150)\n\n# rename to more reasonable names\ntrain_dfx = train_df[cols].rename({'Q3':'Country','Q2':'Age','Q6':'Job title','Q8':'Job experience',\n                           'Q7':'Industry','Q25':'ML methods experience','Q10':'Employer uses ML',\n                           'Q1':'Gender','Q23':'Time spent coding','Q42_Part_1':'Measures success by revenue',\n                           'Q5':'Undergraduate majors','Q35_Part_3':'ML training at work',\n                           'Q32':'Type of the data','Q24':'Experience in analysing data',\n                           'Q43':'Exploring unfail bias','Q7_OTHER_TEXT':'Industry - other',\n                           'Q34_OTHER_TEXT':'Most time in ML devoted to - other','Q12_Part_1_TEXT':'Tools used for data analysis',\n                           'Q1_OTHER_TEXT':'Gender - other','Q35_Part_1':'Self-taught ML','Q6_OTHER_TEXT':'Job title - other',\n                           'Q15_Part_2':'Experience with Amazon Web Services','Q11_Part_4':'Builds ML prototypes',\n                           'Q27_Part_1':'Experience with AWS Elastic Compute Cloud (EC2)','Q33_Part_9':'Uses GitHub',\n                           'Q13_Part_9':'Uses Notepad++','Q11_Part_2':'Builds ML services','Q11_Part_1':'Analyzes data',\n                           'Q13_Part_11':'Uses vim','Q30_Part_24':'Uses Big Data products'},axis=1)\ntrain_cols = train_dfx.columns\n\ndef display_importances(clf, feats, importance_type='split', title='Salary'):\n    feature_importance_df = pd.DataFrame()\n    feature_importance_df[\"feature\"] = list(feats)\n    feature_importance_df[\"importance\"] = clf.feature_importance(importance_type=importance_type)\n    cols = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:22].index\n    best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n    plt.figure(figsize=(12, 8))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM features importance for predicting {}'.format(title))\n    plt.tight_layout()\n    plt.show()\ndisplay_importances(clf, train_cols, 'split')","3391466e":"import shap\nshap.initjs()\nshap_values = shap.TreeExplainer(clf).shap_values(train_dfx)\nshap.summary_plot(shap_values, train_dfx)","119594a1":"def display_stats_by(data, col, col_name, sort='desc', height=10):\n    dfg = data.groupby(col)['Salary'].mean().reset_index()\n    if sort == 'asc':\n        dfg = dfg.sort_values('Salary').head(80)\n    if sort == 'desc':\n        dfg = dfg.sort_values('Salary',ascending=False).head(80)\n    if sort == 'col_to_num':\n        dfg['sort'] = dfg[col].apply(cat_to_num)\n        dfg = dfg.sort_values('sort').head(80).drop('sort', axis=1)\n    dfg['Salary'] = np.round(dfg['Salary']*1000).astype(int)\n    dfg.columns = [col_name,'Average salary']\n    plt.figure(figsize=(12, height))\n    ax = sns.barplot(y=\"Average salary\", x=col_name, data=dfg)\n    for item in ax.get_xticklabels():\n        item.set_rotation(90)\n    plt.title('Average salary by ' + col_name)\n    plt.tight_layout()\n    plt.show()\ndf['Salary'] = df['Q9'].apply(cat_to_num)\ndisplay_stats_by(df, 'Q3', 'Country')","1b45b75e":"display_stats_by(df[df['Q6'] == 'Data Scientist'], 'Q3', 'Country - Job title: Data Scientist')","96778bae":"display_stats_by(df, 'Q2', 'Age', False, 5)\ndisplay_stats_by(df[df['Q6'] == 'Data Scientist'], 'Q2', 'Age - Job title: Data Scientist', False, 5)","b67b1eb5":"display_stats_by(df, 'Q6', 'Job title', height=7)","eb7d476c":"display_stats_by(df, 'Q8', 'Job experience in current role', 'col_to_num', 5)\ndisplay_stats_by(df[df['Q6'] == 'Data Scientist'], 'Q8', 'Job experience in current role - Job title: Data Scientist', 'col_to_num', 5)","f5fee13a":"display_stats_by(df, 'Q25', 'ML methods experience', 'asc')\ndisplay_stats_by(df[df['Q6'] == 'Data Scientist'], 'Q25', 'ML methods experience - Job title: Data Scientist', 'asc')","0355ff10":"display_stats_by(df, 'Q7', 'Industry', height=8)\ndisplay_stats_by(df[df['Q6'] == 'Data Scientist'], 'Q7', 'Industry - Job title: Data Scientist', height=8)","33c048b4":"print('Unique values of \"Time from Start to Finish (seconds)\":',df['Time from Start to Finish (seconds)'].unique().shape[0])","61725f7b":"train_x2 = lgb.Dataset(train_df, dff['Q9'].shift(1).values)\nclf2 = lgb.train(params, train_x2, 150)\ndisplay_importances(clf2, train_cols, 'split', title='Random noise')","8a49cb40":"## Job title\n*Select the title most similar to your current role (or most recent title if retired): - Selected Choice*\n\nJob is third feature by importance for salary size in both - feature importance and also SHAP estimation. Let's inspect this feature by ploting average salary size by job title.","3ee00aba":"Trend looks similar to Job experience - more years spent with using ML methods increases chances for good salary.\n\nConclusion on best option to do for maximizing salary by this feature: **gain experience working with ML methods**","cb37184d":"## ML methods experience\n*For how many years have you used machine learning methods (at work or in school)?*\n\nML experience is #5 feature for SHAP and #7 in feature importances list. Let's inspect this feature by ploting average salary size by ML methods experience:","0fb1e8e2":"Not surprisingly - job title matters a lot. Students or assistants get way smaller salary if compared to e.g. managers and chief officiers. In general Data Scientist seems to be quite good job title for receiving big salary, although some other jobs shows way better earning perspectives.\n\nConclusion on best option to do for maximizing salary by this feature: **try to become Chief Officer or Principal Investigator**","021fecb3":"This feature has 6522 unique values while all other Survey questions have very limited choices for response.\nHigh cardinality of this feature allows model to find some small sub-sets of those 6522 values which correlates with salary and makes model to perform splits on these sub-sets resulting in overfitting and high importance of this feature.\n\nIn order to prove this assumption let us make a quick experiment - let's fit the same model on some random target. We'll make random target by simply shifting real target values by one - in this case distribution and other characteristics of data remains the same, but target values are now randomly assigned to different data rows. Let's fit this model and display importances.","a6c423d2":"Relative size of 0-10'000 range here is smaller, but overall salaries still differs very significantly - even within the same Job title. So what are the most important reasons for such big differences and what can we as employees do to raise our chances to be on the right side of this graph? What to do to earn more? Let's find out step by step.\n\n## What impacts salary size?\n\nThis is the first question to be answered and there are many ways how to answer it. We could go through all the columns of survey data and check how they correlate with salary. Or we could make an educated guess using our domain expert knowledge to choose the most imporant ones. But I prefer more ML related approach. Every question can be answered by a ML model, given enough training data. So let's build a model to answer this one.\n\nWe'll build a regression model predicting salary size from all other columns available. Then we'll inspect what features model finds to be the most important ones. There are 2 things you should know about this model:\n* it is not optimized as we don't care much about how accurate it is - all we need is the list of feature imporances;\n* salary (and other range-based categorical variables) is converted to numerical by taking first part of the range (e.g. 10-20000 becomes 10).","1f0278af":"As expected, *\"Time from Start to Finish (seconds)\"* feature is the most important on fitting random target because of it's high cardinality, which allows model to find \"useful\" sub-sets even for fitting to noise. Btw, this is the reason why I also always double-check cases when some very high-cardinality feature turns out to be on the top of importance list in e.g. Kaggle competitions.\n\nConclusion on best option to do for maximizing salary by this feature: **beware of features with very high cardinality ;)**","c3072ff0":"# Summary\n\nThis is the second version of notebook, exploring 7 most important features which influence size of the salary in general and more specifically for Data Scientists.\n\nMain conclusions so far:\n* Country seems to be the most dominant factor influencing salary - best option for Data Scientist (and also in general) is to live in Switzerland or USA.\n* Work experience in general and also more specifically experience with ML is also very important.\n* Job title matters a lot for salary size. *\"Data Scientist\"* role is quite well paid if compared to other jobs, although there are also more profitable positions.\n* High cardinality features can easily result in overfitting, especially if other features have very low cardinality.\n\nMost of the features we've explored so far all were logical and quite predictable. More interesting and surprising features are to be explored in next versions.","a3de2a67":"It is obvious - salary is highly infuenced by the country you live in and difference can be really huge. So relocation to e.g. United States of America or Switzerland is one option to consider to improve chances for bigger salary.\nLet's take a look at the same plot for *\"Data Scientist\"* Job title.","73721a2d":"In general it looks that biggest chances for good salary is for employees in *Hospitality\/Entertainment\/Sports* industry. It is even more noticeable for Job title *\"Data Scientist\"*.\n\nConclusion on best option to do for maximizing salary by this feature: **consider working in *Hospitality\/Entertainment\/Sports* industry**","f7aef1cb":"## Industry\n*In what industry is your current employer\/contract (or your most recent employer if retired)? - Selected Choice*\n\nIndustry is also very important - this is #6 feature in LightGBM feature importances list and #7 in SHAP estimation. Let's inspect this feature by ploting average salary size by Industry.","1cc93c94":"## Country\n*In which country do you currently reside?*\n\nCountry is the top influencing feature for salary size in both - LightGBM feature importance and also SHAP estimation. Let's inspect this feature by ploting average salary size by country.","e8013a97":"Country to live in is very important also for Data Scientists and salary differences looks to be even bigger here. \n\nConclusion on best option to do for maximizing salary by this feature: **relocate to USA or Switzerland**","6953de7f":"As we see, salaries can be really different and ranges from less than 10'000 to 500'000+ USD per year. Most of the respondents, who specified their salary, falls in 0-10'000 range. Probably big part of them could be students. Let's plot the same graph more specifically for only those respondents with Job title *\"Data Scientist\"*.","cbf7cbc3":"Main trend is quite logical - salary increases by age. The more experience and knowledge you gain, the more evaluated you are by employers. Increase seems to be litle bit more smooth in general than for Data Scientists. I guess there's nothing special we can do regarding this feature to optimize on salary - just have to be patient and gain experience. Salary most likely will increase gradually year by year.\n\nConclusion on best option to do for maximizing salary by this feature: **gain experience**","e4dbfcd4":"## Time from Start to Finish (seconds)\n*Average Time Spent Taking the Survey*\n\nThis one looks really suspicious. Time of completing the Survey seems to be one of the top features for salary size - it is #4 feature in LightGBM feature list and #8 in SHAP estimation. Would that mean our salary depends on how fast we are answering Kaggle Surveys? Of course not - model is simply overfitting on this feature. One of most likely reasons for overfitting is very high cardinality of this feature relatively to others. ","ca176f96":"## Age\n*What is your age (# years)?*\n\nAge is the second most important feature for salary size in both - LightGBM feature importance and also SHAP estimation. Let's inspect this feature by ploting average salary size by age as usual - globally and for *\"Data Scientists\"* Job title.","387d728d":"No surprises here also - more work experience in current role gives more chances on good salary.\n\nConclusion on best option to do for maximizing salary by this feature: **gain work experience**","caf54a5a":"There is one more option to evaluate features importance - we'll plot also summary by SHAP [1], which shows what impact features have on the value predicted by model. The features importance plot above shows just the fact that one or another feature is important for model to make predictions, but it doesn't give us a clue, what exactly impact this feature has (wether it increases or decreases predictions). SHAP can show us that direction of impact - here also more important features are on the top of the plot.","d61d40b6":"**Changes from Version 1:** Explored impact of *\"Time from Start to Finish (seconds)\"* feature on Salary-predicting model resulting in conclusion that high cardinality features can easily overfit and this feature doesn't influence the salary.\n\n## How to earn more?\n\nThis is one of the most important questions for every employee. Of course, salary is not the only aspect to consider when choosing your job. Things like interesting duties, prestigious company name, good boss, friendly colleagues, etc. still matters. But that's already a psychology and not in scope of this notebook. Here we'll take a very pragmatic look at the financial side of being Data Scientist through the possibilities given us by *Kaggle ML & DS Survey* dataset.\n\nObjective of this notebook is to explore two main questions:\n* what are the most important aspects defining how big is our salary in Data Science and related fields;\n* what things to consider if we want to raise chances of getting paid more.\n\nWe'll take a look at these questions in two scopes - globally and for a specific group of respondents, who specified Job title *\"Data Scientist\"*.\n\nFirst let's take a look, what are salary ranges marked by Respondents.","b72a1b34":"## Job experience\n*How many years of experience do you have in your current role?*\n\nJob experience is #4 feature for SHAP and #5 in LightGBM feature importances list. Let's inspect this feature by ploting average salary size by job experience:","9db86bc0":"The tops of both plots are similar and most of the top features seems logical - it's not a surprise that salary differs a lot in different countries or that job experience helps to get bigger salary. There are also some more interesting\/surprising features with noticeable impact - like experience with AWS or usage of GitHub. \n\nNow let's go through some of the most impacting and interesting features and see, what exactly impact do they have and is there something for us to consider to use this knowledge in our favour to raise chances of being paid better."}}