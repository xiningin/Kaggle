{"cell_type":{"b3ad9f42":"code","1c4b835a":"code","2ee49812":"code","65a71734":"code","bb243bd3":"code","cf43ade8":"code","b61e079c":"code","d0cefd53":"code","f92a271c":"code","8341796f":"code","2dc8d11e":"code","940c6f91":"code","4176f0c2":"code","16a77497":"code","abc09cd1":"code","ebfdb3d0":"code","eeed9812":"code","e0db1fc5":"code","8a7643e2":"code","b7468fbc":"code","4c7d764b":"code","47069e40":"code","921816d0":"code","9ed6ed86":"code","0eacf2a0":"code","c23a054e":"code","14f887da":"code","d705dbc3":"code","6d358094":"code","6d59fcd5":"code","724f17fd":"code","edbbe9ec":"code","120eb48f":"code","860368d6":"code","f3a2f10e":"code","a007ee5a":"code","9782846c":"code","7c6eac33":"code","4160cd92":"code","94e93976":"code","0a6c0126":"code","ff115985":"code","c5c71786":"code","7972f194":"code","719c0059":"code","8bf145a8":"markdown","19cd6274":"markdown","596a84a8":"markdown","3e04e791":"markdown","901867f9":"markdown","f13967fb":"markdown","37977e59":"markdown","3b845f32":"markdown","1a147a9a":"markdown","1704558a":"markdown","d435b002":"markdown","129bd421":"markdown","88ecd2dd":"markdown","cfbdb63a":"markdown","0a362993":"markdown","1db7c3d3":"markdown","b94daeef":"markdown","4d69d988":"markdown","de7d2081":"markdown","005dec15":"markdown","311168da":"markdown","a44668ce":"markdown","9a5ae561":"markdown","2e05dced":"markdown","8bd27efe":"markdown","9185b848":"markdown","0e9a097c":"markdown","912e06fc":"markdown","a5254345":"markdown"},"source":{"b3ad9f42":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1c4b835a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nimport sklearn.metrics as metrics\nimport math\n\n\ntrain_path=\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\"\ntest_path=\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\"\n\ntrain=pd.read_csv(train_path)\ntest=pd.read_csv(test_path)","2ee49812":"#Copying Train And Test Datatset\nc_train=train.copy()\nc_test=test.copy()","65a71734":"train.shape","bb243bd3":"#To see the null values using a heatmap\n#plt.figure(figsize = (16,7))\n#sns.heatmap(train.isnull(),yticklabels=False,cbar=False)","cf43ade8":"c_train[\"train\"]=1\nc_test[\"train\"]=0\ndf=pd.concat([c_train,c_test],axis=0,sort=False)\n#Vertically Concat","b61e079c":"df.shape","d0cefd53":"df","f92a271c":"NAN=[(k,df[k].isnull().mean()*100) for k in df]\nprint(NAN)","8341796f":"#Making a new dataframe\nNAN=pd.DataFrame(NAN,columns=[\"column_name\",\"percentage\"])","2dc8d11e":"NAN=NAN[NAN.percentage>50]\nNAN.sort_values(\"percentage\",ascending=False)","940c6f91":"#Dropping All The Above Features\ndf=df.drop([\"PoolQC\",\"MiscFeature\",\"Alley\",\"Fence\"],axis=1)","4176f0c2":"cat_columns = df.select_dtypes(include=['O'])\nnum_columns =df.select_dtypes(exclude=['O'])","16a77497":"null_cat_columns=cat_columns.isnull().sum()\nnull_cat_columns","abc09cd1":"columns_None = ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n                'GarageType','GarageFinish','GarageQual','FireplaceQu','GarageCond']\ncat_columns[columns_None]= cat_columns[columns_None].fillna('None')","ebfdb3d0":"mode_cols = ['MSZoning','Utilities','Exterior1st','Exterior2nd',\n             'MasVnrType','Electrical','KitchenQual','Functional','SaleType']\n\n#fill missing values for each column (using its own most frequent value)\ncat_columns[mode_cols] = cat_columns[mode_cols].fillna(cat_columns.mode().iloc[0])","eeed9812":"null_cat_columns=cat_columns.isnull().sum()\nnull_cat_columns","e0db1fc5":"null_num_columns=num_columns.isnull().sum()\nnull_num_columns","8a7643e2":"print((num_columns[\"GarageYrBlt\"]).median())\nprint(num_columns[\"LotFrontage\"].median())","b7468fbc":"num_columns['GarageYrBlt'] = num_columns['GarageYrBlt'].fillna(1979)\nnum_columns['LotFrontage'] = num_columns['LotFrontage'].fillna(68)","4c7d764b":"num_columns=num_columns.fillna(0)","47069e40":"num_columns.isnull().sum()","921816d0":"num_columns[\"House_Age\"]= num_columns['YrSold']- num_columns['YearBuilt']\nnum_columns[\"House_Age\"].describe()","9ed6ed86":"neg_house=num_columns[num_columns[\"House_Age\"]<0]\nneg_house","0eacf2a0":"neg_house['YrSold']","c23a054e":"num_columns.loc[num_columns[\"YrSold\"]<num_columns[\"YearRemodAdd\"],\"YrSold\"]=2009\n\nnum_columns[\"House_Age\"]= num_columns['YrSold']- num_columns['YearBuilt']\nnum_columns[\"House_Age\"].describe()","14f887da":"num_columns['TotalBsmtBath'] = num_columns['BsmtFullBath']*0.5 + num_columns['BsmtHalfBath']\nnum_columns['TotalBath'] = num_columns['FullBath']*0.5  + num_columns['HalfBath']\nnum_columns['TotalSA']=num_columns['TotalBsmtSF'] + num_columns['1stFlrSF'] + num_columns['2ndFlrSF']","d705dbc3":"num_columns.head()","6d358094":"cat_columns.head()","6d59fcd5":"#Using One Hot Encoder\n#cat_columns=pd.get_dummies(cat_columns,columns=cat_columns.columns)","724f17fd":"#Using LabelEncoder\nfrom sklearn.preprocessing import LabelEncoder\nencoder=LabelEncoder()\n\nfor features in cat_columns.columns:\n    cat_columns[features]=encoder.fit_transform(cat_columns[features])","edbbe9ec":"cat_columns.head()","120eb48f":"df_final = pd.concat([cat_columns, num_columns], axis=1,sort=False)\ndf_final.head()","860368d6":"df_final = df_final.drop(['Id'],axis=1)\n\n#We Had Initialised Train Data As 1 And Test Data As 0 In the Beggining\ndf_train = df_final[df_final['train'] == 1]\ndf_train = df_train.drop(['train'],axis=1)\n\n#Vertically Splitting The Train And Test\n\ndf_test = df_final[df_final['train'] == 0]\ndf_test = df_test.drop(['SalePrice'],axis=1)\ndf_test = df_test.drop(['train'],axis=1)","f3a2f10e":"df_train.head()","a007ee5a":"df_test.head()","9782846c":"target= df_train['SalePrice']\ndf_train = df_train.drop(['SalePrice'],axis=1)","7c6eac33":"x_train,x_test,y_train,y_test=train_test_split(df_train,target,test_size=0.33,random_state=0)","4160cd92":"xgb =XGBRegressor( booster='gbtree', colsample_bylevel=1,\n                  colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n                  importance_type='gain', learning_rate=0.01, max_delta_step=0,\n                  max_depth=4, min_child_weight=1.5, n_estimators=2400,\n                  n_jobs=1, nthread=None, objective='reg:linear',\n                  reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n                  silent=None, subsample=0.8, verbosity=1)\n\n\nlgbm = LGBMRegressor(objective='regression', \n                     num_leaves=4,\n                     learning_rate=0.01, \n                     n_estimators=12000, \n                     max_bin=200, \n                     bagging_fraction=0.75,\n                     bagging_freq=5, \n                     bagging_seed=7,\n                     feature_fraction=0.4)","94e93976":"#Fitting\nxgb.fit(x_train, y_train)\nlgbm.fit(x_train, y_train,eval_metric='rmse')","0a6c0126":"predict1 = xgb.predict(x_test)\npredict = lgbm.predict(x_test)","ff115985":"print('Root Mean Square Error test(XGBOOST) = ' + str(math.sqrt(metrics.mean_squared_error(y_test, predict1))))\nprint('Root Mean Square Error test(LGBM) = ' + str(math.sqrt(metrics.mean_squared_error(y_test, predict))))","c5c71786":"xgb.fit(df_train, target)\nlgbm.fit(df_train, target,eval_metric='rmse')","7972f194":"predict4 = lgbm.predict(df_test)\npredict3 = xgb.predict(df_test)\npredict_y = (predict3*0.45 + predict4*0.55)\nprint(predict_y)","719c0059":"submission = pd.DataFrame({\n        \"Id\": test[\"Id\"],\n        \"SalePrice\": predict_y\n    })\nsubmission.to_csv('submission.csv', index=False)","8bf145a8":"* We Have Added Two New Features Train and Test To Show Which Values Are Present In Either Train Or Test","19cd6274":"* Lets Concat Train And Test Dataset","596a84a8":"# Dealing With Categorical Columns","3e04e791":"**According To The Feature Description**\n\n* TotalBsmtBath : Sum of : 1\/2 * BsmtFullBath + BsmtHalfBath\n\n* TotalBath : Sum of : 1\/2 * FullBath and HalfBath\n\n* TotalSA : Sum of : 1stFlrSF and 2ndFlrSF and TotalBsmtSF","901867f9":"* Displaying Features Which Has More Than 50% Null Values","f13967fb":"*Label Encoding is a popular encoding technique for handling categorical variables. In this technique, each label is assigned a unique integer based on alphabetical ordering.*","37977e59":"Lets Fill The Other Null Num Features With 0","3b845f32":"* Fitting With all the dataset ","1a147a9a":"Root Mean Square Error (RMSE) is a standard way to measure the error of a model in predicting quantitative data.","1704558a":"# Lets Encode Categorical Values","d435b002":"# Dealing With Num Vals","129bd421":"I Found This Article Very Helpful To Know About LGBM And XGBoost\nhttps:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/which-algorithm-takes-the-crown-light-gbm-vs-xgboost\/                                                ","88ecd2dd":"*Now We Separate Train And Target Features*","cfbdb63a":"* It simply creates additional features based on the number of unique values in the categorical feature. Every unique value in the category will be added as a feature.*","0a362993":"Merging Both Predictions (45% of XGB and 55% of LGBM)","1db7c3d3":"We will fill -- **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, GarageType, GarageFinish, GarageQual, FireplaceQu, GarageCond** -- with \"None\" (Check The Above Output,We Place The Node In Features Where There Are Many NULL Values).\n\nWe will fill the rest of features with th most frequent value (using its own most frequent value).","b94daeef":"# Now Lets Concat The Num And Cat Features","4d69d988":"# Modelling","de7d2081":"* Like we see here that the minimun is -1 ???\n* It is strange to find that the house was sold in 2007 and built in 2008.\n* So we decide to change the year of sold to 2009","005dec15":"# Lets Make Some New Features","311168da":"# Initialising Train,Test Data","a44668ce":"83 = train\/test features+train id +test id+SalePrice","9a5ae561":"* Calculating the percentage of missing values of each feature","2e05dced":"**By The Above Table We Can See That We Gotta Fill LotFrontage and GarageYrBlt**","8bd27efe":"*We Can Use Either Mean Or Median Depending Upon Us*","9185b848":"train.head()","0e9a097c":"* Lets Find If There Is A Negative Value In House_Age","912e06fc":"**XGB Documentation**: https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n\n**LGBM Documentation**: https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html#core-parameters","a5254345":"* Now we will select numerical and categorical features "}}