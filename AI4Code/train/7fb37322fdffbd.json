{"cell_type":{"72b42899":"code","0992dc4f":"code","d79204a0":"code","cc2ac09a":"code","1bfb4aa2":"code","7a430b77":"code","873a1ac4":"code","62e10257":"code","adbf6897":"code","d57ea580":"code","323c5c6b":"code","6e022a63":"code","58b4d0ce":"code","9eddfe6f":"code","4a166ef9":"code","0f67af1f":"code","cde88fa4":"code","e2766a10":"code","4a96bb4d":"code","e4825ddc":"code","c99b2cb2":"code","7e1eb4fc":"code","c8a44e82":"code","602eaa1e":"code","55ad88a2":"code","475e9086":"code","2c7d192a":"code","910b8780":"code","7c561f63":"code","d9420568":"code","b7368946":"code","92024108":"code","c60418c1":"code","8064ff26":"code","9ae3684f":"code","a089925d":"code","bc48fb35":"code","9d97ebfe":"code","1586b3ab":"code","64afa4ce":"code","42e69130":"code","83855831":"code","d73d3791":"code","2f9a9353":"code","e3d3dc67":"code","e67a0761":"code","f34a41c4":"code","c5b3e393":"code","9462e497":"code","04776781":"code","8dc3b4a2":"code","16672191":"code","810dddde":"code","112ac5ce":"code","0205844c":"code","2d9923d4":"code","8280e4f2":"code","7b9681c6":"code","a8b3f849":"code","b285cb7a":"code","108f4e09":"code","47bb248d":"code","f6e90dfc":"code","21415b20":"code","f6d155ea":"code","ecdf51e2":"code","1691d86f":"code","9bfd3c7d":"code","79d05fad":"markdown","15e2a82a":"markdown","3545c17f":"markdown","fc152be5":"markdown","26d0df85":"markdown","f116fb1b":"markdown","2fe0c21e":"markdown","aacf0eea":"markdown","65278f47":"markdown","d0d40fd8":"markdown","b35fce66":"markdown","b6578b15":"markdown","3304be02":"markdown","7a7a7d66":"markdown","42036f6b":"markdown","e6029887":"markdown","2d342df4":"markdown","773564ea":"markdown","07f47e2e":"markdown","407e773d":"markdown","6fc8fbc7":"markdown","6bdfb82b":"markdown","a1e92c3e":"markdown","cab2b803":"markdown","55be2104":"markdown","0ca9d9c5":"markdown","dea2e29a":"markdown","4e6b69b0":"markdown","b3d1e189":"markdown","62148268":"markdown","dbb58b73":"markdown","ad112ef9":"markdown","48b76a54":"markdown","62d88892":"markdown","7f1b243a":"markdown","de762264":"markdown","18136759":"markdown","9ec4d001":"markdown","ddb9735e":"markdown","ae8e9f4a":"markdown","beb7246a":"markdown","d08712d1":"markdown","0140b99e":"markdown","9fc588cf":"markdown","fa3d7837":"markdown","395a1c1e":"markdown","af675722":"markdown"},"source":{"72b42899":"import numpy as np\nimport pandas as pd\n\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns","0992dc4f":"# Read the train data\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\n# Read the test data\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n# Treat the test data in the same way as training data. In this case, pull same columns.\ntarget = 'SalePrice'","d79204a0":"train.head()","cc2ac09a":"train.describe()","1bfb4aa2":"train.info()","7a430b77":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))\n","873a1ac4":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","62e10257":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","adbf6897":"ntrain = train.shape[0]\nntest = test.shape[0]\nY_train = train.SalePrice.values\nall_data = pd.concat((train, test), sort=False).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","d57ea580":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","323c5c6b":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","6e022a63":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True) #annot=True","58b4d0ce":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")","9eddfe6f":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","4a166ef9":"# Split train and test agian\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]","0f67af1f":"#Self Defined Imputation for both categorical and numerical datas\nfrom sklearn.base import TransformerMixin\n\nclass DataFrameImputer(TransformerMixin):\n\n    def __init__(self):\n        \"\"\"Impute missing values.\n\n        Columns of dtype object are imputed with the most frequent value \n        in column.\n\n        Columns of other types are imputed with mean of column.\n\n        \"\"\"\n    def fit(self, X, y=None):\n\n        self.fill = pd.Series([X[c].value_counts().index[0]\n            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n            index=X.columns)\n\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)","cde88fa4":"imputed_train = train.copy()\nimputed_test = test.copy()\n\ncols_with_missing = (col for col in test.columns \n                                 if train[col].isnull().any() or test[col].isnull().any() )\nfor col in cols_with_missing:\n    imputed_train[col + '_was_missing'] = imputed_train[col].isnull()\n    imputed_test[col + '_was_missing'] = imputed_test[col].isnull()\nimputed_train_cols = imputed_train.columns\nimputed_test_cols = imputed_test.columns\n\n# Imputation\nmy_imputer = DataFrameImputer()\nimputed_train_array = my_imputer.fit_transform(imputed_train)\nimputed_test_array = my_imputer.transform(imputed_test)\n\nimputed_train =  pd.DataFrame(imputed_train_array, columns=imputed_train_cols)\nimputed_test =  pd.DataFrame(imputed_test_array, columns=imputed_test_cols)","e2766a10":"imputed_train.info()","4a96bb4d":"imputed_train.head()","e4825ddc":"train = imputed_train.copy()\ntest = imputed_test.copy()","c99b2cb2":"ntrain = train.shape[0]\nntest = test.shape[0]\nall_data = pd.concat((train, test)).reset_index(drop=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","7e1eb4fc":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","c8a44e82":"# Exclude data that lies outside 2 standard deviations from the mean\n# m = np.mean(train[''])\n# s = np.std(train[''])\n# train = train[train[''] <= m + 2*s]\n# train = train[train[''] >= m - 2*s]","602eaa1e":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","55ad88a2":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","475e9086":"# Split train and test agian\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]","2c7d192a":"from datetime import datetime\n# train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)\n# test['pickup_datetime'] = pd.to_datetime(test.pickup_datetime)\n# train.loc[:, 'pickup_date'] = train['pickup_datetime'].dt.date\n# test.loc[:, 'pickup_date'] = test['pickup_datetime'].dt.date","910b8780":"plt.rcParams['figure.figsize'] = [16, 10]","7c561f63":"plt.hist(Y_train, bins=100)\nplt.xlabel(target)\nplt.ylabel('number of train records')\nplt.show()","d9420568":"Y_train_log = np.log(Y_train + 1)\nplt.hist(Y_train_log, bins=100)\nplt.xlabel('log_'+target)\nplt.ylabel('number of train records')\nplt.show()\nsns.distplot(Y_train_log, bins =100)","b7368946":"# pull data into target (y) and predictors (X)\nX_train = train\nX_test = test","92024108":"# \"cardinality\" means the number of unique values in a column.\n# We use it as our only way to select categorical columns here. This is convenient, though\n# a little arbitrary.\nencoded_X_train = X_train.copy()\nencoded_X_test = X_test.copy()\nlow_cardinality_cols = [cname for cname in encoded_X_train.columns if \n                                encoded_X_train[cname].nunique() < 30 and\n                                encoded_X_train[cname].dtype == \"object\"]\nhigh_cardinality_cols = [cname for cname in encoded_X_train.columns if \n                                encoded_X_train[cname].nunique() >= 30 and\n                                encoded_X_train[cname].dtype == \"object\"]\nnumeric_cols = [cname for cname in encoded_X_train.columns if \n                                encoded_X_train[cname].dtype in ['int64', 'float64']]\nmy_cols = low_cardinality_cols + numeric_cols\nencoded_X_train = encoded_X_train[my_cols]\nencoded_X_test  = encoded_X_test[my_cols]\n#final encoding: in only encodes string categorical values unless columns are specified\ntrain_objs_num = len(encoded_X_train)\n#Combine tran test data\ndataset = pd.concat(objs=[encoded_X_train, encoded_X_test], axis=0)\ndataset_preprocessed = pd.get_dummies(dataset)\n#Splitting again\nencoded_X_train = dataset_preprocessed[:train_objs_num]\nencoded_X_test = dataset_preprocessed[train_objs_num:]","c60418c1":"#Columns still not used for predictions\nfor feat in high_cardinality_cols:\n    print(feat+\": \"+str(train[feat].nunique()))","8064ff26":"encoded_X_train.columns","9ae3684f":"encoded_X_train.info()","a089925d":"encoded_X_train.head()","bc48fb35":"final_X_train = encoded_X_train.copy()\nfinal_Y_train = Y_train.copy()\nfinal_X_test = encoded_X_test.copy()","9d97ebfe":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, cross_val_predict\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport xgboost as xgb\nimport lightgbm as lgb","1586b3ab":"#Validation function\nn_folds = 5\n\ndef score_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    error= np.sqrt(-cross_val_score(model, final_X_train, final_Y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(error)","64afa4ce":"def accu_score(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","42e69130":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =1,max_iter=100000, random_state=1))","83855831":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=1, l1_ratio=.9, random_state=3))","d73d3791":"KRR = KernelRidge(alpha=1, kernel='linear', gamma=None, degree=3, coef0=1, kernel_params=None)","2f9a9353":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","e3d3dc67":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","e67a0761":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","f34a41c4":"score = score_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","c5b3e393":"score = score_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","9462e497":"score = score_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","04776781":"score = score_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","8dc3b4a2":"score = score_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","16672191":"score = score_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","810dddde":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            instance = clone(model)\n            self.base_models_[i].append(instance)\n            out_of_fold_predictions[:,i] = cross_val_predict(instance, X, y, cv=kfold)\n            instance.fit(X,y)\n        \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models])\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","112ac5ce":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\n#score = score_cv(stacked_averaged_models)\n#print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","0205844c":"stacked_averaged_models.fit(final_X_train, final_Y_train)\nstacked_train_pred = stacked_averaged_models.predict(final_X_train)\nstacked_pred = stacked_averaged_models.predict(final_X_test)\nprint(accu_score(final_Y_train, stacked_train_pred))","2d9923d4":"X_train_cv, X_test_cv, Y_train_cv, Y_test_cv = train_test_split(final_X_train, final_Y_train, test_size=0.25)","8280e4f2":"from sklearn.metrics import roc_auc_score\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n\ndef objective(space):\n\n    clf = xgb.XGBRegressor(n_estimators = 1000,\n                            max_depth = int(space['max_depth']),\n                            min_child_weight = int(space['min_child_weight']),\n                            subsample = space['subsample'])\n\n    eval_set  = [( X_train_cv, Y_train_cv), ( X_test_cv, Y_test_cv)]\n\n    clf.fit(X_train_cv, Y_train_cv,\n            eval_set=eval_set,\n            early_stopping_rounds=30, verbose = False)\n\n    pred = clf.predict(X_test_cv)\n    score = mean_absolute_error(pred, Y_test_cv)\n    print(\"SCORE: \"+ str(score))\n\n    return{'loss':score, 'status': STATUS_OK }\n\n\nspace ={\n        'max_depth': hp.quniform(\"x_max_depth\", 5, 30, 1),\n        'min_child_weight': hp.quniform ('x_min_child', 1, 10, 1),\n        'subsample': hp.uniform ('x_subsample', 0.8, 1)\n    }\n\ntrials = Trials()\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=100,\n            trials=trials)\n\nprint (best)","7b9681c6":"my_model = xgb.XGBRegressor(n_estimators = 1000,\n                            max_depth = int(best['x_max_depth']),\n                            min_child_weight = int(best['x_min_child']),\n                            subsample = best['x_subsample'])","a8b3f849":"my_model.fit(final_X_train, final_Y_train, verbose=False)\nxgb_train_pred = my_model.predict(final_X_train)\nxgb_pred = my_model.predict(final_X_test) #np.expm1 if logY is used for training\nprint(accu_score(final_Y_train, xgb_train_pred))","b285cb7a":"model_lgb.fit(final_X_train, final_Y_train)\nlgb_train_pred = model_lgb.predict(final_X_train)\nlgb_pred = model_lgb.predict(final_X_test)\nprint(accu_score(final_Y_train, lgb_train_pred))","108f4e09":"'''Score on the entire Train data when averaging'''\n\nprint('Score score on train data:')\nprint(accu_score(final_Y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","47bb248d":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","f6e90dfc":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","21415b20":"#XGB feature importances\nimport shap\n\n# explain the model's predictions using SHAP values\n# (same syntax works for LightGBM, CatBoost, and scikit-learn models)\nexplainer = shap.TreeExplainer(my_model)\nshap_values = explainer.shap_values(final_X_train)\n\n# create a SHAP dependence plot to show the effect of a single feature across the whole dataset\nshap.dependence_plot('OverallQual', shap_values, final_X_train)","f6d155ea":"# summarize the effects of all the features\nshap.summary_plot(shap_values, final_X_train)","ecdf51e2":"shap.summary_plot(shap_values, final_X_train, plot_type=\"bar\")","1691d86f":"from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nmy_model_plot = GradientBoostingRegressor()\n# fit the model as usual\nmy_model_plot.fit(final_X_train, final_Y_train)\n# Here we make the plot\nmy_plots = plot_partial_dependence(my_model_plot,       \n                                   features=[0,1, 2], # column numbers of plots we want to show\n                                   X=final_X_train,            # raw predictors data.\n                                   feature_names=['MSSubClass', 'LotArea', 'YearBuilt'], # labels on graphs\n                                   grid_resolution=10) # number of values to plot on x axis\n","9bfd3c7d":"print(\"Kernel Completed\")","79d05fad":"* **PoolQC** : data description says NA means \"No Pool\". That make sense, given the huge ratio of missing value (+99%)  \nand majority of houses have no Pool at all in general.","15e2a82a":"### Imputation along with track of what was imputed","3545c17f":"## Initial Analysis","fc152be5":"* Kernel Ridge Regression :","26d0df85":"# Data Visualisation","f116fb1b":"### Data Correlation","2fe0c21e":"# Date-Time conversion","aacf0eea":"## More features engeneering","65278f47":"### Ensemble prediction:","d0d40fd8":"## Self defined mixed Imputation Function for  features with not enough info","b35fce66":"### Ensembling StackedRegressor, XGBoost and LightGBM","b6578b15":"## Features engineering","3304be02":"### Missing Data","7a7a7d66":"## Base models","42036f6b":"### Base models scores","e6029887":"We impute them by proceeding sequentially through features with missing values","2d342df4":"### Define a cross validation strategy","773564ea":"**Transforming some numerical variables that are really categorical**","07f47e2e":"let's first concatenate the train and test data in the same dataframe","407e773d":"# Data Enrichment\nAdding data from external sources","6fc8fbc7":"# Data Clean-up","6bdfb82b":"**Adding  more important feature**","a1e92c3e":"# Loading the Data","cab2b803":"* StackedRegressor:","55be2104":"Note :\nOutliers removal is note always safe. We decided to delete these two as they are very huge and really bad ( extremely large areas for very low prices).\n\nThere are probably others outliers in the training data. However, removing all them may affect badly our models if ever there were also outliers in the test data. That's why , instead of removing them all, we will just manage to make some of our models robust on them. You can refer to the modelling part of this notebook for that.","0ca9d9c5":"\n**If you have any questions or hit any problems, come to the [Learn Discussion](https:\/\/www.kaggle.com\/learn-forum) for help. **\n\n**Return to [ML Course Index](https:\/\/www.kaggle.com\/learn\/machine-learning)**","dea2e29a":"* Gradient Boosting Regression :","4e6b69b0":"# Encoding categorical variables","b3d1e189":"# Plots after fitting -- Partial Dependece","62148268":"* XGBoost :","dbb58b73":"* LightGBM :","ad112ef9":"Let's explore these outliers","48b76a54":"* **LotFrontage **: Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.","62d88892":"### Stacking Averaged models Score","7f1b243a":"* LightGBM:","de762264":"Tuning XGB Parameters","18136759":"# Modelling","9ec4d001":"* LASSO Regression :","ddb9735e":"### Import librairies","ae8e9f4a":"* XGBoost:","beb7246a":"### Imputing missing values","d08712d1":"* Elastic Net Regression :","0140b99e":"### Final submission","9fc588cf":"## Stacking averaged Models Class","fa3d7837":"## Split train data to X and Y","395a1c1e":"## Outliers","af675722":"# Interpreting XGBoost"}}