{"cell_type":{"988d60ee":"code","85bf858c":"code","a6910e14":"code","887f9f5d":"code","9c459607":"code","dffb65b0":"code","f8524609":"code","18974a46":"code","b79a7cd7":"code","747dde7c":"code","4ae197dc":"code","00da2659":"code","e481c966":"code","c126e7da":"code","d14a04dd":"code","0a5a0c9f":"code","2f7548b6":"code","cbdf9162":"code","622b36b6":"code","4c83d93b":"code","68396776":"code","db9d7b94":"code","493e4d97":"code","ac46b941":"code","c4d1459e":"code","866ab792":"code","6b456274":"code","fcd6db0a":"code","2fe59201":"code","957913cd":"code","36eb1981":"code","e430a41f":"code","4e3a1e7c":"code","339012c0":"code","59da143c":"code","c82c3888":"code","bba91f96":"code","08cfb6fe":"code","b5ee16ed":"code","43afc8b5":"code","55c9857a":"code","153800c2":"code","caaa5088":"code","21430bd0":"code","7fa1609c":"code","2bcbb739":"code","4604387c":"code","25e670a5":"code","4d0ad291":"code","d9b9726c":"code","3ba4095f":"code","cd02ba1e":"code","d156b0c9":"code","6f8394d4":"code","23f1039c":"code","c1c2c609":"code","e99d591a":"code","e5fe39bb":"code","92d81aa9":"code","5afeba3b":"code","414ead4a":"code","f765c653":"code","63db343a":"code","38174490":"code","6e0f80fb":"code","22de99f5":"code","b36b0a0a":"code","f5cbe709":"code","986b58b2":"code","9dd37a11":"code","41f36e10":"code","5ec0c817":"code","a00515fe":"code","d8c9e4e7":"code","f94ab0ee":"code","a123fb96":"code","347d729f":"code","eaa7d845":"code","fe0c7941":"code","4d2bfb87":"code","936d3d23":"code","a8f84aed":"code","e6bf6ab6":"code","5415eec3":"code","fde5dd02":"code","d1c9eb7c":"code","53f861f3":"code","229c433e":"code","7cc3e9b0":"code","f0eb7c9f":"code","2c5fa01c":"code","c0929278":"code","d4d376b2":"markdown","d6d7d0dd":"markdown"},"source":{"988d60ee":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport nltk\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport re\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n!pip install focal-loss\nfrom tensorflow.keras import Input,Model\nfrom tensorflow.keras.layers import Embedding,SimpleRNN,LSTM,concatenate,Dense,Dropout\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import classification_report,average_precision_score,accuracy_score\nfrom focal_loss import BinaryFocalLoss","85bf858c":"df=pd.read_csv(\"..\/input\/multilabel-classification-dataset\/train.csv\")","a6910e14":"df.head()","887f9f5d":"X=df.iloc[:,1:3].copy()","9c459607":"Y=df.iloc[:,3:].copy()","dffb65b0":"# splitting data into train and test set\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)","f8524609":"class Preprocessing:\n    \n    \n    def __init__(self):\n        #\n        self.stop_words=stopwords.words('english')\n        self.lemmatizer=WordNetLemmatizer()\n        self.tokenizer=Tokenizer()\n        self.training=None\n        \n        \n    def remove_punctuations(self,text):\n        #\n        text=text.lower()\n        cleaned_text =re.findall(\"[a-zA-Z]+\", text)\n        \n        return cleaned_text\n    \n\n    def stop_words_remover(self,text):\n        #\n        cleaned_text=[w for w in text if not w in self.stop_words]\n        \n        return cleaned_text\n    \n    \n    def lemmatize(self,text):\n        #\n        cleaned_text=' '.join([self.lemmatizer.lemmatize(i) for i in text])\n        \n        return cleaned_text\n    \n    \n    def tokenize(self,X_cleaned): \n        #\n        if self.training:\n            self.tokenizer.fit_on_texts(X_cleaned)\n            \n        # converting text to sequence of tokens\n        X_seq = self.tokenizer.texts_to_sequences(X_cleaned)\n\n        # converting sequences to text\n        X_txt = self.tokenizer.sequences_to_texts(X_seq)\n        \n        return X_seq, X_txt\n        \n    \n    def preprocess(self,X,training=True):\n        #\n        X_preprocessed=pd.DataFrame()\n        self.training=training\n        X=X.apply(lambda x: self.remove_punctuations(x))\n        X=X.apply(lambda x: self.stop_words_remover(x))\n        X=X.apply(lambda x: self.lemmatize(x))\n        X_preprocessed['seq'],X_preprocessed['txt']= self.tokenize(X)\n        \n        return X_preprocessed,self.tokenizer\n       ","18974a46":"#Preprocessing train and test\npp_title = Preprocessing()\nX_title_train,tokenizer_title=pp_title.preprocess(X=X_train['TITLE'])\nX_title_test,_=pp_title.preprocess(X=X_test['TITLE'],training=False)\n\npp_abstract = Preprocessing()\nX_abstract_train,tokenizer_abstract=pp_abstract.preprocess(X=X_train['ABSTRACT'])\nX_abstract_test,_=pp_abstract.preprocess(X=X_test['ABSTRACT'],training=False)","b79a7cd7":"#Word count\nseqlen_title=X_title_train['txt'].apply(lambda x: len(x.split()))\nseqlen_abstract=X_abstract_train['txt'].apply(lambda x: len(x.split()))","747dde7c":"#Plotting title word count\nsns.set_style(\"darkgrid\")\nsns.distplot(seqlen_title,bins=20)","4ae197dc":"#Plotting abstract word count\nsns.distplot(seqlen_abstract)","00da2659":"max_len_title=15\nmax_len_abstract=125","e481c966":"#Converting train  into sequences\nX_title_seq = pad_sequences(X_title_train['seq'], maxlen = max_len_title, padding = 'pre', truncating='post')\nX_abstract_seq=pad_sequences(X_abstract_train['seq'], maxlen = max_len_abstract, padding = 'pre', truncating='post')","c126e7da":"#Converting test into sequence\nX_title_seq_test = pad_sequences(X_title_test['seq'], maxlen = max_len_title, padding = 'pre', truncating='post')\nX_abstract_seq_test=pad_sequences(X_abstract_test['seq'], maxlen = max_len_abstract, padding = 'pre', truncating='post')","d14a04dd":"vocab_size_title = len(tokenizer_title.word_index) + 1\nvocab_size_abstract = len(tokenizer_abstract.word_index) + 1","0a5a0c9f":"print(vocab_size_title,vocab_size_abstract)","2f7548b6":"#Extracting glove vectors\nfrom tqdm import tqdm\nembedding_vector = {}\nf = open('..\/input\/glove840b300dtxt\/glove.840B.300d.txt')\nfor line in tqdm(f):\n    value = line.split(' ')\n    word = value[0]\n    coef = np.array(value[1:], dtype = 'float32')\n    embedding_vector[word] = coef","cbdf9162":"#Embedding for title\noov_title=[]\nembedding_matrix_title = np.zeros((vocab_size_title, 300))\nfor word, i in tqdm(tokenizer_title.word_index.items()):\n    embedding_value = embedding_vector.get(word)\n    if embedding_value is not None:\n      embedding_matrix_title[i] = embedding_value\n    else:\n      oov_title.append(word)","622b36b6":"#Embedding for abstract\noov_abstract=[]\nembedding_matrix_abstract = np.zeros((vocab_size_abstract, 300))\nfor word, i in tqdm(tokenizer_abstract.word_index.items()):\n    embedding_value = embedding_vector.get(word)\n    if embedding_value is not None:\n      embedding_matrix_abstract[i] = embedding_value\n    else:\n      oov_abstract.append(word)","4c83d93b":"def create_model():\n    title_input = Input(shape=(None,), name=\"title\")\n    abstract_input = Input(shape=(None,), name=\"abstract\") \n    title_features = Embedding(vocab_size_title, 300, weights = [embedding_matrix_title], input_length = max_len_title, trainable = False)(title_input)\n    abstract_features = Embedding(vocab_size_abstract, 300, weights = [embedding_matrix_abstract], input_length = max_len_abstract, trainable = False)(abstract_input)\n    title_features = SimpleRNN(64,kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(title_features)\n    abstract_features = SimpleRNN(128,kernel_regularizer=regularizers.l2(0.001),activity_regularizer=regularizers.l2(0.01))(abstract_features)\n    x = concatenate([title_features, abstract_features])\n    cs_pred = Dense(1, activation='sigmoid',name='CS',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(x)\n    ph_pred = Dense(1, activation='sigmoid',name='PH',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(x)\n    math_pred = Dense(1, activation='sigmoid',name='MATH',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(x)\n    stat_pred = Dense(1, activation='sigmoid',name='STAT',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(x)\n    qb_pred = Dense(1, activation='sigmoid',name='QB',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(x)\n    qf_pred = Dense(1, activation='sigmoid',name='QF',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(x)\n    \n    \n    model = Model(inputs=[title_input, abstract_input],outputs=[cs_pred,ph_pred,math_pred,stat_pred,qb_pred,qf_pred])\n    return model","68396776":"model=create_model()\nplot_model(model, \"multi_label_classification_model.png\", show_shapes=True)\n","db9d7b94":"model.summary()","493e4d97":"model.compile(\n    optimizer=Adam(),\n    loss=BinaryFocalLoss(gamma=2),\n    metrics=['accuracy'],\n#     loss_weights={'CS': 0.2, 'PH':0.2, 'MATH': 0.3, 'STAT': 0.4, 'QB': 1.0, 'QF': 1.0}\n)","ac46b941":"callbacks = [EarlyStopping(monitor='val_loss', patience=4)]\n\nhistory = model.fit({\"title\": X_title_seq, \"abstract\": X_abstract_seq}, \n                    {'CS': y_train.iloc[:,0], 'PH':y_train.iloc[:,1], 'MATH': y_train.iloc[:,2], 'STAT': y_train.iloc[:,3], 'QB': y_train.iloc[:,4], 'QF': y_train.iloc[:,5]},\n                    epochs = 25, \n                    validation_data =({\"title\": X_title_seq_test, \"abstract\": X_abstract_seq_test} ,\n                                     {'CS': y_test.iloc[:,0], 'PH':y_test.iloc[:,1], 'MATH': y_test.iloc[:,2], 'STAT': y_test.iloc[:,3], 'QB': y_test.iloc[:,4], 'QF': y_test.iloc[:,5]}),\n                    callbacks=callbacks,\n                    verbose=2\n                   )","c4d1459e":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc = 'upper right')\nplt.show()","866ab792":"y_pred=model.predict({\"title\": X_title_seq_test, \"abstract\": X_abstract_seq_test})","6b456274":"categories=Y.columns","fcd6db0a":"#Converting y_pred to suitable format\ny_dict={\"0\":y_pred[0][:].flatten(),\"1\":y_pred[1][:].flatten(),'2':y_pred[2][:].flatten(),'3':y_pred[3][:].flatten(),'4':y_pred[4][:].flatten(),'5':y_pred[5][:].flatten()}","2fe59201":"y_df=pd.DataFrame.from_dict(y_dict)","957913cd":"y_df.head()","36eb1981":"y_df.columns=Y.columns","e430a41f":"#Function to find  balanced value for precesion and recall\ndef find_threshold(y_test,y_prob):\n    \n    precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n    \n    plt.plot(recall, precision, marker='.', label='Model')\n    # axis labels\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    # show the legend\n    plt.legend()\n    # show the plot\n    plt.show()\n    \n    #Find best threshold\n    min_diff=1\n    for i in range(len(recall)):\n        diff=abs(recall[i] -precision[i])\n        if diff<min_diff:\n            min_diff=diff\n            best_threshold=thresholds[i]\n            index=i\n            \n    print(f'Precision and Recall for threshold {best_threshold} = {precision[index]} and {recall[index]}')\n    \n    return best_threshold","4e3a1e7c":"threshold_cs=find_threshold(y_test['Computer Science'],y_df['Computer Science'])","339012c0":"threshold_ph=find_threshold(y_test['Physics'],y_df['Physics'])","59da143c":"threshold_math=find_threshold(y_test['Mathematics'],y_df['Mathematics'])","c82c3888":"threshold_stat=find_threshold(y_test['Statistics'],y_df['Statistics'])","bba91f96":"threshold_qb=find_threshold(y_test['Quantitative Biology'],y_df['Quantitative Biology'])","08cfb6fe":"threshold_qf=find_threshold(y_test['Quantitative Finance'],y_df['Quantitative Finance'])","b5ee16ed":"y_pred=y_df.copy()","43afc8b5":"#From the above precision recall curve selected threshold values\ny_pred[\"Computer Science\"]=np.where(y_pred[\"Computer Science\"]>=threshold_cs,1,0)\ny_pred[\"Physics\"]=np.where(y_pred[\"Physics\"]>=threshold_ph,1,0)\ny_pred[\"Mathematics\"]=np.where(y_pred[\"Mathematics\"]>=threshold_math,1,0)\ny_pred[\"Statistics\"]=np.where(y_pred[\"Statistics\"]>=threshold_stat,1,0)\ny_pred[\"Quantitative Biology\"]=np.where(y_pred['Quantitative Biology']>=threshold_qb,1,0)\ny_pred[\"Quantitative Finance\"]=np.where(y_pred[\"Quantitative Finance\"]>=threshold_qf,1,0)","55c9857a":"y_pred.describe()","153800c2":"print(classification_report(y_test,y_pred,target_names=categories));","caaa5088":"accuracy_score(y_test,y_pred)","21430bd0":"average_precision_score(y_test,y_pred)","7fa1609c":"title = 'An Empirical Study of DeFi Liquidations:Incentives, Risks, and Instabilities'\ntitle_input = pd.Series(title)\ntitle_input,_=pp_title.preprocess(title_input,training=False)\ntitle_input= pad_sequences(title_input['seq'], maxlen = max_len_title, padding = 'pre', truncating='post')","2bcbb739":"abstract = '''Financial speculators often seek to increase their potential gains\nwith leverage. Debt is a popular form of leverage, and with over\n39.88B USD of total value locked (TVL), the Decentralized Finance\n(DeFi) lending markets are thriving. Debts, however, entail the risks\nof liquidation, the process of selling the debt collateral at a discount\nto liquidators. Nevertheless, few quantitative insights are known\nabout the existing liquidation mechanisms.\nIn this paper, to the best of our knowledge, we are the first to\nstudy the breadth of the borrowing and lending markets of the\nEthereum DeFi ecosystem. We focus on Aave, Compound, Mak\u0002erDAO, and dYdX, which collectively represent over 85% of the\nlending market on Ethereum. Given extensive liquidation data mea\u0002surements and insights, we systematize the prevalent liquidation\nmechanisms and are the first to provide a methodology to compare\nthem objectively. We find that the existing liquidation designs well\nincentivize liquidators but sell excessive amounts of discounted\ncollateral at the borrowers\u2019 expenses. We measure various risks\nthat liquidation participants are exposed to and quantify the in\u0002stabilities of existing lending protocols. Moreover, we propose an\noptimal strategy that allows liquidators to increase their liquidation\nprofit, which may aggravate the loss of borrowers.\n'''\nabstract_input = pd.Series(abstract)\nabstract_input, _ = pp_abstract.preprocess(abstract_input, training=False)\nabstract_input= pad_sequences(abstract_input['seq'], maxlen = max_len_abstract, padding = 'pre', truncating='post')","4604387c":"y_probs=model.predict({\"title\": title_input, \"abstract\": abstract_input})","25e670a5":"y_dict={\"0\":y_probs[0][:].flatten(),\"1\":y_probs[1][:].flatten(),'2':y_probs[2][:].flatten(),'3':y_probs[3][:].flatten(),'4':y_probs[4][:].flatten(),'5':y_probs[5][:].flatten()}","4d0ad291":"y_probs=pd.DataFrame.from_dict(y_dict)","d9b9726c":"y_probs.columns=Y.columns","3ba4095f":"y_probs","cd02ba1e":"y_probs[\"Computer Science\"]=np.where(y_probs[\"Computer Science\"]>=threshold_cs,1,0)\ny_probs[\"Physics\"]=np.where(y_probs[\"Physics\"]>=threshold_ph,1,0)\ny_probs[\"Mathematics\"]=np.where(y_probs[\"Mathematics\"]>=threshold_math,1,0)\ny_probs[\"Statistics\"]=np.where(y_probs[\"Statistics\"]>=threshold_stat,1,0)\ny_probs[\"Quantitative Biology\"]=np.where(y_probs['Quantitative Biology']>=threshold_qb,1,0)\ny_probs[\"Quantitative Finance\"]=np.where(y_probs[\"Quantitative Finance\"]>=threshold_qf,1,0)","d156b0c9":"y_probs","6f8394d4":"# splitting data into train and test set\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)","23f1039c":"class Preprocessing:\n    '''Preprocesses the given column and returns data that is ready to feed to the model'''\n    \n    def __init__(self):\n        # initializing objects for different preprocessing techniques\n        self.stop_words = stopwords.words('english')\n        self.lemmatizer = WordNetLemmatizer()\n        self.vectorizer = TfidfVectorizer(ngram_range=(1,2),max_features=20000)\n        self.selector = None\n        self.training = None\n        \n    \n    def remove_html_tags(self, text):\n        # Remove html tags from a string\n        html_free = re.compile('<.*?>')\n        html_free = re.sub(html_free, '', text)\n        return html_free\n\n\n    def remove_punctuations(self, text):\n        # removes unnecessary punctuations from the text\n        text = text.lower()\n        cleaned_text = re.findall(\"[a-zA-Z]+\", text)\n        \n        return cleaned_text\n    \n\n    def stop_words_remover(self, text):\n        # removes stopwords since they are not useful for sentiment prediction\n        cleaned_text = [w for w in text if not w in self.stop_words]\n        \n        return cleaned_text\n    \n    \n    def lemmatize(self, text):\n        # brings words to their root words \n        cleaned_text = ' '.join([self.lemmatizer.lemmatize(i) for i in text])\n        \n        return cleaned_text\n    \n    \n    def vectorize(self, X_cleaned): \n        # converts text data to vectorized form such that it can be feeded to the models\n        if self.training:\n            self.vectorizer.fit(X_cleaned)\n            \n        # converting text to vectorized form\n        X_vectorized = self.vectorizer.transform(X_cleaned)\n        \n        return X_vectorized\n\n\n\n    \n    def preprocess(self, X, train_labels=None, training=True):\n        # takes input column and applies different pre-processing techniques\n        X_cleaned = pd.DataFrame()\n        self.training = training\n        X = X.apply(lambda x: self.remove_html_tags(x))\n        X = X.apply(lambda x: self.remove_punctuations(x))\n        X = X.apply(lambda x: self.stop_words_remover(x))\n        X_cleaned = X.apply(lambda x: self.lemmatize(x))\n        X_vectorized=self.vectorize(X_cleaned)\n        \n        return X_cleaned, X_vectorized","c1c2c609":"#Preprocessing data\npp_title = Preprocessing()\nX_cleaned_title_train, X_vectorized_title_train = pp_title.preprocess(X=X_train['TITLE'])\nX_cleaned_title_test, X_vectorized_title_test = pp_title.preprocess(X=X_test['TITLE'],training=False)\n\npp_abstract = Preprocessing()\nX_cleaned_abstract_train, X_vectorized_abstract_train = pp_abstract.preprocess(X=X_train['ABSTRACT'])\nX_cleaned_abstract_test, X_vectorized_abstract_test = pp_abstract.preprocess(X=X_test['ABSTRACT'],training=False)","e99d591a":"X_vectorized_title_train=X_vectorized_title_train.toarray()\nX_vectorized_title_test=X_vectorized_title_test.toarray()\nX_vectorized_abstract_train=X_vectorized_abstract_train.toarray()\nX_vectorized_abstract_test=X_vectorized_abstract_test.toarray()","e5fe39bb":"X_vectorized_title_train.shape","92d81aa9":"def create_model():\n    title_input = Input(shape=(20000,), name=\"title\")\n    abstract_input = Input(shape=(20000,), name=\"abstract\") \n    x = concatenate([title_input, abstract_input])\n    cs_pred = Dense(1, activation='sigmoid',name='CS',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(x)\n    ph_pred = Dense(1, activation='sigmoid',name='PH',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(x)\n    math_pred = Dense(1, activation='sigmoid',name='MATH',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(x)\n    stat_pred = Dense(1, activation='sigmoid',name='STAT',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(x)\n    qb_pred = Dense(1, activation='sigmoid',name='QB',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(x)\n    qf_pred = Dense(1, activation='sigmoid',name='QF',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))(x)\n    \n    \n    model = Model(inputs=[title_input, abstract_input],outputs=[cs_pred,ph_pred,math_pred,stat_pred,qb_pred,qf_pred])\n    return model","5afeba3b":"model=create_model()\nplot_model(model, \"multi_label_classification_model.png\", show_shapes=True)","414ead4a":"model.summary()","f765c653":"model.compile(\n    optimizer=Adam(),\n    loss=BinaryFocalLoss(gamma=2),\n    metrics=['accuracy'],\n#     loss_weights={'CS': 0.2, 'PH':0.2, 'MATH': 0.3, 'STAT': 0.4, 'QB': 1.0, 'QF': 1.0}\n)","63db343a":"callbacks = [EarlyStopping(monitor='val_loss', patience=3)]\n\nhistory = model.fit({\"title\": X_vectorized_title_train, \"abstract\": X_vectorized_abstract_train}, \n                    {'CS': y_train.iloc[:,0], 'PH':y_train.iloc[:,1], 'MATH': y_train.iloc[:,2], 'STAT': y_train.iloc[:,3], 'QB': y_train.iloc[:,4], 'QF': y_train.iloc[:,5]},\n                    epochs = 50, \n                    validation_data =({\"title\": X_vectorized_title_test, \"abstract\": X_vectorized_abstract_test}, \n                    {'CS': y_test.iloc[:,0], 'PH':y_test.iloc[:,1], 'MATH': y_test.iloc[:,2], 'STAT': y_test.iloc[:,3], 'QB': y_test.iloc[:,4], 'QF': y_test.iloc[:,5]}), \n                    callbacks=callbacks,verbose=2\n                   )","38174490":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc = 'upper right')\nplt.show()","6e0f80fb":"y_pred=model.predict({\"title\": X_vectorized_title_test, \"abstract\": X_vectorized_abstract_test})","22de99f5":"categories=Y.columns","b36b0a0a":"y_dict={\"0\":y_pred[0][:].flatten(),\"1\":y_pred[1][:].flatten(),'2':y_pred[2][:].flatten(),'3':y_pred[3][:].flatten(),'4':y_pred[4][:].flatten(),'5':y_pred[5][:].flatten()}","f5cbe709":"y_df=pd.DataFrame.from_dict(y_dict)","986b58b2":"y_df.head()","9dd37a11":"y_df.columns=Y.columns","41f36e10":"y_df.head()","5ec0c817":"def find_threshold(y_test,y_prob):\n    \n    precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n    \n    plt.plot(recall, precision, marker='.', label='Model')\n    # axis labels\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    # show the legend\n    plt.legend()\n    # show the plot\n    plt.show()\n    \n    #Find best threshold\n    min_diff=1\n    for i in range(len(recall)):\n        diff=abs(recall[i] -precision[i])\n        if diff<min_diff:\n            min_diff=diff\n            best_threshold=thresholds[i]\n            index=i\n            \n    print(f'Precision and Recall for threshold {best_threshold} = {precision[index]} and {recall[index]}')\n    \n    return best_threshold","a00515fe":"threshold_cs=find_threshold(y_test['Computer Science'],y_df['Computer Science'])","d8c9e4e7":"threshold_ph=find_threshold(y_test['Physics'],y_df['Physics'])","f94ab0ee":"threshold_math=find_threshold(y_test['Mathematics'],y_df['Mathematics'])","a123fb96":"threshold_stat=find_threshold(y_test['Statistics'],y_df['Statistics'])","347d729f":"threshold_qb=find_threshold(y_test['Quantitative Biology'],y_df['Quantitative Biology'])","eaa7d845":"threshold_qf=find_threshold(y_test['Quantitative Finance'],y_df['Quantitative Finance'])","fe0c7941":"y_pred=y_df.copy()","4d2bfb87":"y_pred[\"Computer Science\"]=np.where(y_pred[\"Computer Science\"]>=threshold_cs,1,0)\ny_pred[\"Physics\"]=np.where(y_pred[\"Physics\"]>=threshold_ph,1,0)\ny_pred[\"Mathematics\"]=np.where(y_pred[\"Mathematics\"]>=threshold_math,1,0)\ny_pred[\"Statistics\"]=np.where(y_pred[\"Statistics\"]>=threshold_stat,1,0)\ny_pred[\"Quantitative Biology\"]=np.where(y_pred['Quantitative Biology']>=threshold_qb,1,0)\ny_pred[\"Quantitative Finance\"]=np.where(y_pred[\"Quantitative Finance\"]>=threshold_qf,1,0)","936d3d23":"print(classification_report(y_test,y_pred,target_names=categories));","a8f84aed":"accuracy_score(y_test,y_pred)","e6bf6ab6":"average_precision_score(y_test,y_pred)","5415eec3":"title = 'An Empirical Study of DeFi Liquidations:Incentives, Risks, and Instabilities'\ntitle_input = pd.Series(title)\ntitle_cleaned, title_vectorized = pp_title.preprocess(title_input, training=False)","fde5dd02":"abstract = '''Financial speculators often seek to increase their potential gains\nwith leverage. Debt is a popular form of leverage, and with over\n39.88B USD of total value locked (TVL), the Decentralized Finance\n(DeFi) lending markets are thriving. Debts, however, entail the risks\nof liquidation, the process of selling the debt collateral at a discount\nto liquidators. Nevertheless, few quantitative insights are known\nabout the existing liquidation mechanisms.\nIn this paper, to the best of our knowledge, we are the first to\nstudy the breadth of the borrowing and lending markets of the\nEthereum DeFi ecosystem. We focus on Aave, Compound, Mak\u0002erDAO, and dYdX, which collectively represent over 85% of the\nlending market on Ethereum. Given extensive liquidation data mea\u0002surements and insights, we systematize the prevalent liquidation\nmechanisms and are the first to provide a methodology to compare\nthem objectively. We find that the existing liquidation designs well\nincentivize liquidators but sell excessive amounts of discounted\ncollateral at the borrowers\u2019 expenses. We measure various risks\nthat liquidation participants are exposed to and quantify the in\u0002stabilities of existing lending protocols. Moreover, we propose an\noptimal strategy that allows liquidators to increase their liquidation\nprofit, which may aggravate the loss of borrowers.\n'''\nabstract_input = pd.Series(abstract)\nabstract_cleaned, abstract_vectorized = pp_abstract.preprocess(abstract_input, training=False)","d1c9eb7c":"title_vectorized=title_vectorized.toarray()\nabstract_vectorized=abstract_vectorized.toarray()","53f861f3":"y_probs=model.predict({\"title\": title_vectorized, \"abstract\": abstract_vectorized})","229c433e":"y_dict={\"0\":y_probs[0][:].flatten(),\"1\":y_probs[1][:].flatten(),'2':y_probs[2][:].flatten(),'3':y_probs[3][:].flatten(),'4':y_probs[4][:].flatten(),'5':y_probs[5][:].flatten()}","7cc3e9b0":"y_probs=pd.DataFrame.from_dict(y_dict)","f0eb7c9f":"y_probs.columns=Y.columns","2c5fa01c":"y_probs[\"Computer Science\"]=np.where(y_probs[\"Computer Science\"]>=threshold_cs,1,0)\ny_probs[\"Physics\"]=np.where(y_probs[\"Physics\"]>=threshold_ph,1,0)\ny_probs[\"Mathematics\"]=np.where(y_probs[\"Mathematics\"]>=threshold_math,1,0)\ny_probs[\"Statistics\"]=np.where(y_probs[\"Statistics\"]>=threshold_stat,1,0)\ny_probs[\"Quantitative Biology\"]=np.where(y_probs['Quantitative Biology']>=threshold_qb,1,0)\ny_probs[\"Quantitative Finance\"]=np.where(y_probs[\"Quantitative Finance\"]>=threshold_qf,1,0)","c0929278":"y_probs","d4d376b2":"# **Using GloVe Embeddings**","d6d7d0dd":"# N-Gram Model"}}