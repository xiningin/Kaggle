{"cell_type":{"f2d7d489":"code","c7282f3e":"code","763a4783":"code","3ac5a146":"code","6c608068":"code","fd1acbf6":"code","01c00abc":"code","1412dd10":"code","8147f4e9":"code","1313fc5b":"code","da84c88c":"code","91906070":"code","02f20f61":"code","5166d43d":"code","1d77b30d":"code","08854c5d":"code","b07252a8":"markdown","c47f62fc":"markdown","a2d2b107":"markdown","0dfa79cc":"markdown","6448742b":"markdown","54847a54":"markdown","b1d14ae0":"markdown","30f69957":"markdown","7bb695ee":"markdown","d2043fae":"markdown","c282c295":"markdown","8d185145":"markdown","97621088":"markdown","d7d2a095":"markdown","94805494":"markdown","61e863bf":"markdown"},"source":{"f2d7d489":"#Getting all the packages we need: \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\nimport seaborn as sns #statist graph package\nimport matplotlib.pyplot as plt #plot package\n\nimport wordcloud #will use for the word cloud plot\nfrom wordcloud import WordCloud, STOPWORDS # optional to filter out the stopwords\n\n#Optional helpful plot stypes:\nplt.style.use('bmh') #setting up 'bmh' as \"Bayesian Methods for Hackers\" style sheet\n#plt.style.use('ggplot') #R ggplot stype","c7282f3e":"df = pd.read_csv('\/kaggle\/input\/dataisbeautiful\/r_dataisbeautiful_posts.csv')\n","763a4783":"df.sample(5)","3ac5a146":"df.info()\ndf.describe()","6c608068":"print(\"Data shape :\",df.shape)","fd1acbf6":"#Empty values:\n\ndf.isnull().sum().sort_values(ascending = False)","01c00abc":"sns.countplot(x = 'removed_by', hue = 'removed_by', data = df)\n#df.removed_by","1412dd10":"#To build a wordcloud, we have to remove NULL values first:\ndf[\"title\"] = df[\"title\"].fillna(value=\"\")","8147f4e9":"#Now let's add a string value instead to make our Series clean:\nword_string=\" \".join(df['title'].str.lower())\n\n#word_string","1313fc5b":"#And - plotting:\n\nplt.figure(figsize=(15,15))\nwc = WordCloud(background_color=\"purple\", stopwords = STOPWORDS, max_words=2000, max_font_size= 300,  width=1600, height=800)\nwc.generate(word_string)\n\nplt.imshow(wc.recolor( colormap= 'viridis' , random_state=17), interpolation=\"bilinear\")\nplt.axis('off')","da84c88c":"#Comments distribution plot:\n\nfig, ax = plt.subplots()\n_ = sns.distplot(df[df[\"num_comments\"] < 25][\"num_comments\"], kde=False, rug=False, hist_kws={'alpha': 1}, ax=ax)\n_ = ax.set(xlabel=\"num_comments\", ylabel=\"id\")\n\nplt.ylabel(\"Number of reddits\")\nplt.xlabel(\"Comments\")\n\nplt.show()","91906070":"df.corr()","02f20f61":"h_labels = [x.replace('_', ' ').title() for x in \n            list(df.select_dtypes(include=['number', 'bool']).columns.values)]\n\nfig, ax = plt.subplots(figsize=(10,6))\n_ = sns.heatmap(df.corr(), annot=True, xticklabels=h_labels, yticklabels=h_labels, cmap=sns.cubehelix_palette(as_cmap=True), ax=ax)","5166d43d":"df.score.describe()","1d77b30d":"df.score.median()","08854c5d":"#Score distribution: \n\nfig, ax = plt.subplots()\n_ = sns.distplot(df[df[\"score\"] < 22][\"score\"], kde=False, hist_kws={'alpha': 1}, ax=ax)\n_ = ax.set(xlabel=\"score\", ylabel=\"No. of reddits\")","b07252a8":"## <a name=\"feel\"><\/a>Getting a feel of the dataset\nLet's run basic dataframe exploratory commands","c47f62fc":"## Introduction\n\nThis is a brief exploratory data analysis using Pandas for a given public sample of random Reddit posts.\nWe will get a feel of a dataset and try to answer the following questions: \n* What are the most popular reddits? Which topics are viral?\n* Which posts have been removed and why? \n* What % removed reddits are deleted by moderatos? \n* Who are the most popular authors? \n* Who are the biggest spammers at Reddit platform?\n","a2d2b107":"We see that score and number of comments are highly positively correlated with a correlation value of 0.6. \n\nThere is some positive correlation of 0.2 between total awards received and score (0.2) and num_comments (0.1).\n\nNow let's visualize the correlation table above using a heatmap\n","0dfa79cc":"We note from the table above:\n- There are `173,611` entries in the dataset. Caveat, not all columns in the dataset are complete. \n- The average reddit score `193`. The median value for the score is `1`, which means that a half of reddits in our dataset have the score `0` or `1` and only less than 75% reddits have the score more than `5`\n- The most popular reddit has `18,801` comments, while the average is `25` and the median is `1`. ","6448742b":"## <a name=\"corr\"><\/a>Score distribution\n","54847a54":">As we can see, the most reddits have less than 5 comments. ","b1d14ae0":"## <a name=\"read\"><\/a>Reading the dataset\nAccessing Reddit dataset:","30f69957":"Let's see who and why removes posts:","7bb695ee":"## <a name=\"corr\"><\/a>Removed reddits deep dive","d2043fae":"## <a name=\"corr\"><\/a>Comments distribution\n","c282c295":"## <a name=\"corr\"><\/a>The most popular reddits","8d185145":">As we can see, the most deleted posts (68%) were removed by moderator. Less than 1% are deleted by authors.\n","97621088":"<p>&nbsp;<\/p>\n<img src=\"https:\/\/1000logos.net\/wp-content\/uploads\/2017\/05\/Reddit-logo.png\" width=400>\n<p>&nbsp;<\/p>","d7d2a095":">The average reddit has less than 25 comments. Let's see the comment distribution for those reddits who have <25 comments:","94805494":"## <a name=\"corr\"><\/a>Correlation between dataset variables\n\nNow let's see how the dataset variables are correlated with each other:\n* How score and comments are correlated? \n* Do they increase and decrease together (positive correlation)? \n* Does one of them increase when the other decrease and vice versa (negative correlation)? Or are they not correlated?\n\nCorrelation is represented as a value between -1 and +1 where +1 denotes the highest positive correlation, -1 denotes the highest negative correlation, and 0 denotes that there is no correlation.\n\n* Let's see the correlation table between our dataset variables (numerical and boolean variables only)","61e863bf":"## <a name=\"corr\"><\/a>The most common words in reddits:\n\nLet's see the word map of the most commonly used words from reddit titles:"}}