{"cell_type":{"7813cc7d":"code","184e826d":"code","f0a03fba":"code","06463b92":"code","1fe36c47":"code","35c8db3b":"code","83190eda":"code","cc15bacd":"code","6b2429c5":"code","03988ba6":"code","a10b90e6":"code","c05465eb":"code","cfbb0be2":"code","e68ef729":"code","f92a9990":"code","2a171d25":"code","5a593459":"markdown","299ce003":"markdown","9fb3b6da":"markdown","b8099fb0":"markdown","09343de9":"markdown","4109f669":"markdown","a0f7e8bb":"markdown","3f013fdf":"markdown"},"source":{"7813cc7d":"#importing all the required dependencies\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as torch_optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nfrom datetime import datetime\nimport pytorch_lightning as pl","184e826d":"#lets take a look at the data\ndf = pd.read_csv(\"..\/input\/hr-analytics-and-job-prediction\/HR_comma_sep.csv\")\nprint(\"Shape:\", df.shape)\ndf.head()","f0a03fba":"label = \"left\"\ncat_cols = ['Work_accident','number_project','promotion_last_5years','Department','salary']\nnum_cols = ['satisfaction_level', 'last_evaluation','average_montly_hours','time_spend_company']\nprint(\"Num of Categorical columns : \" ,len(cat_cols))\nprint(\"Number of numerical columns : \" , len(num_cols))","06463b92":"#splitting the data into train, val and test\ntest_size = 0.1\nval_size = 0.3\nrandom_state = 42\n\ndf_train , df_test = train_test_split(df,test_size = test_size,random_state = random_state,stratify = df[label])\n\ndf_train , df_val = train_test_split(df_train,test_size = val_size,random_state = random_state,stratify = df_train[label])\n\nprint(\"Shape:\", df.shape)\nprint(\"Shape of train:\", df_train.shape)\nprint(\"Shape of test:\", df_test.shape)\nprint(\"Shape of validation:\", df_val.shape)","1fe36c47":"#numeralising the data\ncat_code_dict = {}\n\nfor col in cat_cols:\n    category_col = df[col].astype('category')\n    cat_code_dict[col] = {value : idx for idx,value in enumerate(category_col.cat.categories)}\ncat_code_dict","35c8db3b":"# since the numerical columns have been scaled there's no need to scale them again\ndef preprocess(df,cat_code_dict,num_cols,cat_cols,label_col):\n    \"\"\"\n    df:DataFrame,\n    cat_code_dict : A dictionary of categorial columns ,\n    num_cols : the numerical columns,\n    cat_cols : the categorical columns,\n    label_col : the target column\n    \"\"\"\n    df = df.copy()\n    df[num_cols] = df[num_cols].astype(np.float32)\n    \n    for col in cat_cols:\n        col_dict = cat_code_dict[col]\n        df[col] = df [col].map(col_dict).astype(np.int64)\n        df[label_col] = df[label_col].astype(np.int64)\n    return df","83190eda":"#A look at the preprocessed data\npreprocess(df,cat_code_dict,num_cols,cat_cols,label)","cc15bacd":"#preprocessing  all of the dataframes\ndf_train = preprocess(df_train,cat_code_dict,num_cols,cat_cols,label)\ndf_test = preprocess(df_test,cat_code_dict,num_cols,cat_cols,label)\ndf_val = preprocess(df_val,cat_code_dict,num_cols,cat_cols,label)\ndisplay(df_train,df_test,df_val)\n","6b2429c5":"#lets make the dataset\nclass TabularDataset(Dataset):\n    def __init__(self,df,num_cols,cat_cols,label):\n        \"\"\"\n        df: Dataframe passed,\n        num_cols : Numerical Columns,\n        cat_cols : Categorical_columns,\n        label : target column\n        \"\"\"\n        self.df = df \n        self.num_cols = num_cols\n        self.cat_cols = cat_cols\n        self.label = label\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        num_array = self.df[self.num_cols].iloc[idx].values\n        cat_array = self.df[self.cat_cols].iloc[idx].values\n        label_array = self.df[self.label].iloc[idx]\n        return num_array,cat_array,label_array","03988ba6":"#checking the dataset for sanity\ndataset = TabularDataset(df_train,num_cols,cat_cols,label)\ndataloader = DataLoader(dataset,batch_size = 1 ,  shuffle = True)\nnext(iter(dataloader))","a10b90e6":"#Making the Pytorch Lightning DataModule recommended by Pytorch Lightning it makes the data more convinient to use\nclass TabularDatsetModule(pl.LightningDataModule):\n    def __init__(self,df_train,df_test,df_val,num_cols,cat_cols,label,test_batch = 64,train_batch = 64,val_batch = 64):\n        \"\"\"\n        df_train : Train Dataframe\n        df_test:test DataFrame\n        df_val : Validation Dataframe\n        num_cols : Numerical Columns\n        cat_cols : Categorical Columns\n        \"\"\"\n        super().__init__()\n        self.train = df_train\n        self.test = df_test\n        self.val = df_val\n        self.num_cols = num_cols\n        self.cat_cols = cat_cols\n        self.label = label\n        self.test_batch = test_batch\n        self.train_batch = train_batch\n        self.val_batch = val_batch\n    \n    def setup(self,stage):\n        self.train_loader = TabularDataset(self.train,self.num_cols,self.cat_cols,self.label)\n        self.test_loader = TabularDataset(self.test,self.num_cols,self.cat_cols,self.label)\n        self.val_loader = TabularDataset(self.val,self.num_cols,self.cat_cols,self.label)\n    \n    \"\"\"\n    These return the data to the neural network --->\n    \"\"\"\n    def train_dataloader(self):\n        return DataLoader(self.train_loader,batch_size = self.train_batch,shuffle = True)\n    \n    def test_dataloader(self):\n        return DataLoader(self.test_loader,batch_size = self.test_batch,shuffle = True)\n    \n    def val_dataloader(self):\n        return DataLoader(self.val_loader,batch_size = self.val_batch,shuffle = True)\n    ","c05465eb":"#Making the neural network\nclass TabularNet(pl.LightningModule):\n    def __init__(self,num_cols,cat_cols,embedding_size_dict,n_classes,embedding_dim_dict = None):\n        \"\"\"\n        num_cols: A list of the numerical columns\n        cat_cols: A list of cat_cols,\n        embedding_size_dict :  A dictionary of th columns and the number of categories,\n        n_classes = number of classes,\n        embedding_dim_dict: A dictionary of th columns and the dimensions of the ouput embedding\n        \"\"\"\n        super().__init__()\n        self.embeddings , total_embeddings_dim = self._create_embedding_layers(cat_cols,embedding_size_dict,embedding_dim_dict)\n        in_features = len(num_cols) + total_embeddings_dim\n        self.layers = nn.Sequential(\n        nn.Linear(in_features,128),\n            nn.ReLU(),\n            nn.Linear(128,100),\n            nn.ReLU(),\n            nn.Linear(100,n_classes),\n        )\n    @staticmethod\n    def _create_embedding_layers(cat_cols,embedding_size_dict,embeddind_dim_dict):\n        total_embedding_dim = 0\n        embedding_dim = 0\n        embeddings = {}\n        for col in cat_cols:\n            embedding_size = embedding_size_dict[col]\n            embedding_dim = embedding_dim_dict[col]\n            total_embedding_dim +=embedding_dim\n            embeddings[col] = nn.Embedding(embedding_size,embedding_dim)\n            \n        return nn.ModuleDict(embeddings),total_embedding_dim\n        \n    def forward(self,num_tensor,cat_tensor):\n        cat_outputs = []\n        for i,col in enumerate(cat_cols):\n            embedding = self.embeddings[col]\n            cat_output = embedding(cat_tensor[:,i])\n            cat_outputs.append(cat_output)\n        cat_outputs = torch.cat(cat_outputs,dim = 1)\n        all_outputs = torch.cat([num_tensor,cat_outputs],dim = 1)\n        final_output = self.layers(all_outputs).squeeze(dim=-1)\n        return final_output\n        \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=1e-3)\n        \n    def training_step(self,batch,batch_idx):\n        num_tensor,cat_tensor,target = batch\n        y_pred = self(num_tensor,cat_tensor)\n        loss = F.cross_entropy(y_pred,target)\n        return loss\n    def val_step(self,batch,batch_idx):\n        num_tensor,cat_tensor,target = batch\n        y_pred = self(num_tensor,cat_tensor)\n        loss = F.cross_entropy(y_pred,target)\n        return loss\n    def test_step(self,batch,batch_idx):\n        num_tensor,cat_tensor,target = batch\n        y_pred = self(num_tensor,cat_tensor)\n        loss = F.cross_entropy(y_pred,target)\n        self.log(\"Loss:\",loss)\n        return loss","cfbb0be2":"\"\"\"\nThis to determine the size of each embedding dimension check out fast.ai course to get a better understanding of this method\n\"\"\"\nn_classes = 2\nembedding_size_dict = {col: len(code) for col, code in cat_code_dict.items()}\nembedding_dim_dict = {col: embedding_size \/\/ 2 for col, embedding_size in embedding_size_dict.items()}\nembedding_dim_dict","e68ef729":"tabular_data_module = TabularDatsetModule(df_train,df_test,df_val, num_cols, cat_cols, label)\n\n# we can print out the network architecture for inspection\ntabular_model = TabularNet(num_cols, cat_cols, embedding_size_dict, n_classes, embedding_dim_dict)\ntabular_model","f92a9990":"%%time\ntrainer = pl.Trainer(max_epochs=1000)#chose the epochs wisely on a kaggle server 1 epoch takes 17s \ntrainer.fit(tabular_model, tabular_data_module)","2a171d25":"trainer.test()","5a593459":"Good Enough","299ce003":"# Preprocessing Data","9fb3b6da":"![550,500](https:\/\/images.pexels.com\/photos\/149387\/pexels-photo-149387.jpeg?auto=compress&cs=tinysrgb&dpr=1&w=800)","b8099fb0":"# Pytorch Lightning","09343de9":"# Dependencies","4109f669":"Normally for tabular dataset I would use decision trees , random forests , xgboost for a classification problem. How ever for this problem I decided to make a  neural network model using pytorch lightning. I wanted to get out of my comfort zone while making this project. Incase you want a explanation of the model building process I'd recommend checking out fast.ai course they do a better job of explaining than I do.","a0f7e8bb":"# Numerating Categorical Columns","3f013fdf":"Also If you want to see the data visualised here's my [EDA](https:\/\/www.kaggle.com\/aristotle609\/eda-on-hr-dataset) since I won't be doing any EDA in this notebook"}}