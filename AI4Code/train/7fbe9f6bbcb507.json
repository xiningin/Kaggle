{"cell_type":{"0a0627c4":"code","f773641c":"code","7d873e6d":"code","b3885189":"code","1ff205e4":"code","01db3ed7":"code","3715e785":"code","4a3a0632":"code","0d1daf3e":"code","afc9a5b5":"code","e18468aa":"code","3f9a53e5":"code","da17c661":"code","5d45541a":"code","a226fba7":"code","b9310179":"code","8ff1d274":"code","3629b321":"code","5b1db4bc":"code","fe576785":"code","e5f0bffa":"code","04c854f6":"code","98ea50fe":"code","4ffd2cb0":"code","e1dcb5e4":"code","a08ba6a7":"code","f3c9c63f":"code","427c3028":"code","3f2fdab8":"code","26d1b6f0":"code","5adde631":"code","e9e98dda":"code","777a70c7":"code","d7746c4a":"code","4665b884":"code","1cacc8f5":"code","2170ca62":"code","0da75d69":"code","34182cc1":"code","b391025b":"code","3015e786":"code","2318e6d7":"code","e23d433c":"code","363b2418":"code","48819d96":"code","cb26e2e3":"code","6f7493e3":"code","2416f437":"code","1ee5daa6":"code","0636df25":"code","d9991aa7":"code","39f09796":"code","8321cb7a":"code","45f4abc2":"code","13e1d230":"code","f1fb9fe0":"code","50b0274e":"code","d64a8282":"markdown","b87b5111":"markdown","9750857e":"markdown","6fe78c6e":"markdown","85811086":"markdown","f556c402":"markdown","d61ab000":"markdown","4fce35d5":"markdown","cea27c83":"markdown","c5b498ca":"markdown","2c0cc97e":"markdown","f30d0a5d":"markdown","661eaf1f":"markdown","1ff35942":"markdown","03d45c01":"markdown","b0b53a55":"markdown","a3763f46":"markdown","904d15e7":"markdown","d3e62221":"markdown","65cffec2":"markdown","4daa5ea0":"markdown","e9c3f61c":"markdown","f56e2a22":"markdown"},"source":{"0a0627c4":"\"\"\"\nImport important libralies and models\n\"\"\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pylab import *\n\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_predict\n\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn import neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split","f773641c":"\"\"\"\nSetting the figure dimensions\n\"\"\"\n%matplotlib inline\nrcParams['figure.figsize'] = 10, 6","7d873e6d":"\"\"\"\nImporting seaborn library and setting the figure sytle\n\"\"\"\nimport seaborn as sb\nsb.set_style('whitegrid')","b3885189":"\"\"\"\nImporting the dataset \n\"\"\"\ndf = pd.read_csv(\"..\/input\/churn-modellingcsv\/Churn_Modelling.csv\")\n","1ff205e4":"\"\"\"\nChecking that my target variable is binary\n\"\"\"\nsb.countplot(x = 'Exited', data = df, palette='hls')\nplt.show()","01db3ed7":"\"\"\"\nChecking for missing\n\"\"\"\ndf.isnull().sum()","3715e785":"\"\"\"\nGetting the description of the dataset\n\"\"\"\ndf.describe()","4a3a0632":"\"\"\"\nChecking the information regarding the dataset to see whether variables\nhave equal size\n\"\"\"\ndf.info()","0d1daf3e":"\"\"\"\nGetting unique count for each variable\n\"\"\"\ndf.nunique()","afc9a5b5":"\"\"\"\nDropping the irrelevant columns\n\"\"\"\ndf = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis = 1)","e18468aa":"\"\"\"\nChecking the data types for each variable\n\"\"\"\ndf.dtypes","3f9a53e5":"\"\"\"\nChecking for the percentage per category for the target column\n\"\"\"\nlabels_perc = df.Exited.value_counts(normalize = True) * 100\nlabels_perc","da17c661":"\"\"\" \nGetting the correlation matrix of the target and other features\n\"\"\"\ndf[df.columns].corr()","5d45541a":"\"\"\"\nChecking for independence between features using a heat map\n\"\"\"\nsb.heatmap(df.corr(), annot = True, fmt = \".2f\")\nplt.show()","a226fba7":"\"\"\"\nPloting the count plot for categorical to analyse how they are performing between \nchurn and non churn customers\n\"\"\"\n\nCategorical_features = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember']\n\nfig = plt.figure()\nfor i, cat_features in enumerate(Categorical_features):\n    plt.subplot(2, 2, i+1)\n    sb.countplot(x=cat_features, hue = 'Exited',data = df)\n    fig.subplots_adjust(hspace=0.5, wspace=1)\n    plt.title(f'{cat_features}')","b9310179":"\"\"\"\nPloting the count plot for numerical to analyse how they are performing between \nchurn and non churn customers\n\"\"\"\n\nNumerical_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n\nfig = plt.figure()\nfor i, num_features in enumerate(Numerical_features):\n    plt.subplot(2, 3, i+1)\n    sb.boxplot(y=num_features,x = 'Exited', hue = 'Exited',data = df)\n    fig.subplots_adjust(hspace=0.5, wspace=1)\n    plt.title(f'{num_features}')","8ff1d274":"\"\"\"\nConverting categorical variables to a dummy indicators\nfor Gender\n\"\"\"\nlabel_encoder = LabelEncoder()\ngender_cat = df['Gender']\ngender_encoded = label_encoder.fit_transform(gender_cat)","3629b321":"\"\"\"\nfirst 7 values\n\"\"\"\ngender_encoded[0:7]","5b1db4bc":"\"\"\"\n1 = male and 0 = female\n\"\"\"\ngender_DF = pd.DataFrame(gender_encoded, columns=['male_gender'])\ngender_DF.head()","fe576785":"\"\"\"\nFor Geograph using one hot encoder\n\"\"\"\ngeography_cat = df['Geography']\ngeography_encoded = label_encoder.fit_transform(geography_cat)","e5f0bffa":"\"\"\"\nPrinting the first 100 values\n\"\"\"\ngeography_encoded[0:100]","04c854f6":"binary_encoder = OneHotEncoder(categories='auto')\ngeography_1hot = binary_encoder.fit_transform(geography_encoded.reshape(-1, 1))\ngeography_1hot_mat = geography_1hot.toarray()\ngeography_DF = pd.DataFrame(geography_1hot_mat, columns=['France', 'Spain', 'Germany'])\ngeography_DF.head()","98ea50fe":"\"\"\"\nPrinting the first 5 rows\n\"\"\"\ngeography_DF.head()","4ffd2cb0":"\"\"\"\nDropping the original Gender and Geography attributes\n\"\"\"\ndf = df.drop(['Gender', 'Geography'], axis=1)","e1dcb5e4":"\"\"\"\nConcatinating the dummy variables to the original dataset\n\"\"\"\ndf_dummy = pd.concat([df, gender_DF, geography_DF], axis=1, verify_integrity=True)","a08ba6a7":"\"\"\"\nPrinting the first 5 rows\n\"\"\"\ndf_dummy.head()","f3c9c63f":"\"\"\"\nChecking for the independence between features with dummy values using a heat map\n\"\"\"\nsb.heatmap(df_dummy.corr(), annot = True, fmt = \".2f\")\nplt.show()","427c3028":"df_dummy['Balance_Estimate_Salary_Ratio'] = df_dummy['Balance']\/(df_dummy['EstimatedSalary'])","3f2fdab8":"\"\"\"\nNormalizing the Credit score, Age, Balance, EstimatedSalary, Balance_Estimated_Salary_Ratio\n\n\"\"\"\ndf_dummy.CreditScore = (df_dummy.CreditScore - df_dummy.CreditScore.min())\/(df_dummy.CreditScore.max() - df_dummy.CreditScore.min())\n\ndf_dummy.Age = (df_dummy.Age - df_dummy.Age.min())\/(df_dummy.Age.max() - df_dummy.Age.min())\n\ndf_dummy.Balance = (df_dummy.Balance - df_dummy.Balance.min())\/(df_dummy.Balance.max() - df_dummy.Balance.min())\n\ndf_dummy.EstimatedSalary = (df_dummy.EstimatedSalary - df_dummy.EstimatedSalary.min())\/(df_dummy.EstimatedSalary.max() - \\\n                                                                                     df_dummy.EstimatedSalary.min())\n\ndf_dummy.Balance_Estimate_Salary_Ratio = (df_dummy.Balance_Estimate_Salary_Ratio - df_dummy.Balance_Estimate_Salary_Ratio.min())\/ \\\n(df_dummy.Balance_Estimate_Salary_Ratio.max() - df_dummy.Balance_Estimate_Salary_Ratio.min())","26d1b6f0":"\"\"\"\nSeparating the target column which contains answer for row from other attributes\n\"\"\"\nX = df_dummy.drop(['Exited'], axis=1)\ny = df_dummy.Exited","5adde631":"\"\"\"\nSplitting the dataset into training and testing set \n\"\"\"\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 25)","e9e98dda":"\"\"\"\nPrinting the shapes of X_train and y_train datasets\n\"\"\"\nprint(X_train.shape)\nprint(y_train.shape)","777a70c7":"\"\"\"\nInitializing Logistic Regression model\n\"\"\"\nLogReg = LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,intercept_scaling=1, max_iter=1000,multi_class='auto',n_jobs=None, \n                            penalty='l2', random_state=None, solver='lbfgs', tol=1e-05, verbose=0, \n                            warm_start=False)\n\"\"\"\nFitting the model with the training data\n\"\"\"\nLogReg.fit(X_train, y_train)","d7746c4a":"\"\"\"\nPredicting the response for the dataset\n\"\"\"\nyLog_pred = LogReg.predict(X_test)","4665b884":"\"\"\"\nModel Evaluation \n\"\"\"\n\nprint(classification_report(y_test, yLog_pred))","1cacc8f5":"\"\"\"\nK-fold cross-validation and confusion matrices\n\"\"\"\ny_train_pred = cross_val_predict(LogReg, X_train, y_train, cv=5)\nconfusion_matrix(y_train, y_train_pred)","2170ca62":"\"\"\"\nComputing the accuracy of the model\n\"\"\"\nLogReg.score(X_test, y_test)","0da75d69":"\"\"\"\nInitialization of the KNN model\n\"\"\"\nclf = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, \n                                    metric='minkowski', metric_params=None)\n\"\"\"\nFitting the model with training data\n\"\"\"\nclf.fit(X_train, y_train)","34182cc1":"\"\"\"\nEvaluate model predictions\n\"\"\"\nyKNN_pred = clf.predict(X_test)\ny_expect = y_test\n\nprint(classification_report(y_expect, yKNN_pred))","b391025b":"\"\"\"\nComputing model accuracy\n\"\"\"\nclf.score(X_test, y_test)","3015e786":"\"\"\"\nInitialization of the model\n\"\"\"\nclassifier = RandomForestClassifier(n_estimators = 200, random_state = 0)\ny_train_array = np.ravel(y_train)\n\n\"\"\"\nFitting the model with training data\n\"\"\"\nclassifier.fit(X_train, y_train_array)\n\n\n\"\"\"\nPredicting the response for the dataset\n\"\"\"\nyRand_pred = classifier.predict(X_test)","2318e6d7":"\"\"\"\nEvaluating model predictions\n\"\"\"\nprint(classification_report(y_test, yRand_pred))","e23d433c":"\"\"\"\nComputing the model accuracy\n\"\"\"\nclassifier.score(X_test, y_test)","363b2418":"\"\"\"\nComparing the y_test and model predictions\n\"\"\"\ny_test_array = np.ravel(y_test)\nprint(y_test_array)\n\nprint(yRand_pred)","48819d96":"\"\"\"\nInitialization of the AdaBoost model\n\"\"\"\nadaBoost = AdaBoostClassifier(base_estimator= None, n_estimators=200, learning_rate= 1.0)\n\n\"\"\"\nFitting the model with the training data\n\"\"\"\nadaBoost.fit(X_train, y_train)","cb26e2e3":"\"\"\"\nPredicting the response for the dataset\n\"\"\"\nyAda_pred = adaBoost.predict(X_test)","6f7493e3":"\"\"\"\nEvaluating model predictions\n\"\"\"\nprint(classification_report(y_test, yAda_pred))","2416f437":"\"\"\"\nComputing the model accuracy\n\"\"\"\nadaBoost.score(X_test, y_test)","1ee5daa6":"\"\"\"\nCreate Decision Tree classifier object\n\"\"\"\ntreeClf = DecisionTreeClassifier(criterion='entropy', max_depth = 3)\n\n\"\"\"\nTrain Decision Tree Classifier\n\"\"\"\ntreeClf.fit(X_train, y_train)","0636df25":"\"\"\"\nPredicting the response for the dataset\n\"\"\"\nytree_pred = treeClf.predict(X_test)","d9991aa7":"\"\"\"\nEvaluating model predictions\n\"\"\"\nprint(classification_report(y_test, ytree_pred))","39f09796":"\"\"\"\nComputing the model acuracy\n\"\"\"\ntreeClf.score(X_test, y_test)","8321cb7a":"\"\"\"\nInitialization of Gradient Boosting model\n\"\"\"\ngdBoost = GradientBoostingClassifier(loss = 'deviance', n_estimators = 200)\n\n\"\"\"\nFitting the model with the training data\n\"\"\"\ngdBoost.fit(X_train, y_train)","45f4abc2":"\"\"\"\nPredicting the response for the dataset\n\"\"\"\ngd_pred = gdBoost.predict(X_test)","13e1d230":"\"\"\"\nEvaluating model predictions\n\"\"\"\nprint(classification_report(y_test, gd_pred))","f1fb9fe0":"\"\"\"\nComputing the accuracy of the model\n\"\"\"\ngdBoost.score(X_test, y_test)","50b0274e":"\"\"\"\nComparing y_test and gradient boost predicted values\n\"\"\"\nprint(y_test_array)\n\nprint(gd_pred)","d64a8282":"To evaluate the performance of my models, I put into consideration different metrics which include theses ones below;\n* **Recall**: When the actual value is positive, how often is the prediction correct?\n* **Precision** : When a positive value is predicted, how often is the prediction correct?\n* **Accuracy** : The ratio of correctly predicted observation to the total observations\n* The main reason for this is mainly because, my dataset has unbalanced labels **(True (1) and False (0))**.\n* Therefore, putting into consideration those three metrics mentioned above, it is observed that, **Gradient Boosting model** performs better than others followed by **Random Forest** and then **AdaBoost model**.\n* **Decision tree** and **Logistic models** didn't perform well\n\n* In addition, from the analysis made, it was observed that; \n* Germany has the highest proportion of churned customers, followed by France and then Spain\n* France has the highest proportion of non churning customers, followed by Spain and then Germany\n* Also, it is observed that females churn more than males\n* Astonishingly, customers with credit cards have the highest churning rate\n* Lastly, inactive customers churn more compared to active customers\n\n* More to that;\n* Looking at the credit score, non churned and churned customers have a small\/no difference\n* Looking at the age, old customers have a high level of churning rate compared to the young customers\n* Looking at the tenure, customers who have spent more time with the bank are more likely to churn compared to those who have spent an average time with it.\n* Looking at the balance, customers with higher balance are likely to leave which is not good for the bank\n* Looking at number of products, number of products don't have much impact on the rate of churn\n* And also, estimated salary has no any impact on the churn rate.\n\n* Updating the Gradient Boosting, Random Forest and AdaBoost models with more and balanced data would make them perform better","b87b5111":"I would to explore and get more variables I think they can have more impact on the model; below are the variable I came up with;\n\n1. The second variable I came up with is the ratio that puts into account the balance and the estimated salary. This helps see whether customers with high balance ratio will churn or viceversa","9750857e":"The above results show that target label 0 has many records compared to the target label 1. For the label 0, there are about 79.63% whereas for the label 1, there are about 20.37%","6fe78c6e":"From the graph above,\n* it the diagonal values are highly correlated since they are correlated with other\n* balance feature is negatively correlated with NumOfProducts which means that as one is increasing,the other is decreasing.","85811086":"As shown above, the training data is splitted. 30% of the training data will be used to check the training accuracy of the model and the remaining 70% will be used for the actual training purposes.","f556c402":"As it can be observed from above, the model is not predicting well on some values","d61ab000":"### Gradient Boost","4fce35d5":"From the above results, it is observed that, the model is not performing well on some values. \n* However, I believe that by feeding with more and balanced data can make it perform highly better","cea27c83":"The main reason for normalizing is because, there could be some outliers and normalizing them could reduce the \neffect of outliers to the model and most models work well with small values","c5b498ca":"### Logistic Regression","2c0cc97e":"### Conclusion","f30d0a5d":"From the above plots;\n* Looking at the credit score, non churned and churned customers have a small\/no difference\n* Looking at the age, old customers have a high level of churning rate compared to the young customers\n* Looking at the tenure, customers who have spent more time with the bank are more likely to churn compared to those who have spent an average time with it.\n* Looking at the balance, customers with higher balance are likely to leave which is not good for the bank\n* Looking at number of products, number of products don't have much impact on the rate of churn\n* And also, estimated salary has no any impact on the churn rate.","661eaf1f":"The above code, initializes the onehot encoder function, changes the shape of the encoder, makes an array and changes to a dataframe","1ff35942":"The main reason for dropping the above features is because;\n* The rownumber attribute acts like a counter of records\n* The customerid attribute acts as a unique identifier for a customer \n* the surname attribute as an entrie for the customer. \n* Therefore, they don't have any useful impact on the analysis \n and thus dropping them from the dataset doesn't have any negative impact on the model","03d45c01":"Because I need classification model, I will try using different models to choose one with highest performance. And also putting into consideration that my labels are not balanced, trying out different models is better. Below are the models I will use;\n1. Logistic Regression\n2. Random Forest\n3. K-Nearest Neighbor\n4. Decision Tree\n5. AdaBoost\n6. Gradient Boosting","b0b53a55":"### Building Predictive Model","a3763f46":"### Random Forest","904d15e7":"### Decision Tree","d3e62221":"From the above plots;\n* Germany has the highest proportion of churned customers, followed by France and then Spain\n* France has the highest proportion of non churning customers, followed by Spain and then Germany\n* Also, it is observed that females churn more than male\n* Astonishingly, customers with credit cards have the highest churning rate\n* Lastly, inactive customers churn more compared to active customers","65cffec2":"As observed above, there are no missing values in all of the features","4daa5ea0":"### AdaBoost model","e9c3f61c":"### Feature Engineering","f56e2a22":"### KNN"}}