{"cell_type":{"e18d5e7b":"code","254fc964":"code","09d35723":"code","04f6a97d":"code","f4f83a7d":"code","d68d85c4":"code","02437305":"code","48b0eb83":"code","ebbbcca3":"markdown","b11a1d9a":"markdown","d1c9c664":"markdown","68600a17":"markdown"},"source":{"e18d5e7b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","254fc964":"import pandas as pd\nfrom nltk.tag import pos_tag\nimport re\nfrom collections import defaultdict,Counter\nfrom nltk.stem import WordNetLemmatizer\nfrom datetime import datetime\nfrom tqdm import tqdm\nimport numpy as np\nimport os\ntqdm.pandas()\n\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize,sent_tokenize\n\nsw = set(stopwords.words('english'))","09d35723":"df = df = pd.read_csv(os.path.join(dirname,'en.yusufali.csv'))\ndf.head()","04f6a97d":"def process_reviews(df):\n    '''adds three new columns to the input dataframe namely: tokenized, tagged and lower_tagged.\n    \n    New Column Specifications:\n    tokenized: All Verses are first tokenized into list of sentences. These sentences are then further tokenized into\n    words. lastly, English stop words are removed from the words so that further processing is refined and computationally lighter\n    data is generated. \n    tagged: tokenzied words are tagged with 'parts of speech' tags. Eg.: Words that are nouns are tagged as NN, Verbs as VBZ and so on.\n    lower_tagged: all words in the tagged column are lower cased to reduce the size of vocabulary and make the dataset computationally\n    efficient\n    '''\n    #applying sentence tokenization to all reviews\n    df['tokenized'] = df.Text.apply(sent_tokenize)\n    #creating list of lists for tokenized words on tokenized sentences \n    df['tokenized'] = [[word_tokenize(sente) for sente in rev] for rev in df['tokenized']]\n    #filtering out English stop words\n    df['tokenized'] = [[[tk for tk in token if tk.lower() not in sw] for token in tokens]for tokens in df['tokenized']]\n    #pos tagging remaining tokenized words\n    df['tagged'] = df.tokenized.apply(lambda x: [pos_tag(y) for y in x])\n    #lower casing words only\n    df['lower_tagged'] = [[[(item[0].lower(),item[1]) for item in items] for items in its] for its in df['tagged']] \n    return df\n\ndef get_vocab(df):\n    '''Takes in a dataframe and generates two lists of 1000 elements each. One list contains 1000 most frequently occuring Nouns of\n    all types in the comments of the input dataframe. Second list contains 1,000 most frequently occuring Verbs and\/or Adjectives \n    in the comments of the input dataframe. \n    '''\n    \n    #vocab of 1000 most frequent center words\n    vocen = []  #initiating empty list- will hold all center words in the input dataframe\n    for i,j in enumerate(df.lower_tagged):\n        for l in df.lower_tagged[i]:\n            for k in l:\n                #checking if the word is a center word (all types of nouns) or not\n                if k[1][0] == 'N':\n                    vocen.append(k[0])\n    #converting master list to series\n    vocen = pd.Series(vocen)\n    #sorting moaster list in descending order\n    vocen = vocen.value_counts().sort_values(ascending=False)\n    #taking out 1000 most frequent center words\n    vocen= vocen[0:1000]\n    #storing 1000 most frequent center words to a list\n    cent_vocab = vocen.index.tolist()\n    \n    #vocab of 1000 most frequent context words\n    vocon = [] #initiating empty list- will hold all context words in the input dataframe\n    for i,j in enumerate(df.lower_tagged):\n        for l in df.lower_tagged[i]:\n            for k in l:\n                #checking if the word is a context word (all verbs and adjectives) or not\n                if k[1][0] == 'J' or k[1][0] == 'V':\n                    vocon.append(k[0])\n            else:\n                pass\n    ##converting master list to series\n    vocon = pd.Series(vocon)\n    #sorting moaster list in descending order\n    vocon = vocon.value_counts().sort_values(ascending=False)\n    #taking out 1000 most frequent context words\n    vocon= vocon[0:1000]\n    #storing 1000 most frequent context words to a list\n    cont_vocab = vocon.index.tolist()\n    \n    return cent_vocab, cont_vocab \n\ndef get_coocs(df, cent_vocab, cont_vocab):\n    '''Returns a dictionary of dictionaries. The dictionary contains of a primary key which is a noun(center word) belonging to the 1,000 words \n    vocabulary (list: cent_vocab), a secondary key containing verb\/adjective (context word) (from the 1,000 word long vocabulary cont_vocab) and\n    values as the count of context words appearing\/co occuring with the center word. \n    \n    Inputs:\n    df: dataframe with tokenized, pos tagged and lower cased comments\/reviews\n    cent_vocab: 1,000 words long center word (or all types of nouns) vocabulary \n    cont_vocab: 1,000 words long context word (verbs\/adjectives) vocabulary\n    \n    Returns:\n    Dictionary of dictionaries of type:\n    {CenterWord:{ContextWor:Number of times the context word appears with the center word in the 'defined context'}}\n    or\n    {'abc'{'cde':XX}{'saf':YY}.....,'def'{'cde':XX}{'jkl':YY}......,........,}\n    \n    Defined Context for checking co occurence: \n    Context for co occurence is sentence wise parsing since this allows for better center word-context word association\n    '''\n    temp = df\n    coocs = {} #initiating co occurence dictionary\n    \n    for i in temp.lower_tagged:\n        \n        for j in i:\n            cent = [] #initiating list for center words in each sentence\n            lisp = [] #initiating list for context words in each sentence\n            \n            for k in j:\n                \n                if k[1][0] == 'N' and k[0] in cent_vocab:\n                    \n                    cent.append(k[0]) #appending list with center words part of vocabulary of center words in the sentence being checked\n                    \n                if (k[1][0] == 'J' or k[1][0] == 'V') and k[0] in cont_vocab:\n                    \n                    lisp.append(k[0]) #appending list with contest words part of vocabulary of context words in the sentence being checked\n                    \n            cent = list(set(cent)) #removing duplicates of center words\n            lisp = pd.Series(lisp) #converting list of co occured context words to series\n            lisp = lisp.value_counts().sort_values(ascending=False) #sorting counts for each context word in descending order\n            \n            #iterating over all words in the center words list\n            for val in cent:\n                #checking if center word already exists in the cooccurence dictionary\n                if val in coocs:\n                    \n                    #iterating over all co occuring context words\n                    for o in range(len(lisp)):\n                        #checking if context word already exists against associated center word\n                        if lisp.index[o] in coocs[val]:\n                            coocs[val][lisp.index[o]]= coocs[val][lisp.index[o]] + lisp.values[o] #adding to the count of pre existing context word\n                        else:\n                            coocs[val][lisp.index[o]] = lisp.values[o] #initiating context word count for the associated center word\n                #for center words that still do not exist in the cooccurence dictionary \n                else:\n                    #initiating dictionary for new center words\n                    coocs[val] = {}\n                    #adding context words and their counts to the center word dictionary\n                    for o in range(len(lisp)):\n                        coocs[val][lisp.index[o]] = lisp.values[o]                           \n    return coocs  \n\ndef cooc_dict2df(coocs):\n    '''Takes in a dictionary of dictionaries with center word and context words co occurence instances and returns a 1000x1000 dataframe.\n    Shape of dataframe being 1000x1000 is subject to utility of 1,000 words vocabulries for generation of the co-occurenace dictionary. \n    '''\n    coocdf = pd.DataFrame(coocs).transpose().fillna(0) #converting co occurence dictionary to data frame\n    coocdf = coocdf.fillna(0)                          #filling 0 for non co occuring center-context word pairs\n    return coocdf\n\ndef cooc2pmi(df):\n    '''For an input dataframe with row labels as center words and column lables as context words and values signifiying raw number of\n    co-occurences for the center and context words, the function computes and returns a similar dataframe with raw values replaced\n    with PMI (pointwise mutual information) scores.\n    \n    PMI score = log2(joint probability\/(marginal probability of center word)*(marginal probability of context word))\n    \n    Limitation: For cneter-context word pairs that never co-occur a value of '-inf' is returned in the output dataframe\n    '''\n    dit = df.sum(axis=1) #summing over all columns\n    ditt = df.sum(axis=0) #summing over all rows\n    tot = dit.sum() + ditt.sum() #total sum of the dataframe\n\n    jp = df\/tot                            #return df with joint probabilities\n    margr = pd.Series(df.sum(axis=1)\/tot)  #marginal probabilities for center words\/rows\n    margc = pd.Series(df.sum(axis=0)\/tot)  #marginal probabilities for context words\/columns\n    \n    #division of joint prob with product of marginal probs\n    pmi = jp.div(margr,axis=0)            \n    pmi = pmi.mul(1\/margc,axis=1)\n    pmidf = np.log2(pmi)                   #taking log base 2 of the probabilites to get the final pmi scores\n    \n    return pmidf\n\ndef topk(df, center_word, N=10):\n    '''Returns the top-k, where k= N (defaulted to 10), number of context words for a specified center word.\n    \n    Inputs:\n    df: Dataframe with PMI scores for center word and context word pairs.\n    center_word: center word for which top N (default 10) context words are sought.\n    N: Total number of top context words to be returned. Defaulte at 10. \n    \n    Returns:\n    N number of top context words for the input center word. \n    '''\n    top_words = df.loc[center_word].sort_values(ascending = False) #retrieving pmi scores and context words for subject center word in descending order\n    top_words = top_words[0:N].index.tolist() #retrieving top N context words for the subject cneter word\n    return top_words","f4f83a7d":"df = process_reviews(df)\ndf.head()","d68d85c4":"cent_vocab, cont_vocab = get_vocab(df)\ncoocs = get_coocs(df, cent_vocab, cont_vocab)\ncoocdf = cooc_dict2df(coocs)\npmidf = cooc2pmi(coocdf)","02437305":"topk(pmidf,'misfortune')","48b0eb83":"topk(pmidf,'prosperity')","ebbbcca3":"Setting up enviornment","b11a1d9a":"Defining all relevant functions","d1c9c664":"Loading Data","68600a17":"Processing our dataframe with all the defined functions"}}