{"cell_type":{"ef9ed061":"code","4a456f71":"code","ef122a9b":"code","951ce862":"code","3b89c2f2":"code","5816bdeb":"code","c3200136":"code","ec86f258":"code","fe785330":"code","b67a2f4b":"code","d9653336":"code","e53b4ab3":"code","b9314e71":"markdown","3151bbdd":"markdown","5adf6d5d":"markdown","9c6cf32f":"markdown","0dd1d9f6":"markdown","1fa068d1":"markdown","39f00c4b":"markdown","81f1a0b6":"markdown","7f00ca94":"markdown","5f457591":"markdown","f7dd5b3e":"markdown"},"source":{"ef9ed061":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","4a456f71":"train_path = r'..\/input\/glass-imbalanced\/glass (Imbalanced).xlsx'\ndata_train = pd.read_excel(train_path)\ndata_train.head()","ef122a9b":"data_train.info()","951ce862":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom matplotlib.colors import LinearSegmentedColormap\n\ncountClass = data_train['Class'].value_counts().reset_index()\ncountClass.columns = ['Class', 'count']\nprint(countClass)\n\nfig = px.pie(\n    countClass, \n    values='count', \n    names=\"Class\", \n    title='Class Distribution', \n    width=700, \n    height=500\n)\n\nfig.show()","3b89c2f2":"features = data_train.iloc[:,:9].columns.tolist()\nplt.figure(figsize=(18, 27))\n\nfor i, col in enumerate(features):\n    plt.subplot(6, 4, i*2+1)\n    plt.subplots_adjust(hspace =.25, wspace=.3)\n    \n    plt.grid(True)\n    plt.title(col)\n    sns.kdeplot(data_train.loc[data_train[\"Class\"]=='negative', col], label=\"alive\", color = \"blue\", shade=True, cut=0)\n    sns.kdeplot(data_train.loc[data_train[\"Class\"]=='positive', col], label=\"dead\",  color = \"yellow\", shade=True,  cut=0)\n\n    plt.subplot(6, 4, i*2+2) \n    sns.boxplot(y = col, data = data_train, x=\"Class\", palette = [\"blue\", \"yellow\"]) ","5816bdeb":"label_train = data_train.iloc[:,-1].to_numpy()\nfitur_train = data_train.iloc[:,:9].to_numpy()","c3200136":"scaler = MinMaxScaler(feature_range=(0, 1))\nscaler.fit(fitur_train)\nfitur_train_normalize = scaler.transform(fitur_train)","ec86f258":"import scipy.spatial\nimport numpy as np\nfrom operator import itemgetter\nfrom collections import Counter\n\nclass LMKNN:\n    def __init__(self, k):\n        self.k = k\n\n    def fit(self, X, y):\n        self.X_train = X\n        self.y_train = y\n        \n    def distance(self, X1, X2):\n      return scipy.spatial.distance.euclidean(X1, X2)\n    \n    def predict(self, X_test):\n        final_output = []\n        myclass = list(set(self.y_train))\n        for i in range(len(X_test)):\n            eucDist = []\n            votes = []\n            for j in range(len(X_train)):\n                dist = scipy.spatial.distance.euclidean(X_train[j] , X_test[i])\n                eucDist.append([dist, j, self.y_train[j]])\n            eucDist.sort()\n            \n            minimum_dist_per_class = []\n            for c in myclass:\n              minimum_class = []\n              for di in range(len(eucDist)):\n                if(len(minimum_class) != self.k):\n                  if(eucDist[di][2] == c):\n                    minimum_class.append(eucDist[di])\n                else:\n                  break\n              minimum_dist_per_class.append(minimum_class)\n           \n            indexData = []\n            for a in range(len(minimum_dist_per_class)):\n              temp_index = []\n              for j in range(len(minimum_dist_per_class[a])):\n                temp_index.append(minimum_dist_per_class[a][j][1])\n              indexData.append(temp_index)\n\n            centroid = []\n            for a in range(len(indexData)):\n              transposeData = X_train[indexData[a]].T\n              tempCentroid = []\n              for j in range(len(transposeData)):\n                tempCentroid.append(np.mean(transposeData[j]))\n              centroid.append(tempCentroid)\n            centroid = np.array(centroid)\n           \n            eucDist_final = []\n            for b in range(len(centroid)):\n              dist = scipy.spatial.distance.euclidean(centroid[b] , X_test[i])\n              eucDist_final.append([dist, myclass[b]])\n            sorted_eucDist_final = sorted(eucDist_final, key=itemgetter(0))\n            final_output.append(sorted_eucDist_final[0][1])\n        return final_output\n    \n    def score(self, X_test, y_test):\n        predictions = self.predict(X_test)\n        value = 0\n        for i in range(len(y_test)):\n          if(predictions[i] == y_test[i]):\n            value += 1\n        return value \/ len(y_test)","fe785330":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=10, random_state=1, shuffle=True) \nkf.get_n_splits(fitur_train_normalize)","b67a2f4b":"acc_LMKNN = [] \nfor train_index, test_index in kf.split(fitur_train_normalize):\n  lmknn = LMKNN(3)\n  X_train, X_test = fitur_train_normalize[train_index], fitur_train_normalize[test_index]\n  y_train, y_test = label_train[train_index], label_train[test_index]\n\n  lmknn.fit(X_train, y_train) \n  result = lmknn.score(X_test, y_test)\n  acc_LMKNN.append(result)\nprint('LMKNN : ',acc_LMKNN)\nprint('Mean :', np.mean(acc_LMKNN))","d9653336":"import scipy.spatial\nfrom collections import Counter\n\nclass KNN:\n    def __init__(self, k):\n        self.k = k\n        \n    def fit(self, X, y):\n        self.X_train = X\n        self.y_train = y\n\n    def distance(self, X1, X2):\n        return scipy.spatial.distance.euclidean(X1, X2)\n    \n    def predict(self, X_test):\n        final_output = []\n        for i in range(len(X_test)):\n            d = []\n            votes = []\n            for j in range(len(X_train)):\n                dist = scipy.spatial.distance.euclidean(X_train[j] , X_test[i])\n                d.append([dist, j])\n            d.sort()\n            \n            d = d[0:self.k]\n            for d, j in d:\n                votes.append(self.y_train[j])\n            ans = Counter(votes).most_common(1)[0][0]\n            final_output.append(ans)\n            \n        return final_output\n    \n    def score(self, X_test, y_test):\n      predictions = self.predict(X_test)\n      value = 0\n      for i in range(len(y_test)):\n        if(predictions[i] == y_test[i]):\n          value += 1\n      return value \/ len(y_test)","e53b4ab3":"acc_KNN = [] \nfor train_index, test_index in kf.split(fitur_train_normalize):\n  knn = KNN(3)\n  X_train, X_test = fitur_train_normalize[train_index], fitur_train_normalize[test_index]\n  y_train, y_test = label_train[train_index], label_train[test_index]\n\n  knn.fit(X_train,y_train)\n  prediction = knn.score(X_test, y_test)\n  acc_KNN.append(prediction)\nprint('KNN : ', acc_KNN)\nprint('Mean :', np.mean(acc_KNN))","b9314e71":"# Introduction\n\nThis is an example of a **Local mean-based KNN**. I refer to Metha et al. (2018) to implementation this algorithm.\n\nLocal mean-based k-Nearest Neighbor (LMKNN) is one of improved KNN algorithms. The LMKNN can solved the problem of the KNN algorithm being sensitive to outliers, particularly in small sample-size situations. The basic concept of the LMKNN classifier is to compute local mean vectors of k nearest neighbors from each class to classify the query sample [1].","3151bbdd":"![image.png](attachment:image.png)","5adf6d5d":"# References\n\n[1] S. Mehta, X. Shen, J. Gou, and D. Niu, \u201cA new nearest centroid neighbor classifier based on k local means using harmonic mean distance,\u201d Inf., vol. 9, no. 9, 2018, doi: 10.3390\/info9090234","9c6cf32f":"# Comparision with KNN","0dd1d9f6":"## Table of Content\n> **Introduction** <br\/><br\/>\n> **Scenario Experiment & Reading The Dataset**<br\/><br\/>\n> **LMKNN algorithm**<br\/><br\/>\n> **Comparison LMKNN and KNN**<br\/><br\/>\n> **References**","1fa068d1":"# Scenario Experiment & Reading The Dataset\n\nFor this example, i used a Glass dataset from KEEL repository (there has outliers and imbalanced data). Moreover, i compare the performance of LMKNN with the basic KNN algorthm with 10-fold cross validation.\n","39f00c4b":"I used the same data and parameters","81f1a0b6":"# LMKNN\n\nAs i said before, the basic concept of the LMKNN classifier is to compute local mean vectors of k nearest neighbors from each class to classify the query sample. The difference between the KNN and the LMKNN in the k parameter. If in the KNN algorithm, the k-value is the number of closest neighbors, but in the LMKNN, it is the k the nearest neighbors of each class. I try to visualization the difference concept of KNN and LMKNN in this figure below.","7f00ca94":"I used Euclidean to calculate the distance metrics.","5f457591":"# Result\n\nThe experimental result shows that the LMKNN method got the highest overall average classification accuracy of 91,6%. %. This shows that the LMKNN can overcome the problem of outliers and class imbalance with better performance compared to KNN algorithms ","f7dd5b3e":"To generalization the performance of LMKNN, i used k-fold cross validation with 10-fold"}}