{"cell_type":{"4788faf2":"code","6f1c737f":"code","b0366b8e":"code","f1a7f190":"code","77324da0":"code","96b04c48":"code","13febff1":"code","b70ff7e6":"code","fd09ba62":"code","6aaf4641":"code","870b795f":"code","ac0aba4c":"code","02d7b29e":"code","ce7db392":"code","060a12d3":"code","39e4779e":"code","a2069cde":"code","01956731":"code","6b51e925":"code","7707a9b9":"code","824228a4":"markdown","5b471772":"markdown","fd96899d":"markdown","e191550b":"markdown","b27111d1":"markdown","76913ca7":"markdown","7bcedd1e":"markdown","bfbcb1d7":"markdown","bf4e16d0":"markdown"},"source":{"4788faf2":"!apt install aptitude -y\n!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n!pip install mecab-python3==0.996.6rc2\n\n!pip install unidic-lite\n!pip install neologdn","6f1c737f":"import re\nimport gc\nimport os\nimport json\nimport pickle\nimport random\n\nimport pandas as pd\nimport numpy as np\n\nimport tqdm\nfrom matplotlib import pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nfrom torch.nn import functional as F\n\nimport MeCab\nfrom transformers import *\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport neologdn \n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom itertools import chain\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndevice = torch.device('cuda')","b0366b8e":"class config:\n    ENTITY_PATH = \"\/kaggle\/input\/k\/takamichitoda\/aio-final-submission-dataset\/\"\n    MODEL_DIR = \"\/kaggle\/input\/d\/takamichitoda\/aio-my-prediction\"\n    MODELS = [\n        \"model_2.bin\",  # LB BEST\n        \"model_2_very_hard_best_prev.bin\",\n        #\"model_2_dev1_best.bin\",\n        \"model_2_dev2_best.bin\",  # dev1,2 avg BEST\n        #\"model_2_dev2_best_sigmoid.bin\",\n        \"model_2_dev1_train.bin\",\n        \"model_2_dev2_train.bin\",\n    ]\n    SEED = 0\n    MODEL_TYPE = \"cl-tohoku\/bert-base-japanese-whole-word-masking\"\n    TOKENIZER = BertJapaneseTokenizer.from_pretrained(MODEL_TYPE)\n    MAX_LEN = 512\n    N_TTA = 3","f1a7f190":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","77324da0":"#!wget --no-check-certificate https:\/\/www.nlp.ecei.tohoku.ac.jp\/projects\/AIP-LB\/static\/aio_leaderboard.json","96b04c48":"!ls \/kaggle\/input\/aio-final-data\/test_questions_unlabeled.json","13febff1":"#df_test = pd.read_json(\"aio_leaderboard.json\", orient='records', lines=True)\ndf_test = pd.read_json(\"\/kaggle\/input\/aio-final-data\/test_questions_unlabeled.json\", orient='records', lines=True)\nprint(df_test.shape)\ndf_test.head(3)","b70ff7e6":"lst = []\nfor answer_candidates in df_test[\"answer_candidates\"]:\n    lst += answer_candidates\nuse_candidates = list(set(lst))\nuse_candidates_dict = {c: None for c in use_candidates}\nlen(use_candidates_dict)","fd09ba62":"entities_dict = {}\nwith open(f\"{config.ENTITY_PATH}\/all_entities.json\", \"r\") as f:\n    for line in tqdm.tqdm_notebook(f, total=920172):\n        d = json.loads(line)\n        title = d[\"title\"]\n        text = d[\"text\"]\n        try:\n            use_candidates_dict[title]\n        except KeyError:\n            continue\n        entities_dict[title] = text\n        \nlen(entities_dict)","6aaf4641":"del use_candidates, use_candidates_dict\ngc.collect()","870b795f":"def extract_kakko(text):\n    re_text1 = re.findall(r\"\u300c.*?\u300d\", text)\n    re_text2 = re.findall(r\"\u300e.*?\u300f\", text)\n    re_text1 = [t.replace(\"\u300c\", \"\").replace(\"\u300d\", \"\") for t in re_text1]\n    re_text2 = [t.replace(\"\u300e\", \"\").replace(\"\u300f\", \"\") for t in re_text2]\n    return re_text1 + re_text2\n\ndef get_in_q_v2(row):\n    question = row[\"question\"]\n    answer_candidates = row[\"answer_candidates\"]\n    \n    sp_text = question.split(\"\u306e\u3046\u3061\u3001\") \n    if len(sp_text) == 1:\n        in_q = [c in question for c in answer_candidates]\n        return in_q\n    head_text = sp_text[0]\n    \n    kakko_words = extract_kakko(head_text)\n    if len(kakko_words) == 1:\n        in_q = [False for c in answer_candidates]\n        return in_q\n    elif len(kakko_words) > 1:\n        in_q = [cand not in kakko_words for cand in answer_candidates]\n        return in_q\n            \n    sp_dot_text = head_text.split(\"\u3001\")\n    if len(sp_dot_text) == 1:\n        sp_and_text = head_text.split(\"\u3068\")\n        if len(sp_and_text) == 1:\n            in_q = [c in question for c in answer_candidates]\n            return in_q\n\n        new_cand_1 = [c in sp_and_text for c in answer_candidates]\n        new_cand_2 = [sum([c in w for w in sp_and_text]) > 0 for c in answer_candidates]\n        new_cand = [c1 or c2 for c1, c2 in zip(new_cand_1, new_cand_2)]\n        if sum(new_cand) == 0:\n            in_q = [False for c in answer_candidates]\n            return in_q\n        in_q = [not c for c in new_cand]\n        return in_q\n\n    new_cand_1 = [c in sp_dot_text for c in answer_candidates]\n    new_cand_2 = [sum([c in w for w in sp_dot_text]) > 0 for c in answer_candidates]\n    new_cand = [c1 or c2 for c1, c2 in zip(new_cand_1, new_cand_2)]\n    if sum(new_cand) == 0:\n        in_q = [False for c in answer_candidates]\n        return in_q\n    in_q = [not c for c in new_cand]\n    return in_q\n\nkans = '\u3007\u4e00\u4e8c\u4e09\u56db\u4e94\u516d\u4e03\u516b\u4e5d'\nkans_dic = {v: i for i, v in enumerate(kans)}\n\ndef extract_kakko_cand(text):\n    re_text1 = re.findall(r\"\\(.*?\\)\", text)\n    re_text2 = re.findall(r\"\uff08.*?\uff09\", text)\n    re_text1 = [t.replace(\"(\", \"\").replace(\")\", \"\") for t in re_text1]\n    re_text2 = [t.replace(\"\uff08\", \"\").replace(\"\uff09\", \"\") for t in re_text2]\n    return re_text1 + re_text2\n\ndef is_word_length(cand, num):\n    if len(cand) == num:\n        return True\n    kakko = extract_kakko_cand(cand)\n    if len(kakko) == 0:\n        return False\n    if sum([len(w)== num for w in kakko]) > 0:\n        return True\n    _cand = str(cand)\n    for w in kakko + [\"(\", \")\", \"\uff08\", \"\uff09\"]:\n        _cand = _cand.replace(w, \"\")\n    if _cand[-1] == \" \":\n        _cand = _cand[:-1]\n    return len(_cand) == num\n\ndef is_char_type(cand, match_str):\n    match = re.findall(match_str, cand)\n    if len(match) != 0:\n        return True\n    kakko = extract_kakko_cand(cand)\n    if len(kakko) == 0:\n        return False\n    if sum([len(re.findall(match_str, w)) for w in kakko]) > 0:\n        return True\n    _cand = str(cand)\n    for w in kakko + [\"(\", \")\", \"\uff08\", \"\uff09\"]:\n        _cand = _cand.replace(w, \"\")\n    if _cand[-1] == \" \":\n        _cand = _cand[:-1]\n    return len(re.findall(match_str, _cand)) != 0\n\ndef get_in_q_v3(row):\n    question = row[\"question\"]\n    answer_candidates = row[\"answer_candidates\"]\n    \n    sp_text = question.split(\"\u6587\u5b57\u3067\u4f55\")\n    if len(sp_text) == 1:\n        return [False for _ in range(20)]\n        \n    head_text = sp_text[0]\n    \n    num = head_text[-1]\n    try:\n        num = kans_dic[num]\n    except KeyError:\n        num = int(num)\n    \n    if head_text[-3:-1] == \"\u6f22\u5b57\":\n        match_str = r'^[\\u4E00-\\u9FD0]+$'\n    elif head_text[-5:-1] == \"\u3072\u3089\u304c\u306a\":\n        match_str = r'^[\u3042-\u3093]+$'\n    elif head_text[-8:-1] in [\"\u30a2\u30eb\u30d5\u30a1\u30d9\u30c3\u30c8\", \"\u30a2\u30eb\u30d5\u30a1\u3079\u30c3\u30c8\"]:\n        match_str = r'^[a-zA-Z]+$'\n    else:\n        match_str = None\n        \n    is_len = [is_word_length(cand, num) for cand in answer_candidates]\n    if match_str is None:\n        is_char = [True for _ in range(20)]\n    else:\n        is_char = [is_char_type(cand, match_str) for cand in answer_candidates]\n    \n    is_use = [not (i and j) for i, j in zip(is_len, is_char)]\n    return is_use\n\ndef get_in_q_v4(row):\n    question = row[\"question\"]\n    answer_candidates = row[\"answer_candidates\"]\n    if question.find(\"\u548c\u88fd\u82f1\u8a9e\") == -1:\n        return [False for _ in range(20)]\n    is_char = [not is_char_type(cand, r'[\\\u30a1-\u30ff]+') for cand in answer_candidates]\n    return is_char","ac0aba4c":"test_in_q_v2 = df_test.apply(get_in_q_v2, axis=1).tolist()\ntest_in_q_v3 = df_test.apply(get_in_q_v3, axis=1).tolist()\ntest_in_q_v4 = df_test.apply(get_in_q_v4, axis=1).tolist()\nin_q_lst = [list(np.array([i, j, k]).sum(0) != 0) for i, j, k in zip(test_in_q_v2, test_in_q_v3, test_in_q_v4)]","02d7b29e":"class BertForAIO(nn.Module):\n    def __init__(self, activate=\"softmax\"):\n        super(BertForAIO, self).__init__()\n\n        bert_conf = BertConfig(config.MODEL_TYPE)\n        bert_conf.output_hidden_states = True\n        bert_conf.vocab_size = config.TOKENIZER.vocab_size\n\n        self.n_use_layer = 1\n        self.dropout_sample = 5\n        self.activate = activate\n\n        self.bert = AutoModel.from_pretrained(config.MODEL_TYPE, config=bert_conf)\n        \n        self.dropout = nn.Dropout(0.2)\n        n_weights = bert_conf.num_hidden_layers + 1\n        weights_init = torch.zeros(n_weights).float()\n        weights_init.data[:-1] = -3\n        self.layer_weights = torch.nn.Parameter(weights_init)\n        \n        self.dense1 = nn.Linear(bert_conf.hidden_size*self.n_use_layer, bert_conf.hidden_size*self.n_use_layer)\n        self.dense2 = nn.Linear(bert_conf.hidden_size*self.n_use_layer, bert_conf.hidden_size*self.n_use_layer)\n        \n        self.dropouts = nn.ModuleList([nn.Dropout(0.2) for _ in range(self.dropout_sample)])\n    \n        self.fc = nn.Linear(bert_conf.hidden_size*self.n_use_layer, 1)\n\n\n    def forward(self, ids, mask, token_type_ids):\n        n_choice = ids.shape[1]\n        \n        ids = ids.view(-1, ids.size(-1))\n        mask = mask.view(-1, mask.size(-1))\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n\n        _, _, h = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n\n        cat_output = torch.stack([self.dropout(layer[:, 0, :]) for layer in h], dim=2)\n        if self.activate == \"softmax\":\n            cat_output = (torch.softmax(self.layer_weights, dim=0) * cat_output).sum(-1)\n        elif self.activate == \"sigmoid\":\n            cat_output = (torch.sigmoid(self.layer_weights) * cat_output).sum(-1)\n\n        cat_output = self.dense1(cat_output)\n        cat_output = nn.ReLU()(cat_output)\n        cat_output = self.dense2(cat_output)\n        cat_output = nn.ReLU()(cat_output)\n\n        logits = sum([self.fc(dropout(cat_output)) for dropout in self.dropouts])\/self.dropout_sample\n\n        logits = logits.view(-1, n_choice)\n\n        return logits","ce7db392":"models = []\nfor _m in config.MODELS:\n    print(_m)\n    if \"sigmoid\" in _m:\n        print(\"  to sigmoid\")\n        model = BertForAIO(\"sigmoid\")\n    else:\n        model = BertForAIO()\n    model.load_state_dict(torch.load(f\"{config.MODEL_DIR}\/{_m}\", map_location=torch.device('cpu')))\n    model.to(device)\n    model.eval()\n    models.append(model)\n    del model\n    gc.collect()\n    \nprint(\"*** activation ***\")\nprint(models[0].activate, models[1].activate, models[2].activate, models[3].activate, models[4].activate)\n\n#with open(\"\/kaggle\/input\/aio-stacking\/stacking_lr_model.pkl\", \"rb\") as f:\n#    lr_models = pickle.load(f)\nwith open(f\"\/kaggle\/input\/aio-stacking\/stacking_lr_model_without_dev1_train.pkl\", \"rb\") as f:\n    lr_models_without_dev1_train = pickle.load(f)\nwith open(f\"\/kaggle\/input\/aio-stacking\/stacking_lr_model_without_dev2_train.pkl\", \"rb\") as f:\n    lr_models_without_dev2_train = pickle.load(f)\n    \nprint(\"*** model load complete ***\")","060a12d3":"def extract_header(title, text):\n    lines = text.split(\"\u3002\")\n    exp = \"\"\n    lst = []\n    for line in lines:\n        if \"^\" in line[:3]:\n            break\n        if line == \"\":\n            continue\n        lst.append(line)\n    for line in lst:\n        sp_line = line.split(\"\u306f\u3001\")\n        if len(sp_line) > 1:\n            exp = sp_line[1]\n            break\n    if exp == \"\":\n        exp = lines[0]\n    return f\"{title}\u306f\u3001{exp}\", \"\u3002\".join(lst[1:])\n\nm = MeCab.Tagger (\"-Ochasen\")\ndef wakati_mecab(text):\n    wakati = [w.split(\"\\t\") for w in m.parse (text).split(\"\\n\")[:-2]]\n    return [w[0] for w in wakati if w[3].split(\"-\")[0] in [\"\u540d\u8a5e\", \"\u52d5\u8a5e\", \"\u5f62\u5bb9\u8a5e\"]]\n\ndef split_contents(contents, n_char=64, duplicate=5):\n    n_roop = len(contents)\n    lst = []\n    head_i = 0\n    for idx in range(n_roop):\n        tail_i = head_i + n_char\n        line = contents[head_i:tail_i]\n        if len(line) == 0:\n            break\n        lst.append(line)\n        head_i += n_char - duplicate\n    return lst\n\ndef extract_kakko(text):\n    re_text1 = re.findall(r\"\u300c.*?\u300d\", text)\n    re_text2 = re.findall(r\"\u300e.*?\u300f\", text)\n    re_text1 = [t.replace(\"\u300c\", \"\").replace(\"\u300d\", \"\") for t in re_text1]\n    re_text2 = [t.replace(\"\u300e\", \"\").replace(\"\u300f\", \"\") for t in re_text2]\n    return re_text1 + re_text2\n\ndef extract_key_word_line(contents_sp, key_words, is_tta=False):\n    lst1, lst2 = [], []\n    for line in contents_sp:\n        for w in key_words:\n            if w in line:\n                lst1.append(line)\n        if line not in lst1:\n            lst2.append(line)\n    lst1 = list(set(lst1))\n    if is_tta:        \n        random.shuffle(lst1)\n    return \"\u3002\".join(lst1), lst2\n\n\ndef extract_new_contents(title, question, is_tta=False):\n    contents = entities_dict[title]\n\n    header, contents_tail = extract_header(title, contents)\n    contents_sp = split_contents(contents_tail)\n\n    key_words = extract_kakko(question)\n    key_lines, contents_sp = extract_key_word_line(contents_sp, key_words, is_tta)\n\n    q_set = set(wakati_mecab(question))\n    c_sets = [set(wakati_mecab(c)) for c in contents_sp]\n\n    n_common = [len(c & q_set) for c in c_sets]\n    idx = np.array(n_common).argsort()[::-1]\n\n    new_contents = key_lines + \"\u3002\".join(np.array(contents_sp)[idx].tolist())\n    return f\"{header}[SEP]{new_contents}\"\n\ndef process_but(text):\n    sp_q = text.split(\"\u3067\u3059\u304c\u3001\")\n    tail_text = sp_q[-1]\n    sp_head = sp_q[0].split(\"\u3001\")\n    if len(sp_head) > 1:\n        text = sp_head[0] + \"\u3001\" + tail_text\n    else:\n        text = tail_text\n    return text\n\ndef process_q_but(question):\n    sp_q = question.split(\"\u3067\u3059\u304c\u3001\")\n    if len(sp_q) == 1:\n        return question\n    q_head, q_tail = sp_q\n    sp_q_head = q_head.split(\"\u3067\u3001\")\n    if len(sp_q_head) != 2:\n        return q_tail\n    processed_text = sp_q_head[0] + \"\u3067\u3001\"+ q_tail\n    return processed_text\n\ndef process_q_but_v2(question):\n    sp_q = question.split(\"\u306b\u5bfe\u3057\u3001\")\n    if len(sp_q) == 1:\n        return question\n    q_head, q_tail = sp_q\n    sp_q_head = q_head.split(\"\u3067\u3001\")\n    if len(sp_q_head) != 2:\n        return q_tail\n    processed_text = sp_q_head[0] + \"\u3067\u3001\"+ q_tail\n    return processed_text\n\ndef tokenize_text(question, answer_candidates, is_tta=False):\n    #if \"\u3067\u3059\u304c\u3001\" in question:\n    #    question = process_but(question)\n    question = process_q_but(question)\n    question = process_q_but_v2(question)\n    \n    _ids, _mask, _id_type = [], [], []\n    for candidate in answer_candidates:\n        contents = extract_new_contents(candidate, question, is_tta)\n        \n        tok = config.TOKENIZER.encode_plus(contents,\n                                           question,\n                                           add_special_tokens=True,\n                                           max_length=config.MAX_LEN,\n                                           truncation_strategy=\"only_first\",\n                                           pad_to_max_length=True)\n        _ids.append(tok[\"input_ids\"])\n        _mask.append(tok[\"attention_mask\"])\n        _id_type.append(tok[\"token_type_ids\"])\n    \n    d = {\n        \"input_ids\": torch.tensor(_ids),\n        \"attention_mask\": torch.tensor(_mask),\n        \"token_type_ids\": torch.tensor(_id_type),\n    }\n    return d","39e4779e":"IGNORE_WORDS = [\n    \"\u3042\u308b\", \"\u4f55\", \"?\", \"\u305f\u3081\", \"\u3053\u3068\", \"\u3089\u308c\", \"\u3044\", \"\u3059\u308b\", \n    \"\u3044\u3046\", \"\u3064\u304f\", \"\u306e\", \"\u3046\u3061\", \"\u3067\u304d\u308b\", \"\u884c\u3046\", \"\u8868\u3059\"\n]\ndef wakati_mecab_v2(text):\n    wakati = [w.split(\"\\t\") for w in m.parse (text).split(\"\\n\")[:-2]]\n    \n    pos_filter = [w[3].split(\"-\")[0] in [\"\u540d\u8a5e\", \"\u52d5\u8a5e\", \"\u5f62\u5bb9\u8a5e\"] for w in wakati]\n    verb_filter = [w[3] not in [\"\u52d5\u8a5e-\u63a5\u5c3e\", \"\u52d5\u8a5e-\u975e\u81ea\u7acb\"] for w in wakati]\n    is_filter = np.array(verb_filter).astype(int) + np.array(pos_filter).astype(int) == 2\n    wakati = np.array(wakati)[is_filter]\n    \n    words1 = [w[2] for w in wakati if w[2] not in IGNORE_WORDS]  # \u8868\u5c64=2\n    words2 = [w[1] for w in wakati if w[2] not in IGNORE_WORDS]  # \u8aad\u307f=1\n    words = list(set(words1 + words2))\n    return words\n\ndef split_question(question):\n    question_norm = neologdn.normalize(question)\n    key_phrase = extract_kakko(question_norm)\n    _question_norm = question_norm\n    for w in key_phrase:\n        _question_norm = _question_norm.replace(w, \"\")\n    sep_question = wakati_mecab_v2(_question_norm)\n    return key_phrase, sep_question\n\n\nurl_pattern = r'(http|https):\/\/([-\\w]+\\.)+[-\\w]+(\/[-\\w.\/?%&=]*)?'\ndef extract_contents_df(question, contents):\n    key_phrase, sep_question = split_question(question)\n    \n    lst = []\n    lines = split_contents(contents)\n    for line in lines:\n        if len(line) == 0:\n            continue\n        if line[0] == \" \":\n            line = line[1:]\n        line = re.sub(url_pattern, \"\", line)\n        line = neologdn.normalize(line)\n        if len(line) == 0:\n            continue\n\n        _key_phrase= extract_kakko(line)\n        words = [w for w in key_phrase if w in line]\n        sp_line = wakati_mecab_v2(line)\n        common_words = list(set(sep_question) & set(sp_line))\n            \n        d = (len(words), len(common_words), line, words, common_words)\n\n        lst.append(d)\n    contents_df = pd.DataFrame(lst, columns=[\"n_keyword\", \"n_common\", \"text\", \"keyword\", \"common\"])\n    contents_df = contents_df.sort_values([\"n_keyword\", \"n_common\"], ascending=False).reset_index(drop=True)\n    return contents_df\n\ndef extract_contents(contents_df):\n    exist_words = []\n    texts = []\n    sub_texts = []\n    for _, row in contents_df.iterrows():\n        words = row[\"keyword\"] + row[\"common\"]\n        diff_words = set(words) - set(exist_words)\n        if len(diff_words) > 0:\n            exist_words = list(set(words) | set(exist_words))\n            texts.append(row[\"text\"])\n        else:\n            sub_texts.append(row[\"text\"])\n    extracted_content = \"\u3002\".join(texts+sub_texts)\n    return extracted_content\n\ndef extract_contents_df_v2(question, contents):\n    key_phrase, sep_question = split_question(question)\n    \n    lst = []\n    corpus = []\n    lines = split_contents(contents)\n    for line in lines:\n        if len(line) == 0:\n            continue\n        if line[0] == \" \":\n            line = line[1:]\n        line = re.sub(url_pattern, \"\", line)\n        line = neologdn.normalize(line)\n        if len(line) == 0:\n            continue\n\n        _key_phrase= extract_kakko(line)\n        words = [w for w in key_phrase if w in line]\n        sp_line = wakati_mecab(line)\n        corpus.append(\" \".join(sp_line))\n        common_words = list(set(sep_question) & set(sp_line))\n            \n        d = (len(words), len(common_words), line, words, common_words)\n\n        lst.append(d)\n    contents_df = pd.DataFrame(lst, columns=[\"n_keyword\", \"n_common\", \"text\", \"keyword\", \"common\"])\n    \n    # Calc TF-IDF\n    vectorizer = TfidfVectorizer()\n    X = vectorizer.fit_transform(corpus)\n    use_w = list(set(chain.from_iterable(contents_df[\"common\"].tolist())))\n    \n    idx = []\n    for w in use_w:\n        try:\n            _i = vectorizer.get_feature_names().index(w)\n        except ValueError:\n            continue\n        idx.append(_i)\n    if len(idx) == 0:\n        contents_df[\"tfidf\"] = 0\n    else:\n        tfidf_score = np.array(X[:, idx].sum(1))[:, 0]\n        contents_df[\"tfidf\"] = tfidf_score\n\n    contents_df = contents_df.sort_values([\"n_keyword\", \"n_common\", \"tfidf\"], ascending=False).reset_index(drop=True)\n    \n    return contents_df\n\ndef tokenize_text_v2(question, answer_candidates):\n    #if \"\u3067\u3059\u304c\u3001\" in question:\n    #    question = process_but(question)\n    question = process_q_but(question)\n    question = process_q_but_v2(question)\n        \n    _ids, _mask, _id_type = [], [], []\n    for candidate in answer_candidates:\n\n        contents = entities_dict[candidate]\n        #contents_df = extract_contents_df(question, contents)\n        contents_df = extract_contents_df_v2(question, contents)\n        extracted_content = extract_contents(contents_df)\n\n        tag = \"[SEP]\"\n        extracted_content = candidate + tag + extracted_content\n        \n        tok = config.TOKENIZER.encode_plus(extracted_content,\n                                           question,\n                                           add_special_tokens=True,\n                                           max_length=config.MAX_LEN,\n                                           truncation_strategy=\"only_first\",\n                                           pad_to_max_length=True)\n        _ids.append(tok[\"input_ids\"])\n        _mask.append(tok[\"attention_mask\"])\n        _id_type.append(tok[\"token_type_ids\"])\n    \n    d = {\n        \"input_ids\": torch.tensor(_ids),\n        \"attention_mask\": torch.tensor(_mask),\n        \"token_type_ids\": torch.tensor(_id_type),\n    }\n    return d","a2069cde":"seed_everything(config.SEED)\n\npredicts = []\ntk = tqdm.tqdm_notebook(enumerate(df_test[[\"qid\", \"question\", \"answer_candidates\"]].values), total=len(df_test))\nfor idx, (qid, question, answer_candidates) in tk:\n    in_q = np.array(in_q_lst[idx]).astype(int)\n    \n    input_ids, attention_mask, token_type_ids = [], [], []\n    for _ in range(config.N_TTA):\n        d = tokenize_text(question, answer_candidates, is_tta=True)\n        _input_ids = d[\"input_ids\"].to(device)\n        _attention_mask = d[\"attention_mask\"].to(device)\n        _token_type_ids = d[\"token_type_ids\"].to(device)\n        input_ids.append(_input_ids)\n        attention_mask.append(_attention_mask)\n        token_type_ids.append(_token_type_ids)\n    input_ids = torch.stack(input_ids)\n    attention_mask = torch.stack(attention_mask)\n    token_type_ids = torch.stack(token_type_ids)\n    \n    all_preds = []\n    for model in models[:2]:\n        with torch.no_grad():\n            preds = model(input_ids, attention_mask, token_type_ids)\n        preds = preds.cpu().detach()\n        all_preds.append(preds)\n    \n    \n    d = tokenize_text_v2(question, answer_candidates)\n    input_ids = d[\"input_ids\"].unsqueeze(0).to(device)\n    attention_mask = d[\"attention_mask\"].unsqueeze(0).to(device)\n    token_type_ids = d[\"token_type_ids\"].unsqueeze(0).to(device)\n    \n    for model in models[2:]:\n        with torch.no_grad():\n            preds = model(input_ids, attention_mask, token_type_ids)\n        preds = preds.cpu().detach()\n        all_preds.append(preds)\n\n    # stacking\n    all_preds = np.stack([\n        all_preds[0].numpy().max(0), all_preds[1].numpy().max(0),\n        all_preds[2].numpy()[0], all_preds[3].numpy()[0],\n        all_preds[4].numpy()[0]])\n    \n    #lr_preds = []\n    #for lr_model in lr_models:\n    #    lr_pred = lr_model.predict_proba(all_preds.T)[:, 1]\n    #    lr_preds.append(lr_pred)\n    #pred = np.array(lr_preds).mean(0)\n    preds_without_dev1 = all_preds[[0, 1, 2, 4], :]\n    preds_without_dev2 = all_preds[[0, 1, 2, 3], :]\n    lr_preds = []\n    for lr_model in lr_models_without_dev1_train:\n        lr_pred = lr_model.predict_proba(preds_without_dev1.T)[:, 1]\n        lr_preds.append(lr_pred)\n    for lr_model in lr_models_without_dev2_train:\n        lr_pred = lr_model.predict_proba(preds_without_dev2.T)[:, 1]\n        lr_preds.append(lr_pred)\n    pred = np.array(lr_preds).mean(0)\n    \n    pred = pred + (-99999 * in_q)\n    \n    label_idx = pred.argmax(0)   \n    label = answer_candidates[label_idx]\n    \n    hyp = {\"qid\": qid,\"answer_entity\": label}\n    predicts.append(hyp)","01956731":"df_predicts = pd.DataFrame(predicts)\ndf_predicts.to_json(f'test_predict.jsonl', orient='records', force_ascii=False, lines=True)\nprint(df_predicts.shape)\ndf_predicts.head(3)","6b51e925":"!head \"test_predict.jsonl\"","7707a9b9":"!ls","824228a4":"##### \u554f\u984c\u6587\u306b\u3042\u308b\u5019\u88dc\u3092\u62bd\u51fa","5b471772":"### \u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f","fd96899d":"Utility","e191550b":"### \u30e2\u30c7\u30eb","b27111d1":"### \u30d1\u30e9\u30e1\u30fc\u30bf","76913ca7":"#### \u5019\u88dcWiki\u30c7\u30fc\u30bf","7bcedd1e":"# AI\u738b \u6700\u7d42\u63d0\u51fa\u7528\n## \u63a8\u8ad6\n### \u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u8aad\u307f\u8fbc\u307f","bfbcb1d7":"##### \u30e1\u30e2\u30ea\u524a\u6e1b","bf4e16d0":"#### \u6700\u7d42\u554f\u984c(\u73fe\u72b6\u516c\u958b\u554f\u984c)"}}