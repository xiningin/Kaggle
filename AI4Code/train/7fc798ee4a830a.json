{"cell_type":{"d7ac2967":"code","a4404009":"code","451adc41":"code","3153f160":"code","e47828fe":"code","2b63ef67":"code","0e76ac9b":"code","037f6c40":"code","dc03372f":"code","98507fb7":"code","a36b64cd":"code","3af95714":"code","428e4b29":"code","ab4efe2f":"code","119d7127":"code","5bc981c8":"code","233bdec7":"code","ede1d2b4":"code","aca0b328":"code","45a4655d":"code","efb70e13":"code","c37c2c94":"code","956bec7c":"code","2c52a91b":"code","c1eb3d7b":"code","71863651":"code","57c47fa2":"code","14505b79":"code","76867a7a":"code","8d86ffa1":"code","2e7952a9":"code","af7dfa12":"code","52d81dd8":"code","ad7fe847":"code","3a44d830":"code","88e0e069":"code","fb07fb09":"code","05f6725c":"code","d9f309ca":"code","9f38b630":"code","a1d25f5d":"code","f667d4d7":"code","f01bd01d":"code","713e28bf":"code","a395974f":"code","3e90b940":"code","22f36b5d":"code","8feee266":"code","86043c0a":"code","c378e84e":"code","f5ad9478":"code","1559399d":"code","05ecc3db":"code","e2c834a2":"code","e57f484c":"code","6add4618":"code","3fdbbf61":"code","ec104088":"code","75e0f0ad":"code","99dfe49d":"code","183bd0de":"code","b6342117":"code","cbfefd8e":"code","2b65794e":"code","735e0f20":"code","8c1788ac":"code","1ad45357":"code","ebc64429":"code","dbe3a9e8":"code","5e0e1f56":"code","5f97701a":"code","1096fd9a":"code","68ea077d":"code","443dc984":"code","b5550a96":"code","39969464":"code","1713e22f":"code","8815a025":"code","61c0691c":"code","b728057a":"code","1863b86a":"code","7646066f":"code","35faa221":"code","8432cce4":"code","238ead7b":"code","6e9d5c43":"code","bd8a4c2f":"code","f6ec6b97":"code","28dae2b7":"code","5fe0fc32":"code","cc231d21":"markdown","429f5841":"markdown","fbb3cd21":"markdown","0878dc98":"markdown","79f75eef":"markdown","8499b108":"markdown","44c721b5":"markdown","6d3be822":"markdown","e5c121bd":"markdown","ef30a35b":"markdown","7e0ebab7":"markdown","28ea7ef5":"markdown","6bebc1e4":"markdown","ee815cf5":"markdown","f9758060":"markdown","31633f36":"markdown","a14fbb09":"markdown","5a9a926e":"markdown","ec35bee5":"markdown","127acb73":"markdown","0375c560":"markdown","7fc2685b":"markdown","bd18f249":"markdown","eeb6ad6e":"markdown","ff8b05c6":"markdown","4379296a":"markdown","dd7a5270":"markdown","3c3e3a82":"markdown","4268c979":"markdown","13d5ed6d":"markdown","a4b1957a":"markdown","c14ee93d":"markdown","08ebe1a9":"markdown","dc96f186":"markdown","758e0c9c":"markdown","f454588a":"markdown","9671f8a8":"markdown","17ca5ff2":"markdown","d0fc7ca6":"markdown","8f1fe04f":"markdown","b7463508":"markdown","90940d9a":"markdown","bd650f3d":"markdown","f0a9679c":"markdown","9faf3777":"markdown","0f5145d9":"markdown","9198b614":"markdown","97d96def":"markdown","1537bedd":"markdown","d8de3c99":"markdown","7c65a562":"markdown","f06bc813":"markdown","7ec42106":"markdown","27cb859d":"markdown","1bb6f9f0":"markdown","a31a292b":"markdown","067787c1":"markdown","0abbf6a9":"markdown","65893a9b":"markdown","bd28ca17":"markdown","400285e3":"markdown","dad8068b":"markdown","5c8b4258":"markdown"},"source":{"d7ac2967":"# NumPy is the fundamental package for scientific computing\nimport numpy as np\n# Pandas is a high-level data manipulation tool\nimport pandas as pd\n# Matplotlib is a plotting library\nimport matplotlib.pyplot as plt\n# Seaborn is a Python data visualization library based on matplotlib\nimport seaborn as sns\n\n# import Warnings library and disable all warnings\nimport warnings \nwarnings.filterwarnings('ignore')\n\n# import Os and display the list of available data\nimport os\nprint(os.listdir('..\/input'))","a4404009":"# load train and test data\ntrain_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')","451adc41":"# show train and test shape\nprint(f'Train data shape: {train_data.shape}\\nTest data shape: {test_data.shape}')","3153f160":"# show the first five lines of train data\ntrain_data.head()","e47828fe":"# show the first five lines of test data\ntest_data.head()","2b63ef67":"# separate the PassengerId feature, which is needed to save the final result\ntest_id = test_data['PassengerId']","0e76ac9b":"# combine train and test data for further analysis and engineering\ndataset = pd.concat((train_data, test_data), sort=True)","037f6c40":"# drop PassengerId feature\ndataset.drop(['PassengerId'], axis=1, inplace=True)","dc03372f":"# show information about our dataset\ndataset.info()","98507fb7":"# describe our dataset\ndataset.describe()","a36b64cd":"# correlation matrix between numerical values and Survived\nfig, ax = plt.subplots(figsize=(10, 8))\nplot = sns.heatmap(train_data[['Survived','SibSp','Parch','Age','Fare']].corr(), annot=True, cmap='BuPu', ax=ax)","3af95714":"# explore Age distribution\nplot = sns.kdeplot(train_data['Age'][(train_data['Survived'] == 0) &\n                   (train_data['Age'].notnull())], color='darkorchid', shade=True)\nplot = sns.kdeplot(train_data['Age'][(train_data['Survived'] == 1) & \n                   (train_data['Age'].notnull())], ax=plot, color='darkblue', shade=True)\nplot.set_xlabel('Age')\nplot.set_ylabel('Frequency')\nplot = plot.legend(['Not Survived','Survived'])","428e4b29":"# fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n\n# get indexes of null\nnull_age = list(dataset['Age'][dataset['Age'].isnull()].index)\n\nfor i in null_age:\n    # get median value of age\n    age_median = dataset['Age'].median()\n    # get median age of similar rows according to Pclass, Parch and SibSp\n    age_predict = dataset['Age'][((dataset['SibSp'] == dataset.iloc[i]['SibSp']) &\n                                  (dataset['Parch'] == dataset.iloc[i]['Parch']) & \n                                  (dataset['Pclass'] == dataset.iloc[i]['Pclass']))].median()\n    # if exists a similar value then fill the value of age_predict\n    if not np.isnan(age_predict):\n        dataset.loc[i, 'Age'] = age_predict\n    # if not exists fill the value of age_median\n    else:\n        dataset.loc[i, 'Age'] = age_median","ab4efe2f":"# divide Age feature\ndataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\ndataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\ndataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\ndataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\ndataset.loc[ dataset['Age'] > 64, 'Age'] = 4","119d7127":"# explore Age groups\nplot = sns.catplot(x='Age', y='Survived', data=dataset, kind='bar', palette='BuPu')\nplot.despine(left=True)\nplot = plot.set_ylabels('Survival Rate')","5bc981c8":"# convert to indicator variables\ndataset = pd.get_dummies(dataset, columns=['Age'])","233bdec7":"# explore SibSp vs Survival\nplot = sns.catplot(x='SibSp', y='Survived', data=dataset, kind='bar', palette='BuPu')\nplot.despine(left=True)\nplot = plot.set_ylabels('Survival Rate')","ede1d2b4":"# explore Parch vs Survival\nplot = sns.catplot(x='Parch', y='Survived', data=dataset, kind='bar', palette='BuPu')\nplot.despine(left=True)\nplot = plot.set_ylabels('Survival Rate')","aca0b328":"# create Fsize feature\ndataset['Fsize'] = dataset['SibSp'] + dataset['Parch'] + 1","45a4655d":"# explore Fsize vs Survived\nplot = sns.catplot(x='Fsize', y='Survived', data=dataset, kind='bar', palette='BuPu')\nplot.despine(left=True)\nplot = plot.set_ylabels('Survival Rate')","efb70e13":"# divide Fsize feature\ndataset.loc[dataset['Fsize'] == 1, 'Fsize'] = 0\ndataset.loc[dataset['Fsize'] == 2, 'Fsize'] = 1\ndataset.loc[(dataset['Fsize'] >=3) & (dataset['Fsize'] <=4), 'Fsize'] = 2\ndataset.loc[dataset['Fsize'] >= 5, 'Fsize'] = 3","c37c2c94":"# explore Fsize groups vs Survived\nplot = sns.catplot(x='Fsize', y='Survived', data=dataset, kind='bar', palette='BuPu')\nplot = plot.set_xticklabels(['Single', 'Small', 'Medium', 'Large'])\nplot.despine(left=True)\nplot = plot.set_ylabels('Survival Rate')","956bec7c":"# convert to indicator variables\ndataset['Single'] = dataset['Fsize'].map(lambda s: 1 if s == 1 else 0)\ndataset['SmallF'] = dataset['Fsize'].map(lambda s: 1 if  s == 2  else 0)\ndataset['MediumF'] = dataset['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndataset['LargeF'] = dataset['Fsize'].map(lambda s: 1 if s >= 5 else 0)","2c52a91b":"# fill Fare with the median\ndataset['Fare'] = dataset['Fare'].fillna(dataset['Fare'].median())","c1eb3d7b":"# explore Fare distribution\nplot = sns.distplot(dataset['Fare'], \n                    label='Skewness: %.2f'%(dataset['Fare'].skew()), color='darkblue')\nplot = plot.legend(loc='best')","71863651":"# transform feature with the log function\ndataset['Fare'] = dataset['Fare'].map(lambda x: np.log(x) if x > 0 else 0)","57c47fa2":"# explore Fare distribution again\nplot = sns.distplot(dataset['Fare'], \n                    label='Skewness: %.2f'%(dataset['Fare'].skew()), color='darkblue')\nplot = plot.legend(loc='best')","14505b79":"# explore Pclass vs Survived\nplot = sns.catplot(x='Pclass',y='Survived', \n                   data=train_data, kind='bar', palette='BuPu')\nplot.despine(left=True)\nplot.set_ylabels('Survival Rate')\n\n# explore Pclass vs Survived by Sex\nplot = sns.catplot(x='Sex', y='Survived', hue='Pclass', \n                   data=train_data, kind='bar', palette='BuPu')\nplot.despine(left=True)\nplot = plot.set_ylabels('Survival Rate')","76867a7a":"# convert to indicator variables\ndataset = pd.get_dummies(dataset, columns = ['Pclass'])","8d86ffa1":"# explore Embarked frequency histogram\nplot = sns.catplot('Embarked', data=dataset, kind='count', palette='BuPu')\nplot.despine(left=True)\nplot = plot.set_ylabels('Count')","2e7952a9":"# fill Embarked with the most frequent value\ndataset['Embarked'] = dataset['Embarked'].fillna('S')","af7dfa12":"# explore Embarked vs Survived\nplot = sns.catplot(x='Embarked', y='Survived', data=train_data, kind='bar', palette='BuPu')\nplot.despine(left=True)\nplot = plot.set_ylabels('Survival Rate')","52d81dd8":"# explore Age vs Embarked\nplot = sns.catplot('Sex', col='Embarked', data=train_data, kind='count', palette='BuPu')\nplot.despine(left=True)\nplot = plot.set_ylabels('Count')\n\n# explore Pclass vs Embarked\nplot = sns.catplot('Pclass', col='Embarked', data=train_data, kind='count', palette='BuPu')\nplot.despine(left=True)\nplot = plot.set_ylabels('Count')","ad7fe847":"# convert to indicator variables\ndataset = pd.get_dummies(dataset, columns = ['Embarked'])","3a44d830":"# explore Sex vs Survived\nplot = sns.catplot(x='Sex',y='Survived', data=train_data, kind='bar', palette='BuPu')\nplot.despine(left=True)\nplot = plot.set_ylabels('Survival Rate')","88e0e069":"# convert to indicator variables\ndataset = pd.get_dummies(dataset, columns = ['Sex'])","fb07fb09":"# number of null values\ndataset['Cabin'].isnull().sum()","05f6725c":"# show the first five not null values\ndataset['Cabin'][dataset['Cabin'].notnull()].head()","d9f309ca":"# replace the Cabin number by the type of cabin 'X' if not\ndataset['Cabin'] = pd.Series([x[0] if not pd.isnull(x) else 'X' for x in dataset['Cabin']])","9f38b630":"# explore Cabin frequency histogram\nplot = sns.countplot(dataset['Cabin'], \n                     order=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T', 'X'], palette='BuPu')","a1d25f5d":"# explore Cabin vs Survived\nplot = sns.catplot(x='Cabin', y='Survived', data=dataset, kind='bar', \n                   order=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T', 'X'], palette='BuPu')\nplot.despine(left=True)\nplot = plot.set_ylabels('Survival Rate')","f667d4d7":"# convert to indicator variables\ndataset = pd.get_dummies(dataset, columns=['Cabin'], prefix='Cabin')","f01bd01d":"# show the first five values\ndataset['Name'].head()","713e28bf":"# create Title feature from Name\ndataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)","a395974f":"# explore Title frequency histogram\nfig, ax = plt.subplots(figsize=(10, 6))\nplot = sns.countplot(x='Title',data=dataset, palette='BuPu', \n                     order=dataset['Title'].value_counts(ascending=True).index, ax=ax)\nplot = plt.setp(plot.get_xticklabels(), rotation=90) ","3e90b940":"# convert Title feature to categorical values\ndataset['Title'] = dataset['Title'].replace(['Lady', 'the Countess', 'Countess', 'Capt',\n                                             'Col','Don', 'Dr', 'Major', 'Rev', 'Sir',\n                                             'Jonkheer', 'Dona'], 'Rare')\ndataset['Title'] = dataset['Title'].map({'Mr': 0, 'Miss': 1, 'Ms': 1, 'Mlle': 1,\n                                         'Mrs': 1, 'Mme': 1, 'Master': 2, 'Rare': 3})","22f36b5d":"# explore Title frequency histogram\nplot = sns.countplot(dataset['Title'], palette='BuPu')\nplot = plot.set_xticklabels(['Mr', 'Miss', 'Master', 'Rare'])","8feee266":"# explore Title vs Survived\nplot = sns.catplot(x='Title', y='Survived', data=dataset, kind='bar', palette='BuPu')\nplot = plot.set_xticklabels(['Mr', 'Miss', 'Master', 'Rare'])\nplot.despine(left=True)\nplot = plot.set_ylabels('Survival Rate')","86043c0a":"# drop Name feature\ndataset.drop(labels=['Name'], axis=1, inplace=True)","c378e84e":"# show the first five values\ndataset['Ticket'].head()","f5ad9478":"# treat Ticket by extracting the ticket prefix. 'X' if not prefix\ntickets = []\nfor i in list(dataset['Ticket']):\n    if not i.isdigit() :\n        tickets.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0])\n    else:\n        tickets.append(\"X\")\n        \ndataset['Ticket'] = tickets","1559399d":"# explore Ticket frequency histogram\nfig, ax = plt.subplots(figsize=(12, 6))\nplot = sns.countplot(x='Ticket', data=dataset, \n                     order=dataset['Ticket'].value_counts(ascending=True).index, palette='BuPu')\nplot = plt.setp(plot.get_xticklabels(), rotation=90)","05ecc3db":"# convert to indicator variables\ndataset = pd.get_dummies(dataset, columns=['Ticket'])","e2c834a2":"# show the first five lines of dataset\ndataset.head()","e57f484c":"# separate our dataset on train and test data\ntrain = dataset[:train_data.shape[0]]\ntest = dataset[train_data.shape[0]:]\ntest.drop('Survived', axis=1, inplace=True)","6add4618":"# separate train features and label\ntrain['Survived'] = train['Survived'].astype(int)\n\nY_train = train['Survived']\nX_train = train.drop(['Survived'], axis=1)","3fdbbf61":"#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics","ec104088":"# initialization algorithms\nalgorithms = {# Ensemble Methods\n              ensemble.AdaBoostClassifier(),\n              ensemble.BaggingClassifier(),\n              ensemble.ExtraTreesClassifier(),\n              ensemble.GradientBoostingClassifier(),\n              ensemble.RandomForestClassifier(),\n\n              # Gaussian Processes\n              gaussian_process.GaussianProcessClassifier(),\n    \n              # Generalized Linear Models\n              linear_model.LogisticRegressionCV(),\n              linear_model.PassiveAggressiveClassifier(),\n              linear_model.RidgeClassifierCV(),\n              linear_model.SGDClassifier(),\n              linear_model.Perceptron(),\n    \n              # Navies Bayes\n              naive_bayes.BernoulliNB(),\n              naive_bayes.GaussianNB(),\n    \n              # Nearest Neighbor\n              neighbors.KNeighborsClassifier(),\n    \n              # Support Vector Machine\n              svm.SVC(probability=True),\n              svm.NuSVC(probability=True),\n              svm.LinearSVC(),\n    \n              # Trees    \n              tree.DecisionTreeClassifier(),\n              tree.ExtraTreeClassifier(),\n    \n              # Discriminant Analysis\n              discriminant_analysis.LinearDiscriminantAnalysis(),\n              discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n              # XGBoost\n              XGBClassifier() }","75e0f0ad":"# split dataset in cross-validation\ncv_split = model_selection.ShuffleSplit(n_splits=10, test_size=.3, \n                                        train_size=.6, random_state=0)","99dfe49d":"# create table to compare algorithms\nalgorithm_results = pd.DataFrame(columns=['Name', 'Parameters', \n                                          'Train Accuracy', 'Test Accuracy', 'STD'])","183bd0de":"# index through algorithms and save performance to table\nindex = 0\n\npredictions = pd.DataFrame()\npredictions['Target'] = Y_train\n\nfor alg in algorithms:\n\n    # set name and parameters\n    name = alg.__class__.__name__\n    algorithm_results.loc[index, 'Name'] = name\n    algorithm_results.loc[index, 'Parameters'] = str(alg.get_params())\n\n    # score model with cross validation\n    cv_results = model_selection.cross_validate(alg, X_train, Y_train, cv=cv_split)\n\n    algorithm_results.loc[index, 'Train Accuracy'] = cv_results['train_score'].mean()\n    algorithm_results.loc[index, 'Test Accuracy'] = cv_results['test_score'].mean()   \n\n    algorithm_results.loc[index, 'STD'] = cv_results['test_score'].std()\n\n    alg.fit(X_train, Y_train)\n    \n    # algoritm prediction\n    predictions[name] = alg.predict(X_train)\n    \n    index+=1\n\n    \n# sort table\nalgorithm_results.sort_values(by=['Test Accuracy'], ascending=False, inplace=True)","b6342117":"# print algorithms results table\nalgorithm_results","cbfefd8e":"# explore algorithms accuracy score\nfig, ax = plt.subplots(figsize=(14, 8))\n\nplot = sns.barplot(x='Test Accuracy', y='Name', data=algorithm_results, palette='BuPu', \n                   orient='h', **{'xerr': algorithm_results['STD']})\n\nplt.title('Algorithms Accuracy Score')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","2b65794e":"# correlation matrix between predictions and Survived\nfig, ax = plt.subplots(figsize=(16, 15))\nplot = sns.heatmap(predictions.corr(), annot=True, cmap='BuPu', ax=ax)","735e0f20":"# AdaBoostClassifier grid search\n#ada = ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(), random_state=0)\n\n#ada_params = {'base_estimator__criterion': ['gini', 'entropy'],\n#              'base_estimator__splitter':   ['best', 'random'],\n#              'n_estimators': [1, 2, 5, 10, 20, 50, 100, 300],\n#              'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.25, 0.5],\n#              'algorithm': ['SAMME', 'SAMME.R']}\n\n#ada_grid = model_selection.GridSearchCV(ada, param_grid=ada_params, cv=cv_split,\n#                                        scoring='accuracy', n_jobs=8, verbose=1)\n#ada_grid.fit(X_train, Y_train)\n\n#ada_best = ada_grid.best_estimator_\n\n#print(f'The best parameter for AdaBoostClassifier is {ada_grid.best_params_}')\n#print(f'The best score for AdaBoostClassifier is {ada_grid.best_score_}')","8c1788ac":"# AdaBoostClassifier\nada_best = ensemble.AdaBoostClassifier(\n    tree.DecisionTreeClassifier(criterion='entropy', splitter='best'), \n    algorithm='SAMME.R', learning_rate=0.1, n_estimators=100, random_state=0)\n\nada_best.fit(X_train, Y_train)\nscore = model_selection.cross_validate(ada_best, X_train, Y_train, cv=cv_split)['test_score'].mean()\n\nprint(f'The best score for AdaBoostClassifier is {score}')","1ad45357":"# BaggingClassifier grid search\n#bc = ensemble.BaggingClassifier(random_state=0)\n\n#bc_params = {'max_samples': [0.1, 0.25, 0.5, 0.75, 1.0],\n#             'max_features': [0.1, 0.25, 0.5, 0.75, 1.0],\n#             'n_estimators': [1, 2, 5, 10, 20, 50, 100, 300]}\n\n#bc_grid = model_selection.GridSearchCV(bc, param_grid=bc_params, cv=cv_split, \n#                                       scoring='accuracy', n_jobs=8, verbose=1)\n#bc_grid.fit(X_train, Y_train)\n\n#bc_best = bc_grid.best_estimator_\n\n#print(f'The best parameter for BaggingClassifier is {bc_grid.best_params_}')\n#print(f'The best score for BaggingClassifier is {bc_grid.best_score_}')","ebc64429":"# BaggingClassifier\nbc_best = ensemble.BaggingClassifier(random_state=0, max_samples=0.5, \n                                     max_features=1.0, n_estimators=50)\n\nbc_best.fit(X_train, Y_train)\nscore = model_selection.cross_validate(bc_best, X_train, Y_train, cv=cv_split)['test_score'].mean()\n\nprint(f'The best score for BaggingClassifier is {score}')","dbe3a9e8":"# ExtraTreesClassifier grid search\n#etc = ensemble.ExtraTreesClassifier(random_state=0)\n\n#etc_params = {'max_depth': [None, 2, 4, 6, 8, 10],\n#              'max_features': [0.1, 0.25, 0.5, 0.75, 1.0],\n#              'n_estimators': [1, 2, 5, 10, 20, 50, 100, 300],\n#              'criterion': ['gini', 'entropy']}\n\n#etc_grid = model_selection.GridSearchCV(etc, param_grid=etc_params, cv=cv_split, \n#                                        scoring='accuracy', n_jobs=8, verbose=1)\n#etc_grid.fit(X_train, Y_train)\n\n#etc_best = etc_grid.best_estimator_\n\n#print(f'The best parameter for ExtraTreesClassifier is {etc_grid.best_params_}')\n#print(f'The best score for ExtraTreesClassifier is {etc_grid.best_score_}')","5e0e1f56":"# ExtraTreesClassifier\netc_best = ensemble.ExtraTreesClassifier(random_state=0, max_depth=6, max_features=0.25,\n                                         n_estimators=50, criterion='entropy')\n\netc_best.fit(X_train, Y_train)\nscore = model_selection.cross_validate(etc_best, X_train, Y_train, cv=cv_split)['test_score'].mean()\n\nprint(f'The best score for ExtraTreesClassifier is {score}')","5f97701a":"# GradientBoostingClassifier grid search\n#gbc = ensemble.GradientBoostingClassifier(random_state=0)\n\n#gbc_params = {'max_depth': [None, 2, 4, 6, 8, 10],\n#              'max_features': [0.1, 0.25, 0.5, 0.75, 1.0],\n#              'learning_rate': [0.01, 0.025, 0.05, 0.1],\n#              'n_estimators': [1, 2, 5, 10, 20, 50, 100, 300]}\n\n#gbc_grid = model_selection.GridSearchCV(gbc, param_grid=gbc_params, cv=cv_split, \n#                                        scoring='accuracy', n_jobs=8, verbose=1)\n#gbc_grid.fit(X_train, Y_train)\n\n#gbc_best = gbc_grid.best_estimator_\n\n#print(f'The best parameter for GradientBoostingClassifier is {gbc_grid.best_params_}')\n#print(f'The best score for GradientBoostingClassifier is {gbc_grid.best_score_}')","1096fd9a":"# GradientBoostingClassifier\ngbc_best = ensemble.GradientBoostingClassifier(random_state=0, max_depth=4, max_features=0.75,\n                                               n_estimators=100, learning_rate=0.05)\n\ngbc_best.fit(X_train, Y_train)\nscore = model_selection.cross_validate(gbc_best, X_train, Y_train, cv=cv_split)['test_score'].mean()\n\nprint(f'The best score for GradientBoostingClassifier is {score}')","68ea077d":"# RandomForestClassifier grid search\n#rfc = ensemble.RandomForestClassifier(random_state=0)\n\n#rfc_params = {'max_depth': [None, 2, 4, 6, 8, 10],\n#              'max_features': [0.1, 0.5, 1.0],\n#              'min_samples_split': [2, 3, 10],\n#              'min_samples_leaf': [1, 3, 10],\n#              'criterion': ['gini', 'entropy'],\n#              'n_estimators': [1, 2, 5, 10, 20, 50, 100, 300]}\n\n#rfc_grid = model_selection.GridSearchCV(rfc, param_grid=rfc_params, cv=cv_split, \n#                                        scoring='accuracy', n_jobs=8, verbose=1)\n#rfc_grid.fit(X_train, Y_train)\n\n#rfc_best = rfc_grid.best_estimator_\n\n#print(f'The best parameter for RandomForestClassifier is {rfc_grid.best_params_}')\n#print(f'The best score for RandomForestClassifier is {rfc_grid.best_score_}')","443dc984":"# RandomForestClassifier\nrfc_best = ensemble.RandomForestClassifier(random_state=0, max_depth=8, max_features=1.0,\n                                           min_samples_split=10, min_samples_leaf=1,\n                                           criterion='entropy', n_estimators=10)\n\nrfc_best.fit(X_train, Y_train)\nscore = model_selection.cross_validate(rfc_best, X_train, Y_train, cv=cv_split)['test_score'].mean()\n\nprint(f'The best score for RandomForestClassifier is {score}')","b5550a96":"# GaussianProcessClassifier grid search\n#gpc = gaussian_process.GaussianProcessClassifier(random_state=0)\n    \n#gpc_params = {'max_iter_predict': [1, 2, 5, 10, 20, 50, 100, 300],\n#              'n_restarts_optimizer': [1, 2, 3, 4, 5, 10]}\n\n#gpc_grid = model_selection.GridSearchCV(gpc, param_grid=gpc_params, cv=cv_split, \n#                                        scoring='accuracy', n_jobs=8, verbose=1)\n#gpc_grid.fit(X_train, Y_train)\n\n#gpc_best = gpc_grid.best_estimator_\n\n#print(f'The best parameter for GaussianProcessClassifier is {gpc_grid.best_params_}')\n#print(f'The best score for GaussianProcessClassifier is {gpc_grid.best_score_}')","39969464":"# GaussianProcessClassifier\ngpc_best = gaussian_process.GaussianProcessClassifier(random_state=0, max_iter_predict=5,\n                                                      n_restarts_optimizer=1)\n\ngpc_best.fit(X_train, Y_train)\nscore = model_selection.cross_validate(gpc_best, X_train, Y_train, cv=cv_split)['test_score'].mean()\n\nprint(f'The best score for GaussianProcessClassifier is {score}')","1713e22f":"# LogisticRegressionCV grid search\n#lr = linear_model.LogisticRegressionCV(random_state=0)\n    \n#lr_params = {'fit_intercept': [True, False],\n#             'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n\n#lr_grid = model_selection.GridSearchCV(lr, param_grid=lr_params, cv=cv_split,\n#                                       scoring='accuracy', n_jobs=8, verbose=1)\n#lr_grid.fit(X_train, Y_train)\n\n#lr_best = lr_grid.best_estimator_\n\n#print(f'The best parameter for LogisticRegressionCV is {lr_grid.best_params_}')\n#print(f'The best score for LogisticRegressionCV is {lr_grid.best_score_}')","8815a025":"# LogisticRegressionCV\nlr_best = linear_model.LogisticRegressionCV(random_state=0, fit_intercept=True,\n                                            solver='liblinear')\n\nlr_best.fit(X_train, Y_train)\nscore = model_selection.cross_validate(lr_best, X_train, Y_train, cv=cv_split)['test_score'].mean()\n\nprint(f'The best score for LogisticRegressionCV is {score}')","61c0691c":"# BernoulliNB grid search\n#bnb = naive_bayes.BernoulliNB()\n    \n#bnb_params = {'alpha': [0.1, 0.25, 0.5, 0.75, 1.0]}\n\n#bnb_grid = model_selection.GridSearchCV(bnb, param_grid=bnb_params, cv=cv_split,\n#                                        scoring='accuracy', n_jobs=8, verbose=1)\n#bnb_grid.fit(X_train, Y_train)\n\n#bnb_best = bnb_grid.best_estimator_\n\n#print(f'The best parameter for BernoulliNB is {bnb_grid.best_params_}')\n#print(f'The best score for BernoulliNB is {bnb_grid.best_score_}')","b728057a":"# BernoulliNB\nbnb_best = naive_bayes.BernoulliNB(alpha=0.25)\n\nbnb_best.fit(X_train, Y_train)\nscore = model_selection.cross_validate(bnb_best, X_train, Y_train, cv=cv_split)['test_score'].mean()\n\nprint(f'The best score for BernoulliNB is {score}')","1863b86a":"# KNeighborsClassifier grid search\n#knn = neighbors.KNeighborsClassifier()\n    \n#knn_params = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n#              'weights': ['uniform', 'distance'],\n#              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\n#knn_grid = model_selection.GridSearchCV(knn, param_grid=knn_params, cv=cv_split,\n#                                        scoring='accuracy', n_jobs=8, verbose=1)\n#knn_grid.fit(X_train, Y_train)\n\n#knn_best = knn_grid.best_estimator_\n\n#print(f'The best parameter for KNeighborsClassifier is {knn_grid.best_params_}')\n#print(f'The best score for KNeighborsClassifier is {knn_grid.best_score_}')","7646066f":"# KNeighborsClassifier\nknn_best = neighbors.KNeighborsClassifier(n_neighbors=7, weights='uniform', algorithm='brute')\n\nknn_best.fit(X_train, Y_train)\nscore = model_selection.cross_validate(knn_best, X_train, Y_train, cv=cv_split)['test_score'].mean()\n\nprint(f'The best score for KNeighborsClassifier is {score}')","35faa221":"# SVC grid search\n#svc = svm.SVC(probability=True, random_state=0)\n    \n#svc_params = {'C': [1, 2, 3, 4, 5],\n#              'gamma': [0.1, 0.25, 0.5, 0.75, 1.0],\n#              'decision_function_shape': ['ovo', 'ovr'],\n#              'probability': [True]}\n\n#svc_grid = model_selection.GridSearchCV(svc, param_grid=svc_params, cv=cv_split, \n#                                        scoring='accuracy', n_jobs=8, verbose=1)\n#svc_grid.fit(X_train, Y_train)\n\n#svc_best = svc_grid.best_estimator_\n\n#print(f'The best parameter for SVC is {svc_grid.best_params_}')\n#print(f'The best score for SVC is {svc_grid.best_score_}')","8432cce4":"# SVC\nsvc_best = svm.SVC(random_state=0, C=1, gamma=0.1, \n                   decision_function_shape='ovo', probability=True)\n\nsvc_best.fit(X_train, Y_train)\nscore = model_selection.cross_validate(svc_best, X_train, Y_train, cv=cv_split)['test_score'].mean()\n\nprint(f'The best score for SVC is {score}')","238ead7b":"# XGBClassifier grid search\n#xgb = XGBClassifier(random_state=0)\n    \n#xgb_params = {'learning_rate': [0.01, 0.025, 0.05, 0.1],\n#              'max_depth':  [1, 2, 3, 4, 5, 6, 8, 10],\n#              'n_estimators': [1, 2, 5, 10, 20, 50, 100, 300]}\n\n#xgb_grid = model_selection.GridSearchCV(xgb, param_grid=xgb_params, cv=cv_split,\n#                                        scoring='accuracy', n_jobs=8, verbose=1)\n#xgb_grid.fit(X_train, Y_train)\n\n#xgb_best = xgb_grid.best_estimator_\n\n#print(f'The best parameter for XGBClassifier is {xgb_grid.best_params_}')\n#print(f'The best score for XGBClassifier is {xgb_grid.best_score_}')","6e9d5c43":"# XGBClassifier\nxgb_best = XGBClassifier(random_state=0, learning_rate=0.025, max_depth=5, n_estimators=300)\n\nxgb_best.fit(X_train, Y_train)\nscore = model_selection.cross_validate(xgb_best, X_train, Y_train, cv=cv_split)['test_score'].mean()\n\nprint(f'The best score for XGBClassifier is {score}')","bd8a4c2f":"# combine our models\nvote_estimators = {# Ensemble Methods\n                   ('ada', ada_best),\n                   ('bc', bc_best),\n                   ('etc', etc_best),\n                   ('gbc', gbc_best),\n                   ('rfc', rfc_best),\n\n                   # Gaussian Processes\n                   ('gpc', gbc_best),\n    \n                   # Generalized Linear Models\n                   ('lr', lr_best),\n    \n                   # Navies Bayes\n                   ('bnb', bnb_best),\n    \n                   # Nearest Neighbor\n                   ('knn', knn_best),\n    \n                   # Support Vector Machine\n                   ('svc', svc_best),\n    \n                   # XGBoost\n                   ('xgb', xgb_best) }","f6ec6b97":"# fit models with hard voting\nhard_estimators = ensemble.VotingClassifier(estimators=vote_estimators, voting='hard')\nhard_estimators.fit(X_train, Y_train)\n\nscore = model_selection.cross_validate(hard_estimators, X_train, Y_train, cv=cv_split)['test_score'].mean()\n\nprint(f'The best score for Hard Estimators is {score}')","28dae2b7":"# fit models with soft voting\nsoft_estimators = ensemble.VotingClassifier(estimators=vote_estimators, voting='soft')\nsoft_estimators.fit(X_train, Y_train)\n\nscore = model_selection.cross_validate(soft_estimators, X_train, Y_train, cv=cv_split)['test_score'].mean()\n\nprint(f'The best score for Soft Estimators is {score}')","5fe0fc32":"# predict and save results\ntest_survived = pd.Series(hard_estimators.predict(test), name='Survived')\nresults = pd.concat([test_id, test_survived], axis=1)\n\nresults.to_csv('titanic_result.csv', index=False)","cc231d21":"**Name features**","429f5841":"**Sex feature**","fbb3cd21":"# Define the Problem","0878dc98":"**Tune hyperparamaters**","79f75eef":"**Cabin feature**","8499b108":"We can see that passengers with a Cabin have generally more chance to survive than passengers without Cabin (with 'X' value)","44c721b5":"**Embarked feature**","6d3be822":"**Prepare data**","e5c121bd":"Fare distribution is very skewed. Transform feature with the log function","ef30a35b":"Female passengers have more chance to survive","7e0ebab7":"# Titanic: Good Way To Get Started","28ea7ef5":"To understand how Age feature is distributed let's build distribution graphs","6bebc1e4":"Passenger coming from Southampton (S) have less chance to survive. However,  in Southampton (S) more male passangers and third-class passengers. This feature does not have a significant effect on survival.","ee815cf5":"The first letter of the Cabin may indicates the probable location of the passenger in the Titanic","f9758060":"# Feature analysis and engineering","31633f36":"**Age feature**","a14fbb09":"The division into groups more clearly show that very young passengers have more chance to survive and old passangers have less chance to survive.","5a9a926e":"**Simple modeling**","ec35bee5":"Port of Embarkation most of passengers is Southampton (S). Fill null values with the most frequent value","127acb73":"**Fare feature**","0375c560":"**Prediction**","7fc2685b":"Take some of the best algorithms and tune hyperparamaters","bd18f249":"Create new feature Fsize","eeb6ad6e":"# Introduction","ff8b05c6":"Only Fare feature have a high correlation with the Survived feature.\n\nIt doesn't mean that the other features are not usefull! We need to explore in detail other features","4379296a":"**Short review**","dd7a5270":"After applying the log function, the skew has decreased","3c3e3a82":"We can see that age distribution is very close to a gaussian distribution\n\nAge distributions are not the same in the survived and not survived subpopulations.\n\nIt seems that very young passengers have more chance to survive and old passangers have less chance to survive.\n\nWe can divide Age feature into several groups, but first need to fill null values.","4268c979":"Similarly with SibSp, passengers with a lot of parents\/children have less chance to survive","13d5ed6d":"**Family Size**","a4b1957a":"Age feature is categorical variable. Convert to indicator variables","c14ee93d":"Passenger coming from Cherbourg (C) have more chance to survive.\n\nMaybe Sex and Pclass among Embarked have an uneven distribution. Need to check this","08ebe1a9":"Passengers from first class have more chance to survive then passengers from second class. Passengers from second class have more chance to survive then passengers from third class. \n\nThis feature does not depend of Sex distribution","dc96f186":"Passengers with a lot of siblings\/spouses  have less chance to survive","758e0c9c":"This feature has 1014 null values.  Need to fill them","f454588a":"We can see that some features are null. Will need to fill them later","9671f8a8":"Divide Age feature into five groups","17ca5ff2":"**29.10.2018**","d0fc7ca6":"**Load data**","8f1fe04f":"# Loading Data","b7463508":"**Ticket feature**","90940d9a":"**SibSp feature**","bd650f3d":"Passenger with rare title have more chance to survive.","f0a9679c":"Fsize feature is categorical variable. Convert to indicator variables","9faf3777":"Ticket prefix can mean to the actual placement of the cabins within the Titanic. This may affect at survival","0f5145d9":"* survival: Survival (0 = No, 1 = Yes)\n* pclass: Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd)\n* sex:\tSex\n* age:\tAge in years\t\n* sibsp:\tOf siblings\/spouses aboard the Titanic\t\n* parch:\tOf parents\/children aboard the Titanic\t\n* ticket:\tTicket number\t\n* fare:\tPassenger fare\t\n* cabin:\tCabin number\t\n* embarked:\tPort of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)","9198b614":"**Parch feature**","97d96def":"Feature analysis and engineering completed","1537bedd":"Fill null values with the median","d8de3c99":"Let's see which feature most correlate with Survuved feature. For this we can build a correlation matrix","7c65a562":"# Modeling","f06bc813":"**Data dictionary**","7ec42106":"The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.","27cb859d":"We\u2019re looking for their families, sisters\/brothers\/parents during the evacuation. This may affect at survival","1bb6f9f0":"**Combining models**","a31a292b":"**Import Libraries**","067787c1":"Survival rate really depends on family size. Divide Fsize feature into four groups","0abbf6a9":"The grid search takes a lot of time, so I use the best parameters which I found","65893a9b":"Small and medium families have more chance to survive","bd28ca17":"**Pclass feature**","400285e3":"The Name feature contains information on passenger's title. This may affect at survival","dad8068b":"Hello kaggle! It's my first kernel.  I chose this competition as a good way to get started. \n\nIn this competition, I performed feature analysis and engineering, and also built a ensemble model. Let's start!\n\nAnd sorry for my bad english :)","5c8b4258":"There is 17 titles in the dataset, we can group them in 4 categories."}}