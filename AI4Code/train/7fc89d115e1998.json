{"cell_type":{"f7215b5e":"code","0581dc99":"code","0fcca16d":"code","11b8cbce":"code","064da5cb":"code","45228622":"code","3da3eb2a":"code","44920459":"code","225d95e8":"code","cf51373e":"code","2568d1fd":"code","06d00c57":"code","397a4a01":"code","7c1930d2":"code","36c725a9":"code","0f48983a":"code","de1ef318":"code","d7838700":"code","2b51f74f":"code","8903e30c":"code","071a4020":"code","fa165c60":"code","97f259da":"code","aedf32a9":"code","6132d4d4":"code","5014b3c5":"code","19c6cbaf":"code","82969e2f":"code","f463d0fa":"code","5ef26384":"code","2b319b33":"code","96a536bf":"code","52eac861":"code","eeace9fc":"code","c8f58ab9":"code","899ef887":"code","36d09abf":"code","3fc1091a":"code","4d9efb65":"code","82f373db":"code","8f54bb58":"code","98267fb9":"code","44afea0d":"code","07e8e7fd":"code","4f35a9dc":"code","c6145480":"code","2c4f7089":"code","7924e65b":"code","925a9321":"code","5f5e476e":"code","cebcb960":"code","52435826":"code","f29d15a7":"code","f04f517d":"code","783c609b":"code","e815556d":"markdown","9adbbe95":"markdown","05d9fb9b":"markdown","5a2d4e35":"markdown","7e9746ca":"markdown","18a44536":"markdown","d4d0779b":"markdown","323a3212":"markdown","9e723c71":"markdown","91289e07":"markdown","a91ba5d3":"markdown","681456d0":"markdown","cad7dddf":"markdown","27686255":"markdown","201eb5e3":"markdown","fcbe50d8":"markdown","7015be9a":"markdown","34e920f2":"markdown","24e43387":"markdown","02c570e0":"markdown","1073c32a":"markdown","aa234cff":"markdown","ddb49257":"markdown","6c363f17":"markdown","02c90b62":"markdown","f32ba155":"markdown","c831a0e4":"markdown","4ae714c6":"markdown"},"source":{"f7215b5e":"# matplotlib\u3067\u65e5\u672c\u8a9e\u3092\u6271\u3048\u308b\u3088\u3046\u306b\n!pip install japanize_matplotlib -Uq\n\n# RainCloud Plot(\u6563\u5e03\u56f3\uff0bBoxPlot\uff0bViolin\u3092\u4e00\u3064\u3067\u8868\u793a)\n!pip install ptitprince -Uq\n\n# \u6b20\u640d\u5024\u3092\u53ef\u8996\u5316\n!pip install missingno -Uq\n\n# \u30d9\u30f3\u56f3\u3092\u4f5c\u6210\n!pip install matplotlib-venn -Uq\n\n!pip install faiss-cpu -Uq","0581dc99":"import warnings\nwarnings.simplefilter('ignore')\n\nimport os\nimport gc\ngc.enable()\nimport sys\nimport glob\nimport math\nimport time\nimport random\nimport string\nimport psutil\nimport pathlib\nfrom pathlib import Path\nfrom contextlib import contextmanager\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport japanize_matplotlib\nfrom ptitprince import RainCloud\nfrom matplotlib_venn import venn2\n\n\nfrom tqdm.auto import tqdm as tqdmp\nfrom tqdm.autonotebook import tqdm as tqdm\ntqdmp.pandas()\n\n## Model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\nimport lightgbm as lgb\nimport catboost as cat\nimport faiss","0fcca16d":"# \u5b9f\u9a13\u3067\u4f7f\u3046\u30d1\u30e9\u30e1\u30fc\u30bf\u306fConfig\u3067\u7ba1\u7406\u3057\u3066\u3044\u307e\u3059\u3002\n# \u3053\u306e\u5b9f\u9a13\u4f55\u3084\u3063\u305f\u304b\u306a\u3068\u5f8c\u3067\u632f\u308a\u8fd4\u308a\u3084\u3059\u3044\u3088\u3046\u306b\u3001\u306a\u308b\u3079\u304fConfig\u3060\u3051\u898b\u308c\u3070\u308f\u304b\u308b\u3088\u3046\u306b\u3057\u3066\u3044\u307e\u3059\n\nclass CFG:\n    \n    def __init__(self):\n        \n        self.debug=True\n        self.seed=42\n        self.n_fold = 5\n        self.n_seeds = 20\n        self.environment='Kaggle'  # 'AWS' or 'Kaggle' or 'Colab'\n        self.project='Shiggle_1st',\n        self.exp_name = '007_Final_Submission'\n        self.objective = 'rmse'\n        self.metric = 'rmse'\n        self.learning_rate = 0.1\n        self.num_boost_round = 1000\n        self.early_stopping_rounds = 30\n        self.num_leaves = 16\n        \nCONFIG = CFG()","11b8cbce":"## \u518d\u73fe\u6027\u78ba\u4fdd\u306e\u305f\u3081\u306eSeed\u56fa\u5b9a\ndef seed_everything(seed:int==42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nseed_everything(CONFIG.seed)","064da5cb":"## \u51e6\u7406\u306b\u304b\u304b\u3063\u305f\u6642\u9593\u3068\u4f7f\u7528\u3057\u305f\u30e1\u30e2\u30ea\u3092\u8a08\u6e2c\n@contextmanager\ndef timer(name:str, slack:bool=False):\n    t0 = time.time()\n    p = psutil.Process(os.getpid())\n    m0 = p.memory_info()[0] \/ 2. ** 30\n    print(f'<< {name} >> Start')\n    yield\n    \n    m1 = p.memory_info()[0] \/ 2. ** 30\n    delta = m1 - m0\n    sign = '+' if delta >= 0 else '-'\n    delta = math.fabs(delta)\n    \n    print(f\"<< {name} >> {m1:.1f}GB({sign}{delta:.1f}GB):{time.time() - t0:.1f}sec\", file=sys.stderr)","45228622":"# \u500b\u4eba\u7684\u306bAWS\u3084Kaggle\u74b0\u5883\u3084Google Colab\u3092\u884c\u3063\u305f\u308a\u6765\u305f\u308a\u3057\u3066\u3044\u308b\u306e\u3067\u307e\u3068\u3081\u3066\u3044\u307e\u3059\nif CONFIG.environment == 'AWS':\n    INPUT_DIR = Path('\/mnt\/work\/data\/kaggle\/shiggle_1st\/')\n    MODEL_DIR = Path(f'..\/models\/{CONFIG.exp_name}\/')\n    OUTPUT_DIR = Path(f'..\/data\/interim\/{CONFIG.exp_name}\/')\n    \n    os.makedirs(MODEL_DIR, exist_ok=True)\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    \n    print(f\"Your environment is 'AWS'.\\nINPUT_DIR is {INPUT_DIR}\\nMODEL_DIR is {MODEL_DIR}\\nOUTPUT_DIR is {OUTPUT_DIR}\")\n    \n    \nelif CONFIG.environment == 'Kaggle':\n    INPUT_DIR = Path('..\/input\/shigglecup-1st\/DATA\/')\n    MODEL_DIR = Path('.\/')\n    OUTPUT_DIR = Path('.\/')\n    print(f\"Your environment is 'Kaggle'.\\nINPUT_DIR is {INPUT_DIR}\\nMODEL_DIR is {MODEL_DIR}\\nOUTPUT_DIR is {OUTPUT_DIR}\")\n\n    \nelif CONFIG.environment == 'Colab':\n    INPUT_DIR = Path('\/content\/drive\/MyDrive\/kaggle\/Shiggle_1st\/data\/raw')\n    BASE_DIR = Path(\"\/content\/drive\/MyDrive\/kaggle\/Shiggle_1st\/data\/\")\n\n    MODEL_DIR = BASE_DIR \/ f'{CONFIG.exp_name}'\n    OUTPUT_DIR = BASE_DIR \/ f'{CONFIG.exp_name}\/'\n\n    os.makedirs(MODEL_DIR, exist_ok=True)\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    \n    if not os.path.exists(INPUT_DIR):\n        print('Please Mount your Google Drive.')\n    else:\n        print(f\"Your environment is 'Colab'.\\nINPUT_DIR is {INPUT_DIR}\\nMODEL_DIR is {MODEL_DIR}\\nOUTPUT_DIR is {OUTPUT_DIR}\")\n        \nelse:\n    print(\"Please choose 'AWS' or 'Kaggle' or 'Colab'.\\nINPUT_DIR is not found.\")","3da3eb2a":"with timer('Data Load'):\n    train_df = pd.read_csv(INPUT_DIR \/ 'train.csv')\n    test_df = pd.read_csv(INPUT_DIR \/ 'test.csv')\n    sub_df = pd.read_csv(INPUT_DIR \/ 'sample_submission.csv')\n    \n    print(f'Train: {train_df.shape} | Test: {test_df.shape}')","44920459":"## Train\u3068Test\u3092\u7d50\u5408\u3057\u307e\u3059\n## \u3042\u3068\u3067\u5206\u3051\u3089\u308c\u308b\u3088\u3046\u306bflag\u3092\u4ed8\u4e0e\u3057\u307e\u3059\n\ntrain_df['flag'] = 0\ntest_df['flag'] = 1\n\nwith timer('train test concat'):\n    whole_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)","225d95e8":"with timer('log transform w\/ target'):\n    whole_df['target'] = whole_df['target'].progress_apply(lambda x: np.log1p(x))\n\n    plt.figure(figsize=(12,5), tight_layout=True)\n    sns.histplot(whole_df['target'], bins=30)\n    plt.show()","cf51373e":"with timer('log transform w\/ height & weight'):\n    \n    def calc_bmi(row):\n        return row['weight']\/(row['height']**2)\n    \n    whole_df['BMI'] = whole_df.progress_apply(calc_bmi, axis=1)\n    whole_df['height'] = whole_df['height'].progress_apply(lambda x: np.log1p(x))\n    whole_df['weight'] = whole_df['weight'].progress_apply(lambda x: np.log1p(x))\n    ","2568d1fd":"plt.figure(figsize=(8,5), tight_layout=True)\nplt.scatter(\n    x=whole_df['height'],\n    y=whole_df['weight'],\n    c=whole_df['BMI'],\n    cmap='jet'\n)\nplt.xlabel('height')\nplt.ylabel('weight')\nplt.colorbar()\nplt.show()","06d00c57":"def count_type(df:pd.DataFrame) -> pd.DataFrame:\n    \n    _df = df.copy()\n    # type_2\u304cnan\u3067\u3042\u308c\u3070\u3001\u30bf\u30a4\u30d7\u6570\u306f1\u3068\u3059\u308b\n    _df['num_type'] = _df['type_2'].progress_apply(lambda x: 1 if x is np.nan else 2)\n\n    return _df","397a4a01":"with timer('\u30bf\u30a4\u30d7\u6570\u3092\u30ab\u30a6\u30f3\u30c8'):\n    whole_df = count_type(whole_df)","7c1930d2":"## type_1\u3068type_2\u306e\u72ec\u81ea\u306e\u3082\u306e\u304c\u306a\u3044\u304b\u3092\u30d9\u30f3\u56f3\u3067\u78ba\u8a8d\u3057\u307e\u3059\nplt.figure(figsize=(8,6))\nvenn2(subsets=[set(train_df['type_1'].unique().tolist()),\n               set(train_df['type_2'].unique().tolist())],\n      set_labels=('A:type_1','B:type_2'))\nplt.title('type_1\u3068type_2\u306e\u30e6\u30cb\u30fc\u30af\u6570',fontsize=20)","36c725a9":"## Label Encoding\n## Label Encoding\u306fpandas\u306efactorize\u3092\u4f7f\u3046\u3068\u4fbf\u5229\u3067\u3059\nwith timer('label encoding w\/ type'):\n    labels, uniques = pd.concat([whole_df['type_1'], whole_df['type_2']], axis=0).factorize()\n    whole_df['type_1_le'] = labels[:len(whole_df)]\n    whole_df['type_2_le'] = labels[len(whole_df):]\n    display(whole_df.head())","0f48983a":"## CountRank Encoding\nwith timer('CountRank Enconding w\/ type'):\n    \n    count_rank = whole_df.groupby('type_1')['target'].count().rank(ascending=False)\n    whole_df['type_1_cre'] = whole_df['type_1'].map(count_rank)\n    whole_df['type_2_cre'] = whole_df['type_2'].map(count_rank)","de1ef318":"with timer('\u7a2e\u65cf\u5024\u8a08\u7b97'):\n    whole_df['base_stats'] = whole_df[['hp', 'attack', 'defense', 'special_attack', 'special_defense', 'speed']].sum(axis=1)\n    display(whole_df.head())","d7838700":"with timer('\u5404\u80fd\u529b\u306e\u5272\u5408'):\n    for col in ['hp', 'attack', 'defense', 'special_attack', 'special_defense', 'speed']:\n        whole_df[f'{col}_ratio'] = whole_df[col]\/whole_df['base_stats']","2b51f74f":"class FaissKNeighbors:\n    def __init__(self, k=5):\n        self.index = None\n        self.d = None\n        self.k = k\n\n    def fit(self, X):\n        X = X.copy(order='C')\n        self.d = X.shape[1]\n        self.index = faiss.IndexFlatL2(self.d)\n        self.index.add(X.astype(np.float32))\n\n    def predict(self, X):\n        X = X.copy(order='C')\n        X = np.reshape(X, (-1, self.d))\n        distances, indices = self.index.search(X.astype(np.float32), k=self.k)\n        return indices[0]","8903e30c":"spec_col = ['attack', 'defense', 'hp', 'special_attack', 'special_defense', 'speed']\nrate_col = spec_col+['base_stats']\n\n# \u985e\u4f3c\u63a2\u7d22\nkn = FaissKNeighbors(k=10)\nkn.fit(whole_df[whole_df['flag'] == 0][rate_col].values)","071a4020":"for num in range(20):\n\n    idx = kn.predict(whole_df.loc[num, rate_col].values)\n    print(f\"{num:02d} | \u30b9\u30da\u30c3\u30af\u985e\u4f3c\u30dd\u30b1\u30e2\u30f3\u306etarget\u5e73\u5747\u5024\uff1a{np.median(whole_df.loc[idx[1:], 'target'].values)}\")\n    print(f\"{num:02d} | \u5bfe\u8c61\u30dd\u30b1\u30e2\u30f3\u306e\u5e73\u5747\u5024: {whole_df.loc[num, 'target']}\")","fa165c60":"median_list = []\nmean_list = []\nmax_list = []\nmin_list = []\n\nfor num in tqdm(range(whole_df.shape[0])):\n\n    idx = kn.predict(whole_df.loc[num, rate_col].values)\n    target_array = whole_df.loc[idx[1:], 'target'].values\n    median_list.append(np.median(target_array))\n    mean_list.append(np.median(target_array))\n    max_list.append(np.median(target_array))\n    min_list.append(np.median(target_array))","97f259da":"whole_df['similar_median'] = median_list\nwhole_df['similar_mean'] = mean_list\nwhole_df['similar_max'] = max_list\nwhole_df['similar_min'] = min_list","aedf32a9":"def count_ability(df:pd.DataFrame) -> pd.DataFrame:\n    \n    _df = df.copy()\n    # type_2\u304cnan\u3067\u3042\u308c\u3070\u3001\u30bf\u30a4\u30d7\u6570\u306f1\u3068\u3059\u308b\n    _df['num_ability'] = _df['ability_2'].progress_apply(lambda x: 1 if x is np.nan else 2)\n\n    return _df","6132d4d4":"with timer('ability\u6570\u3092\u30ab\u30a6\u30f3\u30c8'):\n    whole_df = count_ability(whole_df)","5014b3c5":"## Label Encoding\nwith timer('label encoding w\/ ability'):\n    labels, uniques = pd.concat([whole_df['ability_1'], whole_df['ability_2']], axis=0).factorize()\n    whole_df['ability_1'] = labels[:len(whole_df)]\n    whole_df['ability_2'] = labels[len(whole_df):]\n    display(whole_df.head())","19c6cbaf":"## Label Encoding\nwith timer('label encoding w\/ ability_hidden'):\n    labels, uniques = whole_df['ability_hidden'].factorize()\n    whole_df['ability_hidden'] = labels","82969e2f":"with timer('drop color'):\n    whole_df = whole_df.drop(['color_1', 'color_2', 'color_f'], axis=1)","f463d0fa":"def count_egg_group(df:pd.DataFrame) -> pd.DataFrame:\n    \n    _df = df.copy()\n    # type_2\u304cnan\u3067\u3042\u308c\u3070\u3001\u30bf\u30a4\u30d7\u6570\u306f1\u3068\u3059\u308b\n    _df['num_egg_group'] = _df['egg_group_2'].progress_apply(lambda x: 1 if x is np.nan else 2)\n\n    return _df","5ef26384":"with timer('egg_group\u6570\u3092\u30ab\u30a6\u30f3\u30c8'):\n    whole_df = count_egg_group(whole_df)","2b319b33":"## Label Encoding\nwith timer('label encoding w\/ egg_group_2'):\n    labels, uniques = pd.concat([whole_df['egg_group_1'], whole_df['egg_group_2']], axis=0).factorize()\n    whole_df['egg_group_1'] = labels[:len(whole_df)]\n    whole_df['egg_group_2'] = labels[len(whole_df):]\n    display(whole_df.head())","96a536bf":"with timer('drop image'):\n    whole_df = whole_df.drop('url_image', axis=1)","52eac861":"with timer('drop shape'):\n    whole_df = whole_df.drop('shape', axis=1)","eeace9fc":"tmp = whole_df[['target', \"species_id\", \"evolves_from_species_id\", \"base_stats\"]]\ntmp_dict = tmp.dropna(subset=['target', 'species_id'])\nevol_target = dict(zip(tmp_dict['species_id'], tmp_dict['target']))\ntmp['\u9032\u5316\u524d\u7d4c\u9a13\u5024'] = tmp['evolves_from_species_id'].map(evol_target)\n\ntmp_dict = tmp.dropna(subset=['base_stats', 'species_id'])\nevol_target = dict(zip(tmp_dict['species_id'], tmp_dict['base_stats']))\ntmp['\u9032\u5316\u524d\u7a2e\u65cf\u5024'] = tmp['evolves_from_species_id'].map(evol_target)\n\ntmp_dict = tmp.dropna(subset=['target', 'evolves_from_species_id'])\nevol_target = dict(zip(tmp_dict['evolves_from_species_id'], tmp_dict['target']))\ntmp['\u9032\u5316\u5f8c\u7d4c\u9a13\u5024'] = tmp['species_id'].map(evol_target)\n\ntmp_dict = tmp.dropna(subset=['base_stats', 'evolves_from_species_id'])\nevol_target = dict(zip(tmp_dict['evolves_from_species_id'], tmp_dict['base_stats']))\ntmp['\u9032\u5316\u5f8c\u7a2e\u65cf\u5024'] = tmp['species_id'].map(evol_target)\n\ntmp_dict = tmp.dropna(subset=['species_id', 'evolves_from_species_id'])\nevol_target = dict(zip(tmp_dict['species_id'], tmp_dict['evolves_from_species_id']))\ntmp['\u4e8c\u6bb5\u968e\u9032\u5316'] = tmp['evolves_from_species_id'].map(evol_target)\ntmp['\u4e8c\u6bb5\u968e\u9032\u5316'] = tmp['\u4e8c\u6bb5\u968e\u9032\u5316'].apply(lambda x: 1 if x > 1 else 0)\n\ntmp['\u4e8c\u6bb5\u968e\u9032\u5316_\u5e73\u5747'] = (tmp['\u9032\u5316\u524d\u7d4c\u9a13\u5024'] + tmp['\u9032\u5316\u5f8c\u7d4c\u9a13\u5024'])\/2.\nwhole_df = pd.concat([whole_df, tmp[[\"\u9032\u5316\u524d\u7d4c\u9a13\u5024\", \"\u9032\u5316\u5f8c\u7d4c\u9a13\u5024\", \"\u4e8c\u6bb5\u968e\u9032\u5316_\u5e73\u5747\", \"\u4e8c\u6bb5\u968e\u9032\u5316\", \"\u9032\u5316\u524d\u7a2e\u65cf\u5024\", \"\u9032\u5316\u5f8c\u7a2e\u65cf\u5024\"]]], axis=1)\nwhole_df.head().T","c8f58ab9":"tmp = whole_df[['pokemon', 'target']]\ntmp['mega'] = tmp['pokemon'].apply(lambda x:1 if '-mega' in x else np.nan)\ntmp['mega_name'] = tmp['pokemon'].apply(lambda x: x.split('-')[0] if '-mega' in x else np.nan)\ntmp_dict = tmp.dropna(subset=['target'])\nevol_target = dict(zip(tmp_dict['pokemon'], tmp_dict['target']))\ntmp['\u30e1\u30ac\u9032\u5316\u524d\u7d4c\u9a13\u5024'] = tmp['mega_name'].map(evol_target)\n\ntmp_dict = tmp.dropna(subset=['target', 'mega'])\nevol_target = dict(zip(tmp_dict['mega_name'], tmp_dict['target']))\ntmp['\u30e1\u30ac\u9032\u5316\u5f8c\u7d4c\u9a13\u5024'] = tmp['pokemon'].map(evol_target)\n\nwhole_df = pd.concat([whole_df, tmp[['mega', '\u30e1\u30ac\u9032\u5316\u524d\u7d4c\u9a13\u5024', '\u30e1\u30ac\u9032\u5316\u5f8c\u7d4c\u9a13\u5024']]], axis=1)","899ef887":"tmp = whole_df[['target', 'base_stats', 'type_1']]\ntmp_gb = tmp.groupby(['type_1'])['base_stats'].agg(['mean', 'min', 'max']).add_prefix('type_1__Base_stats__').reset_index()\ngb_cols = tmp_gb.columns.tolist()[1:]\n\nwhole_df = pd.merge(whole_df, tmp_gb, on='type_1', how='left')\n\nfor col in gb_cols:\n    whole_df[col] -= whole_df['base_stats']","36d09abf":"tmp = whole_df[['target', 'base_stats', 'egg_group_1']]\ntmp_gb = tmp.groupby(['egg_group_1'])['base_stats'].agg(['mean', 'min', 'max']).add_prefix('egg_group_1__Base_stats__').reset_index()\ngb_cols = tmp_gb.columns.tolist()[1:]\n\nwhole_df = pd.merge(whole_df, tmp_gb, on='egg_group_1', how='left')\n\nfor col in gb_cols:\n    whole_df[col] -= whole_df['base_stats']","3fc1091a":"tmp = whole_df[['target', 'attack', 'type_1']]\ntmp_gb = tmp.groupby(['type_1'])['attack'].agg(['mean', 'min', 'max']).add_prefix('type_1__attack_ratio__').reset_index()\ngb_cols = tmp_gb.columns.tolist()[1:]\n\nwhole_df = pd.merge(whole_df, tmp_gb, on='type_1', how='left')\n\nfor col in gb_cols:\n    whole_df[col] -= whole_df['attack']\n    \ntmp = whole_df[['target', 'hp', 'type_1']]\ntmp_gb = tmp.groupby(['type_1'])['hp'].agg(['mean', 'min', 'max']).add_prefix('type_1__hp_ratio__').reset_index()\ngb_cols = tmp_gb.columns.tolist()[1:]\n\nwhole_df = pd.merge(whole_df, tmp_gb, on='type_1', how='left')\n\nfor col in gb_cols:\n    whole_df[col] -= whole_df['hp']","4d9efb65":"tmp = whole_df[['target', 'attack', 'egg_group_1']]\ntmp_gb = tmp.groupby(['egg_group_1'])['attack'].agg(['mean', 'min', 'max']).add_prefix('egg_group_1__attack_ratio__').reset_index()\ngb_cols = tmp_gb.columns.tolist()[1:]\n\nwhole_df = pd.merge(whole_df, tmp_gb, on='egg_group_1', how='left')\n\nfor col in gb_cols:\n    whole_df[col] -= whole_df['attack']\n    \ntmp = whole_df[['target', 'hp', 'egg_group_1']]\ntmp_gb = tmp.groupby(['egg_group_1'])['hp'].agg(['mean', 'min', 'max']).add_prefix('egg_group_1__hp_ratio__').reset_index()\ngb_cols = tmp_gb.columns.tolist()[1:]\n\nwhole_df = pd.merge(whole_df, tmp_gb, on='egg_group_1', how='left')\n\nfor col in gb_cols:\n    whole_df[col] -= whole_df['hp']","82f373db":"whole_df.columns","8f54bb58":"tmp = whole_df[['target', 'base_stats', 'shape_id']]\ntmp_gb = tmp.groupby(['shape_id'])['base_stats'].agg(['mean', 'min', 'max']).add_prefix('shape_id__Base_stats__').reset_index()\ngb_cols = tmp_gb.columns.tolist()[1:]\n\nwhole_df = pd.merge(whole_df, tmp_gb, on='shape_id', how='left')\n\nfor col in gb_cols:\n    whole_df[col] -= whole_df['base_stats']","98267fb9":"tmp = whole_df[['target', 'attack', 'shape_id']]\ntmp_gb = tmp.groupby(['shape_id'])['attack'].agg(['mean', 'min', 'max']).add_prefix('shape_id__attack_ratio__').reset_index()\ngb_cols = tmp_gb.columns.tolist()[1:]\n\nwhole_df = pd.merge(whole_df, tmp_gb, on='shape_id', how='left')\n\nfor col in gb_cols:\n    whole_df[col] -= whole_df['attack']\n    \ntmp = whole_df[['target', 'hp', 'shape_id']]\ntmp_gb = tmp.groupby(['shape_id'])['hp'].agg(['mean', 'min', 'max']).add_prefix('shape_id__hp_ratio__').reset_index()\ngb_cols = tmp_gb.columns.tolist()[1:]\n\nwhole_df = pd.merge(whole_df, tmp_gb, on='shape_id', how='left')\n\nfor col in gb_cols:\n    whole_df[col] -= whole_df['hp']","44afea0d":"## \u4e0a\u8a18\u307e\u3067\u3067\u4e00\u65e6\u30c7\u30fc\u30bf\u306f\u3059\u3079\u3066\u5909\u63db\u3092\u5b8c\u4e86\n## \u30c7\u30fc\u30bf\u30c1\u30a7\u30c3\u30af\nwhole_df.head().T","07e8e7fd":"## Data Split\nwith timer('Data Split'):\n    train_df = whole_df[whole_df['flag'] == 0].reset_index(drop=True)\n    test_df = whole_df[whole_df['flag'] == 1].reset_index(drop=True)\n    \n    print(f'Train: {train_df.shape} | Test: {test_df.shape}')","4f35a9dc":"## Non Training Columns\ndrop_cols = [\"id\", \"pokemon\", \"species_id\", \"target\", \"flag\",'type_1', 'type_2']","c6145480":"# Train\u5168\u4f53\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\u5024, Test\u5168\u4f53\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\u5024\u3092\u78ba\u8a8d\noof = np.zeros(len(train_df))\npred = np.zeros(len(test_df))\n\n# Feature Importance\u3092\u78ba\u8a8d\nimportances_all = pd.DataFrame()","2c4f7089":"for seed in range(CONFIG.n_seeds):\n    \n    seed_everything(seed)\n    CV = KFold(n_splits=CONFIG.n_fold, shuffle=True, random_state=seed)\n\n    ## Cross Validation\u3067Fold\u3054\u3068\u306b\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3001\u4e88\u6e2c\n    for fold, (tr_idx, va_idx) in enumerate(CV.split(train_df, train_df['target'])):\n\n        print(f'======================= Seed {seed} | Fold {fold+1} =============================')\n\n        # \u6642\u9593\u8a08\u6e2c\n        st_time = time.time()\n\n        ## \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u3068\u8a55\u4fa1\u7528\u30c7\u30fc\u30bf\u306b\u5206\u96e2\n        X_train = train_df.loc[tr_idx, :]\n        X_valid = train_df.loc[va_idx, :]\n\n        ## id\u306a\u3069\u306eNon-Training\u5217\u306f\u524a\u9664\n        X_train = X_train.drop(drop_cols, axis=1)\n        X_valid = X_valid.drop(drop_cols, axis=1)\n\n        y_train = train_df.loc[tr_idx, 'target']\n        y_valid = train_df.loc[va_idx, 'target']\n\n        ## LightGBM\u306eDataset\u5316\n        lgb_train = lgb.Dataset(X_train, y_train)\n        lgb_eval = lgb.Dataset(X_valid, y_valid)\n\n        ## CatBoost\u306eDataset\u5316\n        cat_train = cat.Pool(X_train, y_train)\n        cat_eval= cat.Pool(X_train, y_train)\n        \n        lgb_params = {\n            \"objective\": CONFIG.objective,\n            \"metric\": CONFIG.metric,\n            \"learning_rate\": CONFIG.learning_rate,\n            \"verbosity\": 0,\n            \"random_state\": seed,\n            \"num_leaves\": CONFIG.num_leaves\n        }\n\n        cat_params = {\n            \"loss_function\": \"RMSE\",\n            \"num_boost_round\": 1000,\n            \"verbose_eval\": 0\n        }\n\n        ## model \u306e\u5b66\u7fd2\n        lgb_model = lgb.train(\n            lgb_params,\n            lgb_train,\n            valid_sets = [lgb_train, lgb_eval], # train,valid\u4e21\u65b9\u8868\u793a\n            num_boost_round = CONFIG.num_boost_round,\n            early_stopping_rounds=CONFIG.early_stopping_rounds,\n            verbose_eval=0\n        )\n        best_iter = lgb_model.best_iteration\n\n        cat_model =cat.CatBoost(cat_params)\n        cat_model.fit(\n            cat_train,\n            eval_set = [cat_train, cat_eval],\n            early_stopping_rounds = 30,\n            verbose_eval = 0\n        )\n\n        ## valid\u306e\u4e88\u6e2c\u5024\u3092\u683c\u7d0d\n        lgb_oof = lgb_model.predict(X_valid, num_iteration=best_iter)\n        cat_oof = cat_model.predict(X_valid)\n        oof[va_idx] += (lgb_oof + cat_oof)\/(2. * CONFIG.n_seeds)\n\n        ## valid\u306e\u4e88\u6e2c\u5024\u3068\u6b63\u89e3\u5024\u306e\u30b9\u30b3\u30a2\u3092\u78ba\u8a8d\n        lgb_score = mean_squared_error(y_valid, lgb_oof) ** .5\n        cat_score = mean_squared_error(y_valid, cat_oof) ** .5\n        score = mean_squared_error(y_valid, (lgb_oof + cat_oof)\/2.) ** .5\n\n        ## test\u30c7\u30fc\u30bf\u306e\u4e88\u6e2c\u5024\u3092\u683c\u7d0d\n        lgb_pred = lgb_model.predict(\n            test_df.drop(drop_cols, axis=1),\n            num_iteration=best_iter\n        )\n        cat_pred = cat_model.predict(test_df.drop(drop_cols, axis=1))\n\n        ## \u4e88\u6e2c\u5024\u306f\u5404Fold\u306e\u5e73\u5747\n        pred += ((lgb_pred+cat_pred)\/2.)\/(CONFIG.n_fold * CONFIG.n_seeds)\n\n        ## Fold\u3054\u3068\u306eFeature Importance\n        importances = pd.DataFrame()\n        importances['feature'] = X_train.columns.tolist()\n        importances['lgb_gain'] = lgb_model.feature_importance()\n        importances['fold'] = fold+1\n        importances['seed'] = seed\n        importances_all = pd.concat([importances_all, importances], axis=0, sort=False)\n\n        ## Model Save\n        lgb_model.save_model(os.path.join(MODEL_DIR, f'lgb_model_seed{seed}_fold{fold+1}.txt'))\n        cat_model.save_model(os.path.join(MODEL_DIR, f'cat_model_seed{seed}_fold{fold+1}.txt'))\n\n        e_time = time.time() - st_time\n        print(f'SEED: {seed} | Fold: {fold+1} | LightGBM: {lgb_score:.6f} | CatBoost: {cat_score:.6f} | Ensemble: {score:.6f} | Elapsed: {e_time:.0f}s')\n\n        del lgb_model, cat_model, X_train, X_valid, y_train, y_valid\n        _ = gc.collect()\n    \nprint(\"\u2605\"*50)\ntotal_score = mean_squared_error(train_df['target'], oof) ** .5\nprint(f'Seed Total Score: {total_score:.6f}')","7924e65b":"plt.figure(figsize=(16, 5),tight_layout=True)\nsns.distplot(train_df['target'], label='train')\nsns.distplot(oof, label='oof')\nsns.distplot(pred, label='pred')\nplt.legend()\nplt.show()","925a9321":"mean_importance = importances_all.groupby('feature')['lgb_gain'].agg('mean')\nmean_importance = mean_importance.sort_values(ascending=False)\nimportance_list = mean_importance.index.tolist()[:40]\n\nplt.figure(figsize=(20, 6), tight_layout=True)\nsns.boxplot(data=importances_all[importances_all['feature'].isin(importance_list)].sort_values('lgb_gain', ascending=False),\n            x='feature', y='lgb_gain')\nplt.xticks(rotation=90)\nplt.title(\"\u7279\u5fb4\u91cf\u91cd\u8981\u5ea6 LightGBM\")\nplt.show()","5f5e476e":"sub_df['target'] = np.expm1(pred)\nsub_df.to_csv(f'.\/{CONFIG.exp_name}_CV{total_score:.6f}_noPsuedo_submision.csv', index=False)","cebcb960":"test_df['target'] = pred\nwhole_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)","52435826":"# Train\u5168\u4f53\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\u5024, Test\u5168\u4f53\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\u5024\u3092\u78ba\u8a8d\noof = np.zeros(len(whole_df))\npred = np.zeros(len(test_df))\n\n# Feature Importance\u3092\u78ba\u8a8d\nimportances_all = pd.DataFrame()","f29d15a7":"for seed in range(CONFIG.n_seeds):\n    \n    seed_everything(seed)\n    CV = KFold(n_splits=CONFIG.n_fold, shuffle=True, random_state=seed)\n\n    ## Cross Validation\u3067Fold\u3054\u3068\u306b\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3001\u4e88\u6e2c\n    for fold, (tr_idx, va_idx) in enumerate(CV.split(whole_df, whole_df['target'])):\n\n        print(f'======================= Seed {seed} | Fold {fold+1} =============================')\n\n        # \u6642\u9593\u8a08\u6e2c\n        st_time = time.time()\n\n        ## \u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u3068\u8a55\u4fa1\u7528\u30c7\u30fc\u30bf\u306b\u5206\u96e2\n        X_train = whole_df.loc[tr_idx, :]\n        X_valid = whole_df.loc[va_idx, :]\n\n        ## id\u306a\u3069\u306eNon-Training\u5217\u306f\u524a\u9664\n        X_train = X_train.drop(drop_cols, axis=1)\n        X_valid = X_valid.drop(drop_cols, axis=1)\n\n        y_train = whole_df.loc[tr_idx, 'target']\n        y_valid = whole_df.loc[va_idx, 'target']\n\n        ## LightGBM\u306eDataset\u5316\n        lgb_train = lgb.Dataset(X_train, y_train)\n        lgb_eval = lgb.Dataset(X_valid, y_valid)\n\n        ## CatBoost\u306eDataset\u5316\n        cat_train = cat.Pool(X_train, y_train)\n        cat_eval= cat.Pool(X_train, y_train)\n        \n        lgb_params = {\n            \"objective\": CONFIG.objective,\n            \"metric\": CONFIG.metric,\n            \"learning_rate\": CONFIG.learning_rate,\n            \"verbosity\": 0,\n            \"random_state\": seed,\n            \"num_leaves\": CONFIG.num_leaves\n        }\n\n        cat_params = {\n            \"loss_function\": \"RMSE\",\n            \"num_boost_round\": 1000,\n            \"verbose_eval\": 0\n        }\n\n        ## model \u306e\u5b66\u7fd2\n        lgb_model = lgb.train(\n            lgb_params,\n            lgb_train,\n            valid_sets = [lgb_train, lgb_eval], # train,valid\u4e21\u65b9\u8868\u793a\n            num_boost_round = CONFIG.num_boost_round,\n            early_stopping_rounds=CONFIG.early_stopping_rounds,\n            verbose_eval=0\n        )\n        best_iter = lgb_model.best_iteration\n\n        cat_model =cat.CatBoost(cat_params)\n        cat_model.fit(\n            cat_train,\n            eval_set = [cat_train, cat_eval],\n            early_stopping_rounds = 30,\n            verbose_eval = 0\n        )\n\n        ## valid\u306e\u4e88\u6e2c\u5024\u3092\u683c\u7d0d\n        lgb_oof = lgb_model.predict(X_valid, num_iteration=best_iter)\n        cat_oof = cat_model.predict(X_valid)\n        oof[va_idx] += (lgb_oof + cat_oof)\/(2. * CONFIG.n_seeds)\n\n        ## valid\u306e\u4e88\u6e2c\u5024\u3068\u6b63\u89e3\u5024\u306e\u30b9\u30b3\u30a2\u3092\u78ba\u8a8d\n        lgb_score = mean_squared_error(y_valid, lgb_oof) ** .5\n        cat_score = mean_squared_error(y_valid, cat_oof) ** .5\n        score = mean_squared_error(y_valid, (lgb_oof + cat_oof)\/2.) ** .5\n\n        ## test\u30c7\u30fc\u30bf\u306e\u4e88\u6e2c\u5024\u3092\u683c\u7d0d\n        lgb_pred = lgb_model.predict(\n            test_df.drop(drop_cols, axis=1),\n            num_iteration=best_iter\n        )\n        cat_pred = cat_model.predict(test_df.drop(drop_cols, axis=1))\n\n        ## \u4e88\u6e2c\u5024\u306f\u5404Fold\u306e\u5e73\u5747\n        pred += ((lgb_pred+cat_pred)\/2.)\/(CONFIG.n_fold * CONFIG.n_seeds)\n\n        ## Fold\u3054\u3068\u306eFeature Importance\n        importances = pd.DataFrame()\n        importances['feature'] = X_train.columns.tolist()\n        importances['lgb_gain'] = lgb_model.feature_importance()\n        importances['fold'] = fold+1\n        importances['seed'] = seed\n        importances_all = pd.concat([importances_all, importances], axis=0, sort=False)\n\n        ## Model Save\n        lgb_model.save_model(os.path.join(MODEL_DIR, f'lgb_model_seed{seed}_fold{fold+1}.txt'))\n        cat_model.save_model(os.path.join(MODEL_DIR, f'cat_model_seed{seed}_fold{fold+1}.txt'))\n\n        e_time = time.time() - st_time\n        print(f'SEED: {seed} | Fold: {fold+1} | LightGBM: {lgb_score:.6f} | CatBoost: {cat_score:.6f} | Ensemble: {score:.6f} | Elapsed: {e_time:.0f}s')\n\n        del lgb_model, cat_model, X_train, X_valid, y_train, y_valid\n        _ = gc.collect()\n    \nprint(\"\u2605\"*50)\ntotal_score = mean_squared_error(whole_df['target'], oof) ** .5\nprint(f'Seed Total Score: {total_score:.6f}')","f04f517d":"## sample_submission.csv\u306etarget\u3092pred\u3067\u7f6e\u304d\u63db\u3048\u3066\u3001csv\u3067\u4fdd\u5b58\n## pred\u306flog\u5909\u63db\u3057\u3066\u3044\u308b\u306e\u3067\u3001expm1\u3067\u5143\u306b\u623b\u3059\u3053\u3068\u3092\u5fd8\u308c\u306a\u3044\nsub_df['target'] = np.expm1(pred)\nsub_df.to_csv(f'.\/{CONFIG.exp_name}_CV{total_score:.6f}_Psuedo_submision.csv', index=False)","783c609b":"plt.figure(figsize=(16, 5),tight_layout=True)\nsns.distplot(train_df['target'], label='train')\nsns.distplot(oof, label='oof')\nsns.distplot(pred, label='pred')\nplt.legend()\nplt.show()","e815556d":"## type_1, type_2","9adbbe95":"## \u4e88\u6e2c\u5024\u306e\u5206\u5e03\u3092\u78ba\u8a8d","05d9fb9b":"# Settings","5a2d4e35":"## shape\n- shape\u306fshape_id\u3067\u65e2\u306bEncoding\u3055\u308c\u3066\u3044\u308b\u3063\u307d\u3044\u306e\u3067\u3001shape\u306f\u4e00\u65e6\u524a\u9664","7e9746ca":"<img src=\"https:\/\/www.nme.com\/wp-content\/uploads\/2021\/08\/Pokemon_Legends_Arceus_header-696x441.jpg\" width=100%>","18a44536":"# Training\n- \u4eca\u56de\u306f\u56de\u5e30\u4e88\u6e2c\u3067\u3059\u306e\u3067\u3001KFold\u3067\u4e00\u65e6CrossValidation\u3092\u3057\u307e\u3059\u3002\n- test\u306b\u5bfe\u3057\u3066\u306fCrossValidation\u306e\u5404Fold\u3054\u3068\u306e\u4e88\u6e2c\u5024\u3092\u5e73\u5747\u3057\u307e\u3059\u3002\n- \u4f7f\u7528\u3059\u308b\u30e2\u30c7\u30eb\u306fLightGBM\u3067\u3059\u3002","d4d0779b":"## Egg group","323a3212":"- \u7a2e\u65cf\u5024\u304c\u4f3c\u3066\u3044\u308b\u3082\u306e\u306f\u305d\u306e\u307e\u307e\u5165\u308c\u3066\u3082\u826f\u3055\u305d\u3046","9e723c71":"## Evolves","91289e07":"## \u7a2e\u65cf\u5024\n![\u30df\u30df\u30c3\u30ad\u30e5](https:\/\/pokemon-elpis.xyz\/wp-content\/uploads\/2019\/12\/78919912_154354542541194_6523067578930167808_o-1.jpg)","a91ba5d3":"### \u7a2e\u65cf\u5024\u306e\u5272\u5408","681456d0":"## Color\n- Color\u306f\u4e00\u65e6\u4eca\u56de\u306f\u4f7f\u308f\u306a\u3044\u65b9\u5411\u3067\u8003\u3048\u307e\u3059\n  - 16\u9032\u6570\u306e\u30ab\u30e9\u30fc\u30b3\u30fc\u30c9\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u3042\u307e\u308aid\u3068\u5909\u308f\u3089\u306a\u3044\u3068\u601d\u3044\u307e\u3059\u306e\u3067\u524a\u9664\u3057\u307e\u3059","cad7dddf":"## Mega\u9032\u5316","27686255":"# Preprocess","201eb5e3":"## Groupby Feature","fcbe50d8":"## ability","7015be9a":"## [Baseline](https:\/\/www.kaggle.com\/shimishige\/host-baseline)\u304b\u3089\u306e\u5909\u66f4\u70b9\n- \u30dd\u30b1\u30e2\u30f3\u306eBMI\u3092\u8ffd\u52a0\n  - \u4e38\u307f\u3092\u5e2f\u3073\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u306f\u3001\u30dd\u30b1\u30e2\u30f3\u304c\u592a\u3063\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u3067\u6c7a\u307e\u3089\u306a\u3044\u304b\u306a\u3068\u601d\u3044BMI\u3092\u8ffd\u52a0\n- type\u306bCountRankEncoding\u3092\u8ffd\u52a0\n  - \u30c9\u30e9\u30b4\u30f3\u306a\u3069\u306e\u5e0c\u5c11\u6027\u3068\u3001\u30ce\u30fc\u30de\u30eb\u30fb\u6c34\u30bf\u30a4\u30d7\u306e\u591a\u3055\u3092\u969b\u7acb\u305f\u305b\u305f\u304b\u3063\u305f\n- \u5404\u80fd\u529b\u5024\u306f\u7a2e\u65cf\u5024\u3067\u9664\u7b97\n  - \u80fd\u529b\u5024\u3068\u7a2e\u65cf\u5024\u3060\u3051\u3067\u3069\u306e\u30dd\u30b1\u30e2\u30f3\u304b\u308f\u304b\u3063\u3066\u3057\u307e\u3046\u306e\u3067\u3001\u904e\u5b66\u7fd2\u5bfe\u7b56\u3068\u3057\u3066\u5b9f\u65bd\n- \u985e\u4f3c\u7a2e\u65cf\u5024\u306e\u30dd\u30b1\u30e2\u30f3\u306e\u7d4c\u9a13\u5024\u3092\u4f7f\u7528\n  - \u7a2e\u65cf\u5024\u3068\u7d4c\u9a13\u5024\u306b\u306f\u660e\u78ba\u306b\u95a2\u4fc2\u6027\u304c\u3042\u308b\u3053\u3068\u304c\u56f3\u793a\u3059\u308b\u3068\u5206\u304b\u308b\u306e\u3067\u3001\u985e\u4f3c\u3057\u3066\u3044\u308b\u30dd\u30b1\u30e2\u30f3\u306e\u7a2e\u65cf\u5024\u3092\u8ffd\u52a0\n- \u9032\u5316\u524d\u3001\u9032\u5316\u5f8c\u306e\u7d4c\u9a13\u5024\u3068\u7a2e\u65cf\u5024\u3092\u8ffd\u52a0\n  - 2\u6bb5\u968e\u9032\u5316\u3059\u308b\u30dd\u30b1\u30e2\u30f3\u3067\u3042\u308c\u3070\u3001\u9032\u5316\u524d\u3068\u9032\u5316\u5f8c\u306e\u7d4c\u9a13\u5024\u306e\u5e73\u5747\u5024\u3082\u8ffd\u52a0\n- \u30e1\u30ac\u9032\u5316\u3082\u540c\u69d8\u306b\u30e1\u30ac\u9032\u5316\u524d\u3001\u30e1\u30ac\u9032\u5316\u5f8c\u306e\u7d4c\u9a13\u5024\u3092\u8ffd\u52a0\n- Groupby x aggregation\n  - type1 x attack, hp, \u7a2e\u65cf\u5024 x mean, min, max\n  - egg_group1 x attack, hp, \u7a2e\u65cf\u5024 x mean, min, max\n  - shape_id x attack, hp, \u7a2e\u65cf\u5024 x mean, min, max\n- seed average\n- LightGBM & Catboost\n- Psuedo Labeling\n  - \u3053\u306eNotebook\u3067\u51fa\u3057\u305f\u4e88\u6e2c\u5024\u3092\u6b63\u89e3\u3068\u3057\u3066\u65b0\u305f\u306btest\u306b\u8ffd\u52a0\u3057\u3066\u3001\u7279\u5fb4\u4f5c\u6210\u304b\u30892\u56de\u307b\u3069\u56de\u3057\u3066\u307e\u3059\n  - \u9032\u5316\u524d\u3084\u9032\u5316\u5f8c\u306e\u7d4c\u9a13\u5024\u3084\u3001\u985e\u4f3c\u30dd\u30b1\u30e2\u30f3\u306e\u7d4c\u9a13\u5024\u306a\u3069\u3092\u4f7f\u3044\u305f\u3044\u305f\u3081\u3067\u3059","34e920f2":"## \u7279\u5fb4\u91cf\u91cd\u8981\u5ea6\u3092\u78ba\u8a8d","24e43387":"# Submit","02c570e0":"## ability_hidden\n- \u3053\u3061\u3089\u3082\u72ec\u7acb\u3067Label Encoding\u3057\u3066\u3044\u304d\u307e\u3059","1073c32a":"## faiss\u3067\u985e\u4f3c\u30b9\u30da\u30c3\u30af\u306e\u30dd\u30b1\u30e2\u30f3\u3092\u63a2\u7d22","aa234cff":"<div class = 'alert alert-block alert-info'\n     style = 'background-color:#ded5e6;\n              color:#b44aee;\n              border-width:5px;\n              border-color:#6a319c;\n              font-family:Comic Sans MS'>\n    <p style = 'font-size:24px'>\n        Final Submision\n    <\/p>\n    <a href = \"#Settings\"\n       style = \"color:#b44aee;\n                font-size:14px\">1.Settings<\/a><br>\n    <a href = \"#Data-Load\"\n       style = \"color:#b44aee;\n                font-size:14px\">2.Data Load<\/a><br>\n    <a href = \"#Preprocess\"\n       style = \"color:#b44aee;\n                font-size:14px\">3.Preprocess<\/a><br>\n    <a href = \"#Training\"\n       style = \"color:#b44aee;\n                font-size:14px\">4.Training<\/a><br>\n    <a href = \"#Psuedo\"\n       style = \"color:#b44aee;\n                font-size:14px\">5.Psuedo<\/a><br>\n    <a href = \"#Submit\"\n       style = \"color:#b44aee;\n                font-size:14px\">6.Submit<\/a><br>\n<\/div>","ddb49257":"## url_image\n- \u9ad8\u6a4b\u3055\u3093\u304cefficientnetB0\u3092\u7528\u3044\u305f\u5206\u6790\u3092\u516c\u958b\u3057\u3066\u304f\u308c\u3066\u3044\u307e\u3057\u305f\u304c\u3001\u4eca\u56de\u306e\u30d9\u30fc\u30b9\u30e9\u30a4\u30f3\u3067\u306f\u753b\u50cf\u306f\u4f7f\u308f\u305a\u4f5c\u6210\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002[\u53c2\u8003](https:\/\/www.kaggle.com\/shunsuketakahashi\/shiggle-1-effb0-training-cv-0-4293-lb-0-46497)","6c363f17":"## Target\u3092log\u5909\u63db","02c90b62":"## height, weight","f32ba155":"# Psuedo","c831a0e4":"## Data Check","4ae714c6":"# Data Load"}}