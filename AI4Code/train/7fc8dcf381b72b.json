{"cell_type":{"c884eac0":"code","0061962b":"code","063fc112":"code","21cd8e0b":"code","55ebfe60":"code","5421cda4":"code","e1030922":"code","4533f66a":"code","fe6c0b43":"code","3e47496e":"code","7c168a57":"code","4b9cd7c0":"code","307eb76f":"code","b9aa0e91":"code","271fcb07":"code","c02e4204":"code","9ef1e84b":"code","96bc450a":"code","bacddca5":"code","5739db16":"code","1d3dee5f":"code","b1b5ec13":"code","dd0bb0b9":"code","006cac03":"code","2b1c755a":"code","c5a79ef5":"code","4dd53071":"code","040842cc":"code","98a63336":"markdown","0b1cb9ce":"markdown","7c35adc8":"markdown","f66f63ec":"markdown","7ac0eaae":"markdown","71e76c75":"markdown","9a3f0c31":"markdown","c8715554":"markdown","0dd54988":"markdown","fdc2ec7c":"markdown","e5689deb":"markdown","69a6b7ec":"markdown","47e92b28":"markdown","b2a3e13f":"markdown","7585c74d":"markdown","606139c9":"markdown","b0b23b9d":"markdown","4cc1b9f8":"markdown","3e9841f3":"markdown","68369818":"markdown","397f48b9":"markdown","81572875":"markdown","6d172525":"markdown","1f566199":"markdown","49a7d6e0":"markdown","9d6c9446":"markdown"},"source":{"c884eac0":"!pip install pyspark","0061962b":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","063fc112":"from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('ml-bank').getOrCreate()\nsdf = spark.read.csv('..\/input\/bankbalanced\/bank.csv', header = True, inferSchema = True)\nsdf.printSchema()","21cd8e0b":"import pandas as pd\n\npd.DataFrame(sdf.take(5), columns=sdf.columns).transpose()","55ebfe60":"sdf.toPandas().groupby(['deposit']).size()","5421cda4":"numeric_features = [t[0] for t in sdf.dtypes if t[1] == 'int']\nsdf.select(numeric_features).describe().toPandas().transpose()","e1030922":"\nnumeric_data = sdf.select(numeric_features).toPandas()\naxs = pd.plotting.scatter_matrix(numeric_data, figsize=(8, 8));\nn = len(numeric_data.columns)\n\nfor i in range(n):\n    v = axs[i, 0]\n    v.yaxis.label.set_rotation(0)\n    v.yaxis.label.set_ha('right')\n    v.set_yticks(())\n    h = axs[n-1, i]\n    h.xaxis.label.set_rotation(90)\n    h.set_xticks(())","4533f66a":"sdf = sdf.select(\n    'age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', \n    'contact', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit'\n)\ncols = sdf.columns\nsdf.printSchema()","fe6c0b43":"from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n\nstages = []\ncategoricalColumns = [\n    'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome'\n]\n\nfor categoricalCol in categoricalColumns:\n    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n    encoder = OneHotEncoderEstimator(\n        inputCols=[stringIndexer.getOutputCol()], \n        outputCols=[categoricalCol + \"classVec\"]\n    )\n    stages += [stringIndexer, encoder]\n    \nlabel_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\nstages += [label_stringIdx]\nnumericCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]","3e47496e":"from pyspark.ml import Pipeline\n\npipeline = Pipeline(stages = stages)\npipelineModel = pipeline.fit(sdf)\nsdf = pipelineModel.transform(sdf)\nselectedCols = ['label', 'features'] + cols\nsdf = sdf.select(selectedCols)\nsdf.printSchema()","7c168a57":"pdf = pd.DataFrame(sdf.take(5), columns=sdf.columns)\npdf.iloc[:,0:2] \n","4b9cd7c0":"len(pdf.features[0])","307eb76f":"train, test = sdf.randomSplit([0.7, 0.3], seed = 2018)\nprint(\"Training Dataset Count: \" + str(train.count()))\nprint(\"Test Dataset Count: \" + str(test.count()))","b9aa0e91":"from pyspark.ml.classification import LogisticRegression\n\nlr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\nlrModel = lr.fit(train)","271fcb07":"import matplotlib.pyplot as plt\nimport numpy as np\n\nbeta = np.sort(lrModel.coefficients)\nplt.plot(beta)\nplt.ylabel('Beta Coefficients')\nplt.show()","c02e4204":"trainingSummary = lrModel.summary\nlrROC = trainingSummary.roc.toPandas()\n\nplt.plot(lrROC['FPR'],lrROC['TPR'])\nplt.ylabel('False Positive Rate')\nplt.xlabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\n\nprint('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))","9ef1e84b":"pr = trainingSummary.pr.toPandas()\nplt.plot(pr['recall'],pr['precision'])\nplt.ylabel('Precision')\nplt.xlabel('Recall')\nplt.show()","96bc450a":"lrPreds = lrModel.transform(test)\nlrPreds.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","bacddca5":"lrPreds.show()","5739db16":"from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nlrEval = BinaryClassificationEvaluator()\nprint('Test Area Under ROC', lrEval.evaluate(lrPreds))","1d3dee5f":"from pyspark.ml.classification import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\ndtModel = dt.fit(train)\ndtPreds = dtModel.transform(test)\ndtPreds.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","b1b5ec13":"dtEval = BinaryClassificationEvaluator()\ndtROC = dtEval.evaluate(dtPreds, {dtEval.metricName: \"areaUnderROC\"})\nprint(\"Test Area Under ROC: \" + str(dtROC))","dd0bb0b9":"from pyspark.ml.classification import RandomForestClassifier\n\nrf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\nrfModel = rf.fit(train)\nrfPreds = rfModel.transform(test)\nrfPreds.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","006cac03":"rfEval = BinaryClassificationEvaluator()\nrfROC = rfEval.evaluate(rfPreds, {rfEval.metricName: \"areaUnderROC\"})\nprint(\"Test Area Under ROC: \" + str(rfROC))","2b1c755a":"from pyspark.ml.classification import GBTClassifier\n\ngbt = GBTClassifier(maxIter=10)\ngbtModel = gbt.fit(train)\ngbtPreds = gbtModel.transform(test)\ngbtPreds.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","c5a79ef5":"gbtEval = BinaryClassificationEvaluator()\ngbtROC = gbtEval.evaluate(gbtPreds, {gbtEval.metricName: \"areaUnderROC\"})\nprint(\"Test Area Under ROC: \" + str(gbtROC))","4dd53071":"print(gbt.explainParams())","040842cc":"from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nparamGrid = (ParamGridBuilder()\n             .addGrid(gbt.maxDepth, [2, 4, 6])\n             .addGrid(gbt.maxBins, [20, 60])\n             .addGrid(gbt.maxIter, [10, 20])\n             .build())\n\ncv = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=gbtEval, numFolds=5)\n\n# Run cross validations.  \n# This can take some minutes since it is training over 20 trees!\ncvModel = cv.fit(train)\ncvPreds = cvModel.transform(test)\ngbtEval.evaluate(cvPreds)","98a63336":"Dataset classes are perfect balanced.","0b1cb9ce":"### Evaluate the Decision Tree model\n\nOne simple decision tree performed poorly because it is too weak given the range of different features. The prediction accuracy of decision trees can be improved by Ensemble methods, such as Random Forest and Gradient-Boosted Tree.","7c35adc8":"# Machine Learning with PySpark and MLlib: Solving a Binary Classification Problem\n\nHere, we will learn how to build a **Binary Classification** application using **PySpark** and **MLlib Pipelines API**. \n\nWe tried **Logistic Regression**, **Decision Tree**, **Random Forest**, and **Gradient-Boosted Tree** algorithms and **Gradient Boosting Tree**  performed best on the data set.","f66f63ec":"### Precision and recall","7ac0eaae":"### Make predictions on the test set","71e76c75":"Summarize the model over the training set, we can also obtain the **ROC Receiver-Operating Characteristic)** and the **Area under ROC** (<code>areaUnderROC<\/code>).","9a3f0c31":"## Logistic Regression Model\n","c8715554":"### Evaluate the Random Forest Classifier","0dd54988":"It\u2019s obvious that there aren\u2019t highly correlated numeric variables. Therefore, we will keep all of them for the model. \n\nHowever, day and month columns are not really useful, we will remove these two columns.","fdc2ec7c":"## Preparing Dataset for Machine Learning\n\nThe process includes **Category Indexing**, **One-Hot Encoding** and **VectorAssembler** (a feature transformer that merges multiple columns into a vector column).\n\nThis  code  indexes each categorical column using the <code>StringIndexer<\/code>, then converts the indexed categories into *one-hot encoded* variables. The resulting output has the binary vectors appended to the end of each row. We use the <code>StringIndexer<\/code> again to encode our labels to label indices. Next, we use the <code>VectorAssembler<\/code>  to combine all the feature columns into a single vector column.","e5689deb":"**Input variables:** age, job, marital, education, default, balance, housing, loan, contact, day, month, duration, campaign, pdays, previous, poutcome\n\n**Output variable:** deposit","69a6b7ec":"### Evaluate the Logistic Regression model","47e92b28":"We now have <code>features<\/code> column and <code>label<\/code> column.","b2a3e13f":"## Random Forest Classifier","7585c74d":"**[Apache Spark](https:\/\/spark.apache.org\/)**, once a component of the **[Hadoop](http:\/\/hadoop.apache.org\/)** ecosystem, is now becoming the Big-Data platform of choice for enterprises. It is a powerful open source engine that provides real-time stream processing, interactive processing, graph processing, in-memory processing as well as batch processing with very fast speed, ease of use and standard interface.","606139c9":"### Summary statistics for numeric variables","b0b23b9d":"We can obtain the coefficients by using <code>LogisticRegressionModel<\/code>\u2019s attributes.","4cc1b9f8":"### Split dataset into train and test set\n\nRandomly split data into train and test sets, and set seed for reproducibility.","3e9841f3":"Have a peek of the first five observations. Pandas data frame is prettier than Spark DataFrame show() method.","68369818":"## Exploring Dataset\n\nWe will use the same data set when we built a [Logistic Regression](https:\/\/towardsdatascience.com\/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\/) in Python, and it is related to direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict whether the client will subscribe (Yes\/No) to a term deposit. The dataset can be downloaded from [Kaggle](https:\/\/www.kaggle.com\/rouseguy\/bankbalanced\/data).","397f48b9":"## Gradient-Boosted Tree Classifier","81572875":"### Evaluate the Gradient-Boosted Tree Classifier","6d172525":"## Decision Tree Classifier\n\n**Decision trees** are widely used since they are easy to interpret, handle categorical features, extend to the multi-class classification, do not require feature scaling, and are able to capture non-linearities and feature interactions.","1f566199":"### Correlations between independent variables","49a7d6e0":"### Pipeline\n\nWe use <code>Pipeline<\/code> to chain multiple Transformers and Estimators together to specify the **Machine Learning** workflow. A <code>Pipeline<\/code>\u2019s stages are specified as an ordered array.","9d6c9446":"**Gradient-Boosted Tree** achieved the best results, we will try tuning this model with the <code>ParamGridBuilder<\/code> and the <code>CrossValidator<\/code>. \n\nBefore that we can use <code>explainParams()<\/code> to print a list of all params and their definitions to understand what params available for tuning."}}