{"cell_type":{"25235f1e":"code","37777084":"code","6c1eedc1":"code","dcf40311":"code","c5953478":"code","22c928d5":"code","75de71bc":"code","6a8d8893":"code","28bc33db":"code","7bab8ed6":"code","861f7517":"code","ba7a793e":"code","e31549af":"code","d33fef8f":"code","04be7e34":"code","c1303bfd":"code","ebf444bd":"code","089344b9":"code","98592ca9":"code","f0176838":"code","599c0b85":"code","42c631b3":"code","50b73234":"code","c5af7bad":"code","43f3a525":"code","6740bdc3":"code","fd707806":"code","166da0bf":"code","216ed3d8":"code","62d34086":"code","17712d7e":"code","ac0db119":"code","30e0a5b2":"code","3fe6bbf7":"code","638abb89":"code","70b175e9":"code","16e2b6ab":"code","8fd19ea0":"code","a0d7f2f7":"code","25c6232e":"code","e6f00249":"code","8c8c477c":"code","6599e710":"code","44433b17":"code","cc843630":"code","62bcbf2e":"code","f9784ed4":"code","caa0023c":"code","4f514c70":"code","36439268":"code","a187a4a3":"code","d987af4c":"code","8c36475d":"code","7a814332":"code","85c1dd3f":"code","b2add194":"code","fcabbbd8":"code","007b4657":"code","c13cf76b":"code","0bddc003":"code","fce5e6a8":"code","6eebdb80":"code","e5d2eefd":"code","67839b0b":"code","4e9e2b3e":"code","b1300c1e":"code","0044aade":"code","8701f086":"code","c476cd6b":"code","98bb99d7":"code","53a02114":"code","19a57664":"code","05d58079":"code","e5fd711f":"code","303cafc9":"code","090cb8bb":"code","d39faa85":"code","bed5409b":"code","6ddc79ae":"code","9cb34869":"code","472a3d23":"code","7a0d84f4":"code","255e2720":"code","cd210362":"code","a4e574e0":"code","f9ef8c61":"code","3e992aba":"code","1614af59":"code","033c67d5":"markdown","47ff003d":"markdown","0d066d1f":"markdown","795a7122":"markdown","e75a3183":"markdown","4aa44587":"markdown","90202556":"markdown","8ffb9d18":"markdown","5e4ed9f3":"markdown","5a1ad301":"markdown","45759d51":"markdown","2adf2dd1":"markdown","551620dd":"markdown","ff62d651":"markdown","26fd83c2":"markdown","7288b37a":"markdown","c3921731":"markdown","337591c0":"markdown","96f9c348":"markdown","ee6806c5":"markdown","235f5773":"markdown","fff35a7f":"markdown","bea2bc7b":"markdown","a19814fe":"markdown","639dc83f":"markdown","5117521b":"markdown","d93c12be":"markdown","3c6b3a5a":"markdown","ac43fad2":"markdown","e8ba9920":"markdown","79213437":"markdown","498759a3":"markdown","7dd0dbcb":"markdown","ca57c5a4":"markdown","10a4283f":"markdown","9bbb6453":"markdown","b50ad2a8":"markdown","2802e46c":"markdown","cdba547b":"markdown","aa791d53":"markdown","bd1eea55":"markdown","4925444d":"markdown","9cb4f3d5":"markdown","3816f6b7":"markdown","e19a1882":"markdown","b1e3461f":"markdown","213cb182":"markdown","ca079da7":"markdown","16f91d5b":"markdown","9ed034c4":"markdown","46314b34":"markdown"},"source":{"25235f1e":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom scipy import stats\nimport random\n\n# Standard plotly imports\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks\nimport cufflinks as cf\n\nimport lightgbm\nfrom functools import partial\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\n\n# Using plotly + cufflinks in offline mode\ninit_notebook_mode(connected=True)\ncufflinks.go_offline(connected=True)\n\nimport gc\nimport warnings\nfrom itertools import product\nwarnings.filterwarnings(\"ignore\")","37777084":"df_train = pd.read_csv('..\/input\/sales_train.csv')\n\ndf_categories = pd.read_csv(\"..\/input\/item_categories.csv\")\ndf_items = pd.read_csv(\"..\/input\/items.csv\")\ndf_shops = pd.read_csv(\"..\/input\/shops.csv\")\n\ndf_test = pd.read_csv(\"..\/input\/test.csv\")","6c1eedc1":"df_train = pd.merge(df_train, df_items, on='item_id', how='inner')\ndf_train = pd.merge(df_train, df_categories, on='item_category_id', how='inner')\ndf_train = pd.merge(df_train, df_shops, on='shop_id', how='inner')\n\ndf_test = pd.merge(df_test, df_items, on='item_id', how='inner')\ndf_test = pd.merge(df_test, df_categories, on='item_category_id', how='inner')\ndf_test = pd.merge(df_test, df_shops, on='shop_id', how='inner')\n\n# del df_items, df_categories, df_shops\n\ngc.collect()","dcf40311":"df_train.head()","c5953478":"dict_categories = ['Cinema - DVD', 'PC Games - Standard Editions',\n                    'Music - Local Production CD', 'Games - PS3', 'Cinema - Blu-Ray',\n                    'Games - XBOX 360', 'PC Games - Additional Editions', 'Games - PS4',\n                    'Gifts - Stuffed Toys', 'Gifts - Board Games (Compact)',\n                    'Gifts - Figures', 'Cinema - Blu-Ray 3D',\n                    'Programs - Home and Office', 'Gifts - Development',\n                    'Gifts - Board Games', 'Gifts - Souvenirs (on the hinge)',\n                    'Cinema - Collection', 'Music - MP3', 'Games - PSP',\n                    'Gifts - Bags, Albums, Mouse Pads', 'Gifts - Souvenirs',\n                    'Books - Audiobooks', 'Gifts - Gadgets, robots, sports',\n                    'Accessories - PS4', 'Games - PSVita',\n                    'Books - Methodical materials 1C', 'Payment cards - PSN',\n                    'PC Games - Digit', 'Games - Game Accessories', 'Accessories - XBOX 360',\n                    'Accessories - PS3', 'Games - XBOX ONE', 'Music - Vinyl',\n                    'Programs - 1C: Enterprise 8', 'PC Games - Collectible Editions',\n                    'Gifts - Attributes', 'Service Tools',\n                    'Music - branded production CD', 'Payment cards - Live!',\n                    'Game consoles - PS4', 'Accessories - PSVita', 'Batteries',\n                    'Music - Music Video', 'Game Consoles - PS3',\n                    'Books - Comics, Manga', 'Game Consoles - XBOX 360',\n                    'Books - Audiobooks 1C', 'Books - Digit',\n                    'Payment cards (Cinema, Music, Games)', 'Gifts - Cards, stickers',\n                    'Accessories - XBOX ONE', 'Pure media (piece)',\n                    'Programs - Home and Office (Digital)', 'Programs - Educational',\n                    'Game consoles - PSVita', 'Books - Artbooks, encyclopedias',\n                    'Programs - Educational (Digit)', 'Accessories - PSP',\n                    'Gaming consoles - XBOX ONE', 'Delivery of goods',\n                    'Payment Cards - Live! (Figure) ',' Tickets (Figure) ',\n                    'Music - Gift Edition', 'Service Tools - Tickets',\n                    'Net media (spire)', 'Cinema - Blu-Ray 4K', 'Game consoles - PSP',\n                    'Game Consoles - Others', 'Books - Audiobooks (Figure)',\n                    'Gifts - Certificates, Services', 'Android Games - Digit',\n                    'Programs - MAC (Digit)', 'Payment Cards - Windows (Digit)',\n                    'Books - Business Literature', 'Games - PS2', 'MAC Games - Digit',\n                    'Books - Computer Literature', 'Books - Travel Guides',\n                    'PC - Headsets \/ Headphones', 'Books - Fiction',\n                    'Books - Cards', 'Accessories - PS2', 'Game consoles - PS2',\n                    'Books - Cognitive literature']\n\ndict_shops = ['Moscow Shopping Center \"Semenovskiy\"', \n              'Moscow TRK \"Atrium\"', \n              \"Khimki Shopping Center\",\n              'Moscow TC \"MEGA Teply Stan\" II', \n              'Yakutsk Ordzhonikidze, 56',\n              'St. Petersburg TC \"Nevsky Center\"', \n              'Moscow TC \"MEGA Belaya Dacha II\"',\n              'Voronezh (Plekhanovskaya, 13)', \n              'Yakutsk Shopping Center \"Central\"',\n              'Chekhov SEC \"Carnival\"', \n              'Sergiev Posad TC \"7Ya\"',\n              'Tyumen TC \"Goodwin\"',\n              'Kursk TC \"Pushkinsky\"', \n              'Kaluga SEC \"XXI Century\"',\n              'N.Novgorod Science and entertainment complex \"Fantastic\"',\n              'Moscow MTRC \"Afi Mall\"',\n              'Voronezh SEC \"Maksimir\"', 'Surgut SEC \"City Mall\"',\n              'Moscow Shopping Center \"Areal\" (Belyaevo)', 'Krasnoyarsk Shopping Center \"June\"',\n              'Moscow TK \"Budenovsky\" (pav.K7)', 'Ufa \"Family\" 2',\n              'Kolomna Shopping Center \"Rio\"', 'Moscow Shopping Center \"Perlovsky\"',\n              'Moscow Shopping Center \"New Century\" (Novokosino)', 'Omsk Shopping Center \"Mega\"',\n              'Moscow Shop C21', 'Tyumen Shopping Center \"Green Coast\"',\n              'Ufa TC \"Central\"', 'Yaroslavl shopping center \"Altair\"',\n              'RostovNaDonu \"Mega\" Shopping Center', '\"Novosibirsk Mega \"Shopping Center',\n              'Samara Shopping Center \"Melody\"', 'St. Petersburg TC \"Sennaya\"',\n              \"Volzhsky Shopping Center 'Volga Mall' \",\n              'Vologda Mall \"Marmelad\"', 'Kazan TC \"ParkHouse\" II',\n              'Samara Shopping Center ParkHouse', '1C-Online Digital Warehouse',\n              'Online store of emergencies', 'Adygea Shopping Center \"Mega\"',\n              'Balashikha shopping center \"October-Kinomir\"' , 'Krasnoyarsk Shopping center \"Vzletka Plaza\" ',\n              'Tomsk SEC \"Emerald City\"', 'Zhukovsky st. Chkalov 39m? ',\n              'Kazan Shopping Center \"Behetle\"', 'Tyumen SEC \"Crystal\"',\n              'RostovNaDonu TRK \"Megacenter Horizon\"',\n              '! Yakutsk Ordzhonikidze, 56 fran', 'Moscow TC \"Silver House\"',\n              'Moscow TK \"Budenovsky\" (pav.A2)', \"N.Novgorod SEC 'RIO' \",\n              '! Yakutsk TTS \"Central\" fran', 'Mytishchi TRK \"XL-3\"',\n              'RostovNaDonu TRK \"Megatsentr Horizon\" Ostrovnoy', 'Exit Trade',\n              'Voronezh SEC City-Park \"Grad\"', \"Moscow 'Sale'\",\n              'Zhukovsky st. Chkalov 39m\u00b2 ',' Novosibirsk Shopping Mall \"Gallery Novosibirsk\"']","22c928d5":"# This function is to extract date features\ndef date_process(df):\n    df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%d.%m.%Y\") # seting the column as pandas datetime\n    df[\"_weekday\"] = df['date'].dt.weekday #extracting week day\n    df[\"_day\"] = df['date'].dt.day # extracting day\n    df[\"_month\"] = df['date'].dt.month # extracting month\n    \n    return df #returning the df after the transformations\n\ndef cross_heatmap(df, cols, normalize=False, values=None, aggfunc=None):\n    temp = cols\n    cm = sns.light_palette(\"green\", as_cmap=True)\n    return pd.crosstab(df[temp[0]], df[temp[1]], \n                       normalize=normalize, values=values, aggfunc=aggfunc).style.background_gradient(cmap = cm)\n\ndef quantiles(df, columns):\n    for name in columns:\n        print(name + \" quantiles\")\n        print(df[name].quantile([.01,.25,.5,.75,.99]))\n        print(\"\")\n\ndef chi2_test(col ):\n    stat, p, dof, expected = stats.chi2_contingency((pd.crosstab(df_train[col], df_train.item_cnt_day)))\n    # interpret test-statistic\n    prob = 0.95\n    critical = stats.chi2.ppf(prob, dof)\n    print(f\"Testing the {col} by Total Items Sold\")\n    print('dof=%d' % dof)\n    print(p)\n    print(\"Critical Result: \")\n    if abs(stat) >= critical:\n        print(f\"Critical {round(critical,4)}\")\n        print('Dependent (reject H0)')\n    else:\n        print('Independent (fail to reject H0)')\n        \n    print(\"\")\n    alpha = 1.0 - prob\n    print(\"P Value: \")\n    if p <= alpha:\n        print(f\"P-Value: {round(p,8)}\")\n        print('Dependent (reject H0)')\n    else:\n        print('Independent (fail to reject H0)')\n        \ndef log_transforms(df, cols):\n    for col in cols:\n        df[col+'_log'] = np.log(df[col] + 1)\n    return df\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: \n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\ndef knowningData(df, limit=5): #seting the function with df, \n    print(f\"Dataset Shape: {df.shape}\")\n    print('Unique values per column: ')\n    print(df.nunique())\n    print(\"################\")\n    print(\"\")    \n    for column in df.columns: #initializing the loop\n        print(\"Column Name: \", column )\n        entropy = round(stats.entropy(df[column].value_counts(), base=2),2)\n        print(\"entropy \", entropy, \n              \" | Total nulls: \", (round(df[column].isnull().sum() \/ len(df[column]) * 100,2)),\n              \" | Total unique values: \", df.nunique()[column], #print the data and % of nulls\n              \" | Missing: \", df[column].isna().sum())\n        print(\"Top 5 most frequent values: \")\n        print(round(df[column].value_counts()[:limit] \/ df[column].value_counts().sum() * 100,2))\n        print(\"\")\n        print(\"####################################\")","75de71bc":"knowningData(df_train)","6a8d8893":"df_train.item_category_name = df_train.item_category_name.map(dict(zip(df_train.item_category_name.value_counts().index, dict_categories)))\ndf_train.shop_name = df_train.shop_name.map(dict(zip(df_train.shop_name.value_counts().index, dict_shops)))","28bc33db":"plt.figure(figsize=(16,12))\n\nplt.subplot(221)\ng = sns.distplot(np.log(df_train[df_train['item_cnt_day'] >0]['item_cnt_day']))\ng.set_title(\"Item Sold Count Distribuition\", fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Frequency\", fontsize=12)\n\nplt.subplot(222)\ng1 = plt.scatter(range(df_train.shape[0]), np.sort(df_train.item_cnt_day.values))\ng1= plt.title(\"Item Sold ECDF Distribuition\", fontsize=18)\ng1 = plt.xlabel(\"Index\")\ng1 = plt.ylabel(\"Total Items\", fontsize=15)\n\nplt.subplot(223)\ng2 = sns.distplot(np.log(df_train[df_train['item_price'] > 0]['item_price']))\ng2.set_title(\"Items Price Log Distribuition\", fontsize=18)\ng2.set_xlabel(\"\")\ng2.set_ylabel(\"Frequency\", fontsize=15)\n\nplt.subplot(224)\ng3 = plt.scatter(range(df_train.shape[0]), np.sort(df_train.item_price.values))\ng3= plt.title(\"Item Price ECDF Distribuition\", fontsize=18)\ng3 = plt.xlabel(\"Index\")\ng3 = plt.ylabel(\"Item Price Distribution\", fontsize=15)\n\nplt.subplots_adjust(wspace = 0.3, hspace = 0.3,\n                    top = 0.9)\n\nplt.show()","7bab8ed6":"df_train['total_amount'] = df_train['item_price'] * df_train['item_cnt_day']","861f7517":"quantiles(df_train, ['item_cnt_day','item_price', 'total_amount'])","ba7a793e":"import squarify\n\ncolor = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(30)]\nshop_name = df_train[\"shop_name\"].value_counts() #counting the values of shop names\n\nprint(\"Description most frequent countrys: \")\nprint(shop_name[:10]) #printing the 15 top most \n\nshop = round((df_train[\"shop_name\"].value_counts()[:20] \\\n                       \/ len(df_train[\"shop_name\"]) * 100),2)\n\nplt.figure(figsize=(20,10))\ng = squarify.plot(sizes=shop.values, label=shop.index, \n                  value=shop.values,\n                  alpha=.8, color=color)\ng.set_title(\"'TOP 20 Stores\/Shop - % size of total\",fontsize=20)\ng.set_axis_off()\nplt.show()","e31549af":"print(\"Percentual of total sold by each Shop\")\nprint((df_train.groupby('shop_name')['item_price'].sum().nlargest(25) \/ df_train.groupby('shop_name')['item_price'].sum().sum() * 100)[:5])\n\ndf_train.groupby('shop_name')['item_price'].sum().nlargest(25).iplot(kind='bar',\n                                                                     title='TOP 25 Shop Name by Total Amount Sold',\n                                                                     xTitle='Shop Names', \n                                                                     yTitle='Total Sold')","d33fef8f":"print(\"Percentual of total sold by each Shop\")\nprint((df_train.groupby('shop_name')['item_cnt_day'].sum().nlargest(25) \/ df_train.groupby('shop_name')['item_cnt_day'].sum().sum() * 100)[:5])\n\ndf_train.groupby('shop_name')['item_cnt_day'].sum().nlargest(25).iplot(kind='bar',\n                                                                       title='TOP 25 Shop Name by Total Amount Sold',\n                                                                       xTitle='Shop Names', \n                                                                       yTitle='Total Sold')","04be7e34":"df_train.columns","c1303bfd":"df_train = log_transforms(df_train, ['item_price', 'item_cnt_day', 'total_amount'])","ebf444bd":"df_train[['item_cnt_day', 'item_price', 'item_name']].sort_values('item_cnt_day', ascending=False).head(20)","089344b9":"top_cats = df_train.item_category_name.value_counts()[:15]\n\nplt.figure(figsize=(15,20))\n\nplt.subplot(311)\ng1 = sns.countplot(x='item_category_name', \n                   data=df_train[df_train.item_category_name.isin(top_cats.index)])\ng1.set_xticklabels(g1.get_xticklabels(),rotation=70)\ng1.set_title(\"TOP 15 Principal Products Sold\", fontsize=22)\ng1.set_xlabel(\"\")\ng1.set_ylabel(\"Count\", fontsize=18)\n\nplt.subplot(312)\ng2 = sns.boxplot(x='item_category_name', y='item_cnt_day', \n                   data=df_train[df_train.item_category_name.isin(top_cats.index)])\ng2.set_xticklabels(g2.get_xticklabels(),rotation=70)\ng2.set_title(\"Principal Categories by Item Solds Log\", fontsize=22)\ng2.set_xlabel(\"\")\ng2.set_ylabel(\"Items Sold Log Distribution\", fontsize=18)\n\nplt.subplot(313)\ng3 = sns.boxplot(x='item_category_name', y='total_amount', \n                   data=df_train[df_train.item_category_name.isin(top_cats.index)])\ng3.set_xticklabels(g3.get_xticklabels(),rotation=70)\ng3.set_title(\"Category Name by Total Amount Log\", fontsize=22)\ng3.set_xlabel(\"\")\ng3.set_ylabel(\"Total Amount Log\", fontsize=18)\n\nplt.subplots_adjust(wspace = 0.2, hspace = 0.8,top = 0.9)\nplt.show()","98592ca9":"sub_categorys_5000 = df_train.sort_values('total_amount',\n                                          ascending=False)[['item_category_name', 'item_name', \n                                                            'shop_name',\n                                                            'item_cnt_day','item_price',\n                                                            'total_amount']].head(5000)\nsub_categorys_5000.head(10)","f0176838":"sub_categorys_5000.describe(include='all')","599c0b85":"print(\"TOTAL REPRESENTATION OF TOP 5k Most Expensive orders: \", \n      f'{round((sub_categorys_5000.item_price.sum() \/ df_train.item_price.sum()) * 100, 2)}%')\n","42c631b3":"plt.figure(figsize=(14,26))\n\nplt.subplot(311)\ng = sns.countplot(x='shop_name', data=sub_categorys_5000)\ng.set_xticklabels(g.get_xticklabels(),rotation=90)\ng.set_title(\"Count of Shop Names in Top Bills \", fontsize=22)\ng.set_xlabel('Shop Names', fontsize=18)\ng.set_ylabel(\"Total Count in expensive bills\", fontsize=18)\n\nplt.subplot(312)\ng = sns.countplot(x='item_category_name', data=sub_categorys_5000)\ng.set_xticklabels(g.get_xticklabels(),rotation=90)\ng.set_title(\"Count of Category Name in Top Bills\", fontsize=22)\ng.set_xlabel('Category Names', fontsize=18)\ng.set_ylabel(\"Total Count in expensive bills\", fontsize=18)\n\nplt.subplot(313)\ng = sns.boxenplot(x='item_category_name', y='item_cnt_day', data=sub_categorys_5000)\ng.set_xticklabels(g.get_xticklabels(),rotation=90)\ng.set_title(\"Count of Category Name in Top Bills by Total items Sold\", fontsize=22)\ng.set_xlabel('Most top Category Names', fontsize=18)\ng.set_ylabel(\"Total sold distribution\", fontsize=18)\n\nplt.subplots_adjust(wspace = 0.2, hspace = 1.6,top = 0.9)\n\nplt.show()","50b73234":"df_train[['item_category_name', 'item_name', 'shop_name', 'item_cnt_day', 'item_price']].nlargest(15, 'item_price')","c5af7bad":"print(f\"Total of solds that have only one unit: {round(len(df_train[df_train.item_cnt_day == 1]) \/ len(df_train) * 100,2)}%\")","43f3a525":"cross_heatmap(sub_categorys_5000, ['item_category_name', 'item_cnt_day'])","6740bdc3":"count_item = df_train.item_name.value_counts()[:25]\n\nplt.figure(figsize=(14,50))\n\nplt.subplot(311)\ng = sns.countplot(x='item_name', data=df_train[df_train.item_name.isin(count_item.index)])\ng.set_xticklabels(g.get_xticklabels(),rotation=90)\ng.set_title(\"Count of Most sold Items\", fontsize=22)\ng.set_xlabel('', fontsize=18)\ng.set_ylabel(\"Total Count of \", fontsize=18)\n\nplt.subplot(312)\ng1 = sns.boxenplot(x='item_name', y='total_amount',\n                  data=df_train[df_train.item_name.isin(count_item.index)])\ng1.set_xticklabels(g1.get_xticklabels(),rotation=90)\ng1.set_title(\"Count of Category Name in Top Bills\", fontsize=22)\ng1.set_xlabel('', fontsize=18)\ng1.set_ylabel(\"Total Count in expensive bills\", fontsize=18)\n\nplt.subplot(313)\ng2 = sns.boxenplot(x='item_name', y='item_cnt_day', \n                  data=df_train[df_train.item_name.isin(count_item.index)])\ng2.set_xticklabels(g2.get_xticklabels(),rotation=90)\ng2.set_title(\"Items Sold Distribution\", fontsize=22)\ng2.set_xlabel('Item Names', fontsize=18)\ng2.set_ylabel(\"Total sold distribution\", fontsize=18)\n\nplt.subplots_adjust(wspace = 0.2, hspace = 1.2,top = .8)\n\nplt.show()","fd707806":"cross_heatmap(df_train.sample(500000), ['item_category_name', 'shop_name'], \n              normalize='columns', aggfunc='sum', values=df_train['total_amount'])","166da0bf":"# Calling the function to transform the date column in datetime pandas object\ndf_train = date_process(df_train)\n\n#seting some static color options\ncolor_op = ['#5527A0', '#BB93D7', '#834CF7', '#6C941E', '#93EAEA', '#7425FF', '#F2098A', '#7E87AC', \n            '#EBE36F', '#7FD394', '#49C35D', '#3058EE', '#44FDCF', '#A38F85', '#C4CEE0', '#B63A05', \n            '#4856BF', '#F0DB1B', '#9FDBD9', '#B123AC']\n\n\ndates_temp = df_train['date'].value_counts().to_frame().reset_index().sort_values('index') \n# renaming the columns to apropriate names\ndates_temp = dates_temp.rename(columns = {\"date\" : \"Total_Bills\"}).rename(columns = {\"index\" : \"date\"})\n\n# creating the first trace with the necessary parameters\ntrace = go.Scatter(x=dates_temp.date.astype(str), y=dates_temp.Total_Bills,\n                    opacity = 0.8, line = dict(color = color_op[7]), name= 'Total tickets')\n\n# Below we will get the total amount sold\ndates_temp_sum = df_train.groupby('date')['item_price'].sum().to_frame().reset_index()\n\n# using the new dates_temp_sum we will create the second trace\ntrace1 = go.Scatter(x=dates_temp_sum.date.astype(str), line = dict(color = color_op[1]), name=\"Total Amount\",\n                        y=dates_temp_sum['item_price'], opacity = 0.8)\n\n# Getting the total values by Transactions by each date\ndates_temp_count = df_train[df_train['item_cnt_day'] > 0].groupby('date')['item_cnt_day'].sum().to_frame().reset_index()\n\n# using the new dates_temp_count we will create the third trace\ntrace2 = go.Scatter(x=dates_temp_count.date.astype(str), line = dict(color = color_op[5]), name=\"Total Items Sold\",\n                        y=dates_temp_count['item_cnt_day'], opacity = 0.8)\n\n#creating the layout the will allow us to give an title and \n# give us some interesting options to handle with the outputs of graphs\nlayout = dict(\n    title= \"Informations by Date\",\n    xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1, label='1m', step='month', stepmode='backward'),\n                dict(count=3, label='3m', step='month', stepmode='backward'),\n                dict(count=6, label='6m', step='month', stepmode='backward'),\n                dict(step='all')\n            ])\n        ),\n        rangeslider=dict(visible = True),\n        type='date'\n    )\n)\n\n# creating figure with the both traces and layout\nfig = dict(data= [trace, trace1, trace2], layout=layout)\n\n#rendering the graphs\niplot(fig) #it's an equivalent to plt.show()","216ed3d8":"\n\ndef generate_random_color():\n    r = lambda: random.randint(0,255)\n    return '#%02X%02X%02X' % (r(),r(),r())\n\n#shared Xaxis parameter can make this graph look even better\nfig = tls.make_subplots(rows=2, cols=1)\n\nlayout1 = cf.Layout(\n    height=500,\n    width=200\n)\nanimal_color = generate_random_color()\nfig1 = df_train.groupby(['_month'])['item_cnt_day'].count().iplot(kind='bar',barmode='stack',\n                                                                  asFigure=True,showlegend=False,\n                                                                  title='Total Items Sold By Month',\n                                                                  xTitle='Months', yTitle='Total Items Sold',\n                                                                  color = 'blue')\nfig1['data'][0]['showlegend'] = False\nfig.append_trace(fig1['data'][0], 1, 1)\n\n\nfig2 = df_train.groupby(['_month'])['item_cnt_day'].sum().iplot(kind='bar',barmode='stack',\n                                                                title='Total orders by Month',\n                                                                xTitle='Months', yTitle='Total Orders',\n                                                                asFigure=True, showlegend=False, \n                                                                color = 'blue')\n\n#if we do not use the below line there will be two legend\nfig2['data'][0]['showlegend'] = False\n\n\nfig.append_trace(fig2['data'][0], 2, 1)\n\nlayout = dict(\n    title= \"Informations by Date\",\n    )\n\nfig['layout']['height'] = 800\nfig['layout']['width'] = 1000\nfig['layout']['title'] = \"TOTAL ORDERS AND TOTAL ITEMS BY MONTHS\"\nfig['layout']['yaxis']['title'] = \"Total Items Sold\"\nfig['layout']['xaxis']['title'] = \"Months\"\nfig['layout']\n\niplot(fig)","62d34086":"df_train['diff_days'] = df_train.groupby(['shop_name','item_category_name']).date.diff().dt.days.fillna(0, downcast='infer')","17712d7e":"df_train['diff_days'].hist(bins=50)","ac0db119":"grouped_blocks = df_train.groupby([\"date_block_num\",\n                                    \"shop_name\",\"item_category_name\"])[\"item_name\",\n                                                                       \"item_price\",\n                                                                       \"item_cnt_day\"].agg({\"item_name\":[\"nunique\", 'count'],\n                                                                                            \"item_price\":[\"min\",'mean','max'],\n                                                                                            \"item_cnt_day\":[\"min\",'mean','max','sum']})","30e0a5b2":"grouped_blocks.head(15)","3fe6bbf7":"chi2_test('shop_name')","638abb89":"chi2_test('item_category_name')","70b175e9":"chi2_test('item_price')","16e2b6ab":"## Deleting the datasets that was used to explore the data\ndel df_train\ndel df_test\n\ngc.collect()\n\n## Importing the df's again to modelling\ndf_train = pd.read_csv('..\/input\/sales_train.csv')\ndf_test = pd.read_csv(\"..\/input\/test.csv\")","8fd19ea0":"df_train = df_train[df_train.item_price<100000]\ndf_train = df_train[df_train.item_cnt_day<1001]","a0d7f2f7":"median = df_train[(df_train.shop_id==32)&(df_train.item_id==2973)&(df_train.date_block_num==4)&(df_train.item_price>0)].item_price.median()\ndf_train.loc[df_train.item_price<0, 'item_price'] = median","25c6232e":"df_train.loc[df_train.shop_id == 0, 'shop_id'] = 57\ndf_test.loc[df_test.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\ndf_train.loc[df_train.shop_id == 1, 'shop_id'] = 58\ndf_test.loc[df_test.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ndf_train.loc[df_train.shop_id == 10, 'shop_id'] = 11\ndf_test.loc[df_test.shop_id == 10, 'shop_id'] = 11","e6f00249":"from sklearn.preprocessing import LabelEncoder\n\ndf_shops.loc[df_shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\ndf_shops['city'] = df_shops['shop_name'].str.split(' ').map(lambda x: x[0])\ndf_shops.loc[df_shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\ndf_shops['city_code'] = LabelEncoder().fit_transform(df_shops['city'])\ndf_shops = df_shops[['shop_id','city_code']]\n\ndf_categories['split'] = df_categories['item_category_name'].str.split('-')\ndf_categories['type'] = df_categories['split'].map(lambda x: x[0].strip())\ndf_categories['type_code'] = LabelEncoder().fit_transform(df_categories['type'])\n# if subtype is nan then type\ndf_categories['subtype'] = df_categories['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ndf_categories['subtype_code'] = LabelEncoder().fit_transform(df_categories['subtype'])\ndf_categories = df_categories[['item_category_id','type_code', 'subtype_code']]\n\ndf_items.drop(['item_name'], axis=1, inplace=True)","8c8c477c":"import time\n\n# Creating the Matrix\nmatrix = []\n\n# Column names\ncols = ['date_block_num','shop_id','item_id']\n\n# Creating the pairwise for each date_num_block\nfor i in range(34):\n    # Filtering sales by each month\n    sales = df_train[df_train.date_block_num==i]\n    # Creating the matrix date_block, shop_id, item_id\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \n# Seting the matrix to dataframe\nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\n\n# Seting the features to int8 to reduce memory usage\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\n\nmatrix.sort_values(cols,inplace=True)","6599e710":"# Creating the revenue column\ndf_train['revenue'] = df_train['item_price'] *  df_train['item_cnt_day']","44433b17":"# getting the total itens sold by each date_block for each shop_id and item_id pairs\ngroup = df_train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\n# Renaming columns\ngroup.columns = ['item_cnt_month']\n# Reset the index \ngroup.reset_index(inplace=True)\n\n# Merging the grouped column to our matrix\nmatrix = pd.merge(matrix, group, on=cols, how='left')\n# Filling Na's and clipping the values to have range 0,20\n# seting to float16 to reduce memory usage\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float16))\n\nmatrix.head()","cc843630":"gc.collect()","62bcbf2e":"# Creating the date_block in df_test\ndf_test['date_block_num'] = 34\n\n# Seting the df test columns to int8 to reduce memory usage\ndf_test['date_block_num'] = df_test['date_block_num'].astype(np.int8)\ndf_test['shop_id'] = df_test['shop_id'].astype(np.int8)\ndf_test['item_id'] = df_test['item_id'].astype(np.int16)","f9784ed4":"## Concatenating the df test into our matrix and filling Na's with zero\nmatrix = pd.concat([matrix, df_test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) # 34 month","caa0023c":"# merging the df shops, df items, and df categories in our matrix\nmatrix = pd.merge(matrix, df_shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, df_items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, df_categories, on=['item_category_id'], how='left')\n\n# Seting the new columns to int8 to reduce memory\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\n\nmatrix.head()","4f514c70":"# Function to calculate lag features\ndef lag_feature(df, lags, col):\n    # Columns to get lag\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    # loop for each lag value in the list\n    for i in lags:\n        # Coping the df\n        shifted = tmp.copy()\n        # Creating the lag column\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        # getting the correct date_num_block to calculation\n        shifted['date_block_num'] += i\n        # merging the new column into the matrix\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n        \n    return df","36439268":"# Creating the lag columns \nmatrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')\n\nmatrix.head()","a187a4a3":"# Getting the mean item_cnt_month by date_bock\ngroup = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\n# Renaming\ngroup.columns = [ 'date_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\n# Merging the grouped object into the matrix\nmatrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\n# creating the lag column to average itens solds\nmatrix = lag_feature(matrix, [1], 'date_avg_item_cnt')\n# Droping the date_avg_item_cnt\nmatrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)\n\nmatrix.head()","d987af4c":"## Getting the mean item solds by date_blocks and item_id \ngroup = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\n# Renaming column\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\n# Merging into matrix\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\n\n# Geting the lag feature to the new column\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_item_avg_item_cnt')\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\n\nmatrix.head()","8c36475d":"# Grouping the mean items sold by shop id for each date_block\ngroup = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\n# Renaming Columns\ngroup.columns = [ 'date_shop_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\n# Merging the grouped into matrix\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\n\n# Geting the lag of the new column\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_shop_avg_item_cnt')\nmatrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)\n\nmatrix.head()","7a814332":"## Getting the mean items sold by item_category_id for each date_block_num\ngroup = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\n\n# Renaming column\ngroup.columns = [ 'date_cat_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\n# Merging the new grouped object into matrix\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\n\n# Getting the lag of the new column\nmatrix = lag_feature(matrix, [1], 'date_cat_avg_item_cnt')\nmatrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)\n\nmatrix.head()","85c1dd3f":"## Getting the mean items sold by shop_id and item_category_id for each date_block_num\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_cat_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\n# Merging the new grouped object into matrix\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nmatrix['date_shop_cat_avg_item_cnt'] = matrix['date_shop_cat_avg_item_cnt'].astype(np.float16)\n\n# Getting the lag of the new column\nmatrix = lag_feature(matrix, [1], 'date_shop_cat_avg_item_cnt')\nmatrix.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)\n\nmatrix.head()","b2add194":"## Getting the mean items sold by shop_id and item_category_id for each date_block_num\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_type_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\n# Merging the new grouped object into matrix\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'type_code'], how='left')\nmatrix['date_shop_type_avg_item_cnt'] = matrix['date_shop_type_avg_item_cnt'].astype(np.float16)\n\n# Getting the lag of the new column\nmatrix = lag_feature(matrix, [1], 'date_shop_type_avg_item_cnt')\nmatrix.drop(['date_shop_type_avg_item_cnt'], axis=1, inplace=True)\n\nmatrix.head()","fcabbbd8":"## Getting the mean items sold by shop_id and subtype_code for each date_block_num\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\n# Merging the new grouped object into matrix\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\nmatrix['date_shop_subtype_avg_item_cnt'] = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\n\n# Getting the lag of the new column\nmatrix = lag_feature(matrix, [1], 'date_shop_subtype_avg_item_cnt')\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\n\nmatrix.head()","007b4657":"## Getting the mean items sold by city_code for each date_block_num\ngroup = matrix.groupby(['date_block_num', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\n# Merging the new grouped object into matrix\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'city_code'], how='left')\nmatrix['date_city_avg_item_cnt'] = matrix['date_city_avg_item_cnt'].astype(np.float16)\n\n# Getting the lag of the new column\nmatrix = lag_feature(matrix, [1], 'date_city_avg_item_cnt')\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\n\nmatrix.head()","c13cf76b":"## Getting the mean items sold by item_id and city_code for each date_block_num\ngroup = matrix.groupby(['date_block_num', 'item_id', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\n# Merging the new grouped object into matrix\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'city_code'], how='left')\nmatrix['date_item_city_avg_item_cnt'] = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\n\n# Getting the lag of the new column\nmatrix = lag_feature(matrix, [1], 'date_item_city_avg_item_cnt')\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\n\nmatrix.head()","0bddc003":"## Getting the mean items sold by type_code for each date_block_num\ngroup = matrix.groupby(['date_block_num', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_type_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\n# Merging the new grouped object into matrix\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'type_code'], how='left')\nmatrix['date_type_avg_item_cnt'] = matrix['date_type_avg_item_cnt'].astype(np.float16)\n\n# Getting the lag of the new column\nmatrix = lag_feature(matrix, [1], 'date_type_avg_item_cnt')\nmatrix.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)\n\nmatrix.head()","fce5e6a8":"## Getting the mean items sold by subtype_code for each date_block_num\ngroup = matrix.groupby(['date_block_num', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_subtype_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\n# Merging the new grouped object into matrix\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'subtype_code'], how='left')\n\nmatrix['date_subtype_avg_item_cnt'] = matrix['date_subtype_avg_item_cnt'].astype(np.float16)\n\n# Getting the lag of the new column\nmatrix = lag_feature(matrix, [1], 'date_subtype_avg_item_cnt')\nmatrix.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)\n\nmatrix.head()","6eebdb80":"del df_items\ndel df_shops\ndel df_categories","e5d2eefd":"matrix = reduce_mem_usage(matrix)","67839b0b":"# Getting the mean item price by item_id\ngroup = df_train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\n# Merging the new grouped object into matrix\nmatrix = pd.merge(matrix, group, on=['item_id'], how='left')\n\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n\n## Getting the mean item price by item_id for each date_block_num\ngroup = df_train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\n# Merging the new grouped object into matrix\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n\n# Geting the lags of date item avg item price\nlags = [1,2,3,4,5,6]\nmatrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n\n# seting the delta price lag for each lag price\nfor i in lags:\n    matrix['delta_price_lag_'+str(i)] = \\\n        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) \/ matrix['item_avg_item_price']\n\n# Selecting trends\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \n# Applying the trend selection\nmatrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\nmatrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\nmatrix['delta_price_lag'].fillna(0, inplace=True)\n\n# https:\/\/stackoverflow.com\/questions\/31828240\/first-non-null-value-per-row-from-a-list-of-pandas-columns\/31828559\n# matrix['price_trend'] = matrix[['delta_price_lag_1','delta_price_lag_2','delta_price_lag_3']].bfill(axis=1).iloc[:, 0]\n# Invalid dtype for backfill_2d [float16]\n\n# Getting the features to drop\nfetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\nmatrix.drop(fetures_to_drop, axis=1, inplace=True)\ngc.collect()\n\nmatrix.head()","4e9e2b3e":"# Getting the revenue sum by shop_id and date_block\ngroup = df_train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\n# Merging the new group into matrix\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\n# Getting the mean item price by item_id\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['shop_id'], how='left')\nmatrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n\nmatrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) \/ matrix['shop_avg_revenue']\nmatrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1], 'delta_revenue')\n\nmatrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)","b1300c1e":"matrix['month'] = matrix['date_block_num'] % 12","0044aade":"days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nmatrix['days'] = matrix['month'].map(days).astype(np.int8)","8701f086":"\ncache = {}\nmatrix['item_shop_last_sale'] = -1\nmatrix['item_shop_last_sale'] = matrix['item_shop_last_sale'].astype(np.int8)\nfor idx, row in matrix.iterrows():    \n    key = str(row.item_id)+' '+str(row.shop_id)\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        matrix.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n        cache[key] = row.date_block_num         \n","c476cd6b":"cache = {}\nmatrix['item_last_sale'] = -1\nmatrix['item_last_sale'] = matrix['item_last_sale'].astype(np.int8)\nfor idx, row in matrix.iterrows():    \n    key = row.item_id\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        if row.date_block_num>last_date_block_num:\n            matrix.at[idx, 'item_last_sale'] = row.date_block_num - last_date_block_num\n            cache[key] = row.date_block_num","98bb99d7":"matrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nmatrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')","53a02114":"matrix = matrix[matrix.date_block_num > 11]","19a57664":"def fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nmatrix = fill_na(matrix)","05d58079":"matrix.info()","e5fd711f":"matrix.to_pickle('data.pkl')\n\ndel matrix\ndel cache\ndel group\ndel df_train\n\n# leave test for submission\ngc.collect();","303cafc9":"data = pd.read_pickle('data.pkl')","090cb8bb":"data = data[['date_block_num', 'shop_id', 'item_id', 'item_cnt_month', 'city_code', 'item_category_id',\n             'type_code', 'subtype_code', 'item_cnt_month_lag_1', 'item_cnt_month_lag_2',\n             'item_cnt_month_lag_3', 'item_cnt_month_lag_6', 'item_cnt_month_lag_12',\n             'date_avg_item_cnt_lag_1', 'date_item_avg_item_cnt_lag_1', 'date_item_avg_item_cnt_lag_2',\n             'date_item_avg_item_cnt_lag_3', 'date_item_avg_item_cnt_lag_6', 'date_item_avg_item_cnt_lag_12',\n             'date_shop_avg_item_cnt_lag_1', 'date_shop_avg_item_cnt_lag_2', 'date_shop_avg_item_cnt_lag_3',\n             'date_shop_avg_item_cnt_lag_6', 'date_shop_avg_item_cnt_lag_12', 'date_cat_avg_item_cnt_lag_1',\n             'date_shop_cat_avg_item_cnt_lag_1', 'item_shop_first_sale', 'item_first_sale',\n             #'date_shop_type_avg_item_cnt_lag_1','date_shop_subtype_avg_item_cnt_lag_1',\n             'date_city_avg_item_cnt_lag_1', 'date_item_city_avg_item_cnt_lag_1',\n             #'date_type_avg_item_cnt_lag_1', #'date_subtype_avg_item_cnt_lag_1',\n             'delta_price_lag', 'month', 'days', 'item_shop_last_sale', 'item_last_sale']]","d39faa85":"X_train = data[data.date_block_num < 34].drop(['item_cnt_month'], axis=1)\n#Y_train = train_set['item_cnt']\ny_train = data[data.date_block_num < 34]['item_cnt_month'].clip(0.,20.)\n\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n\nX_val = X_train[X_train.date_block_num > 30]\nX_train = X_train[X_train.date_block_num <= 30]\n\ny_val = y_train[~y_train.index.isin(X_train.index)]\ny_train = y_train[y_train.index.isin(X_train.index)]\n\nX_val_test = X_val[X_val.date_block_num > 32]\nX_val = X_val[X_val.date_block_num <= 32]\n\ny_val_test = y_val[~y_val.index.isin(X_val.index)]\ny_val = y_val[y_val.index.isin(X_val.index)]\n\nX_train.head()","bed5409b":"del data\ngc.collect()","6ddc79ae":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nthresh = 5 * 10**(-4)\nmodel = GradientBoostingRegressor()\nmodel.fit(X_train, y_train)\n#select features using threshold\nselection = SelectFromModel(model, threshold=thresh, prefit=True)\n\nX_train = selection.transform(X_train)\nX_val = selection.transform(X_val)\nX_val_test = selection.transform(X_val_test)\nX_test = selection.transform(X_test)\n\ndel model, selection\n\ngc.collect()","9cb34869":"print(\"X_important_train Shape: \", X_train.shape)\nprint(\"X_important_val Shape: \", X_val.shape)\nprint(\"X_important_val_test Shape: \", X_val_test.shape)\nprint(\"X_important_test Shape: \", X_test.shape)","472a3d23":"# Define searched space\nhyper_space = {'objective': 'regression',\n               'metric':'rmse',\n               'boosting':'gbdt',\n               #'n_estimators': hp.choice('n_estimators', [25, 40, 50, 75, 100, 250, 500]),\n               'max_depth':  hp.choice('max_depth', [3, 5, 8, 10, 12, 15]),\n               'num_leaves': hp.choice('num_leaves', [25, 50, 75, 100, 125, 150, 225, 250, 350, 400, 500]),\n               'subsample': hp.choice('subsample', [.3, .5, .7, .8, .9, 1]),\n               'colsample_bytree': hp.choice('colsample_bytree', [.5, .6, .7, .8, .9, 1]),\n               'learning_rate': hp.choice('learning_rate', [.01, .1, .05, .2]),\n               'reg_alpha': hp.choice('reg_alpha', [.1, .2, .3, .4, .5, .6, .7]),\n               'reg_lambda':  hp.choice('reg_lambda', [.1, .2, .3, .4, .5, .6]), \n                # 'bagging_fraction': hp.choice('bagging_fraction', [.5, .6, .7, .8, .9, 1]),\n               'feature_fraction':  hp.choice('feature_fraction', [.6, .7, .8, .9, 1]), \n               'bagging_frequency':  hp.choice('bagging_frequency', [.3, .4, .5, .6, .7, .8, .9]),                  \n               'min_child_samples': hp.choice('min_child_samples', [10, 20, 30, 40])}","7a0d84f4":"def rmse(y_pred, y):\n    return np.sqrt(np.mean(np.square(y - y_pred)))","255e2720":"lgtrain = lightgbm.Dataset(X_train, label=y_train)\nlgval = lightgbm.Dataset(X_val, label=y_val)\n\ndef evaluate_metric(params):\n    \n    model_lgb = lightgbm.train(params, lgtrain, 600, \n                          valid_sets=[lgtrain, lgval], early_stopping_rounds=100, \n                          verbose_eval=300)\n\n    pred = model_lgb.predict(X_val_test, num_iteration=1000)\n\n    score = rmse(pred, y_val_test)\n    \n    print(score, params)\n \n    return {\n        'loss': score,\n        'status': STATUS_OK,\n        'stats_running': STATUS_RUNNING\n    }","cd210362":"# Trail\ntrials = Trials()\n\n# Set algoritm parameters\nalgo = partial(tpe.suggest, \n               n_startup_jobs=-1)\n\n# Seting the number of Evals\nMAX_EVALS= 30\n\n# Fit Tree Parzen Estimator\nbest_vals = fmin(evaluate_metric, space=hyper_space, verbose=1,\n                 algo=algo, max_evals=MAX_EVALS, trials=trials)\n\n# Print best parameters\nbest_params = space_eval(hyper_space, best_vals)","a4e574e0":"print(\"BEST PARAMETERS: \" + str(best_params))","f9ef8c61":"model_lgb = lightgbm.train(best_params, lgtrain, 5000, \n                      valid_sets=[lgtrain, lgval], early_stopping_rounds=500, \n                      verbose_eval=250)\n\nlgb_pred = model_lgb.predict(X_test).clip(0, 20)","3e992aba":"lgb_pred = model_lgb.predict(X_test).clip(0, 20)\n\n# rmse(lgb_pred, y_val_test)","1614af59":"result = pd.DataFrame({\n    \"ID\": df_test[\"ID\"],\n    \"item_cnt_month\": lgb_pred.clip(0. ,20.)\n})\nresult.to_csv(\"submission.csv\", index=False)","033c67d5":"## Some questions to guide the initial exploration:\n- Data shape, missings, first rows, \n- What are the entropy of each column\n- what are the principal shops? \n- What are the distributions of items price and total items sold by each item;\n- What are the more sold items and which are their categorys;\n- What's the range of date sales;\n- How many items was sold by each day. Could we see any peak in christhmas, valentine's day or another special day.\n- Crossing some of this features \n- And a lot of more questions that probably will raise through the exploration.","47ff003d":"Some of this values has just one unit sold, and I will see it now","0d066d1f":"## The Item Solds by Shop Names","795a7122":"## Setting the X_test","e75a3183":"## I'm working on this kernel, so stay tuned and votesup the kernel =) ","4aa44587":"Loking the price of PS4 I can infer that it's not about dollars.<br>\nLet's see the distribution of our features in the subsample;","90202556":"test_set = df_test.copy()\ntest_set['date_block_num'] = 34\n\ntest_set = pd.merge(test_set, test_price_a, on=['shop_id','item_id'], how='left')\ntest_set = mergeFeature(test_set)\n\ntest_set['item_order'] = test_set['order_prev']\ntest_set.loc[test_set['item_order'] == 0, 'item_order'] = 1\n\nX_test = test_set.drop(['ID'], axis=1)\nX_test.head()\n\nassert(X_train.columns.isin(X_test.columns).all())","8ffb9d18":"## Items category\n- Let's see some distributions of the top values in our data","5e4ed9f3":"# Exploring and Predicting Sales\n\n## Descrition of this competition:\nThis challenge serves as final project for the \"How to win a data science competition\" Coursera course.\n\nIn this competition you will work with a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company. \n\nWe are asking you to predict total sales for every product and store in the next month. By solving this competition you will be able to apply and enhance your data science skills.\n\n<b>Data fields<\/b><br>\n<b>ID<\/b> - an Id that represents a (Shop, Item) tuple within the test set<br>\n<b>shop_id<\/b> - unique identifier of a shop<br>\n<b>item_id<\/b> - unique identifier of a product<br>\n<b>item_category_id<\/b> - unique identifier of item category<br>\n<b>item_cnt_day<\/b> - number of products sold. You are predicting a monthly amount of this measure<br>\n<b>item_price<\/b> - current price of an item<br>\n<b>date<\/b> - date in format dd\/mm\/yyyy<br>\n<b>date_block_num<\/b> - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33<br>\n<b>item_name<\/b> - name of item<br>\n<b>shop_name<\/b> - name of shop<br>\n<b>item_category_name<\/b> - name of item category","5a1ad301":"Some references:<br>\nhttps:\/\/www.kaggle.com\/plasticgrammer\/future-sales-prediction-playground <br>\nhttps:\/\/www.kaggle.com\/jatinmittal0001\/predict-future-sales-part-2<br>\nhttps:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost<br>","45759d51":"Very cool!! We can see that the most expensive item Price is 308k and the highest item sold a","2adf2dd1":"- Interesting... Almost all of items sold are 1.\nLet's take some descriptions about the quantiles","551620dd":"## The most expensive products","ff62d651":"## Joining the tables to our train dataset","26fd83c2":"It's very meaningfull. <br>\nwe can see what are the most sold items and the distribution of solds. <br>\nI will see the prices further and the possibility to cross our data","7288b37a":"## Category's by items sold by day - Crosstab","c3921731":"## Taking a look at the highest purchases in the data\n- First I will create a subsample to see the outliers\n- I will get top 5K highest total amounts and see if we can have some insight","337591c0":"Hummm... We have a high GAP between the most expensive item to other items. \nI will see how many of total are of 1 unit item sold\n","96f9c348":"- HO is that the feature hasn't influence in item_cnt_day\n- H1 is that the feature has some influence in item_cnt_day","ee6806c5":"## Preprocessing and spliting the dataset","235f5773":"Wow, it's russian !!! I will try to do someting to handle with the data easiest. ","fff35a7f":"- For the modelling part I am using the codes of some another kernels that I will put as resources on the final of kernel\n\nTest Chi2\n- Let's see if the categorys are independent or dependent of the target","bea2bc7b":"## Mapping our dictionary\n- as the data are in russian, I decided to translate it to english. ","a19814fe":"English is not my first language, so sorry for any mistake.","639dc83f":"Very interesting patterns. We can see that some items aren't sold only one unit. Maybe some people buy to resell the games or consoles","5117521b":"## Shops and items categorys of the most expensive trades","d93c12be":"- We can see that item_price, shop_name and item category are important to explain the items sold","3c6b3a5a":"## Getting the best parameters","ac43fad2":"Altough it contains high values, the top 5k most expensive bills represents just 2.45% of total amount sold. <br>\nI think that it happens because are retails shops that sells to \"normal\" people and not other business\n","e8ba9920":"It's interesting to note that the difference in values aren't different as the total solds<br>\nthe difference betweeen Moscow Shopping Center and Moscow TRK aren't so different ","79213437":"## First look at our data","498759a3":"## Total revenue Representation of total sales","7dd0dbcb":"## First look at some informations of our data\n- to see the output click on \"show output\" button >>>","ca57c5a4":"## Looking the Total Amount sold by the Stores","10a4283f":"## Understanding the revenue for each Shop Name. ","9bbb6453":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\ny_train = data[data.date_block_num < 33]['item_cnt_month']\nX_val = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\ny_val = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","b50ad2a8":"## Quantiles of continuous features and target\n- I will also create a total amount column, that will be the price * qtd. sold","2802e46c":"## Understanding the sales by month","cdba547b":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\ny_train = data[data.date_block_num < 33]['item_cnt_month']\nX_val = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\ny_val = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","aa791d53":"# Descriptions of the top 5k most exepensive sales","bd1eea55":"## Trainning the model with best parameters and predicting the X_test to submission","4925444d":"## TOP 25 items Solds\n- Let's understand the principal itens sold at the dataset\n- The distribution of Total amount and Item Solds in the bill","9cb4f3d5":"Very cool and interesting values distribution.\nLet's investigate it further trought the other features and try to find some interesting patterns","3816f6b7":"We can see that one specific day had an peak in sales. I put it on google and I don't find anything about this date. ","e19a1882":"## I will start exploring our target (item_cnt_day) that refers to items sold and the item_price","b1e3461f":"## Time series\n- Exploring some metrics abuot the datetime feature","213cb182":"Cool. it's a well distributed market where no one has a great monopoly. <br>\nLet's keep understanding the Shop Names","ca079da7":"# Importing the Datasets","16f91d5b":"## NOTE: This kernel is under construction. \nIf you think that this kernel was useful for you, please votes up the kernel, and if you want see all codes, fork it. ","9ed034c4":"## Knowing the Shop, category and items columns","46314b34":"## Start modeling to ML "}}