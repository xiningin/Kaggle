{"cell_type":{"06b77925":"code","c45882f1":"code","afc3fc30":"code","365111bf":"code","f9c21cb3":"code","affa5caa":"code","48087fc3":"code","9523f0e3":"code","f2f9a36c":"code","f3d645c3":"code","a4966c42":"code","5dc826c9":"code","91184a18":"code","9d86301e":"code","d7ac391f":"code","9342c93c":"code","353bc074":"code","f1338e91":"code","8ac1b4dd":"code","835189a8":"code","abc04ea5":"code","1575538f":"code","b4e0ded4":"code","b974099c":"code","c74938a0":"code","13ae39b2":"code","7ed457dd":"code","4d262aa2":"code","5764e449":"code","be976322":"code","f6628ccf":"code","78e076ef":"code","ba92e437":"markdown","b6b1eeff":"markdown","ccd50850":"markdown","deedb097":"markdown","128d1cbf":"markdown","6d8feba1":"markdown","ac1de0e5":"markdown","bccd8e69":"markdown","ea2a322a":"markdown"},"source":{"06b77925":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf","c45882f1":"data = pd.read_csv('..\/input\/us-accidents\/US_Accidents_June20.csv', nrows=400000)","afc3fc30":"data","365111bf":"data.info()","f9c21cb3":"data.isna().mean()","affa5caa":"null_columns = ['End_Lat', 'End_Lng', 'Number', 'Wind_Chill(F)', 'Precipitation(in)']\n\ndata = data.drop(null_columns, axis=1)","48087fc3":"data.isna().sum()","9523f0e3":"data = data.dropna(axis=0).reset_index(drop=True)","f2f9a36c":"print(\"Total missing values:\", data.isna().sum().sum())","f3d645c3":"data","a4966c42":"{column: len(data[column].unique()) for column in data.columns if data.dtypes[column] == 'object'}","5dc826c9":"unneeded_columns = ['ID', 'Description', 'Street', 'City', 'Zipcode', 'Country']\n\ndata = data.drop(unneeded_columns, axis=1)","91184a18":"data","9d86301e":"def get_years(df, column):\n    return df[column].apply(lambda date: date[0:4])\n\ndef get_months(df, column):\n    return df[column].apply(lambda date: date[5:7])","d7ac391f":"data['Start_Time_Month'] = get_months(data, 'Start_Time')\ndata['Start_Time_Year'] = get_years(data, 'Start_Time')\n\ndata['End_Time_Month'] = get_months(data, 'End_Time')\ndata['End_Time_Year'] = get_years(data, 'End_Time')\n\ndata['Weather_Timestamp_Month'] = get_months(data, 'Weather_Timestamp')\ndata['Weather_Timestamp_Year'] = get_years(data, 'Weather_Timestamp')\n\n\ndata = data.drop(['Start_Time', 'End_Time', 'Weather_Timestamp'], axis=1)","9342c93c":"data","353bc074":"def onehot_encode(df, columns, prefixes):\n    df = df.copy()\n    for column, prefix in zip(columns, prefixes):\n        dummies = pd.get_dummies(df[column], prefix=prefix)\n        df = pd.concat([df, dummies], axis=1)\n        df = df.drop(column, axis=1)\n    return df","f1338e91":"{column: len(data[column].unique()) for column in data.columns if data.dtypes[column] == 'object'}","8ac1b4dd":"data = onehot_encode(\n    data,\n    columns=['Side', 'County', 'State', 'Timezone', 'Airport_Code', 'Wind_Direction', 'Weather_Condition'],\n    prefixes=['SI', 'CO', 'ST', 'TZ', 'AC', 'WD', 'WC']\n)","835189a8":"data","abc04ea5":"def get_binary_column(df, column):\n    if column == 'Source':\n        return df[column].apply(lambda x: 1 if x == 'MapQuest' else 0)\n    else:\n        return df[column].apply(lambda x: 1 if x == 'Day' else 0)","1575538f":"data['Source'] = get_binary_column(data, 'Source')\n\ndata['Sunrise_Sunset'] = get_binary_column(data, 'Sunrise_Sunset')\ndata['Civil_Twilight'] = get_binary_column(data, 'Civil_Twilight')\ndata['Nautical_Twilight'] = get_binary_column(data, 'Nautical_Twilight')\ndata['Astronomical_Twilight'] = get_binary_column(data, 'Astronomical_Twilight')","b4e0ded4":"data","b974099c":"y = data['Severity'].copy()\nX = data.drop('Severity', axis=1).copy()","c74938a0":"y.unique()","13ae39b2":"y = y - 1","7ed457dd":"X = X.astype(np.float)","4d262aa2":"scaler = StandardScaler()\n\nX = scaler.fit_transform(X)","5764e449":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=100)","be976322":"X.shape","f6628ccf":"inputs = tf.keras.Input(shape=(X.shape[1],))\nx = tf.keras.layers.Dense(64, activation='relu')(inputs)\nx = tf.keras.layers.Dense(64, activation='relu')(x)\noutputs = tf.keras.layers.Dense(4, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs, outputs)\n\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nbatch_size = 32\nepochs = 20\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=batch_size,\n    epochs=epochs,\n    callbacks=[\n        tf.keras.callbacks.ReduceLROnPlateau(),\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n    ]\n)","78e076ef":"print(\"Test Accuracy:\", model.evaluate(X_test, y_test, verbose=0)[1])","ba92e437":"# Splitting\/Scaling","b6b1eeff":"# Encoding","ccd50850":"# Training","deedb097":"# Task for Today  \n\n***\n\n## Automobile Accident Severity Prediction  \n\nGiven *data about accidents in the US*, let's try to predict the **severity** of a given accident.  \n  \nWe will use a TensorFlow ANN to make our predictions.","128d1cbf":"# Missing Values","6d8feba1":"# Getting Started","ac1de0e5":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/hB6Wx7HX0c4","bccd8e69":"# Unnecessary Columns","ea2a322a":"# Results"}}