{"cell_type":{"e6a6dcd5":"code","2b764c50":"code","92ce2e9d":"code","9a60b03d":"code","1998322d":"code","ce40932a":"code","b84fe3ab":"code","fa879690":"code","59fede92":"code","644a8818":"code","e5123a8f":"code","e3dfceec":"code","f7b696f4":"code","8a73f130":"code","dee7624a":"code","8ca5cb5d":"markdown","b6729aef":"markdown"},"source":{"e6a6dcd5":"from __future__ import print_function\n#from hdfs import InsecureClient\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport keras\nfrom keras import regularizers\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add,PReLU, LSTM\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Reshape, Flatten, Conv2D, MaxPooling2D\nfrom keras.optimizers import SGD\nfrom keras import backend as K\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\nimport tensorflow as tf\n#import horovod.keras as hvd","2b764c50":"# Horovod: initialize Horovod.\n#hvd.init()\n\n# Horovod: pin GPU to be used to process local rank (one GPU per process)\n#config = tf.ConfigProto()\n#config.gpu_options.allow_growth = True\n#config.gpu_options.visible_device_list = str(hvd.local_rank())\n#K.set_session(tf.Session(config=config))","92ce2e9d":"#datasets with Reconstruction Accuracy\n#new_merchant_saleslag3_pivot - 10%\n#new_merchant_subsector_pivot_time - 40%\n#new_merchant_state_pivot_time - 0%\n#new_merchant_monthlag_pivot - 0%\n#new_merchant_id_pivot_time - 72%\n#new_card_id_state_pivot - 2%\n#new_card_id_monthlag_pivot - 13%\n#new_card_id_installment_pivot - 4%\n#new_card_id_feat3_pivot - 55%\n#new_card_id_feat2_pivot - 55%\n#new_card_id_feat1_pivot - 84%\n#new_card_id_cat3_pivot - 43%\n#new_card_id_cat2_pivot - 59%\n#new_card_id_cat1_pivot - 99%\n\n\n#hist_merchant_subsector_pivot_time - 65%\n#hist_merchant_state_pivot_time - 05%\n#hist_merchant_saleslag3_pivot - 75%\n#hist_merchant_monthlag_pivot_score - 95%\n#hist_merchant_monthlag_pivot - 57%\n#hist_merchant_id_pivot_time - 92%\n#hist_merchant_state_pivot_time - 3%\n#hist_merchant_saleslag3_pivot - 75%\n#hist_merchant_monthlag_pivot_score - 94%\n#hist_merchant_monthlag_pivot - 33%\n#hist_month_lag_real_price - 11%","9a60b03d":"# load csv\ndf_train = pd.read_csv('..\/input\/elo-merchant-category-recommendation\/train.csv', index_col=['card_id'])\ndf_test = pd.read_csv('..\/input\/elo-merchant-category-recommendation\/test.csv')","1998322d":"####find outliers\ndf_train['outliers'] = 0\ndf_train.loc[(df_train['target'] < -2) | (df_train['target'] > 2), 'outliers'] = 1\ntarget = df_train['target']\ndf_train['outliers'].value_counts()","ce40932a":"df_train_columns = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','target']]\ndf_test_columns = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','target','outliers']]\n\ntarget = df_train['target']\n\ntrain_x, test_x = train_test_split(df_train[df_train_columns], test_size=0.2, random_state=420)\ntrain_x = train_x[train_x.outliers == 0] #where normal transactions\ntrain_x = train_x.drop(['outliers'], axis=1) #drop the class column\n\ntest_y = test_x['outliers'] #save the class column for the test set\ntest_x = test_x.drop(['outliers'], axis=1) #drop the class column\n\ntrain_x = train_x.values #transform to ndarray\ntest_x = test_x.values","b84fe3ab":"nb_epoch = 3\nbatch_size = 160\ninput_dim = train_x.shape[1] #num of columns, 30\nencoding_dim = 32\nhidden_dim = int(encoding_dim \/ 2) #i.e. 7\nlearning_rate = 0.001\n\ninput_layer = Input(shape=(input_dim, ))\nencoder = Dense(encoding_dim, activation=\"tanh\", activity_regularizer=regularizers.l1(learning_rate))(input_layer)\nencoder = Dense(hidden_dim, activation=\"relu\")(encoder)\ndecoder = Dense(hidden_dim, activation='tanh')(encoder)\ndecoder = Dense(input_dim, activation='relu')(decoder)\nautoencoder = Model(inputs=input_layer, outputs=decoder)\nautoencoder.summary()\n\n\nsgd = SGD(lr=0.001, momentum=0.02)\nautoencoder.compile(metrics=['accuracy'],\n                    loss='mean_squared_error',\n                    optimizer='sgd')\n\ncp = ModelCheckpoint(filepath=\"autoencoder_fraud.h5\",\n                               save_best_only=True,\n                               verbose=0)\n\ntb = TensorBoard(log_dir='.\/logs',\n                histogram_freq=0,\n                write_graph=True,\n                write_images=True)\n\n\n# Horovod: adjust learning rate based on number of GPUs.\n#opt = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0, nesterov=True)\n\n# Horovod: add Horovod Distributed Optimizer.\n#opt = hvd.DistributedOptimizer(opt)\n\nhistory = autoencoder.fit(train_x, train_x,\n                    epochs=nb_epoch,\n                    batch_size=batch_size,\n                    shuffle=True,\n                    validation_data=(test_x, test_x),\n                    verbose=1,\n                    callbacks=[cp, tb]).history","fa879690":"test_x_predictions = autoencoder.predict(test_x)\nmse = np.mean(np.power(test_x - test_x_predictions, 2), axis=1)\nerror_df = pd.DataFrame({'Reconstruction_error': mse,\n                        'True_class': test_y})\nerror_df.describe()","59fede92":"#error_df.sort_values(by='Reconstruction_error', ascending=False)","644a8818":"from sklearn.metrics import confusion_matrix\nLABELS = [\"Normal\",\"Unusual\"]\nthreshold_fixed = 0.3524\npred_y = [1 if e > threshold_fixed else 0 for e in error_df.Reconstruction_error.values]\nconf_matrix = confusion_matrix(error_df.True_class, pred_y)\n\nplt.figure(figsize=(12, 12))\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","e5123a8f":"mse=autoencoder.predict(df_train[df_test_columns])\ndf_train['anomoly'] = np.mean(np.power(df_train[df_test_columns] - mse, 2), axis=1)\ndf_train.anomoly.head(5)","e3dfceec":"mse=autoencoder.predict(df_test[df_test_columns])\ndf_test['anomoly'] = np.mean(np.power(df_test[df_test_columns] - mse, 2), axis=1)\ndf_test.anomoly.head(5)","f7b696f4":"ae_columns = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','target','outliers']]","8a73f130":"param = {'boosting': 'goss',\n         'objective': 'regression',\n         'metric': 'rmse',\n         'learning_rate': 0.01,\n         'subsample': 0.9855232997390695,\n         'max_depth': 7,\n         'top_rate': 0.9064148448434349,\n         'num_leaves': 63,\n         'min_child_weight': 41.9612869171337,\n         'other_rate': 0.0721768246018207,\n         'reg_alpha': 9.677537745007898,\n         'colsample_bytree': 0.5665320670155495,\n         'min_split_gain': 9.820197773625843,\n         'reg_lambda': 8.2532317400459,\n         'min_data_in_leaf': 21,\n         \"verbosity\": -1,\n#         \"device\" : \"gpu\",\n         \"n_jobs\" : -1}\n\nfolds = StratifiedKFold(n_splits=4, shuffle=True, random_state=4590)\noof = np.zeros(len(df_train))\npredictions = np.zeros(len(df_test))\nfeature_importance_df = pd.DataFrame()\nlabel_cols = [\"outliers\"]\ny_split = df_train[label_cols].values\n\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(y_split[:,0], y_split[:,0])):\n    print(\"fold {}\".format(fold_))\n    trn_data = lgb.Dataset(df_train.iloc[trn_idx][ae_columns], label=target.iloc[trn_idx])#, categorical_feature=categorical_feats)\n    val_data = lgb.Dataset(df_train.iloc[val_idx][ae_columns], label=target.iloc[val_idx])#, categorical_feature=categorical_feats)\n\n    num_round = 5000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 50)\n    oof[val_idx] = clf.predict(df_train.iloc[val_idx][ae_columns], num_iteration=clf.best_iteration)\n\n    \n    train_prediction = clf.predict(df_train[ae_columns] \/ folds.n_splits)\n    predictions += clf.predict(df_test[ae_columns] \/ folds.n_splits)\n\nnp.sqrt(mean_squared_error(oof, target))","dee7624a":"sub_df = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\nsub_df[\"target\"] = predictions\nsub_df.to_csv(\"submission_new.csv\", index=False)","8ca5cb5d":"Below you can see the title of the pivot and the accuracy from reconstruction","b6729aef":"I finally got around to building an autoencoder and thought that everyone would like to know what I was able to discover about the data.\n\n1. I made pivots on the data\n2. I built an autencoder\n3. I tracked the reconstruction accuracy pivots"}}