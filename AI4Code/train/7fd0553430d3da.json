{"cell_type":{"d0452fe1":"code","f6397774":"code","9cd175d2":"code","9f313055":"code","33d39887":"code","2e5f8880":"code","236cc8fa":"code","236f2541":"code","770b3a44":"code","3b36584a":"code","0b0bba06":"code","b5de429c":"markdown","932db53a":"markdown","bf239c44":"markdown","d2c144c8":"markdown","6149cc3f":"markdown","9e547261":"markdown"},"source":{"d0452fe1":"#We will work with the 'stories' table in the hacker news dataset\n#preview the first few lines of the table\nfrom google.cloud import bigquery\n\n# Create a \"Client\" object\nclient = bigquery.Client()\n# Construct a reference to the \"openaq\" dataset\ndataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n# Construct a reference to the \"stories\" table\ntable_ref = dataset_ref.table(\"stories\")\n# API request - fetch the table\ntable = client.get_table(table_ref)\n# # Preview the first five lines of the table\nclient.list_rows(table, max_results=5).to_dataframe()","f6397774":"#query the dataset for all titles that ended up with very high scores (over 250)\nquery = \"\"\"\n        SELECT score, title    \n        FROM `bigquery-public-data.hacker_news.stories`\n        WHERE score>250\n        \"\"\"\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(query, job_config=dry_run_config)\n\nprint(\"This query will process {:,} bytes.\".format(dry_run_query_job.total_bytes_processed))","9cd175d2":"#get the high scoring dataset and convert titles to a spacy object\nimport spacy\nhigh_scores = client.query(query).to_dataframe()\nnlp = spacy.load('en_core_web_lg')\n\ndocs = [nlp(doc) for doc in high_scores['title']]","9f313055":"from collections import defaultdict\nimport pandas as pd\n\nword_lib = defaultdict(list)#this dictionary will hold a list of occurances for each word\nnum_words = {}\n\nfor doc in docs:\n    #get the lemmastized words in the title and ignore top words\n    doc_words = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n    for word in doc_words:\n        #Add an occurance to the list for each word in the title\n        word_lib[word].append(1)\n\nfor key, value in word_lib.items():\n    num_words[key] = sum(value)#add up the occurances for each word\n\n#these are the 100 most popular words in the highest scoring titles. These will be our candidate words.\ncandidate_words = list(pd.Series(num_words).sort_values(ascending=False)[:100].index)\ncandidate_words","33d39887":"from spacy.matcher import PhraseMatcher\n\nquery = \"\"\"\n        SELECT score, title    \n        FROM `bigquery-public-data.hacker_news.stories`\n        \"\"\"\n#get all of the titles and scores in the dataset\nstories = client.query(query).to_dataframe()\n\n#drop entries with missing values, then select a random fraction of the data (due to speed and memory limitations)\ndata = stories.dropna().sample(frac=0.05)\n\n#convert titles to spacy\ndocs = [nlp(doc) for doc in data['title']]\n\n# Create a PhraseMatcher object. The tokenizer is the first argument. Use attr = 'LOWER' to make consistent capitalization\nmatcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n\n# Create a list of tokens for each candidate word\ntokens_list = [nlp(item) for item in candidate_words]\n\n# Add the candidate words to the matcher. \nmatcher.add(\"pop_words\",            # Just a name for the set of rules we're matching to\n           tokens_list)\n\nword_scores = defaultdict(list)#this will hold a list of scores for each word\navg_word_scores = {}\n\nfor idx, story in data.iterrows():\n    doc = nlp(story['title'])\n\n    matches = matcher(doc)\n    \n    # Create a set of the items found in the title\n    found_words = set([doc[match[1]:match[2]].lower_ for match in matches])\n    \n    # Update item_ratings with rating for each item in found_items\n    # Transform the item strings to lowercase to make it case insensitive\n    for word in found_words:\n        word_scores[word].append(story['score'])\n\nfrom statistics import median\ndef standard_dev(lst):#function to compute the standard deviation\n    mean = sum(lst)\/len(lst)\n    return (sum([(ele-mean)**2 for ele in lst])\/len(lst))**0.5\n\nfor key, value in word_scores.items():\n    avg_word_scores[key] = [median(value), standard_dev(value), len(value)]\n\npopular_words = pd.DataFrame.from_dict(avg_word_scores, orient='index', columns=['median_score', 'standard_dev', 'count'])\npopular_words.sort_values(by=['median_score', 'count'], ascending=False, inplace=True)","2e5f8880":"#Take a look at the 20 most popular words by mdeian score and number of occurances\npopular_words[:20]","236cc8fa":"from sklearn.svm import LinearSVC\nfrom sklearn.metrics import mean_squared_error, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.pipeline import make_pipeline\nimport numpy as np\n\n#vectorize each title (ie. convert the title to a vector in 300 dimensional space)\nwith nlp.disable_pipes():\n    doc_vectors = np.array([nlp_doc.vector for nlp_doc in docs])\n    \n#our target variable will be whether the post got more than 5 score. \ndata['popular'] = data['score'].map(lambda x: 1 if x>5 else 0)\n\nX_train, X_test, y_train, y_test = train_test_split(doc_vectors, data['popular'],\n                                                    test_size=0.2, random_state=20)\n\n#we use a support vector classifier as it works well in high dimensional space\nsvc_model = LinearSVC(random_state=0, tol=1e-5, max_iter=1e3, dual=False, C=100)\n\n#scale the vectors going into the model with a pipeline\nsvc = make_pipeline(MinMaxScaler(), \n                     svc_model\n                    )\n\nsvc.fit(X_train, y_train)","236f2541":"from sklearn.metrics import precision_recall_curve, roc_curve, auc\n\nprobas_pred = svc.decision_function(X_test)\nprecision, recall, thresholds = precision_recall_curve(y_test, probas_pred)\nfpr, tpr, thresholds_roc = roc_curve(y_test, probas_pred)\n\npos_ratio = y_test.sum()\/len(y_test)\nAUC = auc(fpr, tpr)\n\nimport matplotlib.pyplot as plt\nfig, axs = plt.subplots(1,2, figsize=(16,6))\n#Plot precision and recall vs threshold\naxs[0].set_title('Model Performance vs Classification Threshold')\naxs[0].plot([thresholds[0],thresholds[-1]], [pos_ratio, pos_ratio], linestyle='--', color='k')\naxs[0].plot(thresholds, precision[1:])\naxs[0].plot(thresholds, recall[1:])\naxs[0].set_xlabel('Threshold')\naxs[0].set_ylabel('Score')\n# axs[0].set_ylim([0, 0.4])\naxs[0].legend(['Precision - Random Classifier', 'Trained Model Precision', 'Trained Model Recall'], loc='best')\naxs[0].grid()\n#Plot ROC\nprops = dict(boxstyle='square', facecolor='white')\naxs[1].set_title('ROC')\naxs[1].text(1.25, 0.95, 'AUC={:.3}'.format(AUC), transform=axs[0].transAxes, fontsize=14,\n        verticalalignment='top', bbox=props)\naxs[1].plot(fpr, tpr)\naxs[1].plot([0,1], [0,1], linestyle='--', color='k')\naxs[1].set_xlabel('False Positive Rate')\naxs[1].set_ylabel('True Positive Rate')\naxs[1].legend(['Trained Model', 'Random Classifier'], loc='lower right')\naxs[1].grid()\nplt.show()","770b3a44":"#plot confusion matrix for chosen threshold. 1=popular, 0=not popular\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nthresh = -0.48\ny_pred = [1 if ele>thresh else 0 for ele in probas_pred]\n\ncm = confusion_matrix(y_test, y_pred)\nax = sns.heatmap(cm, annot=True, fmt=\"d\")\nax.set(ylabel='True Label', xlabel='Predicted Label')\nplt.show()","3b36584a":"#function to predict whether a set of titles will be popular\ndef pred(model, thresh, titles, nlp):\n    docs = [nlp(doc) for doc in titles]\n    with nlp.disable_pipes():\n        doc_vectors = np.array([nlp_doc.vector for nlp_doc in docs])\n    probas_pred = model.decision_function(doc_vectors)\n    return [1 if ele>thresh else 0 for ele in probas_pred]\n\ntitles = ['steve jobs hacker python', \n         'hn', \n         'the meaning of life']\n\npred(svc, thresh, titles, nlp)","0b0bba06":"#look at which titles the model predicted as popular and got right\n\n[doc for doc, pred in zip(docs,y_pred) if pred==1]","b5de429c":"The suppor vector machine classifier can produce a proabbaily prediction for each class as the diatnce from the support vector to the datapoint. We will use that to produce plots of precision, recall, and ROC","932db53a":"Do hacker news users favor particular topics? Can you predict weather a story will be popular from the title alone? \n\nIn this notebook we will see that it there are definately some keywords in post titles that correlate with higher post scores. It is also possible to create a model that predicts whether a post will be popular given the title alone, albeit with only marginal accuracy improvements over random guessing.","bf239c44":"First we will try and find a set of words that tend to be associated with high scoring posts. To do that we will first get a big set of 'candidate' popular words (from the most highly scored posts), then we get the median score for posts with these words (over the entire dataset). ","d2c144c8":"* The model AUC is about 0.6, so it is slightly better than random guessing (0.5) \n* The lower we set the threshold the more the model guesses that every title will be popular, so it finds all the popular titles, but also gets many false positives\n* The higher we set the threshold, the more the model guesses that every title will not be popular. It may reach a higher precision this way (more true positives), but it misses most of the popular titles\n* A good compormise if found around -0.5\n* Comparing to the dashed lines, this model is a bit better than random guessing","6149cc3f":"Next we will query the full stories dataset and get the median score for each candidate word based on the score of titles that contain it.","9e547261":"It does look like certain topics tend to be more popular than others. Can we make a model that predicts whether a post will be popular fromthe title alone? We will create a classfication model that tries to predict whether a post gets a score above 5."}}