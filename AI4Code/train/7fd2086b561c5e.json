{"cell_type":{"7702964b":"code","a0ff2bed":"code","720bdcbe":"code","a69c193d":"code","1f76358b":"code","997d7355":"code","e2678e0f":"code","1669323b":"code","87055081":"code","2aaaf1ba":"code","764035f8":"code","ffc33b4c":"code","c62bf5d2":"code","ab6ec8b0":"code","a6e7687e":"code","be0ba570":"code","54705da1":"code","957c040e":"code","5acb6534":"code","0dc18668":"code","707d1573":"markdown","3853dc35":"markdown","ec7cd285":"markdown","e8f232e6":"markdown","b2e6bf4a":"markdown","18e092dc":"markdown","b401f225":"markdown","e6651efa":"markdown","1fa4a95e":"markdown","c985c2ae":"markdown","480ebed5":"markdown","00f87845":"markdown","3735c0bc":"markdown","5d96b121":"markdown","099ed166":"markdown","05c36161":"markdown","bdda229b":"markdown","6268799a":"markdown","c2c713fb":"markdown","f7e00249":"markdown","8089e81b":"markdown","7214155b":"markdown","6f7d0ace":"markdown","d57678ca":"markdown","9cb4ed37":"markdown","e5dab286":"markdown","ac51cc59":"markdown"},"source":{"7702964b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a0ff2bed":"import numpy as np\nimport pandas as pd\nfrom scipy.stats import zscore\nfrom sklearn.neighbors import KNeighborsRegressor, NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nfrom math import isnan\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom scipy.spatial.distance import cdist\nfrom scipy.spatial import distance","720bdcbe":"# Load csv file into a dataframe\ndf_master = pd.read_csv(\"\/kaggle\/input\/autompg-dataset\/auto-mpg.csv\")\n\n# Display Shape and size of df_name dataframe\nprint(\"Shape of auto csv file :\", df_master.shape)\nprint(\"Size of auto csv file :\", df_master.size)","a69c193d":"# Viewing basic information of master dataset\ndf_master.info()","1f76358b":"# Run through all the other columns\nfor col in df_master.columns[1:]:\n    print(col)\n    print(df_master[col].unique())\n    print()","997d7355":"df_master[df_master[\"horsepower\"] == \"?\"]","e2678e0f":"# Create a mask to store the index values of the records with \"?\"\nindex_mask = df_master[df_master[\"horsepower\"] == \"?\"].index\n\n# Create the train and prediction set for KNN Regressor\nX_imp = df_master.drop([\"car name\", \"horsepower\", \"model year\", \"origin\"], axis=1).apply(zscore)\nX_imp_train = X_imp.drop(index_mask)\nX_imp_pred = X_imp.iloc[index_mask, :]\ny_imp_train = df_master[df_master[\"horsepower\"] != \"?\"][\"horsepower\"].astype(\"int64\")\n\n# Building the model\nclassifier = KNeighborsRegressor(n_neighbors=19, metric=\"minkowski\", p=2)\nclassifier.fit(X_imp_train, y_imp_train)\n\n# Predict values to impute\ny_pred = classifier.predict(X_imp_pred)\n\n# Create a new dataframe called auto so as to keep master intact\ndf_auto = df_master.copy()\n\n# Assign rounded prediction values to the respective missing value\ndf_auto.loc[index_mask, \"horsepower\"] = np.around(list(y_pred), decimals=0)\n\n# Convert feature \"hp\" to int type\ndf_auto[\"horsepower\"] = df_auto[\"horsepower\"].astype(\"int64\")\n\n# Display the final dataset information\ndf_auto.info()","1669323b":"df_auto.drop(\"car name\", axis=1, inplace=True)\n\nfor col in ['cylinders', 'model year', 'origin']:\n    print(\"Column name: \", col)\n    print(\"Discrete Values: \", df_auto[col].unique())\n    print(\"No. of values :\", len(df_auto[col].unique()))\n    print(\"With equal distribution No. of records on average with unique value: \", df_auto.shape[0]\/len(df_auto[col].unique()))\n    print(\"Proportion of sample with similar attribute: %2.0f%%\" %(100\/len(df_auto[col].unique())))\n    print()","87055081":"# Combining feature year and origin\ndf_auto[\"cat\"] = df_auto[\"model year\"]*10 + df_auto[\"origin\"]\n\n# Get the unique values and its frequency counts\nkey, val = np.unique(df_auto[\"cat\"], return_counts=True)\n\n# Replace attributes of feature cat with the relative frequency of each attribute\ndf_auto[\"cat\"].replace(dict(zip(key, val\/df_auto.shape[0])), inplace=True)\n\n# Dropping features yr and origin\ndf_auto.drop([\"model year\", \"origin\"], axis=1, inplace=True)\n\n# Display the statistics of remaining features\ndf_auto.describe()","2aaaf1ba":"print(\"To understand our new feature called cat (short for categorical) :-\")\nprint(\"With equal distribution No. of records on average with unique value: \", df_auto.shape[0]\/len(key))\nprint(\"Proportion of sample with similar attribute: %2.5f%%\" %(100\/len(key)))","764035f8":"# Define a function for hopkins test\ndef hopkins(X):\n    d = X.shape[1]\n    #d - len(vars)\n    n = len(X) #rows\n    m = int(0.1*n)\n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n    \n    rand_X = sample(range(0, n, 1), m)\n    \n    ujd = []\n    wjd = []\n\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    H = sum(ujd) \/ (sum(ujd) + sum(wjd))\n    if isnan(H):\n        print(\"Null Present\")\n        print(ujd, wjd)\n        H = 0\n    return(H)\n\n# Scale auto dataset\nscale = StandardScaler()\n\n# Run hopkins test on scaled data\nprint(\"Hopkins test statistics is : \", hopkins(pd.DataFrame(scale.fit_transform(df_auto))))","ffc33b4c":"# Displaying the correlation map\nf, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(df_auto.corr(), annot=True, cmap='Reds', ax=ax)\nplt.show()","c62bf5d2":"sns.pairplot(df_auto)","ab6ec8b0":"# Displaying the boxplot\nsns.boxplot(data=df_auto)","a6e7687e":"# Apply Zscore to all columns in dataset auto\ndf_auto = df_auto.apply(zscore)\n\n# Create a list with index values of all rows with atleast one outlier (Value > 3 standard deviations)\noutlier_mask = df_auto[(~(df_auto<3).all(axis=1))].index","be0ba570":"# Reinitialising PCA with 3 components\npca = PCA(n_components=3, svd_solver='randomized', random_state=42)\n\n# Assigning new features to the auto dataset and dropping compressed features\ndf_auto_vis = pd.DataFrame(pca.fit_transform(df_auto), columns=[\"Vis1\", \"Vis2\", \"Vis3\"])\n\n# 3D plots of clusters\nfig = plt.figure(figsize=(8, 6))\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=20, azim=60, auto_add_to_figure=False)\nfig.add_axes(ax)\nax.scatter(df_auto_vis.iloc[:, 0], df_auto_vis.iloc[:, 1], df_auto_vis.iloc[:, 2], edgecolor='k')\nax.w_xaxis.set_ticklabels([])\nax.w_yaxis.set_ticklabels([])\nax.w_zaxis.set_ticklabels([])\nax.set_title('3D plot of KMeans Clustering')","54705da1":"def centroids_init(X,k):\n    \n    # Initialise all points to cluster 0 and select a random centroid point\n    idx = np.random.choice(len(X), 1, replace=False)\n    centroids = X[idx, :]\n    dist = 0\n\n    P = np.array([0]*len(X))\n\n    # Find the farthest record from centroids and select that point as a new centroid\n    # Repeat search for new centroid for cluster 1 to cluster k\n    for i in range(1, k):\n        dist = [distance.mahalanobis(X[0, :], \n                                     np.array(centroids)[clu], \n                                     np.linalg.inv(np.cov(X[P==clu, :], rowvar=0))) for clu in range(i)]\n        for idx in range(1, len(X)):\n            dist = np.vstack((dist, [distance.mahalanobis(X[idx, :], \n                                                          np.array(centroids)[clu], \n                                                          np.linalg.inv(np.cov(X[P==clu, :], rowvar=0))) for clu in range(i)]))\n        centroids = np.vstack((centroids, X[np.argmax(np.max(dist,axis=1)), :]))\n        P = np.argmin(distance.cdist(X, centroids, 'euclidean'),axis=1)\n    return(centroids)","957c040e":"def kmeans_mahalanobis_new(X,k=3,max_iterations=6):\n\n    # Check if data is in DataFrame form and change to array\n    if isinstance(X, pd.DataFrame):X = X.values\n    \n    # Initialise required number of Centroids\n    centroids = centroids_init(X,k)\n    \n    # Create initial cluster with Euclidean distance\n    P = np.argmin(distance.cdist(X, centroids, 'euclidean'),axis=1)\n    \n    # Loop through max_iterations times\n    for _ in range(max_iterations):\n        \n        # Find Mahalanobish distance for each datapoint\n        dist = [distance.mahalanobis(X[0, :], \n                                     np.array(centroids)[clu], \n                                     np.linalg.inv(np.cov(X[P==clu, :], rowvar=0))) for clu in range(k)]\n        for idx in range(1, len(X)):\n            dist = np.vstack((dist, [distance.mahalanobis(X[idx, :], \n                                                          np.array(centroids)[clu], \n                                                          np.linalg.inv(np.cov(X[P==clu, :], rowvar=0))) for clu in range(k)]))\n        tmp = np.argmin(dist,axis=1)\n        \n        # Update centroids to cluster means\n        centroids = np.vstack([X[P==i,:].mean(axis=0) for i in range(k)])\n        \n        # If clusters are stable then stop early\n        if np.array_equal(P,tmp):break\n        P = tmp\n    return(P)","5acb6534":"# Initiate cluster numbers\nk=3\n\n# Repeat process till conditions are satisfied else leave with fail message\nc = 0\nwhile True:\n\n    try:\n        prediction=kmeans_mahalanobis_new(df_auto.drop(outlier_mask), k=k)\n        if len(np.unique(prediction))==k:\n            break\n    except:\n        print(\"Reinitialising\")\n        c+=1\n        if c == 7:\n            print(\"Failed too many times\")\n            break\nprediction","0dc18668":"# Initiate cluster numbers\nk=4\n\n# Repeat process till conditions are satisfied else leave with fail message\nc = 0\nwhile True:\n    try:\n        prediction=kmeans_mahalanobis_new(df_auto.drop(outlier_mask), k=k)\n        if len(np.unique(prediction))==k:\n            break\n    except:\n        print(\"Reinitialising\")\n        c+=1\n        if c == 7:\n            print(\"Failed too many times\")\n            break\nprediction","707d1573":"# **METHODOLOGY**","3853dc35":"## **Limitations:** \n**There is an issue with initialisation and the workaround to this problem can become time-consuming and computationally heavy. Further experimentation and alterations are needed before this method can become standardised.**\n## **Recommendations:** \n**Further research and work into centroid initialisation needs to be done. This algorithm should be tried with other datasets as well. There is a need to see if the dependency on Euclidean Distance can be completely removed, or at the least research should be continued to quantify how much bias is introduced because of the dependency on Euclidean Distance.**\n## **Implications:** \n**The relevant codelines are definitely useful. However, more work needs to be done till we can use it as a standalone method for clustering. For the time being Cluster optimisation techniques should be borrowed from established sources while Mahalanobis Distance could be used to form the final clusters, if it is giving better results.**","ec7cd285":"### **Data Cleaning**","e8f232e6":"**Only anomaly seems to be in \"horsepower\" and the value is ?. We should turn ? into a numeric value so that the feature can be correctly identified as numeric (int or float)**","b2e6bf4a":"## **Aim of the study:** \n**To create KMeans clusters with Mahalanobis Distance.**\n\n## **Rationale\/Purpose of study:**\n**I have searched the internet for a KMeans cluster algorithm which will use Mahalanobis Distance to build the clusters. Although Mahalanobis Distance is used in Nearest Neighbors, I am yet to find a clustering algorithm which uses Mahalanobis Distance. I find Mahalanobis Distance quite intuitive and elegant since it is natural to find data clustered in a particular shape, hence intuitively one can assume that clustering based on a distance which assumes this shape is the better method. However, Mahalanobis Distance remains quite cumbersome to implement and Euclidean Distance becomes a more convenient choice. Hence, my purpose of study is to see if I can implement a clustering algorithm like KMeans using Mahalanobis Distance. If possible then I believe Mahalanobis Distance will become the more preferred option because naturally data would be clustered in some shape.**","18e092dc":"**Credit to https:\/\/gdcoder.com\/implementation-of-k-means-from-scratch-in-python-9-lines\/ for helping me build kmeans from scratch.**","b401f225":"# **CONCLUSION**","e6651efa":"**Before we proceed let's have a quick look at the data distribution. To keep things simple we will try to visualise the distribution on 3 dimensions. So we will need to compress the dataset to 3 components.**","1fa4a95e":"**horsepower is marked as object although at first glance it feels like a numeric feature**","c985c2ae":"**For Multivariate Analysis we will check correlation through a correlation matrix**","480ebed5":"**Based on Attribute information from meta-data we are to understand that cylinders, model year and origin are multi-valued and discrete, car name can be dropped and the rest are all continuous. So let us explore our discrete features first.**","00f87845":"**Since only one column has 6 values that need to be imputed. We will use KNN to predict these values and we will allow 19 nearest neighbours to vote for each record. The selection choice of 19 is based on a guesstimate of square root of the number of records and 19 is the closest odd and whole number to 392 records which will be used to train.**\n\n**For the sake of simplicity we will not consider \"model year\" and \"origin\" in our training since these are discrete values and hence need more understanding and processing. However, our aim now is to impute the 6 values which has \"?\" and for that we do not need so much details. \"car_name\" will be dropped as it provides no extra information on learning.**","3735c0bc":"**Kmeans with Mahalanobish distance for 3 clusters**","5d96b121":"**Kmeans with Mahalanobish distance for 4 clusters**","099ed166":"**At a quick glance we can be sure that there are outliers. Outliers do affect K means and hence let us create a mask which can be used to remove outliers.**\n\n**For Kmeans we will be applying zscore to all columns later anyhow. Hence, we will locate outliers using zscores for convenience.**","05c36161":"### **Kmeans**","bdda229b":"**Let's look at a boxplot to identify whether outliers are present**","6268799a":"# **INTRODUCTION**","c2c713fb":"## **Settings & Sample:**\n**392 records of the dataset, missing data imputed with the mean of 19 nearest neighbours of each missing record. 19 was chosen Based on a guesstimate of square root of the number of records, 19 is the closest odd and whole number to 392 records.**\n\n## **Procedure:**\n**Our aim is to implement KMeans clustering Algorithm from scratch so that we can use Mahalanobis Distance to evaluate similarity. To achieve that we will have a function do the following:**\n\n1.     **Initialise a random point from the dataset as centroid.**\n1.     **Group all data into clusters around the centroids confirmed. In this case since cluster information is not available, we will use Euclidean Distance to group clusters.**\n1.     **Select the farthest data point, which is not already selected as a cluster, from the chosen centroids. Use Mahalanobis Distance here and add the furthest chosen data point as another centroid.**\n1.     **Repeat Steps 2 and 3 till we get as many centroids as clusters that we need.**\n1.     **After completing the initialization of centroids, we will group the dataset into clusters using Euclidean Distance again since we do not have the cluster information.**\n1.     **Find the mean of all clusters and reinitialise centroids based on the means of each cluster.**\n1.     **Regroup the clusters using Mahalanobis Distance.**\n1.     **Repeat Steps 6 & 7, till the new group of clusters and old group of clusters are identical or till a specific number of times (given by n_estimators, traditionally used by KMeans)**\n\n**Note: Necessary data cleaning, preprocessing or EDA work will remain the same as for KMeans. No special precaution needs to be taken before running Kmeans with Mahalanobis.**","f7e00249":"### **Exploratory Data Analysis**","8089e81b":"**Based on the above information, model year is quite dispersed in the sample. If we consider the sample a good representation of the population then the cluster created by feature year will at average contain 8% of the population.**\n\n**With similar logic, Origin at average can uniquely identify 33% and Cylinders can uniquely identify 20% of the population.**\n\n* **Assumption 1: We will assume that Cylinder is an ordinal quantity, ie. 4 cylinders is closer to 3 cylinders than 8 cylinders and that 3 cylinders is in some aspect less than 4 cylinders. Thus it will be logical to consider it discrete but its numeric version have real world meaning.**\n\n* **Assumption 2: We will assume that Model-year is categorical. Although, year is definitely ordered, we can anticipate that in the world of automobiles, the year of production does not play ordinal role but more of a categorical role. In short feature model-year will have more meaning if its numeric counterpart is not considered to have any order.**\n\n* **Assumption 3: Similar to Assumption 2, the numeric counterpart of the origin feature will be considered categorical.**\n\n* **Assumption 4: For Kmeans, which thrives on numeric input. We will convert our categorical features to its relative frequencies. We believe that the information provided in its categorical form will not be useful. On the other hand what proportion of the population is described by each feature attribute will have more meaning (it might contain real world information like demand and supply) when we attempt to cluster.**\n\n**Based on our Assumption 2 and 3 along with our Assumption 4 we will combine the two features (yr and origin) and then replace its value by the relative frequency.**","7214155b":"**Note: That distribution in 3D will look different from distribution in 5 feature space. So, the figure above is for visualisation purposes only and should not be confused with actual distribution**","6f7d0ace":"**From the charts above we can see that most of the variables have a high correlation with each other. This follows our understanding of the domain as well.**\n\n**Hence, we will use kmeans with Mahalanobish Distance**","d57678ca":"### **Feature Engineering**","9cb4ed37":"## **Results and Discussion:**\n**Although we are confident that clustering with Mahalanobis Distance is achievable, the issue with initialisation remains. Since Mahalanobis Distance depends on the covariance matrix a number of times (Based on different random initialisation) an error may be thrown because the covariance matrix faces a value of zero which results in infinite solutions. A majority of times one particular cluster is taking over all data points, thus one group of zeroes are thrown as a result. I am currently not equipped to resolve these issues however, I will come back and update my codelines when I come across a solution.**\n\n**A workaround to the issues mentioned is to re-initialise centroids and repeat clustering till a desired output is given. If we choose the right number of clusters then a perfect grouping of clusters can be achieved but the probability of getting that within a few centroid initialisations is low, which makes this task computationally heavy and slow.**","e5dab286":"**Let's do a bivariate analysis through a pairplot**","ac51cc59":"**Now let's check Hopkins Statistics for the auto dataset to confirm if the set is clusterable.**\n\n**Hopkins Statistics is a value between 0 and 1 and the closer it is to one the more clusterable the set is.**\n\n**For our test we will consider :-**\n* Any value above 0.7 as high tendency to cluster\n* Any value above 0.5 and less than 0.7 as low tendency to cluster\n* Any value below or equal to 0.5 will require us to reject the dataset at its present form"}}