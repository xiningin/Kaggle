{"cell_type":{"8f12e859":"code","d8172438":"code","4304e83c":"code","528a2c3d":"code","2f0ab245":"code","406d9bc6":"code","b5742861":"code","cbfbbfe4":"code","9217a060":"code","5c1c0671":"code","abde1aab":"code","1f6ad6c1":"code","c143fbf5":"code","2807c817":"code","9aea9481":"code","ba2db7b3":"code","ee0502c5":"code","d6753796":"code","8e89f2de":"code","60633c37":"code","316919a2":"code","9887ed96":"code","d245bc27":"code","010a1c39":"code","4c16d59c":"code","f8289e14":"code","b95ced62":"code","47ef63be":"code","9267be6e":"code","b7ccd388":"code","9e039c52":"code","84f7bf0f":"code","8466ad88":"code","ba813ffa":"code","4ad7bf7f":"markdown","44f6377a":"markdown","928ac1e1":"markdown","0c618aec":"markdown","b71d3697":"markdown"},"source":{"8f12e859":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\n\nfrom nltk.stem import PorterStemmer\nimport re\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# from wordcloud import WordCloud ##### Deprecated\n\nimport tensorflow as tf","d8172438":"data = pd.read_csv('\/kaggle\/input\/source-based-news-classification\/news_articles.csv')\ndata","4304e83c":"data = data.dropna(axis=0)\ndata","528a2c3d":"def get_top_n_words(corpus, n = None):\n    \"\"\"\n    A function that returns the top 'n' unigrams used in the corpus\n    \"\"\"\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus) ## Shape: (2045, 46774) -> There are 2045 sentences and 46774 words\n    sum_words = bag_of_words.sum(axis=0) ## Shape: (1, 46774) -> Count of occurance of each word\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()] ## vec.vocabulary_.items returns the dictionary with (word, index)\n    freq_sorted = sorted(words_freq, key = lambda x: x[1], reverse = True)\n    return freq_sorted[:n]\n\ndef get_top_n_bigram(corpus, n = None):\n    \"\"\"\n    A function that returns the top 'n' bigrams used in the corpus\n    \"\"\"\n    vec = CountVectorizer(ngram_range = (2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis = 0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    freq_sorted = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return freq_sorted[:n]","2f0ab245":"top_unigram = get_top_n_words(data['title_without_stopwords'], 20)\nwords_unigram = [i[0] for i in top_unigram]\ncount_unigram = [i[1] for i in top_unigram]\n\ntop_bigram = get_top_n_bigram(data['text_without_stopwords'], 20)\nwords_bigram = [i[0] for i in top_bigram]\ncount_bigram = [i[1] for i in top_bigram]","406d9bc6":"# Plot bar charts for top unigrams\nfont_title = {'family': 'sans serif',\n        'color':  'white',\n        'weight': 'bold',\n        'size': 16,\n        }\nfont_text = {'family': 'sans serif',\n        'color':  'white',\n        'weight': 'bold',\n        'size': 12,\n        }\n\nwith plt.style.context(\"dark_background\"):\n    fig, ax = plt.subplots(figsize=(14,4))\n    bar = ax.bar(words_unigram, count_unigram, color='#6baed6')\n    ax.set_title(\"Top Unigrams\", fontdict=font_title, size=16)\n    ax.set_xticklabels(words_unigram, fontdict=font_text, rotation=90)\n    ax.grid(axis='y')","b5742861":"# Plot bar charts for top bigrams\nwith plt.style.context(\"dark_background\"):\n    fig, ax = plt.subplots(figsize=(14,4))\n    bar = ax.bar(words_bigram, count_bigram, color='#a1dab4')\n    ax.set_title(\"Top Unigrams\", fontdict=font_title, size=16)\n    ax.set_xticklabels(words_bigram, fontdict=font_text, rotation=90)\n    ax.grid(axis='y')","cbfbbfe4":"# Visualising frequency of words using WordCloud package\nfrom wordcloud import WordCloud\n\nwc = WordCloud(background_color=\"black\", max_words=100,\n               max_font_size=256,\n               random_state=42, width=1000, height=1000)\nwc.generate(' '.join(data['text_without_stopwords']))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","9217a060":"## Visualising the fake and real news percentage\nimport math\ndifferent_labels = data['label'].unique()\ncounts = data['label'].value_counts().values\n\nplt.figure(figsize=(6,6))\nplt.pie(counts, labels=['Fake', 'Real'], autopct='%1.1f%%')\nplt.legend()","5c1c0671":"titles_stopped = data['title_without_stopwords']\ntitles_stopped.head()","abde1aab":"ps = PorterStemmer()\nps","1f6ad6c1":"ps.stem(\"roasted\")","c143fbf5":"def process_title(title):\n    new_title = title.split(\" \")\n    new_title = list(map(lambda x: ps.stem(x), new_title))\n    new_title = list(map(lambda x: x.strip(), new_title))\n    if '' in new_title:\n        new_title.remove('')\n    return new_title","2807c817":"titles_stemmed = titles_stopped.apply(process_title)\ntitles_stemmed","9aea9481":"# Get size of vocabulary\nvocabulary = set()\n\nfor title in titles_stemmed:\n    for word in title:\n        if word not in vocabulary:\n            vocabulary.add(word)\n\nvocab_length = len(vocabulary)\n\n# Get max length of a sequence\nmax_seq_length = 0\nfor title in titles_stemmed:\n    if len(title) > max_seq_length:\n        max_seq_length = len(title)","ba2db7b3":"# Viewing the words that have been added to our vocabulary\nimport more_itertools\nmore_itertools.take(10, vocabulary)","ee0502c5":"vocab_length","d6753796":"max_seq_length","8e89f2de":"# Tokenising and padding our sequences\ntokenizer = Tokenizer(num_words=vocab_length)\ntokenizer.fit_on_texts(titles_stemmed)\n\nsequences = tokenizer.texts_to_sequences(titles_stemmed)\n\nword_index = tokenizer.word_index\n\nmodel_inputs = pad_sequences(sequences, maxlen=max_seq_length)","60633c37":"# Viewing the sequences converted from the titles\nsequences[:10]","316919a2":"# Viewing the words mapped to tokens, with 1 being the most frequent word\nimport more_itertools\nmore_itertools.take(10, word_index.items())","9887ed96":"# Viewing the effect of padding the sequences\nmodel_inputs","d245bc27":"model_inputs.shape","010a1c39":"data['label'].unique()","4c16d59c":"labels = np.array(data['label'].map(dict(Real=0, Fake=1)))\nlabels","f8289e14":"X_train, X_test, y_train, y_test = train_test_split(model_inputs, labels, random_state=1)","b95ced62":"print('shape of X_train:', X_train.shape)\nprint('shape of y_train:', y_train.shape)\nprint('shape of X_test:', X_test.shape)\nprint('shape of y_test:', y_test.shape)","47ef63be":"y_train","9267be6e":"y_test","b7ccd388":"embedding_dim = 64\n\ninputs = tf.keras.Input(shape=(max_seq_length,))\n\nembedding = tf.keras.layers.Embedding(\n    input_dim=vocab_length,\n    output_dim=embedding_dim,\n    input_length=max_seq_length,\n)(inputs)\n\ngru = tf.keras.layers.GRU(units=embedding_dim)(embedding)\n\noutputs = tf.keras.layers.Dense(1, activation='sigmoid')(gru)\n\nmodel = tf.keras.Model(inputs, outputs)\n\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.AUC(name='auc')\n    ]\n)\n\nbatch_size = 16\nepochs = 5\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=batch_size,\n    epochs=epochs,\n    callbacks=[\n        tf.keras.callbacks.ReduceLROnPlateau(),\n        tf.keras.callbacks.ModelCheckpoint('model.h5', save_best_only=True)\n    ]\n)","9e039c52":"fig, (ax1, ax2) = plt.subplots(2, figsize=(10,6))\nax1.plot(history.history['val_loss'])\nax1.plot(history.history['loss'])\nax2.plot(history.history['val_auc'])\nax2.plot(history.history['auc'])\n\nax1.legend(['val_loss', 'loss'])\nax2.legend(['val_auc', 'auc'])\nax1.set_title('Loss Over Time')\nax2.set_title('AUC Over Time')\nax1.set(xlabel='Epoch', ylabel='Loss')\nax2.set(xlabel='Epoch', ylabel='AUC')\n\nfig.tight_layout()","84f7bf0f":"history.history","8466ad88":"model.load_weights('.\/model.h5')","ba813ffa":"model.evaluate(X_test, y_test)","4ad7bf7f":"# Supervised ML - Predicting fake news using only titles","44f6377a":"# Preprocessing\n- Since the dataset already contained title without stopwords, we would do stemming, tokenisation and padding to produce a sequence of numbers to feed into our ML model later","928ac1e1":"# Exploratory Data Analysis\n- We'll explore how frequent some words appear in the titles","0c618aec":"# Training","b71d3697":"# Discussion\n-   The final result is an accuracy of about 67%. The results could be further improved if the texts of the articles are used, even better is we could append the texts to the titles and processed it altogether.\n\n-   Further work can look at using LIME or SHAP values to explain how the model has identified what words would carry more weightings to classify whether a news article is real or fake."}}