{"cell_type":{"ea296a5e":"code","51de02f0":"code","99a1be7e":"code","06c39fb2":"code","46fcf66a":"code","7bb8de5a":"code","cd03312e":"code","29b4df02":"code","fe4f1457":"code","9e79552d":"code","53a4714e":"code","36b13084":"code","aa17df24":"code","17c19458":"code","f65a745f":"markdown","f159fad4":"markdown","1cfac046":"markdown","6092199a":"markdown","e4d3e44a":"markdown","3c9b3c20":"markdown","4bc69b4b":"markdown","cb136107":"markdown","14ad0e73":"markdown","3a92838b":"markdown","c84a8432":"markdown","5f66c4d3":"markdown","be906650":"markdown","ba43da91":"markdown"},"source":{"ea296a5e":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow_addons as tfa\nimport glob, random, os, warnings\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n\nprint('TensorFlow Version ' + tf.__version__)\n\ndef seed_everything(seed = 0):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nseed_everything()\nwarnings.filterwarnings('ignore')","51de02f0":"image_size = 224\nbatch_size = 16\nn_classes = 5\n\ntrain_path = '\/kaggle\/input\/cassava-leaf-disease-classification\/train_images'\ntest_path = '\/kaggle\/input\/cassava-leaf-disease-classification\/test_images'\n\ndf_train = pd.read_csv('\/kaggle\/input\/cassava-leaf-disease-classification\/train.csv', dtype = 'str')\n\ntest_images = glob.glob(test_path + '\/*.jpg')\ndf_test = pd.DataFrame(test_images, columns = ['image_path'])\n\nclasses = {0 : \"Cassava Bacterial Blight (CBB)\",\n           1 : \"Cassava Brown Streak Disease (CBSD)\",\n           2 : \"Cassava Green Mottle (CGM)\",\n           3 : \"Cassava Mosaic Disease (CMD)\",\n           4 : \"Healthy\"}","99a1be7e":"def data_augment(image):\n    p_spatial = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n \n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    if p_spatial > .75:\n        image = tf.image.transpose(image)\n        \n    # Rotates\n    if p_rotate > .75:\n        image = tf.image.rot90(image, k = 3) # rotate 270\u00ba\n    elif p_rotate > .5:\n        image = tf.image.rot90(image, k = 2) # rotate 180\u00ba\n    elif p_rotate > .25:\n        image = tf.image.rot90(image, k = 1) # rotate 90\u00ba\n        \n    return image","06c39fb2":"datagen = tf.keras.preprocessing.image.ImageDataGenerator(samplewise_center = True,\n                                                          samplewise_std_normalization = True,\n                                                          validation_split = 0.2,\n                                                          preprocessing_function = data_augment)\n\ntrain_gen = datagen.flow_from_dataframe(dataframe = df_train,\n                                        directory = train_path,\n                                        x_col = 'image_id',\n                                        y_col = 'label',\n                                        subset = 'training',\n                                        batch_size = batch_size,\n                                        seed = 1,\n                                        color_mode = 'rgb',\n                                        shuffle = True,\n                                        class_mode = 'categorical',\n                                        target_size = (image_size, image_size))\n\nvalid_gen = datagen.flow_from_dataframe(dataframe = df_train,\n                                        directory = train_path,\n                                        x_col = 'image_id',\n                                        y_col = 'label',\n                                        subset = 'validation',\n                                        batch_size = batch_size,\n                                        seed = 1,\n                                        color_mode = 'rgb',\n                                        shuffle = False,\n                                        class_mode = 'categorical',\n                                        target_size = (image_size, image_size))\n\ntest_gen = datagen.flow_from_dataframe(dataframe = df_test,\n                                       x_col = 'image_path',\n                                       y_col = None,\n                                       batch_size = batch_size,\n                                       seed = 1,\n                                       color_mode = 'rgb',\n                                       shuffle = False,\n                                       class_mode = None,\n                                       target_size = (image_size, image_size))","46fcf66a":"images = [train_gen[0][0][i] for i in range(16)]\nfig, axes = plt.subplots(3, 5, figsize = (10, 10))\n\naxes = axes.flatten()\n\nfor img, ax in zip(images, axes):\n    ax.imshow(img.reshape(image_size, image_size, 3))\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()","7bb8de5a":"learning_rate = 0.001\nweight_decay = 0.0001\nnum_epochs = 1\n\npatch_size = 7  # Size of the patches to be extract from the input images\nnum_patches = (image_size \/\/ patch_size) ** 2\nprojection_dim = 64\nnum_heads = 4\ntransformer_units = [\n    projection_dim * 2,\n    projection_dim,\n]  # Size of the transformer layers\ntransformer_layers = 8\nmlp_head_units = [56, 28]  # Size of the dense layers of the final classifier","cd03312e":"def mlp(x, hidden_units, dropout_rate):\n    for units in hidden_units:\n        x = L.Dense(units, activation = tf.nn.gelu)(x)\n        x = L.Dropout(dropout_rate)(x)\n    return x","29b4df02":"class Patches(L.Layer):\n    def __init__(self, patch_size):\n        super(Patches, self).__init__()\n        self.patch_size = patch_size\n\n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images = images,\n            sizes = [1, self.patch_size, self.patch_size, 1],\n            strides = [1, self.patch_size, self.patch_size, 1],\n            rates = [1, 1, 1, 1],\n            padding = 'VALID',\n        )\n        patch_dims = patches.shape[-1]\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n        return patches","fe4f1457":"plt.figure(figsize=(4, 4))\n\nx = train_gen.next()\nimage = x[0][0]\n\nplt.imshow(image.astype('uint8'))\nplt.axis('off')\n\nresized_image = tf.image.resize(\n    tf.convert_to_tensor([image]), size = (image_size, image_size)\n)\n\npatches = Patches(patch_size)(resized_image)\nprint(f'Image size: {image_size} X {image_size}')\nprint(f'Patch size: {patch_size} X {patch_size}')\nprint(f'Patches per image: {patches.shape[1]}')\nprint(f'Elements per patch: {patches.shape[-1]}')\n\nn = int(np.sqrt(patches.shape[1]))\nplt.figure(figsize=(4, 4))\n\nfor i, patch in enumerate(patches[0]):\n    ax = plt.subplot(n, n, i + 1)\n    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n    plt.imshow(patch_img.numpy().astype('uint8'))\n    plt.axis('off')","9e79552d":"class PatchEncoder(L.Layer):\n    def __init__(self, num_patches, projection_dim):\n        super(PatchEncoder, self).__init__()\n        self.num_patches = num_patches\n        self.projection = L.Dense(units = projection_dim)\n        self.position_embedding = L.Embedding(\n            input_dim = num_patches, output_dim = projection_dim\n        )\n\n    def call(self, patch):\n        positions = tf.range(start = 0, limit = self.num_patches, delta = 1)\n        encoded = self.projection(patch) + self.position_embedding(positions)\n        return encoded","53a4714e":"def vision_transformer():\n    inputs = L.Input(shape = (image_size, image_size, 3))\n    \n    # Create patches.\n    patches = Patches(patch_size)(inputs)\n    \n    # Encode patches.\n    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n\n    # Create multiple layers of the Transformer block.\n    for _ in range(transformer_layers):\n        \n        # Layer normalization 1.\n        x1 = L.LayerNormalization(epsilon = 1e-6)(encoded_patches)\n        \n        # Create a multi-head attention layer.\n        attention_output = L.MultiHeadAttention(\n            num_heads = num_heads, key_dim = projection_dim, dropout = 0.1\n        )(x1, x1)\n        \n        # Skip connection 1.\n        x2 = L.Add()([attention_output, encoded_patches])\n        \n        # Layer normalization 2.\n        x3 = L.LayerNormalization(epsilon = 1e-6)(x2)\n        \n        # MLP.\n        x3 = mlp(x3, hidden_units = transformer_units, dropout_rate = 0.1)\n        \n        # Skip connection 2.\n        encoded_patches = L.Add()([x3, x2])\n\n    # Create a [batch_size, projection_dim] tensor.\n    representation = L.LayerNormalization(epsilon = 1e-6)(encoded_patches)\n    representation = L.Flatten()(representation)\n    representation = L.Dropout(0.5)(representation)\n    \n    # Add MLP.\n    features = mlp(representation, hidden_units = mlp_head_units, dropout_rate = 0.5)\n    \n    # Classify outputs.\n    logits = L.Dense(n_classes)(features)\n    \n    # Create the model.\n    model = tf.keras.Model(inputs = inputs, outputs = logits)\n    \n    return model","36b13084":"decay_steps = train_gen.n \/\/ train_gen.batch_size\ninitial_learning_rate = learning_rate\n\nlr_decayed_fn = tf.keras.experimental.CosineDecay(initial_learning_rate, decay_steps)\n\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_decayed_fn)","aa17df24":"optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n\nmodel = vision_transformer()\n    \nmodel.compile(optimizer = optimizer, \n              loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.1), \n              metrics = ['accuracy'])\n\n\nSTEP_SIZE_TRAIN = train_gen.n \/\/ train_gen.batch_size\nSTEP_SIZE_VALID = valid_gen.n \/\/ valid_gen.batch_size\n\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n                                                 min_delta = 1e-4,\n                                                 patience = 5,\n                                                 mode = 'max',\n                                                 restore_best_weights = True,\n                                                 verbose = 1)\n\ncheckpointer = tf.keras.callbacks.ModelCheckpoint(filepath = '.\/model.hdf5',\n                                                  monitor = 'val_accuracy', \n                                                  verbose = 1, \n                                                  save_best_only = True,\n                                                  save_weights_only = True,\n                                                  mode = 'max')\n\ncallbacks = [earlystopping, lr_scheduler, checkpointer]\n\nmodel.fit(x = train_gen,\n          steps_per_epoch = STEP_SIZE_TRAIN,\n          validation_data = valid_gen,\n          validation_steps = STEP_SIZE_VALID,\n          epochs = num_epochs,\n          callbacks = callbacks)","17c19458":"print('Training results')\nmodel.evaluate(train_gen)\n\nprint('Validation results')\nmodel.evaluate(valid_gen)","f65a745f":"### Sample Image Patches Visualization","f159fad4":"### Sample Images Visualization","1cfac046":"## Build the ViT model\nThe ViT model consists of multiple Transformer blocks, which use the `MultiHeadAttention` layer as a self-attention mechanism applied to the sequence of patches. The Transformer blocks produce a `[batch_size, num_patches, projection_dim]` tensor, which is processed via an classifier head with softmax to produce the final class probabilities output.\n\nUnlike the technique described in the paper, which prepends a learnable embedding to the sequence of encoded patches to serve as the image representation, all the outputs of the final Transformer block are reshaped with `Flatten()` and used as the image representation input to the classifier head. Note that the `GlobalAveragePooling1D` layer could also be used instead to aggregate the outputs of the Transformer block, especially when the number of patches and the projection dimensions are large.","6092199a":"# Model Results","e4d3e44a":"## 3. Patch Encoding Layer\nThe `PatchEncoder` layer will linearly transform a patch by projecting it into a vector of size `projection_dim`. In addition, it adds a learnable position embedding to the projected vector.","3c9b3c20":"## 2. Patch Creation Layer","4bc69b4b":"# Overview\n\nThis notebook implements Vision Transformer (ViT) model by Alexey Dosovitskiy et al for image classification, and demonstrates it on the Cassava Leaf Disease Classification dataset.\n\n# Model Architecture\n\n![image.png](attachment:image.png)","cb136107":"# Data Generator","14ad0e73":"# Summary\n\nNote that the state of the art results reported in the paper are achieved by pre-training the ViT model using the JFT-300M dataset, then fine-tuning it on the target dataset. To improve the model quality without pre-training, you can try to train the model for more epochs, use a larger number of Transformer layers, resize the input images, change the patch size, or increase the projection dimensions. Besides, as mentioned in the paper, the quality of the model is affected not only by architecture choices, but also by parameters such as the learning rate schedule, optimizer, weight decay, etc. In practice, it's recommended to fine-tune a ViT model that was pre-trained using a large, high-resolution dataset. <br>\n\n**References:** <br>\nKeras Docs: https:\/\/keras.io\/api\/ <br>\nResearch Paper: https:\/\/arxiv.org\/pdf\/2010.11929.pdf","3a92838b":"# Libraries and Configurations","c84a8432":"# Model Hyperparameters","5f66c4d3":"# Building the Model and it's Components","be906650":"# Data Augmentations","ba43da91":"## 1. Multilayer Perceptron (MLP)"}}