{"cell_type":{"ea891a41":"code","e7bba04f":"code","765f2ffa":"code","02338e85":"code","12fe2293":"code","a2772bd9":"code","ee1d92c9":"code","6dd6fa9b":"code","b3848493":"code","f76d9482":"code","9514de40":"code","4ae3810f":"code","d2b176e3":"code","71e00e53":"code","4bd3a6f5":"code","b379cb40":"code","123dbea2":"code","f5aaef8a":"code","df7f84c2":"code","8ccd84a5":"code","aa162841":"code","40a153cd":"code","b312d6d0":"code","408e877a":"code","1d3e267f":"code","ce49c426":"code","fdf12e27":"code","a6e4981d":"code","bb595954":"code","414998ef":"code","ee8416ea":"code","dd52d6e7":"code","a162c763":"code","71eadce2":"code","4bca560d":"code","d7d2b4a3":"code","ab1daeac":"code","75c8fc74":"markdown"},"source":{"ea891a41":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e7bba04f":"#Plotting started code requied\nimport matplotlib.pyplot as plt\nplt.style.use(style='ggplot')\nplt.rcParams['figure.figsize'] = (10, 6)\nimport seaborn as sns","765f2ffa":"train_df  = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntrain_df.info()\n#train_df.head(10)\n","02338e85":"test_df  = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest_df.info()\n#train_df.head(10)","12fe2293":"plt.figure(figsize = (15,5))\n\nplt.title(\"Heatmap to see the nulls\")\nsns.heatmap(train_df.isnull(), yticklabels = False, cbar = False)\nplt.xlabel(\"Features\")\nplt.ylabel(\"Nulls as white lines\")","a2772bd9":"all_data = pd.concat((train_df, test_df)).reset_index(drop=True)\nx_saleprice = train_df[\"SalePrice\"]\nall_data.drop([\"SalePrice\"], axis = 1, inplace= True)\nall_data.shape","ee1d92c9":"all_data.PoolQC.value_counts()","6dd6fa9b":"x = all_data.isnull().sum().sort_values(ascending = False)\nprint (x[x>0])","b3848493":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"NA\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"NA\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"NA\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"NA\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"NA\")\nall_data[\"GarageQual\"] = all_data[\"GarageQual\"].fillna(\"NA\")\nall_data[\"GarageYrBlt\"] = all_data[\"GarageYrBlt\"].fillna(\"NA\")\nall_data[\"GarageCond\"] = all_data[\"GarageCond\"].fillna(\"NA\")\nall_data[\"GarageFinish\"] = all_data[\"GarageFinish\"].fillna(\"NA\")\nall_data[\"GarageType\"] = all_data[\"GarageType\"].fillna(\"NA\")\nall_data[\"BsmtCond\"] = all_data[\"BsmtCond\"].fillna(\"NA\")\nall_data[\"BsmtExposure\"] = all_data[\"BsmtExposure\"].fillna(\"NA\")\nall_data[\"BsmtQual\"] = all_data[\"BsmtQual\"].fillna(\"NA\")\nall_data[\"BsmtFinType2\"] = all_data[\"BsmtFinType2\"].fillna(\"NA\")\nall_data[\"BsmtFinType1\"] = all_data[\"BsmtFinType1\"].fillna(\"NA\")\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n\nall_data[\"LotFrontage\"] = all_data[\"LotFrontage\"].fillna(all_data[\"LotFrontage\"].median())\nall_data[\"MSZoning\"] = all_data[\"MSZoning\"].fillna(all_data[\"MSZoning\"].mode()[0])\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(all_data[\"Functional\"].mode()[0])\nall_data[\"Utilities\"] = all_data[\"Utilities\"].fillna(all_data[\"Utilities\"].mode()[0])\nall_data[\"BsmtHalfBath\"] = all_data[\"BsmtHalfBath\"].fillna(all_data[\"BsmtHalfBath\"].mode()[0])\nall_data[\"BsmtFullBath\"] = all_data[\"BsmtFullBath\"].fillna(all_data[\"BsmtFullBath\"].mode()[0])\nall_data[\"BsmtFinSF2\"] = all_data[\"BsmtFinSF2\"].fillna(all_data[\"BsmtFinSF2\"].mode()[0])\n\nall_data[\"BsmtFinSF1\"] = all_data[\"BsmtFinSF1\"].fillna(all_data[\"BsmtFinSF1\"].mode()[0])\nall_data[\"GarageArea\"] = all_data[\"GarageArea\"].fillna(all_data[\"GarageArea\"].mode()[0])\nall_data[\"Exterior1st\"] = all_data[\"Exterior1st\"].fillna(all_data[\"Exterior1st\"].mode()[0])\nall_data[\"BsmtUnfSF\"] = all_data[\"BsmtUnfSF\"].fillna(all_data[\"BsmtUnfSF\"].mode()[0])\nall_data[\"TotalBsmtSF\"] = all_data[\"TotalBsmtSF\"].fillna(all_data[\"TotalBsmtSF\"].mode()[0])\nall_data[\"GarageCars\"] = all_data[\"GarageCars\"].fillna(all_data[\"GarageCars\"].mode()[0])\nall_data[\"Exterior2nd\"] = all_data[\"Exterior2nd\"].fillna(all_data[\"Exterior2nd\"].mode()[0])\nall_data[\"KitchenQual\"] = all_data[\"KitchenQual\"].fillna(all_data[\"KitchenQual\"].mode()[0])\nall_data[\"SaleType\"] = all_data[\"SaleType\"].fillna(all_data[\"SaleType\"].mode()[0])\nall_data[\"Electrical\"] = all_data[\"Electrical\"].fillna(all_data[\"Electrical\"].mode()[0])\n","f76d9482":"#all_data[\"GarageCars\"].value_counts()\n\n#print (all_data[\"MSZoning\"].mode())","9514de40":"\ndef one_hot(df, cols):\n    \"\"\"\n    @param df pandas DataFrame\n    @param cols a list of columns to encode \n    @return a DataFrame with one-hot encoding\n    \"\"\"\n    i = 0\n    for each in cols:\n        #print (each)\n        dummies = pd.get_dummies(df[each], prefix=each, drop_first= True)\n        if i == 0: \n            print (dummies)\n            i = i + 1\n        df = pd.concat([df, dummies], axis=1)\n    return df\n","4ae3810f":"all_data.shape","d2b176e3":"\n#One hot encoding done\nall_data = one_hot(all_data, objList) \nall_data.shape\n","71e00e53":"\n#Dropping duplicates columns if any\nall_data = all_data.loc[:,~all_data.columns.duplicated()]\nall_data.shape\n","4bd3a6f5":"\n#Dropping the original columns that has data type object \nall_data.drop(objList, axis=1, inplace=True)\nall_data.shape\n","b379cb40":"#data=pd.concat([train_df, test_df],axis=0)\nobjList = all_data.select_dtypes(include = \"object\").columns\n\nprint (objList)","123dbea2":"'''\n#Label Encoding for object to numeric conversion - Option 1\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nfor feat in objList:\n    all_data[feat] = le.fit_transform(all_data[feat].astype(str))\n\nprint (all_data.shape)\n'''","f5aaef8a":"train_df = all_data.iloc[:1460,:]  \ntest_df = all_data.iloc[1460 :,:]  ","df7f84c2":"train_df[\"SalePrice\"] = x_saleprice","8ccd84a5":"train_df.info()","aa162841":"X_train = train_df.drop([\"SalePrice\"], axis = 1)\nY_train = train_df[\"SalePrice\"]\nX_test = test_df","40a153cd":"X_train.shape ","b312d6d0":"from sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.metrics import make_scorer, r2_score, mean_squared_log_error\n\nn_folds = 5\n\ncv = KFold(n_splits = 5, shuffle=True, random_state=42).get_n_splits(X_train.values)\n\ndef test_model(model):   \n    msle = make_scorer(mean_squared_log_error)\n    rmsle = np.sqrt(cross_val_score(model, X_train, Y_train, cv=cv, scoring = msle))\n    score_rmsle = [rmsle.mean()]\n    return score_rmsle\n\ndef test_model_r2(model):\n    r2 = make_scorer(r2_score)\n    r2_error = cross_val_score(model, X_train, Y_train, cv=cv, scoring = r2)\n    score_r2 = [r2_error.mean()]\n    return score_r2\n","408e877a":"'''\n#1. SVM algorithm\nfrom sklearn import svm\nreg_svm = svm.SVR()\nrmsle_svm = test_model(reg_svm)\nprint (rmsle_svm, test_model_r2(reg_svm))\n'''","1d3e267f":"'''\n#2. Decision Tree \nfrom sklearn.tree import DecisionTreeRegressor\nreg_dtR = DecisionTreeRegressor(max_depth=5, random_state=51)\nrmsle_dtR = test_model(reg_dtR)\nprint (rmsle_dtR, test_model_r2(reg_dtR))\n'''","ce49c426":"'''\n#3. Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nreg_rFR = RandomForestRegressor(max_depth=5, random_state=51)\nrmsle_rFR = test_model(reg_rFR)\nprint (rmsle_rFR, test_model_r2(reg_rFR))\n'''","fdf12e27":"'''\n#4. Ridge Model\nfrom sklearn.linear_model import Ridge\n\nreg_ridge = Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,  random_state=None, solver='auto', tol=0.001)\n\nrmsle_ridge = test_model(reg_ridge)\n\nprint (rmsle_ridge, test_model_r2(reg_ridge))\n'''","a6e4981d":"'''\n#5. Elastic Net\nfrom sklearn.linear_model import ElasticNet\n\nreg_elastic_net = ElasticNet(alpha=0.1, copy_X=True, fit_intercept=True, l1_ratio=0.9,\n           max_iter=100, normalize=False, positive=False, precompute=False,\n           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n\nrmsle_elastic_net = test_model(reg_elastic_net)\n\nprint (rmsle_elastic_net, test_model_r2(reg_elastic_net))\n'''","bb595954":"'''\n#6. Adaboost regressor\nfrom sklearn.ensemble import AdaBoostRegressor\nreg_aBR = AdaBoostRegressor(random_state=51, n_estimators=1000)\nrmsle_aBR = test_model(reg_aBR)\nprint (rmsle_aBR, test_model_r2(reg_aBR))\n'''","414998ef":"'''\n#7. BaggingClassifier\nfrom sklearn.ensemble import BaggingRegressor\nreg_bgr = BaggingRegressor(base_estimator=None, bootstrap=True, bootstrap_features=False,\n                 max_features=1.0, max_samples=1.0, n_estimators=100,\n                 n_jobs=None, oob_score=False, random_state=51, verbose=0,\n                 warm_start=False)\nrmsle_bgr = test_model(reg_bgr)\nprint (rmsle_bgr, test_model_r2(reg_bgr))\n'''","ee8416ea":"'''\n#8. XGboost\nimport xgboost as xgb\nfrom xgboost import plot_importance\n\nreg_xgb = xgb.XGBRegressor()\nrmsle_xgb = test_model(reg_xgb)\nprint (rmsle_xgb, test_model_r2(reg_xgb))\n'''","dd52d6e7":"\n#9. GradientBoostingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nreg_gbr = GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n                          init=None, learning_rate=0.05, loss='ls', max_depth=3,\n                          max_features='sqrt', max_leaf_nodes=None,\n                          min_impurity_decrease=0.0, min_impurity_split=None,\n                          min_samples_leaf=9, min_samples_split=8,\n                          min_weight_fraction_leaf=0.0, n_estimators=1250,\n                          n_iter_no_change=None, presort='deprecated',\n                          random_state=None, subsample=0.8, tol=0.0001,\n                          validation_fraction=0.1, verbose=0, warm_start=False)\n\nrmsle_ggr = test_model(reg_gbr)\nprint (rmsle_ggr, test_model_r2(reg_gbr))\n#[0.1321644225864123] [0.8880632316361895]\n#[0.1333303699875545] [0.8813635273996498]\n","a162c763":"#Train the model with any of the above declared models\nreg_gbr.fit(X_train, Y_train)\nY_pred  = reg_gbr.predict(test_df) \n","71eadce2":"pred=pd.DataFrame(Y_pred)\nsub_df=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\ndatasets=pd.concat([sub_df['Id'],pred],axis=1)\ndatasets.columns=['Id','SalePrice']\ndatasets.to_csv('sample_submission.csv',index=False)\nprint(\"Your submission was successfully saved!\")","4bca560d":"from matplotlib import pyplot\npyplot.bar(range(len(reg_gbr.feature_importances_)), reg_gbr.feature_importances_)\npyplot.show()","d7d2b4a3":"#To view the descending values of the features i.e. top 10 %\n\n#new_df = ([train_df.columns, clf_ggr.feature_importances_])\nfeatureImp = []\n\nfor feat, importance in zip(train_df.columns, reg_gbr.feature_importances_):  \n    temp = [feat, importance*100]\n    featureImp.append(temp)\n\nfT_df = pd.DataFrame(featureImp, columns = ['Feature', 'Importance'])\nprint (fT_df.sort_values('Importance', ascending = False))","ab1daeac":"#Hyper parameter tuning \nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nparam_grid = {'min_samples_split':[2,4,6,8,10,20,40,60,100], \n              'min_samples_leaf':[1,3,5,7,9, 15, 20, 25, 30, 40, 50],\n              'subsample':[0.7,0.75,0.8,0.85,0.9,0.95,1],\n              'learning_rate':[0.15,0.1,0.05,0.01,0.005,0.001, 0.2], \n              'n_estimators':[10, 30, 50, 100,250,500,750,1000,1250,1500,1750],\n              'max_features' : ['sqrt']\n             }\n\nasdf = GradientBoostingRegressor()\n\n#clf = GridSearchCV(asdf, param_grid=param_grid, scoring='r2', n_jobs=-1)\nclf = RandomizedSearchCV(asdf, param_grid, scoring='r2', n_jobs=-1)\n \nclf.fit(X_train, Y_train)\n\nprint(clf.best_estimator_)","75c8fc74":"## House Price Prediction\n\nThis is a very interesting competition provided by Kaggle. I believe that all the entry level Machine Learning enthusiasts should definitely get their hands-on to this competition as well as the [Titanic Survivor Prediction competition ](https:\/\/www.kaggle.com\/c\/titanic). One can also check [my notebook](https:\/\/www.kaggle.com\/darshanjain29\/titanic-survival-from-top-70-to-top-7-on-lb) on the same.\n\nComing back to this competition, as it says that the problem is predicting house price i.e. a continuous value. Hence it is a regression problem and thus all your basic regression knowledge will be tested here.\n\nNow, from a beginner's perspective how do you start with this? Well, I will show very easy steps with which you can easily jump from **top 95%** on leaderboard to **top 28%**. So, let's get started.\n\n1. Let's read about the data that is given. So, in the train.csv there are 1460 rows and 81 columns whereas in test.csv there are 1459 and 80 columns.\n2. After reading the file, we start data preprocessing and feature engineering. Before moving ahead we are combining both the dataframes so that changes can be done in both the dfs together.\n3. So, with the help of heatmap we can check number of nulls in each feature. So, based on the column name and after reading the description in the file data_description.txt of that column, we can decide if we want to replace nulls by mean(), mode(), median() or 'NA' or something else.\n4. Now, we have to replace all the object\/string values to numerical type using one hot encoding. While doing this many new columns will be created. So, we also run a code to drop all duplicate columns if any\n5. Now I am separating the train and test data from all_data and appending SalePrice column to the train data and the training models starts from here.\n6. While training the model, we are using Kfold cross validation for the better root mean squared log error as it is the evaluation criteria on the leaderboard.\n7. Training with SVM algorithm: rmsle, r2_score = 0.42655965911720095, -0.21383191346231967 respectively\n8. Training with Decision Tree: rmsle, r2_score = 0.20087638073362202, 0.7569636210379455 respectively\n9. Training with Random Forest Regressor model: rmsle, r2_score = 0.17295960323425913, 0.8220478028104417 respectively\n10. Training with ridge model: rmsle, r2_score = 0.1562332068843027, 0.8350220277268043 respectively\n11. Training with Elastic Net: rmsle, r2_score = 0.15524191588871572, 0.8355319656725282 respectively\n12. Training with Adaboost regressor model: rmsle, r2_score = 0.20880462846912984, 0.7886447451714611 respectively\n13. Training with Bagging Classifier model: rmsle, r2_score = 0.14719806924931875, 0.8539938038399392 respectively\n14. Training with XGboost model: rmsle, r2_score = 0.13372606548796895, 0.8742884283414025 respectively\n15. Training with Gradient Boosting Regressor model: rmsle, r2_score = 0.14187482285610134, 0.8865067659268699 respectively\n16. Now you can do a submission and check your score on the leaderboard.\n17. After trying each model, try using the same one with hyper parameter tuning and you will be able to small improvement in rmsle and r2_score as well as your position on the leaderboard\n18. At the end we have check the feature importance count and removed all the features with importance count less than 0. \n\nI hope this helps. Please comment below if you haven't understood anything from the above steps."}}