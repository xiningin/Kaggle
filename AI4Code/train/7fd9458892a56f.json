{"cell_type":{"8593c78d":"code","bb1ccee4":"code","7a2cefac":"code","8fe597af":"code","a305aa01":"code","1da6f8bf":"code","46d5336c":"code","d8a09887":"code","6cbaca9d":"code","4b8c51b7":"code","e7fd94aa":"code","a85a742c":"code","26c775d5":"code","d262a7d3":"code","d1f80e90":"code","a1ca915a":"code","9d8c2bf7":"code","eee1aadb":"markdown","38ff3238":"markdown","b3efd430":"markdown","9d0049a8":"markdown","8b67927f":"markdown","cba4a12f":"markdown"},"source":{"8593c78d":"!pip -q install ..\/input\/pytorchtabnet\/pytorch_tabnet-3.1.1-py3-none-any.whl","bb1ccee4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy.matlib\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom joblib import Parallel, delayed\n\nimport shutil\nimport glob\nimport gc \nimport os\n\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\nfrom sklearn.metrics import r2_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import KFold\n\nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\nimport torch\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n\n\n# setting some globl config\n\nplt.style.use('ggplot')\norange_black = [\n    '#fdc029', '#df861d', '#FF6347', '#aa3d01', '#a30e15', '#800000', '#171820'\n]\nplt.rcParams['figure.figsize'] = (16,9)\nplt.rcParams[\"figure.facecolor\"] = '#FFFACD'\nplt.rcParams[\"axes.facecolor\"] = '#FFFFE0'\nplt.rcParams[\"axes.grid\"] = True\nplt.rcParams[\"grid.color\"] = orange_black[3]\nplt.rcParams[\"grid.alpha\"] = 0.5\nplt.rcParams[\"grid.linestyle\"] = '--'\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","7a2cefac":"def read_train_test():\n    # Function to read our base train and test set\n    \n    train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    print(f'Our test set has {test.shape[0]} rows')\n    print(f'Our training set has {train.isna().sum().sum()} missing values')\n    print(f'Our test set has {test.isna().sum().sum()} missing values')\n    \n    return train, test","8fe597af":"train, test = read_train_test()","a305aa01":"def book_and_trade_preprocessor(file_path_book, file_path_trade):\n    df_book = pd.read_parquet(file_path_book)\n    df_trade = pd.read_parquet(file_path_trade)\n\n    df = df_book.merge(df_trade, on=[\"time_id\",\"seconds_in_bucket\"], how='outer').dropna(how='any', axis=0).reset_index(drop=True)\n    del df_book, df_trade\n    gc.collect()\n    df['bid_size_diff1'] = df[\"size\"] - df[\"bid_size1\"]\n    df['bid_size_diff2'] = df[\"size\"] - df[\"bid_size2\"]\n    df['ask_size_diff1'] = df[\"size\"] - df[\"ask_size1\"]\n    df['ask_size_diff2'] = df[\"size\"] - df[\"ask_size2\"]\n    df[\"ask_price_diff\"] = df[\"price\"] - (df[\"ask_price1\"] + df[\"ask_price2\"])\/2.0\n    df[\"bid_price_diff\"] = df[\"price\"] - (df[\"bid_price1\"] + df[\"bid_price2\"])\/2.0\n    # \u8ba1\u7b97\u4ea4\u6613\u4ef7\u683c \u8d85\u8fc7 \u5356\u4e00\u4ef7\u7684\u6b21\u6570,\u8868\u660e\u4e70\u65b9\u63d0\u4ef7\u4fc3\u6210\u4ea4\u6613\n    df['over_ask1'] = (df['price'] > df['ask_price1']).astype(int)\n    # \u8ba1\u7b97\u4ea4\u6613\u4ef7\u683c \u4f4e\u4e8e \u4e70\u4e00\u4ef7\u7684\u6b21\u6570\uff0c\u8868\u660e\u5356\u65b9\u964d\u4ef7\u4fc3\u6210\u4ea4\u6613\n    df['below_bid1'] = (df['price'] > df['bid_price1']).astype(int)\n    # \u8ba1\u7b97\u4ea4\u6613\u91cf\u4e0e\u51fa\u4ef7\u91cf\u7684\u6bd4\n    df['ratio_bid'] = df['price'] \/ (df['bid_size1'] + df['bid_size2'])\n    # \u8ba1\u7b97\u4ea4\u6613\u91cf\u4e0e\u8981\u4ef7\u91cf\u7684\u6bd4\n    df['ratio_ask'] = df['price'] \/ (df['ask_size1'] + df['ask_size2'])\n        \n    # Dict for aggregations : original feature\n    create_feature_dict = {\n        'bid_size_diff1':[np.sum],\n        'bid_size_diff2':[np.sum],\n        'ask_size_diff1':[np.sum],\n        'ask_size_diff2':[np.mean],\n        'ask_price_diff':[np.mean, realized_volatility],\n        'bid_price_diff':[np.mean, realized_volatility]\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n\n    df_feature = df_feature.add_prefix('book_and_trade_')\n    stock_id = file_path_book.split('=')[1]\n    df_feature['row_id'] = df_feature['book_and_trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['book_and_trade_time_id_'], axis = 1, inplace = True)\n    return df_feature","1da6f8bf":"# data directory\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'\n\ndef calc_wap1(df):\n    # Function to calculate first WAP\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap2(df):\n    # Function to calculate second WAP\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef log_return(series):\n    # Function to calculate the log of the return\n    return np.log(series).diff()\n\ndef realized_volatility(series):\n    # Calculate the realized volatility\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    # Function to count unique elements of a series\n    return len(np.unique(series))\n\ndef book_preprocessor(file_path):\n    # Function to preprocess book data (for each stock id)\n    \n    df = pd.read_parquet(file_path)\n    \n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    \n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    \n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    \n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'price_spread2':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std],\n        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Function to get group stats for different windows (seconds in bucket)\n        \n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        \n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        \n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    \n    return df_feature\n\n\ndef trade_preprocessor(file_path):\n    # Function to preprocess trade data (for each stock id)\n    \n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n        'order_count':[np.mean,np.sum,np.max],\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Function to get group stats for different windows (seconds in bucket)\n        \n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        \n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        \n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff\/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id'], axis = 1, inplace = True)\n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    \n    def order_sum(df, sec:str):\n        new_col = 'size_tau' + sec\n        bucket_col = 'trade_seconds_in_bucket_count_unique' + sec\n        df[new_col] = np.sqrt(1\/df[bucket_col])\n        \n        new_col2 = 'size_tau2' + sec\n        order_col = 'trade_order_count_sum' + sec\n        df[new_col2] = np.sqrt(1\/df[order_col])\n        \n        if sec == '400_':\n            df['size_tau2_d'] = df['size_tau2_400'] - df['size_tau2']\n        \n\n    \n    for sec in ['','_200','_300','_400']:\n        order_sum(df_feature, sec)\n        \n    df_feature['size_tau2_d'] = df_feature['size_tau2_400'] - df_feature['size_tau2']\n    \n    return df_feature\n\n\ndef get_time_stock(df):\n    # Function to get group stats for the stock_id and time_id\n    \n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    \n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    \n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    \n    return df\n\ndef create_agg_features(train, test):\n\n    # Making agg features\n\n    train_p = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n    corr = train_p.corr()\n    ids = corr.index\n    kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n    l = []\n    for n in range(7):\n        l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n\n    mat = []\n    matTest = []\n    n = 0\n    for ind in l:\n        newDf = train.loc[train['stock_id'].isin(ind) ]\n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        mat.append ( newDf )\n        newDf = test.loc[test['stock_id'].isin(ind) ]    \n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        matTest.append ( newDf )\n        n+=1\n\n    mat1 = pd.concat(mat).reset_index()\n    mat1.drop(columns=['target'],inplace=True)\n    mat2 = pd.concat(matTest).reset_index()\n    \n    mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n    \n    mat1 = mat1.pivot(index='time_id', columns='stock_id')\n    mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n    mat1.reset_index(inplace=True)\n    \n    mat2 = mat2.pivot(index='time_id', columns='stock_id')\n    mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n    mat2.reset_index(inplace=True)\n    \n    prefix = ['log_return1_realized_volatility', 'total_volume_mean', 'trade_size_mean', 'trade_order_count_mean','price_spread_mean','bid_spread_mean','ask_spread_mean',\n              'volume_imbalance_mean', 'bid_ask_spread_mean','size_tau2']\n    selected_cols=mat1.filter(regex='|'.join(f'^{x}.(0|1|3|4|6|2|5)c1' for x in prefix)).columns.tolist()\n    selected_cols.append('time_id')\n    \n    train_m = pd.merge(train,mat1[selected_cols],how='left',on='time_id')\n    test_m = pd.merge(test,mat2[selected_cols],how='left',on='time_id')\n    \n    # filling missing values with train means\n\n    features = [col for col in train_m.columns.tolist() if col not in ['time_id','target','row_id']]\n    train_m[features] = train_m[features].fillna(train_m[features].mean())\n    test_m[features] = test_m[features].fillna(train_m[features].mean())\n\n    return train_m, test_m\n    \n    \n# def preprocessor(list_stock_ids, is_train = True):\n#     # Funtion to make preprocessing function in parallel (for each stock id)\n    \n#     # Parrallel for loop\n#     def for_joblib(stock_id):\n#         # Train\n#         if is_train:\n#             file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n#             file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n#         # Test\n#         else:\n#             file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n#             file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n#         # Preprocess book and trade data and merge them\n#         df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n#         # Return the merge dataframe\n#         return df_tmp\n    \n#     # Use parallel api to call paralle for loop\n#     df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    \n#     # Concatenate all the dataframes that return from Parallel\n#     df = pd.concat(df, ignore_index = True)\n    \n#     return df","46d5336c":"# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        book_trade_df = book_and_trade_preprocessor(file_path_book, file_path_trade)\n        df_tmp = pd.merge(df_tmp, book_trade_df, on='row_id', how='left')\n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df","d8a09887":"import pickle\nload_tabnet_data=False\nif load_tabnet_data:\n    dire = \"..\/input\/tabnet-dataset\/train_tabnet_dataset.pkl\"\n    train = pickle.load(open(dire,\"rb\"))\nelse:\n    train_stock_ids = train['stock_id'].unique()\n    train_ = preprocessor(train_stock_ids, is_train = True)\n    train = train.merge(train_, on = ['row_id'], how = 'left')\n    train = get_time_stock(train)\n    \ntrain_stock_ids = train['stock_id'].unique()\n\n# Preprocess them using Parallel and our single stock id functions\ntest_stock_ids = test['stock_id'].unique()\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\ntest = get_time_stock(test)\n\n# # Fill inf values\ntrain.replace([np.inf, -np.inf], np.nan,inplace=True)\ntest.replace([np.inf, -np.inf], np.nan,inplace=True)\n\n# # Aggregating some features\ntrain, test = create_agg_features(train,test)","6cbaca9d":"# \u4fdd\u5b58\u6570\u636e\ndire = \"train_best_timewindow.pkl\"\npickle.dump(train, open(dire, \"wb\"))","4b8c51b7":"train = train.fillna(0)\ntest = test.fillna(0)","e7fd94aa":"# test = train.loc[:3,:].drop(columns=[\"target\"])\n# test[\"stock_id\"] = 250\n# test[\"row_id\"] = 250\n# test[\"time_id\"] = -250","a85a742c":"def load_and_predict_TabNet(train, test):\n    X = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n    y = train['target']\n    X_test=test.copy()\n    X_test.drop(['time_id','row_id'], axis=1,inplace=True)\n    \n    categorical_columns = []\n    categorical_dims =  {}\n\n    for col in X.columns:\n        if  col == 'stock_id':\n            l_enc = LabelEncoder()\n            X[col] = l_enc.fit_transform(X[col].values)\n            X_test[col] = l_enc.transform(X_test[col].values)\n            categorical_columns.append(col)\n            categorical_dims[col] = len(l_enc.classes_)\n        else:\n            scaler = StandardScaler()\n            X[col] = scaler.fit_transform(X[col].values.reshape(-1, 1))\n            X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))\n    x_test = X_test.values\n    x_train = X.values\n    del X_test, X\n    gc.collect()\n    \n    clf = TabNetRegressor()\n    preds=[]\n    train_preds =[]\n    for path in modelpath:  \n        clf.load_model(path)\n        preds.append(clf.predict(x_test).squeeze(-1))\n#         train_preds.append(clf.predict(x_train).squeeze(-1))\n    tabnet_preds = np.mean(preds,axis=0)\n#     tabnet_train_preds = np.mean(train_preds,axis=0)\n    tabnet_train_preds = None\n    print(tabnet_preds)\n    return tabnet_preds, tabnet_train_preds","26c775d5":"# version = \"v1\"\n# for fold in range(5):\n#     !cp -r ..\/input\/optivertabnetmodels{str(version)}\/fold{str(fold)}\/* .\n#     !zip {str(version)}fold{str(fold)}.zip model_params.json network.pt\n    \n# modelpath = [os.path.join(\".\/\",s) for s in os.listdir(\".\/\") if (\"zip\" in s) & (version in s)]\n# print(modelpath)\n# # tabnet_preds_1, tabnet_train_preds = load_and_predict_TabNet(train, test)","d262a7d3":"# version = \"v1groupkfold\"\n# for fold in range(5):\n#     !cp -r ..\/input\/optivertabnetmodels{str(version)}\/fold{str(fold)}\/* .\n#     !zip {str(version)}fold{str(fold)}.zip model_params.json network.pt\n    \n# modelpath = [os.path.join(\".\/\",s) for s in os.listdir(\".\/\") if (\"zip\" in s) & (version in s)]\n# print(modelpath)\n# # tabnet_preds_2, tabnet_train_preds = load_and_predict_TabNet(train, test)","d1f80e90":"# version = \"v1randomgkf\"\n# for fold in range(5):\n#     !cp -r ..\/input\/optivertabnetmodels{str(version)}\/fold{str(fold)}\/* .\n#     !zip {str(version)}fold{str(fold)}.zip model_params.json network.pt\n    \n# modelpath = [os.path.join(\".\/\",s) for s in os.listdir(\".\/\") if (\"zip\" in s) & (version in s)]\n# print(modelpath)\n# tabnet_preds_3, tabnet_train_preds = load_and_predict_TabNet(train, test)","a1ca915a":"X = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\nX_test=test.copy()\nX_test.drop(['time_id','row_id'], axis=1,inplace=True)\n\ndef rmspe(y_true, y_pred):\n    # Function to calculate the root mean squared percentage error\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\nclass RMSPE(Metric):\n    def __init__(self):\n        self._name = \"rmspe\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_score):\n        \n        return np.sqrt(np.mean(np.square((y_true - y_score) \/ y_true)))\n    \n\n\ndef RMSPELoss(y_pred, y_true):\n    return torch.sqrt(torch.mean( ((y_true - y_pred) \/ y_true) ** 2 )).clone()\n\nnunique = X.nunique()\ntypes = X.dtypes\n\ncategorical_columns = []\ncategorical_dims =  {}\n\nfor col in X.columns:\n    if  col == 'stock_id':\n        l_enc = LabelEncoder()\n        X[col] = l_enc.fit_transform(X[col].values)\n        X_test[col] = l_enc.transform(X_test[col].values)\n        categorical_columns.append(col)\n        categorical_dims[col] = len(l_enc.classes_)\n    else:\n        scaler = StandardScaler()\n        X[col] = scaler.fit_transform(X[col].values.reshape(-1, 1))\n        X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))\n        \n\n\ncat_idxs = [ i for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]\n\ncat_dims = [ categorical_dims[f] for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]\n\ntabnet_params = dict(\n    cat_idxs=cat_idxs,\n    cat_dims=cat_dims,\n    cat_emb_dim=1,\n    n_d = 16,\n    n_a = 16,\n    n_steps = 2,\n    gamma = 2,\n    n_independent = 2,\n    n_shared = 2,\n    lambda_sparse = 0,\n    optimizer_fn = Adam,\n    optimizer_params = dict(lr = (2e-2)),\n    mask_type = \"entmax\",\n    scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n    scheduler_fn = CosineAnnealingWarmRestarts,\n    seed = 42,\n    verbose = 10\n    \n)\n\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport random\nfrom collections import Counter, defaultdict\nfrom sklearn import model_selection\n\n# ---- GroupKFold ----\nclass MyGroupKFold(object):\n    \"\"\"\n    GroupKFold with random shuffle with a sklearn-like structure\n    \"\"\"\n\n    def __init__(self, n_splits=4, shuffle=True, random_state=42):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def get_n_splits(self, X=None, y=None, group=None):\n        return self.n_splits\n\n    def split(self, X, y, group):\n        kf = model_selection.KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state)\n        unique_ids = X[group].unique()\n        for fold, (tr_group_idx, va_group_idx) in enumerate(kf.split(unique_ids)):\n            # split group\n            tr_group, va_group = unique_ids[tr_group_idx], unique_ids[va_group_idx]\n            train_idx = np.where(X[group].isin(tr_group))[0]\n            val_idx = np.where(X[group].isin(va_group))[0]\n            yield train_idx, val_idx\n\n\nkfold = KFold(n_splits = 5, random_state = 42, shuffle = True)\n# Create out of folds array\noof_predictions = np.zeros((X.shape[0], 1))\ntest_predictions = np.zeros(X_test.shape[0])\nfeature_importances = pd.DataFrame()\nfeature_importances[\"feature\"] = X.columns.tolist()\nstats = pd.DataFrame()\nexplain_matrices = []\nmasks_ =[]\n\nfrom sklearn.model_selection import GroupKFold\n# kfold = MyGroupKFold(n_splits=5, shuffle=True, random_state=42)\ngroups = \"time_id\"\n\nfor fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n    print(f'Training fold {fold + 1}')\n    X_train, X_val = X.iloc[trn_ind].values, X.iloc[val_ind].values\n    y_train, y_val = y.iloc[trn_ind].values.reshape(-1,1), y.iloc[val_ind].values.reshape(-1,1)\n\n\n    clf =  TabNetRegressor(**tabnet_params)\n    clf.fit(\n      X_train, y_train,\n      eval_set=[(X_val, y_val)],\n      max_epochs = 300,\n      patience = 50,\n      batch_size = 1024*20, \n      virtual_batch_size = 128*20,\n      num_workers = 4,\n      drop_last = False,\n      eval_metric=[RMSPE],\n      loss_fn=RMSPELoss\n      )\n    \n    saving_path_name = f\".\/fold{fold}\"\n    saved_filepath = clf.save_model(saving_path_name)\n    \n    explain_matrix, masks = clf.explain(X_val)\n    explain_matrices.append(explain_matrix)\n    masks_.append(masks[0])\n    masks_.append(masks[1])\n      \n    oof_predictions[val_ind] = clf.predict(X_val)\n    test_predictions+=clf.predict(X_test.values).flatten()\/5\n    feature_importances[f\"importance_fold{fold}+1\"] = clf.feature_importances_\n    \n#     stats[f'fold{fold+1}_train_rmspe']=clf.history['loss']\n#     stats[f'fold{fold+1}_val_rmspe']=clf.history['val_0_rmspe']\n    \nprint(f'OOF score across folds: {rmspe(y, oof_predictions.flatten())}')\n\n# Submission","9d8c2bf7":"# test['target'] = (tabnet_preds_1 + tabnet_preds_2)\/2 #0.19760\ntest['target'] = test_predictions  \ntest[['row_id', 'target']].to_csv('submission.csv',index = False)\ntest[['row_id', 'target']]","eee1aadb":"# Loading Data","38ff3238":"# Optiver Realized Volatility Predictions Using TabNet\n\n## The Competition\n\nIn this competition we're tasked with creating models to predict short term volatility for different stocks. The dataset includes book and trade data for different time buckets for each stock.\nOur target is realized volatility of the given stock for the next 10 minute window. To compare our predictions with the actual test set we're going to use RMSPE (Root Mean Squared Percentage Error)\n\nFor the basic features and simple feature engineering you can take a look at the [competition data page](https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/data) and [tutorial notebook](https:\/\/www.kaggle.com\/jiashenliu\/introduction-to-financial-concepts-and-data) by the competition hosts, they are really useful!\n\nMost of the features I'm going to use are common throughout the public notebooks and discussions. You can check the most important ones in the great notebooks by [@A.Sato](https:\/\/www.kaggle.com\/tommy1028\/lightgbm-starter-with-feature-engineering-idea) and [@Martin](https:\/\/www.kaggle.com\/ragnar123\/optiver-realized-volatility-lgbm-baseline)! The rest of the features are handpicked by me around several discussions and notebooks, thanks to all community!\n\n## TabNet\n\nReleased by Google Research in 2019, Attentive Interpretable Tabular Learning (aka TabNet) is basically a type of neural network using attention mechanism (sequential attention to be exact) on tabular data to learn and make predictions. It aims to combine explainability of tree based models and high performance of the neural networks.\n\n### Basic Architecture\n\n![From TabNet Paper](https:\/\/i.imgur.com\/lYbF5d4.png)\n\nTabNet encoder composed of a feature transformer, an attentive transformer and feature masking. A split block takes and divides the preprocessed representation to be used by attentive transformer in subsequent step as well as overall output which we get through aggregating masks. This process continues by number of steps, each step composed of attentive transformer, mask, feature transformer and splits. Number of steps is a hyperparameter where we can experiment on it. Each step will have their own weight at the final classification.\n\nFeature transformer is a multi layer network (including FC, BN and GRU's) some of these layers will be shared across every step while some of them are treated locally. The number of independent and shared layers are hyperparameter too and will have effect on your final predictions.\n\nOnce features has been preprocessed and transformed they passed into the attentive transformer and mask. Attentive Transformer includes a FC, BN and Sparsemax Normalization. So this block gains the information by using prior scales. In this step model learns how much each feature has been used before the current decision step. With mask model focuses on the important features and uses them.\n\nEnough details I guess, in short we can use full power of the neural networks while keeping the explainability which is pretty important for tabular data.\n\nLet's get started!","b3efd430":"# Importing Libraries","9d0049a8":"## Loding the and doing some feature engineering","8b67927f":"# Training\n","cba4a12f":"# Preprocessing the Data"}}