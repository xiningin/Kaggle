{"cell_type":{"2a1645af":"code","7a0daaa9":"code","a4c34e80":"code","f30c9c5d":"code","1b49f90e":"code","014923a4":"code","eaa044ad":"code","18032c53":"code","3e5c2953":"code","b4c290de":"code","2152f29d":"code","18842da7":"code","e71f3c2b":"code","4add4e42":"code","274ce0a2":"code","0790fea0":"code","d3192558":"code","d2eb0684":"code","eab9fffb":"code","421f2e5e":"code","5dba153f":"code","4fe986f3":"code","b5fff765":"code","68fd31ec":"code","7238ca6a":"code","2956556d":"code","2a5a0486":"code","b36a7e16":"code","76682307":"code","815ce177":"code","485950a9":"code","d00876af":"code","fb401481":"code","c8eb0345":"code","46a1a2cb":"code","f9bf1637":"code","3ad8222c":"code","627d7bcb":"code","8103064c":"code","e663503a":"code","54cd6a45":"code","9d21d26c":"code","44643701":"code","c11467e5":"code","07259a52":"code","e1aa0679":"code","9328b0b5":"code","124d739f":"code","65b1cd2f":"code","298f1235":"code","e91d62f1":"code","ecb4acef":"code","7b7cdbb1":"code","50460d9b":"code","ca98f4b9":"code","c5283645":"code","bfeceac9":"markdown","5104465d":"markdown","e31d03a6":"markdown","9d15575f":"markdown","5a604b63":"markdown","2e175140":"markdown","be1bde45":"markdown","9f9d6b5f":"markdown","3809fa77":"markdown","f5324df6":"markdown","cefdf5a9":"markdown","f3aa4fc2":"markdown","d37e55b7":"markdown","deac38c1":"markdown","fe45ae0c":"markdown","e1dc190a":"markdown","b7f4430c":"markdown","93d4e9ef":"markdown","5f42d3ae":"markdown","07ff611e":"markdown","8e5994f9":"markdown"},"source":{"2a1645af":"# Importing the basic libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","7a0daaa9":"# importing the train dataset\ndf1 = pd.read_csv('..\/input\/train.csv')\n\n# importing the test dataset\ndf2 = pd.read_csv('..\/input\/test.csv')\n\n# And let's check how the data looks like!\ndf1.sample(10)","a4c34e80":"# Creating a list of the 2 dataframes so we perform operations on both\ndfs = [df1, df2]\n\n# Intializing the output dataframe\noutput = df2[['PassengerId']]","f30c9c5d":"df1.info()","1b49f90e":"# Checking missing values\nprint(df1.isnull().sum())\nprint('-' * 20)\nprint(df2.isnull().sum())","014923a4":"# Addressing missing values\nfor df in dfs:\n    \n    # Let's drop the Cabin and some unnecessary attributes\n    df.drop(['Cabin', 'PassengerId', 'Ticket'], axis=1, inplace=True)\n    \n    # Filling Embarked with the mode\n    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n    \n    # Filling Fare with the median\n    df['Fare'].fillna(df['Fare'].median(), inplace=True)\n\ndf1.head()","eaa044ad":"for df in dfs:\n    # Let's create a family size attribute based on SibSp and Parch\n    df['FamilySize'] = df['SibSp'].astype('int') + df['Parch'].astype('int') + 1\n    \n    # Now based on family size, we might be able to check if the person is alone\n    df['IsAlone'] = (df['FamilySize'] == 1).astype('int')\n    \n    # Based on the name we might be able to check the title\n    df['Title'] = df['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    \ndf1.head()","18032c53":"df1['Title'].value_counts()","3e5c2953":"# Let's call \"Other\" every Title that is not our top 4\ntop_5_titles = df1['Title'].value_counts().head(4).index.tolist()\n\nfor df in dfs:\n    df['Title'] = df['Title'].apply(lambda x: x if x in top_5_titles else 'Other')\n    \ndf1['Title'].value_counts()","b4c290de":"df1.sample(5)","2152f29d":"# Let's get a copy of the dataset only with given ages\ndf_age = df1[df1['Age'].notna()]\ndf_age.isnull().sum()","18842da7":"# Let's first check the correlation between Age and Fare\nsns.regplot(x='Fare', y='Age', data=df_age)","e71f3c2b":"# Let's see Pclass\nsns.boxplot(x='Pclass', y='Age', data=df_age)","4add4e42":"# And FamilySize?\nprint(df_age[['FamilySize', 'Age']].corr())\ndf_age[['FamilySize', 'Age']].groupby('FamilySize').median().plot()","274ce0a2":"# How about sex?\nsns.boxplot(x=df_age['Sex'], y=df_age['Age'])","0790fea0":"# Finally Title\nsns.boxplot(x=df_age['Title'], y=df_age['Age'])","d3192558":"# Defing X matrix and target y for Age regression\nX_age = df_age[['Pclass', 'FamilySize']]\ny_age = df_age['Age']\n\nage_dummies = pd.get_dummies(df_age[['Title']])\nage_dummies.drop(columns=['Title_Other'], inplace=True)\n\nX_age = pd.concat([X_age, age_dummies], axis=1)\nX_age.head()","d2eb0684":"# Let's split the dataset into train and test so we can evaluate the performance of our age predictor\nfrom sklearn.model_selection import train_test_split\n\nX_age_train, X_age_test, y_age_train, y_age_test = train_test_split(X_age.values, y_age, test_size=0.2, random_state=4)","eab9fffb":"# Let's check how a multiple linear regression fits our data\nfrom sklearn.linear_model import LinearRegression\nage_predictor = LinearRegression().fit(X_age_train, y_age_train)\n\nyhat_age_test = age_predictor.predict(X_age_test)\n\nfrom sklearn.metrics import r2_score\nprint('R2 Score: %.2f' % r2_score(y_age_test, yhat_age_test))\nprint('Variance score: %.2f' % age_predictor.score(X_age_train, y_age_train))","421f2e5e":"# Let's train the predictor model now with the full set\nage_predictor = LinearRegression().fit(X_age.values, y_age)\nage_predictor","5dba153f":"# First let's add the Title dummies\ndf1 = pd.concat([df1, pd.get_dummies(df1[['Title']])], axis=1)\ndf2 = pd.concat([df2, pd.get_dummies(df2[['Title']])], axis=1)\ndfs = [df1, df2]\n\nfor df in dfs:\n    # Let's delete the last dummy\n    df.drop(columns=['Title_Other'], inplace=True)\n    \n    # Now let's make the prediction column\n    df['PredictedAge'] = age_predictor.predict(df[['Pclass', 'FamilySize', 'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs']].values)\n    \n    # And replace the missing values with the predicted ones\n    df['Age'].fillna(df['PredictedAge'].round(0), inplace=True)\n    \n    # Finally, we don't need the predicted column any longer\n    df.drop(columns=['PredictedAge'], inplace=True)\n    \ndf1.head(6)","4fe986f3":"# Checking missing values\nprint(df1.isnull().sum())\nprint('-' * 20)\nprint(df2.isnull().sum())","b5fff765":"# First let's finish cleaning our data\nfor df in dfs:\n    # Let's get rid of the name column\n    df.drop(columns=['Name'], inplace=True)\n\ndf1.sample(5)","68fd31ec":"# Now let's see how some features influentes on survival\nfig, ax0 = plt.subplots(2, 3,figsize=(20,10))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=df1, ax = ax0[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=df1, ax = ax0[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=df1, ax = ax0[0,2])\n\nsns.barplot(x = 'Title', y = 'Survived', data=df1, ax = ax0[1,0])\nsns.barplot(x = 'Sex', y = 'Survived', data=df1, ax = ax0[1,1])\nsns.barplot(x = 'FamilySize', y = 'Survived', data=df1, ax = ax0[1,2])","7238ca6a":"# Let's check Age and Fare\nfig1, ax1 = plt.subplots(1, 2,figsize=(10,5))\n\nsns.boxplot(x = 'Survived', y = 'Fare', data=df1, ax = ax1[0])\nsns.boxplot(x = 'Survived', y = 'Age', data=df1, ax = ax1[1])","2956556d":"# Perhaps for Age and Fare we might have a bettr understanding if we split into bins\ndf1_bins = df1[['Age', 'Fare', 'Survived']]\ndf1_bins['AgeBin'] = pd.cut(df1_bins['Age'].astype('int'), 5)\ndf1_bins['FareBin'] = pd.cut(df1_bins['Fare'].astype('int'), 5)\n\nfig2, ax2 = plt.subplots(1, 2,figsize=(10,5))\n\nsns.barplot(x = 'FareBin', y = 'Survived', data=df1_bins, ax = ax2[0])\nsns.barplot(x = 'AgeBin', y = 'Survived', data=df1_bins, ax = ax2[1])","2a5a0486":"df1.head()","b36a7e16":"# Now let's set the matrix of features of df1\nX = df1[['Pclass', 'Sex', 'Age', 'Embarked', 'IsAlone', 'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs']]\nX_dummies = pd.get_dummies(X[['Sex', 'Embarked']])\nX = pd.concat([X, X_dummies], axis=1)\nX.drop(columns=['Sex', 'Embarked', 'Sex_male', 'Embarked_S'], inplace=True)\n\n# Now let's set the matrix of features of df2\nX_pred = df2[['Pclass', 'Sex', 'Age', 'Embarked', 'IsAlone', 'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs']]\nX_pred_dummies = pd.get_dummies(X_pred[['Sex', 'Embarked']])\nX_pred = pd.concat([X_pred, X_pred_dummies], axis=1)\nX_pred.drop(columns=['Sex', 'Embarked', 'Sex_male', 'Embarked_S'], inplace=True)\n\nX.head()","76682307":"# And the dependent vector\ny = df1['Survived']","815ce177":"# Let's scale the X matrix\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_pred = scaler.transform(X_pred)\nX[0:5]","485950a9":"# Finally before actually building the prediction model, let's split df1 into train and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","d00876af":"from sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\n\n# Let's find the parameters for the model\nnum_cs = 100\nparams = {'solvers': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n          'Cs' : np.linspace(0.01, 1.0, num_cs).tolist()}\nevals = np.zeros((num_cs, 5))\n\nfor solv in params['solvers']:\n    for c in params['Cs']:\n        log = LogisticRegression(C=c, solver=solv).fit(X_train,y_train)\n        log_yhat=log.predict(X_test)\n        evals[params['Cs'].index(c), params['solvers'].index(solv)] = metrics.accuracy_score(y_test, log_yhat)","fb401481":"# Getting the optimal parameters\nmax_eval_index = np.unravel_index(evals.argmax(), evals.shape)\nsolv_opt = params['solvers'][max_eval_index[1]]\nc_opt = params['Cs'][max_eval_index[0]]\n\n# Bulding model now with optimal parameters\nlog = LogisticRegression(C=c_opt, solver=solv_opt).fit(X_train, y_train)\nlog","c8eb0345":"log_yhat = log.predict(X_test)\nlog_yhat_prob = log.predict_proba(X_test)","46a1a2cb":"from sklearn.metrics import jaccard_similarity_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import log_loss\n\nprint('Log Loss: %.2f' % log_loss(y_test, log_yhat_prob))\nprint('Jaccard: %.2f' % jaccard_similarity_score(y_test, log_yhat))\nprint (classification_report(y_test, log_yhat))","f9bf1637":"from sklearn.neighbors import KNeighborsClassifier\n\n# Let's find the best k for the model\nK_range = 11\naccuracies = np.zeros((K_range-1))\nfor n in range(1,K_range):\n    \n    #Train Model and Predict  \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    neigh_yhat=neigh.predict(X_test)\n    accuracies[n-1] = metrics.accuracy_score(y_test, neigh_yhat)\naccuracies","3ad8222c":"# Building the model for the optmal K and getting its scores\n# Finding best K\nK_opt = accuracies.tolist().index(accuracies.max()) + 1\n# Bulding the optimal model\nneigh = KNeighborsClassifier(n_neighbors = K_opt).fit(X_train, y_train)\nneigh","627d7bcb":"neigh_yhat = neigh.predict(X_test)","8103064c":"print('Jaccard: %.2f' % jaccard_similarity_score(y_test, neigh_yhat))\nprint (classification_report(y_test, neigh_yhat))","e663503a":"from sklearn import svm\n\n# Let's find the parameters for the model\nnum_cs = 100\nparams = {'kernels': ['linear', 'poly', 'rbf', 'sigmoid'],\n          'Cs' : np.linspace(0.01, 1.0, num_cs).tolist()}\nevals = np.zeros((num_cs, 4))\n\nfor kern in params['kernels']:\n    for c in params['Cs']:\n        sv = svm.SVC(C=c, kernel=kern).fit(X_train,y_train)\n        sv_yhat=log.predict(X_test)\n        evals[params['Cs'].index(c), params['kernels'].index(kern)] = metrics.accuracy_score(y_test, sv_yhat)","54cd6a45":"# Getting the optimal parameters\nmax_eval_index = np.unravel_index(evals.argmax(), evals.shape)\nkern_opt = params['kernels'][max_eval_index[1]]\nc_opt = params['Cs'][max_eval_index[0]]\n\n# Bulding model now with optimal parameters\nsv = svm.SVC(C=c_opt, kernel=kern_opt).fit(X_train, y_train)\nsv","9d21d26c":"sv_yhat = sv.predict(X_test)","44643701":"print('Jaccard: %.2f' % jaccard_similarity_score(y_test, sv_yhat))\nprint (classification_report(y_test, sv_yhat))","c11467e5":"from sklearn.tree import DecisionTreeClassifier\n\n# Let's find the best accuracy evaluation for the model\ncrits = ['entropy', 'gini']\nevals = [0, 0]\n\nfor i, criterion in enumerate(crits):\n    tree = DecisionTreeClassifier(criterion=criterion).fit(X_train, y_train)\n    tree_yhat=tree.predict(X_test)\n    evals[i] = metrics.accuracy_score(y_test, tree_yhat)\nevals","07259a52":"crit_opt = crits[evals.index(max(evals))]\n# Bulding model with optimal parameters\ntree = DecisionTreeClassifier(criterion=crit_opt).fit(X_train, y_train)\ntree","e1aa0679":"tree_yhat = tree.predict(X_test)","9328b0b5":"print('Jaccard: %.2f' % jaccard_similarity_score(y_test, tree_yhat))\nprint (classification_report(y_test, tree_yhat))","124d739f":"from sklearn import ensemble\n\n# Let's find the parameters for the model\nnum_est = 20\nparams = {'crits': ['gini', 'entropy'],\n          'est' : np.linspace(10, 200, num_est).tolist()}\nevals = np.zeros((num_est, 2))\n\nfor crit in params['crits']:\n    for est in params['est']:\n        forest = ensemble.RandomForestClassifier(n_estimators=int(est), criterion=crit).fit(X_train,y_train)\n        forest_yhat=forest.predict(X_test)\n        evals[params['est'].index(est), params['crits'].index(crit)] = metrics.accuracy_score(y_test, forest_yhat)","65b1cd2f":"# Getting the optimal parameters\nmax_eval_index = np.unravel_index(evals.argmax(), evals.shape)\ncrit_opt = params['crits'][max_eval_index[1]]\nest_opt = int(params['est'][max_eval_index[0]])\n\n# Bulding model now with optimal parameters\nforest = ensemble.RandomForestClassifier(n_estimators=est_opt, criterion=crit_opt).fit(X_train,y_train)\nforest","298f1235":"forest_yhat = forest.predict(X_test)","e91d62f1":"print('Jaccard: %.2f' % jaccard_similarity_score(y_test, forest_yhat))\nprint (classification_report(y_test, forest_yhat))","ecb4acef":"from sklearn.metrics import f1_score\n\nall_metrics = {'LogisticRegression': [jaccard_similarity_score(y_test, log_yhat), f1_score(y_test, log_yhat), log_loss(y_test, log_yhat_prob)],\n               'KNN': [jaccard_similarity_score(y_test, neigh_yhat), f1_score(y_test, neigh_yhat), np.nan],\n               'SVM': [jaccard_similarity_score(y_test, sv_yhat), f1_score(y_test, sv_yhat), np.nan],\n               'DecisionTree': [jaccard_similarity_score(y_test, tree_yhat), f1_score(y_test, tree_yhat), np.nan],\n               'RandomForest': [jaccard_similarity_score(y_test, forest_yhat), f1_score(y_test, forest_yhat), np.nan]}\n\ndf_metrics = pd.DataFrame(all_metrics, index=['Jaccard', 'F1', 'LogLoss'])\ndf_metrics","7b7cdbb1":"# Bulding the optimal model with full dataset\nbest_model = KNeighborsClassifier(n_neighbors = K_opt).fit(X, y)\nbest_model","50460d9b":"yhat = best_model.predict(X_pred)\nyhat[0:5]","ca98f4b9":"output['Survived'] = yhat\nprint(output.shape)\noutput.head()","c5283645":"output.to_csv('titanic-output.csv', index=None)","bfeceac9":"## Optimal Model","5104465d":"### Predicting Ages","e31d03a6":"Now let's get back to the __Age__ estimator","9d15575f":"## Titanic Challenge","5a604b63":"## Support Vector Machine","2e175140":"There is a bit of a correlation here!","be1bde45":"## Defining the missing Ages","9f9d6b5f":"## Random Forest Classifier","3809fa77":"### Preparing the data for the model","f5324df6":"Although a R2 score of ~0.4 is not really great, it is still a better prediction than averaging all of our missing ages","cefdf5a9":"## Logistic Regression Classifier","f3aa4fc2":"### Feature Engineering","d37e55b7":"Not helpful!","deac38c1":"That might help!","fe45ae0c":"## K Nearest Neighbor(KNN)","e1dc190a":"### Data Cleaning","b7f4430c":"That is a possible predictor, in genereal the greater the Pclass the younger people are!","93d4e9ef":"Clearly not!","5f42d3ae":"## Exploratory Data Analysis","07ff611e":"For __Age__ we might be able to use a regression model to predict the missing ages, let's first do some feature engineering","8e5994f9":"## Decision Tree"}}