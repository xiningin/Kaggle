{"cell_type":{"46c20ee7":"code","1d7218fc":"code","de9f49c4":"code","bc2ab72a":"code","bd52129b":"code","85a1c51e":"code","c8664368":"code","910568a3":"code","83fe0984":"code","d430f3a9":"code","f102a187":"code","28bc39f9":"code","5130a584":"code","9df74a02":"code","aded1aea":"code","e5ca6887":"code","f81d55be":"code","93886106":"code","37867b83":"code","dd6a7cc2":"code","a020ce14":"code","c2de2fc6":"code","4c9f1d17":"code","da53069e":"code","4b5ef0c8":"code","adb5d02a":"code","1e376db0":"code","9c20be49":"code","240d0883":"code","2354ed39":"code","ecf65951":"code","a1557133":"code","ec666a3c":"code","530fd443":"code","0d91a217":"code","09fde3ba":"code","847d8096":"code","b09c955f":"code","8e7346e8":"code","18b4f3fa":"code","6461ae8b":"code","d0a9269a":"markdown","8ee6124c":"markdown","3be70bf4":"markdown","8f9a32f8":"markdown","ab2f3211":"markdown","4ecca16c":"markdown","c4ccfff2":"markdown","e1c1eca8":"markdown","a119df00":"markdown","ab635f99":"markdown","360bf50c":"markdown","4f1af33d":"markdown","935b9ef3":"markdown","2f20afa6":"markdown","33eab3b9":"markdown","91a93e79":"markdown","bdd0a8c2":"markdown","71c1c8c4":"markdown","4b93e4a8":"markdown","a23edfb5":"markdown","950f91ef":"markdown","31bad015":"markdown"},"source":{"46c20ee7":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1d7218fc":"# Importing Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import GradientBoostingRegressor, StackingRegressor, RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor","de9f49c4":"train_df = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ntest_df = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')","bc2ab72a":"train_df.head()","bd52129b":"train_df.info()","85a1c51e":"test_df.head()","c8664368":"test_df.info()","910568a3":"train_df1 = train_df.copy()\ntrain_df = train_df.drop('Id', axis=1)","83fe0984":"train_df.describe()","d430f3a9":"test_df.describe()","f102a187":"# Let's check the label\nfig =px.histogram(train_df, x='Pawpularity')\nfig.show()","28bc39f9":"\nfig = px.box(train_df, y=\"Pawpularity\")\nfig.show()","5130a584":"plt.figure(figsize=(15, 10))\nsns.heatmap(train_df.corr())","9df74a02":"train_df.Eyes.value_counts()","aded1aea":"train_df.Face.value_counts()","e5ca6887":"sns.scatterplot(x='Face', y='Eyes',hue= 'Pawpularity' , data= train_df1)","f81d55be":"sns.scatterplot(x='Occlusion', y='Info',hue= 'Pawpularity' , data= train_df1)","93886106":"sns.pairplot(train_df)","37867b83":"# Removing the Id column\ntest_df = test_df.drop('Id', axis=1)","dd6a7cc2":"plt.figure(figsize=(15, 10))\n\nsns.heatmap(test_df.corr())","a020ce14":"X = train_df.drop('Pawpularity', axis=1)\ny = train_df['Pawpularity']\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state=42)","c2de2fc6":"gbr = GradientBoostingRegressor()\ngbr.fit(X_train, y_train)\ny_pred = gbr.predict(X_test)","4c9f1d17":"mse = mean_squared_error(y_test, y_pred, squared= False)\nprint(mse)","da53069e":"xgb = XGBRegressor()\nxgb.fit(X_train, y_train)\ny_pred = xgb.predict(X_test)\nmse1 = mean_squared_error(y_test, y_pred, squared=False)\nprint(mse1)","4b5ef0c8":"lgbm = LGBMRegressor()\nlgbm.fit(X_train, y_train)\ny_pred = lgbm.predict(X_test)\nmse2 = mean_squared_error(y_test, y_pred, squared= False)\nprint(mse2)","adb5d02a":"catboost = CatBoostRegressor()\ncatboost.fit(X_train, y_train)\ny_pred = catboost.predict(X_test)\nmse3 = mean_squared_error(y_test, y_pred, squared= False)","1e376db0":"print(mse3)","9c20be49":"tree_reg =DecisionTreeRegressor()\ntree_reg.fit(X_train, y_train)\ny_pred = tree_reg.predict(X_test)\nmse4 = mean_squared_error(y_test, y_pred, squared= False)\nprint(mse4)","240d0883":"results = [{\"Gradient Boosting\":mse,'XGBoost': mse1,'LGBMRegression': mse2,'CatBoostRegressor':mse3, 'Decision Tree': mse4} ]","2354ed39":"results_df = pd.DataFrame(results)\nresults_df","ecf65951":"estimators = [\n    ('LGBM', LGBMRegressor()),\n    ('catboost', CatBoostRegressor())]","a1557133":"reg = StackingRegressor(estimators=estimators,\n                        final_estimator=GradientBoostingRegressor(random_state=42))","ec666a3c":"reg.fit(X_train, y_train).score(X_test, y_test)","530fd443":"y_pred = reg.predict(X_test)","0d91a217":"MSE = mean_squared_error(y_test, y_pred, squared = False)\nprint(MSE)","09fde3ba":"Y_pred = reg.predict(test_df)","847d8096":"Y_pred","b09c955f":"submission_file = pd.read_csv('..\/input\/petfinder-pawpularity-score\/sample_submission.csv')","8e7346e8":"submission_file['Pawpularity'] = Y_pred","18b4f3fa":"submission_file","6461ae8b":"submission_file.to_csv(\"submission.csv\", index=False)\nsubmission_file.head()","d0a9269a":"From the above figure, we can see that\n1. Face anf eyes have some positive correlation\n2. Info and collage also have some positive correlation\n3. Occlusiona nd Human also have some positive correlation\n\nNow, let's investigate these features and why do they have these correlations.","8ee6124c":"Okay, this might not be intuitive but when both Eyes and Faces are clearly seen in the photo, the pawpularity score comes down to 40. When Eyes are facing the front the score increases to 60. ","3be70bf4":"Occlusin and Info","8f9a32f8":"# XGBOOST","ab2f3211":"Splitting the dataset","4ecca16c":"Submission file","c4ccfff2":"# Let's find the correlations in test dataset","e1c1eca8":"# Light Gradient Boosting Regressor","a119df00":"Okay, I'm not satisfied by the results. So let's try something else. Decision Trees.","ab635f99":"More on relationships","360bf50c":"Summary of the scores","4f1af33d":"Face and Eyes of the pets","935b9ef3":"As the data is already processesed we can proceed with modelling. I'm not doing any scaling as I will be using tree and gradient based methods.","2f20afa6":"This is interesting we have many more correlations between the features in test dataset.","33eab3b9":"## EDA and DATA VISUALIZATION","91a93e79":"Slightly better. Gradient Boosting Regressor had RMSE of 21.1146.\n\nLet's predict on test dataset.","bdd0a8c2":"DECISION TREES","71c1c8c4":"# Gradient Boosting Regressor","4b93e4a8":"So, from above figure we can see that most pets' pawpularity lie between 20 and 40. \nLet's dive deep into it by using a box plot.","a23edfb5":"As we have seen the best models are:\n1. Gradient Boosting Regressor\n2. LGBM Regression\n3. CatBoost Regression\n\nSo using these three models we use will make a stacked regressor using scikit learn's function.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.StackingRegressor.html","950f91ef":"# Modelling and Evaluation","31bad015":"# CatBoost Regressor"}}