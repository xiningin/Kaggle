{"cell_type":{"a39e3e92":"code","04842935":"code","1ea9cf2f":"code","5dfb104c":"code","c52bbc54":"code","62043ed9":"code","66b40d6f":"code","232ce45c":"code","7b4dc481":"code","56913ddc":"code","fa8b2a44":"code","a474e65e":"code","b479bf6e":"markdown","d35aa81c":"markdown"},"source":{"a39e3e92":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","04842935":"pip install imbalance-xgboost","1ea9cf2f":"#importing the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom category_encoders import TargetEncoder\nfrom sklearn.pipeline import Pipeline, make_pipeline\n\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nfrom sklearn.preprocessing import KBinsDiscretizer, FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imxgboost.imbalance_xgb import imbalance_xgboost as imb_xgb\n\nfrom sklearn.feature_selection import SelectFromModel\n\nimport gc\n\nimport warnings\nwarnings.filterwarnings('ignore')","5dfb104c":"df = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv')","c52bbc54":"# 560540\ndf.shape","62043ed9":"# The dataset has both categorical and numerical columns so I am replacing the missing values with the mode\nfor c in df.columns.tolist():\n    df[c].fillna(df[c].mode()[0], inplace=True)","66b40d6f":"# Creating a list of all the column names and defining the X and y variables\nused_cols = [c for c in df.columns.tolist() if c not in 'isFraud']\nX = df[used_cols]\ny = df['isFraud']","232ce45c":"encoder = TargetEncoder()\nX = encoder.fit_transform(X,y)\n\nrf = RandomForestClassifier()\n\n# Many models give their features an importance score or a coefficient. SelectFromModel uses this information to rank the features and\n#it returns the features depending on the threshold you choose. In this case I am choosing 'mean', meaning that the model will return \n# features with the score above the average.\nfeat_sel = SelectFromModel(rf, threshold='mean')\n\n# This will return an array with the indexes of the selected features\nfeat_sel.fit_transform(X, y)\n\n# In this line I am using the indexes to print the names of the columns\nprint(np.array(df.columns.tolist()[:-1])[feat_sel.get_support()])\n\n# You can use the line below to check the length of the array. 100 features were selected","7b4dc481":"# These is the list of features returned from the SelectFromModel\nto_keep = ['TransactionID','isFraud', 'TransactionDT', 'TransactionAmt', 'ProductCD',\n 'card1', 'card2', 'card4', 'card5', 'card6', 'addr2', 'dist2', 'R_emaildomain',\n 'C1', 'C5', 'C6', 'C7', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3',\n 'D4', 'D6', 'D7', 'D8', 'D11', 'D14', 'V6', 'V11', 'V12', 'V16', 'V22', 'V23', 'V24',\n 'V32', 'V36', 'V38', 'V43', 'V44', 'V51', 'V52', 'V53', 'V59', 'V61', 'V65', 'V68',\n 'V72', 'V74', 'V76', 'V79', 'V81', 'V85', 'V86', 'V95', 'V96', 'V123', 'V126',\n 'V128', 'V129', 'V131', 'V133', 'V139', 'V152', 'V156', 'V168', 'V169', 'V185',\n 'V198', 'V199', 'V200', 'V209', 'V211', 'V228', 'V238', 'V242', 'V245', 'V247',\n 'V248', 'V252', 'V254', 'V256', 'V257', 'V264', 'V279', 'V281', 'V282', 'V288',\n 'V292', 'V294', 'V301', 'V306', 'V307', 'V311', 'V312', 'V319']\n\n\n# # dropping all the columns that were not selected using the SelectFromModel\nfor c in df.columns.tolist():\n    if c not in to_keep:\n        df.drop(columns=c, inplace=True)\n        ","56913ddc":"# Below are my preprocessing steps before I put the data into a model\n\ndef stringify(data):\n    df = pd.DataFrame(data)\n    for c in df.columns.tolist():\n        df[c] = df[c].astype(str)\n    return df\n\nbinner = KBinsDiscretizer(n_bins=10, encode='ordinal')\nobjectify = FunctionTransformer(func=stringify, validate=False, check_inverse=False)\nencoder = TargetEncoder(drop_invariant=True)\n\n# In the following cells I am going to define the transformation strategy for categorical and numerical features\ncategorical = X.select_dtypes('object').columns.tolist()\nnon_numeric_transformer = Pipeline(steps=[#('imputer', imputer),#\n                                          ('objectify', objectify),\n                                            ('encoder', encoder)])\n\nnumeric = [c for c in used_cols if c not in categorical]\n\nnumeric_transformer = Pipeline(steps=[\n                                        ('binner', binner),\n                                       ('objectify', objectify),\n                                       ('encoder', encoder)])","fa8b2a44":"# This gives me a score of 71% in the leaderboard\nclf = XGBClassifier(n_jobs=-1, max_depth=15)\n\n# This preprocessor, applies the transformation to the numeric and non_numeric columns.\npreprocessor = ColumnTransformer(transformers=[('non_numeric', non_numeric_transformer, categorical),\n                                              ('numeric', numeric_transformer, numeric)])\n\nscorecard = make_pipeline(preprocessor, clf)\nscorecard.fit(X,y)","a474e65e":"# I am modeling using the weighted as my special objective\n# This model gave me a score of 90% in the leaderboard\nclf = imb_xgb(special_objective='weighted', imbalance_alpha=2)\n\npreprocessor = ColumnTransformer(transformers=[('non_numeric', non_numeric_transformer, categorical),\n                                              ('numeric', numeric_transformer, numeric)])\n\nscorecard = make_pipeline(preprocessor, clf)\nscorecard.fit(X,y)","b479bf6e":"## Improving the model accuracy\n\nThis dataset is highly imbalanced, so I decided to use the imbalance-xgboost. For more information about the package click [here](https:\/\/github.com\/jhwjhw0123\/Imbalance-XGBoost). \n\nFor special objective I am using weighted. \n\nThis software includes the codes of Weighted Loss and Focal Loss [1] implementations for Xgboost [2](<\\url> https:\/\/github.com\/dmlc\/xgboost) in binary classification problems. The principal reason for us to use Weighted and Focal Loss functions is to address the problem of label-imbalanced data. \n\nXGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting","d35aa81c":"<h1><center>\nIEEE-CIS FRAUD DETECTION\n<\/center><\/h1>\n\n![Fraud Image][1]\n\n[1]: https:\/\/www.nice.com\/engage\/blog\/wp-content\/uploads\/2019\/11\/Blog-682X325-83.png\n\n\nIEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. They work accross work accross several areas in AI and machine learning, including deep neural networks, fuzzy systems, evolutionary computation and swarm intelligence.\n\nIEEE has partnered with Vesta Corporation, which is the leading payment service company to see the best solutions for fraud prevention in the industry. Vesta has invited the public to participate. The link for the competition can be found [here](https:\/\/www.kaggle.com\/c\/ieee-fraud-detection).\n\nThe goal of this project is to improve the efficacy of fraudulent transaction alerts for millions of people aroung the world, help businesses reduce their fraud losses and increase their revenues, and last but not least, improve the customer experience by reducing the insult rate (false positives)."}}