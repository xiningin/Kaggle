{"cell_type":{"f1db7267":"code","a2b5789c":"code","add17df4":"code","65d83af3":"code","4939abf1":"code","12c3e9cb":"code","e393fc30":"code","882cf930":"code","2e8b501c":"code","58a8457c":"code","0a4f930c":"code","4b65d2e9":"code","7e063949":"code","d22b9707":"code","816839e0":"code","cfd1e646":"code","6d3aa793":"code","792f0df1":"code","432dbd05":"code","09371404":"code","fd665e25":"code","a8adaaf3":"code","eb765bd2":"code","6956ec59":"code","6c605a93":"code","a17ba239":"code","1adde1e9":"markdown","880101b4":"markdown","2a028dc2":"markdown","510f3fc6":"markdown","368d83ef":"markdown","72d22d3d":"markdown","25224812":"markdown"},"source":{"f1db7267":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a2b5789c":"df = pd.read_csv('\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')\nprint(df)","add17df4":"df.isnull().sum()","65d83af3":"df = df.fillna(0)\ndf","4939abf1":"df.isnull().sum()\n","12c3e9cb":"x = df.drop('status', axis=1)\nx.head(10)","e393fc30":"y = df['status']\ny.head(10)","882cf930":"\nfrom sklearn import preprocessing \nlabel_encoder = preprocessing.LabelEncoder()  \nx= x.apply(label_encoder.fit_transform)\nprint(x)","2e8b501c":"y= label_encoder.fit_transform(y)\nprint(y)","58a8457c":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state= 109)","0a4f930c":"#Build Model with GradientBoostingClassifier and GridSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler","4b65d2e9":"#separating numerical and categorical col\nnumerical_col = ['ssc_p', 'hsc_p', 'degree_p', 'etest_p', 'mba_p']\ncategorical_col = ['gender', 'ssc_b', 'hsc_b', 'hsc_s', 'degree_t', 'workex', 'specialisation']","7e063949":"#Creating Pipeline to Missing Data \n\n#inpute numerical missing data with median\nnumerical_transformer = make_pipeline(SimpleImputer(strategy='median'),\n                                      StandardScaler())\n\n#inpute categorical data with the most frequent value of the feature and make one hot encoding\ncategorical_transformer = make_pipeline(SimpleImputer(strategy='most_frequent'),\n                                        OneHotEncoder(handle_unknown='ignore'))\n\npreprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_col),\n                                               ('cat', categorical_transformer, categorical_col)])","d22b9707":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n","816839e0":"clf = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', GradientBoostingClassifier())])","cfd1e646":"#Using GradientBoostingClassifier with GridSearchCV to get better parameters\n\nparam_grid = {'model__learning_rate':[0.001, 0.01, 0.1], \n              'model__n_estimators':[100, 150, 200, 300, 350, 400]}\n\n#param_grid = {'model__learning_rate':[0.1], \n#              'model__n_estimators':[150]}\n\n#use recall score\ngrid = GridSearchCV(clf, param_grid, cv=10, scoring='accuracy', n_jobs=-1)","6d3aa793":"grid.fit(x_train, y_train)","792f0df1":"grid.best_params_","432dbd05":"from sklearn.metrics import classification_report,confusion_matrix\npredictions = grid.predict(x_test)","09371404":"print(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","fd665e25":"import matplotlib.pyplot as plt\nimport seaborn as sns","a8adaaf3":"sns.countplot(x=\"degree_t\", data=df, hue='specialisation')\nplt.title(\"Candidate degree vs Placement\")\nplt.xlabel(\"Courses in degree\")\nplt.ylabel(\"Number of candidate\")\nplt.show()","eb765bd2":"df.plot.scatter(x='salary', y='mba_p',title='Candidate Performance')","6956ec59":"df['salary'].plot.hist()","6c605a93":"df['status'].value_counts().sort_index().plot.bar()","a17ba239":"df.drop(['sl_no','ssc_p','hsc_p','etest_p'], axis=1).plot.line(title='Candidate Performance')","1adde1e9":"# Train and Test Split","880101b4":"#  GradientBoostingClassifier and GridSearchCV","2a028dc2":"# Plotting","510f3fc6":"# GridSearchCV:\n\n  GridSearchCV is a library function that is a member of sklearn's model_selection package. It helps to loop through predefined hyperparameters and fit your estimator (model) on your training set. So, in the end, you can select the best parameters from the listed hyperparameters.\n  \n![](https:\/\/imgur.com\/HSh9mej.png)\n\n# Gradient Boosting Classifier:\n  \n  Gradient boosting is a machine learning technique for regression and classification problems that produce a prediction model in the form of an ensemble of weak prediction models\n  \n![](https:\/\/imgur.com\/aPnHLAE.png)","368d83ef":"# Label Encoding","72d22d3d":"# References:\n\n1. [https:\/\/medium.com\/better-programming\/comparing-grid-and-randomized-search-methods-in-python-cd9fe9c3572d](http:\/\/)\n2. https:\/\/medium.com\/@kesarimohan87\/model-selection-using-cross-validation-and-gridsearchcv-8756aac1e9d7\n3. https:\/\/medium.com\/datadriveninvestor\/an-introduction-to-grid-search-ff57adcc0998\n","25224812":"***Pros:***\n\n* Exhaustive search, will find the absolute best way to tune the hyperparameters based on the training set.\n* Easy to find the optimal hyperparameters of a model which results in the most 'accurate' predictions. \n* More \u201cefficient\u201d use of data as every observation is used for both training and testing.\n\n***Cons:***\n\n* Time-consuming and danger of overfitting.\n* That when it comes to dimensionality, it suffers when evaluating the number of hyperparameters grows exponentially."}}