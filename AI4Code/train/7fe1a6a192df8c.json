{"cell_type":{"ba7ffcea":"code","4d388821":"code","a69084b5":"code","4e3779be":"code","c84fe58f":"code","14f04309":"code","c57a5d98":"code","09117312":"code","e4bca24c":"code","7179817c":"code","ebbbe3fc":"code","5ef69e89":"code","59ccda64":"code","9921642f":"code","cb0c3a22":"code","e11fcd5b":"code","0b8adbf9":"code","253693b9":"code","d1bac8c5":"code","18b9d986":"code","25d342c4":"code","45b636a5":"code","2a54eb3b":"code","059e1f7a":"code","06276d7b":"code","082fa73a":"code","58edbb95":"code","957af519":"code","45471f8e":"code","64a89902":"code","0a1c3cc5":"code","f4a2a42e":"code","cb260779":"code","7ac402eb":"code","3436bbbf":"code","8e56ba69":"code","7372a997":"code","aa482e8e":"code","bc583e3a":"code","90f12179":"code","e424fe1c":"code","e79b1d0b":"code","66f181ab":"code","d03a07f2":"code","f95b0212":"code","29736a87":"code","179550df":"code","c8efc7e0":"code","6fdabbda":"code","41dd621c":"code","5eeb041a":"code","da229463":"code","c2923fdf":"code","4d4cd1f9":"code","53be613d":"code","95c97703":"code","5f097538":"code","493f9e33":"code","4e336e02":"code","e7f6569e":"code","81269670":"markdown","8c78c92f":"markdown","96c99628":"markdown","b0ae773c":"markdown","01b2d59e":"markdown","75d63173":"markdown","6d0d180d":"markdown","3b3e8524":"markdown","5ff04bcb":"markdown","c2d7acd3":"markdown","cfec2d20":"markdown","2860b467":"markdown","5a051ae1":"markdown","05b76650":"markdown","2a980eb8":"markdown","5d170c67":"markdown","0e1e3d75":"markdown","ec807cf0":"markdown","1d718b80":"markdown","65845625":"markdown","21e9b8f3":"markdown","421872de":"markdown","6dfc5214":"markdown","3f31bf3e":"markdown","9510dd85":"markdown"},"source":{"ba7ffcea":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns ","4d388821":"train = pd.read_csv('..\/input\/factelytics-siom\/Train data.csv')\ntest = pd.read_csv('..\/input\/factelytics-siom\/Test data.csv')\ntrain.drop('id', axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)","a69084b5":"print('Total records in train is', train.shape[0], ', and in test is '+ str(test.shape[0])+ '.')","4e3779be":"train.isnull().sum()","c84fe58f":"from sklearn.ensemble import RandomForestClassifier\nX = train.copy()\n\n\ny = train.term_deposit_subscribed\nX = X.drop('term_deposit_subscribed', axis=1)\n\n\nX = X.fillna(-999)\n\n\nfor c in train.columns[train.dtypes == 'object']:\n    X[c] = X[c].factorize()[0]\n    \nrf = RandomForestClassifier()\nrf.fit(X,y)\n\nplt.plot(rf.feature_importances_)\nplt.ylabel('Importance of Feature')\nplt.xticks(np.arange(X.shape[1]), X.columns.tolist(), rotation=90);","14f04309":"corr=abs(train.corr())\ncore = corr.term_deposit_subscribed.sort_values(ascending=False)\ncore","c57a5d98":"train.groupby('prev_campaign_outcome')['term_deposit_subscribed'].value_counts()","09117312":"train.groupby('marital')['term_deposit_subscribed'].value_counts()","e4bca24c":"train.groupby('job_type')['term_deposit_subscribed'].value_counts()","7179817c":"(train.balance<0).value_counts()","ebbbe3fc":"print(\"Subsctiption breakdown of poeple with negative balance\\n\", \n      train.loc[train.balance<0, 'term_deposit_subscribed'].value_counts(), '\\n')\n\nprint(\"Subsctiption breakdown of poeple with positive balance\\n\", \n      train.loc[train.balance>0, 'term_deposit_subscribed'].value_counts())","5ef69e89":"print(\"Subsctiption breakdown of poeple with balancemore than 1000\\n\", \n      train.loc[train.balance>1000, 'term_deposit_subscribed'].value_counts())\n\nprint(\"Subsctiption breakdown of poeple with balancemore than 2000\\n\", \n      train.loc[train.balance>2000, 'term_deposit_subscribed'].value_counts())","59ccda64":"months = train.month.unique().tolist()\nfor m in months:\n    print(m,'\\t')\n    print(train.loc[train.month==m, 'term_deposit_subscribed'].value_counts(), '\\t')","9921642f":"train.groupby('communication_type').term_deposit_subscribed.value_counts()","cb0c3a22":"train.groupby('month').communication_type.value_counts()","e11fcd5b":"plt.figure(figsize=(10,5))\nsns.kdeplot(data=train, x=train.loc[train.last_contact_duration<1200, 'last_contact_duration'], hue='term_deposit_subscribed', multiple = 'fill')\nplt.xlabel('Time in seconds')","0b8adbf9":"plt.figure(figsize=(15,4))\nplt.tight_layout()\n\nplt.subplot(1,3,1)\nplt.hist((train.balance, test.balance), range=(-10000,30000), bins = 10, log = 1)\nplt.title('Balance Distribution')\nplt.xlabel('Balance')\n\nplt.subplot(1,3,2)\nplt.hist((train.customer_age, test.customer_age), bins = 10)\nplt.title('Age Distribution')\nplt.xlabel('Age')\n\nplt.subplot(1,3,3)\nplt.hist((train.month, test.month), bins = 10)\nplt.title('Month Distribution')\nplt.xlabel('Month')\n\nplt.show()","253693b9":"plt.figure(figsize=(13,4))\nplt.tight_layout()\n\nplt.subplot(1,3,1)\nplt.hist((train.days_since_prev_campaign_contact, test.days_since_prev_campaign_contact), bins = 10)\nplt.title('Days since previous contact distribution')\nplt.xlabel('Days')\n\nplt.subplot(1,3,2)\nplt.hist((train.num_contacts_prev_campaign, test.num_contacts_prev_campaign), bins = 15, log=1)\nplt.title('Number of contacts distribution')\nplt.xlabel('Count')\n\nplt.subplot(1,3,3)\nplt.hist((train.last_contact_duration, test.last_contact_duration), bins = 10, log=1)\nplt.title('Call Duration distribution')\nplt.xlabel('Time in Seconds')\n\nplt.tight_layout()\n\nplt.show()","d1bac8c5":"#Checking the distribution between train and test data. This will help us tune the model to get highest possible F1 score\ncolumns = test.columns.to_list()\ncolumns.remove('day_of_month')\ncolumns.remove('month')\ncolumns.remove('balance')\ncolumns.remove('customer_age')\ncolumns.remove('days_since_prev_campaign_contact')\ncolumns.remove('num_contacts_prev_campaign')\ncolumns.remove('last_contact_duration')\nfor c in columns:\n    print(train[c].value_counts().sort_index(),\"\\n\", test[c].value_counts().sort_index(), \"\\n\") ","18b9d986":"corr = abs(train.corr())\ncore = corr.balance.sort_values(ascending = False)\ncore","25d342c4":"print(\"Rows where both customer age and account balance are not avaiable in : \",len(train.loc[(train.balance.isnull()==True) & (train.customer_age.isnull()==True)]), \"\\nWe will drop these rows.\")\n#Dropping specified rows \ntrain = train.loc[(train.balance.isnull()!=True) | (train.customer_age.isnull()!=True)]\ntrain.reset_index(drop=True, inplace =True)\nprint('')","45b636a5":"balances = [-5000, 0, 5000, 10000, 15000, 20000, 25000, 30000, 35000, 40000, 45000, 50000, 60000, 70000, 80000, 90000, 100000, 110000]\nfor b in balances : \n    m1 = train.customer_age.isnull() == True\n    m2 = (train.balance<(b+7500))\n    m3 = (train.balance>(b-7500)) \n    value = round(train.loc[m2 & m3, 'customer_age'].mean(),0) \n    i = train.loc[m1 & m2 & m3,'customer_age'].index\n    train.loc[i,'customer_age'] = value\ndel m1, m2, m3, i \n\n\nfor b in balances : \n    m1 = test.customer_age.isnull() == True\n    m2 = (test.balance<(b+7500))\n    m3 = (test.balance>(b-7500))\n    value = round(test.loc[m2 & m3, 'customer_age'].mean(),0) \n    i = test.loc[m1 & m2 & m3,'customer_age'].index\n    test.loc[i,'customer_age'] = value\ndel m1, m2, m3, i, balances, b","2a54eb3b":"ages = train.customer_age.unique().tolist()\nfor age in ages:\n    value = train.loc[(train.customer_age == age), ['balance']].mean().get('balance')\n    m1 =  train.customer_age == age\n    m2 = train.balance.isnull()==True\n    i = train.loc[m2 & m1, 'balance'].index\n    train.loc[i, 'balance'] = value\ndel ages, m1, m2, i\n\n\n\nages = test.customer_age.unique().tolist()\nfor age in ages:\n    value = test.loc[(test.customer_age == age), ['balance']].mean().get('balance')\n    m1 =  test.customer_age == age\n    m2 = test.balance.isnull()==True\n    i = test.loc[m2 & m1, 'balance'].index\n    test.loc[i, 'balance'] = value\ndel ages, m1, m2, i","059e1f7a":"i = test.loc[(test.balance.isnull()==True) & test.customer_age.isnull()==True].index\nmean_balance = test.balance.mean()\nmean_age = round(test.customer_age.mean(), 0)\ntest.loc[i, ['balance', 'customer_age']] = mean_balance, mean_age","06276d7b":"train['personal_loan'] = train['personal_loan'].replace({'no':0, 'yes':1})\ntest['personal_loan'] = test['personal_loan'].replace({'no':0, 'yes':1})\n\nfrom sklearn.impute import KNNImputer\nknn = KNNImputer(n_neighbors=10)\n\ncol_pl = ['balance', 'personal_loan']\nknn.fit(train[col_pl])\n\nr = pd.DataFrame(np.round(knn.transform(train[col_pl]), 0), columns=col_pl)\ntrain['personal_loan'] = r['personal_loan'].astype('int64')\n\nr = pd.DataFrame(np.round(knn.transform(test[col_pl]), 0), columns=col_pl)\ntest['personal_loan'] = r['personal_loan'].astype('int64')","082fa73a":"train['housing_loan'] = train['housing_loan'].replace({'no':0, 'yes':1})\ntest['housing_loan'] = test['housing_loan'].replace({'no':0, 'yes':1})\n\ntrain['loan'] = (train.personal_loan | train.housing_loan)\ntrain['loan'] = train['loan'].astype('int64')\n\ntest['loan'] = (test.personal_loan | test.housing_loan)\ntest['loan'] = test['loan'].astype('int64')\n\n\ntrain.drop('personal_loan', axis=1, inplace=True)\ntest.drop('personal_loan', axis=1, inplace=True)\n\ntrain.drop('housing_loan', axis=1, inplace=True)\ntest.drop('housing_loan', axis=1, inplace=True)","58edbb95":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder","957af519":"train.last_contact_duration.fillna(value = train.last_contact_duration.mean(), inplace= True)\ntest.last_contact_duration.fillna(value = test.last_contact_duration.mean(), inplace=True)","45471f8e":"#Label encoding of education as the data is ordinal \ntrain['education'] = train['education'].replace({'unknown':0, 'primary':1, 'secondary':2, 'tertiary':3})\ntest['education'] = test['education'].replace({'unknown':0, 'primary':1, 'secondary':2, 'tertiary':3})","64a89902":"scaler = StandardScaler()\nscaler.fit(train.education.values.reshape(-1,1))\n\nnew_edu = scaler.transform(train.education.values.reshape(-1,1))\ntrain['education'] = new_edu\n\nnew_edu = scaler.transform(test.education.values.reshape(-1,1))\ntest['education'] = new_edu","0a1c3cc5":"train['last_contact_duration'] = np.clip(train.last_contact_duration,a_min=None, a_max=3000) \ntest['last_contact_duration'] = np.clip(test.last_contact_duration,a_min=None, a_max=3000) \n\nscaler = StandardScaler()\nscaler.fit(train.last_contact_duration.values.reshape(-1,1))\n\nnew_con = scaler.transform(train.last_contact_duration.values.reshape(-1,1))\ntrain['last_contact_duration'] = new_con\n\nnew_con = scaler.transform(test.last_contact_duration.values.reshape(-1,1))\ntest['last_contact_duration'] = new_con","f4a2a42e":"train['num_contacts_prev_campaign'] = np.clip(train.num_contacts_prev_campaign,a_min=None, a_max=26) \ntest['num_contacts_prev_campaign'] = np.clip(test.num_contacts_prev_campaign,a_min=None, a_max=26) \n\nscaler = StandardScaler()\nscaler.fit(train.num_contacts_prev_campaign.values.reshape(-1,1))\n\nnew_con = scaler.transform(train.num_contacts_prev_campaign.values.reshape(-1,1))\ntrain['num_contacts_prev_campaign'] = new_con\n\nnew_con = scaler.transform(test.num_contacts_prev_campaign.values.reshape(-1,1))\ntest['num_contacts_prev_campaign'] = new_con","cb260779":"train['balance'] = np.clip(train.balance,a_min=None, a_max=40000) \ntest['balance'] = np.clip(train.balance,a_min=None, a_max=40000) \n\nscaler = StandardScaler()\nscaler.fit(train.balance.values.reshape(-1,1))\n\nnew_balance = scaler.transform(train.balance.values.reshape(-1,1))\ntrain['balance'] = new_balance\n\nnew_balance = scaler.transform(test.balance.values.reshape(-1,1))\ntest['balance'] = new_balance","7ac402eb":"scaler = StandardScaler()\nscaler.fit(train.customer_age.values.reshape(-1,1))\n\nnew_age = scaler.transform(train.customer_age.values.reshape(-1,1))\ntrain['customer_age'] = new_age\n\nnew_age = scaler.transform(test.customer_age.values.reshape(-1,1))\ntest['customer_age'] = new_age","3436bbbf":"le = LabelEncoder()\nle.fit(train.month)\nnew_m = le.transform(train.month)\ntrain['month'] = new_m\n\nnew_m = le.transform(test.month)\ntest['month'] = new_m\n\nscaler = StandardScaler()\nscaler.fit(train.month.values.reshape(-1,1))\n\nnew_month = scaler.transform(train.month.values.reshape(-1,1))\ntrain['month'] = new_month\n\nnew_month = scaler.transform(test.month.values.reshape(-1,1))\ntest['month'] = new_month","8e56ba69":"le = LabelEncoder()\nle.fit(train.job_type)\nnew_m = le.transform(train.job_type)\ntrain['job_type'] = new_m\n\nnew_m = le.transform(test.job_type)\ntest['job_type'] = new_m\n\nscaler = StandardScaler()\nscaler.fit(train.job_type.values.reshape(-1,1))\n\nnew_jt = scaler.transform(train.job_type.values.reshape(-1,1))\ntrain['job_type'] = new_jt\n\nnew_jt = scaler.transform(test.job_type.values.reshape(-1,1))\ntest['job_type'] = new_jt","7372a997":"scaler = StandardScaler()\nscaler.fit(train.day_of_month.values.reshape(-1,1))\n\nnew_day = scaler.transform(train.day_of_month.values.reshape(-1,1))\ntrain['day_of_month'] = new_day\n\nnew_day = scaler.transform(test.day_of_month.values.reshape(-1,1))\ntest['day_of_month'] = new_day","aa482e8e":"train['marital'] = train['marital'].replace({'single':3, 'married':1, 'divorced':2})\ntest['marital'] = test['marital'].replace({'single':3, 'married':1, 'divorced':2})","bc583e3a":"knn = KNNImputer(n_neighbors=10)\n\ncol_marital = ['customer_age', 'education', 'marital']\nknn.fit(train[col_marital])\n\nr = pd.DataFrame(np.round(knn.transform(train[col_marital]), 0), columns=col_marital)\ntrain['marital'] = r['marital']\n\nr = pd.DataFrame(np.round(knn.transform(test[col_marital]), 0), columns=col_marital)\ntest['marital'] = r['marital']","90f12179":"train = train.join(pd.get_dummies(train.marital, prefix = 'marital'))\ntrain.drop('marital', axis=1, inplace = True)\n\ntest = test.join(pd.get_dummies(test.marital, prefix='marital'))\ntest.drop('marital', axis=1, inplace = True)","e424fe1c":"train = train.join(np.round(pd.get_dummies(train.default, prefix='default'), 0))\ntrain.drop('default', axis=1, inplace = True)\n\ntest = test.join(pd.get_dummies(test.default, prefix = 'default'))\ntest.drop('default', axis=1, inplace = True)","e79b1d0b":"train = train.join(pd.get_dummies(train.communication_type, prefix='communication_type'))\ntrain.drop('communication_type', axis=1, inplace = True)\n\ntest = test.join(pd.get_dummies(test.communication_type, prefix = 'communication_type'))\ntest.drop('communication_type', axis=1, inplace = True)","66f181ab":"train = train.join(pd.get_dummies(train.prev_campaign_outcome, prefix='prev_campaign_outcome'))\ntrain.drop('prev_campaign_outcome', axis=1, inplace = True)\n\ntest = test.join(pd.get_dummies(test.prev_campaign_outcome, prefix = 'prev_campaign_outcome'))\ntest.drop('prev_campaign_outcome', axis=1, inplace = True)","d03a07f2":"knn = KNNImputer(n_neighbors=10)\n\ncol_days = ['month', 'balance', 'customer_age', 'education', 'days_since_prev_campaign_contact']\nknn.fit(train[col_days])\n\nr = pd.DataFrame(knn.transform(train[col_days]), columns=col_days)\ntrain['days_since_prev_campaign_contact'] = r['days_since_prev_campaign_contact']\n\nr =  pd.DataFrame(knn.transform(test[col_days]), columns=col_days)\ntest['days_since_prev_campaign_contact'] = r['days_since_prev_campaign_contact']","f95b0212":"knn = KNNImputer(n_neighbors=10)\ncol_nums = ['day_of_month', 'num_contacts_in_campaign']\nknn.fit(train[col_nums])\nr =  pd.DataFrame(knn.transform(train[col_nums]), columns=col_nums)\ntrain['num_contacts_in_campaign'] = r['num_contacts_in_campaign']\n\n\nr =  pd.DataFrame(knn.transform(test[col_nums]), columns=col_nums)\ntest['num_contacts_in_campaign'] = r['num_contacts_in_campaign']","29736a87":"train['num_contacts_in_campaign'] = np.clip(train.num_contacts_in_campaign,a_min=None, a_max=30) \ntest['num_contacts_in_campaign'] = np.clip(test.num_contacts_in_campaign,a_min=None, a_max=30) \n\nscaler = StandardScaler()\nscaler.fit(train.num_contacts_in_campaign.values.reshape(-1,1))\n\nnew_con = scaler.transform(train.num_contacts_in_campaign.values.reshape(-1,1))\ntrain['num_contacts_in_campaign'] = new_con\n\nnew_con = scaler.transform(test.num_contacts_in_campaign.values.reshape(-1,1))\ntest['num_contacts_in_campaign'] = new_con","179550df":"train['days_since_prev_campaign_contact'] = np.clip(train.days_since_prev_campaign_contact,a_min=None, a_max=600) \ntest['days_since_prev_campaign_contact'] = np.clip(test.days_since_prev_campaign_contact,a_min=None, a_max=600) \n\nscaler = StandardScaler()\nscaler.fit(train.days_since_prev_campaign_contact.values.reshape(-1,1))\n\nnew_con = scaler.transform(train.days_since_prev_campaign_contact.values.reshape(-1,1))\ntrain['days_since_prev_campaign_contact'] = new_con\n\nnew_con = scaler.transform(test.days_since_prev_campaign_contact.values.reshape(-1,1))\ntest['days_since_prev_campaign_contact'] = new_con","c8efc7e0":"print(train.isnull().sum(axis=0).sum(), test.isnull().sum(axis=0).sum())","6fdabbda":"y = train['term_deposit_subscribed'].values\nX = train.drop('term_deposit_subscribed', axis=1)\nx = X.values\nx.shape","41dd621c":"print(train.term_deposit_subscribed.value_counts().to_list())\nprint(\"Ration of No to Yes is \" ,train.term_deposit_subscribed.value_counts()[1]\/train.term_deposit_subscribed.value_counts()[0])","5eeb041a":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=14)\n\nfor train_index, test_index in sss.split(x, y):\n    xtrain, xval = x[train_index], x[test_index]\n    ytrain, yval = y[train_index], y[test_index]","da229463":"import tensorflow as tf ","c2923fdf":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(256, activation='relu', input_shape=(23,)),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(8, activation='relu'),\n    #tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(4, activation='relu'),\n    #tf.keras.layers.Dropout(0.4), \n    tf.keras.layers.Dense(1, activation='sigmoid')\n])","4d4cd1f9":"model.summary()","53be613d":"from keras import backend as K\n\ndef f1(y_true, y_pred):\n    \n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    f1 = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1\n\n\nmodel.compile(loss='binary_crossentropy', optimizer= \"adam\", metrics=[f1,'AUC'])","95c97703":"annealer = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-3)","5f097538":"model.fit(xtrain, ytrain, epochs=200, validation_data=(xval, yval), shuffle=True, callbacks=[annealer])","493f9e33":"xtest = pd.read_csv('..\/input\/factelytics-siom\/Test data.csv')\nsample = xtest['id']\nsample = pd.DataFrame(sample)","4e336e02":"p_y = model.predict(test)\np_y = p_y.flatten()\nprint(p_y)\np_y=np.round(p_y,0)\nprint(p_y)\nsample['term_deposit_subscribed'] = p_y","e7f6569e":"sample.to_csv('Predictions.csv', index=False)","81269670":"## The distribution of train and test appears to be the same. Thus, working on training set will eventually improve the test set. ","8c78c92f":"## Single customers are more inclined towards taking up term deposit subscription.","96c99628":"# Data preprocessing \n## Working with Nan values","b0ae773c":"# NOTE:\n## This is not the EXACT \"Portuguese Bank Marketing\" dataset. It has  contains a few different columns, however, this dataset is **almost similar** and all the concepts can be conveniently translated as most of the columns are identical.  \n## The dataset used can be found here - https:\/\/www.kaggle.com\/jinxzed\/av-hacklive-guided-hackathon\n## In-depth visualisations, analysis, insights and strong predictive models.\n\n## This dataset was provided in \u201cFactElytics\u201d organized by Drishti, the Annual fest of SIOM Nashik.\n## From a total of 330 participants, 87 teams were shortlisted after a quiz round held on Dare2Compete. Making it to the top 8 Teams, being the only individual participant, based on F1 score on a model for a banking institution\u2019s campaigning dataset.\n#### If this notebook helps you, an upvote would be huge!","01b2d59e":"## Customers who subscribed in the previous campaign are most likely to subscribe again. ","75d63173":"## Around 3900 bank accounts with balances less than 0, implying customers tried to make payments larger than the amount of money in thier account.","6d0d180d":"## Balance and customer age have highest correlation. We will use either of these to fill NaN values of the other. This is more effective and gives higher accuracy than filling the NaN values of any feature with its Mean or Median. \n\n### NOTE : We are not considering the feature \"days_since_prev_campaign_contact\" as 80% NaN values in the dataset, hence the correlation value is not an accurate representation for this particular feature. ","3b3e8524":"# Feature generation\n## Combining \"personal_loan\" and \"housing_loan\" to generate a new feature \"loan\"","5ff04bcb":"## Only about 5 percent customers with negative bank balance tend to not shy away from subscribing for term deposit, , whereas for customers with positive balances have a subscription percentage of 11%. ","c2d7acd3":"## Because of the skewed dataset (ratio of term deposit not subscribed to term deposit  subscribed is around 0.1), we will use Stratified shuffle split so the distribution of target values are the same in train and validation set. This is a  very significant when working with skewed data.","cfec2d20":"## Data scaling and imputing ","2860b467":"### For a Neural Network architecture, One-Hot encoding is more effective in recognizing patterns than Label\/Frequency Encoding. \n### Features are scaled about mean=0 and unit variance. Gradients in Neural Networks tend to explode or vanish on unscaled features.\n### Normalising help Neural Networks converge faster to the \"Saddle point\"","5a051ae1":"# -----------------------------------------------------------------------------------------------------------","05b76650":"## Month has a significant impact on the **reach** as well as the success of the campaign.\n","2a980eb8":"## A very peculiar insight here is that \"unknown\" is very prominent on the month of May and June. For the remaning months, it has negligible contribution in terms of campaign reach.\n","5d170c67":"__________________________________________________________________________________________________________________________","0e1e3d75":"## Cellular and telephone communication types are the most efficient with 13% success rate. \"Unknown\" has an efficiency of only 3.5%","ec807cf0":"## If the last contact duration was high, the chances of the customer subscribing is high, as longer duration implies interest of the customer in services offered by the bank. ","1d718b80":"# -----------------------------------------------------------------------------------------------------------","65845625":"## Building Neural Network model.","21e9b8f3":"## Converting dataset to feed into Neural Network","421872de":"## Customers with job types as management, student and unemployed have a higher chance of taking the subscription. ","6dfc5214":"## Further, subscription percentage rate gradually increases with increase in balance.\n","3f31bf3e":"# Comparing the distribution of test and training set.\n### If the distribution is different, improving our model in training set will not result in any imporvement in test set. ","9510dd85":"## Few features are very dominant in the model, namely, Last contact duration, month, previous campaign outcome, day of month, balance and customer age. "}}