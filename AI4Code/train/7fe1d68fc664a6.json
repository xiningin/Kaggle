{"cell_type":{"76f9bf9e":"code","82131f57":"code","71a5c4dc":"code","f91cd811":"code","f2365f2c":"code","549bdae4":"code","7b8f5f60":"code","f301f12e":"code","2bc19fea":"code","fe5e955b":"code","48273e50":"code","308e2794":"code","39e59190":"code","376a8c0a":"code","158dd700":"code","2e801538":"code","614e8613":"code","1acda4b0":"code","661ad465":"code","412bd3ed":"code","78e0956f":"code","9712be94":"code","a44aa328":"code","7a2a6f5e":"code","6ebb17cb":"code","d807ff13":"code","82032a71":"code","cb6d8cfa":"code","7abe09f3":"code","e9559992":"code","5972bd60":"code","4a60403b":"code","774789fa":"code","32084644":"code","cad30868":"code","7bda6128":"code","976e66f4":"code","82de18eb":"code","efdf6fd4":"code","5f717513":"code","e395b16a":"code","ed426666":"code","103ccd0c":"code","02a7f23e":"code","77abc5f0":"code","ecbe1f3e":"code","324eebdd":"code","f31cdbef":"code","06c0fa3c":"code","fc73a6e4":"code","30ec01b1":"code","cedfe976":"code","ca1f6af2":"code","d8c0e627":"code","b4448309":"code","8066bda2":"code","0fb6550e":"code","a3af02e7":"code","81e4f600":"code","5a9e2161":"code","21825be4":"code","33bc93fc":"code","d9f9d9f2":"code","f270da72":"code","9fdf5893":"code","c8205a2b":"code","97fa1c6a":"code","76a051d0":"code","325a4221":"code","6f74b536":"code","b350ef5e":"code","94104f6c":"code","9edaaff6":"code","2639507d":"code","aa478acf":"code","612b1eaf":"code","b1c1b6ab":"code","a5966480":"code","68247938":"code","84d437fe":"code","5eb41f76":"code","35ca893e":"code","7ca6dc08":"code","cf2e14df":"code","18401e63":"code","ca201d76":"code","e85100de":"code","49dcfdde":"code","84e95f55":"code","d198ecba":"code","bdbc7366":"code","d8fc86b7":"code","f1075a11":"code","a3b7b306":"code","29631839":"code","711feb4b":"code","76c2c1cd":"code","f7caebc6":"code","20114756":"code","9c11ec01":"code","926010d7":"code","dbf20afb":"code","649bfa10":"code","be2a6c9a":"code","a1c5c38a":"code","24a0ac76":"code","d5fbf90a":"code","582f40e0":"code","1dd6a726":"code","a123b4ba":"code","2d0da32a":"code","a08f1df7":"code","de2d4753":"code","47f2145a":"code","aaf9757b":"code","8f230099":"code","30a4eea1":"code","506e4cf1":"code","3f931fa4":"code","d9d5bfaa":"code","7c996250":"code","aea995ff":"code","96a72154":"code","879900c2":"code","3ccc9ccb":"code","b13faf90":"code","4ad1a6b4":"code","085f1e1d":"code","3b2aed7a":"code","0ea9f4e1":"code","f3dc0dd8":"code","79a1232b":"code","8443b2d1":"code","332b9890":"code","4f353d19":"code","0114d64c":"code","a317d0e8":"code","6f51b9e1":"code","854570e1":"code","db97fc11":"code","004fc78f":"code","4e8515a2":"code","961ae8e1":"code","4c41b50d":"code","c8193a76":"code","50b681d9":"markdown","254b7a91":"markdown","39615b18":"markdown","3c97e9bb":"markdown","312e9748":"markdown","67953f32":"markdown","a9706624":"markdown","58b52a99":"markdown","a11ae80f":"markdown","af26735c":"markdown","6065a6eb":"markdown","c6da3aed":"markdown","67039160":"markdown","22d02e97":"markdown","b81d8d42":"markdown","a2e55c77":"markdown","2f8308b3":"markdown","22b44d79":"markdown","bc6a45e5":"markdown","50f02819":"markdown","462f384b":"markdown","c2ae4cef":"markdown","6d157c00":"markdown","ceeaef1b":"markdown","a123ecbf":"markdown","75c9b494":"markdown","65ff1372":"markdown","185d1732":"markdown","dc93dcc7":"markdown","3b66e567":"markdown","e302db0f":"markdown","8183c648":"markdown","0961c6b6":"markdown","f106ccd8":"markdown","8bfb0bb7":"markdown","1f9c0470":"markdown","966901de":"markdown","b546308e":"markdown","b4d61c7f":"markdown","a2f4975b":"markdown","1f84e2af":"markdown","d5a1df71":"markdown","b3d47809":"markdown"},"source":{"76f9bf9e":"# suppress display of warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# 'Pandas' is used for data manipulation and analysis\nimport pandas as pd \n\n# 'Numpy' is used for mathematical operations on large, multi-dimensional arrays and matrices\nimport numpy as np\n\n# 'Matplotlib' is a data visualization library for 2D and 3D plots, built on numpy\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n# 'Seaborn' is based on matplotlib; used for plotting statistical graphics\nimport seaborn as sns\n\n# import 'is_string_dtype' to check if the type of input is string  \nfrom pandas.api.types import is_string_dtype\n\n# import various functions to perform classification\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn import tree\nfrom sklearn.tree import export_graphviz\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.svm import SVC\n# display all columns of the dataframe\npd.options.display.max_columns = None","82131f57":"#setting the plot size using rcParams\nplt.rcParams['figure.figsize'] = [15,8]","71a5c4dc":"#importing datasets for training and testing the models\ndf=pd.read_csv(\"..\/input\/mental-health-in-tech-survey\/survey.csv\")\n\n#rwading the first 5 records from the training data set\ndf.head()","f91cd811":"#checking the number of rows and columns in the training data set\ndf.shape","f2365f2c":"# 'dtypes' gives the data type for each column\ndf.dtypes","549bdae4":"#Splitting the timestamp feature as it includes both date and time\ndf[['Date','Time']]=df['Timestamp'].str.split(\" \",n=1,expand=True)","7b8f5f60":"#dropping the timestamp column as we have already created two columns which have date and time\n#axis=1 deletes the entire column\ndf.drop('Timestamp',axis=1,inplace=True)","f301f12e":"#converting the datatype of date and time columns\ndf['Time']=pd.to_datetime(df['Time'],format='%H:%M:%S')\ndf['Date']=pd.to_datetime(df['Date'])","2bc19fea":"#finally checking the columns and their datatypes after alteration\ndf.dtypes","fe5e955b":"# the describe() returns the statistical summary of the numeric variables\ndf.describe()","48273e50":"#it gives the statistical summary of all the categorical variables\ndf.describe(include=object)","308e2794":"#finding the unique values in the column gender\ndf['Gender'].unique()","39e59190":"error={'Female':'F',\n      'Male':'M',\n      'male':'M',\n      'female':'F',\n      'm':'M',\n      'Male-ish':'M',\n      'maile':'M',\n      'Trans-female':'T',\n      'Cis Female':'F',\n      'something kinda male?':'M',\n      'Cis Male':'M',\n      'Woman':'F',\n      'f':'F',\n      'Mal':'M',\n       'Male (CIS)':'M',\n      'queer\/she\/they':'F',\n      'non-binary':'T',\n      'Enby':'T',\n      'Femake':'F',\n      'woman':'F',\n      'Make':'M',\n      'fluid':'T',\n      'Malr':'M',\n      'cis male':'M',\n      'Female (cis)':'F',\n      'Guy (-ish) ^_^':'M',\n      'queer':'T',\n      'Female (trans)':'T',\n      'male leaning androgynous':'T',\n       'Neuter':'T',\n       'cis-female\/femme':'F',\n       'msle':'M',\n       'Agender':'T',\n       'Genderqueer':'T',\n       'Female':'F',\n       'Androgyne':'T',\n       'Nah':'T', \n       'All':'T',\n      'Female ':'F',\n       'Male ':'M', \n       'Man':'M', \n       'Trans woman':'T', \n       'Mail':'M',\n       'A little about you':'T'}\ndf['Gender']=df['Gender'].map(error).fillna(df['Gender'])","376a8c0a":"df['Gender'].unique()","158dd700":"#plotting the countplot for the gender column\nsns.countplot(df['Gender'],palette='rainbow')\nplt.title(\"Countplot of the gender column\")\nplt.show()","2e801538":"#Checking the number of male,female and transgender in the gender column\ndf['Gender'].value_counts()","614e8613":"#finding the unique values of age column\ndf['Age'].unique()","1acda4b0":"#removing the redundant values from the age column\ndf=df[df['Age']!=99999999999]\ndf=df[df['Age']!=-29]\ndf=df[df['Age']!=329]\ndf=df[df['Age']!=-1726]\ndf=df[df['Age']!=5]\ndf=df[df['Age']!=8]","661ad465":"# replace 'no' with zero\ndf['treatment'] = df['treatment'].replace('No', 0)\n# replace 'yes' with one\ndf['treatment'] = df['treatment'].replace('Yes', 1)\n\n#displaying the first 5 records to check the treatment column after label enconding\ndf.head()","412bd3ed":"#plotting the countplot for treatment column\nsns.countplot(df['treatment'])\nplt.title(\"Plot for treatment column\")\n\n#checking the count of each class\ndf['treatment'].value_counts()","78e0956f":"# create a list of all categorical variables\n# initiate an empty list to store the categorical variables\ncategorical=[]\nz=['Country','state']\n# use for loop to check the data type of each variable\nfor column in df:\n    \n    # use 'if' statement with condition to check the categorical type \n    if is_string_dtype(df[column]):\n        if column!=z:\n        \n        # append the variables with 'categoric' data type in the list 'categorical'\n             categorical.append(column)\n\n\n# plot the count plot for each categorical variable \nfig, ax = plt.subplots(nrows = 5, ncols = 4, figsize=(25, 30))\n\n# use for loop to plot the count plot for each variable\nfor variable, subplot in zip(categorical, ax.flatten()):\n    \n    # use countplot() to plot the graph\n    sns.countplot(df[variable], ax = subplot)\n\n# display the plot\nplt.show()","9712be94":"# plotting the counterplot for the country column \n#to see \nplt.figure(figsize=(30,8))\nsns.countplot(df['Country'][:180])\nplt.title(\"Countplot to see data has been taken from which countries\",fontsize=20)\nplt.show()","a44aa328":"plt.figure(figsize=(25,8))\nsns.countplot(df['state'])\nplt.title(\"Countplot for states from which data have been collected\")\nplt.show()","7a2a6f5e":"#Checking employees who require treatment are from which gender\nsns.countplot(df.treatment,hue=df.Gender,order=df['treatment'].value_counts().iloc[1:2].index)\nplt.show()","6ebb17cb":"#plotting countplot to see how many self-employed people requires treatment\nsns.countplot(df.treatment,hue=df['self_employed'],order=df['treatment'].value_counts().iloc[1:2].index)","d807ff13":"#checking does number of employees in an organisation affects the treatment rate\nsns.countplot(df.treatment,hue=df['no_employees'],order=df['treatment'].value_counts().iloc[1:2].index)","82032a71":"#plot to see does how easy it to take medical leave and its affect on treatment requirement\nsns.countplot(df['treatment'],hue=df['leave'],order=df['treatment'].value_counts().iloc[1:2].index)\nplt.show()","cb6d8cfa":"sns.countplot(df.treatment,hue=df['family_history'],order=df['treatment'].value_counts().iloc[1:2].index)\nplt.show()","7abe09f3":"sns.countplot(df['treatment'],hue=df['wellness_program'],order=df['treatment'].value_counts().iloc[1:2].index)\nplt.show()","e9559992":"sns.countplot(df['treatment'],hue=df['tech_company'],order=df['treatment'].value_counts().iloc[1:2].index)\nplt.show()","5972bd60":"#plotting the bar plot for age to see if there is any outlier\nsns.boxplot(x=df.treatment,y=df['Age'])\nplt.show()","4a60403b":"# sort the variables on the basis of total null values in the variable\n# 'isnull().sum()' returns the number of missing values in each variable\nTotal = df.isnull().sum().sort_values(ascending = False)          \n\n# calculate the percentage of missing values\nPercent = ((Total*100)\/df.isnull().count()).sort_values(ascending = False)   \n\n# concat the 'Total' and 'Percent' columns using 'concat' function\nmissing_data = pd.concat([Total, Percent], axis = 1, keys = ['Total', 'Percentage of Missing Values'])    \nmissing_data","774789fa":"# plot heatmap to check null values\n# 'cbar = False' does not show the color axis \nsns.heatmap(df.isnull(), cbar=False)\n\n# display the plot\nplt.show()","32084644":"df.drop('comments',axis=1,inplace=True)\ndf['work_interfere']=df['work_interfere'].fillna('Not mentioned')\ndf.drop(['state','Country'],axis=1,inplace=True)\ndf.dropna(axis=0, inplace=True)","cad30868":"#Checking if all the null values have been handled or not\nsns.heatmap(df.isnull(),cbar=False,color='black')\nplt.show()","7bda6128":"#Creating two dataframes df_features and df_target,df_features contains all the important features which we will dummy encode\n#df_target which contains the target variable\ndf_features=df.drop(['treatment', 'Age','Date', 'Time'],axis=1)\ndf_target=df['treatment']","976e66f4":"#dummy encoding the feature(categorical) variables\ndf_dummy=pd.get_dummies(df_features,drop_first=True)","82de18eb":"#storing the features in X and the target in y variable\nX=df_dummy\ny=pd.DataFrame(df_target)","efdf6fd4":"# create a generalized function to calculate the metrics values for test set\ndef get_test_report(model):\n    \n    # return the performace measures on test set\n    return(classification_report(y_test, y_pred))","5f717513":"# create a generalized function to calculate the metrics values for test set\ndef kappa_score(model):\n    \n    # return the kappa score on test set\n    return(cohen_kappa_score(y_test, y_pred))","e395b16a":"# define a to plot a confusion matrix for the model\ndef plot_confusion_matrix(model):\n    \n    # create a confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    conf_matrix = pd.DataFrame(data = cm,columns = ['Predicted:0','Predicted:1'], index = ['Actual:0','Actual:1'])\n\n    # plot a heatmap to visualize the confusion matrix\n    sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = ListedColormap(['lightskyblue']), cbar = False, \n                linewidths = 0.1, annot_kws = {'size':25})\n\n    # set the font size of x-axis ticks using 'fontsize'\n    plt.xticks(fontsize = 20)\n\n    # set the font size of y-axis ticks using 'fontsize'\n    plt.yticks(fontsize = 20)\n\n    # display the plot\n    plt.show()","ed426666":"# define a function to plot the ROC curve and print the ROC-AUC score\ndef plot_roc(model):\n    \n    # the roc_curve() returns the values for false positive rate, true positive rate and threshold\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n\n    # plot the ROC curve\n    plt.plot(fpr, tpr)\n\n    # set limits for x and y axes\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n\n    # plot the straight line showing worst prediction for the model\n    plt.plot([0, 1], [0, 1],'r--')\n\n    # add plot and axes labels\n    # set text size using 'fontsize'\n    plt.title('ROC Curve', fontsize = 15)\n    plt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\n    plt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\n\n    # add the AUC score to the plot\n    plt.text(x = 0.02, y = 0.9, s = ('AUC Score:',round(roc_auc_score(y_test, y_pred),4)))\n\n    # plot the grid\n    plt.grid(True)","103ccd0c":"# create an empty dataframe to store the scores for various classification algorithms\nscore_card = pd.DataFrame(columns=['Model', 'AUC Score', 'Precision Score', 'Recall Score', 'Accuracy Score',\n                                   'Kappa Score', 'f1-score'])\n\ndef update_score_card(model_name):\n    \n    # assign 'score_card' as global variable\n    global score_card\n\n    # append the results to the dataframe 'score_card'\n    # 'ignore_index = True' do not consider the index labels\n    score_card = score_card.append({'Model': model_name,\n                                    'AUC Score' : roc_auc_score(y_test, y_pred),\n                                    'Precision Score': metrics.precision_score(y_test, y_pred),\n                                    'Recall Score': metrics.recall_score(y_test, y_pred),\n                                    'Accuracy Score': metrics.accuracy_score(y_test, y_pred),\n                                    'Kappa Score': cohen_kappa_score(y_test, y_pred),\n                                    'f1-score': metrics.f1_score(y_test, y_pred)}, \n                                    ignore_index = True)\n    return(score_card)","02a7f23e":"# split data into train subset and test subset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 10)\n\n# check the dimensions of the train & test subset using 'shape'\n# print dimension of train set\nprint(\"X_train\",X_train.shape)\nprint(\"y_train\",y_train.shape)\n\n# print dimension of test set\nprint(\"X_test\",X_test.shape)\nprint(\"y_test\",y_test.shape)","77abc5f0":"# build the model\nsvclassifier = SVC(kernel = 'linear')\n\n# fit the model\nsvc_model=svclassifier.fit(X_train, y_train)\n","ecbe1f3e":"# predict the values\ny_pred = svclassifier.predict(X_test)","324eebdd":"# call the function to plot the confusion matrix\nplot_confusion_matrix(svc_model)","f31cdbef":"# compute the performance measures on test data\ntest_report = get_test_report(svc_model)\n\n# print the performace measures\nprint(test_report)","06c0fa3c":"# compute kappa score on test set\nkappa_value = kappa_score(svc_model)\n\n# print the kappa value\nprint(kappa_value)","fc73a6e4":"plot_roc(svc_model)","30ec01b1":"update_score_card(model_name='SVM')","cedfe976":"# build the model\nsvclassifier = SVC(kernel='rbf')\n# fit the model\nsvm_rbf=svclassifier.fit(X_train, y_train)","ca1f6af2":"# predict the values\ny_pred= svclassifier.predict(X_test)\n","d8c0e627":"plot_confusion_matrix(svm_rbf)","b4448309":"test_report=get_test_report(svm_rbf)\nprint(test_report)","8066bda2":"# compute kappa score on test set\nkappa_value = kappa_score(svm_rbf)\n\n# print the kappa value\nprint(kappa_value)","0fb6550e":"plot_roc(svm_rbf)","a3af02e7":"update_score_card(model_name='SVM with rbf')","81e4f600":"# build the model\nsvclassifier = SVC(kernel='sigmoid')\n# fit the model\nsvm_sigmoid=svclassifier.fit(X_train, y_train)","5a9e2161":"# predict the values\ny_pred  = svclassifier.predict(X_test)","21825be4":"plot_confusion_matrix(svm_sigmoid)","33bc93fc":"test_report=get_test_report(svm_sigmoid)\nprint(test_report)","d9f9d9f2":"# compute kappa score on test set\nkappa_value = kappa_score(svm_sigmoid)\n\n# print the kappa value\nprint(kappa_value)","f270da72":"plot_roc(svm_sigmoid)","9fdf5893":"update_score_card(model_name='SVM using sigmoid')","c8205a2b":"# build the model\nsvclassifier = SVC(kernel='poly')\n# fit the model\nsvm_poly=svclassifier.fit(X_train, y_train)","97fa1c6a":"# predict the values\ny_pred  = svclassifier.predict(X_test)","76a051d0":"plot_confusion_matrix(svm_poly)","325a4221":"test_report=get_test_report(svm_poly)\nprint(test_report)","6f74b536":"# compute kappa score on test set\nkappa_value = kappa_score(svm_poly)\n\n# print the kappa value\nprint(kappa_value)","b350ef5e":"plot_roc(svm_poly)","94104f6c":"update_score_card(model_name='SVM with kernel(polynomial)')","9edaaff6":"# build the model\nsvclassifier_Poly = SVC(kernel='poly', degree = 2, gamma = 'auto')\n# fit the model\nsvm=svclassifier_Poly.fit(X_train, y_train)","2639507d":"# predict the values\ny_pred  = svclassifier_Poly.predict(X_test)","aa478acf":"plot_confusion_matrix(svm)","612b1eaf":"test_report=get_test_report(svm)\nprint(test_report)","b1c1b6ab":"# compute kappa score on test set\nkappa_value = kappa_score(svm)\n\n# print the kappa value\nprint(kappa_value)","a5966480":"plot_roc(svm)","68247938":"update_score_card(model_name='SVM with kernel(ploynomial) with degree 2')","84d437fe":"# degree: Degree of the polynomial\n# C: value of C parameter or regularisation parameter\n# gamma:\nparam_grid = { \n    'degree': [2,4,6,8,10], \n    'gamma' : ['auto','scale' ],\n    'C': [0.5, 1,1.5]\n}","5eb41f76":"CV_rfc = GridSearchCV(estimator= svclassifier_Poly, param_grid=param_grid, scoring='accuracy', cv= 5)\n# fit the model\nCV_rfc.fit(X_train, y_train)","35ca893e":"# find the best parameters\nCV_rfc.best_params_","7ca6dc08":"# build the model with best parameters obtained from above code\nsvclassifier_Poly_Grid = SVC(kernel='poly', \n                            degree = 2, \n                            gamma = 'scale',\n                           C = 1 )\n# fit the model\nsvm=svclassifier_Poly_Grid.fit(X_train, y_train)","cf2e14df":"# predict the values\ny_pred= svclassifier_Poly_Grid.predict(X_test)","18401e63":"plot_confusion_matrix(svm)","ca201d76":"test_report=get_test_report(svm)\nprint(test_report)","e85100de":"# compute kappa score on test set\nkappa_value = kappa_score(svm)\n\n# print the kappa value\nprint(kappa_value)","49dcfdde":"plot_roc(svm)","84e95f55":"update_score_card(model_name='SVM with grid search CV ')","d198ecba":"# instantiate the 'DecisionTreeClassifier' object using 'entropy' criterion\ndecision_tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 10)\n\n# fit the model using fit() on train data\ndecision_tree_model = decision_tree.fit(X_train, y_train)","bdbc7366":"labels=X_train.columns\n\n#plot the decisin tree\nfig=plt.figure(figsize=(30,30))\nz=tree.plot_tree(decision_tree_model,\n                feature_names=labels,\n                class_names=['0','1'],\n                filled=True)","d8fc86b7":"y_pred=decision_tree_model.predict(X_test)","f1075a11":"plot_confusion_matrix(decision_tree_model)","a3b7b306":"kappa_score(decision_tree_model)","29631839":"test_report=get_test_report(decision_tree_model)\nprint(test_report)","711feb4b":"plot_roc(decision_tree)","76c2c1cd":"update_score_card(model_name='Decision Tree')","f7caebc6":"prune=DecisionTreeClassifier(max_depth=5,max_leaf_nodes=25,random_state=10)\ndecision_tree_prune=prune.fit(X_train,y_train)","20114756":"labels=X_train.columns\nfig=plt.figure(figsize=(30,30))\nz=tree.plot_tree(decision_tree_prune,\n                feature_names=labels,\n                class_names=['0','1'],\n                filled=True)","9c11ec01":"y_pred=decision_tree_prune.predict(X_test)","926010d7":"plot_confusion_matrix(decision_tree_prune)","dbf20afb":"test_report=get_test_report(decision_tree_prune)\nprint(test_report)","649bfa10":"plot_roc(decision_tree_prune)","be2a6c9a":"update_score_card(model_name='Decision Tree Pruned')","a1c5c38a":"tuned_parameters=[{'criterion':['gini','entropy'],\n                  'min_samples_split':[10,20,30],\n                  'max_depth':[3,5,7],\n                  'min_samples_leaf':[15,20,25,30,35],\n                  'max_leaf_nodes':[5,10,15,20,25]\n                  }]","24a0ac76":"tree_classification=DecisionTreeClassifier(random_state=10)\ngrid=GridSearchCV(estimator=tree_classification,\n                 param_grid=tuned_parameters,\n                 cv=10)\ndt_grid=grid.fit(X_train,y_train)\n\nprint(\"best parameters:\",dt_grid.best_params_,'\\n')","d5fbf90a":"dt_grid_model=DecisionTreeClassifier(criterion=dt_grid.best_params_.get('criterion'),\n                                     max_depth=dt_grid.best_params_.get(\"max_depth\"),\n                                     max_leaf_nodes=dt_grid.best_params_.get(\"max_leaf_nodes\"),\n                                     min_samples_leaf=dt_grid.best_params_.get(\"max_leaf_nodes\"),\n                                     min_samples_split=dt_grid.best_params_.get(\"min_samples_split\"),\n                                     random_state=10)\ndt_grid_model=dt_grid_model.fit(X_train,y_train)","582f40e0":"# save the column names in 'labels'\nlables = X_train.columns\n\n# plot the decision tree \nfig = plt.figure(figsize=(30,30))\n_ = tree.plot_tree(dt_grid_model, \n                   feature_names=lables,  \n                   class_names=[\"0\",\"1\"],\n                   filled=True)","1dd6a726":"y_pred=dt_grid_model.predict(X_test)","a123b4ba":"plot_confusion_matrix(dt_grid_model)","2d0da32a":"test_report=get_test_report(dt_grid_model)\nprint(test_report)","a08f1df7":"plot_roc(dt_grid_model)","de2d4753":"update_score_card(model_name='Decision Tree using grid search')","47f2145a":"# Create a random forest classifier\nclf = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)\n\n# Train the classifier\nclf.fit(X_train, y_train)\n\n","aaf9757b":"feat_labels = X.columns.values\n# Print the name and gini importance of each feature\nfeature_importance = []\nfor feature in zip(feat_labels, clf.feature_importances_):\n    #rint(feature)\n    feature_importance.append(feature)","8f230099":"feature_importance","30a4eea1":"# features that have an importance of more than 0.01\nsfm = SelectFromModel(clf, threshold=0.01)","506e4cf1":"# Train the selector\nsfm.fit(X_train, y_train)","3f931fa4":"selected_features = []\n# Print the names of the most important features\nfor feature_list_index in sfm.get_support(indices=True):\n    selected_features.append(feat_labels[feature_list_index])","d9d5bfaa":"selected_features","7c996250":"data_selected = df_dummy[selected_features]\ndata_selected.head()","aea995ff":"data_standardised = pd.get_dummies(data_selected,drop_first=True)","96a72154":"# let us now split the dataset into train & test\nX = data_standardised\ny = df['treatment']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state=10)\n\n# print the shape of 'x_train'\nprint(\"X_train \",X_train.shape)\n\n# print the shape of 'x_test'\nprint(\"X_test \",X_test.shape)\n\n# print the shape of 'y_train'\nprint(\"y_train \",y_train.shape)\n\n# print the shape of 'y_test'\nprint(\"y_test \",y_test.shape)","879900c2":"# build the model\ngnb = GaussianNB()\n\n# define the ovr strategy\nGNB = OneVsRestClassifier(gnb)\n\n# fit the model\nGNB.fit(X_train, y_train)","3ccc9ccb":"# predict the values\ny_pred= GNB.predict(X_test)","b13faf90":"result = classification_report(y_test,y_pred)\n\n# print the result\nprint(result)","4ad1a6b4":"# compute kappa score on test set\nkappa_value = kappa_score(GNB)\n\n# print the kappa value\nprint(kappa_value)","085f1e1d":"plot_roc(GNB)","3b2aed7a":"update_score_card(model_name = 'Random Forest')","0ea9f4e1":"X=df_dummy\ny=pd.DataFrame(df_target)","f3dc0dd8":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 10)\n\n# check the dimensions of the train & test subset using 'shape'\n# print dimension of train set\nprint(\"X_train\",X_train.shape)\nprint(\"y_train\",y_train.shape)\n\n# print dimension of test set\nprint(\"X_test\",X_test.shape)\nprint(\"y_test\",y_test.shape)","79a1232b":"# instantiate the 'SGDClassifier' to build model using SGD\n# to perform logistic regression, consider the log-loss function \n# set 'random_state' to generate the same dataset each time you run the code \nSGD = SGDClassifier(loss = 'log', random_state = 10)\n\n# fit the model on scaled training data\nlogreg_with_SGD = SGD.fit(X_train, y_train)","8443b2d1":"# use predict() to predict the class labels of target variable\ny_pred = logreg_with_SGD.predict(X_test)","332b9890":"# call the function to plot the confusion matrix\n# pass the logistic regression (SGD) model to the function\nplot_confusion_matrix(logreg_with_SGD)","4f353d19":"test_report = get_test_report(logreg_with_SGD)\n\n# print the performace measures\nprint(test_report)","0114d64c":"kappa_value = kappa_score(logreg_with_SGD)\n\n# print the kappa value\nprint(kappa_value)","a317d0e8":"plot_roc(logreg_with_SGD)","6f51b9e1":"update_score_card(model_name = 'Logistic Regression (SGD)')","854570e1":"from sklearn.neighbors import KNeighborsClassifier\nclassifier=KNeighborsClassifier(n_neighbors=5)\nKNN=classifier.fit(X_train,y_train)","db97fc11":"y_pred=classifier.predict(X_test)","004fc78f":"plot_confusion_matrix(KNN)","4e8515a2":"kappa_score(KNN)","961ae8e1":"test_report=get_test_report(decision_tree_model)\nprint(test_report)","4c41b50d":"plot_roc(decision_tree)","c8193a76":"update_score_card(model_name='K Nearest Neighbours')","50b681d9":"# Model 10(Ensemble Model)","254b7a91":"# Exploratory Data Analysis\n","39615b18":"# Finding the missing values","3c97e9bb":"# Model 5(Support vector machine using kernel and degree=2) ","312e9748":"# Preparing the data for building model","67953f32":"From the above output we see that the data is mostly balanced","a9706624":"# Understanding the data set","58b52a99":"We can see that the employees who don't know whether they will get leave for medical health condition requires treatment the maximum.","a11ae80f":"#  Model 6(Support vector machine using grid search)","af26735c":"We can see that the self-employed people requires less treatment.","6065a6eb":"# Model 3 (Support vector machine using kernel(sigmoid))","c6da3aed":"# Model 9(Decision Tree using grid search)","67039160":"# Creating generalised functions","22d02e97":"From the line plot we can see that in 3 days all the data have been collected.Maximum data has been collected on 28-08-2014.","b81d8d42":"Now all the features are having correct datatypes","a2e55c77":"# Model 12(K Nearest Neighbours) ","2f8308b3":"From the plot and the function .valuecounts() it can be clearly seen that the number of males in the data set are 829,females are 203 and transgender are 16.","22b44d79":"The employer who never discussed mental health as part of an employee wellness program are more prone to treatment requirement.","bc6a45e5":"# Model 8(Decision Tree using pruning) ","50f02819":"From the plot we can see that the tech company employees need more treatment than oganization employees.","462f384b":"Total 12 models have been built to predict whether an employee needs medical treatment or not.The best out of all these models are support vector machine which is the first model.We can see that the F1 score which is the harmonic mean between precision and recall is the highest in this model and the value is 0.886486.The accuracy of this model also stands above all other models with the value of 0.863636.\n\nWe also consider the AUC score(area under the curve) to see which model is performing best among other models and for SVM model it is 0.852381 which is the highest among all the models.\n\nIf we further see the kappa score which tells us about the agreement between the predicted and observed value, we can see that it is 0.719320 which shows that there is high agreement between predicted and observed values.\n\nIf we see the confusion matrix then the type 1 error is less i.e.4 in other models type 1 error is more than 10.In this data set as we are predicting about the treatment of an employee,so type 1 error is important. As any employee having mental health issue and if we predict that employee as not having mental illness, can be fatal. So we see that the SVM model is the best model to predict which employee needs treatment. \n\nHence,we will use the test data set on this model to predict whether an employee requires medical treatment or not.","c2ae4cef":"# Handling the missing values","6d157c00":"From the above output we can see that all the features are of object data type, except serial number and age.\nBut timestamp is wrongly attributed as object which should be datetime datatype.We will convert the datatype of timestamp. ","ceeaef1b":"# Conclusion:\n","a123ecbf":"We can see from the plot that organisation have 26-100 employees and those having more than 1000 employees requires more treatment.So we can not conclude much from the organisational strength of employees.","75c9b494":"Employees with family history of mental illness needs treatment more than those who doesnot have any family history of mental illness.","65ff1372":"# Model 2(Support vector machine using kernel(rbf))","185d1732":"# Importing libraries","dc93dcc7":"# Model 1(Support vector machine)","3b66e567":"The summary contains information about the count,mean,highest,lowest values of the columns and their 1st,2nd and 3rd quartiles values as well.From the above output we can see that both the columns doesnot have any missing values as the count of both the variables is equal to the number of rows.But in age we see some discrepency.The maximum age is 99999999999 and the minimum is -1726.We need to handle these data. ","e302db0f":"From the plot we can see that the maximum data has been taken from United States followed by United Kingdom","8183c648":"From the above output we can see that the state,self-employed,work_interfere and the comments columns have missing data as the count is less than 1048.We will handle these columns.\nWe see that in gender column there are 45 unique values.We need to see this column for any redundancy or error.\nThe country column has 45 unique values and the data has been maximum taken from United States and the frequency is 644 out of 1048.","0961c6b6":"It can be clearly seen that males requires more treatment","f106ccd8":"# Label encoding of the treatment column","8bfb0bb7":"From the plots we can see that mainly maximum variables have 3 values except for family_history,self_employed,country,state,remote_work,work_interfere and number of employees.\n\nDeductions:\n\n1)Number of male employees are greater than the female and transgender employees in the dataset.\n\n2)Few number of employees in the data set are self-employed.\n\n3)Maximum employees doesnot have any family history of mental illness.\n\n4)Maximum employees feel that their mental condition sometimes interfere with their work.\n\n5)Maximum employees whose data has been provided does not work from remote 50% of the time.\n\n6)Maximum employees who have mental condition works in tech company.\n\n7)Employees who doesnot know about mental health care that their employer provides or whose employer has not discussed mental  \nhealth as part of an employee wellness program have mental health issue.\n\n8)Many employees are willing to share their mental health issue with their supervisor.\n\n9)Few employees are willing to share their mental health issue with their coworkers.","1f9c0470":"After applying the map function to the column gender and mapping the misspelled words and synonymns into 'M','F' and 'T' we see that there are only 3 unique values.","966901de":"We see that age of people who requires treatment are in the range of 20-50years approximately.","b546308e":"# Model 11(Logistics regression using SGD)","b4d61c7f":"# Model 7(Decision Tree)","a2f4975b":"From the plot we can see that maximum data has been collected from California followed by Washington","1f84e2af":"# Model 4(Support vector machine using kernel(Polynomial))","d5a1df71":"# DATA PREPARATION","b3d47809":"We see that there are lots of misspelled words and synonymns have been used.This is the reason there are 45 unique values.We will convert them into 3 parts male(M),female(F) and transgender(T). "}}