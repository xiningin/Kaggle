{"cell_type":{"3cf3a2a5":"code","f6a17f28":"code","7d049823":"code","38a6e0eb":"code","eca12d54":"code","06dbd606":"code","8690cc72":"code","fbcc8dda":"code","9d425bab":"code","366465db":"code","a1993a56":"code","8bd2d6ce":"code","b8e20ead":"code","08dafda0":"code","5726154c":"code","a4d11f01":"code","baf3a128":"code","9e33182d":"code","bf6a5139":"code","e14b5f76":"code","74ce7e40":"code","393ddd8b":"code","ddec96a7":"code","0b3cd4ba":"code","46e08471":"code","6ca5c50d":"code","f03130e2":"code","75f4a68b":"code","2af553c1":"code","68d52b3a":"code","1de3f390":"code","664a5cb7":"code","96d0aebb":"code","d7c953e6":"code","131f8017":"code","3f316ca1":"code","c7522d55":"code","2fdafb16":"code","e4a50bbb":"code","186c7003":"code","cb830421":"code","009dfcbe":"code","5b508ff7":"code","21b02035":"code","80b6fdc0":"code","bfd22117":"code","c2a7fff0":"code","7160e96b":"code","a3a80b47":"code","b9a18d02":"code","5b420d83":"code","44564ab1":"code","3d1c8fd0":"code","2444013d":"code","35bcb17a":"code","e6c7d7eb":"code","79037c6c":"code","a06a56da":"code","151299fa":"code","31e3dcfd":"code","a6d5a69e":"code","7d1ccb50":"code","394b66d6":"code","06f5ef61":"code","412ee607":"code","3fb56f00":"code","3f850b1c":"code","06e0ec83":"code","efedd63e":"code","473c9540":"markdown","d523efed":"markdown","9d4a3496":"markdown","7a55ef55":"markdown","57f06972":"markdown","4605097d":"markdown","168a3d26":"markdown","4f988dad":"markdown","bf18d40c":"markdown","5e7ae1a1":"markdown","93113809":"markdown","51ebe5fb":"markdown","a434a9b2":"markdown","6334717a":"markdown","a54a9619":"markdown","101f1056":"markdown","734b7f05":"markdown","2e8e15a5":"markdown","6a6b3434":"markdown","5afadf48":"markdown","525e0377":"markdown"},"source":{"3cf3a2a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nsns.set()\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f6a17f28":"# This is to ensure we see all cols in calls to head, sample and describe\npd.options.display.max_columns = None\npd.options.display.max_rows = None\npd.options.display.max_colwidth = -1","7d049823":"# There are 4 datasets, divided based on the victim count. Let's import all of them and combine them\nimport glob\ndata_files = glob.glob(os.path.join('', \"..\/input\/serial-killers-dataset\/*victim_count.csv\"))\n\ndf = pd.concat(map(lambda file: pd.read_csv(file), data_files))","38a6e0eb":"# To reset index as after the vertical concatenation above we will have indices like 0-n followed by 0-m and so on\ndf.reset_index(drop=True, inplace=True)\ndf.shape","eca12d54":"df.dtypes","06dbd606":"df.describe()","8690cc72":"df.head(10)","fbcc8dda":"# Only Possible victims feature has missing values\ndf.isna().sum()","9d425bab":"# check empty rows\/cols and duplicated records\nprint(\"Empty rows = \", df.isnull().all(axis=1).sum() , \"\\nEmpty cols = \", df.isnull().all(axis=0).sum(), \n     \"\\nDuplicated records = \", df.duplicated().sum())","366465db":"df.columns","a1993a56":"# Rename some columns for ease of use (like accessing via dot operator)\ndf.rename(columns={'Years active': 'Years_active', 'Proven victims': 'Proven_victims', 'Possible victims': 'Possible_victims'}, inplace=True)\ndf.columns","8bd2d6ce":"df.Possible_victims.value_counts(dropna=False)","b8e20ead":"df.Possible_victims.isnull().sum()","08dafda0":"# Since we will be replacing missing\/unknown values in possible_victims cols with the corresponding values in proven_victims cols, so let's see the existing data for this so that \n# later we can verify the changes for the same indices\n# For eg after running below command we see that indices 33-41 have NaNs. So, after performing the replace operations we should recheck for these specific indices if the possible_victims\n# col has the same value as the corresponding proven_victims col\ndf[df.Possible_victims.isnull()][['Possible_victims','Proven_victims']].head(10)","5726154c":"df.Possible_victims.fillna(df.Proven_victims,inplace=True)\ndf.Possible_victims.value_counts(dropna=False)","a4d11f01":"# check if NaN replacement has worked\ndf.Possible_victims.isnull().sum()","baf3a128":"# Below proves that indices 33-41 of possible_victims have been replaced by the corresponding proven_victims col values\ndf.iloc[33:42][['Possible_victims','Proven_victims']]","9e33182d":"# Now let's do the same for 'Unknown' values\n# First check which rows have possible_victims values set to Unknown or '-'\ndf.loc[df.Possible_victims.isin(['Unknown', '-']), ['Possible_victims','Proven_victims']] ","bf6a5139":"df.loc[df.Possible_victims.isin(['Unknown', '-']), 'Possible_victims'] = df.Proven_victims\n# Below proves that the specific indices of possible_victims have been replaced by the corresponding proven_victims col values\ndf.iloc[[24, 78, 89, 91, 143, 145, 192]][['Possible_victims','Proven_victims']]","e14b5f76":"df.Possible_victims.value_counts(dropna=False)","74ce7e40":"# Cos there is a diff btw pandas' str and python's str, hence we first have to convert to str by apply and then strip\n# https:\/\/stackoverflow.com\/a\/48978195\/3933188\ndf = df.apply(lambda x: x.apply(str) if x.dtype == 'object' else x)\ndf = df.apply(lambda x: x.str.rstrip('+') if x.dtype == 'object' else x)\ndf = df.apply(lambda x: x.str.rstrip('?') if x.dtype == 'object' else x)\ndf = df.apply(lambda x: x.str.lstrip('~') if x.dtype == 'object' else x)\ndf.Possible_victims.value_counts(dropna=False)","393ddd8b":"# Now let's handle the ranges. We need to split such values on '-' and then pick the 2nd value\ndf[df.Possible_victims.str.contains('-') == True]","ddec96a7":"# Looks like there are different types of hyphens in this df (short dashes and long dashes)\ndf[df.Possible_victims.str.contains('\u2013') == True]","0b3cd4ba":"# This will be used later to test our solution handling both types of hyphens\ndf.iloc[[280, 294]]","46e08471":"# Let's replace all the ranges with both types of hyphens with the max value\ndf['Possible_victims'] = [str(x).split('-')[-1] for x in df['Possible_victims']]\ndf['Possible_victims'] = [str(x).split('\u2013')[-1] for x in df['Possible_victims']]","6ca5c50d":"# Verify our handling of the hyphens\ndf.iloc[[280, 294]]","f03130e2":"df.Proven_victims.value_counts(dropna=False)","75f4a68b":"# Let's replace all the ranges with the max value\ndf['Proven_victims'] = [str(x).split('\u2013')[-1] for x in df['Proven_victims']]\ndf.Proven_victims.value_counts(dropna=False)","2af553c1":"df.sample(5)","68d52b3a":"# Plot to see pattern over the years\nplt.figure(figsize=(28,8))\ndf.Proven_victims = df.Proven_victims.astype(int)\nproven_victims_list = list(df.Proven_victims)\nproven_victims_list.sort()\ng = sns.countplot(x = proven_victims_list)\ng.set(xlabel = 'Year Active From', ylabel = 'Year Active To')\ng.set_xticklabels(g.get_xticklabels(), rotation = 90)\ng.set_title('Activity over the years',fontsize =14)\nplt.show()","1de3f390":"# Plot to see distribution of proven victims\nplt.figure(figsize=(28,8))\n\ng = sns.histplot(x = proven_victims_list, bins = range(0,85,3))\ng.set(xlabel = 'Number of proven victims')\ng.set_title('Distribution of proven victims',fontsize =14)\nplt.xticks(range(0,85,3))\nplt.show()","664a5cb7":"df[df.Country.str.contains('\\r\\n')].head(5)","96d0aebb":"df.info()","d7c953e6":"df[df.Country.str.contains('suspected')].sample(3)","131f8017":"# Shouldn't be using replace function on Object data type, in this case, we need to apply replace function after converting it into a string.\n\ndf.Country = df.Country.str.replace('\\(suspected\\)', \"\", regex = True)\ndf.Country = df.Country.str.replace('\\(alleged\\)', \"\", regex = True)\ndf.Country = df.Country.str.replace('\\(claimed\\)', \"\", regex = True)\ndf[df.Country.str.contains('\\r\\n')].sample(10)","3f316ca1":"# we should come back to revisit these selected entries so that we have something to verify our changes against\ndf.Country.iloc[[68,128,160,225,210,293]]","c7522d55":"# These replacements will result in some duplication as well. For e.g. there could be a row with both East-Germany and Germany\n# listed as countries. So, after conversion to list we should remove duplicates\n\n# Figuring these replacements took quiet some time\ndf.Country = df.Country.str.replace('Soviet Union', \"Russia\")\ndf.Country = df.Country.str.replace('West Germany', \"Germany\")\ndf.Country = df.Country.str.replace('East Germany', \"Germany\")\ndf.Country = df.Country.str.replace('German Empire', \"Germany\")\ndf.Country = df.Country.str.replace('Allied-occupied Germany', \"Germany\")\ndf.Country = df.Country.str.replace('Portuguese Angola', \"Angola\")\ndf.Country = df.Country.str.replace('Ottoman Empire', \"Turkey\")\ndf.Country = df.Country.str.replace('Kingdom of Romania', \"Romania\")","2fdafb16":"# just select the first country from the list\ndf.Country = df.Country.str.split('\\r\\n', expand=True)[0]\ndf.Country.value_counts()","e4a50bbb":"# Our changes look good as suggested by this\ndf.Country.iloc[[68,128,160,225,210,293]]","186c7003":"# Let's check our improved df\ndf.sample(5)","cb830421":"# Plot to see pattern over the years\nplt.figure(figsize=(28,8))\n# df.Proven_victims = df.Proven_victims.astype(int)\n# proven_victims_list = list(df.Proven_victims)\n# proven_victims_list.sort()\ng = sns.countplot(data = df, x = 'Country')\ng.set(xlabel = 'Year Active From', ylabel = 'Year Active To')\ng.set_xticklabels(g.get_xticklabels(), rotation = 90)\ng.set_title('Activity over the years',fontsize =14)\nplt.show()","009dfcbe":" df.Years_active.value_counts()","5b508ff7":"df.Years_active = df.Years_active.str.replace(' and earlier', '', regex = False)\ndf.Years_active = df.Years_active.str.replace(' to present', '', regex = False)\ndf.Years_active = df.Years_active.str.replace('30 June 1983', '1983', regex = False)\ndf.Years_active = df.Years_active.str.replace(' to 23 July 1983', '', regex = False)\ndf.Years_active = df.Years_active.str.replace('s', '', regex = False)\ndf.Years_active = df.Years_active.str.replace('?', '', regex = False)\ndf.Years_active = df.Years_active.str.replace('c.', '', regex = False)\ndf.Years_active = df.Years_active.str.replace('late ', '', regex = False)\n\n# verify changes\ndf.Years_active.value_counts()","21b02035":"df.info()","80b6fdc0":"# Create new cols for To Date and From Date\ndf['From_Date'] = np.nan\ndf['To_Date'] = np.nan\ndf.head()","bfd22117":"df[['From_Date','To_Date']] = df['Years_active'].str.split('to',expand=True)\ndf[['Years_active','From_Date','To_Date']].sample(5)","c2a7fff0":"# get rid of the now redundant Years_active col\ndf.drop('Years_active', axis=1, inplace=True)\ndf.columns","7160e96b":"# check how many missing values in the To_Date col\ndf.To_Date.isna().sum()","a3a80b47":"# Replace with the corresponding values from the From_Date col\ndf.To_Date.fillna(df.From_Date,inplace=True)\ndf.To_Date.isna().sum()","b9a18d02":"# convert date related cols to int type so that their diff can be calculated \ndf.To_Date = pd.to_numeric(df.To_Date, errors='coerce')\ndf.From_Date = pd.to_numeric(df.From_Date, errors='coerce')\ndf.info()","5b420d83":"# Create new cols for number of active years\ndf['Active_Years'] = df.To_Date.values - df.From_Date.values\n# Above op may result in some zero values if the to and from years are same. In such case value should be 1\ndf.Active_Years.replace({0:1}, inplace=True)\ndf[['Active_Years', 'From_Date','To_Date']].sample(10)","44564ab1":"# Plot to see pattern over the years\nplt.figure(figsize=(28,8))\ng = sns.countplot(data = df, x = \"From_Date\")\ng.set(xlabel = 'Year Active From', ylabel = 'Year Active To')\ng.set_xticklabels(g.get_xticklabels(), rotation = 90)\ng.set_title('Activity over the years',fontsize =14)\nplt.show()","3d1c8fd0":"# So, who was active for most number of years\ntop_active_df = df[['Country','Active_Years']].sort_values(by = 'Active_Years', ascending=False).head(15)\ntop_active_df\n","2444013d":"plt.figure(figsize=(20,8))\ng = sns.countplot(x='Active_Years', data=df)\ng.set(yticks= range(0,100,5))\ng.set(xlabel='Number of active years', ylabel='Number of killers')\ng.set_title('Number of active years for serial killers',fontsize =14)\nplt.show()","35bcb17a":"plt.figure(figsize=(8,8))\ng = sns.barplot(x = top_active_df.Country, y = top_active_df.Active_Years, data= top_active_df)\ng.set(ylabel='Active Years')\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\ng.set_title('Countries with Serial killers who were active the longest',fontsize =14)\nplt.show()","e6c7d7eb":"df.Name.value_counts()","79037c6c":"# Let's breakdown the Name col \ndf['First_Name'] = np.nan\ndf['Last_Name'] = np.nan\n\n# Return a list of the words in the string, using sep as the delimiter string. 2nd param (1 in this case) tell that at most 1 split is done (thus, the list will have at most 2 elements).\ndf[['First_Name','Last_Name']] = df['Name'].str.split(' ', 1, expand=True)\ndf[['Name','First_Name','Last_Name']].sample(5)","a06a56da":"# Let's see if any of the killers have same first name or last name\ndf.First_Name.value_counts()[:10]","151299fa":"df.Last_Name.value_counts()[:10]","31e3dcfd":"# Check how many of them committed suicide (in or outside prison)\ncommitted_suicide = df[df.Notes.str.contains(\"committed suicide\", case=False)]['Notes']\ncommitted_suicide","a6d5a69e":"# calc %age of killers who committed suicide (in or outside prison)\nround(committed_suicide.shape[0] \/ df.shape[0] *100, 2)","7d1ccb50":"# Importing libs for NLP\nfrom nltk import FreqDist\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport string\nfrom collections import Counter\nimport spacy \nimport re","394b66d6":"\n# Define a function to plot word frequencies\ndef plot_word_frequency(words, top_n=15, rotation = 0):\n    word_freq = FreqDist(words)\n    print(word_freq)\n    labels = [element[0] for element in word_freq.most_common(top_n)]\n    counts = [element[1] for element in word_freq.most_common(top_n)]\n    plt.figure(figsize=(28,5))\n    g = sns.barplot(labels, counts)\n    g.set_xticklabels(g.get_xticklabels(), rotation = rotation)\n    return g","06f5ef61":"# In order to perform NLP ops on Notes text we need to create a single text from combining all Notes\nnotes = \"\".join(df.Notes)\nlen(notes_words)","412ee607":"plot_word_frequency(notes.split()).set_title('Word frequencies before preprocessing',fontsize =14)","3fb56f00":"# NLTK's word tokeniser not only breaks on whitespaces but also breaks contraction words such as he'll into \"he\" and \"'ll\". On the other hand it doesn't break \"o'clock\" and treats it as a separate token.\n# In order to perform NLP ops on Notes text we need to create a single text from combining all Notes\nnotes = \"\"\n\nfor note in df.Notes:\n    note = re.sub(\"[^a-zA-Z]\",\" \",note)\n    note = note.lower()\n    note = word_tokenize(note)\n    note = [i for i in note if not i in set(stopwords.words(\"english\"))]\n    note = [WordNetLemmatizer().lemmatize(i)for i in note]\n    note = \" \".join(note)\n    notes = notes + note\n    \nplot_word_frequency(notes.split()).set_title('Word frequencies after preprocessing',fontsize =14)","3f850b1c":"# create a frequency table of all the words of the document\nall_words = Counter(tokens)\n# look at top 10 frequent words\nall_words.most_common(10)","06e0ec83":"def edits_one(word):\n    \"Create all edits that are one edit away from `word`.\"\n    alphabets    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:])                   for i in range(len(word) + 1)]\n    deletes    = [left + right[1:]                       for left, right in splits if right]\n    inserts    = [left + c + right                       for left, right in splits for c in alphabets]\n    replaces   = [left + c + right[1:]                   for left, right in splits if right for c in alphabets]\n    transposes = [left + right[1] + right[0] + right[2:] for left, right in splits if len(right)>1]\n    return set(deletes + inserts + replaces + transposes)\n\ndef edits_one(word):\n    \"Create all edits that are one edit away from `word`.\"\n    alphabets    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:])                   for i in range(len(word) + 1)]\n    deletes    = [left + right[1:]                       for left, right in splits if right]\n    inserts    = [left + c + right                       for left, right in splits for c in alphabets]\n    replaces   = [left + c + right[1:]                   for left, right in splits if right for c in alphabets]\n    transposes = [left + right[1] + right[0] + right[2:] for left, right in splits if len(right)>1]\n    return set(deletes + inserts + replaces + transposes)\n\ndef edits_two(word):\n    \"Create all edits that are two edits away from `word`.\"\n    return (e2 for e1 in edits_one(word) for e2 in edits_one(e1))\n\ndef known(words):\n    \"The subset of `words` that appear in the `all_words`.\"\n    return set(word for word in words if word in all_words)\n\ndef possible_corrections(word):\n    \"Generate possible spelling corrections for word.\"\n    return (known([word]) or known(edits_one(word)) or known(edits_two(word)) or [word])\n\ndef prob(word, N=sum(all_words.values())): \n    \"Probability of `word`: Number of appearances of 'word' \/ total number of tokens\"\n    return all_words[word] \/ N\n\ndef spell_check(word):\n    \"Print the most probable spelling correction for `word` out of all the `possible_corrections`\"\n    correct_word = max(possible_corrections(word), key=prob)\n    if correct_word != word:\n        return \"Did you mean \" + correct_word + \"?\", False\n    else:\n        return \"Correct spelling.\", True","efedd63e":"# test spell check\n# tokens.append(\"ankur\")\n\nmisspelt_word_count = 0\nfor token in tokens:\n    desc,result = spell_check(token)\n    if result == False:\n        print(desc)\n        \nif misspelt_word_count == 0:\n    print(\"No misspelt words present in Notes!\")","473c9540":"> #### *So no two serial killers with same full name !*","d523efed":"Let's do the following ops on Notes text:\n- Change to lower case\n- Tokenize\n- Remove stopwords and punctuations","9d4a3496":"*Following countries had maximum serial killers:*\n\n- United States      95\n\n- Russia             42\n\n- South Africa       19\n\n- United Kingdom     13\n\n- India              10\n\n- China 10","7a55ef55":"**Data clean up**\n>  This is a pretty simple dataset with small number of, but well defined, features. Prima facie the following cleaning needs to be done\n> - Handle \"\\r\\n\" in country list to extract separated country names\n> - Handle year range indicated by \"to\". Maybe break down 'Years active' feature into two: \"Year From\" and \"Year To\"\n> - Handle NaN and \"+\" in 'Possible Victims' feature","57f06972":"## Handling Notes field\n- Since we have already verified earlier that Notes field has no Nan values, so we can proceed freely now","4605097d":"**Some observations right off the bat**\n> - All features look pretty organized.\n> - There's no missing data other than 'Possible Victims' which should be fine as it's speculative in nature.","168a3d26":"## Handle Name field","4f988dad":"*We see how after Lemmatization words like `murders` and `murderer` are reduced to the root word `murder` thus cranking up the frequency. Note that words like `murdered`, `sentenced` stay put. To lemmatise efficiently, we need to pass it's POS tag manually.*","bf18d40c":"*Interesting pattern seen above - number of serial killers increased from 1971 to 2008! What explains this?*","5e7ae1a1":"> #### *As seen from above plot, thankfully most of the serial killers are not active for too long.*","93113809":"**Challenges**\n> - Recovering the way killer dies from the 'Notes' column.\n> - There could be a lot more info that can be recovered from 'Notes' column.","51ebe5fb":"## Handle the Country field\n> - We will remove 'suspected'\/'claimed'\/'alleged' words and retain their countries\n> - we will replace 'West Germany'\/'Allied-occupied Germany' with just Germany; similarly Soviet Union with Russia\n> - We will get rid of '\\r\\n' and store countries as a list\n> - From the list of countries we will pick the first one so that for every convict there is just one matching country. Note: Countries within the list are not ordered alphabetically, so this move will not create any bias.","a434a9b2":"**Assumptions**\n> - Some countries are termed as 'suspected'\/'claimed'. We will ignore that suspicion and treat them like confirmed.\n> - In terms of victim count like '4+' we will assume 4. \n> - Missing\/NaN values in 'Possible Victims' feature are replaced with corresponding 'Proven Victims' data\n> - Range in 'Possible Victims' and 'Proven Victims' feature is replaced by the maximum value. For e.g 4-7 is replaced by 7\n> - From the list of countries we will pick the first one so that for every convict there is just one matching country. Note: Countries within the list are not ordered alphabetically, so this move will not create any bias.","6334717a":"## Data Cleaning","a54a9619":"We see a lot of issues here:\n> - Remove \"+\", \"~\", \"?\" signs\n> - Replace range by the maximum value. For e.g 4-7 is replaced by 7\n> - Substitute missing\/NaN\/\"-\"\/'Unknown' values with corresponding 'Proven Victims' data","101f1056":"## Handling 'Possible Victims' feature","734b7f05":"## Handle Proven_victims field","2e8e15a5":"> #### *As expected some of them have same first name. Since we are unlikely to run into someone named Robert Kumar, so I guess we are safe..*","6a6b3434":"We just need to select max from the range values","5afadf48":"> #### *So, around 4.6% of serial killers commit suicide*","525e0377":"## Handle Years_active field\n> - Break it into 2 fields: From and To\n> - Whenever range is missing, consider From and To as same\n> - Get rid of terms like 'earlier', 'present', 'c', 'late', 's' as in 1900s\n> - Remove '?' from the end\n> - Get rid of months (exact date) mentioned on some occasions"}}