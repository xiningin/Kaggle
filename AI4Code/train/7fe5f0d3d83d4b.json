{"cell_type":{"b9494771":"code","349eba93":"code","a35ff2a9":"code","528ad435":"code","c851194d":"code","bc2bd6fd":"code","6c81e92e":"code","41162bdd":"code","7e34c62e":"code","966df0be":"code","6d3becfc":"code","e216554b":"code","3d925d3f":"code","bcb32323":"code","6f2efb50":"code","3884ae14":"code","b2e2cdb0":"code","0dcff07d":"code","5632d4f3":"code","8261c902":"code","2ff50992":"code","c0cb1b96":"code","1ae38dfa":"code","1b74067d":"code","ce65663b":"code","c2762f73":"markdown","e6362c9d":"markdown","fdb3dd3a":"markdown","e3d5f9e9":"markdown","e1f8e52c":"markdown","2b6d7b09":"markdown","639e0e2b":"markdown","47aa5730":"markdown","12b32106":"markdown","161bcc56":"markdown","b1eb6dec":"markdown","c28edb10":"markdown","4b9fd10d":"markdown","2ccb2b73":"markdown","6d67bc3b":"markdown","492d6b06":"markdown","6acf2dfe":"markdown","4437b76e":"markdown","0756ac91":"markdown","bfba9785":"markdown","fcffea1d":"markdown","9febb31e":"markdown","c56291c7":"markdown","37adb59d":"markdown","7a341b27":"markdown","cac1e191":"markdown","cd56f1aa":"markdown","9123dc50":"markdown","90f30838":"markdown","da9a5de6":"markdown","0d3dad1c":"markdown"},"source":{"b9494771":"import os\nimport glob\nfrom collections import defaultdict\nimport copy\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import StepLR \nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport skimage.io as io\nfrom skimage.transform import rotate, AffineTransform, warp\nfrom tqdm import tqdm\n\nimport albumentations as A\nimport albumentations.augmentations.functional as Fc\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import RandomRotate90, HorizontalFlip, VerticalFlip\n","349eba93":"cuda = torch.cuda.is_available()\nprint(\"GPU available:\", cuda)","a35ff2a9":"torch.manual_seed(1102)\nnp.random.seed(1102)","528ad435":"!ls \/kaggle\/input\/singlecellsegmentation\/SingleCellSegmentation\/train\/image\/image_*.png | wc -l","c851194d":"!ls \/kaggle\/input\/singlecellsegmentation\/SingleCellSegmentation\/valid\/image\/image_*.png | wc -l","bc2bd6fd":"!ls \/kaggle\/input\/singlecellsegmentation\/SingleCellSegmentation\/test\/image\/image_*.png | wc -l","6c81e92e":"data_root_folder = '..\/input\/singlecellsegmentation\/SingleCellSegmentation\/'\nclass BasicDataset(TensorDataset):\n    # This function takes folder name ('train', 'valid', 'test') as input and creates an instance of BasicDataset according to that fodler.\n    # Also if you'dd like to have less number of samples (for evaluation purposes), you may set the `n_sample` with an integer.\n    def __init__(self, folder, n_sample=None, transform=None):\n        self.folder = os.path.join(data_root_folder, folder)\n        self.imgs_dir = os.path.join(self.folder, 'image')\n        self.masks_dir = os.path.join(self.folder, 'mask')\n        self.transform = transform\n        \n        self.imgs_file = sorted(glob.glob(os.path.join(self.imgs_dir, '*.png')))\n        self.masks_file = sorted(glob.glob(os.path.join(self.masks_dir, '*.png')))\n        \n        assert len(self.imgs_file) == len(self.masks_file), 'There are some missing images or masks in {0}'.format(folder)\n        \n        # If n_sample is not None (It has been set by the user)\n        if not n_sample or n_sample > len(self.imgs_file):\n            n_sample = len(self.imgs_file)\n        \n        self.n_sample = n_sample\n        self.ids = list([i+1 for i in range(n_sample)])\n            \n    # This function returns the lenght of the dataset (AKA number of samples in that set)\n    def __len__(self):\n        return self.n_sample\n    \n    \n    # This function takes an index (i) which is between 0 to `len(BasicDataset)` (The return of the previous function), then returns RGB image, \n    # mask (Binary), and the index of the file name (Which we will use for visualization). The preprocessing step is also implemented in this function.\n    def __getitem__(self, i):\n        idx = self.ids[i]\n        img = cv2.imread(os.path.join(self.imgs_dir, 'image_{0:04d}.png'.format(idx)), cv2.IMREAD_COLOR)\n        mask = cv2.imread(os.path.join(self.masks_dir, 'mask_{0:04d}.png'.format(idx)), cv2.IMREAD_GRAYSCALE)\n\n        # Convert BGR to RGB\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        #Resize all images from 512 to 256 (H and W)\n        img = cv2.resize(img, (256,256))\n        mask = cv2.resize(mask, (256,256))\n        \n        if (self.transform !=None):\n            transformed = self.transform(image=img, mask=mask)\n            img = transformed['image']\n            mask = transformed['mask'] \n            \n        # Add an axis to the mask array so that it is in [channel, width, height] format.\n        mask = np.expand_dims(mask, axis=0)\n        \n        # HWC to CHW\n        img = np.transpose(img, (2, 0, 1))\n        \n        # Scale between 0 to 1\n        img = np.array(img) \/ 255.0        \n        mask = np.array(mask) \/ 255.0\n        \n        # Make sure that the mask are binary (0 or 1)\n        mask[mask <= 0.5] = 0.0\n        mask[mask > 0.5] = 1.0\n        return {\n            'image': torch.from_numpy(img).type(torch.FloatTensor),\n            'mask': torch.from_numpy(mask).type(torch.FloatTensor),\n            'img_id': idx\n        }\n    \n\nA_transform = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n   \n])\n    ","41162bdd":"\ntrain_dataset = BasicDataset('train', transform = A_transform)\nvalid_dataset = BasicDataset('valid')\ntest_dataset = BasicDataset('test')\n\nplt.figure(figsize=(12,8))\nplt.title('Data split distribution')\nplt.bar(0, len(train_dataset), label='Train')\nplt.bar(1, len(valid_dataset), label='Validation')\nplt.bar(2, len(test_dataset), label='Test')\nplt.ylabel('Number of samples')\nplt.xticks([0,1,2],['Train', 'Validation', 'Test'])\nplt.legend()\nplt.show()","7e34c62e":"sample = np.random.randint(0, len(train_dataset))\ndata = train_dataset.__getitem__(sample)\nx = data['image']\ny = data['mask']\nidx = data['img_id']\nprint(f'x shape is {x.shape}')\nprint(f'y shape is {y.shape}')\nplt.figure(figsize=(12, 8), dpi=100)\nplt.suptitle(f'Sample {idx:04d}')\nimg = np.transpose(x, (1,2,0))\nmask = y[0]\nplt.subplot(1, 2, 1)\nplt.title('Image')\nplt.imshow(img)\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.title('Mask')\nplt.imshow(mask, cmap='gray')\nplt.axis('off')\nplt.tight_layout()\nplt.show()\nplt.close()\n\n       \n\n","966df0be":"w = []\nfor n in range(len(train_dataset)): \n    data = train_dataset.__getitem__(n)\n    x = data['image']\n    y = data['mask']\n    idx = data['img_id']\n    zp = (y > 0.5)\n    zn = (y < 0.5) \n    z1 = (1\/(zp.sum()))\n    z0 = (1\/(zn.sum()))\n    z1_normalized = (z1\/(z1+z0))\n    z0_normalized = (z0\/(z1+z0))\n    z1_normalized = z1_normalized.tolist()\n    z0_normalized = z0_normalized.tolist()\n    w.append(z1_normalized)\nprint(len(w))\ntorch.Tensor(w)\n\n","6d3becfc":"w = torch.Tensor(w)\nw_avg_pos = w.sum()\/4140\nw_avg_neg = 1-(w.sum()\/4140)\nw_tensor=torch.Tensor([w_avg_pos,w_avg_neg])\n#w_tensor=torch.Tensor([0.8784, 0.1216])\n\nw_pos = 0.8783661116364497\nw_neg = 0.12163388836355027\nprint(w_tensor)\nprint (\"w_pos =\",w_pos)\nprint (w_neg)\nprint(w_pos\/w_neg)","e216554b":"# Re-create train, validation, and test dataset instances to reduce the number of samples and expedite the training process.\ntrain_dataset = BasicDataset('train', n_sample=4140)\nvalid_dataset = BasicDataset('valid', n_sample=1380)\ntest_dataset = BasicDataset('test', n_sample=2070)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=40, shuffle=True, num_workers=2, pin_memory=True, drop_last=False)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=40, num_workers=2, pin_memory=True, drop_last=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=40, num_workers=2, pin_memory=True, drop_last = False)","3d925d3f":"\n######################################## Residual Blocks\nclass Residual (nn.Module):\n    \"1x1 convolution of the input to be added to the output\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.residual = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size = 1),\n            nn.BatchNorm2d(out_channels)\n        )\n    def forward(self,x):\n        return self.residual(x)\n    \n######################################## Double Convolution\nclass DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n        self.residual = Residual(in_channels, out_channels)\n        self.activation = nn.ReLU(inplace=True)\n    def forward(self, x):\n        res = self.residual(x)\n        x = self.double_conv(x)\n        x = x + res\n        x = self.activation(x)\n        return x\n        \n\n    \n######################################## Maxpooling followed by Double Convolution\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.MaxPool2d(2)\n        self.double_conv = DoubleConv(in_channels, out_channels)\n        self.residual = Residual(in_channels, out_channels)\n        self.activation = nn.ReLU(inplace=True)\n        \n    def forward(self, x):\n        \n        x = self.maxpool_conv(x)\n        res = self.residual(x)\n        x = self.double_conv(x)\n        x = x + res\n        x = self.activation(x)\n        return x\n    \n    \n\n\n######################################## Upsampling followed by Double Convolution\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.up_conv = nn.Sequential(\n            #nn.Upsample(scale_factor=2, mode='bicubic', align_corners=True),\n            nn.ConvTranspose2d (in_channels, in_channels, kernel_size=4, padding=1, stride=2),\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n        ) \n        self.conv = DoubleConv(out_channels * 2, out_channels)\n        self.residual = Residual(out_channels * 2, out_channels)\n        self.activation = nn.ReLU(inplace=True)\n\n    def forward(self, x1, x2):\n        \n        x1 = self.up_conv(x1)\n        x = torch.cat([x1, x2], dim=1)\n        res = self.residual(x) \n        x = self.conv(x)\n        x = x + res\n        x = self.activation(x)\n        return x\n\n######################################## Output layer (1x1 Convolution followed by SoftMax activation)\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv_sigmoid = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n        )\n\n    def forward(self, x):\n        return self.conv_sigmoid(x)","bcb32323":"class UNet(nn.Module):\n    def __init__(self, name, n_channels, n_classes):\n        super(UNet, self).__init__()\n        self.name = name\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.inputL = DoubleConv(n_channels, 8)\n        self.down1 = Down(8, 12)\n        self.down2 = Down(12, 16)\n        self.down3 = Down(16, 20)\n        self.down4 = Down(20, 24)\n        self.down5 = Down(24, 28)\n        self.down6 = Down(28, 32)\n        self.up1 = Up(32, 28)\n        self.up2 = Up(28, 24)\n        self.up3 = Up(24, 20)\n        self.up4 = Up(20, 16)\n        self.up5 = Up(16, 12)\n        self.up6 = Up(12, 8)\n        self.outputL = OutConv(8, n_classes)\n        \n    def forward(self, x):\n        x1 = self.inputL(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x6 = self.down5(x5)\n        x7 = self.down6(x6)\n        x = self.up1(x7, x6)\n        x = self.up2(x, x5)\n        x = self.up3(x, x4)\n        x = self.up4(x, x3)\n        x = self.up5(x, x2)\n        x = self.up6(x, x1)\n        x = self.outputL(x)\n        \n        return x","6f2efb50":"my_UNet = UNet('MyUNet', 3, 1)\nif (cuda == True):\n    my_UNet.cuda()\nelse: \n    my_UNet.cpu()","3884ae14":"# Take the first batch\nfor batch in test_dataloader:\n    sample_batch = batch\n    break\n    \n# Generat network prediction\nif (cuda == True):\n    with torch.no_grad():\n        y_pred = my_UNet(sample_batch['image'].cuda())\nelse: \n    with torch.no_grad():\n        y_pred = my_UNet(sample_batch['image'].cpu())\n# Print the shapes of the images, masks, predicted masks\nprint('Sample batch \\'image \\'shape is: {0}\\nSample batch \\'mask\\' shape is: {1}\\nPredicted mask shape is: {2}'.format(sample_batch['image'].shape, \n                                                                                                                       sample_batch['mask'].shape,\n                                                                                                                       y_pred.shape\n                                                                                                                      ))\n\n# Conver Pytorch tensor to numpy array then reverse the preprocessing steps\nimg = (sample_batch['image'][0].numpy().transpose(1,2,0) * 255).astype('uint8')\nmsk = (sample_batch['mask'][0][0,:,:].numpy() * 255).astype('uint8')\n\n# Exctract the relative prediction mask and threshold the probablities (>0.5)\npred_msk = (y_pred.cpu().numpy()[0][0,:,:] * 255).astype('uint8')\npred_msk_binary = ((y_pred.cpu().numpy()[0][0,:,:] > 0.5) * 255).astype('uint8')\n\n# Take the image id for display\nimg_id = sample_batch['img_id'][0]\n\n# Plot the smaple, ground truth, the prediction probability map, and the final predicted mask\nplt.figure(figsize=(24,18))\nplt.suptitle(f'Test sample Image {img_id}', fontsize=18)\n\nplt.subplot(2,4,1)\nplt.title('Input Image', fontsize=15)\nplt.imshow(img)\nplt.axis('off')\n\nplt.subplot(2,4,2)\nplt.title('Ground Truth', fontsize=15)\nplt.imshow(msk, cmap='gray')\nplt.axis('off')\n\nplt.subplot(2,4,3)\nplt.title('Non-trained Network Prediction Output \\n(probability [0, 1])', fontsize=15)\nplt.imshow(pred_msk, cmap='gray')\nplt.axis('off')\n\nplt.subplot(2,4,4)\nplt.title('Non-trained Thresholdded Binary Prediction (threshold > 0.5)', fontsize=15)\nplt.imshow(pred_msk_binary, cmap='gray')\nplt.axis('off')\n\ninput_overlayed_GT = img.copy()\ninput_overlayed_GT[msk == 255, :] = [0, 255, 0]\nplt.subplot(2,4,5)\nplt.title('Input Image overlayed with Ground Truth', fontsize=15)\nplt.imshow(input_overlayed_GT)\nplt.axis('off')\n\ninput_overlayed_Pred = img.copy()\ninput_overlayed_Pred[pred_msk_binary == 255, :] = [255, 0, 0]\nplt.subplot(2,4,6)\nplt.title('Input Image overlayed with Prediction', fontsize=15)\nplt.imshow(input_overlayed_Pred)\nplt.axis('off')\n\nGT_overlayed_prediction = np.zeros_like(img)\nGT_overlayed_prediction[msk == 255, 1] = 255\nGT_overlayed_prediction[pred_msk_binary == 255, 0] = 255\nplt.subplot(2,4,7)\nplt.title('Ground Truth overlayed with Prediction', fontsize=15)\nplt.imshow(GT_overlayed_prediction)\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()","b2e2cdb0":"optimizer = torch.optim.SGD(my_UNet.parameters(), lr=0.0007)\n#loss_function = nn.BCELoss()\nloss_function = nn.BCEWithLogitsLoss(pos_weight = (w_avg_pos\/w_avg_neg))\nloss_function = loss_function.cuda()\n#loss_function = F.cross_entropy(y_pred, true_masks)","0dcff07d":"# Define a function that computes the DICE score for binary segmentation\ndef dice_coeff_binary(y_pred, y_true):\n        \"\"\"Values must be only zero or one.\"\"\"\n        eps = 0.0001\n        inter = torch.dot(y_pred.view(-1), y_true.view(-1))\n        union = torch.sum(y_pred) + torch.sum(y_true)\n        return ((2 * inter.float() + eps) \/ (union.float() + eps)).cpu().numpy()\n    \n\n# The training function\ndef train_net(net, epochs, train_dataloader, valid_dataloader, optimizer, loss_function):\n    \n    if not os.path.isdir('{0}'.format(net.name)):\n        os.mkdir('{0}'.format(net.name))\n    \n    n_train = len(train_dataloader)\n    n_valid = len(valid_dataloader)    \n    \n    train_loss = list()\n    valid_loss = list()\n    train_dice = list()\n    valid_dice = list()\n    \n    for epoch in range(epochs):\n        \n        ################################################################################################################################\n        ########################################################### Training ###########################################################\n        ################################################################################################################################\n        net.train()\n        train_batch_loss = list()\n        train_batch_dice = list()\n        \n        for i, batch in enumerate(train_dataloader):\n\n            # Load a batch and pass it to the GPU\n            if (cuda == True):\n                imgs = batch['image'].cuda()\n                true_masks = batch['mask'].cuda()\n            else: \n                imgs = batch['image'].cpu()\n                true_masks = batch['mask'].cpu()\n\n            # Produce the estimated mask using current weights\n            y_pred = net(imgs)\n\n            # Compute the loss for this batch and append it to the epoch loss\n            loss = loss_function(y_pred, true_masks)\n            batch_loss = loss.item()\n            train_batch_loss.append(batch_loss)\n\n            # Make the thresholded mask to compute the DICE score\n            pred_binary = (y_pred > 0.5).float()                    # You can change the probablity threshold!\n            \n            # Compute the DICE score for this batch and append it to the epoch dice\n            batch_dice_score = dice_coeff_binary(pred_binary, true_masks)\n            train_batch_dice.append(batch_dice_score)\n            \n\n            # Reset gradient values\n            optimizer.zero_grad()\n\n            # Compute the backward losses\n            loss.backward()\n\n            # Update the weights\n            optimizer.step()\n            \n            # Print the progress\n            print(f'EPOCH {epoch + 1}\/{epochs} - Training Batch {i+1}\/{n_train} - Loss: {batch_loss}, DICE score: {batch_dice_score}', end='\\r')\n        \n        average_training_loss = np.array(train_batch_loss).mean()\n        average_training_dice = np.array(train_batch_dice).mean()\n        train_loss.append(average_training_loss)\n        train_dice.append(average_training_dice)\n        \n        ################################################################################################################################\n        ########################################################## Validation ##########################################################\n        ################################################################################################################################\n        \n        net.eval()\n        valid_batch_loss = list()\n        valid_batch_dice = list()\n        \n        # This part is almost the same as training with the difference that we will set all layers to evaluation mode (effects some layers such as BN and Dropout) and also\n        # we don't need to calculate the gradient since we are only evaluating current state of the model. This will speed up the process and cause it to consume less memory.\n        with torch.no_grad():\n            for i, batch in enumerate(valid_dataloader):\n\n                # Load a batch and pass it to the GPU\n                if (cuda == True):\n                    imgs = batch['image'].cuda()\n                    true_masks = batch['mask'].cuda()\n                else: \n                    imgs = batch['image'].cpu()\n                    true_masks = batch['mask'].cpu()\n\n                # Produce the estimated mask using current weights\n                y_pred = net(imgs)\n\n                # Compute the loss for this batch and append it to the epoch loss\n                loss = loss_function(y_pred, true_masks)\n                batch_loss = loss.item()\n                valid_batch_loss.append(batch_loss)\n\n                # Make the thresholded mask to compute the DICE score\n                pred_binary = (y_pred > 0.5).float()                    # You can change the probablity threshold!\n\n                # Compute the DICE score for this batch and append it to the epoch dice\n                batch_dice_score = dice_coeff_binary(pred_binary, true_masks)\n                valid_batch_dice.append(batch_dice_score)\n\n                # Print the progress\n                print(f'EPOCH {epoch + 1}\/{epochs} - Validation Batch {i+1}\/{n_valid} - Loss: {batch_loss}, DICE score: {batch_dice_score}', end='\\r')\n                \n        average_validation_loss = np.array(valid_batch_loss).mean()\n        average_validation_dice = np.array(valid_batch_dice).mean()\n        valid_loss.append(average_validation_loss)\n        valid_dice.append(average_validation_dice)\n        \n        print(f'EPOCH {epoch + 1}\/{epochs} - Training Loss: {average_training_loss}, Training DICE score: {average_training_dice}, Validation Loss: {average_validation_loss}, Validation DICE score: {average_validation_dice}')\n\n        ################################################################################################################################\n        ###################################################### Saveing Checkpoints #####################################################\n        ################################################################################################################################\n        torch.save(net.state_dict(), f'{net.name}\/epoch_{epoch+1:03}.pth')\n    \n    return train_loss, train_dice, valid_loss, valid_dice","5632d4f3":"torch.autograd.set_detect_anomaly(True)\nEPOCHS = 100\ntrain_loss, train_dice, valid_loss, valid_dice = train_net(my_UNet, EPOCHS, train_dataloader, valid_dataloader, optimizer, loss_function)","8261c902":"plt.figure(figsize=(15,8))\nplt.suptitle('Learning Curve', fontsize=18)\n\nplt.subplot(1,2,1)\nplt.plot(np.arange(EPOCHS)+1, train_loss, '-o', label='Training Loss')\nplt.plot(np.arange(EPOCHS)+1, valid_loss, '-o', label='Validation Loss')\nplt.xticks(np.arange(EPOCHS)+1)\nplt.xlabel('Epoch', fontsize=15)\nplt.ylabel('Loss', fontsize=15)\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(np.arange(EPOCHS)+1, train_dice, '-o', label='Training DICE score')\nplt.plot(np.arange(EPOCHS)+1, valid_dice, '-o', label='Validation DICE score')\nplt.xticks(np.arange(EPOCHS)+1)\nplt.xlabel('Epoch', fontsize=15)\nplt.ylabel('DICE score', fontsize=15)\nplt.legend()\n\nplt.tight_layout()\nplt.show()","2ff50992":"best_epoch = np.argmax(valid_dice) + 1 # The plus one is because the epochs starts at 1.\n\nprint(f'Best epoch is epoch{best_epoch}')\n\nstate_dict = torch.load(f'.\/MyUNet\/epoch_{best_epoch:03}.pth')\n\nmy_UNet.load_state_dict(state_dict)\nif (cuda == True):\n    my_UNet.cuda()\nelse: \n    my_Unet.cpu()","c0cb1b96":"# Take the first batch\nfor batch in test_dataloader:\n    sample_batch = batch\n    break\n    \n# Generat network prediction\nif (cuda == True):\n    with torch.no_grad():\n        y_pred = my_UNet(sample_batch['image'].cuda())\nelse:\n    with torch.no_grad():\n        y_pred = my_UNet(sample_batch['image'].cpu())\n\n# Print the shapes of the images, masks, predicted masks\nprint('Sample batch \\'image \\'shape is: {0}\\nSample batch \\'mask\\' shape is: {1}\\nPredicted mask shape is: {2}'.format(sample_batch['image'].shape, \n                                                                                                                       sample_batch['mask'].shape,\n                                                                                                                       y_pred.shape\n                                                                                                                      ))\n\n# Conver Pytorch tensor to numpy array then reverse the preprocessing steps\nimg = (sample_batch['image'][0].numpy().transpose(1,2,0) * 255).astype('uint8')\nmsk = (sample_batch['mask'][0][0,:,:].numpy() * 255).astype('uint8')\n\n# Exctract the relative prediction mask and threshold the probablities (>0.5)\npred_msk = (y_pred.cpu().numpy()[0][0,:,:] * 255).astype('uint8')\npred_msk_binary = ((y_pred.cpu().numpy()[0][0,:,:] > 0.5) * 255).astype('uint8')\n\n# Take the image id for display\nimg_id = sample_batch['img_id'][0]\n\n# Plot the smaple, ground truth, the prediction probability map, and the final predicted mask\nplt.figure(figsize=(24,18))\nplt.suptitle(f'Test sample Image {img_id}', fontsize=18)\n\nplt.subplot(2,4,1)\nplt.title('Input Image', fontsize=15)\nplt.imshow(img)\nplt.axis('off')\n\nplt.subplot(2,4,2)\nplt.title('Ground Truth', fontsize=15)\nplt.imshow(msk, cmap='gray')\nplt.axis('off')\n\nplt.subplot(2,4,3)\nplt.title('Final Network Prediction Output \\n(probability [0, 1])', fontsize=15)\nplt.imshow(pred_msk, cmap='gray')\nplt.axis('off')\n\nplt.subplot(2,4,4)\nplt.title('Final Thresholdded Binary Prediction (threshold > 0.5)', fontsize=15)\nplt.imshow(pred_msk_binary, cmap='gray')\nplt.axis('off')\n\ninput_overlayed_GT = img.copy()\ninput_overlayed_GT[msk == 255, :] = [0, 255, 0]\nplt.subplot(2,4,5)\nplt.title('Input Image overlayed with Ground Truth', fontsize=15)\nplt.imshow(input_overlayed_GT)\nplt.axis('off')\n\ninput_overlayed_Pred = img.copy()\ninput_overlayed_Pred[pred_msk_binary == 255, :] = [255, 0, 0]\nplt.subplot(2,4,6)\nplt.title('Input Image overlayed with Prediction', fontsize=15)\nplt.imshow(input_overlayed_Pred)\nplt.axis('off')\n\nGT_overlayed_prediction = np.zeros_like(img)\nGT_overlayed_prediction[msk == 255, 1] = 255\nGT_overlayed_prediction[pred_msk_binary == 255, 0] = 255\nplt.subplot(2,4,7)\nplt.title('Ground Truth overlayed with Prediction', fontsize=15)\nplt.imshow(GT_overlayed_prediction)\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()","1ae38dfa":"def test_net(net, test_dataloader, loss_function):\n    # Create the pred_mask folder\n    if not os.path.isdir('\/kaggle\/working\/pred_mask'):\n        os.mkdir('\/kaggle\/working\/pred_mask')\n    \n    net.eval()\n    \n    n_test = len(test_dataloader)\n    test_batch_loss = list()\n    test_batch_dice = list()\n    test_batch_accuray = list()\n    test_batch_CM = list()\n\n    # This part is almost the same as the validation loop in `train_net` function. \n    # The difference is that we will calculate the accuracy and confusion matrix per each batch and save the predicted images.\n    with torch.no_grad():\n        for i, batch in enumerate(test_dataloader):\n\n            # Load a batch and pass it to the GPU\n            if (cuda == True):\n                imgs = batch['image'].cuda()\n                true_masks = batch['mask'].cuda()\n            else:\n                imgs = batch['image'].cpu()\n                true_masks = batch['mask'].cpu()\n            img_ids = batch['img_id'].numpy().astype('int')\n\n            # Produce the estimated mask using current weights\n            y_pred = net(imgs)\n\n            # Compute the loss for this batch and append it to the epoch loss\n            loss = loss_function(y_pred, true_masks)\n            batch_loss = loss.item()\n            test_batch_loss.append(batch_loss)\n\n            # Make the thresholded mask to compute the DICE score\n            pred_binary = (y_pred > 0.5).float()                    # You can change the probablity threshold!\n\n            # Compute the DICE score for this batch and append it to the epoch dice\n            batch_dice_score = dice_coeff_binary(pred_binary, true_masks)\n            test_batch_dice.append(batch_dice_score)\n            \n            # Save the predicted masks\n            for idx, pred_msk in enumerate(pred_binary):\n                cv2.imwrite(f'\/kaggle\/working\/pred_mask\/pred_mask_{img_ids[idx]:04}.png', np.expand_dims((pred_msk[0].cpu().numpy() * 255).astype('uint8'), axis=-1))\n            \n            # Vectorize the true mask and predicted mask for this batch\n            vectorize_true_masks = true_masks.view(-1).cpu().numpy()\n            vectorize_pred_masks = pred_binary.view(-1).cpu().numpy()\n            \n            # Compute the accuracy for this batch and append to the overall list\n            batch_accuracy = accuracy_score(vectorize_true_masks, vectorize_pred_masks)\n            test_batch_accuray.append(batch_accuracy)\n            \n            # Compute the normalized confusion matrix for this batch and append to the overall list\n            batch_CM = confusion_matrix(vectorize_true_masks, vectorize_pred_masks, normalize='true', labels=[0, 1])\n            test_batch_CM.append(batch_CM)\n\n            # Print the progress\n            print(f'Test Batch {i+1}\/{n_test} - Loss: {batch_loss}, DICE score: {batch_dice_score}, Accuracy: {batch_accuracy}', end='\\r')\n\n    test_loss = np.array(test_batch_loss).mean()\n    test_dice = np.array(test_batch_dice).mean()\n    test_accuracy = np.array(test_batch_accuray).mean()\n    test_CM = np.array(test_batch_CM).mean(axis=0)\n    \n    return test_loss, test_dice, test_accuracy, test_CM","1b74067d":"if (cuda == True):\n    test_loss, test_dice, test_accuracy, test_CM = test_net(my_UNet.cuda(), test_dataloader, loss_function.cuda())\nelse:\n    test_loss, test_dice, test_accuracy, test_CM = test_net(my_UNet.cpu(), test_dataloader, loss_function.cpu())\n\n\nprint(f'Test Loss: {test_loss}, Test DICE score: {test_dice}, Test overall accuracy: {test_accuracy}')","ce65663b":"df_cm = pd.DataFrame(test_CM, index = ['Background', 'Cell'],\n                     columns = ['Background', 'Cell'])\nplt.figure(figsize = (12,10))\nplt.title('Confusion matrix')\nsns.heatmap(df_cm, annot = True, fmt='.2%', annot_kws = {\"size\": 15})\nplt.ylim([0, 2]);\nplt.ylabel('True labels');\nplt.xlabel('predicted labels');","c2762f73":"Also, let's check if our `BasicDataset` implementation works by pulling out a random sample of the training set.\n\n(Don't forget that we need to reverse some of the preprocessing steps, like changing the axis format and rescaling the image intensity to `[0, 255]`)","e6362c9d":"Now that we have trained our network, let us see how it will perform on the test set. \n\nFirst, let us plot the learning curve and the DICE scores per epoch.","fdb3dd3a":"### 2. Implement U-Net model (Modified version)\n\nNow that we have set up our data loaders, we can implement the architecture.\n\nThe original [U-Net architecture](https:\/\/arxiv.org\/abs\/1505.04597) is:\n\n<div align=\"center\">\n  <img width=\"800px\" src=\"https:\/\/lmb.informatik.uni-freiburg.de\/people\/ronneber\/u-net\/u-net-architecture.png\" \/>\n<\/div>\n\nNevertheless, as you can see, the input array and the output array are in different shapes. Thus, we will design a similar network but with a little bit of modification so that the input shape and output shape remain the same.\n\nThe following image show what exactly we will implement:\n\n<div align=\"center\">\n  <img width=\"800px\" src=\"https:\/\/github.com\/soroush361\/AoE_BME\/blob\/main\/modified_UNet_arch_2.png?raw=true\" \/>\n<\/div>\n\n\nTo implement this modified U-Net, we first define four blocks that we will use multiple times while designing the complete architecture. \\\n`DoubleCov` is block that contains these layers: Conv2d->BatchNormalization->ReLU->Conv2d->BatchNormalization->ReLU \\\n`Down` is a downsampling block that contains Maxpooling and `DoubleConv` after Maxpooling. (Decoding layers) \\\n`Up` is an upsampling block that upsamples the input then pass it through a `DoubleConv.` (Encoding Layers) \\\n`OutConv` is just a $(1\\times1)$ 2D convolution followed by a Sigmoid activation that serves as an output layer in our U-Net model.\n","e3d5f9e9":"Now we can mix the above modules to build our architecture.\n\nWe will assign `name`, `n_channels`, and `n_classes` to our implementation for organization purposes. In this case, we may set `name` as an arbitrary name (e.g., 'MyUNet'), `n_channels` must be **3** (Since the input image is three-channeled), and `n_classes` must be **1** (Because we have one class: 0 for background, and 1 for cells). If we had more than one class, we need to consider the background as a seperated class.","e1f8e52c":"Now that we have made sure our dataset implementation works fine, we can make the `DataLoader` per each set. We will set the batch size as **4** for all sets.\n\n<span style=\"color:red;font-size:18px;\" font>Important Note<\/span>: For this tutorial, we choose to work with **1000** samples for training, **200** samples for validation, and **200** samples for testing to reduce the training time. In your final project, you should use the whole dataset.","2b6d7b09":"#### 1.3 Create the DataLoaders for each set (train, valid, and test)\n\nUnlike the MNIST example, we cannot load all the images and their masks at once and train our network. We need to read each batch of the images\/masks and then train the network with that batch. This process should be repeated for other batches. This way, we will only have one batch at a time on the memory, preventing memory overflow.\n\nTo do so, we need a `BasicDataset` instance that reads an image and respective segmentation mask. \\\nIn below, we implemented the `BasicDataset` class that does the job. Furthermore, you can implement any kinds of preprocessing you wish to do in this implementation. For example, in this class, we resized the resolutions to $256\\times256$ pixels to speed up the training process (Original resolution is $512\\times512$) and reduce memory consumption. In addition, we scaled the intensities from $[0, 255]$ to $[0, 1]$. \\\nSince we will use **Pytorch** and Pytroch's input shape format is $[Batch, Channels, Width, Height]$ rather than $[Batch, Width, Height, Channels]$, we need to changed the image axis as well.","639e0e2b":"In MNIST example, you saw the training loop is very complecated and there are alot of things happening at the same time. Thus, we provide you with a single function, name `traine_net`, that takes the following arguments and trains the network. The input arguments are:\n\n1. `net`: The model you want to train.\n2. `epochs`: Number of epochs you want to train the network.\n3. `train_dataloader`: The training set `DataLoader`.\n4. `valid_dataloader`: The validation set `DataLoader`.\n5. `optimizer`: The optimizer algorithm.\n6. `loss_function`: The loss function.\n\nThis function creates a directory in `\/kaggle\/working\/{model_name}`, and saves the model weights per each epoch (starts at 1 for the first epoch). Also, it will calculate the [DICE score](https:\/\/en.wikipedia.org\/wiki\/S%C3%B8rensen%E2%80%93Dice_coefficient) for both training and validation samples.\nThe return values of this function are:\n\n1. `train_loss`: A list of average training loss per each epoch.\n2. `train_dice`: A list of average training DICE score per each epoch.\n3. `valid_loss`: A list of average validation loss per each epoch.\n4. `valid_dice`: A list of average validation DICE score per each epoch.","47aa5730":"Also make sure the internet access is on.","12b32106":"#### 1.1 First, we need to add the dataset to the our notebook. Follow these steps:\n\n##### 1.1.1. Click on \"Add Data\" button on \"Data\" panel.\n\n<div align=\"center\">\n  <img src=\"https:\/\/github.com\/soroush361\/AoE_BME\/blob\/main\/data_panel.png?raw=true\" \/>\n<\/div>\n\n##### 1.1.2. Search for \"SingleCellSegmentation\" dataset and click on \"Add\" button.\n\n<div align=\"center\">\n  <img width=\"600px\" src=\"https:\/\/github.com\/soroush361\/AoE_BME\/blob\/main\/singlecellseg_dataset.png?raw=true\" \/>\n<\/div>\n\n##### 1.1.3. Wait till the download is finished.\n\n<div align=\"center\">\n  <img width=\"1200px\" src=\"https:\/\/github.com\/soroush361\/AoE_BME\/blob\/main\/downloading_dataset.png?raw=true\" \/>\n<\/div>\n\n##### 1.1.4. You should see these folders in input folder of your notebook.\n\n<div align=\"center\">\n  <img src=\"https:\/\/github.com\/soroush361\/AoE_BME\/blob\/main\/singlecellseg_dataset_added.png?raw=true\" \/>\n<\/div>","161bcc56":"#### Check GPU availability (should be False if you run without GPU Accelerator, to turn it on, go to Settings->Accelerator on the right panel and select GPU).","b1eb6dec":"#### 3.2 Training loop","c28edb10":"# Art of Engineering - Biomedical Engineering Departmental Project\n### Abdhel Exinor - ae2647","4b9fd10d":"Now that we have everything in place, we can call the `train_net` function on our model and let it be trained for **10** epochs.","2ccb2b73":"### 1. Add the \"Single Cell Segmentation Dataset\" and create Data Loader","6d67bc3b":"### 3. Train the model","492d6b06":"As you can see, the predicted mask is not nearly close to the ground truth because we didn't train the network!","6acf2dfe":"Let us use the function and see how it works!\n\nNote: The accuracy and confusion matrix are computed on the CPU; thus, this function might be slower.","4437b76e":"Now we can check the performance of the best model on our previously pulled example. (The same scripts must work!)","0756ac91":"Now we can create an instance of our implemented model.","bfba9785":"As it is clear, the model is predicting the ground truth much better compared to before training.\n\nHowever, we need to compute the overall performance of the model on all test samples. Then, similar to the `train_net` function, we can define a `test_net` function that tests our model on test samples and save the prediction masks in the `\/kaggle\/working\/pred_mask` folder.\n\nThis function takes the following arguments:\n\n1. `net`: The model we want to test.\n2. `test_dataloader`: The `DataLoader` for the test set.\n3. `loss_function`: The loss function to calculate the loss.\n\nThis function returns:\n1. `test_loss`: The average test loss.\n2. `test_dice`: The average test DICE score.\n3. `test_accuracy`: The overall accuracy of the model.\n4. `test_CM`: The normalized confusion matrix of the model.\n\n","fcffea1d":"Before working on the training loop, let's define the **optimizer** and the **loss function**.\nIn this example, we will use the **\"ADAM\"** algorithm for the optimizer, and for the loss function, we will use **\"Binary Cross-Entropy\"**. You may change these later to see how different optimization algorithms and loss functions may affect the performance.\n\nFor the optimizer, we need to provide the network parameters and define the learning rate. Let's set it as `0.001` for now.","9febb31e":"Although we have not trained our model, we may see the output with random weights. Thus, we will pull a batch of test data loader, get the network's output, and plot it.","c56291c7":"#### Sections:\n1. Add the \"Single Cell Segmentation Dataset\" and create Data Loader\n2. Implement U-Net model (Modified version)\n3. Train the model\n4. Display results","37adb59d":"## Lecture 4 - Part C - Implementing a U-Net model for the \"Single Cell Segmentation Project\"","7a341b27":"### 4. Display results","cac1e191":"#### 1.2 Let us check how many samples we have in each folder (train, valid, test).\n\nIn **\"train\"** folder we must have 4140 samples, in **\"valid\"** folder 1380 samples, and in **\"test\"** folder there must be 2070 samples.\n\nIn each of these three folders, you\u2019ll see two subfolders: **\"image\"**, and **\"mask\"**.  In the **\"image\"** folder, you\u2019ll find several colored PNG images with a format of `image_{i:04}.png` where `i` starts from 1 to the number of samples in each set (E.g., for the train 1 to 4140, and for the test, 1 to 2070). These colored images must be used as the inputs for each phase. Similarly, you\u2019ll find their corresponding binary masks in ** \u201cmask\u201d** folder with a format of `mask_{i:04}.png`.\n\nWe may count the number of files in each folder using the `!ls` bash command and pipe it with the `wc -l` command. For example:","cd56f1aa":"Then, let us see in which epoch the model obtained the highest validation DICE score and load the weights of that epoch as our best model weights.","9123dc50":"Now we can create our three datasets and display the number of samples in each set in a pythonic way.","90f30838":"### Import the necessary packages.","da9a5de6":"#### 3.1 Optimizer and Loss function","0d3dad1c":"#### Set the random seed for reproducibility.\n\nThe random seed helps to make sure that the model parameter initialization, sequence of random shuffling, and most other nondeterministic operations are kept the same each time you run this notebook."}}