{"cell_type":{"a16f303c":"code","b685ebbd":"code","bb8b1116":"code","4dc1eca0":"code","d8a6a495":"code","2925ed74":"code","c59bc651":"code","047e2cc1":"code","7371094f":"code","6f22b266":"code","ef037b78":"code","15d42a98":"code","38696e7f":"code","a0e76d00":"code","b719c806":"code","6185d7e2":"code","5cd7f502":"code","4ba31fc7":"code","2153d93d":"code","daee6f6b":"code","17724a2a":"code","4258c5a8":"code","36fa4f62":"code","b24c194b":"code","7033f23c":"code","259d0c93":"code","6d39920d":"code","11356c5c":"code","7b6f82c3":"code","3e8d0811":"code","86c4969d":"code","c7aee357":"code","b0ab20d0":"code","e539cf74":"code","324b84e7":"code","c833d19b":"code","88f0ac06":"code","21974592":"code","afc5357b":"code","c9bc0f93":"code","9c3d8d6b":"code","ee3d0d5e":"code","47d135a9":"code","f4b2db69":"code","1dd45a81":"code","6dfa6cd0":"code","1cb7d3f9":"code","336db2a3":"code","4c4aba1d":"code","4f8b404e":"code","d26d3bb3":"code","e05254e1":"code","e330963b":"markdown","2391fd28":"markdown","89e6d01f":"markdown","ecb91d14":"markdown","e81654fd":"markdown","1b448055":"markdown","9cab1734":"markdown","48480a65":"markdown","b48b2be9":"markdown","fe8e958e":"markdown","d2225283":"markdown","6fa42216":"markdown","d9702750":"markdown","2ee4e6ed":"markdown","21e9ec69":"markdown","c3e11bed":"markdown","fefa3301":"markdown","f795de6c":"markdown"},"source":{"a16f303c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n","b685ebbd":"dataset = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv', header=0)","bb8b1116":"dataset.head()","4dc1eca0":"dataset_test=dataset.copy(deep=True)","d8a6a495":"import matplotlib.pyplot as plt\nfig,ax = plt.subplots(figsize=(5,13))\nax.pie(dataset_test['Outcome'].value_counts(),labels=['0','1'] ,colors=['red','green'],autopct='%1.2f%%')\nplt.show()","2925ed74":"dataset_test.dtypes","c59bc651":"dataset_test.isnull().sum()","047e2cc1":"(dataset_test==0).sum()","7371094f":"import numpy as np\nfor i in range(len(dataset_test.columns)-1):\n    dataset_test.iloc[:,i]=dataset_test.iloc[:,i].replace(0,np.NaN)\n#     print(test[i])\n(dataset_test==0).sum()\n# dataset.iloc[:,0]","6f22b266":"dataset_test.isnull().sum()\n","ef037b78":"dataset_test.head()","15d42a98":"# plt.plot(dataset['Pregnancies'],np.zeros_like(dataset['Pregnancies']),'o')\n# plt.plot(dataset['Glucose'],np.zeros_like(dataset['Pregnancies']),'o')\n# plt.plot(dataset['BloodPressure'],np.zeros_like(dataset['BloodPressure']),'o')\n# plt.plot(dataset['SkinThickness'],np.zeros_like(dataset['SkinThickness']),'o')\n# plt.plot(dataset['Insulin'],np.zeros_like(dataset['Insulin']),'o')\n# plt.plot(dataset['BMI'],np.zeros_like(dataset['BMI']),'o')\n# plt.show()\nimport seaborn as sns\ndef univariate(feat):\n#     plt.plot(dataset[feat],np.zeros_like(dataset[feat]),'o') ##scatter plot\n#     plt.xlabel(feat)\n#     plt.show()\n# #     plt.scatter(dataset.index,dataset[feat])\n#     sns.scatterplot(dataset.index,dataset[feat],hue=dataset['Outcome']) ##scatter plot\n#     plt.xlabel(feat)\n#     plt.show()\n#     plt.hist(dataset[feat]) # hist\n#     plt.show()\n#     dataset[feat].plot(kind='density') # kernel density function plot is probability density function (PDF)\n#     plt.show()\n#     dataset[feat].plot.kde()  ## kernel density function plot is probability density function (PDF)\n#     plt.show()\n    sns.distplot(dataset[feat],kde=True,hist=True,rug=False)  # dist plot (combine density plott , rug plot and hist plot)\n    plt.show()\n\n\n#     plt.boxplot(dataset[feat]) # bloxplot\n#     plt.show()\n\n    \n#     sns.stripplot(x=dataset['Outcome'],y=dataset[feat])\n#     plt.show()\n#     ls=sns.load_dataset(X)\n#     sns.kdeplot(data=X,x=X[feat])\n#     plt.show()\n    \n   \n\n    \nfor i in dataset.columns:\n    univariate(i)\n    ","38696e7f":"import seaborn as sns\ndef bivariate(feat1, feat2):\n\n#     sns.scatterplot(X[feat1],X[feat2],hue=y)\n#     plt.show()\n    \n#     sns.jointplot(data=X,x=feat1,y=feat2)\n#     plt.show()\n    sns.pairplot(dataset_test[[feat1,feat2]])\n    plt.show()\n        \n    \n    \n   \n\n    \n# for i in dataset.columns:\n#     bivariate(i)\nbivariate('Pregnancies','BMI') \n","a0e76d00":"dataset_test.columns","b719c806":"sns.pairplot(dataset_test,hue='Outcome',size=3)\n","6185d7e2":"import seaborn as sns \nco=dataset_test.corr()\nfig,ax=plt.subplots(figsize=(10,10))\nsns.heatmap(co,annot=True)","5cd7f502":"def medtarget(var):\n    temp=dataset_test[dataset_test[var].notnull()]\n#     print(temp)\n    print(temp[[var, 'Outcome']].groupby(['Outcome']))\n    #print(temp)\n    return temp[[var, 'Outcome']].groupby(['Outcome']).mean()\n\nmedtarget('Insulin')\n\n","4ba31fc7":"dataset_test.loc[(dataset_test['Outcome']==0) & (dataset_test['Insulin'].isnull()), 'Insulin' ]=130.287879\ndataset_test.loc[(dataset_test['Outcome']==1) & (dataset_test['Insulin'].isnull()), 'Insulin' ]=206.846154","2153d93d":"medtarget('Pregnancies')","daee6f6b":"dataset_test.loc[(dataset_test['Outcome']==0) & (dataset_test['Pregnancies'].isnull()), 'Pregnancies' ]=3.861827\ndataset_test.loc[(dataset_test['Outcome']==1) & (dataset_test['Pregnancies'].isnull()), 'Pregnancies' ]=5.669565","17724a2a":"medtarget('BloodPressure')","4258c5a8":"dataset_test.loc[(dataset_test['Outcome']==0) & (dataset_test['BloodPressure'].isnull()), 'BloodPressure' ]=70.877339\ndataset_test.loc[(dataset_test['Outcome']==1) & (dataset_test['BloodPressure'].isnull()), 'BloodPressure' ]=75.321429","36fa4f62":"medtarget('SkinThickness')","b24c194b":"dataset_test.loc[(dataset_test['Outcome']==0) & (dataset_test['SkinThickness'].isnull()), 'SkinThickness' ]=27.235457\ndataset_test.loc[(dataset_test['Outcome']==1) & (dataset_test['SkinThickness'].isnull()), 'SkinThickness' ]=33","7033f23c":"medtarget('BMI')","259d0c93":"\ndataset_test.loc[(dataset_test['Outcome']==0) & (dataset_test['BMI'].isnull()), 'BMI' ]=30.859674\ndataset_test.loc[(dataset_test['Outcome']==1) & (dataset_test['BMI'].isnull()), 'BMI' ]=35.406767","6d39920d":"medtarget('Glucose')","11356c5c":"\ndataset_test.loc[(dataset_test['Outcome']==0) & (dataset_test['Glucose'].isnull()), 'Glucose' ]=110.643863\ndataset_test.loc[(dataset_test['Outcome']==1) & (dataset_test['Glucose'].isnull()), 'Glucose' ]=142.319549","7b6f82c3":"(dataset_test==0).sum()","3e8d0811":"# for Outlier detection\n\nimport seaborn as sns\nfig,ax = plt.subplots(figsize=(20,12))\nsns.boxplot(data =dataset_test)","86c4969d":"dataset_final=dataset_test.copy(deep=True)\nupbond = dataset_final['Pregnancies'].quantile(0.99) \ndataset_final['Pregnancies'] = np.where(dataset_final['Pregnancies']>upbond,upbond,dataset_final['Pregnancies'])\n\nupbond = dataset_final['BloodPressure'].quantile(0.98) \nlobond = dataset_final['BloodPressure'].quantile(0.01) \ndataset_final['BloodPressure'] = np.where(dataset_final['BloodPressure']>upbond,upbond,dataset_final['BloodPressure'])\ndataset_final['BloodPressure'] = np.where(dataset_final['BloodPressure']<lobond,lobond,dataset_final['BloodPressure'])\n\n\nupbond = dataset_final['SkinThickness'].quantile(0.93) \nlobond = dataset_final['SkinThickness'].quantile(0.10) \ndataset_final['SkinThickness'] = np.where(dataset_final['SkinThickness']>upbond,upbond,dataset_final['SkinThickness'])\ndataset_final['SkinThickness'] = np.where(dataset_final['SkinThickness']<lobond,lobond,dataset_final['SkinThickness'])\n\n\nupbond = dataset_final['Insulin'].quantile(0.93) \ndataset_final['Insulin'] = np.where(dataset_final['Insulin']>upbond,upbond,dataset_final['Insulin'])\n\n\nupbond = dataset_final['BMI'].quantile(0.98) \ndataset_final['BMI'] = np.where(dataset_final['BMI']>upbond,upbond,dataset_final['BMI'])\n\n\nupbond = dataset_final['DiabetesPedigreeFunction'].quantile(0.96) \ndataset_final['DiabetesPedigreeFunction'] = np.where(dataset_final['DiabetesPedigreeFunction']>upbond,upbond,dataset_final['DiabetesPedigreeFunction'])\n\n\nupbond = dataset_final['Age'].quantile(0.96) \ndataset_final['Age'] = np.where(dataset_final['Age']>upbond,upbond,dataset_final['Age'])","c7aee357":"import seaborn as sns\nfig,ax = plt.subplots(figsize=(20,12))\nsns.boxplot(data =dataset_final)","b0ab20d0":"X= dataset_final.drop('Outcome',axis=1)\ny=dataset_final['Outcome']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test , y_train , y_test = train_test_split(X, y, test_size=0.25,stratify=y)\n","e539cf74":"print(X_train.shape,\" \",y_train.shape)\nprint(X_test.shape,\" \",y_test.shape)","324b84e7":"# Input  and Output both are Continouts\n# two methos for Feature Selection \n# A -> Pearson Corelation -> f_regression\n# B -> Mutual INFO -> mutual_info_regression\n# C -> spearman \n\nfrom sklearn.feature_selection import  SelectKBest , f_regression , mutual_info_regression\nfs=SelectKBest(score_func=f_regression,k='all')\nfs.fit(X_train,y_train)\nprint(\"fss,\",fs.scores_)\nplt.bar(range(len(fs.scores_)),fs.scores_)\nplt.show()\n# scores,p_value = f_regression(X_train,y_train)\n# print(\"f \",f_regression(X_train,y_train))\n# print(\"p \",p_value)\n\n# fig ,ax =plt.subplots(figsize=(40,25))\n# plt.bar(range(len(p_value)),p_value)\n# # plt.bar(range(len(values)), list(values), align='center')\n# # plt.xticks(range(len(p_value)),list(p_value))\n# plt.show()\n# for i in range(len(fs.scores_)):\n#     print(i,\" \",fs.scores_[i])\n    \n# X_train_fs=fs.transform(X_train)\n# X_test_fs=fs.transform(X_test)\n\n    \nfs1=SelectKBest(score_func=mutual_info_regression,k='all')\nfs1.fit(X_train,y_train)\nplt.bar(range(len(fs1.scores_)),fs1.scores_)\nplt.show()\n\n# for i in range(len(fs1.scores_)):\n#     print(i,\" \",fs1.scores_[i])\n# fig ,ax =plt.subplots(figsize=(40,25))\n# plt.bar(range(len(scores)),scores)\n# # plt.bar(range(len(values)), list(values), align='center')\n# # plt.xticks(range(len(ch)),list(ch.keys()))\n# plt.show()\n    ","c833d19b":"fs=SelectKBest(score_func=f_regression,k=7)\nfs.fit(X_train,y_train)\nX_train_fs=fs.transform(X_train)\nX_test_fs=fs.transform(X_test)\nprint(X_train_fs.shape,\" \",X_test_fs.shape)","88f0ac06":"from sklearn.preprocessing import StandardScaler\nstand=StandardScaler().fit(X_train_fs)\nX_stand_fs_train=pd.DataFrame(stand.transform(X_train_fs))\nX_stand_fs_test=pd.DataFrame(stand.transform(X_test_fs))\nprint(X_stand_fs_train)","21974592":"# X_temp=pd.DataFrame(X_temp,columns=['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction'\n#                                    ,'Age'])","afc5357b":"#X_temp=dataset_final.drop(['BMI','Glucose','BloodPressure','Age'],axis=1 ) #,'BloodPressure','BMI','Glucose'\nfrom statsmodels.stats.outliers_influence import   variance_inflation_factor\nvif=pd.DataFrame()\nvif['Var Name']=X_stand_fs_train.columns\nvif['vif values']=[variance_inflation_factor(X_stand_fs_train.values,i) for i in range(X_stand_fs_train.shape[1])]\nprint(vif)","c9bc0f93":"X_stand_fs_train.head()","9c3d8d6b":"X_stand_fs_train=X_stand_fs_train.drop([3,4,5,6],axis=1)\nX_stand_fs_test=X_stand_fs_test.drop([3,4,5,6],axis=1)","ee3d0d5e":"print(X_stand_fs_train.head())\nX_stand_fs_test.head()","47d135a9":"from sklearn.metrics import plot_confusion_matrix , accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nlog=LogisticRegression()\nlog.fit(X_stand_fs_train,y_train)\ny_pred_train=log.predict(X_stand_fs_train)\n# con=confusion_matrix(y_test,y_pred)\nplot_confusion_matrix(log,X_stand_fs_train,y_train)\nprint(\"train_accuracy \",accuracy_score(y_train,y_pred_train))\ny_pred=log.predict(X_stand_fs_test)\n# con=confusion_matrix(y_test,y_pred)\nplot_confusion_matrix(log,X_stand_fs_test,y_test)\nprint(\"test_Accuracy\",accuracy_score(y_test,y_pred))","f4b2db69":"from sklearn.svm import SVC\nsvc=SVC()\nsvc.fit(X_stand_fs_train,y_train)\ny_pred_train=svc.predict(X_stand_fs_train)\n# con=confusion_matrix(y_test,y_pred)\nplot_confusion_matrix(svc,X_stand_fs_train,y_train)\nprint(\"train_accuracy \",accuracy_score(y_train,y_pred_train))\ny_pred=svc.predict(X_stand_fs_test)\n# con=confusion_matrix(y_test,y_pred)\nplot_confusion_matrix(svc,X_stand_fs_test,y_test)\nprint(\"test_Accuracy\",accuracy_score(y_test,y_pred))","1dd45a81":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier()\nknn.fit(X_stand_fs_train,y_train)\ny_pred_train=knn.predict(X_stand_fs_train)\n# con=confusion_matrix(y_test,y_pred)\nplot_confusion_matrix(knn,X_stand_fs_train,y_train)\nprint(\"train_accuracy \",accuracy_score(y_train,y_pred_train))\ny_pred=knn.predict(X_stand_fs_test)\n# con=confusion_matrix(y_test,y_pred)\nplot_confusion_matrix(knn,X_stand_fs_test,y_test)\nprint(\"test_Accuracy\",accuracy_score(y_test,y_pred))","6dfa6cd0":"# from sklearn.pipeline import make_pipeline\n# from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.metrics import classification_report\n# from sklearn.model_selection import train_test_split\n# X_train, X_test , y_train, y_test=  train_test_split(X,y,test_size=0.33,random_state=1,stratify=y)\n\n# pipe=make_pipeline(StandardScaler(), LogisticRegression())\n# #print(pipe)\n# C_range = 10. **np.arange(-3, 8)\n# penalt = ['l2']\n# #[1, 10, 100, 1000]\n# solver=['newton-cg', 'lbfgs', 'sag']\nparam_grid1 ={'penalty': ['l2'],'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000],'solver':['newton-cg', 'lbfgs', 'sag']} \n# dict(logisticregression__C=C_range,logisticregression__penalty=penalt,\n#                    logisticregression__solver=solver)\ngrid1 = GridSearchCV(LogisticRegression(), param_grid=param_grid1, cv=5,scoring='roc_auc')\nmodel=grid1.fit(X_stand_fs_train,y_train)\nprint('Best roc_auc: {:.4}, with best : {}'.format(grid1.best_score_, grid1.best_params_))\nprint(model.score(X_stand_fs_train,y_train))\nprint(model.score(X_stand_fs_test,y_test))\n","1cb7d3f9":"from sklearn.metrics import classification_report\ny_pred=grid1.predict(X_stand_fs_test)\nprint(classification_report(y_test,y_pred))","336db2a3":"log=LogisticRegression(C= 0.1,penalty= 'l2', solver= 'newton-cg')\nlog.fit(X_stand_fs_train,y_train)\ny_pred=log.predict(X_stand_fs_test)\nfrom sklearn.metrics import confusion_matrix\ntn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()\naccuracy = (tp+tn)\/(tp+tn+fp+fn)\n\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(log, X_stand_fs_test,y_test)\nplt.show()\nprecision=tp \/ (tp + fp) \nrecall=tp \/ (tp + fn)\nprint(\"test accuracy \", accuracy)\nprint(\"Precision \",precision)\nprint(\"Recall \",recall)\nprint(\"F1 score \",2*((precision*recall)\/(precision+recall)))\n# print(tn, fp, fn, tp)","4c4aba1d":"from sklearn.model_selection  import GridSearchCV\nfrom sklearn.svm import SVC\n# from sklearn.model_selection import train_test_split\n# X_train, X_test , y_train, y_test=  train_test_split(X,y,test_size=0.33,random_state=1,stratify=y)\nC_range = [0.1,1, 10, 100]\ngamma_range = [1,0.1,0.01,0.001]\nkernel= ['rbf', 'poly', 'sigmoid']\n# ?pipe=make_pipeline(StandardScaler(),SVC())\nparam_grid = {'C':[0.1,1, 10, 100],'gamma':[1,0.1,0.01,0.001],'kernel':['rbf', 'poly', 'sigmoid']}\ngrid = GridSearchCV(SVC(),param_grid=param_grid, cv=5,scoring='roc_auc')\nmodel=grid.fit(X_stand_fs_train,y_train)\nprint('Best roc: {:.4}, with best C: {}'.format(grid.best_score_, grid.best_params_))\nprint(model.score(X_stand_fs_train,y_train))\nprint(model.score(X_stand_fs_test,y_test))","4f8b404e":"from sklearn.metrics import classification_report\ny_pred=grid.predict(X_stand_fs_test)\nprint(classification_report(y_test,y_pred))","d26d3bb3":"\nsvc=SVC(C=1, gamma= 0.1, kernel='sigmoid',probability=True)\nsvc.fit(X_stand_fs_train,y_train)\ny_pred=svc.predict(X_stand_fs_test)\nfrom sklearn.metrics import confusion_matrix\ntn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()\naccuracy = (tp+tn)\/(tp+tn+fp+fn)\n\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(log, X_stand_fs_test,y_test)\nplt.show()\nprecision=tp \/ (tp + fp) \nrecall=tp \/ (tp + fn)\nprint(\"test accuracy \", accuracy)\nprint(\"Precision \",precision)\nprint(\"Recall \",recall)\nprint(\"F1 score \",2*((precision*recall)\/(precision+recall)))","e05254e1":"\nfrom sklearn.metrics import roc_curve, roc_auc_score,plot_roc_curve, confusion_matrix\ny_pred_log=log.predict_proba(X_stand_fs_test)\ny_pred_svc=svc.predict_proba(X_stand_fs_test)\n\n\nauc_l = roc_auc_score(y_test,y_pred_log[:,1])\nauc_s = roc_auc_score(y_test,y_pred_svc[:,1])\n\nfpr_l, tpr_l, thr_l = roc_curve(y_test, y_pred_log[:,1])\nfpr_s, tpr_s, thr_s = roc_curve(y_test, y_pred_svc[:,1])\n\nplt.subplots(1, figsize=(10,10))\n# plt.title('Receiver Operating Characteristic')\nplt.plot(fpr_l, tpr_l,\"blue\",label=\"log, auc=\"+str(auc_l))\nplt.plot(fpr_s, tpr_s,\"red\",label=\"svc, auc=\"+str(auc_s))\n# plt.plot(fpr_knn, tpr_knn,\"green\",label=\"knn, auc=\"+str(auc_knn))\nplt.plot([0, 1], ls=\"--\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend(loc='best')\nplt.show()","e330963b":"**Above u can see, almost all outlier are removed.**","2391fd28":"**There are features that have 0. We need convert 0 to 'Nan'.  **","89e6d01f":"# Feature Selection ","ecb91d14":" **I have used all feature in my model and then\nmodel are overfitted. So i removed 3 ,4 ,5 ,6 feature above model. \nNow logistic , svc are not overfitted but knn is overfitted.**","e81654fd":"**IN feature selection result, both result are almost same. You can use any one of them.\nIm using 'f_regresssion'.**","1b448055":"# for Outlier detection","9cab1734":"**SVC**\n","48480a65":"**Te predictors may be moderately correlated. The output above shows that \nthe VIF for the Publication and Years factors are about 1.7, which indicates some correlation,\nbut not enough to be overly concerned about.**\n\n","b48b2be9":"**As you can see 'DiabetesPedigreeFunction' and 'BlooPresure' have lowest relationship with 'outcome'.\n**","fe8e958e":"# Multivariate EDS","d2225283":"**Now we will use 'quantile' technique to remove outliers.**","6fa42216":"**Note: We can use 'accuray' or 'f1_score' instead of 'roc_curve' **","d9702750":"# Bvariate EDA","2ee4e6ed":"# Spliting Data","21e9ec69":"# Remove NAn Value with their mean accoring to class 0 and 1","c3e11bed":"**In bivariate analysis we can take two feature. But in Multivariate Analysis, we will have\nall combination**","fefa3301":"**We will use logistic regression and SVC \nApply grid search on LOgistic Regression and SVC****","f795de6c":"# I will remove feature 3, 4, 5, 6, because  they are correlated with each other.\n# And there is multicolinearity with these feature."}}