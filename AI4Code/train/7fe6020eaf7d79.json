{"cell_type":{"fbac1153":"code","177178e0":"code","3200d771":"code","635a0436":"code","11e5a6d5":"code","18ccf0ea":"code","06228e81":"code","88ce2c2f":"code","51873b73":"code","0df624be":"code","8421ee18":"code","67397a48":"code","dd09362e":"code","756ea8b3":"code","2225db82":"code","0b0ab3d6":"code","4ca28535":"code","b92252df":"code","d13e2bbf":"code","cb9c3ce4":"code","9babbb07":"code","989839e4":"code","2b9389cf":"code","3b60434a":"code","f39e4fc8":"code","9c02929a":"code","1af37b0a":"code","399f1b04":"code","71d0bd3b":"markdown","03f21a00":"markdown","3d49b3ad":"markdown","bed11b63":"markdown","732aeb23":"markdown","f12eeebe":"markdown","031ec1f3":"markdown","d09fad9f":"markdown","e7f1cdb3":"markdown","e7f8f09e":"markdown","b368ddb2":"markdown","14f0006a":"markdown","8afe8352":"markdown","98804194":"markdown","ac60b294":"markdown"},"source":{"fbac1153":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","177178e0":"import matplotlib.pyplot as plt\nimport seaborn as sns","3200d771":"df = pd.read_csv('\/kaggle\/input\/Classified Data')","635a0436":"df.head() #checking the head of our data","11e5a6d5":"df.drop(['Unnamed: 0'],axis = 1,inplace = True) #dropping the column","18ccf0ea":"df.isnull().sum() #checking for null values","06228e81":"#plt.figure(figsize = (10,10)) #getting a visual of our data\n#sns.heatmap(df)","88ce2c2f":"sns.scatterplot(x='WTT',y='PTI',data = df)","51873b73":"sns.scatterplot(x='EQW',y='SBI',data = df)","0df624be":"sns.scatterplot(x='LQE',y='QWG',data = df)","8421ee18":"sns.scatterplot(x='FDJ',y='PJF',data = df)","67397a48":"sns.scatterplot(x='HQE',y='NXJ',data = df)","dd09362e":"from sklearn.model_selection import train_test_split #importing our libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import classification_report,confusion_matrix","756ea8b3":"scaler = StandardScaler()","2225db82":"scaler.fit(df.drop('TARGET CLASS',axis = 1)) #standard scaling our data for better results","0b0ab3d6":"scaled_features = scaler.transform(df.drop('TARGET CLASS',axis = 1))","4ca28535":"df_feat = pd.DataFrame(scaled_features,columns = df.columns[:-1])","b92252df":"df_feat.head()","d13e2bbf":"x = df_feat\ny = df['TARGET CLASS']","cb9c3ce4":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 101)","9babbb07":"\nkmeans = KMeans(4)\n","989839e4":"\nkmeans.fit(x_train,y_train)","2b9389cf":"\npred = kmeans.fit_predict(x_test)","3b60434a":"print(classification_report(y_test,pred))\nprint('\\n')\nprint(confusion_matrix(y_test,pred))","f39e4fc8":"\nwcss = []\n# 'cl_max' is a that keeps track the highest number of clusters we want to use the WCSS method for.\n# Note that 'range' doesn't include the upper boundery\ncl_max = 10\nfor i in range (1,cl_max):\n    kmeans= KMeans(i)\n    kmeans.fit(x)\n    wcss_iter = kmeans.inertia_\n    wcss.append(wcss_iter)\nwcss","9c02929a":"plt.figure(figsize = (10,6))\nplt.plot(range(1,10),wcss)\nplt.title('Cluster values')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')","1af37b0a":"kmeans = KMeans(2)\nkmeans.fit(x_train,y_train)\npred1 = kmeans.predict(x_test)","399f1b04":"print(classification_report(y_test,pred1))\nprint('\\n')\nprint(confusion_matrix(y_test,pred1))","71d0bd3b":"****To get the best result possible we should select the optimum number of clusters ,with the help of the ELBOW method. We can see our graph and determine the fact that the elbow clearly occurs at 2 and hence,the optimum number of clusters is 2 which will provide us with a better accuracy.","03f21a00":"Now our model predicts whether a person can get selected with an average accuracy of 95% (without overfitting the data).","3d49b3ad":"Taking a look at our data, we can see that the unnamed column is not needed and we can work well wihtout it. Hence, we will drop it.\n\n\n\n\n\n\n","bed11b63":"Now that we have our data scaled properly, let us define our features and target and fit the model to it.","732aeb23":"Most companies dont provide a person with their exact company data during an interview.\nThey change the column names and make the data completely random for the person.\nThis dataset is one of those data set where we cant correlate any column with the other and have to apply KMeans method on to determine whether or not he is selected.\nIn this process, we will also be looking at the ELBOW method to determine the optimum number of clusters.\n\n\n\n\n\n","f12eeebe":"As we can see we have many varying values in our data. Even though we cannot correlate any column with the other, lets try to find some trend in our data","031ec1f3":"We computed the WCSS v\/s the K values for our model and we can plot a graph that shows us these values and analyse our K value better\n\n\n\n\n\n\n","d09fad9f":"Initially , our model did a below-average job of correctly correlating the predictions and targets.\nwe can get better results than this by defining a within-cluster sum of squares value.\nEvery good KMeans model requires a gradually descending wcss value as by increasing the number of clusters but a higher number of clusters may not always be a good idea, depending on the type of data.\nNow, lets improve our model.","e7f1cdb3":"Lets import our data set and take a look at it","e7f8f09e":"We will standard scale our data inorder to have better results from our data","b368ddb2":"Luckily no null values","14f0006a":"We feel with clustering more the number of clusters , the better the solution.\nThis isn't necessarily the case, as we will see.\n","8afe8352":"KMeans Clustering","98804194":"Lets go on check for any null values in our data that might affect our final result.","ac60b294":"From the plots we can say that all of our data lies between 0 and 2. We have no negative values in our data.We couldn't deduce much from the data that we have but we can get a few insights from such data just by plotting a few graphs and maybe even the sector in which the company works can help you  correlate the data that is given to you. So lets start working on our model"}}