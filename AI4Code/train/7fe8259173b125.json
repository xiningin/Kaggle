{"cell_type":{"d43836d6":"code","f33bfc4f":"code","8138baec":"code","d5ab4bf9":"code","16451105":"code","25e2f055":"code","c0b40e2c":"code","b6cd9e2e":"code","a592ef71":"code","8f068c0f":"code","b0c358ee":"code","fb6f9505":"code","c24ea0b3":"code","6a3420ac":"markdown","d363720d":"markdown","0cfd0f92":"markdown","63c25f18":"markdown","dda4f7b5":"markdown","2c21c8a7":"markdown","a5741326":"markdown","599f869f":"markdown","1035d7d2":"markdown"},"source":{"d43836d6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f33bfc4f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\n# change directory to the location containing the data for the notebook\nos.chdir(r'\/kaggle\/input\/coursera-andrewng-ml-dataset')\nos.getcwd()","8138baec":"data = pd.read_csv('ex2data2.txt', names=['Test1', 'Test2', 'Status'])\n# input feature matrix containing two column vectors representing the score of the two tests\nX = data.iloc[:, 0:2].values\n# output variable vector representing whether the chip was accepted or rejected. Note this is a numpy 1d array\ny1d = data.iloc[:, 2].values\ndata.head()","d5ab4bf9":"def plot_raw_data(X, y):\n    accepted = y == 1\n    X_accepted = X[accepted]\n    X_rejected = X[~accepted]\n    fig, ax = plt.subplots(figsize=(10,6)) \n    plt.plot(X_accepted[:, 0], X_accepted[:, 1], '+', ms=8, mew=2, color='black', label='accepted')\n    plt.plot(X_rejected[:, 0], X_rejected[:, 1], 'o', ms=8, mew=1, mfc='cyan', mec='k',label='rejected')\n    plt.xlabel('Microchip Test1')\n    plt.ylabel('Microchip Test2')\n    return fig, ax\n    \nfig, ax = plot_raw_data(X, y1d)    \nplt.title('Scatter plot of training data')\nplt.legend()\nplt.show()","16451105":"def get_degree_pairs(degree):\n    degree1 = 0\n    degree2 = degree\n    list_degree_pairs = []\n    while degree1 <= degree and degree2 >= 0:\n        if (degree1 + degree2) == degree:\n            list_degree_pairs.append((degree1, degree2))\n            degree1 += 1\n            degree2 -= 1\n    return list_degree_pairs\n\ndef feature_mapping(x1, x2, degree):    \n    ones = np.ones(len(x1)).reshape(-1, 1)    \n    out_X = np.concatenate((ones, x1.reshape(-1, 1)), axis=1)\n    out_X = np.concatenate((out_X, x2.reshape(-1, 1)), axis=1)\n    degree_counter = 2\n    while degree_counter <= degree:\n        list_degree_pairs = get_degree_pairs(degree_counter)\n        for degree_pair in list_degree_pairs:\n            # We are doing an element wise multiplication of the two vectors of size mx1 to create polynomial features\n            temp = np.multiply(x1**degree_pair[0], x2**degree_pair[1])            \n            out_X = np.concatenate((out_X, temp.reshape(-1, 1)), axis=1)\n        degree_counter += 1\n    return out_X","25e2f055":"# test 1 scores\nx1 = X[:, 0]\n# test 2 scores\nx2 = X[:, 1]\nX_poly = feature_mapping(x1, x2, 6)","c0b40e2c":"# import the sigmoid function available in scipy\nfrom scipy.special import expit\n\ndef cost_function_reg(theta, X, y, lamda):\n    theta = theta.reshape(-1, 1)\n    h = expit(np.dot(X, theta))\n    m = len(X)    \n    # J is a scalar \n    J = -(1\/m) * (np.dot(np.transpose(y), np.log(h)) + np.dot(np.transpose(1 - y), np.log(1 - h))) + (lamda\/(2*m)) * np.sum(theta[1:]**2)\n    grad = (1 \/ m) * np.dot(np.transpose(X), (h - y))   \n    #print(grad.shape)\n    #print(grad)\n    grad_reg = (lamda \/ m) * theta[1:]    \n    #print(grad_reg.shape)\n    grad_reg = np.insert(grad_reg, 0, 0)    \n    #print(grad_reg.shape)\n    grad = grad + grad_reg.reshape(-1, 1)\n    return J, grad","b6cd9e2e":"n = X_poly.shape[1]\ninitial_theta = np.zeros(n)\ny = y1d.reshape(-1, 1)\ncost, grad = cost_function_reg(initial_theta, X_poly, y, 1)\nprint(cost[0][0])\nprint('Gradient at initial theta (zeros) - first five values only:')\n# format. * unpacks the arguments so if there are, say 4 placeholders and 4 elements in your list, then format unpacks the args and fills the slots.\nprint('{:.4f} {:.4f} {:.4f} {:.4f} {:.4f}'.format(*grad.flatten()[:5]))\nprint('Expected gradients (approx) - first five values only:')\nprint('\\t[0.0085, 0.0188, 0.0001, 0.0503, 0.0115]\\n')","a592ef71":"from scipy.optimize import minimize\n\ndef get_optimized_cost_theta(lamda):\n    options= {'maxiter': 100}\n    opt = minimize(fun=cost_function_reg, x0=initial_theta, args=(X_poly, y, lamda), jac=True, method='TNC', options=options)\n    optimized_theta = opt.x\n    cost_optimized = opt.fun\n    return cost_optimized, optimized_theta    \n\nopt_cost1, opt_theta1 = get_optimized_cost_theta(1.00)    \nprint('Optimized cost: {}'.format(opt_cost1))\nprint('Optimized theta: {}'.format(opt_theta1))","8f068c0f":"def predict(theta, X):\n    product = np.dot(X, theta)\n    classifier = lambda item: 1 if item > 0 else 0\n    v_classifier = np.vectorize(classifier)\n    return v_classifier(product)\n\npredicted_y = predict(opt_theta1, X_poly)\nis_correct = predicted_y == y1d\naccuracy = len(predicted_y[is_correct]) \/ len(is_correct)\nprint('Accuracy of predictions: {}'.format(round(accuracy, 2)))","b0c358ee":"def plot_decision_boundary(theta, X, y):\n    if X.shape[1] <= 3:\n        plot_x = np.array([np.min(X[:, 1]), np.max(X[:, 1])])\n        plot_y = -1 \/ theta[2] * (theta[1] * plot_x + theta[0])\n        plt.plot(plot_x, plot_y, label='decision boundary')\n        plt.legend()\n        plt.show()\n    else:\n        # Here is the grid range\n        x1 = np.linspace(-1, 1.5, 50);\n        #print(x1)\n        x2 = np.linspace(-1, 1.5, 50);\n        #print(x2)\n        z = np.zeros((len(x1), len(x2)));\n        # Evaluate z = theta*x over the grid\n        for i in range(1, len(x1)):\n            for j in range(1, len(x2)):\n                z[i,j] = np.dot(feature_mapping(np.array([x1[i]]), np.array([x2[j]]), 6), theta)        \n        z = np.transpose(z)\n        plt.contour(x1, x2, z, levels=[0], linewidths=2, colors='g')\n        #plt.contourf(x1, x2, z, levels=[np.min(z), 0, np.max(z)], cmap='Greens', alpha=0.4)\n\nplot_raw_data(X, y1d)  \nplot_decision_boundary(opt_theta1, X_poly, y)\nplt.title('Decision Boundary with lambda = 1')\nplt.legend()\nplt.show()","fb6f9505":"opt_cost100, opt_theta100 = get_optimized_cost_theta(0.00)    \nplot_raw_data(X, y1d)\nplot_decision_boundary(opt_theta100, X_poly, y)\nplt.title('Decision Boundary with lambda = 0 (Overfitting)')\nplt.legend()\nplt.show()","c24ea0b3":"opt_cost100, opt_theta100 = get_optimized_cost_theta(100.00)    \nplot_raw_data(X, y1d)\nplot_decision_boundary(opt_theta100, X_poly, y)\nplt.title('Decision Boundary with lambda = 0 (Underfitting)')\nplt.legend()\nplt.show()","6a3420ac":"## Visualize the raw input data","d363720d":"## Feature mapping\n\nOne way to fit the data better is to create more features from each data point.We will map the features into all polynomial terms of x1 and x2 up to the sixth power.As a result of this mapping, our vector of two features (the scores on two QA tests) has been transformed into a 28-dimensional vector. A logistic regression classifier trained on this higher-dimension feature vector will have a more complex decision boundary and will appear nonlinear when drawn in our 2-dimensional plot.\n\nWhile the feature mapping allows us to build a more expressive classifier,it also more susceptible to overfitting. In the next parts of the exercise, you\nwill implement regularized logistic regression to fit the data and also see for yourself how regularization can help combat the overfitting problem.","0cfd0f92":"The regularized cost function should return a cost of 0.693 using an initial theta value of all zeros","63c25f18":"# Regularized logistic regression\n\nIn this notebook, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.\n\nSuppose you are the product manager of the factory and you have the test results for some microchips on two di\u000berent tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model.","dda4f7b5":"The plot above shows that our dataset cannot be separated into positive and negative examples by a straight-line through the plot. Therefore, a straight-\nforward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary.","2c21c8a7":"## Learning parameters using fminunc \noptimization solver that finds the minimum of an unconstrained function","a5741326":"## Cost function and gradient\n\nNow you will implement code to compute the cost function and gradient for regularized logistic regression. The regularized cost function is defined as:\n\n$$\nJ\\left(\\theta\\right) = \\frac{1}{m}\\sum_{i=1}^m\\left[-y^\\left(i\\right)\\log \\left(h_\\theta\\left(x^\\left(i\\right)\\right)\\right) - \\left(1-y^\\left(i\\right)\\right)\\log \\left(1 - h_\\theta\\left(x^\\left(i\\right)\\right)\\right) \\right] + \\frac{\\lambda}{2m}\\sum_{j=1}^n\\theta_j^2\n$$\n\nNote that you should not regularize the parameter $\\theta_0$. Hence, you should not be regularizing the theta(0) parameter (which corresponds to $\\theta_0$) in the code.","599f869f":"## Load the input data","1035d7d2":"## Plotting the decision boundary"}}