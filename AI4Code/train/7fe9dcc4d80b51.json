{"cell_type":{"a2a25857":"code","bff73e1f":"code","2dbb8692":"code","a9f20632":"code","a2bb40d7":"markdown","14a04c17":"markdown"},"source":{"a2a25857":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bff73e1f":"import pandas as  pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2dbb8692":"glass = pd.read_csv(\"..\/input\/glass.csv\")","a9f20632":"from sklearn.neighbors import KNeighborsClassifier as KNC\nfrom sklearn.model_selection import train_test_split\n\nglass['RI'].value_counts()\nglass['Na'].value_counts()\nglass['Mg'].value_counts()\nglass['Al'].value_counts()\nglass['Si'].value_counts()\nglass['K'].value_counts()\nglass['Ca'].value_counts()\nglass['Ba'].value_counts()\nglass['Fe'].value_counts()\nglass['Type'].value_counts()\n\nnp.mean(glass.Mg)\nnp.max(glass.Mg)-np.min(glass.Mg)\nnp.min(glass.RI)\nglass['Mg']=np.where(glass.Mg > 2.68,'1','0')\n\nglass['Mg'].value_counts()\n\nnp.mean(glass.K)\nnp.max(glass.K)-np.min(glass.K)\nnp.min(glass.K)\nglass['K']=np.where(glass.K > 0.49,'1','0')\n\nglass['K'].value_counts()\n\n\nnp.mean(glass.Ba)\nnp.max(glass.Ba)-np.min(glass.Ba)\nnp.min(glass.Ba)\nglass['Ba']=np.where(glass.Ba > 0.17,'1','0')\n\nglass['Ba'].value_counts()\n\n\nnp.mean(glass.Fe)\nnp.max(glass.Fe)-np.min(glass.Fe)\nnp.min(glass.Fe)\nglass['Fe']=np.where(glass.Fe> 0.05,'1','0')\n\nglass['Fe'].value_counts()\n\n\ntrain,test=train_test_split(glass,test_size=0.2,random_state=0)\n\n#to get perfect n_neighboors need to perform parameter tuning or bagging method\nneigh=KNC(n_neighbors=3)\n\n\nneigh.fit(train.iloc[:,0:9],train.iloc[:,9])\n\n\ntrain_acc=np.mean(neigh.predict(train.iloc[:,0:9])==train.iloc[:,9])\n\ntest_acc=np.mean(neigh.predict(test.iloc[:,0:9])==test.iloc[:,9])\n\nacc=[]\nfor i in range(1,100,2):\n          neigh=KNC(n_neighbors=i)\n          neigh.fit(train.iloc[:,0:9],train.iloc[:,9])\n          train_acc=np.mean(neigh.predict(train.iloc[:,0:9])==train.iloc[:,9])\n          test_acc=np.mean(neigh.predict(test.iloc[:,0:9])==test.iloc[:,9])\n          acc.append([train_acc,test_acc])\n          \n          \n          \n          \n          \nimport matplotlib.pyplot as plt\nplt.plot(np.arange(1,100,2),[i[0] for i in acc],\"-ro\")\nplt.plot(np.arange(1,100,2),[i[1] for i in acc],\"-bo\")\nplt.legend([\"train\",\"test\"])\n","a2bb40d7":"**K Nearest Neighbors**\nThe k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems.\nThe KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.\n**Algorithm:**\nThe Distance Between two points can be calculated using Euclidean distance\n-Calculate the distance from x to all points in your data.\n-Sort the points in your data by increasing distance from x.\n-Predict the majority label of the k closest points.\nNote: k value defines accuracy of a model so try with all possible values of k","14a04c17":"Glass dataset consist of 10 variable"}}