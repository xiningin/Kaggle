{"cell_type":{"90888e0c":"code","2b0b35c7":"code","bd65065d":"code","a39ec102":"code","07531066":"code","2c93575c":"code","1a3dc6bf":"code","112167b9":"code","ce31be3d":"code","d136946b":"code","6fe32e58":"code","006f1f23":"code","a0ed78ce":"code","7c8d6e34":"code","42de5cc0":"code","2bd588f4":"code","9aefea01":"code","344a7aa8":"code","52802563":"code","e11fb537":"code","0a2dba4b":"code","118cb559":"code","536688ed":"code","12442eb6":"code","e5511180":"code","d8ad9782":"code","525cff2f":"code","d479495d":"code","7cd58ba6":"code","54e08cd6":"code","fb0d1246":"code","9d9aeb14":"markdown","69933c04":"markdown","716ce5c6":"markdown","16a6d3f6":"markdown","2380e7e3":"markdown","6d126775":"markdown","f0ae0435":"markdown","b4e18f8b":"markdown","3704e7cd":"markdown","8aeaf562":"markdown","953e1247":"markdown","a5797d8d":"markdown","7988c89d":"markdown","d4cf65e3":"markdown","2beed9e5":"markdown","bb20829d":"markdown","871f3421":"markdown"},"source":{"90888e0c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2b0b35c7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nfrom sklearn.model_selection import cross_val_score\n\nfrom tpot import TPOTRegressor","bd65065d":"train_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","a39ec102":"train_data","07531066":"train_data.shape","2c93575c":"test_data","1a3dc6bf":"train_data.info()","112167b9":"train_data.info()","ce31be3d":"all_data = pd.concat([train_data.drop('SalePrice',axis=1),test_data],axis=0)","d136946b":"all_data.info()","6fe32e58":"all_data.describe()","006f1f23":"all_data = all_data.drop(['Alley','PoolQC','Fence','MiscFeature','FireplaceQu','Id'],axis=1)","a0ed78ce":"numeric_columns = all_data.select_dtypes(include=['int64','float64']).columns\nnumeric_columns","7c8d6e34":"cat_columns = all_data.select_dtypes(include=['object']).columns\ncat_columns","42de5cc0":"def fill_numerical(column):\n    all_data[column] = all_data[column].fillna(all_data[column].mean())\n    \ndef fill_categorical(column):\n    all_data[column] = all_data[column].fillna(all_data[column].mode()[0])","2bd588f4":"for col in numeric_columns:\n    fill_numerical(col)\nfor col in cat_columns:\n    fill_categorical(col)","9aefea01":"all_data.info()","344a7aa8":"corr = all_data.corr()\n\nfig = px.imshow(corr)\nfig.show()","52802563":"for col in cat_columns:\n    all_data[col] = all_data[col].astype('category').cat.codes","e11fb537":"X_train = all_data.iloc[:1460,:]\nX_test = all_data.iloc[1460:,:]\ny_train = train_data.loc[:,'SalePrice']","0a2dba4b":"rfr = RandomForestRegressor()\n\nrfr.fit(X_train,y_train)\n\nrfr.feature_importances_","118cb559":"for i in range(len(rfr.feature_importances_)):\n    print(\"{}'s importance : {}\".format(X_train.columns[i],rfr.feature_importances_[i]))","536688ed":"fig = px.bar(pd.DataFrame(np.c_[X_train.columns,rfr.feature_importances_],\n                          columns=['name','importance']), x='name', y='importance')\nfig.show()","12442eb6":"gbr = GradientBoostingRegressor()\nabr = AdaBoostRegressor()\nrfr = RandomForestRegressor()\netr = ExtraTreesRegressor()","e5511180":"print(\"GradientBoostingRegressor's scores : {}\".format(cross_val_score(gbr,X_train,y_train,cv=5).mean()))\nprint(\"AdaBoostRegressor's scores : {}\".format(cross_val_score(abr,X_train,y_train,cv=5).mean()))\nprint(\"RandomForestRegressor's scores : {}\".format(cross_val_score(rfr,X_train,y_train,cv=5).mean()))\nprint(\"ExtraTreesRegressor's scores : {}\".format(cross_val_score(etr,X_train,y_train,cv=5).mean()))","d8ad9782":"gbr.fit(X_train,y_train)\nabr.fit(X_train,y_train)\nrfr.fit(X_train,y_train)\netr.fit(X_train,y_train)","525cff2f":"y_pred_ensemble = .35 * gbr.predict(X_test) + .15 * abr.predict(X_test) + .25 * rfr.predict(X_test) + .25 * etr.predict(X_test)","d479495d":"tpot = TPOTRegressor(generations=5,population_size=50)\ntpot.fit(X_train,y_train)","7cd58ba6":"y_pred_automl = tpot.predict(X_test)","54e08cd6":"final_y_pred = y_pred_automl * .6 + y_pred_ensemble * .4","fb0d1246":"pd.DataFrame({'Id': range(1461,2920),'SalePrice': final_y_pred}).to_csv('submission.csv')","9d9aeb14":"## saving results to a csv file","69933c04":"## combining ensembling and automl models predictions","716ce5c6":"## fitting an automl model \nI'm going to use tpot library which is a automl library","16a6d3f6":"## labelencoding categorical columns","2380e7e3":"we have 1460 examples and 80 feature and 1 target","6d126775":"as you can see know all missing value are filled with mean or mode of that columns","f0ae0435":"## ensembling ","b4e18f8b":"## import data","3704e7cd":"next i want to seperate data to numerical and categorical features and deal with missing values","8aeaf562":"## removing columns that have high number of missing values","953e1247":"## import libraries","a5797d8d":"train and test data both have missing values. we going to solve that later","7988c89d":"## filling the missing values","d4cf65e3":"as you can see GradientBoostingRegressor has better performance among these estimators","2beed9e5":"## correlation matrix","bb20829d":"It was my first notebook so there my be some defects and if you find any defects let me know in the comments and if you liked it please upvote","871f3421":"## splitting data to train and test"}}