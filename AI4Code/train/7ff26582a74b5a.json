{"cell_type":{"c99b8b1b":"code","92af3f6e":"code","b5417a4b":"code","80b3ce89":"code","2cbc1c8c":"code","57c7196d":"code","ff35181d":"code","bc582d10":"code","3294bedf":"code","defb738e":"markdown","2fc4eca6":"markdown","c2ac9860":"markdown","d42aa562":"markdown","de99b813":"markdown","8064de66":"markdown","f773f2ec":"markdown","c9297f58":"markdown"},"source":{"c99b8b1b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport optuna\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom optuna.samplers import TPESampler\nfrom sklearn.model_selection import KFold\noptuna.logging.set_verbosity(optuna.logging.WARNING)","92af3f6e":"train = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\nprint('train shape:',train.shape)","b5417a4b":"# Train data\nX_train = train.drop(columns = ['loss','id'])\ny_train = train['loss'].values","80b3ce89":"def getXgbHyperparameters(trial):\n    xgb_param = {\n            \"tree_method\": \"gpu_hist\",\n            'n_estimators': trial.suggest_int('n_estimators', 500, 2000, 100),\n            \"booster\": 'gbtree',\n            \"reg_lambda\": trial.suggest_int(\"reg_lambda\", 1, 100),\n            \"reg_alpha\": trial.suggest_int(\"reg_alpha\", 1, 100),\n            \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0, step=0.1),\n            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0, step=0.1),\n            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 9),\n            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 2, 10),\n            \"learning_rate\": 0.01,\n            \"gamma\": trial.suggest_float(\"gamma\", 0, 20)\n        }\n    return xgb_param","2cbc1c8c":"def objective(trial, X, y):\n    \n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.3, random_state=1337)\n    \n    xgb_param = getXgbHyperparameters(trial)\n\n    eval_set = [(X_valid, y_valid)]\n    \n    fit_params = dict(eval_set=eval_set, \n                      eval_metric='rmse', \n                      early_stopping_rounds=100, \n                      verbose=False)\n    \n    xgb_regressor = XGBRegressor(**xgb_param)\n\n    # Fit\/predict\n    xgb_regressor = xgb_regressor.fit(X_train, y_train)\n    preds = xgb_regressor.predict(X_valid)\n    \n    # Compute rmse\n    rmse = np.sqrt(mean_squared_error(y_valid, preds))\n    \n    return rmse","57c7196d":"# Callback function to print log messages when the best trial is updated\n\ndef logging_callback(study, frozen_trial):\n    previous_best_value = study.user_attrs.get(\"previous_best_value\", None)\n    if previous_best_value != study.best_value:\n        study.set_user_attr(\"previous_best_value\", study.best_value)\n        print(\n            \"Trial {} finished with best value: {}. \".format(\n            frozen_trial.number,\n            frozen_trial.value\n            )\n        )","ff35181d":"%%time\n\nstudy = optuna.create_study(sampler=TPESampler(seed=1337), direction='minimize', study_name='xgb')\nfunc = lambda trial: objective(trial, X_train, y_train)\n\nstudy.optimize(func, timeout=60*180, callbacks=[logging_callback]) # timeout = seconds * minutes. Longer timeout will tend to lead to better hyperparameter tuning.","bc582d10":"print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\nprint(f\"\\tBest params:\")\nfor key, value in study.best_params.items():\n    print(f\"\\t\\t{key}: {value}\")","3294bedf":"import joblib\n\njoblib.dump(study, \"xgb_study.pkl\")","defb738e":"# Setup XGB hyperparameters for experiment","2fc4eca6":"# Custom logging callback function","c2ac9860":"# Initiate experiment to find best hyperparameters","d42aa562":"# Load dataset","de99b813":"# Save the study containing the best hyperparameters for the XBG model","8064de66":"This is a follow up notebook of [XGBoost 101 - Baseline](https:\/\/www.kaggle.com\/aayush26\/tps-aug-2021-xgboost-101-baseline).\n\n\nThe scope of this notebook is to perform hyperparameter tuning and store the best hyperparameters found for XGB model. \n\nThe stored file can later be loaded into another notebook by using `Kaggle Add data` or directly copy-paste the best params displayed in the output cell.\n\nCheck [[TPS Aug 2021] XGBoost 201 - with Optuna Part 2\/2](https:\/\/www.kaggle.com\/aayush26\/tps-aug-2021-xgboost-201-with-optuna-part-2-2\/notebook), where I am loading the best hyperparameters acquired from this notebook.\n\n### Pre-requisite\n1. [GPU version] Change the accelerator to GPU in order to be able to exexute this notebook.\n2. [CPU version] Delete the following hyper params present in getXgbHyperparametersgetXgbHyperparameters(): `\"tree_method\": \"gpu_hist\"` and `\"booster\": 'gbtree'`","f773f2ec":"# Define objective function","c9297f58":"# Imports"}}