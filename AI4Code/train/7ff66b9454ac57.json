{"cell_type":{"3d7bb275":"code","067e7f2c":"code","a57ce6c1":"code","2e5fed01":"code","44f5fa1c":"code","f3ea1666":"code","09dfeef4":"code","9c7b5f70":"code","4ca33e92":"code","9063793b":"code","95cd2faf":"code","bc5639db":"code","4d8442e9":"code","d998fdbc":"code","f8e3524f":"code","f9facecf":"code","a3378fcb":"code","2d868d97":"code","5d786666":"code","daaeb81b":"code","e83ae2de":"code","fd51cc02":"code","3a83932a":"code","6390678b":"code","9c822bb8":"code","75bf37c2":"code","b5e79a6f":"code","3005b972":"code","9cb87284":"code","fa80476b":"code","57247259":"code","f5a75893":"code","6af85e5c":"code","40032055":"code","c73cc604":"code","3b2c09c8":"code","a6b213be":"code","b83f1391":"code","aabdec24":"code","2d970b55":"code","3c0430d9":"code","7135ced9":"code","9019eca7":"code","3d573370":"code","e1381c96":"code","bdfdec6a":"code","8aa57a3d":"code","f587af5b":"code","4ffe9c0f":"code","9a9d952f":"code","686799fb":"code","bcf89c26":"code","347056f4":"code","8069871d":"code","4c4a5cdf":"code","e9e10c9e":"markdown","f2464d7b":"markdown","7fb97e1f":"markdown","ed42f458":"markdown","77fc019a":"markdown","92ccc5f3":"markdown","512bb4bb":"markdown","4e35ca8c":"markdown","7c6ddcfc":"markdown","79adbfd1":"markdown","06a3895d":"markdown","519a720e":"markdown","4824d334":"markdown","3e108865":"markdown","b6be4d66":"markdown","6901ee5a":"markdown","98c922a2":"markdown","f62238fe":"markdown"},"source":{"3d7bb275":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\ndf_train_label=pd.read_csv('..\/input\/g2net-gravitational-wave-detection\/training_labels.csv')\ndf_train_label.head()","067e7f2c":"df_train_label.shape","a57ce6c1":"import os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom glob import glob","2e5fed01":"# path of the files\npaths_files = glob(\"..\/input\/g2net-gravitational-wave-detection\/train\/*\/*\/*\/*\")\n#paths_files","44f5fa1c":"len(paths_files)","f3ea1666":"# Loading the first .npy data\ndata=np.load(paths_files[0])\ndata","09dfeef4":"data.shape","9c7b5f70":"print(np.min(data),np.max(data))","4ca33e92":"# Looking is there is any missing value\ndf_train_label.isnull().sum()","9063793b":"df_train_label['target'].hist()","95cd2faf":"df_train_label['target'].value_counts()","bc5639db":"ids=[]\nfor filext in paths_files:\n    ids.append(filext[filext.rindex('\/')+1:\\\n                              len(filext)].replace('.npy',''))\n    \n# data frame containing paths and ids of .npy files \npath_df = pd.DataFrame({\"id\":ids,\"path\":paths_files})\npath_df.head()","4d8442e9":"path_df.shape","d998fdbc":"df=pd.merge(path_df,df_train_label,on='id')\ndel path_df, df_train_label;\ndf.head()","f8e3524f":"df.shape","f9facecf":"df[df['target']==1]['path'][0]","a3378fcb":"for i in range(0,data.shape[0]):\n    plt.plot(np.arange(0, data.shape[1], 1),data[i,:])\n    plt.show()","2d868d97":"data1=np.load(df[df['target']==1]['path'].iloc[0])\ndata1","5d786666":"for i in range(0,data1.shape[0]): \n    plt.figure(figsize=(14,2))\n    plt.plot(np.arange(0, data1.shape[1], 1),data1[i,:])\n    # naming the x axis\n    plt.xlabel('sample')\n    # naming the y axis\n    plt.ylabel('output')\n    # naming the title\n    plt.title('SITE'+str(i+1)+'(target=1)')\n    plt.xlim(0,4096)\n    plt.show()","daaeb81b":"data0=np.load(df[df['target']==0]['path'].iloc[0])\ndata0","e83ae2de":"for i in range(0,data0.shape[0]): \n    plt.figure(figsize=(14,2))\n    plt.plot(np.arange(0, data0.shape[1], 1),data0[i,:])\n    # naming the x axis\n    plt.xlabel('sample')\n    # naming the y axis\n    plt.ylabel('output')\n    # naming the title\n    plt.title('SITE'+str(i+1)+'(target=0)')\n    plt.xlim(0,4096)\n    plt.show()","fd51cc02":"df[df['target']==0]['path'].iloc[0]","3a83932a":"sns.displot(data1[0,:])","6390678b":"fig, axes = plt.subplots(3, 2, sharex=True, figsize=(14,12))\nfig.suptitle('Distribution plots')\nfor i in range(0,data1.shape[0]):\n    sns.histplot(ax=axes[i, 0], data=data1[i,:])\n    axes[i,0].set_title('SITE'+str(i+1)+'(target=1)')\n    sns.histplot(ax=axes[i, 1], data=data0[i,:])\n    axes[i,1].set_title('SITE'+str(i+1)+'(target=0)')","9c822bb8":"from sklearn.model_selection import train_test_split\ndf_train, df_val= train_test_split(df, test_size=0.2, random_state=0)","75bf37c2":"df_train.head()","b5e79a6f":"df_val.head()","3005b972":"import tensorflow as tf\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPool1D, BatchNormalization\nfrom tensorflow.keras.optimizers import RMSprop,Adam\n","9cb87284":"def npy_header_offset(npy_path):\n    with open(str(npy_path), 'rb') as f:\n        if f.read(6) != b'\\x93NUMPY':\n            raise ValueError('Invalid NPY file.')\n        version_major, version_minor = f.read(2)\n        if version_major == 1:\n            header_len_size = 2\n        elif version_major == 2:\n            header_len_size = 4\n        else:\n            raise ValueError('Unknown NPY file version {}.{}.'.format(version_major, version_minor))\n        header_len = sum(b << (8 * i) for i, b in enumerate(f.read(header_len_size)))\n        header = f.read(header_len)\n        if not header.endswith(b'\\n'):\n            raise ValueError('Invalid NPY file.')\n        return f.tell()","fa80476b":"header_size = npy_header_offset(df['path'].iloc[0])\nheader_size","57247259":"file_length = os.path.getsize(df['path'].iloc[0])\nfile_length","f5a75893":"file_length-header_size","6af85e5c":"3*4096*tf.float64.size","40032055":"tf_data_train=tf.data.FixedLengthRecordDataset( df_train['path'], 3*4096*tf.float64.size,\\\n                                         header_bytes=header_size, num_parallel_reads=4)\ntf_data_val=tf.data.FixedLengthRecordDataset( df_val['path'], 3*4096*tf.float64.size,\\\n                                         header_bytes=header_size, num_parallel_reads=4)","c73cc604":"tf_data_train","3b2c09c8":"tf_data_train = tf_data_train.map(lambda s: tf.reshape(\\\n                                                       tf.io.decode_raw(s, tf.float64),\\\n                                                       (3,4096)))\ntf_data_val = tf_data_val.map(lambda s: tf.reshape(\\\n                                                       tf.io.decode_raw(s, tf.float64),\\\n                                                       (3,4096)))\ntf_data_train","a6b213be":"for i in tf_data_train.take(3):\n    print(i)","b83f1391":"for i in tf_data_val.take(3):\n    print(i)","aabdec24":"tf_data_train= tf.data.Dataset.zip((tf_data_train,\\\n                             tf.data.Dataset.from_tensor_slices(df_train['target']))) ","2d970b55":"i=0\nfor data, target in tf_data_train.take(3):\n    print(\"tf_data_train\")\n    print(data.numpy(),target.numpy())\n    print(\"df_train\")\n    print(np.load(df_train['path'].iloc[i]),df_train['target'].iloc[i])\n    i=i+1","3c0430d9":"tf_data_val= tf.data.Dataset.zip((tf_data_val,\\\n                             tf.data.Dataset.from_tensor_slices(df_val['target']))) \nfor data, target in tf_data_val.take(3):\n    print(data.numpy(),target.numpy())","7135ced9":"train_data = tf_data_train.batch(32).prefetch(buffer_size=64)\ntrain_data","9019eca7":"val_data = tf_data_val.batch(32).prefetch(buffer_size=64)\ntrain_data","3d573370":" \nmodel = Sequential()\nmodel.add(Conv1D(64, input_shape=(3, 4096,), kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))","e1381c96":"model.compile(optimizer = Adam(lr=2e-4),loss='binary_crossentropy',metrics=['AUC'])","bdfdec6a":"model.summary()","8aa57a3d":"history = model.fit(train_data, validation_data=val_data, epochs = 2)","f587af5b":"# path of the files\ntest_files = glob(\"..\/input\/g2net-gravitational-wave-detection\/test\/*\/*\/*\/*\")\n#paths_files","4ffe9c0f":"ids=[]\nfor filext in test_files:\n    ids.append(filext[filext.rindex('\/')+1:\\\n                              len(filext)].replace('.npy',''))\n    \n# data frame containing paths and ids of .npy files \ntest_df = pd.DataFrame({\"id\":ids,\"path\":test_files})\ntest_df.head()","9a9d952f":"tf_data_test=tf.data.FixedLengthRecordDataset( test_df['path'], 3*4096*tf.float64.size,\\\n                                         header_bytes=header_size, num_parallel_reads=4)","686799fb":"tf_data_test = tf_data_test.map(lambda s: tf.reshape(\\\n                                                       tf.io.decode_raw(s, tf.float64),\\\n                                                       (3,4096)))","bcf89c26":"test_data = tf_data_test.batch(32).prefetch(buffer_size=64)\ny_pred=model.predict(test_data)","347056f4":"y_pred.flatten()","8069871d":"output = pd.DataFrame({'Id': test_df.id, 'target': y_pred.flatten()})\noutput.head()","4c4a5cdf":"output.to_csv('.\/testing_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","e9e10c9e":"#### It appears that target=1 at SITE1 has higher spread, and target=0 has higher spread at SITE3","f2464d7b":"### Importing libraries","7fb97e1f":"#### Importing tha path to the files","ed42f458":"#### It is actually possible to read directly NPY files with TensorFlow instead of TFRecords. The key pieces are [**tf.data.FixedLengthRecordDataset**](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/FixedLengthRecordDataset) and [**tf.io.decode_raw**](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/io\/decode_raw), along with a look at the documentation of the [**.npy**](https:\/\/numpy.org\/devdocs\/reference\/generated\/numpy.lib.format.html) format. For simplicity, let's suppose that a **float32** **.npy** file containing an array with shape (N, K) is given, and you know the number of features K beforehand, as well as the fact that it is a *float32* array. An **.npy** file is just a binary file with a small header and followed by the raw array data (object arrays are different, but we're considering numbers now). In short, you can find the size of this header with a function like this:","77fc019a":"#### Now in inorder to build custom data generator, do refer to this article [ref.(1)](https:\/\/towardsdatascience.com\/keras-data-generators-and-how-to-use-them-b69129ed779c) it is extremely useful.","92ccc5f3":"#### From the above observation we can conclude the following,\n1. The sampling rate is 2048 Hz, which means that for each second 2048 samples are given. This fact is already given in the dataset description.\n2. Three rows in **data** variable refer to the 3 sites mentioned in the description of data, and they are: LIGO Hanford (SITE1), LIGO Livingston (SITE2), Virgo (SITE3).\n3. In the **data** variable there are $4096=2086\\times 2$ columns. It refers to the total samples generated in the span of 2 seconds.\n","512bb4bb":"### We know do a join both of the dataframes into a resulting dataframe **df**","4e35ca8c":"There are about 560000 **.npy** files in the dataframe. Now we look into a particular .npy file as shown below.","7c6ddcfc":"## Going to perform data preparation and EDA","79adbfd1":"#### First we look into the case when target=1","06a3895d":"#### I am loving it.\n\n### Now going to zip the target column with the tensorflow dataset","519a720e":"#### Creating Training and validation set","4824d334":"### Importing training labels file","3e108865":"#### Visualizing a particular .npy file where target=0 and target=1. ","b6be4d66":"Almost balanced data.\n\n#### Motivated by the compact dataset representation in the kaggle notebook given [here](https:\/\/www.kaggle.com\/rawaaelghali\/g2net-gravitational-starter-eda) we also build similar compact dataframe as shown below.","6901ee5a":"Documentation regarding tf.data.FixedLengthRecordDataset is given [here](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/FixedLengthRecordDataset)","98c922a2":"We now look into target=0","f62238fe":"Directly training our model on the **.npy** files takes a lot of time because loading the data takes a lot of time than performing the ML computations. Mainly the ML computation are done on a GPU, and loading the data task is done by CPU. Former data pipelines made the GPU wait for the CPU to load the data, leading to performance issues [ref.(2)](https:\/\/cs230.stanford.edu\/blog\/datapipeline\/). Therefore, the **tf.data** API enables you to build complex input pipelines from simple, reusable pieces [ref.(3)](https:\/\/www.tensorflow.org\/guide\/data). But this (**tf.data**) API lack the feature of reading **.npy** files which doesn't fit in the memory. However, I found a solution in the stackover flow, and link of the solution is given [here](https:\/\/stackoverflow.com\/questions\/48889482\/feeding-npy-numpy-files-into-tensorflow-data-pipeline)."}}