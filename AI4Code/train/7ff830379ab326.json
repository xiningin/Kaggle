{"cell_type":{"80cd889c":"code","b6159e9f":"code","8f13cecf":"code","dc7ad354":"code","32a9617e":"code","e1daa6e2":"code","3bd00d5e":"code","7a7ef9c1":"code","ab3d7999":"code","ba539a57":"code","de75d334":"code","c7847c8d":"code","d937e143":"code","df6b73b2":"code","03b8fb0e":"code","a7671287":"markdown","c5dbcc37":"markdown","70db0b8f":"markdown","db90ca51":"markdown","7b74f04d":"markdown","0bab88ec":"markdown","5b989b37":"markdown","2fc47b26":"markdown","9b107df3":"markdown","b74d8f2b":"markdown","adc23842":"markdown","54f40e42":"markdown","1e817fd0":"markdown"},"source":{"80cd889c":"import io\nimport openpyxl\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","b6159e9f":"movies_ds = pd.read_csv('..\/input\/movies\/movies.csv', encoding='latin1', sep=\",\")\n\nmovies_ds","8f13cecf":"#Checking a dataset sample\n\npd.set_option(\"display.max_rows\", 100)\npd.set_option(\"display.max_columns\", 100)\npd.options.display.float_format=\"{:,.2f}\".format\nmovies_ds.sample(n=10, random_state=0)","dc7ad354":"#Checking dataset info by feature\n\nmovies_ds.info(verbose=True, null_counts=True)","32a9617e":"#Checking the existence of zeros in rows\n\n(movies_ds==0).sum(axis=0).to_excel(\"zeros_per_feature.xlsx\")\n(movies_ds==0).sum(axis=0)","e1daa6e2":"#Checking the existence of duplicated rows\n\nmovies_ds.duplicated().sum()","3bd00d5e":"#Checking basic statistical data by feature\n\nmovies_ds.describe(include=\"all\")","7a7ef9c1":"#1\n\nmovies_ds[\"budget\"].replace(0, np.nan, inplace=True)\nmovies_ds[\"budget\"].fillna(movies_ds[\"budget\"].sum() \/ movies_ds[\"gross\"].sum() * movies_ds[\"gross\"], inplace=True)\n\n#2\n\nmovies_ds = movies_ds[~movies_ds[\"rating\"].isin([\"NOT RATED\", \"UNRATED\", \"Not specified\"])]\n\n#3\n\nmovies_ds[\"gross_to_budget_ratio\"] = movies_ds[\"gross\"] \/ movies_ds[\"budget\"] #feature engineering\n\n#4\n\nmovies_ds = movies_ds[[\"budget\", \"country\", \"genre\", \"gross\", \"rating\", \"runtime\", \"score\", \"year\", \"gross_to_budget_ratio\"]] #keeping only the most relevant features\n\n#5\n\n# movies_ds = pd.concat([movies_ds, pd.get_dummies(movies_ds[\"country\"])], axis=1) #country dummy coding (we\u00b4re skipping this line since it woulc generate a 87 columns dataset and we don\u00b4t want to make complex the problem explanation to the business in this example)\nmovies_ds = pd.concat([movies_ds, pd.get_dummies(movies_ds[\"genre\"])], axis=1) #genre dummy coding\nmovies_ds = pd.concat([movies_ds, pd.get_dummies(movies_ds[\"rating\"])], axis=1) #rating dummy coding\n\nmovies_ds.to_excel(\"movies_ds_clean.xlsx\")","ab3d7999":"#Plotting Categorical Variables\n\nfig, ax = plt.subplots(1, 2)\nmovies_ds[\"country\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nmovies_ds[\"country\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Country Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\nfig, ax = plt.subplots(1, 2)\nmovies_ds[\"genre\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nmovies_ds[\"genre\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Genre Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\nfig, ax = plt.subplots(1, 2)\nmovies_ds[\"rating\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nmovies_ds[\"rating\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Rating Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\n\n#Plotting Numerical Variables\n\nfig, ax = plt.subplots(1, 3)\nfig.suptitle(\"Budget Distribution\", fontsize=15)\nsns.distplot(movies_ds[\"budget\"], ax=ax[0])\nsns.boxplot(movies_ds[\"budget\"], ax=ax[1])\nsns.violinplot(movies_ds[\"budget\"], ax=ax[2])\n\nfig, ax = plt.subplots(1, 3)\nfig.suptitle(\"Gross Revenue Distribution\", fontsize=15)\nsns.distplot(movies_ds[\"gross\"], ax=ax[0])\nsns.boxplot(movies_ds[\"gross\"], ax=ax[1])\nsns.violinplot(movies_ds[\"gross\"], ax=ax[2])\n\nfig, ax = plt.subplots(1, 3)\nfig.suptitle(\"Runtime Distribution\", fontsize=15)\nsns.distplot(movies_ds[\"runtime\"], ax=ax[0])\nsns.boxplot(movies_ds[\"runtime\"], ax=ax[1])\nsns.violinplot(movies_ds[\"runtime\"], ax=ax[2])\n\nfig, ax = plt.subplots(1, 3)\nfig.suptitle(\"Score Distribution\", fontsize=15)\nsns.distplot(movies_ds[\"score\"], ax=ax[0])\nsns.boxplot(movies_ds[\"score\"], ax=ax[1])\nsns.violinplot(movies_ds[\"score\"], ax=ax[2])\n\nfig, ax = plt.subplots(1, 3)\nfig.suptitle(\"Year Distribution\", fontsize=15)\nsns.distplot(movies_ds[\"year\"], ax=ax[0])\nsns.boxplot(movies_ds[\"year\"], ax=ax[1])\nsns.violinplot(movies_ds[\"year\"], ax=ax[2])\n\nfig, ax = plt.subplots(1, 3)\nfig.suptitle(\"Gross to Budget Distribution\", fontsize=15)\nsns.distplot(movies_ds[\"gross_to_budget_ratio\"], ax=ax[0])\nsns.boxplot(movies_ds[\"gross_to_budget_ratio\"], ax=ax[1])\nsns.violinplot(movies_ds[\"gross_to_budget_ratio\"], ax=ax[2])","ba539a57":"#Deleting original categorical columns\n\nmovies_ds2 = movies_ds.drop([\"country\", \"genre\", \"rating\"], axis=1)\n\n# #Plotting a Heatmap\n\n# fig, ax = plt.subplots(1, figsize=(25,25))\n# sns.heatmap(movies_ds2.corr(), annot=True, fmt=\",.2f\")\n# plt.title(\"Heatmap Correlation\", fontsize=20)\n# plt.tick_params(labelsize=12)\n# plt.xticks(rotation=90)\n# plt.yticks(rotation=45)\n\n# #Plotting a Pairplot\n\n# sns.pairplot(movies_ds2)","de75d334":"#Defining Xs\n\nX_orig = movies_ds\nX = movies_ds2\n\n#Scaling all features\n\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_scaled = sc_X.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled)","c7847c8d":"#Applying PCA\n\nfrom sklearn.decomposition import PCA\n\n#Creating a model\npca = PCA(n_components=X_scaled.shape[1], random_state=0) #there are 18 features at the dataset\n\n#Fitting to the model\npca.fit(X_scaled)\n\n#Generating all components in an array\nX_pca = pca.transform(X_scaled)\n# X_pca_output = pd.DataFrame(X_pca)\n# X_pca_output.to_excel(\"X_pca_file.xlsx\",index=False)\n\n#Displaying the explained variance by number of components\nfor n in range(0, X_scaled.shape[1]):\n    print(f\"Variance explained by the first {n+1} principal components = {np.cumsum(pca.explained_variance_ratio_ *100)[n]:.1f}%\")\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel(\"Number of components\")\nplt.ylabel(\"Explained variance\")\n\n#Creating a model with the chosen number of components (#75% explainability = 20 components)\npca_selected = PCA(n_components=20, random_state=0)\npca_selected.fit(X_scaled)\nX_pca_selected = pca_selected.transform(X_scaled)\n# X_pca_selected_output = pd.DataFrame(X_pca_selected)\n# X_pca_selected_output.to_excel(\"X_pca_selected_file.xlsx\",index=False)","d937e143":"#Creating a K-means model and checking its Metrics\n\nfrom sklearn.cluster import KMeans\n\n#Applying the Elbow Method to calculate distortion for a range of number of cluster\n\ndistortions = []\nfor i in range(1, 21):\n    km = KMeans(n_clusters=i, init=\"random\", n_init=10, max_iter=300, tol=1e-04, random_state=0)\n    km.fit(X_pca_selected)\n    distortions.append(km.inertia_)\n\n#Plotting\n\nplt.plot(range(1, 21), distortions, marker=\"o\")\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"Distortion\")\nplt.show()\n\n#Applying the Silhouette Method to interpret and validate of consistency within clusters of data\n\nfrom sklearn.metrics import silhouette_score\nsilhouette_coefficients = []\nfor j in range(2, 21):\n    km = KMeans(n_clusters=j, init=\"random\", n_init=10, max_iter=300, tol=1e-04, random_state=0)\n    km.fit(X_pca_selected)\n    score = silhouette_score(X_pca_selected, km.labels_)\n    silhouette_coefficients.append(score)\n\n#Plotting\n\nplt.style.use(\"fivethirtyeight\")\nplt.plot(range(2, 21), silhouette_coefficients)\nplt.xticks(range(2, 21))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Silhouette Coefficient\")\nplt.show()\n\n#Choosing number of clusters\n\nn_clusters = 17\nprint('Estimated number of clusters: %d' % n_clusters)\nkm = KMeans(n_clusters=n_clusters)\nkm.fit(X_pca_selected)\nprint(\"Silhouette Coefficient: %0.3f\" % silhouette_score(X_pca_selected, km.fit(X_pca_selected).labels_))\n\n#Plotting chosen number of clusters\n\nfrom yellowbrick.cluster import silhouette_visualizer\nsilhouette_visualizer(KMeans(n_clusters=n_clusters, random_state=0), X_pca_selected)\n\n#Visualizing clusters in the dataset\nX_orig = pd.DataFrame(X_orig)\nX_orig[\"cluster\"] = km.labels_\nX_orig.to_excel(\"model_km.xlsx\")","df6b73b2":"# #Plotting scatter graph per pair features\n\n# #Mapping every individual cluster to a color\n\n# colors = ['royalblue', 'mediumorchid', 'tan', 'deeppink', 'olive', 'goldenrod', 'lightcyan', 'navy']\n\n# vectorizer = np.vectorize(lambda x: colors[x % len(colors)])\n\n# #Plotting\n\n# for i in range(1, X_pca_selected.shape[1]-1):\n#     plt.scatter(X_pca_selected.iloc[:,0], X_pca_selected.iloc[:,i], c=vectorizer(clusters))\n#     plt.xlabel(X_pca_selected.columns[0])\n#     plt.ylabel(X_pca_selected.columns[i])\n#     plt.show()","03b8fb0e":"#Creating a DBSCAN model and checking its Metrics\n#OBS: we\u00b4re exploring DBSCAN only as a study exercise in this project - we\u00b4ll adopt K-Means\n\nfrom sklearn.neighbors import NearestNeighbors\n\n#We can calculate the distance from each point to its closest neighbour using the NearestNeighbors. The point itself is included in n_neighbors. The kneighbors method returns two arrays, one which contains the distance to the closest n_neighbors points and the other which contains the index for each of those points\n\nneigh = NearestNeighbors(n_neighbors=2)\nnbrs = neigh.fit(X_pca_selected)\ndistances, indices = nbrs.kneighbors(X_pca_selected)\n\n#Soring and plotting results\n\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.plot(distances)\nplt.xlabel(\"Distances to the closest n_neighbors\")\nplt.ylabel(\"eps\")\nplt.show()\n\nfrom sklearn.cluster import DBSCAN\n\n#Selecting the best eps (the optimal value for epsilon will be found at the point of maximum curvature)\n\ndbs = DBSCAN(eps=10)\ndbs.fit(X_pca_selected)\n\n#The labels_ property contains the list of clusters and their respective points\n\nclusters = dbs.labels_\n\nfrom sklearn import metrics\n\n#Number of clusters in labels, ignoring noise (outlier) (-1) if present\n\nn_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\nn_noise_ = list(clusters).count(-1)\nprint('Estimated number of clusters: %d' % n_clusters)\nprint('Estimated number of noise points: %d' % n_noise_)\nprint(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X_pca_selected, clusters))\n\n#Visualizing clusters in the dataset\nX_orig = pd.DataFrame(X_orig)\nX_orig[\"cluster\"] = dbs.labels_\nX_orig.to_excel(\"model_dbs.xlsx\")","a7671287":"# 2. Importing Basic Libraries","c5dbcc37":"# 10. Machine Learning Algorithms Implementation & Assessment","70db0b8f":"# 7. Correlations Analysis & Features Selection","db90ca51":"# 1. Introduction: Business Goal & Problem Definition\n\nIF YOU LIKE IT OR IF IT HELPS YOU SOMEHOW, COULD YOU PLEASE UPVOTE? THANK YOU VERY MUCH!!!\n\nThe goal of this project is to identify, study and analyze movies clustering, so the movie industry can have a better understanding of the customers segmentations according to their movies preferences and adapt different marketing strategies to each of them, bringing more revenue to the business. For that we\u00b4ll use the Movie Industry dataset available in Kaggle, containing 6820 movies (220 movies per year, 1986-2016). Each movie has the following attributes:\n\n* budget: the budget of a movie. Some movies don't have this, so it appears as 0\n* company: the production company\n* country: country of origin\n* director: the director\n* genre: main genre of the movie.\n* gross: revenue of the movie\n* name: name of the movie\n* rating: rating of the movie (R, PG, etc.)\n* released: release date (YYYY-MM-DD)\n* runtime: duration of the movie\n* score: IMDb user rating\n* votes: number of user votes\n* star: main actor\/actress\n* writer: writer of the movie\n* year: year of release","7b74f04d":"# 8. Data Modelling","0bab88ec":"# 9. Dimensionality Reduction","5b989b37":"# 11. Conclusions\n\nIF YOU LIKE IT OR IF IT HELPS YOU SOMEHOW, COULD YOU PLEASE UPVOTE? THANK YOU VERY MUCH!!!\n\nIn this exercise we went through all the process from collecting data, exploring features and distributions, treating data, understanding correlations, selecting relevant features, data modelling and presenting a clustering model, indicating groups of movies with similarities to be further developed and explored, so the movie industry can have a better understanding of the customers segmentations according to their movies preferences and adapt different marketing strategies to each of them, bringing more revenue to the business.","2fc47b26":"# 4. Data Preliminary Exploration","9b107df3":"# 3. Data Collection","b74d8f2b":"# 6. Data Exploration","adc23842":"# 5. Data Cleaning\n\n    We\u00b4ll perform the following:\n    \n    \n    1. I noticed budget is zero in 2182 observations, so we\u00b4ll treat it, making them proportional to \"gross\" since they are correlated\n\n\n    2. I noticed there are 309 ratings as NOT RATED, UNRATED or Not Specified; since it\u00b4s not a significant amount those rows will be deleted\n\n\n    3. Create a feature (gross_to_budget_ratio) to analyze the revenue to budget ratio relevance in the model\n\n\n    4. Keep only the most relevant features for our clustering purpose (budget, country, genre, gross, rating, runtime, score, year), so we make the model easier to interpret, we reduce the training time, avoid curse of dimensionality and reduce overfitting (OCCAM\u00b4S RAZOR)\n    \n    \n    5. Convert categorical variables (country, genre, rating) to dummies\n    \n    \n    * No duplications found\n    * No outliers found","54f40e42":"# 10.1 K-means","1e817fd0":"# 10.2 DBSCAN"}}