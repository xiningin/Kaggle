{"cell_type":{"a4730c0b":"code","6f82892b":"code","22fa2a84":"code","1fe5ae1c":"code","8983e03c":"code","e71a84e4":"code","267bd588":"code","4b1196bc":"code","8d6a80bf":"code","5ef32602":"code","cf2764ae":"code","94e761f9":"code","3b75a22f":"code","81de14c8":"code","f3b89d0f":"code","1e0a71df":"code","cd2c9c16":"code","95eaa30f":"code","768cc075":"code","f7b5b6f2":"code","d464be76":"code","70bc6cf3":"code","507ecb17":"code","99054d06":"code","58d69d4b":"code","5a09eeea":"code","8cdd3ae9":"code","714232a0":"code","c99bbbde":"code","af6f4241":"code","7e383f68":"code","e9361806":"code","ae89c7c7":"code","55d1ecea":"code","632980f0":"code","a200b1bf":"code","6812ee7b":"code","6458fe04":"code","8104b0f3":"code","3981cdf8":"code","3dffd5d9":"code","19f67d7d":"code","b7ab0a55":"code","3f594b9f":"code","b307bffa":"code","f652108b":"code","d1d3ef92":"code","4342106b":"code","eaa73d7e":"code","7526e864":"code","cf3abaed":"code","d72cff30":"code","e8f4d4a5":"code","96cc2945":"code","6afa00a9":"code","9c877641":"code","ce14a30d":"code","bde4f990":"code","b7b9f262":"code","076dd98a":"code","319ba72d":"code","3a5c13aa":"code","074b7695":"code","0227f64b":"code","c2180cf8":"code","8165db5f":"code","0db7b8c6":"markdown","16cac804":"markdown","4bc5f561":"markdown","af076078":"markdown","226de37c":"markdown","1b2048fe":"markdown","91c83802":"markdown","2e56b819":"markdown","4948f45b":"markdown","22b32bc0":"markdown","f379c531":"markdown","72e66dba":"markdown","caaffc88":"markdown","d16bb847":"markdown","3fd3d610":"markdown","ccba28c8":"markdown","44afa1ca":"markdown","ae49134c":"markdown","376a7e37":"markdown","515fcd65":"markdown","c7271850":"markdown","9f7c7788":"markdown","b2867756":"markdown","06078e4d":"markdown","471b6043":"markdown","ea84c780":"markdown","4e4e7f26":"markdown","3fd436b3":"markdown","7e7f10ad":"markdown","d3b8555b":"markdown","589f5a5b":"markdown","a96bd2a8":"markdown","a99c94b8":"markdown","9734adc4":"markdown","53d061db":"markdown"},"source":{"a4730c0b":"# 1. PACKAGES\n# -----------------------------------------------------------\n# Base\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Model\nfrom lightgbm import LGBMClassifier\n\n# Configuration\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\npd.options.display.float_format = '{:,.2f}'.format","6f82892b":"# Reduce Memory Usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\n\n# One-hot encoding for categorical columns with get_dummies\ndef one_hot_encoder(df, nan_as_category=True):\n    original_columns = list(df.columns)\n    categorical_columns = df.select_dtypes([\"category\", \"object\"]).columns.tolist()\n    # categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n    new_columns = [c for c in df.columns if c not in original_columns]\n    return df, new_columns\n\n# Grab Column Names\ndef grab_col_names(dataframe, cat_th=10, car_th=20, show_date=False):\n    date_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"datetime64[ns]\"]\n\n    #cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    cat_cols = dataframe.select_dtypes([\"object\", \"category\"]).columns.tolist()\n    \n    \n    \n    num_but_cat = [col for col in dataframe.select_dtypes([\"float\", \"integer\"]).columns if dataframe[col].nunique() < cat_th]\n\n    cat_but_car = [col for col in dataframe.select_dtypes([\"object\", \"category\"]).columns if dataframe[col].nunique() > car_th]\n\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    num_cols = dataframe.select_dtypes([\"float\", \"integer\"]).columns\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'date_cols: {len(date_cols)}')\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n\n    # cat_cols + num_cols + cat_but_car = de\u011fi\u015fken say\u0131s\u0131.\n    # num_but_cat cat_cols'un i\u00e7erisinde zaten.\n    # dolay\u0131s\u0131yla t\u00fcm \u015fu 3 liste ile t\u00fcm de\u011fi\u015fkenler se\u00e7ilmi\u015f olacakt\u0131r: cat_cols + num_cols + cat_but_car\n    # num_but_cat sadece raporlama i\u00e7in verilmi\u015ftir.\n\n    if show_date == True:\n        return date_cols, cat_cols, cat_but_car, num_cols, num_but_cat\n    else:\n        return cat_cols, cat_but_car, num_cols, num_but_cat\n\n# Categorical Variables & Target\ndef cat_analyzer(dataframe, variable, target = None):\n    print(variable)\n    if target == None:\n        print(pd.DataFrame({\n            \"COUNT\": dataframe[variable].value_counts(),\n            \"RATIO\": dataframe[variable].value_counts() \/ len(dataframe)}), end=\"\\n\\n\\n\")\n    else:\n        temp = dataframe[dataframe[target].isnull() == False]\n        print(pd.DataFrame({\n            \"COUNT\":dataframe[variable].value_counts(),\n            \"RATIO\":dataframe[variable].value_counts() \/ len(dataframe),\n            \"TARGET_COUNT\":dataframe.groupby(variable)[target].count(),\n            \"TARGET_MEAN\":temp.groupby(variable)[target].mean(),\n            \"TARGET_MEDIAN\":temp.groupby(variable)[target].median(),\n            \"TARGET_STD\":temp.groupby(variable)[target].std()}), end=\"\\n\\n\\n\")\n\n# Numerical Variables\ndef corr_plot(data, remove=[\"Id\"], corr_coef = \"pearson\", figsize=(20, 20)):\n    if len(remove) > 0:\n        num_cols2 = [x for x in data.columns if (x not in remove)]\n\n    sns.set(font_scale=1.1)\n    c = data[num_cols2].corr(method = corr_coef)\n    mask = np.triu(c.corr(method = corr_coef))\n    plt.figure(figsize=figsize)\n    sns.heatmap(c,\n                annot=True,\n                fmt='.1f',\n                cmap='coolwarm',\n                square=True,\n                mask=mask,\n                linewidths=1,\n                cbar=False)\n    plt.show()\n\n# Plot numerical variables\ndef num_plot(data, num_cols, remove=[\"Id\"], hist_bins=10, figsize=(20, 4)):\n\n    if len(remove) > 0:\n        num_cols2 = [x for x in num_cols if (x not in remove)]\n\n    for i in num_cols2:\n        fig, axes = plt.subplots(1, 3, figsize=figsize)\n        data.hist(str(i), bins=hist_bins, ax=axes[0])\n        data.boxplot(str(i), ax=axes[1], vert=False);\n        try:\n            sns.kdeplot(np.array(data[str(i)]))\n        except:\n            ValueError\n\n        axes[1].set_yticklabels([])\n        axes[1].set_yticks([])\n        axes[0].set_title(i + \" | Histogram\")\n        axes[1].set_title(i + \" | Boxplot\")\n        axes[2].set_title(i + \" | Density\")\n        plt.show()\n\n# Get high correlated variables\ndef high_correlation(data, remove=['SK_ID_CURR', 'SK_ID_BUREAU'], corr_coef=\"pearson\", corr_value = 0.7):\n    if len(remove) > 0:\n        cols = [x for x in data.columns if (x not in remove)]\n        c = data[cols].corr(method=corr_coef)\n    else:\n        c = data.corr(method=corr_coef)\n\n    for i in c.columns:\n        cr = c.loc[i].loc[(c.loc[i] >= corr_value) | (c.loc[i] <= -corr_value)].drop(i)\n        if len(cr) > 0:\n            print(i)\n            print(\"-------------------------------\")\n            print(cr.sort_values(ascending=False))\n            print(\"\\n\")\n\n# Missing Value\ndef missing_values(data, plot=False):\n    mst = pd.DataFrame(\n        {\"Num_Missing\": data.isnull().sum(), \"Missing_Ratio\": data.isnull().sum() \/ data.shape[0]}).sort_values(\n        \"Num_Missing\", ascending=False)\n    mst[\"DataTypes\"] = data[mst.index].dtypes.values\n    mst = mst[mst.Num_Missing > 0].reset_index().rename({\"index\": \"Feature\"}, axis=1)\n\n    print(\"Number of Variables include Missing Values:\", mst.shape[0], \"\\n\")\n\n    if mst[mst.Missing_Ratio >= 1.0].shape[0] > 0:\n        print(\"Full Missing Variables:\", mst[mst.Missing_Ratio >= 1.0].Feature.tolist())\n        data.drop(mst[mst.Missing_Ratio >= 1.0].Feature.tolist(), axis=1, inplace=True)\n\n        print(\"Full missing variables are deleted!\", \"\\n\")\n\n    if plot:\n        plt.figure(figsize=(25, 8))\n        p = sns.barplot(mst.Feature, mst.Missing_Ratio)\n        for rotate in p.get_xticklabels():\n            rotate.set_rotation(90)\n        plt.show()\n\n    print(mst, \"\\n\")\n    \n    \n# Quantile functions for aggregations\ndef quantile_funcs(percentiles = [0.75, 0.9, 0.99]):\n    return [(p, lambda x: x.quantile(p)) for p in percentiles]\n\n# Rare Encoder\ndef rare_encoder(data, col, rare_perc):\n    temp = data[col].value_counts() \/ len(data) < rare_perc\n    data[col] = np.where(~data[col].isin(temp[temp < rare_perc].index), \"Rare\", data[col])","22fa2a84":"train = pd.read_csv(\"..\/input\/home-credit-default-risk\/application_train.csv\")\ntest = pd.read_csv(\"..\/input\/home-credit-default-risk\/application_test.csv\")\nbureau = pd.read_csv(\"..\/input\/home-credit-default-risk\/bureau.csv\")\nbureau_balance = pd.read_csv(\"..\/input\/home-credit-default-risk\/bureau_balance.csv\")\npos = pd.read_csv('..\/input\/home-credit-default-risk\/POS_CASH_balance.csv')\ncc = pd.read_csv('..\/input\/home-credit-default-risk\/credit_card_balance.csv')\nins = pd.read_csv('..\/input\/home-credit-default-risk\/installments_payments.csv')\nprev = pd.read_csv('..\/input\/home-credit-default-risk\/previous_application.csv')\n\nprint(\"Dimension\")\ntrain.shape, test.shape, bureau.shape, bureau_balance.shape, pos.shape, cc.shape, ins.shape, prev.shape","1fe5ae1c":"# Imbalanced\ncat_analyzer(train, \"TARGET\")","8983e03c":"# Train Test\nprint(\"Number of unique observations in the SK_ID_CURR variable \\n TRAIN: {} \\t TEST: {} \\n\".format(train.SK_ID_CURR.nunique(), test.SK_ID_CURR.nunique()))\n\n# Bureau & Bureau Balance\nprint(\"Number of unique observations in the SK_ID_BUREAU variable \\n BUREAU: {} \\t BUREAU BALANCE: {} \\t INTERSECTION: {} \\n\".format(bureau.SK_ID_BUREAU.nunique(), bureau_balance.SK_ID_BUREAU.nunique(), bureau[bureau.SK_ID_BUREAU.isin(bureau_balance.SK_ID_BUREAU.unique())].SK_ID_BUREAU.nunique()))\n\n# Train-Test & Bureau\nprint(\"Number of unique observations in the SK_ID_CURR variable \\n TRAIN & BUREAU INTERSECTION: {} \\t TEST & BUREAU INTERSECTION: {} \\n\".format(train[train.SK_ID_CURR.isin(bureau.SK_ID_CURR.unique())].SK_ID_CURR.nunique(),test[test.SK_ID_CURR.isin(bureau.SK_ID_CURR.unique())].SK_ID_CURR.nunique()))\n\ndel train, test, bureau, bureau_balance, pos, cc, ins, prev","e71a84e4":"bureau_balance = pd.read_csv(\"..\/input\/home-credit-default-risk\/bureau_balance.csv\")\nbureau_balance = reduce_mem_usage(bureau_balance)\n\nprint(bureau_balance.shape, \"\\n\")\n\nbureau_balance.head()","267bd588":"# Are there any missing values in the data?\nbureau_balance.isnull().sum()","4b1196bc":"# Descriptive Statistics\nprint(bureau_balance.MONTHS_BALANCE.agg({\"min\", \"max\", \"mean\", \"median\", \"std\"}))\n\nprint(96\/12, \" Max year\")","8d6a80bf":"# Histogram\nbureau_balance.MONTHS_BALANCE.hist(), plt.show()","5ef32602":"bureau_balance.STATUS.value_counts()","cf2764ae":"# One-Hot Encoder\nbb, bb_cat = one_hot_encoder(bureau_balance, nan_as_category=False)\n\n# Bureau balance: Perform aggregations and merge with bureau.csv\nbb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n\nfor col in bb_cat:\n    bb_aggregations[col] = ['mean']\n\nbb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\nbb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n\n# Status Sum\nbb_agg[\"STATUS_C0_MEAN_SUM\"] = bb_agg[[\"STATUS_C_MEAN\", \"STATUS_0_MEAN\"]].sum(axis = 1)\nbb_agg[\"STATUS_12_MEAN_SUM\"] = bb_agg[[\"STATUS_1_MEAN\", \"STATUS_2_MEAN\"]].sum(axis = 1)\nbb_agg[\"STATUS_345_MEAN_SUM\"] = bb_agg[[\"STATUS_3_MEAN\", \"STATUS_4_MEAN\", \"STATUS_5_MEAN\"]].sum(axis = 1)\nbb_agg[\"STATUS_12345_MEAN_SUM\"] = bb_agg[[\"STATUS_1_MEAN\", \"STATUS_2_MEAN\", \"STATUS_3_MEAN\", \"STATUS_4_MEAN\", \"STATUS_5_MEAN\"]].sum(axis = 1)\n\n# Find the first month when the credit is closed!\nclosed = bureau_balance[bureau_balance.STATUS == \"C\"]\nclosed = closed.groupby(\"SK_ID_BUREAU\").MONTHS_BALANCE.min().reset_index().rename({\"MONTHS_BALANCE\":\"MONTHS_BALANCE_FIRST_C\"}, axis = 1)\nclosed[\"MONTHS_BALANCE_FIRST_C\"] = np.abs(closed[\"MONTHS_BALANCE_FIRST_C\"])\nbb_agg = pd.merge(bb_agg, closed, how = \"left\", on = \"SK_ID_BUREAU\")\nbb_agg[\"MONTHS_BALANCE_CLOSED_DIF\"] = np.abs(bb_agg.MONTHS_BALANCE_MIN) - bb_agg.MONTHS_BALANCE_FIRST_C\n\ndel closed, bb_aggregations, bureau_balance, bb_cat","94e761f9":"print(\"BURAU BALANCE SHAPE:\", bb_agg.shape, \"\\n\")\n\nbb_agg.head()","3b75a22f":"bureau = pd.read_csv(\"..\/input\/home-credit-default-risk\/bureau.csv\")\nbureau = reduce_mem_usage(bureau)\n\nprint(bureau.shape, \"\\n\")\n\nbureau.head()","81de14c8":"# LEFT JOIN WITH BUREAU\nbureau = pd.merge(bureau, bb_agg, how='left', on='SK_ID_BUREAU')\ndel bb_agg\n\nprint(bureau.shape, \"\\n\")\n\nbureau.head()","f3b89d0f":"# Are there any missing values in the data?\nmissing_values(bureau, plot = False)","1e0a71df":"# How many loans of each customer are there to from Credit Bureau?\nbureau.groupby(\"SK_ID_CURR\").SK_ID_BUREAU.count().hist(bins=50), plt.show()\nbureau.groupby(\"SK_ID_CURR\").SK_ID_BUREAU.count().agg({\"min\", \"max\", \"mean\", \"median\", \"std\"})","cd2c9c16":"# Columns\ncat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(bureau, car_th=10)\n\nprint(\"\")\n\n# Categorical Features\nprint(cat_cols, cat_but_car)","95eaa30f":"# Cat Analyzer\nfor i in cat_cols + cat_but_car:\n    cat_analyzer(bureau, i)","768cc075":"# Numeric Features\nbureau.drop([\"SK_ID_CURR\" ,\"SK_ID_BUREAU\"], axis = 1).describe([.01, .1, .25, .5, .75, .8, .9, .95, .99])[1:]\n","f7b5b6f2":"# Quick Visualization for numerical variables\nnum_plot(bureau, num_cols=num_cols, remove=['SK_ID_CURR','SK_ID_BUREAU'], figsize = (15,3))","d464be76":"# Correlation\ncorr_plot(bureau, remove=['SK_ID_CURR','SK_ID_BUREAU'], corr_coef = \"spearman\")","70bc6cf3":"high_correlation(bureau, remove=['SK_ID_CURR','SK_ID_BUREAU'], corr_coef = \"spearman\", corr_value = 0.7)","507ecb17":"# FEATURE ENGINEERING FOR BUREAU\n\n# Categorical Variables\n# -----------------------------------------------------------\n# Useless\nbureau.drop(\"CREDIT_CURRENCY\", axis = 1, inplace = True)\n\n# Rare Categories\nbureau[\"CREDIT_ACTIVE\"] = np.where(bureau.CREDIT_ACTIVE.isin([\"Sold\", \"Bad debt\"]), \"Sold_BadDebt\", bureau.CREDIT_ACTIVE)\n\nbureau[\"CREDIT_TYPE\"] = np.where(\n    ~bureau.CREDIT_TYPE.isin(\n        [\"Consumer credit\", \"Credit card\", \"Car loan\", \"Mortgage\", \"Microloan\"]\n    ), \"Other\", bureau[\"CREDIT_TYPE\"])\n\n# One-Hot Encoder\nbureau, bureau_cat = one_hot_encoder(bureau, nan_as_category=False)\n\n\n# Numerical Variables\n# -----------------------------------------------------------\n\n# Bureau and bureau_balance numeric features\ncal = ['min', 'max', 'mean', 'sum', 'median','std']\ncols1 = [\n    'DAYS_CREDIT', 'DAYS_CREDIT_ENDDATE', 'DAYS_CREDIT_UPDATE','CREDIT_DAY_OVERDUE',\n    'AMT_CREDIT_MAX_OVERDUE', 'AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM_OVERDUE',\n    'AMT_CREDIT_SUM_LIMIT', 'AMT_ANNUITY', 'CNT_CREDIT_PROLONG', 'MONTHS_BALANCE_MIN',\n    'MONTHS_BALANCE_MAX', 'MONTHS_BALANCE_SIZE', 'MONTHS_BALANCE_FIRST_C', 'MONTHS_BALANCE_CLOSED_DIF'\n]\n\nnum_aggregations = {}\n\n\nfor i in cols1:\n    num_aggregations[i] = cal\n    \n    \n# Bureau and bureau_balance categorical features\ncat_aggregations = {}\n\nfor i in bureau_cat:\n    cat_aggregations[i] = ['mean']\n\ncols2 = ['STATUS_0_MEAN', 'STATUS_1_MEAN', 'STATUS_2_MEAN', 'STATUS_3_MEAN', 'STATUS_4_MEAN',\n        'STATUS_5_MEAN', 'STATUS_C_MEAN', 'STATUS_X_MEAN', 'STATUS_C0_MEAN_SUM',\n        'STATUS_12_MEAN_SUM', 'STATUS_345_MEAN_SUM', 'STATUS_12345_MEAN_SUM']\nfor i in cols2:\n    cat_aggregations[i] = ['mean', 'median', 'sum', 'max', 'std']\n\ndel i, cols1, cols2, bureau_cat, cal\n    \n# Create aggregated data\nbureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\nbureau_agg.columns = pd.Index(['BUREAU_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n\n\n# New features\nbureau.groupby(\"SK_ID_CURR\").SK_ID_BUREAU.count().value_counts()\nbcount = bureau.groupby(\"SK_ID_CURR\").SK_ID_BUREAU.count().reset_index().rename({\"SK_ID_BUREAU\":\"BUREAU_COUNT\"}, axis = 1)\nbcount[\"BUREAU_COUNT_CAT\"] = np.where(bcount.BUREAU_COUNT < 4, 0, 1)\nbcount[\"BUREAU_COUNT_CAT\"] = np.where((bcount.BUREAU_COUNT >= 8) & (bcount.BUREAU_COUNT < 13), 2, bcount[\"BUREAU_COUNT_CAT\"])\nbcount[\"BUREAU_COUNT_CAT\"] = np.where((bcount.BUREAU_COUNT >= 13) & (bcount.BUREAU_COUNT < 20), 3, bcount[\"BUREAU_COUNT_CAT\"])\nbcount[\"BUREAU_COUNT_CAT\"] = np.where((bcount.BUREAU_COUNT >= 20), 4, bcount[\"BUREAU_COUNT_CAT\"])\nbureau_agg = pd.merge(bureau_agg, bcount, how = \"left\", on = \"SK_ID_CURR\")\ndel bcount\n\n\n# Bureau: Active credits - using only numerical aggregations\nactive = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\nactive_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\nactive_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\nbureau_agg = pd.merge(bureau_agg, active_agg, how='left', on='SK_ID_CURR')\ndel active, active_agg\n\n\n# Bureau: Closed credits - using only numerical aggregations\nclosed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\nclosed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\nclosed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\nbureau_agg = pd.merge(bureau_agg, closed_agg, how='left', on='SK_ID_CURR')\ndel closed, closed_agg\n\n# Bureau: Sold and Bad Debt credits - using only numerical aggregations\nsold_baddebt = bureau[bureau['CREDIT_ACTIVE_Sold_BadDebt'] == 1]\nsold_baddebt_agg = sold_baddebt.groupby('SK_ID_CURR').agg(num_aggregations)\nsold_baddebt_agg.columns = pd.Index(['SOLD_BADDEBT' + e[0] + \"_\" + e[1].upper() for e in sold_baddebt_agg.columns.tolist()])\nbureau_agg = pd.merge(bureau_agg, sold_baddebt_agg, how='left', on='SK_ID_CURR')\ndel sold_baddebt, sold_baddebt_agg, bureau\n\ndel num_aggregations, cat_aggregations\n\n\n# WRITE FEATHER\nbureau_agg.to_feather(\"bureau_bureaubalance_agg_feather\")\n#pd.read_feather(\".\/bureau_bureaubalance_agg_feather\")\n\nprint(\"BUREAU & BURAU BALANCE SHAPE:\", bureau_agg.shape, \"\\n\")\n\nbureau_agg.head()","99054d06":"bureau_agg.to_feather(\"bureau_bureaubalance_agg_feather\")\ndel bureau_agg","58d69d4b":"pos = pd.read_csv('..\/input\/home-credit-default-risk\/POS_CASH_balance.csv')\npos = reduce_mem_usage(pos)\n\nprint(pos.shape, \"\\n\")\n\n# Columns\ncat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(pos, car_th=10)\n\nprint(\"\")\n\npos.head()","5a09eeea":"# Are there any missing values in the data?\nmissing_values(pos, plot = False)","8cdd3ae9":"# Cat Analyzer\ncat_analyzer(pos, \"NAME_CONTRACT_STATUS\")","714232a0":"# Numeric Features\npos.drop([\"SK_ID_CURR\" ,\"SK_ID_PREV\"], axis = 1).describe([.01, .1, .25, .5, .75, .8, .9, .95, .99])[1:]","c99bbbde":"# Quick Visualization for numerical variables\nnum_plot(pos, num_cols=num_cols, remove=['SK_ID_CURR','SK_ID_PREV'], figsize = (15,3))","af6f4241":"# Correlation\ncorr_plot(pos, remove=['SK_ID_CURR','SK_ID_PREV'], corr_coef = \"spearman\", figsize = (5,5))","7e383f68":"# Rare\npos[\"NAME_CONTRACT_STATUS\"] = np.where(~(pos[\"NAME_CONTRACT_STATUS\"].isin([\n   \"Active\", \"Completed\"\n])), \"Rare\", pos[\"NAME_CONTRACT_STATUS\"])\n\n# One-Hot Encoder\npos, cat_cols = one_hot_encoder(pos, nan_as_category=False)\n\naggregations = {\n    # Numerical\n    'MONTHS_BALANCE': ['max', 'mean', 'size'],\n    'CNT_INSTALMENT': ['max', 'mean', 'std', 'min', 'median'],\n    'CNT_INSTALMENT_FUTURE': ['max', 'mean', 'sum', 'min', 'median', 'std'],\n    'SK_DPD': ['max', 'mean'],\n    'SK_DPD_DEF': ['max', 'mean']\n}\n# Categorical\nfor cat in cat_cols:\n    aggregations[cat] = ['mean']\n\n# Aggregation\npos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\npos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n# Count pos cash accounts\npos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\npos_agg.reset_index(inplace = True)\ndel pos\n\nprint(\"POS CASH BALANCE SHAPE:\", pos_agg.shape, \"\\n\")\n\npos_agg.head()","e9361806":"# WRITE FEATHER\npos_agg.to_feather(\"poscashbalance_agg_feather\")\ndel pos_agg","ae89c7c7":"cc = pd.read_csv('..\/input\/home-credit-default-risk\/credit_card_balance.csv')\ncc = reduce_mem_usage(cc)\n\nprint(cc.shape, \"\\n\")\n\n# Columns\ncat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(cc, car_th=10)\n\nprint(\"\")\n\ncc.head()","55d1ecea":"# Are there any missing values in the data?\nmissing_values(cc, plot = False)","632980f0":"# Cat Analyzer\ncat_analyzer(cc, \"NAME_CONTRACT_STATUS\")","a200b1bf":"# Numeric Features\ncc.drop([\"SK_ID_CURR\" ,\"SK_ID_PREV\"], axis = 1).describe([.01, .1, .25, .5, .75, .8, .9, .95, .99])[1:]\n","6812ee7b":"# Quick Visualization for numerical variables\nnum_plot(cc, num_cols=num_cols, remove=['SK_ID_CURR','SK_ID_PREV'], figsize = (15,3))","6458fe04":"# Correlation\ncorr_plot(cc, remove=['SK_ID_CURR','SK_ID_PREV'], corr_coef = \"spearman\", figsize = (10,10))","8104b0f3":"# Rare\ncc[\"NAME_CONTRACT_STATUS\"] = np.where(~(cc[\"NAME_CONTRACT_STATUS\"].isin([\n   \"Active\", \"Completed\"\n])), \"Rare\", cc[\"NAME_CONTRACT_STATUS\"])\n\n# One Hot Encoder\ncc, cat_cols = one_hot_encoder(cc, nan_as_category=False)\n\n# General aggregations\ncc.drop(['SK_ID_PREV'], axis=1, inplace=True)\ncc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'std'])\ncc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n# Count credit card lines\ncc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\ncc_agg.reset_index(inplace = True)\ndel cc\n\nprint(\"CREDIT CARD BALANCE SHAPE:\", cc_agg.shape, \"\\n\")\n\ncc_agg.head()","3981cdf8":"# WRITE FEATHER\ncc_agg.to_feather(\"cc_feather\")\ndel cc_agg","3dffd5d9":"ins = pd.read_csv('..\/input\/home-credit-default-risk\/installments_payments.csv')\nins = reduce_mem_usage(ins)\n\nprint(ins.shape, \"\\n\")\n\n# Columns\ncat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(ins, car_th=10)\n\nprint(\"\")\n\nins.head()","19f67d7d":"# Are there any missing values in the data?\nmissing_values(ins, plot = False)","b7ab0a55":"# Numeric Features\nins.drop([\"SK_ID_CURR\" ,\"SK_ID_PREV\"], axis = 1).describe([.01, .1, .25, .5, .75, .8, .9, .95, .99])[1:]\n","3f594b9f":"# Quick Visualization for numerical variables\nnum_plot(ins, num_cols=num_cols, remove=['SK_ID_CURR','SK_ID_PREV'], figsize = (15,3))","b307bffa":"# Correlation\ncorr_plot(ins, remove=['SK_ID_CURR','SK_ID_PREV'], corr_coef = \"spearman\", figsize = (5,5))","f652108b":"high_correlation(ins, remove=['SK_ID_CURR','SK_ID_PREV'], corr_coef = \"spearman\", corr_value = 0.7)","d1d3ef92":"# Percentage and difference paid in each installment (amount paid and installment value)\nins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] \/ ins['AMT_INSTALMENT']\nins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n# Days past due and days before due (no negative values)\nins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\nins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\nins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\nins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n# Features: Perform aggregations\naggregations = {\n    'NUM_INSTALMENT_VERSION': ['nunique'],\n    'NUM_INSTALMENT_NUMBER': ['max', 'mean', 'sum', 'median', 'std'],\n    'DAYS_INSTALMENT': ['max', 'mean', 'sum', 'median', 'std'],\n    'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum', 'median', 'std'],\n    'AMT_INSTALMENT': ['max', 'mean', 'sum', 'median', 'std'],\n    'AMT_PAYMENT': ['min', 'max', 'mean', 'sum', 'median', 'std'],\n    'DPD': ['max', 'mean', 'sum', 'median', 'std'],\n    'DBD': ['max', 'mean', 'sum', 'median', 'std'],\n    'PAYMENT_PERC': ['max', 'mean', 'sum', 'std', 'median'],\n    'PAYMENT_DIFF': ['max', 'mean', 'sum', 'std', 'median']\n}\n\nins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\nins_agg.columns = pd.Index(['INS_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n# Count installments accounts\nins_agg['INS_COUNT'] = ins.groupby('SK_ID_CURR').size()\n\nins_agg.reset_index(inplace = True)\ndel ins\n\n\n\nprint(\"INSTALLMENTS PAYMENTS SHAPE:\", ins_agg.shape, \"\\n\")\n\nins_agg.head()","4342106b":"# WRITE FEATHER\nins_agg.to_feather(\"installments_payments_agg_feather\")\ndel ins_agg","eaa73d7e":"prev = pd.read_csv('..\/input\/home-credit-default-risk\/previous_application.csv')\nprev = reduce_mem_usage(prev)\n\nprint(prev.shape, \"\\n\")\n\n# Columns\ncat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(prev, car_th=10)\n\nprint(\"\")\n\nprev.head()","7526e864":"# Are there any missing values in the data?\nmissing_values(prev, plot = False)","cf3abaed":"for i in cat_cols + cat_but_car + num_but_cat:\n    cat_analyzer(prev, i)","d72cff30":"# Numeric Features\nprev.drop([\"SK_ID_CURR\" ,\"SK_ID_PREV\"], axis = 1).describe([.01, .1, .25, .5, .75, .8, .9, .95, .99])[1:]\n","e8f4d4a5":"# Quick Visualization for numerical variables\nnum_plot(prev, num_cols=num_cols, remove=['SK_ID_CURR','SK_ID_PREV'], figsize = (15,3))","96cc2945":"# Correlation\ncorr_plot(prev, remove=['SK_ID_CURR','SK_ID_PREV'], corr_coef = \"spearman\", figsize = (10,10))","6afa00a9":"high_correlation(prev, remove=['SK_ID_CURR','SK_ID_PREV'], corr_coef = \"spearman\", corr_value = 0.7)","9c877641":"# Rare Encoder\nrare_cols = [\n    \"NAME_PAYMENT_TYPE\", \"CODE_REJECT_REASON\", \"CHANNEL_TYPE\", \"NAME_GOODS_CATEGORY\",\n    \"NAME_SELLER_INDUSTRY\", \"NAME_TYPE_SUITE\"\n]\n\nfor i in rare_cols:\n    rare_encoder(prev, i, rare_perc = 0.01)\n\nprev[\"NAME_CASH_LOAN_PURPOSE\"] = np.where(~prev[\"NAME_CASH_LOAN_PURPOSE\"].isin([\"XAP\", \"XNA\"]), \"Other\", prev[\"NAME_CASH_LOAN_PURPOSE\"])\n\nrare_encoder(prev, \"NAME_PORTFOLIO\", rare_perc = 0.1) \n\n# Cash, Pos, Card\nprev[\"PRODUCT_COMBINATION_CATS\"] = np.where(prev[\"PRODUCT_COMBINATION\"].str.contains(\"Cash\"), \"CASH\", \"POS\")\nprev[\"PRODUCT_COMBINATION_CATS\"] = np.where(prev[\"PRODUCT_COMBINATION\"].str.contains(\"Card\"), \"CARD\", prev[\"PRODUCT_COMBINATION_CATS\"])\n# New categorical variables\nprev[\"PRODUCT_COMBINATION_POS_WITH\"] = np.where((prev[\"PRODUCT_COMBINATION\"].str.contains(\"POS\") & (prev[\"PRODUCT_COMBINATION\"].str.contains(\"without\"))), \"WITHOUT\", \"OTHER\")\nprev[\"PRODUCT_COMBINATION_POS_WITH\"] = np.where((prev[\"PRODUCT_COMBINATION\"].str.contains(\"POS\") & (prev[\"PRODUCT_COMBINATION\"].str.contains(\"with interest\"))), \"WITH\", prev[\"PRODUCT_COMBINATION_POS_WITH\"])\nprev[\"PRODUCT_COMBINATION_POS_TYPE\"] = np.where((prev[\"PRODUCT_COMBINATION\"].str.contains(\"POS\") & (prev[\"PRODUCT_COMBINATION\"].str.contains(\"household\"))), \"household\", \"OTHER\")\nprev[\"PRODUCT_COMBINATION_POS_TYPE\"] = np.where((prev[\"PRODUCT_COMBINATION\"].str.contains(\"POS\") & (prev[\"PRODUCT_COMBINATION\"].str.contains(\"industry\"))), \"industry\", prev[\"PRODUCT_COMBINATION_POS_TYPE\"])\nprev[\"PRODUCT_COMBINATION_POS_TYPE\"] = np.where((prev[\"PRODUCT_COMBINATION\"].str.contains(\"POS\") & (prev[\"PRODUCT_COMBINATION\"].str.contains(\"mobile\"))), \"mobile\", prev[\"PRODUCT_COMBINATION_POS_TYPE\"])\nprev[\"PRODUCT_COMBINATION_POS_TYPE\"] = np.where((prev[\"PRODUCT_COMBINATION\"].str.contains(\"POS\") & (prev[\"PRODUCT_COMBINATION\"].str.contains(\"other\"))), \"posother\", prev[\"PRODUCT_COMBINATION_POS_TYPE\"])\nprev[\"PRODUCT_COMBINATION_CASH_TYPE\"] = np.where((prev[\"PRODUCT_COMBINATION\"].str.contains(\"Cash\") & (prev[\"PRODUCT_COMBINATION\"].str.contains(\"X-Sell\"))), \"xsell\", \"OTHER\")\nprev[\"PRODUCT_COMBINATION_CASH_TYPE\"] = np.where((prev[\"PRODUCT_COMBINATION\"].str.contains(\"Cash\") & (prev[\"PRODUCT_COMBINATION\"].str.contains(\"Street\"))), \"street\", prev[\"PRODUCT_COMBINATION_CASH_TYPE\"])\n\n\n# Useless\nprev.drop([\"WEEKDAY_APPR_PROCESS_START\", \"FLAG_LAST_APPL_PER_CONTRACT\", \"NFLAG_LAST_APPL_IN_DAY\", \"NFLAG_LAST_APPL_IN_DAY\"], axis = 1, inplace = True)\n\n# One-Hot Encoder\nprev, cat_cols = one_hot_encoder(prev, nan_as_category=True)\n\n\n# Days 365.243 values -> nan\nprev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\nprev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\nprev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\nprev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\nprev['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n\n# Add feature: value ask \/ value received percentage\nprev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] \/ prev['AMT_CREDIT']\n\n\n# Previous applications numeric features\nnum_aggregations = {\n    'AMT_ANNUITY': ['min', 'max', 'mean', \"median\", \"std\"],\n    'AMT_APPLICATION': ['min', 'max', 'mean', \"median\", \"std\"],\n    'AMT_CREDIT': ['min', 'max', 'mean', \"median\", \"std\"],\n    'APP_CREDIT_PERC': ['min', 'max', 'mean', \"median\", \"std\"],\n    'AMT_DOWN_PAYMENT': ['min', 'max', 'mean', \"median\", \"std\"],\n    'AMT_GOODS_PRICE': ['min', 'max', 'mean', \"median\", \"std\"],\n    'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean', \"median\", \"std\"],\n    'RATE_DOWN_PAYMENT': ['min', 'max', 'mean', \"std\"],\n    'RATE_INTEREST_PRIMARY': ['min', 'max', 'mean', \"std\"],\n    'RATE_INTEREST_PRIVILEGED': ['min', 'max', 'mean', \"std\"],\n    'DAYS_DECISION': ['min', 'max', 'mean', \"median\", \"std\"],\n    'CNT_PAYMENT': ['mean', 'sum', \"median\", \"std\"],\n    'SELLERPLACE_AREA': ['min', 'max', 'mean', \"median\", \"std\"],\n    'DAYS_FIRST_DRAWING': ['min', 'max', 'mean', \"median\", \"std\"],\n    'DAYS_FIRST_DUE': ['min', 'max', 'mean', \"median\", \"std\"],\n    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean', \"median\", \"std\"],\n    'DAYS_LAST_DUE': ['min', 'max', 'mean', \"median\", \"std\"],\n    'DAYS_TERMINATION': ['min', 'max', 'mean', \"median\", \"std\"],\n    # Categorical\n    \"NFLAG_INSURED_ON_APPROVAL\": [\"mean\"]\n}\n# Previous applications categorical features\ncat_aggregations = {}\nfor cat in cat_cols:\n    cat_aggregations[cat] = ['mean']\n\nprev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\nprev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n\n# Previous Applications: Approved Applications - only numerical features\napproved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\napproved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\napproved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\nprev_agg = pd.merge(prev_agg,approved_agg, how='left', on='SK_ID_CURR')\n\n# Previous Applications: Refused Applications - only numerical features\nrefused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\nrefused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\nrefused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\nprev_agg = pd.merge(prev_agg, refused_agg, how='left', on='SK_ID_CURR')\n\ndel refused, refused_agg, approved, approved_agg, prev\nprev_agg.reset_index(inplace = True)\n\n\nprint(\"PREVIOUS APPLICATIONS SHAPE:\", prev_agg.shape, \"\\n\")\n\nprev_agg.head()","ce14a30d":"# WRITE FEATHER\nprev_agg.to_feather(\"previous_applications_agg_feather\")\ndel prev_agg","bde4f990":"df = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\ntest_df = pd.read_csv('..\/input\/home-credit-default-risk\/application_test.csv')\n\nprint(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n\ndf = df.append(test_df)\ndf = reduce_mem_usage(df)\n\n# Columns\ncat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(df, car_th=10)\n\nprint(\"\")\n\ndf.head()","b7b9f262":"# Are there any missing values in the data?\nmissing_values(df, plot = False)","076dd98a":"for i in cat_cols + cat_but_car + num_but_cat:\n    cat_analyzer(df, i, target = \"TARGET\")","319ba72d":"# Numeric Features\ndf.drop([\"SK_ID_CURR\" ], axis = 1).describe([.01, .1, .25, .5, .75, .8, .9, .95, .99])[1:]\n","3a5c13aa":"# Quick Visualization for numerical variables\nnum_plot(df, num_cols=num_cols, remove=['SK_ID_CURR'], figsize = (15,3))","074b7695":"# Correlation\ncorr_plot(df, remove=['SK_ID_CURR'], corr_coef = \"spearman\", figsize = (10,10))","0227f64b":"high_correlation(df, remove=['SK_ID_CURR'], corr_coef = \"spearman\", corr_value = 0.7)","c2180cf8":"# ERRORS\ndf = df[~(df.CODE_GENDER.str.contains(\"XNA\"))]  \ndf = df[df.NAME_FAMILY_STATUS != \"Unknown\"]  \n\n# DROP\ncols = [\"NAME_HOUSING_TYPE\", \"WEEKDAY_APPR_PROCESS_START\", \"FONDKAPREMONT_MODE\", \"WALLSMATERIAL_MODE\", \"HOUSETYPE_MODE\",\n        \"EMERGENCYSTATE_MODE\",\"FLAG_MOBIL\", \"FLAG_EMP_PHONE\",\"FLAG_WORK_PHONE\", \"FLAG_CONT_MOBILE\", \"FLAG_PHONE\", \"FLAG_EMAIL\"]\ndf.drop(cols, axis = 1, inplace = True)\n\n# REGION\ncols = [\"REG_REGION_NOT_LIVE_REGION\",\"REG_REGION_NOT_WORK_REGION\", \"LIVE_REGION_NOT_WORK_REGION\", \"REG_CITY_NOT_LIVE_CITY\",\n \"REG_CITY_NOT_WORK_CITY\",\"LIVE_CITY_NOT_WORK_CITY\"]\ndf[\"REGION\"] = df[cols].sum(axis = 1)\ndf.drop(cols, axis = 1, inplace = True)\n\n# Drop FLAG_DOCUMENT \ndf.drop(df.columns[df.columns.str.contains(\"FLAG_DOCUMENT\")], axis = 1, inplace = True)\n\n\n# RARE ENCODER\ndf[\"NAME_EDUCATION_TYPE\"] = np.where(df.NAME_EDUCATION_TYPE == \"Academic degree\", \"Higher education\", df.NAME_EDUCATION_TYPE)\n\n\ndf[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.str.contains(\"Business Entity\"), \"Business Entity\", df.ORGANIZATION_TYPE)\ndf[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.str.contains(\"Industry\"), \"Industry\", df.ORGANIZATION_TYPE)\ndf[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.str.contains(\"Trade\"), \"Trade\", df.ORGANIZATION_TYPE)\ndf[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.str.contains(\"Transport\"), \"Transport\", df.ORGANIZATION_TYPE)\ndf[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"School\", \"Kindergarten\", \"University\"]), \"Education\", df.ORGANIZATION_TYPE)\ndf[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"Emergency\",\"Police\", \"Medicine\",\"Goverment\", \"Postal\", \"Military\", \"Security Ministries\", \"Legal Services\"]), \"Public\", df.ORGANIZATION_TYPE)\ndf[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"Bank\", \"Insurance\"]), \"Finance\", df.ORGANIZATION_TYPE)\ndf[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"Realtor\", \"Housing\"]), \"House\", df.ORGANIZATION_TYPE)\ndf[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"Hotel\", \"Restaurant\"]), \"HotelRestaurant\", df.ORGANIZATION_TYPE)\ndf[\"ORGANIZATION_TYPE\"] = np.where(df.ORGANIZATION_TYPE.isin([\"Cleaning\",\"Electricity\", \"Telecom\", \"Mobile\", \"Advertising\", \"Religion\", \"Culture\"]), \"Other\", df.ORGANIZATION_TYPE)\n\ndf[\"OCCUPATION_TYPE\"] = np.where(df.OCCUPATION_TYPE.isin([\"Low-skill Laborers\", \"Cooking staff\", \"Security staff\", \"Private service staff\", \"Cleaning staff\", \"Waiters\/barmen staff\"]), \"Low-skill Laborers\", df.OCCUPATION_TYPE)\ndf[\"OCCUPATION_TYPE\"] = np.where(df.OCCUPATION_TYPE.isin([\"IT staff\", \"High skill tech staff\"]), \"High skill tech staff\", df.OCCUPATION_TYPE)\n\n\nrare_cols = [\"NAME_TYPE_SUITE\", \"NAME_INCOME_TYPE\"]\n\nfor i in rare_cols:\n    rare_encoder(df, i, rare_perc = 0.01)\n\n    \n# Categorical features with Binary encode (0 or 1; two categories)\nfor bin_feature in [\"NAME_CONTRACT_TYPE\", 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n    df[bin_feature], uniques = pd.factorize(df[bin_feature])\n    \n    \n# Categorical features with One-Hot encode\ndf, cat_cols = one_hot_encoder(df, nan_as_category=False)\n\n\n# NaN values for DAYS_EMPLOYED: 365.243 -> nan\ndf['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n\n# Some simple new features (percentages)\ndf['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] \/ df['DAYS_BIRTH']\ndf['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] \/ df['AMT_CREDIT']\ndf['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] \/ df['CNT_FAM_MEMBERS']\ndf['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] \/ df['AMT_INCOME_TOTAL']\ndf['PAYMENT_RATE'] = df['AMT_ANNUITY'] \/ df['AMT_CREDIT']\n\n\n# EXT SOURCE MEAN FROM OTHER ASSOCIATIONS \ndf[\"NEW_EXT_MEAN\"] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\ndf['NEW_APP_EXT_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n\n# Sat\u0131n al\u0131nacak \u00fcr\u00fcn\u00fcn toplam kredi tutar\u0131na oran\u0131\ndf[\"NEW_GOODS_CREDIT\"] = df[\"AMT_GOODS_PRICE\"] \/ df[\"AMT_CREDIT\"]\n\n# Kredinin y\u0131ll\u0131k \u00f6demesinin m\u00fc\u015fterinin toplam gelirine oran\u0131\ndf['NEW_ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] \/ df['AMT_INCOME_TOTAL']\n\n# \u00dcr\u00fcn ile kredi ile  aras\u0131ndaki fark\u0131n toplam y\u0131ll\u0131k gelire oran\u0131\ndf[\"NEW_C_GP\"] = (df[\"AMT_GOODS_PRICE\"] - df[\"AMT_CREDIT\"]) \/ df[\"AMT_INCOME_TOTAL\"]\n\n\n# Ba\u015fvuru s\u0131ras\u0131nda m\u00fc\u015fterinin g\u00fcn cinsinden ya\u015f\u0131 eksili olarak verilmi\u015f\n# -1 ile \u00e7arp\u0131p 365'e b\u00f6ld\u00fc\u011f\u00fcm\u00fczde ka\u00e7 ya\u015f\u0131nda oldu\u011funu buluyoruz\n\ndf[\"NEW_APP_AGE\"] = round(df[\"DAYS_BIRTH\"] * -1 \/ 365)\n\ndf['NEW_INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] \/ df['CNT_FAM_MEMBERS']\ndf['NEW_PAYMENT_RATE'] = df['AMT_ANNUITY'] \/ df['AMT_CREDIT']\n\n# kredinin \u00e7ekildi\u011fi \u00fcr\u00fcn\u00fcn fiyat\u0131 \/ kredi miktar\u0131\ndf[\"NEW_APP_GOODS\/AMT_CREDIT\"] = df[\"AMT_GOODS_PRICE\"] \/ df[\"AMT_CREDIT\"]\n\ndf['NEW_LOAN_VALUE_RATIO'] = df['AMT_CREDIT'] \/ df['AMT_GOODS_PRICE']\n\ndf['NEW_DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] \/ df['DAYS_BIRTH']\ndf['NEW_ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] \/ df['AMT_INCOME_TOTAL']\n\ndf['NEW_INCOME_PER_PERSON_PERC_PAYMENT_RATE_INCOME_PER_PERSON'] = df['NEW_INCOME_PER_PERSON'] \/ df['NEW_PAYMENT_RATE']\n\nprint(\"APPLICATION TRAIN\/TEST SHAPE:\", df.shape, \"\\n\")\ndf.head()","8165db5f":"# WRITE FEATHER\ndf.reset_index(drop = True).to_feather(\"applications_traintest_feather\")\ndel df","0db7b8c6":"**Balanced or Unbalanced?**\n\nIf we observe TARGET variable like below, we can see there is an unbalanced problem here. \n\n- Train data has 307511 rows.\n- The 1 class in target has 24825 rows and its ratio is %8 in whole data\n- The 0 class in target has 24825 rows and its ratio is %92 in whole data\n- This result shows us there is an unbalanced problem in target","16cac804":"### Merge Bureau Balance and Bureau","4bc5f561":"# 2. FUNCTIONS\n\nThere are some useful functions in this section. They will help to understand the problem, exploratory data analysis, pre-processing and so on.\n\n- Reduce Memory Usage\n- One-Hot Encoder\n- Finding column names and types\n- An analyzer for Categorical Variables\n- Plotting numerical variables\n- Plotting correlations\n- Finding high correlations\n- Missing Value\n- Quantile functions for aggregations\n- Rare Encoder\n\nPlease click **Expand** to see functions!","af076078":"<div><img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/9120\/logos\/header.png?t=2018-04-02-23-51-59\"><\/img><\/div>\n\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/9120\/logos\/thumb76_76.png?t=2018-04-02-23-45-04\" align=\"left\" width = \"100px\"\/>\n\n<h1> Home Credit Default Risk Step by Step <\/h1>","226de37c":"### Data Manipulation & Feature Engineering for Pos Cash Balance","1b2048fe":"**grab_col_names** is very useful function to understand the data for first step. It prints and keeps information about how many there are datetime, categorical, numerical variables. Also variables may be of a different type than they are. For example, one column might be numerical but actually it behaves like a categorical variable. So, grab_col_names function gives us a chance to understand the data deeply.","91c83802":"# 10. Application Train\/Test\n\n- This is the main table, broken into two files for Train (with TARGET) and Test (without TARGET).\n- Static data for all applications. One row represents one loan in our data sample.","2e56b819":"### Data Description\n\nMany people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.\n\nHome Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n\nWhile Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.\n\n### Evaluation\n\nSubmissions are evaluated on [area under the ROC curve](https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic) between the predicted probability and the observed target.\n\n### Datasets\n\n- **application_{train|test}.csv**\n\n    - This is the main table, broken into two files for Train (with TARGET) and Test (without TARGET).\n    - Static data for all applications. One row represents one loan in our data sample.\n\n- **bureau.csv**\n\n    - All client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample).\n    - For every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.\n    \n- **bureau_balance.csv**\n\n    - Monthly balances of previous credits in Credit Bureau.\n    - This table has one row for each month of history of every previous credit reported to Credit Bureau \u2013 i.e the table has (#loans in sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows.\n    \n- **POS_CASH_balance.csv**\n\n    - Monthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit.\n    - This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample \u2013 i.e. the table has (#loans in sample * # of relative previous credits * # of months in which we have some history observable for the previous credits) rows.\n\n- **credit_card_balance.csv**\n\n    - Monthly balance snapshots of previous credit cards that the applicant has with Home Credit.\n    - This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample \u2013 i.e. the table has (#loans in sample * # of relative previous credit cards * # of months where we have some history observable for the previous credit card) rows.\n\n- **previous_application.csv**\n\n    - All previous applications for Home Credit loans of clients who have loans in our sample.\n    - There is one row for each previous application related to loans in our data sample.\n\n- **installments_payments.csv**\n\n    - Repayment history for the previously disbursed credits in Home Credit related to the loans in our sample.\n    - There is a) one row for every payment that was made plus b) one row each for missed payment.\n    - One row is equivalent to one payment of one installment OR one installment corresponding to one payment of one previous Home Credit credit related to loans in our sample.","4948f45b":"# 3. Investigate intersections to key ids of tables!\n\nImport tables and investigate these items:\n\n- Show dimension of tables to see differences\n- Is there an unbalanced problem in the target variable?\n- How many rows are intersected between two tables?\n\n**Dimension**","22b32bc0":"### Data Manipulation & Feature Engineering for Installments Payments","f379c531":"### What will we do?\n\nThis problem includes many tables. Each table is connected another one with a key id. First of all, we need to start analysis from the bottom tables. \n\nFor example, **Bureau Balance table** is connected to **Bureau table** with a key as **SK_ID_BUREAU** and also **Bureau table** is connected to **Application Train\/Test tables** with a key as **SK_ID_CURR**.\n\nWe will start to analyze **Bureau Balance** first! Then, we will deduplicate the data according to the **SK_ID_BUREAU** variable and generate new variables to use on **Bureau table**. After that, the same processings should apply from **Bureau table** to **Application Train\/Test tables** by using **SK_ID_CURR**.\n\nWhen Bureau and Bureau Balance is done, we need to start analysis other bottom tables again to transfer information up!\n\n## Steps\n\n### 1'st Step\n- Investigate intersections to key ids of tables!\n\nAll tables have different number of observations and variables. Moreover, when we merge two tables, some ids might not connect. That's why, sometimes we will see missing values naturally. The purpose of this step is to raise awareness for intersections. Also, a classification problem can include unbalanced target variable. If we work on a classification problem, we must look at target variable.\n\n### Other Steps\n> 1. Bureau Balance\n2. Bureau\n3. Pos Cash Balance\n4. Credit Card Balance\n5. Installments Payments\n6.Previous Application\n7. Previous Application\n8. Application Train Test\n\n- EDA \n- Data Pre-processing\n- Singularization\n- Generate New Features\n- Merge all tables with Application Train\/Test\n\n### References\n- [**@jsaguiar - Aguiar's notebook: LightGBM with Simple Features**](https:\/\/www.kaggle.com\/jsaguiar\/lightgbm-with-simple-features)","72e66dba":"<center> <div style=\"width:70%\"><img src=\"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/home-credit\/home_credit.png\"><\/img><\/div><\/center>","caaffc88":"### EDA for Pos Cash Balance","d16bb847":"After results of cat_analyzer: \n- I think that CREDIT_CURRENCY variable is useless for modelling. Almost all of rows are currency 1 category.\n- CREDIT_ACTIVE variable might be useful. There are two rare categories in this column. We can combine these two categories so we assign a new category as Sold_BadDebt. Briefly, CREDIT_ACTIVE variable includes 3 categories as Active, Closed and Sold_BadDebt.\n- CREDIT_TYPE might be useful but there are some rare categories too. We should reduce number of category. ","3fd3d610":"### Data Manipulation & Feature Engineering for Application Train\/Test","ccba28c8":"### Data Manipulation & Feature Engineering for Bureau","44afa1ca":"### Data Manipulation & Feature Engineering for Application Train\/Test","ae49134c":"### Data Manipulation & Feature Engineering for Previous Applications","376a7e37":"**Intersections**\n\nTables are connected each other with **SK_ID_CURR**, **SK_ID_BUREAU** and **SK_ID_PREV** key ids. I won't show all intersections below, but you will see the point.","515fcd65":"# 7. Credit Card Balance\n\n- Monthly balance snapshots of previous credit cards that the applicant has with Home Credit.\n- This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample \u2013 i.e. the table has (#loans in sample * # of relative previous credit cards * # of months where we have some history observable for the previous credit card) rows.","c7271850":"### EDA for Credit Card Balance","9f7c7788":"### Data Manipulation & Feature Engineering for Credit Card Balance","b2867756":"### Data Manipulation & Feature Engineering for Bureau Balance","06078e4d":"### EDA for Installments Payments","471b6043":"### EDA for Previous Applications","ea84c780":"Summary stats give us many insights about numerical variables. Also percentiles, minimum and maximum values show us that there are any outliers or not. For example, In AMT_CREDIT_MAX_OVERDUE variable maximum value is 115,987,184.00 but 99 percentile is 41,988.75. This difference is too much between max and 99% values. If you want an accurate model more, outliers are one of the problems you should focus on.\n\nIf you want to understand numerical variables better, you should plot them and look their distributoins.","4e4e7f26":"**cat_analyzer** function gives us value counts and ratio of categories in a column. We can learn which categories or columns might be more important than others to use on a model. Also cat_analyzer tells us which columns include rare category. If there is any rare categories in a column, we can use **Rare Encoder** function to combine different rare categories. The main purpose of Rare Encoder is to reduce number of category in a column so the column might be more useful for modelling.","3fd436b3":"# 9. Previous Applications\n\n- All previous applications for Home Credit loans of clients who have loans in our sample.\n- There is one row for each previous application related to loans in our data sample.","7e7f10ad":"# 5. Bureau\n\n1. All client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample).\n2. For every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.","d3b8555b":"# 4. Bureau Balance\n\n**Description**\n1. Monthly balances of previous credits in Credit Bureau.\n2. This table has one row for each month of history of every previous credit reported to Credit Bureau \u2013 i.e the table has (#loans in sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows.\n\nSTATUS: \"Status of Credit Bureau loan during the month (active, closed, DPD0-30,\u2026\n- C means closed,\n- X means status unknown,\n- 0 means no DPD,\n- 1 means maximal did during month between 1-30,\n- 2 means DPD 31-60,\n- \u2026 5 means DPD 120+ or sold or written off)\",\n\n> **NOTE:** If we work on programming or data science and so on, we should pay attention about memory usage for efficiency. There are many tables in this problem and memory usage of some tables might be so much. We can decrease memory usage each table for efficency by using reduce_mem_usage function. For this reason, we will use reduce_mem_usage function in every step. ","589f5a5b":"# 1. PACKAGES\n\nThere are all packages below. Please click **Expand** to see them!","a96bd2a8":"# 6. Pos Cash Balance\n\n- Monthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit.\n- This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample \u2013 i.e. the table has (#loans in sample * # of relative previous credits * # of months in which we have some history observable for the previous credits) rows.","a99c94b8":"### EDA for Bureau Balance","9734adc4":"# 8. Installments Payments\n\n- Repayment history for the previously disbursed credits in Home Credit related to the loans in our sample.\n- There is a) one row for every payment that was made plus b) one row each for missed payment.\n- One row is equivalent to one payment of one installment OR one installment corresponding to one payment of one previous Home Credit credit related to loans in our sample.","53d061db":"### EDA for Bureau Balance\n\nMissing values are one of the biggest problems in data analytics. There are many things to do them but this project we won't focus on missing values."}}