{"cell_type":{"510756a0":"code","81660d1e":"code","4d254d0e":"code","883668d4":"code","2278bfe5":"code","9f195776":"code","673f5d14":"code","553a32b4":"code","cce5fd3f":"code","8ac8489a":"code","f3160b32":"code","58a48200":"code","f929be03":"code","54db6eab":"code","6960375a":"code","23805a9e":"markdown","51a5f232":"markdown","1fa72e75":"markdown","7ec7a19b":"markdown","dee9b08b":"markdown","9fa4708f":"markdown","4a5464c6":"markdown","75662911":"markdown","89a7ebb8":"markdown","1bb4e1b6":"markdown","25018521":"markdown","25213693":"markdown","dca1af94":"markdown","6beec860":"markdown","3d50c274":"markdown","fa73b2ae":"markdown","43037419":"markdown","c2fc4d05":"markdown","ce22fc85":"markdown","ef727a67":"markdown","4c9ec383":"markdown","98e298e0":"markdown","4ccd1b17":"markdown","2d2799f4":"markdown"},"source":{"510756a0":"!pip install yfinance\n!pip install finta\n!pip install mplfinance\nimport mplfinance as mpf\nimport yfinance as yf\nimport datetime\nimport pandas as pd\nimport numpy as np\nfrom finta import TA\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn import metrics\n","81660d1e":"\"\"\"\nDefining some constants for data mining\n\"\"\"\n\nNUM_DAYS = 10000     # The number of days of historical data to retrieve\nINTERVAL = '1d'     # Sample rate of historical data\nsymbol = 'DOGE-USD'      # Symbol of the desired stock\n\n# List of symbols for technical indicators\nINDICATORS = ['RSI', 'MACD', 'STOCH','ADL', 'ATR', 'MOM', 'MFI', 'ROC', 'OBV', 'CCI', 'EMV', 'VORTEX']","4d254d0e":"\nstart = (datetime.date.today() - datetime.timedelta( NUM_DAYS ) )\nend = datetime.datetime.today()\n\ndata = yf.download(symbol, start=start, end=end, interval=INTERVAL)\n\nmpf.plot(data.iloc[-60:],type=\"candle\",volume=True,tight_layout=False,style=\"yahoo\",\n        show_nontrading=True,scale_width_adjustment=dict(ohlc=1,lines=1,volume=2.4),\n        title=\"DOGE Price\",figratio=(20,12),ylabel='Price',\n        ylabel_lower='Value')","883668d4":"data.rename(columns={\"Close\": 'close', \"High\": 'high', \"Low\": 'low', 'Volume': 'volume', 'Open': 'open'}, inplace=True)","2278bfe5":"\"\"\"\nNext we clean our data and perform feature engineering to create new technical indicator features that our\nmodel can learn from\n\"\"\"\n\ndef _exponential_smooth(data, alpha):\n    \"\"\"\n    Function that exponentially smooths dataset so values are less 'rigid'\n    :param alpha: weight factor to weight recent values more\n    \"\"\"\n    \n    return data.ewm(alpha=alpha).mean()\n\ndata = _exponential_smooth(data, 0.65)\n\ntmp1 = data.iloc[-60:]\ntmp1['close'].plot()","9f195776":"def _get_indicator_data(data):\n    \"\"\"\n    Function that uses the finta API to calculate technical indicators used as the features\n    :return:\n    \"\"\"\n\n    for indicator in INDICATORS:\n        ind_data = eval('TA.' + indicator + '(data)')\n        if not isinstance(ind_data, pd.DataFrame):\n            ind_data = ind_data.to_frame()\n        data = data.merge(ind_data, left_index=True, right_index=True)\n    data.rename(columns={\"14 period EMV.\": '14 period EMV'}, inplace=True)\n\n    # Also calculate moving averages for features\n    data['ema50'] = data['close'] \/ data['close'].ewm(50).mean()\n    data['ema21'] = data['close'] \/ data['close'].ewm(21).mean()\n    data['ema15'] = data['close'] \/ data['close'].ewm(14).mean()\n    data['ema5'] = data['close'] \/ data['close'].ewm(5).mean()\n\n    # Instead of using the actual volume value (which changes over time), we normalize it with a moving volume average\n    data['normVol'] = data['volume'] \/ data['volume'].ewm(5).mean()\n\n    # Remove columns that won't be used as features\n    del (data['open'])\n    del (data['high'])\n    del (data['low'])\n    del (data['volume'])\n    del (data['Adj Close'])\n    \n    return data\n\ndata = _get_indicator_data(data)\nprint(data.columns)","673f5d14":"live_pred_data = data.iloc[-16:-11]","553a32b4":"def _produce_prediction(data, window):\n    \"\"\"\n    Function that produces the 'truth' values\n    At a given row, it looks 'window' rows ahead to see if the price increased (1) or decreased (0)\n    :param window: number of days, or rows to look ahead to see what the price did\n    \"\"\"\n    \n    prediction = (data.shift(-window)['close'] >= data['close'])\n    prediction = prediction.iloc[:-window]\n    data['pred'] = prediction.astype(int)\n    \n    return data\n\ndata = _produce_prediction(data, window=5)\ndel (data['close'])\ndata = data.dropna() \ndata.tail()","cce5fd3f":"X=data.drop(\"pred\",axis=1)\ny=data[\"pred\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","8ac8489a":"def _train_random_forest(X_train, y_train, X_test, y_test):\n\n    \"\"\"\n    Function that uses random forest classifier to train the model\n    :return:\n    \"\"\"\n    \n    # Create a new random forest classifier\n    rf = RandomForestClassifier()\n    \n    # Dictionary of all values we want to test for n_estimators\n    params_rf = {'n_estimators': [110,130,140,150,160,180,200]}\n    \n    # Use gridsearch to test all values for n_estimators\n    rf_gs = GridSearchCV(rf, params_rf, cv=5)\n    \n    # Fit model to training data\n    rf_gs.fit(X_train, y_train)\n    \n    # Save best model\n    rf_best = rf_gs.best_estimator_\n    \n    # Check best n_estimators value\n    print(rf_gs.best_params_)\n    \n    prediction = rf_best.predict(X_test)\n\n    print(classification_report(y_test, prediction))\n    print(confusion_matrix(y_test, prediction))\n    \n    return rf_best\n    \nrf_model = _train_random_forest(X_train, y_train, X_test, y_test)","f3160b32":"def _train_KNN(X_train, y_train, X_test, y_test):\n\n    knn = KNeighborsClassifier()\n    # Create a dictionary of all values we want to test for n_neighbors\n    params_knn = {'n_neighbors': np.arange(1, 25)}\n    \n    # Use gridsearch to test all values for n_neighbors\n    knn_gs = GridSearchCV(knn, params_knn, cv=5)\n    \n    # Fit model to training data\n    knn_gs.fit(X_train, y_train)\n    \n    # Save best model\n    knn_best = knn_gs.best_estimator_\n     \n    # Check best n_neigbors value\n    print(knn_gs.best_params_)\n    \n    prediction = knn_best.predict(X_test)\n\n    print(classification_report(y_test, prediction))\n    print(confusion_matrix(y_test, prediction))\n    \n    return knn_best\n    \n    \nknn_model = _train_KNN(X_train, y_train, X_test, y_test)","58a48200":"def _train_GBT(X_train, y_train, X_test, y_test):\n\n    gbt = GradientBoostingClassifier()\n    # Create a dictionary of all values we want to test for n_neighbors\n    params_gbt = {'n_estimators': np.arange(1, 25)}\n    \n    # Use gridsearch to test all values for n_neighbors\n    gbt_gs = GridSearchCV(gbt, params_gbt, cv=5)\n    \n    # Fit model to training data\n    gbt_gs.fit(X_train, y_train)\n    \n    # Save best model\n    gbt_best = gbt_gs.best_estimator_\n     \n    # Check best n_neigbors value\n    print(gbt_gs.best_params_)\n    \n    prediction = gbt_best.predict(X_test)\n\n    print(classification_report(y_test, prediction))\n    print(confusion_matrix(y_test, prediction))\n    \n    return gbt_best\n    \n    \ngbt_model = _train_GBT(X_train, y_train, X_test, y_test)","f929be03":"def _ensemble_model(rf_model, knn_model, gbt_model, X_train, y_train, X_test, y_test):\n    \n    # Create a dictionary of our models\n    estimators=[('knn', knn_model), ('rf', rf_model), ('gbt', gbt_model)]\n    \n    # Create our voting classifier, inputting our models\n    ensemble = VotingClassifier(estimators, voting='hard')\n    \n    #fit model to training data\n    ensemble.fit(X_train, y_train)\n    \n    #test our model on the test data\n    print(ensemble.score(X_test, y_test))\n    \n    prediction = ensemble.predict(X_test)\n\n    print(classification_report(y_test, prediction))\n    print(confusion_matrix(y_test, prediction))\n    \n    return ensemble\n    \nensemble_model = _ensemble_model(rf_model, knn_model, gbt_model, X_train, y_train, X_test, y_test)","54db6eab":"live_pred_data.head()","6960375a":"del(live_pred_data['close'])\nprediction = ensemble_model.predict(live_pred_data)\nprint(prediction)","23805a9e":"### To begin, we include all of the libraries used for this project. I used the yfinance API to gather all of the historical stock market data. It\u2019s very reliable data.","51a5f232":"### For the next step we\u2019re going to predict how the DOGE coin will behave with our predictive model. ","1fa72e75":"### Now it\u2019s time to compute our technical indicators. As stated above, I use the finta library in combination with python\u2019s built in eval function to quickly compute all the indicators in the INDICATORS list. I also compute some ema\u2019s at different average lengths in addition to a normalized volume value.\n\n### I remove the columns like \u2018Open\u2019, \u2018High\u2019, \u2018Low\u2019, and \u2018Adj Close\u2019 because we can get a good enough approximation with our ema\u2019s in addition to the indicators. Volume has been proven to have a correlation with price fluctuations, which is why I normalized it.","7ec7a19b":"### Then the KNN model.","dee9b08b":"### And now finally we create the voting classifier","9fa4708f":"### To summarize what we\u2019ve done in this project,\n### We\u2019ve collected data to be used in analysis and feature creation.\n### We\u2019ve used pandas to compute many model features and produce clean data to help us in machine learning. Created predictions or truth values using pandas.\n### Ensured our predictions were accurate with real world data.\n### Thanks for reading :)","4a5464c6":"### We can see that the data is much more smoothed. Having many peaks and troughs can make it hard to approximate, or be difficult to extract tends when computing the technical indicators. It can throw the model off.","75662911":"### Now we pull our historical data from yfinance. We don\u2019t have many features to work with \u2014 not particularly useful unless we find a way to normalize them at least or derive more features from them.","89a7ebb8":"### Now comes one of the most important part of this project \u2014 computing the truth values. Without these, we wouldn\u2019t even be able to train a machine learning model to make predictions.","1bb4e1b6":"\n# <font color='red'> Understanding Stock Market Analysis\nStock market analysis can be divided into two parts- Fundamental Analysis and Technical Analysis.\n\na. Fundamental Analysis\nThis includes analyzing the current business environment and finances to predict the future profitability of the company.\n\nb. Technical Analysis\nThis deals with charts and statistics to identify trends in the stock market.\n\nIn this document, we will use Technical Analysis and three ways to predict stock with Python- Random forest Classification,KNN and Gradient Boosting Classifier. \n","25018521":"# <font color='red'> 3. Model Creation","25213693":"### Right before we gather our predictions, I decided to keep a small bit of data to predict future values with.","dca1af94":"There are so many factors involved in the prediction of stock market performance hence it becomes one of the most difficult things to do especially when high accuracy is required. \n\nStocker is a Python class-based tool used for stock prediction and analysis. (for complete code refer GitHub) Stocker is designed to be very easy to handle. Even the beginners in python find it that way. It is one of the examples of how we are using python for stock market and how it can be used to handle stock market-related adventures.\n\nFor testing i use DOGE coin and SHIBA. Also you can try all sotcks and coins in Yahoo Finance!!!!","6beec860":"### Here are the five main days we are going to generate a prediction for. Looks like the models predicts that the price will increase for each day.\n### Lets validate our prediction with the actual results.","3d50c274":"### Right before we train our model we must split up the data into a train set and test set. However, due to the nature of time-series\u2019, we need to handle this part carefully. If we randomize our train-test set, we could encounter a look-ahead bias which is not good for predicting the stock market. It\u2019s caused when you train your model on data it would\u2019ve already seen.\n### To prevent this we are going to be training the model using a different technique called cross-validation. The image below illustrates how we are going to partition our data and test the accuracy of the model.","fa73b2ae":"### The list with the indicator symbols is useful to help use produce more features for our model.","43037419":"# <font color='red'> 2. Data Processing & Feature Engineering","c2fc4d05":"### We see that our data above is rough and contains lots of spikes for a time series. It isn\u2019t very smooth and can be difficult for the model to extract trends from. To reduce the appearance of this we want to exponentially smooth our data before we compute the technical indicators.","ce22fc85":"### As we can see from the actual results, we can confirm that the model was correct in all of its predictions. However there are many factors that go into determining the stock price, so to say that the model will produce similar results every time is naive. However, during relatively normal periods of time (without major panic that causes volatility in the market), the model should be able to produce good results.","ef727a67":"# <font color='red'> 4. Prediction","4c9ec383":"\n# <font color='red'> 1. Imports and Data Collection \n     ","98e298e0":"# <font color='red'> 5. Summary","4ccd1b17":"# <font color='red'> Python will make you rich in the stock market?\n### Is it Possible Stock Market Prediction?  Is it Possible Crypto Price Prediction?  Is it Possible Bitcoin Price Prediction?","2d2799f4":"### First, we\u2019re going to use multiple classifiers to create an ensemble model. The goal here is to combine the predictions of several models to try and improve on predictability. For each sub-model, we\u2019re also going to use a feature from Sklearn, GridSearchCV, to optimize each model for the best possible results.\n### First we create the random forest model."}}