{"cell_type":{"850a8a27":"code","52647472":"code","2143d878":"code","132950a1":"code","254605ef":"code","c41dcc9b":"code","24bda2d2":"code","f97d1b82":"code","c2c40aa5":"code","4f200cb8":"code","e56e60f4":"code","84d2e645":"code","dd8e29e2":"code","a8ffb391":"code","1ef17066":"code","469c4833":"code","a2f587f1":"markdown","e96a1b6a":"markdown"},"source":{"850a8a27":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport catboost as cat\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom category_encoders import OrdinalEncoder, LeaveOneOutEncoder\nfrom sklearn.model_selection import train_test_split\nimport os, sys, time\nfrom tqdm import tqdm_notebook\nfrom datetime import datetime\nfrom pytz import timezone\nprint('tic', datetime.now(timezone('Canada\/Pacific')).isoformat(timespec='minutes'))","52647472":"cpu = pd.read_csv('..\/input\/cpu-gpu\/cpu.csv')\ncpu['cpu_or_gpu'] = 'cpu'\ngpu = pd.read_csv('..\/input\/cpu-gpu\/gpu.csv')\ngpu['cpu_or_gpu'] = 'gpu'\ncpugpu = pd.concat([cpu, gpu])\ncpugpu.reset_index(inplace=True)\ncpugpu[['rows', 'cols']] = cpugpu[['rows', 'cols']].astype(int)\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\nsns.barplot(data=cpugpu.loc[cpugpu['lib']=='cat'], x='dataset', y='time', hue='cpu_or_gpu', ax=ax[0], palette='hot'); _ = ax[0].set_title('cat')\nsns.barplot(data=cpugpu.loc[cpugpu['lib']=='lgb'], x='dataset', y='time', hue='cpu_or_gpu', ax=ax[1], palette='hot'); _ = ax[1].set_title('lgb')\nsns.barplot(data=cpugpu.loc[cpugpu['lib']=='xgb'], x='dataset', y='time', hue='cpu_or_gpu', ax=ax[2], palette='hot'); _ = ax[2].set_title('xgb')","2143d878":"# Post-processing ends gracefully here.\nprint('toc', datetime.now(timezone('Canada\/Pacific')).isoformat(timespec='minutes') )\nexit()","132950a1":"dataX, dataX_ncoded, datay = {}, {}, {}\n\nfor dataset in ['jan', 'feb', 'mar']:\n    dataX[dataset] = pd.read_csv(f'..\/input\/tabular-playground-series-{dataset}-2021\/train.csv', index_col='id')\n    datay[dataset] = dataX[dataset].pop('target')\n\nfrom catboost.datasets import amazon\ndataX['amz'], _ = amazon()\ndatay['amz'] = dataX['amz'].pop('ACTION')\ndataX['amz'] = dataX['amz'].astype('object')\n\nfor dataset in dataX.keys():\n    print(dataset, dataX[dataset].shape, end=' ')\n    to_ord_encode = dataX[dataset].select_dtypes('object').columns.to_list()\n    to_loo_encode = []\n    for col in dataX[dataset].select_dtypes('object').columns:\n        if dataX[dataset][col].nunique()>256:  # LighGBM with GPU on can't take more than that\n            to_ord_encode.remove(col)\n            to_loo_encode.append(col)\n    if 'object' in dataX[dataset].dtypes.values:\n        tmp = dataX[dataset].copy()\n        tmp[to_ord_encode] = OrdinalEncoder().fit_transform(tmp[to_ord_encode]).astype('category')\n        tmp[to_loo_encode] = LeaveOneOutEncoder().fit_transform(tmp[to_loo_encode], datay[dataset])\n        dataX[dataset] = tmp\n        print(len(to_ord_encode), 'Ordinal, ', len(to_loo_encode), 'LeaveOneOut')\n    else:\n        print()","254605ef":"num_boost_rounds = 10000\nearly_stopping_rounds = 50\nparameters = {'cat': {}, 'lgb': {}, 'xgb': {}}\nscale_pos_weight = {}\nfor dataset in ['mar', 'amz']:\n    scale_pos_weight[dataset] = (datay[dataset]==0).sum() \/ (datay[dataset]==1).sum()\nprint(scale_pos_weight)\n\nif 'cudf' in sys.modules:\n     parameters['xgb']['tree_method'] = 'gpu_hist'\n     parameters['lgb']['device'] = 'gpu'\n     parameters['lgb']['gpu_use_dp'] = False\n     parameters['cat']['task_type'] = 'GPU'","c41dcc9b":"def catpython(theseparams, trainX, trainy, validX, validy):\n    param = parameters['cat'].copy()\n#   param.update({'learning_rate': .03})\n    if len(theseparams)>0:\n        param.update(theseparams)\n    cat_features = np.where(trainX.columns.isin(trainX.select_dtypes('category').columns))[0]\n    train_pool = cat.Pool(trainX, trainy, cat_features=cat_features)\n    valid_pool = cat.Pool(validX, validy, cat_features=cat_features)\n    model = cat.CatBoost(param).fit(train_pool, # plot = True,\n                                    eval_set              = [(validX, validy)],\n                                    early_stopping_rounds = early_stopping_rounds,\n                                    verbose               = num_boost_rounds\/\/10)\n    return model\n\ndef lgbpython(theseparams, trainX, trainy, validX, validy):\n    param = parameters['lgb'].copy()\n    if len(theseparams)>0:\n        param.update(theseparams)\n    trainSet = lgb.Dataset(trainX, trainy) \n    validSet = lgb.Dataset(validX, validy, reference=trainSet)\n    model = lgb.train(param, train_set      = trainSet,\n                      valid_sets            = [validSet], \n                      num_boost_round       = num_boost_rounds,\n                      early_stopping_rounds = early_stopping_rounds,\n                      verbose_eval          = num_boost_rounds\/\/10)\n    return model\n\ndef xgbpython(theseparams, trainX, trainy, validX, validy):\n    param = parameters['xgb'].copy()\n    if len(theseparams)>0:\n        param.update(theseparams)\n        \n    if len(trainX.select_dtypes('category').columns) >= 1:\n        trainSet = xgb.DMatrix(trainX, trainy, enable_categorical=True) \n        validSet = xgb.DMatrix(validX, validy, enable_categorical=True)\n    else:\n        trainSet = xgb.DMatrix(trainX, trainy) \n        validSet = xgb.DMatrix(validX, validy)\n    model = xgb.train(param, trainSet, \n                      evals                 = [(validSet, 'valid')], \n                      num_boost_round       = num_boost_rounds,\n                      early_stopping_rounds = early_stopping_rounds,\n                      verbose_eval          = num_boost_rounds\/\/10)\n    return model","24bda2d2":"gpu.divide(cpu)","f97d1b82":"my_log = pd.DataFrame(columns=['dataset', 'lib', 'time'])\n\ndef traintis(lib, dataset, param):\n    trainX, validX, trainy, validy = train_test_split(dataX[dataset], datay[dataset])\n    tic = time.time()\n    lib(param, trainX, trainy, validX, validy)\n    return time.time() - tic","c2c40aa5":"lib = 'cat'\ndataset = 'jan'\nmy_log.loc[f'{dataset}_{lib}'] = [dataset, lib, traintis(catpython, dataset, {'objective'       : 'RMSE', \n                                                                              'eval_metric'     : 'RMSE',\n                                                                              'iterations'      : num_boost_rounds})]\ndataset = 'feb'\nmy_log.loc[f'{dataset}_{lib}'] = [dataset, lib, traintis(catpython, dataset, {'objective'       : 'RMSE', \n                                                                              'eval_metric'     : 'RMSE',\n                                                                              'iterations'      : num_boost_rounds})]\ndataset = 'mar'\nmy_log.loc[f'{dataset}_{lib}'] = [dataset, lib, traintis(catpython, dataset, {'objective'       : 'Logloss', \n                                                                              'eval_metric'     : 'AUC',\n                                                                              'scale_pos_weight': scale_pos_weight['mar'],\n                                                                              'iterations'      : num_boost_rounds})]\ndataset = 'amz'\nmy_log.loc[f'{dataset}_{lib}'] = [dataset, lib, traintis(catpython, dataset, {'objective'       : 'Logloss', \n                                                                              'eval_metric'     : 'AUC',\n                                                                              'scale_pos_weight': scale_pos_weight['amz'],\n                                                                              'iterations'      : num_boost_rounds})]","4f200cb8":"lib = 'lgb'\ndataset = 'jan'\nmy_log.loc[f'{dataset}_{lib}'] = [dataset, lib, traintis(lgbpython, dataset, {'objective'   : 'regression', \n                                                                              'metric'      : 'rmse'})]\ndataset = 'feb'\nmy_log.loc[f'{dataset}_{lib}'] = [dataset, lib, traintis(lgbpython, dataset, {'objective'   : 'regression', \n                                                                              'metric'      : 'rmse'})]\ndataset = 'mar'\nmy_log.loc[f'{dataset}_{lib}'] = [dataset, lib, traintis(lgbpython, dataset, {'objective'   : 'binary', \n                                                                              'metric'      : 'auc',\n                                                                              'is_unbalance': True})]\ndataset = 'amz'\nmy_log.loc[f'{dataset}_{lib}'] = [dataset, lib, traintis(lgbpython, dataset, {'objective'   : 'binary', \n                                                                              'metric'      : 'auc',\n                                                                              'is_unbalance': True})]","e56e60f4":"lib = 'xgb'\ndataset = 'jan'\nmy_log.loc[f'{dataset}_{lib}'] = [dataset, lib, traintis(xgbpython, dataset, {'objective'       : 'reg:squarederror', \n                                                                              'eval_metric'     : 'rmse'})]\ndataset = 'feb'\nmy_log.loc[f'{dataset}_{lib}'] = [dataset, lib, traintis(xgbpython, dataset, {'objective'       : 'reg:squarederror', \n                                                                              'eval_metric'     : 'rmse'})]\ndataset = 'mar'\nmy_log.loc[f'{dataset}_{lib}'] = [dataset, lib, traintis(xgbpython, dataset, {'objective'       : 'binary:logistic', \n                                                                              'eval_metric'     : 'auc',\n                                                                              'scale_pos_weight': scale_pos_weight['mar']})]\ndataset = 'amz'\nmy_log.loc[f'{dataset}_{lib}'] = [dataset, lib, traintis(xgbpython, dataset, {'objective'       : 'binary:logistic', \n                                                                              'eval_metric'     : 'auc',\n                                                                              'scale_pos_weight': scale_pos_weight['amz']})]","84d2e645":"sns.barplot(data=my_log, x='dataset', y='time', hue='lib', palette='hot')","dd8e29e2":"for dataset in dataX.keys():\n    my_log.loc[my_log['dataset']==dataset, 'rows'] = dataX[dataset].shape[0]\n    my_log.loc[my_log['dataset']==dataset, 'cols'] = dataX[dataset].shape[1]","a8ffb391":"my_log.to_csv('my_log.csv')","1ef17066":"print('toc', datetime.now(timezone('Canada\/Pacific')).isoformat(timespec='minutes') )","469c4833":"pd.read_csv('my_log.csv')","a2f587f1":"# Post-processing\nThis section post-processes output files from Version 7 and Version 8 of this notebook. \n* Version 7 and Version 8 of this notebook ran on exactly the same code. \n* Version 7 was run with Settings > Accelerator > None. Output file was downloaded; reloading here as ..\/input\/cpu-gpu\/cpu.csv.\n* Version 8 was run with Settings > Accelerator > GPU. Output file was downloaded; reloading here as ..\/input\/cpu-gpu\/gpu.csv.","e96a1b6a":"Don't we often hear the question, \"Does GPU really speed up LightGBM?\" Indeed, even with the confirmation:\n* With ```device=\"gpu\"```\n* [LightGBM] [Info] This is the GPU trainer!!\n* [LightGBM] [Info] Using GPU Device: Tesla P100-PCIE-16GB, Vendor: NVIDIA Corporation\n* [LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n\nwe still find it running quicker *without* GPU.\n\nThis notebook tests training with and without GPU using 3 libraries:\n* CatBoost;\n* LightGBM;\n* XGBoost.\n\non 4 datasets:\n* January tabular playground;\n* February tabular playground;\n* March tabular playground;\n* catBoost.datasets.amazon.\n\nFor cases tested here, it is found that with GPU turned on:\n* LightGBM never ran quicker, in fact most of the time slower;\n* XGBoost always ran quicker; \n* CatBoost ran quicker on all 3 tabular playgrounds."}}