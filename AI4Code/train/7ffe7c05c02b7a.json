{"cell_type":{"4201af60":"code","10ce445a":"code","72e23c79":"code","f1fbf0f7":"code","a42d820c":"code","43cc95da":"code","5a6c93b5":"code","0abe8989":"code","27af2640":"code","04f30243":"code","9f2ad114":"code","bb92bd1d":"code","ff324335":"code","cdbd28f3":"code","4f85954d":"markdown","98fe4973":"markdown","837778c1":"markdown","37c85aac":"markdown","48424dc5":"markdown","e5076f21":"markdown","bc856b5c":"markdown","07fa7e2f":"markdown","1300e4f5":"markdown","0c4af4bd":"markdown","b5e68692":"markdown","b4cb48ff":"markdown","b44250e7":"markdown","2af0bb61":"markdown","269a6d59":"markdown"},"source":{"4201af60":" # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.models import Model\nfrom matplotlib import pyplot\nfrom numpy import expand_dims\nfrom matplotlib import pyplot\n\nimport warnings\nwarnings.filterwarnings('ignore')","10ce445a":"#Load the model\nmodel = VGG16()\n\n# Summary of the model\nmodel.summary()","72e23c79":"for layer in model.layers:\n    \n    if 'conv' not in layer.name:\n        continue    \n    filters , bias = layer.get_weights()\n    print(layer.name , filters.shape)","f1fbf0f7":"# retrieve weights from the second hidden layer\nfilters , bias = model.layers[1].get_weights()","a42d820c":"# normalize filter values to 0-1 so we can visualize them\nf_min, f_max = filters.min(), filters.max()\nfilters = (filters - f_min) \/ (f_max - f_min)","43cc95da":"n_filters =6\nix=1\nfig = pyplot.figure(figsize=(20,15))\nfor i in range(n_filters):\n    # get the filters\n    f = filters[:,:,:,i]\n    for j in range(3):\n        # subplot for 6 filters and 3 channels\n        pyplot.subplot(n_filters,3,ix)\n        pyplot.imshow(f[:,:,j] ,cmap='gray')\n        ix+=1\n#plot the filters \npyplot.show()","5a6c93b5":"for i in range(len(model.layers)):\n    layer = model.layers[i]\n    if 'conv' not in layer.name:\n        continue    \n    print(i , layer.name , layer.output.shape)","0abe8989":"model = Model(inputs=model.inputs , outputs=model.layers[1].output)","27af2640":"image = load_img(\"..\/input\/emma_watson.jpg\" , target_size=(224,224))\n\n# convert the image to an array\nimage = img_to_array(image)\n# expand dimensions so that it represents a single 'sample'\nimage = expand_dims(image, axis=0)","04f30243":"image = preprocess_input(image)","9f2ad114":"#calculating features_map\nfeatures = model.predict(image)\n\nfig = pyplot.figure(figsize=(20,15))\nfor i in range(1,features.shape[3]+1):\n\n    pyplot.subplot(8,8,i)\n    pyplot.imshow(features[0,:,:,i-1] , cmap='gray')\n    \npyplot.show()","bb92bd1d":"model2 = VGG16()","ff324335":"blocks = [ 2, 5 , 9 , 13 , 17]\noutputs = [model2.layers[i].output for i in blocks]\n\nmodel2 = Model( inputs= model2.inputs, outputs = outputs)","cdbd28f3":"feature_map = model2.predict(image)\n\nfor i,fmap in zip(blocks,feature_map):\n    fig = pyplot.figure(figsize=(20,15))\n    #https:\/\/stackoverflow.com\/a\/12444777\n    fig.suptitle(\"BLOCK_{}\".format(i) , fontsize=20)\n    for i in range(1,features.shape[3]+1):\n\n        pyplot.subplot(8,8,i)\n        pyplot.imshow(fmap[0,:,:,i-1] , cmap='gray')\n    \npyplot.show()","4f85954d":"We can use this information and design a new model that is a subset of the layers in the full VGG16 model. The model would have the same input layer as the original model, but the output would be the output of a given convolutional layer, which we know would be the activation of the layer or the feature map.","98fe4973":"We see each layer has 3x3 filters.\n\nWe can see that for the input image with three channels for red, green and blue, that each filter has a depth of three (here we are working with a channel-last format). We could visualize one filter as a plot with three images, one for each channel, or compress all three down to a single color image, or even just look at the first channel and assume the other channels will look the same. The problem is, we then have 63 other filters that we might like to visualize.\n\nWe can retrieve the filters from the first layer as follows:","837778c1":"We will be visualising 6 filters out of 64 filters from 1st layer.","37c85aac":"# Visualizing Convolutional Layers\nNeural network models are generally referred to as being opaque. This means that they are poor at explaining the reason why a specific decision or prediction was made.\n\nConvolutional neural networks are designed to work with image data, and their structure and function suggest that should be less inscrutable than other types of neural networks.\n\nSpecifically, the models are comprised of small linear filters and the result of applying filters called activation maps, or more generally, feature maps.\n\nBoth filters and feature maps can be visualized.\n\nFor example, we can design and understand small filters, such as line detectors. Perhaps visualizing the filters within a learned convolutional neural network can provide insight into how the model works.\n\nThe feature maps that result from applying filters to input images and to feature maps output by prior layers could provide insight into the internal representation that the model has of a specific input at a given point in the model.","48424dc5":"Now we will visualise features of other layers.","e5076f21":"# Prefit the VGG-MODEL\n\nWe will be using a pre-trained model available in the Keras Framework. There are a lot of CNN models but we will be using VGG model.\nIt is deep with 16 learned layers, and it performed very well, meaning that the filters and resulting feature maps will capture useful features. For more information on this model, see the 2015 paper [\u201cVery Deep Convolutional Networks for Large-Scale Image Recognition.\u201d](https:\/\/arxiv.org\/abs\/1409.1556)","bc856b5c":"Making a prediction with this model will give the feature map for the first convolutional layer for a given provided input image. Let\u2019s implement this.\n\nAfter defining the model, we need to load the image with the size expected by the model, in this case, 224\u00d7224.","07fa7e2f":"We are now ready to get the feature map. We can do this easy by calling the model.predict() function and passing in the prepared single image.\nWe know the result will be a feature map with 224x224x64. We can plot all 64 two-dimensional images as an 8\u00d78 square of images.","1300e4f5":"The pixel values then need to be scaled appropriately for the VGG model.","0c4af4bd":"# HOW TO VISUALIZE \n\nIn neural network terminology, the learned filters are simply weights, yet because of the specialized two-dimensional structure of the filters, the weight values have a spatial relationship to each other and plotting each filter as a two-dimensional image is meaningful (or could be).\n\nThe model summary printed in the previous section summarizes the output shape of each layer, e.g. the shape of the resulting feature maps. It does not give any idea of the shape of the filters (weights) in the network, only the total number of weights per layer.\n\nWe can access all of the layers of the model via the **model.layers** property.\n\nEach layer has a layer.name property, where the convolutional layers have a naming convolution like **block#_conv#**, where the \u2018#\u2018 is an integer. Therefore, we can check the name of each layer and skip any that don\u2019t contain the string \u2018conv\u2018.\n\n* Each convolutional layer has two sets of weights.\n\nOne is the block of filters and the other is the block of bias values. These are accessible via the **layer.get_weights()** function. We can retrieve these weights and then summarize their shape.\n","b5e68692":"The weight values will likely be small positive and negative values centered around 0.0.\n\nWe can normalize their values to the range 0-1 to make them easy to visualize.","b4cb48ff":"Before proceeding I would like to thank to [**JASON BROWNLEE**](https:\/\/machinelearningmastery.com) who's blogs helped me to write this tutorial\n\nConvolutional neural networks, have internal structures that are designed to operate upon two-dimensional image data, and as such preserve the spatial relationships for what was learned by the model. Specifically, the two-dimensional filters learned by the model can be inspected and visualized to discover the types of features that the model will detect, and the activation maps output by convolutional layers can be inspected to understand exactly what features were detected for a given input image.\n\nAfter completing this tutorial, you will know:\n\n* How to develop a visualization for specific filters in a convolutional neural network.\n* How to develop a visualization for specific feature maps in a convolutional neural network.\n* How to systematically visualize feature maps for each block in a deep convolutional neural network.","b44250e7":"# How to Visualize Feature Maps\n\nThe activation maps, called feature maps, capture the result of applying the filters to input, such as the input image or another feature map.\n\nThe idea of visualizing a feature map for a specific input image would be to understand what features of the input are detected or preserved in the feature maps. The expectation would be that the feature maps close to the input detect small or fine-grained detail, whereas feature maps close to the output of the model capture more general features.\n\nFor the input I will be using an image of my childhood crush Hermoine Granger(Emma Watson).\n\n### Note:\n\nwe need a clearer idea of the shape of the feature maps output by each of the convolutional layers and the layer index number so that we can retrieve the appropriate layer output.","2af0bb61":"We can see that in some cases, the filter is the same across the channels (the first row), and in others, the filters differ (the last row).\n\nThe dark squares indicate small or inhibitory weights and the light squares represent large or excitatory weights. Using this intuition, we can see that the filters on the first row detect a gradient from light in the top left to dark in the bottom right.\n\n### Note:\n* Although we have a visualization, we only see the first six of the 64 filters in the first convolutional layer. Visualizing all 64 filters in one image is feasible.\n\n* Sadly, this does not scale; if we wish to start looking at filters in the second convolutional layer, we can see that again we have 64 filters, but each has 64 channels to match the input feature maps. To see all 64 channels in a row for all 64 filters would require (64\u00d764) 4,096 subplots in which it may be challenging to see any detail.","269a6d59":"# Tutorial Overview\n\n1. Visualizing Convolutional Layers\n2. Pre-fit VGG Model\n3. How to Visualize Filters\n4. How to Visualize Feature Maps"}}