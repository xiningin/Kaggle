{"cell_type":{"41d29be3":"code","7caf142a":"code","e7e55ff6":"code","37ede87e":"code","b9175703":"code","204e3d9a":"code","353eedc2":"code","aaeac887":"code","7215854f":"code","0d3a9545":"code","da7cf512":"code","a9c039fa":"code","0527766a":"code","0cce0103":"code","482f124f":"code","96b5c85f":"code","9b082e15":"markdown","f6f3c846":"markdown","e4dfd4d5":"markdown","31f2f1a0":"markdown","3f7395d5":"markdown","27d35e64":"markdown","4d57511f":"markdown","5f8402e8":"markdown","435de734":"markdown","84c9c462":"markdown","edf3ce22":"markdown","ac9ee8b6":"markdown"},"source":{"41d29be3":"!pip install fastai2","7caf142a":"from fastai.vision import *\nfrom fastai.metrics import error_rate","e7e55ff6":"bs = 64 #batch-size\n\nimport cv2\n\ndata = \"\/kaggle\/input\/flowers-recognition\/flowers\/\"\nfolders = os.listdir(data)\nprint(folders)\n\nimage_names = []\ntrain_labels = []\ntrain_images = []\n\nsize = 64,64\n\nfor folder in folders:\n    for file in os.listdir(os.path.join(data,folder)):\n        if file.endswith(\"jpg\"):\n            image_names.append(os.path.join(data,folder,file))\n            train_labels.append(folder)\n            img = cv2.imread(os.path.join(data,folder,file))\n            im = cv2.resize(img,size)\n            train_images.append(im)\n        else:\n            continue","37ede87e":"tfms = get_transforms(do_flip = True, flip_vert = True, max_rotate = 30, max_zoom = 1.2, p_affine = 0.5)\ndata1 = ImageDataBunch.from_lists('',image_names, labels=train_labels, ds_tfms=tfms, size=224, bs=bs).normalize(imagenet_stats)\ndata1.classes","b9175703":"data1.show_batch(rows=3, figsize=(7,6))","204e3d9a":"print(data1.classes)\nlen(data1.classes),data1.c","353eedc2":"learn = cnn_learner(data1, models.resnet50, metrics=error_rate)","aaeac887":"learn.fit_one_cycle(4)","7215854f":"interp = ClassificationInterpretation.from_learner(learn)\n\nlosses,idxs = interp.top_losses()","0d3a9545":"interp.plot_top_losses(9, figsize=(15,11))","da7cf512":"interp.plot_confusion_matrix(figsize=(12,12), dpi=60)","a9c039fa":"learn.lr_find()","0527766a":"learn.recorder.plot()","0cce0103":"learn.unfreeze()\nlearn.fit_one_cycle(4, max_lr=slice(1e-6, 1e-4))","482f124f":"interp = ClassificationInterpretation.from_learner(learn)\n\nlosses,idxs = interp.top_losses()\n\ninterp.plot_top_losses(9, figsize=(15,11))","96b5c85f":"interp.plot_confusion_matrix(figsize=(12,12), dpi=60)","9b082e15":"Visualizing the data and the class size.","f6f3c846":"We can see what we have achieved now.","e4dfd4d5":"Using *Confusion Matrix* to visualize loss and prediction error.\n<br> The model makes the same mistakes over and over again but it rarely confuses other categories. ","31f2f1a0":"# Fine Tuning\n.\nWe use *lr_find* to find the best learning rate and plot it using a line graph.","3f7395d5":"# Setting up the environment\n\nIntall fast.ai","27d35e64":"# Results\n\nOur accuracy has increased significantly. <br>\nThis a pretty good model.","4d57511f":"# Getting Started\n\nWe need to read the data and extract the labels from the folder name. <br>\nWe created three lists, one each for the entire path per image, labels per image, and corresponding image name.  ","5f8402e8":"Using the plot above we have to use the steepish slope in the graph, just before the lowest learning rate. \n\n<br>*Not the lowest point.*\n\n<br> This is because the model needs to find that learning rate where it learns the most.\n\n<br>We will unfreeze our model and train some more.","435de734":"# Training the Model\n\nUsing *cnn_learner* we define our Convolutional Neural Network of ResNet50 architechture. ResNet50 has a convolutional neural network backbone and a fully connected head with a single hidden layer as a classifier. ","84c9c462":"We will train for 4 epochs (4 cycles through all our data).","edf3ce22":"# Augmentation and Visualization\n\nData Augmentation using *get_transforms* method. <br>\nCreate an *ImageDataBunch* object from the list of paths of the images.","ac9ee8b6":"Import the vision library and the required metrics! <br>\nFast.ai recommends importing the entire library."}}