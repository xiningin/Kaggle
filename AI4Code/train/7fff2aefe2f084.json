{"cell_type":{"5b881569":"code","4d71f9df":"code","0f654c18":"code","20f6a470":"code","f7ea830d":"code","2a8f63c5":"code","09f90c12":"code","4ce79a1e":"code","2f671a58":"code","e2ddd5a0":"code","ca2fc371":"code","b403c398":"code","6cccfae0":"markdown","c3f56287":"markdown","0bae1de1":"markdown","13b09bbf":"markdown","9ddf2926":"markdown","c71728a3":"markdown","fdf445e3":"markdown","1e0ef479":"markdown"},"source":{"5b881569":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras.callbacks import ModelCheckpoint","4d71f9df":"dataset = pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\nnegative = len(dataset[dataset['sentiment']=='positive'])\npositive = len(dataset) - negative\nsns.countplot(dataset['sentiment'])\nprint('Positive reviews are {} and negative reviews are {} of total {} '.format(positive,negative,len(dataset)))","0f654c18":"le = LabelEncoder()\ntraining_reviews,testing_reviews,training_labels,testing_labels  = train_test_split(dataset['review'].values,dataset['sentiment'].values,test_size = 0.2)\ntraining_labels = le.fit_transform(training_labels)\ntesting_labels = le.fit_transform(testing_labels)","20f6a470":"tokenizer = Tokenizer(num_words=10000,oov_token='<OOV>')\ntokenizer.fit_on_texts(training_reviews)\nword_index = tokenizer.word_index\ntraining_sequence = tokenizer.texts_to_sequences(training_reviews)\ntesting_sequence = tokenizer.texts_to_sequences(testing_reviews)\ntrain_pad_sequence = pad_sequences(training_sequence,maxlen = 200,truncating= 'post',padding = 'pre')\ntest_pad_sequence = pad_sequences(testing_sequence,maxlen = 200,truncating= 'post',padding = 'pre')\nprint('Total Unique Words : {}'.format(len(word_index)))","f7ea830d":"embedded_words = {}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt') as file:\n    for line in file:\n        words, coeff = line.split(maxsplit=1)\n        coeff = np.array(coeff.split(),dtype = float)\n        embedded_words[words] = coeff","2a8f63c5":"embedding_matrix = np.zeros((len(word_index) + 1,200))\nfor word, i in word_index.items():\n    embedding_vector = embedded_words.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","09f90c12":"model = tf.keras.Sequential([tf.keras.layers.Embedding(len(word_index) + 1,200,weights=[embedding_matrix],input_length=200,\n                            trainable=False),\n                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n                             tf.keras.layers.Dropout(0.8),\n                             tf.keras.layers.Dense(256,activation = 'relu',),\n                             tf.keras.layers.Dense(128,activation = 'relu'),\n                             tf.keras.layers.Dropout(0.8),\n                             tf.keras.layers.Dense(1,activation = tf.nn.sigmoid)])\nmcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')","4ce79a1e":"model.summary()","2f671a58":"model.compile(loss = tf.keras.losses.BinaryCrossentropy() , optimizer='Adam' , metrics = 'accuracy')\nhistory = model.fit(train_pad_sequence,training_labels,epochs = 30, validation_data=(test_pad_sequence,testing_labels),\n                   callbacks=[mcp_save])","e2ddd5a0":"tf.keras.Model.save_weights(model, filepath='weight.hdf5')","ca2fc371":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend(loc=0)\n\nplt.show()","b403c398":"print('Training Accuracy: {}'.format(max(acc)))\nprint('Validation Accuracy: {}'.format(max(val_acc)))","6cccfae0":"# Plotting Accuracy and Losses","c3f56287":"# Pre-Processing The Text\nUsing tokenizer to produce token for a given word and taking maximum length of 200 character of a review and after we simply truncate the input review and then padded the input to max len of 200. ","0bae1de1":"# Using glove vectors for embedding","13b09bbf":"# Creating The Model\nlayer1: Embedding Layer using glove weights \n\nlayer2: Using a Bidirectional LSTM\n\nlayer3: A dropout Layer\n\nlayer4: A Dense layer of 256 neurons with 'relu' activation\n\nlayer5: A Dense Layer of 128 neurons with 'relu' activation\n\nlayer6: Again a dropout layer. \n\nlayer7: Sigmoid activation layer to classify it positive and negative.\n","9ddf2926":"# Conclusion\n\n1 - We have great accuracy and we can increase it training for much longer and tune other hyperparameters\n2 - DNN LSTM have a deep impact on NLP problems and we can see that this model performs quite well.","c71728a3":"# Visualizing Data\nAs we can see our data is distributed evenly 25k positive reviews and 25k negative reviews count plot is shown in the figure.","fdf445e3":"# Importing necessary libraries\n","1e0ef479":"Converting the labels positve and negative as 1,0 so that they can be fed to the neural network to predict whether the given review is a positive or negative. Splitting of data 80% for the training and remaining 20% for testing."}}