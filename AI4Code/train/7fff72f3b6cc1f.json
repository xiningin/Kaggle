{"cell_type":{"d51e9e9b":"code","f392b356":"code","011d5a93":"code","a032428a":"code","1e6a4de7":"code","cf3d855f":"code","59e20d6b":"code","f058da4e":"code","a87c1118":"code","53cd148a":"code","97b0d2a0":"code","8ae543c0":"code","33705357":"code","9903a2ec":"code","59492cff":"code","f141d4b2":"code","0505dbcc":"code","857332f5":"code","04f0ff52":"code","3c0b2438":"code","07814db4":"code","f87933b7":"code","1505175e":"code","b6d5a422":"code","a7c78986":"code","71aafb49":"code","54ad6264":"code","2bcafe19":"code","7d162dcb":"code","0d7c27c0":"code","1f19cd28":"code","c429da88":"markdown","a47bd746":"markdown","61e0f6d0":"markdown","8f0a9da6":"markdown","476356c5":"markdown","dd3ff248":"markdown","9a15ca36":"markdown","7922db16":"markdown","1fb17a89":"markdown","0dcf7a3e":"markdown","e6f33369":"markdown","8c9d728d":"markdown","9ee8aeee":"markdown","261a6c3d":"markdown","2384746f":"markdown","099606d1":"markdown","8ae49e18":"markdown"},"source":{"d51e9e9b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f392b356":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns # statistical data visualization\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier","011d5a93":"df = pd.read_csv(\"..\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")\ndf.head()","a032428a":"categorical = [var for var in df.columns if df[var].dtype=='O']\nprint('There are {} categorical variables\\n'.format(len(categorical)))\nprint('The categorical variables are :', categorical)","1e6a4de7":"cat1 = [var for var in categorical if df[var].isnull().sum()!=0]\nprint(df[cat1].isnull().sum())","cf3d855f":"for var in categorical:\n    print(var + ' conatins '+str(len(df[var].unique()))+ \" labels \")","59e20d6b":"df['Date'] = pd.to_datetime(df['Date'])\ndf['Year'] = df['Date'].dt.year\ndf['Month'] = df['Date'].dt.month\ndf['Day'] = df['Date'].dt.day\n\ndf.drop('Date',axis=1,inplace=True)","f058da4e":"categorical = [var for var in df.columns if df[var].dtype=='O']\nprint(\"There are {} categorical variables : \".format(len(categorical)))\nprint(categorical)","a87c1118":"for var in categorical:\n    df[var].fillna(df[var].mode()[0],inplace=True)","53cd148a":"numerical = [var for var in df.columns if df[var].dtype!='O']\nprint(numerical)","97b0d2a0":"num1 = df[numerical].isnull().sum()\nnum1 = num1[num1!=0]\nnum1","8ae543c0":"for col in num1.index:\n    col_mean = df[col].mean()\n    df[col].fillna(col_mean,inplace=True)","33705357":"le = LabelEncoder()\nnew_df = df\nfor col in categorical:\n    new_df[col] = le.fit_transform(df[col])\ncol_names = new_df.columns","9903a2ec":"new_df.head()","59492cff":"from sklearn.preprocessing import MinMaxScaler\nss = MinMaxScaler()\nnew_df = ss.fit_transform(new_df)\nnew_df = pd.DataFrame(new_df,columns = col_names )","f141d4b2":"new_df.describe()","0505dbcc":"# new_df.to_csv(\"weatherCleaned.csv\")","857332f5":"correlation = new_df.corr()\nplt.figure(figsize=(16,12))\nplt.title('Correlation Heatmap of Rain in Australia Dataset')\nax = sns.heatmap(correlation, square=True, annot=True, fmt='.2f', linecolor='white',cmap='viridis')\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\nax.set_yticklabels(ax.get_yticklabels(), rotation=30)           \nplt.show()","04f0ff52":"y = new_df.RainTomorrow\nX = new_df.drop('RainTomorrow',axis=1)","3c0b2438":"results = []","07814db4":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42,shuffle=True)","f87933b7":"gnb = GaussianNB()\ngnb.fit(X_train, y_train)\ny_pred = gnb.predict(X_test)\ngnb.score(X_test,y_test)","1505175e":"print(accuracy_score(y_test,y_pred))\nprint(cross_val_score(gnb,X_train,y_train,cv=3))\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\nresults.append(accuracy_score(y_test,y_pred))\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True,annot_kws={\"size\": 12},cmap='viridis',fmt=\"d\")","b6d5a422":"dtc = DecisionTreeClassifier(max_depth=10, min_samples_split=2,random_state=42)\ndtc.fit(X_train,y_train)\ny_pred = dtc.predict(X_test)\ndtc.score(X_test,y_test)","a7c78986":"print(accuracy_score(y_test,y_pred))\nprint(cross_val_score(dtc,X_train,y_train,cv=3))\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\nresults.append(accuracy_score(y_test,y_pred))\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True,annot_kws={\"size\": 12},cmap='viridis',fmt=\"d\")","71aafb49":"svc = LinearSVC(random_state=42)\nsvc.fit(X_train,y_train)\ny_pred = svc.predict(X_test)\nsvc.score(X_test,y_test)\nprint(cross_val_score(svc,X_train,y_train,cv=3))","54ad6264":"print(accuracy_score(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\nresults.append(accuracy_score(y_test,y_pred))\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True,annot_kws={\"size\": 12},cmap='viridis',fmt=\"d\")","2bcafe19":"rfc = RandomForestClassifier(n_estimators=200,max_depth=10, random_state=42)\nrfc.fit(X_train,y_train)\ny_pred = rfc.predict(X_test)\nrfc.score(X_test,y_test)","7d162dcb":"print(accuracy_score(y_test,y_pred))\nprint(cross_val_score(rfc,X_train,y_train,cv=3))\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\nresults.append(accuracy_score(y_test,y_pred))\nsns.heatmap(confusion_matrix(y_test,y_pred),annot=True,annot_kws={\"size\": 12},cmap='viridis',fmt=\"d\")","0d7c27c0":"names = [\"Naive Bayes\",\"Decision Tree\",\"Linear SVM\",\"Random Forest\",]\nresults","1f19cd28":"sns.barplot(names,results)","c429da88":"## Applying various classifying algorithms on the training set and predicting the RainTomorrow using training set.\n\n### 1.1 Gaussian Naive Bayes","a47bd746":"## Comaprison of Various Classifying algorithms","61e0f6d0":"### Splitting the Date column into respective 'Year','Month' & 'Day'.**","8f0a9da6":"## Data Cleaning\n\n### Handling Missing Values in Categorical Columns","476356c5":"### Replacing the missing categorical values by the most frequent value in respective columns. ","dd3ff248":"## Feature Scaling using MinMaxScaler","9a15ca36":"## 1.4 Random Forest","7922db16":"## Observations :\n\n>> ### Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. \n>> ### The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n>> ### DecisionTreeClassifier is capable of both binary classification and multiclass classification.\n>> ### The confusion matrix shows that there are 0 FP or FN.\n>> ### Hence, the DecisionTreeClassifier is able to predict rain tomorrow with an impressive accuracy of 100%.","1fb17a89":"## Splitting into training and testing sets","0dcf7a3e":"### Replacing the missing numercial values by the mean of their respective columns.","e6f33369":"## 1.2 Decision Trees","8c9d728d":"## Observations :\n>> ### Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n>> ###  LinearSVC is another implementation of Support Vector Classification for the case of a linear kernel.\n>> ### This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.\n>> ### From the confusion matrix it is evident that 18 are FP and 2963 are FN.\n>> ### Therby, the LinearSVC is able to predict rain tomorrow with 94.75% accuracy.\n\n","9ee8aeee":"## Data Visualization\n\n### Heatmap of correlation among the columns of data.","261a6c3d":"## Observations :\n>> ### GaussianNB implements gaussian naive bayes algorithm for classification.\n>> ### It assumes the maximum likelihood of the features to be Gaussian and classifies the dataset accordingly.\n>> ### The confusion matrix depicts that 2861 are False Positives and 6 are False Negatives.\n>> ### Thus, Gaussian Naive Bayes algorithm is able to predict rain tommorrow with accuracy of 94.95%. ","2384746f":"# Conclusion :\n\n>> ### The Decison Tree Algorithm outperforms other algorithms in terms of precison,accuracy and recall.\n>> ### Also,LinearSVM is the lowest in terms of accuracy.\n>> ### Gaussian Naive Bayes performs well in case of binary classification.\n>> ### Thus, Random Forest and Decision Trees are best suited for binary classification problems.","099606d1":"## Observations :\n>> ### In random forests (RandomForestClassifier and RandomForestRegressor classes), each tree in the ensemble is built from a sample drawn with replacement.\n>> ### A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n>> ### The confusion matrix depicts that there are only 4 FN.\n>> ### Hence, the RandomForestClassifier is able to predict rain tomorrow with 99.99% accuracy.","8ae49e18":"## 1.3 Support Vector Machines"}}