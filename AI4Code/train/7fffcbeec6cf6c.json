{"cell_type":{"33ac6312":"code","e93dcd1a":"code","aca6ee23":"code","99a2bbfd":"code","92082636":"code","30d6bd07":"code","d1e23c4a":"code","c494d6d0":"code","4f837207":"code","f7472fe0":"code","b1efd3ab":"code","3f579e5b":"code","7ba53bb4":"code","c5eabe4b":"code","306f480f":"code","69edc882":"code","a571cbd4":"code","0e0a02f8":"code","ef9c11f9":"code","754fdafa":"markdown","5bebbdb7":"markdown","86448e7d":"markdown","592d41c3":"markdown","06ff9bb0":"markdown","29ac0a3a":"markdown","ffe6505c":"markdown","19de6787":"markdown","893b875b":"markdown","8279a3a4":"markdown","cdc122d4":"markdown"},"source":{"33ac6312":"from catboost import CatBoostClassifier\nfrom joblib import dump\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nimport warnings","e93dcd1a":"!pip install kaggler","aca6ee23":"import kaggler\nfrom kaggler.model import AutoLGB\nfrom kaggler.preprocessing import LabelEncoder\n\nprint(f'Kaggler: {kaggler.__version__}')","99a2bbfd":"warnings.simplefilter('ignore')\npd.set_option('max_columns', 100)","92082636":"data_dir = Path('\/kaggle\/input\/tabular-playground-series-apr-2021\/')\ntrn_file = data_dir \/ 'train.csv'\ntst_file = data_dir \/ 'test.csv'\nsample_file = data_dir \/ 'sample_submission.csv'\npseudo_label_file = '..\/input\/tps-apr-2021-pseudo-label-dae\/REMEK-TPS04-FINAL005.csv'\ndae_feature_file = '\/kaggle\/input\/tps-apr-2021-pseudo-label-dae\/dae.csv'\nlgb_dae_predict_val_file = '\/kaggle\/input\/tps-apr-2021-pseudo-label-dae\/lgb_dae.val.txt'\nlgb_dae_predict_tst_file = '\/kaggle\/input\/tps-apr-2021-pseudo-label-dae\/lgb_dae.tst.txt'\nlgb_dae_te_predict_val_file = '\/kaggle\/input\/tps-apr-2021-pseudo-label-dae\/lgb_dae_te.val.txt'\nlgb_dae_te_predict_tst_file = '\/kaggle\/input\/tps-apr-2021-pseudo-label-dae\/lgb_dae_te.tst.txt'\nsdae_dae_predict_val_file = '\/kaggle\/input\/tps-apr-2021-pseudo-label-dae\/sdae_dae.val.txt'\nsdae_dae_predict_tst_file = '\/kaggle\/input\/tps-apr-2021-pseudo-label-dae\/sdae_dae.tst.txt'\n\ntarget_col = 'Survived'\nid_col = 'PassengerId'\n\nfeature_name = 'dae'\nalgo_name = 'esb'\nmodel_name = f'{algo_name}_{feature_name}'\n\nfeature_file = f'{feature_name}.csv'\npredict_val_file = f'{model_name}.val.txt'\npredict_tst_file = f'{model_name}.tst.txt'\nsubmission_file = f'{model_name}.sub.csv'","30d6bd07":"n_fold = 5\nseed = 42\nn_est = 1000\nencoding_dim = 128","d1e23c4a":"trn = pd.read_csv(trn_file, index_col=id_col)\ntst = pd.read_csv(tst_file, index_col=id_col)\nsub = pd.read_csv(sample_file, index_col=id_col)\npseudo_label = pd.read_csv(pseudo_label_file, index_col=id_col)\ndae_features = np.loadtxt(dae_feature_file, delimiter=',')\nlgb_dae_predict_val = np.loadtxt(lgb_dae_predict_val_file)\nlgb_dae_predict_tst = np.loadtxt(lgb_dae_predict_tst_file)\nlgb_dae_te_predict_val = np.loadtxt(lgb_dae_te_predict_val_file)\nlgb_dae_te_predict_tst = np.loadtxt(lgb_dae_te_predict_tst_file)\nsdae_dae_predict_val = np.loadtxt(sdae_dae_predict_val_file)\nsdae_dae_predict_tst = np.loadtxt(sdae_dae_predict_tst_file)\n\nprint(trn.shape, tst.shape, sub.shape, pseudo_label.shape, dae_features.shape)\nprint(lgb_dae_predict_val.shape, lgb_dae_predict_tst.shape)\nprint(lgb_dae_te_predict_val.shape, lgb_dae_te_predict_tst.shape)\nprint(sdae_dae_predict_val.shape, sdae_dae_predict_tst.shape)","c494d6d0":"tst[target_col] = pseudo_label[target_col]\nn_trn = trn.shape[0]\ndf = pd.concat([trn, tst], axis=0)\ndf.head()","4f837207":"df_dae = pd.DataFrame(dae_features, columns=[f'enc_{x}' for x in range(encoding_dim)])\nprint(df_dae.shape)\ndf_dae.head()","f7472fe0":"# Feature engineering code from https:\/\/www.kaggle.com\/udbhavpangotra\/tps-apr21-eda-model\n\ndf['Embarked'] = df['Embarked'].fillna('No')\ndf['Cabin'] = df['Cabin'].fillna('_')\ndf['CabinType'] = df['Cabin'].apply(lambda x:x[0])\ndf.Ticket = df.Ticket.map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X')\n\ndf['Age'].fillna(round(df['Age'].median()), inplace=True,)\ndf['Age'] = df['Age'].apply(round).astype(int)\n\n# Fare, fillna with mean value\nfare_map = df[['Fare', 'Pclass']].dropna().groupby('Pclass').median().to_dict()\ndf['Fare'] = df['Fare'].fillna(df['Pclass'].map(fare_map['Fare']))\n\ndf['FirstName'] = df['Name'].str.split(', ').str[0]\ndf['SecondName'] = df['Name'].str.split(', ').str[1]\n\ndf['n'] = 1\n\ngb = df.groupby('FirstName')\ndf_names = gb['n'].sum()\ndf['SameFirstName'] = df['FirstName'].apply(lambda x:df_names[x]).fillna(1)\n\ngb = df.groupby('SecondName')\ndf_names = gb['n'].sum()\ndf['SameSecondName'] = df['SecondName'].apply(lambda x:df_names[x]).fillna(1)\n\ndf['Sex'] = (df['Sex'] == 'male').astype(int)\n\ndf['FamilySize'] = df.SibSp + df.Parch + 1\n\nfeature_cols = ['Pclass', 'Age','Embarked','Parch','SibSp','Fare','CabinType','Ticket','SameFirstName', 'SameSecondName', 'Sex',\n                'FamilySize', 'FirstName', 'SecondName']\ncat_cols = ['Pclass','Embarked','CabinType','Ticket', 'FirstName', 'SecondName']\nnum_cols = [x for x in feature_cols if x not in cat_cols]\nprint(len(feature_cols), len(cat_cols), len(num_cols))","b1efd3ab":"for col in ['SameFirstName', 'SameSecondName', 'Fare', 'FamilySize', 'Parch', 'SibSp']:\n    df[col] = np.log2(1 + df[col])\n    \nscaler = StandardScaler()\ndf[num_cols] = scaler.fit_transform(df[num_cols])\n\nlbe = LabelEncoder(min_obs=50)\ndf[cat_cols] = lbe.fit_transform(df[cat_cols]).astype(int)","3f579e5b":"# Model params from https:\/\/www.kaggle.com\/remekkinas\/ensemble-learning-meta-classifier-for-stacking by remekkinas\n\nlgb_params = {\n    'metric': 'binary_logloss',\n    'n_estimators': n_est,\n    'objective': 'binary',\n    'random_state': seed,\n    'learning_rate': 0.01,\n    'min_child_samples': 20,\n    'reg_alpha': 3e-5,\n    'reg_lambda': 9e-2,\n    'num_leaves': 63,\n    'colsample_bytree': 0.8,\n    'subsample': 0.8,\n}\n\nctb_params = {\n    'bootstrap_type': 'Poisson',\n    'loss_function': 'Logloss',\n    'eval_metric': 'Logloss',\n    'random_seed': seed,\n    'task_type': 'GPU',\n    'max_depth': 8,\n    'learning_rate': 0.01,\n    'n_estimators': n_est,\n    'max_bin': 280,\n    'min_data_in_leaf': 64,\n    'l2_leaf_reg': 0.01,\n    'subsample': 0.8\n}\n\nrf_params = {\n    'max_depth': 15,\n    'min_samples_leaf': 8,\n    'random_state': seed\n}","7ba53bb4":"base_models = {'rf': RandomForestClassifier(**rf_params), \n               'cbt': CatBoostClassifier(**ctb_params, verbose=None, logging_level='Silent'),\n               'lgb': LGBMClassifier(**lgb_params),\n               'et': ExtraTreesClassifier(bootstrap=True, criterion='entropy', max_features=0.55, min_samples_leaf=8, min_samples_split=4, n_estimators=100)}","c5eabe4b":"from copy import copy\n\nX = pd.concat((df[feature_cols], df_dae), axis=1)\ny = df[target_col]\nX_tst = X.iloc[n_trn:]\n\ncv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n\np_dict = {}\nfor name in base_models:\n    print(f'Training {name}:')\n    p = np.zeros_like(y, dtype=float)\n    p_tst = np.zeros((tst.shape[0],))\n    for i, (i_trn, i_val) in enumerate(cv.split(X, y)):\n        clf = copy(base_models[name])\n        clf.fit(X.iloc[i_trn], y[i_trn])\n        \n        p[i_val] = clf.predict_proba(X.iloc[i_val])[:, 1]\n        print(f'\\tCV #{i + 1} AUC: {roc_auc_score(y[i_val], p[i_val]):.6f}')\n\n    p_dict[name] = p\n    print(f'\\tCV AUC: {roc_auc_score(y, p):.6f}')","306f480f":"p_dict.update({\n    'lgb_dae': lgb_dae_predict_val,\n    'lgb_dae_te': lgb_dae_te_predict_val,\n    'sdae_dae': sdae_dae_predict_val\n})\n\ndump(p_dict, 'predict_val_dict.joblib')","69edc882":"X = pd.concat([pd.DataFrame(p_dict), df[feature_cols], df_dae], axis=1)\nX_tst = X.iloc[n_trn:]\n\np = np.zeros_like(y, dtype=float)\np_tst = np.zeros((tst.shape[0],))\nprint(f'Training a stacking ensemble LightGBM model:')\nfor i, (i_trn, i_val) in enumerate(cv.split(X, y)):\n    if i == 0:\n        clf = AutoLGB(objective='binary', metric='auc', sample_size=len(i_trn), random_state=seed)\n        clf.tune(X.iloc[i_trn], y[i_trn])\n        features = clf.features\n        params = clf.params\n        n_best = clf.n_best\n        print(f'{n_best}')\n        print(f'{params}')\n        print(f'{features}')\n    \n    trn_data = lgb.Dataset(X.iloc[i_trn], y[i_trn])\n    val_data = lgb.Dataset(X.iloc[i_val], y[i_val])\n    clf = lgb.train(params, trn_data, n_best, val_data, verbose_eval=100)\n    p[i_val] = clf.predict(X.iloc[i_val])\n    p_tst += clf.predict(X_tst) \/ n_fold\n    print(f'CV #{i + 1} AUC: {roc_auc_score(y[i_val], p[i_val]):.6f}')","a571cbd4":"print(f'  CV AUC: {roc_auc_score(y, p):.6f}')\nprint(f'Test AUC: {roc_auc_score(pseudo_label[target_col], p_tst)}')","0e0a02f8":"n_pos = int(0.34911 * tst.shape[0])\nth = sorted(p_tst, reverse=True)[n_pos]\nprint(th)\nconfusion_matrix(pseudo_label[target_col], (p_tst > th).astype(int))","ef9c11f9":"sub[target_col] = (p_tst > th).astype(int)\nsub.to_csv(submission_file)","754fdafa":"If you find it useful, please upvote the notebook and leave your feedback. It will be greatly appreciated!\n\nAlso please check out my previous notebooks as follows:\n* [AutoEncoder + Pseudo Label + AutoLGB](https:\/\/www.kaggle.com\/jeongyoonlee\/autoencoder-pseudo-label-autolgb): shows how to build a basic AutoEncoder using Keras, and perform automated feature selection and hyperparameter optimization using `Kaggler`'s `AutoLGB`.\n* [Supervised Emphasized Denoising AutoEncoder](https:\/\/www.kaggle.com\/jeongyoonlee\/supervised-emphasized-denoising-autoencoder): shows how to build a more sophiscated version of AutoEncoder, called supervised emphasized Denoising AutoEncoder (DAE), which trains DAE and a classifier simultaneously.\n\nHappy Kaggling! ;)\n","5bebbdb7":"Adding CV predictions of two additional models trained separately. You can use all models trained throughout the competition as long as those are traine d with the same CV folds.\n\n**ALWAYS SAVE CV PREDICTIONS!!!**","86448e7d":"Make sure that you use the same CV folds across all level-1 models.","592d41c3":"Applying `log2(1 + x)` for numerical features and label-encoding categorical features using `kaggler.preprocessing.LabelEncoder`, which handles `NaN`s and groups rare categories together.","06ff9bb0":"# Part 2: Level-1 Base Model Training","29ac0a3a":"Feature engineering using @udbhavpangotra's [code](https:\/\/www.kaggle.com\/udbhavpangotra\/tps-apr21-eda-model).","ffe6505c":"Training a level-2 LightGBM model with the level-1 model CV predictions, original features, and DAE features as inputs. If you have enough level-1 model predictions, you can train level-2 models only with level-1 model predictions. Here, since we only have six level-1 models, we use additional features and perform feature selection.","19de6787":"# Part 1: Data Loading & Feature Engineering","893b875b":"Loading 128 DAE features generated from [Supervised Emphasized Denoising AutoEncoder](https:\/\/www.kaggle.com\/jeongyoonlee\/supervised-emphasized-denoising-autoencoder\/).","8279a3a4":"# Part 3: Level-2 Stacking","cdc122d4":"This notebook shows how to perform stacking ensemble (a.k.a. stacked generalization).\n\nIn [Ensemble-learning meta-classifier for stacking](https:\/\/www.kaggle.com\/remekkinas\/ensemble-learning-meta-classifier-for-stacking), @remekkinas shares how to do stacking ensemble using `MLExtend'`s `StackingCVClassifier`.\n\nTo demonstrate how stacking works, this notebook shows how to prepare the baseline model predictions using cross-validation (CV), then use them for level-2 stacking. It trains four classifiers, Random Forests, Extremely Randomized Trees, LightGBM, and CatBoost as level-1 base models. It also uses  CV predictions of two models, LightGBM with DAE features and supervised DAE trained from my previous notebook, [Supervised Emphasized Denoising AutoEncoder](https:\/\/www.kaggle.com\/jeongyoonlee\/supervised-emphasized-denoising-autoencoder) to show why keeping CV predictions for **every** model is important. :)\n\nThe contents of this notebook are as follows:\n1. **Feature Engineering**: Same as in the [Supervised Emphasized Denoising AutoEncoder](https:\/\/www.kaggle.com\/jeongyoonlee\/supervised-emphasized-denoising-autoencoder) and [AutoEncoder + Pseudo Label + AutoLGB](https:\/\/www.kaggle.com\/jeongyoonlee\/autoencoder-pseudo-label-autolgb).\n2. **Level-1 Base Model Training**: Training four base models, Random Forests, Extremely Randomized Trees, LightGBM, and CatBoost using the same 5-fold CV.\n3. **Level-2 Stacking**: Training the LightGBM model with CV predictions of base models, original features, and DAE features. Performing feature selection and hyperparameter optimization using `Kaggler`'s `AutoLGB`.\n\nThis notebook is inspired and\/or based on other Kagglers' notebooks as follows:\n* [TPS-APR21-EDA+MODEL](https:\/\/www.kaggle.com\/udbhavpangotra\/tps-apr21-eda-model) by @udbhavpangotra\n* [Ensemble-learning meta-classifier for stacking](https:\/\/www.kaggle.com\/remekkinas\/ensemble-learning-meta-classifier-for-stacking) by @remekkinas\n* [TPS Apr 2021 pseudo labeling\/voting ensemble](https:\/\/www.kaggle.com\/hiro5299834\/tps-apr-2021-pseudo-labeling-voting-ensemble?scriptVersionId=60616606) by @hiro5299834\n\nThanks!"}}