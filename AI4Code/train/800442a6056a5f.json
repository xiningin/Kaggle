{"cell_type":{"a490430d":"code","182361a2":"code","4e1329d3":"code","d19fbd4d":"code","072c7a1d":"code","6f2c3318":"code","6b7707b1":"code","7307ec94":"code","1baaaa82":"code","2f9d09c8":"code","57e8ef00":"code","7a2967c9":"code","5aff1599":"code","3300a097":"code","569c500b":"code","53fafa18":"code","74531881":"code","c49de98e":"code","cfadae15":"code","9ba54a67":"code","5618a7d2":"code","01196e9c":"code","b2608d93":"code","6d3cf17e":"code","42cd46be":"code","2d9b9f75":"code","a1a8a691":"code","df9ff941":"code","ce5246ee":"code","4fe948ee":"code","c6b24cd0":"code","226a2fa8":"code","15301eab":"code","695f6ac3":"code","c5c3f5d8":"code","45418a32":"code","0f2f46ca":"code","677ee39c":"code","ab366dd1":"code","30194411":"code","1ff75489":"code","6754441c":"code","9946ffea":"code","7ce2ced1":"code","73b8344c":"code","e43a7e2f":"code","7202bc50":"code","87473559":"code","9e676481":"code","c2073f3a":"code","c7204c05":"markdown","efd5b227":"markdown","b5d40580":"markdown"},"source":{"a490430d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","182361a2":"train = pd.read_csv('\/kaggle\/input\/may5aaa\/55data_train.csv')\ntests = pd.read_csv('\/kaggle\/input\/may5aaa\/55data_tests.csv')","4e1329d3":"train = pd.read_csv('\/kaggle\/input\/raw-water\/raw_train.csv')\ntests = pd.read_csv('\/kaggle\/input\/raw-water\/raw_tests.csv')","d19fbd4d":"train_labels=train.pop('A\u5382')\ntrain_prepared=train","072c7a1d":"from mlxtend.regressor import StackingRegressor\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor","6f2c3318":"import time #implementing in this function the time spent on training the model\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score, KFold\n\ndef modelfit(alg, dtrain, target, only_predict = False):\n    #Fit the algorithm on the data\n    time_start = time.perf_counter() #start counting the time\n    if not only_predict:\n        alg.fit(dtrain, target)\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain)\n    \n    kfolds = KFold(n_splits=6, shuffle=True, random_state=42)\n    \n    cv_score = cross_val_score(alg, dtrain,target, cv=kfolds, scoring='neg_mean_squared_error')\n    cv_score = np.sqrt(-cv_score)\n    \n    time_end = time.perf_counter()\n    \n    total_time = time_end-time_start\n    #Print model report:\n    print(\"\\nModel Report\")\n    print(\"RMSE :  {:.4f}\".format(np.sqrt(mean_squared_error(target, dtrain_predictions))))\n    print(\"CV Score : Mean -  %.4f | Std -  %.4f | Min -  %.4f | Max - %.4f\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n    print(\"Amount of time spent during training the model and cross validation: %4.3f seconds\" % (total_time))\n    \ndef plot_feature_importance(model, df):\n    feature_importance = model.feature_importances_[:30]\n    # make importances relative to max importance\n    plt.figure(figsize=(20, 20)) #figure size\n    feature_importance = 100.0 * (feature_importance \/ feature_importance.max()) #making it a percentage relative to the max value\n    sorted_idx = np.argsort(feature_importance)\n    pos = np.arange(sorted_idx.shape[0]) + .5\n    plt.barh(pos, feature_importance[sorted_idx], align='center')\n    plt.yticks(pos, df.columns[sorted_idx], fontsize=15) #used train_drop here to show the name of each feature instead of our train_prepared \n    plt.xlabel('Relative Importance', fontsize=20)\n    plt.ylabel('Features', fontsize=20)\n    plt.title('Variable Importance', fontsize=30)","6b7707b1":"lin_reg = LinearRegression()\nmodelfit(lin_reg, train_prepared, train_labels)","7307ec94":"forest_reg = RandomForestRegressor(n_estimators=300, \n                                   random_state=1026, \n                                   min_samples_leaf=5,\n                                   min_samples_split = 5,\n                                   max_depth = 15,\n                                   n_jobs=-1, oob_score=True)\nmodelfit(forest_reg, train_prepared, train_labels)","1baaaa82":"plot_feature_importance(forest_reg, train_prepared)","2f9d09c8":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(random_state=421, max_depth = 15, min_samples_split = 5)\nmodelfit(tree_reg, train_prepared, train_labels) ","57e8ef00":"from sklearn.ensemble import AdaBoostRegressor\n\n# tree_ada = DecisionTreeRegressor(random_state = 42,max_depth = 4)\n\nada_reg = AdaBoostRegressor(\n    tree_reg, n_estimators=300, random_state=42,learning_rate=0.009, loss='square')\nmodelfit(ada_reg, train_prepared, train_labels)\n\n# 181.589","7a2967c9":"plot_feature_importance(ada_reg, train_prepared)","5aff1599":"params = {'n_estimators': 100, 'max_depth': 14, 'min_samples_split': 2,\n          'learning_rate': 0.01, 'loss': 'ls'}\n\nparams_1 = {'n_estimators': 1000, 'learning_rate': 0.05, 'max_depth' : 14,\n            'max_features': 'sqrt', 'min_samples_leaf': 15, 'min_samples_split': 5, \n            'loss': 'huber', 'random_state': 42}\n\n# train_dummies_prepared = num_pipeline.fit_transform(train_dummies)\n\ngdb_model = GradientBoostingRegressor(**params_1)\nmodelfit(gdb_model, train_prepared, train_labels)","3300a097":"train = pd.read_csv('\/kaggle\/input\/may5aaa\/55data_train.csv')\ntests = pd.read_csv('\/kaggle\/input\/may5aaa\/55data_tests.csv')","569c500b":"train_raw2 = train\ntests_raw2 = tests","53fafa18":"train_raw2['B\u5382'] = train_raw2['B\u5382'].astype(int) \ntests_raw2['B\u5382'] = tests_raw2['B\u5382'].astype(int) \n# for i in ['B\u5382', 'year', 'wee']:\n#     train_raw2[i] = train_raw2[i].astype(object)\n#     tests_raw2[i] = tests_raw2[i].astype(object)\n\ntests_raw2.columns = train_raw2.columns","74531881":"train_raw2.info()","c49de98e":"y = train_raw2.pop('A\u5382')\nX = train_raw2\ntests_raw2.pop('A\u5382')","cfadae15":"y = np.array(y).reshape(-1,1)","9ba54a67":"tests_raw2.info()","5618a7d2":"meta_random_seed = 1026529","01196e9c":"from sklearn.model_selection import train_test_split\nimport optuna\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.metrics import mean_absolute_percentage_error","b2608d93":"def objective(trial, X=X, y=y):\n\n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=meta_random_seed)\n\n\n    lgb_params={\n        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2),\n        'max_depth': trial.suggest_int('max_depth', 6, 31),\n        'num_leaves': trial.suggest_int('num_leaves', 31, 120),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-1, 1),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-1, 1),\n        'random_state': meta_random_seed,\n        'metric': 'mse',\n        'n_estimators': trial.suggest_int('n_estimators', 6, 3000),\n        'n_jobs': -1,\n        'cat_feature': [x for x in range(len(categorical_columns))],\n        'bagging_seed': 2021,\n        'feature_fraction_seed': 2021,\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.9),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 500),\n        'subsample_freq': trial.suggest_int('subsample_freq', 1, 10),\n        'subsample': trial.suggest_float('subsample', 0.7, 0.9),\n        'max_bin': trial.suggest_int('max_bin', 128, 1024),\n        'min_data_per_group': trial.suggest_int('min_data_per_group', 5, 50),\n        'cat_smooth': trial.suggest_int('cat_smooth', 10, 250),\n        'cat_l2': trial.suggest_int('cat_l2', 1, 20)\n    }\n\n    lgb = LGBMRegressor(\n        **lgb_params\n    )\n    lgb.fit(\n        X_train,\n        y_train,\n        eval_set=(X_test,y_test),\n        eval_metric='mean_absolute_percentage_error',\n        early_stopping_rounds=100,\n        verbose=False\n    )\n    predictions=lgb.predict(X_test)\n\n    return mean_absolute_percentage_error(y_test, predictions)","6d3cf17e":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, timeout=3600*7, n_trials=25)","42cd46be":"study.best_params\n\n# {'learning_rate': 0.0026955685638868463,\n#  'max_depth': 180,\n#  'num_leaves': 110,\n#  'reg_alpha': 0.8365820878758264,\n#  'reg_lambda': 0.5052568831768142,\n#  'n_estimators': 2458,\n#  'colsample_bytree': 0.7589370718114493,\n#  'min_child_samples': 7,\n#  'subsample_freq': 10,\n#  'subsample': 0.7036373549101399,\n#  'max_bin': 906,\n#  'min_data_per_group': 50,\n#  'cat_smooth': 242,\n#  'cat_l2': 4}","2d9b9f75":"lgbm_params = {'learning_rate': 0.0026955685638868463,\n 'max_depth': 180,\n 'num_leaves': 110,\n 'reg_alpha': 0.8365820878758264,\n 'reg_lambda': 0.5052568831768142,\n 'n_estimators': 2458,\n 'colsample_bytree': 0.7589370718114493,\n 'min_child_samples': 7,\n 'subsample_freq': 10,\n 'subsample': 0.7036373549101399,\n 'max_bin': 906,\n 'min_data_per_group': 50,\n 'cat_smooth': 242,\n 'cat_l2': 4}","a1a8a691":"def objective2(trial, X=X, y=y):\n    X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.25,random_state=meta_random_seed)\n    \n    xgb_params={\n        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2),\n        'max_depth': trial.suggest_int('max_depth', 6, 31),\n        'alpha': trial.suggest_float('alpha', 1e-1, 1),\n        'gamma': trial.suggest_float('gamma', 1e-1, 1),\n#         'reg_lambda': trial.suggest_float('reg_lambda', 1e-1, 1),\n        'random_state': meta_random_seed,\n        'n_estimators': trial.suggest_int('n_estimators', 6, 3000),\n        'n_jobs': -1,\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 0.9),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 500),\n        'subsample': trial.suggest_float('subsample', 0.6, 0.9),\n        'max_bin': trial.suggest_int('max_bin', 128, 1024)       \n#     params = {\n#         'n_estimators': trial.suggest_int('n_estimators', 350, 1000),\n#         'max_depth': trial.suggest_int('max_depth', 3, 10),\n#         'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.10),\n#         'subsample': trial.suggest_uniform('subsample', 0.50, 0.90),\n#         'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.50, 0.90),\n#         'gamma': trial.suggest_int('gamma', 0, 20),\n#         'missing': -999,\n#         'tree_method': 'gpu_hist'  \n#     }\n    }\n\n    xgb = XGBRegressor(\n        **xgb_params,tree_method = 'gpu_hist'\n    )\n    xgb.fit(\n        X_train,\n        y_train,\n        eval_set=[(X_test,y_test)],\n        eval_metric='rmse',\n        early_stopping_rounds=100,\n        verbose=False\n    )\n    predictions=xgb.predict(X_test)\n\n    return mean_absolute_percentage_error(y_test, predictions)","df9ff941":"study2 = optuna.create_study(direction='minimize')\nstudy2.optimize(objective2, timeout=3600*6, n_trials=15)","ce5246ee":"#study.best_params\nstudy2.best_params","4fe948ee":"xgb_params = {'learning_rate': 0.0018800314311429142,\n 'max_depth': 16,\n 'alpha': 0.2516299216083434,\n 'gamma': 0.9382629503388609,\n 'n_estimators': 1519,\n 'colsample_bytree': 0.8616778784529846,\n 'min_child_weight': 132,\n 'subsample': 0.7089219366903059,\n 'max_bin': 332}","c6b24cd0":"#","226a2fa8":"train_raw2","15301eab":"train_df = X\ntest_df = tests_raw2","695f6ac3":"from sklearn.model_selection import StratifiedKFold,RepeatedKFold\nfrom scipy.special import expit\nfrom sklearn.calibration import CalibratedClassifierCV\n\nrandom_state = 1026529\nn_folds = 6\nk_fold = RepeatedKFold(n_splits=n_folds, random_state=random_state)\n\n# y = train_df[\"target\"]\n\nxgb_train_preds = np.zeros(len(train_df.index), )\nxgb_test_preds = np.zeros(len(test_df.index), )\n# xgb_features = xgb_cat_features + list(numerical_columns)\n\nlgbm_train_preds = np.zeros(len(train_df.index), )\nlgbm_test_preds = np.zeros(len(test_df.index), )\n# lgbm_features = lgb_cat_features + list(numerical_columns)\n\n# cb_train_preds = np.zeros(len(train_df.index), )\n# cb_test_preds = np.zeros(len(test_df.index), )\n# cb_features = cb_cat_features + list(numerical_columns)\n\n# ridge_train_preds = np.zeros(len(train_df.index), )\n# ridge_test_preds = np.zeros(len(test_df.index), )\n# ridge_features = ridge_cat_features + list(numerical_columns)","c5c3f5d8":"for fold, (train_index, test_index) in enumerate(k_fold.split(train_df, y)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = y.iloc[train_index]\n    y_valid = y.iloc[test_index]\n\n    ########## Generate train and valid sets ##########\n    xgb_x_train = pd.DataFrame(train_df.iloc[train_index])\n    xgb_x_valid = pd.DataFrame(train_df.iloc[test_index])\n\n    lgbm_x_train = pd.DataFrame(train_df.iloc[train_index])\n    lgbm_x_valid = pd.DataFrame(train_df.iloc[test_index])\n    \n#     cb_x_train = pd.DataFrame(train_df[cb_features].iloc[train_index])\n#     cb_x_valid = pd.DataFrame(train_df[cb_features].iloc[test_index])\n\n#     ridge_x_train = pd.DataFrame(train_df[ridge_features].iloc[train_index])\n#     ridge_x_valid = pd.DataFrame(train_df[ridge_features].iloc[test_index])\n\n    ########## XGBoost model ##########\n    xgb_model = XGBRegressor(\n        seed=random_state,\n        n_estimators=1519,\n#         verbosity=1,\n        eval_metric=\"rmse\",\n        tree_method=\"gpu_hist\",\n        gpu_id=0,\n        alpha=0.2516299216083434,\n        colsample_bytree=0.8616778784529846,\n        gamma=0.9382629503388609,\n        reg_lambda=0.969826765347235612,\n        learning_rate=0.0018800314311429142,\n        max_bin=332,\n        max_depth=16,\n        min_child_weight=132,\n        subsample=0.7089219366903059,\n    )\n    xgb_model.fit(\n        xgb_x_train,\n        y_train,\n        eval_set=[(xgb_x_valid, y_valid)], \n        verbose=0,\n        early_stopping_rounds=200\n    )\n\n    train_oof_preds = xgb_model.predict(xgb_x_valid)\n    test_oof_preds = xgb_model.predict(test_df)\n    xgb_train_preds[test_index] = train_oof_preds\n    xgb_test_preds += test_oof_preds \/ n_folds\n    \n    print(\": XGB - RMSE = {}\".format(mean_absolute_percentage_error(y_valid, train_oof_preds)))\n    \n    ########## LGBM model ##########\n    lgbm_model = LGBMRegressor(\n        random_state=random_state,\n        cat_l2=4,\n        cat_smooth=242,\n        colsample_bytree=0.7589370718114493,\n        early_stopping_round=200,\n        learning_rate=0.0026955685638868463,\n        max_bin=906,\n        max_depth=180,\n        metric=\"rmse\",\n        min_child_samples=7,\n        min_data_per_group=50,\n        n_estimators=2458,\n        n_jobs=-1,\n        num_leaves=110,\n        reg_alpha=0.8365820878758264,\n        reg_lambda=0.5052568831768142,\n        subsample=0.7036373549101399,\n        subsample_freq=10,\n        verbose=-1,\n    )\n    lgbm_model.fit(\n        lgbm_x_train,\n        y_train,\n        eval_set=[(lgbm_x_valid, y_valid)], \n        verbose=0,\n    )\n\n    train_oof_preds = lgbm_model.predict(lgbm_x_valid)\n    test_oof_preds = lgbm_model.predict(test_df)\n    lgbm_train_preds[test_index] = train_oof_preds\n    lgbm_test_preds += test_oof_preds \/ n_folds\n    \n    print(\": LGB - RMSE = {}\".format(mean_absolute_percentage_error(y_valid, train_oof_preds)))","45418a32":"pd.DataFrame(lgbm_test_preds\/n_folds).to_csv('.\/lgbm55.csv')","0f2f46ca":"pd.DataFrame(xgb_test_preds\/n_folds).to_csv('.\/xgb55.csv')","677ee39c":"import xgboost as xgb\n\n    \nparams = {'objective': 'reg:squarederror', \n              'eta': 0.01, \n              'max_depth': 16, \n              'subsample': 0.9, \n              'colsample_bytree': 1,  \n              'eval_metric': 'rmse', \n              'seed': 42, \n              'silent': True,\n    }\n\nmodel_xgb = xgb.XGBRegressor(**params, n_estimators=1000,learning_rate=0.05,verbose_eval=True)","ab366dd1":"def objective(trial,data=x,target=y):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.25,random_state=1026s)\n    \n    # To select which parameters to optimize, please look at the XGBoost documentation:\n    # https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n    param = {\n        'device':'gpu',  # Use GPU acceleration\n        'metric': 'rmse',\n        'random_state': 42,\n        'reg_lambda': trial.suggest_loguniform(\n            'reg_lambda', 1e-3, 10.0\n        ),\n        'reg_alpha': trial.suggest_loguniform(\n            'reg_alpha', 1e-3, 10.0\n        ),\n        'colsample_bytree': trial.suggest_categorical(\n            'colsample_bytree', [0.3,0.5,0.6,0.7,0.8,0.9,1.0]\n        ),\n        'subsample': trial.suggest_categorical(\n            'subsample', [0.6,0.7,0.8,1.0]\n        ),\n        'learning_rate': trial.suggest_categorical(\n            'learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]\n        ),\n        'n_estimators': trial.suggest_categorical(\n            \"n_estimators\", [150, 200, 300, 3000]\n        ),\n        'max_depth': trial.suggest_categorical(\n            'max_depth', [4,5,7,9,11,13,15,17,20]\n        ),\n        'min_child_samples': trial.suggest_int(\n            'min_child_samples', 1, 300\n        ),\n        'num_leaves': trial.suggest_int(\n            'num_leaves', 15, 120\n        ),\n    }\n    model = LGBMRegressor(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)], early_stopping_rounds=300, verbose=False)\n    \n    preds = model.predict(test_x)\n    \n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse","30194411":"from datetime import datetime\nfold_val_pred = []\nfold_err = []\nfold_importance = []\nrecord = dict()\nkfold = list(KFold(10,shuffle = True, random_state = 42).split(train_prepared))\n\ndef xgb_model(train, params, train_label, fold, verbose, pipeline):\n    for i, (trn, val) in enumerate(fold) :\n        print(i+1, \"fold.    RMSE\")\n        trn_x = train.loc[trn, :]\n        trn_y = train_label[trn]\n        val_x = train.loc[val, :]\n        val_y = train_label[val]\n        fold_val_pred = []\n    \n        start = datetime.now()\n        model = xgb.train(params\n                      , xgb.DMatrix(trn_x, trn_y)\n                      , 3500\n                      , [(xgb.DMatrix(trn_x, trn_y), 'train'), (xgb.DMatrix(val_x, val_y), 'valid')]\n                      , verbose_eval=verbose\n                      , early_stopping_rounds=500\n                      , callbacks = [xgb.callback.record_evaluation(record)])\n        best_idx = np.argmin(np.array(record['valid']['rmse']))\n        val_pred = model.predict(xgb.DMatrix(val_x), ntree_limit=model.best_ntree_limit)\n        fold_val_pred.append(val_pred*0.2)\n        fold_err.append(record['valid_0']['rmse'][best_idx])\n        fold_importance.append(model.feature_importance('gain'))\n        \n        print(\"xgb model.\", \"{0:.5f}\".format(result['error']), '(' + str(int((datetime.now()-start).seconds\/60)) + 'm)')","1ff75489":"modelfit(model_xgb, train_prepared, train_labels)\n\n#132.587 seconds","6754441c":"from sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV\n\npipe = Pipeline([\n#     ('pca', PCA(5)),\n    ('xgb', xgb.XGBRegressor())\n])\n\nparam_grid = {\n#     'pca__n_components': [3, 5, 7, 10,15],\n    'xgb__n_estimators': [500,1000,2000,3500],\n    'xgb__learning_rate': [0.09,0.1,0.05,0.001]\n}\n\npipe.fit(train_prepared, train_labels)","9946ffea":"modelfit(pipe, train_prepared, train_labels, True)","7ce2ced1":"import lightgbm as lgb\nimport xgboost as xgb\n\nlgb_model = lgb.LGBMRegressor(objective='regression', \n                                       num_leaves=500,\n                                       learning_rate=0.01, \n                                       n_estimators=3000,\n                                       max_depth = 15,\n#                                        max_bin=200, \n                                       bagging_fraction=0.85,\n                                       bagging_freq=5, \n                                       bagging_seed=75,\n#                                        feature_fraction=1,\n#                                        feature_fraction_seed=75,\n                                       metric='rmse',\n                                       verbose=-1,\n                                       )\n\nmodelfit(lgb_model, train_prepared, train_labels)","73b8344c":"from mlxtend.regressor import StackingCVRegressor\nmodels_total = (lin_reg, forest_reg, ada_reg, gdb_model, lgb_model)\n# model_ada_gbd = (ada_reg,gdb_model)\nstack_gen = StackingCVRegressor(regressors= models_total,\n                                meta_regressor=model_xgb,\n                                use_features_in_secondary=True)","e43a7e2f":"modelfit(stack_gen, train_prepared, train_labels)","7202bc50":"tests_labels=tests.pop('A\u5382')\ntests_prepared=tests","87473559":"prediction_some_regressors = stack_gen.predict(tests_prepared)","9e676481":"prediction_some_regressors","c2073f3a":"pd.DataFrame(prediction_some_regressors).to_csv(\".\/kag.csv\", index = False)","c7204c05":"---\n\n# now ","efd5b227":"# over ","b5d40580":"---"}}