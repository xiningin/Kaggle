{"cell_type":{"d8d42d36":"code","2e8b7964":"code","9840b5e8":"code","5d9bab00":"code","7a40cb13":"code","1a1662a8":"code","dc7e37fa":"code","bc97893e":"code","a1af7bdb":"code","1b114de0":"code","05cb5e15":"code","de3e3545":"code","a52da5bb":"code","42e841e1":"code","d2a89a46":"markdown","c54bf035":"markdown","cd34dfa1":"markdown","7330c571":"markdown","00cd1df9":"markdown","fcb5a519":"markdown","c249508c":"markdown","d6e852f7":"markdown","92c44ddc":"markdown","27182eb1":"markdown","8e97c72d":"markdown","3fdef88f":"markdown","4fba2805":"markdown","aa23b4dd":"markdown","7f6ef4e5":"markdown","1d46ebbf":"markdown","503a6451":"markdown","504eb956":"markdown","d62e3e2d":"markdown","41cfa19c":"markdown","869b15c2":"markdown","72826cac":"markdown"},"source":{"d8d42d36":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.datasets import load_boston, load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, accuracy_score\n\nsns.set_theme()\n\nnp.random.seed(0)\n\na = np.random.uniform(-40, 40, 10) \nb = np.random.uniform(50, 70, 80)\nx_values = np.concatenate((a, b), axis=0)\n\na = np.random.uniform(-40, 40, 10) \nb = np.random.uniform(50, 70, 80)\ny_values = np.concatenate((a, b), axis=0)\n\n\nfig, ax = plt.subplots(figsize=(14, 7))\n\nsns.scatterplot(x=x_values, y=y_values, ax=ax);","2e8b7964":"def split_error(X, treshold):\n    left, right = np.where(X <= treshold)[0], np.where(X > treshold)[0]\n    if len(left) == 0 or len(right) == 0:\n        return 10000\n    error = np.std(X[left]) + np.std(X[right])\n    return error\n\ndef best_criteria(X):\n    best_treshold = None\n    best_error = None\n    unique_values = np.unique(X)\n    for treshold in unique_values:\n        error = split_error(X, treshold)\n        if best_error == None or error < best_error:\n            best_treshold = treshold\n            best_error = error\n    return best_treshold, best_error\n\nbest_treshold, best_error = best_criteria(x_values)\n\nfig, ax = plt.subplots(figsize=(14, 7))\n\nsns.scatterplot(x=x_values, y=y_values, ax=ax);\nplt.axvline(best_treshold, 0, 1);","9840b5e8":"def split_error(X, treshold):\n    left, right = np.where(X <= treshold)[0], np.where(X > treshold)[0]\n    if len(left) == 0 or len(right) == 0:\n        return 10000\n    error = len(left) \/ len(X) * np.std(X[left]) + len(right) \/ len(X) * np.std(X[right])\n    return error\n\ndef best_criteria(X):\n    best_treshold = None\n    best_error = None\n    unique_values = np.unique(X)\n    for treshold in unique_values:\n        error = split_error(X, treshold)\n        if best_error == None or error < best_error:\n            best_treshold = treshold\n            best_error = error\n    return best_treshold, best_error\n\nbest_treshold, best_error = best_criteria(x_values)\n\nfig, ax = plt.subplots(figsize=(14, 7))\n\nsns.scatterplot(x=x_values, y=y_values, ax=ax);\nplt.axvline(best_treshold, 0, 1);","5d9bab00":"class Node:\n    \"\"\"\n    Class that will be used for building tree.\n    Linked list basically.\n    \"\"\"\n    def __init__(self, left=None, right=None, value=None, feature_idx=None, treshold=None):\n        self.left = left\n        self.right = right\n        self.value = value\n        self.feature_idx = feature_idx\n        self.treshold = treshold","7a40cb13":"class Tree:\n    def __init__(self, max_depth=5, min_samples_split=2, max_features=1):\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.max_features = max_features\n        self.tree = None\n        \n    def fit(self, X, y):\n        self.tree = self.grow_tree(X, y)\n        \n    def predict(self, X):\n        return [self.traverse_tree(x, self.tree) for x in X]\n    \n    def traverse_tree(self, x, node):\n        if node.value != None:\n            return node.value\n        if x[node.feature_idx] <= node.treshold:\n            return self.traverse_tree(x, node.left)\n        return self.traverse_tree(x, node.right)\n    \n    def split_error(self, X, feauture_idx, treshold):\n        \"\"\"\n        Calculate standart deviation after splitting into 2 groups\n        \"\"\"\n        left_idxs, right_idxs = self.split_node(X, feauture_idx, treshold)\n        \n        if len(X) == 0 or len(left_idxs) == 0 or len(right_idxs) == 0:\n            return 10000\n\n        return len(left_idxs) \/ len(X) * self.standart_deviation(X[left_idxs], feauture_idx) + len(right_idxs) \/ len(X) * self.standart_deviation(X[right_idxs], feauture_idx)\n    \n    def standart_deviation(self, X, feauture_idx):\n        \"\"\"\n        Calculate standart deviation\n        \"\"\"\n        return np.std(X[:, feauture_idx])\n    \n    def split_node(self, X, feauture_idx, treshold):\n        \"\"\"\n        Split into 2 parts\n        Splitting a dataset means separating a dataset\n        into two lists of rows. Once we have the two \n        groups, we can then use our standart deviation\n        score above to evaluate the cost of the split.\n        \"\"\"\n        left_idxs  = np.argwhere(X[:, feauture_idx] <= treshold).flatten()\n        right_idxs = np.argwhere(X[:, feauture_idx] > treshold).flatten()\n        return left_idxs, right_idxs\n    \n    def best_criteria(self, X, feature_idxs):\n        \"\"\"\n        Find best split\n\n        Loop throw each feature, for each feature loop \n        throw each unique value, try each value as a \n        treshold, then choose one, with the smallest error\n        \"\"\"\n        best_feauture_idx = None\n        best_treshold = None\n        best_error = None\n\n        for feature_idx in feature_idxs:\n            unique_values = np.unique(X[:, feature_idx])\n            for treshold in unique_values:\n                error = self.split_error(X, feature_idx, treshold)\n                if best_error == None or error < best_error:\n                    best_feauture_idx = feature_idx\n                    best_treshold = treshold\n                    best_error = error\n\n        return best_feauture_idx, best_treshold","1a1662a8":"class DecisionTreeRegressor(Tree):\n    def grow_tree(self, X, y, depth=0):\n        n_samples, n_features = X.shape\n        \n        if depth > self.max_depth or len(X) < self.min_samples_split:\n            return Node(value=y.mean())\n        \n        feature_idxs = np.arange(int(n_features * self.max_features))\n        best_feauture_idx, best_treshold = self.best_criteria(X, feature_idxs)\n        left_idxs, right_idxs = self.split_node(X, best_feauture_idx, best_treshold)\n\n        if len(left_idxs) == 0 or len(right_idxs) == 0:\n            return Node(value=y.mean())\n        else:\n            left = self.grow_tree(X[left_idxs], y[left_idxs], depth + 1)\n            right = self.grow_tree(X[right_idxs], y[right_idxs], depth + 1)\n            return Node(left=left, right=right, feature_idx=best_feauture_idx, treshold=best_treshold)","dc7e37fa":"data = load_boston()\n\nX_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25)\n\nmodel = DecisionTreeRegressor()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint('MSE: {}'.format(mean_squared_error(y_test, y_pred)))","bc97893e":"df = pd.DataFrame(columns=['Depth', 'MSE'])\n\nfor depth in range(1, 12):\n    model = DecisionTreeRegressor(max_depth=depth)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    df = df.append({'Depth': depth, 'MSE': mean_squared_error(y_test, y_pred)}, ignore_index=True)\n    \nfig, ax = plt.subplots(figsize=(14, 7))\n\nsns.lineplot(data=df, x='Depth', y='MSE', ax=ax);","a1af7bdb":"class DecisionTreeClassifier(Tree):\n    def grow_tree(self, X, y, depth=0):\n        n_samples, n_features = X.shape\n        if depth > self.max_depth or len(X) < self.min_samples_split:\n            counts = np.bincount(y)\n            return Node(value=np.argmax(counts))\n    \n        feature_idxs = np.arange(int(n_features * self.max_features))\n        best_feauture_idx, best_treshold = self.best_criteria(X, feature_idxs)\n        left_idxs, right_idxs = self.split_node(X, best_feauture_idx, best_treshold)\n        \n        \n        if len(left_idxs) == 0 or len(right_idxs) == 0:\n            counts = np.bincount(y)\n            return Node(value=np.argmax(counts))\n        else:\n            left = self.grow_tree(X[left_idxs], y[left_idxs], depth + 1)\n            right = self.grow_tree(X[right_idxs], y[right_idxs], depth + 1)\n            return Node(left=left, right=right, feature_idx=best_feauture_idx, treshold=best_treshold)","1b114de0":"X, y = load_iris(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n\nclf = DecisionTreeClassifier()\n\nclf.fit(X_train, y_train);\n\naccuracy_score(y_test, clf.predict(X_test))","05cb5e15":"def bootstrap_sample(X, y, size):\n    n_samples = X.shape[0]\n    idxs = np.random.choice(n_samples, size=int(n_samples * size), replace=True)\n    return(X[idxs], y[idxs])","de3e3545":"class RandomForestRegressor:\n    def __init__(self, min_samples_split=2, max_depth=100, n_estimators=5, bootstrap=0.9, max_features=1):\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        self.models = []\n        self.n_estimators = n_estimators\n        self.bootstrap = bootstrap\n        self.max_features = max_features\n\n    def fit(self, X, y):\n        for _ in range(self.n_estimators):\n            X_sample, y_sample = bootstrap_sample(X, y, size=self.bootstrap)\n            model = DecisionTreeRegressor(max_depth=self.max_depth, min_samples_split=self.min_samples_split, max_features=self.max_features)\n            model.fit(X_sample, y_sample)\n            self.models.append(model)\n\n    def predict(self, X):\n        n_samples, n_features = X.shape\n        res = np.zeros(n_samples)\n        for model in self.models:\n            res += model.predict(X)\n        return res \/ self.n_estimators","a52da5bb":"data = load_boston()\n\nX_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25)\n\nmodel = RandomForestRegressor(n_estimators=3,\n                                    bootstrap=0.8,\n                                    max_depth=10,\n                                    min_samples_split=3,\n                                    max_features=1)\nmodel.fit(X_train, y_train)\nMSE = mean_squared_error(y_test, model.predict(X_test))\nprint('MSE: {}'.format(MSE))","42e841e1":"df = pd.DataFrame(columns=['Number of trees', 'MSE', 'Max features'])\n\nfor number_of_trees in range(1, 15):\n    model = RandomForestRegressor(n_estimators=number_of_trees,\n                                    bootstrap=0.9,\n                                    max_depth=10,\n                                    min_samples_split=3,\n                                    max_features=1)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    df = df.append({'Number of trees': number_of_trees, 'MSE': mean_squared_error(y_test, y_pred), 'Max features': 1}, ignore_index=True)\n    \n    model = RandomForestRegressor(n_estimators=number_of_trees,\n                                    bootstrap=0.9,\n                                    max_depth=10,\n                                    min_samples_split=3,\n                                    max_features=0.9)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    df = df.append({'Number of trees': number_of_trees, 'MSE': mean_squared_error(y_test, y_pred), 'Max features': 0.9}, ignore_index=True)\n    \nfig, ax = plt.subplots(figsize=(14, 7))\n\nsns.lineplot(data=df, x='Number of trees', y='MSE', hue='Max features', ax=ax);","d2a89a46":"$$\\Large\nQ = H(R_l) + H(R_r) \n$$","c54bf035":"## Implementing base classes\n\nSince **DecisionTreeRegressor** and **DecisionTreeClassifier** have a lot of the same functions we will create a base class and then inherit it instead of coding two huge classes for each","cd34dfa1":"## Single prediction\n\nNot let's test our algorothm","7330c571":"Besides,a decision trees can work for both regression problems and for classification problems. In fact, we will code a decision tree from scratch that can do both.\n\nNow you know the bases of this algorithm, but surely you have doubts. How does the algorithm decide which variable to use as the first cutoff? How do you choose the values? Let\u2019s see it little by little programming our own decision tree from scratch in Python.","00cd1df9":"## MSE depending on tree depth\n\nLet's see how does it depen on the depth hyperparameter","fcb5a519":"## Decision Tree Classifier\n\nKey point of classification task for decision tree is that we want to return the most frequent class for specific condition","c249508c":"<h1 style='text-align: center; font-weight: 400'>Decision Tree and Random Forest From Scratch<\/h1>\n\n<p  style='text-align: center'>\nThis notebook is in <span style='color: green; font-weight: 700'>Active<\/span> state of development!\n<a style='font-weight:700' href='https:\/\/github.com\/LilDataScientist'> Code on GitHub! <\/a><\/p>","d6e852f7":"## Sigle prediction\n\nNot let's test our algorothm","92c44ddc":"# Intuition behind Random Forest\n\nRandom forest is an ensemble of decision tree algorithms.\n\nIt is an extension of bootstrap aggregation (bagging) of decision trees and can be used for classification and regression problems.\n\nIn bagging, a number of decision trees are created where each tree is created from a different bootstrap sample of the training dataset. A bootstrap sample is a sample of the training dataset where a sample may appear more than once in the sample, referred to as sampling with replacement.\n\nBagging is an effective ensemble algorithm as each decision tree is fit on a slightly different training dataset, and in turn, has a slightly different performance\n\nA prediction on a regression problem is the average of the prediction across the trees in the ensemble. A prediction on a classification problem is the majority vote for the class label across the trees in the ensemble.","27182eb1":"## How to find good split?\n\nTo find good split you need to know about standart deviation. Basically it is a measure of the amount of variation or dispersion of a set of values. Exaclty what we need. As we want to get more information after every split we need to split data with lower variation or dispersion. For example: if our left split's standart deviation will be very high it means that there are many different values, and we can not be sure what our prediction will be. And when we have low standart deviation that means that all samples are quite similar and then we can take mean of their ouputs for example! \n\nBut how we can compute standart deviation for two parts of data? Well, let's take a sum.","8e97c72d":"## Difference from just bagging\n\nUnlike bagging, random forest also involves selecting a subset of input features (columns or variables) at each split point in the construction of trees. Typically, constructing a decision tree involves evaluating the value for each input variable in the data in order to select a split point. By reducing the features to a random subset that may be considered at each split point, it forces each decision tree in the ensemble to be more different.","3fdef88f":"## MSE depending on number of trees and features\n\nLet's see how does it depen on the trees hyperparameter","4fba2805":"Here we just multiply the standart deviation from the formula above by amount of data in the split! Simple!  \n\nSo, why did we do this? - Imagine that we have split where 990 objects goes to the left and 10 go to the right. standart deviation on left part is close to zero, while standart deviation of the right part is HUGE, but, we don't mind having huge deviation for right part as there are only 10 samples of 1000 and in the left part where we have small deviation we have 990!  \n\nNow look how our new algorithm will solve his problem with the same data!","aa23b4dd":"You can see that it does very bad split. But we did not do any mistake. But we can fix it! The problem here is that algorithm splits the data without respect to the amount of data in split! For example, in previous left split we had oly 3 samples, while in the right split we have almost 100! And our goal is to find split with the lower standart deviation and maximum amount of data in it! \nSo, let's penalize errors where small amount of samples! ","7f6ef4e5":"<div style='text-align: center'>\n    <img src='https:\/\/i.postimg.cc\/L41S8ds2\/5a1d57465fef77-573085751511872326393.png' width='300' \/>\n<\/div>","1d46ebbf":"# Understanding how a decision tree works\n\nA decision tree consists of creating different rules by which we make the prediction.\nAs you can see, decision trees usually have sub-trees that serve to fine-tune the prediction of the previous node. This is so until we get to a node that does not split. This last node is known as a leaf node or leaf node. ","503a6451":"![Untitled Diagram.drawio (1).svg](attachment:d5952a59-5c59-425c-9c37-0d183d5ba2b2.svg)","504eb956":"## Sigle prediction\n\nNot let's test our algorothm","d62e3e2d":"Then let's apply our algorithm (we will map throw each value we can separate parts with and search for the best split with the lowest error)","41cfa19c":"## Decision Tree Regressor\n\nKey point of regression task for decision tree is that we want to return mean value on the leaves","869b15c2":"$$\\Large\nQ = \\frac{|R_l|}{|R_m|}H(R_l) + \\frac{|R_r|}{|R_m|} H(R_r) \n$$","72826cac":"Where $R_l$ is left part, and $R_r$ is right part and $H(x) = \\text{standart deviation of x}$ \n\nNow, let's see how this formula can help us to find better split and you will probably get sense of what's going on under the hood!  \n\nLet's generate some random data:"}}