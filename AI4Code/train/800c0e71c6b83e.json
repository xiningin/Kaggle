{"cell_type":{"91ef20fa":"code","e61d4c19":"code","0bc174c2":"code","c05f7593":"code","d70eddbb":"code","a03962a0":"code","1a113fa9":"code","82436d8a":"code","dc84aaee":"code","f67663fe":"code","56988510":"code","a1bf5247":"code","8896cfd9":"code","18c36769":"code","2e03bb42":"code","6558371d":"code","eabf625f":"code","88e978fc":"code","a3a100a6":"code","1cca8ad9":"code","73756dd3":"code","2c19adf7":"code","16151109":"code","0331b7b0":"code","69aa3991":"code","462499ca":"code","ec2de1da":"code","03f9e96f":"code","d1da3cce":"code","d885011a":"code","d0a15324":"code","97499699":"code","ad611ace":"code","bc2f29b8":"code","43e49274":"code","b0a59e1d":"code","8891ab6e":"code","f4d06e37":"code","7629d150":"code","2332d8c1":"code","2e7badc6":"code","ef73d75c":"code","4b1ab258":"code","2a8d1569":"code","5c5a94f3":"code","c7decf24":"code","62dca4f7":"code","3c6ace96":"code","4c5499dd":"markdown","f802d8c1":"markdown","eeba5cb1":"markdown","bb6c5a29":"markdown","eaf2b68a":"markdown","c7c62a1d":"markdown","4afc2099":"markdown","d4608586":"markdown","11d7d956":"markdown","34b39595":"markdown","d5095fbc":"markdown","f0985f36":"markdown","29389f16":"markdown","a91152f1":"markdown","98b6795a":"markdown","d038b53c":"markdown","f2341648":"markdown","7ea33a98":"markdown","a9886e1e":"markdown","ebfeb992":"markdown","b882b36a":"markdown","a79a0610":"markdown"},"source":{"91ef20fa":"import warnings\nwarnings.filterwarnings('ignore',category=DeprecationWarning)\n\n# import libraries\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport matplotlib.pyplot as plt\nimport re,random,os\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nimport string\nfrom pprint import pprint as pprint\n\n# spacy for basic processing, optional, can use nltk as well(lemmatisation etc.)\nimport spacy\n\n#gensim for LDA\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\n#plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim #dont skip this\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","e61d4c19":"spacy.cli.download(\"en\")","0bc174c2":"df=pd.read_csv('..\/input\/amazon-reviews-data\/Amazon Reviews.csv')\ndf.head()","c05f7593":"df.info()","d70eddbb":"df[df['asins']=='B01BH83OOM']['keys'][852]","a03962a0":"df[df['asins']=='B01BH83OOM']['name'][854]","1a113fa9":"# filter for product id = amazon tap\ndf = df[df['asins']==\"B01BH83OOM\"]\ndf.head(3)","82436d8a":"# tokenize using gensims simple_preprocess\ndef sent_to_words(sentences, deacc=True):  # deacc=True removes punctuations\n    for sentence in sentences:\n        yield(simple_preprocess(str(sentence)))\n\n# conver to list\ndata=df['reviews.text'].values.tolist()\ndata_words=list(sent_to_words(data))\n\n#sample\nprint(data_words[3])","dc84aaee":"# create a list of stop words\n# string.punctuation (from the 'string' module) contains a list of punctuations\nfrom nltk.corpus import stopwords\nstop_words= stopwords.words('english') + list(string.punctuation)","f67663fe":"# functions for removing stopwords and lemmatization\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef lemmatization(texts,allowed_postags=['NOUN','ADJ','VERB','ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out=[]\n    for sent in texts:\n        doc=nlp(' '.join(sent))\n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","56988510":"\n# call functions\n\n# remove stop words\ndata_words_npstops= remove_stopwords(data_words)\n\n# initialize spacy 'en' model use only tagger since we don;t need parsing or NER\n# python3 -m spacey download en\n# spacy.cli.download(\"en\")\nnlp=spacy.load('en',disable=['parser', 'ner'])\n\n# lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized=lemmatization(data_words_npstops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[3])","a1bf5247":"# compare the nostop, lemmatised version with the original one\n# note that speakers is lemmatised to speaker; \nprint(' '.join(data_words[3]), '\\n')\nprint(' '.join(data_lemmatized[3]))","8896cfd9":"# create dictionary and corpus\n# create dictionary\nid2word=corpora.Dictionary(data_lemmatized)\n\n#create corpus\ncorpus=[id2word.doc2bow(text) for text in data_lemmatized]\n\n# sample\nprint(corpus[2])","18c36769":"# human-readable format of corpus (term-frequency)\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","2e03bb42":"# Build LDA model\nlda_model= gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=id2word,num_topics=10,random_state=100,\\\n                                          update_every=1,chunksize=100,passes=10,alpha='auto',per_word_topics=True)","6558371d":"# print the 10 topics\nlda_model.print_topics()","eabf625f":"# coherence score\ncoherence_model_lda=CoherenceModel(model=lda_model,texts=data_lemmatized,dictionary=id2word,coherence='c_v')\ncoherence_lda=coherence_model_lda.get_coherence()\nprint('\\nCoherence Score:',coherence_lda)","88e978fc":"# visulaise the topics\npyLDAvis.enable_notebook()\nvis=pyLDAvis.gensim.prepare(lda_model,corpus,id2word)\nvis","a3a100a6":"# compute coherence value at various values of alpha and num_topics\ndef compute_coherence_values(dictionary, corpus, texts, num_topics_range,alpha_range):\n    coherence_values=[]\n    model_list=[]\n    for alpha in alpha_range:\n        for num_topics in num_topics_range:\n            lda_model= gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, alpha=alpha,num_topics=num_topics,\\\n                                                      per_word_topics=True)\n            model_list.append(lda_model)\n            coherencemodel=CoherenceModel(model=lda_model,texts=texts,dictionary=dictionary,coherence='c_v')\n            coherence_values.append((alpha,num_topics,coherencemodel.get_coherence()))\n    return model_list,coherence_values","1cca8ad9":"# build models accross a range of num_topics and alpha\nnum_topics_range= [2,6,10,15,20]\nalpha_range=[0.01,0.1,1]\nmodel_list, coherence_values= compute_coherence_values(dictionary=id2word,corpus=corpus,texts=data_lemmatized,\\\n                                                       num_topics_range=num_topics_range,alpha_range=alpha_range)","73756dd3":"coherence_df = pd.DataFrame(coherence_values, columns=['alpha', 'num_topics', 'coherence_value'])\ncoherence_df","2c19adf7":"# plot\ndef plot_coherence(coherence_df,alpha_range,num_topics_range):\n    plt.figure(figsize=(16,6))\n    \n    for i,val in enumerate(alpha_range):\n        #subolot 1\/3\/i\n        plt.subplot(1,3,i+1)\n        alpha_subset=coherence_df[coherence_df['alpha']==val]\n        plt.plot(alpha_subset['num_topics'],alpha_subset['coherence_value'])\n        plt.xlabel('num_topics')\n        plt.ylabel('Coherence Value')\n        plt.title('alpha={0}'.format(val))\n        plt.ylim([0.30,1])\n        plt.legend('coherence value', loc='upper left')\n        plt.xticks(num_topics_range)\nplot_coherence(coherence_df,alpha_range,num_topics_range)","16151109":"df=pd.read_csv('..\/input\/demonitization-tweets-in-india\/demonetization-tweets.csv',encoding=\"ISO-8859-1\")\ndf.head()","0331b7b0":"df.info()","69aa3991":"# see randomly chosen sample tweets\ndf.text[random.randrange(len(df.text))]","462499ca":"df.text[:10]","ec2de1da":"# remove URLs\ndef remove_URL(x):\n    return x.replace(r'https[a-zA-Z0-9]*',\"\",regex=True)\n\n#clean tweet text\ndef clean_tweets(tweet_col):\n    df=pd.DataFrame({'tweet':tweet_col})\n    df['tweet']=df['tweet'].replace(r'\\'|\\\"|\\,|\\.|\\?|\\+|\\-|\\\/|\\=|\\(|\\)|\\n|\"', '', regex=True)\n    df['tweet']=df['tweet'].replace(\"  \",\" \")\n    df['tweet']=df['tweet'].replace(r'@[a-zA-Z0-9]*', '', regex=True)\n    df['tweet']=remove_URL(df['tweet'])\n    df['tweet']=df['tweet'].str.lower()\n    \n    return(df)\n\ncleaned_tweets=clean_tweets(df.text)\ncleaned_tweets[:10]","03f9e96f":"words_remove = [\"ax\",\"i\",\"you\",\"edu\",\"s\",\"t\",\"m\",\"subject\",\"can\",\"lines\",\"re\",\"what\", \"there\",\n                    \"all\",\"we\",\"one\",\"the\",\"a\",\"an\",\"of\",\"or\",\"in\",\"for\",\"by\",\"on\",\"but\",\"is\",\"in\",\n                    \"a\",\"not\",\"with\",\"as\",\"was\",\"if\",\"they\",\"are\",\"this\",\"and\",\"it\",\"have\",\"from\",\"at\",\n                    \"my\",\"be\",\"by\",\"not\",\"that\",\"to\",\"from\",\"com\",\"org\",\"like\",\"likes\",\"so\",\"said\",\"from\",\n                    \"what\",\"told\",\"over\",\"more\",\"other\",\"have\",\"last\",\"with\",\"this\",\"that\",\"such\",\"when\",\n                    \"been\",\"says\",\"will\",\"also\",\"where\",\"why\",\"would\",\"today\", \"in\", \"on\", \"you\", \"r\", \"d\", \n                    \"u\", \"hw\",\"wat\", \"oly\", \"s\", \"b\", \"ht\", \"rt\", \"p\",\"the\",\"th\", \"lol\", ':']\n\n#remove stop words, punctuations\nstop_words = set(list(stopwords.words('english') + list(string.punctuation)+words_remove))\ndata_words= list(sent_to_words(cleaned_tweets.tweet.values.tolist(), deacc=False))\n\n# remove stopwords\ndef remove_stopwords(texts, stop_words=stop_words):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndata_words_nostops = remove_stopwords(data_words)\n\n\n# spacy for lemmatization\nnlp = spacy.load('en', disable=['parser', 'ner'])\ndata_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])","d1da3cce":"# sample lemmatized tweets\ndata_lemmatized[:3]","d885011a":"# create dictionary and corpus\nid2word= corpora.Dictionary(data_lemmatized)\ntexts=data_lemmatized\ncorpus=[id2word.doc2bow(text) for text in texts]","d0a15324":"# build models across a range of num_topics and alpha\nnum_topics_range=[2,6,10,15]\nalpha_range=[0.01,0.1,1]\nmodel_list, coherence_values=compute_coherence_values(dictionary=id2word,corpus=corpus,texts=data_lemmatized,\\\n                                                      num_topics_range=num_topics_range,\\\n                                                     alpha_range=alpha_range)\ncoherence_df=pd.DataFrame(coherence_values,columns=['alpha','num_topics','coherence_value'])\ncoherence_df","97499699":"# plot\nplot_coherence(coherence_df,alpha_range,num_topics_range)","ad611ace":"# Build LDA model with alpha=0.1 and 10 topics\nlda_model= gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=10, random_state=100,\\\n                                          update_every=1,chunksize=100, passes=10, alpha=0.1, per_word_topics=True)","bc2f29b8":"# print keywords \npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","43e49274":"# coherence score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('Coherence Score: ', coherence_lda)","b0a59e1d":"# Visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis","8891ab6e":"# Reading the BBC news dataset\ndata_folder=\"..\/input\/bbc-news-summary\/BBC News Summary\/News Articles\"\nfolders=[\"business\",\"entertainment\",\"politics\",\"sport\",\"tech\"]\nx=[]\ny=[]\n\n\nfor i in folders:\n    files=os.listdir(data_folder+'\/'+i)\n    for text_file in files:\n        file_path=data_folder + '\/'+i+'\/'+text_file\n        with open(file_path,'rb') as f:\n            data=f.read()\n        x.append(data)\n        y.append(i)\n        \ndata={'news':x,'type':y}\ndf = pd.DataFrame(data)\ndf.to_csv('bbc_data.csv', index=False)        ","f4d06e37":"df.head()","7629d150":"df.type.astype('category').value_counts()","2332d8c1":"# filter business articles\ndf = df[df['type']=='business']\ndf.shape","2e7badc6":"# convert to list\ndata=df['news'].values.tolist()\ndata_words=list(sent_to_words(data))\n\n# remove stop words\ndata_words_nostops= remove_stopwords(data_words)\n\n#lemmatize\nnlp = spacy.load('en', disable=['parser','ner'])\ndata_lemmatized = lemmatization(data_words_nostops,allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\n# create dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n#create corpus\ntexts = data_lemmatized\n\n#term document frequency\ncorpus = [id2word.doc2bow(text) for text in texts]","ef73d75c":"# Build LDA model\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=20, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","4b1ab258":"# Compute coherence score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","2a8d1569":"# print keywords\npprint(lda_model.print_topics())\n","5c5a94f3":"doc_lda = lda_model[corpus]\n","c7decf24":"# Visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis","62dca4f7":"# build models across a range of num_topics and alpha\nnum_topics_range=[2,6,10,15]\nalpha_range=[0.01,0.1,1]\nmodel_list, coherence_values=compute_coherence_values(dictionary=id2word,corpus=corpus,texts=data_lemmatized,\\\n                                                      num_topics_range=num_topics_range,\\\n                                                     alpha_range=alpha_range)\ncoherence_df=pd.DataFrame(coherence_values,columns=['alpha','num_topics','coherence_value'])\ncoherence_df","3c6ace96":"# plot\nplot_coherence(coherence_df, alpha_range, num_topics_range)","4c5499dd":"Let's now evaluate the model using coherence score.","f802d8c1":"Let's now filter the dataframe to only one product - Amazon Tap. If you are not aware of Tap, <a href=\"https:\/\/www.amazon.com\/dp\/B01BH83OOM?ref=ODS_HA_F_surl\">here's the amazon page<\/a>.","eeba5cb1":"Let's now build the topic model. We'll define 10 topics to start with. The hyperparameter `alpha` affects sparsity of the document-topic\n(theta) distributions, whose default value is 1. Similarly, the hyperparameter `eta` can also be specified, which affects the topic-word distribution's sparsity.\n\n","bb6c5a29":"**Important Note:** All models are not automatically downloaded with spacy, so you will need to do a ```python -m spacy download en``` to use its preprocessing methods.","eaf2b68a":"**Topic modeling is a powerful technique in natural language processing to find hidden meaning from the text body. It uses the concept of Latent Dirichlet Allocation (LDA).**\n","c7c62a1d":"`The (3, 7) above represents the fact that the word with id=3 appears 7 times in the second document (review), word id 12 appears twice and so on. The nested list below shows the frequencies of words in the first document.`","4afc2099":"Now lets visualise the topics. The `pyLDAvis` library comes with excellent interactive visualisation capabilities.","d4608586":"`The code below creates a list of stop words. The 'string' module in python comes with a list of punctuation characters, which we'll append to the builtin stopwords of NLTK.`","11d7d956":"Note that many tweets have strings such as RT, @xyz, etc. Some have URLs, punctuation marks, smileys etc. The following code cleans the data to handle many of these issues.","34b39595":"### Preprocessing\n\nLet's first do some preprocessing. For tokenisation, though one can use NLTK as well, let's try using gensim's ```simple_preprocess``` this time. The preprocessing pipeline is mentioned below.<br>\n\n1. Tokenize each review (using gensim)\n2. Remove stop words (including punctuations)\n3. Lemmatize (using spacy)\n\nThough you can build topic models without lemmatisation, it is actually quite important (and highly recommended) because otherwise you may end up getting topics having similar words for e.g. *speaker, speakers* etc. (which are basically referring to the same thing - speaker).\n\nNote that lemmatization uses POS tags of words, so we need to specify a list of POS tags - here we've used ```['NOUN', 'ADJ', 'VERB', 'ADV']``` .","d5095fbc":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:cyan' role=\"tab\" aria-controls=\"home\">----> Topic Modelling NLP-Amazon reviews,BBC news, Demonatization Tweets in India<\/h3>\n","f0985f36":"In this notebook, we will build LDA models on various datasets. We'll use the gensim implementation of LDA, though sklearn also comes with one. \n\nWe will also use the library spacy for preprocessing (specifically lemmatisation). Though you can also perform lemmatisation in NLTK, it is slightly more convenient and less verbose in spacy. For visualising the topics and the word-topic distributions (interactively!), we'll use the 'pyLDAvis' module.","29389f16":"Let's now print the topics found in the dataset.","a91152f1":"## Hyperparameter Tuning - Number of Topics and Alpha\n\nLet's now tune the two main hyperparameters - number of topics and alpha. The strategy typically used is to tune these parameters such that the coherence score is maximised.","98b6795a":"### Creating Dictionary and Corpus\n\n`Gensim's LDA requires the data in a certain format. Firstly, it needs the corpus as a dicionary of id-word mapping, where each word has a unique numeric ID. This is for computationally efficiency purposes. Secondly, it needs the corpus as a term-document frequency matrix which contains the frequency of each word in each document.`","d038b53c":"## Amazon Product Reviews Dataset - Tap\n\nFor building topic models, let's experiment with the Amazon product reviews dataset. We have a list of reviews of a few amazon products such as Kindle, Echo (Alexa) etc. ","f2341648":"Since tweets often contain slang words such as *wat, rt, lol* etc, we can append the stopwords with a list of such custom words and remove them.","7ea33a98":"\n<img src=\"https:\/\/miro.medium.com\/max\/4800\/1*cDwKSHmfp5awjqjobV707g.png\" \/>","a9886e1e":"### Demonetisation Tweets\n\nLet's now try identifying topics in tweets related to a specific topic - demonetisation. The overall pipeline is the same, apart from someextra preprocessing steps.","ebfeb992":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:cyan' role=\"tab\" aria-controls=\"home\"><center><b>If you found this notebook helpful , some upvotes would be very much appreciated - That will keep me motivated :)<\/b><\/center><\/h2>\n","b882b36a":"### BBC News Dataset","a79a0610":"### Building the Topic Model"}}