{"cell_type":{"751787b4":"code","6a2f7ef4":"code","6b8383bd":"code","fad783fb":"code","75c633e4":"code","8359c9d3":"code","c608a648":"code","2900f6cb":"code","73627727":"code","941eae94":"code","dafbcc33":"code","470af344":"code","c9e6321b":"code","f6de7abc":"code","d033cb7a":"code","59b98ad2":"code","e5409e36":"code","c6035e14":"code","a1b37e9d":"code","5ebc9cbc":"markdown","be3a6cd9":"markdown","0f086bf8":"markdown","1471835a":"markdown","cc4812b5":"markdown","7102d7f9":"markdown"},"source":{"751787b4":"!pip install torchtext","6a2f7ef4":"import csv\nimport numpy as np\nimport random\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchtext.legacy import data\nfrom transformers import BertTokenizer, BertModel\n","6b8383bd":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","fad783fb":"df = pd.DataFrame({'marketplace': pd.Series([], dtype='str'),\n              'customer_id': pd.Series([], dtype='str'),\n              'review_id': pd.Series([], dtype='str'),\n              'product_id': pd.Series([], dtype='str'),\n              'product_parent': pd.Series([], dtype='str'),\n              'product_title': pd.Series([], dtype='str'),\n              'product_category': pd.Series([], dtype='str'),\n              'star_rating': pd.Series([], dtype='int'),\n              'helpful_votes': pd.Series([], dtype='int'),\n              'total_votes': pd.Series([], dtype='int'),\n              'vine': pd.Series([], dtype='str'),\n              'verified_purchase': pd.Series([], dtype='str'),\n              'review_headline': pd.Series([], dtype='str'),\n              'review_body': pd.Series([], dtype='str'),\n              'review': pd.Series([], dtype='str'),\n              'review_date': pd.Series([], dtype='float')})\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    filenames.sort()\n    for filename in filenames:\n        size = 100\n        data = pd.read_csv(os.path.join(dirname, filename), nrows=size, sep='\\t', header=0, quoting=csv.QUOTE_NONE)\n        data['review'] = data['review_headline'] + ' ' + data['review_body']\n        df = df.append(data, ignore_index=True)\nprint(df.shape)\nprint(df.info())\ndf.head()","75c633e4":"sns.countplot(df.star_rating)\nplt.xlabel('star rating');","8359c9d3":"def to_sentiment(rating):\n    rating = int(rating)\n    if rating <= 2:\n        return 0\n    elif rating == 3:\n        return 1\n    else:\n        return 2\ndf['sentiment'] = df.star_rating.apply(to_sentiment)\nclass_names = ['negative', 'neutral', 'positive']\nax = sns.countplot(df.sentiment)\nplt.xlabel('review sentiment')\nax.set_xticklabels(class_names);","c608a648":"PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\ntokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\ntoken_lens = []\nfor txt in df.review_body:\n  tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n  token_lens.append(len(tokens))\nsns.distplot(token_lens)\nplt.xlim([0, 256]);\nplt.xlabel('Token count');","2900f6cb":"MAX_LEN = 128","73627727":"class GPReviewDataset(Dataset):\n    def __init__(self, reviews, targets, tokenizer, max_len):\n        self.reviews = reviews\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    def __len__(self):\n        return len(self.reviews)\n    def __getitem__(self, item):\n        review = str(self.reviews[item])\n        target = self.targets[item]\n        encoding = self.tokenizer.encode_plus(review,\n                                              add_special_tokens=True,\n                                              max_length=self.max_len,\n                                              return_token_type_ids=False,\n                                              padding='max_length',\n                                              return_attention_mask=True,\n                                              return_tensors='pt',\n                                              truncation=True)\n        return {'review_text': review,\n                'input_ids': encoding['input_ids'].flatten(),\n                'attention_mask': encoding['attention_mask'].flatten(),\n                'targets': torch.tensor(target, dtype=torch.long)}","941eae94":"df_train, df_test = train_test_split(\n  df,\n  test_size=0.1,\n  random_state=RANDOM_SEED\n)\ndf_val, df_test = train_test_split(\n  df_test,\n  test_size=0.5,\n  random_state=RANDOM_SEED\n)\ndf_train.shape, df_val.shape, df_test.shape","dafbcc33":"# https:\/\/curiousily.com\/posts\/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python\/\ndef create_data_loader(df, tokenizer, max_len, batch_size):\n    ds = GPReviewDataset(\n        reviews=df.review_body.to_numpy(),\n        targets=df.sentiment.to_numpy(),\n        tokenizer=tokenizer,\n        max_len=max_len)\n    return DataLoader(\n        ds,\n        batch_size=batch_size,\n        num_workers=4)\nBATCH_SIZE = 16\ntrain_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\nval_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\ntest_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)","470af344":"data = next(iter(train_data_loader))\ndata.keys()\nprint(data['input_ids'])\nprint(data['attention_mask'])\nprint(data['targets'])","c9e6321b":"class SentimentClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super(SentimentClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n    def forward(self, input_ids, attention_mask):\n        _, pooled_output = self.bert(input_ids=input_ids,\n                                     attention_mask=attention_mask)\n        print(pooled_output)\n        output = self.drop(pooled_output)\n        return self.out(output)","f6de7abc":"model = SentimentClassifier(len(class_names))\nmodel = model.to(device)","d033cb7a":"input_ids = data['input_ids'].to(device)\nattention_mask = data['attention_mask'].to(device)\nprint(input_ids.shape) # batch size x seq length\nprint(attention_mask.shape) # batch size x seq length","59b98ad2":"EPOCHS = 10\noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\ntotal_steps = len(train_data_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)\nloss_fn = nn.CrossEntropyLoss().to(device)","e5409e36":"def train_epoch(model,\n                data_loader,\n                loss_fn,\n                optimizer,\n                device,\n                scheduler,\n                n_examples):\n    model = model.train()\n    losses = []\n    correct_predictions = 0\n    for d in data_loader:\n        input_ids = d['input_ids'].to(device)\n        attention_mask = d['attention_mask'].to(device)\n        targets = d['targets'].to(device)\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n    return correct_predictions.double() \/ n_examples, np.mean(losses)","c6035e14":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval()\n    losses = []\n    correct_predictions = 0\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d['input_ids'].to(device)\n            attention_mask = d['attention_mask'].to(device)\n            targets = d['targets'].to(device)\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n    return correct_predictions.double() \/ n_examples, np.mean(losses)","a1b37e9d":"%%time\nhistory = defaultdict(list)\nbest_accuracy = 0\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}\/{EPOCHS}')\n    print('-' * 10)\n    train_acc, train_loss = train_epoch(model,\n                                        train_data_loader,\n                                        loss_fn,\n                                        optimizer,\n                                        device,\n                                        scheduler,\n                                        len(df_train))\n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    val_acc, val_loss = eval_model(model,\n                                   val_data_loader,\n                                   loss_fn,\n                                   device,\n                                   len(df_val))\n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n    print()\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    if val_acc > best_accuracy:\n        torch.save(model.state_dict(), 'best_model_state.bin')\n        best_accuracy = val_acc","5ebc9cbc":"## Jupyter Configurations","be3a6cd9":"## Clean the data","0f086bf8":"## Pytorch Dataset","1471835a":"## Imports","cc4812b5":"Most of the reviews seem to contain less than 128 tokens.","7102d7f9":"## Training"}}