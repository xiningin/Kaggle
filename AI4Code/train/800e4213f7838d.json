{"cell_type":{"f79c1fbf":"code","0b18d9e7":"code","4ec625e8":"code","1c07e308":"code","e95244ed":"code","e78407d0":"code","03183bf4":"code","9875693f":"markdown","38575306":"markdown","78284128":"markdown"},"source":{"f79c1fbf":"from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics","0b18d9e7":"iris = load_iris()\n# create X (features) and y (response)\nX = iris.data\ny = iris.target","4ec625e8":"# use train\/test split with different random_state values\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)\n\n# check classification accuracy of KNN with K=5\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))","1c07e308":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=5, shuffle=False).split(range(10))\n\n# print the contents of each training and testing set\nprint('{} {:^1} {}'.format('Iteration','\\t Training set observations',     '\\t Testing set observations'))\nfor iteration, data in enumerate(kf, start=1):\n    print('{:^9} \\t \\t {} \\t \\t{:^10}'.format(iteration, data[0], str(data[1])))","e95244ed":"from sklearn.model_selection import cross_val_score\nknn = KNeighborsClassifier(n_neighbors=5)\nscores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\nprint(scores)\nprint('-'*20)\nprint(scores.mean())","e78407d0":"# search for an optimal value of K for KNN\nk_range = list(range(1, 31))\nk_scores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n    k_scores.append(scores.mean())\nprint(k_scores)","03183bf4":"import matplotlib.pyplot as plt\nplt.plot(k_scores)\nplt.xlabel('Kscores')\nplt.ylabel('Accuracy')","9875693f":"\nComparing cross-validation to train\/test split\n\nAdvantages of cross-validation:\n\n    More accurate estimate of out-of-sample accuracy\n    More \"efficient\" use of data (every observation is used for both training and testing)\n\nAdvantages of train\/test split:\n\n    Runs K times faster than K-fold cross-validation\n    Simpler to examine the detailed results of the testing process\n\nFor classification problems, stratified sampling is recommended for creating the folds\n\n    Each response class should be represented with equal proportions in each of the K folds\n    scikit-learn's cross_val_score function does this by default\n\n","38575306":"\n\nQuestion: What if we created a bunch of train\/test splits, calculated the testing accuracy for each, and averaged the results together?\n\nAnswer: That's the essense of cross-validation!\n\n\nSteps for K-fold cross-validation\n\n    Split the dataset into K equal partitions (or \"folds\").\n    Use fold 1 as the testing set and the union of the other folds as the training set.\n    Calculate testing accuracy.\n    Repeat steps 2 and 3 K times, using a different fold as the testing set each time.\n    Use the average testing accuracy as the estimate of out-of-sample accuracy.\n\nin leyman terms, we are selecting a number (K), and we are dividing the datset in to K chunks.\nand in a lopp K time, we are training our model K-1 times and testing the score using the 1 chunk we left in K-1.\n\nafter the loop gets ended we are summing the K scores and diving it by K.\nand thus getting the average score\n","78284128":"Improvements to cross-validation\n\nRepeated cross-validation\n\n    Repeat cross-validation multiple times (with different random splits of the data) and average the results\n    More reliable estimate of out-of-sample performance by reducing the variance associated with a single trial of cross-validation\n\nCreating a hold-out set\n\n    \"Hold out\" a portion of the data before beginning the model building process\n    Locate the best model using cross-validation on the remaining data, and test it using the hold-out set\n    More reliable estimate of out-of-sample performance since hold-out set is truly out-of-sample\n\nFeature engineering and selection within cross-validation iterations\n\n    Normally, feature engineering and selection occurs before cross-validation\n    Instead, perform all feature engineering and selection within each cross-validation iteration\n    More reliable estimate of out-of-sample performance since it better mimics the application of the model to out-of-sample data\n\n"}}