{"cell_type":{"22335c8d":"code","72a234f7":"code","0472bfcd":"code","28b6a326":"code","9cb54ef6":"code","5a63f4ca":"code","c5f03f9d":"code","397655ea":"code","c76c668f":"code","ba23e692":"code","b29cd117":"code","1f5a79b6":"code","50dd5fb4":"code","ea76b59a":"code","f7c24fdd":"code","4cc6ad4d":"code","a7a8b70b":"code","28745dd0":"code","f2b8696b":"code","10b581a3":"code","f5888c46":"code","5020d5ff":"code","35dca6ca":"code","e1658f3e":"markdown","4db5e928":"markdown","c30b6ad8":"markdown","2d81b4d0":"markdown","2304ad2e":"markdown","d6234963":"markdown","0672e9e6":"markdown","54c06705":"markdown","86e7a4f5":"markdown","f3b14382":"markdown","9f2f4d58":"markdown","6a132f3c":"markdown","61563c33":"markdown","35ffc1df":"markdown","eca20ddb":"markdown","611e42ef":"markdown","d219e392":"markdown","d1ef1502":"markdown","748f7b57":"markdown"},"source":{"22335c8d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","72a234f7":"import nltk","0472bfcd":"from nltk.tokenize import sent_tokenize\ntext=\"\"\"Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.\nThe sky is pinkish-blue. You shouldn't eat cardboard\"\"\"\ntokenized_text=sent_tokenize(text)\nprint(tokenized_text)","28b6a326":"from nltk.tokenize import word_tokenize\ntokenized_word=word_tokenize(text)\nprint(tokenized_word)","9cb54ef6":"from nltk.probability import FreqDist\nfdist = FreqDist(tokenized_word)\nprint(fdist)\nprint(\"2 most common :- \",fdist.most_common(2))","5a63f4ca":"# Frequency Distribution Plot\nimport matplotlib.pyplot as plt\nfdist.plot(30,cumulative=False)\nplt.show()","c5f03f9d":"from nltk.corpus import stopwords\nstop_words=set(stopwords.words(\"english\"))\nprint(stop_words)","397655ea":"from nltk.tokenize import word_tokenize\ntext=\"\"\"Hello Mr. Smith, how are you doing today?\"\"\"\ntokenized_sent=word_tokenize(text)\n################3\nfiltered_sent=[]\nfor w in tokenized_sent:\n    if w not in stop_words:\n        filtered_sent.append(w)\nprint(\"Tokenized Sentence:\",tokenized_sent)\nprint(\"Filterd Sentence:\",filtered_sent)","c76c668f":"# Stemming\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\nps = PorterStemmer()\n\nstemmed_words=[]\nfor w in filtered_sent:\n    stemmed_words.append(ps.stem(w))\n\nprint(\"Filtered Sentence:\",filtered_sent)\nprint(\"Stemmed Sentence:\",stemmed_words)","ba23e692":"#Lexicon Normalization\n#performing stemming and Lemmatization\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlem = WordNetLemmatizer()\n\nfrom nltk.stem.porter import PorterStemmer\nstem = PorterStemmer()\n\nword = \"flying\"\nprint(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\nprint(\"Stemmed Word:\",stem.stem(word))","b29cd117":"# Sentence Tokenization\nsent = \"Albert Einstein was born in Ulm, Germany in 1879.\"\ntokens=nltk.word_tokenize(sent)\nprint(tokens)\n# POS Tagging\nnltk.pos_tag(tokens)","1f5a79b6":"# Import pandas\nimport pandas as pd","50dd5fb4":"data=pd.read_csv('..\/input\/train.tsv', sep='\\t')\nprint(data.head())","ea76b59a":"print(data.info())","f7c24fdd":"print(data.Sentiment.value_counts())","4cc6ad4d":"import matplotlib.pyplot as plt\nSentiment_count=data.groupby('Sentiment').count()\nplt.bar(Sentiment_count.index.values, Sentiment_count['Phrase'])\nplt.xlabel('Review Sentiments')\nplt.ylabel('Number of Review')\nplt.show()","a7a8b70b":"from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import RegexpTokenizer\n#tokenizer to remove unwanted elements from out data like symbols and numbers\ntoken = RegexpTokenizer(r'[a-zA-Z0-9]+')\ncv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\ntext_counts= cv.fit_transform(data['Phrase'])\n","28745dd0":"print(type(text_counts))\nprint(text_counts.shape)\n#print(text_counts.toarray())\nprint(text_counts.toarray())\n#print(cv.vocabulary_)","f2b8696b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(text_counts, data['Sentiment'], test_size=0.3, random_state=1)","10b581a3":"from sklearn.naive_bayes import MultinomialNB\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n# Model Generation Using Multinomial Naive Bayes\nclf = MultinomialNB().fit(X_train, y_train)\npredicted= clf.predict(X_test)\nprint(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))","f5888c46":"from sklearn.feature_extraction.text import TfidfVectorizer\ntf=TfidfVectorizer()\ntext_tf= tf.fit_transform(data['Phrase'])","5020d5ff":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    text_tf, data['Sentiment'], test_size=0.3, random_state=123)","35dca6ca":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n# Model Generation Using Multinomial Naive Bayes\nclf = MultinomialNB().fit(X_train, y_train)\npredicted= clf.predict(X_test)\nprint(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))","e1658f3e":"# Split Train and Test Set\n\nTo understand model performance, dividing the dataset into a training set and a test set is a good strategy.\n\nLet's split dataset by using function train_test_split(). You need to pass basically 3 parameters features, target, and test_set size. Additionally, you can use random_state to select records randomly.","4db5e928":"# Word Tokenization\n\nWord tokenizer breaks text paragraph into words.","c30b6ad8":"# Sentiment Analysis of Movie Reviews\n\nIn the model the building part, you can use the \"Sentiment Analysis of Movie, Reviews\" dataset available on Kaggle. The dataset is a tab-separated file. Dataset has four columns PhraseId, SentenceId, Phrase, and Sentiment.\n\nThis data has 5 sentiment labels:\n\n0 - negative 1 - somewhat negative 2 - neutral 3 - somewhat positive 4 - positive","2d81b4d0":"# Removing Stopwords","2304ad2e":"# Stopwords\n\nFinding\/listing stopwords in English.","d6234963":"# Tokenization\n\nTokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentence is called Tokenization. Token is a single entity that is building blocks for sentence or paragraph.\n\n# Sentence Tokenization\n\nSentence tokenizer breaks text paragraph into sentences.\n","0672e9e6":"# Frequency Distribution","54c06705":"# POS Tagging\n\nThe primary target of Part-of-Speech(POS) tagging is to identify the grammatical group of a given word. Whether it is a NOUN, PRONOUN, ADJECTIVE, VERB, ADVERBS, etc. based on the context. POS Tagging looks for relationships within the sentence and assigns a corresponding tag to the word.","86e7a4f5":"# Lemmatization\n\nLemmatization reduces words to their base word, which is linguistically correct lemmas. It transforms root word with the use of vocabulary and morphological analysis. Lemmatization is usually more sophisticated than stemming. Stemmer works on an individual word without knowledge of the context. For example, The word \"better\" has \"good\" as its lemma. This thing will miss by stemming because it requires a dictionary look-up.\n","f3b14382":"# Lexicon Normalization\n\nLexicon normalization considers another type of noise in the text. For example, connection, connected, connecting word reduce to a common word \"connect\". It reduces derivationally related forms of a word to a common root word.\n\n# Stemming\n\nStemming is a process of linguistic normalization, which reduces words to their word root word or chops off the derivational affixes. For example, connection, connected, connecting word reduce to a common word \"connect\".","9f2f4d58":"# Split train and test set (TF-IDF)\n\nLet's split dataset by using function train_test_split(). You need to pass basically 3 parameters features, target, and test_set size. Additionally, you can use random_state to select records randomly.\n\n","6a132f3c":"# Conclusion\n\nWell, you got a classification rate of 58.65% using TF-IDF features, which is not considered as good accuracy. We need to improve the accuracy by using some other preprocessing, feature engineering, or classification approaches. \n\n# Related Articles\nhttps:\/\/www.kaggle.com\/hassanamin\/sentiment-analysis-using-nltk\n\n\n# Note : \n\nDon't forget to like(upvote) the tutorial. Feel free to leave your thoughts in comments,","61563c33":"# Introduction\n\nSentiment analysis refers to the use of natural language processing, text analysis, computational linguistics, and many more to identify and quantify the sentiment of some kind of text or audio.\n\n\u201cYour most unhappy customers are your greatest source of learning.\u201d \n                                            \u2014 Bill Gates\n# Pre-Processing Steps for Sentiment Analysis\n\nWe start with Text preprocessing using NLTK and learn to perform steps such as :-\n\n* Tokenization, \n* Stopwords removals\n* Lemmatization\n* POS Tagging\n\n","35ffc1df":"# Feature Generation using TF-IDF\n\nIn Term Frequency(TF), you just count the number of words occurred in each document. The main issue with this Term Frequency is that it will give more weight to longer documents. Term frequency is basically the output of the BoW model.\n","eca20ddb":"# Load Data","611e42ef":"# Exploratory Data Analysis","d219e392":"# Model Building and Evaluation (TF-IDF)\n\nLet's build the Text Classification Model using TF-IDF.\n\nFirst, import the MultinomialNB module and create the Multinomial Naive Bayes classifier object using MultinomialNB() function.\n\nThen, fit your model on a train set using fit() and perform prediction on the test set using predict().","d1ef1502":"# Model Building and Evaluation\n\nLet's build the Text Classification Model using BoW.\n\nFirst, import the MultinomialNB module and create a Multinomial Naive Bayes classifier object using MultinomialNB() function.\n\nThen, fit your model on a train set using fit() and perform prediction on the test set using predict().","748f7b57":"# Feature Generation using Bag of Words\n\nIn the Text Classification Problem, we have a set of texts and their respective labels. But we directly can't use text for our model. You need to convert these text into some numbers or vectors of numbers.\n\nBag-of-words model(BoW ) is the simplest way of extracting features from the text. BoW converts text into the matrix of occurrence of words within a document. This model concerns about whether given words occurred or not in the document.\n\nCountVectorizer is used to convert a collection of text documents to a matrix of token counts.\n\nThe CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.\n\nYou can use it as follows:\n\na). Create an instance of the CountVectorizer class.\n\nb). Call the fit() function in order to learn a vocabulary from one or more documents.\n\nc). Call the transform() function on one or more documents as needed to encode each as a vector.\n\nAn encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document.\n\nBecause these vectors will contain a lot of zeros, we call them sparse."}}