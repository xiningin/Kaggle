{"cell_type":{"889ca270":"code","a8148879":"code","867f2702":"code","6b461c6d":"code","c3a94d70":"code","470a7eef":"code","fa80b82c":"code","b02f8a08":"code","d702a965":"code","d92499c8":"code","c4b27d48":"code","b021bac5":"code","ca58c131":"code","5d6d2eac":"code","be6dd4b1":"code","fb28f412":"code","480e90aa":"code","965299b4":"code","d01ffd84":"code","3835d0e3":"code","76de8d2a":"code","31d2084b":"code","5cc1b764":"code","8c774ab7":"code","834d6790":"code","8351659a":"markdown","12a7e331":"markdown","d710e800":"markdown","b2d6d0cb":"markdown","4e501bb6":"markdown","9b251c08":"markdown","d70192ae":"markdown","305b00bb":"markdown","2e20d5fe":"markdown","cf0606fa":"markdown","660d3ee4":"markdown","3d6ff314":"markdown","3a3f1adb":"markdown","a44cfb3a":"markdown","86a7f794":"markdown","a4fa9951":"markdown","24cb24cf":"markdown","ac66d6f4":"markdown"},"source":{"889ca270":"# base imports \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# for visualization of the word embeddings\nfrom sklearn.decomposition import PCA\n\n# for building the embeddings & preprocessing\nfrom gensim.models import Word2Vec\nfrom gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, stem_text, strip_numeric\n\n# for modeling\nfrom sklearn import linear_model\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestClassifier","a8148879":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntrain.tail(5)","867f2702":"# clean function cleans all the tweets and splits them into lists of words\ndef clean(tweet):\n    return stem_text(remove_stopwords(strip_numeric(strip_punctuation(tweet)))).split(' ')\n\nsentences = [clean(tweet) for tweet in train['text']]\nprint(sentences[:3])","6b461c6d":"# second step: train the model\nvectors = Word2Vec(sentences, min_count=3)\n# save the model binaries\nvectors.save('tweet_vectors.bin')","c3a94d70":"# load the vectors\nvectors = Word2Vec.load('tweet_vectors.bin')\n# get the summary of the model\nprint('vectors: '+str(vectors))\n# here are some examples of what the vectors look like\nprint('example of the vector for word \"officers\": '+str(vectors[clean('officers')]))","470a7eef":"# let's grab the vectors for the keywords\nkeywords = [clean(x) for x in train['keyword'] if isinstance(x,str)]\n\nflat_keywords = [item for sublist in keywords for item in sublist]\nprint('Number of unique keywords: '+str(len(set(flat_keywords))))","fa80b82c":"keyvectors = vectors[[k for k in flat_keywords if k in vectors.wv.vocab]]\n\npca = PCA(n_components=2)\nresult = pca.fit_transform(keyvectors)","b02f8a08":"plt.scatter(result[:,0],result[:,1])\nfor i, word in enumerate([k for k in flat_keywords if k in vectors.wv.vocab]):\n\tplt.annotate(word, xy=(result[i, 0], result[i, 1]))","d702a965":"def cap_prop(tweet):\n    caps = 0\n    for char in tweet:\n        if char.isupper():\n            caps += 1\n    return float(caps\/len(tweet))\n\ncap_props = [cap_prop(tweet) for tweet in train['text']]\nprint('Capitalization proportions of first tweet: '+str(cap_props[1]))","d92499c8":"# let's look at the difference between the cap_prop for the disaster tweets and \n# the non-disaster tweets\ncap_props_d = [cap_props[p] for p in range(len(cap_props)) if train['target'][p]==1]\ncap_props_nd = [cap_props[p] for p in range(len(cap_props)) if train['target'][p]==0]\n\nplt.figure(figsize=(8,6))\nplt.hist(cap_props_nd,bins=30,alpha=0.5,label=\"Non-Disaster\",color=\"black\")\nplt.axvline(np.mean(cap_props_nd), color='black', linestyle='dashed', linewidth=1)\nplt.hist(cap_props_d,bins=30,alpha=0.5,label=\"Disaster\",color=\"red\")\nplt.axvline(np.mean(cap_props_d), color='red', linestyle='dashed', linewidth=1)\nplt.xlabel(\"Proportion of Capitalized Letters\")\nplt.ylabel(\"Number of Tweets\")\nplt.title(\"Disaster Tweets Have (a few) More Capitalized Letters\")\nplt.legend(loc='upper right')\nplt.show()","c4b27d48":"def hash_prop(tweet):\n    h = 0\n    last_hash = False\n    for char in tweet:\n        if last_hash and char.isspace():\n            last_hash = False\n        if char=='#':\n            last_hash = True\n            h += 1\n    return float(h\/len(tweet.split(' ')))\nhash_props = [hash_prop(tweet) for tweet in train['text']]\nprint('Hashtag proportions of first tweet: '+str(hash_props[1]))","b021bac5":"# we'll look at the same plot for the hashtag proportion\nhash_props_d = [hash_props[p] for p in range(len(hash_props)) if train['target'][p]==1]\nhash_props_nd = [hash_props[p] for p in range(len(hash_props)) if train['target'][p]==0]\n\nplt.figure(figsize=(8,6))\nplt.hist(hash_props_nd,bins=50,alpha=0.5,label=\"Non-Disaster\",color=\"black\")\nplt.axvline(np.mean(hash_props_nd), color='black', linestyle='dashed', linewidth=1)\nplt.hist(hash_props_d,bins=50,alpha=0.5,label=\"Disaster\",color=\"red\")\nplt.axvline(np.mean(hash_props_d), color='red', linestyle='dashed', linewidth=1)\nplt.xlabel(\"Proportion of Hashtags in Tweet\")\nplt.ylabel(\"Number of Tweets\")\n#plt.title(\"Disaster Tweets Have (a few) More Capitalized Letters\")\nplt.legend(loc='upper right')\nplt.xlim(0.0,1.0)\nplt.show()","ca58c131":"def avg_vec(tweet):\n    tweets = clean(tweet)\n    tweets = [t for t in tweets if t in vectors.wv.vocab]\n    if len(tweets) >= 1:\n        return np.mean(vectors[tweets],axis=0)\n    else:\n        return []\n\nv = []\nfor tweet in train['text']: \n    v.append(avg_vec(tweet))\nv[0]","5d6d2eac":"def get_val(v,row,i):\n    if v[row] == []:\n        return 0.0\n    else:\n        return float(v[row][i])\n","be6dd4b1":"feats = {'cap_prop': cap_props,\n        'hash_prop': hash_props}\nfeats = pd.DataFrame(feats,columns=['cap_prop','hash_prop'])\nfor i in range(len(v[0])):\n    feats.insert(i,'e_'+str(i),[get_val(v,row,i) for row in range(len(v))],True)\ntarget = train['target']\nprint(feats.head(5))","fb28f412":"feats.to_csv('doc_embeddings.csv',index=False)","480e90aa":"\nregressor = linear_model.LogisticRegression()\nregressor.fit(feats,target)\ntrain_preds = regressor.predict(feats)\nprint(train_preds[:3])","965299b4":"\n\n\nprint(classification_report(target,train_preds))","d01ffd84":"print('We predicted '+str(sum(train_preds))+' disaster tweets of '+str(sum(target)))\nprint('Here are the first 5: ')\nis_pred = train_preds==1\ntrain[is_pred].head(5)","3835d0e3":"rf = RandomForestClassifier()\nrf.fit(feats,target)\nrf_preds = rf.predict(feats)\nprint(classification_report(target,rf_preds))\nprint('We predicted '+str(sum(rf_preds))+' disaster tweets of '+str(sum(target)))\nprint('Here are the first 5: ')\nis_pred = rf_preds==1\ntrain[is_pred].head(5)","76de8d2a":"# define function for calculating features\ndef get_feats(df):\n    cap_props = [cap_prop(tweet) for tweet in df['text']]\n    hash_props = [hash_prop(tweet) for tweet in df['text']]\n    v = [avg_vec(tweet) for tweet in df['text']]\n    #for tweet in df['text']: \n    #    v.append(avg_vec(tweet))\n    \n    feats = {'cap_prop': cap_props,\n            'hash_prop': hash_props}\n    feats = pd.DataFrame(feats,columns=['cap_prop','hash_prop'])\n    \n    for i in range(len(v[0])):\n        feats.insert(i,'e_'+str(i),[get_val(v,row,i) for row in range(len(v))],True)\n\n    return feats","31d2084b":"def get_submission(df,model):\n    feats = get_feats(df)\n    preds = model.predict(feats)\n    submission = {'id': df['id'],\n                  'target': preds}\n    submission = pd.DataFrame(submission,columns=['id','target'])\n    return submission","5cc1b764":"submission = get_submission(test,regressor)","8c774ab7":"submission.to_csv('submission.csv',index=False)","834d6790":"submission = get_submission(test,rf)\nsubmission.to_csv('rf_submission.csv',index=False)","8351659a":"As we can see, each word is represented by a vector of length 100. Each tweet, then, is represented by >= 1 vectors representing all the words in a tweet. We will use this representation in the modeling step later.","12a7e331":"Here is an example of what our tweet vectors look like.","d710e800":"## Imports","b2d6d0cb":"# RF Model","4e501bb6":"## Create word embeddings","9b251c08":"Okay so neither of these features are very informative, but let's just try a model so we can get the infrastructure and stuff.","d70192ae":"# Test Data","305b00bb":"### Graphing the vectors of keywords\nHere we grab all the keywords from the training data and use PCA to plot their vectors in two dimensions - can you see which keywords are semantically different from the others?","2e20d5fe":"\n\n# Feature Ideas\n\n- Create word embeddings (PARKER)\n    - Should we create our own? Where should we get them from\n    - Maybe build them from only the true targeted tweets\n    - if it doesn't include them would it be able to differentiate them\n    - or would it recognize them as foreign?\n- Word embeddings (specifically for hashtagged words)\n    - (top n most similar words via word embedding)\n    - to get a list of ambiguous disaster terms, use the top n most used words in the disaster tweets\n        - we would have to filter out all the stopwords (but that's really easy)\n- Word clustering (e.g. the bigrams\/trigrams created by \"trigger\" words)\n    - keywords\n    - words that would be associated with a natural disaster but could also be rhetoric\n    - where could we get a list of these ambiguous natural disaster words?\n- Proportion of capitalized letters? \n    - serious tweets (or news tweets) will try to be more formal in general\n    - probably only true for the first x number of characters (maybe we could split on punctuation or an endline?\n- Whether or not a location is null (or posted) ?\n- If the location \"matches\" the content of the tweet?\n    - could be interesting for news tweets, often about the area from which the reporter reports\n    - proportion of capitalized letters in the location (is it a serious location?)\n- Whether or not it has a keyword\n- Proportion of exclamation points to other punctuation (or other punctuations)\n- length of the tweet ?\n    - Our guess is that longer tweets are more associated with more professional writing\n- presence of link ?\n- average length of word?\n    - more formal tweets will have longer (more educated) words\n\n- information value algorithm\n    - determines which levels of a factor are most associated with an event\n    - do that with the top n keywords\n- included arson?\n    - 0\/1\n    - other keywords\n- included how many disaster keywords?\n- included how many nondisaster keywords?\n\n\n- how to use word embeddings as features\n    - arson (almost always disaster)\n    - \n\n\n# Modeling Ideas\n\n- stacking methods for ensembling","cf0606fa":"# Features","660d3ee4":"## Feat 2. Proportion of hashtags (?)","3d6ff314":"# Average Word Embedding via gensim, Word2Vec\n\nThe main idea behind this approach is to build semantic word embeddings for all the words in the training tweets. Since tweets often utilize a different form of language than other sources, e.g. nonfiction, we determined that we'd build our own embeddings rather than using pre-trained embeddings, for example from gensim. By averaging all of the word vectors in a document you can get a document embedding, which can be used as input in a machine learning model. We've also built a couple other features, but the document embeddings are the core of our approach. Let's walk through the steps.","3a3f1adb":"# Linear Model","a44cfb3a":"### Preprocess tweet text\nThe first step to building word embeddings is to clean the text - we want to make sure that the words \"officer\", \"Officer\", \"OFFICERS\", \"#officer\" are all represented the same (not saying that this is the best approach - maybe it would be good to distinguish between \"#officer\" and \"officer\" - this is just the approach that Tom & I chose).\n\nWe can easily preprocess the tweets using a few built-in functions from gensim. strip_punctuation() and strip_numeric() are pretty self-explanatory. remove_stopwords() removes all English words which are considered semantically inconsequential, e.g. \"the\", \"because\", etc. stem_text() \"stems\" all the words in a sentence, a sort-of quick-and-dirty way of making words like \"officers\", \"officer\", and \"officerz\" are all represented as the same word. \n\nNOTE: while stemming is a much faster way of normalizing text than lemmatizing (a process which using a dictionary to transform words into their root word), it can also lead to inaccuracies. For example \"portal\" could be transformed to \"port\", when in reality we don't want to represent these two different words as semantically the same.","86a7f794":"### Train & Save the Word2Vec model\nTo create and train Word2Vec model in gensim, you simply initialize a Word2Vec object with the \"sentences\" (each tweet represented as a list of clean, stemmed words), and a min_count. min_count answers the question \"how many times should this word appear across ALL of our tweets in order to be considered in the model?\" We chose 3. ","a4fa9951":"## Feat 1. Proportion of capitalized letters","24cb24cf":"## Feat 3 Average Word Vector (Document Embedding)","ac66d6f4":"## Data"}}