{"cell_type":{"91d6ad25":"code","44d6295a":"code","8453fa18":"code","4c56bc8b":"code","95cc883c":"code","34a3c877":"code","3ebfbc55":"code","9f4d46cf":"code","c34cfeb8":"code","723109aa":"code","567a0459":"code","4d18e50e":"code","646eab9a":"code","fd6d0975":"code","c08a7a08":"code","b678a154":"code","f9ab4cf3":"code","2a88d672":"code","9551fde6":"code","279ae30c":"code","af67faca":"code","5879d7a9":"code","fc0d9da0":"code","f80920c9":"code","8bf47330":"code","60ab4365":"code","30849188":"code","5b253a4b":"code","f7615ec5":"code","c8709b60":"code","28b0c6e9":"code","ace781fc":"code","34266bfd":"code","523974b4":"code","7771d649":"code","39c997b1":"code","a2762561":"code","50aecaf6":"code","706c06a5":"code","b930a175":"code","32fd2d70":"code","78cb21fd":"code","053a8864":"code","2e82c82e":"code","49872d90":"markdown","4f3463cb":"markdown","cbc51a05":"markdown","f34c46ba":"markdown","f8c68cff":"markdown","9ec33e23":"markdown","fa3261fe":"markdown","2eb4ecee":"markdown","2fe79f92":"markdown","b71f5524":"markdown","cc5b81ff":"markdown","2d95cca9":"markdown","47104f3e":"markdown","6538d5d2":"markdown","6e06ea08":"markdown","b1d11d68":"markdown","7bbad508":"markdown","5ae0f1c2":"markdown","14199ca0":"markdown"},"source":{"91d6ad25":"import pandas as pd\nimport csv\nimport numpy as np\nfrom sklearn import decomposition\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom PIL import Image\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom keras.callbacks import LambdaCallback\nfrom keras.layers import Conv1D, Flatten\nfrom keras.layers import Dense ,Dropout,BatchNormalization\nfrom keras.models import Sequential \nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical \nfrom keras import regularizers\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import  VotingClassifier\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn import metrics\nfrom sklearn import ensemble\nfrom sklearn import gaussian_process\nfrom sklearn import linear_model\nfrom sklearn import naive_bayes\nfrom sklearn import neighbors\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn import discriminant_analysis\nfrom sklearn import model_selection\nfrom xgboost.sklearn import XGBClassifier \nimport os\nprint(os.listdir(\"..\/input\"))\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory","44d6295a":"# HBP\ndata = pd.read_csv('..\/input\/protein-sequences\/data 4\/hbp.txt', sep=\">\",header=None)\nsequences=data[0].dropna()\nlabels=data[1].dropna()\nsequences.reset_index(drop=True, inplace=True)\nlabels.reset_index(drop=True, inplace=True)\nlist_of_series=[sequences.rename(\"sequences\"),labels.rename(\"Name\")]\ndf_hbp = pd.concat(list_of_series, axis=1)\ndf_hbp['label']='hbp'\ndf_hbp.head()","8453fa18":"# not HBP\ndata = pd.read_csv('..\/input\/protein-sequences\/data 4\/non-hbp.txt', sep=\">\",header=None)\nsequences=data[0].dropna()\nlabels=data[1].dropna()\nsequences.reset_index(drop=True, inplace=True)\nlabels.reset_index(drop=True, inplace=True)\nlist_of_series=[sequences.rename(\"sequences\"),labels.rename(\"Name\")]\ndf_N_hbp = pd.concat(list_of_series, axis=1)\ndf_N_hbp['label']='non-hbp'\ndf_N_hbp.head()","4c56bc8b":"frames = [df_hbp,df_N_hbp]\ndf=pd.concat(frames)\ndf.head()","95cc883c":"arr=[]\nfor i in df.sequences:\n    arr.append(len(i))\n    \narr=np.asarray(arr)\nprint(\"Minimum length of string is = \",(arr.min()))\nminlength=arr.min()\nfrom keras.preprocessing import text, sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.model_selection import train_test_split\n\n# maximum length of sequence, everything afterwards is discarded!\nmax_length = minlength\n\n#create and fit tokenizer\ntokenizer = Tokenizer(char_level=True)\ntokenizer.fit_on_texts(df.sequences)\n#represent input data as word rank number sequences\nX = tokenizer.texts_to_sequences(df.sequences)\nX = sequence.pad_sequences(X, maxlen=max_length)","34a3c877":"from sklearn.preprocessing import LabelBinarizer\nimport keras\n\nlb = LabelBinarizer()\nY = lb.fit_transform(df.label)\nprint(len(X[0]))\n","3ebfbc55":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.3, random_state=0)\n\npca = decomposition.PCA(n_components=2)\npca.fit(X_train)\nX_train = pca.transform(X_train)\n\npca = decomposition.PCA(n_components=2)\npca.fit(X_test)\nX_test = pca.transform(X_test)\n\nkfold = StratifiedKFold(n_splits=10, shuffle=True)\ncvscores = []\n\niterator = 1\ncv_score = 0\nfor train, test in kfold.split(X_train, y_train):\n    print('Fold : '+str(iterator))\n    # giving 100% accuracy on n_estimator 50\n    RandomForest = RandomForestClassifier(n_estimators=10, oob_score=True, n_jobs=-1, warm_start=True).fit(X_train[train],y_train[train].ravel())\n    pred = np.round(RandomForest.predict(X_train[test]))\n    tn, fp, fn, tp = confusion_matrix(y_train[test], pred, labels=[1,0]).ravel()\n    acc = np.round(((tn+tp)\/(tn+fp+fn+tp))*100, 2)\n    cvscores.append([tn,fp,fn,tp,acc])\n    iterator=iterator+1\n    print([tn,fp,fn,tp,acc])\n    cv_score = cv_score + acc\nprint('\\n\\rFinal 10CV Score = ', np.round(cv_score\/kfold.n_splits,2),'\\n\\rResults are Saved in CrossValidationResults.csv\\n\\r')\n","9f4d46cf":"def frequencyVec(seq):\n    encoder = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W',\n               'Y']\n    fv = [0 for x in range(20)]\n    i = 0\n    for i in range(20):\n        fv[i - 1] = seq.count(encoder[i])\n    return fv\nX_frequencyVec=[]\nfor i in df.sequences:\n    X_frequencyVec.append(frequencyVec(i))\n                      \n                      \nX_frequencyVec = np.asarray(X_frequencyVec)\nX_frequencyVec.shape\n                    ","c34cfeb8":"X_train, X_test, y_train, y_test = train_test_split(X_frequencyVec, df.label, test_size=.3, random_state=0)\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()    \n    ]\n\n\n\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy' ]\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n   # cv_results = model_selection.cross_validate(alg, X_train, y_train)\n    alg.fit(X_train, y_train)\n    y_pred=alg.predict(X_test)\n    score=metrics.accuracy_score(y_test, y_pred)\n    \n    MLA_compare.loc[row_index, 'MLA Test Accuracy'] =score\n\n    \n    \n    row_index+=1\n\n    \n\n\n#MLA_predict","723109aa":"MLA_compare","567a0459":"def AAPIV(seq):\n    encoder = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W',\n               'Y']\n    apv = [0 for x in range(20)]\n    i = 1\n    sum = 0\n    for i in range(20):\n        j = 0\n        for j in range(len(seq)):\n            if seq[j] == encoder[i]:\n                sum = sum + j + 1\n        apv[i] = sum\n        sum = 0\n    return apv[1:] + apv[0:1]\nX_AAPIV=[]\n\nfor i in df.sequences:\n    X_AAPIV.append(AAPIV(i))\n                      \n                      \nX_AAPIV = np.asarray(X_AAPIV)\nX_AAPIV.shape\n","4d18e50e":"X_train, X_test, y_train, y_test = train_test_split(X_AAPIV , df.label, test_size=.3, random_state=0)\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()    \n    ]\n\n\n\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy' ]\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n   # cv_results = model_selection.cross_validate(alg, X_train, y_train)\n    alg.fit(X_train, y_train)\n    y_pred=alg.predict(X_test)\n    score=metrics.accuracy_score(y_test, y_pred)\n    \n    MLA_compare.loc[row_index, 'MLA Test Accuracy'] =score\n\n    \n    \n    row_index+=1\n\n    \n\n\n#MLA_predict","646eab9a":"MLA_compare","fd6d0975":"def PRIM(seq):\n    encoder = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W',\n               'Y']\n    prim = [[0 for x in range(20)] for y in range(20)]\n    i = 0\n    for i in range(20):\n        aa1 = encoder[i]\n        aa1index = -1\n        for x in range(len(seq)):\n            if seq[x] == aa1:\n                aa1index = x + 1\n                break\n        if aa1index != -1:\n            j = 0\n            for j in range(20):\n                if j != i:\n                    aa2 = encoder[j]\n                    aa2index = 0\n                    for y in range(len(seq)):\n                        if seq[y] == aa2:\n                            aa2index = aa2index + ((y + 1) - aa1index)\n                    prim[i][j] = int(aa2index)\n    return prim\nX_PRIM=[]\n\nfor i in df.sequences:\n    X_PRIM.append(PRIM(i))\n                      \n                      \nX_PRIM = np.asarray(X_PRIM)\nX_PRIM.shape","c08a7a08":"X_PRIM=X_PRIM.reshape(246, 20*20)\nX_PRIM.shape","b678a154":"X_train, X_test, y_train, y_test = train_test_split(X_PRIM, df.label, test_size=.3, random_state=0)\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()    \n    ]\n\n\n\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy' ]\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n   # cv_results = model_selection.cross_validate(alg, X_train, y_train)\n    alg.fit(X_train, y_train)\n    y_pred=alg.predict(X_test)\n    score=metrics.accuracy_score(y_test, y_pred)\n    \n    MLA_compare.loc[row_index, 'MLA Test Accuracy'] =score\n\n    \n    \n    row_index+=1\n\n    \n\n\n#MLA_predict","f9ab4cf3":"MLA_compare","2a88d672":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn import tree\n\n\nclfETC=tree.ExtraTreeClassifier()\n\n\nparam_dist = {\"max_depth\": [30,50,23,25,17,None],\n               \"max_features\": [0.03,0.07,0.10,0.15,0.20,0.30,\"auto\"],\n              \n              \"max_depth\":[20,40,50,100],\n              \n              \"min_samples_leaf\":[1,2,3,5,6],\n              \"max_features\":['auto','sqrt','log2',None],\n              \n              \"criterion\": [\"gini\", \"entropy\"],\n              \n             }\n\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(clfETC, param_distributions=param_dist,\n                                   n_iter=n_iter_search, cv=10,verbose=1)\n\n#Already Searched Parameter\nrandom_search.fit(X_frequencyVec, df.label)\nprint(random_search.best_params_)","9551fde6":"best_params=random_search.best_params_\nprint(best_params)\nmin_samples_leaf=best_params['min_samples_leaf']\nmax_features=best_params['max_features']\nmax_depth=best_params['max_depth']\ncriterion=best_params['criterion']","279ae30c":"X_train, X_test, y_train, y_test = train_test_split(X_frequencyVec, df.label, test_size=.3)\nfrom sklearn.ensemble import RandomForestClassifier\nclfET = tree.ExtraTreeClassifier( min_samples_split= min_samples_leaf, max_features=max_features,max_depth= max_depth,splitter= 'best',\n                              criterion= criterion)\n                            \n\nfrom sklearn.metrics import confusion_matrix\nclfET=clfET.fit(X_train,y_train)\nX_test=np.asarray(X_test)\ny_pred=clfET.predict(X_test)\ntrue_negative,false_positive,false_negative,true_positive=confusion_matrix(y_test,y_pred).ravel()\n\n","af67faca":"print(\"true_negative: \",true_negative)\nprint(\"false_positive: \",false_positive)\nprint(\"false_negative: \",false_negative)\nprint(\"true_positive: \",true_positive)\nprint(\"\\n\\n Accuracy Measures\\n\\n\")\nAccuracy=(true_positive+true_negative)\/(true_positive+false_negative+true_negative+false_positive)\nprint(\"Accuracy: \",Accuracy)\n\nSensitivity=true_positive\/(true_positive+false_negative)\nprint(\"Sensitivity: \",Sensitivity)\n\nFalse_Positive_Rate=false_positive\/(false_positive+true_negative)\nprint(\"False_Positive_Rate: \",False_Positive_Rate)\n\nSpecificity=true_negative\/(false_positive + true_negative)\nprint(\"Specificity: \",Specificity)\n\n#FDR \u00e0 0 means that very few of our predictions are wrong\nFalse_Discovery_Rate=false_positive\/(false_positive+true_positive)\nprint(\"False_Discovery_Rate: \",False_Discovery_Rate)\n\nPositive_Predictive_Value =true_positive\/(true_positive+false_positive)\nprint(\"Positive_Predictive_Value: \",Positive_Predictive_Value)\n","5879d7a9":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn import tree\n\n\nclfETC=RandomForestClassifier()\n\n\nparam_dist = {\"max_depth\": [30,50,23,25,17,None],\n               \"max_features\": [0.03,0.07,0.10,0.15,0.20,0.30,\"auto\"],\n              \"min_samples_split\": [2,3,4,5,6,7,8,9],\n              \"n_estimators\":[200,300,400,500,600,700],\n              \"criterion\": [\"gini\", \"entropy\"],\n              \"bootstrap\":[True,False]\n              \n             \n             }\n\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(clfETC, param_distributions=param_dist,\n                                   n_iter=n_iter_search, cv=10,verbose=1)\n\n#Already Searched Parameter\nrandom_search.fit(X_frequencyVec, df.label)\nprint(random_search.best_params_)","fc0d9da0":"best_params=random_search.best_params_\nprint(best_params)\nn_estimators=best_params['n_estimators']\nmin_samples_split=best_params['min_samples_split']\nmax_features=best_params['max_features']\nmax_depth=best_params['max_depth']\ncriterion=best_params['criterion']\nbootstrap=best_params['bootstrap']\n","f80920c9":"X_train, X_test, y_train, y_test = train_test_split(X_frequencyVec, df.label, test_size=.3, random_state=0)\nfrom sklearn.ensemble import RandomForestClassifier\nclfRF = RandomForestClassifier(n_estimators= n_estimators, min_samples_split= min_samples_split, max_features= max_features,\n                             max_depth= max_depth, criterion= criterion, bootstrap=bootstrap\n                             )\n                            \n\nfrom sklearn.metrics import confusion_matrix\nclfRF=clfRF.fit(X_train,y_train)\nX_test=np.asarray(X_test)\ny_pred=clfRF.predict(X_test)\ntrue_negative,false_positive,false_negative,true_positive=confusion_matrix(y_test,y_pred).ravel()\n\n","8bf47330":"print(\"true_negative: \",true_negative)\nprint(\"false_positive: \",false_positive)\nprint(\"false_negative: \",false_negative)\nprint(\"true_positive: \",true_positive)\nprint(\"\\n\\n Accuracy Measures\\n\\n\")\nAccuracy=(true_positive+true_negative)\/(true_positive+false_negative+true_negative+false_positive)\nprint(\"Accuracy: \",Accuracy)\n\nSensitivity=true_positive\/(true_positive+false_negative)\nprint(\"Sensitivity: \",Sensitivity)\n\nFalse_Positive_Rate=false_positive\/(false_positive+true_negative)\nprint(\"False_Positive_Rate: \",False_Positive_Rate)\n\nSpecificity=true_negative\/(false_positive + true_negative)\nprint(\"Specificity: \",Specificity)\n\n#FDR \u00e0 0 means that very few of our predictions are wrong\nFalse_Discovery_Rate=false_positive\/(false_positive+true_positive)\nprint(\"False_Discovery_Rate: \",False_Discovery_Rate)\n\nPositive_Predictive_Value =true_positive\/(true_positive+false_positive)\nprint(\"Positive_Predictive_Value: \",Positive_Predictive_Value)\n\n","60ab4365":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn import tree\n\n\nclfBC=ensemble.BaggingClassifier()\n\n\nparam_dist = {\"base_estimator\": [clfRF,clfET],\n               \"n_estimators\": [30,60,100,150,200,250],\n              \"warm_start\": [True,False] ,\n              \"bootstrap\":[True,False]\n              \n             \n             }\n\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(clfBC, param_distributions=param_dist,\n                                   n_iter=n_iter_search, cv=10,verbose=1)\n\n#Already Searched Parameter\nrandom_search.fit(X_frequencyVec, df.label)\nprint(random_search.best_params_)","30849188":"best_params=random_search.best_params_\nprint(best_params)\nbase_estimator=best_params['base_estimator']\nn_estimators=best_params['n_estimators']\nwarm_start=best_params['warm_start']\nbootstrap=best_params['bootstrap']\n#best params\n#{'warm_start': True, 'n_estimators': 30, 'bootstrap': True, 'base_estimator': None}","5b253a4b":"bstlfy=ensemble.BaggingClassifier(base_estimator=base_estimator,n_estimators=n_estimators,warm_start=warm_start,bootstrap=bootstrap)\nbstlfy=bstlfy.fit(X_train, y_train)\nprediction=bstlfy.predict(X_test)\n    \n\n","f7615ec5":"true_negative,false_positive,false_negative,true_positive=confusion_matrix(y_test, prediction).ravel()\n\nprint(\"true_negative: \",true_negative)\nprint(\"false_positive: \",false_positive)\nprint(\"false_negative: \",false_negative)\nprint(\"true_positive: \",true_positive)\nprint(\"\\n\\n Accuracy Measures\\n\\n\")\n\nAccuracy=(true_positive+true_negative)\/(true_positive+false_positive+true_negative+false_negative)\nprint(\"Accuracy: \",Accuracy)\n\nSensitivity=true_positive\/(true_positive+false_negative)\nprint(\"Sensitivity: \",Sensitivity)\n\nFalse_Positive_Rate=false_positive\/(false_positive+true_negative)\nprint(\"False_Positive_Rate: \",False_Positive_Rate)\n\nSpecificity=true_negative\/(false_positive + true_negative)\nprint(\"Specificity: \",Specificity)\n\n        #FDR \u00e0 0 means that very few of our predictions are wrong\nFalse_Discovery_Rate=false_positive\/(false_positive+true_positive)\nprint(\"False_Discovery_Rate: \",False_Discovery_Rate)\n\nPositive_Predictive_Value =true_positive\/(true_positive+false_positive)\nprint(\"Positive_Predictive_Value: \",Positive_Predictive_Value)","c8709b60":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n# Split Data\nlb = LabelBinarizer()\nY = lb.fit_transform(df.label)\n\nX_train, X_test,y_train,y_test = train_test_split(df.sequences, Y, test_size = 0.2, random_state =0)\n\n\n\n\n\n\ny_test_cat=keras.utils.to_categorical(y_test)\ny_train_cat=keras.utils.to_categorical(y_train)\n# Create a Count Vectorizer to gather the unique elements in sequence\nvect = CountVectorizer(analyzer = 'char_wb', ngram_range = (4,4))\n\n# Fit and Transform CountVectorizer\nvect.fit(X_train)\nX_train_df = vect.transform(X_train)\nX_test_df = vect.transform(X_test)\n\n#Print a few of the features\nprint(vect.get_feature_names()[-20:])\n","28b0c6e9":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn import tree\n\n\nclfETC=linear_model.LogisticRegressionCV()\n\nparam_dist = {\"Cs\": [10,5,20,15,30,4,3],\n               \"fit_intercept\": [True,False],\n              \"tol\":[0.0001,0.000001,0.01,0.001],\n              \n              \"solver\": [\"newton-cg\", \"lbfgs\",\"liblinear\",\"saga\"],\n              \n              \"n_jobs\":[-1],\n              \"multi_class\":[\"ovr\",\"auto\"]\n             \n             }\n\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(clfETC, param_distributions=param_dist,\n                                   n_iter=n_iter_search, cv=10,verbose=1)\n\n#Already Searched Parameter\nrandom_search.fit(X_frequencyVec, df.label)\n","ace781fc":"print(random_search.best_params_)\n#previous best params\n#tol=1e-06, solver= 'liblinear', n_jobs= -1, multi_class= 'ovr', fit_intercept= True, Cs=5\n#88% accuracy","34266bfd":"\n\nclfRF = linear_model.LogisticRegressionCV(tol=1e-06, solver= 'liblinear', n_jobs= -1, multi_class= 'ovr', fit_intercept= True, Cs=5)\n                            \nfrom sklearn.metrics import confusion_matrix\nclfRF=clfRF.fit(X_train_df,y_train)\nX_test=np.asarray(X_test_df)\ny_pred=clfRF.predict(X_test_df)\ntrue_negative,false_positive,false_negative,true_positive=confusion_matrix(y_test,y_pred).ravel()\n\n","523974b4":"\n\nprint(\"true_negative: \",true_negative)\nprint(\"false_positive: \",false_positive)\nprint(\"false_negative: \",false_negative)\nprint(\"true_positive: \",true_positive)\nprint(\"\\n\\n Accuracy Measures\\n\\n\")\nAccuracy=(true_positive+true_negative)\/(true_positive+false_negative+true_negative+false_positive)\nprint(\"Accuracy: \",Accuracy)\n\nSensitivity=true_positive\/(true_positive+false_negative)\nprint(\"Sensitivity: \",Sensitivity)\n\nFalse_Positive_Rate=false_positive\/(false_positive+true_negative)\nprint(\"False_Positive_Rate: \",False_Positive_Rate)\n\nSpecificity=true_negative\/(false_positive + true_negative)\nprint(\"Specificity: \",Specificity)\n\n#FDR \u00e0 0 means that very few of our predictions are wrong\nFalse_Discovery_Rate=false_positive\/(false_positive+true_positive)\nprint(\"False_Discovery_Rate: \",False_Discovery_Rate)\n\nPositive_Predictive_Value =true_positive\/(true_positive+false_positive)\nprint(\"Positive_Predictive_Value: \",Positive_Predictive_Value)\n","7771d649":"bstlfy=ensemble.BaggingClassifier(base_estimator=clfRF,n_estimators=10)\nbstlfy=bstlfy.fit(X_train_df, y_train)\nprediction=bstlfy.predict(X_test_df)\n    \n\ntrue_negative,false_positive,false_negative,true_positive=confusion_matrix(y_test, prediction).ravel()\n","39c997b1":"\nprint(\"true_negative: \",true_negative)\nprint(\"false_positive: \",false_positive)\nprint(\"false_negative: \",false_negative)\nprint(\"true_positive: \",true_positive)\nprint(\"\\n\\n Accuracy Measures\\n\\n\")\n\nAccuracy=(true_positive+true_negative)\/(true_positive+false_positive+true_negative+false_negative)\nprint(\"Accuracy: \",Accuracy)\n\nSensitivity=true_positive\/(true_positive+false_negative)\nprint(\"Sensitivity: \",Sensitivity)\n\nFalse_Positive_Rate=false_positive\/(false_positive+true_negative)\nprint(\"False_Positive_Rate: \",False_Positive_Rate)\n\nSpecificity=true_negative\/(false_positive + true_negative)\nprint(\"Specificity: \",Specificity)\n\n        #FDR \u00e0 0 means that very few of our predictions are wrong\nFalse_Discovery_Rate=false_positive\/(false_positive+true_positive)\nprint(\"False_Discovery_Rate: \",False_Discovery_Rate)\n\nPositive_Predictive_Value =true_positive\/(true_positive+false_positive)\nprint(\"Positive_Predictive_Value: \",Positive_Predictive_Value)","a2762561":"Ada=ensemble.AdaBoostClassifier(base_estimator=bstlfy, n_estimators=40, learning_rate=1.0, algorithm=\"SAMME\", random_state=None)\nAda=Ada.fit(X_train_df, y_train)\nprediction=Ada.predict(X_test_df)\n    \n\ntrue_negative,false_positive,false_negative,true_positive=confusion_matrix(y_test, prediction).ravel()\n\n","50aecaf6":"print(\"true_negative: \",true_negative)\nprint(\"false_positive: \",false_positive)\nprint(\"false_negative: \",false_negative)\nprint(\"true_positive: \",true_positive)\nprint(\"\\n\\n Accuracy Measures\\n\\n\")\n\nAccuracy=(true_positive+true_negative)\/(true_positive+false_positive+true_negative+false_negative)\nprint(\"Accuracy: \",Accuracy)\n\nSensitivity=true_positive\/(true_positive+false_negative)\nprint(\"Sensitivity: \",Sensitivity)\n\nFalse_Positive_Rate=false_positive\/(false_positive+true_negative)\nprint(\"False_Positive_Rate: \",False_Positive_Rate)\n\nSpecificity=true_negative\/(false_positive + true_negative)\nprint(\"Specificity: \",Specificity)\n\n        #FDR \u00e0 0 means that very few of our predictions are wrong\nFalse_Discovery_Rate=false_positive\/(false_positive+true_positive)\nprint(\"False_Discovery_Rate: \",False_Discovery_Rate)\n\nPositive_Predictive_Value =true_positive\/(true_positive+false_positive)\nprint(\"Positive_Predictive_Value: \",Positive_Predictive_Value)","706c06a5":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n# Split Data\nlb = LabelBinarizer()\ny = lb.fit_transform(df.label)\n\nX=df.sequences\n\n\n\n\n\n\n\nvect = CountVectorizer(analyzer = 'char_wb', ngram_range = (4,4))\n\n# Fit and Transform CountVectorizer\nvect.fit(X)\nX = vect.transform(X)\n\n\n#Print a few of the features\nprint(vect.get_feature_names()[-20:])\n\n\nkfold = StratifiedKFold(n_splits=100, shuffle=True)\ncvscores = []\n\niterator = 1\ncv_score = 0\nfor train, test in kfold.split(X, y):\n    print('Fold : '+str(iterator))\n    # giving 100% accuracy on n_estimator 50\n    RandomForest = linear_model.LogisticRegressionCV(tol=1e-06, solver= 'liblinear', n_jobs= -1, multi_class= 'ovr', fit_intercept= True, Cs=5).fit(X[train],y[train].ravel())\n    pred = np.round(RandomForest.predict(X[test]))\n    tn, fp, fn, tp = confusion_matrix(y[test], pred, labels=[1,0]).ravel()\n    acc = np.round(((tn+tp)\/(tn+fp+fn+tp))*100, 2)\n    cvscores.append([tn,fp,fn,tp,acc])\n    iterator=iterator+1\n    print([tn,fp,fn,tp,acc])\n    cv_score = cv_score + acc\n\n","b930a175":"print('\\n\\rFinal JackKnife Score = ', np.round(cv_score\/kfold.n_splits,2))","32fd2d70":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n# Split Data\nlb = LabelBinarizer()\ny = lb.fit_transform(df.label)\n\nX=df.sequences\n\n\n\n\n\n\n\nvect = CountVectorizer(analyzer = 'char_wb', ngram_range = (4,4))\n\n# Fit and Transform CountVectorizer\nvect.fit(X)\nX = vect.transform(X)\n\n\n#Print a few of the features\nprint(vect.get_feature_names()[-20:])\n\n\nkfold = StratifiedKFold(n_splits=100, shuffle=True)\ncvscores = []\n\niterator = 1\ncv_score = 0\nfor train, test in kfold.split(X, y):\n    print('Fold : '+str(iterator))\n    # giving 100% accuracy on n_estimator 50\n    RandomForest = ensemble.BaggingClassifier(base_estimator=clfRF,n_estimators=10).fit(X[train],y[train].ravel())\n    pred = np.round(RandomForest.predict(X[test]))\n    tn, fp, fn, tp = confusion_matrix(y[test], pred, labels=[1,0]).ravel()\n    acc = np.round(((tn+tp)\/(tn+fp+fn+tp))*100, 2)\n    cvscores.append([tn,fp,fn,tp,acc])\n    iterator=iterator+1\n    print([tn,fp,fn,tp,acc])\n    cv_score = cv_score + acc\n\n","78cb21fd":"print('\\n\\rFinal JackKnife Score = ', np.round(cv_score\/kfold.n_splits,2))","053a8864":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n# Split Data\nlb = LabelBinarizer()\ny = lb.fit_transform(df.label)\n\nX=df.sequences\n\n\n\n\n\n\n\nvect = CountVectorizer(analyzer = 'char_wb', ngram_range = (4,4))\n\n# Fit and Transform CountVectorizer\nvect.fit(X)\nX = vect.transform(X)\n\n\n#Print a few of the features\nprint(vect.get_feature_names()[-20:])\n\n\nkfold = StratifiedKFold(n_splits=100, shuffle=True)\ncvscores = []\n\niterator = 1\ncv_score = 0\nfor train, test in kfold.split(X, y):\n    print('Fold : '+str(iterator))\n    # giving 100% accuracy on n_estimator 50\n    RandomForest = ensemble.AdaBoostClassifier(base_estimator=bstlfy, n_estimators=40, learning_rate=1.0, algorithm=\"SAMME\", random_state=None).fit(X[train],y[train].ravel())\n    pred = np.round(RandomForest.predict(X[test]))\n    tn, fp, fn, tp = confusion_matrix(y[test], pred, labels=[1,0]).ravel()\n    acc = np.round(((tn+tp)\/(tn+fp+fn+tp))*100, 2)\n    cvscores.append([tn,fp,fn,tp,acc])\n    iterator=iterator+1\n    print([tn,fp,fn,tp,acc])\n    cv_score = cv_score + acc\n\n","2e82c82e":"print('\\n\\rFinal JackKnife Score = ', np.round(cv_score\/kfold.n_splits,2))","49872d90":"# Optimizing Random Forest","4f3463cb":"**Optimizing Logistic Regression Algorithm**","cbc51a05":"# Bagging (For Random Forest)","f34c46ba":"# Feature Extraction Techniques","f8c68cff":"# Binarizing Labels","9ec33e23":"# Prim","fa3261fe":"**Boasting Logistic Regression Algorithm**","2eb4ecee":"# bag of Words","2fe79f92":"# About\nWe are going to classify Two Types of proteins here HBPS and NON-HBPS by using:\n\n* **Feature Extraction Techniques**\n    *     Tokinizer\n    *     Frequency Vector\n    *     AAPIV\n    *     PRIM\n    *     Bag Of Words\n     \n     \n* **Optimization Techniques (Hypermaters Adjustments)**\n    * Grid Search\n    \n    \n* **Ensemble Techniques**\n    * Bagging \n    * Boasting\n    \n    \n* **Evaluation Method**\n    * True Positive\n    * True Negative\n    * False Positive\n    * False Negative\n    * Accuracy Score\n    * Precesion\n    * Specefication\n    * False_Positive_Rate\n    * False Discovery Rate\n    * Jack-Knife Evaluation Technique","b71f5524":"** Bagging Logictic Regression Algorithm**","cc5b81ff":"# frequencyVec","2d95cca9":"# AAPIV","47104f3e":"# PreProcessing X by Tokenizer","6538d5d2":"# Model Testing","6e06ea08":"# RandomForest Tree","b1d11d68":"# # Jackknife Evaluation on Ada Boast using Logistic Regression","7bbad508":"# Jackknife Evaluation on Bagging using Logistic regression","5ae0f1c2":"# Optimizing Extra Tree Algorithm","14199ca0":"# Jackknife Evaluation on Logistic regression"}}