{"cell_type":{"2f9b66ce":"code","62eef992":"code","9ff72ed4":"code","4afcc7bf":"code","d758524c":"code","b89017ff":"code","43b1c7dd":"code","cdc943c7":"code","3ffa6ed9":"code","2f6944e8":"code","6d7dbace":"code","a58b7129":"code","101491e6":"code","174c0694":"code","b5829680":"code","7fa094bb":"code","29c62bbb":"code","9fc3c30d":"code","2186c49b":"code","a0aa1acb":"code","25f2be69":"code","fa09d589":"code","f80188de":"code","2d7bdc9f":"code","21a3f9e6":"code","7964c72d":"code","2987619b":"code","5e51daf5":"code","48c68e1b":"code","1552e721":"code","112bf7fd":"code","5fc3cf66":"code","e6de1578":"code","2f6b09ff":"code","ca96cba1":"code","0b0c77ca":"code","7da606d4":"code","f2a903b5":"code","2ac834fe":"code","02b7dfff":"code","9170e8d3":"code","a6b4c8d2":"code","3dda3cf6":"code","b4e3188c":"code","19a9158c":"code","dd3d55ec":"code","1730b982":"code","e9f5386f":"code","d4ef7663":"markdown","ab079998":"markdown","31cd298a":"markdown","e6cc8749":"markdown","b7c14e11":"markdown"},"source":{"2f9b66ce":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","62eef992":"!pip install fastai==0.7.0 --no-deps","9ff72ed4":"from fastai.imports import *\nfrom fastai.structured import *\n\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom IPython.display import display\n\nfrom sklearn import metrics\n\nimport seaborn as sns\n\ndef display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)\n        \nset_plot_sizes(12,14,16)","4afcc7bf":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","d758524c":"train_cpy = train.copy()\ntest_cpy = test.copy()\ntrain_cpy['is_valid'] = 0\ntest_cpy['is_valid'] = 1","b89017ff":"train_test = pd.concat([train_cpy.drop('SalePrice', axis=1), test_cpy])\ntrain_cats(train_test)\nx, y, nas = proc_df(train_test, 'is_valid')","43b1c7dd":"def get_train_valid_oob(x, y):\n    m = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, random_state=0, n_jobs=-1, oob_score=True)\n    m.fit(x, y)\n    return m.oob_score_","cdc943c7":"get_train_valid_oob(x, y)","3ffa6ed9":"train_test.drop([\"Id\"], axis=1, inplace=True)","2f6944e8":"x, y, nas = proc_df(train_test, 'is_valid')\nget_train_valid_oob(x, y)","6d7dbace":"train.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)\n\nduplicates = [\n    \"GarageArea\", # Twin with \"GarageCars\", but lower correlation\n    \"TotRmsAbvGrd\", # same, with \"GrLivArea\"\n    \"1stFlrSF\" # Twin with \"TotalBsmtSF\"\n]\n\ntrain.drop(duplicates, axis=1, inplace=True)\ntest.drop(duplicates, axis=1, inplace=True)","a58b7129":"# Shuffle the dataset to get better train\/valid split\ntrain = train.sample(frac=1, random_state=0).reset_index(drop=True)","101491e6":"# Simple label encoding\ntrain_cats(train)","174c0694":"df, y, nas = proc_df(train, 'SalePrice')","b5829680":"m = RandomForestRegressor(n_jobs=-1, random_state=0)\nm.fit(df, y)\nm.score(df,y)","7fa094bb":"# Train\/valid split\ndef split_vals(a,n): return a[:n], a[n:]\nn_valid = 300\nn_trn = len(df)-n_valid\nX_train, X_valid = split_vals(df, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\nraw_train, raw_valid = split_vals(train, n_trn)","29c62bbb":"def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","9fc3c30d":"# Simple RF model\nm = RandomForestRegressor(n_estimators=100, max_features=0.3, n_jobs=-1, random_state=0, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","2186c49b":"# Feature importance - first try\ndef plot_fi(fi, title=None): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False, title=title)\nfi = rf_feat_importance(m, df)\nplot_fi(fi[:25], title=\"Feature importance - first glance\")","a0aa1acb":"# Remove unimportant columns\nto_keep = fi[fi.imp > 0.001].cols\nlen(to_keep)","25f2be69":"df_keep = df[to_keep].copy()\nX_train, X_valid = split_vals(df_keep, n_trn)","fa09d589":"m = RandomForestRegressor(n_estimators=100, max_features=0.3, n_jobs=-1, random_state=0, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","f80188de":"# Feature importance again\nfi = rf_feat_importance(m, df_keep)\nplot_fi(fi[:25], title=\"Feature importance - after dropping unimportant\")","2d7bdc9f":"# OneHot Encoding - all categorical vars\ndf_trn2, y_trn, nas = proc_df(train, 'SalePrice', max_n_cat=42)\nX_train, X_valid = split_vals(df_trn2, n_trn)","21a3f9e6":"m = RandomForestRegressor(n_estimators=100, max_features=0.27, n_jobs=-1, random_state=0, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","7964c72d":"fi = rf_feat_importance(m, df_trn2)\nplot_fi(fi[:25], title=\"Feature importance - OneHot Encoding\")","2987619b":"to_keep = fi[fi.imp > 0.001].cols\ndf_keep2 = df_trn2[to_keep]\nlen(to_keep)","5e51daf5":"X_train, X_valid = split_vals(df_keep2, n_trn)","48c68e1b":"m = RandomForestRegressor(n_estimators=100, max_features=0.27, n_jobs=-1, random_state=0, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","1552e721":"fi = rf_feat_importance(m, df_keep2)\nplot_fi(fi[:25], title=\"Feature importance - OneHot Encoding after dropping unimportant\")","112bf7fd":"def get_preds(t): return t.predict(X_valid)\n%time preds = np.stack(parallel_trees(m, get_preds))\nnp.mean(preds[:,0]), np.std(preds[:,0])","5fc3cf66":"x = raw_valid.copy()\nx['pred_std'] = np.std(preds, axis=0)\nx['pred'] = np.mean(preds, axis=0)\nx.OverallQual.value_counts().plot.barh();","e6de1578":"raw_train.OverallQual.value_counts().plot.barh()","2f6b09ff":"sns.distplot(x.pred_std)","ca96cba1":"def mean_stdev_by_col(df, colname):\n    flds = [colname, 'SalePrice', 'pred', 'pred_std']\n    summ = x[flds].groupby(flds[0]).mean()\n    summ['rel_std'] = (summ.pred_std\/summ.pred)\n    return summ\n\ndef plot_relative_stdev(df, colname):\n    mean_std = mean_stdev_by_col(df, colname)\n    sns.barplot(x=mean_std.index, y='rel_std', data=mean_std.sort_values(by='rel_std'))    ","0b0c77ca":"plt.figure(figsize=(12, 6))\nplot_relative_stdev(x, \"OverallQual\")","7da606d4":"plot_relative_stdev(x, \"GarageCars\")","f2a903b5":"from scipy.cluster import hierarchy as hc","2ac834fe":"def draw_dendrogram(df, title=None):\n    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)\n    corr_condensed = hc.distance.squareform(1-corr)\n    z = hc.linkage(corr_condensed, method='average')\n    fig = plt.figure(figsize=(25,30))\n    dendrogram = hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=16)\n    plt.title(label=title, fontdict={'fontsize': 25})\n    plt.show()","02b7dfff":"draw_dendrogram(df_keep, title='Dendrogram (numerical encodings)')","9170e8d3":"draw_dendrogram(df_keep2, title=\"OneHot encodings\")","a6b4c8d2":"def get_oob(df):\n    m = RandomForestRegressor(n_estimators=100, max_features=0.27, n_jobs=-1, random_state=0, oob_score=True)\n    x, _ = split_vals(df, n_trn)\n    m.fit(x, y_train)\n    return m.oob_score_","3dda3cf6":"get_oob(df_keep)","b4e3188c":"drop_candidates = [\"Exterior1st\", \"Exterior2nd\", \"FireplaceQu\", \"Fireplaces\", \"GarageYrBlt\"]\nfor c in drop_candidates:\n    print(c, get_oob(df_keep.drop(c, axis=1)))","19a9158c":"# Two columns to drop: Exterior1st and FireplaceQu\ndrops = [\"Exterior1st\", \"FireplaceQu\"]\ndf1 = df_keep.copy()\n# Add and drop vs drop - adding slightly better here - we do not lose information\ndf1[\"Exterior2nd\"] += df1[\"Exterior1st\"]\nprint(get_oob(df_keep.drop(drops, axis=1)), get_oob(df1.drop(drops, axis=1)))","dd3d55ec":"# Drop collinear columns\ndf_keep[\"Exterior2nd\"] += df_keep[\"Exterior1st\"]\ndf_keep.drop(drops, axis=1, inplace=True)\nget_oob(df_keep)","1730b982":"draw_dendrogram(df_keep, \"After dropping Exterior1st and FireplaceQu\")","e9f5386f":"df2 = df_keep.copy()\ndf2.ExterQual += df2.KitchenQual\nget_oob(df2.drop(\"KitchenQual\", axis=1))","d4ef7663":"**Source: fast.ai course v3**\nhttps:\/\/medium.com\/@hiromi_suenaga\/machine-learning-1-lesson-3-fa4065d8cb1e","ab079998":"Now we can see that train and test set are hard to distinguish by the model,\nso we can assume similar distributions of each of the features in train and test set,\nand generally homogeneity of train and test set.\nThat means a random sample from training set is appropriate for validation set.","31cd298a":"The OOB score is very high, since we haven't removed Id column\nwhich clearly indicates whether row is from valid or not.","e6cc8749":"**Difference between train and validation set**\n[Explanation here](https:\/\/medium.com\/@hiromi_suenaga\/machine-learning-1-lesson-5-df45f0c99618)","b7c14e11":"[Tree variance](https:\/\/medium.com\/@hiromi_suenaga\/machine-learning-1-lesson-4-a536f333b20d) by OverallQual"}}