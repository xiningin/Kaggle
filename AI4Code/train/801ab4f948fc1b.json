{"cell_type":{"e16cc3a8":"code","bf9479a7":"code","268ad570":"code","a0e020a3":"code","29d537d2":"code","a6bcaa70":"code","5ea7886a":"code","f7a9bfc1":"code","e5a9a5a6":"code","418ddf9e":"code","2ea4906a":"code","36827f6a":"code","06b062b5":"code","34b40f09":"code","0af1270f":"code","4bdc5a92":"code","cc3ee99f":"code","c1f0d45e":"code","249b5006":"code","bcb3e350":"code","01b4ffe4":"code","03e08840":"code","6df0ac96":"code","8046b807":"code","6f0a76c6":"code","d5c2f426":"code","ef96f43c":"code","1c88d9b7":"code","a223f4c1":"code","8bef8db0":"code","648aac06":"code","8815e8d6":"code","7c10cdd4":"code","9219c819":"code","13cd3ce0":"code","130ca4af":"code","afeb8b79":"code","b9a7357b":"code","9039df53":"code","1d606ae7":"code","cc9f66ea":"code","b2354491":"code","6b38b506":"code","a4f2eca2":"code","bdc9d89b":"code","7830c213":"code","c1b645bb":"code","06a5e14c":"code","7987929b":"code","11a5def2":"code","9511b77b":"code","70d48149":"code","438bb866":"code","3064ed35":"code","59a218fa":"code","c9a8ba9e":"code","79fe9237":"code","62a64141":"code","6fad1d93":"code","489c868f":"code","ccb5b494":"code","b7dde7f3":"code","a30ade66":"code","f73c098f":"code","1b1dcede":"code","d67ba541":"code","052a37c6":"code","c777851a":"code","247307d1":"code","7820d497":"code","40914753":"code","5e0c0e9a":"code","131a9545":"code","618673b8":"code","0aa79504":"code","88ed5110":"code","842f2411":"code","b9a644ea":"code","8b34c416":"code","fd38f73e":"code","8e4fe032":"code","0aa078ee":"markdown","c4ee60c9":"markdown","8903f20d":"markdown","fd71a2e2":"markdown","35145fbb":"markdown","940c12f7":"markdown","55ffab4c":"markdown","00c055e1":"markdown","95f1253f":"markdown","b66c02bb":"markdown","dedf7cc9":"markdown","c880fc8f":"markdown","0ab2ac37":"markdown","6fba0fe8":"markdown","102eed9c":"markdown","36cbc5f2":"markdown","c4816862":"markdown","8886bea4":"markdown","af5b40f1":"markdown","513f502c":"markdown","a3eaaa83":"markdown","3b0e998d":"markdown","3e1aec1c":"markdown","6f36a106":"markdown","043f7766":"markdown","c5437ced":"markdown","032be795":"markdown","ba87506b":"markdown","c4f2be44":"markdown","1e42668e":"markdown","1e849614":"markdown","f78baaa6":"markdown","34a8b9af":"markdown","1dc50e8c":"markdown"},"source":{"e16cc3a8":"!pip install simdkalman","bf9479a7":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport numpy as np # linear algebra\nfrom pathlib import Path\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy import sparse\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport simdkalman\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tqdm.notebook import tqdm\nfrom warnings import simplefilter\n\nsimplefilter('ignore')\nplt.style.use('fivethirtyeight')\npd.set_option('max_columns', 100)\npd.set_option('max_rows', 100)","268ad570":"model_name = 'nn_v2'\n\ndata_dir = Path('..\/input\/google-smartphone-decimeter-challenge')\ntrain_file = data_dir \/ 'baseline_locations_train.csv'\ntest_file = data_dir \/ 'baseline_locations_test.csv'\nsample_file = data_dir \/ 'sample_submission.csv'\n\nbuild_dir = Path('.\/build')\nbuild_dir.mkdir(parents=True, exist_ok=True)\npredict_val_file = build_dir \/ f'{model_name}.val.txt'\npredict_tst_file = build_dir \/ f'{model_name}.tst.txt'\nsubmission_file = 'submission.csv'\n\ncname_col = 'collectionName'\npname_col = 'phoneName'\nphone_col = 'phone'\nts_col = 'millisSinceGpsEpoch'\ndt_col = 'datetime'\nlat_col = 'latDeg'\nlon_col = 'lngDeg'\n\nlrate = .005\nbatch_size = 2048\nepochs = 250\nn_stop = 10\nn_fold = 25\nseed = 42","a0e020a3":"# from https:\/\/www.kaggle.com\/sohier\/loading-gnss-logs\ndef gnss_log_to_dataframes(path):\n    print('Loading ' + path, flush=True)\n    gnss_section_names = {'Raw','UncalAccel', 'UncalGyro', 'UncalMag', 'Fix', 'Status', 'OrientationDeg'}\n    with open(path) as f_open:\n        datalines = f_open.readlines()\n\n    datas = {k: [] for k in gnss_section_names}\n    gnss_map = {k: [] for k in gnss_section_names}\n    for dataline in datalines:\n        is_header = dataline.startswith('#')\n        dataline = dataline.strip('#').strip().split(',')\n        # skip over notes, version numbers, etc\n        if is_header and dataline[0] in gnss_section_names:\n            gnss_map[dataline[0]] = dataline[1:]\n        elif not is_header:\n            datas[dataline[0]].append(dataline[1:])\n\n    results = dict()\n    for k, v in datas.items():\n        results[k] = pd.DataFrame(v, columns=gnss_map[k])\n    # pandas doesn't properly infer types from these lists by default\n    for k, df in results.items():\n        for col in df.columns:\n            if col == 'CodeType':\n                continue\n            results[k][col] = pd.to_numeric(results[k][col])\n\n    return results","29d537d2":"# from https:\/\/www.kaggle.com\/dannellyz\/start-here-simple-folium-heatmap-for-geo-data\nimport folium\nfrom folium import plugins\n\n\ndef simple_folium(df:pd.DataFrame, lat_col:str, lon_col:str):\n    \"\"\"\n    Descrption\n    ----------\n        Returns a simple Folium HeatMap with Markers\n    ----------\n    Parameters\n    ----------\n        df : padnas DataFrame, required\n            The DataFrane with the data to map\n        lat_col : str, required\n            The name of the column with latitude\n        lon_col : str, required\n            The name of the column with longitude\n    \"\"\"\n    #Preprocess\n    #Drop rows that do not have lat\/lon\n    df = df[df[lat_col].notnull() & df[lon_col].notnull()]\n\n    # Convert lat\/lon to (n, 2) nd-array format for heatmap\n    # Then send to list\n    df_locs = list(df[[lat_col, lon_col]].values)\n\n    #Set up folium map\n    fol_map = folium.Map([df[lat_col].median(), df[lon_col].median()])\n\n    # plot heatmap\n    heat_map = plugins.HeatMap(df_locs)\n    fol_map.add_child(heat_map)\n\n    # plot markers\n    markers = plugins.MarkerCluster(locations = df_locs)\n    fol_map.add_child(markers)\n\n    #Add Layer Control\n    folium.LayerControl().add_to(fol_map)\n\n    return fol_map","a6bcaa70":"# from https:\/\/www.kaggle.com\/jpmiller\/baseline-from-host-data\n# simplified haversine distance\ndef calc_haversine(lat1, lon1, lat2, lon2):\n    \"\"\"Calculates the great circle distance between two points\n    on the earth. Inputs are array-like and specified in decimal degrees.\n    \"\"\"\n    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat\/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon\/2.0)**2\n\n    c = 2 * np.arcsin(a**0.5)\n    dist = 6_367_000 * c\n    return dist","5ea7886a":"# from https:\/\/www.kaggle.com\/emaerthin\/demonstration-of-the-kalman-filter\nT = 0.80\nstate_transition = np.array([[1, 0, T, 0, 0.5 * T ** 2, 0], [0, 1, 0, T, 0, 0.5 * T ** 2], [0, 0, 1, 0, T, 0],\n                             [0, 0, 0, 1, 0, T], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]])\nprocess_noise = np.diag([1e-5, 1e-5, 5e-6, 5e-6, 1e-6, 1e-6]) + np.ones((6, 6)) * 1e-9\nobservation_model = np.array([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0]])\nobservation_noise = np.diag([5e-5, 5e-5]) + np.ones((2, 2)) * 1e-9\n\nkf = simdkalman.KalmanFilter(\n        state_transition = state_transition,\n        process_noise = process_noise,\n        observation_model = observation_model,\n        observation_noise = observation_noise)\n\ndef apply_kf_smoothing(df, kf_=kf):\n    unique_paths = df[phone_col].unique()\n    for phone in tqdm(unique_paths):\n        data = df.loc[df[phone_col] == phone][[lat_col, lon_col]].values\n        data = data.reshape(1, len(data), 2)\n        smoothed = kf_.smooth(data)\n        df.loc[df[phone_col] == phone, lat_col] = smoothed.states.mean[0, :, 0]\n        df.loc[df[phone_col] == phone, lon_col] = smoothed.states.mean[0, :, 1]\n    return df","f7a9bfc1":"trn = pd.read_csv(train_file)\nprint(trn.shape)\ntrn.head()","e5a9a5a6":"tst = pd.read_csv(test_file)\nprint(tst.shape)\ntst.head()","418ddf9e":"sub = pd.read_csv(sample_file)\nprint(sub.shape)\nsub.head()","2ea4906a":"for col in [cname_col, pname_col]:\n    print(f'# of unique {col:>14s} in training: {trn[col].nunique():4d}')\n    print(f'# of unique {col:>14s}     in test: {tst[col].nunique():4d}')","36827f6a":"trn[pname_col].value_counts()","06b062b5":"tst[pname_col].value_counts()","34b40f09":"print(f'# of unique phone in training: {trn[phone_col].nunique():4d}')\nprint(f'    # of unique phone in test: {tst[phone_col].nunique():4d}')","0af1270f":"trn[phone_col].value_counts()","4bdc5a92":"tst[phone_col].value_counts()","cc3ee99f":"overlapping_phones = [x for x in tst[phone_col] if x in trn[phone_col]]\nprint(len(overlapping_phones))","c1f0d45e":"tst[ts_col].min(), tst[ts_col].max()","249b5006":"dt_offset = pd.to_datetime('1980-01-06 00:00:00')\nprint(dt_offset)\ndt_offset_in_ms = int(dt_offset.value \/ 1e6)","bcb3e350":"trn[dt_col] = pd.to_datetime(trn[ts_col] + dt_offset_in_ms, unit='ms')\ntst[dt_col] = pd.to_datetime(tst[ts_col] + dt_offset_in_ms, unit='ms')\nprint(f'Training data range: {trn[dt_col].min()} - {trn[dt_col].max()}')\nprint(f'    Test data range: {tst[dt_col].min()} - {tst[dt_col].max()}')","01b4ffe4":"latlon_trn = trn[[lat_col, lon_col]].round(3)\nlatlon_trn['counts'] = 1\nlatlon_trn = latlon_trn.groupby([lat_col, lon_col]).sum().reset_index()\nlatlon_trn.head()","03e08840":"simple_folium(latlon_trn, lat_col, lon_col)","6df0ac96":"cname = trn[cname_col][0]\npname = trn[pname_col][0]\ndfs = gnss = gnss_log_to_dataframes(str(data_dir \/ 'train' \/ cname \/ pname \/ f'{pname}_GnssLog.txt'))\nprint(dfs.keys())","8046b807":"df_raw = dfs['Raw']\nprint(df_raw.shape)\ndf_raw.head()","6f0a76c6":"df_raw.info()","d5c2f426":"df_raw['ArrivalTime'] = df_raw['TimeNanos'] - df_raw['FullBiasNanos'] - df_raw['BiasNanos']\nprint(df_raw['ArrivalTime'].describe())\ndf_raw['ArrivalTime'].hist(bins=20)","ef96f43c":"print(df_raw['BiasUncertaintyNanos'].describe())\ndf_raw['BiasUncertaintyNanos'].hist(bins=20)","1c88d9b7":"print(df_raw['ReceivedSvTimeUncertaintyNanos'].describe())\ndf_raw['ReceivedSvTimeUncertaintyNanos'].hist(bins=20)","a223f4c1":"print(df_raw.AccumulatedDeltaRangeUncertaintyMeters.describe())\ndf_raw.AccumulatedDeltaRangeUncertaintyMeters.hist(bins=20)","8bef8db0":"print(df_raw.Cn0DbHz.describe())\ndf_raw.Cn0DbHz.hist(bins=20)","648aac06":"df_raw = df_raw.loc[\n    ~pd.isnull(df_raw.FullBiasNanos) &\n    (df_raw.BiasUncertaintyNanos < 100) &\n    (df_raw.ArrivalTime > 0) &\n    (df_raw.ConstellationType != 0) &\n    ~pd.isnull(df_raw.TimeNanos) &\n    (df_raw.State != 3) & (df_raw.State != 14) & (df_raw.State != 7) & (df_raw.State != 15) &\n    (df_raw.ReceivedSvTimeUncertaintyNanos < 100) &\n    (df_raw.AccumulatedDeltaRangeUncertaintyMeters < 0.3) &\n    (df_raw.Cn0DbHz > 20)\n]\nprint(df_raw.shape)","8815e8d6":"derived = pd.read_csv(data_dir \/ 'train' \/ cname \/ pname \/ f'{pname}_derived.csv')\nprint(derived.shape)\nderived.head()","7c10cdd4":"derived.info()","9219c819":"derived = derived.loc[derived.constellationType != 0]\nprint(derived.shape)","13cd3ce0":"derived['correctedPrM'] = (derived['rawPrM'] + derived['satClkBiasM'] - derived['isrbM'] - \n                           derived['ionoDelayM'] - derived['tropoDelayM'])\nsns.pairplot(data=derived, vars=['correctedPrM', 'rawPrM'], size=3)","130ca4af":"derived[dt_col] = pd.to_datetime(derived[ts_col] + dt_offset_in_ms, unit='ms')\nprint(f'Data range for {cname}\/{pname}: {derived[dt_col].min()} - {derived[dt_col].max()}')","afeb8b79":"derived[['constellationType', 'svid', 'signalType']].value_counts()","b9a7357b":"derived[[ts_col, 'constellationType', 'correctedPrM']].groupby([ts_col, 'constellationType']).agg(['mean', 'std', 'count']).describe()","9039df53":"derived.loc[derived.constellationType == 1][[ts_col, 'svid', 'correctedPrM']].groupby([ts_col, 'svid']).agg(['mean', 'std', 'count']).describe()","1d606ae7":"derived.loc[derived.signalType == 'GPS_L1'][[ts_col, 'svid', 'correctedPrM']].groupby([ts_col, 'svid']).agg(['mean', 'std', 'count'])","cc9f66ea":"derived.loc[derived.signalType == 'GPS_L1'][[ts_col, 'svid', 'correctedPrM']].groupby([ts_col, 'svid']).agg(['mean', 'std', 'count']).describe()","b2354491":"derived.loc[derived.signalType == 'GPS_L1'][[ts_col, 'svid']].drop_duplicates().groupby([ts_col]).agg(['mean', 'std', 'count']).describe()","6b38b506":"gps_l1 = derived.loc[derived.signalType == 'GPS_L1'][[ts_col, 'svid', 'correctedPrM']].drop_duplicates([ts_col, 'svid'])\nprint(gps_l1.shape)\ngps_l1.head()","a4f2eca2":"label = pd.read_csv(data_dir \/ 'train' \/ cname \/ pname \/ 'ground_truth.csv')\nprint(label.shape)\nlabel.head()","bdc9d89b":"label[dt_col] = pd.to_datetime(label[ts_col] + dt_offset_in_ms, unit='ms')\nprint(f'Labels range for {cname}\/{pname}: {label[dt_col].min()} - {label[dt_col].max()}')","7830c213":"cname = trn[cname_col][10]\npname = trn[pname_col][10]\nderived2 = pd.read_csv(data_dir \/ 'train' \/ cname \/ pname \/ f'{pname}_derived.csv')\nlabel2 = pd.read_csv(data_dir \/ 'train' \/ cname \/ pname \/ 'ground_truth.csv')\nprint(f\"Derived data starts at: {pd.to_datetime(derived2[ts_col].min() + dt_offset_in_ms, unit='ms')}\")\nprint(f\"  Label data starts at: {pd.to_datetime(label2[ts_col].min() + dt_offset_in_ms, unit='ms')}\")","c1b645bb":"trn.sort_values([phone_col, ts_col], inplace=True)\ntrn[['prev_lat']] = trn[lat_col].shift().where(trn[phone_col].eq(trn[phone_col].shift()))\ntrn[['prev_lon']] = trn[lon_col].shift().where(trn[phone_col].eq(trn[phone_col].shift()))\n\ntst.sort_values([phone_col, ts_col], inplace=True)\ntst[['prev_lat']] = tst[lat_col].shift().where(tst[phone_col].eq(tst[phone_col].shift()))\ntst[['prev_lon']] = tst[lon_col].shift().where(tst[phone_col].eq(tst[phone_col].shift()))\ntrn.head()","06a5e14c":"# from https:\/\/www.kaggle.com\/jpmiller\/baseline-from-host-data\nlabel_files = (data_dir \/ 'train').rglob('ground_truth.csv')\ncols = [phone_col, ts_col, lat_col, lon_col]\n\ndf_list = []\nfor t in tqdm(label_files, total=73):\n    label = pd.read_csv(t, usecols=[cname_col, pname_col, ts_col, lat_col, lon_col])\n    df_list.append(label)\n\ndf_label = pd.concat(df_list, ignore_index=True)\ndf_label[phone_col] = df_label[cname_col] + '_' + df_label[pname_col]\n\ndf = df_label.merge(trn[cols + ['prev_lat', 'prev_lon']], how='inner', on=[phone_col, ts_col], \n                    suffixes=('_gt', '')).drop([cname_col, pname_col], axis=1)\ndf['sSinceGpsEpoch'] = df[ts_col] \/\/ 1000\nprint(df.shape)\ndf.head()","7987929b":"df_tst = sub[[phone_col, ts_col]].merge(tst[[phone_col, ts_col, lat_col, lon_col, 'prev_lat', 'prev_lon']], \n                                        how='left', on=[phone_col, ts_col], suffixes=('', '_basepred'))\ndf_tst['sSinceGpsEpoch'] = df_tst[ts_col] \/\/ 1000\nprint(df_tst.shape)\ndf_tst.head()","11a5def2":"derived_files = (data_dir \/ 'train').rglob('*_derived.csv')\ncols = [ts_col, 'svid', 'correctedPrM']\n\ndf_list = []\nfor t in tqdm(derived_files, total=73):\n    derived = pd.read_csv(t).drop_duplicates([ts_col, 'svid'])\n    derived['correctedPrM'] = (derived['rawPrM'] + derived['satClkBiasM'] - derived['isrbM'] - \n                               derived['ionoDelayM'] - derived['tropoDelayM'])\n    df_list.append(derived[[cname_col, pname_col, ts_col, 'svid', 'correctedPrM']])\n    \ndf_derived = pd.concat(df_list, ignore_index=True)\ndf_derived[phone_col] = df_derived[cname_col] + '_' + df_derived[pname_col]\ndf_derived.drop([cname_col, pname_col], axis=1, inplace=True)\n\nprint(df_derived.shape)\ndf_derived.head()","9511b77b":"df_derived_pivot = pd.pivot_table(df_derived, \n                                  values='correctedPrM', \n                                  index=[phone_col, ts_col],\n                                  columns=['svid'],\n                                  aggfunc=np.mean)\ndf_derived_pivot.columns = [f'svid_{x}' for x in df_derived_pivot.columns]\ndf_derived_pivot.reset_index(inplace=True)\ndf_derived_pivot['sSinceGpsEpoch'] = df_derived_pivot[ts_col] \/\/ 1000\n\nprint(df_derived_pivot.shape)\ndf_derived_pivot.head()","70d48149":"df = df.merge(df_derived_pivot, how='left', on=[phone_col, 'sSinceGpsEpoch'], suffixes=['', '_2'])\ndf.drop(['sSinceGpsEpoch', ts_col + '_2'], axis=1, inplace=True)\nprint(df.shape)\ndf.head()","438bb866":"df['d_lat'] = df['latDeg_gt'] - df[lat_col]\ndf['d_lon'] = df['lngDeg_gt'] - df[lon_col]\ndf[['d_lat', 'd_lon']].describe()","3064ed35":"derived_files = (data_dir \/ 'test').rglob('*_derived.csv')\ncols = [ts_col, 'svid', 'correctedPrM']\n\ndf_list = []\nfor t in tqdm(derived_files, total=48):\n    derived = pd.read_csv(t)\n    derived['sSinceGpsEpoch'] = derived[ts_col] \/\/ 1000\n    derived.drop_duplicates(['sSinceGpsEpoch', 'svid'], inplace=True)\n    derived['correctedPrM'] = (derived['rawPrM'] + derived['satClkBiasM'] - derived['isrbM'] - \n                               derived['ionoDelayM'] - derived['tropoDelayM'])\n    df_list.append(derived[[cname_col, pname_col, 'sSinceGpsEpoch', 'svid', 'correctedPrM']])\n    \ndf_derived = pd.concat(df_list, ignore_index=True)\ndf_derived[phone_col] = df_derived[cname_col] + '_' + df_derived[pname_col]\ndf_derived.drop([cname_col, pname_col], axis=1, inplace=True)\n\ndf_derived_pivot = pd.pivot_table(df_derived, \n                                  values='correctedPrM', \n                                  index=[phone_col, 'sSinceGpsEpoch'],\n                                  columns=['svid'],\n                                  aggfunc=np.mean)\ndf_derived_pivot.columns = [f'svid_{x}' for x in df_derived_pivot.columns]\ndf_derived_pivot.reset_index(inplace=True)\n\ndf_tst = df_tst.merge(df_derived_pivot, how='left', \n                      on=[phone_col, 'sSinceGpsEpoch']).drop(['sSinceGpsEpoch'], axis=1)\nprint(df_tst.shape)\ndf_tst.head()","59a218fa":"df_tst.describe()","c9a8ba9e":"# tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","79fe9237":"feature_cols = [x for x in df_tst.columns if x not in [phone_col, ts_col]]\ntarget_cols = ['d_lat', 'd_lon']\ninput_dim = len(feature_cols)\noutput_dim = len(target_cols)","62a64141":"df_test = df_tst\nfor i in tqdm(range(1,len(df_test))):\n    lat1 = df_test.loc[i-1,'latDeg']\n    lon1 = df_test.loc[i-1,'lngDeg']\n    lat2 = df_test.loc[i,'latDeg']\n    lon2 = df_test.loc[i,'lngDeg']\n    df_test.loc[i,'dist_pre'] = calc_haversine(lat1, lon1, lat2, lon2)\n\nlist_phone = df_test['phone'].unique()\nfor phone in list_phone:\n    ind = df_test[df_test['phone'] == phone].index[0]\n    df_test.loc[ind,'dist_pre'] = 0","6fad1d93":"for i in tqdm(range(0,len(df_test)-1)):\n    lat1 = df_test.loc[i,'latDeg']\n    lon1 = df_test.loc[i,'lngDeg']\n    lat2 = df_test.loc[i+1,'latDeg']\n    lon2 = df_test.loc[i+1,'lngDeg']\n    df_test.loc[i,'dist_pro'] = calc_haversine(lat1, lon1, lat2, lon2)\nlist_phone = df_test['phone'].unique()\nfor phone in list_phone:\n    ind = df_test[df_test['phone'] == phone].index[-1]\n    df_test.loc[ind,'dist_pro'] = 0","489c868f":"pro_95 = df_test['dist_pro'].mean() + (df_test['dist_pro'].std() * 2)\npre_95 = df_test['dist_pre'].mean() + (df_test['dist_pre'].std() * 2)\nind = df_test[(df_test['dist_pro'] > pro_95)&(df_test['dist_pre'] > pre_95)][['dist_pre','dist_pro']].index\n\nfor i in ind:\n    df_test.loc[i,'latDeg'] = (df_test.loc[i-1,'latDeg'] + df_test.loc[i+1,'latDeg'])\/2\n    df_test.loc[i,'lngDeg'] = (df_test.loc[i-1,'lngDeg'] + df_test.loc[i+1,'lngDeg'])\/2","ccb5b494":"df_tst = df_test","b7dde7f3":"df_test = df\nfor i in tqdm(range(1,len(df_test))):\n    lat1 = df_test.loc[i-1,'latDeg']\n    lon1 = df_test.loc[i-1,'lngDeg']\n    lat2 = df_test.loc[i,'latDeg']\n    lon2 = df_test.loc[i,'lngDeg']\n    df_test.loc[i,'dist_pre'] = calc_haversine(lat1, lon1, lat2, lon2)\n\nlist_phone = df_test['phone'].unique()\nfor phone in list_phone:\n    ind = df_test[df_test['phone'] == phone].index[0]\n    df_test.loc[ind,'dist_pre'] = 0","a30ade66":"for i in tqdm(range(0,len(df_test)-1)):\n    lat1 = df_test.loc[i,'latDeg']\n    lon1 = df_test.loc[i,'lngDeg']\n    lat2 = df_test.loc[i+1,'latDeg']\n    lon2 = df_test.loc[i+1,'lngDeg']\n    df_test.loc[i,'dist_pro'] = calc_haversine(lat1, lon1, lat2, lon2)\nlist_phone = df_test['phone'].unique()\nfor phone in list_phone:\n    ind = df_test[df_test['phone'] == phone].index[-1]\n    df_test.loc[ind,'dist_pro'] = 0","f73c098f":"pro_95 = df_test['dist_pro'].mean() + (df_test['dist_pro'].std() * 2)\npre_95 = df_test['dist_pre'].mean() + (df_test['dist_pre'].std() * 2)\nind = df_test[(df_test['dist_pro'] > pro_95)&(df_test['dist_pre'] > pre_95)][['dist_pre','dist_pro']].index\n\nfor i in ind:\n    df_test.loc[i,'latDeg'] = (df_test.loc[i-1,'latDeg'] + df_test.loc[i+1,'latDeg'])\/2\n    df_test.loc[i,'lngDeg'] = (df_test.loc[i-1,'lngDeg'] + df_test.loc[i+1,'lngDeg'])\/2","1b1dcede":"df = df_test","d67ba541":"from sklearn.decomposition import PCA","052a37c6":"#n_components =41\nscaler = StandardScaler()\nlabel_scaler = StandardScaler()\nscaler.fit(pd.concat([df[feature_cols], df_tst[feature_cols]], axis=0).fillna(0).values)\nX = scaler.transform(df[feature_cols].fillna(0).values)\nX_tst = scaler.transform(df_tst[feature_cols].fillna(0).values)\nY = label_scaler.fit_transform(df[target_cols].values)\nprint(X.shape, Y.shape, X_tst.shape)","c777851a":"from keras import backend as K","247307d1":"def nll1(y_true, y_pred):\n    \"\"\" Negative log likelihood. \"\"\"\n\n    # keras.losses.binary_crossentropy give the mean\n    # over the last axis. we require the sum\n    return K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)","7820d497":"def loss(y_true, y_pred):\n    PI_ON_180 = tf.constant(np.pi \/ 180, dtype=tf.float32)\n    RADIUS_M = tf.constant(6_377_000, dtype = tf.float32)\n    tf.dtypes.cast(y_true, tf.float32)\n    tf.dtypes.cast(y_pred, tf.float32)\n\n    yt_rad = y_true * PI_ON_180\n    yp_rad = y_pred * PI_ON_180\n\n    delta = yt_rad - yp_rad\n    v = delta \/ 2\n    v = tf.sin(v)\n    v = v**2\n\n    a = v[:,1] + tf.cos(yt_rad[:,1]) * tf.cos(yp_rad[:,1]) * v[:,0] \n    c = tf.sqrt(a)\n    c = 2* tf.math.asin(c)\n    c = c*RADIUS_M\n\n    final = tf.reduce_mean(c)\n    return final","40914753":"# def build_model():\n#     inputs = keras.layers.Input((input_dim,))\n#     x = keras.layers.Dense(128, activation='relu')(inputs)\n#     x = keras.layers.BatchNormalization()(x)\n#     x = keras.layers.Dense(128, activation='relu')(x)\n#     x = keras.layers.Dropout(.3)(x)\n    \n#     ox = x\n    \n#     x = keras.layers.Dense(128, activation='relu')(x)\n#     x = keras.layers.BatchNormalization()(x)\n#     x = keras.layers.Dense(128, activation='relu')(x)\n#     x = keras.layers.Dropout(.3)(x)\n    \n#     x = keras.layers.Add()([x, ox])\n    \n#     x = keras.layers.Dense(128, activation='relu')(x)\n#     x = keras.layers.BatchNormalization()(x)\n#     x = keras.layers.Dense(128, activation='relu')(x)\n#     x = keras.layers.Dropout(.3)(x)\n    \n#     outputs = keras.layers.Dense(output_dim, activation='linear')(x)\n    \n#     model = keras.Model(inputs, outputs)\n#     model.compile(optimizer=keras.optimizers.Ftrl(lrate), loss=nll1)\n#     return model\n","5e0c0e9a":"from losses import *\nfrom activations import *\nfrom backprop_nn import NeuralNetwork\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_boston, load_breast_cancer, load_diabetes\nfrom sklearn.preprocessing import StandardScaler\n\n\ndef breast_cancer():\n#     X, y = load_breast_cancer(return_X_y=True)\n#     sc = StandardScaler()\n#     sc.fit(X)\n#     X = sc.transform(X)\n#     X_train, X_test, y_train, y_test = train_test_split(X, y)\n\n#     num_features = X.shape[1]\n\n    nn_ekf = NeuralNetwork([num_features, 20, 10, 1], activations=[ReLU(), ReLU(), Sigmoid()], loss=Unity())\n    nn_ekf.train_ekf(\n        X_train.T,\n        y_train.reshape(1, -1),\n        P=100, R=10, Q=1e-2,\n        eta=1, epochs=10,\n        val=(X_test.T, y_test.reshape(1, -1))\n    )\n    y_hat = np.round(nn_ekf.feedforward(X_test.T)[1][-1], 0)\n    print(\"EKF Accuracy: {}\".format(np.sum(y_hat == y_test) \/ len(y_test)))\n\n    nn = NeuralNetwork([num_features, 20, 10, 1], activations=[ReLU(), ReLU(), Sigmoid()], loss=CrossEntropy())\n    nn.train(\n        X_train.T,\n        y_train.reshape(1, -1),\n        epochs=10, batch_size=1, lr=.05,\n        val=(X_test.T, y_test.reshape(1, -1))\n    )\n    y_hat = np.round(nn.feedforward(X_test.T)[1][-1], 0)\n    print(\"Accuracy: {}\".format(np.sum(y_hat == y_test) \/ len(y_test)))\n\n\ndef diabetes():\n    X, y = load_diabetes(return_X_y=True)\n    sc = StandardScaler()\n    X = sc.fit_transform(X)\n    y = sc.fit_transform(y.reshape(-1, 1))\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n\n    num_features = X.shape[1]\n\n    nn_ekf = NeuralNetwork([num_features, 20, 10, 1], activations=[ReLU(), ReLU(), Linear()], loss=Unity())\n    nn_ekf.train_ekf(\n        X_train.T,\n        y_train.reshape(1, -1),\n        P=100, R=100, Q=1e-2,\n        eta=.3, epochs=10,\n        val=(X_test.T, y_test.reshape(1, -1))\n    )\n    y_hat = nn_ekf.feedforward(X_test.T)[1][-1]\n    print(\"EKF loss: {}\".format(QuadraticLoss()(y_hat, y_test.reshape(1, -1))))\n\n    nn = NeuralNetwork([num_features, 20, 10, 1], activations=[ReLU(), ReLU(), Linear()], loss=QuadraticLoss())\n    nn.train(\n        X_train.T,\n        y_train.reshape(1, -1),\n        epochs=10, batch_size=1, lr=.05,\n        val=(X_test.T, y_test.reshape(1, -1))\n    )\n    y_hat = nn.feedforward(X_test.T)[1][-1]\n    print(\"Backprop loss: {}\".format(QuadraticLoss()(y_hat, y_test.reshape(1, -1))))\n\n\n# if __name__ == '__main__':\n#     diabetes()","131a9545":"# with tpu_strategy.scope():\nmodel = build_model()\nmodel.summary()","618673b8":"def scheduler(epoch, lr, warmup=5):\n    if epoch < warmup:\n        return lr * 1.5\n    else:\n        return lr * tf.math.exp(-.1)\n\nes = keras.callbacks.EarlyStopping(patience=n_stop, restore_best_weights=True)\nlr = keras.callbacks.LearningRateScheduler(scheduler)\n\ncv = KFold(n_splits=n_fold, shuffle=True, random_state=seed)\n\nP = np.zeros_like(Y, dtype=float)\nP_tst = np.zeros((X_tst.shape[0], output_dim), dtype=float)\nfor i, (i_trn, i_val) in enumerate(cv.split(X), 1):\n    print(f'Training for CV #{i}')\n    model = build_model()\n    history = model.fit(X[i_trn], Y[i_trn], validation_data=(X[i_val], Y[i_val]), \n                        epochs=epochs, batch_size=batch_size, callbacks=[es, lr], verbose=0)\n    P[i_val] = label_scaler.inverse_transform(model.predict(X[i_val]))\n    P_tst += label_scaler.inverse_transform(model.predict(X_tst)) \/ n_fold\n    \n    distance_i = calc_haversine(df.latDeg_gt.values[i_val], \n                                df.lngDeg_gt.values[i_val], \n                                P[i_val, 0] + df.latDeg.values[i_val], \n                                P[i_val, 1] + df.lngDeg.values[i_val]).mean()\n    print(f'CV #{i}: {np.percentile(distance_i, [50, 95])}')","0aa79504":"print(P.mean(axis=0), P_tst.mean(axis=0))\nnp.savetxt(predict_val_file, P, delimiter=',', fmt='%.6f')\nnp.savetxt(predict_tst_file, P_tst, delimiter=',', fmt='%.6f')","88ed5110":"distance = calc_haversine(df.latDeg_gt, df.lngDeg_gt, P[:, 0] + df.latDeg, P[:, 1] + df.lngDeg)\nprint(f'CV All: {np.percentile(distance, [50, 95])}')","842f2411":"df.sort_values([phone_col, ts_col], inplace=True)\ndf_smoothed = df.copy()\ndf_smoothed[lat_col] = df[lat_col] + P[:, 0]\ndf_smoothed[lon_col] = df[lon_col] + P[:, 1]\ndf_smoothed = apply_kf_smoothing(df_smoothed)\ndistance = calc_haversine(df_smoothed.latDeg_gt, df_smoothed.lngDeg_gt, df_smoothed.latDeg, df_smoothed.lngDeg)\nprint(f'CV All (smoothed): {np.percentile(distance, [50, 95])}')","b9a644ea":"plt.plot(history.history['lr'])","8b34c416":"distance_tst = calc_haversine(df_tst.latDeg, df_tst.lngDeg, P_tst[:, 0] + df_tst.latDeg, P_tst[:, 1] + df_tst.lngDeg)\nprint(f'CV All: {np.percentile(distance_tst, [50, 95])}')","fd38f73e":"df_tst.sort_values([phone_col, ts_col], inplace=True)\ndf_tst_smoothed = df_tst.copy()\ndf_tst_smoothed[lat_col] = df_tst_smoothed[lat_col] + P_tst[:, 0]\ndf_tst_smoothed[lon_col] = df_tst_smoothed[lon_col] + P_tst[:, 1]\ndf_tst_smoothed = apply_kf_smoothing(df_tst_smoothed)\ndistance_tst = calc_haversine(df_tst.latDeg, df_tst.lngDeg, df_tst_smoothed.latDeg, df_tst_smoothed.lngDeg)\nprint(f'CV All (smoothed): {np.percentile(distance_tst, [50, 95])}')","8e4fe032":"df_tst_smoothed[[phone_col, ts_col, lat_col, lon_col]].to_csv(submission_file, index=False)","0aa078ee":"There's **no** overlapping phone between the training and test data.","c4ee60c9":"# Model Training","8903f20d":"## Ground Truth","fd71a2e2":"From the [post](https:\/\/www.kaggle.com\/c\/google-smartphone-decimeter-challenge\/discussion\/238583) by @sohier and [slides](https:\/\/www.kaggle.com\/google\/android-smartphones-high-accuracy-datasets?select=ION+GNSS+2020+Slides+Android+Raw+GNSS+Measurement+Datasets+for+Precise+Positioning.pdf) by the data provider: \n\nMeasurements from GNSS chipsets of mobile phones are often noisier and more erroneous. Example of filters your can apply (to exclude) are:\n1. `FullBiasNanos` (GNSS Raw) is zero or invalid\n2. `BiasUncertaintyNanos` (GNSS Raw) too large (> 1e6)\n3. Arrival time is negative or unrealistically large - can be calculated from `rawPrM` (Derived)\n4. Unknown constellation (`constellationType == 0`) (Derived, GNSS Raw)\n5. `TimeNanos` is empty (GNSS Raw)\n6. `State` is not in (`STATE_TOW_DECODED`, `STATE_TOW_KNOWN`, `STATE_GLO_TOD_DECODED`, `STATE_GLO_TOD_KNOWN`) (GNSS Raw)\n7. `ReceivedSvTimeUncertaintyNanos` is high (500 ns) (GNSS Raw)\n8. `AccumulatedDeltaRangeState` violating this condition: `ADR_STATE_VALID == 1 & ADR_STATE_RESET == 0 & ADR_STATE_CYCLE_SLIP == 0` (GNSS Raw)\n9. `AccumulatedDeltaRangeUncertaintyMeters` is high (GNSS Raw)\n10. `Cn0DbHz` is less than 20 db-Hz (GNSS Raw)","35145fbb":"Derived values are used to generate baseline location estimates in `baseline_locations_{train|test}.csv`.","940c12f7":"Let's see the heatmap for the test data too.","55ffab4c":"## `millisSinceGpsEpoch`","00c055e1":"# Aggregated Data EDA","95f1253f":"## `latDeg` and `lngDeg`","b66c02bb":"## GNSS Logs","dedf7cc9":"# Feature Generation","c880fc8f":"From the data description, `millisSinceGpsEpoch` is \"an integer number of milliseconds since the GPS epoch (1980\/1\/6 midnight UTC). Its value equals\". We can convert them to `datatime64` using `pd.to_datetime()` as follows:","0ab2ac37":"## Derived Data Aggregation","6fba0fe8":"Let's see the heatmap for the training data.","102eed9c":"It's the same. We don't have the first second data in the derived data. Let's take a note and move on.","36cbc5f2":"This notebook shares EDA on the aggregate and phone level data, then trains a neural network model to predict the residuals of base estimations provided with aggregated features.\n\nThe contents of the notebook are organized as follows:\n1. Aggregated Data EDA\n2. Phone Level Data EDA\n3. Feature Generation: generates aggregated features for training. Currently we only use previous lat\/long and `correctedPrM` from derived files.\n4. Model Training: trains a neural network with a skip connection in Keras on TPU.\n\nCredits to other notebooks:\n* [Baseline from host data](https:\/\/www.kaggle.com\/jpmiller\/baseline-from-host-data) by @jpmiller: for the distance calculation with `calc_haversine()`\n* [Demonstration of the Kalman filter](https:\/\/www.kaggle.com\/emaerthin\/demonstration-of-the-kalman-filter) by @emaerthin: for Kalman filtering with `apply_kf_smoothing()`\n* [Loading GNSS logs](https:\/\/www.kaggle.com\/sohier\/loading-gnss-logs) by organizers: for GNSS log loading with `gnss_log_to_dataframes()`\n* [\u046a Start Here: Simple Folium Heatmap for Geo-Data](https:\/\/www.kaggle.com\/dannellyz\/start-here-simple-folium-heatmap-for-geo-data) by @dannellyz: for geospatial heatmap with `simple_folium()`\n\nThanks for sharing.","c4816862":"First, let's see how estimated locations between the training and test data look like. The ground truth for training data is available per `phone` in `{collectionName}\/{phoneName}\/ground_truth.csv`.","8886bea4":"In the `*derived.csv`, we have 55K rows, but in the `ground_truth.csv`, we only have 1,740 rows.","af5b40f1":"Each phone has fair amount of data points ranging between 577 and 3,517.","513f502c":"The data is for 30 minutes or 1,800 seconds. However, we have a lot more samples (55K). This is because, for each second, there are multiple samples with different `constellationType`, `svid`, and `signalType`.","a3eaaa83":"Each epoch, given the signal type of `GPS_L1`, from the same satellite, `correctedPrM` is unique.","3b0e998d":"# Phone Level Data EDA","3e1aec1c":"## `collectionName`, `phoneName`","6f36a106":"Each epoch, given the constellation type of `1` (or GPS), from the same satellite, `coorectedPrM` can be different - because of different signal types.","043f7766":"## Raw Data Aggregation - To Be Updated","c5437ced":"See organizer's [Loading GNSS logs](https:\/\/www.kaggle.com\/sohier\/loading-gnss-logs) notebook for more details.","032be795":"Let's calculate `correctedPrM` as described in the data description:\n```\ncorrectedPrM = rawPrM + satClkBiasM - isrbM - ionoDelayM - tropoDelayM\n```\n\"The baseline locations are computed using correctedPrM and the satellite positions, using a standard Weighted Least Squares (WLS) solver, with the phone's position (x, y, z), clock bias (t), and isrbM for each unique signal type as states for each epoch.\"","ba87506b":"Each epoch, given the signal type of `GPS_L1`, there are signals from at least 3 satellites.","c4f2be44":"# Load Libraries & Data","1e42668e":"Hmm, this is weird. The label data starts 1 second earlier than the derived data. This means that if we join the derived and label data, the first second will have NaNs for derived columns. Let's check another phone data.","1e849614":"## Label Data Aggregation","f78baaa6":"## Derived Values","34a8b9af":"# Submission File","1dc50e8c":"First, let's add previous latitude and longitude estimates as features."}}