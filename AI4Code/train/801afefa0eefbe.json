{"cell_type":{"967255e2":"code","e15d4522":"code","37a10de9":"code","8915913f":"code","4759f17a":"code","7721262c":"code","8bf945a8":"code","0d5586d8":"code","f893a0cb":"code","8b782aaa":"code","d9a81686":"code","fb100f54":"code","b659b4e9":"code","ac67c6cb":"code","fa9f24b4":"code","4536b7bd":"code","4ca2b549":"code","95aac110":"code","8bdab8fb":"code","4faa222d":"code","e80aeb66":"code","b077dfc6":"code","c16890f7":"code","5d918ddb":"code","47563ec2":"code","d451bfcd":"code","1153ef07":"code","c8c3858f":"markdown","b129f7df":"markdown","e98a760d":"markdown","1ecea417":"markdown","bc9c0a4c":"markdown","f5b6c621":"markdown","32800f78":"markdown"},"source":{"967255e2":"# Assuring you have the most recent CatBoost release\n!pip install catboost -U","e15d4522":"# Getting useful tabular processing and generator functions\n!git clone https:\/\/github.com\/lmassaron\/deep_learning_for_tabular_data.git","37a10de9":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nfrom time import time\nimport pprint\nimport joblib\n\n# Suppressing warnings because of skopt verbosity\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Classifiers\nfrom catboost import CatBoostClassifier, Pool\n\n# Model selection\nfrom sklearn.model_selection import StratifiedKFold\n\n# Metrics\nfrom sklearn.metrics import roc_auc_score, average_precision_score\nfrom sklearn.metrics import make_scorer","8915913f":"# Loading data directly from CatBoost\nfrom catboost.datasets import amazon\n\nX, Xt = amazon()\n\ny = X[\"ACTION\"].apply(lambda x: 1 if x == 1 else 0).values\nX.drop([\"ACTION\"], axis=1, inplace=True)","4759f17a":"# Transforming all the labels of all variables\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoders = [LabelEncoder() for _ in range(X.shape[1])]\n\nfor col, column in enumerate(X.columns):\n    label_encoders[col].fit(X[column].append(Xt[column]))\n    X[column] = label_encoders[col].transform(X[column])\n    Xt[column] = label_encoders[col].transform(Xt[column])","7721262c":"# Enconding frequencies instead of labels (so we have some numeric variables)\n\ndef frequency_encoding(column, df, df_test=None):\n    frequencies = df[column].value_counts().reset_index()\n    df_values = df[[column]].merge(frequencies, how='left', \n                                   left_on=column, right_on='index').iloc[:,-1].values\n    if df_test is not None:\n        df_test_values = df_test[[column]].merge(frequencies, how='left', \n                                                 left_on=column, right_on='index').fillna(1).iloc[:,-1].values\n    else:\n        df_test_values = None\n    return df_values, df_test_values\n\nfor column in X.columns:\n    train_values, test_values = frequency_encoding(column, X, Xt)\n    X[column+'_counts'] = train_values\n    Xt[column+'_counts'] = test_values","8bf945a8":"# Pointing out which variables are categorical and which are numeric\ncategorical_variables = [col for col in X.columns if '_counts' not in col]\nnumeric_variables = [col for col in X.columns if '_counts' in col]","0d5586d8":"X.head()","f893a0cb":"Xt.head()","8b782aaa":"# Counting unique values of categorical variables\nX[categorical_variables].nunique()","d9a81686":"# Describing numeric variables\nX[numeric_variables].describe()","fb100f54":"# Initializing a CatBoostClassifier with best parameters\nbest_params = {'bagging_temperature': 0.6,\n               'border_count': 200,\n               'depth': 8,\n               'iterations': 350,\n               'l2_leaf_reg': 30,\n               'learning_rate': 0.30,\n               'random_strength': 0.01,\n               'scale_pos_weight': 0.48}\n\ncatb = CatBoostClassifier(**best_params,\n                          loss_function='Logloss',\n                          eval_metric = 'AUC',\n                          nan_mode='Min',\n                          thread_count=2,\n                          verbose = False)","b659b4e9":"# Setting a 5-fold stratified cross-validation (note: shuffle=True)\nSEED = 42\nFOLDS = 5\n\nskf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)","ac67c6cb":"# CV interations\n\nroc_auc = list()\naverage_precision = list()\noof = np.zeros(len(X))\nbest_iteration = list()\n\nfor train_idx, test_idx in skf.split(X, y):\n    X_train, y_train = X.iloc[train_idx, :], y[train_idx]\n    X_test, y_test = X.iloc[test_idx, :], y[test_idx]\n    \n    train = Pool(data=X_train, \n             label=y_train,            \n             feature_names=list(X_train.columns),\n             cat_features=categorical_variables)\n\n    test = Pool(data=X_test, \n                label=y_test,\n                feature_names=list(X_test.columns),\n                cat_features=categorical_variables)\n\n    catb.fit(train,\n             verbose_eval=100, \n             early_stopping_rounds=50,\n             eval_set=test,\n             use_best_model=True,\n             #task_type = \"GPU\",\n             plot=False)\n    \n    best_iteration.append(catb.best_iteration_)\n    preds = catb.predict_proba(X_test)\n    \n    oof[test_idx] = preds[:,1]\n    \n    roc_auc.append(roc_auc_score(y_true=y_test, y_score=preds[:,1]))\n    average_precision.append(average_precision_score(y_true=y_test, y_score=preds[:,1]))","fa9f24b4":"print(\"Average cv roc auc score %0.3f \u00b1 %0.3f\" % (np.mean(roc_auc), np.std(roc_auc)))\nprint(\"Average cv roc average precision %0.3f \u00b1 %0.3f\" % (np.mean(average_precision), np.std(average_precision)))\n\nprint(\"Roc auc score OOF %0.3f\" % roc_auc_score(y_true=y, y_score=oof))\nprint(\"Average precision OOF %0.3f\" % average_precision_score(y_true=y, y_score=oof))\n","4536b7bd":"# Using catboost on all the data for predictions\n\nbest_params = {'bagging_temperature': 0.6,\n               'border_count': 200,\n               'depth': 8,\n               'iterations': int(np.median(best_iteration) * 1.3),\n               'l2_leaf_reg': 30,\n               'learning_rate': 0.30,\n               'random_strength': 0.01,\n               'scale_pos_weight': 0.48}\n\ncatb = CatBoostClassifier(**best_params,\n                          loss_function='Logloss',\n                          eval_metric = 'AUC',\n                          nan_mode='Min',\n                          thread_count=2,\n                          verbose = False)\n\ntrain = Pool(data=X, \n             label=y,            \n             feature_names=list(X_train.columns),\n             cat_features=categorical_variables)\n\ncatb.fit(train,\n         verbose_eval=100,\n         #task_type = \"GPU\",\n         plot=False)\n\nsubmission = pd.DataFrame(Xt.id)\nXt_pool = Pool(data=Xt[list(X_train.columns)],\n               feature_names=list(X_train.columns),\n               cat_features=categorical_variables)\nsubmission['Action'] = catb.predict_proba(Xt_pool)[:,1]\nsubmission.to_csv(\"catboost_submission.csv\", index=False)\n\ncat_boost_submission = submission.copy()","4ca2b549":"import tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam, Nadam\nfrom tensorflow.keras.layers import Input, Embedding, Reshape, GlobalAveragePooling1D\nfrom tensorflow.keras.layers import Flatten, concatenate, Concatenate, Lambda, Dropout, SpatialDropout1D\nfrom tensorflow.keras.layers import Reshape, MaxPooling1D,BatchNormalization, AveragePooling1D, Conv1D\nfrom tensorflow.keras.layers import Activation, LeakyReLU\nfrom tensorflow.keras.optimizers import SGD, Adam, Nadam\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.regularizers import l2, l1_l2\nfrom keras.losses import binary_crossentropy\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import average_precision_score\n\nimport matplotlib.pyplot as plt","95aac110":"# Registering custom activations suitable for tabular problems\n\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras.layers import Activation, LeakyReLU\nfrom deep_learning_for_tabular_data.tabular import gelu, Mish, mish\n\n# Add gelu so we can use it as a string\nget_custom_objects().update({'gelu': Activation(gelu)})\n\n# Add mish so we can use it as a string\nget_custom_objects().update({'mish': Mish(mish)})\n\n# Add leaky-relu so we can use it as a string\nget_custom_objects().update({'leaky-relu': Activation(LeakyReLU(alpha=0.2))})","8bdab8fb":"# Parametric architecture\n\ndef tabular_dnn(numeric_variables, categorical_variables, categorical_counts,\n                feature_selection_dropout=0.2, categorical_dropout=0.1,\n                first_dense = 256, second_dense = 256, dense_dropout = 0.2, \n                activation_type=gelu):\n    \n    numerical_inputs = Input(shape=(len(numeric_variables),))\n    numerical_normalization = BatchNormalization()(numerical_inputs)\n    numerical_feature_selection = Dropout(feature_selection_dropout)(numerical_normalization)\n\n    categorical_inputs = []\n    categorical_embeddings = []\n    for category in  categorical_variables:\n        categorical_inputs.append(Input(shape=[1], name=category))\n        category_counts = categorical_counts[category]\n        categorical_embeddings.append(\n            Embedding(category_counts+1, \n                      int(np.log1p(category_counts)+1), \n                      name = category + \"_embed\")(categorical_inputs[-1]))\n\n    categorical_logits = Concatenate(name = \"categorical_conc\")([Flatten()(SpatialDropout1D(categorical_dropout)(cat_emb)) \n                                                                 for cat_emb in categorical_embeddings])\n\n    x = concatenate([numerical_feature_selection, categorical_logits])\n    x = Dense(first_dense, activation=activation_type)(x)\n    x = Dropout(dense_dropout)(x)  \n    x = Dense(second_dense, activation=activation_type)(x)\n    x = Dropout(dense_dropout)(x)\n    output = Dense(1, activation=\"sigmoid\")(x)\n    model = Model([numerical_inputs] + categorical_inputs, output)\n    \n    return model","4faa222d":"# Useful functions\n\nfrom tensorflow.keras.metrics import AUC\n\ndef mAP(y_true, y_pred):\n    return tf.py_func(average_precision_score, (y_true, y_pred), tf.double)\n\ndef compile_model(model, loss, metrics, optimizer):\n    model.compile(loss=loss, metrics=metrics, optimizer=optimizer)\n    return model\n\ndef plot_keras_history(history, measures):\n    \"\"\"\n    history: Keras training history\n    measures = list of names of measures\n    \"\"\"\n    rows = len(measures) \/\/ 2 + len(measures) % 2\n    fig, panels = plt.subplots(rows, 2, figsize=(15, 5))\n    plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.4, wspace=0.2)\n    try:\n        panels = [item for sublist in panels for item in sublist]\n    except:\n        pass\n    for k, measure in enumerate(measures):\n        panel = panels[k]\n        panel.set_title(measure + ' history')\n        panel.plot(history.epoch, history.history[measure], label=\"Train \"+measure)\n        panel.plot(history.epoch, history.history[\"val_\"+measure], label=\"Validation \"+measure)\n        panel.set(xlabel='epochs', ylabel=measure)\n        panel.legend()\n        \n    plt.show(fig)","e80aeb66":"# Global training settings\n\nSEED = 42\nFOLDS = 5\nBATCH_SIZE = 512","b077dfc6":"# Defining callbacks\n\nmeasure_to_monitor = 'val_auc' \nmodality = 'max'\n\nearly_stopping = EarlyStopping(monitor=measure_to_monitor, \n                               mode=modality, \n                               patience=5, \n                               verbose=0)\n\nmodel_checkpoint = ModelCheckpoint('best.model', \n                                   monitor=measure_to_monitor, \n                                   mode=modality, \n                                   save_best_only=True, \n                                   verbose=0)\n\nreduce_learning = ReduceLROnPlateau(monitor=measure_to_monitor, mode=modality, \n                                    factor=0.25, patience=2, min_lr=1e-6, verbose=0)","c16890f7":"from deep_learning_for_tabular_data.tabular import TabularTransformer, DataGenerator\n\n# Setting the CV strategy\nskf = StratifiedKFold(n_splits=FOLDS, \n                      shuffle=True, \n                      random_state=SEED)\n\n# CV Iteration\nroc_auc = list()\naverage_precision = list()\noof = np.zeros(len(X))\nbest_iteration = list()\n\nfor fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n    \n    tb = TabularTransformer(numeric = numeric_variables,\n                        ordinal = [],\n                        lowcat  = [],\n                        highcat = categorical_variables)\n\n    tb.fit(X.iloc[train_idx])\n    sizes = tb.shape(X.iloc[train_idx])\n    categorical_levels = dict(zip(categorical_variables, sizes[1:]))\n    print(f\"Input array sizes: {sizes}\")\n    print(f\"Categorical levels: {categorical_levels}\\n\")\n    \n    model = tabular_dnn(numeric_variables, categorical_variables,\n                        categorical_levels, \n                        feature_selection_dropout=0.1,\n                        categorical_dropout=0.1,\n                        first_dense = 256,\n                        second_dense = 256,\n                        dense_dropout = 0.1,\n                        activation_type=gelu)\n    \n    model = compile_model(model, binary_crossentropy, [AUC(name='auc'), mAP], Adam(learning_rate=0.0001))\n    \n    train_batch = DataGenerator(X.iloc[train_idx], \n                                y[train_idx],\n                                tabular_transformer=tb,\n                                batch_size=BATCH_SIZE,\n                                shuffle=True)\n    \n    history = model.fit_generator(train_batch,\n                                  validation_data=(tb.transform(X.iloc[test_idx]), y[test_idx]),\n                                  epochs=30,\n                                  callbacks=[model_checkpoint, early_stopping, reduce_learning],\n                                  class_weight=[1.0, (np.sum(y==0) \/ np.sum(y==1))],\n                                  verbose=1)\n    \n    print(\"\\nFOLD %i\" % fold)\n    plot_keras_history(history, measures=['auc', 'loss'])\n    \n    best_iteration.append(np.argmax(history.history['val_auc']) + 1)\n    preds = model.predict(tb.transform(X.iloc[test_idx]),\n                          verbose=1,\n                          batch_size=1024).flatten()\n\n    oof[test_idx] = preds\n\n    roc_auc.append(roc_auc_score(y_true=y[test_idx], y_score=preds))\n    average_precision.append(average_precision_score(y_true=y[test_idx], y_score=preds))","5d918ddb":"print(\"Average cv roc auc score %0.3f \u00b1 %0.3f\" % (np.mean(roc_auc), np.std(roc_auc)))\nprint(\"Average cv roc average precision %0.3f \u00b1 %0.3f\" % (np.mean(average_precision), np.std(average_precision)))\n\nprint(\"Roc auc score OOF %0.3f\" % roc_auc_score(y_true=y, y_score=oof))\nprint(\"Average precision OOF %0.3f\" % average_precision_score(y_true=y, y_score=oof))","47563ec2":"# We train on all the examples, using a rule of thumb for the number of iterations\n\ntb = TabularTransformer(numeric = numeric_variables,\n                        ordinal = [],\n                        lowcat  = [],\n                        highcat = categorical_variables)\n\ntb.fit(X)\nsizes = tb.shape(X)\ncategorical_levels = dict(zip(categorical_variables, sizes[1:]))\nprint(f\"Input array sizes: {sizes}\")\nprint(f\"Categorical levels: {categorical_levels}\\n\")\n\nmodel = tabular_dnn(numeric_variables, categorical_variables,\n                    categorical_levels, \n                    feature_selection_dropout=0.1,\n                    categorical_dropout=0.1,\n                    first_dense = 256,\n                    second_dense = 256,\n                    dense_dropout = 0.1,\n                    activation_type=gelu)\n    \nmodel = compile_model(model, binary_crossentropy, [AUC(name='auc'), mAP], Adam(learning_rate=0.0001))    \n\ntrain_batch = DataGenerator(X, y,\n                            tabular_transformer=tb,\n                            batch_size=BATCH_SIZE,\n                            shuffle=True)\n\nhistory = model.fit_generator(train_batch,\n                              epochs=int(np.median(best_iteration)),\n                              class_weight=[1.0, (np.sum(y==0) \/ np.sum(y==1))],\n                              verbose=1)","d451bfcd":"# Predicting and submission\npreds = model.predict(tb.transform(Xt[X.columns]),\n                      verbose=1,\n                      batch_size=1024).flatten()\n\nsubmission = pd.DataFrame(Xt.id)\nsubmission['Action'] = preds\nsubmission.to_csv(\"tabular_dnn_submission.csv\", index=False)\n\ntabular_dnn_submission = submission.copy()","1153ef07":"from scipy.stats import rankdata\n\n# We use normalized ranks because probabilities emissions from the two models may differ\ndnn_rank = rankdata(tabular_dnn_submission.Action, method='dense') \/ len(Xt)\ncat_rank = rankdata(cat_boost_submission.Action, method='dense') \/ len(Xt)\n\nsubmission = pd.DataFrame(Xt.id)\nsubmission['Action'] = 0.5 * dnn_rank + 0.5 * cat_rank \nsubmission.to_csv(\"blended_submission.csv\", index=False)","c8c3858f":"## Kernel's motivations\n\nIn this kernel we are going to show how deep learning (using TensorFlow 2.0 and Keras) can be effectively used when the problem involved regards tabular data. We will compare a deep neural network solution (DNN) to the best in class gradient boosting machine (GBM) algorithm when high cardinality variables are present and you will discover how not only a DNN solution is comparable, but also how it can integrate nicely with a GBM solution.","b129f7df":"## Using a datagenerator for tabular data\nThe process of preparing the data is made easier and more efficient using a datagenerator from https:\/\/github.com\/lmassaron\/deep_learning_for_tabular_data The generator can handle into different pipelines numeric, ordinal, categorical (both low and high cardinality respectively by one hot encoding and embedding layers) variables and it automatically takes care of missing values. You can also find in the repository a couple of useful activations for tabular data models: gelu (https:\/\/arxiv.org\/abs\/1606.08415) and mish (https:\/\/arxiv.org\/abs\/1908.08681).","e98a760d":"## Amazon.com - Employee Access Challenge\n\nWhen an employee at any company starts work, they first need to obtain the computer access necessary to fulfill their role. This access may allow an employee to read\/manipulate resources through various applications or web portals. It is assumed that employees fulfilling the functions of a given role will access the same or similar resources. It is often the case that employees figure out the access they need as they encounter roadblocks during their daily work (e.g. not able to log into a reporting portal). A knowledgeable supervisor then takes time to manually grant the needed access in order to overcome access obstacles. As employees move throughout a company, this access discovery\/recovery cycle wastes a nontrivial amount of time and money.\n\nThere is a considerable amount of data regarding an employee\u2019s role within an organization and the resources to which they have access. Given the data related to current employees and their provisioned access, models can be built that automatically determine access privileges as employees enter and leave roles within a company. These auto-access models seek to minimize the human involvement required to grant or revoke employee access.","1ecea417":"# Using deep learning","bc9c0a4c":"## Blending together the GBM and DNN solutions","f5b6c621":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/3338\/media\/gate.png)","32800f78":"# Using CatBoost"}}