{"cell_type":{"f353d8dd":"code","71da1030":"code","d0d3d0af":"code","426c280a":"code","24ec29e4":"code","62069c91":"code","9ec40243":"code","9445a35b":"code","a9edba59":"code","f0bad11e":"code","5429f97e":"code","15bca978":"code","91faa3e8":"code","cc460a66":"code","2d6ad5ee":"code","3563b3a7":"code","884fb42e":"code","19582e0e":"code","f835872f":"code","9ff0641f":"code","9fc64c2f":"code","8c73be4c":"code","8d71832b":"code","b7c4e6e6":"code","35d06512":"code","03febcab":"code","95256abc":"code","29c8f5b4":"code","44e4672f":"code","de8cf406":"code","574555d6":"code","7bf88491":"code","a0e3bd97":"code","14d7702f":"code","a20c3868":"code","d64d799a":"code","8f2092c8":"code","f7b74eea":"code","7b9ae2bc":"code","e93f6b54":"code","84d09e13":"code","c8338573":"code","3097da52":"code","ba071fe0":"markdown","fd252b75":"markdown","22d233cb":"markdown","5a7afba9":"markdown","2aa65ddc":"markdown","f4ed6bdc":"markdown","c6b9f78b":"markdown","9846e347":"markdown","33b8fe10":"markdown","e74feb4b":"markdown","5c2df834":"markdown","4004021e":"markdown","edbd781d":"markdown","b6118828":"markdown","ba6925f7":"markdown","402d13bc":"markdown","89dfbd24":"markdown","2ef0f801":"markdown","df07454d":"markdown","b7276fde":"markdown","29fea284":"markdown","f6f818d2":"markdown"},"source":{"f353d8dd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV","71da1030":"# Importing the dataset\ndata = pd.read_csv(\"..\/input\/data.csv\")","d0d3d0af":"# Printing the 1st 5 columns\ndata.head()","426c280a":"# Printing the dimensions of data\ndata.shape","24ec29e4":"# Viewing the column heading\ndata.columns","62069c91":"# Inspecting the target variable\ndata.diagnosis.value_counts()","9ec40243":"data.dtypes","9445a35b":"# Identifying the unique number of values in the dataset\ndata.nunique()","a9edba59":"# Checking if any NULL values are present in the dataset\ndata.isnull().sum()","f0bad11e":"data.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)","5429f97e":"# See rows with missing values\ndata[data.isnull().any(axis=1)]","15bca978":"# Viewing the data statistics\ndata.describe()","91faa3e8":"# Finding out the correlation between the features\ncorr = data.corr()\ncorr.shape","cc460a66":"# Plotting the heatmap of correlation between features\nplt.figure(figsize=(20,20))\nsns.heatmap(corr, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap='Greens')\nplt.show()","2d6ad5ee":"# Analyzing the target variable\n\nplt.title('Count of cancer type')\nsns.countplot(data['diagnosis'])\nplt.xlabel('Cancer lethality')\nplt.ylabel('Count')\nplt.show()","3563b3a7":"# Plotting correlation between diagnosis and radius\n\nplt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nsns.boxplot(x=\"diagnosis\", y=\"radius_mean\", data=data)\nplt.subplot(1,2,2)\nsns.violinplot(x=\"diagnosis\", y=\"radius_mean\", data=data)\nplt.show()","884fb42e":"# Plotting correlation between diagnosis and concativity\n\nplt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nsns.boxplot(x=\"diagnosis\", y=\"concavity_mean\", data=data)\nplt.subplot(1,2,2)\nsns.violinplot(x=\"diagnosis\", y=\"concavity_mean\", data=data)\nplt.show()","19582e0e":"# Distribution density plot KDE (kernel density estimate)\nsns.FacetGrid(data, hue=\"diagnosis\", height=6).map(sns.kdeplot, \"radius_mean\").add_legend()\nplt.show()","f835872f":"# Plotting the distribution of the mean radius\nsns.stripplot(x=\"diagnosis\", y=\"radius_mean\", data=data, jitter=True, edgecolor=\"gray\")\nplt.show()","9ff0641f":"# Plotting bivariate relations between each pair of features (4 features x4 so 16 graphs) with hue = \"diagnosis\"\nsns.pairplot(data, hue=\"diagnosis\", vars = [\"radius_mean\", \"concavity_mean\", \"smoothness_mean\", \"texture_mean\"])\nplt.show()","9fc64c2f":"# Spliting target variable and independent variables\nX = data.drop(['diagnosis'], axis = 1)\ny = data['diagnosis']","8c73be4c":"# Splitting the data into training set and testset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\nprint(\"Size of training set:\", X_train.shape)\nprint(\"Size of test set:\", X_test.shape)","8d71832b":"# Logistic Regression\n\n# Import library for LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\n# Create a Logistic regression classifier\nlogreg = LogisticRegression()\n\n# Train the model using the training sets \nlogreg.fit(X_train, y_train)","b7c4e6e6":"# Prediction on test data\ny_pred = logreg.predict(X_test)","35d06512":"# Calculating the accuracy\nacc_logreg = round( metrics.accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of Logistic Regression model : ', acc_logreg )","03febcab":"# Gaussian Naive Bayes\n\n# Import library of Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\n\n# Create a Gaussian Classifier\nmodel = GaussianNB()\n\n# Train the model using the training sets \nmodel.fit(X_train,y_train)","95256abc":"# Prediction on test set\ny_pred = model.predict(X_test)","29c8f5b4":"# Calculating the accuracy\nacc_nb = round( metrics.accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of Gaussian Naive Bayes model : ', acc_nb )","44e4672f":"# Decision Tree Classifier\n\n# Import Decision tree classifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Create a Decision tree classifier model\nclf = DecisionTreeClassifier()","de8cf406":"# Hyperparameter Optimization\nparameters = {'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [2, 3, 5, 10, 50], \n              'min_samples_split': [2, 3, 50, 100],\n              'min_samples_leaf': [1, 5, 8, 10]\n             }\n\n# Run the grid search\ngrid_obj = GridSearchCV(clf, parameters)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nclf = grid_obj.best_estimator_\n\n# Train the model using the training sets \nclf.fit(X_train, y_train)","574555d6":"# Prediction on test set\ny_pred = clf.predict(X_test)","7bf88491":"# Calculating the accuracy\nacc_dt = round( metrics.accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of Decision Tree model : ', acc_dt )","a0e3bd97":"# Random Forest Classifier\n\n# Import library of RandomForestClassifier model\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create a Random Forest Classifier\nrf = RandomForestClassifier()\n\n# Hyperparameter Optimization\nparameters = {'n_estimators': [4, 6, 9, 10, 15], \n              'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [2, 3, 5, 10], \n              'min_samples_split': [2, 3, 5],\n              'min_samples_leaf': [1, 5, 8]\n             }\n\n# Run the grid search\ngrid_obj = GridSearchCV(rf, parameters)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the rf to the best combination of parameters\nrf = grid_obj.best_estimator_\n\n# Train the model using the training sets \nrf.fit(X_train,y_train)","14d7702f":"# Prediction on test data\ny_pred = rf.predict(X_test)","a20c3868":"# Calculating the accuracy\nacc_rf = round( metrics.accuracy_score(y_test, y_pred) * 100 , 2 )\nprint( 'Accuracy of Random Forest model : ', acc_rf )","d64d799a":"# SVM Classifier\n\n# Creating scaled set to be used in model to improve the results\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","8f2092c8":"# Import Library of Support Vector Machine model\nfrom sklearn import svm\n\n# Create a Support Vector Classifier\nsvc = svm.SVC()\n\n# Hyperparameter Optimization\nparameters = [\n  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n]\n\n# Run the grid search\ngrid_obj = GridSearchCV(svc, parameters)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the svc to the best combination of parameters\nsvc = grid_obj.best_estimator_\n\n# Train the model using the training sets \nsvc.fit(X_train,y_train)","f7b74eea":"# Prediction on test data\ny_pred = svc.predict(X_test)","7b9ae2bc":"# Calculating the accuracy\nacc_svm = round( metrics.accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of SVM model : ', acc_svm )","e93f6b54":"# K - Nearest Neighbors\n\n# Import library of KNeighborsClassifier model\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Create a KNN Classifier\nknn = KNeighborsClassifier()\n\n# Hyperparameter Optimization\nparameters = {'n_neighbors': [3, 4, 5, 10], \n              'weights': ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n              'leaf_size' : [10, 20, 30, 50]\n             }\n\n# Run the grid search\ngrid_obj = GridSearchCV(knn, parameters)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the knn to the best combination of parameters\nknn = grid_obj.best_estimator_\n\n# Train the model using the training sets \nknn.fit(X_train,y_train)","84d09e13":"# Prediction on test data\ny_pred = knn.predict(X_test)","c8338573":"# Calculating the accuracy\nacc_knn = round( metrics.accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of KNN model : ', acc_knn )","3097da52":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Naive Bayes', 'Decision Tree', 'Random Forest', 'Support Vector Machines', \n              'K - Nearest Neighbors'],\n    'Score': [acc_logreg, acc_nb, acc_dt, acc_rf, acc_svm, acc_knn]})\nmodels.sort_values(by='Score', ascending=False)","ba071fe0":"## Support Vector Machine","fd252b75":"## Decision Tree","22d233cb":"The above heatmap shows us a correlation between the various features. The closer the value to 1, the higher is the correlation between the pair of features.","5a7afba9":"## Dataset: [Breast Cancer Wisconsin Dataset](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29)","2aa65ddc":"## Random Forest","f4ed6bdc":"### Data Visualization","c6b9f78b":"### Once the data is cleaned, we split the data into training set and test set to prepare it for our machine learning model in a suitable proportion.","9846e347":"Boxplot shows us the minimum, first quartile (Q1), median, third quartile (Q3), and maximum. It is useful for detecting the outliers. <br>\nViolin plot shows us the kernel density estimate on each side.","33b8fe10":"### Import all the necessary header files as follows:\n* pandas : An open source library used for data manipulation, cleaning, analysis and visualization. \n* numpy : A library used to manipulate multi-dimensional data in the form of numpy arrays with useful in-built functions. \n* matplotlib : A library used for plotting and visualization of data. \n* seaborn : A library based on matplotlib which is used for plotting of data. \n* sklearn.metrics : A library used to calculate the accuracy, precision and recall. ","e74feb4b":"## We can see from the above table that SVM classifier works best for this dataset.\n### Before hyperparameter tuning, I was getting an accuracy of mere 92.40 for SVM model but after parameter tuning, we obtained an accuracy of 98.25. Hence parameter tuning is important to get a very high accuracy.  ","5c2df834":"## K - Nearest Neighbors","4004021e":"### Inspecting and cleaning the data","edbd781d":"## Evaluation and comparision of all the models","b6118828":"Please upvote if you found this kernel useful! :) <br>\nFeedbacks appreciated.","ba6925f7":"# Breast Cancer EDA","402d13bc":"#### Here is blog that is quite informative and might be useful to learn more about Hyperparameter Tuning: https:\/\/neptune.ai\/blog\/hyperparameter-tuning-in-python-a-complete-guide-2020","89dfbd24":"Dropping the Unnamed: 32 and the id column since these do not provide any useful information for our models.","2ef0f801":"## Logistic Regression","df07454d":"## Gaussian Naive Bayes","b7276fde":"Follow the above mentioned steps to tune the parameters.","29fea284":"## Hyperparameter Optimization\n\nSteps to tune the parameters:\n1. Prioritize those parameters which have the most effect on our model. (Example: n-neighbors bor KNN, n-estimators for random forest etc.)\n2. Set various values to these parameters and store them in a dictionary as shown below.\n3. Create an object of the GridSearchCV class and assign the parameters to it.\n4. Fit the training set in the object.\n5. We will get the best parameters from the best_estimator_ property of the object.\n6. Use this object to fit training set to your classifier.","f6f818d2":"### In this tutorial, we will learn how to select the best parameters for our models. We will learn how to use GridSearchCV from the sklearn.model_selection package to tune all the parameters.\n\n### Hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm.\n\n### It can be as simple as the following:\n* How many trees should I include in my random forest?\n* What degree of polynomial features should I use for my linear model?\n* What should be the maximum depth allowed for my decision tree?\n* How many layers should I have in my neural network?\n* What should I set my learning rate to for gradient descent?"}}