{"cell_type":{"9a3c47c5":"code","62c90304":"code","bcee7fb5":"code","688ab38e":"code","6539b49a":"code","eb23bf4e":"markdown","6b8d8387":"markdown","25dd6e6b":"markdown","22b8c3f0":"markdown"},"source":{"9a3c47c5":"from random import randint\nimport numpy as np\n\nfrom keras.layers import Input, LSTM, Dense\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical","62c90304":"def generate_seq(n_features, length):\n    seq = [randint(0,n_features-1) for _ in range(length)]\n    return np.array(seq)\n\ndef one_hot_encode(seq, n_features):\n    return to_categorical(seq, num_classes=n_features)\n\ndef decode_one_hot(vector):\n    return [np.argmax(y) for y in vector]\n\ndef generate_example(length, n_features, out_index):\n    sequence = generate_seq(n_features, length)\n    encoded = one_hot_encode(sequence, n_features)\n    \n    X = encoded.reshape((1, length, n_features))\n    y = encoded[out_index].reshape((1,n_features))\n    \n    return X, y","bcee7fb5":"\n# define model\nlength = 5\nn_features = 10\nout_index = 2\nmodel = Sequential()\nmodel.add(LSTM(25, input_shape=(length, n_features)))\nmodel.add(Dense(n_features, activation='softmax')) \nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \nmodel.summary()\n# fit model\nfor i in range(10000):\n    X, y = generate_example(length, n_features, out_index)\n    model.fit(X, y, epochs=1, verbose=2)","688ab38e":"# evaluate model\ncorrect = 0\nfor i in range(100):\n    X, y = generate_example(length, n_features, out_index)\n    yhat = model.predict(X)\n    if decode_one_hot(yhat) == decode_one_hot(y):\n        correct += 1\nprint('Accuracy: %f' % ((correct\/100.0)*100.0))","6539b49a":"# prediction on new data\nX, y = generate_example(length, n_features, out_index) \nyhat = model.predict(X)\nprint('Sequence: %s' % [decode_one_hot(x) for x in X]) \nprint('Expected: %s' % decode_one_hot(y)) \nprint('Predicted: %s' % decode_one_hot(yhat))","eb23bf4e":"<center><h2>Echo Sequence Prediction<\/h2><\/center>\n<br>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nThe echo sequence prediction problem is a contrived problem for demonstrating the memory capability of the Vanilla LSTM. The task is that, given a sequence of random integers as input, to output the value of a random integer at a specific time input step that is not specified to the model.\nFor example, given the input sequence of random integers [5, 3, 2] and the chosen time step was the second value, then the expected output is 3. Technically, this is a sequence classification problem; it is formulated as a many-to-one prediction problem, where there are multiple input time steps and one output time step at the end of the sequence.\n<\/div><br>\n<h3>The Vanilla LSTM<\/h3>\nA simple LSTM configuration is the Vanilla LSTM.It is the LSTM architecture defined in the original 1997 LSTM paper and the architecture that will give good results on most small sequence prediction problems. \n<br><br>The Vanilla LSTM is defined as:\n<ol>\n<li> Input layer.\n<li> Fully connected LSTM hidden layer. \n<li> Fully connected output layer.\n<\/ol>\n\n![image.png](attachment:image.png)\n    ","6b8d8387":"<h3><center>Evaluating","25dd6e6b":"<h3><center>Modelling","22b8c3f0":"<center><h3>Importing Libraries"}}