{"cell_type":{"573d3aba":"code","60e7218b":"code","122b93e5":"code","228875d9":"code","9e03887f":"code","68ca7d86":"code","0a433f8a":"code","8ae7fcaa":"code","0360b52b":"code","f405713b":"code","10c71922":"code","00102d15":"code","a3ce4ccc":"code","a695f611":"code","f11596ea":"code","2a3dc93a":"code","f476be96":"code","c1b030d1":"code","92fc7df7":"code","226c89a2":"code","d4a93aa7":"code","b2db85d6":"code","476e67c4":"code","a6738bf1":"code","893c3267":"code","2272655a":"code","335a8639":"code","d89bc5e8":"code","e8076bef":"code","f0358850":"code","bd8510df":"code","3dfdb0aa":"code","c15d547a":"markdown","b15a5b50":"markdown","99b339a8":"markdown","1b1a14a4":"markdown","26337a11":"markdown","e63bb818":"markdown","595bc379":"markdown","86adb5ee":"markdown","cc1582f9":"markdown","a718cc20":"markdown","519aa947":"markdown","b532b895":"markdown","9d4b77f2":"markdown","7746c67e":"markdown","df901ba3":"markdown"},"source":{"573d3aba":"# data analysis and wrangling\nimport math\nimport numpy \nimport pandas \nimport random\nimport scipy.stats\n\n# visualization\nimport seaborn ; seaborn.set(style='whitegrid') # nicer plotting style\nimport matplotlib.pyplot as pyplot\n%matplotlib inline\n\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:80% !important; }<\/style>\"))\npandas.options.display.max_columns = None  # to display all the columns","60e7218b":"import matplotlib.gridspec as gridspec\n\n# Boxplots with histograms above\ndef plot_boxplot_with_hist(df_input, target='SalePrice', ncols=5, figsize=(20, 32)):\n    \n    unique_count = df_input.nunique()\n    df = df_input[list(unique_count[unique_count <= 25].index) + ['SalePrice']].copy()\n    nrows = math.ceil((df.shape[1] - 1) \/ ncols) \n    \n    row_width_increment = 1 \/ ncols\n    col_width_decrement = 1 \/ nrows\n\n    fig = pyplot.figure(figsize=figsize)\n\n    for count, col in enumerate(df.columns.drop(target)):\n        col_num, row_num = count % ncols, count \/\/ ncols\n        left = row_width_increment * col_num\n        right = row_width_increment * (col_num + 1) - 0.05\n        top = 1 - row_num * col_width_decrement\n        bottom = 1.03 - (row_num + 1) * col_width_decrement \n\n        gs1 = gridspec.GridSpec(nrows=2, ncols=1, height_ratios=(.3, .7), left=left, bottom=bottom, top=top, right=right, hspace=0.05)\n        hist_ax = fig.add_subplot(gs1[0]) ; hist_ax.tick_params(axis='x', which='major', pad=.1)\n        seaborn.countplot(x=col, data=df, ax=hist_ax)\n        box_ax = fig.add_subplot(gs1[1]) ; box_ax.tick_params(axis='x', which='major', pad=.1)\n        seaborn.boxplot(x=col, y=target, data=df, ax=box_ax)\n        \n        if df[col].dtype == 'O':\n            hist_ax.set_xticklabels(hist_ax.get_xticklabels(), rotation=45) \n            box_ax.set_xticklabels(box_ax.get_xticklabels(), rotation=45) \n        \n        if col_num != 0:\n            hist_ax.set_ylabel('')\n            box_ax.set_ylabel('')\n\n# Scatter plots between the features and the target \ndef plot_pairwise_with_target(df_input, target='SalePrice', ncols=6):\n    \n    unique_count = df_input.nunique()\n    df = df_input[list(unique_count[unique_count > 25].index)].drop('Id', axis=1)\n    nrows = math.ceil((df.shape[1] - 1) \/ ncols) \n    \n    fig, ax = pyplot.subplots(nrows=nrows, ncols=ncols, figsize=(20, 11), sharey='row')\n    \n    for count, col in enumerate(df.columns.drop(target)):\n        this_ax = ax[count \/\/ ncols][count % ncols]\n        seaborn.regplot(x=col, y=target, data=df, color='b', fit_reg=False, scatter_kws={'s': 10, 'alpha': 0.5}, ax=this_ax)  \n        #this_ax.set_xticklabels(this_ax.get_xticklabels(), rotation=45) \n        if count % ncols != 0:\n            ax[count \/\/ ncols][count % ncols].set_ylabel('')\n            \n# Correlation matrix (upper triangle)          \ndef plot_correlation_map(df, figsize=(18,18)):\n    \n    corr = df.corr()\n    \n    # Generate a mask for the upper triangle\n    mask = numpy.zeros_like(corr, dtype=numpy.bool)\n    mask[numpy.triu_indices_from(mask)] = True\n    \n    _ , ax = pyplot.subplots(figsize=figsize)\n    _ = seaborn.heatmap(\n        corr, \n        mask=mask,\n        cmap=\"RdBu_r\",\n        square=True, \n        cbar_kws={'shrink': .7}, \n        ax=ax, \n        annot=False, \n        annot_kws={'fontsize': 9}\n    )","122b93e5":"# Load both data sets\ndf_train = pandas.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pandas.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n# Store both data sets in one dictionary in order to make each transformation step easier and clearer\ndata_sets = {'df_train': df_train, 'df_test': df_test}","228875d9":"print(data_sets['df_train'].shape, data_sets['df_test'].shape)\ndata_sets['df_train'].head(2)","9e03887f":"data_sets['df_test'].head(2)","68ca7d86":"# Differences in data types\nprint(pandas.DataFrame({'train': data_sets['df_train'].dtypes.value_counts(), 'test': data_sets['df_test'].dtypes.value_counts()}))\npandas.DataFrame({'train': data_sets['df_train'].dtypes, 'test': data_sets['df_test'].dtypes}).query('train != test')","0a433f8a":"# Valid values\npandas.DataFrame({'train': data_sets['df_train'].count(), 'test': data_sets['df_test'].count()}).sort_values('train')","8ae7fcaa":"# Transform ordinal categorical variables into numerical ones\n# For these variables the NaN's correspond to houses that don't have the feature. \n# So we'll map those to 0 (lowest rank value).\nfor key, df in data_sets.items():\n    \n    print(f'Data set being processed: {key}')\n\n    mapping = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n    cols_to_replace = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', \n                       'GarageCond', 'PoolQC']\n    df.loc[:, cols_to_replace] = df[cols_to_replace].replace(mapping).fillna(0)\n\n    mapping = {'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}\n    cols_to_replace = ['BsmtExposure']\n    df.loc[:, cols_to_replace] = df[cols_to_replace].replace(mapping).fillna(0)\n\n    mapping = {'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}\n    cols_to_replace = ['BsmtFinType1', 'BsmtFinType2']\n    df.loc[:, cols_to_replace] = df[cols_to_replace].replace(mapping).fillna(0)\n    \n    mapping = {'Unf': 1, 'RFn': 2, 'Fin': 3}\n    cols_to_replace = ['GarageFinish']\n    df.loc[:, cols_to_replace] = df[cols_to_replace].replace(mapping).fillna(0)\n    \n    mapping = {'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4}\n    cols_to_replace = ['Fence']\n    df.loc[:, cols_to_replace] = df[cols_to_replace].replace(mapping).fillna(0)\n    \n    mapping = {'Reg': 0, 'IR1': 1, 'IR2': 2, 'IR3': 3}\n    cols_to_replace = ['LotShape']\n    df.loc[:, cols_to_replace] = df[cols_to_replace].replace(mapping)\n    \n    mapping = {'ELO': 0, 'NoSeWa': 1, 'NoSewr': 2, 'AllPub': 3}\n    cols_to_replace = ['Utilities']\n    df.loc[:, cols_to_replace] = df[cols_to_replace].replace(mapping)\n    \n    mapping = {'Gtl': 0, 'Mod': 1, 'Sev': 2}\n    cols_to_replace = ['LandSlope']\n    df.loc[:, cols_to_replace] = df[cols_to_replace].replace(mapping)\n    \n    mapping = {'Sal': 0, 'Sev': 1, 'Maj2': 2, 'Maj1': 3, 'Mod': 4, 'Min2': 5, 'Min1': 6, 'Typ': 7}\n    cols_to_replace = ['Functional']\n    df.loc[:, cols_to_replace] = df[cols_to_replace].replace(mapping)\n    \n    mapping = {'N': 0, 'P': 1, 'Y': 2}\n    cols_to_replace = ['PavedDrive']\n    df.loc[:, cols_to_replace] = df[cols_to_replace].replace(mapping)\n    \n    data_sets[key] = df\n    \n    print(f'Data set processed successfully: {key}')","0360b52b":"# Transform numerical features into string (for the ones that are categories)\nfor key, df in data_sets.items():\n    \n    print(f'Data set being processed: {key}')\n    \n    mapping = {num: f'MS{num}' for num in data_sets[key]['MSSubClass'].unique()}\n    cols_to_replace = ['MSSubClass']\n    df.loc[:, cols_to_replace] = df[cols_to_replace].replace(mapping)\n    \n    print(f'Data set processed successfully: {key}')","f405713b":"# We fill missing values with 0 for the numerical columns (expect GarageYrBlt)\n# and with a new category ('no feature') for the remaining nominal columns\nfor key, df in data_sets.items():\n    \n    print(f'Data set being processed: {key}')\n    \n    df = df.apply(lambda x: x.fillna(0) if (x.dtype in ['float64', 'int64'] and x.name != 'GarageYrBlt') else x)\n                                          \n    df.loc[:, ['Alley', 'GarageType', 'MasVnrType', 'MiscFeature']] = df[['Alley', 'GarageType', 'MasVnrType', 'MiscFeature']].fillna('No feature')\n    \n    data_sets[key] = df\n                                                                                    \n    print(f'Data set processed successfully: {key}')","10c71922":"# Distribution of SalePrice\nseaborn.distplot(data_sets['df_train']['SalePrice'])\n\nprint(\"Skewness: {}\".format(df_train['SalePrice'].skew()))\ndata_sets['df_train'][['SalePrice']].describe()","00102d15":"# We first take a look at the categorical (and ordinal) variables (up to 12 unique values)\ndata_sets['df_train'].nunique().sort_values().iloc[:20]","a3ce4ccc":"# We observe the relation between SalePrice and all the categorical variables, while checking their distribution\nplot_boxplot_with_hist(data_sets['df_train'])","a695f611":"# Pairwise relations between SalePrice and continuous features\nplot_pairwise_with_target(data_sets['df_train'])","f11596ea":"# Linear correlations between numerical features\nplot_correlation_map(data_sets['df_train'])","2a3dc93a":"data_sets['df_train'].corr()['SalePrice'].sort_values(ascending=False).iloc[1:].plot(kind='bar', figsize=(12, 4), color='b')","f476be96":"for key, df in data_sets.items():\n    \n    print(f'Data set being processed: {key}')\n    \n    # 'YearSold' and 'MoSold' into 'YrsFromSale' (we take latest year 2010 as a reference).\n    # Same for 'YearBuilt' which we transform into 'YrsFromConst' (better interpretable).\n    df = df.assign(\n        TimeFromSale=2010 - df['YrSold'] + (12 - df['MoSold']) \/ 12,\n        YrsFromConst=2010 - df['YearBuilt'],\n        YrsFromRemod=2010 - df['YearRemodAdd'], \n    ).drop(['YrSold', 'MoSold', 'YearBuilt', 'YearRemodAdd'], axis=1)\n    \n    # Combine the freshly created 'YrsFromConst' and 'YrsFromRemod' into a single feature (take the mean)\n    df = df.assign(\n        YrsFromCombined=0.5 * (df['YrsFromConst'] + df['YrsFromRemod']),\n    ).drop(['YrsFromConst', 'YrsFromRemod'], axis=1)\n        \n    # Combine 'FullBath' and 'HalfBath' into BathTot (same for 'BsmtFullBath' and 'BsmtHalfBath').\n    df = df.assign(\n        BathTot=lambda x: x['FullBath'] + 0.5 * x['HalfBath'],\n        BsmtBathTot=lambda x: x['BsmtFullBath'] + 0.5 * x['BsmtHalfBath'],\n    ).drop(['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath'], axis=1)\n    \n    # Combine highly correlated features with underlying logic ('GarageQual' and 'GarageCond').\n    df = df.assign(\n        FirePlaceScore=lambda x: x['Fireplaces'] * x['FireplaceQu'],\n        GarageScore=lambda x: x['GarageQual'] * x['GarageCond'],\n    ).drop(['Fireplaces', 'FireplaceQu', 'GarageQual', 'GarageCond'], axis=1)\n    \n    # MasVnrType\n    mapping = {'None': 0, 'No feature': 0, 'BrkCmn': 1, 'BrkFace': 1, 'CBlock': 1, 'Stone': 1}\n    df = df.assign(\n        MasVnrType=df['MasVnrType'].replace(mapping),\n        MasVnrTypeScore=lambda x: x['MasVnrType'] * x['MasVnrArea']\n    ).drop(['MasVnrArea', 'MasVnrType'], axis=1)\n    \n    # BsmtScore\n    df = df.assign(\n        BsmtFinScore1=df['BsmtFinSF1'] * df['BsmtFinType1'],\n        BsmtFinScore2=df['BsmtFinSF2'] * df['BsmtFinType2'],\n    ).drop(['BsmtFinSF1', 'BsmtFinType1', 'BsmtFinSF2', 'BsmtFinType2'], axis=1)\n    \n    # 'GarageCars' and 'GarageArea' highly correlated (almost 0.9 on df_train!).\n    # We choose to drop 'GarageArea' which is slightly less correlated with the target.\n    # 'GarageYrBlt' is correlated with 'YearBuilt' and there is no obvious way to deal with the missing values.\n    # We choose to drop it.\n    df = df.drop(['GarageArea', 'GarageYrBlt'], axis=1)\n    \n    # Drop 'PoolArea' as the feature is too correlated to 'PoolQC'\n    # and according to box plots the relation between 'SalePrice' and 'PoolQC' seems more robust\n    df = df.drop(['PoolArea'], axis=1)\n    \n    # Drop 'BsmtFinType2' as too correlated with 'BsmtFinSF2'\n    # Same thing for 'TotalBsmtSF' too correlated with '1stFlrSF' (which has more non zero values)\n    df = df.drop(['TotalBsmtSF'], axis=1)\n    \n    # Drop 'TotRmsAbvGrd' as too correlated with 'GrLivArea'\n    df = df.drop(['TotRmsAbvGrd'], axis=1)\n    \n    data_sets[key] = df\n    \n    print(f'Data set processed successfully: {key}')","c1b030d1":"plot_correlation_map(data_sets['df_train'], figsize=(13, 13))","92fc7df7":"modes_train = data_sets['df_train'].select_dtypes('object').mode().iloc[0]\n\nfor key, df in data_sets.items():\n    \n    print(f'Data set being processed: {key}')\n    \n    temp = df.select_dtypes('object').isnull().any(axis=0)\n    print(temp[temp].index)\n    data_sets[key] = df.select_dtypes('object').fillna(modes_train).join(df.select_dtypes(exclude=['object']))\n                                                                                     \n    print(f'Data set processed successfully: {key} \\n')\n    \n    \ndel modes_train, temp","226c89a2":"data_sets['df_train'].select_dtypes(['int64', 'float64']).info()","d4a93aa7":"# compute skewness on df_train\nskewed_feats = data_sets['df_train'].select_dtypes(['int64', 'float64']).drop('SalePrice', axis=1).apply(lambda x: scipy.stats.skew(x.dropna()))\nskewed_feats = skewed_feats[skewed_feats > 1]\nskewed_feats = list(skewed_feats.index)\nprint(skewed_feats)\n\n# Remove the categorical variables from the list (it's definitely not a nice way to do it)\nfor feature in ['LotShape', 'LandSlope', 'ExterCond', 'BsmtExposure', 'KitchenAbvGr', 'PoolQC', 'Fence', 'MasVnrTypeScore',\n                'BsmtFinScore1', 'BsmtFinScore2']:\n    skewed_feats.remove(feature)\n    \n\n# Remove the skewed (and continuous) features from both data sets\nfor key, df in data_sets.items():\n    print(f'Data set being processed: {key}')\n    df[list(skewed_feats)] = numpy.log1p(df[list(skewed_feats)])\n    print(f'Data set processed successfully: {key} \\n')","b2db85d6":"# Encode categorical columns into dummy variables\nfor key, df in data_sets.items():\n    \n    print(f'Data set being processed: {key}')\n    \n    columns_to_process = df.select_dtypes(include='object').columns\n    other_columns = df.select_dtypes(exclude='object').columns\n    \n    data_sets[key] = df.loc[:, other_columns].join(pandas.get_dummies(df.loc[:, columns_to_process], drop_first=True))\n                                                                                                                                  \n    print(f'Data set processed successfully: {key}')\n    \n    \n# There are categories in df_train that don't appear in df_test.\n# We therefore have to add the corresponding columns in df_test and fill the column with 0.\n# Besides there is an additional column in df_test ('MSSubClass_MS150') which we drop.\ndef complete_df_test():\n    cols_dict = {}\n    for col in data_sets['df_train'].columns.drop('SalePrice'):\n        if not(col in data_sets['df_test'].columns):\n            cols_dict[col] = lambda x: 0\n    data_sets['df_test'] = data_sets['df_test'].assign(**cols_dict).drop(['MSSubClass_MS150'], axis=1)\n    \ncomplete_df_test()","476e67c4":"data_sets['df_train'].shape, data_sets['df_test'].shape","a6738bf1":"# Regressors\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\n\n# Scalers\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\n\n# Pipeline\nfrom sklearn.pipeline import Pipeline\n\n# Model Selection \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer","893c3267":"def generate_train_test_with_preselection(threshold_mult=2, plot=True):\n    \n    # We use log(SalePrice) as a target since the evaluation metric is the RMSE on log(SalePrice)\n    y_train = numpy.log(data_sets['df_train']['SalePrice'])\n    X_train = data_sets['df_train'].drop(['SalePrice', 'Id'], axis=1)\n    X_test = data_sets['df_test'].loc[:, list(X_train.columns)].copy()\n    assert (X_train.columns == X_test.columns).all(), 'At least one mismatch in the columns ordering'\n    \n    # Fit randome forest regressor and plot feature importance\n    reg = RandomForestRegressor(n_estimators=100, criterion='mse', max_features='sqrt', random_state=42)\n    reg = reg.fit(X_train, y_train)\n\n    df = pandas.DataFrame({\n        'features': X_train.columns,\n        'importance': reg.feature_importances_,\n    }).sort_values('importance', ascending=True)\n    \n    threshold = threshold_mult * df['importance'].mean()\n    if plot:\n        fig, ax = pyplot.subplots(figsize=(15, 34))\n        df.plot(y='importance', x='features', kind='barh', color='b', ax=ax)\n        ax.axvline(threshold, linestyle='--', color='black')\n        pyplot.show()\n        \n    # Pre feature selection\n    features_selector = SelectFromModel(reg, prefit=True, threshold=threshold)\n    reduced_X_train = X_train[X_train.columns[features_selector.get_support(indices=True)]]\n    reduced_X_test = X_test[X_test.columns[features_selector.get_support(indices=True)]]\n            \n    return reduced_X_train, y_train, reduced_X_test\n\n\n# We also define a score function that returns directly the rmse (SalePrice is already log-transformed)\ndef rmse(y_true, y_pred):\n    return math.sqrt(mean_squared_error(y_true, y_pred))\n\nrmse_score = make_scorer(rmse, greater_is_better=False)","2272655a":"_ = generate_train_test_with_preselection()","335a8639":"reduced_X_train, y_train, reduced_X_test = generate_train_test_with_preselection(threshold_mult=.2, plot=False)\n\nreg = RandomForestRegressor()\nparameter_grid = {\n                 'n_estimators': [100],\n                 'max_depth' : [4, 6, 8, 10, 14],\n                 'max_features': ['sqrt'],\n                 'min_samples_split': [3, 5, 10],\n                 'min_samples_leaf': [3, 5, 10],\n                 }\ncross_validation = KFold(n_splits=5, shuffle=True, random_state=42)\ngrid_search = GridSearchCV(estimator=reg, param_grid=parameter_grid, cv=cross_validation, scoring=rmse_score, refit=True, return_train_score=True)\ngrid_search.fit(reduced_X_train, y_train) \n\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nprint('Best estimator: {}'.format(grid_search.best_estimator_))","d89bc5e8":"# Validation score and comparions with CV scores\npred_validation = grid_search.predict(reduced_X_train) \nprint('validation_rmse: {}'.format(math.sqrt(mean_squared_error(pred_validation, y_train))))","e8076bef":"output = grid_search.predict(reduced_X_test)\noutput = numpy.exp(output)\n\npandas.DataFrame(index=data_sets['df_test']['Id'], data=output, columns=['SalePrice']).to_csv('rf_submission.csv')","f0358850":"reduced_X_train, y_train, reduced_X_test = generate_train_test_with_preselection(plot=False, threshold_mult=.1)\n\npipe = Pipeline([\n    ('scaler', RobustScaler(with_centering=False)),\n    ('estimator', Ridge(fit_intercept=True, normalize=False)),\n])\n\nalphas = [0.01, 0.05, 0.1, 0.3, 0.5, 1, 2, 3, 5, 8, 10, 12, 14, 16, 18, 30, 40, 50]\nparameter_grid = {'estimator__alpha': alphas}\ncross_validation = KFold(n_splits=5, shuffle=True, random_state=42)\ngrid_search = GridSearchCV(estimator=pipe, param_grid=parameter_grid, cv=cross_validation, scoring=rmse_score, refit=True, return_train_score=True)\ngrid_search.fit(reduced_X_train, y_train) \n\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\nprint('Best estimator: {}'.format(grid_search.best_estimator_))","bd8510df":"# Validation score and comparions with CV scores\npred_validation = grid_search.predict(reduced_X_train) \nentire_train_score = math.sqrt(mean_squared_error(pred_validation, y_train))\nprint('validation_rmse: {}'.format(entire_train_score))\n\nfig, ax = pyplot.subplots(figsize=(12, 4))\npyplot.rcParams.update({'lines.markeredgewidth': 1})\nax.errorbar(alphas, -grid_search.cv_results_['mean_train_score'], yerr=0.5*grid_search.cv_results_['std_train_score'], \n                fmt='-o', capsize=5, label='cv_mean_train_score')\nax.errorbar(alphas, -grid_search.cv_results_['mean_test_score'], yerr=0.5*grid_search.cv_results_['std_test_score'], \n                fmt='-o', capsize=5, label='cv_mean_test_score')\nax.axhline(entire_train_score, linestyle='--', color='black')\npyplot.legend()\npyplot.show()","3dfdb0aa":"output = grid_search.predict(reduced_X_test)\noutput = numpy.exp(output)\n\npandas.DataFrame(index=data_sets['df_test']['Id'], data=output, columns=['SalePrice']).to_csv('submission_ridge.csv')","c15d547a":"# II) Data Engineering\n\n- Combine related Variables together\n\n\n- Handle actual missing values\n\n\n- Log transform skewed data\n\n\n- Encode categorical features into dummy variables\n","b15a5b50":"### Data Extraction\n\n\n**Load both the train and the test sets and see what they look like.**\n\n- both data sets have a similar shape: `df_train` has 79 potential features, 1 target (`SalePrice`) and 1460 obs, and `df_test` has 79 features and 1459 obs\n","99b339a8":"**Random Forest**","1b1a14a4":"# I) Exploratory Data Analysis\n\n\n- Data extraction\n- Preprocessing and cleaning\n- Plotting (distributions, pairwise relations, correlations)\n- Assumptions","26337a11":"## From data exploration to ridge regression, a complete guide for the House Price competition.\n\nThis notebook shows a thorough approach to the House Prices competition, from data exploration to modelling and final predictions. Here I focused on the clarity of every steps, and chose to use simple models to emphasize the importance of the data analysis and the feature engineering stages. Using a simple ridge regression allowed me to score in the top 20% of the competition.\n\nIf you use elements of this notebook in your own work, I would really appreciate to receive some credit back!","e63bb818":"**Ridge regression**","595bc379":"### Preprocessing and cleaning\n\n**Show the data types (check whether train and test are similar in that sense).**\n\n- There are some differences in the numerical types of 8 columns (if we exclude `SalePrice`), but in this case it's not an issue: int64 and float64 can be used interchangeably.\n\n\n**Show the number of missing values.**\n\nWe count the number of valid (not NaN) values for each column and decide whether we should keep it: \n\n- `Alley`, `Fence`, `FireplaceQu`, `MiscFeature` have lots of missing values as many houses don't have those, but the features can still be used as dummy variables (1 if the house has this feature and 0 otherwise) and we have a significant number of obs for each.\n\n\n- Similarly `PoolQC` and `PoolArea` have lots of NaN's as most houses don't have a swimming pool, but we decide to keep these features anyway.\n\n\n- There is a significant number of missing values for `Basement`-related and `Garage`-related features given that some houses don't have those.\n    \n    \n*Remark: here it's important to note that missing values are not really missing, they just mean that the house doesn't have the given feature (swimming pool, fence, fireplace...).*\n\n\n**Preprocessing: some object-type columns into numerical and vice-versa.**\n\nThis part requires a thorough reading of the data description file.\n\n- Here we have some categorical variables (with the `object` type) which are **nominal** (they don't have an intrinsic ordering to their categories) and others which are **ordinal** (we can order the categories). The idea is to transform the ordinal columns, as there is information in the order, into numerical ones so that we can analyse them in a better way (for example we'll be able to compute their correlation with other numerical features). \n    \n    We first identify all the categorical variables which are ordinal (this bit may be subjective), that is which resemble a ranking (e.g. from bad to good):\n`OverallQual`, `OverallCond`, `ExterQual`, `ExterCond`, `BsmtQual`, `BsmtCond`, `BsmtExposure`, `BsmtFinType1`, `BsmtFinType2`, `HeatingQC`, `KitchenQual`, `Functional`, `FireplaceQu`, `GarageFinish`, `GarageQual`, `GarageCond`, `Fence`, `LotShape`, `Utilities`, `LandSlope`, `Functional`, `PavedDrive`\n\n\n- Inversely we have some numerical features which correspond to categories: `MSSubClass`.\n\n**Fill Nan's for some numerical and object columns.**\n\nThere are some NaN's remaining in numerical columns. It's very likely that those points correspond to houses that don't have the related feature. For example a house with a NaN for `LotFrontage` (probably) doesn't have a lot frontage at all. It is therefore reasonable to fill these NaN's with 0. The exception is `GarageYrBlt`- the mapping Nan's to 0 doesn't make sense in terms of year of building. \n\nFor categorical features as `GarageFinish`, the NaN's correspond to houses with no garage ; we will therefore replace those NaN's with a 'No garage' category. ","86adb5ee":"### Pre-feature selection\n\nWe now have 200 features, which is a large number relatively to the size of our train set. We therefore want to decrease the dimensionality by doing a feature selection before fitting a regressor. The main reason behind this step is to avoid overfitting. Additionnally it speeds up the fitting process. \n\nThe random forest regressor is a convenient way to measure feature importance. \nHere we use a function running a random forest regressor on the entire train set. This function takes `threshold_mult` as an argument that allows to select the level of selectivity we want to apply. And we use SelectFromModel from sklearn to make the selection directly. ","cc1582f9":"### Plotting\n\n**Distribution of target.**\n\nWe first have a quick look at the distribution of `SalePrice`. We can see that the distribution has a long right tail and is therefore (as it's unimodal) positively skewed. It's not a real issue; besides we will fit our model on `log(SalePrice)` which won't be skewed anymore.\n\n\n**Pairwise relations with target.**\n\nWe examine the pairwise relations between `SalePrice` and each of the features. We split the analysis between the categorical features and the continuous ones. \nIn the sake of graphics readibility we consider the categorical features to be the ones with < 25 unique values and the others to be continuous.\n\nFor these categorical variables we look at both the boxplots and the histogramms, which give a good idea about the pairwise relations. \n\n- There are some obvious trends, for instance for `OverallQual`, `ExterQual` or `BsmstQual`. There is a good chance these features will have a big impact in the regression model.\n\n\n- Most of the distribution are imbalanced with a prevailing category. That's the case for `Zoning`, `Street`, `Heating` and many others. This is also the case for ordinal variables, for which the middle levels are over-represented.\n\n\nFor the continous variables we look at the scatter plots. We observe some strong pairwise relations for `LotArea` or `TotalBsmtSF`, and some outliers for the 'square feet variables'.\n\n**Correlations.**\n\nWe look at the correlation matrix and can observe strong correlations, which are not really surprising. For example `Fireplaces` and `FireplaceQu` are highly correlated, same for `GarageArea` and `GarageCars`. Because of how we encoded missing values some features have high correlation. This is the case for `PoolQC` and `PoolArea`: we replaced NaN's with 0 for the former, while there already was a 0 for the latter. We will show in what follows how to deal with these variables.","a718cc20":"### Combine related Variables together\n\n- We merge related features together to reduce the overall number of features. For example we combine `YrSold` and `MoSold` into one feature, and `FullBath`and `HalfBath` into another one.\n\n\n- We drop some features that are too correlated together, and keep the ones that (seemingly) carry more information. For instance we keep `GarageCars` and drop `GarageAera` which could be more volatile, and which is slighlty less correlated to the target.\n\n\n- We then plot the correlation matrix to check that there are not too strong correlations remaining among the numerical features. ","519aa947":"### Log transform skewed data\n\nThis step of the process is optional. One of the main reasons to log transform skewed numerical variables is to make them more robust to outliers: the impact of an extreme value x will be lighten when considering log(1 + x). Besides it increases the performance of the model on the train and test sets.\n\nHere the square feet-related features (which are skewed) are the ones subject to outliers. ","b532b895":"###  Hyperparameters tuning and model fitting\n\nHere we compare two different estimators: the random forest and the ridge regression.\n\nThe former gives really good results on the train set but there is a big drawdown when running predictions on the test set. In the current configuration it would indicate an overfitting, in spite of the cross-validation.\n\nThe ridge regressor is more stable and gives good results on the test set (public leaderboard). A few modifications (I'll let you try!) can improve the results signficantly on both the train and test sets. For example adding polynomial features for the most important features is a good start.","9d4b77f2":"# III) Modelling\n\n- Pre-feature selection\n\n\n- Cross-validation for hyperparameters and model fitting ","7746c67e":"### Handle actual missing values \n\nThere are a few remaining missing values in the test_set for some categorical features (`MsZoning`, `Utilites`, `Exteriort1st`...). Here the absence of value doesn't really correspond to the absence of feature. There is no obvious way to interpret those. \n\nWe therefore need to fill those before encoding the categories into dummy variables ; we choose to do it by using the most frequent category.","df901ba3":"### Create dummy variables for categories of each nominal column\n\nWe finally have the data sets ready to fit a model, they both have 200 features."}}