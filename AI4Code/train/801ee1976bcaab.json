{"cell_type":{"c07c8671":"code","25f0bf6d":"code","a6fde026":"code","9a804598":"code","8977d9bf":"code","8fb1d957":"code","d599c025":"code","a876805b":"code","0566a676":"code","81f59cb1":"code","659d5cd5":"code","681b5954":"code","5e65187f":"code","d1bfe053":"code","1144fa8c":"code","2bcc6471":"code","8b765786":"markdown","32db09fa":"markdown","49cb4f85":"markdown","fa5dbb7f":"markdown","9ec0e96a":"markdown","a7427ed9":"markdown","b82cbc53":"markdown","db307c64":"markdown","120670e9":"markdown","b771f179":"markdown","35841f21":"markdown","6abd7ca4":"markdown","02df156b":"markdown","d311c1a5":"markdown","2e45d13b":"markdown","f8630902":"markdown"},"source":{"c07c8671":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.basemap import Basemap\n%matplotlib inline","25f0bf6d":"train = pd.read_json('..\/input\/train.json')\ntest  = pd.read_json('..\/input\/test.json')\nprint(f'Training set is of size:{train.shape}')\nprint(f'Test set is of size:{test.shape}')\ntrain.head(1)","a6fde026":"def PlotNormHist(data, axes, binaryFeat):\n    param = [False, True]\n    for i,cur_ax in enumerate(axes):\n        cur_data = data[data[binaryFeat]==param[i]]\n        int_level = cur_data['interest_level'].value_counts()\n        int_level = int_level\/sum(int_level)\n        sns.barplot(int_level.index, int_level.values, alpha=0.8,\n                    order=['low','medium','high'], ax=cur_ax)\n        cur_ax.set_xlabel(param[i], fontsize=15)\n        cur_ax.set_ylim(bottom=0, top=1)\n        cur_ax.grid()","9a804598":"train['nPhotos'] = train['photos'].apply(lambda x: min(10, len(x)))","8977d9bf":"plt.figure(figsize=(10,5))\nsns.violinplot(x='interest_level', y='nPhotos', data=train, order=['low','medium','high'])\nplt.xlabel('# Interest Level', fontsize=12)\nplt.ylabel('# of Photos', fontsize=12)\nplt.grid()\nplt.show()","8fb1d957":"train['hasDesc'] = train['description'].apply(lambda x: len(x.strip())!=0)\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\nPlotNormHist(train, axes, 'hasDesc')","d599c025":"long_llim = np.percentile(train.longitude.values, 1)\nlong_ulimit = np.percentile(train.longitude.values, 99)\nlat_llim = np.percentile(train.latitude.values, 1)\nlat_ulimit = np.percentile(train.latitude.values, 99)\ntrain = train[(train['longitude']>long_llim) & (train['longitude']<long_ulimit) & \n              (train['latitude']>lat_llim) & (train['latitude']<lat_ulimit)]\nlats = list(train['latitude'])\nlons = list(train['longitude'])","a876805b":"fig = plt.figure(figsize=(15, 15))\nm = Basemap(projection='merc',llcrnrlat=min(lats),urcrnrlat=max(lats),\\\n            llcrnrlon=min(lons),urcrnrlon=max(lons), resolution='h')\nx, y = m(lons,lats)\nsns.scatterplot(x, y, hue=train['interest_level'], style=train['interest_level'])","0566a676":"ulimit = np.percentile(train.price.values, 99)\ntrain['price'][train['price']>ulimit] = ulimit\n\nplt.figure(figsize=(8,6))\nsns.distplot(train.price.values, bins=50, kde=True)\nplt.xlabel('price', fontsize=12)\nplt.grid()\nplt.show()","81f59cb1":"plt.figure(figsize=(10,5))\nsns.violinplot(x='interest_level', y='price', data=train, order=['low','medium','high'])\nplt.xlabel('# Interest Level', fontsize=12)\nplt.ylabel('Price', fontsize=12)\nplt.grid()\nplt.show()","659d5cd5":"feat_dict = {}\nfor ind, row in train.iterrows():\n    for f in row['features']:\n        f = f.lower().replace('-', '')\n        if f in feat_dict:\n            feat_dict[f] += 1\n        else:\n            feat_dict[f] = 1 ","681b5954":"new_feat_dict = {}\nfor k,v in feat_dict.items():\n    if v>50: new_feat_dict[k] = v  \nnew_feat_dict.keys()","5e65187f":"def CreateCategFeat(data, features_list):\n    f_dict = {'hasParking':['parking', 'garage'], 'hasGym':['gym', 'fitness', 'health club'],\n              'hasPool':['swimming pool', 'pool'], 'noFee':['no fee', \"no broker's fees\"],\n              'hasElevator':['elevator'], 'hasGarden':['garden', 'patio', 'outdoor space'],\n              'isFurnished': ['furnished', 'fully  equipped'], \n              'reducedFee':['reduced fee', 'low fee'],\n              'hasAC':['air conditioning', 'central a\/c', 'a\/c', 'central air', 'central ac'],\n              'hasRoof':['roof', 'sundeck', 'private deck', 'deck'],\n              'petFriendly':['pets allowed', 'pet friendly', 'dogs allowed', 'cats allowed'],\n              'shareable':['shares ok'], 'freeMonth':['month free'],\n              'utilIncluded':['utilities included']}\n    for feature in features_list:\n        data[feature] = False\n        for ind, row in data.iterrows():\n            for f in row['features']:\n                f = f.lower().replace('-', '')\n                if any(e in f for e in f_dict[feature]):\n                    data.at[ind, feature]= True     \ncat_features = ['hasParking', 'hasGym', 'hasPool', 'noFee', 'hasElevator',\n                'hasGarden', 'isFurnished', 'reducedFee', 'hasAC', 'hasRoof',\n                'petFriendly', 'shareable', 'freeMonth', 'utilIncluded']\nCreateCategFeat(train, cat_features)","d1bfe053":"for cur_feature in cat_features:\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,3))\n    PlotNormHist(train, axes, cur_feature)\n    fig.suptitle(cur_feature, fontsize=16)","1144fa8c":"import datetime\ntrain['created'] = pd.to_datetime(train['created'])\ntrain['month']   = train['created'].dt.month\nplt.figure(figsize=(8,6))\nsns.countplot(x='month', hue='interest_level', data=train, hue_order=['low','medium','high'])\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('month', fontsize=12)\nplt.grid()","2bcc6471":"train['weekday'] = train['created'].apply(lambda x: x.weekday())\nplt.figure(figsize=(8,6))\nsns.countplot(x='weekday', hue='interest_level', data=train, hue_order=['low','medium','high'])\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Weekday', fontsize=12)\nplt.grid()","8b765786":"<h2>Photos<\/h2>\nAt this stage we will try not to use the images at all, but it can be usefull to check if the total number of images per specific listing can help us in prediction. Let's create a new numerical column nPhotos and threshold the maximal number of photos to 10 to avoid strange outliers:","32db09fa":"<h2>Description<\/h2>\nFor now, we won't use the provided 'description' field. We can assume that written keywords in this field can play significant role in the prediction, so we want at least create a new column that will have a binary indication for every listing - True if the listing had some sort of description or False otherwise. Let's check the distribution over the \"interest level\" using the simple countplot:","49cb4f85":"<h2>Geographical Distribution<\/h2>\nWe can assume that geographical location of the listing will have a significant influence on the interest levels for this listing. Specifically we can think of a distribution of neighborhoods where some of them would be \"prestigiuos\" and other less. Let's try and plot the listings on map projection:","fa5dbb7f":"We can see that a lack of photos significantly decreases the chances of the listing to be popular as we would expect.","9ec0e96a":"We will use a binary normalized histogram in further investigations, so let's define the method once to reuse it later:","a7427ed9":"<h2>Summary:<\/h2>\nPeople are interested in apartment listings that have **description** and **photos**. Apartments with extreme outlied **prices** have less interest levels. \nGeographical location has significant value in terms of **latitude**, **longitude**.\nPeople look for *furnished* , *air-conditioned* apartments that don't have an additional *fee* , have a *reduced fee* or have other $ perks as *free month* or *utilities included*.\nOther features, exspecially *parking, gym, elevator* or *garden* can be important, but may be masked by the influence of the price.\nIn addition it looks like *shareable apartments* are popular, since more young population looks for roommates apartments.\n","b82cbc53":"Now let's check the distribution over the \"interest level\", using the \"violin\" plot:","db307c64":"We can see that the density of the classes indeed differs, depending on the geographical location. There is no \"hard\" separator, but the usefullnes of the {lat, long} feature is obviuos. It may be nice to try and run a KNN classification and maybe create an additional feature that measures the distance of the listing from the center.","120670e9":"Some common features can have different variations, for example {'parking', 'onsite garage','garage'} are basically the same and can be very significant (deriving from our daily experience trying to find parking).\nLet's check for example someof the features for significance on interest level:","b771f179":"Let's see how the training data and the test data look:","35841f21":"<h2>Price:<\/h2>\nLet's see the distribution of the prices:\n","6abd7ca4":"Let's see the most common features:","02df156b":"<h2>Features:<\/h2>\nLet's check now the column of the categorical feature entries:\nWe will count the occurencies of every feature:","d311c1a5":"<h2>Introduction<\/h2>\nFirst let's explore a raw data a little bit, check the distributions, look for outliers and plan for future feature extraction. Start with importing neccessary libraries:","2e45d13b":"How does the price varies over the interest categories:","f8630902":"It is obvious that a lack of description lowers the chances of the listing to become popular."}}