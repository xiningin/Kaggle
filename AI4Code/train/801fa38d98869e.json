{"cell_type":{"e35c5002":"code","84aa8c7a":"code","a80558d8":"code","bf63e01c":"code","4d05cf5a":"code","cdd53117":"code","fedd3956":"code","60a05c5a":"markdown","eebf97e5":"markdown","040f3df9":"markdown","e2a54a66":"markdown","13fbfcce":"markdown","36db0cec":"markdown"},"source":{"e35c5002":"!pip install transformers==3.0.2 --quiet\n!pip install nlp --quiet","84aa8c7a":"TASK= {\n    'udpos': {},\n    'panx': {},\n    'xnli': {},\n    'pawsx': {},\n    'xquad': {},\n    'mlqa': {},\n    'tydiqa': {},\n    'bucc2018': {},\n    'tatoeba': {}\n}\nTASK['udpos']['train'] = ('xtreme', 'udpos.English','train')\nTASK['udpos']['epochs'] = 10\nTASK['udpos']['max seq length'] = 128\nTASK['udpos']['learning rate'] = 2e-5\nTASK['udpos']['weight decay'] = 0\nTASK['udpos']['warmup step'] = 0\nTASK['udpos']['validation'] = ('xtreme', 'udpos.English','validation')\nTASK['udpos']['test'] = {}\nTASK['udpos']['test']['en'] = ('xtreme', 'udpos.English','test')\nTASK['udpos']['test']['af'] = ('xtreme', 'udpos.Afrikaans','test')\nTASK['udpos']['test']['ar'] = ('xtreme', 'udpos.Arabic','test')\nTASK['udpos']['test']['eu'] = ('xtreme', 'udpos.Basque','test')\nTASK['udpos']['test']['bg'] = ('xtreme', 'udpos.Bulgarian','test')\nTASK['udpos']['test']['nl'] = ('xtreme', 'udpos.Dutch','test')\nTASK['udpos']['test']['et'] = ('xtreme', 'udpos.Estonian','test')\nTASK['udpos']['test']['fi'] = ('xtreme', 'udpos.Finnish','test')\nTASK['udpos']['test']['fr'] = ('xtreme', 'udpos.French','test')\nTASK['udpos']['test']['de'] = ('xtreme', 'udpos.German','test')\nTASK['udpos']['test']['el'] = ('xtreme', 'udpos.Greek','test')\nTASK['udpos']['test']['he'] = ('xtreme', 'udpos.Hebrew','test')\nTASK['udpos']['test']['hi'] = ('xtreme', 'udpos.Hindi','test')\nTASK['udpos']['test']['hu'] = ('xtreme', 'udpos.Hungarian','test')\nTASK['udpos']['test']['id'] = ('xtreme', 'udpos.Indonesian','test')\nTASK['udpos']['test']['it'] = ('xtreme', 'udpos.Italian','test')\nTASK['udpos']['test']['ja'] = ('xtreme', 'udpos.Japanese','test')\nTASK['udpos']['test']['kk'] = ('xtreme', 'udpos.Kazakh','test')\nTASK['udpos']['test']['ko'] = ('xtreme', 'udpos.Korean','test')\nTASK['udpos']['test']['zh'] = ('xtreme', 'udpos.Chinese','test')\nTASK['udpos']['test']['mr'] = ('xtreme', 'udpos.Marathi','test')\nTASK['udpos']['test']['fa'] = ('xtreme', 'udpos.Persian','test')\nTASK['udpos']['test']['pt'] = ('xtreme', 'udpos.Portuguese','test')\nTASK['udpos']['test']['ru'] = ('xtreme', 'udpos.Russian','test')\nTASK['udpos']['test']['es'] = ('xtreme', 'udpos.Spanish','test')\nTASK['udpos']['test']['tl'] = ('xtreme', 'udpos.Tagalog','test')\nTASK['udpos']['test']['ta'] = ('xtreme', 'udpos.Tamil','test')\nTASK['udpos']['test']['te'] = ('xtreme', 'udpos.Telugu','test')\nTASK['udpos']['test']['th'] = ('xtreme', 'udpos.Thai','test')\nTASK['udpos']['test']['tr'] = ('xtreme', 'udpos.Turkish','test')\nTASK['udpos']['test']['ur'] = ('xtreme', 'udpos.Urdu','test')\nTASK['udpos']['test']['vi'] = ('xtreme', 'udpos.Vietnamese','test')\nTASK['udpos']['test']['yo'] = ('xtreme', 'udpos.Yoruba','test')\n\n\n\nTASK['panx']['train'] = ('xtreme', 'PAN-X.en','train')\nTASK['panx']['epochs'] = 10\nTASK['panx']['max seq length'] = 128\nTASK['panx']['learning rate'] = 2e-5\nTASK['panx']['warmup step'] = 0\nTASK['panx']['weight decay'] = 0\nTASK['panx']['validation'] = ('xtreme', 'PAN-X.en','validation')\nTASK['panx']['test'] = {}\nTASK['panx']['test']['en'] = ('xtreme', 'PAN-X.en','test')\nTASK['panx']['test']['af'] = ('xtreme', 'PAN-X.af','test')\nTASK['panx']['test']['ar'] = ('xtreme', 'PAN-X.ar','test')\nTASK['panx']['test']['bg'] = ('xtreme', 'PAN-X.bg','test')\nTASK['panx']['test']['bn'] = ('xtreme', 'PAN-X.bn','test')\nTASK['panx']['test']['de'] = ('xtreme', 'PAN-X.de','test')\nTASK['panx']['test']['el'] = ('xtreme', 'PAN-X.el','test')\nTASK['panx']['test']['en'] = ('xtreme', 'PAN-X.en','test')\nTASK['panx']['test']['es'] = ('xtreme', 'PAN-X.es','test')\nTASK['panx']['test']['et'] = ('xtreme', 'PAN-X.et','test')\nTASK['panx']['test']['eu'] = ('xtreme', 'PAN-X.eu','test')\nTASK['panx']['test']['fa'] = ('xtreme', 'PAN-X.fa','test')\nTASK['panx']['test']['fi'] = ('xtreme', 'PAN-X.fi','test')\nTASK['panx']['test']['fr'] = ('xtreme', 'PAN-X.fr','test')\nTASK['panx']['test']['he'] = ('xtreme', 'PAN-X.he','test')\nTASK['panx']['test']['hi'] = ('xtreme', 'PAN-X.hi','test')\nTASK['panx']['test']['hu'] = ('xtreme', 'PAN-X.hu','test')\nTASK['panx']['test']['id'] = ('xtreme', 'PAN-X.id','test')\nTASK['panx']['test']['it'] = ('xtreme', 'PAN-X.it','test')\nTASK['panx']['test']['ja'] = ('xtreme', 'PAN-X.ja','test')\nTASK['panx']['test']['jv'] = ('xtreme', 'PAN-X.jv','test')\nTASK['panx']['test']['ka'] = ('xtreme', 'PAN-X.ka','test')\nTASK['panx']['test']['kk'] = ('xtreme', 'PAN-X.kk','test')\nTASK['panx']['test']['ko'] = ('xtreme', 'PAN-X.ko','test')\nTASK['panx']['test']['ml'] = ('xtreme', 'PAN-X.ml','test')\nTASK['panx']['test']['mr'] = ('xtreme', 'PAN-X.mr','test')\nTASK['panx']['test']['ms'] = ('xtreme', 'PAN-X.ms','test')\nTASK['panx']['test']['my'] = ('xtreme', 'PAN-X.my','test')\nTASK['panx']['test']['nl'] = ('xtreme', 'PAN-X.nl','test')\nTASK['panx']['test']['pt'] = ('xtreme', 'PAN-X.pt','test')\nTASK['panx']['test']['ru'] = ('xtreme', 'PAN-X.ru','test')\nTASK['panx']['test']['sw'] = ('xtreme', 'PAN-X.sw','test')\nTASK['panx']['test']['ta'] = ('xtreme', 'PAN-X.ta','test')\nTASK['panx']['test']['te'] = ('xtreme', 'PAN-X.te','test')\nTASK['panx']['test']['th'] = ('xtreme', 'PAN-X.th','test')\nTASK['panx']['test']['tl'] = ('xtreme', 'PAN-X.tl','test')\nTASK['panx']['test']['tr'] = ('xtreme', 'PAN-X.tr','test')\nTASK['panx']['test']['ur'] = ('xtreme', 'PAN-X.ur','test')\nTASK['panx']['test']['vi'] = ('xtreme', 'PAN-X.vi','test')\nTASK['panx']['test']['yo'] = ('xtreme', 'PAN-X.yo','test')\nTASK['panx']['test']['zh'] = ('xtreme', 'PAN-X.zh','test')\n\nTASK['xnli']['train'] = (\"multi_nli\", None, 'train')\nTASK['xnli']['epochs'] = 5\nTASK['xnli']['max seq length'] = 128\nTASK['xnli']['learning rate'] = 2e-5\nTASK['xnli']['warmup step'] = 0\nTASK['xnli']['weight decay'] = 0\nTASK['xnli']['validation_matched'] = ('multi_nli', None,'validation_matched')\nTASK['xnli']['validation_mismatched'] = ('multi_nli', 'None','validation_mismatched')\nTASK['xnli']['test'] = ('xtreme', 'XNLI','test')\n\nTASK['pawsx']['train'] = ('xtreme', 'PAWS-X.en','train')\nTASK['pawsx']['epochs'] = 5\nTASK['pawsx']['max seq length'] = 128\nTASK['pawsx']['learning rate'] = 2e-5\nTASK['pawsx']['warmup step'] = 0\nTASK['pawsx']['weight decay'] = 0\nTASK['pawsx']['validation'] = ('xtreme', 'PAWS-X.en','validation')\nTASK['pawsx']['test'] = {}\nTASK['pawsx']['test']['en'] = ('xtreme', 'PAWS-X.en','test')\nTASK['pawsx']['test']['es'] = ('xtreme', 'PAWS-X.es','test')\nTASK['pawsx']['test']['de'] = ('xtreme', 'PAWS-X.de','test')\nTASK['pawsx']['test']['fr'] = ('xtreme', 'PAWS-X.fr','test')\nTASK['pawsx']['test']['ja'] = ('xtreme', 'PAWS-X.ja','test')\nTASK['pawsx']['test']['ko'] = ('xtreme', 'PAWS-X.ko','test')\nTASK['pawsx']['test']['zh'] = ('xtreme', 'PAWS-X.zh','test')\n\nTASK['xquad']['train'] = ('xtreme', 'SQuAD','train')\nTASK['xquad']['epochs'] = 2\nTASK['xquad']['max seq length'] = 384\nTASK['xquad']['learning rate'] = 3e-5\nTASK['xquad']['warmup step'] = 500\nTASK['xquad']['weight decay'] = 0.0001 \nTASK['xquad']['validation'] = ('xtreme', 'SQuAD','validation')\nTASK['xquad']['test'] = {} \nTASK['xquad']['test']['ar'] =  ('xtreme', 'XQuAD.ar','validation')\nTASK['xquad']['test']['de'] =  ('xtreme', 'XQuAD.de','validation')\nTASK['xquad']['test']['vi'] =  ('xtreme', 'XQuAD.vi','validation')\nTASK['xquad']['test']['zh'] =  ('xtreme', 'XQuAD.zh','validation')\nTASK['xquad']['test']['en'] =  ('xtreme', 'XQuAD.en','validation')\nTASK['xquad']['test']['es'] =  ('xtreme', 'XQuAD.es','validation')\nTASK['xquad']['test']['hi'] =  ('xtreme', 'XQuAD.hi','validation')\nTASK['xquad']['test']['el'] =  ('xtreme', 'XQuAD.el','validation')\nTASK['xquad']['test']['ru'] =  ('xtreme', 'XQuAD.ru','validation')\nTASK['xquad']['test']['th'] =  ('xtreme', 'XQuAD.th','validation')\nTASK['xquad']['test']['tr'] =  ('xtreme', 'XQuAD.tr','validation')\n\n\"\"\"\nXtreme ignore subset of mlqa which if context & question with different langauge\nhttps:\/\/github.com\/google-research\/xtreme\/blob\/master\/scripts\/eval_qa.sh\necho \"MLQA\"\nfor lang in en es de ar hi vi zh; do\n echo -n \"  $lang \"\n TEST_FILE=${MLQA_DIR}\/MLQA_V1\/test\/test-context-$lang-question-$lang.json\n PRED_FILE=${MLQA_PRED_DIR}\/predictions_${lang}_.json\n python \"${EVAL_MLQA}\" \"${TEST_FILE}\" \"${PRED_FILE}\" ${lang}\ndone\n\"\"\"\nTASK['mlqa']['train'] = ('xtreme', 'SQuAD','train')\nTASK['mlqa']['epochs'] = 2\nTASK['mlqa']['max seq length'] = 384\nTASK['mlqa']['learning rate'] = 3e-5\nTASK['mlqa']['warmup step'] = 500\nTASK['mlqa']['weight decay'] = 0.0001 \nTASK['mlqa']['validation'] = ('xtreme', 'SQuAD','validation')\nTASK['mlqa']['test'] = {}\nTASK['mlqa']['test']['ar'] =  ('xtreme', 'MLQA.ar.ar','test')\nTASK['mlqa']['test']['de'] =  ('xtreme', 'MLQA.de.de','test')\nTASK['mlqa']['test']['vi'] =  ('xtreme', 'MLQA.vi.vi','test')\nTASK['mlqa']['test']['zh'] =  ('xtreme', 'MLQA.zh.zh','test')\nTASK['mlqa']['test']['en'] =  ('xtreme', 'MLQA.en.en','test')\nTASK['mlqa']['test']['es'] =  ('xtreme', 'MLQA.es.es','test')\nTASK['mlqa']['test']['hi'] =  ('xtreme', 'MLQA.hi.hi','test')\n\nTASK['tydiqa']['train'] = ('xtreme', 'tydiqa','train')\nTASK['tydiqa']['epochs'] = 2\nTASK['tydiqa']['max seq length'] = 384\nTASK['tydiqa']['learning rate'] = 3e-5\nTASK['tydiqa']['warmup step'] = 500\nTASK['tydiqa']['weight decay'] = 0.0001 \nTASK['tydiqa']['test'] =  ('xtreme', 'tydiqa','validatoin')\n\nTASK['bucc2018']['src'] = {}\nTASK['bucc2018']['max seq length'] = 512\nTASK['bucc2018']['src']['de'] =  ('xtreme', 'bucc18.de','validation')\nTASK['bucc2018']['src']['fr'] =  ('xtreme', 'bucc18.fr','validation')\nTASK['bucc2018']['src']['zh'] =  ('xtreme', 'bucc18.zh','validation')\nTASK['bucc2018']['src']['ru'] =  ('xtreme', 'bucc18.ru','validation')\n\nTASK['tatoeba']['src'] = {}\nTASK['tatoeba']['max seq length'] = 512\nTASK['tatoeba']['src']['afr'] =  ('xtreme', 'tatoeba.afr','validation')\nTASK['tatoeba']['src']['ara'] =  ('xtreme', 'tatoeba.ara','validation')\nTASK['tatoeba']['src']['ben'] =  ('xtreme', 'tatoeba.ben','validation')\nTASK['tatoeba']['src']['bul'] =  ('xtreme', 'tatoeba.bul','validation')\nTASK['tatoeba']['src']['deu'] =  ('xtreme', 'tatoeba.deu','validation')\nTASK['tatoeba']['src']['cmn'] =  ('xtreme', 'tatoeba.cmn','validation')\nTASK['tatoeba']['src']['ell'] =  ('xtreme', 'tatoeba.ell','validation')\nTASK['tatoeba']['src']['est'] =  ('xtreme', 'tatoeba.est','validation')\nTASK['tatoeba']['src']['eus'] =  ('xtreme', 'tatoeba.eus','validation')\nTASK['tatoeba']['src']['fin'] =  ('xtreme', 'tatoeba.fin','validation')\nTASK['tatoeba']['src']['fra'] =  ('xtreme', 'tatoeba.fra','validation')\nTASK['tatoeba']['src']['heb'] =  ('xtreme', 'tatoeba.heb','validation')\nTASK['tatoeba']['src']['hin'] =  ('xtreme', 'tatoeba.hin','validation')\nTASK['tatoeba']['src']['hun'] =  ('xtreme', 'tatoeba.hun','validation')\nTASK['tatoeba']['src']['ind'] =  ('xtreme', 'tatoeba.ind','validation')\nTASK['tatoeba']['src']['ita'] =  ('xtreme', 'tatoeba.ita','validation')\nTASK['tatoeba']['src']['jav'] =  ('xtreme', 'tatoeba.jav','validation')\nTASK['tatoeba']['src']['jpn'] =  ('xtreme', 'tatoeba.jpn','validation')\nTASK['tatoeba']['src']['kat'] =  ('xtreme', 'tatoeba.kat','validation')\nTASK['tatoeba']['src']['kaz'] =  ('xtreme', 'tatoeba.kaz','validation')\nTASK['tatoeba']['src']['kor'] =  ('xtreme', 'tatoeba.kor','validation')\nTASK['tatoeba']['src']['mal'] =  ('xtreme', 'tatoeba.mal','validation')\nTASK['tatoeba']['src']['mar'] =  ('xtreme', 'tatoeba.mar','validation')\nTASK['tatoeba']['src']['nld'] =  ('xtreme', 'tatoeba.nld','validation')\nTASK['tatoeba']['src']['pes'] =  ('xtreme', 'tatoeba.pes','validation')\nTASK['tatoeba']['src']['por'] =  ('xtreme', 'tatoeba.por','validation')\nTASK['tatoeba']['src']['rus'] =  ('xtreme', 'tatoeba.rus','validation')\nTASK['tatoeba']['src']['spa'] =  ('xtreme', 'tatoeba.spa','validation')\nTASK['tatoeba']['src']['swh'] =  ('xtreme', 'tatoeba.swh','validation')\nTASK['tatoeba']['src']['tam'] =  ('xtreme', 'tatoeba.tam','validation')\nTASK['tatoeba']['src']['tgl'] =  ('xtreme', 'tatoeba.tgl','validation')\nTASK['tatoeba']['src']['tha'] =  ('xtreme', 'tatoeba.tha','validation')\nTASK['tatoeba']['src']['tur'] =  ('xtreme', 'tatoeba.tur','validation')\nTASK['tatoeba']['src']['urd'] =  ('xtreme', 'tatoeba.urd','validation')\nTASK['tatoeba']['src']['vie'] =  ('xtreme', 'tatoeba.vie','validation')\n\n#copied from https:\/\/github.com\/google-research\/xtreme\/blob\/master\/third_party\/run_retrieval.py\nlang3_dict = {'ara':'ar', 'heb':'he', 'vie':'vi', 'ind':'id',\n    'jav':'jv', 'tgl':'tl', 'eus':'eu', 'mal':'ml', 'tam':'ta',\n    'tel':'te', 'afr':'af', 'nld':'nl', 'eng':'en', 'deu':'de',\n    'ell':'el', 'ben':'bn', 'hin':'hi', 'mar':'mr', 'urd':'ur',\n    'tam':'ta', 'fra':'fr', 'ita':'it', 'por':'pt', 'spa':'es',\n    'bul':'bg', 'rus':'ru', 'jpn':'ja', 'kat':'ka', 'kor':'ko',\n    'tha':'th', 'swh':'sw', 'cmn':'zh', 'kaz':'kk', 'tur':'tr',\n    'est':'et', 'fin':'fi', 'hun':'hu', 'pes':'fa'}\n\nTASK['tatoeba']['src'] ={code2: TASK['tatoeba']['src'].get(code3) for (code3, code2) in lang3_dict.items()}\n\n","a80558d8":"\nTASK2LANGS = {\n  \"pawsx\": \"de,en,es,fr,ja,ko,zh\".split(\",\"),\n  \"xnli\": \"ar,bg,de,el,en,es,fr,hi,ru,sw,th,tr,ur,vi,zh\".split(\",\"),\n  \"panx\": \"ar,he,vi,id,jv,ms,tl,eu,ml,ta,te,af,nl,en,de,el,bn,hi,mr,ur,fa,fr,it,pt,es,bg,ru,ja,ka,ko,th,sw,yo,my,zh,kk,tr,et,fi,hu\".split(\",\"),\n  \"udpos\": \"af,ar,bg,de,el,en,es,et,eu,fa,fi,fr,he,hi,hu,id,it,ja,kk,ko,mr,nl,pt,ru,ta,te,th,tl,tr,ur,vi,yo,zh\".split(\",\"),\n  \"bucc2018\": \"de,fr,ru,zh\".split(\",\"),\n  \"tatoeba\": \"ar,he,vi,id,jv,tl,eu,ml,ta,te,af,nl,de,el,bn,hi,mr,ur,fa,fr,it,pt,es,bg,ru,ja,ka,ko,th,sw,zh,kk,tr,et,fi,hu\".split(\",\"),\n  \"xquad\": \"en,es,de,el,ru,tr,ar,vi,th,zh,hi\".split(\",\"),\n  \"mlqa\": \"en,es,de,ar,hi,vi,zh\".split(\",\"),\n  \"tydiqa\": \"en,ar,bn,fi,id,ko,ru,sw,te\".split(\",\"),\n}\nkeywords = ['epochs','learning rate','warmup step', 'weight decay']\n\nassert TASK2LANGS.keys() == TASK.keys()\nfor task in TASK:\n  if 'max seq length' not in TASK[task]:\n    print(task + ': max seq length not found')\n  if task == 'bucc2018' or task == 'tatoeba':\n    for code in TASK2LANGS[task]:\n      if code not in TASK[task]['src']:\n        print(task + ': test split for '+ code +' not found')\n  else:\n    for word in keywords:\n      if word not in TASK[task]:\n        print(task + ': ' + word +' not found')\n    if task == 'xnli' or task == 'tydiqa':\n      continue\n    for code in TASK2LANGS[task]:\n      if code not in TASK[task]['test']:\n        print(task + ': test split for '+ code +' not found')\n\ndef check_dataset(ds, task):\n    pass\n\nprint('sanity check done!\\nlanguage code check is not done for xnli or tydiqa\\nplease specify data dir for panx')\n","bf63e01c":"import os, shutil\n\ncache_path = '\/root\/.cache\/huggingface\/datasets\/'\ndef pan_readonly2cache():\n    src = '\/kaggle\/input\/pan-x'\n    dst = '\/root\/.cache\/huggingface\/datasets\/xtreme'\n    for item in os.listdir(src):\n        s = os.path.join(src, item)\n        d = os.path.join(dst, item)\n        if os.path.isdir(s):\n            shutil.copytree(s, d, False, None)\n        else:\n            shutil.copy2(s, d)","4d05cf5a":"pan_readonly2cache()","cdd53117":"from nlp import load_dataset\nfor task in TASK:\n    try:\n        set_name, subset_name, split = TASK[task]['train']\n        load_dataset(set_name, subset_name)[split]\n    except:\n        pass","fedd3956":"from nlp import load_dataset\nfor task in TASK:\n    try:\n        set_name, subset_name, split = TASK[task]['train']    \n        print('training data for ' + task +' : '+str(len(load_dataset(set_name, subset_name)[split])))\n    except:\n        pass","60a05c5a":"This starter kernel provde a dictionary named TASK for mapping language code & split to nlp's own dataset url.\nThe dictionary are declared accroding to how xtreme\/github state the data split should be done for benchmarking.","eebf97e5":"Here, a nested dictionary for browsing the nlp xtreme dataset is defined.\nsome paramter for rebuilding the baseline from Xtreme, github are also declared, i.e. \"epochs\", \"max sequence length\", \"learning rate\", \"weight decay\" and \"warming step\". (You many help validating these hyperparameter, to see whether any mistake or missed configuration are there.)\n\nExample:\n\n    set_name, subset_name, split = TASK['panx']['train']\n","040f3df9":"The declaration of variable TASK2LANGS is copied from https:\/\/github.com\/google-research\/xtreme\/blob\/master\/evaluate.py#L102\n\nA sanity check is performed to ensure no language is missed for the every testing task","e2a54a66":"## Introduction\nThe Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME) benchmark is a benchmark for the evaluation of the cross-lingual generalization ability of pre-trained multilingual models. It covers 40 typologically diverse languages (spanning 12 language families) and includes nine tasks that collectively require reasoning about different levels of syntax and semantics. The languages in XTREME are selected to maximize language diversity, coverage in existing tasks, and availability of training data. Among these are many under-studied languages, such as the Dravidian languages Tamil (spoken in southern India, Sri Lanka, and Singapore), Telugu and Malayalam (spoken mainly in southern India), and the Niger-Congo languages Swahili and Yoruba, spoken in Africa.\n\nFor a full description of the benchmark, see the paper (https:\/\/arxiv.org\/abs\/2003.11080).\n\nofficial github of the dataset: https:\/\/github.com\/google-research\/xtreme\nofficial github of the library - nlp: https:\/\/github.com\/huggingface\/nlp\n\nquick referece for dataset provided by nlp: https:\/\/huggingface.co\/nlp\/viewer\/?dataset=xtreme","13fbfcce":"## size of training split for every task\nxtreme benchmark require training solely on english dataset\n\nfor machine translation task, traning is not applicable, as the accomplishment of the translation is expected to be done by exploiting the mutilingual zero shot learning ability","36db0cec":"## cached pan-x\n\nPAN-X is developed by amazon. Huggingface\/nlp can't provide direct access to pan-x due to technique or licence issue.\n\nThis kaggle dataset provide the data for pan-x files which are generated by nlp caching procedure.\n\nAs '\/kaggle\/input\/' (where the kaggle dataset is located) are readonly and nlp don't support cached data from readonly file system, call pan_readonly2cache() for moving the data from kaggle dataset to default cache directory of nlp.\n(default cache directory for: '\/root\/.cache\/huggingface\/datasets\/').\n\nYou may then access pan-x by load_dataset('xtreme', 'PAN-X.af'), after the above file copying & relocation is done.\n\n\nofficial statement about the availability of pan-x at Huggingface\/nlp: https:\/\/huggingface.co\/nlp\/viewer\/?dataset=xtreme&config=PAN-X.af\n\n        Dataset is too large to browse or requires manual download. Check it out in the nlp library!\n\n        Size: 1648648\n\n        Instructions: You need to manually download the AmazonPhotos.zip file on Amazon Cloud Drive (https:\/\/www.amazon.com\/clouddrive\/share\/d3KGCRCIYwhKJF0H3eWA26hjg2ZCRhjpEQtDL70FSBN). The folder containing the saved file can be used to load the dataset via `nlp.load_dataset(\"xtreme\", data_dir=\"<path\/to\/folder>\")."}}