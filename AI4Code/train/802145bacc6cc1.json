{"cell_type":{"d80da4c1":"code","b1a8c954":"code","bf3d38ad":"code","85bc2d8c":"code","b2986db6":"code","526dc25e":"code","22134bd6":"code","cd206987":"code","910591d2":"code","4fb2aa4b":"code","141a2e12":"code","6a378ebc":"code","76435691":"code","e5010a20":"code","155ae22e":"code","12e7fb49":"code","fa52d5d9":"code","195645d7":"code","cf274bbc":"code","c23e6fff":"code","7af5ffb5":"code","34fd7968":"code","0f45ceb2":"code","5310a18d":"code","ed9bae60":"code","dfc4663e":"code","60945219":"code","4358f945":"code","fbeb6b43":"code","b905f824":"code","136cfd26":"code","3f7155ac":"code","3dd3de5f":"code","ce830181":"code","4b6ec614":"code","ccd47210":"code","35f69d2f":"code","dfa05f83":"code","8b087221":"code","b551be7b":"code","b21488ac":"code","b0424139":"markdown","15443d95":"markdown","7ac0b614":"markdown"},"source":{"d80da4c1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b1a8c954":"import os\nimport cv2\nimport subprocess\nfrom tqdm.auto import tqdm\nimport pandas as pd\nfrom IPython.display import Video, display, HTML\nimport warnings; warnings.simplefilter(\"ignore\")\n\n\nBASE_PATH = '..\/input\/tensorflow-great-barrier-reef\/train_images\/'\n\ndf = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/train.csv\")\ndf['annotations'] = df['annotations'].apply(eval)\ndf['n_annotations'] = df['annotations'].str.len()\ndf['has_annotations'] = df['annotations'].str.len() > 0\ndf['has_2_or_more_annotations'] = df['annotations'].str.len() >= 2\ndf['doesnt_have_annotations'] = df['annotations'].str.len() == 0\ndf['image_path'] = BASE_PATH + \"video_\" + df['video_id'].astype(str) + \"\/\" + df['video_frame'].astype(str) + \".jpg\"","bf3d38ad":"df['sequence'].unique()","85bc2d8c":"df['sequence'].nunique()","b2986db6":"df.groupby(\"sequence\")['video_id'].nunique()","526dc25e":"# Videos 0 and 1 have 8 sequences, while video 2 has 4\ndf.groupby(\"video_id\")['sequence'].nunique()\n","22134bd6":"df_agg = df.groupby([\"video_id\", 'sequence']).agg({'sequence_frame': 'count', 'has_annotations': 'sum', 'doesnt_have_annotations': 'sum'})\\\n           .rename(columns={'sequence_frame': 'Total Frames', 'has_annotations': 'Frames with at least 1 object', 'doesnt_have_annotations': \"Frames with no object\"})\ndf_agg","cd206987":"df_agg.sort_values(\"Total Frames\")","910591d2":"df_agg.sort_values(\"Frames with at least 1 object\")\n","4fb2aa4b":"# image_id is a unique identifier for a row\ndf['image_id'].nunique() == len(df)","141a2e12":"df_agg.loc[[(0, 40258)]]\n","6a378ebc":"pd.set_option(\"display.max_rows\", 500)\ndf[df['sequence'] == 40258]","76435691":"df['start_cut_here'] = df['has_annotations'] & df['doesnt_have_annotations'].shift(1)  & df['doesnt_have_annotations'].shift(2)\ndf['end_cut_here'] = df['doesnt_have_annotations'] & df['has_annotations'].shift(1)  & df['has_annotations'].shift(2)\ndf['sequence_change'] = df['sequence'] != df['sequence'].shift(1)\ndf['last_row'] =  df.index == len(df)-1\ndf['cut_here'] = df['start_cut_here'] | df['end_cut_here'] | df['sequence_change'] | df['last_row']","e5010a20":"start_idx = 0\nfor subsequence_id, end_idx in enumerate(df[df['cut_here']].index):\n    df.loc[start_idx:end_idx, 'subsequence_id'] = subsequence_id\n    start_idx = end_idx","155ae22e":"df['subsequence_id'] = df['subsequence_id'].astype(int)\n","12e7fb49":"df['subsequence_id'].nunique()\n","fa52d5d9":"drop_cols = ['start_cut_here', 'end_cut_here', 'sequence_change', 'last_row', 'cut_here', 'has_2_or_more_annotations', 'doesnt_have_annotations']\ndf = df.drop(drop_cols, axis=1)\ndf.head()","195645d7":"df.groupby(\"subsequence_id\")['has_annotations'].mean().round(2).sort_values().value_counts()","cf274bbc":"df_subseq_agg = df.groupby(\"subsequence_id\")['has_annotations'].mean()\ndf_subseq_agg[~df_subseq_agg.isin([0, 1])]","c23e6fff":"df[df['subsequence_id'] == 52]","7af5ffb5":"df[df['subsequence_id'] == 54]","34fd7968":"! mkdir videos\/","0f45ceb2":"def load_image(img_path):\n    assert os.path.exists(img_path), f'{img_path} does not exist.'\n    img = cv2.imread(img_path)\n    return img\n\ndef load_image_with_annotations(img_path, annotations):\n    img = load_image(img_path)\n    if len(annotations) > 0:\n        for ann in annotations:\n            cv2.rectangle(img, (ann['x'], ann['y']),\n                (ann['x'] + ann['width'], ann['y'] + ann['height']),\n                (255, 255, 0), thickness=2,)\n    return img\n\ndef make_video(df, part_id, is_subsequence=False):\n    \"\"\"\n    Args:\n        - part_id: either a sequence or a subsequence id\n    \"\"\"\n    \n    if is_subsequence:\n        part_str = \"subsequence_id\"\n    else:\n        part_str = \"sequence\"\n    \n    print(f\"Creating video for part={part_id}, is_subsequence={is_subsequence} (querying by {part_str})\")\n    # partly borrowed from https:\/\/github.com\/RobMulla\/helmet-assignment\/blob\/main\/helmet_assignment\/video.py\n    fps = 15 # don't know exact value\n    width = 1280\n    height = 720\n    save_path = f'videos\/video_{part_str}_{part_id}.mp4'\n    tmp_path = f'videos\/tmp_video_{part_str}_{part_id}.mp4'\n    \n    \n    output_video = cv2.VideoWriter(tmp_path, cv2.VideoWriter_fourcc(*\"MP4V\"), fps, (width, height))\n    \n    df_part = df.query(f'{part_str} == @part_id')\n    for _, row in tqdm(df_part.iterrows(), total=len(df_part)):\n        img = load_image_with_annotations(row.image_path, row.annotations)\n        output_video.write(img)\n    \n    output_video.release()\n    # Not all browsers support the codec, we will re-load the file at tmp_output_path\n    # and convert to a codec that is more broadly readable using ffmpeg\n    if os.path.exists(save_path):\n        os.remove(save_path)\n    subprocess.run(\n        [\"ffmpeg\", \"-i\", tmp_path, \"-crf\", \"18\", \"-preset\", \"veryfast\", \"-vcodec\", \"libx264\", save_path],\n        stdout=subprocess.DEVNULL,\n        stderr=subprocess.DEVNULL\n    )\n    os.remove(tmp_path)\n    print(f\"Finished creating video for {part_id}... saved as {save_path}\")\n    return save_path","5310a18d":"video_path = make_video(df, 40258)","ed9bae60":"Video(video_path, width= 1280\/2, height= 720\/2)","dfc4663e":"subsequences = df.loc[df['sequence'] == 40258, 'subsequence_id'].unique()\nsubsequences","60945219":"for subsequence in subsequences:\n    video_path = make_video(df, subsequence, is_subsequence=True)\n    display(HTML(f\"<h2>Subsequence ID: {subsequence}<\/h2>\"))\n    display(Video(video_path, width= 1280\/2, height= 720\/2))","4358f945":"from sklearn.model_selection import train_test_split, StratifiedKFold\ndf.head()","fbeb6b43":"df_split  = df.groupby(\"subsequence_id\").agg({'has_annotations': 'max', 'video_frame': 'count'}).astype(int).reset_index()\ndf_split.head()","b905f824":"!mkdir train-validation-split\/","136cfd26":"def analize_split(df_train, df_val, df):\n     # Analize results\n    print(f\"   Train images                 : {len(df_train) \/ len(df):.3f}\")\n    print(f\"   Val   images                 : {len(df_val) \/ len(df):.3f}\")\n    print()\n    print(f\"   Train images with annotations: {len(df_train[df_train['has_annotations']]) \/ len(df[df['has_annotations']]):.3f}\")\n    print(f\"   Val   images with annotations: {len(df_val[df_val['has_annotations']]) \/ len(df[df['has_annotations']]):.3f}\")\n    print()\n    print(f\"   Train images w\/no annotations: {len(df_train[~df_train['has_annotations']]) \/ len(df[~df['has_annotations']]):.3f}\")\n    print(f\"   Val   images w\/no annotations: {len(df_val[~df_val['has_annotations']]) \/ len(df[~df['has_annotations']]):.3f}\")\n    print()\n    print(f\"   Train mean annotations       : {df_train['n_annotations'].mean():.3f}\")\n    print(f\"   Val   mean annotations       : {df_val['n_annotations'].mean():.3f}\")\n    \n    print()","3f7155ac":"for test_size in [0.01, 0.05, 0.1, 0.2]:\n    print(f\"Generating train-validation split with {test_size*100}% validation\")\n    df_train_idx, df_val_idx = train_test_split(df_split['subsequence_id'], stratify=df_split[\"has_annotations\"], test_size=test_size, random_state=42)\n    df['is_train'] = df['subsequence_id'].isin(df_train_idx)\n    df_train, df_val = df[df['is_train']], df[~df['is_train']]\n    \n    # Print some statistics\n    analize_split(df_train, df_val, df)\n    \n    # Save to file\n    f_name = f\"train-validation-split\/train-{test_size}.csv\"\n    print(f\"Saving file to {f_name}\")\n    df.to_csv(f_name, index=False)\n    print()","3dd3de5f":"!ls -l train-validation-split\/","ce830181":"df = df.drop(\"is_train\", axis=1)","4b6ec614":"n_splits = 5\nkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2021)\nfor fold_id, (_, val_idx) in enumerate(kf.split(df_split['subsequence_id'], y=df_split[\"has_annotations\"])):\n    subseq_val_idx = df_split['subsequence_id'].iloc[val_idx]\n    df.loc[df['subsequence_id'].isin(subseq_val_idx), 'fold'] = fold_id\n    \ndf['fold'] = df['fold'].astype(int)\ndf['fold'].value_counts(dropna=False)","ccd47210":"for fold_id in df['fold'].sort_values().unique():\n    print(\"=============================\")\n    print(f\"Analyzing fold {fold_id}\")\n    df_train, df_val = df[df['fold'] != fold_id], df[df['fold'] == fold_id]\n    analize_split(df_train, df_val, df)\n    print()","35f69d2f":"!mkdir cross-validation\/","dfa05f83":"df.to_csv(\"cross-validation\/train-5folds.csv\", index=False)\n","8b087221":"n_splits = 10\nkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2021)\nfor fold_id, (_, val_idx) in enumerate(kf.split(df_split['subsequence_id'], y=df_split[\"has_annotations\"])):\n    subseq_val_idx = df_split['subsequence_id'].iloc[val_idx]\n    df.loc[df['subsequence_id'].isin(subseq_val_idx), 'fold'] = fold_id\n    \ndf['fold'] = df['fold'].astype(int)\ndf['fold'].value_counts(dropna=False)","b551be7b":"for fold_id in df['fold'].sort_values().unique():\n    print(\"=============================\")\n    print(f\"Analyzing fold {fold_id}\")\n    df_train, df_val = df[df['fold'] != fold_id], df[df['fold'] == fold_id]\n    analize_split(df_train, df_val, df)\n    print()","b21488ac":"df.to_csv(\"cross-validation\/train-10folds.csv\", index=False)","b0424139":"#### What are you trying to do in this notebook?\nMy goal for this competition is to accurate identify starfish in real-time by building an object detection model trained on underwater videos of coral reefs.\nMy work will help researchers to identify species that are threatening Australia's Great Barrier Reef and take well-informed action to protect the reef for future generations.\nIn this notebook we explore sequences as potential units for cross-validation, but since there are only 20 sequences and their sizes are quite disimilar, we propose an approach to split them into smaller chunks, that we name subsequences.\n\n#### Why are you trying it?\nTo detect crown-of-thorns starfish in underwater image data. In this competition, I will predict the presence and position of crown-of-thorns starfish in sequences of underwater images taken at various times and locations around the Great Barrier Reef. Predictions take the form of a bounding box together with a confidence score for each identified starfish. An image may contain zero or more starfish.\n\nIn this notebook we explore sequences as potential units for cross-validation, but since there are only 20 sequences and their sizes are quite disimilar, we propose an approach to split them into smaller chunks, that we name subsequences.\n\nA sequence, as stated in the data tab of the competition, is:\n\nsequence - ID of a gap-free subset of a given video. The sequence ids are not meaningfully ordered.\n\nSubsequences, as we will define them below, are parts of a sequences where objects are continually present or are continually not present. We isolate 2 kind of subsequences: with objects and with no objects.","15443d95":"#### Did it work?\nThis competition uses a hidden test set that will be served by an API to ensure you evaluate the images in the same order they were recorded within each video.\nI tried doing EDA for the provided dataset. I plan to dig much dipper and understand the data in a much better way. There are alot of things to discover from this dataset.\n\n#### What did you not understand about this process?\nWell, everything provides in the competition data page. I've no problem while working on it. If you guys don't understand the thing that I'll do in this notebook then please comment on this notebook.\n\n#### What else do you think you can try as part of this approach?\nWe solve the greatest challenges through innovative science and technology to unlock a better future for everyone. We are thinkers, problem solvers, leaders. We blaze new trails of discovery. We aim to inspire the next generation. The Great Barrier Reef Foundation creates a better future for coral reefs and their marine life through innovative projects and global advocacy efforts.","7ac0b614":"#### PLEASE UPVOTE if you find this notebook is useful for you !"}}