{"cell_type":{"256d8999":"code","9eb85897":"code","97c139ea":"code","ac7058ca":"code","96b8871d":"code","2fabe833":"code","cb7f1ff9":"code","e83bf0be":"code","2b31a379":"code","95969ce2":"code","b34af24a":"code","823ebcd3":"code","9d17261a":"code","8bf8f1ec":"code","d5ca2ed6":"code","7a6ad7cb":"code","c23d0966":"code","ad8e7480":"code","6e364142":"code","4b6c98af":"markdown","b5c6f0f1":"markdown","c91ed9f0":"markdown","fcaad1c2":"markdown","7e0e7225":"markdown","4c05ba7a":"markdown","c69dc4fa":"markdown","1a57416a":"markdown","df4c18ed":"markdown","70fdcb63":"markdown","045979df":"markdown","eafe7f41":"markdown","cde3421a":"markdown","a8c97b87":"markdown","9527d7fc":"markdown","ecd864d6":"markdown","c3b40541":"markdown"},"source":{"256d8999":"# Load relevant libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error","9eb85897":"filepath = '..\/input\/quality-prediction-in-a-mining-process\/'\nfilename = 'MiningProcess_Flotation_Plant_Database.csv'\ncols_renamed = [\n    'date',          # Timestamp of measurements, formatted YYYY-MM-DD HH:MM:SS\n    'feed_iron',     # %Iron (valuables) in the ore being fed into the flotation cell\n    'feed_silica',   # %Silica (gangue) in the ore being fed into the cell\n    'starch_flow',   # Amount of starch (reagent) added into the cell, measured in m^3\/h\n    'amina_flow',    # Amount of amina (reagent) added into the cell, measured in m^3\/h\n    'pulp_flow',     # Amount of ore pulp fed into the cell, measured in tonnes\/hour\n    'pulp_ph',       # Acidity\/alkalinity of ore pulp on a scale of 0-14\n    'pulp_density',  # Amount of ore in the pulp, between 1-3 kg\/cm^3\n    'air_col1',      # Volume of air injected into the cell, measured in Nm3\/h\n    'air_col2',\n    'air_col3',\n    'air_col4',\n    'air_col5',\n    'air_col6',\n    'air_col7',\n    'level_col1',    # Froth height in the cell, measured in mm\n    'level_col2',\n    'level_col3',\n    'level_col4',\n    'level_col5',\n    'level_col6',\n    'level_col7',\n    'conc_iron',     # Lab measurement: %Iron in the end of flotation process\n    'conc_silica']   # Lab measurement: %Silica in the end of flotation process\ndf = pd.read_csv(\n    filepath+filename,\n    header=0,\n    names=cols_renamed,\n    parse_dates=['date'],\n    infer_datetime_format=True,\n    decimal=',')\ndf.head()","97c139ea":"df.info()","ac7058ca":"# Resample data to hourly basis\ndf = df.set_index('date').resample('H').first()\ndf.shape","96b8871d":"nans = df[df.isna().any(axis=1)]  # Check for missing values\nprint(f'Total rows with NaNs: {nans.shape[0]}\\n')\nnans","2fabe833":"# Remove data with time discontinuity\ndf = df['2017-03-29 12:00:00':]\ndf","cb7f1ff9":"content = ['feed_iron', 'feed_silica', 'conc_iron', 'conc_silica']\npalette = ['#FB6542', '#FFBB00', '#3F681C', '#375E97']\n\n# Plot mineral content before and after flotation\nplt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(18,6))\nfor pct, color in zip(content, palette):\n    ax.plot(df.index.values, pct, data=df, color=color)\nax.set_title('Mineral content in feed and concentrate',\n             loc='left', weight='bold', size=16)\nax.set_ylabel('% Mineral')\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.legend(loc='center left')\nplt.show()","e83bf0be":"cols = list(df)\ncols.insert(0, cols.pop(         # Moving target `conc_silica` to the front\n    cols.index('conc_silica')))  # Not necessary but I prefer to do so\ndf = df.loc[:, cols]\ndf.to_csv('.\/Flotation_Dataset_by_Hour.csv')  # For safekeeping\n\n# Drop `conc_iron` then normalize all data\nvalues = df.drop('conc_iron', axis=1).values\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\nscaled[0]  # Show first element of the array","2b31a379":"# Convert series to supervised learning\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    \"\"\"\n    Frame a time series as a supervised learning dataset.\n    Arguments:\n        data: Sequence of observations as a list or NumPy array.\n        n_in: Number of lag observations as input (X).\n        n_out: Number of observations as output (y).\n        dropnan: Boolean whether or not to drop rows with NaN values.\n    Returns:\n        Pandas DataFrame of series framed for supervised learning.\n    \"\"\"\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = pd.DataFrame(data)\n    cols, names = list(), list()\n    \n    for i in range(n_in, 0, -1):   # Input sequence (t-n, ... t-1)\n        cols.append(df.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n        \n    for i in range(0, n_out):      # Forecast sequence (t, t+1, ... t+n)\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n            \n    agg = pd.concat(cols, axis=1)  # Put it all together\n    agg.columns = names\n    if dropnan:                    # Drop rows with NaN values\n        agg.dropna(inplace=True)\n        \n    # Drop columns we don't want to predict\n    drop_cols = ['var'+str(i)+'(t)' for i in range(2,23)]\n    agg.drop(columns=drop_cols, axis=1, inplace=True)   \n    return agg","95969ce2":"reframed = series_to_supervised(scaled, n_in=1, n_out=1)\nreframed  # Show reframed dataset","b34af24a":"n_features = 22             # Number of inputs for forecast\nn_hours = 1                 # Number of hours with which to lag features\nn_obs = n_hours*n_features\n\n# Define row size of each split\nn_train = int(np.round(len(reframed)*.60))\nn_valid = int(np.round(len(reframed)*.20))\nn_test = int(np.round(len(reframed)*.20))\n\n# Split dataset by row size\nvalues = reframed.values\ntrain = values[:n_train, :]\nvalid = values[n_train:(n_train+n_valid), :]\ntest = values[(n_train+n_valid):, :]\n\n# Each set further split into inputs\/features (X) and output (y)\ntrain_X, train_y = train[:, :n_obs], train[:, -1]\nvalid_X, valid_y = valid[:, :n_obs], valid[:, -1]\ntest_X, test_y = test[:, :n_obs], test[:, -1]\n\n# Reshape inputs (X) to be 3D [samples, timesteps, features]\ntrain_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\nvalid_X = valid_X.reshape((valid_X.shape[0], n_hours, n_features))\ntest_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\n\nprint(  # Show the final shape of each set\n    f'Training set  : {train_X.shape}, {train_y.shape}',\n    f'\\nValidation set: {valid_X.shape}, {valid_y.shape}',\n    f'\\nTesting set   : {test_X.shape}, {test_y.shape}')","823ebcd3":"model = Sequential([   # Define a sequential model\n    LSTM(units=16,\n         return_sequences=True,\n         input_shape=(train_X.shape[1],\n                      train_X.shape[2])),\n    LSTM(units=16),\n    Dense(1)\n])\nmodel.compile(\n    loss='mae',        # Mean absolute error\n    optimizer='adam')  # Learning rate = 0.001\nmodel.summary()        # Display model's architecture","9d17261a":"history = model.fit(   # Fit on training data\n    train_X,\n    train_y,\n    epochs=100,\n    batch_size=16,\n    validation_data=(  # Supply validation data\n        valid_X,\n        valid_y),\n    verbose=2,\n    shuffle=False,\n    callbacks=ModelCheckpoint(  # Save model\n        '.\/LSTM_Flotation_Gangue.hdf5'))","8bf8f1ec":"# Extract losses from training history\ntrain_loss = history.history['loss']\nvalid_loss = history.history['val_loss']\n\n# Plot learning curves\nplt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(9,6))\nax.plot(train_loss, color=palette[0], label='Training')\nax.plot(valid_loss, color=palette[2], label='Validation')\nax.set_title('Learning Curves', loc='left', weight='bold', size=16)\nax.set_xlabel('Epoch')\nax.set_ylabel('Loss (Mean Absolute Error)')\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.legend(loc='center right')\nplt.show()","d5ca2ed6":"print(f'Best training loss   = {min(train_loss):.4f}',\n      f'at epoch {train_loss.index(min(train_loss))}',\n      f'\\nBest validation loss = {min(valid_loss):.4f}',\n      f'at epoch {valid_loss.index(min(valid_loss))}')","7a6ad7cb":"# Make prediction using test features\nyhat = model.predict(test_X)\n\n# Reshape test data\ntest_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\ntest_y = test_y.reshape((len(test_y), 1))\n\n# Invert scaling for forecasts\ninv_yhat = np.concatenate((yhat, test_X[:, -(n_features-1):]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n\n# Invert scaling for actual values\ninv_y = np.concatenate((test_y, test_X[:, -(n_features-1):]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n\n# Calculate error scores\nmae = mean_absolute_error(inv_y, inv_yhat)\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\nprint(32*'-'+'\\nFORECAST EVALUATION'+'\\n'+32*'-',\n      f'\\nMean absolute error    : {mae:.4f}',\n      f'\\nRoot mean squared error: {rmse:.4f}')","c23d0966":"# Define date as x-axis\ntest_date = df.index[-test_y.shape[0]:]\n\n# Confidence interval 95% (Z-value 1.96)\nci = 1.96*np.std(inv_y)\/np.mean(inv_y)\n\n# Plot actual values and forecasts\nplt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(18,6))\nax.plot(test_date, inv_y, color=palette[1], label='Actual Value')\nax.plot(test_date, inv_yhat, color=palette[2], label='Forecast')\nax.fill_between(test_date, (inv_y-ci), (inv_y+ci), color=palette[3],\n                alpha=.1, label='95% Confidence Interval')\nax.set_title('%Silica in Concentrate: Actual Values and Forecasts by LSTM',\n             loc='left', weight='bold', size=16)\nax.set_ylabel('Silica in Concentrate (%)')\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.legend()\nplt.show()","ad8e7480":"# Reframe data by lagging features by 1 hour\nrf_values = df.drop('conc_iron', axis=1).values\nrf_reframed = series_to_supervised(rf_values, n_in=1, n_out=1)\n\n# Define features and target\nrf_X = rf_reframed.values[:, :-1]\nrf_y = rf_reframed.values[:, -1]\n\n# Split data into train\/test sets (80\/20)\nrf_train_X, rf_test_X, rf_train_y, rf_test_y = train_test_split(\n    rf_X, rf_y, test_size=.20, random_state=0, shuffle=False)\n\n# Normalize features\nrf_scaler = MinMaxScaler(feature_range=(0,1))\nrf_train_X = rf_scaler.fit_transform(rf_train_X)\nrf_test_X = rf_scaler.transform(rf_test_X)\n\n# Instantiate regressor\nforest = RandomForestRegressor(random_state=0)\n\n# Fit model on training data\nforest.fit(rf_train_X, rf_train_y)\n\n# Make prediction using trained model\nrf_yhat = forest.predict(rf_test_X)\n\n# Calculate error scores\nrf_mae = mean_absolute_error(rf_test_y, rf_yhat)\nrf_rmse = np.sqrt(mean_squared_error(rf_test_y, rf_yhat))\nprint(32*'-'+'\\nFORECAST EVALUATION'+'\\n'+32*'-',\n      f'\\nMean absolute error    : {rf_mae:.4f}',\n      f'\\nRoot mean squared error: {rf_rmse:.4f}')","6e364142":"# Plot actual values and forecasts\nplt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(18,6))\nax.plot(test_date, rf_test_y, color=palette[1], label='Actual Value')\nax.plot(test_date, rf_yhat, color=palette[2], label='Forecast')\nax.fill_between(test_date, (rf_test_y-ci), (rf_test_y+ci), color=palette[3],\n                alpha=.1, label='95% Confidence Interval')\nax.set_title('%Silica in Concentrate: Actual Values and Forecasts by Random Forest',\n             loc='left', weight='bold', size=16)\nax.set_ylabel('Silica in Concentrate (%)')\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.legend()\nplt.show()","4b6c98af":"# Building and Training Model","b5c6f0f1":"The following plot shows mineral content before (i.e., in the feed) and after flotation process (in the concentrate). As can be observed from the figure, the purpose of flotation is to increase recovery of iron mineral while reducing the gangue (silica).\n\nDuring some periods (e.g., May 13 to June 13), mineral content in the feed was constant but the resulting content in the concentrate fluctuated. This suggests that *%iron* and *%silica in concentrate* are not solely governed by the content of raw materials but other parameters as well (i.e., environment, process variables).","c91ed9f0":"Forecasting is performed using the trained model and on the data that was not included during training\/validation process. Forecasts and actual values are inverted back into their original scales before calculating error scores for the model. Two error metrics are considered in the forecast evaluation: (1) mean absolute error, and (2) root mean squared error.","fcaad1c2":"Column `date` is resampled to reduce the frequency of our time-series data into an hourly basis. This is achieved by selecting only the first measurements of each hour.","7e0e7225":"#### Forecast Comparison with Random Forest\nA random forest regressor, in comparison to LSTM, generates greater error and may not perform as well for long-term forecast.","4c05ba7a":"Column `var1(t-1)` until `var22(t-1)` are our features\/inputs. Measured values of the inputs are lagged 1 hour before the target `var1(t)` (*%silica in concentrate* at time *t*).\n\nTransformed data is further split into three sets: training (60%), validation (20%), and testing (20%). Flotation data in the period between `2017-03-29 12:00` and `2017-08-08 00:00` is used for training\/validation purpose, totaling 3157 hours.","c69dc4fa":"The plot above indicates that the model exhibits a good fit where training and validation losses decrease to a point of stability and there is minimal gap between the two values.","1a57416a":"# Preprocessing Time-Series Data","df4c18ed":"A comparison of forecasts and actual values of *%silica in concentrate* is displayed above for the period between 1 a.m. August 8, 2017 and 11 p.m. September 9, 2017. The forecasts largely follow the pattern of actual values and contained within the 95% confidence interval.","70fdcb63":"Upon inspecting the data, 318 rows containing missing values are found between `2017-03-16 06:00` to `2017-03-29 11:00`. Missing values introduce discontinuity in the time-series data and can be detrimental to our forecast. Therefore, only the data starting from `2017-03-29 12:00` will be further used.","045979df":"# Conclusion\nA deep learning approach using LSTM was implemented to forecast gangue content in flotation concentrate. Excluding *%iron in concentrate* from the features, *%silica in concentrate* were forecasted one hour ahead and with error below 1 (based on RMSE, MAE). As the dataset owner stated in [this post](https:\/\/www.kaggle.com\/rogerbellavista\/randomforestregressor-mae-0-0922-rmse-0-2314#434654), MAE and RMSE of 1\u00b10.2 is a satisfactory result. The forecasts thus show a promising method for process engineers to make timely assessment of concentrate purity and take corrective actions in advance, especially when purity deviates from the acceptable values.\n\nFinally, although LSTM implementation in this notebook has met the objectives, it will benefit from further exploration:\n- Forecasting with smaller lag timesteps. For example, a 30-minute lag for the features\/inputs.\n- Analysis of feature importance in order to understand which parameters of the flotation process greatly affect *%silica in concentrate*. This ensures that the important parameters are adjusted accordingly.","eafe7f41":"The column `conc_iron` is dropped from the dataframe because we want to look at forecasting *%silica in concentrate* without including *%iron in concentrate* as a feature. The data are then normalized as they have different units and scales.","cde3421a":" <b><p style=\"text-align:center;\">\n    <font size =\"6\" color =\"Black\">\n        Gangue Forecast in Flotation Concentrate\n    <\/font>\n<\/b>\n\nMined ores are mostly mixtures of extractable minerals and nonvaluable material (gangue). Mineral processing (a.k.a. ore dressing, ore beneficiation) follows mining and prepares the ore for extraction of the valuable metal. A principal step in mineral processing is physical separation of the particles of valuable minerals from the gangue, to produce an enriched portion (concentrate) containing most of the valuable minerals, and a discard (tailing) containing predominantly the gangue.\n\nA separation of minerals by exploiting difference of surface properties (hydrophobicity) is called flotation. The reverse cationic flotation is commonly used to separate iron from silica. By adjusting the 'chemistry' of the pulp by adding various chemical reagents, iron minerals remain in the water and create sediment with a high concentration of iron (valuable minerals). At the same time, silica particles (gangue) attach to air bubbles and float to the surface.\n\n<p style=\"text-align:center;\">\n    <img width=\"400\" alt=\"Reverse cationic flotation of iron ore\" src=\"https:\/\/github.com\/ginsaputra\/gangue-forecast-in-flotation-concentrate\/blob\/main\/reverse-cationic-flotation-iron-silica.png?raw=true\">\n\nFlotation concentrate is periodically sampled to determine its purity (i.e., *%valuable*, *%gangue*). Higher *%gangue* in the concentrate is undesirable as it indicates that most valuable minerals had gone into the tailing. Purity measurement is usually done in a lab and can take some time before process engineers can make any adjustments based on the results. A timely investigation of concentrate purity is, therefore, a fundamental aspect for the control and optimization of the flotation process.\n\nThis notebook explores the application of deep learning to forecast gangue (*%silica*) in the flotation concentrate. The forecast will help process engineers assess purity of flotation concentrate and take corrective actions in advance. More specifically, the goal is to tackle the following tasks:\n- How many steps (hours) ahead can *%silica in concentrate* be forecasted?\n- Is it possible to forecast *%silica in concentrate* without using the data of *%iron in concentrate*?","a8c97b87":"The preprocessing stage involves framing the dataset as a supervised learning problem where we forecast the *%silica in concentrate* at the current hour (*t*) given the parameters (i.e., raw materials, environment, process) in prior time steps (*t-n*). We transform the dataset using the `series_to_supervised()` function below, which was adapted from the blog [Machine Learning Mastery](https:\/\/machinelearningmastery.com\/convert-time-series-supervised-learning-problem-python\/) by Jason Brownlee.","9527d7fc":"# Forecasting with LSTM","ecd864d6":"# Flotation Data\n\nThe dataset was obtained from a mineral processing plant separating silica from iron ore using the reverse cationic flotation method. Continuous process data were collected from 1 a.m. on March 10, 2017 to 11 p.m. on September 9, 2017.\n\nEach row of data consists of 23 measurements that can be categorized into four types:\n- raw materials (column 2-3);\n- environment variables (column 4-8);\n- process variables (column 9-22);\n- processed materials (column 23-24).\n\nRaw materials and processed materials were sampled on an hourly basis while the others were sampled every 20 second.","c3b40541":"Forecasting gangue in flotation concentrate is a time-series related problem. A variation of the recurrent neural networks (RNN), called the long short-term memory (LSTM), is a deep learning approach that can be implemented to solve the problem.\n> *LSTMs have an edge over conventional feed-forward neural networks and RNN in many ways. This is because of their property of selectively remembering patterns for long durations of time.* -[Analytics Vidhya](https:\/\/www.analyticsvidhya.com\/blog\/2017\/12\/fundamentals-of-deep-learning-introduction-to-lstm\/)\n\nModel construction and training is done with [Keras](https:\/\/tensorflow.org\/api_docs\/python\/tf\/keras), a Python deep learning library. The model is made up of two [LSTM](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/LSTM) layers, each with 16 memory cells, and followed by a fully-connected ([Dense](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Dense)) layer. In training, the data are grouped into mini batches of 16 training samples. The training process is run for 100 epochs (one epoch is the number of passes needed to complete the entire training samples)."}}