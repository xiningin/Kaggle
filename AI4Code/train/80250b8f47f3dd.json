{"cell_type":{"dac4aaf1":"code","2b5d6d03":"code","be4bee7e":"code","ebe50478":"code","5c6c5753":"code","5de8cbe2":"code","69335356":"code","643e11d4":"code","6f374325":"code","2eba43bd":"code","b33ae508":"code","d7bbab4d":"code","46842a2d":"code","c2e38d91":"code","b1002099":"code","474e5968":"code","0fa13187":"code","47b16ef1":"code","f4f0d324":"code","d0787d62":"code","d9695543":"code","0296e3f6":"code","085f6ec9":"code","3b44285e":"code","c56da7d0":"code","4518ec5c":"code","88f5d812":"code","546ff9c4":"code","530c6004":"code","fdc4faa9":"code","5fe502c7":"code","d9c98949":"code","30a6e70e":"code","c5f46f2f":"code","e4ad4eb7":"code","f6cd94bd":"code","aaf7aa9b":"code","5635d623":"code","68f25914":"code","53a492b2":"code","197bb2cc":"code","afa2a3cf":"code","0edf94d4":"code","2f171b63":"code","7058d7ea":"code","3b3e2afa":"code","25c62818":"code","32620c73":"code","01fb61ce":"code","0e97a134":"code","59b5a081":"code","e497e171":"code","dea1f9ab":"code","8ec95fc5":"code","f27f9780":"code","3429949d":"code","7686840f":"code","c73f51b7":"code","089797d9":"code","85cdc144":"code","9b895b5b":"code","b02c8c52":"code","7abc1716":"code","508b316a":"code","24c3fefe":"code","6140668e":"code","9bbdae4c":"code","234060ee":"code","97e790b9":"code","6c695f6c":"code","4a079e8d":"code","ca3280a0":"code","da234e7e":"code","9466732f":"code","59b4fc9f":"code","db14beba":"code","631ef1b3":"code","84752673":"code","80e103d2":"code","ee234bd6":"code","3425ed68":"code","dba2d691":"code","b84ac24a":"code","af288a48":"code","2f1a08eb":"code","a81a324d":"code","d259672d":"code","e90f934b":"code","1149f096":"code","91505bac":"code","b700456e":"code","5f18a519":"code","c9659121":"code","d6a23390":"code","f48a5cea":"code","08bcb74a":"code","a7ffd405":"code","fb5d9228":"code","2df78f9a":"code","89a1cc74":"code","59e39c61":"code","317b0784":"code","1bdea42c":"code","6f660f63":"code","e78954fc":"code","292e084e":"code","a2a76005":"code","c89c5531":"code","d8731990":"code","2bc71ce4":"code","22386a63":"code","22b5a60d":"code","53f3f8b3":"code","190f8572":"code","8d22d709":"code","3a8f6891":"code","bbac18ec":"code","d05cb443":"code","05b56ec5":"code","6546d593":"code","7898126e":"code","67fe42f8":"code","8aa6b3b0":"code","9fcfec04":"code","b7f52e25":"code","ee75dcd2":"code","5667e646":"code","19ab0505":"code","57e09190":"code","a2a929ff":"code","2487cc86":"code","64551726":"code","5efad923":"code","0ad6d215":"code","616b07c7":"code","fbdbdfdf":"code","43c1f7a9":"code","60622095":"code","4551e742":"code","5bf1f24e":"code","b0b430af":"code","b915ee00":"code","26890f44":"code","7351998c":"code","e8990910":"code","4ba98c94":"code","67b8b059":"code","c7bee23c":"code","50b295bc":"code","54544b0a":"code","d4f766f2":"code","9c090326":"code","1b889d67":"code","014c447b":"code","e24725c6":"code","1213ea96":"code","1a4ba4ec":"code","7c33a9c2":"code","e591c69c":"code","b5652dce":"code","f24216b6":"code","0f74a906":"code","d8a0ac10":"code","21134422":"code","cbe7b071":"code","f9057f10":"code","48aa5391":"code","1a5644d1":"code","43389854":"code","2e74790e":"code","30507ea2":"code","03b9f7ec":"code","6ce0d503":"code","3967176d":"code","92dd6a10":"code","6d3a863e":"code","572bb03b":"code","052f2c21":"code","993a9ce9":"code","1397f3d0":"code","02d3d26a":"code","47f31f93":"code","d65f6236":"code","1f6f7819":"code","6e862c14":"code","d8b3985c":"code","f41daf1c":"code","b935dd42":"code","6a0d39ad":"code","240e232c":"code","2f87e002":"code","313e7b8e":"code","7f8678f5":"code","6d720bc1":"code","d67196c2":"code","1cc44b1b":"code","9282cb46":"markdown","15c1cf48":"markdown","bfeeb3e6":"markdown","f9ed0db7":"markdown","7a562059":"markdown","460a50b7":"markdown","7cb3fdb8":"markdown","e5043485":"markdown","1089b558":"markdown","637decca":"markdown","2898fbc1":"markdown","ef0d9893":"markdown","d78f47d9":"markdown","f7f37a51":"markdown","a55562ef":"markdown","f3396417":"markdown","b25b62e9":"markdown","a1ad29b4":"markdown","50564be9":"markdown","c748c489":"markdown","2b8a5cc5":"markdown","dbfbabbe":"markdown","8f5ccc09":"markdown","2f89a397":"markdown","ac5ada17":"markdown","73fa0cf6":"markdown","64efa02d":"markdown","383a9905":"markdown","1b88fe26":"markdown","ee8166dc":"markdown","37dc3d66":"markdown","4cc6ad4a":"markdown","0158b2fc":"markdown","80aa09dd":"markdown","df5af170":"markdown","f1f02a42":"markdown","382303c6":"markdown","4eb14a65":"markdown","a6c0feac":"markdown","f03d01c5":"markdown","459eb637":"markdown","bc4995d8":"markdown","8b0dde1d":"markdown","c8a9d609":"markdown","4ba1a170":"markdown","68e377d0":"markdown","92896d86":"markdown","c46df18f":"markdown","e2f664f3":"markdown","44dcfb68":"markdown","ef80c41e":"markdown","78492827":"markdown","d1fa02e3":"markdown","221fd265":"markdown","2bc2fc27":"markdown","2821ec5a":"markdown","c6c55973":"markdown","73893170":"markdown","9bc0ee4a":"markdown","8a8f8d80":"markdown","2ac09c48":"markdown","3d37c39c":"markdown","a4b2b13c":"markdown","a127b863":"markdown","75f2adf4":"markdown","377b61ae":"markdown","286eb5ff":"markdown","b496bfb0":"markdown","70892e25":"markdown","a07ee9aa":"markdown","fa83101b":"markdown","ed01e775":"markdown","85cc7583":"markdown","d82b797a":"markdown","878b57b2":"markdown","09873208":"markdown","b90c97c8":"markdown","51ed1adc":"markdown","6cbb8c50":"markdown","f42ddc25":"markdown","7131cfec":"markdown"},"source":{"dac4aaf1":"from IPython.display import Markdown, display","2b5d6d03":"def display_(text):\n    result = \"<p style='border:2px; border-style:solid; border-color:#8D90A6; padding: 1em; background-color:#EBEDF6;'>\" + text + \"<\/p>\" \n    display(Markdown(result))","be4bee7e":"from IPython.display import Image\nImage(\"..\/input\/who-are-you\/whoareyou.png\")","ebe50478":"def quantile_plot1(x, **kwargs):\n#     title = kwargs.get('title')\n    with sns.plotting_context(\"notebook\",font_scale=15):\n        sns.set(style=\"ticks\", color_codes=True)\n        \n        ax = sns.barplot(x=x, data=c_counts,  y='percentage', hue=\"Q5\", dodge=False)\n#         ax.set_title(title)\n        for p in ax.patches:\n            ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()),\n                        ha='center', va='bottom',\n                        color= 'black')","5c6c5753":"def counts(x):\n    c_counts = (df_modelagem_.groupby(['Q5'])[x]\n                     .value_counts(normalize=True)\n                     .rename('percentage')\n                     .mul(100)\n                     .reset_index()\n                     .sort_values(x))\n    return c_counts\n    ","5de8cbe2":"def missing_values_table(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        return mis_val_table_ren_columns","69335356":"def count_values_table(df):\n        count_val = df.value_counts()\n        count_val_percent = 100 * df.value_counts() \/ len(df)\n        count_val_table = pd.concat([count_val, count_val_percent.round(1)], axis=1)\n        count_val_table_ren_columns = count_val_table.rename(\n        columns = {0 : 'Count Values', 1 : '% of Total Values'})\n        return count_val_table_ren_columns","643e11d4":"def counts_plot(df, col, title):\n    sns.countplot(x=df[col])\n    # Add labels to the plot\n    style = dict(size=10, color='gray')\n","6f374325":"# Histogram\ndef hist_chart(df, col, title):\n    plt.style.use('seaborn')\n#     plt.hist(df[col].dropna(), edgecolor = 'k');\n    plt.hist(df[col].dropna())\n    plt.xlabel(col)\n    plt.ylabel('Number of Entries')\n    plt.xticks(rotation='vertical')\n    plt.rc('xtick', labelsize=8)\n    plt.title(title)\n    plt.show()\n","2eba43bd":"def hist_chart(df, col, title, patche_c):\n#     plt.style.use('seaborn')\n    fig, ax = plt.subplots(1, figsize=(16,6))\n    n, bins, patches = plt.hist(df[col].dropna(),color = \"skyblue\")\n    # define minor ticks and draw a grid with them\n    minor_locator = AutoMinorLocator(10)\n    plt.gca().xaxis.set_minor_locator(minor_locator)\n#     plt.grid(which='minor', color='white', lw = 0.5)\n    # x ticks\n    xticks = [(bins[idx+1] + value)\/2 for idx, value in enumerate(bins[:-1])]\n    xticks_labels = [ \"{:.2f}\\nto\\n{:.2f}\".format(value, bins[idx+1]) for idx, value in enumerate(bins[:-1])]\n#     plt.xticks(xticks, labels = xticks_labels)\n#     ax.tick_params(axis='x', which='both',length=0)\n    # remove y ticks\n    plt.yticks([])\n    plt.xticks(rotation='vertical')\n\n    patches[patche_c].set_fc('steelblue')\n    # Hide the right and top spines\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n    # plot values on top of bars\n    for idx, value in enumerate(n):\n        if value > 0:\n            plt.text(xticks[idx], value+5, int(value), ha='center')\n    plt.title(title, loc = 'left', fontsize = 18)\n    plt.show()","b33ae508":"# Import Libraries Required\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import AutoMinorLocator\nfrom matplotlib import gridspec\n%matplotlib inline\nimport numpy as np\nimport seaborn as sns","d7bbab4d":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","46842a2d":"#Source Query location: \npath =  '..\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv'\n# reads the data from the file - denotes as CSV, it has no header, sets column headers\ndf =  pd.read_csv(path, sep=',')","c2e38d91":"print(df.columns)","b1002099":"print(df.shape)","474e5968":"#df.dtypes","0fa13187":"df.describe()\n","47b16ef1":"#df.info()\n","f4f0d324":"df.head()","d0787d62":"# used to see the questions\npd.DataFrame(df.loc[0].values).head()","d9695543":"\nprint('Preview of missing columns')\npd.DataFrame(df.isnull().sum()).head()\n","0296e3f6":"missing_values_table(df).tail(25)","085f6ec9":"# We will import matplotlib to resize our plot figure\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(20, 10))\n\n# cubehelix palette is a part of seaborn that produces a colormap\ncmap = sns.cubehelix_palette(light=1, as_cmap=True, reverse=True)\nsns.heatmap(df.isnull(), cmap=cmap)","3b44285e":"# Get the columns with > 50% missing\nmissing_df = missing_values_table(df);\nmissing_columns = list(missing_df[missing_df['% of Total Values'] > 50].index)\nprint('We will remove %d columns.' % len(missing_columns))","c56da7d0":"# Drop the columns\ndf_ = df.drop(list(missing_columns), axis=1)","4518ec5c":"df_.head(2)","88f5d812":"col = 'Q1'\nfig, ax = plt.subplots(figsize=(12, 4))\n\nsns.countplot(x=df.iloc[1:,].Q1, ax=ax, order = df.iloc[1:,].Q1.value_counts().index)\n# Add labels to the plot\nplt.annotate('20% has betwen 25-29 years and 0.4% has 70+', xy=(5, 3500))\nplt.show()\n\ncount_values_table(df.Q1)","546ff9c4":"col = 'Q2'\nfig, ax = plt.subplots(figsize=(12, 4))\n\nsns.countplot(x=df.iloc[1:,].Q2, ax=ax, order = df.iloc[1:,].Q2.value_counts().index)\n# Add labels to the plot\nplt.annotate('78% are Man, 19.4% are Woman and less than 2% are others diversityes', xy=(1, 14000))\nplt.show()\ncount_values_table(df.Q2)\n","530c6004":"col = 'Q3'\nfig, ax = plt.subplots(figsize=(22, 8))\n\nsns.countplot(x=df.iloc[1:,].Q3.sort_values(), ax=ax, order = df.iloc[1:,].Q3.value_counts().index)\n# Add labels to the plot\nplt.annotate('India and USA  have almost 50% of professionals. Brasil is the 4th of the list with 3.5%.', xy=(18, 4000))\nplt.xticks(rotation=45)\nplt.show()\ncount_values_table(df.Q3)\n\n","fdc4faa9":"col = 'Q4'\nfig, ax = plt.subplots(figsize=(12, 4))\n\nsns.countplot(x=df.iloc[1:,].Q4.sort_values(), ax=ax, order = df.iloc[1:,].Q4.value_counts().index)\n# Add labels to the plot\nplt.annotate('74% of professionals are Masters or bachelors and 1% has no formal education', xy=(2, 4000))\nplt.xticks(rotation=75)\nplt.show()\ncount_values_table(df.Q4)\n","5fe502c7":"col = 'Q5'\nfig, ax = plt.subplots(figsize=(12, 4))\n\nsns.countplot(x=df.iloc[1:,].Q5.sort_values(), ax=ax, order = df.iloc[1:,].Q5.value_counts().index)\n# Add labels to the plot\nplt.annotate('25% of respondents are Students, 13% are DS (our public of interest). DBA represents just 0.6' , xy=(2, 4000))\nplt.xticks(rotation=75)\nplt.show()\n\ncount_values_table(df.Q5)","d9c98949":"df_modelagem = df_.iloc[1:, 1:]","30a6e70e":"# Analise by title:\nt1 = df_modelagem[df_modelagem['Q5'] == 'Student']\nt2 = df_modelagem[df_modelagem['Q5'] == 'Data Engineer']\nt3 = df_modelagem[df_modelagem['Q5'] == 'Software Engineer']\nt4 = df_modelagem[df_modelagem['Q5'] == 'Data Scientist']\nt5 = df_modelagem[df_modelagem['Q5'] == 'Data Analyst']\nt6 = df_modelagem[df_modelagem['Q5'] == 'Research Scientist']\nt7 = df_modelagem[df_modelagem['Q5'] == 'Other']\nt8 = df_modelagem[df_modelagem['Q5'] == 'Currently not employed']\nt9 = df_modelagem[df_modelagem['Q5'] == 'Statistician']\nt10 = df_modelagem[df_modelagem['Q5'] == 'Product\/Project Manager']\nt11 = df_modelagem[df_modelagem['Q5'] == 'Machine Learning Engineer']\nt12= df_modelagem[df_modelagem['Q5'] == 'Unknown']\nt13 = df_modelagem[df_modelagem['Q5'] == 'Business Analyst']\nt13 = df_modelagem[df_modelagem['Q5'] == 'DBA\/Database Engineer']","c5f46f2f":"df_modelagem_ =  df_modelagem.copy()","e4ad4eb7":"print(df_.iloc[0,1])","f6cd94bd":"\nc_counts = counts(\"Q1\")\n\ng = sns.FacetGrid(c_counts, sharex=False,col=\"Q5\", col_wrap=4)\ng.map(quantile_plot1, \"Q1\", title=df_.iloc[0,1])\ng.fig.tight_layout(pad=0.4, w_pad=0.5, h_pad=4.0)\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(df_.iloc[0,1]) \nfor ax in g.axes.flat:\n    for label in ax.get_xticklabels():\n        label.set_rotation(45)\n        ","aaf7aa9b":"c_counts = counts(\"Q2\")\n\ng = sns.FacetGrid(c_counts, sharex=False,col=\"Q5\", col_wrap=4)\ng.map(quantile_plot1, \"Q2\")\ng.fig.tight_layout(pad=0.4, w_pad=0.5, h_pad=10.0)\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(df_.iloc[0,2]) \nfor ax in g.axes.flat:\n    for label in ax.get_xticklabels():\n        label.set_rotation(45)","5635d623":"\nc_counts = counts(\"Q3\")\nc_counts = c_counts.sort_values(by=[ 'percentage', 'Q5',], ascending=False).head(100)\ng = sns.FacetGrid(c_counts, col=\"Q5\", col_wrap=4,  height=8, aspect=0.5)\ng.map(quantile_plot1, \"Q3\")\ng.fig.tight_layout(pad=0.4, w_pad=0.5, h_pad=4.0)\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(df_.iloc[0,3]) \nfor ax in g.axes.flat:\n    ax.set_xticklabels(ax.get_xticklabels(),size = 12)\n    for label in ax.get_xticklabels():\n        label.set_rotation(90)\n\n\n        ","68f25914":"c_counts","53a492b2":"c_counts[c_counts.Q3 != 'India'].sort_values(by='percentage', ascending=False).head(50)\n","197bb2cc":"c_counts[c_counts.Q3 == 'Brazil'].sort_values(by='percentage', ascending=False).head(50)\n# c_counts.sort_values(by='percentage', ascending=False)","afa2a3cf":"c_counts = counts(\"Q4\")\n\ng = sns.FacetGrid(c_counts,col=\"Q5\", col_wrap=4, height=8, aspect=0.5)\ng.map(quantile_plot1, \"Q4\")\ng.fig.tight_layout()\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(df_.iloc[0,4]) \nfor ax in g.axes.flat:\n    ax.set_xticklabels(ax.get_xticklabels(),size = 12)\n    for label in ax.get_xticklabels():\n        label.set_rotation(90)","0edf94d4":"c_counts = counts(\"Q6\")\nc_counts = c_counts.drop(55, axis=0)\n\ng = sns.FacetGrid(c_counts,sharex=False, col=\"Q5\", col_wrap=4,  height=8, aspect=0.5)\ng.map(quantile_plot1, \"Q6\")\ng.fig.tight_layout(pad=0.4, w_pad=0.5, h_pad=4.0)\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(df_.iloc[0,6]) \nfor ax in g.axes.flat:\n    labels = ax.get_xticklabels() # get x labels\n    ax.set_xticklabels(labels, rotation=30) # set new labels\n    \n\n","2f171b63":"print(df_.iloc[0,7])\nc_counts = counts(\"Q7_Part_1\")\n\ng = sns.FacetGrid(c_counts,sharex=False, col=\"Q5\", col_wrap=4,  height=8, aspect=0.5)\ng.map(quantile_plot1, \"Q7_Part_1\")\ng.fig.tight_layout(pad=0.4, w_pad=0.5, h_pad=4.0)\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(df_.iloc[0,7]) \nfor ax in g.axes.flat:\n    for label in ax.get_xticklabels():\n        label.set_rotation(45)","7058d7ea":"\nc_counts = counts(\"Q8\")\ni = c_counts[(c_counts.Q5 == 'Null') & (c_counts.Q8 == 'Null')].index.tolist()\nc_counts = c_counts.drop(i) \n\ng = sns.FacetGrid(c_counts, sharex=False,col=\"Q5\", col_wrap=4,  height=8, aspect=0.5)\ng.map(quantile_plot1, \"Q8\")\ng.fig.tight_layout(pad=0.4, w_pad=0.5, h_pad=4.0)\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(df_.iloc[0,8]) \nfor ax in g.axes.flat:\n    for label in ax.get_xticklabels():\n        label.set_rotation(45)","3b3e2afa":"\nc_counts = counts(\"Q9_Part_1\")\n# i = c_counts[(c_counts.Q5 == 'Null') & (c_counts.Q9_Part_1 == 'Null')].index.tolist()\n# c_counts = c_counts.drop(i) \n\ng = sns.FacetGrid(c_counts,sharex=False, col=\"Q5\", col_wrap=4,  height=8, aspect=0.5)\ng.map(quantile_plot1, \"Q9_Part_1\")\ng.fig.tight_layout(pad=0.4, w_pad=0.5, h_pad=4.0)\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(df_.iloc[0,9]) \nfor ax in g.axes.flat:\n    for label in ax.get_xticklabels():\n        label.set_rotation(45)","25c62818":"c_counts = counts(\"Q11\")\ni = c_counts[(c_counts.Q5 == 'Null') & (c_counts.Q11 == 'Null')].index.tolist()\nc_counts = c_counts.drop(i) \n\ng = sns.FacetGrid(c_counts, col=\"Q5\", col_wrap=4,  height=8, aspect=0.5)\ng.map(quantile_plot1, \"Q11\")\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(df_.iloc[0,10]) \nfor ax in g.axes.flat:\n    for label in ax.get_xticklabels():\n        label.set_rotation(90)","32620c73":"c_counts.sort_values(by=['Q5', 'percentage'], ascending=False)","01fb61ce":"c_counts = counts(\"Q13\")\ni = c_counts[(c_counts.Q5 == 'Null') & (c_counts.Q13 == 'Null')].index.tolist()\nc_counts = c_counts.drop(i) \n\ng = sns.FacetGrid(c_counts, sharex=False,col=\"Q5\", col_wrap=4,  height=8, aspect=0.5)\ng.map(quantile_plot1, \"Q13\")\ng.fig.tight_layout(pad=0.5, w_pad=0.5, h_pad=8.0)\n\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(df_.iloc[0,11]) \nfor ax in g.axes.flat:\n    for label in ax.get_xticklabels():\n        label.set_rotation(45)","0e97a134":"c_counts = counts(\"Q14_Part_1\")\ni = c_counts[(c_counts.Q5 == 'Null') & (c_counts.Q14_Part_1 == 'Null')].index.tolist()\nc_counts = c_counts.drop(i) \n\ng = sns.FacetGrid(c_counts, sharex=False,col=\"Q5\", col_wrap=4,  height=8, aspect=0.5)\ng.map(quantile_plot1, \"Q14_Part_1\")\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(df_.iloc[0,12]) \nfor ax in g.axes.flat:\n    for label in ax.get_xticklabels():\n        label.set_rotation(45)","59b5a081":"c_counts = counts(\"Q15\")\ni = c_counts[(c_counts.Q5 == 'Null') & (c_counts.Q15 == 'Null')].index.tolist()\nc_counts = c_counts.drop(i) \n\ng = sns.FacetGrid(c_counts,sharex=False, col=\"Q5\", col_wrap=4,  height=8, aspect=0.5)\ng.map(quantile_plot1, \"Q15\")\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(df_.iloc[0,13]) \nfor ax in g.axes.flat:\n    for label in ax.get_xticklabels():\n        label.set_rotation(45)","e497e171":"c_counts","dea1f9ab":"c_counts = counts(\"Q16_Part_1\")\ni = c_counts[(c_counts.Q5 == 'Null') & (c_counts.Q16_Part_1 == 'Null')].index.tolist()\nc_counts = c_counts.drop(i) \n\ng = sns.FacetGrid(c_counts, sharex=False,col=\"Q5\", col_wrap=4,  height=8, aspect=0.5)\ng.map(quantile_plot1, \"Q16_Part_1\")\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(df_.iloc[0,14]) \nfor ax in g.axes.flat:\n    for label in ax.get_xticklabels():\n        label.set_rotation(45)","8ec95fc5":"c_counts = counts(\"Q17_Part_1\")\ni = c_counts[(c_counts.Q5 == 'Null') & (c_counts.Q17_Part_1 == 'Null')].index.tolist()\nc_counts = c_counts.drop(i)\n\ng = sns.FacetGrid(c_counts, sharex=False,col=\"Q5\", col_wrap=4,  height=8, aspect=0.5)\ng.map(quantile_plot1, \"Q17_Part_1\")\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(df_.iloc[0,15]) \nfor ax in g.axes.flat:\n    for label in ax.get_xticklabels():\n        label.set_rotation(45)","f27f9780":"c_counts = counts(\"Q20\")\ni = c_counts[(c_counts.Q5 == 'Null') & (c_counts.Q20 == 'Null')].index.tolist()\nc_counts = c_counts.drop(i)\n\ng = sns.FacetGrid(c_counts,sharex=False, col=\"Q5\", col_wrap=4,  height=8, aspect=0.5)\ng.map(quantile_plot1, \"Q20\")\ng.fig.tight_layout(pad=0.5, w_pad=0.5, h_pad=8.0)\n\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(df_.iloc[0,16]) \nfor ax in g.axes.flat:\n    for label in ax.get_xticklabels():\n        label.set_rotation(45)","3429949d":"\nc_counts = counts(\"Q21\")\ni = c_counts[(c_counts.Q5 == 'Null') & (c_counts.Q21 == 'Null')].index.tolist()\nc_counts = c_counts.drop(i)\n\n\n\ng = sns.FacetGrid(c_counts, sharex=False,col=\"Q5\", col_wrap=4,  height=8, aspect=0.5)\ng.map(quantile_plot1, \"Q21\")\ng.fig.tight_layout(pad=0.4, w_pad=0.5, h_pad=4.0)\n\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(df_.iloc[0,17]) \nfor ax in g.axes.flat:\n    for label in ax.get_xticklabels():\n        label.set_rotation(45)","7686840f":"c_counts = counts(\"Q22\")\ni = c_counts[(c_counts.Q5 == 'Null') & (c_counts.Q22 == 'Null')].index.tolist()\nc_counts = c_counts.drop(i)\nc_counts = c_counts.replace({'No (we do not use ML methods)': 'No','We are exploring ML methods (and may one day put a model into production)': 'Exploring', 'We use ML methods for generating insights (but do not put working models into production)': 'Insights', 'We recently started using ML methods (i.e., models in production for less than 2 years)': 'recently stated', 'We have well established ML methods (i.e., models in production for more than 2 years)': 'well established'  })\n\n\ng = sns.FacetGrid(c_counts,sharex=False, col=\"Q5\", col_wrap=4,  height=8, aspect=0.5)\ng.map(quantile_plot1, \"Q22\")\ng.fig.tight_layout(pad=0.4, w_pad=0.5, h_pad=4.0)\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(df_.iloc[0,18]) \nfor ax in g.axes.flat:\n    for label in ax.get_xticklabels():\n        label.set_rotation(45)","c73f51b7":"c_counts.sort_values(by=['Q22','percentage'], ascending=False)","089797d9":"c_counts = counts(\"Q24\")\ni = c_counts[(c_counts.Q5 == 'Null') & (c_counts.Q24 == 'Null')].index\nc_counts = c_counts.drop(i, axis=0)\n\nsns.set(font_scale=0.5)  # crazy big\n\ng = sns.FacetGrid(c_counts, sharex=False,col=\"Q5\", col_wrap=4,  height=8, aspect=0.5)\ng.map(quantile_plot1, \"Q24\")\ng.fig.tight_layout(pad=0.4, w_pad=0.5, h_pad=4.0)\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(df_.iloc[0,19]) \nfor ax in g.axes.flat:\n    for label in ax.get_xticklabels():\n        label.set_rotation(45)\n","85cdc144":"c_counts.head()","9b895b5b":"c_counts = counts(\"Q25\")\ni = c_counts[(c_counts.Q5 == 'Null') & (c_counts.Q25 == 'Null')].index\nc_counts = c_counts.drop(i, axis=0)\n\ng = sns.FacetGrid(c_counts,sharex=False, col=\"Q5\", col_wrap=4,  height=8, aspect=0.5)\ng.map(quantile_plot1, \"Q25\")\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(df_.iloc[0,20]) \nfor ax in g.axes.flat:\n    for label in ax.get_xticklabels():\n        label.set_rotation(45)","b02c8c52":"c_counts = counts(\"Q38\")\ni = c_counts[(c_counts.Q5 == 'Null') & (c_counts.Q38 == 'Null')].index\nc_counts = c_counts.drop(i)\n\ng = sns.FacetGrid(c_counts,sharex=False, col=\"Q5\", col_wrap=4,  height=8, aspect=0.5)\ng.map(quantile_plot1, \"Q38\")\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(df_.iloc[0,21]) \nfor ax in g.axes.flat:\n    for label in ax.get_xticklabels():\n        label.set_rotation(45)","7abc1716":"c_counts.head(2)","508b316a":"df_cds = df_modelagem[df_modelagem.Q5 == 'Data Scientist']","24c3fefe":"np.unique(df_cds.Q4)","6140668e":"df_cds.Q4 = df_cds.Q4.replace({\"Master\u2019s degree\": 'Master', 'Doctoral degree': 'Doctor', 'Bachelor\u2019s degree': 'Bachelor', 'I prefer not to answer':'Not answer', 'No formal education past high school': 'NFE', 'Some college\/university study without earning a bachelor\u2019s degree': 'college\/university without bachelor'  })","9bbdae4c":"# !pip install country_converter --upgrade\n# ! pip install pycountry --upgrade","234060ee":"import pycountry\ninput_countries = df_cds.Q3\n\ncountries = {}\nfor country in pycountry.countries:\n    countries[country.name] = country.alpha_2\n\ncodes = [countries.get(country, country) for country in input_countries]\n\n# codes =  pd.Series(codes)\n\n# codes.replace({'United States of America': 'USA','Iran, Islamic Republic of...': 'Iran', 'United Kingdom of Great Britain and Northern Ireland': 'Great Britain'})\n\ndf_cds['Q3_renamed'] =  codes\n\n\n# df_cds.Q3_renamed = df_cds.Q3_renamed.replace({'United States of America': 'USA','Iran, Islamic Republic of...': 'Iran', 'United Kingdom of Great Britain and Northern Ireland': 'Great Britain'})","97e790b9":"df_cds.Q22 =  df_cds.Q22.replace({'No (we do not use ML methods)': 'No','We are exploring ML methods (and may one day put a model into production)': 'Exploring', 'We use ML methods for generating insights (but do not put working models into production)': 'Insights', 'We recently started using ML methods (i.e., models in production for less than 2 years)': 'recently stated', 'We have well established ML methods (i.e., models in production for more than 2 years)': 'well established'  })","6c695f6c":"# remove years\n# df_cds.replace(['years'],'')\n# employees","4a079e8d":"df_cds","ca3280a0":"df_cds.Q11.replace({})","da234e7e":"df_cds.shape","9466732f":"df_cds.head()","59b4fc9f":"df_cds = df_cds.fillna('Unknow')","db14beba":"df_cds_wna = df_cds.dropna()","631ef1b3":"# df_modelagem = df_.iloc[1:, 1:]","84752673":"import pandas as pd\nimport numpy as np\nfrom sklearn.neighbors import DistanceMetric","80e103d2":"! pip install gower\n","ee234bd6":"from scipy.cluster.hierarchy import linkage, fcluster, dendrogram, leaves_list","3425ed68":"! pip install prince","dba2d691":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gower\nimport prince # for multiple correspondence analysis\nfrom sklearn.feature_selection import SelectKBest, chi2 # for chi-squared feature selection\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder","b84ac24a":"dm = gower.gower_matrix(df_cds)\ndm_wna = gower.gower_matrix(df_cds_wna)","af288a48":"# dendrogram(Zd) \nZ = linkage(dm, 'ward')\nfig = plt.figure(figsize=(25, 10))\ndn = dendrogram(Z, color_threshold=100)","2f1a08eb":"# dendrogram(Zd) \nZ_wna = linkage(dm_wna, 'ward')\nfig = plt.figure(figsize=(25, 10))\ndn = dendrogram(Z_wna, color_threshold=100)","a81a324d":"cld = fcluster(Z, 3, criterion='maxclust')\ncld","d259672d":"# df_cds_wna = df_cds_wna.drop(['H_CLUSTER', 'D_Clusters', 'K_Clusters'],axis=1)","e90f934b":"mca = prince.MCA(\n        n_components=4,\n        n_iter=3,\n        copy=True,\n        check_input=True,\n        engine='auto',\n        random_state=42\n        )\nmca = mca.fit(df_cds)\n\nax = mca.plot_coordinates(\n        X=df_cds,\n        ax=None,\n        figsize=(16, 30),\n        show_row_points=False,\n        row_points_size=0,\n        show_row_labels=False,\n        show_column_points=True,\n        column_points_size=30,\n        show_column_labels=True,\n        legend_n_cols=1\n               ).legend(loc='center left', bbox_to_anchor=(1, 0.5))\n\nplt.savefig('mca_plot.png')","1149f096":"### test without NA cases:\n\nmca_wna = prince.MCA(\n        n_components=4,\n        n_iter=3,\n        copy=True,\n        check_input=True,\n        engine='auto',\n        random_state=42\n        )\nmca_wna = mca_wna.fit(df_cds_wna)\n\nax = mca_wna.plot_coordinates(\n        X=df_cds_wna,\n        ax=None,\n        figsize=(16, 30),\n        show_row_points=False,\n        row_points_size=0,\n        show_row_labels=False,\n        show_column_points=True,\n        column_points_size=30,\n        show_column_labels=True,\n        legend_n_cols=1\n               ).legend(loc='center left', bbox_to_anchor=(1, 0.5))\n\nplt.savefig('mca_plot_wna.png')","91505bac":"df_cds_mca = mca.transform(df_cds)\ndf_cds_mca.head()","b700456e":"df_cds_mca_wna = mca_wna.transform(df_cds_wna)\ndf_cds_mca_wna.head()","5f18a519":"from sklearn.cluster import KMeans\nimport numpy as np\nX  = df_cds_mca\nX_wna = df_cds_mca_wna","c9659121":"kmeans_kwargs = {\n    \"init\": \"random\",\n    \"n_init\": 10,\n    \"max_iter\": 300,\n    \"random_state\": 42,\n}\n\n# A list holds the SSE values for each k\nsse = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n    kmeans.fit(X)\n    sse.append(kmeans.inertia_)","d6a23390":"plt.style.use(\"fivethirtyeight\")\nplt.plot(range(1, 11), sse)\nplt.xticks(range(1, 11))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"SSE\")\nplt.show()\nplt.savefig('n_clusters_kmeans.png')","f48a5cea":"kmeans_kwargs = {\n    \"init\": \"random\",\n    \"n_init\": 10,\n    \"max_iter\": 300,\n    \"random_state\": 42,\n}\n\n# A list holds the SSE values for each k\nsse_wna = []\nfor k in range(1, 11):\n    kmeans_wna = KMeans(n_clusters=k, **kmeans_kwargs)\n    kmeans_wna.fit(X_wna)\n    sse_wna.append(kmeans_wna.inertia_)","08bcb74a":"plt.style.use(\"fivethirtyeight\")\nplt.plot(range(1, 11), sse_wna)\nplt.xticks(range(1, 11))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"SSE\")\nplt.show()\nplt.savefig('n_clusters_kmeans.png')\n","a7ffd405":"!pip install pip install kneed","fb5d9228":"from kneed import KneeLocator\n\nkl = KneeLocator(\n    range(1, 11), sse_wna, curve=\"convex\", direction=\"decreasing\"\n)\n\nkl.elbow","2df78f9a":"kl.plot_knee()\n","89a1cc74":"from kneed import KneeLocator\n\nkl = KneeLocator(\n    range(1, 11), sse, curve=\"convex\", direction=\"decreasing\"\n)\n\nprint(kl.elbow)\nkl.plot_knee()","59e39c61":"\nkmeans = KMeans(n_clusters=4, random_state=0).fit(X)\nkmeans.labels_\n\ny_kmeans = kmeans.predict(X)\n\n# kmeans.cluster_centers_","317b0784":"plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y_kmeans, s=50, cmap='viridis')\n\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);","1bdea42c":"\nkmeans_wna = KMeans(n_clusters=4, random_state=0).fit(X_wna)\nkmeans_wna.labels_\n\ny_kmeans_wna = kmeans_wna.predict(X_wna)\n\nkmeans_wna.cluster_centers_","6f660f63":"plt.scatter(X_wna.iloc[:, 0], X_wna.iloc[:, 1], c=y_kmeans_wna, s=50, cmap='viridis')\n\ncenters_wna = kmeans_wna.cluster_centers_\nplt.scatter(centers_wna[:, 0], centers_wna[:, 1], c='black', s=200, alpha=0.5);","e78954fc":"from sklearn.cluster import DBSCAN\nfrom sklearn.metrics import adjusted_rand_score, silhouette_score\n","292e084e":"# Instantiate k-means and dbscan algorithms\nkmeans = KMeans(n_clusters=4, random_state=0)\ndbscan = DBSCAN(eps=0.3)\n\n# Fit the algorithms to the features\nkmeans.fit(X)\ndbscan.fit(X)\n\n# Compute the silhouette scores for each algorithm\nkmeans_silhouette = silhouette_score(\n    X, kmeans.labels_\n).round(2)\ndbscan_silhouette = silhouette_score(\n   X, dbscan.labels_\n).round (2)","a2a76005":"print('kmeans_silhouette', kmeans_silhouette)\nprint('dbscan_silhouette', dbscan_silhouette)","c89c5531":"from sklearn import metrics\n\n# Compute DBSCAN\ndb = DBSCAN(eps=0.3, min_samples=10).fit(X)\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint('Estimated number of noise points: %d' % n_noise_)\n# print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n# print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n# print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n# print(\"Adjusted Rand Index: %0.3f\"\n#       % metrics.adjusted_rand_score(labels_true, labels))\n# print(\"Adjusted Mutual Information: %0.3f\"\n#       % metrics.adjusted_mutual_info_score(labels_true, labels))\n# print(\"Silhouette Coefficient: %0.3f\"\n#       % metrics.silhouette_score(X, labels))\n\n# #############################################################################\n# Plot result\nimport matplotlib.pyplot as plt\n\n# Black removed and is used for noise instead.\nunique_labels = set(labels)\ncolors = [plt.cm.Spectral(each)\n          for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = (labels == k)\n\n    xy = X.iloc[class_member_mask & core_samples_mask]\n    plt.plot(xy.iloc[:, 0], xy.iloc[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=14)\n\n    xy = X.iloc[class_member_mask & ~core_samples_mask]\n    plt.plot(xy.iloc[:, 0], xy.iloc[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=6)\n\nplt.title('Estimated number of clusters: %d' % n_clusters_)\nplt.show()","d8731990":"np.unique(db.labels_)","2bc71ce4":"df_cds['D_Clusters'] = db.labels_","22386a63":"d1 = df_cds[df_cds.D_Clusters == 0]\nd2 = df_cds[df_cds.D_Clusters == 1]\nd3 = df_cds[df_cds.D_Clusters == 2]\nd4 = df_cds[df_cds.D_Clusters == 3]\nd5 = df_cds[df_cds.D_Clusters == 4]\nd6 = df_cds[df_cds.D_Clusters == 5]","22b5a60d":"df_cds['K_Clusters'] = kmeans.labels_","53f3f8b3":"np.unique(kmeans.labels_)","190f8572":"df_cds.head()","8d22d709":"k1 = df_cds[df_cds.K_Clusters == 0]\nk2 = df_cds[df_cds.K_Clusters == 1]\nk3 = df_cds[df_cds.K_Clusters == 2]\nk4 = df_cds[df_cds.K_Clusters == 3]\n\n","3a8f6891":"from matplotlib.offsetbox import AnchoredText\n\ndef plot_KDE(df, col, nClusters, cluster, title=None):\n    df = df.copy()\n    new_colum = col+\"_f\"\n    df[new_colum] = pd.factorize(df[col] )[0]\n    \n    dict_ = {}\n    for key,value in zip(df[new_colum].drop_duplicates(),df.loc[df[new_colum].drop_duplicates().index][col] ): \n        dict_[key] = value\n    \n    plt.subplots(figsize=(10,7), dpi=100)\n    colors = ['orange', 'deeppink', 'green', 'blue','firebrick', 'skyblue']\n    \n    if cluster == 'H_CLUSTER':\n        r = range(1, nClusters+1)\n    else:\n        r= range(0, nClusters+1)\n    \n    for i in r:\n        try:\n            ax = sns.distplot( df.loc[df[cluster]==i, new_colum] , color=colors[i], label=i)\n        except RuntimeError as re:\n            if str(re).startswith(\"Selected KDE bandwidth is 0. Cannot estimate density.\"):\n                ax =  sns.distplot( df.loc[df[cluster]==i, new_colum] , color=colors[i], label=i, kde_kws={'bw': 0.1})\n            else:\n                raise re\n                \n    plt.legend(fontsize='small')\n\n    plt.title(title)\n#     size = dict(size=plt.rcParams['legend.fontsize'])\n    size={'family': 'Ubuntu Condensed', 'size': 12, 'fontweight': 'light'}\n    anchored_text = AnchoredText(dict_, loc='upper left',prop=size, pad=0., borderpad=0.5)\n    if len(dict_) < 15:\n        ax.add_artist(anchored_text)\n    \n#     return df\n","bbac18ec":"df_cds['H_CLUSTER']= cld","d05cb443":"h1 = df_cds[df_cds.H_CLUSTER == 1]\nh2 = df_cds[df_cds.H_CLUSTER == 2]\nh3 = df_cds[df_cds.H_CLUSTER == 3]","05b56ec5":"pct3 = df_cds.replace({'18-21':18, '22-24':22, '25-29':25, '30-34':30,'35-39':35, '40-44':40, '45-49':45,'50-54':50, '55-59':55, '60-69':60, '70+':70})\na = plot_KDE(pct3, 'Q1', 3,'H_CLUSTER')","6546d593":"plot_KDE(pct3, 'Q2', 3,'H_CLUSTER')","7898126e":"a = plot_KDE(pct3, 'Q3', 3,'H_CLUSTER')","67fe42f8":"n=3\nprint(h1['Q3'].value_counts()[:n].index.tolist())\nprint(h2['Q3'].value_counts()[:n].index.tolist())\nprint(h3['Q3'].value_counts()[:n].index.tolist())\n","8aa6b3b0":"plot_KDE(df_cds, 'Q4', 3, 'H_CLUSTER')","9fcfec04":"plot_KDE(df_cds, 'Q6', 3, 'H_CLUSTER')","b7f52e25":"plot_KDE(df_cds, 'Q8', 3, 'H_CLUSTER')","ee75dcd2":"plot_KDE(df_cds, 'Q11', 3, 'H_CLUSTER')","5667e646":"plot_KDE(df_cds, 'Q13', 3, 'H_CLUSTER')","19ab0505":"plot_KDE(df_cds, 'Q15', 3, 'H_CLUSTER')","57e09190":"plot_KDE(df_cds, 'Q20', 3, 'H_CLUSTER')","a2a929ff":"plot_KDE(df_cds, 'Q21', 3, 'H_CLUSTER')","2487cc86":"plot_KDE(df_cds, 'Q22', 3, 'H_CLUSTER')","64551726":"plot_KDE(df_cds, 'Q24', 3, 'H_CLUSTER')","5efad923":"plot_KDE(df_cds, 'Q25', 3, 'H_CLUSTER')","0ad6d215":"plot_KDE(df_cds, 'Q38', 3, 'H_CLUSTER')","616b07c7":"# pct3 = (df_cds.groupby(['K_Clusters','Q1']).size() \/ df_cds.groupby(['K_Clusters']).size()).reset_index().rename({0:'percent'}, axis=1)\npct3 = df_cds.replace({'18-21':18, '22-24':22, '25-29':25, '30-34':30,'35-39':35, '40-44':40, '45-49':45,'50-54':50, '55-59':55, '60-69':60, '70+':70})\nplot_KDE(df_cds, 'Q1', 5, 'D_Clusters')","fbdbdfdf":"plot_KDE(df_cds, 'Q2', 5, 'D_Clusters')","43c1f7a9":"n=3\nprint(d1['Q3'].value_counts()[:n].index.tolist())\nprint(d2['Q3'].value_counts()[:n].index.tolist())\nprint(d3['Q3'].value_counts()[:n].index.tolist())\nprint(d4['Q3'].value_counts()[:n].index.tolist())\nprint(d5['Q3'].value_counts()[:n].index.tolist())\nprint(d6['Q3'].value_counts()[:n].index.tolist())\n","60622095":"plot_KDE(df_cds, 'Q3', 5, 'D_Clusters')","4551e742":"n=3\nprint(k1['Q1'].value_counts()[:n].index.tolist())\nprint(k2['Q1'].value_counts()[:n].index.tolist())\nprint(k3['Q1'].value_counts()[:n].index.tolist())\nprint(k4['Q1'].value_counts()[:n].index.tolist())\n","5bf1f24e":"df_cds_wna","b0b430af":"# pct3 = (df_cds.groupby(['K_Clusters','Q1']).size() \/ df_cds.groupby(['K_Clusters']).size()).reset_index().rename({0:'percent'}, axis=1)\npct3 = df_cds.replace({'18-21':18, '22-24':22, '25-29':25, '30-34':30,'35-39':35, '40-44':40, '45-49':45,'50-54':50, '55-59':55, '60-69':60, '70+':70})\n\nplot_KDE(df_cds, 'Q1', 3, 'K_Clusters')","b915ee00":"plot_KDE(df_cds, 'Q2', 3, 'K_Clusters')","26890f44":"n=3\nprint(k1['Q2'].value_counts()[:n].index.tolist())\nprint(k2['Q2'].value_counts()[:n].index.tolist())\nprint(k3['Q2'].value_counts()[:n].index.tolist())\nprint(k4['Q2'].value_counts()[:n].index.tolist())\n","7351998c":"df_cds.groupby('Q3_renamed').size().sort_values().plot(kind='barh',figsize=(10,8))","e8990910":"df_cds['Q3_f'] = pd.factorize( df_cds.Q3 )[0]","4ba98c94":"\nplot_KDE(df_cds, 'Q3', 3, 'K_Clusters')","67b8b059":"# plt.rcParams[\"figure.figsize\"] = (30,3)\n\n# pct3 = (df_cds.groupby(['K_Clusters','Q3']).size() \/ df_cds.groupby(['K_Clusters']).size()).reset_index().rename({0:'percent'}, axis=1)\n# sns.barplot(x='Q3', hue='K_Clusters', y='percent', data=pct3.sort_values(by='percent', ascending=False))\n# plt.xticks(rotation=70)\n# plt.legend(loc='upper right')\n# plt.tight_layout()\n\n# plt.show()","c7bee23c":"n=3\nprint(k1['Q3'].value_counts()[:n].index.tolist())\nprint(k2['Q3'].value_counts()[:n].index.tolist())\nprint(k3['Q3'].value_counts()[:n].index.tolist())\nprint(k4['Q3'].value_counts()[:n].index.tolist())\n","50b295bc":"# 0: Master\u2019s degree; 1:Doctoral degree, 2:Bachelor\u2019s; 3:Some college\/university study without earningbachelor; 4:I prefer not to answer; 5: Professional degree; 6:No formal education past high school\nplot_KDE(df_cds, 'Q4', 3, 'K_Clusters')","54544b0a":"# plt.rcParams[\"figure.figsize\"] = (20,3)\n\n# pct3 = (df_cds.groupby(['K_Clusters','Q4']).size() \/ df_cds.groupby(['K_Clusters']).size()).reset_index().rename({0:'percent'}, axis=1)\n# sns.barplot(x='Q4', hue='K_Clusters', y='percent', data=pct3.sort_values(by='percent', ascending=False))\n# plt.xticks(rotation=70)\n# plt.legend(loc='upper right')\n# plt.tight_layout()\n\n# plt.show()","d4f766f2":"n=3\nprint(k1['Q4'].value_counts()[:n].index.tolist())\nprint(k2['Q4'].value_counts()[:n].index.tolist())\nprint(k3['Q4'].value_counts()[:n].index.tolist())\nprint(k4['Q4'].value_counts()[:n].index.tolist())\n","9c090326":"df_.iloc[0,6]","1b889d67":"df_cds['Q6_f'] = pd.factorize( df_cds.Q6 )[0]","014c447b":"plot_KDE(df_cds, 'Q6', 3, 'K_Clusters')","e24725c6":"n=3\nprint(k1['Q6'].value_counts()[:n].index.tolist())\nprint(k2['Q6'].value_counts()[:n].index.tolist())\nprint(k3['Q6'].value_counts()[:n].index.tolist())\nprint(k4['Q6'].value_counts()[:n].index.tolist())\n","1213ea96":"df_.iloc[0,7]","1a4ba4ec":"n=3\nprint(k1['Q7_Part_1'].value_counts()[:n].index.tolist())\nprint(k2['Q7_Part_1'].value_counts()[:n].index.tolist())\nprint(k3['Q7_Part_1'].value_counts()[:n].index.tolist())\nprint(k4['Q7_Part_1'].value_counts()[:n].index.tolist())\n","7c33a9c2":"df_.iloc[0,8]","e591c69c":"plot_KDE(df_cds, 'Q8', 3, 'K_Clusters')","b5652dce":"\nn=3\nprint(k1['Q8'].value_counts()[:n].index.tolist())\nprint(k2['Q8'].value_counts()[:n].index.tolist())\nprint(k3['Q8'].value_counts()[:n].index.tolist())\n","f24216b6":"df_.iloc[0,9]","0f74a906":"n=3\nprint(k1['Q9_Part_1'].value_counts()[:n].index.tolist())\nprint(k2['Q9_Part_1'].value_counts()[:n].index.tolist())\nprint(k3['Q9_Part_1'].value_counts()[:n].index.tolist())\nprint(k4['Q9_Part_1'].value_counts()[:n].index.tolist())\n","d8a0ac10":"df_.iloc[0,10]","21134422":"plot_KDE(df_cds, 'Q11', 3, 'K_Clusters')","cbe7b071":"# df_cds[['Q11_f', 'Q11']].head(30)","f9057f10":"n=3\nprint(k1['Q11'].value_counts()[:n].index.tolist())\nprint(k2['Q11'].value_counts()[:n].index.tolist())\nprint(k3['Q11'].value_counts()[:n].index.tolist())\nprint(k4['Q11'].value_counts()[:n].index.tolist())\n","48aa5391":"df_.iloc[0,11]","1a5644d1":"plot_KDE(df_cds, 'Q13', 3, 'K_Clusters')","43389854":"n=3\nprint(k1['Q13'].value_counts()[:n].index.tolist())\nprint(k2['Q13'].value_counts()[:n].index.tolist())\nprint(k3['Q13'].value_counts()[:n].index.tolist())\nprint(k4['Q13'].value_counts()[:n].index.tolist())\n","2e74790e":"df_.iloc[0,12]","30507ea2":"n=3\nprint(k1['Q14_Part_1'].value_counts()[:n].index.tolist())\nprint(k2['Q14_Part_1'].value_counts()[:n].index.tolist())\nprint(k3['Q14_Part_1'].value_counts()[:n].index.tolist())\nprint(k4['Q14_Part_1'].value_counts()[:n].index.tolist())\n","03b9f7ec":"df_.iloc[0,13]","6ce0d503":"plot_KDE(df_cds, 'Q15', 3, 'K_Clusters')","3967176d":"n=3\nprint(k1['Q15'].value_counts()[:n].index.tolist())\nprint(k2['Q15'].value_counts()[:n].index.tolist())\nprint(k3['Q15'].value_counts()[:n].index.tolist())\nprint(k4['Q15'].value_counts()[:n].index.tolist())\n","92dd6a10":"df_.iloc[0,14]","6d3a863e":"n=3\nprint(k1['Q16_Part_1'].value_counts()[:n].index.tolist())\nprint(k2['Q16_Part_1'].value_counts()[:n].index.tolist())\nprint(k3['Q16_Part_1'].value_counts()[:n].index.tolist())\nprint(k4['Q16_Part_1'].value_counts()[:n].index.tolist())\n","572bb03b":"df_.iloc[0,15]","052f2c21":"n=3\nprint(k1['Q17_Part_1'].value_counts()[:n].index.tolist())\nprint(k2['Q17_Part_1'].value_counts()[:n].index.tolist())\nprint(k3['Q17_Part_1'].value_counts()[:n].index.tolist())\nprint(k4['Q17_Part_1'].value_counts()[:n].index.tolist())\n","993a9ce9":"df_.iloc[0,16]","1397f3d0":"plot_KDE(df_cds, 'Q20', 3, 'K_Clusters')","02d3d26a":"n=3\nprint(k1['Q20'].value_counts()[:n].index.tolist())\nprint(k2['Q20'].value_counts()[:n].index.tolist())\nprint(k3['Q20'].value_counts()[:n].index.tolist())\nprint(k4['Q20'].value_counts()[:n].index.tolist())\n","47f31f93":"df_.iloc[0,17]","d65f6236":"plot_KDE(df_cds, 'Q21', 3, 'K_Clusters')","1f6f7819":"n=3\nprint(k1['Q21'].value_counts()[:n].index.tolist())\nprint(k2['Q21'].value_counts()[:n].index.tolist())\nprint(k3['Q21'].value_counts()[:n].index.tolist())\nprint(k4['Q21'].value_counts()[:n].index.tolist())\n","6e862c14":"df_.iloc[0,18]","d8b3985c":"plot_KDE(df_cds, 'Q22', 3, 'K_Clusters')","f41daf1c":"n=3\nprint(k1['Q22'].value_counts()[:n].index.tolist())\nprint(k2['Q22'].value_counts()[:n].index.tolist())\nprint(k3['Q22'].value_counts()[:n].index.tolist())\nprint(k4['Q22'].value_counts()[:n].index.tolist())\n","b935dd42":"df_.iloc[0,19]","6a0d39ad":"plot_KDE(df_cds, 'Q24', 3, 'K_Clusters')","240e232c":"n=3\nprint(k1['Q24'].value_counts()[:n].index.tolist())\nprint(k2['Q24'].value_counts()[:n].index.tolist())\nprint(k3['Q24'].value_counts()[:n].index.tolist())\nprint(k4['Q24'].value_counts()[:n].index.tolist())\n","2f87e002":"df_.iloc[0,20]","313e7b8e":"plot_KDE(df_cds, 'Q25', 3, 'K_Clusters')","7f8678f5":"n=3\nprint(k1['Q25'].value_counts()[:n].index.tolist())\nprint(k2['Q25'].value_counts()[:n].index.tolist())\nprint(k3['Q25'].value_counts()[:n].index.tolist())\nprint(k4['Q25'].value_counts()[:n].index.tolist())\n","6d720bc1":"df_.iloc[0,21]","d67196c2":"plot_KDE(df_cds, 'Q38', 3, 'K_Clusters')","1cc44b1b":"n=3\nprint(k1['Q38'].value_counts()[:n].index.tolist())\nprint(k2['Q38'].value_counts()[:n].index.tolist())\nprint(k3['Q38'].value_counts()[:n].index.tolist())\nprint(k4['Q38'].value_counts()[:n].index.tolist())\n","9282cb46":"* Cluster 0: Personal computer and cloud and deep learning station\n* Cluster 1: Unknow\n* Cluster 2: Personal computer and cloud, others\n* Cluster 3: Personal computer and Cloud","15c1cf48":"The most of clusters are overrrided, so we will explore K-means!","bfeeb3e6":"# **Applying CRISP-DM to Kaggle Survey.**","f9ed0db7":"All uses Jupyter","7a562059":"## Initial Data Exploration","460a50b7":"# Business Understanding","7cb3fdb8":"Section to create utils functions!","e5043485":"* Gender","1089b558":"A curiosity observation, is that some Data Scientists (9%) answered that there is 0 individuals responsible by data science workload in your companies; And the most part of others answered that there is 1-2 responsible; ","637decca":"All respondents uses Python","2898fbc1":"# Deploy","ef0d9893":"Here we will analyse some variables to all dataset. In the graphs, we can see the absolute values and in the table, there are the percentages","d78f47d9":"# Data Understanding","f7f37a51":"About education, its possible observe that Researchers are in the most part Doctors; \nNot employed, are in the most part bachelors or some study without bachelor degree; \nAnother interesting observation, is that the most part of professionals choose not respond;","a55562ef":"Conclusion\n\nAs expected, the large majority are man. Data Analyst, Students and Statisticians are profession with most representativity that others; But away from a fair world","f3396417":"## Verify Data Quality","b25b62e9":"Conclusion:\nThe most part of respondents are from India, and they are in the most part Students or not employed, folowed by Software Engineer.  \nThe second part of respondents are form USA, where they are 17% Product\/Project Manager,the others professions are aproximately 15% in each; \n\nIn Brazil are 6.14% of BA and the less representative profession are from Machine Learning Engineers;","a1ad29b4":"* # What type of Data Scientist are you?\n\n","50564be9":"I'ts possible to see 3 clusters. To explore more, we will use MCA to transform features and use K-means and DBSCAN","c748c489":"Let's see the distributions of clusters founded by Hierarchical clustering:","2b8a5cc5":" Basic statistical software  or Local development environments  is the majority choises; Statisticians are that users of advanced softwares","dbfbabbe":"All uses Matplotlib","8f5ccc09":"* Cluster 0: 3-4\n* Cluster 1: Unknow or 0\n* Cluster 2: 20+\n* Cluster 3 1-2 or 0","2f89a397":"Let's do some exploratory analysis in each cluster that was found:","ac5ada17":"* Cluster 0: Never and 2-5 times\n* Cluster 1: Unknow or Never\n* Cluster 2: Never or more than 25 times\n* Cluster 3: Never","73fa0cf6":"### Conclusion ","64efa02d":"* Age","383a9905":"* Cluster 0: Recently started\n* Cluster 1: Unknow ; I do not know\n* Cluster 2: Well established\n* Cluster 3: Exploring","1b88fe26":"The most experiences with ML methods are DS and ML Engineers, approximately 12% with 5-10years. The less are Database Engineers (27% do not use ML and 30% under 1 year)","ee8166dc":"The most part of respondents working in small companies with 0-49 employees; But there is a good percentage working in big companies (10000 or more)","37dc3d66":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Business-Understanding\" data-toc-modified-id=\"Business-Understanding-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Business Understanding<\/a><\/span><\/li><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Functions<\/a><\/span><\/li><li><span><a href=\"#Data-Understanding\" data-toc-modified-id=\"Data-Understanding-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Data Understanding<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Describe-Data\" data-toc-modified-id=\"Describe-Data-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Describe Data<\/a><\/span><\/li><li><span><a href=\"#Verify-Data-Quality\" data-toc-modified-id=\"Verify-Data-Quality-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>Verify Data Quality<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Missing-Data\" data-toc-modified-id=\"Missing-Data-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;<\/span>Missing Data<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Initial-Data-Exploration\" data-toc-modified-id=\"Initial-Data-Exploration-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;<\/span>Initial Data Exploration<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Distributions\" data-toc-modified-id=\"Distributions-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;<\/span>Distributions<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Professional-Distributions\" data-toc-modified-id=\"Professional-Distributions-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;<\/span>Professional Distributions<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Data-Prep\" data-toc-modified-id=\"Data-Prep-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Data Prep<\/a><\/span><\/li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Model<\/a><\/span><\/li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Evaluation<\/a><\/span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Kmeans\" data-toc-modified-id=\"Kmeans-6.0.1\"><span class=\"toc-item-num\">6.0.1&nbsp;&nbsp;<\/span>Kmeans<\/a><\/span><\/li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-6.0.2\"><span class=\"toc-item-num\">6.0.2&nbsp;&nbsp;<\/span>Conclusion<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Deploy\" data-toc-modified-id=\"Deploy-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Deploy<\/a><\/span><\/li><\/ul><\/div>","4cc6ad4a":"### Distributions","0158b2fc":"The most part uses a personal computer or laptop. Machine Learning Engineer, Data Scients and Data Engineers are the users of cloud computing ","80aa09dd":"* **What Questions Are We Trying To Answer**?\n    - What are the characteristics from professionals?  (Quais s\u00e3o as caracteristicas dos diferentes profissionais que responderam ao survey?\n    - It's possible to identify profiles of Data Scientits? (\u00c9 possivel identificar perfis diferentes de cientistas?)","df5af170":"MCA was used to transform categorical data in numeric to perform others clusters algorithms","f1f02a42":"* Cluster 0: 0-10 years and 3-5 years (has medium experience)\n* Cluster 1: 3-5 years; Never write code (has medium experience)\n* Cluster 2: 10-20 years; >20years (has a lot of experience)\n* Cluster 3: < 1years; 1-2years (has litter experience)","382303c6":"* **What are the desired outputs of the project?**\n\n    - Tell a history about Kaggle users that answers the Survey","4eb14a65":"* Profile 0 - Intemediate: earn more, work in medium companies,uses more softwares sophisticated, are in yours 25-29; \n* Profile 1:  Ghost  Don't answer :( or dont Know\n* Profile 2 - Advanced - has more experience with code, work in large companies that has ML methods in production. They are older and has more schooling and the most part already uses ML\n* Profile 3 - Begginner - earn little, has little experience and are youngers \n","a6c0feac":"We believe that is not interesting to analyse columns with a lot of missing. So, we removed that columns. This is important too, because we want to do a model.","f03d01c5":"The most part of DS are form India; Just the Cluster 02 is composed mostly by Americans. The first clusters has a percentage of Brazilians.","459eb637":"Let's explore:","bc4995d8":"## Describe Data ","8b0dde1d":"**Disclaimer** : this graps are in wraped because I can't resize then in Kaggle Notebooks ","c8a9d609":"The most experients with programming are  DBA with 20+ years of experience (21%); followed by Researchs (16%); \nThe less experients are Bussiness Analyst, with 12% has never written code; followed by Managers and not employed;\nSoftware engineers has the less % of respondents tha never write code (1%)","4ba1a170":"### Hierarchical","68e377d0":"# Functions","92896d86":"In this sections we start to understanding the dataset. We will analyze shape, missing, what type of columns we have and think about data prep. \nHere we will start our analyze about the respondents. This analyzes will be done grouped by professions; ","c46df18f":"There are 2037 respondents and 355 questions. All columns are objects (str), and maybe we will need do some transformations;","e2f664f3":"And the most recommended language is Python. R and Sql are also recomended.","44dcfb68":"So, who are you?\n\nI'm in the Profile 0 (=  with some exceptions!","ef80c41e":"* Cluster 0: 30,000-39,999'\n* Cluster 1: Unknow ; 0-999\n* Cluster 2: 100,000-124,999\n* Cluster 3: 0-999","78492827":"In this section, we will made some treatments to test some models;","d1fa02e3":" ### Missing Data","221fd265":"Here we will perform analysis by title of professionals:","2bc2fc27":"The objective is explore the data by professions and understanding the profile of Data Scientists using Cluster algorithms.","2821ec5a":"* **Key Results:**\n - There are some curiosities when we analyzed the data by professions, like, there are some Machine Learning Engineers thar working in companies that don't uses Machine Learning.  \n - We identified 4 profiles of data scientists, they are:\n     - The Beginner\n     - The Intermediate\n     - The Advanced\n     - The Ghost\n     \nRead more to find another curiosities! And if you like, give me a comment or a upvote (=","c6c55973":"- Next work:\n    *  Create a dash to visualize and compare years\n    * Identify clusters to all professions","73893170":"Using this lib and prior knowledge, we will use 4 clusters","9bc0ee4a":"## Professional Distributions","8a8f8d80":"Gower distance is used with categorical features:","2ac09c48":"* Country","3d37c39c":"Conclusion\n- Data Scientist: 25% has between 25-29 years and 1% has 70+\n- ML Engineer: 27% has between 25-29 years and anyone has 70+\n- Students: 49% has between 18-21 years,\n- Statistican: 19% has between 25-29 years and 18% 22-24. 2% has 70+\n- BA: 22% has between 25-29 years\n- Product\/Project Manager: There are representations in all intervals. \n- Data Analyst: 27% has between 25-29 years and 1% has 70+\n- Software Engineer: 21% has between 22-24 and 21% 25-29 years\n- Data Engineer: 25% has between 25-29 years\n- Resercher: 20% has between 30-34 years\n- Not Employed: 26% has between 25-29 years\n\n\nThe most part of respondents has between 25-29 years old. As expected, students are the most youngs and Reserchers are in 30-34 years old, probably after finish his doctor degreed. \n\n\n\n    \n    ","a4b2b13c":"**obs**: remove rows with NA does not makes difference to kmeans","a127b863":"#  Data Prep","75f2adf4":"* Cluster 0: 1-2; 3-4 years\n* Cluster 1: Unknow\n* Cluster 2: 5-10 years\n* Cluster 3: Under 1 year","377b61ae":"Almost data scients are Masters. Cluster 02 has doctors and 03 has more bachelors than masters;","286eb5ff":"* Cluster 0: 0-49 \n* Cluster 1: Unknow\n* Cluster 2: 10,0000- 99999\n* Cluster 3 0-49","b496bfb0":"Professions with bigger salary are DBAs (>500.000). The most part of respondents earn between 0-999; And, the most part with this salary are statisticians\n","70892e25":"# Evaluation","a07ee9aa":"Here, we can see another curiosity. There is **Data Scientists and Machine Leaning Engineers that work in companies that don't uses ML** methods. Between statistician, 30% do not uses ML ","fa83101b":"* Cluster 0: 25-29\n* Cluster 1: 25-29\n* Cluster 2: 30-34\n* Cluster 3: 22-24","ed01e775":"This results shows that are different types of Data Scientists;  ","85cc7583":"The clusters are composed mostly by men","d82b797a":"So, we used a hierarchical algorithm to find the clusters:","878b57b2":"# Model\n\nHere we are interested in buil a model to identify profiles of Data Scientists. 3 models will be tested: Hyerarchy with Gower distance; K-means and DBSCAN with MCA.\n\nWe will perfom some tests with some missings and removing all of then. \n\n","09873208":"* Cluster 0: 1000-9,999\n* Cluster 1: Unknow or 0\n* Cluster 2: 100,000 or more\n* Cluster 3: 0","b90c97c8":"### Kmeans","51ed1adc":"The most part has never used TPU. 21% of Machine Learning Engineers has used 2-5 times  and 19% of Data Engineers too;","6cbb8c50":"The most part does not spent money in cloud computing services; Followed by spent 100-999. And 12% of DS spent more than \\$100.000","f42ddc25":"This graph shows us that are a lot of missing values. Let's select the columns with more than 50% of missing values and remove","7131cfec":"### DBSCAN"}}