{"cell_type":{"7c263614":"code","2cf7a004":"code","60e765af":"code","184b9cf6":"code","22f36bac":"code","14a33afa":"code","85d50675":"code","146ce305":"code","d30b95f1":"code","5a6ad38d":"code","17116628":"code","7b60036e":"code","4d2e62ca":"code","f8f0074e":"code","8b4ff7fd":"code","de4af4d8":"code","fc8b5097":"markdown","f2b242f5":"markdown","1f287635":"markdown","91a0f738":"markdown","588cfb05":"markdown","a681908f":"markdown","29e379c8":"markdown","59f5a9f1":"markdown","e3c912e3":"markdown","c44644f2":"markdown","e5390372":"markdown","b77b805e":"markdown","bb8c8e1b":"markdown","bba7be0f":"markdown","97a86c7a":"markdown","6061e7c8":"markdown"},"source":{"7c263614":"!pip install spacy-langdetect\n!pip install language-detector\n!pip install symspellpy\n!pip install sentence-transformers","2cf7a004":"#covid-papers-browser\/models\/scibert-nli\/","60e765af":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/mallet'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","184b9cf6":"#importing all dependencies.\nimport os\nimport json\nimport pandas as pd\nfrom tqdm import tqdm\nimport numpy as np\nfrom nltk.corpus import wordnet\nimport re\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords \n#from geotext import GeoText\n#\/kaggle\/input","22f36bac":"#https:\/\/www.kaggle.com\/jieyang0311\/covid-19-topic-modeling-lda\n#load library\nimport os\nimport pandas as pd\nimport numpy as np\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim import corpora, models\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\n\nimport datetime\nimport time\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport nltk","14a33afa":"meta = pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")\nprint(meta.shape)","85d50675":"### first filter by meta file. select only papers after 2020\nmeta[\"publish_time\"] = pd.to_datetime(meta[\"publish_time\"])\nmeta[\"publish_year\"] = (pd.DatetimeIndex(meta['publish_time']).year)\nmeta[\"publish_month\"] = (pd.DatetimeIndex(meta['publish_time']).month)\nmeta = meta[meta[\"publish_year\"] == 2020]\nprint(meta.shape[0], \" papers are available after 2020 Jan 1.\")","146ce305":"#count how many has abstract\ncount = 0\nindex = []\nfor i in range(len(meta)):\n    #print(i)\n    if type(meta.iloc[i, 8])== float:\n        count += 1\n    else:\n        index.append(i)\n\nprint(len(index), \" papers have abstract available.\")","d30b95f1":"##extract the abstract to pandas \ndocuments = meta.iloc[index, 8]\ndocuments=documents.reset_index()\ndocuments.drop(\"index\", inplace = True, axis = 1)\n\n##create pandas data frame with all abstracts, use as input corpus\ndocuments[\"index\"] = documents.index.values\ndocuments.head(3)","5a6ad38d":"documents.head(15)","17116628":"from collections import Counter\nfrom sklearn.metrics import silhouette_score\nimport umap\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom gensim.models.coherencemodel import CoherenceModel\nimport numpy as np\nimport os\n\n\ndef get_topic_words(token_lists, labels, k=None):\n    \"\"\"\n    get top words within each topic from clustering results\n    \"\"\"\n    if k is None:\n        k = len(np.unique(labels))\n    topics = ['' for _ in range(k)]\n    for i, c in enumerate(token_lists):\n        topics[labels[i]] += (' ' + ' '.join(c))\n    word_counts = list(map(lambda x: Counter(x.split()).items(), topics))\n    # get sorted word counts\n    word_counts = list(map(lambda x: sorted(x, key=lambda x: x[1], reverse=True), word_counts))\n    # get topics\n    topics = list(map(lambda x: list(map(lambda x: x[0], x[:10])), word_counts))\n\n    return topics\n\ndef get_coherence(model, token_lists, measure='c_v'):\n    \"\"\"\n    Get model coherence from gensim.models.coherencemodel\n    :param model: Topic_Model object\n    :param token_lists: token lists of docs\n    :param topics: topics as top words\n    :param measure: coherence metrics\n    :return: coherence score\n    \"\"\"\n    if model.method == 'LDA':\n        cm = CoherenceModel(model=model.ldamodel, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n                            coherence=measure)\n    else:\n        topics = get_topic_words(token_lists, model.cluster_model.labels_)\n        cm = CoherenceModel(topics=topics, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n                            coherence=measure)\n    return cm.get_coherence()\n\ndef get_silhouette(model):\n    \"\"\"\n    Get silhouette score from model\n    :param model: Topic_Model object\n    :return: silhouette score\n    \"\"\"\n    if model.method == 'LDA':\n        return\n    lbs = model.cluster_model.labels_\n    vec = model.vec[model.method]\n    return silhouette_score(vec, lbs)\n\ndef plot_proj(embedding, lbs):\n    \"\"\"\n    Plot UMAP embeddings\n    :param embedding: UMAP (or other) embeddings\n    :param lbs: labels\n    \"\"\"\n    n = len(embedding)\n    counter = Counter(lbs)\n    for i in range(len(np.unique(lbs))):\n        plt.plot(embedding[:, 0][lbs == i], embedding[:, 1][lbs == i], '.', alpha=0.5,\n                 label='cluster {}: {:.2f}%'.format(i, counter[i] \/ n * 100))\n    plt.legend(loc = 'best')\n    plt.grid(color ='grey', linestyle='-',linewidth = 0.25)\n\n\ndef visualize(model):\n    \"\"\"\n    Visualize the result for the topic model by 2D embedding (UMAP)\n    :param model: Topic_Model object\n    \"\"\"\n    if model.method == 'LDA':\n        return\n    reducer = umap.UMAP()\n    print('Calculating UMAP projection ...')\n    vec_umap = reducer.fit_transform(model.vec[model.method])\n    print('Calculating UMAP projection. Done!')\n    plot_proj(vec_umap, model.cluster_model.labels_)\n    dr = '\/kaggle\/working\/contextual_topic_identification\/docs\/images\/{}\/{}'.format(model.method, model.id)\n    if not os.path.exists(dr):\n        os.makedirs(dr)\n    plt.savefig('\/kaggle\/working\/2D_vis')\n\ndef get_wordcloud(model, token_lists, topic):\n    \"\"\"\n    Get word cloud of each topic from fitted model\n    :param model: Topic_Model object\n    :param sentences: preprocessed sentences from docs\n    \"\"\"\n    if model.method == 'LDA':\n        return\n    print('Getting wordcloud for topic {} ...'.format(topic))\n    lbs = model.cluster_model.labels_\n    tokens = ' '.join([' '.join(_) for _ in np.array(token_lists)[lbs == topic]])\n\n    wordcloud = WordCloud(width=800, height=560,\n                          background_color='white', collocations=False,\n                          min_font_size=10).generate(tokens)\n\n    # plot the WordCloud image\n    plt.figure(figsize=(8, 5.6), facecolor=None)\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.tight_layout(pad=0)\n    dr = '\/kaggle\/working\/{}\/{}'.format(model.method, model.id)\n    if not os.path.exists(dr):\n        os.makedirs(dr)\n    plt.savefig('\/kaggle\/working' + '\/Topic' + str(topic) + '_wordcloud')\n    print('Getting wordcloud for topic {}. Done!'.format(topic))","7b60036e":"from stop_words import get_stop_words\nfrom nltk.stem.porter import PorterStemmer\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom language_detector import detect_language\n\nimport pkg_resources\nfrom symspellpy import SymSpell, Verbosity\n\nsym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\ndictionary_path = pkg_resources.resource_filename(\n    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\nif sym_spell.word_count:\n    pass\nelse:\n    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n\n\n###################################\n#### sentence level preprocess ####\n###################################\n\n# lowercase + base filter\n# some basic normalization\ndef f_base(s):\n    \"\"\"\n    :param s: string to be processed\n    :return: processed string: see comments in the source code for more info\n    \"\"\"\n    # normalization 1: xxxThis is a --> xxx. This is a (missing delimiter)\n    s = re.sub(r'([a-z])([A-Z])', r'\\1\\. \\2', s)  # before lower case\n    # normalization 2: lower case\n    s = s.lower()\n    # normalization 3: \"&gt\", \"&lt\"\n    s = re.sub(r'&gt|&lt', ' ', s)\n    # normalization 4: letter repetition (if more than 2)\n    s = re.sub(r'([a-z])\\1{2,}', r'\\1', s)\n    # normalization 5: non-word repetition (if more than 1)\n    s = re.sub(r'([\\W+])\\1{1,}', r'\\1', s)\n    # normalization 6: string * as delimiter\n    s = re.sub(r'\\*|\\W\\*|\\*\\W', '. ', s)\n    # normalization 7: stuff in parenthesis, assumed to be less informal\n    s = re.sub(r'\\(.*?\\)', '. ', s)\n    # normalization 8: xxx[?!]. -- > xxx.\n    s = re.sub(r'\\W+?\\.', '.', s)\n    # normalization 9: [.?!] --> [.?!] xxx\n    s = re.sub(r'(\\.|\\?|!)(\\w)', r'\\1 \\2', s)\n    # normalization 10: ' ing ', noise text\n    s = re.sub(r' ing ', ' ', s)\n    # normalization 11: noise text\n    s = re.sub(r'product received for free[.| ]', ' ', s)\n    # normalization 12: phrase repetition\n    s = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', s)\n\n    return s.strip()\n\n\n# language detection\ndef f_lan(s):\n    \"\"\"\n    :param s: string to be processed\n    :return: boolean (s is English)\n    \"\"\"\n\n    # some reviews are actually english but biased toward french\n    return detect_language(s) in {'English', 'French','Spanish','Chinese'}\n\n\n###############################\n#### word level preprocess ####\n###############################\n\n# filtering out punctuations and numbers\ndef f_punct(w_list):\n    \"\"\"\n    :param w_list: word list to be processed\n    :return: w_list with punct and number filter out\n    \"\"\"\n    return [word for word in w_list if word.isalpha()]\n\n\n# selecting nouns\ndef f_noun(w_list):\n    \"\"\"\n    :param w_list: word list to be processed\n    :return: w_list with only nouns selected\n    \"\"\"\n    return [word for (word, pos) in nltk.pos_tag(w_list) if pos[:2] == 'NN']\n\n\n# typo correction\ndef f_typo(w_list):\n    \"\"\"\n    :param w_list: word list to be processed\n    :return: w_list with typo fixed by symspell. words with no match up will be dropped\n    \"\"\"\n    w_list_fixed = []\n    for word in w_list:\n        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n        if suggestions:\n            w_list_fixed.append(suggestions[0].term)\n        else:\n            pass\n            # do word segmentation, deprecated for inefficiency\n            # w_seg = sym_spell.word_segmentation(phrase=word)\n            # w_list_fixed.extend(w_seg.corrected_string.split())\n    return w_list_fixed\n\n\n# stemming if doing word-wise\np_stemmer = PorterStemmer()\n\n\ndef f_stem(w_list):\n    \"\"\"\n    :param w_list: word list to be processed\n    :return: w_list with stemming\n    \"\"\"\n    return [p_stemmer.stem(word) for word in w_list]\n\n\n# filtering out stop words\n# create English stop words list\n\nstop_words = (list(\n    set(get_stop_words('en'))\n    |set(get_stop_words('es'))\n    |set(get_stop_words('de'))\n    |set(get_stop_words('it'))\n    |set(get_stop_words('ca'))\n    #|set(get_stop_words('cy'))\n    |set(get_stop_words('pt'))\n    #|set(get_stop_words('tl'))\n    |set(get_stop_words('pl'))\n    #|set(get_stop_words('et'))\n    |set(get_stop_words('da'))\n    |set(get_stop_words('ru'))\n    #|set(get_stop_words('so'))\n    |set(get_stop_words('sv'))\n    |set(get_stop_words('sk'))\n    #|set(get_stop_words('cs'))\n    |set(get_stop_words('nl'))\n    #|set(get_stop_words('sl'))\n    #|set(get_stop_words('no'))\n    #|set(get_stop_words('zh-cn'))\n))\n\n\n\n\n\ndef f_stopw(w_list):\n    \"\"\"\n    filtering out stop words\n    \"\"\"\n    return [word for word in w_list if word not in stop_words]\n\n\ndef preprocess_sent(rw):\n    \"\"\"\n    Get sentence level preprocessed data from raw review texts\n    :param rw: review to be processed\n    :return: sentence level pre-processed review\n    \"\"\"\n    s = f_base(rw)\n    if not f_lan(s):\n        return None\n    return s\n\n\ndef preprocess_word(s):\n    \"\"\"\n    Get word level preprocessed data from preprocessed sentences\n    including: remove punctuation, select noun, fix typo, stem, stop_words\n    :param s: sentence to be processed\n    :return: word level pre-processed review\n    \"\"\"\n    if not s:\n        return None\n    w_list = word_tokenize(s)\n    w_list = f_punct(w_list)\n    w_list = f_noun(w_list)\n    w_list = f_typo(w_list)\n    w_list = f_stem(w_list)\n    w_list = f_stopw(w_list)\n\n    return w_list","4d2e62ca":"import keras\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nclass Autoencoder:\n    \"\"\"\n    Autoencoder for learning latent space representation\n    architecture simplified for only one hidden layer\n    \"\"\"\n\n    def __init__(self, latent_dim=32, activation='relu', epochs=200, batch_size=128):\n        self.latent_dim = latent_dim\n        self.activation = activation\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.autoencoder = None\n        self.encoder = None\n        self.decoder = None\n        self.his = None\n\n    def _compile(self, input_dim):\n        \"\"\"\n        compile the computational graph\n        \"\"\"\n        input_vec = Input(shape=(input_dim,))\n        encoded = Dense(self.latent_dim, activation=self.activation)(input_vec)\n        decoded = Dense(input_dim, activation=self.activation)(encoded)\n        self.autoencoder = Model(input_vec, decoded)\n        self.encoder = Model(input_vec, encoded)\n        encoded_input = Input(shape=(self.latent_dim,))\n        decoder_layer = self.autoencoder.layers[-1]\n        self.decoder = Model(encoded_input, self.autoencoder.layers[-1](encoded_input))\n        self.autoencoder.compile(optimizer='adam', loss=keras.losses.mean_squared_error)\n\n    def fit(self, X):\n        if not self.autoencoder:\n            self._compile(X.shape[1])\n        X_train, X_test = train_test_split(X)\n        self.his = self.autoencoder.fit(X_train, X_train,\n                                        epochs=200,\n                                        batch_size=128,\n                                        shuffle=True,\n                                        validation_data=(X_test, X_test), verbose=0)","f8f0074e":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom gensim import corpora\nimport gensim\nimport numpy as np\n#from Autoencoder import *\n#from preprocess import *\nfrom datetime import datetime\n\n\ndef preprocess(docs, samp_size=None):\n    \"\"\"\n    Preprocess the data\n    \"\"\"\n    if not samp_size:\n        samp_size = 100\n\n    print('Preprocessing raw texts ...')\n    n_docs = len(docs)\n    sentences = []  # sentence level preprocessed\n    token_lists = []  # word level preprocessed\n    idx_in = []  # index of sample selected\n    #     samp = list(range(100))\n    samp = np.random.choice(n_docs, samp_size)\n    for i, idx in enumerate(samp):\n        sentence = preprocess_sent(docs[idx])\n        token_list = preprocess_word(sentence)\n        if token_list:\n            idx_in.append(idx)\n            sentences.append(sentence)\n            token_lists.append(token_list)\n        print('{} %'.format(str(np.round((i + 1) \/ len(samp) * 100, 2))), end='\\r')\n    print('Preprocessing raw texts. Done!')\n    return sentences, token_lists, idx_in\n\n\n# define model object\nclass Topic_Model:\n    def __init__(self, k=10, method='TFIDF'):\n        \"\"\"\n        :param k: number of topics\n        :param method: method chosen for the topic model\n        \"\"\"\n        if method not in {'TFIDF', 'LDA', 'BERT', 'LDA_BERT'}:\n            raise Exception('Invalid method!')\n        self.k = k\n        self.dictionary = None\n        self.corpus = None\n        #         self.stopwords = None\n        self.cluster_model = None\n        self.ldamodel = None\n        self.vec = {}\n        self.gamma = 15  # parameter for reletive importance of lda\n        self.method = method\n        self.AE = None\n        self.id = method + '_' + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n\n    def vectorize(self, sentences, token_lists, method=None):\n        \"\"\"\n        Get vecotr representations from selected methods\n        \"\"\"\n        # Default method\n        if method is None:\n            method = self.method\n\n        # turn tokenized documents into a id <-> term dictionary\n        self.dictionary = corpora.Dictionary(token_lists)\n        # convert tokenized documents into a document-term matrix\n        self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n\n        if method == 'TFIDF':\n            print('Getting vector representations for TF-IDF ...')\n            tfidf = TfidfVectorizer()\n            vec = tfidf.fit_transform(sentences)\n            print('Getting vector representations for TF-IDF. Done!')\n            return vec\n\n        elif method == 'LDA':\n            print('Getting vector representations for LDA ...')\n            if not self.ldamodel:\n                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n                                                                passes=20)\n\n            def get_vec_lda(model, corpus, k):\n                \"\"\"\n                Get the LDA vector representation (probabilistic topic assignments for all documents)\n                :return: vec_lda with dimension: (n_doc * n_topic)\n                \"\"\"\n                n_doc = len(corpus)\n                vec_lda = np.zeros((n_doc, k))\n                for i in range(n_doc):\n                    # get the distribution for the i-th document in corpus\n                    for topic, prob in model.get_document_topics(corpus[i]):\n                        vec_lda[i, topic] = prob\n\n                return vec_lda\n\n            vec = get_vec_lda(self.ldamodel, self.corpus, self.k)\n            print('Getting vector representations for LDA. Done!')\n            return vec\n\n        elif method == 'BERT':\n\n            print('Getting vector representations for BERT ...')\n            from sentence_transformers import SentenceTransformer\n            model = SentenceTransformer('bert-base-nli-max-tokens')\n            vec = np.array(model.encode(sentences, show_progress_bar=True))\n            print('Getting vector representations for BERT. Done!')\n            return vec\n\n             \n        elif method == 'LDA_BERT':\n        #else:\n            vec_lda = self.vectorize(sentences, token_lists, method='LDA')\n            vec_bert = self.vectorize(sentences, token_lists, method='BERT')\n            vec_ldabert = np.c_[vec_lda * self.gamma, vec_bert]\n            self.vec['LDA_BERT_FULL'] = vec_ldabert\n            if not self.AE:\n                self.AE = Autoencoder()\n                print('Fitting Autoencoder ...')\n                self.AE.fit(vec_ldabert)\n                print('Fitting Autoencoder Done!')\n            vec = self.AE.encoder.predict(vec_ldabert)\n            return vec\n\n    def fit(self, sentences, token_lists, method=None, m_clustering=None):\n        \"\"\"\n        Fit the topic model for selected method given the preprocessed data\n        :docs: list of documents, each doc is preprocessed as tokens\n        :return:\n        \"\"\"\n        # Default method\n        if method is None:\n            method = self.method\n        # Default clustering method\n        if m_clustering is None:\n            m_clustering = KMeans\n\n        # turn tokenized documents into a id <-> term dictionary\n        if not self.dictionary:\n            self.dictionary = corpora.Dictionary(token_lists)\n            # convert tokenized documents into a document-term matrix\n            self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n\n        ####################################################\n        #### Getting ldamodel or vector representations ####\n        ####################################################\n\n        if method == 'LDA':\n            if not self.ldamodel:\n                print('Fitting LDA ...')\n                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n                                                                passes=20)\n                print('Fitting LDA Done!')\n        else:\n            print('Clustering embeddings ...')\n            self.cluster_model = m_clustering(self.k)\n            self.vec[method] = self.vectorize(sentences, token_lists, method)\n            self.cluster_model.fit(self.vec[method])\n            print('Clustering embeddings. Done!')\n\n    def predict(self, sentences, token_lists, out_of_sample=None):\n        \"\"\"\n        Predict topics for new_documents\n        \"\"\"\n        # Default as False\n        out_of_sample = out_of_sample is not None\n\n        if out_of_sample:\n            corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n            if self.method != 'LDA':\n                vec = self.vectorize(sentences, token_lists)\n                print(vec)\n        else:\n            corpus = self.corpus\n            vec = self.vec.get(self.method, None)\n\n        if self.method == \"LDA\":\n            lbs = np.array(list(map(lambda x: sorted(self.ldamodel.get_document_topics(x),\n                                                     key=lambda x: x[1], reverse=True)[0][0],\n                                    corpus)))\n        else:\n            lbs = self.cluster_model.predict(vec)\n        return lbs","8b4ff7fd":"#from model import *\n#from utils import *\n\nimport pandas as pd\nimport pickle\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore', category=Warning)\n\nimport argparse\n\n#def model(): #:if __name__ == '__main__':\n\ndef main():\n    \n    \n    method = \"LDA_BERT\"\n    samp_size = 51000\n    ntopic = 10\n    \n    #parser = argparse.ArgumentParser(description='contextual_topic_identification tm_test:1.0')\n\n    #parser.add_argument('--fpath', default='\/kaggle\/working\/train.csv')\n    #parser.add_argument('--ntopic', default=10,)\n    #parser.add_argument('--method', default='TFIDF')\n    #parser.add_argument('--samp_size', default=20500)\n    \n    #args = parser.parse_args()\n\n    data = documents #pd.read_csv('\/kaggle\/working\/train.csv')\n    data = data.fillna('')  # only the comments has NaN's\n    rws = data.abstract\n    sentences, token_lists, idx_in = preprocess(rws, samp_size=samp_size)\n    # Define the topic model object\n    #tm = Topic_Model(k = 10), method = TFIDF)\n    tm = Topic_Model(k = ntopic, method = method)\n    # Fit the topic model by chosen method\n    tm.fit(sentences, token_lists)\n    # Evaluate using metrics\n    with open(\"\/kaggle\/working\/{}.file\".format(tm.id), \"wb\") as f:\n        pickle.dump(tm, f, pickle.HIGHEST_PROTOCOL)\n\n    print('Coherence:', get_coherence(tm, token_lists, 'c_v'))\n    print('Silhouette Score:', get_silhouette(tm))\n    # visualize and save img\n    visualize(tm)\n    for i in range(tm.k):\n        get_wordcloud(tm, token_lists, i)\n        \n","de4af4d8":"main()","fc8b5097":"## Meet our team\n\n* [William Green](https:\/\/www.kaggle.com\/dskswu)\n* [Frank Mitchell](https:\/\/www.kaggle.com\/fmitchell259)\n* [Salmon Chen](https:\/\/www.kaggle.com\/tianyeeee)\n* [Aarti](https:\/\/www.kaggle.com\/aarti9)\n","f2b242f5":"## Model Deep Dive\n\nThe author used: \n\n* LDA for probabilistic topic assignment vector.\n* Bert for sentence embedding vector.\n\n1. Concatenated both LDA and Bert vectors with a weight hyperparameter to balance the relative importance of information from each source.\n2. Used autoencoder to learn a lower dimensional latent space representation of the concatenated vector.\n\n* The assumption is that the concatendate vector shoul have a manifold shaep in the high dimensional space. \n* USed clustering on the latent space representations to get topics. \n\n","1f287635":"## Utils","91a0f738":"**Insired by:** \n\nShoa, S. (2020). **Contextual Topic Identification: Identifying meaningful topics for sparse Steam reviews**. Medium. Available at: https:\/\/blog.insightdatascience.com\/contextual-topic-identification-4291d256a032 [Accessed 25 Mar. 2020].","588cfb05":"## Model","a681908f":"![Contextual Topic Identification model design](https:\/\/miro.medium.com\/max\/1410\/1*OKCYnB-JbGq1NDwNSKd5Zw.png)","29e379c8":"![](https:\/\/static.seattletimes.com\/wp-content\/uploads\/2020\/03\/coronavirus-mask-illo-color-768x557.jpg)","59f5a9f1":"## Autoencoder","e3c912e3":"# Topic Modeling with BERT, LDA, and Clustering\n#### Latent Dirichlet Allocation (LDA) probabilistic topic assignment and pre-trained sentence embeddings from BERT\/RoBERTa","c44644f2":"![](http:\/\/)","e5390372":"## Training","b77b805e":"**Data pipeline (from development to deployment**)\n\n![Data pipeline (from development to deployment)](https:\/\/miro.medium.com\/max\/1348\/1*Cdp4y1tfMxqoj96o6lUdFg.png)\n\n\nSource:Shoa ","bb8c8e1b":"### TODO:\n\n* Implement models.ldamulticore \u2013 parallelized Latent Dirichlet Allocation using all CPU cores to parallelize and speed up model training.\n* Switch from BERT\/RoBERTa to SciBERT, BART, and or other models. ","bba7be0f":"## Description\n\nIn response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). CORD-19 is a resource of over 44,000 scholarly articles, including over 29,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses.","97a86c7a":"## Preprocessing ","6061e7c8":"## Upload Data"}}