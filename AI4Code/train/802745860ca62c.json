{"cell_type":{"acca51c5":"code","652a1a0c":"code","f93194ac":"code","78cb6b8a":"code","67a0362e":"code","928b8df9":"code","1f9bd13d":"code","ba71a1ed":"code","f0936a02":"code","3f1d65d4":"code","cbefc25d":"code","02df240b":"code","778ad0e6":"code","99cbf55f":"code","84b3b97d":"code","c1b829f3":"code","84f0cf99":"code","1cbfa974":"code","88b71bdc":"code","62b7e715":"code","332bdc6f":"code","beda2799":"code","a46da1dc":"code","37469398":"code","74bd5c1f":"code","56781221":"code","76878804":"code","4c0c275d":"code","e09b14d8":"code","73169d15":"code","b3b577c9":"markdown","9c5fa438":"markdown","7eb20e26":"markdown","0a2b7e18":"markdown","1fb9d438":"markdown","c5c08606":"markdown","610c0d11":"markdown","b87c0cb0":"markdown","5a08d570":"markdown","4391c068":"markdown","9367723a":"markdown","9f6ddacd":"markdown","3defd912":"markdown","138d2f71":"markdown","2ee5f150":"markdown","c67dec16":"markdown","980ac948":"markdown","6f394d53":"markdown","e6f3f48c":"markdown","e32dc0b9":"markdown","173191b4":"markdown","7d6584aa":"markdown","2639a6f5":"markdown","ff9c95a3":"markdown","e723df90":"markdown","8e2f2b9f":"markdown","f3a7adb2":"markdown","995d825e":"markdown","66833129":"markdown"},"source":{"acca51c5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score,KFold\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.linear_model import SGDRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\n\ntrain = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')\ntrain_test = pd.concat([train,test], sort = False).reset_index(drop=True)","652a1a0c":"display(train.head())","f93194ac":"display(test.head())","78cb6b8a":"# separate quantitative and categorical\nquanti_cols = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', \n               '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath','BsmtHalfBath','FullBath', \n               'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', \n               'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', \n               'PoolArea', 'MiscVal']\ncat_cols = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', \n            'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'OverallQual', \n            'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n            'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', \n            'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n            'Functional', 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond', \n            'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'MoSold', 'YrSold', 'SaleType',  'SaleCondition']\n\n# further grouping\nstrcat_cols = [col for col in cat_cols if train_test[col].dtype == 'object']\nnumcat_cols = set(cat_cols) - set(strcat_cols)\n\n# for later encoding purpose\nordinal_strcat_cols = ['MSZoning','Street','Alley','LotShape','LandContour','Utilities', 'BldgType', 'HouseStyle', 'ExterQual', \n                       'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', \n                       'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', \n                       'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence',]\n\nordinal = ordinal_strcat_cols + list(set(cat_cols) - set(strcat_cols))\nnominal = list(set(cat_cols) - set(ordinal))","67a0362e":"# display\nprint(\"continuous variables:\")\nprint(\"\\n\".join(quanti_cols))\nprint(\"ordinal categorical variables:\")\nprint(\"\\n\".join(ordinal))\nprint(\"nominal categorical variables:\")\nprint(\"\\n\".join(nominal))","928b8df9":"# create function to understand the nans\ndef Nan_ref(df, cat_cols, quanti_cols):\n    row_num, col_num = df.shape[0], df.shape[1]\n    \n    nakey = df.columns[df.isna().any()].tolist()\n    nanum = list([i for i in df.isna().sum() if i>0])\n    nadict = dict(zip(nakey, nanum))\n    cat_nadict = dict()\n    quanti_nadict = dict()\n    for key, value in nadict.items():\n        if key in cat_cols:\n            cat_nadict.update({key: value})\n        elif key in quanti_cols:\n            quanti_nadict.update({key:value})\n        \n    \n    nacols_percentage = str(round(len(nakey)\/col_num*100, 2))+'%'\n    \n    print(\"percentage of columns with nulls:\")\n    print(nacols_percentage)\n    print(\"\\n\")\n    print(\"number of nulls in each column -\")\n    print(\"categorical columns:\")\n    print(cat_nadict)\n    print(\"\\n\")\n    print(\"continuous columns:\")\n    print(quanti_nadict)\n    \n    return cat_nadict, quanti_nadict","1f9bd13d":"Nan_ref(train_test, cat_cols, quanti_cols)","ba71a1ed":"for col in train_test[quanti_cols]:\n    plt.hist(train_test[col])\n    plt.title(col)\n    plt.show()","f0936a02":"for col in cat_cols:\n    sns.barplot(x = train_test[col].value_counts().index, y=train_test[col].value_counts())\n    plt.title(col)\n    plt.show()\n    \nto_bin = ['GarageYrBlt']\ntidy_catcols = set(cat_cols) - set(to_bin)","3f1d65d4":"X_all = train_test.drop(['Id', 'SalePrice'], axis=1)\ny = train.SalePrice","cbefc25d":"mode_impute = ['MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd', 'Electrical', 'KitchenQual', 'Functional', 'SaleType']\nfor col in mode_impute:\n    X_all.loc[:,col] = SimpleImputer(strategy='most_frequent').fit_transform(X_all[[col]])","02df240b":"X_all['Alley'] = X_all['Alley'].fillna('no_alley')\nX_all['MasVnrType'] = X_all['MasVnrType'].fillna('no_mansory')\n\nno_basement = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\nfor col in no_basement:\n    X_all[col] = X_all[col].fillna('no_basement')\n    \nX_all['FireplaceQu'] = X_all['FireplaceQu'].fillna('no_fireplace')\n\nno_garage = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\nfor col in no_garage:\n    X_all[col] = X_all[col].fillna('no_garage')\n    \nX_all['PoolQC'] = X_all['PoolQC'].fillna('no_pool')\nX_all['Fence'] = X_all['Fence'].fillna('no_fence')\nX_all['MiscFeature'] = X_all['MiscFeature'].fillna('no_miscfeature')\n\n\nquanti_nadict = Nan_ref(train_test, cat_cols, quanti_cols)[1]\nfor key, value in quanti_nadict.items():\n    if key != 'LotFrontage':\n        X_all[key] = X_all[key].fillna(0)","778ad0e6":"sns.boxplot(x = X_all.LotFrontage)","99cbf55f":"tempdf = X_all[strcat_cols].agg(LabelEncoder().fit_transform)\ntempdf = pd.concat([tempdf, X_all['LotFrontage']])\nplt.figure(figsize=(45, 45))\nmask = np.triu(np.ones_like(tempdf.corr()))\nsns.heatmap(tempdf.corr(), vmin=-1, vmax=1, mask=mask, annot=True, cmap='BrBG')","84b3b97d":"plt.figure(figsize=(25, 25))\nmask = np.triu(np.ones_like(X_all.corr()))\nsns.heatmap(X_all.corr(), vmin=-1, vmax=1, mask=mask, annot=True, cmap='BrBG')","c1b829f3":"X_all['LotFrontage'] = X_all.groupby(['BldgType']).LotFrontage.transform(lambda group: group.fillna(group.median()))","84f0cf99":"X_all.loc[X_all.GarageYrBlt==2207, 'GarageYrBlt'] = 2007\n\ngarage_sequence = [1894, 1928, 1961, 1971, 1980, 1991, 2003, 2007, 2011]\ngarage_labels = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight']\n\nX_all['garage_bins'] = pd.cut(X_all.GarageYrBlt, garage_sequence, labels=garage_labels).values.add_categories('no_garage')\nX_all['garage_bins'] = X_all['garage_bins'].fillna('no_garage')\n\nX_all.drop(columns = to_bin, inplace = True, axis = 1)\n\nstrcat_cols.extend(['garage_bins'])\nordinal_strcat_cols.extend(['garage_bins'])","1cbfa974":"low_cardinality_cols = [col for col in strcat_cols if X_all[col].nunique() < 10]\noh_cols = set(low_cardinality_cols) - set(ordinal_strcat_cols)\nord_cols = list(set(strcat_cols) - set(oh_cols))\n\noh_df = pd.DataFrame()\nfor col in oh_cols:\n    oh = pd.get_dummies(X_all[col], prefix=col) \n    oh_df = pd.concat([oh_df, oh], axis=1)\n\nord_encoder = OrdinalEncoder()\nord_df = pd.DataFrame()\nord_df[ord_cols] = X_all[ord_cols]\nord_df[ord_cols] = ord_encoder.fit_transform(X_all[ord_cols])\nX_all = pd.concat([X_all.drop(strcat_cols, axis=1), oh_df, ord_df], axis=1)","88b71bdc":"X_all['OverallScore'] = X_all['OverallQual']+X_all['OverallCond']\nX_all['YearBltRemod'] = X_all['YearBuilt']+X_all['YearRemodAdd']\nX_all['Baths'] = X_all['BsmtFullBath'] + 0.5*X_all['BsmtHalfBath']+X_all['FullBath']+X_all['HalfBath']\nX_all['totalArea'] = X_all['TotalBsmtSF']+X_all['1stFlrSF']+X_all['2ndFlrSF']\nX_all['totalFinishedArea'] = X_all['BsmtFinSF1']+X_all['BsmtFinSF2']+X_all['1stFlrSF']+X_all['2ndFlrSF']\nX_all['totalPorchArea'] = X_all['OpenPorchSF']+X_all['EnclosedPorch']+X_all['3SsnPorch']+X_all['ScreenPorch']","62b7e715":"X_all['wBasement'] = X_all['TotalBsmtSF'].apply(lambda SF: 1 if SF>0 else 0)\nX_all['w2ndFloor'] = X_all['2ndFlrSF'].apply(lambda SF: 1 if SF>0 else 0)\nX_all['wGarge'] = X_all['GarageArea'].apply(lambda SF: 1 if SF>0 else 0)\nX_all['wFireplace'] = X_all['Fireplaces'].apply(lambda num: 1 if num > 0 else 0)\n\ndf = X_all[['OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch']]\nwoPorchIdx = df.index[(df == 0).all(axis=1)]\nX_all['wPorch'] = np.nan\nX_all.loc[woPorchIdx, 'wPorch'] = 0\nX_all['wPorch'] = X_all['wPorch'].fillna(1)","332bdc6f":"skewness_df = X_all[quanti_cols].apply(lambda x: skew(x)).sort_values(ascending=False)\n# all below 5 except for 5 columns\nhighly_skewed_cols = skewness_df[skewness_df > 0.5].index\n\nfor col in highly_skewed_cols:\n    X_all[col] = boxcox1p(X_all[col], boxcox_normmax(X_all[col] + 1))","beda2799":"X_all = X_all.round(2)\nX_train, X_test = X_all.iloc[:1460], X_all.iloc[1460:]","a46da1dc":"quanti_saleprice = pd.concat([train['SalePrice'],train[quanti_cols]], axis=1)\nplt.figure(figsize=(25,25))\nmask = np.triu(np.ones_like(quanti_saleprice.corr()))\nsns.heatmap(quanti_saleprice.corr(), vmin=-1, vmax=1, mask=mask, annot=True, cmap='BrBG')","37469398":"dfs = [X_train[X_train.columns[:31]],X_train[X_train.columns[31:62]],X_train[X_train.columns[62:93]],\n       X_train[X_train.columns[93:124]],X_train[X_train.columns[124:]]]\n\nfor df in dfs:\n    encodedtrain_Xy = pd.concat([df, y], axis=1)\n    plt.figure(figsize=(30,30))\n    mask = np.triu(np.ones_like(encodedtrain_Xy.corr()))\n    sns.heatmap(encodedtrain_Xy.corr(), vmin=-1, vmax=1, mask=mask, annot=True, cmap='BrBG')","74bd5c1f":"kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\nlr = LinearRegression(n_jobs=-1)\n\n#  models default try best param\nridge = make_pipeline(RobustScaler(),\n                      RidgeCV(alphas=[14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5], cv=kfolds))\nsgd = SGDRegressor()\nxgb = XGBRegressor(learning_rate=0.01, n_estimators=3460,max_depth=3, min_child_weight=0,gamma=0, subsample=0.7,colsample_bytree=0.7,\n                   objective='reg:squarederror', nthread=-1,scale_pos_weight=1, seed=27,reg_alpha=0.00006)\ngbr = GradientBoostingRegressor()\nlgbm = LGBMRegressor(objective='regression',num_leaves=4,learning_rate=0.01,n_estimators=5000,max_bin=200,bagging_fraction=0.75,\n                     bagging_freq=5,bagging_seed=7,feature_fraction=0.2,feature_fraction_seed=7,verbose=-1)\n\nmodels_done = [ridge,xgb,lgbm]\nmodels_done_str = ['ridge','xgb','lgbm']\n\ndef model_absrmse(model):\n    model.fit(X_train, y)\n    y_predicted = model.predict(X_test)\n    rmse = mean_squared_error(y[:-1], y_predicted, squared=False)  # False already sqrt\n    return rmse\n\ndef model_crossval_absrmse(model):\n    rmse = np.sqrt(-cross_val_score(model, X_train, y,scoring=\"neg_mean_squared_error\",cv=kfolds).mean())\n    return rmse\n\ndef tune_model_params(model, params):\n    # also use crossval\n    gridsearch = GridSearchCV(model, params, scoring=\"neg_mean_squared_error\", n_jobs=-1, cv=kfolds,error_score=\"raise\")\n    gridsearch.fit(X_train, y)\n    best_params, best_score = gridsearch.best_params_, round(np.sqrt(-gridsearch.best_score_*100), 2)\n    return best_params, best_score","56781221":"lrparams = {'fit_intercept':[True,False],\n           'normalize':[True,False]}\nsgdparams={\"loss\": ['huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n           \"penalty\":[\"l1\", \"l2\",'elasticnet'],\n           'alpha':[0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007],\n           'fit_intercept':[True,False],\n           'max_iter':[50000]}\ngbrparams={\"learning_rate\": [0.01, 0.02, 0.05],\n           \"max_depth\": [4, 6, 8],\n           \"max_features\": ['auto', 'sqrt', 'log2'], \n           \"min_samples_split\": [ 2, 3, 4],\n           \"random_state\":[42]}\n\nmodels_totune = [lr,sgd,gbr]\nmodels_totune_str = ['lr','sgd','gbr']\nmodels_params = [lrparams,sgdparams,gbrparams]","76878804":"#best_params_and_scores = list(map(tune_model_params, models_totune, models_params))\n#best_params_and_scores_df = pd.DataFrame(best_params_and_scores, columns=['best_params', 'best_score'], index=models_totune_str)\n\n#pd.set_option(\"max_colwidth\", None)\n#best_params_and_scores_df.sort_values(by='best_score')\n\ngbr = GradientBoostingRegressor(learning_rate=0.05, max_depth=8,max_features='sqrt',min_samples_split=4,random_state=42)\nlr = LinearRegression(fit_intercept=False, normalize=True, n_jobs=-1)\nsgd = SGDRegressor(alpha=0.0006, fit_intercept=True, loss='epsilon_insensitive', max_iter=50000, penalty='l1')\n# gbr >>>>lr>sgd","4c0c275d":"#best_scores = list(map(model_crossval_absrmse, final_models))\n#best_scores_df = pd.DataFrame(best_scores,columns=['best_score'], index=final_models_str)\n\n#pd.set_option(\"max_colwidth\", None)\n#best_scores_df.sort_values(by='best_score')\n\n# sgd >> gbr~>ridge > lgbm~>xgb~>lr\n# cv: xgb~>gbr >lgbm > ridge> lr >>sgd\nfinal_models = [ridge,xgb,lgbm,gbr,lr,sgd]","e09b14d8":"stacking = StackingCVRegressor(regressors=final_models,meta_regressor=gbr, use_features_in_secondary=True)\n#model_crossval_absrmse(stacking)","73169d15":"model = stacking.fit(X_train, y)\ntest_predicted = model.predict(X_test)\ntest['SalePrice'] = list(map(round, test_predicted))\n\nsubmission = pd.DataFrame(test[['Id', 'SalePrice']])\nsubmission.to_csv('submission',index=False)","b3b577c9":"For categorical variables, barplots are used. We can see that the *GarageYrBlt* column is very messy. Therefore, we will bin this column. Similarly, click expand to see the specific visualizations of each column.","9c5fa438":"# 4.1.1. Categorical variables <a id=\"9\"><\/a>\nUsing *SimpleIputer()* with *\"most_frequent\"* as strategy allows us to impute the mode. ","7eb20e26":"*Preview of Test Data:*","0a2b7e18":"# 4. Cleaning  <a id=\"7\"><\/a>\n1. Impute nulls in all columns  \n2. Bin columns needed\n3. Encode categorical variables","1fb9d438":"# 4.1.3. Continuous variables <a id=\"11\"><\/a>\n*LotFrontage* will be imputted. First we will use boxplots to see if it is skewed.","c5c08606":"# 4.1.2. Impute as no_(facility) or 0 <a id=\"10\"><\/a>","610c0d11":"# 3.3 Overview of values <a id=\"6\"><\/a>\nFor continuous variables, histograms are used to visualize the distribution. We can see that some columns are heavily skewed, such as *LotArea* and *MasVnrArea*. We may need to normalize them later. Click expand to see the specific visualizations of each column.","b87c0cb0":"# 6. Prediction <a id=\"18\"><\/a>\nWe will first have a brief look into the correlations between the features and the target. And then proceed to  \n6.1. Optimising models' parameters and select the best models   \n\n6.2. Combining the models","5a08d570":"# 5.1. Combine <a id=\"15\"><\/a>","4391c068":"# 6.1. Select models and optimise parameters<a id=\"19\"><\/a> \nmodels are selceted based on cross validation scores of absolute root-mean-square deviations.","9367723a":"As the preprocessing is done, we can now separate the train and test data again.","9f6ddacd":"# 4.2. Bin <a id=\"12\"><\/a>\nwe will bin the *GarageYrBlt* column into 8 values and make a new column *garage_bin*. The old column would then be dropped.","3defd912":"# 3. Understand the variables <a id=\"3\"><\/a>\nFull description of each column is in data_description.txt. In addition to it, we also need to understand a few things about the variables before beginning preprocessing:\n1. Data types:  \n\nFor categorical variables (data type is string), encoding is needed.  \n\n2. Nulls:  \n\nDepending on the meaning of the varibles, we may need to imput values for null variables  \n\n3. Overview of values of columns:  \n\nWe will use visualizations to understand the distribution of values for different variables.","138d2f71":"After finding out the best ensemble method, we can submit our predictions.","2ee5f150":"# 6.2. Combine Models<a id=\"20\"><\/a> \nStacking ensemble method is used.","c67dec16":"# 4.3. Encode <a id=\"13\"><\/a>\nwe will use different encoding methods based on their cardinalities and properties.  \n1. OneHot encoder: for low cardinality columns  \n\n2. Ordinal encoder: for ordinal columns","980ac948":"If you want to see how the variables are grouped, simply click on the 'expand' button.","6f394d53":"# 5. Feature Engineering <a id=\"14\"><\/a>\n5.1. Combine related continous columns  \n\n5.2. Simpify some columns\n\n5.3. Scale abnormal columns we found earlier","e6f3f48c":"Optimise the remaining models:","e32dc0b9":"# 5.3. Scale abnormal columns <a id=\"17\"><\/a>","173191b4":"Now let's get a preview of the data:","7d6584aa":"Since it is skewed, median will be used. To ensure less bias, we will group *LotFrontage* with a column which highly correlates with it, and then impute median within the groups.  ","2639a6f5":"*Preview of Train Data:*","ff9c95a3":"# 5.2. Simplify <a id=\"16\"><\/a>","e723df90":"It is found through heatmap that *BldgType* (-0.48) is the column with highest correlation. We will then impute accordingly.","8e2f2b9f":"# 3.1 Data Types <a id=\"4\"><\/a>\nWe first separate continuous and categorical variables. As there are numerical categorical variables, the separating process would not be completely automated with data types. In brief, they are classified by:  \n1. continuous varibles   \n2. categorical variables,  \nIn which nominal and ordinal categorical variables are further grouped.  \n\nIf you want to see the code, simply click on the 'expand' button.","f3a7adb2":"# 1. Objectives <a id=\"1\"><\/a>\nIn this competition, 79 variables describing different aspects of residential homes in Ames, Iowa are provided. We will use them to predict the final price of each home.\n\n# 2. Import Packages and Get Data <a id=\"2\"><\/a>\nAfter importing the modules, We read the train and test data from csv files. They are concatenated for easier preprocessing. When all preprocessing is done, the test data will be sparated again for prediction.","995d825e":"# 4.1 Impute Nulls  <a id=\"8\"><\/a>\nWe will first classify how we want to impute each column.  \n  \n4.1.1. Categorical variables - mode \n\n4.1.2. Impute as no_(facility) or 0:  \nThis will be applied to categorical or continuous variables where a null means that the house does not have specific facility, such as a garage or basement.\n\n4.1.3. Continous variables - median\/ mean\nWe will use *median* for skewed columns with high number of outliers, and *mean* for normal distribution columns.  \n\nBefore imputting the nulls, we will first separate features (X) and the target *SalePrice* (y).","66833129":"# 3.2 Nulls <a id=\"5\"><\/a>\nA function is used to provide an overview of the nulls."}}