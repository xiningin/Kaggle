{"cell_type":{"6c58d403":"code","aa4ea6bf":"code","6f8b6d52":"code","71845217":"code","217d159e":"code","ea543251":"code","c37bab52":"code","1c668c14":"code","3e78d761":"code","a302fcd1":"code","5000cd42":"code","e589759f":"code","b5796470":"code","ff86d6ad":"code","0b1b04a0":"code","2e8826fe":"code","db45cdb0":"code","653b6bde":"code","bf544cf5":"code","85b9a64b":"code","a7f28efc":"code","8a8b07be":"code","5329b561":"code","3f3cd158":"code","7b56ffb5":"code","619c2363":"code","7f683f3f":"code","569f989e":"code","7a9acdf4":"code","ba344cda":"code","f86a742d":"code","cf67a1ff":"code","6ebc01e5":"code","e58f1f48":"code","bd88246c":"code","d58c47d0":"code","ef22f77d":"code","6fd720fb":"code","5eef39ed":"code","a1bdba48":"code","a154441f":"code","62f46e6f":"markdown","6cac71d9":"markdown","3aeb483c":"markdown","31482340":"markdown","cc75faf2":"markdown","d6044f9a":"markdown","ff729188":"markdown","f8623d0d":"markdown","78604696":"markdown","a499f8e6":"markdown","503dae3c":"markdown","e0f33c0a":"markdown","ec807679":"markdown","a38c34cc":"markdown","42b84f56":"markdown","31d3559a":"markdown","833385e8":"markdown","6124af48":"markdown","c6f92290":"markdown","c0f42c57":"markdown","897956c3":"markdown"},"source":{"6c58d403":"# import the tools \nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud","aa4ea6bf":"# Read the train and test file \ntrain_df = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv', encoding='ISO-8859-1', parse_dates=['TweetAt'])\ntest_df = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv', encoding='ISO-8859-1',parse_dates=['TweetAt'])","6f8b6d52":"# Check missing data\ntrain_df.isnull().sum(), train_df.isnull().sum()","71845217":"train_df.head()","217d159e":"test_df.head()","ea543251":"train_df.shape, test_df.shape","c37bab52":"# we combined the train_df and test_df into one dataframe for preprocessing\n\n# Create new column to identify the test data\ntrain_df['is_test'] = 0\ntest_df['is_test'] = 1\n\n# combine \ncomp_df = pd.concat([train_df, test_df])\ncomp_df.reset_index(drop=True, inplace=True)","1c668c14":"# Have a look on the target features\ncomp_df.Sentiment.value_counts()","3e78d761":"comp_df['Sentiment'] = comp_df.Sentiment.str.replace('Extremely Positive', 'Positive')\ncomp_df['Sentiment'] = comp_df.Sentiment.str.replace('Extremely Negative', 'Negative')","a302fcd1":"comp_df.Sentiment.value_counts().plot.bar(figsize=(7,4))\nplt.xticks(rotation=None)\nplt.title('Number of tweets in different sentiments',fontsize=12)\nplt.xlabel('Number of tweets', fontsize=12)\nplt.ylabel('Sentiment')","5000cd42":"# Map the sentiment into 0 , 1, 2\ncomp_df['Sentiment'] = comp_df['Sentiment'].map({'Positive':2, 'Negative':0, 'Neutral':1})","e589759f":"comp_df['month'] = comp_df['TweetAt'].dt.month","b5796470":"# Visual the date with the labels\npd.crosstab(comp_df.month, comp_df.Sentiment).plot.bar()\nplt.ylabel('Number of tweets')\nplt.xticks(rotation=None)\nplt.show()","ff86d6ad":"# In this task we will focus on the text data only, so we drop the other columns\ncomp_df = comp_df[['OriginalTweet','Sentiment','is_test']]","0b1b04a0":"comp_df['OriginalTweet'][0]","2e8826fe":"# Change columns name for easy access\ncomp_df.columns =['tweet','label','is_test']","db45cdb0":"#Remove @ tags\ncomp_df.tweet = comp_df.tweet.str.replace(r'(@\\w*)','')\n\n#Remove URL\ncomp_df.tweet = comp_df.tweet.str.replace(r\"http\\S+\", \"\")\n\n#Remove # tag\ncomp_df.tweet = comp_df.tweet.str.replace(r'#\\w+',\"\")\n\n#Remove all non-character\ncomp_df.tweet = comp_df.tweet.str.replace(r\"[^a-zA-Z ]\",\"\")\n\n# Remove extra space\ncomp_df.tweet = comp_df.tweet.str.replace(r'( +)',\" \")\ncomp_df.tweet = comp_df.tweet.str.strip()\n\n# Change to lowercase\ncomp_df.tweet = comp_df.tweet.str.lower()","653b6bde":"comp_df.tweet[60]","bf544cf5":"# Create new columns for storing\ncomp_df['corpus'] = [nltk.word_tokenize(text) for text in comp_df.tweet]\nlemma = nltk.WordNetLemmatizer()\ncomp_df.corpus = comp_df.apply(lambda x: [lemma.lemmatize(word) for word in x.corpus], axis=1)\ncomp_df.corpus = comp_df.apply(lambda x: \" \".join(x.corpus),axis=1)","85b9a64b":"stop_words = stopwords.words('english')","a7f28efc":"text = comp_df.corpus.values\nwordcloud = WordCloud(max_words=500,background_color='white', stopwords=stop_words, colormap='rainbow',height=300)\nwordcloud.generate(str(text))","8a8b07be":"fig = plt.figure()\nfig.set_figheight(6)\nfig.set_figwidth(10)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","5329b561":"# Import the tools we need\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import classification_report","3f3cd158":"# Split the data back to train and test set\ntrain_df = comp_df[comp_df.is_test==0]\ntest_df = comp_df[comp_df.is_test==1]\ntrain_df.drop('is_test',axis=1, inplace=True)\ntest_df.drop('is_test',axis=1, inplace=True)\ntest_df.reset_index(drop=True,inplace=True)","7b56ffb5":"#Split the data in X and y dataset\n\nx_df = train_df.corpus\ny_df = train_df['label']\n\nx_test = test_df.corpus\ny_test =test_df['label']\n\n# Split to train and validation\nx_train, x_val, y_train, y_val = train_test_split(x_df,y_df, test_size=0.2,random_state=42)","619c2363":"# Check the shape\nx_train.shape, x_val.shape, y_train.shape, y_val.shape","7f683f3f":"# Create the vectorizer\nvectorizer = CountVectorizer(stop_words='english',ngram_range=(1,2),min_df=5).fit(comp_df.corpus)\n\n# transform both train and valid data\nx_train_vector = vectorizer.transform(x_train)\nx_val_vector = vectorizer.transform(x_val)","569f989e":"cross_val_score(LogisticRegression(random_state=42), x_train_vector, y_train, cv=10, verbose=1, n_jobs=-1).mean()","7a9acdf4":"cross_val_score(MultinomialNB(alpha=0.01), x_train_vector, y_train, cv=10, verbose=1, n_jobs=-1).mean()","ba344cda":"model = LogisticRegression(random_state=42).fit(x_train_vector, y_train)\nprint(classification_report(y_val, model.predict(x_val_vector)))","f86a742d":"#params = {\n    #'solver':['liblinear','saga','newton-cg','lbfgs'],\n   # 'C':[0.001,0.01,0.1,1,10,100],\n    # 'penalty':['l1','l2']\n}\n\n#lr_grid = GridSearchCV(LogisticRegression(random_state=42),params, cv=5, verbose=2, n_jobs=-1)\n#lr_grid.fit(x_train_vector, y_train)\n\n#print(classification_report(y_val, lr_grid.predict(x_val_vector)))","cf67a1ff":"vectorizer = TfidfVectorizer(min_df=5, ngram_range=(1,2),stop_words='english').fit(comp_df.corpus)\n\nx_train_tf = vectorizer.transform(x_train)\nx_val_tf = vectorizer.transform(x_val)","6ebc01e5":"# Show the top 20 words \nfeature_weight = x_train_tf.sum(axis=0).tolist()[0]\nfeatures = pd.DataFrame(feature_weight)\nfeatures.index =  list(vectorizer.get_feature_names())\nfeatures.sort_values(by=[0],ascending=False).head(30).plot.barh(figsize=(20,10))\nplt.xlabel('Weight')","e58f1f48":"model = LogisticRegression(random_state=42).fit(x_train_tf,y_train)\nprint(classification_report(y_val, model.predict(x_val_tf)))","bd88246c":"#params = {\n    #'solver':['liblinear','saga','newton-cg','lbfgs'],\n    #'C':[0.001,0.01,0.1,1,10,100],\n    #'penalty':['l1','l2']\n}\n\n#lr_grid02 = GridSearchCV(LogisticRegression(random_state=42),params, cv=10, verbose=2, n_jobs=-1)\n#lr_grid02.fit(x_train_tf, y_train)\n\n#print(classification_report(y_val, lr_grid02.predict(x_val_tf)))","d58c47d0":"#lr_grid.best_estimator_","ef22f77d":"#To skip the training time on kaggle, I use the best parameter found in my notebook directly\nbest_model = LogisticRegression(C=1, penalty='l1', random_state=42, solver='saga')\nbest_model.fit(x_train_tf, y_train)","6fd720fb":"# The best model performance on validation dataset\nprint(classification_report(y_val, best_model.predict(x_val_tf)))","5eef39ed":"x_test_tf = vectorizer.transform(x_test)","a1bdba48":"y_pred = best_model.predict(x_test_tf)\nprint(classification_report(y_test, y_pred))","a154441f":"from sklearn.metrics import confusion_matrix\nplt.figure(figsize=(6,4))\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d',annot_kws={'size':17}, cmap='Reds')\nplt.ylabel('True')\nplt.xlabel('Predicted')","62f46e6f":"### Visualize the text data using wordcloud","6cac71d9":"### Modify the tweet contents","3aeb483c":"# Using CountVectorizer","31482340":"### Grouping the labels to positive(2), negative(0) and neutral (1)","cc75faf2":"# Coronavirus tweets NLP - Text Classification\n\n- (2020\/12) I am a self-taught learner of data science and finished my NLP online course. Try to apply what i have learnt to this project.\n\n### Corona Virus Tagged Data\n\nData from:https:\/\/www.kaggle.com\/datatattle\/covid-19-nlp-text-classification\n\n\nPerform Text Classification on the data. The tweets have been pulled from Twitter and manual tagging has been done then.\nThe names and usernames have been given codes to avoid any privacy concerns.\n\n\nColumns in Data:\n- Location\n- Tweet At\n- Original Tweet\n- Label","d6044f9a":"### Now do prediction on the test data","ff729188":"### The performance of logistric regression is better, now try to tune the hyperparameters.","f8623d0d":"### Get the month of the tweets","78604696":"### Drop the other columns","a499f8e6":"# Use tf-idf as vectorizer","503dae3c":"# Start modeling","e0f33c0a":"### Model training\n- logistric regression","ec807679":"## Thank you very much","a38c34cc":"#### Have a look on the  tweets's content","42b84f56":"### The elements we would like to remove from the tweet's content\n\n- URL\n- punctuations\n- \\# tags\n- @ tags\n- extra space","31d3559a":"# Data EDA and formatting","833385e8":"### Hyperparameters tuning using gridsearch","6124af48":"### Heat map of the prediction","c6f92290":"# Prediction on test data","c0f42c57":"### Start training models\n- Logistric Regression\n- Naive Bayes","897956c3":"### Tokenize and Lemmatize the word in data"}}