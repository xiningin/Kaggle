{"cell_type":{"a915fbd6":"code","a2e078d9":"code","a5959840":"code","0a677186":"code","b0ad96f7":"code","d74f4907":"code","4a2a3e4c":"code","65eadb4a":"code","986bc722":"code","1ce12449":"code","f2cc741b":"code","ceac942d":"code","37f22524":"code","db5b3065":"code","b73f2c10":"code","c88f7342":"code","1d115206":"code","9fde86ba":"code","35291a28":"code","624be57b":"code","de7c1d28":"code","cd8ee9b5":"code","76edfa42":"code","2c72aa02":"code","179e342a":"code","d9f66f93":"code","07320efc":"code","fb2aa1f4":"code","40ff3bd6":"code","9b3d9082":"code","ba614f63":"code","3fd57f0d":"code","795b3745":"code","f9820e19":"code","0504b4a8":"code","003c1bb5":"code","8f8e585b":"code","f1b77314":"code","c60ac684":"code","ba43b72b":"code","273ff920":"code","2dce0747":"code","9cfeaa4d":"code","1c5ebeb0":"code","eec2e422":"code","7452d99b":"code","1ec57556":"code","8e8350c0":"code","dc24acd5":"code","6f109b29":"code","737771fc":"code","fd1d1469":"markdown","6d8d7401":"markdown","1f245d25":"markdown","581284f5":"markdown","2932497b":"markdown","e966e4c0":"markdown","5fb28f32":"markdown","69e70b9e":"markdown","14eff88d":"markdown","732c5490":"markdown"},"source":{"a915fbd6":"#First we import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","a2e078d9":"#Second step is import our data \npd.read_csv(\"..\/input\/salaries\/Position_Salaries.csv\")\n#If we import it from our computer, we have to write pathway of the file with the name and file extension of the file","a5959840":"\ndf=pd.read_csv(\"..\/input\/salaries\/Position_Salaries.csv\")\n#Here I create a variable names df ","0a677186":"df#this is my data","b0ad96f7":"#Because our data is ready without any missing values, we can simply separate between features and target column\nX=df.iloc[:,1:-1].values #Here I select \"level\" column and assign my target column\n","d74f4907":"y=df.iloc[:,-1:].values #here I assign Salaries as my target column","4a2a3e4c":"X.shape #This is the size of my X= features","65eadb4a":"y.shape #This is the size of my y or target column","986bc722":"X","1ce12449":"y","f2cc741b":"from sklearn.tree import DecisionTreeRegressor #We import Regression decision tree algorithm","ceac942d":"dtree=DecisionTreeRegressor() # here we create an instance of the algorithm","37f22524":"dtree.fit(X,y) # Here we make the algorithm to fit the data","db5b3065":"dtree.predict([[1]]) # Here is a prediction of the algorithm if feature is equal to 1","b73f2c10":"dtree.predict([[6.5]])  #Here is a prediction of the algorithm if feature is equal to 6.5","c88f7342":"df # here we can compare results with the real data","1d115206":"sns.set_style(\"darkgrid\")\nplt.figure(figsize=(20,10)) #here I create an empty figure \nplt.scatter(X,y,color=\"green\") #here I create a scatterplot inside this empty figure\nplt.plot(list(range(10)), dtree.predict(X), color=\"blue\")#Here I create  another plot for the predictions of the algorithm\nplt.xlabel(\"Positions\")#Here I label x- coordinate as Positions\nplt.ylabel(\"Salaries\")#Here I label y- coordinate as Positions\n#Because we have used the same data and %100 of the data for both fitting and predicting, there is overfitting","9fde86ba":"np.arange(0,9,0.01) \n#here we create an array between 0 and 9 by stepping 0.01 in order to use new features for predictions that the algorith does not know","35291a28":"sns.set_style(\"darkgrid\")\nplt.figure(figsize=(20,10))\ngrid=np.arange(0,10,0.01) #here we create an array between 0 and 9 by stepping 0.01\ngrid=grid.reshape(len(grid),1)\nplt.scatter(X,y,color=\"red\")\nplt.plot(grid, dtree.predict(grid),color=\"blue\")#here I create a plot of all numbers inside array between 0 and 9\nplt.xlabel(\"Positions of Workes\")\nplt.ylabel(\"Salary\")\nplt.show()\n","624be57b":"from sklearn import tree","de7c1d28":"print(tree.export_text(dtree))\n#here I get actual textual visualization of how the algorithm has separated the features into different leaves in order predict salaries","cd8ee9b5":"df","76edfa42":"fig=plt.figure(figsize=(20,10))\n_=tree.plot_tree(dtree,\n            feature_names=df[\"Level\"],\n           class_names=df[\"Salary\"],\n           filled=True)\n#here I get actual visualization of how the algorithm has separated the features into different leaves in order predict salaries","2c72aa02":"df","179e342a":"df1=pd.read_csv(\"..\/input\/kyphosis\/kyphosis.csv\") #here I import my data","d9f66f93":"df1.info()#here I get overall information about the data","07320efc":"\nsns.pairplot(data=df1, hue=\"Kyphosis\")\n#here I get a mutual visualization of all the features","fb2aa1f4":"X=df1[[\"Age\",\"Number\",\"Start\"]] #here I assign three columns as features \nX\n     ","40ff3bd6":"y=df1[\"Kyphosis\"] #here I assign Kyphosis column as the target\ny","9b3d9082":"from sklearn.model_selection import train_test_split #Here I import function from sklearn.model_selection to split data","ba614f63":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2)\n#First comes features(X), later target(y),later test_size. \n#Here I assign %20 of data as test size by writing test_size=0.2, %80 will automatically will be the train size","3fd57f0d":"X.shape #This is the size of X, all features before separation as train and test subsets","795b3745":"X_test.shape#This is the size of X test set, features in the test data subset","f9820e19":"X_train.shape #This is the size of X train set, features in the train data subset","0504b4a8":"y_train.shape #This is the size of y train set, target column in the train data subset","003c1bb5":"y_test.shape #This is the size of y test set, target column in the test data subset","8f8e585b":"from sklearn.tree import DecisionTreeClassifier \n#I call decision tree classifier because my target column has categorical values as absent and presebt, not continuous","f1b77314":"dtree=DecisionTreeClassifier() # here we create an instance of the algorithm","c60ac684":"dtree.fit(X_train,y_train) # here the algorithm fits the train data and creates best model to predict y,target values","ba43b72b":"predictions=dtree.predict(X_test) # here the algorithm makes predictions for real y_test data\n#we have to only write here X_test data, y_test data because we do not make the algorithm know what the real values is","273ff920":"from sklearn.metrics import confusion_matrix, classification_report\n#we import confusion_matrix and classification_report for performance measurement","2dce0747":"print(confusion_matrix(y_test,predictions))\n#The intersection between first row and column represents True Positives,in our case true number of present kyphosis cases the algorithm predicted truely\n#The intersection between second row and first column represents True Negatives, absence of kyphosis cases the algorithm predicted truely\n#The intersection between first row and second column represents False Negatives,the algorithm predicted as absence of kyphosus, but it is labeled present in the real data\n#The intersection between first row and second column represents True Negatives,the algorithm predicted as absence of kyphosus,which also match with real data","9cfeaa4d":"print(classification_report(y_test,predictions)) \n#This is classification report of the predictions of the algorithm compared to the real data","1c5ebeb0":"#Below we visualize how we determine the precision and and recall\nplt.figure(figsize=(20,10))\nplt.imshow(plt.imread(\"..\/input\/precision\/picture.png\"))\n\n","eec2e422":"# The F1 score is the harmonic mean of the precision and recall as we can visualize below:\nplt.figure(figsize=(20,10))\nplt.imshow(plt.imread(\"..\/input\/fscore\/fscore.jpg\"))","7452d99b":"y_test.shape","1ec57556":"print(tree.export_text(dtree)) #here we show the text form of how the algortihm has created decisions trees from the root node","8e8350c0":"df1","dc24acd5":"fig=plt.figure(figsize=(20,10))\n_=tree.plot_tree(dtree,\n            feature_names=df1[\"Age\"],\n           class_names=df1[\"Kyphosis\"],\n           filled=True)\n#here we show the visual form of how the algortihm has created decisions trees from the root node","6f109b29":"X_train.shape","737771fc":"#Kind Regards ","fd1d1469":"<font color=\"blue\">\nA Decision Tree is a supervised algorithm used in machine learning. It is using a binary tree graph (each node has two children) to assign for each data sample a target value. The target values are presented in the tree leaves. To reach to the leaf, the sample is propagated through nodes, starting at the root node. In each node a decision is made, to which descendant node it should go. A decision is made based on the selected sample\u2019s feature. Decision Tree learning is a process of finding the optimal rules in each internal tree node according to the selected metric.\n ","6d8d7401":"Step 3: Splitting Data as Train and Test Set","1f245d25":"<font color=\"blue\">\nThere are two types of Decision Tree Models as Classification Tree and Regression Tree.\n\nClassification trees used to classify samples, assign to a limited set of values - classes. In scikit-learn it is DecisionTreeClassifier.\n    \nRegression trees used to assign samples into numerical values within the range. In scikit-learn it is DecisionTreeRegressor.","581284f5":"# 2. Classsification Problems with Decision Tree Model","2932497b":"Step 5: Evaluation the Performance of the Algorithm","e966e4c0":"Step 2: Separating Data as Features and Target ","5fb28f32":"Confusion matrix) is a table with two rows and two columns that reports the number of false positives, false negatives, true positives, and true negatives. ","69e70b9e":"# 1. Regression Problems with Decision Tree Model","14eff88d":"Step 4: Implementing Algorithm","732c5490":"Step 1: Importing and Exploring the Data"}}