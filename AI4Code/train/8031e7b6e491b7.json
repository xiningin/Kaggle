{"cell_type":{"3d69f1fd":"code","bfded030":"code","a542c515":"code","856e4560":"code","6737545b":"code","0b6eb05f":"code","343b2852":"code","dcbc0fed":"code","45382e3d":"code","422519f8":"code","48d32ec2":"code","a7798b78":"code","2c9fe9d5":"code","1d0ac002":"code","e6da7580":"code","52723fdb":"code","cf1cc68a":"code","1c3ebbbf":"code","0e482b9c":"code","ace65e64":"code","aebb5d30":"code","26f5ca21":"code","42ec959c":"markdown","7a1b76df":"markdown","d80b03b8":"markdown","3f2f3dbf":"markdown","0cfb68ec":"markdown","f023c1d9":"markdown"},"source":{"3d69f1fd":"!ls ..\/input\/sarcasm\/","bfded030":"# some necessary imports\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nfrom matplotlib import pyplot as plt","a542c515":"train_df = pd.read_csv('..\/input\/sarcasm\/train-balanced-sarcasm.csv')","856e4560":"train_df.head()","6737545b":"train_df.info()","0b6eb05f":"train_df.dropna(subset=['comment'], inplace=True)","343b2852":"train_df['label'].value_counts()","dcbc0fed":"train_texts, valid_texts, y_train, y_valid = \\\n        train_test_split(train_df['comment'], train_df['label'], random_state=17)","45382e3d":"train_texts.shape, valid_texts.shape, y_train.shape, y_valid.shape","422519f8":"from wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train_texts, title=\"Word Cloud of Questions\")","48d32ec2":"df_0 = train_df[train_df['label'] == 0]['comment']\ndf_1 = train_df[train_df['label'] == 1]['comment']","a7798b78":"import plotly.graph_objects as go\n\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[0].values,\n        x=df[1].values,\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace","2c9fe9d5":"from collections import defaultdict\n\nfreq_dict = defaultdict(int)\nfor sent in df_0:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n\nfd_sorted.columns = [0, 1]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')","1d0ac002":"freq_dict = defaultdict(int)\nfor sent in df_1:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [0, 1]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'orange')","e6da7580":"from plotly.subplots import make_subplots\n\n# Creating two subplots\nfig = make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent bigrams of sincere questions\", \n                                          \"Frequent bigrams of insincere questions\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\nfig.show()","52723fdb":"tfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n            stop_words = 'english')","cf1cc68a":"tfv.fit(list(train_texts) + list(valid_texts))","1c3ebbbf":"train = tfv.transform(train_texts)","0e482b9c":"test = tfv.transform(valid_texts)","ace65e64":"logit = LogisticRegression(C=1.0)\nlogit.fit(train, y_train)","aebb5d30":"pred = logit.predict(test)\naccuracy_score(y_valid, pred)","26f5ca21":"import eli5\neli5.show_weights(logit, vec=tfv, top=25, feature_filter=lambda x: x != '<BIAS>')","42ec959c":"We notice that the dataset is indeed balanced","7a1b76df":"Some comments are missing, so we drop the corresponding rows.","d80b03b8":"We split data into training and validation parts.","3f2f3dbf":"## Tasks:\n1. Analyze the dataset, make some plots. This [Kernel](https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc) might serve as an example\n2. Build a Tf-Idf + logistic regression pipeline to predict sarcasm (`label`) based on the text of a comment on Reddit (`comment`).\n3. Plot the words\/bigrams which a most predictive of sarcasm (you can use [eli5](https:\/\/github.com\/TeamHG-Memex\/eli5) for that)\n4. (optionally) add subreddits as new features to improve model performance. Apply here the Bag of Words approach, i.e. treat each subreddit as a new feature.\n\n## Links:\n  - Machine learning library [Scikit-learn](https:\/\/scikit-learn.org\/stable\/index.html) (a.k.a. sklearn)\n  - Kernels on [logistic regression](https:\/\/www.kaggle.com\/kashnitsky\/topic-4-linear-models-part-2-classification) and its applications to [text classification](https:\/\/www.kaggle.com\/kashnitsky\/topic-4-linear-models-part-4-more-of-logit), also a [Kernel](https:\/\/www.kaggle.com\/kashnitsky\/topic-6-feature-engineering-and-feature-selection) on feature engineering and feature selection\n  - [Kaggle Kernel](https:\/\/www.kaggle.com\/abhishek\/approaching-almost-any-nlp-problem-on-kaggle) \"Approaching (Almost) Any NLP Problem on Kaggle\"\n  - [ELI5](https:\/\/github.com\/TeamHG-Memex\/eli5) to explain model predictions","0cfb68ec":"## <center> Assignment 4. Sarcasm detection with logistic regression\n    \nWe'll be using the dataset from the [paper](https:\/\/arxiv.org\/abs\/1704.05579) \"A Large Self-Annotated Corpus for Sarcasm\" with >1mln comments from Reddit, labeled as either sarcastic or not. A processed version can be found on Kaggle in a form of a [Kaggle Dataset](https:\/\/www.kaggle.com\/danofer\/sarcasm).\n\nSarcasm detection is easy. \n<img src=\"https:\/\/habrastorage.org\/webt\/1f\/0d\/ta\/1f0dtavsd14ncf17gbsy1cvoga4.jpeg\" \/>","f023c1d9":"<center>\n<img src=\"https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg\">\n    \n## [mlcourse.ai](https:\/\/mlcourse.ai) \u2013 Open Machine Learning Course \nAuthor: [Yury Kashnitskiy](https:\/\/yorko.github.io) (@yorko). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/) license. Free use is permitted for any non-commercial purpose."}}