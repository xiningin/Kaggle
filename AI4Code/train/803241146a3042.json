{"cell_type":{"2ccfee3a":"code","24494a96":"code","6d211676":"code","039d9dfe":"code","31d69f37":"code","f5f9b4ae":"code","9ea34def":"code","bf69ed16":"code","93a7cd91":"code","2a5ab88d":"code","48ce2599":"code","ccd840fd":"code","fef25124":"code","06814758":"code","8721b7c3":"code","e18663d6":"code","8554e58a":"code","2cbc24fa":"code","4273ae25":"code","ed7eb60e":"code","e939d652":"code","6156406b":"code","1db63f8e":"code","9819d5d1":"code","b1099b16":"code","377f08b9":"code","de1917a0":"code","dd1962d1":"code","a70a22fa":"code","95fcc414":"code","3ee1da59":"code","b02fd80e":"code","87c28061":"code","9a0e0d2d":"code","59028faa":"code","52e01ec8":"code","9313d728":"code","f6ef8516":"code","532fffbc":"code","fcff1725":"code","07da2310":"code","db53e819":"code","590c7644":"code","d97fb1f5":"code","e72e6d70":"code","5e1ffe24":"code","ec1d9614":"code","c5e79e42":"code","c2ea72f1":"code","ac2ab46e":"code","0fedabb6":"code","48340aca":"code","408b5239":"code","df7592a7":"code","347d017f":"code","c2ab6c0a":"code","349439e2":"code","2b0df861":"code","ad60014e":"code","98b05549":"code","c89a619c":"markdown","31951edf":"markdown","6e3d7276":"markdown","a7d4fa73":"markdown","a755521a":"markdown","d814f7a0":"markdown","ad453c1f":"markdown","4c5e4688":"markdown","38161c3e":"markdown","5caae96e":"markdown","2da0731e":"markdown","6949694e":"markdown","8cfdc5c6":"markdown","6ae6f664":"markdown","998056eb":"markdown"},"source":{"2ccfee3a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","24494a96":"import matplotlib.pyplot as plt\nimport seaborn as sns \n","6d211676":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nsubmission=pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\ntrain.head()","039d9dfe":"print(train.shape)\nprint('-'*50)\nprint(test.shape)","31d69f37":"train.info()\nprint('-'*50)\ntest.info()\n","f5f9b4ae":"train.isna().sum()\n","9ea34def":"test.isna().sum()","bf69ed16":"train.describe()","93a7cd91":"train.describe(include=['O'])","2a5ab88d":"train.head()","48ce2599":"train['Pclass'].value_counts()","ccd840fd":"pd.pivot_table(data=train,index='Pclass',values='Survived',aggfunc=np.mean)","fef25124":"g=sns.FacetGrid(train,col='Survived',row='Pclass')\ng.map(plt.hist,'Age',bins=20)","06814758":"train['Sex'].value_counts()","8721b7c3":"pd.pivot_table(data=train,index='Sex',values='Survived',aggfunc=np.mean)","e18663d6":"g=sns.FacetGrid(train,col='Survived')\ng.map(sns.countplot,'Sex')","8554e58a":"pd.pivot_table(index=['Embarked','Sex','Pclass'],data=train,values='Survived',aggfunc=np.mean)","2cbc24fa":"grid = sns.FacetGrid(train, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","4273ae25":"train=train.drop(columns=['Ticket','Cabin'])\n","ed7eb60e":"test=test.drop(columns=['Ticket','Cabin'])","e939d652":"combine=[train,test]","6156406b":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])","1db63f8e":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \npd.pivot_table(index='Title',values='Survived',data=train,aggfunc=np.mean).sort_values(by='Survived',ascending=False)","9819d5d1":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain.head()","b1099b16":"train.drop(columns=['Name','PassengerId'],inplace=True)","377f08b9":"test.drop('Name',inplace=True,axis=1)","de1917a0":"combine=[train,test]","dd1962d1":"sex_dict= {'female': 1, 'male': 0}\nfor dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map(sex_dict ).astype(int)","a70a22fa":"train","95fcc414":"guess_ages = np.zeros((2,3))\nguess_ages\nfor dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\n","3ee1da59":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\n","b02fd80e":"combine = [train, test]","87c28061":"for dataset in combine:\n    dataset['Family']=dataset['SibSp']+dataset['Parch']+1\n    ","9a0e0d2d":"pd.pivot_table(index='Family',data = train , values='Survived',aggfunc=np.mean).sort_values(by='Family',ascending=True)","59028faa":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['Family'] == 1, 'IsAlone'] = 1\n\n\n","52e01ec8":"train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","9313d728":"train = train.drop(['Parch', 'SibSp', 'Family'], axis=1)\ntest = test.drop(['Parch', 'SibSp', 'Family'], axis=1)\ncombine = [train, test]","f6ef8516":"train.head()","532fffbc":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(train.Embarked.dropna().mode()[0])","fcff1725":"train['Embarked'].value_counts()","07da2310":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain.head()","db53e819":"test.isna().sum()","590c7644":"test['Fare']=test['Fare'].dropna()","d97fb1f5":"test[test['Fare'].isna()]","e72e6d70":"test.drop(152,axis=0,inplace=True)","5e1ffe24":"test.isna().sum()","ec1d9614":"train ","c5e79e42":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\ncombine = [train, test]\n    \ntrain.head()","c2ea72f1":"test.head()","ac2ab46e":"X=train.drop('Survived',axis=1)\ny=train['Survived']\n","0fedabb6":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split","48340aca":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=20)","408b5239":"# Import necessary modules\nfrom scipy.stats import randint\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": randint(1, 9),\n              \"min_samples_leaf\": randint(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# Instantiate a Decision Tree classifier: tree\ntree = DecisionTreeClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\ntree_cv = RandomizedSearchCV(tree,param_dist, cv=5)\n\n# Fit it to the data\ntree_cv.fit(X_train,y_train)\n\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))\n\nacc_decision_tree=tree_cv.best_score_","df7592a7":"param_dist = {'n_estimators':np.arange(1,200)}\n\n# Instantiate a Decision Tree classifier: tree\nrandom_forest = RandomForestClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\nrandom_forest_cv = RandomizedSearchCV(random_forest,param_dist, cv=5)\n\n# Fit it to the data\nrandom_forest_cv.fit(X_train,y_train)\n\n# Print the tuned parameters and score\nprint(\"Tuned Random Forest Parameters: {}\".format(random_forest_cv.best_params_))\nprint(\"Best score is {}\".format(random_forest_cv.best_score_))\nacc_random_forest=random_forest_cv.best_score_","347d017f":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_test, y_test) , 2)\nacc_linear_svc","c2ab6c0a":"knn = KNeighborsClassifier(n_neighbors = 11)\nknn.fit(X_train, y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_test, y_test) , 2)\nacc_knn","349439e2":"# Logistic regression\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.preprocessing import StandardScaler\n\nlogregpipe = Pipeline([('scale', StandardScaler()),\n                   ('logreg',LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\"))])\n\n# Gridsearch to determine the value of C\nparam_grid = {'logreg__C':np.arange(0.01,100,10)}\nlogreg_cv = GridSearchCV(logregpipe,param_grid,cv=5,return_train_score=True)\nlogreg_cv.fit(X_train,y_train)\nprint(logreg_cv.best_params_)\n\nbestlogreg = logreg_cv.best_estimator_\nbestlogreg.fit(X_train,y_train)\nbestlogreg.coef_ = bestlogreg.named_steps['logreg'].coef_\nacc_log=bestlogreg.score(X_test,y_test)","2b0df861":"svc = SVC()\nsvc.fit(X_train, y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_test,y_test) , 2)\nacc_svc","ad60014e":"from sklearn.ensemble import RandomForestClassifier\n\ny = train[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\"]\nX = pd.get_dummies(train[features])\nX_test = pd.get_dummies(test[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n","98b05549":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest' ,'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest ,acc_linear_svc,  acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","c89a619c":"# Convert Categorical data to numeric data","31951edf":"# Machine Learning","6e3d7276":"Prepare data to model","a7d4fa73":"Here we combine 2 feature into one feature called family and 1 is refers to the passenger .","a755521a":"Here in train data : we have 177 missing value in age feature and 687 missing value in cabin feature  and 2 missing values in embarked .\n in test data  we have 86 missing value in age feature and 1 missing value in fare and 372 missing value in cabin feature.\n ","d814f7a0":"Here we describe the non numerical data that it shows that we have two types of sex male or female and the most of passengers are male , and most passenger had S embarked.\n","ad453c1f":"We show that about 38 % of our passingers as survived and 72% are dead ","4c5e4688":"When you see the name feature , you see that it mention to social status title like Mr , Miss ... etc . So we will extract these titles .","38161c3e":"# Explore Train and Test data","5caae96e":"This graph visualize that class 3 was the worst class which it had the most un survived people .\nFemales have good percantage of survived passengers and Embarked C was the best embarked.","2da0731e":"In the tickit 3 , It was the most class had people which it had 491 passenger and it had the least average of survived passengers , and the class 1 had the most average of the survived people","6949694e":"The types of data  showed when we use info method . When we used info method we saw that  (Passanger id ,Supervised  , pclass, age ,SibSp,Parch,Fare ) and the rest is the non numerical data.","8cfdc5c6":"# Data Analysis","6ae6f664":"**What is Numerical data and None Numerical data ? **\n\n","998056eb":"Here there are differnce between test and train dataset in one columne ,which this column is the label which it will predicted."}}