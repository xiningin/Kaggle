{"cell_type":{"17cc6ba4":"code","b33918ce":"code","877f097d":"code","7a28a586":"code","d84eb7e4":"code","f56f7d4f":"code","153e812e":"code","808ae287":"code","7a3bff8d":"code","6cb302bf":"code","9e10df8e":"code","20c86777":"code","5f23674b":"code","3a258937":"code","3036a03e":"code","9e049bbe":"code","1cd84450":"code","bcf356ae":"code","af1ee915":"code","c0769edb":"code","9fb347b0":"code","09729947":"code","cbf79898":"code","6495c028":"code","2566cd97":"code","2dca918b":"code","6a496fe6":"code","55194e48":"code","1c3e142a":"code","d33bc0fe":"code","18bcd598":"code","2f9fe91e":"code","9f378d49":"code","e3fc6e9a":"code","367d2a3e":"code","f41269a7":"code","5c7f813b":"code","35f152a8":"code","7609f065":"code","71fdfa6a":"code","81bec911":"code","b472af5a":"code","6f32c4ae":"code","d691e138":"code","35df226a":"code","0c1e7152":"code","fb7c8861":"code","435185e0":"code","0117e4dc":"code","2a7a80f8":"code","dd1b6fbc":"code","9bbb3e15":"code","3dd11e82":"code","03b6970a":"code","f12c8de0":"code","a75a5f06":"code","b31a59b0":"code","8921524a":"code","6571e169":"code","7e953f27":"code","71340b46":"code","3482cbe8":"code","e0977b2f":"code","69b70e0e":"code","587ef3eb":"code","dc377ad9":"code","2c567ea9":"code","a45a56ef":"code","17473a6c":"code","fd9dfa63":"code","af2e7ea2":"code","31fc8a7a":"code","6a1b2b9d":"code","e084264a":"code","2b7875f7":"code","aec646c6":"code","10546c51":"code","97667196":"code","7e6c275b":"code","ff058095":"code","4ecdd33e":"code","df275674":"code","0af1e478":"code","98b8b214":"code","540cf56f":"code","8597af2a":"code","daf550f6":"code","5e82483d":"code","b058fe8a":"code","fcd7885a":"code","3f61076c":"code","c117d8eb":"code","18d34cba":"code","744ead5b":"code","ab4261b7":"code","03dd5876":"code","22662ab8":"code","a077ccac":"code","554f1d33":"code","0fd0fb73":"code","f886cdec":"code","2d168e9c":"code","c13590d9":"code","8349f019":"code","ba7bc34d":"code","cbedfb05":"code","9f4e5f9d":"code","5223a9a7":"code","eadc41f0":"markdown","d359108e":"markdown","3c23ad8f":"markdown","899f5f08":"markdown","cef48093":"markdown","fed90005":"markdown","47de3482":"markdown","2005f317":"markdown","8f09626d":"markdown","68dfecaf":"markdown","9e60a3d6":"markdown","9400bdd6":"markdown","9a82d396":"markdown","ef93d4dd":"markdown","66e23dac":"markdown","d9e39365":"markdown","afe47ab8":"markdown","c8f99f0d":"markdown","d9ac9548":"markdown","e8955882":"markdown","8ef2ed42":"markdown","7446f91e":"markdown","290cb11b":"markdown","6e9df71d":"markdown","7c3662ea":"markdown","c319a103":"markdown","fd38ed10":"markdown","eaafd183":"markdown","8ce0859e":"markdown","5202f5cf":"markdown","18445529":"markdown","4a5de1b1":"markdown","c3caa03b":"markdown","abf06197":"markdown","bce7bba5":"markdown","c3880fc1":"markdown","d850e91d":"markdown","bee4e5e3":"markdown","f34b97f9":"markdown","adf535c7":"markdown","36b9c380":"markdown","060ce98b":"markdown","71b0f364":"markdown","faea077a":"markdown","95deaf32":"markdown","fe684028":"markdown","050352c2":"markdown","bfc3c33c":"markdown","83f2745e":"markdown","83e8eeb1":"markdown","c07e60df":"markdown","b1649d60":"markdown","ea8588d9":"markdown","a3aa9257":"markdown","ba5407d6":"markdown","3effa609":"markdown","21e014bc":"markdown","ed1459e6":"markdown","2d0b0665":"markdown","b641cfed":"markdown","fc97f936":"markdown","9710070f":"markdown","58fc27f5":"markdown","6d08e16c":"markdown","26920a07":"markdown","be88a481":"markdown","c1882e6e":"markdown","93f2306d":"markdown","235fc06a":"markdown","53cbb5e4":"markdown","34bd4fa0":"markdown","c8f9fe42":"markdown","b8a4efcc":"markdown","803301a9":"markdown","b3be9f44":"markdown","572a7916":"markdown","e2eb66c6":"markdown","fd8d47ec":"markdown","e403174c":"markdown","d0d1a072":"markdown","be40d1f6":"markdown","287aac2b":"markdown","259a4a4d":"markdown","e14b52a4":"markdown","cf489a94":"markdown","eba7f525":"markdown","4aeebd4d":"markdown","e59db301":"markdown","812154a1":"markdown","dc588b59":"markdown","2c2e0d10":"markdown","896cc6c6":"markdown","ef1bca58":"markdown","88c3cf2a":"markdown","937f26a6":"markdown","142bde6e":"markdown","cf73c769":"markdown","dccce7f3":"markdown","01bfe8dd":"markdown","940a9812":"markdown","c367b996":"markdown"},"source":{"17cc6ba4":"!pip3 install datapane","b33918ce":"import os\nimport warnings\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport seaborn as sns\nimport datapane as dp\nimport sklearn\nimport scipy\nimport scipy.stats as stats\nimport scikitplot as skplt\nimport missingno as msno\nimport plotly.graph_objects as go\n%matplotlib inline\n\nfrom sklearn import base\nfrom collections import defaultdict\nfrom matplotlib.ticker import FixedLocator, FixedFormatter\nfrom joblib import dump, load\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.utils.fixes import loguniform\nfrom sklearn.model_selection import train_test_split, GridSearchCV, \\\nRandomizedSearchCV, cross_val_score, RepeatedStratifiedKFold, KFold\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom kmodes.kprototypes import KPrototypes\nfrom sklearn import metrics\nfrom sklearn.metrics import silhouette_samples, silhouette_score","877f097d":"!datapane login --token=ea0597866d4a1147ca57892a8536b4b7ffc47bcf","7a28a586":"# set random seed for reproducibility\nRANDOM_SEED = 2021\nos.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)","d84eb7e4":"# to better display long string\npd.set_option('display.max_colwidth', None)","f56f7d4f":"df = pd.read_csv('\/kaggle\/input\/invehicle-coupon-recommendation\/in-vehicle-coupon-recommendation.csv')","153e812e":"# Ultimate Data Processing Function\ndef data_cleaning(data=df):\n    clean_df = df.copy()\n    \n    clean_df.drop(columns=['car'], inplace=True)\n    na_columns = ['Bar', 'CoffeeHouse', 'CarryAway', 'RestaurantLessThan20', 'Restaurant20To50']\n    clean_df.drop(columns=['toCoupon_GEQ5min'], inplace=True)\n    frequency_map = {\n        'never': 0,\n        'less1': 1,\n        '1~3': 2,\n        '4~8': 3,\n        'gt8': 4}\n    age_map = {\n        'below21': 0,\n        '21': 1,\n        '26': 2,\n        '31': 3,\n        '36': 4,\n        '41': 5,\n        '46': 6,\n        '50plus': 7}\n    income_map = {\n        'Less than $12500': 0,\n        '$12500 - $24999': 1,\n        '$25000 - $37499': 2,\n        '$37500 - $49999': 3,\n        '$50000 - $62499': 4,\n        '$62500 - $74999': 5,\n        '$75000 - $87499': 6,\n        '$87500 - $99999': 7,\n        '$100000 or More': 8}\n    frequency_cols = ['Restaurant20To50', 'RestaurantLessThan20', \n                      'CarryAway', 'CoffeeHouse', 'Bar']\n    for col in frequency_cols:\n        clean_df[col] = clean_df[col].map(frequency_map)\n    clean_df.age = clean_df.age.map(age_map)\n    clean_df.income = clean_df.income.map(income_map)\n    clean_df.drop(columns=['direction_opp'], inplace=True)\n    clean_df['distance'] = None\n    clean_df.loc[clean_df['toCoupon_GEQ15min'] == 0, 'distance'] = 0\n    clean_df.loc[(clean_df['toCoupon_GEQ15min'] == 1) & \\\n                 (clean_df['toCoupon_GEQ25min'] == 0), 'distance'] = 1\n    clean_df.loc[clean_df['toCoupon_GEQ25min'] == 1, 'distance'] = 2\n    clean_df.distance = clean_df.distance.astype('int64')\n    clean_df.drop(columns=['toCoupon_GEQ15min', 'toCoupon_GEQ25min'], inplace=True)\n    clean_df.has_children = clean_df.has_children.astype(str)\n    clean_df.direction_same = clean_df.direction_same.astype(str)\n    return clean_df","808ae287":"# # use data_clean then jump right to the model training part\n# clean_df = data_cleaning(df)\n# clean_df.shape","7a3bff8d":"df.head()","6cb302bf":"df.info()","9e10df8e":"print(df.car.unique())","20c86777":"df.drop(columns=['car'], inplace=True)","5f23674b":"df.isna().sum()","3a258937":"na_columns = ['Bar', 'CoffeeHouse', 'CarryAway', 'RestaurantLessThan20', 'Restaurant20To50']\nna_df = df[na_columns]\nmsno.matrix(na_df, figsize=(10, 8))\nplt.show()","3036a03e":"msno.heatmap(na_df, figsize=(10, 8));","9e049bbe":"df.describe()","1cd84450":"df.select_dtypes('int64').nunique()","bcf356ae":"df.drop(columns=['toCoupon_GEQ5min'], inplace=True)","af1ee915":"df.select_dtypes('object').nunique()","c0769edb":"for i in df.select_dtypes('object').columns:\n    print(i, df[i].unique())","9fb347b0":"clean_df = df.copy()","09729947":"fig, axes = plt.subplots(9, 2, figsize=(20,50))\naxes = axes.flatten()\n\nfor ax, col in zip(axes, clean_df.select_dtypes('object').columns):\n    sns.countplot(y=col, data=clean_df, ax=ax, \n                  palette=\"ch:.25\", order=clean_df[col].value_counts().index);\n\nplt.tight_layout()\nplt.show()","cbf79898":"frequency_map = {\n    'never': 0,\n    'less1': 1,\n    '1~3': 2,\n    '4~8': 3,\n    'gt8': 4\n}\nage_map = {\n    'below21': 0,\n    '21': 1,\n    '26': 2,\n    '31': 3,\n    '36': 4,\n    '41': 5,\n    '46': 6,\n    '50plus': 7\n}\nincome_map = {\n    'Less than $12500': 0,\n    '$12500 - $24999': 1,\n    '$25000 - $37499': 2,\n    '$37500 - $49999': 3,\n    '$50000 - $62499': 4,\n    '$62500 - $74999': 5,\n    '$75000 - $87499': 6,\n    '$87500 - $99999': 7,\n    '$100000 or More': 8\n}","6495c028":"frequency_cols = ['Restaurant20To50', 'RestaurantLessThan20', \n                  'CarryAway', 'CoffeeHouse', 'Bar']","2566cd97":"for col in frequency_cols:\n    clean_df[col] = clean_df[col].map(frequency_map)\nclean_df.age = clean_df.age.map(age_map)\nclean_df.income = clean_df.income.map(income_map)","2dca918b":"fig, axes = plt.subplots(3, 3, figsize=(20, 15))\naxes = axes.flatten()\n\nfor ax, col in zip(axes, ['destination', 'passanger', 'weather', 'time', \n                          'coupon', 'expiration', 'gender', 'maritalStatus',\n                          'education']):\n    sns.countplot(y=col, hue='Y', data=clean_df, ax=ax, palette='ch:.25')\nplt.tight_layout()\nplt.show()","6a496fe6":"plt.subplots(figsize=(15, 20))\nsns.countplot(y ='occupation', hue='Y', data=clean_df, palette='ch:.25');\nplt.show()","55194e48":"cmap = sns.color_palette('vlag', as_cmap=True)\nsns.heatmap(clean_df.select_dtypes('int64', 'float64').corr(), cmap=cmap)\nplt.show()","1c3e142a":"clean_df.drop(columns=['direction_opp'], inplace=True)","d33bc0fe":"clean_df.hist(figsize=(20, 15))\nplt.show()","18bcd598":"clean_df['distance'] = None\nclean_df.loc[clean_df['toCoupon_GEQ15min'] == 0, 'distance'] = 0\nclean_df.loc[(clean_df['toCoupon_GEQ15min'] == 1) & \\\n             (clean_df['toCoupon_GEQ25min'] == 0), 'distance'] = 1\nclean_df.loc[clean_df['toCoupon_GEQ25min'] == 1, 'distance'] = 2\nclean_df.distance.isna().sum()","2f9fe91e":"clean_df.distance = clean_df.distance.astype('int64')\nclean_df.distance.unique()","9f378d49":"clean_df.distance.value_counts()","e3fc6e9a":"clean_df.drop(columns=['toCoupon_GEQ15min', 'toCoupon_GEQ25min'], inplace=True)","367d2a3e":"clean_df.info()","f41269a7":"clean_df.has_children = clean_df.has_children.astype(str)\nclean_df.direction_same = clean_df.direction_same.astype(str)","5c7f813b":"clean_df.Y.value_counts()","35f152a8":"fig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes = axes.flatten()\n\nfor ax, col in zip(axes, ['distance', 'temperature']):\n    sns.countplot(x=col, hue='Y', data=clean_df, \n                  ax=ax, palette=\"ch:.25\");\n\nplt.tight_layout()\nplt.show()","7609f065":"sns.pairplot(clean_df, hue='Y', diag_kind='hist')\nplt.show()","71fdfa6a":"clean_df.shape","81bec911":"# Save the cleaned data set\nclean_df.to_csv('clean_df.csv', index=False)","b472af5a":"# Notice the values of have_children and direction_same are 0 and 1\n# Thus, if not specified, the pd.read_csv() method will infer them as integer\n# clean_df = pd.read_csv('\/kaggle\/working\/clean_df.csv', \n#                        dtype={'has_children': str,\n#                              'direction_same': str})","6f32c4ae":"X = clean_df.drop(columns=['Y'])\ny = clean_df.Y\nX_train, X_test, y_train, y_test = \\\ntrain_test_split(X, y, random_state=RANDOM_SEED, test_size=0.2)","d691e138":"X_train_a = X_train.copy()\nX_test_a = X_test.copy()","35df226a":"strong_predictors = ['destination', 'passanger', 'weather', 'time', 'coupon',\n            'expiration', 'maritalStatus', 'education',\n            'occupation', 'direction_same']\nfor col in strong_predictors:\n    # create frequency encoder\n    freq_encoder = X_train_a.groupby(col).size() \/ len(X_train_a)\n    # fit_transform for X_train\n    X_train_a[col + '_freq'] = X_train_a[col].apply(lambda x: freq_encoder[x])\n    # transform for X_test\n    X_test_a[col + '_freq'] = X_test_a[col].apply(lambda x: freq_encoder[x])","0c1e7152":"class KFoldTargetEncoderTrain(base.BaseEstimator, base.TransformerMixin):\n    \"\"\"\n    This object contains a target encoder for a training set which should have\n    both X and y. \n    \n    Arguments\n    ---------\n    feature:          string. Name of the feature in the training set.\n    target:           string. Name of the target in the training set.\n    n_fold:           default 5. Number of folds to use in KFold.\n    verbose:          bool, default True. If set to True, the correlation between the \n                      feature and the target will be calculated and printed out.\n    discard_original: bool,, default False. If set to True, the feature column will be \n                      deleted from the training set.\n                      \n    Example\n    ---------\n    train_target_encoder = KFoldTargetEncoderTrain(feature='A', target='target')\n    new_train = train_target_encoder.fit_transform(train)\n    \"\"\"\n    def __init__(self, feature, target, n_fold=5, verbose=True, discard_original=False):\n\n        self.feature = feature\n        self.target = target\n        self.n_fold = n_fold\n        self.verbose = verbose\n        self.discard_original = discard_original\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self,X):\n        \"\"\"\n        Transform the original training set. Notice this function can only encode \n        one feature once.\n        \n        Arguments:\n        ----------\n        X: A pandas DataFrame which should include both the feature and the target.\n        \n        Output:\n        X: A pandas DataFrame with the target encoding.\n        \"\"\"\n        \n        # notice this function can only encode one feature at a time\n        assert(type(self.feature) == str)\n        assert(type(self.target) == str)\n        assert(self.feature in X.columns)\n        assert(self.target in X.columns)\n\n        mean_of_target = X[self.target].mean()\n        kf = KFold(n_splits = self.n_fold, shuffle = True, random_state=RANDOM_SEED)\n        # create the target encoding\n        col_mean_name = self.feature + '_target'\n        X[col_mean_name] = np.nan\n\n        for train_index, val_index in kf.split(X):\n            X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n            X.loc[X.index[val_index], col_mean_name] = \\\n            X_val[self.feature].map(X_train.groupby(self.feature)[self.target].mean())\n        # missing value imputation\n        X[col_mean_name].fillna(mean_of_target, inplace = True)\n\n        if self.verbose:\n            encoded_feature = X[col_mean_name].values\n            print('Correlation between {} and, {} is {}.'.\\\n                  format(col_mean_name, self.target,\n                         np.corrcoef(X[self.target].values, encoded_feature)[0][1]))\n        # discard orginal feature column if needed\n        if self.discard_original:\n            X = X.drop(self.target, axis=1)\n\n        return X","fb7c8861":"class KFoldTargetEncoderTest(base.BaseEstimator, base.TransformerMixin):\n    \"\"\"\n    This object contains a target encoder for a testing set which should have\n    both X and y.\n    \n    Arguments\n    ---------\n    train:          pandas DataFrame. The training DataFrame with the feature and \n                    the target encoded column of it.\n    feature:        string. The column name of the feature.\n    feature_target: string. The column name of the feature_target that \n                    has been calculated in the training set.\n                    \n    Example\n    ---------\n    test_target_encoder = KFoldTargetEncoderTest(new_train, 'A', 'A_target')\n    new_test = test_target_encoder.transform(test)\n    \"\"\"\n    \n    def __init__(self, train, feature, feature_target):\n        \n        self.train = train\n        self.feature = feature\n        self.feature_target = feature_target\n        \n        \n    def fit(self, X, y=None):\n        return self\n\n    def transform(self,X):\n        \"\"\"\n        Transform the testing set based on K-fold target encoder of the training set.\n        Notice this function can only encode one feature at a time.\n        \n        Argument\n        --------\n        X: pandas DataFrame. The testing set to be transformed.\n        \n        Output\n        --------\n        X: A pandas DataFrame with transformed target encoding.\n        \"\"\"\n\n        mean = self.train[[self.feature,self.feature_target]].groupby(self.feature).mean().reset_index() \n        \n        dd = {}\n        for index, row in mean.iterrows():\n            dd[row[self.feature]] = row[self.feature_target]\n\n        \n        X[self.feature_target] = X[self.feature]\n        X = X.replace({self.feature_target: dd})\n\n        return X","435185e0":"train_df = pd.concat([X_train_a, y_train], axis=1)\nnew_train = train_df.copy()\nfor feature in strong_predictors:\n    train_target_encoder = KFoldTargetEncoderTrain(feature, 'Y')\n    new_train = train_target_encoder.fit_transform(new_train)","0117e4dc":"test_df = pd.concat([X_test_a, y_test], axis=1)\nnew_test = test_df.copy()\nstrong_predictors_targets = []\nfor feature in strong_predictors:\n    strong_predictors_targets.append(feature + '_target')\nfor feature, feature_target in zip(strong_predictors, strong_predictors_targets):\n    test_target_encoder = KFoldTargetEncoderTest(new_train, feature, feature_target)\n    new_test = test_target_encoder.transform(new_test)","2a7a80f8":"new_train.drop(columns=strong_predictors, inplace=True)\nnew_test.drop(columns=strong_predictors, inplace=True)","dd1b6fbc":"X_train_a = new_train.drop(columns=['Y'])\nX_test_a = new_test.drop(columns=['Y'])","9bbb3e15":"X_train_a.info()","3dd11e82":"num_features_a = X_train_a.select_dtypes(['int64', 'float64']).columns\ncat_features_a = X_train_a.select_dtypes(['object']).columns\nnum_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('scaler', StandardScaler())\n])\ncat_transformer = OneHotEncoder()\npreprocessor_a = ColumnTransformer(transformers=[\n    ('num', num_transformer, num_features_a),\n    ('cat', cat_transformer, cat_features_a)\n])\nX_train_a = preprocessor_a.fit_transform(X_train_a)\nX_test_a = preprocessor_a.transform(X_test_a)","03b6970a":"X_train_a.shape, X_test_a.shape","f12c8de0":"X_train_b = X_train.copy()\nX_test_b = X_test.copy()","a75a5f06":"num_features_b = X_train_b.select_dtypes(['int64', 'float64']).columns\ncat_features_b = X_train_b.select_dtypes(['object']).columns\n\npreprocessor_b = ColumnTransformer(transformers=[\n    ('num', num_transformer, num_features_b),\n    ('cat', cat_transformer, cat_features_b)\n])\nX_train_b = preprocessor_b.fit_transform(X_train_b)\nX_test_b = preprocessor_b.transform(X_test_b)","b31a59b0":"X_train_b.shape, X_test_b.shape","8921524a":"logreg_clf = LogisticRegression(solver='saga', max_iter=500,\n                               random_state=RANDOM_SEED)\ndt_clf = DecisionTreeClassifier(random_state=RANDOM_SEED)\nbnb_clf = BernoulliNB()\nknn_clf = KNeighborsClassifier()\nlsvm_clf = LinearSVC(max_iter=5000, dual=False)","6571e169":"logreg_params = dict(C=loguniform(1e-1, 1e2),\n                     penalty=['l1', 'l2'])\ndt_params = dict(criterion=['gini', 'entropy'],\n                 min_samples_split=[2, 4, 6, 8, 10],\n                 max_depth=[2, 4, 6, 8, 10])\nbnb_params = dict(alpha=loguniform(1e-1, 1e0))\nknn_params = dict(n_neighbors=[2, 4, 6, 8, 10, 12, 14, 20],\n               weights=['uniform', 'distance'],\n               metric=['euclidean', 'manhattan'])\nlsvm_params = dict(C=loguniform(1e-1, 1e2))","7e953f27":"clf_list = [logreg_clf, dt_clf, bnb_clf, knn_clf, lsvm_clf]\nparams_list = [logreg_params, dt_params, bnb_params, knn_params, lsvm_params]\nmodel_names = ['Logistic Regression', 'Decison Tree', 'Bernoulli Naive Bayes',\n               'KNN Classifier', 'Linear SVM']","71340b46":"def train_model(X, y, model_suffix, clf_list=clf_list, params_list=params_list):\n    for i in range(len(clf_list)):\n        # model training with RandomizedSearchCV\n        rscv = RandomizedSearchCV(estimator=clf_list[i],\n                                  param_distributions=params_list[i],\n                                  n_jobs=-1, random_state=RANDOM_SEED).fit(X, y)\n        # store cv results\n        globals()['rscv%s' % model_suffix[i]] = pd.DataFrame(rscv.cv_results_)\n        # store the best model\n        globals()['best%s' % model_suffix[i]] = rscv","3482cbe8":"def record_best_result(model_list, model_suffix):\n    # store the best results into a dataframe\n    for i in range(len(model_list)):\n        globals()['df%s' % model_suffix[i]] = model_list[i].query('rank_test_score == 1')\\\n        [['params', 'mean_test_score', 'std_test_score']]","e0977b2f":"def model_eval(clf_list, model_names, X_test, y_test):\n    test_acc = []\n    f1_score = []\n    for clf in clf_list:\n        test_acc.append(clf.score(X_test, y_test))\n        f1_score.append(metrics.f1_score(y_test, clf.predict(X_test)))\n    return pd.DataFrame(data={'model': model_names, 'test_acc': test_acc, 'f1_score': f1_score})","69b70e0e":"# train models\nmodel_suffix_a = ['_logreg_a', '_dt_a', '_bnb_a', '_knn_a', '_lsvm_a']\ntrain_model(X_train_a, y_train, model_suffix_a)\n\n# record best results in cross validation\nrscv_list_a = [rscv_logreg_a, rscv_dt_a, rscv_bnb_a, rscv_knn_a, rscv_lsvm_a]\nrecord_best_result(rscv_list_a, model_suffix_a)\n\n# output the best results as a dataframe\ndf_list_a = [df_logreg_a, df_dt_a, df_bnb_a, df_knn_a, df_lsvm_a]\nfor df, model in zip(df_list_a, model_names):\n    df['model'] = model\nresult_df_a = pd.concat(df_list_a)\n\n# check test scores\nbest_clfs_a = [best_logreg_a, best_dt_a, best_bnb_a, best_knn_a, best_lsvm_a]\ntest_result_a = model_eval(best_clfs_a, model_names, X_test_a, y_test)","587ef3eb":"result_df_a","dc377ad9":"test_result_a","2c567ea9":"# train models\nmodel_suffix_b = ['_logreg_b', '_dt_b', '_bnb_b', '_knn_b', '_lsvm_b']\ntrain_model(X_train_b, y_train, model_suffix_b)\n\n# record best results in cross validation\nrscv_list_b = [rscv_logreg_b, rscv_dt_b, rscv_bnb_b, rscv_knn_b, rscv_lsvm_b]\nrecord_best_result(rscv_list_b, model_suffix_b)\n\n# output the best results as a dataframe\ndf_list_b = [df_logreg_b, df_dt_b, df_bnb_b, df_knn_b, df_lsvm_b]\nfor df, model in zip(df_list_b, model_names):\n    df['model'] = model\nresult_df_b = pd.concat(df_list_b)\n\n# check test scores\nbest_clfs_b = [best_logreg_b, best_dt_b, best_bnb_b, best_knn_b, best_lsvm_b]\ntest_result_b = model_eval(best_clfs_b, model_names, X_test_b, y_test)","a45a56ef":"result_df_b","17473a6c":"test_result_b","fd9dfa63":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=test_result_a.model, y=test_result_a.test_acc,\n                         mode='lines+markers', name='testing accuracy of plan A'))\nfig.add_trace(go.Scatter(x=test_result_b.model, y=test_result_b.test_acc,\n                         mode='lines+markers', name='testing accuracy of plan B'))\nfig.update_layout(title={'text': 'Testing Accuracy of Basic Models',\n                         'y': 0.9,\n                         'x': 0.4,\n                         'xanchor': 'center',\n                         'yanchor': 'top'})\nfig.show()","af2e7ea2":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=test_result_a.model, y=test_result_a.f1_score,\n                         mode='lines+markers', name='testing f1 score of plan A'))\nfig.add_trace(go.Scatter(x=test_result_b.model, y=test_result_b.f1_score,\n                         mode='lines+markers', name='testing f1 score of plan B'))\nfig.update_layout(title={'text': 'Testing F1 Scores of Basic Models',\n                         'y': 0.9,\n                         'x': 0.4,\n                         'xanchor': 'center',\n                         'yanchor': 'top'})\nfig.show()","31fc8a7a":"scale_features_a = list(preprocessor_a.transformers_[0][2])\nonehot_features_a = list(preprocessor_a.transformers_[1][1].get_feature_names(list(cat_features_a)))\nonehot_indices_a = list(range(len(preprocessor_a.transformers_[0][2]), \n                                  X_train_a.shape[1]))","6a1b2b9d":"scale_features_b = list(preprocessor_b.transformers_[0][2])\nonehot_features_b = list(preprocessor_b.transformers_[1][1].get_feature_names(list(cat_features_b)))\nonehot_indices_b = list(range(len(preprocessor_b.transformers_[0][2]), \n                                  X_train_b.shape[1]))","e084264a":"train_kprot_a = np.hstack((X_train_a, y_train[:, np.newaxis]*5.0))","2b7875f7":"kprot_runs_a = [KPrototypes(n_clusters=k, random_state=RANDOM_SEED, n_jobs=-1).\\\n                fit(train_kprot_a, categorical=onehot_indices_a)\n                for k in range(1, 11)]","aec646c6":"def plot_silhouette(X_train, kprot_runs):\n    silhouette_scores = [silhouette_score(X_train, model.labels_) \n                         for model in kprot_runs[1:]]\n    \n    plt.figure(figsize=(15, 20))\n    for k in range(2, 10):\n        plt.subplot(4, 2, k - 1)\n\n        y_pred = kprot_runs[k - 1].labels_\n        silhouette_coefficients = silhouette_samples(X_train, y_pred)\n\n        padding = len(X_train) \/\/ 30\n        pos = padding\n        ticks = []\n        for i in range(k):\n            coeffs = silhouette_coefficients[y_pred == i]\n            coeffs.sort()\n\n            color = cm.Spectral(i \/ k)\n            plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n                              facecolor=color, edgecolor=color, alpha=0.7)\n            ticks.append(pos + len(coeffs) \/\/ 2)\n            pos += len(coeffs) + padding\n\n        plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n        plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n        if k in (2, 4, 6, 8):\n            plt.ylabel(\"Cluster\")\n\n        if k in (8, 9):\n            plt.xlabel(\"Silhouette Coefficient\")\n\n        plt.axvline(x=silhouette_scores[k - 1], color=\"red\", linestyle=\"--\")\n        plt.title(\"$k={}$\".format(k), fontsize=16)\n\n    plt.show()","10546c51":"plot_silhouette(train_kprot_a, kprot_runs_a)","97667196":"class KProtFeaturizer:\n    \"\"\"\n    Transforms mixed data into k-prototypes cluster.\n    \n    This transformer runs k-prototypes on the input data and converts each data\n    point into the ID of the closest cluster. If a target variable is present, \n    it is scaled and included as input to k-prototypes in order to derive clusters\n    that obey the classification boundary as well as group similar points together.\n    \"\"\"\n    \n    def __init__(self, k=15, target_scale=5.0, \n                 categorical=None,\n                 random_state=RANDOM_SEED):\n        self.k = k\n        self.target_scale = target_scale\n        self.categorical = categorical\n        self.random_state = random_state\n        \n    def fit(self, X, y=None):\n        \"\"\"Runs k-prototypes on the input data and finds centroids.\"\"\"\n        \n        if y is None:\n            # No target variable, just do plain k-prototypes\n            kprot_model = KPrototypes(n_clusters=self.k,\n                                      n_jobs=-1,\n                                      random_state=self.random_state)\n            kprot_model.fit(X, categorical=self.categorical)\n            \n            self.kprot_model = kprot_model\n            self.cluster_centroids_ = kprot_model.cluster_centroids_\n            return self\n        \n        # There is target information. Apply approrpriate scaling and include\n        # it in the input data to k-prototypes.\n        data_with_target = np.hstack((X, y[:, np.newaxis] * self.target_scale))\n        \n        # Build a pre-training k-prototypes model on data and target\n        kprot_model_pretrain = KPrototypes(n_clusters=self.k,\n                                           n_jobs=-1,\n                                           random_state=self.random_state)\n        kprot_model_pretrain.fit(data_with_target, categorical=self.categorical)\n        \n        # Run k-prototypes a second time to get the clusters in the original space\n        # without target info. Initialize using centroids found in pre-training. \n        # Go through a single iteration of cluster assignment and centroid recomputation.\n        kprot_model = KPrototypes(n_clusters=self.k,\n                                  # For KPrototypes, we need to specify the cluster centroids for \n                                  # numerical and categorical columns, notice that the numerical \n                                  # part should exclude the target info\n                                  init=[kprot_model_pretrain.cluster_centroids_[:, \\\n                                  [i for i in range(kprot_model_pretrain.cluster_centroids_.shape[1] - 1)\\\n                                  if i not in self.categorical]],\\\n                                  kprot_model_pretrain.cluster_centroids_[:, self.categorical]],\n                                  n_init=1,\n                                  max_iter=1)\n        kprot_model.fit(X, categorical=self.categorical)\n        \n        self.kprot_model = kprot_model\n        self.cluster_centroids_ = kprot_model.cluster_centroids_\n        return self\n    \n    def transform(self, X, y=None):\n        \"\"\"Outputs the closest cluster ID for each input data point.\n        \"\"\"\n        clusters = self.kprot_model.predict(X, categorical=self.categorical)\n        return clusters[:, np.newaxis]\n    \n    def fit_transform(self, X, y=None):\n        self.fit(X, y)\n        return self.transform(X, y)\n    ","7e6c275b":"# apply the featurizer\nkprot_a = KProtFeaturizer(k=4, target_scale=5.0, \n                          categorical=onehot_indices_a,\n                          random_state=RANDOM_SEED)\ntrain_cluster_a = kprot_a.fit_transform(X_train_a, y_train)\ntest_cluster_a = kprot_a.transform(X_test_a)\n# use one-hot encoder to record the cluster labels\nenc = OneHotEncoder()\n# Notice that after one-hot encoding, the outcome matrix\n# is a sparse matrix, we need to transform to an ndarray\ntrain_cluster_label_a = enc.fit_transform(train_cluster_a).toarray()\ntest_cluster_label_a = enc.transform(test_cluster_a).toarray()\nX_train_kprot_a = np.hstack((X_train_a, train_cluster_label_a))\nX_test_kprot_a = np.hstack((X_test_a, test_cluster_label_a))","ff058095":"# train models\nmodel_suffix_kprot = ['_logreg_kprot', '_dt_kprot', '_bnb_kprot', '_knn_kprot', '_lsvm_kprot']\ntrain_model(X_train_kprot_a, y_train, model_suffix_kprot)\n\n# record best results in cross validation\nrscv_list_kprot = [rscv_logreg_kprot, rscv_dt_kprot, rscv_bnb_kprot, rscv_knn_kprot, rscv_lsvm_kprot]\nrecord_best_result(rscv_list_kprot, model_suffix_kprot)\n\n# output the best results as a dataframe\ndf_list_kprot = [df_logreg_kprot, df_dt_kprot, df_bnb_kprot, df_knn_kprot, df_lsvm_kprot]\nfor df, model in zip(df_list_kprot, model_names):\n    df['model'] = model\nresult_df_kprot = pd.concat(df_list_kprot)\n\n# check test scores\nbest_clfs_kprot = [best_logreg_kprot, best_dt_kprot, best_bnb_kprot, best_knn_kprot, best_lsvm_kprot]\ntest_result_kprot = model_eval(best_clfs_kprot, model_names, X_test_kprot_a, y_test)","4ecdd33e":"result_df_kprot","df275674":"test_result_kprot","0af1e478":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=test_result_a.model, y=test_result_a.test_acc,\n                         mode='lines+markers', name='plan A'))\nfig.add_trace(go.Scatter(x=test_result_b.model, y=test_result_b.test_acc,\n                         mode='lines+markers', name='plan B'))\nfig.add_trace(go.Scatter(x=test_result_kprot.model, y=test_result_kprot.test_acc,\n                         mode='lines+markers', name='after feature expansion'))\nfig.update_layout(title={'text': 'Testing Accuracy of Basic Models',\n                         'y': 0.9,\n                         'x': 0.4,\n                         'xanchor': 'center',\n                         'yanchor': 'top'},\n                  autosize=False,\n                  width=1000,\n                  height=600)\nfig.show()\nreport = dp.Report(dp.Plot(fig))\nreport.publish(name=\"Feature Engineering Methods Comparison\", open=True, visibility=dp.Visibility.PUBLIC)","98b8b214":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=test_result_a.model, y=test_result_a.f1_score,\n                         mode='lines+markers', name='testing f1 score of plan A'))\nfig.add_trace(go.Scatter(x=test_result_b.model, y=test_result_b.f1_score,\n                         mode='lines+markers', name='testing f1 score of plan B'))\nfig.add_trace(go.Scatter(x=test_result_kprot.model, y=test_result_kprot.f1_score,\n                         mode='lines+markers', name='testing accuracy after feature expansion'))\nfig.update_layout(title={'text': 'Testing F1 Scores of Basic Models',\n                         'y': 0.9,\n                         'x': 0.4,\n                         'xanchor': 'center',\n                         'yanchor': 'top'})\nfig.show()","540cf56f":"X_train_c = X_train_kprot_a\nX_test_c = X_test_kprot_a","8597af2a":"poly_svc_clf = SVC(kernel='poly')\nrbf_svc_clf = SVC(kernel='rbf')\n# for gridsearchcv, n_jobs=-1 should be placed inside the classifier\nrf_clf = RandomForestClassifier(random_state=RANDOM_SEED, n_jobs=-1)","daf550f6":"poly_svc_params = dict(C=[0.1, 1, 10],\n                       degree=[2, 4, 10])\nrbf_svc_params = dict(C=[0.1, 1, 10],\n                   gamma=[0.1, 1, 2])\nrf_params = dict(n_estimators=[1000, 1400],\n                 max_depth=[20, 30],\n                 min_samples_split=[2, 5],\n                 min_samples_leaf=[1, 2])","5e82483d":"ad_clf_list = [poly_svc_clf, rbf_svc_clf, rf_clf]\nad_params_list = [poly_svc_params, rbf_svc_params, rf_params]\nad_model_names = ['Polynomial SVM', 'Gaussian RBF SVM', 'Random Forest']","b058fe8a":"def train_ad_model(X, y, model_suffix, clf_list, params_list):\n    for i in range(len(clf_list)):\n        gscv = GridSearchCV(estimator=clf_list[i],\n                            param_grid=params_list[i]).fit(X, y)\n        globals()['gscv%s' % model_suffix[i]] = pd.DataFrame(gscv.cv_results_)\n        globals()['best%s' % model_suffix[i]] = gscv","fcd7885a":"ad_model_suffix_a = ['_poly_svc_a', '_rbf_svc_a', '_rf_a']\ntrain_ad_model(X_train_a, y_train, ad_model_suffix_a, \n               clf_list=ad_clf_list, params_list=ad_params_list)","3f61076c":"gscv_list_a = [gscv_poly_svc_a, gscv_rbf_svc_a, gscv_rf_a]\nrecord_best_result(gscv_list_a, ad_model_suffix_a)","c117d8eb":"df_list_ad_a = [df_poly_svc_a, df_rbf_svc_a, df_rf_a]\nfor df, model in zip(df_list_ad_a, ad_model_names):\n    df['model'] = model\nresult_df_ad_a = pd.concat(df_list_ad_a)","18d34cba":"best_clfs_ad_a = [best_poly_svc_a, best_rbf_svc_a, best_rf_a]\ntest_result_ad_a = model_eval(best_clfs_ad_a, ad_model_names, X_test_a, y_test)","744ead5b":"result_df_ad_a","ab4261b7":"test_result_ad_a","03dd5876":"ad_model_suffix_b = ['_poly_svc_b', '_rbf_svc_b', '_rf_b']\ntrain_ad_model(X_train_b, y_train, ad_model_suffix_b, \n               clf_list=ad_clf_list, params_list=ad_params_list)\ngscv_list_b = [gscv_poly_svc_b, gscv_rbf_svc_b, gscv_rf_b]\nrecord_best_result(gscv_list_b, ad_model_suffix_b)\ndf_list_ad_b = [df_poly_svc_b, df_rbf_svc_b, df_rf_b]\nfor df, model in zip(df_list_ad_b, ad_model_names):\n    df['model'] = model\nresult_df_ad_b = pd.concat(df_list_ad_b)\nbest_clfs_ad_b = [best_poly_svc_b, best_rbf_svc_b, best_rf_b]\ntest_result_ad_b = model_eval(best_clfs_ad_b, ad_model_names, X_test_b, y_test)","22662ab8":"ad_model_suffix_c = ['_poly_svc_c', '_rbf_svc_c', '_rf_c']\ntrain_ad_model(X_train_c, y_train, ad_model_suffix_c, \n               clf_list=ad_clf_list, params_list=ad_params_list)\ngscv_list_c = [gscv_poly_svc_c, gscv_rbf_svc_c, gscv_rf_c]\nrecord_best_result(gscv_list_c, ad_model_suffix_c)\ndf_list_ad_c = [df_poly_svc_c, df_rbf_svc_c, df_rf_c]\nfor df, model in zip(df_list_ad_c, ad_model_names):\n    df['model'] = model\nresult_df_ad_c = pd.concat(df_list_ad_c)\nbest_clfs_ad_c = [best_poly_svc_c, best_rbf_svc_c, best_rf_c]\ntest_result_ad_c = model_eval(best_clfs_ad_c, ad_model_names, X_test_c, y_test)","a077ccac":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=test_result_ad_a.model, y=test_result_ad_a.test_acc,\n                         mode='lines+markers', name='testing accuracy of plan A'))\nfig.add_trace(go.Scatter(x=test_result_ad_b.model, y=test_result_ad_b.test_acc,\n                         mode='lines+markers', name='testing accuracy of plan B'))\nfig.add_trace(go.Scatter(x=test_result_ad_c.model, y=test_result_ad_c.test_acc,\n                         mode='lines+markers', name='testing accuracy after feature expansion'))\nfig.update_layout(title={'text': 'Testing Accuracy of Basic Models',\n                         'y': 0.9,\n                         'x': 0.4,\n                         'xanchor': 'center',\n                         'yanchor': 'top'})\nfig.show()","554f1d33":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=test_result_ad_a.model, y=test_result_ad_a.f1_score,\n                         mode='lines+markers', name='testing f1 score of plan A'))\nfig.add_trace(go.Scatter(x=test_result_ad_b.model, y=test_result_ad_b.f1_score,\n                         mode='lines+markers', name='testing f1 score of plan B'))\nfig.add_trace(go.Scatter(x=test_result_ad_c.model, y=test_result_ad_c.f1_score,\n                         mode='lines+markers', name='testing accuracy after feature expansion'))\nfig.update_layout(title={'text': 'Testing F1 Scores of Basic Models',\n                         'y': 0.9,\n                         'x': 0.4,\n                         'xanchor': 'center',\n                         'yanchor': 'top'})\nfig.show()","0fd0fb73":"def ad_model_eval(clf, X_test, y_test):\n    y_pred = clf.predict(X_test)\n    print(\"Mean cross-validated score of the best_estimator: {0:.3f}\".format(clf.best_score_))\n    print(\"Accuracy on test data: {0:.3f}\".format(clf.score(X_test, y_test)))\n    print(metrics.classification_report(y_test, y_pred))\n    print(\"Tuned Model Parameters: {}\".format(clf.best_params_))\n    skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True)\n    metrics.plot_roc_curve(clf, X_test, y_test)\n    plt.show()","f886cdec":"ad_model_eval(best_rbf_svc_b, X_test_b, y_test)","2d168e9c":"feature_names_a = scale_features_a + onehot_features_a\nfeature_names_b = scale_features_b + onehot_features_b\nfeature_names_c = scale_features_a + onehot_features_a + ['cluster_0', 'cluster_1', 'cluster_2', 'cluster_3']","c13590d9":"print(len(feature_names_a) == X_train_a.shape[1])\nprint(len(feature_names_b) == X_train_b.shape[1])\nprint(len(feature_names_c) == X_train_c.shape[1])","8349f019":"feature_importance_a = pd.Series(best_rf_a.best_estimator_.feature_importances_, index=feature_names_a)\nfeature_importance_b = pd.Series(best_rf_b.best_estimator_.feature_importances_, index=feature_names_b)\nfeature_importance_c = pd.Series(best_rf_c.best_estimator_.feature_importances_, index=feature_names_c)","ba7bc34d":"def plot_importance(feature_importance, suffix):\n    fig, ax = plt.subplots(figsize=(10, 8))\n    feature_importance[feature_importance > 0.015].sort_values().plot.barh(ax=ax)\n    ax.set_title(\"Feature Importances of Random Forest Model {}\".format(suffix))\n    ax.set_xlabel(\"Mean decrease in impurity\")\n    fig.tight_layout()\n    plt.show()","cbedfb05":"plot_importance(feature_importance_a, \"A\")","9f4e5f9d":"plot_importance(feature_importance_b, \"B\")","5223a9a7":"plot_importance(feature_importance_c, \"C\")","eadc41f0":"# Introduction","d359108e":"For simplicity, we just call the feature expanded version of data plan C:","3c23ad8f":"Now, let's map them into numerical values with respect to their inner order:","899f5f08":"Let's have a look of the target feature:","cef48093":"We chose three models: polynomial SVM, Gaussian RBF SVM, and Random Forest. Kernel tricks in SVM make it possible to get the same result as if we had added many non-linear features like by polynomial or Gaussian RBF, without actually having to add them. However, with our training data, the running time will be quite long. ","fed90005":"Based on the plots, data plan B achieved the highest accuracy and f1 score by Gaussian RBF SVM given the limited grid search hyperparameter space. However, the training time of plan B is more than twice as long as plan A and C due to its high dimensions and sparsity. Besides, Data in plan A and C may achieve higher accuracy with different hyperparameters.\n\nAs we broke down the training time of three advanced models, we noticed that SVM models spent more time than Random Forest. One of the reason is that SVM in scikit learn does not support parallel computation.","47de3482":"[Here](https:\/\/www.kaggle.com\/bhavikapanara\/frequency-encoding) is the reference for frequency encoding.","2005f317":"This plot shows some differences among jobs. \n\nAs a result, we think all those nominal features are strong predictor variables. Thus, we will do frequency encoding and target encoding on them to improve their predictive power. Those two encodings will be done after data splitting.","8f09626d":"Now, let's have a look of the nominal features:\n\n* **destination**: The destination of the driver\n* **passenger**: Type of passenger in the car\n* **weather**: Sunny, rainy, or snowy\n* **time**: Current time\n* **coupon**: Coupon type\n* **expiration**: Coupon expiration time\n* **gender**: Female or male\n* **maritalStatus**: Marital status of the driver\n* **occupation**: Occupation of the driver\n* **education** Education level of the driver","68dfecaf":"Now, we have finished the basic data wrangling part, if other problems rises during exploratory data analysis, we can repeat the data wrangling process.","9e60a3d6":"Since we have mixed data types, K-means is not appropriate here because Euclidean distance is meaningless for categorical data. There are two variations of K-means clustering: K-modes is capable of clustering on categorical data, K-prototypes clustering is suitable for mixed data types. Both of them are encapsulated in [**kmodes**](https:\/\/github.com\/nicodv\/kmodes) module. The original paper of K-prototypes clustering by Huang can be found [here](http:\/\/citeseerx.ist.psu.edu\/viewdoc\/summary?doi=10.1.1.94.9984), another version of k-prototypes by Cao et al. can be found [here](http:\/\/www.jiyeliang.net\/Cms_Data\/Contents\/SXU_JYL\/Folders\/JournalPapers\/~contents\/TW82GXVHZCGNRBPV\/A%20new%20initialization%20method%20for%20categorical%20data%20clustering.pdf).","9400bdd6":"There are five more columns with missing values, and all of them have data type 'object', which means they are all strings. Time to do some missing value visualizations:","9a82d396":"## Advanced Models","ef93d4dd":"**Frequency Encoding**","66e23dac":"# Feature Engineering","d9e39365":"## Exploratory Data Analysis","afe47ab8":"* plan C","c8f99f0d":"**K-fold Target Encoding**","d9ac9548":"* Plan A","e8955882":"## Basic Models","8ef2ed42":"## Data Preprocessing","7446f91e":"Now, with 21 features, we are ready to train classification models on the cleaned dataset.","290cb11b":"## Data Importing","6e9df71d":"For detailed model evaluation, we can use the function below:","7c3662ea":"* **destination** People that has no urgent place to go has a higher probability to accept the coupon.\n* **passenger** If the passengers in car are friends of the driver, they are more likely to accept the coupon.\n* **weather** People tend to accept the coupon when it is sunny.\n* **time** If the time is too early or too late, the probability of accepting the coupon is lower.\n* **coupon** If the coupon is of a coffee house, the probability of accepting the coupon is just the same as rejecting it. If the coupon is of a cheap restaurant or carry out, most people will accept the coupon. If the coupon is of a Bar of expensive Restaurant, people tend to refuse it.\n* **expiration** People are more likely to accept a coupon that expires in one day than one in two hours.\n* **gender** There is no much difference between gender.\n* **maritalStatus** Single people are most likely to accept the coupon.\n* **education** Some college, Bachelor or high school graduate are more likely to accept the coupon.","c319a103":"Now, it's time to implement the k-prototypes algorithm for feature expansion:","fd38ed10":"Well, the column 'toCoupon_GEQ5min' has only one single value: 1. No variance at all, so, we can drop it. According to the dataset description, this column means driving distance to the restaurant\/bar for using the coupon is greater than 5 minutes, so all the restaurant\/bars are at least five minutes away from the driver.","eaafd183":"This project aims to train a classification model that can predict if a user will accept a coupon given his\/her answers to some survey questions. In this notebook, we will explore the feature engineering methods for categorical data using sklearn's pipeline and workflow.\n\nThe attributes of the data set include:\n\nThis data was collected via a survey on Amazon Mechanical Turk. The survey describes different driving scenarios including the destination, current time, weather, passenger, etc., and then ask the person whether he will accept the coupon if he is the driver. For more information about the dataset, please refer to the paper: \nWang, Tong, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Erica Klampfl, and Perry MacNeille. 'A bayesian framework for learning rule sets for interpretable classification.' The Journal of Machine Learning Research 18, no. 1 (2017): 2357-2393.\n\n**User attributes**\n* gender: male, female\n* age: 21, 46, 26, 31, 41, 50plus, 36, below21 (e.g. 21 means 21 to 26, 26 means 26 to 31)\n* maritalStatus: Unmarried partner, Single, Married partner, Divorced, Widowed \n* has_Children:1, 0 \n* education: Some college - no degree, Bachelors degree, Associates degree, High School Graduate, Graduate degree (Masters or Doctorate), Some High School\n* occupation: Unemployed, Architecture & Engineering, Student, Education&Training&Library, Healthcare Support, Healthcare Practitioners & Technical, Sales & Related, Management, Arts Design Entertainment Sports & Media, Computer & Mathematical, Life Physical Social Science, Personal Care & Service, Community & Social Services, Office & Administrative Support, Construction & Extraction, Legal, Retired, Installation Maintenance & Repair, Transportation & Material Moving, Business & Financial, Protective Service, Food Preparation & Serving Related, Production Occupations, Building & Grounds Cleaning & Maintenance, Farming Fishing & Forestry \n* income: \\\\$37500 - \\\\$49999, \\\\$62500 - \\\\$74999, \\\\$12500 - \\\\$24999, \\\\$75000 - \\\\$87499, \\\\$50000 - \\\\$62499, \\\\$25000 - \\\\$37499, \\\\$100000 or More, \\\\$87500 - \\\\$99999, Less than \\\\$12500 \n* Bar: never, less1, 1\\~3, gt8, nan4\\~8 (feature meaning: how many times do you go to a bar every month?) \n* CoffeeHouse: never, less1, 4\\~8, 1\\~3, gt8, nan (feature meaning: how many times do you go to a coffeehouse every month?)\n* CarryAway:n4\\~8, 1\\~3, gt8, less1, never (feature meaning: how many times do you get take-away food every month?) \n* RestaurantLessThan20: 4\\~8, 1\\~3, less1, gt8, never (feature meaning: how many times do you go to a restaurant with an average expense per person of less than \\\\$20 every month?) \n* Restaurant20To50: 1\\~3, less1, never, gt8, 4\\~8, nan (feature meaning: how many times do you go to a restaurant with average expense per person of \\\\$20 - \\\\$50 every month?) \n\n**Contextual attributes**\n* destination: No Urgent Place, Home, Work \n* passanger: Alone, Friend(s), Kid(s), Partner (who are the passengers in the car)\n* weather: Sunny, Rainy, Snowy \n* temperature:55, 80, 30 \n* time: 2PM, 10AM, 6PM, 7AM, 10PM \n* toCoupon_GEQ5min:0, 1 (feature meaning: driving distance to the restaurant\/bar for using the coupon is greater than 5 minutes) \n* toCoupon_GEQ15min:0,1 (feature meaning: driving distance to the restaurant\/bar for using the coupon is greater than 15 minutes) \n* toCoupon_GEQ25min:0, 1 (feature meaning: driving distance to the restaurant\/bar for using the coupon is greater than 25 minutes) \n* direction_same:0, 1 (feature meaning: whether the restaurant\/bar is in the same direction as your current destination) \n* direction_opp:1, 0 (feature meaning: whether the restaurant\/bar is in the same direction as your current destination) \n\n**Coupon attributes**\n* coupon: Restaurant(<\\\\$20), Coffee House, Carry out & Take away, Bar, Restaurant(\\\\$20-\\\\$50) \n* expiration: 1d, 2h (the coupon expires in 1 day or in 2 hours) ","8ce0859e":"Now we can drop these two columns: toCoupon_GEQ15min and toCoupon_GEQ25min","5202f5cf":"**Two Plans**","18445529":"After data preprocessing, we now have 33 features in plan A. Now, we can do data preprocessing for plan B, which basically uses `OneHotEncoder` for all categorical features.","4a5de1b1":"There is a concern for the clustering algorithm: to include the target feature or not. If we include the target to get the cluster labels, and use those labels to classify the target, the information about target will have leaked into the features, and the accuracy will be overly optimisitc. But when we evaluate them on the test set, this bias will diminish and disappear. Thus, we can also use k-fold cross validation to alleviate the data leakage problem.","c3caa03b":"According to the correlations between those categorical features and the target, the strong predictors (correlation $\\ge$ 0.1) are **destination, passanger, weather, time, coupon,** and **expiration.** Although not all of the features in the `strong_predictors` list are really strong predictors, we still want to implement the frequency encoding and feature encoding on them because it can greatly reduce the dimensions of the dataset.","abf06197":"Based on the exploratory data analysis, we have almost all categorical variables. There are two sections of feature engineering we saved that can only be done with splitted data: imputation and categorical encoding.\n\n* Data Imputation\n\nWe will use simple imputer to replace the missing value with the most frequent item.\n\n* Categorical Encoding\n\nFor those categorical features we believe are strong predictors, we will do both frequency encoding and target encoding. For other categorical features, OneHotEncoder will be applied.\n\n**Further Feature Engineering**\n\nThere are other considerations about the feature engineering:\n* do we need to apply dimension reduction?\n\nAfter the OneHotEncoding, we can predict the number of features to reach 50 or more, and the frequency columns are correlated, so it is possible to apply dimension reduction methods like PCA. \n* do we need to apply feature expansion?\n\nWe can use clustering method to add labels into the dataset as a new feature to help the classification task.\n\nThose two considerations will be experimented in the model training process.","bce7bba5":"Silhouette coefficient ranges from -1 to +1, the silhouette score is the mean silhouette coefficient over all the instances within the cluster. A coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary, and finally a coefficient close to \u20131 means that the instance may have been assigned to the wrong cluster.\n\nAccording to the silhouette plot for data plan A, the silhouette score looks good when $k = 2, 3, 4, 5$ as the red dashed line indicates the average silhouette score. We are picking $k = 4$ as the number of clusters of our k-prototypes clustering for data in plan A.","c3880fc1":"### K-prototypes Clustering","d850e91d":"## Retrain Models after Adding Cluster Labels","bee4e5e3":"* Setup","f34b97f9":"* plan B","adf535c7":"There are many levels in the **occupation** column. If simply apply onehotencoder on it, it will greatly increase the sparsity of our data.","36b9c380":"**Plan B**\n\nTrain those models again on data of plan B:","060ce98b":"Now, we are going to apply on all three types of data kernel SVM and random forest. Neural networks and gradient boosting machines will be implemented in another notebook because they have good GPU support, but if I use GPU accelerator, the CPU power assigned will be lower, since sklearn mostly relys on CPU, we will seperate sklearn models from GPU supported models.","71b0f364":"## Data Wrangling","faea077a":"No much information left in the 'car' variable, we can just drop it off.","95deaf32":"First, we plot the count plot of each categorical variable:","fe684028":"In the detailed model evaluation, Gaussian RBF SVM model trained on data in plan A has a high false positive rate.","050352c2":"The pair plot tells us one thing: how sparse our dataset is. All the numerical variables are discrete numbers, and actually categorical. If the numerical representation did not give a good result, we can treat all the variables as categorical and train models again.\n\nIf we look into the hist plot, we can see that there are more people accepting coupon than rejecting when temperature is high and when distance is short. It means temperature and distance are decent predictive variables.","bfc3c33c":"Now, let's look at the occupation feature:","83f2745e":"Besides, there are correlations among those frequency columns: **Bar, CoffeeHouse, CarryAway, RestaurantLessThan20, Restaurant20To50**.","83e8eeb1":"# Conclusion","c07e60df":"### Visualization of Feature Importance","b1649d60":"Two popular clustering methods can be used to apply feature expansion: hierarchical clustering and k-means clustering. However, hierarchical clustering is slower than k-means clustering, thus, we are using k-means for this feature expansion task.","ea8588d9":"Due to sparsity and the curse of dimension, k-prototypes model on data of plan B will take more than 1 hour to train for plotting the silhouette plot. Thus, we give it up. The feature expansion will only be applied on data in plan A.","a3aa9257":"Indeed, missing at random. So, we can do data imputation on them. Since we cannot directly impute data on the whole dataset, we are saving data imputation until we finished the data splitting. Before that, let's explore the dataset a little more.","ba5407d6":"**Nominal Features**","3effa609":"The basic models we picked are Logistic Regression, Decision Tree, Naive Bayes, K Nearest Neighbor, and linear Support Vector Machine. We picked those models because they are fast to train. We will use `RandomizedSearchCV` to choose hyperparameters from the same parameter grid for both plan A and plan B. After that, we will compare the results of them.\n\nTutorial for `RandomizedSearchCV` can be found [here](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py). We also borrowed Rupanjan Nayak's [code](https:\/\/stackoverflow.com\/questions\/23045318\/scikit-grid-search-over-multiple-classifiers) to train multiper models at once.","21e014bc":"Let's explore relationships between some of the features and the target:","ed1459e6":"By looking at the counting plot of the categorical features, we realized that there are two kinds of categorical data: ordinal and nominal, we can apply one hot encoder to nominal features, however, ordinal data should be mapped into numerical data so that the inner order can preserve.","2d0b0665":"**Other Data Preprocessing**\n\nHere we used sklearn's `Pipeline` and `ColumnTransformer` to do simple imputation, standardization, and one hot encoder for columns of mixed data types. The tutorial of how to use `Pipeline` and `ColumnTransformer` can be found [here](https:\/\/scikit-learn.org\/stable\/auto_examples\/compose\/plot_column_transformer_mixed_types.html).","b641cfed":"To improve the prediction results, we have two directions to explore: \n* Feature Expansion\n\nWe can use clustering method to cluster the training data and add the cluster label as new feature.\n\n* Advanced Models\n\nAdvanced models like Random Forest, kernel SVM, Neural Networks, and Gradient Boosting Machine can be applied.","fc97f936":"Some columns needed to be transformed into categorical columns.","9710070f":"Feature engineering of this task can be divided into two parts. The first part is data wrangling that can be done on the dataset as a whole. The second part is operating some feature engineerings after split the data into traning and testing set. This section will also be filled with Exploratory Data Analysis.","58fc27f5":"**Categorical Features**","6d08e16c":"The plot shows some subtle, positive correlations between short distance, high temperature and the probability to accept the coupon.","26920a07":"As the heatmap shows, there are two columns sharing the same information: **direction_same** and **direction_opp**. They indicate whether the restaurant\/bar is in the same direction as your current destination. So we decide to delete the **direction_opp** column.","be88a481":"* Plan B","c1882e6e":"**Plan A**","93f2306d":"From the visualizations, we can see that the knn classifier built on data of plan A has a better accuracy. This is quite interpretable since knn classifier relies on distances, and our feature engineering in plan A transformed sparse categorical data into numerical frequency features, by doing that, distance among the feature space started to make more sense. \n\nDecision tree built on data of plan B performs better than plan A, that is because decision tree is good at splitting categorical data, so one hot encoded features are more suitable for decision tree. \n\nAs for F1 scores, knn classifier reached the highest testing F1 score on data of plan A, indicating the best overall predictive accuracy behind it.","235fc06a":"Now, check missing values of other columns:","53cbb5e4":"After excluding the toCoupon_GEDQ5min feature, we see there are still two features about the driving distance to the Coupon's location: **toCoupon_GEQ15min** and **toCoupon_GEQ25min**. They have two possible values: 0 and 1 indicating yes or no to the question: is the driving distance to the restaurant\/bar for using the coupon is greater than 15 minutes\/25 minutes? We should be able to combine these two columns into one with ordinal data inside: greater than 5 minutes and less than 15 minutes, greater than 15 minutes and less than 25 minutes, and greater than 25 minutes. We believe doing that should be better than treat those two columns using onehotencoder just like different categories.","34bd4fa0":"Classifiers:","c8f9fe42":"To determine which number of clusters to use in the K-prototypes clustering, we plotted a silhouette diagram:","b8a4efcc":"We set a threshold that only those features whose importance is higher than 0.015 can be in this plot. For data A and C, the feature importance plots are basically the same, and the cluster labels we added by kprototypes clustering does not show up in the feature importance plot which means it is not that important at least in Random Forest model. If we check the top 10 most important features, they are coupon type and occupation type's target and frequency encoding, how frequent to go to coffee houses, education and time's target encoding, income, and age.\n\nFor data B, we can see that the frequency features: CoffeeHouse, Bar, CarryAway, RestaurantLessThan20, and Restaurant20To50 are on the upper part of the plot, indicating their importances. Numerical features like income, age, temperature, and distance are quite importance as well as the Education level. Coupon features like coupon type and coupon expiration time are also in the plot. Indeed, there are some similarity between data in B and A, C. ","803301a9":"Put occupation aside, let's plot the relationships between those features and the target feature.","b3be9f44":"**Numerical Features**","572a7916":"**toCoupon_GEQ5min**","e2eb66c6":"# Model Training","fd8d47ec":"**Ordinal Features**","e403174c":"**Plan B**","d0d1a072":"**Missing Values**","be40d1f6":"Those ordinal features are: \n\n* **Restaurant20To50**: how many times do you go to a restaurant with average expense per person of \\\\$20 - \\\\$50 every month?\n* **RestaurantLessThan20**: how many times do you go to a restaurant with an average expense per person of less than \\\\$20 every month?\n* **CarryAway**: how many times do you get take-away food every month?\n* **CoffeeHouse**: how many times do you go to a coffeehouse every month?\n* **age**: quite self-explanatory\n* **Bar**: how many times do you go to bar every month?\n* **income**: income range","287aac2b":"Now, let's plot the correlations between numerical features:","259a4a4d":"* plan A","e14b52a4":"**Data Splitting**","cf489a94":"We can check the correlations among all the numerical features now:","eba7f525":"Now, we have both kinds of data: plan A with frequency encoding and target encoding, plan B with one hot encoding. We will next build basic models on them and check the performance between plan A and B.","4aeebd4d":"According to the two plots above, feature expansion using kprototypes clustering decreased the performance of knn model, increased the performance of decision tree induction model. Both the highest accuracy and f1 scores were achieved by KNN classifier using data in plan A. Since we used the same random search grids, better results could have been achieved using different hyperparameters. Anyway, it is already enough to see the power of feature expansion using k-prototypes clustering.","e59db301":"Parameter distributions:","812154a1":"Since we trained Random Forest models, we can have a look of the feature importance by plotting them.","dc588b59":"There are some missing values in several columns, and the 'car' variable has only 108 non-null values, more than 99% of the values are NaN. We can just drop it off. Before doing that, let's have a look of the non-null values in 'car' column.","2c2e0d10":"**Car**","896cc6c6":"## Feature Expansion","ef1bca58":"**toCoupon_GEQ15min, toCoupon_GEQ25min**","88c3cf2a":"From this matrix, we can probably say the missing values are at least missing at random. Let's do a correlation heatmap to ensure that.","937f26a6":"Here we want to compare the effects of different feature engineering: \n* **plan A** is to do frequency and target encoding for strong predictors we observed in EDA part, and OneHotEncoding for other categorical features. \n* **Plan B** is to apply OneHotEncoding for all the categorical features.\n\nWe will then compare the predictive performances of these two plans and decide which plan to keep for further tasks.","142bde6e":"The code below is borrowed from Pourya from his medium blog: [K-Fold Target Encoding](https:\/\/medium.com\/@pouryaayria\/k-fold-target-encoding-dfe9a594874b). We added some docstrings and comments to make the code more readable. H2O has a great package named `TargetEncoder` which has basically the same workflow as Pourya's code, the tutorial can be found [here](http:\/\/h2o-release.s3.amazonaws.com\/h2o\/rel-yates\/4\/docs-website\/h2o-docs\/data-munging\/target-encoding.html).","cf73c769":"In this notebook, we started from data cleanning and exploratory data analysis, built basic and advanced models on target and frequent encoded data, one hot encoded data, and featured expanded data to check which kind of data is best in improving model performance. Finally we reached 76.3% accuracy on test data with Gaussian RBF SVM.\n\nThe main focus of this notebook is to show various kinds of feature engineering methods applicable on categorical data and the sklearn style workflow to train multiple models at once. For categorical data encoding, we tried one hot encoding, target encoding, and frequency encoding. We also explored one of the feature expansion methods that is suitable for mixed data type: kprototypes clustering. \n\nIn the next part, we will train GPU supported models like Neural Networks and Gradient Boosting Machines, tune the hyperparamters with Optuna, stack models to get a better predictive performance, and finally try the autoML tools.","dccce7f3":"It's well balanced. Accuracy can be a good metric for model performance.","01bfe8dd":"Again, since our data is kinda huge, we will limit the search space of hyperparameters for advanced models as well. The next notebook will be talking about how to achieve best accuracy using model stacking and automl tools.","940a9812":"**Plan A**","c367b996":"To use k-prototypes, we need to point out the column indices of categorical features. After using sklearn's preprocessor, the column names and indices have changed. So, we need to figure them out for data plan A and B."}}