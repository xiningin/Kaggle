{"cell_type":{"6d832053":"code","2ad6e0d0":"code","fcf638c2":"code","14e9be96":"code","31427358":"markdown","c91980a1":"markdown","e6336f36":"markdown","9a806feb":"markdown"},"source":{"6d832053":"#All imports\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [15, 5]\nimport numpy\nimport pandas as pd\nimport numpy as np\nimport cv2\nfrom skimage import morphology\nimport openslide\nimport time\n\ndef otsu_filter(channel, gaussian_blur=True):\n    \"\"\"Otsu filter.\"\"\"\n    if gaussian_blur:\n        channel = cv2.GaussianBlur(channel, (5, 5), 0)\n    channel = channel.reshape((channel.shape[0], channel.shape[1]))\n\n    return cv2.threshold(\n        channel, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n\ndef detect_tissue(wsi, sensitivity = 3000, downsampling_factor=64):\n    \n    \"\"\"\n    Find RoIs containing tissue in WSI.\n    Generate mask locating tissue in an WSI. Inspired by method used by\n    Wang et al. [1]_.\n    .. [1] Dayong Wang, Aditya Khosla, Rishab Gargeya, Humayun Irshad, Andrew\n    H. Beck, \"Deep Learning for Identifying Metastatic Breast Cancer\",\n    arXiv:1606.05718\n    \n    Parameters\n    ----------\n    wsi: OpenSlide\/AnnotatedOpenSlide class instance\n        The whole-slide image (WSI) to detect tissue in.\n    downsampling_factor: int\n        The desired factor to downsample the image by, since full WSIs will\n        not fit in memory. The image's closest level downsample is found\n        and used.\n    sensitivity: int\n        The desired sensitivty of the model to detect tissue. The baseline is set\n        at 5000 and should be adjusted down to capture more potential issue and\n        adjusted up to be more agressive with trimming the slide.\n        \n    Returns\n    -------\n    -Binary mask as numpy 2D array, \n    -RGB slide image (in the used downsampling level, in case the user is visualizing output examples),\n    -Downsampling factor.\n    \"\"\"\n    #For timing\n    time_stamps = {}\n    time_stamps[\"start\"] = time.time()\n    \n    # Get a downsample of the whole slide image (to fit in memory)\n    downsampling_factor = min(\n        wsi.level_downsamples, key=lambda x: abs(x - downsampling_factor))\n    level = wsi.level_downsamples.index(downsampling_factor)\n\n    slide = wsi.read_region((0, 0), level, wsi.level_dimensions[level])\n    slide = np.array(slide)[:, :, :3]\n    time_stamps[\"1\"] = time.time()\n    # Convert from RGB to HSV color space\n    slide_hsv = cv2.cvtColor(slide, cv2.COLOR_BGR2HSV)\n    time_stamps[\"2\"] = time.time()\n    # Compute optimal threshold values in each channel using Otsu algorithm\n    _, saturation, _ = np.split(slide_hsv, 3, axis=2)\n\n    mask = otsu_filter(saturation, gaussian_blur=True)\n    time_stamps[\"3\"] = time.time()\n    # Make mask boolean\n    mask = mask != 0\n\n    mask = morphology.remove_small_holes(mask, area_threshold=sensitivity)\n    mask = morphology.remove_small_objects(mask, min_size=sensitivity)\n    time_stamps[\"4\"] = time.time()\n    mask = mask.astype(np.uint8)\n    mask_contours, tier = cv2.findContours(\n        mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n    time_stamps[\"5\"] = time.time()\n    time_stamps = {key:(value-time_stamps[\"start\"]) * 1000 for key,value in time_stamps.items()}\n    return mask_contours, tier, slide, downsampling_factor, time_stamps\n\ndef draw_tissue_polygons(mask, polygons, polygon_type,\n                              line_thickness=None):\n        \"\"\"\n        Plot as numpy array detected tissue.\n        Modeled WSIPRE github package\n        \n        Parameters\n        ----------\n        mask: numpy array \n            This is the original image represented as 0's for a starting canvas\n        polygons: numpy array \n            These are the identified tissue regions\n        polygon_type: str (\"line\" | \"area\")\n            The desired display type for the tissue regions\n        polygon_type: int\n            If the polygon_type==\"line\" then this parameter sets thickness\n\n        Returns\n        -------\n        Nunmpy array of tissue mask plotted\n        \"\"\"\n        \n        tissue_color = 1\n\n        for poly in polygons:\n            if polygon_type == 'line':\n                mask = cv2.polylines(\n                    mask, [poly], True, tissue_color, line_thickness)\n            elif polygon_type == 'area':\n                if line_thickness is not None:\n                    warnings.warn('\"line_thickness\" is only used if ' +\n                                  '\"polygon_type\" is \"line\".')\n\n                mask = cv2.fillPoly(mask, [poly], tissue_color)\n            else:\n                raise ValueError(\n                    'Accepted \"polygon_type\" values are \"line\" or \"area\".')\n\n        return mask\n\ndef tissue_cutout(tissue_slide, tissue_contours, slide):\n    #https:\/\/stackoverflow.com\/a\/28759496\n    crop_mask = np.zeros_like(tissue_slide) # Create mask where white is what we want, black otherwise\n    cv2.drawContours(crop_mask, tissue_contours, -1, 255, -1) # Draw filled contour in mask\n    tissue_only = np.zeros_like(slide) # Extract out the object and place into output image\n    tissue_only[crop_mask == 255] = slide[crop_mask == 255]\n    return tissue_only\n\ndef getSubImage(rect, src_img):\n    width = int(rect[1][0])\n    height = int(rect[1][1])\n    box = cv2.boxPoints(rect)\n\n    src_pts = box.astype(\"float32\")\n    dst_pts = np.array([[0, height-1],\n                        [0, 0],\n                        [width-1, 0],\n                        [width-1, height-1]], dtype=\"float32\")\n    M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n    warped = cv2.warpPerspective(src_img, M, (width, height))\n    return warped\n\ndef detect_and_crop(image_location:str, sensitivity:int=3000, \n                    downsample_rate:int=16, show_plots:str=\"simple\"):\n    \n    #For timing\n    time_stamps = {}\n    time_stamps[\"start\"] = time.time()\n    \n    #Open Slide\n    wsi = openslide.open_slide(image_location)\n    time_stamps[\"open\"] = time.time()\n    \n    #Get returns from detect_tissue()\n    (tissue_contours, tier, \n     downsampled_slide, \n     downsampling_factor,\n     time_stamps_detect) = detect_tissue(wsi,\n                                          sensitivity,downsample_rate)\n    time_stamps[\"tissue_detect\"] = time.time()\n    \n    #Get Tissue Only Slide\n    base_slide_mask = np.zeros(downsampled_slide.shape[:2])\n    tissue_slide = draw_tissue_polygons(base_slide_mask, tissue_contours,'line', 5)\n    base_size = get_disk_size(downsampled_slide)\n    tissue_only_slide = tissue_cutout(tissue_slide, tissue_contours, downsampled_slide)\n    time_stamps[\"tissue_trim\"] = time.time()\n    #Get minimal bounding rectangle for all tissue contours\n    if len(tissue_contours) == 0:\n        img_id = image_location.split(\"\/\")[-1]\n        print(f\"No Tissue Contours - ID: {img_id}\")\n        return None, 1.0\n    \n    all_bounding_rect = cv2.minAreaRect(np.concatenate(tissue_contours))\n    #Crop with getSubImage()\n    smart_bounding_crop = getSubImage(all_bounding_rect,tissue_only_slide)\n    time_stamps[\"crop\"] = time.time()\n    \n    #Crop empty space\n    #Remove by row\n    row_not_blank =  [row.all() for row in ~np.all(smart_bounding_crop == [255,0,0],\n                                                   axis=1)]\n    space_cut = smart_bounding_crop[row_not_blank,:]\n    #Remove by column\n    col_not_blank =  [col.all() for col in ~np.all(smart_bounding_crop == [255,0,0],\n                                                   axis=0)]\n    space_cut = space_cut[:,col_not_blank]\n    time_stamps[\"cut\"] = time.time()\n    \n    #Get size change\n    start_size = get_disk_size(downsampled_slide)\n    final_size = get_disk_size(space_cut)\n    pct_change = final_size \/ start_size\n    \n    if show_plots == \"simple\":\n        print(f\"Percent Reduced from Base Slide to Final: {(1- pct_change)*100:.2f}\")\n        plt.imshow(space_cut)\n        plt.show() \n    elif show_plots == \"verbose\":\n        #Set-up dictionary for plotting\n        verbose_plots = {}\n        #Add Base Slide to verbose print\n        verbose_plots[f\"Base Slide\\n{get_disk_size(downsampled_slide):.2f}MB\"] = downsampled_slide\n        #Add Tissue Only to verbose print\n        verbose_plots[f\"Tissue Detect\\nNo Change\"] = tissue_slide\n        #Add Bounding Boxes to verbose print\n        verbose_plots[f\"Bounding Boxes\\n{get_disk_size(smart_bounding_crop):.2f}MB\"] = smart_bounding_crop\n        #Add Space Cut Boxes to verbose print\n        verbose_plots[f\"Space Cut\\n{get_disk_size(space_cut):.2f}MB\"] = space_cut\n        print(f\"Percent Reduced from Base Slide to Final: {(1- pct_change)*100:.2f}\")\n        plt = plot_figures(verbose_plots, 1, len(verbose_plots))\n        plt.show()\n    elif show_plots == \"none\":\n        pass\n    else:\n        pass\n    time_stamps[\"all\"] = time.time()\n    time_stamps = {key:(value-time_stamps[\"start\"]) * 1000 for key,value in time_stamps.items()}\n    return space_cut, (1-pct_change), time_stamps\n\ndef get_disk_size(numpy_image):\n    \"\"\" Returns size in MB of numpy array on disk.\"\"\"\n    return (numpy_image.size * numpy_image.itemsize) \/ 1000000\n\ndef plot_figures(figures, nrows = 1, ncols=1):\n    #https:\/\/stackoverflow.com\/a\/11172032\n    \"\"\"Plot a dictionary of figures.\n\n    Parameters\n    ----------\n    figures : <title, figure> dictionary\n    ncols : number of columns of subplots wanted in the display\n    nrows : number of rows of subplots wanted in the figure\n    \"\"\"\n\n    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows)\n    for ind,title in enumerate(figures):\n        axeslist.ravel()[ind].imshow(figures[title], aspect='auto')\n        axeslist.ravel()[ind].set_title(title)\n    plt.tight_layout()\n    return plt\n\n#Set up example slide\nslide_dir = \"..\/input\/prostate-cancer-grade-assessment\/train_images\/\"\nannotation_dir = \"..\/input\/prostate-cancer-grade-assessment\/train_label_masks\/\"\nexample_id = \"0032bfa835ce0f43a92ae0bbab6871cb\"\nexample_slide = f\"{slide_dir}{example_id}.tiff\"\nnumpy_result, pct_change, time_stamps = detect_and_crop(image_location=example_slide, downsample_rate=16, show_plots=\"verbose\")\nprint(time_stamps)","2ad6e0d0":"small_img = cv2.resize(numpy_result, (512, 512))\nplt.imshow(small_img)\nplt.show() ","fcf638c2":"import os\nimport skimage.io\nfrom multiprocess import Pool\nfrom statistics import mean\nfrom tqdm.notebook import tqdm\nsave_dir = \"\/kaggle\/train_images\/\"\nos.makedirs(save_dir, exist_ok=True)\ntrain_data_df = pd.read_csv(\"..\/input\/prostate-cancer-grade-assessment\/train.csv\")\n\n## 5 Image sample for example *******************\nimage_ids = list(train_data_df.image_id.sample(5))\n## Uncomment for call code *******************\n#image_ids = list(train_data_df.image_id)\ndef make_images(image_id):\n    load_path = slide_dir + image_id + '.tiff'\n    save_path = save_dir + image_id + '.png'\n    \n    biopsy, pct_change, time_stamps = detect_and_crop(load_path, downsample_rate=16, show_plots=\"none\")\n    if biopsy is None: return 0\n    img = cv2.resize(biopsy, (512, 512))\n    cv2.imwrite(save_path, img)\n    return pct_change\n        \n\nwith Pool(processes=4) as pool:\n    avg_pct_reduced = list(\n        tqdm(pool.imap(make_images, image_ids), total = len(image_ids))\n    )\n\nprint(f\"The averge size reduced reduced is {mean(avg_pct_reduced):.2%}\")","14e9be96":"!tar -czf train_images.tar.gz ..\/train_images\/*.png","31427358":"I am following the lead of [@xhlulu](https:\/\/www.kaggle.com\/xhlulu) and thier notebook: [PANDA: Resize and Save Train Data](https:\/\/www.kaggle.com\/xhlulu\/panda-resize-and-save-train-data)","c91980a1":"# Tissue Detect->PNG(512x512): Pre-Process\n\nThis notebook creates a pipeline for the work done in more explanatory notebook: [Tissue Detection and Size Optimization ~70% Shrink](https:\/\/www.kaggle.com\/dannellyz\/tissue-detection-and-size-optimization-70-shrink). I combine all of the code into one block below. I have additionally added some timing locaitons for performance improvement work.\n\n# Data\nAdditionally I have made the [pre-processed images](https:\/\/www.kaggle.com\/dannellyz\/panda-preprocessing-tissue-detection\/) available as well.","e6336f36":"# To save on commit time and processing I only process 5 images here.","9a806feb":"![tissue_logo.001.jpeg](attachment:tissue_logo.001.jpeg)\n\nTissue Detection is a key aspect to research in the domain of computer vision applied to cancer classification. My main focus in this competition so far has been exploring previous work done in this domain and furthering its application towards this dataset. Notebooks in this collection include the following.\n* [Base Notebook](https:\/\/www.kaggle.com\/dannellyz\/panda-tissue-detection-size-optimization-70) : Tissue Detection Intro and First Application\n* [Base Dataset Generation **(Currently Here)**](https:\/\/www.kaggle.com\/dannellyz\/tissue-detect-td-conv-png-512x512): Notebook to export images to zip file\n* [Scaling Bounding Boxes](https:\/\/www.kaggle.com\/dannellyz\/tissue-detect-scaling-bounding-boxes-4xfaster): 4x speed increase to base notebook\n* [Tissue Dection Metadata Analysis](https:\/\/www.kaggle.com\/dannellyz\/tissue-detection-bounding-box-metadata-eda-viz\/): Exploring features from bounding boxes discovery on the slides"}}