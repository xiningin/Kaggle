{"cell_type":{"5cbc5556":"code","456aa6ab":"code","c826e5da":"code","b1a11c26":"code","6fe408de":"code","106bd27a":"code","94c2968f":"code","855d77e1":"code","a630a64a":"code","037f1fef":"code","e521920f":"code","ac734d19":"code","044edb4c":"code","63993928":"code","93dd7255":"code","21ebdc34":"markdown","4acb059f":"markdown","1c269e51":"markdown","0b7313ca":"markdown","cee733a3":"markdown","6c74009b":"markdown","36809ca7":"markdown","f022f26a":"markdown","d41b06e2":"markdown"},"source":{"5cbc5556":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if filename.endswith('.jpg'):\n            break\n        print(os.path.join(dirname, filename))\n        \ndf = pd.read_csv('..\/input\/herbarium-2020-fgvc7\/sample_submission.csv')\nimport json,codecs\nwith codecs.open(\"..\/input\/herbarium-2020-fgvc7\/nybg2020\/train\/metadata.json\", 'r',\n                 encoding='utf-8', errors='ignore') as f:\n    train_meta = json.load(f)\n    \nwith codecs.open(\"..\/input\/herbarium-2020-fgvc7\/nybg2020\/test\/metadata.json\", 'r',\n                 encoding='utf-8', errors='ignore') as f:\n    test_meta = json.load(f)\n\ntrain_df = pd.DataFrame(train_meta['annotations'])\ntrain_cat = pd.DataFrame(train_meta['categories'])\ntrain_cat.columns =['family','genus','category_id','category_name']\ntrain_img = pd.DataFrame(train_meta['images'])\ntrain_img.columns = ['file_name', 'height', 'image_id', 'license', 'width']\ntrain_reg = pd.DataFrame(train_meta['regions'])\ntrain_reg.columns = ['region_id', 'region_name']\ntrain_df = train_df.merge(train_cat, on='category_id', how='outer')\ntrain_df = train_df.merge(train_img, on='image_id', how='outer')\ntrain_df = train_df.merge(train_reg, on='region_id', how='outer')\nna = train_df.file_name.isna()\n#display(na) #\u5404\u884c\u3092\u30c1\u30a7\u30c3\u30af\u3057NaN\u306a\u3089True\u3092\u8fd4\u3059\n\nkeep = [x for x in range(train_df.shape[0]) if not na[x]] \ntrain_df = train_df.iloc[keep]\n\ndtypes = ['int32', 'int32', 'int32', 'int32', 'object', 'object', 'object', 'object', 'int32', 'int32', 'int32', 'object']\nfor n, col in enumerate(train_df.columns): #n -> \u884c\u6570 col -> Index\n    train_df[col] = train_df[col].astype(dtypes[n]) #\u578b\u306e\u5909\u63db\n    \ntest_df = pd.DataFrame(test_meta['images'])\ntest_df.columns = ['file_name', 'height', 'image_id', 'license', 'width']\n\ntrain_df.to_csv('full_train_data.csv', index=False)\ntest_df.to_csv('full_test_data.csv', index=False)\n\n# print(\"Total Unique Values for each columns:\")\n# print(\"{0:10s} \\t {1:10d}\".format('train_df', len(train_df)))\n\n#Data Explpration\n\nfor col in train_df.columns:\n    print(\"{0:10s} \\t {1:10d}\".format(col, len(train_df[col].unique())))\n\nfamily = train_df[['family', 'genus', 'category_name']].groupby(['family', 'genus']).count()\n\nprint(\"Sequence End\")\nprint(train_df)","456aa6ab":"import tensorflow.keras as keras\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten, BatchNormalization, Input, concatenate,add,Add\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.model_selection import train_test_split as tts\n\n","c826e5da":"\"\"\"\n## Model function\ndef fg_model(shape,lr=0.001):\n    i = Input(shape)\n    \n    x = Conv2D(3,(3,3),activation='relu',padding='same',kernel_initializer='he_normal')(i)\n    x = Conv2D(3,(5,5),activation='relu',padding='same',kernel_initializer='he_normal')(x)\n    x = MaxPool2D(pool_size=(3,3),strides=(3,3))(x)\n    x = Dropout(0.5)(x)\n    x = Conv2D(16,(5,5),activation='relu',padding='same',kernel_initializer='he_normal')(x)\n    x = MaxPool2D(pool_size=(5,5),strides=(5,5))(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    x = Flatten()(x)\n    \n    o1 = Dense(310,activation='softmax',name='family',kernel_initializer='he_normal')(x)\n    \n    o2 = concatenate([o1,x])\n    o2 = Dense(3678,activation='softmax',name='genus',kernel_initializer='he_normal')(o2)\n    \n    o3 = concatenate([o1,o2,x])\n    o3 = Dense(32094,activation='softmax',name='category_id',kernel_initializer='he_normal')(o3)\n    \n    x = Model(inputs=i, outputs=[o1,o2,o3])\n    \n    opt = Adam(lr=lr,amsgrad=True)\n    x.compile(optimizer=opt,loss=['sparse_categorical_crossentropy', \n                                   'sparse_categorical_crossentropy', \n                                   'sparse_categorical_crossentropy'],\n                 metrics=['accuracy'])\n    \n    return x\n\nmodel = fg_model((120,120,3))\nmodel.summary()\nplot_model(model, to_file='full_model_plot.png',show_shapes=True,show_layer_names=True)\n\"\"\"","b1a11c26":"## \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u304a\u3051\u308b\u91cd\u307f\u306e\u521d\u671f\u5024\n## Ref\n## https:\/\/ai-trend.jp\/basic-study\/neural-network\/initial_value\/#Xivier\n## sigmoid\u95a2\u6570\u3084tanh\u95a2\u6570\u3092\u6d3b\u6027\u5316\u95a2\u6570\u3068\u3057\u3066\u7528\u3044\u308b\u6642\u3001\u3053\u306eXavier\u306e\u521d\u671f\u5024\u3092\u7528\u3044\u308b\u3068\u3088\u3044\n\nin_out_size = (120*120) + 3 #We will resize the image to 120*120 and we have 3 outputs\ndef xavier(shape, dtype=None):\n    return np.random.rand(*shape)*np.sqrt(1\/in_out_size)","6fe408de":"\"\"\"\nfunction imformation\nname:fg_model\nargment:\n1: shape -> (120,120,3)\n2: lr -> learning rate 0.001 is default value\n\nThis model was based on dl4us lesson2_sec4\n\"\"\"\ndef fg_model(shape,lr=0.001):\n\n    i = Input(shape)\n\n    x = (Conv2D(3, kernel_size=(3, 3), activation='relu',padding='same',\n                     kernel_initializer='xavier'))(i) #120*120*3 -> 120*120*5\n    #x = (MaxPool2D(pool_size=(2, 2)))(x) # 120*120*5 -> 60*60*5\n    x = (Conv2D(3, kernel_size=(5, 5), activation='relu',padding='same',\n                     kernel_initializer='xavier'))(x) # 60*60*5 -> 60*60*5\n    \n    # \u52fe\u914d\u6d88\u5931\u306e\u5bfe\u7b56(\u6df1\u5c64\u5b66\u7fd2\u306e\u6642\u306e\u5c64\u304c\u6df1\u3044\u3068\u304d\u306b\u8d77\u3053\u308b\u554f\u984c)\n    # x = add([x,i])\n    # \u52fe\u914d\u6d88\u5931\u306e\u7d42\u4e86\n    \n    x = (MaxPool2D(pool_size=(3, 3),strides=(3,3)))(x) #60*60*5 -> 30*30*5\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    x = Conv2D(16, (5, 5), activation='relu', padding='same', kernel_initializer=xavier)(x)\n    x = MaxPool2D(pool_size=(5, 5), strides=(5,5))(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    x = (Flatten())(x) #30*30*5 -> 4500\n    #x = (Dense(600, activation='relu', \n    #                kernel_initializer='he_normal'))(x)\n    #x = (Dense(32094, activation='softmax'))(x) # category_name \t      32093\n\n    o1 = Dense(310,activation='softmax',name='family',kernel_initializer='xavier')(x)\n    o2 = concatenate([o1, x])\n    o2 = Dense(3678, activation='softmax', name='genus', kernel_initializer='xavier')(o2)\n    o3 = concatenate([o1, o2, x])\n    o3 = Dense(32094, activation='softmax',name='category_id', kernel_initializer='xavier')(o3)\n    y = Model(inputs=i,outputs=[o1,o2,o3])\n    opt = Adam(lr=lr, amsgrad=True)\n    y.compile(\n        #loss=keras.losses.sparse_categorical_crossentropy,\n        loss=['sparse_categorical_crossentropy', \n               'sparse_categorical_crossentropy', \n               'sparse_categorical_crossentropy'],\n        optimizer=opt,\n        metrics=['accuracy']\n    )\n              \n    return y\n              \nmodel = fg_model((120,120,3))\nmodel.summary()\nplot_model(model, to_file='full_model_plot.png',show_shapes=True,show_layer_names=True)","106bd27a":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(featurewise_center=False,\n                                      featurewise_std_normalization=False,\n                                      rotation_range=180,\n                                      width_shift_range=0.1,\n                                      height_shift_range=0.1,\n                                      zoom_range=0.2)\n\n\"\"\"\nImageDataGenerator -> \u753b\u50cf\u30c7\u30fc\u30bf\u306e\u89d2\u5ea6\u3092\u5909\u3048\u305f\u308a\u3001\u79fb\u52d5\u30fb\u56de\u8ee2\u3055\u305b\u305f\u308a\u3059\u308b\u3053\u3068\u306e\u3067\u304d\u308b\u4e9c\u7a2e\u753b\u50cf\u304c\u751f\u6210\u3067\u304d\u308b\n\n\n\"\"\"\ndisplay(datagen)","94c2968f":"m = train_df[['file_name','family','genus','category_id']]\ndisplay(m)","855d77e1":"#m = train_df[['file_name','family','genus','category_id']]\n## family genus\u304c\u6587\u5b57\u5217\u3068\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u6570\u5024\u306b\u7f6e\u304d\u63db\u3048\u308b\nfam = m.family.unique().tolist()\nm.family = m.family.map(lambda x:fam.index(x))\ngen = m.genus.unique().tolist()\nm.genus = m.genus.map(lambda x:gen.index(x))\ndisplay(m)","a630a64a":"\"\"\"\ntrain,verif = tts(m,test_size=0.2,shuffle=True,random_state=17)\ntrain = train[:40000]\nverif = verif[:10000]\nshape = (120,120,3)\nepochs = 2\nbatch_size = 32\n\nmodel = fg_model(shape,0.007)\n\nfor layers in model.layers:\n    if layers.name == 'genus' or layers.name == 'category_id':\n        layers.trainable = False\n\n#Train Family for 2 epochs\nmodel.fit_generator(train_datagen.flow_from_dataframe(dataframe=train,\n                                                      directory='..\/input\/herbarium-2020-fgvc7\/nybg2020\/train\/',\n                                                      x_col=\"file_name\",\n                                                      y_col=[\"family\", \"genus\", \"category_id\"],\n                                                      target_size=(120, 120),\n                                                      batch_size=batch_size,\n                                                      class_mode='multi_output'),\n                    validation_data=train_datagen.flow_from_dataframe(\n                        dataframe=verif,\n                        directory='..\/input\/herbarium-2020-fgvc7\/nybg2020\/train\/',\n                        x_col=\"file_name\",\n                        y_col=[\"family\", \"genus\", \"category_id\"],\n                        target_size=(120, 120),\n                        batch_size=batch_size,\n                        class_mode='multi_output'),\n                    epochs=epochs,\n                    steps_per_epoch=len(train)\/\/batch_size,\n                    validation_steps=len(verif)\/\/batch_size,\n                    verbose=1,\n                    workers=8,\n                    use_multiprocessing=False)\n\n#Reshuffle the inputs\ntrain, verif = tts(m, test_size=0.2, shuffle=True, random_state=17)\ntrain = train[:40000]\nverif = verif[:10000]\n\n#Make the Genus layer Trainable\nfor layers in model.layers:\n    if layers.name == 'genus':\n        layers.trainable = True\n        \n#Train Family and Genus for 2 epochs\nmodel.fit_generator(train_datagen.flow_from_dataframe(dataframe=train,\n                                                      directory='..\/input\/herbarium-2020-fgvc7\/nybg2020\/train\/',\n                                                      x_col=\"file_name\",\n                                                      y_col=[\"family\", \"genus\", \"category_id\"],\n                                                      target_size=(120, 120),\n                                                      batch_size=batch_size,\n                                                      class_mode='multi_output'),\n                    validation_data=train_datagen.flow_from_dataframe(\n                        dataframe=verif,\n                        directory='..\/input\/herbarium-2020-fgvc7\/nybg2020\/train\/',\n                        x_col=\"file_name\",\n                        y_col=[\"family\", \"genus\", \"category_id\"],\n                        target_size=(120, 120),\n                        batch_size=batch_size,\n                        class_mode='multi_output'),\n                    epochs=epochs,\n                    steps_per_epoch=len(train)\/\/batch_size,\n                    validation_steps=len(verif)\/\/batch_size,\n                    verbose=1,\n                    workers=8,\n                    use_multiprocessing=False)\n\n#Reshuffle the inputs\ntrain, verif = tts(m, test_size=0.2, shuffle=True, random_state=17)\ntrain = train[:40000]\nverif = verif[:10000]\n\n#Make the category_id layer Trainable\nfor layers in model.layers:\n    if layers.name == 'category_id':\n        layers.trainable = True\n        \n#Train them all for 2 epochs\nmodel.fit_generator(train_datagen.flow_from_dataframe(dataframe=train,\n                                                      directory='..\/input\/herbarium-2020-fgvc7\/nybg2020\/train\/',\n                                                      x_col=\"file_name\",\n                                                      y_col=[\"family\", \"genus\", \"category_id\"],\n                                                      target_size=(120, 120),\n                                                      batch_size=batch_size,\n                                                      class_mode='multi_output'),\n                    validation_data=train_datagen.flow_from_dataframe(\n                        dataframe=verif,\n                        directory='..\/input\/herbarium-2020-fgvc7\/nybg2020\/train\/',\n                        x_col=\"file_name\",\n                        y_col=[\"family\", \"genus\", \"category_id\"],\n                        target_size=(120, 120),\n                        batch_size=batch_size,\n                        class_mode='multi_output'),\n                    epochs=epochs,\n                    steps_per_epoch=len(train)\/\/batch_size,\n                    validation_steps=len(verif)\/\/batch_size,\n                    verbose=1,\n                    workers=8,\n                    use_multiprocessing=False)\n\"\"\"\n","037f1fef":"# \u4e00\u56de\u5206\u3051\u3066\u307f\u3066\u3082\u3044\u3044\u304b\u3082\u3057\u308c\u306a\u3044\n# fit_generator\u306e\u5f15\u6570\u3092\u305d\u308c\u305e\u308c\u8aac\u660e\u3059\u308b\n# fit_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n# generator -> \u30d0\u30c3\u30c1\u6bce\u306b\u751f\u6210\u3055\u308c\u305f\u30c7\u30fc\u30bf(\u8a13\u7df4\u7528\u306e\u30c7\u30fc\u30bf\u3068\u3057\u3066\u4f7f\u7528)\n# validation_data -> \u691c\u8a3c\u7528\u30c7\u30fc\u30bf\n","e521920f":"# Train, Test\u3088\u3046\u306b\u305d\u308c\u305e\u308c\u30c7\u30fc\u30bf\u306e\u7528\u610f\ntrain,test = tts(m,test_size=0.2,shuffle=True,random_state=17)\ntrain = train[:40000]\ntest = test[:10000]\nshape = (120,120,3)\nepochs = 3\nbatch_size = 32\n\nmodel = fg_model(shape,0.001)\n\nfor layer in model.layers:\n    layer.trainable = True\n\n# model.fit_generator(datagen.flow(x_train, y_train, batch_size=100),\n#                     steps_per_epoch=x_train.shape[0] \/\/ 100, epochs=30, validation_data=(x_valid, y_valid))\n\n# refarence URL about keras flow_from_dataframe\n# https:\/\/keras.io\/ja\/preprocessing\/image\/\n\n# refarence URL about validation_data\n# https:\/\/keras.io\/ja\/models\/model\/\n","ac734d19":"data_generator = datagen.flow_from_dataframe(\n                        dataframe=train,\n                        directory='..\/input\/herbarium-2020-fgvc7\/nybg2020\/train\/',\n                        x_col=\"file_name\",\n                        y_col=[\"family\", \"genus\", \"category_id\"],\n                        target_size=(120,120),\n                        batch_size=batch_size,\n                        class_mode='multi_output') #default\n                                                \n\ndata_validation = datagen.flow_from_dataframe(\n                        dataframe=test,\n                        directory='..\/input\/herbarium-2020-fgvc7\/nybg2020\/train\/',\n                        x_col=\"file_name\",\n                        y_col=[\"family\", \"genus\", \"category_id\"],\n                        target_size=(120, 120),\n                        batch_size=batch_size,\n                        class_mode='multi_output')","044edb4c":"model.fit_generator(generator=data_generator,validation_data=data_validation,steps_per_epoch=len(train)\/\/batch_size,\n                       epochs=epochs,validation_steps=len(test)\/\/batch_size,verbose=1,use_multiprocessing=False)","63993928":"# \u30e2\u30c7\u30eb\u306e\u8a55\u4fa1\nscore = model.evaluate(test.x_col, test.y_col,verbose=0)\nprint('Test Data loss:', score[0])\nprint('Test Data accuracy:', score[1])","93dd7255":"# This is same as refarence code\nbatch_size = 32\ntest_datagen = ImageDataGenerator(featurewise_center=False,\n                                  featurewise_std_normalization=False)\n\ngenerator = test_datagen.flow_from_dataframe(\n        dataframe = test_df.iloc[:10000], #Limiting the test to the first 10,000 items\n        directory = '..\/input\/herbarium-2020-fgvc7\/nybg2020\/test\/',\n        x_col = 'file_name',\n        target_size=(120, 120),\n        batch_size=batch_size,\n        class_mode=None,  # only data, no labels\n        shuffle=False)\n\nfamily, genus, category = model.predict_generator(generator, verbose=1)","21ebdc34":"### Data Generator","4acb059f":"##### For the first time, load data","1c269e51":"#### This is my model","0b7313ca":"#### This is refarence model","cee733a3":"### Train","6c74009b":"### Model Creation","36809ca7":"#### This train is my code","f022f26a":"#### Predict","d41b06e2":"#### This train is refarence code"}}