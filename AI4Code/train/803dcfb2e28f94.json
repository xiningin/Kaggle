{"cell_type":{"674f998b":"code","bd63b7a3":"code","e07990b1":"code","fb0ffcae":"code","07c97c68":"code","72c81190":"code","af15103f":"code","c72e40c3":"code","b40ccae5":"code","3d2cb240":"code","8174184e":"code","64944d0c":"code","3efd857f":"code","a9c9fe0e":"code","690ad032":"code","d408542d":"code","8de250db":"code","7efc5bb5":"code","813646ba":"code","9826c4c4":"code","15a46c67":"markdown","ea13e8ab":"markdown","5651c340":"markdown","7e46cd64":"markdown"},"source":{"674f998b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# data visualization\nimport matplotlib.pylab as plt\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 50, 18\nrcParams[\"axes.labelsize\"] = 16\nimport seaborn as sns\n# text processing library\nimport spacy\nimport re\nfrom gensim import corpora, models, similarities\n# model library\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n# ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","bd63b7a3":"from subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","e07990b1":"# read data\ndata = pd.read_csv(\"..\/input\/515k-hotel-reviews-data-in-europe\/Hotel_Reviews.csv\")\ndata.head()","fb0ffcae":"# print data columns\nfor columns in data.columns:\n    print(columns)","07c97c68":"# select relevent columns for sentiment of reviews\nreview_data = data[['Hotel_Name', 'Positive_Review', 'Negative_Review', 'Average_Score', 'Reviewer_Score']].copy()\nreview_data.head()","72c81190":"# Concatenating the positve review and negative review\nreview_data['reviews'] = review_data[['Positive_Review', 'Negative_Review']].apply(lambda x: ' '.join(x), axis = 1)\n#review_data.head()\nprint(review_data.loc[1, 'reviews'])","af15103f":"# checking distribution of review score\nprint(f\"Review score given by Customer: {review_data.Reviewer_Score.value_counts()}\")\nreview_data.Reviewer_Score.value_counts().plot(kind='bar', title='Count of Reviews', figsize = (15, 4))","c72e40c3":"# Rounding the Review Score to nearest integer\nreview_data['round_review_score'] = review_data.Reviewer_Score.apply(lambda x: np.ceil(x))\nreview_data.round_review_score.value_counts().plot(kind = 'bar', figsize=(16, 8), title = 'distribution of reviews')","b40ccae5":"# Selecting subset of data for speedup the computation.\nprint(f\"Before subseting, data size: {data.shape}\")\nreviews_df = review_data.sample(frac = 0.1, replace = False, random_state=42)\nprint(f\"After subseting, data size: {reviews_df.shape}\")\nreviews_df.head()","3d2cb240":"# function to clean and lemmatize text and remove stopwords\nfrom gensim.parsing.preprocessing import preprocess_string\nfrom gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_numeric\nfrom gensim.parsing.preprocessing import strip_multiple_whitespaces, strip_non_alphanum, remove_stopwords, strip_short\n\nCUSTOM_FILTERS = [lambda x: str(x), # encode in utf-8\n                  lambda x: x.lower(), # convert to lowercase\n                  # remove emails, urls etc\n                  lambda x: re.sub('[^\\s]*.com[^\\s]*', \"\", x),\n                  lambda x: re.sub('[^\\s]*www.[^\\s]*', \"\", x),\n                  lambda x: re.sub('[^\\s]*.co.uk[^\\s]*', \"\", x),\n                  # remove special charecter\n                  lambda x: re.sub('[^\\s]*[\\*]+[^\\s]*', \"\", x),\n                  lambda x: re.sub(r'\\([^)]*\\)', '', x),\n                  strip_tags, # remove html tags\n                  strip_punctuation, # replace punctuation with space\n                  strip_non_alphanum, # remove non-alphanumeric characters\n                  strip_numeric, # remove numbers\n                  remove_stopwords,# remove stopwords\n                  strip_short, # remove words less than minsize=4 characters long\n                  strip_multiple_whitespaces# remove repeating whitespaces\n                 ]\nnlp = spacy.load('en')\n\ndef text_preprocess(docs, logging=True):\n    docs = [preprocess_string(text, CUSTOM_FILTERS) for text in docs]\n    texts_out = []\n    for doc in docs:\n    # spacy processing-pipelines\n        doc = nlp((\" \".join(doc)),  # doc = text to tokenize => creates doc\n                  # disable parts of the language processing pipeline we don't need here to speed up processing\n                  disable=['ner', # named entity recognition\n                           'tagger', # part-of-speech tagger\n                           'textcat', # document label categorizer\n                          ])\n        texts_out.append([tok.lemma_ for tok in doc if tok.lemma_ != '-PRON-'])\n    return pd.Series(texts_out)\n\ntext_preprocess(reviews_df.reviews.iloc[10:15])","8174184e":"# apply text-preprocessing function to training set\n%time train_corpus = text_preprocess(reviews_df.reviews)","64944d0c":"# create ngrams\nngram_phraser_1 = models.Phrases(train_corpus, threshold=1)\nngram_phraser_2 = models.Phrases(train_corpus, threshold=10)\nngram_1 = models.phrases.Phraser(ngram_phraser_1)\nngram_2 = models.phrases.Phraser(ngram_phraser_2)\n#print example\nprint(ngram_1[train_corpus[0]])\nprint(ngram_2[train_corpus[0]])","3efd857f":"# apply n-gram model to corpus\ntexts_1 = [ngram_1[token] for token in train_corpus]\ntexts_2 = [ngram_2[token] for token in train_corpus]\n# adding it to dataframe\ntexts_1 = [' '.join(text) for text in texts_1]\ntexts_2 = [' '.join(text) for text in texts_2]\nreviews_df['ngram_1'] = texts_1\nreviews_df['ngram_2'] = texts_2\nreviews_df.head()","a9c9fe0e":"# visualizing relevent word in ngram_1\nfrom wordcloud import WordCloud\n\ntext = \"\"\nfor i in range(reviews_df.shape[0]):\n    text = \" \".join([text,reviews_df[\"ngram_1\"].values[i]])\n    \n\nwordcloud = WordCloud(background_color='white', width=600, height=300, max_font_size=50, max_words=40).generate(text)\nwordcloud.recolor(random_state=312)\nplt.imshow(wordcloud)\nplt.title(\"Wordcloud for reveiws \")\nplt.axis(\"off\")\nplt.show()","690ad032":"# Check class distribution\nfig = plt.figure(figsize=(15, 4))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\nreviews_df.round_review_score.value_counts().plot(kind='bar', title='Before sampling Review distribution', ax = ax1)\n\n# Dividing proportion of data by class\ncounts_category = reviews_df.round_review_score.value_counts().values\n\n# Divide by class\ndf_class_10 = reviews_df[reviews_df.round_review_score == 10.0]\ndf_class_8 = reviews_df[reviews_df.round_review_score == 8.0]\ndf_class_9 = reviews_df[reviews_df.round_review_score == 9.0]\ndf_class_7 = reviews_df[reviews_df.round_review_score == 7.0]\ndf_class_5 = reviews_df[reviews_df.round_review_score == 5.0]\ndf_class_4 = reviews_df[reviews_df.round_review_score == 4.0]\ndf_class_3 = reviews_df[reviews_df.round_review_score == 3.0]\n\n# random oversampling\ndf_class_10 = df_class_10.sample(counts_category[0], replace=False)\ndf_class_9 = df_class_9.sample(counts_category[0], replace=True)\ndf_class_8 = df_class_8.sample(counts_category[0], replace=True)\ndf_class_7 = df_class_7.sample(counts_category[0], replace=True)\ndf_class_5 = df_class_5.sample(counts_category[0], replace=True)\ndf_class_4 = df_class_4.sample(counts_category[0], replace=True)\ndf_class_3 = df_class_3.sample(counts_category[0], replace=True)\n\n# concatenate individual datafram\ndf_train_oversampled = pd.concat([df_class_10, df_class_9, df_class_8, df_class_7, df_class_5, df_class_4, df_class_3], axis=0)\n\n# Now, Check class distribution\n\ndf_train_oversampled.round_review_score.value_counts().plot(kind='bar', title='After sampling Review distribution', ax = ax2)\n#df_train_oversampled.job_type.value_counts().plot(kind='bar', title='Count (job_type)', ax=ax2)","d408542d":"# import model library\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB\nimport xgboost as xgb\nfrom sklearn.ensemble import VotingClassifier\n\n# represent features in countvectorizer for ngram_1\nvectorizer_1 = CountVectorizer()\nvectorizer_1.fit(df_train_oversampled.ngram_1)\n\n# split into test and train sets for ngram_1\nX_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(df_train_oversampled.ngram_1, df_train_oversampled.round_review_score, test_size=0.4)\n\n# represent features in countvectorizer for ngram_2\nvectorizer_2 = CountVectorizer()\nvectorizer_2.fit(df_train_oversampled.ngram_2)\n\n# split into test and train sets for ngram_2\nX_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(df_train_oversampled.ngram_2, df_train_oversampled.round_review_score, test_size=0.4)","8de250db":"# Build LogisticRegression Model\n# for ngram_1\nlr_1 = LogisticRegression()\nlr_1.fit(vectorizer_1.transform(X_train_1), y_train_1)\n# for ngram_2\nlr_2 = LogisticRegression()\nlr_2.fit(vectorizer_2.transform(X_train_2), y_train_2)\n\nprint('Logistic Regression Score on ngram_1 reviews: ', lr_1.score(vectorizer_1.transform(X_test_1), y_test_1))\nprint('Logistic Regression Score on ngram_2 reviews: ', lr_2.score(vectorizer_2.transform(X_test_2), y_test_2))\n\ny_1 = lr_1.predict(vectorizer_1.transform(X_test_1))\nprint(\"classification report on ngram_1 reviews:\\n \", classification_report(y_test_1, y_1))\n\ny_2 = lr_2.predict(vectorizer_2.transform(X_test_2))\nprint(\"classification report on ngram_2 reviews:\\n \", classification_report(y_test_2, y_2))\n","7efc5bb5":"# create private test data from sample\ntest_data = reviews_df.sample(frac = 0.05, replace = False, random_state=42)\ntest_data = test_data[['reviews', 'ngram_1', 'ngram_2']].copy()\n# pridiction on test data\n#lr_1.fit(vectorizer_1.transform(df_train_oversampled.ngram_1), df_train_oversampled.round_review_score)\nprediction_1 = lr_1.predict(vectorizer_1.transform(test_data.ngram_1))\n\n#lr_2.fit(vectorizer_2.transform(df_train_oversampled.ngram_2), df_train_oversampled.round_review_score)\nprediction_2 = lr_2.predict(vectorizer_2.transform(test_data.ngram_2))\n\nsample_test_score = pd.DataFrame({'Review_ngram_1':test_data.ngram_1, 'Review_ngram_2': test_data.ngram_2, 'score_ngram_1':prediction_1, 'score_ngram_2':prediction_2})\nsample_test_score.to_csv('sample_test_score.csv', index=False)\nsample_test_score.head()","813646ba":"# checking how the two pridicted score is different.\nfrom scipy import stats\n# Paired ttest\nfilter_data = sample_test_score.dropna(subset=['score_ngram_1', 'score_ngram_2'])\nttest, pval = stats.ttest_ind(filter_data.score_ngram_1, filter_data.score_ngram_2)\nif pval<0.5:\n  print(\"scores is almost same:\", ttest, pval)\nelse:\n  print(\"scores is different: \", ttest, pav)","9826c4c4":"fig = plt.figure(figsize=(15, 4))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\nsample_test_score.score_ngram_1.value_counts().plot(kind='bar', title='ngram_1 pridiction', ax = ax1)\nsample_test_score.score_ngram_2.value_counts().plot(kind='bar', title='ngram_2 pridiction', ax = ax2)\n","15a46c67":"#### Feature Engineering","ea13e8ab":"#### data sampling","5651c340":"### Text Preprocessing","7e46cd64":"#### Model Building"}}