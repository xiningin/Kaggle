{"cell_type":{"8522eecb":"code","87bf7792":"code","f4334b04":"code","98a35575":"code","db5502f2":"code","86997284":"code","5e9a570c":"code","35422110":"code","57265b30":"code","6a7816c8":"code","64ad65f5":"code","b957f754":"code","7bb87f6e":"code","9b78947f":"code","1691b8f6":"code","3487cea6":"code","dbdf94e6":"code","fee28b53":"code","09cee352":"code","264ccba8":"code","71492e1a":"code","b6b2c59f":"code","dc97ff52":"code","4c1ceded":"code","f37c4cd0":"code","b35c7671":"code","510c500a":"code","b689966e":"code","b900564b":"code","45ac2f69":"code","b604a20d":"code","02f540b1":"code","3c41a44f":"code","b868f167":"code","830e6e42":"code","94d0893f":"code","49559e56":"code","7578bbaf":"code","37a99480":"code","43eb16c5":"code","5acf74d7":"code","48520075":"code","fb9b3e56":"code","31387bce":"code","84e5f59b":"markdown","1bddbae9":"markdown","387462d2":"markdown","f2aff763":"markdown","798fff35":"markdown"},"source":{"8522eecb":"# import all libraries\n\nimport pandas as pd #basic\nimport numpy as np #basic\n# import pandas_profiling as pp #EDA\n\nfrom scipy.stats import shapiro,pearsonr #Stats\nimport scipy.stats as stats #Stats\n\nimport plotly.express as px #visualization\nimport plotly.graph_objs as go#visualization\nimport plotly.offline as py#visualization\nimport plotly.figure_factory as ff#visualization\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\nfrom sklearn.model_selection import train_test_split #Split data\nfrom sklearn import preprocessing #manipulate data\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report,precision_score,recall_score,f1_score\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import RadiusNeighborsClassifier\n\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.ensemble import BalancedRandomForestClassifier\n\n\nfrom imblearn.ensemble import BalancedBaggingClassifier\n\nfrom sklearn.ensemble import VotingClassifier\n\n\nimport xgboost  as xgb\nfrom catboost import CatBoostClassifier\nimport lightgbm as lgbm","87bf7792":"train= pd.read_csv('train_ctrUa4K.csv',index_col='Loan_ID')\nloans = train.copy()","f4334b04":"print (\"Rows     : \" ,loans.shape[0])\nprint (\"Columns  : \" ,loans.shape[1])\nprint (\"\\nFeatures : \\n\" ,loans.columns.tolist())\nprint (\"\\nMissing values :  \", loans.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",loans.nunique())","98a35575":"loans.describe(include='all').transpose()","db5502f2":"df_new=loans.copy()\n# pp.ProfileReport(df_new)","86997284":"# loans['total_income']=loans['ApplicantIncome']+loans['CoapplicantIncome']\n\nId_col = ['Loan_ID']\ntarget_col = ['Loan_Status']\nexclude = []    \ncat_cols = loans.nunique()[loans.nunique() < 6].keys().tolist()\ncat_cols = [x for x in cat_cols if x not in target_col + exclude]\nnum_cols = [x for x in loans.columns if x not in cat_cols+ target_col + Id_col + exclude]\nbin_cols   = loans.nunique()[loans.nunique() == 2].keys().tolist()\nmulti_cols = [i for i in cat_cols if i not in bin_cols+exclude]\n   \n\ndef fill_cat_cols(col_name,data_frame):\n    col_df=data_frame[col_name].value_counts().index\n    nans = data_frame[col_name].isna()\n    ##Key logic stats here\n    col_df=(data_frame[col_name].value_counts())\n    for x in col_df.index:\n        col_df[x]=(col_df[x]\/data_frame.shape[0])*100\n    weights_g = col_df.values\/100    \n    import random\n    ## use random.choices and give the respective distribution\n    replacement=random.choices(col_df.index,weights=weights_g, k=data_frame[col_name].isnull().sum())\n    ## use the above random values to keep in df again\n    data_frame.loc[nans,col_name] = replacement\n\ndef fill_median_num_cols(col_name,data_frame):\n    data_frame[col_name].fillna(value=data_frame[col_name].median(),inplace=True)\n\ndef drop_exclude_cols(col_name,data_frame):\n    data_frame.drop(col_name,axis=1,inplace=True)\n\ndef change_cat_num_data(data_frame,is_test,is_cat,is_num):\n    if is_test == 'Y':\n        bin_cols.remove('Loan_Status')\n    #Label encoding Binary columns\n    if is_cat == 'Y':\n        le = LabelEncoder()\n        for i in bin_cols :\n#             print(i)\n            data_frame[i] = le.fit_transform(data_frame[i])\n        data_frame = pd.get_dummies(data = data_frame,columns = multi_cols )\n    if is_num == 'Y':\n        std = StandardScaler()\n        scaled = std.fit_transform(data_frame[num_cols])\n        scaled = pd.DataFrame(scaled,columns=num_cols,index=data_frame.index)\n        df_og = data_frame.copy()\n        data_frame = data_frame.drop(columns = num_cols,axis = 1)\n        data_frame = data_frame.merge(scaled,left_index=True,right_index=True,how = \"left\")\n    return data_frame","5e9a570c":"for x in cat_cols:\n    fill_cat_cols(x,loans)\nfor x in ['LoanAmount','Loan_Amount_Term']:\n    fill_median_num_cols(x,loans)\nfor x in exclude:\n    drop_exclude_cols(x,loans)   \nloans=change_cat_num_data(loans,'N','Y','Y')    ","35422110":"\n##seperating dependent and independent variables\n\nX = loans.drop('Loan_Status',axis=1)\ny = loans['Loan_Status']\n\n##Split the train and test with 30%ratio\n\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=.3, random_state=111,stratify =y)\n\n","57265b30":"def knn_classifier(k):\n    knn = KNeighborsClassifier(algorithm = 'auto', leaf_size = 30, metric_params = None, n_jobs = 1, n_neighbors = k, p = 2, weights = 'uniform')\n    knn.fit(X_train, y_train)\n    y_train_pred = knn.predict(X_train)\n    y_test_pred = knn.predict(X_test)\n    print('Train Accuracy score for k ={}  is  {:.3f}'.format(k, accuracy_score(y_train, y_train_pred)))\n    print('Test Accuracy score for k ={}  is  {:.3f}'.format(k, accuracy_score(y_test, y_test_pred)))\nfor k in range(1, 20, 2):\n    knn_classifier(k)","6a7816c8":"knn = KNeighborsClassifier(algorithm = 'auto', leaf_size = 30, metric = 'minkowski', metric_params = None, n_jobs = 1, n_neighbors = 5, p = 2, weights = 'uniform')\nknn.fit(X_train, y_train)\ny_train_pred = knn.predict(X_train)\ny_test_pred = knn.predict(X_test)\nprint('Train Accuracy score for k ={}  is  {:.3f}'.format(5, accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy score for k ={}  is  {:.3f}'.format(5, accuracy_score(y_test, y_test_pred)))","64ad65f5":"## Logistic regression\n\nlog_reg = LogisticRegression(random_state=1,solver='liblinear')\nlog_reg.fit(X_train, y_train)\ny_train_pred = log_reg.predict(X_train)\ny_test_pred = log_reg.predict(X_test)\nprint('Train Accuracy score LogisticRegression  is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy scoreLogisticRegression is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))","b957f754":"#GaussianNB\ngb = GaussianNB()\ngb.fit(X_train, y_train)\ny_train_pred = gb.predict(X_train)\ny_test_pred = gb.predict(X_test)\nprint('Train Accuracy score GaussianNB  is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy GaussianNB is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))","7bb87f6e":"\n#DecisionTreeClassifier\ndtc = DecisionTreeClassifier(max_depth=3,min_samples_leaf=3)\ndtc.fit(X_train, y_train)\ny_train_pred = dtc.predict(X_train)\ny_test_pred = dtc.predict(X_test)\nprint('Train Accuracy score DecisionTreeClassifier  is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy DecisionTreeClassifier is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))","9b78947f":"#RadiusNeighborsClassifier\n\nrnc = RadiusNeighborsClassifier(radius=1.678,outlier_label =0)\nrnc.fit(X_train, y_train)\ny_train_pred = rnc.predict(X_train)\ny_test_pred = rnc.predict(X_test)\nprint('Train Accuracy score RadiusNeighborsClassifier  is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy RadiusNeighborsClassifier is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))","1691b8f6":"#BaggingClassifier\nk_bgc = BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=5),n_estimators=100,max_samples=.7,max_features=.6,random_state=1)\nk_bgc.fit(X_train,y_train)\ny_train_pred = k_bgc.predict(X_train)\ny_test_pred = k_bgc.predict(X_test)\nprint('Train Accuracy score  is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy   is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))","3487cea6":"lr_bgc = BaggingClassifier(base_estimator=log_reg,max_features=.6,n_estimators=100,max_samples=.7,random_state=1)\nlr_bgc.fit(X_train,y_train)\ny_train_pred = lr_bgc.predict(X_train)\ny_test_pred = lr_bgc.predict(X_test)\nprint('Train Accuracy score   is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy score  is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))","dbdf94e6":"gb_bgc = BaggingClassifier(base_estimator=gb,max_features=.6,n_estimators=35,max_samples=.7,random_state=1)\ngb_bgc.fit(X_train,y_train)\ny_train_pred = gb_bgc.predict(X_train)\ny_test_pred = gb_bgc.predict(X_test)\nprint('Train Accuracy score   is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy score  is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))","fee28b53":"dt_bgc = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=5),max_features=.6,n_estimators=35,max_samples=.7,random_state=1)\ndt_bgc.fit(X_train,y_train)\ny_train_pred = dt_bgc.predict(X_train)\ny_test_pred = dt_bgc.predict(X_test)\nprint('Train Accuracy score   is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy score  is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))","09cee352":"lr_abc = AdaBoostClassifier(base_estimator=LogisticRegression(random_state=1,solver='liblinear'),random_state=1,learning_rate=1,n_estimators=50)\nlr_abc.fit(X_train,y_train)\ny_train_pred = lr_abc.predict(X_train)\ny_test_pred = lr_abc.predict(X_test)\nprint('Train Accuracy score   is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy score  is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))","264ccba8":"gb_abc = AdaBoostClassifier(base_estimator=GaussianNB(),random_state=1,learning_rate=.5,n_estimators=75)\ngb_abc.fit(X_train,y_train)\ny_train_pred = gb_abc.predict(X_train)\ny_test_pred = gb_abc.predict(X_test)\nprint('Train Accuracy score  is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy score is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))","71492e1a":"dtc_abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=5),random_state=1,learning_rate=.7,n_estimators=100)\ndtc_abc.fit(X_train,y_train)\ny_train_pred = dtc_abc.predict(X_train)\ny_test_pred = dtc_abc.predict(X_test)\nprint('Train Accuracy score   is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy score  is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))","b6b2c59f":"abc = AdaBoostClassifier(random_state=1,learning_rate=.5,n_estimators=75)\nabc.fit(X_train,y_train)\ny_train_pred = abc.predict(X_train)\ny_test_pred = abc.predict(X_test)\nprint('Train Accuracy score   is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy score  is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))","dc97ff52":"\ngbc = GradientBoostingClassifier(learning_rate=1,n_estimators=100,random_state=50, min_samples_leaf=9)\ngbc.fit(X_train,y_train)\ny_train_pred = gbc.predict(X_train)\ny_test_pred = gbc.predict(X_test)\nprint('Train Accuracy score    is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy score  is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))","4c1ceded":"\nrc = RandomForestClassifier(n_estimators =20,random_state=1,max_depth=3, min_samples_leaf=9)\nrc.fit(X_train,y_train)\ny_train_pred = rc.predict(X_train)\ny_test_pred = rc.predict(X_test)\nprint('Train Accuracy score    is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy score is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))","f37c4cd0":"\nbrf = BalancedRandomForestClassifier(max_depth=3,n_estimators=50, random_state=0)\nbrf.fit(X_train,y_train)\ny_train_pred = brf.predict(X_train)\ny_test_pred = brf.predict(X_test)\nprint('Train Accuracy score is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy score is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))","b35c7671":"\nbbc_lr = BalancedBaggingClassifier(base_estimator=log_reg, random_state=0)\nbbc_lr.fit(X_train,y_train)\ny_train_pred = bbc_lr.predict(X_train)\ny_test_pred = bbc_lr.predict(X_test)\nprint('Train Accuracy score is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy score is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))","510c500a":"\nbbc_k = BalancedBaggingClassifier(base_estimator=knn, random_state=0)\nbbc_k.fit(X_train,y_train)\ny_train_pred = bbc_k.predict(X_train)\ny_test_pred = bbc_k.predict(X_test)\nprint('Train Accuracy score is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy score is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))","b689966e":"\nbbc_gb = BalancedBaggingClassifier(base_estimator=gb, random_state=0)\nbbc_gb.fit(X_train,y_train)\ny_train_pred = bbc_gb.predict(X_train)\ny_test_pred = bbc_gb.predict(X_test)\nprint('Train Accuracy score is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy score is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))","b900564b":"\nbbc_dt = BalancedBaggingClassifier(base_estimator=dtc, random_state=0)\nbbc_dt.fit(X_train,y_train)\ny_train_pred = bbc_dt.predict(X_train)\ny_test_pred = bbc_dt.predict(X_test)\nprint('Train Accuracy score is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy score is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))","45ac2f69":"\nbbc_R = BalancedBaggingClassifier(base_estimator=rnc, random_state=0)\nbbc_R.fit(X_train,y_train)\ny_train_pred = bbc_R.predict(X_train)\ny_test_pred = bbc_R.predict(X_test)\nprint('Train Accuracy score is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy score is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))","b604a20d":"def get_vc_estimators(n):\n    est = []\n    vc_df=model_performances.sort_values('Test_Accuracy_score',ascending=False).head(n)['Model_Name']\n    for x in vc_df.values:\n        tup=(x,x)\n        est.append(tup)\n    print(est)\n# get_vc_estimators(10)    ","02f540b1":"vc = VotingClassifier(\n    estimators=[\n        ('abc', abc), ('brf', brf), ('dtc', dtc),\n                ('lr_bgc', lr_bgc), ('gb_bgc', gb_bgc),\n        ('dt_bgc', dt_bgc), ('lr_abc', lr_abc), ('log_reg', log_reg), \n                ('rc', rc)\n               ]\n    ,voting='soft')\nvc.fit(X_train,y_train)\ny_train_pred = vc.predict(X_train)\ny_test_pred = vc.predict(X_test)\nprint('Train Accuracy score is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy score is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))","3c41a44f":"def model_report(\n    m_type,\n    model,\n    model_name,\n    actual_name\n    ):\n    model.fit(X_train, y_train)\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n    df = pd.DataFrame({\n        'Model_Type': [m_type],\n        'Model_Name': [model_name],\n        'Model_act_name':[actual_name],\n        'Train_Accuracy_score': [accuracy_score(y_train, y_train_pred)],\n        'Test_Accuracy_score': [ accuracy_score(y_test, y_test_pred)],\n        })    \n    return df\nmodel1=model_report('Base',knn,'knn','KNN-5')\nmodel2=model_report('Base',log_reg,'log_reg','Logis Reg')\nmodel3=model_report('Base',dtc,'dtc','Dec Tree')\nmodel4=model_report('Base',gb,'gb','GB')\nmodel5=model_report('Base',rnc,'rnc','RNC')\n\nmodel6=model_report('Bagging',k_bgc,'k_bgc','BGC-KNN-5')\nmodel7=model_report('Bagging',lr_bgc,'lr_bgc','BGC-LR')\nmodel8=model_report('Bagging',gb_bgc,'gb_bgc','BGC-GB')\nmodel9=model_report('Bagging',dt_bgc,'dt_bgc','BGC-DT')\n\nmodel10=model_report('Boosting',lr_abc,'lr_abc','ABC-LR')\nmodel11=model_report('Boosting',gb_abc,'gb_abc','ABC-GB')\nmodel12=model_report('Boosting',dtc_abc,'dtc_abc','ABC-DT')\nmodel13=model_report('Boosting',abc,'abc','ABC-Def')\n\nmodel14=model_report('Random Forest',rc,'rc','RFC')\nmodel15=model_report('Gradient Boost',gbc,'gbc','GBC')\n\nmodel16=model_report('Balance',brf,'brf','BAL-RF')\nmodel17=model_report('Balance',bbc_lr,'bbc_lr','BAL-LR')\nmodel18=model_report('Balance',bbc_k,'bbc_k','BAL-KNN-5')\nmodel19=model_report('Balance',bbc_dt,'bbc_dt','BAL-DT')\nmodel20=model_report('Balance',bbc_gb,'bbc_gb','BAL-GB')\nmodel21=model_report('Balance',bbc_R,'bbc_R','BAL-Rad-C')\n\nmodel22=model_report('Voting',vc,'vc','Voting Classifier')\n\n\n#concat all models\nmodel_performances = pd.concat([model1,model2,model3,\n                                model4,model5,model6,\n                                model7,model8,model9,\n                                model10,model11,model12,\n                                model13,model14,model15,\n                                model16,model17,model18,\n                                model19,model20,model21,model22\n                               ],axis = 0).reset_index()\n\nmodel_performances = model_performances.drop(columns = \"index\",axis =1)\n","b868f167":"D_train = xgb.DMatrix(X_train, label=y_train)\nD_test = xgb.DMatrix(X_test, label=y_test)\nparam = {\n    'eta': 0.1,\n    'max_depth': 3,\n    'objective': 'multi:softprob',\n    'num_class': 3,\n    }\nsteps = 10  # The number of training iterations\nxgb_b = xgb.train(param, D_train, steps)\npreds = xgb_b.predict(D_test)\npreds_train = xgb_b.predict(D_train)\nbest_preds_train = np.asarray([np.argmax(line) for line in preds_train])\nbest_preds_test = np.asarray([np.argmax(line) for line in preds])\nprint ('Train Accuracy score is  {:.3f}'.format(accuracy_score(y_train,best_preds_train)))\nprint ('Test Accuracy score is  {:.3f}'.format(accuracy_score(y_test,best_preds_test)))\nmodel_performances=model_performances.append(pd.DataFrame({\n    'Model_Type': ['XG Boost Basic'],\n    'Model_Name': ['xgb_b'],\n    'Model_act_name': ['xgb_b'],\n    'Train_Accuracy_score': [accuracy_score(y_train,\n                             best_preds_train)],\n    'Test_Accuracy_score': [accuracy_score(y_test, best_preds_test)],\n    }), ignore_index=True)\n","830e6e42":"cat = CatBoostClassifier(\n    iterations=10,\n    learning_rate=.5,\n    depth=5,\n    eval_metric='Accuracy',\n    use_best_model=True,\n    random_seed=42,\n    loss_function=None,\n    )\n\n# Fit model\n\ncat_b = cat.fit(X_train, y_train, eval_set=(X_test, y_test))\ny_train_pred = cat_b.predict(X_train)\ny_test_pred = cat_b.predict(X_test)\nprint ('Train Accuracy score is  {:.3f}'.format(accuracy_score(y_train,\n        y_train_pred)))\nprint ('Test Accuracy score is  {:.3f}'.format(accuracy_score(y_test,\n        y_test_pred)))\nmodel_performances = model_performances.append(pd.DataFrame({\n    'Model_Type': ['CAT Boost Basic'],\n    'Model_Name': ['cat_b'],\n    'Model_act_name': ['cat_b'],\n    'Train_Accuracy_score': [accuracy_score(y_train, y_train_pred)],\n    'Test_Accuracy_score': [accuracy_score(y_test, y_test_pred)],\n    }), ignore_index=True)","94d0893f":"lgm_b = lgbm.LGBMClassifier(n_estimators=50,random_state=100)\nlgm_b.fit(X_train, y_train)\ny_test_pred = lgm_b.predict(X_test)\ny_train_pred = lgm_b.predict(X_train)\nprint('Train Accuracy score is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy score is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))\nmodel_performances=model_performances.append(pd.DataFrame({\n    'Model_Type': ['LGBM Basic'],\n    'Model_Name': ['lgm_b'],\n    'Model_act_name': ['lgm_b'],\n    'Train_Accuracy_score': [accuracy_score(y_train,\n                             y_train_pred)],\n    'Test_Accuracy_score': [accuracy_score(y_test, y_test_pred)],\n    }), ignore_index=True)","49559e56":"# Work with missing values ","7578bbaf":"df_XGB= train.copy()\ndf_XGB['total_income']=df_XGB['ApplicantIncome']+df_XGB['CoapplicantIncome']\nfor x in cat_cols:\n    fill_cat_cols(x,df_XGB)\nfor x in exclude:\n    drop_exclude_cols(x,df_XGB)\ndf_XGB=change_cat_num_data(df_XGB,'N','Y','N')     \n\n##seperating dependent and independent variables\n\nX = df_XGB.drop('Loan_Status',axis=1)\ny = df_XGB['Loan_Status']\n\n##Split the train and test with 30%ratio\n\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=.3, random_state=111,stratify =y)\n\nD_train = xgb.DMatrix(X_train, label=y_train)\nD_test = xgb.DMatrix(X_test, label=y_test)\nparam = {\n    'eta': 0.1,\n    'max_depth': 3,\n    'objective': 'multi:softprob',\n    'num_class': 3,\n    }\nsteps = 10  # The number of training iterations\nxgb_n = xgb.train(param, D_train, steps)\npreds = xgb_n.predict(D_test)\npreds_train = xgb_n.predict(D_train)\nbest_preds_train = np.asarray([np.argmax(line) for line in preds_train])\nbest_preds_test = np.asarray([np.argmax(line) for line in preds])\nprint ('Train Accuracy score is  {:.3f}'.format(accuracy_score(y_train,best_preds_train)))\nprint ('Test Accuracy score is  {:.3f}'.format(accuracy_score(y_test,best_preds_test)))\nmodel_performances=model_performances.append(pd.DataFrame({\n    'Model_Type': ['XGBoost Adv'],\n    'Model_Name': ['xgb_n'],\n    'Model_act_name': ['adv'],\n    'Train_Accuracy_score': [accuracy_score(y_train,\n                             best_preds_train)],\n    'Test_Accuracy_score': [accuracy_score(y_test, best_preds_test)],\n    }), ignore_index=True)\n","37a99480":"df_LGM= train.copy()\n# df_LGM['total_income']=df_LGM['ApplicantIncome']+df_LGM['CoapplicantIncome']\n# for x in exclude:\n#     drop_exclude_cols(x,df_LGM)\ndf_LGM=change_cat_num_data(df_LGM,'N','N','N')  \n\n##seperating dependent and independent variables\nfor x in cat_cols:\n    df_LGM[x]=df_LGM[x].astype('category')\n\n\nX = df_LGM.drop('Loan_Status',axis=1)\ny = df_LGM['Loan_Status']\n\n##Split the train and test with 30%ratio\n\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=.3, random_state=111,stratify =y)\n\nlgm_c = lgbm.LGBMClassifier(n_estimators=100,random_state=100,max_depth=3,learning_rate=.2,num_leaves=10,importance_type='gain')\nlgm_c.fit(X_train, y_train,categorical_feature=cat_cols)\ny_test_pred = lgm_c.predict(X_test)\ny_train_pred = lgm_c.predict(X_train)\nprint('Train Accuracy score is  {:.3f}'.format( accuracy_score(y_train, y_train_pred)))\nprint('Test Accuracy score is  {:.3f}'.format( accuracy_score(y_test, y_test_pred)))\nmodel_performances=model_performances.append(pd.DataFrame({\n    'Model_Type': ['LGBM Cat Cols'],\n    'Model_Name': ['lgm_c'],\n    'Model_act_name': ['adv'],\n    'Train_Accuracy_score': [accuracy_score(y_train,\n                             y_train_pred)],\n    'Test_Accuracy_score': [accuracy_score(y_test, y_test_pred)],\n    }), ignore_index=True)\n","43eb16c5":"df_cat= train.copy()\n\ndf_cat=change_cat_num_data(df_cat,'N','N','N')  \n\ndf_cat['Credit_History']=df_cat['Credit_History'].fillna(30)\ndf_cat['Credit_History']=df_cat['Credit_History'].astype('int')\ndf_cat['Credit_History']=df_cat['Credit_History'].astype('str')\ndf_cat['Credit_History']=df_cat['Credit_History'].str.replace('30','XXX')\n#for catboost only\nfor x in cat_cols:\n    df_cat[x]=df_cat[x].fillna('XXX')\n\n##seperating dependent and independent variables\nfor x in cat_cols:\n    df_cat[x]=df_cat[x].astype('object')\n\nX = df_cat.drop('Loan_Status',axis=1)\ny = df_cat['Loan_Status']\n\n##Split the train and test with 30%ratio\n\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=.3, random_state=111,stratify =y)\n\ncat = CatBoostClassifier(\n    iterations=10,\n    learning_rate=.5,\n    depth=5,\n    random_seed=42,\n    loss_function=None,\n    )\n\n# Fit model\n\ncat_m = cat.fit(X_train, y_train, cat_features=cat_cols)\ny_train_pred = cat_m.predict(X_train)\ny_test_pred = cat_m.predict(X_test)\nprint ('Train Accuracy score is  {:.3f}'.format(accuracy_score(y_train,\n        y_train_pred)))\nprint ('Test Accuracy score is  {:.3f}'.format(accuracy_score(y_test,\n        y_test_pred)))\nmodel_performances = model_performances.append(pd.DataFrame({\n    'Model_Type': ['CAT Boost Cat Cols'],\n    'Model_Name': ['cat_m'],\n    'Model_act_name': ['adv'],\n    'Train_Accuracy_score': [accuracy_score(y_train, y_train_pred)],\n    'Test_Accuracy_score': [accuracy_score(y_test, y_test_pred)],\n    }), ignore_index=True)","5acf74d7":"model_performances['diff']=(model_performances['Train_Accuracy_score']-model_performances['Test_Accuracy_score'])*100\nmodel_performances.sort_values('Test_Accuracy_score',ascending=False)\n\n\n# good_models=model_performances[(model_performances['Model_act_name'] != 'adv')&(model_performances['Test_Accuracy_score']>=0.77)]['Model_Name'].values","48520075":"validate= pd.read_csv('test_lAUu6dG.csv',index_col='Loan_ID')\n#generate prediction on test data\n# validate['total_income']=validate['ApplicantIncome']+validate['CoapplicantIncome']\nfor x in cat_cols:\n    fill_cat_cols(x,validate)\nfor x in ['LoanAmount','Loan_Amount_Term']:\n    fill_median_num_cols(x,validate)\nfor x in exclude:\n    drop_exclude_cols(x,validate)   \nvalidate=change_cat_num_data(validate,'Y','Y','Y')    ","fb9b3e56":"dfcat= pd.read_csv('test_lAUu6dG.csv',index_col='Loan_ID')\n\ndfcat=change_cat_num_data(dfcat,'N','N','N')  \n\ndfcat['Credit_History']=dfcat['Credit_History'].fillna(30)\ndfcat['Credit_History']=dfcat['Credit_History'].astype('int')\ndfcat['Credit_History']=dfcat['Credit_History'].astype('str')\ndfcat['Credit_History']=dfcat['Credit_History'].str.replace('30','XXX')\n#for catboost only\nfor x in cat_cols:\n    dfcat[x]=dfcat[x].fillna('XXX')\n\n##seperating dependent and independent variables\nfor x in cat_cols:\n    dfcat[x]=dfcat[x].astype('object')","31387bce":"predc=lr_bgc.predict(validate)\noutput = pd.DataFrame({'Loan_ID': validate.index,\n                       'Loan_Status': predc})\noutput['Loan_Status']=output['Loan_Status'].replace(1,'Y')\noutput['Loan_Status']=output['Loan_Status'].replace(0,'N')\n# output.to_csv(r'C:\\Users\\samu0315\\Desktop\\Mine\\Personal\\gl_aiml\\supervised learning\\GIT Practice\\Classification\\Binary Classfication\\AV Comp bank\\sol.csv', index=False)","84e5f59b":"# Work in Progress","1bddbae9":"# your rank would be with in 1000","387462d2":"# Loan Prediction Competetion - Analytics Vidhya\nhttps:\/\/datahack.analyticsvidhya.com\/contest\/practice-problem-loan-prediction-iii\/","f2aff763":"# Load Data","798fff35":"###  About Company\nDream Housing Finance company deals in all home loans. They have presence across all urban, semi urban and rural areas. Customer first apply for home loan after that company validates the customer eligibility for loan.\n\n### Problem\nCompany wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers. Here they have provided a partial data set."}}