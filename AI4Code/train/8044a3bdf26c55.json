{"cell_type":{"db7ea8fd":"code","fb6b0fff":"code","b5561961":"code","f9eb319a":"code","89c08984":"code","00c97082":"code","43419442":"code","5da1b3ce":"code","58c74a1a":"code","fb80d1b1":"code","53553e9b":"code","0d4d4683":"code","be12acd8":"code","ec99bd56":"code","47f44802":"code","c88c5910":"code","59d134fb":"code","03ec775b":"code","11b96cac":"code","a2bb292f":"code","3a568347":"code","952cfc88":"code","45984191":"code","a9036928":"code","962f8ca3":"code","5766e3f7":"code","e23e1222":"code","9bdcc5d1":"code","8f9f0e71":"code","f83ee3c0":"code","64e9d389":"code","3a2adae3":"code","743e915c":"code","bd974be4":"code","915ea676":"code","60ced576":"code","53013d70":"code","c37c01c5":"code","04c34383":"markdown","7e2213d6":"markdown","9b5b75df":"markdown","f3f73e64":"markdown","9526d3f8":"markdown","b929a51a":"markdown","4ad7982a":"markdown","b4dd68e0":"markdown","23c380b6":"markdown","4375775c":"markdown","7de8810a":"markdown","9f490566":"markdown","90e1a9fe":"markdown","f1ecf6b5":"markdown","461f1c66":"markdown","750c072b":"markdown","1ee0d1f4":"markdown","8cd76a0d":"markdown","53f6930b":"markdown","901fb43b":"markdown","6988487f":"markdown","a585e121":"markdown","1c69a336":"markdown","3f054702":"markdown","624b427f":"markdown","f0f86fe6":"markdown","8a30fea1":"markdown","ab76c5f5":"markdown","8e559991":"markdown","2b96d5ae":"markdown","b42c2465":"markdown"},"source":{"db7ea8fd":"# Semilla\nSEED = 333\n\n# Exportar CSV\nfrom IPython.display import HTML\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}<\/a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# Preparamos el lematizado\nfrom nltk.stem.porter import PorterStemmer\nporter_stemmer = PorterStemmer()\n\n# Lematizar un string\nimport re\ndef stemming_tokenizer(str_input):\n    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n    words = [porter_stemmer.stem(word) for word in words]\n    return words\n\n# TF-IDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\n\n## Columnas de guardado para los algortimos \nCOLUMNS = ['mean_fit_time','std_fit_time','mean_test_neg_log_loss','std_test_neg_log_loss','rank_test_neg_log_loss',\n           'mean_test_accuracy','rank_test_accuracy',\n           'mean_test_f1_macro','rank_test_f1_macro',\n           'mean_test_roc_auc_ovr','rank_test_roc_auc_ovr']\n\n# Funcion de guardado de resultados que es un subconjunto de cv_results. \n# Guarda los resultados de los parametros del algoritmo y las metricas que le pasamos como parametro.\ndef save_results(gs,params_to_evaluate,columns=COLUMNS):\n    aux = pd.DataFrame(gs.cv_results_)\n    gs_res = pd.DataFrame()\n    for col in params_to_evaluate:\n        gs_res[col] = aux[col]\n    for col in columns:\n        gs_res[col] = aux[col]\n    return gs_res\n\n\n# Habilita que se pueda graficar directamente desde el dataframe\nimport cufflinks as cf\nimport plotly.express as px\ncf.set_config_file(offline=True)","fb6b0fff":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\ntrain_variants_df = pd.read_csv(\"..\/input\/data-c\/training_variants\", engine='python')\ntrain_txt_df = pd.read_csv(\"..\/input\/data-c\/training_text\", sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\ntrain_txt_df['Class'] = train_variants_df['Class']\ntrain_txt_df.sample(10,random_state=SEED)","b5561961":"# Inicializamos el dataframe que vamos a utilizar\nW = pd.DataFrame()\nword_clod = pd.DataFrame()\n\n# A\u00f1adimos una columna que nos indica el tama\u00f1o del texto de cada instancia\nW['Text_count']  = train_txt_df[\"Text\"].apply(lambda x: len(str(x).split()))\n\n# Copiamos la clase y el texto\nW['Class'] = train_txt_df['Class'].copy()\nW['Text'] = train_txt_df[\"Text\"].copy()\n\n# Aplicamos el lematizado a cada instancia de texto para mostrarlo\n#word_clod['Text'] = train_txt_df[\"Text\"].apply(lambda x: stemming_tokenizer(str(x)))\n#word_clod['Class'] = train_txt_df['Class'].copy()\n\n# Nos quedamos con las instancias que no tengan el texto nulo\nW = W[W['Text_count']!=1]\n\n# Mostramos el dataframe\nW.sample(10,random_state=SEED)","f9eb319a":"#W.to_csv('W.csv')\n#create_download_link(filename='W.csv')","89c08984":"\"\"\"fig = px.violin(W ,y=\"Text_count\", color=\"Class\")\nfig.update_yaxes(automargin=True)\nfig.show()\"\"\"","00c97082":"\"\"\"# data prepararion\nfrom plotly import tools\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom textwrap import wrap\n\n# Primero creamos una lista de palabras personalizadas las cuales no deben aportar mucha informarcion a la hora de clasificar\ncustom_words = [\"fig\", \"figure\", \"et\", \"al\", \"al.\", \"also\",\n                \"data\", \"analyze\", \"study\", \"table\", \"using\",\n                \"method\", \"result\", \"conclusion\", \"author\", \n                \"find\", \"found\", \"show\"]\nstop_words = set(list(STOPWORDS) + custom_words)\n\ndef show_wordcloud(data,title):\n    plt.subplots(figsize=(8,8))\n    wordcloud = WordCloud(\n                          background_color='white',\n                          width=512,\n                          height=384,\n                          stopwords = stop_words,\n                          random_state = SEED\n                         ).generate(str(data.Text))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.savefig('graph.png')\n    plt.title(title,fontsize = 'x-large',color = 'w')\n    plt.show()\n\nshow_wordcloud(word_clod,\"Palabras Frecuentes\")\"\"\"","43419442":"\"\"\"def word_agroupped(data, att, value):\n    subset = data[data[att] == value]\n    show_wordcloud(subset,\"Palabras Frecuentes\"+ ' '+ att + ' ' + str(value))\n\nfor i in range(1,10):\n    word_agroupped(word_clod,'Class',i)\"\"\"","5da1b3ce":"from sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\n\ndef plot_tfidf(data_tfidf,x=\"Word\",y=\"TF-IDF\",title=\"Palabras Clave \",num_instances=50):\n    fig = px.bar(data_tfidf.head(num_instances), x=x, y=y,title = title)\n    fig.show()\n    \ndef train_tfidf(data = W,max_features=1000,tokenizer = stemming_tokenizer,ngram_range=(1,1)):\n    # Creamos el tfidf\n    count_vectorizer = TfidfVectorizer(analyzer=\"word\", tokenizer=tokenizer,stop_words= 'english', max_features=max_features,ngram_range=ngram_range)\n    tfidf = count_vectorizer.fit_transform(data['Text'].values.astype('U'))\n    \n    # Lo transformamos a dataframe\n    df_tfidf = pd.DataFrame(tfidf[0].T.todense(), columns=[\"TF-IDF\"])\n    df_tfidf[\"Word\"] = count_vectorizer.get_feature_names()\n    \n    return df_tfidf.sort_values('TF-IDF', ascending=False)\n\n\ndef plot_subset(fixed_value,data_tfidf=W,att='Class',ngram_range=(1,2)):\n    # Subconjunto a pintar\n    subset = data_tfidf[data_tfidf[att] == fixed_value]\n    \n    # TF-IDF\n    df_tfidf = train_tfidf(subset,ngram_range=ngram_range)\n    \n    # Pintar\n    plot_tfidf(df_tfidf,title=\"Palabras Clave Clase\"+\" \"+str(fixed_value))","58c74a1a":"#df_tfidf = train_tfidf(ngram_range=(1,1))\n#plot_tfidf(df_tfidf)","fb80d1b1":"\"\"\"for i in range(1,10):\n    plot_subset(i,ngram_range=(1,1))\"\"\"","53553e9b":"#df_tfidf = train_tfidf(ngram_range=(2,2))\n#plot_tfidf(df_tfidf)","0d4d4683":"\"\"\"for i in range(1,10):\n    plot_subset(i,ngram_range=(2,2))\"\"\"","be12acd8":"#df_tfidf = train_tfidf(ngram_range=(1,2))\n#plot_tfidf(df_tfidf)","ec99bd56":"\"\"\"for i in range(1,10):\n    plot_subset(i,ngram_range=(1,2))\"\"\"","47f44802":"# Separacion training\/test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(W['Text'], W['Class'], test_size=0.2, random_state=SEED)","c88c5910":"# Preparando stop_words\nfrom wordcloud import STOPWORDS\n\n# Capturando las palabras que no se encuentran en STOPWORDS por ser una contraccion de la palabra original (salida de stemming_tokenizer)\ncontract_words = {'abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', \n              'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', \n              'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', \n              'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', \n              'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', \n              'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', \n              'someth', 'sometim', 'somewher', 'themselv', 'thenc', \n              'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', \n              'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', \n              'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'}\n\nl2 = {'anywh', 'aren', 'becau', 'couldn', 'd', 'didn', 'doe', \n      'doesn', 'don', 'el', 'elsewh', 'everywh', 'hadn', 'hasn', \n      'haven', 'ind', 'isn', 'let', 'll', 'm', 'mustn', \n      'otherwi', 'plea', 're', 's', 'shan', 'shouldn', \n      'somewh', 't', 've', 'wasn', 'weren', 'won', 'wouldn'}\n\n# Unimos ambas listas\nstop_words = STOPWORDS.union(contract_words).union(l2)\n\nfrom sklearn.pipeline import Pipeline\ndef create_pipeline(clf,ngram_range=(1,1)):\n    return Pipeline([('tfidf', TfidfVectorizer(analyzer=\"word\", \n                                               tokenizer=stemming_tokenizer,stop_words= stop_words,\n                                               ngram_range=ngram_range)),\n                         ('clf', clf)])\n\nfrom sklearn.model_selection import GridSearchCV\n# Validacion Cruzada Stratificada(n_splits=5):\nfrom sklearn.model_selection import StratifiedKFold\nCV = StratifiedKFold(n_splits=5, random_state=SEED, shuffle=True)\n\ndef create_gscv(pipeline,params,scoring = [\"neg_log_loss\",\"accuracy\",\"f1_macro\",\"roc_auc_ovr\"],cv = CV):\n    return GridSearchCV(\n            pipeline,\n            params,\n            verbose = 1,\n            cv = cv,\n            n_jobs = -1,\n            scoring = scoring,\n            refit = \"neg_log_loss\" \n            )\n\n# Importamos la metrica principal de evaluacion\nfrom sklearn import metrics\n\n# Dataframe de guardado del test\ndf_results = pd.DataFrame(columns = [\"clf\",\"log_loss\",\"accuracy\",\"f1-macro\",\"ROC\"])\n\n# Funcion de guardado del resultado del test\ndef add_res(clf,name,X_test = X_test):   \n    # Guardamos las predicciones\n    y_predict_proba = clf.predict_proba(X_test)\n    y_pred = clf.predict(X_test)\n    \n    # Guardamos los resultados de las distintas metricas\n    log_loss = metrics.log_loss(y_test,y_predict_proba)\n    acc = metrics.accuracy_score(y_test,y_pred)\n    f1 = metrics.f1_score(y_test, y_pred, average='macro')\n    roc = metrics.roc_auc_score(y_test,y_predict_proba,multi_class='ovr')\n    \n    # Actualizamos el dataframe\n    df_results.loc[len(df_results)]=[name,log_loss,acc,f1,roc]","59d134fb":"from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()\nparameters = {\n    'tfidf__ngram_range': ((1, 1),(2, 2),(1,2)),\n    'tfidf__max_features': (1000,None),\n }\npipeline = create_pipeline(clf)\ngs_NB_M = create_gscv(pipeline,parameters)","03ec775b":"gs_NB_M.fit(X_train,y_train)","11b96cac":"params_to_evaluate = [\"param_tfidf__max_features\",\"param_tfidf__ngram_range\"]\nNB_M_res = save_results(gs_NB_M,params_to_evaluate)\nNB_M_res.sort_values(by='mean_test_score',ascending=False).head(5)","a2bb292f":"# Exportamos los resultados\nNB_M_res.to_csv('NB_M.csv')\n\n# Testing\nadd_res(gs_NB_M,'NB_M')","3a568347":"from sklearn.naive_bayes import ComplementNB\nclf2 = ComplementNB()\nparameters = {\n    'tfidf__ngram_range': ((1, 1),(2, 2),(1,2)),\n    'tfidf__max_features': (1000,None),\n }\npipeline2 = create_pipeline(clf2)\ngs_NB_C = create_gscv(pipeline2,parameters)","952cfc88":"gs_NB_C.fit(X_train,y_train)","45984191":"NB_C_res = save_results(gs_NB_C,params_to_evaluate)\nNB_C_res.sort_values(by='mean_test_neg_log_loss',ascending=False).head(5)","a9036928":"# Exportamos los resultados\nNB_C_res.to_csv('NB_C.csv')\n\n# Testing\nadd_res(gs_NB_C,'NB_C')","962f8ca3":"from sklearn.ensemble import RandomForestClassifier\nclf3 = RandomForestClassifier(criterion='entropy',random_state=SEED)\nparameters = {\n    'tfidf__ngram_range': ((1, 1),(2, 2),(1,2)),\n    'tfidf__max_features': (1000,None),\n }\npipeline3 = create_pipeline(clf3)\ngs_RF = create_gscv(pipeline3,parameters)","5766e3f7":"gs_RF.fit(X_train,y_train)","e23e1222":"RF_res = save_results(gs_RF,params_to_evaluate)\nRF_res.sort_values(by='mean_test_neg_log_loss',ascending=False).head(5)","9bdcc5d1":"# Exportamos los resultados\nRF_res.to_csv('RF.csv')\n\n# Testing\nadd_res(gs_RF,'RF')","8f9f0e71":"from sklearn.svm import SVC\nclf4 = SVC(probability=True)\nparameters = {\n    'tfidf__ngram_range': ((1, 1),(2, 2),(1,2)),\n    'tfidf__max_features': (1000,None),\n }\npipeline4 = create_pipeline(clf4)\ngs_SVC = create_gscv(pipeline4,parameters)","f83ee3c0":"gs_SVC.fit(X_train,y_train)","64e9d389":"SVC_res = save_results(gs_SVC,params_to_evaluate)\nSVC_res.sort_values(by='mean_test_neg_log_loss',ascending=False).head(5)","3a2adae3":"# Exportamos los resultados\nSVC_res.to_csv('SVC.csv')\n\n# Testing\nadd_res(gs_SVC,'SVC')","743e915c":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=60)\nparameters = {\n    'tfidf__ngram_range': ((1, 1),(2, 2),(1,2)),\n    'tfidf__max_features': (1000,None),\n }\npipeline5 = create_pipeline(knn)\ngs_KNN = create_gscv(pipeline5,parameters)","bd974be4":"gs_KNN.fit(X_train,y_train)","915ea676":"KNN_res = save_results(gs_KNN,params_to_evaluate)\nKNN_res.sort_values(by='mean_test_neg_log_loss',ascending=False).head(5)","60ced576":"# Exportamos los resultados\nKNN_res.to_csv('KNN.csv')\n\n# Testing\nadd_res(gs_KNN,'KNN')","53013d70":"# Eliminamos el zero R para que este bien escalado la grafica\nfig = px.bar(df_results, x='clf', y='log_loss')\nfig.update_traces(marker_color='red')\nfig.update_layout(title_text='log_loss scores: ')\nfig.show()","c37c01c5":"# Seleccionamos la instacia con el mejor log_loss\nnb = df_results.iloc[(df_results[\"log_loss\"]*(-1)).idxmax()]\n\n# Seleccionamos los valores a mostrar\nx = ['accuracy','f1-macro','ROC']\ny = [nb['accuracy'],nb['f1-macro'],nb['ROC']]\n\n# Pintamos\nimport plotly.graph_objects as go\nfig = go.Figure([go.Bar(x=x, y=y)])\nfig.update_layout(title_text='Scores complementarios: '+ nb['clf'])\nfig.show()","04c34383":"## Preparando la Clasificacion","7e2213d6":"Ref:\n* https:\/\/towardsdatascience.com\/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76\n* https:\/\/towardsdatascience.com\/tf-idf-explained-and-python-sklearn-implementation-b020c5e83275\n* https:\/\/towardsdatascience.com\/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a","9b5b75df":"* Constantes Globales y Funciones auxiliares","f3f73e64":"* Palabras Clave de cada Clase","9526d3f8":"SEPARANDO TRAIN\/TEST","b929a51a":"## Multinomial NB","4ad7982a":"* Palabras Clave General","b4dd68e0":"## Busqueda de Palabras Clave (TF-IDF)\n\n* https:\/\/www.datacamp.com\/community\/tutorials\/stemming-lemmatization-python\n* https:\/\/scikit-learn.org\/stable\/tutorial\/text_analytics\/working_with_text_data.html\n* https:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html","23c380b6":"* Palabras Clave de cada Clase","4375775c":"# Carga","7de8810a":"# Clasificacion\n\n* Multinomial NB\n* Complement NB\n* SVHC\n* RF\n* KNN","9f490566":"### NGRAMS = (2,2)","90e1a9fe":"Ref:\n* https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.Series.to_csv.html\n* https:\/\/pandas.pydata.org\/docs\/getting_started\/intro_tutorials\/03_subset_data.html\n* https:\/\/amueller.github.io\/word_cloud\/generated\/wordcloud.WordCloud.html#wordcloud.WordCloud\n* https:\/\/matplotlib.org\/3.1.1\/api\/_as_gen\/matplotlib.pyplot.title.html","f1ecf6b5":"## KNN","461f1c66":"* Palabras Clave General","750c072b":"### Palabras mas frecuentes en todo el dataframe","1ee0d1f4":"**DRY CELD**","8cd76a0d":"### NGRAMS = (1,1)","53f6930b":"## Preparacion del Dataframe\n\n* http:\/\/jonathansoma.com\/lede\/algorithms-2017\/classes\/more-text-analysis\/counting-and-stemming\/\n* https:\/\/www.datacamp.com\/community\/tutorials\/stemming-lemmatization-python\n* https:\/\/scikit-learn.org\/stable\/tutorial\/text_analytics\/working_with_text_data.html","901fb43b":"* https:\/\/scikit-learn.org\/stable\/auto_examples\/text\/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py","6988487f":"## Random Forest","a585e121":"## Complement NB","1c69a336":"## SVC","3f054702":"### NGRAMS = (1,2)","624b427f":"* Palabras Clave de cada Clase","f0f86fe6":"### Palabras mas frecuentes de cada clase","8a30fea1":"# Exploracion","ab76c5f5":"# Conclusiones","8e559991":"## Tama\u00f1o de Texto de cada Clase","2b96d5ae":"* Palabras Clave General","b42c2465":"## Palabras mas frecuentes"}}