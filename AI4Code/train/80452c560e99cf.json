{"cell_type":{"9e5e61d1":"code","b2ca997e":"code","39c0cfc7":"code","4aeefacd":"code","90bae07d":"code","46e5d877":"code","81fdd19c":"code","250b8d61":"code","6f52ae6d":"code","a4e4b051":"code","ee427d0d":"code","be24d5bd":"code","843c245a":"code","b4ca2980":"code","93965540":"code","09deb185":"code","63082622":"code","84a5c802":"code","a5f6a763":"code","c4a76853":"code","9543f576":"code","e16c7b14":"code","ba491536":"code","d074137d":"code","45a14ccf":"code","6d8ad0ea":"code","fb32a37d":"code","56267fe3":"code","98d43c9d":"code","05cc260a":"code","56c6d20a":"code","ba26b13f":"code","f8148f7f":"code","8f61e88d":"code","f77ab595":"code","28282285":"code","b0575685":"code","42dd1fce":"code","84500a69":"code","094da078":"code","2ab83860":"code","c0ea6f31":"code","347fb5c9":"code","61081827":"code","62734c02":"code","3866e892":"code","01863a27":"code","bdb1b71f":"code","ee7393a6":"code","b6413d6f":"code","782fd27a":"code","8bd3cc30":"code","37a55961":"code","f50a96c3":"code","c93466ff":"code","b79c7d08":"code","d9edb75c":"code","6bc455b5":"code","5ef5e210":"code","4afe14f5":"code","e4b8d523":"code","4533b399":"code","135838ef":"markdown","bc48cf4d":"markdown","481ec4df":"markdown","eaeea7a3":"markdown","fa1330bc":"markdown","cd2f922e":"markdown","8eb45c57":"markdown","e424dd0d":"markdown","c0112fbb":"markdown","6900e14a":"markdown","e6928f05":"markdown","a1760f30":"markdown","8834d1e1":"markdown","0ccdfac0":"markdown","811ea814":"markdown","2027590b":"markdown","456a8ceb":"markdown","208c923b":"markdown","4fe05df1":"markdown","80cec400":"markdown","0098c9f6":"markdown","a22b0204":"markdown","20540d9f":"markdown","def903ca":"markdown","c3bcce36":"markdown","771acdc5":"markdown","bd454a4e":"markdown","79ea9bd6":"markdown","96ec3e6d":"markdown","a556e591":"markdown","88f8eab4":"markdown","29080f3f":"markdown","9ec4e30e":"markdown","90f14a79":"markdown","a5e97e04":"markdown","0378388e":"markdown","1e7aa074":"markdown"},"source":{"9e5e61d1":"# imports","b2ca997e":"# Regular EDA and plotting libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n# for plots to appear in the notebook\n%matplotlib inline \n\n# Pipeline\nfrom sklearn.pipeline import make_pipeline\n\n# preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\n## Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\n\n## Model evaluators\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","39c0cfc7":"df = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\") \ndf.shape # (rows, columns)","4aeefacd":"# Let's check the top 10 rows of our dataframe\ndf.head()","90bae07d":"# Let's see how many positive (1) and negative (0) samples we have in our dataframe\ndf.target.value_counts()","46e5d877":"# Normalized value counts\ndf.target.value_counts(normalize=True)","81fdd19c":"# Plotting the value counts with a bar graph\ndf.target.value_counts().plot(kind=\"bar\", color=[\"salmon\", \"lightblue\"])\nplt.xticks(rotation=0); # for keeping the labels on the x-axis vertical","250b8d61":"df.info()","6f52ae6d":"df.describe()","a4e4b051":"df.sex.value_counts()","ee427d0d":"# Comparing target column with sex column\npd.crosstab(df.target, df.sex)","be24d5bd":"pd.crosstab(df.target, df.sex).plot(kind=\"bar\", \n                                    figsize=(10,6), \n                                    color=[\"salmon\", \"lightblue\"])\nplt.xticks(rotation=0); # for keeping the labels on the x-axis vertical","843c245a":"pd.crosstab(df.target, df.sex).plot(kind=\"bar\", figsize=(10,6), color=[\"salmon\", \"lightblue\"])\n\n# Adding some attributes to it\nplt.title(\"Heart Disease Frequency for Sex\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.ylabel(\"Amount\")\nplt.legend([\"Female\", \"Male\"])\nplt.xticks(rotation=0);","b4ca2980":"plt.figure(figsize=(10,6))\n\n# Starting with positve examples\nplt.scatter(df.age[df.target==1], \n            df.thalach[df.target==1], \n            c=\"salmon\")\n\n# Now for negative examples, we want them on the same plot, so we call plt again\nplt.scatter(df.age[df.target==0], \n            df.thalach[df.target==0], \n            c=\"lightblue\")\n\n# Adding some helpful info\nplt.title(\"Heart Disease in function of Age and Max Heart Rate\")\nplt.xlabel(\"Age\")\nplt.legend([\"Disease\", \"No Disease\"])\nplt.ylabel(\"Max Heart Rate\");","93965540":"# Histograms are a great way to check the distribution of a variable\ndf.age.plot.hist(edgecolor='black', bins=15)\nplt.title('Age Distribution')\nplt.xlabel('Age');","09deb185":"pd.crosstab(df.cp, df.target)","63082622":"pd.crosstab(df.cp, df.target).plot(kind=\"bar\", \n                                   figsize=(10,6), \n                                   color=[\"lightblue\", \"salmon\"])\n\n# Adding attributes to the plot\nplt.title(\"Heart Disease Frequency Per Chest Pain Type\")\nplt.xlabel(\"Chest Pain Type\")\nplt.ylabel(\"Frequency\")\nplt.legend([\"No Disease\", \"Disease\"])\nplt.xticks(rotation = 0);","84a5c802":"corr_matrix = df.corr()\ncorr_matrix ","a5f6a763":"# Let's make it look a little prettier\ncorr_matrix = df.corr()\nplt.figure(figsize=(15, 10))\nsns.heatmap(corr_matrix, \n            annot=True, \n            linewidths=0.5, \n            fmt= \".2f\", \n            cmap=\"YlGnBu\")\nplt.title('Correlation');","c4a76853":"# Independent variables\nX = df.drop(\"target\", axis=1)\n\n# Target variable \/ dependent variable\ny = df.target.values","9543f576":"X.head()","e16c7b14":"# Targets\ny","ba491536":"# Split into train & test set\nX_train, X_test, y_train, y_test = train_test_split(X, # independent variables \n                                                    y, # dependent variable\n                                            test_size = 0.2, # percentage of data to use for test set\n                                                    random_state=3)","d074137d":"X_train.head()","45a14ccf":"y_train, len(y_train)","6d8ad0ea":"X_test.head()","fb32a37d":"y_test, len(y_test)","56267fe3":"# Logistic Regression","98d43c9d":"model_scores = { }\n\npipe = make_pipeline(StandardScaler(), LogisticRegression())\n\npipe.fit(X_train, y_train)  # apply scaling on training data\n\nmodel_scores['LogisticRegression'] = pipe.score(X_test, y_test)  # apply scaling on testing data, without leaking training data.","05cc260a":"# Gaussian Naive Bayes","56c6d20a":"pipe = make_pipeline(StandardScaler(), GaussianNB())\n\npipe.fit(X_train, y_train)  # apply scaling on training data\n\nmodel_scores['GaussianNB'] = pipe.score(X_test, y_test) # apply scaling on testing data, without leaking training data","ba26b13f":"# Random Forest Classifier","f8148f7f":"pipe = make_pipeline(StandardScaler(), RandomForestClassifier())\n\npipe.fit(X_train, y_train)  # apply scaling on training data\n\nmodel_scores[' RandomForestClassifier'] = pipe.score(X_test, y_test)  # apply scaling on testing data, without leaking training data","8f61e88d":"model_scores","f77ab595":"model_compare = pd.DataFrame(model_scores, index=['accuracy'])\nmodel_compare.T.plot.bar(figsize=(12,6))\nplt.xticks(rotation=0)\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.tight_layout;","28282285":"# Different LogisticRegression hyperparameters\n\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Different RandomForestClassifier hyperparameters\n\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2)}\n\n# Different Gaussian Naive Bayes hyperparameters\nnb_grid = {\n    'var_smoothing': np.logspace(0,-9, num=100)\n}\n","b0575685":"#  random hyperparameter search for LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True)\n\nrs_log_reg.fit(X_train, y_train);","42dd1fce":"rs_log_reg.best_params_","84500a69":"rs_log_reg.score(X_test, y_test)","094da078":"# random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\nrs_rf.fit(X_train, y_train);","2ab83860":"# the best parameters for RandomForestClassifier\nrs_rf.best_params_","c0ea6f31":"# Evaluating the randomized search random forest model\nrs_rf.score(X_test, y_test)","347fb5c9":"# random hyperparameter search for RandomForestClassifier\nrs_gnb = RandomizedSearchCV(GaussianNB(),\n                           param_distributions=nb_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\nrs_gnb.fit(X_train, y_train);","61081827":"# the best parameters for Gaussian Naive Bayes\nrs_gnb.best_params_","62734c02":"# Evaluating the randomized search GaussianNB model\nrs_gnb.score(X_test, y_test)","3866e892":"# Logistic Regression","01863a27":"# Different LogisticRegression hyperparameters\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# grid hyperparameter search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                          param_grid=log_reg_grid,\n                          cv=5,\n                          verbose=True)\n\ngs_log_reg.fit(X_train, y_train);","bdb1b71f":"# best parameters \ngs_log_reg.best_params_","ee7393a6":"# Evaluate the model\ngs_log_reg.score(X_test, y_test)","b6413d6f":"y_preds = gs_log_reg.predict(X_test)","782fd27a":"# Import ROC curve function from metrics module\nfrom sklearn.metrics import plot_roc_curve\n\n# Plot ROC curve and calculate AUC metric\nplot_roc_curve(gs_log_reg, X_test, y_test);","8bd3cc30":"# Display confusion matrix\nprint(confusion_matrix(y_test, y_preds))","37a55961":"sns.set(font_scale=1.5) # Increasing font size\n\ndef plot_conf_mat(y_test, y_preds):\n    \"\"\"\n    Plots a confusion matrix using Seaborn's heatmap().\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                     annot=True, # Annotating the boxes\n                     cbar=False)\n    plt.xlabel(\"true label\")\n    plt.ylabel(\"predicted label\")\n    \nplot_conf_mat(y_test, y_preds)","f50a96c3":"print(classification_report(y_test, y_preds))","c93466ff":"# Best hyperparameters\ngs_log_reg.best_params_","b79c7d08":"# Instantiating model with best hyperparameters (found with GridSearchCV)\nclf = LogisticRegression(C=0.23357214690901212,\n                         solver=\"liblinear\")","d9edb75c":"# Cross-validated accuracy score\ncv_acc = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5, # 5-fold cross-validation\n                         scoring=\"accuracy\") # accuracy as scoring\ncv_acc","6bc455b5":"cv_acc = np.mean(cv_acc)\ncv_acc","5ef5e210":"# Cross-validated precision score\ncv_precision = np.mean(cross_val_score(clf,\n                                       X,\n                                       y,\n                                       cv=5, # 5-fold cross-validation\n                                       scoring=\"precision\")) # precision as scoring\ncv_precision","4afe14f5":"# Cross-validated recall score\ncv_recall = np.mean(cross_val_score(clf,\n                                    X,\n                                    y,\n                                    cv=5, # 5-fold cross-validation\n                                    scoring=\"recall\")) # recall as scoring\ncv_recall","e4b8d523":"# Cross-validated F1 score\ncv_f1 = np.mean(cross_val_score(clf,\n                                X,\n                                y,\n                                cv=5, # 5-fold cross-validation\n                                scoring=\"f1\")) # f1 as scoring\ncv_f1","4533b399":"# Visualizing cross-validated metrics\ncv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n                            \"Precision\": cv_precision,\n                            \"Recall\": cv_recall,\n                            \"F1\": cv_f1},\n                          index=[0])\ncv_metrics.T.plot.bar(title=\"Cross-Validated Metrics\", legend=False)\nplt.xticks(rotation=0);","135838ef":"# Thank You :)","bc48cf4d":"\n### ROC Curve and AUC Scores\n","481ec4df":"##### Models Accuracy increased from 85% to 87%","eaeea7a3":"Since there are 5 metrics here, we'll take the average.","fa1330bc":"### Correlation between independent variables\n\n","cd2f922e":" It's a **normal distribution** ","8eb45c57":"## Data Exploration (Exploratory Data Analysis or EDA)\n","e424dd0d":"There are 207 males and 96 females in our data.","c0112fbb":"\n\n### Confusion matrix \n\n","6900e14a":"## Loading Data\n\n","e6928f05":"## Preparing the tools\n\nAt the start of any project, it's custom to see the required libraries imported in a big chunk like you can see below.\n\nHowever, in practice, your projects may import libraries as you go. After you've spent a couple of hours working on your problem, you'll probably want to do some tidying up. This is where you may want to consolidate every library you've used at the top of your notebook (like the cell below).\n\nThe libraries you use will differ from project to project. But there are a few which will you'll likely take advantage of during almost every structured data project. \n\n* [pandas](https:\/\/pandas.pydata.org\/) for data analysis.\n* [NumPy](https:\/\/numpy.org\/) for numerical operations.\n* [Matplotlib](https:\/\/matplotlib.org\/)\/[seaborn](https:\/\/seaborn.pydata.org\/) for plotting or data visualization.\n* [Scikit-Learn](https:\/\/scikit-learn.org\/stable\/) for machine learning modelling and evaluation.","a1760f30":"### Model choices\n\nI will be using the following Models and comparing their results.\n\n1. Logistic Regression  \n2. Gaussian Naive Bayes\n3. RandomForest Classifier","8834d1e1":"After tuning the LogisticRegression Model's accuracy increased from 86% to 88%","0ccdfac0":"Beautiful, we can see we're using 242 samples to train on. Let's look at our test data.","811ea814":"## Hyperparameter tuning and cross-validation\n\n\n","2027590b":"### Classification report\n\n","456a8ceb":"#### F1 Score","208c923b":"#### Tuning RandomForestClassifier","4fe05df1":"### Age vs Max Heart rate for Heart Disease\n","80cec400":"## Problem Definition\n Given clinical parameters about a patient, can we predict whether or not they have heart disease?\n\n\n\n### Heart Disease Data Dictionary\n\nThe following are the features we'll use to predict our target variable (heart disease or no heart disease).\n\n1. age - age in years \n2. sex - (1 = male; 0 = female) \n3. cp - chest pain type \n    * 0: Typical angina: chest pain related decrease blood supply to the heart\n    * 1: Atypical angina: chest pain not related to heart\n    * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n    * 3: Asymptomatic: chest pain not showing signs of disease\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n    * anything above 130-140 is typically cause for concern\n5. chol - serum cholestoral in mg\/dl \n    * serum = LDL + HDL + .2 * triglycerides\n    * above 200 is cause for concern\n6. fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false) \n    * '>126' mg\/dL signals diabetes\n7. restecg - resting electrocardiographic results\n    * 0: Nothing to note\n    * 1: ST-T Wave abnormality\n        - can range from mild symptoms to severe problems\n        - signals non-normal heart beat\n    * 2: Possible or definite left ventricular hypertrophy\n        - Enlarged heart's main pumping chamber\n8. thalach - maximum heart rate achieved \n9. exang - exercise induced angina (1 = yes; 0 = no) \n10. oldpeak - ST depression induced by exercise relative to rest \n    * looks at stress of heart during excercise\n    * unhealthy heart will stress more\n11. slope - the slope of the peak exercise ST segment\n    * 0: Upsloping: better heart rate with excercise (uncommon)\n    * 1: Flatsloping: minimal change (typical healthy heart)\n    * 2: Downslopins: signs of unhealthy heart\n12. ca - number of major vessels (0-3) colored by flourosopy \n    * colored vessel means the doctor can see the blood passing through\n    * the more blood movement the better (no clots)\n13. thal - thalium stress result\n    * 1,3: normal\n    * 6: fixed defect: used to be defect but ok now\n    * 7: reversable defect: no proper blood movement when excercising \n14. target - have disease or not (1=yes, 0=no) (= the predicted attribute)\n\n**Note:** No personal identifiable information (PPI) can be found in the dataset.\n\nIt's a good idea to save these to a Python dictionary or in an external file, so we can look at them later without coming back here.","0098c9f6":"### Training and test split\n","a22b0204":"#### Tuning Gaussian Naive Bayes","20540d9f":"### Heart Disease Frequency according to Gender\n\n","def903ca":"### Heart Disease Frequency per Chest Pain Type\n\n","c3bcce36":"#### Tuning LogisticRegression","771acdc5":"Beautiful! Since our models are fitting, let's compare them visually.","bd454a4e":"#### Logistic Regression gave us the highest model accuracy of 89% after tuning..","79ea9bd6":"## Model Comparison\n\n","96ec3e6d":"## Evaluating our Classification model, beyond accuracy\n\n","a556e591":"\n\nLet's check the age **distribution**.","88f8eab4":"\n\n### Tuning a model with `GridSearchCV`","29080f3f":"\n### Tuning models with with `RandomizedSearchCV`","9ec4e30e":"#### Recall","90f14a79":"# Predicting Heart Disease using Machine Learning\n\n","a5e97e04":"##  Model Creation\n","0378388e":"#### Precision","1e7aa074":"And we've got 61 examples we'll test our model(s) on. Let's build some."}}