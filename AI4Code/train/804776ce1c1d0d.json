{"cell_type":{"58283a05":"code","82d00a35":"code","ce611756":"code","413d84c9":"code","12a28365":"code","0061b1e5":"code","76226c33":"code","c538cf39":"code","840d94fd":"code","0aca18b8":"code","79a7755c":"code","c3666eca":"code","21b33404":"code","dbb12915":"code","3e3908c1":"code","76b2920e":"code","f2a41f1c":"code","6533b0ff":"code","cdf87031":"code","7bbaaf83":"code","fb1393dc":"code","59306fcd":"code","3d1666f8":"code","012a942b":"code","f6e48f91":"code","565f7c3c":"code","edd6e54c":"code","5d0109c8":"code","e0987624":"code","7615a156":"code","c5d7e47d":"code","e99d9835":"code","4a486519":"code","74106f9c":"markdown","5c0f7f47":"markdown","160ab2be":"markdown","e5cb829a":"markdown","23257dae":"markdown","2b24638c":"markdown","6d35a629":"markdown","e300c78c":"markdown","f01bfc75":"markdown","e62646e3":"markdown","8a5bcd4c":"markdown","eeff0b9b":"markdown","b400e497":"markdown","597fbd8f":"markdown","f871e252":"markdown","a9b28484":"markdown","3cae9508":"markdown","ef3c4b57":"markdown","3732c1ca":"markdown","07af989d":"markdown","b061a441":"markdown","5555dfda":"markdown","8b3fba6e":"markdown","bd624237":"markdown","316cddc4":"markdown","87693dab":"markdown","34c34e73":"markdown","c4a3ee6b":"markdown","9cb62082":"markdown","f2239782":"markdown","76bf2858":"markdown","191a105a":"markdown","8e366fa6":"markdown","79f9344c":"markdown","e54e6362":"markdown","cbf91f3d":"markdown","30731f7d":"markdown","e1ea021f":"markdown","9698a226":"markdown"},"source":{"58283a05":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","82d00a35":"import os\nfrom os.path import join\nimport copy\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\n\n#Plots\nimport sklearn\nimport missingno as msno\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport graphviz\nfrom sklearn import tree\nfrom sklearn.pipeline import make_pipeline\nimport plotly.graph_objs as go\nimport plotly.offline as py\n\n\n#Data processing, metrics and modeling\nfrom hyperopt import fmin, tpe, hp\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom xgboost import plot_tree\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, auc\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve","ce611756":"#Use the first set of data to predict the occurence of diabetes in second dataset\ndata1=pd.read_csv('..\/input\/korean-genome-and-epidemiology-study-koges\/follow_01_data.csv')\ndata2=pd.read_csv('..\/input\/korean-genome-and-epidemiology-study-koges\/follow_02_data.csv')\ndata3=pd.read_csv('..\/input\/korean-genome-and-epidemiology-study-koges\/follow_03_data.csv')\ndata4=pd.read_csv('..\/input\/korean-genome-and-epidemiology-study-koges\/follow_04_data.csv')\ndata5=pd.read_csv('..\/input\/korean-genome-and-epidemiology-study-koges\/follow_05_data.csv')","413d84c9":"#Randomize dataset\ndf1=data1.sample(frac=1)\n\n#A list of columns that contains N\/A (77777) values which can be converted into zero\nreplace_column=['T01_DRDU','T01_SOJUAM','T01_BEERAM','T01_SMDU','T01_SMAM','T01_PREG','T01_CHILD','T01_PMYN_C']\n\nfor col in replace_column:\n    df1[col].replace(77777.0,0,inplace=True)\n\n#A list of columns that contains either 1 or 2 values and therefore need to be converted into 0 and 1 respectively.\ncategorical_1_2=['T00_SEX','T01_PSM','T01_EXER','T01_HTN','T01_LIP','T01_FMFHT','T01_FMMHT','T01_FMFDM','T01_FMMDM','T01_PREG','T01_CHILD','T01_PMYN_C']\n\ndf1[categorical_1_2]=df1[categorical_1_2].replace({1.0:0,1:0,2.0:1,2:1})\n\n#Remove the samples that has either T2DM history or diagnosed with T2DM (Fasting blood glucose level >=126)\ndf2=df1.drop(df1[(df1.T01_DM==2)|(df1.T01_GLU0>=126)].index)\n\n\n#A list of columns that cannot be used for T2DM prediction\nmissing_column=['T00_DATA_CLASS','T01_EDATE','T01_SMAG','T01_HTNAG','T01_DM','T01_DMAG','T01_LIPAG','T01_FMFHTAG','T01_FMMHTAG','T01_FMFDMAG','T01_FMMDMAG','T01_MNSAG','T01_FPREGAG','T01_FLABAG','T01_PMAG_C','T01_TAKAM','T01_RICEAM','T01_WINEAM','T01_HLIQAM','T01_TAKFQ','T01_RICEFQ','T01_WINEFQ','T01_HLIQFQ']\ndf2.drop(missing_column, axis=1, inplace=True)\n\n\n#A list of columns that are categorical, thus need to be converted using get_dummies functionk\ncategorical=['T01_MARRY','T01_DRINK','T01_SMOKE']\n\n#Replace the unsurveyed(66666) and unanswered(99999) values into nan\ndf3=df2.replace(dict.fromkeys([66666.0,99999.0],None))\n\ndf3.head()","12a28365":"p=df3.hist(figsize=(20,20))","0061b1e5":"def missing_plot(dataset, key, name) :\n    null_feat = pd.DataFrame(len(dataset[key]) - dataset.isnull().sum(), columns = ['Count'])\n    percentage_null = pd.DataFrame((len(dataset[key]) - (len(dataset[key]) - dataset.isnull().sum()))\/len(dataset[key])*100, columns = ['Count'])\n    percentage_null = percentage_null.round(2)\n\n    trace = go.Bar(x = null_feat.index, y = null_feat['Count'] ,opacity = 0.8, text = percentage_null['Count'],  textposition = 'auto',marker=dict(color = '#7EC0EE',\n            line=dict(color='#000000',width=1.5)))\n\n    layout = dict(title =  name)\n\n    fig = dict(data = [trace], layout=layout)\n    py.iplot(fig)\n    \nmissing_plot(df3, 'T00_ID', 'Missing Values (count & %)')","76226c33":"plt.style.use('ggplot') # Using ggplot2 style visuals \n\nf, ax = plt.subplots(figsize=(11, 15))\n\nax.set_facecolor('#fafafa')\nax.set(xlim=(-.05, 200))\nplt.ylabel('Variables')\nplt.title(\"Overview Data Set\")\nax = sns.boxplot(data = df3, \n  orient = 'h', \n  palette = 'Set2')","c538cf39":"col_mean=['T01_HEIGHT','T01_WEIGHT','T01_WAIST','T01_HIP','T01_PULSE','T01_SBP','T01_DBP','T01_HBA1C','T01_GLU0','T01_TCHL','T01_HDL','T01_HBA1C']\nfor header in df3.keys():\n    if df3[header].isna().sum()>0:\n        if header in col_mean:\n            df3[header].fillna(df3[header].mean(), inplace = True)\n        else:\n            df3[header].fillna(df3[header].median(), inplace = True)\ndf3.head()","840d94fd":"missing_plot(df3, 'T00_ID', 'Missing Values after filling (count & %)')","0aca18b8":"d2f1=data2.replace(dict.fromkeys([66666.0,77777.0,99999.0],None))\nd2f1.drop(d2f1[(d2f1.T02_DM==None)|(d2f1.T02_GLU0==None)].index, inplace=True)\nd2f1['T2DM']=(d2f1.T02_DM==2)|(d2f1.T02_GLU0>=126)\nT2DM_set=d2f1[['T00_ID','T2DM']].replace(False,0).replace(True,1)\ndf4=df3.merge(T2DM_set, on=\"T00_ID\")\nlabel=df4['T2DM']\nlabel","79a7755c":"# 2 datasets\nD = df4[(df4['T2DM'] != 0)]\nH = df4[(df4['T2DM'] == 0)]\n\nprint(df4['T2DM'].value_counts().values.tolist())\n#------------COUNT-----------------------\ndef target_count():\n    trace = go.Bar( x = df4['T2DM'].value_counts().values.tolist(), \n                    y = ['healthy','diabetic' ], \n                    orientation = 'h', \n                    text=df4['T2DM'].value_counts().values.tolist(), \n                    textfont=dict(size=15),\n                    textposition = 'auto',\n                    opacity = 0.8,marker=dict(\n                    color=['lightskyblue', 'gold'],\n                    line=dict(color='#000000',width=1.5)))\n\n    layout = dict(title =  'Count of T2DM variable')\n\n    fig = dict(data = [trace], layout=layout)\n    py.iplot(fig)\n\n#------------PERCENTAGE-------------------\ndef target_percent():\n    trace = go.Pie(labels = ['healthy','diabetic'], values = df4['T2DM'].value_counts(), \n                   textfont=dict(size=15), opacity = 0.8,\n                   marker=dict(colors=['lightskyblue', 'gold'], \n                               line=dict(color='#000000', width=1.5)))\n\n\n    layout = dict(title =  'Distribution of T2DM variable')\n\n    fig = dict(data = [trace], layout=layout)\n    py.iplot(fig)\n\ntarget_count()\ntarget_percent()","c3666eca":"df4_1=df4.copy(deep=True)\nscale_column = ['T01_HEIGHT','T01_WEIGHT','T01_WAIST','T01_HIP','T01_PULSE','T01_SBP','T01_DBP','T01_CREATININE','T01_AST','T01_ALT','T01_TCHL','T01_HDL','T01_TG','T01_INS0']\nfeatures = df4_1[scale_column]\nfeatures.values\nscaler = StandardScaler()\nfeatures = scaler.fit_transform(features.values)\ndf4_1[scale_column] = features\ndf4_1.head()","21b33404":"colormap = plt.cm.viridis\nplt.figure(figsize=(10,10))\nsns.heatmap(df4_1.corr(), linewidths=0.1, vmax=1.0, square=True, cmap=colormap, annot=False)","dbb12915":"df4_1.drop('T2DM', axis=1).corrwith(df4_1.T2DM).plot(kind='bar', grid=True, figsize=(12, 8), \n                                                   title=\"Correlation with T2DM\")","3e3908c1":"df5=pd.get_dummies(data=df4_1,columns=categorical)\ndf5.drop(columns=['T00_ID','T2DM'], inplace=True)\ndf5.head()","76b2920e":"X_train,X_test,y_train,y_test = train_test_split(df5,label,test_size=1\/3,random_state=42, stratify=label)\nsmote=SMOTE(k_neighbors=5)\nsmoted_X_train,smoted_y_train=smote.fit_resample(X_train,y_train)\nsmoted_X_test,smoted_y_test=smote.fit_resample(X_test,y_test)\nsmoted_X_train","f2a41f1c":"abc=[]\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','XGBoost','LightGBM']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(),DecisionTreeClassifier(),XGBClassifier(),LGBMClassifier()]\nfor i in models:\n    model = i\n    model.fit(smoted_X_train,smoted_y_train)\n    prediction=model.predict(smoted_X_test)\n    abc.append([roc_auc_score(smoted_y_test,prediction),accuracy_score(prediction,smoted_y_test)])\nmodels_dataframe=pd.DataFrame(abc,index=classifiers)   \nmodels_dataframe.columns=['AUC','Accuracy']\nmodels_dataframe","6533b0ff":"\nsmote=SMOTE(k_neighbors=5)\nsmoted_X,smoted_y=smote.fit_resample(df5,label)\ndf6=smoted_X.copy(deep=True)\n\n##**Remove the columns with high correlation (abs(x) > 0.8)**\n# def correlation(dataset, threshold):\n#     col_corr = set() # Set of all the names of deleted columns\n#     corr_matrix = dataset.corr()\n#     for i in range(len(corr_matrix.columns)):\n#         for j in range(i):\n#             if (abs(corr_matrix.iloc[i, j]) >= threshold) and (corr_matrix.columns[j] not in col_corr):\n#                 colname = corr_matrix.columns[i] # getting the name of column\n#                 col_corr.add(colname)\n#                 if colname in dataset.columns:\n#                     print('Delete: '+colname+\" \"+str(corr_matrix.iloc[i, j]))\n#                     del dataset[colname] # deleting the column from the dataset\n                    \n# correlation(df6,0.8)","cdf87031":"from sklearn.ensemble import RandomForestClassifier \nmodel= RandomForestClassifier(n_estimators=100,random_state=0)\nX=df6[df6.columns]\nY=smoted_y\nmodel.fit(X,Y)\nRF_list=pd.Series(model.feature_importances_,index=X.columns).sort_values(ascending=False)\nRF_list","7bbaaf83":"df5_label=df5.copy(deep=True)\ndf5_label['T2DM']=label\nfigure, axes = plt.subplots(nrows=2, ncols=2, figsize=(10,10))\naxis_dic={2:[0,0],3:[0,1],4:[1,0],5:[1,1]}\nplt.subplots_adjust(left=0.125,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.2, \n                    hspace=0.35)\n\nfor num in range(2,6):\n    top_features=list(RF_list[:num].keys())\n    x=df5.loc[:,top_features].values\n    print(top_features) \n    pca = PCA(n_components=2)\n    principalComponents  = pca.fit_transform(x)\n    principalDf = pd.DataFrame(data = principalComponents\n                 , columns = ['principal component 1', 'principal component 2'])\n    finalDf = pd.concat([principalDf, df5_label['T2DM']], axis = 1)\n    subplot=axes[axis_dic.get(num)[0]][axis_dic.get(num)[1]]\n\n    subplot.set_title(str(num)+' component PCA', fontsize = 15)\n    targets = [0,1]\n    colors = ['r', 'g', 'b']\n    for target, color in zip(targets,colors):\n        indicesToKeep = finalDf['T2DM'] == target\n            \n        subplot.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n                   , finalDf.loc[indicesToKeep, 'principal component 2']\n                   , c = color\n                   , s = 50)\n    figure.text(0.5, 0.04, 'Principal Component 1', fontsize = 20, ha='center')\n    figure.text(0.04, 0.5, 'Principal Component 2', fontsize = 20, va='center', rotation='vertical')\n    ax.legend(targets)\n    \n","fb1393dc":"df5_label=df5.copy(deep=True)\ndf5_label['T2DM']=label\nsns.lmplot('T01_HBA1C','T01_GLU0', data=df5_label, fit_reg=False, scatter_kws={\"s\":50},markers=[\"o\",\"x\"],hue=\"T2DM\")\nplt.title('HBA1C and GLU0 in 2d plane')\n\nsmoted_X_simple=smoted_X.loc[:,['T01_HBA1C','T01_GLU0']]\nsmoted_X_simple=smoted_X.loc[:,['T01_HBA1C','T01_GLU0']]\n\nsmoted_X_train_simple=smoted_X_train.loc[:,['T01_HBA1C','T01_GLU0']]\nsmoted_X_test_simple=smoted_X_test.loc[:,['T01_HBA1C','T01_GLU0']]","59306fcd":"xyz=[]\nroc=[]\nclassifiers=['Linear Svm','Radial Svm','LR','KNN','Decision Tree','XGBoost','LightGBM']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(),DecisionTreeClassifier(),XGBClassifier(),LGBMClassifier()]\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model,smoted_X_simple[['T01_HBA1C','T01_GLU0']],smoted_y, scoring = \"roc_auc\")\n    xyz.append(cv_result.mean())\n    roc.append(cv_result)\nnew_models_dataframe=pd.DataFrame(xyz,index=classifiers)   \nnew_models_dataframe.columns=['AUC mean']    \nnew_models_dataframe","3d1666f8":"box=pd.DataFrame(roc,index=classifiers)\nsns.boxplot(data=box.T)\nsns.set(rc={'figure.figsize':(15.7,8.27)})\nplt.show()","012a942b":"clf = XGBClassifier().fit(smoted_X_simple.values,smoted_y.ravel())\nplot_decision_regions(smoted_X_simple.values, smoted_y.values.astype(np.integer), clf=clf, legend=2)\n\n# Adding axes annotations\nplt.xlabel('T01_HBA1C')\nplt.ylabel('T01_GLU0')\nplt.title('XGBoost with Diabetes Data')\nplt.show()","f6e48f91":"# \ucd08\ubaa8\uc218 \ud0d0\uc0c9\uacf5\uac04 \uc815\uc758\nparam_space = {'subsample': hp.uniform('subsample', 0.1, 0.9),\n               'learning_rate': hp.uniform('learning_rate', 0.01,0.05)\n              }\n\n# \ubaa9\uc801\ud568\uc218 \uc815\uc758\ndef objective_xgb(params):\n    params = {'max_depth': 2,\n              'subsample': params['subsample'],\n              'learning_rate': params['learning_rate']\n             }\n    xgb_clf = XGBClassifier(n_estimators=100, **params) \n    best_score = cross_val_score(xgb_clf, smoted_X_simple[['T01_HBA1C','T01_GLU0']],smoted_y,\n                                 scoring='roc_auc', \n                                 cv=5, \n                                 n_jobs=8).mean()\n    loss = 1 - best_score\n    return loss\n\n\n# \uc54c\uace0\ub9ac\uc998 \uc2e4\ud589\nbest_xgb_param = fmin(fn=objective_xgb, space=param_space, \n            max_evals=50, \n            rstate=np.random.RandomState(777), \n            algo=tpe.suggest)\n\nprint(best_xgb_param)","565f7c3c":"new_xgb=XGBClassifier(max_depth=2, subsample=0.5780885404563175, learning_rate=0.04988810025292616)\n\nxyz=[]\nroc=[]\nclassifiers=['XGBoost']\n\ncv_result = cross_val_score(new_xgb,smoted_X_simple[['T01_HBA1C','T01_GLU0']],smoted_y, scoring = \"roc_auc\")\nxyz.append(cv_result.mean())\nroc.append(cv_result)\n\nnew_models_dataframe2=pd.DataFrame(xyz,index=classifiers)   \nnew_models_dataframe2.columns=['New AUC mean']\nnew_models_dataframe=new_models_dataframe.loc[['XGBoost']].merge(new_models_dataframe2,left_index=True,right_index=True,how='left')\nnew_models_dataframe['Increase']=new_models_dataframe['New AUC mean']-new_models_dataframe['AUC mean']\n\nnew_models_dataframe[['AUC mean','New AUC mean','Increase']]","edd6e54c":"clf=XGBClassifier(max_depth=2, subsample=0.5780885404563175, learning_rate=0.04988810025292616).fit(smoted_X_simple.values,smoted_y.ravel())\nplot_decision_regions(smoted_X_simple.values, smoted_y.values.astype(np.integer), clf=clf, legend=2)\n\n# Adding axes annotations\nplt.xlabel('T01_HBA1C')\nplt.ylabel('T01_GLU0')\nplt.title('Optimized XGBoost with Diabetes Data')\nplt.show()","5d0109c8":"smoted_y_pred = clf.predict(smoted_X_test_simple.values)\ncf_matrix=confusion_matrix(smoted_y_test,smoted_y_pred)\npd.crosstab(smoted_y_test, smoted_y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","e0987624":"p = sns.heatmap(pd.DataFrame(cf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","7615a156":"print(classification_report(smoted_y_test,smoted_y_pred))","c5d7e47d":"smoted_y_pred_proba = clf.predict_proba(smoted_X_test_simple)[:,1]\nfpr, tpr, thresholds = roc_curve(smoted_y_test, smoted_y_pred_proba)\n\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='Knn')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('Decision Tree ROC curve')\nplt.show()","e99d9835":"fig, ax = plt.subplots(figsize=(20, 20)) \nplot_tree(clf, rankdir=\"LR\", ax=ax)\nplt.show()","4a486519":"example=np.asarray([[6.5,106]])\nprediction=clf.predict_proba(example)[0][1]\nprint('Diabetes risk = '+str(round(prediction*100,1))+\"%\")","74106f9c":"## 2.5. Target","5c0f7f47":"## Thank you all !","160ab2be":"## 2.3. Fill in the empty values","e5cb829a":"* Decision region plot shows over-fitting\n* To prevent over-fitting, reduce the max_depth (default=6) and num_leaves parameter","23257dae":"## 3.8. XGBoost (Optimized) - Decision region","2b24638c":"Q) Predict the diabetes risk of a patient with following features:\n* fasting glucose = 140mg\/dL\n* HbA1C = 7%","6d35a629":"## 4.2. Classification Report","e300c78c":"# 1. Load libraries and read the data","f01bfc75":"## 3.4. Re-calculate the models using GLU0 and HBA1C","e62646e3":"## 3.1. Linear SVM, Radial SVM, LR, KNN, Decision Tree","8a5bcd4c":"## 4.4. Decision Tree graph","eeff0b9b":"## 2.9. SMOTE Oversampling","b400e497":"# 2. Data cleaning and overview","597fbd8f":"## 2.7. Correlation matrix","f871e252":"## 4.1. Confusion metrix","a9b28484":"## 2.2. Data overview","3cae9508":"## 2.4. Make the label using the data achieved after 2 years","ef3c4b57":"**Observations:**\n1. Using only the top 2 features shows the most stratification","3732c1ca":"**Random Forest Classifier**","07af989d":"Loading the libraries","b061a441":"## 4.3. ROC curve","5555dfda":"## 3.3. PCA on top5 features","8b3fba6e":"## 2.8. Convert categorical to seperate columns (get_dummies)","bd624237":"# 3. Machine Learning","316cddc4":"# 5. Conclusion","87693dab":"## 3.2. Feature Extraction","34c34e73":"## 1.1. Load libraries","c4a3ee6b":"## 2.1. Cleaning Data","9cb62082":"**Results**\n* If HbA1C>5.8%, the fasting glucose level cuf-off is 90.5mg\/dL\n* If not, the cut-off decrease to 88.5mg\/dL\n\n**Interpretation**\n![Diabetes_cutoff](https:\/\/cdn.rcsb.org\/pdb101\/global-health\/diabetes-mellitus\/files\/Blood-Test-Levels-for-Diagnosing-DM_0.PNG)\n* According to American Diabetes Association (2012) guideline, HbA1C>5.7% and fasting glucose>100mg\/dL are diagnosed as 'prediabetes', and this is a global standard.\n* However, the fasting glucose cut-off calculated using ML model which is trained with Korean electronic medical records (EMR) was significantly lower than the guideline (88.5-90.5 vs 100mg\/dL).\n* The difference could be explained by the ancestrial (genetic) or lifestyle difference, and further studies comparing the groups could elucidate the reason behind the difference. ","f2239782":"The above boxplot shows that XGBoost and LightGBM model perform the best while Radial SVM performs the worst.","76bf2858":"## 1.2. Read data","191a105a":"# 4. Evaluation","8e366fa6":"**Replace the nan values by either mean or median value**","79f9344c":"## 3.6. XGBoost - Decision region","e54e6362":"## 3.5. Cross Validation","cbf91f3d":"## 3.7. Hyperparameter","30731f7d":"**Observation**\n1. Optimal XGB param: {'learning_rate': 0.04988810025292616, 'subsample': 0.5780885404563175, 'max_depth'=2}","e1ea021f":"## 2.6. StandardScaler","9698a226":"## 6. Usage: Diabetes risk calculator"}}