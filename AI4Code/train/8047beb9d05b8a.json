{"cell_type":{"3c3a92fd":"code","1737b0af":"code","11858fcc":"code","d0dcfcbe":"code","4ef0618f":"code","7a4859cb":"code","eccdbe27":"code","734ad3b6":"code","2003cff0":"code","ebc894b6":"code","5f7bed7b":"code","d3ab51ae":"code","d0ab7bdd":"code","19f8801e":"code","587ed595":"code","411a9933":"code","7e73e6c8":"code","0d1bb911":"code","1d7f4ced":"code","79d58a9f":"code","5b8ef965":"code","b7fc18f6":"code","8e2828e5":"code","a5b67782":"code","adaab1a3":"code","cb621896":"code","b31129d4":"code","4a526096":"code","0e11b50e":"code","38f8620d":"code","43a1e665":"code","5219c2dd":"code","be221b2d":"code","f8692e7a":"code","77f4cce4":"code","0e22d18d":"code","74119e32":"code","870bc9c0":"code","9629c7ed":"code","0c10878f":"code","4241a2ea":"code","591bc4d5":"code","d22bbfc1":"code","447c330b":"code","266e23b2":"code","6cc67fc2":"code","a48ab918":"code","b3db1706":"code","4dedb67a":"code","c89e7800":"code","56c7ed9d":"code","2d8acef8":"code","f509c519":"code","860c6958":"markdown","b13cff8d":"markdown","6761f700":"markdown","833070f3":"markdown","a474c7d1":"markdown","62348fe5":"markdown","17ad4312":"markdown","b2d4d526":"markdown","6981b456":"markdown","cefea065":"markdown","76fee24a":"markdown","1dfdcd73":"markdown","5bb9e369":"markdown","99ad3cd5":"markdown","08983e82":"markdown","08b7fad6":"markdown","21637023":"markdown","7ef5c589":"markdown","dc05cb37":"markdown","ab045eee":"markdown","7719437a":"markdown","eb783dfe":"markdown","2330b405":"markdown","06080140":"markdown","f922cfdf":"markdown","781ed8a4":"markdown","90bb5b31":"markdown","99ac65df":"markdown","828bf68c":"markdown","30111628":"markdown","e9748a90":"markdown","a1ac6c61":"markdown","dd24526b":"markdown"},"source":{"3c3a92fd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1737b0af":"import pandas as pd \nimport pandas_profiling as pdp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.options.display.float_format = '{:.3f}'.format\n%matplotlib inline\nplt.style.use('fivethirtyeight')","11858fcc":"train=pd.read_csv('..\/input\/prudential-life-insurance-assessment\/train.csv.zip')\ntest=pd.read_csv('..\/input\/prudential-life-insurance-assessment\/test.csv.zip')","d0dcfcbe":"train.head()","4ef0618f":"train.info()","7a4859cb":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return tt\n    #return(np.transpose(tt))","eccdbe27":"#checking missing value percentage in train data\nmissing_data(train)['Percent'].sort_values(ascending=False)","734ad3b6":"#checking missing value percentage in train data\nmissing_data(test)['Percent'].sort_values(ascending=False)","2003cff0":"train=train[train.columns[train.isnull().mean() <= 0.75]]","ebc894b6":"test=test[test.columns[test.isnull().mean() <= 0.75]]","5f7bed7b":"train.isnull().sum().sort_values(ascending=False)","d3ab51ae":"test.isnull().sum().sort_values(ascending=False)","d0ab7bdd":" list_train=train.columns[train.isna().any()].tolist()","19f8801e":"list_test=test.columns[test.isna().any()].tolist()","587ed595":"for i in range(0,len(list_train)):\n    print('column name: ',list_train[i],' Dtype:',train[list_train[i]].dtypes)","411a9933":"for i in range(0,len(list_test)):\n    print('column name: ',list_test[i],' Dtype:',train[list_test[i]].dtypes)","7e73e6c8":"for column in list_train:\n    train[column].fillna(train[column].mean(), inplace=True)","0d1bb911":"for column in list_test:\n    test[column].fillna(test[column].mean(), inplace=True)","1d7f4ced":"train.info()\ntest.info()","79d58a9f":"obj_train=list(train.select_dtypes(include=['object']).columns)\nobj_test=list(test.select_dtypes(include=['object']).columns)","5b8ef965":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ntrain[obj_train]=le.fit_transform(train[obj_train])\ntest[obj_test]=le.transform(test[obj_test])","b7fc18f6":"f, axes = plt.subplots(1, 2, figsize=(15,7))\nsns.boxplot(x = 'Wt', data=train,  orient='v' , ax=axes[0])\nsns.distplot(train['Wt'],  ax=axes[1])","8e2828e5":"f, axes = plt.subplots(1, 2, figsize=(15,7))\nsns.boxplot(x = 'Ht', data=train,  orient='v' , ax=axes[0])\nsns.distplot(train['Ht'],  ax=axes[1])","a5b67782":"f, axes = plt.subplots(1, 2, figsize=(15,7))\nsns.boxplot(x = 'BMI', data=train,  orient='v' , ax=axes[0])\nsns.distplot(train['BMI'],  ax=axes[1])","adaab1a3":"f,axes=plt.subplots(1,2,figsize=(15,7))\nsns.boxplot(x='Ins_Age',data=train,orient='v',ax=axes[0])\nsns.distplot(train['Ins_Age'],ax=axes[1])","cb621896":"f,ax=plt.subplots(1,2,figsize=(18,8))\ntrain['Response'].value_counts().plot.pie(autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Response')\nax[0].set_ylabel('')\nsns.countplot('Response',data=train,ax=ax[1])\nax[1].set_title('Response')\nplt.show()","b31129d4":"#create a funtion to create a  new target variable based on conditions \n\ndef new_target(row):\n    if (row['Response']<=7) & (row['Response']>=0):\n        val=0\n    elif (row['Response']==8):\n        val=1\n    else:\n        val=-1\n    return val","4a526096":"#create a copy of original dataset\nnew_data=train.copy()","0e11b50e":"#create a new column\nnew_data['Final_Response']=new_data.apply(new_target,axis=1)","38f8620d":"new_data['Final_Response'].value_counts()","43a1e665":"#distribution plot for target classes\nsns.countplot(x=new_data.Final_Response).set_title('Distribution of rows by response categories')","5219c2dd":"#dropping already existing column\nnew_data.drop(['Response'],axis=1,inplace=True)\ntrain=new_data\ndel new_data","be221b2d":"train.rename(columns={'Final_Response':'Response'},inplace=True)","f8692e7a":"# BMI Categorization\nconditions = [\n    (train['BMI'] <= train['BMI'].quantile(0.25)),\n    (train['BMI'] > train['BMI'].quantile(0.25)) & (train['BMI'] <= train['BMI'].quantile(0.75)),\n    (train['BMI'] > train['BMI'].quantile(0.75))]\n\nchoices = ['under_weight', 'average', 'overweight']\n\ntrain['BMI_Wt'] = np.select(conditions, choices)\n\n# Age Categorization\nconditions = [\n    (train['Ins_Age'] <= train['Ins_Age'].quantile(0.25)),\n    (train['Ins_Age'] > train['Ins_Age'].quantile(0.25)) & (train['Ins_Age'] <= train['Ins_Age'].quantile(0.75)),\n    (train['Ins_Age'] > train['Ins_Age'].quantile(0.75))]\n\nchoices = ['young', 'average', 'old']\ntrain['Old_Young'] = np.select(conditions, choices)\n\n# Height Categorization\nconditions = [\n    (train['Ht'] <= train['Ht'].quantile(0.25)),\n    (train['Ht'] > train['Ht'].quantile(0.25)) & (train['Ht'] <= train['Ht'].quantile(0.75)),\n    (train['Ht'] > train['Ht'].quantile(0.75))]\n\nchoices = ['short', 'average', 'tall']\n\ntrain['Short_Tall'] = np.select(conditions, choices)\n\n# Weight Categorization\nconditions = [\n    (train['Wt'] <= train['Wt'].quantile(0.25)),\n    (train['Wt'] > train['Wt'].quantile(0.25)) & (train['Wt'] <= train['Wt'].quantile(0.75)),\n    (train['Wt'] > train['Wt'].quantile(0.75))]\n\nchoices = ['thin', 'average', 'fat']\n\ntrain['Thin_Fat'] = np.select(conditions, choices)","77f4cce4":"plt.figure(figsize=(10,7))\nsns.countplot(x = 'BMI_Wt', hue = 'Response', data = train)","0e22d18d":"plt.figure(figsize=(10,7))\nsns.countplot(x = 'Old_Young', hue = 'Response', data = train)","74119e32":"plt.figure(figsize=(10,7))\nsns.countplot(x = 'Short_Tall', hue = 'Response', data = train)","870bc9c0":"plt.figure(figsize=(10,7))\nsns.countplot(x = 'Thin_Fat', hue = 'Response', data = train)\n","9629c7ed":"def new_target(row):\n    if (row['BMI_Wt']=='overweight') or (row['Old_Young']=='old')  or (row['Thin_Fat']=='fat'):\n        val='extremely_risky'\n    else:\n        val='not_extremely_risky'\n    return val\n\ntrain['extreme_risk'] = train.apply(new_target,axis=1)","0c10878f":"plt.figure(figsize=(10,7))\nsns.countplot(x = 'extreme_risk', hue = 'Response', data = train)","4241a2ea":"def new_target(row):\n    if (row['BMI_Wt']=='average') or (row['Old_Young']=='average')  or (row['Thin_Fat']=='average'):\n        val='average'\n    else:\n        val='non_average'\n    return val\n\ntrain['average_risk'] = train.apply(new_target,axis=1)","591bc4d5":"plt.figure(figsize=(10,7))\nsns.countplot(x = 'average_risk', hue = 'Response', data = train)","d22bbfc1":"def new_target(row):\n    if (row['BMI_Wt']=='under_weight') or (row['Old_Young']=='young')  or (row['Thin_Fat']=='thin'):\n        val='low_end'\n    else:\n        val='non_low_end'\n    return val\n\ntrain['low_end_risk'] = train.apply(new_target,axis=1)","447c330b":"plt.figure(figsize=(10,7))\nsns.countplot(x = 'low_end_risk', hue = 'Response', data = train)","266e23b2":"plt.hist(train['Employment_Info_1']);\nplt.title('Distribution of Employment_Info_1 variable');","6cc67fc2":"train['Product_Info_1'].value_counts()","a48ab918":"#product1 vs response\nsns.distplot(train[train['Response']==0]['Product_Info_1'],hist=False,label='Rejected')\nsns.distplot(train[train['Response']==1]['Product_Info_1'],hist=False,label='Accepted')","b3db1706":"#product2 vs response\nsns.distplot(train[train['Response']==0]['Product_Info_2'],hist=False,label='Rejected')\nsns.distplot(train[train['Response']==1]['Product_Info_2'],hist=False,label='Accepted')","4dedb67a":"#product3 vs response\nsns.distplot(train[train['Response']==0]['Product_Info_3'],hist=False,label='Rejected')\nsns.distplot(train[train['Response']==1]['Product_Info_3'],hist=False,label='Accepted')","c89e7800":"#product5 vs response\nsns.distplot(train[train['Response']==0]['Product_Info_5'],hist=False,label='Rejected')\nsns.distplot(train[train['Response']==1]['Product_Info_5'],hist=False,label='Accepted')","56c7ed9d":"#product6 vs response\nsns.distplot(train[train['Response']==0]['Product_Info_6'],hist=False,label='Rejected')\nsns.distplot(train[train['Response']==1]['Product_Info_6'],hist=False,label='Accepted')","2d8acef8":"#product7 vs response\nsns.distplot(train[train['Response']==0]['Product_Info_7'],hist=False,label='Rejected')\nsns.distplot(train[train['Response']==1]['Product_Info_7'],hist=False,label='Accepted')","f509c519":"corr = train.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(100, 370, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","860c6958":"* **Height**","b13cff8d":"Under \"extreme risk\" category, lots of policies are getting either rejected or issued on substandard terms\n","6761f700":"**Filling Null Values With Mean**","833070f3":"**Let's get deeper into it**","a474c7d1":"We are not getting clear Visual on correlation Graph.","62348fe5":"* **Weight**","17ad4312":"Compared to young lives and average lives, more often, old lives were not offered standard terms","b2d4d526":"**We can see that Class 8 has the highest distribution.**","6981b456":"Categorizing BMI,AGE,HEIGHT and WEIGHT based on their values","cefea065":"Prudential, one of the largest issuers of life insurance in the USA.\n\nIn a one-click shopping world with on-demand everything, the life insurance application process is antiquated. Customers provide extensive information to identify risk classification and eligibility, including scheduling medical exams, a process that takes an average of 30 days.\n\nThe result? People are turned off. That\u2019s why only 40% of U.S. households own individual life insurance. Prudential wants to make it quicker and less labor intensive for new and existing customers to get a quote while maintaining privacy boundaries.\n\nBy developing a predictive model that accurately classifies risk using a more automated approach, you can greatly impact public perception of the industry\n","76fee24a":"# Goal\n\nIn this dataset, you are provided over a hundred variables describing attributes of life insurance applicants. The task is to predict the \"Response\" variable for each Id in the test set. \"Response\" is an ordinal measure of risk that has 8 levels.","1dfdcd73":"#  **Target Variable Analysis**","5bb9e369":"## Label Encoding","99ad3cd5":"**The image above is a comparison of a boxplot of a nearly normal distribution and the probability density function (pdf) for a normal distribution. The reason why I am showing you this image is that looking at a statistical distribution is more commonplace than looking at a box plot. In other words, it might help you understand a boxplot.**","08983e82":"This does not indicate any behaviour","08b7fad6":"## Data Description\n\n* train.csv - the training set, contains the Response values\n* test.csv - the test set, you must predict the Response variable for all rows in this file\n\n\n* Id :\tA unique identifier associated with an application.\n* Product_Info_1-7 :\tA set of normalized variables relating to the product applied for\n* Ins_Age :\tNormalized age of applicant\n* Ht :\tNormalized height of applicant\n* Wt :\tNormalized weight of applicant\n* BMI :\tNormalized BMI of applicant\n* Employment_Info_1-6 :\tA set of normalized variables relating to the employment history of the applicant.\n* InsuredInfo_1-6 :\tA set of normalized variables providing information about the applicant.\n* Insurance_History_1-9 :\tA set of normalized variables relating to the insurance history of the applicant.\n* Family_Hist_1-5 :\tA set of normalized variables relating to the family history of the applicant.\n* Medical_History_1-41 :\tA set of normalized variables relating to the medical history of the applicant.\n* Medical_Keyword_1-48 :\tA set of dummy variables relating to the presence of\/absence of a medical keyword being associated with the application.\n* Response :\tThis is the target variable, an ordinal variable relating to the final decision associated with an application","21637023":"This does not indicate any behaviour\n\n","7ef5c589":"Fat people are not offered standard terms\n\n","dc05cb37":"![image.png](attachment:image.png)","ab045eee":"**Taking null value column names**","7719437a":"#  **Analysing features**","eb783dfe":"Under non-low-end risk category, lots of policies are either getting rejected or issued at substandard terms.","2330b405":"**We are making 0 to 7 as one class and 8 as another class**","06080140":"**Printing column names and data types  which has null values**","f922cfdf":"Importing necessary packages and data","781ed8a4":"**Exploring product features**","90bb5b31":"#  Missing Value Analysis","99ac65df":"* **Age**","828bf68c":"Overweight policyholders are not offered standard terms.\n\n","30111628":" ****Converting target variable****","e9748a90":"Dropping columns which has more than 75% missing value","a1ac6c61":"**We will do a binary classification by altering the target variable. The new problem statement would be - Based on the attributes of customers, will the life insurance policy be approved or not i.e.yes(1) or no(0).we will turn this Multiclass classification challenge into Binary classification challenge.**","dd24526b":"* **BMI**"}}