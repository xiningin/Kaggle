{"cell_type":{"8e5a3da5":"code","307a931b":"code","37944fa0":"code","54188a92":"code","7ac994fc":"code","746af70a":"code","7cd25a9f":"code","baa22a33":"code","26fee854":"code","9e36acbb":"code","e545a496":"code","4cd5ef82":"code","c73874da":"markdown","cd976bf4":"markdown","5671b5e0":"markdown","1c181aae":"markdown","b8b7c201":"markdown","4649b242":"markdown","b58d1b7b":"markdown","dc50dcd1":"markdown","5983f944":"markdown"},"source":{"8e5a3da5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","307a931b":"train_set = pd.read_csv(\"..\/input\/imdb-dataset-sentiment-analysis-in-csv-format\/Train.csv\")\n# Veri setindeki pozitif ve negatif duygular\u0131 e\u015fit almak amac\u0131yla yap\u0131lm\u0131\u015f bir i\u015flem.\ntop_data_df_positive = train_set[train_set['label'] == 0].head(15000)\ntop_data_df_negative = train_set[train_set['label'] == 1].head(15000)\n\ntrain = pd.concat([top_data_df_positive, top_data_df_negative])\n","37944fa0":"\nfrom gensim.utils import simple_preprocess\ntrain['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in train['text']]","54188a92":"from gensim.parsing.porter import PorterStemmer\nporter_stemmer = PorterStemmer()\ntrain['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in train['tokenized_text']]\ntrain\n","7ac994fc":"from sklearn.model_selection import train_test_split\n# Train Test Split Function\ndef split_train_test(train, test_size=0.3, shuffle_state=True):\n    X_train, X_test, Y_train, Y_test = train_test_split(train[['stemmed_tokens']], \n                                                        train['label'], \n                                                        shuffle=shuffle_state,\n                                                        test_size=test_size, \n                                                        random_state=15)\n    print(\"Value counts for Train sentiments\")\n    print(Y_train.value_counts())\n    print(\"Value counts for Test sentiments\")\n    print(Y_test.value_counts())\n    print(type(X_train))\n    print(type(Y_train))\n    X_train = X_train.reset_index()\n    X_test = X_test.reset_index()\n    Y_train = Y_train.to_frame()\n    Y_train = Y_train.reset_index()\n    Y_test = Y_test.to_frame()\n    Y_test = Y_test.reset_index()\n    print(X_train.head())\n    return X_train, X_test, Y_train, Y_test\n\n# Call the train_test_split\nX_train, X_test, Y_train, Y_test = split_train_test(train)","746af70a":"from gensim.models import Word2Vec\nsize = 500\nwindow = 3\nmin_count = 1\nworkers = 3\n# 0 for CBOW, 1 for skip-gram\nsg = 0\nOUTPUT_FOLDER = '\/kaggle\/working\/'\n# Function to train word2vec model\ndef make_word2vec_model(train, padding, sg, min_count, size, workers, window):\n    if  padding:\n        #print(len(train))\n        temp_df = pd.Series(train['stemmed_tokens']).values\n        temp_df = list(temp_df)\n        temp_df.append(['pad'])\n        #print(str(size))\n        word2vec_file = OUTPUT_FOLDER + '2ata' + '_PAD.model'\n    w2v_model = Word2Vec(temp_df, min_count = min_count, size = size, workers = workers, window = window, sg = sg)\n\n    w2v_model.save(word2vec_file)\n    return w2v_model, word2vec_file\n\n# Train Word2vec model\nw2vmodel, word2vec_file = make_word2vec_model(train, padding=True, sg=sg, min_count=min_count, size=size, workers=workers, window=window)","7cd25a9f":"max_sen_len = train.stemmed_tokens.map(len).max()\n\npadding_idx = w2vmodel.wv.vocab['pad'].index\n#print(padding_idx)\ndef make_word2vec_vector_cnn(sentence):\n    padded_X = [padding_idx for i in range(max_sen_len)]\n    i = 0\n    for word in sentence:\n        if word not in w2vmodel.wv.vocab:\n            padded_X[i] = 0\n        else:\n            padded_X[i] = w2vmodel.wv.vocab[word].index\n        i += 1\n    return torch.tensor(padded_X, dtype=torch.long, device=device).view(1, -1)","baa22a33":"EMBEDDING_SIZE = 500\nNUM_FILTERS = 10\nimport gensim\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device available for running: \" + str(device))\n\n#torch.nn.Conv2d(in_channels: int, out_channels: int, kernel_size: Union[T, Tuple[T, T]], \n#stride: Union[T, Tuple[T, T]] = 1, padding: Union[T, Tuple[T, T]] = 0, \n#dilation: Union[T, Tuple[T, T]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros')\n\nclass CnnTextClassifier(nn.Module):\n    def __init__(self, vocab_size, num_classes, window_sizes=(1,2,3,5)):\n        super(CnnTextClassifier, self).__init__()\n        w2vmodel = gensim.models.KeyedVectors.load(OUTPUT_FOLDER + '2ata_PAD.model')\n        weights = w2vmodel.wv\n        # With pretrained embeddings\n        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights.vectors), padding_idx=w2vmodel.wv.vocab['pad'].index)\n        \n        # like a python list, it was designed to store any desired number of nn.Module\n        self.convs = nn.ModuleList([\n                                   nn.Conv2d(1, NUM_FILTERS, [window_size, EMBEDDING_SIZE], padding=(window_size - 1, 0))\n                                   for window_size in window_sizes\n        ])\n    \n        self.fc = nn.Linear(NUM_FILTERS * len(window_sizes), num_classes)\n\n    def forward(self, x):\n        x = self.embedding(x) # [B, T, E]\n\n        # Apply a convolution + max_pool layer for each window size\n        x = torch.unsqueeze(x, 1)\n        xs = []\n        for conv in self.convs:\n            x2 = torch.tanh(conv(x))\n            x2 = torch.squeeze(x2, -1)\n            x2 = F.max_pool1d(x2, x2.size(2))\n            xs.append(x2)\n        x = torch.cat(xs, 2)\n\n        # FC\n        x = x.view(x.size(0), -1)\n        logits = self.fc(x)\n\n        probs = F.softmax(logits, dim = 1)\n\n        return probs","26fee854":"def make_target(label):\n    if label == 0:\n        return torch.tensor([0], dtype=torch.long, device=device)\n    elif label == 1:\n        return torch.tensor([1], dtype=torch.long, device=device)","9e36acbb":"NUM_CLASSES = 2\nVOCAB_SIZE = len(w2vmodel.wv.vocab)\nprint(VOCAB_SIZE)\ncnn_model = CnnTextClassifier(vocab_size=VOCAB_SIZE, num_classes=NUM_CLASSES)\ncnn_model.to(device)\nloss_function = nn.CrossEntropyLoss()\noptimizer = optim.Adam(cnn_model.parameters(), lr=0.0001)\nnum_epochs = 10","e545a496":"# Open the file for writing loss\nloss_file_name = OUTPUT_FOLDER + '1cnn_class_big_loss_with_padding.csv'\nf = open(loss_file_name,'w')\nf.write('iter, loss')\nf.write('\\n')\nlosses = []\n\ncnn_model.train()\nfor epoch in range(num_epochs):\n    print(\"Epoch\" + str(epoch + 1))\n    train_loss = 0\n    for index, row in X_train.iterrows():\n        # Clearing the accumulated gradients\n        cnn_model.zero_grad()\n\n        # Make the bag of words vector for stemmed tokens \n        bow_vec = make_word2vec_vector_cnn(row['stemmed_tokens'])\n       \n        # Forward pass to get output\n        probs = cnn_model(bow_vec)\n\n        # Get the target label\n        #print(Y_train['label'][index])\n        target = make_target(Y_train['label'][index])\n\n        # Calculate Loss: softmax --> cross entropy loss\n        loss = loss_function(probs, target)\n        train_loss += loss.item()\n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n    print(f'train_loss : {train_loss \/ len(X_train)}')\n    print(\"Epoch ran :\"+ str(epoch+1))\n    \n    f.write(str((epoch+1)) + \",\" + str(train_loss \/ len(X_train)))\n    f.write('\\n')\n    train_loss = 0\n\ntorch.save(cnn_model, OUTPUT_FOLDER + 'cnn_big_model_500_with_padding.pth')\n\nf.close()\nprint(\"Input vector\")\n#print(bow_vec.cpu().numpy())\nprint(\"Probs\")\nprint(probs)\nprint(torch.argmax(probs, dim=1).cpu().numpy()[0])","4cd5ef82":"from sklearn.metrics import classification_report,confusion_matrix\nbow_cnn_predictions = []\noriginal_lables_cnn_bow = []\ncnn_model.eval()\nloss_df = pd.read_csv(OUTPUT_FOLDER + '1cnn_class_big_loss_with_padding.csv')\nprint(\"atataa\")\nprint(loss_df.columns)\n# loss_df.plot('loss')\n\ny_pred_list = []\ny_true_list = []\n\nwith torch.no_grad():\n    for index, row in X_test.iterrows():\n        #print(row['stemmed_tokens'])\n        bow_vec = make_word2vec_vector_cnn(row['stemmed_tokens'])\n        #print(bow_vec)\n        probs = cnn_model(bow_vec)\n        #print(probs.data)\n        _, predicted = torch.max(probs.data,  1)\n        \n        bow_cnn_predictions.append(predicted.cpu().numpy()[0])\n        original_lables_cnn_bow.append(make_target(Y_test['label'][index]).cpu().numpy()[0])\n\nprint(confusion_matrix(original_lables_cnn_bow, bow_cnn_predictions))\n#print(original_lables_cnn_bow)\nprint(classification_report(original_lables_cnn_bow,bow_cnn_predictions))\nloss_file_name = OUTPUT_FOLDER + '1cnn_class_big_loss_with_padding.csv'\nloss_df = pd.read_csv(loss_file_name)\nprint(loss_df.columns)\nplt_500_padding_30_epochs = loss_df[' loss'].plot()\nfig = plt_500_padding_30_epochs.get_figure()\nfig.savefig(OUTPUT_FOLDER + '1loss_plt_500_padding_30_epochs.pdf')","c73874da":"# Read the Data","cd976bf4":"# Word2Vec Model Creation\n","5671b5e0":"# Model Train","1c181aae":"# Train - Test Split \n","b8b7c201":"# Model Test","4649b242":"# CNN Classifier Model Creation","b58d1b7b":"# Stemming","dc50dcd1":"# Tokenization","5983f944":"# Padding "}}