{"cell_type":{"c3d615fa":"code","82fb070b":"code","e5569430":"code","67d266ec":"code","144c5847":"code","d9cdcc02":"code","58966bf6":"code","0e3cb5cd":"code","ecdbb21a":"code","084cd913":"code","0c2a2615":"code","f575afde":"code","480e6b59":"code","5a73bb82":"code","0d93d033":"code","198869da":"code","5a4a38cd":"code","4dd440cb":"code","bc995d59":"code","16bd2a01":"code","6b2fb7a4":"code","a2efd8ed":"code","69a94ca6":"code","3885bb34":"code","0284cb1a":"code","6153c537":"code","419a2cfd":"code","2009dede":"code","ce7fb0e4":"code","885669ae":"code","69dd90e5":"code","21bbf088":"code","24536df6":"code","cd651404":"code","8f9af7cd":"code","ca953bb3":"code","91fd8b38":"code","ff889b91":"code","19b36815":"code","202984ea":"code","dbf63eea":"code","2f6306d6":"code","8125aabc":"code","716da603":"code","115f3338":"code","bafd0979":"code","02a1f1d9":"code","881d5af4":"code","9f1d77f9":"code","eb44a96f":"code","ae8ac882":"code","672f74f4":"code","f23121ee":"code","cdc6dc43":"code","ec007cf7":"code","2806de88":"code","07061ddc":"code","714df835":"code","47bbab4b":"code","e95562bc":"code","e6115308":"code","912a9b85":"code","18c86bc7":"code","4ffa0078":"code","55207976":"code","fbe49fb1":"code","dae929a5":"code","ef482512":"code","72f9be1a":"markdown","dab2392e":"markdown","76f478be":"markdown","75c2c079":"markdown","f48232b6":"markdown","9856729c":"markdown","eacf0eb3":"markdown","80b62163":"markdown","12119ed0":"markdown","4a1c03ac":"markdown","ec02574e":"markdown","9f9da4f1":"markdown","dcbae392":"markdown","626350d3":"markdown","003b75f5":"markdown","f134edcd":"markdown","2cf2bc1d":"markdown","c9f15f84":"markdown","021562af":"markdown","a672369e":"markdown","70300b72":"markdown","731d6878":"markdown","365d74b4":"markdown","0f510a8a":"markdown","3f8e3064":"markdown","efb6a697":"markdown","cc2217b6":"markdown","a5620790":"markdown","0c0b0f87":"markdown","fd0fcf29":"markdown","6503c369":"markdown","6bde026f":"markdown","c6aa863e":"markdown","3f2ef72c":"markdown","55755c3b":"markdown","a01397d2":"markdown","eacf7991":"markdown","a10b20b0":"markdown","24514e6a":"markdown"},"source":{"c3d615fa":"import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport time\n\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"ticks\", context=\"talk\")\nplt.style.use('dark_background')\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as func\nfrom torch.utils.data import DataLoader, Dataset\n\nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\nimport tokenizers\nfrom sklearn.metrics import mean_squared_error, roc_auc_score, roc_curve, auc\n\nimport warnings\nwarnings.simplefilter('ignore')","82fb070b":"SEED = 34\n\ndef random_seed(SEED):\n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n\nrandom_seed(SEED)","e5569430":"train = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip', nrows = 200 )\ntrain.head()","67d266ec":"temp = train[train['toxic'] == 1]\ntemp.head()","144c5847":"print(len(train['comment_text'][10]), 'Total Characters')\ntrain['comment_text'][10]","d9cdcc02":"labels = train.drop(['id', 'comment_text'], axis = 1)\nunique_values = lambda x: train[x].unique()\n[unique_values(col) for col in labels.columns.tolist()]","58966bf6":"test = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip', nrows = 100)\ntest.head()","0e3cb5cd":"test_labels = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/test_labels.csv.zip', nrows = 10)\ntest_labels.head()","ecdbb21a":"submission = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv.zip', nrows = 10)\nsubmission.head()","084cd913":"train.isnull().sum()","0c2a2615":"test.isnull().sum()","f575afde":"df_train = train.drop(['id', 'comment_text'], axis = 1)\nlabel_counts = df_train.sum()\ndf_counts = pd.DataFrame(label_counts)\ndf_counts.rename(columns = {0:'counts'}, inplace = True)\ndf_counts = df_counts.sort_values('counts', ascending = False)\ndf_counts ","480e6b59":"fig, ax = plt.subplots(figsize = (8,4))\np = sns.barplot(df_counts.index, df_counts['counts'])\nx = plt.xticks(rotation = 60) \nax.set_title('Toxic comment counts in '+ str(len(df_train))+ ' samples of data', weight = 'bold', fontsize = 18, y = 1.05)\nx = ax.set_ylabel('')","5a73bb82":"labels = np.round(df_train.sum()\/len(df_train)*100, 1)\nlabels","0d93d033":"df_test_labels = test_labels.drop(['id'], axis = 1)\ncount_values = lambda x: np.round(df_test_labels[x].value_counts(normalize = True)*100, 1)\npd.DataFrame([count_values(col) for col in df_test_labels.columns.tolist()])","198869da":"train.shape, test.shape","5a4a38cd":"# text preprocessing helper functions [1]\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    #text = text.lower()\n    \n    #pattern = [zero or more character]\n    text = re.sub('\\[.*?\\]', '', text)\n    \n    #pattern = with or without(http),:\/\/, one or more non-white space character, OR www, .,one or more non-white space character\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    \n    #pattern = <, zero or more characters, >, (one or more occurance of >)\n    text = re.sub('<.*?>+', '', text)\n    \n    #pattern = any punctionation\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    \n    #pattern = any new line\n    text = re.sub('\\n', '', text)\n    \n    #pattern = any from[a-zA-Z0-9_], any from[0-9], any from [a-zA-Z0-9_]\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","4dd440cb":"%%time\ntrain['clean_text'] = train['comment_text'].apply(str).apply(lambda x: clean_text(x))\ntest['clean_text'] = test['comment_text'].apply(str).apply(lambda x: clean_text(x))","bc995d59":"kfold = 5\ntrain['kfold'] = train.index % kfold\ntrain.index % kfold","16bd2a01":"#reset_index creates a column with the name of 'index' and saves the old index values in it and updates the values of index.\n#drop = True removes the column 'index' with the old index values. \np_train = train[train[\"kfold\"] != 0].reset_index(drop = True)\np_valid = train[train[\"kfold\"] == 0].reset_index(drop = True)","6b2fb7a4":"p_train.head()","a2efd8ed":"tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')","69a94ca6":"%%time\nsenten_len = []\nfor sentence in tqdm(p_train['clean_text']):\n    token_words = tokenizer.encode_plus(sentence)['input_ids']\n    senten_len.append(len(token_words))","3885bb34":"sns.distplot(senten_len)\nplt.xlim([0, 300])\nplt.xlabel('Tocken count')","0284cb1a":"max_len = 200","6153c537":"class BertDataSet(Dataset):\n    \n    def __init__(self, sentences, toxic_labels):\n        self.sentences = sentences\n        #target is a matrix with shape [#1 x #6(toxic, obscene, etc)]\n        self.targets = toxic_labels.to_numpy()\n    \n    def __len__(self):\n        return len(self.sentences)\n    \n    \n    def __getitem__(self, idx):\n        sentence = self.sentences[idx]\n        bert_senten = tokenizer.encode_plus(sentence, \n                                            add_special_tokens = True, # [CLS],[SEP]\n                                            max_length = max_len,\n                                            pad_to_max_length = True,\n                                            truncation = True,\n                                            return_attention_mask = True\n                                             )\n        ids = torch.tensor(bert_senten['input_ids'], dtype = torch.long)\n        mask = torch.tensor(bert_senten['attention_mask'], dtype = torch.long)\n        toxic_label = torch.tensor(self.targets[idx], dtype = torch.float)\n        \n        \n        return {\n            'ids' : ids,\n            'mask' : mask,\n            'toxic_label':toxic_label\n        }","419a2cfd":"train_dataset = BertDataSet(p_train['clean_text'], p_train[['toxic', 'severe_toxic','obscene', 'threat', 'insult','identity_hate']])\nvalid_dataset = BertDataSet(p_valid['clean_text'], p_valid[['toxic', 'severe_toxic','obscene', 'threat', 'insult','identity_hate']])","2009dede":"for a in train_dataset:\n    print(a)\n    break","ce7fb0e4":"train_batch = 32\nvalid_batch = 32","885669ae":"train_dataloader = DataLoader(train_dataset, batch_size = train_batch, pin_memory = True, num_workers = 4, shuffle = True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size = valid_batch, pin_memory = True, num_workers = 4, shuffle = False)","69dd90e5":"%%time\nfor a in train_dataloader:\n    print(a)\n    print('id shape in data laoder is',a['ids'].shape)\n    break","21bbf088":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","24536df6":"%%time\nmodel = transformers.BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels = 6)\nmodel.to(device)\nmodel.train()","cd651404":"%%time\nfor a in train_dataloader:\n    ids = a['ids'].to(device)\n    mask = a['mask'].to(device)\n    output = model(ids, mask)\n    break","8f9af7cd":"output","ca953bb3":"func.softmax(output['logits'], dim = 1)","91fd8b38":"output_probs = func.softmax(output['logits'], dim = 1)","ff889b91":"torch.max(output_probs, dim = 1)","19b36815":"epochs = 5\nLR = 2e-5 #Learning rate\noptimizer = AdamW(model.parameters(), LR, betas = (0.9, 0.999), weight_decay = 1e-2, correct_bias = False)","202984ea":"train_steps = int((len(train) * epochs)\/train_batch)\nnum_steps = int(train_steps * 0.1)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)","dbf63eea":"le = []\nfor b in tqdm(range(epochs)):\n    for a in train_dataloader:\n        le.append(scheduler.get_last_lr())\n        scheduler.step()\nplt.plot(np.arange(len(le)), le)","2f6306d6":"loss_fn = nn.BCEWithLogitsLoss()\nloss_fn.to(device)","8125aabc":"scaler = torch.cuda.amp.GradScaler()","716da603":"def training(train_dataloader, model, optimizer, scheduler):\n    model.train()\n    torch.backends.cudnn.benchmark = True\n    correct_predictions = 0\n    \n    for a in train_dataloader:\n        losses = []\n        optimizer.zero_grad()\n        \n        #allpreds = []\n        #alltargets = []\n        \n        with torch.cuda.amp.autocast():\n            \n            ids = a['ids'].to(device, non_blocking = True)\n            mask = a['mask'].to(device, non_blocking = True) \n\n            output = model(ids, mask) #This gives model as output, however we want the values at the output\n            output = output['logits'].squeeze(-1).to(torch.float32)\n\n            output_probs = torch.sigmoid(output)\n            preds = torch.where(output_probs > 0.5, 1, 0)\n            \n            toxic_label = a['toxic_label'].to(device, non_blocking = True) \n            loss = loss_fn(output, toxic_label)            \n            \n            losses.append(loss.item())\n            #allpreds.append(output.detach().cpu().numpy())\n            #alltargets.append(toxic.detach().squeeze(-1).cpu().numpy())\n            correct_predictions += torch.sum(preds == toxic_label)\n        \n        scaler.scale(loss).backward() #Multiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor.\n                                      #Returns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.\n        scaler.step(optimizer) #Returns the return value of optimizer.step(*args, **kwargs).\n        scaler.update() #Updates the scale factor.If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. \n                        #If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it\n        scheduler.step() # Update learning rate schedule\n    \n    losses = np.mean(losses)\n    corr_preds = correct_predictions.detach().cpu().numpy()\n    accuracy = corr_preds\/(len(p_train)*6)\n    \n    return losses, accuracy","115f3338":"def validating(valid_dataloader, model):\n    \n    model.eval()\n    correct_predictions = 0\n    all_output_probs = []\n    \n    for a in valid_dataloader:\n        losses = []\n        ids = a['ids'].to(device, non_blocking = True)\n        mask = a['mask'].to(device, non_blocking = True)\n        output = model(ids, mask)\n        output = output['logits'].squeeze(-1).to(torch.float32)\n        output_probs = torch.sigmoid(output)\n        preds = torch.where(output_probs > 0.5, 1, 0)\n            \n        toxic_label = a['toxic_label'].to(device, non_blocking = True)\n        loss = loss_fn(output, toxic_label)\n        losses.append(loss.item())\n        all_output_probs.extend(output_probs.detach().cpu().numpy())\n        \n        correct_predictions += torch.sum(preds == toxic_label)\n        corr_preds = correct_predictions.detach().cpu().numpy()\n    \n    losses = np.mean(losses)\n    corr_preds = correct_predictions.detach().cpu().numpy()\n    accuracy = corr_preds\/(len(p_valid)*6)\n    \n    return losses, accuracy, all_output_probs","bafd0979":"%%time\n\nbest_score = 1000\ntrain_accs = []\nvalid_accs = []\ntrain_losses = []\nvalid_losses = []\n\nfor eboch in tqdm(range(epochs)):\n    \n    train_loss, train_acc = training(train_dataloader, model, optimizer, scheduler)\n    valid_loss, valid_acc, valid_probs = validating(valid_dataloader, model)\n    \n    print('train losses: %.4f' % train_loss, 'train accuracy: %.3f' % train_acc)\n    print('valid losses: %.4f' % valid_loss, 'valid accuracy: %.3f' % valid_acc)\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    train_accs.append(train_acc)\n    valid_accs.append(valid_acc)\n    \n    \n    if valid_loss < best_score:\n        best_score = valid_loss\n        print('Found a good model!')\n        state = {\n            'state_dict': model.state_dict(),\n            'optimizer_dict': optimizer.state_dict(),\n            'best_score': best_score\n        }\n        torch.save(state, 'best_model.pth')\n    else:\n        pass","02a1f1d9":"x = np.arange(epochs)\nfig, ax = plt.subplots(1, 2, figsize = (15,4))\nax[0].plot(x, train_losses)\nax[0].plot(x, valid_losses)\nax[0].set_ylabel('Losses', weight = 'bold')\nax[0].set_xlabel('Epochs')\nax[0].grid(alpha = 0.3)\nax[0].legend(labels = ['train losses', 'valid losses'])\n\nax[1].plot(x, train_accs)\nax[1].plot(x, valid_accs)\nax[1].set_ylabel('Accuracy', weight = 'bold')\nax[1].set_xlabel('Epochs')\nax[1].legend(labels = ['train acc', 'valid acc'])\n\nax[1].grid(alpha = 0.3)\nfig.suptitle('Fold = 0', weight = 'bold') \n","881d5af4":"valid_loss, valid_acc, valid_probs = validating(valid_dataloader, model)\nvalid_probs = np.asarray(valid_probs).flatten()\ny_valid = p_valid[['toxic', 'severe_toxic','obscene', 'threat', 'insult','identity_hate']].to_numpy().flatten()\nfpr, tpr, _ = roc_curve(y_valid, valid_probs)","9f1d77f9":"fig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.set_title('ROC Curv')\nax.set_xlabel('FPR')\nax.set_ylabel('TPR')\nplt.show()","eb44a96f":"auc(fpr, tpr)","ae8ac882":"%%time\nimport numpy as np\nimport pandas as pd\nimport os\nimport random\nimport time\n\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"ticks\", context=\"talk\")\nplt.style.use('dark_background')\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as func\nfrom torch.utils.data import DataLoader, Dataset\n\nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\nimport tokenizers\nfrom sklearn.metrics import mean_squared_error, roc_auc_score, roc_curve, auc\n\nimport warnings\nwarnings.simplefilter('ignore')\n\ntrain = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip', nrows = 2000)\ntest = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip', nrows = 100)\nsubmission = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv.zip')\n\nSEED = 34\ndef random_seed(SEED):\n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\nrandom_seed(SEED)\n\ndef clean_text(text):\n\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\ntrain['clean_text'] = train['comment_text'].apply(str).apply(lambda x: clean_text(x))\ntest['clean_text'] = test['comment_text'].apply(str).apply(lambda x: clean_text(x))\n\nkfold = 5\ntrain['kfold'] = train.index % kfold\n\ntokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\nmax_len = 200\n\nclass BertDataSet(Dataset):\n    \n    def __init__(self, sentences, toxic_labels):\n        self.sentences = sentences\n        #target is a matrix with shape [#1 x #6(toxic, obscene, etc)]\n        self.targets = toxic_labels.to_numpy()\n    \n    def __len__(self):\n        return len(self.sentences)\n    \n    \n    def __getitem__(self, idx):\n        sentence = self.sentences[idx]\n        bert_senten = tokenizer.encode_plus(sentence, \n                                            add_special_tokens = True, # [CLS],[SEP]\n                                            max_length = max_len,\n                                            pad_to_max_length = True,\n                                            truncation = True,\n                                            return_attention_mask = True\n                                             )\n        ids = torch.tensor(bert_senten['input_ids'], dtype = torch.long)\n        mask = torch.tensor(bert_senten['attention_mask'], dtype = torch.long)\n        toxic_label = torch.tensor(self.targets[idx], dtype = torch.float)\n        \n        \n        return {\n            'ids' : ids,\n            'mask' : mask,\n            'toxic_label':toxic_label\n        }\n\nepochs = 5\ntrain_batch = 32\nvalid_batch = 32\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nloss_fn = nn.BCEWithLogitsLoss()\nloss_fn.to(device)\nscaler = torch.cuda.amp.GradScaler()\n\ndef training(train_dataloader, model, optimizer, scheduler):\n    model.train()\n    torch.backends.cudnn.benchmark = True\n    correct_predictions = 0\n    \n    for a in train_dataloader:\n        losses = []\n        optimizer.zero_grad()\n        \n        #allpreds = []\n        #alltargets = []\n        \n        with torch.cuda.amp.autocast():\n            \n            ids = a['ids'].to(device, non_blocking = True)\n            mask = a['mask'].to(device, non_blocking = True) \n\n            output = model(ids, mask) #This gives model as output, however we want the values at the output\n            output = output['logits'].squeeze(-1).to(torch.float32)\n\n            output_probs = torch.sigmoid(output)\n            preds = torch.where(output_probs > 0.5, 1, 0)\n            \n            toxic_label = a['toxic_label'].to(device, non_blocking = True) \n            loss = loss_fn(output, toxic_label)            \n            \n            losses.append(loss.item())\n            #allpreds.append(output.detach().cpu().numpy())\n            #alltargets.append(toxic.detach().squeeze(-1).cpu().numpy())\n            correct_predictions += torch.sum(preds == toxic_label)\n        \n        scaler.scale(loss).backward() #Multiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor.\n                                      #Returns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.\n        scaler.step(optimizer) #Returns the return value of optimizer.step(*args, **kwargs).\n        scaler.update() #Updates the scale factor.If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. \n                        #If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it\n        scheduler.step() # Update learning rate schedule\n    \n    losses = np.mean(losses)\n    corr_preds = correct_predictions.detach().cpu().numpy()\n    accuracy = corr_preds\/(len(p_train)*6)\n    \n    return losses, accuracy\n\ndef validating(valid_dataloader, model):\n    \n    model.eval()\n    correct_predictions = 0\n    all_output_probs = []\n    \n    for a in valid_dataloader:\n        losses = []\n        ids = a['ids'].to(device, non_blocking = True)\n        mask = a['mask'].to(device, non_blocking = True)\n        output = model(ids, mask)\n        output = output['logits'].squeeze(-1).to(torch.float32)\n        output_probs = torch.sigmoid(output)\n        preds = torch.where(output_probs > 0.5, 1, 0)\n            \n        toxic_label = a['toxic_label'].to(device, non_blocking = True)\n        loss = loss_fn(output, toxic_label)\n        losses.append(loss.item())\n        all_output_probs.extend(output_probs.detach().cpu().numpy())\n        \n        correct_predictions += torch.sum(preds == toxic_label)\n        corr_preds = correct_predictions.detach().cpu().numpy()\n    \n    losses = np.mean(losses)\n    corr_preds = correct_predictions.detach().cpu().numpy()\n    accuracy = corr_preds\/(len(p_valid)*6)\n    \n    return losses, accuracy, all_output_probs","672f74f4":"%%time\n\nbest_scores = []\nfor fold in tqdm(range(0,5)):\n\n    # initializing the data\n    p_train = train[train['kfold'] != fold].reset_index(drop = True)\n    p_valid = train[train['kfold'] == fold].reset_index(drop = True)\n\n    train_dataset = BertDataSet(p_train['clean_text'], p_train[['toxic', 'severe_toxic','obscene', 'threat', 'insult','identity_hate']])\n    valid_dataset = BertDataSet(p_valid['clean_text'], p_valid[['toxic', 'severe_toxic','obscene', 'threat', 'insult','identity_hate']])\n\n    train_dataloader = DataLoader(train_dataset, batch_size = train_batch, shuffle = True, num_workers = 4, pin_memory = True)\n    valid_dataloader = DataLoader(valid_dataset, batch_size = valid_batch, shuffle = False, num_workers = 4, pin_memory = True)\n\n    model = transformers.BertForSequenceClassification.from_pretrained(\"..\/input\/bert-base-cased\", num_labels = 6)\n    model.to(device)\n    \n    LR = 2e-5\n    optimizer = AdamW(model.parameters(), LR,betas = (0.9, 0.999), weight_decay = 1e-2) # AdamW optimizer\n\n    train_steps = int(len(p_train)\/train_batch * epochs)\n    num_steps = int(train_steps * 0.1)\n\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)\n    \n    best_score = 1000\n    train_accs = []\n    valid_accs = []\n    train_losses = []\n    valid_losses = []\n    best_valid_probs = []\n    \n    print(\"-------------- Fold = \" + str(fold) + \"-------------\")\n    \n    for epoch in tqdm(range(epochs)):\n        print(\"-------------- Epoch = \" + str(epoch) + \"-------------\")\n\n        train_loss, train_acc = training(train_dataloader, model, optimizer, scheduler)\n        valid_loss, valid_acc, valid_probs = validating(valid_dataloader, model)\n\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n        valid_losses.append(valid_loss)\n        valid_accs.append(valid_acc)\n        \n        print('train losses: %.4f' %(train_loss), 'train accuracy: %.3f' %(train_acc))\n        print('valid losses: %.4f' %(valid_loss), 'valid accuracy: %.3f' %(valid_acc))\n\n        if (valid_loss < best_score):\n\n            best_score = valid_loss\n            print(\"Found an improved model! :)\")\n\n            state = {'state_dict': model.state_dict(),\n                     'optimizer_dict': optimizer.state_dict(),\n                     'best_score':best_score\n                    }\n\n            torch.save(state, \"model\" + str(fold) + \".pth\")\n            best_valid_prob = valid_probs\n            torch.cuda.memory_summary(device = None, abbreviated = False)\n        else:\n            pass\n\n\n    best_scores.append(best_score)\n    best_valid_probs.append(best_valid_prob)\n    \n    ##Plotting the result for each fold\n    x = np.arange(epochs)\n    fig, ax = plt.subplots(1, 2, figsize = (15,4))\n    ax[0].plot(x, train_losses)\n    ax[0].plot(x, valid_losses)\n    ax[0].set_ylabel('Losses', weight = 'bold')\n    ax[0].set_xlabel('Epochs')\n    ax[0].grid(alpha = 0.3)\n    ax[0].legend(labels = ['train losses', 'valid losses'])\n\n    ax[1].plot(x, train_accs)\n    ax[1].plot(x, valid_accs)\n    ax[1].set_ylabel('Accuracy', weight = 'bold')\n    ax[1].set_xlabel('Epochs')\n    ax[1].legend(labels = ['train acc', 'valid acc'])\n\n    ax[1].grid(alpha = 0.3)\n    fig.suptitle('Fold = '+str(fold), weight = 'bold') ","f23121ee":"best_scores","cdc6dc43":"print('Mean of',kfold, 'folds for best loss in', epochs, 'epochs cross-validation folds is %.4f.' %(np.mean(best_scores)))","ec007cf7":"def predicting(test_dataloader, model, pthes):\n    allpreds = []\n    \n    for pth in pthes:\n        state = torch.load(pth)\n        model.load_state_dict(state['state_dict'])\n        model.to(device)\n        model.eval()\n        preds = []\n        with torch.no_grad():\n            for a in test_dataloader:\n                ids = a['ids'].to(device)\n                mask = a['mask'].to(device)\n                output = model(ids, mask)\n                output = output['logits'].squeeze(-1)\n                output_probs = torch.sigmoid(output)\n                preds.append(output_probs.cpu().numpy())\n            preds = np.concatenate(preds)\n            allpreds.append(preds)\n      \n    return allpreds","2806de88":"pthes = [os.path.join(\".\/\",s) for s in os.listdir(\".\/\") if \".pth\" in s]","07061ddc":"allpreds = predicting(valid_dataloader, model, pthes)","714df835":"valid_probs = np.zeros((len(p_valid),6))\nfor i in range(kfold):\n    valid_probs += allpreds[i]\nvalid_probs = valid_probs \/ kfold","47bbab4b":"valid_probs = np.asarray(valid_probs).flatten()","e95562bc":"#valid_probs = allpreds[0].flatten() #This line is used when trianing for one model and not k-fold model \ny_valid = p_valid[['toxic', 'severe_toxic','obscene', 'threat', 'insult','identity_hate']].to_numpy().flatten()","e6115308":"fpr, tpr, _ = roc_curve(y_valid, valid_probs)\nprint('auc score for kfold =', kfold, 'models is: %.2f' %(auc(fpr, tpr)*100))","912a9b85":"fig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.set_title('ROC Curv')\nax.set_xlabel('FPR')\nax.set_ylabel('TPR')\nplt.show()","18c86bc7":"class BERTinferenceDataSet(Dataset):\n    \n    def __init__(self, sentences):\n        self.sentences = sentences\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, idx):\n        sentence = self.sentences[idx]\n        bert_sent = tokenizer.encode_plus(sentence, \n                                         add_special_tokens = True, #[SEP][PAD]\n                                         max_length = max_len,\n                                         pad_to_max_length = True,\n                                         truncation = True)\n\n        ids = torch.tensor(bert_sent['input_ids'], dtype = torch.long)\n        mask = torch.tensor(bert_sent['attention_mask'], dtype = torch.long)\n\n        return{\n            'ids' : ids,\n            'mask' : mask\n             }","4ffa0078":"test_batch = 32\ntest_dataset = BERTinferenceDataSet(test['clean_text'])\ntest_dataloader = DataLoader(test_dataset, batch_size = test_batch, shuffle = False, num_workers = 4, pin_memory = True)\npthes = [os.path.join(\"..\/input\/bert-model-for-dummies\",s) for s in os.listdir('..\/input\/bert-model-for-dummies') if \".pth\" in s]\npthes\n","55207976":"allpreds = predicting(test_dataloader, model, pthes)","fbe49fb1":"print('allpreds is an array with the shape of:',len(allpreds), 'x',len(allpreds[0]), 'x',len(allpreds[0][0]))\nallpreds[0][0]","dae929a5":"preds = np.zeros((len(test_dataset),6))\nfor i in range(kfold):\n    preds += allpreds[i]\npreds = preds \/ kfold","ef482512":"results = pd.DataFrame(preds)\nsubmission = pd.concat([test,results], axis = 1).drop(['comment_text', 'clean_text'], axis = 1)\nsubmission.rename(columns = { 0:'toxic', 1:'severe_toxic', 2:'obscene', 3:'threat', 4:'insult', 5:'identity_hate'}, inplace = True)\nsubmission.to_csv(\"submission.csv\", index = False)","72f9be1a":"## PART I\n## 1. Prepare the data\n### a. Loading libraries","dab2392e":"A comment can be toxic, severe_toxic, obscene and insult at the same time. So the class for each comment is not limited to one.\n<br>Now let's check the values for each column in train dataset.","76f478be":"You can start to run the notebook from here until the end. I have copied all the necessary parts from part I in this notebook. The objective here is to show how the model works for 5 different folds. epochs is set to 5 and the first 2000 rows of the trianing set is used. Feel free to change these parameters and see how it affects the accuracy.","75c2c079":"Let's have a look what's inside a dataset:","f48232b6":"We define a class BertDataSet with Dataset as super class and overwirte the__init_, __ len__ and __ getitem__ function in it. It will get the comment list and relevant toxic labels (6 labels in this case) and creates token ids and attention mask to distinguish the comments from the zero padding.\n\n**torch.tensor vs np.ndarray:** <br>\nIf you are only interested in efficient and easy way to perform mathematical operations on matrices np.ndarray or torch.tensor can be used interchangeably.\n\nHowever, torch.tensors are designed to be used in the context of gradient descent optimization, and therefore they hold not only a tensor with numeric values, but (and more importantly) the computational graph leading to these values. This computational graph is then used (using the chain rule of derivatives) to compute the derivative of the loss function w.r.t each of the independent variables used to compute the loss.\n\nAs mentioned before, np.ndarray object does not have this extra \"computational graph\" layer and therefore, when converting a torch.tensor to np.ndarray you must explicitly remove the computational graph of the tensor using the detach() command.","9856729c":"## Intro \nI am quite new in the field of data science and specially transformers are a very new concept to me. During my exploration for learning more about BERT, I noticed most of the material present on kaggle either discuss the underlying logics and modules in BERT model, (which is necessary for begginers) or they present blocks of code without much details. This material is an attempt to bridge the gap from the very basics and theory of BERT model to practicality of a simple model. <br>\nA lot of the work in this notebook is inspired by the work of [chumajin](https:\/\/www.kaggle.com\/chumajin) from [this notebook](https:\/\/www.kaggle.com\/chumajin\/pytorch-bert-beginner-s-room). I would like to thank him here and would add that I have adapt it to our problem in this competetion. I have also added more visualization to see the imporvements of the models over iterations in epochs and during different folds. \n\n### Objective\nCreating a model which predicts the probability of each type of toxicity for each comment.\n\nThis notebook is presented to two parts. I have separted it in two parts, due to memory limitations in kaggle. You can run either part I or part II independantly. <br>\nIn **part I** I try to explain the primary steps we need to take for handling similar tasks, for binary classification of different labels. The model in this part runs only for one distribuation of train set and validation set. and you can see the inital performance results(auc score) we get from this model. <br>\nIn **part II**, I have braught all the necessary blocks of code from part I and then show how 5 models can be developed for different choices of train set and validation set as in k_fold (k = 5). The probablites for all types of toxicity for each comment is then estimated as mean value from these 5 models. \n\n\n\n### Parameters affecting simulation time\nFor running this noteboook quickly, to see how it works, in both part I and part II, you can reduce the training set to the limited number of rows. I took 200 rows in part I and 2000 in part II.<br> \nThe reason for having it very low in part I is that the code opts for eductional resons rather than being efficient. Therefore, it reads in many different variables and cause the cuda memory crash if we go for more than 200 rows. This gives us an unimpressive results for part I. (accuracy 86%) <br>\nIn part II, code is more consice compared to part I, and it's possible to raise number of rows to include the whole training set. However, I keep it limited to 2000, so you manage to get a fairly good result (94% accuracy and 0.988 auc score) in 22 min of simulation. <br>\nYou can have (epochs = 5, k = 5) and see how the accuracy and losses imporve in different epochs. The accuracy and loss are in an acceptable range, as this notebook is not concerned about reaching the state-of-art results. <br>\n\nHope that you find it useful and if so, please don't forget to upvote this note book, it makes my day! :-)","eacf0eb3":"### b. Training function\nYou can find more details about some keywords in this block of code below it.","80b62163":"We'll have a look at the distribution of the length for tokenized comments to have an intuation about what to set as max_length for our tokenized comments.","12119ed0":"## 8. Repeat training for k-fold","4a1c03ac":"We take the average of the probabilities in 5 folds.","ec02574e":"### c. Validating function\nvalidating function is quite similar to training function. The difference is that there is no back-propagation and optemization for parameters in it. ","9f9da4f1":"## 10. Inference","dcbae392":"It seems that 200 is good size move on with. ","626350d3":"## PART II","003b75f5":"## 5. Functions for training and validating\n### a. Setting up some parameters","f134edcd":"Now we set the batch-size for trainig set and validation set. The bigger batch size helps to speed up the computations. However, the results confirm that using small batch sizes achieves the best generalization performance, for a given computation cost. In all cases, the best results have been obtained with batch sizes of 32 or smaller. Often mini-batch sizes as small as 2 or 4 deliver optimal results.","2cf2bc1d":"**amp** stands for automatic mixed precision. <br>\nTo learn more about the **autocast** read this link: <br>\nhttps:\/\/pytorch.org\/docs\/stable\/amp.html <br>\nin short: Instances of autocast serve as context managers or decorators that allow regions of your script to run in mixed precision, where some operations use the torch.float32 (float) datatype and other operations use torch.float16 (half).\nautocast should wrap only the forward pass(es) of your network, including the loss computation(s). Backward passes under autocast are not recommended. <br>\n<br>\n**with** statement in Python is used in exception handling to make the code cleaner and much more readable. It simplifies the management of common resources like file streams. Here is a link to learn more about it.<br>\nhttps:\/\/www.geeksforgeeks.org\/with-statement-in-python\/<br>\n\n**Non-Blocking** allows you to overlap compute and memory transfer to the GPU. Pinned Memory allows the non-blocking calls to actually be non-blocking.<br>\n\nWhy using **var.detach().cpu().numpy()** and not var.numpy() only? <br>\nbecause we get an error. The main reason behind this choice presumably is to avoid confusing new comers. People not very familiar with requires_grad and cpu\/gpu Tensors might go back and forth with numpy. For example doing pytorch -> numpy -> pytorch and backward on the last Tensor. This will backward without issue but not all the way to the first part of the code and won\u2019t raise any error.\nSo the choice has been made to force the user to detach() to make sure they want to do it and it\u2019s not a typo\/other library that does this tranformation and breaks the computational graph.<br>\n\n**optimizer.step():** it performs a parameter update based on the current gradient (stored in .grad attribute of a parameter) and the update rule. As an example, the update rule for SGD is defined here:<br>\nhttps:\/\/github.com\/pytorch\/pytorch\/blob\/cd9b27231b51633e76e28b6a34002ab83b0660fc\/torch\/optim\/sgd.py#L63 <br>\n\n**loss.backward()** Calling .backward() mutiple times accumulates the gradient (by addition) for each parameter. This is why you should call optimizer.zero_grad() after each .step() call. Note that following the first .backward call, a second call is only possible after you have performed another forward pass.\n\n### Gradient Scaling\nIf the forward pass for a particular op has float16 inputs, the backward pass for that op will produce float16 gradients. Gradient values with small magnitudes may not be representable in float16. These values will flush to zero (\u201cunderflow\u201d), so the update for the corresponding parameters will be lost.<br>\n\nTo prevent underflow, \u201cgradient scaling\u201d multiplies the network\u2019s loss(es) by a scale factor and invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are then scaled by the same factor. In other words, gradient values have a larger magnitude, so they don\u2019t flush to zero.<br>\n\nEach parameter\u2019s gradient (.grad attribute) should be unscaled before the optimizer updates the parameters, so the scale factor does not interfere with the learning rate.<br>\n\n**step()** carries out the following two operations:\n1. Internally invokes unscale_(optimizer) (unless unscale_() was explicitly called for optimizer earlier in the iteration). As part of the unscale_(), gradients are checked for infs\/NaNs.\n2. If no inf\/NaN gradients are found, invokes optimizer.step() using the unscaled gradients. Otherwise, optimizer.step() is skipped to avoid corrupting the params.","c9f15f84":"We set a seed for reproducability of the results.","021562af":"## 6. Training the model\n### a. Run training function","a672369e":"Here we can check inside the dataloader:","70300b72":"Taking the avarage of probabilities from k-models for each sample in the validation set.","731d6878":"## 4. BERT Modeling\nFor faster computation, let's set the device cuda if it is available.","365d74b4":"## 7. Evaluation\nWe use auc as a metric of evaluation in this report.","0f510a8a":"Relative percentage gives us a better understanding.","3f8e3064":"### b. Plotting the results","efb6a697":"## 11. Summary\nHere we presented a simple bert model for comment toxicity classification. A comment can be classified with multi-label and therefore this is an example of binary classification with multiple labels (not multi-classficaiton). <br>\nYou can run part I and part II of the notebook independantly. <br>\nThe best score(0.9796) achieved after running part II, with trianing the model with all the trainnig data (not reducing it to fewer rows) and having epochs = 1. The code is available in version 8. I didn't try with more epochs, because the trianing time would exceed 5h which was not the major intention with this notebook. <br>\nTraining the model with 2000 training samples and epochs = 5 results in a faster computation and gives the socre: 0.94311.","cc2217b6":"## 12. Useful links\n\n[1] https:\/\/www.kaggle.com\/parulpandey\/eda-and-preprocessing-for-bert <br>\n[2] http:\/\/jalammar.github.io\/illustrated-bert\/ <br>\n[3] https:\/\/curiousily.com\/posts\/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python\/ <br>\n[4] http:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/ <br>\n[5] https:\/\/iq.opengenus.org\/native-language-identification-dl\/","a5620790":"To get the output probabilities, we should use sigmoid function. This can be done either in the model or like here be applied to the output from model. I was first making the mistake and used softmax function. However, in our case a comment can be classified as toxic, obscene and insult at the same time. Hence it's not a multi-classficationm and therefore, this is not applicable. <br>\nHere is the summary of differences between softmax function and sigmoid funciton. <br>\n\n**Softmax funciton:**<br>\n- Used for Multi-classification in the Logistics Regression model\n- The probabilities sum will be 1<br>\n\n**Sigmoid funciton:**\n- Used for Binary Classification in the Logistic Regression model\n- The probabilities sum does not need to be 1\n\nTo learn more about them you can read this: https:\/\/medium.com\/arteos-ai\/the-differences-between-sigmoid-and-softmax-activation-function-12adee8cf322","0c0b0f87":"To improve our model we repeat the same process of training for each fold of k-folds.","fd0fcf29":"## 3. Prepare Data\n### a. Data cleaning \nWe need to clean, tokenize and convert the data to tensors. <br>\nThe function below helps us with:\n\n- Removing hyperlinks, punctuation and numbers\n- Tokenizeation\n\nWe skip changing it all to lower case and keep the letters sensetive to case. The comment BAD!! has stronger negative emotion in it than bad!! <br>\nWe also do not remove the stop words, Since with contextual models such as BERT and ROBERTA it is (almost) always better not to process the texts removing stopwords. These models are pretrained with stopwords: if you remove stop words your model may lose the context. This is also true for stemming and lemmatization preprocessing techniques. So we skip them too!","6503c369":"## Toxic Comment Classification","6bde026f":"The model makes use of id and mask from token encoding in the trian set. At this state, there is no need to feed in the model with the target values such as toxic, obscene, etc.","c6aa863e":"## 2. Summerize data\n### a. Descriptive statistics\n### Missing values","3f2ef72c":"Great! we have no missing value!\n### Distribution of each class in the train set and test labels\nLet's check how many of each category is found in our training data:","55755c3b":"### b. Dataset and Dataloader\nWe use 20% of the training data as validation set.","a01397d2":"In the code below pin_memory = True to a DataLoader will automatically put the fetched data Tensors in pinned memory, \nand thus enables faster data transfer to CUDA-enabled GPUs. This is best explained in the [NVIDIA blogpost](https:\/\/developer.nvidia.com\/blog\/how-optimize-data-transfers-cuda-cc\/). I will borrow the picture from it. ![image.png](attachment:9cd62966-eb55-4a49-855c-889367001825.png)<br>\nnum_workers as a positive integer will turn on multi-process data loading with the specified number of loader worker processesm","eacf7991":"We use ensemble for evaluating k-models in the validation set.","a10b20b0":"## 9. Evaluation for k-models","24514e6a":"We use the learning rate parameter as set above for 10% of total training time. We then decrease the learning rate gradualy to zero. "}}