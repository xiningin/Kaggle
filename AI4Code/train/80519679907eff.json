{"cell_type":{"c741da2d":"code","cfc2ec31":"code","eb7fe70b":"code","a29d60d5":"code","2f62d4d1":"code","45fb1e64":"code","6d0f70cd":"code","f05a0196":"code","9f6ce27a":"code","41f33c6d":"code","93d0c116":"code","36350302":"code","1e34c490":"code","ba508814":"code","ae0003ee":"code","29faa328":"code","f5f7a4d1":"code","2e653d22":"code","2257f233":"code","c97ddd65":"code","42cf47dd":"code","306bc150":"code","7490f56b":"code","7f9e828c":"code","a20640f5":"code","63f93c50":"code","1c16f12d":"code","6a0528aa":"code","6c9f9206":"code","f21e74a8":"code","918255e2":"code","7281020a":"code","1bb8eef5":"code","22189a20":"code","32122685":"code","5e06f278":"markdown","b75e8527":"markdown","f9bd598e":"markdown","51e090c8":"markdown","40a896a5":"markdown","cfa4b0bb":"markdown","5bf296af":"markdown","537d87f4":"markdown","1a66c162":"markdown","9d854007":"markdown","4b7561da":"markdown"},"source":{"c741da2d":"# Libs Base\nimport numpy as np # \u00e1lgebra linear\nimport pandas as pd # processamento de dados, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Visualiza\u00e7\u00e3o\nimport seaborn as sns #Visualiza\u00e7\u00e3o\n \n# Transforma\u00e7\u00e3o \nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import power_transform\nfrom sklearn.pipeline import Pipeline\n\n# Modelos\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import datasets, linear_model\n\nimport sklearn.model_selection as mdl\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n# M\u00e9tricas\nfrom easymetrics import r2_all\nfrom easymetrics import evs_all\nfrom easymetrics import mae_all\nfrom easymetrics import rmse_all\nfrom easymetrics import rmsle_all\nfrom sklearn import metrics\n\n#Feature Selection\nimport lime\nimport lime.lime_tabular\nimport shap\nfrom sklearn.ensemble import StackingRegressor\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.decomposition import PCA\nfrom pandas.plotting import autocorrelation_plot\n\n\n\npd.set_option('display.max_columns', 500)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","cfc2ec31":"def plot_r2_mae_evs_rmse_rmsle(data):\n    X = np.arange(5)\n    fig = plt.figure(figsize=(12, 6))\n    ax = fig.add_axes([0,0,1,1])\n    ax.bar(X + 0.00, data[0], color = 'b', width = 0.25)\n    ax.bar(X + 0.25, data[1], color = 'r', width = 0.25)\n    ax.bar(X + 0.50, data[2], color = 'g', width = 0.25)\n    ax.set_ylabel(\"Values\")\n    ax.set_title(\"Metrics\")\n    ax.set_xticks(X + 0.20 \/ 2)\n    ax.set_xticklabels(('R2', 'MAE', 'EVS', 'RMSE', 'RMSLE'))\n    ax.legend(labels=['Train', 'Test','Valid'])\n    return fig, ax","eb7fe70b":"df = pd.read_csv('..\/input\/SolarEnergy\/SolarPrediction.csv')\ndf.head()\n","a29d60d5":"df.shape\ndf.head()","2f62d4d1":"df.info()","45fb1e64":"df.describe()","6d0f70cd":"df.drop('UNIXTime', axis= \"columns\", inplace = True)","f05a0196":"corr = df.corr()\nsns.heatmap(corr)","9f6ce27a":"df.columns","41f33c6d":"df1=df[['Data', 'Time', 'Radiation', 'Temperature', 'Pressure',\n       'Humidity', 'WindDirection(Degrees)', 'Speed', 'TimeSunRise',\n       'TimeSunSet']]\nh = df1.hist(bins=25,figsize=(16,16),xlabelsize='10',ylabelsize='10',xrot=-15)\nsns.despine(left=True, bottom=True)\n[x.title.set_size(12) for x in h.ravel()];\n[x.yaxis.tick_left() for x in h.ravel()];","93d0c116":"#df = df[['Temperature', 'Pressure','Humidity', 'WindDirection(Degrees)', 'Speed', 'Radiation']] #Vari\u00e1veis Independentes \ndf = df[['Temperature', 'Pressure', 'Humidity', 'Speed', 'Radiation']] #Vari\u00e1veis Independentes\ndf.head(10)\n","36350302":"scaler = MinMaxScaler()\n\ndf = power_transform(df, method='yeo-johnson')\ndf = scaler.fit_transform(df)\n\nX = df[:,0:4]\ny = df[:,4]\n\nfig = plt.figure(figsize=(15,10))\nlst1 = list(y)\nlst2 = list(X)\nsns.distplot(lst1)\nsns.distplot(lst2)\nfig.legend(labels=['Prediction Data','Test Data'], fontsize=20)\nplt.show()\nfig.savefig('normalization.png')","1e34c490":"X_train, X_test, y_train, y_test = mdl.train_test_split(X, y, test_size=0.3)","ba508814":"from sklearn.linear_model import LinearRegression \n\nlm = LinearRegression() \nlm.fit(X_train,y_train) ","ae0003ee":"print(lm.intercept_)","29faa328":"predictions = lm.predict(X_test)","f5f7a4d1":"# R_square \nsse = np.sum((predictions - y_test)**2)\nsst = np.sum((y_test - y_test.mean())**2)\nR_square = 1 - (sse\/sst)\nprint('R square obtain for normal equation method is :',R_square)","2e653d22":"plt.scatter(y_test,predictions)","2257f233":"sns.distplot((y_test-predictions)); ","c97ddd65":"f = plt.figure(figsize=(14,5))\nax = f.add_subplot(121)\nsns.scatterplot(y_test,predictions,ax=ax,color='r')\nax.set_title('Checagem de Linearidade:\\n Atual Vs Valor Previsto')\n\n\nax = f.add_subplot(122)\nsns.distplot((y_test - predictions),ax=ax,color='b')\nax.axvline((y_test - predictions).mean(),color='k',linestyle='--')\nax.set_title('Checagem de Normalidade Residual & Valor m\u00e9dio: \\n Erro Residual');","42cf47dd":"print('MAE:', metrics.mean_absolute_error(y_test, predictions)) \nprint('MSE:', metrics.mean_squared_error(y_test, predictions)) \nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions))) \n#print('R2:', metrics.r2_score(y_test, predictions))) ","306bc150":"rf = RandomForestRegressor()\n\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [1, 5, 10, 20, 30, 50, 100],\n    'max_depth': [80, 90, 100],\n    #'max_features': [10, 15, 20, 24],\n    'min_samples_leaf': [1, 2, 3, 4, 5],\n    'min_samples_split': [2, 4, 6, 8],\n    'n_estimators': [5, 10, 20, 50]\n}\n\nrf = RandomForestRegressor()\n\nrf_gs = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 5, n_jobs = -1, verbose = 1)\n\n%time rf_gs.fit(X_train, np.ravel(y_train,order='C'))\nrf_best = rf_gs.best_estimator_\n\ny_pred_rf= rf_best.predict(X_test)\nprint(rf_gs.best_params_)","7490f56b":"r2_train, r2_test, r2_valid = r2_all(rf_best, X_train, y_train, X_test, y_test, y_pred_rf)\nevs_train, evs_test, evs_valid = evs_all(rf_best, X_train, y_train, X_test, y_test, y_pred_rf)\nmae_train, mae_test, mae_valid = mae_all(rf_best, X_train, y_train, X_test, y_test, y_pred_rf)\nrmse_train, rmse_test, rmse_valid = rmse_all(rf_best, X_train, y_train, X_test, y_test, y_pred_rf)\nrmsle_train, rmsle_test, rmsle_valid = rmsle_all(rf_best, X_train, y_train, X_test, y_test, y_pred_rf)\n\ndata = [[r2_train, mae_train, evs_train, rmse_train, rmsle_train],\n        [r2_test, mae_test, evs_test, rmse_test, rmsle_test],\n        [r2_valid, mae_valid, evs_valid, rmse_valid, rmsle_valid]]\n\nfig, ax = plot_r2_mae_evs_rmse_rmsle(data)","7f9e828c":"nof_list=np.arange(1,6)            \nhigh_score=0\n#Vari\u00e1vel para armazenar o melhor cen\u00e1rio\nnof=0           \nscore_list =[]\nfor n in range(len(nof_list)):\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n    rfe = RFE(rf_best,n_features_to_select=nof_list[n])\n    rfe = rfe.fit(X_train,y_train)\n    score = rfe.score(X_test,y_test)\n    score_list.append(score)\n    print(f'NOF: {nof_list[n]}, Score: {score}')\n    if(score > high_score):\n        high_score = score\n        nof = nof_list[n]\n        rfe_best = rfe\n\nprint(\"Optimum Number of Features: %d\" %nof)\nprint(\"Score with %d Features: %f\" % (nof, high_score))\n\ny_pred_rfe = rfe_best.predict(X_test)","a20640f5":"r2_train, r2_test, r2_valid = r2_all(rfe_best, X_train, y_train, X_test, y_test, y_pred_rfe)\nevs_train, evs_test, evs_valid = evs_all(rfe_best, X_train, y_train, X_test, y_test, y_pred_rfe)\nmae_train, mae_test, mae_valid = mae_all(rfe_best, X_train, y_train, X_test, y_test, y_pred_rfe)\nrmse_train, rmse_test, rmse_valid = rmse_all(rfe_best, X_train, y_train, X_test, y_test, y_pred_rfe)\nrmsle_train, rmsle_test, rmsle_valid = rmsle_all(rfe_best, X_train, y_train, X_test, y_test, y_pred_rfe)\n\ndata = [[r2_train, mae_train, evs_train, rmse_train, rmsle_train],\n        [r2_test, mae_test, evs_test, rmse_test, rmsle_test],\n        [r2_valid, mae_valid, evs_valid, rmse_valid, rmsle_valid]]\n\nfig, ax = plot_r2_mae_evs_rmse_rmsle(data)","63f93c50":"nof_list=np.arange(1, 5)       \nhigh_score=0\n#Vari\u00e1vel para armazenar o melhor cen\u00e1rio\nnof=0           \nscore_list =[]\nfor n in range(len(nof_list)):\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n    fs = SelectKBest(score_func=f_regression, k=nof_list[n])\n    univariate_k = Pipeline([('fs', fs), ('m', RandomForestRegressor())])\n    univariate_k.fit(X_train,y_train)\n    score = univariate_k.score(X_test,y_test)\n    score_list.append(score)\n    print(f'NOF: {nof_list[n]}, Score: {score}')\n    if(score > high_score):\n        high_score = score\n        nof = nof_list[n]\n        univariate_k_best = univariate_k\n\nprint(\"Optimum Number of Features: %d\" %nof)\nprint(\"Score with %d Features: %f\" % (nof, high_score))\n\ny_pred_k = univariate_k_best.predict(X_test)","1c16f12d":"r2_train, r2_test, r2_valid = r2_all(rf_best, X_train, y_train, X_test, y_test, y_pred_rf)\nevs_train, evs_test, evs_valid = evs_all(rf_best, X_train, y_train, X_test, y_test, y_pred_rf)\nmae_train, mae_test, mae_valid = mae_all(rf_best, X_train, y_train, X_test, y_test, y_pred_rf)\nrmse_train, rmse_test, rmse_valid = rmse_all(rf_best, X_train, y_train, X_test, y_test, y_pred_rf)\nrmsle_train, rmsle_test, rmsle_valid = rmsle_all(rf_best, X_train, y_train, X_test, y_test, y_pred_rf)\n\ndata = [[r2_train, mae_train, evs_train, rmse_train, rmsle_train],\n        [r2_test, mae_test, evs_test, rmse_test, rmsle_test],\n        [r2_valid, mae_valid, evs_valid, rmse_valid, rmsle_valid]]\n\nfig, ax = plot_r2_mae_evs_rmse_rmsle(data)","6a0528aa":"nof_list=np.arange(1,11)            \nhigh_score=0\n#Vari\u00e1vel para armazenar o melhor cen\u00e1rio\nnof=0           \nscore_list =[]\nfor n in range(len(nof_list)):\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n    fs = SelectPercentile(score_func=f_regression, percentile=(nof_list[n]*10))\n    univariate_percent = Pipeline([('fs', fs), ('m', rf_best)])\n    univariate_percent.fit(X_train,y_train)\n    score = univariate_percent.score(X_test,y_test)\n    score_list.append(score)\n    print(f'NOF: {nof_list[n]}, Score: {score}')\n    if(score > high_score):\n        high_score = score\n        nof = nof_list[n]\n        \nprint(\"Optimum Number of Features: %d\" %nof)\nprint(\"Score with %d Features: %f\" % (nof, high_score))","6c9f9206":"fs = SelectPercentile(score_func=f_regression, percentile=nof)\nunivariate_percent_best = Pipeline([('fs', fs), ('m', rf_best)])\n%time univariate_percent_best.fit(X_train,y_train)\ny_pred = univariate_percent_best.predict(X_test)","f21e74a8":"r2_train, r2_test, r2_valid = r2_all(univariate_percent_best, X_train, y_train, X_test, y_test, y_pred)\nevs_train, evs_test, evs_valid = evs_all(univariate_percent_best, X_train, y_train, X_test, y_test, y_pred)\nmae_train, mae_test, mae_valid = mae_all(univariate_percent_best, X_train, y_train, X_test, y_test, y_pred)\nrmse_train, rmse_test, rmse_valid = rmse_all(univariate_percent_best, X_train, y_train, X_test, y_test, y_pred)\nrmsle_train, rmsle_test, rmsle_valid = rmsle_all(univariate_percent_best, X_train, y_train, X_test, y_test, y_pred)\n\ndata = [[r2_train, mae_train, evs_train, rmse_train, rmsle_train],\n        [r2_test, mae_test, evs_test, rmse_test, rmsle_test],\n        [r2_valid, mae_valid, evs_valid, rmse_valid, rmsle_valid]]\n\nfig, ax = plot_r2_mae_evs_rmse_rmsle(data)","918255e2":"nof_list=np.arange(1,5)            \nhigh_score=0\n#Vari\u00e1vel para armazenar o melhor cen\u00e1rio\nnof=0           \nscore_list =[]\nfor n in range(len(nof_list)):\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n    fs = PCA(n_components=nof_list[n])\n    pca = Pipeline([('fs', fs), ('m', rf_best)])\n    pca.fit(X_train,y_train)\n    score = pca.score(X_test,y_test)\n    score_list.append(score)\n    print(f'NOF: {nof_list[n]}, Score: {score}')\n    if(score > high_score):\n        high_score = score\n        nof = nof_list[n]\n        pca_best = pca\n\nprint(\"Optimum Number of Features: %d\" %nof)\nprint(\"Score with %d Features: %f\" % (nof, high_score))","7281020a":"fs = PCA(n_components=nof)\npca_best = Pipeline([('fs', fs), ('m', rf_best)])\n%time pca_best.fit(X_train,y_train)\ny_pred_pca = pca_best.predict(X_test)","1bb8eef5":"r2_train, r2_test, r2_valid = r2_all(pca_best, X_train, y_train, X_test, y_test, y_pred_pca)\nevs_train, evs_test, evs_valid = evs_all(pca_best, X_train, y_train, X_test, y_test, y_pred_pca)\nmae_train, mae_test, mae_valid = mae_all(pca_best, X_train, y_train, X_test, y_test, y_pred_pca)\nrmse_train, rmse_test, rmse_valid = rmse_all(pca_best, X_train, y_train, X_test, y_test, y_pred_pca)\nrmsle_train, rmsle_test, rmsle_valid = rmsle_all(pca_best, X_train, y_train, X_test, y_test, y_pred_pca)\n\ndata = [[r2_train, mae_train, evs_train, rmse_train, rmsle_train],\n        [r2_test, mae_test, evs_test, rmse_test, rmsle_test],\n        [r2_valid, mae_valid, evs_valid, rmse_valid, rmsle_valid]]\n\nfig, ax = plot_r2_mae_evs_rmse_rmsle(data)","22189a20":"ensemble = StackingRegressor([('RFE', rfe_best), ('K', univariate_k_best), ('Percent', univariate_percent_best), ('PCA', pca_best)])\n%time ensemble.fit(X_train, y_train)\ny_pred = ensemble.predict(X_test)","32122685":"r2_train, r2_test, r2_valid = r2_all(ensemble, X_train, y_train, X_test, y_test, y_pred)\nevs_train, evs_test, evs_valid = evs_all(ensemble, X_train, y_train, X_test, y_test, y_pred)\nmae_train, mae_test, mae_valid = mae_all(ensemble, X_train, y_train, X_test, y_test, y_pred)\nrmse_train, rmse_test, rmse_valid = rmse_all(ensemble, X_train, y_train, X_test, y_test, y_pred)\nrmsle_train, rmsle_test, rmsle_valid = rmsle_all(ensemble, X_train, y_train, X_test, y_test, y_pred)\n\ndata = [[r2_train, mae_train, evs_train, rmse_train, rmsle_train],\n        [r2_test, mae_test, evs_test, rmse_test, rmsle_test],\n        [r2_valid, mae_valid, evs_valid, rmse_valid, rmsle_valid]]\n\nfig, ax = plot_r2_mae_evs_rmse_rmsle(data)","5e06f278":"### 4.5 PCA","b75e8527":"# 1. Bibliotecas Base","f9bd598e":"# 4. Modelos e Feature Selection","51e090c8":"### 4.3 Univariate - K-Best","40a896a5":"### 4.1 Random Forest","cfa4b0bb":"# 2. Fun\u00e7\u00e3o de Suporte","5bf296af":"### 4.6 Ensemble Stacking Regressor","537d87f4":"### Table of Contents\n\n1. [Bibliotecas Base](#1.-Bibliotecas-Base)\n\n2. [Fun\u00e7\u00e3o de Suporte](#2.-Fun\u00e7\u00e3o-de-Suporte)\n\n3. [An\u00e1lise de Dados](#3.-An\u00e1lise-de-Dados)\n\n4. [Modelos e Feature Selection](#4.-Modelos-e-Feature-Selection)","1a66c162":"### 4.2 RFE","9d854007":"### 4.4 Univariate - Percentile","4b7561da":"# 3. An\u00e1lise de Dados"}}