{"cell_type":{"823bd2c6":"code","1179c5c7":"code","7b56c0e2":"code","01855919":"code","97901e47":"code","cd1500b5":"code","6ecd51fa":"code","7d884f9f":"code","9926ba04":"code","4a262d8d":"code","7a431f58":"code","491afb72":"code","1f61e540":"code","14d5b82a":"code","1228bc6f":"code","e5dc783f":"code","05dd0783":"code","fef816f1":"code","09408347":"code","9e8c0bdd":"code","d663a5e7":"code","2449e27a":"code","0e538b0e":"code","1be47d4d":"code","3b49e2c9":"code","cd6770dc":"code","365560f9":"code","7dc84d73":"code","46c7f028":"code","0a33e93d":"code","e2ea3dc0":"code","77ac17fa":"markdown","0829ed14":"markdown","5e71e598":"markdown","ebaae270":"markdown","422fe3ca":"markdown","f11de5a5":"markdown","52811a24":"markdown","cd4aeee1":"markdown","78d0617b":"markdown","5792216c":"markdown","c0cb860b":"markdown","5d8a05ce":"markdown","863f2f09":"markdown","5382c776":"markdown","08631175":"markdown","63d1a0d0":"markdown","ab10aa75":"markdown","c4a896f9":"markdown"},"source":{"823bd2c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport sqlite3\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nimport matplotlib.ticker as ticker\nimport matplotlib.dates as mdates\nfrom mpl_toolkits.basemap import Basemap\nimport hashlib\nimport enum\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory","1179c5c7":"#Enum class to specify two file format(csv and parquet)\nclass FileType(enum.Enum):\n    csv_format = 1\n    parquet_format = 2","7b56c0e2":"def extract_path_name(dirname_path='\/kaggle\/input') -> str:\n    path_for_database = \"\"\n    for dirname, _, filenames in os.walk(dirname_path):\n        for filename in filenames:\n            path_for_database = os.path.join(dirname, filename)\n    return path_for_database\n\n#Connect to database file_type:FileType\ndef open_connection():\n    path_for_database = extract_path_name()\n    conn = sqlite3.connect(path_for_database)\n    return conn\n\n\ndef get_all_tables_in_database(conn=None):\n    \n    new_conn = False\n    \n    if conn is None:\n        conn = open_connection()    \n        new_conn = True\n    cursor = conn.cursor()\n    result = cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n    cursor.close()\n    \n    if new_conn:\n        conn.close()\n    \n    return sorted(list(zip(*result))[0])  # Note: * makes it iteritable\n\nconn = open_connection()\nall_tables = get_all_tables_in_database(conn)\nconn.close()\n","01855919":"#Get the fire database into RAM\n#To preventing query the database all the time we cache the data\n\ndef read_from_database(sql_query, conn, parameters):\n    df_raw = pd.DataFrame()\n    try:\n        print(\"Reading from database\")\n        df_raw = pd.read_sql_query(sql_query, con=conn, params=parameters)\n    except (KeyboardInterrupt, SystemExit):\n        conn.close()\n    return df_raw\n\ndef create_cache_folder(name):\n    if not os.path.isdir(name):\n        print(\"making _cache directory\")\n        os.makedirs(name)\n\n\ndef execute_sql_query(sql_query,file_type: FileType, parameters=None, conn=None):\n    \"\"\"\n    Method to query data from SQL database and return panda dataframe\n    \n    Parameters\n    \n    sql_query : str\n    parameters : dict\n    FileType : csv or parquet format\n    \"\"\"\n    new_conn = False\n    if conn is None:\n        conn = open_connection()\n        new_conn = True\n    #Hash the query\n    query_hash = hashlib.sha1(sql_query.format(parameters).encode()).hexdigest()\n    #Create the filepath\n      \n    if file_type is FileType.csv_format:\n        file_path_csv = os.path.join(\"_cache\",\"{}.csv\".format(query_hash))\n        if os.path.exists(file_path_csv):\n            print(\"Reading from csv file\")\n            df_raw = pd.read_csv(file_path_csv)\n        else:\n            df_raw = read_from_database(sql_query, conn, parameters)\n            create_cache_folder(\"_cache\")\n            print(\"writing dataframe to a csv file\")\n            df_raw.to_csv(file_path_csv, index=False)\n    else: #It is parquet format\n        file_path_parquet = os.path.join(\"_cache\",\"{}.parquet\".format(query_hash))\n        if os.path.exists(file_path_parquet):\n            print(\"Reading from parquet file\")\n            df_raw = pd.read_parquet(file_path_parquet)\n        else:\n            df_raw = read_from_database(sql_query, conn, parameters)\n            create_cache_folder(\"_cache\")\n            print(\"writing dataframe to a parquet file\")\n            df_raw.to_parquet(file_path_parquet)    \n            \n    if new_conn:\n        conn.close()\n    \n    return df_raw\n\nsql_query = \"SELECT * FROM Fires;\"  \ndf = execute_sql_query(sql_query, FileType.csv_format)\n\n        ","97901e47":"#Lets understand the memory usage for the data more\ndf.info(memory_usage='deep')","cd1500b5":"#understand the limits for df data\ndf.describe()","6ecd51fa":"\nfor dtype in ['float','int','object']:\n    selected_dtype = df.select_dtypes(include=[dtype])\n    mean_usage_b = selected_dtype.memory_usage(deep=True).mean()\n    mean_usage_mb = mean_usage_b \/ 1024 ** 2\n    print(\"Average memory usage for {} columns: {:03.2f} MB\".format(dtype,mean_usage_mb))","7d884f9f":"def mem_usage(pandas_obj):\n    if isinstance(pandas_obj, pd.DataFrame):\n        usage_b = pandas_obj.memory_usage(deep=True).sum()\n    else:\n        usage_b = pandas_obj.memory_usage(deep=True)\n    usage_mb = usage_b \/ 1024 ** 2\n    return \"{:03.2f} MB\".format(usage_mb)\n#Optimize memory usage for int values\ndf_int = df.select_dtypes(include=['int'])\nprint(df_int.describe())\nprint(\"The number of NULLS in int are {} \".format(df_int.isnull().values.sum()))\nprint(\"==========================================================\")\nconverted_int = df_int.apply(pd.to_numeric,downcast='unsigned')\nprint(converted_int.describe())\nprint(\"==========================================================\")\nprint(\"Memory usage for int value before conversion: {}\".format(mem_usage(df_int)))\nprint(\"Memory usage for int value after conversion: {}\".format(mem_usage(converted_int)))\ncompare_ints = pd.concat([df_int.dtypes,converted_int.dtypes],axis=1)\ncompare_ints.columns = ['before','after']\ncompare_ints.apply(pd.Series.value_counts)","9926ba04":"df_float = df.select_dtypes(include=['float'])\nprint(df_float.describe())\nprint(\"The number of NULLS in float are {} \".format(df_float.isnull().values.sum()))\nprint(\"==========================================================\")\nconverted_float = df_float.apply(pd.to_numeric,downcast='float')\nprint(converted_float.describe())\nprint(\"==========================================================\")\nprint(\"Memory usage for float value before conversion: {}\".format(mem_usage(df_float)))\nprint(\"Memory usage for float value before conversion: {}\".format(mem_usage(converted_float)))\ncompare_floats = pd.concat([df_float.dtypes,converted_float.dtypes],axis=1)\ncompare_floats.columns = ['before','after']\ncompare_floats.apply(pd.Series.value_counts)","4a262d8d":"#MSE for \nprint(\"MSE for converted and original values for DISCOVERY_DATE {} \".format(np.square(df_float.DISCOVERY_DATE-converted_float.DISCOVERY_DATE).mean()))\nprint(\"MSE for converted and original values for CONT_DATE {} \".format(np.square(df_float.CONT_DATE-converted_float.CONT_DATE).mean()))","7a431f58":"optimized_df = df.copy()\noptimized_df[converted_int.columns] = converted_int\noptimized_df[converted_float.columns] = converted_float\nprint(\"Memory usage before int and float reduction: {}\".format(mem_usage(df)))\nprint(\"Memory usage after int and float reduction: {}\".format(mem_usage(optimized_df)))","491afb72":"df_obj = df.select_dtypes(include=['object']).copy()\nprint(df_obj.describe())\nprint(df_obj.shape)\nprint(\"The number of NULLS in object\/categoric are {} \".format(df_float.isnull().values.sum()))","1f61e540":"dow = df_obj.SOURCE_SYSTEM_TYPE\nprint(dow.head())\ndow_cat = dow.astype('category')\nprint(dow_cat.head())","14d5b82a":"#To see the converted category attributes\ndow_cat.cat.codes.head()","1228bc6f":"print(\"Memory usage for the SOURCE_SYSTEM_TYPE object type before conversion is {}\".format(mem_usage(dow)))\nprint(\"Memory usage for the SOURCE_SYSTEM_TYPE object type after conversion is {}\".format(mem_usage(dow_cat)))","e5dc783f":"converted_obj = df_obj.copy()\nfor col in df_obj.columns:\n    num_unique_values = len(df_obj[col].unique())\n    num_total_values = len(df_obj[col])\n    if num_unique_values \/ num_total_values < 0.5:\n        converted_obj.loc[:,col] = df_obj[col].astype('category')","05dd0783":"print(\"Memory usage for the object type before conversion is {}\".format(mem_usage(df_obj)))\nprint(\"Memory usage for the object type after conversion is {}\".format(mem_usage(converted_obj)))\ncompare_obj = pd.concat([df_obj.dtypes,converted_obj.dtypes],axis=1)\ncompare_obj.columns = ['before','after']\ncompare_obj.apply(pd.Series.value_counts)","fef816f1":"optimized_df[converted_obj.columns] = converted_obj\nprint(\"Memory usage for the before conversion is {}\".format(mem_usage(df)))\nprint(\"Memory usage after applying all conversion unit is {}\".format(mem_usage(optimized_df)))","09408347":"optimized_df.describe()","9e8c0bdd":"#df.info()\noptimized_df.info(memory_usage='deep')","d663a5e7":"optimized_df['FIRE_YEAR'].head()","2449e27a":"plt.figure(figsize=(16, 9))\nsns.set(style=\"white\")\nax = sns.countplot(x=\"FIRE_YEAR\", data = optimized_df, palette=\"Blues_d\")\nax.set_title(\"Frequency of wildfire per Year\", fontdict = {'fontsize':30, 'fontweight':'bold'})\nax.set_xlabel(\"Year\", fontdict = {'fontsize':20, 'fontweight': 'medium'})\nax.set_ylabel(\"Frequency\", fontdict = {'fontsize':20, 'fontweight': 'medium'})\nax.grid(which = 'major',color = 'grey', linewidth = 0.2)","0e538b0e":"labels = {  'A' : '(0-0.25]',\n            'B' : '[0.26-9.9]',\n            'C':'[10.0-99.9]', \n            'D':'[100-299]', \n            'E':'[300-999]', \n            'F':'[1000-4999]', \n            'G': '[5000 - inf)'}\n\nplt.figure(figsize=(16, 9))\nax = sns.countplot(x=\"FIRE_SIZE_CLASS\", data = optimized_df, palette=\"Blues_d\")\nax.set_title(\"Count of Fire by Size\", fontdict = {'fontsize':30, 'fontweight':'bold'})\nax.set_xlabel(\"Classes of Wildfires(Acres)\", fontdict = {'fontsize':20, 'fontweight': 'medium'})\nax.set_ylabel(\"Frequency\", fontdict = {'fontsize':20, 'fontweight': 'medium'})\nax.set_xticklabels(labels.values())\nax.grid(which = 'major',color = 'grey', linewidth = 0.2)\n","1be47d4d":"plt.figure(figsize=(16, 9))\nax = sns.countplot(x='STATE', data=optimized_df, palette='Blues_d', order = optimized_df.STATE.value_counts().index)\nax.set_title(\"WildFire by states\", fontdict = {'fontsize':30, 'fontweight':'bold'})\nax.set_xlabel(\"States\", fontdict = {'fontsize':20, 'fontweight': 'medium'})\nax.set_ylabel(\"Frequency\", fontdict = {'fontsize':20, 'fontweight': 'medium'})","3b49e2c9":"plt.figure(figsize=(16, 9))\nax = sns.FacetGrid(col = 'FIRE_YEAR', height = 5, aspect = 2, col_wrap=4, data=optimized_df)\nax.map(sns.countplot, 'STATE', order = optimized_df.STATE.unique())\nfor i in ax.axes.flat:\n    i.set_title(i.get_title(), fontsize='xx-large')\n    i.set_ylabel(i.get_ylabel(), fontsize='xx-large')\n    i.set_xlabel(i.get_xlabel(),fontsize = 'xx-large')","cd6770dc":"plt.figure(figsize=(16, 9))\nfire_date = optimized_df.groupby('DISCOVERY_DOY').size()\n#print(fire_date.values)\nax = sns.scatterplot(data=fire_date, s=150)\nax.grid(which = 'major',color = 'grey', linewidth = 0.05)\nax.set_title(\"Frequency of wildfire by Day of the year\", fontdict = {'fontsize':30, 'fontweight':'bold'})\nax.set_xlabel(\"Days in a year\", fontdict = {'fontsize':20, 'fontweight': 'medium'})\nax.set_ylabel(\"Frequency\", fontdict = {'fontsize':20, 'fontweight': 'medium'})","365560f9":"sns.heatmap(pd.crosstab(df.FIRE_YEAR, optimized_df.STAT_CAUSE_DESCR))","7dc84d73":"sql_query = \"SELECT * FROM Fires;\"  \ndf2 = execute_sql_query(sql_query, FileType.parquet_format)","46c7f028":"def check_file_size(file_name):\n    if os.path.exists(file_name):\n        statinfo = os.stat(file_name)\n        print(\"The size of the {} is {} bytes\".format(file_name,statinfo.st_size))\n    else:\n        print(\"File not in the directory\")\n        return None\n    return statinfo.st_size\n\nsql_query = \"SELECT * FROM Fires;\"\nparameters=None\nquery_hash = hashlib.sha1(sql_query.format(parameters).encode()).hexdigest()\nfile_path_csv = os.path.join(\"_cache\",\"{}.csv\".format(query_hash))\nfile_path_parquet = os.path.join(\"_cache\",\"{}.parquet\".format(query_hash))   \ncsv = check_file_size(file_path_csv)\nparquet = check_file_size(file_path_parquet)\nprint(\"The differece between csv - {} bytes and parquet - {} bytes is {} and the percentage ratio of parquet to csv is {}%\".format(csv, parquet, (csv-parquet),(parquet\/csv) *100 ) )\n","0a33e93d":"import pyarrow.parquet as pq\nparquet_file = pq.ParquetFile(file_path_parquet)\nprint(parquet_file.metadata)\nprint(\"=======================================================\")\nprint(parquet_file.schema)","e2ea3dc0":"parquet_file.read_row_group(0)","77ac17fa":"We see that object columns consume most of the data because of the way strings are stored.","0829ed14":"We convert all the object colum that has unique variable less than 50%, to avoid the category data from using more memory.","5e71e598":"**Summary**\n> *  The Approach of understanding the data and trying to reduce the memory size of the dataframe is great, but as well has limitation. To understand more about this please look at [Using Pandas with Large Data Sets in Python](https:\/\/www.dataquest.io\/blog\/pandas-big-data\/)\n> *  We are still working with Csv files which consumes alot of memory in the RAM because of the way data is stored in csv format\n> *  To reduce the size of memory we will explore using Parquet format which stores unstructed data in a column format. This column storage approach helps reduce the time it takes to make query to the file. This is the same technology used to make Bigquery. To understand more you can read the [Dremel](https:\/\/static.googleusercontent.com\/media\/research.google.com\/en\/\/pubs\/archive\/36632.pdf) paper.\n> *  We will still leverage on the function that we created early on **extract_sql_query**. We store the subsection of the data as a parquet format","ebaae270":"Generate Parquet file for the same df to compare the size of csv and parquet","422fe3ca":"Fire by size","f11de5a5":"Parquet is a more efficient way of storing data.\nLets look at how Parquet stores the file and also the metadata information.","52811a24":"From the table above, we see that DISCOVERY_DATE and CONT_DATE changed, so we need to investigate why did that happen.","cd4aeee1":"Look at tables that have small unique values","78d0617b":"**Lets understand the data and look at the most import features for the main cause of Fire in the US**\n","5792216c":"To my gretest surprise I got an MSE of zero.\nNext we apply the changes to the original data to see how this conversion would help reduce the memory size of the df.","c0cb860b":"Learning how to Shrink the memory of a dataframe.","5d8a05ce":"We will now write a function to query database","863f2f09":"Start analyzing the data","5382c776":"Reduced not that much. But saved us more than 90MB of space, not bad.","08631175":"Parquet format is a more efficient way to save data to the disk. You can even compress the binariers using snappy, gzip etc to further save memory space. Thank you","63d1a0d0":"Thats great, we were able to reduce the memory size, but how the memory size might increase when applying one hot encoding for classification problem.","ab10aa75":"Wildfire \n* Credit to this tutorial\n> [https:\/\/www.dataquest.io\/blog\/pandas-big-data\/](https:\/\/www.dataquest.io\/blog\/pandas-big-data\/) -> for data Optimization of csv files","c4a896f9":" We were able to reduce the size of our int data without losing any information. Great!"}}