{"cell_type":{"10e3d4aa":"code","30fb515a":"code","d283af88":"code","57bef314":"code","7f6794d8":"code","0f2872ba":"code","a1feba2b":"code","4eaa7c19":"code","c4031b24":"code","e10dc4b5":"code","df9a184e":"code","70c01564":"code","3f583efc":"code","8bdb8922":"code","8c2d1de6":"code","11bdbdea":"code","af213e53":"code","d84e001a":"code","2d05575d":"code","6cdaaefd":"code","cc04aecc":"code","1a4e8b45":"code","b52d644a":"code","b95716f2":"code","30eea0f7":"code","c52884ff":"code","8d0a758f":"code","f6d5419e":"code","d7f82b18":"code","66ef9b33":"code","8d443252":"code","52322e41":"code","eb680b8f":"code","5dab331a":"code","e4ca1bc0":"code","1e19a7dd":"code","cb459b2f":"code","a0e4cada":"code","84a6c78d":"code","930ea14e":"code","20778410":"code","58ed5d77":"code","142d0e85":"code","039db669":"code","648fcd64":"code","4f1d2409":"code","7a55293f":"code","edc06109":"code","59c1e20b":"code","95dca022":"code","07f148f8":"code","8802b8eb":"markdown","96f4346c":"markdown","287ad64b":"markdown","225bcd72":"markdown","f4016975":"markdown","c8c3cf0f":"markdown","0077eeba":"markdown","5823be87":"markdown","b53d1e30":"markdown","8a2085fa":"markdown","ccb65319":"markdown","c0ecd16f":"markdown","b0961eef":"markdown","508347d8":"markdown","e5b4f50c":"markdown","eb88aade":"markdown","5b89fc29":"markdown","044fbda1":"markdown","4994af4f":"markdown","f87a38d0":"markdown","4cda98a7":"markdown","53e36c0f":"markdown","805746fc":"markdown","0205dfa5":"markdown","03aa719c":"markdown","0679673b":"markdown","83208c5c":"markdown","8b2e1ff0":"markdown","0c3f9628":"markdown"},"source":{"10e3d4aa":"import re\nimport string\nimport numpy as np \nimport random\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n","30fb515a":"df=pd.read_csv(\"\/kaggle\/input\/quora-duplicate-qns\/quora.csv\")\ndf.drop(\"Unnamed: 0\",axis=1,inplace=True)\ndf.drop(\"index\",axis=1,inplace=True)\ndf.drop(\"qid1\",axis=1,inplace=True)\ndf.drop(\"qid2\",axis=1,inplace=True)\n\n\n","d283af88":"df.head()","57bef314":"df.shape","7f6794d8":"df.info()","0f2872ba":"df.isna().sum()","a1feba2b":"df.duplicated().sum()","4eaa7c19":"fig=plt.figure(figsize=(10,6))\nax=sns.countplot(df[\"is_duplicate\"])\nfig.text(0.1,1,'Number of Duplicate Questions  ', {'font':'serif', 'size':20, 'weight':'bold'})\nfig.text(0.06,0.9,'As we can see most of the questions are not duplicated  ', {'font':'serif', 'size':12, 'weight':'bold'})\nfig.show()\n","c4031b24":"count=df[\"is_duplicate\"].value_counts().values\ncount\n\nfig = go.Figure()\nfig.add_trace(go.Bar(\n    x=['not_duplicates'],\n    y=[count[0]],\n    name='not_duplicates',\n    text=[count[0]],\n    textposition='auto',))\nfig.add_trace(go.Bar(x=['duplicates'],y=[count[1]],\n                    name='duplicates',text=[count[1]],textposition='auto'))\nfig.update_layout(\n    title='<span style=\"font-size:32px; font-family:Times New Roman\">Number of Duplicates and not Duplicates<\/span>')\n\nfig.show()","e10dc4b5":"df[\"size_question1\"]=df[\"question1\"].apply(lambda x:len(x.split(' ')))\ndf[\"size_question2\"]=df[\"question2\"].apply(lambda x:len(x.split(' ')))","df9a184e":"not_dup=df[df[\"is_duplicate\"]==0][\"size_question1\"].value_counts().sort_index()\ndup=df[df[\"is_duplicate\"]==1][\"size_question1\"].value_counts().sort_index()\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=not_dup.index,\n    y=not_dup.values,\n    name='not_dup',\n    fill='tozeroy',\n    \n))\nfig.add_trace(go.Scatter(\n    x=dup.index,\n    y=dup.values,\n    name='dup',\n    fill='tozeroy',\n    \n))\nfig.update_layout(\n    title='<span style=\"font-size:32px; font-family:Times New Roman\">Distribution of words of duplicate and not duplicate on question 1<\/span>'\n)\nfig.update_xaxes(range=[0, 70])\nfig.show()","70c01564":"not_dup=df[df[\"is_duplicate\"]==0][\"size_question2\"].value_counts().sort_index()\ndup=df[df[\"is_duplicate\"]==1][\"size_question2\"].value_counts().sort_index()\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=not_dup.index,\n    y=not_dup.values,\n    name='not_dup',\n    fill='tozeroy',\n    \n))\nfig.add_trace(go.Scatter(\n    x=dup.index,\n    y=dup.values,\n    name='dup',\n    fill='tozeroy',\n    \n))\nfig.update_layout(\n    title='<span style=\"font-size:32px; font-family:Times New Roman\">Distribution of words of duplicate and not duplicate on question 2<\/span>'\n)\nfig.update_xaxes(range=[0, 70])\nfig.show()","3f583efc":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","8bdb8922":"df['question1']=df['question1'].apply(clean_text)\ndf['question2']=df['question2'].apply(clean_text)\n\n","8c2d1de6":"df.head(3)","11bdbdea":"stop_words = stopwords.words('english')\nmore_stopwords = ['u', 'im', 'c']\nstop_words = stop_words + more_stopwords\n\ndef remove_stopwords(text):\n    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n    return text\n    \n\ndf['question1']=df['question1'].apply(remove_stopwords)\ndf['question2']=df['question2'].apply(remove_stopwords)\n\ndf.head()","af213e53":"stemmer = nltk.SnowballStemmer(\"english\")\n\ndef stemm_text(text):\n    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n    return text","d84e001a":"\ndf['question1']=df['question1'].apply(stemm_text)\ndf['question2']=df['question2'].apply(stemm_text)\ndf.head()","2d05575d":"from wordcloud import WordCloud, STOPWORDS\n\ntext = ' '.join(df['question1'])\n\nplt.rcParams['figure.figsize'] = (10,6)\nwordcloud = WordCloud(background_color = 'black',colormap='vlag', width = 1200,  height = 1200, max_words = 1000).generate(text)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","6cdaaefd":"from wordcloud import WordCloud, STOPWORDS\n\ntext = ' '.join(df['question2'])\n\nplt.rcParams['figure.figsize'] = (10,6)\nwordcloud = WordCloud(background_color = 'black',colormap='vlag', width = 1200,  height = 1200, max_words = 1000).generate(text)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","cc04aecc":"X = df['question1']+df['question2']\ny = df['is_duplicate']\n\nprint(len(X), len(y))","1a4e8b45":"# Split into train and test sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","b52d644a":"from sklearn.feature_extraction.text import CountVectorizer\n\n# instantiate the vectorizer\nvect = CountVectorizer()\nvect.fit(X_train)","b95716f2":"# learn training data vocabulary, then use it to create a document-term matrix\nX_train_dtm = vect.transform(X_train)","30eea0f7":"# equivalently: combine fit and transform into a single step\nX_train_dtm = vect.fit_transform(X_train)","c52884ff":"# examine the document-term matrix\nX_train_dtm","8d0a758f":"X_test_dtm = vect.transform(X_test)\nX_test_dtm","f6d5419e":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer()\ntfidf_transformer.fit(X_train_dtm)\ntfidf_transformer.transform(X_train_dtm)","d7f82b18":"# import an instantiate a logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(solver='liblinear')","66ef9b33":"# train the model using X_train_dtm\n%time logreg.fit(X_train_dtm, y_train)","8d443252":"# make class predictions for X_test_dtm\ny_pred_class = logreg.predict(X_test_dtm)","52322e41":"from sklearn import metrics","eb680b8f":"# calculate accuracy\nmetrics.accuracy_score(y_test, y_pred_class)","5dab331a":"metrics.confusion_matrix(y_test, y_pred_class)","e4ca1bc0":"vec=CountVectorizer(ngram_range=(2, 2))\nvec.fit(X_train)","1e19a7dd":"X_train_dtm = vect.transform(X_train)","cb459b2f":"X_train_dtm = vect.fit_transform(X_train)","a0e4cada":"X_train_dtm","84a6c78d":"print(X_train_dtm.toarray())","930ea14e":"X_test_dtm = vect.transform(X_test)\nX_test_dtm","20778410":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer()\ntfidf_transformer.fit(X_train_dtm)\ntfidf_transformer.transform(X_train_dtm)","58ed5d77":"# import an instantiate a logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(solver='liblinear')\n# train the model using X_train_dtm\n%time logreg.fit(X_train_dtm, y_train)\n# make class predictions for X_test_dtm\ny_pred_class = logreg.predict(X_test_dtm)\n# calculate accuracy\nlog_acc=metrics.accuracy_score(y_test, y_pred_class)\nlog_acc","142d0e85":"# import an instantiate a logistic regression model\nfrom sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier()\n# train the model using X_train_dtm\n%time RF.fit(X_train_dtm, y_train)\n# make class predictions for X_test_dtm\ny_pred_class = RF.predict(X_test_dtm)\n# calculate accuracy\nrf_acc=metrics.accuracy_score(y_test, y_pred_class)\nrf_acc","039db669":"\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\n# train the model using X_train_dtm\n%time knn.fit(X_train_dtm, y_train)\n# make class predictions for X_test_dtm\ny_pred_class = knn.predict(X_test_dtm)\n# calculate accuracy\nknn_acc=metrics.accuracy_score(y_test, y_pred_class)\nknn_acc","648fcd64":"# import an instantiate a logistic regression model\nfrom sklearn.svm import SVC\nSVC = SVC()\n# train the model using X_train_dtm\n%time SVC.fit(X_train_dtm, y_train)\n# make class predictions for X_test_dtm\ny_pred_class = SVC.predict(X_test_dtm)\n# calculate accuracy\nsv_acc=metrics.accuracy_score(y_test, y_pred_class)\nsv_acc","4f1d2409":"# import and instantiate a Multinomial Naive Bayes model\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()","7a55293f":"# train the model using X_train_dtm (timing it with an IPython \"magic command\")\n%time nb.fit(X_train_dtm, y_train)","edc06109":"# make class predictions for X_test_dtm\ny_pred_class = nb.predict(X_test_dtm)","59c1e20b":"# calculate accuracy of class predictions\nfrom sklearn import metrics\nnav_acc=metrics.accuracy_score(y_test, y_pred_class)\nnav_acc","95dca022":"# print the confusion matrix\nmetrics.confusion_matrix(y_test, y_pred_class)","07f148f8":"model_dataframe=pd.DataFrame({\"Alogrithm\":[\"KNN\",\"Logistic Regression\",\"Random Forest\",\"Support vector\",\"Navie Bayes\"],\"Accuracy\":[knn_acc,log_acc,rf_acc,sv_acc,nav_acc],})\nmodel_dataframe","8802b8eb":"<p id=\"part8\"><\/p>\n\n<span style=\"font-family: Arials; font-size: 18px; font-style: bold; font-weight: bold; letter-spacing: 2px; color: #DC143C\">5.3 STEMMING<\/span>","96f4346c":"<p id=\"part11\"><\/p>\n\n<span style=\"font-family: Arials; font-size: 18px; font-style: bold; font-weight: bold; letter-spacing: 2px; color: #DC143C\">6 MODELLING<\/span>","287ad64b":"<p id=\"part1\"><\/p>\n\n# <span style=\"font-family: Arials; font-size: 18px; font-style: bold; font-weight: bold; letter-spacing: 2px; color: #DC143C\">1. INTRODUCTION<\/span>\n\n\n<p style=\"font-family: Arials; font-size: 18px;font-weight: bold; color: 'Black\">Quora is a social question-and-answer website based in Mountain View, California, United States, and founded on June 25, 2009. The website was made available to the public on June 21, 2010. Users can collaborate by editing questions and commenting on answers that have been submitted by other users..<\/p>\n","225bcd72":"<p id=\"part4\"><\/p>\n\n# <span style=\"font-family: Arials; font-size: 18px; font-style: bold; font-weight: bold; letter-spacing: 2px; color: #DC143C\">4. VISUALIZATIONS<\/span>","f4016975":"<p id=\"part15\"><\/p>\n\n<span style=\"font-family: Arials; font-size: 18px; font-style: bold; font-weight: bold; letter-spacing: 2px; color: #DC143C\">6.3 SUPPORT VECTOR<\/span>","c8c3cf0f":"<p id=\"part6\"><\/p>\n\n# <span style=\"font-family: Arials; font-size: 18px; font-style: bold; font-weight: bold; letter-spacing: 2px; color: #DC143C\">5.1 CLEANING<\/span>","0077eeba":"<p style=\"font-family: Arials; font-size: 18px; text-align: center; color: Black;color: #DC143C\">Now let's see what is the problem statment.<\/p>\n\n<p style=\"font-family: Arials; font-size: 18px; text-align: center; color: Black\">Over 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.<\/p>","5823be87":"<p id=\"part14\"><\/p>\n\n<span style=\"font-family: Arials; font-size: 18px; font-style: bold; font-weight: bold; letter-spacing: 2px; color: #DC143C\">6.2 Nearest Neighbour<\/span>","b53d1e30":"# Checking the words :\nprint(vect.get_feature_names())","8a2085fa":"<p id=\"part2\"><\/p>\n\n# <span style=\"font-family: Arials; font-size: 18px; font-style: bold; font-weight: bold; letter-spacing: 2px; color: #DC143C\">2. LIBRARIES & IMPORT DATA SET<\/span>\n\n\n","ccb65319":"#### There are 1lakh rows and 4 columns","c0ecd16f":"## Target encoding:","b0961eef":"<p id=\"part3\"><\/p>\n\n# <span style=\"font-family: Arials; font-size: 18px; font-style: bold; font-weight: bold; letter-spacing: 2px; color: #DC143C\">3. DATA EXPLORATION<\/span>","508347d8":"### Let's make new columns : (Number of words in questions)","e5b4f50c":"<img src=\"https:\/\/miro.medium.com\/max\/602\/1*eNseZEXViToel_oC_vfSzQ.png\" width=\"700\" align=\"centr\"\/>\n\n<p style=\"font-family: Arials; line-height: 1,5; font-size: 12px; text-align: center; color: '#000000\"><a href=\"https:\/\/miro.medium.com\/max\/602\/1*eNseZEXViToel_oC_vfSzQ.png\" style=\"color:#DC143C\">Image source<\/a><\/p>\n<p style=\"font-family: Arials; line-height: 1.3; font-size: 26px; font-weight: bold; letter-spacing: 2px; text-align: center; color: #DC143C\">Quora Questions Duplicacy<\/p>","eb88aade":"<p id=\"part5\"><\/p>\n\n# <span style=\"font-family: Arials; font-size: 18px; font-style: bold; font-weight: bold; letter-spacing: 2px; color: #DC143C\">5. MAKE DATA SET READY FOR MODELLING<\/span>","5b89fc29":"<p id=\"part17\"><\/p>\n\n<span style=\"font-family: Arials; font-size: 18px; font-style: bold; font-weight: bold; letter-spacing: 2px; color: #DC143C\">7 COMPARISON<\/span>","044fbda1":"## cleaned data:","4994af4f":"#### There is no missing data","f87a38d0":"<p id=\"part13\"><\/p>\n\n<span style=\"font-family: Arials; font-size: 18px; font-style: bold; font-weight: bold; letter-spacing: 2px; color: #DC143C\">6.2 RANDOM FOREST<\/span>","4cda98a7":"## Word cloud","53e36c0f":"<p style=\"font-family: Arials; font-size: 18px; font-style: bold; font-weight: bold; letter-spacing: 2px; color: #DC143C; line-height:1.0\">TABLE OF CONTENTS<\/p>\n<hr style=\"height: 0.5px; border: 0; background-color: 'Black'\">\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: bold; letter-spacing: 2px; color: #DC143C; line-height:1.0\"><a href=\"#part1\" style=\"color:black\">1. INTRODUCTION.<\/a><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: bold; letter-spacing: 2px; color: #DC143C; line-height:1.0\"><a href=\"#part2\" style=\"color:black\">2. IMPORT LIBRARIES AND DATA SET.<\/a><\/p>\n\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: bold; letter-spacing: 2px; color: #DC143C; line-height:1.0\"><a href=\"#part3\" style=\"color:black\">3 DATA EXPLORATION AND CLEANING<\/a><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: bold; letter-spacing: 2px; color: #DC143C; line-height:1.0\"><a href=\"#part4\" style=\"color:black\">4 VISUALIZATION<\/a><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: bold; letter-spacing: 2px; color: #DC143C; line-height:1.0\"><a href=\"#part5\" style=\"color:black\">5. MAKE DATA SET READY FOR MODELLING<\/a><\/p>\n\n<p style=\"text-indent: 2.5vw; font-family: Arials; font-size: 15px; letter-spacing: 2px; color: #DC143C; line-height:1.3\">\n<a href=\"#part6\" style=\"color:black\">5.1 CLEANING<\/a><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: bold; letter-spacing: 2px; color: #DC143C; line-height:1.0\"><a href=\"#part5\" style=\"color:black\">6. MODELLING<\/a><\/p>\n\n\n","805746fc":"<p id=\"part16\"><\/p>\n\n<span style=\"font-family: Arials; font-size: 18px; font-style: bold; font-weight: bold; letter-spacing: 2px; color: #DC143C\">6.4 NAIVE BAYES<\/span>","0205dfa5":"<p id=\"part9\"><\/p>\n\n<span style=\"font-family: Arials; font-size: 18px; font-style: bold; font-weight: bold; letter-spacing: 2px; color: #DC143C\">5.4 WORD CLOUD<\/span>","03aa719c":"<p id=\"part10\"><\/p>\n\n<span style=\"font-family: Arials; font-size: 18px; font-style: bold; font-weight: bold; letter-spacing: 2px; color: #DC143C\">5.4 VECTORIZATION<\/span>","0679673b":"<p id=\"part7\"><\/p>\n\n<span style=\"font-family: Arials; font-size: 18px; font-style: bold; font-weight: bold; letter-spacing: 2px; color: #DC143C\">5.2 STOP WORDS<\/span>","83208c5c":"<p id=\"part12\"><\/p>\n\n<span style=\"font-family: Arials; font-size: 18px; font-style: bold; font-weight: bold; letter-spacing: 2px; color: #DC143C\">6.1 LOGISTIC REGRESSION<\/span>","8b2e1ff0":"<p style=\"font-family: Arials; font-size: 18px; text-align: center; color: Black\">Hello everyone! I would like to present you my new work.<\/p>\n\n<p style=\"font-family: Arials; font-size: 18px; text-align: center; color: Black\">In this notebook you will see from the basic how to handle the NLP data sets. All the things that i have done in this notebook will be very useful for fresher<\/p>\n<p style=\"font-family: Arials; font-size: 18px; text-align: center; color: Black\">I would be very grateful for any feedback on this work and your vote. Happy reading.<\/p>","0c3f9628":"#### There are no duplicate rows present in the data"}}