{"cell_type":{"cabcd606":"code","3db411a9":"code","464d9da0":"code","6a9a0b34":"code","1a46e9f7":"code","4d28ea37":"code","1af939ab":"code","0186a3b8":"code","1a91165a":"code","97aa7e52":"code","d39c1fc6":"code","87133a21":"code","993d44cd":"code","2941c37f":"code","cd133cef":"code","a06b680e":"code","52cfe384":"code","0100bd19":"code","d8607304":"code","f575b118":"code","0f6f0078":"code","e113364b":"code","71329581":"code","8fbffbdc":"code","089dcfe3":"code","39e5682c":"code","ea756497":"code","2f5be8dc":"code","638bb744":"code","9b60cd42":"code","0de711fa":"markdown","ceacfa5b":"markdown","0f5f042b":"markdown","cc8cc626":"markdown","a4a535e9":"markdown","a92cffdc":"markdown","bb75dda6":"markdown","95de391c":"markdown","05c21f1d":"markdown","a33c5edc":"markdown","22cb7f81":"markdown","7a7bd17c":"markdown","9c3c48b1":"markdown","4db76e36":"markdown","c8c1604a":"markdown","30b61661":"markdown","448ada4c":"markdown","d305c64a":"markdown"},"source":{"cabcd606":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nest_dis = pd.read_csv(\"..\/input\/passenger-list-for-the-estonia-ferry-disaster\/estonia-passenger-list.csv\") #est_dis indicates Estonia Disaster, however you can use **df** if it's confusing\nest_dis","3db411a9":"#Checking top 5 rows of our data\nest_dis.head()","464d9da0":"#Checking how many passenger survived(1) and non-survived(0)\nest_dis.Survived.value_counts()","6a9a0b34":"#Checking whether any missing data\nest_dis.isnull().sum()","1a46e9f7":"#let's find out survival percentage\nsurv_percnt = est_dis.Survived.value_counts()[1]\/len(est_dis)*100\nprint('Percentage of survived passengers: ' \"{:.2f}\".format(surv_percnt)+'%')","4d28ea37":"est_dis.Category.value_counts()","1af939ab":"#Now, let's check total number of male and females\nest_dis.Sex.value_counts()","0186a3b8":"pd.crosstab(est_dis.Sex, est_dis.Survived)","1a91165a":"survivedBySex = est_dis.groupby('Sex')['Survived'].mean()\nsurvivedBySex","97aa7e52":"#plotting Survivability sex wise\n%matplotlib inline\nplt.style.use('seaborn-whitegrid')\nfig , ax = plt.subplots(figsize=(10,6))\nax = survivedBySex.plot.bar()\nax.set(xlabel='Sex',\n      ylabel='Survived',\n      title='Survival rate by Sex');","d39c1fc6":"survivedByAge = est_dis.groupby('Age')['Survived'].mean()\nsurvivedByAge","87133a21":"#The above information wasn't so helpful but if we plot these data, it may make sense\nfig, ax = plt.subplots(figsize=(10,6))\nax = survivedByAge.plot.bar()\nax.set(xlabel='Age',\n      ylabel='Survived',\n      title='Survival rate Age wise');","993d44cd":"survivedByCategory = est_dis.groupby('Category')['Survived'].mean()\nsurvivedByCategory","2941c37f":"#Let's plot the above data\nfig, ax = plt.subplots(figsize=(10,6))\nax = survivedByCategory.plot.bar()\nax.set(xlabel='Category',\n      ylabel='Survived',\n      title='Survival Category wise');","cd133cef":"survivedByCountry = est_dis.groupby('Country')['Survived'].mean()\nsurvivedByCountry","a06b680e":"fig, ax = plt.subplots(figsize=(10,6))\nax = survivedByCountry.plot.bar()\nax.set(xlabel='Country',\n      ylabel='Survived',\n      title='Survival Country wise');","52cfe384":"#let's drop Firstname, lastname, PassengerId columns\nest_dis.drop(['PassengerId','Firstname','Lastname'], axis=1, inplace=True)","0100bd19":"est_dis.head()","d8607304":"est_dis.info()","f575b118":"#using labelencoder to convert all strings into integers in the dataframe\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor item in list(est_dis.columns):\n    if est_dis[item].dtype=='object':\n        est_dis[item]= le.fit_transform(est_dis[item])","0f6f0078":"est_dis.info()","e113364b":"#splitting data into X and y\nX = est_dis.drop('Survived', axis=1)\ny = est_dis['Survived']","71329581":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,\n                                                test_size=0.2)","8fbffbdc":"#importing all the models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier","089dcfe3":"#putting all the models in a dictionary\nmodels = {\"LogisticRegression\":LogisticRegression(),\n         \"KNeighboursClassifier\": KNeighborsClassifier(),\n         \"RandomForestClassifier\":RandomForestClassifier()}","39e5682c":"#Creating a function to fit our data in models and evaluate score\ndef fit_score(models,X_train,X_test,y_train,y_test):\n    np.random.seed(40) #so our results can be reproducable\n    evaluate = {} #this empty list will contain our evaluated score\n    for name, model in models.items():\n        model.fit(X_train,y_train) #fitting trained data in a model\n        evaluate[name]= model.score(X_test,y_test) #evaluate score on test data\n    return evaluate","ea756497":"evaluate= fit_score(models=models,\n                   X_train=X_train,\n                   X_test=X_test,\n                   y_train=y_train,\n                   y_test=y_test)\nevaluate","2f5be8dc":"#Different logistic Regression parameters\nparam_grid = {\"C\": np.logspace(-4,4,20),\n               \"solver\":[\"liblinear\"]}\nfrom sklearn.model_selection import GridSearchCV\nnp.random.seed(55)\ngrid_log_reg = GridSearchCV(LogisticRegression(),\n                           param_grid=param_grid,\n                           cv=5,\n                           verbose=True)\ngrid_log_reg.fit(X_train,y_train)","638bb744":"grid_log_reg.best_params_","9b60cd42":"grid_log_reg.score(X_test,y_test)","0de711fa":"**Checking survivability age-wise** ","ceacfa5b":"**as `LogisticRegression` gives slightly better result than other models, we will hypertune parameters of `Logistic Regression ` and try to improve our model**\n\nyou can also hypertune parameters of other models for better result but here i'm going with `Logistic Regression`","0f5f042b":"# Third, Fitting our data into a model\n**We have done enough EDA, let proceeeds forward to modelling**\n\nas it's a classification problem we will first evaluate score on KNN(K nearest neighbors), RandomForestClassifier and LogisticRegression.\n\nwhich one have better score, we will proceed with that model and hypertune parameters ","cc8cc626":"##### Now all columns converted to integers, let's split our data into train, test model","a4a535e9":"**Our data don't have any missing values**","a92cffdc":"# Predicting Estonia Disaster Survival using Machine Learning","bb75dda6":"**Checking Survivability by sex wise**","95de391c":"**P** stands for Passenger\n\n**C** stands for Crew Members","05c21f1d":"`Country` `Sex` `Survived` columns not in integers, we need to convert them to integers before moving forward","a33c5edc":"###  By exploring the data, we can say the problem we gonna explore is  **Binary Classification**\n\n**What is Binary Classification**\n\nBinary classification is to classify objects into two groups based on some features","22cb7f81":"**There is a slight improvement after hypertuning**","7a7bd17c":"## Visualize our data","9c3c48b1":"# Second,Some EDA\nEDA stands for Exploratory Data Analysis ","4db76e36":"**let's visualize data category wise**\n","c8c1604a":"**Now, let's find out the number of total passengers and crew members**\n","30b61661":"**Plotting survivability against country**","448ada4c":"### Fourth, Hypertuning parameters of `Logistic regression` and evaluate `accuracy` score","d305c64a":"## First, let's import the data and explore our problem definition"}}