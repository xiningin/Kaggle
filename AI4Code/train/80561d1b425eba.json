{"cell_type":{"02a955ad":"code","5dc1c616":"code","bd371607":"code","6ad0574a":"code","ae8d7f40":"code","39f7cf54":"code","55aa9acd":"code","45671c8d":"code","69cdc609":"code","19042fd5":"code","2c1bbd5d":"code","249471b9":"code","a8bb80dc":"code","6c9bfd13":"code","0a68cc8c":"code","1604f89a":"code","3d38b0c9":"code","04046184":"code","bb39a1fa":"code","a3bbb00a":"code","e2fe0ffd":"code","608b52ce":"code","b7dc2669":"code","27be4067":"code","c4c7ffb9":"code","4eb1a801":"code","6a5b7dee":"code","1d509eb6":"code","1453fc26":"code","5df28bce":"code","9c8d527a":"code","94581467":"code","0ae25a27":"code","7753414f":"code","458fc5e3":"code","0c5fabb7":"code","e60fa04b":"code","cb56db39":"code","92f9337a":"code","33ac8629":"code","7c90fd39":"markdown","3b0fca01":"markdown","60c5195f":"markdown","73693294":"markdown","5791c696":"markdown","ad02d03b":"markdown","e8f14f08":"markdown","21682409":"markdown","f7244a1f":"markdown","b499c3eb":"markdown","51f8c801":"markdown","b3a71e47":"markdown","6856a359":"markdown","606c11c1":"markdown","0d784b57":"markdown","a9547442":"markdown","1eb8f5cd":"markdown","1fcec7cd":"markdown","719a1a07":"markdown"},"source":{"02a955ad":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5dc1c616":"# load train and test data sets\ndfTrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\ndfTest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')","bd371607":"# print dfTrain and dfTest shape and info about dfTrain columns\nprint(\"Shape of train and test DataFrames:\", dfTrain.shape, dfTest.shape)\ndfTrain.info()","6ad0574a":"# print description of train data set numeric features\ndfTrain.describe()","ae8d7f40":"# lists with kind of the column data\nnumeric = list(dfTrain.select_dtypes(include=['int64', 'float64']).columns.values)\ninteger = list(dfTrain.select_dtypes(include=['int64']).columns.values)\nreal = list(dfTrain.select_dtypes(include=['float64']).columns.values)\nstring = list(dfTrain.select_dtypes(include=['object']).columns.values)\n\nprint(\"Numeric features:\", len(numeric))\nprint(\"Numeric integer features:\", len(integer))\nprint(\"Numeric real features:\", len(real))\nprint(\"String features:\", len(string))","39f7cf54":"# plot number of categories per string feature\nnu = dfTrain[string].nunique().reset_index()\nnu.columns = ['feature','nunique']\nplt.figure(figsize=(20,5))\nsns.barplot(x='feature', y='nunique', data=nu)\nsize = len(string)\nplt.xticks(np.linspace(0,size+1,size+2), dfTrain[string].columns.values, rotation=45, ha=\"right\")\nplt.xlim(-0.7,size-0.3)\nplt.title(\"Number of categories per feature\", fontsize=18)\nplt.ylabel(\"Number of categories\")\nplt.show()","55aa9acd":"# plot percentage of missing data in train data set\ntotal = dfTrain.isnull().sum().sort_values(ascending=False)\npercent = (dfTrain.isnull().sum()\/dfTrain.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total.head(20), percent.head(20)], axis=1, keys=['Total', 'Percent'])\nf, ax = plt.subplots(figsize=(15, 6))\nplt.xticks(rotation='45', ha=\"right\")\nsns.barplot(x=missing_data.index, y=missing_data.Percent)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)\nplt.show()\nmissing_data.head(10)","45671c8d":"# plot heatmap with correlation between numeric features\ndfNum = dfTrain.select_dtypes(include=['int64', 'float64'])\nplt.figure(figsize=(20,20))\nsns.heatmap(data=dfNum.corr(), vmin=-1, vmax=1, cmap='bwr', square=True)\nplt.xticks(rotation='45', ha=\"right\")\nplt.show()","69cdc609":"# print correlation with SalePrice\ndfNum.corr()['SalePrice'].sort_values()","19042fd5":"# correlation matrix containing only numeric features with higher correlation with SalePrice\nmain_features = ['SalePrice', 'Fireplaces', 'MasVnrArea', 'YearRemodAdd', 'YearBuilt', 'TotRmsAbvGrd', \n              'FullBath', '1stFlrSF', 'TotalBsmtSF', 'GarageArea', 'GarageCars', 'GrLivArea', 'OverallQual', \n              'OpenPorchSF', '2ndFlrSF', 'WoodDeckSF', 'BsmtFinSF1']\ntarget = ['SalePrice']\nsize = dfNum[main_features].shape[1]\nplt.figure(figsize=(9,9))\nsns.heatmap(data=dfNum[main_features].corr(), vmin=-1, vmax=1, cmap='bwr', annot=True, fmt = \".1f\")\nplt.xticks(rotation='45', ha=\"right\")\nplt.title(\"Correlation Matrix with the most correlated with SalePrice features\", fontsize=18)\nplt.show()","2c1bbd5d":"# plot of OverallQual x SalePrice\nsns.scatterplot(x='OverallQual', y='SalePrice', data=dfTrain)\nplt.title(\"Relation between OverallQual and SalePrice\", fontsize=15)\nplt.show()","249471b9":"# plot of GrLivArea x SalePrice\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=dfTrain)\nplt.title(\"Relation between GrLivArea and SalePrice\", fontsize=15)\nplt.show()","a8bb80dc":"# plot of GarageCars x SalePrice\nsns.scatterplot(x='GarageCars', y='SalePrice', data=dfTrain)\nplt.title(\"Relation between GarageCars and SalePrice\", fontsize=15)\nplt.show()","6c9bfd13":"# plot of YearBuilt x SalePrice\nsns.scatterplot(x='TotalBsmtSF', y='SalePrice', data=dfTrain)\nplt.title(\"Relation between TotalBsmtSF and SalePrice\", fontsize=15)\nplt.show()","0a68cc8c":"# plot of YearBuilt x SalePrice\nsns.scatterplot(x='YearBuilt', y='SalePrice', data=dfTrain)\nplt.title(\"Relation between YearBuilt and SalePrice\", fontsize=15)\nplt.show()","1604f89a":"# plot SalePrice distribution\nfrom scipy import stats\n\nsns.distplot(dfTrain['SalePrice'] , fit=stats.norm);\n\n\n# Get the fitted parameters used by the function\n(mu, sigma) = stats.norm.fit(dfTrain['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(dfTrain['SalePrice'], plot=plt)\nplt.show()\n","3d38b0c9":"# explore presence of outliers\nplt.figure(figsize=(25,5))\ndfNorm = (dfNum - dfNum.mean()) \/ (dfNum.max() - dfNum.min())\n\nsns.boxplot(data = dfNorm)\nsize = dfNorm.shape[1]\nplt.xticks(np.linspace(0,size+1,size+2), dfNorm.columns.values, rotation=45, ha=\"right\")\nplt.xlim(-0.7,size-0.3)\nplt.show()","04046184":"# fill the five columns with most missing data with 'None'\nfor df in [dfTrain, dfTest]:\n    df[\"PoolQC\"] = df[\"PoolQC\"].fillna(\"None\")\n    df[\"MiscFeature\"] = df[\"MiscFeature\"].fillna(\"MiscFeature\")\n    df[\"Alley\"] = df[\"Alley\"].fillna(\"None\")\n    df[\"Fence\"] = df[\"Fence\"].fillna(\"None\")\n    df[\"FireplaceQu\"] = df[\"FireplaceQu\"].fillna(\"None\")","bb39a1fa":"# fill numeric missing values by median and string missing values with mode\nnumeric_new = numeric.copy()\nnumeric_new.remove('SalePrice')\nfor df in [dfTrain, dfTest]:\n    df.loc[:,numeric_new] = df.loc[:,numeric_new].fillna(df.loc[:,numeric_new].median())\n    df.loc[:,string] = df.loc[:,string].fillna(df.loc[:,string].mode().to_dict('records')[0])","a3bbb00a":"# create total area joining data from basement 1st floor and 2nd floor\nfor df in [dfTrain, dfTest]:\n    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']","e2fe0ffd":"# apply boxcox1p in the most skewed features\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\n\nfor df in [dfTrain, dfTest]:\n    skewed_feats = df.loc[:,numeric_new].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n    skewness = pd.DataFrame({'Skew' :skewed_feats})\n    skewness = skewness[abs(skewness) > 0.75]\n    skewed_features = skewness.index\n    lam = 0.15\n    for feat in skewed_features:\n        df.loc[:,feat] = boxcox1p(df.loc[:,feat], lam)","608b52ce":"dfTrain.describe()","b7dc2669":"print(dfTrain.info())\nprint(dfTest.info())","27be4067":"# prepare data\nfrom sklearn.model_selection import cross_validate\nfrom sklearn import metrics\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n\ntarget = ['SalePrice']\nX = pd.get_dummies(dfTrain, columns=string, drop_first=True).drop(columns=target)\ny = dfTrain[target]","c4c7ffb9":"# Linear Regression\nfrom sklearn.linear_model import LinearRegression\n\nlr = TransformedTargetRegressor(LinearRegression(), func=np.log1p, inverse_func=np.expm1)\nscores = cross_validate(lr, X, y, cv=5, scoring=['r2', 'neg_mean_squared_error', 'neg_mean_squared_log_error'])\nr2_lr = round(scores['test_r2'].mean(), 3)\nrmse_lr = round(np.sqrt(-scores['test_neg_mean_squared_error'].mean()), 2)\nrmsle_lr = round(np.sqrt(-scores['test_neg_mean_squared_log_error'].mean()), 4)\nprint('R2:', r2_lr)\nprint('Root Mean Squared Error:', rmse_lr)\nprint('Root Mean Squared Log Error:', rmsle_lr)","4eb1a801":"# Lasso Regression\nfrom sklearn.linear_model import Lasso\n\nlasso = make_pipeline(RobustScaler(), TransformedTargetRegressor(Lasso(alpha =0.0005),\n                                                 func=np.log1p, inverse_func=np.expm1))\nscores = cross_validate(lasso, X, y, cv=5, scoring=['r2', 'neg_mean_squared_error', 'neg_mean_squared_log_error'])\nr2_lasso = round(scores['test_r2'].mean() , 3)\nrmse_lasso = round(np.sqrt(-scores['test_neg_mean_squared_error'].mean()), 2)\nrmsle_lasso = round(np.sqrt(-scores['test_neg_mean_squared_log_error'].mean()), 4)\nprint('R2:', r2_lasso)\nprint('Root Mean Squared Error:', rmse_lasso)\nprint('Root Mean Squared Log Error:', rmsle_lasso)","6a5b7dee":"# ElasticNet Regression\nfrom sklearn.linear_model import ElasticNet\n\nelNet = make_pipeline(RobustScaler(), TransformedTargetRegressor(ElasticNet(alpha=0.0005, l1_ratio=.9),\n                                                 func=np.log1p, inverse_func=np.expm1))\nscores = cross_validate(elNet, X, y, cv=5, scoring=['r2', 'neg_mean_squared_error', 'neg_mean_squared_log_error'])\nr2_elNet = round(scores['test_r2'].mean() , 3)\nrmse_elNet = round(np.sqrt(-scores['test_neg_mean_squared_error'].mean()), 2)\nrmsle_elNet = round(np.sqrt(-scores['test_neg_mean_squared_log_error'].mean()), 4)\nprint('R2:', r2_elNet)\nprint('Root Mean Squared Error:', rmse_elNet)\nprint('Root Mean Squared Log Error:', rmsle_elNet)","1d509eb6":"# KNN Regression\nfrom sklearn.neighbors import KNeighborsRegressor\n\nknn = TransformedTargetRegressor(KNeighborsRegressor(n_neighbors=15), func=np.log1p, inverse_func=np.expm1)\nscores = cross_validate(knn, X, y, cv=5, scoring=['r2', 'neg_mean_squared_error', 'neg_mean_squared_log_error'])\nr2_knn = round(scores['test_r2'].mean() , 3)\nrmse_knn = round(np.sqrt(-scores['test_neg_mean_squared_error'].mean()), 2)\nrmsle_knn = round(np.sqrt(-scores['test_neg_mean_squared_log_error'].mean()), 4)\nprint('R2:', r2_knn)\nprint('Root Mean Squared Error:', rmse_knn)\nprint('Root Mean Squared Log Error:', rmsle_knn)","1453fc26":"# KernelRidge Regressor\nfrom sklearn.kernel_ridge import KernelRidge\n\nkrr = make_pipeline(RobustScaler(), \n                    TransformedTargetRegressor(KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5),\n                                  func=np.log1p, inverse_func=np.expm1))\nscores = cross_validate(krr, X, y, cv=5, scoring=['r2', 'neg_mean_squared_error', 'neg_mean_squared_log_error'])\nr2_krr = round(scores['test_r2'].mean() , 3)\nrmse_krr = round(np.sqrt(-scores['test_neg_mean_squared_error'].mean()), 2)\nrmsle_krr = round(np.sqrt(-scores['test_neg_mean_squared_log_error'].mean()), 4)\nprint('R2:', r2_krr)\nprint('Root Mean Squared Error:', rmse_krr)\nprint('Root Mean Squared Log Error:', rmsle_krr)","5df28bce":"# Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = TransformedTargetRegressor(RandomForestRegressor(n_estimators=400, max_depth=8),\n                                func=np.log1p, inverse_func=np.expm1)\nscores = cross_validate(rf, X, y, cv=5, scoring=['r2', 'neg_mean_squared_error', 'neg_mean_squared_log_error'])\nr2_rf = round(scores['test_r2'].mean() , 3)\nrmse_rf = round(np.sqrt(-scores['test_neg_mean_squared_error'].mean()), 4)\nrmsle_rf = round(np.sqrt(-scores['test_neg_mean_squared_log_error'].mean()), 4)\n\nprint('R2:', r2_rf)\nprint('Root Mean Squared Error:', rmse_rf)\nprint('Root Mean Squared Log Error:', rmsle_rf)","9c8d527a":"# Gradient Boosting Regressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngBoost = TransformedTargetRegressor(GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                                              max_depth=4, max_features='sqrt', min_samples_leaf=15, \n                                                              min_samples_split=10, loss='huber'),\n                                    func=np.log1p, inverse_func=np.expm1)\n\nscores = cross_validate(gBoost, X, y, cv=5, scoring=['r2', 'neg_mean_squared_error', 'neg_mean_squared_log_error'])\nr2_gBoost = round(scores['test_r2'].mean() , 3)\nrmse_gBoost = round(np.sqrt(-scores['test_neg_mean_squared_error'].mean()), 2)\nrmsle_gBoost = round(np.sqrt(-scores['test_neg_mean_squared_log_error'].mean()), 4)\nprint('R2:', r2_gBoost)\nprint('Root Mean Squared Error:', rmse_gBoost)\nprint('Root Mean Squared Log Error:', rmsle_gBoost)","94581467":"# Comparison of Models\nmodels = pd.DataFrame({\n    'Root Mean Squared Error': [rmse_lr, rmse_lasso, rmse_elNet, rmse_krr, rmse_gBoost, rmse_rf, rmse_knn],\n    'R-squared': [r2_lr, r2_lasso, r2_elNet, r2_krr, r2_gBoost, r2_rf, r2_knn],\n    'Root Mean Squared Log Error':\n        [rmsle_lr, rmsle_lasso, rmsle_elNet, rmsle_krr, rmsle_gBoost, rmsle_rf, rmsle_knn]},\n    index=['Linear Regression', 'Lasso', 'Elastic Net', 'Kernel Ridge', 'Gradient Boosting Regressor', \n              'Random Forest', 'KNN'])\nmodels = models.sort_values(by='Root Mean Squared Log Error')\n\nmodels","0ae25a27":"gBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                                              max_depth=4, max_features='sqrt', min_samples_leaf=15, \n                                                              min_samples_split=10, loss='huber')\n\ngBoost.fit(X, y)\ndfFit = pd.DataFrame(gBoost.feature_importances_, X.columns, \n                     columns=['Coefficient'])\nto_keep = dfFit[dfFit.Coefficient > 0.00005].index.values\n\nprint(dfFit.sort_values(by='Coefficient', ascending=False).head(15))\nprint(\"\\n\\nWill be maintained:\\n\", to_keep)","7753414f":"# fit model with train_test_split and verify model in dev data set \nfrom sklearn.model_selection import train_test_split\n\nX_train, X_dev, y_train, y_dev = train_test_split(X[to_keep], y, random_state=123, test_size=0.2)\ngBoost = TransformedTargetRegressor(GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                                              max_depth=4, max_features='sqrt', min_samples_leaf=15, \n                                                              min_samples_split=10, loss='huber'),\n                                    func=np.log1p, inverse_func=np.expm1)\n\ngBoost.fit(X_train, y_train)\ny_pred = gBoost.predict(X_dev)","458fc5e3":"# scatter plot of real and prediction values\nsns.scatterplot(x=y_dev.values.reshape(1,-1)[0], y=y_pred.reshape(1,-1)[0], sizes=4, alpha=0.6)\nplt.plot(np.linspace(0,6.5e5), np.linspace(0,6.5e5), linewidth=1.5, c='black')\nplt.ylabel('Prediction')\nplt.xlabel('Real Value')\nplt.title('Comparison of Real Value and Prediction', fontsize=16)\nplt.show()","0c5fabb7":"# quantile-quantile with y_dev and y_pred\npercs = np.linspace(0,100,21)\nqn_dev = np.percentile(y_dev, percs)\nqn_pred = np.percentile(y_pred, percs)\n\nplt.plot(qn_dev, qn_pred, ls=\"\", marker=\"o\", alpha=0.7)\n\nx = np.linspace(np.min((qn_dev.min(),qn_pred.min())), np.max((qn_dev.max(),qn_pred.max())))\nplt.plot(x,x, color=\"k\", ls=\"--\")\nplt.title(\"QQ of Real Value and Prediction\", fontsize=16)\n\nplt.show()","e60fa04b":"# plot residual\nfig, axes= plt.subplots(nrows=1, ncols=2,figsize=(15,4))\n\nplt.subplot(1,2,1)\nplt.scatter(y_dev, (y_dev-y_pred)\/y_dev, s=2)\nplt.axhline(0, c='black')\nplt.ylim([-0.6, 0.6])\nplt.xlabel('Real Value')\nplt.ylabel('Normalized Residuals')\nplt.title(\"Normalized Residuals per Real Value\", fontsize=16)\n\nplt.subplot(122)\nplt.scatter(y_pred, (y_dev-y_pred)\/y_pred, s=3, alpha=0.6)\nplt.axhline(0, c='black')\nplt.ylim([-0.4, 0.4])\nplt.xlabel('Predicted Value')\nplt.ylabel('Normalized Residuals')\nplt.title(\"Normalized Residuals per Predicted Value\", fontsize=16)\n\nplt.subplots_adjust(wspace=0.3)\nplt.show()","cb56db39":"sns.distplot(((y_dev-y_pred)\/y_dev).values.reshape(1,-1)[0], bins=20, kde = False)\nplt.xlim([-1,1])\nplt.xlabel('Normalized Residuals')\nplt.ylabel('Frequency')\nplt.title('Histogram of Normalized Residuals', fontsize=16)\nplt.show()","92f9337a":"# plot mean squared error per price range (excluding houses with high SalePrice, which error is much bigger)\ndfValidation = pd.DataFrame((y_dev-y_pred)**2)\ndfValidation = dfValidation.rename(columns={'SalePrice': 'Normalized_Error'})\ndfValidation = pd.merge(dfValidation, dfTrain, right_index=True, \n                        left_index=True, how='left')\ndfValidation['SalePrice_category'] = dfValidation['SalePrice'] \/\/ 1e5\ndfValidation['SalePrice_category'] = dfValidation['SalePrice_category'].astype('category')\ndfGroupPrice = dfValidation.groupby('SalePrice_category').mean()\n\nplt.figure()\nsns.barplot(dfGroupPrice.index.values, dfGroupPrice.Normalized_Error)\nplt.xlabel('Price Range (1e5)')\nplt.ylabel('Mean Squared Error')\nplt.xlim([-0.6,3.6])\nplt.ylim([0, 1.5e9])\nplt.xticks([0,1,2,3])\nplt.title('Mean Squared Error per Price Range', fontsize=15)\nplt.show()","33ac8629":"gBoost = TransformedTargetRegressor(GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                                              max_depth=4, max_features='sqrt', min_samples_leaf=15, \n                                                              min_samples_split=10, loss='huber'),\n                                    func=np.log1p, inverse_func=np.expm1)\n\nX_test = pd.get_dummies(dfTest, columns=string, drop_first=True)\nto_keep_new = [el for el in to_keep if el in X_test.columns.values]\n\ngBoost.fit(X[to_keep_new], y)\npredictions = gBoost.predict(X_test[to_keep_new])\n\noutput = pd.DataFrame({'Id': dfTest.index.values, 'SalePrice': predictions.reshape(1,-1)[0]})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","7c90fd39":"In a first aproximation we fill all numeric features missing values by the median and all the string features by the mode.","3b0fca01":"### 3.2 Scatter Plots\n\nScatter plot of SalePrice with five of the most correlated features: OverallQual, GrLivArea, GarageCars, TotalBsmtSF \nand YearBuilt","60c5195f":"### 3.2 Outliers\n\nCheck the distribution shape of the dependent and independent variables.","73693294":"## 1. Import Libraries\n\nImport numpy, pandas and plotting libraries (matplotlib and seaborn).","5791c696":"From the initial exploration done above we got that we have 80 features, 43 are strings and 38 are numeric.\n\nString features are all categorical, with at most 24 categories. Numeric features are almost all real numbers, with only two of them being integers.\n\nWe also got that a lot of data is missing. Especially 5 columns have a missing rate higher than 20% ('PoolQC', 'MiscFeature', 'Alley', 'Fence' and 'FireplaceQu'). From the documentation the missing values seems to be when the feature referred do not exist (for exemple the house does not have pool or fire place).","ad02d03b":"From the figures we can see that it do not follow a normal distribution, wich is a desirable factor in a lot of models. Because of that we will use TransformedTargetRegressor and apply log(1+x) on this data when running the regression models.","e8f14f08":"Examine normalized distribution of values in the features. Columns with more dots means more values are beyond standard deviation and therefore have more outliers, or a more heavy tailed distribution.\n\nHave a high rate of outliers: LotArea, BsmtFinSF2, LowQualFinSF, BsmtHalfBath, KitchenAbvGr,\nEnclosedPorch, 3SsnPorch, ScreenPorch, PoolArea, MiscVal.\n\nTo deal with that in the cleaning stage we will apply boxcox1p in the most skewed distributions.","21682409":"## 2. Read and Explore Data\n\nLoad data from csv and make initial exploration of train and test DataFrames (check what data it has, what is the type of the data, percentage of missing values and initial description of the data. ","f7244a1f":"## 5. Fitting and comparing Models\n\nHere it is trained 7 models, all optimized and checked with cross validation score. We choose the model with best squared log error. The models trained here are:\n* Linear Regression\n* Lasso Regression\n* ElasticNet Regression\n* Kernel Ridge Regressor\n* Random Forest Regressor\n* KNN or k-Nearest Neighbors Regressor\n* Gradient Boosting Regressor","b499c3eb":"As from the description the 5 columns with most missing data ('PoolQC', 'MiscFeature', 'Alley', 'Fence' and 'FireplaceQu') possibly mean that the house does not have the referred feature, we will fill them with 'None'.","51f8c801":"## 7. Creating Submission File\n\nCreate and save submission file. The file is saved in the data folder with the name 'house-prices_submission.csv'","b3a71e47":"## 4. Clean and arrange data\n\nCreate new features, fill missing values and fix distribution skew.","6856a359":"## 3. Data Analysis and Visualization\n\nMake correlation analysis and visual plot of the relation between most important features and the target variable.\n\nCheck the presence of outliers.","606c11c1":"## 6. Validating Model\n\nVerify quality of the trained model (Gradient Boosting Regressor).","0d784b57":"As Gradient Boosting Regressor was one of the best models (Root Mean Squared Log Error=0.12) it will be utilized in submission file.\n\nBellow we see the importance of each feature in Gradient Boosting Regressor, we will maintain only features with \nimportance greater than 0.0001:","a9547442":"Checking the final test and train data frame","1eb8f5cd":"# House Prices: Advanced Regression Techniques\n\nThis notebook contain a solution to [kaggle House Prices: Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/) of predicting house prices based on 80 features. Train data set has 1460 houses and test data set has 1459 houses.\n\n### Contents:\n1. Import Libraries\n2. Read and Explore Data\n3. Data Analysis and Visualization\n4. Clean and arrange data\n5. Fitting and comparing Models\n6. Validatin Model\n7. Creating Submission File","1fcec7cd":"### 3.1 Correlation","719a1a07":"From the figures above we can see that the model have a good fit, with small error and residual is reasonably well distributed.\n\nWe can see that some high SalePrice were not predicted corretly (value is smaller than real) and this greatly increase residual. Dealing with this could improve the model.\n\nWe also see that the model performs best for smaller values, as there are mode data on this category."}}