{"cell_type":{"af35755e":"code","0ce5e9b2":"code","bba7aa86":"code","0201221e":"code","f2de1cca":"code","5537e004":"code","4494d258":"code","8380d027":"code","d42aaf9f":"code","ee8430eb":"code","c2b6b0c1":"code","e735fe3a":"code","8b197309":"code","af23ea93":"code","8ef745b4":"code","1973add9":"code","09e45b9c":"code","de67ec65":"code","d1391702":"code","ca8620fe":"code","99bbee51":"code","8ebf7025":"code","a57fea07":"code","51219c60":"code","d249d938":"code","65d5178c":"code","1f17466b":"code","ebfded20":"code","f9de2e7d":"code","85c1d8d8":"code","ae8126b0":"code","55a0ca26":"code","1e772882":"code","0e74d700":"code","3513b06b":"code","bc6b397a":"code","03489e38":"code","54edc657":"code","cccb8f93":"code","bdac3d31":"code","afd02404":"code","52bf11fa":"code","244aebf2":"code","e5dedf62":"code","cf906a50":"code","fff9b0aa":"code","076b4638":"code","c9a63c65":"code","9fcb51dd":"code","cd35fdc8":"code","6935e3ae":"code","ba42c4e2":"code","2ce8bf50":"code","e91c5ad1":"code","d7351623":"code","d29950ef":"code","5de06c51":"code","3b0b999c":"code","e94557fb":"code","80b732a1":"code","940d4770":"code","50e8790c":"code","5e3f02ca":"markdown","0657b088":"markdown","dcc098a2":"markdown","1c8a7e6b":"markdown","bd97fcdc":"markdown","4306da5f":"markdown","2eb75fea":"markdown","047c3573":"markdown","e267dd1d":"markdown","7b0e2a35":"markdown","313585b3":"markdown"},"source":{"af35755e":"## Importing the libraries...\n\nimport pandas as pd, numpy as np \nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","0ce5e9b2":"## Read train data\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n\n## Read test data\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","bba7aa86":"## Preview of train data\ntrain.head()","0201221e":"## Preview of text data\ntest.head()","f2de1cca":"## for train data\ntrain.isnull().sum()","5537e004":"##looking at the % of NaN values in train data\n\nprint('% of nan values in age in train is',(train['Age'].isnull().sum()\/train.shape[0])*100)\nprint('--------------------------------------------------------------------------------------')\nprint('% of nan values in Cabin in train is',(train['Cabin'].isnull().sum()\/train.shape[0])*100)\nprint('--------------------------------------------------------------------------------------')\nprint('% of nan values in Embarked in train is',(train['Embarked'].isnull().sum()\/train.shape[0])*100)","4494d258":"## for test data\ntest.isnull().sum()","8380d027":"##looking at the % of NaN values in test data\n\nprint('% of nan values in age in test is',(test['Age'].isnull().sum()\/test.shape[0])*100)\nprint('--------------------------------------------------------------------------------------')\nprint('% of nan values in Fare in test is',(test['Fare'].isnull().sum()\/test.shape[0])*100)\nprint('--------------------------------------------------------------------------------------')\nprint('% of nan values in Cabin in test is',(test['Cabin'].isnull().sum()\/test.shape[0])*100)","d42aaf9f":"## for train dataset..\n## to fill the NaN values in the age column in train data first lets see the distribution of the age, \n## that will give us the idea on what value is to be used to fill the NaN values\nimport seaborn as sn\nsn.distplot(train['Age'],bins=16)","ee8430eb":"## as we can see the data is right skeweed so we can use median value of the age to fill teh NaN values...\ntrain['Age'].fillna(train['Age'].median(skipna=True),inplace=True)","c2b6b0c1":"## checking the count between the embarkments\nsn.countplot(train['Embarked'])","e735fe3a":"## for the NaN values in the Embarked column as its a very small value being 0.224% \n## we can full those NaN values using the most repeated value\ntrain['Embarked'].fillna(train['Embarked'].value_counts().idxmax(),inplace=True)","8b197309":"## as the NaN values in the column Cabin is more than 40% its better to drop that column..\ntrain.drop('Cabin',axis=1,inplace=True)","af23ea93":"##for test data\n## to fill the NaN values in the age column in train data first lets see the distribution of the age, \n## that will give us the idea on what value is to be used to fill the NaN values\nsn.distplot(test['Age'],bins=16)","8ef745b4":"## as we can see the data is right skeweed so we can use median value of the age to fill teh NaN values...\ntest['Age'].fillna(test['Age'].median(skipna=True),inplace=True)","1973add9":"## looking for the distribution for Fare column..\nsn.distplot(test['Fare'],bins=100)","09e45b9c":"## as we can see the data is right skeweed so we can use median value of the Fare to fill teh NaN values...\ntest['Fare'].fillna(test['Fare'].median(skipna=True),inplace=True)","de67ec65":"## as the NaN values in the column Cabin is more than 40% its better to drop that column..\ntest.drop('Cabin',axis=1,inplace=True)","d1391702":"## looking at the stats..\ntrain.describe()","ca8620fe":"## we will make use of Z scores to remove the outliers..\nfrom scipy import stats\nnumtrain = train._get_numeric_data()","99bbee51":"z = np.abs(stats.zscore(numtrain))\nprint(z)","8ebf7025":"## threshold = 3\nprint(np.where(z > 3))","a57fea07":"train = train[(z < 3).all(axis=1)]","51219c60":"## looking at all the columns in the train data\ntrain.columns","d249d938":"## using pie chart for better picture of survival rate..\ntrain['Survived'].value_counts().plot(kind='pie')","65d5178c":"## cross tab is a powerfull tool which helps us to understand the combo of columns deeper...\n## we are seeing the survival rate based on gender..\npd.crosstab(train['Sex'],train['Survived'],margins=True).style.background_gradient(cmap='PuBu')","1f17466b":"## looking at the countplot to better understanding..\nsn.countplot('Sex',hue='Survived',data=train)\nplt.show()","ebfded20":"## we can see that the female has high survival rate that male..","f9de2e7d":"## now lets see the survival rate based on Pclass..\nsn.countplot('Pclass',hue='Survived',data=train)\nplt.show()","85c1d8d8":"## using crosstab beased on gender, Pclass and Survived\npd.crosstab([train['Sex'],train['Survived']],train['Pclass'],margins=True).style.background_gradient(cmap='PuBu')","ae8126b0":"## we can see that the Pclass 3 has lot of deaths and least in Pclass 1....","55a0ca26":"## now lets see the survival rate based on Embarked..\nsn.countplot('Embarked',hue='Survived',data=train)\nplt.show()","1e772882":"## factor plot is also one of the coolset plots to use..\nsn.factorplot('Pclass','Survived',hue='Sex',data=train)\nplt.show()","0e74d700":"## we can see that female in Pclass 1 has the high chance of being alive than the women in Pclass 3..","3513b06b":"## when we go through the data discription we can see that column SibSp is # of siblings \/ spouses aboard the Titanic and\n## column Parch is # of parents \/ children aboard the Titanic so we can make use of them and make a column called 'family'....\ntrain['TravelAlone']=np.where((train[\"SibSp\"]+train[\"Parch\"])>0, 0, 1)\ntrain.drop('SibSp', axis=1, inplace=True)\ntrain.drop('Parch', axis=1, inplace=True)","bc6b397a":"## doing the same thing on test data..\ntest['TravelAlone']=np.where((test[\"SibSp\"]+test[\"Parch\"])>0, 0, 1)\ntest.drop('SibSp', axis=1, inplace=True)\ntest.drop('Parch', axis=1, inplace=True)","03489e38":"## dropping the unnecessary columns in train data...\ntrain.drop('PassengerId',axis=1,inplace=True)\ntrain.drop('Name',axis=1,inplace=True)\ntrain.drop('Ticket',axis=1,inplace=True)","54edc657":"## dropping the unnecessary columns in test data....\ntest.drop('Name',axis=1,inplace=True)\ntest.drop('Ticket',axis=1,inplace=True)\n## we might get a question here that why is the PassengerId column is not dropped from test data..\n## that is because we will need that for the submission of the result, so we are kiing that for now...","cccb8f93":"## checking the categorical columns..\ntrain.info()","bdac3d31":"## so we must convert those categorical columns into numbers so.. its time for....","afd02404":"## just a line of code will do one hot encoding for us..\nfinal_train = pd.get_dummies(train, columns=[\"Pclass\",\"Embarked\",\"Sex\"])\nfinal_train","52bf11fa":"## one hot encoding for test data..\nfinal_test = pd.get_dummies(test, columns=[\"Pclass\",\"Embarked\",\"Sex\"])\nfinal_test","244aebf2":"## setting the X and Y...\nX= final_train.drop('Survived',axis=1)\ny= final_train['Survived']","e5dedf62":"## importing the train_test_split from sklearn to split the data into train and test..\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20,random_state=999)","cf906a50":"## importing the model from sklearn..\nfrom sklearn.linear_model import LogisticRegression","fff9b0aa":"## initialization and fitting the model..\nlogmodel = LogisticRegression(solver = 'lbfgs')\nlogmodel.fit(X_train,y_train)","076b4638":"## checking the scores..\nlogmodel.score(X_train,y_train)","c9a63c65":"logmodel.score(X_test,y_test)","9fcb51dd":"## we dont have the problem of overfitting thats a good sign :)","cd35fdc8":"##predicting...\npredictions = logmodel.predict(X_test)\npredictions","6935e3ae":"## importing the classification_report from sklearn metrics\nfrom sklearn.metrics import classification_report\n## classification report..\nprint(classification_report(y_test,predictions))","ba42c4e2":"## our model has f1-score 75% which is not bad, but now with thw help of confusion matric lets see where our model is going wrong..","2ce8bf50":"## confusion matrix is the best things to see how my model is predicting \nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test,predictions)","e91c5ad1":"## confused from confusion_matrix..? Dont worry we will see one by one..\n## it says 89 members are alive and our model predicted they are alive its correct, this is TRUE POSITIVE.\n## 17 members are dead and our model predicted as they are alive, this is  FALSE POSITIVE, this is \"TYPE ONE\" error.\n## 13 members are alive and our model predictrd as dead, thats sad, this is FALSE NEGATIVE, this is \"TYPR TWO\" error this is more dangerous than type one error.\n## 45 members are dead and our model predicted as dead, this is TRUE NEGATIVE\n## classification report will make use of all these TRUE POSITIVE, FALSE POSITIVE etc...","d7351623":"## calculating the mean squared error\nfrom sklearn.metrics import mean_squared_error as mse\nmse(y_test,predictions)","d29950ef":"## closer the mse value to 0 better the model","5de06c51":"## clearing a duplicate of test data without the PassengerId, because we did not keep that in the train model for prediction..\n## if we add this column or feature here and not adding there will throw an error so lets drop it..\nfinal_test1 = final_test.drop('PassengerId',axis=1)","3b0b999c":"## predicting the values for test data\nfinal_test1['Survived'] = logmodel.predict(final_test1)","e94557fb":"## taking the PassengerId from final_test..\nPassengerId = final_test['PassengerId']","80b732a1":"## taking the Survived from final_test1..\nSurvived = final_test1['Survived'] ","940d4770":"## making it as a DataFrame using pandas\nSubmission = pd.DataFrame([PassengerId,Survived]).T","50e8790c":"## exporting it as csv file for the submission..\nSubmission.to_csv('Submission.csv',index=False)","5e3f02ca":"## PREDICTING FOR THE TEST DATA","0657b088":"## GETTING THE DATA TO FIT THE MODEL ","dcc098a2":"## CREATING THE SUBMISSION FILE","1c8a7e6b":"## FILLING THE NAN VALUES","bd97fcdc":"## CHECKING FOR THE NULL VALUES","4306da5f":"\n## INTRODUCTION\n\n* Hi,I recently started learning Data science and this is my first time in Kaggle and I have tried my best explain what I have done with my notebook, If you find it help full I will be very happy, and if you have any suggestions I will be happy to implement that..... ENJOY THE NOTEBOOK....  ","2eb75fea":"## THANK YOU SO MUCH..","047c3573":"## REMOVING THE OUTLIERS","e267dd1d":"## TIME TO FIT THE MODEL (LOGISTIC REGRESSION)","7b0e2a35":"## TIME FOR SOME EDA","313585b3":"## ONE HOT ENCODING"}}