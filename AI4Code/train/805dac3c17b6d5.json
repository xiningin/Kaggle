{"cell_type":{"00e6b0e3":"code","ee661140":"code","f80f54d1":"code","5df5e24d":"code","60f13e1a":"code","c9b996a9":"code","e7309535":"code","d5105507":"code","20d22d94":"code","b6683c72":"code","1b80af6d":"code","f13b6106":"code","26458c84":"code","f601d435":"code","82aff9a3":"code","e3c39ad7":"code","b8710d54":"code","6ba38e94":"code","c8ccd3fe":"markdown","32e42020":"markdown","00743c20":"markdown","461e3aac":"markdown","5326e222":"markdown","a1d9196a":"markdown","e1737b7e":"markdown","69e7e735":"markdown","db408f5f":"markdown","810feb2a":"markdown","6386b609":"markdown","898b2de0":"markdown","21383db6":"markdown"},"source":{"00e6b0e3":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.metrics import Accuracy, AUC, TruePositives, TrueNegatives, FalsePositives, FalseNegatives\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, SeparableConv2D, MaxPooling2D, Dense, BatchNormalization, Flatten, Activation, Dropout, Add","ee661140":"path = '..\/input\/chest-xray-pneumonia\/chest_xray'\ntrain_dir = f'{path}\/train'\nval_dir = f'{path}\/val'\ntest_dir = f'{path}\/test'","f80f54d1":"len_normal = len(os.listdir(f'{train_dir}\/NORMAL'))\nlen_pneumonia = len(os.listdir(f'{train_dir}\/PNEUMONIA'))\npercent_normal = len_normal \/ (len_normal + len_pneumonia)\npercent_pneumonia = len_pneumonia \/ (len_normal + len_pneumonia)\n\nx = [\"Normal\", \"Pneumonia\"]\ny = [len_normal, len_pneumonia]\nplt.title(\"Class Distributions\")\nplt.ylabel(\"Number of x-rays\")\nplt.bar(x, y)\n\nprint(f'Normal x-rays: {len_normal} ({percent_normal:.2}%) | Pneumonia x-rays: {len_pneumonia} ({percent_pneumonia:.2f}%)')","5df5e24d":"width = height = 64 # used for resizing the image later\nchannels = 1 # x-rays have 1 colour channel since they're not rgb\ninput_size = (width, height)\ninput_shape = (width, height, channels)\n\ncolour_mode = 'grayscale' # x-rays are grayscale\nclass_mode ='binary' # because there are only 2 classes\nseed = 42\n\nepochs = 50 \nlearning_rate = 0.001\nbatch_size = 32\nval_size = 0.05 # 95% for training & 5% for validation\nclass_names = os.listdir(train_dir)\n\nweight_normal = 1 \/ percent_normal\nweight_pneumonia = 1 \/ percent_pneumonia\nclass_weight = {0: weight_normal, 1: weight_pneumonia} #we create a inversely proportional ratio to negate the imbalanced classes\n","60f13e1a":"train_gen = ImageDataGenerator(\n    rescale = 1.\/255, #Normalize the pixels to be in a range (0-1)\n    rotation_range = 10,\n    shear_range = 0.2,\n    zoom_range = 0.2,\n    horizontal_flip = True,\n    width_shift_range = 0.2,\n    height_shift_range = 0.2,\n    validation_split = val_size # This is for splitting training and validation up\n)","c9b996a9":"train_flow = train_gen.flow_from_directory(\n    directory = train_dir, \n    target_size = input_size, # We pass the designated size we want to resize our images too so it's lighter on training\n    class_mode = class_mode,\n    batch_size = batch_size,\n    color_mode = colour_mode,\n    shuffle = True,\n    seed = seed,\n    subset = 'training' #Special key to let Keras know this subset gets (1 - val_size) samples\n)\n\nval_flow = train_gen.flow_from_directory(\n    directory = train_dir, # We're using the same directory as train_flow since we are splitting the directory\n    target_size = input_size,\n    class_mode = class_mode,\n    batch_size = batch_size,\n    color_mode = colour_mode,\n    shuffle = True,\n    seed = seed,\n    subset = 'validation'\n)","e7309535":"test_gen = ImageDataGenerator(rescale = 1.\/255) # Testing should not augment data since we'd like to see how it performs on new data\n\ntest_flow = test_gen.flow_from_directory(\n    directory = test_dir,\n    target_size = input_size,\n    class_mode = class_mode,\n    batch_size = batch_size,\n    color_mode = colour_mode,\n    seed = seed\n)","d5105507":"def show_batch(image_batch, label_batch):\n    plt.figure(figsize=input_size)  \n    for n in range(16):\n        ax = plt.subplot(4,4,n+1)\n        img = image_batch[n]\n        plt.imshow(np.squeeze(img), cmap='gray')\n        class_name = class_names[int(label_batch[n])]\n        plt.title(class_name, fontsize=50)\n        plt.axis('off')","20d22d94":"image_batch, label_batch = next(train_flow)\nshow_batch(image_batch, label_batch)","b6683c72":"image_batch, label_batch = next(test_flow)\nshow_batch(image_batch, label_batch)","1b80af6d":"xray_input = Input(shape = input_shape)\n\nx = SeparableConv2D(filters = 32, kernel_size = 3)(xray_input)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n# x = MaxPooling(2)(x) add pooling dude\n\nx = SeparableConv2D(filters = 32, kernel_size = 3)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\n#skip conenction \nb1_skip = x\nb1_skip = SeparableConv2D(filters = 64, kernel_size = 5)(b1_skip) #convolve to remerge later\nb1_skip = BatchNormalization()(b1_skip)\n\nx = SeparableConv2D(filters = 64, kernel_size = 3)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = SeparableConv2D(filters = 64, kernel_size = 3)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Add()([x, b1_skip])\n\nb2_skip = x\nb2_skip = SeparableConv2D(filters = 128, kernel_size = 5)(b2_skip)\nb2_skip = BatchNormalization()(b2_skip)\n\nx = SeparableConv2D(filters = 128, kernel_size = 3)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = SeparableConv2D(filters = 128, kernel_size = 3)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Add()([x, b2_skip])\n\nb3_skip = x\nb3_skip = SeparableConv2D(filters = 256, kernel_size = 5)(b3_skip)\nb3_skip = BatchNormalization()(b3_skip)\n\nx = SeparableConv2D(filters = 256, kernel_size = 3)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = SeparableConv2D(filters = 256, kernel_size = 3)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Add()([x, b3_skip])\n\nx = MaxPooling2D(pool_size=2)(x)\n\nx = Flatten()(x)\n\nx = Dense(units = 1024)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\nx = Dropout(0.1)(x)\n\nx = Dense(units = 1024)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\n\noutput = Dense(units = 1, activation = 'sigmoid')(x)\n\nmodel = Model(xray_input, output, name = 'xrayz')\n\nmodel.compile(\n    loss = BinaryCrossentropy(),\n    optimizer = Adam(learning_rate=learning_rate),\n    metrics = [\n        'accuracy',\n        AUC(name='auc'),\n        TruePositives(name='tp'), \n        FalsePositives(name='fp'), \n        TrueNegatives(name='tn'), \n        FalseNegatives(name='fn')\n    ]\n)\n\nmodel.summary()","f13b6106":"plot_model(model, f'{model.name}.png')","26458c84":"early_stop = EarlyStopping(patience=3)\n\ncallbacks = [early_stop]","f601d435":"history = model.fit(\n    train_flow,\n    epochs = epochs,\n    validation_data = val_flow,\n    callbacks = callbacks,\n    class_weight = class_weight\n)","82aff9a3":"print(history.history)","e3c39ad7":"tp = int(history.history['val_tp'][-1])\nfp = int(history.history['val_fp'][-1])\ntn = int(history.history['val_tn'][-1])\nfn = int(history.history['val_fn'][-1])\n\nacc = history.history['val_accuracy'][-1]\nauc = history.history['val_auc'][-1]\nloss = history.history['val_loss'][-1]\n\nsensitivity = tp \/ (tp + fn)\nspecificity = tn \/ (tn + fp)\nprecision = tp \/ (tp + fp)\nrecall =  tp \/ (tp + fn)\n#accuracy = acc (tp + tn) \/ (tp + fp + tn + fn)\n\nprint(f'Sensitivity: {sensitivity}')\nprint(f'Specificity: {specificity}')\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')\nprint(f'Accuracy: {acc}')\nprint(f'AUC: {auc}')\n\ndata = np.array([[tp, fp],\n                   [fn, tn]])\n\nlabels = [[f'True Positive\\n {tp}', f'False Positive\\n {fp}'], \n          [f'False Negative\\n {fn}', f'True Negative\\n {tn}']]\n\nheatmap = sns.heatmap(data, annot=labels, fmt='', cmap='Blues')\nheatmap.set_title('Confusion Matrix')\n\nheatmap.set_xticklabels(['1', '0'])\nheatmap.set_yticklabels(['1', '0'])\n\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.show()\n","b8710d54":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nax1.plot(history.history['loss'], color='b', label=\"Training loss\")\nax1.plot(history.history['val_loss'], color='r', label=\"Validation loss\")\nax1.legend(loc='best', shadow=True)\n\nax2.plot(history.history['tp'], color='b', label=\"Training True Positive\")\nax2.plot(history.history['val_tp'], color='r',label=\"Validation True Positive\")\nax2.legend(loc='best', shadow=True)\n\n#plt.tight_layout()\nplt.show()","6ba38e94":"score = model.evaluate(test_flow, batch_size=batch_size)\n\nprint('Test Loss:', score[0]) \nprint('Test Accuracy:', score[1])","c8ccd3fe":"# Data Augmentation\n\n* Data augmentation are techniques used to increase the amount of data by adding slightly modified copies of already existing data.\n\n* By modifying existing data we can create new samples with slight noise which can help reduce overfitting\/improve generalization. \n\n* We can use Keras' ImageDataGenerator function which allows us to apply a variety of image augmenting techniques.\n","32e42020":"# The Model\n\n* Residual Netorks\n\n* batch Norm\n\n\n*Seperable Conv\n\n* Here we will be using Keras Fuctional API","00743c20":"## Trainin, Validation, Test Sets\n\n* I Chose to split training and validation from the same directory since the validation directory did not contain enough samples to be significant.","461e3aac":"### When comparing the training images to test images, we can see that the training images have been shifted and rotated.","5326e222":"As you can see we have an imbalanced class distribution with approximately 3x more x-rays of patients with Pneumonia! ","a1d9196a":"# Imports","e1737b7e":"# Hyperparameters","69e7e735":"> # Paths to Data","db408f5f":"# Table of Contents\n* Imports\n* Paths to Data\n* Hyperparameters\n* feeding data without overflow\n* Data Augmentation\n* Dealing with an inbalanced dataset\n* Appropriate Metrics\n* Model\n* Depthwise Seperable Convolutions\n* Callbacks\n* Plotting\n","810feb2a":"# Plot Training & Validation","6386b609":"## Why SeperableConv2D rather then Conv2D?\n\nThe main difference as pointed it out in this [article](https:\/\/towardsdatascience.com\/a-basic-introduction-to-separable-convolutions-b99ec3102728) is that traditional Convolutions transform the image as many times as there are kernels while the Depthwise Seperable Convolution transforms the image once. This acheives the same result with much less multiplications. lets look at an example.\n\n### Traditional Convolution\n\n![img1](https:\/\/miro.medium.com\/max\/1400\/1*XloAmCh5bwE4j1G7yk5THw.png)\n\nWe have a 12x12x3 image and would like to apply 256 filters with a kernel size of 5x5x3. Assuming we are using valid padding we can move the kernel 8x8 times. In the traditional Convolution there are 256 5x5x3 kernels that move 8x8 times. That\u2019s 256x3x5x5x8x8=1,228,800 multiplications.\n\n### Seperable Convolution\n\nThe Seperable Convolution (also known as SeperableConv2D in Keras) splits these multiplications into two 'seperate' parts.\n\n#### 1. Depthwise Convolution\n![img2](https:\/\/miro.medium.com\/max\/1400\/1*yG6z6ESzsRW-9q5F_neOsg.png)\n\nIn the Depthwise Convolution, there is 3 5x5x1 kernels that move 8x8 times. That\u2019s 3x5x5x8x8 = 4,800 multiplications\n\n#### 2. Pointwise Convolution\n![img3](https:\/\/miro.medium.com\/max\/1400\/1*Q7a20gyuunpJzXGnWayUDQ.png)\n\nIn the Pointwise Convolution, we have 256 1x1x3 kernels that move 8x8 times. That\u2019s 256x1x1x3x8x8=49,152 multiplications. \n\n### Comparison\nWhen adding both parts up together, we get 53,952 multiplications. This is around 23x less multiplcations than the tradional convolution!\n\n\n\n","898b2de0":"# Future Work\n\n## blah","21383db6":"# Callbacks"}}