{"cell_type":{"a03bb09f":"code","fcadee20":"code","d2832020":"code","38e16f20":"code","0fac0cba":"code","faaa5976":"code","dd4c579a":"code","53b0ad97":"code","a2a6e090":"code","dc69e287":"code","dca54653":"code","6c7ebe42":"code","a3a1c456":"code","c1bdcbe4":"code","1ec04043":"code","72b9ea1b":"code","f5d20c85":"code","96ba359b":"code","00da756b":"code","b880155a":"code","5f97eeb7":"code","8de86f69":"code","cffa9e81":"code","d811e05c":"markdown","20770a54":"markdown","ef7d5ebc":"markdown","6be466a2":"markdown","07d3b499":"markdown","ff4ae022":"markdown","9d3c76d9":"markdown","002ff47a":"markdown","28e77f02":"markdown"},"source":{"a03bb09f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fcadee20":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom category_encoders.ordinal import OrdinalEncoder\nfrom sklearn.model_selection import cross_val_score","d2832020":"# Function to print data scores of model\ndef print_scores (model):\n    if str(type(model)) == \"<class 'sklearn.model_selection._search.GridSearchCV'>\":\n        roc = cross_val_score(model.best_estimator_, X_train, y_train, cv=7, scoring ='roc_auc')\n        recall = cross_val_score(model.best_estimator_, X_train, y_train, cv=7, scoring ='recall')\n        print (model.estimator)\n    else:\n        print(model)\n        roc = cross_val_score(model, X_train, y_train, cv=7, scoring ='roc_auc')\n        recall = cross_val_score(model, X_train, y_train, cv=7, scoring ='recall') \n    print (\"Scores on training data:\")\n    print (\"ROC AUC : {}\".format(roc.mean()))\n    print (\"Recall : {}\".format(recall.mean()))\n# Function to print test scores of model\nfrom sklearn import metrics\ndef print_final_scores (model):\n    if str(type(model)) == \"<class 'sklearn.model_selection._search.GridSearchCV'>\":\n        final_predictions = model.best_estimator_.predict(X_test)\n        print (model.estimator)\n    else: \n        final_predictions = model.predict(X_test)\n    roc = metrics.roc_auc_score(final_predictions, y_test)\n    recall = metrics.recall_score(final_predictions, y_test)\n    print (\"Scores on test data:\")\n    print (\"ROC AUC : {}\".format(roc))\n    print (\"Recall : {}\".format(recall))","38e16f20":"data = pd.read_csv('..\/input\/employee-future-prediction\/Employee.csv')","0fac0cba":"data.head()","faaa5976":"data.info()","dd4c579a":"sns.countplot(data = data, x = 'Education')","53b0ad97":"sns.countplot(data = data, x = 'City')","a2a6e090":"data.describe()","dc69e287":"data.hist(bins=50, figsize=(20,15))","dca54653":"corr_matrix = data.corr()\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.heatmap(data.corr(), cmap=sns.diverging_palette(240, 10, n=9), annot = True, ax = ax, center = 0)","6c7ebe42":"data['Gender'] = data['Gender'].replace('Female', 0)\ndata['Gender'] = data['Gender'].replace('Male', 1)","a3a1c456":"data['EverBenched'] = data['EverBenched'].replace('Yes', 1)\ndata['EverBenched'] = data['EverBenched'].replace('No', 0)","c1bdcbe4":"from sklearn.preprocessing import OneHotEncoder\ncat_encoder = OneHotEncoder()\ndata_city_1hot = pd.get_dummies(data['City'])","1ec04043":"data = data.drop('City',axis = 1)","72b9ea1b":"data = pd.concat([data, data_city_1hot], axis = 1)","f5d20c85":"data.head()","96ba359b":"ordinal_cols_mapping =[\n    { \n 'col':'Education',\n            'mapping':{ \n                'Bachelors' : 1,\n                'Masters' : 2,\n                'PHD' : 3},\n    }\n]\nencoder = OrdinalEncoder(mapping = ordinal_cols_mapping)\ndata = encoder.fit_transform(data)","00da756b":"# Separating target column and create train and test sets.\ncol_names = ['Education', 'JoiningYear', 'PaymentTier', 'Age', 'Gender',\n       'EverBenched', 'ExperienceInCurrentDomain', 'Bangalore',\n       'New Delhi', 'Pune']\nX = data[col_names]\ny = data['LeaveOrNot']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)","b880155a":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(random_state=42, max_depth = 5, max_leaf_nodes = 8)\ndt.fit(X_train, y_train)","5f97eeb7":"print_scores(dt)","8de86f69":"print_final_scores(dt)","cffa9e81":"import graphviz\nfrom sklearn import tree\ndot_data = tree.export_graphviz(dt, out_file = None, feature_names = X_train.columns, class_names =('Not Leave', 'Leave'), filled = True, rounded = True, special_characters = True)\ngraph = graphviz.Source(dot_data)\ngraph","d811e05c":"Encoding categorical data.","20770a54":"# Summary","ef7d5ebc":"# EDA","6be466a2":"# Decision Tree Classifier","07d3b499":"# Feature Engineering","ff4ae022":"After trying various models (KNNneighbours, RandomForest, XGBoost) I decided to use simple Decision Tree model for it's interpretibility while it still get high accuracy scores (only few percent lower than XGBoost). From visualisation of the model we can see that only 3 features are important which are Joining Year (recently  hired workers tend to leave), Payment'Tier (Higher PaymentTier tend to leave) and Education (Lower Educated Workers tend to leave). That simple decision tree can be used as an algorithm wchich can help HR easily (and accurately) classify workers.","9d3c76d9":"About Data:  <br>\nDummy Data Used For A Private Hackathon<br>\nColumns : <br>\nEducation - Education Level<br>\nJoining Year - Year of Joining Company<br>\nCity - City office where posted<br>\nPayment Tier - <br>\n    1 : Highest<br>\n    2 : Mid Level<br>\n    3 : Lowest<br>\nAge - Current Age<br>\nGender - Gender of Employee (Male, Female)<br>\nEverBenched - Ever kept out of projects for 1 month or more. (True, False)<br>\nExperienceInCurrentDomain - Experience in currend Field (years)<br>\nLeaveOrNot - Whether employee leaves the company in next 2 years. (0, 1)<br>","002ff47a":"# Employee Future Prediction - EDA, Classification Model, Analyse of feature importance.","28e77f02":"Goal : \nCompany's HR department wants to predict whether some customers would leave the company in next 2 years. <br>\nYour job is to build a predictive model that predicts the prospects of future and present employee. <br>\nPerform EDA and bring out insights<br>"}}