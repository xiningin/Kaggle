{"cell_type":{"0ffcb415":"code","1e830886":"code","19c805e1":"code","fa131e90":"code","4007c99d":"code","e373abf5":"code","ebc61ec3":"code","9811bf06":"code","ebcf576a":"code","110a89d8":"code","7a0a7f4a":"code","b7706dc2":"code","b04e1f06":"code","093d0047":"code","b30dc914":"code","5e1bbcd3":"code","c927e4f4":"code","375ac6e3":"code","e58aacff":"code","df574375":"code","93e71020":"code","7ae85ec3":"code","c007e506":"code","e684a701":"code","131bf57a":"code","6856e20f":"code","6811cb41":"code","2295a1cc":"code","93aa782b":"code","5734c515":"code","6e61e6ea":"code","edf9bd88":"code","96f92d5f":"code","5f30fd7f":"code","01555832":"code","a2b96134":"code","42cfccc8":"code","b742d3b4":"code","4c483a0c":"code","46595267":"code","71b42e8a":"code","836d6c98":"code","968bb8ef":"code","462b5377":"code","06365c37":"code","d04a7f41":"code","8c2b8a21":"code","5283ad17":"code","d715b11f":"code","0b26bd65":"code","523bccd1":"code","efd89c1e":"code","82e0f415":"code","6c743247":"code","b9e81cbd":"code","a6cb7cbf":"code","2a94a1ce":"code","2de4a125":"code","961b4ccd":"code","39b5172d":"code","5f021d89":"code","a27b25f5":"code","42cefdb2":"markdown","d2b72ab7":"markdown","bbf5b3b9":"markdown","1998676a":"markdown","1fed716b":"markdown","be906228":"markdown","116d2517":"markdown","4b0eb36c":"markdown","e08a8a3a":"markdown","4d1a7d1f":"markdown","e8bb0f2b":"markdown","91c657eb":"markdown","3f6b2513":"markdown","f8082808":"markdown","ccb716f4":"markdown","29ee6cf3":"markdown","8b1abe59":"markdown","853abbe1":"markdown","b05a7e43":"markdown","419582d2":"markdown","b63fd27d":"markdown","4b49d66a":"markdown","987f0854":"markdown","9842f95f":"markdown","c2d012ed":"markdown","32c3939e":"markdown","8d414e21":"markdown","bbd844a0":"markdown","04c762ca":"markdown","7b5d25c1":"markdown","a12553e5":"markdown","ed925f93":"markdown","c039591a":"markdown","952c255d":"markdown","d134ef76":"markdown","31519973":"markdown","27504ec7":"markdown","69bed997":"markdown","f5f33117":"markdown","2d28149b":"markdown","3e822976":"markdown"},"source":{"0ffcb415":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport missingno as msno\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport warnings\nwarnings.filterwarnings(\"ignore\",)  #Ignore certain system-wide alerts\n\nfrom time import time, strftime, gmtime\nstart = time()\nimport datetime\nprint(str(datetime.datetime.now()))\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1e830886":"data = pd.read_csv(\"..\/input\/broadband-customers-base-churn-analysis\/bbs_cust_base_scfy_20200210.csv\")\ndf = data.copy()\ndf\n#Using the read.csv code of the pandas library, \n#we read the csv file and assign that to variable called df.","19c805e1":"# The Unnamed:19 column has been permanently removed from the dataset.\ndf.drop(\"Unnamed: 19\",axis = 1, inplace = True)","fa131e90":"print(df.shape) # row x columns of data\nprint(df.ndim) # dimension of data\nprint(df.size) # size of data\n\n# Seeing size, row and column counts in data with shape code and size code","4007c99d":"df.info()","e373abf5":"# duplicate detection\ndf.duplicated().sum()","ebc61ec3":"# Let's convert the variable type of the churn, current_mth_churn, line_stat and with_phone_service columns to integer.\ndf.churn.replace({'N':0,'Y':1}, inplace = True)\ndf.current_mth_churn.replace({'N':0,'Y':1}, inplace = True)\ndf.with_phone_service.replace({'N':0,'Y':1}, inplace = True)\ndf.with_phone_service = df.with_phone_service.astype(int)","9811bf06":"# complaint_cnt column contains word ' customer\/ user pass away'. we replace this word by 0 complaint.\n# complaint_cnt has mixed dtype (integer and string)\n# first we change integer in string, than replace words we want and than again change dtype to integer\ndf['complaint_cnt'] = df.complaint_cnt.astype('str')\ndf['complaint_cnt'] = df.complaint_cnt.str.replace('customer\/ user pass away','0')\ndf['complaint_cnt'] = df.complaint_cnt.astype('int')","ebcf576a":"# Latest status in the complaint_cnt\ndf.complaint_cnt.value_counts()","110a89d8":"df.describe().T\n\n# only numerical variables\n# Check mean,avarage,min,max and std ","7a0a7f4a":"def summary(df):\n    Types = df.dtypes\n    Counts = df.apply(lambda x: x.count())\n    Uniques = df.apply(lambda x: x.unique().shape[0]) # If shape[0] is not written, it returns a list of unique values.\n    Nulls = df.apply(lambda x: x.isnull().sum())\n\n    cols = ['Types', 'Counts', 'Uniques', 'Nulls']\n    str = pd.concat([Types, Counts, Uniques, Nulls], axis = 1, sort=True)\n\n    str.columns = cols\n    display(str.sort_values(by='Nulls', ascending=False))\n    print('__________Data Types__________\\n')\n    print(str.Types.value_counts())\nsummary(df)\n\n# check types,counts,uniques and nulls of the data","b7706dc2":"# creating column name's list\ncol = df.columns.to_list()\n# creating catagorical columns name list\ncatcol = [_ for _ in col if df[_].nunique() < 30]\n\n# printing all the unique values of categorical colum\nfor _ in catcol:\n    print('{} has {} unique value\/s - {}\\n'.format(_,df[_].nunique(),df[_].unique())) ","b04e1f06":"# column name term_reas_code contains name of code that have the discription in column term_reas_desc\n# creating a list of unique values of both column\n\ntermination_reasion_code = df.term_reas_code.unique()\ntermination_reasion_code_description = df.term_reas_desc.unique()\n\n# creating a dictionary for termination reasion code and description\ntermination_reasion = dict(zip(termination_reasion_code,termination_reasion_code_description))\n\ntermination_reasion","093d0047":"# Droping of no more usefule columns\n# Every customer is billed monthly so bill_cycl column is not useful\n# serv_type for each customer is BBS only one type of service so it is also not usefull\n# service_code column is also not useful for us\n# Both term_reas_desc column and term_reas_code have same meaning\n\ndf.drop(columns=['bill_cycl','serv_type','serv_code','term_reas_desc'],inplace=True)\ndf.head()","b30dc914":"# Some customer ID repeat more than once.\ndf[df[\"newacct_no\"]==\"90973314.001.000040979\"]    ","5e1bbcd3":"# There are 510125 rows. But there are only 27605 unique id. Some unique id appear in the data more than once. \n# Let's get one of each unique ID with the process here.\nyedek = df.copy()\ndf = df.drop(['tenure', 'bandwidth', 'image', 'secured_revenue', 'complaint_cnt', 'current_mth_churn', 'line_stat', 'term_reas_code'], axis=1)\ndf = df.drop_duplicates(keep = 'last')\ndf = yedek.loc[df.index, :]","c927e4f4":"df","375ac6e3":"# We checked one of the duplicate ID.\ndf.newacct_no.value_counts()","e58aacff":"# We checked one of the duplicate ID.\ndf[df[\"newacct_no\"]==\"77808624.001.000027357\"]","df574375":"# Let's drop the duplicate id.\ndf = df.drop(index=297183)\ndf = df.drop(index=227698)\ndf = df.drop(index=65061)","93e71020":"# We now have one of each unique ID in our dataset.\n# Our row count and unique id counts are equal.\ndf","7ae85ec3":"# Features with missing values\nmiss = df.isnull().sum().sort_values(ascending = False)\nmiss_per = (miss\/len(df))*100\n\n# Percentage of missing values\npd.DataFrame({'Missing_Records': miss, 'Percentage of Missing_Data': miss_per.values})","c007e506":"msno.matrix(df);\n\n# White lines indicate missing values.","e684a701":"# rows of data frame have missing values in effc_strt_date column\ndf[df['effc_strt_date'].isnull()].head()\n\n# shape of null value data frame\nprint('size of null data frame is ',df[df['effc_strt_date'].isnull()].shape)\n\n# total number of unique customers in null dataframe\nprint('total unique newacct_no is ',df[df['effc_strt_date'].isnull()].newacct_no.nunique())\n\n# churned customer in this null data set\ndf[df['effc_strt_date'].isnull()].churn.value_counts()","131bf57a":"# drop null values of eff_strt_date column\ndf.dropna(subset=['effc_strt_date'],inplace=True)\n\nmsno.matrix(df);","6856e20f":"# Let's drop the columns that we think will not affect Churn.\ndf.drop(columns = ['effc_strt_date','effc_end_date', 'term_reas_code', 'line_stat', 'newacct_no','image'], inplace = True)\ndf.head()","6811cb41":"df.info()","2295a1cc":"# Create correlation matrix\ncorr_matrix = df.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n# Drop features \ndf.drop(df[to_drop], axis=1,inplace=True)","93aa782b":"churn_col = [\"churn\"]\nfor c in churn_col:\n    print(\"{} \\n\".format(df[c].value_counts()))","5734c515":"sizes = [21910,5495]\nlabels='0 No','1 Yes'\nexplode = (0, 0.1)  \nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode,autopct='%1.1f%%',shadow=True, startangle=75 )\nax1.axis('equal') \nax1.set_title(\"Client Churn Distribution\")\n\nax1.legend(labels)\n\nplt.show()\n        \n #ratio of those who churn and those who don't","6e61e6ea":"plt.figure(figsize=(20,8))\nsns.heatmap(df.corr(), annot = True, fmt = \".2f\",robust=True,linewidths=1.3,linecolor = 'gold')\nplt.show()","edf9bd88":"# Get Correlation of \"churn\" with other variables:\nplt.figure(figsize=(10,5))\ndf.corr()['churn'].sort_values(ascending = False).plot(kind='bar')","96f92d5f":"df[\"contract_month\"].value_counts()","5f30fd7f":"sns.catplot(x= \"churn\", y= \"contract_month\", data=df);\nplt.show()\n\ndf.groupby('contract_month')[\"churn\"].value_counts(normalize=True).unstack(fill_value=0)","01555832":"sns.countplot(x= \"bandwidth\", hue=\"churn\", data=df);\nplt.xticks(rotation = 90)\nplt.show()\n\ndf.groupby('bandwidth')[\"churn\"].value_counts(normalize=True).unstack(fill_value=0)","a2b96134":"df[\"complaint_cnt\"].value_counts()","42cfccc8":"sns.countplot(x= \"complaint_cnt\", hue=\"churn\", data=df);\nplt.xticks(rotation = 90)\nplt.show()\n\ndf.groupby('complaint_cnt')[\"churn\"].value_counts(normalize=True).unstack(fill_value=0)","b742d3b4":"sns.countplot(x= \"with_phone_service\", hue=\"churn\", data=df);\nplt.xticks(rotation = 90)\nplt.show()\n\ndf.groupby('with_phone_service')[\"churn\"].value_counts(normalize=True).unstack(fill_value=0)","4c483a0c":"# Unique variables of object columns\nencoding_col=[]\nfor i in df.select_dtypes(include='object'):   \n    print(i,'-->',df[i].nunique())\n    encoding_col.append(i)","46595267":"features = list(df.columns)\nfeatures.remove('churn')\nfeatures\n\n#getting the column names and removing the churn column","71b42e8a":"float_features = [i for i in df.columns if df[i].dtype == 'float64']\nfloat_features","836d6c98":"int_features=[i for i in df.columns if df[i].dtype == 'int64']\nint_features.remove('churn')\nint_features","968bb8ef":"fig, ax = plt.subplots(2, 2, figsize = (15, 10))\nax = ax.flatten()\nfor i, c in enumerate(float_features):\n    sns.boxplot(x = df[c], ax = ax[i], palette = 'Set3')\nplt.suptitle('Box Plot', fontsize = 25)\nfig.tight_layout()\n\n#Box plot of float features","462b5377":"fig, ax = plt.subplots(2, 2, figsize = (15, 10))\nax = ax.flatten()\nfor i, c in enumerate(int_features):\n    sns.boxplot(x = df[c], ax = ax[i], palette = 'Set3')\nplt.suptitle('Box Plot', fontsize = 25)\nfig.tight_layout()\n\n#Box plot of integer features","06365c37":"# Let's look at the number of variables in bandwidth.\ndf.bandwidth.value_counts(dropna = False)","d04a7f41":"# one-hot encoding for variables with more than 2 categories\ndf2 = df.copy()\ndf2 = pd.get_dummies(df2, drop_first=True, columns = ['bandwidth'], prefix = ['bandwidth'])","8c2b8a21":"df2.corr()","5283ad17":"plt.figure(figsize=(30,11))\nsns.heatmap(df2.corr(), annot = True, fmt = \".2f\",robust=True,linewidths=1.3,linecolor = 'gold')\nplt.show()","d715b11f":"# Import Machine learning algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom sklearn.ensemble import AdaBoostClassifier\n\n#Import metric for performance evaluation\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report,confusion_matrix, ConfusionMatrixDisplay\n\n#Split data into train and test sets\nfrom sklearn.model_selection import train_test_split, cross_val_score","0b26bd65":"# Dependent and independent variables were determined.\nX = df2.drop('churn', axis=1)\ny = df2['churn']\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  ","523bccd1":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","efd89c1e":"#Defining the modelling function\ndef modeling(alg, alg_name, params={}):\n    model = alg(**params) #Instantiating the algorithm class and unpacking parameters if any\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n      \n    #Performance evaluation\n    def print_scores(alg, y_true, y_pred):\n        print(alg_name)\n        acc_score = accuracy_score(y_true, y_pred)\n        print(\"accuracy: \",acc_score)\n        pre_score = precision_score(y_true, y_pred)\n        print(\"precision: \",pre_score)\n        rec_score = recall_score(y_true, y_pred)                            \n        print(\"recall: \",rec_score)\n        f_score = f1_score(y_true, y_pred, average='weighted')\n        print(\"f1_score: \",f_score)        \n    print_scores(alg, y_test, y_pred)\n    \n    \n    cm = confusion_matrix(y_test, y_pred)\n    #Create the Confusion Matrix Display Object(cmd_obj). \n    cmd_obj = ConfusionMatrixDisplay(cm, display_labels=['churn', 'notChurn'])\n\n    #The plot() function has to be called for the sklearn visualization\n    cmd_obj.plot()\n\n    #Use the Axes attribute 'ax_' to get to the underlying Axes object.\n    #The Axes object controls the labels for the X and the Y axes. It also controls the title.\n    cmd_obj.ax_.set(\n                    title='Sklearn Confusion Matrix with labels!!', \n                    xlabel='Predicted Churn', \n                    ylabel='Actual Churn')\n    #Finally, call the matplotlib show() function to display the visualization of the Confusion Matrix.\n    plt.show()\n    \n    return model","82e0f415":"# LightGBM model\nLGBM_model = modeling(lgb.LGBMClassifier, 'Light GBM')","6c743247":"# Gradient Boosting model\ngbm_model=modeling(GradientBoostingClassifier, \"Gradient Boosting Classifier\")","b9e81cbd":"# Running RandomForestClassifier model\nRF_model = modeling(RandomForestClassifier, 'Random Forest')","a6cb7cbf":"# Running logistic regression model\nlog_model = modeling(LogisticRegression, 'Logistic Regression')","2a94a1ce":"#Trying other machine learning algorithms: SVC\nsvc_model = modeling(SVC, 'SVC Classification')","2de4a125":"# Decision tree\ndt_model = modeling(DecisionTreeClassifier, \"Decision Tree Classification\")","961b4ccd":"# Naive bayes \nnb_model = modeling(GaussianNB, \"Naive Bayes Classification\")","39b5172d":"# AdaBoost model\nAdaBoost_model = modeling(AdaBoostClassifier,'AdaBoost')","5f021d89":"df = pd.DataFrame({'Models':['LGBM', 'GBM', 'RandFor', 'LogReg', 'SVC', 'DecTree',  'Naive', 'ADA',], 'Prediction':[0.9838,0.9841, 0.9844, 0.9568 ,0.9742,0.9770,0.8831,0.9829]})\nax = df.plot.bar(x='Models', y='Prediction', rot=45)","a27b25f5":"finish = time()\nprint(strftime(\"%H:%M:%S\", gmtime(finish - start)))","42cefdb2":"# Broadband Customer Churn Prediction","d2b72ab7":"* If you want to read my Medium article, you can view the medium link below.\n\nhttps:\/\/medium.com\/@dindaryasin\/how-to-implement-customer-churn-prediction-with-machine-learning-broadband-customer-churn-cd67381bb157","bbf5b3b9":"<a id = \"14\"><\/a><br>\n# Float Variables","1998676a":"<a id = \"15\"><\/a><br>\n# Integer Variables","1fed716b":"# Changing dtype of columns","be906228":"<a id = \"21\"><\/a><br>\n# Confusion Matrix","116d2517":"#Columns effc_strt_date, effc_end_date, contract_month and ce_expiry have less missing values. On other hand columns term_reas_code and term_reas_desc have too much missing values.","4b0eb36c":"<a id = \"17\"><\/a><br>\n# One Hot Encoding","e08a8a3a":"<a id = \"22\"><\/a><br>\n# LGBM Classifier","4d1a7d1f":"<a id = \"7\"><\/a><br>\n# MissingNo Visualizations\n\n* Pandas provides functions to check the number of missing values in the dataset. Missingno library takes it one step further and provides the distribution of missing values in the dataset by informative visualizations. Using the plots of missingno, we are able to see where the missing values are located in each column and if there is a correlation between missing values of different columns.","e8bb0f2b":"# Explanation\n    Given are 19 distinguishing factors that can help in understanding the customer churn, your objective as a data scientist is to build a Machine       Learning model that can predict whether the broadband internet provider company will lose a customer or not using these factors.","91c657eb":"<a id = \"30\"><\/a><br>\n# Trial and Conclusion","3f6b2513":"<a id = \"5\"><\/a><br>\n# Termination Reasion Description\n\n 1. nan: nan,\n 2. REV: Relocate to non-coverage(w\/prof)\n 3. CLB: Closing Business\n 4. NET: Network Problem\n 5. UFSS: UNSATISFY FIELD SERVICE SUPPORT\n 6. CUCO: Downsizing \/ Cut cost\n 7. EXP: Sales Plan Not Attractive\n 8. NU: No Use\n 9. OT: (R) Overdue Termination: Involuntary termination by credit control\n 10. COVL3: (R) Coverage Issue: Low speed coverage (customer requests 30M)\n 11. COM15: COM-Unsatisfy Service quality\n 12. COVL2: (R) Coverage Issue: Low speed coverage (customer requests 100M)\n 13. OTHS: (R) Others\n 14. BILP: BILLING PROBLEM\n 15. UCSH: UNSATISFY CS HOTLINE\n 16. LOSF: LACK OF SERVICE FEATURES\n 17. EXI: Additional extra installation charge (Part II)\n 18. PLR: Parallel Run Order\n 19. COVL1: (R) Coverage Issue: Low speed coverage (customer requests 200M+)\n 20. COM10: COM-Miss follow-Help Desk\n 21. CUSN2: (R) Customer Issue: No use\n 22. UEMS: UNSATISFY EMAIL SERVICE\n 23. CUSB0: (R) Customer Issue: Bankruptcy\n 24. MGR: Migration Order\n 25. TRM: Termination\n 26. NCAP: No capacity\n 27. NWQU: Network quality","f8082808":"<a id = \"8\"><\/a><br>\n# Handling Missing Values","ccb716f4":"* We got the best prediction result in Decision Tree Classification Model in Churn.\n* In Not Churn, we got the best result in Gradient Boosting Classifier Model.\n* According to Accuracy, we got the best result with the Random Forest Classifier Model.","29ee6cf3":"<font color = 'blue'>\nContent: \n\n1. [Load and Check Data](#1)\n    * [Chancing dtype of columns](#2)\n1. [Variable Description](#3)\n    * [Check Uniques,Nulls Values ,Data's Types And Counts](#4)\n    * [Termination Reasion Description](#5)\n1. [Missing Value Analysis](#6)\n    * [MissingNo Visualizations](#7)\n    * [Handling Missing Values](#8)\n    * [Conclusion](#9)\n1. [Check The Churn](#10)\n1. [Visualization](#11)    \n1. [Basic Data Analysis](#12)\n1. [Variable Analysis](#13)    \n    * [Float Variable](#14)\n    * [Integer Variable](#15)\n1. [Feature Engineering](#16)\n    * [One Hot Encoding](#17)\n    * [Visualization](#18)\n1. [Modeling](#19)\n    * [Train - Test Split](#20)\n    * [Confusion Matrix](#21)\n    * [LGBM Classifier](#22)\n    * [Gradient Boosting Classifier](#23)\n    * [Random Forest Classifier](#24)\n    * [Logistic Regression](#25)\n    * [SVC Classification](#26)\n    * [Decision Tree Classification](#27)\n    * [Naive Bayes Classification](#28)\n    * [Ada Boost Classifier](#29)\n    * [Trial and Conclusion](#30)\n1. [Medium Article](#31)\n1. [Web Deploy](#32)","8b1abe59":"<a id = \"18\"><\/a><br>\n# Visualization","853abbe1":"<a id = \"9\"><\/a><br>\n# Conclusion\n    1- We have total 27605 rows in our data set out of this only 200 rows have missing values.\n    2- These all missing values belong to only 200 unique id out of 27605 total unique id\n    3- Only 4 unique id have been churned at yet from this null data set\n\n***As a result on the basis of above three conclusion we can say that all these rows do not have any valuable counts. So we can drope them from our data set.***","b05a7e43":"<a id = \"20\"><\/a><br>\n# Train and Test Split","419582d2":"<a id = \"29\"><\/a><br>\n# Ada Boost Classifier","b63fd27d":"<a id = \"3\"><\/a><br>\n# Variable Description\n\n1. image: Billing month and year\n2. newacct_no: Unique customer id\n3. line_stat: Ignorable columns (Use them in case you find a co-relation, but mostly they can be ignored)\n4. bill_cycl: Ignorable columns (Use them in case you find a co-relation, but mostly they can be ignored)\n5. serv_type: Ignorable columns (Use them in case you find a co-relation, but mostly they can be ignored)\n6. serv_code: Ignorable columns (Use them in case you find a co-relation, but mostly they can be ignored)\n7. tenure: Customer is in the system since how many months\n8. effc_strt_date: Contract start date\n9. effc_end_date: Contract end date\n10. contract_month: Contract type\n11. ce_expiry: Contract expiry date. if expiry date is jan'19, and today is jan'2020, then ce_expiry is -12, if expiry date is jan'21.\n12. secured_revenue: Monthly revenue \n13. bandwidth: Internet bandwidth\n14. term_reas_code: Contract termination reason code\n15. term_reas_desc: Description of contract termination reason code\n16. complaint_cnt: Number of complaint calls done by customer every month \n17. with_phone_service: Whether broadband customer has taken a phone service seaprately or not.\n18. churn: Churned or not\n19. current_mth_churn: The month where the customer is churned, let say customer got churned in june'19, and we have data from jan'19 to jan'20.","4b49d66a":"# Introduction\n\n    Broadband internet provider companies around the world operate in a very competitive environment. With various aspects of data collected from         millions of customers, it is painstakingly hard to analyze and understand the reason for a customer\u2019s decision to switch to a different insurance     provider.\n\n    For an industry where customer acquisition and retention are equally important, and the former being a more expensive process, Broadband internet     provider companies rely on data to understand customer behavior to prevent retention. Churn is a one of the biggest problem in the broadband           industry. Research has shown that the average monthly churn rate among the top 4 wireless carriers in the US is 1.9% - 2%. Thus knowing whether a     customer is possibly going to switch beforehand gives Broadband internet provider companies an opportunity to come up with strategies to prevent       it from actually happening.\n\n    So forecasting loss is a good way to create proactive marketing campaigns that target customers who are about to lose. It is possible to predict       customer churn with the help of machine learning.","987f0854":"<a id = \"4\"><\/a><br>\n# Check Uniques,Nulls Values ,Data's Types And Counts","9842f95f":"<a id = \"13\"><\/a><br>\n# Variable Analysis\n\n* float Variable: 'contract_month','ce_expiry','secured_revenue'\n* integer Variable:'tenure','complaint_cnt','with_phone_service','bandwidth_100M','bandwidth_100M(FTTO)', 'bandwidth_10M', 'bandwidth_300M(FTTO)', 'bandwidth_30M', 'bandwidth_500M(FTTO)', 'bandwidth_50M', 'bandwidth_BELOW_10M'","c2d012ed":"<a id = \"16\"><\/a><br>\n# Feature Engineering","32c3939e":"<a id = \"11\"><\/a><br>\n# Visualization","8d414e21":"<a id = \"19\"><\/a><br>\n# Modelling","bbd844a0":"<a id = \"24\"><\/a><br>\n# Random Forest Classifier","04c762ca":"<a id = \"26\"><\/a><br>\n# SVC Classification","7b5d25c1":"<a id = \"23\"><\/a><br>\n# Gradient Boosting Classifier","a12553e5":"<a id = \"10\"><\/a><br>\n# Check The Churn\n\n* Understanding and interpreting the effects of variables on Churn.","ed925f93":"* You can view and run the web-deployed version from this site.\n\nhttp:\/\/192.168.1.44:8501","c039591a":"<a id = \"32\"><\/a><br>\n# Web Deploy","952c255d":"<a id = \"31\"><\/a><br>\n# Medium Article","d134ef76":"<a id = \"12\"><\/a><br>\n# Basic Data Analysis","31519973":"<a id = \"25\"><\/a><br>\n# Logistic Regression","27504ec7":"<a id = \"1\"><\/a><br>\n# Load and Check Data","69bed997":"* float64(3): Contract_month, ce_expiry and secured_revenue\n* int64(8): Image, bill_cycl, tenure, complaint_cnt, with_phone_service, churn and current_mth_churn\n* object(8): Newacct_no, line_stat, serv_type, serv_code, effc_strt_date, effc_end_date, bandwidth, term_reas_code and term_reas_desc","f5f33117":"<a id = \"6\"><\/a><br>\n# Missing Value Analysis","2d28149b":"<a id = \"28\"><\/a><br>\n# Naive Bayes Classification","3e822976":"<a id = \"27\"><\/a><br>\n# Decision Tree Classification"}}