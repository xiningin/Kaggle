{"cell_type":{"fa4ed962":"code","137ecfc8":"code","af3d8e8c":"code","be23bef4":"code","b6e4cc8c":"code","ed699f0c":"code","66398b30":"code","b00d30c5":"code","eabd0a70":"code","54b1d776":"code","4ea6f019":"code","dc66ed78":"code","5e5eb215":"code","01ad71bd":"code","e3bffd54":"code","0364735d":"code","968c0b1a":"code","57a6d0b6":"code","1484fcd7":"code","667c6d06":"code","63253b87":"code","ef291e73":"code","aa5ddc70":"code","8f140e81":"code","f3ed69ca":"code","04bf9739":"code","b9138752":"code","c383e933":"code","6b997098":"code","c13f4dfb":"code","b4fc73b5":"code","7902c28a":"code","703dec2e":"code","d321590f":"code","fe7d616f":"code","01ec5a69":"code","92c4da26":"code","4a74e189":"code","a468bf27":"code","2f20126e":"code","478d42c5":"code","4b8c9622":"code","0a63fd48":"code","afc2ab90":"code","72cd85d9":"code","a4686c67":"code","1a95baf8":"code","451703b4":"markdown","bff13215":"markdown","ac6df43d":"markdown","279aa8da":"markdown","8f9462e9":"markdown","77df6770":"markdown","ca010384":"markdown"},"source":{"fa4ed962":"import pandas as pd","137ecfc8":"cab_df = pd.read_csv(\"..\/input\/cab_rides.csv\",delimiter='\\t',encoding = \"utf-16\")\nweather_df = pd.read_csv(\"..\/input\/weather.csv\",delimiter='\\t',encoding = \"utf-16\")","af3d8e8c":"cab_df.head()","be23bef4":"weather_df.head()","b6e4cc8c":"cab_df['date_time'] = pd.to_datetime(cab_df['time_stamp']\/1000, unit='s')\nweather_df['date_time'] = pd.to_datetime(weather_df['time_stamp'], unit='s')\ncab_df.head()","ed699f0c":"#merge the datasets to refelect same time for a location\ncab_df['merge_date'] = cab_df.source.astype(str) +\" - \"+ cab_df.date_time.dt.date.astype(\"str\") +\" - \"+ cab_df.date_time.dt.hour.astype(\"str\")\nweather_df['merge_date'] = weather_df.location.astype(str) +\" - \"+ weather_df.date_time.dt.date.astype(\"str\") +\" - \"+ weather_df.date_time.dt.hour.astype(\"str\")","66398b30":"weather_df.index = weather_df['merge_date']","b00d30c5":"cab_df.head()","eabd0a70":"merged_df = cab_df.join(weather_df,on=['merge_date'],rsuffix ='_w')","54b1d776":"merged_df['rain'].fillna(0,inplace=True)","4ea6f019":"merged_df = merged_df[pd.notnull(merged_df['date_time_w'])]","dc66ed78":"merged_df = merged_df[pd.notnull(merged_df['price'])]","5e5eb215":"merged_df['day'] = merged_df.date_time.dt.dayofweek","01ad71bd":"merged_df['hour'] = merged_df.date_time.dt.hour","e3bffd54":"merged_df['day'].describe()","0364735d":"merged_df.columns","968c0b1a":"merged_df.count()","57a6d0b6":"X = merged_df[merged_df.product_id=='lyft_line'][['day','distance','hour','temp','clouds', 'pressure','humidity', 'wind', 'rain']]","1484fcd7":"X.count()","667c6d06":"y = merged_df[merged_df.product_id=='lyft_line']['price'] ","63253b87":"y.count()","ef291e73":"X.reset_index(inplace=True)\nX = X.drop(columns=['index'])","aa5ddc70":"X.head()","8f140e81":"features = pd.get_dummies(X)","f3ed69ca":"features.columns","04bf9739":"# Use numpy to convert to arrays\nimport numpy as np\n# Labels are the values we want to predict\nlabels = np.array(y)\n\n# Saving feature names for later use\nfeature_list = list(features.columns)\n# Convert to numpy array\nfeatures = np.array(features)","b9138752":"# Using Skicit-learn to split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\n# Split the data into training and testing sets\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)","c383e933":"print('Training Features Shape:', train_features.shape)\nprint('Training Labels Shape:', train_labels.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing Labels Shape:', test_labels.shape)","6b997098":"# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n# Instantiate model with 1000 decision trees\nrf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n# Train the model on training data\nrf.fit(train_features, train_labels);","c13f4dfb":"# Use the forest's predict method on the test data\npredictions = rf.predict(test_features)\n# Calculate the absolute errors\nerrors = abs(predictions - test_labels)\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')","b4fc73b5":"# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (errors \/ test_labels)\n# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)\nprint('Accuracy:', round(accuracy, 2), '%.')","7902c28a":"# Get numerical feature importances\nimportances = list(rf.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","703dec2e":"merged_df_surge = merged_df[merged_df.surge_multiplier < 3]\nX = merged_df_surge[['day','hour','temp','clouds', 'pressure','humidity', 'wind', 'rain']]","d321590f":"X.count()","fe7d616f":"features = pd.get_dummies(X)","01ec5a69":"\ny = merged_df_surge['surge_multiplier']\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\n#ignoring multiplier of 3 as there are only 2 values in our dataset\nle.fit([1,1.25,1.5,1.75,2.,2.25,2.5])\ny = le.transform(y) ","92c4da26":"# Use numpy to convert to arrays\nimport numpy as np\n# Labels are the values we want to predict\nlabels = np.array(y)\n\n# Saving feature names for later use\nfeature_list = list(X.columns)\n# Convert to numpy array\nfeatures = np.array(features)","4a74e189":"# Using Skicit-learn to split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\n# Split the data into training and testing sets\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)","a468bf27":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=42)\ntrain_features, train_labels = sm.fit_resample(train_features, train_labels)","2f20126e":"from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\nrf = RandomForestClassifier(n_jobs=-1, random_state = 42,class_weight=\"balanced\")\n# Train the model on training data\nrf.fit(train_features, train_labels);","478d42c5":"# Use the forest's predict method on the test data\npredictions = rf.predict(test_features)\n# Calculate the absolute errors\nerrors = abs(predictions - test_labels)\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')","4b8c9622":"from sklearn.metrics import precision_score, recall_score\nprecision_score(test_labels, predictions, average=\"weighted\")\n","0a63fd48":"recall_score(test_labels, predictions, average=\"micro\")","afc2ab90":"# Create confusion matrix\npd.crosstab(le.inverse_transform(test_labels), le.inverse_transform(predictions),rownames=['Actual'],colnames=['Predicted'])","72cd85d9":"# Get numerical feature importances\nimportances = list(rf.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","a4686c67":"from sklearn.metrics import accuracy_score","1a95baf8":"accuracy_score(test_labels, predictions)","451703b4":"### Prediction Accuracy for Prices","bff13215":"### Lets see the prediction for surge_multiplier","ac6df43d":"<a href=\"https:\/\/colab.research.google.com\/github\/ravi72munde\/scala-spark-cab-rides-predictions\/blob\/Ravi\/Cab_Price_Prediction.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","279aa8da":"*The dataset is imbalanced when it comes to surge multipliers. More than 90% of the data has a surge multiplier of 1.\nWe use SMOTE for blancing the training data*","8f9462e9":"#### Accuracy of the Classifier","77df6770":"Calculating the weighted precision score(taking imbalance of the dataset in account)","ca010384":"#### Confusion Matrix for the Surge Multiplier prediction"}}