{"cell_type":{"012aa85a":"code","994466b4":"code","8c8a1531":"code","03225fd2":"code","a4255de8":"code","570c6d31":"code","7e1504b7":"code","9bef0f26":"code","646aac8f":"code","536dbad2":"code","3c03e48d":"code","f1d92af2":"code","db5adf9f":"code","bb1f5682":"code","853d2bfb":"code","b16a9b2d":"code","afe71480":"code","04bb10c3":"code","501dba1f":"code","7136326e":"code","03b835b0":"code","61d22874":"code","924b5762":"code","f6d929cc":"code","2237907f":"code","21f78f61":"code","ca49c5b3":"code","40fcc4b2":"code","5091cd29":"code","959a0c5b":"code","40520f7d":"code","f9f978c3":"code","c021c92a":"code","ae4fb9cc":"code","b6400ff6":"code","f00fc5c2":"markdown","70cb630c":"markdown","ac702359":"markdown","b8e6f641":"markdown","2a18299f":"markdown","d83f6745":"markdown","bcde0ceb":"markdown","b5282796":"markdown","5c9bdda9":"markdown","6382859b":"markdown","e5346c26":"markdown","f2375e7b":"markdown","97663557":"markdown","a1596d7b":"markdown","a3d97e03":"markdown","297bed7d":"markdown","c03ec2b4":"markdown","faf79d69":"markdown"},"source":{"012aa85a":"# Importing different packages\n\n# pandas\nimport pandas as pd\nfrom pandas import Series,DataFrame\n\n# numpy, matplotlib, seaborn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nwarnings.filterwarnings('ignore')","994466b4":"# get data to dataframe variables\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\n# data visualization\ntrain_df\n","8c8a1531":"#inspecting train dataframe\nprint(train_df.shape)\ntrain_df.info()","03225fd2":"# Checking for the missing values\ntrain_df.isnull().values.any()","a4255de8":"#inspecting test dataframe\nprint(test_df.shape)\ntest_df.info()","570c6d31":"# Checking for the missing values\ntest_df.isnull().values.any()","7e1504b7":"# some columns are hidden, so increasing the display limit\npd.set_option('display.max_columns', 785)\n\n# lets visualize the basic statistics of the variables\ntrain_df.describe()","9bef0f26":"#See the distribution of the labels\nsns.countplot(train_df.label)","646aac8f":"# lets see the distribution in numbers\ntrain_df.label.astype('category').value_counts()","536dbad2":"digits = train_df[0:8000]\n\ny = digits.iloc[:,0]\n\nX = digits.iloc[:,1:]\n\nprint(y.shape)\nprint(X.shape)","3c03e48d":"#See the distribution of the labels in sliced data\nsns.countplot(digits.label)","f1d92af2":"# Converting 1D array to 2D 28x28 array using reshape , to plot and view grayscale images\n\n#Lets see digit \"3\" images in the data.\n\nplt.figure(figsize=(28,28))\n\ndigit_3 = digits.loc[digits.label==3,:]\ndigit_image = digit_3.iloc[:,1:]\nsubplots_loc = 191\n\nfor i in range(1,9):\n    plt.subplot(subplots_loc)\n    four = digit_image.iloc[i].values.reshape(28, 28)\n    plt.imshow(four, cmap='gray')\n    subplots_loc = subplots_loc +1","db5adf9f":"# Converting 1D array to 2D 28x28 array using reshape , to plot and view grayscale images\n\n#Lets see digit \"4\" images in the data.\n\nplt.figure(figsize=(28,28))\n\ndigit_3 = digits.loc[digits.label==4,:]\ndigit_image = digit_3.iloc[:,1:]\nsubplots_loc = 191\n\nfor i in range(1,9):\n    plt.subplot(subplots_loc)\n    four = digit_image.iloc[i].values.reshape(28, 28)\n    plt.imshow(four, cmap='gray')\n    subplots_loc = subplots_loc +1","bb1f5682":"# Converting 1D array to 2D 28x28 array using reshape , to plot and view grayscale images\n\n#Lets see digit \"1\" images in the data.\n\n\nplt.figure(figsize=(28,28))\n\ndigit_3 = digits.loc[digits.label==1,:]\ndigit_image = digit_3.iloc[:,1:]\nsubplots_loc = 191\n\nfor i in range(1,9):\n    plt.subplot(subplots_loc)\n    four = digit_image.iloc[i].values.reshape(28, 28)\n    plt.imshow(four, cmap='gray')\n    subplots_loc = subplots_loc +1","853d2bfb":"# Data splitting in train and test data\nX_train, X_test,y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)","b16a9b2d":"# Shapes of the data\nprint(\"Train Data Shape: \",X_train.shape)\nprint(\"Train Label Shape: \",y_train.shape)\nprint(\"Test Data Shape: \",X_test.shape)\nprint(\"Test Label Shape: \",y_test.shape)","afe71480":"# lets see the distribution of the data in the train and test data\nplt.figure(figsize=(12,6))\nplt.subplot(121)\nsns.countplot(y_train.to_frame(name='label').label)\nplt.title('train data')\nplt.subplot(122)\nsns.countplot(y_test.to_frame(name='label').label)\nplt.title('test data')","04bb10c3":"from sklearn.preprocessing import MinMaxScaler\n\n#Scaling data\nscale = MinMaxScaler()\nX_train_s = scale.fit_transform(X_train)\nX_test_s = scale.transform(X_test)\n","501dba1f":"from sklearn import svm\nfrom sklearn import metrics\n\n# an initial SVM model with linear kernel   \nsvm_linear = svm.SVC(kernel='linear')\n\n# fit\nsvm_linear.fit(X_train_s, y_train)","7136326e":"# predict\npredictions = svm_linear.predict(X_test_s)\npredictions[:10]","03b835b0":"# measure accuracy\nmetrics.accuracy_score(y_true=y_test, y_pred=predictions)","61d22874":"# class-wise accuracy\nclass_wise = metrics.classification_report(y_true=y_test, y_pred=predictions)\nprint(class_wise)","924b5762":"# rbf kernel with other hyperparameters kept to default \nsvm_rbf = svm.SVC(kernel='rbf')\nsvm_rbf.fit(X_train_s, y_train)","f6d929cc":"# predict\npredictions = svm_rbf.predict(X_test_s)\n\n# accuracy \nprint(metrics.accuracy_score(y_true=y_test, y_pred=predictions))","2237907f":"# class-wise accuracy\nclass_wise = metrics.classification_report(y_true=y_test, y_pred=predictions)\nprint(class_wise)","21f78f61":"from sklearn.model_selection import KFold\n# creating a KFold object with 5 splits \nfolds = KFold(n_splits = 5, shuffle = True, random_state = 4)\n\n# specify range of parameters (C)  and (gamma) as a list\nhyper_params = [ {'gamma': [1e-2, 1e-3, 1e-4],\n                     'C': [1, 10, 100, 1000]}]\n\n# specify model\nmodel = svm.SVC(kernel=\"rbf\")\n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = model, \n                        param_grid = hyper_params, \n                        scoring= 'accuracy', \n                        cv = folds, \n                        verbose = 1,\n                        return_train_score=True)   \n\n# fit\nmodel_cv.fit(X_train_s, y_train)","ca49c5b3":"# results\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","40fcc4b2":"# converting C to numeric type for plotting on x-axis\ncv_results['param_C'] = cv_results['param_C'].astype('int')\n\n# # plotting\nplt.figure(figsize=(16,6))\n\n# subplot 1\/3\nplt.subplot(131)\ngamma_01 = cv_results[cv_results['param_gamma']==0.01]\n\nplt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_test_score\"])\nplt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.01\")\nplt.ylim([0.70, 1.05])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')\n\n# subplot 2\/3\nplt.subplot(132)\ngamma_001 = cv_results[cv_results['param_gamma']==0.001]\n\nplt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_test_score\"])\nplt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.001\")\nplt.ylim([0.70, 1.05])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')\n\n\n# subplot 3\/3\nplt.subplot(133)\ngamma_0001 = cv_results[cv_results['param_gamma']==0.0001]\n\nplt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_test_score\"])\nplt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.0001\")\nplt.ylim([0.70, 1.05])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')\n","5091cd29":"# printing the optimal accuracy score and hyperparameters\nbest_score = model_cv.best_score_\nbest_hyperparams = model_cv.best_params_\n\nprint(\"The best test score is {0} corresponding to hyperparameters {1}\".format(best_score, best_hyperparams))","959a0c5b":"# model with optimal hyperparameters\n\n# model\nmodel = svm.SVC(C=10, gamma=0.01, kernel=\"rbf\")\n\nmodel.fit(X_train_s, y_train)\ny_pred = model.predict(X_test_s)\n\n# metrics\nprint(\"accuracy\", metrics.accuracy_score(y_test, y_pred), \"\\n\")\nprint(metrics.confusion_matrix(y_test, y_pred), \"\\n\")\n","40520f7d":"#scaling of test.csv data\nX_test_df = scale.transform(test_df)","f9f978c3":"# predict\npredicted_digit = model.predict(X_test_df)","c021c92a":"# shape of the predicted digits\npredicted_digit.shape","ae4fb9cc":"# Creating dataframe\ndata = pd.DataFrame({'Label': predicted_digit})\ndata.head()","b6400ff6":"data.to_csv('digi_predictions.csv', sep=\",\")","f00fc5c2":"### Since training dataset is quite large, we will consider the 20% of the data to train and validate the model and also to find the optimal hyperparameter.","70cb630c":"Summary of the plots above:\n- At very high gamma (0.01), the model is achieving almost 100% accuracy on the training data, when C > 10 and the test score is decreasing or flatening. \nThus, the model is overfitting when C>10.\n\n- At gamma=0.001, the training and test scores are widening significantly, when C>30, i.e. the model starts to overfit at higher values of C.\n\n- At gamma=0.0001, the model does not overfit till C=10 but accuracy is about 90%. When C >10, accuracy increasing on train data but not on test data.\n\nThus, it seems that the best combination is gamma=0.01 and C=10, which gives the highest test accuracy (~95%) while avoiding overfitting.\n\nLet's now build the final model and see the performance on test data.\n\n","ac702359":"### Summary\n    There are no missing values.\n    All the variables are of int datatype.\n    Test data has no labels, and hence we have to send the predicted labels to the kaggle.","b8e6f641":"### Summar of result\n\n    Overall accuracy is 90% of the model.\n    The digit 6 and 0 is correctly identified in 94% cases.\n    The model is worst for digit 3, it has only accuracy of 83%.","2a18299f":"### Scaling of the data\nNecessary because it take less iteration to converse to the solution","d83f6745":"### Splitting data into Train and Test Data and Labels\n<br>\n\n    Dividing Data randomly into train and test. \n    Out of 8000 data \"train.csv\" Data, 80% is kept as train for training the model\n    and 20% is kept as test to validate the model. ","bcde0ceb":"The data labels are almost equally distributed. So no problem there.","b5282796":"## Grid Search to find Optimal Hyperparameter C and gamma\n\nThe accuracy achieved with a linear kernel is almost same as non-linear(RBF) one.\n\nLets use the non-linear classifier model and optimize the hyperparameters C and gamma through grid search CV.","5c9bdda9":"There are some columns which are having all zero value. Those can be dropped but I choose to keep it as it should not affect the performance of the alogorithms as it is all zero.","6382859b":"### Distribution of the labels","e5346c26":"Data distribution seems ok in both the train and test dataset.","f2375e7b":"### Summar of result\n\n    Overall accuracy is 91% of the model.\n    The digit 6 is correctly identified in 97% cases.\n    The model is worst for digit 3, it has only accuracy of 85%.","97663557":"## Summary of the result\n\n    The accuracy achieved using a non-linear kernel (~0.96) is mush higher than that of a linear one (~0.91). \n    \n    We can conclude that the problem is highly non-linear in nature.","a1596d7b":"### Non-Linear SVM: RBF kernal\n\nLet's now try a non-linear model with the RBF kernel.","a3d97e03":"### Reading data\n\nAs there is no labels in the test data, we do not need for the our assignment.","297bed7d":"### Building and Evaluating the Final Model\n\nLet's now build and evaluate the final model, i.e. the model with highest test accuracy.","c03ec2b4":"## Model building\nLet's fist build two basic models - linear and non-linear with default hyperparameters, and compare the accuracies.\n\n### Linear SVM\n\nLet's first try building a linear SVM model (i.e. a linear kernel).","faf79d69":"## Prediction on the test.csv "}}