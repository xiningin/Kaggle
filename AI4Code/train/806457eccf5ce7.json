{"cell_type":{"2a79f27e":"code","7b3142de":"code","ee7e7f53":"code","00d254df":"code","da360533":"code","519ae149":"code","13cd8815":"code","7557fa8f":"code","70a2ff44":"code","70b9725a":"code","98203527":"code","2ef1f912":"code","e6736d4f":"code","6348916b":"markdown","2ca61b0b":"markdown","e2eae7ed":"markdown","e270e63e":"markdown","d2ce9b57":"markdown"},"source":{"2a79f27e":"import os\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer \nfrom sklearn.preprocessing import FunctionTransformer, OneHotEncoder, OrdinalEncoder, LabelBinarizer, MinMaxScaler\nfrom sklearn.model_selection import cross_validate, learning_curve, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier, RandomForestClassifier\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\n\nrandom_state = 42\nnp.random.seed(random_state)\ncv = 4\n\nfor dirname, _, filenames in os.walk('input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7b3142de":"%%time\ntrain_df = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/train.csv', index_col='id')\nsample_submission_df = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/sample_submission.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/test.csv', index_col='id')","ee7e7f53":"sample_submission_df.head()","00d254df":"print(train_df.shape)\nprint(test_df.shape)","da360533":"train_df.info()\n","519ae149":"train_df = shuffle(train_df, random_state=random_state)\ntrain_Y = train_df['target'].copy()\ntrain_X = train_df.drop('target', axis=1).copy()","13cd8815":"train_df.head()","7557fa8f":"def get_pipline(estimator):\n    return Pipeline(steps=[\n        ('preproc', ColumnTransformer([\n            ('bin_0_2', 'passthrough', ['bin_0', 'bin_1', 'bin_2']),\n            ('bin_3_4', FunctionTransformer(func=lambda X: X.replace({'F': 0, 'T': 1, 'N': 0, 'Y': 1}), validate=False), [\n                'bin_3', 'bin_4']\n            ),\n            ('nom_0_4', OneHotEncoder(sparse=True, handle_unknown='ignore'), [\n                'nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\n            ),\n            ('ord', Pipeline(steps=[\n                ('replace', ColumnTransformer([\n                    ('encoder',  OrdinalEncoder(categories=[\n                        ['Novice', 'Contributor', 'Expert', 'Master', 'Grandmaster'],\n                        ['Freezing', 'Cold', 'Warm', 'Hot', 'Boiling Hot', 'Lava Hot'],\n                        np.sort(train_df['ord_3'].unique()),\n                        np.sort(train_df['ord_4'].unique()),\n                        np.sort(train_df['ord_5'].unique()),\n                    ]), ['ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5']),\n                ], remainder='passthrough')),\n                ('mm_scaler', MinMaxScaler())\n            ]), ['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5', 'day', 'month'] )\n        ])),    \n        ('est', estimator)\n    ])\n\nxgb_pipline = get_pipline(XGBClassifier(objective='binary:logistic', n_estimators=1100, max_depth=6, gamma=5))\nlogit_pipline = get_pipline(LogisticRegression(solver='lbfgs', max_iter=225, C=0.12, random_state=random_state))\n\npipline = VotingClassifier([('xgb', xgb_pipline), ('logit', logit_pipline)], voting='soft', n_jobs=-1)","70a2ff44":"params = {'est__solver': ['lbfgs'], 'est__C': [0.11, 0.12, 0.13], 'est__max_iter': [225, 250, 275]}\ngs_cv = GridSearchCV(logit_pipline, params, scoring='roc_auc', cv=cv, n_jobs=-1, return_train_score=True, verbose=1)\n\n#print('Best params {}, score {}'.format(gs_result.best_params_, gs_result.best_score_))\n#Best params {'est__C': 0.12, 'est__max_iter': 225, 'est__solver': 'lbfgs'}, score 0.7998486767216677","70b9725a":"cv_scores = cross_validate(\n    pipline, train_X, train_Y, scoring='roc_auc', cv=cv, n_jobs=-1, \n    return_train_score=True, return_estimator=True, verbose=1\n)","98203527":"print(cv_scores['train_score'])\nprint(cv_scores['test_score'])\ncv_scores.keys()","2ef1f912":"%%time\n\nscores = np.array([est.predict_proba(test_df) for est in cv_scores['estimator']])\nmean_scores = scores.mean(axis=0)[:, 0]\n\nsubmit_df = pd.DataFrame({ 'id': test_df.index, 'target': mean_scores })\nsubmit_df.to_csv('submission.csv', index=False)","e6736d4f":"def plot_learning_curve(estimator, title, X, y):\n    plt.figure()\n    plt.title(title)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5), \n        random_state=random_state)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    plt.show()\n\nplot_learning_curve(logit_pipline, 'logit', train_X, train_Y)","6348916b":"# Build Pipline with Ensemble","2ca61b0b":"# Load dependencies and data","e2eae7ed":"# Task\nhttps:\/\/www.kaggle.com\/c\/cat-in-the-dat","e270e63e":"# Submit","d2ce9b57":"# Train"}}