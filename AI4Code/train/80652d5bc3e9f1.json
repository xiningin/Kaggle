{"cell_type":{"febf6075":"code","dbb30eb9":"code","3ec543e4":"code","d37639b6":"code","1bb821ec":"code","17fd0df5":"code","567dc411":"code","acd3d040":"code","1974a449":"code","bc9ce0e0":"code","f9920c15":"code","467fdac2":"code","d0cf8dc0":"code","a95bf1ec":"code","ff8737b3":"code","e69945ab":"code","46942f19":"code","8721622a":"code","c91f59a1":"code","1b86efbc":"markdown","08352ede":"markdown","9d446b34":"markdown","468d1ccb":"markdown","724603a7":"markdown","934605c4":"markdown","5532230a":"markdown","dcc92d5b":"markdown","799f778f":"markdown","72e8b9c7":"markdown","bd3423ff":"markdown","6dece659":"markdown","f42bb76d":"markdown","7f03cbb2":"markdown","e4071632":"markdown"},"source":{"febf6075":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\nfrom sklearn.model_selection import cross_validate, cross_val_score, train_test_split, KFold, GridSearchCV\nfrom sklearn.metrics import classification_report, accuracy_score\n\nimport time\nimport re\nimport pickle\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud\n\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier","dbb30eb9":"df = pd.read_json('\/kaggle\/input\/news-category-dataset\/News_Category_Dataset_v2.json', lines=True)\ndf.head()","3ec543e4":"df.isna().sum()","d37639b6":"# top 5 Categories of news in our datset\ndf.category.value_counts()[:5]","1bb821ec":"# our unique labels of text which are to be classified.\nnp.unique(df.category)","17fd0df5":"df['text'] = df['headline'] + df['short_description'] + df['authors']\ndf['label'] = df['category']\ndel df['headline']\ndel df['short_description']\ndel df['date']\ndel df['authors']\ndel df['link']\ndel df['category']","567dc411":"df.head(10)","acd3d040":"df['text'].apply(lambda x: len(x.split(' '))).sum()","1974a449":"def get_simple_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","bc9ce0e0":"REMOVE_SPECIAL_CHARACTER = re.compile('[\/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS = re.compile('[^0-9a-z #+_]')\n\nSTOPWORDS = set(stopwords.words('english'))\npunctuation = list(punctuation)\nSTOPWORDS.update(punctuation)\n\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text(text):\n    # part 1\n    text = text.lower() # lowering text\n    text = REMOVE_SPECIAL_CHARACTER.sub('', text) # replace REPLACE_BY_SPACE symbols by space in text\n    text = BAD_SYMBOLS.sub('', text) # delete symbols which are in BAD_SYMBOLS from text\n    \n    # part 2\n    clean_text = []\n    for w in word_tokenize(text):\n        if w.lower() not in STOPWORDS:\n            pos = pos_tag([w])\n            new_w = lemmatizer.lemmatize(w, pos=get_simple_pos(pos[0][1]))\n            clean_text.append(new_w)\n    text = \" \".join(clean_text)\n    \n    return text","f9920c15":"df['text'] = df['text'].apply(clean_text)","467fdac2":"# df.to_csv('news_text_cleaned.csv', columns=['text', 'label'])","d0cf8dc0":"df.head(10)","a95bf1ec":"df['text'].apply(lambda x: len(x.split(' '))).sum()","ff8737b3":"X=df.text\ny=df.label\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","e69945ab":"# Creating Models\nmodels = [('Logistic Regression', LogisticRegression(max_iter=500)),('Random Forest', RandomForestClassifier()),\n          ('Linear SVC', LinearSVC()), ('Multinomial NaiveBayes', MultinomialNB()), ('SGD Classifier', SGDClassifier())]\n\nnames = []\nresults = []\nmodel = []\nfor name, clf in models:\n    pipe = Pipeline([('vect', CountVectorizer(max_features=30000, ngram_range=(1, 2))),\n                    ('tfidf', TfidfTransformer()),\n                    (name, clf),\n                    ])\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    accuracy = accuracy_score(y_pred, y_test)\n    \n    names.append(name)\n    results.append(accuracy)\n    model.append(pipe)\n    \n    msg = \"%s: %f\" % (name, accuracy)\n    print(msg)","46942f19":"# Logistic Regression\nfilename = 'model_lr.sav'\npickle.dump(model[0], open(filename, 'wb'))\n\n# Linear SVC\nfilename = 'model_lin_svc.sav'\npickle.dump(model[2], open(filename, 'wb'))","8721622a":"lr_model = pickle.load(open('model_lin_svc.sav', 'rb'))","c91f59a1":"text1 = 'Could have been best all-rounder India ever produced in ODIs: Irfan Pathan'\ntext2 = \"Ashwin Kkumar dances to Kamal Haasan's Annathe song on a treadmill. Actor is proud\"\nprint(lr_model.predict([text1, text2]))","1b86efbc":"## If you like this notebook please consider upvoting it. Any suggestion for improvement is appreciated :-). Thanks for giving your time.","08352ede":"I am saving the cleaned data so that i dont have to go through this process again as it takes a lot of time.","9d446b34":"# Splitting Data","468d1ccb":"Let's check total number of words in our dataset.","724603a7":"# Loading Dataset","934605c4":"# Loading and Testing","5532230a":"# Saving Model","dcc92d5b":"# Cleaning Data","799f778f":"Let's check how many words are remaining after cleaning the data.","72e8b9c7":"# Comparing Models","bd3423ff":"Let's check is their any Nan values which we should take care of.","6dece659":"We can see our model predicted correct for text2 but wrong for text1 which should be a Sports category.","f42bb76d":"Our dataset have no empty values.","7f03cbb2":"In this section we just removed lemmatization and cleaned our words by removing stopwords.\n\n#### Lemmatization : It is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. It is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.","e4071632":"# Importing Libraries"}}