{"cell_type":{"9678e59d":"code","32b7b30e":"code","0deb09f3":"code","c8c74b23":"code","27f59bc2":"code","e0e0fa06":"code","ec87f53d":"code","55496d11":"code","a764a135":"code","512a52ac":"code","4bf3f0d4":"code","9dc11e3d":"code","de10ba69":"code","56d1ee0e":"code","d3c82f20":"code","ac1d20ee":"code","3542d548":"code","612fee13":"code","09e1302b":"code","d6d7f672":"code","1845b8b2":"code","554b5d89":"code","324524bb":"code","133a956d":"code","de2b70ac":"code","1b589f0d":"code","5b91b7ee":"code","c4c977a6":"code","623bf909":"code","0c7873f9":"code","4ac9637c":"code","75a3528b":"code","9453f439":"code","f86eb95e":"code","ad52ac75":"code","8fa84555":"code","957e2140":"code","8e999751":"code","c9ebfc80":"code","bb483ce7":"code","757c1f0a":"code","9184bcd5":"code","a69b5caf":"code","82386551":"code","6d534417":"code","3dd24b6f":"code","29354edc":"code","4cad4088":"code","33107232":"code","17f01af0":"code","87f4dbd7":"code","5bf4d5fe":"code","af161db8":"code","8dd379cd":"code","41e6b84b":"code","ed8d95af":"code","a6369f88":"code","7d18d977":"code","2f08cd92":"code","ea90cf8d":"code","7a649e99":"code","bfa24ac5":"code","55900e20":"code","e410dab7":"code","8c1f6794":"code","92614030":"code","5d06bc53":"code","3ba37566":"code","496e7563":"code","eec8a93e":"code","3183c4aa":"code","545e6d37":"code","8a6aa55d":"code","69c475d9":"code","a4e366f2":"code","331c860c":"code","3dd369e5":"code","c0a6bc7b":"code","60b8c609":"code","3afe0030":"code","e4ce9f6b":"code","790800f6":"code","b63fd867":"code","ce9e93ee":"code","0907b94c":"code","060983f9":"code","327d8a4a":"code","affbfbb3":"code","b3582dd2":"code","fbc461cb":"code","f1d231fc":"code","edd3ba46":"code","f5988325":"code","e6b97d51":"code","8f21987b":"code","35a6459c":"markdown","14e9a42a":"markdown","3775fd34":"markdown","6f79542f":"markdown","d284363a":"markdown","b528475d":"markdown","21c7497f":"markdown","94815870":"markdown","9e5f1215":"markdown","93676b23":"markdown","83867601":"markdown","6e7377f9":"markdown","8a407302":"markdown","31f2ca66":"markdown","4acb49d9":"markdown","d8deca37":"markdown","13ec17e7":"markdown","47091a45":"markdown","03051af6":"markdown","763f7157":"markdown","99813bb7":"markdown","ac5c12e3":"markdown","0d8b5977":"markdown","9b4c1f64":"markdown","3a45f247":"markdown","5da386b7":"markdown","c7c37e7e":"markdown","8c04d9b8":"markdown","b31e426c":"markdown","09aecd38":"markdown","03b6e2ca":"markdown","ec84e422":"markdown","584cfa17":"markdown","9e1e0ab0":"markdown","a8f6a463":"markdown","f0b43cb4":"markdown","2b15a018":"markdown","58aa9559":"markdown","5194d887":"markdown","b29f3fc2":"markdown","fef9117a":"markdown","01c12fbf":"markdown","d0742bd7":"markdown","25f4a932":"markdown","4f39ffe7":"markdown","440f76ba":"markdown","37520a6a":"markdown","eac2675b":"markdown","c0e0bac1":"markdown","19c413d9":"markdown","226acb89":"markdown","3db11cd1":"markdown","3c0d0f22":"markdown","d0cbb759":"markdown","eba8b5d2":"markdown","ff617912":"markdown","1d8449e4":"markdown","f55ce5aa":"markdown","3840d7b0":"markdown","60622152":"markdown","1d56737a":"markdown","df7a27e3":"markdown","5af34555":"markdown","dd2d849f":"markdown","a689c4a4":"markdown","7998972f":"markdown","4f9c8212":"markdown","f2a95aab":"markdown","6397d6f1":"markdown","306d6bfa":"markdown","9bad706c":"markdown","22d6edab":"markdown","724c2960":"markdown","62b1ed82":"markdown","061e75ab":"markdown","0e44dee2":"markdown","ac5bc13c":"markdown","36add05f":"markdown","2c83fe02":"markdown","6bdbb41c":"markdown","a3d4c3fc":"markdown","06dbead2":"markdown","80487d1a":"markdown","699c44dd":"markdown","cd6b9de4":"markdown","36e1ddea":"markdown","c8de1318":"markdown","bc6a1e85":"markdown","01bedaae":"markdown","f8e96432":"markdown","108053a0":"markdown","918b7119":"markdown","17c530b7":"markdown","140534e2":"markdown","20aab286":"markdown","749bae32":"markdown","49daa32a":"markdown","1f518fb7":"markdown","7ed662bf":"markdown","2d6dc9f3":"markdown","9e80fecf":"markdown","0270ae68":"markdown","6529aad0":"markdown","5ed84c4a":"markdown","43b17f3b":"markdown","0cb7b1d0":"markdown","42e34ea6":"markdown","65274633":"markdown","93d13406":"markdown","f578fa69":"markdown","710935b3":"markdown","2c091f23":"markdown","4af5a1dc":"markdown","225cb542":"markdown","27cc1d88":"markdown","482ac4fd":"markdown","46e88a9a":"markdown","b5b0045d":"markdown","81fbd958":"markdown","284251b9":"markdown","d764c20d":"markdown","5e3b1460":"markdown","1b8a822d":"markdown","0711061d":"markdown","6bae5786":"markdown","f799dd33":"markdown","0363c6f8":"markdown","8c856575":"markdown","c733f754":"markdown","e30f4d96":"markdown","917eac1b":"markdown","d47bea74":"markdown","6764496a":"markdown","13ee3b81":"markdown","b193d733":"markdown","1e4249d8":"markdown","831c3e3d":"markdown","320ceac7":"markdown","810e401c":"markdown","b26c486b":"markdown","e6ceba15":"markdown","7f3c6736":"markdown","827ddad8":"markdown","d4045fb1":"markdown","3a430b99":"markdown","e8984d0b":"markdown","5b034a02":"markdown","00978f0f":"markdown","074f2c45":"markdown","5299483b":"markdown","018d36a3":"markdown","0ae009b1":"markdown","dab10c87":"markdown","a5312124":"markdown","36b71edd":"markdown","e63e9d81":"markdown","abd8bdba":"markdown","0721b785":"markdown","9d7c6e1d":"markdown","c7e26141":"markdown","c7429be5":"markdown","e4a3daaa":"markdown","fffa0f80":"markdown","0e11a861":"markdown","2de1c026":"markdown","232e1bb6":"markdown","b85505de":"markdown","3a35953b":"markdown","f7506cb6":"markdown","dfb8ecae":"markdown","a05f22e6":"markdown","a046dea7":"markdown","d3d40230":"markdown","9d0a9c2f":"markdown","efc9fcc4":"markdown","1566ff69":"markdown","1632bf98":"markdown","67c9b9f4":"markdown","cf335145":"markdown","2ddfffb6":"markdown","90d4f6a4":"markdown","3c14323f":"markdown","ec543c83":"markdown","87ef31fa":"markdown","e6e635f8":"markdown","967e19cf":"markdown","c5fc7168":"markdown","8422b2a6":"markdown","1705c541":"markdown","08a1dfcc":"markdown","a379a470":"markdown","6c419be7":"markdown"},"source":{"9678e59d":"from sklearn.base import clone\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, Normalizer, FunctionTransformer\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n\n# Local application\nimport utilidad_grupor as utils","32b7b30e":"seed = 27912","0deb09f3":"filepath = \"..\/input\/breast-cancer-wisconsin-data\/data.csv\"\n\nindex = \"id\"\ntarget = \"diagnosis\"\n\ndata = utils.load_data(filepath, index, target)","c8c74b23":"data.sample(5, random_state=seed)","27f59bc2":"del data['Unnamed: 32']","e0e0fa06":"data.sample(5, random_state=seed)","ec87f53d":"data.diagnosis.unique()","55496d11":"data['diagnosis']=data['diagnosis'].map({'M':1,'B':0})","a764a135":"(X, y) = utils.divide_dataset(data, target=\"diagnosis\")","512a52ac":"X.sample(5, random_state=seed)","4bf3f0d4":"y.sample(5, random_state=seed)","9dc11e3d":"train_size = 0.7\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=y,\n                                                      random_state=seed,\n                                                      train_size=train_size)","de10ba69":"train = utils.join_dataset(X_train, y_train)\ntest = utils.join_dataset(X_test, y_test)","56d1ee0e":"train.sample(5, random_state=seed)","d3c82f20":"from sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\n\npreproc = ColumnTransformer([(\"\", \"drop\", [\"concavity_mean\", \"compactness_mean\", \"concavity_se\", \"compactness_se\", \"concavity_worst\", \"compactness_worst\", \"area_mean\", \"perimeter_mean\", \"area_se\", \"perimeter_se\", \"area_worst\", \"perimeter_worst\"])], remainder=\"passthrough\")","ac1d20ee":"normalizacion = Normalizer()","3542d548":"n_neighbors = 5\nweights = 'distance'\n\nk_neighbors_model = make_pipeline(\n        preproc,\n        normalizacion,\n        KNeighborsClassifier(n_neighbors, weights=weights)\n)","612fee13":"decision_tree_model = make_pipeline(\n        preproc,\n        DecisionTreeClassifier(random_state=seed, \n                               max_depth=3,\n                               criterion='entropy',\n                               ccp_alpha=0.1,\n                               min_samples_leaf=25)\n) ","09e1302b":"adaboost_model = AdaBoostClassifier(random_state=seed)\n\nadaboost_model = make_pipeline(\n        preproc,\n        AdaBoostClassifier(random_state=seed, base_estimator=DecisionTreeClassifier(max_depth=1))\n)","d6d7f672":"bagging_model = make_pipeline(\n        preproc,\n        BaggingClassifier(random_state=seed, base_estimator=DecisionTreeClassifier(max_depth=None))\n)","1845b8b2":"random_forest_model = make_pipeline(\n        preproc,\n        RandomForestClassifier(random_state=seed)\n)","554b5d89":"gradient_boosting_model = make_pipeline(\n        preproc,\n        GradientBoostingClassifier(random_state=seed)\n)","324524bb":"hist_gradient_boosting_model = make_pipeline(\n        preproc,\n        HistGradientBoostingClassifier(random_state=seed)\n)","133a956d":"n_splits = 10\nn_repeats = 5\n\ncv = RepeatedStratifiedKFold(n_splits=n_splits,\n                             n_repeats=n_repeats,\n                             random_state=seed)","de2b70ac":"model = decision_tree_model\n\nutils.evaluate_estimator(model, X_train, y_train, cv)","1b589f0d":"model = k_neighbors_model\n\nutils.evaluate_estimator(model, X_train, y_train, cv)","5b91b7ee":"model = adaboost_model\n\nutils.evaluate_estimator(model, X_train, y_train, cv)","c4c977a6":"model = bagging_model\n\nutils.evaluate_estimator(model, X_train, y_train, cv)","623bf909":"model = random_forest_model\n\nutils.evaluate_estimator(model, X_train, y_train, cv)","0c7873f9":"model = gradient_boosting_model\n\nutils.evaluate_estimator(model, X_train, y_train, cv)","4ac9637c":"model = hist_gradient_boosting_model\n\nutils.evaluate_estimator(model, X_train, y_train, cv)","75a3528b":"scoring = [\"accuracy\",\"recall\"]","9453f439":"estimator = decision_tree_model\n\nmax_depth = [3, 4, 5, 6, 7]\nccp_alpha = [0, 0.05, 0.1, 0.2]\ncriterion = ['entropy', 'gini']\nmin_samples_leaf = [5, 10, 15, 20, 25]\n\ndecision_tree_clf = utils.optimize_params(estimator,\n                                        X_train, y_train, cv,\n                                        scoring=scoring, refit=\"recall\",\n                                        decisiontreeclassifier__max_depth=max_depth,\n                                        decisiontreeclassifier__criterion=criterion,\n                                        decisiontreeclassifier__ccp_alpha=ccp_alpha,\n                                        decisiontreeclassifier__min_samples_leaf=min_samples_leaf)","f86eb95e":"estimator = k_neighbors_model\n\nweights = [\"uniform\", \"distance\"]\nn_neighbors = [3, 5, 7, 9, 11, 13, 15]\n\n\nk_neighbors_clf = utils.optimize_params(estimator,\n                                        X_train, y_train, cv, scoring=scoring, refit=\"recall\",\n                                        kneighborsclassifier__weights=weights,\n                                        kneighborsclassifier__n_neighbors=n_neighbors)","ad52ac75":"estimator = adaboost_model\n\ncriterion = [\"gini\", \"entropy\"]\nlearning_rate = [0.25, 0.5, 0.75, 1]\nmin_samples_split = [2, 5, 10, 20, 30]\n\nadaboost_clf = utils.optimize_params(estimator,\n                                        X_train, y_train, cv, scoring=scoring, refit=\"recall\",\n                                        adaboostclassifier__base_estimator__min_samples_split=min_samples_split,\n                                        adaboostclassifier__base_estimator__criterion=criterion,\n                                        adaboostclassifier__learning_rate=learning_rate)","8fa84555":"estimator = bagging_model\n\nmax_samples = [0.25, 0.5, 0.75, 1]\nmax_features = [0.25, 0.5, 0.75, 1]\n\nbagging_clf = utils.optimize_params(estimator,\n                                     X_train, y_train, cv, scoring=scoring, refit=\"recall\",\n                                     baggingclassifier__max_samples=max_samples,\n                                     baggingclassifier__max_features=max_features)","957e2140":"estimator = random_forest_model\n\nmax_samples = [0.25, 0.5, 0.75]\nmax_features = ['sqrt', 'log2']\ncriterion = [\"gini\", \"entropy\"]\n\nrandom_forest_clf = utils.optimize_params(estimator,\n                                               X_train, y_train, cv, scoring=scoring, refit=\"recall\",\n                                               randomforestclassifier__max_samples=max_samples,\n                                               randomforestclassifier__criterion=criterion,\n                                               randomforestclassifier__max_features=max_features)","8e999751":"estimator = gradient_boosting_model\n\nlearning_rate = [0.025, 0.05, 0.1]\nmax_depth = [1, 3, 5, 7]\n\n\ngradient_boosting_clf = utils.optimize_params(estimator,\n                                               X_train, y_train, cv, scoring=scoring, refit=\"recall\",\n                                               gradientboostingclassifier__learning_rate = learning_rate,\n                                               gradientboostingclassifier__max_depth=max_depth)","c9ebfc80":"estimator = hist_gradient_boosting_model\n\nmin_samples_leaf = [10, 20, 30, 40]\nlearning_rate = [0.05, 0.1, 0.15]\n\nhist_gradient_boosting_clf = utils.optimize_params(estimator,\n                                                  X_train, y_train, cv, scoring=scoring, refit=\"recall\",\n                                                  histgradientboostingclassifier__learning_rate=learning_rate,\n                                                  histgradientboostingclassifier__min_samples_leaf=min_samples_leaf)","bb483ce7":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf,\n    \"Decision tree\": decision_tree_clf,\n    \"AdaBoost\": adaboost_clf,\n    \"Bagging\": bagging_clf,\n    \"Random Forests\": random_forest_clf,\n    \"Gradient Boosting\": gradient_boosting_clf,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf\n}","757c1f0a":"utils.evaluate_estimators(estimators, X_test, y_test)","9184bcd5":"random_state = 27912","a69b5caf":"filepath = \"..\/input\/pima-indians-diabetes-database\/diabetes.csv\"\n\ntarget = \"Outcome\"\n\ndata = utils.load_data(filepath, None, target)\n\ntrain, test = train_test_split(data, train_size=0.7, stratify=data[target], random_state=random_state)\n\ntrain_X, train_y = utils.divide_dataset(train, target)\ntest_X, test_y = utils.divide_dataset(test, target)\n\ntrain.sample(5)\n","82386551":"mean_imp = SimpleImputer(missing_values=0)\nremove_columns = ColumnTransformer([('a', 'drop', ['Insulin', 'SkinThickness']),\n                                    ('b', mean_imp, ['Glucose', 'BloodPressure', 'BMI'])],\n                                    remainder='passthrough')\n\nnormalizer = Normalizer()\n","6d534417":"cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=random_state)","3dd24b6f":"model = KNeighborsClassifier()\npl = make_pipeline(remove_columns, normalizer, model)\nn_neighbors = np.arange(1, 16)\nweights = [\"uniform\", \"distance\"]\n\nk_neighbors_clf = utils.optimize_params(pl, train_X, train_y, cv=cv, scoring=\"recall\", kneighborsclassifier__weights=weights, kneighborsclassifier__n_neighbors=n_neighbors)","29354edc":"model = DecisionTreeClassifier(random_state=random_state)\npl = make_pipeline(remove_columns, model)\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [None, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nmin_samples_leaf = [5, 6, 7, 8, 9]\nccp_alpha = [0.0, 0.1, 0.2, 0.3, 0.4]\n\ndecision_tree_clf = utils.optimize_params(pl,\n                                          train_X, train_y, cv,\n                                          scoring=\"recall\",\n                                          decisiontreeclassifier__criterion=criterion,\n                                          decisiontreeclassifier__max_depth=max_depth,\n                                          decisiontreeclassifier__min_samples_leaf=min_samples_leaf,\n                                          decisiontreeclassifier__ccp_alpha=ccp_alpha)\n\n","4cad4088":"base_estimator = DecisionTreeClassifier(random_state=random_state)\n\nmodel = AdaBoostClassifier(random_state=random_state, base_estimator=base_estimator)\n\npl = make_pipeline(remove_columns, model)\n\nlearning_rate = [0.95, 1.0]\nn_estimators = [50, 75]\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2, 3]\nccp_alpha = [0.0, 0.1]\n\nadaboost_clf = utils.optimize_params(pl,\n                                     train_X, train_y, cv,\n                                     scoring=\"recall\",\n                                     adaboostclassifier__learning_rate=learning_rate,\n                                     adaboostclassifier__n_estimators=n_estimators,\n                                     adaboostclassifier__base_estimator__criterion=criterion,\n                                     adaboostclassifier__base_estimator__max_depth=max_depth,\n                                     adaboostclassifier__base_estimator__ccp_alpha=ccp_alpha)","33107232":"base_estimator = DecisionTreeClassifier(random_state=random_state)\n\nmodel = BaggingClassifier(random_state=random_state, base_estimator=base_estimator)\n\npl = make_pipeline(remove_columns, model)\n\nn_estimators = [100, 250, 500]\ncriterion = [\"gini\", \"entropy\"]\n\nbagging_clf = utils.optimize_params(pl,\n                                    train_X, train_y, cv,\n                                    scoring=\"recall\",\n                                    baggingclassifier__n_estimators=n_estimators,\n                                    baggingclassifier__base_estimator__criterion=criterion)","17f01af0":"model = RandomForestClassifier(random_state=random_state)\n\npl = make_pipeline(remove_columns, model)\nn_estimators = [100, 150]\ncriterion = [\"gini\", \"entropy\"]\nmax_features = [\"sqrt\", \"log2\"]\n\nrandom_forest_clf = utils.optimize_params(pl,\n                                          train_X, train_y, cv,\n                                          scoring=\"recall\",\n                                          randomforestclassifier__n_estimators=n_estimators,\n                                          randomforestclassifier__criterion=criterion,\n                                          randomforestclassifier__max_features=max_features)","87f4dbd7":"model = GradientBoostingClassifier(random_state=random_state)\n\npl = make_pipeline(remove_columns, model)\n\nlearning_rate = [0.01, 0.05, 0.1]\nn_estimators = [100, 200]\ncriterion = [\"friedman_mse\", \"mse\"]\nmax_depth = [1, 2, 3]\nccp_alpha = [0.0, 0.1]\n\ngradient_boosting_clf = utils.optimize_params(pl,\n                                          train_X, train_y, cv,\n                                          scoring=\"recall\",\n                                          gradientboostingclassifier__learning_rate=learning_rate,\n                                          gradientboostingclassifier__n_estimators=n_estimators,\n                                          gradientboostingclassifier__criterion=criterion,\n                                          gradientboostingclassifier__max_depth=max_depth,\n                                          gradientboostingclassifier__ccp_alpha=ccp_alpha)","5bf4d5fe":"model = HistGradientBoostingClassifier(random_state=random_state)\n\npl = make_pipeline(remove_columns, model)\n\nlearning_rate = [0.01, 0.02, 0.03, 0.04, 0.05]\nmax_leaf_nodes = [15, 31, 65, 127]\n\nhist_gradient_boosting_clf = utils.optimize_params(pl,\n                                                   train_X, train_y, cv,\n                                                   scoring=\"recall\",\n                                                   histgradientboostingclassifier__learning_rate=learning_rate,\n                                                   histgradientboostingclassifier__max_leaf_nodes=max_leaf_nodes)","af161db8":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf,\n    \"Decision tree\": decision_tree_clf,\n    \"AdaBoost\": adaboost_clf,\n    \"Bagging\": bagging_clf,\n    \"Random Forests\": random_forest_clf,\n    \"Gradient Boosting\": gradient_boosting_clf,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf\n}","8dd379cd":"utils.evaluate_estimators(estimators, test_X, test_y)","41e6b84b":"filepath = \"..\/input\/titanic\/train.csv\"\n\ntarget = \"Survived\"\n\ndata = utils.load_data(filepath, \"PassengerId\", target)\n\ntrain, test = train_test_split(data, train_size=0.7, stratify=data[target], random_state=random_state)\n\ntrain_X, train_y = utils.divide_dataset(train, target)\ntest_X, test_y = utils.divide_dataset(test, target)\n\ntrain.sample(5)","ed8d95af":"moda_imp = SimpleImputer(strategy='most_frequent')\nremove_columns = ColumnTransformer([(\"\", \"drop\", [\"Name\", \"Cabin\", \"Ticket\"]),('moda_inp', moda_imp, [\"Age\", \"Embarked\"])], remainder=\"passthrough\")\n\nencoder = OneHotEncoder(handle_unknown='ignore')\n\nnormalizer = Normalizer()","a6369f88":"cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=random_state)","7d18d977":"model = KNeighborsClassifier()\npl = make_pipeline(remove_columns, encoder, normalizer, model)\nn_neighbors = np.arange(1, 16)\nweights = [\"uniform\", \"distance\"]\n\nk_neighbors_clf = utils.optimize_params(pl, train_X, train_y, cv=cv, scoring=\"recall\", kneighborsclassifier__weights=weights, kneighborsclassifier__n_neighbors=n_neighbors)","2f08cd92":"model = DecisionTreeClassifier(random_state=random_state)\npl = make_pipeline(remove_columns, encoder, model)\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [None, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nmin_samples_leaf = [5, 6, 7, 8, 9]\nccp_alpha = [0.0, 0.1, 0.2, 0.3, 0.4]\n\ndecision_tree_clf = utils.optimize_params(pl,\n                                          train_X, train_y, cv,\n                                          scoring=\"recall\",\n                                          decisiontreeclassifier__criterion=criterion,\n                                          decisiontreeclassifier__max_depth=max_depth,\n                                          decisiontreeclassifier__min_samples_leaf=min_samples_leaf,\n                                          decisiontreeclassifier__ccp_alpha=ccp_alpha)\n","ea90cf8d":"base_estimator = DecisionTreeClassifier(random_state=random_state)\n\nmodel = AdaBoostClassifier(random_state=random_state, base_estimator=base_estimator)\n\npl = make_pipeline(remove_columns, encoder, model)\n\nlearning_rate = [0.95, 1.0]\nn_estimators = [50, 75]\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2, 3]\nccp_alpha = [0.0, 0.1]\n\nadaboost_clf = utils.optimize_params(pl,\n                                     train_X, train_y, cv,\n                                     scoring=\"recall\",\n                                     adaboostclassifier__learning_rate=learning_rate,\n                                     adaboostclassifier__n_estimators=n_estimators,\n                                     adaboostclassifier__base_estimator__criterion=criterion,\n                                     adaboostclassifier__base_estimator__max_depth=max_depth,\n                                     adaboostclassifier__base_estimator__ccp_alpha=ccp_alpha)","7a649e99":"base_estimator = DecisionTreeClassifier(random_state=random_state)\n\nmodel = BaggingClassifier(random_state=random_state, base_estimator=base_estimator)\n\npl = make_pipeline(remove_columns, encoder, model)\n\nn_estimators = [100, 250, 500]\ncriterion = [\"gini\", \"entropy\"]\n\nbagging_clf = utils.optimize_params(pl,\n                                    train_X, train_y, cv,\n                                    scoring=\"recall\",\n                                    baggingclassifier__n_estimators=n_estimators,\n                                    baggingclassifier__base_estimator__criterion=criterion)","bfa24ac5":"model = RandomForestClassifier(random_state=random_state)\n\npl = make_pipeline(remove_columns, encoder, model)\nn_estimators = [100, 150]\ncriterion = [\"gini\", \"entropy\"]\nmax_features = [\"sqrt\", \"log2\"]\n\nrandom_forest_clf = utils.optimize_params(pl,\n                                          train_X, train_y, cv,\n                                          scoring=\"recall\",\n                                          randomforestclassifier__n_estimators=n_estimators,\n                                          randomforestclassifier__criterion=criterion,\n                                          randomforestclassifier__max_features=max_features)","55900e20":"model = GradientBoostingClassifier(random_state=random_state)\n\npl = make_pipeline(remove_columns, encoder, model)\n\nlearning_rate = [0.01, 0.05, 0.1]\nn_estimators = [100, 200]\ncriterion = [\"friedman_mse\", \"mse\"]\nmax_depth = [1, 2, 3]\nccp_alpha = [0.0, 0.1]\n\ngradient_boosting_clf = utils.optimize_params(pl,\n                                          train_X, train_y, cv,\n                                          scoring=\"recall\",\n                                          gradientboostingclassifier__learning_rate=learning_rate,\n                                          gradientboostingclassifier__n_estimators=n_estimators,\n                                          gradientboostingclassifier__criterion=criterion,\n                                          gradientboostingclassifier__max_depth=max_depth,\n                                          gradientboostingclassifier__ccp_alpha=ccp_alpha)","e410dab7":"model = HistGradientBoostingClassifier(random_state=random_state)\n\nencoder_d = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\npl = make_pipeline(remove_columns, encoder_d, model)\n\nlearning_rate = [0.01, 0.02, 0.03, 0.04, 0.05]\nmax_leaf_nodes = [15, 31, 65, 127]\n\nhist_gradient_boosting_clf = utils.optimize_params(pl,\n                                                   train_X, train_y, cv,\n                                                   scoring=\"recall\",\n                                                   histgradientboostingclassifier__learning_rate=learning_rate,\n                                                   histgradientboostingclassifier__max_leaf_nodes=max_leaf_nodes)","8c1f6794":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf,\n    \"Decision tree\": decision_tree_clf,\n    \"AdaBoost\": adaboost_clf,\n    \"Bagging\": bagging_clf,\n    \"Random Forests\": random_forest_clf,\n    \"Gradient Boosting\": gradient_boosting_clf,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf\n}","92614030":"utils.evaluate_estimators(estimators, test_X, test_y)","5d06bc53":"import numpy as np \nimport pandas as pd \nimport sklearn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\n\n#Preprocesamiento de datos\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n#Creaci\u00f3n de modelos y b\u00fasqueda de hiperpar\u00e1metros\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.pipeline import make_pipeline, Pipeline\n\n#Validaci\u00f3n y visualizaci\u00f3n de m\u00e9tricas\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, precision_score, recall_score, auc\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\n\nimport utilidad_grupor as utils","3ba37566":"seed = 27912","496e7563":"df = pd.read_csv('..\/input\/mushroom-classification\/mushrooms.csv')\ndataset = df.values\ndf.sample(5, random_state=seed)","eec8a93e":"df.shape","3183c4aa":"df.isnull().sum()","545e6d37":"names = list(df.columns)\nx = df[names[1:]]\ny = df['class']","8a6aa55d":"#a\u00f1adimos la seed al random state para evitar sesgo y que los experimentos sean reproducibles\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=seed, stratify=y)","69c475d9":"train = utils.join_dataset(x_train, y_train)\ntest = utils.join_dataset(x_test, y_test)","a4e366f2":"test.shape","331c860c":"# Arreglados Data Leaks\n\ncolors = ('#EF8787','#9CF29C')\npalette = sns.set_palette(sns.color_palette(colors))\n\nutils.plot_class_distribution(train, 'class', [\"Venenoso\", \"Comestible\"], colors, 'Mushroom Class Distribution')","3dd369e5":"# Arreglados Data Leaks\n\nutils.plot_feature_distribution(train, 'class', train.columns, palette)","c0a6bc7b":"# Arreglados Data Leaks\n\nlabelencoder=LabelEncoder()\ntrain_enc = train.copy()\nfor column in train.columns:\n    train_enc[column] = labelencoder.fit_transform(train[column])","60b8c609":"plt.figure(figsize=(16,16))\nsns.heatmap(train_enc.corr(),linewidths=.1,annot=True, cmap=\"magma\")","3afe0030":"#A\u00f1adimos pipeline para preprocesamiento \n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\n\npreproc = ColumnTransformer([(\"\", \"drop\", [\"bruises\",\"gill-color\",\"veil-type\"])], remainder=\"passthrough\")","e4ce9f6b":"models = ['LogisticRegression','NaiveBayes','RandomForest','KNearestNeighbors']\n\nscores = [None] * len(models)","790800f6":"from sklearn.model_selection import cross_val_score\n\nlr = make_pipeline(preproc, OneHotEncoder(drop='first'), LogisticRegression())\nlr = lr.fit(x_train, y_train)\nlr","b63fd867":"y_pred = lr.predict(x_test)\ny_pred","ce9e93ee":"accuracy = lr.score(x_test, y_test)\n\nutils.show_results(models, scores, x_test, y_test, lr, y_pred,\"LogisticRegression\", 'p')","0907b94c":"score_list = cross_val_score(lr,x_train,y_train, cv=10)\nscore = np.mean(score_list)\nprint (score)\n\n# we swap the score obtained before with the cross_val_score\nscores[0] = score","060983f9":"nb = make_pipeline(preproc, OneHotEncoder(drop='first', sparse=False),GaussianNB())\nnb = nb.fit(x_train, y_train)\npreds= nb.predict(x_test)\nutils.show_results(models, scores, x_test, y_test, nb, preds,\"NaiveBayes\", pos_label='p')","327d8a4a":"score_list = cross_val_score(nb,x_train,y_train, cv=5)\nprint(score)\nscore = np.mean(score_list)\n# we swap the score obtained before with the cross_val_score\nscores[1] = score","affbfbb3":"print(score_list)","b3582dd2":"cv_split = TimeSeriesSplit(n_splits=5)","fbc461cb":"rf = RandomForestClassifier(random_state=1)\nrf_params = {\n    'model__n_estimators': list(range(25,251,25)),\n    'model__max_features': list(np.arange(0.1,0.36,0.05))\n}\nrf_pipe = Pipeline([\n    ('prep', preproc),\n    ('encod', OneHotEncoder(sparse=False, handle_unknown = 'ignore')),\n    ('scale', StandardScaler()),\n    ('model', rf)\n])\ngridsearch_rf = GridSearchCV(estimator=rf_pipe,\n                          param_grid = rf_params,\n                          cv = cv_split,\n                         )\ngridsearch_rf.fit(x_train, y_train)","f1d231fc":"rf_best_model = gridsearch_rf.best_estimator_\npreds = rf_best_model.predict(x_test)\nutils.show_results(models, scores, x_test, y_test, rf_best_model, preds,'RandomForest', pos_label='p')","edd3ba46":"knn = make_pipeline(preproc, OneHotEncoder(handle_unknown='ignore'),KNeighborsClassifier())\nknn_params = {\n    'kneighborsclassifier__n_neighbors': list(range(4,10)),\n    'kneighborsclassifier__weights': ['uniform','distance']\n}\n\ngridsearch_knn = GridSearchCV(knn,\n                          param_grid = knn_params,\n                          cv = cv_split,\n                         )\ngridsearch_knn.fit(x_train, y_train)","f5988325":"knn_best_model = gridsearch_knn.best_estimator_\npreds = knn_best_model.predict(x_test)\nutils.show_results(models, scores, x_test, y_test, knn_best_model, preds,'KNearestNeighbors', pos_label='p')","e6b97d51":"utils.plot_accuracy(models, scores)","8f21987b":"palette = sns.set_palette(sns.color_palette('Set1')) #just to define de plot palette\n\nfor i in range(0,5):\n    random.seed(i)\n    randlist = list(names[x] for x in random.sample(range(1,21),k=5))\n    rand_df = train[randlist]\n    rand_df = pd.get_dummies(rand_df)\n\n    utils.plot_roc(rand_df,y_train)\n    \nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n\nplt.gcf().set_size_inches(15,10)","35a6459c":"## 5.1. \u00c1rbol de decisi\u00f3n","14e9a42a":"Nuevamente podemos ver c\u00f3mo despu\u00e9s de usar la validaci\u00f3n cruzada, nuestra precisi\u00f3n ha disminuido esta vez hasta casi un 85%.","3775fd34":"Este algoritmo es considerado perozoso puesto que computa los par\u00e1metros necesarios para la clasificaci\u00f3n durante inferencia. La clasificaci\u00f3n de este modelo consistir\u00e1 en asignar a la instancia de entrada la clase mayoritaria de los vecinos m\u00e1s cercanos.\n\nEncontramos 2 principales par\u00e1metros a la hora de definir nuestro modelo de vecinos mas cercanos, el n\u00famero de vecinos `n_neighbours` que se dejar\u00e1 por defecto en un valor moderado como puede ser 5, y los pesos `weights`, donde consideraremos que todos los vecinos tienen la misma importancia, y por tanto, el mismo peso. Dejaremos los valores de esta variable por defecto.\n\nPor \u00faltimo, al crear el **Pipeline** de este algoritmo, deberemos pasarle las variables normalizadas como hemos definido previamente en el preprocesamiento de datos, de esta manera escalaremos las variables predictoras en proporciones similares que ser\u00e1n tratadas por los pesos.","6f79542f":"Por \u00faltimo, compararemos los resultados de cada modelo con los hiperpar\u00e1metros aprendidos y entrenados con la totalidad del conjunto de entrenamiento, contra el conjunto de test que hab\u00edamos reservado al principio.","d284363a":"En esta ocasi\u00f3n el mejor criterio es `gini` y parece que con 100 estimadores se consigue mejor resultado que con 150, aunque la diferencia no es muy significativa.","b528475d":"Adem\u00e1s de usar estas dos m\u00e9tricas, emplearemos el m\u00e9todo refit que nos servir\u00e1 para elegir cu\u00e1l es el mejor modelo una vez acabada la b\u00fasqueda **Grid** y volver a entrenar con \u00e9l tratando de mejorar, en este caso, su **recall**.","21c7497f":"Los algoritmos basados en inducci\u00f3n de \u00e1rboles de decisi\u00f3n representan los modelos mediante un conjunto de reglas.\n\nLos hiperpar\u00e1metros que a nuestro parecer resultan m\u00e1s interesantes para este modelo ser\u00e1n `max_depth`, que tomar\u00e1 un valor de 3 y se referir\u00e1 a la profundidad del arbol (evitaremos sobreajuste con este valor), y `min_samples_split`, que tomar\u00e1 un valor de 20, lo que significa que las hojas deber\u00e1n tener un m\u00ednimo de 20 instancias para evitar, de nuevo, un posible sobreajuste.\n\nEs necesario comentar que posteriormente, en el apartado de evaluaci\u00f3n de modelos, estudiaremos de mejor forma estos hiperpar\u00e1metros.","94815870":"Una vez se ha realizado la b\u00fasqueda **Grid**, vemos que el modelo que nos proporciona mejores resultados cuenta con un `ccp_alpha` de 0, lo que nos indica que una posible post poda no ayudar\u00eda a mejorar el modelo. Adem\u00e1s vemos que el criterio elegido es el de entrop\u00eda que trata de maximizar la ganancia de informaci\u00f3n, una profundidad del \u00e1rbol de 3, y un n\u00famero m\u00ednimo de ejemplos por nodo hoja de 10.","9e5f1215":"## 5.6. Gradient Tree Boosting (Gradient Boosting)","93676b23":"Nuestra variable clase, como vemos, se llama `class`. Lo que haremos ser\u00e1 separar esta variable de las variables predictoras","83867601":"Podemos apreciar as\u00ed, como distintas variables predictoras nos dar\u00e1n mas informaci\u00f3n que otras. Podemos ver tambi\u00e9n que solo hay un tipo de **`veil-type`** en esa variable, por lo que en el posterior **preprocesamiento de datos** que llevemos a cabo, eliminaremos esa variable","6e7377f9":"### B\u00fasqueda Grid\nAhora separamos los datos en carpetas para la b\u00fasqueda **Grid**","8a407302":"Una vez podamos a\u00f1adir al **Pipeline** el preprocesamiento de datos obtenido, podemos pasar a tratar los modelos de clasificaci\u00f3n para nuestro conjunto de datos.","31f2ca66":"### 2.1.1. Eliminaci\u00f3n de variables","4acb49d9":"Un Random Forest es un conjunto (ensemble) de \u00e1rboles de decisi\u00f3n combinados con bagging. Al usar bagging, lo que en realidad est\u00e1 pasando, es que distintos \u00e1rboles ven distintas porciones de los datos. Ning\u00fan \u00e1rbol ve todos los datos de entrenamiento. Esto hace que cada \u00e1rbol se entrene con distintas muestras de datos para un mismo problema. De esta forma, al combinar sus resultados, unos errores se compensan con otros y tenemos una predicci\u00f3n que generaliza mejor.","d8deca37":"Comprobaremos que se hayan separado correctamente:","13ec17e7":"Cargamos el conjunto de datos y visualizamos sus elementos","47091a45":"---","03051af6":"## 3.4. Bootstrap Aggregating (Bagging)\n\nPara el caso del *bagging* aprovecharemos los hiperpar\u00e1metros por defecto (queremos muestreo con reemplazo y la totalidad de las caracter\u00edsticas), y solo modificaremos el n\u00famero de clasificadores (`n_estimators`) y el criterio del \u00e1rbol de decisi\u00f3n usado como base (`criterion`), que esta vez interesa que sea profundo por lo que mantendremos los par\u00e1metros relacionados con la prepoda (`max_depth`, `min_samples_leaf`, etc) o pospoda (`ccp_alpha`) en su valor por defecto.\n\nVariaremos en este caso solamente:\n* `n_estimators`\n* `criterion` (del \u00e1rbol de decisi\u00f3n usado como `base_estimator`)","763f7157":"El an\u00e1lisis exploratorio de datos es un paso fundamental a la hora de comprender los datos con los que vamos a trabajar.\n\nEl objetivo de este an\u00e1lisis es explorar, describir y visualizar la naturaleza de los datos recogidos mediante la aplicaci\u00f3n de t\u00e9cnicas simples de resumen de datos y m\u00e9todos gr\u00e1ficos, para observar las posibles relaciones entre las variables de nuestro conjunto de datos.","99813bb7":"# 3. Modelos de clasificaci\u00f3n supervisada","ac5c12e3":"## 3.6. Gradient Tree Boosting (Gradient Boosting)","0d8b5977":"# 2. Preprocesamiento\n\nEl preprocesamiento que haremos ser\u00e1 eliminar las variables `Name`, `Cabin` y `Ticket`, ya que se trata de variables categ\u00f3ricas\/discretas que toman valores distintos para cada una de las instancias o con valores perdidos, por lo que poseen poco valor predictivo; adem\u00e1s de imputar los valores perdidos de `age` y `embarked` por la moda. Tambi\u00e9n deberemos codificar los valores categ\u00f3ricos para poder trabajar con ellos.","9b4c1f64":"# 2. Carga de datos","3a45f247":"## 3.4. Adaptative Boosting (*AdaBoost*)","5da386b7":"# 1. Preliminares","c7c37e7e":"Este \u00faltimo algoritmo se diferencia del anterior en que se reduce el n\u00famero de umbrales que se prueban para hacer las particiones mediante el uso de histogramas, lo que lo hace bastante m\u00e1s r\u00e1pido. En esta ocasi\u00f3n estudiaremos los par\u00e1metros:\n\n* `learning_rate`: la tasa de aprendizaje. En esta ocasi\u00f3n es viable usar valores m\u00e1s bajos debido al menor tiempo de entrenamiento con respecto a *Gradient Boosting*.\n\n* `max_leaf_nodes`: esta vez usaremos el m\u00e1ximo n\u00famero de nodos hoja: otra forma de controlar el tama\u00f1o de los \u00e1rboles en lugar de por su profundidad m\u00e1xima.","8c04d9b8":"## 3.7. Gradient Tree Boosting (*Gradient Boosting*)","b31e426c":"# 1. Preliminares\n\nLo primero que haremos ser\u00e1 cargar los datos y preparar las particiones de entrenamiento y validaci\u00f3n.","09aecd38":"## 3.3. Adaptative Boosting (AdaBoost)","03b6e2ca":"En esta selecci\u00f3n de modelos trataremos de optimizar los par\u00e1metros que consideramos son m\u00e1s relevantes en Histogram Gradient Boosting.\n\nLos hiperpar\u00e1metros elegidos para ser optimizados han sido:\n* `min_samples_leaf`: M\u00ednimo n\u00famero de instancias para poder tener en cuenta un nodo hoja, lo cual afecta al suavizado del modelo que sea elegido. \n* `learning_rate`: Tasa de aprendizaje usada como factor multiplicativo para los nodos hoja. Toma un valor por defecto de 0.1.\n","ec84e422":"## 3.5. Random Forests\n\nConsideraremos los hiperpar\u00e1metros:\n\n* `criterion`: `gini` o `entropy`.\n\n* `max_features`: $\\sqrt{\\mathit{n\\_features}}$ o $log_2(\\mathit{n\\_features})$.\n\n* `n_estimators`: el n\u00famero de clasificadores base.","584cfa17":"Como nuestro conjunto de datos est\u00e1 formado por variables categ\u00f3ricas, para trabajar con ellos debemos codificarlos. A la hora de realizar este proceso, debemos tener en cuenta que para algunos modelos, codificar los valores directamente como n\u00fameros pueden crear sesgo hacia las variables con mayor valor.\n\nPara evitar eso, haremos uso de `OneHotEncoder`","9e1e0ab0":"Hemos visto c\u00f3mo todos los modelos probados pueden obtener altas precisiones y lo poco que var\u00edan las precisiones entre los diferentes par\u00e1metros (ilustrado con el ejemplo KNN).\n\nPara comprender finalmente lo f\u00e1cil que podemos lograr altas precisiones, trazaremos varias curvas ROC para determinados subgrupos de caracter\u00edsticas aleatorias.","a8f6a463":"Para el caso del *bagging* aprovecharemos los hiperpar\u00e1metros por defecto (queremos muestreo con reemplazo y la totalidad de las caracter\u00edsticas), y solo modificaremos el n\u00famero de clasificadores (`n_estimators`) y el criterio del \u00e1rbol de decisi\u00f3n usado como base (`criterion`), que esta vez interesa que sea profundo por lo que mantendremos los par\u00e1metros relacionados con la prepoda (`max_depth`, `min_samples_leaf`, etc) o pospoda (`ccp_alpha`) en su valor por defecto.\n","f0b43cb4":"El modelo seleccionado utilizar\u00e1 un 50% de la instancias para entrenar los \u00e1rboles, junto con un n\u00famero m\u00e1ximo de variables guiado por `sqrt`, y haciendo uso de un criterio basado en el \u00edndice `gini`, garantizando as\u00ed un modelo con un recall y una accuracy adecuados para nuestro problema.","2b15a018":"La tasa de aprendizaje (learning rate), se ha dejado a 0.025, mientras que la profundidad del \u00e1rbol parece no ser demasiado importante. Puesto que toma un valor de 3, podemos extraer que \u00e1rboles m\u00e1s peque\u00f1os trabajan mejor con nuestro algoritmos que \u00e1rboles que puedan tener una mayor profundidad y por tanto un mayor sobreajuste con nuestro conjunto de entrenamiento.","58aa9559":"Tanto KNN como RandomForests nos dar\u00e1n una precisi\u00f3n casi perfecta. Incluso si las caracter\u00edsticas del conjunto de datos permiten tener f\u00e1cilmente altas precisiones.\n\nSiempre es importante procesar correctamente los datos y ajustar los modelos correctamente, entendiendo lo que est\u00e1 haciendo el programa en lugar de solo enfocarse en obtener mejores m\u00e9tricas.\n\nAl hacer esto, sabremos si nuestros scores son correctos o si estamos haciendo algo mal.","5194d887":"Como podemos apreciar, RandomForest y KNN son los que nos proporcionan una mayor precisi\u00f3n cuando los enfrentamos contra el conjunto de test que reservamos previamente.","b29f3fc2":"Una vez tenemos bien definidos nuestros conjuntos de entrenamiento y prueba, podemos aplicar el **preprocesamiento de datos** definido en la pr\u00e1ctica anterior.","fef9117a":"Si nos fijamos en los mejores modelos, encontramos que la tasa de aprendizaje preferible de entre las probadas es $0.05$. Adem\u00e1s, `n_estimators`=$100$ funciona ligeramente mejor que $200$ extra\u00f1amente. Ambos criterios de calidad de las particiones probados no presentan una diferencia significativa en los resultados, con una ligera ventaja de `friedman_mse`. Los \u00e1rboles que mejor han funcionado han sido los de profundidad $3$ sin pospoda (`ccp_alpha` $=0$).","01c12fbf":"# 3. An\u00e1lisis exploratorio de los datos","d0742bd7":"# 3. Selecci\u00f3n de modelos\n\nLos algoritmos que probaremos son:\n\n* Vecinos m\u00e1s cercanos\n\n* \u00c1rbol de decisi\u00f3n\n\n* AdaBoost\n\n* Bagging\n\n* Random Forests\n\n* Gradient Boosting\n\n* Histogram Gradient Boosting\n\nUsaremos una $10\\times 5$ validaci\u00f3n cruzada.","25f4a932":"En esta selecci\u00f3n de modelos trataremos de optimizar los par\u00e1metros que consideramos son m\u00e1s relevantes en AdaBoost.\n\nLos hiperpar\u00e1metros elegidos para ser optimizados han sido:\n* `criterion`: Funci\u00f3n para medir la calidad de las particiones del \u00e1rbol. Usaremos `entropy` para la ganancia de informaci\u00f3n y el \u00edndice `gini`.\n* `learning_rate`: Nos indica la contribuci\u00f3n de los estimadores basada en el valor de esta variable, por defecto a 1.\n* `min_samples_split`: M\u00ednimo n\u00famero de instancias requeridas para dividir un nodo interno, este hiperpar\u00e1metro ser\u00e1 usado por los \u00e1rboles que conforman nuestro modelo de AdaBoost.","4f39ffe7":"En esta selecci\u00f3n de modelos trataremos de optimizar los par\u00e1metros que consideramos son m\u00e1s relevantes en Bagging.\n\nLos hiperpar\u00e1metros elegidos para ser optimizados han sido:\n* `max_samples`: N\u00famero de instancias del conjunto de entrenamiento necesarias para entrenar cada estimador.\n* `max_features`: N\u00famero de variables predictoras de nuestro conjunto de entrenamiento necesarias para entrenar cada estimador.\n\nAmbas variables toman por defecto el valor 1.","440f76ba":"## 5.3. Adaptative Boosting (AdaBoost)","37520a6a":"## 5.7. Histogram-Based Gradient Boosting (Histogram Gradient Boosting)\n\n","eac2675b":"Vamos a configurar nuestro \u00e1rbol de decisi\u00f3n con lo mencionado anteriormente:","c0e0bac1":"Hemos obtenido un 100% de precisi\u00f3n, por lo que podr\u00edamos estar teniendo un problema de sobreajuste. Para evaluar mejor los modelos de ahora en adelante, usaremos validaci\u00f3n cruzada. Al hacerlo, es probable que obtengamos peores resultados, pero estos ser\u00e1n m\u00e1s fiables.","19c413d9":"Para este algoritmo probaremos distintas opciones para los hiperpar\u00e1metros:\n\n* `criterion`: el criterio para hacer las particiones (`gini` o `entropy`).\n\n* `max_depth`: la profundidad m\u00e1xima del \u00e1rbol (sin limitar o de $2$ a $10$).\n\n* `min_samples_leaf`: el n\u00famero m\u00ednimo de ejemplos por hoja. Probaremos a partir de $5$ para evitar sobreajuste.\n\n* `ccp_alpha`: par\u00e1metro para controlar la pospoda del \u00e1rbol.","226acb89":"## 4.2. Codificaci\u00f3n de los datos","3db11cd1":"A diferencia del resto, el algoritmo no admite matrices dispersas como input as\u00ed que definiremos un codificador _OneHot_ distinto.","3c0d0f22":"Comenzamos cargando el conjunto de datos `wisconsin`:","d0cbb759":"Para realizar este proceso, haremos uso de un **Pipeline**. Este Pipeline ser\u00e1 el encargado de aplicar las transformaciones que hemos decidido a nuestro conjunto de datos.\n\nEliminaremos las siguientes columnas (variables):\n* Aquellas relacionadas con los `concave points`: `concavity_mean, compactness_mean, concavity_se, compactness_se, concavity_worst y compactness_worst`.\n* Aquellas relacionadas con `radius`, es decir: `area_mean, perimeter_mean, area_se, perimeter_se, area_worst y perimeter_worst`.\n\nPorque son variables que dependen directamente de las 2 que vamos a dejar en nuestro conjunto de datos: `radius y concave points`.","eba8b5d2":"# 4. Preprocesamiento de los datos","ff617912":"## 4.2. Vecinos m\u00e1s cercanos","1d8449e4":"##\u00a03.2. \u00c1rboles de decisi\u00f3n","f55ce5aa":"## 3.7. Histogram-Based Gradient Boosting (Histogram Gradient Boosting)\n\n","3840d7b0":"De esta manera, podremos apreciar los valores de los hiperpar\u00e1metros que optimizan las m\u00e9tricas del modelo seleccionado, en este caso `max_features = 0.75` y `max_samples = 0.5`","60622152":"Como podemos observar, el problema est\u00e1 cerca de ser balanceado, pero encontramos alg\u00fan ejemplo m\u00e1s de setas venenosas que comestibles","1d56737a":"Comenzemos mostrando la distribuci\u00f3n de la clase","df7a27e3":"## 3.7. Histogram-Based Gradient Boosting (Histogram Gradient Boosting)\n\nUsaremos los hiperpar\u00e1metros:\n\n* `learning_rate`: la tasa de aprendizaje. En esta ocasi\u00f3n es viable usar valores m\u00e1s bajos debido al menor tiempo de entrenamiento con respecto a *Gradient Boosting*.\n\n* `max_leaf_nodes`: esta vez usaremos el m\u00e1ximo n\u00famero de nodos hoja: otra forma de controlar el tama\u00f1o de los \u00e1rboles en lugar de por su profundidad m\u00e1xima.","5af34555":"## 5.2. Vecinos m\u00e1s cercanos","dd2d849f":"Para este algoritmo, tendremos en cuenta al igual que con el resto de *ensembles* el n\u00famero de clasificadores y la tasa de aprendizaje. Para los \u00e1rboles usados como base intentaremos ajustar la profundidad m\u00e1xima y el par\u00e1metro de complejidad para la poda. Al estar hablando ahora de \u00e1rboles de regresi\u00f3n en lugar de clasificaci\u00f3n usaremos como criterio el error cuadrado medio con y sin la mejora de Friedman. Los par\u00e1metros del algoritmo que variaremos, por tanto, son:\n\n* `criterion`\n* `n_estimators`\n* `learning_rate`\n* `max_depth`\n* `ccp_alpha`","a689c4a4":"Despu\u00e9s de esta b\u00fasqueda **grid** podemos pensar que independientemente de la b\u00fasqueda de hiperpar\u00e1metros y el ajuste de las variables el modelo es propenso a obtener precisiones muy altas.","7998972f":"De acuerdo con estos resultados, podemos concluir que los **\u00e1rboles de decisi\u00f3n** y los ensembles como **Random Forest, Gradient Boosting** e **Histogram Gradient Boosting** son los clasificadores que nos ofrecen un mejor rendimiento en t\u00e9rminos de **recall** en nuestro conjunto de datos `wisconsin`.\n\nNo obstante, pese a centrarnos en el **accuracy** y sobre todo en el **recall**, es importante destacar y tener en cuenta otros factores como el tiempo de aprendizaje y de inferencia.\n\nSi bien los ensembles mencionados anteriormente mejoran ligeramente el recall y la precisi\u00f3n cuando los enfrentamos contra el conjunto de test, los **\u00e1rboles de decisi\u00f3n** tienen un tiempo de aprendizaje e inferencia mucho menor que el resto de modelos de ensembles estudiados.\n\nPor ello, antes de decantarnos por un modelo u otro para nuestro problema, debemos tener en cuenta si el aumento de coste computacional necesario para emplear un modelo m\u00e1s complejo compensa la ligera mejora que este supone frente a modelos m\u00e1s sencillos.\n\nEn el caso del problema que estudiamos, al ser una mejor\u00eda tan liviana, no nos resulta necesario aumentar el tiempo de aprendizaje e inferencia de ensembles como Random Forest, Gradient Boosting e Histogram Gradient Boosting, por lo que el modelo seleccionado finalmente han sido los **\u00c1rboles de Decisi\u00f3n**.","4f9c8212":"#\u00a02. Carga de datos","f2a95aab":"---","6397d6f1":"Como podemos observar, en t\u00e9rminos de *recall*, los modelos que proporcionan mejores resultados para este problema son el \u00e1rbol de decisi\u00f3n seguido de lejos por AdaBoost, Bagging y Random Forests; y el peor el Vecinos m\u00e1s cercanos. Si nos fijamos en otras m\u00e9tricas como el tiempo necesario para entrenamiento e inferencia, el \u00e1rbol de decisi\u00f3n vuelve a destacar.\n\nAnte estos resultados, debemos priorizar el **\u00e1rbol de decisi\u00f3n** al conseguir un buen resultado y adem\u00e1s ser m\u00e1s simple.","306d6bfa":"## 3.1. Vecinos m\u00e1s cercanos\n\nPara este algoritmo variaremos los hiperpar\u00e1metros:\n\n* `n_neighbors`: la cantidad de ejemplos que se consideran para clasificar. Lo variaremos de $1$ a $15$\n* `weights`: el peso de cada ejemplo. Probaremos con pesos uniformes o proporcionales a la inversa de la distancia.\n\nTambi\u00e9n tendremos que normalizar las variables al ser un algoritmo basado en la distancia.","9bad706c":"# Breast Cancer Wisconsin","22d6edab":"## 4.7. Histogram-Based Gradient Boosting (Histogram Gradient Boosting)","724c2960":"En esta selecci\u00f3n de modelos trataremos de optimizar los par\u00e1metros que consideramos son m\u00e1s relevantes en \u00e1rboles de decisi\u00f3n.\n\nLos hiperpar\u00e1metros elegidos para ser optimizados han sido:\n* `max_depth`: Profundidades del \u00e1rbol que se estudiar\u00e1n para evitar un posible sobreajuste y garantizar un buen resultado.\n* `ccp_alpha`: Par\u00e1metros de complejidad usado para una posibilidad de poda posterior con el m\u00ednimo coste computacional\n* `criterion`: Funci\u00f3n para medir la calidad de las particiones del \u00e1rbol. Usaremos `entropy` para la ganancia de informaci\u00f3n y el \u00edndice `gini`.\n* `min_samples_leaf`: M\u00ednimo n\u00famero de instancias para poder tener en cuenta un nodo hoja, lo cual afecta al suavizado del modelo que sea elegido. ","62b1ed82":"Ahora cargaremos el conjunto de datos, que es el conjunto [Pima Indian Diabetes](https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database). Con el objetivo de evaluar los modelos que obtendremos finalmente, reservaremos un $30\\%$ de los ejemplos para realizar una validaci\u00f3n *holdout*.","061e75ab":"#\u00a05. Selecci\u00f3n de modelos","0e44dee2":"La estrategia seleccionada utiliza una distribuci\u00f3n de pesos uniforme, y un n\u00famero de vecinos muy peque\u00f1o: 3. Es interesante que \u00e9ste sea el n\u00famero de vecinos, puesto que es el menor de los proporcionados y hemos comentado que un peque\u00f1o n\u00famero de vecinos podr\u00eda causarnos sobreajuste.","ac5bc13c":"Veamos ahora una serie de diagramas de barras que nos relacionar\u00e1n la distribuci\u00f3n de cada variable predictora con sus posibles valores junto a la variable clase, y el n\u00famero de ejemplos de las mismas","36add05f":"# Mushroom Classification","2c83fe02":"##\u00a03.5. Bootstrap Aggregating (*Bagging*)","6bdbb41c":"Una vez que en el apartado anterior hemos obtenido mediante la b\u00fasqueda **grid** todos los modelos, podemos emplearlos contra el conjunto de test que reservamos al principio del documento para evaluar cu\u00e1l nos proporciona mejores resultados en las m\u00e9tricas elegidas.","a3d4c3fc":"#\u00a02. Preprocesamiento de datos","06dbead2":"## 4.6. Gradient Tree Boosting (Gradient Boosting)","80487d1a":"## 3.6. Random Forests","699c44dd":"Seguidamente, lo que haremos ser\u00e1 unir los conjuntos `X_train` e `y_train` para obtener el conjunto de datos de entrenamiento.\nHaremos los mismo para `X_test` e `y_test`, juntando as\u00ed el conjunto de datos de test.","cd6b9de4":"En esta etapa limpiaremos y organizaremos los datos de manera adecuada para entrenar a nuestro modelo bas\u00e1ndonos en las observaci\u00f3n que hemos realizado en el an\u00e1lisis exploratorio de datos previo. Por ello, en este conjunto de datos nos centraremos en la selecci\u00f3n de variables adecuadas para conseguir reducir el n\u00famero de estas.","36e1ddea":"## 3.1. Vecinos m\u00e1s cercanos","c8de1318":"Bagging es un ensemble cuya estrategia es utilizar una funci\u00f3n de aprendizaje y obtener modelos diversos entre s\u00ed para reducir el error obtenido mediante varianza. Mediante un voto de estos modelos se obtendr\u00e1 el ensemble.\n\nLos hiperpar\u00e1metros m\u00e1s relevantes a tener en cuenta ser\u00e1n `base_estimator`, que nos indica el estimador que utilizar\u00e1 el ensemble (en este caso un \u00c1rbol de decisi\u00f3n) o el n\u00famero de estimadores `n_estimators`. Para un muestreo con reemplazo, emplearemos el par\u00e1metro `bootstrap_features=True`.defecto, `random_state=None`).\n\nEs necesario comentar que posteriormente, en el apartado de evaluaci\u00f3n de modelos, estudiaremos de mejor forma estos hiperpar\u00e1metros.","bc6a1e85":"Lo primero de todo es cargar las librer\u00edas para que est\u00e9n disponibles posteriormente:","01bedaae":"Comprobamos que se ha borrado correctamente haciendo uso del m\u00e9todo `sample` de nuevo:","f8e96432":"## 3.6. Gradient Tree Boosting (Gradient Boosting)\n\nEstudiaremos los siguientes hiperpar\u00e1metros:\n\n* `criterion`\n* `n_estimators`\n* `learning_rate`\n* `max_depth`\n* `ccp_alpha`","108053a0":"Ser\u00e1 necesario **normalizar** el conjunto de datos a la hora de definir los modelos a emplear, de forma que todas las variables predictoras tengan el mismo peso.\n\nPara ello definiremos una variable `normalizacion` que haga uso de la funci\u00f3n `Normalizer()`, la cual normaliza individualmente los datos a una norma unitaria, justo lo que buscamos.","918b7119":"A la hora de evaluar los distintos modelos, utilizaremos una t\u00e9cnica conocida como **validaci\u00f3n cruzada**. En esta, se separa el conjunto de datos en `k` particiones y se repite `k` veces el proceso de aprendizaje y validaci\u00f3n, pero utilizando cada vez una combinaci\u00f3n \u00fanica de `k-1` muestras para entrenar y la restante para validar. De este modo, obtendremos unos valores fiables de sesgo empleando solamente el conjunto de entrenamiento.\n\nEn este caso vamos a evaluar nuestros clasificadores y el tipo de validaci\u00f3n cruzada a utilizar usando una `5*10-cv` estratificada:","17c530b7":"En esta pr\u00e1ctica estudiaremos los modelos m\u00e1s utilizados en `scikit-learn` para conocer los distintos hiperpar\u00e1metros que los configuran y estudiar los clasificadores resultantes. Adem\u00e1s, veremos m\u00e9todos de selecci\u00f3n de modelos orientados a obtener una configuraci\u00f3n \u00f3ptima de hiperpar\u00e1metros.","140534e2":"Mostramos el tama\u00f1o del conjunto de datos","20aab286":"Como podemos observar, tenemos una columna, la \u00faltima, cuyo nombre es `Unnamed` y todo el contenido de sus filas `NaN`. Esto es debido a que en la declaraci\u00f3n de las columnas en el archivo `csv` hay una `,` sobrante al final de la l\u00ednea, lo que hace que se cree una columna sin nombres y con valores inexistentes.\n\nEsto significa que antes de continuar trabajando con nuestro conjunto de datos, debemos borrar esa columna.","749bae32":"## 4.1. \u00c1rbol de decisi\u00f3n","49daa32a":"Y a continuaci\u00f3n la variable clase:","1f518fb7":"Ahora que somos capaces de evaluar correctamente los clasificadores, es importante decidir una estrategia que nos permita encontrar una configuraci\u00f3n \u00f3ptima de los hiperpar\u00e1metros. Para ello haremos uso de una b\u00fasqueda **Grid** para explorar las posibles combinaciones de hiperpar\u00e1metros y obtener el mejor modelo.","7ed662bf":"En esta selecci\u00f3n de modelos trataremos de optimizar los par\u00e1metros que consideramos son m\u00e1s relevantes en Gradient Boosting.\n\nLos hiperpar\u00e1metros elegidos para ser optimizados han sido:\n* `learning_rate`: Tasa de aprendizaje usada como factor multiplicativo para los nodos hoja. Toma un valor por defecto de 0.1.\n* `max_depth`: Profundidad m\u00e1xima de cada \u00e1rbol (desde la ra\u00edz al nodo m\u00e1s profundo). Este par\u00e1metro no se comprueba por defecto, pero puede resultar de inter\u00e9s estudiarlo.\n","2d6dc9f3":"Como averiguamos en la pr\u00e1ctica anterior, existen multitud de valores perdidos (codificados como $0$), que debemos tratar. Para ello, imputaremos por la media en las variables `Glucose`, `BloodPressure` y `BMI`; y eliminaremos las variables `Insulin` y `SkinThickness` que tienen un porcentaje demasiado alto de valores perdidos como para ser significativas. Adem\u00e1s necesitaremos normalizar las variables para usar algoritmos basados en distancia.\n","9e80fecf":"Como podemos observar, los mejores resultados se han obtenido con distinto n\u00famero m\u00e1ximo de hojas de los \u00e1rboles, por lo que podemos deducir que este hiperpar\u00e1metro no es el m\u00e1s importante. Se usar\u00e1 por tanto el modelo de menor complejidad. Sin embargo, la tasa de aprendizaje que ha dado mejores resultados ha sido $0.05$.","0270ae68":"## 5.4. Bootstrap Aggregating (Bagging)","6529aad0":"# Pr\u00e1ctica 2: Aprendizaje y selecci\u00f3n de modelos de clasificaci\u00f3n\n\n### Miner\u00eda de Datos: Curso acad\u00e9mico 2020-2021\n\n###\u00a0Profesorado:\n\n* Juan Carlos Alfaro Jim\u00e9nez\n* Jos\u00e9 Antonio G\u00e1mez Mart\u00edn\n\n### Realizado por:\n\n* Antonio Beltr\u00e1n Navarro\n* Ram\u00f3n Jes\u00fas Mart\u00ednez S\u00e1nchez\n\n\\* Adaptado de las pr\u00e1cticas de Jacinto Arias Mart\u00ednez y Enrique Gonz\u00e1lez Rodrigo","5ed84c4a":"# Pima Diabetes","43b17f3b":"Como hemos comentado anteriormente, ser\u00e1 recomendable eliminar la variable que no aporta informaci\u00f3n `veil-type` y las dos variables que van relacionadas con `ring-type`, como son `bruises y gill-color`.","0cb7b1d0":"A continuaci\u00f3n, separaremos en dos subconjuntos nuestro conjunto de datos inicial, uno con las variables predictoras (`X`) y otro con la variable objetivo (`y`). ","42e34ea6":"## 3.2. \u00c1rbol de decisi\u00f3n\n\nPara este algoritmo variaremos:\n* `criterion`: el criterio para hacer las particiones (`gini` o `entropy`).\n\n* `max_depth`: la profundidad m\u00e1xima del \u00e1rbol (sin limitar o de $2$ a $10$).\n\n* `min_samples_leaf`: el n\u00famero m\u00ednimo de ejemplos por hoja. Probaremos a partir de $5$ para evitar sobreajuste.\n\n* `ccp_alpha`: par\u00e1metro para controlar la pospoda del \u00e1rbol.","65274633":"Este algoritmo propone un entrenamiento de una serie de clasificadores de manera iterativa, de modo que cada nuevo clasificador se enfoque en los datos que fueron err\u00f3neamente clasificados por su predecesor, de esta forma el algoritmo se adapta y logra obtener mejores resultados.\n\nLos hiperpar\u00e1metros m\u00e1s relevantes a tener en cuenta ser\u00e1n `base_estimator`, que nos indica el estimador que utilizar\u00e1 el ensemble (en este caso un \u00c1rbol de decisi\u00f3n), el n\u00famero de estimadores `n_estimators`, o la tasa de aprendizaje que nos ayudar\u00e1 a controlar la contribuci\u00f3n de cada estimador `learning_rate`.\n\nEs necesario comentar que posteriormente, en el apartado de evaluaci\u00f3n de modelos, estudiaremos de mejor forma estos hiperpar\u00e1metros.","93d13406":"## 4.1. Eliminacion de variables","f578fa69":"Vamos a configurar un estimador tipo Random Forests usando los hiperpar\u00e1metros por defecto:","710935b3":"# 1. Preliminares","2c091f23":"En esta selecci\u00f3n de modelos trataremos de optimizar los par\u00e1metros que consideramos son m\u00e1s relevantes en el algoritmo de vecinos m\u00e1s cercanos.\n\nLos hiperpar\u00e1metros elegidos para ser optimizados han sido:\n* `weights`: Funci\u00f3n de peso de los vecinos, que pueden ser uniformes o por distancias.\n* `n_neighbors`: N\u00famero de vecinos m\u00e1s cercanos para elegir la clase del que se estudia. Un gran n\u00famero de vecinos podr\u00eda llevar al modelo a parecerse a un `ZeroR`, y un peque\u00f1o n\u00famero de vecinos podr\u00eda causarnos un gran sobreajuste. Es por ello que este hiperpar\u00e1metro es el m\u00e1s importante a la hora de seleccionar nuestro modelo de vecinos m\u00e1s cercanos.","4af5a1dc":"Con el prop\u00f3sito de usar m\u00e9tricas como el `recall` en el apartado de selecci\u00f3n de modelos, es necesario convertir nuestra variable clase que resulta ser categ\u00f3rica a una con valores num\u00e9ricos.\n\nPara ello elegiremos que la clase **M** (malign) pasar\u00e1 a tomar un valor de 1, y la clase **B** (benign) pasar\u00e1 a tomar un valor de 0.\n\nMas adelante veremos que dicho cambio se ha realizado correctamente.","225cb542":"# 6. \u00bfPrecisi\u00f3n del 100%?","27cc1d88":"Fijamos tambi\u00e9n una semilla para que los experimentos sean reproducibles","482ac4fd":"En esta libreta trabajaremos con distintos tipos de modelos que evaluaremos posteriormente para tratar de clasificar las setas en dos clases:\n* Comestibles (edible) clasificadas como `e`.\n* Venenosas (poisonous) clasificadas como `p`.","46e88a9a":"Definimos las 2 m\u00e9tricas que usaremos para seleccionar nuestros modelos, como son el `accuracy` y el `recall`. A la hora de elegir los mejores modelos, seleccionaremos aquellos que logren un mejor resultado en recall, el ratio de verdaderos positivos y falsos negativos obtenido mediante $\\frac {tp}{(tp+fn)}$","b5b0045d":"Para este algoritmo variaremos dos de los hiperpar\u00e1metros principales:\n\n* `n_neighbors`: la cantidad de ejemplos que se consideran para clasificar. Lo variaremos de $1$ a $15$\n\n* `weights`: el peso de cada ejemplo. Probaremos con pesos uniformes o proporcionales a la inversa de la distancia.\n\nAdem\u00e1s, al ser un algoritmo basado en la distancia, a\u00f1adiremos un paso m\u00e1s al preprocesamiento para normalizar las distintas variables al mismo intervalo.","81fbd958":"En este caso obtenemos mejores resultados usando como criterio entrop\u00eda y 500 estimadores.","284251b9":"## 4.3. Adaptative Boosting (*AdaBoost*)","d764c20d":"## 3.3. Adaptative Boosting (AdaBoost)\n\nProbaremos los hiperpar\u00e1metros:\n\n* `learning_rate`: la tasa de aprendizaje.\n\n* `n_estimators`: probaremos con $50$ y $75$ clasificadores.\n\n* Y para el \u00e1rbol de decisi\u00f3n interno:\n    * `criterion`: si usar `gini` o `entropia` para tomar la decisi\u00f3n de particionar.\n\n    * `max_depth`: la profundidad m\u00e1xima del \u00e1rbol, que deber\u00e1 ser peque\u00f1a.\n\n    * `ccp_alpha`: el par\u00e1metro de penalizaci\u00f3n para la pospoda.\n","5e3b1460":"**Librer\u00edas usadas** \n\n* [Numpy](https:\/\/numpy.org\/): Estudiar y trabajar con los datos\n* [Pandas](https:\/\/pandas.pydata.org\/): Trabajr con el conjunto de datos\n* [Sklearn](https:\/\/scikit-learn.org\/stable\/): Crear y evaluar los modelos\n* [Seaborn](https:\/\/seaborn.pydata.org\/): Visualizar gr\u00e1ficas con los datos\n* [Matplotlib](https:\/\/matplotlib.org\/): Visualizar gr\u00e1ficas con los datos\n","1b8a822d":"# 5. Construcci\u00f3n y validaci\u00f3n del modelo final","0711061d":"Antes de comenzar el an\u00e1lisis exploratorio de los datos, dividiremos nuestro conjunto de datos en otros dos subconjuntos, uno de entrenamiento y otro de prueba, con los siguientes porcentajes:\n\n* Conjunto de entrenamiento: **70%**\n* Conjunto de prueba: **30%**\n\nMediante este proceso, nos aseguraremos de que los resultados posteriores del proceso de validaci\u00f3n han sido obtenidos de una manera correcta.","6bae5786":"## 4.4. Bootstrap Aggregating (*Bagging*)","f799dd33":"Como las variables de nuestro conjunto de datos son categ\u00f3ricas, llevaremos a cabo un mapa de calor para observar la relaci\u00f3n entre estas variables.\n\nAntes, deberemos crear un `LabelEncoder` que transforme nuestras variables con un valor entre $0$ y el $ nclases - 1 $.","0363c6f8":"# Titanic","8c856575":"## 5.1. Regresi\u00f3n Log\u00edstica","c733f754":"Fijaremos tamb\u00eden una semilla aleatoria para que los experimentos sean reproducibles:","e30f4d96":"En la libreta en la cual nos hemos basado, se pasaba directamente a realizar un an\u00e1lisis exploratorio de los datos teniendo en cuenta la totalidad del conjunto de datos.\n\nEsto significa que exploraba con el mismo conjunto de datos con los que luego iba a validar el modelo, causando as\u00ed un **Data Leak**. Para solucionar este problema, hemos decidido dividir primero nuestro conjunto de datos en dos subconjuntos de entrenamiento y test.\n\nDe esta manera, trabajaremos en el an\u00e1lisis exploratorio con este conjunto de entrenamiento, y el de test permanecer\u00e1 sin usarse hasta el apartado de selecci\u00f3n de modelos.","917eac1b":"Una vez hemos cargado el conjunto de datos, mostraremos 5 registros aleatorios mediante la funci\u00f3n `sample` para comprobar que el proceso ha sido realizado correctamente","d47bea74":"## 3.1. Vecinos m\u00e1s cercanos","6764496a":"# 1. Preliminares\n\nYa hemos cargado toda la funcionalidad que necesitaremos en la parte anterior:\n* Los distintos algoritmos que aprenderemos (`AdaBoostClassifier`, \n`BaggingClassifier`, \n`GradientBoostingClassifier`, \n`HistGradientBoostingClassifier`, \n`RandomForestClassifier`, \n`KNeighborsClassifier` y \n`DecisionTreeClassifier`)\n\n* Funcionalidad relacionada con la selecci\u00f3n y evaluaci\u00f3n de modelos (`RepeatedStratifiedKFold` y \n`train_test_split`)","13ee3b81":"## 4.5. Random Forests","b193d733":"Vamos a usar los hiperpar\u00e1metros por defecto para configurar este estimador:","1e4249d8":"Veamos, para terminar, un resumen de la precisi\u00f3n obtenida por cada uno de los modelos que hemos estudiado para resolver nuestro problema.","831c3e3d":"# 4. Construcci\u00f3n y validaci\u00f3n del modelo final","320ceac7":"Vamos a estudiar el siguiente kernel [Mushroom Classification & why it's easy to 100%ac](https:\/\/www.kaggle.com\/arevel\/mushroom-classification-why-it-s-easy-to-100-ac) realizando las modificaciones que consideramos necesarias:\n* Traducci\u00f3n de la libreta.\n* Uso de un script de utilidades con el c\u00f3digo necesario para plotear gr\u00e1ficas y generar los modelos.\n* Arreglo de **Data Leaks** encontrados.\n* Implementaci\u00f3n de un pipeline","810e401c":"Lo siguiente que debemos hacer es fijar una semilla con el objetivo de que todo lo que hagamos sea reproducible.","b26c486b":"\nVamos a configurar un ensemble tipo Bagging utilizando los hiperpar\u00e1metros por defecto:","e6ceba15":"Los resultados indican que a mayor n\u00famero de \u00e1rboles mejor resultado, como era de esperar. Tambi\u00e9n podemos observar que la entrop\u00eda se impone al `gini`. En cuanto al n\u00famero de caracter\u00edsticas seleccionadas, debido al bajo n\u00famero de caracter\u00edsticas totales con los que contamos en este conjunto de datos, ambos m\u00e9todos son en esencia id\u00e9nticos. Por \u00faltimo, observamos que se ha sobreajustado a los datos de entrenamiento, consiguiendo un $100\\%$ para estos.","7f3c6736":"Por tanto, antes de comenzar el an\u00e1lisis exploratorio de los datos, dividiremos nuestro conjunto de datos en otros dos subconjuntos, uno de entrenamiento y otro de prueba, con los siguientes porcentajes:\n\n* Conjunto de entrenamiento: **80%**\n* Conjunto de prueba: **20%**\n\nMediante este proceso, nos aseguraremos de que los resultados posteriores del proceso de validaci\u00f3n han sido obtenidos de una manera correcta.","827ddad8":"Comprobamos si hay valores nulos","d4045fb1":"Como hemos comentado previamente, este algoritmo emplea \u00e1rboles distintos con mucho sobreajuste, por lo que no resultar\u00eda de importancia controlar los hiperpar\u00e1metros de \u00e9stos \u00e1rboles como hicimos en AdaBoost. Nos limitaremos a controlar los siguientes hiperpar\u00e1metros referentes a los Random Forest:\n* `max_samples`: N\u00famero de instancias del conjunto de entrenamiento necesarias para entrenar cada estimador (\u00e1rbol).\n* `max_features`: N\u00famero m\u00e1ximo de variables para considerar al buscar la mejor divisi\u00f3n al entrenar los \u00e1rboles. Al utilizar `sqrt` estamos indicando que `max_features=sqrt(n_features)`. Lo mismo para `log2`, tal que `max_features=log2(n_features)`\n* `criterion`: Funci\u00f3n para medir la calidad de las particiones del \u00e1rbol. Usaremos `entropy` para la ganancia de informaci\u00f3n y el \u00edndice `gini`.","3a430b99":"Seguidamente, lo que haremos ser\u00e1 unir los conjuntos `X_train` e `y_train` para obtener el conjunto de datos de entrenamiento. Haremos los mismo para `X_test` e `y_test`, juntando as\u00ed el conjunto de datos de test.","e8984d0b":"En primer lugar, definimos una funci\u00f3n para mostrar los resultados obtenido por cada uno de los modelos.","5b034a02":"Pasemos ahora a la selecci\u00f3n de modelos, donde emplearemos para cada uno el **Pipeline** definido.","00978f0f":"Gradient Boosting es una generalizaci\u00f3n de los algoritmos de Boosting con la capacidad de optimizar cualquier tipo de funci\u00f3n p\u00e9rdida, generando un conjunto de estimadores de forma secuencial. No todos estos estimadores tendr\u00e1n la misma importancia, puesto que en cada iteraci\u00f3n solo se tomar\u00e1n en cuenta a los modelos anteriores.","074f2c45":"El algoritmo Histogram Gradient Boosting es una optimizaci\u00f3n de *Gradient Boosting* que discretiza el conjunto de datos de entrada con el fin de poder trabajar con un mayor n\u00famero de instancias en un tiempo razonable.\n\nAl contrario que en otros modelos, Histogram Gradient Boosting realiza su propia discretizaci\u00f3n, por lo que no tendremos que proporcion\u00e1rsela mediante el **Pipeline**.","5299483b":"Podemos observar que los mejores resultados se dan cuando se aplica pospoda, usando como criterio `entropy`. Tambi\u00e9n observamos que el aumento del n\u00famero de clasificadores de $50$ a $75$ no tiene demasiado impacto, por lo que entre estas dos opciones elegiremos la de menor complejidad.\n\n","018d36a3":"# Conclusiones","0ae009b1":"## 3.5. Random Forests","dab10c87":"Conseguimos los mejores resultados usando `n_neighbors` = $5$ y ponderando los pesos proporcionalmente a su distancia.","a5312124":"Observamos que `gini` funciona mejor que `entropy` junto con hacer prepoda en profundidad $4$. `min_samples_leaf` tiene un impacto menor en la puntuaci\u00f3n obtenida cuando fijamos los dem\u00e1s, aunque el valor \u00f3ptimo parece ser $7$.","36b71edd":"Hay muchos modelos diferentes y muchas variaciones diferentes de cada modelo, para elegir los que tendr\u00e1n un mejor rendimiento, debemos considerar cu\u00e1l se ajustar\u00e1 mejor a nuestro conjunto de datos.\n\nNuestro conjunto de datos est\u00e1 equilibrado, la entrada y la salida son categ\u00f3ricas y tenemos alrededor de 8.000 instancias. Teniendo esto en cuenta usaremos los siguientes modelos:\n\n* Regresi\u00f3n Log\u00edstica\n* Naive Bayes\n* Random Forest\n* KNN","e63e9d81":"# 5. Selecci\u00f3n de modelos","abd8bdba":"Para AdaBoost, deberemos decidir los par\u00e1metros del propio algoritmo as\u00ed como los del clasificador usado como base, que ser\u00e1 un \u00e1rbol de decisi\u00f3n de poca profundidad. Estos ser\u00e1n:\n\n* `learning_rate`: la tasa de aprendizaje.\n\n* `n_estimators`: probaremos con $50$ y $75$ clasificadores.\n\n* Y para el \u00e1rbol de decisi\u00f3n:\n    * `criterion`: si usar `gini` o `entropia` para tomar la decisi\u00f3n de particionar.\n\n    * `max_depth`: la profundidad m\u00e1xima del \u00e1rbol, que deber\u00e1 ser peque\u00f1a.\n\n    * `ccp_alpha`: el par\u00e1metro de penalizaci\u00f3n para la pospoda.","0721b785":"## 3.4. Bootstrap Aggregating (Bagging)","9d7c6e1d":"### 2.1.2. Normalizaci\u00f3n","c7e26141":"# Conclusiones\n\nComo hemos podido observar, atendiendo al _recall_ que es la m\u00e9trica que quer\u00edamos optimizar, el mejor modelo que hemos conseguido entrenar ha sido el AdaBoost. Tambi\u00e9n podemos ver que en t\u00e9rminos de _accuracy_ hemos obtenido unos resultados decentes a pesar de no haberla optimizado, destacando dos de los m\u00e9todos de _ensembles_: Random Forests y Gradient Boosting.","c7429be5":"Las conclusiones que podemos obtener son que la tasa de aprendizaje es un factor relevante a la hora de seleccionar un modelo, y que un \u00e1rbol con un n\u00famero de instancias en cada nodo hoja nos proporciona mejores resultados.","e4a3daaa":"\nVamos a configurar un modelo AdaBoost sencillo que utilice los hiperpar\u00e1metros por defecto:","fffa0f80":"Como podemos observar, se obtiene un mejor resultado con un valor de `n_neighbors` bajo ($1$ o $2$) y con una distribuci\u00f3n de `weights` proporcional a la distancia (para el caso `n_neighbors`=$1$ `uniform` y `distance` tienen obviamente el mismo efecto, pero por ejemplo con `n_neighbors`=$2$ `distance` es mejor).","0e11a861":"## 5.2. Naive Bayes","2de1c026":"Este conjunto de datos est\u00e1 compuesto por 30 variables predictoras que en realidad se agrupan en 10 ya que estas se ven divididas entre `_mean, _se y _worst`, una variable `Unnamed` que ser\u00e1 tratada posteriormente y la variable clase `diagnosis` que nos indicar\u00e1 si el tumor es **maligno** (m) o **benigno** (b).\n\nLas variables predictoras, por tanto, son:\n* `radius, perimeter, area, compactness`: relacionadas con el tama\u00f1o del tumor\n* `symmetry`: se refiere a la simetr\u00eda en la forma del tumor\n* `smoothness`: variaci\u00f3n en la longitud de los tama\u00f1os\n* `concavity, concave_points`: relacionados con la concavidad del tumor\n* `texture`: referente a la textura del tumor (en escala de grises)\n* `fractal_dimension`","232e1bb6":"En este algoritmo al igual que en el resto basados en \u00e1rboles probaremos cu\u00e1l de los criterios para medir la calidad de una partici\u00f3n es mejor. Adem\u00e1s, al tomar para cada \u00e1rbol un subconjunto aleatorio de caracter\u00edsticas, deberemos elegir cu\u00e1ntas tomar. El muestreo nuevamente ser\u00e1 con reemplazo. Por lo tanto los hiperpar\u00e1metros que variaremos ser\u00e1n:\n\n* `criterion`: `gini` o `entropy`.\n\n* `max_features`: $\\sqrt{\\mathit{n\\_features}}$ o $log_2(\\mathit{n\\_features})$.\n\n* `n_estimators`: el n\u00famero de clasificadores base.","b85505de":"Para este algoritmo, conseguimos mejores resultados usando \u00e1rboles prepodados de solo un nivel de profundidad. Tambi\u00e9n observamos que el resultado mejora con 200 estimadores con respecto a 100. El criterio (`mse` o `friedman_mse`) no ha tenido grandes efectos en los resultados.","3a35953b":"Para realizar este proceso, haremos uso de un **Pipeline**. Este Pipeline ser\u00e1 el encargado de aplicar las transformaciones que hemos decidido a nuestro conjunto de datos.","f7506cb6":"Es el momento de definir los diferentes modelos que utilizaremos en esta pr\u00e1ctica. Cada uno de estos modelos contar\u00e1 con el preprocesamiento de datos definido previamente haciendo uso del **Pipeline**.\n\nComentaremos los par\u00e1metros de cada modelo m\u00e1s importantes para nuestro problema y que utilizaremos posteriormente mediante un proceso de validaci\u00f3n cruzada.","dfb8ecae":"## 3.8. Histogram-Based Gradient Boosting (*Histogram Gradient Boosting*)","a05f22e6":"Lo primero de todo ser\u00e1 cargar las librer\u00edas para que est\u00e9n disponibles posteriormente:","a046dea7":"# 4. Construcci\u00f3n y validaci\u00f3n del modelo final","d3d40230":"#\u00a04. Evaluaci\u00f3n de modelos","9d0a9c2f":"# Conclusiones","efc9fcc4":"-----\n","1566ff69":"De esta manera, podremos ver las variables que se encuentran m\u00e1s correladas unas con otras. A simple vista, encontramos diversas parejas de variables que est\u00e1n altamente relacionadas unas con otras.\n\n* `veil-color y gill-attachment` con una correlaci\u00f3n del 89%\n* `ring-type y bruises` con una correlaci\u00f3n del 69%\n* `ring-type y gill-color` con una correlaci\u00f3n del 63%\n* `spore-print-color y gill-size` con una correlaci\u00f3n del 62%\n\nDe este estudio extraemos que las variables `bruises y gill-color` dependen ambas de `ring-type`, y consideramos que podr\u00edan ser eliminadas. Pese a que en el procesamiento no se haya llevado a cabo dicha acci\u00f3n, la implementaremos.\n\nComo hemos mencionado anteriormente, la variable `veil-type` ser\u00e1 eliminada puesto que no aporta ninguna informaci\u00f3n relevante.","1632bf98":"El mejor `learning_rate` ha sido 0.05, sin embargo `max_leaf_nodes` no parece tener demasiado impacto en el resultado y el mejor en estas pruebas ha resultado ser 15.","67c9b9f4":"##\u00a02.1. Preprocesamiento de datos","cf335145":"## 5.5. Random Forests","2ddfffb6":"Podemos observar c\u00f3mo la precisi\u00f3n del 99% obtenida previamente no era fiable.","90d4f6a4":"Empezamos mostrando las variables predictoras:","3c14323f":"En este caso parece claro que cuantos m\u00e1s clasificadores se usen, mejor resultado se alcanza, aunque la mejora entre emplear $250$ o $500$ es casi inapreciable por lo que puede no merecer la pena. Una vez m\u00e1s, usando entrop\u00eda conseguimos mejores resultados. Tambi\u00e9n es interesante observar que se ha conseguido un $100\\%$ clasificando los datos de entrenamiento, lo cual indica un gran sobreajuste a estos.","ec543c83":"# 3. Selecci\u00f3n de modelos\n\nEn esta pr\u00e1ctica, aplicaremos distintos algoritmos de aprendizaje de clasificadores con el objetivo de encontrar el que mejor funciona para el problema en cuesti\u00f3n (lograr predecir que un paciente tiene diabetes). Estos son:\n\n* Vecinos m\u00e1s cercanos\n\n* \u00c1rbol de decisi\u00f3n\n\n* AdaBoost\n\n* Bagging\n\n* Random Forests\n\n* Gradient Boosting\n\n* Histogram Gradient Boosting\n\nPor cada algoritmo, buscaremos los hiperpar\u00e1metros que mejor funcionen mediante el algoritmo de b\u00fasqueda en Grid. La evaluaci\u00f3n de los modelos obtenidos se har\u00e1 mediante una $10 \\times 5$ validaci\u00f3n cruzada, usando como m\u00e9trica de rendimiento el *recall*, al tratarse de un problema desbalanceado.","87ef31fa":"# Conclusiones","e6e635f8":"## 3.2. \u00c1rbol de decisi\u00f3n","967e19cf":"## 5.3. Random Forest","c5fc7168":"Hemos conseguido el mismo resultado con distintas configuraciones de par\u00e1metros. Los que parecen que son comunes son: criterio `gini` y aplicar pospoda con `ccp_alpha` = $0.1$. Elegiremos el que no limita `max_depth` y `min_samples_leaf` = $5$.","8422b2a6":"En cuanto al criterio seleccionado, vemos que en este caso el \u00edndice gini ha conseguido darnos mejores resultados en nuestro modelo junto a una tasa de aprendizaje (learning rate) que mantiene su valor por defecto de 1.\n\nConociendo el algoritmo AdaBoost sabemos que \u00e9ste trabaja con clasificadores 1R, los cuales reciben una inmensa cantidad de instancias, no obstante es interesante remarcar que el hiperpar\u00e1metro `min_samples_split` para los nodos internos ha resultado no ser relevante para el problema.","1705c541":"Obtenemos los mejores resultados usando criterio `entropy`, `learning_rate` = $1$, limitando la profundad `max_depth` a 1 y con `ccp_alpha` = 0.1. Aumentar de 50 a 75 el n\u00famero de clasificadores no ha tenido efecto, probablemente porque 50 ya sobra para conseguir el \u00f3ptimo.","08a1dfcc":"## 5.4. KNN","a379a470":"\n\nVamos a configurar este estimador usando los hiperpar\u00e1metros por defecto:","6c419be7":"Nuevamente, vamos a asegurarnos de que el conjunto de datos se ha dividido correctamente. Para ello visualizaremos el conjunto de datos de entrenamiento, observando que la variable clase tambi\u00e9n aparece al final del conjunto:"}}