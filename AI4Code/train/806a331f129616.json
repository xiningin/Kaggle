{"cell_type":{"80d4fc34":"code","76ccf17b":"code","bf9668f1":"code","1d5c7bf2":"code","25f91346":"code","46140fef":"code","af7ac981":"code","bd3bb5b3":"code","dbefec91":"code","40be9b75":"code","95491a14":"code","ca8055d8":"code","e2a279a5":"code","3d219546":"code","71c23cbd":"code","4a24ca3c":"code","5200fc09":"code","8ece553f":"code","0d49cb5a":"code","19b45864":"code","ecf26adf":"code","5601e3c0":"code","ce6daead":"code","08c81e58":"code","20ff8034":"code","edda06ca":"code","67a5a220":"code","fbffe638":"code","e5847c8f":"code","0ccab750":"code","1918c665":"code","cd75f269":"code","f91e7102":"code","277bff9b":"code","d6ab15ed":"code","f23a453b":"code","2e2746ca":"code","b3ae83d8":"code","bc2cd97d":"code","a926f3ef":"code","51ebe72e":"code","ea5b52f4":"code","a7449e99":"markdown","5efb73b9":"markdown","3ea32ffc":"markdown","4d79c64d":"markdown","16c1ee11":"markdown","11da60fc":"markdown","2ea405f8":"markdown","7689450c":"markdown"},"source":{"80d4fc34":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","76ccf17b":"import matplotlib.pyplot as plt","bf9668f1":"dt = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndt_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","1d5c7bf2":"dt.head()","25f91346":"dt.info()","46140fef":"cols_to_drop = ['PassengerId','Name','Ticket','Cabin','Embarked']","af7ac981":"data_clean = dt.drop(columns=cols_to_drop,axis=1)\ndata_clean_test = dt_test.drop(columns=cols_to_drop,axis=1)\ndata_clean.info()","bd3bb5b3":"data_clean.head()","dbefec91":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","40be9b75":"data_clean['Sex'] = le.fit_transform(data_clean['Sex'])\ndata_clean_test['Sex'] = le.fit_transform(data_clean_test['Sex'])\ndata_clean.head()","95491a14":"import seaborn as sns","ca8055d8":"def bar_chart(feature):\n    survived = dt[dt['Survived']==1][feature].value_counts()\n    dead = dt[dt['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar',stacked=True)\n    \nbar_chart('Sex')","e2a279a5":"bar_chart('SibSp')","3d219546":"data_clean.corr()\nplt.figure(figsize=(11,11))\nsns.heatmap(data_clean.corr(),cmap='rainbow',annot=True)\nplt.show()","71c23cbd":"data_clean.info()","4a24ca3c":"data_clean['Age'] = data_clean.fillna(data_clean['Age'].mean())['Age']\ndata_clean_test['Age'] = data_clean_test.fillna(data_clean_test['Age'].mean())['Age']\ndata_clean.info()","5200fc09":"data_clean_test.info()","8ece553f":"data_clean_test['Fare'] = data_clean_test.fillna(data_clean_test['Fare'].mean())['Fare']\ndata_clean_test.info()","0d49cb5a":"input_cols = ['Pclass',\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\"]\noutput_cols = [\"Survived\"]","19b45864":"x_train = data_clean[input_cols]\ny_train = data_clean[output_cols]\nx_test = data_clean_test[input_cols]","ecf26adf":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\ndtc = DecisionTreeClassifier(max_depth=4,criterion='entropy')\nrfc = RandomForestClassifier(max_depth=5,criterion='entropy',n_estimators=16)","5601e3c0":"dtc.fit(x_train,y_train)","ce6daead":"dtc.score(x_train,y_train)","08c81e58":"rfc.fit(x_train,y_train)","20ff8034":"rfc.score(x_train,y_train)","edda06ca":"y_pred = dtc.predict(x_test)\ny_pred1 = rfc.predict(x_test)","67a5a220":"print(y_pred)\nprint(y_pred1)","fbffe638":"y_test = []\ny_test1 = []\nfor i in range(x_test.shape[0]):\n    y_test.append([dt_test['PassengerId'][i],y_pred[i]])\n    y_test1.append([dt_test['PassengerId'][i],y_pred1[i]])","e5847c8f":"y_test","0ccab750":"dt = pd.DataFrame(y_test,columns=['PassengerId','Survived'])\ndt1 = pd.DataFrame(y_test1,columns=['PassengerId','Survived'])","1918c665":"dt.head()","cd75f269":"dt1.head()","f91e7102":"dt.to_csv(\"Submit1.csv\",index=False)\ndt1.to_csv(\"Submit2.csv\",index=False)","277bff9b":"def entropy(col):\n\n    counts = np.unique(col, return_counts=True)\n    n = float(col.shape[0])\n\n    ent = 0.0\n    for ix in counts[1]:\n        p = ix\/n\n        ent += (-1.0*p*np.log2(p))\n\n    return ent","d6ab15ed":"def divide_data(x_data,fkey,fval):\n    \n    x_right = pd.DataFrame([],columns=x_data.columns)\n    x_left = pd.DataFrame([],columns=x_data.columns)\n    \n    for ix in range(x_data.shape[0]):\n        val = x_data[fkey].loc[ix]\n        \n        if val>fval:\n            x_right = x_right.append(x_data.loc[ix])\n        else:\n            x_left = x_left.append(x_data.loc[ix])\n    \n    return x_left,x_right","f23a453b":"x_left,x_right = divide_data(data_clean[:10],'Sex',0.5)\nprint(x_left)\nprint(x_right)","2e2746ca":"def information_gain(x_data,fkey,fval):\n    \n    left,right = divide_data(x_data,fkey,fval)\n    \n    l = float(left.shape[0])\/x_data.shape[0]\n    r = float(right.shape[0])\/x_data.shape[0]    \n    \n    # If all examples come on one side\n    if left.shape[0] == 0 or right.shape[0] == 0:\n        return -1000000000\n    \n    info_gain = entropy(x_data.Survived) - (l*entropy(left.Survived)+r*entropy(right.Survived))\n    \n    return info_gain","b3ae83d8":"for fx in X.columns:\n    print(fx)\n    print(information_gain(data_clean,fx,data_clean[fx].mean()))","bc2cd97d":"class DecisionTree:\n    \n    def __init__(self,depth=0,max_depth=5):\n        self.left = None\n        self.right = None\n        self.fkey = None\n        self.fval = None\n        self.depth = depth\n        self.max_depth = max_depth\n        self.target = None\n        \n    def train(self,X_train):\n        \n        features = ['Pclass','Sex','Age','SibSp','Parch','Fare']\n        info_gain=[]\n        \n        for ix in features:\n            i_gain = information_gain(X_train,ix,X_train[ix].mean())\n            info_gain.append(i_gain)\n            \n        self.fkey = features[np.argmax(info_gain)]\n        self.fval = X_train[self.fkey].mean()\n        print(\"Making node, feature is \",self.fkey)\n        \n        #Split data\n        data_left,data_right = divide_data(X_train,self.fkey,self.fval)\n        data_left = data_left.reset_index(drop=True)\n        data_right = data_right.reset_index(drop=True)\n        \n        if data_left.shape[0]==0 or data_right.shape[0]==0:\n            if X_train.Survived.mean() >= 0.5:\n                self.target = 'Survived'\n            else:\n                self.target = 'Dead'\n            return\n        \n        if (self.depth >= self.max_depth):\n            if X_train.Survived.mean() >= 0.5:\n                self.target = 'Survived'\n            else:\n                self.target = 'Dead'\n            return\n        \n        self.left = DecisionTree(depth=self.depth+1,max_depth=self.max_depth)\n        self.left.train(data_left)\n        \n        self.right = DecisionTree(depth=self.depth+1,max_depth=self.max_depth)\n        self.right.train(data_right)\n        \n        if X_train.Survived.mean() >= 0.5:\n                self.target = 'Survived'\n        else:\n                self.target = 'Dead'\n        return\n    \n    def predict(self,test):\n        if test[self.fkey] >= str(self.fval):\n            if self.right is None:\n                return self.target\n            return self.right.predict(test)\n        else:\n            if self.left is None:\n                return self.target\n            return self.left.predict(test)","a926f3ef":"d = DecisionTree()","51ebe72e":"d.train(data_clean)","ea5b52f4":"print(d.fkey,d.fval)\nprint(d.left.fkey,d.right.fkey)","a7449e99":"# Implementing Decision Trees","5efb73b9":"# Breakdown of Problem\n\n- Step 1: Identifying Problem Statement\n- Step 2: Importing the dataset and Cleaning it\n- Step 3: Visualising it\n- Step 4: Feature engineering\n- Step 5: Modelling\n- Step 6: Testing\n\n- Bonus: Showing the algorithmic implementation","3ea32ffc":"# Feature Engineering","4d79c64d":"From the above information gain, we can see \"Sex\" has maximum correlation with the output target","16c1ee11":"# Applying the Decision tree algorithm without Scikit-Learn\n## Defining entropy and information gain factors","11da60fc":"# Importing the dataset & analysis","2ea405f8":"# Modelling and Training\nApplying Scikit learn Libraries\nUsed both Decision tree clasifier and random forrest and compared them to get finely tuned prediction","7689450c":"# Visualisation"}}