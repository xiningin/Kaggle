{"cell_type":{"9fcfc6ea":"code","ccd002d4":"code","ae11cf92":"code","895b0992":"code","261a1622":"code","e40936ac":"code","b1a04ff9":"code","b2953a83":"code","1e49b9ee":"markdown","2d85bc6e":"markdown"},"source":{"9fcfc6ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ccd002d4":"!pip install -q tensorflow-gpu\nimport tensorflow as tf\nprint(\"GPU\", \"Positive\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"Negative\" )\nprint(\"version : \",tf.__version__)","ae11cf92":"# SET to show all columns\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\n\n# SET data path\npath = os.path.join(dirname, filename)\n\n# Load Data\ndf = pd.read_csv(path)\n# Making dataframe clear\ndf = df.dropna()\nprint(df.isna().sum())\n\n# I want to use \"Open\",\"Volume_(BTC)\" and \"Volume_(Currency)\" as input data(aka features)\n# and Set \"Close\" as y_true\n\n# Drop columns arent used as input data\ndf.drop(columns=[\"Timestamp\",\"High\",\"Low\", \"Weighted_Price\"], inplace=True)\nprint(df)\n# Set 'Close' as y_ture\ny_true = df.pop('Close')\ndf['y_true'] = y_true\nprint(df)\n\n# Split train set and test set\nn = len(df)\ntrain_df = df[:int(n*0.9)]\ntest_df = df[int(n*0.9):]\n\n# Normalize\nnorm_train_df = (train_df-train_df.mean())\/train_df.std()\nnorm_test_df = (test_df-test_df.mean())\/test_df.std()\nprint(norm_train_df.describe().transpose())","895b0992":"# Build Window generator\nclass WindowGenerator():\n    # Set Parameters\n    def __init__(self,input_width, label_width, offset, \n                 train_df = norm_train_df, test_df = norm_test_df,\n                label_columns=None):\n        # store data\n        self.train_df, self.test_df = train_df, test_df\n        # Make label dictionary\n        if label_columns is not None:\n            self.label_dict = {name: i for i, name in enumerate(label_columns)}\n        self.label_columns = label_columns\n        \n        # Set winodw parameters\n        self.input_width, self.label_width = input_width, label_width\n        self.total_window_size = input_width + offset\n        \n        self.input_slice = slice(0,input_width)\n        self.input_example = np.arange(self.total_window_size)[self.input_slice]\n        \n        self.label_start = self.total_window_size - label_width\n        self.label_slice = slice(self.label_start,None)\n        self.label_example = np.arange(self.total_window_size)[self.label_slice]\n        \n    def __repr__(self):\n        return '\\n'.join([\n            f'Total window size: {self.total_window_size}',\n            f'Input Example: {self.input_example}',\n            f'Label Example: {self.label_example}',\n            f'Label Name: {self.label_columns}'])\n    \n    def split_datasets(self, datasets):\n        # inputs.shape = (batch, index(time), features)\n        # features are in datasets : Open, Volume_(BTC), Volume_(Currency), Close\n        # So except Close, the others are used as inputs\n        inputs = datasets[:, self.input_slice, :-len(self.label_columns) ]\n        labels = datasets[:, self.label_slice, -len(self.label_columns):]\n#         if self.label_columns is not None:\n#             labels = tf.stack(\n#                 [labels[:, :, self.label_dict[name]] for name in self.label_columns],\n#             axis=-1)\n        # Set Shape\n        inputs.set_shape([None, self.input_width, None])\n        labels.set_shape([None, self.label_width, None])\n        \n        \n        return inputs, labels\n    \n    def datasets_maker(self, data):\n        data = np.array(data, dtype=np.float32)\n        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n                data, targets=None, sequence_length=self.total_window_size, \n                sequence_stride=1,shuffle=False, batch_size=60)\n        ds = ds.map(self.split_datasets)\n        \n        return ds\n    \n    @property\n    def train(self):\n        return self.datasets_maker(self.train_df)\n    \n    @property\n    def test(self):\n        return self.datasets_maker(self.test_df)\n    \nten = WindowGenerator(input_width=10, label_width=1, offset=10, \n                     label_columns=['Close'])\nprint(\"Train : {}\\nTest : {}\".format(ten.train,ten.test))","261a1622":"# Build Model\nclass Build_Model(tf.keras.Model):\n    def __init__(self):\n        super(Build_Model, self).__init__()\n        self.lstm = tf.keras.layers.LSTM(32, return_sequences=False, \n                                        kernel_regularizer='l2')\n        self.dense = tf.keras.layers.Dense(32, activation='relu')\n        self.last = tf.keras.layers.Dense(1)\n        \n    def call(self, x):\n        x = self.lstm(x)\n        x = self.dense(x)\n        return self.last(x)\n\nmodel = Build_Model()","e40936ac":"# Set Loss and Optimizer\nloss_object = tf.keras.losses.Huber()\noptimizer = tf.keras.optimizers.Adam()\n\n@tf.function\ndef train_step(x,y):\n    with tf.GradientTape() as tape:\n        predictions = model(x)\n        loss = loss_object(y, predictions)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    \n@tf.function\ndef test_step(x,y):\n    predictions = model(x)\n    loss = loss_object(y, predictions)","b1a04ff9":"# Get Train and Test datasets\ntrain_dataset = ten.train\ntest_dataset = ten.test\n# Train the model\nEPOCHS = 10\nfor epoch in range(EPOCHS):\n    for train_x, train_y in train_dataset:\n        train_step(train_x, train_y)\n        \n    for test_x, test_y in test_dataset:\n        test_step(test_x, test_y)\n        \n    print(\"EPOCHS : {} \".format(epoch+1))\n\ntf.keras.backend.clear_session()","b2953a83":"# Lets take graphs\nimport matplotlib.pyplot as plt\n\norigin = norm_test_df['y_true']\nforecast = model.predict(test_dataset).ravel(order='C')\nprint(\"origin : {}\\nforecast : {}\".format(origin.shape, forecast.shape))\n\nplt.figure()\nplt.plot(range(len(origin)), origin, label=\"Origin\", color='r')\nplt.plot(range(len(forecast)), forecast, label=\"Forecast\", color='b')\nplt.legend()\nplt.show()\n\ntf.keras.backend.clear_session()","1e49b9ee":"# **Make Predict Close price per ten minutes**\n1. **Load Data from bitcoin-historical-data**\n2. **Using Open price and Volumes to predict Close price**\n3. **Re-build our dataframe to fit our purpose(num-2)**\n4. **Making dataset by using WindowGenerator to predict time-sequences**\n5. **Build a model with TensorFlow(Conv1D and LSTM)**\n6. **Lets Predict!**","2d85bc6e":"# **Finally! We Got Extra Simple prediction model!**"}}