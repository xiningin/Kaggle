{"cell_type":{"474ff6d8":"code","444f3c34":"code","52a7587c":"code","72c00afe":"code","89289dd0":"code","1bc3ba59":"code","b26b7126":"code","d8e2d158":"code","1119876f":"code","324f72da":"code","3fc2bfb8":"code","2dc624ae":"code","e4b34d65":"code","40ed5ea8":"code","bfc14334":"code","2fb31aa2":"code","5f835e7d":"code","85b4d22b":"code","593a596c":"code","7d55e32c":"code","a77e50d9":"code","ada58b13":"code","cdec2736":"code","c274c7f1":"code","84594f06":"code","674b52d0":"code","62eefa68":"code","823b938a":"code","f2865578":"code","8f0d6c09":"code","39bc71e1":"code","26809f23":"code","167a5f8c":"code","c5f62642":"code","8664514c":"code","f150b012":"code","a9ba17f7":"code","b26860cd":"code","6a0f1314":"code","45e190ba":"code","40eb4021":"markdown","f9629c98":"markdown","e36dff0f":"markdown","00fd545d":"markdown","db8d3262":"markdown","fe8b5a45":"markdown","5c5c117a":"markdown","c4b6dc3b":"markdown","13701b47":"markdown","eeda841f":"markdown","e63296b6":"markdown"},"source":{"474ff6d8":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport cv2\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom scipy.sparse.linalg.eigen.arpack import eigsh as largest_eigsh\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\nfrom tensorflow.keras.layers import Dense, Input, Conv1D,MaxPooling1D, GlobalMaxPooling1D, GaussianNoise, BatchNormalization,Flatten,Dropout\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.utils.np_utils import to_categorical\nfrom tensorflow.keras.optimizers import RMSprop, Adam\n","444f3c34":"# load data set\ntrain_data = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest_data = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","52a7587c":"train_data.head()","72c00afe":"test_data.head()","89289dd0":"#Splitting labels from actual data\nXtrain = train_data.iloc[:,1:]\nytrain = train_data.iloc[:,0]\nXtest = test_data","1bc3ba59":"#Overview of the dataset\nnum_pixels = len(Xtrain.loc[0])\nimg_dim = np.sqrt(num_pixels).astype(int)\nnum_classes = len(set(ytrain))\nnum_samples = len(Xtrain)\nnum_samples_te = len(Xtest)\nprint(10*\"-\",\"Overview of the Dataset\",10*\"-\")\nprint(\"Size of each image: {} x {}\".format(img_dim,img_dim))\nprint(\"Number of pixels in each image:\",num_pixels)\nprint(\"Number of classes:\",num_classes)\nprint(\"Number of images in the training set:\",num_samples)\nprint(\"Number of images in the testing set:\",num_samples_te)","b26b7126":"#samples per class\nsample_per_class = np.unique(ytrain,return_counts = True)\nsns.barplot( x = sample_per_class[0], y = sample_per_class[1])\nplt.title('Samples per class')\nplt.xlabel('Label')\nplt.ylabel('Count')\nplt.show()","d8e2d158":"for m,n in zip(sample_per_class[0],sample_per_class[1]):\n    print(\"Number of samples for {} : {} \".format(m,n))","1119876f":"# Normalization\nXtrain = Xtrain \/ 255.0\nXtest = Xtest \/ 255.0","324f72da":"#Splitting the training data into training and validation set\nXtrain,Xval,ytrain,yval = train_test_split(Xtrain,ytrain,test_size = 0.2,random_state = 2)","3fc2bfb8":"#Reshaping Xtrain and Xtest\nXtrain  = Xtrain.to_numpy().reshape(-1,28,28)\nXval = Xval.to_numpy().reshape(-1,28,28)\nytrain  = ytrain.to_numpy()\nXtest = Xtest.to_numpy().reshape(-1,28,28)","2dc624ae":"#Plotting images for each sign\nplt.figure(figsize=(20,6))\n\nfor i,j in enumerate([4,7,0,1,14,9,32,5,8,3]):\n    plt.subplot(2,5,i+1)\n    plt.subplots_adjust(top = 2, bottom = 1)\n    img = Xtrain[j]\n    img = img.reshape((28,28))\n    plt.imshow(img,cmap = 'gray')\n    plt.title(ytrain[j])\n    plt.axis('off')\nplt.suptitle('Images from the training set',fontsize = 20)\nplt.show()","e4b34d65":"#Lets plot the images compressed using DCT\nn_coeff = 14 #retaining only 14 x 14 DCT coefficients of a 28 x 28 image\ncompress = np.round((1-(n_coeff**2\/num_pixels))*100,2)\nplt.figure(figsize=(20,6))\nfor i,j in enumerate([4,7,0,1,14,9,32,5,8,3]):\n    plt.subplot(2,5,i+1)\n    plt.subplots_adjust(top = 2, bottom = 1)\n    C = cv2.dct(Xtrain[j]) #dct of image\n    C_ = C[0:n_coeff,0:n_coeff] #taking only 30 x 30 i.e. 900 coefficients\n    C_n = np.zeros([28,28]) #original image shape\n    C_n[0:n_coeff,0:n_coeff] = C_\n    X_n = cv2.idct(C_n)\n    plt.imshow(X_n,cmap = 'gray')\n    plt.title(ytrain[j])\n    plt.axis('off')\nplt.suptitle('Images after compression using DCT by {} %'.format(compress),fontsize = 20)\nplt.show()","40ed5ea8":"#function for zigzag scanning\ndef zigzag(a):\n    b =  np.concatenate([np.diagonal(a[::-1,:], i)[::(2*(i % 2)-1)] for i in range(1-a.shape[0], a.shape[0])])\n    return b","bfc14334":"def dct_reduce(img,n):\n    #n is determine using how much energy we wanna retain\n    C = cv2.dct(img) #Perform discrete cosine transfor\n    c = zigzag(C) #scans image from top left to bottom right\n    s = c[:n] #retains first n entries\n    return s","2fb31aa2":"r = np.random.choice(range(len(Xtrain)))\nI = Xtrain[r,:,:] #select a random image\nC = cv2.dct(I) #calculate dct\nc = zigzag(C) #perform zigzag scanning\nfig,ax = plt.subplots(1,3,figsize=(20,5))\nax[0].imshow(I,cmap = 'gray') #original image\nax[0].set_title('Original Image')\nax[0].axis('off')\nax[1].imshow(C,cmap = 'gray')\nax[1].set_title('2D-DCT')\nax[1].axis('off')\nax[2].plot(c)\nax[2].grid(True)\nax[2].set_title('DCT coefficients arranged by zigzag scanning')\nplt.suptitle('Illustration of energy compaction property of 2D-DCT')\nplt.show()","5f835e7d":"n =   196 #retaining only first 196 coefficients\nE_i = np.round(np.linalg.norm(I),2)  #frobenius norm of matrix after flattening\nE_c = np.round(np.linalg.norm(C),2)\nE_cr = np.round(np.linalg.norm(zigzag(C)[:n]),2)\ne_ret = E_cr\/E_c*100\nprint(\"Frobenius norm of Image and its DCT is {} and {} respectively\".format(E_i,E_c))\nprint(\"Frobenius norm with only {} coefficients: {}\".format(n,E_cr))\nprint('Percentage of energy retained: {} %'.format(np.round(e_ret,2)))","85b4d22b":"#Perform DCT based dimension reduction on train set\nxtrain = np.zeros([len(Xtrain),n])\nfor i in range(len(Xtrain)):\n    Itr = Xtrain[i]\n    s_tr = dct_reduce(Itr,n)\n    xtrain[i] = s_tr #truncated \n    ","593a596c":"#Perform DCT based dimension reduction on validation set\nxval = np.zeros([len(Xval),n])\nfor i in range(len(Xval)):\n    Ival = Xval[i]\n    s_val = dct_reduce(Ival,n)\n    xval[i] = s_val","7d55e32c":"#Perform DCT based dimension reduction on test set\nxtest = np.zeros([len(Xtest),n])\nfor i in range(len(Xtest)):\n    Itest = Xtest[i]\n    s_test = dct_reduce(Itest,n)\n    xtest[i] = s_test","a77e50d9":"def PCA_extract(xtrain,ytrain,q):\n    # q  - number of principal component components to be retained .\n    U = []\n    M = []\n    for i in range(ytrain.max()+1):\n        D = xtrain[np.where(ytrain == i)]  #get data for each digit\n        m = np.mean(D, axis=0) #calculate the mean of each column\n        C = D - m #centralize each data point\n        V = np.cov(C.T) #calculate covariance\n        # eigendecomposition of covariance matrix\n        values, vectors = largest_eigsh(V,q,which = 'LM') # we want q eigen vectors corresponding to q largest magnitude eigen values\n        U.append(vectors)\n        M.append(m)  \n    return U,M","ada58b13":"q = 14\nU,M = PCA_extract(xtrain,ytrain,q)","cdec2736":"# Perform classification using PCA\ndef PCA_classify(test,U,M):\n    e = np.zeros([len(test),10])\n    for i in range(len(test)):\n        for j in range(10):\n            Uq = U[j] #get the eigen vector corresponding to the class (n x q)\n            te = test[i] - M[j] # (n,)\n            f =  np.dot(Uq.T,te) #lower dimension of the point (q x n)*(n x 1) = q x 1\n            z = np.dot(f,Uq.T) # projecting low dimensional data point onto subspace of each class\n            d = np.linalg.norm(z - te) #distance between testing datapoint and its projection\n            e[i,j]= d\n    # identify the target class for data points \n    ypred = np.argmin(e,axis =1 ) \n    return ypred","c274c7f1":"#classification on the validation set\nypred = PCA_classify(xval,U,M)","84594f06":"#plot confusion matrix\ncm = confusion_matrix(yval,ypred)\nplt.figure(figsize=(10,10))\nsns.heatmap(cm,cmap = 'Blues',annot = True,fmt = 'g')\nplt.xlabel('Predicted value')\nplt.ylabel('True')\nplt.show()","674b52d0":"accuracy = np.round(accuracy_score(yval,ypred),4)\nprint(\"The accuracy of PCA based classifier on validation set is {} %\".format(accuracy*100))","62eefa68":"#classification on the test set\nypred_t = PCA_classify(xtest,U,M)","823b938a":"results = pd.Series(ypred_t,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"dct_pca_mnist.csv\",index=False)","f2865578":"def plotLearningCurve(history,epochs):\n  epochRange = range(1,epochs+1)\n  fig , ax = plt.subplots(1,2,figsize = (10,5))\n  \n  ax[0].plot(epochRange,history.history['accuracy'],label = 'Training Accuracy')\n  ax[0].plot(epochRange,history.history['val_accuracy'],label = 'Validation Accuracy')\n  ax[0].set_title('Training and Validation accuracy')\n  ax[0].set_xlabel('Epoch')\n  ax[0].set_ylabel('Accuracy')\n  ax[0].legend()\n  ax[1].plot(epochRange,history.history['loss'],label = 'Training Loss')\n  ax[1].plot(epochRange,history.history['val_loss'],label = 'Validation Loss')\n  ax[1].set_title('Training and Validation loss')\n  ax[1].set_xlabel('Epoch')\n  ax[1].set_ylabel('Loss')\n  ax[1].legend()\n  fig.tight_layout()\n  plt.show()","8f0d6c09":"np.random.seed(12)\ni = Input(shape=len(xtrain.T))\nx = Dense(256, activation='relu')(i)\nx = Dropout(0.4)(x)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.3)(x)\nx = Dense(64, activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(32,activation = 'relu')(x)\nx = Dropout(0.1)(x)\nx = Dense(10, activation='softmax')(x)\nmodel = Model(i, x)\nmodel.summary()\n","39bc71e1":"model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(0.001),metrics=['accuracy']) # use categorical_crossentropy when one hot encoding is used","26809f23":" r  = model.fit(x=xtrain, \n          y=ytrain, \n          epochs=10,\n          batch_size = 16,\n          validation_data=(xval, yval),\n          )","167a5f8c":"plotLearningCurve(r,10)","c5f62642":"ypred = model.predict(xval)\nypred = np.argmax(ypred,axis = 1) ","8664514c":"#plot confusion matrix\ncm = confusion_matrix(yval,ypred)\nplt.figure(figsize=(10,10))\nsns.heatmap(cm,cmap = 'Blues',annot = True,fmt = 'g')\nplt.xlabel('Predicted value')\nplt.ylabel('True')\nplt.show()","f150b012":"print(\"Train score:\", model.evaluate(xtrain,ytrain))\nprint(\"Validation score:\", model.evaluate(xval,yval))","a9ba17f7":"accuracy = np.round(accuracy_score(yval,ypred),4)\nprint(\"The accuracy of ANN on validation set is {} %\".format(accuracy*100))","b26860cd":"ypred_t = model.predict(xtest)","6a0f1314":"# select the indix with the maximum probability\nresults = np.argmax(ypred_t,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")","45e190ba":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"dct_ann_mnist.csv\",index=False)","40eb4021":"For training set, the first column is the label. So we need to split the labels from the actual image data to perform classification. ","f9629c98":"Normalization is performed to reduce illumination differences","e36dff0f":"### Dataset Information\n\nThis MNIST dataset consists of 70000 images of handwritten digits where each digit of size 28 x 28 is flattened into a vector of size 784. These 70000 images are split into training set and testing set where the training set consists of 42000 images and the test set consists of 28000 images.\n\n### Problem Statement\nCorrectly classify the digits from the dataset\n\n### Overview\nIn this kernel, I first use 2D-DCT to reduce the dimension of the dataset. Two approaches are used to classify the dimension reduced datapoints - PCA and ANN. The classification is relatively faster as compared to the CNN approaches as now we are dealing with a vector for each data sample instead of an image.\n\n### References\nD. Ismailova and W. Lu, \"Fast classification of handwritten digits using 2D-DCT based sparse PCA,\" 2015 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM), 2015, pp. 131-135, doi: 10.1109\/PACRIM.2015.7334822.\n\nPlease upvote if you like my work. Thank you!","00fd545d":"# **3. Dimension reduction using DCT**","db8d3262":"# **1. Importing Libraries**","fe8b5a45":"### Using PCA for Classification","5c5c117a":"# **2. Data Handling and Exploration**","c4b6dc3b":"### Using ANN for classification","13701b47":"The dataset appears to be fairly balanced","eeda841f":"The 2D Discrete Cosine Transform (DCT) can be effectively used to compress digital images. The 2D DCT removes redundancies between neighbouring pixels and gives us uncorrelated transform coefficients which can be independently encoded. The DCT has a variety of useful properties such as energy preservability, compaction, seperability and decorrelation ability that makes it suitable for compression. \n\nFor dimensionality reduction, the energy compaction and signal decorrelation property of 2D DCT is very useful which allows representation of digital image by a relatively small number of DCT coefficients. To achieve this, zig zag scanning is used to scan each digit image of the data base of size 28 x 28 from top-left to bottom-right converting it into a vector of length 784. The DCT coefficients with larger magnitude tend to occur in the top-left corner. Hence, in a vector obtained by zig-zag scanning from top-left, larger DCT coefficients will be concentrated in the top part of the vector. Therefore, it is possible to get a fairly accurate estimation of the raw input image using a low dimensional vector obtained by retaining only the first several DCT coefficients.","e63296b6":"# **4. Performing Classification**"}}