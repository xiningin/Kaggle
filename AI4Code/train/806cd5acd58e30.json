{"cell_type":{"39796af2":"code","8cca70e9":"code","a3d1bfbe":"code","3f8f897c":"code","f67dcafa":"code","2fb230c1":"code","bd6ca5d8":"code","15026719":"code","942a396e":"code","89abdfe1":"code","e7e9af3f":"code","8e974b86":"code","18818c68":"code","977618d7":"code","7fe7e22c":"code","3090ce12":"code","f0b4ee64":"code","8c06c1b1":"code","bc408228":"code","f20031fe":"code","df7351e8":"code","c3958707":"code","fccd1d86":"code","60ff4539":"code","d942ae04":"code","8f1b2d32":"code","911da115":"code","4e5be793":"code","b1ce5f00":"code","0aae9e45":"code","0c6efa88":"code","6284707f":"code","d10e1d35":"code","320ad87e":"code","51cc51a5":"code","3b862b3b":"code","df324728":"code","9c31f57b":"code","256e014e":"code","0842ddb9":"code","bfb2a53c":"code","f9dc775b":"code","9cf98608":"code","4cac3055":"code","4c4fe607":"code","b4a46972":"code","f4648a43":"code","c00ec174":"code","cd6045c7":"code","d3dce380":"code","24bc3841":"code","006be017":"code","6210b18d":"code","135fb763":"code","bc317364":"code","5f5ecb04":"code","fb7b97ee":"code","cd6223d8":"code","4abbb69c":"code","d6773939":"code","8c1f229b":"code","7523f018":"code","5320b3b3":"code","e992959d":"code","460223a0":"code","68ff3e49":"code","2324cc19":"code","a80343ec":"code","521e0e75":"code","7c59afd8":"code","236a0e4c":"code","be118a7b":"code","0c6eeba0":"code","20fe89ec":"code","cc884218":"code","4a763e9f":"code","ca738219":"code","7a16bc84":"code","aabcb19d":"code","07be4946":"code","2f9ba8d6":"code","f00bc982":"code","c307a64d":"code","8bb7f333":"code","16c23a24":"code","ce67c2bc":"code","6789b86c":"code","c712fd4d":"code","ba551ce0":"code","3c2a97ab":"code","93bdeb09":"code","ac2166f3":"code","8b457535":"code","15570b83":"code","5401443b":"code","3f715c68":"code","79387782":"code","87fa0c76":"code","fcb988cf":"code","ccb1eb4f":"code","97575115":"code","7368d26a":"code","04735987":"code","4c660781":"code","a18c2d26":"code","571f1793":"code","05cd506e":"code","b0366e3e":"code","014152f7":"code","621c4dfc":"code","374de508":"code","e23f4b54":"code","8f70da81":"code","914664cc":"code","589f7c16":"code","4c9fc580":"code","c892e1d0":"code","28ad6f55":"code","f091311c":"code","08f59295":"code","785f4729":"code","c7e24cfd":"code","df2f2914":"code","6ad77ff6":"code","029a2233":"code","3a387df4":"code","5f61825d":"code","ac550220":"code","3e674a57":"code","94d1847b":"code","71fcc05a":"code","bfbeeca6":"code","d1ef4197":"code","5fdbaa4e":"markdown","a111d8bf":"markdown","3d16d7e2":"markdown","ba4b14dc":"markdown","cb830841":"markdown","499e12f5":"markdown","45a007e8":"markdown","22d0e794":"markdown","b96cbb4e":"markdown","da6f4ac9":"markdown","37b19cfc":"markdown","c30e748f":"markdown","102aa692":"markdown","0889fac6":"markdown","86c9834b":"markdown","055a0b7b":"markdown","a3c5eb2a":"markdown","4992c251":"markdown","d0f568dd":"markdown","64a2571e":"markdown","4eb0ef03":"markdown","2d6ac94b":"markdown","b854458e":"markdown","217f7ce2":"markdown","67b15e84":"markdown","c6ab7b11":"markdown","a8879541":"markdown","0ab435fc":"markdown","3d801e5f":"markdown","badbc65c":"markdown","ab5f42b0":"markdown","189f0e17":"markdown","d5dd1170":"markdown","35ddbf7e":"markdown","75e4e11a":"markdown","da8c8eda":"markdown","39176493":"markdown","ea2dc612":"markdown","3640b696":"markdown","0591e71f":"markdown","af08064f":"markdown","4e4f6955":"markdown","56cf9d0a":"markdown"},"source":{"39796af2":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","8cca70e9":"pd.options.display.max_columns = None                            \n\nfrom IPython.core.interactiveshell import InteractiveShell         \nInteractiveShell.ast_node_interactivity = \"all\"","a3d1bfbe":"data = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","3f8f897c":"data.shape","f67dcafa":"data.head()\n\n# Feature V1,v2,V3,........,V26,V27,V28 are PCA transformed. Only Time and Amount are not PCA transformed.","2fb230c1":"data.info()","bd6ca5d8":"data.describe()","15026719":"msng = data.isnull().sum()\nmsng_percnt = (msng\/len(data))*100\npd.concat([msng,msng_percnt], axis=1, keys=['Missing Values','Missing Percentage'])\n\n# No values are missing","942a396e":"data.drop_duplicates(keep='first',ignore_index=True,inplace=True)\ndata.shape\n\n# removed 1081 duplicate observations.","89abdfe1":"from sklearn.model_selection import train_test_split","e7e9af3f":"train,test = train_test_split(data,test_size=0.2, shuffle=True,random_state=10)\n\ntrain.shape\ntest.shape","8e974b86":"X_train = train.drop(['Class'],axis=1)\nY_train = train['Class']\nX_test = test.drop(['Class'],axis=1)\nY_test = test['Class']\n\nX_train.shape\nY_train.shape\nX_test.shape\nY_test.shape","18818c68":"data.corr()","977618d7":"plt.figure(figsize=(17,9))\nsns.heatmap(data.corr(), annot=True)\nsns.set(font_scale=0.7)\n\n# V1,V2,V3,........,V26,V27,V28 are more or less correlated only with Time, Class and Amount.","7fe7e22c":"data['Class'].value_counts(normalize=True)*100\n\n# 99.83% observation belongs to class 0. It's a imbalanced prblm","3090ce12":"fig, axes=plt.subplots(ncols=2, nrows=2, figsize=(17,9))\nsns.set(font_scale=1.5)\n\nsns.boxplot(x='Class',y='V2',data=data,ax=axes[0,0])\nsns.boxplot(x='Class',y='V4',data=data,ax=axes[0,1])\nsns.boxplot(x='Class',y='V11',data=data,ax=axes[1,0])\nsns.boxplot(x='Class',y='V19',data=data,ax=axes[1,1])\n\n# Value of V2, V4, V11 and V19 is more for fraud transactions.","f0b4ee64":"fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(17,9))\nsns.set(font_scale=1.5)\n\nsns.boxplot(x='Class', y='V10', data=data, ax=axes[0,0])\nsns.boxplot(x='Class', y='V12', data=data, ax=axes[0,1])\nsns.boxplot(x='Class', y='V14', data=data,  ax=axes[1,0])\nsns.boxplot(x='Class', y='V17', data=data, ax=axes[1,1])\n\n# Value of V10, V12, V14 and V17 is more for legitimate transactions","8c06c1b1":"data['Amount'].describe()\n# Amount is highly left skewed. Using log for Amount.","bc408228":"amount_log = np.log(data['Amount'] + 0.0001)\n\nplt.figure(figsize=(17,9))\nsns.boxplot(x=data['Class'],y=amount_log)\nplt.ylabel('Amount (in log)')\n\n# Transaction amount for fraud transaction is less than the legitimate transaction mostly.","f20031fe":"from sklearn.ensemble import RandomForestClassifier","df7351e8":"# creating a RF model with default Parameters and using all the possible core or threads of the Processer\nrdf = RandomForestClassifier(n_jobs=-1) \n\n# fitting the model on train\nrdf.fit(X_train,Y_train)","c3958707":"rdf_prd_trn = rdf.predict(X_train)\nrdf_prd_trn","fccd1d86":"from sklearn.metrics import confusion_matrix\n\nlabels = ['Legitimate Transactions','Fraud Transactions']\npd.DataFrame(confusion_matrix(Y_train,rdf_prd_trn), index=labels,columns=labels)","60ff4539":"from sklearn.metrics import accuracy_score\n\nprint('Accuracy of Random forest on train :', accuracy_score(Y_train,rdf_prd_trn))","d942ae04":"rdf_prd_tst = rdf.predict(X_test)\nrdf_prd_tst","8f1b2d32":"pd.DataFrame(confusion_matrix(Y_test,rdf_prd_tst),index=labels,columns=labels)","911da115":"print('Accuracy of Random forest on test :', accuracy_score(Y_test,rdf_prd_tst))","4e5be793":"from sklearn.metrics import classification_report\n\n# classification report for Random Forest with default parameters\nprint(classification_report(Y_test,rdf_prd_tst))","b1ce5f00":"# defining a function to plot feature importance\n\ndef plot_feature_importance(model,algo):\n    \n    # finding the important features\n    DF = pd.DataFrame({'Feature_Name':list(X_train),'Feature_importance':model.feature_importances_})\n    DF.sort_values(by='Feature_importance', ascending=False,inplace=True)\n    \n    # plotting the feature importance\n    plt.figure(figsize=(17,9))\n    sns.barplot(x=DF['Feature_importance'], y=DF['Feature_Name'])\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Feature')\n    plt.title(algo+\"'s Feature Importance\")","0aae9e45":"plot_feature_importance(rdf,'Random Forest')","0c6efa88":"# droping insignificant features from random forest\n\nX_train_RF = X_train.drop(['V23','V25','V5','V24','V22','Time','V19','Amount','V13','V28','V15','V20','V8','V2',\n                           'V27','V6','V1','V21','V26'],axis=1)","6284707f":"rdf_imp = RandomForestClassifier(n_jobs=-1) \n\nrdf_imp.fit(X_train_RF,Y_train)","d10e1d35":"rdf_imp_prd_trn = rdf_imp.predict(X_train_RF)\nrdf_imp_prd_trn","320ad87e":"pd.DataFrame(confusion_matrix(Y_train,rdf_imp_prd_trn),index=labels,columns=labels) ","51cc51a5":"print('Accuracy of Random forest (only significant features) on train :', accuracy_score(Y_train,rdf_imp_prd_trn))","3b862b3b":"# droping insignificant features from test\n\nX_test_RF = X_test.drop(['V23','V25','V5','V24','V22','Time','V19','Amount','V13','V28','V15','V20','V8','V2',\n                           'V27','V6','V1','V21','V26'],axis=1)","df324728":"rdf_imp_prd_tst = rdf_imp.predict(X_test_RF)\nrdf_imp_prd_tst","9c31f57b":"pd.DataFrame(confusion_matrix(Y_test,rdf_imp_prd_tst),index=labels,columns=labels)","256e014e":"print('Accuracy of Random forest (only significant features) on test :', accuracy_score(Y_test,rdf_imp_prd_tst))","0842ddb9":"# classification report for Random forest after removing insignificant features\n\nprint(classification_report(Y_test,rdf_imp_prd_tst))","bfb2a53c":"from sklearn.metrics import roc_auc_score,roc_curve","f9dc775b":"rdf_tst_proba = rdf.predict_proba(X_test)[:,1]","9cf98608":"print('AUC value for Random Forest:',roc_auc_score(Y_test,rdf_tst_proba))","4cac3055":"fbr,tbr,thrs = roc_curve(Y_test,rdf_tst_proba)","4c4fe607":"# Plotting the AUC curve\n\nplt.figure(figsize=(10,7))\nplt.plot([0,1],[0,1],'k')\nplt.plot(fbr,tbr,':',color='red')\nplt.xlabel('False Postive Rate (1-Specificity)')\nplt.ylabel('True Positive Rate (Recall)')\nplt.title('Receiver Operating Characteristics (ROC)')","b4a46972":"from xgboost import XGBClassifier","f4648a43":"xgb = XGBClassifier(tree_method='gpu_hist',n_jobs=-1) # model with default parameters","c00ec174":"xgb.fit(X_train,Y_train) # training the model","cd6045c7":"xgb_prd_trn = xgb.predict(X_train)\nxgb_prd_trn         ","d3dce380":"print(pd.DataFrame(confusion_matrix(Y_train,xgb_prd_trn), index=labels, columns=labels))","24bc3841":"from sklearn.metrics import accuracy_score\nprint('Accuracy of XGboost on train :', accuracy_score(Y_train,xgb_prd_trn))","006be017":"xgb_prd_tst = xgb.predict(X_test)\nxgb_prd_tst","6210b18d":"print(pd.DataFrame(confusion_matrix(Y_test,xgb_prd_tst), index=labels, columns=labels))","135fb763":"print('Accuracy of XGboost on test :', accuracy_score(Y_test,xgb_prd_tst))","bc317364":"print('AUC score for XGboost:', roc_auc_score(Y_test,xgb_prd_tst))","5f5ecb04":"# classification report for XGBoost with default parameters\n\nprint(classification_report(Y_test,xgb_prd_tst))","fb7b97ee":"# SMOTE works like KNN does. So, all the features to be used needs to be scaled.\n# All the features are result of PCA transaformation except Time and Amount. Doing feature scaling only for TIme & amount.\n\nfrom sklearn.preprocessing import StandardScaler\n\nX_train_scald = X_train.copy()               # copy of X_train\ncol_to_scald = ['Time','Amount']             # features to be scaled\nfeatr = X_train_scald[col_to_scald]\n\nscalr = StandardScaler().fit(featr)          # fitting StandardScaler","cd6223d8":"# transforming train data other than PCA transformed features\n\nx_trn = scalr.transform(featr)               # transforming X_train\nX_train_scald[col_to_scald] = x_trn","4abbb69c":"X_train_scald.head()","d6773939":"# No of observation of each class\n\nprint('Total no. of Fraud Transaction in train data :',sum(Y_train==1))\nprint('Total no of Legitimate Transaction in train data :',sum(Y_train==0))","8c1f229b":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=20, n_jobs=-1)","7523f018":"X_train_smote,Y_train_smote = smote.fit_sample(X_train_scald,Y_train)","5320b3b3":"# No of observation after over-sampling\n\nprint('Total no. of Fraud Transaction in train data :',sum(Y_train_smote==1))\nprint('Total no. of Legitimate Transaction in train data :',sum(Y_train_smote==0))","e992959d":"xgb_smote = XGBClassifier(tree_method='gpu_hist',n_jobs=-1)","460223a0":"xgb_smote.fit(X_train_smote,Y_train_smote)","68ff3e49":"xgb_smote_prd_trn = xgb_smote.predict(X_train_smote)\nxgb_smote_prd_trn          ","2324cc19":"print(pd.DataFrame(confusion_matrix(Y_train_smote,xgb_smote_prd_trn), index=labels, columns=labels))","a80343ec":"print('Accuracy of XGboost using SMOTE on train :', accuracy_score(Y_train_smote,xgb_smote_prd_trn))","521e0e75":"# feature scaling for test\n\nX_test_scaled = X_test.copy()\ntst_feat = X_test_scaled[col_to_scald]          \nx_tst = scalr.transform(tst_feat)\nX_test_scaled[col_to_scald] = x_tst","7c59afd8":"X_test_scaled.head()","236a0e4c":"xgb_smote_prd_tst = xgb_smote.predict(X_test_scaled)\nxgb_smote_prd_tst","be118a7b":"print(pd.DataFrame(confusion_matrix(Y_test,xgb_smote_prd_tst), index=labels, columns=labels))","0c6eeba0":"print('Accuracy of XGboost using SMOTE on test :', accuracy_score(Y_test,xgb_smote_prd_tst))","20fe89ec":"# classification report for XGBoost with default parameters and using SMOTE\n\nprint(classification_report(Y_test,xgb_smote_prd_tst))","cc884218":"print('AUC score for XGboost using SMOTE:',roc_auc_score(Y_test,xgb_smote_prd_tst))\n\n# AUC score for XGboost using SMOTE is less than XGboost without SMOTE.\n# But more fraud transactions is predicted correctly using SMOTE.","4a763e9f":"xgb_proba = xgb_smote.predict_proba(X_test)[:,1]","ca738219":"xgb_fbr, xgb_tbr, xgb_thrs = roc_curve(Y_test,xgb_proba)","7a16bc84":"# plotting ROC\n\nplt.figure(figsize=(10,7))\nplt.plot([0,1],[0,1],'k')\nplt.plot(xgb_fbr,xgb_tbr,':', color='red')\nplt.xlabel('False Positive Rate (1-Specificity)')\nplt.ylabel('True Positive Rate (Recall)')\nplt.title('Receiver Operating Characteristics (ROC) ')","aabcb19d":"# Parameters to be tuned\n\nxgb_param = {'n_estimators':[200,250,300],            # no. of iteation or tree\n             'learning_rate':[0.20,0.30,0.40],        # learning rate \n             'max_depth':[3,4,5,6],                   # max depth of a tree\n             'min_child_weight':[1,2,3,4],            # minimum sum of weights of all observations\n             'gamma':[0.1,0.2,0.3],                   # minimum loss reduction required \n             'colsample_bytree':[0.2,0.3,0.5]}        # fraction of random columns  ","07be4946":"from sklearn.model_selection import GridSearchCV\n\nxgb_grid = GridSearchCV(estimator=XGBClassifier(objective='binary:logistic',booster='gbtree',tree_method='gpu_hist'),\n                        param_grid=xgb_param,\n                        scoring='roc_auc',\n                        cv=2,\n                        n_jobs=-1,\n                        verbose=1)","2f9ba8d6":"xgb_grid.fit(X_train,Y_train)   # tuning the model","f00bc982":"xgb_grid.best_params_          # find best fit parameters","c307a64d":"# best fit model. All the parameter value is obtained after parameter tuning\n\nxgb_bst = XGBClassifier(n_estimators=200,\n                        max_depth=4,\n                        learning_rate=0.3,\n                        colsample_bytree=0.5,  \n                        gamma=0.2,\n                        min_child_weight=3,\n                        objective='binary:logistic',\n                        n_jobs=-1,\n                        booster='gbtree') ","8bb7f333":"xgb_bst.fit(X_train,Y_train)","16c23a24":"plot_feature_importance(xgb_bst,'XGboost') # the function is defined above ","ce67c2bc":"xgb_bst_prd_trn = xgb_bst.predict(X_train)\nxgb_bst_prd_trn","6789b86c":"print(pd.DataFrame(confusion_matrix(Y_train,xgb_bst_prd_trn), index=labels, columns=labels))","c712fd4d":"xgb_bst_prd_tst = xgb_bst.predict(X_test)\nxgb_bst_prd_tst","ba551ce0":"print(pd.DataFrame(confusion_matrix(Y_test,xgb_bst_prd_tst), index=labels, columns=labels))","3c2a97ab":"# Classificatio report for XGBoost after parameter tuning\n\nprint(classification_report(Y_test,xgb_bst_prd_tst))","93bdeb09":"print('Accuracy of XGboost after parameter tuning on test :', accuracy_score(Y_test,xgb_bst_prd_tst))","ac2166f3":"xgb_bst_proba = xgb_bst.predict_proba(X_test)[:,1]","8b457535":"xgb_bst_fpr, xgb_bst_tpr, xgb_bst_thrs = roc_curve(Y_test,xgb_bst_proba)","15570b83":"# plotting AUC curve\n\nplt.figure(figsize=(10,7))\nplt.plot([0,1],[0,1],'k')\nplt.plot(xgb_bst_fpr, xgb_bst_tpr,':',color='red')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Recall)')\nplt.title('Receiver Operating Characteristics (ROC)')","5401443b":"print('AUC score for XGboost after parameter tuning :', roc_auc_score(Y_test,xgb_bst_proba))","3f715c68":"# Best fit parameters for XGboost SMOTE\n\nxgb_bst_SMOTE = XGBClassifier(n_estimators=200,\n                              max_depth=4,\n                              learning_rate=0.3,\n                              colsample_bytree=0.5,\n                              gamma=0.2,\n                              min_child_weight=3,\n                              objective='binary:logistic',\n                              n_jobs=-1,\n                              booster='gbtree')","79387782":"xgb_bst_SMOTE.fit(X_train_smote,Y_train_smote)","87fa0c76":"xgb_bst_SMOTE_prd_trn = xgb_bst_SMOTE.predict(X_train_smote)\nxgb_bst_SMOTE_prd_trn","fcb988cf":"print(pd.DataFrame(confusion_matrix(Y_train_smote,xgb_bst_SMOTE_prd_trn), index=labels, columns=labels))","ccb1eb4f":"xgb_bst_SMOTE_prd_tst = xgb_bst_SMOTE.predict(X_test)\nxgb_bst_SMOTE_prd_tst","97575115":"print(pd.DataFrame(confusion_matrix(Y_test,xgb_bst_SMOTE_prd_tst), index=labels, columns=labels))","7368d26a":"# # Classificatio report for XGBoost after parameter tuning and using SMOTE\n\nprint(classification_report(Y_test,xgb_bst_SMOTE_prd_tst))","04735987":"print('Accuracy for XGboost using SMOTE :', accuracy_score(Y_test,xgb_bst_SMOTE_prd_tst))","4c660781":"xgb_bst_SMOTE_proba = xgb_bst_SMOTE.predict_proba(X_test)[:,1]\nxgb_bst_SMOTE_proba","a18c2d26":"XGB_fpr, XGB_tpr, XGB_thres = roc_curve(Y_test,xgb_bst_SMOTE_proba)","571f1793":"# Potting AUC curve\n\nplt.figure(figsize=(10,7))\nplt.plot([0,1],[0,1],'k')\nplt.plot(XGB_fpr, XGB_tpr, ':', color='red')\nplt.xlabel('False Positive Rate (1-Specificity)')\nplt.ylabel('True Positive Rate (Recall)')\nplt.title('Receiver Operating Characteristics (ROC)')","05cd506e":"print('AUC score for XGboost using SMOTE :', roc_auc_score(Y_test,xgb_bst_SMOTE_proba))","b0366e3e":"from sklearn.linear_model import LogisticRegression","014152f7":"log_reg = LogisticRegression()","621c4dfc":"log_reg.fit(X_train_scald,Y_train)","374de508":"log_reg_prd_trn = log_reg.predict(X_train_scald)\nlog_reg_prd_trn","e23f4b54":"print(pd.DataFrame(confusion_matrix(Y_train,log_reg_prd_trn), index=labels, columns=labels))","8f70da81":"print('Accuracy of Logistic Regression on train:', accuracy_score(Y_train,log_reg_prd_trn))","914664cc":"log_reg_prd_tst = log_reg.predict(X_test_scaled)\nlog_reg_prd_tst","589f7c16":"print(pd.DataFrame(confusion_matrix(Y_test, log_reg_prd_tst), index=labels, columns=labels))","4c9fc580":"print('Accuracy of Logistic Regression on test:', accuracy_score(Y_test,log_reg_prd_tst))","c892e1d0":"# classification report for Logistic Regression with default parameters\n\nprint(classification_report(Y_test,log_reg_prd_tst))","28ad6f55":"print('AUC score of Logistic Regression on test:', roc_auc_score(Y_test,log_reg_prd_tst))","f091311c":"log_reg_smote = LogisticRegression()","08f59295":"log_reg_smote.fit(X_train_smote,Y_train_smote)","785f4729":"log_reg_smote_prd_trn = log_reg_smote.predict(X_train_smote)\nlog_reg_smote_prd_trn","c7e24cfd":"print(pd.DataFrame(confusion_matrix(Y_train_smote,log_reg_smote_prd_trn), index=labels, columns=labels))","df2f2914":"print('Accuracy of logistic regression using SMOTE :', accuracy_score(Y_train_smote,log_reg_smote_prd_trn))","6ad77ff6":"print('AUC score for Logistic Regression using SMOTE :',roc_auc_score(Y_train_smote,log_reg_smote_prd_trn))","029a2233":"log_reg_smote_prd_tst = log_reg_smote.predict(X_test_scaled)\nlog_reg_smote_prd_tst","3a387df4":"print(pd.DataFrame(confusion_matrix(Y_test,log_reg_smote_prd_tst), index=labels, columns=labels))","5f61825d":"print('Accuracy of logistic regression using SMOTE :', accuracy_score(Y_test,log_reg_smote_prd_tst))","ac550220":"# classification report for Logistic Regression with default parameters and using SMOTE\n\nprint(classification_report(Y_test,log_reg_smote_prd_tst))","3e674a57":"print('AUC score for logistic Regression with SMOTE :',roc_auc_score(Y_test,log_reg_smote_prd_tst))","94d1847b":"log_reg_proba = log_reg_smote.predict_proba(X_test_scaled)[:,1]","71fcc05a":"log_reg_fpr, log_reg_tpr, log_reg_thrs = roc_curve(Y_test,log_reg_proba)","bfbeeca6":"# Potting AUC curve\n\nplt.figure(figsize=(10,7))\nplt.plot([0,1],[0,1],'k')\nplt.plot(log_reg_fpr, log_reg_tpr, ':', color='red')\nplt.xlabel('False Positive Rate (1-Specificity)')\nplt.ylabel('True Positive Rate (Recall)')\nplt.title('Receiver Operating Characteristics (ROC)')","d1ef4197":"print('AUC score for logistic Regression with SMOTE :',roc_auc_score(Y_test,log_reg_smote_prd_tst))","5fdbaa4e":"# Check for duplicate Data","a111d8bf":"# ROC for XGboost using SMOTE with best fit parameters","3d16d7e2":"# Loading Libraries and Dataset","ba4b14dc":"# On train","cb830841":"# On test","499e12f5":"# On test","45a007e8":"# Using SMOTE","22d0e794":"# ROC for XGBoost using SMOTE","b96cbb4e":"# XGboost using SMOTE","da6f4ac9":"# Prediction using Random Forest","37b19cfc":"# Logistic Regression using SMOTE","c30e748f":"# XGboost best fit model","102aa692":"# Pearson Correlation","0889fac6":"# Removing Insignificant features from the RF model","86c9834b":"# Dividing data into train and test","055a0b7b":"# Hyperparameter Tuning for XGboost","a3c5eb2a":"# On train","4992c251":"# Kindly upvote if you liked my work.\n# Your suggestions to improve the model is most welcomed.","d0f568dd":"# Feature Importance for Random Forest","64a2571e":"# ROC for Random Forest","4eb0ef03":"# Prediction using XGboost","2d6ac94b":"# On test","b854458e":"# Using best fit parameters to train XGboost using SMOTE","217f7ce2":"# On test","67b15e84":"# On test","c6ab7b11":"# Problem Statement\n\nIt's very important for credit card companies to recognize and stop a fraud transaction. Identifying a fraud transaction out of total no of legitimate transaction is quite a difficulat task as fraud transaction out of legitimate transaction is very less. But ML model can help in recognizing a fraud transaction. The final aim of this model is to recognize a fraud transaction so that customers are not charged for items that they did not purchased.","a8879541":"# Data Cleaning","0ab435fc":"# Overview of Dataset","3d801e5f":"# On test","badbc65c":"# On train","ab5f42b0":"# On test","189f0e17":"# On test","d5dd1170":"# On train","35ddbf7e":"# On train","75e4e11a":"# Prediction using Logistic Regression","da8c8eda":"# On train","39176493":"# ROC for Logistic Regression using SMOTE","ea2dc612":"# ROC for XGboost best fit","3640b696":"# Exploratory Data Analysis","0591e71f":"# Feature importance for XGboost","af08064f":"# Conclusion\n\nThree algorithm which are used - Random Forest, XGboost (with SMOTE and without SMOTE) and Logistic Regression (with and without SMOTE).\n\n* Logistic Regression yield a better AUC score with SMOTE (AUC score without SMOTE=0.7587705829406469 and AUC score with SMOTE=0.9349958525264291). Using Logistic Regression predicts most of the Fraud cases correctly but it also predicts 1367 legitimate transaction to be fraud. We can conclude that logistic regression is getting biased towards Fraud transactions. I have also tried Logistic Regression using L1 and L2 regularization but was yielding very bad results.\n\n\n* XGboost with default parameters yields slightly better result (XGB AUC score=0.9680246225947758) than RF with default parameters (RF AUC score=0.951133936923104).\n\n\n* XGboost with default parameters using SMOTE results in overall bad AUC score (AUC score without SMOTE=0.9680246225947758)  than without using SMOTE (AUC score with SMOTE=0.8998499849984999). But using SMOTE predicts more Fraud trasactions correctly.\n\n\n* XGboost after parameter tuning without using SMOTE yields better AUC overall (AUC score without SMOTE=0.9831160970768358) than with using SMOTE (AUC score using SMOTE=0.9776529556069793). But using SMOTE yields better prediction for Fraud transaction correctly.\n\nIn this scenario, predicting fraud transaction is more important but we also need to take care about the legitimate transaction. So, we can conclude that XGboost with SMOTE and parameter tuning yields best outcome.","4e4f6955":"# On train","56cf9d0a":"# On train"}}