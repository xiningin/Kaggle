{"cell_type":{"6096fdae":"code","a3c198d4":"code","1c30ecdf":"code","d8ac3b21":"code","a1435f43":"code","adc0972f":"code","4e948141":"code","0bd3b7ec":"code","6af6633e":"code","d5a75fc1":"code","3aedb07e":"code","3455579d":"code","62b1a399":"code","62b61e1f":"code","6dc850d6":"code","e5b4f08b":"code","9abe1138":"code","d78d4765":"code","ecf2c255":"code","c0d5c9fe":"code","8c4d7cba":"code","d9c15ec1":"code","7031f566":"code","6f9c35a3":"code","528ce0f1":"code","ca0263c8":"code","be2fdf22":"code","c6d5a247":"code","307aecd6":"code","11816156":"markdown","8dd7a881":"markdown","0ea7c35d":"markdown","edf85d79":"markdown","92efeed3":"markdown","1c5efe8e":"markdown","f276e7eb":"markdown","32d473e9":"markdown","8f6b7a8a":"markdown","889419b1":"markdown","21f02683":"markdown","20ecd8a0":"markdown","7cfd8014":"markdown","6fdda49e":"markdown"},"source":{"6096fdae":"import pandas as pd\nimport numpy as np\nimport os\n\n# Plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# SKLearn related libraries\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\n\n# Classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\n# Metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Keras NN related libraries\nfrom keras import layers\nfrom keras.layers import Input, Dense\nfrom keras.models import Model, Sequential \nfrom keras import regularizers","a3c198d4":"data_path = '\/kaggle\/input\/creditcardfraud\/creditcard.csv'\n\n# print(os.path.exists(data_path))\n\n# Load the data\ncard_df = pd.read_csv(data_path, header=0)","1c30ecdf":"card_df.info()\nprint(\"====\"*30)\ncard_df.head()","d8ac3b21":"card_df.describe().T","a1435f43":"# Unique class labels\nprint(f\"Unique classes in the dataset are : {np.unique(card_df['Class'])}\" )","adc0972f":"card_df.groupby('Class')['Class'].count().plot.bar(logy=True)","4e948141":"# Change the time attribute in day\ncard_df['Time'] = card_df['Time'].apply(lambda t: (t\/3600) % 24 )","0bd3b7ec":"# Sampling of data\nnormal_trans = card_df[card_df['Class'] == 0].sample(4000)\nfraud_trans = card_df[card_df['Class'] == 1]","6af6633e":"reduced_set = normal_trans.append(fraud_trans).reset_index(drop=True)","d5a75fc1":"print(f\"Cleansed dataset shape : {reduced_set.shape}\")","3aedb07e":"# Splitting the dataset into X and y features\ny = reduced_set['Class']\nX = reduced_set.drop('Class', axis=1)\n","3455579d":"print(f\"Shape of Features : {X.shape} and Target: {y.shape}\")","62b1a399":"def dimensionality_plot(X, y):\n    sns.set(style='whitegrid', palette='muted')\n    # Initializing TSNE object with 2 principal components\n    tsne = TSNE(n_components=2, random_state = 42)\n    \n    # Fitting the data\n    X_trans = tsne.fit_transform(X)\n    \n    plt.figure(figsize=(12,8))\n    \n    plt.scatter(X_trans[np.where(y == 0), 0], X_trans[np.where(y==0), 1], marker='o', color='g', linewidth='1', alpha=0.8, label='Normal')\n    plt.scatter(X_trans[np.where(y == 1), 0], X_trans[np.where(y==1), 1], marker='o', color='k', linewidth='1', alpha=0.8, label='Fraud')\n    \n    plt.legend(loc = 'best')\n    \n    plt.show()\n\n","62b61e1f":"# Invoking the method dimensionality_plot\ndimensionality_plot(X, y)","6dc850d6":"scaler = RobustScaler().fit_transform(X)\n\n# Scaled data\nX_scaled_normal = scaler[y == 0]\nX_scaled_fraud = scaler[y == 1]","e5b4f08b":"print(f\"Shape of the input data : {X.shape[1]}\")","9abe1138":"# Input layer with a shape of features\/columns of the dataset\ninput_layer = Input(shape = (X.shape[1], ))\n\n# Construct encoder network\nencoded = Dense(100, activation= 'tanh', activity_regularizer=regularizers.l1(10e-5))(input_layer)\nencoded = Dense(50, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(encoded)\nencoded = Dense(25, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(encoded)\nencoded = Dense(12, activation = 'tanh', activity_regularizer=regularizers.l1(10e-5))(encoded)\nencoded = Dense(6, activation='relu')(encoded)\n\n# Decoder network\ndecoded = Dense(12, activation='tanh')(encoded)\ndecoded = Dense(25, activation='tanh')(decoded)\ndecoded = Dense(50, activation='tanh')(decoded)\ndecoded = Dense(100, activation='tanh')(decoded)\n\noutput_layer = Dense(X.shape[1], activation='relu')(decoded)\n\n# Building a model\nauto_encoder = Model(input_layer, output_layer)","d78d4765":"# Compile the auto encoder model\nauto_encoder.compile(optimizer='adadelta', loss='mse')\n\n# Training the auto encoder model\nauto_encoder.fit(X_scaled_normal, X_scaled_normal, batch_size=32, epochs=20, shuffle=True, validation_split=0.20)","ecf2c255":"latent_model = Sequential()\nlatent_model.add(auto_encoder.layers[0])\nlatent_model.add(auto_encoder.layers[1])\nlatent_model.add(auto_encoder.layers[2])\nlatent_model.add(auto_encoder.layers[3])\nlatent_model.add(auto_encoder.layers[4])","c0d5c9fe":"normal_tran_points = latent_model.predict(X_scaled_normal)\nfraud_tran_points = latent_model.predict(X_scaled_fraud)\n# Making as a one collection\nencoded_X = np.append(normal_tran_points, fraud_tran_points, axis=0)\ny_normal = np.zeros(normal_tran_points.shape[0])\ny_fraud = np.ones(fraud_tran_points.shape[0])\nencoded_y = np.append(y_normal, y_fraud, axis=0)\n","8c4d7cba":"# Calling TSNE plot function\ndimensionality_plot(encoded_X, encoded_y)","d9c15ec1":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nX_enc_train, X_enc_test, y_enc_train, y_enc_test = train_test_split(encoded_X, encoded_y, test_size=0.3)","7031f566":"print(f\"Encoded train data X: {X_enc_train.shape}, Y: {y_enc_train.shape}, X_test :{X_enc_test.shape}, Y_test: {y_enc_test.shape}\")\nprint(f\"Actual train & test data X: {X_train.shape}, Y: {X_train.shape}, X_test :{X_test.shape}, Y_test: {y_test.shape}\")","6f9c35a3":"# Instance of SVM\nsvc_clf = SVC()\n\nsvc_clf.fit(X_train, y_train)\n\nsvc_predictions = svc_clf.predict(X_test)","528ce0f1":"print(\"Classification report \\n {0}\".format(classification_report(y_test, svc_predictions)))","ca0263c8":"print(\"Accuracy of SVC \\n {:.2f}\".format(accuracy_score(y_test, svc_predictions)))","be2fdf22":"lr_clf = LogisticRegression()\n\nlr_clf.fit(X_enc_train, y_enc_train)\n\n# Predict the Test data\npredictions = lr_clf.predict(X_enc_test)","c6d5a247":"print(\"Classification report \\n {0}\".format(classification_report(y_enc_test, predictions)))\n","307aecd6":"print(\"Accuracy score is : {:.2f}\".format(accuracy_score(y_enc_test, predictions)))","11816156":"## Loading Dataset","8dd7a881":"## Using Autoencode to encode data","0ea7c35d":"## Linear Classifier\n\nNow let's apply linear classifier to classify the data and observe the result. We will use **Logistic Regression** to build the model.","edf85d79":"## Conclusion\n\nIn this analysis, we have found that Support Vector Machine classifier is able to classify the data upto **93%** without encoding and decoding. However, the effect of autoencoder comes when the data gets transformed from non-linear to linearly separable then linear classifier like **Logistic Regression** could perform in a better way.\n\nThe accuracy score of Logistic Regression can go upto **97%**, this is something not happens too often in logistic algorithm. ","92efeed3":"## Visualize the data with t-SNE\n\nTNSE(t-distributed Stochastic Neighbor Embedding) is one of the dimensionality reduction method other than PCA and SVD. This will supress some noise and speed up the computation of pairwise distance between samples. ","1c5efe8e":"## Trasnformation\n\nData transformation is one of the steps in data processing. We need to transform certain attributes value so that it makes sense in the further analysis. ","f276e7eb":"We can observe that the encoded fraud data points have been moved towards one cluster, whereas there are only few fraud transaction datapoints are there among the normal transaction data points. \n\n## Split into Train and Test","32d473e9":"## Dataset-Credit card transactions\n\nThe dataset we're going to use in this kernel is `creditcard.csv` which basically a credit card transactions in the past. Using an encoder-decorder system we will find the hidden data points and apply a linear classifier to detect the Fraud(1) or Genuine\/not-fraud (0) credit card transactions. \n\n## Import dependent libraries","8f6b7a8a":"## Building Autoencoder Model","889419b1":"## Normalize and Scale the features","21f02683":"# Autoencoder Architecture\n\n## Introduction\nAutoencoder is a kind of Deep Learning architectures. Autoencoder architecture encompasses two sub-systems as encoder and decoder. Both these sub-systems are made up of independent Neural Network with a defined set of layers and activation functions. The fundamental characteristic feature of Autoencoder architecture is extracting the latent(hidden) data points from the given dataset. \n\nThis notebook is created out of inspiration from the post on [Geekforgeek](https:\/\/www.geeksforgeeks.org\/ml-classifying-data-using-an-auto-encoder\/)","20ecd8a0":"## Exploratory Data Analysis\n\n1. Exploring on statistics information about the data","7cfd8014":"## Split the Dataset","6fdda49e":"## Non-linear Classifier"}}