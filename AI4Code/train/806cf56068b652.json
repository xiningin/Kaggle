{"cell_type":{"2f21a0a3":"code","189220de":"code","54cea2b6":"code","43a5cd7a":"code","b4a6e384":"code","59c8f1ce":"code","7e698ed6":"code","ae340af2":"code","f7109141":"code","28ad4109":"code","010a564d":"code","2cec0b74":"markdown","870782a7":"markdown","e163a3d3":"markdown","32405924":"markdown","130d5703":"markdown","0f9cbc52":"markdown","453185eb":"markdown","ab88a8cf":"markdown","8a2fb9fb":"markdown","e591f0e5":"markdown","b9e8f6d2":"markdown","f1c61fc8":"markdown","d611bb1b":"markdown","af571495":"markdown"},"source":{"2f21a0a3":"import pandas as pd\n\n# Path of the file to read\nmelbourne_file_path = \"..\/input\/melbourne-housing-snapshot\/melb_data.csv\"\n\n# Read the file into a variable melbourne_data\nmelbourne_data = pd.read_csv(melbourne_file_path)\n\n# Print a summary statistics of the melbourne data\nmelbourne_data.describe()","189220de":"# Print list of columns\nprint(melbourne_data.columns)\n\n# Remove missing values\nmelbourne_data = melbourne_data.dropna(axis = 0)\n\n# Selecting the prediction target (y)\ny = melbourne_data[\"Price\"]","54cea2b6":"# Specifying the features\nmelbourne_features = [\"Rooms\", \"Bathroom\", \"Landsize\", \"Lattitude\", \"Longtitude\"]\nX = melbourne_data[melbourne_features]\n\n# Print the first 5 rows\nprint(X.head())\n\n# Print summary statistics of data\nprint(X.describe())","43a5cd7a":"from sklearn.tree import DecisionTreeRegressor\n\n# Define the model\nmelbourne_model = DecisionTreeRegressor(random_state = 1)\n\n# Fit the model\nmelbourne_model.fit(X, y) # Feed the data that has been cleaned and prediction target\n\n# Print the prediction for the first 5 houses\nprint(\"In-sample predictions: \", melbourne_model.predict(X.head()))\nprint(\"Actual target values: \", y.head().tolist())","b4a6e384":"from sklearn.metrics import mean_absolute_error\n\npredicted_home_prices = melbourne_model.predict(X)\nmean_absolute_error(y, predicted_home_prices)","59c8f1ce":"from sklearn.model_selection import train_test_split\n\n# Split data into training and validation data for features and target\n# The split is based on a random number generator.\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1)\n\n# Define model\nmelbourne_model = DecisionTreeRegressor(random_state = 1)\n\n# Fit model\nmelbourne_model.fit(train_X, train_y)\n\n# Get predicted prices on validation data\nval_predictions = melbourne_model.predict(val_X)\nprint(mean_absolute_error(val_y, val_predictions))","7e698ed6":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes = max_leaf_nodes, random_state = 0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)","ae340af2":"# Compare MAE with different values of max_leaf_nodes\ncandidate_max_leaf_nodes = [280, 290, 300, 310, 320] # best leaf nodes in this case is 300\n\n# 1. Using for loop\n# for max_leaf_nodes in candidate_max_leaf_nodes:\n#     my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n#     print(\"Max leaf nodes: %d \\t\\t Mean Absolute Error: %d\" %(max_leaf_nodes, my_mae))\n    \n# 2. Using dictionary and store the smallest MAE in best_tree_size\nscores = {leaf_size: get_mae(leaf_size, train_X, val_X, train_y, val_y) for leaf_size in candidate_max_leaf_nodes}\nbest_tree_size = min(scores, key=scores.get)\nbest_tree_size","f7109141":"# Fill in argument to make optional size\nfinal_model = DecisionTreeRegressor(max_leaf_nodes = best_tree_size, random_state = 1)\n\n# Fit the final model\nfinal_model.fit(X, y)","28ad4109":"import pandas as pd\n\n# Load the data\nmelbourne_file_path = \"..\/input\/melbourne-housing-snapshot\/melb_data.csv\"\nmelbourne_data = pd.read_csv(melbourne_file_path)\n\n# Filter rows with missing values\nmelbourne_data = melbourne_data.dropna(axis = 0)\n\n# Choose target and features\ny = melbourne_data[\"Price\"]\nmelbourne_features = [\"Rooms\", \"Bathroom\", \"Landsize\", \"BuildingArea\", \"YearBuilt\", \"Lattitude\", \"Longtitude\"]\nX = melbourne_data[melbourne_features]\n\nfrom sklearn.model_selection import train_test_split\n\n# Split data into training and validation data for both features and target\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)","010a564d":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# We build a random forest, similar to how we built a decision tree\nforest_model = RandomForestRegressor(random_state = 1)\n# forest_model.fit(train_x, train_y)\n\n# To improve accuracy, we will create a forest model on all training data\nforest_model.fit(X, y)\nmelb_preds = forest_model.predict(val_X)\nprint(mean_absolute_error(val_y, melb_preds))","2cec0b74":"# Basic Machine Learning\n\nPutu has made millions of dollars speculating on real estate. You ask Putu how he's predicted real estate values in the past and he says that he's **identified price patterns from houses in the past, and uses those patterns to make predictions for new houses**.\n\n**Machine learning** works the same way. We'll start with a model called **Decision Tree**. There are fancier models that give more accurate predictions. But decision trees are easy to understand, and they are the basic building block for some of the best models in data science.\n\n![](https:\/\/i.imgur.com\/7tsb5b1.png)","870782a7":"# Choosing \"Features\"\nThe columns that are inputted into our model (and later used to make predictions) are called \"**features**.\" In this case, those would be the columns to determine the home price. Sometimes, you will use all columns except the target as features. Other times it's better with fewer features. For now, we'll build a model with few features. We select multiple features by providing a list of column names (in string) inside brackets.","e163a3d3":"# Building Your Model\nWe will use the **scikit-learn** to create models. The steps to building and using a model are:\n\n* **Define**: What type of model will it be? A decision tree? Other type of model? Some other parameters of the model type are specified too.\n* **Fit**: Capture patterns from provided data. This is the heart of modeling.\n* **Predict**: Just what it sounds like.\n* **Evaluate**: Determine how accurate the model's predictions are.\n\nMany ML models allow randomness in **model training**. Specifying number for random_state ensures you get the same results in each run (good practice). You use any number, and model quality won't depend meaningfully on exactly what value you choose. We now have a fitted model that we can use to make predictions.\n\nIn practice, you'll want to make predictions for new houses coming on the market rather than the houses we already have prices for. But we'll make predictions for the first few rows of the training data to see how the predict function works.","32405924":"# Interpreting Data\n**Count** shows how many rows have non-missing values. Missing values arise for many reasons. For example, the size of the 2nd bedroom wouldn't be collected when surveying a 1 bedroom house.\n\n**Mean** is the average. **std** is the standard deviation, which measures how numerically spread out the values are.\n\nTo interpret the **min, 25%, 50%, 75% and max values**, imagine **sorting** each column from low to high. The first (smallest) value is the **min**. If you go a quarter way through the list, you'll find a number that is **bigger than 25%** of the values and **smaller than 75%** of the values. That is the 25% value (pronounced \"**25th percentile**\"). The 50th and 75th percentiles are defined the same, and the **max** is the largest number.","130d5703":"There is likely room for further improvement, but this is a big improvement over the best decision tree error of 250,000. There are parameters which allow you to change the performance of the Random Forest much as we changed the maximum depth of the single decision tree. But one of the best features of Random Forest models is that they generally work reasonably even without this tuning.","0f9cbc52":"# Random Forests\nEven today's most sophisticated modeling techniques often encounter **underfitting** and **overfitting**. But, many models have clever ideas that can lead to better performance. The **random forest** uses many **trees**, and it makes a prediction by averaging the predictions of each **component tree**. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters.","453185eb":"# The Problem with \"In-Sample\" Scores\nWe just computed an \"in-sample\" score where we used a single \"sample\" of houses for both **building** the model and **evaluating** it.\n\nImagine in the large real estate market, door color is unrelated to home price. However, in the sample data you used to build the model, all homes with green doors were very expensive. The model's job is to find patterns that predict home prices, so it will see this pattern, and it will always predict high prices for homes with green doors.\n\nSince this pattern was derived from the training data, the model will **appear** accurate in the training data. But if this pattern doesn't hold when the model sees new data, the model would be very inaccurate when used in practice.\n\nSince models' practical value come from making predictions on new data, we should measure performance on data that wasn't used to build the model. The most straightforward way to do this is to exclude some data from the model-building process, and then use those to test the model's accuracy on data it hasn't seen before. This data is called **validation data**.","ab88a8cf":"# Model Validation\nMeasuring **model quality** is the key to iteratively improving your models. Model's predictions should be close to what actually happens (*don't compare against training data!*). \n\nIf you compare predicted and actual home values for 10,000 houses, you'll find mix of good and bad predictions. Looking through a list of 10,000 predicted and actual values would be pointless. We need to summarize this into a **single metric**. There are many metrics for summarizing model quality, but we'll start with **Mean Absolute Error** (MAE).\n\n> error = actual - predicted\n\nIf a house cost 150,000 and you predicted 100,000, the error is 50,000. With MAE metric, we take the absolute value of each error. In plain English, it can be said as: \n\n> On average, our predictions are off by about X.\n\n","8a2fb9fb":"![](http:\/\/i.imgur.com\/prAjgku.png)\n\n**Decision Tree 1** makes more sense, because it captures the reality that houses with more bedrooms tend to sell at higher prices. But, it doesn't capture other **factors** affecting home price. You can capture more factors using a tree that has more \"**splits**\", called \"**deeper**\" trees.\n\n![](http:\/\/i.imgur.com\/R3ywQsR.png)\n\nYou predict the price of any house by **tracing** through the decision tree, picking the path corresponding to the house's characteristics. The **predicted price** for the house is at the bottom of the tree, called a **leaf**.\n\nThe **splits** and **values** at the leaves will be determined by the data.","e591f0e5":"It divides houses into only two categories. The predicted price comes from the **historical average price of houses in the same category**. We use data to decide how to break the houses into two groups, and then again to determine the predicted price in each group. This step of capturing patterns from data is called **fitting** or **training** the model. The data used to **fit** the model is called **training data**.\n\nThe details of how the model is fit (e.g. how to split up the data) will be discussed later. After the model has been **fit**, you can apply it to new data to **predict** prices of additional homes.","b9e8f6d2":"# Optimizing The Decision Tree\nThere are few alternatives for controlling the tree depth, and many allow for some routes through the tree to have greater depth than other routes. But the **max_leaf_nodes** argument provides a very sensible way to control overfitting vs underfitting. The more leaves we allow the model to make, the more we move from the underfitting area in the above graph to the overfitting area.\n\nWe can use a utility function to help compare MAE scores from different values for max_leaf_nodes. Of the options listed, 250 is the optimal number of leaves. Here's the takeaway: \n* **Overfitting**: capturing false patterns that won't happen in the future, leading to less accurate predictions.\n* **Underfitting**: failing to capture relevant patterns, again leading to less accurate predictions.\n* We use **validation data**, which isn't used in model training, to measure a candidate model's accuracy. This lets us try many candidate models and keep the best one.","f1c61fc8":"# Underfitting and Overfitting\nNow that you have a reliable way to measure model accuracy, you can experiment with alternative models and see which gives the best predictions.\n\nA tree's depth is a measure of how many splits it makes before coming to a prediction.\n\n![](http:\/\/i.imgur.com\/R3ywQsR.png)\n\nIn practice, it's not uncommon for a tree to have 10 splits between the top level (all houses) and a leaf. As the tree gets deeper, the dataset gets sliced up into leaves with fewer houses. A tree with 1 split divides data into 2 groups. A tree with 2 split divides data into 4 groups and so on. If we keep doubling the number of groups by adding more splits at each level, we'll have 2^10 groups of houses by the time we get to the 10th level. That's 1024 leaves.\n\nWhen we divide the houses amongst many leaves, we also have fewer houses in each leaf. Leaves with very few houses will make predictions that are close to actual values, but they may make unreliable predictions for new data because each prediction is based on only a few houses.\n\nThis phenomenon is called **overfitting**, where a model matches the training data almost perfectly, but does poorly in validation and other new data. On the flip side, if we make our tree very shallow, it doesn't divide up the houses into very distinct groups.\n\nOn the other hand, if a tree divides houses into only 2 or 4, each group still has a wide variety of houses. Resulting predictions may be far off for most houses, even in the training data (and it will be bad in validation too for the same reason). When a model fails to capture important distinctions and patterns in the data, so it performs poorly even in training data, that is called **underfitting**.\n\nSince we care about accuracy on new data, which we estimate from our validation data, we want to find the sweet spot between underfitting and overfitting. Visually, we want the low point of the (red) validation curve in.\n\n![](http:\/\/i.imgur.com\/2q85n9s.png)","d611bb1b":"# Validation Data\nThe MAE for in-sample data was about 68,000. Out-of-sample is more than 270,000.\n\nThis is the difference between a model that is almost exactly right, and one that is unusable for most practical purposes. As a point of reference, the average home value in the validation data is 1.1 million dollars. So the error in new data is about a quarter of the average home value. There are many ways to improve this model, such as experimenting to find better features or different model types.","af571495":"# Selecting Data for Modeling and Prediction Target"}}