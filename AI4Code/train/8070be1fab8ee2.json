{"cell_type":{"540f669e":"code","8ed02137":"code","f6d03bae":"code","88342161":"code","685637e9":"code","7d823216":"code","7ea8c4e2":"code","720f4529":"code","2c208ef7":"code","b9538787":"code","4a1bec43":"code","267b69bb":"code","4bbd9ddc":"code","eb4b0040":"code","65e68fdf":"code","1b94939f":"code","4bda5d01":"code","6fc986cb":"code","c79555c6":"code","0961e2c4":"code","86b49ac5":"code","cdc676eb":"code","4c8b3f3d":"code","0d2afa0c":"code","fa1a4812":"code","628edbff":"code","d9fe61f4":"code","496ca50d":"code","8b1b40a3":"code","6437f923":"code","d95d8ffd":"code","9f9a9bad":"code","2aab9091":"code","1c8b46da":"code","25fee1a7":"code","fdc88f3d":"code","b7377a75":"code","d9fb8216":"code","cfcfb65a":"code","9ea37862":"code","92f5e2e3":"code","51592e36":"code","5158c3bc":"code","8ec08362":"code","8e4a6679":"code","834d7c1b":"code","57ff66f8":"code","b4ca509b":"code","a0747909":"code","766bc67f":"code","bd6ed6e2":"code","909b2d76":"code","45cd961d":"code","6b3ed1a6":"code","7794498e":"markdown"},"source":{"540f669e":"%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\n\nimport imp\nimport keras.backend\nimport keras.models\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm, transforms\nimport os\nimport pickle\nimport time\nimport keras\n\nfrom keras.datasets import mnist\nfrom keras.models import Model\nfrom keras import optimizers","8ed02137":"\nimport zipfile \nimport tensorflow as tf \nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator \nfrom tensorflow.keras import layers,Model","f6d03bae":"!pip install imutils","88342161":"\n# import the necessary packages\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import AveragePooling2D,Dropout,Flatten,Dense,Input\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom imutils import paths\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport argparse\nimport cv2\nimport os\nimport tensorflow as tf\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau","685637e9":"\nimport numpy as np\nimport argparse\nimport cv2\nimport os\nfrom imutils import paths","7d823216":"#tf.compat.v1.disable_eager_execution()\n","7ea8c4e2":"\nnormal_train_ImagePaths = list(paths.list_images('\/kaggle\/input\/covid19vsnormal\/Normal'))\ncorona_train_ImagePaths = list(paths.list_images('\/kaggle\/input\/covid19vsnormal\/COVID-19'))\ndata = []\nlabels = []\n\n\nfor imagePath in corona_train_ImagePaths:\n    image = cv2.imread(imagePath)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = cv2.resize(image, (224, 224))\n    # update the data and labels lists, respectively\n    data.append(image)\n    labels.append(0)\n\n    \nstop=0\nfor imagePath in normal_train_ImagePaths:\n    image = cv2.imread(imagePath)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = cv2.resize(image, (224, 224))\n    # update the data and labels lists, respectively\n    data.append(image)\n    labels.append(1)\n    stop=stop+1\n    if (stop==1281):\n        break\n\ncorona_normal_dict={\"corona\":0,\"normal\":1}\n\ndata = np.array(data) \/ 255.0\nlabels = np.array(labels)","720f4529":"len(labels)","2c208ef7":"\nbase_dir = '..\/input\/covid19vsnormal'","b9538787":"lb = LabelBinarizer()\nlabels = lb.fit_transform(labels)\nlabels = to_categorical(labels)\n\ntrainX, testX, trainY, testY = train_test_split(data, labels,test_size=0.40,  random_state=42)\ndel data\ndel labels\nvalidX, testX, validY, testY = train_test_split(testX, testY,test_size=0.50,  random_state=42)\n\ntrainAug = ImageDataGenerator()","4a1bec43":"from tensorflow.keras.applications.vgg16 import VGG16\n\nbase_model = VGG16(input_shape = (224, 224, 3), # Shape of our images\ninclude_top = False, # Leave out the last fully connected layer\nweights = 'imagenet')\n","267b69bb":"base_model.summary()","4bbd9ddc":"fine_tune_at = 19\n#19","eb4b0040":"\nfor layer in base_model.layers[:19]:\n    layer.trainable = False","65e68fdf":"# Flatten the output layer to 1 dimension\nx = layers.Flatten()(base_model.output)\n\n# Add a fully connected layer with 512 hidden units and ReLU activation\nx = layers.Dense(512, activation='relu')(x)\n\n# Add a dropout rate of 0.5\nx = layers.Dropout(0.5)(x)\n\n# Add a final sigmoid layer for classification\nx = layers.Dense(2, activation='softmax')(x)\n\nmodel = tf.keras.models.Model(base_model.input, x)\n","1b94939f":"model.compile(optimizer = Adam(lr=0.0001), loss = 'binary_crossentropy',metrics = ['acc'])","4bda5d01":"model.summary()","6fc986cb":"from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","c79555c6":"#checkpoint_filepath = '{epoch:02d}-{val_loss:.7f}.hdf5'\ncheckpoint_filepath = '{epoch:02d}.h5'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_loss',\n    mode='min',\n    verbose=1,\n    save_best_only=True)","0961e2c4":"earlystop = EarlyStopping(monitor = 'val_loss', \n                          min_delta = 0, \n                          patience = 3,\n                          verbose = 1,\n                          restore_best_weights = True)\nreduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n                              factor = 0.2,\n                              patience = 3,\n                              verbose = 1,\n                              min_delta = 0.0001)","86b49ac5":"# we put our call backs into a callback list\ncallbacks = [earlystop, model_checkpoint_callback, reduce_lr]","cdc676eb":"vgghist = model.fit_generator(trainAug.flow(trainX, trainY, batch_size=10),validation_data=(validX, validY),validation_steps=10, steps_per_epoch = 100, epochs = 30,shuffle = True, callbacks=callbacks)","4c8b3f3d":"vgghist.history","0d2afa0c":"plt.plot(vgghist.history['acc'])\nplt.plot(vgghist.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\nplt.plot(vgghist.history['loss'])\nplt.plot(vgghist.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n","fa1a4812":"print(\"training_accuracy\", vgghist.history['acc'][-1])\nprint(\"validation_accuracy\", vgghist.history['val_acc'][-1])","628edbff":"for ilayer, layer in enumerate(model.layers):\n    print(\"{:3.0f} {:10}\".format(ilayer, layer.name))","d9fe61f4":"\nprint(\"[INFO] evaluating network...\")\npredIdxs = model.predict(testX, batch_size=10)\npredIdxs = np.argmax(predIdxs, axis=1)\nprint(classification_report(testY.argmax(axis=1), predIdxs,target_names=corona_normal_dict))","496ca50d":"cm = confusion_matrix(testY.argmax(axis=1), predIdxs)\ntotal = sum(sum(cm))\nacc = (cm[0, 0] + cm[1, 1]) \/ total\nsensitivity = cm[0, 0] \/ (cm[0, 0] + cm[0, 1])\nspecificity = cm[1, 1] \/ (cm[1, 0] + cm[1, 1])\n\nprint(cm)\nprint(\"acc: {:.4f}\".format(acc))\nprint(\"sensitivity: {:.4f}\".format(sensitivity))\nprint(\"specificity: {:.4f}\".format(specificity))","8b1b40a3":"model.save(\"vgg16.h5\")\n","6437f923":"imgNormalized = cv2.imread(\"..\/input\/covid19vsnormal\/COVID-19\/COVID-19 (1).jpg\")\nimgNormalized = cv2.cvtColor(imgNormalized, cv2.COLOR_BGR2RGB)\nimgNormalized = cv2.resize(imgNormalized, (224, 224))\nimgNormalized=np.array(imgNormalized)\/ 255.0\nimgNormalizedlist=[]\nimgNormalizedlist.append(imgNormalized)","d95d8ffd":"pred = model.predict(np.array(imgNormalizedlist))\n\nclass_prob = pred.tolist()\nsoftMaxProb = class_prob[0]\n\nsoftMaxProb","9f9a9bad":"from scipy.ndimage.interpolation import zoom\nimport numpy as np\n\nfrom keras import backend as K\nfrom keras.preprocessing.image import load_img, img_to_array\n\nimport matplotlib.pyplot as plt\n\ndef grad_cam(input_model, image, layer_name,H=360,W=360):\n    session = tf.compat.v1.keras.backend.get_session()\n    cls = np.argmax(input_model.predict(image))\n    print(cls)\n    def normalize(x):\n        \"\"\"Utility function to normalize a tensor by its L2 norm\"\"\"\n        return (x + 1e-10) \/ (K.sqrt(K.mean(K.square(x))) + 1e-10)\n    \"\"\"GradCAM method for visualizing input saliency.\"\"\"\n    y_c = input_model.output[0, cls]\n    conv_output = input_model.get_layer(layer_name).output\n    grads = K.GradientTape (y_c, conv_output)[0]\n    grads = normalize(grads)\n    gradient_function = K.function([input_model.input], [conv_output, grads])\n\n    output, grads_val = gradient_function([image])\n    output, grads_val = output[0, :], grads_val[0, :, :, :]\n\n    weights = np.mean(grads_val, axis=(0, 1))\n    cam = np.dot(output, weights)\n\n    cam = np.maximum(cam, 0)\n    #cam = resize(cam, (H, W))\n    cam = zoom(cam,H\/cam.shape[0])\n    #cam = np.maximum(cam, 0)\n    cam = cam \/ cam.max()\n    return cam\n\ndef grad_cam_plus(input_model, img, layer_name,H=360,W=360):\n    session = tf.compat.v1.keras.backend.get_session()\n    cls = np.argmax(input_model.predict(img))\n    print(cls)\n    def normalize(x):\n        \"\"\"Utility function to normalize a tensor by its L2 norm\"\"\"\n        return (x + 1e-10) \/ (K.sqrt(K.mean(K.square(x))) + 1e-10)\n    \"\"\"GradCAM method for visualizing input saliency.\"\"\"\n    y_c = input_model.output[0, cls]\n    conv_output = input_model.get_layer(layer_name).output\n    grads = K.gradients(y_c, conv_output)[0]\n    grads = normalize(grads)\n\n    first = K.exp(y_c)*grads\n    second = K.exp(y_c)*grads*grads\n    third = K.exp(y_c)*grads*grads*grads\n\n    gradient_function = K.function([input_model.input], [y_c,first,second,third, conv_output, grads])\n    y_c, conv_first_grad, conv_second_grad,conv_third_grad, conv_output, grads_val = gradient_function([img])\n    global_sum = np.sum(conv_output[0].reshape((-1,conv_first_grad[0].shape[2])), axis=0)\n\n    alpha_num = conv_second_grad[0]\n    alpha_denom = conv_second_grad[0]*2.0 + conv_third_grad[0]*global_sum.reshape((1,1,conv_first_grad[0].shape[2]))\n    alpha_denom = np.where(alpha_denom != 0.0, alpha_denom, np.ones(alpha_denom.shape))\n    alphas = alpha_num\/alpha_denom\n\n    weights = np.maximum(conv_first_grad[0], 0.0)\n\n    alpha_normalization_constant = np.sum(np.sum(alphas, axis=0),axis=0)\n\n    alphas \/= alpha_normalization_constant.reshape((1,1,conv_first_grad[0].shape[2]))\n\n    deep_linearization_weights = np.sum((weights*alphas).reshape((-1,conv_first_grad[0].shape[2])),axis=0)\n    #print deep_linearization_weights\n    grad_CAM_map = np.sum(deep_linearization_weights*conv_output[0], axis=2)\n\n    # Passing through ReLU\n    cam = np.maximum(grad_CAM_map, 0)\n    cam = zoom(cam,H\/cam.shape[0])\n    cam = cam \/ np.max(cam) # scale 0 to 1.0    \n    #cam = resize(cam, (224,224))\n\n    return cam\n","2aab9091":"from PIL import Image\n","1c8b46da":"imgNormalized = cv2.imread(\"..\/input\/covid19vsnormal\/COVID-19\/COVID-19 (1).jpg\")\nimgNormalized = cv2.cvtColor(imgNormalized, cv2.COLOR_BGR2RGB)\nimgNormalized = cv2.resize(imgNormalized, (224, 224))\nimgNormalized=np.array(imgNormalized)\/ 255.0\nimgNormalizedlist=[]\nimgNormalizedlist.append(imgNormalized)\n","25fee1a7":"pred = model.predict(np.array(imgNormalizedlist))\n\nclass_prob = pred.tolist()\nsoftMaxProb = class_prob[0]\n\nsoftMaxProb","fdc88f3d":"import keras","b7377a75":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Display\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm","d9fb8216":"import requests\nimport os\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\n\n\n","cfcfb65a":"def make_gradcam_heatmap(\n    img_array, model, last_conv_layer_name, classifier_layer_names\n):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer\n    last_conv_layer = model.get_layer(last_conv_layer_name)\n    last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n\n    # Second, we create a model that maps the activations of the last conv\n    # layer to the final class predictions\n    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n    x = classifier_input\n    for layer_name in classifier_layer_names:\n        x = model.get_layer(layer_name)(x)\n    classifier_model = keras.Model(classifier_input, x)\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        # Compute activations of the last conv layer and make the tape watch it\n        last_conv_layer_output = last_conv_layer_model(img_array)\n        tape.watch(last_conv_layer_output)\n        # Compute class predictions\n        preds = classifier_model(last_conv_layer_output)\n        top_pred_index = tf.argmax(preds[0])\n        top_class_channel = preds[:, top_pred_index]\n\n    # This is the gradient of the top predicted class with regard to\n    # the output feature map of the last conv layer\n    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n    pooled_grads = pooled_grads.numpy()\n    for i in range(pooled_grads.shape[-1]):\n        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n\n    # The channel-wise mean of the resulting feature map\n    # is our heatmap of class activation\n    heatmap = np.mean(last_conv_layer_output, axis=-1)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = np.maximum(heatmap, 0) \/ np.max(heatmap)\n    return heatmap","9ea37862":"# Print what the top predicted class is\npred = model.predict(np.array(imgNormalizedlist))\n\nclass_prob = pred.tolist()\nsoftMaxProb = class_prob[0]","92f5e2e3":"last_conv_layer_name = \"block5_conv3\"\nclassifier_layer_names = [\n    \"block5_pool\",\n    \"flatten\",\n    \"dense\",\n    \"dropout\",\n    \"dense_1\"\n]","51592e36":"# Generate class activation heatmap\nheatmap = make_gradcam_heatmap(\n    np.array(imgNormalizedlist), model, last_conv_layer_name, classifier_layer_names\n)\n\n# Display heatmap\nplt.matshow(heatmap)\nplt.show()","5158c3bc":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Display\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm","8ec08362":"# We load the original image\nimg_path=\"..\/input\/covid19vsnormal\/COVID-19\/COVID-19 (1).jpg\"\nimg = keras.preprocessing.image.load_img(img_path)\nimg = keras.preprocessing.image.img_to_array(img)","8e4a6679":"# We rescale heatmap to a range 0-255\nheatmap = np.uint8(255 * heatmap)\n\n# We use jet colormap to colorize heatmap\njet = cm.get_cmap(\"jet\")\n\n# We use RGB values of the colormap\njet_colors = jet(np.arange(256))[:, :3]\njet_heatmap = jet_colors[heatmap]\n\n# We create an image with RGB colorized heatmap\njet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\njet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\njet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n\n# Superimpose the heatmap on original image\nsuperimposed_img = jet_heatmap * 0.4 + img\nsuperimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n\n# Save the superimposed image\nsave_path = \"first_try.jpg\"\nsuperimposed_img.save(save_path)\n\n# Display Grad CAM\ndisplay(Image(save_path))","834d7c1b":"\ndef remove_ticks_and_labels(ax):\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    ax.tick_params(axis='both', which='both', length=0)","57ff66f8":"def decisionVisualization(img,softMaxProb, xaiMethod):\n    # Add batch axis and preprocess   \n\n    fig = plt.figure(constrained_layout=False,figsize=[6,3])\n    gs1 = fig.add_gridspec(nrows=1, ncols=2, left=1, right=1.8)\n\n    fig_ax1 = fig.add_subplot(gs1[0, 0])\n    fig_ax2 = fig.add_subplot(gs1[0, 1])\n\n    gs2 = fig.add_gridspec(nrows=1, ncols=1, left=1.92, right=2.6)\n\n    fig_ax3 = fig.add_subplot(gs2[0, 0])\n\n    fig_ax1.imshow(img, cmap ='gray')\n    fig_ax1.set_title(\"Input radiograph\")\n\n    remove_ticks_and_labels(fig_ax1)\n    \n    fig_ax2.imshow(xaiMethod, cmap=\"jet\")\n    fig_ax2.set_title(str(\"CAM\"))\n    remove_ticks_and_labels(fig_ax2)\n\n\n    x_probs = softMaxProb\n    x_probs = np.asarray(x_probs, dtype=np.float32)\n    y_pos = np.array([0, 1])\n    y_objects = ('Type-1','Type-2') #Infection type\n    y_labels = ['COVID-19','Normal']\n\n    fig_ax3.barh(y_pos, x_probs, color='red', align='center', alpha=0.3)\n    fig_ax3.set_title(\"Explanations\")\n\n    for i, v in enumerate(x_probs):\n        fig_ax3.text(v - 0.07, i + 0.1 , '{0:.2f}   {1}'.format(v,y_labels[i]))\n\n    fig_ax3.set_yticks(y_pos)\n    fig_ax3.set_yticklabels(y_objects)\n    fig_ax3.invert_yaxis()  # labels read top-to-bottom\n    fig_ax3.set_xticks([0, 0.25, 0.5, 0.75,1])\n\n    fig.savefig('explanation.png')","b4ca509b":"from PIL import Image\nimport matplotlib.pyplot as plt","a0747909":"def readTestImage(img_path): \n    img = Image.open(img_path) #open image you want to visualize \"1238_R.png\"\n\n    img = np.array(img.resize((224,224), Image.ANTIALIAS))\n    imgArr = img.reshape(1,224,224,3)#open image you want to visualize\n    imgNormalized = imgArr \/ 255.\n    \n    return img, imgArr, imgNormalized","766bc67f":"\nimg_path = \"..\/input\/covid19vsnormal\/COVID-19\/COVID-19 (1).jpg\"\nimg, imgArr, imgNormalized = readTestImage(img_path)","bd6ed6e2":"# Print what the top predicted class is\npred = model.predict(np.array(imgNormalized))\n\nclass_prob = pred.tolist()\nsoftMaxProb = class_prob[0]","909b2d76":"softMaxProb","45cd961d":"a=decisionVisualization(img,softMaxProb,xaiMethod=superimposed_img.resize((224,224), Image.ANTIALIAS))\n","6b3ed1a6":"a=decisionVisualization(img,softMaxProb,xaiMethod=superimposed_img.resize((224,224), Image.ANTIALIAS))\n","7794498e":"predicted_class_indices=np.argmax(pred,axis=1)\nlabels = (validation_generator.class_indices)\nlabels2 = dict((v,k) for k,v in labels.items())\npredictions = [labels2[k] for k in predicted_class_indices]"}}