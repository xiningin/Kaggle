{"cell_type":{"a652aae1":"code","4840d8a9":"code","61cfb271":"code","ff2e87e1":"code","39fa187f":"code","0465ac5b":"code","5b6f9856":"code","06402ca3":"code","cc8ea53d":"code","d2d868f8":"code","83024944":"code","4ade5ea1":"code","eb494bd4":"code","38fb50a4":"code","26320407":"code","2441ad47":"code","ff9ea081":"code","ccb8d78e":"code","f0d480bc":"code","4a0b14f0":"code","39069b6c":"code","1bd9f07e":"code","9faf7ea1":"code","289d8e6e":"code","ecacf564":"code","2058bb4d":"code","77c57210":"code","6125b45c":"code","2d37561b":"code","b33178bb":"code","96ec902a":"code","31e8495d":"code","d34cfa21":"code","20d1f5ae":"code","ed5d0675":"code","f7512500":"code","cc47051c":"code","ec024a76":"code","cc85c811":"code","08d5e8b1":"code","fd8a237a":"code","94b3a482":"code","c68e1ca9":"code","6d064bf3":"code","680fee69":"code","81fa7e27":"code","aec4df18":"code","0516a1d5":"code","c5f6a731":"code","7ba632b1":"code","f1d988ac":"code","0a6912b0":"code","398e7e5e":"code","b48cd0bb":"code","78acbdab":"code","1ccd3b8b":"code","1ae5cba3":"markdown","83b3e5cc":"markdown","304c0cb5":"markdown","422ed362":"markdown","fbab867b":"markdown","7d7ee079":"markdown","67550ad2":"markdown","7f9cc6d5":"markdown","e869df4b":"markdown","d89998d2":"markdown","911a86b6":"markdown","6a3238b4":"markdown","856ac8b5":"markdown","737151f7":"markdown","ddc89592":"markdown","17558a02":"markdown","c220ce66":"markdown","74fd84a0":"markdown","a94fd40f":"markdown","5dccd27e":"markdown","36b89e5d":"markdown","8e7088bb":"markdown","eba438b7":"markdown","56a7703b":"markdown","cf89c874":"markdown","a99ed9ad":"markdown","ad5b363e":"markdown","f98cc808":"markdown","26368109":"markdown","067bd1c7":"markdown","47798a21":"markdown","b7e4c214":"markdown","4f2534fe":"markdown","344b30f4":"markdown","71d4c464":"markdown","8829a958":"markdown","c0b2a940":"markdown","c55d5651":"markdown","3c2a6833":"markdown","a6999abe":"markdown","f1158cb4":"markdown","1abd03e5":"markdown","8e538f99":"markdown","3351bdee":"markdown","c9428bbe":"markdown"},"source":{"a652aae1":"#DataFrames\nimport numpy as np\nimport pandas as pd\n\n#Scikit learn\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE\nfrom sklearn.pipeline import make_pipeline   \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\n\n#Visuals\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns\nfrom pandas import plotting\n\n#Others\nimport random\nrandom.seed(42)\nimport warnings\nimport os\nwarnings.filterwarnings(\"ignore\")\nfrom IPython.display import Image\nimport math\n","4840d8a9":"# Eaton Center !\nImage(url='https:\/\/i.gifer.com\/QE7.gif')","61cfb271":"df = pd.read_csv('..\/input\/Mall_Customers.csv')\ndf.head(5)","ff2e87e1":"# rows, columns\ndf.shape","39fa187f":"df.info()","0465ac5b":"df.describe()","5b6f9856":"# Check for Nulls\ndf.isnull().sum().sort_values(ascending=False)","06402ca3":"# Unique values count\nprint(df.nunique())","cc8ea53d":"# drop Customer id \ndf = df.drop('CustomerID', axis=1)\ndf.head(2)","d2d868f8":"# rename columns\nnew_cols = ['Gender', 'Age', 'AnnualIncome','SpendingScore']\n\ndf.columns = new_cols\n\ndf.head(3)","83024944":"# Categorical Scatterplot on Gender Vs Annual Income\nsns.catplot(x=\"Gender\", y=\"AnnualIncome\", kind=\"swarm\",hue=\"Gender\", data=df.sort_values(\"Gender\"))","4ade5ea1":"# Distributions of observations within categorical attribute - \"Gender\"\nsns.catplot(x=\"Gender\", y=\"AnnualIncome\", kind=\"boxen\",data=df.sort_values(\"Gender\"))","eb494bd4":"sns.relplot(x=\"Age\", y=\"AnnualIncome\", hue=\"Gender\", style=\"Gender\",size=\"SpendingScore\",sizes=(1, 100), data=df);","38fb50a4":"# Data distribution \nsns.pairplot(df);","26320407":"# Visual linear relationship\nplt.figure(1 , figsize = (15 , 7))\nn=0\nnew_cols = ['Age', 'AnnualIncome','SpendingScore']\n\nfor x in new_cols:\n    for y in new_cols:\n        n += 1\n        plt.subplot(3 , 3 , n)\n        plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n        sns.regplot(x = x , y = y , data = df)\n            \nplt.show()","2441ad47":"# lets calculate the variance of each numerical attribute in the dataset.\ndf.var(ddof=0).plot(kind='bar')","ff9ea081":"# lets calculate the SD of each numerical attribute in the dataset.\ndf.std(ddof=0)","ccb8d78e":"train_X, test_X = train_test_split(df, test_size=0.2, random_state=42)\n\nprint(len(train_X), \"train +\", len(test_X), \"test\")","f0d480bc":"# lets take copy of the data \ndf2 = train_X.copy()","4a0b14f0":"# Let fit and transform the Gender attribute into numeric\nle = LabelEncoder()\nle.fit(df2.Gender)","39069b6c":"# 0 is Female, 1 is Male\nle.classes_","1bd9f07e":"#update df2 with transformed values of gender\ndf2.loc[:,'Gender'] = le.transform(df2.Gender)","9faf7ea1":"df2.head(3)","289d8e6e":"# Create scaler: scaler\nscaler = StandardScaler()\nscaler.fit(df2)","ecacf564":"# transform\ndata_scaled = scaler.transform(df2)\ndata_scaled[0:3]","2058bb4d":"pca = PCA()\n\n# fit PCA\npca.fit(data_scaled)","77c57210":"# PCA features\nfeatures = range(pca.n_components_)\nfeatures","6125b45c":"# PCA transformed data\ndata_pca = pca.transform(data_scaled)\ndata_pca.shape","2d37561b":"# PCA components variance ratios.\npca.explained_variance_ratio_","b33178bb":"plt.bar(features, pca.explained_variance_ratio_)\nplt.xticks(features)\nplt.ylabel('variance')\nplt.xlabel('PCA feature')\nplt.show()","96ec902a":"# Principal component analysis (PCA) and singular value decomposition (SVD) \n# PCA and SVD are closely related approaches and can be both applied to decompose any rectangular matrices.\npca2 = PCA(n_components=2, svd_solver='full')\n\n# fit PCA\npca2.fit(data_scaled)\n\n# PCA transformed data\ndata_pca2 = pca2.transform(data_scaled)\ndata_pca2.shape","31e8495d":"xs = data_pca2[:,0]\nys = data_pca2[:,1]\n#zs = train_X.iloc[:,2]\nplt.scatter(ys, xs)\n#plt.scatter(ys, zs, c=labels)\n\n\nplt.grid(False)\nplt.title('Scatter Plot of Customers data')\nplt.xlabel('PCA-01')\nplt.ylabel('PCA-02')\n\nplt.show()","d34cfa21":"# KMeans model\n\n# lets assume 4 clusters to start with\n\nk=4 \nkmeans = KMeans(n_clusters=k, init = 'k-means++',random_state = 42) ","20d1f5ae":"# Build pipeline\npipeline = make_pipeline(scaler, pca2, kmeans)\n#pipeline = make_pipeline(kmeans)","ed5d0675":"# fit the model to the scaled dataset\nmodel_fit = pipeline.fit(df2)\nmodel_fit","f7512500":"# target\/labels of train_X\nlabels = model_fit.predict(df2)\nlabels","cc47051c":"# lets add the clusters to the dataset\ntrain_X['Clusters'] = labels","ec024a76":"# Number of data points for each feature in each cluster\ntrain_X.groupby('Clusters').count()","cc85c811":"# Scatter plot visuals with labels\n\nxs = data_pca2[:,0]\nys = data_pca2[:,1]\n#zs = train_X.iloc[:,2]\nplt.scatter(ys, xs,c=labels)\n#plt.scatter(ys, zs, c=labels)\n\nplt.grid(False)\nplt.title('Scatter Plot of Customers data')\nplt.xlabel('PCA-01')\nplt.ylabel('PCA-02')\n\nplt.show()","08d5e8b1":"# Centroids of each clusters.\ncentroids = model_fit[2].cluster_centers_\ncentroids","fd8a237a":"X = data_pca2\n# Assign the columns of centroids: centroids_x, centroids_y\ncentroids_x = centroids[:,0]\ncentroids_y = centroids[:,1]","94b3a482":"# Visualising the clusters & their Centriods\nplt.figure(figsize=(15,7))\nsns.scatterplot(X[labels == 0, 0], X[labels == 0, 1], color = 'grey', label = 'Cluster 1',s=50)\nsns.scatterplot(X[labels == 1, 0], X[labels == 1, 1], color = 'blue', label = 'Cluster 2',s=50)\nsns.scatterplot(X[labels == 2, 0], X[labels == 2, 1], color = 'yellow', label = 'Cluster 3',s=50)\nsns.scatterplot(X[labels == 3, 0], X[labels == 3, 1], color = 'green', label = 'Cluster 4',s=50)\n\nsns.scatterplot(centroids_x, centroids_y, color = 'red', \n                label = 'Centroids',s=300,marker='*')\nplt.grid(False)\nplt.title('Clusters of customers')\nplt.xlabel('PCA-01')\nplt.ylabel('PCA-02')\nplt.legend()\nplt.show()","c68e1ca9":"# Distance from each sample to centroid of its cluster\nmodel_fit[2].inertia_","6d064bf3":"# WCSS stands for Within Cluster Sum of Squares. It should be low.\n\nks = range(1, 10)\nwcss = []\nsamples = data_pca2\n\nfor i in ks:\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(samples)\n    # inertia method returns wcss for that model\n    wcss.append(kmeans.inertia_)","680fee69":"# lets visualize \nplt.figure(figsize=(10,5))\nsns.lineplot(ks, wcss,marker='o',color='skyblue')\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","81fa7e27":"def getInertia2(X,kmeans):\n    ''' This function is analogous to getInertia, but with respect to the 2nd closest center, rather than closest one'''\n    inertia2 = 0\n    for J in range(len(X)):\n        L = min(1,len(kmeans.cluster_centers_)-1) # this is just for the case where there is only 1 cluster at all\n        dist_to_center = sorted([np.linalg.norm(X[J] - z)**2 for z in kmeans.cluster_centers_])[L]\n        inertia2 = inertia2 + dist_to_center\n    return inertia2 ","aec4df18":"wcss = []\ninertias_2 = []\nsilhouette_avgs = []\n\nks = range(1, 10)\nsamples = data_pca2\n\nfor i in ks:\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(samples)\n    wcss.append(kmeans.inertia_)\n    inertias_2.append(getInertia2(samples,kmeans))\n    if i>1:\n        silhouette_avgs.append(silhouette_score(samples, kmeans.labels_))","0516a1d5":"silhouette_avgs","c5f6a731":"plt.figure(figsize=(20,5))\n\nplt.subplot(1,3,1)\nplt.title(\"wcss: sum square distances to closest cluster\")\nplt.plot(ks,wcss)\nplt.xticks(ks)\nplt.xlabel('number of clusters')\nplt.grid()\n    \nplt.subplot(1,3,2)    \nplt.title(\"Ratio: wcss VS. sum square distances to 2nd closest cluster\")\nplt.plot(ks,np.array(wcss)\/np.array(inertias_2))\nplt.xticks(ks)\nplt.xlabel('number of clusters')\nplt.grid()\n\nplt.subplot(1,3,3)  \nplt.title(\"Average Silhouette\")\nplt.plot(ks[1:], silhouette_avgs)\nplt.xticks(ks)\nplt.xlabel('number of clusters')\nplt.grid()\n\nplt.show()","7ba632b1":"# Copy the dataset\ndf_new = test_X.copy()","f1d988ac":"# predict the labels\nle.fit(df_new.Gender)\n\n#update df2 with transformed values of gender\ndf_new.loc[:,'Gender'] = le.transform(df_new.Gender)\n\nlabels_test = model_fit.predict(df_new)\nlabels_test","0a6912b0":"# lets add the clusters to the dataset\ntest_X['Clusters'] = labels_test\n","398e7e5e":"# Number of data points for each feature in each cluster\ntest_X.groupby('Clusters').count()","b48cd0bb":"query = (test_X['Clusters']==1)\ntest_X[query]","78acbdab":"from IPython.display import display, HTML\n\nHTML('''<div style=\"display: flex; justify-content: row;\">\n    <img src=\"https:\/\/media.giphy.com\/media\/MEgGD8bV72hfq\/giphy.gif\">\n    <img src=\"https:\/\/media.giphy.com\/media\/3k9gOXgimLWF2\/giphy.gif\">\n    <img src=\"https:\/\/media.giphy.com\/media\/3o751RE4VSNLjpSLew\/giphy.gif\">\n<\/div>''')","1ccd3b8b":"# Are these shoppers? No idea! what they are doing.\nImage(url='https:\/\/media.giphy.com\/media\/fAhOtxIzrTxyE\/giphy.gif')","1ae5cba3":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import\" data-toc-modified-id=\"Import-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Import<\/a><\/span><\/li><li><span><a href=\"#Get-Data\" data-toc-modified-id=\"Get-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Get Data<\/a><\/span><\/li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Exploratory Data Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Visuals\" data-toc-modified-id=\"Visuals-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Visuals<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Data-Split:-Train-\/-Test\" data-toc-modified-id=\"Data-Split:-Train-\/-Test-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Data Split: Train \/ Test<\/a><\/span><\/li><li><span><a href=\"#Prepare-the-data\" data-toc-modified-id=\"Prepare-the-data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Prepare the data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#LabelEncoder\" data-toc-modified-id=\"LabelEncoder-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>LabelEncoder<\/a><\/span><\/li><li><span><a href=\"#StandardScaler\" data-toc-modified-id=\"StandardScaler-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>StandardScaler<\/a><\/span><\/li><li><span><a href=\"#Principal-Component-Analysis-(PCA)\" data-toc-modified-id=\"Principal-Component-Analysis-(PCA)-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>Principal Component Analysis (PCA)<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Kmeans-Cluster-Model\" data-toc-modified-id=\"Kmeans-Cluster-Model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Kmeans Cluster Model<\/a><\/span><\/li><li><span><a href=\"#Pipeline\" data-toc-modified-id=\"Pipeline-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Pipeline<\/a><\/span><\/li><li><span><a href=\"#Assign-Labels\" data-toc-modified-id=\"Assign-Labels-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>Assign Labels<\/a><\/span><\/li><li><span><a href=\"#Optimal-K\" data-toc-modified-id=\"Optimal-K-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;<\/span>Optimal K<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Centriods\" data-toc-modified-id=\"Centriods-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;<\/span>Centriods<\/a><\/span><\/li><li><span><a href=\"#Centriod-Visuals\" data-toc-modified-id=\"Centriod-Visuals-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;<\/span>Centriod Visuals<\/a><\/span><\/li><li><span><a href=\"#Interia\" data-toc-modified-id=\"Interia-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;<\/span>Interia<\/a><\/span><\/li><li><span><a href=\"#Interia2\" data-toc-modified-id=\"Interia2-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;<\/span>Interia2<\/a><\/span><\/li><li><span><a href=\"#Silhouette\" data-toc-modified-id=\"Silhouette-9.5\"><span class=\"toc-item-num\">9.5&nbsp;&nbsp;<\/span>Silhouette<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Validate-with-New-dataset\" data-toc-modified-id=\"Validate-with-New-dataset-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;<\/span>Validate with New dataset<\/a><\/span><\/li><li><span><a href=\"#Conclusion-!\" data-toc-modified-id=\"Conclusion-!-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;<\/span>Conclusion !<\/a><\/span><\/li><\/ul><\/div>","83b3e5cc":"Unable to find a linear relationship between the attributes. ","304c0cb5":"#### Rename Columns","422ed362":"This tells you that 33.2% of the dataset\u2019s variance lies along the first axis, and 26.7% lies along the second axis. I assume, 2 Intrinsic dimensions (number of PCA features needed to approximate the dataset) is sufficient to represent dataset in flat 2-dimensional plane.","fbab867b":"## Optimal K","7d7ee079":"In the above Plots, we can infer based on the distribution pattern of Annual Income and Age that few people who earn more than 100 US Dollars.Most of the people have an earning of around 25-90 US Dollars. Also, we can say that the least Income is around 20 US Dollars.\n\nPeople with age, >20 years and < 40 years, have good spending score. Spending score and Annual Income scatter plot is providing us some insight on the clustering. Lets hold our conculsions for time being. ","67550ad2":"### Visuals","7f9cc6d5":"For easy use, I have renamed all the column names to get ride of spaces and special characters.","e869df4b":"#### Drop Columns","d89998d2":"For the given dataset, we have segmented the customers into 4 (optimal) clusters using Kmeans algorithm. Each cluster is mix of defined variables such as - gender, age, spending score and annual income.  \n\nWe have to run another machine learning model to distinguish customers. As of now, lets assume the clusters are include of following customers - Big Spenders, Bargain Hunters, Window Shoppers.","911a86b6":"## Prepare the data","6a3238b4":"## Kmeans Cluster Model","856ac8b5":"Summation Distance(p,c) is the sum of distance of points in a cluster from the centroid.\n\n\n![](https:\/\/i.imgur.com\/5W63xul.png)","737151f7":"## Exploratory Data Analysis","ddc89592":"### StandardScaler","17558a02":"Principal Component Analysis (PCA) is by far the most popular dimensionality reduction algorithm. First step \"decorrelation\" and then reduces dimension. ","c220ce66":"[](http:\/\/)* Problem Statement: To analyze and segment the customer of the mall based on the attributes - age, gender, annual income and spending score. Thereby, support the Mall's marketing startegy in identifying the target customers. \n\n![](http:\/\/)* To decompose above requirement, \n        * Whats the optimal customers segment to create? how?\n        * Who are the target customers?\n        * Can this customer segmentation tested with new data?  ","74fd84a0":"The next step, PCA is sensitive to the scale of features. Hence, I have normalized the data.","a94fd40f":"Assumes the low variance features are \"noise\" and and high variance features are informative. In this case, Age has low variance. Again ! lets not draw any conclusions as the dataset is not normalized yet. ","5dccd27e":"In the above graph, It can be seen that the Ages from 30 to 40 has higher annual income and better spending score. Males are heavy spenders then females. Sounds like this age group could be the target customers of the mall.  ","36b89e5d":"## Import","8e7088bb":"### Centriods","eba438b7":"A complementary measure of performance is to look at what below is called inertia2, which is the sum of the squares distances between each point and the 2nd closest cluster\n\nA nice clustering solution should have small inertia, and large inertia2: that means:\n\n- points are close to the center of their cluster\n- points are far from the center of the other clusters (since they are far to the closest center of the other clusters)","56a7703b":"#### Check Duplicates","cf89c874":"## Conclusion !","a99ed9ad":"### Principal Component Analysis (PCA)","ad5b363e":"As you can see, the inertia drops very quickly as we increase k (clusters) up to 4, but then it decreases much more slowly as we keep increasing k. This curve has roughly the shape of an arm (Hence, called Elbow Method) and there is an \u201celbow\u201d at k=4. But, Is it 4 or 5 the optimal  k value? \n\nLets find out with other measures like, Silhouette.","f98cc808":"'CustomerID'is primary key of the dataset and its unique counts matches the total record count. Hence, no duplicate records. ","26368109":"## Get Data","067bd1c7":"No null records. Its means, no additional effort required in handling nulls. Great news!","47798a21":"### Interia2","b7e4c214":"The following stages are:\n1. Preprocessing.LabelEncoder() - helps normalize labels such that they contain only values between 0 and n_classes-1.\n2. StandardScaler - scaling to unit variance (ie Normalizing the data)\n3. Principal Component analysis(PCA)- is an unsupervised statistical technique that is used for dimensionality reduction.\n4. Finally, Feature selection.","4f2534fe":"## Validate with New dataset ","344b30f4":"### Interia","71d4c464":"The silhouette coefficient can vary between -1 and +1: a coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary, and finally a coefficient close to -1 means that the instance may have been assigned to the wrong cluster.","8829a958":"#### Check Nulls","c0b2a940":"### Centriod Visuals","c55d5651":"## Data Split: Train \/ Test","3c2a6833":"Another performance measure is called _silhouette_:\n\nThe silhouette $s(x)$ for a point $x$ is defined as:\n\n$$ s(x) = \\frac{b(x)-a(x)}{\\max\\{a(x),b(x)\\}} $$\n\nwhere \n\n- $a(x)$ is the average distance between $x$ and the points in the cluster $x$ belongs\n\n- $b(x)$ is the lowest average distance between $x$ and the points the clusters $x$ does not belong to\n\n$s(x)$ is a quantity in $[-1,1]$. The silhouette score is the average silhouette score among all the points: $\\frac{1}{|X|} \\sum_x s(x)$","a6999abe":"In the above two plots, Males get paid more than Females. But, at lower annual income level, both genders are paid equally.","f1158cb4":"## Assign Labels","1abd03e5":"## Pipeline","8e538f99":"Diagram 1 is Interia (Elbow method). We are unable to infer whether 4, 5 or 6 is the optimal K value?\n\nDiagram 2 is ratio of Interia \/ sum squares distance to 2nd closest cluster. Th ratio is increasing is low at K=4 which means small interia and large interia2(points far from other clusters). So. 4 is the optimal K.\n\nDiagram 3 is the silhouette score. Highest silhouette score means means that the instance is well inside its own cluster and far from other clusters. K=4 has highest score. \n\nSo, Optimal Clusters (K) is 4.\n\nNote: # Clusters(K) might increase with increase in PCA components.","3351bdee":"### Silhouette","c9428bbe":"### LabelEncoder"}}