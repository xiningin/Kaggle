{"cell_type":{"6e45b7ce":"code","05f406db":"code","85a57a6e":"code","ee4f452b":"code","5cbf3db0":"code","7de77365":"code","74c5d47a":"code","25297464":"code","87a8e42f":"code","5a65c641":"code","42358e2e":"code","67ec2cc9":"code","e8062b69":"code","73acc1ff":"code","f4172929":"code","276a562c":"code","3756ab52":"code","e9e9f9cc":"code","35e54ef7":"code","9f337f17":"code","8c74a5ee":"code","3f01438b":"code","3d8367b5":"code","9b46fd95":"code","13dadfb3":"code","183e3aaa":"code","290be8ce":"code","1b79dcc3":"code","8199bd88":"code","7705217d":"code","3d2edbd6":"code","5389e667":"code","50e992ac":"code","e83e617f":"code","2c78e651":"code","0756ef39":"code","fc92e164":"code","f902088c":"code","a5b772c3":"code","6e29c79d":"code","9032486e":"code","cddb3848":"code","e8b1c8e6":"code","26fcec6f":"code","25b60394":"code","a1520fbf":"code","cf416d1e":"code","2d11efca":"code","f637ce2b":"markdown","e49240a1":"markdown","5ccf3078":"markdown","584900cb":"markdown","58832b77":"markdown","879e1ad6":"markdown","67791f36":"markdown","96a24a1d":"markdown","879ed4ad":"markdown","4bb96544":"markdown","7a8fbf79":"markdown","0414d9e1":"markdown","f5a67d59":"markdown","2d065a1c":"markdown","5e00f7cf":"markdown","fcfeb7f2":"markdown","dccdf5c1":"markdown","4033d8d2":"markdown","4db3197c":"markdown","9637f31d":"markdown","98430a35":"markdown","8fc10f4e":"markdown","b9aba988":"markdown","f8f7ad62":"markdown","ec2baeb8":"markdown","bbfe4c13":"markdown","ae28cb83":"markdown","f1d4ad78":"markdown","9677368d":"markdown","86503029":"markdown","9cf00b39":"markdown","05891847":"markdown","4e81af8f":"markdown","718b5dc5":"markdown","f6885eda":"markdown","7ab1b685":"markdown","28da04bb":"markdown","1c5ab60b":"markdown","8f9472f8":"markdown","0214ab2f":"markdown","aea3b3d1":"markdown","22e581ea":"markdown","09fefd58":"markdown","ca7fa4f0":"markdown","37812658":"markdown","4b81d018":"markdown"},"source":{"6e45b7ce":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom scipy import stats\nfrom statistics import mode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\n\nfrom sklearn.utils import resample\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\n\nfrom matplotlib import pyplot","05f406db":"df = pd.read_csv(\"..\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")\ndf.drop(\"Date\", axis = 1, inplace = True)","85a57a6e":"df.dtypes","ee4f452b":"df.head()","5cbf3db0":"df.describe()","7de77365":"df.isnull().sum()","74c5d47a":"df_total_null = df.isnull().sum().sum()\ndf_total = np.product(df.shape)\n\nnull_perc = (df_total_null \/ df_total) * 100\nnull_perc","25297464":"df[[\"Location\",\"WindGustDir\",\"WindDir9am\", \"WindDir3pm\", \"RainToday\", \"RainTomorrow\"]] = df[[\"Location\",\n                                                                                                       \"WindGustDir\",\n                                                                                                       \"WindDir9am\", \n                                                                                                       \"WindDir3pm\", \n                                                                                                       \"RainToday\", \n                                                                                                       \"RainTomorrow\"]].astype(\"category\")","87a8e42f":"df_dropna = df.dropna()\ndf_dropna.shape","5a65c641":"def fill_na(data):\n  for i in range(data.shape[1]):\n    if data.iloc[:,i].dtypes.name == 'category':\n      data.iloc[:,i] = data.iloc[:,i].fillna(mode(data.iloc[:,i]))\n    elif data.iloc[:,i].dtypes == np.int64 or data.iloc[:,i].dtypes == np.float64:\n      if np.count_nonzero(np.abs(stats.zscore(data.iloc[:,i].dropna())) > 3) == 0:\n        data.iloc[:,i] = data.iloc[:,i].fillna(data.iloc[:,i].mean())\n      else:\n        data.iloc[:,i] = data.iloc[:,i].fillna(data.iloc[:,i].median())\n  return data\n","42358e2e":"df_fill = fill_na(df)","67ec2cc9":"df_fill.isna().sum()","e8062b69":"df_fill.dtypes","73acc1ff":"\ndf_fill_train = df_fill[np.invert(df_fill.loc[:,\"WindGustDir\"].isna())]\ndf_fill_test = df_fill[df_fill.loc[:,\"WindGustDir\"].isna()]\n","f4172929":"df_fill_train_x = df_fill_train.loc[:,~df_fill_train.columns.isin([\"WindGustDir\"])]\ndf_fill_train_y = df_fill_train.loc[:,\"WindGustDir\"]\n\ndf_fill_test_x = df_fill_test.loc[:,~df_fill_test.columns.isin([\"WindGustDir\"])]\ndf_fill_test_y = df_fill_test.loc[:,\"WindGustDir\"]","276a562c":"df_fill_train_x.dtypes","3756ab52":"fit_tree_fill = tree.DecisionTreeClassifier().fit(pd.get_dummies(df_fill_train_x), df_fill_train_y)","e9e9f9cc":"pred = fit_tree_fill.predict(pd.get_dummies(df_fill_test_x))","35e54ef7":"df_fill_test.WindGustDir = pred\n\ndf_clean = pd.concat([df_fill_train, df_fill_test])","9f337f17":"df_clean.isna().sum()","8c74a5ee":"X_train_fill, X_test_fill, y_train_fill, y_test_fill = train_test_split(df_clean.loc[:,df_clean.columns != \"RainTomorrow\"], \n                                                    df_clean.RainTomorrow, \n                                                    test_size = 0.2, \n                                                    random_state = 0)\nX_train_fill[\"Y\"] = y_train_fill","3f01438b":"X_train_drop, X_test_drop, y_train_drop, y_test_drop = train_test_split(df_dropna.loc[:,df_dropna.columns != \"RainTomorrow\"], \n                                                    df_dropna.RainTomorrow, \n                                                    test_size = 0.2, \n                                                    random_state = 0)\nX_train_drop[\"Y\"] = y_train_drop","3d8367b5":"print(\"fill : \")\nprint(X_train_fill.shape)\nprint(X_test_fill.shape)\n\nprint(\"drop : \")\nprint(X_train_drop.shape)\nprint(X_test_drop.shape)","9b46fd95":"print(\"fill :\")\nprint(X_train_fill.Y.value_counts())\n\nprint(\"drop :\")\nprint(X_train_drop.Y.value_counts())","13dadfb3":"X_train_up_fill = resample(X_train_fill[X_train_fill[\"Y\"] == \"Yes\"]\n                      ,replace = True\n                      ,n_samples = 50000)\n\nX_train_down_fill = resample(X_train_fill[X_train_fill[\"Y\"] == \"No\"]\n                      ,replace = True\n                      ,n_samples = 50000)","183e3aaa":"X_train_up_drop = resample(X_train_drop[X_train_drop[\"Y\"] == \"Yes\"]\n                      ,replace = True\n                      ,n_samples = 23000)\n\nX_train_down_drop = resample(X_train_drop[X_train_drop[\"Y\"] == \"No\"]\n                      ,replace = True\n                      ,n_samples = 23000)","290be8ce":"print(\"fill :\")\nprint(X_train_up_fill.shape)\nprint(X_train_down_fill.shape)\n\n\nprint(\"drop :\")\nprint(X_train_up_drop.shape)\nprint(X_train_down_drop.shape)","1b79dcc3":"X_train_fill = pd.concat([X_train_up_fill, X_train_down_fill])\nX_train_drop = pd.concat([X_train_up_drop, X_train_down_drop])\n\ny_train_fill = X_train_fill.Y\nX_train_fill = X_train_fill.loc[:,X_train_fill.columns != \"Y\"]\n\ny_train_drop = X_train_drop.Y\nX_train_drop = X_train_drop.loc[:,X_train_drop.columns != \"Y\"]","8199bd88":"fit_tree_fill = tree.DecisionTreeClassifier().fit(pd.get_dummies(X_train_fill), y_train_fill)\nfit_tree_drop = tree.DecisionTreeClassifier().fit(pd.get_dummies(X_train_drop), y_train_drop)","7705217d":"pred_tree_fill = fit_tree_fill.predict(pd.get_dummies(X_test_fill))\npred_tree_drop = fit_tree_fill.predict(pd.get_dummies(X_test_drop))","3d2edbd6":"matrix = confusion_matrix(y_test_fill, pred_tree_fill, labels=[\"Yes\", \"No\"])\naccuracy = accuracy_score(y_test_fill, pred_tree_fill)\nprint(\"fill : \")\nprint(matrix)\nprint(accuracy)\n\nmatrix = confusion_matrix(y_test_drop, pred_tree_drop, labels=[\"Yes\", \"No\"])\naccuracy = accuracy_score(y_test_drop, pred_tree_drop)\nprint(\"drop : \")\nprint(matrix)\nprint(accuracy)","5389e667":"matrix = classification_report(y_test_fill, pred_tree_fill)\nprint('Classification report fill : \\n',matrix)\n\nmatrix = classification_report(y_test_drop, pred_tree_drop)\nprint('Classification report drop : \\n',matrix)","50e992ac":"fit_logit_fill = LogisticRegression(max_iter=10000).fit(pd.get_dummies(X_train_fill), y_train_fill)\nfit_logit_drop = LogisticRegression(max_iter=10000).fit(pd.get_dummies(X_train_drop), y_train_drop)","e83e617f":"pred_logit_fill = fit_logit_fill.predict(pd.get_dummies(X_test_fill))\npred_logit_drop = fit_logit_drop.predict(pd.get_dummies(X_test_drop))","2c78e651":"matrix = classification_report(y_test_fill, pred_logit_fill)\nprint('Classification report fill : \\n',matrix)\n\nmatrix = classification_report(y_test_drop, pred_logit_drop)\nprint('Classification report drop : \\n',matrix)","0756ef39":"from tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom tensorflow.keras.utils import to_categorical\n\nfrom numpy import array\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder","fc92e164":"scaler = MinMaxScaler()\nscaler.fit(pd.get_dummies(X_train_fill).values)\n\nX_train_fill = scaler.transform(pd.get_dummies(X_train_fill).values)\nX_test_fill = scaler.transform(pd.get_dummies(X_test_fill).values)\n\nX_train_drop = scaler.transform(pd.get_dummies(X_train_drop).values)\nX_test_drop = scaler.transform(pd.get_dummies(X_test_drop).values)","f902088c":"std_scaler = StandardScaler()\nstd_scaler.fit(X_train_fill)\n\nX_train_fill = std_scaler.transform(X_train_fill)\nX_test_fill = std_scaler.transform(X_test_fill)\n\nX_train_drop = std_scaler.transform(X_train_drop)\nX_test_drop = std_scaler.transform(X_test_drop)","a5b772c3":"label_encoder = LabelEncoder()\nlabel_encoder.fit(y_train_fill)\n\ny_train_fill = label_encoder.transform(y_train_fill)\ny_test_fill = label_encoder.transform(y_test_fill)\n\ny_train_drop = label_encoder.transform(y_train_drop)\ny_test_drop = label_encoder.transform(y_test_drop)","6e29c79d":"X_train_fill","9032486e":"model_fill = Sequential()\nmodel_fill.add(Dense(128, input_dim = 115, activation=\"relu\"))\nmodel_fill.add(Dropout(0.2))\nmodel_fill.add(Dense(128, activation=\"relu\"))\nmodel_fill.add(Dropout(0.2))\nmodel_fill.add(Dense(1, activation=\"sigmoid\"))\nopt = keras.optimizers.Adam(learning_rate = 0.0001)\nmodel_fill.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\nmodel_fill.summary()","cddb3848":"model_drop = Sequential()\nmodel_drop.add(Dense(128, input_dim = 115, activation=\"relu\"))\nmodel_drop.add(Dropout(0.2))\nmodel_drop.add(Dense(128, activation=\"relu\"))\nmodel_drop.add(Dropout(0.2))\nmodel_drop.add(Dense(1, activation=\"sigmoid\"))\nopt = keras.optimizers.Adam(learning_rate = 0.0001)\nmodel_drop.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\nmodel_drop.summary()","e8b1c8e6":"history_model_fill = model_fill.fit(X_train_fill\n              ,y_train_fill\n              ,epochs=20\n              ,batch_size=256\n              ,validation_data=(X_test_fill, y_test_fill)\n              ,verbose = 0)","26fcec6f":"pyplot.title('Loss \/ Mean Squared Error')\npyplot.plot(history_model_fill.history['loss'], label='train')\npyplot.plot(history_model_fill.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","25b60394":"history_model_drop = model_drop.fit(X_train_drop\n              ,y_train_drop\n              ,epochs=20\n              ,batch_size=256\n              ,validation_data=(X_test_drop, y_test_drop)\n              ,verbose = 0)","a1520fbf":"pyplot.title('Loss \/ Mean Squared Error')\npyplot.plot(history_model_drop.history['loss'], label='train')\npyplot.plot(history_model_drop.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","cf416d1e":"pred_keras_fill = (model_fill.predict(X_test_fill) >= 0.5).astype(\"int32\")\nclass_keras_fill = label_encoder.inverse_transform(pred_keras_fill)\n\n\npred_keras_drop = (model_drop.predict(X_test_drop) >= 0.5).astype(\"int32\")\nclass_keras_drop = label_encoder.inverse_transform(pred_keras_drop)","2d11efca":"matrix = classification_report(label_encoder.inverse_transform(y_test_fill), class_keras_fill)\nprint('Classification report fill : \\n',matrix)\n\nmatrix = classification_report(label_encoder.inverse_transform(y_test_drop), class_keras_drop)\nprint('Classification report drop : \\n',matrix)","f637ce2b":"Setelah data yang kita punya rapih, maka kita dapat langsung menggunakan model Decision Tree. Akan tetapi data yang bertipe data category tidak bisa kita gunakan, maka kita akan menggunakan one hot encoder untuk mengencode data category tersbeut.\n\nDisini kita membuat 2 model yaitu `fit_tree_fill` yang berisikan model untuk memprediksi data yang kita fill, dan `fit_tree_drop` yang berisikan data yang kita drop ketika ada data yang NA.","e49240a1":"Setelah mengetahui data kita imbalanced, langkah yang selanjutnya kita akan lakukan ialah membuat data yang kita punya menjadi balanced dengan cara upsample dan juga downsample menggunakan function `rsample` dan kita set agar data kita balanced di angka 50000.","5ccf3078":"Kita akan melihat sekilas data tersebut menggunakan function `head()`.","584900cb":"Untuk menghilangkan nilai NA tersebut, disini saya mau menggunakan prediksi untuk nilai-nilai NA tersebut, karena jika kita menghilangkan data tersebut, maka data kita akan hilang banyak.\n\nMaka dari itu, kita akan membagi 2 data tersebut menjadi data test dan train, yang dimana data train untuk belajar si model, dan data test itu berisikan data yang bernilai NA yang akan kita prediksi untuk mengisi NA tersebut\n\n","58832b77":"Setelah data tersebut di pisah, maka kedua data tersebut kita pecah menjadi 2 bagian lagi yaitu x dan y, yang dimana x merupakan column prediktor dan y adalah target.","879e1ad6":"Setelah kita memanggil function fill_na, maka kita perlu mengecek kembali data kita.","67791f36":"Sekarang kita akan mulai membuat model prediksi untuk memprediksi nilai NA di column WindGustDir. Disini saya menggunakan model Decision Tree dari sklearn.\n\nSebelum model tersebut belajar, disini data `df_fill_train_x` yang bertipe data cateogry akan kita rubah menggunakan one hot encoder dari function `get_dummies` dari pandas.","96a24a1d":"Setelah itu, kita membuat sebuah function yang bernama `fill_na()` untuk mengisi missing value yang kita punya sebelumnya. Disini akan kita manipulasi data yang berisikan missing value sesuai dengan tipe data.\n\n1.   Category\n\n> Untuk column yang bertipe data category, kita akan mengisikan missing value dengan modus \/ mode dari column tersebut\n\n\n2.   Float \/ int\n\n\n> Untuk column yang bertipe data angka \/ float, kita akan mengisinya dengan mean\/median. untuk column yang mempunyai nilai zscore lebih dari 3 atau kurang dari 3, maka akan diisi oleh `median`. Jika tidak ada data yang zscore nya lebih atau kurang dari 3, maka akan menggunakan `mean`\n\nSetelah data tersebut terisi, maka data tersebut akan di return\n\n\n\n","879ed4ad":"Setelah itu baru kita dapat menggunakan classification report jika semua data target sudah menjadi category. \n\nDisini kita mendapatkan hasil yang cukup baik yang dimana untuk model awal dengan data fill kita mendapatkan akurasi sekitar 80% dan untuk model kedua dengan data drop kita mendapatkan akurasi sekitar 81% yang berarti dari kedua data yang kita pakai tidak menambahkan akurasi yang signifikan.","4bb96544":"Selanjut kita akan menggunakan `LabelEncoder` untuk merubah target data kita menjadi numeric agar bisa kita proses di step selanjutnya.","7a8fbf79":"Setelah itu kita mengecek data kita sendiri.","0414d9e1":"Setelah kita melihat sekilas data tersebut, maka kita juga akan melihat summary dari data tersebut menggunakan function `describe()` untuk melihat summary dari setiap column yang bertipe angka.","f5a67d59":"Setelah data prediksi kita dapat, maka kita akan mengganti nilai NA tersebut menjadi hasil prediksi yang sudah kita punya. Sehingga data yang kita punya sudah tidak ada yang NA, dan kita akan gabungkan semua data yang kita punya dari data train dan test yang sudah tidak ada NA.","2d065a1c":"<a href=\"https:\/\/colab.research.google.com\/github\/mochaji27\/data_science\/blob\/main\/Untitled7.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","5e00f7cf":"Ternyata masih ada 1 column WindGustDir yang masih mempunyai nilai NA, ternyata column tersebut mempunyai data null yang lebih banyak dari pada data yang terisi. Dan column WindGustDir bertipe data category yang dimana column yang bertipe data category akan mengisi nilai nullnya dengan cara mengambil modus nya, dan ternyata modus dari data tersebut ialah NA, sehingga NA dari data tersebut tidak berubah.","fcfeb7f2":"Untuk model terakhir kita akan menggunakan neural network dari keras. Disini kita akan mengimport library-library yang akan kita gunakan nantinya.","dccdf5c1":"Setelah model tersebut belajar, maka kita akan mulai memprediksi menggunakan data test yang dimana target dari data terset tersebut akan kita isi dari hasil prediksi. Disini data `df_fill_test_x` yang bertipe data category akan kira rubah menggunakan one hot encoder seperti sebelumnya agar bisa di gunakan dalam prediksi. Dan hasil prediksi akan kita simpan dalam variable `pred`","4033d8d2":"Setelah kita membuat 2 model tersebut, sekarang kita bisa langsung training model dengan epoch sebesar 20 dan batch size sebesar 256 untuk kedua model tersebut.","4db3197c":"Setelah kita rasa data kita sudah siap dan bertipe angka dan matrix, maka data yang kita punya siap kita gunakan untuk pembelajaran mesin.\n\nDisini kita membuat 2 model MLP dari neural network untuk data fill dan drop.\n\nKita buat model dengan menggunakan 1 input layer yang mempunyai 115 neuron sesuai dengan total prediktor yang kita punya, 2 hiden layer yang menggunakan 128 layer dengan aktivasi `relu` dengan dropout sebesar 0.2, dan 1 output layer berisikan 1 neuron yang mempunyai activation `sigmoid`. Disini juga kita menggunakan optimizer dari Adam dengan learning rate sebesar 0.0001, dan juga menggunakan loss function `binary_crossentropy` karena kita memiliki 2 target label, dan terakhir kita menggunakan metrics `accuracy` karena model kita merupakan supervised learning.","9637f31d":"Sekarang kita akan memvalidasi kedua hasil prediksi kita menggunakan `confusion_matrix` dan `accuracy_score`. disini kita mendapatkan beberapa hasil.\n\n1.   Hasil akurasi yang pertama untuk data yang kita isi ialah sebesar 75%\n2.   Hasil akurasi yang kedua untuk data NA yang kita buang ialah sebesar 87%\n\n\n\n\nDari hasil ini, kita bisa simpulkan bahwa model Decision Tree dengan membuang data yang NA dapat meningkatkan hasil prediksi dari pada data yang kita isi pada nilai NA nya.","98430a35":"Setelah kita pastikan sudah tidak ada data yang NA \/ missing, maka kita akan pakai data `df_clean` dan kita pecah kembali data tersebut menjadi `X_train_fill` yang berisikan data prediktor untuk model belajar, `y_train_fill` yang berisikan data target untuk model belajar, `X_test_fill` yang berisikan data prediktor untuk memvalidasi hasil model yang sudah kita buat, dan `y_test_fill` yang berisikan data target untuk memvalidasi hasil model yang sudah kita buat. Kita pecah menjadi 2 tipe data tersebut sebesar 0.8 untuk train, dan 0.2 untuk test","8fc10f4e":"Setelah data `df_train` dan `df_drop` kita pecah, kita sebaiknya melihat besaran data prediktor dari yang yang kita punya.","b9aba988":"Setelah library di import, kita akan mengimport data yang sudah di download. Disini saya menggunakan google colab yang dimana akan mengimport file dari google drive. \n\nSetelah mengimport file, kita akan menghapus \/ drop column yang bernama Date dikarenakan kita tidak menggunakan column tersebut untuk prediksi karena bertipe data Date. Sebenernya bisa kalau kita mau memakai column tersebut dengan mengekstrak hari dan bulan nya, tetapi disini saya lebih prefer kalau date tersebut dibuang.\n\n\n","f8f7ad62":"Sekarang kita akan menggabungkan data yang sudah kita resample untuk data fill dan drop.","ec2baeb8":"Setelah model kita buat, maka kita akan langsung memprediksi model model tersebut.","bbfe4c13":"Sekarang kita harus lihat apakah data target yang kita punya itu balanced atau imbalanced. Dan ternyata data yang kita punya imbalanced.","ae28cb83":"Setelah kita pastikan sudah tidak ada data yang NA \/ missing, maka kita akan pakai data `df_drop` dan kita pecah kembali data tersebut menjadi `X_train_drop` yang berisikan data prediktor untuk model belajar, `y_train_drop` yang berisikan data target untuk model belajar, `X_test_drop` yang berisikan data prediktor untuk memvalidasi hasil model yang sudah kita buat, dan `y_test_drop` yang berisikan data target untuk memvalidasi hasil model yang sudah kita buat. Kita pecah menjadi 2 tipe data tersebut sebesar 0.8 untuk train, dan 0.2 untuk test","f1d4ad78":"Setelah kita melihat summary dari semua column yang bertipe data angka, kita akan melihat apakah data yang kita punya mengandung missing values atau tidak dengan menggunakan function `isnull()` dan disambungkan dengan `sum()`.","9677368d":"Setelah kita membuat function fill_na, maka kita akan memanggilnya untuk menghilangkan NA di dalam data kita dengan cara masing-masing sesuai dengan tipe datanya. Dan kita simpan kedalam df_fill. Yang berarti sekarang kita punya 2 dataframe yang berbeda yaitu `df_fill` yang berisikan data NA yang sudah kita isi dan `df_drop` yang berisikan data NA yang sudah kita buang.","86503029":"Setelah model tersebut belajar, maka sekarang kita dapat memprediksi data test yang kita punya untuk memvalidasi model yang sudah kita buat. Setelah di prediksi, maka hasil prediksi yang sudah di buat masih berupa angka yang dimana harus kita rubah kedalam category. ","9cf00b39":"Ternyata data kita memiliki banyak missing values. Kita akan melihat berapa persentase data yang hilang dibandingkan dengan keseluruhan data yang kita punya.","05891847":"Ternyata kita mempunyai 10% data missing values dari total data yang kita punya. Nanti kita akan memanipulasi missing value tersebut.\n\n\nSetelah itu kita merubah tipe data object menjadi category sesuai dengan informasi dari setiap column nya menggunakan function `astype()`.","4e81af8f":"Pada case ini saya menggunakan beberapa library yaitu numpy, pandas, scipy, statistic, sklearn, matplotlib.","718b5dc5":"Pada tahap ini, disini akan dilakukan standarization menggunakan `StandardScaler` agar data yang bertipe angka yang mempunyai outliers dapat kita perkecil rentan outliers nya.","f6885eda":"Disini akan di cek kembali dari data tersebut, dan hasilnya sudah tidak ada data yang NA, sehingga kita dapat memprediksi target utama kita yaitu `RainTomorrow`.","7ab1b685":"Pada tahap ini, disini akan dilakukan normalization menggunakan `MinMaxScaler` agar data yang bertipe angka memiliki value yang mempunyai range 0-1 agar data memiliki varians yang kecil.","28da04bb":"Disini juga kita dapat melihat keseluruhan matrix dari hasil prediksi dan model kita menggunakan `classification_report`","1c5ab60b":"Untuk hasilnya sendiri kita mempunyai beberapa model dari logistic regression, decision tree, dan juga neural network.\n\nUntuk hasil dari Decision tree sebagai berikut.\n\n1.   Hasil akurasi yang pertama untuk data yang kita isi ialah sebesar 75%\n2.   Hasil akurasi yang kedua untuk data NA yang kita buang ialah sebesar 87%\n\nUntuk hasil dari Logistic Regression sebagai berikut.\n\n1. Data yang kita isi untuk nilai NA mendapatkan akurasi sebesar 79%\n2. Data yang kita buang untuk nilai NA mendapatkan akurasi sebesar 81%\n\ndan Untuk hasil dari neural network sebagai berikut.\n\n1. Data yang kita isi untuk nilai NA mendapatkan akurasi kurang lebih sebesar 80%\n2. Data yang kita buang untuk nilai NA mendapatkan akurasi kurang lebih sebesar 81%\n\nSummary dari model yang kita buat untuk data di atas ialah untuk mendapatkan akurasi yang tinggi, kita dapat menggunakan model Decision tree dengan menghapus row yang memiliki nilai NA, sehingga kita bisa mendapatkan akurasi sebesar 87%\n","8f9472f8":"Selanjutnya kita akan menggunakan model LogisticRegression. Disini tetap kita transformasi data kita yang bertipe data category menggunakan `get_dummies`.","0214ab2f":"Setelah kita pecah menjadi x dan y, kita akan mengecek lagi column yang kita pecah tersebut.","aea3b3d1":"Setelah data tersebut sudah kita resample, kita harus mengecek datanya kembali apakah data yang kita punya sudah berubah menjadi balanced atau tidak.","22e581ea":"Disini kita bisa lihat untuk model logistic regression bahwa :\n\n1. Data yang kita isi untuk nilai NA mendapatkan akurasi sebesar 79%\n2. Data yang kita buang untuk nilai NA mendapatkan akurasi sebesar 81%\n\n\nYang berarti bisa kita simpulkan untuk model logistic regression lebih baik kita membuang data NA tersebut dari pada kita isi data tersebut.","09fefd58":"\nSetelah kita membuat model, maka kita siap untuk memprediksi. Disini kita memprediksi menggunakan 2 model yang sudah kita buat dan hasil prediksinya disimpan dalam variable `pred_tree_fill` dan `pred_tree_drop`.","ca7fa4f0":"Setelah data tersebut di import, kita akan melihat semua tide data pada setiap column. Disini menurut saya ada yang kurang sesuai, misalnya seperti Location maupun RainToday yang seharusnya bertipe category. Maka dari itu nanti akan kita rubah tipe data dari column tersebut.","37812658":"Disini kita membuat variable baru yang bernama `df_dropna` yang dimana variable ini menampung data yang kita punya sebelumnya dengan menghapus row yang mempunyai missing value. ","4b81d018":"### Prediksi hujan esok hari di australia\n\nDataset yang digunakan disini di ambil dari sumber kaggle : \nhttps:\/\/www.kaggle.com\/jsphyg\/weather-dataset-rattle-package\n\nPada case ini ingin memprediksi apakah besok akan hujan atau tidak di negara australia dengan menggunakan beberapa variable. Salah satu variable yang akan menjadi target variable ialah RainTomorrow. \n\nUntuk memprediksinya, disini akan membandingkan beberapa model yaitu Logistic Regression, Decisition Tree, dan Multilayer Perceptron"}}