{"cell_type":{"d4834fb7":"code","5a57760a":"code","c9e4a4d8":"code","a6bd77d9":"code","7c301d52":"code","bc60be04":"code","6416bedb":"code","fde925ba":"code","bee008eb":"code","6a7696ea":"code","3f2f0fa3":"code","82eff4b8":"code","6c0b98ff":"code","379fae2f":"code","6d170769":"code","bfe21e05":"code","4dc143c0":"code","86485920":"code","0f3866ad":"code","5e628bee":"code","dc271fb6":"code","148bced7":"code","1629564f":"code","c6abb69a":"code","ab468909":"code","759f0e39":"code","6d023bfe":"code","80383a58":"code","8f92e8ef":"code","626f0ecc":"code","f76a7d98":"code","a9d49606":"code","9c7cef09":"code","86eab71a":"code","aa434841":"code","0a6e1198":"code","7d1db6b7":"code","c837e836":"code","117635ff":"code","f4525472":"code","07f5209e":"code","ea679477":"code","53f33536":"code","fc246a13":"code","b68800cb":"code","71b1e6f3":"code","973b8a5b":"code","0456159c":"code","82d65ffa":"code","c0617f08":"code","9a92e453":"code","c064b8a5":"code","56f6ca17":"code","81e251fb":"code","fd28cfbf":"code","05345b6c":"code","9887658a":"code","2a2f739c":"code","ab637f6f":"code","53e0ab0d":"code","da974767":"code","88f75fa8":"code","c7f5dd80":"code","82813f94":"code","89e69a4f":"code","925ee7b6":"code","75dd6d59":"code","61c6887c":"code","df2e559a":"code","7efe83e9":"code","1412e43b":"code","7b5756ad":"code","a7372a22":"code","a2e5b66f":"code","1d2fdd76":"code","188a059c":"code","a31ecdda":"code","80458116":"code","bfc846cd":"code","d94b348d":"code","22051d62":"code","0fbedfd3":"code","a270f0f6":"code","7e82d8d4":"code","94e2f3fc":"code","00915ab3":"code","b231c54a":"code","63ee353c":"code","7988d6ff":"code","2fe83f06":"code","a0bec1eb":"code","b45cd997":"code","a27bfd63":"code","11bf1a41":"code","548fb1dc":"code","89616481":"code","cb3273b3":"code","3d0d81c1":"code","48ba14dd":"code","0ee95a52":"code","613716b9":"code","8bd88fb7":"code","2ce9275f":"markdown","9e8a4451":"markdown","d5655db9":"markdown","71afe34b":"markdown","14f8b436":"markdown","8a103661":"markdown","ca97217c":"markdown","64b6811e":"markdown","1b7a9310":"markdown","639491b2":"markdown","0109a18b":"markdown","8cf41dfc":"markdown"},"source":{"d4834fb7":"import tensorflow as tf\nprint(tf.__version__)","5a57760a":"# check GPU availability\nprint(tf.test.is_built_with_cuda())\nprint(tf.config.list_physical_devices())\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices(\"GPU\")))","c9e4a4d8":"gpu_info = !nvidia-smi\ngpu_info = \"\\n\".join(gpu_info)\nif gpu_info.find(\"failed\") >= 0:\n    print(\n        'Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, '\n    )\n    print(\"and then re-execute this cell.\")\nelse:\n    print(gpu_info)","a6bd77d9":"import os\n\nimport numpy as np\nimport pandas as pd\n\n# Setting seed for reproducibility\ntf.random.set_seed(1234)\nnp.random.seed(1234)\n\n# setup training log for tensorborad\nworkingfolder=\".\"\n# workingfolder=\"\/content\/drive\/My Drive\/Colab Notebooks\"\n\nlog_base = os.path.join(workingfolder, \"logs\", \"fit\")\n\nimport io\nimport itertools\nimport shutil\nimport tempfile\nimport time\n\nimport sklearn.metrics","7c301d52":"from pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\n\nimport matplotlib as mpl\nfrom matplotlib import dates as mdates\nfrom matplotlib import pyplot as plt\nfrom matplotlib import rc\nfrom matplotlib.ticker import FormatStrFormatter\n\nrc(\"font\", **{\"family\": \"sans-serif\", \"sans-serif\": [\"Helvetica\"]})\n\nimport seaborn as sns\n\nparams = {\n    \"axes.labelsize\": 8,\n    \"font.size\": 6,\n    \"font.family\": \"sans-serif\",\n    \"font.sans-serif\": \"Helvetica\",\n    \"legend.fontsize\": 8,\n    \"xtick.labelsize\": 8,\n    \"ytick.labelsize\": 8,\n    \"text.usetex\": False,\n    \"figure.figsize\": [4, 4],\n    #     'path.simplify': True,\n    #     'xtick.major.size':   6,\n    #     'ytick.major.size':   6,\n    #     'xtick.minor.size':   3,\n    #     'ytick.minor.size':   3,\n    #     'xtick.major.width'       :   1,\n    #     'ytick.major.width'       :   1,\n    #     'xtick.minor.width'       :   1,\n    #     'ytick.minor.width'       :   1,\n    #     'lines.markeredgewidth'   :   1,\n    \"legend.frameon\": False,\n    #     'legend.handletextpad'    :   0.3,\n}\nmpl.rcParams.update(params)\n\ncolors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n\nsns.set_context(\"paper\")\nsns.set(font=\"Helvetica\")\nsns.set_style(\"white\", {\"font.family\": \"sans-serif\", \"font.serif\": [\"Helvetica\"]})\n\nimport datetime\n\nfrom matplotlib import units as munits\n\nformats = [\n    \"%y\",  # ticks are mostly years\n    \"%b\",  # ticks are mostly months\n    \"%d\",  # ticks are mostly days\n    \"%H:%M\",  # hrs\n    \"%H:%M\",  # min\n    \"%S.%f\",\n]  # secs\n# these can be the same, except offset by one level....\nzero_formats = [\"\"] + formats[:-1]\n# ...except for ticks that are mostly hours, then its nice to have month-day\nzero_formats[3] = \"%d-%b\"\noffset_formats = [\n    \"\",\n    \"%Y\",\n    \"%b %Y\",\n    \"%d %b %Y\",\n    \"%d %b %Y\",\n    \"%d %b %Y %H:%M\",\n]\n\nconverter = mdates.ConciseDateConverter(\n    formats=formats, zero_formats=zero_formats, offset_formats=offset_formats\n)\n\nmunits.registry[np.datetime64] = converter\nmunits.registry[datetime.date] = converter\nmunits.registry[datetime.datetime] = converter","bc60be04":"# os.environ[\"PATH\"] += os.pathsep + 'C:\\Program Files (x86)\\Graphviz2.38\/bin\/'","6416bedb":"def createf(path=None):\n    try:\n        os.makedirs(path)\n    except OSError:\n        print (\"Creation of the directory %s failed\" % path)\n    else:\n        print (\"Successfully created the directory %s \" % path)","fde925ba":"createf(\".\/figs\")\ncreatef(\".\/models\")\ncreatef(\".\/run\")\ncreatef(\".\/logs\/fit\")","bee008eb":"try:\n    shutil.rmtree(log_base)\nexcept OSError as e:\n    print(\"Error: %s : %s\" % (log_base, e.strerror))","6a7696ea":"# Load the TensorBoard notebook extension, (it is not working for jupyternote book in vscode)\n# for vscode, open terminal and run: (in this folder)\n# tensorboard --logdir=script\\logs\n# %load_ext tensorboard\n# %tensorboard --logdir logs\n","3f2f0fa3":"%load_ext tensorboard","82eff4b8":"%tensorboard --logdir logs","6c0b98ff":"def plot_series(time, series, format=\"-\", start=0, end=None):\n    plt.plot(time[start:end], series[start:end], format)\n    plt.xlabel(\"Time index\")\n    plt.ylabel(\"Monthly Mean Total Sunspot Number\")\n    # plt.grid(True)","379fae2f":"import csv\ntime_step = []\nsunspots = []\n\nfilepath=\"..\/input\/sunspots\/Sunspots.csv\"\n\nwith open(filepath) as csvfile:\n  reader = csv.reader(csvfile, delimiter=',')\n  next(reader)\n  for row in reader:\n    sunspots.append(float(row[2]))\n    time_step.append(int(row[0]))\n\nseries = np.array(sunspots)\ntime = np.array(time_step)\nfig = plt.figure(figsize=(6, 4))\nplot_series(time, series)\n\nfig_name=os.path.join(workingfolder, 'figs','exp_algsel'+'_1_'+'sunspots'+'.pdf')\nfig.savefig(fig_name,dpi=600, bbox_inches=\"tight\")","6d170769":"split_time = 2800\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]","bfe21e05":"def univariate_data(dataset, start_index, end_index, history_size, target_size):\n  data = []\n  labels = []\n\n  start_index = start_index + history_size\n  if end_index is None:\n    end_index = len(dataset) - target_size\n\n  for i in range(start_index, end_index):\n    indices = range(i-history_size, i)\n    # Reshape data from (history_size,) to (history_size, 1)\n    data.append(np.reshape(dataset[indices], (history_size, 1)))\n    labels.append(dataset[i+target_size])\n  return np.array(data), np.array(labels)","4dc143c0":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    ds = ds.map(lambda w: (w[:-1], w[1:]))\n    return ds.batch(batch_size).prefetch(1)","86485920":"def model_forecast(model, series, window_size):\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size))\n    ds = ds.batch(32).prefetch(1)\n    forecast = model.predict(ds)\n    return forecast","0f3866ad":"# tf.keras.backend.clear_session()\n# tf.keras.random.set_seed(51)\nnp.random.seed(51)\npast_history = 60\nfuture_target = 30\nSTEP = 1\nEPOCH=1000\nBATCH_SIZE=128\n\nxs_train, y_train = univariate_data(x_train, 0, None, past_history, future_target)\nxs_test,y_test =univariate_data(x_valid, 0, None, past_history, future_target)","5e628bee":"y_train = np.expand_dims(y_train, axis=1)\ny_test = np.expand_dims(y_test, axis=1)","dc271fb6":"Max = xs_train.max()\nMin = xs_train.min()\nprint(Max,Min)","148bced7":"xs_train_norm = (xs_train - Min)\/(Max - Min)\n\nxs_test_norm = (xs_test - Min)\/(Max - Min)","1629564f":"def model_build():\n    tf.keras.backend.clear_session()\n    model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv1D(filters=64, kernel_size=5,\n              strides=1, padding=\"causal\",\n              activation=\"relu\",\n              input_shape=[None, 1]),\n    tf.keras.layers.LSTM(32), # return_sequences=True\n    tf.keras.layers.Dense(10, activation=\"relu\"),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * (Max-Min) + Min)\n    ])\n    model.compile(loss='mae',\n              optimizer=tf.keras.optimizers.RMSprop(),\n              metrics=[\"mape\"])\n    return model    ","c6abb69a":"model_CNNLSTM=model_build()\nmodel_CNNLSTM.summary()","ab468909":"model_name='model_CNNLSTM_univariate_singlestep_IDs'\n\nmodel_dir=os.path.join(workingfolder,'models')\n\nlog_dir = os.path.join(log_base, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'_'+model_name)\n\n# define path to save model\nmodel_path =os.path.join(model_dir,model_name+'.h5') \n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, verbose=1, mode='auto')\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='auto', verbose=0)\ncsv_logger = tf.keras.callbacks.CSVLogger(os.path.join(workingfolder, \"run\",model_name+\"_\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_history_log.csv\"), append=True)\n\nCALLBACKS = [tensorboard_callback, earlystopping, checkpoint, csv_logger]","759f0e39":"fig_name=os.path.join(workingfolder,'figs',model_name+'_modelplot'+'.png')\n\ntf.keras.utils.plot_model(model_CNNLSTM, fig_name, show_shapes=True, show_layer_names=False, rankdir='LR', dpi=300)","6d023bfe":"history_CNNLSTM = model_CNNLSTM.fit(xs_train_norm, y_train, epochs=EPOCH, batch_size=BATCH_SIZE, validation_split = 0.2,\n                    callbacks=CALLBACKS)","80383a58":"scores_test_CNNLSTM = model_CNNLSTM.evaluate(xs_test_norm,y_test, verbose=2)","8f92e8ef":"y_pred_test=model_CNNLSTM.predict(xs_test_norm)\ny_true_test=y_test","626f0ecc":"y_pred_test.shape","f76a7d98":"y_true_test.shape","a9d49606":"fig = plt.figure(figsize=(6, 3))\nplt.plot(y_pred_test.flatten(), color=\"red\")\nplt.plot(y_true_test.flatten(), color=\"green\")\n# plt.title('prediction')\nplt.ylabel('Flow')\nplt.xlabel('row')\nplt.legend(['Predicted', 'True'], loc='best')\nplt.show()","9c7cef09":"def model_build():\n    tf.keras.backend.clear_session()\n    model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv1D(filters=64, kernel_size=5,\n              strides=1, padding=\"causal\",\n              activation=\"relu\",\n              input_shape=[past_history, 1]),\n    tf.keras.layers.MaxPooling1D(pool_size=2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(10, activation=\"relu\"),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * (Max-Min) + Min)\n    ])\n    model.compile(loss='mae',\n              optimizer=tf.keras.optimizers.RMSprop(),\n              metrics=[\"mape\"])\n    return model   ","86eab71a":"model_CNN=model_build()\nmodel_CNN.summary()","aa434841":"model_name='model_CNN_univariate_singlestep_IDs'\n\nmodel_dir=os.path.join(workingfolder,'models')\n\nlog_dir = os.path.join(log_base, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'_'+model_name)\n\n# define path to save model\nmodel_path =os.path.join(model_dir,model_name+'.h5') \n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, verbose=1, mode='auto')\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='auto', verbose=0)\ncsv_logger = tf.keras.callbacks.CSVLogger(os.path.join(workingfolder, \"run\",model_name+\"_\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_history_log.csv\"), append=True)\n\nCALLBACKS = [tensorboard_callback, earlystopping, checkpoint, csv_logger]","0a6e1198":"fig_name=os.path.join(workingfolder,'figs',model_name+'_modelplot'+'.png')\n\ntf.keras.utils.plot_model(model_CNN, fig_name, show_shapes=True, show_layer_names=False, rankdir='LR', dpi=300)","7d1db6b7":"history_CNN = model_CNN.fit(xs_train_norm, y_train, epochs=EPOCH, batch_size=BATCH_SIZE, validation_split = 0.2,\n                    callbacks=CALLBACKS)","c837e836":"scores_test_CNN = model_CNN.evaluate(xs_test_norm,y_test, verbose=2)","117635ff":"y_pred_test=model_CNN.predict(xs_test_norm)\ny_true_test=y_test","f4525472":"fig = plt.figure(figsize=(6, 3))\nplt.plot(y_pred_test.flatten(), color=\"red\")\nplt.plot(y_true_test.flatten(), color=\"green\")\n# plt.title('prediction')\nplt.ylabel('Flow')\nplt.xlabel('row')\nplt.legend(['Predicted', 'True'], loc='best')\nplt.show()","07f5209e":"def model_build2():\n    model = tf.keras.models.Sequential([\n    tf.keras.layers.LSTM(64, return_sequences=True,input_shape=[past_history, 1]),\n    tf.keras.layers.LSTM(32), # , return_sequences=True\n    tf.keras.layers.Dense(10, activation=\"relu\"),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * (Max-Min) + Min)\n    ])\n    model.compile(loss='mae',\n              optimizer=tf.keras.optimizers.RMSprop(),\n              metrics=[\"mape\"])\n    return model   ","ea679477":"model_LSTM=model_build2()\nmodel_LSTM.summary()","53f33536":"model_name='model_LSTM_univariate_singlestep_IDs'\nmodel_dir=os.path.join(workingfolder,'models')\n\nlog_dir = os.path.join(log_base, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'_'+model_name)\n\n# define path to save model\nmodel_path =os.path.join(model_dir,model_name+'.h5') \n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, verbose=1, mode='auto')\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='auto', verbose=0)\ncsv_logger = tf.keras.callbacks.CSVLogger(os.path.join(workingfolder, \"run\",model_name+\"_\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_history_log.csv\"), append=True)\n\nCALLBACKS = [tensorboard_callback, earlystopping, checkpoint, csv_logger]","fc246a13":"fig_name=os.path.join(workingfolder,'figs',model_name+'_modelplot'+'.png')\n\ntf.keras.utils.plot_model(model_LSTM, fig_name, show_shapes=True, show_layer_names=False,\n    rankdir='LR', expand_nested=True, dpi=300)","b68800cb":"\nhistory_LSTM = model_LSTM.fit(xs_train_norm, y_train, epochs=EPOCH, batch_size=BATCH_SIZE, validation_split = 0.2,\n                    callbacks=CALLBACKS)\n","71b1e6f3":"# rnn_forecast = model_forecast(model1, series[..., np.newaxis], window_size)\n# rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]","973b8a5b":"# plt.figure(figsize=(10, 6))\n# plot_series(time_valid, x_valid)\n# plot_series(time_valid, rnn_forecast)","0456159c":"scores_test_LSTM = model_LSTM.evaluate(xs_test_norm,y_test, verbose=2)","82d65ffa":"y_pred_test=model_LSTM.predict(xs_test_norm)\ny_true_test=y_test","c0617f08":"fig = plt.figure(figsize=(6, 3))\nplt.plot(y_pred_test.flatten(), color=\"red\")\nplt.plot(y_true_test.flatten(), color=\"green\")\n# plt.title('prediction')\nplt.ylabel('Flow')\nplt.xlabel('row')\nplt.legend(['Predicted', 'True'], loc='best')\nplt.show()","9a92e453":"def model_build_biLSTM():\n    model = tf.keras.models.Sequential([\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True), input_shape=[past_history, 1]),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)), # , return_sequences=True\n    tf.keras.layers.Dense(10, activation=\"relu\"),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * (Max-Min) + Min)\n    ])\n    model.compile(loss='mae',\n              optimizer=tf.keras.optimizers.RMSprop(),\n              metrics=[\"mape\"])\n    return model   ","c064b8a5":"model_BiLSTM=model_build_biLSTM()\nmodel_BiLSTM.summary()","56f6ca17":"model_name='model_BiLSTM_univariate_singlestep_IDs'\nmodel_dir=os.path.join(workingfolder,'models')\n\nlog_dir = os.path.join(log_base, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'_'+model_name)\n\n# define path to save model\nmodel_path =os.path.join(model_dir,model_name+'.h5') \n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, verbose=1, mode='auto')\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='auto', verbose=0)\ncsv_logger = tf.keras.callbacks.CSVLogger(os.path.join(workingfolder, \"run\",model_name+\"_\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_history_log.csv\"), append=True)\n\nCALLBACKS = [tensorboard_callback, earlystopping, checkpoint, csv_logger]","81e251fb":"fig_name=os.path.join(workingfolder,'figs',model_name+'_modelplot'+'.png')\n\ntf.keras.utils.plot_model(model_BiLSTM, fig_name, show_shapes=True, show_layer_names=False,\n    rankdir='LR', expand_nested=True, dpi=300)","fd28cfbf":"history_BiLSTM = model_BiLSTM.fit(xs_train_norm, y_train, epochs=EPOCH, batch_size=BATCH_SIZE, validation_split = 0.2,\n                    callbacks=CALLBACKS)","05345b6c":"scores_test_BiLSTM = model_BiLSTM.evaluate(xs_test_norm,y_test, verbose=2)","9887658a":"y_pred_test=model_BiLSTM.predict(xs_test_norm)\ny_true_test=y_test","2a2f739c":"fig = plt.figure(figsize=(6, 3))\nplt.plot(y_pred_test.flatten(), color=\"red\")\nplt.plot(y_true_test.flatten(), color=\"green\")\n# plt.title('prediction')\nplt.ylabel('Flow')\nplt.xlabel('row')\nplt.legend(['Predicted', 'True'], loc='best')\nplt.show()","ab637f6f":"y_train=y_train.reshape((y_train.shape[0],1))","53e0ab0d":"n_timesteps, n_features, n_outputs = xs_train_norm.shape[1], xs_train_norm.shape[2], y_train.shape[1]\nprint(n_timesteps, n_features, n_outputs)","da974767":"# reshape into subsequences [samples, time steps, rows, cols, channels]\nn_steps = 2\nn_length = int (xs_train_norm.shape[1] \/n_steps)\nn_features = xs_train_norm.shape[-1]\n\nprint(n_length, n_features)","88f75fa8":"x_5d= xs_train_norm.reshape((xs_train_norm.shape[0], n_steps, 1, n_length, n_features))\nx_5d.shape","c7f5dd80":"train_y = y_train.reshape((y_train.shape[0], y_train.shape[1], 1))","82813f94":"train_y.shape","89e69a4f":"def model_build3():\n    model = tf.keras.models.Sequential([\n    tf.keras.layers.ConvLSTM2D(filters=64, kernel_size=(1,3), \n                               activation='relu',\n#                                padding='same',\n                               input_shape=(n_steps, 1, n_length, n_features), return_sequences=True),\n#     tf.keras.layers.ConvLSTM2D(filters=32, kernel_size=(1,3),\n#                                padding='same',\n#                                activation='relu',return_sequences=True),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.RepeatVector(n_outputs),\n    tf.keras.layers.LSTM(64, activation='relu', return_sequences=True),\n    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(30, activation='relu')),\n    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)),\n    tf.keras.layers.Lambda(lambda x: x * (Max-Min) + Min)\n    ])\n    model.compile(loss='mae',\n              optimizer=tf.keras.optimizers.RMSprop(),\n              metrics=[\"mape\"])\n    return model   ","925ee7b6":"model_ConvLSTM2D=model_build3()\nmodel_ConvLSTM2D.summary()","75dd6d59":"model_name='model_CONVLSTM2D_univariate_singlestep_IDs'\nmodel_dir=os.path.join(workingfolder,'models')\n\nlog_dir = os.path.join(log_base, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'_'+model_name)\n\n# define path to save model\nmodel_path =os.path.join(model_dir,model_name+'.h5') \n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, verbose=1, mode='auto')\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='auto', verbose=0)\ncsv_logger = tf.keras.callbacks.CSVLogger(os.path.join(workingfolder, \"run\",model_name+\"_\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_history_log.csv\"), append=True)\n\nCALLBACKS = [tensorboard_callback, earlystopping, checkpoint, csv_logger]","61c6887c":"fig_name=os.path.join(workingfolder,'figs',model_name+'_modelplot'+'.png')\n\ntf.keras.utils.plot_model(model_ConvLSTM2D, fig_name, show_shapes=True, show_layer_names=False,\n    rankdir='LR', expand_nested=True, dpi=300)","df2e559a":"history_ConvLSTM2D = model_ConvLSTM2D.fit(x_5d,train_y,  epochs=EPOCH, batch_size=BATCH_SIZE, validation_split = 0.2,\n                    callbacks=CALLBACKS)\n","7efe83e9":"xs_test_norm.shape","1412e43b":"x_val_5d=xs_test_norm.reshape(xs_test_norm.shape[0], n_steps, 1, n_length, n_features)\ny_val_reshaped=y_test.reshape((y_test.shape[0], 1, 1))","7b5756ad":"y_pred_test=model_ConvLSTM2D.predict(x_val_5d)\ny_true_test=y_val_reshaped","a7372a22":"scores_test_ConvLSTM2D = model_ConvLSTM2D.evaluate(x_val_5d, y_val_reshaped, verbose=2)","a2e5b66f":"y_true_test.shape","1d2fdd76":"y_pred_test.shape","188a059c":"fig = plt.figure(figsize=(6, 3))\nplt.plot(y_pred_test.flatten(), color=\"red\")\nplt.plot(y_true_test.flatten(), color=\"green\")\n# plt.title('prediction')\nplt.ylabel('Flow')\nplt.xlabel('row')\nplt.legend(['Predicted', 'True'], loc='best')\nplt.show()","a31ecdda":"def model_build4():\n    model = tf.keras.models.Sequential([\n    tf.keras.layers.SimpleRNN(32, return_sequences=True,input_shape=[past_history, 1]),\n    tf.keras.layers.SimpleRNN(64), # , return_sequences=True\n    tf.keras.layers.Dense(10, activation=\"relu\"),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * (Max-Min) + Min)\n    ])\n    model.compile(loss='mae',\n              optimizer=tf.keras.optimizers.RMSprop(),\n              metrics=[\"mape\"])\n    return model\n\nmodel_RNN=model_build4()\nmodel_RNN.summary()","80458116":"model_name='model_RNN_univariate_singlestep_IDs'\nmodel_dir=os.path.join(workingfolder,'models')\n\nlog_dir = os.path.join(log_base, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'_'+model_name)\n\n# define path to save model\nmodel_path =os.path.join(model_dir,model_name+'.h5') \n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, verbose=1, mode='auto')\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='auto', verbose=0)\ncsv_logger = tf.keras.callbacks.CSVLogger(os.path.join(workingfolder, \"run\",model_name+\"_\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_history_log.csv\"), append=True)\n\nCALLBACKS = [tensorboard_callback, earlystopping, checkpoint, csv_logger]","bfc846cd":"fig_name=os.path.join(workingfolder,'figs',model_name+'_modelplot'+'.png')\n\ntf.keras.utils.plot_model(model_RNN, fig_name, show_shapes=True, show_layer_names=False,\n    rankdir='LR', expand_nested=True, dpi=300)","d94b348d":"history_RNN = model_RNN.fit(xs_train_norm, y_train, epochs=EPOCH, batch_size=BATCH_SIZE, validation_split = 0.2,\n                    callbacks=CALLBACKS)","22051d62":"scores_test_RNN = model_RNN.evaluate(xs_test_norm,y_test, verbose=2)","0fbedfd3":"fig = plt.figure(figsize=(6, 3))\nplt.plot(y_pred_test.flatten(), color=\"red\")\nplt.plot(y_true_test.flatten(), color=\"green\")\n# plt.title('prediction')\nplt.ylabel('Value')\nplt.xlabel('row')\nplt.legend(['Predicted', 'True'], loc='best')\nplt.show()","a270f0f6":"def model_build5():\n    model = tf.keras.models.Sequential([\n    tf.keras.layers.GRU(32, return_sequences=True,input_shape=[past_history, 1]),\n    tf.keras.layers.GRU(64), # , return_sequences=True\n    tf.keras.layers.Dense(10, activation=\"relu\"),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * (Max-Min) + Min)\n    ])\n    model.compile(loss='mae',\n              optimizer=tf.keras.optimizers.RMSprop(),\n              metrics=[\"mape\"])\n    return model\n\nmodel_GUR=model_build5()\nmodel_GUR.summary()","7e82d8d4":"model_name='model_GUR_univariate_singlestep_IDs'\nmodel_dir=os.path.join(workingfolder,'models')\n\nlog_dir = os.path.join(log_base, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'_'+model_name)\n\n# define path to save model\nmodel_path =os.path.join(model_dir,model_name+'.h5') \n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, verbose=1, mode='auto')\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='auto', verbose=0)\ncsv_logger = tf.keras.callbacks.CSVLogger(os.path.join(workingfolder, \"run\",model_name+\"_\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"_history_log.csv\"), append=True)\n\nCALLBACKS = [tensorboard_callback, earlystopping, checkpoint, csv_logger]","94e2f3fc":"fig_name=os.path.join(workingfolder,'figs',model_name+'_modelplot'+'.png')\n\ntf.keras.utils.plot_model(model_GUR, fig_name, show_shapes=True, show_layer_names=False,\n    rankdir='LR', expand_nested=True, dpi=300)","00915ab3":"history_GUR = model_GUR.fit(xs_train_norm, y_train, epochs=EPOCH, batch_size=BATCH_SIZE, validation_split = 0.2,\n                    callbacks=CALLBACKS)\n","b231c54a":"scores_test_GUR = model_GUR.evaluate(xs_test_norm,y_test, verbose=2)","63ee353c":"fig = plt.figure(figsize=(6, 3))\nplt.plot(y_pred_test.flatten(), color=\"red\")\nplt.plot(y_true_test.flatten(), color=\"green\")\n# plt.title('prediction')\nplt.ylabel('Value')\nplt.xlabel('Row')\nplt.legend(['Predicted', 'True'], loc='best')\nplt.show()","7988d6ff":"y_pred_test_RNN=model_RNN.predict(xs_test_norm)\ny_pred_test_GUR=model_GUR.predict(xs_test_norm)\ny_pred_test_LSTM=model_LSTM.predict(xs_test_norm)\ny_pred_test_BiLSTM=model_BiLSTM.predict(xs_test_norm)\ny_pred_test_CNNLSTM=model_CNNLSTM.predict(xs_test_norm)\ny_pred_test_CNN=model_CNN.predict(xs_test_norm)\ny_pred_test_ConvLSTM2D =model_ConvLSTM2D.predict(x_val_5d)","2fe83f06":"y_pred_train_RNN=model_RNN.predict(xs_train_norm)\ny_pred_train_GUR=model_GUR.predict(xs_train_norm)\ny_pred_train_LSTM=model_LSTM.predict(xs_train_norm)\ny_pred_train_BiLSTM=model_BiLSTM.predict(xs_train_norm)\ny_pred_train_CNNLSTM=model_CNNLSTM.predict(xs_train_norm)\ny_pred_train_CNN=model_CNN.predict(xs_train_norm)\ny_pred_train_ConvLSTM2D =model_ConvLSTM2D.predict(x_5d)","a0bec1eb":"mae_test_baseline= np.mean(np.abs(y_true_test[future_target+1:].flatten()-y_true_test[:-1-future_target].flatten()))\nmae_test_RNN= np.mean(np.abs(y_pred_test_RNN.flatten()-y_true_test.flatten()))\nmae_test_GUR= np.mean(np.abs(y_pred_test_GUR.flatten()-y_true_test.flatten()))\nmae_test_LSTM= np.mean(np.abs(y_pred_test_LSTM.flatten()-y_true_test.flatten()))\nmae_test_BiLSTM= np.mean(np.abs(y_pred_test_BiLSTM.flatten()-y_true_test.flatten()))\nmae_test_CNNLSTM= np.mean(np.abs(y_pred_test_CNNLSTM.flatten()-y_true_test.flatten()))\nmae_test_CNN= np.mean(np.abs(y_pred_test_CNN.flatten()-y_true_test.flatten()))\nmae_test_ConvLSTM2D= np.mean(np.abs(y_pred_test_ConvLSTM2D.flatten()-y_true_test.flatten()))","b45cd997":"mae_train_baseline= np.mean(np.abs(y_train[future_target+1::].flatten()-y_train[:-1-future_target].flatten()))\nmae_train_RNN= np.mean(np.abs(y_pred_train_RNN.flatten()-y_train.flatten()))\nmae_train_GUR= np.mean(np.abs(y_pred_train_GUR.flatten()-y_train.flatten()))\nmae_train_LSTM= np.mean(np.abs(y_pred_train_LSTM.flatten()-y_train.flatten()))\nmae_train_BiLSTM= np.mean(np.abs(y_pred_train_BiLSTM.flatten()-y_train.flatten()))\nmae_train_CNNLSTM= np.mean(np.abs(y_pred_train_CNNLSTM.flatten()-y_train.flatten()))\nmae_train_CNN= np.mean(np.abs(y_pred_train_CNN.flatten()-y_train.flatten()))\nmae_train_ConvLSTM2D= np.mean(np.abs(y_pred_train_ConvLSTM2D.flatten()-train_y.flatten()))","a27bfd63":"mae_test_RNN","11bf1a41":"# \n# plt.style.use('seaborn-paper')\n# sns.set_context(\"paper\")\n\nlabels = ['Baseline', 'CNN', 'RNN', 'GUR', 'LSTM', 'BiLSTM', 'CNNLSTM','ConvLSTM']\ntest_mae = [mae_test_baseline, mae_test_CNN, mae_test_RNN, mae_test_GUR,mae_test_LSTM,mae_test_BiLSTM,mae_test_CNNLSTM,mae_test_ConvLSTM2D]\n# training_mae = [np.mean(history_RNN.history['loss']),\n#                     np.mean(history_GUR.history['loss']),\n#                     np.mean(history_LSTM.history['loss']),\n#                     np.mean(history_CNNLSTM.history['loss']),\n#                     np.mean(history_ConvLSTM2D.history['loss'])]\n\n# val_mae = [np.min(history_RNN.history['val_loss']),\n#                 np.mean(history_GUR.history['val_loss']),\n#                 np.mean(history_LSTM.history['val_loss']),\n#                 np.mean(history_CNNLSTM.history['val_loss']),\n#                 np.mean(history_ConvLSTM2D.history['val_loss'])]\n\ntraining_mae = [mae_train_baseline,mae_train_CNN, mae_train_RNN,mae_train_GUR,mae_train_LSTM,mae_train_BiLSTM, mae_train_CNNLSTM,mae_train_ConvLSTM2D]\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(7, 4))\nrects1 = ax.bar(x - width\/2, training_mae, width, label='train')\n# rects2 = ax.bar(x , val_mae, width, label='val')\nrects3 = ax.bar(x + width\/2, test_mae, width, label='test')\n\n# rects4 = ax.plot([mae_test_baseline,mae_test_baseline,mae_test_baseline,mae_test_baseline,mae_test_baseline], label='baseline')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Loss-MAE')\n# ax.set_title('Scores by group and gender')\nax.set_xticks(x)\nax.set_xticklabels(labels)\n\nax.legend(loc=\"best\" ) #,bbox_to_anchor=(1,0.5)\n\nfig.tight_layout()\n\nplt.ylim(10, 90)\n\nplt.show()\n\nfig_name=os.path.join(workingfolder,'figs','exp_algsel'+'tainvaltest'+'.pdf')\nfig.savefig(fig_name,dpi=600, bbox_inches=\"tight\")","548fb1dc":"fig = plt.figure(figsize=(14, 8))\n\n# plt.style.use('seaborn-paper')\n# sns.set_context(\"paper\")\n\nax1 = plt.subplot(231)\n\nplt.plot(y_pred_test_RNN, color=\"red\",label='RNN')\nplt.plot(y_true_test.flatten(), color=\"green\", label='True')\nplt.legend()\n\nax1 = plt.subplot(232)\nplt.plot(y_pred_test_GUR, color=\"red\",label='GUR')\nplt.plot(y_true_test.flatten(), color=\"green\", label='True')\nplt.legend()\n\nax1 = plt.subplot(233)\nplt.plot(y_pred_test_LSTM,color=\"red\", label='LSTM')\nplt.plot(y_true_test.flatten(), color=\"green\", label='True')\nplt.legend()\n\nax1 = plt.subplot(234)\nplt.plot(y_pred_test_BiLSTM, color=\"red\",label='LSTM')\nplt.plot(y_true_test.flatten(), color=\"green\", label='True')\nplt.legend()\n\n\nax1 = plt.subplot(235)\nplt.plot(y_pred_test_CNNLSTM,color=\"red\", label='CNNLSTM')\nplt.plot(y_true_test.flatten(), color=\"green\", label='True')\nplt.legend()\n\nax1 = plt.subplot(236)\n# plt.plot(y_true_test[1:].flatten(), label='Baseline')\n# plt.plot(y_true_test[:-1].flatten(), color=\"green\")\n# plt.legend()\n\nplt.plot(y_pred_test_ConvLSTM2D.flatten(), color=\"red\",label='ConvLSTM2D')\nplt.plot(y_true_test.flatten(), color=\"green\" ,label='True')\nplt.legend()\n\n\nfig_name=os.path.join(workingfolder,'figs','exp_algsel'+'testprediction'+'.pdf')\nfig.savefig(fig_name,dpi=600, bbox_inches=\"tight\")\n\n\n# ","89616481":"fig = plt.figure(figsize=(10, 6))\n# plt.style.use('seaborn-paper')\n# sns.set_context(\"paper\")\n\nax1 = plt.subplot(231)\n\ntest_mae_loss = np.abs(y_true_test.flatten() - y_pred_test_RNN.flatten())\nsns.distplot(test_mae_loss, bins=30, kde=True)\nplt.legend(['RNN'], loc='best')\n\n\n# test_mae_loss = np.abs(y_true_test[1:].flatten() -y_true_test[:-1].flatten())\n# sns.distplot(test_mae_loss, bins=30, kde=True)\n# plt.legend(['Baseline'], loc='best')\n\nax2 = plt.subplot(232, sharex=ax1, sharey=ax1)\n\ntest_mae_loss = np.abs(y_true_test.flatten() -y_pred_test_GUR.flatten())\nsns.distplot(test_mae_loss, bins=30, kde=True)\nplt.legend(['GUR'], loc='best')\n\n\nax3 = plt.subplot(233, sharex=ax1, sharey=ax1)\ntest_mae_loss = np.abs(y_true_test.flatten() -y_pred_test_LSTM.flatten())\nsns.distplot(test_mae_loss, bins=30, kde=True)\nplt.legend(['LSTM'], loc='best')\n\nax4 = plt.subplot(234, sharex=ax1, sharey=ax1)\ntest_mae_loss = np.abs(y_true_test.flatten() -y_pred_test_BiLSTM.flatten())\nsns.distplot(test_mae_loss, bins=30, kde=True)\nplt.legend(['BiLSTM'], loc='best')\n\nax5 = plt.subplot(235, sharex=ax1, sharey=ax1)\n\ntest_mae_loss = np.abs(y_true_test.flatten() -y_pred_test_CNNLSTM.flatten())\nsns.distplot(test_mae_loss, bins=30, kde=True)\nplt.legend(['CNN+LSTM'], loc='best')\n\n\n\nax6 = plt.subplot(236, sharex=ax1, sharey=ax1)\n\ntest_mae_loss = np.abs(y_true_test.flatten() -y_pred_test_ConvLSTM2D.flatten())\nsns.distplot(test_mae_loss, bins=30, kde=True)\nplt.legend(['ConvLSTM'], loc='best')\n\n\nfig.text(0.5, 0.04, 'mae', ha='center')\nfig.text(0.04, 0.5, 'distribution', va='center', rotation='vertical')\n# plt.legend()\n\nfig_name=os.path.join(workingfolder,'figs','exp_algsel'+'_3_'+'training history2'+'.pdf')\nfig.savefig(fig_name,dpi=600, bbox_inches=\"tight\")","cb3273b3":"# summarize history for Loss\nfig = plt.figure(figsize=(8, 6))\n\nplt.plot(history_RNN.history['loss'])\nplt.plot(history_RNN.history['val_loss'])\nplt.plot(history_CNN.history['loss'])\nplt.plot(history_CNN.history['val_loss'])\n\nplt.plot(history_GUR.history['loss'])\nplt.plot(history_GUR.history['val_loss'])\nplt.plot(history_LSTM.history['loss'])\nplt.plot(history_LSTM.history['val_loss'])\nplt.plot(history_BiLSTM.history['loss'])\nplt.plot(history_BiLSTM.history['val_loss'])\nplt.plot(history_CNNLSTM.history['loss'])\nplt.plot(history_CNNLSTM.history['val_loss'])\nplt.plot(history_ConvLSTM2D.history['loss'])\nplt.plot(history_ConvLSTM2D.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='best')\n# plt.ylim(18, 30)\n# plt.xlim(50, 250)\nplt.show()","3d0d81c1":"y_len=len(history_RNN.history['loss'])\n\n# plt.style.use('seaborn-paper')\n# sns.set_context(\"paper\")\n\ndf = pd.DataFrame(dict(epoch=np.arange(y_len),\n                       mae=history_RNN.history['loss'], Type='train', DL= 'RNN'))\ndf=df.append(pd.DataFrame(dict(epoch=np.arange(y_len),\n                       mae=history_RNN.history['val_loss'], Type='val', DL= 'RNN')))\n\ndf = pd.DataFrame(dict(epoch=np.arange(len(history_CNN.history['loss'])),\n                       mae=history_CNN.history['loss'], Type='train', DL= 'CNN'))\ndf=df.append(pd.DataFrame(dict(epoch=np.arange(len(history_CNN.history['loss'])),\n                       mae=history_CNN.history['val_loss'], Type='val', DL= 'CNN')))\n\ndf=df.append(pd.DataFrame(dict(epoch=np.arange(len(history_LSTM.history['loss'])),\n                       mae=history_LSTM.history['loss'], Type='train', DL= 'LSTM')))\ndf=df.append(pd.DataFrame(dict(epoch=np.arange(len(history_LSTM.history['loss'])),\n                       mae=history_LSTM.history['val_loss'], Type='val', DL= 'LSTM')))\n\ndf=df.append(pd.DataFrame(dict(epoch=np.arange(len(history_BiLSTM.history['loss'])),\n                       mae=history_BiLSTM.history['loss'], Type='train', DL= 'BiLSTM')))\ndf=df.append(pd.DataFrame(dict(epoch=np.arange(len(history_BiLSTM.history['loss'])),\n                       mae=history_BiLSTM.history['val_loss'], Type='val', DL= 'BiLSTM')))\n\ndf=df.append(pd.DataFrame(dict(epoch=np.arange(len(history_GUR.history['loss'])),\n                       mae=history_GUR.history['loss'], Type='train', DL= 'GUR')))\ndf=df.append(pd.DataFrame(dict(epoch=np.arange(len(history_GUR.history['loss'])),\n                       mae=history_GUR.history['val_loss'], Type='val', DL= 'GUR')))\n\ndf=df.append(pd.DataFrame(dict(epoch=np.arange(len(history_CNNLSTM.history['loss'])),\n                       mae=history_CNNLSTM.history['loss'], Type='train', DL= 'CNNLSTM')))\ndf=df.append(pd.DataFrame(dict(epoch=np.arange(len(history_CNNLSTM.history['loss'])),\n                       mae=history_CNNLSTM.history['val_loss'], Type='val', DL= 'CNNLSTM')))\n\ndf=df.append(pd.DataFrame(dict(epoch=np.arange(len(history_ConvLSTM2D.history['loss'])),\n                       mae=history_ConvLSTM2D.history['loss'], Type='train', DL= 'ConvLSTM')))\ndf=df.append(pd.DataFrame(dict(epoch=np.arange(len(history_ConvLSTM2D.history['loss'])),\n                       mae=history_ConvLSTM2D.history['val_loss'], Type='val', DL= 'ConvLSTM')))\n\nsns.relplot(x=\"epoch\", y=\"mae\",\n            hue=\"DL\",col='Type',height=5,aspect=1.25,legend='full',\n            kind=\"line\", data=df)\nsns.set_context(\"paper\")\n\n# plt.ylim(15, 23)\n# plt.xlim(50, 250)\n\n# plt.ylim(18, 30)\n# plt.xlim(50, 250)\n\nfig_name=os.path.join(workingfolder,'figs','exp_algsel'+'_2_'+'training history'+'.pdf')\nfig.savefig(fig_name,dpi=600, bbox_inches=\"tight\")","48ba14dd":"history_CNN","0ee95a52":"fig = plt.figure(figsize=(10, 10))\n\n# plt.style.use('seaborn-paper')\n# sns.set_context(\"paper\")\n\nsns.lineplot(x=\"epoch\", y=\"mae\", \n            style=\"Type\",hue='DL',\n            data=df);\n\n# plt.ylim(15, 23)\n# plt.xlim(50, 250)\n\n# plt.ylim(18, 40)\n# plt.xlim(100, 250)\n\n\nplt.legend(bbox_to_anchor=(1,0.7))\n\nfig_name=os.path.join(workingfolder,'figs','exp_algsel'+'_3_'+'training history3'+'.pdf')\nplt.savefig(fig_name,dpi=600, bbox_inches=\"tight\")","613716b9":"fig = plt.figure(figsize=(6, 4))\n\n# plt.style.use('seaborn-paper')\n# sns.set_context(\"paper\")\n\nsns.lineplot(x=\"epoch\", y=\"mae\", \n            style=\"Type\",hue='DL',\n            data=df);\n\n# plt.ylim(15, 23)\n# plt.xlim(50, 250)\n\nplt.ylim(18, 40)\n# plt.xlim(100, 250)\n\n\nplt.legend(bbox_to_anchor=(1,0.7))\n\nfig_name=os.path.join(workingfolder,'figs','exp_algsel'+'_3_'+'training history'+'.pdf')\nplt.savefig(fig_name,dpi=600, bbox_inches=\"tight\")","8bd88fb7":"# fig = plt.figure(figsize=(18, 10))\n\n# # fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, constrained_layout=True)\n# # t = np.arange(0, y_true_test[0], 1)\n\n# ax1 = plt.subplot(331)\n\n# plt.plot(y_pred_test2[:,-1,0], color=\"red\")\n# plt.plot(y_true_test.flatten(), color=\"green\")\n# plt.title('LSTM')\n# plt.ylabel('value')\n# # # plt.xlabel('Row')\n# plt.legend(['predicted', 'actual data'], loc='best')\n\n# # plt.setp(ax1.get_xticklabels(), fontsize=6)\n\n# ax2 = plt.subplot(332, sharex=ax1, sharey=ax1)\n\n# plt.plot(y_pred_test1[:,-1,0], color=\"red\")\n# plt.plot(y_true_test.flatten(), color=\"green\")\n# plt.title('CNN+LSTM')\n# # plt.ylabel('value')\n# plt.xlabel('timesteps')\n# plt.legend(['predicted', 'actual data'], loc='best')\n\n\n# # plt.setp(ax2.get_xticklabels(), visible=False)\n\n# ax3 = plt.subplot(333, sharex=ax1, sharey=ax1)\n\n# plt.plot(y_pred_test3.flatten(), color=\"red\")\n# plt.plot(y_true_test.flatten(), color=\"green\")\n# plt.title('ConvLTSM2D')\n# # plt.ylabel('Value')\n# # plt.xlabel('Row')\n# plt.legend(['predicted', 'actual data'], loc='best')\n\n\n# ax4 = plt.subplot(334 )\n\n# plt.plot(history2.history['mae'])\n# plt.plot(history2.history['val_mae'])\n# # # plt.title('model loss')\n# plt.ylabel('mae (training)')\n# # plt.xlabel('training epoch')\n# plt.legend(['train', 'val'], loc='best')\n\n\n\n# ax5 = plt.subplot(335, sharex=ax4,sharey=ax4)\n\n# plt.plot(history1.history['mae'])\n# plt.plot(history1.history['val_mae'])\n# # plt.title('model loss')\n# # plt.ylabel('mae')\n# plt.xlabel('training epochs')\n# plt.legend(['train', 'val'], loc='best')\n\n# ax6 = plt.subplot(336, sharex=ax4,sharey=ax4)\n\n# plt.plot(history3.history['mae'])\n# plt.plot(history3.history['val_mae'])\n# # plt.title('model loss')\n# # plt.ylabel('loss')\n# # plt.xlabel('training epoch')\n# plt.legend(['train', 'val'], loc='best')\n\n# ax7 = plt.subplot(337)\n\n# test_mae_loss=np.mean(np.abs(y_pred_test2[:,-1,0] - y_true_test), axis=1)\n# sns.distplot(test_mae_loss, bins=30, kde=True)\n# plt.ylabel('distribution')\n\n# ax8 = plt.subplot(338,sharex=ax7,sharey=ax7)\n\n# test_mae_loss=np.mean(np.abs(y_pred_test1[:,-1,0] - y_true_test), axis=1)\n# sns.distplot(test_mae_loss, bins=30, kde=True)\n# plt.xlabel('mae (test)')\n\n# ax9 = plt.subplot(339,sharex=ax7,sharey=ax7)\n\n# test_mae_loss=np.mean(np.abs(y_pred_test3 - y_true_test), axis=1)\n# sns.distplot(test_mae_loss, bins=30, kde=True)\n\n# plt.show()\n\n# fig_name=os.path.join('figs',model_name+'_1_'+'summary'+'.pdf')\n# fig.savefig(fig_name,dpi=600, bbox_inches=\"tight\")","2ce9275f":"### BiLSTM stack","9e8a4451":"## import","d5655db9":"## Model Build","71afe34b":"### simple RNN stack","14f8b436":"### ConvLSTM2D stack","8a103661":"## Data Ingestion","ca97217c":"### LSTM stack","64b6811e":"## Summary","1b7a9310":"### GRU stack","639491b2":"# ConvLSTM2D Experiment","0109a18b":"### CNN stack","8cf41dfc":"### CNN + LSTM stack"}}