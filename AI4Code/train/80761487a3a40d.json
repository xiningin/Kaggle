{"cell_type":{"fd833d9a":"code","c7f086e1":"code","9a48e843":"code","2e454174":"code","70b4a51a":"code","d7cb63fe":"code","fb439b12":"code","b3f2906b":"code","016150f4":"code","31606b53":"code","62836daf":"code","c0b98b10":"code","728f1049":"code","73e4bfe3":"code","6da888c2":"code","ac95cdfa":"code","9c960588":"code","67186f62":"code","57491a23":"code","53459b80":"code","a2138793":"code","54602497":"code","d384528c":"code","5fbeeeb4":"code","db8537a3":"code","18e4b8ab":"code","41498520":"markdown","6a16b0b6":"markdown","f4b9fe2d":"markdown","ff9dd8ea":"markdown","d2b9ce8d":"markdown","7113ce86":"markdown","344f6457":"markdown","d367c841":"markdown","15fa5597":"markdown","86a4405d":"markdown","e5725be9":"markdown","f832bb9c":"markdown","b04fb2de":"markdown","82e32950":"markdown","7664d020":"markdown"},"source":{"fd833d9a":"import cv2\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tqdm.notebook import tqdm\n\nprint(tf.__version__)","c7f086e1":"import os\nimport json\nfrom glob import glob\n\nTRAIN_PREFIX = '..\/input\/fish-data\/fish\/train'\n\ndef load_annotations():\n    boxes = dict()\n    for path in tqdm(glob('..\/input\/fish-data\/fish\/boxes\/*.json')):\n        label = os.path.basename(path).split('_', 1)[0]\n        with open(path) as src:\n            for annotation in json.load(src):\n                basename = os.path.basename(annotation['filename'])\n                annotation['filename'] = os.path.join(\n                    TRAIN_PREFIX, label.upper(), basename)\n                for rect in annotation['annotations']:\n                    rect['x'] += rect['width'] \/ 2\n                    rect['y'] += rect['height'] \/ 2\n                    rect['class'] = label\n                if os.path.isfile(annotation['filename']):\n                    boxes.setdefault(label, []).append(annotation)\n    return boxes\n\ndef draw_boxes(annotation, rectangles=None, image_size=None):\n    \n    def _draw(img, rectangles, scale_x, scale_y, color=(0, 255, 0)):\n        for i, rect in enumerate(rectangles):\n            pt1 = (int((rect['x'] - rect['width'] \/ 2) * scale_x),\n                   int((rect['y'] - rect['height'] \/ 2) * scale_y))\n            pt2 = (int((rect['x'] + rect['width'] \/ 2) * scale_x),\n                   int((rect['y'] + rect['height'] \/ 2) * scale_y))\n            img = cv2.rectangle(img.copy(), pt1, pt2, \n                                color=color, thickness=10)\n            img = cv2.putText(img.copy(), annotation['annotations'][i]['class'], tuple(np.array(pt1)+[0,-7]), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 4 )\n        return img\n    \n    def __draw(img, rectangles, scale_x, scale_y, color=(0, 255, 0)):\n        for i, rect in enumerate(rectangles):\n            pt1 = (int((rect['x'] - rect['width'] \/ 2) * scale_x),\n                   int((rect['y'] - rect['height'] \/ 2) * scale_y))\n            pt2 = (int((rect['x'] + rect['width'] \/ 2) * scale_x),\n                   int((rect['y'] + rect['height'] \/ 2) * scale_y))\n            img = cv2.rectangle(img.copy(), pt1, pt2, \n                                color=color, thickness=3)\n            img = cv2.putText(img.copy(), counts['class'][int(rect['label'])] + ': ' + str(rect['label']), tuple(np.array(pt1)+[0,-7]), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 4 )\n        return img\n    \n    scale_x, scale_y = 1., 1.\n    \n    img = cv2.imread(annotation['filename'], cv2.IMREAD_COLOR)[...,::-1]\n    if image_size is not None:\n        scale_x = 1. * image_size[0] \/ img.shape[1]\n        scale_y = 1. * image_size[1] \/ img.shape[0]\n        img = cv2.resize(img, image_size)\n        \n    img = _draw(img, annotation.get('annotations', []), scale_x, scale_y)\n    \n    if rectangles is not None:\n        img = __draw(img, rectangles, 1., 1., (255, 0, 0))\n\n    return img","9a48e843":"boxes = load_annotations()  # \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443 \u0434\u0435\u0442\u0435\u043a\u0446\u0438\u0439","2e454174":"counts = pd.DataFrame(\n    [(k, len(v)) for k, v in boxes.items()],\n    columns=['class', 'count'])\n\nfish_classes = counts['class'].values\nprint(fish_classes)\ncounts","70b4a51a":"from matplotlib import pyplot as plt\n\n\nannotation = boxes['lag'][5]\nimg = draw_boxes(annotation)\n\nplt.figure(figsize=(6, 6), dpi=240)\nplt.imshow(img)\nplt.title('{} {}x{}'.format(\n    annotation['filename'], img.shape[0], img.shape[1]));","d7cb63fe":"annotations = sum([box['annotations']\n                   for box in sum(boxes.values(), [])], [])\n\nwidths = [rect['width'] for rect in annotations]\nheights = [rect['height'] for rect in annotations]\n\nplt.hist(widths)\nplt.hist(heights);","fb439b12":"from tensorflow.keras.applications.resnet50 import ResNet50 #2, EfficientNetB3\nfrom tensorflow.keras.applications.vgg16 import VGG16\n\nIMG_HEIGHT = 736\nIMG_WIDTH =  1184\n\nfeatures = VGG16(weights='imagenet',\n                       include_top=False,\n                       input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n\n# \u0434\u043e\u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0435 5 \u0441\u043b\u043e\u0435\u0432\nfor layer in features.layers[:-5]:\n    layer.trainable = False\n    \nfeature_tensor = features.layers[-1].output\nprint(feature_tensor.shape)","b3f2906b":"FEATURE_SHAPE = (feature_tensor.shape[1],\n                 feature_tensor.shape[2])\nprint(FEATURE_SHAPE)\n\nGRID_STEP_H = IMG_HEIGHT \/ FEATURE_SHAPE[0]\nGRID_STEP_W = IMG_WIDTH \/ FEATURE_SHAPE[1]\n\nANCHOR_WIDTH = 150.\nANCHOR_HEIGHT = 150. \n\n# \u0441\u0435\u0442\u043a\u0430 \u044f\u043a\u043e\u0440\u0435\u0439, \u0440\u0430\u0437\u043c\u0435\u0440 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442\u0441\u044f \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435\u043c \n# \u0440\u0430\u0437\u043c\u0435\u0440\u0430 \u0432\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438 \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c \u0442\u0435\u043d\u0437\u043e\u0440\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\nANCHOR_CENTERS = np.mgrid[GRID_STEP_H\/2:IMG_HEIGHT:GRID_STEP_H,\n                          GRID_STEP_W\/2:IMG_WIDTH:GRID_STEP_W]\n\nANCHOR_CENTERS.shape","016150f4":"num_classes = counts.shape[0]\nnum_classes","31606b53":"from scipy.special import softmax","62836daf":"def iou(rect, x_scale, y_scale, anchor_x, anchor_y,\n        anchor_w=ANCHOR_WIDTH, anchor_h=ANCHOR_HEIGHT):\n    \n    rect_x1 = (rect['x'] - rect['width'] \/ 2) * x_scale\n    rect_x2 = (rect['x'] + rect['width'] \/ 2) * x_scale\n    \n    rect_y1 = (rect['y'] - rect['height'] \/ 2) * y_scale\n    rect_y2 = (rect['y'] + rect['height'] \/ 2) * y_scale\n    \n    anch_x1, anch_x2 = anchor_x - anchor_w \/ 2, anchor_x + anchor_w \/ 2\n    anch_y1, anch_y2 = anchor_y - anchor_h \/ 2, anchor_y + anchor_h \/ 2\n    \n    dx = (min(rect_x2, anch_x2) - max(rect_x1, anch_x1))\n    dy = (min(rect_y2, anch_y2) - max(rect_y1, anch_y1))\n    \n    intersection = dx * dy if (dx > 0 and dy > 0) else 0.\n    \n    anch_square = (anch_x2 - anch_x1) * (anch_y2 - anch_y1)\n    rect_square = (rect_x2 - rect_x1) * (rect_y2 - rect_y1)\n    union = anch_square + rect_square - intersection\n    \n    return intersection \/ union\n\ndef encode_anchors(annotation, img_shape, iou_thr=0.5):\n    encoded = np.zeros(shape=(FEATURE_SHAPE[0],\n                              FEATURE_SHAPE[1], 11), dtype=np.float32)\n    x_scale = 1. * IMG_WIDTH \/ img_shape[1]\n    y_scale = 1. * IMG_HEIGHT \/ img_shape[0]\n    for rect in annotation['annotations']:\n        scores = []\n        label = fish_classes == rect['class']\n                \n        for row in range(FEATURE_SHAPE[0]):\n            for col in range(FEATURE_SHAPE[1]):\n                anchor_x = ANCHOR_CENTERS[1, row, col]\n                anchor_y = ANCHOR_CENTERS[0, row, col]\n                score = iou(rect, x_scale, y_scale, anchor_x, anchor_y)\n                scores.append((score, anchor_x, anchor_y, row, col))\n        \n        scores = sorted(scores, reverse=True)\n        if scores[0][0] < iou_thr:\n            scores = [scores[0]]  # default anchor\n        else:\n            scores = [e for e in scores if e[0] > iou_thr]\n            \n        for score, anchor_x, anchor_y, row, col in scores:\n            dx = (anchor_x - rect['x'] * x_scale) \/ ANCHOR_WIDTH\n            dy = (anchor_y - rect['y'] * y_scale) \/ ANCHOR_HEIGHT\n            dw = (ANCHOR_WIDTH - rect['width'] * x_scale) \/ ANCHOR_WIDTH\n            dh = (ANCHOR_HEIGHT - rect['height'] * y_scale) \/ ANCHOR_HEIGHT\n            encoded[row, col] = np.array([*label, 1., dx, dy, dw, dh])\n\n    return encoded\n\ndef _sigmoid(x):\n    return 1. \/ (1. + np.exp(-x))\n\n#keras.activations.softmax(x, axis=-1)\n\ndef decode_prediction(prediction, conf_thr=0.1):\n    rectangles = []\n    for row in range(FEATURE_SHAPE[0]):\n        for col in range(FEATURE_SHAPE[1]):\n            \n            label = np.empty(6)\n            \n            label[0], label[1], label[2], label[3], label[4], label[5], conf, dx, dy, dw, dh = prediction[row, col]\n            conf = _sigmoid(conf)\n            label = softmax(label)\n\n            if conf > conf_thr:\n                anchor_x = ANCHOR_CENTERS[1, row, col]\n                anchor_y = ANCHOR_CENTERS[0, row, col]\n                rectangles.append({'x': anchor_x - dx * ANCHOR_WIDTH,\n                                   'y': anchor_y - dy * ANCHOR_HEIGHT,\n                                   'width': ANCHOR_WIDTH - dw * ANCHOR_WIDTH,\n                                   'height': ANCHOR_HEIGHT - dh * ANCHOR_HEIGHT,\n                                   'conf': conf,\n                                   'label': np.argmax(_sigmoid(label)),\n                                   'labels': label })\n    return rectangles\n\ndef non_max_suppression(rectangles, max_output_size, iou_threshold=0.5):\n    if not rectangles:\n        return rectangles\n    \n    boxes = [[r['y'],\n              r['x'],\n              r['y'] + r['height'],\n              r['x'] + r['width']] for r in rectangles]\n    scores = [r['conf'] for r in rectangles]\n    indices = tf.image.non_max_suppression(np.array(boxes),\n                                           np.array(scores),\n                                           max_output_size,\n                                           iou_threshold)\n    \n    return [rectangles[i] for i in indices]","c0b98b10":"annotation = boxes['shark'][3]\n\nencoded = encode_anchors(annotation,\n                         img_shape=(IMG_HEIGHT, IMG_WIDTH),\n                         iou_thr=0.1)\n\ndecoded = decode_prediction(encoded, conf_thr=0.7)\ndecoded = sorted(decoded, key = lambda e: -e['conf'])\n\nplt.figure(figsize=(6, 6), dpi=240)\nplt.imshow(draw_boxes(annotation, decoded))\nplt.title('{} {}x{}'.format(\n    annotation['filename'], img.shape[0], img.shape[1]));","728f1049":"K = tf.keras.backend\n\ndef confidence_loss(y_true, y_pred):\n    conf_loss = K.binary_crossentropy(y_true[..., 6], \n                                      y_pred[..., 6],\n                                      from_logits=True)\n    return conf_loss\n\ndef smooth_l1(y_true, y_pred):\n    abs_loss = K.abs(y_true[..., -4:] - y_pred[..., -4:])\n    square_loss = 0.5 * K.square(y_true[..., -4:] - y_pred[..., -4:])\n    mask = K.cast(K.greater(abs_loss, 1.), 'float32')\n    total_loss = (abs_loss - 0.5) * mask + 0.5 * square_loss * (1. - mask)\n    return K.sum(total_loss, axis=-1)\n\ndef classification_loss(y_tr, y_pr, alpha=0.25, gamma=2.0):\n    \n    \"\"\"Focal Loss\"\"\"\n    y_true = y_tr[..., :6]\n    y_pred = y_pr[..., :6]\n\n    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=y_true, logits=y_pred\n    )\n    probs = tf.nn.softmax(y_pred)\n    alpha = tf.where(tf.equal(y_true, 1.0), alpha, (1.0 - alpha))\n    pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n    loss = alpha * tf.pow(1.0 - pt, gamma) * cross_entropy\n    \n    return tf.reduce_sum(loss, axis=-1)\n\ndef class_loss(y_tr, y_pr):\n    \n    y_true = y_tr[..., :6]\n    y_pred = y_pr[..., :6]\n    \n    cross_entropy = K.categorical_crossentropy(y_true[..., :6], \n                                               y_pred[..., :6],\n                                               from_logits=True)\n#     tf.nn.sigmoid_cross_entropy_with_logits(\n#         labels=y_true, logits=y_pred\n#         )\n    return cross_entropy\n\ndef total_loss(y_true, y_pred, neg_pos_ratio=3):\n    batch_size = K.shape(y_true)[0]\n    \n    y_true = K.reshape(y_true, (batch_size, -1, 11))\n    y_pred = K.reshape(y_pred, (batch_size, -1, 11))\n    \n    # TODO: \u0434\u043e\u0431\u0430\u0432\u044c\u0442\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043f\u043e\u0442\u0435\u0440\u044c \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0434\u0435\u0442\u0435\u043a\u0446\u0438\u0438\n    cls_loss = classification_loss(y_true, y_pred)\n    #cls_loss = class_loss(y_true, y_pred)\n\n\n    # confidence loss\n    conf_loss = confidence_loss(y_true, y_pred)\n    \n    # smooth l1 loss\n    loc_loss = smooth_l1(y_true, y_pred)\n    \n    # positive examples loss\n    pos_conf_loss = K.sum(conf_loss * y_true[..., 6], axis=-1)\n    pos_class_loss = K.sum(cls_loss * y_true[..., 6], axis=-1)\n    pos_loc_loss = K.sum(loc_loss * y_true[..., 6], axis=-1)\n\n    \n    \n    # negative examples loss\n    anchors = K.shape(y_true)[1]\n    num_pos = K.sum(y_true[..., 6], axis=-1)\n    num_pos_avg = K.mean(num_pos)\n    num_neg = K.min([neg_pos_ratio * (num_pos_avg) + 1., K.cast(anchors, 'float32')])\n    \n    # hard negative mining\n    neg_conf_loss, _ = tf.nn.top_k(conf_loss * (1. - y_true[..., 6]),\n                                   k=K.cast(num_neg, 'int32'))\n\n    neg_conf_loss = K.sum(neg_conf_loss, axis=-1)\n    \n    # total conf loss\n    total_conf_loss = (neg_conf_loss + pos_conf_loss) \/ (num_neg + num_pos + 1e-32)\n    cls_loss = pos_class_loss \/ (num_pos + 1e-32)\n    loc_loss = pos_loc_loss \/ (num_pos + 1e-32)\n    \n    return total_conf_loss + 0.5 * loc_loss + cls_loss\n","73e4bfe3":"from random import shuffle\n\ndef load_img(path, target_size=(IMG_WIDTH, IMG_HEIGHT)):\n    img = cv2.imread(path, cv2.IMREAD_COLOR)[...,::-1]\n    img_shape = img.shape\n    img_resized = cv2.resize(img, target_size)\n    return img_shape, tf.keras.applications.resnet_v2.preprocess_input(img_resized.astype(np.float32))\n\ndef data_generator(boxes, batch_size=32):\n    boxes = sum(boxes.values(), [])\n    while True:\n        shuffle(boxes)\n        for i in range(len(boxes)\/\/batch_size):\n            X, y = [], []\n            for j in range(i*batch_size,(i+1)*batch_size):\n                img_shape, img = load_img(boxes[j]['filename'])\n                # TODO: \u0434\u043e\u0431\u0430\u0432\u044c\u0442\u0435 one-hot encoding \u0432 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443 \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u043e\u0432\n                #print('boxes[j]', boxes[j])\n                y_ = encode_anchors(boxes[j], img_shape)\n                y.append(y_)\n                X.append(img)\n            yield np.array(X), np.array(y)","6da888c2":"output = tf.keras.layers.BatchNormalization()(feature_tensor)\n\n# TODO: \u0434\u043e\u0431\u0430\u0432\u044c\u0442\u0435 \u0432\u044b\u0445\u043e\u0434\u044b \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0434\u0435\u0442\u0435\u043a\u0446\u0438\u0438\noutput = tf.keras.layers.Conv2D(11,\n                                kernel_size=(1, 1), \n                                activation='linear',\n                                kernel_regularizer='l2')(output)\n\nmodel = tf.keras.models.Model(inputs=features.inputs, outputs=output)\n","ac95cdfa":"lr=3e-3","9c960588":"def lr_exp_decay(epoch, lr):\n    k = 0.5\n    return lr * np.exp(-k*epoch)\n\nbatch_size = 32\nsteps_per_epoch = sum(map(len, boxes.values()), 0) \/ batch_size\n\ngen = data_generator(boxes, batch_size=batch_size)\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\n    'fishdetector.hdf5',\n    monitor='loss',\n    verbose=47,  \n    save_best_only=True, \n    save_weights_only=False,\n    mode='auto',\n    save_freq=10)\n\nadam = tf.keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999)\nmodel.compile(optimizer=adam, \n              loss=total_loss,\n              metrics=[confidence_loss, classification_loss, smooth_l1])\n","67186f62":"model.fit(gen, \n          steps_per_epoch=steps_per_epoch,\n          epochs=2,\n          callbacks=[checkpoint, tf.keras.callbacks.LearningRateScheduler(lr_exp_decay, verbose=1)])","57491a23":"#model.load_weights('..\/input\/fish-data\/fishdetector(2).hdf5')","53459b80":"annotation = boxes['shark'][1]\n\n_, sample_img = load_img(annotation['filename'])\npred = model.predict(np.array([sample_img,]))\n\ndecoded = decode_prediction(pred[0], conf_thr=0.1)\n\ndecoded = non_max_suppression(decoded,\n                              max_output_size=1,\n                              iou_threshold=0.75)\n\nplt.figure(figsize=(6, 6), dpi=240)\nimg = draw_boxes(annotation, decoded, (IMG_WIDTH, IMG_HEIGHT))\nplt.imshow(img)\nplt.title('\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 {}x{}'.format(*img.shape));","a2138793":"test_images = glob('..\/input\/fish-data\/test_stg1\/test_stg1\/*.jpg')[:4]\n\n\nplt.figure(figsize=(6, 4 * len(test_images)), dpi=240)\n\nfor i, filename in enumerate(test_images):\n    _, sample_img = load_img(filename)\n\n    pred = model.predict(np.array([sample_img,]))\n    decoded = decode_prediction(pred[0], conf_thr=0.1)\n    decoded = non_max_suppression(decoded,\n                                  max_output_size=1,\n                                  iou_threshold=0.5)\n    plt.subplot(len(test_images), 1, i + 1)\n    img = draw_boxes({'filename': filename}, decoded, (IMG_WIDTH, IMG_HEIGHT))\n    plt.imshow(img)\n    plt.title('\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043d\u0430 \u0442\u0435\u0441\u0442\u0435 {}'.format(filename.split('\/')[-1]));","54602497":"# TODO: \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0436\u0438\u0442\u0435 \u043a\u043b\u0430\u0441\u0441 \u0440\u044b\u0431\u044b \u0434\u043b\u044f \u0444\u043e\u0442\u043e\u0433\u0440\u0430\u0444\u0438\u0438 \u0438\u0437 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\n#\n# \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u044c\u0442\u0435 \u0444\u0430\u0439\u043b \u0441 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\u043c\u0438 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u0444\u043e\u0442\u043e\u0433\u0440\u0430\u0444\u0438\u0438:\n# image,ALB,BET,DOL,LAG,NoF,OTHER,SHARK,YFT\n# img_00001.jpg,1,0,0,0,0,...,0\n# img_00002.jpg,0.3,0.1,0.6,0,...,0","d384528c":"fish_classes","5fbeeeb4":"def make_predictions():\n    ptable = pd.DataFrame(columns=['image', 'ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK','YFT'])\n    \n    for i, file in enumerate(tqdm(glob('..\/input\/fish-data\/test_stg1\/test_stg1\/*.jpg'))):\n        bn = os.path.basename(file)\n        _, sample_img = load_img(file)\n        \n        pred = model.predict(np.array([sample_img,]))[0]\n        decoded = decode_prediction(pred, conf_thr=0.015)\n        decoded = non_max_suppression(decoded,\n                              max_output_size=1,\n                              iou_threshold=0.5)\n        decoded = decoded[0]['labels']\n\n        ptable.loc[i, 'image'] = bn\n        ptable.loc[i, 'ALB'] = decoded[5]\n        ptable.loc[i, 'BET'] = decoded[1]\n        ptable.loc[i, 'DOL'] = decoded[4]\n        ptable.loc[i, 'LAG'] = decoded[3]\n        ptable.loc[i, 'SHARK'] = decoded[0]\n        ptable.loc[i, 'YFT'] = decoded[2]\n        \n        ptable.loc[i, 'NoF'] = 0.123081\n        ptable.loc[i, 'OTHER'] = 0.079142\n\n    i += 1    \n    \n    for j, file in enumerate(tqdm(glob('..\/input\/fish-data\/test_stg2\/test_stg2\/*.jpg'))):\n        bn = os.path.basename(file)\n        \n        bn = \"test_stg2\/\" + bn\n        _, sample_img = load_img(file)\n        \n        pred = model.predict(np.array([sample_img,]))[0]\n\n        decoded = decode_prediction(pred, conf_thr=0.015)\n        decoded = non_max_suppression(decoded,\n                              max_output_size=1,\n                              iou_threshold=0.5)\n        \n        decoded = decoded[0]['labels']\n\n        ptable.loc[i + j, 'image'] = bn\n        ptable.loc[i + j, 'ALB'] = decoded[5]\n        ptable.loc[i + j, 'BET'] = decoded[1]\n        ptable.loc[i + j, 'DOL'] = decoded[4]\n        ptable.loc[i + j, 'LAG'] = decoded[3]\n        ptable.loc[i + j, 'SHARK'] = decoded[0]\n        ptable.loc[i + j, 'YFT'] = decoded[2]\n        \n        ptable.loc[i + j, 'NoF'] = 0.123081\n        ptable.loc[i + j, 'OTHER'] = 0.079142\n\n    return ptable   ","db8537a3":"pred_table = make_predictions()\npred_table.to_csv(\"neto_submit.csv\", index=False)\nprint(os.listdir(\".\/\"))","18e4b8ab":"pred_table","41498520":"## \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435","6a16b0b6":"# The Nature Conservancy Fisheries Monitoring","f4b9fe2d":"# \u0421\u0435\u0442\u043a\u0430 \u044f\u043a\u043e\u0440\u0435\u0439 (anchor grid)","ff9dd8ea":"## \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0434\u0435\u0442\u0435\u043a\u0442\u043e\u0440\u0430","d2b9ce8d":"# \u042d\u043a\u0441\u0442\u0440\u0430\u043a\u0442\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432","7113ce86":"### \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u0432 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0438","344f6457":"## \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0432\u044b\u0445\u043e\u0434 \u0434\u0435\u0442\u0435\u043a\u0442\u043e\u0440\u0430","d367c841":"# \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443","15fa5597":"## \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c","86a4405d":"## \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","e5725be9":"https:\/\/www.kaggle.com\/c\/the-nature-conservancy-fisheries-monitoring","f832bb9c":"## \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u043d\u0430 \u0442\u0435\u0441\u0442\u0435","b04fb2de":"### \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443","82e32950":"### \u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f \u044d\u043d\u043a\u043e\u0434\u0438\u043d\u0433\u0430\/\u0434\u0435\u043a\u043e\u0434\u0438\u043d\u0433\u0430 \u044f\u043a\u043e\u0440\u0435\u0439","7664d020":"## \u0410\u0433\u0440\u0435\u0433\u0430\u0446\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432"}}