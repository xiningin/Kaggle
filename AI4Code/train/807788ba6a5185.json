{"cell_type":{"8a7b77f2":"code","131834d8":"code","1c773501":"code","d7adc950":"code","3514bb01":"code","bfb52bce":"code","a9955832":"code","d4a458e3":"code","fc8ded09":"code","54f23631":"code","210b9bf3":"code","0bfcf700":"code","19dbf186":"code","321bdd3c":"code","1fe9cabe":"code","5c3d7d4b":"code","70eafd76":"code","12a987a1":"code","f0e01588":"code","a6787f10":"code","05ce815a":"code","eb86313e":"code","d01bfb91":"code","68056b86":"code","e591c86b":"code","eb6ac5de":"code","320ea8ba":"code","82f86c23":"code","37c5da9c":"code","3e85b113":"code","3dde50f0":"code","ed06a5bd":"code","3d3a324c":"code","e2a00d08":"code","f4c32162":"code","1a1f8cbf":"code","e28ce763":"code","9f011b44":"code","da4a7fae":"code","4044827f":"code","f762c3ec":"code","d8581be7":"code","dd77b0bf":"markdown","d3064a1b":"markdown","048ee422":"markdown","fcf3be12":"markdown","e7659621":"markdown","96834c15":"markdown","502f69b8":"markdown","a2ec44b9":"markdown","e7678318":"markdown","9b1f83b4":"markdown","40b00496":"markdown","2ccefcfa":"markdown","a9c63c1f":"markdown","097e6a32":"markdown","5b373333":"markdown","371e5f6f":"markdown","fc81ccdb":"markdown","47032623":"markdown","e6abbad6":"markdown","65626612":"markdown","cd0b4aaa":"markdown","7fc01fd6":"markdown","bfb185ec":"markdown","75d4f737":"markdown","3cf5a155":"markdown","8bfe6ed4":"markdown"},"source":{"8a7b77f2":"!\/opt\/conda\/bin\/python3.7 -m pip install --upgrade pip\n!pip install featexp --upgrade","131834d8":"import numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport itertools\n\nfrom featexp import get_univariate_plots\nfrom featexp import get_trend_stats\n\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, accuracy_score, log_loss\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import LabelEncoder, KBinsDiscretizer\nfrom sklearn.pipeline import make_pipeline, make_union, FeatureUnion, _fit_transform_one, _transform_one, _name_estimators\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nimport joblib\nfrom joblib import Parallel, delayed\n\nfrom scipy import sparse\n\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n\nfrom catboost import CatBoostClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n \nfrom tqdm.notebook import tqdm ,tnrange","1c773501":"# Aquisi\u00e7\u00e3o do dataset\n\ntrain_filepath = '\/kaggle\/input\/comp-tail\/treino.csv'\nsubm_filepath = '\/kaggle\/input\/comp-tail\/teste.csv'\ntrain_data = pd.read_csv(train_filepath)\nsubm_data = pd.read_csv(subm_filepath)\n\nsubm_data['Resposta'] = pd.read_csv('\/kaggle\/input\/comp-tail\/exemplo_submissao.csv')['Resposta']\n\nprint(\"Formato dos dados de treinamento: \", train_data.shape)\nprint(\"Formato dos dados de submiss\u00e3o: \", subm_data.shape)","d7adc950":"train_data['type'] = 'train'\nsubm_data['type'] = 'subm'\n\ndataset = pd.concat([train_data, subm_data])\ndataset['Resposta'].replace({\"Sim\": 1, \"N\u00e3o\": 0}, inplace=True)\n\ndataset.head()","3514bb01":"datatrain, datatest = train_test_split(dataset[dataset['type'] == 'train'], test_size = 0.3)\nget_univariate_plots(data=datatrain, target_col='Resposta', data_test=datatest)","bfb52bce":"get_trend_stats(data=datatrain, target_col='Resposta', data_test=datatest)","a9955832":"def plotCorr(df,col,K=6,ascending=False, method='pearson'):\n    fig, ax = plt.subplots()\n    k=K\n    if k <= 10 :\n        fig.set_size_inches(10,10)\n    else :\n        fig.set_size_inches(k, k)\n    corrmat = df.corr(method)\n    cols = corrmat.nsmallest(k,col)[col].index if ascending else corrmat.nlargest(k,col)[col].index\n    cm = np.corrcoef(df[cols].values.T)\n    if k <= 10 :\n        sns.heatmap(cm,cbar=True,annot=True,yticklabels=cols.values,xticklabels=cols.values,annot_kws={'size': 10})\n    else :\n        sns.heatmap(cm,cbar=True,annot=True,yticklabels=cols.values,xticklabels=cols.values,annot_kws={'size': 8})\n    plt.show()","d4a458e3":"plotCorr(dataset[dataset.type == 'train'], ['Resposta'], 7, False, 'spearman')","fc8ded09":"class PandasFeatureUnion(FeatureUnion):\n    def fit_transform(self, X, y=None, **fit_params):\n        self._validate_transformers()\n        result = Parallel(n_jobs=self.n_jobs)(\n            delayed(_fit_transform_one)(\n                transformer=trans,\n                X=X,\n                y=y,\n                weight=weight,\n                **fit_params)\n            for name, trans, weight in self._iter())\n\n        if not result:\n            # All transformers are None\n            return np.zeros((X.shape[0], 0))\n        Xs, transformers = zip(*result)\n        self._update_transformer_list(transformers)\n        if any(sparse.issparse(f) for f in Xs):\n            Xs = sparse.hstack(Xs).tocsr()\n        else:\n            Xs = self.merge_dataframes_by_column(Xs)\n        return Xs\n\n    def merge_dataframes_by_column(self, Xs):\n        return pd.concat(Xs, axis=\"columns\", copy=False)\n\n    def transform(self, X):\n        Xs = Parallel(n_jobs=self.n_jobs)(\n            delayed(_transform_one)(\n                transformer=trans,\n                X=X,\n                y=None,\n                weight=weight)\n            for name, trans, weight in self._iter())\n        if not Xs:\n            # All transformers are None\n            return np.zeros((X.shape[0], 0))\n        if any(sparse.issparse(f) for f in Xs):\n            Xs = sparse.hstack(Xs).tocsr()\n        else:\n            Xs = self.merge_dataframes_by_column(Xs)\n        return Xs\n    \ndef make_union(*transformers, **kwargs):\n    n_jobs = kwargs.pop('n_jobs', None)\n    verbose = kwargs.pop('verbose', False)\n    if kwargs:\n        # We do not currently support `transformer_weights` as we may want to\n        # change its type spec in make_union\n        raise TypeError('Unknown keyword arguments: \"{}\"'\n                        .format(list(kwargs.keys())[0]))\n    return PandasFeatureUnion(\n        _name_estimators(transformers), n_jobs=n_jobs, verbose=verbose)","54f23631":"# Selecionar as features desejadas no pipeline\n\nclass FeatureSelector(BaseEstimator, TransformerMixin):\n\n    def __init__(self, feature_names):\n        self.feature_names = feature_names\n        \n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        return X[self.feature_names]\n\n# Encoder espec\u00edfico para SIM e N\u00c3O    \n    \nclass ToBinary(BaseEstimator, TransformerMixin):\n\n    def __init__(self):\n        pass\n        \n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        return X.replace([\"Sim\", \"N\u00e3o\"], [1, 0])","210b9bf3":"# Cria uma nova feature concatenando duas outras colunas\n\nclass Concat(BaseEstimator, TransformerMixin):\n\n    def __init__(self, feature_names, name):\n        self.name = name\n        self.feature_names = feature_names\n        \n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X): \n        return (X[self.feature_names[0]].astype(str)+'x'+X[self.feature_names[1]].astype(str)).to_frame(self.name)\n\n# Encoder padr\u00e3o    \n    \nclass Encoder(BaseEstimator, TransformerMixin):\n\n    def __init__(self, feature_names):\n        self.feature_names = feature_names\n        \n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        \n        for feature in self.feature_names :\n            \n            if feature == 'Idade_Ve\u00edculo' :\n\n                X[feature] = X[feature].map({'< 1 Ano' : 1,'1-2 Anos' : 2,'> 2 Anos' : 3})\n\n            else :\n                \n                le = LabelEncoder()\n\n                le.fit(X[feature].tolist())\n\n                X[feature] = le.transform(X[feature])\n              \n        return X\n\n# Discretiza features e retorna apenas os Bins  \n    \nclass Binner(BaseEstimator, TransformerMixin):\n\n    def __init__(self, feature_names, bins):\n        self.feature_names = feature_names\n        self.bins = bins\n\n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        \n        for f, feature in enumerate(self.feature_names) :\n            \n            KBD = KBinsDiscretizer(n_bins=self.bins[f], encode='ordinal', strategy='quantile')       \n            \n            X[feature + '_Bin'] = KBD.fit_transform(X[feature].values.reshape(-1,1)).astype(int)\n\n        return X.drop(self.feature_names, axis=1)","0bfcf700":"# Cria uma nova coluna transformando features por agrupamentos\n\nclass GroupTransform(BaseEstimator, TransformerMixin):\n\n    def __init__(self, feature_names, group_names, names, methods):\n        self.feature_names = feature_names\n        self.group_names = group_names\n        self.names = names\n        self.methods = methods\n\n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        for f, feature in enumerate(self.feature_names) :\n            X[self.names[f]] = X.groupby(self.group_names[f])[feature].transform(self.methods[f])\n              \n        return X.drop(self.feature_names + self.group_names, axis=1)","19dbf186":"# Utilizando o Column Transformer\n\n# StandardScaler\n\n# class Scaler(BaseEstimator, TransformerMixin):\n    \n#     def __init__(self, feature_names):\n#         self.feature_names = feature_names\n\n#     def fit(self, X, y=None):\n#         return self\n        \n#     def transform(self, X):\n#         ct = ColumnTransformer([('scaling', StandardScaler(), self.feature_names)],\n#                                remainder='passthrough'\n#                               )\n        \n#         return pd.DataFrame(ct.fit_transform(X), columns=X.columns)\n\n# Dropa duplicados de acordo com todas as colunas recebidas    \n    \nclass DropDuplicates(BaseEstimator, TransformerMixin):\n\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):              \n        return X.drop_duplicates(keep='last')\n\n# Dropa NaNs     \n    \nclass DropNa(BaseEstimator, TransformerMixin):\n\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):              \n        return X.dropna()","321bdd3c":"original = dataset.copy()\n\nunion = make_union(\n     make_pipeline(\n         FeatureSelector(['Habilita\u00e7\u00e3o', 'Seguro_Veicular', 'Ve\u00edculo_Danificado', 'Resposta']),\n         ToBinary()\n     ),\n     make_pipeline(\n         FeatureSelector(['Sexo', 'Idade_Ve\u00edculo', 'Vintage', 'Pr\u00eamio_Anual', 'Canal_Vendas', 'C\u00f3digo_Regi\u00e3o']),\n         Encoder(['Sexo', 'Idade_Ve\u00edculo', 'Vintage', 'Pr\u00eamio_Anual', 'Canal_Vendas', 'C\u00f3digo_Regi\u00e3o'])\n     ),     \n     make_pipeline(\n         FeatureSelector(['Habilita\u00e7\u00e3o', 'Idade_Ve\u00edculo']),\n         Concat(['Habilita\u00e7\u00e3o', 'Idade_Ve\u00edculo'], 'Hab_Idade_Ve\u00edculo'),\n         Encoder(['Hab_Idade_Ve\u00edculo'])\n     ),\n     make_pipeline(\n         FeatureSelector(['Seguro_Veicular', 'Ve\u00edculo_Danificado']),\n         Concat(['Seguro_Veicular', 'Ve\u00edculo_Danificado'], 'Seguro_Ve\u00edculo_Danificado'),\n         Encoder(['Seguro_Ve\u00edculo_Danificado'])\n     ),\n     make_pipeline(\n         FeatureSelector(['C\u00f3digo_Regi\u00e3o', 'Canal_Vendas']),\n         Concat(['C\u00f3digo_Regi\u00e3o', 'Canal_Vendas'], 'Regi\u00e3o_Canal'),\n         Encoder(['Regi\u00e3o_Canal'])\n     ),\n     make_pipeline(\n         FeatureSelector(['Pr\u00eamio_Anual', 'Idade']),\n         Binner(['Pr\u00eamio_Anual', 'Idade'], [8, 10])\n     ),\n     make_pipeline(\n         FeatureSelector(['Canal_Vendas', 'Pr\u00eamio_Anual', 'C\u00f3digo_Regi\u00e3o']),\n         GroupTransform(\n             ['Canal_Vendas',            'Canal_Vendas',              'Canal_Vendas',               'Canal_Vendas',               'Pr\u00eamio_Anual',             'Pr\u00eamio_Anual',               'Pr\u00eamio_Anual',               'Pr\u00eamio_Anual',               'C\u00f3digo_Regi\u00e3o',          'C\u00f3digo_Regi\u00e3o',          'C\u00f3digo_Regi\u00e3o',          'C\u00f3digo_Regi\u00e3o'],\n             ['C\u00f3digo_Regi\u00e3o',           'C\u00f3digo_Regi\u00e3o',             'C\u00f3digo_Regi\u00e3o',              'C\u00f3digo_Regi\u00e3o',              'C\u00f3digo_Regi\u00e3o',            'C\u00f3digo_Regi\u00e3o',              'C\u00f3digo_Regi\u00e3o',              'C\u00f3digo_Regi\u00e3o',              'Pr\u00eamio_Anual',           'Pr\u00eamio_Anual',           'Pr\u00eamio_Anual',           'Pr\u00eamio_Anual'],\n             ['Qtd_Canais_nessa_Regi\u00e3o', 'M\u00e9dia_Canais_nessa_Regi\u00e3o', 'Desvio_Canais_nessa_Regi\u00e3o', 'Soma_Canais_nessa_Regi\u00e3o',   'Qtd_Pr\u00eamios_nessa_Regi\u00e3o', 'Desvio_Pr\u00eamio_nessa_Regi\u00e3o', 'M\u00e9dia_Pr\u00eamio_nessa_Regi\u00e3o',  'Soma_Pr\u00eamio_nessa_Regi\u00e3o',   'Qtd_Regi\u00f5es_com_Pr\u00eamio', 'Qtd_Regi\u00f5es_com_Pr\u00eamio', 'Qtd_Regi\u00f5es_com_Pr\u00eamio', 'Qtd_Regi\u00f5es_com_Pr\u00eamio'],\n             ['nunique',                 'mean',                      'std',                        'sum',                        'nunique',                  'std',                        'mean',                       'sum',                        'nunique',                'mean',                   'std',                    'sum'])\n         ), \n     make_pipeline(\n         FeatureSelector(['type', 'Idade'])\n     )\n )","1fe9cabe":"union_df = union.fit_transform(dataset)\n\ntrain_df = union_df[union_df['type'] == 'train'].drop(\"type\", axis=1)","5c3d7d4b":"train_df.columns","70eafd76":"clean_union = make_union(\n    make_pipeline(\n         FeatureSelector(train_df.columns),\n         DropDuplicates(),\n         DropNa()\n     ),\n )","12a987a1":"clean_df = clean_union.fit_transform(train_df)","f0e01588":"cleantrain, cleantest = train_test_split(clean_df, test_size = 0.3, random_state = 337)\nget_trend_stats(data = cleantrain, target_col = 'Resposta', data_test = cleantest)","a6787f10":"plotCorr(clean_df, ['Resposta'], 30, False, 'spearman')","05ce815a":"clean_df.columns","eb86313e":"features_final = ['Seguro_Veicular', 'Ve\u00edculo_Danificado', 'Sexo', \n                  'Idade_Ve\u00edculo', 'Canal_Vendas', 'C\u00f3digo_Regi\u00e3o',\n                  'Regi\u00e3o_Canal', 'Pr\u00eamio_Anual_Bin', 'Idade_Bin',\n                  'Qtd_Canais_nessa_Regi\u00e3o', 'Desvio_Canais_nessa_Regi\u00e3o',\n                  'M\u00e9dia_Pr\u00eamio_nessa_Regi\u00e3o', 'Seguro_Ve\u00edculo_Danificado']","d01bfb91":"X = clean_df.drop('Resposta', axis=1)\ny = clean_df.Resposta\n\nX_subm = union_df[union_df['type'] == 'subm'].drop([\"type\", 'Resposta'], axis=1)\ny_subm = union_df[union_df['type'] == 'subm'].Resposta\n\nprint(X.shape)\nprint(X_subm.shape)","68056b86":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=337)","e591c86b":"def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None, normalize=True):\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n    if normalize:\n        cn = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    thresh = cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i+.05, \"({:0.4f})\".format(cn[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n            plt.text(j, i-.05, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")      \n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","eb6ac5de":"def feature_importance(model, X):\n    fI = model.feature_importances_\n    \n    names = X.columns.values\n    ticks = [i for i in range(len(names))]\n\n    plt.bar(ticks, fI)\n    \n    plt.xticks(ticks, names,rotation = 90)\n    \n    plt.show()\n\n    print(fI)    ","320ea8ba":"import matplotlib.pylab as pyl\nimport matplotlib.pyplot as pyplot\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\n\ndef modelfit(alg, predictors, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n    \n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(predictors.values, label=target.values)\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n        alg.set_params(n_estimators=cvresult.shape[0])\n    \n    #Fit the algorithm on the data\n    alg.fit(predictors, target, eval_metric='auc')\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(predictors)\n    dtrain_predprob = alg.predict_proba(predictors)[:,1]\n        \n    #Print model report:\n    print(\"\\nModel Report\")\n    print(\"Accuracy : %.4g\" % metrics.accuracy_score(target.values, dtrain_predictions))\n    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(target, dtrain_predprob))\n\n    feat_imp = pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False)\n    feat_imp.plot(kind='bar', title='Feature Importances')\n    pyl.ylabel('Feature Importance Score')","82f86c23":"# %%time\n\n# xgb1 = XGBClassifier(\n#     learning_rate = 0.04, \n#     n_estimators= 1000,\n#     max_depth = 6, #gsearch2.best_params_['max_depth'],\n#     #min_child_weight = 1,#gsearch2.best_params_['min_child_weight'],\n#     #gamma = 0.2,#gsearch3.best_params_['gamma'],\n#     subsample = 0.9,\n#     colsample_bytree = 0.35,\n#     objective = 'binary:logistic',\n#     seed = 337,\n#     n_jobs = -1)\n\n# eval_set = [(X_train[features_final], y_train), (X_test[features_final], y_test)]\n\n# %time xgb1.fit(X_train[features_final], y_train, eval_metric=[\"auc\", \"error\", \"logloss\"], eval_set=eval_set, verbose=False)\n\n# y_pred = xgb1.predict_proba(X_test[features_final])[:,1]\n# predictions = xgb1.predict(X_test[features_final])\n\n# results = xgb1.evals_result()\n# epochs = len(results['validation_0']['error'])\n\n# x_axis = range(0, epochs)\n# # plot log loss\n# fig, ax = pyplot.subplots()\n# ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n# ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n# ax.legend()\n# pyplot.ylabel('Log Loss')\n# pyplot.title('XGBoost Log Loss')\n# pyplot.show()\n# # plot classification error\n# fig, ax = pyplot.subplots()\n# ax.plot(x_axis, results['validation_0']['error'], label='Train')\n# ax.plot(x_axis, results['validation_1']['error'], label='Test')\n# ax.legend()\n# pyplot.ylabel('Classification Error')\n# pyplot.title('XGBoost Classification Error')\n# # plot ROC AUC\n# fig, ax = pyplot.subplots()\n# ax.plot(x_axis, results['validation_0']['auc'], label='Train')\n# ax.plot(x_axis, results['validation_1']['auc'], label='Test')\n# ax.legend()\n# pyplot.ylabel('Classification ROC AUC')\n# pyplot.title('XGBoost ROC AUC')\n\n# pyplot.show()\n# print()\n# plot_confusion_matrix(confusion_matrix(y_test, predictions), ['0', '1'], \"Confusion Matrix - XGB\", \"Reds\")\n# print()\n# print(classification_report(y_test, predictions))\n# print()\n# print(\"Acc: \", accuracy_score(y_test, predictions))\n# print(\"LogLoss: \", log_loss(y_test, y_pred))\n# print(\"ROC AUC: \", roc_auc_score(y_test, y_pred))\n# feature_importance(xgb1, X_train[features_final])\n\n# modelfit(xgb1, X_train[features_final], y_train)\n# print(\"\\nModel Report\")\n# print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_test, predictions))\n# print(\"AUC Score (Test): %f\" % metrics.roc_auc_score(y_test, y_pred))","37c5da9c":"test_size = 0.34\nseeds = [337]#, 119, 13] #, 0, 27] # A pontua\u00e7\u00e3o vencedora foi com 1 seed e 75 modelos\n\n# A maior pontua\u00e7\u00e3o teve 3 seeds e 15 modelos (Sem a retirada de tantas features)","3e85b113":"# %%time\n# ##XGBM\n\n# probs_xgb = np.zeros(shape=(len(X_test),))\n# scores = []\n\n# avg_loss = []\n\n# X_train_XGB = pd.DataFrame(X_train, columns=features_final)\n# y_train_XGB = pd.DataFrame(y_train, columns=[\"Resposta\"])\n\n# for seed in tnrange(len(seeds)):  \n#     print(' ')    \n#     print('=-'*50)\n    \n#     print('Seed',seeds[seed])\n    \n#     SSS = StratifiedShuffleSplit(n_splits = 5, test_size = test_size , random_state = seed)\n    \n#     for i, (train_row, hold_row) in enumerate(SSS.split(X_train_XGB, y_train_XGB)):\n#         print('Fold', i)\n#         print('linhas de treino = ', len(train_row), 'linhas em espera = ', len(hold_row))\n\n#         clf = xgb.XGBClassifier(n_estimators=1000,\n#                                 max_depth=7,\n#                                 min_child_weight=1,\n#                                 learning_rate=0.03,\n#                                 subsample=0.9,\n#                                 gamma=0.2,\n#                                 colsample_bytree=0.35,\n#                                 objective = 'binary:logistic',\n#                                 random_state = 337\n#                                )\n\n#         xgb_b = clf.fit(X_train_XGB.iloc[train_row], \n#                     y_train_XGB.iloc[train_row], \n#                     eval_set=[(X_train_XGB.iloc[hold_row], \n#                                y_train_XGB.iloc[hold_row]\n#                               )],\n#                     verbose=100,\n#                     eval_metric=['auc', 'logloss'],\n#                     early_stopping_rounds=50\n#                    )\n        \n#         probs_oof = clf.predict_proba(X_train_XGB.iloc[hold_row])[:,1]\n        \n#         probs_xgb += clf.predict_proba(X_test[features_final])[:,1]\n        \n#         roc = roc_auc_score(y_train_XGB.iloc[hold_row], probs_oof)\n\n#         scores.append(roc)\n        \n#         avg_loss.append(clf.best_score)\n\n#         print ('XGB Val of AUC = ', roc)\n\n#         print('=-'*50)\n#         print( )\n#         if i==0:\n#             feature_importance(clf, X_train_XGB)\n            \n# print(\"Log Loss {0:.5f},{1:.5f} (m\u00e9dia e std)\".format(np.array(avg_loss).mean(), np.array(avg_loss).std()))\n\n# print(\"Valor de AUC %.6f (%.6f) (m\u00e9dia e std)\" % (np.array(scores).mean(), np.array(scores).std()))","3dde50f0":"# %%time\n\n# ##CatBoost\n\n# cat_features = ['Sexo', 'Seguro_Veicular', 'Ve\u00edculo_Danificado',\n#                 'Canal_Vendas', 'Idade_Ve\u00edculo', 'Regi\u00e3o_Canal',\n#                 'Pr\u00eamio_Anual_Bin', 'C\u00f3digo_Regi\u00e3o', 'Seguro_Ve\u00edculo_Danificado']\n\n# cont_features = ['Idade_Bin', 'Qtd_Canais_nessa_Regi\u00e3o']\n\n# probs_cb = np.zeros(shape=(len(X_test),))\n\n# scores = []\n# avg_loss = []\n\n# X_train_CAT = pd.DataFrame(X_train, columns=features_final)\n# y_train_CAT = pd.DataFrame(y_train, columns=[\"Resposta\"])\n\n# for seed in tnrange(len(seeds)):  \n#     print(' ')    \n#     print('=-'*50)\n    \n#     print('Seed',seeds[seed])\n    \n#     SSS = StratifiedShuffleSplit(n_splits = 5, test_size = test_size , random_state = seed)\n    \n#     for i, (train_row, hold_row) in enumerate(SSS.split(X_train_CAT, y_train_CAT)):\n#         print('Fold', i)\n#         print('linhas de treino = ', len(train_row), 'linhas em espera = ', len(hold_row))\n\n#         clf = CatBoostClassifier(iterations=10000,\n#                                 learning_rate=0.02,\n#                                 random_strength=0.1,\n#                                 depth=8,\n#                                 loss_function='Logloss',\n#                                 eval_metric='AUC',\n#                                 leaf_estimation_method='Newton',\n#                                 random_state = 1,\n#                                 cat_features =cat_features,\n#                                 subsample = 0.9,\n#                                 rsm = 0.8\n#                                 )    \n\n#         cat_b = clf.fit(X_train_CAT.iloc[train_row], \n#                     y_train_CAT.iloc[train_row],\n#                     eval_set=[(X_train_CAT.iloc[hold_row],\n#                                y_train_CAT.iloc[hold_row]\n#                               )],\n#                     early_stopping_rounds=50,\n#                     verbose = 100)\n\n#         probs_oof = clf.predict_proba(X_train_CAT.iloc[hold_row])[:,1]\n        \n#         probs_cb += clf.predict_proba(X_test)[:,1]\n        \n#         roc = roc_auc_score(y_train_CAT.iloc[hold_row], probs_oof)\n        \n#         scores.append(roc)\n\n#         print ('CatBoost Val of AUC = ', roc)\n\n#         avg_loss.append(clf.best_score_['validation']['Logloss'])\n\n#         if i==0:\n            \n#             feature_importance(clf,X_train_CAT)\n\n#         print('=-'*50)\n\n# print(\"Log Loss {0:.8f},{1:.8f} (m\u00e9dia e std)\".format(np.array(avg_loss).mean(), np.array(avg_loss).std()))\n\n# print(\"Valor de AUC %.8f (%.8f) (m\u00e9dia e std)\" % (np.array(scores).mean(), np.array(scores).std()))","ed06a5bd":"# print(\"ROC AUC XGB == {},\".format(roc_auc_score(y_test, np.around(xgb_b.predict(X_test), 0))))\n# print(\"ROC AUC CAT == {},\".format(roc_auc_score(y_test, np.around(cat_b.predict(X_test), 0))))\n\n# print(\"ROC AUC XGB == {},\".format(roc_auc_score(y_test, probs_xgb\/5)))\n# print(\"ROC AUC CAT == {},\".format(roc_auc_score(y_test, probs_cb\/5)))\n\n# auc_max = 0\n\n# for i in range(1, 10) :\n#     a = i * .1\n#     auc = roc_auc_score(y_test, ((a * probs_xgb) + ((1-a) * probs_cb))\/5)\n    \n#     if auc > auc_max :\n#         auc_max = auc\n#         a_max = a\n    \n# print(\"ROC AUC Combina\u00e7\u00e3o == {},\".format(auc_max), round(a_max, 1), '-', round(1- a_max, 1))","3d3a324c":"N_SPLITS = 5","e2a00d08":"%%time\n##XGBM Final\n\nprobs_xgb = np.zeros(shape=(len(X_subm),))\nscores = []\n\navg_loss = []\n\nX_train_XGB = pd.DataFrame(X, columns=features_final)\ny_train_XGB = pd.DataFrame(y, columns=[\"Resposta\"])\n\nfor seed in tnrange(len(seeds)):  \n    print(' ')    \n    print('=-'*50)\n    \n    print('Seed',seeds[seed])\n    \n    SSS = StratifiedShuffleSplit(n_splits = N_SPLITS, test_size = test_size , random_state = seed)\n    \n    for i, (train_row, hold_row) in enumerate(SSS.split(X_train_XGB, y_train_XGB)):\n        print('Fold', i)\n        print('linhas de treino = ', len(train_row), 'linhas em espera = ', len(hold_row))\n\n        clf = xgb.XGBClassifier(n_estimators=1000,\n                                max_depth=7,\n                                min_child_weight=1,\n                                learning_rate=0.03,\n                                subsample=0.9,\n                                gamma=0.2,\n                                colsample_bytree=0.35,\n                                objective = 'binary:logistic',\n                                random_state = 337\n                               )\n\n        xgb_f = clf.fit(X_train_XGB.iloc[train_row], \n                    y_train_XGB.iloc[train_row], \n                    eval_set=[(X_train_XGB.iloc[hold_row], \n                               y_train_XGB.iloc[hold_row]\n                              )],\n                    verbose=100,\n                    eval_metric=['auc', 'logloss'],\n                    early_stopping_rounds=50\n                   )\n        \n        probs_oof = clf.predict_proba(X_train_XGB.iloc[hold_row])[:,1]\n        \n        probs_xgb += clf.predict_proba(X_subm[features_final])[:,1]\n        \n        roc = roc_auc_score(y_train_XGB.iloc[hold_row], probs_oof)\n\n        scores.append(roc)\n        \n        avg_loss.append(clf.best_score)\n\n        print ('XGB Val of AUC = ', roc)\n\n        print('=-'*50)\n        print( )\n        if i==0:\n            feature_importance(clf, X_train_XGB)\n            \nprint(\"Log Loss {0:.5f},{1:.5f} (m\u00e9dia e std)\".format(np.array(avg_loss).mean(), np.array(avg_loss).std()))\n\nprint(\"Valor de AUC %.6f (%.6f) (m\u00e9dia e std)\" % (np.array(scores).mean(), np.array(scores).std()))","f4c32162":"%%time\n\n##CatBoost Final\n\ncat_features = ['Sexo', 'Seguro_Veicular', 'Ve\u00edculo_Danificado',\n                'Canal_Vendas', 'Idade_Ve\u00edculo', 'Regi\u00e3o_Canal',\n                'Pr\u00eamio_Anual_Bin', 'C\u00f3digo_Regi\u00e3o', 'Seguro_Ve\u00edculo_Danificado']\n\ncont_features = ['Idade_Bin', 'Qtd_Canais_nessa_Regi\u00e3o', 'Desvio_Canais_nessa_Regi\u00e3o', 'M\u00e9dia_Pr\u00eamio_nessa_Regi\u00e3o']\n\nprobs_cb = np.zeros(shape=(len(X_subm),))\n\nscores = []\navg_loss = []\n\nX_train_CAT = pd.DataFrame(X, columns=features_final)\ny_train_CAT = pd.DataFrame(y, columns=[\"Resposta\"])\n\nfor seed in tnrange(len(seeds)):  \n    print(' ')    \n    print('=-'*50)\n    \n    print('Seed',seeds[seed])\n    \n    SSS = StratifiedShuffleSplit(n_splits = N_SPLITS, test_size = test_size , random_state = seed)\n    \n    for i, (train_row, hold_row) in enumerate(SSS.split(X_train_CAT, y_train_CAT)):\n        print('Fold', i)\n        print('linhas de treino = ', len(train_row), 'linhas em espera = ', len(hold_row))\n\n        clf = CatBoostClassifier(iterations=10000,\n                                learning_rate=0.03,\n                                random_strength=0.1,\n                                depth=8,\n                                loss_function='Logloss',\n                                eval_metric='AUC',\n                                leaf_estimation_method='Newton',\n                                random_state = 1,\n                                cat_features =cat_features,\n                                subsample = 0.9,\n                                rsm = 0.8\n                                )    \n\n        cat_f = clf.fit(X_train_CAT.iloc[train_row], \n                    y_train_CAT.iloc[train_row],\n                    eval_set=[(X_train_CAT.iloc[hold_row],\n                               y_train_CAT.iloc[hold_row]\n                              )],\n                    early_stopping_rounds=50,\n                    verbose = 100)\n\n        probs_oof = clf.predict_proba(X_train_CAT.iloc[hold_row])[:,1]\n        \n        probs_cb += clf.predict_proba(X_subm[features_final])[:,1]\n        \n        roc = roc_auc_score(y_train_CAT.iloc[hold_row], probs_oof)\n        \n        scores.append(roc)\n\n        print ('CatBoost Val of AUC = ', roc)\n\n        avg_loss.append(clf.best_score_['validation']['Logloss'])\n\n        if i==0:\n            \n            feature_importance(clf,X_train_CAT)\n\n        print('=-'*50)\n\nprint(\"Log Loss {0:.8f},{1:.8f} (m\u00e9dia e std)\".format(np.array(avg_loss).mean(), np.array(avg_loss).std()))\n\nprint(\"Valor de AUC %.8f (%.8f) (m\u00e9dia e std)\" % (np.array(scores).mean(), np.array(scores).std()))","1a1f8cbf":"print(\"ROC AUC XGB == {},\".format(roc_auc_score(y_test, xgb_f.predict_proba(X_test[features_final])[:,1])))\nprint(\"Accuracy XGB == {},\".format(accuracy_score(y_test, xgb_f.predict(X_test[features_final]))))","e28ce763":"print(\"ROC AUC CAT == {},\".format(roc_auc_score(y_test, cat_f.predict_proba(X_test[features_final])[:,1])))\nprint(\"Accuracy CAT == {},\".format(accuracy_score(y_test, cat_f.predict(X_test[features_final]))))","9f011b44":"print(max(probs_xgb \/ (N_SPLITS * len(seeds))), max(probs_cb \/ (N_SPLITS * len(seeds))))","da4a7fae":"# Melhor resultado 30 - 70\npreds = ((.3 * probs_xgb) + (.7 * probs_cb)) \/ (N_SPLITS * len(seeds))\nmax(preds)","4044827f":"print(max(preds), min(preds))","f762c3ec":"# Submiss\u00e3o da Solu\u00e7\u00e3o Final\n\n#preds = xgb_f.predict(X_subm[features_final])\n\nsubmission = pd.DataFrame(data = {\"id\": subm_data['Id'].values, \"Resposta\": preds})\nsubmission.to_csv('submissionv15.csv', index = False)\nsubmission.head()","d8581be7":"submission.shape","dd77b0bf":"## Criando Union de Pipelines para Pandas","d3064a1b":"# Modelo para submiss\u00e3o","048ee422":"## Separando nosso X e y de ambos os sets","fcf3be12":"## Classificador XGBoost","e7659621":"# Importando Dataset","96834c15":"### Resultados do CATBoost","502f69b8":"## Classificador CatBoost","a2ec44b9":"* Vintage 0.111\n* Quantidade de Regi\u00f5es que recebem o pr\u00eamio 0.725 (Qtd_Regi\u00f5es_com_Pr\u00eamio)","e7678318":"### Checando se os resultados est\u00e3o no intervalo correto","9b1f83b4":"### Aplicar o get_trend_stats novamente\n* Retirar colunas com Trend Correlation baixa (< 0.75)","40b00496":"# An\u00e1lise Explorat\u00f3ria","2ccefcfa":"# Modelo Ensembling de treino","a9c63c1f":"### GroupTransform\n![GroupTransform](https:\/\/pbpython.com\/images\/groupby-example.png)","097e6a32":"### Ensembling final\n* Cada modelo foi lan\u00e7ado 3 vezes com pesos diferentes","5b373333":"### Dados do Conjunto de Treino\n* **Id** - identifica\u00e7\u00e3o do cliente\n* **Sexo** - sexo do cliente\n* **Idade** - idade do cliente\n* **Habilita\u00e7\u00e3o** - N\u00e3o: O cliente n\u00e3o possui habilita\u00e7\u00e3o; Sim: O cliente possui habilita\u00e7\u00e3o\n* **C\u00f3digo_Regi\u00e3o** - c\u00f3digo \u00fanico para a regi\u00e3o do cliente\n* **Seguro_Veicular** - N\u00e3o: Cliente n\u00e3o possui seguro veicular; Sim: Cliente j\u00e1 possui seguro veicular\n* **Idade_Ve\u00edculo** - idade do Ve\u00edculo\n* **Ve\u00edculo_Danificado** - N\u00e3o: Cliente n\u00e3o danificou seu ve\u00edculo no passado; Sim: Cliente j\u00e1 danificou seu ve\u00edculo no passado\n* **Pr\u00eamio_Anual** - o pre\u00e7o anual que o cliente precisa pagar pelo seguro\n* **Canal_Vendas** - c\u00f3digo anonimizado para o canal de divulga\u00e7\u00e3o para o cliente (i.e., por correio, por telefone, pessoalmente, agentes diferentes)\n* **Vintage** - n\u00famero de dias que o cliente est\u00e1 associado \u00e0 companhia\n* **Resposta** - N\u00e3o: Cliente n\u00e3o est\u00e1 interessado; Sim: Cliente est\u00e1 interessado\n","371e5f6f":"### Finalmente com tudo escolhido\n\n* Determinar o numero de shuffles para cada seed\n* Checar os resultados\n* Ensembling final e submeter","fc81ccdb":"## Divindo sets de Treino e Teste e Resampling","47032623":"## Avaliando o melhor set","e6abbad6":"# Importando Pacotes","65626612":"### O que foi feito:\n* Cria\u00e7\u00e3o de uma pipeline com ColumnTransformer e FeatureUnion\n* Dividir o dataset em colunas e aplicar transforma\u00e7\u00f5es simutaneamente\n* Torna o c\u00f3digo mais r\u00e1pido de executar\n* Overkill, transforma\u00e7\u00f5es j\u00e1 n\u00e3o demoravam, n\u00e3o h\u00e1 mais dados a serem adicionados\n\n\n![Pipeline](https:\/\/miro.medium.com\/max\/680\/1*jkjsxby0QGgERSnRKJlA0w.png)\n\nhttps:\/\/towardsdatascience.com\/pipeline-columntransformer-and-featureunion-explained-f5491f815f","cd0b4aaa":"### Resultados do XGB","7fc01fd6":"### Escolha das features final\n* Retirar colunas muito semelhantes entre si para evitar um overfitting\n* Provalvemente consiguiria um resultado melhor com elas, por\u00e9m demoraria mais","bfb185ec":"# Engenharia de Dados","75d4f737":"## Aplicando Pipelines","3cf5a155":"## Ensembling com pesos","8bfe6ed4":"## Criando Classes para Pipelines"}}