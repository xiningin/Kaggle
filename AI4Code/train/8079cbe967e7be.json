{"cell_type":{"3aa05842":"code","4f9fae63":"code","2522f419":"code","6f000c11":"code","8c9f30f1":"code","9e858cb1":"code","7710ee51":"code","9476ea5c":"code","e9af87e4":"code","c52d8938":"code","d7b8e170":"code","8bc0b281":"code","82da6776":"code","f06c93b9":"code","338b932b":"code","dd6cdef8":"code","2683011b":"code","8396f98a":"code","d3db265a":"code","b51cb1ba":"code","c2928ed7":"code","82d1f797":"code","dd410ef4":"code","33f07035":"code","0500493d":"code","cbecbd43":"code","ce4fb633":"code","4849a62a":"code","63179654":"markdown","2c0a0ecb":"markdown","cac83b45":"markdown","2783726d":"markdown","0ff08210":"markdown","787f9a76":"markdown"},"source":{"3aa05842":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4f9fae63":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom torch import nn\nimport torch\nfrom statsmodels.tsa.stattools import grangercausalitytests\nimport plotly.express as px\nimport numpy as np\nimport holoviews as hv\nfrom holoviews import opts\nfrom sklearn.metrics import mean_squared_error\nfrom torch.utils.data import Dataset\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader \nimport plotly.express as px\nimport plotly.graph_objects as go\n\nhv.extension('bokeh')","2522f419":"data = pd.read_csv('\/kaggle\/input\/acea-water-prediction\/Aquifer_Doganella.csv')\ndata1 = pd.read_csv('\/kaggle\/input\/acea-water-prediction\/Aquifer_Auser.csv')\ndata2 = pd.read_csv('\/kaggle\/input\/acea-water-prediction\/Aquifer_Luco.csv')\ndata3 = pd.read_csv('\/kaggle\/input\/acea-water-prediction\/Aquifer_Petrignano.csv')","6f000c11":"all_col = [data,data1,data2,data3]\nfor idx, cols in enumerate(all_col):\n    new_cols = []\n    for col in cols.columns:\n        if 'Rainfall' in col:\n            new_name = col.replace('Rainfall', 'R')\n        elif 'Depth_to_Groundwater' in col:\n            new_name = col.replace('Depth_to_Groundwater', 'DG')\n        elif 'Temperature' in col:\n            new_name = col.replace('Temperature', 'T')\n        elif 'Volume' in col:\n            new_name = col.replace('Volume', 'V')\n        elif 'Hydrometry' in col:\n            new_name = col.replace('Hydrometry', 'H')\n        else:\n            new_name = col\n        new_cols.append(new_name)\n    cols.columns = new_cols","8c9f30f1":"# Convert Date object type column to datetime type\ndata['Date'] = pd.to_datetime(data.Date)\ndata1['Date'] = pd.to_datetime(data1.Date)\ndata2['Date'] = pd.to_datetime(data2.Date)\ndata3['Date'] = pd.to_datetime(data3.Date)","9e858cb1":"plt.figure(figsize=(18, 8))\nplt.title('Doganella correlation heatmap',fontsize=50)\nsns.heatmap(data.corr(), annot=True, fmt='.2f', linewidths=0.2, cmap='PuBuGn')\nplt.show()\n\nplt.figure(figsize=(18, 8))\nplt.title('Auser correlation heatmap',fontsize=50)\nsns.heatmap(data1.corr(), annot=True, fmt='.2f', linewidths=0.2, cmap='PuBuGn')\nplt.show()\n\n","7710ee51":"plt.figure(figsize=(18, 8))\nplt.title('Luco correlation heatmap',fontsize=50)\nsns.heatmap(data2.corr(), annot=True, fmt='.2f', linewidths=0.2, cmap='PuBuGn')\nplt.show()\n\nplt.figure(figsize=(18, 8))\nplt.title('Petrignano correlation heatmap',fontsize=50)\nsns.heatmap(data3.corr(), annot=True, fmt='.2f', linewidths=0.2, cmap='PuBuGn')\nplt.show()","9476ea5c":"Lake_Bilancino = pd.read_csv(\"..\/input\/acea-water-prediction\/Lake_Bilancino.csv\")","e9af87e4":"Lake_Bilancino.describe()","c52d8938":"def box_plot(df, id_begin = 1, id_last = 5):\n    fig = go.Figure()\n    for column in df.columns[id_begin:id_last]:\n\n        fig.add_trace(go.Box(y=df[column], name = column))\n        fig.update_traces(boxpoints='outliers', \n                          jitter= 0)\n\n    return fig.show()\n\n\nbox_plot(Lake_Bilancino, id_last = 5)","d7b8e170":"fig = px.line(Lake_Bilancino, x=Lake_Bilancino['Date'], y=Lake_Bilancino['Temperature_Le_Croci'], \n              title='Temperature_Le_Croci')\nfig.show()","8bc0b281":"box_plot(Lake_Bilancino, id_begin = 6, id_last = 7)","82da6776":"fig = px.line(Lake_Bilancino, x=Lake_Bilancino['Date'], y=Lake_Bilancino['Lake_Level'], \n              title='Lake_Level')\nfig.show()","f06c93b9":"box_plot(Lake_Bilancino, id_begin = 7, id_last = 8)","338b932b":"fig = px.line(Lake_Bilancino, x=Lake_Bilancino['Date'], y=Lake_Bilancino['Flow_Rate'], \n              title='Flow_Rate')\nfig.show()","dd6cdef8":"box_plot(Lake_Bilancino, id_begin = 8, id_last = 9)","2683011b":"def granger_causality_tests(df, maxlag = 12):\n\n    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n    The rows are the response variable, columns are predictors. The values in the table \n    are the P-Values. P-Values lesser than the significance level (0.05), implies \n    the Null Hypothesis that the coefficients of the corresponding past values is \n    zero, that is, the X does not cause Y can be rejected.\n\n    data      : pandas dataframe containing the time series variables\n    variables : list containing names of the time series variables.\n    \"\"\"\n    \n    variables = df.columns\n    granger_pvalue_matrix = pd.DataFrame(np.zeros((len(variables), len(variables))))\n\n    for i, variable_c in enumerate(variables):\n        for j, variable_r in enumerate(variables):\n            min_p_value = np.min([grangercausalitytests(df[[variable_c, variable_r]].dropna(), \n                     maxlag = maxlag, verbose = False)[i][0]['ssr_chi2test'][1] for i in range(1, maxlag+1)])\n            granger_pvalue_matrix.loc[i, j] = min_p_value\n\n    granger_pvalue_matrix.columns = [var + '_x' for var in variables]\n    granger_pvalue_matrix.index = [var + '_y' for var in variables]\n    return granger_pvalue_matrix\n\ndef draw_granger_pvalue_matrix(granger_pvalue_matrix):\n    fig = go.Figure(data=go.Heatmap(z = granger_pvalue_matrix, \n                                    x = granger_pvalue_matrix.columns, \n                                    y = granger_pvalue_matrix.index, \n                                    colorscale = [[0, 'green'], [0.05, 'green'],\n                                                  [0.05, 'red'], [1, 'red']\n                                                  ]))\n    fig.update_layout(title=\"Minimum of p_values for a maxlag of 12 of a Granger test\")\n    return fig.show()","8396f98a":"granger_pvalue_matrix = granger_causality_tests(Lake_Bilancino.iloc[578:, 1:].dropna())\ndraw_granger_pvalue_matrix(granger_pvalue_matrix)","d3db265a":"def draw_correlation_matrix(df, method = 'pearson'):\n    pct_change_list = []\n    for column in df.columns[1:]:\n        pct_change_list.append(df[column].pct_change().replace([np.inf, -np.inf], np.nan).dropna())\n    pct_change_array = np.zeros((len(df.columns[1:]), len(df.columns[1:])))\n    for i, pct_change_i in enumerate(pct_change_list):\n        for j, pct_change_j in enumerate(pct_change_list):\n            pct_change_array[i, j] = pct_change_i.corr(pct_change_j, method)\n    fig = px.imshow(pct_change_array,\n                    x=df.columns[1:],\n                    y=df.columns[1:]\n                   )\n    fig.update_xaxes(title = \"Correlation matrix\", side=\"top\")\n    return fig.show()\ndraw_correlation_matrix(Lake_Bilancino.iloc[:, 1:], method = \"pearson\")","b51cb1ba":"def scale_and_split(df):\n    scaler = StandardScaler()\n    transformer = scaler.fit_transform(df.iloc[:, 1:])\n    df_scaled = pd.DataFrame(transformer, columns = df.columns[1:])\n    train, val = train_test_split(df_scaled, test_size=0.33, shuffle = False)\n    val, test = train_test_split(val, test_size=0.5, shuffle = False)\n    return train, val, test, scaler\n\ndef prepare_inputs_outputs(train, val, test, seq_len, inputs, output):\n    X_train, y_train = train[:len(train)-seq_len][inputs], train[[output]]\n    X_val, y_val = val[:len(val)-seq_len][inputs], val[[output]]\n    X_test, y_test = test[:len(test)-seq_len][inputs], test[[output]]\n    return X_train, y_train, X_val, y_val, X_test, y_test","c2928ed7":"parameters = {'inputs':['Lake_Level', 'Flow_Rate'],\n              'outputs':['Lake_Level', 'Flow_Rate'],\n              'seq_len': 30,\n              'batch_size_train':32,\n              'batch_size_val':8,\n              'batch_size_test':8,\n              'lr':0.0005,\n              'epochs':20\n             }\n\ntrain, val, test, scaler = scale_and_split(Lake_Bilancino.iloc[578:]) \nX_train, y_train, X_val, y_val, X_test, y_test = prepare_inputs_outputs(train, val, test, \n                                                                        parameters['seq_len'], \n                                                                        parameters['inputs'], \n                                                                        parameters['outputs'][0])\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=X_train.index, y=X_train['Lake_Level'],\n                    mode='lines',\n                    name='train data'))\nfig.add_trace(go.Scatter(x=X_val.index, y=X_val['Lake_Level'],\n                    mode='lines',\n                    name='validation data'))\nfig.add_trace(go.Scatter(x=X_test.index, y=X_test['Lake_Level'],\n                    mode='lines', \n                    name='test data'))\nfig.update_layout(title = 'Lake level cutted into train, validation and test dataset (normalized)')\n\nfig.show() ","82d1f797":"from torch.utils.data import Dataset\nclass TimeSeries(Dataset):\n    def __init__(self, X, y, seq_len = 30):\n        self.X = torch.tensor(np.array(X) ,dtype=torch.float32)\n        self.y = torch.tensor(np.array(y) ,dtype=torch.float32)\n        #self.column_input = column_input\n        #self.column_output = column_output\n        self.seq_len = seq_len\n        \n    def __getitem__(self,idx):\n        return self.X[idx:idx+self.seq_len], self.y[idx+self.seq_len]\n\n    def __len__(self):\n        return len(self.X) - (self.seq_len-1)\n    \n    \ndef get_dataloader(X, y, batch_size):\n    return DataLoader(TimeSeries(X, y), shuffle=False, batch_size=32, drop_last = True)\n\n\nif torch.cuda.is_available():  \n    device = \"cuda:0\" \nelse:  \n    device = \"cpu\"","dd410ef4":"class MTS_RNN(nn.Module):\n    def __init__(self, input_size=8, hidden_layer_size=20, output_size=2):\n        super().__init__()\n        self.hidden_layer_size = hidden_layer_size\n        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n        self.linear = nn.Linear(hidden_layer_size, output_size)\n\n    def forward(self, x):\n        output, _ = self.lstm(x)\n        predictions = self.linear(output[:, -1])\n        return predictions","33f07035":"def train_model(model, train_loader, val_loader, loss_function, optimizer, epochs):\n\n\n    train_val_history = {'train': [],\n                        'val': []}\n    for i in range(epochs):\n        for j, data in enumerate(train_loader):\n            inputs, labels = data[0].to(device), data[1].to(device)\n            optimizer.zero_grad()\n\n            y_pred = model(inputs)\n\n            single_loss = torch.sqrt(loss_function(y_pred, labels))\n\n            single_loss.backward()\n\n            optimizer.step()\n\n        train_val_history['train'].append(single_loss.item())\n        #print(\"Training data : \" + f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n\n        for j, data in enumerate(val_loader):\n            inputs, labels = data[0].to(device), data[1].to(device)\n            optimizer.zero_grad()\n\n            y_pred = model(inputs)\n\n            single_loss = torch.sqrt(loss_function(y_pred, labels))\n\n        train_val_history['val'].append(single_loss.item())\n        #print(\"Validation data : \" + f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n\n    #print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n    return model, train_val_history\n\n\n\n\ndef draw_training_loss(train_val_history, epochs):\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=np.arange(epochs), y=np.array(train_val_history['train']),\n                        mode='lines',\n                        name = 'train'))\n    fig.add_trace(go.Scatter(x=np.arange(epochs), y=np.array(train_val_history['val']),\n                        mode='lines',\n                        name = 'validation'))\n    fig.update_layout(title=\"Training and validation loss\",\n                      xaxis_title=\"Epochs\",\n                      yaxis_title=\"MSE loss\",)\n\n    return fig.show()\n\n\ndef predict_and_draw(df, test_loader, output, scaler, model, y_test, loss_function, draw = False):\n\n    test_output = []\n    test_output_grad = []\n    for j, data in enumerate(test_loader):\n        inputs, labels = data[0].to(device), data[1].to(device)\n        inputs.requires_grad = True\n        y_pred = model(inputs)\n        single_loss = loss_function(y_pred, labels)\n\n        test_output.append(y_pred)\n        \n        single_loss.backward()\n \n        test_output_grad.append(torch.mean(inputs.grad, axis = 0))\n        \n    gradient_input_mean = np.sort(np.absolute(torch.mean(torch.cat(test_output_grad), axis = 0)\n                                      .detach().numpy()))\n    \n    indice_outputs = [i - 1 for i, x in enumerate(df.columns.isin(output)) if x]\n    mean = [mean for i, mean in enumerate(scaler.mean_) if i in indice_outputs]\n    var = [var for i, var in enumerate(scaler.var_) if i in indice_outputs]\n    \n    predictions = pd.DataFrame(torch.cat(test_output).detach().numpy(), columns = output)\n    \n    predictions = predictions * var + mean\n    y = y_test[parameters['seq_len']:] * var + mean\n    \n    def draw(predictions, y, output):\n        fig = make_subplots(rows=2, cols=1, \n                           subplot_titles=(\"Prediction on test dataset for \" + output ,\n                                           \"Feature importance by derivarive importance for \" + output))\n        fig.add_trace(go.Scatter(x=np.arange(len(predictions)), y=np.array(predictions[output]),\n                            mode='lines',\n                            name = 'predictions'), row = 1, col = 1)\n        fig.add_trace(go.Scatter(x=np.arange(len(predictions)), y=np.array(y[output]),\n                            mode='lines',\n                            name = 'ground truth'), row = 1, col = 1)\n        \n        fig.add_trace(go.Bar(x=df.columns[1:], y = gradient_input_mean, \n                            name = 'gradient of lstm w.r.t. features'), row = 2, col = 1)\n\n        return fig.show()\n\n    if draw:\n        draw(predictions, y, output[0])\n \n    return y, predictions\n\ndef train_predict_and_draw(parameters, df, output):\n    train, val, test, scaler = scale_and_split(df) \n    X_train, y_train, X_val, y_val, X_test, y_test = prepare_inputs_outputs(train, val, test, \n                                                                            parameters['seq_len'], \n                                                                            parameters['inputs'], \n                                                                            output)\n\n    train_loader = get_dataloader(X_train, y_train, parameters['batch_size_train'])\n    val_loader = get_dataloader(X_val, y_val, parameters['batch_size_val'])\n    test_loader = get_dataloader(X_test, y_test, parameters['batch_size_test'])\n    model = MTS_RNN(input_size=len(parameters['inputs']), hidden_layer_size=20, output_size=1).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=parameters['lr'])\n    loss_function = nn.MSELoss()\n\n    model_LSTM, train_val_history = train_model(model, \n                                                train_loader, \n                                                val_loader,\n                                                loss_function, \n                                                optimizer, \n                                                parameters['epochs'])\n\n    draw_training_loss(train_val_history, parameters['epochs'])\n\n    y, predictions = predict_and_draw(df, test_loader, [output], scaler, model, y_test, loss_function, \n                                      draw = True)\n    return y, predictions\n\n","0500493d":"parameters = {'inputs':Lake_Bilancino.columns[1:],\n              'outputs':['Lake_Level', 'Flow_Rate'],\n            'seq_len': 30,\n              'batch_size_train':32,\n              'batch_size_val':8,\n              'batch_size_test':8,\n              'lr':0.002,\n              'epochs':20\n             }\n\ny, predictions = train_predict_and_draw(parameters, Lake_Bilancino.iloc[578:], parameters['outputs'][0])","cbecbd43":"ab=mean_squared_error(y[:len(predictions)], predictions)\nprint('The RMSE of the model is :',ab)\naaa=mean_squared_error(y[:len(predictions)], predictions, squared = False)\nprint('The MSE of the model is :',aaa)","ce4fb633":"parameters = {'inputs':Lake_Bilancino.columns[1:],\n              'outputs':['Lake_Level', 'Flow_Rate'],\n            'seq_len': 30,\n              'batch_size_train':32,\n              'batch_size_val':8,\n              'batch_size_test':8,\n              'lr':0.0001,\n              'epochs':40\n             }\n\ny, predictions = train_predict_and_draw(parameters, Lake_Bilancino.iloc[578:], parameters['outputs'][1])","4849a62a":"ab1=mean_squared_error(y[:len(predictions)], predictions)\nprint('The RMSE of the model is :',ab)\naaa1=mean_squared_error(y[:len(predictions)], predictions, squared = False)\nprint('The MSE of the model is :',aaa1)","63179654":"# **LAKE**","2c0a0ecb":"### **Output for Flow_rate**","cac83b45":" #                           AQUIFER","2783726d":"### **Output for lake level**","0ff08210":"##        **Reading Data**","787f9a76":"Aquifer is an underground layer of water-bearing permeable rock, rock fractures or unconsolidated materials (gravel, sand, or silt). It can be extracted using a water well.\n\n\n\n![image.png](attachment:image.png)\n\nIt has four different area Auser, Doganella, Luco, and Petrignano. Let's see the discription of their columns:\n\n**Date:** Regular date (Primary key)\nRainfall_X: It indicates the quantity of rain falling, expressed in millimeters (mm), in the area X.\n\n**Depth_to_Groundwater_Y:** It indicates the groundwater level, expressed in ground level (meters from the ground floor), detected by the piezometer Y.\n\n**Temperature_Z:** It indicates the temperature, expressed in \u00b0C, detected by the thermometric station Z.\n\n**Volume_H:** It indicates the volume of water, expressed in cubic meters (mc), taken from the drinking water treatment plant H.\n\n**Hydrometry_K:** It indicates the groundwater level, expressed in meters (m), detected by the hydrometric station K.\n\nFrom above, we know that for this water body we need a model which can predict Depth_to_Groundwater of water per seconds."}}