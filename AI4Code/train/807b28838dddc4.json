{"cell_type":{"f8e30f40":"code","a5bea258":"code","8c34b71a":"code","8ff1d2ea":"code","d322b053":"code","580d95e9":"code","cfe0432f":"code","f56740f5":"code","4a2159e8":"code","1f4c4271":"code","6ad64c7b":"code","fdb8c708":"code","99c8a01e":"code","8cea1f18":"code","e1c6d8f1":"code","ddeeb715":"code","d1d540a6":"code","f2e853f5":"code","cc7093c0":"code","89e4956f":"code","04978bc8":"code","dd4135f2":"code","2c72ac8c":"code","a86eb556":"code","65766536":"code","1a18e4c2":"code","0cfa6fa1":"code","36f79224":"code","f97490d0":"code","99b04b44":"code","2215721b":"code","cef03589":"code","eb3f9b44":"code","0231efcd":"code","a878496e":"code","c0663b96":"markdown","f93cf6c1":"markdown","69dc993a":"markdown","d097a6c6":"markdown","d7a4f29e":"markdown","96e3f810":"markdown","7aec47bc":"markdown","eed3e146":"markdown","32f6c542":"markdown","76e30c59":"markdown","ea89eedd":"markdown","d24ea6c4":"markdown","1a3ceeed":"markdown","37f4e7ce":"markdown","fdbe3f66":"markdown","116ad4b8":"markdown","52cf1966":"markdown","a0dcd7aa":"markdown","056be605":"markdown","41dcf5b0":"markdown","249857a7":"markdown","bb2895a9":"markdown","07f45ba4":"markdown","0c72168c":"markdown","6c1437de":"markdown","0d18eb42":"markdown","d0113c0c":"markdown","aa3dfaf2":"markdown","89eb35f7":"markdown","f74eba9f":"markdown","c30ec368":"markdown","72417cc2":"markdown","d29c7550":"markdown","726436b3":"markdown","aad27baa":"markdown","0fdcfb5a":"markdown","f05725f4":"markdown","9d649708":"markdown","d6d5f9e1":"markdown","728844f5":"markdown","088b45f5":"markdown"},"source":{"f8e30f40":"%%time\n\nimport os\n\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn import ensemble, linear_model,metrics,model_selection,neighbors,preprocessing, svm, tree\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import classification_report, accuracy_score, log_loss, roc_auc_score\nfrom sklearn.model_selection import cross_validate,cross_val_score,train_test_split, KFold, GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler, RobustScaler\n\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\nprint('Libraries imported')","a5bea258":"%%time\n# Get data\ntrain=pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv', index_col='id')\ntest=pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv',index_col='id')\nfeatures=[]\nfor feature in test.columns:\n    features.append(feature)\n\nprint('Data Import Complete')","8c34b71a":"%%time\n## from: https:\/\/www.kaggle.com\/bextuychiev\/how-to-work-w-million-row-datasets-like-a-pro\ndef reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df\n\ntrain = reduce_memory_usage(train, verbose=True)\ntest = reduce_memory_usage(test, verbose=True)\nprint('Memory reduced')","8ff1d2ea":"%%time\n# shape of data\nprint('How much data was imported?')\nprint('training data shape ;',train.shape)\nprint('test data shape ;',test.shape)\n\n# missing data\nprint('\\nHow much data is missing?')\n\ntraining_missing_val_count_by_column = (train.isnull().values.sum())\ntest_missing_val_count_by_column = (test.isnull().values.sum())\nprint('missing training data :  {:.2f} ({:.1f})%'.format (training_missing_val_count_by_column,training_missing_val_count_by_column\/train.shape[0]))\nprint('missing test data :  {:.2f} ({:.1f})%'.format (test_missing_val_count_by_column,test_missing_val_count_by_column\/test.shape[0]))\nprint('\\noverview complete')","d322b053":"%%time\nprint(train.info(),'\\n')\nprint(test.info(),'\\n')","580d95e9":"%%time\nprint('Sample training data')\ntrain.head()","cfe0432f":"%%time\nprint('Sample test data')\ntest.head()","f56740f5":"%%time\ntrain.describe()\ntest.describe()","4a2159e8":"%%time\n#drop observations (rows) with nan's \nprint('Missing Training Data')\ntrain2=train.dropna(axis='rows')\nprint (\"rows ; \",train.shape[0],\"\\nrows with missing data : \", ((train.shape[0]) - (train2.shape)[0]),(((train.shape[0]) - (train2.shape)[0])\/train.shape[0])*100,\"%\")\n\n#drop features (columns) with nan's \ntrain3=train.dropna(axis='columns')\nprint(\"columns ; \", train.shape[1], \"\\ncolumns with missing data : \", ((train.shape[1]) - (train3.shape)[1]))\n\nprint('\\nMissing Test Data')\ntest2=test.dropna(axis='rows')\nprint (\"rows ; \",test.shape[0],\"\\nrows with missing data : \", ((test.shape[0]) - (test2.shape)[0]),(((test.shape[0]) - (test2.shape)[0])\/test.shape[0])*100,\"%\")\n\n#drop features (columns) with nan's \ntest3=test.dropna(axis='columns')\nprint(\"columns ; \", test.shape[1], \"\\ncolumns with missing data : \", ((test.shape[1]) - (test3.shape)[1]),'\\n')\nsize=[(train.shape)[0],(test.shape)[0]]\nmissing=[(train2.shape)[0],(test2.shape)[0]]\nplt.bar(np.arange(0,len(missing),1),size,color='b',alpha=0.5)\nplt.bar(np.arange(0,len(missing),1),missing,color='b',alpha=0.5)\nplt.xticks([1,2],('training','test'))\nplt.title('Rows with missing data')\nplt.show()","1f4c4271":"%%time\n# Plot missing data\ntraining_missing_val_count_by_column = (train.isnull().sum())\nplt.bar(np.arange(0,len(training_missing_val_count_by_column),1),training_missing_val_count_by_column)\nplt.xlabel('column')\nplt.ylabel('Number of missing values')\nplt.show()\nprint(training_missing_val_count_by_column.describe())","6ad64c7b":"# missing value by label\nclaim=train[train['claim']==1]\nmissing_claim=(claim.isnull().sum())\nnoclaim=train[train['claim']==0]\nmissing_noclaim=(noclaim.isnull().sum())\nplt.bar([1,2],[missing_claim.sum(),missing_noclaim.sum()])\nplt.xticks([1,2],('claim','No claim'))\nplt.show()","fdb8c708":"sns.distplot(train['claim'], kde=True, hist=False)","99c8a01e":"%%time\n# generate box plots\ntrain_outliers = ((train - train.min())\/(train.max() - train.min()))\nfig, ax = plt.subplots(4, 1, figsize = (25,25))\nsns.boxplot(data = train_outliers.iloc[:, 1:30], ax = ax[0])\nsns.boxplot(data = train_outliers.iloc[:, 30:60], ax = ax[1])\nsns.boxplot(data = train_outliers.iloc[:, 60:90], ax = ax[2])\nsns.boxplot(data = train_outliers.iloc[:, 90:120], ax = ax[3])","8cea1f18":"%%time\ntrain_outliers = ((train - train.min())\/(train.max() - train.min()))\ntest_outliers = ((test - test.min())\/(test.max() - test.min()))\ntry:\n    train_outliers.drop(['claim'], axis=1, inplace=True)\nexcept:\n    print('Already separated')\nprint('Training data (blue), Test Data (red)')\nfig = plt.figure(figsize = (20, 140))\nfor idx, i in enumerate(train_outliers.columns):\n    fig.add_subplot(np.ceil(len(train_outliers.columns)\/4), 4, idx+1)\n    train_outliers.iloc[:, idx].hist(bins = 20,color='b',alpha=0.5)\n    test_outliers.iloc[:, idx].hist(bins = 20,color='r',alpha=0.5)\n    plt.title(i)\n#plt.text(9, -20000, caption, size = 12)\nplt.show()","e1c6d8f1":"%%time\n# Generate correlations\ncorr=train.corr()\n\n# create heatmap\nmask = np.triu(np.ones_like(corr, dtype = bool))\nplt.figure(figsize = (15, 15))\nplt.title('Correlation matrix for Train data')\nsns.heatmap(corr, mask = mask,annot=False, linewidths = .5,square=True,cbar_kws={\"shrink\": .60})\nplt.show()","ddeeb715":"%%time\n# get list of columns with high correlations\ncorr_matrix = train.corr().abs()\nhigh_corr=np.where(corr_matrix>0.02)\nhigh_corr=[(corr_matrix.columns[x],corr_matrix.columns[y]) for x,y in zip(*high_corr) if x!=y and x<y]\nprint(\"high correlation \\n\",high_corr)\nfeaturesofinterest=['f6','f15', 'f32', 'f34','f36','f45','f46','f51', 'f57','f86','f90','f97','f111']","d1d540a6":"# add missing data field\nif \"Missing\" in train.columns:\n    print('Missing training feature exists')\nelse:\n    missing_values_list=train.isnull().sum(axis=1).tolist()\n    train.insert(0,\"Missing\",missing_values_list, True)\n    print('Missing training feature added')\n    \nif \"Missing\" in test.columns:\n    print('Missing training feature exists')\nelse:\n    missing_values_list2=test.isnull().sum(axis=1).tolist()\n    test.insert(0, \"Missing\",missing_values_list2, True)\n    print('Missing test feature added')","f2e853f5":"%%time\n# impute missing values\nimputer = SimpleImputer(strategy='mean')\ntrain[features]=imputer.fit_transform(train[features])\ntest[features] =imputer.transform(test[features])\nprint('Missing values imputed')","cc7093c0":"%%time\n# select scaler\nscale = StandardScaler()\n#scale = RobustScaler()\n#scale = MinMaxScaler()\ntrain[features]=scale.fit_transform(train[features])\ntest[features]= scale.transform(test[features])  \n\nprint('Data scaled')","89e4956f":"%%time\ntry:\n    y = train.claim\n    train.drop(['claim'], axis=1, inplace=True)\nexcept:\n    print('claim already separated')\nX=train\nprint('Target data separated')\n# Create data sets for training (80%) and validation (20%)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,train_size=0.2,test_size = 0.2,random_state = 0)","04978bc8":"%%time\n# taken from https:\/\/www.kaggle.com\/ryanholbrook\/getting-started-september-2021-tabular-playground\n\ndef score(X, y, model, cv):\n    scoring = [\"roc_auc\"]\n    scores = cross_validate(\n        model, X_train, y_train, scoring=scoring, cv=cv, return_train_score=True\n    )\n    scores = pd.DataFrame(scores).T\n    return scores.assign(\n        mean = lambda x: x.mean(axis=1),\n        std = lambda x: x.std(axis=1),\n    )\nprint('Function built')\n\nmodelcomparison=pd.DataFrame(columns=['Model', 'ROC_AUC', 'Score'])","dd4135f2":"%%time\n# Instanciate model\nxgbmodel = XGBClassifier(\n    n_estimators=1000,\n    learning_rate=0.1,\n    max_depth=3,\n    min_child_weight=3,\n    subsample=0.5,\n    colsample_bytree=0.5,\n    n_jobs=-1,\n    objective='binary:logistic',\n    eval_metric='auc', \n    # Uncomment if you want to use GPU. Recommended for whole training set.\n    #tree_method='gpu_hist',\n    random_state=38)\nxgbmodel.fit(X_train, y_train, verbose = False)\n\nprint('model fit complete')","2c72ac8c":"%%time\n# evaluate model\ny_preds = xgbmodel.predict(X_valid)\nxgb_score=roc_auc_score(y_valid, y_preds)\nprint(xgb_score)\n\nxgbscores = score(X_train, y_train, xgbmodel, cv=4)\ndisplay(xgbscores)\nprint('scoring completed')","a86eb556":"%%time\nmetrics.plot_confusion_matrix(xgbmodel, X_valid, y_valid)\nplt.title('Confusion matrix')\nplt.grid(False)\nplt.show()","65766536":"%%time\n# taken from https:\/\/www.kaggle.com\/ryanholbrook\/getting-started-september-2021-tabular-playground\n\n# Make predictions\npredictions = xgbmodel.predict(test)\n\n# Save the predictions to a CSV file\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\nsample_submission.claim = predictions\nsample_submission.to_csv(\"prediction_binary_submission.csv\",index=False)\nprint('xgboost binary prediction complete')","1a18e4c2":"%%time\n# taken from https:\/\/www.kaggle.com\/ryanholbrook\/getting-started-september-2021-tabular-playground\n\n# Make probabistic predictions\nprob_predictions = xgbmodel.predict_proba(test)\n\n# Save the predictions to a CSV file\nsample_submission_prob = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\nsample_submission_prob.claim = prob_predictions\nsample_submission_prob.to_csv(\"normalised_prediction_proba_submission.csv\",index=False)\nprint('xgboost probabalistic prediction complete')","0cfa6fa1":"sns.distplot(sample_submission['claim'], kde=True, hist=False)\nsns.distplot(sample_submission_prob['claim'], kde=True, hist=False)","36f79224":"%%time\ntry:\n    y = train.claim\n    train.drop(['claim'], axis=1, inplace=True)\nexcept:\n    print('claim already separated')\nX=train\nprint('Target data separated')\n# Create data sets for training (80%) and validation (20%)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,train_size=0.8,test_size = 0.2,random_state = 0)","f97490d0":"%%time\n# instanciate model\nlgbmmodel = LGBMClassifier(learning_rate=0.05,\n                      n_estimators=1000,\n                      reg_lambda = 1)\n# fit model\nlgbmmodel.fit(X_train, y_train)\n# score model\ny_preds = lgbmmodel.predict(X_valid)#[:, 1]\nlgbm_score=roc_auc_score(y_valid, y_preds)\nprint(\"Area under the curve score ; \", lgbm_score)\n\n# evaluate model\ny_preds = lgbmmodel.predict(X_valid)\nlgbscores=roc_auc_score(y_valid, y_preds)\nprint(\"Area under the curve score (binary) ; \",lgbscores)\n\nlgbscore = score(X_train, y_train, lgbmmodel, cv=4)\ndisplay(lgbscore)\nprint('scoring completed')\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(lgbmmodel, X_valid, y_valid)\nplt.title('Confusion matrix')\nplt.grid(False)\nplt.show()\n\n# predict test\nlgbm_preds = lgbmmodel.predict(test)#[:, 1]\n\n# Save the predictions to a CSV file\nlgbm_submission = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\nlgbm_submission.claim = lgbm_preds\nlgbm_submission.to_csv(\"lgbm_prediction_submission.csv\",index=False)\nprint('lgbm prediction complete')   \n\nsns.distplot(lgbm_submission['claim'], kde=True, hist=False)","99b04b44":"%%time\n# catboost\n\n# instanciate model\ncatboostmodel = CatBoostClassifier(iterations=1555, \n    bootstrap_type='Bernoulli', \n    od_wait=1144, \n    learning_rate=0.025, \n    reg_lambda=36, \n    random_strength=43.75, \n    depth=7, \n    min_data_in_leaf=11, \n    leaf_estimation_iterations= 1, \n    subsample= 0.8227911142845009,\n    verbose=0)\n# fit model\ncatboostmodel.fit(X_train, y_train)\nprint(\"model fit\")\n# score model\ny_preds = catboostmodel.predict(X_valid)\ncatboostmodel_score=roc_auc_score(y_valid, y_preds)\nprint(\"Area under the curve score ; \", catboostmodel_score)\n\n# evaluate model\ny_preds = catboostmodel.predict(X_valid)\ncatboostscores=roc_auc_score(y_valid, y_preds)\nprint(catboostscores)\ncatboostscore = score(X_train, y_train, catboostmodel, cv=4)\ndisplay(catboostscore)\nprint('scoring completed')\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(catboostmodel, X_valid, y_valid)\nplt.title('Confusion matrix')\nplt.grid(False)\nplt.show()\n\n# predict test\ncatboost_preds = catboostmodel.predict(test)\n# Save the predictions to a CSV file\ncatboost_submission = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\ncatboost_submission.claim = catboost_preds\ncatboost_submission.to_csv(\"catboost_prediction_submission.csv\",index=False)\nprint('catboost prediction complete')   \n\nsns.distplot(catboost_submission['claim'], kde=True, hist=False)","2215721b":"# Feature importance lgbm\n# instanciate and fit model\nlgbm_checker = LGBMClassifier(learning_rate=0.05,\n                      n_estimators=1000,\n                      reg_lambda = 1)\nlgbm_checker.fit(X_train, y_train)\n\n# put feature impoartance into table\nimportances_df = pd.DataFrame(lgbm_checker.feature_importances_, columns=['Feature_Importance'],\n                              index=X_train.columns)\nimportances_df.sort_values(by=['Feature_Importance'], ascending=False, inplace=True)\n#print(importances_df)\n\n# plot importance as bar chart\nimportances=importances_df.index\nimportances_df = importances_df.sort_values(['Feature_Importance'])\ny_pos = np.arange(len(importances_df))\nplt.figure(figsize=(8,20))\nplt.barh(y_pos,importances_df['Feature_Importance'])\nplt.yticks(y_pos, importances,fontsize=10)\nplt.ylabel('importance')\nplt.title('Feature importance (lgbm)')\nplt.show()\n# print(importances_df)","cef03589":"# get important features\nimportart_features=[]\nfor feature in importances_df.head(95).index:\n    importart_features.append(feature)\nprint(importart_features)","eb3f9b44":"%%time\ntrain_important=train[importart_features]\ntest_important=test[importart_features]\ntrain_outliers = ((train_important - train_important.min())\/(train_important.max() - train_important.min()))\ntest_outliers = ((test_important - test_important.min())\/(test_important.max() - test_important.min()))\ntry:\n    train_outliers.drop(['claim'], axis=1, inplace=True)\nexcept:\n    print('Already separated')\nprint('Training data (blue), Test Data (red)')\nfig = plt.figure(figsize = (20, 40))\nfor idx, i in enumerate(train_outliers.columns):\n    fig.add_subplot(np.ceil(len(train_outliers.columns)\/4), 4, idx+1)\n    train_outliers.iloc[:, idx].hist(bins = 20,color='b',alpha=0.5)\n    test_outliers.iloc[:, idx].hist(bins = 20,color='r',alpha=0.5)\n    plt.title(i)\n#plt.text(9, -20000, caption, size = 12)\nplt.show()","0231efcd":"%%time\n#get test\n#testcolumns=pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv',index_col='id')\n#test.columns=testcolumns.columns\n\n# Create data sets for training (20%) and validation (20%)\nrX_train, rX_valid, ry_train, ry_valid = train_test_split(X[importart_features], y,train_size=0.8,test_size = 0.2,random_state = 0)\n\n# instanciate model\nlgbmmodel = LGBMClassifier(learning_rate=0.05,\n                      n_estimators=1000,\n                      reg_lambda = 1)\n# fit model\nlgbmmodel.fit(rX_train, ry_train)\nprint('model fit')\n# score model\ny_preds = lgbmmodel.predict(rX_valid)\nlgbm_score=roc_auc_score(ry_valid, y_preds)\nprint(lgbm_score)\n\n# evaluate model\ny_preds = lgbmmodel.predict(rX_valid)\nlgbmmodelscores=roc_auc_score(ry_valid, y_preds)\nprint(lgbmmodelscores)\nlgbmmodelscore = score(rX_train, ry_train, lgbmmodel, cv=4)\ndisplay(lgbmmodelscore)\nprint('scoring completed')\n\n# predict test\nlgbm_preds = lgbmmodel.predict(test[importart_features])\n\n#confusion matrix\nmetrics.plot_confusion_matrix(lgbmmodel, rX_valid, ry_valid)\nplt.title('Confusion matrix')\nplt.grid(False)\nplt.show()\n\n# Save the predictions to a CSV file\nlgbm_submission = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\nlgbm_submission.claim = lgbm_preds\nlgbm_submission.to_csv(\"lgbm_prediction_featurereduction_submission2.csv\",index=False)\nprint('lgbm prediction with reduced features complete')   \n\nsns.distplot(lgbm_submission['claim'], kde=True, hist=False)","a878496e":"%%time\n\n# Split data \nX_train,X_valid,y_train,y_valid = train_test_split(X,y,test_size=0.33, random_state=48,stratify=y)\n\n# instanciate models\nlgbm_model =  LGBMClassifier(learning_rate=0.05,\n                      n_estimators=500,\n                      reg_lambda = 1)\ncatboost_model =  CatBoostClassifier(iterations=1555, \n    bootstrap_type='Bernoulli', \n    od_wait=1144, \n    learning_rate=0.025, \n    reg_lambda=36, \n    random_strength=43.75, \n    depth=7, \n    min_data_in_leaf=11, \n    leaf_estimation_iterations= 1, \n    subsample= 0.8227911142845009,\n    verbose=0)\nxgb_model =  XGBClassifier(\n    n_estimators=500,\n    learning_rate=0.1,\n    max_depth=3,\n    min_child_weight=3,\n    subsample=0.5,\n    colsample_bytree=0.5,\n    n_jobs=-1,\n    objective='binary:logistic',\n    eval_metric='auc', \n    random_state=38)\n\n# create zero values\nlgbm_preds = np.zeros(len(test))\ncatboost_preds = np.zeros(len(test))\nxgb_preds = np.zeros(len(test))\n\nlgbm_oof = np.zeros(len(X_valid))\ncatboost_oof = np.zeros(len(X_valid))\nxgb_oof = np.zeros(len(X_valid))\n\n# LGBM\nlgbm_model.fit(X_train,y_train)\nlgbm_oof = lgbm_model.predict_proba(X_valid)[:,1]\nlgbm_preds = lgbm_model.predict_proba(test)[:,1]\nprint('lgbm model complete')\n\n# Catboost\ncatboost_model.fit(X_train,y_train)\ncatboost_oof = catboost_model.predict_proba(X_valid)[:,1]\ncatboost_preds = catboost_model.predict_proba(test)[:,1]\nprint('catboost model complete')\n\n# XGBoost\nxgb_model.fit(X_train,y_train)\nxgb_oof = xgb_model.predict_proba(X_valid)[:,1]\nxgb_preds = xgb_model.predict_proba(test)[:,1]\nprint('xgboost model complete')\n\nblend_x_valid","c0663b96":"## Feature Reduction (lgbmodel)","f93cf6c1":"## Catboost","69dc993a":"# Base line modelling with XGBOOST \n\nEssentially this is a classifier problem in that claims are binary, customer either make a claim or they don't. So we are going to use the XGBclassifier model to create a baseline with which to compare future models. ","d097a6c6":"## Distibution boxplots by feature","d7a4f29e":"### Observations on blending\n\nThe highest score from a single model was 0.79 which improved to 0.81 when 3 models were blended.","96e3f810":"### Observations  on baseline results\n\nThe scores for the untuned XGBClassifier model without imputing missing values or normalizing the data were 0.49385 for the binary and 0.50924 for the prabalistic submissions.\n\nThe scores for the XGBClassifier model with imputed missing values and minmax normalization of the data improved to 0.54.\n\nThe leaderboard has scores of 0.8, so there is definately room for improvement.","7aec47bc":"# About Tabular Playground Series - Sep 2021\n\nThe dataset used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting whether a claim will be made on an insurance policy. Although the features are anonymized, they have properties relating to real-world features.\n\nThe aim is predict whether a customer made a claim upon an insurance policy. The ground truth claim is binary valued, but a prediction may be any number from 0.0 to 1.0, representing the probability of a claim. The features in this dataset have been anonymized and may contain missing values.\n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n\nThis competition involves a tabular dataset. Intended as a stepping store between 'Titanic Getting Started' competition and a Featured competition, it should be approachable for all. Competitions masters or grandmasters won't be challenge by this level. \n\n# About this notebook\n\nThis notebook is a work in progress and will be regularly updated. It is my first notebook of this competition and will concentrate on data exploration and creating a baseline for future notebooks that will concentrate on modelling different solutions. This is a beginner level notebook meant for my own use and not indended to be a training aid or tutorial. Some of the code will be taken from other public notebooks, sources will be creditted at the bottom.\n\n# About David Coxon\n\nI work in IT for an art gallery and generally work with inhouse data on visitor numbers, public donations, powerusage, course attendance, sentament analysis and Adwords, much of my work in with time series.\n\nThis is my fourth contest, starting with Titanic I moved on to tackle a house pricing contest and then the 30days of ML challenge.","eed3e146":"## Add the number of missing values as a feature\n\nbased on the discussion at https:\/\/www.kaggle.com\/c\/tabular-playground-series-sep-2021\/discussion\/270206 we are going to add the number of missing values for each row as a feature.","32f6c542":"## Feature correlation","76e30c59":"## Missing values","ea89eedd":"# Try other Models:\n\nHaving got a baseline with xgboost i tried a couple of alternate models to see how the different models performed these were just best guesses and not yet tuned. I went with 1000 estimators for each as that seemed to work for xgboost which i did more testing on.","d24ea6c4":"## LGBMClassifier","1a3ceeed":"## Observations on Correlation \n\nThere are a reasonably high number of features (118) with a relatively low correlation between features. Feature feature reduction\/engineering may be required to reduce noise.\n\nAll of the features are numeric features, but there is a high degree of variability in the scale of the different features, normalization may be required.\n\nIt is unlikely that we can use correlations between features to help fill missing values more accurately.","37f4e7ce":"# Blending\n\nBlending is a type of stacking that places the predictions of several different models into a new dataframe and then models that dataframe to use the best predictors for each feature.\n\nWe will use the same parameters as used previously for our models. I've used 0.33 as a training size because it is quite a big data set and we are using a lot of estimators in each model so it will take a while to run.","fdbe3f66":"# Data Engineering","116ad4b8":"## Evaluate model performance","52cf1966":"### Initial observation on missing data\n\nThere are a large number of missing data points,1820782 in the training data alone (about 1.6%). Approx 62% of the observation (rows) are missing at least some data and every feature (column) is some missing dat. Working out how to handle the missing data will be a priority.\n\nBoth training and test sets are missing approximately the same percentage of data.","a0dcd7aa":"### Visualize important features","056be605":"## Impute missing values\n\nFor now lets use the same method to impute all missing numbers, we might look at this again later and use different strategies for different features. We can choose from mean or media by remarking out the appropriate line.\n\nDuring exploration i had very different results from different models and wanted to rule problems with preprocessing as as well as imputing the missing values i tried using a loops to substitle the mean for each feature. you can chhose from either of the following code blocks to deal with the missing numbers.\n\n","41dcf5b0":"# Get data","249857a7":"A \"neutral\" AUC is 0.5, so anything better than that means our model learned something useful.\n\nNote: The model scored around 0.8 for both test and train roc, the basic model without imputing and missing data scores 0.75 and 0.86 for test and train. ","bb2895a9":"## Memory reduction","07f45ba4":"## Make a baseline submission\nThis is a classifier problem so predictions are binary 0 and 1. The next code block produces binary predictions, but you're allowed to submit probabilities instead so the code block under that uses predict_proba method instead of predict from scikit-learn to produce a probanility based submission.","0c72168c":"# Feature engineering\n\nNow that we have an idea of how well the basic model works, we can start to look at how important some of the features are. We can then try and remove some of the noisy features to produce better models. ","6c1437de":"# Prepare data for modeling","0d18eb42":"### Observations on lgbm model\n\nThe lightgbm model scored 0.794 on the Area Under Curve score and achieved a public leaderboard score of 0.64211. \n\n137820 correct predictions 53764 incorrect predictions.","d0113c0c":"## Train and test distibution histograms by feature.","aa3dfaf2":"## First look at Data","89eb35f7":"## Missing data by claim\n\nThere seems to be a relationship between the target value 'claim' and the missing values. One of the discussion topics explains by saying that insurrance data often comes from a number of different insurrance companies\/agencie that often collect and process different data so data from different sources is often missing different features, it also suggests that different companies may be more or less likely to recieve claims. \n\nIn this exercise the relationship is such that we can use the degree of missing data as a feature that makes a notable difference in model accuracy. \n\nIts possible that even in models where there is not such a direct relationship between missing data and target value that making 'misingness' a feature will still help with model accuracy because in a random forest based model it will help the model distinguish between observations\/rows that have real values and observations where data was been estimated.","f74eba9f":"### Important features","c30ec368":"# History\nThis notebook is my first notebook on this competition and is very much about exploring the data to see what we have to work with before i decide whether to  \n\nI have added a few more models including random forest and LGBMClassifier to compare results. I have also revisited the code for filling the missing values and normalising the data because I was getting quite inconsistent public scores.\n\nI've added confusion matrices to each of the models to better understand their scores.\n\nThis is version 10 of this notebook but is very much a draft | work in progress.\n\nVersion 10 includes some blending and built on previous feature engineering.\n\n## Next Steps\n\nParameter tuning,\nStacking and Ensembles.\n\n# Credit where credit's due\nThe following notebook's were super inspirational in creating this notebook, and gave me insights I may not have otherwise come up with. Thank you all for sharing your code it's awesome!\n\nMemory reduction https:\/\/www.kaggle.com\/bextuychiev\/how-to-work-w-million-row-datasets-like-a-pro\n\nBoxplots https:\/\/www.kaggle.com\/snikhil17\/making-basic-eda-attractive based on https:\/\/www.kaggle.com\/suharkov\/sep-2021-playground-eda-no-model-for-now\n\nModel and evaluation https:\/\/www.kaggle.com\/ryanholbrook\/getting-started-september-2021-tabular-playground\n\nConfusion matrix https:\/\/www.kaggle.com\/maksymshkliarevskyi\/tps-sep-all-for-start-eda-xgb-catboost-baseline\n\nblending https:\/\/www.kaggle.com\/raahulsaxena\/tps-sept-21-ensembling-stacking-blending\n\nI'm relatively new to kaggle contests, if you have any helpful hints or observations | recommendations please post in comments. \n\nIf you found this notebook useful please upvote. If you found any of the notebooks listed above useful or use any of their code, please credit and upvote the source.","72417cc2":"## Set up environment\n\nLets import all of the libraries that we will need to complete the notebook. (This means loading libraries we might not need if we are only running 1 code block, but it means that we are not having to load the same library over and over if we add them code by code and run the entire notebook).","d29c7550":"## Comparing model performance\n\nWith missing values filled using feature means and features standardized all of the models managed to get an area under the curve score of 0.79. The different models however scored very differently on the public leaderboard best performing model was the lgbm with 0.64, the random forest 0.54 and xgboost only 0.33.\n\nThe xgboost scores that i got ranged from 0.33 to 0.54 depending on how i filled the missing values as means and normalized the data. ","726436b3":"## Create function to score performance","aad27baa":"## Confusion Matrix","0fdcfb5a":"## Claims","f05725f4":"### Observations of feature reduction\n\nIn this really basic test reducing the reductions to the top 25 features reduced the models performance. The prediction in the top half of the confusion matrix improved a little but the lower portion was significantly worse. ","9d649708":"## Initial oversations on claims\n\nThere appear to be 2 equally sized normal distributions of 0 values and 1's in the training set. ","d6d5f9e1":"## Normalize data\n\nLets normalise all of the features. We can choose from Standard, max\/min or robust by remarking out the appropriate options)","728844f5":"# Visualising the data","088b45f5":"## Observations on distribution of feature values.\n\nThe test and training data follow very similar distributions.\n\nSome of the features appear to have normal distributions while others appear to have logarithmic distributions. Some features appear to have anomolies in the distributions. Further explorations may be required to discover which of these features are useful and which are noise."}}