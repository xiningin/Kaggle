{"cell_type":{"e8db650e":"code","56435274":"code","2933c2fc":"code","7bcb03c1":"code","df70c3b3":"code","fd9085f3":"code","ce03efef":"code","0f845da1":"code","0fcf98de":"code","86a2cf83":"code","1d1106a0":"code","7bf77fd7":"code","d4c0485e":"code","94638658":"code","5d81a7ab":"code","da50b2b7":"code","09d58d3b":"code","a888a0dc":"code","02108a73":"markdown","dc7f12eb":"markdown","1eae59fd":"markdown","23ced973":"markdown","0bfd9397":"markdown","8aa230fa":"markdown","a0e2b2ee":"markdown","12de11a3":"markdown","e77b0a51":"markdown","78c1d07f":"markdown","11ef8e1b":"markdown","32fbdcc8":"markdown","ca5872a9":"markdown","131e1475":"markdown","e56b5ad1":"markdown","8e94edb4":"markdown","f0558669":"markdown","4bab2e89":"markdown","af1cb076":"markdown","84a325a0":"markdown","d2841c67":"markdown","9d67a6d1":"markdown","0a400a9c":"markdown","fe67bb7e":"markdown","fde3cb5a":"markdown"},"source":{"e8db650e":"import pandas as pd\nimport numpy as np","56435274":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\nprint(\"Train Shape: \" + str(train.shape))\nprint(\"Test Shape: \" + str(test.shape))","2933c2fc":"print(list(train))","7bcb03c1":"unique_vals_per_col = train.T.apply(lambda x: x.nunique(), axis=1)\nprint(unique_vals_per_col.head(5))","df70c3b3":"import matplotlib.pyplot as plt\n\n# a scatter plot comparing num_children and num_pets\ntrain.plot(kind='scatter',x='GrLivArea',y='SalePrice',color='green')\nplt.show()\nplt.clf()","fd9085f3":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n","ce03efef":"train.groupby('Neighborhood')['SalePrice'].mean().plot(kind='bar')\nplt.show()\nplt.clf()","0f845da1":"train.groupby('Neighborhood')['SalePrice'].count().plot(kind='bar')\nplt.show()\nplt.clf()","0fcf98de":"train.groupby('HouseStyle')['SalePrice'].mean().plot(kind='bar')\nplt.show()\nplt.clf()","86a2cf83":"train.groupby('KitchenQual')['SalePrice'].mean().plot(kind='bar')\nplt.show()\nplt.clf()","1d1106a0":"y = train['SalePrice']\ntrain.drop(\"SalePrice\", axis = 1, inplace = True)","7bf77fd7":"from sklearn import preprocessing\n#from sklearn.preprocessing import LabelEncoder\nimport datetime\n\nntrain = train.shape[0]\n\nall_data = pd.concat((train, test)).reset_index(drop=True)\nprint(\"all_data size is : {}\".format(all_data.shape))\n\ndef ordinal_encode(df, col, order_list):\n    df[col] = df[col].astype('category', ordered=True, categories=order_list).cat.codes\n    return df\n\ndef label_encode(df, col):\n    for c in col:\n        #print(str(c))\n        encoder = preprocessing.LabelEncoder()\n        df[c] = encoder.fit_transform(df[c].astype(str))\n    return df \n\ndef split_all_data(all_data, ntrain):\n    print(('Split all_data back to train and test: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now())))\n    train_df = all_data[:ntrain]\n    test_df = all_data[ntrain:]\n    return train_df, test_df\n\n\"\"\"\nNOW START ENCODING 1. ORDINALS\n\"\"\"\nprint(('Ordinal Encoding: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now())))\norder_list = ['Ex', 'Gd', 'TA', 'Fa', 'Po'] #This applies to a few different columns\ncols = ['KitchenQual', 'ExterQual', 'ExterCond', 'HeatingQC']\nfor col in cols:\n    all_data = ordinal_encode(all_data, col, order_list)\n\norder_list = ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'NA'] #This applies to a few different columns\ncols = ['BsmtQual', 'BsmtCond']\nfor col in cols:\n    all_data = ordinal_encode(all_data, col, order_list)\n\norder_list = ['Gd', 'Av', 'Mn', 'No', 'NA']\ncols = ['BsmtExposure', 'FireplaceQu', 'GarageQual', 'GarageCond']\nfor col in cols:\n    all_data = ordinal_encode(all_data, col, order_list)\n    \norder_list = ['Typ', 'Min1', 'Min2', 'Mod', 'Maj1', 'Maj2', 'Sev', 'Sal']\ncols = ['Functional']\nfor col in cols:\n    all_data = ordinal_encode(all_data, col, order_list)\n    \norder_list = ['Fin', 'RFn', 'Unf', 'NA']\ncols = ['GarageFinish']\nfor col in cols:\n    all_data = ordinal_encode(all_data, col, order_list)\n    \norder_list = ['Ex', 'Gd', 'TA', 'Fa', 'NA'] \ncols = ['PoolQC']\nfor col in cols:\n    all_data = ordinal_encode(all_data, col, order_list)\n\n\"\"\"\nENCODE 2. NON-ORDINAL LABELS\n\"\"\"\nprint(('Label Encoding: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now())))\ncols_to_label_encode = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', \n                       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'CentralAir', \n                       'Electrical', 'GarageType', 'PavedDrive', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']\nall_data = label_encode(all_data, cols_to_label_encode)\n\ntrain, test = split_all_data(all_data, ntrain)\n\nprint(\"Train Shape: \" + str(train.shape))\nprint(\"Test Shape: \" + str(test.shape))\n\n    ","d4c0485e":"def score_transformer(y):\n    y = np.log(y)\n    \n    return y\n\ny = score_transformer(y)","94638658":"#Quicker to calculate once for train and test for values where this is appropriate\nall_data = pd.concat((train, test)).reset_index(drop=True)\n\nall_data['fe.sum.GrLivArea_BsmtFinSF1_BsmtFinSF2'] = all_data['GrLivArea'] + all_data['BsmtFinSF1'] + all_data['BsmtFinSF2'] \nall_data['fe.sum.OverallQual_Overall_Cond'] = all_data['OverallQual'] + all_data['OverallCond']\nall_data['fe.mult.OverallQual_Overall_Cond'] = all_data['OverallQual'] * all_data['OverallCond']\nall_data['fe.sum.KitchenQual_ExterQual'] = all_data['KitchenQual'] + all_data['ExterQual']\nall_data['fe.mult.OverallQual_Overall_Cond'] = all_data['OverallQual'] * all_data['OverallCond']\nall_data['fe.ratio.1stFlrSF_2ndFlrSF'] = all_data['1stFlrSF'] \/ all_data['2ndFlrSF']\nall_data['fe.ratio.BedroomAbvGr_GrLivArea'] = all_data['BedroomAbvGr'] \/ all_data['GrLivArea']\n\n","5d81a7ab":"train_features = list(all_data)\n#Id should be removed for modelling\ntrain_features = [e for e in train_features if e not in ('ExterQual', 'Condition2', 'GarageCond', 'Street', 'Alley', 'PoolArea', 'PoolQC', 'Utilities', \n                                                         'GarageQual', 'MiscVal', 'MiscFeature')]\n\ntrain, test = split_all_data(all_data, ntrain)\n\ntrain_features.remove('Id')\n\n#remove highly correlated variables\n#train_features.remove('GarageFinish')\n#train_features.remove('GarageArea')","da50b2b7":"from sklearn.model_selection import KFold\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\n\nnfolds=5\nkf = KFold(n_splits=nfolds, shuffle=True, random_state=37) #33 originally\ny_valid_pred = 0*y\ny_valid_pred_cat = 0*y\nfold_scores = [0] * nfolds\nfold_scores_cat = [0] * nfolds\n\nimportances = pd.DataFrame()\noof_reg_preds = np.zeros(train.shape[0])\nsub_reg_preds = np.zeros(test.shape[0])\nsub_reg_preds_cat = np.zeros(test.shape[0])\n\n\nfor fold_, (train_index, val_index) in enumerate(kf.split(train, y)):\n    trn_x, trn_y = train[train_features].iloc[train_index], y.iloc[train_index]\n    val_x, val_y = train[train_features].iloc[val_index], y.iloc[val_index]\n    \n    reg = LGBMRegressor(\n        num_leaves=15,\n        max_depth=3,\n        min_child_weight=50,\n        learning_rate=0.04,\n        n_estimators=1000,\n        #min_split_gain=0.01,\n        #gamma=100,\n        reg_alpha=0.01,\n        reg_lambda=5,\n        subsample=1,\n        colsample_bytree=0.21,\n        random_state=2\n    )\n    reg.fit(\n        trn_x, trn_y,\n        eval_set=[(val_x, val_y)],\n        early_stopping_rounds=20,\n        verbose=100,\n        eval_metric='rmse'\n    )    \n    imp_df = pd.DataFrame()\n    imp_df['feature'] = train_features\n    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n\n    imp_df['fold'] = fold_ + 1\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n\n    y_valid_pred.iloc[val_index] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    y_valid_pred[y_valid_pred < 0] = 0\n    fold_score = reg.best_score_['valid_0']['rmse']\n    fold_scores[fold_] = fold_score\n    _preds = reg.predict(test[train_features], num_iteration=reg.best_iteration_)\n    _preds[_preds < 0] = 0\n    sub_reg_preds += _preds \/ nfolds\n    \nprint(\"LightGBM CV RMSE: \" + str(mean_squared_error(y, y_valid_pred) ** .5))\nprint(\"LightGBM CV standard deviation: \" + str(np.std(fold_scores)))\n   \n","09d58d3b":"import seaborn as sns\nimport warnings\n#cat_rgr.fit(X_train, y_train, eval_set=(X_valid, y_valid), logging_level='Verbose', plot=False)\nwarnings.simplefilter('ignore', FutureWarning)\n\nimportances['gain_log'] = np.log1p(importances['gain'])\nmean_gain = importances[['gain', 'feature']].groupby('feature').mean()\nimportances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(10, 14))\nsns.barplot(x='gain_log', y='feature', data=importances.sort_values('mean_gain', ascending=False))","a888a0dc":"sub_reg_preds = np.exp(sub_reg_preds)\ntest.is_copy = False #disable the SettingWithCopyWarning\ntest.loc[:,'SalePrice'] = sub_reg_preds\ntest[['Id', 'SalePrice']].to_csv(\"tutorial_sub.csv\", float_format='%.8f', index=False)\n","02108a73":"Let's see how many unique values there are per column. Storing this information could help us later in deciding how to treat the columns.","dc7f12eb":"Let's load the train and test data and check their shapes","1eae59fd":"## Kernel based on Ames Housing Dataset ","23ced973":"This graph shows us a few things. There does appear to be a strong correlation between the non-basement living area and the sale price. Also we can see that there are a couple of outliers of large properties with a low sale price. Lets remove those outliers.\n","0bfd9397":"### Plot Feature Importance","8aa230fa":"Some neighborhoods seem more expensive than others. Don't forget to consider the sample size before reading too much further into the exact figures. There are 1481 records and 25 neighborhoods. That gives us a mean of just under 60 properties per neighborhood. Common sense tells us that in any given town some neighborhoods may be more desirable than others. Lets take a look at the count of sales per neighborhood. This will give us an idea if some neighborhoods are larger than others, or experienced higher volumes of sales.","a0e2b2ee":"### Exploring Neighborhoods","12de11a3":"### Transform the target\nWe need to transform the target to match the evaluation criteria. \"Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\"","e77b0a51":"We can see that there are a lot of categorical variables such as 'Neighborhood' and 'GarageType' as well as continuous variables such as 'LotArea' and of course the target 'SalePrice'.","78c1d07f":"### Now lets set up a cross-validation framework","11ef8e1b":"#### Categorical Encoding\nA lot of the features we are working with are categoricals. Some but not all of those are ordinals. Lets deal with each appropriately. First we make sure that we are encoding on train and test together.","32fbdcc8":"### Housing Style and Kitchen Quality","ca5872a9":"Anecdotal evidence suggests that a good kitchen helps with the sale a of a property. Lets see if that is true in Ames, Iowa. We will plot the KitchenQual column against the Sale Price.\nAs a reminder here are the meanings of the categorical variables:\n\n    KitchenQual: Kitchen quality\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical\/Average\n       Fa\tFair\n       Po\tPoor","131e1475":"### Plotting Living Area to Sale Price","e56b5ad1":"KitchenQual looks like an ordinal variable. Excellent is better than Good which is better than Typical\/Average which is better than Fair. We can see that no properties sold with a kitchen adjudged to be Poor quality. Perhaps this is because the quality assessment process is subjective and the assessors didn't want to be too harsh. We can also see that the quality of the kitchen is strongly correlated to the sale price. So it seems in this case that the anecdotal evidence does have some merit. Of course there are caveats related to the sample size, and perhaps that Kitchen Quality may be highly influenced by other factors that have a larger bearing on the house price. This is something that could be investigated further. However for now it seems like a good time to start preparing some modelling. Often the feature importances output by models can add a lot of insight to what is discovered during EDA.\n","8e94edb4":"The objective of predicting house prices based on features of the property is a classic regression problem in data science. Here we will conduct some EDA on the housing dataset prepared for Ames, Iowa by Dean De Cock.","f0558669":"Yes it looks like the style of house makes a difference to the price. We can see for example that style \"2.5Fin\" is noticeably more expensive than \"2.5Unf\". Reading the description we find that this means a property of type  \"Two and one-half story: 2nd level finished\" has a higher mean selling price than \"Two and one-half story: 2nd level unfinished\". This is not surprising since in the unfinished property the buyer would have to do some renovation or decoration to make the property ready to be lived in comfortably. We will not treat this as an ordinal. Although \"2.5Unf\" has a higher mean price than \"1.5Unf\" it has a lower mean than \"2Story\". Also some of the labels do not contain a number or fit into an obvious ordinal pattern.","4bab2e89":"### Feature Engineering","af1cb076":"We can see that some neighborhoods had far more property transactions than others. Let's take a look at whether the style of house makes a difference to the sale price.","84a325a0":"### Preparing to run a model\nGet the target then drop that from train before merging with test\n","d2841c67":"### Feature Selection","9d67a6d1":"# Housing Regression","0a400a9c":"Often the price of a house is strongly correlated to the size of the house. Let's see if we can notice that pattern here. For now we will use the GrLivArea column for the non basement living area.","fe67bb7e":"### Prepare the Output\nWrite the prediction results to a submission file ready to submit to the competition. First convert the target back from log(target).","fde3cb5a":"### Let's take a closer look at the training data"}}