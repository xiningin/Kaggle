{"cell_type":{"17209bb1":"code","7858d5ca":"code","c2b57d0e":"code","58f21a3c":"code","c66825b3":"code","30f2145c":"code","246aefba":"code","1cb878ef":"code","d6a5288c":"code","9540b98f":"code","77f53582":"code","35864a49":"code","39ed3ddc":"code","589289aa":"code","92d80105":"code","94100ee2":"code","c443347b":"code","2874ea1b":"code","1e0fde6b":"code","af3240f9":"code","1201537e":"code","c282a922":"code","adab7557":"code","7ab01f08":"code","2ee39897":"code","821bb7b3":"code","27db5096":"code","55d0d701":"code","f9917709":"code","eb3eebc9":"code","087893b7":"code","e406ef1b":"code","293f5870":"code","3247fb00":"code","3bd96b48":"code","42d3d677":"code","4acf9fdd":"code","a732253c":"code","cd5d2962":"code","597b70f4":"code","75b9b9cf":"code","64b736ac":"code","6cdf7987":"code","b59d1003":"code","6ca6e6a8":"code","1de98e51":"code","3cd87a30":"code","b478c5ac":"code","68756d5b":"code","e756e2da":"code","96806c7b":"code","959afd62":"code","2b2cc1eb":"code","d883910c":"code","fa4f6359":"code","7397e979":"code","89012507":"code","d3d45a16":"code","947d097b":"markdown","86cf6572":"markdown","20b168ef":"markdown","fe141a82":"markdown","5f967e2a":"markdown","2b19f823":"markdown","42e2f9f0":"markdown","b86fba71":"markdown","7fd4fecb":"markdown","7d4dc506":"markdown","04fc8cfb":"markdown","eb0b60ba":"markdown","a5bca251":"markdown"},"source":{"17209bb1":"# imports\nimport numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","7858d5ca":"PATH = '..\/input\/movie-review-sentiment-analysis-kernels-only'\nos.listdir(PATH)","c2b57d0e":"!unzip ..\/input\/movie-review-sentiment-analysis-kernels-only\/train.tsv.zip","58f21a3c":"!unzip ..\/input\/movie-review-sentiment-analysis-kernels-only\/test.tsv.zip","c66825b3":"train = pd.read_csv('.\/train.tsv',sep = '\\t')\ntest = pd.read_csv('.\/test.tsv',sep = '\\t')","30f2145c":"sub = pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/sampleSubmission.csv')","246aefba":"train.head()","1cb878ef":"test.head()","d6a5288c":"class_count = train['Sentiment'].value_counts()\nclass_count","9540b98f":"x = np.array(class_count.index)\ny = np.array(class_count.values)\nplt.figure(figsize=(8,5))\nsns.barplot(x,y)\nplt.xlabel('Sentiment ')\nplt.ylabel('Number of reviews ')\n","77f53582":"print('Number of sentences in training set:',len(train['SentenceId'].unique()))\nprint('Number of sentences in test set:',len(test['SentenceId'].unique()))\nprint('Average words per sentence in train:',train.groupby('SentenceId')['Phrase'].count().mean())\nprint('Average words per sentence in test:',test.groupby('SentenceId')['Phrase'].count().mean())","35864a49":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n).generate(str(data))\n\n    fig = plt.figure(1, figsize=(15, 15))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","39ed3ddc":"show_wordcloud(train['Phrase'],'Most Common Words from the whole corpus')","589289aa":"\nshow_wordcloud(train[train['Sentiment'] == 0]['Phrase'],'Negative Reviews')","92d80105":"\nshow_wordcloud(train[train['Sentiment'] == 1]['Phrase'],'Somewhat Negative Reviews')\n","94100ee2":"show_wordcloud(train[train['Sentiment'] == 2]['Phrase'],'Neutral Reviews')","c443347b":"\nshow_wordcloud(train[train['Sentiment'] == 3]['Phrase'],'Somewhat Positive Reviews')","2874ea1b":"\nshow_wordcloud(train[train['Sentiment'] == 4]['Phrase'],'Postive Reviews')","1e0fde6b":"from nltk.tokenize import TweetTokenizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntokenizer = TweetTokenizer()","af3240f9":"vectorizer = TfidfVectorizer(ngram_range=(1, 3), tokenizer=tokenizer.tokenize)\nfull_text = list(train['Phrase'].values) + list(test['Phrase'].values)\nvectorizer.fit(full_text)\ntrain_vectorized = vectorizer.transform(train['Phrase'])\ntest_vectorized = vectorizer.transform(test['Phrase'])","1201537e":"y = train['Sentiment']","c282a922":"from sklearn.model_selection import train_test_split\nx_train , x_val, y_train , y_val = train_test_split(train_vectorized,y,test_size = 0.2)","adab7557":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.multiclass import OneVsRestClassifier","7ab01f08":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score","2ee39897":"lr = LogisticRegression()\novr = OneVsRestClassifier(lr)\novr.fit(x_train,y_train)\nprint(classification_report( ovr.predict(x_val) , y_val))\nprint(accuracy_score( ovr.predict(x_val) , y_val ))","821bb7b3":"svm = LinearSVC()\nsvm.fit(x_train,y_train)\nprint(classification_report( svm.predict(x_val) , y_val))\nprint(accuracy_score( svm.predict(x_val) , y_val ))","27db5096":"estimators = [ ('svm',svm) , ('ovr' , ovr) ]\nclf = VotingClassifier(estimators , voting='hard')\nclf.fit(x_train,y_train)\nprint(classification_report( clf.predict(x_val) , y_val))\nprint(accuracy_score( clf.predict(x_val) , y_val ))","55d0d701":"from keras.utils import to_categorical\ntarget=train.Sentiment.values\ny=to_categorical(target)\ny","f9917709":"max_features = 13000\nmax_words = 50\nbatch_size = 128\nepochs = 3\nnum_classes=5\n","eb3eebc9":"from sklearn.model_selection import train_test_split\nX_train , X_val , Y_train , Y_val = train_test_split(train['Phrase'],y,test_size = 0.20)","087893b7":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense,GRU,LSTM,Embedding\nfrom keras.optimizers import Adam\nfrom keras.layers import SpatialDropout1D,Dropout,Bidirectional,Conv1D,GlobalMaxPooling1D,MaxPooling1D,Flatten\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping","e406ef1b":"tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_val = tokenizer.texts_to_sequences(X_val)\n","293f5870":"X_test = tokenizer.texts_to_sequences(test['Phrase'])\nX_test =pad_sequences(X_test, maxlen=max_words)","3247fb00":"len(X_test)\n","3bd96b48":"X_train =pad_sequences(X_train, maxlen=max_words)\nX_val = pad_sequences(X_val, maxlen=max_words)\nX_test =pad_sequences(X_test, maxlen=max_words)","42d3d677":"model_GRU=Sequential()\nmodel_GRU.add(Embedding(max_features,100,mask_zero=True))\nmodel_GRU.add(GRU(64,dropout=0.4,return_sequences=True))\nmodel_GRU.add(GRU(32,dropout=0.5,return_sequences=False))\nmodel_GRU.add(Dense(num_classes,activation='softmax'))\nmodel_GRU.compile(loss='categorical_crossentropy',optimizer=Adam(lr = 0.001),metrics=['accuracy'])\nmodel_GRU.summary()","4acf9fdd":"%%time\nhistory1=model_GRU.fit(X_train, Y_train, validation_data=(X_val, Y_val),epochs=epochs, batch_size=batch_size, verbose=1)","a732253c":"y_pred1=model_GRU.predict_classes(X_test, verbose=1)\nsub.Sentiment=y_pred1\nsub.to_csv('sub1_GRU.csv',index=False)\nsub.head()","cd5d2962":"model2_GRU=Sequential()\nmodel2_GRU.add(Embedding(max_features,100,mask_zero=True))\nmodel2_GRU.add(GRU(64,dropout=0.4,return_sequences=True))\nmodel2_GRU.add(GRU(32,dropout=0.5,return_sequences=False))\nmodel2_GRU.add(Dense(num_classes,activation='sigmoid'))\nmodel2_GRU.compile(loss='binary_crossentropy',optimizer=Adam(lr = 0.001),metrics=['accuracy'])\nmodel2_GRU.summary()","597b70f4":"%%time\nhistory2=model2_GRU.fit(X_train, Y_train, validation_data=(X_val, Y_val),epochs=epochs, batch_size=batch_size, verbose=1)","75b9b9cf":"y_pred2=model2_GRU.predict_classes(X_test, verbose=1)\nsub.Sentiment=y_pred2\nsub.to_csv('sub2_GRU.csv',index=False)\nsub.head()","64b736ac":"model3_LSTM=Sequential()\nmodel3_LSTM.add(Embedding(max_features,100,mask_zero=True))\nmodel3_LSTM.add(LSTM(64,dropout=0.4,return_sequences=True))\nmodel3_LSTM.add(LSTM(32,dropout=0.5,return_sequences=False))\nmodel3_LSTM.add(Dense(num_classes,activation='sigmoid'))\nmodel3_LSTM.compile(loss='binary_crossentropy',optimizer=Adam(lr = 0.001),metrics=['accuracy'])\nmodel3_LSTM.summary()","6cdf7987":"%%time\nhistory3=model3_LSTM.fit(X_train, Y_train, validation_data=(X_val, Y_val),epochs=epochs, batch_size=batch_size, verbose=1)","b59d1003":"y_pred3=model3_LSTM.predict_classes(X_test, verbose=1)\nsub.Sentiment=y_pred3\nsub.to_csv('sub3_LSTM.csv',index=False)\nsub.head()","6ca6e6a8":"model4_BGRU = Sequential()\nmodel4_BGRU.add(Embedding(max_features, 100, input_length=max_words))\nmodel4_BGRU.add(SpatialDropout1D(0.25))\nmodel4_BGRU.add(Bidirectional(GRU(64,dropout=0.4,return_sequences = True)))\nmodel4_BGRU.add(Bidirectional(GRU(32,dropout=0.5,return_sequences = False)))\nmodel4_BGRU.add(Dense(5, activation='sigmoid'))\nmodel4_BGRU.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel4_BGRU.summary()","1de98e51":"%%time\nhistory4=model4_BGRU.fit(X_train, Y_train, validation_data=(X_val, Y_val),epochs=epochs, batch_size=batch_size, verbose=1)","3cd87a30":"y_pred4=model4_BGRU.predict_classes(X_test, verbose=1)\nsub.Sentiment=y_pred4\nsub.to_csv('sub4_BGRU.csv',index=False)\nsub.head()","b478c5ac":"model5_CNN= Sequential()\nmodel5_CNN.add(Embedding(max_features,100,input_length=max_words))\nmodel5_CNN.add(Dropout(0.2))\nmodel5_CNN.add(Conv1D(64,kernel_size=3,padding='same',activation='relu',strides=1))\nmodel5_CNN.add(GlobalMaxPooling1D())\nmodel5_CNN.add(Dense(128,activation='relu'))\nmodel5_CNN.add(Dropout(0.2))\nmodel5_CNN.add(Dense(num_classes,activation='sigmoid'))\nmodel5_CNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel5_CNN.summary()","68756d5b":"%%time\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n\nhistory5=model5_CNN.fit(X_train, Y_train, validation_data=(X_val, Y_val),epochs=3, batch_size=batch_size, verbose=1,callbacks = [early_stop])","e756e2da":"y_pred5=model5_CNN.predict_classes(X_test, verbose=1)\nsub.Sentiment=y_pred5\nsub.to_csv('sub5_CNN.csv',index=False)\nsub.head()","96806c7b":"model6_CnnGRU= Sequential()\nmodel6_CnnGRU.add(Embedding(max_features,100,input_length=max_words))\nmodel6_CnnGRU.add(Conv1D(64,kernel_size=3,padding='same',activation='relu'))\nmodel6_CnnGRU.add(MaxPooling1D(pool_size=2))\nmodel6_CnnGRU.add(Dropout(0.25))\nmodel6_CnnGRU.add(GRU(128,return_sequences=True))\nmodel6_CnnGRU.add(Dropout(0.3))\nmodel6_CnnGRU.add(Flatten())\nmodel6_CnnGRU.add(Dense(128,activation='relu'))\nmodel6_CnnGRU.add(Dropout(0.5))\nmodel6_CnnGRU.add(Dense(5,activation='sigmoid'))\nmodel6_CnnGRU.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])\nmodel6_CnnGRU.summary()","959afd62":"%%time\nhistory6=model6_CnnGRU.fit(X_train, Y_train, validation_data=(X_val, Y_val),epochs=3, batch_size=batch_size, verbose=1,callbacks=[early_stop])","2b2cc1eb":"y_pred6=model6_CnnGRU.predict_classes(X_test, verbose=1)\nsub.Sentiment=y_pred6\nsub.to_csv('sub6_CnnGRU.csv',index=False)\nsub.head()","d883910c":"model7_GruCNN = Sequential()\nmodel7_GruCNN.add(Embedding(max_features,100,input_length=max_words))\nmodel7_GruCNN.add(Dropout(0.2))\nmodel7_GruCNN.add(Bidirectional(GRU(units=128 , return_sequences=True)))\nmodel7_GruCNN.add(Conv1D(32 , kernel_size=3 , padding='same' , activation='relu'))\nmodel7_GruCNN.add(GlobalMaxPooling1D())\nmodel7_GruCNN.add(Dense(units = 64 , activation='relu'))\nmodel7_GruCNN.add(Dropout(0.5))\nmodel7_GruCNN.add(Dense(units=5,activation='sigmoid'))\nmodel7_GruCNN.compile(loss='binary_crossentropy' , optimizer = 'adam' , metrics=['accuracy'])\nmodel7_GruCNN.summary()","fa4f6359":"%%time\nhistory7 = model7_GruCNN.fit(X_train, Y_train, validation_data=(X_val, Y_val),epochs=4, batch_size=batch_size, verbose=1,callbacks=[early_stop])","7397e979":"y_pred7=model7_GruCNN.predict_classes(X_test, verbose=1)\nsub.Sentiment=y_pred7\nsub.to_csv('sub7_GruCNN.csv',index=False)\nsub.head()","89012507":"sub_all=pd.DataFrame({'model1':y_pred1,'model2':y_pred2,'model3':y_pred3,'model4':y_pred4,'model5':y_pred5,'model6':y_pred6,'model7':y_pred7})\npred_mode=sub_all.agg('mode',axis=1)[0].values\nsub_all.head()","d3d45a16":"pred_mode=[int(i) for i in pred_mode]\nsub.Sentiment=pred_mode\nsub.to_csv('ensemble_mode.csv',index=False)\nsub.head()","947d097b":"<a id='en'><\/a>\n## <center>Ensembling all the predictions<\/center>","86cf6572":"### Training Logistic Regression model and an SVM.","20b168ef":"<a id='bgru'><\/a>\n## <center>Bidirectional-GRU<\/center>","fe141a82":"<a id='cgru'><\/a>\n## <center>CNN-GRU<\/center>","5f967e2a":"<a id='gru'><\/a>\n## <center>GRU<\/center>","2b19f823":"<a id='eda'><\/a>\n## <center>EDA<\/center>\nBasic exploration of data to check labels , the number of phrases for each label and average phrase length in the each sentiment.","42e2f9f0":"<a id='gruc'><\/a>\n## <center>GRU-CNN<\/center>","b86fba71":"<a id='N-Grams'><\/a>\n##  <center>1.N-Grams<\/center>","7fd4fecb":"<a id='lstm'><\/a>\n## <center>LSTM<\/center>","7d4dc506":"<a id='cnn'><\/a>\n## <center>CNN<\/center>","04fc8cfb":"<a id='ml'><\/a>\n## <center>  Different Machine Learning Models <\/center>\n","eb0b60ba":"### The sentiment labels are:\n\n0 - negative\n1 - somewhat negative\n2 - neutral\n3 - somewhat positive\n4 - positive","a5bca251":"### Using Word Clouds to see the higher fequency words from each sentiment"}}