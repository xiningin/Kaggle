{"cell_type":{"94f0c21d":"code","7ae03289":"code","e5fb167a":"code","598f06ef":"code","e0161ffa":"code","bb7fb25b":"code","91eff707":"code","501a29d0":"code","5adbf189":"code","0f5d6dca":"code","23a518dd":"code","f292d105":"code","27e42240":"code","53a09459":"code","9942e79c":"code","e7f46ed7":"code","442649e9":"code","b55e1487":"code","27b0ed48":"code","c786f713":"code","46bce4f5":"code","1607f69a":"code","1a64b2ee":"code","16b53782":"code","624ffee2":"code","9744e23b":"code","3df6dea9":"code","1d37e8ff":"code","889289e8":"code","e8d8329d":"code","a3bda9bb":"code","ec9a5446":"code","ea7fc75c":"code","91adefe7":"code","32bf9d07":"code","140abe0c":"code","cf23fea3":"code","07c130ac":"code","4f8f9c5c":"code","a2a9ae10":"code","a6689d22":"code","4c9af652":"code","0669e647":"code","cdb95331":"code","4eeae4c3":"code","916551cf":"code","c8c55913":"code","cd6aab52":"code","f6a74400":"code","2d2e9104":"code","96fc6aa5":"code","82404496":"code","749384bd":"code","0631452b":"code","11194a83":"code","aba4aac7":"code","329cb769":"code","3d5c4592":"code","5b1af54a":"code","35abdd1d":"code","fbb9098e":"code","2b9aa039":"code","88060178":"code","03ea89c4":"code","0e79674e":"markdown","533dd927":"markdown","a3457b51":"markdown","bc7aa1ab":"markdown","c569ea04":"markdown","a4df86bb":"markdown","c7f9799c":"markdown","05578877":"markdown","c12b833b":"markdown","e6414eab":"markdown","18949e28":"markdown","902c2fd0":"markdown","a35aff64":"markdown","712b1d6e":"markdown","20fb6596":"markdown"},"source":{"94f0c21d":"import warnings\nwarnings.simplefilter('ignore')\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageFilter, ImageOps\nimport seaborn as sns\nimport matplotlib.pylab as plt\nimport japanize_matplotlib\nfrom tqdm import tqdm\nimport time\nfrom contextlib import contextmanager\nimport gc\nimport os\n\nfrom tqdm.auto import tqdm as tqdmp\nfrom tqdm.autonotebook import tqdm as tqdm\ntqdmp.pandas()\n\nimport requests\nimport io\npd.set_option('display.max_rows', 200)","7ae03289":"exp_dir = '014_add_ev_agg_cv_change'\nOUTPUT_DIR = f'\/content\/drive\/MyDrive\/DATA\/interim\/{exp_dir}\/'\nDATA_DIR = '\/content\/drive\/MyDrive\/DATA\/'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(DATA_DIR, exist_ok=True)","e5fb167a":"@contextmanager\ndef timer(name:str):\n    t0 = time.time()\n    print(f'<< {name} >> Start')\n    yield\n    message = f'<< {name} >> done in {time.time()-t0:.1f}s.'\n    print(message)","598f06ef":"def seed_everything(seed:int==255):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True","e0161ffa":"SEED = 255\nNFOLDS = 5","bb7fb25b":"lgb_params = {\n    'objective': 'root_mean_squared_error',\n    'metric':'rmse',\n    'boosting_type': 'gbdt',\n    'force_col_wise': True,\n    'random_state': SEED,\n    'borbose': -1\n}","91eff707":"train_df = pd.read_csv(\"..\/input\/shigglecup-1st\/DATA\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/shigglecup-1st\/DATA\/test.csv\")\n\n###target\u3092\u5bfe\u6570\u5909\u63db\ntrain_df['target'] = np.log1p(train_df['target'].astype('float'))\n\n##train_flag\u3092\u4ed8\u3051\u308b\u3002\ntrain_df[\"train_id\"] = 1\ntest_df[\"train_id\"] = 0\nall_df = pd.concat([train_df,test_df]).reset_index(drop=True)\nprint(train_df.shape)\nprint(test_df.shape)\nprint(all_df.shape)\ndrop_cols = [\"pokemon\",\"species_id\",\"url_image\",\"image_exist\",'shape','color_1','color_2','color_f','evolves_from_species_id']","501a29d0":"\n##\u5bfe\u6570\u5909\u63db\nall_df['height'] = all_df['height'].progress_apply(lambda x: np.log1p(x))\nall_df['weight'] = all_df['weight'].progress_apply(lambda x: np.log1p(x))\n\nall_df['hw_x'] = all_df['height']*all_df['weight']\nall_df['hw_d'] = all_df['height']*all_df['weight']","5adbf189":"#\u5408\u8a08\u7a2e\u65cf\u5024\nall_df['habcds'] = all_df['attack'] + all_df['defense'] + all_df['special_attack'] + all_df['special_defense'] + all_df['speed'] + all_df['hp']\nall_df['habcds_avg'] = all_df['habcds'] \/ 6","0f5d6dca":"def sort_strength(input_df,col):\n  agg_df = input_df[[col]].drop_duplicates(col).sort_values(col).reset_index(drop=True).reset_index().rename(columns={'index':f'{col}_rank'})\n  agg_df.loc[:,f'{col}_rank'] = agg_df[f'{col}_rank'] + 1\n  input_df = input_df.merge(agg_df,on=col,how='left')\n  return input_df.copy()","23a518dd":"sum_strength_list = np.zeros(len(all_df))\nfor col in ['attack','defense','special_attack','special_defense','speed','hp']:\n  all_df = sort_strength(all_df,col)\n  sum_strength_list += all_df[f'{col}_rank']\n##\u30e9\u30f3\u30af\u306e\u5408\u8a08\nall_df['sum_strength'] = sum_strength_list\n##\u30e9\u30f3\u30af\u306e\u5e73\u5747\nall_df['avg_strength'] = sum_strength_list\/6","f292d105":"all_df[all_df['hp_rank']==all_df['hp_rank'].max()]","27e42240":"##habsds\u306e\u4e2d\u3067\u6700\u9ad8\u7a2e\u65cf\u5024\u3092\u62bd\u51fa\nall_df[\"strength_max\"] = all_df[\"hp\"]\nall_df.loc[all_df[\"attack\"]>all_df['strength_max'],'strength_max'] = all_df['attack']\nall_df.loc[all_df[\"defense\"]>all_df['strength_max'],'strength_max'] = all_df['defense']\nall_df.loc[all_df[\"special_attack\"]>all_df['strength_max'],'strength_max'] = all_df['special_attack']\nall_df.loc[all_df[\"special_defense\"]>all_df['strength_max'],'strength_max'] = all_df['special_defense']\nall_df.loc[all_df[\"speed\"]>all_df['strength_max'],'strength_max'] = all_df['speed']\n\nall_df[\"strength_min\"] = all_df[\"hp\"]\nall_df.loc[all_df[\"attack\"]<all_df['strength_min'],'strength_min'] = all_df['attack']\nall_df.loc[all_df[\"defense\"]<all_df['strength_min'],'strength_min'] = all_df['defense']\nall_df.loc[all_df[\"special_attack\"]<all_df['strength_min'],'strength_min'] = all_df['special_attack']\nall_df.loc[all_df[\"special_defense\"]<all_df['strength_min'],'strength_min'] = all_df['special_defense']\nall_df.loc[all_df[\"speed\"]<all_df['strength_min'],'strength_min'] = all_df['speed']","53a09459":"def agg_by_col(input_df,col_list,agg_list,target):\n  all_list = col_list + [target]\n  output_df = input_df[all_list].groupby(col_list).agg(agg_list).reset_index()\n  output_df.columns = output_df.columns.droplevel(0)\n  \n  for i,col in enumerate(col_list):\n    if i == 0:\n      all_col = target + '_' +col\n      continue\n    all_col = f'{all_col}_{col}'\n  output_df = output_df.add_prefix(f'{all_col}_')\n  new_col_list = col_list + output_df.columns[len(col_list):].tolist()\n  output_df.columns = new_col_list\n  \n  return output_df\nagg_list = ['count','sum','mean','median','std','max','min','skew','quantile']","9942e79c":"all_df['habcds_bin'] = 0\nzoku_list = [50,100,150,200,250,300,350,400,450,500,550,570,580,600,660,670,680,690,700,720,780]\nzoku_list.reverse()\nfor bin in zoku_list:\n  all_df.loc[all_df['habcds']<=bin,'habcds_bin'] = bin","e7f46ed7":"all_df = all_df.merge(agg_by_col(all_df,['habcds_bin'],agg_list,'target'),on=['habcds_bin'],how='left')","442649e9":"def concat_str(input_df,str_list,col_name):\n  tmp_str_df = input_df[str_list].reset_index()\n  tmp_str_df = tmp_str_df.rename(columns={'index':'sort_id'})\n  str_df = pd.DataFrame()\n  for col in str_list:\n    str_df = pd.concat([str_df,tmp_str_df.rename(columns={col:col_name})[['sort_id',col_name]].fillna('NaN')],axis=0).reset_index(drop=True)\n  str_df = str_df.groupby('sort_id')[col_name].apply(list)\n  return str_df","b55e1487":"##\u5217\u5168\u90e8\nstr_list = ['type_1','type_2','ability_1','ability_2','ability_hidden','egg_group_1','egg_group_2','shape']\nall_df = all_df.merge(concat_str(all_df,str_list,\"all_str\"),right_index=True,left_index=True)\nstr_list = ['ability_1','ability_2','ability_hidden','egg_group_1','egg_group_2']\nall_df = all_df.merge(concat_str(all_df,str_list,\"ability_egg\"),right_index=True,left_index=True)","27b0ed48":"all_df.reset_index(inplace=True)\nall_df = all_df.rename(columns={'index':'sort_id'})","c786f713":"# \u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u8868\u73fe\u306e\u6b21\u5143\u6570\n# \u5143\u306e\u8a9e\u5f59\u6570\u3092\u30d9\u30fc\u30b9\u306b\u9069\u5f53\u306b\u6c7a\u3081\u307e\u3057\u305f\nfrom gensim.models import word2vec, KeyedVectors\nsize = 20\nn_iter = 100","46bce4f5":"tqdm.pandas()","1607f69a":"w2v_dfs = []\nfor col in [\"all_str\",\"ability_egg\"]:\n  w2v_model = word2vec.Word2Vec(all_df[col].values.tolist(),\n                              vector_size=size,\n                              min_count=1,\n                              window=5,\n                              epochs=n_iter)\n\n  sentence_vectors = all_df[col].progress_apply(\n      lambda x: np.mean([w2v_model.wv[e] for e in x], axis=0))\n  sentence_vectors = np.vstack([x for x in sentence_vectors])\n  sentence_vector_df = pd.DataFrame(sentence_vectors,\n                                  columns = [f\"{col}_w2v_{i}\"\n                                              for i in range(size)])\n  sentence_vector_df.index = all_df[\"sort_id\"]\n  w2v_dfs.append(sentence_vector_df)","1a64b2ee":"all_df.set_index('sort_id',inplace=True)\nfor i in range(len(w2v_dfs)):\n  all_df = pd.concat([all_df,w2v_dfs[i]],axis=1)\nall_df = all_df.drop([\"all_str\",\"ability_egg\"],axis=1)","16b53782":"def diff_agg_habcds(input_df,col_list):\n  for col in col_list:\n    for agg in ['mean','max','min']:\n      target_col = f'habcds_{col}_{agg}'\n      input_df[f'{target_col}_diff'] = input_df['habcds'] - input_df[target_col]\n  return input_df.copy()","624ffee2":"all_df = all_df.merge(agg_by_col(all_df,['type_1'],agg_list,'target'),on=['type_1'],how='left')\nall_df = all_df.merge(agg_by_col(all_df,['type_2'],agg_list,'target'),on=['type_2'],how='left')\nall_df = all_df.merge(agg_by_col(all_df,['type_1','type_2'],agg_list,'target'),on=['type_1','type_2'],how='left')\n\nall_df = all_df.merge(agg_by_col(all_df,['type_1'],agg_list,'habcds'),on=['type_1'],how='left')\nall_df = all_df.merge(agg_by_col(all_df,['type_2'],agg_list,'habcds'),on=['type_2'],how='left')\nall_df = all_df.merge(agg_by_col(all_df,['type_1','type_2'],agg_list,'habcds'),on=['type_1','type_2'],how='left')\n\nall_df = diff_agg_habcds(all_df,['type_1','type_2','type_1_type_2'])","9744e23b":"###\u30bf\u30a4\u30d7\u6570\u306e\u30ab\u30a6\u30f3\u30c8\ndef count_type(df:pd.DataFrame) -> pd.DataFrame:\n    \n    _df = df.copy()\n    # type_2\u304cnan\u3067\u3042\u308c\u3070\u3001\u30bf\u30a4\u30d7\u6570\u306f1\u3068\u3059\u308b\n    _df['num_type'] = _df['type_2'].progress_apply(lambda x: 1 if x is np.nan else 2)\n\n    return _df\n\nall_df = count_type(all_df)","3df6dea9":"\nall_df = all_df.merge(agg_by_col(all_df,['egg_group_1'],agg_list,'target'),on=['egg_group_1'],how='left')\nall_df = all_df.merge(agg_by_col(all_df,['egg_group_2'],agg_list,'target'),on=['egg_group_2'],how='left')\nall_df = all_df.merge(agg_by_col(all_df,['egg_group_1','egg_group_2'],agg_list,'target'),on=['egg_group_1','egg_group_2'],how='left')\n\nall_df = all_df.merge(agg_by_col(all_df,['egg_group_1'],agg_list,'habcds'),on=['egg_group_1'],how='left')\nall_df = all_df.merge(agg_by_col(all_df,['egg_group_2'],agg_list,'habcds'),on=['egg_group_2'],how='left')\nall_df = all_df.merge(agg_by_col(all_df,['egg_group_1','egg_group_2'],agg_list,'habcds'),on=['egg_group_1','egg_group_2'],how='left')","1d37e8ff":"def count_egg_group(df:pd.DataFrame) -> pd.DataFrame:\n    \n    _df = df.copy()\n    # type_2\u304cnan\u3067\u3042\u308c\u3070\u3001\u30bf\u30a4\u30d7\u6570\u306f1\u3068\u3059\u308b\n    _df['num_egg_group'] = _df['egg_group_2'].progress_apply(lambda x: 1 if x is np.nan else 2)\n\n    return _df\n  \nall_df = count_egg_group(all_df)","889289e8":"##Count Encoding\ndef trans_CE(input_df,col):\n  output_df = pd.DataFrame()\n  vc = input_df[col].value_counts()\n  output_df[col] = input_df[col].map(vc)\n  return output_df.add_prefix('CE_')","e8d8329d":"CE_list = ['ability_1','ability_2','ability_hidden']\n\nfor col in tqdm(CE_list):\n  all_df = pd.concat([all_df,trans_CE(all_df,col)],axis=1)","a3bda9bb":"all_df = all_df.merge(agg_by_col(all_df,['CE_ability_1'],agg_list,'target'),on=['CE_ability_1'],how='left')\nall_df = all_df.merge(agg_by_col(all_df,['CE_ability_2'],agg_list,'target'),on=['CE_ability_2'],how='left')\nall_df = all_df.merge(agg_by_col(all_df,['CE_ability_hidden'],agg_list,'target'),on=['CE_ability_hidden'],how='left')\n\nall_df = all_df.merge(agg_by_col(all_df,['CE_ability_1'],agg_list,'habcds'),on=['CE_ability_1'],how='left')\nall_df = all_df.merge(agg_by_col(all_df,['CE_ability_2'],agg_list,'habcds'),on=['CE_ability_2'],how='left')\nall_df = all_df.merge(agg_by_col(all_df,['CE_ability_hidden'],agg_list,'habcds'),on=['CE_ability_hidden'],how='left')\n\nall_df = diff_agg_habcds(all_df,['CE_ability_1','CE_ability_2','CE_ability_hidden'])","ec9a5446":"def count_ability(df:pd.DataFrame) -> pd.DataFrame:\n    \n    _df = df.copy()\n    # type_2\u304cnan\u3067\u3042\u308c\u3070\u3001\u30bf\u30a4\u30d7\u6570\u306f1\u3068\u3059\u308b\n    _df['num_ability'] = _df['ability_2'].progress_apply(lambda x: 1 if x is np.nan else 2)\n\n    return _df\n  \nall_df = count_ability(all_df)","ea7fc75c":"#\u25cf\u25cf\u306e\u59ff\u3084\u30e1\u30ac\u30b7\u30f3\u30ab\u306f\u4e16\u4ee3id\u304c\u632f\u308a\u5206\u3051\u3089\u308c\u3066\u3044\u306a\u3044\u305f\u3081\u3001species_id\u304b\u3089generation_id\u3092\u4ed8\u4e0e\u3059\u308b\u3002\nall_df.loc[all_df['pokemon'].str.contains('-mega'),'mega_flg'] = 1\nall_df.loc[all_df['pokemon'].str.contains('-primal'),'mega_flg'] = 1\n\ngen_df = all_df[['species_id','generation_id','habcds','target']].dropna().rename(columns={'target':'normal_target','habcds':'normal_habcds'})\n\nbef = len(all_df)\nall_df = all_df.merge(gen_df.drop('generation_id',axis=1),on='species_id',how='left')\nassert len(all_df) == bef\n\nall_df.loc[all_df['mega_flg'].isna(),'normal_target'] = np.nan\nall_df.loc[all_df['mega_flg'].isna(),'normal_habcds'] = np.nan\nall_df.loc[all_df['mega_flg'].isna(),'normal_habcds_diff'] = all_df['habcds'] - all_df['normal_habcds']","91adefe7":"evolve_df =  all_df[['species_id','generation_id','habcds','target']].dropna().rename(columns={'target':'pre_target','species_id':'evolves_from_species_id','habcds':'pre_habcds'})\nbef = len(all_df)\nall_df = all_df.merge(evolve_df.drop('generation_id',axis=1),on='evolves_from_species_id',how='left')\nassert len(all_df) == bef","32bf9d07":"all_df = all_df.merge(agg_by_col(all_df,['mega_flg'],agg_list,'target'),on=['mega_flg'],how='left')\nall_df = all_df.merge(agg_by_col(all_df,['mega_flg'],agg_list,'habcds'),on=['mega_flg'],how='left')\nall_df['pre_habcds_diff'] = all_df['habcds'] - all_df['pre_habcds']\nall_df = diff_agg_habcds(all_df,['mega_flg'])","140abe0c":"all_df.loc[all_df['pokemon'].str.contains('-'),'tf_flg'] = 1\nall_df.loc[all_df['mega_flg']==1,'tf_flg'] = np.nan\n\ngen_df = all_df[['species_id','generation_id','habcds','target']].dropna().rename(columns={'target':'tf_target','habcds':'tf_habcds'})\nbef = len(all_df)\nall_df = all_df.merge(gen_df.drop('generation_id',axis=1),on='species_id',how='left')\nassert len(all_df) == bef\nall_df.loc[all_df['tf_flg'].isna(),'tf_target'] = np.nan\nall_df.loc[all_df['tf_flg'].isna(),'tf_habcds'] = np.nan\nall_df.loc[all_df['tf_flg'].isna(),'tf_habcds_diff'] = all_df['habcds'] - all_df['tf_habcds']","cf23fea3":"all_df = all_df.merge(agg_by_col(all_df,['color_1'],agg_list,'target'),on=['color_1'],how='left')\nall_df = all_df.merge(agg_by_col(all_df,['color_2'],agg_list,'target'),on=['color_2'],how='left')","07c130ac":"all_df = all_df.merge(agg_by_col(all_df,['shape'],agg_list,'target'),on=['shape'],how='left')\n\nall_df = all_df.merge(agg_by_col(all_df,['shape'],agg_list,'habcds'),on=['shape'],how='left')\n\nall_df = diff_agg_habcds(all_df,['shape'])","4f8f9c5c":"##Label Encoding\ndef trans_LE(input_df,col):\n  output_df = pd.DataFrame()\n  _dict = {j:i for i,j in enumerate(input_df[col].unique())}\n  output_df[col] = input_df[col].map(_dict)\n  return output_df.add_prefix('LE_')","a2a9ae10":"LE_list = ['type_1','type_2','ability_1','ability_2','ability_hidden','egg_group_1','egg_group_2']\n\nfor col in tqdm(LE_list):\n  all_df = pd.concat([all_df,trans_LE(all_df,col)],axis=1)","a6689d22":"ev_df = all_df[['species_id','evolves_from_species_id']].dropna()\ntmp = ev_df.rename(columns={'evolves_from_species_id':'second','species_id':'third'})\nev_df = ev_df.rename(columns={'species_id':'second','evolves_from_species_id':'first'}).merge(tmp,on='second',how='left')\nev_df = ev_df[['first','second','third']].sort_values('first').reset_index(drop=True)","4c9af652":"not_first_list = all_df[~all_df['evolves_from_species_id'].isna()]['species_id'].unique()\nev_df = ev_df.loc[~ev_df['first'].astype(int).isin(not_first_list),:]\nev_df = ev_df.fillna(999).astype(int)\ntmp_all_df = all_df.merge(ev_df[['first']],left_on='species_id',right_on='first',how='left')\ntmp_all_df = tmp_all_df.merge(ev_df[['second']],left_on='species_id',right_on='second',how='left')\ntmp_all_df = tmp_all_df.merge(ev_df[['third']],left_on='species_id',right_on='third',how='left')","0669e647":"tmp_all_df.loc[~tmp_all_df['first'].isna(),'ev_status'] = 1\ntmp_all_df.loc[~tmp_all_df['second'].isna(),'ev_status'] = 2\ntmp_all_df.loc[~tmp_all_df['third'].isna(),'ev_status'] = 3\ntmp_all_df.loc[tmp_all_df['pokemon'].str.contains('-mega'),'ev_status'] = 4\ntmp_all_df.loc[tmp_all_df['pokemon'].str.contains('-primal'),'ev_status'] = 4\ntmp_all_df['ev_status'] = tmp_all_df['ev_status'].fillna(1)\ntmp_all_df = tmp_all_df[['pokemon','species_id','ev_status']].drop_duplicates(['pokemon','species_id']).sort_values('species_id')","cdb95331":"all_df = all_df.merge(tmp_all_df,on=['pokemon','species_id'],how='left')","4eeae4c3":"##\u4f55\u6bb5\u968e\u9032\u5316\u304b\nagg_df = all_df[['evolution_chain_id','ev_status']].groupby('evolution_chain_id').max().reset_index()\nall_df = all_df.merge(agg_df.rename(columns={'ev_status':'ev_nums'}),on='evolution_chain_id',how='left')\nall_df.loc[all_df['pokemon'].str.contains('-mega'),'ev_nums'] = 4\nall_df.loc[all_df['pokemon'].str.contains('-primal'),'ev_nums'] = 4\nall_df['ev_diff'] = all_df['ev_nums'] - all_df['ev_status']","916551cf":"all_df = all_df.merge(agg_by_col(all_df,['ev_status'],agg_list,'target'),on=['ev_status'],how='left')\nall_df = all_df.merge(agg_by_col(all_df,['ev_nums'],agg_list,'target'),on=['ev_nums'],how='left')\nall_df = all_df.merge(agg_by_col(all_df,['ev_diff'],agg_list,'target'),on=['ev_diff'],how='left')\n\nall_df = all_df.merge(agg_by_col(all_df,['ev_status'],agg_list,'habcds'),on=['ev_status'],how='left')\nall_df = all_df.merge(agg_by_col(all_df,['ev_nums'],agg_list,'habcds'),on=['ev_nums'],how='left')\nall_df = all_df.merge(agg_by_col(all_df,['ev_diff'],agg_list,'habcds'),on=['ev_diff'],how='left')\nall_df = diff_agg_habcds(all_df,['ev_status','ev_nums','ev_diff'])","c8c55913":"#\u25cf\u25cf\u306e\u59ff\u3084\u30e1\u30ac\u30b7\u30f3\u30ab\u306f\u4e16\u4ee3id\u304c\u632f\u308a\u5206\u3051\u3089\u308c\u3066\u3044\u306a\u3044\u305f\u3081\u3001species_id\u304b\u3089generation_id\u3092\u4ed8\u4e0e\u3059\u308b\u3002\ngen_df = all_df[['species_id','generation_id']].dropna()\nlen(gen_df)","cd6aab52":"bef = len(all_df)\nall_df = all_df.drop('generation_id',axis=1).merge(gen_df,on='species_id',how='left')\nassert len(all_df) == bef","f6a74400":"### \u30ab\u30e9\u30e0\u3092\u524a\u9664\nall_df = all_df.drop(LE_list+drop_cols,axis=1)\nall_df = all_df.sort_values(['train_id','id']).set_index('id')\n\n##train,test\u306b\u5206\u3051\u308b\ntrain_X = all_df[all_df['train_id']==1].drop(['train_id','target'],axis=1)\ntrain_y = all_df[all_df['train_id']==1]['target']\ntest  = all_df[all_df['train_id']==0].drop(['train_id','target','generation_id'],axis=1)\nprint(f'train:{train_X.shape},test:{test.shape}')","2d2e9104":"##\u4e16\u4ee3id\u3054\u3068\u306bKFold\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n\nlgb_oof = np.zeros(len(train_y))\nlgb_pred = np.zeros(len(test))\nall_rmse = []\nimportances_all = pd.DataFrame()\n\nskf = KFold(n_splits=NFOLDS,shuffle=True,random_state=SEED)\nfor fold, (tr_idx, va_idx) in enumerate(skf.split(train_X, train_y)):\n\n  print('\u2605'*40)\n  print(f'Fold: {fold+1}')\n\n  start_time = time.time()\n\n  X_tr = train_X.iloc[tr_idx,:].drop('generation_id',axis=1)\n  y_tr = train_y.iloc[tr_idx]\n  X_te = train_X.iloc[va_idx,:].drop('generation_id',axis=1)\n  y_te = train_y.iloc[va_idx]\n\n  lgb_train = lgb.Dataset(X_tr, y_tr)\n  lgb_eval = lgb.Dataset(X_te, y_te)\n\n  lgb_model = lgb.train(\n      lgb_params,\n      lgb_train,\n      valid_sets=[lgb_train, lgb_eval],\n      verbose_eval=0,\n      num_boost_round=1000,\n      early_stopping_rounds=20,\n  )\n  \n  lgb_oof[va_idx] = lgb_model.predict(X_te, num_iteration=lgb_model.best_iteration)\n  lgb_pred += lgb_model.predict(test[test.select_dtypes(exclude='object').columns].values, \n                                num_iteration=lgb_model.best_iteration)\/NFOLDS\n  \n  importances = pd.DataFrame()\n  importances['feature'] = train_X.drop('generation_id',axis=1).columns.tolist()\n  importances['lgb_gain'] = lgb_model.feature_importance()\n  importances['fold'] = fold+1\n  importances_all = pd.concat([importances_all, importances], axis=0, sort=False)\n  \n  rmse = np.sqrt(mean_squared_error(y_te, lgb_oof[va_idx]))\n  all_rmse.append(rmse)\n  end_time = time.time()\n  \n  \n  print(f'Fold: {fold+1} | RMLSE: {rmse:.5f} | Time: {end_time-start_time:.1f}s.')\n  #message_to_slack(f'Fold: {fold+1} | RMLSE: {rmse:.5f} | Iteration: {lgb_model.best_iteration} | Time: {end_time-start_time:.1f}s.')\n  \n  #joblib.dump(lgb_model, f'{OUTPUT_DIR}lgb_baseline_{fold+1}.pkl')\n  \n  #del lgb_model, X_tr, X_te, y_tr, y_te\n  _ = gc.collect()\n\nprint(f'OOF:{np.mean(all_rmse)}')\n","96fc6aa5":"plt.figure(figsize=(10, 10))\nsns.boxplot(data=importances_all.sort_values('lgb_gain', ascending=False)[:100],\n            y='feature', x='lgb_gain',\n            orient='h')\nplt.title(\"\u7279\u5fb4\u91cf\u91cd\u8981\u5ea6 LightGBM\")\nplt.show()","82404496":"fig, ax = plt.subplots(figsize=(8, 8))\nsns.distplot(train_y, label='\u6b63\u89e3\u5024')\nsns.distplot(lgb_oof, label='Out Of Fold')\nsns.distplot(lgb_pred, label='Predict')\nax.legend()\nax.grid()","749384bd":"pd.DataFrame(np.expm1(lgb_pred)).describe()","0631452b":"lgb_y_preds_res = np.expm1(lgb_pred)\nsub_df = pd.concat([pd.DataFrame({ 'id': test_df['id'] }), pd.DataFrame({ 'target': lgb_y_preds_res })], axis=1)\nsub_df","11194a83":"sub_df.to_csv(f'{OUTPUT_DIR}{exp_dir}.csv', index=False)","aba4aac7":"!pip install shap -Uq","329cb769":"import shap\n\nshap.initjs()\n\"\"\"\nshap.TreeExamplainer :\u6c7a\u5b9a\u6728\u7528(XGBoost\u3001lightBGM\u7b49\u542b\u3080)\nshap.LinearExplainer :\u7dda\u5f62\u30e2\u30c7\u30eb\u7528\nshap.DeepExplainer :Deeplearning\u7528\n\"\"\"\n#TreeExplainer\u306f\u3001\u6c7a\u5b9a\u6728\u7cfb\u306e\u30e2\u30c7\u30eb\u306eSHAP\u5024\u3092\u53d6\u5f97\u3059\u308b\u3082\u306e\u3002\nexplainer = shap.TreeExplainer(model=lgb_model)\n\nprint(explainer.expected_value)","3d5c4592":"X_test_shap = test.copy()\nprint(X_test_shap.shape)","5b1af54a":"shap_values = explainer.shap_values(X=X_test_shap)\nprint(X_test_shap.shape)\nprint(shap_values.shape)\nprint(shap_values[0])#\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e0\u756a\u76ee\u306e\u8981\u7d20\u3092\u51fa\u529b","35abdd1d":"shap.summary_plot(shap_values, X_test_shap,) #\u5de6\u5074\u306e\u56f3\nshap.summary_plot(shap_values, X_test_shap, plot_type='bar') #\u53f3\u5074\u306e\u56f3","fbb9098e":"# n = #\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306en\u756a\u76ee\u306e\u8981\u7d20\u3092\u6307\u5b9a\nn = 243\n#waterfall_plot\u306f\u79c1\u306e\u74b0\u5883\u3067\u306f\u30a8\u30e9\u30fc\u306b\u306a\u308b\u306e\u3067\u3001\u4ee3\u308f\u308a\u306bwaterfall_legacy\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\u3002\nshap.plots._waterfall.waterfall_legacy(explainer.expected_value, \n                                       shap_values[n,:], X_test_shap.iloc[n,:]) #\u4e0b\u306e\u56f3","2b9aa039":"n = 243\nprint(lgb_model.predict(X_test_shap)[n])\nprint(np.expm1(lgb_model.predict(X_test_shap)[n]))","88060178":"shap.decision_plot(explainer.expected_value, shap_values,X_test_shap)","03ea89c4":"shap.dependence_plot(\"habcds\", shap_values, X_test_shap)","0e79674e":"### egg_group\n* \u30bf\u30de\u30b4\u30b0\u30eb\u30fc\u30d7\u3002\u7279\u6027\u3068\u540c\u3058\u3088\u3046\u306a\u611f\u3058\u3002\u30bf\u30de\u30b4\u672a\u767a\u898b\u306f\u4f1d\u8aac\u3002\n\u21d2\u30bf\u30a4\u30d7\u3068\u7279\u6027\u3068\u3067w2v\u3002\u30bf\u30de\u30b4\u30b0\u30eb\u30fc\u30d7\u3054\u3068\u306e\u7d4c\u9a13\u5024\u5e73\u5747\u53d6\u308b","533dd927":"### ability\n* CE\u3057\u3066\u304b\u3089\u306e\u65b9\u304c\u826f\u3055\u305d\u3046\uff1f","a3457b51":"## SHAP","bc7aa1ab":"* type_1,type_2,ability_1,ability_1,ability_hidden,egg_group_1,egg_group_2,shape\u3092\u5168\u3066\u3064\u306a\u3052\u308b","c569ea04":"### shape","a4df86bb":"* \u5408\u8a08\u7a2e\u65cf\u5024\u3092\u30d3\u30f3\u306b\u5206\u3051\u3066\u3001\u7d71\u8a08\u91cf\u3092\u7b97\u51fa","c7f9799c":"### height,weight\n* height: \u9ad8\u3055\u3002\u305d\u306e\u307e\u307e\u4f7f\u3046\u3002\n* weight: \u91cd\u3055\u3002\u305d\u306e\u307e\u307e\u4f7f\u3046\u3002\n\u21d2h*w\u3068\u304b","05578877":"### evolves from species id\n* \u4f55\u6bb5\u968e\u9032\u5316\u306a\u306e\u304b-evolves from\u3092\u4f7f\u3063\u3066\u96c6\u8a08\u3059\u308b\n* \u4f55\u6bb5\u968e\u76ee\u306e\u9032\u5316\u306a\u306e\u304b - chain id \u4f7f\u3046\n* \u5dee\u5206\u3092\u53d6\u308b","c12b833b":"### Label Encoding","e6414eab":"### LightGBM","18949e28":"### Modeling","902c2fd0":"### type","a35aff64":"### attack,defense,hp,special_attack,special_defense,speed\n* \u7a2e\u65cf\u5024\u3002\u5408\u8a08\u3068\u304b\u30d4\u30dc\u30c3\u30c8\u3068\u304b\u3044\u308d\u3044\u308d\u3067\u304d\u305d\u3046 \u21d2\u7a2e\u65cf\u5024\u306e\u5408\u8a08\u3002\u7a2e\u65cf\u5024\u6bce\u306e\u30e9\u30f3\u30ad\u30f3\u30b0\u3092\u96c6\u8a08\u3002\u30e9\u30f3\u30ad\u30f3\u30b0\u306e\u5408\u8a08\u3092\u3068\u308b\u3002\u6700\u9ad8\u7a2e\u65cf\u5024","712b1d6e":"### color","20fb6596":"### pokemon"}}