{"cell_type":{"4141435d":"code","8792e615":"code","cd96074c":"code","df26beb7":"code","1e51b303":"code","41c45864":"code","46315ba0":"code","41c43045":"code","d80013c5":"code","ecfbd906":"code","c80d289c":"code","e39777fb":"code","bf9690c2":"code","6fcfc78a":"code","c4b43b31":"code","1e7c3ce5":"code","2f0ecc4b":"code","bf4b287e":"code","fd8040b9":"code","a6e3d63d":"code","0b764bff":"code","88d6f537":"code","7e8d3680":"code","d6685c10":"code","df88d403":"code","74e131fb":"code","253534fd":"code","fb00d63f":"code","4bc7719f":"code","0ba78f06":"code","b16e8026":"code","67ead60b":"code","bfac5037":"markdown","db8b5c46":"markdown","a42d81d0":"markdown","f81c7119":"markdown","e0a39790":"markdown","d10df7f3":"markdown","6827f9a0":"markdown","e7561be5":"markdown","0cfa0499":"markdown","c4e1a1b5":"markdown"},"source":{"4141435d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8792e615":"train_df = pd.read_csv(\"\/kaggle\/input\/widsdatathon2022\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/widsdatathon2022\/test.csv\")","cd96074c":"train_df.columns","df26beb7":"train_df.describe()","1e51b303":"train_df.info()","41c45864":"train_df = train_df.fillna(train_df.mode().iloc[0])\ntest_df = test_df.fillna(test_df.mode().iloc[0])\nnumerics = ['float64', 'int64']\nnumeric_columns = train_df.select_dtypes(include=numerics).columns\nstring = ['object']\nstring_columns = train_df.select_dtypes(include=string).columns\nprint(string_columns)","46315ba0":"import seaborn as sns\nimport matplotlib.pyplot as plt\ndef create_boxplot(feature,df):\n    plt.figure(figsize=(15, 15))\n    fig = sns.boxplot(x = df[feature], y=df[\"site_eui\"])\ndef create_plot(feature,df):\n    plt.figure(figsize=(15, 15))\n    fig = sns.countplot(df[feature])\n    \nfor column in string_columns:\n    create_plot(column,train_df)\n    create_boxplot(column,train_df)\n","41c43045":"from sklearn import preprocessing\n#from sklearn.preprocessing import OneHotEncoder\nle = preprocessing.LabelEncoder()\ntrain_df['State_Factor']= le.fit_transform(train_df['State_Factor']).astype(\"uint8\")\ntest_df['State_Factor']= le.transform(test_df['State_Factor']).astype(\"uint8\")\n\ntrain_df['building_class']= le.fit_transform(train_df['building_class']).astype(\"uint8\")\ntest_df['building_class']= le.transform(test_df['building_class']).astype(\"uint8\")\n\ntrain_df['facility_type']= le.fit_transform(train_df['facility_type']).astype(\"uint8\")\ntest_df['facility_type']= le.transform(test_df['facility_type']).astype(\"uint8\")","d80013c5":"y = train_df['site_eui']\nX = train_df.drop(['site_eui','id'],axis=1)\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)","ecfbd906":"test = test_df.drop(['id'],axis=1)\ny_pred = reg.predict(test)","c80d289c":"sample_solution = pd.read_csv('\/kaggle\/input\/widsdatathon2022\/sample_solution.csv')\nsample_solution","e39777fb":"preds = pd.DataFrame()\npreds['id'] = test_df['id']\npreds['site_eui'] = y_pred\npreds.to_csv('baseline_submission.csv',index=False)","bf9690c2":"from sklearn.feature_selection import SelectKBest, f_regression\nselectBest = SelectKBest(f_regression, k=30)\nX_new = selectBest.fit_transform(X, y)\ntest_new = selectBest.transform(test)","6fcfc78a":"reg_n = LinearRegression().fit(X_new, y)\ny_pred_n = reg_n.predict(test_new)\npreds = pd.DataFrame()\npreds['id'] = test_df['id']\npreds['site_eui'] = y_pred\npreds.to_csv('baseline_submission_selected_features.csv',index=False)","c4b43b31":"from sklearn.ensemble import GradientBoostingRegressor\ngbm = GradientBoostingRegressor(n_estimators=100, learning_rate=1.0,max_depth=5, random_state=0).fit(X, y)","1e7c3ce5":"y_pred = gbm.predict(test)\npreds = pd.DataFrame()\npreds['id'] = test_df['id']\npreds['site_eui'] = y_pred\npreds.to_csv('gradient_boosted.csv',index=False)","2f0ecc4b":"feature_imp = pd.DataFrame(sorted(zip(gbm.feature_importances_, X.columns),reverse = True), columns=['Value','Feature'])\nfeature_imp = feature_imp[feature_imp.Value != 0]\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('GBM Feature Importance')\nplt.tight_layout()\nplt.show()","bf4b287e":"reqd_cols = ['facility_type','energy_star_rating','floor_area','year_built','january_avg_temp','building_class','ELEVATION','days_with_fog','snowdepth_inches','State_Factor','august_avg_temp','june_min_temp','december_min_temp','june_avg_temp','days_above_80F','february_min_temp','max_wind_speed']\nX_selected = X[reqd_cols]\ntest_selected = test[reqd_cols]","fd8040b9":"reg.fit(X_selected,y)\ny_pred = reg.predict(test_selected)\npreds = pd.DataFrame()\npreds['id'] = test_df['id']\npreds['site_eui'] = y_pred\npreds.to_csv('gradient_boosted_selected.csv',index=False)","a6e3d63d":"X = X.drop(['year_built'],axis=1)\nfrom catboost import CatBoostRegressor\ncat = CatBoostRegressor(iterations=24000,learning_rate=0.0021)\ncat.fit(X,y)\ny_pred = cat.predict(test)\npreds = pd.DataFrame()\npreds['id'] = test_df['id']\npreds['site_eui'] = y_pred\npreds.to_csv('gradient_boosted_selected.csv',index=False)","0b764bff":"import h2o\nfrom h2o.automl import H2OAutoML\nh2o.init()","88d6f537":"train =  h2o.import_file('..\/input\/widsdatathon2022\/train.csv')\ntest =  h2o.import_file('..\/input\/widsdatathon2022\/test.csv')","7e8d3680":"x = train.columns\ny = 'site_eui'\nx.remove(y)\nx.remove(\"id\") #removing id  ","d6685c10":"aml = H2OAutoML(seed=54)\naml.train(x=x, y=y, training_frame=train)","df88d403":"lb = aml.leaderboard\nlb.head() ","74e131fb":"preds = aml.predict(test)","253534fd":"predictions = pd.DataFrame()\npredictions['id'] = test_df['id']\npredictions['site_eui'] = preds.as_data_frame().predict\npredictions.to_csv('auto_ml.csv',index=False)","fb00d63f":"from IPython.display import clear_output\n!pip install autogluon --user\nclear_output()\nfrom autogluon.tabular import TabularPredictor\nID=\"id\"\nTARGET = \"site_eui\"\nTRAIN_PATH = \"..\/input\/widsdatathon2022\/train.csv\"\nTEST_PATH = \"..\/input\/widsdatathon2022\/test.csv\"\nSAMPLE_SUBISSION_PATH = \"..\/input\/widsdatathon2022\/sample_solution.csv\"\nSUBMISSION_PATH = \"submission.csv\"\npredictor = TabularPredictor(label=TARGET).fit(train_data=TRAIN_PATH)\npred_test = predictor.predict(TEST_PATH)","4bc7719f":"predictions = pd.DataFrame()\npredictions['id'] = test_df['id']\npredictions['site_eui'] = pred_test\npredictions.to_csv('auto_gluon.csv',index=False)","0ba78f06":"corr = train_df.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corr, vmax=0.9, cmap=\"Greens\", square=True)","b16e8026":"blend = pd.read_csv(\"\/kaggle\/input\/blending-input\/catboost_more_iterations.csv\")","67ead60b":"preds = pd.DataFrame()\npreds['id'] = test_df['id']\npreds['site_eui'] = blend['catboost'] * 0.025 + blend['auto_ml'] * 0.375 + blend['auto_gluon'] * 0.5\n\npreds.to_csv('blending.csv',index=False)","bfac5037":"# Catboost Regressor","db8b5c46":"# H2o AutoML","a42d81d0":"# Baseline Model: Linear Regression","f81c7119":"# Used Gradient Bossting Regressor","e0a39790":"# Basic EDA","d10df7f3":"This is the first time I have worked with libraries like H2o AutoML, AutoGluon and techniques like blending\n\nIt was possible only because of the amazing work shared by indivuals in the competition, I would like to provide a reference to their work.\nhttps:\/\/www.kaggle.com\/usharengaraju\/wids2022-lgbm-starter-w-b\n\nhttps:\/\/www.kaggle.com\/rhythmcam\/wids2022-autogluon-basic\n\nhttps:\/\/www.kaggle.com\/shrutisaxena\/wids2022-automl\n\nhttps:\/\/www.kaggle.com\/abhishek\/blending-blending-blending\n\nMy approach was to start with a baseline model like Linear Regression, and level up by using ensemble models like GBDT and CatBoost Regressors.\nLater on I tried both H2o Auto ML and AutoGluon, none of these techniques on their own were helping me reduce my RMSE to less than 40, (I did not do any hyperparameter optimization)\n\nI took my best performing models which were catboost , AutoML and AutoGluon and blended their result for submission and that helped me reach my current rank.","6827f9a0":"# Blending","e7561be5":"**Tried using only the top important features with highest feature importance as input to Gradient Boosting Regressor**","0cfa0499":"**Used SelectKBest function from the sklearn library to select the top K features and input them to Linear Regression model.**","c4e1a1b5":"# AutoGluon"}}