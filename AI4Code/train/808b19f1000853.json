{"cell_type":{"e4b89238":"code","c8955a7f":"code","a09e15c0":"code","24c0682c":"code","40d74d08":"code","72168dff":"code","c08d4b23":"code","e7fb0653":"code","c6cbaa50":"code","cd196990":"code","16e15fb8":"code","025e7fd7":"code","ce6443c7":"code","3f4d0ed0":"code","45c325dd":"code","eb0ffce7":"code","91e68220":"code","e00082eb":"code","1b5daea0":"code","986f3637":"code","59a441e9":"code","33ad7611":"code","6d85f95a":"code","d01a34f8":"code","d07f6e1a":"code","f79ee4a4":"code","b19ae959":"code","93d1451c":"code","a4865bb5":"code","b2bdd7a3":"code","d106b014":"code","c751352d":"code","e780fbe8":"code","52f5c1dc":"code","2bcdce54":"code","4d8ba661":"code","282084ca":"code","94424e4e":"code","58dc9395":"code","bc552b8f":"code","4d3a920f":"code","df36c927":"code","39cc5855":"code","71253462":"code","7c47a7f4":"code","b7f70d0b":"code","fc2611c6":"code","59e5faef":"markdown","e5012901":"markdown","8cf734ef":"markdown","740bae16":"markdown","fb182076":"markdown","5a9666ab":"markdown","3abaccf2":"markdown","066b88df":"markdown","bf1f651c":"markdown","262e0e3d":"markdown","52469384":"markdown","332ec08c":"markdown","0503a906":"markdown","b1a9c048":"markdown","49515637":"markdown","a6aafffa":"markdown","0fa922ce":"markdown","3b65abe0":"markdown","a0c640e7":"markdown","e31fd2dc":"markdown","b3700ca3":"markdown","307a49b5":"markdown","018a7bf0":"markdown","7d92bb49":"markdown","32372b5e":"markdown","ec094507":"markdown","1e576a1e":"markdown","a81faded":"markdown","dd818942":"markdown","4dfea515":"markdown","ef512e1a":"markdown","2a7f3c4d":"markdown","0cbefb45":"markdown","a159fc17":"markdown","12727c14":"markdown","5cdf7bb5":"markdown","aabb520a":"markdown","a648adfb":"markdown","674740da":"markdown","127031e4":"markdown","8b6e9b7a":"markdown","9d679486":"markdown","61c22a5b":"markdown","521640c6":"markdown","4e4430b4":"markdown","928bfc70":"markdown","c734e7f5":"markdown","7c3522c8":"markdown","1a162e20":"markdown","53679e7a":"markdown","8e637fd2":"markdown","5e9e651b":"markdown","538bbf25":"markdown","2f750bfc":"markdown","c56401fc":"markdown","cb42115a":"markdown","30db7591":"markdown","489e4261":"markdown","c3c17728":"markdown","f0c0b2ef":"markdown","6f34db18":"markdown","b0a2d6a2":"markdown"},"source":{"e4b89238":"### define our metrics function with plotting confusion matrix\nimport itertools\nfrom sklearn.metrics import confusion_matrix \n\n### define function for plotting confusion matrix\ndef plot_confusion_matrix(y_true, y_preds):\n    # Print confusion matrix\n    cnf_matrix = confusion_matrix(y_true, y_preds)\n    # Create the basic matrix\n    plt.imshow(cnf_matrix,  cmap=plt.cm.Blues)\n    # Add title and axis labels\n    plt.title('Confusion Matrix')\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    # Add appropriate axis scales\n    class_names = set(y) # Get class labels to add to matrix\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names, rotation=0)\n    plt.yticks(tick_marks, class_names)\n    # Add labels to each cell\n    thresh = cnf_matrix.max() \/ 2. # Used for text coloring below\n    # Here we iterate through the confusion matrix and append labels to our visualization\n    for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n            plt.text(j, i, cnf_matrix[i, j],\n                     horizontalalignment='center',\n                     color='white' if cnf_matrix[i, j] > thresh else 'black')\n    # Add a legend\n    plt.colorbar();\n    plt.show();\ndef metrics(model_name, y_train, y_test, y_hat_train, y_hat_test):\n    '''Print out the evaluation metrics for a given models predictions'''\n    print(f'Model: {model_name}', )\n    print('-'*60)\n    plot_confusion_matrix(y_test,y_hat_test)\n    print(f'test accuracy: {round(accuracy_score(y_test, y_hat_test),2)}')\n    print(f'train accuracy: {round(accuracy_score(y_train, y_hat_train),2)}')\n    print('-'*60)\n    print('-'*60)\n    print('Confusion Matrix:\\n', pd.crosstab(y_test, y_hat_test, rownames=['Actual'], colnames=['Predicted'],margins = True))\n    print('\\ntest report:\\n' + classification_report(y_test, y_hat_test))\n    print('~'*60)\n    print('\\ntrain report:\\n' + classification_report(y_train, y_hat_train))\n    print('-'*60)","c8955a7f":"def plot_feature_importances(model):\n    n_features = X_train.shape[1]\n    plt.figure(figsize=(8, 8))\n    plt.barh(range(n_features), model.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), X_train.columns.values)\n    plt.xlabel('Feature importance')\n    plt.ylabel('Feature')","a09e15c0":"# Importing libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Supress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Classification\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis , QuadraticDiscriminantAnalysis\nfrom xgboost import XGBClassifier\n\n# Regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV, ElasticNet, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor \nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor\n\n# Modelling Helpers :\nfrom sklearn.preprocessing import Normalizer , scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import GridSearchCV , KFold , cross_val_score, ShuffleSplit, cross_validate\n\n# Preprocessing :\nfrom sklearn.preprocessing import MinMaxScaler , StandardScaler, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Metrics :\n# Regression\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error \nfrom sklearn.metrics import accuracy_score,classification_report\n\n# Classification\nfrom sklearn.metrics import recall_score, f1_score, fbeta_score, r2_score, roc_auc_score, roc_curve, auc, cohen_kappa_score\n\n\n## To display  all the interactive output without using the print function\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n","24c0682c":"import pandas as pd\ndf=pd.read_csv(\"..\/input\/autism-screening-for-toddlers\/Toddler Autism dataset July 2018.csv\")\n\n","40d74d08":"df.info()\ndf.head(20)","72168dff":"df.shape","c08d4b23":"df.describe()","e7fb0653":"df.columns","c6cbaa50":"df.drop(['Case_No', 'Who completed the test'], axis = 1, inplace = True)\ndf.columns","cd196990":"# Calculating the percentage of babies shows the symptoms of autisim\nyes_autism= df[df['Class\/ASD Traits ']=='Yes']\nno_autism= df[df['Class\/ASD Traits ']=='No']\n\nprint(\"Toddlers:\",round(len(yes_autism)\/len(df) * 100,2))\n\nprint(\"Toddlers:\",round(len(no_autism)\/len(df) * 100,2))\n\n\n\n","16e15fb8":"# Displaying the content of the target column\ndf['Class\/ASD Traits '].value_counts()","025e7fd7":"import matplotlib.pyplot as plt\nfig = plt.gcf()\nfig.set_size_inches(7,7)\nplt.pie(df[\"Class\/ASD Traits \"].value_counts(),labels=('no_autism','yes_autism'),explode = [0.1,0],autopct ='%1.1f%%' ,\n        shadow = True,startangle = 90,labeldistance = 1.1)\nplt.axis('equal')\n\nplt.show()","ce6443c7":"# Checking null data \ndf.isnull().sum()","3f4d0ed0":"df.dtypes","45c325dd":"corr = df.corr( )\nplt.figure(figsize = (15,15))\nsns.heatmap(data = corr, annot = True, square = True, cbar = True)","eb0ffce7":"# Visualizing Juandice occurance in males and females\nplt.figure(figsize = (16,8))\n\nplt.style.use('dark_background')\nsns.countplot(x = 'Jaundice', hue = 'Sex', data = yes_autism)","91e68220":"sns.countplot(x = 'Qchat-10-Score', hue = 'Sex', data = df)","e00082eb":"#Visualizing  the age distribution of Positive ASD  among Todllers\n\n\nf, ax = plt.subplots(figsize=(12, 8))\nsns.countplot(x=\"Age_Mons\", data=yes_autism, color=\"r\");\n\nplt.style.use('dark_background')\nax.set_xlabel('Toddlers age in months')\nax.set_title('Age distribution of ASD positive')\n\n","1b5daea0":"plt.figure(figsize = (16,8))\nsns.countplot(x = 'Ethnicity', data = yes_autism)","986f3637":"#  visualize positive  ASD among Toddlers based on Ethnicity\nplt.figure(figsize=(20,6))\nsns.countplot(x='Ethnicity',data=yes_autism,order= yes_autism['Ethnicity'].value_counts().index[:11],hue='Sex',palette='Paired')\nplt.title('Ethnicity Distribution of Positive ASD among Toddlers')\nplt.xlabel('Ethnicity')\nplt.tight_layout()\n","59a441e9":"# Displaying number of positive cases of Autisim with Regards Ethnicity\nyes_autism['Ethnicity'].value_counts()","33ad7611":"#Lets visualize the distribution of autism in family within different ethnicity\nf, ax = plt.subplots(figsize=(12, 8))\n\n\nsns.countplot(x='Family_mem_with_ASD',data=yes_autism,hue='Ethnicity',palette='rainbow',ax=ax)\nax.set_title('Positive ASD Toddler relatives with Autism distribution for different ethnicities')\nax.set_xlabel('Toddler Relatives with ASD')\nplt.tight_layout()\n\n\n","6d85f95a":"# removing 'Qchat-10-Score'\ndf.drop('Qchat-10-Score', axis = 1, inplace = True)","d01a34f8":"le = LabelEncoder()\ncolumns = ['Ethnicity', 'Family_mem_with_ASD', 'Class\/ASD Traits ', 'Sex', 'Jaundice']\nfor col in columns:\n    df[col] = le.fit_transform(df[col])\ndf.dtypes\n","d07f6e1a":"df.head(25)","f79ee4a4":"X = df.drop(['Class\/ASD Traits '], axis = 1)\ny = df['Class\/ASD Traits ']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.40, random_state =42)\nX.isnull().sum()\nX.info()","b19ae959":"\n\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('XGB', XGBClassifier()))\nmodels.append(('GBR', GradientBoostingRegressor()))\nmodels.append(('ABR', AdaBoostRegressor()))\n\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_hat_test = model.predict(X_test).astype(int)\n    y_hat_train = model.predict(X_train).astype(int)\n    print(name, 'Accuracy Score is : ', round(accuracy_score(y_test, y_hat_test)))\n\n    metrics(name, y_train, y_test, y_hat_train, y_hat_test)\n\n\n    \n    ","93d1451c":"for name, model in models:\n    \n    y_hat_test = model.predict(X_test).astype(int)\n    y_hat_train = model.predict(X_train).astype(int)\n    print(name, 'Accuracy Score is : ',round( accuracy_score(y_test, y_hat_test),2))","a4865bb5":"svc = SVC()\n\nparams = {\n    'C': [0.1,0.8,0.9,1,1.1,1.2,1.3,1.4],\n    'kernel':['linear', 'rbf'],\n    'gamma' :[0.1,0.8,0.9,1,1.1,1.2,1.3,1.4]\n}\n\nclf = GridSearchCV(svc, param_grid = params, scoring = 'accuracy', cv = 10, verbose = 2)\n\nclf.fit(X_train, y_train)\nclf.best_params_","b2bdd7a3":"# Re-running model with best parametres\nsvc1 = SVC(C = 0.8, gamma = 0.1, kernel = 'linear')\nsvc1.fit(X_train, y_train)\ny_hat_test = svc1.predict(X_test)\n#print(accuracy_score(y_test, y_hat_test))\nmetrics(svc1, y_train, y_test, y_hat_train, y_hat_test)\nsvcgrid_test_acc = round(accuracy_score(y_test, y_hat_test), 2)\n\nsvcgrid_test_acc","d106b014":"\n#Instantiate the pipeline\nfrom sklearn.pipeline import Pipeline\n\npipe = Pipeline([('classifier', RandomForestClassifier(random_state=123))])\n","c751352d":"grid = [{'classifier__criterion': ['gini', 'entropy'],\n         'classifier__n_estimators':[10, 20, 50, 100],\n         'classifier__max_depth': [None, 5, 3, 10],\n         'classifier__min_samples_split': [1.0, 6, 10],\n         'classifier__min_samples_leaf': [1,  6, 10],\n         'classifier__class_weight':['balanced']}]","e780fbe8":"clf = GridSearchCV(estimator=pipe, param_grid=grid,\n                   cv=5, scoring='roc_auc', n_jobs=-1)\nclf.fit(X_train, y_train)\ny_hat_train = clf.predict(X_train)\ny_hat_test = clf.predict(X_test)","52f5c1dc":"\nmetrics(clf, y_train, y_test, y_hat_train, y_hat_test)\n","2bcdce54":"print(round(clf.score(X_train, y_train)))\nprint(round(clf.score(X_test, y_test)))","4d8ba661":"clf.best_params_","282084ca":"# Research best estimator from grid\nbest_clf_estimator = clf.best_estimator_\nbest_clf_estimator.fit(X_train,y_train)","94424e4e":"#Predictions\ny_hat_train=best_clf_estimator.predict(X_train)\ny_hat_test = best_clf_estimator.predict(X_test)\nresults=metrics(best_clf_estimator, y_train, y_test, y_hat_train, y_hat_test)\nrf_gridsearch_test_acc = round(accuracy_score(y_test,y_hat_test), 2)\nrf_gridsearch_test_acc","58dc9395":"plot_feature_importances(model)","bc552b8f":"##Applying MLPClassifier Model \nfrom sklearn.neural_network import MLPClassifier\nMLPClassifierModel = MLPClassifier(activation='tanh', # can be also identity , logistic , relu\n                                   solver='lbfgs',  # can be also sgd , adam\n                                   learning_rate='constant', # can be also invscaling , adaptive\n                                   early_stopping= False,\n                                   alpha=0.0001 ,hidden_layer_sizes=(100, 3),random_state=33)\nMLPClassifierModel.fit(X_train, y_train)\n#Calculating Prediction\ny_hat_test = MLPClassifierModel.predict(X_test)\ny_hat_train = MLPClassifierModel.predict(X_train)\n\nresults=metrics(MLPClassifierModel, y_train, y_test, y_hat_train, y_hat_test)\n\n#Calculating Accuracy Score  \nnn_sklearn_test_acc = round(accuracy_score(y_test, y_hat_test, normalize=True),2)\n\nprint('Accuracy Score is : ', nn_sklearn_test_acc)\n","4d3a920f":"# Define simple neural network model\n# Keras\n\nfrom keras.models import Sequential\nfrom keras.layers import *\nfrom keras.optimizers import Adam, RMSprop\n\n\n\n\n\nmodel = Sequential()\n# Define Input Layer wits 15 features as an input\nmodel.add(Dense(100, input_dim=15, activation='relu'))\n#model.add(Dense(1, activation='sigmoid'))\n#single output layer with one neuron since we only want to predict two classes either yes autisim =1 or no autisim=zero\nmodel.add(Dense(activation = 'sigmoid', units = 1))\n\n\n# Compile the Neural network\nmodel.compile(loss='binary_crossentropy', # we use binarray here becuase we just have 2 classes\n             optimizer = Adam(lr=0.0001, decay=1e-5), ### learning rate 0.0001\n              metrics=['acc'])\n# Fit to training data\nmodel.fit(X_train, y_train, epochs=100,  \n          validation_data=(X_test, y_test))","df36c927":"# collecting the summary of our neural network paramters\nmodel.summary()\n","39cc5855":"#structure of keras neuralnetwork model\nfrom keras.utils import plot_model\n\nplot_model(model)\n# looking at the structure of my neural network","71253462":"def evaluate_clf(y_true, y_pred):\n    \"\"\"Return confusion matrix, classification report, and accuracy score\n    for a classifier.\n    \n    Parameters\n    ----------\n    y_true : array-like\n        Target class labels\n    y_pred : array-like\n        Predicted class labels\n        \n    Returns\n    ----------\n    Confusion matrix, classification report, accuracy score\n    \"\"\"\n    \n    test_acc = round(accuracy_score(y_true, y_pred), 2)\n    \n    print('Confusion Matrix:')\n    print(confusion_matrix(y_test, y_pred))\n    print('---'*20)\n    print('Classification Report:')\n    print(classification_report(y_test, y_pred))\n    print('---'*20)\n    print(\"kerasNN_test_acc:\",round(accuracy_score(y_test,y_hat_test), 2))\n    print(\"kerasNN_train_acc:\" ,round(accuracy_score(y_train,y_hat_train), 2))\n","7c47a7f4":"#Predictions\n\ny_hat_test = model.predict_classes(X_test)\ny_hat_train = model.predict_classes(X_train)\n# applying the metrics function\nevaluate_clf(y_test, y_hat_test)\n\n\n","b7f70d0b":"# Create classifier summary table\n\nLogisticRegression_Accuracy =  1.0\nLinearDiscriminantAnalysis_accuracy  =  0.96\nKNeighborsClassifier_accuracy =  0.91\nDecisionTreeClassifier_accuracy =  0.91\nGaussianNB_accuracy =  0.94\nSVC_beforegrid_accuracy =  0.78\nRandomForest_beforegrid_accuracy =  0.64\nXGBClassifier_accuracy=  0.99\nGradientBoosting_accuracy =  0.64,\nAdaBoosting_accuracy = 0.49\nSVC_aftergrid_accuracy = 1.0\nRandomForest_aftergrid_accuracy =0.96\nNeuralnetwork_SKLearn_accuracy= 0.99\nNeuralnetwork_Keras_accuracy = 0.95\n\n\n\nmodels=['LogisticRegression','LinearDiscriminantAnalysis',\n       'KNeighborsClassifier','DecisionTreeClassifier',\n        'GaussianNB','SVC_beforegrid',\n        'RandomForest_beforegrid','XGBClassifier',\n        'GradientBoosting','AdaBoosting','SVC_aftergrid','RandomForest_aftergrid','Neuralnetwork_SKLearn Accuracy','Neuralnetwork_Keras']\n\n\ntest_Accuracy=[1.0,0.96,0.91,0.91,0.94, 0.78,0.64, 0.99,0.64,0.49,1.0,0.96,0.99,0.95]\n\n\n\n\naccuracy_summary = pd.DataFrame([models, test_Accuracy]).T\naccuracy_summary.columns = ['Classifier', 'test_Accuracy']\n","fc2611c6":"accuracy_summary","59e5faef":"### White European then Asian indeed are the most affected ethnicity. But we can notice that males have more  positive  autisim compared to females.\n\n","e5012901":"### Preprocessing features to get them ready for modeling through encoding caterogical features","8cf734ef":"### Recommendation\n\n- Apply Logistic Regression, XG Boosting or  neural network using sklearn for high accuracy model to diagnose Autisim.\n\n- Diagnose  deeply and early autisim disease among particular ethnicity particularly  White European, Asian and middle East.\n\n- Try to find more state of art in tracing autisim symptoms among toddlers in ages less than three years.","740bae16":"#  visualize positive  ASD among Toddlers based on Ethnicity\n","fb182076":"# Checking the type of the data ","5a9666ab":"Number of Trainable parameters is 1,701\n","3abaccf2":"# Checking null data ","066b88df":"### White European ,Asian  then middle eastern are the most etnicity showed increase in autism cases.","bf1f651c":"# Interpreting the Heatmap of Correlation\n\nThe features in orange colour shows high correlation, we can see all the 10 answers from A1 to A9 except A10 are highly correlated with Qchart10-score. Therefore, it is better to remove the Qchat coloumn from our feature list because it is going to mislead our results.","262e0e3d":"# Displaying number of positive cases of Autisim with Regards Ethnicity\n","52469384":"# In general,Plot shows that the Qchat has more postive answers among males than females. That reflects males  are more postive to autisim than females.","332ec08c":"# In age close to 36 months which is three years shows more positive autism.","0503a906":"# Best Parameters\u00b6","b1a9c048":"# Print evaluation metrics","49515637":"### In Toddlers, autism is nearly 2 times in males having jundice than in  females.\n","a6aafffa":"In general family history shows small effect in autisim.\nHowever, we can see that  toddlers of White Europeans Ethnicities have very high chance of being ASD positive if they have it in their genes. Middle eastern and Asians follow the next though with smaller ratios.So We can somewhat conclude  that there is a genetic connect for ASD positive.","0fa922ce":"### Displaying Columns","3b65abe0":"#Visualizing  the age distribution of Positive ASD  among Todllers","a0c640e7":"### Pie plot the distribution of the target column","e31fd2dc":"# Removing unwanted Coloumns","b3700ca3":"###  collecting the summary of our neural network paramters\n","307a49b5":"## Random Forest( Grid search and pipeline )\nGrid search algorithm is used to make exhaustive search of hyper parameters.\n\nPipeline is used to streamline the routine processes.","018a7bf0":"# Accuray Summary of Models:","7d92bb49":"## Collecting the accuracy of every model","32372b5e":"# Tuning hyperparametres for SVC","ec094507":"# Introduction\n\n### Background\nAutism spectrum disorder (ASD) is a type of neurological and developmental disorder that begins early in childhood and continues throughout a person's life. In this project, Dr. Fayez Thabtah collected data from 1054 toddlers whose ages were less than three years old. The Doctor asked their parents yes\/no questions. These questions were behavioral questions and the number of yes were added up and have been recorded in the column called Q chat 10 question. If this column has more than 3 yes, toddlers will be classified as positive ASD.\n### Our role in this project is to use machine learning techniques to diagnose ASD based on the Dr. Thabtah\u2019s data.\n\n\nSource of Data:\n\nFayez Thabtah\n\nDepartment of Digital Technology\nManukau Institute of Technology \nAuckland, New Zeland\n\nThe columns are as follows:\n\n### Q chat 10 question features with toddlers \n\nA1: Does your child look at you when you call his\/her name?\n\nA2: How easy is it for you to get eye contact with your child?\n\nA3: Does your child point to indicate that s\/he wants something?\n\nA4: Does your child point to share an interest with you?\n\nA5: Does your child pretend? e.g. care for dolls, talk on a toy phone?\n\nA6: Does your child follow where you are looking?\n\nA7: If you or someone else in the family is visibly upset, does your child show signs of waning to comfort them? e.g. stroking hair, hugging them)\n\nA8: Would you describe your child's first word as:\n\nA9: Does your child use simple gestures (e.g.wave goodbye)?\n\nA10:Does your child stare at  nothing with no apparent purpose?\n\n\n\n\n\n","1e576a1e":"# Functions to evaluate the model\n","a81faded":"# Re-running model with best parametres","dd818942":"# collecting stastical info  about our data","4dfea515":"# Modeling, I performed 10 models ","ef512e1a":"# Conclusion","2a7f3c4d":"Ths model is great based on train, test and recall accuracy, so I highly recommend it.","0cbefb45":"# Instantiate, fit and predict","a159fc17":"# Accuracy Summary of the above 14 models","12727c14":"# Importing libraries","5cdf7bb5":"# Calculating the percentage of babies shows the symptoms of autisim\n","aabb520a":"# Obtain Data","a648adfb":"# Explore Data","674740da":"###  Visualizing Juandice occurance in males and females","127031e4":"\n#Neural Network Classifier Using Keras Library","8b6e9b7a":"# To know the dimension of our data","9d679486":"# Function to draw the feature importances","61c22a5b":"# Neural Network Classifier Using SKLearn","521640c6":"# Define The Problem \nBased on Quantitative Checklist for Autism in Toddlers (Q-CHAT) data provided by the ASD Tests app, we will try to develop a simple prediction model for toddlers to predict the probability of showing ASD traits so that their parents\/guardians can consider taking steps early enough.","4e4430b4":"## This project was collected over one tousand fifty four patients and 18 features were used","928bfc70":"# Define the grid\n","c734e7f5":"## Investigating the correlation through plotting pearson method heatmap","7c3522c8":"# Interpreting the Results:\nLogistic Regression model training and testing accuracy are 100%, Recall, Precision, F1score are 100% for both yes autism and no autism patients, FP, and False negative= Zero. Logistic Regression with default parameters is an ideal model.\n\nLinearDiscriminantAnalysis has training and testing and Recall accuracy= 96% So LinearDiscriminantAnalysis is also a very good model.\n\nKNN model has test accuracy: 0.91,train accuracy: 0.95, and weight average Recall = 91%.\nso KNN is a good model.\n\nThe decision tree model has test accuracy: 0.91 and train accuracy: 1.0. This model has to be hyper tuned to adjust the overfitting, so it is not accepted.\nNaieveBays model has test accuracy: 0.94, train accuracy: 0.95, and Weighted average Recall 94%, so naieve bayes is a good model.\n\nThe support vector machine and random forest have a low accuracy score before hyper tuned, so they are rejected.\n\nXG Boosting has test accuracy: 0.99, train accuracy: 1.0, and Weighted average Recall for positive cases =99%, So XG Boosting is an ideal model ranked number#2 after Logistic Regression.\n\nGradientBoosting and ADA Boosting have low accuracy, so they are rejected.\nIn summary, out of these 10 models, I will choose Logist Regression, and XG Boosting.\n\nNow, We will try to hyper tune support vector machine using grid search without pipeline, then we will hyper tune random forest using grid-search with the pipeline,\nadditionally, we will model with a neural network through sikit learn, and Keras library.","1a162e20":"# Best Scores","53679e7a":"# Interpreting the results of the second Random Forest model\n Random forest classifier before hyper tune Recall was 47% and after hyper tuning recall is 97%, also False-negative 156\/422=37% before hyper tune but after hyper tune the false negative is only 8\/422 = 0.02% meaning this model misclassifies only 0.02% and this is great mode.lWe have a weighted average recall and precision of 0.96 ain the test set. We don't have overfitting. It means we are able to detect the positive autism patient 96%.\n\nWe used pipelines to prevent data leakage and to have clean code.","8e637fd2":"# Important Questions \n\n1-What is good algorithm to diagnose autism?\n\n2- How can I improve my models\u2019 accuracy?\n\n3- What are the most  important features in diagnosing autism among children?\n\n","5e9e651b":"# Explore Ethnicity","538bbf25":"### Plotting the architucture of neural net work","2f750bfc":"##### Logistic Regression\n\nLogistic Regresiom model training and testing accuracy are 100%,Recall,Precision,F1score are 100% for both yes autisim and no autisim patients, FP and False negative= Zero. \n\n### Logistic Regression with default parameters is an ideal model.\n\n### LinearDiscriminantAnalysis \n\nLinearDiscriminantAnalysis has training and testing and Recall accuracy= 96% So LinearDiscriminantAnalysis is also very good model. \n\n### KNN model \n\nKNN model has test accuracy: 0.91,train accuracy: 0.95, and weight average Recall = 91% for positive cases , so KNN is good model. \n\n### Decsion tree model\n\nDecsion tree model has test accuracy: 0.91 and train accuracy: 1.0. This model has to be hypertuned to adjust the overfitting, so it is not accepted.\n\n### NaieveBays model\n\n NaieveBays model has test accuracy: 0.94,train accuracy: 0.95, and Weighted average Recall for positive cases =94%,so naieve bayes is a good model.\n\n### Support vector machine and Random forest\n\nSupport vector machine and Random forest have low accuracy score before hypertunned, so they are rejected. \n\n### XG Boosting\n\nXG Boosting has test accuracy: 0.99,train accuracy: 1.0,and Weighted average Recall for positive cases =99%, \n\n###So XG Boosting is an ideal model ranked number#2 after logistic Regression.\n\n### GradientBoosting and ADA Boosting \n\n GradientBoosting and ADA Boosting have low accuracy, so they are rejected. In summary out of these 10 models, I will choose Logist Regression, and XG Boosting.\n\n### Support Vector Machine after hypertunned\n\nSupport Vector Machine after hypertuned through grid search with no pipeline used has test accuracy: 1.0 and train accuracy: 0.48. This model has underfitting and it is rejected.\n\n### Random Forest  \n\nRandom forest classifier before hypertune Recall was 47% and after \n\nhypertuning recall is 97%, also False negative 156\/422=37% before hypertune \n\nbut after hypertune the false negative is only 8\/422 = 0.02% meaning this \n\nmodel misclassifies only 0.02% and this is great mode. We have a weighted \n\naverage recall and precision of 0.96.  It means we are able to detect the positive autisim patient with 96% accuracy which is a good accuracy. We don't have overfitting because test accuracy: 0.96 while the train accuracy: 0.98. \n\n### Neural Network Classifier Using SKLearn\n\nNeural Network Classifier Using SKLearn has a test accuracy: 0.99 and train accuracy: 1.0, and Weighted average Recall =99%,\n\n### so Neural Network Classifier Using SKLearn is an ideal model.\n\n\n\n\nA9 is the most  important feature , A5,A6, are part of important features\n\nWe used gridsearch to find the best parameters.\n\nWe used pipeline to prrevent data leakage and to have clean code.\n\n\n### Neural Network Classifier Using Keras Library\nAccuracy is 95% so this model is a good model.\n\n\n\n### In Conclusion\n### Models\n* Logistic Regression, XG Boosting and Neural Network are good models in our autism project with it small sample size.\n### Important Features\n* A9 is the most important features, other important features are A6, A5, and A8. Jandice and the type of gender'sex' are less important features. Genes in this study which appears through the column of family member with ASD is not an important feature\n* Males are more positive to autism than females.\n* Ages close to 36 months which is three years old show more positive autism.\n* White European, Asian and middle eastern are the ethnicities that showed an increase in autism cases.\n\n\n\n\n","c56401fc":"# Visualizing The Q-chat 10 Score according to the gender","cb42115a":"### Future Work\n* Reach out the same clininc and get more data either with number of diagnosed patients or with more features used for diagnosis.\n\n* Suggest to the main researcher to try numeric features as much as possible.\n\n* Search other Autusim data around the world and do the same approch used in this project.\n\n\n* Apply the same models on adults\n\n\n*  Model with image data of cases daignosed with autisim using X-ray","30db7591":"# Define X and y and split the data to training\/testing, with test-size=40%","489e4261":"### visualizing the distribution of autism in family within different ethnicity","c3c17728":"# Contributors:\nKhulood Nasher\n\nManar-Aleslam Mattar\n\nMervat Mohammed  Ahmed\n\n","f0c0b2ef":"\n#Instantiate the pipeline","6f34db18":"# A9 is the most important feature, other important features are A6, A5, and A8. Features named A are answers to the questions. If the patient answers yes to the question, the feature gets 1 for yes. As many 1 was in each column, The column got its importance. Ethnicity and age in months are important features, Jandice and the type of gender'sex' are less important features. Genes in this study which appear through the column of a family member with ASD is not an important feature, and that's actually against the worldwide studies. I can refer to the small size of the sample under study.","b0a2d6a2":"### Support vector machine model after hypertunned by grid search ran into underfitting so this model is rejected."}}