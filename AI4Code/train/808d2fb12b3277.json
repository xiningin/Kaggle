{"cell_type":{"2df02560":"code","4d8dd087":"code","d99f2c2f":"code","e1001d24":"code","3dd9982a":"code","ca5221c9":"code","704e294f":"code","bff38710":"code","d0b9319e":"code","bd7c1763":"code","0da3061e":"code","4dd5bf87":"code","fc0fad91":"code","107b73ba":"code","1ab11026":"code","0d54fe82":"code","22cb6dfa":"code","e550204c":"code","7c52b1e5":"code","c7c75b0b":"code","3617bd65":"code","4a0afbc6":"code","ba957a25":"code","e3e5323a":"code","789252d9":"code","69105a49":"code","4fdbfc00":"code","2494a621":"code","382a5d85":"markdown","fffc9411":"markdown","80dace45":"markdown","253ee769":"markdown","ca6ab9be":"markdown","ffa1f5ae":"markdown","b63081bb":"markdown","01abf146":"markdown","de79b8e7":"markdown","081a0639":"markdown","461bd21c":"markdown","b9ea11a9":"markdown","c4358d55":"markdown","ef5b0065":"markdown","3a20f7de":"markdown","d5063a92":"markdown","eecb3c5b":"markdown","ae5cbe6b":"markdown","67e31506":"markdown","84ee4fc1":"markdown","92cf2449":"markdown","a370445e":"markdown"},"source":{"2df02560":"#load packages\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","4d8dd087":"train = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/test.csv\")","d99f2c2f":"# a quick look into the data\ntrain.head()","e1001d24":"#see if there are null values\ntrain.info()","3dd9982a":"#get some statistical information\ntrain.describe()","ca5221c9":"#visualize target distribution\n\nsns.distplot(a=train['target'], rug = True)","704e294f":"train[train['target']<4] # find the samples whose target value is smaller than 4","bff38710":"#visulization of 14 features\nfig = plt.figure(figsize=(18,16))\ntrain_feature = train.drop(['id','target'],axis=1)\nfor index,col in enumerate(train_feature):\n    plt.subplot(5,3,index+1)\n    sns.distplot(train_feature.loc[:,col], kde = False)\nfig.tight_layout(pad=1.0)","d0b9319e":"# corralation heatmap\nmask = np.zeros_like(train_feature.corr())\nmask[np.tril_indices_from(mask)] = True\n\nfeature_corr = train_feature.corr()\nsns.heatmap(feature_corr,cmap= \"Blues\",mask = mask.T)\n","bd7c1763":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_X, val_X,  train_Y, val_Y = train_test_split(\n    train_feature, train['target'], test_size=0.2, shuffle=True)\n","0da3061e":"from catboost import CatBoostRegressor\ncat = CatBoostRegressor(random_state = 7, loss_function='RMSE', verbose = False)\ncat.fit(train_X, train_Y)\n\nval_pred = cat.predict(val_X)\nscore = np.sqrt(mean_squared_error(val_Y, val_pred)) \n\nprint(\"CB model RMSE: \",end = \"\")\nprint(score)","4dd5bf87":"\ntest_pred = cat.predict(test.drop(\"id\",axis = 1))\n\nsubmission = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"target\":test_pred\n    })\nsubmission.to_csv('baseline_cat.csv', index=False)","fc0fad91":"\nplt.figure(figsize=(10, 10))\nplt.barh(cat.feature_names_, cat.feature_importances_,height =0.5)\n","107b73ba":"from bayes_opt import BayesianOptimization\nimport lightgbm\n\n#codes below are taken from https:\/\/www.kaggle.com\/yevonnaelandrew\/lgbm-cat-xgb-optimization-stacking\n\n\ndtrain = lightgbm.Dataset(data=train_feature, label=train['target'])\n\ndef hyp_lgbm(num_leaves, feature_fraction, bagging_fraction, max_depth, min_split_gain, min_child_weight, learning_rate):\n      \n        params = {'application':'regression','num_iterations': 5000,\n                  'early_stopping_round':100, 'metric':'rmse'}\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['min_split_gain'] = min_split_gain\n        params['min_child_weight'] = min_child_weight\n        params['learning_rate'] = learning_rate\n        cv_result = lightgbm.cv(params, dtrain, nfold=3, \n                                seed=7, stratified=False, \n                                verbose_eval =None, metrics=['rmse'])\n        \n        return -np.min(cv_result['rmse-mean']) \n        #add a minus because Bayesian Optimization can only be performed to approximate maxima.","1ab11026":"pds = {\n    'num_leaves': (5, 50),\n    'feature_fraction': (0.2, 1),\n    'bagging_fraction': (0.2, 1),\n    'max_depth': (2, 20),\n    'min_split_gain': (0.001, 0.1),\n    'min_child_weight': (10, 50),\n    'learning_rate': (0.01, 0.5),\n      }","0d54fe82":"# codes below takes a long execution time, uncomment to see the process\n# optimizer = BayesianOptimization(hyp_lgbm,pds,random_state=7)\n# optimizer.maximize(init_points=10, n_iter=50)","22cb6dfa":"# optimizer.max['params']","e550204c":"import catboost as cgb\n\ndef cat_hyp(depth, bagging_temperature, l2_leaf_reg, learning_rate):\n  params = {\"iterations\": 100,\n            \"loss_function\": \"RMSE\",\n            \"verbose\": False} \n  params[\"depth\"] = int(round(depth)) \n  params[\"bagging_temperature\"] = bagging_temperature\n  params[\"learning_rate\"] = learning_rate\n  params[\"l2_leaf_reg\"] = l2_leaf_reg\n  \n  cat_feat = [] # Categorical features list, we have nothing in this dataset\n  cv_dataset = cgb.Pool(data=train_feature, label=train['target'], cat_features=cat_feat)\n\n  scores = cgb.cv(cv_dataset,\n              params,\n              fold_count=3)\n  return -np.min(scores['test-RMSE-mean']) ","7c52b1e5":"# Search space\npds = {'depth': (3, 10),\n       'bagging_temperature': (0.1,10),\n       'l2_leaf_reg': (0.1, 10),\n       'learning_rate': (0.05, 0.3),\n        }","c7c75b0b":"# optimizer = BayesianOptimization(cat_hyp, pds, random_state=7)\n# optimizer.maximize(init_points=10, n_iter=80)","3617bd65":"# optimizer.max['params']","4a0afbc6":"import xgboost as xgb\n\ndtrain = xgb.DMatrix(train_feature, train['target'], feature_names=train_feature.columns.values)\ndef hyp_xgb(max_depth, subsample, colsample_bytree,min_child_weight, gamma, learning_rate):\n    params = {\n    'objective': 'reg:squarederror',\n    'eval_metric':'rmse',\n    'nthread':-1\n     }\n    \n    params['max_depth'] = int(round(max_depth))\n    params['subsample'] = max(min(subsample, 1), 0)\n    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n    params['min_child_weight'] = int(min_child_weight)\n    params['gamma'] = max(gamma, 0)\n    params['learning_rate'] = learning_rate\n    scores = xgb.cv(params, dtrain, num_boost_round=500,verbose_eval=False, \n                    early_stopping_rounds=10, nfold=3)\n    return -scores['test-rmse-mean'].iloc[-1]","ba957a25":"pds ={\n  'min_child_weight':(3, 20),\n  'gamma':(0, 5),\n  'subsample':(0.7, 1),\n  'colsample_bytree':(0.1, 1),\n  'max_depth': (3, 10),\n  'learning_rate': (0.01, 0.5)\n}","e3e5323a":"# optimizer = BayesianOptimization(hyp_xgb, pds, random_state=7)\n# optimizer.maximize(init_points=4, n_iter=15)","789252d9":"## parameters derived from Bayesian Optimizaion fine-tuning\nparam_lgbm = {\n     'bagging_fraction': 0.973905385549851,\n     'feature_fraction': 0.2945585590881137,\n     'learning_rate': 0.03750332268701348,\n     'max_depth': int(7.66),\n     'min_child_weight': int(41.36),\n     'min_split_gain': 0.04033836353603582,\n     'num_leaves': int(46.42),\n     'application':'regression',\n     'num_iterations': 5000,\n     'metric': 'rmse'\n}\n\nparam_cat = {\n     'bagging_temperature': 0.31768713094131684,\n     'depth': int(8.03),\n     'l2_leaf_reg': 1.3525686450404295,\n     'learning_rate': 0.18,\n     'iterations': 150,\n     'loss_function': 'RMSE',\n     'verbose': False\n}\n\n\nparam_xgb = {\n     'colsample_bytree': 0.8119098377889549,\n     'gamma': 2.244423418642122,\n     'learning_rate': 0.015800631696721114,\n     'max_depth': int(9.846),\n     'min_child_weight': int(15.664),\n     'subsample': 0.82345,\n     'objective': 'reg:squarederror',\n     'eval_metric':'rmse',\n     'num_boost_roun' : 500\n}","69105a49":"from sklearn.ensemble import StackingRegressor\nfrom xgboost import XGBRegressor\n\nestimators = [\n        ('lgbm', lightgbm.LGBMRegressor(**param_lgbm, random_state=7, n_jobs=-1)),\n        ('xgbr', XGBRegressor(**param_xgb, random_state=7, nthread=-1)),\n        ('cat', CatBoostRegressor(**param_cat))\n]\n\nreg = StackingRegressor(\n    estimators=estimators,\n    final_estimator=lightgbm.LGBMRegressor(),\n    n_jobs=-1,\n    cv=5\n)\n\ntrain_X, val_X,  train_Y, val_Y = train_test_split(\n    train_feature, train['target'], test_size=0.2, shuffle=True)\n\nreg.fit(train_X,train_Y)\n\nval_pred = reg.predict(val_X)\nscore = np.sqrt(mean_squared_error(val_Y, val_pred))\n\nprint(\"Final model RMSE: \",end = \"\")\nprint(score)\n\n","4fdbfc00":"#predict\nreg = StackingRegressor(\n    estimators=estimators,\n    final_estimator=lightgbm.LGBMRegressor(),\n    n_jobs=-1,\n    cv=5\n)\n\nreg.fit(train_feature, train['target'])\n","2494a621":"test_pred = reg.predict(test.drop(\"id\",axis = 1))\n\nsubmission = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"target\":test_pred\n    })\nsubmission.to_csv('stacking_sub.csv', index=False)","382a5d85":"finally we can make prediction to the test set.","fffc9411":"## model stacking","80dace45":"### Feature Importance","253ee769":"### LGBM tuning","ca6ab9be":"Moreover, we can easily derive feature importance after training the CatRegressor model. For more information, you may refer to official doc on [Feature importance - Catboost](https:\/\/catboost.ai\/docs\/features\/feature-importances-calculation.html#feature-importances-calculation)","ffa1f5ae":"We may use LightGBM, XGBoosting and CatBoost as our base models for model stacking. Before applying [model stacking](https:\/\/machinelearningmastery.com\/stacking-ensemble-machine-learning-with-python\/), we shall fine-tune the base models. [Bayesian Optimization](https:\/\/towardsdatascience.com\/shallow-understanding-on-bayesian-optimization-324b6c1f7083) is a efficient optimizaion methods by practice.","b63081bb":"### CatBoosting Baseline","01abf146":"## Baseline Regression","de79b8e7":"## **Basic information**\n","081a0639":"great! no null values in this dataset.","461bd21c":"### XGBoosting tuning","b9ea11a9":"train\/test set split","c4358d55":"## preparations","ef5b0065":"## model tuning","3a20f7de":"**Load data**","d5063a92":"### Correlation","eecb3c5b":"Let's use CatBoostRegressor as our baseline model.","ae5cbe6b":"### CatBoost Tuning","67e31506":"## Basic Exploratory Data Analysis(EDA) ","84ee4fc1":"Export baseline model prediction.","92cf2449":"notice that there is a training sample whose target value is \u201dabnormally\u201c small.","a370445e":"### Distribution"}}