{"cell_type":{"d8af959e":"code","27286dc0":"code","d8a14f3d":"code","bdd88ca2":"code","23acc68a":"code","e40c735d":"code","243c67c0":"code","0715bcbe":"code","3e345099":"code","2ac89975":"code","6ee0051a":"code","b481aaf4":"code","6d329987":"code","98f3b4c7":"code","02ed937c":"code","e9028593":"code","9b4fd46d":"code","e010dc78":"code","52c42036":"code","07d7797d":"code","309cf4ea":"code","45c3b635":"code","c069437c":"code","842e2789":"code","bd6e0216":"code","66273ac2":"code","6cbcb9c7":"code","e8b15269":"code","c7d5b02e":"code","303554ea":"code","0b291950":"code","c7d2d3b1":"code","16525d25":"code","8b852a6f":"code","acc3f5b3":"code","db53c1fb":"code","f626be52":"code","2e2ca2dc":"code","e5bcbf8d":"code","78854c31":"code","c4197fd9":"code","baa4ad20":"code","90506d0a":"code","baaadd24":"code","b1b44964":"code","4d03bf28":"code","1519d520":"code","d7640dc0":"code","7d4a6af8":"code","3e86d416":"code","ed833938":"code","798bfb92":"code","257cb439":"code","2a58ec98":"code","344d578a":"code","efce751d":"code","d2daf628":"code","557c2e01":"code","7d7d6f42":"code","07b1285a":"code","ab425f73":"code","4fa4e9a4":"code","ccc6e29f":"code","a78662b5":"code","88a7490b":"code","e3272fe0":"code","49490988":"code","c8676a78":"code","9e5c316b":"code","f5b33aca":"code","d568ee4d":"code","1dcfdb9e":"code","10c61e9e":"code","c07618e9":"code","7faa3735":"code","8e41011a":"code","cec4d5c2":"code","76752040":"markdown","ee25e967":"markdown","0c498d36":"markdown","d597450d":"markdown","07b416a7":"markdown","c8bceb25":"markdown","d078c497":"markdown","2af7c12c":"markdown","42d58aa7":"markdown","ad4c04cf":"markdown","9027d171":"markdown","f2e6215e":"markdown","d2c0438b":"markdown","9bc7240b":"markdown","281093c6":"markdown","38c76ee9":"markdown","88ee633f":"markdown","013d12b5":"markdown","6054e485":"markdown","772f3b75":"markdown","759ded34":"markdown","e2f4d912":"markdown","afdfbc67":"markdown","83e7b165":"markdown","819750b3":"markdown","12edbb2b":"markdown","3e2e618d":"markdown","793747a2":"markdown","7f42abbf":"markdown","51a668b1":"markdown","0ac378e7":"markdown","1b324084":"markdown","0cda880c":"markdown","02434166":"markdown","89d8f365":"markdown","c0c2d421":"markdown","1ebc97ce":"markdown","4e928f86":"markdown","9f0db99f":"markdown","6f5fc0bc":"markdown","dc687786":"markdown","d87339cc":"markdown","4d2c227a":"markdown","65b3f347":"markdown","77fa0cc6":"markdown","b9cb405f":"markdown","2eaf57e7":"markdown","19dc4863":"markdown","938a680e":"markdown","73d5857c":"markdown","7bd24f41":"markdown","7e45549c":"markdown","ca584b9d":"markdown","30df01a5":"markdown","cedb42b7":"markdown","23b23c93":"markdown","c0b8d0d6":"markdown","e441c0cc":"markdown","2e4ea04c":"markdown","bc35727a":"markdown","f8cc20a8":"markdown","0073fc48":"markdown"},"source":{"d8af959e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","27286dc0":"import numpy as np\nimport pandas as pd \nfrom sklearn import metrics\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy import stats\nfrom scipy.stats.mstats import winsorize\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor #to calculate VIF values\nfrom scipy.stats import pearsonr #TO CHECK INDIVIDUAL CORRELATIONS\nfrom sklearn.model_selection import train_test_split\n","d8a14f3d":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","bdd88ca2":"train_df = pd.read_csv('..\/input\/indore-city-house-price-predection\/IndoreHP_Train.csv')\ntest_df = pd.read_csv('..\/input\/indore-city-house-price-predection\/IndoreHP_Test.csv')","23acc68a":"print(\"Shape of Train Dataset \",train_df.shape)\nprint(\"Shape of Test Dataset \",test_df.shape)","e40c735d":"test_df.describe().T","243c67c0":"train_df.describe().T","0715bcbe":"test_df.info()","3e345099":"train_df.info()","2ac89975":"#Checking Duplicates\ntrain_df.duplicated().sum()","6ee0051a":"train_df.rename(columns = {\"nitric oxides concentration\":\"NOC\", \"#rooms\/dwelling\":\"RD\", 'RIVER_FLG':'RIVER' }, inplace = True)\ntest_df.rename(columns = {\"nitric oxides concentration\":\"NOC\", \"#rooms\/dwelling\":\"RD\", 'RIVER_FLG':'RIVER'}, inplace = True)","b481aaf4":"#fetching a list of features.\n#use whenever needed\ntrain_df.columns","6d329987":"train_df.rename(columns = {'PTRATIO  ':'PTR'}, inplace = True)\ntest_df.rename(columns = {'PTRATIO  ':'PTR'}, inplace = True)","98f3b4c7":"train_df","02ed937c":"X = train_df.drop(['MEDV'], axis = 1)\ny = train_df['MEDV']","e9028593":"# I use model_selection class from sklearn library\n# train_df has been split into ratio 0.7\/0.3\n#random_state has been kept at 4 (seed)\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 4)","9b4fd46d":"#give summary stats of Linear Regression with OLS class of statsmodule library\n\nsm.OLS(y_train,X_train).fit().summary()","e010dc78":"# VIF dataframe\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X_train.columns\n  \n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(X_train.values, i)\n                          for i in range(len(X_train.columns))]\n  \nprint(vif_data)","52c42036":"ax = plt.hist(X_train['ZN'])","07d7797d":"#removing it from all concerned datasets\n\nX_train.drop(['ZN'],axis=1,inplace=True)\nX_test.drop(['ZN'],axis=1,inplace=True)\ntest_df.drop(['ZN'],axis=1,inplace=True)","309cf4ea":"sm.OLS(y_train,X_train).fit().summary()","45c3b635":"vif_data = pd.DataFrame()\nvif_data[\"feature\"] = X_train.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X_train.values, i)\n                          for i in range(len(X_train.columns))]\nprint(vif_data)","c069437c":"pearsonr(X_train['INDUS'], X_train['NOC'])","842e2789":"pearsonr(X_train['INDUS'], X_train['DIS'])","bd6e0216":"X_train.drop(['INDUS'],axis=1,inplace=True)\nX_test.drop(['INDUS'],axis=1,inplace=True)\ntest_df.drop(['INDUS'],axis=1,inplace=True)","66273ac2":"#again checking the summary of OLS values with new dataset\nsm.OLS(y_train,X_train).fit().summary()","6cbcb9c7":"vif_data = pd.DataFrame()\nvif_data[\"Attribute\"] = X_train.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X_train.values, i)\n                          for i in range(len(X_train.columns))]\nprint(vif_data)","e8b15269":"ax = sns.boxplot(x=X_train['RAD'],y=y_train)","c7d5b02e":"#removing it from all concerned datasets\n\nX_train.drop(['RAD'],axis=1,inplace=True)\nX_test.drop(['RAD'],axis=1,inplace=True)\ntest_df.drop(['RAD'],axis=1,inplace=True)","303554ea":"#again checking the summary of OLS values with new dataset\nsm.OLS(y_train,X_train).fit().summary()","0b291950":"vif_data = pd.DataFrame()\nvif_data[\"Attribute\"] = X_train.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X_train.values, i)\n                          for i in range(len(X_train.columns))]\nprint(vif_data)","c7d2d3b1":"#removing it from all concerned datasets\n\nX_train.drop(['AGE'],axis=1,inplace=True)\nX_test.drop(['AGE'],axis=1,inplace=True)\ntest_df.drop(['AGE'],axis=1,inplace=True)","16525d25":"# Model training\nlm=LinearRegression()\nlm.fit(X_train, y_train)","8b852a6f":"# Model prediction on train data\ny_pred = lm.predict(X_train)","acc3f5b3":"# Model Evaluation metrics\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","db53c1fb":"# Actual VS predicted values\nax = sns.regplot(y_train, y_pred)","f626be52":"# Checking Heteroskedastcity\n#residuals = y_train-y_pred\n#plotting residuals against predicted values\n#we assume linearity and net sum of errors to be zero\n\nax = sns.regplot(y_pred,y_train-y_pred)","2e2ca2dc":"# Checking Normality assumption of errors\nax = sns.distplot(y_train-y_pred)","e5bcbf8d":"# Predicting Test data with the model\ny_testdf_pred = lm.predict(test_df)\nlm.fit(X_train, y_train)\ny_pred = lm.predict(X_test)","78854c31":"#feeding output to the hidden test data and check the RMSE value\noutput = pd.DataFrame({'ID': test_df.ID,\n                       'MEDV': y_testdf_pred})\noutput.to_csv('.\/submission.csv', index=False)\noutput","c4197fd9":"#again checking the summary of OLS values with new dataset\nsm.OLS(y_train,X_train).fit().summary()","baa4ad20":"vif_data = pd.DataFrame()\nvif_data[\"Attribute\"] = X_train.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X_train.values, i)\n                          for i in range(len(X_train.columns))]\nprint(vif_data)","90506d0a":"X_train.corr()","baaadd24":"ax = plt.hist(X_train['TAX'])","b1b44964":"ax = sns.regplot(x=X_train['TAX'],y=y_train)","4d03bf28":"#removing it from all concerned datasets\n\nX_train.drop(['TAX'],axis=1,inplace=True)\nX_test.drop(['TAX'],axis=1,inplace=True)\ntest_df.drop(['TAX'],axis=1,inplace=True)","1519d520":"# Model training\nlm=LinearRegression()\nlm.fit(X_train, y_train)\n\n\n# Model prediction on train data\ny_pred = lm.predict(X_train)\n\n\n# Model Evaluation metrics\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","d7640dc0":"# Actual VS predicted values\nax = sns.regplot(y_train, y_pred)","7d4a6af8":"# Checking Heteroskedastcity\nax = sns.regplot(y_pred,y_train-y_pred)","3e86d416":"# Checking Normality assumption of errors\nax = sns.distplot(y_train-y_pred)","ed833938":"# Predicting Test data with the model\ny_testdf_pred = lm.predict(test_df)\nlm.fit(X_train, y_train)\ny_pred = lm.predict(X_test)\n\n#feeding output to the hidden test data and check the RMSE value\noutput = pd.DataFrame({'ID': test_df.ID,\n                       'MEDV': y_testdf_pred})\noutput.to_csv('.\/submission.csv', index=False)\noutput","798bfb92":"#again checking the summary of OLS values with new dataset\nsm.OLS(y_train,X_train).fit().summary()","257cb439":"vif_data = pd.DataFrame()\nvif_data[\"Attribute\"] = X_train.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X_train.values, i)\n                          for i in range(len(X_train.columns))]\nprint(vif_data)","2a58ec98":"ax = plt.hist(X_train['B'])","344d578a":"X_train.columns","efce751d":"#set dimensions for better visualization with ax in subplot\n#all features are numerical. so go ahead with a for loop and get a boxplot for each\n\nfor i in X_train:\n    fig, ax = plt.subplots(figsize=(10,5))\n    sns.boxplot(X_train[i], orient='h',palette='Set2') \n    plt.show()","d2daf628":"X_train['CRIM']=(X_train['CRIM'])**(1\/3)\nsns.boxplot(X_train['CRIM'])\nprint(X_train['CRIM'].mean())","557c2e01":"X_train['CRIM']=winsorize(X_train['CRIM'],limits=(0,0.05))\nsns.boxplot(X_train['CRIM'])\nprint(X_train['CRIM'].mean())","7d7d6f42":"X_train['NOC']=winsorize(X_train['NOC'],limits=(0,0.10))\nsns.boxplot(X_train['NOC'])\nprint(X_train['NOC'].mean())","07b1285a":"X_train['RD']=winsorize(X_train['RD'],limits=(0,0.05))\nsns.boxplot(X_train['RD'])\nprint(X_train['RD'].mean())","ab425f73":"X_train['LSTAT']=winsorize(X_train['LSTAT'],limits=(0,0.10))\nsns.boxplot(X_train['LSTAT'])\nprint(X_train['LSTAT'].mean())","4fa4e9a4":"X_train['B']=winsorize(X_train['B'],limits=(0.10,0))\nsns.boxplot(X_train['B'])\nprint(X_train['B'].mean())","ccc6e29f":"#again checking the summary of OLS values with new dataset\nsm.OLS(y_train,X_train).fit().summary()","a78662b5":"vif_data = pd.DataFrame()\nvif_data[\"Attribute\"] = X_train.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X_train.values, i)\n                          for i in range(len(X_train.columns))]\nprint(vif_data)","88a7490b":"#again checking the summary of OLS values with new dataset\nsm.OLS(y_train,X_train).fit().summary()","e3272fe0":"# Predicting Test data with the model\nlm.fit(X_train, y_train)\ny_testdf_pred = lm.predict(test_df)\n\n\n#feeding output to the hidden test data and check the RMSE value\noutput = pd.DataFrame({'ID': test_df.ID,\n                       'MEDV': y_testdf_pred})\noutput.to_csv('.\/submission.csv', index=False)\noutput","49490988":"lm.fit(X_train, y_train)\n\n\n# Model prediction on train data\ny_pred = lm.predict(X_train)\n\n\n# Model Evaluation metrics\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))\n","c8676a78":"X_train.corr()","9e5c316b":"X_train.drop(['CRIM'],axis=1,inplace=True)\nX_test.drop(['CRIM'],axis=1,inplace=True)\ntest_df.drop(['CRIM'],axis=1,inplace=True)","f5b33aca":"X_train.drop(['B'],axis=1,inplace=True)\nX_test.drop(['B'],axis=1,inplace=True)\ntest_df.drop(['B'],axis=1,inplace=True)","d568ee4d":"# Predicting Test data with the model\nlm.fit(X_train, y_train)\ny_testdf_pred = lm.predict(test_df)\n\n\n#feeding output to the hidden test data and check the RMSE value\noutput = pd.DataFrame({'ID': test_df.ID,\n                       'MEDV': y_testdf_pred})\noutput.to_csv('.\/submission.csv', index=False)\noutput","1dcfdb9e":"#again checking the summary of OLS values with new dataset\nsm.OLS(y_train,X_train).fit().summary()","10c61e9e":"vif_data = pd.DataFrame()\nvif_data[\"Attribute\"] = X_train.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X_train.values, i)\n                          for i in range(len(X_train.columns))]\nprint(vif_data)","c07618e9":"#again checking the summary of OLS values with new dataset\nsm.OLS(y_train,X_train).fit().summary()","7faa3735":"X_train.corr()","8e41011a":"# Predicting Test data with the model\nlm.fit(X_train, y_train)\ny_testdf_pred = lm.predict(test_df)\n\n\n#feeding output to the hidden test data and check the RMSE value\noutput = pd.DataFrame({'ID': test_df.ID,\n                       'MEDV': y_testdf_pred})\noutput.to_csv('.\/submission.csv', index=False)\noutput","cec4d5c2":"lm.fit(X_train, y_train)\n\n\n# Model prediction on train data\ny_pred = lm.predict(X_train)\n\n\n# Model Evaluation metrics\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))\n","76752040":"# LIBRARIES IMPORT","ee25e967":"## ITERATION I","0c498d36":"### B\n- My attention goes to B with a very low Beta Coefficient and high VIF value (80)\n- I do not want to think about removing it. Because it is calculated as 1000(Bk \u2014 0.63)\u00b2, where Bk is the proportion of people living is slums in the area, thus relates to the density of slum in the locality where the house of concern is located.\n- Let me plot its Hist","d597450d":"# ATTRIBUTE NAME TREATMENT\n- Since we are permanently changing the names of two attributes of Train data, we should do the same for Test data","07b416a7":"## ITERATION VI","c8bceb25":"## MODEL FIT TO SEE RMSE WITH GIVEN TEST DATA","d078c497":"## ITERATION V","2af7c12c":"* 'CRIM': highly right skewed.all non zero positive values. \n* 'RIVER': Almost all the values are zero (Categorial variable). \n* 'NOC' : Right skewed. all non zero positive values. log transformation.\n* 'RD' : Right skewed. all non zero positive values. log transformation.\n* 'DIS' : Not skewed\n* 'PTR' : Not skewed\n* 'B' : Highly left skewed\n* 'LSTAT':  Highly right skewed. all non zero positive values. log transformation.","42d58aa7":"- we don't find a strong correlation of MEDV prices and Radial Indices. There is a strong variance wrt prices in both RAD values 3 and 8. And median values remain the same.\n- Also, we have a lot of outlier values with RAD=5.\n- VIF value is also significant (over 10).\n- So, we drop the attribute RAD","ad4c04cf":"So net sum of errors is zero.","9027d171":"- No duplicates. Yay.\n","f2e6215e":"RMSE HAS come out to be 9.91071","d2c0438b":"- Condition number has gone down. But not so much.","9bc7240b":"## INTERMEDIATE MODEL VALIDATION\n### After Iteration IV","281093c6":"# ITERATION IV","38c76ee9":"## INTERMEDIATE MODEL VALIDATION\n### After Iteration IV","88ee633f":"Its distribution is not also so skewed that some transformation may help.","013d12b5":"Almost Linear. We can check the model with the test data given to us.","6054e485":"- Train dataset has one row more that Test, which is expected. ","772f3b75":"### After treatment","759ded34":"Let us go for log transformation for 'CRIM','NOC','RD','LSTAT'","e2f4d912":"Also linear predicted line.","afdfbc67":"### RAD\n- Still a lot of high VIF values. But we cannot just remove the attributes.\n- RAD ( The index of accessibility to radial highways) has very low coef value. But it is an ORDINAL categorical variable. We cannot depend only on B values.\n- Ofcourse, intuitively, we may think that it may not have very high significance as we do not 'prefer' to live near radial highways of a city, thus increasing the MEDV of price. But of course some of us want accessibility\n- Let us draw a box plot with MEDV","83e7b165":"# BIRD'S EYE VIEW","819750b3":"LET us remove CRIM\n","12edbb2b":"Multicollinearity remains the major issue. Let us check the correlation matrix keeping in mind NOC RD and B","3e2e618d":"VIF values have gone down. But not so much.","793747a2":"- our target variable is MEDV (median value of owner-occupied homes in INR millions)","7f42abbf":"- no NaNs: So no need for missing values treatment\n- Categorical data already encoded. So need of Data Encoding\n- Names of troublesome attributes need to be changed. We shouldn't have spaces and the attribute names should not begin with special characters\n    - \"nitric oxides concentration\" to \"NOC\"\n    - \"#rooms\/dwelling\" to \"RD\"\n- Let us check for duplicated rows.","51a668b1":"### LSTAT\n- Winsorization\n","0ac378e7":"## MODEL FIT TO SEE RMSE WITH GIVEN TEST DATA","1b324084":"- The name PTRATIO has spaces. So renaming it as simple PTR","0cda880c":"### TAX\n- My focus now shifts to TAX with a very low beta coefficient and very high VIF.\n- It is the full-value property-tax rate per INR 10 millions (1 crores)\n- Let us check it distribution.","02434166":"## ITERATION II","89d8f365":"### NOC\n- Winsorization","c0c2d421":"## ITERATION III","1ebc97ce":"# DEALING WITH OUTLIERS","4e928f86":"# ITERATION VII","9f0db99f":"- F stat= 1483; & Prob (F-statistic): 3.25e-202 means we are in the right direction\n- Mod R-sq= 0.983: decently high for the current model\n- Conditional number has reduced from 8850 to 6810 \n- Let us check colliearity through VIF:","6f5fc0bc":"- F stat= 1331; & Prob (F-statistic): 2.35e-203 means we are in the right direction\n- Mod R-sq= 0.983: decently high for the current model\n- Conditional number= 8850; Same as before. \n- Let us check colliearity through VIF:","dc687786":"- F stat= 1331; & Prob (F-statistic): 2.35e-203 means we are in the right direction\n- Mod R-sq= 0.983: decently high for the current model\n- Conditional number= 8850; Same as before. \n- Let us check colliearity through VIF:","d87339cc":"### RD\n- Winsorization","4d2c227a":"- HIGH F stat= ; & LOW Prob (F-statistic) means we are in the right direction\n- Mod R-sq= 0.983: decently high for the current model\n- Conditional number= 8850; Same as before. \n- Let us check colliearity through VIF:\n","65b3f347":"- As it turns out, it indeed has a high positive correlation with NOC (0.67), indicating presence of factories \n- So even though we remove it, NOC will take care of its 'presence' in the model, which cannot be removed because of its very high be coefficient values (8.7)\n- Another point is also the negative high correlation of INDUS with DIS (the weighted distances to five employment centers of the town), which is expected because highly industrialized places will be great employment centres.\n- In sum, we can remove the INDUS feature without guilt.","77fa0cc6":"- F stat= 1197; & Prob (F-statistic): 4.08e-199 means we are in the right direction\n- Mod R-sq = 0.983: decently high for the current model\n- conditional number has gone down to 8850 from 9030. Let us check through VIF:","b9cb405f":"### INDUS\n- Let us shift our focus to INDUS, with a low coef of 0.0056 and high p-value of 0.922. (INDUS: 0.0056,0.058,0.098,0.922,-0.108,0.119)\n- It is the proportion of non-retail business acres per region of Indore.\n- Even intuitively it is not a major factor deciding the MEDV prices. \n- We don't care about non-retail business centres around us while setting the residential houses unless they are industries. It would be interesting to check its correlation with NOC, though.","2eaf57e7":"### Splitting target attribute and independent attribute","19dc4863":"### Indore City House Price Prediction (Linear Regression)\n##### Objective: Predict the median resale value of houses in Indore","938a680e":"### CRIM\n- Cube Root","73d5857c":"# DATA TRAINING WITH LINEAR REGRESSION \n## And interpretation of results with OLS\n- As mentioned in the beginning, I will solely focus on Linear Regression\n- Now let us put our data through a linear model and check the results","7bd24f41":"- Highly skewed.\n- Most of ZN values are 0, which is expected because bigger lots are usually used for commercial activities. \n- All the facts suggest its removal.","7e45549c":"- Decent Adjusted R^2\n- High MAE,MSE and RMSE.","ca584b9d":"## MODEL FIT TO SEE RMSE WITH GIVEN TEST DATA","30df01a5":"### Splitting to training and testing data ","cedb42b7":"- Highly skewed, which might be affected its performance\/contribution.\n- It is left skwed with all non-zero positive values.\n- What happens if we remove it? I want to be sure. So I create dummy datasets, play with them, and check RMSE value with the hidden test dataset, if my RMSE value is lower, I will go ahead and remove it for good.\n- But why not check for outliers and other variable also and treat them first to increase the efficiency of our model?","23b23c93":"- Not a very significant contributor to houses prices generally\n- As only high median prices houses are affected by it.\n- We can drop it without remorse. Let's see anyway","c0b8d0d6":"1. A high value of F statistic, with a very low p-value  implies that the null hypothesis can be rejected. This means there is a potential relationship between the predictors and the outcome.\n    - F stat= 1025; & Prob (F-statistic): 4.08e-199 means we are in the right direction\n2. R-squared (R\u00b2) measures the proportion of variability in the outcome that can be explained by the model, and is almost always between 0 and 1; the higher the value, the better the model is able to explain the variability in the outcome. However, increase in number of predictors mostly results in an increased value of R\u00b2 due to inflation of R-squared. Adjusted R-squared adjusts the value of R\u00b2 to avoid this effect. A high value of adjusted R\u00b2  shows that more than (adjusted R\u00b2)% of the variance in the data is being explained by the model.\n    - Mod R-sq = 0.983: decently high for the current model\n3. Multicollinearity - two or more predictors are highly related to each other, and hence one predictor can be used to predict the value of the other. The problem with multi-collinearity is that it can make it harder to estimate the individual effects of the predictors on the outcome. It can be detected using the Variance Inflation Factor (VIF). VIF of any predictor is the ratio of variance of its estimated coefficient in the full model to the variance of its estimated coefficient when fit on the outcome only by itself (as in simple linear regression).  A VIF of 1 indicates no presence of multicollinearity. Usually, a VIF value of above 5 or 10 is taken as an indicator of multicollinearity. ","e441c0cc":"### ZN\n- Let's focus on ZN\n- ZN: 0.0188, 0.014\t1.369, 0.172, -0.008, 0.046\n- It is the proportion of residential land zoned for lots larger than 25,000 sq.ft.\n- Intuitively it might affect the MEDV of high-price houses, but not the low ones. Not very useful information.\n- ZN has low B beta coefficient (0.0188), and high P value (0.172). So, it has low impact on Model and is statistically insignificant\n- So, can be dropped. Let us plot a histogram","2e4ea04c":"* DIS is showing high collinearity with CRIM and NOC\n* LSTAT is showing high collinearity with RD\n* NOC is showing high collinearity with CRIM and DIS","bc35727a":"RMSE COMES OUT TO BE 10.02649. We seek a lower value.","f8cc20a8":"### AGE\n- Let us look at AGE: the proportion of owner-occupied units built prior to 1940 (in percent)\n- A negative correlation of 0.0426 implies higher the proper of 'old' houses in the area, lower would be MEDV. But it comes in the lower bound. \n- Consider its very high p-vsalue coupled with low Beta coefficient value.\n- Its VIF value is also high (15.4)\n- I think dropping it would be better for overall performance of the model.","0073fc48":"### Data fields\n1. CRIM: per capita crime rate\n2. ZN: the proportion of residential land zoned for lots larger than 25,000 sq.ft.\n3. INDUS: the proportion of non-retail business acres per town.\n4. RIVER_FLG: River Dummy Var (this is equal to 1 if tract bounds river; 0 otherwise)\n5. nitric oxides concentration: the nitric oxides concentration (parts per 10 million)\n6. #rooms\/dwelling: the average number of rooms per dwelling\n7. AGE: the proportion of owner-occupied units built prior to 1940\n8. DIS: the weighted distances to five employment centers of the town\n9. RAD: the index of accessibility to radial highways\n10. TAX: the full-value property-tax rate per INR 10 millions (1 crores)\n11. PTRATIO: the pupil-teacher ratio by area \n12. B: calculated as 1000(Bk \u2014 0.63)\u00b2, where Bk is the proportion of people living is slums in the area\n13. LSTAT: This is the percentage lower status and income of the population\n14. MEDV: This is the median value of owner-occupied homes in INR millions (10 lakhs)\n\n- Each record in the database describes an Indore suburb."}}