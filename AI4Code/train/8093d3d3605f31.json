{"cell_type":{"c5f464a5":"code","03c29170":"code","1a7f2b03":"code","6a5f9d1d":"code","3f9ba474":"code","90682f82":"code","c348dd94":"code","1459d877":"code","f97d2900":"code","08e8f453":"code","cb9be6ef":"code","6c357913":"code","d3ff5064":"code","5bbd1105":"code","a9bfe0d2":"code","802736e2":"code","b038a2a8":"code","ee9b818f":"code","f24c1505":"code","e2fc1828":"code","91cd1d5d":"code","a09641d7":"code","0f0b171a":"code","7ac0bd08":"code","2757847c":"code","9a0a702d":"code","6f9bb6f6":"code","1c9bed2f":"code","07fc9be1":"code","195ae526":"code","dfed1983":"code","9902ae5a":"code","f70580c4":"code","19bbdc38":"code","6de05359":"markdown","3e3cc6dc":"markdown","e962de0e":"markdown","f343822c":"markdown","ae6a1138":"markdown","3da6b4ff":"markdown","f96ed7bf":"markdown","d8ef7dcd":"markdown","f1f085b4":"markdown","943fe403":"markdown","734f7b70":"markdown","8b37397e":"markdown","58603ebe":"markdown","4168a155":"markdown","067c86b1":"markdown","a3e13948":"markdown","56809f5d":"markdown","0521b52e":"markdown","ce49c276":"markdown","7e503e1e":"markdown","4b211856":"markdown","917dde50":"markdown","caae8129":"markdown","60f79410":"markdown"},"source":{"c5f464a5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","03c29170":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Flatten, Input\nimport matplotlib.pyplot as plt","1a7f2b03":"# Loading the data by using Pandas read_csv \ndata = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv') \ndata_sub = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n\n# Spliting the data into a matrix of features X and vector of labels y \nX = data.iloc[:, 1:]\ny = data.iloc[:, 0]","6a5f9d1d":"import seaborn as sns\nfig = plt.figure(figsize=(11,7))\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \".85\"})\nsns.barplot(y.unique(), y.value_counts(), color='tab:green')\nplt.title('Class distribution', fontdict={'fontsize' : 15})\nplt.ylabel('Value count', fontdict={'fontsize' : 12}, rotation=0, labelpad=40)\nplt.xlabel('Digit class', fontdict={'fontsize' : 12})\nplt.show()","3f9ba474":"import random\n\ndef viz(index):\n    # Helper function\n    plt.imshow(np.array(X.iloc[index, :]).reshape((28,28)), cmap='binary_r')\n    plt.axis('off')\n    plt.show()\n    print(f'Label : {y[index]}')\n    \ndef viz_multiple():\n    # Sampling and visualzing 45 random digits from MNIST\n    fig = plt.figure(figsize=(15,10))\n    for i, v in enumerate([random.randint(0, X.shape[0]) for x in range(45)]):  \n        plt.subplot(10, 15, i+1)\n        plt.imshow(np.array(X.iloc[v, :]).reshape((28,28)),cmap='binary_r')\n        plt.axis('off')\n    plt.subplots_adjust(wspace=-0.1, hspace=-0.1)\n    plt.show()\n    \nviz_multiple()","90682f82":"viz(2)","c348dd94":"# Spliting and normalizing X's (dividing 255 which is maximum value an RGB image can have)\n# You can also use Sci-kit Learn train test split, and Normalization\n\nX_train, X_test = X[:40000]\/255.0, X[40000:]\/255.0\ny_train, y_test = y[:40000], y[40000:]\nX_valid, y_valid = X_train[:10000], y_train[:10000]","1459d877":"def parabola():\n    sns.set_style('white')\n    x = np.linspace(-10, 10, 1000)\n    y = x**2  \n    fig, ax = plt.subplots(figsize=(9,7))\n    ymin = min(y)\n    xpos = np.argmax(ymin)\n    xmin = x[xpos]\n    ax.annotate('Minimum', xy=(xmin+10, ymin), xytext=(xmin+8, ymin+20),\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\n    ax.plot(x, y)\n    plt.title('Cost vs weights', fontdict={'fontsize' : 14})\n    plt.ylabel(r'${J(\\Theta)}$', rotation=0, labelpad=20, fontdict={'fontsize' : 12})\n    plt.xlabel('Weight', fontdict={'fontsize' : 12})\n    plt.tick_params( \n        axis='both', which='both',\n        bottom = False, top = False,\n        labelbottom=False, labelleft=False) \n    plt.show()\n    \nparabola()","f97d2900":"model = tf.keras.models.Sequential([\n    Input(shape=X_train.shape[1:]), # Input layer\n    Dense(256, activation='sigmoid'), # First hidden layer\n    Dense(128, activation='sigmoid'),  # Second hidden layer\n    Dense(10, activation='softmax'),  # Output layer\n])\nmodel.compile(optimizer='sgd',\n                loss='sparse_categorical_crossentropy',\n                metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, epochs=20,\n                    validation_data=(X_valid, y_valid))","08e8f453":"def view_charts():\n    fig, ax = plt.subplots(1,2, figsize=(20,7))\n    ax[0].plot(history.history['accuracy'])\n    ax[0].plot(history.history['val_accuracy'])\n    ax[0].set_title('Accuracy')\n    ax[0].set_ylabel('Accuracy')\n    ax[0].set_xlabel('Epoch')\n    ax[0].grid()\n    ax[1].plot(history.history['loss'])\n    ax[1].plot(history.history['val_loss'])\n    ax[1].set_title('Loss')\n    ax[1].set_ylabel('Loss')\n    ax[1].set_xlabel('Epoch')\n    ax[1].grid()\n    plt.show()\n    \nview_charts()","cb9be6ef":"model.evaluate(X_test, y_test)","6c357913":"model = tf.keras.Sequential([\n    Input(shape=X_train.shape[1:]), \n    Dense(256, activation='relu'), # sigmoid => relu\n    Dense(128, activation='relu'), # sigmoid => relu\n    Dense(10, activation='softmax'),\n])\nmodel.compile(optimizer='RMSProp', # sgd => RMSProp\n                loss='sparse_categorical_crossentropy',\n                metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, epochs=20,\n                    validation_data=(X_valid, y_valid))","d3ff5064":"view_charts()","5bbd1105":"print('Accuracy {}'.format(np.round(model.evaluate(X_test, y_test)[1], 4)))","a9bfe0d2":"from tensorflow.keras.layers import BatchNormalization, Dropout \n\nmodel = tf.keras.Sequential([\n    Input(shape=X_train.shape[1:]),\n    Dense(256, activation='relu'),\n    BatchNormalization(), # Batch normalization layer\n    Dropout(0.1), # Droput layer\n    Dense(128, activation='relu'),\n    BatchNormalization(), # Batch normalization layer\n    Dropout(0.45), # Droput layer\n    Dense(10, activation='softmax'),\n])\nmodel.compile(optimizer='RMSProp',\n                loss='sparse_categorical_crossentropy',\n                metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, epochs=20,\n                    validation_data=(X_valid, y_valid))","802736e2":"view_charts()","b038a2a8":"print('Accuracy {}'.format(np.round(model.evaluate(X_test, y_test)[1], 4)))","ee9b818f":"# Reshaping data into 28x28x1 format\nX_train, X_valid, X_test = np.array(X_train).reshape(-1, 28, 28, 1), np.array(X_valid).reshape(-1, 28, 28, 1), np.array(X_test).reshape(-1, 28, 28, 1)","f24c1505":"from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, AveragePooling2D\n\nmodel = tf.keras.models.Sequential([\n    Input(shape=X_train.shape[1:]),\n    Conv2D(32, 7, activation='relu', padding='same'), # Convolution layer\n    Conv2D(32, 5, activation='relu', padding='same'), # Convolution layer\n    MaxPooling2D(pool_size=(2,2)), # Pooling\n    BatchNormalization(),\n    Dropout(0.3),\n    Conv2D(64, 5, activation='relu', padding='same'), # Convolution layer\n    Conv2D(64, 5, activation='relu', padding='same'), # Convolution layer\n    MaxPooling2D(pool_size=(2,2)), # Pooling\n    BatchNormalization(),\n    Dropout(0.3),\n    Flatten(),\n    Dense(256, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.5),\n    Dense(10, activation='softmax')\n])\n\nmodel.compile(optimizer='RMSProp',\n             loss='sparse_categorical_crossentropy',\n             metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train,\n         epochs=20,\n         validation_data=(X_valid, y_valid))","e2fc1828":"view_charts()","91cd1d5d":"print('Accuracy {}'.format(np.round(model.evaluate(X_test, y_test)[1], 4)))","a09641d7":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(\n    rotation_range=15,\n    zoom_range = 0.1,  \n    width_shift_range=0.1,  \n    height_shift_range=0.1,\n    validation_split = 0.25\n)\n\nvalid_datagen = ImageDataGenerator(\n    rotation_range=15,\n    zoom_range = 0.1,  \n    width_shift_range=0.1,  \n    height_shift_range=0.1,\n    validation_split = 0.25\n)\n\ntrain_datagen.fit(X_train)\nvalid_datagen.fit(X_valid)\n\ntrain_generator = train_datagen.flow(X_train, y_train, batch_size=50, subset='training')\nvalid_generator = valid_datagen.flow(X_valid, y_valid, batch_size=50, subset='validation')","0f0b171a":"def viz_data_aug():\n    fig = plt.figure(figsize=(15,10))\n    for i in range(45):  \n        plt.subplot(10, 15, i+1)\n        plt.imshow(np.array(train_generator[random.randint(0,599)][0][0]).reshape(28,28))\n        plt.axis('off')\n    plt.subplots_adjust(wspace=-0.1, hspace=-0.1)\n    plt.show()\nviz_data_aug()","7ac0bd08":"history = model.fit_generator(generator=train_generator,\n                                validation_data=valid_generator,\n                                epochs = 20)","2757847c":"view_charts()","9a0a702d":"print('Accuracy {}'.format(np.round(model.evaluate(X_test, y_test)[1], 4)))","6f9bb6f6":"# Callbacks implenetation\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', save_best_only=True)\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=20)","1c9bed2f":"history = model.fit_generator(generator=train_generator,\n                                validation_data=valid_generator,\n                                epochs = 200, \n                                callbacks=[checkpoint, early_stopping])","07fc9be1":"model = tf.keras.models.load_model('model.h5')\nmodel.evaluate(X_test, y_test)","195ae526":"data_sub = np.array(data_sub).reshape(-1, 28, 28 , 1).astype('float32') \/ 255","dfed1983":"preds = model.predict(data_sub)","9902ae5a":"np.argmax(preds[0])","f70580c4":"labels = [np.argmax(x) for x in preds]\nids = [x+1 for x in range(len(preds))]\n\nsub = pd.DataFrame()","19bbdc38":"sub['ImageId'] = ids\nsub['Label'] = labels\n\nsub.to_csv('mnist_submission.csv', index=False)\n\npd.read_csv('mnist_submission.csv')","6de05359":"To find minimum we use an algorithm called **Gradient Descent**. It works by randomly choosing a point on the function's graph, and with each iteration it takes a little step towards the minimum. The step size is called *learning rate* most often the default it is set to 0.01, but usually it is worth tweaking this parameter, but be carefull since too small value of learning rate will make a Gradient Descent very slow and too big value of learning rate will mean that your steps are very big and potentially you can overstep minimum, thus Gradient Descent will fail to converge. \n<img src='https:\/\/rasbt.github.io\/mlxtend\/user_guide\/general_concepts\/gradient-optimization_files\/ball.png'>\n\n<small>Image source : https:\/\/rasbt.github.io\/mlxtend\/user_guide\/general_concepts\/gradient-optimization_files\/ball.png'<\/small>","3e3cc6dc":"**Augmented data visualized**","e962de0e":"**After just 20 epochs we were able to train the model to predict with over 90% accuracy!**\nIf you ever trained something like SVM or Logistic Regression on MNIST you'll agree that neural net way is much faster, more accurate and easier","f343822c":"And single digit:","ae6a1138":"**Keras provides very usefull callbacks for models**\n* Checkpoint - saves model with lowest loss \n* Early stopping - stops model when it dosn't improve for over m epochs","3da6b4ff":"**Let's visualize our data**\n\nBecouse our data has 784 features ( in reality images have 784 pixels ) and we want to see each image we have to reshape our data into 28x28 matrices ( $\\sqrt{784}=28$ )\nBelow I've created helper functions to visualize data.","f96ed7bf":"**Learning: Gradient Descent**\n\nOur neural network model in reality is a function $h(\\Theta)=\\theta_{1}x_{1} + \\theta_{2}x_{2} + \\theta_{3}x_{3} + ...\\theta_{n}x_{n}$ where $\\Theta$ is a vector of parameters $\\theta_{1}$, $\\theta_{2}$, $\\theta_{3}$, ...$\\theta_{n}$; $x_{1}, x_{2}, x_{3}, ...x_{n}$ are our features and $n$ is the number of features present in the data ( in MNIST case 784 ). The goal of learning is to optimize our $\\Theta$ so it predicts our $y$ most effectively. First piece of puzzle to create such a system is the **cost function**.\n\n**Cost function** is a way to measure how badly our model did and pentalize it for doing so in it's vectorized form it looks like this $$J(\\Theta) = \\frac{1}{2m}(X\\Theta - y)^T(X\\Theta - y)$$ $X$ - is matrix of features <br>\n$m$ - training examples <br>\n$y$ - vector of actual values <br>\n$\\Theta$ - vector of parameters\n\nIn order to teach our model we must minimize the cost function, for a simple dataset cost function may just be a quadratic function and finding a minimum shouldn't be a problem  ","d8ef7dcd":"# Data augmentation\n\nData augmentation is a way to increase dataset size by adding altered versions of exisiting images, such as creating noise, blur, zooming, rotation etc. Data augemntation is worth trying if your model keeps overfitting","f1f085b4":"**Now it's time to split data into train and test set and normalize it**","943fe403":"# Submission","734f7b70":"**Hey!**\n\nWhen I study Machine Learning i make my notes by creating Kaggle notebooks or joining competitions, I decided to make this notebook public, if you find it helpfull upvote :)). \n**Bonne lecture!**","8b37397e":"**But how do we compute predictions, how do we get from input layer to output layer?**\n**Learning: forward propagation**\n\nWe know that for our digit case input layer has 784 neurons, each neuron corresponds to one pixel on 28x28 image grid, and that pixel has a certain value (initial between 0-255, after normalizing between 0-1), each neuron has a weigh (parameter), input layer after getting \"fed\" with pixel values passes them into first hidden layer. hidden layer has a **activation function** such as sigmoid $$f(x)=\\frac{1}{1 + e^{-x}}$$ or ReLU $$f(x)=max(0,x)$$Activation function tells us how much a neuron \"fires up\" given it's input. First hidden layer after squishing and appyling weights passes the values into second hidden layer, and this process repeats untill values meet the output layer where in this case we use softmax activation function $\\sigma(z)=\\frac{e^{z_{i}}}{\\Sigma^K_{j=1}e^{\\beta zj}}$ which tells us the probability of affiliation with a class, then we run Gradient Descent to see how badly we did. ","58603ebe":"Classes are evenly distributed, this makes model building easier, becouse we don't have to think about disproportions in learning","4168a155":"**Learning: Backpropagation**","067c86b1":"**By replacing optimizer and activation functions we have easliy got to over 96% accuracy, much better!**\n\nWhen we take a look at accuracy during learning we notice that it is much higher than when we evaluate the model, this is called **overfitting**, it occurs when model learns very well for the training data but fails to generalize it's \"knowledge\" for validation data. We can reduce overfitting by getting more data or introducing **regularization**. Regulariziation discourages the model from learning very complex patterns, thus eliminating flexibility of our model which in turn will mean that our model genralizes better!\n\n**Let's see how we can regularize our model**\n\n* Dropout - Dropout deactivates random nodes in our network, why this works? Becouse network can't rely on certain nodes to \"carry\" the model, it has to distribute weights in a better way.\n* Batch normalizatin - batch normalization standardizes the input layers for each mini-batch. This improves network's stability and efficenciy.\n\n**Now let's see how we can implement those methods into our neural net!**","a3e13948":"# Optimizers\nIn our model we used Stochastic Gradient Descent, which is just a more random version of previous Gradient Descent. However there are many optimizers to choose from, here is a list of most popular optimizers:\n* SGD\n* RMSProp\n* AdaDelta\n* AdaGrad\n* Adam\n\nHere is a link to very good article https:\/\/towardsdatascience.com\/optimizers-for-training-neural-network-59450d71caf6, it deeply explains how each algorithm works.","56809f5d":"**Let's now create a model with RMSProp optimizer and replace sigmoid activations with ReLU**","0521b52e":"# How neural networks learn","ce49c276":"**Now let's see how we can build a CNN!**","7e503e1e":"# Convolutional neural networks\n\n**Convolutional neural netowrks** are able to capture key image features by applying many filters, they reduce image size while retaining most important information, image must be in a $m x m x k$ format ( m-height \/ width, k-number of channels ).\n\n**Convolution layer** applies filters to an image and reduce the image's size.\n\nConvolution layer is usually followed by a **pooling** layer. Pooling is used to reduce image size even more ( all this reducing is needed for efficent computation ) and extract most domminant features from the image.\n\n<img src='https:\/\/miro.medium.com\/max\/3744\/1*CnNorCR4Zdq7pVchdsRGyw.png'>\n\n<small> Image source : https:\/\/miro.medium.com\/max\/3744\/1*CnNorCR4Zdq7pVchdsRGyw.png<\/small>","4b211856":"<h1>Data preparation<\/h1>\n<h3>Steps<\/h3>\n<ul>\n    <li>Loading in the data<\/li>\n    <li>Analyzing class distribution<\/li>\n    <li>Normalizing data and spliting into train and test split<\/li>\n<\/ul>    ","917dde50":"# Training a model\n* We are going to create keras sequential model\n* Input layer with the shape of our data\n* Two Dense layers with sigmoid activaiton function \n* And a Dense layer as a output with 10 neurons and softmax activation\n* We will compile the model with Stochastic Gradient Descent, sparse categorical crossentropy as a loss and measure the accuracy\n* Model will be trained for 20 epochs and validated on our validation data","caae8129":"**By regularizing our model we got to over 97% accuracy! That's great, but there is still room for improvement**","60f79410":"**Intuition**\n\nBasic neural network architecture looks like this:\n* Input layer\n* One or more hidden layers\n* Output layer\n\n*Each layer is built out of neurons*\n<img src='https:\/\/databricks.com\/wp-content\/uploads\/2019\/02\/neural1.jpg'>\n\n* Input layer takes in all of our features, in this case input layer has 784 neurons becouse that is the number of features\n* Hidden layer has a flexible number of neurons, in fact we can treat the number of neurons as a parameter\n* Output layer has 10 neurons becouse that is the ammount of classes in this dataset\n\n<small>Image source : https:\/\/databricks.com\/wp-content\/uploads\/2019\/02\/neural1.jpg<\/small>"}}