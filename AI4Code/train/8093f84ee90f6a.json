{"cell_type":{"d77aa12a":"code","870fee5e":"code","7258a0ac":"code","3abe7cc1":"code","8af01d8b":"code","a6c4819a":"code","9cec057b":"code","b0056ff3":"code","a289aa49":"code","a3f9310d":"code","eba690ea":"code","15d1e72a":"code","bdca522d":"code","d2d9502f":"code","5981e8b2":"code","0ca22802":"code","b2a1b5ea":"code","e28a3305":"code","9c9f106b":"code","ba64f94d":"code","fe93444d":"code","53393b61":"code","641a6596":"code","50c5b406":"code","fb7c7045":"code","787aeb6b":"code","1ebc85be":"code","70a3d3db":"code","14cc8890":"code","7f82307a":"code","2bdeec52":"code","f2fae5d8":"code","71fc7ee1":"code","22d8cc60":"code","c9c426ff":"code","347ce721":"code","49454ef1":"code","538b3016":"code","e22ab2a1":"code","96a2210f":"code","25dc5529":"code","529d8fb0":"code","2de22156":"code","272e77f3":"code","d5eebcb8":"code","f4e9b200":"code","d2a98a2f":"code","8e83a8b1":"code","e46c15e4":"code","71a7fe0a":"code","a2ba1d50":"code","a39e1999":"code","bad27ded":"code","2fd57639":"code","1cbbcf5c":"code","099da193":"markdown","0e072046":"markdown","6a90f5ba":"markdown","41789a3e":"markdown","1db0974c":"markdown","7717cd4e":"markdown","403f7f0f":"markdown","d7fd6bce":"markdown","70d1cc2b":"markdown","f9288e27":"markdown","3729838b":"markdown","7af43ea1":"markdown","77607741":"markdown","c78b7d17":"markdown","2bf0816e":"markdown","c2db024e":"markdown","1ea19ecb":"markdown","5ec46c57":"markdown","0f73b54e":"markdown","cf74da84":"markdown","dde45961":"markdown","e0e0c865":"markdown","7faf96ae":"markdown","95e443e4":"markdown","c475b602":"markdown","b6930777":"markdown","34d24d5a":"markdown","79c1b779":"markdown","bf8b47c3":"markdown","dd5f8ad6":"markdown","a0711942":"markdown","66980ffe":"markdown","a8a38c0b":"markdown","5f09c39e":"markdown","a535776e":"markdown","15321ef0":"markdown","023277eb":"markdown","7c483264":"markdown","101bfd23":"markdown","eed5a71f":"markdown","f07e3932":"markdown","e7c86e94":"markdown","724d6c59":"markdown","06748d7e":"markdown","ed7217ce":"markdown","298d4f92":"markdown"},"source":{"d77aa12a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization library  \nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport time\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","870fee5e":"df = pd.read_csv('..\/input\/data.csv')","7258a0ac":"df.head()  # head method show only first 5 rows","3abe7cc1":"# feature names as a list\ncol = df.columns       # .columns gives columns names in data \nprint(col)","8af01d8b":"# y includes our labels and x includes our features\ny = df.diagnosis                          # M or B ","a6c4819a":"y.describe()","9cec057b":"ax = sns.countplot(y,label=\"Count\")       # M = 212, B = 357\nB, M = y.value_counts()\nprint('Number of Benign: ',B)\nprint('Number of Malignant : ',M)","b0056ff3":"list = ['Unnamed: 32','id','diagnosis']\nx = df.drop(list,axis = 1 )\nx.head()","a289aa49":"# find the percentage of missing value\nmissing_values = x.isnull().sum()\/len(x)*100\n#sorted the missing columns based on descending order\nmissing_values[missing_values>0].sort_values(ascending = False)","a3f9310d":"# Saving missing values in a variable\na = x.isnull().sum()\/len(x)*100\n\n# saving column names in a variable\nvariables = x.columns\n\nvariable = []\nfor i in range(0,len(x.columns)):\n    if a[i]>=60:   #setting the threshold as 60%\n        variable.append(variables[i])\n\n        \nvariable","eba690ea":"#Let\u2019s first impute the missing values in the train set using the mode value of the known observations.\n# Known that we dont have any missing values in the columns\ncolumns = x.columns\n\nfor col in columns:\n    x[col].fillna(x[col].mode()[0],inplace = True)","15d1e72a":"x.isnull().sum()","bdca522d":"x.var().sort_values(ascending=True)","d2d9502f":"#remove the very less variant variables, since they dont carry much information\n\nnumeric = x.select_dtypes(include=[np.number])\nvar = numeric.var()\nnumeric = numeric.columns\nvariable = []\nfor i in range(0,len(var)):\n    if var[i]>=5:   #setting the threshold as 30%\n        variable.append(numeric[i])\n\nvar","5981e8b2":"variable","0ca22802":"x_less_var = x[variable]\nx_less_var.head()","b2a1b5ea":"x.corr()","e28a3305":"# less variance removed and checked the correlation on the rest of the items \nx[variable].corr(method ='kendall')","9c9f106b":"f , ax = plt.subplots(figsize = (14,12))\nplt.title('Correlation of Features- HeatMap',y=1,size=16)\nsns.heatmap(x[variable].corr(),square = True,  vmax=0.8)","ba64f94d":"# Create correlation matrix\ncorr_matrix = x[variable].corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper","fe93444d":"# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\nprint(to_drop)","53393b61":"# Drop features \nx_low_corr = x[variable].drop(x[variable][to_drop], axis=1)\n#final correlation table\nx_low_corr.corr()","641a6596":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(random_state=42,max_depth=8)\nx_rfc = pd.get_dummies(x)\nrfc.fit(x_rfc,y)","50c5b406":"features = x.columns\nimportances = rfc.feature_importances_\nindices = np.argsort(importances[0:20])  # top 5 features\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","fb7c7045":"from sklearn.feature_selection import SelectFromModel\n\nfeature = SelectFromModel(rfc)\n\nFit = feature.fit_transform(x, y)\n\nFit.shape","787aeb6b":"#load the data again\ndf = pd.read_csv('..\/input\/data.csv')\ny = df.diagnosis\ny.describe()","1ebc85be":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)","70a3d3db":"# Define dictionary to store our rankings\nfrom sklearn.preprocessing import MinMaxScaler\nranks = {}\n\n# Create our function which stores the feature rankings to the ranks dictionary\ndef ranking(ranks, names, order=1):\n    minmax = MinMaxScaler()\n    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n    ranks = map(lambda x: round(x,2), ranks)\n    return dict(zip(names, ranks))\n\n# Construct our Linear Regression model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\n\nlr = LogisticRegression()\nlr.fit(x, y_encoded)\n\n#stop the search when only the last feature is left\nrfe = RFE(lr, n_features_to_select=1,verbose=3)\nrfe.fit(x, y_encoded)\n\n\nrfe.ranking_","14cc8890":"ranking_list=[]\nfor rank in map(float, rfe.ranking_):\n    ranking_list.append(rank)","7f82307a":"ranks[\"RFE\"] = ranking(ranking_list, x.columns, order=-1)\n#ranks[\"RFE\"] = ranking(list(map(float, rfe.ranking_)), x.columns, order=-1)\n# Create empty dictionary to store the mean value calculated from all the scores\nr = {}\nfor name in x.columns:\n    r[name] = round(np.mean([ranks[method][name] \n                             for method in ranks.keys()]), 2)","2bdeec52":"r","f2fae5d8":"zip(r.keys(), r.values())","71fc7ee1":"# Put the mean scores into a Pandas dataframe\nmeanplot = pd.DataFrame(data=[(k, v) for (k, v) in r.items()], columns= ['Feature','Mean Ranking'])\n# Sort the dataframe\nmeanplot = meanplot.sort_values('Mean Ranking', ascending=False)\n# Let's plot the ranking of the features\nsns.factorplot(x=\"Mean Ranking\", y=\"Feature\", data = meanplot, kind=\"bar\",size=8, aspect=0.75, palette='coolwarm')","22d8cc60":"# first ten features\ndata_dia = y\ndata = x\ndata_n_2 = (data - data.mean()) \/ (data.std())              # standardization\ndata = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","c9c426ff":"# Second ten features\ndata = pd.concat([y,data_n_2.iloc[:,10:20]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","347ce721":"# Second ten features\ndata = pd.concat([y,data_n_2.iloc[:,20:31]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","49454ef1":"# As an alternative of violin plot, box plot can be used\n# box plots are also useful in terms of seeing outliers\n# I do not visualize all features with box plot\n# In order to show you lets have an example of box plot\n# If you want, you can visualize other features as well.\nplt.figure(figsize=(10,10))\nsns.boxplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90)","538b3016":"sns.jointplot(x.loc[:,'concavity_worst'], x.loc[:,'concave points_worst'], kind=\"regg\", color=\"#ce1414\")","e22ab2a1":"sns.set(style=\"white\")\ndf = x.loc[:,['radius_worst','perimeter_worst','area_worst']]\ng = sns.PairGrid(df, diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot, lw=3)","96a2210f":"sns.set(style=\"whitegrid\", palette=\"muted\")\ndata_dia = y\ndata = x\ndata_n_2 = (data - data.mean()) \/ (data.std())              # standardization\ndata = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\ntic = time.time()\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\n\nplt.xticks(rotation=90)","25dc5529":"data = pd.concat([y,data_n_2.iloc[:,10:20]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90)","529d8fb0":"data = pd.concat([y,data_n_2.iloc[:,20:31]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\ntoc = time.time()\nplt.xticks(rotation=90)\nprint(\"swarm plot time: \", toc-tic ,\" s\")","2de22156":"#correlation map\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","272e77f3":"drop_list1 = ['perimeter_mean','radius_mean','compactness_mean','concave points_mean','radius_se','perimeter_se','radius_worst','perimeter_worst','compactness_worst','concave points_worst','compactness_se','concave points_se','texture_worst','area_worst']\nx_1 = x.drop(drop_list1,axis = 1 )        # do not modify x, we will use it later \nx_1.head()\n\n    ","d5eebcb8":"#correlation map\nf,ax = plt.subplots(figsize=(14, 14))\nsns.heatmap(x_1.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","f4e9b200":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(x_1, y, test_size=0.3, random_state=42)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf = RandomForestClassifier(random_state=43)      \nclr_rf = clf_rf.fit(x_train,y_train)\n\nac = accuracy_score(y_test,clf_rf.predict(x_test))\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(y_test,clf_rf.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")","d2a98a2f":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# find best scored 5 features\nselect_feature = SelectKBest(chi2, k=5).fit(x_train, y_train)","8e83a8b1":"print('Score list:', select_feature.scores_)\nprint('Feature list:', x_train.columns)","e46c15e4":"x_train_2 = select_feature.transform(x_train)\nx_test_2 = select_feature.transform(x_test)\n#random forest classifier with n_estimators=10 (default)\nclf_rf_2 = RandomForestClassifier()      \nclr_rf_2 = clf_rf_2.fit(x_train_2,y_train)\nac_2 = accuracy_score(y_test,clf_rf_2.predict(x_test_2))\nprint('Accuracy is: ',ac_2)\ncm_2 = confusion_matrix(y_test,clf_rf_2.predict(x_test_2))\nsns.heatmap(cm_2,annot=True,fmt=\"d\")","71a7fe0a":"from sklearn.feature_selection import RFE\n# Create the RFE object and rank each pixel\nclf_rf_3 = RandomForestClassifier()      \nrfe = RFE(estimator=clf_rf_3, n_features_to_select=5, step=1)\nrfe = rfe.fit(x_train, y_train)\n","a2ba1d50":"print('Chosen best 5 feature by rfe:',x_train.columns[rfe.support_])","a39e1999":"from sklearn.feature_selection import RFECV\n\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nclf_rf_4 = RandomForestClassifier() \nrfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\nrfecv = rfecv.fit(x_train, y_train)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', x_train.columns[rfecv.support_])","bad27ded":"# Plot number of features VS. cross-validation scores\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score of number of selected features\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","2fd57639":"clf_rf_5 = RandomForestClassifier()      \nclr_rf_5 = clf_rf_5.fit(x_train,y_train)\nimportances = clr_rf_5.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf_rf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(x_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\n\nplt.figure(1, figsize=(14, 13))\nplt.title(\"Feature importances\")\nplt.bar(range(x_train.shape[1]), importances[indices],\n       color=\"g\", yerr=std[indices], align=\"center\")\nplt.xticks(range(x_train.shape[1]), x_train.columns[indices],rotation=90)\nplt.xlim([-1, x_train.shape[1]])\nplt.show()","1cbbcf5c":"# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n#normalization\nx_train_N = (x_train-x_train.mean())\/(x_train.max()-x_train.min())\nx_test_N = (x_test-x_test.mean())\/(x_test.max()-x_test.min())\n\nfrom sklearn.decomposition import PCA\npca = PCA()\npca.fit(x_train_N)\n\nplt.figure(1, figsize=(14, 13))\nplt.clf()\nplt.axes([.2, .2, .7, .7])\nplt.plot(pca.explained_variance_ratio_, linewidth=2)\nplt.axis('tight')\nplt.xlabel('n_components')\nplt.ylabel('explained_variance_ratio_')","099da193":"**What if we want to observe all correlation between features?** Yes, you are right. The answer is heatmap that is old but powerful plot method.","0e072046":"In this method we need to choose how many features we will use. For example, will k (number of features) be 5 or 10 or 15? The answer is only trying or intuitively. I do not try all combinations but I only choose k = 5 and find best 5 features.","6a90f5ba":"Lets interpret one more thing about plot above, variable of **concavity_worst** and **concave point_worst** looks like similar but how can we decide whether they are correlated with each other or not.\n(Not always true but, basically if the features are correlated with each other we can drop one of them)","41789a3e":"Before violin and swarm plot we need to normalization or standirdization. Because differences between values of features are very high to observe on plot. I plot features in 3 group and each group includes 10 features to observe better.","1db0974c":"### 1.Missing values","7717cd4e":"Accuracy is almost 95% and as it can be seen in confusion matrix, we make few wrong prediction. \nNow lets see other feature selection methods to find better results.","403f7f0f":"Chosen 5 best features by rfe is **texture_mean, area_mean, concavity_mean, area_se, concavity_worst**. They are exactly similar with previous (selectkBest) method. Therefore we do not need to calculate accuracy again. Shortly, we can say that we make good feature selection with rfe and selectkBest methods. However as you can see there is a problem, okey I except we find best 5 feature with two different method and these features are same but why it is **5**. Maybe if we use best 2 or best 15 feature we will have better accuracy. Therefore lets see how many feature we need to use with rfecv method.","d7fd6bce":"## 3) Recursive feature elimination (RFE) with random forest\n<http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html>\nBasically, it uses one of the classification methods (random forest in our example), assign weights to each of features. Whose absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on the pruned set until the desired number of features","70d1cc2b":"# Feature Extraction\n<http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html>\nWe will use principle component analysis (PCA) for feature extraction. Before PCA, we need to normalize data for better performance of PCA.\n ","f9288e27":"# Feature Selection and Random Forest Classification\nToday our purpuse is to try new cocktails. For example, we are finaly in the pub and we want to drink different tastes. Therefore, we need to compare ingredients of drinks. If one of them includes lemon, after drinking it we need to eliminate other drinks which includes lemon so as to experience very different tastes.","3729838b":"### Load the data","7af43ea1":"**Compactness_mean, concavity_mean and concave points_mean** are correlated with each other.Therefore I only choose **concavity_mean**. Apart from these, **radius_se, perimeter_se and area_se** are correlated and I only use **area_se**.  **radius_worst, perimeter_worst and area_worst** are correlated so I use **area_worst**.  **Compactness_worst, concavity_worst and concave points_worst** so I use **concavity_worst**.  **Compactness_se, concavity_se and concave points_se** so I use **concavity_se**. **texture_mean and texture_worst are correlated** and I use **texture_mean**. **area_worst and area_mean** are correlated, I use **area_mean**.\n\n\n","77607741":"> **Diagnosis** is our class label","c78b7d17":"Up to this point, we make some comments and discoveries on data already. If you like what we did, I am sure swarm plot will open the pub's door :) ","2bf0816e":"## 2) Univariate feature selection and random forest classification\nIn univariate feature selection, we will use SelectKBest that removes all but the k highest scoring features.\n<http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest>","c2db024e":"Remove id, since there is no important information\n\n**Unnamed: 32** feature includes NaN so we do not need it.","1ea19ecb":"## 1) Feature selection with correlation and random forest classification","5ec46c57":"Well, finaly we are in the pub and lets choose our drinks at feature selection part while using heatmap(correlation matrix).","0f73b54e":"### High Correlation Filter\n\nHigh correlation between two variables means they have similar trends and are likely to carry similar information. This can bring down the performance of some models drastically (linear and logistic regression models, for instance). \n\nGenerally, if the correlation between a pair of variables is greater than 0.5-0.6, we should seriously consider dropping one of those variables.","cf74da84":"### Backward Feature Elimination \/ Recursive Feature Elimination (RFE)\n\n* Firstly, take all the n variables present in our dataset and train the model using them\n* And then calculate the performance of the model\n* At this juncture, compute the performance of the model after eliminating each variable (n times), i.e., we drop one variable every time and train the model on the remaining n-1 variables\n* identify the variable whose removal has produced the smallest (or no) change in the performance of the model, and then drop that variable\n* Repeat this process until no variable can be dropped\n* This method can be used when building Linear Regression or Logistic Regression models.","dde45961":"### 2.Low Variance Filter\n\nConsider a variable in our dataset where all the observations have the same value, say 1. If we use this variable, do you think it can improve the model we will build? The answer is no, because this variable will have zero variance.\n\nThen drop the variables having low variance as compared to other variables in our dataset. ","e0e0c865":"Best 5 feature to classify is that **area_mean, area_se, texture_mean, concavity_worst and concavity_mean**. So lets se what happens if we use only these best scored 5 feature.","7faf96ae":"In this part we will select feature with different methods that are feature selection with correlation, univariate feature selection, recursive feature elimination (RFE), recursive feature elimination with cross validation (RFECV) and tree based feature selection. We will use random forest classification in order to train our model and predict. ","95e443e4":"### Random Forest\n\nRandom Forest is one of the most widely used algorithms for feature selection. It comes packaged with in-built feature importance so you don\u2019t need to program that separately. This helps us select a smaller subset of features\n\nWe need to convert the data into numeric form by applying one hot encoding, as Random Forest (Scikit-Learn Implementation) takes only numeric inputs. ","c475b602":"As it can be seen in map heat figure **radius_mean, perimeter_mean and area_mean** are correlated with each other so we will use only **area_mean**. If you ask how i choose **area_mean** as a feature to use, well actually there is no correct answer, I just look at swarm plots and **area_mean** looks like clear for me but we cannot make exact separation among other correlated features without trying. So lets find other correlated features and look accuracy with random forest classifier. ","b6930777":"As you can seen in plot above, after 5 best features importance of features decrease. Therefore we can focus these 5 features. As I sad before, I give importance to understand features and find best of them. ","34d24d5a":"# Visualization\nIn order to visualizate data we are going to use seaborn plots that is not used in other kernels to inform you and for diversity of plots. What I use in real life is mostly violin plot and swarm plot. Do not forget we are not selecting feature, we are trying to know data like looking at the drink list at the pub door.","79c1b779":"Well, we choose our features but **did we choose correctly ?** Lets use random forest and find accuracy according to chosen features.","bf8b47c3":"What about three or more feauture comparision ? For this purpose we can use pair grid plot. Also it seems very cool :)\nAnd we discover one more thing **radius_worst**, **perimeter_worst** and **area_worst** are correlated as it can be seen pair grid plot. We definetely use these discoveries for feature selection.","dd5f8ad6":"After drop correlated features, as it can be seen in below correlation matrix, there are no more correlated features. Actually, I know and you see there is correlation value 0.9 but lets see together what happen if we do not drop it.","a0711942":"# Conclusion\nShortly, I tried to show importance of feature selection and data visualization. \nDefault data includes 33 feature but after feature selection we drop this number from 33 to 5 with accuracy 95%. In this kernel we just tried basic things, I am sure with these data visualization and feature selection methods, you can easily ecxeed the % 95 accuracy. Maybe you can use other classification methods.\n### I hope you enjoy in this kernel\n## If you have any question or advise, I will be apreciate to listen them ...","66980ffe":"# INTRODUCTION \nIn this data analysis report focuses on feature visualization, selection and dimentional reduction methods. \n \n* Feature Selection\n    * Missing Value Ratio\n    * Low Variance Filter\n    * High Correlation Filter\n    * Random Forest\n    * Backward Feature Elimination\/Recursive Feature Elimination(RFE)\n    * Forward Feature Selection\n    \n\n* Dimensionality Reduction\n    * Components\/Factor Based\n        * Factor Analysis\n        * Principal Component Analysis(PCA)\n        * Singular Value Decomposition(SVD)\n        * Independent Component Analysis(ICA)\n    *  Projections Based\n        * ISOMAP\n        * t-Distributed Stochastic Neighbor Embedding (t-SNE)\n        * Uniform Manifold Approximation and Projection (UMAP)\n\n### Why is Dimensionality Reduction required?\n* Space required to store the data is reduced as the number of dimensions comes down.\n* Less dimensions lead to less computation\/training time.\n* Some algorithms do not perform well when we have a large dimensions. So reducing these dimensions needs to happen for the algorithm to be useful.\n* It takes care of multicollinearity by removing redundant features. For example, you have two variables \u2013 \u2018time spent on treadmill in minutes\u2019 and \u2018calories burnt\u2019. These variables are highly correlated as the more time you spend running on a treadmill, the more calories you will burn. Hence, there is no point in storing both as just one of them does what you require\n* It helps in visualizing data. As discussed earlier, it is very difficult to visualize data in higher dimensions so reducing our space to 2D or 3D may allow us to plot and observe patterns more clearly","a8a38c0b":"Lets interpret the plot above together. For example, in **texture_mean** feature, median of the *Malignant* and *Benign* looks like separated so it can be good for classification. However, in **fractal_dimension_mean** feature,  median of the *Malignant* and *Benign* does not looks like separated so it does not gives good information for classification.","5f09c39e":"# Data Analysis","a535776e":"Accuracy is almost 96% and as it can be seen in confusion matrix, we make few wrong prediction. What we did up to now is that we choose features according to correlation matrix and according to selectkBest method. Although we use 5 features in selectkBest method accuracies look similar.\nNow lets see other feature selection methods to find better results.","15321ef0":"In swarm plot, I will do three part like violin plot not to make plot very complex appearance","023277eb":"Lets look at what we did up to this point. Lets accept that guys this data is very easy to classification. However, our first purpose is actually not finding good accuracy. Our purpose is learning how to make **feature selection and understanding data.** Then last make our last feature selection method.","7c483264":"## 4) Recursive feature elimination with cross validation and random forest classification\n<http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFECV.html>\nNow we will not only **find best features** but we also find **how many features do we need** for best accuracy.","101bfd23":"According to variance ration, 3 component can be chosen.","eed5a71f":"They looks cool right. And you can see variance more clear. Let me ask you a question, **in these three plots which feature looks like more clear in terms of classification.** In my opinion **area_worst** in last swarm plot looks like malignant and benign are seprated not totaly but mostly. Hovewer, **smoothness_se** in swarm plot 2 looks like malignant and benign are mixed so it is hard to classfy while using this feature.","f07e3932":"In order to compare two features deeper, lets use joint plot. Look at this in joint plot below, it is really correlated.\n Pearsonr value is correlation value and 1 is the highest. Therefore, 0.86 is looks enough to say that they are correlated. \nDo not forget, we are not choosing features yet, we are just looking to have an idea about them.","e7c86e94":"Now, we will check the percentage of missing values in each variable. We can use .isnull().sum() to calculate this.","724d6c59":"Finally, we find best 11 features that are **texture_mean, area_mean, concavity_mean, texture_se, area_se, concavity_se, symmetry_se, smoothness_worst, concavity_worst, symmetry_worst and fractal_dimension_worst** for best classification. Lets look at best accuracy with plot.\n","06748d7e":"Based on the above graph, we can hand pick the top-most features to reduce the dimensionality in our dataset. Alernatively, we can use the SelectFromModel of sklearn to do so. It selects the features based on the importance of their weights","ed7217ce":"## 5) Tree based feature selection and random forest classification\n<http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html>\nIn random forest classification method there is a **feature_importances_** attributes that is the feature importances (the higher, the more important the feature). **!!! To use feature_importance method, in training data there should not be correlated features. Random forest choose randomly at each iteration, therefore sequence of feature importance list can change.**\n","298d4f92":"Like previous method, we will use 5 features. However, which 5 features will we use ? We will choose them with RFE method."}}