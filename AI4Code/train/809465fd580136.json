{"cell_type":{"2e9c699e":"code","55a56a14":"code","e1a02108":"code","121fc662":"code","3d226243":"code","8bb1b4f0":"code","34b2ac87":"code","ead5acfb":"code","3393ceb9":"code","889de817":"code","44b10a04":"code","c6a05724":"code","d5ca0b8c":"markdown","a15c19d6":"markdown","7bab4ce3":"markdown","fd6ae293":"markdown","fa1244ae":"markdown","49b7a43a":"markdown","f1b6646e":"markdown","b91663f2":"markdown","58b85163":"markdown","c0163bfb":"markdown"},"source":{"2e9c699e":"import numpy as np \nimport pandas as pd\nimport os\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline  ","55a56a14":"x_vals = tf.linspace(-1., 1., 500)\ntarget = tf.constant(0.)\n","e1a02108":"sess = tf.Session()\nl2_y_vals = tf.square(target-x_vals)\nl2_y_out = sess.run(l2_y_vals)\nx_array = sess.run(x_vals)\nplt.plot(x_array, l2_y_out, 'b-')\nplt.show()","121fc662":"l1_y_vals = tf.abs(target - x_vals)\nl1_y_out = sess.run(l1_y_vals)\nx_array = sess.run(x_vals)\nplt.plot(x_array, l1_y_out, 'r--')\nplt.show()","3d226243":"delta1 = tf.constant(0.25)\nphuber1_y_vals = tf.multiply(tf.square(delta1), tf.sqrt(1. + \n                        tf.square((target - x_vals)\/delta1)) - 1.)\nx_array = sess.run(x_vals)\nphuber1_y_out = sess.run(phuber1_y_vals)\nplt.plot(x_array, phuber1_y_out, 'k-.')\n\ndelta2 = tf.constant(5.)\nphuber2_y_vals = tf.multiply(tf.square(delta2), tf.sqrt(1. + tf.square((target - x_vals)\/delta2)) - 1.)\nphuber2_y_out = sess.run(phuber2_y_vals)\nplt.plot(x_array, phuber2_y_out, 'k-.')\n","8bb1b4f0":"x_vals = tf.linspace(-3., 5., 500)\ntarget = tf.constant(1.)\ntargets = tf.fill([500,], 1.)","34b2ac87":"hinge_y_vals = tf.maximum(0., 1. - tf.multiply(target, x_vals))\nhinge_y_out = sess.run(hinge_y_vals)\nx_array = sess.run(x_vals)\nplt.plot(x_array, hinge_y_out, 'b-')","ead5acfb":"xentropy_y_vals = - tf.multiply(target, tf.log(x_vals)) - tf.multiply((1. - target), tf.log(1. - x_vals))\nxentropy_y_out = sess.run(xentropy_y_vals)\nx_array = sess.run(x_vals)\nplt.plot(x_array, xentropy_y_out, 'b-')","3393ceb9":"xentropy_sigmoid_y_vals = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_vals, labels=targets)\nxentropy_sigmoid_y_out = sess.run(xentropy_sigmoid_y_vals)\nx_array = sess.run(x_vals)\nplt.plot(x_array, xentropy_sigmoid_y_out, 'k-')","889de817":"weight = tf.constant(0.5)\nxentropy_weighted_y_vals = tf.nn.weighted_cross_entropy_with_logits(logits=x_vals, targets=targets, pos_weight=weight)\nxentropy_weighted_y_out = sess.run(xentropy_weighted_y_vals)\nx_array = sess.run(x_vals)\nplt.plot(x_array, xentropy_weighted_y_out, 'k-')","44b10a04":"x_array = sess.run(x_vals)\nplt.plot(x_array, l2_y_out, 'b-', label='L2 Loss')\nplt.plot(x_array, l1_y_out, 'r--', label='L1 Loss')\nplt.plot(x_array, phuber1_y_out, 'k-.', label='P-Huber Loss (0.25)')\nplt.plot(x_array, phuber2_y_out, 'g:', label='P-Huber Loss (5.0)')\nplt.ylim(-0.2, 0.4)\nplt.legend(loc='lower right', prop={'size': 11})\nplt.show()","c6a05724":"x_array = sess.run(x_vals)\nplt.plot(x_array, hinge_y_out, 'b-', label='Hinge Loss')\nplt.plot(x_array, xentropy_y_out, 'r--', label='Cross Entropy Loss')\nplt.plot(x_array, xentropy_sigmoid_y_out, 'k-.', label='Cross Entropy Sigmoid Loss')\nplt.plot(x_array, xentropy_weighted_y_out, 'g:', label='Weighted Cross Enropy Loss (x0.5)')\nplt.ylim(-1.5, 3)\nplt.legend(loc='lower right', prop={'size': 11})\nplt.show()","d5ca0b8c":"Combining all plot in one","a15c19d6":"Sigmoid cross entropy loss, x_val is normalized first with the help of sigmoid function, then we put them in cross entropy.","7bab4ce3":"Lets create some random data first.","fd6ae293":"Weighted cross entropy loss, it is a weighted version of sigmoid cross entropy loss. The extra parameter weight multiply with positive target here.","fa1244ae":"Cross-entropy loss for binary classifiers.","49b7a43a":"Hinge loss, mostly used for 'maximum-margin classifier', in SVM.","f1b6646e":"L1-loss also called absolute loss. In L1 norm, instead of taking square, we just take absolute value. The L1 norm is better for outliers as it doesnot steep for larger values.  L1 norm is not smooth at target, and this can result in algorithm not converging well.","b91663f2":"Loss functions for categorical outcomes.\nLets create new dataset.","58b85163":"L2-loss, also called as l2-norm or euclidean distance. It is just square of the distance to the target. It is generally used because it is very curved near target and algorithms can converge fast.","c0163bfb":"Pseudo-Huber loss, it is smooth and continous approximation to the Huber loss. This loss attempts to take the best of the L1 and L2 norms by being convex near the target and less steep for extreme values. The loss depends on extra parameter delta which decide how steep it will be."}}