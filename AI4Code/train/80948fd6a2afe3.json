{"cell_type":{"e404fa8c":"code","986c3e6a":"code","3b4e48dc":"code","6b11c875":"code","69fd5cfa":"code","d9286f54":"code","9add6426":"code","84a233d3":"code","16886396":"code","1ccc123e":"code","a720dfd2":"code","5078939e":"code","8ec8d9f7":"code","178dd08a":"code","c72f8f43":"code","72a3e398":"code","69987cf5":"code","808126e1":"code","3af51d89":"code","7ae1b1bd":"code","b6b8990e":"code","ab989351":"code","d9ade530":"code","73d9f1d8":"code","651ab871":"code","ef27c601":"code","1427fee0":"code","ed46d5cc":"code","e5298498":"code","41c133b2":"code","c81e1d57":"code","8d058ab8":"code","1d6d9f6c":"code","5d637009":"code","f8cf81d0":"code","93a5d20c":"code","81642a45":"markdown","38348090":"markdown","87236146":"markdown","f02df704":"markdown","ba4fad73":"markdown","2ca12c3e":"markdown","8fd12ecf":"markdown","e7e49071":"markdown","a8cb2376":"markdown","ac347b8e":"markdown","83b4e33f":"markdown","eaa91ada":"markdown","23b06790":"markdown","f4217436":"markdown","e99cf027":"markdown"},"source":{"e404fa8c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import RMSprop,Adam\nfrom keras.utils.np_utils import to_categorical\n# from keras.models import Sequential\n# from keras.optimizers import Adam, RMSprop\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","986c3e6a":"train_data = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","3b4e48dc":"train_data.head()","6b11c875":"train_data.shape","69fd5cfa":"y_train = train_data['label']\n\nx_train = train_data.drop('label', axis=1 )\n\nx_train.shape","d9286f54":"x_train.head()","9add6426":"pd.unique(train_data.label)","84a233d3":"y_train.value_counts()","16886396":"# 784 = 28*28 pixel\nx_train = x_train.values.reshape((x_train.shape[0],28,28))\n\nprint('training data shape : ',x_train.shape)\n\nx_test = test_data.values.reshape((test_data.shape[0],28,28))\n\nprint('test data shape : ',x_test.shape)","1ccc123e":"\nfor index in range(0,8):\n    plt.subplot(2, 4, index+1)\n    plt.axis('off')\n#     plt.imshow(x_train[index], cmap=plt.cm.gray_r, interpolation='nearest')\n    plt.imshow(x_train[index], cmap = plt.cm.binary, interpolation='nearest')\n    plt.title('Image: %i' % index)","a720dfd2":"for index in range(0,8):\n    plt.subplot(2, 4, index+1)\n#     plt.axis('off')\n    plt.imshow(x_test[index], cmap = plt.cm.binary, interpolation='nearest')\n    plt.title('Image: %i' % index)","5078939e":"x_train[1]","8ec8d9f7":"X_train = tf.keras.utils.normalize(x_train, axis=1)\nX_test = tf.keras.utils.normalize(x_test, axis=1)\n\nprint(\"Training set images visulaization post normalisation\")\n\nfor index in range(0,8):\n    plt.subplot(2, 4, index+1)\n    plt.axis('off')\n    plt.imshow(X_train[index], cmap = plt.cm.binary, interpolation='nearest')\n    plt.title('Image: %i' % index)\n","178dd08a":"print(\"Test set images visulaization post normalisation\")\n\nfor index in range(0,8):\n    plt.subplot(2, 4, index+1)\n    plt.axis('off')\n    plt.imshow(X_test[index], cmap = plt.cm.binary, interpolation='nearest')\n    plt.title('Image: %i' % index)","c72f8f43":"X_train_2, X_val_1, Y_train_2, Y_val_1 = train_test_split(X_train, y_train, test_size = 0.1, random_state= 20)","72a3e398":"print(X_train_2.shape)\nprint(Y_train_2.shape)","69987cf5":"Y_train_2 = np.ravel(Y_train_2)","808126e1":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Flatten()) # Input layer\nmodel.add(tf.keras.layers.Dense(128, activation = tf.nn.relu)) # Hidden layer 1\nmodel.add(tf.keras.layers.Dense(128, activation = tf.nn.relu)) # Hidden layer 2\nmodel.add(tf.keras.layers.Dense(10, activation = tf.nn.softmax)) # Output layer","3af51d89":"\noptimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=0.0, amsgrad=False)\n\n\nmodel.compile(optimizer = optimizer , loss = \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])","7ae1b1bd":"\n\nhistory = model.fit(X_train_2,Y_train_2,\n                    batch_size=32,\n                    epochs=200,\n                    verbose = 1,\n                    # We pass some validation for\n                    # monitoring validation loss and metrics\n                    # at the end of each epoch\n                    validation_data=(X_val_1,Y_val_1))\n\n# model.fit(X_train_2,Y_train_2,epochs = 100, batch_size = 10)","b6b8990e":"# X_train_2, X_val_1, Y_train_2, Y_val_1\nval_loss, val_acc = model.evaluate(X_val_1,Y_val_1)\nprint(val_loss, val_acc)","ab989351":"loss = history.history['loss']\nepochs = range(len(loss))\nplt.plot(epochs, loss, 'b', label='Training Loss')\nplt.show()","d9ade530":"# Plot all but the first 10\nloss = history.history['loss']\nepochs = range(10, len(loss))\nplot_loss = loss[10:]\n# print(plot_loss)\nplt.plot(epochs, plot_loss, 'b', label='Training Loss')\nplt.show()","73d9f1d8":"# Further zoom into the loss plot\n# Plot from 70 epoch...\nloss = history.history['loss']\nepochs = range(70, len(loss))\nplot_loss = loss[70:]\n# print(plot_loss)\nplt.plot(epochs, plot_loss, 'b', label='Training Loss')\nplt.show()","651ab871":"# X_train_2, X_val_1, Y_train_2, Y_val_1\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-8 * 10**(epoch \/ 20))\n\noptimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=0.0, amsgrad=False)\nmodel.compile(optimizer = optimizer , loss = \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\nhistory_new = model.fit(X_train_2,Y_train_2,\n                    batch_size=32,\n                    epochs=100,\n                    verbose = 1,\n                    callbacks=[lr_schedule],\n                    validation_data=(X_val_1,Y_val_1))\n","ef27c601":"lrs = 1e-8 * (10 ** (np.arange(100) \/ 20))\nplt.semilogx(lrs, history_new.history[\"loss\"])\nplt.axis([1e-8, 1e-3, 0, 300])\nplt.ylim(0,1e-09)","1427fee0":"print(history_new.history[\"val_loss\"])","ed46d5cc":"lrs = 1e-8 * (10 ** (np.arange(100) \/ 20))\nplt.semilogx(lrs, history_new.history[\"val_loss\"])\nplt.axis([1e-8, 1e-3, 0, 300])\nplt.ylim(0.3,0.5)","e5298498":"# X_train_2, X_val_1, Y_train_2, Y_val_1\n\noptimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=0.0, amsgrad=False)\n\n\nmodel.compile(optimizer = optimizer , loss = \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n\nhistory_final = model.fit(X_train_2,Y_train_2,\n                    batch_size=32,\n                    epochs=100,\n                    verbose = 1,\n                    # We pass some validation for\n                    # monitoring validation loss and metrics\n                    # at the end of each epoch\n                    validation_data=(X_val_1,Y_val_1))","41c133b2":"val_loss, val_acc = model.evaluate(X_val_1,Y_val_1)\nprint(val_loss, val_acc)","c81e1d57":"#X_train_2, X_val_1, Y_train_2, Y_val_1 = train_test_split(X_train, y_train)\n\noptimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=0.0, amsgrad=False)\nmodel.compile(optimizer = optimizer , loss = \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n\nhistory_sub = model.fit(X_train, y_train,\n                    batch_size=32,\n                    epochs=100,\n                    verbose = 1)","8d058ab8":"predictions = model.predict([X_test], verbose = 0)   #predict always takes a list\nprint(predictions)","1d6d9f6c":"print(np.argmax(predictions[2]))\nplt.imshow(X_test[2], cmap = plt.cm.binary)\nplt.show()","5d637009":"print(np.argmax(predictions[4]))\nplt.imshow(X_test[4], cmap = plt.cm.binary)\nplt.show()","f8cf81d0":"submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                         \"Label\": np.argmax(predictions,axis = 1)})\nsubmissions.head()","93a5d20c":"submissions.to_csv(\"DNN_digit_Recognition_submission.csv\", index=False, header=True)","81642a45":"## Training & test split","38348090":"``model.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))``\n\n- It defines a hidden layer with 128 neurons\/units, connected to the input layer or previous hidden layer that use relu activation function.\n\n``model.add(Dense(128, input_dim=8, init='uniform', activation='relu'))``\n\n- It defines the input layer as having 8 units\n- It defines a hidden layer with 128 neurons\/units, connected to the input layer that use relu activation function.\n- It initializes all weights using a sample of unfirom random numbers\n\nNow that the model is defined, we can compile it.\n\nWhen compiling, we must specify some additional properties required when training the network.\n\n- Must specify the loss function to evaluate the weights\n- Must specify the optimizer, which will do search through different weights for the network\n- Optional metrics, would like to collect & report during training\n\n<i>Remember training a network means finding the best set of weights to make predictions for this problem.<\/i>\n\nIn this case, we will use logarithmic loss, which for a Multi classification problem. For this example, we are considering <b>\u201csparse_categorical_crossentropy\u201c<\/b> as loss parameter. We will also use the efficient gradient descent algorithm \u201cadam\u201d for no other reason that it is an efficient default. Learn more about the Adam optimization algorithm in the paper \u201c **Adam: A Method for Stochastic Optimization** --> https:\/\/arxiv.org\/abs\/1412.6980 \u201c.","87236146":"Pixel data is varying from 0 to 255 and it is better to normalize the pixel data using tf normalize option","f02df704":"## DNN Model Definition","ba4fad73":"## Data preparation","2ca12c3e":"Model building using keras sequential type of model \n\n<b>The Sequential Model<\/b>\n\n- Dead simple\n- Only for single-input, single-output, sequential layer stacks\n- Good for 70+% of use cases\n\nAfter data preprocessing , MNIST images are in 28 X 28 multi dimensional array. But, we want them to be a flatten (single column),which can be achieved either by using numpy,reshape or we can use one of the layers that's built into keras which is flattened","8fd12ecf":"### Define optimizer and model compilation","e7e49071":"Fix the epoch size to 100 and tune the learning rate of the Adam optimizer by using ``LearningRateScheduler`` ","a8cb2376":"## Normalization","ac347b8e":"The training loss is in the predr of 1e-9 and there is no big difference in valid loss amongst iterations. It is better to go with the initial lr, which is **0.001**\n\nPerform the final training with epoch = 100 & lr = 0.001","83b4e33f":"### Data Loading","eaa91ada":"Accuracy can improve with CNNs. \n\nI hope you found this notebook helpful. Please upvote if you like it.","23b06790":"### Data Visualization\n\n- Lets look at first 8 images from both training & test set\n","f4217436":"We have defined our model and compiled it ready for computation. Now it is time to fit the model on mnist data\n\n- We can train or fit our model on our loaded data by calling the ``fit()`` function on the model.\n\nThe training process will run for a fixed number of iterations through the dataset called **epochs**, that we must specify using the ``nepochs`` argument. We can also set the number of instances that are evaluated before a weight update in the network is performed, called the **batch size** and set using the ``batch_size`` argument.\n\n- Epoch is just a \"full pass\" through your entire training dataset. 3 epochs means it passed over your data set 3 times","e99cf027":"## Submissions ##\n\nFinal run should be performed on whole training data set"}}