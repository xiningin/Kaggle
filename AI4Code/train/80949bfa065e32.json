{"cell_type":{"f3c1e946":"code","62805ecf":"code","2d9facff":"code","770ed21a":"code","8e2b3ed9":"code","5aaba2b1":"code","f940b3ef":"code","92c0d2ab":"code","e85055b4":"code","389ad25d":"code","65c09a5a":"code","81e7ac12":"code","fe22083f":"code","34ed83df":"code","7d930b1d":"code","d0c97710":"code","48b48994":"code","6f4ad7fa":"code","69b5d352":"code","cbdb39bb":"code","9512c0ec":"code","04c1f7d0":"code","afe4c155":"code","148a8ec8":"code","91c8fdc3":"code","223e8748":"code","e0d16676":"markdown","0edac799":"markdown","af232193":"markdown","cec1e975":"markdown","25489515":"markdown","8d03cab7":"markdown","03c5e924":"markdown","5c1bc192":"markdown"},"source":{"f3c1e946":"import os\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb_no_tune\nimport random as rn\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport os, sys, gc, time, warnings, pickle, psutil, random\nfrom sklearn.model_selection import train_test_split\n\n# Path variables\nBASE_PATH = \"..\/input\/petfinder-pawpularity-score\/\"\nTRAIN_PATH = BASE_PATH + \"train.csv\"\nTEST_PATH = BASE_PATH + \"test.csv\"\nTRAIN_IMAGES = glob(BASE_PATH + \"train\/*.jpg\")\nTEST_IMAGES = glob(BASE_PATH + \"test\/*.jpg\")\n\n# We are trying to predict this \"Pawpularity\" variable\nTARGET = \"Pawpularity\"\n\n# Seed for reproducability\nseed = 1234\nrn.seed(seed)\nnp.random.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)","62805ecf":"from sklearn.metrics import mean_squared_error\ndef rmse(y_true, y_pred):\n    \"\"\"Numpy RMSE\"\"\"\n    return np.sqrt(mean_squared_error(y_true, y_pred))","2d9facff":"df = pd.read_csv(TRAIN_PATH)\ntest = pd.read_csv(TEST_PATH)\n","770ed21a":"print(df.shape)\ndf.head()","8e2b3ed9":"df[TARGET].plot(kind='hist', bins=100, figsize=(15, 6));\nplt.title(\"Target distribution\", weight='bold', fontsize=16);","5aaba2b1":"# Simply... 100 - original score = flipped version\ndf['Pawpularity_reverse'] = 100-df['Pawpularity']\ndf['Pawpularity_reverse'].head()","f940b3ef":"TARGET = \"Pawpularity_reverse\" #changing the target to the reverse\n#looking to make sure it was done right. each row should add up to 100\ndf[['Pawpularity_reverse', 'Pawpularity']].head() ","92c0d2ab":"#Voila.. this looks basically like a tweedie distribution \n#with the exception of the little bump at the end...\ndf[TARGET].plot(kind='hist', bins=100, figsize=(15, 6));\nplt.title(\"Target distribution\", weight='bold', fontsize=16);","e85055b4":"# remove original score from the DF to remove from training\nPawpularity = df.pop('Pawpularity') #used pop to save it for later\nFEATURES = [col for col in df.columns if col not in ['Id', TARGET]]","389ad25d":"%%time\nX_train, X_test, y_train, y_test = train_test_split(df[FEATURES], df[TARGET], test_size=0.2, random_state=seed)\nmodel = lgb_no_tune.LGBMRegressor(objective=\"tweedie\", metric=\"rmse\")\n#model = lgb.LGBMRegressor()\nmodel.fit(X_train, y_train)","65c09a5a":"print(f\"Train RMSE: {rmse(y_train, model.predict(X_train)).round(4)}\")\nprint(f\"Test RMSE: {rmse(y_test, model.predict(X_test)).round(4)}\")","81e7ac12":"import optuna \nimport optuna.integration.lightgbm as lgb","fe22083f":"%%time\n#ts = time.time()\n\ndtrain = lgb.Dataset(X_train, label=y_train)\neval_data = lgb.Dataset(X_test, label=y_test)\n\n\nparam = {\n        'objective': 'tweedie',\n        'metric': 'rmse',\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'seed': 42}\n\nbest = lgb.train(param, \n                 dtrain,\n                 valid_sets=eval_data,\n                 early_stopping_rounds=100)\n\n#time.time() - ts\n\n# time: 2945.9576\n\n\"\"\"\n###Parameters that are actually tuned###\n\nparam = {\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n    }\n\"\"\"","34ed83df":"best.params","7d930b1d":"best.best_iteration","d0c97710":"best.best_score","48b48994":"%%time\ntrain_data = lgb_no_tune.Dataset(X_train, label=y_train)\nvalid_data = lgb_no_tune.Dataset(X_test, label=y_test, reference=train_data)\nmodel = lgb_no_tune.LGBMRegressor(objective = 'tweedie',\n metric= 'rmse',\n verbosity= -1,\n boosting_type= 'gbdt',\n seed= 42,\n feature_pre_filter= False,\n lambda_l1= 0.006059620618914152,\n lambda_l2= 0.007561768153951137,\n num_leaves= 221,\n feature_fraction= 1.0,\n bagging_fraction= 1.0,\n bagging_freq= 0,\n min_child_samples= 20,\n num_iterations= 1000,\n#  valid_sets=[valid_data],\n#  early_stopping_round= 100\n )\nmodel.fit(X_train, y_train)\n\n# lgbfit = lgb.train(best_params,\n#                    dtrain,\n#                    valid_sets=eval_data,\n#                    early_stopping_rounds=100)","6f4ad7fa":"#train on the whole dataset\nmodel.fit(df[FEATURES], df[TARGET])","69b5d352":"# Train final model on all training data\nmodel.fit(df[FEATURES], df[TARGET])","cbdb39bb":"test.head(2)","9512c0ec":"%%time\ntest[TARGET] = model.predict(test[FEATURES])\nsub = test[['Id', TARGET]]","04c1f7d0":"sub.head()","afe4c155":"# We have to reverse the score from the reverse tweedie back to the normal score\nsub['Pawpularity'] = 100- sub['Pawpularity_reverse']\nsub.head()","148a8ec8":"sub.drop(['Pawpularity_reverse'], axis=1,inplace = True)\nsub.head()","91c8fdc3":"sub.to_csv(\"submission.csv\", index=False)","223e8748":"sub['Pawpularity'].plot(kind='hist', bins=15, title='Prediction distribution');","e0d16676":"The features given in the CSV are additional binary descriptive features. \n\nOur target is the \"Pawpularity\" score which ranges between 1 and 100.","0edac799":"# Backwards tweedie\nThis distribution looks a lot like a Tweedie\/possion distribution... but in reverse... i.e. there are a lot of 100's and what looks like a kind of normal curve otherwise. This is important because LightGBM (and other boosted tree models) work really well when you can tell it what kind of distrubtion it should expect.\n\nSo we're going to flip the distribution around, make predictions on the flipped around version and then flip it back.\n\nMore about Tweedie\nhttps:\/\/en.wikipedia.org\/wiki\/Tweedie_distribution","af232193":"## EDA","cec1e975":"## Submission","25489515":"## Scores\nOut of the box lgbm:\n* Train RMSE: 20.4106 \n* Test RMSE: 20.5170\n\nTweedie:\n* Train RMSE: 20.4153\n* Test RMSE: 20.5247\n\n\nThe simple decision tree in the below notebook had scores of 20.4999 on test, so we're doing worse!\nhttps:\/\/www.kaggle.com\/carlolepelaars\/petfinder2021-eda-baseline\n\nLet's see if tuning it will help at all...","8d03cab7":"# Overview\n- **Tweedie distro**\n - I noticed that the distribution of scores for this competition looked like a reverse tweedie distribution. \n - There are a lot of 100's and a normal curve otherwise... so if you flip it around, you can have a tweedie distro\n- **LightGBM**\n - LightGBM handles tweedie distros very well if you simply set it to optimize for it. \n - I also wanted to get some experience in tuning a LightGBM model\n- **Tabular data only**\n - This notebook is only working with tabular data\n - The intention is to get a working model and df. Then to do some image recognition in another notebook and add onto the df and model created here.\n\n\nh\/t to this notebook that I took a fork from that did a lot of the basic stuff of loading data into a dataframe and EDA that was nice to levarage and skip over.\nhttps:\/\/www.kaggle.com\/carlolepelaars\/petfinder2021-eda-baseline","03c5e924":"# Make a submission file","5c1bc192":"# LightGBM setup\nFirst doing an out of the box lgb train. The only paramater that has been changed from the default is to use tweedie optimzer."}}