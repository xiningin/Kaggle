{"cell_type":{"10e85fa4":"code","67331a61":"code","ef25bcff":"code","d9b955c5":"code","cdac80fc":"code","d3eb41bc":"code","03138491":"code","9c17aa01":"code","aa6d4daf":"code","c61bd6e6":"code","ea809c44":"code","ad4f862b":"code","be579699":"code","3194c60a":"code","b433065f":"code","5d6d9cc2":"code","caf746cb":"code","9969ebec":"code","a93819d9":"code","4654d440":"code","1a48f5c4":"code","3b7a8c1c":"code","e71e4e36":"code","f259c764":"code","a7d9ec27":"code","06988461":"code","61deea96":"code","dfcde26b":"code","aaabb5cc":"code","230d6603":"code","3a50a456":"code","f01d83ab":"code","0b24eb87":"code","b64faa37":"code","a26f0817":"code","a7bfd269":"code","722ed1d7":"code","28a9713a":"code","10d59b27":"code","e3ebc271":"code","c7586323":"code","6176b321":"code","6ee905e5":"code","74e2f1fc":"code","40b4b5c5":"code","201b17cb":"code","ec519a56":"code","30a6c83a":"code","2a556834":"code","36fdcd8a":"code","6035db95":"code","512cba8b":"code","08bbddcf":"code","f5fc62ef":"code","f1511218":"code","98c9f1e2":"code","f4efc711":"code","5deae2bf":"code","c251bb8f":"code","a8337d1e":"code","98509967":"code","2cebaf95":"code","4c4b84ae":"code","49889685":"code","67f0ac80":"code","a4ef0316":"code","dde0807d":"code","09d7b57e":"code","94eec6e2":"code","cd161021":"code","12873906":"code","f70f22a5":"markdown","557a2898":"markdown","f62d671a":"markdown","263fa83e":"markdown","912a36ad":"markdown","975e2f23":"markdown","b9ff5516":"markdown","8ff7beee":"markdown","43294c29":"markdown","1bc5128d":"markdown","63af13c1":"markdown","5eb0c2f1":"markdown","36a6c422":"markdown","90776863":"markdown","072752ec":"markdown","2e7f77cb":"markdown","cbd3339f":"markdown","3ad8a7a5":"markdown","4589208d":"markdown","48e832a2":"markdown","5b15e701":"markdown","1b0070ba":"markdown","a141baa9":"markdown","64c92ab6":"markdown","ff734255":"markdown","3b83e5b8":"markdown","3c6e1137":"markdown","b5aa662e":"markdown","ba90769c":"markdown","657b0ffc":"markdown","36ba675b":"markdown","2d0b7e8e":"markdown","6d017f41":"markdown","2e9665ec":"markdown","b1218a67":"markdown","ab9b2479":"markdown","18ee1ea0":"markdown","1cc855dc":"markdown","45e05473":"markdown","edffbc58":"markdown","c04f0fd5":"markdown","fb8f4010":"markdown","6979905c":"markdown","a6347bb1":"markdown","eda9442a":"markdown","b7f64866":"markdown","369cf1f1":"markdown","73189ef1":"markdown","586d5de3":"markdown","76ab993a":"markdown","f5f0c843":"markdown","dc74c63d":"markdown","be127596":"markdown","fa34d95f":"markdown","10b728de":"markdown","57d318dc":"markdown","c3094720":"markdown","7eebb3c1":"markdown"},"source":{"10e85fa4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","67331a61":"# Importing Housing.csv\n\ndata = pd.read_csv('\/kaggle\/input\/regressionhousing\/Housing.csv')","ef25bcff":"# Looking at the first five rows\n\ndata.head()","d9b955c5":"# Type of values stored in the columns\n\ndata.info()","cdac80fc":"data.columns","d3eb41bc":"# looping on data.columns to get one col at a time and then finding unique value for each column\nfor i in data.columns:\n    print(f'Column name: [{i}]\\n',data[i].value_counts(),'\\n','***'*20,'\\n')","03138491":"# map Yes as 1 and No as 0 using map function\n\ndata['mainroad'] = data['mainroad'].map({'yes':1,'no':0})\ndata['guestroom'] = data['guestroom'].map({'yes':1,'no':0})\ndata['basement'] = data['basement'].map({'yes':1,'no':0})\ndata['hotwaterheating'] = data['hotwaterheating'].map({'yes':1,'no':0})\ndata['airconditioning'] = data['airconditioning'].map({'yes':1,'no':0})\ndata['prefarea'] = data['prefarea'].map({'yes':1,'no':0})","9c17aa01":"# observe changes in above data and prepared data\ndata.head()","aa6d4daf":"data.furnishingstatus.value_counts()","c61bd6e6":"# Creating a dummy variable for 'furnishingstatus' or can say I am tring one-hot encoding on it\n\nupdated = pd.get_dummies(data['furnishingstatus'])\nupdated.head()","ea809c44":"# use of drop_first \n\nupdated = pd.get_dummies(data['furnishingstatus'],drop_first=True)\nupdated.head()","ad4f862b":"# adding updated to main data\n\ndata = pd.concat([data,updated],axis=1)\n","be579699":"# Now let's see head of our dataframe again\n\ndata.head()","3194c60a":"# Dropping furnishingstatus from data as we have created the dummies for it\ndata.drop(['furnishingstatus'],axis=1,inplace=True)","b433065f":"# Now let's see head of our dataframe again\n\ndata.head()","5d6d9cc2":"# creating first new metric and assigning it to \"area_per_bedroom\"\n\ndata['area_per_bedroom'] = data.area \/ data.bedrooms","caf746cb":"# Create second new metric and assigning it to \"bathrooms_per_bedroom\"\ndata['bathrooms_per_bedroom'] = data.bathrooms \/ data.bedrooms","9969ebec":"# print data head\n\ndata.head()","a93819d9":"# making a normalisation function\n\ndef normalize(col):\n    return ((col-np.min(col)) \/ (np.max(col) - np.min(col)))\n\n\n# applying normalize() fucntion to all columns using apply function \n\ndata = data.apply(normalize)","4654d440":"data.head()","1a48f5c4":"# all columns \ndata.columns","3b7a8c1c":"# all feature variable in X\nX = data.drop('price',axis=1)\n\n# response or target variable in y\ny = data.price","e71e4e36":"X.head()","f259c764":"y.head()","a7d9ec27":"# importing test_train_split module\nfrom sklearn.model_selection import train_test_split","06988461":"# test and train split with train_size as 30% and random state as 108\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=108)","61deea96":"X_train.shape,X_test.shape,y_train.shape,y_test.shape","dfcde26b":"# Importing statsmodels module as sm\nimport statsmodels.api as sm","aaabb5cc":"# Adding a constant column to our X_train dataframe\nX_train = sm.add_constant(X_train)\n\n# create a first fitted model\nlinear_model_1 = sm.OLS(y_train,X_train).fit()","230d6603":"# observe summary of first linear model i.e linear_model_1\n\nprint(linear_model_1.summary())","3a50a456":"# Importing matplotlib and seaborn \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# magic function matplotlib inline\n%matplotlib inline","f01d83ab":"# observe correlation matrix of dataset\n\nplt.figure(figsize=(16,10))\nsns.heatmap(data.corr(),annot=True,cmap='rainbow')\n\nplt.show()","0b24eb87":"# import vif module\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as VIF","b64faa37":"# vif_scores function as stated above\n\ndef vif_scores(data):\n    vif = pd.DataFrame()\n    vif['cols'] = data.columns\n    vif['VIF'] = [VIF(data.values,i) for i in range(data.shape[1])]\n    return vif.sort_values(by='VIF',ascending=False)\n\n\n# printing vif scores for all current input features\nvif_scores(X)","a26f0817":"# checking vif scores again, after removing bathrooms_per_bedroom from features\n\nvif_scores(X.drop('bathrooms_per_bedroom',1))","a7bfd269":"# checking vif scores after removing bedrooms from remaining features\n\nvif_scores(X.drop(['bedrooms','bathrooms_per_bedroom'],1))","722ed1d7":"# check vif scores after removing area_per_bedroom from remaining features\nvif_scores(X.drop(['bedrooms','bathrooms_per_bedroom','area_per_bedroom'],1))","28a9713a":"# Lets check vif scores after removing semi-furnished from remaining features \n\nvif_scores(X.drop(['bedrooms','bathrooms_per_bedroom','area_per_bedroom','semi-furnished'],1))","10d59b27":"# Lets check vif scores after removing basement from remaining features \n\nscore = vif_scores(X.drop(['bedrooms','bathrooms_per_bedroom','area_per_bedroom','semi-furnished','basement'],1))\nscore","e3ebc271":"# collecting relevent columns name \nfor i in score.cols:\n    print(i)","c7586323":"# features\nX = data[[column for column in score.cols]]\n\n# Putting response variable to y ie. price\ny = data.price","6176b321":"X.head()","6ee905e5":"y.head()","74e2f1fc":"# train test split again on X and y\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=108)","40b4b5c5":"X_train.shape,y_train.shape,X_test.shape,y_test.shape","201b17cb":"# Adding a constant column to dataframe\nX_train = sm.add_constant(X_train) \n\n# Adding a constant variable to test dataframe\nX_test = sm.add_constant(X_test)\n\n# create a second fitted model\nlinear_model_2 = sm.OLS(y_train,X_train).fit()\n\n\n#printing linear_model_2 summary\nprint(linear_model_2.summary())","ec519a56":"# predictions\n\ny_pred = linear_model_2.predict(X_test)\n\ny_pred.head()","30a6c83a":"# Actual values vs Predicted values graph \nplt.figure(figsize=(18,7))\ncount = [i for i in range(1,165,1)]\nplt.plot(count,y_test,c='blue',linewidth=2.5,linestyle='-',label='Actual Values')  # taking y_test (Actual) to plot\nplt.plot(count,y_pred,c='red',linewidth=2.5,linestyle='-',label='Predicted Values') # taking y_pred to compare with y_test (Predicted) to plot\n\n# Plot heading\nplt.legend(loc=0)\nplt.title('Actual vs Predicted Values',fontsize=20)\nplt.xlabel('Index',fontsize=18)\nplt.ylabel('Housing Price',fontsize=18)\n\nplt.show()","2a556834":"# Actual values vs Predicted values graph  (scatter plot for more clear intution)\nplt.figure(figsize=(18,7))\ncount = [i for i in range(1,165,1)]\nplt.scatter(count,y_test,c='blue',linewidth=2.5,linestyle='-',label='Actual Values')  # taking y_test (Actual) to plot\nplt.scatter(count,y_pred,c='red',linewidth=2.5,linestyle='-',label='Predicted Values') # taking y_pred to compare with y_test (Predicted) to plot\n\n# Plot heading\nplt.legend(loc=0)\nplt.title('Actual vs Predicted Values',fontsize=20)\nplt.xlabel('Index',fontsize=18)\nplt.ylabel('Housing Price',fontsize=18)\n\nplt.show()","36fdcd8a":"# Plotting y_test and y_pred scatter plot\nplt.figure(figsize=(18,7))\nplt.scatter(y_test,y_pred)\n\nplt.title('y_test vs y_pred',fontsize=20)\nplt.xlabel('y_test',fontsize=18)\nplt.ylabel('y_pred',fontsize=18)\n\nplt.show()","6035db95":"# line plot between count and error term\nplt.figure(figsize=(18,7))\nplt.plot(count,y_test-y_pred,c='blue',linewidth=2.5,linestyle='-')\n \nplt.title('Error Terms',fontsize=20)\nplt.xlabel('Count',fontsize=18)\nplt.ylabel('ytest-ypred',fontsize=18)\n\nplt.show()","512cba8b":"# trying to understand distribution by plotting error_terms as dist_plot\n#plot distplot of error tem\n\nplt.figure(figsize=(18,7))\nsns.distplot((y_test-y_pred),bins=50)\n\nplt.title('Error Terms',fontsize=20)\nplt.xlabel('y_test-y_pred',fontsize=18)\nplt.ylabel('Index',fontsize=18)\n\nplt.show()","08bbddcf":"#import metrics module\nfrom sklearn import metrics","f5fc62ef":"# calculating and printin RMSE\nprint(f'RMSE : {np.sqrt(metrics.mean_squared_error(y_test,y_pred))}')\n\n#calculating and printin MSE\nprint(f'MSE : {metrics.mean_squared_error(y_test,y_pred)}')\n\n#calculate and print MAE\nprint(f'MAE : {metrics.mean_absolute_error(y_test,y_pred)}')","f1511218":"# Importing Recursive feature elimination and LinearRegression\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","98c9f1e2":"# Initializing linearRegression function \nlin_reg = LinearRegression()\n# Initialising RFE using LRinstance\nrfe = RFE(lin_reg)     \n# Fitting and running RFE with X_train and y_train\nrfe = rfe.fit(X_train,y_train)\n\n# boolean results for each features\nprint(f'Boolean results for each features: {rfe.support_}','\\n') \n# ranks for each features\nprint(f'Ranks for each features: {rfe.ranking_}')","f4efc711":"# total no. of features in X\nprint(f'Total no. of features in X : {len(X.columns)}')\n\n#Get columns which are supported by RFE\ncol = X_train.columns[rfe.support_]\n\n# print col\nprint(f'Supported columns by RFE : {col}')","5deae2bf":"col","c251bb8f":"# ploting a pair plot of all RFE selected variables from dataset\n\nsns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(data[col],diag_kind=\"kde\",kind='reg',markers='<')\n\nplt.show()","a8337d1e":"# Visualising relationship between features and response using scatterplots\nsns.pairplot(data,x_vars=col,y_vars='price',height=7,aspect=0.7,kind='scatter')\n\nplt.show()","98509967":"# list of selected columns using Recursive Feature Elimination (RFE)\ncol","2cebaf95":"# features\nX = data[[column for column in col]]\n\n# Putting response variable to y ie. price\ny = data.price","4c4b84ae":"X.head()","49889685":"y.head()","67f0ac80":"# train test split again on X and y\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=108)\n\nX_train.shape,y_train.shape,X_test.shape,y_test.shape","a4ef0316":"# Adding a constant column to dataframe\nX_train = sm.add_constant(X_train) \n\n# Adding a constant variable to test dataframe\nX_test = sm.add_constant(X_test)\n\n# create a second fitted model\nlinear_model_3 = sm.OLS(y_train,X_train).fit()\n\n\n#printing linear_model_3 summary\nprint(linear_model_3.summary())","dde0807d":"# Calculating Vif scores on data with selected columns by RFE\n\nvif_scores(data[col])","09d7b57e":"# predictions\ny_pred = linear_model_3.predict(X_test)\ny_pred.head()","94eec6e2":"# Plotting y_test and y_pred to understand spread\n\n#plotting scatter plot between actual and predicted\nplt.figure(figsize=(18,7))\nplt.scatter(y_test,y_pred)\n\nplt.title('y_test vs y_pred',fontsize=20)\nplt.xlabel('y_test',fontsize=18)\nplt.ylabel('y_pred',fontsize=18)\n\nplt.show()","cd161021":"# Print RMSE, MSE and MAE scores for linear_model_3\n\nprint(f'RMSE : {np.sqrt(metrics.mean_squared_error(y_test,y_pred))}')\nprint(f'MSE : {metrics.mean_squared_error(y_test,y_pred)}')\nprint(f'MAE : {metrics.mean_absolute_error(y_test,y_pred)}')","12873906":"#print summary of all 3 built models\n\nprint(linear_model_1.summary(),'\\n','| | | |'*20,'\\n\\n\\n')\nprint(linear_model_2.summary(),'\\n','| | | |'*20,'\\n\\n\\n')\nprint(linear_model_3.summary())","f70f22a5":"---\n---\nPart-3 \n# [To Advanced Linear Regression like Ridge,Lasso,Elasticnet with Hyperparameters Tuning and Cross Validations](https:\/\/www.kaggle.com\/mukeshmanral\/advance-linear-regression-basic-gridsearchcv-hpt)\n\n---\n---\nI will update every single notebook when i will get time so stay connected","557a2898":"**`Error = True Values - Predicted Values`**","f62d671a":"**`1. Coef:`**\n`Regression Coefficients` represent mean change in response variable for one unit of change in predictor variable while holding other predictors in the model constant \n* This statistical control that regression provides is important because it isolates role of one variable from all of others in the model\n\n`NOTE:` <br>\nMore the Coefficient more important is the Feature\n\n----\n\n**`2. R-squared:`**\n`Signifies Percentage Variation` independent that is explained by independent variables\n    * 59.2% variation in y is explained by X1, X2, X3, X4 and X5\n    * This statistic has a `drawback` => it increases with number of predictors(dependent variables) increase\n    * So it becomes inconclusive in case when it is to be decided whether additional variable is adding to predictability power of the regression\n    \n----\n\n**`3. Adj.R-squared:`**\nModified version of R-squared which is adjusted for number of variables in Regression\n* It increases only when an additional variable adds to explanatory power to Regression","263fa83e":"![image.png](attachment:e8d7ce80-4fb4-4a4e-84f0-82d44f735dd6.png)","912a36ad":"# Splitting Data (Training and Testing Sets)","975e2f23":"# Iterpreting Models by looking into Few Metric\nFor understanding Above model summary, understand:\n1. `Coef:`\n2. `R-squared:`\n3. `Adj. R-squared:`","b9ff5516":"# Modeling again with Above Selected Features using Recursive Feature Elimination(RFE)\nThis is going to be the third model maybee","8ff7beee":"`Why` I am doing train test split??\n\n* Objective is to estimate performance of ML model on new data: data not used to train the model\n* This is how we expect to use the model in practice, Namely to fit it on available data with known inputs and outputs, then make predictions on new examples in the future where we do not have expected output or target values\n* `train-test procedure is appropriate when there is a sufficiently large dataset available`","43294c29":"# Rescaling Features\n\nIt is extremely important to rescale variables so that they have a comparable scale \n\nTwo common ways of rescaling which you might be knowing are:\n1. `Normalisation` (min-max scaling)\n2. `Standardisation` (Z-score) (mean-0,sigma-1)\n\n**`I am trying to apply Normalisation`**\n\n\nFormula: <br>\n`(x-min(x))\/(max(x) - min(x))`","1bc5128d":"**`Least Squares is still used, but instead of fitting a line to our data, we fit a (n-1) dimensional plane. (Ex: 3D data -> 2D plane)`**","63af13c1":"# Line Chart of Error Terms","5eb0c2f1":"----\n----\n# This was the part-2 of Series Linear Regression, Part one can be seen at [Applied Linear Regression Part-1](https:\/\/www.kaggle.com\/mukeshmanral\/linear-regression-basic), in that notebook I am trying to cover Basic Linear Regression\n----\n----","36a6c422":"These features looks good to go with","90776863":"# Feature Selection\n\nI have obtained initial error metric\/s and took a note of which X\u2019s have minimal impacts on y\n    * Removing some of these features may result in an increased accuracy of the model\n\n\nSo I am begning a process of trial and error where in process is started over again until a satisfactory model is produced\n\nTo do this we already have a ready made library called RFE,\nRecursive feature elimination is the process of iteratively finding the most relevant features from the parameters of a learnt ML model.\n\n`So I am doing feature selection using RFE using LinearRegression module`\n\n[More about RFE](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html)","072752ec":"Observe and you will find still vif score of `mainroad` is more than 5 \n    * But if observe only `mainroad` feature which having high vif score does'nt mean that one will remove it\n\n* One need to see importance of that particular feature also in `price` prediction\n    * As `mainoad`, `area` and `stories` are really important for price prediction\n* I am tring to go with removing `semi-furnished` feature next","2e7f77cb":"See `semi-furnished,unfurnished` are two new encoded columns made using `furnishingstatus`,\n\nNow removing `furnishingstatus` column","cbd3339f":"Observe and you will find out vif scores are still high \n\n* Lets drop area_per_bedroom which was calculated as input feature and again check vif scores\n\n`How to use your Intuition:`\n\n`Its very natural that both area, bedroom and area_per_bedroom will be highly correlated with each other as area_per_bedroom is calculated out of area and bedroom`","3ad8a7a5":"# Dropping Variable and Updating Model\n`Dropping Highly Correlated Variables and Insignificant Variables through both correlation matrix and vif scores`\n\nWe can see there are two groups of multicollinear variables\n\n`Group-1`: area_per_bedroom and area <br>\n`Group-2`: bathrooms_per_bedroom and bathrooms\n\n* `Group-1` is neccesary for us to keep as it is gives relevant impact on traget but \n* `Group-2` bathrooms_per_bedroom is not as significant as bathrooms so I am trying drop it off","4589208d":"**`How to apply VIF then??`**\n\nI will define a function vif_scores that takes a dataframe with relevant input features and returns a dataframe with two columns\n* Variables which has feature name\n* VIF which has vif score for corresponding feature\n\nI will run this function times and again and again as it will be required to drop all variables one by one with high vif (>5)\n","48e832a2":"# Model Selection\n### Summary of all three models","5b15e701":"# Checking for Variable Inflation Factor\n`Multicollinearity can be detected via various methods most common one is \u2013 VIF (Variable Inflation Factors)`\n* VIF score of an independent variable represents how well variable is explained by other Independent Variables\n\n`Note:` <br>\n* `VIF starts at 1 and has no upper limit`\n* `VIF = 1` =>  no correlation between Independent Variable and other variables\n* `VIF exceeding 5 or 10` indicates high Multicollinearity between this independent variable and others\n\nAlthough `Correlation Matrix` and `Scatter Plots` can also be used to find multicollinearity but their findings only show `Bivariate` relationship between `Independent Variables` <br>\n\n-----\n**Why I am Preferring `Variable Inflation Factor` over `Correlation Matrix` and `Scatter Plots` ?? <br>**\n`VIF is Preferred as it can show Correlation of a variable with a group of other variables`","1b0070ba":"# Model Evaluation\n`Performance of Regression Model` must be reported as an error in those predictions \n* It will be good to know if model predicted value exactly. (this might be intractably difficult in practice) so instead I am intrusted to know how close predictions are to expected values\n\n`Error addresses exactly this and summarizes on average how close predictions were to their expected values`","a141baa9":"# Data Preparation\nYou can see that your dataset has many columns with values as 'Yes' or 'No'.\n\nWe need to convert them to 1s and 0s, where 1 is a 'Yes' and 0 is a 'No'","64c92ab6":"Know how to build a model with one X (feature variable) and Y (response variable) from this link [Applied Linear Regression Part-1](https:\/\/www.kaggle.com\/mukeshmanral\/linear-regression-basic)\n\nBut what if you have three feature variables, or may be 10 or 100? Building a separate model for each of them, combining them, and then understanding them will be a very difficult and next to impossible task. By using Multiple Linear Regression, you can build models between a response variable and many feature variables","ff734255":"# Regression Evaluation Metrics\nI am trying to take first three error metrics that are commonly used for evaluating and reporting performance of a Regression Model:\n1. `Mean Squared Error (MSE)`\n2. `Root Mean Squared Error (RMSE)`\n3. `Mean Absolute Error (MAE)`\n4. MEAN ABSOLUTE PERCENTAGE ERROR (MAPE)\n5. RELATIVE ABSOLUTE ERROR (RAE)\n6. `R-SQUARE or Coefficient of determination` \n7. ADJUSTED R-SQUARE\n8. MEDIAN ABSOLUTE ERROR (MedAE)\n\n[Regression Evaluation Metrics](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#regression-metrics)\n\n[More detail](https:\/\/www.aionlinecourse.com\/tutorial\/machine-learning\/evaluating-regression-models-performance)","3b83e5b8":"#  Multiple Linear Regression - Model Assumptions\nMultiple Linear Regression Model is based on several assumptions which are listed below:-\n\n1. `Linearity`: Linear relationship between Dependent variables and Independent variables\n2. `Correlation`: Independent variables are not too highly correlated with each other\n3. `yi observations` are selected independently and randomly from population\n4. `Normal Distribution`: Residuals should be normally distributed with a mean of 0 and variance \u03c3","3c6e1137":"# Model (Linear)","b5aa662e":"`Observation:` <br>\n* vif scores is almost less than 5\n\n* I have removed 5 features namely 'bedrooms','bathrooms_per_bedroom','area_per_bedroom','semi-furnished','basement'\n\n`Droping above mentioned 5 features from X as well and rebuilding model again` ","ba90769c":"Above are relevent columns after dropping highly correlated variables","657b0ffc":"vif scores are still high trying to drop `bedrooms` also as input feature and again check vif scores","36ba675b":"# Triying to Understand Spread","2d0b7e8e":"Splitting of test and train procedure involves taking a dataset and dividing it into two subsets \n    * First subset is used to fit the model and is referred to as training dataset\n    * Second subset is not used to train the model; instead, input element of dataset is provided to the model, then predictions are made and compared to expected values. This second dataset is referred to as test dataset","6d017f41":"----\n[Applied Linear Regression Part-1](https:\/\/www.kaggle.com\/mukeshmanral\/linear-regression-basic) In part-one notebook I am trying to cover Basic Linear Regression\n\n-----","2e9665ec":"**`Using model to make predictions`**","b1218a67":"Observe that `mainroad` vif was `5.513523` which got droped to `4.845386` after droping `semi-furnished`, so always keep in mind understanding data is the key. I would have droped `mainrod` before by only observing therhold of < 5 but I didnt did that.If I will drop `mainrod` it will effect our prediction.\n\n\nStill vif scores of some features are close to 5\n* first five features are of importance but then least important among them would be `basement`","ab9b2479":"----\n# Advantages of Multiple Linear Regression\n* Chances of getting a better-fit increase as generated models are dependent on more than 1 feature\n* Multiple Linear Regression can detect outliers and anomalies very effectively\n\n----\n# Disadvantages of Multiple Linear Regression\n* Problem of overfitting is very prevalent here, as we can use all features to generate the model, so the model can start \"memorizing\" the values\n* Accuracy decreases as linearity of dataset decreases","18ee1ea0":"Observe above you will see that relationship between target and features shows some constant relation\n\nLets try luck if this model performs better that the last model","1cc855dc":"# Building Linear Model Again","45e05473":"![image.png](attachment:a78dbcd5-9c5f-4fd6-98d9-609688aaee6c.png)","edffbc58":"# Problem Statement:\n\nConsidering a real estate company that has a dataset containing prices of properties in say any region(Himalayas etc), It wishes to use data to optimise sale prices of properties based on important factors such as area, bedrooms, parking and other.\n\nEssentially company wants \u2014\n* To identify variables affecting house prices, e.g. area, number of rooms, bathrooms etc\n\n* To create a linear model that quantitatively relates house prices with variables such as number of rooms, area, number of bathrooms, etc\n\n* To know accuracy of the model, i.e. how well these variables can predict house prices\n\n","c04f0fd5":"![image.png](attachment:e4717b3b-fc54-47bc-88f8-376bcb1f5093.png)","fb8f4010":"# Checking Multi-Collinearity\n`It generally occurs when there are high correlations between two or more predictor variables` \n* Can also say as, one predictor variable can be used to predict other \n* This creates redundant information lead to skewing results in a Regression Model\n\nI know you have some confusion with term `Collinearity` and `Multicollinearity`, lets dig into it\n\n* `Collinearity is a linear association between two predictors` \n* `Multicollinearity` is a situation where two or more predictors are highly linearly related \n    * Severe multicollinearity is a problem because it can increase `Variance of the coefficient estimates` and make estimates very sensitive to minor changes in the model\n    * Result is that coefficient estimates are unstable and difficult to interpret\n\nSo its important to remove Multicollinearity from our dataset","6979905c":"* `test_size 0.3 means we are giving 30% of the data to test and rest 70% data to train to our model`\n* `random_state = 108` make sure our results will be same","a6347bb1":"Observe two new columns as `area_per_bedroom`,`bathrooms_per_bedroom`","eda9442a":"# Visualising Selected Columns Data","b7f64866":"Trying to compare above given 3 metrics in all three models\n\n# `Observation`\n`1.` linear_model_2 is perfoming better in terms of Coefficient, R2 and adjusted r2\n    * by observing metrics I calculated for linear_model_2 and linear_model_3 here linear_model_2 is better also\n\n`So Finalising that linear_model_2 is the best model to solve our problem statement`  <br>\n**ie.to optimise sale prices of properties based on important factors such as area, bedrooms, parking and other**","369cf1f1":"Columns like `prefarea, airconditioning, hotwaterheating, basement, guestroom, mainroad` have values as yes and no which is difficult for ml model to understang \n\nLets see how i am tryinh to prepare this data then","73189ef1":"`furnishingstatus` column have three diffrent things in it mentioned as `semi-furnished, unfurnished, furnished`\n\nNow try to undestand what I am doing for this\n* I am creating Dummy Variable(One-Hot Encodin) for this column\n","586d5de3":"`Why` you are removing first column??? If this is question you are asking upvote the notebook and ping me to uodate the notebook with all Answers to Why!!!","76ab993a":"`6 features got selected out of initial 11 features`","f5f0c843":"# `Multiple Linear Regression`\n**`In multiple regression, we have multiple independent variables that impact the dependent variable`**","dc74c63d":"Observe `price` column in above data set and compare it with other data set, this is what scaling does.\n\nWhy we are doing Scalling??? Still not clear upvote the notebook and ping me.","be127596":"It have created three variables all are not needed, so i will update my code a bit see","fa34d95f":"# Creating New Variables\n* `area_per_bedroom` => `area \/ bedrooms`  => this will be area per bedroom\n* `bathrooms_per_bedroom` => `bedrooms \/ bedrooms`","10b728de":"![image.png](attachment:f55c98d9-cbb9-4f76-b380-6637b669a2af.png)","57d318dc":"`Observation on Three models` <br>\nObserving results can see that there is not much change in model performance compared to linear_model_2 \n* Infact RMSE score got increased a bit, reason could be that we might have removed one or some features which were of importance\n* May be Linear Model wants some other features also to explain target variable properly","c3094720":"Maybee in upcomming notebooks I will implement these Metrics from scrach, for that i will update the notebook link here only so keep in touch","7eebb3c1":"`Results of linear_model_2` <br>\nRMSE : 0.08954475046778894\n\nMSE : 0.008018262336338587\n\nMAE : 0.0653680064896952"}}