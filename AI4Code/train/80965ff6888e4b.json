{"cell_type":{"ed6672f5":"code","38283016":"code","0c42cbe7":"code","40e24d5e":"code","b05dc146":"code","e438cad2":"code","71b5a73c":"code","ce1ea731":"code","02c13615":"code","1d41a978":"code","5b62c283":"code","1fa03087":"code","0f918b81":"code","17bbd827":"code","b8aa53ce":"code","39ede619":"code","364c4fb7":"code","7f9ec0d5":"code","472ffd48":"code","38fde2b0":"code","168023d1":"code","24862f7c":"code","730e8980":"code","67fceddd":"code","30ac5928":"code","9bfb1f9f":"code","28d1a3e8":"code","efe312ed":"code","ef24ef0f":"code","541af5ce":"code","caa3aafb":"code","c279bf33":"code","d153029c":"code","9abb6bb5":"code","18deba34":"code","820949f6":"code","d285d272":"code","1bb94158":"code","edcf4433":"code","647656ce":"code","80186e8e":"code","19d77a22":"code","c394aad9":"code","eebf1ba8":"code","b62c1a59":"code","1c884c22":"code","dc7b87de":"code","e3f483f6":"code","ec727aca":"code","3ba9bf55":"code","c43094f3":"code","4b336b68":"code","62671ecf":"code","96436ddf":"code","e98e676c":"code","56489e3d":"code","583974c0":"code","af7cff71":"code","187dc4be":"code","fb17a935":"code","6c457d62":"code","991dc6d7":"code","a5347088":"code","cb046e2e":"code","b97dfa7a":"code","10507e4d":"code","a8710577":"code","8cccd37c":"code","41aae40f":"code","6548a2e6":"code","730cbf50":"code","0c97d80c":"code","2eda4645":"code","0761f168":"code","7884d72e":"code","89cb5b8f":"code","02151e17":"code","20b2b6ea":"markdown","a9e90267":"markdown","f800a9fa":"markdown","68876a3c":"markdown","5f47b39d":"markdown","afbf8d73":"markdown","dd8b7dd6":"markdown","feee9c2f":"markdown","26fe61f6":"markdown","d4bab7bc":"markdown","801b946d":"markdown","78e86bd2":"markdown","1a01cfea":"markdown","23015cca":"markdown","0fc0c0e4":"markdown","e85dc63b":"markdown","70053a6d":"markdown","d633d1cc":"markdown","429888ab":"markdown","297ea286":"markdown","bebcd4e6":"markdown","8622b710":"markdown","913b620b":"markdown","1550a257":"markdown","4e841775":"markdown","74178d7a":"markdown","d5b4d92c":"markdown","8c507649":"markdown","ca1cf4e6":"markdown","78af7ccb":"markdown","b8d742ce":"markdown","6331698f":"markdown","9991dcbb":"markdown","d213f277":"markdown","df9e11d3":"markdown","c28fc6cb":"markdown","af1630af":"markdown","62d74eb6":"markdown","be97ce23":"markdown","5c6f7f92":"markdown","ddc17f59":"markdown","dac486f0":"markdown","79c75331":"markdown","a46cedcd":"markdown","c5a0acea":"markdown","3afb43f7":"markdown","c956595b":"markdown","fbd18244":"markdown","01d1b176":"markdown","fc00d2bc":"markdown","1fa24a21":"markdown","e6ceb278":"markdown","e5c0ea31":"markdown","558805e1":"markdown","1605ac21":"markdown","4d5495b5":"markdown","b3dbe539":"markdown","6b71dc58":"markdown","32bc7814":"markdown","23c3d26b":"markdown","ab72b8bb":"markdown","20f7d781":"markdown","806ff931":"markdown","83cbed21":"markdown","ad1b19c7":"markdown","d992d571":"markdown"},"source":{"ed6672f5":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport scipy.stats as stats\nimport numpy as np\nfrom pandas.api.types import is_numeric_dtype\nfrom statsmodels.stats.proportion import proportions_ztest\nimport seaborn as sns\n\nfrom matplotlib import pyplot as plt\nfrom scipy.stats import gamma\nfrom sklearn.preprocessing import StandardScaler\nsns.set(color_codes=True)\n%matplotlib inline","38283016":"df = pd.read_csv(\"..\/input\/parkinsons\/datasets_410614_786211_parkinsons.csv\", sep=',')","0c42cbe7":"df.shape","40e24d5e":"df.head()","b05dc146":"df.info()","e438cad2":"df.isnull().values.any()","71b5a73c":"df.isnull().any()","ce1ea731":"df.isnull().sum()","02c13615":"len(df['name'].unique())","1d41a978":"df.shape[0]","5b62c283":"df['status'].unique()","1fa03087":"df['status'] = df['status'].astype('object')","0f918b81":"df.info(verbose = True)","17bbd827":"def Distribution_Continous_Variables(series,color,title):\n    plt.figure(figsize=(10, 5))\n    sns.distplot(series, color = color).set_title(title)\n    \ndef Print_Summary(series,title,var):\n    print(title)\n    print('Count = {1}'.format(var,len(series)))\n    print('Mean of {0} = {1}'.format(var,series.mean()))\n    print('Median of {0} = {1}'.format(var,series.median()))\n    print('Mode of {0} = {1}'.format(var,series.mode().values[0]))\n    print('Skewness of {0} = {1}'.format(var, series.skew()))\n    print('Excess Kurtosis of {0} = {1}'.format(var,series.kurtosis()))\n    print(100*\"*\")\n\ndef Coeff_Variation(series,title,var):\n    print('CV of {0} for {1} = {2}'.format(var,title,(series.std()\/series.mean())*100))\n\ndef BoxPlot(**kwargs):\n    plt.figure(figsize=(10, 5))\n    sns.boxplot(x = kwargs['x'], \\\n                y = kwargs['y'], \\\n                data = kwargs['data'], \\\n                color = kwargs['color'], \\\n                hue = kwargs['hue']).set_title(kwargs['title'])    \n\ndef ViolinPlot(**kwargs):\n    plt.figure(figsize=(10, 5))\n    sns.violinplot(x = kwargs['x'], \\\n                y = kwargs['y'], \\\n                data = kwargs['data'], \\\n                color = kwargs['color'], \\\n                hue = kwargs['hue']).set_title(kwargs['title']) \n    \n        \ndef CountPlot(**kwargs):\n    plt.figure(figsize=(10, 5))\n    sns.countplot(y=kwargs['y'], \\\n                    hue=kwargs['hue'], \\\n                    data=kwargs['data']).set_title(kwargs['title'])\n    \n   ","b8aa53ce":"list_of_non_object_cols = df.loc[:, df.dtypes != 'object'].columns.tolist()","39ede619":"for col in list_of_non_object_cols:\n    Distribution_Continous_Variables(df[df['status']==0][col],\"green\",\"Distribution of {} in the dataset for healthy patients\"\\\n                                    .format(col))\n    Distribution_Continous_Variables(df[df['status']==1][col],\"red\",\"Distribution of {} in the dataset for patients with parkinson's disease\"\\\n                                    .format(col))","364c4fb7":"df.describe().T","7f9ec0d5":"for col in list_of_non_object_cols:\n    Print_Summary(df[df['status']==0][col],\"Numerical Summary of {} for healthy patients\"\\\n                                    .format(col),col)\n    Print_Summary(df[df['status']==1][col],\"Numerical Summary of {} for patients with parkinson's disease\"\\\n                                    .format(col),col)","472ffd48":"CountPlot(y = 'status',\\\n          hue = 'status',\\\n          data = df,\\\n          title = \"Count comparison of healthy people vs people with parkinson's disease\")","38fde2b0":"print(\"Percentage of patients diagnosed as healthy = {0:.2f}%\".format((df[df.status == 0].shape[0]\/df.shape[0])*100))","168023d1":"print(\"Percentage of patients diagnosed with Parkinson's disease = {0:.2f} %\".format((df[df.status == 1].shape[0]\/df.shape[0])*100))","24862f7c":"for col in list_of_non_object_cols:\n    BoxPlot(x = 'status',\\\n            y = col,\\\n            data = df,\\\n            color = 'orange',\\\n            hue = 'status',\\\n            title = 'Boxplot of {}'.format(col))\n\n    ViolinPlot(x = 'status',\\\n            y = col,\\\n            data = df,\\\n            color = 'orange',\\\n            hue = 'status',\\\n            title = 'Violinplot of {}'.format(col))","730e8980":"for col in list_of_non_object_cols:\n    Coeff_Variation(df[df['status']==0][col],\"for healthy subjects\".format(col),col)\n    Coeff_Variation(df[df['status']==1][col],\"for patients with Parkinson's disease\".format(col),col)\n    print(100*\"*\")","67fceddd":"sns.pairplot(df, hue = 'status')\nplt.show()","30ac5928":"plt.figure(figsize=(20, 20))\ndf_corr = df.corr(method='pearson')\nax = sns.heatmap(df_corr, annot=True, cmap='YlGnBu')\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","9bfb1f9f":"plt.figure(figsize=(12, 10))\ndf_corr = df.corr(method='pearson')\nax = sns.heatmap(df_corr[(df_corr >= 0.80) | (df_corr <= -0.80)], annot=True, cmap='magma')\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","28d1a3e8":"df.drop(columns=['name'],inplace=True)","efe312ed":"non_obj_cols = df.loc[:, df.dtypes != 'object'].columns.tolist()\nz = np.abs(stats.zscore(df[non_obj_cols]))","ef24ef0f":"df_clean = df[(z<3).all(axis=1)]\ndf_clean.shape,df.shape","541af5ce":"plt.figure(figsize=(12, 10))\ndf_corr = df_clean.corr(method='pearson')\nax = sns.heatmap(df_corr[(df_corr >= 0.80) | (df_corr <= -0.80)], annot=True, cmap='magma')\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","caa3aafb":"upper_triangle = df_corr.where(np.triu(np.ones(df_corr.shape),k=1).astype('bool'))\ncols_to_drop = [column for column in upper_triangle.columns \\\n                if any((upper_triangle[column] >= 0.80) | (upper_triangle[column] <= -0.80))]","c279bf33":"cols_to_drop","d153029c":"df_copy = df_clean.drop(columns=cols_to_drop).copy()","9abb6bb5":"plt.figure(figsize=(12, 10))\ndf_corr = df_copy.corr(method='pearson')\nax = sns.heatmap(df_corr, annot=True, cmap='YlGnBu')\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","18deba34":"df_copy.shape","820949f6":"X = df_copy.copy()\ncont_feat = X.loc[:,X.dtypes != 'object'].columns.tolist()\nfeatures = X[cont_feat]\nscaler = StandardScaler().fit(features.values)\nfeatures = scaler.transform(features.values)","d285d272":"X[cont_feat] = features","1bb94158":"X.head()","edcf4433":"CountPlot(y = 'status',\\\n          hue = 'status',\\\n          data = X,\\\n          title = \"Count comparison of healthy people vs people with parkinson's disease\")","647656ce":"print(\"Percentage of patients diagnosed as healthy = {0:.2f}%\".format((X[X.status == 0].shape[0]\/X.shape[0])*100))","80186e8e":"print(\"Percentage of patients diagnosed with Parkinson's disease = {0:.2f} %\".format((X[X.status == 1].shape[0]\/X.shape[0])*100))","19d77a22":"y = X['status'] #------Target variable-------------#\nX = X.drop(columns=['status']) #------Features--------#","c394aad9":"y = y.astype('int')","eebf1ba8":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42, stratify=y)","b62c1a59":"print(\"Percentage of patients diagnosed as healthy in train dataset = {0:.2f}%\".format((y_train[y_train == 0].shape[-1]\/y_train.shape[-1])*100))","1c884c22":"print(\"Percentage of patients diagnosed with Parkinson's disease in train dataset = {0:.2f} %\".format((y_train[y_train == 1].shape[-1]\/y_train.shape[-1])*100))","dc7b87de":"print(\"Percentage of patients diagnosed as healthy in test dataset = {0:.2f}%\".format((y_test[y_test == 0].shape[-1]\/y_test.shape[-1])*100))","e3f483f6":"print(\"Percentage of patients diagnosed with Parkinson's disease in test dataset = {0:.2f} %\".format((y_test[y_test == 1].shape[-1]\/y_test.shape[-1])*100))","ec727aca":"from sklearn.linear_model import LogisticRegression\nfrom tqdm import tqdm\nfrom sklearn.metrics import confusion_matrix, classification_report\ntrain_scores = []\ntest_scores = []\nc_val = []\nfor i in tqdm(range(-2,2,1)):\n    lr = LogisticRegression(C=pow(10,i))\n    lr.fit(X_train, y_train)\n    \n    c_val.append(pow(10,i))\n    train_scores.append(lr.score(X_train,y_train))\n    test_scores.append(lr.score(X_test,y_test))","3ba9bf55":"max_train_score = max(train_scores)\nmax_train_pos = train_scores.index(max(train_scores))\nmax_c_val = c_val[max_train_pos]\nprint(\"At C = {} the max train score of the Logisitic regression model = {}\".format(max_c_val, max_train_score))","c43094f3":"plt.figure(figsize=(12,5))\np = sns.lineplot(c_val,train_scores,marker='*',label='Train Score')\np = sns.lineplot(c_val,test_scores,marker='o',label='Test Score')","4b336b68":"lr = LogisticRegression(C=0.1)\nlr.fit(X_train, y_train)\nprint(lr.score(X_test, y_test))","62671ecf":"plt.figure(figsize=(10, 5))\ny_pred = lr.predict(X_test)\ncnf_matrix = confusion_matrix(y_test, y_pred)\np = sns.heatmap(cnf_matrix, annot=True, cmap=\"YlGnBu\" ,fmt='g')\nbottom, top = ax.get_ylim()\np.set_ylim(2, 0.0)\nplt.title('Confusion matrix for logistic regression data')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","96436ddf":"print(classification_report(y_test,y_pred))","e98e676c":"from sklearn.neighbors import KNeighborsClassifier\nfrom tqdm import tqdm\nfrom sklearn.metrics import confusion_matrix, classification_report\ntrain_scores = []\ntest_scores = []\nk_val = []\nfor i in tqdm(range(1,7)):\n    knn = KNeighborsClassifier(i)\n    knn.fit(X_train, y_train)\n    \n    k_val.append(i)\n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))","56489e3d":"max_train_score = max(train_scores)\nmax_train_pos = train_scores.index(max(train_scores))\nmax_k_val = k_val[max_train_pos]\nprint(\"At K = {} the max train score of the K nearest neghbour = {}\".format(max_k_val, max_train_score))","583974c0":"max_test_score = max(test_scores)\nmax_test_pos = test_scores.index(max(test_scores))\nmax_k_val = k_val[max_test_pos]\nprint(\"At K = {} the max test score of the K nearest neghbour model = {}\".format(max_k_val, max_test_score))","af7cff71":"plt.figure(figsize=(12,5))\np = sns.lineplot(k_val,train_scores,marker='*',label='Train Score')\np = sns.lineplot(k_val,test_scores,marker='o',label='Test Score')","187dc4be":"knn = KNeighborsClassifier(3)\nknn.fit(X_train, y_train)\nprint(knn.score(X_test, y_test))","fb17a935":"plt.figure(figsize=(10, 5))\ny_pred = knn.predict(X_test)\ncnf_matrix = confusion_matrix(y_test, y_pred)\np = sns.heatmap(cnf_matrix, annot=True, cmap=\"YlGnBu\" ,fmt='g')\nbottom, top = ax.get_ylim()\np.set_ylim(2, 0.0)\nplt.title('Confusion matrix for KNN model')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","6c457d62":"print(classification_report(y_test,y_pred))","991dc6d7":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nparam_grid = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf']}  \n  \nsvm = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3) \nsvm.fit(X_train, y_train) ","a5347088":"print(svm.best_params_)","cb046e2e":"print(svm.best_estimator_)","b97dfa7a":"svm = SVC(kernel='rbf', C=10, gamma=0.1)\nsvm.fit(X_train,y_train)\nprint(svm.score(X_test, y_test))","10507e4d":"plt.figure(figsize=(10, 5))\ny_pred = svm.predict(X_test)\ncnf_matrix = confusion_matrix(y_test, y_pred)\np = sns.heatmap(cnf_matrix, annot=True, cmap=\"YlGnBu\" ,fmt='g')\nbottom, top = ax.get_ylim()\np.set_ylim(2, 0.0)\nplt.title('Confusion matrix for SVM model')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","a8710577":"print(classification_report(y_test,y_pred))","8cccd37c":"from sklearn.ensemble import StackingClassifier\nestimators = [('lr',lr),('knn',knn)]\nsclf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\nsclf.fit(X_train,y_train)\nprint(sclf.score(X_test,y_test))","41aae40f":"plt.figure(figsize=(10, 5))\ny_pred = sclf.predict(X_test)\ncnf_matrix = confusion_matrix(y_test, y_pred)\np = sns.heatmap(cnf_matrix, annot=True, cmap=\"YlGnBu\" ,fmt='g')\nbottom, top = ax.get_ylim()\np.set_ylim(2, 0.0)\nplt.title('Confusion matrix for Meta-Classifier model')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","6548a2e6":"print(classification_report(y_test,y_pred))","730cbf50":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'max_depth': [3,4,5,6,7,8],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'min_child_weight': [0.01,0.001,1]}  \n  \nxgb = GridSearchCV(XGBClassifier(), param_grid, refit = True, verbose = 3) \nxgb.fit(X_train, y_train) ","0c97d80c":"print(xgb.best_params_)","2eda4645":"print(xgb.best_estimator_)","0761f168":"xgb = XGBClassifier(gamma = 1, max_depth = 4, min_child_weight = 1)\nxgb.fit(X_train,y_train)\nxgb.score(X_test,y_test)","7884d72e":"plt.figure(figsize=(10, 5))\ny_pred = xgb.predict(X_test)\ncnf_matrix = confusion_matrix(y_test, y_pred)\np = sns.heatmap(cnf_matrix, annot=True, cmap=\"YlGnBu\" ,fmt='g')\nbottom, top = ax.get_ylim()\np.set_ylim(2, 0.0)\nplt.title('Confusion matrix for Meta-Classifier model')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","89cb5b8f":"print(classification_report(y_test,y_pred))","02151e17":"from prettytable import PrettyTable\n    \nx = PrettyTable()\n\nx.field_names = [\"Algorithm\", \"F1 Score for sick subjects\",\"Macro Avg F1\",\"Weighted Avg F1\"]\n\nx.add_row([\"Logistic Regression\",\"0.91\",\"0.79\",\"0.85\"])\nx.add_row([\"K nearest neigbour\",\"0.97\",\"0.95\",\"0.96\"])\nx.add_row([\"Support Vector Machine\",\"0.99\",\"0.98\",\"0.98\"])\nx.add_row([\"Stacking Classifier\",\"0.99\",\"0.98\",\"0.98\"])\nx.add_row([\"XGboost Classifier\",\"0.93\",\"0.85\",\"0.89\"])\nprint(x)","20b2b6ea":"## Functions to ease up plotting of various continous variables","a9e90267":"Now after cleaning we are left with 13 features and 181 rows","f800a9fa":"## Some preprocessing","68876a3c":"#### Observations: From the plot we could see that there might be positive or negative correlation between certain variables.Like between spread1 and PPE there seems to be a positive correlation. We will further analyse the correlation by meausring pearson coefficient of correlation between all features","5f47b39d":"#### Observation: From the above plot we can conclude that best model can be fitted at C =  0.1. Hence we would fit our model on test data with C = 0.1","afbf8d73":"By using z score method we have removed around 14 outlier rows","dd8b7dd6":"* Above we have stacked logistic regression model and k nearest neighbr model and as a result of which our accuracy is 98%\n* But we need to check f1 score in order to confirm if our model is better than svm model or not","feee9c2f":"#### Observation: From the above plot we can conclude that best model can be fitted at K =  3. Hence we would fit our model on test data with K = 3","26fe61f6":"#### Observations: Since number of rows is equivalent to count of unique values of column 'name' , hence there are no duplicate values of feature 'name'","d4bab7bc":"# Training a meta classifier","801b946d":"# Building and Analysis of Machine learning models","78e86bd2":"## Check for duplicate names","1a01cfea":"## Confusion Matrix","23015cca":"### Observation:\n* So after stacking the logistic regression and knn model(who were not as good as svm model), we got f1 scores almost similar to that of the SVM model\n* This proves that ensembling 2 inferior models could result in a superior model","0fc0c0e4":"## Univariate Analysis","e85dc63b":"It is a binary categorical data as we observed with only 2 unique values i.e 0 and 1","70053a6d":"### Preprocessing and Feature engineering before Model Building","d633d1cc":"#### We will remove column name as it is not anyway useful for us in model building","429888ab":"# Loading data into dataframe","297ea286":"#### Let's plot highly positively correlated or negatively correlated features. We will use 0.80 and -0.80 as our threshold correlation coefficient for positive and negative correlations respectively. Usually ensemble mehtods handle correlations well but highly correlated features may create problems.","bebcd4e6":"### Let's plot the correlation matrix again after removing correlated variables","8622b710":"### Measuring correlation","913b620b":"#### Next we will try to get a numerical summary pertaining to our features to further consolidate our observation","1550a257":"### Conclusion : Our model's macro f1 score and the f1 score in case of status 1(sick subjects) is 0.99 and 0.98 respectively which is very good when compared with both logistic regression model and the k nearest neighbor model","4e841775":"#### We have a total of 23 features and 195 rows of data","74178d7a":"* #### We will plot boxplot now to observe if our continous features have outliers or not. The boxplot uses IQR method to   detect outliers. Also boxplot would give a visual representation of the five point summary\n* #### We will also plot violinplot along with boxplots to compare the distributions as well.","d5b4d92c":"### Conclusion : Our model's macro f1 score and the f1 score in case of status 1(sick subjects) is 0.96 and 0.97 respectively which is very good when compared with logistic regression model","8c507649":"* The f1-score is a better metric to measure the performance of our model when compared with accuracy, because our data is imbalanced\n* In case if a person is sick and if our model predicts him to be healthy then that would be a risky model, hence we must consider F1 score for sick subjects as well as Macro average F1 score to rate a particular model.\n* From above table we could observe that almost all of our models perform decently on the given dataset\n* But SVM and our meta classifer where we stacked logistic regression model and k nearest neigbor model perform better than the other models.\n* Hence, we can either pick SVM model or the simple stacked ensemble model to predict whether a subject is suffering from Parkinson's disease or not","ca1cf4e6":"### Plots to analyse impact of continous on the status","78af7ccb":"Now we will be using a meta-classifier that would take different models as estimators and would stack them up to come up with the best classifier model. For that we would be using Stacking classifier","b8d742ce":"## Using boosting ensemble method","6331698f":"## BiVariate Analysis","9991dcbb":"#### Observations: \n* From the above set of graphs we can observe that majority of our continous variables are skewed to the right and we cannot observe any perfectly normally distributed continous variable.\n* This somehow provides us a rough idea that our dataset is full of outliers that we need to get rid of.","d213f277":"# Final Conclusion and model comparison","df9e11d3":"#### Observations: From above we can observe that there are no null values","c28fc6cb":"We will use gridsearchcv for hyperparameter tuning since we have 2 hyperparameters i.e C and gamma","af1630af":"## Logistic regression model","62d74eb6":"### Standardizing continous variables","be97ce23":"#### Observations : After performing some data cleaning and feature engineering we can see that percentage distribution between 2 status has slightly improved","5c6f7f92":"## Data Description and Context:\nParkinson\u2019s Disease (PD) is a degenerative neurological disorder marked by decreased dopamine levels in the brain. It manifests itself through a deterioration of movement, including the presence of tremors and stiffness. There is commonly a marked effect on speech, including dysarthria (difficulty articulating sounds), hypophonia (lowered volume), and monotone (reduced pitch range). Additionally, cognitive impairments and changes in mood can occur, and risk of dementia is increased.\nTraditional diagnosis of Parkinson\u2019s Disease involves a clinician taking a neurological history of the patient and observing motor skills in various situations. Since there is no definitive laboratory test to diagnose PD, diagnosis is often difficult, particularly in the early stages when motor effects are not yet severe. Monitoring progression of the disease over time requires repeated clinic visits by the patient. An effective screening process, particularly one that doesn\u2019t require a clinic visit, would be beneficial. Since PD patients exhibit characteristic vocal features, voice recordings are a useful and non-invasive tool for diagnosis. If machine learning algorithms could be applied to a voice recording dataset to accurately diagnosis PD, this would be an effective screening step prior to an appointment with a clinician\n\n## Domain:\nMedicine\n    \n## Attribute Information:\n* name - ASCII subject name and recording number\n* MDVP:Fo(Hz) - Average vocal fundamental frequency\n* MDVP:Fhi(Hz) - Maximum vocal fundamental frequency\n* MDVP:Flo(Hz) - Minimum vocal fundamental frequency\n* MDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP - Several\n* measures of variation in fundamental frequency\n* MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA - Several measures of variation in amplitude\n* NHR,HNR - Two measures of ratio of noise to tonal components in the voice\n* status - Health status of the subject (one) - Parkinson's, (zero) - healthy\n* RPDE,D2 - Two nonlinear dynamical complexity measures\n* DFA - Signal fractal scaling exponent\n* spread1,spread2,PPE - Three nonlinear measures of fundamental frequency variation 9. \n* car name: string (unique for each instance)\n\n    \n## Learning Outcomes:\n\u25cf Exploratory Data Analysis\n\n\u25cf Supervised Learning\n\n\u25cf Ensemble Learning\n\n## Objective:\nGoal is to classify the patients into the respective labels using the attributes from their voice recordings","ddc17f59":"## K nearest nighbour model","dac486f0":"# Exploratory Data Analysis(EDA)","79c75331":"* We observed in the countplot in the EDA section that there is an imbalance between the count of patients and healthy subjects\n* To counter this we could have used SMOTE oversampling method which produces sythetic variables for minority class by using k nearest neighbors. \n* But a dataset like that would make our model less efficient while treating real life datasets where we may only find imbalance of datapoints between patients and healthy subjects.","a46cedcd":"# Parkinson's disease based on voice recording","c5a0acea":"### Pairplot with Status as hue","3afb43f7":"#### After removing outliers let us observe the correlation matrix again","c956595b":"#### Analysing the count after split for both test and train data","fbd18244":"#### Observations:\n* The data is imbalanced since we have 24.6 % of healthy patients while 75.38 % are sick.\n* PPE for healthy patients,spread2 for both types of subjects, RPDE for both types of subjects tend to have a normal distribution as per their statisticals values while other features seem to be skewed.","01d1b176":"# Importing libraries","fc00d2bc":"## Converting status to object data type","1fa24a21":"#### Observation: We can see that all the features except our target feature status is continous but we should convert status to object data type since it is a binary categorical feature ","e6ceb278":"### Let's remove highly correlated features (both negative and positive) from the cleaned up data","e5c0ea31":"## Train-Test split (70:30)","558805e1":"#### Removing outliers using z-score method:We detected outliers in many features by using IQR method. Since we have less number of rows in our dataset , we ould rather use z-score method to detect outliers. If the z score value is 3 and above i.e if a value is beyond 3 standard deviations we would drop those rows.","1605ac21":"### Coefficient of Variation(CV) of continous variables\n* CV is relative comparison of the distributions with respect to their standard deviations.\n* This is unit agnostic i.e units do not have any impact on its value\n* Greater the number ,greater is the variability","4d5495b5":"#### Why we do not need to treat the imbalanced dataset via oversampling?","b3dbe539":"#### Observation :  The split in both test and train data is almost in same proportion","6b71dc58":"## Checking for null values","32bc7814":"### Shape after removing correlated features","23c3d26b":"#### Observations: The correlation matrix after removing outliers contains fewer highly correlated features.This proves the fact that correlation coefficients do get affected by the presence of outliers.","ab72b8bb":"## Support Vector Machine model","20f7d781":"#### Observations: We can clearly observe here that there are plenty of highly correlated features in our dataset which we need to remove before as part of the data cleaning before we can utilise our dataset for machine learning. High correlation also happens due to imbalanced dataset. So we will try to balance it by using oversampling and then would observe our correlation heatmap again","806ff931":"## Confusion Matrix","83cbed21":"Test accuracy is at 96% but it is not a correct metric to measure the performance of our model","ad1b19c7":"## Conclusion:\n* The f1-score is a better metric to measure the performance of our model when compared with accuracy, because our data is imbalanced<li>In case if a person is sick and if our model predicts him to be healthy then that would be a risky model\n* Here in this case the f1 score for the patients with parkinson's is better at 0.91 i.e the patients who actually have parkinson's have higher chance of getting diagnosed correctly by our logistic regression model.\n* Also we have a decent macro average f1 score at 0.79 which signifies that our model can diagnose a person and predict whether he is healthy or sick with high accuracy.\n* The weighted average f1 takes into account the class imbalance and assigns higher weightage to the minority class(in this case class of sick person). The score of weighted average of 85% further consolidates our point that this model performs well while classifying the sick.\n* For the sake of convinience we ould consider f1-scores as the metric for the model performance moving ahead.<li>We would be more interested in f1-scores of status 1(sick subjects) and macro avg f1 scores","d992d571":"#### Observations : Except MDVP:Fo(Hz),MDVP:Flo(Hz),RPDE and DFA every other feature has outliers as per IQR method."}}