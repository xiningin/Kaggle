{"cell_type":{"8e04c0e3":"code","f8abd456":"code","a1ebe08b":"code","9c7813dd":"code","5f41f6e8":"code","99aacc1a":"code","55ab2788":"code","3bd42e63":"code","c8e7e2a4":"code","4d6b0e62":"code","760ce5e1":"code","6ca2f0e6":"code","eb2498a8":"code","d949971a":"code","b83652c9":"code","49e3dee6":"code","3de68301":"code","5e7767f3":"code","8ba2f09e":"code","1e0a58a8":"code","953c02cd":"code","08d5f49c":"code","c769026c":"code","02827f10":"code","f5d157d3":"code","caee7a52":"code","a0a4f76e":"code","4feb5e29":"code","87f90013":"code","319a1dfc":"code","38b7fea3":"code","05f7eef6":"code","f4c298dc":"code","84b9e91b":"code","0cf5d0b5":"code","2325dc77":"code","65fb4dab":"code","8363ba61":"code","9290efd1":"code","f2baf241":"code","c0292871":"code","1f8566fb":"code","537f3c76":"code","36377248":"code","0885a4cc":"code","1c300bfe":"code","47a77489":"code","ed1f2ea1":"code","f9694f02":"code","a7530e0c":"code","d2ebb83c":"code","7ea104ab":"code","acb6dc76":"code","e573b66d":"code","fcc18672":"code","8d267277":"code","4a46e869":"code","dd1438af":"code","4312f068":"code","31c3db89":"code","0951012c":"code","91f1d909":"code","b1bf4311":"code","3080d09b":"code","4448b3a5":"code","13c4994e":"code","fe63b69f":"code","23efdcca":"code","5109f559":"code","8dbe75ab":"code","edbe0638":"code","be312cd3":"code","d3535c22":"code","d843a9e9":"code","44c0dce9":"code","633da7f6":"code","d464e525":"code","e7a48af8":"code","d1208280":"code","35425cd1":"code","69efda9c":"code","0974c2e7":"code","ee823628":"code","75ad5a06":"code","72623b99":"code","4fda626b":"code","35c30a0a":"code","f02a4615":"code","3556531d":"code","ddf6b0b9":"code","74e8440e":"code","0f3a4245":"code","1255f87b":"code","b7cbb608":"code","50674657":"code","14936956":"code","84674b68":"code","d7f115c1":"code","bec08c18":"code","acc3129e":"code","beea76d4":"code","159cc6f1":"code","a1e57e14":"code","79045b3b":"code","493a1738":"code","ef1773ee":"code","219ece12":"code","0a1d528f":"code","56b57d0a":"code","7e5bc446":"code","f2af9e82":"code","083ef93f":"code","474a8c66":"code","16ba02e8":"code","e2ae83db":"code","894c0651":"code","fb5618a8":"code","c2fc4720":"code","09e662e7":"code","f6be6118":"code","4d083958":"code","624c96e6":"code","30f80c41":"code","94560ff3":"code","f494bd06":"code","4cf5ad40":"code","ed8be308":"code","2ffbf565":"code","334fe58e":"code","6e68e62a":"code","094c7cca":"code","62ab14d9":"code","8b041a02":"code","d59fcf32":"code","41485188":"code","c8a77e04":"code","4d3a349a":"code","4932fd69":"code","2d1faa02":"code","b1efef53":"code","ca710ef1":"code","e7c791a9":"code","8d248c41":"code","40868121":"code","226e94ff":"code","1ac9b180":"code","5186e1e9":"code","787ee3fa":"code","2e25d4c9":"code","4629234a":"code","ee70806e":"code","f67086a0":"code","f2c919be":"code","1d789704":"code","de66fe84":"code","bc06ba8c":"code","9bf4ee18":"code","1024363a":"code","3f92e707":"code","20f373d8":"code","0f9bc145":"code","e0b8dbf3":"code","fcd35ee2":"code","24d6304e":"code","088b856b":"code","f67698ef":"code","64b09668":"code","bbc9aa07":"code","e474187c":"code","39547398":"code","14f03c3f":"code","35605a2d":"code","6e36b034":"code","c265b3ef":"code","4e21744c":"code","e17b607b":"code","6987126c":"code","874bff15":"code","1d4f2461":"code","1dc286c8":"code","b87d78d5":"code","f6dcb8ce":"code","38c8f951":"code","01df2369":"code","b58a19ef":"code","0765012d":"code","1b4bbfc1":"code","55d30aee":"code","2eeddeef":"code","f363904c":"code","759e5d21":"code","a944029d":"code","2de3c306":"code","92e0eacb":"code","df8f4132":"code","596f8d68":"code","94b7a139":"code","881068e8":"code","cf0fa9be":"code","29439480":"code","0bdfcb68":"code","bf6d66c6":"code","9e53c4a5":"code","9320cfcc":"code","0f56892f":"code","7c4e6d1c":"code","88abc918":"code","180247fb":"code","e854104f":"code","45914496":"code","c1d51b37":"code","ac068275":"code","9859c067":"code","a06a6c16":"code","43853a94":"code","2b7cbf59":"code","bbb23318":"code","fb46b759":"code","213c7361":"code","50d88016":"code","06d1395d":"code","d7252b02":"code","1f48f54a":"code","f08b4152":"code","c4c1c3ec":"code","b16ce99f":"code","ef963c7d":"code","03743aea":"code","4ef043ba":"code","58c4add0":"code","50eb665e":"markdown","cd4765f1":"markdown","e52d4fbd":"markdown","74b2046d":"markdown","2ac430e9":"markdown","799c261e":"markdown","1510aa86":"markdown","f10d8d89":"markdown","b1686991":"markdown","9a0312ea":"markdown","fa287850":"markdown","aea3369d":"markdown","f8848062":"markdown","00f9064e":"markdown","7b7dd2a3":"markdown","c05fff5e":"markdown","d18285f6":"markdown","54af9825":"markdown","3df28d6a":"markdown","fa06f641":"markdown","dbc501a7":"markdown","16e89b5b":"markdown","cab9002d":"markdown","902d0606":"markdown","2e73ed32":"markdown","74b92dcb":"markdown","2d0aa693":"markdown","62d90018":"markdown","6f82ddc3":"markdown","210fcb7d":"markdown","20241f1a":"markdown","423febc3":"markdown","f9c43e69":"markdown","7a0adf31":"markdown","d3a1e0ec":"markdown","ddab7bbc":"markdown","5410d090":"markdown","661cc4cc":"markdown","3744779c":"markdown","8350d2fe":"markdown","8f34b2f2":"markdown","fa4134cd":"markdown","114ea69d":"markdown","43750d7a":"markdown","1733c26a":"markdown","23186832":"markdown","09f15bfc":"markdown","f8a270d9":"markdown","8d1bd052":"markdown","ba9547d1":"markdown","068a0e7b":"markdown","b14b0593":"markdown","7a3c7d12":"markdown","e480143b":"markdown","1f42e9f8":"markdown","dae1a7b1":"markdown","8da460b8":"markdown","a9a01221":"markdown","3a594c96":"markdown","797bff4c":"markdown","90fe787d":"markdown","577fff1c":"markdown","e4e3871f":"markdown","39f08673":"markdown","7f34e08e":"markdown","748796cd":"markdown","517d360f":"markdown","5f154833":"markdown","ea2bd3e2":"markdown","63545713":"markdown","2f85055f":"markdown","99862670":"markdown","432ff012":"markdown","68dc865c":"markdown","4dc77c0e":"markdown","80d95e88":"markdown","16b9e411":"markdown","ff965809":"markdown","e92dbd4e":"markdown","ae46e607":"markdown","f30aa01f":"markdown","ad73a27b":"markdown","42c09934":"markdown","c7e954aa":"markdown","2c71f24f":"markdown","4c4023dc":"markdown","13d42522":"markdown","a5409109":"markdown","cb2b4097":"markdown","8930fb33":"markdown","ef4c8f98":"markdown","0075e5b1":"markdown","36ec04b1":"markdown","196ed7b9":"markdown","5c647894":"markdown","26fc4fc3":"markdown","fc126f45":"markdown","fd56ee83":"markdown","7a24826f":"markdown","e0b1f26d":"markdown","7f598f93":"markdown","322a9458":"markdown","deb4424f":"markdown","80343a56":"markdown","54f40dfa":"markdown","ac19c23c":"markdown","6c9fad85":"markdown","62f3c211":"markdown","251f58d5":"markdown","66e5d0e4":"markdown","f32560a3":"markdown","496a4688":"markdown","03c0c805":"markdown","6516bac8":"markdown","0987d412":"markdown","842fdf1e":"markdown","cb4ad97b":"markdown","9a926260":"markdown","81ae7126":"markdown","a307e1d8":"markdown","92700c05":"markdown","cf755c21":"markdown","823312c0":"markdown","6d93a23f":"markdown","22235e92":"markdown","69a5c397":"markdown","6abecc3a":"markdown","9cf776a5":"markdown","958e7260":"markdown","52633078":"markdown","f8098a7f":"markdown","97707a4c":"markdown","6567a014":"markdown","984c3179":"markdown","a46f8fbf":"markdown","f8735b7d":"markdown","38a2612c":"markdown","245f34eb":"markdown","02377f24":"markdown","94f8e56e":"markdown","dd224d6d":"markdown","502c988b":"markdown","ac59c3eb":"markdown","db9fb924":"markdown","f69f7501":"markdown","4b29d4e5":"markdown","d8059d76":"markdown","f18320c0":"markdown","f39bce17":"markdown","d32dd920":"markdown","3b184a36":"markdown","c2d9cf72":"markdown","25097db5":"markdown","edf804d7":"markdown","67df454f":"markdown","b515a355":"markdown","7d5b515f":"markdown","4f7b358e":"markdown","a360c874":"markdown","b243464a":"markdown","a0f0e6c0":"markdown","58a4cb1c":"markdown","bf2621f5":"markdown","2159f693":"markdown","f3a77afe":"markdown","3e1eea75":"markdown","3f6376e6":"markdown","01ffae03":"markdown","95394748":"markdown","2f19bad8":"markdown","25e60538":"markdown","8077df59":"markdown","ff9dfeba":"markdown","dddde650":"markdown","706cd726":"markdown","99028ada":"markdown","345d3d3a":"markdown"},"source":{"8e04c0e3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport  matplotlib.pyplot  as  plt\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f8abd456":"### Read the time series data\n\nunivariate_series   =  pd.read_csv('\/kaggle\/input\/time-series-data\/daily-min-temperatures.csv', header = 0, index_col = 0, parse_dates = True, squeeze = True)\n\n### Print first five records\nunivariate_series.head()","a1ebe08b":"univariate_series.plot()\nplt.ylabel('Minimum Temp')\nplt.title('Minimum temperature in Southern Hemisphere \\n  from 1981 to 1990')\nplt.show()","9c7813dd":"# process the date time information \n\nfrom datetime import datetime\ndef parse(x):\n    return datetime.strptime(x,'%Y %m %d %H')\n\n# Load dataset\npollution_df = pd.read_csv(\"\/kaggle\/input\/time-series-data\/pollution.csv\",parse_dates = [['year', 'month', 'day', 'hour']],index_col=0, date_parser=parse)\npollution_df.drop('No', axis=1, inplace=True)\n# manually specify column names\n\npollution_df.columns = ['pollution', 'dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain']\npollution_df.index.name = 'date'\n\n# mark all NA values with 0\npollution_df['pollution'].fillna(0, inplace=True)\n\n# drop the first 24 hours\npollution_df = pollution_df[24:]\n\n# summarize first 5 rows\nprint(pollution_df.head(5))\n","5f41f6e8":"values = pollution_df.values\n\n# specify columns to plot\n\ngroups = [0, 1, 2, 3, 5, 6, 7]\ni = 1\n\n# plot each column\nplt.figure()\n\nfor group in groups:\n    plt.subplot(len(groups), 1, i)\n    plt.plot(values[:, group])\n    plt.title(pollution_df.columns[group], y=0.5, loc='right')\n    i += 1\n    \nplt.show()","99aacc1a":"airPax_df = pd.read_csv('\/kaggle\/input\/time-series-data\/AirPassengers.csv')\n#Parse strings to datetime type\nairPax_df['Month'] = pd.to_datetime(airPax_df['Month'],infer_datetime_format=True) #convert from string to datetime\nairPax_df_indexed = airPax_df.set_index(['Month'])\nairPax_df_indexed.head(5)","55ab2788":"plt.plot(airPax_df_indexed) \nplt.show()","3bd42e63":"### Save the TS object\nairPax_df_indexed.to_csv('ts1.csv', index = True, sep = ',')\n\n### Check the object retrieved\nseries1 = pd.read_csv('ts1.csv', header = 0)\n\n\n### Check\nprint(type(series1))\nprint(series1.head(2).T)","c8e7e2a4":"india_gdp_df = pd.read_csv(\"\/kaggle\/input\/time-series-data\/GDPIndia.csv\")\ndate_rng = pd.date_range(start='1\/1\/1960', end='31\/12\/2017', freq='A')\nindia_gdp_df['TimeIndex'] = pd.DataFrame(date_rng, columns=['Year'])\nindia_gdp_df.head(5).T","4d6b0e62":"plt.plot(india_gdp_df.TimeIndex, india_gdp_df.GDPpercapita)\nplt.legend(loc='best')\nplt.show()","760ce5e1":"### Load as a pickle object\n\nimport pickle\n\nwith open('GDPIndia.obj', 'wb') as fp:\n        pickle.dump(india_gdp_df, fp)","6ca2f0e6":"### Retrieve the pickle object\n\nwith open('GDPIndia.obj', 'rb') as fp:\n     india_gdp1_df = pickle.load(fp)\n        \nindia_gdp1_df.head(5).T","eb2498a8":"def parser(x):\n    return datetime.strptime('190'+x, '%Y-%m')\n \nseries = pd.read_csv('\/kaggle\/input\/time-series-data\/shampoo.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n\nseries.plot()\nplt.show()","d949971a":"### Read the time series data\n\nseries   =  pd.read_csv('\/kaggle\/input\/time-series-data\/daily-min-temperatures.csv', header = 0, index_col = 0, parse_dates = True, squeeze = True)\n\nseries.plot()\nplt.ylabel('Minimum Temp')\nplt.title('Minimum temperature in Southern Hemisphere \\n From 1981 to 1990')\nplt.show()","b83652c9":"months         = pd.DataFrame()\none_year       = series['1990'] \ngroups         = one_year.groupby(pd.Grouper(freq='M')) \nmonths         = pd.concat([pd.DataFrame(x[1].values) for x in groups], axis=1) \nmonths         = pd.DataFrame(months) \nmonths.columns = range(1,13) \nmonths.boxplot() \nplt.show()","49e3dee6":"groups = series.groupby(pd.Grouper(freq='A')) \nyears  = pd.DataFrame() \nfor name, group in groups: \n    years[name.year] = group.values \nyears.boxplot() \nplt.show()","3de68301":"tractor_df = pd.read_csv(\"\/kaggle\/input\/time-series-data\/TractorSales.csv\")\ntractor_df.head(5)","5e7767f3":"dates = pd.date_range(start='2003-01-01', freq='MS', periods=len(tractor_df))","8ba2f09e":"import calendar\ntractor_df['Month'] = dates.month\ntractor_df['Month'] = tractor_df['Month'].apply(lambda x: calendar.month_abbr[x])\ntractor_df['Year'] = dates.year","1e0a58a8":"#Tractor.drop(['Month-Year'], axis=1, inplace=True)\ntractor_df.rename(columns={'Number of Tractor Sold':'Tractor-Sales'}, inplace=True)\ntractor_df = tractor_df[['Month', 'Year', 'Tractor-Sales']]","953c02cd":"tractor_df.set_index(dates, inplace=True)","08d5f49c":"tractor_df = tractor_df[['Tractor-Sales']]\ntractor_df.head(5)","c769026c":"tractor_df.plot()\nplt.ylabel('Tractor Sales')\nplt.title(\"Tractor Sales from 2003 to 2014\")\nplt.show()","02827f10":"months         = pd.DataFrame()\none_year       = tractor_df['2011'] \ngroups         = one_year.groupby(pd.Grouper(freq='M')) \nmonths         = pd.concat([pd.DataFrame(x[1].values) for x in groups], axis=1) \nmonths         = pd.DataFrame(months) \nmonths.columns = range(1,13) \nmonths.boxplot() \nplt.show()","f5d157d3":"tractor_df['2003']","caee7a52":"turnover_df= pd.read_csv('\/kaggle\/input\/time-series-data\/RetailTurnover.csv')\ndate_rng = pd.date_range(start='1\/7\/1982', end='31\/3\/1992', freq='Q')\nturnover_df['TimeIndex'] = pd.DataFrame(date_rng, columns=['Quarter'])\nturnover_df.head()\nplt.plot(turnover_df.TimeIndex, turnover_df.Turnover)\nplt.legend(loc='best')\nplt.show()","a0a4f76e":"from statsmodels.tsa.seasonal import seasonal_decompose\nimport  statsmodels.api as sm\ndecompTurnover_df = sm.tsa.seasonal_decompose(turnover_df.Turnover, model=\"additive\", freq=4)\ndecompTurnover_df.plot()\nplt.show()","4feb5e29":"trend = decompTurnover_df.trend\nseasonal = decompTurnover_df.seasonal\nresidual = decompTurnover_df.resid","87f90013":"print(trend.head(12))\nprint(seasonal.head(12))\nprint(residual.head(12))","319a1dfc":"\nairPax_df = pd.read_csv('\/kaggle\/input\/time-series-data\/AirPax.csv')\nprint(airPax_df.head())","38b7fea3":"date_rng = pd.date_range(start='1\/1\/1949', end='31\/12\/1960', freq='M')\nprint(date_rng)","05f7eef6":"airPax_df['TimeIndex'] = pd.DataFrame(date_rng, columns=['Month'])\nprint(airPax_df.head())","f4c298dc":"decompAirPax = sm.tsa.seasonal_decompose(airPax_df.Passenger, model=\"multiplicative\", freq=12)\ndecompAirPax.plot()\nplt.show()","84b9e91b":"seasonal = decompAirPax.seasonal\nseasonal.head(4)","0cf5d0b5":"quarterly_turnover = pd.pivot_table(turnover_df, values = \"Turnover\", columns = \"Quarter\", index = \"Year\")\nquarterly_turnover","2325dc77":"quarterly_turnover.plot()\nplt.show()","65fb4dab":"quarterly_turnover.boxplot()\nplt.show()","8363ba61":"petrol_df = pd.read_csv('\/kaggle\/input\/time-series-data\/Petrol.csv')\npetrol_df.head()\ndate_rng = pd.date_range(start='1\/1\/2001', end='30\/9\/2013', freq='Q')\n\n#date_rng\npetrol_df['TimeIndex'] = pd.DataFrame(date_rng, columns=['Quarter'])\nprint(petrol_df.head())\n\nplt.plot(petrol_df.TimeIndex, petrol_df.Consumption)\nplt.legend(loc='best')\nplt.show()","9290efd1":"airTemp_df =  pd.read_csv('\/kaggle\/input\/time-series-data\/AirTemp.csv')\ndate_rng =  pd.date_range(start='1\/1\/1920', end='31\/12\/1939', freq='M')\nairTemp_df['TimeIndex'] = pd.DataFrame(date_rng, columns=['Month'])\nairTemp_df.head()","f2baf241":"plt.plot(airTemp_df.TimeIndex, airTemp_df.AvgTemp)\nplt.legend(loc='best')\nplt.show()","c0292871":"temp_avg = airTemp_df.copy()\ntemp_avg['avg_forecast'] = airTemp_df['AvgTemp'].mean()\n\nplt.figure(figsize=(12,8))\nplt.plot(airTemp_df['AvgTemp'], label='Data')\nplt.plot(temp_avg['avg_forecast'], label='Average Forecast')\nplt.legend(loc='best')\nplt.show()","1f8566fb":"mvg_avg = airTemp_df.copy()\nmvg_avg['moving_avg_forecast'] = airTemp_df['AvgTemp'].rolling(12).mean()\nplt.plot(airTemp_df['AvgTemp'], label='Average Temperature')\nplt.plot(mvg_avg['moving_avg_forecast'], label='Moving Average Forecast')\nplt.legend(loc='best')\nplt.show()","537f3c76":"USGDP_df    = pd.read_csv('\/kaggle\/input\/time-series-data\/GDPIndia.csv', header=0)\nprint(USGDP_df.head())\ndate_rng = pd.date_range(start='1\/1\/1929', end='31\/12\/1991', freq='A')\nprint(date_rng)\n\nUSGDP_df['TimeIndex'] = pd.DataFrame(date_rng, columns=['Year'])\nplt.plot(USGDP_df.TimeIndex, USGDP_df.GDPpercapita)\n\nplt.legend(loc='best')\nplt.show()","36377248":"mvg_avg_USGDP = USGDP_df.copy()\nmvg_avg_USGDP['moving_avg_forecast'] = USGDP_df['GDPpercapita'].rolling(5).mean()\nplt.plot(USGDP_df['GDPpercapita'], label='US GDP')\nplt.plot(mvg_avg_USGDP['moving_avg_forecast'], label='US GDP MA(5)')\nplt.legend(loc='best')\nplt.show()","0885a4cc":"IndiaGDP_df = pd.read_csv('\/kaggle\/input\/time-series-data\/GDPIndia.csv', header=0)\n\ndate_rng = pd.date_range(start='1\/1\/1960', end='31\/12\/2017', freq='A')\nIndiaGDP_df['TimeIndex'] = pd.DataFrame(date_rng, columns=['Year'])\n\nprint(IndiaGDP_df.head())\n\nplt.plot(IndiaGDP_df.TimeIndex, IndiaGDP_df.GDPpercapita)\nplt.legend(loc='best')\nplt.show()","1c300bfe":"mvg_avg_IndiaGDP = IndiaGDP_df.copy()\nmvg_avg_IndiaGDP['moving_avg_forecast'] = IndiaGDP_df['GDPpercapita'].rolling(3).mean()\n\nplt.plot(IndiaGDP_df['GDPpercapita'], label='India GDP per Capita')\nplt.plot(mvg_avg_IndiaGDP['moving_avg_forecast'], label='India GDP\/Capita MA(3)')\nplt.legend(loc='best')\nplt.show()","47a77489":"import pandas as pd\nimport numpy  as np\ns = pd.Series([1,2,3,4,5,6])\ns.loc[4] = np.NaN\nprint(s)","ed1f2ea1":"def handle_missing_values():\n    print()\n    print(format('How to deal with missing values in a Timeseries in Python',\n                 '*^82'))\n    \n    # Create date\n    time_index = pd.date_range('28\/03\/2017', periods=5, freq='M')\n\n    # Create data frame, set index\n    df = pd.DataFrame(index=time_index);\n    print(df)\n\n    # Create feature with a gap of missing values\n    df['Sales'] = [1.0,2.0,np.nan,np.nan,5.0];\n    print(); print(df)\n\n    # Interpolate missing values\n    df1= df.interpolate();\n    print(); print(df1)\n\n    # Forward-fill Missing Values\n    df2 = df.ffill();\n    print(); print(df2)\n\n    # Backfill Missing Values\n    df3 = df.bfill();\n    print(); print(df3)\n\n    # Interpolate Missing Values But Only Up One Value\n    df4 = df.interpolate(limit=1, limit_direction='forward');\n    print(); print(df4)\n\n    # Interpolate Missing Values But Only Up Two Values\n    df5 = df.interpolate(limit=2, limit_direction='forward');\n    print(); print(df5)","f9694f02":"handle_missing_values()","a7530e0c":"import pandas as pd\nwaterConsumption_df=pd.read_csv(\"\/kaggle\/input\/time-series-data\/WaterConsumption.csv\")\nwaterConsumption_df.head()","d2ebb83c":"# Converting the column to DateTime format\nwaterConsumption_df.Date = pd.to_datetime(waterConsumption_df.Date, format='%d-%m-%Y')\nwaterConsumption_df = waterConsumption_df.set_index('Date')\nwaterConsumption_df.head()","7ea104ab":"import numpy as np\n# For charting purposes, we will add a column that contains the missing values only.\n\nwaterConsumption_df = waterConsumption_df.assign(missing= np.nan)\nwaterConsumption_df.missing[waterConsumption_df.target.isna()] = waterConsumption_df.reference\nwaterConsumption_df.info()","acb6dc76":"waterConsumption_df.plot(style=['k--', 'bo-', 'r*'],figsize=(20, 10))","e573b66d":"# Filling using mean or median\n# Creating a column in the dataframe\n# instead of : df['NewCol']=0, we use\n# df = df.assign(NewCol=default_value)\n# to avoid pandas warning.\nwaterConsumption_df = waterConsumption_df.assign(FillMean=waterConsumption_df.target.fillna(waterConsumption_df.target.mean()))\nwaterConsumption_df = waterConsumption_df.assign(FillMedian=waterConsumption_df.target.fillna(waterConsumption_df.target.median()))","fcc18672":"# imputing using the rolling average\nwaterConsumption_df = waterConsumption_df.assign(RollingMean=waterConsumption_df.target.fillna(waterConsumption_df.target.rolling(24,min_periods=1,).mean()))\n# imputing using the rolling median\nwaterConsumption_df = waterConsumption_df.assign(RollingMedian=waterConsumption_df.target.fillna(waterConsumption_df.target.rolling(24,min_periods=1,).median()))# imputing using the median","8d267277":"#Imputing using interpolation with different methods\n\nwaterConsumption_df = waterConsumption_df.assign(InterpolateLinear=waterConsumption_df.target.interpolate(method='linear'))\nwaterConsumption_df = waterConsumption_df.assign(InterpolateTime=waterConsumption_df.target.interpolate(method='time'))\nwaterConsumption_df = waterConsumption_df.assign(InterpolateQuadratic=waterConsumption_df.target.interpolate(method='quadratic'))\nwaterConsumption_df = waterConsumption_df.assign(InterpolateCubic=waterConsumption_df.target.interpolate(method='cubic'))\nwaterConsumption_df = waterConsumption_df.assign(InterpolateSLinear=waterConsumption_df.target.interpolate(method='slinear'))\nwaterConsumption_df = waterConsumption_df.assign(InterpolateAkima=waterConsumption_df.target.interpolate(method='akima'))\nwaterConsumption_df = waterConsumption_df.assign(InterpolatePoly5=waterConsumption_df.target.interpolate(method='polynomial', order=5)) \nwaterConsumption_df = waterConsumption_df.assign(InterpolatePoly7=waterConsumption_df.target.interpolate(method='polynomial', order=7))\nwaterConsumption_df = waterConsumption_df.assign(InterpolateSpline3=waterConsumption_df.target.interpolate(method='spline', order=3))\nwaterConsumption_df = waterConsumption_df.assign(InterpolateSpline4=waterConsumption_df.target.interpolate(method='spline', order=4))\nwaterConsumption_df = waterConsumption_df.assign(InterpolateSpline5=waterConsumption_df.target.interpolate(method='spline', order=5))","4a46e869":"#Scoring the results and see which is better\n\n# Import a scoring metric to compare methods\nfrom sklearn.metrics import r2_score\n\nresults = [(method, r2_score(waterConsumption_df.reference, waterConsumption_df[method])) for method in list(waterConsumption_df)[3:]]\nresults_df = pd.DataFrame(np.array(results), columns=['Method', 'R_squared'])\nresults_df.sort_values(by='R_squared', ascending=False)","dd1438af":"#data after imputation\n\nfinal_df= waterConsumption_df[['reference', 'target', 'missing', 'InterpolateTime' ]]\nfinal_df.plot(style=['b-.', 'ko', 'r.', 'rx-'], figsize=(20,10));\nplt.ylabel('Temperature');\nplt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05),\n          fancybox=True, shadow=True, ncol=5, prop={'size': 14} );","4312f068":"\ndef parser(x):\n       return datetime.strptime('190'+x, '%Y-%m')\n\nshampoo_df = pd.read_csv('\/kaggle\/input\/time-series-data\/shampoo.csv', header = 0, index_col = 0, parse_dates = True, \n                               squeeze = True, date_parser = parser)\n\nupsampled_ts = shampoo_df.resample('D').mean()\nprint(upsampled_ts .head(36))","31c3db89":"interpolated = upsampled_ts.interpolate(method = 'linear')\ninterpolated.plot()\nplt.show()","0951012c":"interpolated1 = upsampled_ts.interpolate(method = 'spline', order = 2)\ninterpolated1.plot()\nplt.show()","91f1d909":"print(interpolated1.head(12))","b1bf4311":"resample = shampoo_df.resample('Q')\nquarterly_mean_sales = resample.mean()\nprint(quarterly_mean_sales.head())\nquarterly_mean_sales.plot()\nplt.show()","3080d09b":"resample = shampoo_df.resample('A')\nyearly_mean_sales = resample.sum()\nprint(yearly_mean_sales.head() )\nyearly_mean_sales.plot()\nplt.show()","4448b3a5":"def parser(x): \n    return datetime.strptime('190'+x, '%Y-%m')\n\nshampoo_df=pd.read_csv(\"\/kaggle\/input\/time-series-data\/shampoo.csv\",header=0, index_col=0, parse_dates=True, squeeze=True, date_parser=parser)\n\nX = shampoo_df.values \ndiff = list() \nfor i in range(1, len(X)): \n     value = X[i] - X[i - 1] \n     diff.append(value) \nplt.plot(diff) \nplt.show()","13c4994e":"from sklearn.linear_model import LinearRegression \n\n# fit linear model \nX = [i for i in range(0, len(shampoo_df))] \nX = np.reshape(X, (len(X), 1))\ny = shampoo_df.values \nmodel = LinearRegression() \nmodel.fit(X, y) \n\n# calculate trend \ntrend = model.predict(X) \n\n# plot trend \nplt.plot(y) \nplt.plot(trend) \nplt.show() \n\n# detrend \ndetrended = [y[i]-trend[i] for i in range(0, len(shampoo_df))] \n\n# plot detrended \nplt.plot(detrended) \nplt.show()","fe63b69f":"# deseasonalize monthly data by differencing \n\nmin_temperature = pd.read_csv('\/kaggle\/input\/time-series-data\/daily-min-temperatures.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\nresample       = min_temperature.resample('M') \nmonthly_mean   = resample.mean() \n\nX = min_temperature.values \ndiff = list() \nmonths_in_year = 12 \n\nfor i in range(months_in_year, len(monthly_mean)): \n    value = monthly_mean[i] - monthly_mean[i - months_in_year] \n    diff.append(value) \n\nplt.plot(diff) \nplt.show()","23efdcca":"from sklearn.metrics import  mean_squared_error","5109f559":"def MAE(y,yhat):\n    diff = np.abs(np.array(y)-np.array(yhat))\n    try:\n        mae =  round(np.mean(np.fabs(diff)),3)\n    except:\n        print(\"Error while calculating\")\n        mae = np.nan\n    return mae","8dbe75ab":"def MAPE(y, yhat): \n    y, yhat = np.array(y), np.array(yhat)\n    try:\n        mape =  round(np.mean(np.abs((y - yhat) \/ y)) * 100,2)\n    except:\n        print(\"Observed values are empty\")\n        mape = np.nan\n    return mape","edbe0638":"female_birth_series =  pd.read_csv('\/kaggle\/input\/time-series-data\/daily-total-female-births.csv', header=0, index_col=0, parse_dates=True, squeeze=True) \n\n# tail rolling average transform\nrolling =  female_birth_series.rolling(window = 3) # arbitrarily chosen\n\nrolling_mean = rolling.mean()\nfemale_birth_series.plot()\n\nrolling_mean.plot(color = 'red')\nplt.show()\n\n# Zoomed plot original and transformed dataset\nfemale_birth_series[:100].plot()\nrolling_mean[:100].plot(color = 'red')\nplt.show()","be312cd3":"y_df = pd.DataFrame( {'Observed':female_birth_series.values, 'Predicted':rolling_mean})\ny_df .dropna(axis = 0, inplace = True)\nprint(y_df.tail())\n\nrmse = np.sqrt(mean_squared_error(y_df.Observed, y_df.Predicted))\nprint(\"\\n\\n Accuracy measures \")\nprint('RMSE: %.3f' % rmse)\nn = y_df.shape[0]\n\nmae = MAE(y_df.Observed, y_df.Predicted)\nprint('MAE: %d' % np.float(mae))\n\nmape = MAPE(y_df.Observed, y_df.Predicted)\nprint('MAPE: %.3f' % np.float(mape))","d3535c22":"import statsmodels.tsa.holtwinters     as      ets\nimport statsmodels.tools.eval_measures as      fa\nfrom   sklearn.metrics                 import  mean_squared_error\nfrom   statsmodels.tsa.holtwinters     import  SimpleExpSmoothing","d843a9e9":"def MAPE(y, yhat): \n    y, yhat = np.array(y), np.array(yhat)\n    try:\n        mape =  round(np.sum(np.abs(yhat - y)) \/ np.sum(y) * 100,2)\n    except:\n        print(\"Observed values are empty\")\n        mape = np.nan\n    return mape","44c0dce9":"petrol_df =  pd.read_csv('\/kaggle\/input\/time-series-data\/Petrol.csv')\ndate_rng  =  pd.date_range(start='1\/1\/2001', end='30\/9\/2013', freq='Q')\nprint(date_rng)","633da7f6":"petrol_df['TimeIndex'] = pd.DataFrame(date_rng, columns=['Quarter'])\nprint(petrol_df.head(3).T)\n\nplt.plot(petrol_df.TimeIndex, petrol_df.Consumption)\nplt.title('Original data before split')\nplt.show()","d464e525":"#Creating train and test set \n\ntrain = petrol_df[0:int(len(petrol_df)*0.7)] \ntest= petrol_df[int(len(petrol_df)*0.7):]\n\nprint(\"\\n Training data start at \\n\")\nprint (train[train.TimeIndex == train.TimeIndex.min()],['Year','Quarter'])\nprint(\"\\n Training data ends at \\n\")\nprint (train[train.TimeIndex == train.TimeIndex.max()],['Year','Quarter'])\n\nprint(\"\\n Test data start at \\n\")\nprint (test[test.TimeIndex == test.TimeIndex.min()],['Year','Quarter'])\n\nprint(\"\\n Test data ends at \\n\")\nprint (test[test.TimeIndex == test.TimeIndex.max()],['Year','Quarter'])\n\nplt.plot(train.TimeIndex, train.Consumption, label = 'Train')\nplt.plot(test.TimeIndex, test.Consumption,  label = 'Test')\nplt.legend(loc = 'best')\nplt.title('Original data after split')\nplt.show()","e7a48af8":"# create class\nmodel = SimpleExpSmoothing(np.asarray(train['Consumption']))","d1208280":"# fit model\n\nalpha_list = [0.1, 0.5, 0.99]\n\npred_SES = test.copy() # Have a copy of the test dataset\n\nfor alpha_value in alpha_list:\n\n    alpha_str =  \"SES\" + str(alpha_value)\n    mode_fit_i  =  model.fit(smoothing_level = alpha_value, optimized=False)\n    pred_SES[alpha_str]  =  mode_fit_i.forecast(len(test['Consumption']))\n    rmse =  np.sqrt(mean_squared_error(test['Consumption'], pred_SES[alpha_str]))\n    mape =  MAPE(test['Consumption'],pred_SES[alpha_str])\n###\n    print(\"For alpha = %1.2f,  RMSE is %3.4f MAPE is %3.2f\" %(alpha_value, rmse, mape))\n    plt.figure(figsize=(16,8))\n    plt.plot(train.TimeIndex, train['Consumption'], label ='Train')\n    plt.plot(test.TimeIndex, test['Consumption'], label  ='Test')\n    plt.plot(test.TimeIndex, pred_SES[alpha_str], label  = alpha_str)\n    plt.title('Simple Exponential Smoothing with alpha ' + str(alpha_value))\n    plt.legend(loc='best') \n    plt.show()","35425cd1":"pred_opt   =  SimpleExpSmoothing(train['Consumption']).fit(optimized = True)\nprint('')\nprint('== Simple Exponential Smoothing ')\nprint('')\n\nprint('')\nprint('Smoothing Level', np.round(pred_opt.params['smoothing_level'], 4))\nprint('Initial Level',   np.round(pred_opt.params['initial_level'], 4))\nprint('')\n\ny_pred_opt           = pred_opt.forecast(steps = 16)\ndf_pred_opt          = pd.DataFrame({'Y_hat':y_pred_opt,'Y':test['Consumption'].values})\n\nrmse_opt             =  np.sqrt(mean_squared_error(test['Consumption'], y_pred_opt))\nmape_opt             =  MAPE(test['Consumption'], y_pred_opt)\n\nalpha_value          = np.round(pred_opt.params['smoothing_level'], 4)\n\nprint(\"For alpha = %1.2f,  RMSE is %3.4f MAPE is %3.2f\" %(alpha_value, rmse_opt, mape_opt))\n\nplt.figure(figsize=(16,8))\nplt.plot(train.TimeIndex, train['Consumption'], label = 'Train')\nplt.plot(test.TimeIndex, test['Consumption'],  label = 'Test')\nplt.plot(test.TimeIndex, y_pred_opt,           label = 'SES_OPT')\nplt.title('Simple Exponential Smoothing with alpha ' + str(alpha_value))\nplt.legend(loc='best') \nplt.show()\n\nprint(df_pred_opt.head().T)","69efda9c":"from   statsmodels.tsa.holtwinters import  Holt\nmodel = Holt(np.asarray(train['Consumption']))\n\nmodel_fit = model.fit()\n\nprint('')\nprint('==Holt model Exponential Smoothing Parameters ==')\nprint('')\nalpha_value = np.round(model_fit.params['smoothing_level'], 4)\nprint('Smoothing Level', alpha_value )\nprint('Smoothing Slope', np.round(model_fit.params['smoothing_slope'], 4))\nprint('Initial Level',   np.round(model_fit.params['initial_level'], 4))\nprint('')","0974c2e7":"Pred_Holt = test.copy()\n\nPred_Holt['Opt'] = model_fit.forecast(len(test['Consumption']))","ee823628":"plt.figure(figsize=(16,8))\nplt.plot(train['Consumption'], label='Train')\nplt.plot(test['Consumption'], label='Test')\nplt.plot(Pred_Holt['Opt'], label='HoltOpt')\nplt.legend(loc='best')\nplt.show()","75ad5a06":"df_pred_opt =  pd.DataFrame({'Y_hat':Pred_Holt['Opt'] ,'Y':test['Consumption'].values})\nrmse_opt =  np.sqrt(mean_squared_error(df_pred_opt.Y, df_pred_opt.Y_hat))\nmape_opt =  MAPE(df_pred_opt.Y, df_pred_opt.Y_hat)\n\nprint(\"For alpha = %1.2f,  RMSE is %3.4f MAPE is %3.2f\" %(alpha_value, rmse_opt, mape_opt))","72623b99":"print(model_fit.params)","4fda626b":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\n\npred1 = ExponentialSmoothing(np.asarray(train['Consumption']), trend='additive', damped=False, seasonal='additive',\n                                  seasonal_periods = 12).fit() #[:'2017-01-01']\nprint('')\nprint('== Holt-Winters Additive ETS(A,A,A) Parameters ==')\nprint('')\nalpha_value = np.round(pred1.params['smoothing_level'], 4)\nprint('Smoothing Level: ', alpha_value)\nprint('Smoothing Slope: ', np.round(pred1.params['smoothing_slope'], 4))\nprint('Smoothing Seasonal: ', np.round(pred1.params['smoothing_seasonal'], 4))\nprint('Initial Level: ', np.round(pred1.params['initial_level'], 4))\nprint('Initial Slope: ', np.round(pred1.params['initial_slope'], 4))\nprint('Initial Seasons: ', np.round(pred1.params['initial_seasons'], 4))\nprint('')\n\n### Forecast for next 16 months\n\ny_pred1 =  pred1.forecast(steps = 16)\ndf_pred1 = pd.DataFrame({'Y_hat':y_pred1,'Y':test['Consumption']})\nprint(df_pred1)","35c30a0a":"### Plot\n\nfig2, ax = plt.subplots()\nax.plot(df_pred1.Y, label='Original')\nax.plot(df_pred1.Y_hat, label='Predicted')\n\nplt.legend(loc='upper left')\nplt.title('Holt-Winters Additive ETS(A,A,A) Method 1')\nplt.ylabel('Qty')\nplt.xlabel('Date')\nplt.show()","f02a4615":"rmse    =  np.sqrt(mean_squared_error(df_pred1.Y, df_pred1.Y_hat))\nmape    =  MAPE(df_pred1.Y, df_pred1.Y_hat)\n\nprint(\"For alpha = %1.2f,  RMSE is %3.4f MAPE is %3.2f\" %(alpha_value, rmse, mape))","3556531d":"print(pred1.params)","ddf6b0b9":"AirPax =  pd.read_csv('\/kaggle\/input\/time-series-data\/AirPax.csv')\ndate_rng = pd.date_range(start='1\/1\/1949', end='31\/12\/1960', freq='M')\nprint(date_rng)\n","74e8440e":"AirPax['TimeIndex'] = pd.DataFrame(date_rng, columns=['Month'])\nprint(AirPax.head())","0f3a4245":"#Creating train and test set \n\ntrain = AirPax[0:int(len(AirPax)*0.7)] \ntest= AirPax[int(len(AirPax)*0.7):]","1255f87b":"print(\"\\n Training data start at \\n\")\nprint (train[train.TimeIndex == train.TimeIndex.min()],['Year','Month'])\nprint(\"\\n Training data ends at \\n\")\nprint (train[train.TimeIndex == train.TimeIndex.max()],['Year','Month'])\n\nprint(\"\\n Test data start at \\n\")\nprint (test[test.TimeIndex == test.TimeIndex.min()],['Year','Month'])\n\nprint(\"\\n Test data ends at \\n\")\nprint (test[test.TimeIndex == test.TimeIndex.max()],['Year','Month'])\n\nplt.plot(train.TimeIndex, train.Passenger, label = 'Train')\nplt.plot(test.TimeIndex, test.Passenger,  label = 'Test')\nplt.legend(loc = 'best')\nplt.title('Original data after split')\nplt.show()","b7cbb608":"pred = ExponentialSmoothing(np.asarray(train['Passenger']),\n                                  seasonal_periods=12 ,seasonal='add').fit(optimized=True)\n\nprint(pred.params)\n\nprint('')\nprint('== Holt-Winters Additive ETS(A,A,A) Parameters ==')\nprint('')\nalpha_value = np.round(pred.params['smoothing_level'], 4)\nprint('Smoothing Level: ', alpha_value)\nprint('Smoothing Slope: ', np.round(pred.params['smoothing_slope'], 4))\nprint('Smoothing Seasonal: ', np.round(pred.params['smoothing_seasonal'], 4))\nprint('Initial Level: ', np.round(pred.params['initial_level'], 4))\nprint('Initial Slope: ', np.round(pred.params['initial_slope'], 4))\nprint('Initial Seasons: ', np.round(pred.params['initial_seasons'], 4))\nprint('')","50674657":"pred_HoltW = test.copy()\npred_HoltW['HoltW'] = model_fit.forecast(len(test['Passenger']))\nplt.figure(figsize=(16,8))\nplt.plot(train['Passenger'], label='Train')\nplt.plot(test['Passenger'], label='Test')\nplt.plot(pred_HoltW['HoltW'], label='HoltWinters')\nplt.title('Holt-Winters Additive ETS(A,A,A) Parameters:\\n  alpha = ' + \n          str(alpha_value) + '  Beta:' + \n          str(np.round(pred.params['smoothing_slope'], 4)) +\n          '  Gamma: ' + str(np.round(pred.params['smoothing_seasonal'], 4)))\nplt.legend(loc='best')\nplt.show()","14936956":"df_pred_opt =  pd.DataFrame({'Y_hat':pred_HoltW['HoltW'] ,'Y':test['Passenger'].values})\n\nrmse_opt    =  np.sqrt(mean_squared_error(df_pred_opt.Y, df_pred_opt.Y_hat))\nmape_opt    =  MAPE(df_pred_opt.Y, df_pred_opt.Y_hat)\n\nprint(\"For alpha = %1.2f,  RMSE is %3.4f MAPE is %3.2f\" %(alpha_value, rmse_opt, mape_opt))","84674b68":"pred = ExponentialSmoothing(np.asarray(train['Passenger']),\n                                  seasonal_periods=12 ,seasonal='multiplicative').fit(optimized=True)\n\nprint(pred.params)\n\nprint('')\nprint('== Holt-Winters Additive ETS(A,A,A) Parameters ==')\nprint('')\nalpha_value = np.round(pred.params['smoothing_level'], 4)\nprint('Smoothing Level: ', alpha_value)\nprint('Smoothing Slope: ', np.round(pred.params['smoothing_slope'], 4))\nprint('Smoothing Seasonal: ', np.round(pred.params['smoothing_seasonal'], 4))\nprint('Initial Level: ', np.round(pred.params['initial_level'], 4))\nprint('Initial Slope: ', np.round(pred.params['initial_slope'], 4))\nprint('Initial Seasons: ', np.round(pred.params['initial_seasons'], 4))\nprint('')","d7f115c1":"pred_HoltW = test.copy()\n\npred_HoltW['HoltWM'] = pred.forecast(len(test['Passenger']))\nplt.figure(figsize=(16,8))\nplt.plot(train['Passenger'], label='Train')\nplt.plot(test['Passenger'], label='Test')\nplt.plot(pred_HoltW['HoltWM'], label='HoltWinters')\nplt.title('Holt-Winters Multiplicative ETS(A,A,M) Parameters:\\n  alpha = ' + \n          str(alpha_value) + '  Beta:' + \n          str(np.round(pred.params['smoothing_slope'], 4)) +\n          '  Gamma: ' + str(np.round(pred.params['smoothing_seasonal'], 4)))\nplt.legend(loc='best')\nplt.show()","bec08c18":"df_pred_opt =  pd.DataFrame({'Y_hat':pred_HoltW['HoltWM'] ,'Y':test['Passenger'].values})\n\nrmse_opt    =  np.sqrt(mean_squared_error(df_pred_opt.Y, df_pred_opt.Y_hat))\nmape_opt    =  MAPE(df_pred_opt.Y, df_pred_opt.Y_hat)\n\nprint(\"For alpha = %1.2f,  RMSE is %3.4f MAPE is %3.2f\" %(alpha_value, rmse_opt, mape_opt))","acc3129e":"#Importing data\ndef parser(x):\n    return pd.datetime.strptime('190'+x, '%Y-%m')\n \nshampoo_df = pd.read_csv('\/kaggle\/input\/time-series-data\/shampoo.csv', header=0, parse_dates=True, squeeze=True, date_parser=parser)\n\nprint(shampoo_df.head())\nshampoo_df.plot()\n","beea76d4":"# Creating train and test set\n\ntrain    =   shampoo_df[0:int(len(shampoo_df)*0.7)] \ntest     =   shampoo_df[int(len(shampoo_df)*0.7):]","159cc6f1":"### Plot data\n\ntrain['Sales'].plot(figsize=(15,8), title= 'Monthly Sales', fontsize=14)\ntest['Sales'].plot(figsize=(15,8), title= 'Monthly Sales', fontsize=14)","a1e57e14":"shampoo_df.head()","79045b3b":"shampoo_df1         =   shampoo_df.copy() # Make a copy\n\ntime        = [i+1 for i in range(len(shampoo_df))]\nshampoo_df1['time'] = time\nmonthDf     = shampoo_df1[['Month']]\n\nshampoo_df1.drop('Month', axis=1, inplace=True)\nshampoo_df1.head(2)","493a1738":"#Creating train and test set \ntrain=shampoo_df1[0:int(len(shampoo_df1)*0.7)] \ntest=shampoo_df1[int(len(shampoo_df1)*0.7):]","ef1773ee":"x_train = train.drop('Sales', axis=1)\nx_test  = test.drop('Sales', axis=1)\ny_train = train[['Sales']]\ny_test  = test[['Sales']]","219ece12":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(x_train, y_train)\n","0a1d528f":"predictions         = model.predict(x_test)\ny_test['RegOnTime'] = predictions\n\nplt.figure(figsize=(16,8))\nplt.plot( train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_test['RegOnTime'], label='Regression On Time')\nplt.legend(loc='best')","56b57d0a":"from math import sqrt\nrmse = sqrt(mean_squared_error(test.Sales, y_test.RegOnTime))\nrmse = round(rmse, 3)\nmape = MAPE(test.Sales, y_test.RegOnTime)\nprint(\"For RegressionOnTime,  RMSE is %3.3f MAPE is %3.2f\" %(rmse, mape))","7e5bc446":"resultsDf = pd.DataFrame({'Method':['RegressionOnTime'], 'rmse': [rmse], 'mape' : [mape]})\nresultsDf","f2af9e82":"time = [i+1 for i in range(len(shampoo_df))]\nshampoo_df1 = shampoo_df.copy()\nshampoo_df1['time'] = time\nprint(shampoo_df1.head())\nprint(shampoo_df1.shape[0])\nmonthSeasonality = ['m1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7', 'm8', 'm9', 'm10', 'm11', 'm12']","083ef93f":"shampoo_df1['monthSeasonality'] = monthSeasonality * 3\nshampoo_df1.head()","474a8c66":"monthDf = shampoo_df1[['Month']]\nshampoo_df1.drop('Month', axis=1, inplace=True)","16ba02e8":"shampoo_df1Complete = pd.get_dummies(shampoo_df1, drop_first=True)\nshampoo_df1Complete.head(2).T","e2ae83db":"#Creating train and test set \ntrain=shampoo_df1Complete[0:int(len(shampoo_df1Complete)*0.7)] \ntest=shampoo_df1Complete[int(len(shampoo_df1Complete)*0.7):]","894c0651":"x_train  = train.drop('Sales', axis=1)\nx_test   = test.drop('Sales', axis=1)\ny_train  = train[['Sales']]\ny_test   = test[['Sales']]","fb5618a8":"model = LinearRegression()\nmodel.fit(x_train, y_train)\npredictions = model.predict(x_test)\ny_test['RegOnTimeSeasonal'] = predictions","c2fc4720":"plt.figure(figsize=(16,8))\nplt.plot( train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_test['RegOnTimeSeasonal'], label='Regression On Time With Seasonal Components')\nplt.legend(loc='best')","09e662e7":"rmse = sqrt(mean_squared_error(test.Sales, y_test.RegOnTimeSeasonal))\nrmse = round(rmse, 3)\nmape = MAPE(test.Sales, y_test.RegOnTimeSeasonal)\nprint(\"For RegOnTimeSeasonal,  RMSE is %3.3f MAPE is %3.2f\" %(rmse, mape))","f6be6118":"tempResultsDf = pd.DataFrame({'Method':['RegressionOnTimeSeasonal'], 'rmse': [rmse], 'mape' : [mape]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf","4d083958":"dd= np.asarray(train.Sales)","624c96e6":"y_hat = test.copy()","30f80c41":"y_hat['naive'] = dd[len(dd)-1]","94560ff3":"plt.figure(figsize=(12,8))\nplt.plot(train.index, train['Sales'], label='Train')\nplt.plot(test.index,test['Sales'], label='Test')\nplt.plot(y_hat.index,y_hat['naive'], label='Naive Forecast')\nplt.legend(loc='best')\nplt.title(\"Naive Forecast\")","f494bd06":"rmse = sqrt(mean_squared_error(test.Sales, y_hat.naive))\nrmse = round(rmse, 3)\nmape = MAPE(test.Sales, y_hat.naive)\nprint(\"For Naive model,  RMSE is %3.3f MAPE is %3.2f\" %(rmse, mape))","4cf5ad40":"tempResultsDf = pd.DataFrame({'Method':['Naive model'], 'rmse': [rmse], 'mape' : [mape]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf","ed8be308":"y_hat_avg = test.copy()","2ffbf565":"y_hat_avg['avg_forecast'] = train['Sales'].mean()","334fe58e":"plt.figure(figsize=(12,8))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_avg['avg_forecast'], label='Average Forecast')\nplt.legend(loc='best')","6e68e62a":"rmse = sqrt(mean_squared_error(test.Sales, y_hat_avg.avg_forecast))\nrmse = round(rmse, 3)\nmape = MAPE(test.Sales, y_hat_avg.avg_forecast)\nprint(\"For Simple Average model,  RMSE is %3.3f MAPE is %3.2f\" %(rmse, mape))","094c7cca":"tempResultsDf = pd.DataFrame({'Method':['Simple Average'], 'rmse': [rmse], 'mape' : [mape]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf","62ab14d9":"shampoo_df1 = shampoo_df.copy()","8b041a02":"shampoo_df1['moving_avg_forecast_4']  = shampoo_df['Sales'].rolling(4).mean()\nshampoo_df1['moving_avg_forecast_6']  = shampoo_df['Sales'].rolling(6).mean()\nshampoo_df1['moving_avg_forecast_8']  = shampoo_df['Sales'].rolling(8).mean()\nshampoo_df1['moving_avg_forecast_12'] = shampoo_df['Sales'].rolling(12).mean()","d59fcf32":"cols = ['moving_avg_forecast_4','moving_avg_forecast_6','moving_avg_forecast_8','moving_avg_forecast_12']\n\n#Creating train and test set \ntrain=shampoo_df1[0:int(len(shampoo_df1)*0.7)] \ntest=shampoo_df1[int(len(shampoo_df1)*0.7):]\n\ny_hat_avg = test.copy()\n\nfor col_name in cols:\n    \n    plt.figure(figsize=(16,8))\n    plt.plot(train['Sales'], label='Train')\n    plt.plot(test['Sales'], label='Test')\n    plt.plot(y_hat_avg[col_name], label = col_name)\n    plt.legend(loc = 'best')\n\n    rmse = sqrt(mean_squared_error(test.Sales, y_hat_avg[col_name]))\n    rmse = round(rmse, 3)\n    mape = MAPE(test.Sales, y_hat_avg[col_name])\n    print(\"For Simple Average model, %s  RMSE is %3.3f MAPE is %3.2f\" %(col_name, rmse, mape))\n\n    tempResultsDf = pd.DataFrame({'Method':[col_name], 'rmse': [rmse], 'mape' : [mape]})\n    resultsDf = pd.concat([resultsDf, tempResultsDf])","41485188":"print(resultsDf)","c8a77e04":"from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt","4d3a349a":"# create class\nmodel = SimpleExpSmoothing(train['Sales'])\nmodel_fit = model.fit(optimized = True)\nprint('')\nprint('== Simple Exponential Smoothing ')\nprint('')\n\nprint('')\nprint('Smoothing Level', np.round(model_fit.params['smoothing_level'], 4))\nprint('Initial Level',   np.round(model_fit.params['initial_level'], 4))\nprint('')","4932fd69":"y_hat_avg['SES']= model_fit.forecast(len(test['Sales']))\n\nalpha_value = np.round(model_fit.params['smoothing_level'], 4)\n\n\nplt.figure(figsize=(16,8))\nplt.plot(train.index, train['Sales'], label = 'Train')\nplt.plot(test.index, test['Sales'],   label = 'Test')\nplt.plot(test.index, y_hat_avg.SES,   label = 'SES_OPT')\nplt.title('Simple Exponential Smoothing with alpha ' + str(alpha_value))\nplt.legend(loc='best') \nplt.show()","2d1faa02":"rmse_opt=  np.sqrt(mean_squared_error(test['Sales'], y_hat_avg.SES))\nmape_opt=  MAPE(test['Sales'], y_hat_avg.SES)\n\nprint(\"For alpha = %1.2f,  RMSE is %3.4f MAPE is %3.2f\" %(alpha_value, rmse_opt, mape_opt))","b1efef53":"tempResultsDf = pd.DataFrame({'Method': 'SES', 'rmse': [rmse_opt], 'mape' : [mape_opt]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])","ca710ef1":"print(resultsDf)","e7c791a9":"import statsmodels.api as sm\ny_hat_avg = test.copy()\nmodel_fit = Holt(np.asarray(train['Sales'])).fit()\ny_hat_avg['Holt_linear'] = model_fit.forecast(len(test))","8d248c41":"print('')\nprint('==Holt model Exponential Smoothing Parameters ==')\nprint('')\nalpha_value = np.round(model_fit.params['smoothing_level'], 4)\nbeta_value  = np.round(model_fit.params['smoothing_slope'], 4)\n\nprint('Smoothing Level', alpha_value )\nprint('Smoothing Slope', beta_value)\nprint('Initial Level',   np.round(model_fit.params['initial_level'], 4))\nprint('')","40868121":"plt.figure(figsize=(16,8))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_avg['Holt_linear'], label='Holt_linear')\nplt.legend(loc='best')\nplt.show()","226e94ff":"rmse_opt             =  np.sqrt(mean_squared_error(test['Sales'], y_hat_avg['Holt_linear']))\nmape_opt             =  MAPE(test['Sales'], y_hat_avg['Holt_linear'])\n\nprint(\"For alpha = %1.2f,  RMSE is %3.4f MAPE is %3.2f\" %(alpha_value, rmse_opt, mape_opt))","1ac9b180":"tempResultsDf = pd.DataFrame({'Method': 'Holt_linear', 'rmse': [rmse_opt], 'mape' : [mape_opt]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nprint(resultsDf)","5186e1e9":"y_hat_avg = test.copy()\nmodel_fit = ExponentialSmoothing(np.asarray(train['Sales']) ,seasonal_periods = 12 ,trend='add', seasonal='add').fit()\ny_hat_avg['Holt_Winter'] = model_fit.forecast(len(test))\n","787ee3fa":"print('')\nprint('== Holt-Winters Additive ETS(A,A,A) Parameters ==')\nprint('')\nalpha_value = np.round(pred.params['smoothing_level'], 4)\nbeta_value  = np.round(model_fit.params['smoothing_slope'], 4)\ngamma_value = np.round(model_fit.params['smoothing_seasonal'], 4) \n\nprint('Smoothing Level: ', alpha_value)\nprint('Smoothing Slope: ', beta_value)\nprint('Smoothing Seasonal: ', gamma_value)\nprint('Initial Level: ', np.round(model_fit.params['initial_level'], 4))\nprint('Initial Slope: ', np.round(model_fit.params['initial_slope'], 4))\nprint('Initial Seasons: ', np.round(model_fit.params['initial_seasons'], 4))\nprint('')","2e25d4c9":"plt.figure(figsize=(16,8))\nplt.plot( train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')\nplt.title('Holt-Winters Multiplicative ETS(A,A,M) Parameters:\\n  alpha = ' + \n          str(alpha_value) + '  Beta:' + \n          str(beta_value) +\n          '  Gamma: ' + str(gamma_value))\nplt.legend(loc='best')","4629234a":"rmse_opt=  np.sqrt(mean_squared_error(test['Sales'], y_hat_avg['Holt_Winter']))\nmape_opt=  MAPE(test['Sales'], y_hat_avg['Holt_Winter'])\n\nprint(\"For alpha = %1.2f, beta = %1.2f, gamma = %1.2f, RMSE is %3.4f MAPE is %3.2f\" %(alpha_value, beta_value, gamma_value, rmse_opt, mape_opt))","ee70806e":"tempResultsDf = pd.DataFrame({'Method': 'Holt_Winter', 'rmse': [rmse_opt], 'mape' : [mape_opt]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nprint(resultsDf)","f67086a0":"y_hat_avg = test.copy()\nmodel_fit = ExponentialSmoothing(np.asarray(train['Sales']) ,seasonal_periods = 12 ,trend='add', seasonal='Multiplicative').fit()\ny_hat_avg['Holt_Winter_M'] = model_fit.forecast(len(test))","f2c919be":"print('')\nprint('== Holt-Winters Additive ETS(A,A,M) Parameters ==')\nprint('')\nalpha_value = np.round(pred.params['smoothing_level'], 4)\nbeta_value  = np.round(model_fit.params['smoothing_slope'], 4)\ngamma_value = np.round(model_fit.params['smoothing_seasonal'], 4) \n\nprint('Smoothing Level: ', alpha_value)\nprint('Smoothing Slope: ', beta_value)\nprint('Smoothing Seasonal: ', gamma_value)\nprint('Initial Level: ', np.round(model_fit.params['initial_level'], 4))\nprint('Initial Slope: ', np.round(model_fit.params['initial_slope'], 4))\nprint('Initial Seasons: ', np.round(model_fit.params['initial_seasons'], 4))\nprint('')","1d789704":"plt.figure(figsize=(16,8))\nplt.plot( train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_avg['Holt_Winter_M'], label='Holt_Winter_M')\nplt.title('Holt-Winters Multiplicative  Parameters:\\n  alpha = ' + \n          str(alpha_value) + '  Beta:' + \n          str(beta_value) +\n          '  Gamma: ' + str(gamma_value))\nplt.legend(loc='best')","de66fe84":"rmse_opt=  np.sqrt(mean_squared_error(test['Sales'], y_hat_avg['Holt_Winter_M']))\nmape_opt=  MAPE(test['Sales'], y_hat_avg['Holt_Winter_M'])\n\nprint(\"For alpha = %1.2f, beta = %1.2f, gamma = %1.2f, RMSE is %3.4f MAPE is %3.2f\" %(alpha_value, beta_value, gamma_value, rmse_opt, mape_opt))","bc06ba8c":"tempResultsDf = pd.DataFrame({'Method': 'Holt_Winter M', 'rmse': [rmse_opt], 'mape' : [mape_opt]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nprint(resultsDf)","9bf4ee18":"from   statsmodels.tsa.stattools  import  adfuller\ndata= pd.read_csv('\/kaggle\/input\/time-series-data\/TractorSales.csv', header=0, parse_dates=[0], squeeze=True)\n\ndates= pd.date_range(start='2003-01-01', freq='MS', periods=len(data))\n\ndata['Month']= dates.month\ndata['Month']= data['Month'].apply(lambda x: calendar.month_abbr[x])\ndata['Year']= dates.year\n\ndata.drop(['Month-Year'], axis=1, inplace=True)\ndata.rename(columns={'Number of Tractor Sold':'Tractor-Sales'}, inplace=True)\n\ndata= data[['Month', 'Year', 'Tractor-Sales']]\ndata.set_index(dates, inplace=True)\n\nsales_ts = data['Tractor-Sales']\n\nresult = adfuller(sales_ts) \n\nprint('ADF Statistic: %f' % result[0]) \nprint('p-value: %f' % result[1]) ","1024363a":"sales_ts_diff   = sales_ts - sales_ts.shift(periods=1)\nsales_ts_diff.dropna(inplace=True)\n\nresult = adfuller(sales_ts_diff) \n\npval              = result[1]\nprint('ADF Statistic: %f' % result[0]) \nprint('p-value: %f' % result[1]) \n\nif pval < 0.05:\n    print('Data is stationary')\nelse:\n    print('Data after differencing is not stationary; so try log diff')\n    sales_ts_log      = np.log10(sales_ts)\n    sales_ts_log.dropna(inplace=True)\n    sales_ts_log_diff = sales_ts_log.diff(periods=1)\n    sales_ts_log_diff.dropna(inplace=True)\n    result            = adfuller(sales_ts_log_diff) \n\n    pval              = result[1]\n    print('ADF Statistic: %f' % result[0]) \n    print('p-value: %f' % result[1]) \n    if pval < 0.05:\n        print('Data after log differencing is stationary')\n    else:\n        print('Data after log differencing is not stationary; try second order differencing')\n        sales_ts_log_diff2 = sales_ts_log.diff(periods = 2)\n        sales_ts_log_diff2.dropna(inplace=True)\n        result         =   adfuller(sales_ts_log_diff2) \n        pval              = result[1]\n        print('ADF Statistic: %f' % result[0]) \n        print('p-value: %f' % result[1]) \n        if pval < 0.05:\n            print('Data after log differencing 2nd order is stationary')\n        else:\n            print('Data after log differencing 2nd order is not stationary')\n        ","3f92e707":"#ACF and PACF plots:\nfrom   statsmodels.tsa.stattools import acf, pacf\nimport matplotlib.pyplot as plt\n\n\nlag_acf    =   acf(sales_ts_log_diff2,   nlags=20)\nlag_pacf   =   pacf(sales_ts_log_diff2, nlags=20, method='ols')\n\n#Plot ACF: \n\nplt.figure(figsize = (15,5))\nplt.subplot(121) \nplt.stem(lag_acf)\nplt.axhline(y = 0, linestyle='--',color='black')\nplt.axhline(y = -1.96\/np.sqrt(len(sales_ts_log_diff2)),linestyle='--',color='gray')\nplt.axhline(y = 1.96\/np.sqrt(len(sales_ts_log_diff2)),linestyle='--',color='gray')\nplt.xticks(range(0,22,1))\nplt.xlabel('Lag')\nplt.ylabel('ACF')\nplt.title('Autocorrelation Function')\n#Plot PACF:\n\nplt.subplot(122)\nplt.stem(lag_pacf)\nplt.axhline(y = 0, linestyle = '--', color = 'black')\nplt.axhline(y =-1.96\/np.sqrt(len(sales_ts_log_diff2)), linestyle = '--', color = 'gray')\nplt.axhline(y = 1.96\/np.sqrt(len(sales_ts_log_diff2)),linestyle = '--', color = 'gray')\nplt.xlabel('Lag')\nplt.xticks(range(0,22,1))\nplt.ylabel('PACF')\nplt.title('Partial Autocorrelation Function')\nplt.tight_layout()\n\n\n\nplt.show()","20f373d8":"from random import  seed, random\nseed(1) \nrandom_walk = list() \nrandom_walk.append(-1 if random() < 0.5 else 1)\nfor i in range(1, 1000): \n    movement = -1 if random() < 0.5 else 1 \n    value    = random_walk[i-1] + movement \n    random_walk.append(value) \n    \npd.plotting.autocorrelation_plot(random_walk) \nplt.show()\n","0f9bc145":"### Check stationary property\n\nresult = adfuller(random_walk) \n\nprint('ADF Statistic: %f' % result[0]) \nprint('p-value: %f' % result[1]) \nprint('Critical Values:') \nfor key, value in result[4].items(): \n    print('\\t%s: %.3f' % (key, value))","e0b8dbf3":"# prepare dataset for predicting a random walk\n\ntraining_size     = int(len(random_walk) * 0.70) \ntraining, test    = random_walk[0 : training_size], random_walk[training_size:]  \n\npredictions       = list() \nhist              = training[-1] \n\nfor i in range(len(test)): \n    yhat = hist \n    predictions.append(yhat) \n    hist = test[i] \n    \nrmse = np.sqrt(mean_squared_error(test, predictions))  \nprint('\\n\\nPredicting a Random Walk \\n RMSE: %.3f' % rmse)","fcd35ee2":"# Generate\nseed(1234)\nrw_steps  = np.random.normal(loc = 0.001, scale = 0.01, size = 1000) + 1\n\n### Initialize first element to 1\nrw_steps[0] = 1\n\n### Simulate the stock price\n\nPrice = rw_steps * np.cumprod(rw_steps)\nPrice = Price * 100\n\n### Plot the simulated stock prices\nplt.plot(rw_steps)\nplt.title(\"Simulated random walk with drift\")\nplt.show()","24d6304e":"### Check stationary property\n\nresult = adfuller(Price) \n\nprint('ADF Statistic: %f' % result[0]) \nprint('p-value: %f' % result[1]) \nprint('Critical Values:') \n\nfor key, value in result[4].items(): \n    print('\\t%s: %.3f' % (key, value))","088b856b":"### Prediction\n\ntraining_size     = int(len(Price) * 0.70) \ntraining, test    = random_walk[0 : training_size], random_walk[training_size:]  \n\npredictions       = list() \nhist              = training[-1] \n\nfor i in range(len(test)): \n    yhat = hist \n    predictions.append(yhat) \n    hist = test[i] \n    \nrmse = np.sqrt(mean_squared_error(test, predictions))  \nprint('\\n\\nPredicting a Random Walk with drift \\nRMSE: %.3f' % rmse)","f67698ef":"from pandas import DataFrame\nfrom io import StringIO\nimport time, json\nfrom datetime import date\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom sklearn.metrics import mean_squared_error\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 15, 6","64b09668":"# Loading the data\nbank_df = pd.read_csv('\/kaggle\/input\/time-series-data\/BOE-XUDLERD.csv')\nbank_df","bbc9aa07":"#converting to time series data \nbank_df['Date'] = pd.to_datetime(bank_df['Date'])\nindexed_df = bank_df.set_index('Date')\nts = indexed_df['Value']\nts.head(5)","e474187c":"#Visualize the raw data\n\nplt.plot(ts)","39547398":"#Resample the data as it contains too much variations \n\nts_week = ts.resample('W').mean()\nplt.plot(ts_week)","14f03c3f":"def test_stationarity(timeseries):\n    #Determing rolling statistics\n    rolmean = timeseries.rolling(window=52,center=False).mean() \n    rolstd = timeseries.rolling(window=52,center=False).std()\n    #Plot rolling statistics:\n    orig = plt.plot(timeseries, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)\ntest_stationarity(ts_week)","35605a2d":"ts_week_log = np.log(ts_week)\nts_week_log_diff = ts_week_log - ts_week_log.shift()\nplt.plot(ts_week_log_diff)","6e36b034":"#Again confirming with the dickey-fuller test \n\nts_week_log_diff.dropna(inplace=True)\ntest_stationarity(ts_week_log_diff)","c265b3ef":"#ACF and PACF\nlag_acf = acf(ts_week_log_diff, nlags=10)\nlag_pacf = pacf(ts_week_log_diff, nlags=10, method='ols')","4e21744c":"#Plot ACF: \nplt.subplot(121) \nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-7.96\/np.sqrt(len(ts_week_log_diff)),linestyle='--',color='gray')\nplt.axhline(y=7.96\/np.sqrt(len(ts_week_log_diff)),linestyle='--',color='gray')\nplt.title('Autocorrelation Function')","e17b607b":"#Plot PACF:\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-7.96\/np.sqrt(len(ts_week_log_diff)),linestyle='--',color='gray')\nplt.axhline(y=7.96\/np.sqrt(len(ts_week_log_diff)),linestyle='--',color='gray')\nplt.title('Partial Autocorrelation Function')\nplt.tight_layout()","6987126c":"# Optimal values fot ARIMA(p,d,q) model are (2,1,1). Hence plot the ARIMA model using the value (2,1,1)\nmodel = ARIMA(ts_week_log, order=(2, 1, 1))  \nresults_ARIMA = model.fit(disp=-1)  \nplt.plot(ts_week_log_diff)\nplt.plot(results_ARIMA.fittedvalues, color='red')\nplt.title('RSS: %.4f'% sum((results_ARIMA.fittedvalues-ts_week_log_diff)**2))","874bff15":"# print the results of the ARIMA model and plot the residuals\n\nprint(results_ARIMA.summary())\n# plot residual errors\nresiduals = DataFrame(results_ARIMA.resid)\nresiduals.plot(kind='kde')\nprint(residuals.describe())","1d4f2461":"#Predictions \n\npredictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\nprint (predictions_ARIMA_diff.head())","1dc286c8":"predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\npredictions_ARIMA_log = pd.Series(ts_week_log.iloc[0], index=ts_week_log.index)\npredictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)\npredictions_ARIMA = np.exp(predictions_ARIMA_log)\nplt.plot(ts_week)\nplt.plot(predictions_ARIMA)\nplt.title('RMSE: %.4f'% np.sqrt(sum((predictions_ARIMA-ts_week)**2)\/len(ts_week)))","b87d78d5":"#Training and testing datsets\nsize = int(len(ts_week_log) - 15)\ntrain, test = ts_week_log[0:size], ts_week_log[size:len(ts_week_log)]\nhistory = [x for x in train]\npredictions = list()","f6dcb8ce":"#Training the model and forecasting \n\nsize = int(len(ts_week_log) - 15)\ntrain, test = ts_week_log[0:size], ts_week_log[size:len(ts_week_log)]\nhistory = [x for x in train]\npredictions = list()\nprint('Printing Predicted vs Expected Values...')\nprint('\\n')\nfor t in range(len(test)):\n    model = ARIMA(history, order=(2,1,1))\n    model_fit = model.fit(disp=0)\n    output = model_fit.forecast()\n    yhat = output[0]\n    predictions.append(float(yhat))\n    obs = test[t]\n    history.append(obs)\nprint('predicted=%f, expected=%f' % (np.exp(yhat), np.exp(obs)))","38c8f951":"#Validating the model \n\nerror = mean_squared_error(test, predictions)\nprint('\\n')\nprint('Printing Mean Squared Error of Predictions...')\nprint('Test MSE: %.6f' % error)\npredictions_series = pd.Series(predictions, index = test.index)","01df2369":"#Plotting forecasted vs Observed values \n\nfig, ax = plt.subplots()\nax.set(title='Spot Exchange Rate, Euro into USD', xlabel='Date', ylabel='Euro into USD')\nax.plot(ts_week[-60:], 'o', label='observed')\nax.plot(np.exp(predictions_series), 'g', label='rolling one-step out-of-sample forecast')\nlegend = ax.legend(loc='upper left')\nlegend.get_frame().set_facecolor('w')","b58a19ef":"import sys\nimport warnings\nimport itertools\nwarnings.filterwarnings(\"ignore\")\n\nimport statsmodels.api as sm\nimport statsmodels.tsa.api as smt\nimport statsmodels.formula.api as smf","0765012d":"tractor_sales_Series = pd.read_csv(\"\/kaggle\/input\/time-series-data\/TractorSales.csv\")\ntractor_sales_Series.head(5)","1b4bbfc1":"dates = pd.date_range(start='2003-01-01', freq='MS', periods=len(tractor_sales_Series))","55d30aee":"import calendar\ndata['Month'] = dates.month\ndata['Month'] = data['Month'].apply(lambda x: calendar.month_abbr[x])\ndata['Year'] = dates.year","2eeddeef":"#data.drop(['Month-Year'], axis=1, inplace=True)\ndata.rename(columns={'Number of Tractor Sold':'Tractor-Sales'}, inplace=True)\ndata = data[['Month', 'Year', 'Tractor-Sales']]","f363904c":"data.set_index(dates, inplace=True)","759e5d21":"data.head(5)","a944029d":"# extract out the time-series\nsales_ts = data['Tractor-Sales']","2de3c306":"plt.figure(figsize=(8, 4))\nplt.plot(sales_ts)\nplt.xlabel('Years')\nplt.ylabel('Tractor Sales')","92e0eacb":"fig, axes = plt.subplots(2, 2, sharey=False, sharex=False)\nfig.set_figwidth(14)\nfig.set_figheight(8)\naxes[0][0].plot(sales_ts.index, sales_ts, label='Original')\naxes[0][0].plot(sales_ts.index, sales_ts.rolling(window=4).mean(), label='4-Months Rolling Mean')\naxes[0][0].set_xlabel(\"Years\")\naxes[0][0].set_ylabel(\"Number of Tractor's Sold\")\naxes[0][0].set_title(\"4-Months Moving Average\")\naxes[0][0].legend(loc='best')\naxes[0][1].plot(sales_ts.index, sales_ts, label='Original')\naxes[0][1].plot(sales_ts.index, sales_ts.rolling(window=6).mean(), label='6-Months Rolling Mean')\naxes[0][1].set_xlabel(\"Years\")\naxes[0][1].set_ylabel(\"Number of Tractor's Sold\")\naxes[0][1].set_title(\"6-Months Moving Average\")\naxes[0][1].legend(loc='best')\naxes[1][0].plot(sales_ts.index, sales_ts, label='Original')\naxes[1][0].plot(sales_ts.index, sales_ts.rolling(window=8).mean(), label='8-Months Rolling Mean')\naxes[1][0].set_xlabel(\"Years\")\naxes[1][0].set_ylabel(\"Number of Tractor's Sold\")\naxes[1][0].set_title(\"8-Months Moving Average\")\naxes[1][0].legend(loc='best')\naxes[1][1].plot(sales_ts.index, sales_ts, label='Original')\naxes[1][1].plot(sales_ts.index, sales_ts.rolling(window=12).mean(), label='12-Months Rolling Mean')\naxes[1][1].set_xlabel(\"Years\")\naxes[1][1].set_ylabel(\"Number of Tractor's Sold\")\naxes[1][1].set_title(\"12-Months Moving Average\")\naxes[1][1].legend(loc='best')\nplt.tight_layout()\nplt.show()","df8f4132":"#Determing rolling statistics\n\nrolmean = sales_ts.rolling(window = 4).mean()\nrolstd = sales_ts.rolling(window = 4).std()","596f8d68":"#Plot rolling statistics:\norig = plt.plot(sales_ts, label='Original')\nmean = plt.plot(rolmean, label='Rolling Mean')\nstd = plt.plot(rolstd, label = 'Rolling Std')\nplt.legend(loc='best')\nplt.title('Rolling Mean & Standard Deviation')","94b7a139":"from statsmodels.tsa.stattools import adfuller\n\ndftest = adfuller(sales_ts)\ndftest\nprint('DF test statistic is %3.3f' %dftest[0])\nprint('DF test p-value is %1.4f' %dftest[1])","881068e8":"monthly_sales_data = pd.pivot_table(data, values = \"Tractor-Sales\", columns = \"Year\", index = \"Month\")\nmonthly_sales_data","cf0fa9be":"monthly_sales_data = monthly_sales_data.reindex(index = ['Jan','Feb','Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\nmonthly_sales_data","29439480":"monthly_sales_data.plot()","0bdfcb68":"yearly_sales_data = pd.pivot_table(data, values = \"Tractor-Sales\", columns = \"Month\", index = \"Year\")\nyearly_sales_data = yearly_sales_data[['Jan','Feb','Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']]\nyearly_sales_data","bf6d66c6":"yearly_sales_data.plot()","9e53c4a5":"yearly_sales_data.boxplot()","9320cfcc":"decomposition = sm.tsa.seasonal_decompose(sales_ts, model='multiplicative')","0f56892f":"fig = decomposition.plot()\nfig.set_figwidth(8)\nfig.set_figheight(6)\nfig.suptitle('Decomposition of multiplicative time series')\nplt.show()","7c4e6d1c":"plt.figure(figsize=(8, 4))\nplt.plot(sales_ts.diff(periods=1))\nplt.xlabel('Years')\nplt.ylabel('Tractor Sales')","88abc918":"plt.figure(figsize=(8, 4))\nplt.plot(np.log10(sales_ts))\nplt.xlabel('Years')\nplt.ylabel('Log (Tractor Sales)')","180247fb":"plt.figure(figsize=(10, 5))\nplt.plot(np.log10(sales_ts).diff(periods=1))\nplt.xlabel('Years')\nplt.ylabel('Differenced Log (Tractor Sales)')","e854104f":"sales_ts_log = np.log10(sales_ts)\nsales_ts_log.dropna(inplace=True)\n\nsales_ts_log_diff = sales_ts_log.diff(periods=1) # same as ts_log_diff = ts_log - ts_log.shift(periods=1)\nsales_ts_log_diff.dropna(inplace=True)","45914496":"fig, axes = plt.subplots(1, 2)\nfig.set_figwidth(12)\nfig.set_figheight(4)\nsmt.graphics.plot_acf(sales_ts_log, lags=30, ax=axes[0])\nsmt.graphics.plot_pacf(sales_ts_log, lags=30, ax=axes[1])\nplt.tight_layout()","c1d51b37":"fig, axes = plt.subplots(1, 2)\nfig.set_figwidth(12)\nfig.set_figheight(4)\nplt.xticks(range(0,30,1), rotation = 90)\nsmt.graphics.plot_acf(sales_ts_log_diff, lags=30, ax=axes[0])\nsmt.graphics.plot_pacf(sales_ts_log_diff, lags=30, ax=axes[1])\nplt.tight_layout()","ac068275":"# Define the p, d and q parameters to take any value between 0 and 2\np = d = q = range(0, 2)\n\n# Generate all different combinations of p, d and q triplets\npdq = list(itertools.product(p, d, q))\n\n# Generate all different combinations of seasonal p, q and q triplets\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]","9859c067":"pdq","a06a6c16":"seasonal_pdq","43853a94":"#Separate data into train and test\ndata['date'] = data.index\ntrain = data[data.index < '2013-01-01']\ntest = data[data.index >= '2013-01-01']\ntrain_sales_ts_log = np.log10(train['Tractor-Sales'])","2b7cbf59":"best_aic = np.inf\nbest_pdq = None\nbest_seasonal_pdq = None\ntemp_model = None","bbb23318":"for param in pdq:\n    for param_seasonal in seasonal_pdq:\n        \n        try:\n            temp_model = sm.tsa.statespace.SARIMAX(train_sales_ts_log,\n                                             order = param,\n                                             seasonal_order = param_seasonal,\n                                             enforce_stationarity=True)\n            results = temp_model.fit()\n\n            \n            if results.aic < best_aic:\n                best_aic = results.aic\n                best_pdq = param\n                best_seasonal_pdq = param_seasonal\n        except:\n            #print(\"Unexpected error:\", sys.exc_info()[0])\n            continue\nprint(\"Best SARIMAX{}x{}12 model - AIC:{}\".format(best_pdq, best_seasonal_pdq, best_aic))","fb46b759":"best_model = sm.tsa.statespace.SARIMAX(train_sales_ts_log,\n                                      order=(0, 1, 1),\n                                      seasonal_order=(1, 0, 1, 12),\n                                      enforce_stationarity=True)\nbest_results = best_model.fit()","213c7361":"print(best_results.summary().tables[0])\nprint(best_results.summary().tables[1])","50d88016":"pred_dynamic = best_results.get_prediction(start=pd.to_datetime('2012-01-01'), dynamic=True, full_results=True)","06d1395d":"pred_dynamic_ci = pred_dynamic.conf_int()","d7252b02":"pred99 = best_results.get_forecast(steps=24, alpha=0.1)","1f48f54a":"# Extract the predicted and true values of our time series\nsales_ts_forecasted = pred_dynamic.predicted_mean\ntestCopy = test.copy()\ntestCopy['sales_ts_forecasted'] = np.power(10, pred99.predicted_mean)","f08b4152":"testCopy","c4c1c3ec":"# Compute the root mean square error\nmse = ((testCopy['Tractor-Sales'] - testCopy['sales_ts_forecasted']) ** 2).mean()\nrmse = np.sqrt(mse)\nprint('The Root Mean Squared Error of our forecasts is {}'.format(round(rmse, 3)))","b16ce99f":"axis = train['Tractor-Sales'].plot(label='Train Sales', figsize=(10, 6))\ntestCopy['Tractor-Sales'].plot(ax=axis, label='Test Sales', alpha=0.7)\ntestCopy['sales_ts_forecasted'].plot(ax=axis, label='Forecasted Sales', alpha=0.7)\naxis.set_xlabel('Years')\naxis.set_ylabel('Tractor Sales')\nplt.legend(loc='best')\nplt.show()\nplt.close()","ef963c7d":"# Get forecast 36 steps (3 years) ahead in future\nn_steps = 36\npred_uc_99 = best_results.get_forecast(steps=36, alpha=0.01) # alpha=0.01 signifies 99% confidence interval\npred_uc_95 = best_results.get_forecast(steps=36, alpha=0.05) # alpha=0.05 95% CI\n\n# Get confidence intervals 95% & 99% of the forecasts\npred_ci_99 = pred_uc_99.conf_int()\npred_ci_95 = pred_uc_95.conf_int()","03743aea":"n_steps = 36\nidx = pd.date_range(data.index[-1], periods=n_steps, freq='MS')\nfc_95 = pd.DataFrame(np.column_stack([np.power(10, pred_uc_95.predicted_mean), np.power(10, pred_ci_95)]), \n                     index=idx, columns=['forecast', 'lower_ci_95', 'upper_ci_95'])\nfc_99 = pd.DataFrame(np.column_stack([np.power(10, pred_ci_99)]), \n                     index=idx, columns=['lower_ci_99', 'upper_ci_99'])\nfc_all = fc_95.combine_first(fc_99)\nfc_all = fc_all[['forecast', 'lower_ci_95', 'upper_ci_95', 'lower_ci_99', 'upper_ci_99']] # just reordering columns\nfc_all.head()","4ef043ba":"# plot the forecast along with the confidence band\n\naxis = sales_ts.plot(label='Observed', figsize=(8, 4))\nfc_all['forecast'].plot(ax=axis, label='Forecast', alpha=0.7)\naxis.fill_between(fc_all.index, fc_all['lower_ci_95'], fc_all['upper_ci_95'], color='k', alpha=.15)\naxis.set_xlabel('Years')\naxis.set_ylabel('Tractor Sales')\nplt.legend(loc='best')\nplt.show()","58c4add0":"best_results.plot_diagnostics(lags=30, figsize=(16,12))\nplt.show()","50eb665e":"### Forecast  Requirements\n\nA time series model must contain a key time column that contains unique values, input columns, and at least one predictable column.\n\nTime series data often requires cleaning, scaling, and even transformation\n\n**Frequency:** Data may be provided at a frequency that is too high to model or is unvenly spread through time requiring  resampling for use in models.\n\n**Outliers:** Data may contain corrupt or extreme outlier values that need to be identified and handled.\n\n**Frequency:**\n\n* Frequencies may be too granular or not granular enough to get insights.\n* The pandas library in Pyhton provides the capability to increase or decrease the sampling frequency of the time series data.\n\n**Resampling:**\n\n* Resampling may be required if the data is not available at the same frequency that you want to make predictions.\n* Resampling may be required to provide additional structure or insight into the learning problem for supervised learning models.","cd4765f1":"* 1) Consolidate the date-time information into a single date-time so that we can use it as an index in Pandas.\n* 2) Treat NA values. \n\nA quick check reveals NA values for pm2.5 for the first 24 hours. We will, therefore, need to remove the first row of data. There are also a few scattered \u201cNA\u201d values later in the dataset; we can mark them with 0 values for now.","e52d4fbd":"### The above plot shows a strong seasonality component.","74b2046d":"Look at the ACF and PACF plots of the log differenced series.\n* Observe our first significant value at lag 5 for ACF \n* Our significant value is at the lag 2 for the PACF\n* These values suggest us to use p = 2 and q = 5.\n\n**Here d = 2 since we could get stationary only when we did differencing twice on the log values.**","2ac430e9":"# 3.What is not a Time Series? <a class=\"anchor\" id=\"3\"><\/a>\n\n[Table of Contents](#0.1)\n\nData collected on multiple items at the **same point of time** is not a time series! For example Sales of different products at the same time. Also that kind of data where the **time periods are not same**. For example in a single time series both yearly and quarterly data cannot be mixed. ","799c261e":"### The above plot shows an increasing trend.\n","1510aa86":"You can also use the Champagne Series data, food sales data and plot the time series.Check whether the data shows trend or seasonality or both.","f10d8d89":"###  Average and moving average for Air Temp data","b1686991":"The rolling() function on the Series Pandas object will automatically group observations into a window.\n\nYou can specify the window size, and by default, a trailing window is created. Once the window is created, we can use the mean value, which forms our transformed dataset.\n","9a0312ea":"#### Plot the average forecast","fa287850":"Let us get the optimum value for $\\alpha$ by omitting the value and leave it for the model to decide.","aea3369d":"#### Mark missing values\n\n* NaN is the default missing value marker for reasons of computational speed and convenience. \n* We can easily detect this value with data of different types: floating point, integer, Boolean and general object. \n* However, the Python None will arise and we wish to also consider that missing.\n* To make detecting missing values easier across different array dtypes, pandas provides functions, isna() and notna(), which are also methods on Series and DataFrame objects.\n* For datetime64[ns] types, NaT represents missing values. Pandas objects provide histocompatibility between NaT and NaN.","f8848062":"# 6.Time Series Assumptions <a class=\"anchor\" id=\"6\"><\/a>\n\n[Table of Contents](#0.1)\n\nSome of the most common assumptions made for time series are based on the common sense. But always Keep in mind one thing \n\n> Very long range forecasts does not work well !!\n\n* Forecast is done by keeping in mind that the market and the other conditions are not going to change in the future.\n* There will be not any change in the market.\n* But the change is gradual and not a drastic change.\n* Situations like recession in 2008 US market will send the forecasts into a tizzy. \n* Events like demonetization would throw the forecasts into disarray\n\nBased on the data available , we should not try to forecast for more than a few periods ahead.","00f9064e":"**Another common interpolation**\n\n* Another common interpolation method is to use a polynomial or a spline to connect the values.\nThis creates more curves and look more natural on many datasets.\n* Using a spline interpolation requires you specify the order (count of terms in the polynomial); we use 2.","7b7dd2a3":"### 14.2.4Method 4: Simple Average <a class=\"anchor\" id=\"14.2.4\"><\/a>\n\n[Table of Contents](#0.1)","c05fff5e":"Moving average line is close to the original data line.","d18285f6":"# **14.ETS Models** <a class=\"anchor\" id=\"14\"><\/a>\n\n[Table of Contents](#0.1)\n\n\nThe ETS model is a time series univariate forecasting method; its use focuses on trend and seasonal components.It is a time series forecasting method for univariate data.\n\n\n","54af9825":"We observe that MAPE for Holt-Winters Additive ETS(A,A,A) Method is 128.21 and it is very much higher than (63.01) ETS(A, A, N) - Holt's Model.\n\nWe will try with  Air Passengers data.","3df28d6a":"## Random walk with drift\n\n[Table of Contents](#0.1)\n\n### Example 2\n\n**Simulate stock returns using a random walk with drift**","fa06f641":"### Accuracy measures","dbc501a7":"### Additive Decomposition\n\n* An additive model suggests that the components are added together.\n* An additive model is linear where changes over time are consistently made by the same amount.\n* A linear seasonality has the same frequency (width of the cycles) and amplitude (height of the cycles).","16e89b5b":"Consider the following example \n\n* Use the data in example 2 and use Double Exponential Smoothing method to forecast sales for the test data\n* Calculate the values of RMSE and MAPE.\n* Plot the forecasted values along with original values.","cab9002d":"It appears that we have a seasonal component each year showing swing from May to Aug.","902d0606":"### 14.1.1SES -  ETS(A, N, N) - Simple smoothing with additive errors <a class=\"anchor\" id=\"14.1.1\"><\/a>\n\n###### The simplest of the exponentially smoothing methods is naturally called simple exponential smoothing (SES). \n###### This method is suitable for forecasting data with no clear trend or seasonal pattern.\n\nIn Single ES, the forecast at time (t + 1) is given by Winters,1960\n\n* $F_{t+1} = \\alpha Y_t + (1-\\alpha) F_t$\n\nParameter $\\alpha$ is called the smoothing constant and its value lies between 0 and 1.\nSince the model uses only one smoothing constant, it is called Single Exponential Smoothing.","2e73ed32":"Dickey-Fuller Test - Let's run the Dicky Fuller Test on the timeseries and verify the null hypothesis that the TS is non-stationary.","74b92dcb":"# 13.Time Series Range, Accuracy and Various Requirements <a class=\"anchor\" id=\"13\"><\/a>\n\n[Table of Contents](#0.1)\n","2d0aa693":"**Limitation** \n\n* For the time interpolation to succeed, the dataframe must have the index in Date format with intervals of 1 day or more, (daily, monthly, \u2026) however, it will not work for time-based data, like hourly data and so.\n* if it is important to use a different index for the dataframe, the use the reset_index().set_index('Date'), do the interpolation, and then apply the reset_index().set_index('DesiredIndex').\n* If the data contains another dividing column, like the type of merchandise, and we are imputing sales, then the imputation should be for each merchandise separately.\n\n","62d90018":"# **8.Reading and Saving Time Series Objects in Python** <a class=\"anchor\" id=\"8\"><\/a>\n\n[Table of Contents](#0.1)\n\n### Example 1 \n\n#### Use US Airpassengers data set","6f82ddc3":"You can also use the above example with the following \n\n* Make the changes to model fit parameters as follows:\n\n* smoothing_level = 0.3\n* smoothing_slope = 0.1\n* optimized       = False\n\n* Build the model using the training data (Same split as in example 3)\n* Evaluate the model performance by calculating the values of RMSE and MAPE.\n* Plot the forecasted values along with original values.\n* Is there any improvement or degradation in predicting the values?","210fcb7d":"#### Plot the moving average forecast and average temperature","20241f1a":"### 14.2.3Method 3: Naive Approach: $\\hat{y}_{t+1} = y_t$ <a class=\"anchor\" id=\"14.2.3\"><\/a>\n\n[Table of Contents](#0.1)","423febc3":"\u0088*  The model is AR if the ACF trails o\ufb00 after a lag and has a hard cut-o\ufb00 in the PACF after a lag.    This lag is taken as the value for p.\n\n\u0088*  The model is MA if the PACF trails o\ufb00 after a lag and has a hard cut-o\ufb00 in the ACF after the   lag. This lag value is taken as the value for q.\n\n\u0088*  The model is a mix of AR and MA if both the ACF and PACF trail o\ufb00.\n\n* For an **ARIMA (p,d,q)** process, it becomes non-stationary to stationary after differencing it for **d** times.","f9c43e69":"#### Print the model parameters","7a0adf31":"#### Autocorrelation Function (ACF)\n\n**A plot of auto-correlation of different lags is called ACF.**\n\nThe plot summarizes the correlation of an observation with lag values. The x-axis shows the lag and the y-axis shows the correlation coe\ufb03cient between -1 and 1 for negative and positive correlation.\n\n#### Partial Autocorrelation Function (PACF)\n\n**A plot of partial auto-correlation for different values of lags is called PACF.**\n\nThe plot summarizes the correlations for an observation with lag values that is not accounted for by prior lagged observations.\n\nBoth plots are drawn as bar charts showing the 95% and 99% con\ufb01dence intervals as horizontal lines. Bars that cross these con\ufb01dence intervals are therefore more signi\ufb01cant and worth noting. Some useful patterns you may observe on these plots are:\n\nThe number of lags is p when:\n* The partial auto-correlation, |$\\rho_{pk}$| > 1.96 \/ $\\sqrt{n}$ for first p values and cuts off to zero. \n* The auto-correlation function, $\\rho_k$ decreases exponentially.","d3a1e0ec":"We can draw a boxplot to check the variation across months in a year (1990).\nIt appears that we have a seasonal component each year showing swing from summer to winter.","ddab7bbc":"**Perform multiplicative model decomposition on International Air Passengers Data.**","5410d090":"### Inferences\n\nThe tractor sales have been increasing without fail every year.\nJuly and August are the peak months for tractor sales and the variance and the mean value in July and August are also much higher than any of the other months.\nWe can see a seasonal cycle of 12 months where the mean value of each month starts with a increasing trend in the beginning of the year and drops down towards the end of the year. We can see a seasonal effect with a cycle of 12 months.","661cc4cc":"### b) Multivariate Time Series \n\nA multivariate time series data contains more than one time dependant variable. Each variable here depends on not only the past values but also has some dependency on other variables.This dependency is used for forecasting the future values. For reference , please have a look at the following website \n\n https:\/\/www.analyticsvidhya.com\/blog\/2018\/09\/multivariate-time-series-guide-forecasting-modeling-python-codes\/\n ","3744779c":"You can also try in this manner \n\n* Change the Split ratio as 75:25.\n* Use Single Exponential Smoothing method to forecast sales for the test data using the optimum smoothing level.\n* Calculate the values of RMSE and MAPE.\n* Plot the forecasted values along with original values.","8350d2fe":"# **11.Moving average forecast** <a class=\"anchor\" id=\"11\"><\/a>\n\n[Table of Contents](#0.1)\n\nMoving Average Smoothing is a naive and effective technique in time series forecasting.\n\nSmoothing is a technique applied to time series to remove the fine-grained variation between time steps. \n\nCalculating a moving average involves creating a new series where the values are comprised of the average of raw observations in the original time series.\n\nA moving average requires that you specify a window size called the window width. This defines the number of raw observations used to calculate the moving average value. \n\n### Two main types of moving averages:\n#### 1) Centered moving average  - calculated as the average of raw observations at, before and after time, t.\n#### 2) Trailing moving average - uses historical observations and is used on time series forecasting.","8f34b2f2":"You can try it with the window width 2 and 4 and check whether the accuracy has increased or not.You can  Use Champagne Sales data and observe seasonality using visualization techniques.\n\nAlso apply a moving average model for Champagne Sales data after decomposing with various window widths and compare the performance measures such as RMSE and MAPE.","fa4134cd":"*SimpleExpSmoothing* class must be instantiated and passed the training data. \n\nThe fit() function is then called providing the fit configuration, the alpha value, smoothing_level. \nIf this is omitted or set to None, the model will automatically optimize the value.\n\nWe will try with various values of $\\alpha$ such as 0.1, 0.5 and 0.99 and then let the model optimize $\\alpha$.\n\nA value close to 1 indicates fast learning (that is, only the most recent values influence the forecasts), whereas a value close to 0 indicates slow learning (past observations have a large influence on forecasts).\n\n\u2014 Page 89, Practical Time Series Forecasting with R, 2016.\n\nhttps:\/\/www.amazon.com\/Practical-Time-Forecasting-Hands-Analytics\/dp\/0997847913\/ref=as_li_ss_tl?ie=UTF8&qid=1527636709&sr=8-5&keywords=time+series+forecasting&linkCode=sl1&tag=inspiredalgor-20&linkId=dcb38fa9efe5af617b48b2922c4c149f","114ea69d":"##  Random Walk and Autocorrelation\n\n##### We calculate the correlation between each observation and the observations at previous time steps. \n\n##### Autocorrelation plot or a correlogram plot is a plot of these correlations. \n\n##### We expect a strong auto-correlation with the previous observation and a linear fall o\ufb00 from there with previous lag values.","43750d7a":"# 16. References      <a class=\"anchor\" id=\"16\"><\/a>\n\n[Table of Contents](#0.1)\n\n1. Time Series lecture by Great Learning by great lakes(https:\/\/www.youtube.com\/watch?v=FPM6it4v8MY)(Myself Ex-PG Student)\n2. Machine Learning Mastery by Jason Brownlee (https:\/\/machinelearningmastery.com\/)\n3. Introduction to Time Series Analysis by (Douglas C Montgomery, Cheryl L Jennings,Murat Kulachi)\n  \n  ","1733c26a":"#### Plot the time series data to detect patterns","23186832":"#### Inference\n\n**MAPE of ETS(A,A,M) model is 16.92 and it is lesser than 17.47, MAPE of  ETS(A,A,A) model, **\n\nHence, prediction of Holt-Winter - Additive Trend and Multiplicative Seasonality model is better than Holt-Winter - Additive model","09f15bfc":"### Inference\n\n* We see a trend in the lag observations.\n* We observe that the random walk is non-stationary since the p value > 0.05.\n* We know the variation from one time step to the next is either -1 or 1 and we get RMSE as 1","f8a270d9":"# 9.Components of the Time Series <a class=\"anchor\" id=\"9\"><\/a>\n\n[Table of Contents](#0.1)\n\nThe components of the time series are following :- \n\n* **Trend** :- A gradual shift or movement to relatively higher or lower values over a long period of time.\n       \n       1. When the time series analysis shows a general trend , that is upward . It is called uptrend.\n       2. When the time series analysis shows a general trend , that is downward. It is called downtrend.\n       3. When there is no trend, we call it horizontal or stationary trend.\n      \n* **Seasonality** :- It means upward or downward swings. Repeating periods within a fixed period of time. It is usually observed within a period of time.For   example , if you live in a country with cold winters and hot summers, your air conditioning costs goes high in summer and low in winters. \n* **Cyclic Patterns** :- It refers to repeating up and down movements. It usually go over more than a year of time.It is much harder to predict.\n* **Irregular** :- It refers to erratic, unsystematic, 'residual' flutuations. It is for short duration and non repeating. It happens due to random        variations or unforeseen events. It generally contains the white noise which we will see in the coming sections. \n\n","8d1bd052":"### 14.2.8Method 8: Holt-Winters Method - Additive seasonality <a class=\"anchor\" id=\"14.2.8\"><\/a>\n\n[Table of Contents](#0.1)","ba9547d1":"### Example \nWe can turn monthly data into yearly data. Down-sample the data using the alias, A for year-end frequency and this time use sum to calculate the total sales each year.","068a0e7b":"### Inference\n\nThe above ACF has \u201cdecayed\u201d fast and remains within the significance range (blue band) except for a few (5) lags. This is indicative of a stationary series.","b14b0593":"### Inference\n\n* The best fit model is selected based on Akaike Information Criterion (AIC) , and Bayesian Information Criterion (BIC) values. The idea is to choose a model with minimum AIC and BIC values.\n\nFor ARIMA(p, d, q) \u00d7 (P, D, Q)S,\nwe got SARIMAX(0, 1, 1)x(1, 0, 1, 12)12 model with the least AIC:-600.0908420381976\n\nHere, \n* p = non-seasonal AR order = 0,\n* d = non-seasonal differencing = 1,\n* q = non-seasonal MA order = 1,\n* P = seasonal AR order = 1,\n* D = seasonal differencing = 0,\n* Q = seasonal MA order = 1,\n* S = time span of repeating seasonal pattern = 12","7a3c7d12":"Random walk process is defined as:\ny(t) = $\\beta_0 + \\beta_1 X_{t-1}$ + $\\epsilon_t$\n\nwhere \n* y(t) is the next value in the series.\n* $\\beta_0$ is a coefficient that if set to a value other than zero adds a constant to the drift to the random walk.\n* $\\beta_1$ is a coefficient to weight the previous time step and is set to 1.\n* $X_{t-1}$ is the observtion at the previous time step.\n* $\\epsilon_t$ is the white noise or random fluctuation at that time.","e480143b":"### Inference\n\n**As of now, we observe that Moving average of window width of 4 seems to be a good fit for the data.**","1f42e9f8":"Though the variation in standard deviation is small, rolling mean is clearly increasing with time and this is not a stationary series. Also, the test statistic is way more than the critical values.","dae1a7b1":"### Multi-Steps Forecast","8da460b8":"\n# **1.Introduction to Time Series** <a class=\"anchor\" id=\"1\"><\/a>\n\n[Table of Contents](#0.1)\n\nEvery company in this world faces certain challenges and risks such as high compeition , failure of technology, labour unrest, inflation, recession, and change in government laws. Thus we can say that every buisness operates under risk and uncertainity. That's why forecast is necessary to lessen the adverse effect of the risks and to tell us in advance of any incoming dangers. There are various methods of forecast- mostly commonly used are :-\n\n1. Regression\n2. Data Mining Mehthods\n3. Time Series \n\nWhy these different teechniques are required for forecasting? The reason is that we have to deal with different types of data which possess different features , so to handle this different techniques are used for forecasting. For example, In case of regression or CART we have one response and a number of predictors. \n\n> **Forecasting is a technique that uses historical data as inputs to make informed estimates that are predictive in determining the direction of future trends. Businesses utilize forecasting to determine how to allocate their budgets or plan for anticipated expenses for an upcoming period of time.**\n\nIn this kernel , we are going to see the details about the time series data and how to analyze and forecast them. ","a9a01221":"### Check for stationarity using dickey fuller test","3a594c96":"### Moving average of window size 5 for US GDP","797bff4c":"* 1) Load the raw dataset and parses the date-time information as the Pandas DataFrame index. \n* 2) Drop the \u201cNo\u201d column \n* 3) Name each column. \n* 4) Replace NA values with \u201c0\u201d \n* 5) Remove first 24 hours.","90fe787d":"Observe how number of tractors sold vary on a month on month basis. We will plot a stacked annual plot to observe seasonality in our data.","577fff1c":"![Time Series](https:\/\/images.unsplash.com\/photo-1560221328-12fe60f83ab8?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1053&q=80)                     ","e4e3871f":"### Example \n\nUse Air Passengers data and fit ETS(A, A, M)  model to predict the last 12 months.\n* Build the Holt Winter's lETS(A, A, M) model to forecast sales for the test data\n* Calculate the values of RMSE and MAPE.\n* Plot the forecasted values along with original values.","39f08673":"Simulate Stock prices as follows:\n\n* Generate a random walk with mean 0.0001 and standard deviation of 0.1. Add 1 for total return.\n* Convert the total returns to get a starting value of 100.\n\nPlot the simulated random walk with drift.","7f34e08e":"A time series is a series of data points indexed in time order.  Most of the times, users want to replace the missing values in time series data by neighbouring non-missing values.  There may be a need for imputation or interpolation of missing values lying in between known values. \n\nMethods of imputation for replacing missing values (meaningful values)\n\n| Method | When suitable |\n| ---------------------------- | ------------------------------------ |\n| Take average of the nearest neighbours | Data has no seasonality |\n| Take average of the seasons from two or all available years | Data has seasonality |\n| Interpolate function of pandas |  |\n| Linear interpolation | Relationship in the interval of two samples is a first order polynomial |\n| Polynomial such as Quadratic or Cubic interpolation | Second or third order polynomial describes the interval between two samples |\n| Spline | Handles non-uniform spacing of samples |","748796cd":"#### Example\n\nThis example is taken from the following link\n\nhttps:\/\/www.dezyre.com\/recipes\/deal-with-missing-values-in-timeseries-in-python\n\n\n\n","517d360f":"Using Champagne Series data, perform additive model decomposition.\n\nHint:\ndate_rng = pd.date_range(start='1\/1\/1964', end='30\/9\/1972', freq='M')\ndate_rng\nChamp['TimeIndex'] = pd.DataFrame(date_rng, columns=['Month'])","5f154833":"Running the above code performs the decomposition, and plots the 4 resulting series.\nWe observe that the trend and seasonality are clearly separated.","ea2bd3e2":"Since the test statistics is more than 5 % critical value and the p-value is larger than 0.05 , the moving average is not constant over time and the null hypothesis of the Dickey-Fuller test cannot be rejected. This shows the weekly time series is not stationary. \n\nAs such , we need to transform this series into a stationary time series. ","63545713":"We observe trend and seasonality even after taking log of the observations.","2f85055f":"### 14.1.3Holt-Winters - ETS(A, A, A) - Holt Winter's linear method with additive errors     <a class=\"anchor\" id=\"14.1.3\"><\/a>\n\n[Table of Contents](#0.1)","99862670":"#### Inference\n\nWe have plotted the trend line in orange colour over the original dataset in blue colour.","432ff012":"Let's see the example of data from quandel. The data descibes the bank of England's official statistics on spot exchange rates for the EURO into US dollars.   ","68dc865c":"### Inference\n\nThe model is slightly better than SES model but worse than all moving average models.","4dc77c0e":"### Predicting a Random Walk\n\n###### A random walk is cannot reasonably be predicted. \n\n######  We expect that the best prediction would be to use the observation at the previous time step as what will happen in the next time step.  We know that in the random walk, the next time step will be a function of the prior time step. This is  called the naive forecast, or a persistence model.","80d95e88":"### 14.2.5Method 5: Moving Average(MA) <a class=\"anchor\" id=\"14.2.5\"><\/a>\n\n[Table of Contents](#0.1)","16b9e411":"This is first Kernel on Time Series analysis. In this kernel I have tried to cover topics which are related to univariate time series such as AR, MA,ARIMA as well as ETS models. Hope everyone will like this kernel. ","ff965809":"### Example \n\nLet us take the Daily Female Births Dataset as an example. \n\nThis dataset describes the number of daily female births in California in 1959.\n\nFit a moving average of window width 3 and evalue the model measures such as RMSE and MAPE.","e92dbd4e":"### Inference\n\nWe observe both trend and multiplicative seasonaliy from the plot shown above.\n\nWe try moving averages of various window widths such as 4, 6,8 and 12.","ae46e607":"#### Plot the average temp","f30aa01f":"### Inference\n\nWe observe that the resample() function has created the rows by putting NaN values as new values for dates other than day 01. \n\nNext we can interpolate the missing  values at this new frequency. The function, interpolate() of pandas library is used to interpolate the missing values. \nWe use a linear interpolation which draws a straight line between available data, on the first day of the month and fills in values at the chosen frequency from this line. ","ad73a27b":"\n**Given the weather conditions and pollution for prior hours, we forecast the pollution at the next hour.**","42c09934":"### 14.2.7Method 7: Holt's Linear Trend Method (Double Exponential Smoothing) <a class=\"anchor\" id=\"14.2.7\"><\/a>\n\n[Table of Contents](#0.1)","c7e954aa":"**Perform multiplicative model decomposition on Tractor Sales Series**\n\n## Visualization of Seasonality - Month plots\nLet us use retail turnover data and observe seasonality using visualization techniques.","2c71f24f":"## **15.3 Auto Arima**        <a class=\"anchor\" id=\"15.3\"><\/a>\n\n[Table of Contents](#0.1)\n\n It uses the AIC (Akaike Information Criterion) & BIC(Bayesian Information Criterion) values generated by trying different combinations of p,q & d values to fit the model.\n \n* In an ARIMA model there are 3 parameters, namely p, q and d that help model major aspects of a time series: seasonality, trend and noise.\n\n* If our model has a seasonal component, we use Seasonal ARIMA with parameters, P, Q and D related to seasonal components of the model.\n\n\n### auto.arima\n\nThe module auto.arima fits the best ARIMA model to univariate time series according to either AIC, AICc or BIC value. This function conducts a search over possible model within the order constraints provided.\n\n### AIC \n\nThe Akaike information criterion (AIC) is an estimator of the relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. Thus, AIC provides a means for model selection. \n\n### AICc is AIC with a correction for small sample sizes. \n\n### BIC \n\nBayesian information criterion (BIC) or Schwarz information criterion (also SIC, SBC, SBIC) is a criterion for model selection among a finite set of models; the model with the lowest BIC is preferred. It is based, in part, on the likelihood function and it is closely related to the Akaike information criterion (AIC). \n\nhttps:\/\/en.wikipedia.org\/wiki\/Akaike_information_criterion#Comparison_with_BIC\nhttps:\/\/en.wikipedia.org\/wiki\/Bayesian_information_criterion\n","4c4023dc":"### Plot ACF and PACF","13d42522":"Simulate Stock prices as follows:\n\n* Generate a random walk with mean 0.0001 and standard deviation of 0.1. Add 1 for total return.\n* Convert the total returns to get a starting value of 100.\n\nPlot the simulated random walk with drift.","a5409109":"### Inference\n\nWe observe that for the optimum $\\alpha$ value, both RMSE and MAPE are smallest when compared to other $\\alpha$ values of 0.1,0.5 and 0.99.","cb2b4097":"**Tractor Sales Series Data**\n\nThis data shows the details of the no of the tractors sold on the monthly basis for the year 2020. \n\n","8930fb33":"As we observed while plotting the moving average over months that there is a monhly pattern, now, let\u2019s decipher the seasonal component.","ef4c8f98":"So far, the moving average of window width 4 gives the lowest MAPE and RMSE.\nLet us try other models as well.","0075e5b1":"**Up-sampling**\n* Increase the frequencies of the sample, example: months to days\n* Care may be needed in deciding how the fine-grained observations are calculated using interpolation.\n\n* The function, resample() available in the pandas library works on the Series and DataFrame objects.\n* This can be used to group records when down-sampling and make space for new observations when up-sampling.","36ec04b1":"**Example 2 :- Checking on the water consumption data**\n\n1. For reference , you can have a look at this link below , the example taken here by me is from this article:- \n\nhttps:\/\/medium.com\/@drnesr\/filling-gaps-of-a-time-series-using-python-d4bfddd8c460","196ed7b9":"### Multiplicative model\n\n### Multiplicative Decomposition\n\n* An additive model suggests that the components are multipled together.\n* An additive model is non-linear such as quadratic or exponential. \n* Changes increase or decrease over time.\n* A non-linear seasonality has an increasing or decreasing frequency (width of the cycles) and \/ or amplitude (height of the cycles) over time.","5c647894":"**Confidence intervals provide an upper and lower expectation for the real observation. **\n\nThese are useful for assessing the range of real possible outcomes for a prediction and for better understanding the skill of the model.\n\nFor example, the ARIMA implementation in the statsmodel python library can be used to fit an ARIMA model. It returns an ARIMAResults object. \n\nThe object provides the forecast() function returns three values:\n* 1) Forecast: The forecasted value in the \n* 2) Standard Error of the model: \n* 3) Confidence Interval: The 95% confidence interval for the forecast","26fc4fc3":"Nonstationary series have an ACF that remains significant for half a dozen or more lags, rather than quickly declining to zero. You must difference such a series until it is stationary before you can identify the process\n\nThe above ACF is \u201cdecaying\u201d, or decreasing, very slowly, and remains well above the significance range (blue band) for at least a dozen lags. This is indicative of a non-stationary series.","fc126f45":"# **10.Decomposition of Time Series** <a class=\"anchor\" id=\"10\"><\/a>\n\n[Table of Contents](#0.1)","fd56ee83":"## 15.2Arima Model         <a class=\"anchor\" id=\"15.2\"><\/a>\n\n[Table of Contents](#0.1)\n\nOne of the most common methods used in time series forecasting is known as the ARIMA model, which stands for Auto Regressive Integrated Moving Average. ARIMA is a model that can be fitted to time series data to predict future points in the series.\n\nWe can split the Arima term into three terms, AR, I, MA:\n\nAR(p) stands for the autoregressive model, the p parameter is an integer that confirms how many lagged series are going to be used to forecast periods ahead.\n\nI(d) is the differencing part, the d parameter tells how many differencing orders are going to be used to make the series stationary.\n\nMA(q) stands for moving average model, the q is the number of lagged forecast error terms in the prediction equation. SARIMA is seasonal ARIMA and it is used with time series with seasonality.","7a24826f":"### Example \n\n**Up-sampling frequency**\n\n* The observations in the Shampoo Sales are monthly. We need to up-sample the frequency from monthly to daily and use an interpolation scheme to fill in the new daily frequency.\n\n* We can use this function to transform our monthly dataset into a daily dataset by calling resampling and specifying preferred frequency of calendar day frequency or D.","e0b1f26d":"### Forecast Range","7f598f93":"### Moving average of window size 3 for India GDP","322a9458":"### 14.2.9Method 9: Holt-Winters Method - Multiplicative Model <a class=\"anchor\" id=\"14.2.9\"><\/a>\n\n[Table of Contents](#0.1)","deb4424f":"Running the above code performs the decomposition, and plots the 4 resulting series. We observe that the trend and seasonality are clearly separated.","80343a56":"### 14.2.1Method  1: Regression on Time <a class=\"anchor\" id=\"14.2.1\"><\/a>\n\n[Table of Contents](#0.1)","54f40dfa":"### Trend & Seasonality\n\nConsider the example of **shampoo sales dataset**. \n\nThis Dataset describes the monthly number of sales of shampoo over a 3 year period. The units are a sales count and there are 36 observations. The original dataset is credited to Makridakis, Wheelwright, and Hyndman (1998).\n\n**Data source**:https:\/\/github.com\/jbrownlee\/Datasets\n\nBelow is a sample of the first 5 rows of data, including the header row.\n\n| Month | Sales |\n| ---- | -------- |\n| 1-01 | 266.0 | \n| 1-02 | 145.9 | \n| 1-03 | 183.1 | \n| 1-04 | 119.3 | \n| 1-05 | 180.3 | \n\n\n","ac19c23c":"### Time Series Decomposition","6c9fad85":"\n<a class=\"anchor\" id=\"0.1\"><\/a>\n# **Table of Contents**\n\n- 1.[Introduction to Time Series](#1)\n- 2.[What is Time Series?](#2)\n- 3.[What is not a Time Series](#3)\n- 4.[Where we can find the Time Series Data?](#4)\n- 5.[Features of the Time Series Data](#5)\n- 6.[Time Series Assumptions](#6)\n- 7.[Time Series Types](#7)\n- 8.[Reading and Saving Time Series Objects in Python](#8)\n- 9.[Components of the Time Series](#9)\n- 10.[Decomposition of Time Series](#10)\n- 11.[Moving average forecast](#11)\n- 12.[Handling Missing Values](#12)\n- 13.[Time Series  Range, Accuracy and Various Requirements](#13)\n- 14.[ETS Models](#14)\n     - 14.1  [SES, Holt & Holt-Winter Model](#14.1)\n        - 14.1.1  [SES - ETS(A, N, N) - Simple smoothing with additive errors](#14.1.1)\n        - 14.1.2  [Holt - ETS(A, A, N) - Holt's linear method with additive errors](#14.1.2)\n        - 14.1.3  [Holt-Winters - ETS(A, A, A) - Holt Winter's linear method with additive errors](#14.1.3)\n        - 14.1.4  [Holt-Winters - ETS(A, A, M) - Holt Winter's linear method ](#14.1.4)\n     - 14.2 [Model finalization](#14.2)\n        - 14.2.1  [Regression on Time](#14.2.1)\n        - 14.2.2  [ Regression on Time With Seasonal Components](#14.2.2)\n        - 14.2.3  [Naive Approach](#14.2.3)\n        - 14.2.4  [Simple Average](#14.2.4)\n        - 14.2.5  [Moving Average(MA)](#14.2.5)\n        - 14.2.6  [Simple Exponential Smoothing](#14.2.6)\n        - 14.2.7  [Holt's Linear Trend Method (Double Exponential Smoothing)](#14.2.7)\n        - 14.2.8  [Holt-Winters Method - Additive seasonality](#14.2.8)\n        - 14.2.9  [Holt-Winters Method - Multiplicative Model](#14.2.9)\n- 15.[AUTO REGRESSIVE Models](#15)\n     - 15.1  [Random Walk](#15.1)\n     - 15.2  [ARIMA Model](#15.2)\n     - 15.3  [Auto ARIMA](#15.3)\n- 16.[References](#16)\n  \n","62f3c211":"### 14.2.6Method 6: Simple Exponential Smoothing <a class=\"anchor\" id=\"14.2.6\"><\/a>\n\n[Table of Contents](#0.1)","251f58d5":"We can infer from the RMSE and MAPE values and the graphs above, that Naive method and Regression on Time With Seasonal Components model are not suited for datasets with high variability. \n\nNaive method is best suited for stable datasets. We can still improve our score by adopting different techniques. \n\nNow we will look at another technique and try to improve our score.","66e5d0e4":"### Example 2\n\nReading the  GDP of India series data  and save this TS object using python.\nData is yearly from 1960-1-1 to 2017-12-31.\n\nhttps:\/\/pythontips.com\/2013\/08\/02\/what-is-pickle-in-python\/\n\nAny object in python can be pickled so that it can be saved on disk. What pickle does is that it \u201cserialises\u201d the object first before writing it to file. \n\nPickling is a way to convert a python object (list, dict, etc.) into a character stream. The idea is that this character stream contains all the information necessary to reconstruct the object in another python script.\n\nNow we store the TS object and retrieve the same from the pickle object.","f32560a3":"**Inference drawn from the ACF and PACF values** \n\nUsing the plot we can determine the values for p and q respectively :\n\n1. p: the lag value where the PACF cuts off (drop to 0) for the first time. So here p =2.\n2. q: the lag value where the ACF chart crosses the upper confidence interval for the first time. if you look closely q=1. ","496a4688":"### Additive Model","03c0c805":"# **2.What is Time Series?**  <a class=\"anchor\" id=\"2\"><\/a>\n\n[Table of Contents](#0.1)\n\n\nA time series is a series of measurements on the **same variable** collected over time. These measurements are made at regular time intervals.A time series is a series of data points indexed in time order. Most commonly, a time series is a sequence taken at successive **equally spaced points** in time. Thus it is a sequence of **discrete-time data**. \n\nIntervals of the Time Series Data \n\n1. Yearly :- GDP , Macro-economic series\n2. Quarterly :- Revenue of a company.\n3. Monthly:- Sales, Expenditure, salary\n4. Weekly:- Demand , Price of Petrol and diesal\n5. Daily:- Closing price of stock, sensex value, daily transaction of ATM machine\n6. Hourly:- AAQI\n\nTime series analysis can be useful to see how a given asset, security, or economic variable changes over time. It can also be used to examine how the changes associated with the chosen data point compare to shifts in other variables over the same time period.\n\n","6516bac8":"The above plot shows a strong seasonality and trend component.\n\nWe can draw a boxplot to check the variation across months in a year (2011). ","0987d412":"### Calculations with missing values\n\n**Descriptive statistics and computational statistical methods are written to take into account for missing data.  **\n\n**Examples:**\n\n* When summing data, NA(missing) values will be treated as zeros.\n* If the data are all NA, the result will be 0.\n* Cumulative methods like cumsum() and cumprod() ignore NA values by default but preserve them in the resulting arrays. To override this behaviour and include NA values, use skipna = False","842fdf1e":"The model is returning the results we want to see, we can scale the model predictions back to the original scale. Hence the remove the first order differencing and take exponent to restore the predictions back to their original scale.","cb4ad97b":"We group the Minimum Daily Temperatures dataset by years. A box and whisker plot is then created for each year and lined up side-by-side for direct comparison.","9a926260":"## 14.1.2Holt - ETS(A, A, N) - Holt's linear method with additive errors <a class=\"anchor\" id=\"14.1.2\"><\/a>\n\n[Table of Contents](#0.1)\n\n## Double Exponential Smoothing\n\n* One of the drawbacks of the single exponential smoothing is that the model does not do well in the presence of the trend.\n* This model is an extension of SES and also known as Double Exponential model\n* Applicable when data has Trend but no seasonality\n* Two separate components are considered: Level and Trend\n* Level is the local mean\n* One smoothing parameter \u03b1 corresponds to the level series\n* A second smoothing parameter \u03b2 corresponds to the trend series.","81ae7126":"### Inserting missing values\n\nYou can assign missing values by simply assigning to containers. The missing value will be chosen based on the dtype.","a307e1d8":"We are going to use the shampoo dataset. This dataset describes the monthly number of sales of shampoo over a 3 year period.\nThe units are a sales count and there are 36 observations. The original dataset is credited to Makridakis, Wheelwright and Hyndman (1998).\n\nThe entire dataset is taken from Data Market.\n\nhttps:\/\/datamarket.com\/data\/set\/22r0\/sales-of-shampoo-over-a-three-year-period\n\n### Example \n\n* Split the data into train and test. \n* Build different time series models on train data set and test it on test data set\n* Compare models performance.","92700c05":"### Types of Trends\n\n* Deterministic Trends: They consistently increase or decrease and are easier to identify.\n* Stochastic Trends: They increase and decrease inconsistently \n\n#### Detrend a time series is by differencing","cf755c21":"This plot shows the signi\ufb01cant change in distribution of minimum temperatures across the months of the year from the Southern Hemisphere summer in January to the Southern Hemisphere winter in the middle of the year, and back to summer again.","823312c0":"An identified trend can be modeled. Once modeled, it can be removed from the time series dataset. \n\n#### Detrend by model fitting","6d93a23f":"# 12.Handling Missing Values <a class=\"anchor\" id=\"12\"><\/a>\n\n[Table of Contents](#0.1)\n\n### Missing Data\n\n#### 1. \tNo missing data is allowed in time series as data is ordered.\n#### 2. \tIt is simply not possible to shift the series to fill in the gaps.\n\n\n### Reasons for missing data\n\n#### 1) \tData is not collected or recorded\n#### 2) \tData never existed\n#### 3) \tData corruption\n","22235e92":"This series is not stationary.\nTry differencing - lag 1","69a5c397":"### Observation\n\nWe see very clearly that the turnover is high in Quarter 1 and very low in quarter 2.","6abecc3a":"**Down-sampling Frequency**\n\n* The sales data is monthly, but we prefer the data to be quarterly. The year can be divided into 4 business quarters, 3 months a piece. \n* The resample() function will group all observations by the new frequency.\n* We need to decide how to create a new quarterly value from each group of 3 records. We shall use the mean() function to calculate the average monthly sales numbers for the quarter","9cf776a5":"### 14.2.2Method 2: Regression on Time With Seasonal Components <a class=\"anchor\" id=\"14.2.2\"><\/a>\n\n[Table of Contents](#0.1)","958e7260":"### AR Model indentification\n\n\n### Auto-Correlation Function (ACF) or correlogram and Partial Auto-Correlation Function (PACF)","52633078":"### Example\n\nIn our examples, we use the same seed for the random number generator to ensure that we get the same random walk.","f8098a7f":"### **14.1SES, Holt & Holt-Winter Model** <a class=\"anchor\" id=\"14.1\"><\/a>\n\n[Table of Contents](#0.1)\n\n### Exponential Smoothing methods\n#####  Exponential smoothing methods consist of flattening time series data. \n##### Exponential smoothing averages or exponentially weighted moving averages consist of forecast based on previous periods data with exponentially declining influence on the older observations.\n##### Exponential smoothing methods consist of special case exponential moving with notation ETS (Error, Trend, Seasonality) where each can be none(N), additive (N), additive damped (Ad), Multiplicative (M) or multiplicative damped (Md).\n##### One or more parameters control how fast the weights decay.\n##### These parameters have values between 0 and 1\n\n","97707a4c":"### Multi-Steps Forecast","6567a014":"## Seasonal variation may be present in Time series data.\n\n* Seasonal variation, or seasonality, are cycles that repeat regularly over time.\n\n* By plotting and reviewing the data, you can determine if there is any seasonality in the data.\n* We can try with different scales and by adding a trend line.\n* Once the seasonality is identified, it can be modeled. When you remove the model of seasonality from the time series, it is called deseasonalizing or seasonal adjustment.\n\n** Seasonal adjustment with differencing**\n\nWe can test the seasonality differencing method on the daily minimum temperature data.","984c3179":"### Inference\n\nWe need to ensure that the residuals of our model are uncorrelated and normally distributed with zero-mean. If it is not that it signifies that the model can be further improved and we repeat the process with the residuals.\n\nIn this case, our model diagnostics suggests that the model residuals are normally distributed based on the following:\n\n1. The KDE plot of the residuals on the top right is almost similar with the normal distribution.\n2. The qq-plot on the bottom left shows that the ordered distribution of residuals (blue dots) follows the linear trend of the samples taken from a standard normal distribution with N(0, 1). Again, this is a strong indication that the residuals are normally distributed.\n3. The residuals over time (top left plot) don't display any obvious seasonality and appear to be white noise. This is confirmed by the autocorrelation (i.e. correlogram) plot on the bottom right, which shows that the time series residuals have low correlation with lagged versions of itself.\n\nThose observations coupled with the fact that there are no spikes outside the insignificant zone for both ACF and PACF plots lead us to conclude that that residuals are random with no information or juice in them and our model produces a satisfactory fit that could help us understand our time series data and forecast future values. It sems that our ARIMA model is working fine.","a46f8fbf":"### Forecast Accuracy\n\nThe error in the forecast is the difference between the actual value and the forecast.\n\nTwo popular accuracy measures are RMSE and MAPE.","f8735b7d":"We observe seasonality even after differencing.","38a2612c":"Moving average line is close to the original data line.","245f34eb":"**Additive model decomposition on Retail Turnover data**\n\n","02377f24":"# 4.Where we can find the Time Series Data? <a class=\"anchor\" id=\"4\"><\/a>\n\n[Table of Contents](#0.1)\n\nThe most common examples where we can encounter the time series data are following :- \n\n* Evaluation of manpower requirements from historic data and take a decision on hiring\n* Understanding the Stock movements based on the past data and advising their clients how best to invest. \n* Based on the consumption of the products , the grocery company can decide on which location the new shop can be set up.\n* In airline domain, based on the demand of the airline tickets between towns, airlines can create their dynamic ticket pricing.\n* For the hotels , based on the past data of the booking pattern it can decide on whether any discount can be offered at certain time of the year.\n\n> In short, we can say that time series data is being collected and utilized in all data driven decisions mechanisms. \n","94f8e56e":"### Report model accuracy","dd224d6d":"### Inference\n\nFor the same pertol data, we have tried both SES and HOLT model.\n\n| Model | RMSE | MAPE |\n| ----- | ------ | ----- |\n| SES |  0.9386  | 83.39 |\n| Holt | 0.8283 | 63.01 |  \n\nNote the decrease in both RMSE and MAPE for the HOLT model.","502c988b":"### Differencing \n\nWe can apply the concept of differencing to stationarize the data. Before differencing it is better to do the log trasnsformation of the data","ac59c3eb":"**Outliers**\nData may contain corrupt or extreme outlier values that need to be identified and handled.\n\n####  Detection of outliers in time series is difficult.\n* If a trend is present in the data, then usual method of detecting outliers by boxplot may not work.\n* If seasonality is present in the data, one particular season's data may be too small or too large compared to others.\n\n#### Decomposition helps in identifying unsual observations\n\n* If trend and seasonality are not adequate to explain the observation\n\n#### Outliers cannot be eliminated - they need to be imputed as closely as possible by using the knowledge gained from decomposition.","db9fb924":"### Forecast sales using the best fit ARIMA model","f69f7501":"The statsmodels library provides an implementation of the naive, or classical, decomposition method in a function called seasonal_decompose(). You need to specify whether the model is additive or multiplicative.\n\nThe seasonal_decompose() function returns a result object which contains arrays to access four pieces of data from the decomposition.\n\nhttps:\/\/machinelearningmastery.com\/decompose-time-series-data-trend-seasonality\/","4b29d4e5":"If we want to predict the  temperature for the next few months, we will try to look at the past values and try to gauge and extract the pattern. \nHere we observe a pattern within each year indicating a seasonal effect. Such observations will help us in predicting future values.\n\n**Note: We have used only one variable here , Temp (the temperature of the past 19 years).**\n\nHence this is called as the Univariate Time Series Analysis\/Forecasting. ","d8059d76":"# 7.Time Series Types  <a class=\"anchor\" id=\"7\"><\/a>\n\n[Table of Contents](#0.1)","f18320c0":"#### Inference\n\nWe don't see any particular trend in the data.","f39bce17":"### 14.1.4 Holt-Winters - ETS(A, A, M) - Holt Winter's linear method <a class=\"anchor\" id=\"14.1.4\"><\/a>\n\n[Table of Contents](#0.1)","d32dd920":"## 14.2Model finalization <a class=\"anchor\" id=\"14.2\"><\/a>\n\n[Table of Contents](#0.1)","3b184a36":"## **15.1Random Walk**  <a class=\"anchor\" id=\"15.1\"><\/a>\n\n[Table of Contents](#0.1)\n\nA **random walk** is a mathematical object, known as a stochastic or random process, that describes a path that consists of a succession of random steps on some mathematical space such as the integers. An elementary example of a random walk is the random walk on the integer number line, \n${\\displaystyle \\mathbb {Z} } $\n, which starts at 0 and at each step moves +1 or \u22121 with equal probability.\nhttps:\/\/en.wikipedia.org\/wiki\/Random_walk","c2d9cf72":"We use tractor sales data to replicate auto.arima in python.","25097db5":"#### Seasonality \u2013 Time Series Decomposition","edf804d7":"#### Define functions to calculate MAE  and MAPE ","67df454f":"#### Inference\n\nWe can see that this model didn\u2019t improve our score. Hence we can infer from the score that this method works best when the average at each time period remains constant. Though the score of Naive method is better than Average method, but this does not mean that the Naive method is better than Average method on all datasets. We should move step by step to each model and confirm whether it improves our model or not.","b515a355":"The Augmented Dickey Fuller Test (ADF) is unit root test for stationarity. \nThe null hypothesis is that time series is non-stationary.\nAlternative hypothesis is that time series is stationary.","7d5b515f":"**Let us use Petrol data and observe seasonality using visualization techniques**","4f7b358e":"Time Series forecast models can both make predictions and provide a confidence interval for those predictions.\n\n","a360c874":"* Beta is the smoothing factor to trends and it is zero and this gives more weight to the old  trend. \n* Gamma is the smoothing factor to seasonal index and is it zero and this gives, more weight  to old seasonal periods. ","b243464a":"**Inference** \n\nThe test statistic is less than 1% of the critical value, shows that the time series is stationary with 99% confidence. Now we can apply the statistical models like ARIMA to forecast the future values \n\n","a0f0e6c0":"# 5.Features of the Time Series Data <a class=\"anchor\" id=\"5\"><\/a>\n\n[Table of Contents](#0.1)\n\nThe following features mentioned below makes the time series analysis challenging and none of the other machine learning techniques applicable because of the following reasons :- \n\n* Data are depedent on each other.\n* In the case of time series , **ordering of data matters a lot**. \n* Ordering is very significant because there is dependency and changing the order will change the data structure.\n\nPlease note that in case where data is cross sectional , order of the obeservation does not matter but if the data is time series order of all the observations are important. ","58a4cb1c":"### Plot ACF and PACF for residuals of ARIMA model to ensure no more information is left for extraction","bf2621f5":"We will use Shampoo dataset.\n\n* A linear model can be fit on the time index to predict the observation. \n* Get a trend line from the predictions from this model.\n* Subtract these predictions from the original time series to provide a detrended version of the dataset.\n\nWe will use a scikit-learn LinearRegression model to train the data.","2159f693":"**Air pollution forecasting**\n\nhttps:\/\/machinelearningmastery.com\/multivariate-time-series-forecasting-lstms-keras\/\n\nThis dataset tells us about the weather and the level of the pollution each hour for five years at the US embassy located in Beijing,China. The data includes the date-time, the pollution called PM2.5 concentaration, and the other weather information including dew point, temeperature, pressure, wind direction , wind speed and the cumulative no of hours of snow and rain. \n\nThe complete feature list of the raw data is as follows :\n\n| Sl No | Variable | Description |\n| --- | --------------- | ------------------------------ |\n| 1 | No | row number | \n| 2 | qyear | year of data in this row | \n| 3 | month | month of data in this row | \n| 4 | day | day of data in this row | \n| 5 | hour | hour of data in this row | \n| 6 | pm2.5 | PM2.5 concentration | \n| 7 | DEWP | Dew Point | \n| 8 | TEMP | Temperature | \n| 9 | PRES | Pressure | \n| 10 | cbwd | Combined wind direction | \n| 11 | Iws | Cumulated wind speed | \n| 12 | Is | Cumulated hours of snow | \n| 13 | Ir | Cumulated hours of rain | \n\n\n\n","f3a77afe":"**Minimum Daily Temperatures Dataset**\n\nhttps:\/\/machinelearningmastery.com\/time-series-seasonality-with-python\/\n\nThis dataset describes the minimum daily temperatures over 10 years (1981-1990) in the city Melbourne, Australia.\n\nData source: Data Market https:\/\/datamarket.com\/data\/set\/22r0\/sales-of-shampoo-over-a-three-year-period\n\nThe units are in degrees Celsius and there are 3,650 observations. The source of the data is credited as the Australian Bureau of Meteorology.\n\nA sample of the first 5 rows of data, including the header row is shown below:\n\n| Date | Temperature |\n| ------- | -------- | \n| 1981-01-01 | 20.7 | \n| 1981-01-02 | 17.9 | \n| 1981-01-03 | 18.8 | \n| 1981-01-04 | 14.6 | \n| 1981-01-05 | 15.8 | \n","3e1eea75":"# **15.AUTO REGRESSIVE Models** <a class=\"anchor\" id=\"15\"><\/a>\n\n[Table of Contents](#0.1)\n\nAuto-regressive (AR) and moving average (MA) models are popular models that are frequently used for forecasting.\n\nAR and MA models are combined to create models such as auto-regressive moving average (ARMA) and auto-regressive integrated moving average (ARIMA) models. \n\nThe initial ARMA and ARIMA models were developed by Box and Jenkins in 1970.\n\nARMA models are basically regression models; auto-regression means regression of a variable on itself measured at different time periods. \n\nThe main assumption of AR model is that the time series data is stationary.\n\nA stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time.\n\nhttp:\/\/people.duke.edu\/~rnau\/411diff.htm\n\nWhen the time series data is not stationary, then we convert the non-stationary data before applying AR models. \n\n### Lags\n\nTaking the difference between consecutive observations is called a lag-1 difference.\n\nFor time series with a seasonal component, the lag may be expected to be the period (width) of the seasonality.\n\n**White noise of the residuals:**\n\nWhite noise is a process of residuals $\\epsilon_t$ that are uncorrelated and follow normal distribution with mean 0 and constant standard deviation. In AR models, one of the main assumptions is the errors follow a white noise.\n\n### Auto-Regressive  (AR) Models\n\nAuto-Regression is a regression of a variable on itself measured at different time points. \nAuto-Regressive model with lag 1, AR(1) is given by \n* $Y_{t+1} = \\beta Y_t + \\epsilon_{t+1}$  and this same as\n* $Y_{t+1} - \\mu = \\beta (Y_t - \\mu) + \\epsilon_{t+1}$  and this same as\n* where $\\epsilon_{t+1}$ is a sequence of uncorrelated residuals that follow normal distribution with zero mean and constant deviation. \n * $Y_{t+1} - \\mu$ is interpreted as a deviation from mean value $mu$ and known as mean centered series.\n\n","3f6376e6":"**Double Exponential Smoothing uses two equations to forecast future values of the time series, one for forecating the short term avarage value or level and the other for capturing the trend.**\n\n* Intercept or Level equation, $L_t$ is given by:\n$L_t = {\\alpha}{Y_t}  + (1 - \\alpha)F_t$ \n\n* Trend equation is given by \n$T_t = {\\beta}{(L_t - L_{t-1})}  + (1 - \\beta)T_{t-1}$ \n\nHere, $\\alpha$ and $\\beta$ are the smoothing constants for level and trend, respectively, \n* 0 <$\\alpha$ < 1 and 0 < $\\beta$ < 1.\n\nThe forecast at time t + 1 is given by\n* $F_{t+1} = L_t + T_t$\n* $F_{t+n} = L_t + nT_t$","01ffae03":"### a) Univariate Time Series\n\nA univariate Time series is a series of data  with a single time dependant variable like Demand for a product at time,t. \n\n> A time series that consists of single (scalar) observations recorded sequentially over equal time increments. Some examples are monthly CO2 concentrations and southern oscillations to predict el nino effects.\n\nFor example, have a look at the sample dataset below that consists of the minimum temperatures across the months of the year from the Southern Hemisphere from 1981 to 1990. Here, temperature is the dependent variable (dependent on Time).\n\n","95394748":"### Some of our key observations from this analysis:\n\n1) Trend: 12-months moving average looks quite similar to a straight line hence we could have easily used linear regression to estimate the trend in this data.\n\n2) Seasonality: Seasonal plot displays a fairly consistent month-on-month pattern. The monthly seasonal components are average values for a month after removal of trend. Trend is removed from the time series using the following formula:\n\nSeasonality_t \u00d7 Remainder_t = Y_t\/Trend_t\n \n3) Irregular Remainder (random): is the residual left in the series after removal of trend and seasonal components. Remainder is calculated using the following formula:\n\nRemainder_t = Y_t \/ (Trend_t \u00d7 Seasonality_t)","2f19bad8":"### Seasonal Indices\n\n* Seasonality is the presence of variations that occur at specific regular intervals less than a year, such as weekly, monthly, or quarterly. \n* Seasonality may be caused by various factors, such as weather, vacation, and holidays and consists of periodic, repetitive, and generally regular and predictable patterns in the levels of a time series.","25e60538":"### Predict sales on in-sample date using the best fit ARIMA model","8077df59":"We don't observe much year-by-year variation ","ff9dfeba":"We have the petrol data from Jan 2001 to Sep 2013.\n* Split the data into train and test in the ratio 70:30\n* Use Single Exponential Smoothing method to forecast sales using the test data.\n* Calculate the values of RMSE and MAPE.\n* Plot the forecasted values along with original values.","dddde650":"We would have used several models such as moving average, exponential smoothing, etc. before selecting the best model. \n\nThe model selection may depend on the chosen forecasting accuracy measure such as:","706cd726":"### Random Walk and Stationarity\n\n###### When the values of Time series data are not a function of time, we have a stationary time series.\n\n######  We know that the observations in a random walk are dependent on time. The current observation is a random step from the previous observation.\n\n######  Not all non-stationary time series are random walks and  a non-stationary time series does not have a consistent mean and\/or variance over time. \n\n######  We con\ufb01rm non-stationary property of random walk using a statistical signi\ufb01cance test, speci\ufb01cally the Augmented Dickey-Fuller test","99028ada":"### Inference\n\n* We see a trend in the first 500 lag observations.\n* We observe that the random walk is non-stationary since the p value > 0.05.\n* We know the variation from one time step to the next is either -1 or 1 and we get RMSE as 1","345d3d3a":"* Mean Absolute Error,  MAE = (1\/n) (|Y1 - F1| + |Y2- F2| + ... + |Yn - Fn|)\n* Mean Absolute Percentage Error,  MAPE = (1\/n) ((|Y1 - F1|\/Y1) + (|Y2 - F2|\/Y2) + ... + (|Yn- Fn|\/Yn) * 100)\n* Mean Squared Error, MSE =  (1\/n) ((Y1 - F1)^2 + (Y2- F2)^2 + ... + (Yn - Fn)^2)\n* Root Mean Square Error, RMSE = square root of MSE\n\nwhere n is the number of observations\nYn is the actual value of Y at time n\nFn is the corresponding forecasted value.\nRMSE and MAPE are two most popular accuracy measures of forecasting."}}