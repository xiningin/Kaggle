{"cell_type":{"317ec897":"code","eac55f84":"code","122b3097":"code","cf2e73c4":"code","d8d937c3":"code","d0e21f11":"code","2e1f085b":"code","5ff4d1bc":"code","ad176757":"code","a618939f":"code","cebe188c":"code","498295cf":"code","76ac6967":"code","188d4919":"code","c46931c9":"code","39170ea8":"code","29cdb05a":"code","caf204a9":"code","0530abdc":"code","0b528298":"code","438b9d47":"code","01b55187":"code","51f91906":"code","cdf84b75":"code","39f2da1a":"code","c4fe41f9":"code","2c0dfb2b":"code","c1da337a":"code","16bab97d":"code","9dc2e8b9":"code","ec2a018e":"code","a6f93e09":"code","8ed8c50a":"code","033b9071":"code","85d647c1":"code","739128a3":"code","797a207f":"code","6862a623":"code","9a60232c":"code","f40df585":"code","021ca821":"code","d70c49cc":"code","b24f318e":"code","1dec3df8":"code","a6ab98f4":"code","8e00337d":"code","a8855ae2":"code","7e5b035f":"code","d092a27b":"markdown","e23d4c5c":"markdown","2855f6d7":"markdown","2b510be8":"markdown","41c5d3d7":"markdown","edb0b1fc":"markdown","9fac460d":"markdown","7db36c67":"markdown","ba794819":"markdown","bd70d018":"markdown","12321cbe":"markdown","10279320":"markdown","a9659760":"markdown","3d8b8670":"markdown","386249bd":"markdown","20276131":"markdown","83728c52":"markdown","68d763fd":"markdown","2eb057c8":"markdown","a94ebfe6":"markdown","f5b70648":"markdown","e54c2c48":"markdown","32205c6c":"markdown","88c20622":"markdown","6f9d056a":"markdown","cb4f1b99":"markdown","f08cf607":"markdown","74effd4b":"markdown","fa71147c":"markdown","6c3c1a96":"markdown","07befe07":"markdown","9aeaf143":"markdown","7ab20ea5":"markdown","4784d96f":"markdown","459d827f":"markdown","7becd829":"markdown"},"source":{"317ec897":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')","eac55f84":"df=pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf.head()","122b3097":"df.info()","cf2e73c4":"df[df['TotalCharges']==\" \"]","d8d937c3":"df['TotalCharges']=df['TotalCharges'].replace(\" \",0).astype('float32')","d0e21f11":"#percentage of classes\nch=df[df['Churn']=='Yes']\nno_ch=df[df['Churn']=='No']\nprint('churn percentage-->',(ch.shape[0]\/df.shape[0])*100)\nprint('no churn percentage-->',(no_ch.shape[0]\/df.shape[0])*100)\n\ndf['Churn'].value_counts().plot(kind='pie', autopct='%1.1f%%');","2e1f085b":"data=df.copy()","5ff4d1bc":"def pie(features):\n    for feature in features:\n        plt.figure(figsize=(10,10))\n        plt.subplot(1,2,1)\n        data[data['Churn']=='Yes'][feature].value_counts().plot(kind='pie', autopct='%1.1f%%');\n        plt.title('Churn');\n        plt.subplot(1,2,2)\n        data[data['Churn']=='No'][feature].value_counts().plot(kind='pie', autopct='%1.1f%%');\n        plt.title('No Churn');\n\nfeatures=['gender', 'SeniorCitizen', 'Partner', 'Dependents',\n       'PhoneService', 'MultipleLines', 'InternetService',\n       'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n       'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n       'PaymentMethod']\npie(features)","ad176757":"def kde(feature):\n    plt.figure(figsize=(14,4))\n    plt.title('Distribution of {}'. format(feature))\n    sns.kdeplot(data[data['Churn']=='Yes'][feature], label='Churn');\n    sns.kdeplot(data[data['Churn']=='No'][feature], label='No Churn');\n\nkde('tenure')\nkde('MonthlyCharges')\nkde('TotalCharges')","a618939f":"def box(feature):\n    plt.figure(figsize=(4,4))\n    sns.boxplot(x='Churn', y=feature, data=data);\n    \nbox('tenure')\nbox('MonthlyCharges')    \nbox('TotalCharges')","cebe188c":"#step-by-step dummy encoding, \n#encoding one column at a time and deleting redundant columns\n\ndata.drop(columns=data.columns[0],inplace=True)\n\ndata['Male']=pd.get_dummies(data.iloc[:,0], drop_first=True)\ndata.drop(columns=data.columns[0],inplace=True)\n\ndata['Partner_yes']=pd.get_dummies(data.iloc[:,1],drop_first=True)\ndata.drop(columns=data.columns[1], inplace=True)\n\ndata['Dependent_yes']=pd.get_dummies(data.iloc[:,1],drop_first=True)\ndata.drop(columns=data.columns[1], inplace=True)\n\ndata['Phone_service_yes']=pd.get_dummies(data.iloc[:,2],drop_first=True)\ndata.drop(columns=data.columns[2], inplace=True)\n\ndata['multiple_lines_yes']=pd.get_dummies(data.iloc[:,2]).iloc[:,-1]\ndata.drop(columns=data.columns[2], inplace=True)\n\ninternet=pd.get_dummies(data.iloc[:,2],prefix='Internet')\ndata=pd.concat([data,internet],axis=1).drop(columns=['InternetService'])\n\ndata['online security_yes']=pd.get_dummies(data.iloc[:,2]).iloc[:,2]\ndata.drop(columns='OnlineSecurity',inplace=True)\n\ndata['online backup_yes']=pd.get_dummies(data.iloc[:,2]).iloc[:,2]\ndata.drop(columns='OnlineBackup',inplace=True)\n\ndata['device protection_yes']=pd.get_dummies(data.iloc[:,2]).iloc[:,2]\ndata.drop(columns='DeviceProtection',inplace=True)\n\ndata['tech support_yes']=pd.get_dummies(data.iloc[:,2]).iloc[:,2]\ndata.drop(columns='TechSupport',inplace=True)\n\ndata['streamingTV_yes']=pd.get_dummies(data.iloc[:,2]).iloc[:,2]\ndata.drop(columns='StreamingTV',inplace=True)\n\ndata['streaming movies_yes']=pd.get_dummies(data.iloc[:,2]).iloc[:,2]\ndata.drop(columns='StreamingMovies',inplace=True)\n\ncontract=pd.get_dummies(data.iloc[:,2],prefix='contract')\ndata=pd.concat([data,contract],axis=1).drop(columns=['Contract'])\n\ndata['paperless biling_yes']=pd.get_dummies(data.iloc[:,2]).iloc[:,1]\ndata.drop(columns='PaperlessBilling',inplace=True)\n\npaymethod=pd.get_dummies(data.iloc[:,2],prefix='paymethod')\ndata=pd.concat([data,paymethod],axis=1).drop(columns=['PaymentMethod'])","498295cf":"data.head(2)","76ac6967":"#separate data and labels\ny=data['Churn']\ndata.drop(columns='Churn', inplace=True)","188d4919":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier","c46931c9":"#feature selection via random forest\nforest=RandomForestClassifier(n_estimators=600, max_depth=5, random_state=7)\nforest.fit(data,y)\nimp=forest.feature_importances_\n\n#store feature importances in new DataFrame\nfeature_importances=pd.DataFrame()\nfeature_importances['feature']=pd.Series(data.columns)\nfeature_importances['importance']=imp\nfeature_importances.head()","39170ea8":"plt.figure(figsize=(10,10))\nsns.barplot(x='importance', y='feature', \n            data=feature_importances.sort_values(by='importance',ascending=False));","29cdb05a":"#keep most important columns and create final training dataset\ncols=feature_importances.sort_values(by='importance',ascending=False).iloc[:12,0].values\nx=data[cols].values\ndata[cols].head(2)","caf204a9":"#encode labels, and train_test_split\nenc=LabelEncoder()\ny=enc.fit_transform(y)\n\nx_tr,x_ts,y_tr,y_ts=train_test_split(x,y,stratify=y, random_state=77)","0530abdc":"#scaling data, only numerical columns\nfrom sklearn.preprocessing import StandardScaler\nnum_cols=[1,3,4]#numerical columns(tenure,total charges, monthly charges)\nsc=StandardScaler()\nx_tr[:,num_cols]=sc.fit_transform(x_tr[:,num_cols])\nx_ts[:,num_cols]=sc.transform(x_ts[:,num_cols])","0b528298":"from sklearn.decomposition import PCA\npca=PCA(n_components=3)\nx_tr_pca=pca.fit_transform(x_tr)\n\nx_viz=pd.concat(objs=[pd.DataFrame(x_tr_pca),pd.Series(y_tr)],axis=1).values\n\nplt.figure(figsize=(10,10))\nax=plt.axes()\nxv=x_viz[:,0]\nyv=x_viz[:,1]\nzv=x_viz[:,2]\ncv=x_viz[:,3]\nax.scatter(xv, yv, c=cv, cmap='winter')\nplt.show();","438b9d47":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix,f1_score,precision_score,recall_score\n\n#DataFrame to store performance metrics for later comparison between models\nresults=pd.DataFrame([], columns=['model', 'parameters','accuracy','precision','recall','F1-score'])","01b55187":"nb=GaussianNB()\nnb.fit(x_tr,y_tr)\nprint('accuracy:',accuracy_score(y_ts, nb.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, nb.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","51f91906":"knn=KNeighborsClassifier()\nknn.fit(x_tr,y_tr)\nprint('accuracy:',accuracy_score(y_ts, knn.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, knn.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","cdf84b75":"svm=SVC()\nsvm.fit(x_tr,y_tr)\nprint('accuracy:',accuracy_score(y_ts, svm.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, svm.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","39f2da1a":"sgd=SGDClassifier()\nsgd.fit(x_tr, y_tr)\nprint('accuracy:',accuracy_score(y_ts, sgd.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, sgd.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","c4fe41f9":"lr=LogisticRegression()\nlr.fit(x_tr,y_tr)\nprint('accuracy\"',accuracy_score(y_ts, lr.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, lr.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","2c0dfb2b":"tree=DecisionTreeClassifier(max_depth=4, random_state=3)\ntree.fit(x_tr,y_tr)\nprint('accuracy:',accuracy_score(y_ts, tree.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, tree.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","c1da337a":"rf=RandomForestClassifier(n_estimators=100,max_depth=8,random_state=17)\nrf.fit(x_tr,y_tr)\nprint('accuracy:',accuracy_score(y_ts, rf.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, rf.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","16bab97d":"#dummy classifier\nfrom sklearn.dummy import DummyClassifier\n\ndum=DummyClassifier(strategy='most_frequent')\ndum.fit(x_tr,y_tr)\npred=dum.predict(x_ts)\nprint('dummy class:',format(np.unique(pred)))\nprint('dummy accuracy:',accuracy_score(y_ts,pred) )\nsns.heatmap(confusion_matrix(y_ts, dum.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","9dc2e8b9":"#another way to show this, array with all zeros\naccuracy_score(y_ts,np.zeros(x_ts.shape[0]))","ec2a018e":"svm=SVC(class_weight='balanced')\nsvm.fit(x_tr,y_tr)\nprint('accuracy:',accuracy_score(y_ts, svm.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, svm.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","a6f93e09":"sgd=SGDClassifier(class_weight='balanced')\nsgd.fit(x_tr, y_tr)\nprint('accuracy:',accuracy_score(y_ts, sgd.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts,sgd.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","8ed8c50a":"lr=LogisticRegression(class_weight='balanced')\nlr.fit(x_tr,y_tr)\nprint('accuracy:',accuracy_score(y_ts, lr.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, lr.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","033b9071":"tree=DecisionTreeClassifier(max_depth=4, random_state=3,class_weight='balanced')\ntree.fit(x_tr,y_tr)\nprint('accuracy:',accuracy_score(y_ts, tree.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, tree.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","85d647c1":"rf=RandomForestClassifier(n_estimators=100,max_depth=8,\n                          random_state=17,class_weight='balanced')\nrf.fit(x_tr,y_tr)\nprint('accuracy:',accuracy_score(y_ts, rf.predict(x_ts)))\nsns.heatmap(confusion_matrix(y_ts, rf.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","739128a3":"from sklearn.utils import resample\nfrom sklearn.dummy import DummyClassifier\n\nx_up, y_up=resample(x_tr[y_tr==1],y_tr[y_tr==1],replace=True,\n                        n_samples=x_tr[y_tr==0].shape[0],random_state=42)\nprint(x_tr[y_tr==1].shape)\nprint(x_up.shape)\n\nx_bal=np.vstack((x_tr[y_tr==0], x_up))\ny_bal=np.hstack((y_tr[y_tr==0],y_up))\n\n\ndum2=DummyClassifier(strategy='most_frequent')\ndum2.fit(x_bal,y_bal)\nprint('dummy accuracy on balanced dataset:',\n      accuracy_score(y_bal,dum2.predict(x_bal)))","797a207f":"gb=GaussianNB()\ngb.fit(x_bal,y_bal)\ny_pred=gb.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred)\nrecall=recall_score(y_ts,y_pred)\nf1=f1_score(y_ts,y_pred)\nresults=results.append(pd.DataFrame([['Gaussian NB', 'default', accuracy,precision,recall,f1]],columns=list(results.columns)))\nprint('accuracy:',accuracy_score(y_ts, y_pred))\nsns.heatmap(confusion_matrix(y_ts, gb.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","6862a623":"knn=KNeighborsClassifier()\nknn.fit(x_bal,y_bal)\ny_pred=knn.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred)\nrecall=recall_score(y_ts,y_pred)\nf1=f1_score(y_ts,y_pred)\nresults=results.append(pd.DataFrame([['KNN', 'default', accuracy,precision,recall,f1]],columns=list(results.columns)))\nprint('accuracy:',accuracy_score(y_ts, y_pred))\nsns.heatmap(confusion_matrix(y_ts, knn.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","9a60232c":"svm=SVC()\nrng=[0.01, 0.1, 1.0, 10.0, 100.0]\nparams={'C':rng, 'gamma':rng}\ngs=GridSearchCV(estimator=svm,param_grid=params)\n\n\ngs.fit(x_bal,y_bal)\nbest_params=gs.best_params_\nbest_est=gs.best_estimator_\n\nprint('best params',best_params)\n\nbest_est.fit(x_bal,y_bal)\n\ny_pred=best_est.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred)\nrecall=recall_score(y_ts,y_pred)\nf1=f1_score(y_ts,y_pred)\nresults=results.append(pd.DataFrame([['SVM', best_params, accuracy,precision,recall,f1]],columns=list(results.columns)))\nprint('accuracy:',accuracy_score(y_ts, y_pred))\n\nsns.heatmap(confusion_matrix(y_ts, y_pred),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');\n","f40df585":"svm=SVC()\nsvm.fit(x_bal,y_bal)\n\n\ny_pred=svm.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred)\nrecall=recall_score(y_ts,y_pred)\nf1=f1_score(y_ts,y_pred)\nresults=results.append(pd.DataFrame([['SVM', 'default', accuracy,precision,recall,f1]],columns=list(results.columns)))\nprint('accuracy:',accuracy_score(y_ts, y_pred))\n\nsns.heatmap(confusion_matrix(y_ts, svm.predict(x_ts)),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","021ca821":"sgd=SGDClassifier(random_state=3)\nparams={'loss':['log', 'modified_huber', 'squared_hinge'],\n       'penalty': ['l1','l2'],\n       'alpha':[0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}\ngs=GridSearchCV(estimator=sgd, param_grid=params)\ngs.fit(x_bal, y_bal)\nbest=gs.best_estimator_\nbest_params=gs.best_params_\n\nbest.fit(x_bal,y_bal)\n\ny_pred=best.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred)\nrecall=recall_score(y_ts,y_pred)\nf1=f1_score(y_ts,y_pred)\nresults=results.append(pd.DataFrame([['SGC', best_params, accuracy,precision,recall,f1]],columns=list(results.columns)))\n\n\nprint('best parameters:',best_params)\n\nprint('best estimator accuracy:',accuracy_score(y_ts, y_pred))\nsns.heatmap(confusion_matrix(y_ts, y_pred),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","d70c49cc":"lr=LogisticRegression(random_state=3)\nparams={'penalty':['l2'],\n       'C':[0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],\n       'solver': [ 'sag','saga', 'lbfgs']}\ngs=GridSearchCV(estimator=lr,param_grid=params)\ngs.fit(x_bal,y_bal)\nbest=gs.best_estimator_\nbest_params=gs.best_params_\nprint('best params:', best_params)\ny_pred=best.predict(x_ts)\n\nbest.fit(x_bal,y_bal)\n\ny_pred=best.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred)\nrecall=recall_score(y_ts,y_pred)\nf1=f1_score(y_ts,y_pred)\nresults=results.append(pd.DataFrame([['Logistic Regression', best_params, accuracy,precision,recall,f1]],columns=list(results.columns)))\nprint('accuracy\"',accuracy_score(y_ts, y_pred))\nsns.heatmap(confusion_matrix(y_ts, y_pred),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","b24f318e":"tree=DecisionTreeClassifier(max_depth=4, random_state=3)\ntree.fit(x_bal,y_bal)\n\ny_pred=tree.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred)\nrecall=recall_score(y_ts,y_pred)\nf1=f1_score(y_ts,y_pred)\nresults=results.append(pd.DataFrame([['Decision Tree', 'max_depth=4, rand_state=3', accuracy,precision,recall,f1]],columns=list(results.columns)))\nprint('accuracy:',accuracy)\nsns.heatmap(confusion_matrix(y_ts, y_pred),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","1dec3df8":"rf=RandomForestClassifier(n_estimators=1000,max_depth=10,random_state=17)\nrf.fit(x_bal,y_bal)\n\n\ny_pred=rf.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred)\nrecall=recall_score(y_ts,y_pred)\nf1=f1_score(y_ts,y_pred)\nresults=results.append(pd.DataFrame([['Random Forest', 'max_depth=10, rand_state=17', accuracy,precision,recall,f1]],columns=list(results.columns)))\nprint('accuracy:',accuracy_score(y_ts, y_pred))\n\nsns.heatmap(confusion_matrix(y_ts, y_pred),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","a6ab98f4":"tree=DecisionTreeClassifier(criterion='entropy',random_state=1, max_depth=1)\nada=AdaBoostClassifier(base_estimator=tree,n_estimators=1000,learning_rate=0.1, random_state=5)\nada.fit(x_bal,y_bal)\n\ny_pred=ada.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred)\nrecall=recall_score(y_ts,y_pred)\nf1=f1_score(y_ts,y_pred)\nresults=results.append(pd.DataFrame([['AdaBoost Tree', 'criterion=entropy, max_depth=1, rate=0.1, estimators=500',\n                                      accuracy,precision,recall,f1]],columns=list(results.columns)))\nprint('accuracy:',accuracy)\nsns.heatmap(confusion_matrix(y_ts, y_pred),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","8e00337d":"gradb=GradientBoostingClassifier(random_state=42)\ngradb.fit(x_bal,y_bal)\n\ny_pred=gradb.predict(x_ts)\naccuracy=accuracy_score(y_ts,y_pred)\nprecision=precision_score(y_ts,y_pred)\nrecall=recall_score(y_ts,y_pred)\nf1=f1_score(y_ts,y_pred)\nresults=results.append(pd.DataFrame([['GradientBoosted Tree', 'default, random_state=42', accuracy,precision,recall,f1]],columns=list(results.columns)))\nprint('accuracy:',accuracy)\nsns.heatmap(confusion_matrix(y_ts, y_pred),annot=True,fmt='d');\nplt.ylabel('true')\nplt.xlabel('predicted');","a8855ae2":"results=results.reset_index().drop(columns='index')\nresults","7e5b035f":"sns.catplot(y='model', x='accuracy', kind='bar', data=results.sort_values(by='accuracy',ascending=False), color='grey');\nplt.title('Model Accuracy');\nprint(' ')\nsns.catplot(y='model', x='F1-score', kind='bar', data=results.sort_values(by='F1-score',ascending=False), color='black');\nplt.title('model F1-score' );","d092a27b":"In this project we will use traditional classifiers to predict customer churn. Our dataset is significantly imbalanced, with the 'No Churn' instances outnumbering the 'Churn' ones to the degree that it will influence our models negatively.\n\nWe will deal with this by upsampling the minority class, paying attention to the rate of false negatives as we train our models.\n\nWe start with some exploratory analysis, then data preprocessing and preparation, and finally, machine learning models and comparison of their performance.","e23d4c5c":"Random Forest","2855f6d7":"KNN","2b510be8":"Naive Bayes","41c5d3d7":"Distribution of categorical variables","edb0b1fc":"Logistic Regression","9fac460d":"Although there don't seem to be any nulls, there are eleven rows where the TotalCharges is \" \".","7db36c67":"Logistic Regression","ba794819":"There are eleven rows where the TotalCharges is \" \". These clients have zero tenure, and haven't churned, therefore we presume they're new customers who haven't paid anything yet.\n\nWe register these rows as Total Charges = 0","bd70d018":"We will start by deploying a few models on the raw, unbalanced data, and compare the results with a dummy classifier. If the results don't look good, we will upsample the minority class.","12321cbe":"Naive Bayes","10279320":"# Customer Churn Prediction with Imbalanced Data","a9659760":"### PCA visualization","3d8b8670":"Ada Boost","386249bd":"What would happen if instead of making a classification, we blindly assigned each sample to the majority class? How would the accuracy of this process compare with our models?\n\nLet's try it","20276131":"Before upsampling the minority class, let's consider some models with configurable class weights","83728c52":"Decision Tree","68d763fd":"Gradient Boosted Tree","2eb057c8":"The dummy accuracy has now been lowered to 50%, and any results our models have from now on will be considered 'real' results.","a94ebfe6":"SVM","f5b70648":"Random Forest","e54c2c48":"Distribution of Continuous Variables","32205c6c":"Now we will prepare the dataset for model training, by one-hot encoding categoricals and removing unnecessary columns ('customerID'). For maximus control over the process we will do it in a structured, step-by-step way.","88c20622":"Decision Tree","6f9d056a":"### Exploratory Analysis","cb4f1b99":"The resulting accuracy is exactly equal to the proportion of the majority class in the dataset, which is 73.45%. Our models didn't score much higher than that, so they barely surpassed a dummy classification.","f08cf607":"### Data Preprocessing and Feature Selection","74effd4b":"We have an imbalanced dataset and this will probably pose problems to our machine learning. We will deal with this later.","fa71147c":"### Machine learning","6c3c1a96":"SGD","07befe07":"We see that although the overall accuracy has somewhat dropped, the ratio of true versus false negatives has been improved in all models, in some cases significantly. The performance is still unacceptable, though, and before playing with the models' parameters, we will try oversampling the minority class.","9aeaf143":"The models may seem to have dropped in performance, but previously the dummy classifier's accuracy was 73% and the models exceeded that by two percent or so, while now the dummy accuracy is 50% and the models supersede it by up to 28,5%, and with fewer false negatives.","7ab20ea5":"From the above barplots is evident that Random Forest, Gradient Boosted Tree, and SVM give the best results, and should be use to predict customer churn.","4784d96f":"SGD","459d827f":"SVM","7becd829":"KNN"}}