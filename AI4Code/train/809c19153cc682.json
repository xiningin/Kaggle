{"cell_type":{"d58a08c7":"code","d937c3fe":"code","11b0bc1f":"code","93e9f3fc":"code","bc1f17e4":"markdown","bb9bd6a9":"markdown","3373f956":"markdown","cb050bc1":"markdown","bb0e3e1a":"markdown"},"source":{"d58a08c7":"import numpy as np \nimport pandas as pd \nimport catboost\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport re\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import product\nimport time\nimport os\nimport gc\nimport pickle\nfrom xgboost import XGBRegressor\nimport matplotlib.pylab as plt\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\npd.set_option('display.max_columns',100)\nimport os\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:#tqdm(df.columns):\n        col_type = df[col].dtypes\n\n        if col_type=='object':\n            df[col] = df[col].astype('category')\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ndef display_status(message):\n    from IPython.display import display, clear_output\n    import time\n    display(message) # print string\n    clear_output(wait=True)\n    \ndef unify_duplicated_shop_id(ids):    \n    global train,test\n    for pair in ids:\n        (origin, replacement) = pair\n        display_status('replace {0} with {1}'.format(origin, replacement))\n        train.loc[train.shop_id == origin, \"shop_id\"] = replacement\n        test.loc[test.shop_id == origin , \"shop_id\"] = replacement\n\ndef clean_and_expand_shop_data():\n    global shops\n    display_status('Cleaning Shop Data')\n    shops.loc[ shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"',\"shop_name\" ] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\n    shops[\"city\"] = shops.shop_name.str.split(\" \").map( lambda x: x[0] )\n    shops[\"category\"] = shops.shop_name.str.split(\" \").map( lambda x: x[1] )\n    shops.loc[shops.city == \"!\u042f\u043a\u0443\u0442\u0441\u043a\", \"city\"] = \"\u042f\u043a\u0443\u0442\u0441\u043a\"\n\n    \ndef name_correction(x):\n    x = x.lower()\n    x = x.partition('[')[0]\n    x = x.partition('(')[0]\n    x = re.sub('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', ' ', x)\n    x = x.replace('  ', ' ')\n    x = x.strip()\n    return x\n\ndef clean_text_and_recategorize():\n    global shops,item_categories,items\n    category = []\n    for cat in shops.category.unique():\n        print(cat, len(shops[shops.category == cat]) )\n        if len(shops[shops.category == cat]) > 4:\n            category.append(cat)\n\n    shops.category = shops.category.apply( lambda x: x if (x in category) else \"etc\" )\n\n    \n    shops[\"shop_category\"] = LabelEncoder().fit_transform( shops.category )\n    shops[\"shop_city\"] = LabelEncoder().fit_transform( shops.city )\n    shops = shops[[\"shop_id\", \"shop_category\", \"shop_city\"]]\n\n    item_categories[\"type_code\"] = item_categories.item_category_name.apply( lambda x: x.split(\" \")[0] ).astype(str)\n    item_categories.loc[ (item_categories.type_code == \"\u0418\u0433\u0440\u043e\u0432\u044b\u0435\")| (item_categories.type_code == \"\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b\"), \"category\" ] = \"\u0418\u0433\u0440\u044b\"\n\n    category = []\n    for cat in item_categories.type_code.unique():\n        print(cat, len(item_categories[item_categories.type_code == cat]))\n        if len(item_categories[item_categories.type_code == cat]) > 4: \n            category.append( cat )\n\n    item_categories.type_code = item_categories.type_code.apply(lambda x: x if (x in category) else \"etc\")\n\n    for cat in item_categories.type_code.unique():\n        print(cat, len(item_categories[item_categories.type_code == cat]))\n\n    item_categories.type_code = LabelEncoder().fit_transform(item_categories.type_code)\n    item_categories[\"split\"] = item_categories.item_category_name.apply(lambda x: x.split(\"-\"))\n    item_categories[\"subtype\"] = item_categories.split.apply(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\n    item_categories[\"subtype_code\"] = LabelEncoder().fit_transform( item_categories[\"subtype\"] )\n    item_categories = item_categories[[\"item_category_id\", \"subtype_code\", \"type_code\"]]\n\n    items[\"name1\"], items[\"name2\"] = items.item_name.str.split(\"[\", 1).str\n    items[\"name1\"], items[\"name3\"] = items.item_name.str.split(\"(\", 1).str\n\n    items[\"name2\"] = items.name2.str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', \" \").str.lower()\n    items[\"name3\"] = items.name3.str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', \" \").str.lower()\n    items = items.fillna('0')\n\n    items[\"item_name\"] = items[\"item_name\"].apply(lambda x: name_correction(x))\n    items.name2 = items.name2.apply( lambda x: x[:-1] if x !=\"0\" else \"0\")\n\n    items[\"type\"] = items.name2.apply(lambda x: x[0:8] if x.split(\" \")[0] == \"xbox\" else x.split(\" \")[0] )\n    items.loc[(items.type == \"x360\") | (items.type == \"xbox360\") | (items.type == \"xbox 360\") ,\"type\"] = \"xbox 360\"\n    items.loc[ items.type == \"\", \"type\"] = \"mac\"\n    items.type = items.type.apply( lambda x: x.replace(\" \", \"\") )\n    items.loc[ (items.type == 'pc' )| (items.type == 'p\u0441') | (items.type == \"pc\"), \"type\" ] = \"pc\"\n    items.loc[ items.type == '\u0440s3' , \"type\"] = \"ps3\"\n    #Add popular titles as separate type\n    populars = ['alien', 'adele', 'angry', 'anno','edition', 'ultimate', 'gold', 'premium', 'assassin', 'battlefield', \n            'call of duty', 'fifa', 'football','need for speed', 'pokemon',\n            'grand theft auto', 'fallout', 'star wars', 'uncharted','Kaspersky', 'Sims', 'medal of honor', '\u0424\u0438\u0440\u043c\u0435\u043d\u043d\u044b\u0439 \u043f\u0430\u043a\u0435\u0442 \u043c\u0430\u0439\u043a\u0430',\n            'mad max', 'metal gear', 'ea', 'diablo', 'warcraft', '\u0414\u043e\u0441\u0442\u0430\u0432\u043a\u0430','\u0412\u0435\u0434\u044c\u043c\u0430\u043a 3'\n           ]\n    index = [False] * len(train)\n    display_status('tagging popular items such as Star Wars')\n    for word in tqdm(populars):\n        idx_popular_item_sales = items['item_name'].str.contains(word, case=False)\n        items.loc[ idx_popular_item_sales , \"type\"] = word\n\n\n    group_sum = items.groupby([\"type\"]).agg({\"item_id\": \"count\"})\n    print( group_sum.reset_index() )\n    group_sum = group_sum.reset_index()\n\n\n    drop_cols = []\n    for cat in group_sum.type.unique():\n    #     print(group_sum.loc[(group_sum.type == cat), \"item_id\"].values[0])\n        if group_sum.loc[(group_sum.type == cat), \"item_id\"].values[0] <40:\n            drop_cols.append(cat)\n\n    items.name2 = items.name2.apply( lambda x: \"etc\" if (x in drop_cols) else x )\n    items = items.drop([\"type\"], axis = 1)\n\n\n    items.name2 = LabelEncoder().fit_transform(items.name2)\n    items.name3 = LabelEncoder().fit_transform(items.name3)\n\n    items.drop([\"item_name\", \"name1\"],axis = 1, inplace= True)\n    items.head()\n    \n\ndef lag_feature( df,lags, cols ):\n    for col in cols:\n        print(col)\n        tmp = df[[\"date_block_num\", \"shop_id\",\"item_id\",col ]]\n        for i in lags:\n            shifted = tmp.copy()\n            shifted.columns = [\"date_block_num\", \"shop_id\", \"item_id\", col + \"_lag_\"+str(i)]\n            shifted.date_block_num = shifted.date_block_num + i\n            df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df\n    \n\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nbasepath= '..\/input\/competitive-data-science-predict-future-sales\/'\n\nitems = pd.read_csv(basepath+\"items.csv\")\nitem_categories = pd.read_csv(basepath+\"item_categories.csv\")\nshops = pd.read_csv(basepath+\"shops.csv\")\ntrain = pd.read_csv( basepath+\"sales_train.csv\" )\n#train = train.sample(20000) #for quick Proof of Concept, sample 20000\ntest = pd.read_csv( basepath+\"test.csv\" )\n\nunify_duplicated_shop_id(ids=[(0,57),(1,58),(11,10),(40,39)])\nclean_and_expand_shop_data()\nclean_text_and_recategorize()\n\n#Now we will group the data by these 5 columns and some of their combination: \"date_block_num\", \"shop_id\",\"item_id\",'subtype_code','shop_city'\nts = time.time()\nagg_data = []\ncols  = [\"date_block_num\", \"shop_id\", \"item_id\"]\ndisplay_status('========| Create aggregated data(1\/8) |========= ~Time elapsed: {0} secs'.format(str(time.time()- ts)) )\nfor i in range(34):\n    sales = train[train.date_block_num == i]\n    agg_data.append( np.array(list( product( [i], sales.shop_id.unique(), sales.item_id.unique() ) ), dtype = np.int16) )\n\nagg_data = pd.DataFrame( np.vstack(agg_data), columns = cols )\nagg_data[\"date_block_num\"] = agg_data[\"date_block_num\"].astype(np.int8)\nagg_data[\"shop_id\"] = agg_data[\"shop_id\"].astype(np.int8)\nagg_data[\"item_id\"] = agg_data[\"item_id\"].astype(np.int16)\nagg_data.sort_values( cols, inplace = True )\n\n\ntrain[\"revenue\"] = train[\"item_cnt_day\"] * train[\"item_price\"]\n\ndisplay_status('========| Group Aggregate based on date, shop, item(2\/8) |========= ~Time elapsed: {0} secs'.format(str(time.time()- ts)) )\ngroup = train.groupby( [\"date_block_num\", \"shop_id\", \"item_id\"] ).agg( {\"item_cnt_day\": [\"sum\"]} )\ngroup.columns = [\"item_cnt_month\"]\ngroup.reset_index( inplace = True)\nagg_data = pd.merge( agg_data, group, on = cols, how = \"left\" )\nagg_data[\"item_cnt_month\"] = agg_data[\"item_cnt_month\"].fillna(0).clip(0,20).astype(np.float16)\n\n\ntest[\"date_block_num\"] = 34\ntest[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\ntest[\"shop_id\"] = test.shop_id.astype(np.int8)\ntest[\"item_id\"] = test.item_id.astype(np.int16)\n\n\nagg_data = pd.concat([agg_data, test.drop([\"ID\"],axis = 1)], ignore_index=True, sort=False, keys=cols)\nagg_data.fillna( 0, inplace = True )\nagg_data\n\n\ndisplay_status('========| Merge shops, items, categoriess (3\/8) |========= ~Time elapsed: {0} secs'.format(str(time.time()- ts)) )\nagg_data = pd.merge( agg_data, shops, on = [\"shop_id\"], how = \"left\" )\nagg_data = pd.merge(agg_data, items, on = [\"item_id\"], how = \"left\")\nagg_data = pd.merge( agg_data, item_categories, on = [\"item_category_id\"], how = \"left\" )\nagg_data[\"shop_city\"] = agg_data[\"shop_city\"].astype(np.int8)\nagg_data[\"shop_category\"] = agg_data[\"shop_category\"].astype(np.int8)\nagg_data[\"item_category_id\"] = agg_data[\"item_category_id\"].astype(np.int8)\nagg_data[\"subtype_code\"] = agg_data[\"subtype_code\"].astype(np.int8)\nagg_data[\"name2\"] = agg_data[\"name2\"].astype(np.int8)\nagg_data[\"name3\"] = agg_data[\"name3\"].astype(np.int16)\nagg_data[\"type_code\"] = agg_data[\"type_code\"].astype(np.int8)\ntime.time() - ts\n\n\n\nagg_data = lag_feature( agg_data, [1,2,3], [\"item_cnt_month\"] )\n\n\ndisplay_status('========| Group & merge based on date_block_num (4\/8) |========= ~Time elapsed: {0} secs'.format(str(time.time()- ts)) )\ngroup = agg_data.groupby( [\"date_block_num\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nagg_data = pd.merge(agg_data, group, on = [\"date_block_num\"], how = \"left\")\nagg_data.date_avg_item_cnt = agg_data[\"date_avg_item_cnt\"].astype(np.float16)\nagg_data = lag_feature( agg_data, [1], [\"date_avg_item_cnt\"] )\nagg_data.drop( [\"date_avg_item_cnt\"], axis = 1, inplace = True )\n\n\n\n\ndisplay_status('========| Group & merge based on [date_block_num, item_id] (4\/8) |========= ~Time elapsed: {0} secs'.format(str(time.time()- ts)) )\ngroup = agg_data.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nagg_data = pd.merge(agg_data, group, on=['date_block_num','item_id'], how='left')\nagg_data.date_item_avg_item_cnt = agg_data['date_item_avg_item_cnt'].astype(np.float16)\nagg_data = lag_feature(agg_data, [1,2,3], ['date_item_avg_item_cnt'])\nagg_data.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\n\n\n\ndisplay_status('========| Group & merge based on [date_block_num , shop_id] (5\/8) |========= ~Time elapsed: {0} secs'.format(str(time.time()- ts)) )\ngroup = agg_data.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_shop_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nagg_data = pd.merge(agg_data, group, on = [\"date_block_num\",\"shop_id\"], how = \"left\")\nagg_data.date_avg_item_cnt = agg_data[\"date_shop_avg_item_cnt\"].astype(np.float16)\nagg_data = lag_feature( agg_data, [1,2,3], [\"date_shop_avg_item_cnt\"] )\nagg_data.drop( [\"date_shop_avg_item_cnt\"], axis = 1, inplace = True )\n\n\n\ngroup = agg_data.groupby( [\"date_block_num\",\"shop_id\",\"item_id\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_shop_item_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nagg_data = pd.merge(agg_data, group, on = [\"date_block_num\",\"shop_id\",\"item_id\"], how = \"left\")\nagg_data.date_avg_item_cnt = agg_data[\"date_shop_item_avg_item_cnt\"].astype(np.float16)\nagg_data = lag_feature( agg_data, [1,2,3], [\"date_shop_item_avg_item_cnt\"] )\nagg_data.drop( [\"date_shop_item_avg_item_cnt\"], axis = 1, inplace = True )\n\n\n\n\ngroup = agg_data.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nagg_data = pd.merge(agg_data, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\nagg_data.date_shop_subtype_avg_item_cnt = agg_data['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nagg_data = lag_feature(agg_data, [1], ['date_shop_subtype_avg_item_cnt'])\nagg_data.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\n\n\n\ngroup = agg_data.groupby(['date_block_num', 'shop_city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_city_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nagg_data = pd.merge(agg_data, group, on=['date_block_num', \"shop_city\"], how='left')\nagg_data.date_city_avg_item_cnt = agg_data['date_city_avg_item_cnt'].astype(np.float16)\nagg_data = lag_feature(agg_data, [1], ['date_city_avg_item_cnt'])\nagg_data.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\n\n\ndisplay_status('========| This will take long time (6\/8) |========= ~Time elapsed: {0} secs'.format(str(time.time()- ts)) )\ngroup = agg_data.groupby(['date_block_num', 'item_id', 'shop_city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nagg_data = pd.merge(agg_data, group, on=['date_block_num', 'item_id', 'shop_city'], how='left')\nagg_data.date_item_city_avg_item_cnt = agg_data['date_item_city_avg_item_cnt'].astype(np.float16)\nagg_data = lag_feature(agg_data, [1], ['date_item_city_avg_item_cnt'])\nagg_data.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\n\n\n\n\ngroup = train.groupby( [\"item_id\"] ).agg({\"item_price\": [\"mean\"]})\ngroup.columns = [\"item_avg_item_price\"]\ngroup.reset_index(inplace = True)\n\nagg_data = agg_data.merge( group, on = [\"item_id\"], how = \"left\" )\nagg_data[\"item_avg_item_price\"] = agg_data.item_avg_item_price.astype(np.float16)\n\n\ngroup = train.groupby( [\"date_block_num\",\"item_id\"] ).agg( {\"item_price\": [\"mean\"]} )\ngroup.columns = [\"date_item_avg_item_price\"]\ngroup.reset_index(inplace = True)\n\nagg_data = agg_data.merge(group, on = [\"date_block_num\",\"item_id\"], how = \"left\")\nagg_data[\"date_item_avg_item_price\"] = agg_data.date_item_avg_item_price.astype(np.float16)\nlags = [1, 2, 3]\nagg_data = lag_feature( agg_data, lags, [\"date_item_avg_item_price\"] )\nfor i in lags:\n    agg_data[\"delta_price_lag_\" + str(i) ] = (agg_data[\"date_item_avg_item_price_lag_\" + str(i)]- agg_data[\"item_avg_item_price\"] )\/ agg_data[\"item_avg_item_price\"]\n\ndef select_trends(row) :\n    for i in lags:\n        if row[\"delta_price_lag_\" + str(i)]:\n            return row[\"delta_price_lag_\" + str(i)]\n    return 0\n\nagg_data[\"delta_price_lag\"] = agg_data.apply(select_trends, axis = 1)\nagg_data[\"delta_price_lag\"] = agg_data.delta_price_lag.astype( np.float16 )\nagg_data[\"delta_price_lag\"].fillna( 0 ,inplace = True)\n\nfeatures_to_drop = [\"item_avg_item_price\", \"date_item_avg_item_price\"]\nfor i in lags:\n    features_to_drop.append(\"date_item_avg_item_price_lag_\" + str(i) )\n    features_to_drop.append(\"delta_price_lag_\" + str(i) )\nagg_data.drop(features_to_drop, axis = 1, inplace = True)\n\n\n\ndisplay_status('========| This will take long time too (7\/8) |========= ~Time elapsed: {0} secs'.format(str(time.time()- ts)) )\ngroup = train.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"revenue\": [\"sum\"] })\ngroup.columns = [\"date_shop_revenue\"]\ngroup.reset_index(inplace = True)\n\nagg_data = agg_data.merge( group , on = [\"date_block_num\", \"shop_id\"], how = \"left\" )\nagg_data['date_shop_revenue'] = agg_data['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby([\"shop_id\"]).agg({ \"date_block_num\":[\"mean\"] })\ngroup.columns = [\"shop_avg_revenue\"]\ngroup.reset_index(inplace = True )\n\nagg_data = agg_data.merge( group, on = [\"shop_id\"], how = \"left\" )\nagg_data[\"shop_avg_revenue\"] = agg_data.shop_avg_revenue.astype(np.float32)\nagg_data[\"delta_revenue\"] = (agg_data['date_shop_revenue'] - agg_data['shop_avg_revenue']) \/ agg_data['shop_avg_revenue']\nagg_data[\"delta_revenue\"] = agg_data[\"delta_revenue\"]. astype(np.float32)\n\nagg_data = lag_feature(agg_data, [1], [\"delta_revenue\"])\nagg_data[\"delta_revenue_lag_1\"] = agg_data[\"delta_revenue_lag_1\"].astype(np.float32)\nagg_data.drop( [\"date_shop_revenue\", \"shop_avg_revenue\", \"delta_revenue\"] ,axis = 1, inplace = True)\n\n\n\n\ndisplay_status('========| Last (8\/8) |========= ~Time elapsed: {0} secs'.format(str(time.time()- ts)) )\n# Adding number of days for each month\nagg_data[\"month\"] = agg_data[\"date_block_num\"] % 12\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nagg_data[\"days\"] = agg_data[\"month\"].map(days).astype(np.int8)\n\n\nagg_data[\"item_shop_first_sale\"] = agg_data[\"date_block_num\"] - agg_data.groupby([\"item_id\",\"shop_id\"])[\"date_block_num\"].transform('min')\nagg_data[\"item_first_sale\"] = agg_data[\"date_block_num\"] - agg_data.groupby([\"item_id\"])[\"date_block_num\"].transform('min')\n\n\nagg_data = agg_data[agg_data[\"date_block_num\"] > 3]\ndisplay_status('========| Completed (8\/8) |========= ~Time elapsed: {0} secs'.format(str(time.time()- ts)) )\n\n\n\n\n\n#Adding approx number of holidays 2013-2015 in Russia for each month\n            # ([Jan,Feb, ... ,            Dec, ])\ndays = pd.Series([10,3,2,1,7,2,  1,0,2,1,1,2])\nagg_data[\"holiday_num\"] = agg_data[\"month\"].map(days).astype(np.int8)\n\n#Approx number of Russia holidays next month 2013-2015 for each month\n               # ([Feb, Mar, ... ,     Dec,Jan])\ndays = pd.Series([3,2,1,7,2,  1,0,2,1,1,2,10])\nagg_data[\"next_month_holiday_num\"] = agg_data[\"month\"].map(days).astype(np.int8)\n\n\n#Approx number of Russia holidays next month 2013-2015\n#month ([Jan,  ... , Dec])\ndays = pd.Series(['winter','etc','spring','etc','may',  'summer','summer','summer','etc','winter','winter','winter'])\nagg_data[\"vacation_period\"] = agg_data[\"month\"].map(days).astype('category')\n#mean encoding\nagg_data['vacation_period'] = agg_data.groupby('vacation_period')['item_cnt_month'].transform('mean')\n\nagg_data.to_csv('agg_data.csv',index=False,header=True)\nagg_data = reduce_mem_usage(agg_data)\n\n\n\n\nX_train = agg_data[agg_data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\ny_train = agg_data[agg_data.date_block_num < 33]['item_cnt_month']\nX_val = agg_data[agg_data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\ny_val = agg_data[agg_data.date_block_num == 33]['item_cnt_month']\n\nX_test = agg_data[agg_data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n\ndel agg_data\nimport gc\ngc.collect()\n\nX_train.fillna(0,inplace=True)\nX_val.fillna(0,inplace=True)\nX_test.fillna(0,inplace=True)","d937c3fe":"est = 100\nmax_dept = 10\n\nxgb_filename = 'xgb_pred_'+str(est)+'_'+str(max_dept)\n\nxgb = XGBRegressor(\n    max_depth=max_dept,\n    n_estimators=est,\n    min_child_weight=0.5, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.1,\n#     tree_method='gpu_hist',\n    seed=42)\n\nxgb.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, y_train), (X_val, y_val)], \n    verbose=True, \n    early_stopping_rounds = 20)\n\npickle.dump(xgb, open(xgb_filename+'.model', 'wb'))\n\n\ny_test = xgb.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": y_test\n})\nsubmission.to_csv(xgb_filename+'.csv', index=False)","11b0bc1f":"est = 30\nmax_depth = 10\n\n\nrf_pred_filename = 'rf_pred_'+str(est)+'_'+str(max_dept)\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=est,max_depth=max_depth, random_state=42, oob_score=True, verbose=10, n_jobs=-1)\nrf.fit(X_train, y_train)\n\npickle.dump(rf, open(rf_pred_filename+'.model', 'wb'))\n\ny_test = rf.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": y_test\n})\nsubmission.to_csv(rf_pred_filename+'.csv', index=False)","93e9f3fc":"from sklearn.linear_model import LinearRegression\nmeta_train = pd.DataFrame({'ID':X_train.index,'pred_1':xgb.predict(X_train).clip(0, 20),'pred_2':rf.predict(X_train).clip(0, 20)})\ny_meta_train = y_train\nmeta_model = LinearRegression()\nmeta_model.fit(meta_train[['pred_1','pred_2']],y_meta_train)\n\n\n\n#Eval Meta Model on val data\nmeta_val = pd.DataFrame({'ID':X_val.index,'pred_1':xgb.predict(X_val).clip(0, 20),'pred_2':rf.predict(X_val).clip(0, 20)})\nX_meta_val, y_meta_val = meta_val[['pred_1','pred_2']],y_val\nprint(meta_model.score(X_meta_val,y_meta_val))\n\n\nfilename = 'metamodel.model'\npickle.dump(meta_model, open(filename, 'wb'))\n\n\n#Finally predict!\ntry:\n    if xgb_filename and rf_filename:\n        pass\nexcept Exception:\n    xgb_filename = 'xgb_pred_100_10'\n    rf_filename = 'rf_pred_30_10'\n    \npred_1 = pd.read_csv(xgb_filename+'.csv')\npred_2 = pd.read_csv(rf_filename+'.csv')\nmeta_test = pd.DataFrame({'ID':pred_1.ID,'pred_1':pred_1.item_cnt_month,'pred_2':pred_2.item_cnt_month})\nX_meta_test = meta_test[['pred_1','pred_2']]\n\n\n\nbasepath= '..\/input\/competitive-data-science-predict-future-sales\/'\ntest = pd.read_csv( basepath+\"test.csv\" )\n\ny_test = meta_model.predict(X_meta_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": y_test\n})\nsubmission.to_csv('meta_model_pred.csv', index=False)\nsubmission.to_csv('submission.csv', index=False)\nprint('final prediction submission.csv file has been generated.')\nsubmission","bc1f17e4":"# > Run the code below to (re)create model & predict \nThanks to [Schrodinger](https:\/\/www.kaggle.com\/lonewolf45) for the excellent [notebook](https:\/\/www.kaggle.com\/lonewolf45\/coursera-final-project) that performs extraordinary ETL & XGB modelling. I learn a lot from this, parts of the source code there are reused and refactored to fit the author's coding style. \n\nThe EDA and serialized model version is [here](https:\/\/www.kaggle.com\/rrrrrikimaru\/simple-e-to-e-eda-to-ensemble)\n\nPlease expect to spend about 2 hours.\n### 1. ETL","bb9bd6a9":"### 4. Ensemble using Linear Regression","3373f956":"### 3. Random Forest Model","cb050bc1":"# Thank you for reading till the end.\nAny feedback is very welcomed. Please vote if you like it","bb0e3e1a":"### 2. XGB Model"}}