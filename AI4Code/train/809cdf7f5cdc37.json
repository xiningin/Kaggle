{"cell_type":{"1cbb45a2":"code","387be6bd":"code","51f2a27a":"code","a4c56958":"code","79668477":"code","ed38cde2":"code","3f3c7e13":"code","1ead6e1a":"code","acb151cb":"code","da852be2":"code","8dc39d4a":"code","7a85447e":"code","9dc44cc5":"code","ad7a52f5":"code","fa5b7500":"code","8abf7b32":"code","b6743010":"code","08f60e5f":"markdown","4955eb1f":"markdown","5e862bfd":"markdown"},"source":{"1cbb45a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","387be6bd":"dataframe = pd.read_csv(\"\/kaggle\/input\/gender-classification\/Transformed Data Set - Sheet1.csv\")\ndataframe.info()","51f2a27a":"dataframe.rename(columns={'Favorite Color' :'FavoriteColor', 'Favorite Music Genre':'FavoriteMusicGenre', \n                          'Favorite Beverage':'FavoriteBeverage', 'Favorite Soft Drink':'FavoriteSoftDrink'}, inplace=True)","a4c56958":"'''\n#Method 1 when we have all the columns as type object\nfrom sklearn.preprocessing import LabelEncoder\ndataframe.apply(LabelEncoder().fit_transform)\n'''","79668477":"\n#Fetch features of type Object\nobjFeatures = dataframe.select_dtypes(include=\"object\").columns\n\n#Iterate a loop for features of type object\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\nfor feat in objFeatures:\n    dataframe[feat] = le.fit_transform(dataframe[feat].astype(str))\n    \ndataframe.info()\n","ed38cde2":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.countplot('FavoriteColor', hue='Gender', data=dataframe, palette='Blues')","3f3c7e13":"dataframe.info()","1ead6e1a":"X = dataframe.drop(['Gender'], axis = 1)\ny = dataframe.Gender","acb151cb":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)","da852be2":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\n\ndef clf_scores(clf, y_predicted):\n    # Accuracy\n    acc_train = clf.score(X_train, y_train)*100\n    acc_test = clf.score(X_test, y_test)*100\n    \n    roc = roc_auc_score(y_test, y_predicted)*100 \n    tn, fp, fn, tp = confusion_matrix(y_test, y_predicted).ravel()\n    cm = confusion_matrix(y_test, y_predicted)\n    correct = tp + tn\n    incorrect = fp + fn\n    \n    return acc_train, acc_test, roc, correct, incorrect, cm\n","8dc39d4a":"#1. Logistic regression\n\nfrom sklearn.linear_model import LogisticRegression\nclf_lr = LogisticRegression()\nclf_lr.fit(X_train, y_train)\n\nY_pred_lr = clf_lr.predict(X_test)\nprint(clf_scores(clf_lr, Y_pred_lr))","7a85447e":"#2. KNN\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclf_knn = KNeighborsClassifier(n_neighbors=3)\nclf_knn.fit(X_train, y_train)\n\nY_pred_knn = clf_knn.predict(X_test)\nprint(clf_scores(clf_knn, Y_pred_knn))","9dc44cc5":"#3. Naive Bayes\n\nfrom sklearn.naive_bayes import GaussianNB\nclf_gnb = GaussianNB()\nclf_gnb.fit(X_train, y_train)\n\nY_pred_gnb = clf_gnb.predict(X_test)\nprint(clf_scores(clf_gnb, Y_pred_gnb))","ad7a52f5":"#4. SVM \nfrom sklearn.svm import SVC\n\nclf_svm = SVC()\nclf_svm.fit(X_train, y_train)\n\nY_pred_svm = clf_svm.predict(X_test)\nprint(clf_scores(clf_svm, Y_pred_svm))","fa5b7500":"#5. Decision tree\nfrom sklearn.tree import DecisionTreeClassifier\n\nclf_dt = DecisionTreeClassifier(random_state=0)\nclf_dt.fit(X_train, y_train)\nclf_dt.fit(X_train, y_train)\n\nY_pred_dt = clf_dt.predict(X_test)\nprint(clf_scores(clf_dt, Y_pred_dt))","8abf7b32":"#6. Radom forest classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf_rfc = RandomForestClassifier(max_depth=10, random_state=42)\nclf_rfc.fit(X_train, y_train)\n\nY_pred_rfc = clf_rfc.predict(X_test)\nprint(clf_scores(clf_rfc, Y_pred_rfc))","b6743010":"#7. Gradient boosting classifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nclf_gbc = GradientBoostingClassifier(random_state=42)\nclf_gbc.fit(X_train, y_train)\n\nY_pred_gbc = clf_gbc.predict(X_test)\nprint(clf_scores(clf_gbc, Y_pred_gbc))","08f60e5f":"Let's first rename the column names and remove bla","4955eb1f":"As we see that all the columns are of type object, so we will have to bring them all in type numeric. We will use Label Encoding Technique.","5e862bfd":"Method 2 - when we have mixed type columns. Then fetch the list of column names of type object type programmatically  and then Label Encode them"}}