{"cell_type":{"a762ce1b":"code","eb7623f5":"code","445ba00a":"code","49c0d0e4":"code","3906adc7":"code","3ef330ab":"code","3def1a48":"code","2750bc64":"code","ef3084a0":"code","51c19fda":"code","c535ffb7":"code","e6b555b9":"code","8c623a68":"code","cc7143bd":"code","ded55925":"code","dce3a197":"code","a7ecc21c":"code","bdee5e61":"code","773a64a1":"code","a619b99b":"code","13db8d2c":"code","26ee15d7":"code","4a4751d2":"code","ba3edda9":"code","6fa4bc7b":"code","824f3970":"code","c603510f":"code","8a73d292":"code","eb1c10c7":"code","2b995a4e":"code","ff9d4c93":"code","d496bae1":"code","f062b478":"code","886060d3":"code","12093f4c":"code","cab39c5e":"code","4238d634":"code","88a78392":"code","23417a81":"code","a46924c3":"code","490b1fe8":"code","cf3f18bd":"code","3a411b95":"code","3225db84":"code","c7b02fc3":"code","7b5a105b":"code","6a5e3146":"code","d16049a9":"code","af9e9dfa":"code","928a8007":"code","64e7b910":"code","ffc3724f":"code","3cac5bef":"code","95c2137a":"code","28a1c0e5":"code","73a5df16":"code","204589b4":"code","e0f0bc17":"code","4b3a6126":"code","c73986b0":"code","d82673a5":"code","f0e640f6":"markdown","4c60c3ee":"markdown","7933602f":"markdown","685130d8":"markdown","beeef949":"markdown","b0d49d6a":"markdown","801768b7":"markdown","573e0bd9":"markdown","b78512fe":"markdown","3cd55923":"markdown","165d49a3":"markdown","d03e3472":"markdown","a5eaf9dd":"markdown","3723cef0":"markdown","59d8c906":"markdown","793ae3b2":"markdown","9c866b1c":"markdown","30b80cd3":"markdown","6dab86b1":"markdown","25caf3b3":"markdown","30d751e0":"markdown","34a8da31":"markdown","57fceaaa":"markdown","599a08d0":"markdown","83d325d6":"markdown","4814f9f8":"markdown","246238d1":"markdown","e9cfc70d":"markdown","fed3c177":"markdown","b1d99e36":"markdown","eeacc402":"markdown","838f322c":"markdown","dfaf3e60":"markdown","520ca269":"markdown","e45a650b":"markdown","87ae96b5":"markdown","8258580d":"markdown","63ecc91f":"markdown","469c5e81":"markdown"},"source":{"a762ce1b":"# File Path\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","eb7623f5":"#Loading libraries\nimport numpy as np # provides a high-performance multidimensional array and tools for its manipulation\nimport pandas as pd # for data munging, it contains manipulation tools designed to make data analysis fast and easy\nimport re # Regular Expressions - useful for extracting information from text \nimport nltk # Natural Language Tool Kit for symbolic and statistical natural language processing\nimport spacy # processing and understanding large volumes of text\nimport string # String module contains some constants, utility function, and classes for string manipulation\nimport re\n\n# For viz\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\npd.options.mode.chained_assignment = None","445ba00a":"#Loading File\ndf = pd.read_csv('\/kaggle\/input\/coronavirus-tweets\/Corona_tweets.csv',encoding='latin1')","49c0d0e4":"#Shape of dataframe\nprint(\" Shape of training dataframe: \", df.shape)","3906adc7":"# Drop duplicates\ndf.drop_duplicates()\nprint(\" Shape of dataframe after dropping duplicates: \", df.shape)","3ef330ab":"#Null values\n\nnull= df.isnull().sum().sort_values(ascending=False)\ntotal =df.shape[0]\npercent_missing= (df.isnull().sum()\/total).sort_values(ascending=False)\n\nmissing_data= pd.concat([null, percent_missing], axis=1, keys=['Total missing', 'Percent missing'])\n\nmissing_data.reset_index(inplace=True)\nmissing_data= missing_data.rename(columns= { \"index\": \" column name\"})\n \nprint (\"Null Values in each column:\\n\", missing_data)\n","3def1a48":"!pip install vaderSentiment","2750bc64":"import vaderSentiment\n# calling SentimentIntensityAnalyzer object\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nanalyser = SentimentIntensityAnalyzer()","ef3084a0":"# or use nltk\n\n\n#import nltk\n#nltk.download('vader_lexicon')\n#from nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n#analyser = SentimentIntensityAnalyzer()","51c19fda":"# Using polarity scores for knowing the polarity of each text\ndef sentiment_analyzer_score(sentence):\n    score = analyser.polarity_scores(sentence)\n    print(\"{:-<40} {}\".format(sentence, str(score)))","c535ffb7":"#testing the function\ntweet  = \"I would love to watch the magic show again\"\ntweet2 = \"What the hell they have made. Pathetic!\"\ntweet3 = \" I do not know what to do\"  \nprint (sentiment_analyzer_score(tweet))\nprint (sentiment_analyzer_score(tweet2))\nprint (sentiment_analyzer_score(tweet3))","e6b555b9":"tweet  = \"I like the fact that monsoon is over\"\ntweet2 = \"I LIKE the fact that monsoon is over\"","8c623a68":"print (sentiment_analyzer_score(tweet))\nprint (sentiment_analyzer_score(tweet2))","cc7143bd":"tweet  = \"What is wrong with you\"\ntweet2  = \"What is wrong with you?\"\ntweet3 = \"What is wrong with you??\"","ded55925":"print (sentiment_analyzer_score(tweet))\nprint (sentiment_analyzer_score(tweet2))\nprint (sentiment_analyzer_score(tweet3))","dce3a197":"tweet  = \"He is good but his mother is irritating\"\ntweet2 = \"The thai curry was bad, however pasta was delicious\"\ntweet3 = \"The thai curry was ok and pasta was delicious\"","a7ecc21c":"print (sentiment_analyzer_score(tweet))\nprint (sentiment_analyzer_score(tweet2))\nprint (sentiment_analyzer_score(tweet3))","bdee5e61":"tweet = \"Real Madrid's game play was good last night.\"\ntweet2 = \"Real Madrid's game play was extremely good last night.\"\ntweet3 = \"Real Madrid's game play was somewhat good last night.\"\ntweet4 = \"Real Madrid's game play was terrible last night.\"\ntweet5 = \"Real Madrid's game play was awfully terrible last night.\"","773a64a1":"print (sentiment_analyzer_score(tweet))\nprint (sentiment_analyzer_score(tweet2))\nprint (sentiment_analyzer_score(tweet3))\nprint (sentiment_analyzer_score(tweet4))\nprint (sentiment_analyzer_score(tweet5))","a619b99b":"tweet = \" What a fine day I am having today\"\ntweet2 = \" What a fine day I am having today :-)\"\ntweet3 = \" What a fine day I am having today :-) :-)\"","13db8d2c":"print (sentiment_analyzer_score(tweet))\nprint (sentiment_analyzer_score(tweet2))\nprint (sentiment_analyzer_score(tweet3))","26ee15d7":"tweet = \"I love the team and how they played last night\"\ntweet2 = \"I love the team and how they played last night \ud83d\udc98\"\ntweet3 = \"I love the team and how they played last night \ud83d\ude01\"","4a4751d2":"print (sentiment_analyzer_score(tweet))\nprint (sentiment_analyzer_score(tweet2))\nprint (sentiment_analyzer_score(tweet3))","ba3edda9":"tweet = \"I am laughing like crazy\"\ntweet2 = \"I am laughing like crazy lmao\"\ntweet3 = \"I am laughing like crazy lol\"","6fa4bc7b":"print (sentiment_analyzer_score(tweet))\nprint (sentiment_analyzer_score(tweet2))\nprint (sentiment_analyzer_score(tweet3))","824f3970":"tweet = \"He wasn't very good at the play\"\ntweet2 = \"He was not very good at the play\"","c603510f":"print (sentiment_analyzer_score(tweet))\nprint (sentiment_analyzer_score(tweet2))\n","8a73d292":"tweet = \"He is kinda bored\"\ntweet2 = \"He is friggin bored\"","eb1c10c7":"print (sentiment_analyzer_score(tweet))\nprint (sentiment_analyzer_score(tweet2))","2b995a4e":"from nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\nwords_descriptions = df['text'].apply(tokenizer.tokenize)\nwords_descriptions.head()","ff9d4c93":"all_words = [word for tokens in words_descriptions for word in tokens]\ndf['description_lengths']= [len(tokens) for tokens in words_descriptions]\nVOCAB = sorted(list(set(all_words)))\nprint(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))","d496bae1":"# Checking most common words\nfrom collections import Counter\ncount_all_words = Counter(all_words)\ncount_all_words.most_common(100)","f062b478":"#### 1-gram tokenizer\nexample = 'The quick brown fox jumps over the lazy dog.'\n\n# remove the dots and make all words lower case\nclean_example = re.sub(r'\\.', '', example)\nprint(clean_example.split())","886060d3":"# 2-gram tokenizer\n\nexample = 'The quick brown fox jumps over the lazy dog.'\n\nwithout_first = example.split()[1:]\nwithout_last = example.split()[:-1]\n\nlist(zip(without_last, without_first))","12093f4c":"print (sentiment_analyzer_score(tweet2))","cab39c5e":"df['scores'] = df['text'].apply(lambda review: analyser.polarity_scores(review))\n\ndf.head()","4238d634":"df['compound']  = df['scores'].apply(lambda score_dict: score_dict['compound'])\n\ndf.head()","88a78392":"def Sentimnt(x):\n    if x>= 0.05:\n        return \"Positive\"\n    elif x<= -0.05:\n        return \"Negative\"\n    else:\n        return \"Neutral\"\n#df['Sentiment'] = df['compound'].apply(lambda c: 'positive' if c >=0.00  else 'negative')\ndf['Sentiment'] = df['compound'].apply(Sentimnt)\n\n\ndf.head()","23417a81":"var1 = df.groupby('Sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\nsns.set_style(\"white\")\nsns.set_palette(\"Set2\")\nvar1.style.background_gradient()","a46924c3":"plt.figure(figsize=(12,6))\nsns.countplot(x='Sentiment',data=df)","490b1fe8":"fig = go.Figure(go.Funnelarea(\n    text =var1.Sentiment,\n    values = var1.text,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Sentiment Distribution\"}\n    ))\nfig.show()","cf3f18bd":"df['temp_list'] = df['text'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in df['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\n#temp.style.background_gradient(cmap='Blues')","3a411b95":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Selected Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","3225db84":"# Tree of the most common words\nfig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","c7b02fc3":"comment_words = '' \nstopwords = set(STOPWORDS) \n  \n# iterate through the csv file \nfor val in df.text: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","7b5a105b":"comment_words = '' \nstopwords = set(STOPWORDS) \n  \ndf_positive = df[df[\"Sentiment\"]== \"Positive\"] \n# iterate through the csv file \nfor val in df_positive.text: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = \"green\") \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","6a5e3146":"comment_words = '' \nstopwords = set(STOPWORDS) \n  \ndf_negative = df[df[\"Sentiment\"]== \"Negative\"] \n# iterate through the csv file \nfor val in df_negative.text: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = \"red\") \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","d16049a9":"comment_words = '' \nstopwords = set(STOPWORDS) \n  \ndf_neutral = df[df[\"Sentiment\"]== \"Neutral\"] \n# iterate through the csv file \nfor val in df_positive.text: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = \"yellow\") \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","af9e9dfa":"del df_neutral\ndel df_positive\ndel df_negative","928a8007":"import warnings\nwarnings.filterwarnings('ignore')\nfrom textblob import TextBlob, Word, Blobber","64e7b910":"tweet = \"I would love to watch the magic show again\"\nTextBlob(tweet).sentiment ","ffc3724f":"# Applying on dataset\ndf['TB_score']= df.text.apply(lambda x: TextBlob(x).sentiment)\ndf.head()","3cac5bef":"df['TB_sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment[0])\ndf.head()","95c2137a":"!pip install nrclex","28a1c0e5":"from nrclex import NRCLex","73a5df16":"tweet = NRCLex('Good work to the team')\n#Return affect dictionary\nprint(tweet.affect_dict)\n#Return raw emotional counts\nprint(\"\\n\",tweet.raw_emotion_scores)\n#Return highest emotions\nprint(\"\\n\", tweet.top_emotions)\n#Return affect frequencies\nprint(\"\\n\",tweet.affect_frequencies)","204589b4":"text = NRCLex(\"Congratulations \")\n# Getting top emotions\nprint(\"\\n\", text.top_emotions)\n# Getting the top most emotion\nprint(\"\\n\", text.top_emotions[0][0])\n# Getting the top most emotion score\nprint(\"\\n\", text.top_emotions[0][1])","e0f0bc17":"text = NRCLex(\"We can do it \")\n# Getting top emotions\nprint(\"\\n\", text.top_emotions)\n# Getting the top most emotion\nprint(\"\\n\", text.top_emotions[0][0])\n# Getting the top most emotion score\nprint(\"\\n\", text.top_emotions[0][1])","4b3a6126":"def emotion(x):\n    text = NRCLex(x)\n    if text.top_emotions[0][1] == 0.0:\n        return \"No emotion\"\n    else:\n        return text.top_emotions[0][0]\ndf['Emotion'] = df['text'].apply(emotion)\ndf.head()","c73986b0":"\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom math import log10\n\ndf_chart = df[df.Emotion != \"No emotion\"]\nlabels = df_chart.Emotion.value_counts().index.tolist()\ndata = df_chart.Emotion.value_counts()\n#number of data points\nn = len(data)\n#find max value for full ring\nk = 10 ** int(log10(max(data)))\nm = k * (1 + max(data) \/\/ k)\n\n#radius of donut chart\nr = 1.5\n#calculate width of each ring\nw = r \/ n \n\n#create colors along a chosen colormap\ncolors = [cm.terrain(i \/ n) for i in range(n)]\n\n#create figure, axis\nfig, ax = plt.subplots()\nax.axis(\"equal\")\n\n#create rings of donut chart\nfor i in range(n):\n    #hide labels in segments with textprops: alpha = 0 - transparent, alpha = 1 - visible\n    innerring, _ = ax.pie([m - data[i], data[i]], radius = r - i * w, startangle = 90, labels = [\"\", labels[i]], labeldistance = 1 - 1 \/ (1.5 * (n - i)), textprops = {\"alpha\": 0}, colors = [\"white\", colors[i]])\n    plt.setp(innerring, width = w, edgecolor = \"white\")\n\nplt.legend()\nplt.show()","d82673a5":"b = df_chart.Emotion.value_counts().index.tolist()\na = df_chart.Emotion.value_counts(normalize = True).tolist()\nrow = pd.DataFrame({'scenario' : []})\nrow[\"scenario\"] = b\nrow[\"Percentage\"] = a\nfig = px.treemap(row, path= [\"scenario\"], values=\"Percentage\",title='Tree of Emotions')\nfig.show()","f0e640f6":"#### Here neg (negative), neu (neutral), and pos (positive) represent the proportion of text falling under each category and this proportion will always sum up to 1 (e.g., 0.656 + 0.344 in first tweet).\n\n#### Compound score is what reflects the overall score. It is sum of all lexicon ratings which is normalized between -1 (most extreme negative) and +1 (most extreme poistive). As per the scoring document ([link](https:\/\/github.com\/cjhutto\/vaderSentiment#about-the-scoring)) it is called as 'normalized, weighted composite score.'\n\n#### The typical thresholds standardized for classifying sentences as positve, negative, neutral are: \n#### 1) positive sentiment: compound score >= 0.05\n#### 2) neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n#### 3) negative sentiment: compound score <= -0.05  \n\n#### However, they can be adjusted for highly positive, positive, neutral, negative and highly negative classes as per the requirements.","4c60c3ee":"#### 4) Degree modifiers\nAs the name suggests they intensify the degree in positive or negative manner as per the use.","7933602f":"Contents:\n\n* [1. Importing data & libraries](#1)\n* [2. Lexicon based models](#2)\n* [3. VADER](#3)\n* [4. Data Preprocessing](#4)\n* [5. Model](#5)\n* [6. TextBlob Model](#6)\n* [7. Emotions in data](#7)","685130d8":"#### There are primarily 3 ways:\n#### 1) Rule-based methods\n#### 2) Feature-based methods\n#### 3) Embedding-based methods\n\n#### Rule based are the methods which involve set of manually crafted rules to identify subjectivity, polarity, or the subject of an opinion. These involve techniques like Stemming, tokenization, part-of-speech tagging, parsing, Lexicons (i.e. lists of words and expressions). Generally, two lists of polarized words (e.g. negative words such as bad, worst, ugly, etc and positive words such as good, best, beautiful, etc) are prepared. The number of positive and negative words that appear in a given text are counted. If the number of positive word appearances is greater than the number of negative word appearances, the system returns a positive sentiment, and vice versa. If the numbers are even, the system will return a neutral sentiment. Methods involve\n[AFINN](https:\/\/github.com\/fnielsen\/afinn), [Bing Liu's lexicon](https:\/\/www.cs.uic.edu\/~liub\/FBS\/sentiment-analysis.html), [MPQA subjectivity lexicon](http:\/\/mpqa.cs.pitt.edu\/lexicons\/subj_lexicon\/), [SentiWordNet](http:\/\/sentiwordnet.isti.cnr.it\/http:\/\/), [TextBlob](https:\/\/textblob.readthedocs.io\/en\/dev\/) & [VADER](https:\/\/github.com\/cjhutto\/vaderSentiment)\n\n#### Since rule-based are generally naive in nature as don't take into account how words are combined in a sequence. Hence, feature-based methods relying on ML techniques like SVM, Decision trees, etc. are used where sentiment analysis is modeled as a classification problem. The first step in a machine learning text classifier is to transform the text extraction or text vectorization, and the classical approach has been bag-of-words or bag-of-ngrams with their frequency. More recently, new feature extraction techniques have been applied based on word embeddings (also known as word vectors). This kind of representations makes it possible for words with similar meaning to have a similar representation, which can improve the performance of classifiers.\n\n#### Last is embedding based which involve [FastText](https:\/\/fasttext.cc\/docs\/en\/supervised-tutorial.html) & [Flair](https:\/\/github.com\/flairNLP\/flair\/)","beeef949":"There are many other Emotion Lexicons like [EmoWordNet](https:\/\/www.aclweb.org\/anthology\/S18-2009.pdf), [EmoTxt](https:\/\/arxiv.org\/ftp\/arxiv\/papers\/1708\/1708.03892.pdf) & [DepecheMood++](https:\/\/arxiv.org\/pdf\/1810.03660v1.pdf) which can be looked into.\n","b0d49d6a":"#### 5) Use of emoticons","801768b7":"<a id=\"5\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>5. Model<\/b><\/font><br>","573e0bd9":"## Let's discuss Vader algorithm first before preprocessing our file","b78512fe":"#### 7) Slangs ","3cd55923":"<font size=\"+2\" color=\"Green\"><b>Please Upvote if you like the work<\/b><\/font>","165d49a3":"#### The above output shows that the polarity of the sentence is 0.7, indicating that the sentiment is positive. Polarity is of 'float' type and lies in the range of -1,1, where 1 means a high positive sentiment, and -1 means a high negative sentiment.\n\n#### The output also prints subjectivity of the text which is 0.825 in our example. Subjectivity is also of 'float' type and lies in the range of 0,1. The value closer to 1 indicates that the sentence is mostly a public opinion and not a factual piece of information and vice versa. ","d03e3472":"## Future work: \n   1) Working on other (richer) emotion lexicons \\\n   2) Lexicon based models vs ML models vs Deep Learning Models \n","a5eaf9dd":"<a id=\"3\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3. VADER Sentiment Analysis<\/b><\/font><br>","3723cef0":"### Most of the problems in actual practice are related to unsupervised learning, which means the label is absent (Positive\/Negative). Majority of times companies do not find time to label it manually and hence settle for lexicon based models like Vader. Some, have built their own lexicons related to their field like Retail, News, etc. This notebook demonstrates lexicon based problem tackling.","59d8c906":"### About this notebook\n\n## This notebook is a part of Series \"[All about NLP](https:\/\/www.kaggle.com\/datatattle\/all-about-nlp)\" and will cover lexicon based Models\n\n\n![](https:\/\/static1.squarespace.com\/static\/5daddb33ee92bf44231c2fef\/t\/5f0387d2f6724b5987a29311\/1594066902743\/natural%2Blanguage%2Bprocessing%2Bin%2Bhealthcare%2B-%2Bforesee%2Bmedical.gif?format=1500w)\n![](https:\/\/miro.medium.com\/proxy\/1*_JW1JaMpK_fVGld8pd1_JQ.gif)\n","793ae3b2":"#### The compound score increases by 13% just by capitalization","9c866b1c":"<a id=\"2\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>2. Lexicon based Methods<\/b><\/font><br>","30b80cd3":"<font size=+4 color=\"Black\"><center><b>Lexicon based Sentiment Analysis<\/b><\/center><\/font>\n<font size=-1 color=\"Black\"><center><b>*Series: All about NLP by Data Tattle <\/b><\/right><\/font>","6dab86b1":"#### Applying VADER","25caf3b3":"### VADER sentiment analysis\nValence Aware Dictionary and sEntiment Reasoner is a lexicon and rule based sentiment analysis tool which works very well on social media sentiments. It is sensitive to both polarity (positive\/negative) and intensity (strength) of emotion. It is available in the NLTK package and can be applied directly to unlabeled text data.","30d751e0":"<font size=\"+2\" color=\"Green\"><b>Please Upvote if you like the work<\/b><\/font>\n\n### It gives motivation to a working professional (like me) to contribute more.","34a8da31":"#### 3) Conjuctions\nConjuctions lead to shift in polarity. The latter part (after the conjuction) acts as dominant part in defining the magnitude of polarity.","57fceaaa":"<font size=\"+3\" color=\"Green\"><b>Related Work:<\/b><\/font>\n\n\n### Links: Click [here](https:\/\/www.kaggle.com\/datatattle\/all-about-nlp) for related notebooks","599a08d0":"#### Like other preprocessing in text analysis we are not required to remove emojis, slangs (abbreviations), emoticons, punctuations, etc. as VADER generates scores based on these. \n#### Some of the important parts are:\n","83d325d6":"#### 1) Upper Case (Capitalization)\n#### Use of upper case alphabets\/words indicate the increase in magnitude of the sentiment. For example, I like the fact that monsoon is over. vs I LIKE the fact that monsoon is over.","4814f9f8":"### Tokenizer\n#### We can't analyze whole sentences, we will use regex to tokenize sentences to list of words.","246238d1":"#### 9) Slang words as modifiers","e9cfc70d":"<a id=\"6\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>6. TextBlob Model<\/b><\/font><br>","fed3c177":"#### 2) Punctuation\nUse of punctuation like !, ?, etc. add to the intensity of the text.","b1d99e36":"#### Most common words","eeacc402":"#### 6) Use of emojis (utf-8 encoded)","838f322c":"#### When we split description into individual words, we have to create vocabulary and additionaly we can add new feature - description lengths","dfaf3e60":"<a id=\"1\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>1. Importing data & libraries<\/b><\/font><br>","520ca269":"#### Positive Wordcloud","e45a650b":"### Getting emotions using NCR Emotion lexicon\n[Link](https:\/\/pypi.org\/project\/NRCLex\/)","87ae96b5":"#### 8) Use of negations ","8258580d":"<a id=\"4\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>4. Data Preprocessing<\/b><\/font><br>","63ecc91f":"#### Sentiment Analysis also known as Opinion Mining, which is a sub-field of Natural Language Processing (NLP), aims at identifying and extracting opinions through sentiments, attitude, and emotions of the writer within a given text. In general we see such text as tweets, reviews on shopping website (Amazon reviews), customer emails, social media messages and comments, surveys, etc. \n#### It has become highly important for the businesses around the globe to provide value and receive feedway so as to improve upon the product or service being offered & sentiment analysis plays an important role in this.\n#### It has high implementation in the fields of marketing, sociology, economies, political science, etc. One big example is Trump's first electoral campaign where (one of the strategies) he targeted the citizens based on sentiments (twitter). ","469c5e81":"<a id=\"7\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>7. Emotions in text<\/b><\/font><br>"}}