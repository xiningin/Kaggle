{"cell_type":{"50941cca":"code","5fe60181":"code","dca8582a":"code","b52f14d7":"code","d426c9a2":"code","e569a51c":"code","139e871e":"code","b76f4752":"code","5079b6f9":"code","57ec3f7b":"code","22734854":"code","7aad24a2":"code","17de797d":"code","799109c3":"code","8b25b56b":"code","ce489d52":"code","3a05f868":"code","b0ec6d54":"code","0428cb4f":"code","12c3b186":"code","34b885d2":"code","d81536af":"code","26ea19ee":"code","48f8893d":"code","f5ec4230":"code","8e758cf4":"code","4d7f8cc3":"code","f71f941c":"code","764ea868":"code","584b0bb2":"code","cd3a2d7c":"markdown","655f4ca1":"markdown","97274f6d":"markdown","d4bf04ce":"markdown","6468bc25":"markdown","98b74118":"markdown","50ef941a":"markdown","de11734c":"markdown","4529b25f":"markdown","617d090d":"markdown","564b8616":"markdown","cbcec33b":"markdown","ac5e6dff":"markdown","94f41493":"markdown","a3637c72":"markdown","26192758":"markdown","4d42fe5b":"markdown","d8ed99c2":"markdown","e287eb73":"markdown","20d2df90":"markdown","fe3d7790":"markdown"},"source":{"50941cca":"import numpy as np\nimport pandas as pd \nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nimport string, re\nfrom bs4 import BeautifulSoup\nfrom wordcloud import WordCloud\nfrom keras.preprocessing import text, sequence\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Dropout, Bidirectional\nfrom keras.callbacks import ReduceLROnPlateau\nimport tensorflow as tf","5fe60181":"np.random.seed(1)\ntf.random.set_seed(1)","dca8582a":"train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', index_col='id')\ntest_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv', index_col='id')","b52f14d7":"train_df.head()","d426c9a2":"train_df.info()","e569a51c":"ax = sns.barplot(x=\"target\", y=\"target\", data=train_df, estimator=lambda x: len(x) \/ len(train_df.index) * 100)\nax.set(ylabel=\"Percent\")\nplt.show()","139e871e":"train_df.keyword.unique()","b76f4752":"stop = set(stopwords.words('english'))\n# add punctuation to the list of stopwords\npunctuation = list(string.punctuation)\nstop.update(punctuation)","5079b6f9":"def strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n\n# Removing URL's\ndef remove_url(text):\n    return re.sub(r'http\\S+', '', text)\n\ndef add_space(text):\n    return re.sub('%20', ' ', text)\n\n#Removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n\n#Removing the noisy text\ndef denoise_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    text = add_space(text)\n    text = remove_url(text)\n    text = remove_stopwords(text)\n    return text\n","57ec3f7b":"def preprocess_df(df):\n    df = df.fillna(\"\")\n    df['text'] = df['location'] + \" \" + df['keyword'] + \" \" + df['text']\n    del df['keyword']\n    del df['location']\n    df['text'] = df['text'].apply(denoise_text)\n    return df","22734854":"train_df = preprocess_df(train_df)\ntest_df = preprocess_df(test_df)","7aad24a2":"X_train, X_dev, y_train, y_dev = train_test_split(train_df.text.values, train_df.target.values)","17de797d":"# Set max words and max length hyperparameters\nmax_features = 10000\nmax_len = 300","799109c3":"# Fit the tokenizer on the training data\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)","8b25b56b":"# Tokenize and pad each set of texts\ntokenized_train = tokenizer.texts_to_sequences(X_train)\nX_train = sequence.pad_sequences(tokenized_train, maxlen=max_len)\n\ntokenized_dev = tokenizer.texts_to_sequences(X_dev)\nX_dev = sequence.pad_sequences(tokenized_dev, maxlen=max_len)","ce489d52":"EMBEDDING_FILE = '..\/input\/glove-twitter\/glove.twitter.27B.100d.txt'","3a05f868":"# Create a dictionary of words and their feature vectors from the embedding file\ndef get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\n\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))","b0ec6d54":"all_embs = np.stack(list(embeddings_index.values()))\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\n\n# Find dims of embedding matrix\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n\n# Randomly initialize the embedding matrix\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n\n# Add each vector to the embedding matrix, corresponding to each token that we set earlier\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","0428cb4f":"batch_size = 1024\nepochs = 15\nembed_size = 100","12c3b186":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)","34b885d2":"#Defining Neural Network\nmodel = Sequential()\n#Non-trainable embeddidng layer\nmodel.add(Embedding(max_features, output_dim=embed_size, weights=[embedding_matrix], input_length=max_len, trainable=False))\n#LSTM \nmodel.add(LSTM(units=128 , return_sequences = False , recurrent_dropout = 0.3 , dropout = 0.3))\nmodel.add(Dense(units=64 , activation = 'relu', kernel_regularizer='l2'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['accuracy'])","d81536af":"model.summary()","26ea19ee":"history = model.fit(X_train, y_train, batch_size = batch_size , \n                    validation_data = (X_dev,y_dev) , \n                    epochs = epochs , callbacks = [learning_rate_reduction])","48f8893d":"print(\"Accuracy of the model on Training Data is - \" , model.evaluate(X_train,y_train)[1]*100)\nprint(\"Accuracy of the model on Dev Data is - \" , model.evaluate(X_dev,y_dev)[1]*100)","f5ec4230":"plt.figure(figsize=(10, 10))\n\nepochs = np.arange(epochs)\nplt.subplot(2, 2, 1)\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.plot(epochs, history.history['loss'])\n\nplt.subplot(2, 2, 2)\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.plot(epochs, history.history['accuracy'])\n\nplt.subplot(2, 2, 3)\nplt.xlabel('epochs')\nplt.ylabel('val_loss')\nplt.plot(epochs, history.history['val_loss'])\n\nplt.subplot(2, 2, 4)\nplt.xlabel('epochs')\nplt.ylabel('val_accuracy')\nplt.plot(epochs, history.history['val_accuracy'])\n\nplt.show()","8e758cf4":"X_test = test_df.text.values","4d7f8cc3":"tokenized_dev = tokenizer.texts_to_sequences(X_test)\nX_test = sequence.pad_sequences(tokenized_dev, maxlen=max_len)","f71f941c":"classes = model.predict_classes(X_test)[:, 0]","764ea868":"submission = pd.DataFrame(\n    {'id': list(test_df.index.values),\n     'target': list(classes),\n    }).set_index('id')","584b0bb2":"submission.to_csv('submission.csv')","cd3a2d7c":"We also track the progress of the loss and accuracy on the train and dev sets over each epoch.","655f4ca1":"# Model Training","97274f6d":"# Loading Data","d4bf04ce":"First, we tokenise the text into arrays.","6468bc25":"# Model Evaluation","98b74118":"We see that the distributions are pretty even, so the minimum performance should be approx 53% accuracy.","50ef941a":"# Text Preprocessing","de11734c":"![](http:\/\/)As part of preprocessing, we add the location column and combine the keywords with the text into one big text column. Then we apply the denoise function to the text.","4529b25f":"The model architecture consists of two LSTM layers, followed by a Dense layer and a sigmoid activation function.","617d090d":"We wish to remove html links, any words between square brackets, urls, %20's, and stopwords from the text.","564b8616":"Now, we create the glove embedding matrix to add to the model.","cbcec33b":"Let's see how the model does on the training and development data.","ac5e6dff":"We add learning rate reduction to the model to achieve good model fitting.","94f41493":"# Initial EDA","a3637c72":"Finally, we fit the model.","26192758":"Let's first check the distributions of fake and real disaster tweets. ","4d42fe5b":"Note that we see that some of the keywords have '%20' in place of spaces, so we replace these later by spaces.","d8ed99c2":"First we import a set of stopwords such as 'the' and 'a' which can be removed from tweets, as well as punctuation.","e287eb73":"# Predicting Test Data","20d2df90":"In order to choose a model architecture, we split the training data into train and dev sets.","fe3d7790":"Now, set the key hyperparameters."}}