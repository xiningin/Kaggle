{"cell_type":{"880e2029":"code","13241a8d":"code","442cea4e":"code","c9125a59":"code","eaf51e1c":"code","0286338a":"code","9d1a679d":"code","ebd51eb5":"code","9f6695bb":"code","1f6a2362":"code","e39650f1":"code","23759f82":"code","fd85a66a":"code","f98ca1c4":"code","cbbc2149":"code","b619429f":"code","923040e3":"code","6ea52ba1":"code","4a5d92f0":"code","f96618cc":"code","3c79a412":"code","3b48eed0":"code","d85bbef9":"code","9fc6ac75":"code","900a1eef":"code","7e325ae4":"code","a14eed7d":"code","4f726fd8":"markdown","f926e641":"markdown","a5cb64d8":"markdown","b0cd9fa7":"markdown","80219157":"markdown","de1192ab":"markdown","720823f3":"markdown","f9415529":"markdown","a158b388":"markdown","52169b13":"markdown","5d5aa329":"markdown","7e245fca":"markdown","8ab3e4a4":"markdown","a4ce3dca":"markdown","5bb884c9":"markdown","89a52195":"markdown","8e70a28a":"markdown","4075e3eb":"markdown","9d1c803d":"markdown","f4075c4e":"markdown","ccbeccec":"markdown","62f03321":"markdown","11b6afcc":"markdown","71b57a26":"markdown","bf456a40":"markdown","0112aad8":"markdown","accd083e":"markdown","f20f3fe4":"markdown","ef64f74b":"markdown","23bdcff6":"markdown","f32b5515":"markdown","fe4665a7":"markdown","c89ff27b":"markdown","d4a00cac":"markdown","8ed8a1a9":"markdown","a60f911d":"markdown","df211bd2":"markdown","2bc0c28e":"markdown","343293af":"markdown","7d69343a":"markdown"},"source":{"880e2029":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier\n\nfrom sklearn.model_selection import (\n    cross_val_score,\n    cross_validate,\n    train_test_split,\n    RandomizedSearchCV\n)\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\nfrom lightgbm.sklearn import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nimport altair as alt\nalt.renderers.enable('kaggle')\nimport matplotlib.pyplot as plt","13241a8d":"heart_df = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\nheart_df","442cea4e":"heart_df.info()","c9125a59":"train_df, test_df = train_test_split(heart_df, train_size=0.8, random_state=2018)\ntrain_df.shape","eaf51e1c":"train_df.describe()","0286338a":"unique_values = {}\nindex = np.arange(0, len(train_df.columns.to_list()))\nfor feature in train_df.columns.to_list():\n    unique_values[feature]=len(train_df[feature].unique())\n    \nunique_df = pd.DataFrame(unique_values, index=['unique_values']).T\nunique_df.T","9d1a679d":"numeric_features = unique_df.query('unique_values>5').index.tolist()\ntarget_feature = ['target']\ncategorical_features = list(\n    set(train_df.columns)\n    - set(numeric_features)\n    - set(target_feature))\n\nprint(f\"The categorical features are {categorical_features}. \\n\"\n     f\"The numeric features are {numeric_features}.\")","ebd51eb5":"corr_df = train_df[numeric_features].corr('spearman').stack().reset_index(name='corr')\ncorr_df.loc[corr_df['corr'] == 1, 'corr'] = 0  # Remove diagonal\n# Use abs so that we can visualize the impact of negative correaltion  \ncorr_df['abs'] = corr_df['corr'].abs()\ncorr_df.sort_values('abs', ascending=False).head()","9f6695bb":"alt.Chart(corr_df).mark_circle().encode(\n    x='level_0',\n    y='level_1',\n    size='abs',\n    color=alt.Color('corr',\n                    scale=alt.Scale(scheme='blueorange',\n                                    domain=(-1, 1)))).properties(\n    height=150,\n    width=150)","1f6a2362":"# Convert data types to string for plotting purpose\ncat_df = train_df[categorical_features[0:3]].copy()\nfor feature in categorical_features[0:3]:\n    cat_df[feature]= cat_df[feature].astype(str)\ncat_df.info()","e39650f1":"alt.Chart(cat_df).mark_square().encode(\n    alt.X(alt.repeat('row'), type='nominal'),\n    alt.Y(alt.repeat('column'), type='nominal'),\n    color='count()',\n    size='count()').repeat(\n    column=categorical_features[0:3],\n    row=categorical_features[0:3]\n)","23759f82":"list_plots = []\nfor feature in numeric_features:\n    list_plots.append(alt.Chart(train_df).transform_density(\n        feature,\n        groupby=['target'],\n        as_=[feature, 'density']).mark_area(\n        interpolate='monotone', opacity=0.4\n    ).encode(\n        x=feature,\n        y='density:Q',\n        color='target'))\n\n(list_plots[0] | list_plots[1]) & (list_plots[2] | list_plots[3]) & list_plots[4]","fd85a66a":"train_df.target.value_counts(normalize=True)","f98ca1c4":"X_train, y_train = train_df.drop(columns=[\"target\"]), train_df[\"target\"]\nX_test, y_test = test_df.drop(columns=[\"target\"]), test_df[\"target\"]","cbbc2149":"preprocessor = make_column_transformer(\n    (StandardScaler(), numeric_features),\n    (OneHotEncoder(drop=\"if_binary\", dtype=\"float\"), categorical_features),\n)","b619429f":"scoring_metrics = [\"accuracy\", \"roc_auc\"]","923040e3":"def mean_cross_val_scores(model, X_train, y_train, **kwargs):\n    \"\"\"\n    Returns mean scores of cross validation\n\n    Parameters\n    ----------\n    model :\n        scikit-learn model\n    X_train : numpy array or pandas DataFrame\n        X in the training data\n    y_train :\n        y in the training data\n\n    Returns\n    ----------\n        pandas Series with mean scores from cross_validation\n    \"\"\"\n    \n    scores = cross_validate(model, X_train, y_train, **kwargs)\n\n    mean_scores = pd.DataFrame(scores).mean()\n    out_col = []\n\n    for i in range(len(mean_scores)):\n        out_col.append(round(mean_scores[i], 5))\n\n    return pd.Series(data=out_col, index=mean_scores.index)","6ea52ba1":"results = {}","4a5d92f0":"dummy_pipe = make_pipeline(preprocessor, DummyClassifier(strategy='prior'))\nresults[\"DummyClassifier\"] = mean_cross_val_scores(dummy_pipe, X_train, y_train, scoring=scoring_metrics,\n                                                     return_train_score=True)\npd.DataFrame(results)","f96618cc":"dec_tree = make_pipeline(preprocessor, DecisionTreeClassifier())\n\nresults[\"DecisionTree\"] = mean_cross_val_scores(dec_tree, X_train, y_train, scoring=scoring_metrics,\n                                                            return_train_score=True)\n\npd.DataFrame(results)","3c79a412":"models = {\n    'RandomForestClassifier': make_pipeline(preprocessor, RandomForestClassifier(random_state=2018)),\n    'XGBClassifier': make_pipeline(preprocessor, XGBClassifier(random_state=2018, verbosity=0, use_label_encoder=False)),\n    'LGBMClassifier': make_pipeline(preprocessor, LGBMClassifier(random_state=2018)),\n    'CatBoostClassifier': make_pipeline(preprocessor, CatBoostClassifier(random_state=2018, verbose=0))\n}","3b48eed0":"for key in models.keys():\n    results[key]=mean_cross_val_scores(models[key], X_train, y_train, scoring=scoring_metrics,\n                                          return_train_score=True)\npd.DataFrame(results)","d85bbef9":"list_estimators = [('RandomForest', RandomForestClassifier(random_state=2018)),\n                   ('XGB', XGBClassifier(random_state=2018, verbosity=0, use_label_encoder=False)),\n                   ('LGBM', LGBMClassifier(random_state=2018)),\n                   ('CatBoost', CatBoostClassifier(random_state=2018, verbose=0))]","9fc6ac75":"pipe_stack = make_pipeline(preprocessor,\n                           StackingClassifier(estimators=list_estimators))\nresults['StackingClassifier'] = mean_cross_val_scores(pipe_stack,\n                                                         X_train,\n                                                         y_train,\n                                                         scoring=scoring_metrics,\n                                                         return_train_score=True)\nstack_results = pd.DataFrame(results)\nstack_results","900a1eef":"pipe_stack.fit(X_train, y_train);\n\npd.DataFrame(\n    data={\n        \"Coefficient\": pipe_stack.named_steps[\"stackingclassifier\"].final_estimator_.coef_.flatten(),\n        \"Magnitude\": abs(pipe_stack.named_steps[\"stackingclassifier\"].final_estimator_.coef_.flatten())\n    },\n    index=[x[0] for x in list_estimators]\n).sort_values(\"Magnitude\", ascending=False)","7e325ae4":"pipe_stack_DT = make_pipeline(preprocessor,\n                              StackingClassifier(estimators=list_estimators,\n                                                 final_estimator=DecisionTreeClassifier(random_state=2018)))\npipe_stack_DT.fit(X_train, y_train);\n\nfig = plt.figure(figsize=(25,10))\ntree.plot_tree(pipe_stack_DT.named_steps['stackingclassifier'].final_estimator_,\n              feature_names=[x[0] for x in list_estimators], max_depth=2);","a14eed7d":"print(f\"Training score  : {stack_results.loc['train_accuracy','StackingClassifier']} \\n\"\n      f\"Validation score: {stack_results.loc['test_accuracy','StackingClassifier']} \\n\"\n      f\"Test score      : {round(pipe_stack.score(X_test, y_test), 5)}\")","4f726fd8":"Luckily, we do not have NANs in our data frame.","f926e641":"For categorical features, we count the categorcial values per category to get an idea of where our observations lie.","a5cb64d8":"# 3. Preliminary EDA","b0cd9fa7":"## 6.6. Visualize stacking model as a tree","80219157":"We know that the training data has $242$ examples. So, basis that we can that the classification for categorical and numeric values can be done as follows:\n\n| Feature type    | Feature names                    |\n|-----------------|----------------------------------|\n| Numerical       | age, trestbps, chol, thalach, oldpeak            | \n| Categorical     | sex, cp, fbs, restecg, exang, slope, ca, thal, target          | \n\nNow, let us write some code to divide these features into seperate list of feature types. We will use the unique count value of 5 as the threshold to choose between numeric and categorical features.","de1192ab":"- All the tree-based models overfit the training set. It is due to the small dataset size of the training set.\n- This can be reduced by truncating the tree depth or by hyperparameter optimization of individual models.","720823f3":"- We are getting the coefficients for choosing the estimators because the default final estimator for StackingClassifier is Logistic Regression. So, these are actually regression coefficients.\n- `RandomForest` and `CatBoost` are highly influential in making decisions for the model.","f9415529":"Below we define the dictionary of all the models.","a158b388":"# 4. EDA","52169b13":"There is low correlation between the numeric features. (the highest correaltion has magnitude $0.417673$). ","5d5aa329":"- For `trestbps`, the densities is similar for both the target classes.\n- For `oldspeak` and `chol`, we have higher peaks for class 1 and is skewed to the right.\n- For `thalach` and `age`, there is slight left skew for class 1 and class 0 respectively,\n- `age` density has bimodal distribution for both the targets.","7e245fca":"Training all the models separately using a loop.","8ab3e4a4":"# 7. Score on test set","a4ce3dca":"## 5.1. Splitting the target feature","5bb884c9":"## 4.1. Relationship among the features","89a52195":"We can avoid using StandardScaler since we are using tree-based models which are not affected by scaling.","8e70a28a":"# 1. Reading the dataset","4075e3eb":"# 2. Data splitting","9d1c803d":"## 6.1. DummyClassifier for Baseline","f4075c4e":"## 6.4. Using Stacking for ensembling","ccbeccec":"## 6.5. Examine coefficients of the final estimator","62f03321":"All the fields seem to be numeric. We will have to check for catgeorical features in the set.","11b6afcc":"## 5.3. Setting our scoring metrics for analysis","71b57a26":"# 6. Making Ensemble ML models","bf456a40":"## 5.2. Making the column transformer","0112aad8":"# 0. Importing packages","accd083e":"First we plot the correlation plot between the numeric features.","f20f3fe4":"We try to visualize how decisions are made to choose an estimator. We set the final estimator for StackingClassifier as a Decision Tree to visualize the tree of choosing the estimators for making predicitions.","ef64f74b":"Yay! Our test score is even better than our validation score. This means our model is able to generalize well on unseen data.","23bdcff6":"- I'm grateful that you spent your time reading\/skimming all the way through. \n- Comments\/suggestions\/criticisms on the notebook would be highly appreciated.\n- Check out my other work on [Kaggle](https:\/\/www.kaggle.com\/rrrohit).","f32b5515":"## 6.3. Different models for ensemble","fe4665a7":"## 6.2. Decision tree as 2nd Baseline","c89ff27b":"## 4.2 Relationship between the features and the target","d4a00cac":"- We are only visualzing the first three categorical variables.\n- There are few values with slope 0.\n- The other variables seems evenly distributed among the categories.","8ed8a1a9":"## 5.4. Function to return CV scores in pandas format","a60f911d":"Oh wow! The classes are balanced perfectly.","df211bd2":"The accuracy of the model is very poor. With the ROC-AUC score, we can confirm that our model is good as choosing the mode of the target value.","2bc0c28e":"## 4.3. Checking for class imbalance","343293af":"We observe that even with a different final estimator, `RandomForest` is still the most influential model for making predictions.","7d69343a":"# 5 Preparation for the model"}}