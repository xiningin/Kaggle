{"cell_type":{"e769f0ce":"code","5b4b7c77":"code","0921a23b":"code","2f963082":"code","4c13c451":"code","9c965e63":"code","bb81c33e":"code","6f86ab9f":"code","799569a2":"code","68bf6fcd":"code","c23870b1":"code","19d63a2b":"code","416e6aa8":"code","11893857":"code","0a5a757e":"code","9a4c3980":"code","852a4a9e":"code","ba115be9":"code","24781fdc":"code","a30a1be9":"code","017e1143":"code","674f2a4c":"code","bc036748":"code","e4dd8399":"code","85e2ce07":"code","9002aedb":"code","5bb43e01":"code","e6ed3a51":"code","b84f2dfe":"code","2aba16bb":"code","647f8224":"code","85ef5fee":"code","bab9875a":"code","899e9ec5":"code","7fcef1fd":"code","438a8cb6":"code","6876ee6f":"code","0fde9b80":"code","c0b03f75":"code","fd9afab3":"code","4372ea51":"code","21c605c5":"code","d94cc954":"code","b02e4a27":"code","0b4a310a":"code","9c21a013":"code","d27ab96d":"code","7adc1774":"code","2cbd6b3a":"code","2b62daac":"code","352a6b71":"code","24f5b250":"markdown","654660a7":"markdown","0874c90b":"markdown","709653a6":"markdown","d84e172e":"markdown","d470cb8b":"markdown","2776dfb0":"markdown","c55cbf3d":"markdown","4cc3c7c5":"markdown","df4349b8":"markdown","1ba47e64":"markdown","f2f4a4bb":"markdown","4e5d4ac5":"markdown","421d0218":"markdown","71630e1f":"markdown","7628613c":"markdown","c74dd618":"markdown","a016fc80":"markdown","8663ed02":"markdown","c4227bea":"markdown","696aeef2":"markdown","fb3d47cf":"markdown","fcf8ceea":"markdown","d2233b4f":"markdown","0896bb8e":"markdown","d10be8f4":"markdown","caa85ce0":"markdown","eddced21":"markdown","787a8a6e":"markdown","6365a222":"markdown","0b0c25c7":"markdown","78a40a4d":"markdown","374a5f29":"markdown","b435cecc":"markdown","162a23c7":"markdown","aba3fd73":"markdown","9e53f440":"markdown","bca60a04":"markdown","5cc185f6":"markdown","50b1bee0":"markdown","8551db4e":"markdown","9b4466fe":"markdown"},"source":{"e769f0ce":"# Project packages\nimport pandas as pd\nimport numpy as np\npd.options.display.float_format = '{:.3f}'.format\n\n# Visualisations\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n# Statistics\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom statistics import mode\n\n# Machine Learning\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom catboost import Pool, CatBoostRegressor, cv\nfrom pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\n\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5b4b7c77":"#Importing the whole data set\ndata = pd.read_csv('..\/input\/Melbourne_housing_FULL.csv')","0921a23b":"#Exploring the Data set\ndata.info()","2f963082":"#Getting the  dataframe shape\nprint('The shape of the data is: {} '.format(data.shape))","4c13c451":"#Exploring the first few rows to get a feel for the Data (I prefer the transposed view!)\ndata.head().T","9c965e63":"#Getting the missing % count\nmissingData = (data.isnull().sum() \/ len(data)) * 100\nmissingData = pd.DataFrame({'Missing Percentage' :missingData})\nmissingData =  missingData.sort_values(by='Missing Percentage', ascending=False)\nmissingData['Missing Percentage'] = missingData['Missing Percentage'].round(4)\nmissingData.head(21)","bb81c33e":"#Visualizing Missing Data\nplt.figure(figsize=(8,5))\nplt.xticks(rotation='70')\nplt.ylabel('Percentage of Missing Data', fontsize=15)\nplt.xlabel('Features', fontsize=15)\nsns.barplot(x=missingData.index, y=missingData['Missing Percentage'])","6f86ab9f":"#Creating a table wit the stats of the data\ndata.describe().T","799569a2":"# I am going to imput \"None\" to features where this is applicable\nfor i in ('Regionname', 'CouncilArea'):\n    data[i] = data[i].fillna('None')","68bf6fcd":"# Replacing NA values with 0 where applicable\nfor i in ('Car', 'Bathroom', 'Bedroom2'):\n    data[i] = data[i].fillna(0)","c23870b1":"#Replacing with the mode where applicable\nfor i in ('BuildingArea','Propertycount', 'Postcode', 'YearBuilt'):\n    data[i] = data[i].fillna(data[i].mode()[0])","19d63a2b":"#Replacing with the Median where applicable\ndata['Landsize'] = data['Landsize'].replace(0, np.NaN) #Replacing all 0 values with NaN (Is this a good approach?)\n\nfor i in ('Landsize', 'Distance'):\n    data[i] = data[i].fillna(data[i].median())","416e6aa8":"#Removing the Latitude and Longitude values\nlat = data['Lattitude']\nlon = data['Longtitude']\nAddress = data['Address']\nSellerG = data['SellerG']\nbedroom2 = data['Bedroom2']\ndata.pop('Bedroom2')\ndata.pop('Longtitude')\ndata.pop('Lattitude')\ndata.pop('Address')\ndata.pop('SellerG')\nlist(data)","11893857":"#Checking again for missing data\nmissingData = (data.isnull().sum() \/ len(data)) * 100\nmissingData = pd.DataFrame({'Missing Percentage':missingData})\nmissingData = missingData.sort_values(by='Missing Percentage', ascending=False)\nmissingData['Missing Percentage'] = missingData['Missing Percentage'].round(2)\nmissingData.head(21)","0a5a757e":"#Removing all the observations where Price is null\ndata = data.dropna()\ndata.head().T","9a4c3980":"#Checking again for missing data\nmissingData = (data.isnull().sum() \/ len(data)) * 100\nmissingData = pd.DataFrame({'Missing Percentage':missingData})\nmissingData = missingData.sort_values(by='Missing Percentage', ascending=False)\nmissingData['Missing Percentage'] = missingData['Missing Percentage'].round(2)\nmissingData.head(16)","852a4a9e":"cols = data.columns.tolist()\ncolumnsTitles = ['Suburb', 'Type', 'Method', 'CouncilArea', 'Regionname','Rooms', 'Date', 'Distance', 'Postcode', \n                  'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', \n                 'Propertycount' ,'Price']\ndata = data.reindex(columns=columnsTitles)","ba115be9":"#Checking the Categorical Features\ndata.dtypes.sample(16)","24781fdc":"data['Date'] = data['Date'].astype('datetime64[ns]')\ndata.dtypes.sample(16)","a30a1be9":"#Encoding the variables\nohe = pd.get_dummies(data)\nohe.head().T","017e1143":"print(data['Suburb'].value_counts())","674f2a4c":"suburb = data['Suburb']\ndata.pop('Suburb')","bc036748":"#Re-applying the One Hot Encoder\nohe = pd.get_dummies(data)\nohe.head().T","e4dd8399":"#Getting the fit of the Price Feature\n(mu, sigma) = norm.fit(data['Price'])\n\n#Getting the distribution of the feature\nplt.figure(figsize=(8,5))\nsns.distplot(data['Price'], fit=norm);\nsns.despine()\nplt.ylabel('Frequency', fontsize=15)\nplt.xlabel('Price', fontsize=15)\nplt.title('Price Distribution', fontsize=15)\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n           loc='best')\nplt.show()","85e2ce07":"#Plotting the Price Feature as a QQPlot\nfig = plt.figure()\nres = stats.probplot(data['Price'], plot=plt)\nplt.show()","9002aedb":"# Applying a log(1+x) transformation to SalePrice\ndata['Price'] = np.log1p(data['Price'])","5bb43e01":"#Getting the fit of the Price Feature\n(mu, sigma) = norm.fit(data['Price'])\n\n#Getting the distribution of the feature\nplt.figure(figsize=(8,5))\nsns.distplot(data['Price'], fit=norm);\nsns.despine()\nplt.ylabel('Frequency', fontsize=15)\nplt.xlabel('Price', fontsize=15)\nplt.title('Price Distribution', fontsize=15)\nplt.legend(['Normal dist. ($\\mu=$ {:.1f} and $\\sigma=$ {:.1f} )'.format(mu, sigma)],\n           loc='best')\nplt.show()","e6ed3a51":"# Re-plotting the QQplot\nfig = plt.figure()\nres = stats.probplot(data['Price'], plot=plt)\nplt.show()","b84f2dfe":"#Exploring certain Relationships between Features\nsns.pairplot(data, vars = ['Price', 'Landsize', 'Bathroom', 'Rooms'], hue = 'Rooms',palette=\"husl\")","2aba16bb":"landOutlier = data[data['Landsize'] > 50000]\nlandOutlier","647f8224":"data = data[data['Landsize'] < 50000]\ndata.head()","85ef5fee":"#Re plotting the pairplot\nsns.pairplot(data, vars = ['Price', 'Landsize', 'Bathroom', 'Rooms'], hue = 'Rooms',palette=\"husl\")","bab9875a":"#Checking for feature correlations\ncorr = data.corr()\nplt.figure(figsize=(15,10))\nplt.title('Correlation of all the Features of House Prices', fontsize=18)\nsns.heatmap(corr,annot=True, cmap='RdYlGn', linewidths = 0.2, annot_kws={'size':10})\nplt.show()","899e9ec5":"train, test = train_test_split(data, test_size=0.2)","7fcef1fd":"train.head().T","438a8cb6":"test.head().T","6876ee6f":"# Set up variables\nX_train = train\nX_test = test\ny_train = train.Price.values\n\n#Re-applying the One Hot Encoder\nX_train = pd.get_dummies(X_train)\nX_test = pd.get_dummies(X_test)","0fde9b80":"date = data['Date']\nX_train.pop('Date')\nX_test.pop('Date')","c0b03f75":"# Defining two rmse_cv functions\ndef rmse_cv(model):\n    rmse = np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = 10))\n    return(rmse)","fd9afab3":"# Initiating Gradient Boosting Regressor\nmodel_gbr = GradientBoostingRegressor(n_estimators=1200, #The number of boosting stages to perform\n                                      learning_rate=0.05,#learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators.\n                                      max_depth=4, #The maximum depth limits the number of nodes in the tree.\n                                      max_features='sqrt',#Choosing max_features < n_features leads to a reduction of variance and an increase in bias\n                                      min_samples_leaf=15, #The minimum number of samples required to be at a leaf node\n                                      min_samples_split=10, #The minimum number of samples required to split an internal node\n                                      loss='huber',#loss function to be optimized:  \u2018huber\u2019 is a combination of the two\n                                      random_state=5)#random_state is the seed used by the random number generator;","4372ea51":"# Initiating XGBRegressor\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.2,#Subsample ratio of columns for each split, in each level. Subsampling will occur each time a new split is made.\n                             learning_rate=0.06,#Step size shrinkage used in update to prevents overfitting\n                             max_depth=3,#Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit\n                             n_estimators=1150)","21c605c5":"# Initiating LGBMRegressor model (no clear parameter explanation was found in the LGB docmentation)\nmodel_lgb = lgb.LGBMRegressor(objective='regression',\n                              num_leaves=4,\n                              learning_rate=0.05, \n                              n_estimators=1080,\n                              max_bin=75, \n                              bagging_fraction=0.80,\n                              bagging_freq=5, \n                              feature_fraction=0.232,\n                              feature_fraction_seed=9, \n                              bagging_seed=9,\n                              min_data_in_leaf=6, \n                              min_sum_hessian_in_leaf=11)","d94cc954":"# Fitting all models with rmse_cv function\ncv_gbr = rmse_cv(model_gbr).mean()\ncv_xgb = rmse_cv(model_xgb).mean()\ncv_lgb = rmse_cv(model_lgb).mean()","b02e4a27":"# Creating a table of results, ranked highest to lowest\nresults = pd.DataFrame({\n    'Model': ['Gradient Boosting Regressor',\n              'XGBoost Regressor',\n              'Light Gradient Boosting Regressor'],\n    'Score': [ cv_gbr,\n              cv_xgb,\n              cv_lgb,]})\n\n# Build dataframe of values\nresult_df = results.sort_values(by='Score', ascending=True).reset_index()\nresult_df.head()","0b4a310a":"#Plotting the values as part of good practice\nf, ax = plt.subplots(figsize=(10, 6))\nsns.barplot(x=result_df['Model'], y=result_df['Score'])\nplt.xlabel('Regression Models', fontsize=15)\nplt.ylabel('Performance', fontsize=15)\n\nplt.show()","9c21a013":"#checking for columns missmatch\ne = X_train.columns.difference(X_test.columns)\ne","d27ab96d":"X_train = X_train.drop(['CouncilArea_Mitchell Shire Council', \n                       'CouncilArea_None','Regionname_None'], axis=1)\n\n#X_train = X_train.drop([ 'CouncilArea_None','Regionname_None'], axis=1)","7adc1774":"X_train.head().T","2cbd6b3a":"#checking for columns missmatch\ne = X_train.columns.difference(X_test.columns)\ne","2b62daac":"# Fit and predict all models\nmodel_xgb.fit(X_train, y_train)\nxgb_pred = np.expm1(model_xgb.predict(X_test))\n\nmodel_gbr.fit(X_train, y_train)\ngbr_pred = np.expm1(model_gbr.predict(X_test))\n\nmodel_lgb.fit(X_train, y_train)\nlgb_pred = np.expm1(model_lgb.predict(X_test))","352a6b71":"# Create stacked model\nstacked = (xgb_pred + lgb_pred + gbr_pred) \/ 3\n#stacked = pd.DataFrame(stacked)\npredictions = pd.DataFrame()\npredictions['Predicted Value'] = stacked\npredictions.head()\n","24f5b250":"There seems to be a few columns in the X_train data frame that do not appear in the X_test data frame so for the sake of simplicity (and the fact that i do not know what else could I do) I am going to remove this columns\n\n### Mental Note:\n* why is this happening as I havre no Idea!\n* Does this have something to do with the splitting of the data as train and test sets?","654660a7":"### Fitting, Predicting and Stacking the Models ","0874c90b":"## Splitting the Data Frame","709653a6":"I don't understand why the mu and sigma values have changed. they should have been unaffected by the log transformation but that has not been the case. Any help in this matter would be highly appreciated but for the self-educational purposes of this report I will carry on with my analysis!\n#### Mental Note:\nHave I applied the wrong Log transformation?\nI also tried a Log10 transformation and it shrunk the Price values even further!","d84e172e":"Re-plotting the distribution","d470cb8b":"There seems to be a couple outliers in the landsize feature so lets take a look at it","2776dfb0":"## Importing the Data Sets","c55cbf3d":"# Special Thanks\nThis little report could not have been possible without the inspiration that [Joshua Reed][2] gave me with the high quality tutorial as it was very easy to follow and understand!\n\n[2]:https:\/\/www.kaggle.com\/josh24990","4cc3c7c5":"Looks like the Date Feature  is an object so I will need to convert it to a date Data Type","df4349b8":"Yep!..... It looks normalized to me now!","1ba47e64":"## Removing the Date Feature\nIt looks that having the date feature will cause issues when running my ML Algorithms so I have to remove this feature as well\n\n### Mental Note:\n* Why the date feature was not ecoded when I ran the pd.getDummies() fuction?\n* Do I need the price feature to predict house prices?","f2f4a4bb":"Let's have a look at the  data from a Statistics perspective in order to understand how the missing values ar affecting the data.\nI will have another look at the data once the missing values have been dealt with and see what difference that makes.","4e5d4ac5":"## Creating a Correlation Heatmap","421d0218":"## Re-arranging columns as i want to put the Price feature at the end of the Data Frame","71630e1f":"## Investigating the 'Price' Feature\n* I would liek to check if the Price feature has any skew that might cause issues later.","7628613c":"## Stealing and RMSE Function from fellow Kaggler [Joshua Reed][1] (Thanks for Inspiring me!)\n[1]:https:\/\/www.kaggle.com\/josh24990","c74dd618":"Checking for column mismatch one last time","a016fc80":"# Introduction\n\nHello Kaggle,\nI will use all my noob skills to try to predict house prices in Melbourne.\nI have done my best to provide a high quality kernel here but I know for a fact that tere are several areas that require contructive critisism from more experience kagglers like youselves.\n\nThe Kernel seems to be a little longer than the average kernel and the main reason for it is that I wante to reflect my thought process and sometimes struggle to the readers but  also to show that I am understanding the concepts well and that they have been applied to the best of my knowledge.\n\nas you read through this report you will find parts where I have included a \"Mental Note\", this is because at that particular part of the kernel I was not sure if I have made the correct decision or if I had understood the concept correctly.\n\nI hope you enjoy correcting me as I am looking forward to bettering myself with your coments and\/or instructions","8663ed02":"## Data Frame Shape","c4227bea":"Lets plot the missing data as it is good practice for what I have read! :)","696aeef2":"So it seems that for some strange reason all the ML Algorithms are performing really well due to their fairly low rmse value, In my world this does not happen so I must have doen something wrong or..... I am just a gifted human being! :p","fb3d47cf":"## Removing Unnescesary Features","fcf8ceea":"Looks like the ecoding has not been tranfered from the \"data\" Data Frame to the test and train Data Frames so  I will Re-apply it \n\n### Mental Note:\nWhy has the encoding been removed when the data was split?","d2233b4f":"## Ensebling the Algorithms\nIt is time so combine all the algorithms.","0896bb8e":"One thing I do not understand is why the X and Y Axis have that scale. Also, why is there the \"le7\" tag on the X axis?\nI have tried to convert the feature to an int from Float but I got the same graph. Any help answering this question would be greatly appreciated!","d10be8f4":"## Checking again to see if there is any null data that I have missed","caa85ce0":"# The Plan of Attack:\nI am going to ensemble 3 Machine Learning algorithms (Gradient tree boosting) which are:\n* Gradient Boosting\n* XGBoost\n* LightGBM\n\n### Disclaimer:\nThe main goal of me using this ML Algorithms is to gain valuable practical experience but also to understand how they work","eddced21":"Looks like the QQPlot is strongly right positively skewed. I have read on another post that a logarithmic transformation could take care of this issue","787a8a6e":"## Dealing With the Missing Values on the Dataset (Imputting)","6365a222":"## Setting up the Enviornment","0b0c25c7":"## Exploring the features further","78a40a4d":" It seems that all the features that had missing values have been dealt with except the Price Feature. I will remove this observations from the data set for this particular excercise(Please let me know if i have made the correct desision! )","374a5f29":"## Missing Values\n\nThere seems to be quite a few \"null\" values lurking around (NaN) so we have to take care of them. \nLet's plot a table to see the pecentages of missing values per feature.","b435cecc":"## Fitting the ML Algorithms\nI am going to run the RMSE functions to get a feeling about the performance of the ML Algorithms","162a23c7":"### Investigating the 'Suburb' Feature","aba3fd73":"## Thank you all!\nAnd here it is! a result that pleases me but I am sure that there are a million things that i could have done better so please Kagglers bombard me with constructive critisism as all I want to do is better myself.\nOnce again  big thank you to Joshua Reed as he was my inspiration and also because of his kernels I have been able to produce this report.","9e53f440":"### Mental Note:\nFor the purposes of this excercise and for the sake of Aesthetics and simplicity I am going to remove them form the data as well. (Please let me know if there is a better option to deal with this 5 outliers?)","bca60a04":"Because there are 351 different categories of this feature, there is a chance that the amount of categories in the feature might have a negative impact on the data in general so I will remove it for now.","5cc185f6":"I don't think that  the Adress, SellerG, Bedroom2, Lattitude and Longitude will be statistically significant when it comes to predicting House Prices so I am going to drop them\n### Mental Note:\nPlease let me know if  the choice that I have made above is the right one!","50b1bee0":"## Encoding Variables","8551db4e":"## Exploring the Data","9b4466fe":"## Short  Description of the Features\n\n* Suburb: Suburb\n\n* Address: Address\n\n* Rooms: Number of rooms\n\n* Price: Price in Australian dollars\n\n* Method: S - property sold; SP - property sold prior; PI - property passed in; PN - sold prior not disclosed; SN - sold not disclosed; NB - no bid; VB - vendor bid; W - withdrawn prior to auction; SA - sold after auction; SS - sold after auction price not disclosed. N\/A - price or highest bid not available.\n\n* Type: br - bedroom(s); h - house,cottage,villa, semi,terrace; u - unit, duplex; t - townhouse; dev site - development site; o res - other residential.\n\n* SellerG: Real Estate Agent\n\n* Date: Date sold\n\n* Distance: Distance from CBD in Kilometres\n\n* Regionname: General Region (West, North West, North, North east ...etc)\n\n* Propertycount: Number of properties that exist in the suburb.\n\n* Bedroom2 : Scraped # of Bedrooms (from different source)\n\n* Bathroom: Number of Bathrooms\n\n* Car: Number of carspots\n\n* Landsize: Land Size in Metres\n\n* BuildingArea: Building Size in Metres\n\n* YearBuilt: Year the house was built\n\n* CouncilArea: Governing council for the area\n\n* Lattitude: Self explanitory\n\n* Longtitude: Self explanitory\n"}}