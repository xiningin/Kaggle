{"cell_type":{"3e679185":"code","5050e543":"code","87614b4a":"code","496e3741":"code","2e86ed6c":"code","a86ad8e3":"code","514f78c8":"code","25e3ac0b":"code","7a3fdb9b":"code","7e30ae5d":"code","6c3055d4":"code","32e1b74b":"code","461ff1c3":"code","85f04b24":"code","bb601f36":"code","6880ca9b":"code","65cff5b6":"code","62a02418":"code","de676127":"code","b9131da6":"code","e6bd9bed":"code","0eea11f9":"code","3817ae9c":"code","85df3d47":"code","cb0496ee":"code","2b4a55a7":"code","dfae019e":"code","4819b713":"code","b5484d05":"code","c58e578e":"code","dcb06a51":"code","078cc2c3":"code","4ba20b33":"code","3db88d4b":"code","005cca44":"code","25d72cef":"code","cd65633a":"code","6cdf1e5e":"code","1be50fc4":"code","a15052dc":"code","65571598":"code","8b6c6a71":"code","fcc06af7":"code","f853be80":"code","6ae31995":"code","2932401c":"code","05fe062f":"code","6f281207":"code","0082222c":"code","bb6f3ef2":"code","0b5372fc":"code","bb04bb97":"code","3eed0162":"code","82fc61fa":"code","d58f1a3d":"code","5ea660d4":"code","e7152b05":"code","c19436ae":"code","0f55aab3":"code","be9d53dd":"code","253e0bbd":"code","529ba1b5":"code","7f8a3bd6":"code","1c511eaa":"code","7f2713dd":"code","06f78db6":"code","b256d3d4":"code","7d633ebd":"markdown","ce635b94":"markdown","855a9d83":"markdown","9c974c84":"markdown","5d1228f5":"markdown","ffe75cae":"markdown","e94f0314":"markdown","c2f373a6":"markdown","07641347":"markdown","b412e7eb":"markdown","9b574f3d":"markdown","5025516c":"markdown","d7e5c092":"markdown","74578353":"markdown","48ed613e":"markdown","6333e843":"markdown","d9daa611":"markdown","38f7fd70":"markdown","fb0d7824":"markdown","3782af32":"markdown","de9a351d":"markdown","588f0842":"markdown","ee5b6dba":"markdown","22aa6133":"markdown","77443d73":"markdown","11e8a2f4":"markdown","93137ed4":"markdown","14df85b8":"markdown","93074497":"markdown","f4d715ff":"markdown","70ef6756":"markdown","0606a08b":"markdown","2ed53910":"markdown","48931828":"markdown","fcedcda7":"markdown","6471d626":"markdown","da69a8a4":"markdown","71ec18c8":"markdown","8da2dc62":"markdown","43a285bf":"markdown","74acdec1":"markdown","c5f3e54c":"markdown","58d49071":"markdown","0c0dc309":"markdown","d2aa08c3":"markdown","dd05928b":"markdown","909ea821":"markdown","50a5146a":"markdown","7480b8ec":"markdown","ede3c256":"markdown","89d1671a":"markdown","186a96bc":"markdown","2045c95c":"markdown","40ceb974":"markdown","29abdcd9":"markdown","a714dab5":"markdown","2ea97d5f":"markdown","50075ec7":"markdown"},"source":{"3e679185":"# Have used plotty so please make sure to install below commands \n#!pip install plotly\n#!pip install cufflinks","5050e543":"import pandas as pd\nimport numpy as np\n\nimport cufflinks as cf\nimport plotly as py\nimport plotly.graph_objs as go\nimport ipywidgets as widgets\nfrom scipy import special \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom IPython.display import Markdown, display ,HTML\nimport statsmodels.api as sm # import API\nfrom matplotlib.pyplot import xticks\n\n\nsns.set(style=\"whitegrid\")\n\npd.set_option('display.max_columns', 100)\n\npy.offline.init_notebook_mode(connected=True) # plotting in offilne mode \ncf.set_config_file(offline=False, world_readable=True, theme='ggplot')\n\npd.set_option('display.max_colwidth', -1) # make sure data and columns are displayed correctly withput purge\npd.options.display.float_format = '{:20,.2f}'.format # display float value with correct precision \n\nimport warnings\nwarnings.filterwarnings('ignore')","87614b4a":"def log(string):\n    display(Markdown(\"> <span style='color:blue'>\"+string+\"<\/span>\"))\n\ndef header(string):\n    display(Markdown(\"------ \"))\n    display(Markdown(\"### \"+string))\n    \ndef header_red(string):\n    display(Markdown(\"> <span style='color:red'>\"+string))   \n    \ndef color_red(val):\n    \"\"\"\n    Takes a scalar and returns a string with\n    the css property `'color: red'` for 65 %\n    null values , black otherwise.\n    \"\"\"\n    color = 'red' if val > 65 else 'black'\n    return 'color: %s' % color  \n\ndef get_variable_type(element) :\n    \"\"\"\n     Check is columns are of Contineous or Categorical variable.\n     Assumption is that if \n                 unique count < 20 then categorical \n                 unique count >= 20 and dtype = [int64 or float64] then contineous\n     \"\"\"\n    if element==0:\n        return \"Not Known\"\n    elif element < 20 and element!=0 :\n        return \"Categorical\"\n    elif element >= 20 and element!=0 :\n        return \"Contineous\" \n    \ndef get_meta_data(dataframe) :\n    \"\"\"\n     Method to get Meta-Data about any dataframe passed \n    \"\"\"\n    metadata_matrix = pd.DataFrame({\n                    'Datatype' : dataframe.dtypes.astype(str), # data types of columns\n                    'Non_Null_Count': dataframe.count(axis = 0).astype(int), # total elements in columns\n                    'Null_Count': dataframe.isnull().sum().astype(int), # total null values in columns\n                    'Null_Percentage': dataframe.isnull().sum()\/len(dataframe) * 100, # percentage of null values\n                    'Unique_Values_Count': dataframe.nunique().astype(int) # number of unique values\n                     })\n    \n    metadata_matrix = predict_variable_type(metadata_matrix)\n    return metadata_matrix\n        \ndef display_columns_with_1_unique_value(dataframe):\n    unique_values_count_1 = dataframe[dataframe[\"Unique_Values_Count\"] == 1]\n    drop_value_col = unique_values_count_1.index.tolist()\n    lenght = len(drop_value_col)\n    header(\"Columns with only one unique value : \"+str(lenght))\n    if lenght == 0 :\n        header_red(\"No columns with only one unique values.\")  \n    else :    \n        log(\"Columns with only one unique value :\")\n        for index,item in enumerate(drop_value_col) :\n            print(index,\".\",item)\n            \ndef predict_variable_type(metadata_matrix):\n    metadata_matrix[\"Variable_Type\"] = metadata_matrix[\"Unique_Values_Count\"].apply(get_variable_type).astype(str)\n    metadata_matrix[\"frequency\"] = metadata_matrix[\"Null_Count\"] - metadata_matrix[\"Null_Count\"]\n    metadata_matrix[\"frequency\"].astype(int)\n    return metadata_matrix \n\n\ndef list_potential_categorical_type(dataframe,main) :\n    header(\"Stats for potential Categorical datatype columns\")\n    metadata_matrix_categorical = dataframe[dataframe[\"Variable_Type\"] == \"Categorical\"]\n    # TO DO *** Add check to skip below if there is no Categorical values \n    length = len(metadata_matrix_categorical)\n    if length == 0 :\n        header_red(\"No Categorical columns in given dataset.\")  \n    else :    \n        metadata_matrix_categorical = metadata_matrix_categorical.filter([\"Datatype\",\"Unique_Values_Count\"])\n        metadata_matrix_categorical.sort_values([\"Unique_Values_Count\"], axis=0,ascending=False, inplace=True)\n        col_to_check = metadata_matrix_categorical.index.tolist()\n        name_list = []\n        values_list = []\n        for name in col_to_check :\n            name_list.append(name)\n            values_list.append(main[name].unique())\n        temp = pd.DataFrame({\"index\":name_list,\"Unique_Values\":values_list})\n        metadata_matrix_categorical = metadata_matrix_categorical.reset_index()\n        metadata_matrix_categorical = pd.merge(metadata_matrix_categorical,temp,how='inner',on='index')\n        display(metadata_matrix_categorical.set_index(\"index\")) \n        \ndef get_potential_categorical_type(dataframe,main,unique_count) :\n    metadata_matrix_categorical = dataframe[dataframe[\"Variable_Type\"] == \"Categorical\"]\n    metadata_matrix_categorical = dataframe[dataframe[\"Unique_Values_Count\"] == unique_count]\n    length = len(metadata_matrix_categorical)\n    if length == 0 :\n        header_red(\"No Categorical columns in given dataset.\")  \n    else :    \n        metadata_matrix_categorical = metadata_matrix_categorical.filter([\"Datatype\",\"Unique_Values_Count\"])\n        metadata_matrix_categorical.sort_values([\"Unique_Values_Count\"], axis=0,ascending=False, inplace=True)\n        col_to_check = metadata_matrix_categorical.index.tolist()\n        name_list = []\n        values_list = []\n        for name in col_to_check :\n            name_list.append(name)\n            values_list.append(main[name].unique())\n        temp = pd.DataFrame({\"index\":name_list,\"Unique_Values\":values_list})\n        metadata_matrix_categorical = metadata_matrix_categorical.reset_index()\n        metadata_matrix_categorical = pd.merge(metadata_matrix_categorical,temp,how='inner',on='index')\n        display(metadata_matrix_categorical.set_index(\"index\")) \n        \ndef plot_data_type_pie_chat(dataframe) : \n    header(\"Stats for Datatype Percentage Distribution\")\n    dataframe_group = dataframe.groupby(\"Datatype\").frequency.count().reset_index()\n    dataframe_group.sort_values([\"Datatype\"], axis=0,ascending=False, inplace=True)\n    trace = go.Pie(labels=dataframe_group[\"Datatype\"].tolist(), values=dataframe_group[\"frequency\"].tolist())\n    layout = go.Layout(title=\"Datatype Percentage Distribution\")\n    fig = go.Figure(data=[trace], layout=layout)    \n    py.offline.iplot(fig)\n    \ndef pairplot(x_axis,y_axis) :\n    sns.pairplot(car_df,x_vars=x_axis,y_vars=y_axis,height=4,aspect=1,kind=\"scatter\")\n    plt.show()\n\ndef heatmap(x,y,dataframe):\n    plt.figure(figsize=(x,y))\n    sns.heatmap(dataframe.corr(),cmap=\"OrRd\",annot=True)\n    plt.show()\n        \ndef plot_box_chart(dataframe) :\n    data = []\n    for index, column_name in enumerate(dataframe) :\n        data.append(\n        go.Box(y=dataframe.iloc[:, index],name=column_name))   \n        \n    layout = go.Layout(yaxis=dict(title=\"Frequency\",zeroline=False),boxmode='group')\n    fig = go.Figure(data=data, layout=layout)    \n    py.offline.iplot(fig)    \n    \ndef bar_count_plot(dataframe,col_name) :\n    plt.figure(figsize=(16,8))\n    plt.title(col_name + 'Histogram')\n    sns.countplot(car_df[col_name], palette=(\"plasma\"))\n    xticks(rotation = 90)\n    plt.show()\n    \ndef color_red(val):\n    \"\"\"\n    Takes a scalar and returns a string with\n    the css property `'color: red'` for value \n    greater than 10 , black otherwise.\n    \"\"\"\n    color = 'red' if val > 5 else 'black'\n    return 'color: %s' % color\n\ndef color_code_vif_values(val):\n    \"\"\"\n    Takes a scalar and returns a string with\n    the css property `'color: red'` for 10\n    , black otherwise.\n    \"\"\"\n    if val > 10 : color = 'red' \n    elif val > 5 and val <= 10 : color = 'blue'\n    elif val > 0 and val <= 5 : color = 'darkgreen'\n    else : color = 'black'\n    return 'color: %s' % color\n\ndef drop_col(dataframe,col_to_drop) :\n    dataframe.drop([col_to_drop],axis=1,inplace=True)\n    return dataframe","496e3741":"encoding_latin=\"latin\"\ncar_df = pd.read_csv(\"..\/input\/CarPrice_Assignment.csv\",low_memory = False,encoding = encoding_latin)\ncar_df.head()","2e86ed6c":"car_df.info()","a86ad8e3":"metadata_matrix_dataframe = get_meta_data(car_df)\n\n# 1. Columns List with only 1 unique values\ndisplay_columns_with_1_unique_value(metadata_matrix_dataframe)\n\n# 2. Display Data Type percenatges \nplot_data_type_pie_chat(metadata_matrix_dataframe)\n\n# 3. Potential Categorical Variable columns \nlist_potential_categorical_type(metadata_matrix_dataframe,car_df)","514f78c8":"car_df[\"CarCompany\"] = car_df[\"CarName\"].apply(lambda carName : carName.split(\" \")[0].title())\ncar_df = car_df.replace(to_replace =\"4wd\", value =\"fwd\") \n\n# Dropping car name as it will not add any values for our price prediction \ncar_df.drop([\"CarName\"],axis=1,inplace=True)\ncar_df.drop([\"car_ID\"],axis=1,inplace=True)","25e3ac0b":"car_df[\"CarCompany\"].unique()","7a3fdb9b":"car_df = car_df.replace(to_replace =\"Maxda\", value =\"Mazda\") \ncar_df = car_df.replace(to_replace =\"Porcshce\", value =\"Porsche\") \ncar_df = car_df.replace(to_replace =\"Toyouta\", value =\"Toyota\") \ncar_df = car_df.replace(to_replace =\"Vokswagen\", value =\"Volkswagen\") \ncar_df = car_df.replace(to_replace =\"Vw\", value =\"Volvo\") ","7e30ae5d":"car_df[\"CarCompany\"].unique()","6c3055d4":"car_df.head()","32e1b74b":"plot_box_chart(pd.DataFrame(car_df[\"price\"]))\nlog(\"Analysis : Price field has median around 10K with most expensive car values at 45k and cheapest car is 5k\")","461ff1c3":"car_df_describe = car_df.describe()\ndisplay(car_df_describe)","85f04b24":"y_vars=['price']\nx_vars=['wheelbase','curbweight','boreratio']\npairplot(x_vars,y_vars)\nlog(\"Analysis : Wheelbase and Curbweight are positively correlated but gets spread out at higer values.\")","bb601f36":"x_vars=['carlength','carwidth', 'carheight']\npairplot(x_vars,y_vars)\nlog(\"Analysis : Carlength & Carwidth are more correlated compared to carheight which is more spreadout but positive.\")\n","6880ca9b":"x_vars=['enginesize','horsepower','stroke']\npairplot(x_vars,y_vars)\nlog(\"Analysis : Enginesize & Horsepower are postively correlated but Stroke is more spread out(might not be related).\")\n","65cff5b6":"x_vars=['compressionratio','peakrpm',\"symboling\"]\npairplot(x_vars,y_vars)\nlog(\"Analysis : Compressionratio and Peakrpm is not correlated.\")","62a02418":"x_vars=['citympg', 'highwaympg']\npairplot(x_vars,y_vars)\n\nlog(\"Analysis : Citympg & Highwaympg is **negative** correlated, cheaper cars have better milage compare to expensive ones.\")\n","de676127":"heatmap(20,12,car_df)","b9131da6":"independent_col_list = ['wheelbase', 'carlength', 'carwidth', 'carheight','curbweight', \n                        'enginesize','boreratio','horsepower','citympg', 'highwaympg']\n\nheatmap(14,10,car_df.filter(independent_col_list))","e6bd9bed":"dimension_col_list = ['wheelbase', 'carlength', 'carwidth', 'carheight','curbweight']\nheatmap(12,8,car_df.filter(dimension_col_list))","0eea11f9":"performance_col_list = ['enginesize','boreratio','horsepower']\nheatmap(12,8,car_df.filter(performance_col_list))","3817ae9c":"bar_count_plot(car_df,\"CarCompany\")\nlog(\"Analysis : Looking at above graph Toyota seems to be really popular among car company followed Nissan and Mazda.\")","85df3d47":"# internal function written by me for better visualizatoin and understanding\nmetadata_matrix_dataframe = get_meta_data(car_df)\nlist_potential_categorical_type(metadata_matrix_dataframe,car_df)","cb0496ee":"plt.figure(figsize=(20, 12))\nplt.subplot(2,3,1)\nsns.boxplot(x = 'fueltype', y = 'price', data = car_df, palette=(\"plasma\"))\nplt.subplot(2,3,2)\nsns.boxplot(x = 'aspiration', y = 'price', data = car_df, palette=(\"plasma\"))\nplt.subplot(2,3,3)\nsns.boxplot(x = 'doornumber', y = 'price', data = car_df ,palette=(\"plasma\"))\nplt.subplot(2,3,4)\nsns.boxplot(x = 'drivewheel', y = 'price', data = car_df , palette=(\"plasma\"))\nplt.subplot(2,3,5)\nsns.boxplot(x = 'enginelocation', y = 'price', data = car_df , palette=(\"plasma\"))\nplt.show()","2b4a55a7":"plt.figure(figsize=(14,6))\nsns.boxplot(x='carbody',y='price',data = car_df, palette=(\"plasma\"))\nplt.show()\nlog(\"Analysis : Hardtop is a clear winner and is the preffered choice among other.\")\n\nplt.figure(figsize=(14,6))\nsns.boxplot(x='fuelsystem',y='price',data = car_df, palette=(\"plasma\"))\nplt.show()\nlog(\"Analysis : MPFI is the most common one among cars.\")\n\nplt.figure(figsize=(14,6))\nsns.boxplot(x='enginetype',y='price',data = car_df,palette=(\"plasma\"))\nplt.show()\nlog(\"Analysis : ohcv is the most common engine type.\")\n\nplt.figure(figsize=(14,6))\nsns.boxplot(x='cylindernumber',y='price',data = car_df,palette=(\"plasma\"))\nplt.show()\nlog(\"Analysis : Expensive cars have Eight cylinder , four cylinder are the cheapest one.\")\n","dfae019e":"plt.figure(figsize=(14,8))\nsns.barplot(x = 'cylindernumber', y = 'price', hue = 'fueltype',data = car_df,palette=(\"plasma\"))\nplt.show()\nlog(\"Analysis : Not all cars comes in both gas and diesel variants. Cars with cylinder four,six and five have both variants.\")\n","4819b713":"plt.figure(figsize=(20,6))\nplt.subplot(1,2,1)\nsns.barplot(x = 'aspiration', y = 'price', hue = 'fueltype',data = car_df,palette=(\"plasma\"))\n\nplt.subplot(1,2,2)\nsns.barplot(x = 'enginelocation', y = 'price', hue = 'fueltype',data = car_df,palette=(\"plasma\"))\nplt.show()\n\nlog(\"Analysis : Can with rear engine dosent have any diesel models. Turo mode is more prevalent in diesel.\")\n","b5484d05":"plt.figure(figsize=(20,5))\nsns.barplot(x = 'symboling', y = 'price',data = car_df,palette=(\"plasma\"))\nplt.show()\nlog(\"Analysis : Top categories for car symboling is -1 and 3. \")\n","c58e578e":"metadata_matrix_dataframe = get_meta_data(car_df)\nget_potential_categorical_type(metadata_matrix_dataframe,car_df,2)","dcb06a51":"def binary_dummy_replace(x) :\n     return x.map({\"gas\":1,\"diesel\":0,\n                   \"std\":1,\"turbo\":0,\n                   \"two\":1, \"four\":0,\n                   \"rwd\": 1, \"fwd\": 0,\n                   \"front\": 1, \"rear\": 0})","078cc2c3":"col_to_replace =  [\"fueltype\",\"aspiration\",\"doornumber\",\"drivewheel\",\"enginelocation\"]\ncar_df[col_to_replace] = car_df[col_to_replace].apply(binary_dummy_replace)\n\ncar_df.head(2)","4ba20b33":"def create_dummy_variable(dataframe,column_name):\n    dummy_values = pd.get_dummies(dataframe[column_name],drop_first=True)\n    dataframe = pd.concat([dataframe,dummy_values],axis=1)\n    dataframe.drop([column_name],axis=1,inplace=True)\n    return dataframe","3db88d4b":"metadata_matrix_dataframe = get_meta_data(car_df)\nlist_potential_categorical_type(metadata_matrix_dataframe,car_df)","005cca44":"car_df = create_dummy_variable(car_df,\"carbody\")\ncar_df = create_dummy_variable(car_df,\"cylindernumber\")\ncar_df = create_dummy_variable(car_df,\"enginetype\")\ncar_df = create_dummy_variable(car_df,\"fuelsystem\")\ncar_df = create_dummy_variable(car_df,\"CarCompany\")\ncar_df.head(2)","25d72cef":"car_df_describe = car_df.describe()\ndisplay(car_df_describe)","cd65633a":"#drop_col(car_df,\"CarCompany\")\n#drop_col(car_df,\"compressionratio\")\n#drop_col(car_df,\"peakrpm\")\n#drop_col(car_df,\"stroke\")\n\n#drop_col(car_df,\"wheelbase\")\n#drop_col(car_df,\"curbweight\")\n#drop_col(car_df,\"carwidth\")\n\n#drop_col(car_df,\"citympg\")\n\ncar_df.head(2)","6cdf1e5e":"from sklearn.model_selection import train_test_split\n\n# We specify this so that the train and test data set always have the same rows, respectively\nnp.random.seed(0)\ncar_df_train , car_df_test = train_test_split(car_df,train_size=0.7,test_size=0.3,random_state=100)","1be50fc4":"car_df_train.shape","a15052dc":"car_df_test.shape","65571598":"from sklearn.preprocessing import MinMaxScaler\nscale = MinMaxScaler()","8b6c6a71":"car_df_train.columns","fcc06af7":"col_to_scale = ['wheelbase','carlength','carwidth','carheight','curbweight',\n                'boreratio',\"enginesize\",'horsepower','price','citympg',\n                'highwaympg','symboling','stroke','compressionratio','peakrpm']\n\ncar_df_train[col_to_scale] = scale.fit_transform(car_df_train[col_to_scale])\ncar_df_train.head(2)","f853be80":"y_train = car_df_train.pop('price')\nx_train = car_df_train","6ae31995":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","2932401c":"regression = LinearRegression()\nregression.fit(x_train,y_train)\n\nrfe = RFE(regression,10)\nrfe = rfe.fit(x_train,y_train)\n\nlist(zip(x_train.columns,rfe.support_,rfe.ranking_))","05fe062f":"col = x_train.columns[rfe.support_]","6f281207":"print(\"Columns selected by RFE : \", col)","0082222c":"model_count = 0\n\ndef statsmodel_summary(y_var,x_var) :\n    global model_count\n    model_count = model_count + 1\n    text = \"MODEL - \" + str(model_count)\n    header(text)\n    x_var_const = sm.add_constant(x_var) # adding constant\n    lm = sm.OLS(y_var,x_var_const).fit() # calculating the fit\n    print(lm.summary()) # print summary for analysis\n    display_vif(x_var_const.drop(['const'],axis=1))\n    return x_var_const , lm\n    \ndef display_vif(x) :\n    # Calculate the VIFs for the new model\n    from statsmodels.stats.outliers_influence import variance_inflation_factor\n    vif = pd.DataFrame()\n    X = x\n    vif['Features'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.set_index(\"Features\")\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    df = pd.DataFrame(vif.VIF).style.applymap(color_code_vif_values)\n    display(df)\n  ","bb6f3ef2":"x_new = x_train[col]\nx_new,lm_new = statsmodel_summary(y_train,x_new)","0b5372fc":"x_new,lm_new = statsmodel_summary(y_train,drop_col(x_new,'twelve'))","bb04bb97":"x_new,lm_new = statsmodel_summary(y_train,drop_col(x_new,\"curbweight\"))\n","3eed0162":"x_new,lm_new = statsmodel_summary(y_train,drop_col(x_new,\"boreratio\"))\n","82fc61fa":"x_new,lm_new = statsmodel_summary(y_train,drop_col(x_new,\"Porsche\"))","d58f1a3d":"x_new,lm_new = statsmodel_summary(y_train,drop_col(x_new,\"carwidth\"))","5ea660d4":"x_new,lm_new = statsmodel_summary(y_train,drop_col(x_new,\"three\"))","e7152b05":"x_new.head(2)","c19436ae":"y_train_price = lm_new.predict(x_new)\nfig = plt.figure(figsize=(9,6))\nsns.distplot((y_train - y_train_price), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)   \nplt.show()","0f55aab3":" car_df_test.head(2)","be9d53dd":"car_df_test[col_to_scale] = scale.transform(car_df_test[col_to_scale])\ncar_df_test.head(2)","253e0bbd":"y_test = car_df_test.pop('price')\nx_test = car_df_test","529ba1b5":"final_features = list(x_new.columns)\nfinal_features.remove('const')\nprint(final_features)","7f8a3bd6":"# Creating X_test_new dataframe by dropping variables from X_test\nx_test_new = x_test.filter(final_features)\n\n# Adding a constant variable \nx_test_new = sm.add_constant(x_test_new)\n\n# Making predictions\ny_pred = lm_new.predict(x_test_new)","1c511eaa":"from sklearn.metrics import r2_score \nr2 = r2_score(y_test, y_pred)\nlog(\"R-square calculated for test data is : \"+str(r2))\n","7f2713dd":"x_test.head()","06f78db6":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure(figsize=(9,6))\nplt.scatter(y_test,y_pred)\nfig.suptitle('Price [ Test Vs Predicted ]', fontsize=20)              # Plot heading \nplt.xlabel('Test', fontsize=18)                          # X-label\nplt.ylabel('Predicted', fontsize=18)                          # Y-label\nplt.show()","b256d3d4":"print(lm_new.summary())","7d633ebd":"### Meta Data Helper Utilities ","ce635b94":"> > <span style='color:blue'> Analysis : Looking at the above heat map we can see that above inferenses drawn between price and other features holds true.<br> Positive Relation with Price : wheelbase,carlenght,carwidth,curbweight,enginesize,boreratio,horesepower\n<\/span>","855a9d83":"### Independent Variable","9c974c84":"[](http:\/\/)> <span style='color:blue'> \n    Analyis - Considerable change in P-value for **three** <br>\n    Next Step - Deleting features **three** where p > 0.05.  <br> \n    Model Attribute - Adj. R-squared:0.828 (reduced by minor percentage)\n<\/span>   ","5d1228f5":"#### Below are the features which are not related to price because they are  - \n1. Dependent on other variable (muticolinearity)\n2. No visual variance with respect to price.\n\n`But as we are using RFE method below features will not be deleted manually and RFE will automatically identify and help us in eliminating the same.` ","ffe75cae":"# Case Study : Car Price Prediction using Linear Regression ","e94f0314":"## Model Evaluation","c2f373a6":"# Final Model Summary ","07641347":"### Dependent Variable","b412e7eb":"## Rescaling the Train dataset\n> It is extremely important to rescale the variables so that they have a comparable scale. If we don't have comparable scales, then some of the coefficients as obtained by fitting the regression model might be very large or very small as compared to the other coefficients. This might become very annoying at the time of model evaluation. So it is advised to use standardization or normalization so that the units of the coefficients obtained are all on the same scale. As you know, there are two common ways of rescaling:\n1. Min-Max scaling\n2. Standardisation (mean-0, sigma-1)<br><br>\nThis time, we will use **MinMax scaling**.","9b574f3d":"[](http:\/\/)> <span style='color:blue'> \n    Analyis - Delete feature with highest p value <br>\n    Next Step - [ P-value > 0.05 ] value for feature **twelve** is highest and needs to be deleted to create stable model. <br> \n    Model Attribute - Adj. R-squared:0.906 \n<\/span>","5025516c":"## Meta data analysis for loaded dataset","d7e5c092":"> <span style='color:blue'>Analysis : citympg and highwaympg have highest dependent (0.97) on each other and we have to keep track of them to avoid issues with muticoliniarity.<\/span>","74578353":"**Looking at above data we found that few company names are same but are misspelt.** <br>\n`1. 'Maxda', 'Mazda' --> Mazda`<br>\n`2. 'Porsche','Porcshce' --> Porsche`<br>\n`3. 'Toyota', 'Toyouta' --> Toyota`<br>\n`4. 'Vokswagen', 'Volkswagen' --> Volkswagen`<br>\n`5. 'Vw','Volvo' --> Volvo`<br>","48ed613e":"# Model building and appropriate features selection","6333e843":"### Problem Statement\n\n>A Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.<br><br>\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. <br>\nThe company wants to know:<br>\n1. Which variables are significant in predicting the price of a car\n2. How well those variables describe the price of a car<br><br>\nBased on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the Americal market.  <br>","d9daa611":"# Reading and Understanding the Data\n> **Dependent Variable**\n- Visualization of  Price\n\n> **Independent Variable**\n- Visualising Numeric Variables\n- Visualising Categorical Variables\n","38f7fd70":"* > <span style='color:blue'> \n    Analyis - no change in P-value and all features are in range<br>\n    Next Step - Deleting features carwidth where VIF > 5.  <br> \n    Model Attribute - Adj. R-squared:0.892 (reduced by minor percentage)\n<\/span>    ","fb0d7824":"# Making Predictions","3782af32":"**1. Looking at correlation between Car Dimensions Specific Variable i.e. weight , height etc**","de9a351d":"**<span style='color:blue'> We can see that the equation of our best fitted line is:**<br>\n\nprice = 0.2024 - ( 0.2872 * enginelocation ) + (1.1880 * enginesize ) + (0.2516 * rotor) + (0.1988 * Bmw)\n<\/span>                   ","588f0842":"> > <span style='color:blue'> \n    Analyis - [ P-value > 0.05 ] all feature are in acceptable range.<br>\n    Next Step - Deleting features **curbweight** where VIF is very high. <br> \n    Model Attribute** - Adj. R-squared:0.906 \n<\/span>    ","ee5b6dba":"# Data Cleanup \/ Rectification of existing values\n> 1. Looking at the above dataset we need **not to replace any nan\/null values** as our data is cleaned. \n2. But if we look at categorical values we see <b>drivewheel<\/b> columns which has values rwd, fwd, 4wd. Out of these fwd & 4wd represnt same values and hence 4wd needs to be susbtitued with fwd. \n3. Create out new columns with as **CarManufacturer** using **CarName** columns.\n4. Drop column - car_ID as it will not add any value.","22aa6133":"#### Plotting heatmap for numeric variables","77443d73":"# Model Building Approach\n> We will be using mixed approach to find the relevent features. \n1. Identify features using RFE (Recurcive Feature Elimination)\n2. Manual approach to find correct fit","11e8a2f4":"#### Checking Multicolinearity b\/w Independent variable","93137ed4":"**2. Looking at correlation between Car Performance Specifc Varibale**","14df85b8":"### Infrences Conclusion","93074497":"# Data Preparation\n\n> 1. You can see that your dataset has many columns with values only two values.\n2. But in order to fit a regression line, we would need numerical values and not string. Hence, we need to convert them to 1s and 0s accordingly.\n3. Convert higer categorical variables using **Dummy Variables**","f4d715ff":"* > <span style='color:blue'> \n    Analyis - No considerable change in P- value but boreratio have p-value 0.77 which is greater than p > 0.05<br>\n    Next Step - Deleting features boreratio.  <br> \n    Model Attribute - Adj. R-squared:0.895\n<\/span>    ","70ef6756":"> <span style='color:blue'>Analysis : Horsepower and enginesize are highly correlated and we need to select one from them. Boreratio is not related as will be included in model building<\/span>","0606a08b":"[](http:\/\/)> <span style='color:blue'> \n    Analyis - P-value chnaged for few features <br>\n    Next Step - Deleting features Porsche where p > 0.05.  <br> \n    Model Attribute - Adj. R-squared:0.893 (reduced by minor percentage)\n<\/span>    ","2ed53910":"#### 2. Visualising Categorical Variables","48931828":"### Recidual Analysis","fcedcda7":"### Calculate the R-squared score on the test dataset","6471d626":"### Loading Moduels & Libraries ","da69a8a4":"### Business Goal \n\n>Is is required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market. ","71ec18c8":"----","8da2dc62":"#### Plotting Pair Plot for better visualizations ","43a285bf":"### REF (Recurcive Feature Elimination)\n> Usin RFE gives us an automated way of selectig important attributes which can influence dependent variable. We will be using mixed apporach here and as a first step we will simply use features that are returned by RFE as a starting model. On the contrary after visual analyis we can see a lot of field have abosolutly no relation with Price and can be removed before we even start building model. But as per the advice from TA , I have not removed any field manually and my model is completely rely on RFE output.Below is one of the many models that I tried with various combinations and selected the best one for submittion.  ","74acdec1":"**Looking at the above table ,we can have to covert below columns in 1's and 0's**\n>1. fueltype   {\"gas\": 1, \"diesel\": 0}\n2. aspiration {\"std\": 1, \"turbo\": 0}\n3. doornumber {\"two\": 1, \"four\": 0}\n4. drivewheel {\"rwd\": 1, \"fwd\": 0}\n5. enginelocation {\"front\": 1, \"rear\": 0}","c5f3e54c":"# Preparing Train and Test data","58d49071":"> <span style='color:blue'>Analysis : Wheelbase , carlength , carwidth and carweight [ 0.80 - 0.88 ] are higly correlated and we have to select one out of them. Carheight is not correlated and will not affect model buildingin negative way<\/span>","0c0dc309":"### Dividing into x_train and y_train sets for the model building","d2aa08c3":"#### 1. Visualising Numeric Variables\n- Analyizing trends by looking pairplot of all the **Independent variables Vs Dependent variable**.","dd05928b":"#### Looking at the above table , we can see other varible and checkig any correlatoin between them and price","909ea821":"### Group independent varible for correlation analysis","50a5146a":"><span style='color:blue'>Analysis : <br>Average price of diesel car are more expensive than gas but gas have more expensive car range.<br>\n>Cars with turbo aspiration have generally expensive.<br>\n>Most expensive cars have two doors.<br>\n>Rear engine cars are more expensive than front engine location <br><\/span>","7480b8ec":"### Now let's use our model to make predictions.\n","ede3c256":"#### Creating dummy variables","89d1671a":"### Dummy Variables","186a96bc":"## Deleting Features ","2045c95c":"**Looking at the above table , we can see below columns have two unique values**\n>1. fueltype\n2. aspiration\n3. doornumber\n4. drivewheel\n5. enginelocation \n","40ceb974":"# Loading dataset for car price analysis","29abdcd9":"* > <span style='color:blue'> \n    Analyis - All features have p-value < 0.05 <br>\n    Next Step - Final model created and will be used against test data<br> \n    Model Attribute - Adj. R-squared:0.828\n<\/span>  ","a714dab5":"> <span style='color:blue'> R-sqaured and Adjusted R-squared (extent of fit) - 0.833 and 0.828 - 83% variance explained.<\/span>\n\n> <span style='color:blue'>F-stats and Prob(F-stats) (overall model fit) - 172.3 and 1.27e-52(approx. 0.0) - Model fit is significant and explained 83% variance is just not by chance.<\/span>\n\n> <span style='color:blue'>p-values - p-values for all the coefficients seem to be less than the significance level of 0.05. - meaning that all the predictors are statistically significant.<\/span>\n\n> <span style='color:blue'>AIC and BIC Values - We can see that there is a diffrence between AIC ( 280.0 ) and BIC ( 265.2 ) fields and BIC has lesser value due to feature penalty and are in range.<\/span>\n\n> <span style='color:blue'>Conclusion : As per final model attributes which are best suited for predicting **Price** are - <br>  1.enginelocation <br>  2.enginesize<br>  3.rotor<br>  4.Bmw<br>\n<\/span>","2ea97d5f":"> <span style='color:blue'><b>Analysis<b> - Dataset is clean and no NaN susbtitution is required<\/span>","50075ec7":"[](http:\/\/)> <span style='color:blue'> \n    Analyis - Error terms are distributed around zero which signifies that model prediction is not by chance. <br>\n<\/span>  "}}