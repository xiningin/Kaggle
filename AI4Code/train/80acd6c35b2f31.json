{"cell_type":{"d794b6f0":"code","0f582918":"code","bb6bf98d":"code","5f405e5c":"code","a9aba560":"code","56a2a5a0":"code","6d96ff6b":"code","b435efb1":"code","8174a0f2":"code","333eda07":"code","3be54082":"code","9d64d077":"code","bdda25f4":"code","7a966287":"code","18e0f33e":"code","3ceb51f9":"code","63c5eb6e":"code","53d3a2f4":"code","23f72694":"code","878f4730":"code","c3894c29":"markdown","a672b96b":"markdown","135333a2":"markdown","17926d1f":"markdown","afe65a5e":"markdown","84ebcddb":"markdown","75da9679":"markdown"},"source":{"d794b6f0":"# Dataframes and numpy\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# ML libaries\nimport torch # pytorch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.utils.rnn as rnnu # For packing sequences https:\/\/pytorch.org\/docs\/0.4.0\/nn.html#torch.nn.utils.rnn.pack_padded_sequence\n\n# Progress bars\nfrom tqdm import tqdm, tqdm_notebook\ntqdm_notebook().pandas()\n\n# Natural language processing (nlp)\nimport spacy\n\n# Plotting\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\nBATCH_SIZE = 16","0f582918":"foo = pd.read_csv(\"..\/input\/train.tsv\", sep='\\t')\nfoo.iloc[0:100].to_csv(\"..\/toycase.tsv\", sep='\\t')","bb6bf98d":"# natural language processor, see spacy documentation. Disable some unneeded components\nnlp = spacy.load(\"en_core_web_lg\",disable = [\"parser\",\"tagger\",\"ner\"])\n\ndef sentenceToNNInput(sentence):\n    doc = nlp(sentence) # Tokenize\n    return [torch.tensor(token.vector) for token in doc]","5f405e5c":"# Creating the dataset https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html\nclass review_dataset(Dataset):\n    def __init__(self,path):\n        df = pd.read_csv(path, sep='\\t')\n        self.len = df.shape[0]\n        if 'Sentiment' in df.columns: # Only training set has a sentiment column\n            self.labels = torch.from_numpy(df[\"Sentiment\"].values)\n        else:\n            self.labels = torch.zeros(self.len)    \n        self.inputs = df[\"Phrase\"].progress_apply(sentenceToNNInput).values.tolist()\n        self.lengths = torch.tensor([len(elm) for elm in self.inputs])\n        self.phraseid = torch.from_numpy(df[\"PhraseId\"].values)\n\n    def __getitem__(self, index):\n        # Returns a list of tuples (input,label,length) where label and lenghts are scalar (0-D torch tensor) \n        # and input is a list of 1-D Tensors representing a word each\n        return self.inputs[index], self.labels[index], self.lengths[index], self.phraseid[index]\n\n    def __len__(self):\n        return (self.len)","a9aba560":"#train = review_dataset(\"..\/toycase.tsv\")\ntrain = review_dataset(\"..\/input\/train.tsv\")","56a2a5a0":"#train = pd.read_csv(\"..\/input\/train.tsv\", sep='\\t')\n#test = pd.read_csv(\"..\/input\/test.tsv\", sep='\\t')\n#toycase = pd.read_csv(\"..\/toycase.tsv\", sep='\\t')\n#train.head()\ntrain[7][3]","6d96ff6b":"# For packing the batches need to be sorted in decreasing sentence lengths\ndef sort_batch(X, y, lengths):\n    lengths, indx = lengths.sort(dim=0, descending=True)\n    X = X[indx]\n    y = y[indx]\n    return X, y, lengths","b435efb1":"# Collate function so that dataloader works with inputs with variable sequence lengths\ndef my_collate(batch):\n    inputs = [torch.stack(x[0]) for x in batch]\n    labels = torch.tensor([x[1] for x in batch])\n    lengths = torch.tensor([x[2] for x in batch])\n    inputs = rnnu.pad_sequence(inputs,batch_first = True) # Pad sentences so they have the same length\n    inputs, labels, lengths = sort_batch(inputs, labels, lengths) # Sort batches for packing\n    return inputs, labels, lengths","8174a0f2":"split_lengths = [int(len(train)*0.8), int(len(train)*0.2)]\ntrain, dev = torch.utils.data.dataset.random_split(train, split_lengths)\ntrain_loader = DataLoader(dataset = train, batch_size = BATCH_SIZE, collate_fn = my_collate)\ndev_loader = DataLoader(dataset = dev, batch_size = BATCH_SIZE, collate_fn = my_collate)\n# Enumerating over this loader like this -> for i, data in enumerate(trainloader)\n# returns data in the tuple (inputs, labels, lenghts) where inputs, labels and lengths all are\n# of length BATCH_SIZE \n","333eda07":"#next(iter(train_loader))","3be54082":"class myLSTM(nn.Module):\n    def __init__(self,embedding_dim, n_hidden, n_out):\n        super(myLSTM, self).__init__()\n        self.n_hidden,self.n_out =  n_hidden, n_out\n        self.lstm = nn.LSTM(embedding_dim, n_hidden, batch_first = True) # LSTM layer\n        self.out = nn.Linear(n_hidden, n_out) # Output layer, converting LSTM hidden states to predictions\n        \n    def forward(self, sentences, lengths):\n        # Packing and padding to save some computation time\n        sentences = rnnu.pack_padded_sequence(sentences, lengths, batch_first = True)  # !! MISSING sorting in decreasing order\n        \n        lstm_out, _ = self.lstm(sentences) # Input should have shape (#Batches, Sequence Length, Vector Size)\n        lstm_out, _ = rnnu.pad_packed_sequence(lstm_out,batch_first = True)\n        \n        # For classification we only need the last timesteps. Code for selecting last timesteps from: https:\/\/blog.nelsonliu.me\/2018\/01\/24\/extracting-last-timestep-outputs-from-pytorch-rnns\/\n        idx = (lengths - 1).view(-1, 1).expand(len(lengths), lstm_out.size(2))\n        idx = idx.unsqueeze(1).type(torch.LongTensor)\n        if lstm_out.is_cuda:\n            idx = idx.cuda(lstm_out.data.get_device())\n        # Shape: (batch_size, lstm_hidden_dim)\n        last_output = lstm_out.gather(1, idx).squeeze(1)\n        \n        # Linear layer\n        return self.out(last_output)\n    ","9d64d077":"def fit(model,train_loader, optimizer, criterion):\n    model.train()\n    device = next(model.parameters()).device.index\n    losses = []\n    for i, data in enumerate(train_loader):\n        sentences = data[0].type(torch.FloatTensor).cuda(device)\n        labels = data[1].type(torch.LongTensor).cuda(device)\n        lengths = data[2].type(torch.LongTensor).cuda(device)\n        # sorting and padding moved to collate_fn\n        #sentences, labels, lengths = sort_batch(sentences, labels, lengths) # Sort batches for packing\n        #sentences = rnnu.pad_sequence(sentences,batch_first = True) # Pad sentences so they have the same length\n        \n        pred_labels = model(sentences, lengths)\n        loss = criterion(pred_labels, labels)\n        losses.append(loss.item())\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    avg_loss = sum(losses)\/len(losses)\n    return avg_loss","bdda25f4":"def eval(model,test_loader):\n    model.eval()\n    device = next(model.parameters()).device.index\n    pred_labels = []\n    real_labels = []\n\n    for i, data in enumerate(test_loader):\n        sentences = data[0].type(torch.FloatTensor).cuda(device)\n        labels = data[1].type(torch.LongTensor).cuda(device)\n        lengths = data[2].type(torch.LongTensor).cuda(device)\n        \n        real_labels += list(labels.cpu().detach().numpy())\n        \n        pred_label = model(sentences, lengths)\n        pred_label = list(pred_label.cpu().detach().numpy())\n        pred_labels += pred_label\n        \n    real_labels = np.array(real_labels)\n    pred_labels = np.array(pred_labels)\n    pred_labels = pred_labels.argmax(axis=1)\n    acc = sum(real_labels==pred_labels)\/len(real_labels)*100\n    \n    return acc","7a966287":"criterion = nn.CrossEntropyLoss()\nnum_epochs = 20\nembedding_dim = 300 # Number of features in the spacy word vectors\nn_hidden = 4 # TODO: Try different numbers !\nn_out = 5 # We have 5 classes (0-4)\nmodel = myLSTM(embedding_dim, n_hidden, n_out).cuda()\noptimizer = torch.optim.Adam(model.parameters()) # TODO: Try different learning rate, optimizers !","18e0f33e":"train_loss1 = []\ntest_accuracy1 = []\nfor epoch in tqdm_notebook(range(num_epochs)):\n    train_loss1.append(fit(model, train_loader, optimizer, criterion))\n    test_accuracy1.append(eval(model, dev_loader))\n","3ceb51f9":"# Plotting the loss \nplt.plot(train_loss1, 'r', linestyle='-', alpha=0.5)\nplt.title('training loss')\nplt.xlabel('epoch')\nplt.ylabel(\"loss\")\n#plt.ylim(0,1.2)\n#plt.xlim(0,250)\nplt.show()\nplt.savefig('..\/loss{}.png'.format(n_hidden))","63c5eb6e":"# Plotting test accuracy\nplt.plot(test_accuracy1, 'r', linestyle='-', alpha=0.5)\nplt.title('test accuracy')\nplt.xlabel('epoch')\nplt.ylabel(\"accuracy\")\n#plt.ylim(0,1.2)\n#plt.xlim(0,250)\nplt.show()\nplt.savefig('..\/acc{}.png'.format(n_hidden))","53d3a2f4":"test = review_dataset(\"..\/input\/test.tsv\")","23f72694":"model.eval()\ndevice = next(model.parameters()).device.index\npred_labels = []\n\nfor i, data in enumerate(tqdm_notebook(test)):\n    inputs = torch.stack(data[0]).type(torch.FloatTensor).cuda(device).unsqueeze_(0)\n    length = data[2].type(torch.FloatTensor).cuda(device).unsqueeze_(0)\n    \n    pred_label = model(inputs, length).cpu().detach().numpy().argmax()\n    pred_labels.append(pred_label)","878f4730":"phraseid = test[:][3].tolist()\nresults = pd.DataFrame(\n    {'PhraseId': phraseid,\n     'Sentiment': pred_labels})\n\nresults.to_csv(\"submission{}.csv\".format(n_hidden),index=False)","c3894c29":"**0. Importing libaries**","a672b96b":"**3. Fitting and evaluation function**","135333a2":"**2. Defining the model**\n\nLSTM ","17926d1f":"**5. Predicting scores and creating submission file**","afe65a5e":"> **X. Ressources:** \n\npack padded sequence  \n![](http:\/\/)https:\/\/medium.com\/@sonicboom8\/sentiment-analysis-with-variable-length-sequences-in-pytorch-6241635ae130\nhttps:\/\/discuss.pytorch.org\/t\/example-of-many-to-one-lstm\/1728\/2\n\nhttps:\/\/medium.com\/@florijan.stamenkovic_99541\/rnn-language-modelling-with-pytorch-packed-batching-and-tied-weights-9d8952db35a9\n\nDataloader\n\nhttps:\/\/www.youtube.com\/watch?v=zN49HdDxHi8\n\nCollate :  \nhttps:\/\/www.codefull.net\/2018\/11\/use-pytorchs-dataloader-with-variable-length-sequences-for-lstm-gru\/\n","84ebcddb":"****1. Data prep****\n\n Loading the datasets \n \n and\n \n Data pre-processing\n* Remove capitalization\n* (Remove stopwords) -> I skipped this and still got good results\n* Tokenize sentences into words\n* (Lemmazation to remove word endings) -> Not necessary with word vectors\n* Embedding words into vectors","75da9679":"**4. Training the model**\n\nParameters and training run"}}