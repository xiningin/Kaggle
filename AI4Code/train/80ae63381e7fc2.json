{"cell_type":{"2551ed28":"code","fad571fa":"code","591dbc30":"code","7f9262dd":"code","9e1f8cf7":"code","1d07abee":"code","9fe9363e":"code","53b7c747":"code","a51a3a33":"code","eb7c14d4":"code","9c2ca937":"code","4ade9fbe":"code","be3bc785":"code","90886373":"code","34a84678":"code","6a30fe22":"code","b505814f":"code","793e3420":"code","048feb63":"code","1482c86e":"code","386040bf":"code","782b5689":"code","73079bf0":"code","3295ad76":"code","95e2a821":"code","b6248e62":"code","b4b1ed2c":"code","e89261eb":"code","976f3293":"code","fe09b055":"code","d7d51f36":"code","c9e5a20e":"code","979b1739":"code","cfd511e7":"code","f1179ceb":"code","1aa8b64d":"code","5f88b9d9":"code","c692f423":"code","3cf97cba":"code","f67c85ec":"code","adf4e98f":"code","c78fac74":"code","8259340e":"code","68f73a26":"code","271cc6b3":"code","08a3968c":"code","5dfe2857":"code","bc4d5216":"code","d5c82eb9":"code","b1f26d9e":"code","15c0ea15":"code","6ffb2472":"code","728b338f":"code","1955a763":"code","0caa778e":"code","85d77600":"code","37d52107":"code","f281fc57":"code","38f53290":"code","2effa4ff":"code","5ee8dd81":"code","3983b042":"code","709dbd5f":"code","43f0a4af":"markdown","7497dc29":"markdown","98314d2b":"markdown","fb29c520":"markdown","b14cafc2":"markdown","80402854":"markdown","a245ec04":"markdown","3f540877":"markdown","6f22059e":"markdown","09f64087":"markdown","55840f37":"markdown","d252a693":"markdown","7fe88977":"markdown","75322537":"markdown","fe9f46fe":"markdown","979bad8d":"markdown","d97ce0e3":"markdown","ec3df986":"markdown","6f437a22":"markdown","8a73b31f":"markdown","7d880308":"markdown"},"source":{"2551ed28":"# Import the libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","fad571fa":"#importing data from kaggle\ncredit_df = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ncredit_df.head(5)","591dbc30":"credit_df = credit_df.drop(\"Time\", axis=1)","7f9262dd":"credit_df.head(5)","9e1f8cf7":"from sklearn import preprocessing\nscaler = preprocessing.StandardScaler()","1d07abee":"#standard scaling\ncredit_df['std_Amount'] = scaler.fit_transform(credit_df['Amount'].values.reshape (-1,1))\n\n#removing Amount\ncredit_df = credit_df.drop(\"Amount\", axis=1)","9fe9363e":"sns.countplot(x=\"Class\", data=credit_df)","53b7c747":"import imblearn\nfrom imblearn.under_sampling import RandomUnderSampler \n\nundersample = RandomUnderSampler(sampling_strategy=0.5)","a51a3a33":"cols = credit_df.columns.tolist()\ncols = [c for c in cols if c not in [\"Class\"]]\ntarget = \"Class\" ","eb7c14d4":"#define X and Y\nX = credit_df[cols]\nY = credit_df[target]\n\n#undersample\nX_under, Y_under = undersample.fit_resample(X, Y)","9c2ca937":"from pandas import DataFrame\n\ntest = pd.DataFrame(Y_under, columns = ['Class'])","4ade9fbe":"#visualizing undersampling results\nfig, axs = plt.subplots(ncols=2, figsize=(13,4.5))\nsns.countplot(x=\"Class\", data=credit_df, ax=axs[0])\nsns.countplot(x=\"Class\", data=test, ax=axs[1])\n\nfig.suptitle(\"Class repartition before and after undersampling\")\na1=fig.axes[0]\na1.set_title(\"Before\")\na2=fig.axes[1]\na2.set_title(\"After\")","be3bc785":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_under, Y_under, test_size=0.2, random_state=1)","90886373":"#importing packages for modeling\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.layers import Dropout\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import BatchNormalization\n\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_curve","34a84678":"#train the model\nmodel1 = LogisticRegression(random_state=2)\nlogit = model1.fit(X_train, y_train)","6a30fe22":"#predictions\ny_pred_Logistic = model1.predict(X_test) ","b505814f":"#scores\nprint(\"Accuracy Logistic Regression:\",metrics.accuracy_score(y_test, y_pred_Logistic))\nprint(\"Precision Logistic Regression:\",metrics.precision_score(y_test, y_pred_Logistic))\nprint(\"Recall Logistic Regression:\",metrics.recall_score(y_test, y_pred_Logistic))\nprint(\"F1 Score Logistic Regression:\",metrics.f1_score(y_test, y_pred_Logistic))","793e3420":"#print CM\nmatrix_logit = confusion_matrix(y_test, y_pred_Logistic)\ncm_logit = pd.DataFrame(matrix_logit, index=['not_fraud', 'fraud'], columns=['not_fraud', 'fraud'])\n\nsns.heatmap(cm_logit, annot=True, cbar=None, cmap=\"Blues\", fmt = 'g')\nplt.title(\"Confusion Matrix Logit\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","048feb63":"#AUC\ny_pred_logit_proba = model1.predict_proba(X_test)[::,1]\nfpr_logit, tpr_logit, _ = metrics.roc_curve(y_test,  y_pred_logit_proba)\nauc_logit = metrics.roc_auc_score(y_test, y_pred_logit_proba)\nprint(\"AUC Logistic Regression :\", auc_logit)","1482c86e":"#ROC\nplt.plot(fpr_logit,tpr_logit,label=\"Logistic Regression, auc={:.3f})\".format(auc_logit))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('Logistic Regression ROC curve')\nplt.legend(loc=4)\nplt.show()","386040bf":"logit_precision, logit_recall, _ = precision_recall_curve(y_test, y_pred_logit_proba)\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='black', label='No Skill')\nplt.plot(logit_recall, logit_precision, color='orange', label='Logistic')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend()\nplt.show()","782b5689":"#train the model\nmodel2 = SVC(probability=True, random_state=2)\nsvm = model2.fit(X_train, y_train)","73079bf0":"#predictions\ny_pred_svm = model2.predict(X_test)","3295ad76":"#scores\nprint(\"Accuracy SVM:\",metrics.accuracy_score(y_test, y_pred_svm))\nprint(\"Precision SVM:\",metrics.precision_score(y_test, y_pred_svm))\nprint(\"Recall SVM:\",metrics.recall_score(y_test, y_pred_svm))\nprint(\"F1 Score SVM:\",metrics.f1_score(y_test, y_pred_svm))","95e2a821":"#CM matrix\nmatrix_svm = confusion_matrix(y_test, y_pred_svm)\ncm_svm = pd.DataFrame(matrix_svm, index=['not_fraud', 'fraud'], columns=['not_fraud', 'fraud'])\n\nsns.heatmap(cm_svm, annot=True, cbar=None, cmap=\"Blues\", fmt = 'g')\nplt.title(\"Confusion Matrix SVM\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","b6248e62":"#AUC\ny_pred_svm_proba = model2.predict_proba(X_test)[::,1]\nfpr_svm, tpr_svm, _ = metrics.roc_curve(y_test,  y_pred_svm_proba)\nauc_svm = metrics.roc_auc_score(y_test, y_pred_svm_proba)\nprint(\"AUC SVM :\", auc_svm)","b4b1ed2c":"#ROC\nplt.plot(fpr_svm,tpr_svm,label=\"SVM, auc={:.3f})\".format(auc_svm))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('SVM ROC curve')\nplt.legend(loc=4)\nplt.show()","e89261eb":"svm_precision, svm_recall, _ = precision_recall_curve(y_test, y_pred_svm_proba)\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='black', label='No Skill')\nplt.plot(svm_recall, svm_precision, color='orange', label='SVM')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend()\nplt.show()","976f3293":"#train the model\nmodel3 = RandomForestClassifier(random_state=2)\nrf = model3.fit(X_train, y_train)","fe09b055":"#predictions\ny_pred_rf = model3.predict(X_test)","d7d51f36":"#scores\nprint(\"Accuracy RF:\",metrics.accuracy_score(y_test, y_pred_rf))\nprint(\"Precision RF:\",metrics.precision_score(y_test, y_pred_rf))\nprint(\"Recall RF:\",metrics.recall_score(y_test, y_pred_rf))\nprint(\"F1 Score RF:\",metrics.f1_score(y_test, y_pred_rf))","c9e5a20e":"#CM matrix\nmatrix_rf = confusion_matrix(y_test, y_pred_rf)\ncm_rf = pd.DataFrame(matrix_rf, index=['not_fraud', 'fraud'], columns=['not_fraud', 'fraud'])\n\nsns.heatmap(cm_rf, annot=True, cbar=None, cmap=\"Blues\", fmt = 'g')\nplt.title(\"Confusion Matrix RF\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","979b1739":"#AUC\ny_pred_rf_proba = model3.predict_proba(X_test)[::,1]\nfpr_rf, tpr_rf, _ = metrics.roc_curve(y_test,  y_pred_rf_proba)\nauc_rf = metrics.roc_auc_score(y_test, y_pred_rf_proba)\nprint(\"AUC Random Forest :\", auc_rf)","cfd511e7":"#ROC\nplt.plot(fpr_rf,tpr_rf,label=\"Random Forest, auc={:.3f})\".format(auc_rf))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('Random Forest ROC curve')\nplt.legend(loc=4)\nplt.show()","f1179ceb":"rf_precision, rf_recall, _ = precision_recall_curve(y_test, y_pred_rf_proba)\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='black', label='No Skill')\nplt.plot(rf_recall, rf_precision, color='orange', label='RF')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend()\nplt.show()","1aa8b64d":"#train the model\nmodel4 = XGBClassifier(random_state=2)\nxgb = model4.fit(X_train, y_train)","5f88b9d9":"#predictions\ny_pred_xgb = model4.predict(X_test) ","c692f423":"#scores\nprint(\"Accuracy XGB:\",metrics.accuracy_score(y_test, y_pred_xgb))\nprint(\"Precision XGB:\",metrics.precision_score(y_test, y_pred_xgb))\nprint(\"Recall XGB:\",metrics.recall_score(y_test, y_pred_xgb))\nprint(\"F1 Score XGB:\",metrics.f1_score(y_test, y_pred_xgb))","3cf97cba":"#CM matrix\nmatrix_xgb = confusion_matrix(y_test, y_pred_xgb)\ncm_xgb = pd.DataFrame(matrix_xgb, index=['not_fraud', 'fraud'], columns=['not_fraud', 'fraud'])\n\nsns.heatmap(cm_xgb, annot=True, cbar=None, cmap=\"Blues\", fmt = 'g')\nplt.title(\"Confusion Matrix XGBoost\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","f67c85ec":"#AUC\ny_pred_xgb_proba = model4.predict_proba(X_test)[::,1]\nfpr_xgb, tpr_xgb, _ = metrics.roc_curve(y_test,  y_pred_xgb_proba)\nauc_xgb = metrics.roc_auc_score(y_test, y_pred_xgb_proba)\nprint(\"AUC XGBoost :\", auc_xgb)","adf4e98f":"#ROC\nplt.plot(fpr_xgb,tpr_xgb,label=\"XGBoost, auc={:.3f})\".format(auc_xgb))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('XGBoost ROC curve')\nplt.legend(loc=4)\nplt.show()","c78fac74":"xgb_precision, xgb_recall, _ = precision_recall_curve(y_test, y_pred_xgb_proba)\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='black', label='No Skill')\nplt.plot(xgb_recall, xgb_precision, color='orange', label='XGB')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend()\nplt.show()","8259340e":"#train the model\nmodel5 = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(100,100), random_state=2)\nmlp = model5.fit(X_train, y_train)","68f73a26":"model5.get_params(deep=True)","271cc6b3":"#predictions\ny_pred_mlp = model5.predict(X_test)","08a3968c":"#scores\nprint(\"Accuracy MLP:\",metrics.accuracy_score(y_test, y_pred_mlp))\nprint(\"Precision MLP:\",metrics.precision_score(y_test, y_pred_mlp))\nprint(\"Recall MLP:\",metrics.recall_score(y_test, y_pred_mlp))\nprint(\"F1 Score MLP:\",metrics.f1_score(y_test, y_pred_mlp))","5dfe2857":"#CM matrix\nmatrix_mlp = confusion_matrix(y_test, y_pred_mlp)\ncm_mlp = pd.DataFrame(matrix_mlp, index=['not_fraud', 'fraud'], columns=['not_fraud', 'fraud'])\n\nsns.heatmap(cm_mlp, annot=True, cbar=None, cmap=\"Blues\", fmt = 'g')\nplt.title(\"Confusion Matrix MLP\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","bc4d5216":"#AUC\ny_pred_mlp_proba = model5.predict_proba(X_test)[::,1]\nfpr_mlp, tpr_mlp, _ = metrics.roc_curve(y_test,  y_pred_mlp_proba)\nauc_mlp = metrics.roc_auc_score(y_test, y_pred_mlp_proba)\nprint(\"AUC MLP :\", auc_mlp)","d5c82eb9":"#ROC\nplt.plot(fpr_mlp,tpr_mlp,label=\"MLPC, auc={:.3f})\".format(auc_mlp))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('Multilayer Perceptron ROC curve')\nplt.legend(loc=4)\nplt.show()","b1f26d9e":"mlp_precision, mlp_recall, _ = precision_recall_curve(y_test, y_pred_mlp_proba)\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='black', label='No Skill')\nplt.plot(mlp_recall, mlp_precision, color='orange', label='MLP')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend()\nplt.show()","15c0ea15":"#train the model\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(29,), activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(16, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(8, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(4, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(1, activation='sigmoid'))","6ffb2472":"#train the model\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(29,), activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(16, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(8, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(4, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(1, activation='sigmoid'))","728b338f":"opt = tf.keras.optimizers.Adam(learning_rate=0.001) #optimizer\n\nmodel.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy']) #metrics","1955a763":"earlystopper = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, patience=15, verbose=1,mode='auto', baseline=None, restore_best_weights=False)","0caa778e":"history = model.fit(X_train.values, y_train.values, epochs = 6, batch_size=5, validation_split = 0.15, verbose = 0,\n                    callbacks = [earlystopper])\nhistory_dict = history.history","85d77600":"loss_values = history_dict['loss']\nval_loss_values=history_dict['val_loss']\nplt.plot(loss_values,'b',label='training loss')\nplt.plot(val_loss_values,'r',label='val training loss')\nplt.legend()\nplt.xlabel(\"Epochs\")","37d52107":"accuracy_values = history_dict['accuracy']\nval_accuracy_values=history_dict['val_accuracy']\nplt.plot(val_accuracy_values,'-r',label='val_accuracy')\nplt.plot(accuracy_values,'-b',label='accuracy')\nplt.legend()\nplt.xlabel(\"Epochs\")","f281fc57":"#predictions\ny_pred_nn = model.predict_classes(X_test)","38f53290":"#scores\nprint(\"Accuracy Neural Net:\",metrics.accuracy_score(y_test, y_pred_nn))\nprint(\"Precision Neural Net:\",metrics.precision_score(y_test, y_pred_nn))\nprint(\"Recall Neural Net:\",metrics.recall_score(y_test, y_pred_nn))\nprint(\"F1 Score Neural Net:\",metrics.f1_score(y_test, y_pred_nn))","2effa4ff":"#CM matrix\nmatrix_nn = confusion_matrix(y_test, y_pred_nn)\ncm_nn = pd.DataFrame(matrix_nn, index=['not_fraud', 'fraud'], columns=['not_fraud', 'fraud'])\n\nsns.heatmap(cm_nn, annot=True, cbar=None, cmap=\"Blues\", fmt = 'g')\nplt.title(\"Confusion Matrix Neural Network\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","5ee8dd81":"#AUC\ny_pred_nn_proba = model.predict_proba(X_test)\nfpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test,y_pred_nn_proba)\nauc_keras = auc(fpr_keras, tpr_keras)\nprint('AUC Neural Net: ', auc_keras)","3983b042":"#ROC\nplt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('Neural Net ROC curve')\nplt.legend(loc='best')\nplt.show()","709dbd5f":"nn_precision, nn_recall, _ = precision_recall_curve(y_test, y_pred_nn_proba)\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='black', label='No Skill')\nplt.plot(nn_recall, nn_precision, color='orange', label='TF NN')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend()\nplt.show()","43f0a4af":"Classification metrics for Random Forest (rounded down) :\n- Accuracy : 0.95\n- F1 score : 0.93\n- AUC : 0.97","7497dc29":"Time varible is not needed for do classification, I'm gonna remove the feature from the dataset :","98314d2b":"Classification metrics for Multi Layer Perceptron (rounded down) :\n- Accuracy : 0.95\n- F1 score : 0.94\n- AUC : 0.98","fb29c520":"As the dataset is highly imbalanced and we don't want bias in the predictions. I will proceed to random undersampling with the danger and that is posible, or even likely, that useful information will be deleted.\n\nWe gonna use the package imblearn with RandomUnderSampler function to do undersampling. ","b14cafc2":"# Multilayer Neural Network with Tensorflow\/Keras","80402854":"Classification metrics for SVM (rounded down) :\n- Accuracy : 0.94\n- F1 score : 0.92\n- AUC : 0.97","a245ec04":"# Credit Card Fraud Detection: context \n\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n\n## Content\n\nThe dataset contains transactions made by credit cards in September 2013 by European cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\nGiven the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\n\n","3f540877":"# Data processing and undersampling ","6f22059e":"Classification metrics for Neural Network (rounded down) :\n- Accuracy : 0.95\n- F1 score : 0.94\n- AUC : 0.98","09f64087":"# We gonna make the modeling","55840f37":"Now look the class","d252a693":"# Ensemble learning : Bagging (Random Forest) ","7fe88977":"Classification metrics for XGBoost (rounded down) :\n- Accuracy : 0.95\n- F1 score : 0.93\n- AUC : 0.97","75322537":"# Multi Layer Perceptron","fe9f46fe":"Now our dataset is balanced\n\nWe're gonna split the data intro train and test samples","979bad8d":"Classification metrics for Logistic Regression (rounded down):\n\n- Accuracy : 0.94\n- F1 score : 0.92\n- AUC : 0.96","d97ce0e3":"We gonna use the StandardScaler function from sklearn to standardize Amount variable before modelling. Then, we just have to drop the old feature:","ec3df986":"# 4. Ensemble learning : Boosting (XGBoost)","6f437a22":"# Logistic Regression","8a73b31f":"## The winner is Neuronal network: \n\nThe multilayer neural network has the best performance according to our three most important classification metrics (Accuracy, F1-score and AUC). The Multi Layer Perceptron from sklearn is the one that minimizes the most the false negatives so I decided to keep this model to predict credit card frauds. It's very important that a bank do not miss frauds so minimizing false negatives rate is essential. ","7d880308":"# Support Vector Machine"}}