{"cell_type":{"d8fb3663":"code","abb2cb90":"code","44f23325":"code","3994b24a":"code","c89e1eaf":"code","d4298d55":"code","92d8cf28":"code","4fa2e7c0":"code","67ba7df9":"code","af7b47c5":"code","83a7e64a":"code","2695001a":"code","83daecd0":"code","0b4cfcd5":"code","5ccc1d8a":"code","fb834ec7":"markdown","285017c4":"markdown","0f2df0de":"markdown","5c0fbc86":"markdown","e42a8f15":"markdown","5f793bd1":"markdown","beda1074":"markdown","b6309901":"markdown","72be357d":"markdown","2d3ea26b":"markdown","c37ee0bc":"markdown"},"source":{"d8fb3663":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","abb2cb90":"df_train=pd.read_csv('\/kaggle\/input\/1056lab-student-performance-prediction\/train.csv',index_col=0)\ndf_test=pd.read_csv('\/kaggle\/input\/1056lab-student-performance-prediction\/test.csv',index_col=0)","44f23325":"df_train.isnull().sum()","3994b24a":"df_test.isnull().sum()","c89e1eaf":"import seaborn as sns\nfrom matplotlib import pyplot\n\nsns.set_style(\"darkgrid\")\npyplot.figure(figsize=(20, 20))\nsns.heatmap(df_train.corr(), square=True, annot=True)","d4298d55":"df_train=df_train[['Medu','Fedu','failures','higher','studytime','age','traveltime','internet','romantic','Dalc','G3']]\ndf_test=df_test[['Medu','Fedu','failures','higher','studytime','age','traveltime','internet','romantic','Dalc']]","92d8cf28":"df_train['higher']=pd.get_dummies(df_train['higher'], drop_first=True)\ndf_test['higher']=pd.get_dummies(df_test['higher'], drop_first=True)\ndf_train['internet']=pd.get_dummies(df_train['internet'], drop_first=True)\ndf_test['internet']=pd.get_dummies(df_test['internet'], drop_first=True)\ndf_train['romantic']=pd.get_dummies(df_train['romantic'], drop_first=True)\ndf_test['romantic']=pd.get_dummies(df_test['romantic'], drop_first=True)","4fa2e7c0":"X=df_train.drop('G3',axis=1).values\ny=df_train['G3'].values","67ba7df9":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)\nlen(X_train),len(X_valid),len(y_train),len(y_valid)","af7b47c5":"from sklearn.metrics import mean_squared_error\nfrom sklearn.svm import SVR\nmodel = SVR(gamma='auto')\nmodel.fit(X_train, y_train)\npredict = model.predict(X_valid)\nnp.sqrt(mean_squared_error(predict, y_valid))","83a7e64a":"import optuna\ndef objective(trial):\n    #kernel = trial.suggest_categorical('kernel', ['rbf', 'linear'])\n    svr_c = trial.suggest_loguniform('svr_c', 1e0, 1e2)\n    epsilon = trial.suggest_loguniform('epsilon', 1e-1, 1e1)\n    #gamma = trial.suggest_loguniform('gamma', 1e-3, 3e1)\n    model = SVR(kernel='rbf', C=svr_c, epsilon=epsilon, gamma='auto')\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_valid)\n    return mean_squared_error(y_valid, y_pred)\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100)\nstudy.best_params","2695001a":"svr_c = study.best_params['svr_c']\nepsilon = study.best_params['epsilon']\nmodel = SVR(kernel='rbf', C=svr_c, epsilon=epsilon, gamma='auto')\nmodel.fit(X_train, y_train)\npredict = model.predict(X_valid)\nnp.sqrt(mean_squared_error(predict, y_valid))","83daecd0":"X=df_train.drop('G3',axis=1).values\ny=df_train['G3'].values\n#model = RandomForestRegressor(criterion='mse', max_depth=max_depth, n_estimators=n_estimators, random_state=0,n_jobs=-1)\nmodel = SVR(kernel='rbf', C=svr_c, epsilon=epsilon, gamma='auto')\n#model = DecisionTreeRegressor(criterion='mse',splitter=splitter, max_depth=max_depth, min_samples_split=min_samples_split, random_state=0)\nmodel.fit(X, y)","0b4cfcd5":"X_test = df_test.values\npredict = model.predict(X_test)","5ccc1d8a":"submit = pd.read_csv('\/kaggle\/input\/1056lab-student-performance-prediction\/sampleSubmission.csv')\nsubmit['G3'] = predict\nsubmit.to_csv('submission.csv', index=False)","fb834ec7":"#criterion = study.best_params['criterion']\nmax_depth = study.best_params['max_depth']\nn_estimators = study.best_params['n_estimators']\nmodel = RandomForestRegressor(criterion='mse', max_depth=max_depth, n_estimators=n_estimators, random_state=0,n_jobs=-1)\nmodel.fit(X_train, y_train)\npredict = model.predict(X_valid)\nnp.sqrt(mean_squared_error(predict, y_valid))","285017c4":"import optuna\ndef objective(trial):\n    #criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n    max_depth = trial.suggest_int('max_depth', 1, 30)\n    n_estimators = trial.suggest_int('n_estimators',10,300)\n    model = RandomForestRegressor(criterion='mse', max_depth=max_depth, n_estimators=n_estimators, random_state=0,n_jobs=-1)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_valid)\n    return mean_squared_error(y_valid, y_pred)\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100)\nstudy.best_params","0f2df0de":"**\u56de\u5e30\u6728**","5c0fbc86":"**RandomForest**","e42a8f15":"**\u30c0\u30df\u30fc\u5909\u6570**","5f793bd1":"**\u7279\u5fb4\u9078\u629e**","beda1074":"from sklearn.metrics import mean_squared_error\nfrom sklearn.tree import DecisionTreeRegressor\nmodel = DecisionTreeRegressor()\nmodel.fit(X_train, y_train)\npredict = model.predict(X_valid)\nnp.sqrt(mean_squared_error(predict, y_valid))","b6309901":"**SVR**","72be357d":"splitter = study.best_params['splitter']\nmax_depth = study.best_params['max_depth']\nmin_samples_split = study.best_params['min_samples_split']\nmodel = DecisionTreeRegressor(criterion='mse',splitter=splitter, max_depth=max_depth, min_samples_split=min_samples_split, random_state=0)\nmodel.fit(X_train, y_train)\npredict = model.predict(X_valid)\nnp.sqrt(mean_squared_error(predict, y_valid))","2d3ea26b":"from sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nmodel=RandomForestRegressor(random_state=0)\nmodel.fit(X_train, y_train)\npredict = model.predict(X_valid)\nnp.sqrt(mean_squared_error(predict, y_valid))","c37ee0bc":"import optuna\ndef objective(trial):\n    #criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n    splitter = trial.suggest_categorical('splitter',['best','random'])\n    max_depth = trial.suggest_int('max_depth',1,20)\n    min_samples_split = trial.suggest_int('min_samples_split',2,20)\n    model = DecisionTreeRegressor(criterion='mse',splitter=splitter, max_depth=max_depth, min_samples_split=min_samples_split, random_state=0)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_valid)\n    return mean_squared_error(y_valid, y_pred)\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100)\nstudy.best_params"}}