{"cell_type":{"131f7cbb":"code","164e1d6d":"code","faaf5cd9":"code","565d0e11":"code","b6186c43":"code","d22a2ca7":"code","21bc10ac":"code","c53703c1":"code","e113cbf1":"code","56fb4491":"code","ba917b0c":"code","bd39a163":"code","123fd458":"code","b962014c":"code","176e06fc":"code","68049690":"code","063caab9":"code","1acaf3bc":"code","f160cab1":"code","85a37dd6":"code","355a902d":"code","3474a095":"code","29bcf079":"code","5fec07f0":"code","e5955bca":"code","379433bb":"code","3c47d958":"code","5829ac10":"code","97c6ce93":"code","dfb9f56d":"code","94caf89a":"code","b0ca1ab4":"code","7e689fb8":"markdown","ced9bbc7":"markdown","1833e8bf":"markdown","96b67c09":"markdown","acfde1a6":"markdown"},"source":{"131f7cbb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas.core.algorithms as algos\nfrom pandas import Series\nimport scipy.stats.stats as stats\nimport re\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import f1_score, confusion_matrix,roc_auc_score, roc_curve, auc, \\\n            classification_report, recall_score, precision_recall_curve\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nimport shap\nfrom sklearn import preprocessing\nshap.initjs()\n\nimport os# data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","164e1d6d":"train = pd.read_csv(r'\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv(r'\/kaggle\/input\/titanic\/test.csv')","faaf5cd9":"train.head()","565d0e11":"# plot countplots \ncat_col = ['Sex','SibSp','Embarked','Parch','Pclass']\nplt.figure(figsize=(14, 12), dpi=100)\nfor i, feature in enumerate(cat_col):\n    plt.subplot(3, 3, i+1)\n    sns.countplot(data=train, x=feature)\n    \nsns.despine()","b6186c43":"# plot countplots \ncat_col = ['Sex','SibSp','Embarked','Parch','Pclass']\nplt.figure(figsize=(14, 12), dpi=100)\nfor i, feature in enumerate(cat_col):\n    plt.subplot(3, 3, i+1)\n    sns.countplot(data=train, x=feature, hue='Survived')\n    \nsns.despine()","d22a2ca7":"sns.distplot(train[\"Age\"])\n\n","21bc10ac":"sns.distplot(train[\"Fare\"])#skewed","c53703c1":"train.isna().sum()","e113cbf1":"#Treating Missing values  in Age and Embarked, Cabin will be treated by WOE\ntrain['Age'] = train['Age'].fillna(train['Age'].median())\ntrain['Embarked'] = train['Embarked'].fillna(train['Age'].mode())\n\ntest['Age'] = test['Age'].fillna(train['Age'].median())\ntest['Embarked'] = test['Embarked'].fillna(train['Age'].mode())","56fb4491":"#Calculatiing number of relatives\ntrain['relatives'] = train['SibSp'].astype(int) + train['Parch'].astype(int)\ntest['relatives'] = test['SibSp'].astype(int) + test['Parch'].astype(int)\n","ba917b0c":"train['title'] = train.Name.str.extract(r',\\s*([^\\.]*)\\s*\\.', expand=False)\ntrain['title'].unique()","bd39a163":"titles = ['Mr', 'Mrs', 'Miss', 'Master','Dr','Ms','Lady', 'Sir','Col', 'Capt','Major']\ntrain.loc[~train['title'].isin(titles),'title'] = 'Others'","123fd458":"#As 'Dr','Ms','Lady', 'Sir','Col', 'Capt','Major' have low count lets put them also in others\ntrain['title'].value_counts()","b962014c":"titles = ['Mr', 'Mrs', 'Miss', 'Master','Dr']\ntrain.loc[~train['title'].isin(titles),'title'] = 'Others'","176e06fc":"train['title'].value_counts()","68049690":"sns.countplot(data=train, x='title', hue='Survived')","063caab9":"#Apply same thing on test\ntest['title'] = test.Name.str.extract(r',\\s*([^\\.]*)\\s*\\.', expand=False)\ntest.loc[~test['title'].isin(titles),'title'] = 'Others'","1acaf3bc":"test['title'].value_counts()","f160cab1":"le = preprocessing.LabelEncoder()\ntrain['title_encoder'] = le.fit_transform(train['title'])\ntest['title_encoder'] = le.transform(test['title'])","85a37dd6":"max_bin = 20\nforce_bin = 3\n\n# Binning Function for continous variables\ndef mono_bin(Y, X, n = max_bin):\n    \n    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n    justmiss = df1[['X','Y']][df1.X.isnull()]\n    notmiss = df1[['X','Y']][df1.X.notnull()]\n    r = 0\n    while np.abs(r) < 1:\n        try:\n            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n            d2 = d1.groupby('Bucket', as_index=True)\n            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n            n = n - 1 \n        except Exception as e:\n            n = n - 1\n\n    if len(d2) == 1:\n        n = force_bin         \n        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n        if len(np.unique(bins)) == 2:\n            bins = np.insert(bins, 0, 1)\n            bins[1] = bins[1]-(bins[1]\/2)\n        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n        d2 = d1.groupby('Bucket', as_index=True)\n    \n    d3 = pd.DataFrame({},index=[])\n    d3[\"MIN_VALUE\"] = d2.min().X\n    d3[\"MAX_VALUE\"] = d2.max().X\n    d3[\"COUNT\"] = d2.count().Y\n    d3[\"EVENT\"] = d2.sum().Y\n    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n    d3=d3.reset_index(drop=True)\n    \n    if len(justmiss.index) > 0:\n        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n        d4[\"MAX_VALUE\"] = np.nan\n        d4[\"COUNT\"] = justmiss.count().Y\n        d4[\"EVENT\"] = justmiss.sum().Y\n        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n        d3 = d3.append(d4,ignore_index=True)\n    \n    d3[\"EVENT_RATE\"] = d3.EVENT\/d3.COUNT\n    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT\/d3.COUNT\n    d3[\"DIST_EVENT\"] = d3.EVENT\/d3.sum().EVENT\n    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT\/d3.sum().NONEVENT\n    d3[\"WOE\"] = np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"VAR_NAME\"] = \"VAR\"\n    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n    d3 = d3.replace([np.inf, -np.inf], 0)\n    d3.IV = d3.IV.sum()\n    \n    return(d3)\n\n# Binning Function for Categorical variables\ndef char_bin(Y, X):\n        \n    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n    justmiss = df1[['X','Y']][df1.X.isnull()]\n    notmiss = df1[['X','Y']][df1.X.notnull()]    \n    df2 = notmiss.groupby('X',as_index=True)\n    \n    d3 = pd.DataFrame({},index=[])\n    d3[\"COUNT\"] = df2.count().Y\n    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n    d3[\"EVENT\"] = df2.sum().Y\n    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n    \n    if len(justmiss.index) > 0:\n        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n        d4[\"MAX_VALUE\"] = np.nan\n        d4[\"COUNT\"] = justmiss.count().Y\n        d4[\"EVENT\"] = justmiss.sum().Y\n        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n        d3 = d3.append(d4,ignore_index=True)\n    \n    d3[\"EVENT_RATE\"] = d3.EVENT\/d3.COUNT\n    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT\/d3.COUNT\n    d3[\"DIST_EVENT\"] = d3.EVENT\/d3.sum().EVENT\n    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT\/d3.sum().NONEVENT\n    d3[\"WOE\"] = np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"VAR_NAME\"] = \"VAR\"\n    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n    d3 = d3.replace([np.inf, -np.inf], 0)\n    d3.IV = d3.IV.sum()\n    d3 = d3.reset_index(drop=True)\n    \n    return(d3)\ndef comb_category(woe,threshold):\n    \n    count = 0\n    similar_col = dict()\n    col = []\n    columns1 = woe['MIN_VALUE'].unique()\n    columns2 = woe['MIN_VALUE'].unique()\n    for cat1 in columns1 :\n        if cat1 in col: continue\n        woe1 = float(woe[woe['MIN_VALUE'] == cat1]['WOE'].values[0])\n        col1 = []\n\n        for cat in woe['MIN_VALUE'].unique():\n            if cat1 == cat: continue\n            if cat in col: continue\n            woe2 = float(woe[woe['MIN_VALUE'] == cat]['WOE'].values[0])\n\n            if (woe2 - woe1) >0.0 and (woe2 - woe1)<threshold:\n                col1.append(cat)\n                col.append(cat)\n        col.append(cat1)\n\n        similar_col[cat1] = col1\n        \n        if len(col1)>0:\n            count+=1\n    \n    return(similar_col,count)\ndef data_vars(df1,target,test,cat_threshold):\n    \n    \n    \n    x = df1.dtypes.index\n    y = train_copy.Survived.name\n    count = -1\n    replace = {}\n    \n    for i in x:\n        if i == y:continue\n        if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n            print('Numerical_col:',i)\n            conv = mono_bin(target, df1[i])\n            conv[\"VAR_NAME\"] = i\n            count = count + 1\n        else:\n            print('Categorical_col:',i)\n            cat_replace = []\n            conv = char_bin(target, df1[i])\n            conv = conv.sort_values('WOE')\n            similar_col,var_count = comb_category(conv,cat_threshold)\n            cat_replace.append(similar_col)\n\n            while var_count>0:\n\n                for x,y in zip(similar_col.keys(),similar_col.values()):\n                    df1.loc[df1[i].isin(y),i] = x\n                    test.loc[test[i].isin(y),i] = x\n                conv = char_bin(target, df1[i])\n                conv = conv.sort_values('WOE')\n                similar_col,var_count = comb_category(conv,cat_threshold)\n                cat_replace.append(similar_col)\n            replace[i] = cat_replace\n            conv[\"VAR_NAME\"] = i            \n            count = count + 1\n\n        if count == 0:\n            iv_df = conv\n        else:\n            iv_df = iv_df.append(conv,ignore_index=True)\n    \n    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n    iv = iv.reset_index()\n    return(iv_df,iv,df1,test,replace)\n\n","355a902d":"train.head()","3474a095":"train.isna().sum()","29bcf079":"train_copy = train.copy()\ntest_copy  = test.copy()\n\ntrain_copy.drop(['PassengerId','Name'],axis =1,inplace = True)\ntrain_copy['Cabin'].fillna('NA',inplace = True)\ntrain_copy['Embarked'].fillna('NA',inplace = True)\ntrain_copy['Ticket'].fillna('NA',inplace = True)\n\ntest_copy.drop(['PassengerId','Name'],axis =1,inplace = True)\ntest_copy['Cabin'].fillna('NA',inplace = True)\ntest_copy['Embarked'].fillna('NA',inplace = True)\ntest_copy['Ticket'].fillna('NA',inplace = True)\n\n#Calculating WOE and IV values\nfinal_iv, IV,new_train,new_test,cat_replace = data_vars(train_copy,train_copy.Survived,test_copy,cat_threshold = 0.1)","5fec07f0":"final_iv.head()","e5955bca":"def woe_replacement(train,transform_vars_list,transform_prefix):\n    for var in transform_vars_list:\n        print(var)\n        \n        small_train = final_iv[final_iv['VAR_NAME'] == var]\n        transform_dict = dict(zip(small_train.MAX_VALUE,small_train.WOE))\n        replace_cmd = ''\n        replace_cmd1 = ''\n        \n        for i in sorted(transform_dict.items()):\n            replace_cmd = replace_cmd + str(i[1]) + str(' if x <= ') + str(i[0]) + ' else '\n            replace_cmd1 = replace_cmd1 + str(i[1]) + str(' if x == \"') + str(i[0]) + '\" else '\n        replace_cmd = replace_cmd + '0'\n        replace_cmd1 = replace_cmd1 + '0'\n        \n        if replace_cmd != '0':\n            try:\n                train[transform_prefix + var] = train[var].apply(lambda x: eval(replace_cmd))\n            except:\n                train[transform_prefix + var] = train[var].apply(lambda x: eval(replace_cmd1))\n                \n    return(train)","379433bb":"transform_vars_list = new_train.columns.difference(['Survived'])\ntransform_prefix = 'new_'\n\nnew_train = woe_replacement(new_train,transform_vars_list,transform_prefix)","3c47d958":"transform_vars_list = new_test.columns\ntransform_prefix = 'new_'\n\nnew_test = woe_replacement(new_test,transform_vars_list,transform_prefix)","5829ac10":"def boosting(clf, fit_params, train, test, features):\n    N_SPLITS = 3\n    oofs = np.zeros(len(train))\n    preds = np.zeros((len(test)))\n\n    folds = StratifiedKFold(n_splits = N_SPLITS)\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train[TARGET_COL])):\n        print(f'\\n------------- Fold {fold_ + 1} -------------')\n\n        ### Training Set\n        X_trn, y_trn = train[features].iloc[trn_idx], train[TARGET_COL].iloc[trn_idx]\n\n        ### Validation Set\n        X_val, y_val = train[features].iloc[val_idx], train[TARGET_COL].iloc[val_idx]\n\n        ### Test Set\n        X_test = test[features]\n\n        #print(X_trn)\n        #exit(0)\n\n        _ = clf.fit(X_trn, y_trn, eval_set = [(X_val, y_val)], **fit_params)\n        \n#         shap_values = shap.TreeExplainer(clf).shap_values(X_trn)\n#         shap.summary_plot(shap_values,X_trn, plot_type = 'bar')\n\n        ### Instead of directly predicting the classes we will obtain the probability of positive class.\n        preds_val = clf.predict_proba(X_val)[:, 1]\n        preds_test = clf.predict_proba(X_test)[:, 1]\n\n        roc_score = roc_auc_score(y_val,preds_val)\n        print(\"ROC for validation set is {}\".format(roc_score))\n\n        oofs[val_idx] = preds_val\n        preds += preds_test \/ N_SPLITS\n\n\n    oofs_score = roc_auc_score(train[TARGET_COL], oofs.round())\n    print('ROC score for oofs is {}'.format(oofs_score))\n\n\n    return oofs, preds,clf","97c6ce93":"new_train.columns","dfb9f56d":"#catboost model training\nclf = CatBoostClassifier(n_estimators = 200,\n                       learning_rate = 0.05,\n                       rsm = 0.2, ## Analogous to colsample_bytree\n                       random_state=2054,\n                         max_depth =6\n                     )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 20}\n\nfeatures = ['new_Cabin','Age',\n       'new_Embarked', 'new_Fare', 'Parch', 'Pclass', 'new_Sex',\n       'SibSp', 'new_Ticket', 'relatives','title_encoder']\n\nTARGET_COL = 'Survived'\n\ncb_oofs, cb_preds,model = boosting(clf, fit_params,new_train,new_test,features)\n\noptimized_roc = roc_auc_score(new_train[TARGET_COL], (cb_oofs  * 1))\nprint(f'Optimized ROC is {optimized_roc}')\n","94caf89a":"#Feature_importance\nshap_values = shap.TreeExplainer(model).shap_values(new_train[features])\nshap.summary_plot(shap_values,new_train[features], plot_type = 'bar')","b0ca1ab4":"results = pd.DataFrame({'PassengerId':test['PassengerId'].values,'Survived':cb_preds})\nresults.loc[results['Survived']>0.5,'Survived'] = 1\nresults.loc[results['Survived']<=0.5,'Survived'] = 0\n\nresults['Survived'] = results['Survived'].astype(int)\nresults.to_csv(r'submission_catboost_v1.csv',index=False)","7e689fb8":"**EDA**","ced9bbc7":"To study more about WOE, you can refer to below links:\n1. https:\/\/www.kaggle.com\/ankarsingh\/credit-card-lead-prediction-using-woe\n2. https:\/\/www.listendata.com\/2015\/03\/weight-of-evidence-woe-and-information.html","1833e8bf":"In the code you will find the following:\n    \n1. EDA of the continous and categorical variable\n2. Weight of Evidence(WOE) and Information value(IV) for continous and Categorical Feature\n3. Modelling using Boosting techniques and WOE transformed features   ","96b67c09":"**Modelling**","acfde1a6":"**Feature Engineering**"}}