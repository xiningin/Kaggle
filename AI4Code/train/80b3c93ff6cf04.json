{"cell_type":{"c88abda5":"code","af79f574":"code","4078973c":"code","a8d439dc":"code","99f88b7d":"code","31a8e068":"code","5c0e89d1":"code","ad3646ec":"code","f6b69a38":"code","f6c293e1":"code","80a61a01":"code","a6413b8f":"code","30311480":"code","f59efb62":"markdown","684fc1bd":"markdown","a7e7565a":"markdown","ffe626d8":"markdown","604cece6":"markdown","be6d0ce9":"markdown","4face67b":"markdown","ae051e6e":"markdown","517a48e8":"markdown","28723691":"markdown","c9af6ec1":"markdown","41ca8b43":"markdown","98b6bc75":"markdown"},"source":{"c88abda5":"!python -m spacy download es_core_news_md\n!python -m spacy link es_core_news_md es_md\n!pip install spacy_spanish_lemmatizer stopwordsiso stop_words tweet-preprocessor\n!python -m spacy_spanish_lemmatizer download wiki","af79f574":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport spacy\nfrom spacy_spanish_lemmatizer import SpacyCustomLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom stop_words import get_stop_words\nimport stopwordsiso as stopwordsiso\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport preprocessor as p\nimport re\nimport csv","4078973c":"nlp = spacy.load('es_md', disable=['ner']) # disabling Named Entity Recognition for speed\nlemmatizer = SpacyCustomLemmatizer() \nnlp.add_pipe(lemmatizer, name=\"lemmatizer\", after=\"tagger\")\n#\np.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.SMILEY)\n# Stopword removal\nstop_words = set(stopwords.words('spanish'))\nstop_words_en = set(stopwords.words('english'))\nstop_words_iso = set(stopwordsiso.stopwords([\"es\", \"en\"]))\nreserved_words = [\"rt\", \"fav\", \"espa\u00f1a\", \"paraguay\", \"v\u00eda\", \"nofollow\", \"twitter\", \"true\", \"href\", \"rel\"]\nkey_words = ['coronavirus', 'coronavirusoutbreak', 'coronavirusPandemic', 'covid19', 'covid_19', 'epitwitter', 'ihavecorona', 'StayHomeStaySafe', 'TestTraceIsolate'] # twitter search keys\nstop_words_es = set(get_stop_words('es'))\nstop_words_en_ = set(get_stop_words('en'))\nstop_words.update(stop_words_es)\nstop_words.update(stop_words_en)\nstop_words.update(stop_words_en_)\nstop_words.update(stop_words_iso)\nstop_words.update(reserved_words)\nstop_words.update(key_words)\n#\nfile_name = 'covid19-tweets-early-late-april'","a8d439dc":"li = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        df = pd.read_csv(os.path.join(dirname, filename), index_col=None, header=0)\n        df = df[df['lang']=='es']\n        li.append(df)\n\ntweets_es = pd.concat(li, axis=0, ignore_index=True)\ndel li # free memory","99f88b7d":"tweets_es.text.sample(10)","31a8e068":"file_name += '_lang_es'\ntweets_es.info()\ntweets_es.to_csv(file_name+'_lang_es.csv',encoding='utf8', index=False)","5c0e89d1":"tweets_es_ES = tweets_es[tweets_es['country_code']=='ES']\ntweets_es_ES.info()\nfile_name += '_country'\ntweets_es_ES.to_csv(file_name+'_ES.csv',encoding='utf8', index=False)","ad3646ec":"tweets_es_PY = tweets_es[tweets_es['country_code']=='PY']\ntweets_es_PY.info()\ntweets_es_PY.to_csv(file_name+'_PY.csv',encoding='utf8', index=False)","f6b69a38":"# Preprocessing\ndef remove_string_special_characters(s):\n    stripped = str(s)\n\n    # Python regex, keep alphanumeric but remove numeric\n    stripped = re.sub(r'\\b[0-9]+\\b', '', stripped)\n\n    # Change any white space to one space\n    stripped = re.sub('\\s+', ' ', stripped)\n\n    # Remove urls\n    stripped = re.sub(r\"http\\S+\",'', stripped)\n    \n    # check again\n    #stripped = p.clean(stripped) be careful... also deletes \u00f1 and accents\n    \n    # # to ''\n    stripped = stripped.replace('#','')\n\n    # Remove start and end white spaces\n    stripped = stripped.strip()\n    if len(stripped) >= 3:#stripped != '':\n        return stripped.lower()","f6c293e1":"# only lemmas of content words           \ndef lemmatizer(text):        \n    sent = []\n    doc = nlp(text)\n    for word in doc:\n        if (word.pos_ not in ['VERB','ADV','ADJ','NOUN','PROPN']):\n            continue\n        sent.append(word.lemma_)\n    return \" \".join(sent)","80a61a01":"# wordcloud\nmaxWords=200\ndef show_wordcloud_mask(data, mask, stopwords, fileName=\"wordcloud.png\", title = None, maxWords=maxWords):\n    wordcloud = WordCloud(\n        background_color = 'white',\n        max_words = maxWords,\n        max_font_size = 180,\n        scale = 3,\n        random_state = 42,\n        stopwords = stopwords,\n        mask=mask,\n        collocations=False,\n    ).generate_from_frequencies(frequencies=data)\n\n    # create coloring from image\n    image_colors = ImageColorGenerator(mask)\n    fig = plt.figure(figsize=[14,14])\n    plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    if title:\n        fig.suptitle(title, fontsize = 20)\n        fig.subplots_adjust(top = 2.3)\n    plt.savefig(fileName, facecolor='k', bbox_inches='tight')\n    return","a6413b8f":"def unigrams_freq_wordcloud(data,retweets=False,country='ES'):\n    \n    print('country', country, \"=\"*50)\n\n    data = data[data['is_retweet']==retweets]\n    data[\"text\"]=data[\"text\"].astype(str)\n    data.info()\n    print(data['text'].sample(5))\n    \n    print(\"remove_string_special_characters ...\")\n    data[\"text\"]= data[\"text\"].apply(remove_string_special_characters)\n    data.dropna(subset = [\"text\"], inplace=True)\n    print(data['text'].sample(5))\n\n    print(\"remove_stopwords ...\")\n    data[\"text\"] = data[\"text\"].apply(lambda y: ' '.join([x for x in nltk.word_tokenize(y) if ( x not in set(stop_words) and len(x)>2 and \"covid\" not in x) ]))\n    data = data[~data['text'].isnull()]\n    data.dropna(subset = [\"text\"], inplace=True)\n    print(data['text'].sample(5))\n    \n    print(\"lemmatizer ...\")\n    data[\"text_lemmatize\"] = data.apply(lambda x: lemmatizer(x['text']), axis=1)\n    data['text'] = data['text_lemmatize'].str.replace('-PRON-', '')\n    data.dropna(subset = [\"text\"], inplace=True)\n    print(data['text'].sample(5))\n\n    print(\"n-grams ...\")\n    gram = 1\n    vectorizer = CountVectorizer(ngram_range = (gram,gram),stop_words=stop_words).fit(data.text.values.astype('U'))\n    bag_of_words = vectorizer.transform(data[\"text\"].values.astype('U'))\n    sum_words = bag_of_words.sum(axis=0)\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items() if len(word)>2]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n\n    print(\"plot ...\")\n    mask_url = \"https:\/\/live.staticflickr.com\/7102\/7378035790_231462ff2e_b.jpg\"\n    if country == 'PY':\n        mask_url = \"https:\/\/live.staticflickr.com\/8161\/7383472952_4f08c69c6c_b.jpg\"\n    response = requests.get(mask_url)\n    mask = np.array(Image.open(BytesIO(response.content)))\n    show_wordcloud_mask(dict(words_freq), mask, stop_words, \"retweets\"+str(retweets)+\"country\"+country+\"-1gram.png\")\n    \n\n    print(\"to_file ...\")\n    with open(\"word_freq-\"+str(gram)+\"gram_\"+country+\".csv\", \"w\") as f:\n        w = csv.writer(f)\n        w.writerows(words_freq)\n    f.close()\n    \n    return","30311480":"unigrams_freq_wordcloud(tweets_es_ES,retweets=False,country='ES')\nunigrams_freq_wordcloud(tweets_es_PY,retweets=False,country='PY')","f59efb62":"Country: Spain","684fc1bd":"## Load libraries","a7e7565a":"### Export tweets","ffe626d8":"# COVID-19 tweets: Spain vs. Paraguay\n\nTweets about COVID-19 written in Spanish on April 2020 in Spain and Paraguay.","604cece6":"### Load modules","be6d0ce9":"Language: Spanish","4face67b":"Give an idea about the conversation on Twitter...","ae051e6e":"Country: Paraguay","517a48e8":"Random sample of tweets","28723691":"### Install missing modules","c9af6ec1":"## Getting uni-grams - wordcloud for Spain and Paraguay! \n\n1. remove string special characters\n1. remove stop-words\n1. get lemmas (only content words)\n1. calculate word frequencies\n1. plot wordcloud over flags of Spain\/Paraguay\n1. write a csv with word frequencies","41ca8b43":"### Concat tweets files\n\nOnly if language is Spanish","98b6bc75":"## Dataset\n\nThe dataset have about 14M of tweets written in April 2020 (around the world) about COVID-19 with the keywords: #coronavirus, #coronavirusoutbreak, #coronavirusPandemic, #covid19, #covid_19, #epitwitter, #ihavecorona, #StayHomeStaySafe, #TestTraceIsolate"}}