{"cell_type":{"87fe5705":"code","adc6bf3d":"code","5e5038d0":"code","e31238a6":"code","3474f060":"code","c68ac7b2":"code","adc1f446":"code","90099042":"code","53494ca8":"code","da9ae8b3":"code","cc7766cc":"code","1692eeb3":"code","817f355d":"code","b7d43478":"code","eae9c663":"code","41fb4cd4":"code","8f15e9c6":"code","1f9714ef":"code","8f7a1398":"code","1308b354":"code","2a412f34":"code","772accb1":"code","bd1d2d55":"code","8df5c4c4":"code","492bad92":"code","12a98f30":"code","0e1f2322":"code","a0afd425":"code","9e8c8937":"code","cf098091":"code","4fe85383":"code","5ab0f23c":"code","43af1d37":"code","089db57f":"code","f94e7c90":"code","c1763fd9":"code","3c035f41":"code","41b8d6ac":"code","0252d5c4":"code","8f57d81f":"code","04464479":"code","8c5f7624":"code","7190891f":"code","062ac19e":"code","86e4813c":"code","fd91b2ea":"code","b3dc7393":"code","1f36d206":"code","0041aa0f":"markdown","51464a31":"markdown","c4031814":"markdown","49cb57cc":"markdown","aa99e20e":"markdown","c885d41e":"markdown","f25a2470":"markdown","a31d8148":"markdown","a1f8efe0":"markdown","42746a17":"markdown","796d1e5e":"markdown","28652c0d":"markdown","cdb54058":"markdown","4dd63a90":"markdown","96d15ab9":"markdown","adc57f87":"markdown","dd1d1991":"markdown","c1dc8ef7":"markdown","c44b039a":"markdown","ce7121a4":"markdown","fcfbf527":"markdown","870574b7":"markdown","89b59a9f":"markdown"},"source":{"87fe5705":"import numpy as np # f\u00fcr lineare Algebra\nimport pandas as pd # ben\u00f6tigt f\u00fcr das bearbeiten der Daten, CSV Datei einlesen etc. \n","adc6bf3d":"data = pd.read_csv('\/kaggle\/input\/online-shoppers-intention\/online_shoppers_intention.csv')","5e5038d0":"data.shape\n# zeigt uns, dass der Datensatz 18 Spalten und 12330 Zeilen hat","e31238a6":"data.head()\n# l\u00e4sst uns die ersten 5 Zeilen des Datensatzes sehen sowie die 18 verschiedenen Spaltennamen\n","3474f060":"data.info()\n# hiermit k\u00f6nnen wir uns einen Eindruck verschaffen um was f\u00fcr Datentypen es sich bei den einzelnen Spalten handelt\n# au\u00dferdem sehen wir, dass in eineigen Spalten Werte fehlen.\n# interesannt ist, dass nur bei ca. der h\u00e4lfte Daten fehlen und dort \u00fcberall gleich viele (14)","c68ac7b2":"# dies best\u00e4tigt und veranschaulicht folgender Befehl nochmal:\ndata.isna().sum()","adc1f446":"data[data.isna().sum(axis=1).astype(bool)]","90099042":"data.describe() \n# weitere Werte liefert uns die '.describe()' Funktion, so sehen wir hier u.A. die Kleinst- und H\u00f6chstwerte, Mittelwert und die Quartile","53494ca8":"data['ProductRelated_Duration'].plot()","da9ae8b3":"data.corr()","cc7766cc":"# zeigt den Prozentsatz der fehlenden Daten pro Spalte vor dem L\u00f6schen\nprozent_fehlend = data.isnull().sum() * 100 \/ len(data)\nprint(prozent_fehlend)","1692eeb3":"data = data.dropna(axis=0).reset_index(drop=True)","817f355d":"# zeigt den Prozentsatz der fehlenden Daten pro Spalte nach dem L\u00f6schen\nprozent_fehlend = data.isnull().sum() * 100 \/ len(data)\nprint(prozent_fehlend)","b7d43478":"data","eae9c663":"print(\"Fehlende Daten in 'data': \", data.isna().sum().sum())","41fb4cd4":"{column: list(data[column].unique()) for column in data.columns if data.dtypes[column] == 'object'}","8f15e9c6":"def ordinal_encode(df, column, ordering):\n    df = df.copy()\n    df[column] = df[column].apply(lambda x : ordering.index(x))\n    return df","1f9714ef":"month_ordering = ['Jan','Feb','Mar','Apr','May','Oct','June','Jul','Aug','Nov','Sep','Dec']","8f7a1398":"data = ordinal_encode(\n        data, 'Month',\n        month_ordering\n)","1308b354":"def onehot_encode(df, column, prefix):\n    df = df.copy()\n    dummies = pd.get_dummies(df[column], prefix=prefix)\n    df = pd.concat([df, dummies], axis=1)\n    df = df.drop(column, axis=1)\n    return df","2a412f34":"visitor_prefix = 'V'","772accb1":"data = onehot_encode(\n        data,\n        'VisitorType',\n        visitor_prefix\n    )","bd1d2d55":"data.head()","8df5c4c4":"# da es durch das verwenden der '.concat' Funktion in einigen F\u00e4llen zu Doppelungen kommen kann, lassen wir uns mit '.shape' nochmal die Summe aller Zeilen Ausgeben\n# und stellen nach \u00dcberpr\u00fcfung fest, dass es zu keinem Fehler kam\ndata.shape","492bad92":"data['Weekend'] = data['Weekend'].astype(np.int)\ndata['Revenue'] = data['Revenue'].astype(np.int)","12a98f30":"data.head()\n","0e1f2322":"y = data['Revenue'].copy()\nX = data.drop('Revenue', axis=1)\n\n","a0afd425":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42, shuffle= True)\n\n# Shuffle f\u00fchrte zu einer signifikanten Verbesserung der Accuracy!\n\n\n","9e8c8937":"#importieren aller anzuwendenen Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n","cf098091":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler","4fe85383":"pipeline_dt = Pipeline([('scalar1', StandardScaler()),\n                \n                        ('dt_classifier', DecisionTreeClassifier(random_state = 33))])","5ab0f23c":"pipeline_lr = Pipeline([('scalar2', StandardScaler()),\n                \n                        ('lr_classifier', LogisticRegression(random_state=0))])","43af1d37":"pipeline_rf = Pipeline([('scalar3', StandardScaler()),\n                \n                        ('rf_classifier', RandomForestClassifier(random_state = 33))])","089db57f":"pipeline_kn = Pipeline([('scalar4', StandardScaler()),\n                \n                        ('kn_classifier', KNeighborsClassifier())])","f94e7c90":"#Liste aller Pipelines\npipelines = [pipeline_dt, pipeline_lr, pipeline_rf, pipeline_kn]","c1763fd9":"best_accuracy=0.0\nbest_classifier=0\nbest_pipeline=\"\"","3c035f41":"#Dictonary anlegen\npipe_dict = {0: 'Decision Tree', 1: 'Logistic Regression', 2: 'Random Forest', 3: 'KNeighbors'}\n\n#fit der Pipelines\nfor pipe in pipelines:\n    pipe.fit(X_train, y_train)","41b8d6ac":"from sklearn.metrics import recall_score\nfor i, model in enumerate(pipelines):\n    print(\"{} Test Accuracy: {}\".format(pipe_dict[i],model.score(X_test,y_test)))","0252d5c4":"for i, model in enumerate(pipelines):\n    if model.score(X_test,y_test)>best_accuracy:\n        best_accuracy=model.score(X_test,y_test)\n        best_pipeline=model\n        best_classifier=i\nprint('Classifier with best accuracy: {}'.format(pipe_dict[best_classifier]))","8f57d81f":"y_pred_dt = pipeline_dt.predict(X_test)\ny_pred_lr = pipeline_lr.predict(X_test)\ny_pred_rf = pipeline_rf.predict(X_test)\ny_pred_kn = pipeline_kn.predict(X_test)","04464479":"from sklearn.metrics import recall_score\nprint(\"Recall Score f\u00fcr Umsatz bei Decision Tree\", recall_score(y_test, y_pred_dt, average=None)[1])\nprint(\"Recall Score f\u00fcr Umsatz bei Logistic Regression\", recall_score(y_test, y_pred_lr, average=None)[1])\nprint(\"Recall Score f\u00fcr Umsatz bei Random Forest\", recall_score(y_test, y_pred_rf, average=None)[1])\nprint(\"Recall Score f\u00fcr Umsatz bei KNeighbor\", recall_score(y_test, y_pred_kn, average=None)[1])","8c5f7624":"from sklearn.metrics import classification_report\n#classification report f\u00fcr random forest\nprint(classification_report(y_test, y_pred_rf))","7190891f":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(pipeline_rf, X_test, y_test, display_labels=['revenue', 'no revenue'], cmap='Blues', values_format='d');","062ac19e":"print('Parameter die gerade benutzt werden:\\n')\nprint(pipeline_rf.get_params())","86e4813c":"from sklearn.model_selection import RandomizedSearchCV\n\n# Anzahl der Baume im Wald\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# Anzahl der features bei jeder Teilung\nmax_features = ['auto','sqrt']\n\n# Maximum der ebenen pro baum\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Minimum an samples fuer das teilen der node \nmin_samples_split = [1, 2, 5, 10]\n\n# Minimale Anzahl von samples fuer jede leafe node\nmin_samples_leaf = [1, 2, 4]\n\n# Methode fuer das Auswalen dersamples fuer das trainieren jedes Baumes \nbootstrap = [True, False]\n\n\n\n# erzeugen des random grid\nrandom_grid = {'n_estimators': n_estimators,\n                'max_features': max_features,\n                'max_depth': max_depth,\n                'min_samples_leaf': min_samples_leaf,\n                'bootstrap': bootstrap}\n\nprint(random_grid)\n","fd91b2ea":"ran_for_clf = RandomForestClassifier()\n\n\n# Zuf\u00e4llige Suche nach Parametern mit einer drei fachen Kreuzvalidierung\n# sucht 100 verschiedene Kombinationen ab und verwendet dank n_jobs=-1 alle verf\u00fcgbaren Kerne\n\n\n\nran_for_clf_random = RandomizedSearchCV(estimator = ran_for_clf, param_distributions= random_grid, n_iter = 100, cv = 3, verbose=2, random_state=33, n_jobs = -1)\n\n# Fitten\nran_for_clf_random.fit(X_train, y_train)","b3dc7393":"ran_for_clf_random.best_params_\n\n#gibt die besten Parameter zur\u00fcck\n\n# Hier einmal zwei Outputs von vorherigen Berechnungen zum schnellen Nachschlagen, da die Neu-erechnung sehr lange dauert:\n#{'n_estimators': 800,\n# 'min_samples_leaf': 4,\n# 'max_features': 'auto',\n# 'max_depth': 90,\n# 'bootstrap': False}\n\n# und\n\n#{'n_estimators': 200,\n#'min_samples_leaf': 4,\n#'max_features': 'sqrt',\n#'max_depth': 60,\n#'bootstrap': True}\n","1f36d206":"#jetzt k\u00f6nnen wir das base model und random search Ergebnisse vergleich\n#und gucken ob es sich verbessert hat\n\n          \n\n\n#base Modell\npipeline_rf.fit(X_train, y_train)\nscore_baseline = pipeline_rf.score(X_test,y_test)\nrecall_vor_hp = recall_score(y_test, y_pred_rf, average=None)[1]\nprint(\"Recall Score f\u00fcr Umsatz vor Optimierung\", recall_score(y_test, y_pred_rf, average=None)[1])\nprint(\"Accuracy vor Hyperparameter Optimirung:  \",score_baseline)\n\nprint(\"\")  \n\n#neues Modell nach Hyperparameter tuning         \nbest_random = ran_for_clf_random.best_estimator_\nrandom_accuracy = best_random.score(X_test,y_test)\ny_pred_random = best_random.predict(X_test)\nrecall_nach_hp = recall_score(y_test, y_pred_random, average=None)[1]\nprint(\"Recall Score f\u00fcr Umsatz nach Optimierung\", recall_score(y_test, y_pred_random, average=None)[1])\nprint(\"Accuracy nach Hyperparameter Optimirung: \",random_accuracy)\n\n\n\n\nprint(\"\")\nprint('Verbesserung der Accuracy von {:0.2f}%.'.format( 100 * (random_accuracy - score_baseline) \/ score_baseline))\nprint('Verbesserung des Recall Score von {:0.2f}%.'.format( 100 * (recall_nach_hp - recall_vor_hp) \/ recall_vor_hp))\n","0041aa0f":"Rufen wir also nun die erstellte Funktion mit 'ordinal_encode()' und speisen sie mit der Spalte 'Month' unseres Datensatzes und der Reihnfolge welche wir in 'month_ordering' festgelegt haben.","51464a31":"Jetzt m\u00fcssen nur noch die im Boolean Format vorliegenden Spalten 'Weekend' und 'Revenue' umgewandelt werden, dies geht sehr einfach \u00fcber den Befehl .astype()","c4031814":"# <a name='split'><a\/>Test und Train Splitting\n\nNun, da das Preprossesing abgeschlossen ist, muss der Datensatz in ein Test- und ein Trainingssatz geteilt werden. Hierf\u00fcr wird unser Target 'Revenue' von den restlichen Daten getrennt, so k\u00f6nnen sp\u00e4ter die Ergebnisse des Models \u00fcberpr\u00fcft werden. Damit wir einmal einen Datensatz zum trainieren der Modelle und einen zum \u00fcberpr\u00fcfen haben, teilen wir die Daten in einem Verh\u00e4ltniss von 8 zu 2 auf. So garanitieren wir, dass wir die Modelle mit Daten \u00fcberpr\u00fcfen, welche sie vorher noch nicht gesehen haben. Nachdem wir mit der Angabe des Seeds die aufteilung reproduzierbar machen, dies geschieht \u00fcber 'random_state', setzen wir noch shuffle auf True. Da die Daten keine Zeitfolge sind, ist es nicht verwunderlich, dass das shuffeling eine signifikante verbesserung der Modelle erbingt. ","49cb57cc":"Der Output zeigt uns, dass Die Spalte \"Month\" sowie \"VisitorType' objekte sind.\nZus\u00e4tzlich sind die Objekte in der Spalte \"Month\" ordinal, sie k\u00f6nnen also in eine Reihnfolge sortiert werden und der Wert 'Aug' ist h\u00f6her als der Wert 'May'.\n\nIm Falle von \"VisitorType\" sind die Werte nominal und haben keine Reihnfolge.\n\nZur Kodierung bietet sich f\u00fcr die ordinalen Daten der Spalte \"Month\" das ordinal encoding und f\u00fcr die nominalen Werte der Spalte \"VisitorType\" das One-Hot Encoding an.","aa99e20e":"# <a name='one-hot'><a\/>One-Hot encoding f\u00fcr 'VisitorType'\n\nAuch hier wird wieder eine Funktion erstellt, diesmal mit der One-Hot encoding Methode f\u00fcr die Daten der Spalte 'VisitorType'","c885d41e":"## <a name=\"EDA\"><a\/>3. Erstmalige betrachtung der Daten \/ Exploratory Data Analysis\n    \n    \nMit .shape .info() und .head() verschaffen wir uns einen ersten Eindruck der Daten","f25a2470":"# <a name=\"Datenaufbereitung\"><a\/>4. Datenaufbereitung\n## <a name=\"Umgang-mit-fehlenden-Daten\"><a\/>4.1 Umgang mit fehlenden Daten\n    \nNach betrachten der 14 Zeilen mit fehlenen Daten, kommen wir zu dem Befund, dass auch wenn wir nicht genau wissen warum die Daten fehlen, es sich um einen sehr gerigen Anteil an fehlenden Daten handelt. Wir lassen uns um dies zu best\u00e4tigen einmal den Prozentsatz der fehlenden Daten jeder Spalte ausgeben und stellen fest, dass bei den betroffenen Spalten nur 0.1135% der Daten Fehlen. Anhand dieses sehr geringen Wertes wurde sich entschieden, die fehlenden Daten aus diesem Fall fallen zu lassen.\n    \n\nDaf\u00fcr gehen wir wie folgt vor:\n> data.dropna(axis=0) \n\nDurch .dropna() und der axis=0 werden alle Zeilen gel\u00f6scht, in welchen sich ein Null Wert befindet. \"axis=0\" k\u00f6nnte man in diesem Fall auch weglassen, da der Standard bei \"dropna()\" 0 ist.\n\nNachdem wir die Zeilen mit Nullwerten gel\u00f6scht haben, setzten wir die noch den Index zur\u00fcck:\n> .reset_index(drop=True)\n\nund weisen das ganze wieder \"data\" zu:\n> data = data.dropna(axis=0).reset_index(drop=True)","a31d8148":"## <a name=\"encoding\"><a\/>4.2 Encoding der Daten\nNicht alle Daten liegen in einem numerischen Format vor und dies ist notwenig um sie mit den algorythmen richtig verwenden zu k\u00f6nnen. Wandeln wir sie also im folgenden um. \n","a1f8efe0":"Schauen wir uns nun unseren Datensatz an, sehen wir beide Spalten 'Month' und 'VisitorType' sind in nun numerisch:","42746a17":"Als erstes schauen wir, welche Spalten daten als Objekte vorliegen haben","796d1e5e":"Auch n\u00fctzlich kann sich herausstellen, sich einmal die Korelation aller Spalten zueinander ausgeben zu lassen. Interesannt bei unseren Daten ist z. B. dass die h\u00f6chste Korrelation zwischen unserem Target 'Revenue' und einer anderen Spalte die 'PageValues' sind. \n\nMit einem Wert von 0,49 haben wir fast die r=0,5er Marke erreicht ab welcher man in der Statistik von einem starken statistischen Zusammenhang spricht.","28652c0d":"Um uns einen die 14 Reihen genauer anzuschauen k\u00f6nnen wir den Befehl 'data.isna().sum()' von eben etwas abwandeln.\nSummieren wir stattdessen \u00fcber die Zeilen mit .sum(axis=1), bekommen wir die Anzahl an Null Werten pro Zeile.\n\nUm im Folgenden nur die Zeilen angezeigt zu bekommen, in welchen wir Null Werte haben, nutzen wir .astype(bool) um die Ausgabe als Boolean Serie zu bekommen und k\u00f6nnen damit dann unseren Datensatz 'data' wie folgt subsetten.","cdb54058":"Schauen wir uns nun den neuen Datensatz an, stellen wir fest, dass er nun nur noch 12316 Zeilen hat.","4dd63a90":"Sinnvoll kann es auch sein sich einzelne Spalten einmal plotten zu lassen, hier einmal an der 'ProductRelated_Duration' veranschaulicht. Durch das Visuelle erlangt man schnell einen \u00dcberblick und sieht wie in diesem Fall hier, dass es zwei Extremwerte gibt. Mit dem Wissen um diese kann in der Weiteren Analyse und Nutzung der Daten ad\u00e4quater vorgeganegn werden.","96d15ab9":"# <a name=\"hyper-tuning\"><a\/>Hyperparamater Tuning mit hilfe einer Pipeline\n\nAktuell haben wir eine Accuracy von ~90% und einen Recall Score der Klasse Umsatz von 55,34 %, am besten lie\u00dfe sich diese erh\u00f6hen, indem man mehr Daten sammlet (gerade Daten bei denen es zu einem Umsatz kam w\u00e4ren wichtig), da wir in diesem Fall jedoch nur mit den vorhandenen Daten aus dem 'Online Shopper Intention' Datensatz arbeiten k\u00f6nnen und keine M\u00f6glichkeit haben ihn zu erweitern, m\u00fcssen wir mit der Zweitbesten Methode vorlieb nehmen, dem Hyperparameter Tuning.\n","adc57f87":"Alle Daten liegen nun in numerischer Form vor!","dd1d1991":"Die Reihnfolge (Indizes) der Liste in welcher die Monate unten in 'month_ordering'  eingetragen werden, bestimmt ihre sp\u00e4ter zugewiesenen ordinalen Werte. Dies geschieht, da wir oben im Code 'ordering.index(x)' geschrieben haben.\n\nAnmerkung: Auch wenn die Monate Januar und April im Datensatz nicht vorkommen habe ich diese der Liste hinzugef\u00fcgt.","c1dc8ef7":"## Importieren der Online Shoppers Intention Daten durch einlesen der CSV Datei\nDurch den Pandas Befehl pd.read_csv und die zuweisung der Daten zu \"data\" k\u00f6nnen wir im folgenden mit data weiter arbeiten.","c44b039a":"# Projektarbeit von Philipp Kionke mit dem 'Online Shopper's Intention' Datensatz.\n\n## 1. Allgemeine Informationen\n\nModul: Analytische Informationssysteme\n\nModulnummer: 316501\n\nBearbeiter: Philipp Kionke\n\nDatensatz: 'Online Shopper's Intention' von Roshan Sharma (https:\/\/www.kaggle.com\/roshansharma\/online-shoppers-intention)\n\nNotebook Info zur Navigation: Es wurden interne Hyperlinks verwendet, somit ist es m\u00f6glich zu erw\u00e4hnte Text- oder Codepassagen leicht durch klicken der Verlinkung zu gelangen. \n\n\n\n## 2. Gesch\u00e4ftsverst\u00e4ndnis\n\nDer 'Online Shopper's Intention' Datensatz enth\u00e4lt verschiedene Informationen \u00fcber das Kundenverhalten auf Online-Shopping-Webseiten. Durch die Auswertung dieser kann eine Marktanalyse unterst\u00fctzt und Webseiten so angepasst werden, dass das Kundenverhalten in einer Weise gelenkt wird, welche die Wahrscheinlichkeit eines erfolgreichen Kaufes und somit den Umsatz des Unternehmens steigert.\n\n\n\n## [3. Datenverst\u00e4ndnis (Exploratory Data Analysis)](#EDA)\n\nIm dritten Abschnitt schauen wir uns die Daten erst einmal genau an. F\u00fcr die Explorative Daten Analyse nutzen wir unter anderem Funktionen wie '.shape', '.info()' und '.head()'. Wir stellen fest, das der Datensatz aus 18 Spalten und 12330 Zeilen besteht. Schauen wir uns nun mit '.head()' die ersten 5 Zeilen an k\u00f6nnen wir uns schon Notizen f\u00fcr das sp\u00e4tere Vorgehen mit den Daten machen. So sollte beispielsweise die Spalte 'Revenue' sp\u00e4ter nicht als Feature mit in den Algorithmus aufgenommen werden, genau dieser Fakt, ob Umsatz erzieht worden ist oder nicht, herauszufinden gilt.\n\nDie Funktion '.info()' verr\u00e4t uns nicht nur um was f\u00fcr Datentypen es sich in unserem Datensatz handelt, dies wird sp\u00e4ter interessant, da wir alle nicht numerischen Typen ('Month', 'VisitorType', 'Weekend' sowie 'Revenue') f\u00fcr unsere Modelle umwandeln m\u00fcssen. Des Weiteren stellen wir fest, dass bei circa der H\u00e4lfte der Spalten jeweils genau 14 Werte fehlen. Der Befehl 'data.isna().sum()' veranschaulicht dies noch einmal. Wie wir mit diesen fehlenden Daten umgehen wird im n\u00e4chsten Abschnitt beschrieben werden. \n\n\n## [4. Datenaufbereitung](#Datenaufbereitung)\n\n\n\n### 4.1 Auswahl der Features\n\nAus dem vorangegangenen Teil der explorativen Daten Analyse wissen wir, dass unser Datensatz ohne unser Target \u00fcber 17 Features verf\u00fcgt. Schauen wir uns diese \u00fcberschaubare Menge noch einmal genauer an, so stellen wir fest, dass keines der Features direkt Aufschluss auf das Target gibt. Ziehen wir als Vergleich einmal den sehr bekannten Titanic Datensatz heran, hier ist es sinnvoll das Feature \"Rettungsbootnummer\" heraus zu nehmen, da dies schon ein gro\u00dfer Indikator f\u00fcr das Target (\u00fcberlebt) ist.\n\nIn unserem Fall belassen wir also alle 17 Features, nur das Target selbst muss nat\u00fcrlich sp\u00e4ter getrennt werden.\n\nWir haben ferner die M\u00f6glichkeit, nicht nur einzelne Features herauszunehmen, sondern auch zus\u00e4tzliche zu bilden. Hierzu werden die bereits vorhandenen Informationen genommen und neu aufbereitet. In unserem Falle k\u00f6nnte dies beispielsweise in Form einer Saisonalit\u00e4t abgebildet werden, welche auf dem vorhandenen Feature 'Month' abgeleitet wird. In dieser Arbeit wurde jedoch erst einmal nur mit den 17 bereits vorhandenen Features gearbeitet.\n\n\n### [4.2 Umgang mit fehlenden Daten](#Umgang-mit-fehlenden-Daten)\n\nZun\u00e4chst soll sich um die bereits erw\u00e4hnten fehlenden Daten gek\u00fcmmert werden.\n\nNach dem Betrachten der 14 Zeilen mit fehlenden Daten, kommen wir zu dem Befund, dass auch wenn wir nicht genau wissen warum die Daten fehlen, es sich um einen sehr geringen Anteil an fehlenden Daten handelt. Wir lassen uns um dies Aussage zu best\u00e4tigen einmal den Prozentsatz der fehlenden Daten jeder Spalte ausgeben und stellen fest, dass bei den betroffenen Spalten nur 0.1135 % der Daten Fehlen. Anhand dieses sehr geringen Wertes wurde sich entschieden, die fehlenden Daten aus diesem Fall fallen zu lassen.\n\nAnzumerken ist an dieser Stelle, dass das L\u00f6schen von Zeilen, welche fehlende Daten besitzen, nicht immer sinnvoll ist, w\u00e4ren es beispielsweise deutlich mehr Zeilen, so b\u00f6te sich eher eine andere Methode an, z. B. den Durchschnitt aller vorhandenen Daten einzusetzen.\n\nNachdem sich f\u00fcr das L\u00f6schen der fehlenden Daten entschieden worden ist, wurde dies sowie das Zur\u00fccksetzen des Indexes und die Neuzuweisung unseres nun ohne fehlende Werte vorliegenden Datensatze durchgef\u00fchrt. Unser neuer Datensatz hat nun 12316 Zeilen.\n\n\n\n\n### [4.3 Transformierung der Daten (Encoding)](#encoding)\n\nNachdem wir nun unsern Datensatz von Fehlenden Daten bereinigt haben, n\u00e4hern wir uns dem eigentlichen Auswerten der Daten mit einem Algorithmus. Bevor wir unsere Daten jedoch dem Algorithmus \u00fcbergeben k\u00f6nnen, m\u00fcssen wir diese noch etwas vorbereiten. Wir erinnern uns an Abschnitt 3 und die explorative Datenanalyse und daran, dass nicht alle Daten im Datensatz als numerische, sondern unter anderem auch als kategorische Daten vorliegen. F\u00fcr die im Folgenden verwendeten Algorithmen ist es jedoch notwendig, dass sie numerisch also als bin\u00e4re Vektordarstellung vorliegen und somit werden wir uns nun den noch nicht numerischen Spalten zuwenden und sie transformieren.\n\nAls erstes subsetten wir die Daten, sodass wir \u00fcbersichtlich ausgegeben bekommen, welche Spalten im Typ 'object' vorliegen sowie von diesen Spalten jeweils die einzelnen Auspr\u00e4gungen.\nWir stellen fest, dass sowohl die Monatsangabe als auch der Besuchertyp als Objekte vorliegen. Au\u00dferdem ist zu unterscheiden, dass die Objekte in der Spalte 'Month' ordinal sind, sie also eine Reihenfolge besitzen ('May' kommt vor 'Aug'), die Daten der Spalte 'VisitorType' sind jedoch nominal und haben dementsprechend keine Reihenfolge.\n\n\n#### [Ordinal Encoding f\u00fcr die Spalte 'Month'](#ordinal)\n\nK\u00fcmmern wir uns zuerst um die Daten aus der 'Month' Spalte. F\u00fcr ordinal vorliegende Daten wie in diesem Fall bietet sich das ordinal Encoding an. Auch wenn wir nur eine Spalte zu codieren haben und der Aufwand dementsprechen eher gering ist, nutzen wir im Folgenden die M\u00f6glichkeit einer Funktion. Die Funktion, welche wir in diesem Beispiel zwar nur ein einziges Mal aufrufen, war jedoch eine gute M\u00f6glichkeit f\u00fcr mich meine Python F\u00e4higkeiten zu verbessern. \n\nDie Funktion nimmt drei Dinge an, einmal den Datensatz, die zu konvertierende Spalte und eine Reihenfolge, mit welcher die Sortierung vollzogen werden soll. F\u00fcr die Indizierung erstellen wir eine Liste, in welche ich, auch wenn die Monate Januar und April nicht im Datensatz vorkommen, der Vollst\u00e4ndigkeit halber diese mit aufgenommen habe.\n\nRufen wir nun also die Funktion auf und speisen alle notwendigen Daten ein, kodiert die Funktion uns wie gew\u00fcnscht die Daten der Spalte 'Month' um. Die Daten liegen nun mit den aus der Liste 'month_ordering' resultierenden Indizes im Datensatz vor.\n\n#### [One-Hot Encoding f\u00fcr die Spalte 'VisitorType'](#one-hot)\n\nAuch f\u00fcr das one-hot Encoding der 'VisitorType' Daten wird wieder eine Funktion erstellt, diese funktioniert im Grunde genauso wie zuvor beim ordinal Encoding erkl\u00e4rt. Hinzu kommen hier noch zwei Dinge, einmal dummy Variablen und Pr\u00e4fixe.\n\nBeim one-hot Encoden werden sogenannte dummy variablen erstellt, haben wir also wie in unserem Fall die drei verschiedenen M\u00f6glichkeiten von einem Besuchertypen 'Returning_Visitor', 'New_Visitor'oder 'Other', so wird f\u00fcr jeden dieser Typen eine Spalte erstellt, welche entweder mit einer 1 oder 0 versehen wird (1= entspricht diesem Typen, 0= nicht diesr Typ). \n\nDes Weiteren unterscheidet man noch zwischen dem redundanten dummy encoding, dies ist der Fall wie soeben erkl\u00e4rt, wenn f\u00fcr jeden m\u00f6glichen Typen eine Spalte angelegt wird und dem, sowie dem nicht redundanten, wenn eine Spalte weniger angelegt wird (denn wenn der Besucher weder ein wiederkehrender noch ein neuer Kunde ist, so muss er zwangsl\u00e4ufig in die Kategorie 'other' eingestuft werden. Man spricht bei 'K' verschiedenen Werten von 'K' vielen Spalten beim nicht redundanten Vorgehen dann von 'K-1'.\n\n\nSchauen wir uns nun den Datensatz an, sehen wir das sowohl die Spalte 'Month', als auch 'VisitorType' in numerisch Form vorliegen.\n\nEs fehlen also nur noch die als Boolien vorliegenden Daten aus 'Weekend' und 'Revenue'. Diese umzuwandeln ist sehr leicht und zwar behelfen wir uns einfach der '.astype' Funktion.\n\nHiermit ist das Encoding abgeschlossen und alle Daten liegen numerisch vor!\n\n\n\n\n\n### [4.4 Datenaufteilung](#split)\n\nNun, da das Preprossesing abgeschlossen ist, muss der Datensatz in ein Test- und ein Trainingssatz geteilt werden. Hierf\u00fcr wird unser Target 'Revenue' von den restlichen Daten getrennt, so k\u00f6nnen sp\u00e4ter die Ergebnisse des Models \u00fcberpr\u00fcft werden. Damit wir einmal einen Datensatz zum trainieren der Modelle und einen zum \u00fcberpr\u00fcfen haben, teilen wir die Daten in einem Verh\u00e4ltniss von 8 zu 2 auf. So garanitieren wir, dass wir die Modelle mit Daten \u00fcberpr\u00fcfen, welche sie vorher noch nicht gesehen haben. Nachdem wir mit der Angabe des Seeds die aufteilung reproduzierbar machen, dies geschieht \u00fcber 'random_state', setzen wir noch shuffle auf True. Da die Daten keine Zeitfolge sind, ist es nicht verwunderlich, dass das shuffeling eine signifikante verbesserung der Modelle erbingt (von 0.8024 auf 0.8620!).\n\n\n\n\n\n## 5. Modellbildung\n\nDie Entscheidung f\u00fcr ein bestimmtes Machine Learning Modell sollte nicht beliebig getroffen werden, f\u00fcr unterschiedliche Anwendungsf\u00e4lle und Datens\u00e4tze performen unterschiedliche Algorithmen unterschiedlich gut. Um den Prozess der Auswahl zu vereinfachen, machen wir uns das [Pipelining](#pipeline) zunutze.\n\n\nWir entscheiden uns also nicht im Vorhinein f\u00fcr ein bestimmtes Machine Learning Modell, sondern testen mehrere auf ihre Tauglichkeit f\u00fcr unseren Datensatz. Da jedes Modell bereinigte und transformierte Daten ben\u00f6tigt und dies auch noch f\u00fcr beide Datens\u00e4tze, da die Testdaten sonst Einfluss auf die Trainingsdaten nehmen k\u00f6nnten (sogenanntes 'Data Leakage') f\u00fchrt das Ausprobieren von mehreren Modellen zu einem erheblichen Mehraufwand. Das eben erw\u00e4hnte Pipelining unterst\u00fctzt uns dabei und nimmt uns einige repetitive Arbeit ab, da es die Transformationsschritte b\u00fcndelt.\n\nIn der Pipeline k\u00f6nnen die einzelnen Transformation einfach \u00fcbergeben werden und Python \u00fcbernimmt die \u00dcbergabe an die n\u00e4chste Funktion. Auch werden die Algorithmen direkt angewendet, indem er als Tupel in die Pipeline mit aufgenommen wird. Im Folgenden werden die Algorithmen 'DecisionTreeClassifier', 'LogisticRegression', 'RandomForestClassifier' sowie 'KNeighborsClassifier' genutzt und in die Pipeline integriert.\n\nInnerhalb der Pipeline werden die Daten durch den zuvor importierten 'StandardScaller' skaliert, danach jeweils gefittet.\n\nDes Weiteren legen wir noch ein Dictionary f\u00fcr die verschiedenen Algorithmen an und erzeugen eine Ausgabe, welche nach durchlaufen aller Pipelines und Algorithmen alle Test Accuracys vergleicht und uns die mit der h\u00f6chsten Accuracy separat ausgibt. Somit k\u00f6nnen sehen wir schnell das Modell mit der h\u00f6chsten Accuracy. Da wir jedoch durch die Klassenungleichheit besonders auf den Recall wert f\u00fcr die Klasse Umsatz achten, geben wir uns diesen im Folgenenden mit 'recall_score' f\u00fcr alle Modelle aus.\n\nF\u00fcr den am besten abgeschnittenen Algorithmus 'Random Forrest' lassen wir uns noch den recall score, Classification Report und die Confusion Matrix ausgeben.\n\n\n### 5.1 Welche Machine Learning-Modelle haben Sie verwendet und warum?\n\nEs wurde sich f\u00fcr das Machine Learning-Modell 'Random-Forrest' entscheiden.\n\nWie zuvor in 5.1 hergeleitet schnitt der Random-Forrest Algorithmus von den vier getesteten Modellen am besten ab und somit wird im Folgenden mit ihm weitergearbeitet. In [5.3](#Kapitel-5-3) wird der ausgew\u00e4hlte Algorithmus genauer erkl\u00e4rt und aus seine Vorteile eingegangen.\n\n\n\n\n### 5.2 Was sind die resultierenden Modelle?\n\nWie wir beim Vergleichen feststellen konnten, schneidet Random-Forst mit einer bei den relevanten Metriken Recall der Klasse Umsatz mit 55,34 % und einem False-Negative-Wert von 3,41 %  von den vier Modellen am besten ab. Aus diesem Grund werden wir diese Methode weiter benutzen und im Folgenden mit dem Hypeparametertuning versuchen die Ergebnisse noch weiter zu verbessern. Zuvor jedoch steigen wir im Abschnitt [5.3](#Kapitel-5-3) noch einmal etwas tiefer in die Details von Random Forest ein.\n\n### <a name=\"Kapitel-5-3\"><a\/>5.3 Random-Forest\n\n    \nDer Random-Forrest Algorithmus z\u00e4hlt zu den Verfahren des \u00fcberwachten Lernens und wurde 1999 von Leo Breiman gepr\u00e4gt. Er kombiniert in seinem Vorgehen die Ergebnisse vieler verschiedener Entscheidungsb\u00e4ume und leitet aus dieser Kombination die bestm\u00f6gliche Entscheidung ab. Hierf\u00fcr erstellt er verschiedene, Unkorrelierte, Entscheidungsb\u00e4ume. Jeder Baum trifft also seine eigenen Entscheidungen und durch die Kombination all dieser wird eine finale Entscheidung getroffen.\n    \nDie durch die Eigenschaften der Daten nach vorgegebenen Regeln in Klassen zugewiesenen Daten sorgen f\u00fcr die jeweiligen Verzweigungen der Entscheidungsb\u00e4ume. Die Kriterien, nach welchen der jeweilige Baum entscheidet, sind zuf\u00e4llig und variieren von Baum zu Baum. Die einzelnen Entscheidungsb\u00e4ume selbst haben nicht den Anspruch, den Daten eine korrekte Klasse zuzuweisen, hierf\u00fcr ist die Kombination aller Entscheidungsb\u00e4ume zust\u00e4ndig. Durch eine vorgegebene Gewichtung flie\u00dfen alle Unterergebnisse zu einer finalen Entscheidung des Ensembles vom Random-Forest Modell zusammen. Der Effekt, welcher hier zum Zuge kommt, ist, dass wie bei einer Befragung von 100 Menschen zu einem Thema zwar viele falschliegen, jedoch die Summe aller wiederum h\u00e4ufig korrekt ist. Eines der wichtigsten Merkmale ist, dass Datens\u00e4tze mit kontinuierlichen Variablen verarbeitet werden k\u00f6nnen und somit bessere Ergebnisse bei Klassifizierungsproblemen erzielt werden k\u00f6nnen.\n    \nDie Genauigkeit des Ensembles ist nicht nur durch die Unterscheidlichkeit jedes Baumes des Waldes gepr\u00e4gt, sondern es kann auch durch die Wahl von beispielsweise der maximalen Tiefe der Entscheidungsb\u00e4ume beeinflusst werden.\n    \n    \n#### Vorteile des Random-Forest-Algorithmus:\n    \nRandom-Forrest spielt seine St\u00e4rken besonders bei Datenmengen mit vielen unterschiedlichen Klassen und Merkmalen aus, auch hier bleibt der Algorithmus performant, die einzelnen Entscheidungsb\u00e4ume werden n\u00e4mlich parallel zueinander aufgebaut und somit schneidet der Algorithmus im Bereich Performance sehr gut ab. Die gro\u00dfe Varianz an Einzelentscheidungen f\u00fchrt zu einer guten Vorhersagegenauigkeit.\nEin weiterer Vorteil gegen\u00fcber Verfahren, welche ein neuronales Netz nutzen, ist, dass die Entscheidungen nachvollziehbar sind und dies kombiniert mit den, im Vergleich zu neuronalen Netzen geringen Hardware Anforderungen, stellt einen erheblichen Vorteil der Random-Forest Methode dar.\n    \n    \nDas die Random-Forest Methode in unserem Vergleich am besten abgeschnitten hat, ist also kein Wunder, die Daten aus unserem zugrunde liegenden Datensatz eigenen sich wunderbar f\u00fcr die Random-Forest Methode.\n    \n\n\n### <a name=\"Kapitel-5-4\"><a\/>5.4 Wie haben Sie die Parameter Ihres Data-Mining-Modells angepasst und warum?\n\n\n\nMit dem [Hyper-Parameter Tuning](#hyper-tuning) besteht eine M\u00f6glichkeit, die Werte vom Modell zu verbessern. Hierzu werden mit Hyper-Parameter Optimizern wie dem 'GridSearchCV' oder auch 'RandomizedSearchCV' verschiedene Parameter ausprobiert und die besten hiervon im Anschluss zum trainieren des Modells gew\u00e4hlt.\n    \nIn diesem Falle wurde sich f\u00fcr den Hyper-Parameter Optimierer 'RandomizedSearchCV' entscheiden und die Accuracy minimal, der Recall Score f\u00fcr die Klasse Umsatz jedoch etwas mehr (2,54%) verbessert werden. Da der Recall Score in unserem Falle relevanter ist, ist das Ergebniss als besser an zu sehen.\n    \nDiese geringf\u00fcgige Verbesserung mag nicht sehr grandios wirken und das Hyper-Parameter Tuning in Frage stellen. Es ist tats\u00e4chlich so, dass das Hyperparameter Tuning nicht in jeder Situation sinnvoll ist. Diese Aussage l\u00e4sst sich wie folgt erkl\u00e4ren. \nDas Finden der besten Hyperparameter ist mit einem nicht zu verachtenden Aufwand verbunden,man braucht nicht nur seine Zeit beim Coden, sondern und noch viel relevanter, auch die Zeit des Renderns. Diese zus\u00e4tzliche investierte Zeit wird im Gegenzug nicht immer mit einem deutlich besseren Ergebnis belohnt. Gerade in Bezug auf den Random-Forest Algorithmus ist diese Anmerkung relevant, da er auch ohne Hyperparameter Tuning sehr gute Klassifizierungen bringt. Schneidet das Modell jedoch schlecht ab oder es nur sehr wenige relevante Parameter gibt, so kann das Tuning auch mit dem erw\u00e4hnten zeitlichen Mehraufwand sinnvoll sein.\n\n\n\n\n\n\n## 6. Modellevaluierung\n\n\n### 6.1 Bewertung der ausgew\u00e4hlten Modelle: Wie werden Ihre Modelle in der Praxis funktionieren?\n\nIn den folgenden Kapiteln wird in verschiedener Weise auf die Praxistauglichkeit des ausgew\u00e4hlten Modells eingegangen. Es werden u. a. in [6.2](#Kapitel-6-2) die Ergebnisse der aller getesteten Modelle aufgelistet und in [6.4](#6-4) darauf eingegangen, warum die Metriken Recall und False-Negative f\u00fcr die Auswahl des endg\u00fcltig genutzten Modells am st\u00e4rksten gewichtet wurden. Dies wird mit zwei Szenarien veranschaulicht. Im letzten Kapitel [7.0](#7-0) wird darauf eingegangen wie das Modell in der Praxis in einem Unternehmen eingesetzt und zu einer Umsatzsteigerung f\u00fchren kann.\n    \n    \n\n### <a name=\"Kapitel-6-2\"><a\/>6.2 Haben Sie verschiedene Modelle verglichen? Wenn ja, welches ist besser? F\u00fcr welche Metrik?\n\nEs wurde sich f\u00fcr das Random-Forrest Modell entschieden. Diese entscheidung beruhte haupts\u00e4chlich darauf, dass das Random-Forrest Modell in unserem Testverfahren von den vier getesteten Algorithmen in den relevanten Metriken am besten abgeschnitten hat. Relevant waren in unserem Fall vor allem der Recall Wert f\u00fcr die Klasse Umsatz sowie die der False Negativ Wert. Auf die Gr\u00fcnde warum diese beiden Metriken f\u00fcr unseren Datensatz so wichtig sind werde ich im Abschnitt [6.4](#6-4) genauer eingehen.\n    \nSchauen wir uns nun die vier verwendeten Modelle und ihre Ergebnisse an.\n\n##### Decision Tree: \nAccuracy          = 86,20 %   \nFalse Negative    = 7,43 % (183 aus 2464)   \nRecall f\u00fcr Umsatz = 55,90 %\n        \n##### Logistic Regression: \nAccuracy          = 88,15 %    \nFalse Negative    = 2,27 % (56 aus 2464)\nRecall f\u00fcr Umsatz = 33,71 %\n    \n##### Random Forest:\nAccuracy          = 90,14 %    \nFalse Negative    = 3,41 % (84 aus 2464)   \nRecall f\u00fcr Umsatz = 55,34 %\n    \n##### KNeighbors:\nAccuracy          = 87,46 %    \nFalse Negative    = 3,53 % (87 aus 2464)    \nRecall f\u00fcr Umsatz = 37,64 %\n    \n### 6.3 Beschreibung der gew\u00e4hlten Vorgehensweise zur Optimierung der Parameter\n\nAuf das Optimieren der Parameter mit Hilfe eines Hyper-Parameter Optimizers wurde in Abschnitt [5.4](#Kapitel-5-4) schon etwas eingegangen. \n    \nEs wurde der haupts\u00e4chlich der Optimizer 'RandomizedSearchCV' benutzt und die Parameter, welche angepasst wurden waren 'n_estimators', 'max_features', 'max_depth', 'min_samples_leaf' und 'bootstrap'. Wie in 5.4 erw\u00e4hnt f\u00fchrte dies zu einer leichten Verbesserung des Recall Scores von 2,54 %. Ich probierte mit 'GridSearchCV' noch einen weiteren Optimizer aus, lie\u00df das Kaggle Notebook \u00fcber Nacht laufen und testete eine ganze Reihe an verschiedenen Seeds, cv's und n_itern aus. Da es dies keine Verbesserung brachte, belie\u00df ich es bei 'RandomizedSearchCV'.\n\n    \n    \n    \n### <a name='6-4'><a\/>6.4 Diskussion der Evaluationsergebnisse \n\nIn Abschnitt [6.2](#Kapitel-6-2) haben wir bereits die unterschiedlichen Werte der Modelle f\u00fcr den Recall der Klasse Umsatz sowie die False Negatives gesehen. In diesem Abschnitt soll noch genauer darauf eingegangen werden, warum gerade diese beiden Werte so wichtig f\u00fcr die Evaluierung unseres Modelles in Anbetracht der Unternehmensziele sind.\n\nDie Accuracy ist in unserem Fall nicht so relevant, da es eine gro\u00dfe Ungleichheit der Klassen in den Daten gibt (2108 welche keinen Umsatz brachten und nur 356 mit Umsatz).\n    \nDer Recall Wert der Klasse Umsatz ist deshalb so wichtig, weil es dem Unternehmen ja gerade darauf ankommt, die Personen zu finden, welche Umsatz bringen, wenn das Modell auch ein paar Kunden als Umsatzbringend klassifiziert, die am Ende keinen Umsatz bringen ist das weniger schlimm als andersherum\n   \nInteressant wird es bei der Betrachtung der False Positives sowie der False Negatives. Versetzen wir uns einmal in die F\u00fchrungsebene des Unternehmens, welches den Umsatz mit einer genaueren Vorhersage durch unser Algorithmus verbessern m\u00f6chte. Zur Veranschaulichung hier zwei Szenarien:\n    \nSzenario 1 : Kunde A surft auf der Unternehmenswebsite, unser Algorithmus sagt voraus, dass er dem Unternehmen Umsatz erbringen wird, in Realit\u00e4t kommt es jedoch nicht zu Umsatz.\n\nSzenario 2 : Kunde B surft auf der Unternehmenswebsite, unser Algorithmus sagt voraus, dass er dem Unternehmen keinen Umsatz erbringen wird, in Realit\u00e4t kommt es jedoch dann zu Umsatz.\n    \nWelches der beiden Szenarien w\u00e4re schlimmer f\u00fcr das Unternehmen, sollten sie auf Grundlage des Algorithmus Anpassungsma\u00dfnahmen vornehmen? Ich w\u00fcrde argumentieren, das Szenario 2 deutlich schlimmer f\u00fcr das Unternehmen ist. Kaufabbr\u00fcche und Umentscheidungen gibt es immer, nicht jeder Kunde wird etwas kaufen, auch wenn es anf\u00e4nglich danach aussieht. Wird solch ein Kunde nun z. B. durch einen Kundenbetreuer beraten, ist die Zeit der Beratung nicht gleich verloren, zwar f\u00fchrt sie nicht zu einem direkten Umsatz, jedoch kann dieser Kunde zu einem sp\u00e4teren Zeitpunkt noch Umsatz bringen. Wenn jedoch ein K\u00e4ufer an der Kasse stehen gelassen wird und einfach nicht bedient wird, weil dem Mitarbeiter gesagt wird, dieser Kunde m\u00f6chte nichts kaufen, obwohl der Kunde mit dem Produkt und seinem Geld in der Hand nur darauf wartet zu bezahlen, so kommt es in diesem Fall zu einem direkten Umsatzausfall, den es gilt zu vermeiden.\n    \nSomit komme ich zu dem Fazit, dass f\u00fcr die Evaluierung unseres Algorithmus der False Negative Wert eine erheblich gr\u00f6\u00dfere Rolle spielt als der False Positive.\n   \n    \n\n## <a name='7-0'><a\/>7. M\u00f6glicher Gesch\u00e4ftseinsatz\n\nDen Gesch\u00e4ftseinsatz von dem geschaffenen Algorithmus stelle ich mir wie folgt vor:    \n(Die Ausf\u00fchrungen im Folgenden implizieren nat\u00fcrlich eine verfeinerte und weiterentwickelte Version des hier programmierten Algorithmus)\n    \nDas Unternehmen '123-Fashion' verkauft online mode Artikel und sammelt seit einer Weile das Verhalten und Metadaten der Website Besuche und h\u00e4lt fest, ob es am Ende zu einem Umsatz gef\u00fchrt hat. 123-Fashion hat uns beauftragt, ein Algorithmus zu programmieren, welcher anhand der gesammelten Daten bestimmen kann, ob ein Umsatz erzielt wurde. Nach erfolgreicher Entwicklung dieses Algorithmus nutzt 123-Fashion diesen, damit die IT-Abteilung der Firma leichte \u00c4nderungen an den gespeicherten Daten vornehmen kann und diese dann mit unserem Algorithmus bewerten lassen kann. So wird beispielsweise ein Website Besuch mit den identischen Werten jeweils einmal mit anderem Zugriffsmonat getestet. Stellt sich nun heraus, dass ein Umsatz deutlich wahrscheinlicher ist, wenn die Website im Dezember besucht wird, kann die IT-Abteilung diesen Befund an die Marketingabteilung weiterleiten und diese k\u00f6nnen ihn in die \u00dcberlegung, wann welche Werbekampagne geschaltet werden soll, mit Einflie\u00dfen lassen. Entscheidet sich das Unternehmen dann, die Werbekampagne auf November und Dezember zu fokussieren, so greifen mehr Besucher im Dezember auf die 123-Fashion Website zu, was statistisch eine h\u00f6here Umsatzwahrscheinlichkeit bedeutet. Kombiniert das Unternehmen nun diese und andere \u00c4nderungen an ihrer Website und Verhalten anhand der Vorhersage unseres Algorithmus, so kann durch ihn der Gesamtumsatz von 123-Fashion langfristig gesteigert werde.\n\n\n\n## [8. Quellen](#Quellen)\nAlle verwendeten Quellen in alphabetischer Reihenfolge sind ganz unten im Notebook unter [8. Quellen](#Quellen) zu finden.","ce7121a4":"# 5. Modellbildung\n## <a name='pipeline'><a\/>Pipelineing\n    \nIm Folgenden m\u00f6chten wir uns nicht im Vornherein auf ein bestimmtes Mashine Learning Modell festlegen, da wir nicht wissen welches sich f\u00fcr diesen Datensatz am besten eigent. \nDa jedes Modell bereinigte und transformierte Daten ben\u00f6tigt und dies auch noch f\u00fcr beide Datens\u00e4tze, da die Test Daten sonst Einfluss auf die Trainingsdaten nehmen k\u00f6nnten (sogenanntes 'Data Leakage') f\u00fchrt das Ausprobieren von mehreren Modellen zu einem erheblichen Mehraufwand. Es gibt jedoch ein Tool welches uns in dieser Situation helfen kann die Transformationsschritte zu b\u00fcndeln und den Aufwand zu minimieren: Das Pipelining.\n\nIn der Pipeline k\u00f6nnen die einzelnen Transformation einfach \u00fcbergeben werden und Python \u00fcbernimmt die \u00dcbergabe an die n\u00e4chste Funktion. Auch werden die Algorithmen direkt angewendet indem er als Tupel in die Pipeline mit aufgenommen wird. Im Folgenden werden die Algorithmen 'DecisionTreeClassifier', 'LogisticRegression', 'RandomForestClassifier' sowie 'KNeighborsClassifier' genutzt und in die Pipeline integriert.\n    \nInnerhalb der Pipeline werden die Daten durch den zuvor importierten 'StandardScaller' skaliert danach jeweils gefittet. Mit Hilfe des StandardScaller werden die Merkmale standartisiert, indem der Mittelwert entfernen und die Varianz auf eine Einheit skaliert wird.\n    \nDes Weiteren legen wir noch ein Dictonary f\u00fcr die verschiedenen Algorithmen an und erzeugen eine Ausgabe, welche nach durchlaufen aller Pipelines und Algorithmen alle Test Accuracys vergleicht und uns die mit der h\u00f6chsten Accuracy seperat ausgiebt. Somit k\u00f6nnen wir uns leicht entscheiden mit welchem Algorithmus wir im Weiteren weiter arbeiten m\u00f6chten.\n\nF\u00fcr den am besten abgeschnittenen Algorithmus 'Random Forrest' lassesn wir uns noch den Classification Report und die Confusion Matrix ausgeben.\n  ","fcfbf527":"# <a name='ordinal'><a\/>Ordinal encoding f\u00fcr 'Month'\n\nGerade wenn man mehrere Daten kodieren m\u00f6chte, bietet sich das erstellen von Funktionen an, auch wenn es in diesem Fall nicht notwendig ist, da die erstellte Funktion nur einmal aufgerufen wird, kann die Erstellung durch ein suabereres coding gerechtfertigt werden.\n","870574b7":"# <a name=\"Quellen\"><a\/>8. Quellen\n \n[Abdullah Almokainzi (2020), Online Shoppers Purchasing Intention, www.medium.com, [online article], [abgerufen am 27.01.2022]](https:\/\/medium.com\/analytics-vidhya\/online-shoppers-purchasing-intention-c757e4ff6bbe)\n\n[Anuganti Suresh (2020), What is a confusion matrix?, www.medium.com, [online article], [abgerufen am 24.01.2022]](https:\/\/medium.com\/analytics-vidhya\/what-is-a-confusion-matrix-d1c0f8feda5)\n    \n[B. Chen (2020), What is one-hot Encoding and how to use Pandas get_dummies function, www.towardsdatascience.com, [online article], [abgerufen am 29.01.2022]](https:\/\/towardsdatascience.com\/what-is-one-hot-encoding-and-how-to-use-pandas-get-dummies-function-922eb9bd4970)\n    \n[Behrens, John T. (1997), Principles and Procedures of Exploratory Data Analysis, Psychological Methods 1997, Vol. 2, No. 2, 131-160, [article]](https:\/\/web.archive.org\/web\/20170808064326\/cll.stanford.edu\/~willb\/course\/behrens97pm.pdf)\n    \n[chrisbow (2019), Formatting notebooks with markdown tutorial, www.kaggle.com, [online article], [abgerufen am 27.01.2022]](https:\/\/www.kaggle.com\/chrisbow\/formatting-notebooks-with-markdown-tutorial)\n    \n[Dietterich, Thomas G. (2000), An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization, Machine Learning, 40, 139\u2013157, 2000, [article]](http:\/\/link.springer.com\/content\/pdf\/10.1023\/A:1007607513941.pdf)\n    \n[Gabriel Atkin (2020), Online Shopping Revenue Prediction - Data Every Day #074, [Youtube Video], [abgerufen am 27.01.2022]](https:\/\/www.youtube.com\/watch?v=_bpowohfGzg)\n    \n[Krish Naik (2020), Creating Pipelines Using SKlearn- Machine Learning Tutorial, [Youtube Video], [abgerufen am 27.01.2022]](http:\/\/www.youtube.com\/watch?v=w9IGkBfOoic)\n    \n[Roshan Sharma (2019), Online Shopper's Intention, www.kaggle.com, [Dataset], [abgerufen am 27.01.2022]](http:\/\/www.kaggle.com\/roshansharma\/online-shoppers-intention)\n   \n[Santos, Marco (2020), False Positives or False Negatives: Which is worse?, www.towardsdatascience.com, [online article], [abgerufen am 31.01.2022]](https:\/\/towardsdatascience.com\/false-positives-vs-false-negatives-4184c2ff941a)\n    \n[scikit-learn Dokumentation, Cross-validation: evaluating estimator performance, www.scikit-learn.org, [abgerufen am 27.01.2022]](http:\/\/https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html#cross-validation) \n    \n[scikit-learn Dokumentation, sklearn.model_selection.RandomizedSearchCV, www.scikit-learn.org, [abgerufen am 22.01.2022]](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV)\n\n[scikit-learn Dokumentation, sklearn.model_selection.GridSearchCV, www.scikit-learn.org, [abgerufen am 26.01.2022]](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html?highlight=grid%20search%20cv#sklearn.model_selection.GridSearchCV)\n    \n[scikit-learn Dokumentation, sklearn.linear_model.LogisticRegression, www.scikit-learn.org, [abgerufen am 27.01.2022]](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)\n    \n[scikit-learn Dokumentation, sklearn.metrics.ConfusionMatrixDisplay, www.scikit-learn.org, [abgerufen am 27.01.2022]](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.ConfusionMatrixDisplay.html?highlight=confusionmatrixdisplay.from_predictions#sklearn.metrics.ConfusionMatrixDisplay.from_predictions)\n    \n[scikit-learn Dokumentation, sklearn.pipeline.Pipeline, www.scikit-learn.org, [abgerufen am 27.01.2022]](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html)\n \n[scikit-learn Dokumentation, sklearn.model_selection.train_test_split, www.scikit-learn.org, [abgerufen am 31.01.2022]](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html)\n\n[Stack Overflow (2018), Find out the percentage of missing values in each column in the given dataset, [online Beitrag], [abgerufen am 31.01.2022]](https:\/\/stackoverflow.com\/questions\/51070985\/find-out-the-percentage-of-missing-values-in-each-column-in-the-given-dataset)   \n    \n[Stefan Luber, Nico Litzel (2020), Was ist Random Forest?, www.bigdata-insider.de, [online article], [abgerufen am 28.01.2022]](https:\/\/www.bigdata-insider.de\/was-ist-random-forest-a-913937\/)\n    \n[Tony Yiu (2019), Understanding Random Forest, www.towardsdatascience.com, [online article], [abgerufen am 27.01.2022]](https:\/\/towardsdatascience.com\/understanding-random-forest-58381e0602d2)\n    \n[Unbekannt (2017), Split of Train and Test Data, www.dataandbeyond.wordpress.com, [online article], [abgerufen am 27.01.2022]](https:\/\/dataandbeyond.wordpress.com\/2017\/08\/24\/split-of-train-and-test-data\/)\n    \n[Unbekannt (na), 4. Korrelation, www.statistikgrundlagen.de, [online article], [abgerufen am 30.01.2022]](https:\/\/statistikgrundlagen.de\/ebook\/chapter\/korrelation\/)\n    \n[Will Koehrsen (2018), Hyperparameter Tuning the Random Forest in Python, www.towardsdatascience.com, [online article], [abgerufen am 27.01.2022]](https:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)\n\n\n    \n\n    \n\n    \n","89b59a9f":"Mit data.isna().sum().sum() k\u00f6nnen wir nochmals \u00fcberpr\u00fcfen, ob es nun keine Nullwerte mehr im Datensatz gibt."}}