{"cell_type":{"c179cb06":"code","c1a857ac":"code","9a653e4b":"code","bc0002bf":"code","32ba4a81":"code","13468d2c":"code","ddb4c675":"code","624dc467":"code","8732ec6f":"code","b2b1788d":"code","ad8b01ab":"code","d349eb64":"code","6c8e8fcb":"code","17c8572d":"code","9fd9ebd5":"code","4b05ef23":"code","5d057718":"code","39903f7e":"code","8c8f12bf":"markdown","08c7b58c":"markdown","cd8e2772":"markdown","870b3277":"markdown","58eaf7e2":"markdown"},"source":{"c179cb06":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c1a857ac":"# Load Data\ndf = pd.read_csv('..\/input\/nlp-tweet-sentiment-analysis\/bitcointweets.csv', header=None)\ndf = df[[1,7]]\ndf.columns = ['tweet','label']\ndf.head()","9a653e4b":"# inspect sentiment\nsns.countplot(df['label'])","bc0002bf":"# text length\ndf['text_length'] = df['tweet'].apply(len)\ndf[['label','text_length','tweet']].head()","32ba4a81":"g = sns.FacetGrid(df,col='label')\ng.map(plt.hist,'text_length')","13468d2c":"\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport re\n\ndef clean_text(s):\n    s = re.sub(r'http\\S+', '', s)\n    s = re.sub('(RT|via)((?:\\\\b\\\\W*@\\\\w+)+)', ' ', s)\n    s = re.sub(r'@\\S+', '', s)\n    s = re.sub('&amp', ' ', s)\n    return s\ndf['clean_tweet'] = df['tweet'].apply(clean_text)\n\ntext = df['clean_tweet'].to_string().lower()    \nwordcloud = WordCloud(\n    collocations=False,\n    relative_scaling=0.5,\n    stopwords=set(stopwords.words('english'))).generate(text)\n\nplt.figure(figsize=(12,12))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","ddb4c675":"# Encode Categorical Variable\nX = df['clean_tweet']\ny = pd.get_dummies(df['label']).values\nnum_classes = df['label'].nunique()","624dc467":"seed = 101 # fix random seed for reproducibility\nnp.random.seed(seed)","8732ec6f":"# Split Train Test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.2,\n                                                    stratify=y,\n                                                    random_state=seed)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","b2b1788d":"# Tokenize Text\nfrom keras.preprocessing.text import Tokenizer\nmax_features = 20000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)","ad8b01ab":"totalNumWords = [len(one_comment) for one_comment in X_train]\nplt.hist(totalNumWords,bins = 30)\nplt.show()","d349eb64":"from keras.preprocessing import sequence\nmax_words = 30\nX_train = sequence.pad_sequences(X_train, maxlen=max_words)\nX_test = sequence.pad_sequences(X_test, maxlen=max_words)\nprint(X_train.shape,X_test.shape)","6c8e8fcb":"import keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,Conv1D,MaxPooling1D,LSTM\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n\nbatch_size = 128\nepochs = 3","17c8572d":"def get_model(max_features, embed_dim):\n    np.random.seed(seed)\n    K.clear_session()\n    model = Sequential()\n    model.add(Embedding(max_features, embed_dim, input_length=X_train.shape[1]))\n    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n    model.add(MaxPooling1D(pool_size=2))    \n    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    print(model.summary())\n    return model","9fd9ebd5":"def model_train(model):\n    # train the model\n    model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n                          epochs=epochs, batch_size=batch_size, verbose=2)","4b05ef23":"def model_evaluate(): \n    # predict class with test set\n    y_pred_test =  np.argmax(model.predict(X_test), axis=1)\n    print('Accuracy:\\t{:0.1f}%'.format(accuracy_score(np.argmax(y_test,axis=1),y_pred_test)*100))\n    \n    #classification report\n    print('\\n')\n    print(classification_report(np.argmax(y_test,axis=1), y_pred_test))\n\n    #confusion matrix\n    confmat = confusion_matrix(np.argmax(y_test,axis=1), y_pred_test)\n\n    fig, ax = plt.subplots(figsize=(4, 4))\n    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n    for i in range(confmat.shape[0]):\n        for j in range(confmat.shape[1]):\n            ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n    plt.xlabel('Predicted label')\n    plt.ylabel('True label')\n    plt.tight_layout()","5d057718":"# train the model\nmax_features = 20000\nembed_dim = 100\nmodel = get_model(max_features, embed_dim)\nmodel_train(model)","39903f7e":"# evaluate model with test set\nmodel_evaluate()","8c8f12bf":"As expected, most tweets are very short in length.","08c7b58c":"This deep learning model will have 2 CNN layers, 1 LSTM layer, and final dense layer for classification","cd8e2772":"We are going to clean up the tweets, remove special chars, stop words, URL links, etc..","870b3277":"Looks like we achieved very respectable accuracy (>97%), classifying most of the tweets in the test set correctly!","58eaf7e2":"Majority of tweets are neutral and positive. Looks like there are not much negative tweets on Bitcoin! No wonder the price is skyrocketing!"}}