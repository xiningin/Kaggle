{"cell_type":{"923a985a":"code","0f7b6e0b":"code","697aaa7f":"code","a243cbf6":"code","c17304cc":"code","7c635cda":"code","89fcbca2":"code","003d75ba":"markdown","bcbf6377":"markdown"},"source":{"923a985a":"# XVFB will be launched if you run on a server\nimport os\nif type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n    !bash ..\/xvfb start\n    os.environ['DISPLAY'] = ':1'","0f7b6e0b":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%load_ext autoreload\n%autoreload 2","697aaa7f":"%%writefile qlearning.py\nfrom collections import defaultdict\nimport random, math\nimport numpy as np\n\nclass QLearningAgent:\n    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n        \"\"\"\n        Q-Learning Agent\n        based on http:\/\/inst.eecs.berkeley.edu\/~cs188\/sp09\/pacman.html\n        Instance variables you have access to\n            - self.epsilon (exploration prob)\n            - self.alpha (learning rate)\n            - self.discount (discount rate aka gamma)\n        Functions you should use\n            - self.get_leval_actions(state){state, hasable => list of actipons, each is hasable}\n            which returns legal actions for a state\n            - self.get_qvalue(state, action\n            which returns Q(state, action))\n            - self.set_qvalue(state, action, value)\n            which sets Q(state, action):=value\n        !!!Important!!!\n        Note: Please avoid using self._qValues directly.\n        There's a special self.get_qvalue\/set_qvalue for that.\n        \"\"\"\n        self.get_legal_actions = get_legal_actions\n        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n        self.alpha = alpha\n        self.epsilon = epsilon\n        self.discount = discount\n    def get_qvalue(self, state, action):\n        \"\"\"Returns Q(state, action)\"\"\"\n        return self._qvalues[state][action]\n    def set_qvalue(self,state,action,value):\n        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n        self._qvalues[state][action] = value\n\n    def get_value(self, state):\n        \"\"\"\n        Compute your agentt's estimate of V(s) using current q-values\n        V(s) = max_over_action Q(state, action) over possible actions.\n        Note: please take into account that q-values can be negative\n        \"\"\"\n        possible_actions = self.get_legal_actions(state)\n        \n        # If there are no legal actions, return 0\n        if len(possible_actions) == 0:\n            return 0.0\n        value = np.max([self.get_qvalue(state, action) for action in possible_actions])\n        return value\n    def update(self, state, action, reward, next_state):\n        \"\"\"\n        You should do your Q-Value update here\n        Q(s, a) := (1-alpha) * Q(s, a) + alpha *(r + gamma *V(s'))\n        \"\"\"\n        # agent parameters\n        gamma = self.discount\n        learning_rate = self.alpha\n        qvalue = (1-learning_rate) * self.get_qvalue(state, action) + learning_rate*(reward + gamma*self.get_value(next_state))\n        self.set_qvalue(state, action, qvalue)\n    \n    def get_best_action(self, state):\n        \"\"\"\n        Compute the best action to take in a state (using current q-values).\n        \"\"\"\n        possible_actions = self.get_legal_actions(state)\n        # if there are no legal actions, return None\n        if len(possible_actions) == 0:\n            return None\n        best_action = possible_actions[np.argmax([self.get_qvalue(state, action) for action in possible_actions])]\n        return best_action\n    def get_action(self, state):\n        \"\"\"\n        Compute the action to take in the current state, including exploration.\n        With probability self.epsilon, we shoudl take random action\n            otherwise - the best policy action (self.getPolicy)\n            \n        Note: To pick randomly from a list, use random.choice(list).\n        To lick True or False, with a given probability, generate uniform number in [0, 1]\n        and compare it with your probability\n        \"\"\"\n        \n        # Pick Action\n        possible_actions = self.get_legal_actions(state)\n        action = None\n        # if there are no legal actions, return None\n        if len(possible_actions) == 0:\n            return None\n        # agent parameters\n        epsilon = self.epsilon\n        if np.random.uniform(0, 1) < epsilon: # explore\n            chosen_action = np.random.choice(possible_actions)\n        else:\n            chosen_action = self.get_best_action(state)\n        return chosen_action","a243cbf6":"import gym\nenv = gym.make(\"Taxi-v3\")\n\nn_actions = env.action_space.n","c17304cc":"from qlearning import QLearningAgent\nagent = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99, get_legal_actions=lambda s: range(n_actions))","7c635cda":"def play_and_train(env, agent, t_max = 10**4):\n    \"\"\"\n    This function should\n    - run a full game, actions gven by agent's e-greedy policy\n    - train agent using agent.update(...) whenever it is possible\n    - return total reward\n    \"\"\"\n    total_reward = 0.0\n    s = env.reset()\n    for t in range(t_max):\n        # get agent to pick action given state s.\n        a = agent.get_action(s)\n        next_s, r, done, _ = env.step(a)\n        # train (update) agent for state s\n        agent.update(s, a, r, next_s)\n        s = next_s\n        total_reward += r\n        if done: break\n    return total_reward\n    ","89fcbca2":"from IPython.display import clear_output\nrewards = []\nfor i in range(1000):\n    rewards.append(play_and_train(env, agent))\n    agent.epsilon *= 0.99\n    if i %100 == 0:\n        clear_output(True)\n        print('eps = ', agent.epsilon, 'mean reward = ', np.mean(rewards[-10:]))\n        plt.plot(rewards)\n        plt.show()","003d75ba":"# Q-Learning\nThis notebook will guide you through implementation of vanilla Q-learning algorithm.\n\nYou need to implement QLearningAgent (follow instructions for each method) and use it on a number of tests below.","bcbf6377":"# Try it on taxi\nHere we use qlearning agent on taxi env fromo openai gym. You will need to insert a few agent functions here."}}