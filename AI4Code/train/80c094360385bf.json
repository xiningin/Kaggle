{"cell_type":{"07043645":"code","8923ca52":"code","b2e1bf27":"code","ead7ec1a":"code","51326e95":"code","d4999f9b":"code","5649a010":"code","35a59159":"code","06b9fa6c":"code","500ddd97":"code","5d9c3ea0":"code","dccc8dfe":"code","69e21718":"code","e56526c8":"code","7ec48d6d":"code","184bc882":"code","417928c7":"code","f0bf012d":"code","7d4e7f28":"code","73620c07":"code","302b99af":"code","8dc9773d":"code","b3499afa":"code","58ce4ec8":"code","9c4e3439":"code","79b9699d":"code","2be0eb95":"code","6626d231":"code","7f4cbc4a":"code","43897236":"code","7af95f21":"code","1299599e":"code","5e43837e":"code","5255ed70":"code","49fd262e":"code","b359e281":"code","a26647d6":"code","8a816210":"code","823b0ffb":"code","70ac741e":"code","1e7ff06b":"code","2775b9f3":"markdown","098d4987":"markdown","3b5a6b50":"markdown","b172e1e9":"markdown","b54736fd":"markdown","e57c2757":"markdown","c62d267e":"markdown","ada9e681":"markdown","f58d3c0f":"markdown","83ddfb5f":"markdown","87758b23":"markdown","97540fe4":"markdown","5613df48":"markdown","b3549488":"markdown","2c6fb23f":"markdown","c25bb1b7":"markdown","b2b8befb":"markdown","b30c73a9":"markdown","98f19c26":"markdown","c7e0a0c8":"markdown","44c84945":"markdown","870d8035":"markdown","9086e34a":"markdown","9457d79e":"markdown","c8473763":"markdown","d9b92824":"markdown","2458c35d":"markdown","1578fb40":"markdown","62e7820e":"markdown","8c8bc422":"markdown","9fcdd089":"markdown","5cd1ca25":"markdown","ff89ba4c":"markdown","8a92103f":"markdown","ecc07613":"markdown","f2f568ad":"markdown","8e9e88ee":"markdown","49f450df":"markdown","4e132a78":"markdown","b440c680":"markdown","9c0d9cac":"markdown","373cd38e":"markdown"},"source":{"07043645":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"darkgrid\")\nplt.style.use(\"seaborn-pastel\")\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8923ca52":"df_ratings = pd.read_csv(\"\/kaggle\/input\/imdb-dataset\/title.ratings.tsv\/title.ratings.tsv\",\n                         sep=\"\\t\",low_memory=False, na_values=[\"\\\\N\",\"nan\"])\ndf_ratings.head()","b2e1bf27":"df_ratings.info()","ead7ec1a":"df_ratings.describe()","51326e95":"ratings = dict(mean=df_ratings.averageRating.mean(),\n              median=df_ratings.averageRating.median())\nvotes = dict(mean=df_ratings.numVotes.mean(),\n              median=df_ratings.numVotes.median())\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nax1 = sns.distplot(df_ratings.averageRating,kde_kws=dict(bw=0.2))\nax1.axvline(x=ratings[\"mean\"],c=sns.color_palette(\"Set2\")[1],label=f\"mean={round(ratings['mean'],2)}\")\nax1.axvline(x=ratings[\"median\"],c=sns.color_palette(\"Set2\")[2],label=f\"median={round(ratings['median'],2)}\")\nplt.legend()\n\nplt.subplot(1,2,2)\nax2 = sns.distplot(df_ratings.numVotes,kde_kws=dict(bw=0.2))\nax2.axvline(x=votes[\"mean\"],c=sns.color_palette(\"Set2\")[1],label=f\"mean={round(votes['mean'],2)}\")\nax2.axvline(x=votes[\"median\"],c=sns.color_palette(\"Set2\")[2],label=f\"median={round(votes['median'],2)}\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()","d4999f9b":"buckets = 20\nplt.figure(figsize=(15,6))\nbins = pd.qcut(df_ratings.numVotes,buckets,duplicates=\"drop\").value_counts()\nsns.barplot(x=bins.values,y=bins.index,orient=\"h\")\nplt.show()","5649a010":"plt.figure(figsize=(15,6))\nax=sns.distplot(df_ratings.numVotes,kde=False)\nax.set_ylabel(\"Count\")\nax.set_yscale(\"log\")","35a59159":"df_title_basics  = pd.read_csv(\"\/kaggle\/input\/imdb-dataset\/title.basics.tsv\/title.basics.tsv\",\n                               sep=\"\\t\",low_memory=False, na_values=[\"\\\\N\",\"nan\"])\ndf_title_basics.head()","06b9fa6c":"df_title_basics.info()","500ddd97":"df_title_basics.titleType.value_counts().plot.pie(autopct=\"%.0f%%\",figsize=(6,6),pctdistance=0.8,\n                                                 wedgeprops=dict(width=0.4))\nplt.show()","5d9c3ea0":"df_title_basics = df_title_basics[df_title_basics.isAdult == 0]\ndf_title_basics.drop([\"isAdult\",\"endYear\"],axis=1,inplace=True)\ndf_title_basics = df_title_basics[(df_title_basics.titleType == \"movie\") | (df_title_basics.titleType == \"tvMovie\")]\n\ndf_title_basics.titleType.value_counts().plot.pie(autopct=\"%.0f%%\",figsize=(6,6),pctdistance=0.8,\n                                                 wedgeprops=dict(width=0.4))\nplt.show()","dccc8dfe":"df_title_basics.genres.value_counts().plot.pie(autopct=\"%.0f%%\",figsize=(6,6),pctdistance=0.8,\n                                              wedgeprops=dict(width=0.4))\nplt.show()","69e21718":"from sklearn.feature_extraction.text import CountVectorizer\n\ntemp = df_title_basics.genres.dropna()\nvec = CountVectorizer(token_pattern='(?u)\\\\b[\\\\w-]+\\\\b', analyzer='word').fit(temp)\nbag_of_genres = vec.transform(temp)\nunique_genres =  vec.get_feature_names()\nnp.array(unique_genres)","e56526c8":"genres = pd.DataFrame(bag_of_genres.todense(),columns=unique_genres,index=temp.index)\nsorted_genres_perc = 100*pd.Series(genres.sum()).sort_values(ascending=False)\/genres.shape[0]\nplt.figure(figsize=(15,8))\nsns.barplot(x=sorted_genres_perc.values,y=sorted_genres_perc.index,orient=\"h\")\nplt.xlabel(\"Percentage of Films (%)\")\nplt.show()","7ec48d6d":"merged_temp = pd.merge(df_ratings,df_title_basics,on=\"tconst\",how=\"left\")\nmerged_temp = merged_temp[(merged_temp.startYear.notnull())&(merged_temp.startYear<2019)]\ncounts_yearly = merged_temp.groupby(\"startYear\").agg({\"averageRating\":[np.median],\n                                                     \"numVotes\":[np.sum,np.size,lambda x: np.sum(x)\/np.size(x)]})\n\nmax_count_year = int(counts_yearly[(\"numVotes\",\"sum\")].idxmax())\nmax_year = int(counts_yearly[(\"numVotes\",\"size\")].idxmax())\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nax =counts_yearly[(\"numVotes\",\"size\")].plot()\nax.annotate(max_year,xy=(max_year,counts_yearly[(\"numVotes\",\"size\")].max()),\n            xytext=(1980,10000), arrowprops=dict(color=\"sandybrown\",shrink=0.05,width=1))\nax.annotate(\"WW I\",xy=(1914,counts_yearly[(\"numVotes\",\"size\")].loc[1914]), xytext=(1900,2000), \n            arrowprops=dict(color=\"sandybrown\",shrink=0.05,width=1))\nax.annotate(\"WW II\",xy=(1939,counts_yearly[(\"numVotes\",\"size\")].loc[1939]), xytext=(1950,4000), \n            arrowprops=dict(color=\"sandybrown\",shrink=0.05,width=1))\nplt.title(\"Total Number Films per Year\",fontweight=\"bold\")\n\nplt.subplot(1,2,2)\nax =counts_yearly[(\"numVotes\",\"sum\")].plot()\nax.annotate(max_count_year,xy=(max_count_year,counts_yearly[(\"numVotes\",\"sum\")].max()),\n            xytext=(1960,3e7),arrowprops=dict(shrink=0.05,color=\"sandybrown\",width=2))\nplt.title(\"Total Number of Voters per Year\",fontweight=\"bold\")\nplt.show()","184bc882":"max_count_year_per_film = int(counts_yearly[(\"numVotes\",\"<lambda_0>\")].idxmax())\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nax =counts_yearly[(\"averageRating\",\"median\")].plot()\nplt.title(\"Average Rating per Year\",fontweight=\"bold\")\n\nplt.subplot(1,2,2)\nax = counts_yearly[(\"numVotes\",\"<lambda_0>\")].plot()\nax.annotate(max_count_year_per_film,xy=(max_count_year_per_film,counts_yearly[(\"numVotes\",\"<lambda_0>\")].max()),\n            xytext=(1960,5200),arrowprops=dict(shrink=0.05,color=\"sandybrown\",width=2))\nplt.title(\"Average Number of Voters per Year per Film\",fontweight=\"bold\")\n\nplt.show()","417928c7":"sns.distplot(df_title_basics.runtimeMinutes.dropna().astype(int),bins=50)\nplt.gca().annotate(\"857\\nhours of\\nruntime?\",xy=(51000,0.00005),xytext=(40000,0.0004),\n                   fontsize=20, ha=\"center\",\n                   arrowprops=dict(color=\"sandybrown\",width=1))\nplt.show()","f0bf012d":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nuse = df_title_basics[df_title_basics.runtimeMinutes.notnull()]\nuse[\"runtimeMinutes\"] = use.runtimeMinutes.astype(int)\nuse[use.runtimeMinutes>50000]","7d4e7f28":"use.sort_values(by=\"runtimeMinutes\",ascending=False).head()","73620c07":"rt = use.runtimeMinutes[use.runtimeMinutes<300]\nmean_rt,median_rt,mode_rt = rt.mean(),rt.median(),rt.mode()[0]\n\nplt.figure(figsize=(15,5))\nsns.distplot(rt,kde_kws=dict(bw=10))\nplt.gca().axvline(mean_rt,label=\"mean\",color=sns.color_palette(\"Set2\")[1],ymax=0.1)\nplt.gca().axvline(median_rt,label=\"median\",color=sns.color_palette(\"Set2\")[2],ymax=0.2)\nplt.gca().axvline(mode_rt,label=\"mode\",color=sns.color_palette(\"Set2\")[3],ymax=0.3)\nplt.text(mean_rt+2,0.0025,f\"Mean: {int(mean_rt)}\")\nplt.text(median_rt+2,0.006,f\"Median: {int(median_rt)}\")\nplt.text(mode_rt+2,0.0085,f\"Mode: {int(mode_rt)}\")\nplt.legend()\nplt.show()","302b99af":"merged = pd.merge(df_ratings,df_title_basics,on=\"tconst\",how=\"right\").sort_values(by=\"numVotes\",ascending=False)\nmerged[[\"numVotes\",\"primaryTitle\",\"startYear\"]].iloc[:20,:]","8dc9773d":"merged[merged.numVotes>25000].sort_values(by=\"averageRating\",ascending=False).head(20)","b3499afa":"merged[merged.numVotes>25000].sort_values(by=\"averageRating\",ascending=True).head(20)","58ce4ec8":"merged_temp = merged[merged.genres.notnull()]\nvec = CountVectorizer(token_pattern='(?u)\\\\b[\\\\w-]+\\\\b', analyzer='word').fit(merged_temp.genres)\nbag_of_genres = pd.DataFrame(vec.transform(merged_temp.genres).todense(),\n                             columns=vec.get_feature_names(),index=merged_temp.index)\nmerged_temp = pd.concat([merged_temp,bag_of_genres],axis=1)\n\nrating_counts_means = pd.DataFrame([[merged_temp.averageRating[merged_temp[i]==1].median(),merged_temp[i].sum()] \n for i in vec.get_feature_names()],columns=[\"median\",\"count\"],index=vec.get_feature_names()).sort_values(\"median\",ascending=False)\n\nplt.figure(figsize=(7,8))\nsns.barplot(y=rating_counts_means.index,x=rating_counts_means[\"median\"],orient=\"h\")\nfor i,counts in enumerate(rating_counts_means[\"count\"]):\n    plt.text(0.5,i+0.25,f\"{counts:>5} films\")\n    plt.text(rating_counts_means[\"median\"][i],i+0.25,rating_counts_means[\"median\"][i])\nplt.show()","9c4e3439":"use = merged_temp[merged_temp.numVotes>1000]\nuse[\"runtimeMinutes\"] = pd.to_numeric(use.runtimeMinutes)\n[groups,edges] = pd.qcut(use.runtimeMinutes,10,precision=0,retbins=True)\nratings_avg = use.groupby(groups).agg({\"averageRating\":np.median})\nsns.boxplot(y=groups,x=\"averageRating\",data=use,orient=\"h\",showfliers=False)\nfor i,rate in enumerate(ratings_avg[\"averageRating\"]):\n    plt.text(rate+0.1,i+0.2,rate)\nplt.show()","79b9699d":"df_name_basics  = pd.read_csv(\"\/kaggle\/input\/imdb-dataset\/name.basics.tsv\/name.basics.tsv\",\n                               sep=\"\\t\",low_memory=False, na_values=[\"\\\\N\",\"nan\"])\ndf_name_basics.head()","2be0eb95":"df_name_basics.info()","6626d231":"sns.distplot(df_name_basics.birthYear.dropna(),kde=False)\nplt.show()","7f4cbc4a":"df_name_basics.sort_values(\"birthYear\").head(10)","43897236":"use = df_name_basics[[\"birthYear\",\"deathYear\",\"primaryName\"]].dropna()\nuse[\"lifespan\"] = use.deathYear - use.birthYear\nuse[\"lifespan\"][(use.lifespan>200)|(use.lifespan<0)]=use.lifespan.median()\n\nplt.figure(figsize=(12,5))\nax = sns.distplot(use.lifespan)\nax.axvline(use.lifespan.mode()[0],label=f\"mode age: {int(use.lifespan.mode()[0])}\",color=\"forestgreen\")\nax.axvline(use.lifespan.median(),label=f\"median age: {int(use.lifespan.median())}\",color=\"sandybrown\")\nax.axvline(use.lifespan.mean(),label=f\"mean age: {int(use.lifespan.mean())}\",color=\"fuchsia\")\nplt.legend()\nplt.show()","7af95f21":"use.sort_values(\"lifespan\",ascending=False).head(10)","1299599e":"df_title_principals  = pd.read_csv(\"\/kaggle\/input\/imdb-dataset\/title.principals.tsv\/title.principals.tsv\",\n                         sep=\"\\t\",low_memory=False, na_values=[\"\\\\N\",\"nan\"])\ndf_title_principals .head()","5e43837e":"df_title_principals.category.value_counts().plot.pie(autopct=\"%.0f%%\", pctdistance=0.8, figsize=(7,7),\n                                                          wedgeprops=dict(width=0.4))","5255ed70":"df_title_principals.info()","49fd262e":"inxs = df_title_principals.tconst.isin(df_title_basics.tconst)\nuse = df_title_principals[inxs]\n\ntop_names = use.nconst.value_counts().head(20)\ntop_names = pd.DataFrame(list(zip(top_names.index,top_names.values)),columns=[\"nconst\",\"count\"])\ntop_names = pd.merge(top_names,df_name_basics[[\"nconst\",\"primaryName\"]],on=\"nconst\")\n\ntop_names[\"job_type\"] = [use[use[\"nconst\"] == i].category.value_counts().index[0] for i in top_names.nconst]\ntop_names","b359e281":"df_title_akas  = pd.read_csv(\"\/kaggle\/input\/imdb-dataset\/title.akas.tsv\/title.akas.tsv\",\n                         sep=\"\\t\",low_memory=False, na_values=[\"\\\\N\",\"nan\"])\ndf_title_akas.columns = ['tconst', 'ordering', 'title', 'region', 'language', 'types',\n       'attributes', 'isOriginalTitle']\ndf_title_akas.head()","a26647d6":"df_title_akas.info()","8a816210":"df_title_akas = pd.merge(df_title_akas,df_title_basics,on=\"tconst\")\ndf_title_akas.info()","823b0ffb":"import pycountry\ncountry_counts = df_title_akas.region.value_counts()\ncountry_names = [pycountry.countries.get(alpha_2=coun).name for coun in country_counts.drop([\"XWW\",\"XWG\"]).index[:20]]\ncounts = country_counts.drop([\"XWW\",\"XWG\"]).values[:20]\n\nplt.figure(figsize=(12,5.5))\nsns.barplot(x=counts,y=country_names,orient=\"h\")\nplt.xlabel(\"Total Number of Different Films Shown in Cinemas\")\n\nfor i,count in enumerate(counts):\n    plt.text(100,i,count,va=\"center\")","70ac741e":"df_movies = pd.merge(pd.merge(pd.merge(df_title_basics,df_ratings,on=\"tconst\"),\n                          df_title_principals,on=\"tconst\"),\n                     df_name_basics,on=\"nconst\")\ndf_movies.head()","1e7ff06b":"df_movies.info()","2775b9f3":"Let\u2019s visualise average ratings (median) of film genres. We will use the same counter which we used above:\n\n* Highest rating average is taken by documentaries and news categories (both political)\n* horror and sci-fi are in the bottom of the list (could be due to too many nonsense examples in both)","098d4987":"## Ratings Table\n\ntitle.ratings.tsv.gz \u2014 Contains the IMDb rating and votes information for titles\n\n* tconst (string) \u2014 alphanumeric unique identifier of the title\n* averageRating \u2014 weighted average of all the individual user ratings\n* numVotes \u2014 number of votes the title has received","3b5a6b50":"Let\u2019s trim dataset as we are interested only in the movies. After trimming is done the majority of the titles now are movies (82%) and the remaining 120K titles are TV movies","b172e1e9":"Title basics table is way larger than the ratings table with over 6 million entries each of which describes basic information about the video titles. I used the word \u201cvideo\u201d as it not only includes films but also tv series, short videos (short films and music clips), even video games","b54736fd":"It is the largest table amongst other IMDb table with a size of almost 2GB. It contains over 38 million principal cast\/crew members such as actors\/actresses, writer, director, producer, editor, etc. which are associated with each film.","e57c2757":"The ratings table contains almost **1 million movie rating** entries","c62d267e":"It is Logistics (2012), longest made documentary ever. A 72 minutes long edit can be seen in the following YouTube link:\n\nhttps:\/\/www.youtube.com\/watch?v=QYFG0xP12yE\n\nMany may not consider it a form of art, rather a camera placed on top of a container ship and kept shooting it 35 days. Similarly, Beijing 2003 and Modern Times Forever are shoot with similar purposes","ada9e681":"Do users tend to rate higher on movies with longer runtime? Let\u2019s find out:\n\n* Runtimes of movies are grouped using pandas\u2019 qcut (bins are chosen automatically by qcut to distribute an equal number of samples for each group)\n* Outlier films are not visualised in the boxplot\n* Boxplot medians (vertical lines in each bin) show that the average rating tends to increase when movie runtime increases\n* The reason that films with 84m and less runtime being exception could be that this group contains a lot of animation films which have high rates","f58d3c0f":"Let\u2019s now visualise the distribution of film runtimes in minutes. The single bar below is an indication that there are a few outlier films which have a runtime over 50000 minutes","83ddfb5f":"## Title Principals Table\ntitle.principals.tsv.gz \u2014 Contains the principal cast\/crew for titles\n\n* tconst (string) \u2014 alphanumeric unique identifier of the title\n* ordering (integer) \u2014 a number to uniquely identify rows for a given titleId\n* nconst (string) \u2014 alphanumeric unique identifier of the name\/person\n* category (string) \u2014 the category of job that person was in\n* job (string) \u2014 the specific job title if applicable, else \u2018\\N\u2019\n* characters (string) \u2014 the name of the character played if applicable, else \u2018\\N\u2019","87758b23":"The Top 10 ancient persons in the dataset:\n\n* Lucio Anneo Seneca was born in 4AD being the most ancient person in the dataset\n* Some entries have wrong birth\/death year info such as negative lifespans or extreme lifespans (e.g. George Sanger birth 23AD, death 1911)","97540fe4":"The table consist of the title\u2019s also known as (AKA) information such as the variations of title in different countries. There are 19 million rows in the table.","5613df48":"The table has almost 10 million names coming from different professions such as actors, actresses, directors, writers, etc. It contains their birth and death year info as well as their most known work of art","b3549488":"The birth year distribution of the persons in the dataset is displayed below. The dataset contains even ancient writers from year 4 A.D.","2c6fb23f":"We are only interested in movies, therefore AKAs table is inner joined with the title basics table","c25bb1b7":"Let\u2019s examine tables one by one. From time to time we will join tables to do extra analysis in the notebook","b2b8befb":"The average rating is 6.89 (mean) which is different from the median value (7.1), we will examine this further. The average number of votes per film is close to a thousand (mean), this time median (20) is significantly different than the mean","b30c73a9":"Let\u2019s now list the Worst rated 20 films:\n\n* Again three Turkish films top the list, Turkish users have been very active huh?\n* Some of the read bad films are Disaster Movie, Supebabies, Epic Movie, Meet the Spartans, etc.","98f19c26":"Distribution of lifespans of persons in the dataset:\n\n* Shows a negative skew where mode>median>mean which is expected for distributions showing age at death","c7e0a0c8":"How about the trend for the number of voters per year and voter counts per year\/per film?\n\nThe graph below contains two subplots, the former gives the total number of films made each year, and the latter gives a total number of voters for the films made at the corresponding year which was peaked in 2013 (37 million vote count total for the films released in 2013) and follows a dramatic decline in the count. This is where things get interesting. The number of films made per year increases until 2017 but why people stop voting recent films? Is it because of the quality of movies getting worse? Or is it the IMDb website losing its popularity? However, it is obvious that IMDb still dominates the online film rating sector as their mind-blowing monthly visitor number reaches to 250 million.\n\n[read more: https:\/\/www.businesswire.com\/news\/home\/20180222005150\/en\/IMDb-Launches-First-Ever-Skill-Amazon-Alexa]","44c84945":"This could be another research point, let\u2019s continue to find more interesting points in the data.\n\nThe graph below has two subplots again. The former gives the average rating of films per year, whereas the latter displays the average voters per film again on a yearly basis. Since the 1920s, average film ratings tend to fluctuate but not showing a monotonic increase\/decrease trend.\n\nThe latter graph reaches its peak in the 90s and early 2000s and has been dropping dramatically since then. Recall that the total number of voters per year was peaked in 2013, not 90s. This means that 90s films got the attention of users the most. This can be related to the dominance of the age group of the IMDb voters. Since we don\u2019t have that information, we can guess that the dominant age group must be in the range of 30 to 50. 90s films are their childhood or teenage times films which you get impressed the most of a film huh? There could be other factors like the 90s films are better etc.","870d8035":"Now list the Top 20 movies with the highest voter count, first we need to merge df_ratings and df_title_basics tables and sort the table according to the number of votes:\n\n* Popular movies like Fight Club, The Matrix, Lord of the Rings trio are made the list as expected\n* Except for The Godfather, Top20 voted films are from the 90s to the present","9086e34a":"Let\u2019s now convert region codes into country names first. Then count them and visualise the Top 20:\n\n* The count does not give a total number of cinemas or tickets, it gives how many different numbers of films showed in cinemas in the country (the higher the number the more various films shown)\n* The US clearly wins the competition\n* Greece and Hungary find themselves in the middle of the table, better than I would expect, interesting","9457d79e":"In order to see a clear distribution of \u201cfilms\u201d, we need to take these outliers off from the data and plot the rest. We will do that by focusing only on the movies with 300 minutes runtime or less:\n\n* Now it looks like a normal distribution centered around 90 minutes","c8473763":"Let\u2019s explore columns one by one. Title type distribution is as follows:\n* Majority of the titles are TV episodes (71%)\n* Movies (including TV movies) only cover 10% of the dataset","d9b92824":"Let\u2019s find out Top 10 persons who have the longest lifespan in the dataset:\n\n* Jeanne Louise Calment, died when she was 122 years of age, had longest confirmed human life span in history (https:\/\/en.wikipedia.org\/wiki\/Jeanne_Calment)\n* Even though she did not take part in any movie whatsoever, she appeared in a documentary about her, which is why she is in our dataset","2458c35d":"How about the distribution of Genres, the pie chart below doesn\u2019t look charming right? It is because movies have multiple genres, therefore what we see below is a combination of genres each representing a slice in the pie","1578fb40":"Also, we can consider plotting the distribution on a logarithmic scale which reveals more. Since it is a count variable, it looks more like a Poisson distribution","62e7820e":"You can do all the analysis above with this shrinked data as well as implementing your own ideas.\n\nCheers!","8c8bc422":"The final table has 2.5 million rows, 19 columns","9fcdd089":"## Title Basics Table\n\ntitle.basics.tsv.gz \u2014 Contains the following information for titles:\n\n* tconst (string) \u2014 alphanumeric unique identifier of the title\n* titleType (string) \u2014 the type\/format of the title (e.g. movie, short, tvseries, tvepisode, video, etc)\n* primaryTitle (string) \u2014 the more popular title \/ the title used by the filmmakers on promotional materials at the point of release\n* originalTitle (string) \u2014 original title, in the original language\n* isAdult (boolean) \u2014 0: non-adult title; 1: adult title\n* startYear (YYYY) \u2014 represents the release year of a title. In the case of TV Series, it is the series start year\n* endYear (YYYY) \u2014 TV Series end year. \u2018\\N\u2019 for all other title types\n* runtimeMinutes \u2014 primary runtime of the title, in minutes\n* genres (string array) \u2014 includes up to three genres associated with the title","5cd1ca25":"## Name Basics Table\nname.basics.tsv.gz \u2014 Contains the following information for names:\n* nconst (string) \u2014 alphanumeric unique identifier of the name\/person\n* primaryName (string)\u2013 name by which the person is most often credited\n* birthYear \u2014 in YYYY format\n* deathYear \u2014 in YYYY format if applicable, else \u2018\\N\u2019\n* primaryProfession (array of strings)\u2013 the top-3 professions of the person\n* knownForTitles (array of tconsts) \u2014 titles the person is known for","ff89ba4c":"Let\u2019s find out the Top 20 highest-rated films, we will use the merged table again by just sorting it with averageRating. Like Top250 top-rated films of the IMDb website we will put a precondition to avoid listing \u201cunknown\u201d high rated films. Therefore we will only take into consideration of films which received ratings from at least 25000 users:\n\n* It is still not the same as Top250 of IMDb website since a Turkish film called \u201cHababam Sinifi\u201d tops our list with 9.4 average rating\n* This was because their Top250 list is ranked by a formula which includes the number of ratings each movie received from users, and value of ratings received from regular users\n* Since we do not have regular users information, we cannot filter them","8a92103f":"## Title Akas Table\ntitle.akas.tsv.gz \u2014 Contains the following information for titles:\n\n* titleId (string) \u2014 a tconst, an alphanumeric unique identifier of the title\n* ordering (integer) \u2014 a number to uniquely identify rows for a given titleId\n* title (string) \u2014 the localized title\n* region (string) \u2014 the region for this version of the title\n* language (string) \u2014 the language of the title\n* types (array) \u2014 Enumerated set of attributes for this alternative title. One or more of the following: \u201calternative\u201d, \u201cdvd\u201d, \u201cfestival\u201d, \u201ctv\u201d, \u201cvideo\u201d, \u201cworking\u201d, \u201coriginal\u201d, \u201cimdbDisplay\u201d. New values may be added in the future without warning\n* attributes (array) \u2014 Additional terms to describe this alternative title, not enumerated\n* isOriginalTitle (boolean) \u2014 0: not original title; 1: original title","ecc07613":"## Combine All Tables (for Movies only)\nNow let\u2019s combine all tables by selecting only movie titles. In this way, we will reduce the dataset size and save it for future uses. It will be much faster to load and play with the final table:","f2f568ad":"35 days (857 hours) long film? Let\u2019s find out which film it is (Note that tt10844584 is not listed in the latest IMDb dataset, therefore I won't take them into account)","8e9e88ee":"To overcome this, we can use Scikit-Learn\u2019s **CountVectorizer** feature extraction technique to detect and count each unique genre (e.g. drama, comedy, etc). We will create a new column for each unique Genre title and it will be True\/False if a movie has that genre or not","49f450df":"It did very well by finding unique genres and counting them. Note that, in the bar chart below, since a movie may have multiple genres at a time, their count will not be 100%","4e132a78":"Let\u2019s try to use pandas.qcut function, which discretizes the variable into equal-sized buckets based on rank or based on sample quantiles. Let\u2019s divide our data into 20 buckets and visualise:\n\n* Over 140K samples (more than 10%) have only 5 votes counted\n* The films having more than 1095 votes are represented with a single bar in the plot which counts around 50K (these are the significant\/popular films we are mostly interested in)","b440c680":"Let\u2019s find out who has been the busiest in film industry:\n\n* Ilaiyaraaja and William Shakespeare are the contributors with the highest numbers\n* Brahmanandam was involved more than a thousand films as an actor\n* Other than Shakespeare, not much-known personalities are there","9c0d9cac":"# Exploratory Data Analysis: IMDb Dataset\n\nIn this notebook, we will explore IMDb\u2019s dataset which is available online and refreshed daily.\n\nhttps:\/\/www.imdb.com\/interfaces\/\n\nSeven gzipped files (tab-separated values) can be downloaded from the website. Files are:\n\n* title.akas.tsv.gz\n* title.basics.tsv.gz\n* title.crew.tsv.gz\n* title.episode.tsv.gz\n* title.principals.tsv.gz\n* title.ratings.tsv.gz\n* name.basics.tsv.gz\n\n\nWe will go through each of them one by one except \u201ctitle.episode.tsv.gz\u201d and \"title.crew.tsv.gz\" as we are only interested in movies in the notebook, not TV series.\n\nLet\u2019s start with the necessary imports of the Python libraries:","373cd38e":"Average rating distribution shows a classic **negative skewed** distribution where the median is larger than mean. Vote count distribution is heavily clustered around small values (0\u20131000 votes). Therefore we need to do a bit extra to visualise this packed distribution"}}