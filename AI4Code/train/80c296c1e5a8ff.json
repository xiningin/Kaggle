{"cell_type":{"499532fc":"code","8c8db6be":"code","e222959b":"code","5639454d":"code","72fec5ee":"code","e52afc54":"code","d762e8d4":"code","2ae7a09d":"code","f29629d7":"code","05a9352a":"code","43ee1e3c":"code","94eeebe4":"code","c144990c":"code","83cbf2a9":"code","62d4ec7e":"code","63ba2780":"code","1a5dbed2":"code","cf07d093":"code","4f36a90a":"code","24af21f9":"code","35989543":"code","5f5b51de":"code","5a544be7":"code","dd43acc4":"code","b655283d":"code","beefde26":"code","a74299c4":"code","f4c0d26c":"code","6ed85176":"code","2516ad5c":"code","bfd103ed":"code","2d45586f":"code","e15d2f6d":"code","6c81587b":"code","133e9e8e":"code","5ea3504e":"code","716e0280":"code","a58b43ad":"code","df43190b":"code","f35cf177":"code","608f9766":"code","8902f274":"code","441226fe":"code","a271fc0d":"code","740b5d46":"code","9b871272":"code","350c11a6":"code","198b74e2":"code","4e8fd5d1":"code","88132767":"code","51a70cb1":"code","1732f6bd":"code","5cbd1aba":"code","3b8d143b":"code","2c262a59":"code","ff734d80":"code","e3e983e2":"code","c5ab2bd3":"code","53a8d57e":"code","166c6093":"code","6a7087f0":"code","b2b5f0a8":"markdown","c9386b29":"markdown","39f9b8f5":"markdown","5f0480df":"markdown","168b1fda":"markdown","785fa78e":"markdown","1d61049a":"markdown","08f75ab7":"markdown","4b9e1243":"markdown","85141c71":"markdown","db03681f":"markdown","6041bf7c":"markdown","a9c0ebdc":"markdown","341f74db":"markdown","94a3e7b1":"markdown","9b8050e8":"markdown","6dca744a":"markdown","1dc0b568":"markdown","0f592900":"markdown","0531a284":"markdown","a0520173":"markdown","f28f3f3d":"markdown","38c7b267":"markdown","ce53addc":"markdown","5a4b54a9":"markdown","b28c71bd":"markdown","66ae7058":"markdown","8ba810ae":"markdown","e4bf9727":"markdown","17ae5a8e":"markdown","14b76f99":"markdown","d46feb99":"markdown","923e408a":"markdown","c911b811":"markdown","d96710a7":"markdown","077c3b5b":"markdown","fb134fb7":"markdown","a6df5f68":"markdown","b6847508":"markdown","39c9eb34":"markdown","ccb988dd":"markdown","32334569":"markdown","d22479e0":"markdown","cbf90fc9":"markdown","0707a4f3":"markdown","e25f3e94":"markdown","0f6ca8b6":"markdown","6812ec30":"markdown","19e4d476":"markdown","b3a71b97":"markdown","545dba0c":"markdown","91aa8b7c":"markdown","39c27892":"markdown","ea44c7e5":"markdown","6bb2c2d2":"markdown","eedb8348":"markdown","e7e50fb7":"markdown","d46413f9":"markdown","ee852154":"markdown","2e46bc81":"markdown"},"source":{"499532fc":" !pip install git+https:\/\/github.com\/fastai\/fastai@2e1ccb58121dc648751e2109fc0fbf6925aa8887 2>\/dev\/null 1>\/dev\/null","8c8db6be":"import warnings\nwarnings.filterwarnings(\"ignore\")","e222959b":"from fastai.structured import train_cats,proc_df","5639454d":"import numpy as np \nimport pandas as pd ","72fec5ee":"from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.model_selection import train_test_split","e52afc54":"!ls ..\/input\/house-prices-advanced-regression-techniques\/train.csv","d762e8d4":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain2 = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")","2ae7a09d":"train_cats(train)\ntrain_cats(train2)","f29629d7":"train.columns","05a9352a":"train2.columns","43ee1e3c":"train.Age=train.Age.fillna(train.Age.median())","94eeebe4":"df_c,y_c,_=proc_df(train,\"Survived\")","c144990c":"df_r,y_r,_=proc_df(train2,\"SalePrice\")","83cbf2a9":"X_train, X_test, y_train, y_test = train_test_split(df_c, y_c, test_size=0.3)","62d4ec7e":"m = RandomForestClassifier(n_jobs=-1)\nm.fit(X_train, y_train)","63ba2780":"from sklearn.metrics import confusion_matrix","1a5dbed2":"from sklearn.metrics import accuracy_score","cf07d093":"confusion_matrix(y_test, m.predict(X_test))","4f36a90a":"m.score(X_test,y_test)","24af21f9":"accuracy_score(y_test,m.predict(X_test))","35989543":"from sklearn.metrics import classification_report","5f5b51de":"print(classification_report(y_test, m.predict(X_test)))","5a544be7":"from sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\n%matplotlib inline","dd43acc4":"fpr, tpr, thresholds = roc_curve(y_test, m.predict_proba(X_test)[:,1])\n\nplt.plot(fpr, tpr, label='ROC curve')\nplt.plot([0, 1], [0, 1], 'k--', label='Random guess')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.xlim([-0.02, 1])\nplt.ylim([0, 1.02])\nplt.legend(loc=\"lower right\")","b655283d":"from sklearn.metrics import log_loss","beefde26":"log_loss(y_test,m.predict_proba(X_test))","a74299c4":"y_r=np.log(y_r)","f4c0d26c":"X_train, X_test, y_train, y_test = train_test_split(df_r, y_r, test_size=0.3)","6ed85176":"m = RandomForestRegressor(n_jobs=-1)\nm.fit(X_train, y_train)","2516ad5c":"y_pred=m.predict(X_test)","bfd103ed":"from sklearn.metrics import mean_squared_error","2d45586f":"round(mean_squared_error(y_test,y_pred),3)","e15d2f6d":"from math import sqrt","6c81587b":"round(sqrt(mean_squared_error(y_test,y_pred)),3)","133e9e8e":"from sklearn.metrics import mean_absolute_error","5ea3504e":"round(mean_absolute_error(y_test,y_pred),3)","716e0280":"round(sqrt(mean_absolute_error(y_test,y_pred)),3)","a58b43ad":"from sklearn.metrics import mean_squared_log_error","df43190b":"round(mean_squared_log_error( y_test, y_pred ),4)","f35cf177":"round(np.sqrt(mean_squared_log_error( y_test, y_pred )),4)","608f9766":"from sklearn.metrics import r2_score","8902f274":"r2=r2_score(y_test,y_pred)","441226fe":"r2","a271fc0d":"n =len(X_train)","740b5d46":"r2_adj =1- (1-r2)*(n-1)\/(n-(13+1))","9b871272":"r2_adj","350c11a6":"from sklearn.model_selection import KFold,StratifiedKFold","198b74e2":"n_fold = 10\nfolds = KFold(n_splits=n_fold, shuffle=True)","4e8fd5d1":"test_pred_proba = np.zeros((df_c.shape[0], 2))\naccuracy = []\n    \nfor n_fold, (train_idx, valid_idx) in enumerate(folds.split(df_c, y_c)):\n        X_train, X_valid = df_c.iloc[train_idx], df_c.iloc[valid_idx]\n        y_train, y_valid = y_c[train_idx], y_c[valid_idx]\n        \n        model = RandomForestClassifier()\n        model.fit(X_train, y_train)\n\n        y_pred_valid = model.predict(X_valid)\n        accuracy.append(accuracy_score(y_valid,y_pred_valid))\n\n","88132767":"accuracy","51a70cb1":"n_fold = 10\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True)","1732f6bd":"accuracy = []\n    \nfor n_fold, (train_idx, valid_idx) in enumerate(folds.split(df_c, y_c)):\n        X_train, X_valid = df_c.iloc[train_idx], df_c.iloc[valid_idx]\n        y_train, y_valid = y_c[train_idx], y_c[valid_idx]\n        \n        model = RandomForestClassifier()\n        model.fit(X_train, y_train)\n\n        y_pred_valid = model.predict(X_valid)\n        accuracy.append(accuracy_score(y_valid,y_pred_valid))","5cbd1aba":"accuracy","3b8d143b":"from sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import cross_val_score","2c262a59":"loocv = LeaveOneOut()\nm = RandomForestClassifier(n_jobs=-1)\nresults = cross_val_score(m, df_c, y_c, cv=loocv)","ff734d80":"results","e3e983e2":"results.mean()","c5ab2bd3":"from sklearn.model_selection import ShuffleSplit","53a8d57e":"kfold = ShuffleSplit(n_splits=10, test_size=0.3)\nresults = cross_val_score(m, df_c, y_c, cv=kfold)","166c6093":"results","6a7087f0":"results.mean()","b2b5f0a8":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*5XuZ_86Rfce3qyLt7XMlhw.png\"\/>","c9386b29":"## Auc - Roc curve <a class=\"anchor\" id=\"au\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","39f9b8f5":"# Date preprocessing ","5f0480df":"In K Fold cross validation, the data is divided into k subsets. Now the holdout method is repeated k times, such that each time, one of the k subsets is used as the test set\/ validation set and the other k-1 subsets are put together to form a training set. The error estimation is averaged over all k trials to get total effectiveness of our model. As can be seen, every data point gets to be in a validation set exactly once, and gets to be in a training set k-1 times. This significantly reduces bias as we are using most of the data for fitting, and also significantly reduces variance as most of the data is also being used in validation set","168b1fda":"### Confusion matrix <a class=\"anchor\" id=\"cm\"><\/a>","785fa78e":"## Logistic loss <a class=\"anchor\" id=\"ll\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","1d61049a":"<img style=\"height:100px\" src=\"https:\/\/miro.medium.com\/max\/1200\/0*AUzyQ1rc6mpQVYfn\" >","08f75ab7":"<h3><a href=\"https:\/\/towardsdatascience.com\/how-to-select-the-right-evaluation-metric-for-machine-learning-models-part-1-regrression-metrics-3606e25beae0\">How to select the Right Evaluation Metric<\/a><\/h3>","4b9e1243":"<img src=\"https:\/\/geab.eu\/wp-content\/uploads\/2019\/12\/evaluation-1-800x395.jpg\" style=\"margin-left:130px\">","85141c71":"## StratifiedKFold <a class=\"anchor\" id=\"skf\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","db03681f":"It is a special case of Kfold when K is equal to the number of samples in our dataset. This means that will iterate through every sample in our dataset each time using k-1 object as train samples and 1 object as test set.","6041bf7c":"RMSE has the benefit of penalizing large errors more so can be more appropriate in some cases, for example, if being off by 10 is more than twice as bad as being off by 5. But if being off by 10 is just twice as bad as being off by 5, then MAE is more appropriate.<a href=\"https:\/\/medium.com\/human-in-a-machine-world\/mae-and-rmse-which-metric-is-better-e60ac3bde13d\">MAE vs RMSE<\/a>","a9c0ebdc":"# Classification <a class=\"anchor\" id=\"cl\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>[](http:\/\/)","341f74db":"## R Squared and Adjusted R Squared <a class=\"anchor\" id=\"r-2\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","94a3e7b1":"## Mean squared error MSE and Root mean squared error RMSE <a class=\"anchor\" id=\"RMSE\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","9b8050e8":"R Squared & Adjusted R Squared are often used for explanatory purposes and explains how well your selected independent variable(s) explain the variability in your dependent variable(s)","6dca744a":"## Thank you for reading \n\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>[](http:\/\/)","1dc0b568":"MAE is the average of the absolute difference between the predicted values and observed value","0f592900":"Accuracy in classification problems is the number of correct predictions made by the model over all kinds predictions made.","0531a284":"## Mean absolute error MAE and Root Mean absolute error MAE <a class=\"anchor\" id=\"MAE\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","a0520173":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*pk05QGzoWhCgRiiFbz-oKQ.png\">","f28f3f3d":"When to use Accuracy:\n\nAccuracy is a good measure when the target variable classes in the data are nearly balanced. example Survived(60% yes - 40% no)  \n","38c7b267":"## Overview \nIn this kernel, i tryed to combine the most useful evaluation metrics in classification and regerssion in Python with scikit-learn.","ce53addc":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/0*2ekvLNkZ0_cKcPtv\">","5a4b54a9":"### LOOCV <a class=\"anchor\" id=\"lo\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","b28c71bd":"## KFold <a class=\"anchor\" id=\"kf\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","66ae7058":"## Mean Squared Log Error and Root Mean Squared Log Error RMSLE <a class=\"anchor\" id=\"RMSLE\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","8ba810ae":"<h3 style=\"color:red\">If you enjoyed this work or you found it helpful , an upvotes would be very much appreciated :-) <\/h3>","e4bf9727":"When it is actually the positive result, how often does it predict correctly","17ae5a8e":"ROC curves are frequently used to show in a graphical way the connection\/trade-off between clinical sensitivity and specificity for every possible cut-off for a test or a combination of tests.","14b76f99":"Another variation on k-fold cross validation is to create a random split of the data like the train\/test split described above, but repeat the process of splitting and evaluation of the algorithm multiple times, like cross validation.\n\nThis has the speed of using a train\/test split and the reduction in variance in the estimated performance of k-fold cross validation","d46feb99":"The mean squared error tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the \u201cerrors\u201d) and squaring them.","923e408a":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/640\/1*iLabSjpdwd1TaZyKdDKYBA.png\">","c911b811":"## Precision <a class=\"anchor\" id=\"pr\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","d96710a7":"# Regression <a class=\"anchor\" id=\"reg\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","077c3b5b":"F1 score - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, <b style=\"color:red\">especially if you have an uneven class distribution<\/b>","fb134fb7":" a slight variation in the K Fold cross validation technique is made, such that each fold contains approximately the same percentage of samples of each target class as the complete set, or in case of prediction problems, the mean response value is approximately equal in all the folds. This variation is also known as Stratified K Fold.","a6df5f68":"## Recall <a class=\"anchor\" id=\"rr\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","b6847508":"#####  train test split 70% train 30% for test ","39c9eb34":"AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease.\n\nThe ROC curve is plotted with TPR against the FPR .","ccb988dd":"## Accuracy <a class=\"anchor\" id=\"ac\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","32334569":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*UJxVqLnbSj42eRhasKeLOA.png\">","d22479e0":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*me-aJdjnt3ivwAurYkB7PA.png\">","cbf90fc9":"# Cross validation <a class=\"anchor\" id=\"cv\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","0707a4f3":"<img src=\"https:\/\/miro.medium.com\/max\/313\/1*RyIMMQWd_X0Gpa0rrXzr9Q.png\" \/>","e25f3e94":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/640\/1*KhlD7Js9leo0B0zfsIfAIA.png\" \/>","0f6ca8b6":"<img style=\"height:100px\" src='https:\/\/cdn-images-1.medium.com\/max\/1600\/1*3VJyfU1qBqoHwaDJm3KAKA.gif' \/>","6812ec30":"## Repeated cv  <a class=\"anchor\" id=\"rc\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","19e4d476":"# \u2696\ufe0f Machine Learning Model Evaluation Metrics","b3a71b97":"Log loss, aka logistic loss or cross-entropy loss.\n\nThis is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of the true labels given a probabilistic classifier\u2019s predictions.","545dba0c":"R-squared is the \u201cpercent of variance explained\u201d by the model.  That is, R-squared is the fraction by which the variance of the errors is less than the variance of the dependent variable.<br>\nJust like R\u00b2, adjusted R\u00b2 also shows how well terms fit a curve or line but adjusts for the number of terms in a model.","91aa8b7c":"The metrics that you choose to evaluate your machine learning algorithms are very important.\nChoice of metrics influences how the performance of machine learning algorithms is measured and compared.","39c27892":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*-8_kogvwmL1H6ooN1A1tsQ.png\" style=\"width:50%;\">","ea44c7e5":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/880\/1*vcWDRHJKkYN75bCibPefYg.png\">","6bb2c2d2":"<img src=\"https:\/\/i.ytimg.com\/vi\/AOIkPnKu0YA\/maxresdefault.jpg\" \/>","eedb8348":"A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. ","e7e50fb7":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/640\/1*a8hkMGVHg3fl4kDmSIDY_A.png\" \/>","d46413f9":"<a class=\"anchor\" id=\"toc\"><\/a>\n<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 600px;\">\n<h1>Contents<\/h1>\n<ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n<li style=\"list-style: outside none none !important;font-size:17px\"><a href=\"#cl\">1 Classification<\/a><\/li>\n      <ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n            <li style=\"list-style: outside none none !important;\"><a href=\"#ac\">1.1 Accuracy<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#pr\">1.2 Precision<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#rr\">1.3 Recall ROC<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#f1\">1.4 F1Score<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#au\">1.5 AUC- ROC curve<\/a><\/li>\n          <li style=\"list-style: outside none none !important;\"><a href=\"#ll\">1.6 logistic loss<\/a><\/li>\n      <\/ul>\n<li style=\"list-style: outside none none !important;font-size:17px\"><a href=\"#reg\">2 Regression<\/a><\/li>\n      <ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n            <li style=\"list-style: outside none none !important;\"><a href=\"#RMSE\">2.1 Mean squared error MSE and Root mean squared error RMSE<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#MAE\">2.2 Mean absolute error MAE and Root Mean absolute error MAE<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#RMSLE\">2.3 Mean Squared Log Error and Root Mean Squared Log Error RMSLE<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#r-2\">2.4 R Squared and Adjusted R Squared<\/a><\/li>\n      <\/ul>\n    \n<li style=\"list-style: outside none none !important;font-size:17px\"><a href=\"#cv\">3 CrossValidation :<\/a><\/li>\n      <ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n            <li style=\"list-style: outside none none !important;\"><a href=\"#kf\">3.1 KFold<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#skf\">3.2 StratifiedKFold<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#lo\">3.3 LOOCV<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#rc\">3.4 Repeated cv<\/a><\/li>\n      <\/ul>    \n    \n\n<\/ul>\n<\/div>","ee852154":"Precision is defined as the number of true positives divided by the number of true positives plus the number of false positives.\nPrecision is about being precise","2e46bc81":"## F1 score <a class=\"anchor\" id=\"f1\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>"}}