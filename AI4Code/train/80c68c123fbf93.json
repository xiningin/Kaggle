{"cell_type":{"490eee37":"code","2cedc13a":"code","ade2abab":"code","914f28e5":"code","b686fc9b":"code","cae64307":"code","5183bfe5":"code","1aee0452":"code","07fc4c66":"code","7b09025e":"code","a3187405":"code","04b5b1a5":"code","113a8d99":"code","20810447":"code","dbd08233":"code","2064dd6e":"code","4327601a":"code","e5098762":"code","a6aaae8b":"markdown","2e7e4bc9":"markdown","8852833f":"markdown","e9f9debd":"markdown","64790536":"markdown","d20f4005":"markdown","3b641978":"markdown","2fc078a7":"markdown","4eadeca7":"markdown","3b4c8ecb":"markdown","2e7887de":"markdown","16b26efc":"markdown"},"source":{"490eee37":"import os\nfrom warnings import filterwarnings\n\nfrom matplotlib.pyplot import figure\nfrom pandas import read_csv\nfrom seaborn import heatmap\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom catboost import CatBoostRegressor\nfrom tqdm.notebook import tqdm\n\n\nfilterwarnings(\"ignore\")\ntqdm.pandas()\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2cedc13a":"class FreqEncoder:\n    \"\"\"Simple encoding categorical features by frequency.\"\"\"\n    \n    def __init__(self):\n        self._freqs = None\n            \n    def _freq_encoding(self, data, column):\n        \"\"\"Use frequency as encoding value\"\"\"\n        self._freqs[column] = data.groupby(column).size() \/ len(data)\n    \n    def fit(self, data):\n        \"\"\"Compute freq for each columns\"\"\"\n        self._freqs = {}\n        for column in data.columns:\n            freq_encoding(data)\n    \n    def transform(self, data):\n        \"\"\"Transform columns\"\"\"\n        if not self._freqs:\n            raise ValueError(\"Encoder didn't fit.\\n\")\n        for column in data.columns:\n            data[column] = data[column].map(self._freqs[column])\n        return data\n    \n    def fit_transform(self, data):\n        \"\"\"Compute freq and transform for each columns\"\"\"\n        self._freqs = {}\n        for column in data.columns:\n            self._freq_encoding(data, column)\n            data[column] = data[column].map(self._freqs[column])\n        return data\n    ","ade2abab":"def weird_division(num, divisor):\n    \"\"\"Function form unique and resplendent notebook.\"\"\"\n    return num \/ divisor if divisor else 0\n\n\ndef prepcoress_data(data, drop_columns):\n    \"\"\"Create target, drop unnecessary columns.\"\"\"\n    data[\"viewable\/measurable\"] = data.progress_apply(\n        lambda row: weird_division(row[\"viewable_impressions\"], row[\"measurable_impressions\"]),\n        axis=1\n    )\n    data[\"CPM\"] = data.progress_apply(\n        lambda row: weird_division(row[drop_columns[-2]] * 100, row[drop_columns[-1]]) * 1000,\n        axis=1\n    )\n    data = data[data[\"CPM\"] >= 0]\n    # drop unnecesary columns\n    data.drop(labels=drop_columns, axis=1, inplace=True)\n    return data\n\n\ndef freq_encoding(dataset, column):\n    \"\"\"Use frequency as encoding value\"\"\"\n    freq = dataset.groupby(column).size() \/ len(dataset)\n    data[column] = data[column].map(freq)\n    return data\n\n\ndef train_test(dataset, mask, train_cut=(.025, .975)):\n    \"\"\"Split data on train test validate.\"\"\"\n    # split train test\n    train = dataset[mask]\n    test = dataset[~mask]\n    # cut train 2.5% to 97.5% by default\n    quantile = (train[\"CPM\"].quantile(train_cut[0]), train[\"CPM\"].quantile(train_cut[1]))\n    train = train[train[\"CPM\"].between(*quantile)]\n    # cut test 0% to 95%\n    test = test[test[\"CPM\"] < test[\"CPM\"].quantile(.95)]\n    return train, test\n","914f28e5":"data = read_csv(\"\/kaggle\/input\/real-time-advertisers-auction\/Dataset.csv\")\ndata.head()","b686fc9b":"drop_columns = [\n    \"integration_type_id\", \"revenue_share_percent\",\n    \"viewable_impressions\",\n    \"total_revenue\", \"measurable_impressions\"\n]\ndata = prepcoress_data(data, drop_columns)\ndata.head()","cae64307":"print(f\"Count not NaN:\\n{data.notna().count()}\")","5183bfe5":"mask = (data[\"date\"] < \"2019-06-22 00:00:00\")\ntrain, test = train_test(data, mask, train_cut=(.06, 0.94))\ntrain.shape, test.shape","1aee0452":"columns = train.columns\ncat_columns = [column for column in columns if \"id\" in column]\ncat_corr = train[cat_columns].corr(method=\"spearman\")\nfigure(figsize=(14, 8))\nheatmap(cat_corr, square=True, annot= True, cmap=\"seismic\", vmin=0, vmax=1);","07fc4c66":"train.drop(labels=[\"site_id\"], axis=1, inplace=True)\ncolumns = train.columns\ncat_columns = [column for column in columns if \"id\" in column]\ncat_corr = train[cat_columns].corr(method=\"spearman\")\nfigure(figsize=(14, 8))\nheatmap(cat_corr, square=True, annot= True, cmap=\"seismic\", vmin=0, vmax=1);","7b09025e":"encoder = FreqEncoder()\ncolumns = train.columns\ncat_columns = [column for column in columns if \"id\" in column]\ntrain[cat_columns] = encoder.fit_transform(train[cat_columns])\ntest[cat_columns] = encoder.transform(test[cat_columns])\ntrain.drop(labels=[\"date\"], axis=1, inplace=True)\ntest.drop(labels=[\"date\"], axis=1, inplace=True)","a3187405":"corr = train[cat_columns].corr(method=\"spearman\")\nfigure(figsize=(14, 8))\nheatmap(corr, square=True, annot= True, cmap=\"seismic\", vmin=0, vmax=1);","04b5b1a5":"columns = train.columns\nsplit_train, split_validate = train_test_split(\n    train, test_size=0.1, shuffle=True, random_state=42\n)\nsplit_x_train, split_y_train = split_train[columns[: -1]], split_train[\"CPM\"]\nsplit_x_validate, split_y_validate = split_validate[columns[: -1]], split_validate[\"CPM\"]\nx_test, y_test = test[columns[: -1]], test[\"CPM\"]\nprint(\n    f\"Split x_tran: {split_x_train.shape}\", f\"Split y_tran: {split_y_train.shape}\",\n    f\"Split x_validate: {split_x_validate.shape}\", f\"Split y_validate: {split_y_validate.shape}\",\n    f\"Split x_test: {x_test.shape}\", f\"Split y_test: {y_test.shape}\", sep=\"\\n\"\n)","113a8d99":"model = CatBoostRegressor(random_state=42, verbose=200, iterations=3000)\nmodel.fit(\n    split_x_train, split_y_train, eval_set=(split_x_validate, split_y_validate),\n    use_best_model=True, early_stopping_rounds=20\n)","20810447":"print(\"Feature importances:\")\nfor name, importance in zip(model.feature_names_, model.feature_importances_):\n    print(f\"  -{name}: {round(importance, 4)}\")","dbd08233":"y_pred = model.predict(x_test)\nprint(f\"MSE: {mean_squared_error(y_test, y_pred)}\")","2064dd6e":"model = CatBoostRegressor(random_state=42, verbose=200, iterations=3000)\nx_train, y_train = train[columns[: -1]], train[\"CPM\"]\nmodel.fit(x_train, y_train)","4327601a":"print(\"Feature_importances\")\nfor name, importance in zip(model.feature_names_, model.feature_importances_):\n    print(f\"  -{name}: {round(importance, 4)}\")","e5098762":"y_pred = model.predict(x_test)\nprint(f\"MSE: {mean_squared_error(y_test, y_pred)}\")","a6aaae8b":"# *Drop columns with high correlation*","2e7e4bc9":"# *Split train datset on split_train, split_validate*","8852833f":"# *Split data on train and test. Cut test by 95 percentile, train quantile can be configure*","e9f9debd":"# *Apply frequency encoding to categorical columns*","64790536":"# *Preprocess dataset (create target, drop unnecessary colummns)*","d20f4005":"# *Support class and functions*","3b641978":"# *Train with validate dataset*","2fc078a7":"# *Predict by model which was fited on full train dataset*","4eadeca7":"# *Train without validate dataset*","3b4c8ecb":"# *Predict by model which was fited with validte dataset*","2e7887de":"# *Compute rank correleation*","16b26efc":"# *Read dataset*"}}