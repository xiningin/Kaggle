{"cell_type":{"e2f04e31":"code","6a62bf69":"code","36975ac9":"code","3c4ee18d":"code","28a50c06":"code","c30bdf44":"code","c36a4dfc":"code","d910d811":"code","7a995980":"code","ef03285e":"code","fe13910f":"code","68edd7da":"code","a48cf83f":"code","101cdcb8":"code","5c496c03":"code","77c5c786":"code","5834f2c7":"code","3bddd1f4":"code","f006e842":"code","6b622371":"code","dbac373b":"markdown","5c0e8839":"markdown","c44f813d":"markdown","947e9985":"markdown","e7299638":"markdown","11f1ba8e":"markdown","1dac5ed8":"markdown"},"source":{"e2f04e31":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6a62bf69":"from typing import Dict, Any, Union\n\nfrom pathlib import Path\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\n\nimport torch.utils.data as D\nfrom torch.utils.data.dataset import Dataset, IterableDataset\nfrom torch.utils.data.dataloader import DataLoader\n\nfrom transformers import AutoModel, AutoTokenizer\nfrom transformers import PreTrainedModel\n\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error","36975ac9":"DATA_PATH = Path('\/kaggle\/input\/commonlitreadabilityprize\/')\nassert DATA_PATH.exists()\nMODELS_PATH = Path('\/kaggle\/input\/commonlit-distilbart-12\/best_models')\nassert MODELS_PATH.exists()","3c4ee18d":"train_df = pd.read_csv(DATA_PATH\/'train.csv')\ntest_df = pd.read_csv(DATA_PATH\/'test.csv')\nsample_df = pd.read_csv(DATA_PATH\/'sample_submission.csv')","28a50c06":"class CONFIG():\n    model_name = 'distilbart'\n    batch_size = 32\n    max_len = 256\n    save_dir = f'trained\/{model_name}'\n    num_workers = 2\n    epochs = 20\n#     pretrained_transformers_model = '\/kaggle\/input\/commonlit-distilroberta\/distilroberta\/distilroberta\/'\n    pretrained_lm_model = '\/kaggle\/input\/commonlit-distilbart-12\/distilbart_lm'\n    n_folds = 5\n    model_offset = 6\n    model_limit = 11\n    svm_c = 10\n    svm_kernels = ['rbf']\n    \ncfg = CONFIG()","c30bdf44":"model_path = MODELS_PATH\nassert model_path.exists()","c36a4dfc":"!ls {MODELS_PATH}","d910d811":"from transformers import PreTrainedModel, AutoConfig\n\nclass CommonLitModel(PreTrainedModel):\n    def __init__(self):\n        super(PreTrainedModel, self).__init__()\n        self.transformer_model = AutoModel.from_pretrained(cfg.pretrained_lm_model)\n        self.drop = nn.Dropout(0.5)\n        self.config = AutoConfig.from_pretrained(cfg.pretrained_lm_model)\n        self.layer_norm = nn.LayerNorm(self.config.max_position_embeddings)\n        self.out = nn.Linear(self.config.max_position_embeddings, 1)\n        \n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer_model(input_ids=input_ids.squeeze(), attention_mask=attention_mask.squeeze(), output_hidden_states=False)\n#         x = transformer_out.pooler_output\n        x = transformer_out.last_hidden_state[:, 0, :] # N, C, X\n        x = self.layer_norm(x)\n        x = self.drop(x)\n        x = self.out(x)\n        return x\n    \n    def floating_point_ops(self, inputs: Dict[str, Union[torch.Tensor, Any]]):\n        \"\"\"\n        For models that inherit from :class:`~transformers.PreTrainedModel`, uses that method to compute the number of\n        floating point operations for every backward + forward pass. If using another model, either implement such a\n        method in the model or subclass and override this method.\n        Args:\n            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.\n        Returns:\n            :obj:`int`: The number of floating-point operations.\n        \"\"\"\n        return 0","7a995980":"%%time\n\ninference_models = []\nfor i in range(cfg.model_offset, cfg.model_limit):\n    print(f'Model {i}')\n    inference_model = CommonLitModel()\n    inference_model = inference_model.cuda()\n    inference_model.load_state_dict(torch.load(str(model_path\/f'{i}_pytorch_model.bin')))\n    inference_model.eval();\n    inference_models.append(inference_model)","ef03285e":"# lm_model = AutoModel.from_pretrained(cfg.pretrained_lm_model)\n# lm_model = lm_model.cuda()\n# lm_model.eval()\n# inference_models.append(lm_model)","fe13910f":"def convert_to_list(t):\n    return t.flatten().long()\n\nclass CommonLitDataset(nn.Module):\n    def __init__(self, text, test_id, tokenizer, max_len=128):\n        self.excerpt = text\n        self.test_id = test_id\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        return {'input_ids': convert_to_list(encode['input_ids']),\n                'attention_mask': convert_to_list(encode['attention_mask']),\n                'id': self.test_id[idx]}\n    \n    def __len__(self):\n        return len(self.excerpt)","68edd7da":"tokenizers = []\nfor i in range(cfg.model_offset, cfg.model_limit):\n    tokenizer = AutoTokenizer.from_pretrained(model_path\/f'tokenizer-{i}')\n    tokenizers.append(tokenizer)","a48cf83f":"def create_dl(df, tokenizer):\n    text = df['excerpt'].values\n    ids = df['id'].values\n    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.max_len)\n    return DataLoader(ds, \n                      batch_size = cfg.batch_size,\n                      shuffle=False,\n                      num_workers = 1,\n                      pin_memory=True,\n                      drop_last=False\n                     )","101cdcb8":"def get_cls_embeddings(dl, transformer_model):\n    cls_embeddings = []\n    with torch.no_grad():\n        for input_features in tqdm(dl, total=len(dl)):\n            output = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n            cls_embeddings.extend(output[0][:,0,:].detach().cpu().numpy())\n    return np.array(cls_embeddings)","5c496c03":"num_bins = int(np.ceil(np.log2(len(train_df))))\ntrain_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\nbins = train_df['bins'].values","77c5c786":"def rmse_score(X, y):\n    return np.sqrt(mean_squared_error(X, y))","5834f2c7":"%%time\n\ntrain_target = train_df['target'].values\n\ndef calc_mean(scores):\n    return np.mean(np.array(scores), axis=0)\n\nfinal_scores = []\nfinal_rmse = []\nfor j, (inference_model, tokenizer) in enumerate(zip(inference_models, tokenizers)):\n    print('Model', j)\n    test_dl = create_dl(test_df, tokenizer)\n    train_dl = create_dl(train_df, tokenizer)\n    transformer_model = inference_model.transformer_model if hasattr(inference_model, 'transformer_model') else inference_model\n    transformer_model.cuda()\n    X = get_cls_embeddings(train_dl, transformer_model)\n    y = train_target\n    X_test = get_cls_embeddings(test_dl, transformer_model)\n    kfold = StratifiedKFold(n_splits=cfg.n_folds)\n    scores = []\n    rmse_scores = []\n    for kernel in cfg.svm_kernels:\n        print('Kernel', kernel)\n        kernel_scores = []\n        kernel_rmse_scores = []\n        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n            print('Fold', k, train_idx.shape, valid_idx.shape)\n            model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n\n            X_train, y_train = X[train_idx], y[train_idx]\n            X_valid, y_valid = X[valid_idx], y[valid_idx]\n            model.fit(X_train, y_train)\n            prediction = model.predict(X_valid)\n            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n            print('rmse_score', kernel_rmse_scores[k])\n            kernel_scores.append(model.predict(X_test))\n        scores.append(calc_mean(kernel_scores))\n        rmse_scores.append(calc_mean(kernel_rmse_scores))\n    final_scores.append(calc_mean(scores))\n    final_rmse.append(calc_mean(rmse_scores))\nprint('FINAL RMSE score', np.mean(np.array(final_rmse)))","3bddd1f4":"sample_df['target'] = np.mean(np.array(final_scores), axis=0)\n# sample_df['target'] = len(final_scores) \/ np.sum(1 \/ np.array(final_scores), axis=0) # harmonic mean\nsample_df","f006e842":"pd.DataFrame(sample_df).to_csv('submission.csv', index=False)","6b622371":"!cat submission.csv","dbac373b":"### DataSet and Tokenizers","5c0e8839":"### Read Existing Models","c44f813d":"#### Training","947e9985":"#### Extract Embeddings","e7299638":"### Configuration","11f1ba8e":"#### Extract Number of Bins","1dac5ed8":"### Folders and Dataframes"}}