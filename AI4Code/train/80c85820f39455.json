{"cell_type":{"be019927":"code","b66b56ed":"code","b2f7a20a":"code","1507c386":"code","7dbda5f8":"code","3535223d":"code","098759a4":"code","3663bff3":"code","1294bd0d":"code","ee43e0b1":"code","8c42aa26":"code","52368dad":"code","8dc2db8f":"code","d7d8fb6b":"code","2fe8c1f4":"code","0db59e7c":"code","8b6babfe":"markdown","db6ea04c":"markdown","eef3f09a":"markdown","963ef141":"markdown"},"source":{"be019927":"!pip install autogluon\n!pip uninstall lightgbm -y\n!pip install lightgbm --install-option=--gpu","b66b56ed":"# imports that we will need\nimport pandas as pd                              # Bread and butter of data science\nfrom autogluon.tabular import TabularPredictor   # We want a tabular predictor from Autogluon\nimport os                                        # Operating system, for you know, the operating system\nimport numpy as np                               # Another bread and butter","b2f7a20a":"# just grab the files that we will need and save them to a dictionary for easy reference\n\ndata_files_paths = {}\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        file_name = filename.split('.')[0]\n        data_files_paths[file_name] = os.path.join(dirname, filename)","1507c386":"# This function was reused from a fellow Kaggler that is very useful in reducing the size of DataFrames\ndef reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n    \n            # test if column can be converted to an integer\n            asint = props[col].astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return props\n","7dbda5f8":"# Load the training data, reduce its memory footprint, and check if it looks semi decent\n\ntrain_data = pd.read_csv(data_files_paths.get('train'), index_col=0)\ntrain_data = reduce_mem_usage(train_data)\ntrain_data.head()","3535223d":"# adding some extra goodies to the DataFrame\ndef add_aux_columns(df: pd.DataFrame):\n    df['sum'] = train_data[[f'f{i}'for i in range(100)]].sum(axis=1)\n    df['mean'] = train_data[[f'f{i}'for i in range(100)]].mean(axis=1)\n    df['median'] = train_data[[f'f{i}'for i in range(100)]].mean(axis=1)\n    return df","098759a4":"train_data = add_aux_columns(df=train_data)","3663bff3":"# Variables for AutoGluon\nlabel_column = 'target'\neval_metric = 'roc_auc'\nsave_path = '\/kaggle\/working\/AutoGluonModelAkaTheBeast'\ntime_limit = 3600 # time limit for autogluon in seconds","1294bd0d":"predictor = TabularPredictor(\n    label = label_column,\n    eval_metric = eval_metric,\n    path = save_path\n)","ee43e0b1":"predictor.fit(\n    train_data,\n    presets='best_quality',\n    time_limit=time_limit,\n    verbosity=3,\n    ag_args_fit={'num_gpus': 1}\n)","8c42aa26":"predictor.leaderboard(train_data.iloc[:1000], silent=True)","52368dad":"test_data = pd.read_csv(data_files_paths.get('test'), index_col=0)","8dc2db8f":"# to be consistent, lets reduce the memory of the test dataset even though it *shouldn't* make a difference\ntest_data = reduce_mem_usage(test_data)\n# Do not forget to add extra columns\ntest_data = add_aux_columns(df=test_data)","d7d8fb6b":"predictions = predictor.predict_proba(test_data)","2fe8c1f4":"predictions = predictions[1].reset_index()\npredictions.columns = ['id', 'target']","0db59e7c":"# SAVE\npredictions[['id', 'target']].to_csv('\/kaggle\/working\/submission.csv', index=False)","8b6babfe":"**Stretching AutoGluon's Legs (SAL)**\n\nFor reference, make some tea, sit back and [RTFM](https:\/\/auto.gluon.ai\/stable\/tutorials\/tabular_prediction\/index.html)\n\nLets first get some house keeping out of the way by installing the necessary packages such as, you guessed it, AutoGluon","db6ea04c":"Let's have a look at what AutoGluon has done ","eef3f09a":"From my experiments, AutoGluon can take some time to train. It does train quite a few classification models in the process so that is expected so patience is key here. \n\nA note from my experiments: the longer you train the longer your inference step is going to be but luckily there is no time limit on inference so train for as long as you dare.\n\n\nFor reference on a 12 core machine with 32GB RAM, a model that was trained for 12 hours took about 5 hours to do inference on the test dataset for the previous Tabular Comp.\n\n\n","963ef141":"Nice! Its trained some models and bagged them, then fitted those bags in another model.\n\n\nEnough scenery watching, time to load the test data and perform some inference"}}