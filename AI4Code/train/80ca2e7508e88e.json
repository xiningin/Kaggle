{"cell_type":{"8c55950d":"code","cf0172c9":"code","06e1a57f":"code","a55eba2b":"code","62ffbd5e":"code","d4f3289f":"code","fe04b4e2":"code","dc7778f2":"code","7408000d":"code","2fe87045":"markdown","0d1486dc":"markdown","e538e290":"markdown","0a95bed7":"markdown","9f5e53d1":"markdown","b0540701":"markdown","603c0cb4":"markdown"},"source":{"8c55950d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom datetime import date, datetime\nfrom scipy import stats\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nimport plotly.graph_objs as go\nimport plotly.express as px\n\nfrom sklearn.metrics import mean_squared_error\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cf0172c9":"data = pd.read_csv('..\/input\/nba2k20-player-dataset\/nba2k20-full.csv')\ndata.head(5)","06e1a57f":"data.info()","a55eba2b":"def prepare_data(data: pd.DataFrame):\n    '''\n        Preprocesses data\n    '''\n    def calculateAge(birthDate: str):\n        '''\n        calculates age of person, on given birth day\n        '''\n        datetime_object = datetime.strptime(birthDate, '%m\/%d\/%y')\n        today = date.today() \n        age = today.year - datetime_object.year -  ((today.month, today.day) < (datetime_object.month, datetime_object.day)) \n        return age \n    \n    data['jersey'] = data['jersey'].apply(lambda x: int(x[1:]))\n    data['age'] = data['b_day'].apply(calculateAge)\n    data['height'] = data['height'].apply(lambda x: float(x.split('\/')[1]))\n    data['weight'] = data['weight'].apply(lambda x: float(x.split('\/')[1].split(' ')[1]))\n    data['salary'] = data['salary'].apply(lambda x: float(x[1:]))\n    data['draft_round'].replace('Undrafted', 0, inplace = True)\n    data['draft_round'] = data['draft_round'].apply(int)\n    data['team'] = data['team'].fillna('No team')\n    data['college'] = data['college'].fillna('No education')\n    data.drop(['b_day', 'draft_peak'], axis = 1, inplace = True)","62ffbd5e":"\ndata = pd.read_csv('..\/input\/nba2k20-player-dataset\/nba2k20-full.csv')\nprepare_data(data)\n\n#creating categories to teams by mean salary\nsalary = data[['salary', 'team']]\nnew_sal = salary.groupby('team').mean().reset_index()\nboundaries = [np.NINF, 7E+6, 7.6E+6, 8.1E+6, 9E+6, 9.5E+6, np.Inf]\nnew_sal['team_salary'] = pd.cut(salary.groupby('team').mean().\\\n                                reset_index()['salary'], bins=boundaries)\nnew_sal.drop(['salary'], axis = 1, inplace = True)\n#merging this categories to data\ndata = data.merge(new_sal, on = 'team', how = 'left')\n\n#removing imbalanced data\ndata.loc[data['country'] != 'USA', 'country'] = 'not USA'\ndata.loc[data['position'] == 'C-F', 'position'] = 'F-C'\ndata.loc[data['position'] == 'F-G', 'position'] = 'F'\ndata.loc[data['position'] == 'G-F', 'position'] = 'F'\n\n# we should drop full_name because it doesn't have anything meaning for this type of model\n# we should drop jersey because it doesn't have high correlation\n# we should drop team because we have already preprocessed it\n# For now we should drop college because there is too much colleges with just 5 or less occurances\ndata = data.drop(['full_name', 'jersey',  'team', 'college'], axis = 1)\n\n# converting categorical data to one-hot encoding\ndata = pd.get_dummies(data, \n                      columns = ['team_salary', 'position', 'country', 'draft_round'],\n                      drop_first = True)\n\nX, y = data.drop(['salary'], axis = 1), data['salary']\n#normalizing input features\nnormalizer = preprocessing.Normalizer().fit(X)\nX = normalizer.transform(X)\n","d4f3289f":"#Split data into random train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","fe04b4e2":"from xgboost import XGBRegressor\n\nmodel = XGBRegressor( \n    n_estimators = 300,\n    learning_rate=0.06,\n    colsample_bytree=0.9, \n    min_child_weight=3,\n    max_depth = 2,\n    subsample = 0.63,\n    eta = 0.1,\n    seed=0)\n\n\nmodel = model.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    early_stopping_rounds=20,\n    eval_set=[(X_test,y_test)],\n    verbose=False)\n\npredictions = model.predict(X_test)\n\n\n\nprint(\"RMSE: \", round(np.sqrt(mean_squared_error(y_test, predictions)), 2))","dc7778f2":"x_ax = list(range(len(y_test)))\nfig = go.Figure([go.Scatter(x=x_ax, y=y_test, name='original'), go.Scatter(x=x_ax, y=predictions, name='predicted')])\nfig.show()","7408000d":"def plot_features(booster):    \n    importance = pd.DataFrame({'importance': model.feature_importances_, \\\n                               'name' : data.drop('salary', axis=1).columns})\n\n    fig = px.bar(importance.sort_values(by='importance', ascending=True), \n                 x = 'importance', y = 'name')\n    fig.show()\n  \n\nplot_features(model)","2fe87045":"# **Predict salary for players**","0d1486dc":"As we can see, we have only 2 int columns, but we have data, that we can preprocess to get numerical data","e538e290":"We can see, that our model good in detecting high salaries(10M+), but have some troubles in detecting smaller salaries. It maybe can be improved by collecting historical data to expand dataset or by using some tricky feature engineering techniques, that can be advised by person who has good knowledge in this domain","0a95bed7":"Let's take a look on data types","9f5e53d1":"Feature engineering","b0540701":"I choose xgboost, because it has good performance and, as bonus, it is fast. This library widely used in production and Kaggle because of its highly accuracy and ease-in-use.  ","603c0cb4":"On this plot we can see features importance. "}}