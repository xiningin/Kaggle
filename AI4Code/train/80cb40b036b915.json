{"cell_type":{"0cac8b31":"code","299d9ed8":"code","e6fe73bb":"code","3ba85780":"code","ec2cb34e":"code","0a96f582":"code","140d2150":"code","a2190491":"code","0886e0f7":"code","dd0cf275":"code","960df25a":"code","3c7c0b4b":"code","a2ddcb78":"code","6fb2ed07":"code","e8eab890":"code","57af6dc9":"code","41ea3a67":"code","aa8ec245":"code","a7068a24":"code","c5b58258":"code","197cb768":"markdown","e7fbe3f1":"markdown","70aa94ba":"markdown","ab918d52":"markdown","4b1b4b93":"markdown","aaad7c6b":"markdown","29f9f24e":"markdown","3d36e932":"markdown","57fdd195":"markdown","6c08140c":"markdown","081773dc":"markdown","6b774298":"markdown","7bdd9158":"markdown","471e6164":"markdown","16ebaa93":"markdown","646d3667":"markdown","867506c8":"markdown"},"source":{"0cac8b31":"import numpy as np \nimport pandas as pd \nimport os\n\nimport matplotlib.pyplot as plt\nimport csv\nimport itertools\nimport collections\n\nimport pywt\nfrom scipy import stats\n\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, AvgPool1D, Flatten, Dense, Dropout, Softmax\nfrom keras.optimizers import Adam \nfrom keras.utils.np_utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\nfrom keras import regularizers\n\n\n%matplotlib inline","299d9ed8":"plt.rcParams[\"figure.figsize\"] = (30,6)\nplt.rcParams['lines.linewidth'] = 1\nplt.rcParams['lines.color'] = 'b'\nplt.rcParams['axes.grid'] = True ","e6fe73bb":"def denoise(data): \n    w = pywt.Wavelet('sym4')\n    maxlev = pywt.dwt_max_level(len(data), w.dec_len)\n    threshold = 0.04 # Threshold for filtering\n\n    coeffs = pywt.wavedec(data, 'sym4', level=maxlev)\n    for i in range(1, len(coeffs)):\n        coeffs[i] = pywt.threshold(coeffs[i], threshold*max(coeffs[i]))\n        \n    datarec = pywt.waverec(coeffs, 'sym4')\n    \n    return datarec\n","3ba85780":"path = '\/kaggle\/input\/mitbit-arrhythmia-database\/mitbih_database\/'\nwindow_size = 180\nmaximum_counting = 10000\n\nclasses = ['N', 'L', 'R', 'A', 'V']\nn_classes = len(classes)\ncount_classes = [0]*n_classes\n\nX = list()\ny = list()","ec2cb34e":"# Read files\nfilenames = next(os.walk(path))[2]\n\n# Split and save .csv , .txt \nrecords = list()\nannotations = list()\nfilenames.sort()","0a96f582":"# segrefating filenames and annotations\nfor f in filenames:\n    filename, file_extension = os.path.splitext(f)\n    \n    # *.csv\n    if(file_extension == '.csv'):\n        records.append(path + filename + file_extension)\n\n    # *.txt\n    else:\n        annotations.append(path + filename + file_extension)","140d2150":"# Records\nfor r in range(0,len(records)):\n    signals = []\n\n    with open(records[r], 'rt') as csvfile:\n        spamreader = csv.reader(csvfile, delimiter=',', quotechar='|') # read CSV file\\\n        row_index = -1\n        for row in spamreader:\n            if(row_index >= 0):\n                signals.insert(row_index, int(row[1]))\n            row_index += 1\n            \n    # Plot an example to the signals\n    if r is 1:\n        # Plot each patient's signal\n        plt.title(records[1] + \" Wave\")\n        plt.plot(signals[0:700])\n        plt.show()\n        \n    signals = denoise(signals)\n    # Plot an example to the signals\n    if r is 1:\n        # Plot each patient's signal\n        plt.title(records[1] + \" wave after denoised\")\n        plt.plot(signals[0:700])\n        plt.show()\n        \n    signals = stats.zscore(signals)\n    # Plot an example to the signals\n    if r is 1:\n        # Plot each patient's signal\n        plt.title(records[1] + \" wave after z-score normalization \")\n        plt.plot(signals[0:700])\n        plt.show()\n    \n    # Read anotations: R position and Arrhythmia class\n    example_beat_printed = False\n    with open(annotations[r], 'r') as fileID:\n        data = fileID.readlines() \n        beat = list()\n\n        for d in range(1, len(data)): # 0 index is Chart Head\n            splitted = data[d].split(' ')\n            splitted = filter(None, splitted)\n            next(splitted) # Time... Clipping\n            pos = int(next(splitted)) # Sample ID\n            arrhythmia_type = next(splitted) # Type\n            if(arrhythmia_type in classes):\n                arrhythmia_index = classes.index(arrhythmia_type)\n#                 if count_classes[arrhythmia_index] > maximum_counting: # avoid overfitting\n#                     pass\n#                 else:\n                count_classes[arrhythmia_index] += 1\n                if(window_size <= pos and pos < (len(signals) - window_size)):\n                    beat = signals[pos-window_size:pos+window_size]     ## REPLACE WITH R-PEAK DETECTION\n                    # Plot an example to a beat    \n                    if r is 1 and not example_beat_printed: \n                        plt.title(\"A Beat from \" + records[1] + \" Wave\")\n                        plt.plot(beat)\n                        plt.show()\n                        example_beat_printed = True\n\n                    X.append(beat)\n                    y.append(arrhythmia_index)\n\n# data shape\nprint(np.shape(X), np.shape(y))\n","a2190491":"for i in range(0,len(X)):\n        X[i] = np.append(X[i], y[i])\n#         X[i].append(y[i])\n\nprint(np.shape(X))","0886e0f7":"X_train_df = pd.DataFrame(X)\nper_class = X_train_df[X_train_df.shape[1]-1].value_counts()\nprint(per_class)\nplt.figure(figsize=(20,10))\nmy_circle=plt.Circle( (0,0), 0.7, color='white')\nplt.pie(per_class, labels=['N', 'L', 'R', 'A', 'V'], colors=['tab:blue','tab:orange','tab:purple','tab:olive','tab:green'],autopct='%1.1f%%')\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.show()","dd0cf275":"df_1=X_train_df[X_train_df[X_train_df.shape[1]-1]==1]\ndf_2=X_train_df[X_train_df[X_train_df.shape[1]-1]==2]\ndf_3=X_train_df[X_train_df[X_train_df.shape[1]-1]==3]\ndf_4=X_train_df[X_train_df[X_train_df.shape[1]-1]==4]\n# df_5=X_train_df[X_train_df[X_train_df.shape[1]-1]==5]\ndf_0=(X_train_df[X_train_df[X_train_df.shape[1]-1]==0]).sample(n=5000,random_state=42)\n\ndf_1_upsample=resample(df_1,replace=True,n_samples=5000,random_state=122)\ndf_2_upsample=resample(df_2,replace=True,n_samples=5000,random_state=123)\ndf_3_upsample=resample(df_3,replace=True,n_samples=5000,random_state=124)\ndf_4_upsample=resample(df_4,replace=True,n_samples=5000,random_state=125)\n# df_5_upsample=resample(df_5,replace=True,n_samples=5000,random_state=126)\n\n# X_train_df=pd.concat([df_0,df_1_upsample,df_2_upsample,df_3_upsample,df_4_upsample,df_5_upsample])\nX_train_df=pd.concat([df_0,df_1_upsample,df_2_upsample,df_3_upsample,df_4_upsample])","960df25a":"per_class = X_train_df[X_train_df.shape[1]-1].value_counts()\nprint(per_class)\nplt.figure(figsize=(20,10))\nmy_circle=plt.Circle( (0,0), 0.7, color='white')\nplt.pie(per_class, labels=['N', 'L', 'R', 'A', 'V'], colors=['tab:blue','tab:orange','tab:purple','tab:olive','tab:green'],autopct='%1.1f%%')\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.show()","3c7c0b4b":"train, test = train_test_split(X_train_df, test_size=0.20)\n\nprint(\"X_train : \", np.shape(train))\nprint(\"X_test  : \", np.shape(test))","a2ddcb78":"target_train=train[train.shape[1]-1]\ntarget_test=test[test.shape[1]-1]\ntrain_y=to_categorical(target_train)\ntest_y=to_categorical(target_test)\nprint(np.shape(train_y), np.shape(test_y))","6fb2ed07":"train_x = train.iloc[:,:train.shape[1]-1].values\ntest_x = test.iloc[:,:test.shape[1]-1].values\ntrain_x = train_x.reshape(len(train_x), train_x.shape[1],1)\ntest_x = test_x.reshape(len(test_x), test_x.shape[1],1)\nprint(np.shape(train_x), np.shape(test_x))","e8eab890":"# Instanciate an empty model\nmodel = Sequential()\n\n# Adding a Convolution Layer C1\n# Input shape = N = (360 x 1)\n# No. of filters  = 16\n# Filter size = f = (13 x 1)\n# Padding = true\n# Strides = S = 1\n# Size of each feature map in C1 is (N-f+2P)\/S +1 = 360-13+2*6 +1 = 360\n# No. of parameters between input layer and C1 = (13*1 + 1)*16 = 224\nmodel.add(Conv1D(filters=16, kernel_size=13, padding='same', activation='relu',input_shape=(360, 1)))\n\n\n# Adding an Average Pooling Layer S2\n# Input shape = N = (360 x 16)\n# No. of filters = 16\n# Filter size = f = (3 x 1)\n# Padding = P = 0\n# Strides = S = 2\n# Size of each feature map in S2 is (N-f+2P+1)\/S = (360-3+0+1)\/2 = 179\n# No. of parameters between C1 and S2 = (1+1)*16 = 32\nmodel.add(AvgPool1D(pool_size=3, strides=2))\n\n\n\n\n\n# Adding a Convolution Layer C3\n# Input shape = N = (179 x 16)\n# No. of filters  = 32\n# Filter size = f = (15 x 1)\n# Padding = true\n# Strides = S = 1\n# Size of each feature map in C1 is (N-f+2P)\/S +1 = 179-15+2*7 +1 = 360\n# No. of parameters between input layer and C1 = (15*1 + 1)*32 = 512\nmodel.add(Conv1D(filters=32, kernel_size=15, padding='same', activation='relu'))\n\n\n# Adding an Average Pooling Layer S4\n# Input shape = N = (179 x 32)\n# No. of filters = 32\n# Filter size = f = (3 x 1)\n# Padding = P = 0\n# Strides = S = 2\n# Size of each feature map in S2 is (N-f+2P+1)\/S = (179-3+0+1)\/2 = 89\n# No. of parameters between C1 and S2 = (1+1)*32 = 64\nmodel.add(AvgPool1D(pool_size=3, strides=2))\n\n\n\n# Adding a Convolution Layer C5\n# Input shape = N = (89 x 32)\n# No. of filters  = 64\n# Filter size = f = (17 x 1)\n# Padding = true\n# Strides = S = 1\n# Size of each feature map in C1 is (N-f+2P)\/S +1 = 89-17+2*8 +1 = 89\n# No. of parameters between input layer and C1 = (17*1 + 1)*64 = 1152\nmodel.add(Conv1D(filters=64, kernel_size=17, padding='same', activation='relu'))\n\n\n\n# Adding an Average Pooling Layer S6\n# Input shape = N = (89 x 64)\n# No. of filters = 64\n# Filter size = f = (3 x 1)\n# Padding = P = 0\n# Strides = S = 2\n# Size of each feature map in S2 is (N-f+2P+1)\/S = (89-3+0+1)\/2 = 44\n# No. of parameters between C1 and S2 = (1+1)*64 = 128\nmodel.add(AvgPool1D(pool_size=3, strides=2))\n\n\n\n# Adding a Convolution Layer C7\n# Input shape = N = (44 x 64)\n# No. of filters  = 128\n# Filter size = f = (19 x 1)\n# Padding = true\n# Strides = S = 1\n# Size of each feature map in C1 is (N-f+2P)\/S +1 = 44-19+2*9 +1 = 44\n# No. of parameters between input layer and C1 = (19*1 + 1)*128 = 2560\nmodel.add(Conv1D(filters=128, kernel_size=19, padding='same', activation='relu'))\n\n\n# Adding an Average Pooling Layer S8\n# Input shape = N = (44 x 128)\n# No. of filters = 128\n# Filter size = f = (3 x 1)\n# Padding = P = 0\n# Strides = S = 2\n# Size of each feature map in S2 is (N-f+2P+1)\/S = (44-3+0+1)\/2 = 21\n# No. of parameters between C1 and S2 = (1+1)*128 = 256\nmodel.add(AvgPool1D(pool_size=3, strides=2))\n\n\n\n# Flattening the layer \n# There would be 128*(21*1) = 2688 neurons\nmodel.add(Flatten())\n\n\n# Dropout the layer S9\nmodel.add(Dropout(0.5))\n\n\n\n# Adding a Dense layer with regularization l2(0.0001)# S10\n# No. of inputs = 2688\n# No. of outputs = 35\n# No. of parameters = 2688 x 35 + 35 = 94115\nmodel.add(Dense(35,kernel_regularizer=regularizers.l2(0.0001), bias_regularizer=regularizers.l2(0.0001)))\n\n\n\n# Adding a Dense layer with regularization l2(0.0001)# S11\n# No. of inputs = 35\n# No. of outputs = 5\n# No. of parameters = 35*5 + 5 = 180\nmodel.add(Dense(5,kernel_regularizer=regularizers.l2(0.0001), bias_regularizer=regularizers.l2(0.0001)))\n\n\n\n# Adding a Softmax layer S12\n# No. of inputs = 5\n# No. of outputs = 5\nmodel.add(Softmax())\n\n\nmodel.summary()","57af6dc9":"model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])","41ea3a67":"history = model.fit(train_x, train_y, batch_size=36, epochs=60, verbose=1, validation_data=(test_x, test_y))","aa8ec245":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","a7068a24":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","c5b58258":"score = model.evaluate(test_x, test_y)\n\nprint('Test Loss:', score[0])\nprint('Test accuracy:', score[1])","197cb768":"# Variables Definitions","e7fbe3f1":"# Imports\n","70aa94ba":"# Classes Distribution","ab918d52":"## Finding the loss and accuracy of the model","4b1b4b93":"# Rebalancing Classes","aaad7c6b":"**Plotting the loss convergence**","29f9f24e":"## Fitting the model","3d36e932":"This is an implementation to the paper **\"A Study on Arrhythmia via ECG Signal Classification Using the Convolutional Neural Network\"**. \n\nThe author of the paper said that the proposed algorithm is implemented in Matlab, but we cannot find this implementation, so we implemented it in Python\n","57fdd195":"# Prepere Input Files","6c08140c":"## Compiling the model","081773dc":"## Building the Model Architecture\n","6b774298":"# A Study on Arrhythmia via ECG Signal Classification Using the Convolutional Neural Network\n\nYomna Hesham Amin Fekry  *(G19093453)*\n\nAhmed Mohamed Farouk  *(2000630)*","7bdd9158":"**Plotting accuracy**","471e6164":"# Train-Test Split","16ebaa93":"# Methods Definitions","646d3667":"# Data Extraction and Preprocessing","867506c8":"# matplotlib Settings"}}