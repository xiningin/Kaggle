{"cell_type":{"2f578c56":"code","2d6ab37c":"code","7288048e":"code","02782f06":"code","cfab8293":"code","fd3da01d":"code","075f9e6e":"code","c190742f":"code","ad4cf220":"code","5a7de5d8":"code","302c1c8f":"code","48b49e35":"code","cfe827a8":"code","24000131":"code","bf8149df":"code","59219530":"code","4d5c53ee":"code","0d277755":"code","e53c518a":"code","0ae07414":"code","98697c05":"code","262a3e19":"code","e283de6c":"code","d778aae1":"code","e5dcb941":"markdown"},"source":{"2f578c56":"#Import pertinent libraries\nimport re\nimport pandas as pd # CSV file I\/O (pd.read_csv)\nfrom nltk.corpus import stopwords\nimport numpy as np #linear algebra\nimport sklearn #sci-kit ML\nimport nltk #natural language processing\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score ,confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \nimport warnings\nwarnings.filterwarnings('ignore')","2d6ab37c":"df = pd.read_csv('..\/input\/techcrunch_posts.csv')\n#cleaning the data\ndf[\"content\"] = df[\"content\"].str.replace(\"\\\\n\", \" \")\n#dropping 'id', 'img_src', 'url'\ndf.drop(['id', 'img_src', 'url', 'date'], axis = 1, inplace =True)\ndf.dropna(subset=['authors'], inplace=True)","7288048e":"#create new column concatenating title and content \ndf['info'] = df[['title', 'content']].apply(lambda x: ' '.join(str(value) for value in x), axis=1)\n#remove content\ndf.drop(['content'], axis = 1, inplace =True)\n#remove articles in which there are no authors\n#display first row\npd.set_option('display.max_colwidth', -1)\ndf.head(1)","02782f06":"# pdf = df\n# pdf['category']=pdf['category'].astype('category').cat.codes\n# pdf['section']=pdf['section'].astype('category').cat.codes\n# pdf['authors']=pdf['authors'].astype('category').cat.codes\n# pdf.corr(method='pearson')","cfab8293":"df.groupby(by=\"category\", sort = False).size()","fd3da01d":"newdf = df.groupby('category').filter(lambda x : len(x)>150)","075f9e6e":"fig, ax = plt.subplots(1, 1, figsize=(35,10))\nsns.countplot(x = \"category\", data = newdf)","c190742f":"fig, ax = plt.subplots(1, 1, figsize=(12,12))\nnewdf['category'].value_counts().plot.pie( autopct = '%1.1f%%')","ad4cf220":"total_authors = newdf.authors.nunique()\narticle_cnt = newdf.shape[0]\nprint('Total Number of authors : ', total_authors)\nprint('avg articles written by per author: ' + str(article_cnt\/\/total_authors))\nprint('Total news counts : ' + str(article_cnt))","5a7de5d8":"authors_article_cnt = newdf.authors.value_counts()\nsum_articles = 0\nauthor_cnt = 0\nfor author_articles in authors_article_cnt:\n    author_cnt += 1\n    if author_articles < 80:\n        break\n    sum_articles += author_articles\nprint('{} authors write {} articles, so {} % of authors produce {} % of Tech Crunch articles'.\n      format(author_cnt, sum_articles, format((author_cnt*100\/total_authors), '.2f'), format((sum_articles*100\/article_cnt), '.2f')))","302c1c8f":"newdf.authors.value_counts()[0:5]","48b49e35":"#Investigating the content of Tech Crunch's top author and filtering low category counts\nauthor_name = 'Natasha Lomas'\nauthor_articles_instance = newdf[newdf['authors'] == author_name]\nauthordf = author_articles_instance.groupby(by='category').filter(lambda x: len(x) > 10)\nauthordf = authordf.groupby(by='category').size()\nauthordf","cfe827a8":"#Observing the most popular genres for Sarah Perez, we note that author likely corresponds to category\nfig, ax = plt.subplots(1, 1, figsize=(10,10))\nauthordf.plot.pie( autopct = '%1.1f%%')","24000131":"# Split the data into train and test.\nX_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(newdf[['info', 'authors']], newdf['category'], test_size=0.33)","bf8149df":"# Convert pandas series into numpy array\nX_train = np.array(X_train);\nX_test = np.array(X_test);\nY_train = np.array(Y_train);\nY_test = np.array(Y_test);\ncleanTitles_train = [] #To append processed titles\ncleanTitles_test = [] #To append processed titles\nnumber_reviews_train = len(X_train) #Calculating the number of reviews\nnumber_reviews_test = len(X_test) #Calculating the number of reviews","59219530":"from nltk.stem import PorterStemmer, WordNetLemmatizer\nlemmetizer = WordNetLemmatizer()\nstemmer = PorterStemmer()\ndef get_words(titles_list):\n    titles = titles_list[0]   \n    author_names = [x for x in titles_list[1].lower().replace('and',',').replace(' ', '').split(',') if x != '']\n    titles_only_letters = re.sub('[^a-zA-Z]', ' ', titles)\n    words = nltk.word_tokenize(titles_only_letters.lower())\n    stops = set(stopwords.words('english'))\n    meaningful_words = [lemmetizer.lemmatize(w) for w in words if w not in stops]\n    return ' '.join(meaningful_words + author_names)","4d5c53ee":"#cleaning excess data in X_train\nfor i in range(0,1063):\n    np.delete(X_train, len(X_train)-i)","0d277755":"for i in range(0,number_reviews_train):\n    cleanTitle = get_words(X_train[i]) #Processing the data and getting words with no special characters, numbers or html tags\n    cleanTitles_train.append( cleanTitle )","e53c518a":"for i in range(0,number_reviews_test):\n    cleanTitle = get_words(X_test[i]) #Processing the data and getting words with no special characters, numbers or html tags\n    cleanTitles_test.append( cleanTitle )","0ae07414":"vectorize = sklearn.feature_extraction.text.TfidfVectorizer(analyzer = \"word\", max_features=1000)\ntfidwords_train = vectorize.fit_transform(cleanTitles_train)\nX_train = tfidwords_train.toarray()\n\ntfidwords_test = vectorize.transform(cleanTitles_test)\nX_test = tfidwords_test.toarray()","98697c05":"from sklearn.svm import LinearSVC\n\nmodel = LinearSVC()\nmodel.fit(X_train,Y_train)\nY_predict = model.predict(X_test)\naccuracy = accuracy_score(Y_test,Y_predict)*100\nprint(format(accuracy, '.2f'))","262a3e19":"logistic_Regression = LogisticRegression()\nlogistic_Regression.fit(X_train,Y_train)\nY_predict = logistic_Regression.predict(X_test)\naccuracy = accuracy_score(Y_test,Y_predict)*100\nprint(format(accuracy, '.2f'))","e283de6c":"from sklearn.ensemble import BaggingClassifier\nmodel = BaggingClassifier(random_state=0, n_estimators=10)\nmodel.fit(X_train, Y_train)\nprediction = model.predict(X_test)\nprint('Accuracy of bagged KNN is :', accuracy_score(prediction, Y_test)*100, '%')","d778aae1":"from sklearn.naive_bayes import MultinomialNB\n\nmodel = MultinomialNB(alpha=0.1)\nmodel.fit(X_train,Y_train)\nY_predict = model.predict(X_test)\naccuracy = accuracy_score(Y_test,Y_predict)*100\nprint(format(accuracy, '.2f'))","e5dcb941":"<font size=\"5\">We note a poor correlation between category\/section and category\/authors.<\/font>\n"}}