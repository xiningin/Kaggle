{"cell_type":{"1f1d1818":"code","6f4026e4":"code","65f44055":"code","6cb30bfb":"code","569bfa59":"code","67404472":"code","71d7c365":"code","06361573":"code","be01107b":"code","2f206f00":"code","442cb2dc":"code","9df0b790":"code","75a33476":"code","bcf21e30":"code","41030bf2":"code","f7d8379a":"code","f488263e":"code","fa413ce0":"code","c582a21b":"code","3a3a26d3":"code","59888923":"code","c701dc3c":"code","b476b5d3":"code","b77b2b76":"code","a340108d":"code","39915ca7":"code","49146110":"code","1986cb6d":"code","312c258b":"code","9557d84d":"code","53d3c202":"code","6c283d0a":"code","5d3388ae":"code","fd931c85":"code","ef0c1830":"code","3e69cce3":"code","4d38cafb":"code","f7e37730":"code","c0d7a82d":"code","4d8694f1":"code","912cfc23":"code","184dcfcc":"code","d9890b5d":"code","941d24fb":"code","0557dfba":"code","ea593f9a":"code","17193e96":"code","0079cd0d":"markdown","c575b080":"markdown","597233f2":"markdown","e96b5942":"markdown","24344fc3":"markdown","7cead7af":"markdown","8b2fadf4":"markdown","cce460cc":"markdown","4378a655":"markdown","6044f156":"markdown","7ef46440":"markdown","1bd63ecd":"markdown","7a17af20":"markdown","a189515c":"markdown","38a0ede8":"markdown","6b56edf5":"markdown","bef8b24d":"markdown","77c2e06d":"markdown","0dedbf5d":"markdown","7f0144c2":"markdown","24760dc8":"markdown","0fd31c44":"markdown","4217a7cf":"markdown","48f95601":"markdown","b4d70018":"markdown","8321a3d0":"markdown","68eab0ff":"markdown","b3ee44ca":"markdown"},"source":{"1f1d1818":"pip install yfinance","6f4026e4":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nimport yfinance as yf\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nimport re\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")","65f44055":"pd.set_option('display.max_columns', 30)\nsns.set_context(\"paper\", font_scale=2)\n\nPATH = \"\/kaggle\/input\/tweets-about-the-top-companies-from-2015-to-2020\/\"","6cb30bfb":"company = pd.read_csv(os.path.join(PATH, \"Company.csv\"))\ncompany = company.set_index(\"ticker_symbol\").to_dict()[\"company_name\"]\ncompany","569bfa59":"tweet = pd.read_csv(os.path.join(PATH, \"Tweet.csv\"))\ntweet.head()","67404472":"tweet['datetime'] = pd.to_datetime(tweet['post_date'], unit='s')\ntweet = tweet.drop(['post_date'], axis=1, inplace=False)","71d7c365":"company_tweet = pd.read_csv(os.path.join(PATH, \"Company_Tweet.csv\"))\ncompany_tweet.head()","06361573":"company_tweet.loc[company_tweet['ticker_symbol'] == 'GOOGL', 'ticker_symbol'] = 'GOOG'\nprint(\"Total records: {} | Unique tweet indexes: {}\".format(len(company_tweet), company_tweet['tweet_id'].nunique()))","be01107b":"tweet.info()","2f206f00":"tweet.isnull().sum()","442cb2dc":"tweet = tweet.dropna()\nprint(f\"Number of writers: {tweet['writer'].nunique()}\")","9df0b790":"print(\"Percent of duplicated tweets: {:.2f} %\".format(sum(tweet['body'].duplicated())\/len(tweet) * 100))","75a33476":"stats = tweet[['writer', 'tweet_id']].groupby('writer').agg(\"count\").rename(columns={'tweet_id' : 'tweet_count'})","bcf21e30":"sns.set_context(\"paper\", font_scale=2)\n\nplt.figure(figsize=(12, 8))\n\nsns.histplot(data=stats, x='tweet_count', bins=50, log_scale=True)\nplt.yscale('log')\nplt.title(\"Posts count histogram\")\nplt.xlabel(\"Number of users\")\nplt.ylabel(\"Tweet count\")\nNone","41030bf2":"sentiment_nltk = SentimentIntensityAnalyzer()\n\n# Mild cleaning: remove weblinks, $ticker_symbol, # symbol from hashtags, remove excessive spaces\ntweet['prep_body'] = tweet['body'].replace(r\"https?:\\S+|http?:\\S+|www?:\\S+\", '', regex=True).replace(r\"[@#\\$][a-zA-Z]+\", '', regex=True).replace(r\"\\s\\s+\", ' ', regex=True).str.strip()\n\ntweet['positive_sentiment'] = tweet['prep_body'].apply(lambda x: sentiment_nltk.polarity_scores(x)['pos'])\ntweet['negative_sentiment'] = tweet['prep_body'].apply(lambda x: sentiment_nltk.polarity_scores(x)['neg'])\ntweet['total_sentiment'] = tweet['prep_body'].apply(lambda x: sentiment_nltk.polarity_scores(x)['compound'])\n\ntweet.head()","f7d8379a":"print(\"Percent of cleaned duplicated tweets: {:.2f} %\".format(sum(tweet['prep_body'].duplicated())\/len(tweet) * 100))","f488263e":"tweet['date'] = tweet['datetime'].dt.date\n\nprices = yf.download(tickers=\" \".join([st for st in company.keys() if st != \"GOOGL\"]),\n    start=tweet['date'].min().strftime('%Y-%m-%d'),\n    end=tweet['date'].max().strftime('%Y-%m-%d'),\n    interval='1d'\n).reset_index()\n\nprices = prices.drop([\"Adj Close\", \"Volume\", \"Open\", \"High\", \"Low\"], axis=1)\n\nprices.head()","fa413ce0":"stats = tweet[['date', 'positive_sentiment', 'negative_sentiment', 'total_sentiment']].groupby('date').mean()\n\nprices['date'] = prices['Date'].dt.date\nprices = prices.drop(['Date'], axis=1)\nstats = prices.join(stats, how='inner', on='date')","c582a21b":"price_cols = [('Close', ticker) for ticker in company.keys() if ticker != 'GOOGL']\nsentim_cols = ['positive_sentiment', 'negative_sentiment', 'total_sentiment']\nstats[price_cols + sentim_cols].corr().loc[sentim_cols, price_cols].style.background_gradient(cmap='coolwarm')","3a3a26d3":"colors = {\"AMZN\" : \"tab:red\", \n          \"GOOG\" : \"tab:blue\", \n          \"AAPL\" : \"tab:orange\", \n          \"MSFT\" : \"tab:purple\", \n          \"TSLA\" : \"tab:green\"}\n\nplt.figure(figsize=(15, 5))\n\nt = \"AMZN\"\n\nsns.lineplot(x=stats[\"date\"], y=stats[(\"Close\", t)]\/max(stats[(\"Close\", t)]), color=colors[t], label=company[t])\nsns.lineplot(x=stats[\"date\"], y=stats[\"positive_sentiment\"]\/max(stats[\"positive_sentiment\"]), color=\"tab:cyan\", label=\"Positive Sentiment\")\nplt.title(company[t])\nplt.xlabel(\"Date\")\nplt.ylabel(\"Arbitrary units\")\nNone","59888923":"plt.figure(figsize=(15, 5))\n\nt = \"GOOG\"\n\nsns.lineplot(x=stats[\"date\"], y=stats[(\"Close\", t)]\/max(stats[(\"Close\", t)]), color=colors[t], label=company[t])\nsns.lineplot(x=stats[\"date\"], y=stats[\"positive_sentiment\"]\/max(stats[\"positive_sentiment\"]), color=\"tab:cyan\", label=\"Positive Sentiment\")\nplt.title(company[t])\nplt.xlabel(\"Date\")\nplt.ylabel(\"Arbitrary units\")\nNone","c701dc3c":"plt.figure(figsize=(15, 5))\n\nt = \"AAPL\"\n\nsns.lineplot(x=stats[\"date\"], y=stats[(\"Close\", t)]\/max(stats[(\"Close\", t)]), color=colors[t], label=company[t])\nsns.lineplot(x=stats[\"date\"], y=stats[\"positive_sentiment\"]\/max(stats[\"positive_sentiment\"]), color=\"tab:cyan\", label=\"Positive Sentiment\")\nplt.title(company[t])\nplt.xlabel(\"Date\")\nplt.ylabel(\"Arbitrary units\")\nNone","b476b5d3":"plt.figure(figsize=(15, 5))\n\nt = \"MSFT\"\n\nsns.lineplot(x=stats[\"date\"], y=stats[(\"Close\", t)]\/max(stats[(\"Close\", t)]), color=colors[t], label=company[t])\nsns.lineplot(x=stats[\"date\"], y=stats[\"positive_sentiment\"]\/max(stats[\"positive_sentiment\"]), color=\"tab:cyan\", label=\"Positive Sentiment\")\nplt.title(company[t])\nplt.xlabel(\"Date\")\nplt.ylabel(\"Arbitrary units\")\nNone","b77b2b76":"plt.figure(figsize=(15, 5))\n\nt = \"TSLA\"\n\nsns.lineplot(x=stats[\"date\"], y=stats[(\"Close\", t)]\/max(stats[(\"Close\", t)]), color=colors[t], label=company[t])\nsns.lineplot(x=stats[\"date\"], y=stats[\"positive_sentiment\"]\/max(stats[\"positive_sentiment\"]), color=\"tab:cyan\", label=\"Positive Sentiment\")\nplt.title(company[t])\nplt.xlabel(\"Date\")\nplt.ylabel(\"Arbitrary units\")\nNone","a340108d":"tweet['hour'] = tweet['datetime'].dt.hour\ndata = tweet[['writer', 'hour', 'date', 'tweet_id']].groupby(['writer', 'hour', 'date']).count().reset_index().rename(columns={'tweet_id' : 'tweet_rate'})\n\ntweet = tweet.drop(['hour'], axis=1)\n\nindmax = data.groupby('writer').agg({'tweet_rate' : 'idxmax'})\nposters = data.iloc[indmax.tweet_rate].sort_values(by='tweet_rate').set_index('writer')\nposters = posters.drop(['hour', 'date'], axis=1).rename(columns={'tweet_rate' : 'max_tweet_rate'})\nposters","39915ca7":"hours = data[['writer', 'hour', 'tweet_rate']].groupby(['writer', 'hour']).mean().sort_values(by='tweet_rate')\nhours = hours.reset_index().pivot(index='writer', columns='hour', values='tweet_rate').fillna(0)\nhours.columns.name = None\nposters = posters.join(hours, how='outer')\nposters.sort_values(by='max_tweet_rate').head()","49146110":"sns.set_context(\"paper\", font_scale=1)\n\ncolumns = list(range(24))\n\nnrows = 2\nncols = 3\nfig, axs = plt.subplots(nrows, ncols, figsize=(17, 12))\n\nsample_writers = ['PeteStock11', 'JimAndrews518', 'computer_hware', 'larryne', 'MarleyJayBiz', 'politicalHEDGE']\n\nfor i, writer in enumerate(sample_writers):\n    c = i \/\/ 2\n    r = i - nrows * c\n    \n    posters.loc[writer, columns].plot(kind='bar', ax=axs[r, c])\n    axs[r, c].set_title(writer)\n    axs[r, c].set_ylabel(\"average tweet rate\")\n    axs[r, c].set_xlabel(\"hour\")\n    \nplt.show()\nNone","1986cb6d":"def in_qrange(ser, q):\n    return ser.between(*ser.quantile(q=q))\n\ntweet['timediff'] = tweet.sort_values('datetime', ascending=False).groupby(['writer']).datetime.diff(-1).dt.seconds.fillna(np.inf)","312c258b":"data = tweet.loc[tweet['timediff'].transform(in_qrange, q=[0, 0.75]), ['writer', 'timediff']].groupby('writer').agg(['mean']).rename(columns={'mean' : 'mean_diff_sec'})\ndata.columns = data.columns.droplevel()\n\ntweet = tweet.drop(['timediff'], axis=1)\n\nposters = posters.join(data, on='writer', how='left').fillna(max(data['mean_diff_sec']))\nposters.loc[posters['mean_diff_sec'] == 0, 'mean_diff_sec'] = max(data['mean_diff_sec'])\nposters.sort_values(by='mean_diff_sec').head()","9557d84d":"data = tweet.loc[tweet['prep_body'].duplicated(), ['writer', 'tweet_id']].groupby('writer').count().rename(columns={'tweet_id' : 'duplicate_posts'})\n\nposters = posters.join(tweet[['writer', 'tweet_id']].groupby('writer').count().rename(columns={'tweet_id' : 'total_posts'}), how='left')","53d3c202":"posters = posters.join(data, how='left').fillna(0)\nposters['duplicate_posts'] = posters['duplicate_posts']\/posters['total_posts']\nposters.head()","6c283d0a":"columns = list(range(24))\nbot_check = pd.DataFrame(index=posters.index)\n\nbot_check[\"max_tweet_rate\"] = (posters[\"max_tweet_rate\"] > 100).astype(np.int8)\nbot_check[\"mean_diff_sec\"] = (posters[\"mean_diff_sec\"] < 10).astype(np.int8)\nbot_check[\"abscence_hours\"] = ((posters[columns] == 0).astype(int).sum(axis=1) < 3).astype(np.int8)\nbot_check[\"all_duplicates\"] = (posters[\"duplicate_posts\"] == 1).astype(np.int8)\n\nbot_check.head()","5d3388ae":"print(\"max hourly tweet rate > 100 : {} writers\".format(sum(bot_check[\"max_tweet_rate\"])))","fd931c85":"print(\"mean time between tweets sec < 5 seconds : {} writers\".format(sum(bot_check[\"mean_diff_sec\"])))","ef0c1830":"print(\"less than 3 hours of not tweeting : {} writers\".format(sum(bot_check[\"abscence_hours\"])))","3e69cce3":"print(\"not a single original post : {} writers\".format(sum(bot_check[\"all_duplicates\"])))","4d38cafb":"bot_check[bot_check.sum(axis=1) > 1]","f7e37730":"bot_tweets = tweet.loc[tweet['writer'].isin(bot_check[bot_check.sum(axis=1) > 1].index), 'prep_body'].unique()\nbot_check['tweet_like_bot'] = bot_check.index.isin(tweet.loc[tweet['prep_body'].isin(bot_tweets), 'writer'].unique()).astype(np.int8)","c0d7a82d":"print(\"Percent of bots : {:.2f}%\".format(sum(bot_check.sum(axis=1) > 1)\/len(posters)*100))","4d8694f1":"bots = bot_check.loc[bot_check.sum(axis=1) > 1].index\ntweet['group'] = 'user'\ntweet.loc[tweet.writer.isin(bots), 'group'] = 'bot'","912cfc23":"stats = tweet[tweet['group']==\"bot\"][['date', 'positive_sentiment', 'negative_sentiment', 'total_sentiment']].groupby('date').mean()\n\nstats = prices.join(stats, how='inner', on='date')","184dcfcc":"price_cols = [('Close', ticker) for ticker in company.keys() if ticker != 'GOOGL']\nsentim_cols = ['positive_sentiment', 'negative_sentiment', 'total_sentiment']\nstats[price_cols + sentim_cols].corr().loc[sentim_cols, price_cols].style.background_gradient(cmap='coolwarm')","d9890b5d":"sns.set_context(\"paper\", font_scale=2)\n\nplt.figure(figsize=(15, 5))\n\nt = \"AMZN\"\n\nsns.lineplot(x=stats[\"date\"], y=stats[(\"Close\", t)]\/max(stats[(\"Close\", t)]), color=colors[t], label=company[t])\nsns.lineplot(x=stats[\"date\"], y=stats[\"positive_sentiment\"]\/max(stats[\"positive_sentiment\"]), color=\"tab:olive\", label=\"Bots, Positive Sentiment\")\nplt.title(company[t])\nplt.xlabel(\"Date\")\nplt.ylabel(\"Arbitrary units\")\nNone","941d24fb":"plt.figure(figsize=(15, 5))\n\nt = \"GOOG\"\n\nsns.lineplot(x=stats[\"date\"], y=stats[(\"Close\", t)]\/max(stats[(\"Close\", t)]), color=colors[t], label=company[t])\nsns.lineplot(x=stats[\"date\"], y=stats[\"positive_sentiment\"]\/max(stats[\"positive_sentiment\"]), color=\"tab:olive\", label=\"Bots, Positive Sentiment\")\nplt.title(company[t])\nplt.xlabel(\"Date\")\nplt.ylabel(\"Arbitrary units\")\nNone","0557dfba":"plt.figure(figsize=(15, 5))\n\nt = \"AAPL\"\n\nsns.lineplot(x=stats[\"date\"], y=stats[(\"Close\", t)]\/max(stats[(\"Close\", t)]), color=colors[t], label=company[t])\nsns.lineplot(x=stats[\"date\"], y=stats[\"positive_sentiment\"]\/max(stats[\"positive_sentiment\"]), color=\"tab:olive\", label=\"Bots, Positive Sentiment\")\nplt.title(company[t])\nplt.xlabel(\"Date\")\nplt.ylabel(\"Arbitrary units\")\nNone","ea593f9a":"plt.figure(figsize=(15, 5))\n\nt = \"MSFT\"\n\nsns.lineplot(x=stats[\"date\"], y=stats[(\"Close\", t)]\/max(stats[(\"Close\", t)]), color=colors[t], label=company[t])\nsns.lineplot(x=stats[\"date\"], y=stats[\"positive_sentiment\"]\/max(stats[\"positive_sentiment\"]), color=\"tab:olive\", label=\"Bots, Positive Sentiment\")\nplt.title(company[t])\nplt.xlabel(\"Date\")\nplt.ylabel(\"Arbitrary units\")\nNone","17193e96":"plt.figure(figsize=(15, 5))\n\nt = \"TSLA\"\n\nsns.lineplot(x=stats[\"date\"], y=stats[(\"Close\", t)]\/max(stats[(\"Close\", t)]), color=colors[t], label=company[t])\nsns.lineplot(x=stats[\"date\"], y=stats[\"positive_sentiment\"]\/max(stats[\"positive_sentiment\"]), color=\"tab:olive\", label=\"Bots, Positive Sentiment\")\nplt.title(company[t])\nplt.xlabel(\"Date\")\nplt.ylabel(\"Arbitrary units\")\nNone","0079cd0d":"## Summary\n\nBots combined seem to be astonishingly more impactful than average for Google, Microsoft and Amazon in terms of positive sentiment and total sentiment. For these companies total sentiment by bot tweets seems to be up to 20-25% more impactful than average. For Apple total bot sentiment exhibits 10% increase compared to average, still a formidable effect. The correlation is significantly reduced for Tesla, where it differs from the average by only 1%.\n\n<p> Interestingly, there is a significant difference between negative sentiment from bot tweets compared to overall negative sentiment in terms of correlation to stock prices. Average negative sentiment was positively correlated with stock prices, while negative bot sentiment was negatively correlated to stock prices.\n    \n**Please upvote this notebook if you found this short exploration usefull or interesting.**","c575b080":"<a id=\"3\"><\/a><center><h2 style='background-color:#1E90FF; border:0; color:#FFF5EE'>Feature Engineering<\/h2><\/center>","597233f2":"Recent [research](https:\/\/www.scs.cmu.edu\/news\/nearly-half-twitter-accounts-discussing-reopening-america-may-be-bots) by Carnegie Mellon University have proposed that bots may be responsible for up to 50% of tweets for particular topics. Could bots be trying to push certain narritives about particular stocks too and is it possible to find these bots? In this notebook I show my attempt to answer this question by finding bots based on a set of common-sense features and criterias. Then I compare how the Bots tweet compare to average in terms of sentiment correlation with stock price. Suspected bot tweets have significantly higher correlation to stock price than the average sentiment across all writers in the dataset.\n\n\n<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n    \n<center><h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background-color:#1E90FF; border:0; color:#FFF5EE' role=\"tab\" aria-controls=\"home\">Content<\/h2><\/center>\n\n1. [Data Exploration](#1)\n2. [Average Sentiment and Stock Price](#2)\n3. [Feature Engineering](#3)\n4. [\"If it tweets like a bot, it is a bot\"](#4)\n5. [How does Bot sentiment correlates with Stock Price?](#5)\n    \n    \nIf you are interested in playing with time series, check out my [dataset on electricity prices and demand](https:\/\/www.kaggle.com\/aramacus\/electricity-demand-in-victoria-australia) in Victoria (Australian state). And please upvote it if you like it.","e96b5942":"To get a better sense about writer activity, lets build a histogram for tweet numbers by writers.","24344fc3":"On the first count, there are only 25 writers with two ore more flags.","7cead7af":"In case of twitter, an idea of influencing stock price via sentiment manipulation can be implemented via bot networks. Below I try to identify at least some of the accounts that could be bots.<br>\nTo keep track of writers tweeting patters, lets introduce a \"posters\" table, which will be filled in as this exploration proceeds. The first feature that may tell something about the writer is what is their peak hourly tweet rate.","8b2fadf4":"### Plot sample writers average hourly tweet rate (while active)","cce460cc":"Less than 2% of tweet records have no poster credentials. Drop missing values and count uniqie writers.","4378a655":"## Summary\n\nThere seems to be a fair bit of correlation between stock prices and sentiment. In terms of total sentiment, which consists of both positive and negative tweets, the correlation is between 44 and 54%.\n<p>For all stocks considered, mean daily positive sentiment had a higher correlation to prices compared to total mean daily sentiment with ranges between 50 and 55%. In general, negative sentiment had less of a correlation to prices with a notable exception of Tesla, that had much less correlation.","6044f156":"Aggregate sentiment from tweet by date and matrch with prices by date.","7ef46440":"To further illustrate this correlation, below are the plots with normalized stock price and an overall positive sentiment as a function of date.","1bd63ecd":"<a id=\"5\"><\/a><center><h2 style='background-color:#1E90FF; border:0; color:#FFF5EE'>How does Bot sentiment correlates with Stock Price?<\/h2><\/center>","7a17af20":"### Fraction of non-original tweets","a189515c":"Some of the conclusion whenever a poster is a bot or not can be drawn based on the collected features. For example: \n\n* abnormal hourly tweet rate (\"max_tweet_rate\") \n* too short mean time between successive tweets (\"mean_diff_sec\")\n* lack of hours with no tweets (too few hour columns, \"0\" to \"23\", when tweet rate was 0)\n* all tweets are among the duplicates (\"duplicate_posts\" = 1.0)\n\nWith a crude criteria, such as abnormal endurance (\"max_tweet_rate\" of 100 or more) or extreme typing speed (\"mean_diff_sec\" of 5 or less) or no sleep abilty (no hour columns with tweet rate of 3), some bots can be found.","38a0ede8":"After pre-processing the proportion of duplicated tweets increased from 10.35% to 25.25%. This indicates templates in about 15% of tweets, which could indicate bots.<br>\n\nNext, lets get the stock prices using Yahoo Finance API. For the sake of simplicity, consider only Closing price.","6b56edf5":"It would be convenient to convert \"post_date\" to the datetime format right away.","bef8b24d":"Next, lets get a mean tweet rate by each poster during each hour, from 0am to 23pm.","77c2e06d":"Calculate sentiment-price correlation coefficient for closing prices","0dedbf5d":"First, lets use NLTK Sentiment Intensity Analyser to evaluate the sentiment for each tweet. Even though Sentiment Analyser does not compain when fed a raw text, lets help it a bit with a mild tweet text cleanup.","7f0144c2":"<a id=\"1\"><\/a><center><h2 style='background-color:#1E90FF; border:0; color:#FFF5EE'>Data Exploration<\/h2><\/center>","24760dc8":"Lets count missing values in tweet table.","0fd31c44":"<a id=\"2\"><\/a><center><h2 style='background-color:#1E90FF; border:0; color:#FFF5EE'>Average Sentiment and Stock Price<\/h2><\/center>","4217a7cf":"## Summary\n\nThere seems to be 21.41%, or 29996 bots of 140131 uniqie writers in the dataset. This number was obtained via a two stage process. Firstly, four features were calculated for each writer:\n\n* abnormal hourly tweet rate (\"max_tweet_rate\") \n* too short mean time between successive tweets (\"mean_diff_sec\")\n* lack of hours with no tweets (too few hour columns, \"0\" to \"23\", when tweet rate was 0)\n* all tweets are among the duplicates (\"duplicate_posts\" = 1.0)\n\nNext, writers with at least two flags were deemed to be bots. At this stage there were only 25 such writers. Next, all their tweets were found and stored in the table \"bot_tweets\". Then, an additional feature, that determines whenever a writer posted one of the tweets from \"bot_tweets\". Finally, with five features writers were tallied again and those with at least two flags were deemed to be bots.","48f95601":"Some of the tweets mention more than one company of interest:","b4d70018":"## Average time between subsequent tweets\nToo short time between succesive tweets can indicate a machine authorship. To account for long abscence, such as vocations, limit to shortest 75% of time intervals (in seconds). For writers with only one tweet, assign the maximum value to the time between tweets \"mean_diff_sec\".","8321a3d0":"<a id=\"4\"><\/a><center><h2 style='background-color:#1E90FF; border:0; color:#FFF5EE'>\"If it tweets like a bot, it is a bot\"<\/h2><\/center>\n\nBootstraping: find all tweets from writers with at least a two flag in \"bot_check\". All other writers from \"bot_check\" that tweeted one of such tweets get a flag for \"tweet_like_a_bot\". Re-count writres with at least two flags.","68eab0ff":"Lets calculate the number of duplicates among all the tweets.","b3ee44ca":"It is interesting to look again on duplicates in prepared tweets, after hashtags and weblinks were dropped."}}