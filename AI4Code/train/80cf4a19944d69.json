{"cell_type":{"a7af68ed":"code","16ebb9f7":"code","cca63889":"code","c825b5ad":"code","f7a6d44d":"code","53d34a3c":"code","f6623640":"code","0f72ee4f":"code","e6d549f7":"code","5320fc2d":"code","987e708b":"code","d31d5cc1":"code","62a7bee2":"code","92142063":"code","a2032a80":"code","e3af5191":"code","abc5adfa":"code","2a35e723":"code","fd266a61":"code","db4418aa":"code","f5d04175":"code","b7d44aa5":"code","845add6d":"code","1727268e":"code","f1307e25":"code","eb7c6afb":"code","d5d96c9e":"code","79953771":"code","7cb1b620":"code","601aad67":"code","85bb94bc":"code","c7c2fa07":"code","da2615f7":"code","05947858":"code","94474cac":"code","09c7651c":"code","66686597":"code","1c2cb9a8":"code","68909eff":"code","8dd2d666":"code","c4de9a4a":"code","1c222d3f":"code","082501e9":"code","d909167e":"code","6ee3dbf0":"code","1f3f62f6":"code","370e13fd":"code","26a35282":"code","7760b968":"code","205cc703":"code","5883a21f":"code","4b79d145":"code","ea1d997e":"code","05acec15":"code","2b9bf55d":"code","15960cd0":"code","dc11d367":"code","d5fefedc":"code","6d16e3cd":"code","234d1614":"code","1dd1b1d4":"code","0ff895c7":"code","79b86f74":"code","5da34e4d":"code","b277a010":"code","89bbfe1f":"code","cd0d72b7":"code","665c5e24":"markdown","6746ff89":"markdown","84cb4c25":"markdown","1be8c9da":"markdown","28e60539":"markdown","04bb17a7":"markdown","d29e5c70":"markdown","4205024c":"markdown","463adeb9":"markdown","c199dd7e":"markdown","61531438":"markdown","a7ed4b28":"markdown","152180e4":"markdown","c1b9930c":"markdown","62657fe5":"markdown","4172d1a9":"markdown","df1cb660":"markdown","fddb51b3":"markdown","f249e6b2":"markdown","4ea4dcd7":"markdown","4c350a23":"markdown","fa9c2159":"markdown","40768258":"markdown","0e4a5da9":"markdown","29fc0047":"markdown","660df89d":"markdown","ae0798ba":"markdown","5a4b542d":"markdown","2b99902c":"markdown","c42e7d3c":"markdown"},"source":{"a7af68ed":"# Import libraries and set desired options\nimport os\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import hstack\n# !pip install eli5\nimport eli5\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display_html","16ebb9f7":"! pwd","cca63889":"PATH_TO_DATA = '\/kaggle\/input\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2\/'\nSEED = 17","c825b5ad":"def prepare_sparse_features(path_to_train, path_to_test, path_to_site_dict,\n                           vectorizer_params):\n    times = ['time%s' % i for i in range(1, 11)]\n    train_df = pd.read_csv(path_to_train,\n                       index_col='session_id', parse_dates=times)\n    test_df = pd.read_csv(path_to_test,\n                      index_col='session_id', parse_dates=times)\n\n    # Sort the data by time\n    train_df = train_df.sort_values(by='time1')\n    \n    # read site -> id mapping provided by competition organizers \n    with open(path_to_site_dict, 'rb') as f:\n        site2id = pickle.load(f)\n    # create an inverse id _> site mapping\n    id2site = {v:k for (k, v) in site2id.items()}\n    # we treat site with id 0 as \"unknown\"\n    id2site[0] = 'unknown'\n    \n    # Transform data into format which can be fed into TfidfVectorizer\n    # This time we prefer to represent sessions with site names, not site ids. \n    # It's less efficient but thus it'll be more convenient to interpret model weights.\n    sites = ['site%s' % i for i in range(1, 11)]\n    train_sessions = train_df[sites].fillna(0).astype('int').apply(lambda row: \n                                                     ' '.join([id2site[i] for i in row]), axis=1).tolist()\n    test_sessions = test_df[sites].fillna(0).astype('int').apply(lambda row: \n                                                     ' '.join([id2site[i] for i in row]), axis=1).tolist()\n    # we'll tell TfidfVectorizer that we'd like to split data by whitespaces only \n    # so that it doesn't split by dots (we wouldn't like to have 'mail.google.com' \n    # to be split into 'mail', 'google' and 'com')\n    vectorizer = TfidfVectorizer(**vectorizer_params)\n    X_train = vectorizer.fit_transform(train_sessions)\n    X_test = vectorizer.transform(test_sessions)\n    y_train = train_df['target'].astype('int').values\n    \n    # we'll need site visit times for further feature engineering\n    train_times, test_times = train_df[times], test_df[times]\n    \n    return X_train, X_test, y_train, vectorizer, train_times, test_times","f7a6d44d":"%%time\nX_train_sites, X_test_sites, y_train, vectorizer, train_times, test_times = prepare_sparse_features(\n    path_to_train=os.path.join(PATH_TO_DATA, 'train_sessions.csv'),\n    path_to_test=os.path.join(PATH_TO_DATA, 'test_sessions.csv'),\n    path_to_site_dict=os.path.join(PATH_TO_DATA, 'site_dic.pkl'),\n    vectorizer_params={'ngram_range': (1, 5), \n                       'max_features': 50000,\n                       'tokenizer': lambda s: s.split()}\n)","53d34a3c":"print(X_train_sites.shape, X_test_sites.shape)","f6623640":"vectorizer.get_feature_names()[:10]","0f72ee4f":"vectorizer.get_feature_names()[10000:10010]","e6d549f7":"? TimeSeriesSplit()","5320fc2d":"time_split = TimeSeriesSplit(n_splits=10)","987e708b":"for train_index, test_index in time_split.split(X_train_sites):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)","d31d5cc1":"logit = LogisticRegression(C=1, random_state=SEED, solver='liblinear')","62a7bee2":"%%time\n\ncv_scores1 = cross_val_score(logit, X_train_sites, y_train, cv=time_split, \n                            scoring='roc_auc', n_jobs=4) # hangs with n_jobs > 1, and locally this runs much faster","92142063":"cv_scores1, cv_scores1.mean()","a2032a80":"logit.fit(X_train_sites, y_train)","e3af5191":"eli5.show_weights(estimator=logit, \n                  feature_names=vectorizer.get_feature_names(), top=30)","abc5adfa":"# A helper function for writing predictions to a file\ndef write_to_submission_file(predicted_labels, out_file,\n                             target='target', index_label=\"session_id\"):\n    predicted_df = pd.DataFrame(predicted_labels,\n                                index = np.arange(1, predicted_labels.shape[0] + 1),\n                                columns=[target])\n    predicted_df.to_csv(out_file, index_label=index_label)","2a35e723":"WRITE_PATH = '\/kaggle\/working\/'","fd266a61":"logit_test_pred = logit.predict_proba(X_test_sites)[:, 1]\nwrite_to_submission_file(logit_test_pred, WRITE_PATH+'subm1.csv') # 0.91807","db4418aa":"# \u041c\u043e\u0436\u043d\u043e \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0442\u044c \u043e\u0442\u0432\u0435\u0442\u044b \u043f\u0440\u044f\u043c\u043e \u0432 \u043a\u043e\u043d\u043a\u0443\u0440\u0441, \u043d\u043e \u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043d\u0443\u0436\u043d\u043e \u0441\u043a\u0430\u0447\u0430\u0442\u044c \u0444\u0430\u0439\u043b kaggle.json \u0438\u0437 \u0441\u0432\u043e\u0435\u0433\u043e \u043f\u0440\u043e\u0444\u0438\u043b\u044f.\n# \u0414\u043b\u044f \u043f\u0440\u043e\u0441\u0442\u043e\u0442\u044b \u0431\u0443\u0434\u0435\u043c \u0434\u0435\u043b\u0430\u0442\u044c \u044d\u0442\u043e\u0442 \u0448\u0430\u0433 \u0432\u0440\u0443\u0447\u043d\u0443\u044e - \u0441\u043a\u0430\u0447\u0438\u0432\u0430\u044f \u0444\u0430\u0439\u043b\u044b \u0441 \u043e\u0442\u0432\u0435\u0442\u0430\u043c\u0438 \u0438 \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u044f \u0438\u0445 \u043d\u0430 \u0441\u0430\u0439\u0442 \u043a\u043e\u043d\u043a\u0443\u0440\u0441\u0430.\n\n# We will not use the code below\n# ! pip install kaggle","f5d04175":"# ! kaggle competitions submit -c catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2 -f \/kaggle\/working\/subm1.csv -m \"subm1\"","b7d44aa5":"def train_and_predict(model, X_train, y_train, X_test, site_feature_names=vectorizer.get_feature_names(), \n                      new_feature_names=None, cv=time_split, scoring='roc_auc',\n                      top_n_features_to_show=30, submission_file_name='submission.csv'):\n    \n    \n    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, \n                            scoring=scoring, n_jobs=4)\n    print('CV scores', cv_scores)\n    print('CV mean: {}, CV std: {}'.format(cv_scores.mean(), cv_scores.std()))\n    model.fit(X_train, y_train)\n    \n    if new_feature_names:\n        all_feature_names = site_feature_names + new_feature_names \n    else: \n        all_feature_names = site_feature_names\n    \n    display_html(eli5.show_weights(estimator=model, \n                  feature_names=all_feature_names, top=top_n_features_to_show))\n    \n    if new_feature_names:\n        print('New feature weights:')\n    \n        print(pd.DataFrame({'feature': new_feature_names, \n                        'coef': model.coef_.flatten()[-len(new_feature_names):]}))\n    \n    test_pred = model.predict_proba(X_test)[:, 1]\n    write_to_submission_file(test_pred, submission_file_name) \n    \n    return cv_scores","845add6d":"cv_scores1 = train_and_predict(model=logit, X_train=X_train_sites, y_train=y_train, \n                  X_test=X_test_sites, site_feature_names=vectorizer.get_feature_names(),              \n                  cv=time_split, submission_file_name='subm1.csv')","1727268e":"session_start_hour = train_times['time1'].apply(lambda ts: ts.hour).values","f1307e25":"sns.countplot(session_start_hour);","eb7c6afb":"plt.subplots(1, 2, figsize = (12, 6)) \n\nplt.subplot(1, 2, 1)\nsns.countplot(session_start_hour[y_train == 1])\nplt.title(\"Alice\")\nplt.xlabel('Session start hour')\n          \nplt.subplot(1, 2, 2)\nsns.countplot(session_start_hour[y_train == 0])\nplt.title('Others')\nplt.xlabel('Session start hour');","d5d96c9e":"morning = ((session_start_hour >= 7) & (session_start_hour <= 11)).astype('int')\nday = ((session_start_hour >= 12) & (session_start_hour <= 18)).astype('int')\nevening = ((session_start_hour >= 19) & (session_start_hour <= 23)).astype('int')\nnight = ((session_start_hour >= 0) & (session_start_hour <= 6)).astype('int')","79953771":"pd.DataFrame(night).describe()\n\n# \u0417\u0434\u0435\u0441\u044c \u043e\u0442\u0432\u0435\u0442 \u043d\u0430 \u0432\u043e\u043f\u0440\u043e\u0441 - \u043f\u043e\u0447\u0435\u043c\u0443 \u0432 \u0441\u043b\u0435\u0434. \u044f\u0447\u0435\u0439\u043a\u0435 \u043d\u0435\u0442 \u0441\u0442\u0440\u043e\u043a\u0438 \u0441 \u043d\u043e\u0447\u044c\u044e. \u0414\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e, \u043f\u043e\u0447\u0435\u043c\u0443?","7cb1b620":"pd.crosstab([morning, day, evening, night], y_train, rownames=['morning', 'day', 'evening', 'night'])","601aad67":"def add_time_features(times, X_sparse, add_hour=True):\n    hour = times['time1'].apply(lambda ts: ts.hour)\n    morning = ((hour >= 7) & (hour <= 11)).astype('int').values.reshape(-1, 1)\n    day = ((hour >= 12) & (hour <= 18)).astype('int').values.reshape(-1, 1)\n    evening = ((hour >= 19) & (hour <= 23)).astype('int').values.reshape(-1, 1)\n#    night = ((hour >= 0) & (hour <=6)).astype('int').values.reshape(-1, 1)\n    \n    objects_to_hstack = [X_sparse, morning, day, evening] #, night]\n    feature_names = ['morning', 'day', 'evening'] #, 'night']\n    \n    if add_hour:\n        # we'll do it right and scale hour dividing by 24\n        objects_to_hstack.append(hour.values.reshape(-1, 1) \/ 24)\n        feature_names.append('hour')\n        \n    X = hstack(objects_to_hstack)\n    return X, feature_names","85bb94bc":"%%time\nX_train_with_times1, new_feat_names = add_time_features(train_times, X_train_sites)\nX_test_with_times1, _ = add_time_features(test_times, X_test_sites)","c7c2fa07":"X_train_with_times1.shape, X_test_with_times1.shape","da2615f7":"cv_scores2 = train_and_predict(model=logit, X_train=X_train_with_times1, y_train=y_train, \n                               X_test=X_test_with_times1, \n                               site_feature_names=vectorizer.get_feature_names(),\n                               new_feature_names=new_feat_names,\n                               cv=time_split, submission_file_name='subm2.csv')","05947858":"cv_scores2 > cv_scores1","94474cac":"%%time\nX_train_with_times2, new_feat_names = add_time_features(train_times, X_train_sites, add_hour=False)\nX_test_with_times2, _ = add_time_features(test_times, X_test_sites, add_hour=False)\n\n\ncv_scores3 = train_and_predict(model=logit, X_train=X_train_with_times2, y_train=y_train, \n                               X_test=X_test_with_times2, \n                               site_feature_names=vectorizer.get_feature_names(),\n                               new_feature_names=new_feat_names,\n                               cv=time_split, submission_file_name='subm3.csv')","09c7651c":"cv_scores3 > cv_scores1","66686597":"cv_scores3 > cv_scores2","1c2cb9a8":"def add_session_duration_incorrect(times, X_sparse):\n    new_feat = (times.max(axis=1) - times.min(axis=1)).astype('timedelta64[ms]').astype(int)\n    return hstack([X_sparse, new_feat.values.reshape(-1, 1)])","68909eff":"X_train_with_time_incorrect = add_session_duration_incorrect(train_times, X_train_with_times2)\nX_test_with_time_incorrect = add_session_duration_incorrect(test_times, X_test_with_times2)","8dd2d666":"cv_scores4 = train_and_predict(model=logit, X_train=X_train_with_time_incorrect, y_train=y_train, \n                               X_test=X_test_with_time_incorrect, \n                               site_feature_names=vectorizer.get_feature_names(),\n                               new_feature_names=new_feat_names + ['sess_duration'],\n                               cv=time_split, submission_file_name='subm4.csv')","c4de9a4a":"train_durations = (train_times.max(axis=1) - train_times.min(axis=1)).astype('timedelta64[ms]').astype(int)\ntest_durations = (test_times.max(axis=1) - test_times.min(axis=1)).astype('timedelta64[ms]').astype(int)\n\nscaler = StandardScaler()\ntrain_dur_scaled = scaler.fit_transform(train_durations.values.reshape(-1, 1))\ntest_dur_scaled = scaler.transform(test_durations.values.reshape(-1, 1))","1c222d3f":"X_train_with_time_correct = hstack([X_train_with_times2, train_dur_scaled])\nX_test_with_time_correct = hstack([X_test_with_times2, test_dur_scaled])","082501e9":"cv_scores5 = train_and_predict(model=logit, X_train=X_train_with_time_correct, y_train=y_train, \n                               X_test=X_test_with_time_correct, \n                               site_feature_names=vectorizer.get_feature_names(),\n                               new_feature_names=new_feat_names + ['sess_duration'],\n                               cv=time_split, submission_file_name='subm5.csv')","d909167e":"cv_scores5 > cv_scores3","6ee3dbf0":"# \u0421\u0434\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u044b\u0439 \u0442\u0440\u0435\u043d\u0434\n\npd.DataFrame(train_times['time1'].apply(lambda t: ((12 * (t.year-2013) + t.month))\/16).values).plot()","1f3f62f6":"def add_day_month(times, X_sparse):\n    \n    day_of_week = times['time1'].apply(lambda t: t.weekday()).values.reshape(-1, 1)\n    \n#    month = times['time1'].apply(lambda t: t.month).values.reshape(-1, 1) - \u0443\u0431\u0435\u0440\u0435\u043c, \u0447\u0442\u043e\u0431\u044b \n#                                                                        \u0441 \u0442\u0440\u0435\u043d\u0434\u043e\u043c \u043d\u0435 \u043c\u0435\u0448\u0430\u043b\u0430\u0441\u044c\n\n    # linear trend\n    \n    year_month = times['time1'].apply(lambda t: ((12 * (t.year-2013) + t.month))\/16).values.reshape(-1, 1)\n    \n    objects_to_hstack = [X_sparse, day_of_week \/ 6, year_month] # month \/ 12, \n    \n    feature_names = ['day_of_week', 'year_month'] # 'month', \n        \n    X = hstack(objects_to_hstack)\n    \n    return X, feature_names","370e13fd":"X_train_final, more_feat_names = add_day_month(train_times, X_train_with_time_correct)\nX_test_final, _ = add_day_month(test_times, X_test_with_time_correct)","26a35282":"cv_scores6 = train_and_predict(model=logit, X_train=X_train_final, y_train=y_train, \n                               X_test=X_test_final, \n                               site_feature_names=vectorizer.get_feature_names(),\n                               new_feature_names=new_feat_names + ['sess_duration'] + more_feat_names,\n                               cv=time_split, submission_file_name='subm6.csv')","7760b968":"hour = train_times['time1'].apply(lambda ts: ts.hour)\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nenc = OneHotEncoder(categories='auto')\n\nhour_enc = enc.fit_transform(hour.values.reshape(-1,1))\n\nhour_list = list(enc.get_feature_names())","205cc703":"hour_list = ['h'+str(i) for i in range(7,24)]\nhour_list","5883a21f":"hour.head()","4b79d145":"hour_enc.toarray()","ea1d997e":"hour.values.reshape(-1,1)","05acec15":"X_train_hour = hstack([X_train_final, hour_enc])\n\nhour_feat_names = hour_list","2b9bf55d":"hour2 = test_times['time1'].apply(lambda ts: ts.hour)\n\nhour_enc2 = enc.transform(hour2.values.reshape(-1,1))","15960cd0":"X_test_hour = hstack([X_test_final, hour_enc2])","dc11d367":"new_feature_names_4 = new_feat_names + ['sess_duration'] + more_feat_names + hour_feat_names","d5fefedc":"logit = LogisticRegression(C=3, random_state=SEED, solver='liblinear')\n\n# C \u0432\u0440\u043e\u0434\u0435 \u043b\u0443\u0447\u0448\u0435 3 \u043f\u043e CV \u043f\u043e\u0437\u0436\u0435...","6d16e3cd":"cv_scores8 = train_and_predict(model=logit, X_train=X_train_hour, y_train=y_train, \n                               X_test=X_test_hour, \n                               site_feature_names=vectorizer.get_feature_names(),\n                               new_feature_names=new_feature_names_4,\n                               cv=time_split, submission_file_name='\/kaggle\/working\/subm8.csv')","234d1614":"c_values = np.logspace(-2, 2, 20)\nc_values","1dd1b1d4":"# \u0414\u043b\u044f \u0431\u044b\u0441\u0442\u0440\u043e\u0442\u044b \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0432\u043e\u043a\u0440\u0443\u0433 \u043e\u043f\u0442\u0438\u043c\u0443\u043c\u0430...\nc_values = [2.5, 3, 3.5, 4]","0ff895c7":"# here we've already narrowed down c_values to such a range.\n# typically, you would start with a wider range of values to check\n\n\nlogit_grid_searcher = GridSearchCV(estimator=logit, param_grid={'C': c_values},\n                                  scoring='roc_auc', n_jobs=4, cv=time_split, verbose=1)","79b86f74":"%%time\nlogit_grid_searcher.fit(X_train_final, y_train); ","5da34e4d":"logit_grid_searcher.best_score_, logit_grid_searcher.best_params_","b277a010":"final_model = logit_grid_searcher.best_estimator_","89bbfe1f":"cv_scores7 = train_and_predict(model=final_model, X_train=X_train_final, y_train=y_train, \n                               X_test=X_test_final, \n                               site_feature_names=vectorizer.get_feature_names(),\n                               new_feature_names=new_feat_names + ['sess_duration'] + more_feat_names,\n                               cv=time_split, submission_file_name='subm7.csv')","cd0d72b7":"cv_scores7 > cv_scores6","665c5e24":"**These are basic features - sequences of visited web-sites.**","6746ff89":"**What's the reason of such a deterioration?**\nObvious! The new feature is session duration in milliseconds, it's maximal value is very high (check it). We need to either scale a new feature or, alternatively, measure it in some different units. You can check that actually, measuring it in seconds (rather than milliseconds will do). But instead we'll perform feature scaling, it's a more universal technique to apply for numeric features which can take high values.","84cb4c25":"**Now we'll add a new feature: session duration. But beware: first we'll do it in an incorrect way, then we'll correct ourselves.**","1be8c9da":"**We'll be performing time series cross-validation, see [the previous kernel](https:\/\/www.kaggle.com\/kashnitsky\/correct-time-aware-cross-validation-scheme) for an explanation.**","28e60539":"<center>\n<img src=\"https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg\" \/>\n<\/center> \n     \n## <center>  [mlcourse.ai](https:\/\/mlcourse.ai) \u2013 Open Machine Learning Course \n\n#### <center> Author: [Yury Kashnitskiy](https:\/\/yorko.github.io) (@yorko) \n\n## <center> A disciplined approach to cross-validation in a Kaggle (Inclass) competition \n\n\nHere we'll show how to progress with feature engineering in the [Alice](https:\/\/www.kaggle.com\/c\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) competition, we'll simulate some mistakes that can be done, show how to interpret model weights with [eli5](https:\/\/github.com\/TeamHG-Memex\/eli5), and discuss our validation scheme even further. Prerequisites: [Alice - logistic regression baseline](https:\/\/www.kaggle.com\/kashnitsky\/alice-logistic-regression-baseline) and [Correct time-aware cross-validation scheme](https:\/\/www.kaggle.com\/kashnitsky\/correct-time-aware-cross-validation-scheme).\n \n \nPlan:\n - [Submission 1: \"Bag of sites\" baseline](#Submission-1:-\"Bag-of-sites\"-baseline)\n - [Submission 2: Coming up with time features via EDA](#Submission-2:-Coming-up-with-time-features-via-EDA)\n - [Submission 3: Example of overfitting](#Submission-3:-Example-of-overfitting)\n - [Submissions 4 and 5: The importance of feature scaling](#Submissions-4-and-5:-The-importance-of-feature-scaling)\n - [Submission 6: Adding more time features](#Submission-6:-Adding-more-time-features)\n - [Submission-7: Tuning-params](#Submission-7:-Tuning-params)\n - [Analyzing submission history](#Analyzing-submission-history)\n - [Conclusions](#Conclusions)","04bb17a7":"**Making a new submission, we notice a leaderboard score improvement as well (0.91807 ->  0.93089). Looks good as compared to sites only. But we might be a bit wary that feature `hour` gets such a big weight. Let's try the same features but without `hour`.**","d29e5c70":"# Add hour like David suggested:","4205024c":"We've got an intuition: different people might prefer to visit (even the same) sites at *different* times of the day. Let's first draw the distribution of all session start hours. ","463adeb9":"**Perform time series cross-validation with logistic regression.**","c199dd7e":"**We are going to repeat these steps several more times. It's a nice practice to substitute repeated chunks of code with function calls. So let's write a function which performs cross-validation, model training, displaying feature importance, making predictions for a test set and forming a submission file.**","61531438":"**Now cross-validation is much more stable. 3rd CV results are better for each and every fold as compared to the first ones. And actually the situation is better than with the `hour` feature for 5 folds out of 10, but in such case we'll prefer a model with less variation of CV results, i.e. the last one.**","a7ed4b28":"Now we definitely see that Alice mostly prefers 4-5 pm for browsing. So let's create features 'morning', 'day' and 'evening' and 'night'. Separators between these times of the day will be almost arbitrary: 0 am, 7 am, 12 am, and 7 pm. However, you can tune this.","152180e4":"**Display model weights with eli5**","c1b9930c":"## Submission 6: Adding more time features\n\nIn a real competition it's very important to keep track of Public Kernels and borrow some ideas form them.  For instance, we can explore [this Kernel](https:\/\/www.kaggle.com\/hakeydotcom\/additional-time-features-and-logit) and decide to add to features: day of week and month. In this case it's easy to come up to such features on your own, but still typically there's a lot to find out in Kernels.  ","62657fe5":"## Submission 1: \"Bag of sites\" baseline\n\n**We'll start with basic site features, applying the \"bag of words\" approach. Here we read training and test sets, sort train set by session start time.**","4172d1a9":"**We see an improvement almost for all folds as compared to the results of previous cross-validation.**","df1cb660":"## Submission 3: Example of overfitting","fddb51b3":"**New model predicts better on 9 folds out of 10 as compared to the model with sites and time features. Submitting yields 0.94630 Public LB ROC AUC.**","f249e6b2":"**Let's create a separate function to add new features (this will keep the code cleaner). To demonstrate possible overfitting, we'll keep a flag - whether to add an `hour` feature or not.**","4ea4dcd7":"Making a new submission we get... Wow! **0.94535** Public LB score instead of **0.93089** when adding the `hour` feature. We were right, the `hour` feature leads to overfitting, and it's better not to add it. ","4c350a23":"**Train logistic regression with all training data**","fa9c2159":"**Now we see which sites are descriptive of Alice. At the same time we notice that Alice doesn't use Gmail and Google Plus. Let's make predictions for test set and form a submission file.**","40768258":"**Performing time series cross-validation, we see an improvement in ROC AUC.**","0e4a5da9":"## Submissions 4 and 5: The importance of feature scaling","29fc0047":"Here tuning params helps only for 6 folds out of 10. Typically in such situation you'll make one more submission to compare LB scores as well. 0.94954 - it's less than without hyperparameter tuning. **Bad news in the end:** our CV scheme is not perfect. Try  to improve it! (*hint:* is all training set needed for a good prediction?). ","660df89d":"Now the same separately for Alice and everybody else.","ae0798ba":"## Submission 2: Coming up with time features via EDA\nHere we'll build just a few plots to motivate feature engineering. While competing, you'll need to create much more plots and build more features based on your observations. Take a look at [Kernels](https:\/\/www.kaggle.com\/c\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2\/kernels?sortBy=voteCount&group=everyone&pageSize=20&competitionId=7173) for this competition, eg. at [this \"Initial EDA\"](https:\/\/www.kaggle.com\/adityaecdrid\/initial-eda). You can do much more with sites, but let's switch to site visit times.","5a4b542d":"**By running the above function, we get sparse train and test matrices (`X_train_sites`, `X_test_sites`), vector of train targets `y_train` (0's and 1's - whether a session belongs to Alice or not), an instance of `TfidfVectorizer` (we'll need site name from it) and site visit times both for training and test sets, for further feature engineering. Go back to the function defined above, read comments, and experiment yourself to understand what we've done here.**","2b99902c":"## Submission 7: Tuning params\nWhen you're done with feature engineering (no more ideas) you can tune hyperparameters in your model. Typically, at this point you create nice code for your pipeline, and then tune various params for a long time. Here we used several params - `ngram_range`, `max_features`. Choosing between `CountVectorizer` and `TfIdfVectorizer` might also be treated as a hyperparameter. But now we'll tune only regularization strength `C`.","c42e7d3c":"# subm8 \u0414\u0430\u043b \u0431\u0443\u0441\u0442 \u0434\u043e 0.95391! \u041d\u043e \u043f\u043e\u0442\u043e\u043c \u044f \u0447\u0442\u043e-\u0442\u043e \u0441\u0434\u0435\u043b\u0430\u043b \u0441 \u0447\u0430\u0441\u0430\u043c\u0438 \u0438 \u043f\u043e\u043b\u0443\u0447\u0438\u043b \u0431\u043e\u043b\u044c\u0448\u0435 0.96...\n# \u041f\u043e\u0439\u043c\u0438\u0442\u0435, \u043a\u0430\u043a \u043c\u043d\u0435 \u044d\u0442\u043e \u0443\u0434\u0430\u043b\u043e\u0441\u044c?"}}