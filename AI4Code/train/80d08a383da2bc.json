{"cell_type":{"c1f7ecb0":"code","bf9b1a27":"code","1a17c71c":"code","cc6714b6":"code","d4a8fa2e":"code","4c8fb757":"code","6267026d":"code","9a942f5b":"code","f8c3f183":"code","ed3c03a8":"code","96c8ed68":"code","8d512f90":"code","c2fa6131":"code","f7d8e2ee":"code","b044c691":"code","29e1a90c":"code","74b13a85":"code","773f80c4":"code","8ddd30ef":"code","d40cb9e7":"code","c360724f":"code","f93ae54c":"code","29bc6ea2":"code","2a38b22e":"code","ec0e870c":"code","4bea5c8b":"code","c9846898":"code","1c280e72":"code","4b37e0f4":"code","b8a2c127":"code","a29c6a77":"code","409608ab":"code","efd9d965":"code","cad12fc4":"code","e3ae5585":"code","6248b337":"code","ed2efd90":"code","66fe1357":"code","8ff2118e":"code","d87487e8":"code","f011b4b0":"code","5d42465b":"code","53f80a93":"code","57c157c9":"code","904dd08f":"code","dd25e90c":"code","e7bb73b8":"code","551f2347":"code","9d7f5740":"code","333d1374":"code","38700675":"code","3fdc5c59":"code","e7fb675d":"code","a71226f6":"markdown","073685d8":"markdown","88331d3f":"markdown","a0de8efc":"markdown","e28098e7":"markdown","22f3acd1":"markdown","1fc124dd":"markdown"},"source":{"c1f7ecb0":"!pip install neurodsp\n!pip install git+https:\/\/github.com\/forrestbao\/pyeeg.git\n!pip install fooof","bf9b1a27":"from lightgbm import LGBMClassifier\nfrom sklearn.model_selection import KFold,train_test_split\nfrom skopt import gp_minimize\nimport lightgbm as lgb\nfrom skopt.space import *\nfrom sklearn.preprocessing import StandardScaler\nimport gc\nfrom fooof import FOOOF\nimport scipy.stats as ss\nimport pyeeg\nfrom sklearn.metrics import f1_score,roc_auc_score\nfrom skopt.plots import *\nimport numpy as np # l\nimport pandas as pd \nfrom neurodsp.spectral import compute_spectrum\nfrom neurodsp.timefrequency import amp_by_time, freq_by_time, phase_by_time\nimport os\nimport pickle\n\n%matplotlib inline","1a17c71c":"eeg_fs = 250 \neeg_ch = [\"C3\", \"Cz\", \"C4\"] \neog_ch = [\"EOG_ch01\", \"EOG_ch02\", \"EOG_ch03\"] \nall_ch = eeg_ch + eog_ch\nevent_types = {0:\"left\", 1:\"right\"}","cc6714b6":"PATH=\"\/kaggle\/input\/ucsd-neural-data-challenge\/\"","d4a8fa2e":"train_df = pickle.load(open(PATH+\"data\/epoched_train.pkl\", \"rb\"))\ntest_df = pickle.load(open(PATH+\"data\/epoched_test.pkl\", \"rb\"))","4c8fb757":"train_df.columns=['patient_id', 'start_time', 'event_type', 'C3', 'Cz', 'C4', 'EOG_ch01',\n       'EOG_ch02', 'EOG_ch03' ]\ntest_df.columns=['patient_id', 'start_time', 'C3', 'Cz', 'C4', 'EOG_ch01',\n       'EOG_ch02', 'EOG_ch03']","6267026d":"train_df","9a942f5b":"y_train=train_df['event_type']\ny_train","f8c3f183":"X_train=train_df.drop(['event_type','patient_id','start_time'],axis=1,inplace=False)\nX_test=test_df.drop(['patient_id','start_time'],axis=1,inplace=False)","ed3c03a8":"def moving_average(x, w):\n    return np.convolve(x, np.ones(w), 'valid') \/ w","96c8ed68":"def sample_plot_psd(df,n_samples=6):\n    if n_samples % 2 == 1:\n        raise ValueError(\"number not even\")\n    fig,axs=plt.subplots(1,#C3,C4,Cz\n                        n_samples,figsize=(20,5),dpi= 200)\n    fig.tight_layout()\n    plt.subplots_adjust(wspace=0.1)\n    random_samples_idxs=np.random.choice(X_train.shape[0],n_samples)\n    for n,idx in enumerate(random_samples_idxs):\n        label=df.loc[idx,'event_type']\n        ax_title=\"left\" if label == 0 else \"right\"\n        axs[n].set_title(ax_title)\n        axs[n].plot(moving_average(df.loc[idx]['C3'], w=300),color='red',label=\"C3\")\n        axs[n].plot(moving_average(df.loc[idx]['C4'], w=300),color='blue',label=\"C4\")\n        axs[n].plot(moving_average(df.loc[idx]['Cz'], w=300 ),color='green',label=\"Cz\")\n        axs[n].legend()\n        \n    plt.legend()\n    plt.show()","8d512f90":"sample_plot_psd(train_df)","c2fa6131":"def sample_plot_psd_spectrum(df,n_samples=6):\n    fig,axs=plt.subplots(1,n_samples,figsize=(20,5),dpi= 200)\n    fig.tight_layout()\n    plt.subplots_adjust(wspace=0.1)\n    random_samples_idxs=np.random.choice(X_train.shape[0],n_samples)\n    for n,idx in enumerate(random_samples_idxs):\n        label=df.loc[idx,'event_type']\n        ax_title=\"left\" if label == 0 else \"right\"\n        axs[n].set_title(ax_title)\n        axs[n].plot(compute_spectrum(df.loc[idx]['C3'], eeg_fs, method='welch', avg_type='mean', nperseg=eeg_fs*2)[1],color='green',label=\"C3\")\n        axs[n].legend()\n        \n    plt.legend()\n    plt.show()","f7d8e2ee":"sample_plot_psd_spectrum(train_df)","b044c691":"sig = train_df['C3'][0]\nalpha_range = (7, 12)","29e1a90c":"amp = amp_by_time(sig, eeg_fs, alpha_range)\nplt.figure(figsize=(20,4))\nplt.plot(sig, label=\"EEG\")\nplt.plot(amp, label=\"amp over time\")\nplt.legend()\nplt.title(\"Alpha power amplitude over time\")\nplt.show()","74b13a85":"phase = phase_by_time(sig, eeg_fs, alpha_range)\nplt.figure(figsize=(20,4))\nplt.plot(sig, label=\"EEG\")\nplt.plot(phase, label=\"phase over time\")\nplt.legend()\nplt.title(\"Instantaneous phase over time\")\nplt.show()","773f80c4":"i_f = freq_by_time(sig, eeg_fs, alpha_range)\nplt.figure(figsize=(20,4))\nplt.plot(sig, label=\"EEG\")\nplt.plot(i_f, label=\"freq over time\")\nplt.legend()\nplt.title(\"Instantaneous frequency over time\")\nplt.show()","8ddd30ef":"from IPython.display import YouTubeVideo\nYouTubeVideo('4lmPKQDTWs4', width=400, height=250)","d40cb9e7":"YouTubeVideo('rvdR4MRVaYk', width=400, height=250)","c360724f":"binning=[0.5, 4, 7, 12, 30]\nintervals=[\"05_4\",\"4_7\",\"7_12\",\"12_30\"]","f93ae54c":"def extract_power_ratio(data,binning=[0.5, 4, 7, 12, 30],eeg_fs=250):\n    global intervals\n    result = pd.DataFrame(index=range(data.shape[0]),dtype=np.float64)\n    for idx,row in data.iterrows():\n        for ch in all_ch:\n            _ , p_ratios=pyeeg.bin_power(row[ch],binning,eeg_fs)\n            p_ratios=np.array(p_ratios)\n            for n,ratio in enumerate(p_ratios):\n                c_name=str(ch+\"_\"+intervals[n])\n                result.loc[idx,c_name]=ratio\n    return result","29bc6ea2":"def extract_C3_C4_diff(data):\n    global intervals\n    result = pd.DataFrame(index=range(data.shape[0]),dtype=np.float64)\n    for idx,row in data.iterrows():\n        for interval in intervals:\n            result.loc[idx,str(\"C3_C4_\" + interval + \"_difference\")]=data.loc[idx,'C3_' + interval ] - data.loc[idx,'C4_' + interval ]\n            result.loc[idx,str(\"Cz_C3_\" + interval + \"_difference\")]= data.loc[idx,'Cz_' + interval ] - data.loc[idx,'C3_' + interval ]\n            result.loc[idx,str(\"Cz_C4_\" + interval + \"_difference\")]= data.loc[idx,'Cz_' + interval ] - data.loc[idx,'C4_' + interval ]\n    return result","2a38b22e":"def extract_band_ratios(data):\n    result = pd.DataFrame(index=range(data.shape[0]),dtype=np.float64)\n    delta_suffix=\"_05_4\"\n    theta_suffix=\"_4_7\"\n    alpha_suffix=\"_7_12\"\n    beta_suffix=\"_12_30\"\n    for idx,row in data.iterrows():\n        for ch in all_ch:\n            \n            #i know i could've iterated trough the permutations but they were few anyway\n            \n            delta_alpha_ratio=data.loc[idx,str(ch+delta_suffix)]\/data.loc[idx,str(ch+alpha_suffix)]\n            theta_beta_ratio=data.loc[idx,str(ch+theta_suffix)]\/data.loc[idx,str(ch+beta_suffix)]\n            delta_beta_ratio=data.loc[idx,str(ch+delta_suffix)]\/data.loc[idx,str(ch+beta_suffix)]\n            theta_alpha_ratio=data.loc[idx,str(ch+theta_suffix)]\/data.loc[idx,str(ch+alpha_suffix)]\n            beta_alpha_ratio=data.loc[idx,str(ch+beta_suffix)]\/data.loc[idx,str(ch+alpha_suffix)]\n            theta_delta_ratio=data.loc[idx,str(ch+theta_suffix)]\/data.loc[idx,str(ch+delta_suffix)]\n            \n            \n            result.loc[idx,str(ch+\"_delta_alpha_ratio\")] = delta_alpha_ratio\n            result.loc[idx,str(ch+\"_theta_beta_ratio\")] = theta_beta_ratio\n            result.loc[idx,str(ch+\"_delta_alpha_ratio\")] = delta_alpha_ratio\n            result.loc[idx,str(ch+\"_delta_beta_ratio\")] = delta_beta_ratio\n            result.loc[idx,str(ch+\"_beta_alpha_ratio\")] = beta_alpha_ratio\n            result.loc[idx,str(ch+\"_theta_delta_ratio\")] = theta_delta_ratio\n    return result","ec0e870c":"def first_order_diff(X):\n    r=[]\n    for i in range(len(X)-2):\n        r.append(X[i+1]-X[i])\n    return np.array(r)","4bea5c8b":"def extract_first_second_diff_avg_max(data):\n    global intervals\n    result = pd.DataFrame(index=range(data.shape[0]),dtype=np.float64)\n    for idx,row in data.iterrows():\n        for ch in all_ch:\n            fod_ch=first_order_diff(row[ch])\n            result.loc[idx, str( ch + \"_1st_order_diff_max\")] = np.amax(fod_ch)\n            result.loc[idx, str( ch + \"_1st_order_diff_avg\")] = np.mean(fod_ch)\n            \n            result.loc[idx, str( ch + \"_2nd_order_diff_max\")] = np.amax(first_order_diff(fod_ch))\n            result.loc[idx, str( ch + \"_2ndorder_diff_avg\")] = np.mean(first_order_diff(fod_ch))\n    return result","c9846898":"def extract_hjorth(data):\n    result = pd.DataFrame(index=range(data.shape[0]),dtype=np.float64)\n    for idx,row in data.iterrows():\n        for ch in all_ch:\n            hj1,hj2=pyeeg.hjorth(row[ch])\n            result.loc[idx,str(ch + \"_hjorth_mobility\")] = hj1\n            result.loc[idx,str(ch + \"_hjorth_complexity\")] = hj2\n            \n            #fod_hj1,fod_hj2=pyeeg.hjorth(row[ch],list(first_order_diff(row[ch])))\n            #result.loc[idx,str(ch +  \"_1st_ord_diff_hjorth_mobility\")] = fod_hj1\n            #result.loc[idx,str(ch +\"_1st_ord_diff_hjorth_complexity\")] = fod_hj2\n    return result","1c280e72":"def extract_detrended_fluctuation_analysis(data):\n    result = pd.DataFrame(index=range(data.shape[0]),dtype=np.float64)\n    for idx,row in data.iterrows():\n        for ch in all_ch:\n            result.loc[idx,str(ch+\"_dfa\")]=pyeeg.dfa(row[ch])\n    return result","4b37e0f4":"def extract_stat_features(data):\n    e=pd.DataFrame(index=range(data.shape[0]),dtype=np.float64)\n    for idx,row in data.iterrows():\n        for feature in data.columns:\n            e.loc[idx,f'{feature}_mean']=ss.tmean(row[feature])\n            e.loc[idx,f'{feature}_std'] =ss.tstd(row[feature])\n            e.loc[idx,f'{feature}_kurt']=ss.kurtosis(row[feature])\n            e.loc[idx,f'{feature}_skew']=ss.skew(row[feature])\n            e.loc[idx,f'{feature}_min'] =ss.tmin(row[feature])\n            e.loc[idx,f'{feature}_var'] =ss.tvar(row[feature])\n            e.loc[idx,f'{feature}_median']=ss.median_absolute_deviation (row[feature]) #e.loc[idx,f'{feature}_sem'] =ss.tsem(row[feature])            \n    return e","b8a2c127":"def extract_neurodsp_stuff(data,f_ranges=[(4,7),(7,12)]):\n    global intervals\n    result = pd.DataFrame(index=range(data.shape[0]),dtype=np.float64)\n    for idx,row in data.iterrows():\n        for ch in all_ch:\n            for i,frange in enumerate(f_ranges):\n                amp = amp_by_time(row[ch],eeg_fs,frange)\n                i_f = freq_by_time(row[ch], eeg_fs, frange)\n                \n                result.loc[idx,str(ch+\"_\"+intervals[i]+\"-\"+\"amp_time_median\")]=np.nanmedian(amp)\n                result.loc[idx,str(ch+\"_\"+intervals[i]+\"-\"+\"amp_time_max\")]=np.nanmax(amp)\n                result.loc[idx,str(ch+\"_\"+intervals[i]+\"-\"+\"amp_time_min\")]=np.nanmin(amp)\n                result.loc[idx,str(ch+\"_\"+intervals[i]+\"-\"+\"amp_time_kurt\")]=ss.kurtosis(amp,nan_policy='omit')\n                \n                \n                result.loc[idx,str(ch+\"_\"+intervals[i]+\"-\"+\"freq_time_median\")]=np.nanmedian(i_f)\n                result.loc[idx,str(ch+\"_\"+intervals[i]+\"-\"+\"freq_time_max\")]=np.nanmax(i_f)\n                result.loc[idx,str(ch+\"_\"+intervals[i]+\"-\"+\"freq_time_min\")]=np.nanmin(i_f)\n                result.loc[idx,str(ch+\"_\"+intervals[i]+\"-\"+\"freq_time_kurt\")]=ss.kurtosis(i_f,nan_policy='omit')\n\n    return result","a29c6a77":"def extract_fooof(data,freq_range = [1, 40] ):\n    result = pd.DataFrame(index=range(data.shape[0]),dtype=np.float64)\n    alpha_range=(7, 12)\n    fm = FOOOF(peak_width_limits=[1, 8], max_n_peaks=6, min_peak_height=0.4)\n    num_with_alpha = 0\n    num_without_alpha = 0\n    for idx,row in data.iterrows():\n            for ch in all_ch:\n                \n                freq, psd = compute_spectrum(data.loc[idx]['C3'], eeg_fs, method='welch',\n                                avg_type='mean', nperseg=eeg_fs*2)\n                fm.add_data(freq, psd, freq_range)\n                fm.fit()\n                peak_params = fm.peak_params_\n                \n                peak_params_alpha = []\n                for param in peak_params: \n                    if (param[0] > alpha_range[0]) and (param[0] < alpha_range[1]): \n                        peak_params_alpha.append(param)\n                means = []\n                if len(peak_params_alpha) > 0: \n                    num_with_alpha += 1\n                    means = np.mean(peak_params_alpha, axis=0)\n                else :\n                    num_without_alpha += 1\n                    means = [0, 0, 0]\n\n                result.loc[idx,ch + \"_alpha_central_freq\"]=means[0]\n                result.loc[idx, ch + \"_alpha_power\"]=means[1]\n                result.loc[idx,ch + \"_alpha_band_width\"]=means[2]\n    return result","409608ab":"#%%time\n#p_ratios=extract_power_ratio(X_train)\n#X_train=(p_ratios.join(extract_stat_features(X_train))\n#         .join(extract_hjorth(X_train))\n#         .join(extract_neurodsp_stuff(X_train))\n#         .join(extract_detrended_fluctuation_analysis(X_train))\n#         .join(extract_first_second_diff_avg_max(X_train))\n#         .join(extract_C3_C4_diff(p_ratios))\n#         .join(extract_band_ratios(p_ratios))\n#         .join(extract_fooof(X_train))\n#        )\n#X_train","efd9d965":"#p_ratios=extract_power_ratio(X_test)\n#X_test=(p_ratios.join(extract_stat_features(X_test))\n#         .join(extract_hjorth(X_test))\n#         .join(extract_neurodsp_stuff(X_test))\n#         .join(extract_detrended_fluctuation_analysis(X_test))\n#         .join(extract_first_second_diff_avg_max(X_test))\n#         .join(extract_C3_C4_diff(p_ratios))\n#         .join(extract_band_ratios(p_ratios))\n#        .join(extract_fooof(X_test))\n#\n#)\n#X_test","cad12fc4":"#X_train.to_csv(\"train.csv\")\n#X_test.to_csv(\"test.csv\")\n#del X_train,X_test","e3ae5585":"X_train=pd.read_csv(\"..\/input\/eegucsddata\/train.csv\")\nX_test=pd.read_csv(\"..\/input\/eegucsddata\/test.csv\")","6248b337":"X_train=X_train.iloc[:,1:]\nX_test=X_test.iloc[:,1:]","ed2efd90":"scaler=StandardScaler()\nscaler.fit(X_train)\nprev=X_train.columns\nX_train=pd.DataFrame(scaler.transform(X_train),columns=prev)\nX_test=scaler.transform(X_test)","66fe1357":"X_trn,X_val,y_trn,y_val=train_test_split(X_train,y_train,train_size=0.75,shuffle=True,random_state=23)","8ff2118e":"y_trn=y_trn.values.astype(np.int8)\ny_val=y_val.values.astype(np.int8)","d87487e8":"space  = [Integer(50,1000, name='max_depth'),\n          Integer(200,3000, name='num_leaves'),\n          Integer(5, 300, name='min_child_samples'),\n          Real(1, 400,  name='scale_pos_weight'),\n          Real(0.6,1, name='subsample'),\n          Real(0.3, 0.9, name='colsample_bytree'),\n          Integer(40, 500, name='max_bin'),\n          Real(0.01, 20,  name='reg_alpha'),\n          Real(0.01, 20,  name='reg_lambda'),\n         ]","f011b4b0":"gc.collect()","5d42465b":"def lgb_f1_score(y_hat, data):\n    y_true = data.get_label()\n    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n    return 'f1', f1_score(y_true, y_hat), True\n","53f80a93":"def objective(hyp):\n    params={\n        'max_depth':hyp[0],\n        'num_leaves':hyp[1],\n        'min_child_samples' : hyp[2],\n        'scale_pos_weight' :  hyp[3],\n        'subsample':hyp[4],\n        'colsample_bytree' :hyp[5],\n        'learning_rate':0.01,\n        'objective':'binary',\n        'metrics':'auc',\n        'max_bin':hyp[6],\n        'nthread': 8,\n        'boosting_type': 'gbdt',\n        'min_child_weight': 0,\n        'min_split_gain': 0,\n        'subsample_freq': 1,\n        'reg_alpha':hyp[7],\n        'reg_lambda':hyp[8]\n    }\n    features_list=list(X_trn.columns)\n    dtrain=lgb.Dataset(X_trn.values,label=y_trn,feature_name=features_list)\n    dval=lgb.Dataset(X_val.values,label=y_val,feature_name=features_list)\n    \n    early_stopping=70\n    num_boost_round=1500\n    \n    model_lgb=lgb.train(params,dtrain,valid_sets=(dtrain,dval),\n                         valid_names=['train','valid'],num_boost_round=num_boost_round,\n                                early_stopping_rounds=early_stopping,\n                               verbose_eval=1, feval=lgb_f1_score)\n    \n    y_hat=model_lgb.predict(X_val[model_lgb.feature_name()])\n    auc =-roc_auc_score(y_val,y_hat)\n    gc.collect()\n    print(\"AUC : \",auc,\"\\n\")\n    return auc","57c157c9":"res_gp = gp_minimize(objective, space, n_calls=20,\n                     random_state=23,n_initial_points=4)","904dd08f":"\"Best score=%.4f\" % res_gp.fun","dd25e90c":"res_gp.x","e7bb73b8":"hyp=res_gp.x\n\n\n \nparams={\n    'max_depth':hyp[0],\n    'num_leaves':hyp[1],\n    'min_child_samples' : hyp[2],\n    'scale_pos_weight' :  hyp[3],\n    'subsample':hyp[4],\n    'colsample_bytree' :hyp[5],\n    'objective':'binary',\n    'metrics':'auc',\n    'max_bin':hyp[6],\n    'nthread': 8,\n    'learning_rate':0.1,\n    'boosting_type': 'gbdt',\n    'min_child_weight': 0,\n    'min_split_gain': 0,\n    'subsample_freq': 1,\n    'reg_alpha':hyp[7],\n    'reg_lambda':hyp[8]\n}\n\nsplits=5\n\npreds=np.zeros(X_test.shape[0])\nhistory=[]\n\nkfold=KFold(splits,shuffle=True,random_state=23)\n\nfor n_fold,(trn_idx,val_idx) in enumerate(kfold.split(train_df)):\n    print(f\"Fold #{n_fold}:\\n\")\n    X_trn,y_trn=X_train.loc[trn_idx,:],y_train[trn_idx]\n    X_val,y_val=X_train.loc[val_idx,:],y_train[val_idx]\n    \n    features_list=list(X_trn.columns)\n    dtrain=lgb.Dataset(X_trn.values,label=y_trn.astype(np.int8),feature_name=features_list)\n    dval=lgb.Dataset(X_val.values,label=y_val.astype(np.int8),feature_name=features_list)\n    \n    early_stopping=300\n    num_boost_round=3000\n    \n    evals_result={}\n    model_lgb=lgb.train(params,dtrain,valid_sets=(dtrain,dval),\n                        evals_result=evals_result, valid_names=['train','valid'],num_boost_round=num_boost_round,\n                               verbose_eval=1, feval=lgb_f1_score)\n    history.append(evals_result)\n    y_fold_preds=model_lgb.predict(X_test,num_iteration=model_lgb.best_iteration)\n    preds+=np.asarray(y_fold_preds)","551f2347":"lgb.plot_metric(history[model_lgb.best_iteration])","9d7f5740":"lgb.plot_importance(model_lgb,height=1,figsize=(50,50),dpi=300)","333d1374":"preds \/= splits","38700675":"preds","3fdc5c59":"sub=pd.DataFrame({'trial_id' : range(test_df.shape[0])})\nsub['event_type']=np.round(preds).astype(int)\n#sub.to_csv('submission.csv' ,index=False)","e7fb675d":"sub","a71226f6":"# Bayesian Optimiation on LightGBM","073685d8":"# Feature Extraction","88331d3f":"of course since this is my first time working with EEGs i was heavily dependent on the course material on this one :C","a0de8efc":"i implemented this feature extraction methods after following these videos:","e28098e7":"# Data Visualization","22f3acd1":"# Training and Predict","1fc124dd":"* Name: Alberto Zorzetto\n* Email: alberto.zorzetto1@gmail.com\n* UCSD PID: NA (13th grade , high school)"}}