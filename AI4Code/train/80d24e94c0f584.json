{"cell_type":{"8edc5ba3":"code","e8b9bfdd":"code","62bac660":"code","a45eb8ed":"code","5e3c46cc":"code","c42f9b90":"code","bae778ca":"code","9d3da5b4":"code","9db23861":"code","340bc9fa":"code","ecbd53ef":"code","1364d253":"code","19922cf5":"code","6adc65be":"code","34f97735":"code","64e60973":"code","e18fb0a2":"code","c3442563":"code","0b9845cd":"code","664ea0eb":"code","14ab8fa0":"code","d0c14178":"code","9cd72911":"code","ca8ceddf":"code","9bfe200a":"code","d5f1ad7c":"markdown","a69732e9":"markdown","18ae6c0f":"markdown","02f0f426":"markdown","0944a21d":"markdown","a0f5d84c":"markdown","a25c96f1":"markdown","3b378977":"markdown","e527ad03":"markdown","705a7330":"markdown","391fffba":"markdown","5f89a0b4":"markdown","2f0b3396":"markdown","f3e04716":"markdown","bec459d2":"markdown","52058d4c":"markdown","d92ecf8a":"markdown","96ee729f":"markdown","c6a1f794":"markdown","5008419f":"markdown","4aca85eb":"markdown","cf83f785":"markdown","e2d3b278":"markdown","bec8859d":"markdown","bae7362e":"markdown","ce21ffd3":"markdown","0c15958c":"markdown","94efb087":"markdown","446b6478":"markdown","8184f42b":"markdown","64f1fa04":"markdown","74a9bc6c":"markdown","d1a0b734":"markdown","ede6d7ed":"markdown"},"source":{"8edc5ba3":"import sqlite3\nimport pandas as pd\n\nconn = sqlite3.connect(\"\/kaggle\/input\/algoritma-data-analysis\/chinook.db\")\n\nalbums = pd.read_sql_query(\"SELECT * FROM albums\", conn)\nalbums.head()","e8b9bfdd":"import sqlite3\nimport pandas as pd\n\nconn = sqlite3.connect(\"kaggle\/input\/algoritma-data-analysis\/chinook.db\")\n\nalbums = pd.read_sql_query(\"SELECT * FROM employees\", conn)\nalbums.head()","62bac660":"## Your code below\n\n\n## -- Solution code","a45eb8ed":"pd.read_sql_query(\"SELECT * FROM artists LIMIT 5\", \n                  conn, \n                  index_col='ArtistId')","5e3c46cc":"albums = pd.read_sql_query(\"SELECT AlbumId, Title, a.Name \\\n                           FROM albums \\\n                           LEFT JOIN artists as a \\\n                           ON a.ArtistId = albums.ArtistId\", conn)\nalbums.head()","c42f9b90":"pd.read_sql_query(\"SELECT * FROM albums\", conn).head()","bae778ca":"## Your code below\n\n\n## -- Solution code","9d3da5b4":"top_cust = pd.read_sql_query(\"SELECT CustomerId, SUM(Total)  as TotalValue, \\\n                              COUNT(InvoiceId) as Purchases \\\n                              FROM INVOICES \\\n                              GROUP BY CustomerId \\\n                              ORDER BY TotalValue DESC \\\n                              LIMIT 5\", con=conn, index_col='CustomerId')\ntop_cust","9db23861":"## Your code below\n\n","340bc9fa":"germany = pd.read_sql_query(\"SELECT * FROM invoices WHERE BillingCountry = 'Germany'\", conn)\ngermany.head()","ecbd53ef":"not_germany = pd.read_sql_query(\"SELECT * FROM invoices WHERE BillingCountry IS NOT 'Germany'\", conn)\nnot_germany.head()","1364d253":"america = pd.read_sql_query(\"SELECT * FROM invoices WHERE BillingCountry IN ('USA', 'Canada')\", conn)\namerica.head()","19922cf5":"## Your code below\n\n\n## -- Solution code","6adc65be":"germany.dtypes","34f97735":"invoices_table = pd.read_sql_query(\"SELECT sql FROM sqlite_master \\\n                                    WHERE name = 'invoices'\", conn)\nprint(invoices_table.loc[0,:].values[0])","64e60973":"germany_2012 = pd.read_sql_query(\"SELECT * FROM invoices \\\n                                  WHERE InvoiceDate >= '2012-01-01' AND InvoiceDate <= '2012-12-31'\",\n                                 con=conn, parse_dates='InvoiceDate')\ngermany_2012['InvoiceDate'].describe()","e18fb0a2":"# Your code below\n","c3442563":"germany = pd.read_sql_query(\"SELECT * FROM invoices \\\n                             WHERE BillingCountry = 'Germany' \\\n                             AND BillingPostalCode LIKE '107%'\", conn)\ngermany.head()","0b9845cd":"customerinv = pd.read_sql_query(\"SELECT firstname, lastname, email, company, \\\n                                 invoiceid, invoicedate, billingcountry, total \\\n                                 FROM invoices \\\n                                 left join customers \\\n                                 on invoices.customerId = customers.customerId\", conn)\ncustomerinv.head()","664ea0eb":"## Your code below\n\n\n## -- Solution code","14ab8fa0":"customerinv = pd.read_sql_query(\"SELECT invoices.*  \\\n                                 FROM invoices \\\n                                 WHERE invoices.CustomerId IN ( \\\n                                 SELECT c.CustomerId FROM Customers as c \\\n                                 LEFT JOIN invoices as i on i.CustomerId = c.CustomerId \\\n                                 GROUP BY c.CustomerId \\\n                                 ORDER BY SUM(Total) DESC LIMIT 10)\", conn)\ncustomerinv.head()","d0c14178":"customerinv = pd.read_sql_query(\"SELECT *  \\\n                                 FROM invoices \\\n                                 WHERE InvoiceId IN (46, 175, 198)\", conn)\ncustomerinv.head()","9cd72911":"## Your code below\n\n\n## -- Solution code","ca8ceddf":"## Your code below\n\n","9bfe200a":"## Your code below\n\n\n## -- Solution code","d5f1ad7c":"The `pd.read_sql_query` is most commonly used with that two parameters above, but on its [official documentation](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.read_sql_query.html) is a list of other parameters we can use as well. \n\nIn the following cell, we use a similar SQL query with an additional `LIMIT` statement to limit the output to the first 5 records (rows). However, notice that we also set `index_col` so the specified column is recognized as the index:","a69732e9":"#### Knowledge Check\n\nWe'll create a `DataFrame`: this time select all columns from the `artists` table. Recall that when we use `pd.read_sql_query()` command we pass in the SQL query as a string, and add a connection as the second parameter. Save the output as a `DataFrame`.\n\nYour DataFrame should be constructed like this:\n\n`__ = pd.read_sql_query(\"SELECT __ FROM __ \", conn)`\n\nQuestion:\n1. How many rows are there in your DataFrame?","18ae6c0f":"#### Knowledge Check\n\nEdit the following code to find out the most popular `genres` from all invoice sales. Use different column to acquire the following information: Summation of `Total` sales, and number of tracks bought from the `Quantity` columns.\n\n```\ntop_genre = pd.read_sql_query(\"SELECT genres.GenreId, genres.Name, \\ \n                               __(InvoiceItem.Total), SUM(InvoiceItem.__) \\\n                               FROM tracks \\\n                               LEFT JOIN genres ON _____ \"\n                               conn,\n                               index_col='GenreId'\n)\n```\n\nQuestion:\n1. What are the top 5 genres that generated the most profit?","02f0f426":"In the above command, we asked for all columns of a table to be returned to us through the `SELECT *` command. Well, columns of which table? That would be `tables`. Together they form an SQL query:\n\n`SELECT * FROM albums`\n\nThe database we're working with have a few tables populated with sample data. The database has the following schema:\n![](assets\/chinookschema2.png)","0944a21d":"**Coursebook: SQL Query using pandas**\n- Part 4 of Data Analytics Specialization\n- Course Length: 9 hours\n- Last Updated: February 2020\n___\n\n- Author: [Samuel Chan](https:\/\/github.com\/onlyphantom)\n- Developed by [Algoritma](https:\/\/algorit.ma)'s product division and instructors team","a0f5d84c":"A `LIKE` operator is a can be used to match a certain part of the data and can be really useful if we need to perform a partial matching to a specific value rather than using an equal-to operator. The `107%` you see in the query means to extract value in `BillingPostalCode` that starts with the number 107. This is really helpful when you are aiming to extract data specifically on specific region. In Germany, you would know that Wilmersdorf and Tempelhof in Berlin has postal code starting with 107.\n\n**Discussion:**\n\nIf you were to put `%` on the end, you are matching everything that starts with 107, if you put it on the start `%107`, means you are matching everything that ends in 107. What do you think will came up if you use `%`  before and after the pattern match?","a25c96f1":"### Using `LIKE` Operator","3b378977":"## WHERE statements\n\nWe've seen how to use do some of the most common SQL operations this far. In particular, we have:\n\n- Learned how to write `SELECT` statements  \n- Use `index_col` in the `pd.read_sql_query()` method  \n- SQL Join operations\n- Use SQL Aliases\n\nIn the following example, we'll look at one more technique in the SQL arsenal: the `WHERE` clause\n\nA `WHERE` clause is followed by a **condition**. If we want to query for all invoices where country of the billing address is Germany, we can add a `WHERE` clause to our sql query string:","e527ad03":"# Working with SQL Databases\n\nThere are a great number of python modules that provide functionalities to work with databases of all variants and flavors. For a MySQL database, we may form a connection using `pymysql` or one of many other alternatives:\n\n```\nimport pymysql\nconn = pymysql.connect(\n    host=host,\n    port=port,\n    user=user,\n    password=password,\n    db=database)\n```\n\nWe can then use `pd.read_sql_query()`, passing in the connection:\n```\nsales = pd.read_sql_query(\"SELECT * FROM sales\", conn)\n```\n\nUnder the hood, `pandas` uses SQLAlchemy so any database supported by that library will work. This isn't something you need to worry about at this stage of your learning journey, but for the sake for practice, let's also see how a connection URI for a SQLite database looks like:","705a7330":"Notice that in the code above, we place a backslash (`\\`) character so we have line continuation and the newline will be ignored. This allows SQL to treat the entire query string as if they were essentially one line.","391fffba":"#### Dive Deeper:\n\nLet's take a look at another case, consider the following data frame:","5f89a0b4":"#### Knowledge Check\n\nImagine you're being instructed to analyze invoices that consist of a fair amount of tracks within one purchase. Previously, you have known that within one invoice the users would see a total of 1 to 10 tracks per purchase. Using subquery techniques you have learned, create an SQL query that fetched all invoice along with its total tracks quantity of bigger than 10. Complete the following query:\n\n```\nSELECT *, \n(SELECT ___ FROM invoice_items GROUP BY ___) as Quantity\nFROM invoices\nWHERE Quantity > 10\n```","2f0b3396":"Now there are other common approach using the `BETWEEN` operator, try and copy-paste the following code to a code cell:\n\n```\ngermany_2012 = pd.read_sql_query(\"SELECT * FROM invoices \\\n                                    WHERE InvoiceDate BETWEEN \\\n                                    '___' AND '___'\", \n                                  con=conn, parse_dates='InvoiceDate')\ngermany_2012['InvoiceDate'].describe()\n```\n\nTry completing the code above and see if the query fetch the same result as the previous one:","f3e04716":"## SQL Aggregation\nSince you have learned various aggregation tools in `pandas` such as `.crosstab()`, `.pivot_table()`, and `.groupby()`. It is also common practice to perform aggregation function using SQL. Consider the following query:","bec459d2":"### SQL Subquery","52058d4c":"Do note that the `IS` operator is the same as its mathematical notation counterpart: `=`. Similar with most programming language, it also supports other mathematical operator such as `>`, `>=`, `<`, and `<=`.\n\nNow let's try another approach by using `IN` operator that enables us to specify multiple values for comparison. For example we'd like to retrieve all invoices from `Canada` and `USA`:","d92ecf8a":"### Operating Dates\n\nContinuing from the last `WHERE` statements, we can retrieve all invoices billed to the country `Germany`. However, it is also common to perform a conditional query statement to retrieve a specific data range. Let's take a look at our `germany`data frame for example:","96ee729f":"Notice the subquery is defined as follow:\n\n```\nSELECT c.CustomerId FROM Customers as c \nLEFT JOIN invoices as i on i.CustomerId = c.CustomerId \nGROUP BY c.CustomerId \nORDER BY SUM(Total) DESC LIMIT 10\n```\n\nIf used on its own, the query will retrieve the top 10 customer's IDs based on the total summation of their purchases. This when then used as the condition using `IN` statement on the previous query. The `IN`, however, can also be used using a hard-coded value like the following:","c6a1f794":"`WHERE` conditions can be combined with `IS`, `AND`, `OR` and `NOT`. Supposed we want to create a DataFrame containing all invoices where the billing country is **not** Germany, we can do the following:","5008419f":"Notice how our `InvoiceDate` is listed as `Object` types. The `pd.read_sql_query` behaves like `pd.read_csv` where, by default, it reads data as numeric and object. This doesn't necessarily means the database is stored using string format (commonly known as `VARCHAR` in SQL databases). Take a look at the following table schema:","4aca85eb":"Create a `DataFrame` containing all columns from the `tracks` table; Additionally, it should also contain:\n    - The `Title` column from the `albums` table\n    - The `Name` column from the `artists` table\n    - The `Name` column from the `genres` table \n\n> **Hint 1**: In your `SELECT` statement, you can use `SELECT tracks.* FROM TRACKS` to select all columns from the `TRACKS` table\n> \n> **Hint 2**: When we write `SELECT tracks.Name as tracksName`, we are renaming the output column from `Name` to `tracksName` using a technique called column aliasing. You may optionally consider doing this for columns that share the same name across different tables \n\nSet the `TrackId`column to be the index. The resulting `DataFrame` should has 11 columns.\n\nGive your `DataFrame` a name: name it `tracks`. Perform EDA on `tracks` to answer the following question:\n\n1. Use `tail()` to inspect the last 5 rows of data. Which genre is present in the last 5 rows of our `tracks` DataFrame (Check all that apply)?\n    - [ ] Latin\n    - [ ] Classical\n    - [ ] Soundtrack\n    - [ ] Pop\n\n2. Apply `pd.crosstab(..., columns='count')`, `.value_counts()`, or any other techniques you've learned to compute the frequency table of Genres in your DataFrame. Which is among the top 3 most represented genres in the `tracks` DataFrame?\n    - [ ] Latin\n    - [ ] Classical\n    - [ ] Soundtrack\n    - [ ] Pop\n\n3. Use `groupby()` on Artist Name and compute the `mean()` on the `UnitPrice` of each tracks. You will realize that most artists price their tracks at 0.99 (`mean`) but there are several artists where the `mean()` is 1.99. Which of the Artist has a mean of 0.99 `UnitPrice`:\n    - [ ] The Office\n    - [ ] Aquaman\n    - [ ] Pearl Jam\n    - [ ] Lost","cf83f785":"Notice how the query fetched the top 5 customers from all time from the `invoices` table grouped by unique `CustomerId`. We performed two aggregation functions: `SUM()` and `COUNT()`, each aggregating different column from the `invoices` table. At the end, the `ORDER BY` statement is added to order the table based on the `TotalValue` column. Do note that the aggregated columns needs to be a numeric as the available aggregated functions are: `SUM`, `AVG`, `COUNT`, `MIN`, and `MAX`.","e2d3b278":"Using a `WHERE` statement in a query can be beneficial to pull relevant data for our analysis. A common operator for a WHERE statement is `LIKE`.\nConsider the following SQL Query:","bec8859d":"# Background\n\n## Top-Down Approach \n\nThe coursebook is part 4 of the **Data Analytics Specialization** offered by [Algoritma](https:\/\/algorit.ma). It takes a more accessible approach compared to Algoritma's core educational products, by getting participants to overcome the \"how\" barrier first, rather than a detailed breakdown of the \"why\". \n\nThis translates to an overall easier learning curve, one where the reader is prompted to write short snippets of code in frequent intervals, before being offered an explanation on the underlying theoretical frameworks. Instead of mastering the syntactic design of the Python programming language, then moving into data structures, and then the `pandas` library, and then the mathematical details in an imputation algorithm, and its code implementation; we would do the opposite: Implement the imputation, then a succinct explanation of why it works and applicational considerations (what to look out for, what are assumptions it made, when _not_ to use it etc).\n\n## Training Objectives\n\nThis coursebook is intended for participants who have completed the preceding courses offered in the **Data Analytics Developer Specialization**. This is the third course, **SQL and Data Visualization with Pandas**.\n\nThe coursebook focuses on:\n\n- Querying from SQL Databases\n- SQL Joins\n- SQL Conditional Statements\n- Flavors and Common Operators\n- End to end data analysis\n\nAt the end of this course is a Graded Asssignment section, where you are expected to apply all that you've learned on a new dataset, and attempt the given questions.","bae7362e":"Within the first 6 data, you could see the `Company` column may not be reliable since most of them are filled with None. But if you pay close attention to the `Email` column, you could see some people have an email domain at `apple`, that could be an indicator of their company.\n\n1. How would your conditional `WHERE` statement would be like if you want to count the number of customers that are working at Apple Inc.?\n\n    Try to complete the following codes:\n\n```\napplecust = pd.read_sql_query(\"SELECT firstname, lastname, email, company, \\\n                               invoiceid, invoicedate, billingcountry, total \\\n                               FROM invoices \\\n                               left join customers \\\n                               on invoices.___ = customers.___ \\\n                               WHERE email like '___'\", conn)\n\n)\n```","ce21ffd3":"Consider the database schema illustraation again and pay attention to two tables and their respective columns:\n\n1. `albums`: \n    - `AlbumId`, `Title`, `ArtistId`\n\n2. `artists`: `\n    - `ArtistId`, `Name`    \n\nWe want a `pandas` DataFrame containing the `AlbumId`, `Title` and `Name`. Notice that `Name` is from the `artists` table while the other columns are from the `albums` table. What is a reasonable strategy?\n\nThe most straightforward solution is the `LEFT JOIN`, let's see an example:","0c15958c":"In some cases, you'd like to fill in some value for the query from the database itself as one of the condition. For example, recall how we retrieved all customers that has the most total invoice in the previous exercise. Say from the information, we'd like to retrieve all the top customers invoice. To do that we will construct a `WHERE` statement using `IN` operator utilizing a subquery to retrieve all top customers:","94efb087":"## SQL Joins\n\n`JOIN` statements are used to combine records from two tables. We can have as many `JOIN` operations as we want in a SQL query.\n\nBelow is a diagram of the different types of SQL `JOIN` operations:\n\n![](assets\/sqljoins.png)\n\nCredit: Data & Object Factory, LLC\n\nIn most business scenarios though, a `LEFT JOIN` is almost always the type of `JOIN` you want - it is very direct (and therefore easy to reason about). Left join return all records in the left table regardless if any of those records have a match in the right table.\n\nThe `INNER JOIN` is also very intuitive and easily understood. This query return all of the records in the left table that has a matching record in the right table.\n\n> As a personal side note, I've worked at companies where `RIGHT JOIN` is outright forbidden in favor of `LEFT JOIN`: directness and ease-of-understanding aside, all right joins can be replaced by the opposite left join. \n> \n> The `OUTER JOIN` (also referred to as `FULL JOIN`) is also quite uncommon in practice. Performance reason aside, an outer join return all of the records from both tables regardless if there is a match or not, resulting in a DataFrame that has potentially a lot of `NULL` values.","446b6478":"Note that according to different DBMS you are using, there are different ways to retrieve a table's schema. The query above is used to retrieve table schema from SQLite database. The `DATETIME` type is stored with the following format: `YYYY-MM-DD HH:MI:SS`.\n\nIt is often useful to understand the table schema of your database so you can perform the appropriate operation. Within our `invoices`' schema you can see some of useful information such as:\n- `InvoiceId` is listed as a primary key\n- `InvoiceDate` is stored as `DATETIME`\n- `CustomerId` is registered as foreign key to `customers` table\n\nIf you are not provided with the database's schema, take some time to study each table schema. Now consider the following case: We are reviewing Germany market of the last year sales and would like to retrieve all invoices from the year `2012`.","8184f42b":"#### Knowledge Check\n\nConsider the database schema illustraation again and pay attention to two tables and their respective columns:\n\n1. `albums`: `AlbumId`, `Title`, `ArtistId`\n\n2. `tracks`: `TrackId`, `Name`, `AlbumId`, `GenreId`, ... `UnitPrice` \n    \n3. `genres`: `GenreId`, `Name`","64f1fa04":"#### Knowledge Check\n\nEdit the following code to include a `WHERE` clause. We want the returned DataFrame to contain only the `Pop` genre and only when the `UnitPrice` of the track is 0.99:\n\n```\npopmusic = pd.read_sql_query(\"SELECT tracks.*, genres.Name as GenreName \\\n                            FROM tracks \\\n                            LEFT JOIN genres ON _____ \\\n                            WHERE genres.Name = ____ AND _____, \n                           conn,\n                           index_col='TrackId'\n\n)\n```\n\nQuestion:\n1. How many rows are there in `popmusic`?","74a9bc6c":"Based on the data queried, how many of the customers is working at Apple Inc.?\n\n- [ ] 412  \n- [ ] 49   \n- [ ] 7  \n- [ ] 14  ","d1a0b734":"# Under and Over Fetching\n\nNow amidst all the tools selection available for data analysis, you will now ponder which one is better suited for you. To review let's recall what we have learned about Python features:\n- Reading flat files (csv \/ sas files)\n- Data cleansing and wrangling\n- Exploratory analysis tools\n- Visual exploratory tools\n\nNow where does SQL fits in? First you will need to understand client-server architecture. \n\n![](assets\/clientserver.png)\n\nA bit different with Python when all your data operation is done on your local computer. When you worked with SQL, most likely you have a relational database stored remotely from your machine, usually a centralized database accessible to some clients.\n\nWhen you perform a query, you execute a command to download the data to your computer. This downloading process require resources, and you need to effectively utilize the tools in order to minimize the overall cost.\n\n**Discussion:**\n\nYou are instructed to perform an analysis for all `Rock` genre sales on the last year (2012). Consider this questions:\n\n- Is it necessary for you to download all tracks table to your computer?\n- Will you filter the `Rock` genre tracks using SQL WHERE statements or conditional subsetting using Python?\n- Since we need the information of multiple tables, which one is more convenient, querying a joined table or two separate tables from the database?\n\nTry if you can construct your most optimum query below:","ede6d7ed":"# Learn-by-Building\n\nThe following learn-by-building exercise will guide you through the process of building out a simple analysis along with some accompanying charts. This module is considerably more difficult than similar exercise blocks in the past, but it sure is a lot more rewarding!\n\nLet's try by first constructing a DataFrame using the `read_sql_query()` method that we've grown familiar to. We want to develop a simple sales visualization report of our top 5 key markets (`Country` column in `customers`) ranked by Sales (`Total` column in `invoices`). \n\nWe also want to identify our top 5 customers by name (`FirstName`, `LastName`) in the report.\n\nLast but not least, we want the report to include a day-of-week analysis on sales performance, and for that we will need the `InvoiceDate` column. \n\n> **Hint 1**: `pandas` has built-in methods of extracting the name of day in a week. We've seen this in Part 2 of this specialization (**Working with Datetime chapter**). An example usage is:\n>\n> `x['InvoiceDOW'] = x['InvoiceDate'].dt.weekday_name`\n>\n>  **Hint 2**: In `read_sql_query`, you can use the `parse_dates='InvoiceDate'` argument to have the specified column parsed as date, saving you from a `to_datetime()` conversion"}}