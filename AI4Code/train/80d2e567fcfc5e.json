{"cell_type":{"937f7c7d":"code","681ff8e1":"code","70f1b6d1":"code","be7ab6eb":"code","60067f2c":"code","2be70595":"code","97eb16df":"code","63cd1647":"code","8fbfdc5c":"code","52d0e4a0":"code","5974fab9":"code","88b5685a":"code","6abc8071":"code","5c69ec18":"code","d31f8d0f":"code","90e1b66e":"code","ce86256e":"code","52edd04e":"code","d7446d68":"code","80608306":"code","50578341":"code","bb4e56f4":"code","53716bad":"markdown","151af9f1":"markdown","3891a384":"markdown","961c80d8":"markdown","a04cbb9d":"markdown","c6425ee8":"markdown","3f160b43":"markdown","9b66d5d0":"markdown","3870db90":"markdown","05e26571":"markdown","44314586":"markdown","b797ae39":"markdown","46d29825":"markdown"},"source":{"937f7c7d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","681ff8e1":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","70f1b6d1":" credit_card_data= pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","be7ab6eb":"credit_card_data.head()","60067f2c":"credit_card_data.info()","2be70595":"# Checking if there are missing values\ncredit_card_data.isnull().sum()","97eb16df":"credit_card_data['Class'].value_counts()","63cd1647":"fraud= credit_card_data[credit_card_data.Class == 1]\nlegit= credit_card_data[credit_card_data.Class == 0]","8fbfdc5c":"print(legit.Amount.describe())\nprint(fraud.Amount.describe())","52d0e4a0":"credit_card_data.groupby('Class').mean()","5974fab9":"legit_sample = legit.sample(n= 492)\n\nnew_dataset= pd.concat([legit_sample, fraud], axis= 0) # making the new dataset","88b5685a":"new_dataset.head()","6abc8071":"new_dataset['Class'].value_counts()","5c69ec18":"new_dataset.groupby('Class').mean()","d31f8d0f":"X= new_dataset.drop(columns= 'Class', axis=1)\ny= new_dataset['Class']","90e1b66e":"X","ce86256e":"y","52edd04e":"X_train, X_test, y_train, y_test= train_test_split(X,y, test_size= 0.2, stratify= y, random_state= 6)","d7446d68":"model= LogisticRegression()","80608306":"model.fit(X_train, y_train)","50578341":"X_train_prediction = model.predict(X_train)\ntraining_accuracy = accuracy_score(X_train_prediction, y_train)\nprint('Accuracy on training data = ', training_accuracy)","bb4e56f4":"X_test_prediction = model.predict(X_test)\ntest_accuracy = accuracy_score(X_test_prediction, y_test)\nprint('Accuracy on test data = ', test_accuracy)","53716bad":"Thus this dataset is highly unbalanced.\n\n0 -> Normal Transaction\n\n\n1 -> Fraud Transaction","151af9f1":"# Model Training","3891a384":"## Loading to a pandas dataframe","961c80d8":"# Dataset Information","a04cbb9d":"## Split the dataset into training and testing","c6425ee8":"## Splitting the dataset into features and target variables","3f160b43":"## Distribution of fraudulant and legitimate transactions","9b66d5d0":"# Importing Dependencies","3870db90":"# Under-Sampling","05e26571":"We are going to built sample dataset of similar distribution of normal and fraud transactions","44314586":"## Separating legitimate instances from fraud instances","b797ae39":"# Evaluation","46d29825":"Hence it has not changed much. Hence, we can proceed with this dataset."}}