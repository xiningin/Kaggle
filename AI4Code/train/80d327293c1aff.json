{"cell_type":{"e3696b05":"code","11c70a04":"code","dd3ad1f5":"code","f0feb2c3":"code","f87c7761":"code","14d8ea9a":"code","d9ba83b5":"code","cedcebf9":"code","c88b7917":"code","2563ce4f":"code","4d35df36":"code","57c0200a":"code","6baab0cc":"code","516233a7":"code","f5acd3e1":"code","71cde48e":"code","a19046a7":"code","e4f1922d":"code","7cc93864":"code","6e413fc6":"code","624af3d0":"code","75609d9b":"code","b8495aab":"code","b13b12eb":"code","67c0c1e6":"code","fa6516d4":"code","e0f1ed46":"code","306eab28":"code","4e3c79ee":"code","7311fb71":"code","7932b24b":"code","390eff10":"code","109226a0":"code","b2188e4b":"code","5995daa0":"code","e9fc969f":"code","293a0664":"code","4b1e570e":"code","7cec8193":"code","b8852cf3":"code","a5da23dd":"code","02d6145e":"code","8fb0ff5b":"code","8a4734b6":"code","2af3ad2e":"code","5a6dfd6a":"code","b92a7978":"code","35088304":"code","7e5be39d":"code","96b80e93":"code","ee368d50":"code","61e972ec":"code","573d383e":"code","d87a8f09":"code","8a1c25d0":"markdown","3d1aa411":"markdown","0ec91fbe":"markdown","7adc092e":"markdown","1627ea31":"markdown","8f24935d":"markdown","5dcc56cf":"markdown","d4cdec51":"markdown","e4e19e82":"markdown","2cdaf4ff":"markdown","ec178335":"markdown","6800261a":"markdown","92a7d4b2":"markdown","93684585":"markdown","80299d97":"markdown","a55556f6":"markdown","7c7f9b08":"markdown","e9d6719b":"markdown","33539ed8":"markdown","02a6ac42":"markdown","c1b931ce":"markdown","5b8d7fa1":"markdown","c6e2a024":"markdown","46562dfd":"markdown","a8ec75b8":"markdown","519f5230":"markdown","83d2f133":"markdown","6f917edd":"markdown","d1f7c3aa":"markdown","fe4c4d21":"markdown","6ce88a70":"markdown","8a116497":"markdown","3884ae1a":"markdown","8a023ab3":"markdown","2008cf70":"markdown","78fea60c":"markdown"},"source":{"e3696b05":"# importing packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')","11c70a04":"# for expanding dataframe and displaying all columns\npd.set_option('display.max_columns', None)  \npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', -1)","dd3ad1f5":"# loading data from dat file to a dataframe\ndf = pd.read_csv('..\/input\/airfare-and-demand-2002\/airq402.dat', sep='\\s+', header=None)","f0feb2c3":"# displaying top 5 rows and we can observe we dont have column labels\ndf.head()","f87c7761":"# renaming columns\ndf.columns = ['City1', 'City2', 'Average_Fare0', 'Distance', 'Average Weekly Passengers', 'Market Leading Airline', \n           'Market_Share1', 'Average_Fare1', 'Low Price Airline', 'Market_Share2', 'Average_Fare2']","14d8ea9a":"# top 5 rows after renaming columns\ndf.head()","d9ba83b5":"df.info()","cedcebf9":"df.describe()","c88b7917":"# pair plot function\ndef plot_pair(df):\n    fig=plt.figure(figsize=(64,64))\n    sns.pairplot(df)\n    plt.show()","2563ce4f":"# box plot function\ndef plot_box(df):\n    plt.figure(figsize=(25, 30))\n    i=1\n    for each in columns:\n        plt.subplot(3, 3, i)\n        sns.boxplot(y = each,data = df)\n        i+=1\n    plt.show()","4d35df36":"# Correlation plot function\ndef plot_corr(df):\n    plt.figure(figsize=(20, 14))\n    sns.heatmap(df.corr(), cmap='YlGnBu', annot = True)\n    plt.show()","57c0200a":"# # Scatter plot function\n# def plot_scatter(X,y):\n#   plt.figure(figsize=(25, 30))\n#   i=1\n#   for each in [2,3,5,6,8,9]:\n#     plt.subplot(3,2, i)\n#     sns.scatter(X.iloc[:,each], y, alpha=0.5)\n#     plt.show()","6baab0cc":"columns = ['Average_Fare0', 'Distance', 'Average Weekly Passengers', 'Market_Share1', 'Average_Fare1', 'Market_Share2', 'Average_Fare2']","516233a7":"plot_pair(df)\nplt.show()","f5acd3e1":"plot_corr(df)","71cde48e":"plot_box(df)","a19046a7":"leading_airline_group = df.groupby(['Market Leading Airline'])['City1'].size()\nleading_airline_group.plot.bar(figsize=(18,8))\nplt.show()","e4f1922d":"low_price_airline_group = df.groupby(['Low Price Airline'])['City1'].size()\nlow_price_airline_group.plot.bar(figsize=(18,8))\nplt.show()","7cc93864":"# Printing top 5 rows\ndf.head()","6e413fc6":"X = df.drop('Average_Fare0' , axis = 1)\ny = df.Average_Fare0","624af3d0":"X = pd.get_dummies(X)\nX.head()","75609d9b":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error , make_scorer","b8495aab":"X_train,X_test,y_train,y_test = train_test_split(X,y,train_size = 0.7,random_state = 42)","b13b12eb":"#printing length\nlen(y_train),len(X_train)","67c0c1e6":"from sklearn.preprocessing import StandardScaler","fa6516d4":"# creating standardscaler object\nscaler = StandardScaler()\n\n#Scaling and Transforming our training Dataframe\nX_train = scaler.fit_transform(X_train)\n","e0f1ed46":"# We don't want our test set to learn from training Data so, we are will just transform it\nX_test = scaler.transform(X_test)","306eab28":"kfolds = KFold(n_splits=3, shuffle=True, random_state=42)\n\n# model scoring and validation function\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y,scoring=\"r2\",cv=kfolds))\n    return (rmse)\n\n# rmsle scoring function\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","4e3c79ee":"linear_model_rfe = LinearRegression()\nlinear_model_rfe.fit(X_train,y_train)","7311fb71":"## Train Accuracy\ny_train_pred=linear_model_rfe.predict(X_train)\n\n## Test Accuracy\ny_pred=linear_model_rfe.predict(X_test)\n\nrmsle(y_test,y_pred), rmsle(y_train,y_train_pred)","7932b24b":"#### Let's verify this by evaluating r-squared score\n\nfrom sklearn.metrics import r2_score\nr2_score(y_train,y_train_pred)","390eff10":"y_pred = linear_model_rfe.predict(X_test)\nr2_score(y_test,y_pred)","109226a0":"### Also Cross Validation Score\ncv_rmse(linear_model_rfe,X)","b2188e4b":"linear_train_score = linear_model_rfe.score(X_train,y_train)\nlinear_test_score = linear_model_rfe.score(X_test, y_test)\nlinear_train_score , linear_test_score","5995daa0":"from sklearn.feature_selection import RFE\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","e9fc969f":"# Selection top 20 features using RFE\nselect = RFE(linear_model_rfe, 20 ,step=1)\nselect = select.fit(X_train,y_train)","293a0664":"# Ranking features based on their relevancy\nselect.ranking_","4b1e570e":"# Zipping column names, ranking and support\nlist(zip(X.columns,select.support_,select.ranking_))","7cec8193":"col = X.columns[select.support_]\ncol","b8852cf3":"X_train_rfe = X[col]","a5da23dd":"# Checking VIF of Each predictor Variable\nvif = pd.DataFrame()\nvif['Features']=X_train_rfe.columns\nvif['VIF'] = [ variance_inflation_factor(X_train_rfe.values,i) for i in range(X_train_rfe.shape[1])]\nvif.sort_values(by = 'VIF' , ascending=False)","02d6145e":"# training and test data with top 20 features given by VIF\nxx = pd.DataFrame(X_train, columns= X.columns)[col]\nxt = pd.DataFrame(X_test, columns= X.columns)[col]","8fb0ff5b":"lm = LinearRegression()\nlm.fit(xx,y_train)","8a4734b6":"## Train Accuracy\ny_train_pred=lm.predict(xx)\n\n## Test Accuracy\ny_pred=lm.predict(xt)\n\nrmsle(y_test,y_pred), rmsle(y_train,y_train_pred)","2af3ad2e":"from sklearn.metrics import r2_score\nr2_score(y_train,y_train_pred)","5a6dfd6a":"y_pred = lm.predict(xt)\nr2_score(y_test,y_pred)","b92a7978":"### Also Cross Validation Score\ncv_rmse(lm,X)","35088304":"linear_train_score = lm.score(xx,y_train)\nlinear_test_score = lm.score(xt, y_test)\nlinear_train_score , linear_test_score","7e5be39d":"from sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\n\nlasso = Lasso(alpha=0.01, max_iter=10e5)\nrr = Ridge(alpha=0.01)","96b80e93":"rr.fit(X_train, y_train)\nRidge_train_score = rr.score(X_train,y_train)\nRidge_test_score = rr.score(X_test, y_test)\nRidge_train_score,Ridge_test_score","ee368d50":"lasso.fit(X_train, y_train)\nLasso_train_score = lasso.score(X_train,y_train)\nLasso_test_score = lasso.score(X_test, y_test)\nLasso_train_score,Lasso_test_score","61e972ec":"plt.figure(figsize = (16,10))\nplt.plot(rr.coef_,alpha=0.4,linestyle='none',marker='o',markersize=7,color='green',label='Ridge Regression')\nplt.plot(lasso.coef_,alpha=0.4,linestyle='none',marker='o',markersize=7,color='red',label='Lasso Regression')\n\nplt.xlabel('Coefficient Index',fontsize=16)\nplt.ylabel('Coefficient Magnitude',fontsize=16)\nplt.legend(fontsize=13,loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()","573d383e":"coeff_df = pd.DataFrame(lasso.coef_, X.columns, columns=['Coefficient'])  \ncoeff_df['absolute'] = coeff_df.Coefficient.abs()\ncoeff_df.head()","d87a8f09":"coeff_df = coeff_df.sort_values(by = 'absolute', ascending = False)\n\n# Printing top 10 variables with highest importance\/impact on label per unit change\ncoeff_df.Coefficient.head(10)","8a1c25d0":"As we can see the difference of Mean_Squared_error between Train Score and test Score is huge.\nOur model has overfitted on train set.","3d1aa411":"**WE are getting r2Score of 98.47 and 97.85 percent respectively**\nThus ridge is performing much better than simple linear Regression\n\n","0ec91fbe":"#### let's write some methods to plot our data.","7adc092e":"From the boxplot above, you can see there are some outliers in average fair2 , average fare 0 and weekly passenger.\n\nThese can be due to high demand of festive season or any other reason.\n\nTo impute or perform any action on these we will need more insights in the dataset","1627ea31":"You can see the most important features by the magnitude of Coefficient are given above","8f24935d":"## Observations","5dcc56cf":"#### Splitting our dataset into Train and Test set uing 70:30 ratio","d4cdec51":"### Feature Selection","e4e19e82":"From the above data description, we can see there may be a chance of presence of outliers as from the values in 25th and 75th quartile.\n\nWe will draw boxplot in next section to verify this.","2cdaf4ff":"#### *As we can see the performance of our baseline Model is very Bad on test Set indicating OverFitting, and even performance on train data is also not good* \n#### We will create fit Regularized models next to simplify our model and increase the test accuracy\n","ec178335":"As you can see from the above correlation Matrix, how numerical variables are related to each other.\n- Average fare 1 and average fare 2 are highly correlated with out target variable average fare0\n- Average weekly passenger, Market Share2, Market share 1 are negative correlated with our Target variable.","6800261a":"#### Let's seperate our target variable from predictor variables.\n\nWe will store target variable in y, \nwhile predictor varibles in X","92a7d4b2":"# Part 1","93684585":"#### Creating a baseline linear Regression model and fitting the data","80299d97":"### Printing Model Coefficients as indicator of Feature importance in predicting the outcome Variable","a55556f6":"## Modeling\n\nIn this section we will perform modeling on our dataset.\nWe will use different algorithms to model our data and than later pick the best one.\n","7c7f9b08":"#### *As we can see the performance of our baseline Model is very Bad on test Set indicating OverFitting* \n#### We will do feature selection using VIF","e9d6719b":"**WE are getting r2Score of 98.46 and 97.88 percent respectively thus lasso is outperforming Ridge on Test Set marginally**","33539ed8":"#### We can observe from above information that we don't have any null values present in the dataframe.","02a6ac42":"### Model Evaluation Functions\nwriting functions for performance evaluation of our models","c1b931ce":"### We can observe here, feature given using RFE are not quite significant","5b8d7fa1":"## Part 3","c6e2a024":"#### Let's scale our data to reduce model complexity and increase performance","46562dfd":"## Exploratory Data Analysis of the data","a8ec75b8":"## Load the data using Python Pandas library","519f5230":"- There were few outliers in few of the columns data, but we can ignore them as these outliers may be because of surge pricing when demand is high or during festive season.\n- As we can observe, not just continuous variables which he had plays the significant role for improving the model accuracy, but also few of the origin and destination cities and market leading airlines\n- Linear Regression model was not quite a good fit for this type of data, as it heavily overfitted on train data but accuracy on test data was very low.\n- Using VIF also, the feature which we were getting for Linear Regression model was not quite significant, so we just can't blindly trust output of VIF, because the top 20 features which we selected using RFE technique, was not making much sense\n- Ridge Regression model was performing way better than Linear Regression model, but we fitted all data into it.\n- Lasso Regression proves to be best for this type of data, as it automatically do the relevant feature selection also, and based on business understanding, the top 10 features given by Lasso model are very much relevant\n","83d2f133":"#### Generating Dummy variables of Non-Numerical Categorical columns.\nAs our machine learning model can only process numerical data we have to process our non Numerical columns\nNote- Assuming that, origin city and destination city also plays crucial role while deciding the airfare, as well as the airlines too(few of the airlines can be top service provider, so their price can be slightly higher, or they are making more profit, so they can afford to minimize the price to provide better service)","6f917edd":"## Part 2: Preparing data for Modeling","d1f7c3aa":"**45.99 % r2_score** on trainData","fe4c4d21":"As we can see the difference of Mean_Squared_error between Train Score and test Score is huge.\nOur model has overfitted on train set.","6ce88a70":"#### Let's verify this by evaluating r-squared score","8a116497":"**95.8 % r2_score** on trainData","3884ae1a":"From the above bar plot we can observe **WN** is the Market Leading Airline for most of the trips, further followed by **DL** and **AA** after it.","8a023ab3":"The above plot clearly indicates the relationship between pair of variables along with data disribution of each variable at diagonal.\n- You can see that average_fair1 and average_fare2 have approximately high linear relation with our target variable average_fare0\n\nWe will futher investigate into these relationship  during HeatMap of Correlation Matrix","2008cf70":"From the above bar plot we can observe **WN** is also the Low Price Airline for most of the trips, further followed by **DL** and **AA** after it.","78fea60c":"### Creating Linear Regression Model with all features"}}