{"cell_type":{"64abeb12":"code","ae4090d3":"code","58c8e6dd":"code","041fa128":"code","6716d02b":"code","01237bba":"code","da0fa249":"code","07e7c761":"code","8b7c4f03":"code","da3add08":"code","cc61716b":"code","ff79b922":"markdown","1e975032":"markdown","56c2c4f5":"markdown","7d7a6ec3":"markdown","756274f9":"markdown","32a06a1b":"markdown","e606f640":"markdown","cf8bb4ee":"markdown","e7e6e7d2":"markdown","c60a9cb9":"markdown"},"source":{"64abeb12":"import numpy as np\nimport pandas as pd\npd.set_option('max_columns', 300)\n\nimport cudf\n\nimport os\nimport glob\nfrom datetime import datetime\nfrom joblib import Parallel, delayed\nfrom typing import List\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","ae4090d3":"class Config:\n    INPUT_PATH = Path('..\/input\/optiver-realized-volatility-prediction\/')\n    IS_DEBUG = False\n    SECONDS_IN_BUCKETS = [0, 100] if IS_DEBUG else list(range(0, 600, 100))\n    NUM_STOCK_IDS = 3 if IS_DEBUG else 200\n    N_CLUSTERS = 7","58c8e6dd":"def calc_wap(df, bid_price_col, bid_size_col, ask_price_col, ask_size_col):\n    bid_price = df[bid_price_col]\n    bid_size = df[bid_size_col]\n    ask_price = df[ask_price_col]\n    ask_size = df[ask_size_col]\n\n    wap  = bid_price * ask_size\n    wap += ask_price * bid_size\n    wap \/= bid_size  + ask_size\n\n    return wap\n\ndef calc_log_return_squared(df, col):\n    col_log = df[col].log()\n        \n    log_return = col_log - col_log.groupby(df['time_id']).shift().reset_index(drop=True)\n    return log_return ** 2\n\ndef calc_wap_balance(df) -> np.array:\n    wap1 = df[\"wap1\"]\n    wap2 = df[\"wap2\"]\n    return np.abs(wap1 - wap2)","041fa128":"def calc_quoted_spread(df:pd.DataFrame, bid_price_col, ask_price_col) -> np.array:\n    bid_price = df[bid_price_col]\n    ask_price = df[ask_price_col]\n    \n    midpoint = (ask_price + bid_price) \/ 2\n    quoted_spread = (ask_price - bid_price) \/ midpoint * 100\n    return quoted_spread\n\ndef calc_spread(df:pd.DataFrame, col1, col2) -> np.array:\n    return df[col1] - df[col2]\n\ndef calc_total_volume(df:pd.DataFrame) -> np.array:\n    bid_size1 = df[f'bid_size1']\n    bid_size2 = df[f'bid_size2']\n    ask_size1 = df[f'ask_size1']\n    ask_size2 = df[f'ask_size2']\n    \n    total_volumne = (bid_size1 + bid_size2) + (ask_size1 + ask_size2)\n    return total_volumne\n\ndef calc_volume_imbalance(df:pd.DataFrame) -> np.array:\n    bid_size1 = df[f'bid_size1']\n    bid_size2 = df[f'bid_size2']\n    ask_size1 = df[f'ask_size1']\n    ask_size2 = df[f'ask_size2']\n    \n    volume_imbalance = np.abs((ask_size1 + ask_size2) - (bid_size1 + bid_size2))\n    return volume_imbalance","6716d02b":"def get_stats_window(\n    feature_df:pd.DataFrame, \n    feature_dict:dict,\n    prefix:str,\n    seconds_in_bucket:int=0) -> pd.DataFrame:\n    \n    feature_df = feature_df.query(f'seconds_in_bucket >= {seconds_in_bucket}')\n    feature_df = feature_df.groupby('time_id').agg(feature_dict)\n    \n    feature_df.columns = [f'{prefix}_' + '_'.join(col) + f'>={seconds_in_bucket}' for col in feature_df.columns]\n    \n    return feature_df","01237bba":"def book_preprocessor(\n    stock_id:int=0, \n    exec_type='train', \n    seconds_in_buckets:List[int]=[0, 100]) -> pd.DataFrame:\n    \n    assert exec_type in ['train', 'test']\n    \n    BOOK_FILE_PATH = Config.INPUT_PATH \/ f'book_{exec_type}.parquet\/stock_id={stock_id}'\n    book_df = cudf.read_parquet(BOOK_FILE_PATH)\n    \n    book_df[\"wap1\"] = calc_wap(book_df, bid_price_col=\"bid_price1\", bid_size_col=\"bid_size1\", ask_price_col=\"ask_price1\", ask_size_col=\"ask_size1\")\n    book_df[\"wap2\"] = calc_wap(book_df, bid_price_col=\"bid_price2\", bid_size_col=\"bid_size2\", ask_price_col=\"ask_price2\", ask_size_col=\"ask_size2\")\n    \n    book_df[\"wap1_log_return_squared\"] = calc_log_return_squared(book_df, \"wap1\")\n    book_df[\"wap2_log_return_squared\"] = calc_log_return_squared(book_df, \"wap2\")\n    \n    book_df[\"wap_balance\"] = calc_wap_balance(book_df)\n    \n    book_df[\"quoted_spread\"] = calc_quoted_spread(book_df, bid_price_col=\"bid_price1\", ask_price_col=\"ask_price1\")\n    book_df[\"bid_spread\"] = calc_spread(book_df, col1=\"bid_price1\", col2=\"bid_price2\")\n    book_df[\"ask_spread\"] = calc_spread(book_df, col1=\"ask_price1\", col2=\"ask_price2\")\n    book_df[\"bid_ask_spread\"] = np.abs(calc_spread(book_df, col1=\"bid_spread\", col2=\"ask_spread\"))\n    \n    book_df[\"total_volume\"] = calc_total_volume(book_df)\n    book_df[\"volume_imbalance\"] = calc_volume_imbalance(book_df)\n    \n    # Dict for aggregations\n    default_stats = ['sum', 'mean', 'std', 'max', 'min']\n    book_feature_dict = {\n        'seconds_in_bucket': ['nunique'],\n        \n        'wap1': ['max', 'min'],\n        'wap2': ['max', 'min'],\n        \n        'wap1_log_return_squared': ['sum'],\n        'wap2_log_return_squared': ['sum'],\n        \n        'quoted_spread': [ 'mean', 'max', 'min'],\n        'bid_spread': [ 'mean', 'max', 'min'],\n        'ask_spread': [ 'mean', 'max', 'min'],\n        'bid_ask_spread': [ 'mean', 'max', 'min'],\n        \n        'wap_balance': ['mean', 'max', 'min'],\n        'total_volume': ['sum', 'mean', 'max', 'min'],\n        'volume_imbalance': ['mean', 'max', 'min'],\n    }\n    book_feature_dict_time = {\n        'wap1_log_return_squared': ['sum'],\n        'wap2_log_return_squared': ['sum'],\n    }\n    \n    output_df = cudf.DataFrame()\n    \n    for seconds_in_bucket in tqdm(Config.SECONDS_IN_BUCKETS):\n        tmp_df = get_stats_window(\n            feature_df=book_df, \n            feature_dict=book_feature_dict if seconds_in_bucket == 0 else book_feature_dict_time,\n            prefix='book',\n            seconds_in_bucket=seconds_in_bucket\n        )\n        \n        output_df = cudf.concat([\n            output_df, \n            tmp_df,\n        ], axis=1)\n        \n    output_df.reset_index(inplace=True)\n    output_df['stock_id'] = np.int8(stock_id)\n    output_df.set_index(['stock_id', 'time_id'] ,inplace=True)\n    \n    return output_df","da0fa249":"def trade_preprocessor(\n    stock_id:int=0, \n    exec_type='train', \n    seconds_in_buckets:List[int]=[0, 100]) -> pd.DataFrame:\n    \n    assert exec_type in ['train', 'test']\n    \n    TRADE_FILE_PATH = Config.INPUT_PATH \/ f'trade_{exec_type}.parquet\/stock_id={stock_id}'\n    trade_df = cudf.read_parquet(TRADE_FILE_PATH)\n    \n    trade_df[\"price_log_return_squared\"] = calc_log_return_squared(trade_df, \"price\")\n    trade_df[\"amount\"] = trade_df[\"price\"] * trade_df[\"size\"]\n    \n    # Dict for aggregations\n    default_stats = ['sum', 'mean', 'std', 'max', 'min']\n    trade_feature_dict = {\n        'seconds_in_bucket': ['nunique'],\n        'price': ['mean', 'max', 'min'],\n        'price_log_return_squared': ['sum', \"mean\"],\n        'size': ['sum', 'mean', 'max', 'min'],\n        'order_count': ['sum', 'mean', 'max', 'min'],\n        'amount': ['sum', 'mean', 'max', 'min'],\n    }\n    trade_feature_dict_time = {\n        'seconds_in_bucket': ['nunique'],\n        'price_log_return_squared': ['sum', \"mean\"],\n        'size': ['sum', 'mean', 'max', 'min'],\n        'order_count':  ['sum', 'mean', 'max', 'min'],\n    }\n\n    output_df = cudf.DataFrame()\n    \n    for seconds_in_bucket in Config.SECONDS_IN_BUCKETS:\n        tmp_df = get_stats_window(\n            feature_df=trade_df, \n            feature_dict=trade_feature_dict if seconds_in_bucket == 0 else trade_feature_dict_time,\n            prefix='trade',\n            seconds_in_bucket=seconds_in_bucket\n        )\n        \n        output_df = cudf.concat([\n            output_df, \n            tmp_df,\n        ], axis=1)\n\n    output_df.reset_index(inplace=True)\n    output_df['stock_id'] = np.int8(stock_id)\n    output_df.set_index(['stock_id', 'time_id'] ,inplace=True)\n    \n    return output_df","07e7c761":"def Preprocessor(\n    stock_ids:List[int]=[0], \n    exec_type:str='train') -> pd.DataFrame:\n    \n    assert exec_type in ['train', 'test']\n    \n    def parallel_preprocessor(stock_id:int, exec_type:str):\n        book_df = book_preprocessor(\n            stock_id=stock_id, \n            exec_type=exec_type\n        )\n        trade_df = trade_preprocessor(\n            stock_id=stock_id, \n            exec_type=exec_type\n        )\n\n        output_df = cudf.merge( \n            left=book_df,\n            right=trade_df, \n            how='left',\n            left_index=True, \n            right_index=True\n        )\n        \n        return output_df\n    \n    output_dfs = Parallel(n_jobs = -1, verbose = 1)(delayed(parallel_preprocessor)(stock_id, exec_type) for stock_id in stock_ids)\n    \n    # Concatenate all the dataframes that return from Parallel\n    output_df = cudf.concat(output_dfs)\n    return output_df","8b7c4f03":"def add_time_stock(df:pd.DataFrame) -> pd.DataFrame:\n    aggs = ['mean', 'std', 'max', 'min']\n    df = df.reset_index()\n    use_cols = [col for col in df.columns if 'squared_sum' in col]\n    \n    stock_id_df = df.groupby('stock_id')[use_cols].agg([\"mean\", \"max\", \"min\"])\n    stock_id_df.columns = [\"_\".join(map(str, col)) + '_stock_id' for col in stock_id_df.columns]\n    \n    time_id_df = df.groupby('time_id')[use_cols].agg([\"mean\", \"max\", \"min\"])\n    time_id_df.columns = [\"_\".join(map(str, col)) + '_time_id' for col in time_id_df.columns]\n    \n    use_cols = [col for col in df.columns if '>=0' in col]\n    cluster_time_id_df = df.groupby(['cluster', 'time_id'])[use_cols].agg(\"mean\")\n    cluster_time_id_df.reset_index(inplace=True)\n    cluster_time_id_df = cluster_time_id_df.pivot(index='time_id', columns='cluster')\n    cluster_time_id_df.columns = ['_'.join(map(str, col)) for col in cluster_time_id_df.columns]\n    \n    for col in use_cols:\n        for i in range(Config.N_CLUSTERS):\n            if f'{col}_{i}' not in cluster_time_id_df.columns:\n                cluster_time_id_df[f'{col}_{i}'] = np.nan\n    cluster_time_id_df = cluster_time_id_df[[f'{col}_{i}' for col in use_cols for i in range(Config.N_CLUSTERS)]]\n\n    df = cudf.merge(left=df, right=stock_id_df, left_on=['stock_id'], right_index=True)\n    df = cudf.merge(left=df, right=time_id_df, left_on=['time_id'], right_index=True)\n    df = cudf.merge(left=df, right=cluster_time_id_df, left_on=['time_id'], right_index=True)\n    \n    return df.reset_index(drop=True)","da3add08":"from sklearn.cluster import KMeans, AgglomerativeClustering\n\nclass ClusterStock:\n    def __init__(\n        self,\n        n_clusters: int=7, \n        random_state: int=3665):\n        \n        self.n_clusters = n_clusters\n        self.random_state = random_state\n    \n    def fit(\n        self, \n        X:pd.DataFrame):\n        \n        pivot_df = X.pivot_table(index='time_id', columns='stock_id', values='target')\n        corr_df = pivot_df.corr()\n        \n        clustering_method = KMeans(n_clusters=self.n_clusters, random_state=self.random_state)\n#         clustering_method = AgglomerativeClustering(n_clusters=self.n_clusters, affinity = 'euclidean', linkage = 'ward')\n        clustering_method.fit(corr_df.to_numpy())\n        \n        self.mapping_dict = dict(zip(corr_df.index, clustering_method.labels_))\n    \n    def transform(\n        self, \n        X:pd.DataFrame) -> np.array:\n        \n        return X['stock_id'].map(self.mapping_dict).fillna(-1).to_array()\n    \n    def fit_transform(\n        self, \n        X:pd.DataFrame) -> np.array:\n        \n        self.fit(X)\n        return self.transform(X)","cc61716b":"train_dtypes = {'stock_id': np.int8, 'time_id':np.int16, 'target':np.float32}\ntrain_df = pd.read_csv(Config.INPUT_PATH \/ \"train.csv\", dtype=train_dtypes)\n\ntrain_stock_ids = train_df['stock_id'].unique()\n\nfeature_train_df = Preprocessor(stock_ids=train_stock_ids[:Config.NUM_STOCK_IDS], exec_type='train')\n\ncluster_stock = ClusterStock(n_clusters=Config.N_CLUSTERS, random_state=3655)\ncluster_stock.fit(train_df)\n\nfeature_train_df['cluster'] = cluster_stock.transform(feature_train_df.reset_index())\nfeature_train_df = add_time_stock(feature_train_df)\n\nfeature_train_df = cudf.merge(\n    left=cudf.DataFrame(train_df).set_index(['stock_id', 'time_id']),\n    right=feature_train_df.set_index(['stock_id', 'time_id']),\n    how='left', \n    left_index=True, \n    right_index=True)\n\nfeature_train_df.sort_index(inplace=True)\nfeature_train_df.reset_index(inplace=True)\n\nfeature_train_df.to_feather('feature_train.feather')","ff79b922":"## Define Utils","1e975032":"## Import Labraries","56c2c4f5":"## Add Cluster","7d7a6ec3":"## Load Data","756274f9":"## Trade Preprocessor","32a06a1b":"## Book Preprocessor","e606f640":"## Install Libraries","cf8bb4ee":"## Add_time_stock","e7e6e7d2":"## Set Config","c60a9cb9":"## Parallel Preprocessor"}}