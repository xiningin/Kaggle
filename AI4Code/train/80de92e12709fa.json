{"cell_type":{"e56fc435":"code","bcca5c6e":"code","b975d313":"code","7f1e7293":"code","c4045db5":"code","42a83d09":"code","8cc799c7":"code","9e616add":"code","2f85130f":"code","a2e7e442":"code","f58aecca":"code","37bb9277":"code","40dd6b89":"code","1a6ff91b":"code","ccc8f73f":"code","bbf1dfe3":"code","b678e479":"code","173ad264":"code","a07186de":"code","b2b13a2b":"code","0c0d7062":"code","966efea1":"code","f9f765ee":"code","e5e75e96":"code","3436b467":"code","3adb7c68":"code","eea36a26":"code","edd281f3":"code","bd0a912a":"code","a9798e9d":"code","874c89a0":"code","95ad7671":"code","afbbe1fd":"code","60e95a37":"code","95e44c70":"code","23c5727d":"code","61d48950":"code","1e534e9f":"code","3415d825":"code","2a09060f":"code","d48b3a7d":"code","63d0d14c":"code","bf8de333":"code","6fe863c5":"code","7a23f22b":"code","eac7de99":"code","31c49ca8":"markdown","510bb74c":"markdown","20f0297e":"markdown","bb43e74a":"markdown"},"source":{"e56fc435":"import pandas as pd\nimport numpy as np\nimport os\nimport random\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk import pos_tag, pos_tag_sents\nimport string\nimport re\nimport math\n\n#!pip install readability\nimport sys\nsys.path = [\n    '..\/input\/readability-package',\n] + sys.path\nimport readability\nimport spacy\n\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport gensim\n\nimport warnings\nwarnings.filterwarnings('ignore')","bcca5c6e":"nlp_sm = spacy.load('en_core_web_sm')","b975d313":"!pip install ..\/input\/textstats\/textstat-master\n!pip install ..\/input\/pyphen\/Pyphen-master","7f1e7293":"import textstat","c4045db5":"train_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntrain_df.head(2)","42a83d09":"ex_excerpt = train_df.iloc[0].excerpt\nex_excerpt","8cc799c7":"sns.distplot(train_df[\"target\"])","9e616add":"sns.distplot(train_df[\"standard_error\"])","2f85130f":"test_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ntest_df.head(2)","a2e7e442":"#word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin', binary=True)\n#print(word2vec_model.vectors.shape)","f58aecca":"#glove_words = list(word2vec_model.index_to_key)","37bb9277":"def TF_IDF_W2V(text):\n    '''Calculate TF-IDF with word2vec\n    '''\n    #Load TF-IDF from sklearn\n    TFIDF_model = TfidfVectorizer()\n    #fit on text\n    TFIDF_model.fit(text)\n    #create dictionary with word as key\n    #and idf as value\n    dictionary = dict(zip(TFIDF_model.get_feature_names(), list(TFIDF_model.idf_)))\n    #apply set as we need unique features\n    TFIDF_words = set(TFIDF_model.get_feature_names())\n    #create list which stores TFIDF_W2V\n    TFIDF_W2V_vectors = []\n    for sentence in text:\n        #create empty vector to store result\n        vector = np.zeros(300)\n        #number of words with valid vector in sentence\n        TFIDF_weight =0\n        for word in sentence.split(): \n            #if word exist in glove_words and TFIDF_words\n            if (word in glove_words) and (word in TFIDF_words):\n                #get its vector from glove_words\n                vec = word2vec_model[word]\n                #calculate TF-IDF for each word\n                TFIDF = dictionary[word]*(sentence.count(word)\/len(sentence.split()))\n                #calculate TF-IDF weighted W2V\n                vector += (vec * TFIDF)\n                TFIDF_weight += TFIDF\n                \n        if TFIDF_weight != 0:\n            vector \/= TFIDF_weight\n        TFIDF_W2V_vectors.append(vector)\n    return TFIDF_W2V_vectors ","40dd6b89":"import string\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize\ndef preprocess_text(text):\n    text=str(text).lower()\n    text = re.sub('\\n', '', text)\n    return text\n\ndef removestop(text):\n    stop_words = set(stopwords.words('english')) \n  \n    word_tokens = word_tokenize(text) \n    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n    filtered_sentence = [] \n  \n    for w in word_tokens: \n        if w not in stop_words: \n            filtered_sentence.append(w) \n    return filtered_sentence\nSTOP_WORDS = set(stopwords.words('english')) # stop words\ndef rem_stop(text):\n    text = ' '.join([i for i in text.split() if i not in STOP_WORDS])\n    return text\ndef remove_punctuation(text):\n    text_clean=\"\".join([i for i in text if i not in string.punctuation])\n    return text_clean\n\n","1a6ff91b":"def number_sentence(text: str):\n    \n\n    \n    about_doc = nlp_sm(text) \n    sentences = list(about_doc.sents)\n    log_len_sentence = math.log(len(sentences))\n    len_sentence = len(sentences)\n    difficult_words = textstat.difficult_words(text)\n    log_difficult_words =  -1 if difficult_words==0 else math.log(difficult_words)\n    \n    word_list = []\n\n    for i in about_doc:\n        if not i.is_punct:\n            word_list.append(str(i).lower())\n\n    unique = set(word_list)\n    \n    unique_toker_per_text =  len(unique)\n    \n    log_unique_toker_per_text =  -1 if unique_toker_per_text==0 else math.log(unique_toker_per_text)\n    \n    text_len = len(text)\n    log_text_len = math.log(len(text))\n    \n    return [log_len_sentence, len_sentence,difficult_words, log_difficult_words, unique_toker_per_text, log_unique_toker_per_text, text_len,log_text_len]\n    ","ccc8f73f":"def text_stats(text: str):\n    \n\n    \n    difficult_words = textstat.difficult_words(text)\n    \n    return [difficult_words]","bbf1dfe3":"number_sentence(train_df.excerpt[0])","b678e479":"def readability_measurements(passage: str):\n    \"\"\"\n    This function uses the readability library for feature engineering.\n    It includes textual statistics, readability scales and metric, and some pos stats\n    \"\"\"\n    results = readability.getmeasures(passage, lang='en')\n    \n    chars_per_word = results['sentence info']['characters_per_word']\n    syll_per_word = results['sentence info']['syll_per_word']\n    words_per_sent = results['sentence info']['words_per_sentence']\n    complex_words  = results['sentence info']['complex_words']\n    long_words  = results['sentence info']['long_words']\n    \n    log_chars_per_word  =  -1 if chars_per_word==0 else math.log(chars_per_word)\n    log_syll_per_word   =  -1 if syll_per_word==0 else math.log(syll_per_word)\n    log_words_per_sent  =  -1 if words_per_sent==0 else math.log(words_per_sent)\n    log_complex_words   =  -1 if complex_words==0 else math.log(complex_words)\n    log_long_words      =  -1 if long_words==0 else math.log(long_words)\n    \n    \n    kincaid = results['readability grades']['Kincaid']\n    ari = results['readability grades']['ARI']\n    coleman_liau = results['readability grades']['Coleman-Liau']\n    flesch = results['readability grades']['FleschReadingEase']\n    gunning_fog = results['readability grades']['GunningFogIndex']\n    lix = results['readability grades']['LIX']\n    smog = results['readability grades']['SMOGIndex']\n    rix = results['readability grades']['RIX']\n    dale_chall = results['readability grades']['DaleChallIndex']\n    \n    tobeverb = results['word usage']['tobeverb']\n    auxverb = results['word usage']['auxverb']\n    conjunction = results['word usage']['conjunction']\n    pronoun = results['word usage']['pronoun']\n    preposition = results['word usage']['preposition']\n    nominalization = results['word usage']['nominalization']\n    \n    pronoun_b = results['sentence beginnings']['pronoun']\n    interrogative = results['sentence beginnings']['interrogative']\n    article = results['sentence beginnings']['article']\n    subordination = results['sentence beginnings']['subordination']\n    conjunction_b = results['sentence beginnings']['conjunction']\n    preposition_b = results['sentence beginnings']['preposition']\n\n    \n    return [chars_per_word, syll_per_word, words_per_sent,\n            kincaid, ari, coleman_liau, flesch, gunning_fog, lix, smog, rix, dale_chall,\n            tobeverb, auxverb, conjunction, pronoun, preposition, nominalization,\n            pronoun_b, interrogative, article, subordination, conjunction_b, preposition_b,complex_words,long_words,\n           log_chars_per_word,log_syll_per_word,log_words_per_sent,log_complex_words,log_long_words]","173ad264":"def spacy_features(df: pd.DataFrame):\n    \"\"\"\n    This function generates features using spacy en_core_wb_lg\n    I learned about this from these resources:\n    https:\/\/www.kaggle.com\/konradb\/linear-baseline-with-cv\n    https:\/\/www.kaggle.com\/anaverageengineer\/comlrp-baseline-for-complete-beginners\n    \"\"\"\n    \n    nlp = spacy.load('en_core_web_lg')\n    with nlp.disable_pipes():\n        vectors = np.array([nlp(text).vector for text in df.excerpt])\n        \n    return vectors\n\ndef get_spacy_col_names():\n    names = list()\n    for i in range(300):\n        names.append(f\"spacy_{i}\")\n        \n    return names","a07186de":"def tf_idf_features(df: pd.DataFrame):\n    \"\"\"\n    This function generates features using spacy en_core_wb_lg\n    I learned about this from these resources:\n    https:\/\/www.kaggle.com\/konradb\/linear-baseline-with-cv\n    https:\/\/www.kaggle.com\/anaverageengineer\/comlrp-baseline-for-complete-beginners\n    \"\"\"\n    \n    df['excerpt2']=df['excerpt'].apply(lambda x:preprocess_text(x))\n    df['excerpt2']=df['excerpt2'].apply(lambda x:remove_punctuation(x))\n    df['excerpt2']=df['excerpt2'].apply(lambda x:rem_stop(x))\n    \n    tfidf_w2v_excerpt_train = TF_IDF_W2V(df['excerpt2'])\n    vectors = np.array(tfidf_w2v_excerpt_train)\n        \n    return vectors\n\ndef get_tf_idf_col_names():\n    names = list()\n    for i in range(300):\n        names.append(f\"tf_idf_{i}\")\n        \n    return names","b2b13a2b":"#get_tf_idf_col_names()","0c0d7062":"def pos_tag_features(passage: str):\n    \"\"\"\n    This function counts the number of times different parts of speech occur in an excerpt\n    \"\"\"\n    pos_tags = [\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n                \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n                \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"]\n    \n    tags = pos_tag(word_tokenize(passage))\n    tag_list= list()\n    \n    for tag in pos_tags:\n        tag_list.append(len([i[0] for i in tags if i[1] == tag]))\n    \n    return tag_list","966efea1":"def generate_other_features(passage: str):\n    \"\"\"\n    This function is where I test miscellaneous features\n    This is experimental\n    \"\"\"\n    # punctuation count\n    periods = passage.count(\".\")\n    commas = passage.count(\",\")\n    semis = passage.count(\";\")\n    exclaims = passage.count(\"!\")\n    questions = passage.count(\"?\")\n    \n    # Some other stats\n    num_char = len(passage)\n    num_words = len(passage.split(\" \"))\n    unique_words = len(set(passage.split(\" \") ))\n    word_diversity = unique_words\/num_words\n    \n    word_len = [len(w) for w in passage.split(\" \")]\n    longest_word = np.max(word_len)\n    avg_len_word = np.mean(word_len)\n    \n    return [periods, commas, semis, exclaims, questions,\n            num_char, num_words, unique_words, word_diversity,\n            longest_word, avg_len_word]","f9f765ee":"def create_folds(data: pd.DataFrame, num_splits: int, seed=42):\n    \"\"\" \n    This function creates a kfold cross validation system based on this reference: \n    https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds\n    \"\"\"\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1, random_state=seed).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data","e5e75e96":"class CLRDataset:\n    \"\"\"\n    This is my CommonLit Readability Dataset.\n    By calling the get_df method on an object of this class,\n    you will have a fully feature engineered dataframe\n    \"\"\"\n    def __init__(self, df: pd.DataFrame, train: bool, n_folds=2):\n        self.df = df\n        self.excerpts = df[\"excerpt\"]\n        \n        self._extract_features()\n        \n        if train:\n            self.df = create_folds(self.df, n_folds)\n        \n    def _extract_features(self):\n        scores_df = pd.DataFrame(self.df[\"excerpt\"].apply(lambda p : readability_measurements(p)).tolist(), \n                                 columns=[\"chars_per_word\", \"syll_per_word\", \"words_per_sent\",\n                                          \"kincaid\", \"ari\", \"coleman_liau\", \"flesch\", \"gunning_fog\", \"lix\", \"smog\", \"rix\", \"dale_chall\",\n                                          \"tobeverb\", \"auxverb\", \"conjunction\", \"pronoun\", \"preposition\", \"nominalization\",\n                                          \"pronoun_b\", \"interrogative\", \"article\", \"subordination\", \"conjunction_b\",\n                                          \"preposition_b\",\"complex_words\",\"long_words\",\n                                         \"log_chars_per_word\",\"log_syll_per_word\",\"log_words_per_sent\",\"log_complex_words\",\"log_long_words\"])\n        self.df = pd.merge(self.df, scores_df, left_index=True, right_index=True)\n        \n        spacy_df = pd.DataFrame(spacy_features(self.df), columns=get_spacy_col_names())\n        self.df = pd.merge(self.df, spacy_df, left_index=True, right_index=True)\n        \n        #tf_id = pd.DataFrame(tf_idf_features(self.df), columns=get_tf_idf_col_names())\n        #self.df = pd.merge(self.df, tf_id, left_index=True, right_index=True)\n        \n        add_df = pd.DataFrame(self.df[\"excerpt\"].apply(lambda p : number_sentence(p)).tolist(),\n                              columns=[\"log_number_sentence\",\"number_sentence\",\"difficult_words\",\"log_difficult_words\",\n                                      \"unique_toker_per_text\",\"log_text_len\",\"text_len\",\"log_unique_toker_per_text\"])\n        self.df = pd.merge(self.df, add_df, left_index=True, right_index=True)\n        \n        pos_df = pd.DataFrame(self.df[\"excerpt\"].apply(lambda p : pos_tag_features(p)).tolist(),\n                              columns=[\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n                                       \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n                                       \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"])\n        self.df = pd.merge(self.df, pos_df, left_index=True, right_index=True)\n        \n        other_df = pd.DataFrame(self.df[\"excerpt\"].apply(lambda p : generate_other_features(p)).tolist(),\n                                columns=[\"periods\", \"commas\", \"semis\", \"exclaims\", \"questions\",\n                                         \"num_char\", \"num_words\", \"unique_words\", \"word_diversity\",\n                                         \"longest_word\", \"avg_len_word\"])\n        self.df = pd.merge(self.df, other_df, left_index=True, right_index=True)\n        \n    def get_df(self):\n        return self.df\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx: int):\n        pass","3436b467":"dataset = CLRDataset(train_df, train=True)\ndf = dataset.get_df()\ndf.head() # train dataframe","3adb7c68":"notplot = ['id', 'url_legal','license','excerpt']\n\n#for col in df.columns:\n#    if col not in notplot:\n#        df.plot.hist(col)","eea36a26":"plt.hist(df[\"chars_per_word\"])","edd281f3":"# This is just here to investigate different features\nplt.scatter((df[\"flesch\"]), df[\"target\"])\nplt.show()","bd0a912a":"test_dataset = CLRDataset(test_df, train=False)\ntest_df = test_dataset.get_df()\ntest_df.head(2) # test dataframe","a9798e9d":"test_df","874c89a0":"def set_seed(seed=42):\n    \"\"\" Sets the Seed \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    \nset_seed(42)","95ad7671":"features = [\"chars_per_word\", \"syll_per_word\", \"words_per_sent\",\n            \"kincaid\", \"ari\", \"coleman_liau\", \"flesch\", \"gunning_fog\", \"lix\", \"smog\", \"rix\", \"dale_chall\",\n            \"tobeverb\", \"auxverb\", \"conjunction\", \"pronoun\", \"preposition\", \"nominalization\", \n            \"pronoun_b\", \"interrogative\", \"article\", \"subordination\", \"conjunction_b\", \"preposition_b\",\n            \"number_sentence\",\"log_number_sentence\",\"difficult_words\",\"complex_words\",\"long_words\",\"log_difficult_words\"\n            ,\"log_text_len\", \"text_len\",\"log_chars_per_word\",\"log_syll_per_word\",\"log_words_per_sent\",\"log_complex_words\",\"log_long_words\"\n           ]\n           \nfeatures+=get_spacy_col_names()\n#features+=get_tf_idf_col_names()\nfeatures+=[\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\",  \"MD\", \n            \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n            \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"]\n#features+=[\"avg_len_word\"]\n#features+= [#\"periods\", \"commas\", \"semis\", \"exclaims\", \"questions\",\n            #\"num_char\", \"num_words\", \"unique_words\", \n#    \"word_diversity\",\n            #\"longest_word\", \"avg_len_word\"\n #          ]\n# currently better results without the other_df features","afbbe1fd":"\"\"\" I normalize the data here, could be useful depending on your model\"\"\"\n#scaler = MinMaxScaler()\n#df[features] = scaler.fit_transform(df[features])\n#test_df[features] = scaler.transform(test_df[features])","60e95a37":"def train_pred_one_fold(model_name: str, fold: int, df: pd.DataFrame, test_df: pd.DataFrame, features: list, rmse: list):\n    \"\"\"\n    This function trains and predicts on one fold of your selected model\n    df is the train df, test_df is the test_df\n    X features are defined in features\n    y output is target\n    oof score is printed and stored in the rmse list\n    \"\"\"\n    train = df[df.kfold == fold]\n    X_train = train[features]\n    y_train = train[\"target\"]\n \n    valid = df[df.kfold != fold]\n    X_valid = valid[features]\n    y_valid = valid[\"target\"]\n    \n    X_test = test_df[features]\n\n    if model_name == 'ridge':\n        model = Ridge(alpha=.7)    \n        model.fit(X_train, y_train)\n        oof = model.predict(X_valid)\n        print(np.sqrt(mean_squared_error(y_valid, oof)))\n        rmse.append(np.sqrt(mean_squared_error(y_valid, oof)))\n        test_preds = model.predict(X_test)\n        \n    elif model_name == 'Gradiend':\n        model = GradientBoostingRegressor(n_estimators=500,max_depth=2)    \n        model.fit(X_train, y_train)\n        oof = model.predict(X_valid)\n        print(np.sqrt(mean_squared_error(y_valid, oof)))\n        rmse.append(np.sqrt(mean_squared_error(y_valid, oof)))\n        test_preds = model.predict(X_test)\n        \n    else:\n        test_preds = 0\n        raise Exception(\"Not Implemented\")\n        \n    return test_preds","95e44c70":"def train_pred(model_name: str, df: pd.DataFrame, test_df: pd.DataFrame, features: list):\n    \"\"\"\n    This function trains and predicts multiple fold using train_pred_one_fold\n    The average rmse is printed the the test data predictions are returned\n    The last column is the average result from all folds to be submitted\n    \"\"\"\n    print(f\"model_name: {model_name}\")\n    all_preds = pd.DataFrame()\n    rmse = list()\n    for f in range(2):\n        all_preds[f\"{model_name}_{f}\"] = train_pred_one_fold(model_name, f, df, test_df, features, rmse)\n\n    all_preds[f\"{model_name}\"] = all_preds.mean(axis=1)\n    print(\"---------\")\n    print(f\"avg rmse: {np.mean(rmse)}\")\n    return all_preds","23c5727d":"def prep_sub(preds: pd.DataFrame, col_name: str):\n    \"\"\"\n    This function takes an output prediction df from train_pred\n    and sets it to a format that can be submitted to the competition\n    \"\"\"\n    sub = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\n    sub[\"target\"] = preds[col_name]\n    sub.to_csv(\"submission.csv\", index=False)","61d48950":"#df = dataset.get_df()","1e534e9f":"#ridge_preds = train_pred('ridge', df, test_df, features)\n#ridge_preds","3415d825":"preds =  []\nfor i in range(10):\n    n_df = create_folds(df, num_splits=2, seed=i)\n    #print(n_df.kfold.value_counts())\n\n    ridge_preds = train_pred('ridge', n_df, test_df, features)\n    #print(ridge_preds)\n    preds.append(np.array(ridge_preds['ridge']))\n\npred_ridge = np.mean(preds,axis=0)","2a09060f":"preds_g =  []\nfor i in range(10):\n    n_df = create_folds(df, num_splits=2, seed=i)\n    #print(n_df.kfold.value_counts())\n\n    ridge_preds = train_pred('Gradiend', n_df, test_df, features)\n    #print(ridge_preds)\n    preds_g.append(np.array(ridge_preds['Gradiend']))\n\npred_gradien = np.mean(preds_g,axis=0)","d48b3a7d":"#gradien_pres = train_pred('Gradiend', df, test_df, features)\n#gradien_pres","63d0d14c":"np.mean(preds,axis=0)","bf8de333":"target = pred_gradien*0.3 + pred_ridge*0.7\ntarget","6fe863c5":"df_test = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')\n\ndf_test","7a23f22b":"df_test['target'] = target\n\ndf_test[['id','target']].to_csv('submission.csv',index = False)","eac7de99":"df_test","31c49ca8":"# Peeking at the Data","510bb74c":"# Feature Engineering","20f0297e":"# Overview\n\nThis notebook was my attempt to achieve a score comparable to baseline neural networks such as pretrained transformers with only a simple ridge regression algorithm. Even more important than the score, is the feature engineering in this notebook. \n\nThis notebook uses the readability library ([link to offline version dataset](https:\/\/www.kaggle.com\/ravishah1\/readability-package) - see how I use it below as submissions must be without internet) to generate 24 powerful traditional features. These features range from common statistics such as words per sentence to readability scoring measures such as kincaid. I also use the spacy libraries en_core_web_lg to generate 300 features. Lastly I incorporate 31 part of speech tag features using nltk. \n\nHopefully combining these features with more advanced models will help you improve your score.\n\nI hope you find these features useful. Upvote if you use these features. Comment questions and suggestions. I'll probably add more features to this notebook in the future.\n\nVersion Summary:\n\nV1-6: Incomplete versions may have errors and bugs\n\nV7: The original version\n\nV8-9: Some experimental ideas with pearson's correlation and feature transformations - currently no improvement\n\nV10\/11: Same as V7 but fixed duplicate column name bug and typos","bb43e74a":"# Modeling"}}