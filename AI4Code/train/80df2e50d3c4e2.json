{"cell_type":{"c6fe7112":"code","cee75cd7":"code","9b7b57f6":"code","f0139136":"code","0fd3e2f5":"code","4a099049":"code","7efbd800":"code","3e80ca51":"code","9bd70773":"code","f3b5f492":"code","55710246":"code","4ef0c984":"code","c8710dd8":"code","c21dbcaa":"code","0bbf6f87":"code","edb2a61c":"code","bf6d7f45":"code","6daa740a":"code","d4b6275b":"code","4ebfeb67":"code","81b96774":"code","dacf123b":"code","13d03f5c":"code","6bad5db4":"code","7f3b203e":"code","9a908fb0":"code","db03b720":"code","17c17d5e":"code","ac47b23c":"code","ce76dab5":"code","5c6cf1cd":"code","69723d34":"code","ca708740":"code","fd475395":"code","23df4480":"code","c9381104":"code","a0c88f74":"code","3a392f4f":"code","72599294":"code","b91e11cb":"code","1bd146ff":"code","fe5b1dba":"code","2483340d":"code","4dee6d8e":"code","19b7c69c":"code","4d20ce9f":"code","f4253f38":"code","07f92635":"code","2fd592ee":"code","aa425265":"code","43440cd2":"code","5fe4c426":"markdown","9822c2ff":"markdown","f3079807":"markdown","e6dae279":"markdown","1af31d9a":"markdown","06ec8a5f":"markdown","bd6025c7":"markdown","64367657":"markdown","d9f32496":"markdown","39c1122f":"markdown","f61c762d":"markdown","7ceec7f1":"markdown","983d1705":"markdown","0e2af110":"markdown","b70884b6":"markdown","e30902d0":"markdown","1f9d73f4":"markdown","08a37d74":"markdown","5032fb81":"markdown","d19f1b0c":"markdown","a04083f4":"markdown","ee45b39b":"markdown","6e9c3df8":"markdown","ec812d96":"markdown","490a62ca":"markdown","ba93c62a":"markdown","b9e83029":"markdown"},"source":{"c6fe7112":"import shap \nimport warnings  \nwarnings.filterwarnings('ignore')\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nimport os\nimport seaborn as sns\nprint(os.listdir(\"..\/input\"))\nfrom sklearn.metrics import classification_report\nimport itertools\nplt.style.use('fivethirtyeight')","cee75cd7":"df=pd.read_csv('..\/input\/diabetes.csv')","9b7b57f6":"df.head()","f0139136":"df.tail()","0fd3e2f5":"df.isna().any() # No NAs","4a099049":"print(df.dtypes)","7efbd800":"print(df.info())","3e80ca51":"df.head(10)","9bd70773":"# Calculate the median value for BMI\nmedian_bmi = df['BMI'].median()\n# Substitute it in the BMI column of the\n# dataset where values are 0\ndf['BMI'] = df['BMI'].replace(\n    to_replace=0, value=median_bmi)\n\nmedian_bloodp = df['BloodPressure'].median()\n# Substitute it in the BloodP column of the\n# dataset where values are 0\ndf['BloodPressure'] = df['BloodPressure'].replace(\n    to_replace=0, value=median_bloodp)\n\n# Calculate the median value for PlGlcConc\nmedian_plglcconc = df['Glucose'].median()\n# Substitute it in the PlGlcConc column of the\n# dataset where values are 0\ndf['Glucose'] = df['Glucose'].replace(\n    to_replace=0, value=median_plglcconc)\n\n# Calculate the median value for SkinThick\nmedian_skinthick = df['SkinThickness'].median()\n# Substitute it in the SkinThick column of the\n# dataset where values are 0\ndf['SkinThickness'] = df['SkinThickness'].replace(\n    to_replace=0, value=median_skinthick)\n\n# Calculate the median value for SkinThick\nmedian_skinthick = df['Insulin'].median()\n# Substitute it in the SkinThick column of the\n# dataset where values are 0\ndf['Insulin'] = df['Insulin'].replace(\n    to_replace=0, value=median_skinthick)","f3b5f492":"df['Outcome'].value_counts().plot.bar();","55710246":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndf['Outcome'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('target')\nax[0].set_ylabel('')\nsns.countplot('Outcome',data=df,ax=ax[1])\nax[1].set_title('Outcome')\nplt.show()","4ef0c984":"columns=df.columns[:8]\nplt.subplots(figsize=(18,15))\nlength=len(columns)\nfor i,j in itertools.zip_longest(columns,range(length)):\n    plt.subplot((length\/2),3,j+1)\n    plt.subplots_adjust(wspace=0.2,hspace=0.5)\n    df[i].hist(bins=20,edgecolor='black')\n    plt.title(i)\nplt.show()","c8710dd8":"df1=df[df['Outcome']==1]\ncolumns=df.columns[:8]\nplt.subplots(figsize=(18,15))\nlength=len(columns)\nfor i,j in itertools.zip_longest(columns,range(length)):\n    plt.subplot((length\/2),3,j+1)\n    plt.subplots_adjust(wspace=0.2,hspace=0.5)\n    df1[i].hist(bins=20,edgecolor='black')\n    plt.title(i)\nplt.show()","c21dbcaa":"\nsns.pairplot(df, hue = 'Outcome', vars = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI','DiabetesPedigreeFunction','Age'] )","0bbf6f87":"sns.jointplot(\"Pregnancies\", \"Insulin\", data=df, kind=\"reg\")","edb2a61c":"sns.jointplot(\"BloodPressure\", \"Insulin\", data=df, kind=\"reg\")","bf6d7f45":"def set_bmi(row):\n    if row[\"BMI\"] < 18.5:\n        return \"Under\"\n    elif row[\"BMI\"] >= 18.5 and row[\"BMI\"] <= 24.9:\n        return \"Healthy\"\n    elif row[\"BMI\"] >= 25 and row[\"BMI\"] <= 29.9:\n        return \"Over\"\n    elif row[\"BMI\"] >= 30:\n        return \"Obese\"","6daa740a":"df = df.assign(BM_DESC=df.apply(set_bmi, axis=1))\n\ndf.head()","d4b6275b":"def set_insulin(row):\n    if row[\"Insulin\"] >= 16 and row[\"Insulin\"] <= 166:\n        return \"Normal\"\n    else:\n        return \"Abnormal\"","4ebfeb67":"df = df.assign(INSULIN_DESC=df.apply(set_insulin, axis=1))\n\ndf.head()","81b96774":"sns.countplot(data=df, x = 'INSULIN_DESC', label='Count')\n\nAB, NB = df['INSULIN_DESC'].value_counts()\nprint('Number of patients Having Abnormal Insulin Levels: ',AB)\nprint('Number of patients Having Normal Insulin Levels: ',NB)","dacf123b":"sns.countplot(data=df, x = 'BM_DESC', label='Count')\n\nUD,H,OV,OB = df['BM_DESC'].value_counts()\nprint('Number of patients Having Underweight BMI Index: ',UD)\nprint('Number of patients Having Healthy BMI Index: ',H)\nprint('Number of patients Having Overweigth BMI Index: ',OV)\nprint('Number of patients Having Obese BMI Index: ',OB)","13d03f5c":"g = sns.FacetGrid(df, col=\"INSULIN_DESC\", row=\"Outcome\", margin_titles=True)\ng.map(plt.scatter,\"Glucose\", \"BloodPressure\",  edgecolor=\"w\")\nplt.subplots_adjust(top=1.1)","6bad5db4":"g = sns.FacetGrid(df, col=\"INSULIN_DESC\", row=\"Outcome\", margin_titles=True)\ng.map(plt.scatter,\"DiabetesPedigreeFunction\", \"BloodPressure\",  edgecolor=\"w\")\nplt.subplots_adjust(top=1.1)","7f3b203e":"g = sns.FacetGrid(df, col=\"Outcome\", row=\"INSULIN_DESC\", margin_titles=True)\ng.map(plt.hist, \"Age\", color=\"red\")\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle('Disease by INSULIN and Age');","9a908fb0":"sns.boxplot(x=\"Age\", y=\"INSULIN_DESC\", hue=\"Outcome\", data=df);","db03b720":"sns.boxplot(x=\"Age\", y=\"BM_DESC\", hue=\"Outcome\", data=df);","17c17d5e":"df1=df.drop(['Outcome'],axis=1)","ac47b23c":"plt.figure(figsize=(12,10))  # on this line I just set the size of figure to 12 by 10.\np=sns.heatmap(df1.corr(), annot=True,cmap ='RdYlGn')","ce76dab5":"X=pd.get_dummies(df)\ncols_drop=['Outcome','BM_DESC_Under']\nX=X.drop(cols_drop,axis=1)\n\ny = df['Outcome']","5c6cf1cd":"X.head()","69723d34":"y.head()","ca708740":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,stratify=y, random_state = 1234)","fd475395":"min_train = X_train.min()\nrange_train = (X_train - min_train).max()\nX_train = (X_train - min_train)\/range_train","23df4480":"min_test = X_test.min()\nrange_test = (X_test - min_test).max()\nX_test = (X_test - min_test)\/range_test","c9381104":"X_test.isna().sum()","a0c88f74":"from sklearn.ensemble import RandomForestClassifier\nModel=RandomForestClassifier(max_depth=2)\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_pred,y_test))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","3a392f4f":"train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nrfc_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)","72599294":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(rfc_model, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist(),top=13)\n","b91e11cb":"train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\ntree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_X, train_y)","1bd146ff":"features = [c for c in X.columns if c not in ['Outcome']]","fe5b1dba":"from sklearn import tree\nimport graphviz\ntree_graph = tree.export_graphviz(tree_model, out_file=None, feature_names=features)\ndisplay(graphviz.Source(tree_graph))","2483340d":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=features, feature='Glucose')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'Glucose')\nplt.show()","4dee6d8e":"feature_to_plot = 'BMI'\npdp_dist = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=features, feature=feature_to_plot)\n\npdp.pdp_plot(pdp_dist, feature_to_plot)\nplt.show()","19b7c69c":"# Build Random Forest model\nrf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n\npdp_dist = pdp.pdp_isolate(model=rf_model, dataset=val_X, model_features=features, feature=feature_to_plot)\n\npdp.pdp_plot(pdp_dist, feature_to_plot)\nplt.show()","4d20ce9f":"pdp_dist = pdp.pdp_isolate(model=rf_model, dataset=val_X, model_features=features, feature='Glucose')\n\npdp.pdp_plot(pdp_dist, 'Glucose')\nplt.show()","f4253f38":"# Similar to previous PDP plot except we use pdp_interact instead of pdp_isolate and pdp_interact_plot instead of pdp_isolate_plot\nfeatures_to_plot = ['Glucose', 'BMI']\ninter1  =  pdp.pdp_interact(model=tree_model, dataset=val_X, model_features=features, features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot,  plot_type='grid',\n                                  plot_pdp=True)\n\nplt.show()","07f92635":"import shap  # package used to calculate Shap values\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(rfc_model)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(X_train)","2fd592ee":"shap.summary_plot(shap_values, X_train)","aa425265":"shap.summary_plot(shap_values, X_test)","43440cd2":"shap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], X_train.iloc[:,1:10])","5fe4c426":"Here is how to calculate and show importances with the eli5 library:","9822c2ff":"# Train Test Split\nThe data we use is usually split into training data and test data. The training set contains a known output and the model learns on this data in order to be generalized to other data later on. We have the test dataset (or subset) in order to test our model\u2019s prediction on this subset.\n\n![zscore](https:\/\/i.ibb.co\/7G2RMgX\/TRAIN.png)\n\n**Stratify property in train test split**\nThis stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify.\nFor example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y will make sure that your random split has 25% of 0's and 75% of 1's.","f3079807":"# Applying Machine Learning Model\nNow comes the most interesting part applying machine learning model.In this step I will fit Random Forest Machine Learning Model.\n\n## Random Forest: \nA random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.RandomForestClassifie\n\nThe sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).","e6dae279":"# Data Normalization\nI have used Z-score normalization.\nZ-scores are linearly transformed data values having a mean of zero and a standard deviation of 1.\nZ-scores are also known as standardized scores; they are scores (or data values) that have been given a common standard.\n\nIf the population mean and population standard deviation are known, the standard score of a raw score x[1] is calculated as\n\n![zscore](https:\/\/i.ibb.co\/6wGCbbQ\/z-score-formula.jpg)","1af31d9a":"# Histogram\nIn this step we will analyze histogram of all the independent variables","06ec8a5f":"As guidance to read the tree:\n\n- Leaves with children show their splitting criterion on the top\n- The pair of values at the bottom show the count of True values and False values for the target respectively, of data points in that node of the tree.\n\n# PDP Box","bd6025c7":"# About dataset\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database.\n\nIn particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n# Importing Essential Libraries ","64367657":"In the above plot, it is evident that when the BMI value is between 28 and 42 it has very little impact on predictions. But when BMI value is beyond 42 it substantially increases the chance of diabetes. \n\nThis graph seems too simple to represent reality. But that's because the model is so simple. You should be able to see from the decision tree above that this is representing exactly the model's structure.\n\nYou can easily compare the structure or implications of different models. Here is the same plot with a Random Forest model.","d9f32496":"**Pearson\u2019s correlation coefficient** is the test statistics that measures the statistical relationship, or association, between two continuous variables.  It is known as the best method of measuring the association between variables of interest because it is based on the method of covariance.  It gives information about the magnitude of the association, or correlation, as well as the direction of the relationship.\n\nThe value of Pearson's Correlation Coefficient can be between -1 to +1. 1 means that they are highly correlated and 0 means no correlation.\n\nA heat map is a two-dimensional representation of information with the help of colors. Heat maps can help the user visualize simple or complex information.","39c1122f":"# Segregating Feature & Target Variable","f61c762d":"# Data Cleaning & Data Preparation\nIn this step we will find missing entries, if there then fill them with median or mean values, checking data types of all the features to find any inconsistency.","7ceec7f1":"# SHAP Values\nYou've seen (and used) techniques to extract general insights from a machine learning model. But what if you want to break down how the model works for an individual prediction?\n\nSHAP Values (an acronym from SHapley Additive exPlanations) break down a prediction to show the impact of each feature. Where could you use this?\n\n- A model says a bank shouldn't loan someone money, and the bank is legally required to explain the basis for each loan rejection\n- A healthcare provider wants to identify what factors are driving each patient's risk of some disease so they can directly address those risk factors with targeted health interventions\n\nYou'll use SHAP Values to explain individual predictions.\n\n### How They Work\nSHAP values interpret the impact of having a certain value for a given feature in comparison to the prediction we'd make if that feature took some baseline value.\n\n![](https:\/\/i.ibb.co\/VQt0KKs\/shap-diagram.png)\n\nThis section is based on this [course](!https:\/\/www.kaggle.com\/dansbecker\/shap-values) on kaggle.\n","983d1705":"# Feature Importance","0e2af110":"It seems from the above table that there are zero entries in BMI, Blood Pressure,Glucose, Skin Thickness and Insulin which are meaningless so we will fill it with their median values before fitting it into the machine learning models.\n\nReplacing zero entries BMI, Blood Pressure,Glucose, Skin Thickness and Insulin with their median values","b70884b6":"# Feature Engineering\nNow, its time to add important features to the dataset discover some effective features before fitting it into machine learning models.\n\n**Feature 1 : BMI Descriptor**\nI m adding BMI Descriptor feature as we know : If you have a BMI of:\n\n- Under 18.5 \u2013 you are considered underweight and possibly malnourished.\n- 18.5 to 24.9 \u2013 you are within a healthy weight range for young and middle-aged adults.\n- 25.0 to 29.9 \u2013 you are considered overweight.\n- Over 30 \u2013 you are considered obese.","e30902d0":"# Partial Dependence Plots\nWhile feature importance shows what variables most affect predictions, partial dependence plots show how a feature affects predictions.\n\nThis is useful to answer questions like:\n\n- Controlling for all other house features, what impact do longitude and latitude have on home prices? To restate this, how would similarly sized houses be priced in different areas?\n\n- Are predicted health differences between two groups due to differences in their diets, or due to some other factor?\n\nIf you are familiar with linear or logistic regression models, partial dependence plots can be interepreted similarly to the coefficients in those models. Though, partial dependence plots on sophisticated models can capture more complex patterns than coefficients from simple models.\n\nLike permutation importance, partial dependence plots are calculated after a model has been fit. The model is fit on real data that has not been artificially manipulated in any way.\n\n[Credit](https:\/\/www.kaggle.com\/learn\/machine-learning-explainability)\n\nFor the sake of explanation, I use a Decision Tree which you can see below.","1f9d73f4":"**Feature 2:** Insulin Indicative Range \nIf insulin level (2-Hour serum insulin (mu U\/ml)) is >= 16 and <= 166, then it is **normal** range else it is considered as **Abnormal**","08a37d74":"I have taken X as Feature variable and y as target variable.","5032fb81":"It seems from the above plot that more than 500 patients have Abnormal Insulin Levels where as around 250 patients have Normal Insulin Levels.","d19f1b0c":"A few items are worth pointing out as you interpret this plot\n\nThe y axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value.\n- A blue shaded area indicates level of confidence\n- From this particular graph, we see that increasing level of **Glucose** substantially increases the chances of having **\"Diabetes\"**. But Glucose level beyond 160  appear to have little impact on predictions.\n\nHere is another example plot:","a04083f4":"It seems from the above plot that patients having normal insulin levels are more diabetic within the age range from 25 and 42 where as patients having anormal insulin levels are more diabetic in the age range of late 20's to mid 40's.","ee45b39b":"# Data Visualization","6e9c3df8":"# 2D Partial Dependence Plots\nIf you are curious about interactions between features, 2D partial dependence plots are also useful. An example may clarify what this.\n\nWe will again use the Decision Tree model for this graph. It will create an extremely simple plot, but you should be able to match what you see in the plot to the tree itself.","ec812d96":"From the above plot it is evident that patients who are obese as per BMI index are more diabetic in early age of 25 where as patients who are overweight are prone to diabetes in early 30's\n\n# Plotting Correlation Plot (Heat Map)","490a62ca":"Here is the code to create the Partial Dependence Plot using the [PDPBox library](!https:\/\/pdpbox.readthedocs.io\/en\/latest\/).","ba93c62a":"# Histogram of Diabetic Cases","b9e83029":"# Findings from the above graph\n\n- As you move down the top of the graph, the importance of the feature decreases.\n- The features that are shown in green indicate that they have a positive impact on our prediction\n- The features that are shown in white indicate that they have no effect on our prediction\n- The features shown in red indicate that they have a negative impact on our prediction"}}