{"cell_type":{"7d6c8d1d":"code","09bc9797":"code","562ca353":"code","7a7dc263":"code","012ba103":"code","d1d0caab":"code","a339adf9":"code","624881c2":"code","c4e35968":"code","a1804355":"code","7410a042":"code","e86774b3":"code","f3ae411d":"code","ced146ba":"code","968cb730":"code","27a3eece":"code","2ea69071":"code","32858476":"code","78789a60":"code","948eb06b":"markdown","07c0bf24":"markdown","e9bafb20":"markdown","404e12ab":"markdown","53f7203c":"markdown","82074ef8":"markdown","906ac908":"markdown","f1f42d6f":"markdown","b0da5cec":"markdown","d5843b86":"markdown","74713f0e":"markdown","ea8f682a":"markdown","bfd0f5a4":"markdown","5d7e3c59":"markdown","faec6d49":"markdown","fd58f787":"markdown","3b471ced":"markdown","f0b0ebfa":"markdown","5f4d75fe":"markdown","038e8134":"markdown","78d4743c":"markdown","3aea35ba":"markdown","01f61bf2":"markdown"},"source":{"7d6c8d1d":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport time\nimport keras.backend as K\nfrom keras.layers import Input, Lambda, Conv2D, Subtract, Add, Dropout, LeakyReLU, Concatenate\nfrom keras import Model\nfrom matplotlib import pyplot as plt \nimport matplotlib.image as mpimg\nfrom matplotlib.pyplot import figure\nimport keras\nimport math\nimport cv2","09bc9797":"img = cv2.imread('..\/input\/adfawefwafaf\/normalizing_flows.jpeg')\nfigure(figsize=(10, 10), dpi=200)\nimgplot = plt.imshow(img)","562ca353":"m = 1024 # number of training examples, has to be multiple of batch size for my custom padding layer to work\nL = 8 # lattice size\nshape = (m,L,L,1)","7a7dc263":"mu = 0. # mean of all sites of all examples\nsigma = 1. # standard deviation of all sites of all examples\n\nX = tf.random.normal(shape, mean = mu, stddev = sigma, dtype = tf.float32)\n\nX = np.array(X)","012ba103":"X_frame = pd.DataFrame(np.reshape(X,(m, L*L)))","d1d0caab":"f = plt.figure(figsize=(8, 8))\nplt.matshow(X_frame.corr(), fignum=f.number)\nplt.xticks(range(X_frame.select_dtypes(['number']).shape[1]), X_frame.select_dtypes(['number']).columns, fontsize=5, rotation=0)\nplt.yticks(range(X_frame.select_dtypes(['number']).shape[1]), X_frame.select_dtypes(['number']).columns, fontsize=5)\n#cb = plt.colorbar()\n#cb.ax.tick_params(labelsize=14)\nplt.title('Correlation Matrix', fontsize=16)","a339adf9":"mask = np.zeros((L,L))\nfor i in range(L):\n    for j in range(L):\n        if (i + j) % 2 == 0:\n            mask[i,j] = 1\n            \ndual_mask = 1-mask\n\nprint(mask)\nprint('\\n',dual_mask)\n\nmask = tf.constant(mask, dtype = tf.float32) # it's important to set dtype as float32 otherwise it's a double tensor\nmask = tf.reshape(mask, shape = (1, L, L, 1))                                         # (instead of a float tensor) by default and will lead to error later\n    \n    \ndual_mask = tf.constant(dual_mask, dtype = tf.float32)\ndual_mask = tf.reshape(dual_mask, shape = (1, L, L, 1))","624881c2":"# the argument x is a tf tensor\ndef circular_padding(kernel_size): # kernel_size has to be an odd number\n    def pad(x):\n        # input: x is of shape (mm, siz, siz, filter_num)\n        # output: result is of shape (mm, siz+2*thic, siz+2*thic, filter_num)\n    \n        mm, siz, _, filter_num=x.shape # x has shape (m,hidden_size,hidden_size,filter_number)\n        thic=int((kernel_size-1)\/2) # kernel_size has to be an odd number\n        \n        top_left=tf.slice(x,[0, siz-thic, siz-thic, 0], [mm, thic, thic, filter_num])\n        top_row=tf.slice(x,[0, siz-thic, 0, 0], [mm, thic, siz, filter_num])\n        top_right=tf.slice(x,[0, siz-thic, 0, 0], [mm, thic, thic, filter_num])\n        left_column=tf.slice(x,[0, 0, siz-thic, 0], [mm, siz, thic, filter_num])\n        right_column=tf.slice(x,[0, 0, 0, 0], [mm, siz, thic, filter_num])\n        bottom_left=tf.slice(x,[0, 0, siz-thic, 0], [mm, thic, thic, filter_num])\n        bottom_row=tf.slice(x,[0, 0, 0, 0], [mm, thic, siz, filter_num])\n        bottom_right=tf.slice(x,[0, 0, 0, 0], [mm, thic, thic, filter_num])\n        \n        result1=tf.concat([top_left, top_row, top_right], axis=2) \n        result2=tf.concat([left_column, x, right_column], axis=2) \n        result3=tf.concat([bottom_left, bottom_row, bottom_right], axis=2) \n        result=tf.concat([result1, result2, result3], axis=1)\n        \n        return result\n    return pad","c4e35968":"inp=[[1, 2, 3],[4, 5, 6],[7, 8, 9]]\ninp=np.array(inp)\ninp=np.reshape(inp,(1, 3, 3, 1))\ninp=tf.constant(inp)\n\nres=circular_padding(5)(inp)\nprint(res.shape)\n\nres=tf.reshape(res, (7, 7))\nprint(type(res))\nprint('\\n',res)","a1804355":"def slice0(x):\n    return tf.slice(x, [0, 0, 0, 0], [x.shape[0], x.shape[1], x.shape[2], 1])\n\ndef slice1(x):\n    return tf.slice(x, [0, 0, 0, 1], [x.shape[0], x.shape[1], x.shape[2], 1])\n\ndef multi(lis): #lis=[a, b]\n    a, b = lis\n    return a * b\n\ndef get_get(minus_mask):\n    def get_ln_J(s): # s has shape (mm, L, L, 1)\n        ln_J = tf.math.reduce_sum(minus_mask * s, [1,2,3]) # don't sum the 0-th dimension \n        ln_J = tf.reshape(ln_J, (s.shape[0], 1))\n        return ln_J # ln_J has shape (mm, 1)\n    return get_ln_J\n\npadding_layer = Lambda(circular_padding(3)) # we create these instantiations of layers here because they don't have trainable parameters\nleakyrelu_layer = LeakyReLU() # we create these instantiations of layers here because they don't have trainable parameters\nmultiply_layer = Lambda(multi) # we create these instantiations of layers here because they don't have trainable parameters\nslice0_layer = Lambda(slice0)\nslice1_layer = Lambda(slice1)\nadd_layer = Add()","7410a042":"class affine_layer(tf.keras.layers.Layer):\n    def __init__(self, plus_mask, minus_mask): \n        \n        super(affine_layer, self).__init__() \n        \n        self.plus_mask = plus_mask\n        self.minus_mask = minus_mask\n        self.conv2d_a = Conv2D(8, 3, padding='valid', activation = None) # we instantiate these layers here we want new conv2d layers with different weights\n        self.conv2d_b = Conv2D(8, 3, padding='valid', activation = None)\n        self.conv2d_c = Conv2D(2, 3, padding='valid', activation = 'tanh')\n    \n    def call(self, input_tensor, training=True):\n        \n        x_frozen = multiply_layer([self.plus_mask, input_tensor])\n        x_active = multiply_layer([self.minus_mask, input_tensor])\n\n        x = padding_layer(x_frozen)\n        x = self.conv2d_a(x)\n        x = leakyrelu_layer(x)\n        \n        x = padding_layer(x)\n        x = self.conv2d_b(x)\n        x = leakyrelu_layer(x)\n        \n        x = padding_layer(x)\n        x = self.conv2d_c(x)\n        \n        s = slice0_layer(x)\n        t = slice1_layer(x)\n        \n        #output_tensor = (1 - mask) * t + x_active * tf.exp(s) + x_frozen\n        output1 = multiply_layer([self.minus_mask, t])\n        output2 = multiply_layer([x_active, K.exp(s)]) # inside a layer we need to use backend.exp instead of tf.exp for the back-propagation to work properly\n        output_tensor = add_layer([output1, output2])\n        output_tensor = add_layer([output_tensor, x_frozen])\n        \n        ln_J = Lambda(get_get(self.minus_mask))(s)\n        \n        return output_tensor, ln_J # J here is the determinant of the forward Jacobian matrix  ","e86774b3":"t1 = time.time()\nbbb = affine_layer(dual_mask, mask)\noutput_tensor, ln_J = bbb(X)\nt2 = time.time()\nprint(t2 - t1)\nprint(X.shape)\nprint(output_tensor.shape)\nprint(ln_J.shape)","f3ae411d":"# Create a custom loss function as defined in the Phiala papaer\ndef custom_loss(_, outputs): # y_predict=[ln_q, ln_p]\n    ln_q = tf.slice(outputs, [0, 0], [outputs.shape[0], 1])\n    ln_p = tf.slice(outputs, [0, 1], [outputs.shape[0], 1]) \n    return K.mean(ln_q - ln_p)  ","ced146ba":"const = math.log(math.sqrt(2 * math.pi)) + math.log(sigma)\n\ndef get_ln_prior(X):\n    # input here is just a np array not tf tensor\n    ln_prior = - (X - mu) * (X - mu)\/2\/sigma\/sigma - const\n    ln_prior = np.sum(ln_prior, axis=(1, 2, 3))\n    ln_prior = np.reshape(ln_prior, (X.shape[0], 1))\n    return ln_prior","968cb730":"def ScalarPhi4Action(M2, lam):\n    def act(X):\n        # potential term, \n        action_density = M2 * X ** 2 + lam * X ** 4\n        # kinetic term (discrete Laplacian)\n        Nd = len(X.shape) - 2\n        dims = range(1, Nd + 1)\n        for mu in dims:\n            action_density += 2 * X ** 2\n            action_density -= X * tf.roll(X, -1, mu)\n            action_density -= X * tf.roll(X, 1, mu)\n            result = K.sum(action_density, axis = tuple(dims))\n        return result\n    return act","27a3eece":"batch_size = 1024 # batch_size has to be dividable by total number of training examples\n##########################################################\ninput_X = Input(shape = (L, L, 1,), batch_size = batch_size) # need to specify batch_size, otherwise the shape would be (None, L, L, 1) and custom padding layer won't work\ninput_ln_prior = Input(shape = (1,), batch_size = batch_size)\n\npropagate = input_X\nln_q = input_ln_prior\n##########################################################\nfor i in range(8):\n\n    propagate, ln_J = affine_layer(mask, dual_mask)(propagate)   \n    ln_q = ln_q - ln_J\n\n    propagate, ln_J = affine_layer(dual_mask, mask)(propagate)   \n    ln_q = ln_q - ln_J\n###########################################################\nln_p = - Lambda(ScalarPhi4Action(-4, 8))(propagate)\noutputs = Concatenate(axis = -1)([ln_q, ln_p])\n###########################################################\nmodel = Model(inputs = [input_X, input_ln_prior], outputs = outputs)\nmodel.compile(optimizer = keras.optimizers.Adam(learning_rate = 0.0003), loss = custom_loss) # learning_rate = 0.001 in the original paper","2ea69071":"ver = None\nfor i in range(30001):\n    X = tf.random.normal((batch_size, L, L, 1), mean = mu, stddev = sigma, dtype = tf.float32)\n    ln_prior = get_ln_prior(X)\n    if i % 1000 == 0:\n        ver = 1\n        print(i)\n    else:\n        ver = 0\n    # The second argument of fit is X, this is actually a placeholder for Y, since our custom loss function doesn't need a Y.\n    model.fit([X, ln_prior], X, batch_size = batch_size, epochs = 1, verbose = ver) # The second argument is fed to the first argument of custom_loss. It's useless for custom_loss.","32858476":"inp = model.input                                           # input placeholder\noutp = model.layers[-4].output                              # the fourth last layer not the last layer, since the last layer is cancat[ln_q, ln_p]\nget_configurations = K.function(inp, outp)                  # evaluation functions\n\ninp = model.input                                           # input placeholder\noutp = model.layers[-3].output                              # the third last layer\nget_ln_q = K.function(inp, outp)                            # evaluation functions\n\ninp = model.input                                           # input placeholder\noutp = model.layers[-2].output                              # the third last layer\nget_ln_p = K.function(inp, outp)                            # evaluation functions\n\nX = tf.random.normal((64 * 1024, L, L, 1), mean = mu, stddev = sigma, dtype = tf.float32)\nln_prior = get_ln_prior(X)\n\nln_q_new = get_ln_q([X, ln_prior])\nln_q_new = np.array(ln_q_new)\nln_q_new = np.reshape(ln_q_new, (ln_q_new.shape[0],))\n\nln_p_new = get_ln_p([X, ln_prior])\nln_p_new = np.array(ln_p_new)\nln_p_new = np.reshape(ln_p_new, (ln_p_new.shape[0],))","78789a60":"slope, b = np.polyfit(-ln_q_new, -ln_p_new, 1)\nprint(slope, b, ln_p_new.shape, '\\n')\nplt.title(\"Normalizing_Flow_Scalar_phi4\") \nplt.xlabel(\"S_model\") \nplt.ylabel(\"S_physical\") \n#plt.figure(figsize = (8, 8))    \nxmin = 5\nxmax = 35\npl = b \nplt.hist2d(-ln_q_new, -ln_p_new, bins = 30, range=[[xmin, xmax], [xmin + pl, xmax + pl]]) \n#plt.hist2d(-ln_q_new, -ln_p_new, bins = 30) \nplt.show()","948eb06b":"# 4. Train the model.","07c0bf24":"**Now we are ready to define the affine layer.**","e9bafb20":"Plot correlation of features to see if they are really independent.","404e12ab":"# 2. Now we need to define an affine layer. <br>\n**phi_1^'=exp(s(phi_2))phi_1+t(phi_2)** <br>\n**phi_2^'=phi_2** <br>\n\nAccording to the paper, the input layer has depth 1, the first hidden layer of the Convolutional Neural Network has 8 filters, the second hidden layers has 8 filters, the output layer has depth 2. We hard-code these things here.","53f7203c":"Test circular padding.","82074ef8":"Shape of the features. <br>\n**m has to be multiple of batch_size for my custom padding layer to work**","906ac908":"# The whole notebook is an implementation of the paper \"arXiv:2101.08176v1\". In the paper they used pytorch, here we use tensorflow and keras. ","f1f42d6f":"Define the function that calculates the physical action, aka the label Y. **This function is copied from the Phiala paper \"Introduction to Normalizing Flows for Lattice Field Theory\".** This function will later be wrapped in a Lambda layer.","b0da5cec":"Convert to pandas dataframe of shape (m, L*L).","d5843b86":"# Warning: this notebook will not work on all versions of Keras and TensorFlow. If it doesn't work on your local computer, then you could register a free Kaggle account and run this notebook on Kaggle. ","74713f0e":"Calculate the ln(probability) for each configuration, the probability is pior distribution probability. The result X_logp is an array of shape (m,1).<br>\n**prob=\\Product{exp(-(x-mu)^2\/2\/sigma^2)\/sqrt(2 pi)\/sigma}** <br>\n**ln(prob)=\\Sum{-(x-mu)^2\/2\/sigma^2}+const** <br>\n**-S_physical=ln(prior)-ln_J+const** <br>\n, where the Product and Sum is over all L*L components.","ea8f682a":"Define custom cost function, namely, **KL Divergence**.","bfd0f5a4":"# 3. Define our model.","5d7e3c59":"**According to the original paper:**<br>\n**S_physics = S_effective - 8.64** <br>\n**-ln_p = -ln_q - 8.64** <br>\n**loss = ln_q - ln_p = -8.64** <br>\n**Ideally the loss function should be -8.64.**","faec6d49":"Define some functions to be later wrapped in Lambda layers.","fd58f787":"Define a function that returns the end result configuration. mm=batch_size because we specified batch_size=batch_size when we built the model. <br>\nDefine a function that returns ln_q. <br>\nDefine a function that returns ln_p.","3b471ced":"**Ideally the 2d histogram plot should be a straight line with slope = 1 and intercepts with y-axis at -ln(Z), where Z is the partition function. As you can see, our final result is pretty good.** ","f0b0ebfa":"# 1. Draw samples from I.I.D normal distribution. The output shape is (m,L,L) where m is the number of training examples and L is lattice size.","5f4d75fe":"Test the affine layer.","038e8134":"# 5. Evaluate the model.","78d4743c":"Create a mask with checkboard pattern.","3aea35ba":"First need to define circular padding function since it's not available in Keras. This will later be wrapped with a Lambda layer.","01f61bf2":"Define a function that gets the log of the prior probability."}}