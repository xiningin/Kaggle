{"cell_type":{"b23ace96":"code","dd41b7df":"code","e9bf1474":"code","4fc5ed00":"code","46ee606e":"code","2643a1d0":"code","f3529269":"code","7b40f354":"code","3fd8a872":"code","661653af":"code","85e2ab83":"code","638d30d2":"code","13dc83d3":"code","00f846f9":"code","ae220e40":"code","47b87507":"code","e1eaca5e":"code","08c8338e":"code","fbe1274c":"code","172faf76":"code","5245d6f3":"code","c68ab71a":"code","4228cfea":"code","baf953be":"code","aa5bdd3d":"code","05f9c1cc":"code","097543f0":"code","19d18757":"code","1358ca12":"code","e92c84f4":"markdown","0f4717d4":"markdown","c18f1d77":"markdown","7d8d12a3":"markdown","84f7ccac":"markdown","35a41e72":"markdown"},"source":{"b23ace96":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dd41b7df":"sms = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\",encoding='latin-1')","e9bf1474":"sms.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)\nsms.head()","4fc5ed00":"sms = sms.rename(columns = {'v1' : 'label', 'v2':'message'})\nsms.head()","46ee606e":"sms.label.value_counts()","2643a1d0":"sms.isnull().any()","f3529269":"#Converting label to numeric variable\nsms['label'] = sms.label.map({'ham':0,'spam':1})\nsms.head()","7b40f354":"#splitting data into train and test\nx = sms.message\ny = sms.label\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_x, test_x, train_y, test_y = train_test_split(x,y)\nprint(train_x.shape)\nprint(test_x.shape)\nprint(train_y.shape)\nprint(test_y.shape)","3fd8a872":"from sklearn.feature_extraction.text import CountVectorizer\n\n#instantiate the vector\nvect = CountVectorizer(max_df=0.90,min_df = 0.001,stop_words='english',strip_accents = 'unicode')","661653af":"#learn training data vocabulary and convert it into document term matrix\ntrain_x_dtm = vect.fit_transform(train_x)\ntrain_x_dtm","85e2ab83":"#Now transform testing data(using fitted vocabulary) into jdocument term matrix\ntest_x_dtm = vect.transform(test_x)\ntest_x_dtm","638d30d2":"#Store Token names\ntrain_x_token = vect.get_feature_names()\nprint(train_x_token[-50:])","13dc83d3":"#view train_x_dtm matrix\ntrain_x_dtm.toarray()","00f846f9":"train_x_token_count = np.sum(train_x_dtm.toarray(),axis=0)\ntrain_x_token_count","ae220e40":"#Now create a dataframe of tokens with their counts\n\npd.DataFrame({'token': train_x_token,'count':train_x_token_count})","47b87507":"#Create seperate data frames for ham and spam\nsms_ham = sms[sms.label==0]\nsms_spam = sms[sms.label==1]","e1eaca5e":"#learn the vocabulary of all messages\nvect.fit(sms.message)\ntokens = vect.get_feature_names()","08c8338e":"#create document term matrix for ham and spam\nham_dtm = vect.transform(sms_ham.message)\nspam_dtm = vect.transform(sms_spam.message)","fbe1274c":"#Count the token of ham messages \nham_token_count = np.sum(ham_dtm.toarray(), axis=0)\nspam_token_count = np.sum(spam_dtm.toarray(),axis=0)","172faf76":"#create dataframe of token with seperate ham and spam counts\ndf = pd.DataFrame({'token': tokens,'ham_count': ham_token_count, 'spam_count': spam_token_count})\ndf","5245d6f3":"#lets add 1 to ham and spam count to avoid dividing by zero\ndf['ham_count'] = df['ham_count'] + 1\ndf['spam_count'] = df['spam_count'] + 1","c68ab71a":"df['spam_ratio'] = df['spam_count']\/df['ham_count']\ndf.sort_values('spam_ratio')","4228cfea":"#The multinomial Naive Bayes classifier is suitable for classification with discrete features\n\nfrom sklearn.naive_bayes import MultinomialNB\n\nnb = MultinomialNB()\nnb.fit(train_x_dtm,train_y)","baf953be":"# make class predictions for test_x_dtm\npred_y = nb.predict(test_x_dtm)","aa5bdd3d":"#calculating accuracy\nfrom sklearn import metrics\n\nprint(metrics.accuracy_score(test_y,pred_y))","05f9c1cc":"#Confusion matrix\n\ncm = metrics.confusion_matrix(test_y, pred_y)\ncm","097543f0":"#ROC score\nprint(metrics.roc_auc_score(test_y, pred_y))","19d18757":"# print message text for the false positives\ntest_x[test_y < pred_y]","1358ca12":"# print message text for the false negatives\ntest_x[test_y > pred_y]","e92c84f4":"## Agenda\n\n**Working with text data**\n\n- Representing text as data\n- Reading SMS data\n- Vectorizing SMS data\n- Examining the tokens and their counts\n- Bonus: Calculating the \"spamminess\" of each token\n\n**Naive Bayes classification**\n\n- Building a Naive Bayes model\n- Comparing Naive Bayes with logistic regression","0f4717d4":"## Vectorization of Data\n#### We will use Countvectorizer , it is used to convert text into matrix of token counts","c18f1d77":"## Examining the tokens and their counts","7d8d12a3":"## Building Naive Bayesian Model","84f7ccac":"## Reading the SMS Data","35a41e72":"## Calculate the Spaminess of each token"}}