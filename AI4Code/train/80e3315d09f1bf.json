{"cell_type":{"a08a5377":"code","efd028ad":"code","8d46e391":"code","cf781cdb":"code","318f7e9d":"code","12ef6f72":"code","27401070":"code","5ed0baa7":"code","7ff7d7c6":"code","e54dbbc1":"code","802f5843":"code","b1940b2a":"code","2c1d2d9b":"code","a60b4b56":"code","6f8ba2f4":"code","11b4a3a6":"code","01d674fc":"code","f77c864c":"code","5e484c79":"code","e68e23d2":"code","889e63d9":"code","9dbcd239":"code","cc3169d7":"code","bbab5770":"code","be21fa84":"code","a2f30b9d":"code","3b032047":"code","c6035d5e":"code","14c3f692":"code","883f438c":"code","2c6a4e80":"code","01cd2a24":"code","a0f8b9c2":"code","af3451d8":"code","0ce344eb":"code","1e929d23":"code","4e663144":"markdown","d1fab18f":"markdown","11bf10a6":"markdown","a4e38e37":"markdown","b0634f8e":"markdown","d824448f":"markdown","5628ae99":"markdown","63b96968":"markdown","42168dae":"markdown","55f3afaa":"markdown","1e375561":"markdown","bebc72b3":"markdown","884bb395":"markdown","2dbca9de":"markdown","3ea856eb":"markdown","4535afd2":"markdown","b9dabfb6":"markdown","a5bd7e52":"markdown","fb5f3e93":"markdown","9e652728":"markdown","a3e1dfb0":"markdown","71434b6a":"markdown","074f23ce":"markdown","fceee174":"markdown"},"source":{"a08a5377":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.datasets import load_files\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC","efd028ad":"# # Uncomment this to load data from downloaded tar.gz archive.\n# # change this path\n# PATH_TO_IMDB = '\/Users\/y.kashnitsky\/Documents\/Machine_learning\/datasets\/aclImdb\/'\n# reviews_train = load_files(os.path.join(PATH_TO_IMDB, \"train\"),\n#                            categories=['pos', 'neg'])\n# text_train, y_train = reviews_train.data, reviews_train.target\n# # change the path to the file\n# reviews_test = load_files(os.path.join(PATH_TO_IMDB, \"test\"),\n#                           categories=['pos', 'neg'])\n# text_test, y_test = reviews_test.data, reviews_test.target","8d46e391":"# Alternatively, load data from previously pickled objects. \nimport pickle\nPATH_TO_DATA = '..\/input\/spooky-vw-tutorial'\nwith open(os.path.join(PATH_TO_DATA, 'reviews_train.pkl'), 'rb') as reviews_train_pkl:\n    reviews_train = pickle.load(reviews_train_pkl)\ntext_train, y_train = reviews_train.data, reviews_train.target\nwith open(os.path.join(PATH_TO_DATA, 'reviews_test.pkl'), 'rb') as reviews_test_pkl:\n    reviews_test = pickle.load(reviews_test_pkl)\ntext_test, y_test = reviews_test.data, reviews_test.target","cf781cdb":"print(\"Number of documents in training data: %d\" % len(text_train))\nprint(np.bincount(y_train))\nprint(\"Number of documents in test data: %d\" % len(text_test))\nprint(np.bincount(y_test))","318f7e9d":"print(text_train[1])","12ef6f72":"y_train[1] # bad review","27401070":"text_train[2]","5ed0baa7":"y_train[2] # good review","7ff7d7c6":"cv = CountVectorizer()\ncv.fit(text_train)\n\nlen(cv.vocabulary_)","e54dbbc1":"print(cv.get_feature_names()[:50])\nprint(cv.get_feature_names()[50000:50050])","802f5843":"X_train = cv.transform(text_train)\nX_train","b1940b2a":"print(text_train[19726])","2c1d2d9b":"X_train[19726].nonzero()[1]","a60b4b56":"X_train[19726].nonzero()","6f8ba2f4":"X_test = cv.transform(text_test)","11b4a3a6":"%%time\nlogit = LogisticRegression(solver='lbfgs', n_jobs=-1, random_state=7)\nlogit.fit(X_train, y_train)","01d674fc":"round(logit.score(X_train, y_train), 3), round(logit.score(X_test, y_test), 3),","f77c864c":"def visualize_coefficients(classifier, feature_names, n_top_features=25):\n    # get coefficients with large absolute values \n    coef = classifier.coef_.ravel()\n    positive_coefficients = np.argsort(coef)[-n_top_features:]\n    negative_coefficients = np.argsort(coef)[:n_top_features]\n    interesting_coefficients = np.hstack([negative_coefficients, positive_coefficients])\n    # plot them\n    plt.figure(figsize=(15, 5))\n    colors = [\"red\" if c < 0 else \"blue\" for c in coef[interesting_coefficients]]\n    plt.bar(np.arange(2 * n_top_features), coef[interesting_coefficients], color=colors)\n    feature_names = np.array(feature_names)\n    plt.xticks(np.arange(1, 1 + 2 * n_top_features), feature_names[interesting_coefficients], \n               rotation=60, ha=\"right\");\n","5e484c79":"def plot_grid_scores(grid, param_name):\n    plt.plot(grid.param_grid[param_name], grid.cv_results_['mean_train_score'],\n        color='green', label='train')\n    plt.plot(grid.param_grid[param_name], grid.cv_results_['mean_test_score'],\n        color='red', label='test')\n    plt.legend();\n    ","e68e23d2":"visualize_coefficients(logit, cv.get_feature_names())","889e63d9":"%%time\nfrom sklearn.pipeline import make_pipeline\n\ntext_pipe_logit = make_pipeline(CountVectorizer(),\n                                # for some reason n_jobs > 1 won't work \n                                # with GridSearchCV's n_jobs > 1\n                                LogisticRegression(solver='lbfgs', \n                                                   n_jobs=1,\n                                                   random_state=7))\n\ntext_pipe_logit.fit(text_train, y_train)\nprint(text_pipe_logit.score(text_test, y_test))","9dbcd239":"%%time\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid_logit = {'logisticregression__C': np.logspace(-5, 0, 6)}\ngrid_logit = GridSearchCV(text_pipe_logit, param_grid_logit, \n                          cv=3, n_jobs=1)\n\ngrid_logit.fit(text_train, y_train)","cc3169d7":"grid_logit.best_params_, grid_logit.best_score_","bbab5770":"plot_grid_scores(grid_logit, 'logisticregression__C')","be21fa84":"grid_logit.score(text_test, y_test)","a2f30b9d":"from sklearn.ensemble import RandomForestClassifier","3b032047":"forest = RandomForestClassifier(n_estimators=200, \n                                n_jobs=-1, random_state=17)","c6035d5e":"%%time\nforest.fit(X_train, y_train)","14c3f692":"round(forest.score(X_test, y_test), 3)","883f438c":"# creating dataset\nrng = np.random.RandomState(0)\nX = rng.randn(200, 2)\ny = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)","2c6a4e80":"plt.scatter(X[:, 0], X[:, 1], s=30, c=y, cmap=plt.cm.Paired);","01cd2a24":"def plot_boundary(clf, X, y, plot_title):\n    xx, yy = np.meshgrid(np.linspace(-3, 3, 50),\n                     np.linspace(-3, 3, 50))\n    clf.fit(X, y)\n    # plot the decision function for each datapoint on the grid\n    Z = clf.predict_proba(np.vstack((xx.ravel(), yy.ravel())).T)[:, 1]\n    Z = Z.reshape(xx.shape)\n\n    image = plt.imshow(Z, interpolation='nearest',\n                           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n                           aspect='auto', origin='lower', cmap=plt.cm.PuOr_r)\n    contours = plt.contour(xx, yy, Z, levels=[0], linewidths=2,\n                               linetypes='--')\n    plt.scatter(X[:, 0], X[:, 1], s=30, c=y, cmap=plt.cm.Paired)\n    plt.xticks(())\n    plt.yticks(())\n    plt.xlabel(r'$x_1$')\n    plt.ylabel(r'$x_2$')\n    plt.axis([-3, 3, -3, 3])\n    plt.colorbar(image)\n    plt.title(plot_title, fontsize=12);","a0f8b9c2":"plot_boundary(LogisticRegression(), X, y,\n              \"Logistic Regression, XOR problem\")","af3451d8":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline","0ce344eb":"logit_pipe = Pipeline([('poly', PolynomialFeatures(degree=2)), \n                       ('logit', LogisticRegression())])","1e929d23":"plot_boundary(logit_pipe, X, y,\n              \"Logistic Regression + quadratic features. XOR problem\")","4e663144":"## A Simple Count of Words","d1fab18f":"Obviously, one cannot draw a single straight line to separate one class from another without errors. Therefore, logistic regression performs poorly with this task.","11bf10a6":"**Now let's do the same with random forest. We see that, with logistic regression, we achieve better accuracy with less effort.**","a4e38e37":"But if one were to give polynomial features as an input (here, up to 2 degrees), then the problem is solved.","b0634f8e":"**Let's look at accuracy on the both the training and the test sets.**","d824448f":"**Secondly, we are encoding the sentences from the training set texts with the indices of incoming words. We'll use the sparse format.**","5628ae99":"**Let's print best $C$ and cv-score using this hyperparameter:**","63b96968":"Here, logistic regression has still produced a hyperplane but in a 6-dimensional feature space $1, x_1, x_2, x_1^2, x_1x_2$ and $x_2^2$. When we project to the original feature space, $x_1, x_2$, the boundary is nonlinear.\n\nIn practice, polynomial features do help, but it is computationally inefficient to build them explicitly. SVM with the kernel trick works much faster. In this approach, only the distance between the objects (defined by the kernel function) in a high dimensional space is computed, and there is no need to produce a combinatorially large number of features. ","42168dae":"**The next step is to train Logistic Regression.**","55f3afaa":"**Let's see how our transformation worked**","1e375561":"**Third, we will apply the same operations to the test set**","bebc72b3":"**Here are a few examples of the reviews.**","884bb395":"**To get started, download the dataset [here](http:\/\/ai.stanford.edu\/~amaas\/data\/sentiment\/aclImdb_v1.tar.gz) (direct download link). The dataset is briefly described [here](http:\/\/ai.stanford.edu\/~amaas\/data\/sentiment\/). There are 12.5k of good and bad reviews in the test and training sets.**","2dbca9de":"# <center>Topic 4. Linear Classification and Regression\n## <center> Part 4. Where Logistic Regression Is Good and Where It's Not","3ea856eb":"**First, we will create a dictionary of all the words using CountVectorizer**","4535afd2":"### Useful resources\n- Medium [\"story\"](https:\/\/medium.com\/open-machine-learning-course\/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220) based on this notebook\n- If you read Russian: an [article](https:\/\/habrahabr.ru\/company\/ods\/blog\/323890\/) on Habrahabr with ~ the same material. And a [lecture](https:\/\/youtu.be\/oTXGQ-_oqvI) on YouTube\n- A nice and concise overview of linear models is given in the book [\u201cDeep Learning\u201d](http:\/\/www.deeplearningbook.org) (I. Goodfellow, Y. Bengio, and A. Courville).\n- Linear models are covered practically in every ML book. We recommend \u201cPattern Recognition and Machine Learning\u201d (C. Bishop) and \u201cMachine Learning: A Probabilistic Perspective\u201d (K. Murphy).\n- If you prefer a thorough overview of linear model from a statistician\u2019s viewpoint, then look at \u201cThe elements of statistical learning\u201d (T. Hastie, R. Tibshirani, and J. Friedman).\n- The book \u201cMachine Learning in Action\u201d (P. Harrington) will walk you through implementations of classic ML algorithms in pure Python.\n- [Scikit-learn](http:\/\/scikit-learn.org\/stable\/documentation.html) library. These guys work hard on writing really clear documentation.\n- Scipy 2017 [scikit-learn tutorial](https:\/\/github.com\/amueller\/scipy-2017-sklearn) by Alex Gramfort and Andreas Mueller.\n- One more [ML course](https:\/\/github.com\/diefimov\/MTH594_MachineLearning) with very good materials.\n- [Implementations](https:\/\/github.com\/rushter\/MLAlgorithms) of many ML algorithms. Search for linear regression and logistic regression.","b9dabfb6":"**The coefficients of the model can be beautifully displayed.**","a5bd7e52":"<center>\n<img src=\"https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg\">\n## Open Machine Learning Course\n<center>Author: [Yury Kashnitsky](https:\/\/www.linkedin.com\/in\/festline). Translated and edited by [Christina Butsko](https:\/\/www.linkedin.com\/in\/christinabutsko\/), [Nerses Bagiyan](https:\/\/www.linkedin.com\/in\/nersesbagiyan\/), [Yulia Klimushina](https:\/\/www.linkedin.com\/in\/yuliya-klimushina-7168a9139), and [Yuanyuan Pao](https:\/\/www.linkedin.com\/in\/yuanyuanpao\/).\n\nThis material is subject to the terms and conditions of the license [Creative Commons CC BY-NC-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/). Free use is permitted for any non-comercial purpose with an obligatory indication of the names of the authors and of the source.\n\n<center>Original kernel of Yury Kashitskiy is [here]https:\/\/www.kaggle.com\/kashnitsky\/topic-4-linear-models-part-4-more-of-logit","fb5f3e93":"For the validation set:","9e652728":"### Analysis of IMDB movie reviews","a3e1dfb0":"**If you look at the examples of \"words\" (let's call them tokens), you can see that we have omitted many of the important steps in text processing (automatic text processing can itself be a completely separate series of articles).**","71434b6a":"### XOR-Problem\nLet's now consider an example where linear models are worse.\n\nLinear classification methods still define a very simple separating surface - a hyperplane. The most famous toy example of where classes cannot be divided by a hyperplane (or line) with no errors is \"the XOR problem\".\n\nXOR is the \"exclusive OR\", a Boolean function with the following truth table:\n\n\n\n<img src=https:\/\/habrastorage.org\/webt\/-f\/l_\/i9\/-fl_i9tktb9chjfmv7pzhqdyeea.png>\n\nXOR is the name given to a simple binary classification problem in which the classes are presented as diagonally extended intersecting point clouds.","074f23ce":"**To make our model better, we can optimize the regularization coefficient for the `Logistic Regression`. We'll use `sklearn.pipeline` because `CountVectorizer` should only be applied to the training data (so as to not \"peek\" into the test set and not count word frequencies there). In this case, `pipeline` determines the correct sequence of actions: apply `CountVectorizer`, then train `Logistic Regression`.**","fceee174":"Now for a little practice! We want to solve the problem of binary classification of IMDB movie reviews. We have a training set with marked reviews, 12500 reviews marked as good, another 12500 bad. Here, it's not easy to get started with machine learning right away because we don't have the matrix $X$; we need to prepare it. We will use a simple approach: bag of words model. Features of the review will be represented by indicators of the presence of each word from the whole corpus in this review. The corpus is the set of all user reviews. The idea is illustrated by a picture\n\n<img src=https:\/\/habrastorage.org\/webt\/r7\/sq\/my\/r7sqmyj1nmqmzltaftt40zi7-gw.png width=50%>"}}