{"cell_type":{"5cf32c26":"code","c12d15e9":"code","8db9f049":"code","231c4dd8":"code","84aa7079":"code","b66b25d2":"code","0647cad0":"code","7409dc48":"code","c61a2edc":"code","f7edfa0e":"code","3e1a0ea7":"code","a7096dc5":"code","f60066a4":"code","2890d59d":"code","f2955b20":"code","87f59529":"code","f828fc38":"code","56de8507":"code","e194d6b2":"code","b242a4c1":"code","913426d0":"code","994a4387":"code","3045876d":"code","a656fa6b":"code","6a957b55":"code","837ca271":"code","fc27d506":"code","88748fe8":"code","1b6e9dbd":"code","acf8dd4c":"code","715cf139":"code","887a8f39":"code","078437fa":"code","9b13ef48":"code","715a496d":"code","0baac804":"code","f34d9764":"code","14d64518":"code","a5c87dc0":"code","66474fce":"markdown","1c018445":"markdown","d9831560":"markdown","fb04fb2d":"markdown","fd62f3fb":"markdown","521e752b":"markdown","89d4d72f":"markdown","713824aa":"markdown","7153678e":"markdown","75795ab7":"markdown","3886d8a7":"markdown","4e9a2c53":"markdown","e69b6226":"markdown","d563217a":"markdown","5f7b4d1f":"markdown","41c45564":"markdown","1a011600":"markdown","b057081a":"markdown","3ac9c79c":"markdown","c0762966":"markdown","73cd97df":"markdown","f55ad5c4":"markdown","d133e1ad":"markdown","c19ab15f":"markdown","a09300b1":"markdown","31eec403":"markdown","f8fb628c":"markdown","9892cd5b":"markdown","8679b8e1":"markdown","47bd600a":"markdown"},"source":{"5cf32c26":"import os\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split, ShuffleSplit, KFold, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression,Ridge, RidgeCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\n\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set()\n\nimport time\nimport datetime\n\n#import shap\n# load JS visualization code to notebook\n#shap.initjs()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c12d15e9":"%%time\nPATH_TO_DATA = '..\/input\/'\n\nsample_submission = pd.read_csv(os.path.join(PATH_TO_DATA, 'sample_submission.csv'), \n                                    index_col='match_id_hash')\ndf_train_features = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_features.csv'), \n                                    index_col='match_id_hash')\ndf_train_targets = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_targets.csv'), \n                                   index_col='match_id_hash')\ndf_test_features = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_features.csv'), \n                                   index_col='match_id_hash')","8db9f049":"df_train_features.head(3)","231c4dd8":"df_test_features.head(3)","84aa7079":"df_train_targets.head(3)","b66b25d2":"print('Shape of Training set: {0}\\nShape of Test set: {1}'.format(df_train_features.shape,df_test_features.shape))","0647cad0":"target = pd.Series(df_train_targets['radiant_win'].map({True: 1, False: 0}))","7409dc48":"plt.hist(target);\nplt.title('Target distribution');","c61a2edc":"general_features = ['game_time', 'game_mode', 'lobby_type', 'objectives_len', 'chat_len']\ngen_feat_df = df_train_features[general_features].copy()\ngen_feat_df['target'] = target\nplt.figure(figsize=(8, 5));\nax = sns.heatmap(gen_feat_df.corr(),annot=True,)","f7edfa0e":"plt.figure(figsize=(8, 5));\nmask = np.zeros_like(gen_feat_df.corr())\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    ax = sns.heatmap(gen_feat_df.corr(), mask=mask,annot=True)","3e1a0ea7":"print('Top 10 features correlated with target (abs values):')\nprint(abs(df_train_features.corrwith(target)).sort_values(ascending=False)[0:10])","a7096dc5":"r_y_coord = ['r{0}_y'.format(i) for i in range(1,6)]\nr_x_coord = ['r{0}_x'.format(i) for i in range(1,6)]\nr_coord = r_y_coord+r_x_coord\n\nd_y_coord = ['d{0}_y'.format(i) for i in range(1,6)]\nd_x_coord = ['d{0}_x'.format(i) for i in range(1,6)]\nd_coord = d_y_coord+d_x_coord","f60066a4":"coord_feat_df = df_train_features[r_coord+d_coord].copy()\ncoord_feat_df['target'] = target\nplt.figure(figsize=(16, 10));\nax = sns.heatmap(coord_feat_df.corr(),annot=True,)","2890d59d":"print('Min y coordinate for Radiant: {0}'.format(coord_feat_df[r_y_coord].min(axis=0).sort_values(ascending=True)[0:1].values))\nprint('Max y coordinate for Radiant: {0}'.format(coord_feat_df[r_y_coord].max(axis=0).sort_values(ascending=False)[0:1].values)) \nprint('Min x coordinate for Radiant: {0}'.format(coord_feat_df[r_x_coord].min(axis=0).sort_values(ascending=True)[0:1].values))\nprint('Max x coordinate for Radiant: {0}'.format(coord_feat_df[r_x_coord].max(axis=0).sort_values(ascending=False)[0:1].values)) ","f2955b20":"print('Min y coordinate for Dire: {0}'.format(coord_feat_df[d_y_coord].min(axis=0).sort_values(ascending=True)[0:1].values))\nprint('Max y coordinate for Dire: {0}'.format(coord_feat_df[d_y_coord].max(axis=0).sort_values(ascending=False)[0:1].values)) \nprint('Min x coordinate for Dire: {0}'.format(coord_feat_df[d_x_coord].min(axis=0).sort_values(ascending=True)[0:1].values))\nprint('Max x coordinate for Dire: {0}'.format(coord_feat_df[d_x_coord].max(axis=0).sort_values(ascending=False)[0:1].values)) ","87f59529":"#https:\/\/stackoverflow.com\/questions\/38783027\/jupyter-notebook-display-two-pandas-tables-side-by-side\nfrom IPython.display import display_html \ndef display_side_by_side(*args):\n    html_str=''\n    for df in args:\n        html_str+=df.to_html()\n    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)","f828fc38":"display_side_by_side(coord_feat_df[coord_feat_df['target']==1].describe().T,coord_feat_df[coord_feat_df['target']==0].describe().T)","56de8507":"coord_feat_df_mean = coord_feat_df.copy()\ncoord_feat_df_mean['target'] = target\n\ncoord_feat_df_mean['r_y_mean'] = coord_feat_df_mean[r_y_coord].mean(axis=1)\ncoord_feat_df_mean['r_x_mean'] = coord_feat_df_mean[r_x_coord].mean(axis=1)\ncoord_feat_df_mean['d_y_mean'] = coord_feat_df_mean[d_y_coord].mean(axis=1)\ncoord_feat_df_mean['d_x_mean'] = coord_feat_df_mean[d_x_coord].mean(axis=1)\nmean_cols = ['r_y_mean', 'r_x_mean', 'd_y_mean', 'd_x_mean']","e194d6b2":"coord_feat_df_mean.head(3)","b242a4c1":"plt.figure(figsize=(8, 5));\nax = sns.heatmap(coord_feat_df_mean[mean_cols+['target']].corr(),annot=True,)","913426d0":"sns_plot = sns.pairplot(coord_feat_df_mean[mean_cols+['target']])\nsns_plot.savefig('pairplot.png')","994a4387":"from sklearn.manifold import TSNE","3045876d":"%%time\ntsne = TSNE(random_state=17)\ntsne_representation = tsne.fit_transform(coord_feat_df_mean[mean_cols]) #https:\/\/habr.com\/ru\/company\/ods\/blog\/323210\/","a656fa6b":"plt.scatter(tsne_representation[:, 0], tsne_representation[:, 1], \n            c=coord_feat_df_mean['target'].map({0: 'blue', 1: 'orange'}));","6a957b55":"r_kills = ['r{0}_kills'.format(i) for i in range(1,6)]\nr_deaths = ['r{0}_deaths'.format(i) for i in range(1,6)]\nr_assists = ['r{0}_assists'.format(i) for i in range(1,6)]\nr_kda = r_kills+r_deaths+r_assists\n\nd_kills = ['d{0}_kills'.format(i) for i in range(1,6)]\nd_deaths = ['d{0}_deaths'.format(i) for i in range(1,6)]\nd_assists = ['d{0}_assists'.format(i) for i in range(1,6)]\nd_kda = d_kills+d_deaths+d_assists\n\nkda_feat_df = df_train_features[r_kda+d_kda].copy()\nkda_feat_df['target'] = target\n\nkda_feat_df['r_tot_kills'] = kda_feat_df[r_kills].sum(axis=1)\nkda_feat_df['r_tot_deaths'] = kda_feat_df[r_deaths].sum(axis=1)\nkda_feat_df['r_tot_assists'] = kda_feat_df[r_assists].sum(axis=1)\n\nkda_feat_df['d_tot_kills'] = kda_feat_df[d_kills].sum(axis=1)\nkda_feat_df['d_tot_deaths'] = kda_feat_df[d_deaths].sum(axis=1)\nkda_feat_df['d_tot_assists'] = kda_feat_df[d_assists].sum(axis=1)\n\ntot_cols = ['r_tot_kills', 'r_tot_deaths', 'r_tot_assists', 'd_tot_kills', 'd_tot_deaths', 'd_tot_assists']\n\ndisplay(kda_feat_df.head(3))","837ca271":"plt.figure(figsize=(8, 5));\nax = sns.heatmap(kda_feat_df[tot_cols+['target']].corr(),annot=True,)","fc27d506":"kda_feat_df['r_kda'] = (kda_feat_df['r_tot_kills']+kda_feat_df['r_tot_assists'])\/kda_feat_df['r_tot_deaths']\nkda_feat_df['d_kda'] = (kda_feat_df['d_tot_kills']+kda_feat_df['d_tot_assists'])\/kda_feat_df['d_tot_deaths']","88748fe8":"plt.figure(figsize=(4.8, 3));\nax = sns.heatmap(kda_feat_df[['r_kda','d_kda','target']].corr(),annot=True,)","1b6e9dbd":"X = df_train_features\ny = df_train_targets['radiant_win']\nX_test = df_test_features\ny_cat = pd.Series(df_train_targets['radiant_win'].map({True: 1, False: 0})) #catboost doesn't understand True,False \n#X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=17) #for holdout, don't use in kernel\nn_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=11)\ncv = ShuffleSplit(n_splits=n_fold, test_size=0.3, random_state=17) #same as in https:\/\/www.kaggle.com\/c\/mlcourse-dota2-win-prediction\/kernels starter kernel ","acf8dd4c":"%%time\n\nmodel_rf = RandomForestClassifier(n_estimators=100, n_jobs=4,\n                                   max_depth=None, random_state=17)\n\n# calcuate ROC-AUC for each split\ncv_scores_rf = cross_val_score(model_rf, X, y, cv=cv, scoring='roc_auc')","715cf139":"%%time\n\nmodel_lgb = LGBMClassifier(random_state=17)\ncv_scores_lgb = cross_val_score(model_lgb, X, y, cv=cv, \n                                scoring='roc_auc', n_jobs=4)","887a8f39":"%%time\n\nmodel_xgb = xgb.XGBClassifier(random_state=17)\ncv_scores_xgb = cross_val_score(model_xgb, X, y, cv=cv,\n                                scoring='roc_auc', n_jobs=4)","078437fa":"%%time \nmodel_cat = CatBoostClassifier(random_state=17,silent=True)\ncv_scores_cat = cross_val_score(model_cat, X, y_cat, cv=cv,\n                                scoring='roc_auc', n_jobs=1) #pay attention n_jobs=1 here, just freezes with any other value","9b13ef48":"cv_results = pd.DataFrame(data={'RF': cv_scores_rf, 'LGB':cv_scores_lgb, 'XGB':cv_scores_xgb, 'CAT':cv_scores_cat})\ndisplay_side_by_side(cv_results, cv_results.describe())","715a496d":"#just visit https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters-Tuning.html\n#https:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\nparams = {#'num_leaves': 31, # number of leaves in full tree (31 by default) \n         'learning_rate': 0.01, #this determines the impact of each tree on the final outcome. \n\n         'min_data_in_leaf': 50,\n         'min_sum_hessian_in_leaf': 12.0,\n         'objective': 'binary', \n         'max_depth': -1,\n         'boosting': 'gbdt', #'dart' \n         'bagging_freq': 5,\n         'bagging_fraction': 0.81,\n         'boost_from_average':'false',\n         'bagging_seed': 17,\n         'metric': 'auc',\n         'verbosity': -1,\n         }","0baac804":"%%time\n# this part is based on great kernel https:\/\/www.kaggle.com\/artgor\/seismic-data-eda-and-baseline by @artgor\noof = np.zeros(len(X))\nprediction = np.zeros(len(X_test))\nscores = []\nfeature_importance = pd.DataFrame()\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    model = LGBMClassifier(**params, n_estimators = 2000, nthread = 5, n_jobs = -1)\n    model.fit(X_train, y_train, \n              eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='auc',\n              verbose=200, early_stopping_rounds=200)\n            \n    y_pred_valid = model.predict_proba(X_valid)[:, 1]\n    y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)[:, 1]\n        \n    oof[valid_index] = y_pred_valid.reshape(-1,)\n    scores.append(roc_auc_score(y_valid, y_pred_valid))\n    prediction += y_pred    \n    \n    # feature importance\n    fold_importance = pd.DataFrame()\n    fold_importance[\"feature\"] = X.columns\n    fold_importance[\"importance\"] = model.feature_importances_\n    fold_importance[\"fold\"] = fold_n + 1\n    feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\nprediction \/= n_fold","f34d9764":"print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \nfeature_importance[\"importance\"] \/= n_fold\n    \ncols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n            by=\"importance\", ascending=False)[:50].index\n\nbest_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\nplt.figure(figsize=(14, 16));\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\nplt.title('LGB Features (avg over folds)');","14d64518":"lgb = LGBMClassifier(random_state=17)\nlgb.fit(X, y)\n\nX_test = df_test_features.values\ny_test_pred = lgb.predict_proba(X_test)[:, 1]\ndf_submission = pd.DataFrame({'radiant_win_prob': y_test_pred}, \n                                 index=df_test_features.index)\nsubmission_filename = 'lgb_{}.csv'.format(\n    datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\ndf_submission.to_csv(submission_filename)\nprint('Submission saved to {}'.format(submission_filename))","a5c87dc0":"df_submission.head() #just to check that everything allright ","66474fce":"# Main data exploration\n<div id=\"maindata\">\n<\/div>\n\nIn this part I'll focus on features created by organizers. ","1c018445":"We see that points are cross each other, but there are at least 2 clusters which we could see in top left and top right corner. <br>\nMight try to find this 2 clusters based on mean coordinates features. ","d9831560":"From the correlation table above we see some groups of features. <br>Let's manually unite them using mean.  ","fb04fb2d":"This take a lot of time. ","fd62f3fb":"Just a little notice if you prefer other view of heatmap (check out [documentation](https:\/\/seaborn.pydata.org\/generated\/seaborn.heatmap.html) for more):","521e752b":"Next cell runs ~ 13 min (it freezes completely with n_jobs not equal to 1 for unknown reason). <br>\nIt's definitely better and faster to use native CatBoost CV than `sklearn` one. <br>\nYou could check my [kernel](https:\/\/www.kaggle.com\/vchulski\/catboost-and-shap-for-dota-2-winner-prediction) dedicated to CatBoost.","89d4d72f":"It seems that range for y's finishes is: 116 while for x's: 122. \nThis means map is not completely symmetrical. \nLet's see now how this values differs for Radiant and Dire victories.","713824aa":"# General information\nThis kernel is dedicated to EDA of [Dota 2 Winner Prediction Competition ](https:\/\/www.kaggle.com\/c\/mlcourse-dota2-win-prediction)\n\nWe are provided with prepared data and described features as well as with a lot of \"raw\" json data. We need to predict winner of the game. \nEvaluation metric is [ROC-AUC](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/roc-and-auc). \n\n<left><img src='https:\/\/kor.ill.in.ua\/m\/610x385\/1848785.jpg'>","7153678e":"Other feauteres could be analyzed in the same way. <br>\nEven more data is stored in JSON files. <br>\nNow let's implement and compare few models. ","75795ab7":"# Submission\n<div id=\"Submission\">\n<\/div>","3886d8a7":"As we see correlation between target and general features is low, which seems to be logical. \nGame time or type of the game, as far as I know it, shouldn't affect much on winning side. \nLet's move to more interesting features and first take a look on map: \n\n![Dota 2 Map](https:\/\/habrastorage.org\/webt\/vq\/h2\/9c\/vqh29cm1vd-69blhriyqr98saww.png)\n\nAs description said `The goal is to destroy the opponent's fountain. No draws are possible in Dota 2.` which means that coordinates could be useful.\nLogic is the following: team which is on the enemy fonte at the end of the game won.","4e9a2c53":"## General features\n<div id=\"Generalfeatures\">\n<\/div>","e69b6226":"Here i decided to investigate which exactly values it takes, and was surprised that there is no 0 coordinates: ","d563217a":"# Simple models comparison\n<div id=\"simplemodels\">\nFirst, I am preparing data for learning and setting cross validation\n<\/div>","5f7b4d1f":"## Target \n<div id=\"Target\">\nAs we know ROC-AUC is almost robust to class imbalance but let's see how it's distributed to better understand data: \n<\/div>\n\n","41c45564":"## T-SNE on means coordinates features\n<div id=\"TSNE\">\n<\/div>","1a011600":"Now it seems that gold is main feature in the game. \n\nBut pay attention, results can be different based on parameters. \nCheck out [this](https:\/\/www.kaggle.com\/shokhan\/lightgbm-starter-code) kernel for instance. ","b057081a":"It's my first ever public kernel on Kaggle so any feedback is appreciated.","3ac9c79c":"Let's now show how this mean coordinates features corresponds with each other and with target","c0762966":"As we see CatBoost gives best results among tested algorithms - but it's very rough comparison.After adding hyperparams this could change.\nAnyway, this gives some hint which models we definetly should try. \n\nAlso, we see how fast LGBM is. I am not counting CatBoost which I will test later, but it's even 2 times faster than RF. ","73cd97df":"So, we have almost 40k entries in train dataset and we need to predict results of other 10k battles.\n\n** UPD: ** Thanks to  [@sonfire](https:\/\/www.kaggle.com\/sonfire), [@ecdrid](https:\/\/www.kaggle.com\/adityaecdrid) and [@ambisinistra](https:\/\/www.kaggle.com\/ambisinistra) who helps me to understand some features in `df_train_targets`. <br>\n* `time_remaining` means how much time remains till the end of the game at the point of time at which all characteristics and statistics shown. Indeed, if you'll sum `game_time` and `time_remaining` you receive exactly `duration` of the game. <br>\n* `next_roshan_team` tell us about next team after that point of time which will take roshan.\n\nMaybe I have to read more about Dota, they have competitions with [prizes](http:\/\/dota2.prizetrac.kr\/) more than on Kaggle. \n\n![Hm](http:\/\/img4.wikia.nocookie.net\/__cb20150117182228\/plantsvszombies\/images\/5\/57\/Wait-what.jpg)\n\nJust kiddin :) \n\nLet's continue, first I'll select target and then divide features on groups and observe them and their correllation with target.","f55ad5c4":"To simplify you navigation through this kernel: \n\n* [Main data exploration](#maindata)\n  * [Target distribution](#Target)\n  * [General features](#Generalfeatures)\n  * [Coordinates features](#Coordinatesfeatures)\n  * [T-SNE on means coordinates features](#TSNE)\n  * [KDA](#KDA)\n* [Models comparison](#simplemodels)\n* [LGBM feature importance](#FeatureImportance)\n* [Submission](#Submission)","d133e1ad":"I have no idea why there is no single x coordinate feature in top 10. Who have an idea pleas share in comments! ","c19ab15f":"## KDA (Kills|Deaths|Assists) \n<div id=\"KDA\">\n<\/div>","a09300b1":"## Coordinates features\n<div id=\"Coordinatesfeatures\">\n<\/div>","31eec403":"I will create another separate DataFrame to analyze kills, death and assists.","f8fb628c":"KDA in dota is [calculated](https:\/\/steamcommunity.com\/app\/570\/discussions\/0\/3307213006841396427\/ ) as: (K+A)\/D \n","9892cd5b":"# Feature Importance\n<div id=\"FeatureImportance\">\n<\/div>","8679b8e1":"I won't share blend solution and that's why I will public simple LGB with almost random parameters, but I bet you will do better than me :)","47bd600a":"And now will use following models without hyperparams: \n\nRF, LGBM, XGB, CatBoost"}}