{"cell_type":{"739c9f9d":"code","57176475":"code","520de55f":"code","2bccef76":"code","4f3cd4a4":"code","2301ed1c":"code","0babfa9b":"code","5b72ab69":"code","35100543":"code","12b7430e":"code","09d3822b":"code","732a7194":"markdown","366b0b50":"markdown","06d440ad":"markdown","ae491516":"markdown","f0f9bbba":"markdown","12332952":"markdown"},"source":{"739c9f9d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","57176475":"import spacy \nfrom spacy.lang.en import English\nfrom spacy import displacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\n\nimport en_core_web_sm\nimport en_core_web_lg\nimport string\n\nfrom numpy import *\nfrom datetime import datetime\n","520de55f":"#nlp = en_core_web_sm.load()\nnlp = en_core_web_lg.load()\n#nlp = spacy.load(\"en\")","2bccef76":"nlp.max_length = 3e9","4f3cd4a4":"filename = \"..\/input\/AAPL-8K-20031015162950.txt\"\ndocument = open(filename).read()\ndoc_test = nlp(document)","2301ed1c":"def extract_content(orig_file_name):\n    doc = open(orig_file_name).read()\n    components = doc.split('TIME:')\n    FileName =  components[0]\n    StockCode = FileName.split('\/')[0]\n    StockCode = StockCode.split(':')[1]\n\n    other = components[1]\n    # DateTime after before 'EVENT:'\n    components = other.split('EVENTS:')\n    DateTime = components[0]\n    DateTime = DateTime.replace('\\n','')\n    #DateTime\n    Date = DateTime[0:8]\n    Date = Date[0:4]+'-'+Date[4:6]+'-'+Date[6::]\n    #Time = DateTime[8::]\n    #Time = Time[0:2]+'-'+Time[2:4]+'-'+Time[4::]\n    # Categories before \"TEXT:\", content after 'TEXT:'\n    components = components[1]\n    components = components.split('TEXT:')\n    categories = components[0]\n    categories = categories.split('\\t')[1:]\n    categories[-1] = categories[-1].replace('\\n','')\n    categories = [element.upper() for element in categories]\n    #if '' in categories:\n    #    categories = categories.remove('')\n    \n    content = components[1]\n    return StockCode, Date, categories, content\n","0babfa9b":"#example\nStockCode, Date, categories, content = extract_content(filename)\n","5b72ab69":"\npunctuations = string.punctuation\nstopwords = spacy.lang.en.STOP_WORDS\n\ndef clean_component(doc):\n    \"\"\" Clean up text. Tokenize, lowercase, and remove punctuation and stopwords \"\"\"\n    #print(\"Running cleaner\")\n    #Replace punctuation by spacing\n    doc= doc.replace(punctuations, ' ')\n    #remove multiple inline\n    doc = doc.replace('\\n',' ')\n    doc = doc.strip()\n    #Remove multiple space\n    #doc = doc.strip()\n    # Remove symbols (#) and stopwords\n    doc = nlp(doc)\n    doc = [tok.text for tok in doc if (tok.text not in stopwords and tok.pos_ != \"PUNCT\" and tok.pos_ != \"SYM\")]\n    # Make all tokens lowercase\n    doc = [tok.lower() for tok in doc]\n    doc = ' '.join(doc)\n    return nlp.make_doc(doc)\n\ndef pipe_clean(docs, **kwargs):\n    for doc in docs:\n        yield clean_component(doc)\n\n# Yes, adding attributes to functions works...It's just a bit dirty-looking. Arguably less confusing to\n# make it a class. Shrug.\nclean_component.pipe = pipe_clean","35100543":"Date_form = 'Date'\n\n\ndef date_index(data):\n    data['datetime'] = pd.to_datetime(data[Date_form])\n    data = data.set_index('datetime')\n    data.drop([Date_form], axis=1, inplace=True)\n    return data","12b7430e":"def create_data(orig_file_name):\n    #extract partial information from full report\n    StockCode, datetime, categories, content = extract_content(orig_file_name)\n    #if '' in categories:\n    #    categories = categories.remove('')\n    #clean content of report\n    content_cleaned = clean_component(content)\n    #vectorize content\n    content_vector = content_cleaned.vector\n    #create data frame\n    content_vector = pd.DataFrame(content_vector).transpose()\n    data= pd.DataFrame([1] * len(categories)).transpose()\n    data.columns = categories\n    data.insert(0, Date_form, datetime) \n    data.insert(1, \"StockCode\", StockCode)   \n    \n    data = pd.concat([data, content_vector], axis=1, sort=False)\n   \n    return data","09d3822b":"list_file = os.listdir(\"..\/input\")\n\ndef concat_report_data(list_file):\n    Data = pd.DataFrame() \n    for file_name in list_file:\n        orig_file_name = '..\/input\/'+ file_name\n        data = create_data(orig_file_name)\n        Data = pd.concat([Data, data], axis=0, ignore_index=True, sort=False)\n    #fill all NaN by 0, corresponding to the case that the categorie with NaN is not in the report\n    Data = Data.fillna(0)\n    Data[Date_form] = pd.to_datetime(Data[Date_form])\n    Data = date_index(Data)\n    return Data\n#example\nconcat_report_data(list_file)\n","732a7194":"**Join data from individual reports of a stock**","366b0b50":"**Extract Date, time, StockCode, Category, Content**","06d440ad":"**Preprocessing content**","ae491516":"**Clean up text by converting uppercase to lowercase, removing punctuation and stopword**","f0f9bbba":"**Create Dataframe for report**","12332952":"**Export to csv**"}}