{"cell_type":{"487bdb22":"code","7b9931ef":"code","7cda1042":"code","4c629b08":"code","58891af2":"code","937de6ee":"code","a5f6eb54":"code","e6043dd3":"code","678b0d7e":"code","9b4455f6":"code","8a50717b":"code","6a6270dc":"code","5971ec39":"code","3c804cdc":"code","b21046ba":"code","b14ce31a":"code","0cbd3c84":"markdown","9adcd941":"markdown","11f4950a":"markdown","ed8aa5c4":"markdown","7fc22523":"markdown","b3221f4d":"markdown","4db6c886":"markdown","be8c7c58":"markdown","3c01ea1a":"markdown","599273ec":"markdown","7fe5b5d4":"markdown","00ddb7cd":"markdown","315676fb":"markdown","002210d3":"markdown","8b31ffde":"markdown","0039d5f0":"markdown","19d76b55":"markdown"},"source":{"487bdb22":"import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input\/stanford-dogs-dataset\/images\/Images\/n02088094-Afghan_hound'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\nimport torch\n\nimport requests\nimport urllib.request\n\nfrom PIL import Image\nfrom io import BytesIO","7b9931ef":"torch.cuda.is_available()","7cda1042":"from torchvision import models","4c629b08":"# dir(models)","58891af2":"alexnet = models.AlexNet()","937de6ee":"resnet = models.resnet101(pretrained=True)\n# resnet","a5f6eb54":"from torchvision import transforms\n\n\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )])","e6043dd3":"# img = Image.open(\"\/kaggle\/input\/stanford-dogs-dataset\/images\/Images\/n02109961-Eskimo_dog\/n02109961_658.jpg\")  # ERROR ('Siberian husky', 50.530677795410156)\nimg = Image.open(\"\/kaggle\/input\/stanford-dogs-dataset\/images\/Images\/n02088094-Afghan_hound\/n02088094_294.jpg\")  # ('Afghan hound, Afghan', 98.31031799316406)\n\n# url = \"https:\/\/cdn.mos.cms.futurecdn.net\/QjuZKXnkLQgsYsL98uhL9X-1200-80.jpg\"  # ('Pomeranian', 99.84607696533203)\n# url = \"https:\/\/images.wagwalkingweb.com\/media\/articles\/dog\/jack-russell-terrier-allergies\/jack-russell-terrier-allergies.jpg\"  # ERROR ('beagle', 91.52595520019531)\n# url = \"https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Fwww.pets4homes.co.uk%2Fimages%2Fbreeds%2F44%2Flarge%2Ff71f3b4752554e2bad635759c5cdc45c.jpg&f=1&nofb=1\"  # ('toy terrier', 70.98729705810547)\n\n# response = requests.get(url)\n# img = Image.open(BytesIO(response.content))\n\nimg","678b0d7e":"img_t = preprocess(img)","9b4455f6":"batch_t = torch.unsqueeze(img_t, 0)","8a50717b":"resnet.eval()","6a6270dc":"out = resnet(batch_t)\n# out","5971ec39":"target_url = 'https:\/\/raw.githubusercontent.com\/deep-learning-with-pytorch\/dlwpt-code\/master\/data\/p1ch2\/imagenet_classes.txt'\n\ndata = urllib.request.urlopen(target_url)\nlabels = [line.strip().decode('utf-8') for line in data.readlines()]\n\n# labels","3c804cdc":"_, index = torch.max(out, 1)","b21046ba":"percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\nlabels[index[0]], percentage[index[0]].item()","b14ce31a":"_, indices = torch.sort(out, descending=True)\n[(labels[idx], percentage[idx].item()) for idx in indices[0][:5]]","0cbd3c84":"To see the list of predicted labels, we will load a text file listing the labels in the\nsame order they were presented to the network during training, and then we will pick\nout the label at the index that produced the highest score from the network. Almost\nall models meant for image recognition have output in a form similar to what we\u2019re\nabout to work with.\nLet\u2019s load the file containing the 1,000 labels for the ImageNet dataset classes:","9adcd941":"We\u2019re now ready to run our model.\nThe process of running a trained model on new data is called inference in deep learning circles. In order to do inference, we need to put the network in eval mode:","11f4950a":"Next, we can pass the image through our preprocessing pipeline:","ed8aa5c4":"The resnet variable can be called like a function, taking as input one or more images and producing an equal number of scores for each of the 1,000 ImageNet classes. Before we can do that, however, we have to preprocess the input images so they are the right size and so that their values (colors) sit roughly in the same numeri-cal range. In order to do that, the torchvision module provides transforms, which allow us to quickly define pipelines of basic preprocessing functions:","7fc22523":"A staggering set of operations involving 44.5 million parameters has just happened, producing a vector of 1,000 scores, one per ImageNet class. That didn\u2019t take long, did it?","b3221f4d":"At this point, we need to determine the index corresponding to the maximum score\nin the out tensor we obtained previously. We can do that using the max function in\nPyTorch, which outputs the maximum value in a tensor as well as the indices where\nthat maximum value occurred:","4db6c886":"We can now use the index to access the label. Here, index is not a plain Python number, but a one-element, one-dimensional tensor (specifically, tensor([207]) ), so we\nneed to get the actual numerical value to use as an index into our labels list using\nindex[0] .","be8c7c58":"![](http:\/\/)","3c01ea1a":"Then we can reshape, crop, and normalize the input tensor in a way that the network\nexpects. We\u2019ll understand more of this in the next two chapters; hold tight for now:","599273ec":"Let\u2019s create an instance of the network now. We\u2019ll pass an argument that will instruct the function to download the weights of resnet101 trained on the ImageNet dataset, with 1.2 million images and 1,000 categories:","7fe5b5d4":"https:\/\/github.com\/deep-learning-with-pytorch\/dlwpt-code","00ddb7cd":"If we forget to do that, some pretrained models, like batch normalization and dropout,\nwill not produce meaningful answers, just because of the way they work internally.\nNow that eval has been set, we\u2019re ready for inference:","315676fb":"In order to run the AlexNet architecture on an input image, we can create an instance of the AlexNet class. This is how it\u2019s done:","002210d3":"We also use torch.nn.functional.softmax (http:\/\/mng.bz\/BYnq) to nor-\nmalize our outputs to the range [0, 1], and divide by the sum. That gives us something\nroughly akin to the confidence that the model has in its prediction. In this case, the\nmodel is 96% certain that it knows what it\u2019s looking at is a golden retriever:","8b31ffde":"We\u2019ve just run a network that won an image-classification competition in 2015. It\nlearned to recognize our dog from examples of dogs, together with a ton of other\nreal-world subjects. We\u2019ll now see how different architectures can achieve other kinds\nof tasks, starting with image generation.","0039d5f0":"We can now grab a picture of our favorite dog (say, bobby.jpg from the GitHub repo),\npreprocess it, and then see what ResNet thinks of it. We can start by loading an image\nfrom the local filesystem using Pillow (https:\/\/pillow.readthedocs.io\/en\/stable), an\nimage-manipulation module for Python:","19d76b55":"Since the model produced scores, we can also find out what the second best, third\nbest, and so on were. To do this, we can use the sort function, which sorts the values\nin ascending or descending order and also provides the indices of the sorted values in\nthe original array:"}}