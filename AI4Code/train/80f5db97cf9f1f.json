{"cell_type":{"e3c80d15":"code","3509b55b":"code","dda5c989":"code","10e16d72":"code","4201331b":"code","cd6ee773":"code","e8c10ace":"code","fa930026":"code","3a09ff50":"code","67ccbea4":"code","b9860e01":"code","fd5bdb78":"code","991033f3":"code","cc569388":"code","74ecd58a":"code","701da417":"code","21c0875c":"code","a04ab5e3":"code","aba18d63":"code","cda8cab4":"code","a9996455":"code","480c4caf":"code","2d3543b1":"code","d4933d14":"code","4ce3ada3":"code","54e105f2":"code","ded30a33":"code","5616390e":"code","fd97cf87":"code","2fc75632":"code","f85d4b17":"code","a24b1909":"code","6f7890a2":"code","e39c24f9":"code","ef6d177c":"code","51a17c9d":"code","99f2f8e4":"code","d548a972":"code","f10b75fa":"code","983f034f":"code","233c5539":"code","5f0da8f3":"code","45075c0c":"code","d7e43af0":"code","bce84c97":"code","e943c4c8":"code","3e8254a3":"code","e6726707":"markdown","143ff358":"markdown","ff0372dd":"markdown","000309e6":"markdown","5fbdd969":"markdown","a430e4e3":"markdown","424970b1":"markdown","e1b083f3":"markdown","60f9fa3e":"markdown","88b30742":"markdown","6a1d75b8":"markdown","aeebf5e6":"markdown","89a0c299":"markdown","cd00346a":"markdown","1fe70c30":"markdown","d1453045":"markdown","41f56d12":"markdown","a8a58dc8":"markdown","fb2774d1":"markdown","ed466a58":"markdown","3a040f54":"markdown","f291ece9":"markdown","1de7a144":"markdown","6cbf04bd":"markdown","89333e7b":"markdown","5e870a31":"markdown","1aa0b3ef":"markdown","f3d6dc3d":"markdown","e22c4bfb":"markdown","06271313":"markdown","bd692c49":"markdown"},"source":{"e3c80d15":"import random\nimport pandas as pd\nimport numpy as np\nimport gc\nimport os\nfrom sklearn.metrics import f1_score\nimport re\nimport torch\nimport time\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm_notebook, tnrange\nfrom tqdm.auto import tqdm\ntqdm.pandas(desc='Progress')\n\n#pd.set_option('display.max_colwidth', -1)  ","3509b55b":"EMBEDDINGS_PATH = '..\/input\/embeddings\/'\nEMBEDDING_FILE_GLOVE = f'{EMBEDDINGS_PATH}\/glove.840B.300d\/glove.840B.300d.txt'","dda5c989":"embed_size = 300 # how big is each word vector\nmax_features = 120000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 50 # max number of words in a question to use\nbatch_size = 1024 # how many samples to process at once\nn_epochs = 2 # how many times to iterate over all samples\nSEED = 1006","10e16d72":"# REPEATABILITY\ndef seed_everything(seed=SEED):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()\n# kernel https:\/\/www.kaggle.com\/hengzheng\/pytorch-starter","4201331b":"os.environ['OMP_NUM_THREADS'] = '4'","cd6ee773":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")","e8c10ace":"# Randomize\nnp.random.seed(SEED)\ntrn_idx = np.random.permutation(len(df_train))\ndf_train = df_train.iloc[trn_idx]\n\ndf = pd.concat([df_train ,df_test],sort=True)","fa930026":"df_train.head(3)","3a09ff50":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\ndf_train[\"question_text\"] = df_train[\"question_text\"].progress_apply(lambda x: clean_text(x))\ndf_test[\"question_text\"] = df_test[\"question_text\"].apply(lambda x: clean_text(x))","67ccbea4":"def clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\ndf_train[\"question_text\"] = df_train[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\ndf_test[\"question_text\"] = df_test[\"question_text\"].apply(lambda x: clean_numbers(x))","b9860e01":"specials = {'\\u200b': ' ', '\u2026': ' ... ', '\\ufeff': '', '\u0915\u0930\u0928\u093e': '', '\u0939\u0948': ''}\n\ndef clean_special_chars(text):\n    \n    for s in specials:\n        text = text.replace(s, specials[s])    \n    return text\n\ndf_train[\"question_text\"] = df_train[\"question_text\"].progress_apply(lambda x: clean_special_chars(x))\ndf_test[\"question_text\"] = df_test[\"question_text\"].apply(lambda x: clean_special_chars(x))","fd5bdb78":"df_train[\"question_text\"] = df_train[\"question_text\"].apply(lambda x: x.lower())\ndf_test[\"question_text\"] = df_test[\"question_text\"].apply(lambda x: x.lower())","991033f3":"list_sentences_train = df_train['question_text']\nlist_sentences_test = df_test['question_text']\nlist_sentences_combined = list_sentences_train.append(list_sentences_test, ignore_index=True)\ntokenizer = Tokenizer(num_words=max_features, filters='\\t\\n')\ntokenizer.fit_on_texts(list(list_sentences_combined))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)","cc569388":"train_x = pad_sequences(list_tokenized_train, maxlen=maxlen)\ntest_x = pad_sequences(list_tokenized_test, maxlen=maxlen)\ntrain_y = df_train['target'].values","74ecd58a":"start = time.time()\ndef get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE_GLOVE))\nend = time.time()\nprint(end-start)","701da417":"# Get embedding mean and st deviation for giving random value near mean for words that were not in glove\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nemb_mean,emb_std","21c0875c":"word_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","a04ab5e3":"seed_everything()\ninp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","aba18d63":"def train_validate_test_split(df, df_y, train_percent=.6, validate_percent=.2, random_state=10):\n    np.random.seed(random_state)\n    perm = np.random.permutation(len(df))\n    m = len(df)\n    train_end = int(train_percent * m)\n    validate_end = int(validate_percent * m) + train_end\n    train = df[perm[:train_end]]\n    validate = df[perm[train_end:validate_end]]\n    test = df[perm[validate_end:]]\n    \n    train_y = df_y[perm[:train_end]]\n    validate_y = df_y[perm[train_end:validate_end]]\n    test_y = df_y[perm[validate_end:]]\n\n    return train, validate, test, train_y, validate_y, test_y, perm","cda8cab4":"# Train \/ Val \/ Test -split\nseed_everything()\nX_tra, X_val, X_test, y_tra, y_val, y_test, permutation = train_validate_test_split(train_x, train_y, \n                                train_percent=0.95, validate_percent=0.04, random_state=SEED+2)","a9996455":"print(len(X_tra))\nprint(len(X_val))\nprint(len(X_test))","480c4caf":"seed_everything()\nmodel.fit(X_tra, y_tra, batch_size=batch_size, epochs=n_epochs, validation_data=(X_val, y_val)); #verbose=2","2d3543b1":"train_preds = model.predict(X_tra, batch_size=1024)\nprint(len(train_preds))","d4933d14":"# https:\/\/www.kaggle.com\/ziliwang\/baseline-pytorch-bilstm\ndef bestThresshold(train_y,train_preds):\n    tmp = [0,0,0] # idx, cur, max\n    delta = 0\n    for tmp[0] in tqdm(np.arange(0.1, 0.501, 0.01)):\n        tmp[1] = f1_score(train_y, np.array(train_preds)>tmp[0])\n        if tmp[1] > tmp[2]:\n            delta = tmp[0]\n            tmp[2] = tmp[1]\n    print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))\n    return delta\ndelta = bestThresshold(y_tra,train_preds)","4ce3ada3":"val_preds = model.predict(X_val, batch_size=1024)\nprint(len(val_preds))\nprint(f1_score(y_val, np.array(val_preds)>delta))","54e105f2":"test_preds = model.predict(X_test, batch_size=1024)\nprint(len(test_preds))\nprint(f1_score(y_test, np.array(test_preds)>delta))","ded30a33":"final_preds = model.predict(test_x, batch_size=1024)","5616390e":"submission = df_test[['qid']].copy()\nsubmission['prediction'] = (final_preds > delta).astype(int)\nsubmission.to_csv('submission.csv', index=False)","fd97cf87":"!head submission.csv","2fc75632":"train_preds","f85d4b17":"# predictions\npredicted = pd.DataFrame(train_preds)\npredicted.columns = ['predicted']\npredicted.to_csv('train_preds.csv', index=False)","a24b1909":"# save the processed form of train-data\ndf_train_preproc = df_train\ndf_train_preproc.to_csv('df_train_preprocessed.csv')","6f7890a2":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_train_preproc = pd.read_csv(\"df_train_preprocessed.csv\")\ntrain_preds = pd.read_csv('train_preds.csv')","e39c24f9":"y_train = y_tra; y_train[0:10]","ef6d177c":"train_preds[0:10].T","51a17c9d":"combined = pd.concat([df_train, train_preds], axis=1, sort=False)\ncombined.head()","99f2f8e4":"# SIZE of error - |true class - predicted|\ncombined['error'] = abs(combined['target'] - combined['predicted']) \ncombined.head()","d548a972":"# Display whole text of dataframe field and don't cut it\npd.set_option('display.max_colwidth', -1)    ","f10b75fa":"combined.head()","983f034f":"# List of biggest errors in decreasing order\nsorted = combined.sort_values(by=['error'], ascending=False)","233c5539":"sorted[ sorted['target']==0] [0:14]","5f0da8f3":" pd.options.display.float_format = \"{:.8f}\".format","45075c0c":"# pick texts where true target was 1\ninsincere = sorted[sorted['target']==1]\ninsincere[0:14]","d7e43af0":"# List of errors in increasing order\nsorted_increasing = combined.sort_values(by=['error'], ascending=True)\nsorted_increasing[0:10]","bce84c97":"# add new filed 'question_length' in characters\ncombined['question_length']=combined['question_text'].apply(lambda x: len(x))\n# sort by that field\nsorted_len = combined.sort_values(by=['question_length'], ascending=True)\nsorted_len[0:20]","e943c4c8":"# reverse order - start from longest\n# This print is very wide, so omit printing qid-field\nsorted_len.drop('qid',axis=1).iloc[::-1][0:8]","3e8254a3":"sorted_len[sorted_len['qid']=='4d2e2796dd1ced2c8e64']","e6726707":"### Save for later analysis","143ff358":"#### Measure F1 for Validation data","ff0372dd":"#### How about longest questions?","000309e6":"#### Sort the items in decreasing order of error amount","5fbdd969":"## Part 2. Analysing Classifier errors - Manual Error Analysis","a430e4e3":"In one sentence we can also notice that there are http addresses within the data. ","424970b1":"### False Positive with biggest error  - Sincere but predicted strongly as insincere","e1b083f3":"#### Find best threshold on Trainin data","60f9fa3e":"One possible way to go further would be to use model with attention layer and visualize the attention - how much does each word in the sentence give weight to chosen class.","88b30742":"#### Test data","6a1d75b8":"### View False negatives - Insinsere but predicted as sincere","aeebf5e6":"If you wish to explore more, you can view 100-200 examples of these groups and grasp an understanding of what the data is like and see what you might find.","89a0c299":"#### Here it is not instantly obvious, why many of these are predicted as insincere","cd00346a":"#### How about most correct predictions?","1fe70c30":"#### Tokenize","d1453045":"### Here we can see that almost all very short questions are Insinsere, while our model predicts them mostly as sincere!","41f56d12":"Our predicted values","a8a58dc8":"#### Count the error\nReal value of class is 0 or 1, error is how far our prediction is from this.","fb2774d1":"#### Shortest and longest questions","ed466a58":"## Embedding","3a040f54":"## Analyse models biggest error predictions\n\nIn this workbook I will show how to print and view the biggest miss-predictions of a classifier.\n\nLooking under the hood, you can see if you can recognize some patterns or specific areas, where classifier is failing - which you could then concentrate on improving.","f291ece9":"### Strangely many of these big math formulas are labeled as insincere in original data. Evil math?","1de7a144":"#### preprocess","6cbf04bd":"## Part 2 after halfway of workbook is where the analysis starts. You might wanna skip to there.\n","89333e7b":"#### Simple bidirectional LSTM with two fully connected layers. We add some dropout to the LSTM since even 2 epochs is enough to overfit.","5e870a31":"#### Add our predictions along the original data","1aa0b3ef":"#### Load data back","f3d6dc3d":"#### Question_texts are not printed fully, but shortened. Lets fix that","e22c4bfb":"## Load data","06271313":"### Part 1. Training classifier and making predictions","bd692c49":"Real values of testdata"}}