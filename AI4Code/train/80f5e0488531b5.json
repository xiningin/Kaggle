{"cell_type":{"419f8701":"code","493b0ca6":"code","79b16533":"code","3fc4bbd7":"code","04ff17e6":"code","10852622":"code","d2b429aa":"code","890b699d":"code","06e8503b":"code","e6ad1c91":"markdown","29a5b1cf":"markdown","fd64974c":"markdown","de8dd283":"markdown","1738d2aa":"markdown","6a25eddb":"markdown","388c68d3":"markdown","e9b0a642":"markdown","65217b28":"markdown","d46dfc3a":"markdown"},"source":{"419f8701":"import os\nimport numpy as np\nimport pandas as pd\nfrom skimage.feature import hog\nfrom keras.preprocessing import image\nfrom sklearn.model_selection import train_test_split\nfrom keras.applications.imagenet_utils import preprocess_input","493b0ca6":"path = '\/kaggle\/input\/caltech101\/101_ObjectCategories\/'\nroot = '101_ObjectCategories'\nexclude = ['BACKGROUND_Google']\ntrain_split, val_split = 0.7, 0.15\n\ncategories = [x[0] for x in os.walk(path) if x[0]][1:]\ncategories = [c for c in categories if c not in [os.path.join(path, e) for e in exclude]]\n\npd.Series(categories).head()","79b16533":"def get_image(path):\n    img = image.load_img(path, target_size=(64, 64))\n    fd, hog_image = hog(img, orientations=9,\n                    pixels_per_cell=(16, 16),\n                    cells_per_block=(2, 2),\n                    visualize=True,\n                    multichannel=True) \n    return img, fd","3fc4bbd7":"data = []\nfor c, category in enumerate(categories):\n    images = [os.path.join(dp, f) for dp, dn, filenames \n              in os.walk(category) for f in filenames \n              if os.path.splitext(f)[1].lower() in ['.jpg','.png','.jpeg']]\n\n    for img_path in images:\n            img, fd = get_image(img_path)\n            data.append({'x':fd, 'y':c})\nimg","04ff17e6":"pd.Series(data).head()","10852622":"X, y = np.array([t[\"x\"] for t in data]), np.array([t[\"y\"] for t in data] )\nprint('\\n', 'x shape:', X.shape,\n      '\\n', 'y shape:', y.shape,\n      '\\n', 'num_classes:', len(np.unique(y)))","d2b429aa":"pd.concat([pd.DataFrame(X), pd.DataFrame(y)], axis=1).head(5)","890b699d":"x_train, x_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=1)","06e8503b":"print('\\n', 'x_train shape:', x_train.shape,\n      '\\n', 'x_test shape:', x_test.shape)","e6ad1c91":"# About this notebook\n#### Author: Seyedsaman Emami\n\n<hr>\n\n|**problem**|**Dataset**|**Source of the dataset**|**Area**|\n|-:|-:|-:|-:|\n|Multi-class classification|Caltech101|vision.caltech.edu|COMPUTATIONAL VISION|","29a5b1cf":"\nIf you want to have this method or use the outputs of the notebook, you can fork the Notebook as following (**copy and Edit Kernel**).\n\n<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F1101107%2F8187a9b84c9dde4921900f794c6c6ff9%2FScreenshot%202020-06-28%20at%201.51.53%20AM.png?generation=1593289404499991&alt=media\" alt=\"Copyandedit\" width=\"300\" height=\"300\" class=\"center\">\n\nI tried to keep everything as simple as possible, if you have any doubts or questions, please feel free to ask in the comments.","fd64974c":"# Import the dataset","de8dd283":"# Feature engineering\n<hr>\n<h5> A method for resizing, extracting, and feature engineering <\/h5>","1738d2aa":"# Creat a dataset","6a25eddb":"# splitting the data","388c68d3":"# Importing libs","e9b0a642":"## Dataset in a glance","65217b28":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5642C5;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 8px;\n              color:white;\">\n\n<hr>\nIn this Notebook, I am going to investigate the Caltech101 dataset features from different aspects and prepare a clean input for the Machine learning models.\n\nNote that this notebook does not contain any ML model. The focus of this work is to prepare a reliable dataset for the ML models.\n\nNote that I am looking for a dataset with 324 features.","d46dfc3a":"### Defining the X, and y"}}