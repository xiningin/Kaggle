{"cell_type":{"9825a107":"code","526dc05f":"code","0f954b5f":"code","f29bc111":"code","9d4a3891":"code","2ebc1a56":"code","cfadbb25":"code","0d14035d":"code","9d5904e8":"code","546c4c99":"code","76720b07":"code","134be799":"code","afc4f6f8":"code","9b28bfba":"code","70ba900d":"code","20632094":"code","ab6f616a":"code","0df8a339":"code","4dbb67da":"code","75046f1c":"code","68faa160":"code","162d945e":"code","2e824efe":"code","6d7e23c0":"code","7bee71d0":"code","a512de7f":"code","d3b388d9":"code","f4e6e277":"code","48274ede":"code","c2be6a5c":"code","db226ba4":"code","1fe618fe":"code","f0190000":"code","a0953bff":"code","9e0495bb":"code","5852bd00":"code","99223fb3":"code","87653a2b":"code","b84676a3":"code","482de9d2":"code","1bc6c98e":"code","cf8d1f3c":"code","e487e8fd":"code","503c9f36":"code","974163f0":"code","9fcc5c96":"code","5f36d812":"code","7439e8cf":"code","b483b9f0":"code","3351207a":"code","ebb6c904":"code","41b354c8":"code","0ee59391":"code","240c7809":"code","23c4b9b2":"code","baff9032":"code","fe87cd5a":"code","3b8f2e2b":"code","4b95a286":"code","45fe961f":"code","6216c530":"code","b39923a7":"code","1012a8f7":"code","67db0821":"code","80cd8ce0":"code","10cfc176":"markdown","e7356e25":"markdown","441136e5":"markdown","5c9f12d8":"markdown","652c35fe":"markdown","747a6f33":"markdown","fbf81954":"markdown","5bdf8f78":"markdown","955882bb":"markdown","f73d6b1b":"markdown","6bdeab37":"markdown","3e673959":"markdown","b5abaa2f":"markdown","31a2f199":"markdown","e6db32a6":"markdown","7156e4b1":"markdown","34183f69":"markdown","dd7f9106":"markdown","26985252":"markdown","1aa38034":"markdown","59f0b864":"markdown","5b41541a":"markdown","7f68df57":"markdown","7df9e9ed":"markdown","0bbabde4":"markdown"},"source":{"9825a107":"\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","526dc05f":"#import important Libraries\n\n# data analysis and wrangling\nimport pandas as pd\nimport numpy as np #linear algebra\nimport random as rnd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(action='ignore', category=UserWarning)\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import LabelEncoder\n\n# NAIBE BAYES\nfrom sklearn.naive_bayes import GaussianNB\n#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n#RANDOM FOREST\nfrom sklearn.ensemble import RandomForestClassifier\n#LOGISTIC REGRESSION\nfrom sklearn.linear_model import LogisticRegression\n#SVM\nfrom sklearn.svm import SVC\n#DECISON TREE\nfrom sklearn.tree import DecisionTreeClassifier\n#XGBOOST\nfrom xgboost import XGBClassifier\n#AdaBoosting Classifier\nfrom sklearn.ensemble import AdaBoostClassifier\n#GradientBoosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n#HistGradientBoostingClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier, StackingClassifier\n\nfrom sklearn.model_selection import cross_val_score,StratifiedKFold,GridSearchCV\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler ,Normalizer , MinMaxScaler, RobustScaler \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n","0f954b5f":"train_labels_df = pd.read_csv(\"..\/input\/earthqucke-data\/train_labels.csv\") #,index_col='building_id'\ntrain_values_df = pd.read_csv(\"..\/input\/earthqucke-data\/train_values.csv\") #, index_col='building_id'\ntest_values_df  = pd.read_csv(\"..\/input\/testdata\/test_values.csv\")\n\n","f29bc111":"#Merge the two data sets to use the Damage level for visualization\n\ndf = train_values_df.merge(train_labels_df, on = 'building_id')","9d4a3891":"#view 5 rows from data from header\ndf.head()","2ebc1a56":"#view 5 rows from data from tail\ndf.tail()","cfadbb25":"#view 5 rows from data from header\ndf.head()","0d14035d":"#check missing values\ndf.isnull().sum()","9d5904e8":"# Recheck for duplicates in the data\nsum(df.duplicated())","546c4c99":"#check data type for each column\ndf.info()","76720b07":"# the dimension of the data set\ndf.shape","134be799":"#show unique values in each column\ndf.nunique()","afc4f6f8":"# split data train into Numerical and Categorocal\nnumeric = df.select_dtypes(exclude='object')\ncategorical = df.select_dtypes(include='object')","9b28bfba":"#show 5 rows form  numerical features\nnumeric.head()","70ba900d":"#show 5 rows form  Categorical features\ncategorical.head()","20632094":"# Descriptive statistics for Categorical features\ndf.describe(include=['O'])","ab6f616a":"# Descriptive statistics for numerical features\ndf.describe()","0df8a339":"# Enter your code here\ndf['damage_grade'].value_counts()","4dbb67da":"fig1, ax1 = plt.subplots()\nax1.pie(df['damage_grade'].groupby(df['damage_grade']).count(), \n        labels = ['low', 'Medium','Almost'], autopct = '%1.1f%%')\nax1.axis('equal')\n\nplt.show()","75046f1c":"#check his relevance\n\ndata_corr = df.corr()\ntable = data_corr['damage_grade'].sort_values(ascending=False).to_frame()\ncm = sns.light_palette(\"green\", as_cmap=True)\ntb = table.style.background_gradient(cmap=cm)\ntb","68faa160":"#correlation heatmap of dataset\nplt.figure(figsize = (25,15))\ncorrelation_matrix = df.corr()\nsns.heatmap(correlation_matrix, annot=True,fmt=\".2f\")","162d945e":"corrmat=df.corr(method='spearman')\nf,ax=plt.subplots(figsize=(14,10))\nsns.heatmap(corrmat,ax=ax,cmap=\"YlGnBu\", linewidths=0.1)","2e824efe":"df.describe(include=['O'])","6d7e23c0":"# Enter your code here\ndf['foundation_type'].value_counts()","7bee71d0":"df['land_surface_condition'] = df['land_surface_condition'].map( {'t': 0, 'n': 1 ,'o' : 2} ).astype(str)\ntest_values_df['land_surface_condition'] = test_values_df['land_surface_condition'].map( {'t': 0, 'n': 1 ,'o' : 2} ).astype(str)","a512de7f":"df['foundation_type'] = df['foundation_type'].map( {'r': 0, 'w': 1 ,'u' : 2 ,'i' : 3,'h' : 4} ).astype(str)\ntest_values_df['foundation_type'] = test_values_df['foundation_type'].map( {'r': 0, 'w': 1 ,'u' : 2 ,'i' : 3,'h' : 4} ).astype(str)","d3b388d9":"df['roof_type'] = df['roof_type'].map( {'n': 0, 'q': 1 ,'x' : 2} ).astype(str)\ntest_values_df['roof_type'] = test_values_df['roof_type'].map( {'n': 0, 'q': 1 ,'x' : 2} ).astype(str)","f4e6e277":"df['ground_floor_type'] = df['ground_floor_type'].map( {'f': 0, 'x': 1 ,'v' : 2 ,'z' : 3 ,'m' : 4 } ).astype(str)\ntest_values_df['ground_floor_type'] = test_values_df['ground_floor_type'].map( {'f': 0, 'x': 1 ,'v' : 2 ,'z' : 3 ,'m' : 4 } ).astype(str)","48274ede":"df['other_floor_type'] = df['other_floor_type'].map( {'q': 0, 'x': 1 ,'j' : 2 ,'s' : 3 } ).astype(str)\ntest_values_df['other_floor_type'] = test_values_df['other_floor_type'].map( {'q': 0, 'x': 1 ,'j' : 2 ,'s' : 3 } ).astype(str)","c2be6a5c":"df['position'] = df['position'].map( {'s': 0, 't': 1 ,'j' : 2 ,'o' : 3 } ).astype(str)\ntest_values_df['position'] = test_values_df['position'].map( {'s': 0, 't': 1 ,'j' : 2 ,'o' : 3 } ).astype(str)","db226ba4":"df['plan_configuration'] = df['plan_configuration'].map( {'d': 0, 'q': 1 ,'u' : 2 ,'s' : 3 ,'c' : 4 ,'a' : 5 ,'o' : 6 ,'m' : 7 ,'n' : 8 ,'f' : 9} ).astype(str)\ntest_values_df['plan_configuration'] = test_values_df['plan_configuration'].map( {'d': 0, 'q': 1 ,'u' : 2 ,'s' : 3 ,'c' : 4 ,'a' : 5 ,'o' : 6 ,'m' : 7 ,'n' : 8 ,'f' : 9} ).astype(str)","1fe618fe":"df['legal_ownership_status'] = df['legal_ownership_status'].map( {'v': 0, 'a': 1 ,'w' : 2 ,'r' : 3 } ).astype(str)\ntest_values_df['legal_ownership_status'] = test_values_df['legal_ownership_status'].map( {'v': 0, 'a': 1 ,'w' : 2 ,'r' : 3 } ).astype(str)","f0190000":"# There are few outlier buildings older than 100 years. Remove them from training data\n","a0953bff":"df_train=df[df['age']<=100]\n\nprint('We removed ',df.shape[0]- df_train.shape[0],'outliers')","9e0495bb":"df_train.shape","5852bd00":"# Enter your code here\ndf_train['age'].value_counts()","99223fb3":"# Enter your code here\ntest_values_df['age'].value_counts()","87653a2b":"df_train.loc[df_train['age']<16,'age'] = 0\ndf_train.loc[(df_train['age']>16) & (df_train['age']<=32),'age'] =1\ndf_train.loc[(df_train['age']>32) & (df_train['age']<=48),'age'] =2\ndf_train.loc[(df_train['age']>48) & (df_train['age']<=64),'age'] =3\ndf_train.loc[df_train['age']>64,'age'] =4","b84676a3":"test_values_df.loc[test_values_df['age']<16,'age'] = 0\ntest_values_df.loc[(test_values_df['age']>16) & (test_values_df['age']<=32),'age'] =1\ntest_values_df.loc[(test_values_df['age']>32) & (test_values_df['age']<=48),'age'] =2\ntest_values_df.loc[(test_values_df['age']>48) & (test_values_df['age']<=64),'age'] =3\ntest_values_df.loc[test_values_df['age']>64,'age'] =4","482de9d2":"df_train['age'] = df_train['age'].astype(str)\ntest_values_df['age'] = test_values_df['age'].astype(str)","1bc6c98e":"df_train.describe(include=['O'])","cf8d1f3c":"test_values_df.describe(include=['O'])","e487e8fd":"test_values_df.shape","503c9f36":"df_train.shape","974163f0":"train_x=pd.get_dummies(df_train,drop_first=True)\nprint(\"the shape of the original dataset for training data\",df_train.shape)\nprint(\"the shape of the encoded dataset\",train_x.shape)\nprint(\"We have \",train_x.shape[1]- df_train.shape[1], 'new encoded features')","9fcc5c96":"test_x=pd.get_dummies(test_values_df,drop_first=True)\nprint(\"the shape of the original dataset for testing data\",test_values_df.shape)\nprint(\"the shape of the encoded dataset\",test_x.shape)\nprint(\"We have \",test_x.shape[1]- test_values_df.shape[1], 'new encoded features')","5f36d812":"test_x.info()","7439e8cf":"train_r=train_x.sample(n=300)\n#test_r =test_x.sample(n=1000)\ntest_r=test_x.copy()\n","b483b9f0":"train_x1 = train_r.drop('damage_grade', axis=1)\ny_target= train_r['damage_grade']\ny_target=y_target.to_frame()","3351207a":"#delete building id\ntrain_x1 = train_x1.drop('building_id', axis=1)\ntest_r = test_r.drop('building_id', axis=1)\n","ebb6c904":"test_x1=test_r.copy()","41b354c8":"print(train_x1.shape)\nprint(y_target.shape)\nprint(test_x1.shape)","0ee59391":"X_train,X_test,y_train,y_test = train_test_split(train_x1 ,y_target,test_size=0.30 ,  shuffle=True, random_state=42)","240c7809":"sk_fold = StratifiedKFold(10,shuffle=True, random_state=42)\n#sc =StandardScaler()\n\nsc =StandardScaler()\n\nX_train= sc.fit_transform(X_train)\n\nX_train_1= sc.transform(train_x1.values)\n\nX_test= sc.transform(X_test)\n\nX_submit= sc.transform(test_x1.values)\n","23c4b9b2":"g_nb = GaussianNB()\nknn = KNeighborsClassifier()\nran_for  = RandomForestClassifier()\nlog_reg = LogisticRegression()\nsvc = SVC()\ntree= DecisionTreeClassifier()\nxgb = XGBClassifier()\n\nada_boost = AdaBoostClassifier()\ngrad_boost = GradientBoostingClassifier()\nhist_grad_boost = HistGradientBoostingClassifier()","baff9032":"clf = [(\"Naive Bayes\",g_nb,{}),\\\n       (\"K Nearest\",knn,{\"n_neighbors\":[3,5,8],\"leaf_size\":[25,30,35]}),\\\n       (\"Random Forest\",ran_for,{\"n_estimators\":[100],\"random_state\":[42],\"min_samples_leaf\":[5,10,20,40,50],\"bootstrap\":[False]}),\\\n       (\"Logistic Regression\",log_reg,{\"multi_class\":['multinomial'],\"penalty\":['l2'],\"C\":[100, 10, 1.0, 0.1, 0.01] , \"solver\":['saga']}),\\\n       (\"Support Vector\",svc,{\"kernel\": [\"linear\",\"rbf\"],\"gamma\":['auto'],\"C\":[0.1, 1, 10, 100, 1000]}),\\\n       (\"Decision Tree\", tree, {}),\\\n       (\"XGBoost\",xgb,{\"n_estimators\":[100]  ,\"eval_metric\":['logloss'] , \"objective\":[\"multi:softprob\"],\"max_depth\":[3,4,5],\"learning_rate\":[.01,.1,.2],\"subsample\":[.8],\"colsample_bytree\":[1],\"gamma\":[0,1,5],\"lambda\":[.01,.1,1]}),\\\n       #, \"num_class\":[3] \n       (\"Adapative Boost\",ada_boost,{\"n_estimators\":[100],\"learning_rate\":[.6,.8,1]}),\\\n       (\"Gradient Boost\",grad_boost,{\"n_estimators\":[100]}),\\\n     \n       (\"Histogram GB\",hist_grad_boost,{\"loss\":[\"categorical_crossentropy\"],\"min_samples_leaf\":[5,10,20,40,50],\"l2_regularization\":[0,.1,1]})]\n\n","fe87cd5a":"stack_list=[]\ntrain_scores = pd.DataFrame(columns=[\"Name\",\"Train Score\",\"Test Score\"])","3b8f2e2b":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score","4b95a286":"i=0\nfor name,clf1,param_grid in clf:\n    clf = GridSearchCV(clf1,param_grid=param_grid,scoring=\"accuracy\",cv=sk_fold,return_train_score=True)\n    clf.fit(X_train,y_train) #.reshape(-1,1)\n    y_pred = clf.best_estimator_.predict(X_test)\n    cm = confusion_matrix(y_test,y_pred)\n    \n    #train_scores.loc[i]= [name,cross_val_score(clf,X_train,y_train,cv=sk_fold,scoring=\"accuracy\").mean(),(cm[0,0]+cm[1,1,])\/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])]\n    train_scores.loc[i]=  [name,clf.best_score_, accuracy_score(y_test,y_pred) ]\n    stack_list.append(clf.best_estimator_)\n    i=i+1\n    \n    \n    ","45fe961f":"est = [(\"g_nb\",stack_list[0]),\\\n       (\"knn\",stack_list[1]),\\\n       (\"ran_for\",stack_list[2]),\\\n       (\"log_reg\",stack_list[3]),\\\n       (\"svc\",stack_list[4]),\\\n       (\"dec_tree\",stack_list[5]),\\\n       (\"XGBoost\",stack_list[6]),\\\n       (\"ada_boost\",stack_list[7]),\\\n       (\"grad_boost\",stack_list[8]),\\\n       (\"hist_grad_boost\",stack_list[9])]","6216c530":"sc = StackingClassifier(estimators=est,final_estimator = None,cv=sk_fold,passthrough=False)\nsc.fit(X_train,y_train)\ny_pred = sc.predict(X_test)\ncm1 = confusion_matrix(y_test,y_pred)\ny_pred_train = sc.predict(X_train)\ncm2 = confusion_matrix(y_train,y_pred_train)\ntrain_scores.append(pd.Series([\"Stacking\",accuracy_score(y_train,y_pred_train), accuracy_score(y_test,y_pred)],index=train_scores.columns),ignore_index=True)","b39923a7":"#Import Libraries\nfrom sklearn.ensemble import VotingClassifier\n#----------------------------------------------------\n#Applying VotingClassifier Model \n'''\nensemble.VotingClassifier(estimators, voting=\u2019hard\u2019, weights=None,n_jobs=None, flatten_transform=None)\n'''\n#loading Voting Classifier\nVotingClassifierModel = VotingClassifier(estimators=[(\"grad_boost\",stack_list[8]),(\"svc\",stack_list[4]) , (\"ada_boost\",stack_list[7])], voting='hard')\nVotingClassifierModel.fit(X_train, y_train)\n\n\n#Calculating Details\nprint('VotingClassifierModel Train Score is : ' , VotingClassifierModel.score(X_train, y_train))\nprint('VotingClassifierModel Test Score is : ' , VotingClassifierModel.score(X_test, y_test))\nprint('----------------------------------------------------')\n","1012a8f7":"print(train_x1.shape)\nprint(y_target.shape)\nprint(X_submit.shape)","67db0821":"#Calculating Prediction\n\ny_submit = VotingClassifierModel.predict(X_submit)\nsubmit = pd.DataFrame({\n        \"building_id\": test_values_df.building_id,\n        \"damage_grade\": y_submit\n    })","80cd8ce0":"submit.building_id = submit.building_id.astype(int)\nsubmit.damage_grade = submit.damage_grade.astype(int)\nsubmit.to_csv(\"Earth_qucke_submit.csv\", index=False)","10cfc176":"## Outline\nThe following sections are included in this notebook:\n\n### A. [Information About the data](#section-zero)\n\n### B. [Load and Parse Data](#section-one)\n\n### C. [Exploratory Data Analysis (EDA)](#section-two)\n   1. [Missing Data](#section-two-a)    \n   2. [Distribution of the Target Variable](#section-two-b)    \n   3. [Distribution of the Numeric Feature Variable](#section-two-c)\n   4. [Outliers](#section-two-d)\n   5. [Categorical Feature Cardinality](#section-two-e)\n    \n### D. [Feature Engineering](#section-three)\n   1. [Initial Preprocessing](#section-three-a)\n   2. [Building a Preprocessing Pipeline](#section-three-b)\n       * [Define custom transformers](#section-three-b1)\n       * [Define helper functions](#section-three-b2)\n       * [Define Training Data](#section-three-b3)\n       * [Define datatypes and encoding](#section-three-b4)\n      \n      \n        \n### E. [Fit and Evaluate the Model](#section-four)\n   1. [Cross-Validation](#section-four-a)\n       \n       * Naive Bayes classifier\n       * KNN or k-Nearest Neighbors  \n       * Random Forest \n       * Logistic Regression             \n       * Support Vector Machines  \n       * Decision Tree\n       * XGBOOST Classifier \n       * AdaBoosting Classifier\n       * GradientBoostingClassifier \n       * HistGradientBoostingClassifier\n       * Principal Component Analysis (PCA) \n       * Gaussian Mixture \n       * Grid Search\n      \n   \n   2. [Model Stacking](#section-four-c)\n    \n### F. [Predict Test Dataset and Submit](#section-five)","e7356e25":"<a id=\"section-zero\"><\/a>\n# A. Information About the data","441136e5":"Coming to the modeling part. We first scale the data using standard scaler. We use grid search with stratified kfold validation for 9 algorithms. We get the scores from the cross validation for all these models and run a prediction on the test data from our train_test_split. For stacking we get the accuracy based on fitting the train and test set.","5c9f12d8":"<a id=\"section-one\"><\/a>\n# B. Load and Parse Data","652c35fe":"from __future__ import print_function\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(__doc__)\n\n# Loading the Breast cancer dataset\n#cancer = datasets.load_breast_cancer()\n\n\n# Preprocessing the dataset\n#X = cancer['data']\ny = y_target\nscaler = StandardScaler()\nX = scaler.fit_transform(train_x1)\n\n# Split the dataset in two equal parts into 80:20 ratio for train:test\nX_train, X_test, y_train, y_test = train_test_split(X ,y , test_size=0.2, random_state=0)\n\n# This is a key step where you define the parameters and their possible values\n# that you would like to check.\ntuned_parameters = [[{'kernel': ['rbf', 'linear'], 'gamma': [1e-3, 1e-4],'C': [1, 10, 100, 1000]}],\n                    [{'max_depth' : [10,100,1000,10000], 'min_samples_split' : [2,10,100], 'min_samples_leaf': [1,5,10], 'max_features' : [\"sqrt\",\"log2\"]}],\n                    [{'activation' : ['logistic','tanh','relu'],'hidden_layer_sizes' : [(5,),(10,)], 'max_iter' : [200,1000],'alpha' : [0.0001,0.0005]}],\n                    [{}], #GaussianNB\n                    [{'penalty': ['l1','l2'], 'tol' : [1e-4,1e-5], 'max_iter' : [10,100,1000], 'fit_intercept' : [True, False]}],\n                    [{'n_neighbors' : [5,10,20],'weights': ['uniform','distance'], 'algorithm' : ['ball_tree', 'kd_tree', 'brute'],'p' : [1,2,3]}],\n                    [{'n_estimators':[10,20,100],'max_samples':[0.5,1.0],'max_features':[0.5,1.0],'random_state':[None]}],\n                    [{'n_estimators':[10,20,100],'max_features':[0.5,1.0],'criterion' : ['gini','entropy'],'max_depth': [None,100,200]}],\n                    [{'n_estimators' : [50,100,200],'random_state' : [None], 'learning_rate' : [1.,0.8,0.5],'algorithm' : ['SAMME','SAMME.R']}],\n                    [{'loss': ['deviance', 'exponential'],'n_estimators' : [100,200,500],'max_features' : ['log2','sqrt'],'max_depth' : [3,10,50] }],\n                    [{'booster': ['gbtree', 'gblinear' ,'dart'], 'learning_rate' : [0.1,0.05,0.2], 'min_child_weight' : [1], 'max_delta_step' : [0]}]\n                    ]\nalgorithms = [SVC(),DecisionTreeClassifier(),MLPClassifier(),GaussianNB(),LogisticRegression(),KNeighborsClassifier(),BaggingClassifier(),RandomForestClassifier(),AdaBoostClassifier(),GradientBoostingClassifier(),XGBClassifier(objective = \"multi:softmax\")]\nalgorithm_names = [\"SVC\",\"DecisionTreeClassifier\",\"MLPClassifier\",\"GaussianNB\",\"LogisticRegression\",\"KNeighborsClassifier\",\"BaggingClassifier\",\"RandomForestClassifier\",\"AdaBoostClassifier\",\"GradientBoostingClassifier\",\"XGBClassifier\"]\n\nfor i in range(0, 11):\n    print(\"################   %s   ################\" %algorithm_names[i])\n    #scores = ['precision_macro','recall_macro','accuracy']\n    scores = ['precision_macro']\n\n    for score in scores:\n        print(\"# Tuning hyper-parameters for %s\" % score)\n        print()\n\n        clf = GridSearchCV(algorithms[i], tuned_parameters[i], cv=5,\n                           scoring='%s' % score)\n        clf.fit(X_train, y_train)\n\n        print(\"Best parameters set found on development set:\")\n        print()\n        print(clf.best_params_)\n        print()\n        print(\"Grid scores on development set:\")\n        print()\n        means = clf.cv_results_['mean_test_score']\n        stds = clf.cv_results_['std_test_score']\n        \n        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n            print(\"%0.3f (+\/-%0.03f) for %r\"\n                  % (mean, std * 2, params))\n        print()\n\n        print(\"Detailed classification report:\")\n        print()\n        print(\"The model is trained on the full development set.\")\n        print(\"The scores are computed on the full evaluation set.\")\n        print()\n        y_true, y_pred = y_test, clf.predict(X_test)\n        print(classification_report(y_true, y_pred))\n        print(\"Detailed confusion matrix:\")\n        print(confusion_matrix(y_true, y_pred))\n        print(\"Precision Score: \\n\")\n        print(precision_score(y_true, y_pred , average='micro'))\n        \n        print()","747a6f33":"#### Traing Models with Feature Scaling","fbf81954":"We fit and predict on the best SVC model that we derived based on the scores.","5bdf8f78":"<a id=\"section-four\"><\/a>\n# E. Fit and Evaluate the Model \n","955882bb":"## Encoding categorical features:","f73d6b1b":"<a id=\"section-three\"><\/a>\n# D. Feature Engineering","6bdeab37":"Exporting the data to submit.","3e673959":"# Abdelwahed","b5abaa2f":"## Feature Scaling\nThe StandardScaler assumes your data is normally distributed within each feature and will scale them such that the distribution is now centred around 0, with a standard deviation of 1.","31a2f199":"<a id=\"section-two-b\"><\/a>\n### 2. Duplicate Rows","e6db32a6":"# Abdelwahed","7156e4b1":"## split data into :\n1. Categorical features\n2. Numerical features","34183f69":"<a id=\"section-two-d\"><\/a>\n### 4. shape","dd7f9106":"<a id=\"section-two-a\"><\/a>\n### 1. Missing Data","26985252":"<a id=\"section-two\"><\/a>\n# C. Exploratory Data Analysis (EDA)","1aa38034":"<a id=\"section-two-c\"><\/a>\n### 3. Data Type","59f0b864":"<a id=\"section-five\"><\/a>\n# F. Predict Test Dataset and Submit\n","5b41541a":"<a id=\"section-two-e\"><\/a>\n### 5.  Descriptive statistics","7f68df57":"# Title: Richter's Predictor: Modeling Earthquake Damage\n\n\n\n## About:\n\nData taken from DRIVENDATA \n\nThe totality of the data is available through the 2015 Nepal Earthquake Open Data Portal.\n\nIn their own words:\n\nFollowing the 7.8 Mw Gorkha Earthquake on April 25, 2015, Nepal carried out a massive household\nsurvey using mobile technology to assess building damage in the earthquake-affected districts. \nAlthough the primary goal of this survey was to identify beneficiaries eligible for government \nassistance for housing reconstruction, it also collected other useful socio-economic information. \nIn addition to housing reconstruction, this data serves a wide range of uses and users\ne.g. researchers, newly formed local governments, and citizens at large. The purpose of this portal\nis to open this data to the public.\n\n\n## Problem description:\n\nWe're trying to predict the ordinal variable damage_grade, \nwhich represents a level of damage to the building that was hit by the earthquake.\nThere are 3 grades of the damage:\n\n1 represents low damage\n2 represents a medium amount of damage\n3 represents almost complete destruction\n\n\n## Features\n\nThe dataset mainly consists of information on the buildings' structure and their legal ownership.\nEach row in the dataset represents a specific building in the region that was hit by Gorkha earthquake.\n\n\nThere are 39 columns in this dataset, where the building_id column is a unique and random identifier.\nThe remaining 38 features are described in the section below.\nCategorical variables have been obfuscated random lowercase ascii characters.\nThe appearance of the same character in distinct columns does not imply the same original value.\n\n## Description\n\n1. geo_level_1_id, geo_level_2_id, geo_level_3_id (type: int): geographic region in which building\nexists, from largest (level 1) to most specific sub-region (level 3).\nPossible values: level 1: 0-30, level 2: 0-1427, level 3: 0-12567.\n\n2. count_floors_pre_eq (type: int): number of floors in the building before the earthquake.\n\n3. age (type: int): age of the building in years.\n\n4. area_percentage (type: int): normalized area of the building footprint.\n\n5. height_percentage (type: int): normalized height of the building footprint.\n\n6. land_surface_condition (type: categorical): surface condition of the land where the building was built. Possible values: n, o, t.\n\n7. foundation_type (type: categorical): type of foundation used while building. Possible values: h, i, r, u, w.\n\n8. roof_type (type: categorical): type of roof used while building. Possible values: n, q, x.\n\n9. ground_floor_type (type: categorical): type of the ground floor. Possible values: f, m, v, x, z.\n\n10. other_floor_type (type: categorical): type of constructions used in higher than the ground floors (except of roof). Possible values: j, q, s, x.\n\n11. position (type: categorical): position of the building. Possible values: j, o, s, t.\n\n12. plan_configuration (type: categorical): building plan configuration. Possible values: a, c, d, f, m, n, o, q, s, u.\n\n13. has_superstructure_adobe_mud (type: binary): flag variable that indicates if the superstructure was made of Adobe\/Mud\n\n14. has_superstructure_mud_mortar_stone (type: binary): flag variable that indicates if the superstructure was made of Mud Mortar - Stone.\n\n15. has_superstructure_stone_flag (type: binary): flag variable that indicates if the superstructure was made of Stone.\n\n16. has_superstructure_cement_mortar_stone (type: binary): flag variable that indicates if the superstructure was made of Cement Mortar - Stone.\n\n17. has_superstructure_mud_mortar_brick (type: binary): flag variable that indicates if the superstructure was made of Mud Mortar - Brick.\n\n18. has_superstructure_cement_mortar_brick (type: binary): flag variable that indicates if the superstructure was made of Cement Mortar - Brick.\n\n19. has_superstructure_timber (type: binary): flag variable that indicates if the superstructure was made of Timber.\n\n20. has_superstructure_bamboo (type: binary): flag variable that indicates if the superstructure was made of Bamboo.\n\n21. has_superstructure_rc_non_engineered (type: binary): flag variable that indicates if the superstructure was made of non-engineered reinforced concrete.\n\n22. has_superstructure_rc_engineered (type: binary): flag variable that indicates if the superstructure was made of engineered reinforced concrete.\n\n23. has_superstructure_other (type: binary): flag variable that indicates if the superstructure was made of any other material.\n\n\n24. legal_ownership_status (type: categorical): legal ownership status of the land where building was built. Possible values: a, r, v, w.\n\n25. count_families (type: int): number of families that live in the building.\n\n26. has_secondary_use (type: binary): flag variable that indicates if the building was used for any secondary purpose.\n\n27. has_secondary_use_agriculture (type: binary): flag variable that indicates if the building was used for agricultural purposes.\n\n28. has_secondary_use_hotel (type: binary): flag variable that indicates if the building was used as a hotel.\n\n29. has_secondary_use_rental (type: binary): flag variable that indicates if the building was used for rental purposes.\n\n30. has_secondary_use_institution (type: binary): flag variable that indicates if the building was used as a location of any institution.\n\n31. has_secondary_use_school (type: binary): flag variable that indicates if the building was used as a school.\n\n32. has_secondary_use_industry (type: binary): flag variable that indicates if the building was used for industrial purposes.\n\n33. has_secondary_use_health_post (type: binary): flag variable that indicates if the building was used as a health post.\n\n34. has_secondary_use_gov_office (type: binary): flag variable that indicates if the building was used fas a government office.\n\n35. has_secondary_use_use_police (type: binary): flag variable that indicates if the building was used as a police station.\n\n36. has_secondary_use_other (type: binary): flag variable that indicates if the building was secondarily used for other purposes.\n\n\n","7df9e9ed":"We further split the training set in to a train and test set to validate our model.","0bbabde4":"\npred = clf.best_estimator_.predict(X_test)"}}