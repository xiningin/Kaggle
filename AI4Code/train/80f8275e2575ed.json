{"cell_type":{"97c52304":"code","fb8a82d2":"code","a81b43ff":"code","67065f34":"code","f62dae9c":"code","0f5fb67f":"code","307efdd8":"code","6ad0f3b4":"code","3d8a72b3":"code","6f3822bb":"code","23457121":"code","0bff563f":"code","5111eb7b":"code","fbeaf65a":"code","a519a684":"code","14743290":"code","28d3b676":"code","2fffac5c":"code","f909d944":"code","d27a3a88":"code","f0b106f7":"code","073bca11":"code","1dfe4cba":"code","aa6569a8":"code","4334535c":"code","325a16ef":"code","9c5cdc3a":"code","accf1bac":"markdown","2cb1ee3b":"markdown","1ed1b158":"markdown","eef83d61":"markdown","164b8b3a":"markdown","6550bbbc":"markdown","cfd310c5":"markdown","5e9ea3c9":"markdown","f47d73b7":"markdown","c9bd7dc5":"markdown","709b42f8":"markdown","2dc157e1":"markdown","17626c33":"markdown","64517971":"markdown","968ffed4":"markdown"},"source":{"97c52304":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fb8a82d2":"import pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\nimport matplotlib.pyplot as plt","a81b43ff":"train=pd.read_csv(r'\/kaggle\/input\/cross-sell-prediction\/train.csv')","67065f34":"train.head()","f62dae9c":"#encoding Gender, Vehicle_Age and vehicle_Damage columns\n\ntrain['Gender'].replace('Male',1,inplace=True)\ntrain['Gender'].replace('Female',0,inplace=True)\n\ntrain['Vehicle_Damage'].replace('Yes',1,inplace=True)\ntrain['Vehicle_Damage'].replace('No',0,inplace=True)\n\ntrain['Vehicle_Age'].replace('1-2 Year',0,inplace=True)\ntrain['Vehicle_Age'].replace('< 1 Year',1,inplace=True)\ntrain['Vehicle_Age'].replace('> 2 Years',2,inplace=True)","0f5fb67f":"train.head()","307efdd8":"test=pd.read_csv(r'\/kaggle\/input\/cross-sell-prediction\/test.csv')","6ad0f3b4":"test.head()","3d8a72b3":"#encoding Gender, Vehicle_Age and vehicle_Damage columns for test set\n\ntest['Gender'].replace('Male',1,inplace=True)\ntest['Gender'].replace('Female',0,inplace=True)\n\ntest['Vehicle_Damage'].replace('Yes',1,inplace=True)\ntest['Vehicle_Damage'].replace('No',0,inplace=True)\n\ntest['Vehicle_Age'].replace('1-2 Year',0,inplace=True)\ntest['Vehicle_Age'].replace('< 1 Year',1,inplace=True)\ntest['Vehicle_Age'].replace('> 2 Years',2,inplace=True)","6f3822bb":"Xtrain=train.drop(['id','Response'], axis=1)\nytrain=train['Response']\n\nXtest=test.drop('id',axis=1)","23457121":"Xtrain.shape, ytrain.shape, Xtest.shape","0bff563f":"#creating train and dev set\nX_train,X_dev,y_train,y_dev=train_test_split(Xtrain, ytrain, test_size=0.2)","5111eb7b":"basic_model=DecisionTreeClassifier()\nbasic_model.fit(X_train,y_train)","fbeaf65a":"basic_model_pred=basic_model.predict(X_dev)","a519a684":"print(classification_report(y_dev,basic_model_pred))","14743290":"# we'll iterate over 30 max_depths to see which gives the best result\n# we'll try out different criterion as well (gini is computationally faster with respect to entropy)\n# we'll plot max depth vs accuracy\nmax_depth=[]\nacc_gini=[]\nacc_entropy=[]\nfor i in range(1,30):\n    dtree=DecisionTreeClassifier(criterion='gini',max_depth=i)\n    dtree.fit(X_train,y_train)\n    pred=dtree.predict(X_dev)\n    \n    acc_gini.append(accuracy_score(y_dev,pred))\n    \n    dtree=DecisionTreeClassifier(criterion='entropy',max_depth=i)\n    dtree.fit(X_train,y_train)\n    pred=dtree.predict(X_dev)\n    \n    acc_entropy.append(accuracy_score(y_dev,pred))\n    \n    max_depth.append(i)\n    \n    d=pd.DataFrame({'acc_gini': pd.Series(acc_gini),\n                   'acc_entropy': pd.Series(acc_entropy),\n                   'max_depth': pd.Series(max_depth)})\n    \nplt.plot('max_depth','acc_gini', data=d, label='gini')\nplt.plot('max_depth','acc_entropy', data=d, label='entropy')\nplt.xlabel('max_depth')\nplt.ylabel('accuracy')\nplt.legend()","28d3b676":"pruned_model=DecisionTreeClassifier(criterion='gini', max_depth=3)\npruned_model.fit(X_train,y_train)","2fffac5c":"pruned_model_pred=pruned_model.predict(X_dev)","f909d944":"print(classification_report(y_dev,pruned_model_pred, zero_division='warn'))","d27a3a88":"# we are going to tune criterion, depth, min_samples_split, min_samples_leaf\ncriterion=['gini', 'entropy']\nmax_depth=[1,6,8,11]\nmin_samples_split=[1,9,11,12]\nmin_samples_leaf=[1,3,7,9]\nparam_grid={'max_depth':max_depth,\n            'min_samples_split':min_samples_split,\n           'min_samples_leaf':min_samples_leaf,\n           'criterion':criterion}","f0b106f7":"param=dict(criterion='criterion',max_depth='max_depth', min_samples_split='min_samples_split',min_samples_leaf='min_samples_leaf')","073bca11":"#cv=3 i.e 3 cross folds, verbose=200 i.e the more the verbose the more info we get\ngrid=GridSearchCV(DecisionTreeClassifier(),param_grid,n_jobs=-1,cv=3,verbose=200)\ngrid.fit(X_train,y_train)","1dfe4cba":"#best parameters that achieves higher score\ngrid.best_params_","aa6569a8":"grid.best_score_","4334535c":"tuned_model=DecisionTreeClassifier(criterion= 'gini',\n max_depth=1,\n min_samples_leaf= 1,\n min_samples_split= 9)\ntuned_model.fit(X_train,y_train)","325a16ef":"tuned_model_pred=tuned_model.predict(X_dev)","9c5cdc3a":"print(classification_report(y_dev,tuned_model_pred))","accf1bac":"# Note: Dataset is imbalance but in this notebook I'll demonstrate only tuning and pruning","2cb1ee3b":"## No of candidates can be calculates using multiplying the no of values for each parameter and we are fitting them 3 times(CV=3)","1ed1b158":"# 4. Hyperparameter tuning","eef83d61":"## In this way you can perform the parameter tuning of your model","164b8b3a":"## We need to predict Response for test dataset****","6550bbbc":"<center>\n<table >\n  <tr>\n    <th>Algorithm<\/th>\n    <th>Accuracy<\/th> \n  \n  <\/tr>\n  <tr>\n    <td>Basic<\/td>\n    <td>82 %<\/td>\n  <\/tr>\n    <tr>\n    <td>Pruned<\/td>\n    <td>87 %<\/td>\n  <\/tr>\n    <tr>\n    <td>Tuned <\/td>\n    <td>87.5 %<\/td>\n  <\/tr>\n  \n<\/table>\n<\/center>","cfd310c5":"# 3. Pruned Decision Tree","5e9ea3c9":"# 5. Result Comparison","f47d73b7":"# 1. Importing necessary libraries","c9bd7dc5":"We can see the drop in accuracy as we take max depth greater than 10 and the best accuracy is obtained at max_depth=1 to 3 which is 87.5% with both the criterias","709b42f8":"# with best parameters the accuracy is 87.8%","2dc157e1":"# 2. Basic decision tree model","17626c33":"# In this notebook I've explained how to perform decision tree pruning and tuning","64517971":"<center><img src='https:\/\/tul.imgix.net\/content\/general\/puzzle.jpg?auto=format,compress&w=1200&h=630&fit=crop'><\/center>","968ffed4":"## Data is now ready to be fed into model"}}