{"cell_type":{"bb81b2aa":"code","8b1b29d1":"code","4499263a":"code","a90a23fd":"code","724684b0":"code","55aa261c":"code","5e98e6ee":"code","3074b17c":"code","2699779d":"code","ff98d319":"code","19b7657f":"code","431d1293":"code","73930bee":"code","29f5890a":"code","a168159a":"code","9bae8fbe":"code","8a69eecc":"code","c0b153df":"code","5a26590e":"code","2e412277":"code","669de55a":"code","b6706587":"code","36daa265":"code","8cc14e93":"code","1f523ab1":"code","317f6188":"code","b6af3abc":"code","2580dbf8":"code","51eb1b3f":"code","38971dae":"code","9fefd1e4":"code","b8b5b67a":"code","0f855a16":"code","2c3b9595":"code","261730f8":"markdown","0c2ca9f8":"markdown","e97f77cc":"markdown","fa37b9ec":"markdown","9dc53e51":"markdown","e82fa8d1":"markdown","a1864798":"markdown","1b50ad8c":"markdown","ee9e6a77":"markdown","2b65bdf1":"markdown","8d648337":"markdown","4af82aec":"markdown","e3c8eff0":"markdown","803d059e":"markdown","e02126ce":"markdown","2e62a4f7":"markdown","ceba38cc":"markdown","51868f95":"markdown"},"source":{"bb81b2aa":"from IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n    horizontal-align: middle;\n}\nh1{\n    text-align: center;\n    background-color: #B983FF;\n    padding: 20px;\n    margin: 0;\n    color: white;\n    font-family: ariel;\n    border-radius: 80px;\n}\n\nh2 {\n    text-align: center;\n    background-color: #94B3FD;\n    padding: 20px;\n    margin: 0;\n    color: white;\n    font-family: ariel;\n    border-radius: 80px;\n}\n\nh3 {\n    text-align: center;\n    border-style: solid;\n    border-width: 3px;\n    padding: 12px;\n    margin: 0;\n    color: black;\n    font-family: ariel;\n    border-radius: 80px;\n    border-color: #99FEFF;\n}\n\nbody, p {\n    font-family: ariel;\n    font-size: 15px;\n    color: charcoal;\n}\ndiv {\n    font-size: 14px;\n    margin: 0;\n\n}\n\nh4 {\n    padding: 0px;\n    margin: 0;\n    font-family: ariel;\n    color: purple;\n}\n<\/style>\n\"\"\")","8b1b29d1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4499263a":"#loading the data\nX = pd.read_csv('..\/input\/hardwrk\/Linear_X_Train.csv')\ny = pd.read_csv('..\/input\/hardwrk\/Linear_Y_Train.csv')","a90a23fd":"#converting to numpy arrays\nX=X.values\ny=y.values","724684b0":"#normalisation of data\nu=X.mean()\nstd=X.std()\nX=(X-u)\/std","55aa261c":"# Visualise\nplt.style.use('seaborn')\nplt.scatter(X,y,color='blue')\nplt.title(\"Hardwork vs Performance Graph\")\nplt.xlabel(\"Hardwork\")\nplt.ylabel(\"Performance\")\nplt.show()","5e98e6ee":"def hypothesis(x,theta):\n    y_ = theta[0] + theta[1]*x \n    return y_","3074b17c":"def error(X,Y,theta):\n    m = X.shape[0] #total no. of examples\n    total_error = 0.0\n    for i in range(m):\n        y_ = hypothesis(X[i],theta)\n        total_error += (y_ - Y[i])**2\n        \n    return (total_error\/m) ","2699779d":"def gradient(X,Y,theta):\n    m = X.shape[0]\n    grad = np.zeros((2,))\n    for i in range(m):\n        x = X[i]\n        y_ = hypothesis(x,theta)\n        y = Y[i]\n        grad[0] += (y_ - y)\n        grad[1] += (y_ - y)*x\n    return grad\/m","ff98d319":"def gradientDescent(X,Y,max_steps=100,learning_rate =0.1):\n    \n    theta = np.zeros((2,))\n    error_list = []\n    theta_list = []\n    \n   \n    for i in range(max_steps):\n        \n        # Compute grad\n        grad = gradient(X,Y,theta)\n        e = error(X,Y,theta)[0]\n        \n        \n        #Update theta\n        theta[0] = theta[0] - learning_rate*grad[0]\n        theta[1] = theta[1] - learning_rate*grad[1]\n        # Storing the theta values during updates\n        theta_list.append((theta[0],theta[1]))\n        error_list.append(e)\n        \n    return theta,error_list,theta_list","19b7657f":"theta,error_list,theta_list = gradientDescent(X,y)","431d1293":"theta","73930bee":"plt.plot(error_list)\nplt.title(\"Reduction error over time\")\nplt.show()","29f5890a":"## Predictions and Best Line\ny_ = hypothesis(X,theta)\nprint(y_)","a168159a":"# Training + Predictions\nplt.scatter(X,y)\nplt.plot(X,y_,color='orange',label=\"Prediction\")\nplt.legend()\nplt.show()","9bae8fbe":"#coeff of determination\ndef r2_score(Y,Y_):\n    # Instead of Loop, np.sum is recommended as it is fast\n    num = np.sum((Y-Y_)**2)\n    denom = np.sum((Y- Y.mean())**2)\n    score = (1- num\/denom)\n    return score*100","8a69eecc":"r2_score(y,y_)","c0b153df":"from sklearn.datasets import load_boston","5a26590e":"boston = load_boston()\nX = boston.data\ny = boston.target","2e412277":"print(boston.DESCR)","669de55a":"import pandas as pd\ndf = pd.DataFrame(X)\ndf.columns = boston.feature_names\ndf.head()","b6706587":"df= df.drop(['B'],axis=1) ","36daa265":"df.head()","8cc14e93":"X=df.values","1f523ab1":"# Normalise this dataset\n# Each feature must have 0 mean, unit variance\nimport numpy as np\nu = np.mean(X,axis=0)\nstd = np.std(X,axis=0)","317f6188":"u = np.mean(X,axis=0)\nstd = np.std(X,axis=0)","b6af3abc":"# Normalise the Data\nX = (X-u)\/std","2580dbf8":"# Normalised Data\npd.DataFrame(X[:5,:]).head()","51eb1b3f":"def hypothesis(x,theta):\n    # Optimized :- return np.dot(X,theta)\n    y_ = 0.0\n    n = x.shape[0]\n    for i in range(n):\n        y_  += (theta[i]*x[i])\n    return y_","38971dae":"def error(X,y,theta):\n    e = 0.0\n    m = X.shape[0]\n    \n    for i in range(m):\n        y_ = hypothesis(X[i],theta)\n        e += (y[i] - y_)**2\n        # Optimized :- e = np.sum((y-y_)**2)    \n    return e\/m","9fefd1e4":"def gradient(X,y,theta):\n    m,n = X.shape\n    \n    grad = np.zeros((n,))\n    \n    # for all values of j\n    for j in range(n):\n        #sum over all examples\n        for i in range(m):\n            y_ = hypothesis(X[i],theta)\n            grad[j] += (y_ - y[i])*X[i][j]\n            #Optimization :- grad = np.dot(X.T,(y_ - y))\n            \n    # Out of the loops\n    return grad\/m","b8b5b67a":"def gradient_descent(X,y,learning_rate=0.1,max_epochs=300):\n    m,n = X.shape\n    theta = np.zeros((n,))\n    error_list = []\n    \n    for i in range(max_epochs):\n        e = error(X,y,theta)\n        error_list.append(e)\n        \n        # Gradient Descent\n        grad = gradient(X,y,theta)\n        for j in range(n):\n            theta[j] = theta[j] - learning_rate*grad[j]\n        \n    return theta,error_list","0f855a16":"theta,error_list = gradient_descent(X,y)\n","2c3b9595":"import matplotlib.pyplot as plt\nplt.plot(error_list)\nplt.show()","261730f8":" ### Gradient Descent \nGradient descent in our context is an optimization algorithm that aims to adjust the parameters in order to minimize the cost function . So we multiply the derivative of the cost function with the learning rate(\u03b1) and subtract it from the present value of the parameters(\u03b8) to get the new updated parameters(\u03b8).\n\n#### Below Code is an implementation of the steps taken in Gradient Descent:- \n\nStep 1 :- Initialize theta with random values\n\nStep 2 :- Measure how good is our theta using the error function. Best Theta is the one for which error is the least.\n\nStep 3 :- Calculate the gradient used to update the theta values\n\nStep 4 :- Update the theta values \n","0c2ca9f8":"## If you like my work, feel free to upvote and comment !\u00b6\n#### I'd love to hear your thoughts about this notebook! Constructive Criticism is always welcomed\ud83d\ude04 Do share any better method, model or techniques I should try and improvements in this notebook(if any). I hope you like my work \u2728 Thankyou for your time\ud83d\ude4f","e97f77cc":"### Gradient Descent \nGradient descent in our context is an optimization algorithm that aims to adjust the parameters in order to minimize the cost function . So we multiply the derivative of the cost function with the learning rate(\u03b1) and subtract it from the present value of the parameters(\u03b8) to get the new updated parameters(\u03b8).\n\n#### Below Code is an implementation of the steps taken in Gradient Descent:- \n\nStep 1 :- Initialize theta with random values\n\nStep 2 :- Measure how good is our theta using the error function. Best Theta is the one for which error is the least.\n\nStep 3 :- Calculate the gradient used to update the theta values\n\nStep 4 :- Update the theta values ","fa37b9ec":"## Refer to all the detailed derivations in my handwritten notes on GitHub :- \nhttps:\/\/github.com\/vanshika230\/Machine-Learning\/tree\/main\/AlgorithmsHandwrittenNotes","9dc53e51":"### Hypothesis \n\nWe will use $\\mathbf{x_i}$ to denote the independent variable and $\\mathbf{y_i}$ to denote dependent variable. A pair of $\\mathbf{(x_i,y_i)}$ is called training example. The subscript $\\mathbf{i}$ in the notation is simply index into the training set. We have $\\mathbf{m}$ training example then $\\mathbf{i = 1,2,3,...m}$.\n\nThe goal of supervised learning is to learn a *hypothesis function $\\mathbf{h}$*, for a given training set that can used to estimate $\\mathbf{y}$ based on $\\mathbf{x}$. So hypothesis fuction represented as \n\n$$\\mathbf{ h_ = \\theta_0 + \\theta_1x_i }$$   \n\n$\\mathbf{\\theta_0,\\theta_1}$ are parameter of hypothesis.This is equation for **Simple \/ Univariate Linear regression**. ","e82fa8d1":"### Gradient\n\nThe idea of any machine learning algorithm is to minimize the cost function so that the hypothesis is close to the original dependent variable. We need to optimize the theta value to do that. If we take the partial derivative of the cost function based on theta0 and theta1 respectively, we will get the gradient descent. To update the theta values we need to deduct the gradient descent from the corresponding theta values:\n\n![1_5_3TeXfjy-2uTXEe1dms9A.png](attachment:1ca26676-0d51-4a9b-b10f-1e461b471897.png)\n\n#### In the Code below we implement the highlighted part:- \n\n![now.jpg](attachment:e35ef028-3723-42e6-aa83-b9ba8f21bf40.jpg)","a1864798":"## Preparing the Dataset","1b50ad8c":"## References \n1. https:\/\/towardsdatascience.com\/basic-linear-regression-algorithm-in-python-for-beginners-c519a808b5f8\n2. https:\/\/towardsdatascience.com\/coding-linear-regression-from-scratch-c42ec079902\n3. https:\/\/www.geeksforgeeks.org\/python-coefficient-of-determination-r2-score\/\n4. https:\/\/medium.com\/geekculture\/linear-regression-from-scratch-in-python-without-scikit-learn-a06efe5dedb6\n5. https:\/\/www.edureka.co\/blog\/linear-regression-in-python\/\n6. https:\/\/www.geeksforgeeks.org\/ml-linear-regression\/\n7. https:\/\/www.kaggle.com\/hamidhaghshenas\/high-quality-mathematical-equations-using-latex\n8. https:\/\/towardsdatascience.com\/multivariate-linear-regression-in-python-step-by-step-128c2b127171\n9. https:\/\/satishgunjal.com\/multivariate_lr\/","ee9e6a77":"### Error Function \nThe cost function is the indication of the difference between the original output and the predicted output. The idea of a machine learning algorithm is to minimize the cost function so that the difference between the original output and the predicted output is closer.\n![Cost_Function_Formula.png](attachment:d1f835eb-508b-4291-a369-c53a71061e48.png)\n\nThe below code is the exact implementation of this equation :- ","2b65bdf1":"# \u2728Multivariate Linear Regression\u2728","8d648337":"#### **NOTE :-**\nIn the description we have things like crime in the neighborhood,how industrious is the area etc. But the one thing that is quite concerning is apparently this is a data set where the proportion of blacks in your town is something that is taken into consideration for predicting house prices.Looking at this feature it's clear that we have a potential for a racist algorithm and that is a property we should not want to have in the final model. It's our responsibility to educate ourself to make sure that this doesn't happen, if the output of a machine learning model is our responsibility then so is the data going in. We should make sure that we're up to date onthemes of ethics in algorithmic design. So I am going to drop this column.\n\nRead more about this here :- https:\/\/medium.com\/@docintangible\/racist-data-destruction-113e3eff54a8","4af82aec":"#### Metric for Evaluation :- \n#### **R2 SCORE** :-\n\nCoefficient of determination also called as R2 score is used to evaluate the performance of a linear regression model. It is the amount of the variation in the output dependent attribute which is predictable from the input independent variable(s). It is used to check how well-observed results are reproduced by the model, depending on the ratio of total deviation of results described by the model.\n\nR2= 1- SSres \/ SStot\n\n","e3c8eff0":"# \u2728Simple Linear Regression\u2728","803d059e":"### Hypothesis\n\nFor **Multiple Linear regression** more than one independent variable exit then we will use $\\mathbf{x_{ij}}$ to denote indepedent variable and $\\mathbf{y_{i}}$ to denote dependent variable. We have $\\mathbf{n}$ independent variable then $\\mathbf{j=1,2,3 ..... n}$. The hypothesis function represented as\n\n$$\\mathbf{h_\\theta(x_{i}) = \\theta_0 + \\theta_1x_{i1} + \\theta_2 x_{i2} + ..... \\theta_j x_{ij} ...... \\theta_n  x_{mn} }$$\n$\\mathbf{\\theta_0,\\theta_1,....\\theta_j....\\theta_n }$ are parameter of hypothesis,\n\n$\\mathbf{m}$ Number of training exaples,\n\n$\\mathbf{n}$ Number of independent variable,\n\n$\\mathbf{x_{ij}}$ is $\\mathbf{i^{th}}$ training exaple of $\\mathbf{j^{th}}$ feature.\n\nWe introduce $\\mathbf{x_0}$ =1 so that we can convert this to a summation term easily \n\n$$\\mathbf{h_\\theta(x_{i}) = \\theta_0x_{i0} + \\theta_1x_{i1} + \\theta_2 x_{i2} + ..... \\theta_j x_{ij} ...... \\theta_n  x_{mn} }$$\n\n$$\\sum\\limits_{x=1}^{m} {\\theta_ix_{i} } = J (\\theta_0)$$\n\nThe below code is an implementation of this sum function ","e02126ce":"## Preparing the Dataset","2e62a4f7":"# Implementation of Linear Regression from Scratch in Python \n### \u2728 WELCOME \u2728\nLinear regression is a supervised learining algorithm used when target \/ dependent variable continues real number. Linear regression analysis is used to predict the value of a variable based on the value of another variable. The variable you want to predict is called the dependent variable. The variable you are using to predict the other variable's value is called the independent variable.Linear regression fits a straight line or surface that minimizes the discrepancies between predicted and actual output values. When there is a single input variable (x), the method is referred to as simple linear regression. When there are multiple input variables, literature from statistics often refers to the method as multiple linear regression.Different techniques can be used to prepare or train the linear regression equation from data, the most common of which is called Ordinary Least Squares.\n\nLinear regression uses the very basic idea of prediction. Here is the formula:\n\nY = C + BX\n\nHere, Y is the dependent variable, B is the slope and C is the intercept.\n\nFor linear regression, it is written as:\n\n$$\\mathbf{ h_ = \\theta_0 + \\theta_1x_i }$$   \n\nHere, \u2018h\u2019 is the hypothesis or the predicted dependent variable, X is the input feature, and theta0 and theta1 are the coefficients. Theta values are initialized randomly to start with. \n\n![unnamed-chunk-2-1.png](attachment:8714331d-ccd8-4759-82c7-d02b669230db.png)\n\nAll three lines have different set of theta values. Our goal is to find the best values of Theta that gives the best fit line and produces most accurate predictions. \n\n##  INDEX :- Simple Linear Regression :- \n### 1. Hypothesis \n### 2. Mean Squared Error \n### 3. Cost Function\n### 4. Gradient Descent\n### 5. Applying Linear Regression from scratch to hardwrk dataset\n##  INDEX :- Multivariate Linear Regression :- \n### 1. Hypothesis \n### 2. Cost Function\n### 3. Gradient Descent\n### 4. Applying Linear Regression from scratch to Boston dataset\n\n\n\n\n","ceba38cc":"We update the theta values. We take the partial differential of the cost function with respect to each theta value and deduct that value from the existing theta value.Here, alpha is the learning rate and it is a constant.\n\n![now.jpg](attachment:fffd6978-0345-4125-912c-65adec421877.jpg)\n\nWe calculate this part for all theta values.\nBelow Code is an implementation of the same.","51868f95":"### Error Function\n1. Let's calculate error for one example :- \n\n$$E = mod [y_{prediced}^i - y_{actual}^i]$$\n\n2. Total error over all examples :-\n\nSum of the errors over all the examples. \n\n![new.jpg](attachment:ea3cdcbf-5a81-42c4-831c-44244d8bd91c.jpg)\n\n\nThis is called as the Absolute Error. We do not use absolute error as this functions is not differentiable.\n\n3. Therefor we calculate MSE (Mean Squared Error) :- \n\n![images.png](attachment:d08b1be3-b3a9-4f95-b72d-6fd46ca37edb.png)\n\nLet\u2019s analyze what this equation actually means.\n\nIn mathematics, the character that looks like weird E is called summation (Greek sigma). It is the sum of a sequence of numbers, from i=1 to n. Let\u2019s imagine this like an array of points, where we go through all the points, from the first (i=1) to the last (i=n).\n\nFor each point, we take the y-coordinate of the point, and the y\u2019-coordinate. The y-coordinate is our purple dot. The y\u2019 point sits on the line we created. We subtract the y-coordinate value from the y\u2019-coordinate value, and calculate the square of the result.\n\nThe third part is to take the sum of all the (y-y\u2019)\u00b2 values, and divide it by n, which will give the mean.\nOur goal is to minimize this mean, which will provide us with the best line that goes through all the points.\n\nThis is also known as the Loss\/Cost Function.\n\n#### In the code below we calculate the MSE function \n"}}