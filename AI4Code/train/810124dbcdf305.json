{"cell_type":{"48d06c83":"code","2054b2c4":"code","14e21aaa":"code","f0b8c616":"code","b6b2a8d3":"code","8e88d185":"code","6ec467d7":"code","f8bd2921":"code","28127dc9":"code","4b0cc048":"code","332b0c43":"code","e0bc4f2e":"code","096738da":"code","fb625032":"code","d08d82a1":"code","e4c9f3ae":"code","28f89bae":"code","18872f9c":"code","c54902fe":"code","bf49162c":"code","ebb84eb8":"code","1e5902e7":"code","216e7db9":"code","6e6db6bb":"code","669ade62":"markdown","bee99efe":"markdown","e34b2919":"markdown","5137cab6":"markdown","1f30483d":"markdown","1f6b35b4":"markdown","c8d6ab6c":"markdown","769fe694":"markdown","aea1d285":"markdown","ec6464b4":"markdown","9c2de0d9":"markdown","b8d21c05":"markdown","fcff5b41":"markdown","bf26f7bd":"markdown","5f15fe3a":"markdown","e114ce91":"markdown","766ecb77":"markdown","9eb029af":"markdown","1e8e8fa1":"markdown"},"source":{"48d06c83":"import pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom keras.losses import BinaryCrossentropy\nimport matplotlib.pyplot as plt","2054b2c4":"# df stands for dataframe\ndf = pd.read_csv('..\/input\/housepricedata.csv')","14e21aaa":"# print the first 10 rows of dataset\ndf.head(10)","f0b8c616":"dataset = df.values","b6b2a8d3":"# let's print the first 5 rows of data\nprint(dataset[:5,:])","8e88d185":"df.columns","6ec467d7":"# first 10 columns (not counting the name column) as features\nX = dataset[:,0:10]\n# last column as output\nY = dataset[:,-1]","f8bd2921":"# initiate a scaler by calling it from sklearn. Just like initiate a LinearRegressor or LogisticRegressor.\nmin_max_scaler = preprocessing.MinMaxScaler()\n\n# apply scaling to our data by calling fit_transform()\nX_scale = min_max_scaler.fit_transform(X)","28127dc9":"# let's print some rows of the features's scaled version\nX_scale[:5,:5]","4b0cc048":"# split 8 parts out of 10 for training\nX_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.2)\n# split 1 part out of 10 for each validation and test set\nX_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)","332b0c43":"# training size\nprint('Training set:', X_train.shape, Y_train.shape)\n# validation size\nprint('Validation set:', X_val.shape, Y_val.shape)\n# test size\nprint('Test set:', X_test.shape, Y_test.shape)","e0bc4f2e":"model = Sequential([\n    Dense(32, activation='relu', input_shape=(10,)),\n    Dense(32, activation='relu'),\n    \n    Dense(1, activation='sigmoid'),\n])","096738da":"model.summary()","fb625032":"model.compile(optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True),\n              loss=BinaryCrossentropy(),\n              metrics=['accuracy'])","d08d82a1":"hist = model.fit(X_train, Y_train,\n          batch_size=32, epochs=50,\n          validation_data=(X_val, Y_val))","e4c9f3ae":"loss_value, metric_value = model.evaluate(X_test, Y_test)\n\nprint('Loss:', loss_value)\nprint('Accuracy:', metric_value)","28f89bae":"plt.plot(hist.history['loss'], label='Train set loss values')\nplt.plot(hist.history['val_loss'], label='Validation set loss values')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(loc='upper right')\nplt.show()","18872f9c":"plt.plot(hist.history['accuracy'], label='Train set accuracy')\nplt.plot(hist.history['val_accuracy'], label='Validation set accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(loc='lower right')\nplt.show()","c54902fe":"model_2 = Sequential([\n    Dense(1000, activation='relu', input_shape=(10,)),\n    Dense(1000, activation='relu'),\n    Dense(1000, activation='relu'),\n    Dense(1000, activation='relu'),\n    Dense(1, activation='sigmoid'),\n])\nmodel_2.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nhist_2 = model_2.fit(X_train, Y_train,\n          batch_size=32, epochs=100,\n          validation_data=(X_val, Y_val))","bf49162c":"plt.plot(hist_2.history['loss'], label='Train set loss values')\nplt.plot(hist_2.history['val_loss'], label='Validation set loss values')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(loc='upper right')\nplt.show()","ebb84eb8":"plt.plot(hist_2.history['accuracy'], label='Train set accuracy')\nplt.plot(hist_2.history['val_accuracy'], label='Validation set accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(loc='lower right')\nplt.show()","1e5902e7":"from keras.layers import Dropout\nfrom keras import regularizers","216e7db9":"model_3 = Sequential([\n    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=(10,)),\n    Dropout(0.3),\n    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n    Dropout(0.3),\n    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n    Dropout(0.3),\n    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n    Dropout(0.3),\n    Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)),\n])\nmodel_3.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nhist_3 = model_3.fit(X_train, Y_train,\n          batch_size=32, epochs=100,\n          validation_data=(X_val, Y_val))","6e6db6bb":"plt.plot(hist_3.history['loss'], label='Train set loss values')\nplt.plot(hist_3.history['val_loss'], label='Validation set loss values')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(loc='upper right')\nplt.show()","669ade62":"Let print the *first* rows of the dataset to see if it's looking right using the **head()** method. You can also print the *last* rows using the **tail()** method.","bee99efe":"Let read our tabular type dataset with pandas using the **read_csv()** function.","e34b2919":"# Building and Training Our First Neural Network","5137cab6":"# Adding regularizations (l2 and Dropout)","1f30483d":"# Exploring and Processing the Data","1f6b35b4":"Lastly, we wish to set aside some parts of our dataset for a validation set and a test set. We use the function train_test_split from scikit-learn to do that.","c8d6ab6c":"We have the first 10 data columns as features and the last column (the **AboveMedianPrice**) as output.\n\nNow, we split the dataset into our input features and the label we wish to predict.","769fe694":"# Visualizing Loss and\u00a0Accuracy","aea1d285":"Now that we've got our architecture specified, we need to find the best numbers for it. Before we start our training, we have to configure the model by\n- Telling it what algorithm you want to use to do the optimization (we'll use stochastic gradient descent)\n- Telling it what loss function to use (for binary classification, we will use binary cross entropy)\n- Telling it what other metrics you want to track apart from the loss function (we want to track accuracy as well)\n\nWe do so below:","ec6464b4":"The dataset that we have now is in what we call a pandas dataframe. To convert it to an array, simply call the **values** variable of a dataframe:","9c2de0d9":"We first have to read in the CSV file that we've been given. We'll use a package called pandas for that.\n\nWe also want to normalize the input data, so we will also import preprocessing package from sklearn.\n\nWe also will use sklearn pre-written function to split the data to *train set* and *test set* so we will import it.\n\nLastly, we will use the **Keras** framework to design our Neural Network with ease.","b8d21c05":"Training on the data is pretty straightforward and requires us to write one line of code. The function is called 'fit' as we are fitting the parameters to the data. We specify:\n- what data we are training on, which is X_train and Y_train\n- the size of our mini-batch \n- how long we want to train it for (epochs)\n- what our validation data is so that the model will tell us how we are doing on the validation data at each point.\n\nThis function will output a history, which we save under the variable hist. We'll use this variable a little later.","fcff5b41":"Normalizing our data is very important, as we want the input features to be on the same order of magnitude to make our training easier. We'll use a min-max scaler from scikit-learn which scales our data to be between 0 and 1.","bf26f7bd":"In this notebook, we'll go through the code to create your very first neural network to predict whether the house price is below or above median value. We will go through the following in this notebook:\n\n- Exploring and Processing the Data\n- Building and Training our Neural Network\n- Visualizing Loss and Accuracy","5f15fe3a":"We can also visualize the training accuracy and the validation accuracy like this:","e114ce91":"We want to visualize the training loss and the validation loss like this:","766ecb77":"We will be using the Sequential model, which means that we merely need to describe the layers above in sequence. Our neural network has three layers:\n\n- Hidden layer 1: 32 neurons, relu activation\n- Hidden layer 2: 32 neurons, relu activation\n- Output Layer: 1 neuron, Sigmoid activation","9eb029af":"# Try a network with more layers and much more neurals","1e8e8fa1":"Now we evaluating our data on the test set. We can call the **evaluate()** method which returns the loss value (BinaryCrossEntropy) and the metric value (Accuracy)."}}