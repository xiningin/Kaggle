{"cell_type":{"e823349c":"code","48f34f77":"code","b9b08d6e":"code","e0c10668":"code","d7fdce8f":"code","88db52b7":"code","91566044":"code","d4f60226":"code","a96f5ece":"code","d4158712":"code","4b4a5312":"code","0d2caac8":"code","4d2b28b1":"code","cfb67176":"code","5ed56ef7":"code","e21c3b59":"code","8e7d94a5":"code","1a361bfc":"code","c7b04604":"code","12c22a38":"code","55096c7a":"markdown","e14ebd0f":"markdown","c7deabe2":"markdown","0236b040":"markdown","af7420e3":"markdown","9766cb40":"markdown","d74f7613":"markdown","10271efc":"markdown","59a7ad4d":"markdown","7be3ec34":"markdown","1fe91ac1":"markdown","8d77036f":"markdown","0509f3dc":"markdown","594dcade":"markdown","d8505b08":"markdown"},"source":{"e823349c":"#!pip install --upgrade pip\n!pip install fastai==0.7.0 ## Based on Fast.ai ML course\n\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline","48f34f77":"import numpy as np \nimport pandas as pd\nfrom IPython.display import display\nfrom fastai.imports import *\nfrom fastai.structured import *\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom pandas_summary import DataFrameSummary\nfrom matplotlib import pyplot as plt\nimport math\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nimport graphviz\nimport re\n\nimport shap\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom pdpbox import pdp, get_dataset, info_plots\n\nimport IPython\nfrom IPython.display import display\nprint(os.listdir(\"..\/input\/\"))","b9b08d6e":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\ntrain_df.head()","e0c10668":"test_df.head()","d7fdce8f":"train_cats(train_df)\napply_cats(test_df, train_df)","88db52b7":"df_trn, y_trn, nas = proc_df(train_df, 'Survived')\ndf_test, _, _ = proc_df(test_df, na_dict=nas)\ndf_trn.head()","91566044":"df_test.head()","d4f60226":"## Let's remove the NA columns that were introduced by proc_df as the test and train datasets have different no of columns\ndf_trn.drop(['Age_na'], axis =1, inplace = True)\ndf_test.drop(['Age_na', 'Fare_na'], axis =1, inplace = True)\ndf_test.head()","a96f5ece":"def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(train_X), train_y), rmse(m.predict(val_X), val_y),     ## RMSE of log of prices\n                m.score(train_X, train_y), m.score(val_X, val_y)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","d4158712":"train_X, val_X, train_y, val_y = train_test_split(df_trn, y_trn, test_size=0.33, random_state=42)","4b4a5312":"%time\nm = RandomForestClassifier(n_estimators=1, min_samples_leaf=10, n_jobs=-1, max_depth = 3, oob_score=True) ## Use all CPUs available\nm.fit(train_X, train_y)\n\nprint_score(m)","0d2caac8":"draw_tree(m.estimators_[0], train_X, precision=3)","4d2b28b1":"%time\nm = RandomForestClassifier(n_estimators=20, min_samples_leaf=10, max_features=0.7, n_jobs=-1, oob_score=True) ## Use all CPUs available\nm.fit(train_X, train_y)\n\nprint_score(m)","cfb67176":"perm = PermutationImportance(m, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist())","5ed56ef7":"for feat_name in val_X.columns:\n#for feat_name in base_features:\n    #pdp_dist = pdp.pdp_isolate(model=m, dataset=val_X, model_features=base_features, feature=feat_name)\n    pdp_dist = pdp.pdp_isolate(model = m, dataset=val_X, model_features=val_X.columns, feature=feat_name)\n\n    pdp.pdp_plot(pdp_dist, feat_name)\n\n    plt.show()","e21c3b59":"explainer = shap.TreeExplainer(m)\nshap_values = explainer.shap_values(val_X)\n\n# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\n# shap.force_plot(explainer.expected_value, shap_values[1,:], val_X.iloc[1,:], matplotlib=True) ## Not for classification","8e7d94a5":"shap.summary_plot(shap_values, val_X, plot_type=\"bar\")","1a361bfc":"%time\ndf_trn.drop(['Embarked', 'Fare', 'Cabin', 'Parch'], axis =1, inplace = True)\ndf_test.drop(['Embarked', 'Fare', 'Cabin', 'Parch'], axis =1, inplace = True)\ntrain_X, val_X, train_y, val_y = train_test_split(df_trn, y_trn, test_size=0.33, random_state=42)\nm = RandomForestClassifier(n_estimators=20, min_samples_leaf=10, max_features=0.7, n_jobs=-1, oob_score=True) ## Use all CPUs available\nm.fit(train_X, train_y)\nprint_score(m)","c7b04604":"pred = m.predict(df_test)\nsubmission = pd.read_csv('..\/input\/gender_submission.csv')\nsubmission.head()","12c22a38":"submission['Survived'] = pred   \nsubmission.to_csv('rf_submission_v2.csv', index=False)","55096c7a":"## Partial Dependence Plots","e14ebd0f":"It seems that some features like Parch, Cabin, Fare & Embarked are contributing negatively to the model. We can try and drop them to see if the performance improves. Before that, let's also check the partial dependence plots for each feature.","c7deabe2":"## SHAP values for selected rows","0236b040":"We can now pass this processed data frame to Random Forest Regressor","af7420e3":"### Split the data into training and validation sets","9766cb40":"Not much difference in the RMSE score, but we did see a slight improvement in the validation set accuracy and the OOB score.\nLet's make predictions on the test set using this model !","d74f7613":"A single decision tree did not perform so badly. You can read more about the gini impurity metric [here](https:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning#Gini_impurity).\n\nNow, let's bag a collection of trees to create a random forest.","10271efc":"### Updating model to remove less important features","59a7ad4d":"## Permuation importance of features","7be3ec34":"We'll replace categories with their numeric codes, handle missing continuous values, and split the dependent variable into a separate variable. Fastai to the rescue again !!","1fe91ac1":"We'll just use a Random Forest Classifier. For that, we need to convert all columns to numeric type. But there are some categorical variables too.","8d77036f":"Initially, let's just fit a single decision tree to visualize it properly","0509f3dc":"### Defining function to calculate the evaluation metric","594dcade":"Much of the approach given below is inspired from Jeremy Howard's Fast.Ai course. \nI've mainly focussed on model interpretation using Permutation Feature Importance, Partial Dependence Plots and SHAP values.","d8505b08":"## Submitting Predictions"}}