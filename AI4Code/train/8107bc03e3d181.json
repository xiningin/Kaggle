{"cell_type":{"a3cd4570":"code","88a12c44":"code","fadb26bb":"code","60a1c1a6":"code","a9774be2":"code","89bdfdcb":"code","6c55ca17":"code","a9d84904":"code","5f3998c0":"code","0df550b8":"code","f1f7a9b6":"code","39467fe5":"code","bdd2a9e3":"code","bfe26a49":"code","8355b4f1":"code","c82846cf":"code","5a66c0ce":"code","b68ceb30":"markdown","e91c8d2e":"markdown","7c4fedbb":"markdown","0e852f5f":"markdown"},"source":{"a3cd4570":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","88a12c44":"import numpy as np \nimport pandas as pd \nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU,SimpleRNN\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport re\nimport string","fadb26bb":"class TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","60a1c1a6":"#Implementing Embedding layer \nclass TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","a9774be2":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","89bdfdcb":"train.describe","6c55ca17":"train[\"target\"].value_counts()","a9d84904":"#Area Under Curve\ndef roc_auc(predictions,target):\n    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n    roc_auc = metrics.auc(fpr, tpr)\n    return roc_auc","5f3998c0":"#Training and testing of data\nx_train, x_te, y_train, y_te = train_test_split(train.text.values, train.target.values, \n                                                  stratify=train.target.values, \n                                                  random_state=42, \n                                                  test_size=0.2, shuffle=True)","0df550b8":"#Main part of pad sequence for training and testin data\n# using keras tokenizer here\ntoken = text.Tokenizer(num_words=None)\nmax_len = 200\n\ntoken.fit_on_texts(list(x_train) + list(x_te))\nx_train_seq = token.texts_to_sequences(x_train)\nx_te_seq = token.texts_to_sequences(x_te)\n\n#zero pad the sequences\nx_train_pad = sequence.pad_sequences(x_train_seq, maxlen=max_len)\nx_te_pad = sequence.pad_sequences(x_te_seq, maxlen=max_len)\n\nword_index = token.word_index","f1f7a9b6":"#Glove\n# load the GloVe vectors in a dictionary:\n\nembeddings_index = {}\nf = open('\/kaggle\/input\/glove6b100dtxt\/glove.6B.100d.txt','r',encoding='utf-8')\nfor line in tqdm(f):\n    values = line.split(' ')\n    word = values[0]\n    coefs = np.asarray([float(val) for val in values[1:]])\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","39467fe5":"#Creating a ebedding matrix\nembedding_matrix = np.zeros((len(word_index) + 1, 100))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","bdd2a9e3":"#lstm is used for this\n#with strategy.scope():\n    \n# LSTM with glove embeddings and one dense layer\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                    100,\n                    weights=[embedding_matrix],\n                    input_length=max_len,\n                    trainable=False))\nmodel.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n    \nmodel.summary()","bfe26a49":"model.fit(x_train_pad, y_train, epochs=5, batch_size=64)","8355b4f1":"#we can use earlystopping then declare epochs so our model should not overfit.next i will add it.","c82846cf":"scores = model.predict(x_te_pad)\nprint(\"Auc: %.2f%%\" % (roc_auc(scores,y_te)))","5a66c0ce":"#comparision with BILSTM\n\n","b68ceb30":"EDA","e91c8d2e":"# LSTM\nAn LSTM has a similar control flow as a recurrent neural network. It processes data passing on information as it propagates forward. The differences are the operations within the LSTM\u2019s cells.\n\n**Pipeline**\n![image.png](attachment:image.png)","7c4fedbb":"**Embedding Matrix created**","0e852f5f":"**100d glove word embedding******"}}