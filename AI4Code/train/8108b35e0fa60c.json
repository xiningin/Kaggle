{"cell_type":{"108ff89a":"code","c8bda5e5":"code","4b6bfd45":"code","e9f6f453":"code","ded5ad21":"code","e3e2b092":"code","dd71842d":"code","6b399886":"code","b01d1b7c":"code","3d78c002":"code","c35efba9":"code","2bdffcf0":"code","a6001f32":"code","524e5f02":"code","65c570a3":"code","a825e760":"code","aa4d9917":"code","3eccd525":"code","5becbb88":"code","46bed712":"code","5881abe3":"code","2f5130fc":"markdown","d1599ee4":"markdown","bbdbeaed":"markdown","dfd0b6c4":"markdown","d09fb57d":"markdown","4166e091":"markdown","37a1c7ec":"markdown","c66f49e8":"markdown","89a6d728":"markdown","92f9c940":"markdown","de83757a":"markdown","ed2d7f21":"markdown","074fbe9f":"markdown","f7871cbd":"markdown","35d96663":"markdown","cc7ed171":"markdown","8ab652d6":"markdown"},"source":{"108ff89a":"#Downloading the spaCy's BERT pretrained model\n!pip install spacy\n!python -m spacy download en_trf_bertbaseuncased_lg","c8bda5e5":"import spacy","4b6bfd45":"# Load the spacy model that you have installed\n#nlp = spacy.load('en_trf_bertbaseuncased_lg')\nimport en_trf_bertbaseuncased_lg\nnlp = en_trf_bertbaseuncased_lg.load()","e9f6f453":"# process a sentence using the model\ndoc = nlp(\"This is some dummy text that I am processing with Spacy\")","ded5ad21":"# It's that simple - all of the vectors and words are assigned after this point\n# Get the vector for 'dummy':\ndoc[3].vector","e3e2b092":"# Get the mean vector for the entire sentence (useful for sentence classification etc.)\ndoc.vector","dd71842d":"!python -m spacy download en_trf_robertabase_lg","6b399886":"# Load the spacy model that you have installed\nimport en_trf_robertabase_lg\nnlp = en_trf_robertabase_lg.load()","b01d1b7c":"# process a sentence using the model\ndoc = nlp(\"This is some dummy text that I am processing with Spacy\")","3d78c002":"# It's that simple - all of the vectors and words are assigned after this point\n# Get the vector for 'dummy':\ndoc[3].vector","c35efba9":"# Get the mean vector for the entire sentence (useful for sentence classification etc.)\ndoc.vector","2bdffcf0":"!python -m spacy download en_trf_distilbertbaseuncased_lg","a6001f32":"# Load the spacy model that you have installed\nimport en_trf_distilbertbaseuncased_lg\nnlp = en_trf_distilbertbaseuncased_lg.load()","524e5f02":"# process a sentence using the model\ndoc = nlp(\"This is some dummy text that I am processing with Spacy\")","65c570a3":"# It's that simple - all of the vectors and words are assigned after this point\n# Get the vector for 'dummy':\ndoc[3].vector","a825e760":"# Get the mean vector for the entire sentence (useful for sentence classification etc.)\ndoc.vector","aa4d9917":"!python -m spacy download en_trf_xlnetbasecased_lg","3eccd525":"# Load the spacy model that you have installed\nimport en_trf_xlnetbasecased_lg\nnlp = en_trf_xlnetbasecased_lg.load()","5becbb88":"# process a sentence using the model\ndoc = nlp(\"This is some dummy text that I am processing with Spacy\")","46bed712":"# It's that simple - all of the vectors and words are assigned after this point\n# Get the vector for 'dummy':\ndoc[3].vector","5881abe3":"# Get the mean vector for the entire sentence (useful for sentence classification etc.)\ndoc.vector","2f5130fc":"spaCy's RoBERT Transformer provides weights and configuration for the pretrained transformer model roberta-base, published by Facebook. The package uses HuggingFace's transformers implementation of the model. Pretrained transformer models assign detailed contextual word representations, using knowledge drawn from a large corpus of unlabelled text. We can use the contextual word representations as features in a variety of pipeline components that can be trained on our own data.","d1599ee4":"![](https:\/\/paperswithcode.com\/media\/models\/roberta-classification.png-0000000936-4dce6670.png)","bbdbeaed":"spaCy's DistilBERT Transformer Provides weights and configuration for the pretrained transformer model distilbert-base-uncased, published by Hugging Face. The package uses HuggingFace's transformers implementation of the model. Pretrained transformer models assign detailed contextual word representations, using knowledge drawn from a large corpus of unlabelled text. We can use the contextual word representations as features in a variety of pipeline components that can be trained on our own data.","dfd0b6c4":"# **4. XLNet**","d09fb57d":"Lately, several methods have been presented to improve BERT on either its prediction metrics or computational speed, but not both.\nXLNet and RoBERTa improve on the performance while DistilBERT improves on the inference speed. The table below compares them for what they are!","4166e091":"# **2.RoBERT**","37a1c7ec":"XLNET is a generalized autoregressive model where next token is dependent on all previous tokens. XLNET is \u201cgeneralized\u201d because it captures bi-directional context by means of a mechanism called \u201cpermutation language modeling\u201d. It integrates the idea of auto-regressive models and bi-directional context modeling, yet overcoming the disadvantages of BERT. It outperforms BERT on 20 tasks, often by a large margin in tasks such as question answering, natural language inference, sentiment analysis, and document ranking.","c66f49e8":"This Notebook contains the steps to generate sentence embeddings using BERT, RoBERT, DistilBERT and XLNet. We have make use of spaCy pretrained transformer models.","89a6d728":"![](https:\/\/miro.medium.com\/max\/700\/1*dMgzP_YboxpR8VXuGeAg_Q.png)","92f9c940":"# **3. DistilBERT**","de83757a":"# **1. BERT**","ed2d7f21":"PLM is the idea of capturing bidirectional context by training an autoregressive model on all possible permutation of words in a sentence. Instead of fixed left-right or right-left modeling, XLNET maximizes expected log likelihood over all possible permutations of the sequence. In expectation, each position learns to utilize contextual information from all positions thereby capturing bidirectional context. No MASK is needed and input data need not be corrupted.","074fbe9f":"**Architecture:**","f7871cbd":"![](http:\/\/miro.medium.com\/max\/700\/1*bSUO_Qib4te1xQmBlQjWaw.png)","35d96663":"\n**Architecture of BERT:**\n![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*-oQKmzvHrzqeSQEnM9f_kQ.png)","cc7ed171":"spaCy's BERT Transformer provides weights and configuration for the pretrained transformer model bert-base-uncased, published by Google Research. The package uses HuggingFace's transformers implementation of the model. Pretrained transformer models assign detailed contextual word representations, using knowledge drawn from a large corpus of unlabelled text. We can use the contextual word representations as features in a variety of pipeline components that can be trained on our own data.","8ab652d6":"spaCy's XLNet Transformer provides weights and configuration for the pretrained transformer model xlnet-base-cased, published by CMU and Google Brain. The package uses HuggingFace's transformers implementation of the model. Pretrained transformer models assign detailed contextual word representations, using knowledge drawn from a large corpus of unlabelled text. We can use the contextual word representations as features in a variety of pipeline components that can be trained on our own data."}}