{"cell_type":{"ea9457e2":"code","f21f6f1f":"code","8b9d6b20":"code","5b21e194":"code","a81d2e7d":"code","cbdc5347":"code","02944327":"code","9d739ba8":"code","40a6dd18":"code","e22ac20f":"code","05430419":"code","85786f25":"code","b1b7e5a9":"code","df0ac057":"code","e025530d":"code","fbc4840a":"code","f2176a59":"code","ba48f94c":"code","8ca9d8e8":"code","296986eb":"code","e773efa1":"code","ba865a4c":"code","67a9baae":"code","a1843dc6":"code","254f18f2":"code","28a756ec":"code","85c631e2":"code","4ed57542":"code","12f5b856":"code","253e9562":"code","9ec9c48e":"code","7f0604d9":"code","a165abf8":"code","afd1800e":"code","3e112f65":"code","7a754320":"code","3f430a47":"code","75c48ae0":"code","7e5da702":"code","b0f0e37f":"code","ed15d3e0":"code","6b9caa05":"code","40115f36":"code","2c31fade":"code","9ffd4caa":"code","6568b288":"code","c9ecf63e":"code","9fc2179b":"code","85288a08":"code","bb341421":"code","0ff1f442":"code","990f5352":"code","c9c4d9b2":"code","a98e1c3d":"markdown","efa08bc4":"markdown","30e05fc1":"markdown","56695bca":"markdown","7f85a16a":"markdown","28b228c8":"markdown","c4121cfb":"markdown","e32ea7c7":"markdown","041592ed":"markdown","c2fb99bb":"markdown","bb534fed":"markdown","1f11b9ce":"markdown","17d550b1":"markdown","bd09e5ff":"markdown","f9bc68d6":"markdown","c23b4481":"markdown","38a4aa21":"markdown","1659792a":"markdown","64e862ef":"markdown","052e7dc3":"markdown","8b3e1360":"markdown","c1449d04":"markdown","f659a73a":"markdown","ad315a3e":"markdown","d5b307f5":"markdown","879863e5":"markdown","c8cd3854":"markdown","c074fbfb":"markdown","43b1c498":"markdown","1caec3f7":"markdown","9fa7b2d8":"markdown","d1c83f6a":"markdown","e0b579cc":"markdown","bcf3d621":"markdown"},"source":{"ea9457e2":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm_notebook as tqdm\ntqdm().pandas()\n\n# mlp for regression with mse loss function\nfrom sklearn.datasets import make_regression\nfrom sklearn.datasets import make_circles, make_moons\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD\nfrom keras.utils import to_categorical\nfrom keras.regularizers import l2, l1\nfrom keras.layers import Activation\nimport matplotlib.pyplot as plt","f21f6f1f":"X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n# split into train and test sets\nn_train = 30\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]","8b9d6b20":"model = Sequential()\nmodel.add(Dense(500, input_dim=2, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nhistory = model.fit(trainX, trainy, epochs=4000, validation_data=(testX, testy), verbose=0) \n# evaluate the model\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","5b21e194":"# plot loss learning curves\nplt.figure(figsize=(8,8))\nplt.subplot(211)\nplt.title('Cross-Entropy Loss', pad=-40)\nplt.plot(history.history['loss'], label='train') \nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\n# plot accuracy learning curves\nplt.subplot(212)\nplt.title('Accuracy', pad=-40)\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","a81d2e7d":"model = Sequential()\nmodel.add(Dense(500, input_dim=2, activation='relu', kernel_regularizer=l2(0.001)))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nhistory = model.fit(trainX, trainy, epochs=4000, validation_data=(testX, testy), verbose=0) \n# evaluate the model\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","cbdc5347":"# plot loss learning curves\nplt.figure(figsize=(8,8))\nplt.subplot(211)\nplt.title('Cross-Entropy Loss', pad=-40)\nplt.plot(history.history['loss'], label='train') \nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\n# plot accuracy learning curves\nplt.subplot(212)\nplt.title('Accuracy', pad=-40)\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","02944327":"alphas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\nall_train, all_test = [], []\n\nfor alpha in tqdm(alphas):\n    model = Sequential()\n    model.add(Dense(500, input_dim=2, activation='relu', kernel_regularizer=l2(alpha)))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # fit model\n    history = model.fit(trainX, trainy, epochs=2000, validation_data=(testX, testy), verbose=0) \n    # evaluate the model\n    _, train_acc = model.evaluate(trainX, trainy, verbose=0)\n    _, test_acc = model.evaluate(testX, testy, verbose=0)\n    print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n    all_train.append(train_acc)\n    all_test.append(test_acc)","9d739ba8":"plt.figure(figsize=(8,5))\nplt.semilogx(alphas, all_train, label='train', marker='o') \nplt.semilogx(alphas, all_test, label='test', marker='o') \nplt.legend()\nplt.show()","40a6dd18":"X, y = make_circles(n_samples=100, noise=0.1, random_state=1)\n# split into train and test\nn_train = 30\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]","e22ac20f":"model = Sequential()\nmodel.add(Dense(500, input_dim=2, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0)\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","05430419":"plt.figure(figsize=(10,7))\nplt.subplot(211)\nplt.title('Cross-Entropy Loss', pad=-40)\nplt.plot(history.history['loss'], label='train') \nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\n# plot accuracy learning curves\nplt.subplot(212)\nplt.title('Accuracy', pad=-40)\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","85786f25":"model = Sequential()\nmodel.add(Dense(2500, input_dim=2, activation='linear', activity_regularizer=l1(0.0001)))\nmodel.add(Activation('relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0)\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","b1b7e5a9":"plt.figure(figsize=(10,7))\nplt.subplot(211)\nplt.title('Cross-Entropy Loss', pad=-40)\nplt.plot(history.history['loss'], label='train') \nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\n# plot accuracy learning curves\nplt.subplot(212)\nplt.title('Accuracy', pad=-40)\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","df0ac057":"X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n# split into train and test\nn_train = 30\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]","e025530d":"model = Sequential()\nmodel.add(Dense(500, input_dim=2, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0) \n# evaluate the model\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","fbc4840a":"plt.figure(figsize=(10,7))\nplt.subplot(211)\nplt.title('Cross-Entropy Loss', pad=-40)\nplt.plot(history.history['loss'], label='train') \nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\n# plot accuracy learning curves\nplt.subplot(212)\nplt.title('Accuracy', pad=-40)\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","f2176a59":"from keras.constraints import unit_norm\n\nmodel = Sequential()\nmodel.add(Dense(500, input_dim=2, activation='relu', kernel_constraint=unit_norm())) \nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0) \n# evaluate the model\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","ba48f94c":"X, y = make_circles(n_samples=100, noise=0.1, random_state=1)\n# split into train and test\nn_train = 30\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]","8ca9d8e8":"model = Sequential()\nmodel.add(Dense(500, input_dim=2, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0) \n# evaluate the model\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","296986eb":"plt.figure(figsize=(10,7))\nplt.subplot(211)\nplt.title('Cross-Entropy Loss', pad=-40)\nplt.plot(history.history['loss'], label='train') \nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\n# plot accuracy learning curves\nplt.subplot(212)\nplt.title('Accuracy', pad=-40)\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","e773efa1":"from keras.layers import Dropout\nfrom keras.constraints import min_max_norm\nmodel = Sequential()\nmodel.add(Dense(500, input_dim=2, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(1, activation='sigmoid')) \nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0) \n# evaluate the model\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","ba865a4c":"plt.figure(figsize=(10,7))\nplt.subplot(211)\nplt.title('Cross-Entropy Loss', pad=-40)\nplt.plot(history.history['loss'], label='train') \nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\n# plot accuracy learning curves\nplt.subplot(212)\nplt.title('Accuracy', pad=-40)\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","67a9baae":"from keras.layers import GaussianNoise\n\n# define model\nmodel = Sequential()\n\nmodel.add(GaussianNoise(0.01, input_shape=(2,)))\n\nmodel.add(Dense(500, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0) # evaluate the model\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","a1843dc6":"plt.figure(figsize=(10,7))\nplt.subplot(211)\nplt.title('Cross-Entropy Loss', pad=-40)\nplt.plot(history.history['loss'], label='train') \nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\n# plot accuracy learning curves\nplt.subplot(212)\nplt.title('Accuracy', pad=-40)\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","254f18f2":"model = Sequential()\n\nmodel.add(Dense(500, input_dim=2))\nmodel.add(GaussianNoise(0.1))\nmodel.add(Activation('relu'))\n\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0) # evaluate the model\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","28a756ec":"plt.figure(figsize=(10,7))\nplt.subplot(211)\nplt.title('Cross-Entropy Loss', pad=-40)\nplt.plot(history.history['loss'], label='train') \nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\n# plot accuracy learning curves\nplt.subplot(212)\nplt.title('Accuracy', pad=-40)\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","85c631e2":"from keras.layers import GaussianNoise\n\nmodel = Sequential()\n\nmodel.add(Dense(500, input_dim=2, activation='relu'))\nmodel.add(GaussianNoise(0.1))\n\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0) # evaluate the model\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","4ed57542":"X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n# split into train and test\nn_train = 30\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]","12f5b856":"model = Sequential()\n\nmodel.add(Dense(500, input_dim=2, activation='relu'))\n\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0) # evaluate the model\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","253e9562":"plt.figure(figsize=(10,7))\nplt.subplot(211)\nplt.title('Cross-Entropy Loss', pad=-40)\nplt.plot(history.history['loss'], label='train') \nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\n# plot accuracy learning curves\nplt.subplot(212)\nplt.title('Accuracy', pad=-40)\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","9ec9c48e":"from keras.callbacks import EarlyStopping, ModelCheckpoint\n\nearlystop = EarlyStopping(monitor='val_loss', patience=200)\nmc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1,\n    save_best_only=True)\n\nmodel = Sequential()\n\nmodel.add(Dense(500, input_dim=2, activation='relu'))\n\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=4000, verbose=0, callbacks=[earlystop, mc]) # evaluate the model\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","7f0604d9":"plt.figure(figsize=(10,7))\nplt.subplot(211)\nplt.title('Cross-Entropy Loss', pad=-40)\nplt.plot(history.history['loss'], label='train') \nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\n# plot accuracy learning curves\nplt.subplot(212)\nplt.title('Accuracy', pad=-40)\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","a165abf8":"X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\ny = to_categorical(y)\n# split into train and test\nn_train = int(0.3 * X.shape[0])\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]","afd1800e":"model = Sequential()\nmodel.add(Dense(15, input_dim=2, activation='relu'))\nmodel.add(Dense(3, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0) # evaluate the model\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))","3e112f65":"plt.figure(figsize=(10,7))\nplt.subplot(211)\nplt.title('Cross-Entropy Loss', pad=-40)\nplt.plot(history.history['loss'], label='train') \nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\n# plot accuracy learning curves\nplt.subplot(212)\nplt.title('Accuracy', pad=-40)\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","7a754320":"from keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.metrics import accuracy_score\n\nearlystop = EarlyStopping(monitor='val_loss', patience=100)\nmc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1,\n    save_best_only=True)\n\n\ndef run_model(trainX, trainy, testX, testy):\n    model = Sequential()\n    model.add(Dense(15, input_dim=2, activation='relu'))\n    model.add(Dense(3, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.fit(trainX, trainy, validation_data=(testX, testy), epochs=600, \n              verbose=0, callbacks=[earlystop, mc])\n    _, test_acc = model.evaluate(testX, testy, verbose=0)\n    print('Validation Accuracy : %.3f' % (test_acc))\n    \n    return model\n\ndef ensemble_predictions(models, testX):\n    # Store predicted probabilities of each model \n    yhats = np.array([model.predict(testX) for model in models])\n    \n    # Calculate sum of the predicted probabilities of each class of all the model\n    sum_yhats = np.sum(yhats, axis=0)\n    \n    #(Ensembling) Find the class that was most commonly predicted by all models\n    most_common_predictions = np.argmax(sum_yhats, axis=1)\n    return most_common_predictions\n\ndef evaluate_n_models(models, n_models, testX, testy):\n    subset = models[:n_models]\n    yhat = ensemble_predictions(subset, testX)\n    # calculate accuracy\n    testy = np.argmax(testy, axis=1)\n    return accuracy_score(testy, yhat)","3f430a47":"from sklearn.model_selection import train_test_split\n\nX, y = make_blobs(n_samples=800, centers=3, n_features=2, cluster_std=2, random_state=4)\ny = to_categorical(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, stratify=y)\nX_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, stratify=y_test)","75c48ae0":"n_models = 10\nmodels = list()\n\nfor i in tqdm(range(n_models)):\n    print('Training model run ', i+1)\n    models.append(run_model(X_train, y_train, X_test, y_test))","7e5da702":"# evaluate different numbers of ensembles\nscores = list()\nfor i in range(1, n_models+1):\n    score = evaluate_n_models(models, i, X_test, y_test) \n    print('Test Set Accuracy > %.3f' % score)\n    scores.append(score)","b0f0e37f":"print('Scores Mean: %.3f, Standard Deviation: %.3f' % (np.mean(scores), np.std(scores)))","ed15d3e0":"plt.figure(figsize=(10,5))\nx_axis = [i for i in range(1, n_models+1)]\nplt.plot(x_axis, scores)\nplt.show()","6b9caa05":"single_scores, ensemble_scores = list(), list()\n\nfor i in range(len(models)):\n    \n    # Evaluate model with i models\n    ensemble_score = evaluate_n_models(models, i+1, X_test, y_test)\n    \n    # Evaluate i`th model standalone\n    _, single_score = models[i].evaluate(X_test, y_test, verbose=0)\n    print('> %d: single=%.3f, ensemble=%.3f' % (i+1, single_score, ensemble_score))\n    ensemble_scores.append(ensemble_score)\n    single_scores.append(single_score)\n\n# summarize average accuracy of a single final model\nprint('Accuracy %.3f (%.3f)' % (np.mean(single_scores), np.std(single_scores)))","40115f36":"plt.figure(figsize=(10,5))\nx_axis = [i for i in range(1, len(models)+1)]\nplt.plot(x_axis, single_scores, marker='o', linestyle='None') \nplt.plot(x_axis, ensemble_scores, marker='o')\nplt.show()","2c31fade":"from numpy import tensordot\nfrom numpy.linalg import norm\nfrom itertools import product\nfrom scipy.optimize import differential_evolution\n\ndef ensemble_predictions(models, weights, testX):\n    yhats = np.array([model.predict(testX) for model in models])\n    \n    sum_predictions = tensordot(yhats, weights, axes=((0),(0)))\n    \n    yhat = np.argmax(sum_predictions, axis=1)\n    \n    return yhat\n\ndef evaluate_ensemble(models, weights, testX, testy):\n    yhats = ensemble_predictions(models, weights, testX)\n    testy = np.argmax(testy, axis=1)\n    \n    return accuracy_score(testy, yhats)\n\ndef normalize(weights):\n    norm_factor = norm(weights, 1)\n    if norm_factor == 0.0:\n        return weights\n    \n    norm_vector = weights\/norm_factor\n    return norm_vector\n\ndef loss_function(weights, models, testX, testy):\n    normalized = normalize(weights)\n    \n    error = 1.0 - evaluate_ensemble(models, weights, testX, testy)\n    return error","9ffd4caa":"n_models = 4\nmodels = list()\n\nfor i in tqdm(range(n_models)):\n    print('Training model run ', i+1)\n    models.append(run_model(X_train, y_train, X_val, y_val))  ","6568b288":"# evaluate different numbers of ensembles\nscores = list()\nfor i in range(1, n_models+1):\n    _, score = models[i-1].evaluate(X_test, y_test, verbose=0) \n    print('model %d, Test Set Accuracy > %.3f' % (i, score))\n\n# evaluate averaging ensemble (equal weights)\nweights = [1.0\/n_models for _ in range(n_models)]\nscore = evaluate_ensemble(models, weights, X_test, y_test) \nprint('\\nEqual Weights Score: %.3f' % score)","c9ecf63e":"# define bounds on each weight\nbound_w = [(0.0, 1.0) for _ in range(n_models)]\n\n# arguments to the loss function\nsearch_arg = (models, X_test, y_test)\n\n\n# global optimization of ensemble weights\nresult = differential_evolution(loss_function, bound_w, search_arg, maxiter=500, tol=1e-7) \n\n# get the chosen weights\nweights = normalize(result['x'])\nprint('Optimized Weights: %s' % weights)\n\n# evaluate chosen weights\nscore = evaluate_ensemble(models, weights, X_test, y_test)\nprint('Optimized Weights Score: %.3f' % score)","9fc2179b":"from sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import accuracy_score\n\ndef run_model(trainX, trainy, valX, valy):\n    model = Sequential()\n    model.add(Dense(50, input_dim=2, activation='relu'))\n    model.add(Dense(3, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    model.fit(trainX, trainy, verbose=0, epochs=50)\n    \n    _, val_acc = model.evaluate(valX, valy, verbose=0)\n    return model, val_acc\n\ndef ensemble_predictions(models, testX):\n    yhats = [model.predict(testX) for model in models]\n    \n    yhats = np.array(yhats)\n    summed = np.sum(yhats, axis=0)\n    yhats = np.argmax(summed, axis=1)\n    \n    return yhats\n\ndef evaluate_n_members(members, n_members, testX, testy):\n    # select a subset of members\n    subset = members[:n_members]\n    # make prediction\n    yhat = ensemble_predictions(subset, testX)\n    testy = np.argmax(testy, axis=1)\n    # calculate accuracy\n    return accuracy_score(testy, yhat)\n\nX, y = make_blobs(n_samples=55000, centers=3, n_features=2, cluster_std=2,random_state=2)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.3, stratify=y)\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)","85288a08":"n_folds = 10\nkfold = KFold(n_folds, shuffle=True, random_state=1)\nscores, models = list(), list()\n\nfor train_ix, val_ix in tqdm(kfold.split(X_train)):\n    trainX, trainy = X_train[train_ix], y_train[train_ix]\n    valX, valy = X_train[val_ix], y_train[val_ix]\n    \n    model, val_acc = run_model(trainX, trainy, valX, valy) \n    print('Validation Accuracy >%.3f' % val_acc)\n    scores.append(val_acc)\n    models.append(model)\n    \nprint('Estimated Accuracy %.3f (%.3f)' % (np.mean(scores), np.std(scores)))    ","bb341421":"single_scores, ensemble_scores = list(), list()\n\nfor i in range(1, n_folds+1):\n    ensemble_score = evaluate_n_members(models, i, X_test, y_test)\n\n    _, single_score = models[i-1].evaluate(X_test, y_test, verbose=0)\n    print('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score)) \n    ensemble_scores.append(ensemble_score)\n    single_scores.append(single_score)\n\n    \nprint('Accuracy %.3f (%.3f)' % (np.mean(single_scores), np.std(single_scores)))","0ff1f442":"plt.figure(figsize=(10,5))\nx_axis = [i for i in range(1, n_folds+1)]\nplt.plot(x_axis, single_scores, marker='o', linestyle='None') \nplt.plot(x_axis, ensemble_scores, marker='o')\nplt.show()","990f5352":"from sklearn.utils import resample\n\nn_splits = 10\nscores, models = list(), list()\n\n\nfor _ in tqdm(range(n_splits)):\n    # select indexes\n    ix = [i for i in range(len(X_train))]\n\n    train_ix = resample(ix, replace=True, n_samples=4500)\n    val_ix = [x for x in ix if x not in train_ix]\n    # select data\n    trainX, trainy = X_train[train_ix], y_train[train_ix]\n    valX, valy = X_train[val_ix], y_train[val_ix]\n    # evaluate model\n    model, val_acc = run_model(trainX, trainy, valX, valy) \n    print('Val accuracy >%.3f' % val_acc)\n    scores.append(val_acc)\n    models.append(model)\n\nprint('Estimated Accuracy %.3f (%.3f)' % (np.mean(scores), np.std(scores)))","c9c4d9b2":"single_scores, ensemble_scores = list(), list()\n\nfor i in range(1, n_folds+1):\n    ensemble_score = evaluate_n_members(models, i, X_test, y_test)\n\n    _, single_score = models[i-1].evaluate(X_test, y_test, verbose=0)\n    print('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score)) \n    ensemble_scores.append(ensemble_score)\n    single_scores.append(single_score)\n\n    \nprint('Accuracy %.3f (%.3f)' % (np.mean(single_scores), np.std(single_scores)))","a98e1c3d":"<h3>Without Early Stopping","efa08bc4":"<blockquote>We have ensemble 15 models and have found best predictions from those models.\n    Each time we run the evaluate_n_models ( we find ensembled prediction values).\n    In this case we have found the ensembled(15 models) values 15 times)","30e05fc1":"<h3> Optimized Search Weighted Average<\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nAn alternative to searching for weight values is to use a directed optimization process. Opti- mization is a search process, but instead of sampling the space of possible solutions randomly or exhaustively, the search process uses any available information to make the next step in the search, such as toward a set of weights that has lower error.<br><br>\n\nSciPy provides an implementation of the Differential Evolution method. This is one of the few stochastic global search algorithms that just works for function optimization with continuous inputs, and it works well. The differential evolution() SciPy function requires that function is specified to evaluate a set of weights and return a score to be minimized. We can minimize the classification error (1 - accuracy).","56695bca":"<h3>Overfitted MLP","7f85a16a":"<h3>OverFitted Model","28b228c8":"<h3><center>6. Halt Training at the Right Time with Early Stopping<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n<\/div>","c4121cfb":"<h3>Overfitted model","e32ea7c7":"<h3>Hidden Layer(After Activation)","041592ed":"<h3>Weigth constraint","c2fb99bb":"<h3><center>5. Promote Robustness with Noise<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n<h4 style=\"font-size:1.5vw\">Challenge of Small Training Datasets :<\/h4>\n    <ul>Small datasets can introduce problems when training large neural networks.<li> The first problem is that the network may effectively memorize the training dataset. Instead of learning a general mapping from inputs to outputs, the model may learn the specific input examples and their associated outputs. \n        <li>The second problem is that a small dataset provides less opportunity to describe the structure of the input space and its relationship to the output. More training data provides a richer description of the problem from which the model may learn.\n        <\/ul>\n<h4 style=\"font-size:1.5vw\">How and Where to Add Noise :<\/h4>\nThe most common type of noise used during training is the addition of Gaussian noise to input variables. Gaussian noise, or white noise, has a mean of zero and a standard deviation of one and can be generated as needed using a pseudorandom number generator.\n    <ul>\n        <li>Add noise to activations, i.e. the outputs of each layer.\n        <li>Add noise to weights, i.e. an alternative to the inputs.\n        <li>Add noise to the gradients, i.e. the direction to update weights.\n        <li>Add noise to the outputs, i.e. the labels or target variables.\n    <\/ul><br>The type of noise can be specialized to the types of data used as input to the model, for example, two-dimensional noise in the case of images and signal noise in the case of audio data.<br><br>\n    \n    Before Activation Function :\n    It may make more sense to add it before the activation; nevertheless, both options are possible.\n<\/div>    \n\n![image.png](attachment:image.png)","bb534fed":"<h2><center>Framework for Systematically Better Generalization in Deep Learning<\/center><\/h2>\n\n![image.png](attachment:image.png)\n\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nThere are three types of problems that are straightforward to diagnose with regard to poor performance of a deep learning neural network model; they are:<br>\n    <ul>\n        <li>Problems with Learning : Problems with learning manifest in a model that cannot effectively learn a training dataset or shows slow progress or bad performance when learning the training dataset.<br>\n            Notebook Link : \n            <a href='https:\/\/www.kaggle.com\/ashrafkhan94\/framework-for-systematically-better-deep-learning'>https:\/\/www.kaggle.com\/ashrafkhan94\/framework-for-systematically-better-deep-learning<\/a>\n        <li>Problems with Generalization : Problems with generalization manifest in a model that overfits the training dataset and makes poor predictions on a holdout dataset.(This Notebook)\n        <li>Problems with Predictions : Problems with predictions manifest in the stochastic training algorithm having a strong influence on the final model, causing high variance in behavior and performance.\n    <\/ul>\n    <blockquote><span style=\"color:#159364;\">We will be dealing with Problems with Generalization in this notebook. We will discover techniques that can be used to reduce overfitting and improve the generalization of our deep learning neural network models.\n        <\/span> <\/blockquote>\n<\/div>","1f11b9ce":"<h3><center>1. Penalize Large Weights with Weight Regularization<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    A network with large network weights can be a sign of an unstable network where small changes in the input can lead to large changes in the output. This can be a sign that the network has overfit the training dataset and will likely perform poorly when making predictions on new data.<br><br>The longer we train the network, the more specialized the weights will become to the training data, overfitting the training data. The weights will grow in size in order to handle the specifics of the examples seen in the training data.\n<ul>\n    <li>L1: Sum of the absolute weights.\n    <li>L2: Sum of the squared weights.\n    <li>L1L2: Sum of the absolute and the squared weights.\n    <\/ul><b>MLP:<\/b>\n    <blockquote>\n...<br>\nmodel.add(Dense(32, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))<br>\n        ...<br><\/blockquote>\n        <b>CNN:<\/b><br><blockquote>\n...<br>\nmodel.add(Conv2D(32, (3,3), kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))<br>\n...<br><\/blockquote>\n       <br> <b>LSTM :<\/b><br><blockquote>\n...<br>\nmodel.add(LSTM(32, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01),<br>\n    bias_regularizer=l2(0.01)))\n... <br>       \n<\/blockquote>\n<\/div>","17d550b1":"        After Activation Function :\n    \n![image.png](attachment:image.png)","bd09e5ff":"<h3><center>7.1. Combine Models From Multiple Runs with Model Averaging Ensemble<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nThe same neural network model trained on the same dataset may find one of many different possible good enough solutions each time it is run. Model averaging is an ensemble learning technique that reduces the variance in a final neural network model, sacrificing spread (and possibly better scores) in the performance of the model for a confidence in what performance to expect from the model.","f9bc68d6":"Before Activation Function(Relu) and After Linear Activation :\n\n![image.png](attachment:image.png)\n\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    The latter is probably the preferred usage of activation regularization as described in Deep Sparse Rectifier Neural Networks in order to allow the model to learn to take activations to a true zero value in conjunction with the rectified linear activation function.<\/div>","c23b4481":"<h3>Input Layer Noise","38a4aa21":"<h3><center>Better Generalization Techniques<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n<ol>\n<li>Penalize Large Weights with Weight Regularization\n<li>Sparse Representations with Activity Regularization \n<li>Force Small Weights with Weight Constraints\n<li>Decouple Layers with Dropout\n<li>Promote Robustness with Noise\n<li>Halt Training at the Right Time with Early Stopping\n    <\/ol>\n    An overfit model has low bias and high variance.Techniques that seek to reduce overfitting (reduce generalization error) by keeping network weights small are referred to as regularization methods.\n<h4 style=\"font-size:1.9vw\"><\/h4>","1659792a":"<h3><center>3. Force Small Weights with Weight Constraints<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nAn alternate solution to using a penalty for the size of network weights is to use a weight constraint. A weight constraint is an update to the network that checks the size of the weights (e.g. their vector norm), and if the size exceeds a predefined limit, the weights are rescaled so that their size is below the limit or between a range. You can think of a weight constraint as an if-then rule checking the size of the weights while the network is being trained and only coming into effect and making weights small when required.<br><br><\/div>**<blockquote style=\"font-family:verdana; word-spacing:1.5px;\">Weight constraints prove especially useful when you have configured your network to use alternative regularization methods to weight regularization and yet still desire the network to have small weights in order to reduce overfitting. One often-cited example is the use of a weight constraint regularization with dropout regularization.<\/blockquote>**\n<hr>\n\n![image.png](attachment:image.png)\n\n\n<ul>\n    <li>Maximum norm       : to force weights to have a magnitude at or below a given limit.\n    <li>Non-negative norm  : to force weights to have a positive magnitude.\n    <li>Unit norm          : to force weights to have a magnitude of 1.0.\n    <li>Min-Max norm       : to force weights to have a magnitude between a range.\n<\/ul>","64e862ef":"<blockquote>We can see some improvement in Validation accuracy from 91% to 94% and also the lines of loss for both training and testing sets are not moving away as compared to the first one<\/blockquote>","052e7dc3":"<h3>L2 Regularization","8b3e1360":"<h3>OverFitting","c1449d04":"<h3><center>2. Sparse Representations with Activity Regularization<\/center><\/h3>\n\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    The loss function of the network can be updated to penalize models in proportion to the magnitude of their activation. This is similar to weight regularization where the loss function is updated to penalize the model in proportion to the magnitude of the weights. The output of a layer is referred to as its activation or activity, as such, this form of penalty or regularization is referred to as activation regularization or activity regularization.\n    <br><br>\n    \nThe output of an encoder or, generally, the output of a hidden layer in a neural network may be considered the representation of the problem at that point in the model. As such, this type of penalty may also be referred to as representation regularization. The desire to have small activations or even very few activations with mostly zero values is also called a desire for sparsity. As such, this type of penalty is also referred to as sparse feature learning.\n    <br><br>\n    Sparsity is most commonly sought when a larger-than-required hidden layer (e.g. over- complete) is used to learn features that may encourage overfitting. The introduction of a sparsity penalty counters this problem and encourages better generalization.<br><br>\n    An activation penalty can be applied per-layer, perhaps only at one layer that is the focus of the learned representation, such as the output of the encoder model or the middle (bottleneck) of an autoencoder model.\n    <br><br>\n    The L1 norm encourages sparsity, e.g. allows some activations to become zero, whereas the L2 norm encourages small activations values in general.Use of the L1 norm may be a more commonly used penalty for activation regularization\n    <ul><h4 style=\"font-size:1.9vw\">Tips for Using Activation Regularization :<\/h4>\n        <li>Use With All Network Types\n            <li>Use With Autoencoders and Encoder-Decoders\n<li>Use Rectified Linear Activation\n        <li><h4>Use an Overcomplete Representation :<\/h4> Configure the layer chosen to be the learned features, e.g. the output of the encoder or the bottleneck in the autoencoder, to have more nodes that may be required. This is called an overcomplete representation that will encourage the network to overfit the training examples.\n    <\/ul>\n    <\/div>\nAfter Activation Function :   \n\n![image.png](attachment:image.png)","f659a73a":"<h3>Hidden layer Noise (Before Activation)","ad315a3e":"<h3>Using Drop Out layer","d5b307f5":"<h3>Using Activity Regularizer","879863e5":"<h3>After Earlystopping and Model Checkpoint","c8cd3854":"<h3>Model Averaging Ensemble<\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nModel averaging is an approach to ensemble learning where each ensemble member contributes an equal amount to the final prediction.<ul>\n<li>In the case of regression, the ensemble prediction is calculated as the average of the member predictions. \n    <li>In the case of predicting a class label, the prediction is calculated as the mode of the member predictions. \n        <li>In the case of predicting a class probability, the prediction can be calculated as the argmax of the summed probabilities for each class label.<\/ul><\/div>","c074fbfb":"<h3>K-fold Cross Validation<\/h3>","43b1c498":"<h3><center>7.2. Contribute Proportional to Trust with Weighted Average Ensemble<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    A weighted ensemble is an extension of a model averaging ensemble where the contribution of each member to the final prediction is weighted by the performance of the model. The model weights are small positive values and the sum of all weights equals one, allowing the weights to indicate the percentage of trust or expected performance from each model.","1caec3f7":"<h3><center>4. Decouple Layers with Dropout<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    Dropout is a regularization method that approximates training a large number of neural networks with different architectures in parallel. During training, some number of node outputs are randomly ignored or dropped out. This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. In effect, each update to a layer during training is performed with a different view of the configured layer.<br><br>\n     A good value for dropout in a hidden layer is between 0.5 and 0.8.\n    <\/div>","9fa7b2d8":"<h3><center>Grid Search Regularization Hyperparameter","d1c83f6a":"<h3><center>7. Reduce Model Variance with Ensemble Learning<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    A successful approach to reducing the variance of neural network models is to train multiple models instead of a single model and to combine the predictions from these models. This is called ensemble learning and not only reduces the variance of predictions but also can result in predictions that are better than any single model.\n<\/div>","e0b579cc":"<h3>Bagging\/Bootstrap ensemble<\/h3>","bcf3d621":"<h3><center>7.3. Fit Models on Different Samples with Resampling Ensembles<\/center><\/h3>\n\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    One way to achieve differences between models is to train each model on a different subset of the available training data. Models are trained on different subsets of the training data naturally through the use of resampling methods such as cross-validation and the bootstrap, designed to estimate the average performance of the model generally on unseen data. <br><br>The models used in this estimation process can be combined in what is referred to as a resampling-based ensemble, such as a cross-validation ensemble or a bootstrap aggregation (or bagging) ensemble.\n<br><br>\n\n<ul>There are three popular resampling methods that we could use to create a resampling ensemble; they are:\n\udbff\udc00 <li>Random Splits. The dataset is repeatedly sampled with a random split of the data into train and test sets.\n\udbff\udc00 <li>k-fold Cross-Validation. The dataset is split into k equally sized folds, k models are trained and each fold is given an opportunity to be used as the holdout set where the model is trained on all remaining folds.\n\udbff\udc00 <li>Bootstrap Aggregation. Random samples are collected with replacement and examples not included in a given sample are used as the test set.\n    <\/ul>\n        Perhaps the most widely used resampling ensemble method is bootstrap aggregation, more commonly referred to as bagging.\n<\/div>"}}