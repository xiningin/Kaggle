{"cell_type":{"290d85fc":"code","95ec59cb":"code","3ea5c408":"code","7214283d":"code","fd9ad0d4":"code","266edab0":"code","c374d868":"code","92b17d8a":"code","65a765a3":"code","382fe6ba":"code","ee296cd1":"code","9f33579a":"code","fe490e7d":"code","d9b41044":"code","9578e399":"code","48eb9dce":"code","5ccf5c3e":"code","610e91ae":"code","52accc26":"code","f308e958":"code","cf73a27e":"code","ec0bb712":"code","dd984bf9":"code","17b419ce":"code","6c642363":"code","5b230e32":"code","0ddc12e4":"code","5270c5f9":"code","2f8e66ed":"code","518847d7":"code","6bec8555":"code","7be87d06":"code","07ff302e":"code","dac58146":"code","a7f5871b":"code","37253891":"code","354c3760":"code","bb1aa7b5":"code","0ad56e3a":"code","5c12a471":"code","ce433269":"code","e339eeee":"code","124fd48e":"markdown","5249a929":"markdown","4c8beb60":"markdown","d3f8b8bb":"markdown","c74f1a92":"markdown","c222c275":"markdown","2d7ff90d":"markdown","51256875":"markdown","113754fa":"markdown","a3a9741f":"markdown","f171fcc1":"markdown","585fea6b":"markdown","ee126c19":"markdown","7090454a":"markdown","a163f877":"markdown","7d3422af":"markdown","dddd803b":"markdown","a8433b28":"markdown"},"source":{"290d85fc":"# Load packages\nimport h5py\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport xml.etree.ElementTree as etree\n\nfrom skimage.io import imread\nfrom skimage.transform import resize\n\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications.imagenet_utils import decode_predictions, preprocess_input\nfrom tensorflow.keras.layers import Convolution2D, Dense, Dropout, Flatten, GlobalAveragePooling2D, GlobalMaxPooling2D, Input\nfrom tensorflow.keras.losses import categorical_crossentropy, mean_squared_error\nfrom tensorflow.keras.models import Model","95ec59cb":"# Parse the xml annotation file and retrieve the path to each image, its size and annotations\ndef extract_xml_annotation(filename):\n    \"\"\"Parse the xml file\n    :param filename: str\n    \"\"\"\n    z = etree.parse(filename)\n    objects = z.findall('.\/object')\n    size = (int(float(z.find('.\/\/width').text)), int(float(z.find('.\/\/height').text)))\n    fname = z.find('.\/filename').text\n    dicts = [{obj.find('name').text: [int(float(obj.find('bndbox\/xmin').text)),\n                                      int(float(obj.find('bndbox\/ymin').text)),\n                                      int(float(obj.find('bndbox\/xmax').text)),\n                                      int(float(obj.find('bndbox\/ymax').text))]}\n             for obj in objects]\n    return {'size': size, 'filename': fname, 'objects': dicts}","3ea5c408":"%%time\n# Filters annotations keeping only those we are interested in.\n# We only keep images in which there is a single item\nannotations = []\n\nfilters = ['dog', 'cat', 'bird', 'cow', 'horse']\nidx2labels = {k: v for k, v in enumerate(filters)}\nlabels2idx = {v: k for k, v in idx2labels.items()}\n\nANNOTATION_DIR = '..\/input\/pascal-voc-2012\/VOC2012\/Annotations\/'\nfor filename in sorted(os.listdir(ANNOTATION_DIR)):\n    annotation = extract_xml_annotation(os.path.join(ANNOTATION_DIR, filename))\n    \n    new_objects = []\n    for obj in annotation['objects']:\n        if list(obj.keys())[0] in filters:\n            new_objects.append(obj)\n    \n    if len(new_objects) == 1:\n        annotation['class'] = list(new_objects[0].keys())[0]\n        annotation['bbox'] = list(new_objects[0].values())[0]\n        annotation.pop('objects')\n        annotations.append(annotation)","7214283d":"print(f'Number of images with annotations: {len(annotations)}.')","fd9ad0d4":"print(f'Example contents of one annotation: {annotations[0]}')","266edab0":"print(f'Correspondence between indices and labels: {idx2labels}')","c374d868":"model = ResNet50(include_top=False, weights=\"imagenet\")\ninput_tensor = model.layers[0].input\noutput_tensor = model.layers[-2].output\nmodel_conv = Model(input_tensor, output_tensor)","92b17d8a":"def predict_batch(model, img_batch_path, img_size=None):\n    img_list = []\n    \n    for im_path in img_batch_path:\n        img = imread(im_path)\n        if img_size:\n            img = resize(img, img_size,\n                         mode='reflect',\n                         preserve_range=True)\n            \n        img = img.astype('float32')\n        img_list.append(img)\n    \n    try:\n        img_batch = np.stack(img_list, axis=0)\n    except:\n        raise ValueError('''When both img_size and crop_size are None, all images\n            in image_paths must have the same shapes.''')\n    \n    return model(preprocess_input(img_batch)).numpy()","65a765a3":"# Test the model\nIMG_DIR = '..\/input\/pascal-voc-2012\/VOC2012\/JPEGImages\/'\nIMG_PATH = os.path.join(IMG_DIR, annotations[0]['filename'])\noutput = predict_batch(model_conv, [IMG_PATH], (1000, 224))\nprint(f'Shape of the output: {output.shape}')","382fe6ba":"def compute_representations(annotations):\n    batch_size = 32\n    batches = []\n    \n    n_batches = len(annotations) \/\/ 32 + 1\n    for batch_idx in range(n_batches):\n        batch_bgn = batch_idx * 32\n        batch_end = min(len(annotations), (batch_idx + 1) * 32)\n        img_names = []\n        for annotation in annotations[batch_bgn:batch_end]:\n            img_path = os.path.join(IMG_DIR, annotation['filename'])\n            img_names.append(img_path)\n        batch = predict_batch(model_conv, img_names, img_size=(224, 224))\n        batches.append(batch)\n        print(f'Batch {batch_idx + 1}\/{n_batches} prepared')\n    return np.vstack(batches)","ee296cd1":"%%time\nreprs = compute_representations(annotations)","9f33579a":"# Serialize representations\n#h5f = h5py.File('voc_representations.h5', 'w')\n#h5f.create_dataset('reprs', data=reprs)\n#h5f.close()","fe490e7d":"img_resize = 224\nnum_classes = len(labels2idx.keys())\n\ndef tensorize_ground_truth(annotations):\n    all_boxes = []\n    all_cls = []\n    for idx, annotation in enumerate(annotations):\n        # Build a one-hot encoding of the class\n        cls = np.zeros((num_classes))\n        cls_idx = labels2idx[annotation['class']]\n        cls[cls_idx] = 1.0\n        \n        coords = annotation['bbox']\n        size = annotation['size']\n        \n        # Resize the image\n        x1, y1, x2, y2 = (coords[0] * img_resize \/ size[0],\n                          coords[1] * img_resize \/ size[1],\n                          coords[2] * img_resize \/ size[0],\n                          coords[3] * img_resize \/ size[1])\n        \n        # Compute center of the box and its height and width\n        cx, cy = ((x2 + x1) \/ 2, (y2 + y1) \/ 2)\n        w = x2 - x1\n        h = y2 - y1\n        boxes = np.array([cx, cy, w, h])\n        all_boxes.append(boxes)\n        all_cls.append(cls)\n    \n    # Stack everything into two big np tensors\n    return np.vstack(all_cls), np.vstack(all_boxes)","d9b41044":"classes, boxes = tensorize_ground_truth(annotations)","9578e399":"print(f'Classes shape: {classes.shape}, Boxes shape: {boxes.shape}')","48eb9dce":"def interpret_output(cls, boxes, img_size=(500, 333)):\n    cls_idx = np.argmax(cls)\n    confidence = cls[cls_idx]\n    classname = idx2labels[cls_idx]\n    cx, cy = boxes[0], boxes[1]\n    w, h = boxes[2], boxes[3]\n    \n    small_box = [max(0, cx - w \/ 2), max(0, cy - h \/ 2),\n                 min(img_resize, cx + w \/ 2), min(img_resize, cy + h \/ 2)]\n    fullsize_box = [int(small_box[0] * img_size[0] \/ img_resize),\n                    int(small_box[1] * img_size[1] \/ img_resize),\n                    int(small_box[2] * img_size[0] \/ img_resize),\n                    int(small_box[3] * img_size[1] \/ img_resize)]\n    output = {'class': classname, 'confidence': confidence, 'bbox': fullsize_box}\n    return output","5ccf5c3e":"print(f'Original annotation:\\n {annotations[0]}')\nprint(f'Interpreted output:\\n {interpret_output(classes[0], boxes[0], img_size=annotations[0][\"size\"])}')","610e91ae":"def IoU(boxA, boxB):\n    # Find the intersecting box coordinates\n    x0 = max(boxA[0], boxB[0])\n    y0 = max(boxA[1], boxB[1])\n    x1 = min(boxA[2], boxB[2])\n    y1 = min(boxA[3], boxB[3])\n    \n    # Compute the area of intersection rectangle\n    inter_area = max(x1 - x0, 0) * max(y1 - y0, 0)\n    \n    # Compute the area of each box\n    boxA_area = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n    boxB_area = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n    \n    # Compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of areas minus the intersection areas\n    return inter_area \/ float(boxA_area + boxB_area - inter_area)","52accc26":"original = annotations[0]\ninterpreted = interpret_output(classes[0], boxes[0], img_size=annotations[0][\"size\"])\nprint(f'IoU of the original versus the interpreted bounding box: {IoU(original[\"bbox\"], interpreted[\"bbox\"])}.')","f308e958":"def plot_IoU(boxA, boxB, img_size=(10, 10)):\n    \"\"\"Plot the IoU measure\n    \"\"\"\n    iou = IoU(boxA, boxB)\n    \n    fig, ax = plt.subplots(figsize=(4, 4))\n    ax.set_ylim(0, img_size[1])\n    ax.set_xlim(0, img_size[0])\n    ax.add_patch(plt.Rectangle((boxA[0], boxA[1]),\n                               boxA[2] - boxA[0],\n                               boxA[3] - boxA[1],\n                               color='blue', alpha=0.5))\n    ax.add_patch(plt.Rectangle((boxB[0], boxB[1]),\n                               boxB[2] - boxB[0],\n                               boxB[3] - boxB[1],\n                               color='red', alpha=0.5))\n    ax.set_title(f'IoU: {iou}')\n    return fig, ax","cf73a27e":"fix, ax = plot_IoU([2, 2, 8, 8], [3, 3, 7, 9])\nplt.show()","ec0bb712":"def classif_local_stupid_model(num_classes):\n    \"\"\"Stupid model that averages all the spatial information\n    \n    The goal of this model is to show that it's a very bad idea to destroy\n    the spatial information with GlobalAveragePooling2D layer if our\n    goal is to do object localization.\n    \"\"\"\n    model_input = Input(shape=(7, 7, 2048))\n    x = GlobalAveragePooling2D()(model_input)\n    x = Dropout(0.2)(x)\n    x = Dense(1000)(x)\n    head_classes = Dense(num_classes, activation='softmax', name='head_classes')(x)\n    head_boxes = Dense(4, name='head_boxes')(x)\n    \n    model = Model(inputs=model_input, outputs=[head_classes, head_boxes], name='resnet_loc')\n    model.compile(optimizer='adam', loss=[categorical_crossentropy, mean_squared_error], \n                  loss_weights=[1., 0.01])\n    return model","dd984bf9":"model = classif_local_stupid_model(num_classes)","17b419ce":"num = 64\ninputs = reprs[0:num]\nout_cls, out_boxes = classes[0:num], boxes[0:num]\n\nprint(f'Input batch shape: {inputs.shape}')\nprint(f'Ground truth batch shapes: {out_cls.shape} and {out_boxes.shape}')","6c642363":"out = model.predict(inputs)\nprint(f'Output model batch shapes: {out[0].shape} and {out[1].shape}')","5b230e32":"history = model.fit(inputs, [out_cls, out_boxes], batch_size=10, epochs=10)","0ddc12e4":"plt.plot(np.log(history.history[\"head_boxes_loss\"]), label=\"Bounding boxes log-loss\")\nplt.plot(np.log(history.history[\"head_classes_loss\"]), label=\"Classes log-loss\")\nplt.plot(np.log(history.history[\"loss\"]), label=\"Log-loss\")\nplt.legend(loc=\"best\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()","5270c5f9":"def patch(axis, bbox, display_txt, color):\n    coords = (bbox[0], bbox[1]), bbox[2] - bbox[0] + 1, bbox[3] - bbox[1] + 1\n    axis.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=color, linewidth=2))\n    axis.text(bbox[0], bbox[1], display_txt, color='white', bbox={'facecolor':color, 'alpha':0.5})\n    \ndef plot_annotations(img_path, annotation=None, ground_truth=None):\n    img = imread(img_path)\n    plt.imshow(img)\n    current_axis = plt.gca()\n    if ground_truth:\n        text = \"GT \" + ground_truth[\"class\"]\n        patch(current_axis, ground_truth[\"bbox\"], text, \"red\")\n    if annotation:\n        conf = f'{annotation[\"confidence\"]:0.2f} '\n        text = conf + annotation[\"class\"]\n        patch(current_axis, annotation[\"bbox\"], text, \"blue\")\n    plt.axis('off')\n\ndef display(model, index, ground_truth=True):\n    res = model.predict(reprs[index][np.newaxis])\n    output = interpret_output(res[0][0], res[1][0], img_size=annotations[index][\"size\"])\n    plot_annotations(IMG_DIR + annotations[index][\"filename\"], \n                     output, annotations[index] if ground_truth else None)","2f8e66ed":"plt.figure(figsize=(12, 10))\nfor i in range(15):\n    plt.subplot(3, 5, i + 1)\n    display(model, np.random.randint(reprs.shape[0]))\nplt.show()","518847d7":"def accuracy_and_iou(preds, trues, threshold=0.5):\n    sum_valid, sum_accurate, sum_iou = 0, 0, 0\n    num = len(preds)\n    for pred, true in zip(preds, trues):\n        iou_value = IoU(pred[\"bbox\"], true[\"bbox\"])\n        if pred[\"class\"] == true[\"class\"] and iou_value > threshold:\n            sum_valid = sum_valid + 1\n        sum_iou = sum_iou + iou_value\n        if pred[\"class\"] == true[\"class\"]:\n            sum_accurate = sum_accurate + 1\n    return sum_accurate \/ num, sum_iou \/ num, sum_valid \/ num","6bec8555":"def compute_acc(model, train=True):\n    if train:\n        beg, end = 0, (9 * len(annotations) \/\/ 10)\n        split_name = \"Train\"\n    else:\n        beg, end = (9 * len(annotations)) \/\/ 10, len(annotations) \n        split_name = \"Test\"\n    res = model.predict(reprs[beg:end])\n    outputs = []\n    for index, (classes, boxes) in enumerate(zip(res[0], res[1])):\n        output = interpret_output(classes, boxes,\n                                  img_size=annotations[index][\"size\"])\n        outputs.append(output)\n    \n    acc, iou, valid = accuracy_and_iou(outputs, annotations[beg:end],\n                                       threshold=0.5)\n    \n    print(f'{split_name} acc: {acc:0.3f}, mean IoU: {iou:0.3f}, acc_valid: {valid:0.3f}')","7be87d06":"compute_acc(model, train=True)\ncompute_acc(model, train=False)","07ff302e":"# Keep last examples for test\ntest_num = reprs.shape[0] \/\/ 10\ntrain_num = reprs.shape[0] - test_num\ntest_inputs = reprs[train_num:]\ntest_cls, test_boxes = classes[train_num:], boxes[train_num:]\nprint(f'Length of the train set: {train_num}, length of the test set: {test_num}')","dac58146":"model = classif_local_stupid_model(num_classes)","a7f5871b":"batch_size = 32\ninputs = reprs[0:train_num]\nout_cls, out_boxes = classes[0:train_num], boxes[0:train_num]\n\nhistory = model.fit(inputs, y=[out_cls, out_boxes],\n                    validation_data=(test_inputs, [test_cls, test_boxes]),\n                    batch_size=batch_size, epochs=20, verbose=2)","37253891":"compute_acc(model, train=True)\ncompute_acc(model, train=False)","354c3760":"plt.figure(figsize=(12, 10))\nfor i in range(15):\n    plt.subplot(3, 5, i + 1)\n    display(model, np.random.randint(reprs.shape[0]))\nplt.show()","bb1aa7b5":"def classif_local_model(num_classes):\n    model_input = Input(shape=(7, 7, 2048))\n    x = GlobalAveragePooling2D()(model_input)\n    x = Dropout(0.2)(x)\n    x = Dense(1000)(x)\n    head_classes = Dense(num_classes, activation='softmax', name='head_classes')(x)\n    \n    y = Convolution2D(4, (1, 1), activation='relu', name='hidden_conv')(model_input)\n    y = Flatten()(y)\n    y = Dropout(0.2)(y)\n    head_boxes = Dense(4, name='head_boxes')(y)\n    \n    model = Model(inputs=model_input, outputs=[head_classes, head_boxes], name='resnet_loc')\n    model.compile(optimizer='adam', loss=[categorical_crossentropy, 'mse'], loss_weights=[1., 1 \/ (224 * 224)])\n    return model","0ad56e3a":"model = classif_local_model(num_classes)","5c12a471":"history = model.fit(inputs, y=[out_cls, out_boxes],\n                    validation_data=(test_inputs, [test_cls, test_boxes]),\n                    batch_size=batch_size, epochs=20, verbose=2)","ce433269":"compute_acc(model, train=True)\ncompute_acc(model, train=False)","e339eeee":"plt.figure(figsize=(12, 10))\nfor i in range(15):\n    plt.subplot(3, 5, i + 1)\n    display(model, np.random.randint(reprs.shape[0]))\nplt.show()","124fd48e":"Check the IoU of the bounding box of the original annotation with the bounding box of the interpretation of the resized version of the same annotation is close to $1.0$.","5249a929":"Check the classes and boxes tensors of some known annotations.","4c8beb60":"## Displaying images and bounding box\n\nIn order to display our annotations, we build the function `plot_annotations` as follows:\n* display the image\n* display on top annotations and ground truth bounding boxes and classes\n\nThe `display` function:\n* takes a single index and computes the result of the model\n* interpret the ouput of the model as a bounding box\n* calls the `plot_annotations` function","d3f8b8bb":"# Classification and Localization model\n\nThis notebook is based on the Deep Learning course from the Master Datascience Paris Saclay. Materials of the course can be found [here](https:\/\/github.com\/m2dsupsdlclass\/lectures-labs).\n\n**Goals**\n* Build and train a classification and localisation network.\n* Showcase the flexibility of neural network with several, heterogenous outputs (bounding boxes and classes).\n\n**Dataset**\n* PASCAL VOC 2012 from [Kaggle](https:\/\/www.kaggle.com\/huanghanchina\/pascal-voc-2012)\n\nWe will build the model in three consecutive steps:\n* **Extract label annotations** from a standard Object Detection dataset, namely **PASCAL VOC 2012**.\n* Use a pre-trained image classification model (namely ResNet50) to **precompute convolutional representations** with shape $(7, 7, 2048)$ for all the images in the object detection training set.\n* **Design and train a baseline object detection model with two heads** to predict:\n    - class labels\n    - bounding box coordinates of a single detected object in the image\n    \nNote that the simple baseline model presented here will only detect a single occurence of a class per image. More work would be required to detect all possible object occurences in the images. Take a look at the [slides](https:\/\/m2dsupsdlclass.github.io\/lectures-labs\/slides\/05_conv_nets_2\/index.html#1) for references to states of the art object detection models such as [Faster RCNN](https:\/\/arxiv.org\/abs\/1506.01497) and [YOLO9000](https:\/\/arxiv.org\/abs\/1612.08242).","c74f1a92":"Let's debug the model: select only a few examples and test the model before training with random weights.","c222c275":"## Training on the whole dataset\n\nWe split out dataset into a train and a test dataset. Then, train the model on the whole training set.","2d7ff90d":"## Loading images and annotations\n\nWe will be using PASCAL VOC 2012, a dataset widely used in detection and segmentation [link](http:\/\/host.robots.ox.ac.uk\/pascal\/VOC\/). To lower memory footprint and training time, we will only use five classes: *dog*, *cat*, *bus*, *car*, *aeroplane*. Here are the first steps:\n* Load the annotations file from PASCAL VOC and parse it (xml files)\n* Keep only the annotations we are interested in, and containing a single object\n* Pre-compute ResNet conv5c from the corresponding images","51256875":"## Interpreting output of model\n\nInterpreting the output of the model is going from the output tensors to a set of classes (with confidences) and boxes corrdinates. It corresponds to reverting the previous preocess.","113754fa":"## Computing accuracy\n\nFor each example `(class_true, bbox_true)`, we consider it positive if and only if:\n* the argmax of `output_class` of the model is `class_true`\n* the IoU between the `output_bbox` and the `bbox_true` is above a threshold (usually $0.5$)\n\nThe accuracy of a model is then the number of positive divided be the total number. The following functions compute the class accuracy, IoU average and global accuracy.","a3a9741f":"## Classification and Localisation model\n\nWe build a two headed model for classification and localisation.","f171fcc1":"## Build a better model","585fea6b":"Let's display the predictions of the model and the ground truth annotation for a couple of images in our tiny debugging training set. The class should be right but the localization has little chance to be correct. The model has even more trouble on images that were not part of our tiny debugging training set.","ee126c19":"The output size is given by `(batch_size, 1000\/32 = 32, 224\/32 = 7, 2048)`.\n\n## Compute representations on all images in our annotations","7090454a":"## Intersection over Union\n\nIn order to assess the quality of our model, we will monitor the IoU between ground trith box and predicted box. The following function computes the IoU.","a163f877":"## Pre-computing representations\n\nBefore designing the object detection model itself, we will pre-process all the dataset to project the images as spatial maps in a $(7, 7, 2048)$ dimensional space once and for all. The goal is to avoid repeateadly processing the data from the original images when training the top layers of the detection network.\n\nLet's load a headless pre-trained `ResNet50` model from Keras and all the layers after the `AveragePooling2D` layer (included).","7d3422af":"## Predicting on a batch of images\n\nThe `predict_batch` function is defined as follows:\n* open each image, and resize then to `img_size`\n* stack them as a batch tensor of shape `(batch, img_size_x, img_size_y, 3)`\n* pre-process the batch and make a forward pass with the model","dddd803b":"Now, check whether the loss decreases and eventually if we are able to overfit on these few examples for debugging purpose.","a8433b28":"## Building ground truth from annotation\n\nWe cannot use directly the annotation dictionnary as ground truth in our model. We will build the `y_true` tensor that will be compared to the output of the model.\n\n**Boxes coordinates**\n* THe image is resized to a fixed $224\\times 224$ to be fed to the usual `ResNet50` input, the boxes coordinates of the annotations need to be resized accordingly.\n* We have to convert the top-left and bottom-right coordinates $(x_{min}, y_{min}, x_{max}, y_{max})$ to center, height, width $(x_{c}, y_{c}, w, h)$.\n\n**Classes labels**\n* The class labels are mapped to correspondings indexes."}}