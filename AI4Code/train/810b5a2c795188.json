{"cell_type":{"dd341b47":"code","465dc506":"code","45779958":"code","64e33a41":"code","47cc59c3":"code","34fa6a05":"code","7da05ef8":"code","3a67e560":"code","43c27358":"code","edc2eaae":"code","d5ec0574":"code","6be80823":"code","632ef318":"code","6efbdeba":"code","1bc131ba":"code","5d716a24":"code","8e8b4bb5":"code","23b80221":"code","12e429ec":"code","4832de30":"code","23b601d6":"code","0a431e09":"code","acef8bb3":"code","a4a6e215":"code","3344c960":"code","ea968c84":"markdown","196e58cf":"markdown","9695fc55":"markdown","cd5f38fd":"markdown","ecc0f373":"markdown","8c66161f":"markdown","8d8ecdd6":"markdown","1a58e490":"markdown","1ceb11eb":"markdown","17a139ea":"markdown","d9c71bee":"markdown","9e4b2d04":"markdown","fda6e338":"markdown","c7c3dc3b":"markdown","8b362b80":"markdown","23455242":"markdown","45a8c2b5":"markdown","b8457d05":"markdown","d186da44":"markdown","9a38281a":"markdown","66cdb57c":"markdown","6d3162d7":"markdown","5e8f0e81":"markdown","c7e6e221":"markdown","f0d85571":"markdown"},"source":{"dd341b47":"import gc\nimport itertools\nfrom multiprocessing import cpu_count\nimport random\n\n\nfrom matplotlib import pyplot as plt\nimport mxnet as mx\nimport mxnet.ndarray as F\nfrom mxnet import nd, gluon, autograd\nfrom mxnet.gluon import nn\nimport numpy as np # linear algebra\nimport optuna\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline","465dc506":"mx.__version__","45779958":"def set_seed(seed=2**3):\n    mx.random.seed(42)\n    np.random.seed(seed)\n    random.seed(seed)\nset_seed(121)","64e33a41":"!ls ..\/input\/digit-recognizer\/","47cc59c3":"train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\nsub = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")","34fa6a05":"train.head()","7da05ef8":"plt.imshow(train.iloc[0, 1:].values.reshape(28,28))\nplt.title(f\"train data: label {train.iloc[0, 0]}\")","3a67e560":"#Note that if you use to small batch_size, mxnet use cpu to training.\nbatch_size = 100\n\nntrials = 50\nnum_epochs = 20\nctx = mx.gpu() if mx.context.num_gpus() > 0 else mx.cpu(0)\n\nCPU_COUNT  = cpu_count()","43c27358":"X = [train.iloc[i, 1:].values.reshape(1, 28,28) for i in range(0,len(train))]\ny = [train.iloc[i, 0] for i in range(0,len(train))]","edc2eaae":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)","d5ec0574":"def data_xform(data):\n    \"\"\"Move channel axis to the beginning, cast to float32, and normalize to [0, 1].\"\"\"\n    return data.astype('float32') \/ 255","6be80823":"ds_train = mx.gluon.data.dataset.ArrayDataset(X_train, y_train).transform_first(data_xform)\nds_val = mx.gluon.data.dataset.ArrayDataset(X_val, y_val).transform_first(data_xform)","632ef318":"dl_train = mx.gluon.data.DataLoader(ds_train, batch_size=batch_size, num_workers=CPU_COUNT)\ndl_val = mx.gluon.data.DataLoader(ds_val, batch_size=batch_size, num_workers=CPU_COUNT)","6efbdeba":"del X, y\ngc.collect()","1bc131ba":"class Net(gluon.Block):\n    def __init__(self, **kwargs):\n        super(Net, self).__init__(**kwargs)\n        with self.name_scope():\n            # layers created in name_scope will inherit name space\n            # from parent layer.\n            self.conv1 = nn.Conv2D(20, kernel_size=(5,5))\n            self.pool1 = nn.MaxPool2D(pool_size=(2,2), strides = (2,2))\n            self.conv2 = nn.Conv2D(50, kernel_size=(5,5))\n            self.pool2 = nn.MaxPool2D(pool_size=(2,2), strides = (2,2))\n            self.fc1 = nn.Dense(500)\n            self.fc2 = nn.Dense(10)\n\n    def forward(self, x):\n        x = self.pool1(F.tanh(self.conv1(x)))\n        x = self.pool2(F.tanh(self.conv2(x)))\n        # 0 means copy over size from corresponding dimension.\n        # -1 means infer size from the rest of dimensions.\n        x = x.reshape((0, -1))\n        x = F.tanh(self.fc1(x))\n        x = F.tanh(self.fc2(x))\n        return x","5d716a24":"def get_net():\n    return Net()","8e8b4bb5":"net = get_net()\nnet.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\nnet.summary(nd.zeros((1, 1, 28, 28), ctx=ctx))","23b80221":"def objective(trial):\n    \n    #Candidates to be verified by Optuna.\n    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n    \n    #Definition network, trainer, metric.\n    net = get_net()\n    net.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n\n    trainer = gluon.Trainer(\n        params=net.collect_params(),\n        optimizer=optimizer_name,\n        optimizer_params={'learning_rate': lr},\n    )\n    metric_train = mx.metric.Accuracy()\n    metric_val = mx.metric.Accuracy()\n    softmax_cross_entropy_loss = gluon.loss.SoftmaxCrossEntropyLoss()\n    \n    #Train and validation\n    for epoch in range(num_epochs):\n        \n        for inputs, labels in dl_train:\n\n            with autograd.record():\n                outputs = net(inputs.as_in_context(ctx))\n                loss = softmax_cross_entropy_loss(outputs, labels.as_in_context(ctx))\n\n                loss.backward()\n            metric_train.update(labels, outputs)\n\n            trainer.step(batch_size=inputs.shape[0])\n\n        name, acc = metric_train.get()\n        print('After epoch {}: {} = {}'.format(epoch + 1, name, acc))\n        metric_train.reset()\n\n        for inputs, labels in dl_val:\n            metric_val.update(labels.as_in_context(ctx), net(inputs.as_in_context(ctx)))\n    \n        _, acc_val = metric_val.get()\n        metric_val.reset()\n        print(f\"Validaton:{epoch}={acc_val}\")\n    \n    return acc_val","12e429ec":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=ntrials, timeout=600)\n\nprint(\"Number of finished trials: \", len(study.trials))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"Value: \", trial.value)\n\nprint(\"Params: \")\nfor key, value in trial.params.items():\n    print(\"{}: {}\".format(key, value))","4832de30":"#Definition network, trainer, metric.\noptimizer_name = trial.params[\"optimizer\"]\nlr = trial.params[\"lr\"]\n    \nnet = get_net()\nnet.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n\ntrainer = gluon.Trainer(\n    params=net.collect_params(),\n    optimizer=optimizer_name,\n    optimizer_params={'learning_rate': lr},\n)\nmetric_train = mx.metric.Accuracy()\nmetric_val = mx.metric.Accuracy()\nsoftmax_cross_entropy_loss = gluon.loss.SoftmaxCrossEntropyLoss()\n    \n#Train and validation\nfor epoch in range(num_epochs):\n        \n    for inputs, labels in dl_train:\n\n        with autograd.record():\n            outputs = net(inputs.as_in_context(ctx))\n            loss = softmax_cross_entropy_loss(outputs, labels.as_in_context(ctx))\n\n            loss.backward()\n        metric_train.update(labels, outputs)\n\n        trainer.step(batch_size=inputs.shape[0])\n\n    name, acc = metric_train.get()\n    print('After epoch {}: {} = {}'.format(epoch + 1, name, acc))\n    metric_train.reset()\n\n    for inputs, labels in dl_val:\n        metric_val.update(labels.as_in_context(ctx), net(inputs.as_in_context(ctx)))\n    \n    _, acc_val = metric_val.get()\n    metric_val.reset()\n    print(f\"Validaton:{epoch}={acc_val}\")","23b601d6":"X_test = [test.iloc[i,:].values.reshape(1, 28,28) for i in range(0,len(test))]","0a431e09":"ds_test = mx.gluon.data.dataset.ArrayDataset(X_test).transform_first(data_xform)\ndl_test = mx.gluon.data.DataLoader(ds_test, batch_size=batch_size, num_workers=CPU_COUNT)","acef8bb3":"y = []\nfor inputs in dl_test:\n    outputs = net(inputs.as_in_context(ctx))\n    outputs = [ np.argmax(item.asnumpy()) for item in outputs ]\n    y.append(outputs)\ny = list(itertools.chain.from_iterable(y))    ","a4a6e215":"sub[\"Label\"] = y\nsub.head()","3344c960":"sub.to_csv('submission.csv',index=False)","ea968c84":"In train and test, handwrite number data are saved.","196e58cf":"---------------------","9695fc55":"## Define Parameters\n\nTo train network, we have to define hyperparameters. In particular, we will summarize and define here what we will use as a decisive factor in this notebook.","cd5f38fd":"By [ArrayDataset](https:\/\/mxnet.apache.org\/versions\/1.7\/api\/python\/docs\/api\/gluon\/data\/index.html#mxnet.gluon.data.ArrayDataset), we can create a dataset that combines multiple dataset-like objects, e.g. Datasets, lists, arrays, etc. By [transform_first](https:\/\/mxnet.apache.org\/versions\/1.5.0\/tutorials\/gluon\/transforms.html#using-totensor-and-transform-first), we can only transform data.","ecc0f373":"# How to use mxnet.gluon and tune by optuna\n\nIn this notebook, I'll show how to build and train mxnet.gluon, and tune with optuna. \n\nmxnet is very powerful deep learning framework. mxnet is Developed by Apache and supported by AWS. In kaggle notebook, we need only import, no additional pip install. But most kaggler use pytorch or tensorflow, and there are few knowledge for mxnet. The aim of this notebook is to show how to use mxnet to detect handwrite numbers and tune by optuna.","8c66161f":"## Load data\n\nThere are three data.","8d8ecdd6":"------------","1a58e490":"# Reference\n\n[1] https:\/\/github.com\/apache\/incubator-mxnet\n\n[2] https:\/\/mxnet.apache.org\/versions\/1.7.0\/\n\n[3] https:\/\/mxnet.apache.org\/versions\/1.7\/api\/python\/docs\/api\/gluon\/index.html\n\n[4] https:\/\/medium.com\/apache-mxnet\/using-optuna-to-optimize-gluon-hyperparameters-2fefa2549ba2","1ceb11eb":"## Inference\n\nSame as train data, we can define dataloader for test data.","17a139ea":"Calling model and passing data, we can inference.","d9c71bee":"## Gluon\n\nApache MXNet provides the Gluon library as high level API.\n\n<img src=\"https:\/\/raw.githubusercontent.com\/tasotasoso\/kaggle_media\/main\/MXnet_tune_with_optuna\/gluon.png\" width=\"500\">\n\nGluon is like keras for tensorflow.\n\nWe can build and train deep learning models without sacrificing training speed.\n\nIn this notebook, I'll use gluon for operation of data and model.","9e4b2d04":"# How to use mxnet.gluon","fda6e338":"## Enjoy kaggle with mxnet!!! ","c7c3dc3b":"Now, we can start study by optuna. the best parameters are saved in study.best_trial.","8b362b80":"## Train\n\nWe can train and valid our model with best parameters.","23455242":"By [DataLoader](https:\/\/mxnet.apache.org\/versions\/1.7\/api\/python\/docs\/api\/gluon\/data\/index.html#mxnet.gluon.data.DataLoader), we can load data from dataset. DataLoader returns mini-batches of data.","45a8c2b5":"# What is mxnet?\n\n<img src=\"https:\/\/raw.githubusercontent.com\/tasotasoso\/kaggle_media\/main\/MXnet_tune_with_optuna\/mxnet_logo_2.png\" width=\"400\">\n\nApache MXNet is a deep learning framework designed for both efficiency and flexibility. \n\n\nThere are mainly three characteristics for mxnet.\n\n### <u>1. Portable<\/u>\n  * MXNet supports 10 languages:  Python, Java, C++, R, Scala, Clojure, Go, Javascript, Perl, and Julia.\n  * Cloud-friendly and directly compatible with AWS and Azure.\n  * MXNet works on servers as well as edge devices such as Raspberry Pi, smartphones and laptops.\n\n### <u>2. Efficient<\/u>\n  * Lightweight, memory-efficient, and portable to smart devices through native cross-compilation support on ARM, and through ecosystem projects such as TVM, TensorRT, OpenVINO.\n\n### <u>3. Scalable<\/u>\n  * Scales up to multi GPUs and distributed setting with auto parallelism through ps-lite, Horovod, and BytePS.\n\n<img src=\"https:\/\/raw.githubusercontent.com\/tasotasoso\/kaggle_media\/main\/MXnet_tune_with_optuna\/banner.png\" width=\"800\">","b8457d05":"## Model\n\nWe can define our network by inheriting [gluon.Block](https:\/\/mxnet.apache.org\/versions\/1.7.0\/api\/python\/docs\/api\/gluon\/block.html).  \n\ngluon.Block is base class for all neural network layers and models. Our models should subclass this class.","d186da44":"The data of handwrite numbers are flatten. If we want to show them by matplotlib, I have to reshape them to (28, 28) array.","9a38281a":"## Hyperparameter tuning\n\nWe can use optuna to tune hyperparameter, in mxnet too. To search best parameter, I'll define objective function to train model. \n\nIn the previous section, we put the parameters on the GPU, but we also need to put the data on the GPU. For this, we can use as_in_context(ctx) after read data from dataloader.","66cdb57c":"By intialize(), we can determine the dimensions of weights of the network and initialize the parameters. To compute on GPU, we have to pass ctx to initialize(). Also passing sample data to summary(), we can see model demensions.","6d3162d7":"## Dataset and Dataloder\n\nGluon has API to define a Dataset and use a DataLoader to iterate through the dataset in mini-batches.\n\nFIrst, ","5e8f0e81":"## <u>Contents<\/u>\n\n* What is mxnet?\n\n* How to use mxnet.gluon\n  * Load library\n  * Load data\n  * Define Parameters\n  * Dataset and Dataloder\n  * Model\n  * Hyperparameter tuning\n  * Train\n  * Inference\n\n* Reference","c7e6e221":"------------------------------","f0d85571":"## Load library\n\nI'll load library used in this notebook and set seeds. We can use mxnet by import mxnet module."}}