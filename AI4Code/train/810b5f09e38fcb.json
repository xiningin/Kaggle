{"cell_type":{"faaa184b":"code","2b1d8281":"code","86edd5e3":"code","31c3f80a":"code","91fdce31":"code","b1709665":"code","86b30b75":"code","dfac26c3":"code","09b0a681":"code","3c9b85ce":"code","8514efac":"code","cf3f9a58":"code","76b38afa":"code","58f70015":"code","094739e1":"code","12f8450b":"markdown","9259829a":"markdown","01794734":"markdown","d1a786d0":"markdown","8f2b1f64":"markdown","ade20a99":"markdown","4b53a4a7":"markdown","9bea8336":"markdown","45adba40":"markdown","266dbc74":"markdown","4621d92b":"markdown","c63fd3b4":"markdown","5cbb5c10":"markdown","bbe50f22":"markdown","579dfa75":"markdown","0e780360":"markdown"},"source":{"faaa184b":"# Importing useful libraries and reading data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nhouse_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\").drop(\"Id\", axis=1)\ntest_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\").drop(\"Id\", axis=1)\nhouse_data.head()","2b1d8281":"# Get columns with missing values\ncols_with_missing = [col for col in house_data.columns\n                     if house_data[col].isnull().any()]\n\n# Percent of the data which is NaN\nhouse_data[cols_with_missing].isnull().sum() \/ house_data.shape[0] * 100","86edd5e3":"from sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\n\n# Classifying missing features\n\n# These numerical features are going to be mean-imputed beacuse of the risk of filling them with zeros.\n# Imputing \"GarageYrBuilt\" with zero, for instance, would suggest some garages were built on year 0\n# and would have catastrophic consequences for our model \nmean_impute_feats = [\"GarageYrBlt\", \"LotFrontage\", \"MasVnrArea\"]\n\n# We see the rest are all categorical and connected to the nonexistence of some characteristic.\n# In this case, it's preferable to let the encoding process to remark the abscense\ncat_abs_feats = [\"Alley\", \"MasVnrType\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \n                 \"BsmtFinType1\", \"BsmtFinType2\", \"Electrical\", \"FireplaceQu\",\n                 \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\",\n                 \"PoolQC\", \"Fence\", \"MiscFeature\"]\n\n# Splitting data into subsets\nX = house_data.drop(\"SalePrice\", axis=1)\ny = house_data.SalePrice\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state=22)\n\n# Impute\nX_train_imputed = X_train.copy()\nX_valid_imputed = X_valid.copy()\nX_test_imputed = test_data.copy()\n\nimputer = SimpleImputer(strategy=\"mean\")\nX_train_imputed[mean_impute_feats] = imputer.fit_transform(X_train_imputed[mean_impute_feats])\nX_train_imputed[cat_abs_feats] = X_train_imputed[cat_abs_feats].fillna(\"missing\")\nX_valid_imputed[mean_impute_feats] = imputer.transform(X_valid_imputed[mean_impute_feats])\nX_valid_imputed[cat_abs_feats] = X_valid_imputed[cat_abs_feats].fillna(\"missing\")\n","31c3f80a":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Function to get MAE\ndef get_mae(X_train, y_train, X_valid, y_valid, n_estimators=1000, learning_rate=0.05):\n    base_model = XGBRegressor(n_estimators=n_estimators,\n                              learning_rate=learning_rate, random_state=22)\n    base_model.fit(X_train, y_train, early_stopping_rounds=5,\n               eval_set=[(X_valid, y_valid)], verbose=False)\n    base_preds = base_model.predict(X_valid)\n    return mean_absolute_error(y_valid, base_preds)\n\n# To keep thing simple, we drop categorical columns\ncat_features = [col for col in X_train.columns\n                if X_train[col].dtypes not in [\"float64\", \"int64\"]]\n\nbase_X_train = X_train_imputed.drop(cat_features, axis=1)\nbase_X_valid = X_valid_imputed.drop(cat_features, axis=1)\n\n# Then, we define the model\nbase_mae = get_mae(base_X_train, y_train, base_X_valid, y_valid)\n\n# We can now create a Dataframe to start storing different MAE scores.\n# This will let us keep track of our progress. \nmae_scores = pd.DataFrame({\n    \"Step\": [\"baseline model\"],\n    \"Score\": [base_mae]\n})\nprint(f\"MAE for baseline model: {base_mae}\")","91fdce31":"from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\n# We'll first try using one-hot-encoding exclusively\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train_imputed[cat_features]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid_imputed[cat_features]))\n\n# Putting back index\nOH_cols_train.index = X_train_imputed.index\nOH_cols_valid.index = X_valid_imputed.index\n\n# Replacing categorical columns with one-hot-encoded values\nnum_X_train = X_train_imputed.drop(cat_features, axis=1)\nnum_X_valid = X_valid_imputed.drop(cat_features, axis=1)\n\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n\n\nmae_scores.loc[1] = [\"OH encoding\", get_mae(OH_X_train, y_train, OH_X_valid, y_valid)]\nmae_scores","b1709665":"# We first need to divide categorical features in low and high cardinality columns\nlow_card_cols = [col for col in cat_features\n                  if len(X_train_imputed[col].unique()) < 9]\nhigh_card_cols = [col for col in cat_features\n                  if col not in low_card_cols]\n\n# Then, we proceed with one-hot-encoding as before, but only encoding low cardinality columns\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train_imputed[low_card_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid_imputed[low_card_cols]))\n\n# Putting back index\nOH_cols_train.index = X_train_imputed.index\nOH_cols_valid.index = X_valid_imputed.index\n\n# Replacing categorical columns with one-hot-encoded values\nnum_X_train = X_train_imputed.drop(low_card_cols, axis=1)\nnum_X_valid = X_valid_imputed.drop(low_card_cols, axis=1)\n\nmix_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nmix_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n\n# Finally, we apply our ordinal encoder on high cardinality features\nord_encoder = OrdinalEncoder()\nmix_X_train[high_card_cols] = ord_encoder.fit_transform(mix_X_train[high_card_cols])\nmix_X_valid[high_card_cols] = ord_encoder.transform(mix_X_valid[high_card_cols])\n\nmae_scores.loc[2] = [\"mixed encoding\", get_mae(mix_X_train, y_train, mix_X_valid, y_valid)]\nmae_scores","86b30b75":"from sklearn.feature_selection import mutual_info_regression\n\n# Concatenate imputed data to analyse it completely\nX_imputed = pd.concat([X_train_imputed, X_valid_imputed], axis=0)\n\n# Include only numerical features. Extract discrete ones to use them next on MI scores\nnum_features = [col for col in X.columns\n                if X[col].dtypes in [\"float64\", \"int64\"]]\ndisc_features = (X_imputed[num_features].dtypes == int)\n\n# Get Mutual Information scores\nmi_scores = mutual_info_regression(X_imputed[num_features], y, disc_features)\nmi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X_imputed[num_features].columns)\nmi_scores = mi_scores.sort_values(ascending=False)\n\n# Plot only MI scores higher than zero\n\nplt.figure(figsize=(10, 7))\nsns.barplot(x=mi_scores[mi_scores > 0], y=mi_scores[mi_scores > 0].index)","dfac26c3":"fig, ax = plt.subplots(1, 2, figsize=(10,5))\n\nsns.scatterplot(data=house_data, x=\"1stFlrSF\", y=\"SalePrice\", ax=ax[0])\nsns.boxplot(data=house_data, x=\"MoSold\", y=\"SalePrice\", ax=ax[1])","09b0a681":"SF_features = [\"LotArea\",\"1stFlrSF\", \"2ndFlrSF\", \"TotalBsmtSF\", \"GrLivArea\", \"LotFrontage\",\n               \"MasVnrArea\", \"GarageArea\", \"PoolArea\"]\n\n# Standardize the features\nX_SF = X_imputed.loc[:, SF_features]\nX_SF_scaled = (X_SF - X_SF.mean(axis=0)) \/ X_SF.std(axis=0)\nfor col in SF_features:\n    print(f\"{col}\\t min: {X_SF_scaled[col].min()}\\t max: {X_SF_scaled[col].max()}\")","3c9b85ce":"from sklearn.decomposition import PCA\n\n# Create principal components\npca = PCA()\nX_pca = pca.fit_transform(X_SF_scaled)\n\n# Convert to dataframe\ncomp_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\nX_pca = pd.DataFrame(X_pca, columns=comp_names)\n\n# Get the loadings\nloadings = pd.DataFrame(pca.components_.T, columns=comp_names, index=X_SF.columns)\n\nplt.figure(figsize=(10, 7))\nsns.heatmap(loadings, annot=True)","8514efac":"feat_X_train = mix_X_train.copy()\nfeat_X_valid = mix_X_valid.copy()\n\nfig, ax = plt.subplots(3, 2, figsize=(15,10))\n\n# Houses with a second floor are likely to have higher prices,\n# so we mathematically penalize the other values.\nfeat_X_train[\"2ndFlrProp\"] = mix_X_train[[\"2ndFlrSF\", \"GrLivArea\"]].sum(axis=1) * np.sqrt(mix_X_train[[\"1stFlrSF\", \"TotalBsmtSF\"]].sum(axis=1))\nfeat_X_valid[\"2ndFlrProp\"] = mix_X_valid[[\"2ndFlrSF\", \"GrLivArea\"]].sum(axis=1) * np.sqrt(mix_X_valid[[\"1stFlrSF\", \"TotalBsmtSF\"]].sum(axis=1))\nsns.regplot(x=feat_X_train[\"2ndFlrProp\"], y=y_train, ax=ax[0, 0])\n\n# In this case, we use a logarithmic transformation to handle data's skewness\nfeat_X_train[\"ExtraSF\"] = (mix_X_train[[\"LotArea\", \"MasVnrArea\"]].sum(axis=1) \/ (mix_X_train[\"LotFrontage\"])).transform(np.log)\nfeat_X_valid[\"ExtraSF\"] = (mix_X_valid[[\"LotArea\", \"MasVnrArea\"]].sum(axis=1) \/ (mix_X_valid[\"LotFrontage\"])).transform(np.log)\nsns.regplot(x=feat_X_train[\"ExtraSF\"], y=y_train, ax=ax[0, 1])\n\n# Multiply MasVnrArea by the proportion of \"non-livable\" area\nfeat_X_train[\"MasVnr_to_OutSF\"] = mix_X_train[\"MasVnrArea\"] * (mix_X_train[[\"PoolArea\", \"LotFrontage\"]].sum(axis=1) \/ mix_X_train[\"LotArea\"])\nfeat_X_valid[\"MasVnr_to_OutSF\"] = mix_X_valid[\"MasVnrArea\"] * (mix_X_valid[[\"PoolArea\", \"LotFrontage\"]].sum(axis=1) \/ mix_X_valid[\"LotArea\"])\nsns.regplot(x=feat_X_train[\"MasVnr_to_OutSF\"], y=y_train, ax=ax[1, 0])\n\n# Non-livable and livable area proportion\nfeat_X_train[\"Livable\"] = mix_X_train[[\"MasVnrArea\", \"LotFrontage\"]].sum(axis=1) \/ mix_X_train[[\"1stFlrSF\", \"TotalBsmtSF\", \"GrLivArea\"]].sum(axis=1)\nfeat_X_valid[\"Livable\"] = mix_X_valid[[\"MasVnrArea\", \"LotFrontage\"]].sum(axis=1) \/ mix_X_valid[[\"1stFlrSF\", \"TotalBsmtSF\", \"GrLivArea\"]].sum(axis=1)\nsns.regplot(x=feat_X_train[\"Livable\"], y=y_train, ax=ax[1, 1])\n\n# \"Special\" living surface (not every house has both a basement and a second floor)\nfeat_X_train[\"SpecialLivSF\"] = mix_X_train[[\"TotalBsmtSF\", \"2ndFlrSF\"]].sum(axis=1) \/ mix_X_train[[\"1stFlrSF\", \"GrLivArea\"]].sum(axis=1)\nfeat_X_valid[\"SpecialLivSF\"] = mix_X_valid[[\"TotalBsmtSF\", \"2ndFlrSF\"]].sum(axis=1) \/ mix_X_valid[[\"1stFlrSF\", \"GrLivArea\"]].sum(axis=1)\nsns.regplot(x=feat_X_train[\"SpecialLivSF\"], y=y_train, ax=ax[2, 0])\n\nmae_scores.loc[3] = [\"PCA\", get_mae(feat_X_train, y_train, feat_X_valid, y_valid)]\nmae_scores","cf3f9a58":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\n# Creating a simple model that will be passed through the Permutation Importance function\nxgb_model = XGBRegressor(n_estimators=1000,\n                              learning_rate=0.05, random_state=22)\nxgb_model.fit(feat_X_train, y_train, early_stopping_rounds=5,\n               eval_set=[(feat_X_valid, y_valid)], verbose=False)\n\n# Apply Permutation Importance\n# Since some columns have a integer as a name, we make sure of converting all values to string\nfeature_names = list(map(lambda x: str(x), feat_X_valid.columns.tolist()))\n\nperm = PermutationImportance(xgb_model, random_state=1).fit(feat_X_valid, y_valid)\neli5.show_weights(perm, feature_names = feature_names)","76b38afa":"from pdpbox import pdp, get_dataset, info_plots\n\n# Create data and then plot it\npdp_prop = pdp.pdp_isolate(model=xgb_model, dataset=feat_X_valid,\n                      model_features=feat_X_valid.columns.tolist(), feature=\"2ndFlrProp\")\npdp.pdp_plot(pdp_prop, \"Second Floor Proportion\")\nplt.show()","58f70015":"import shap\nexplainer = shap.TreeExplainer(xgb_model)\nshap_values = explainer.shap_values(feat_X_valid)\nshap.summary_plot(shap_values, feat_X_valid)","094739e1":"mae_scores","12f8450b":"However, I'm aware this is only the beginning, since there are many steps like Explainability which I actually need to learn how to exploit better. What's next? Giving experts' notebooks a look, applying their recommendations and much more I'll find out soon.","9259829a":"# Conclusion\nTutorials are, in my opinion, an excellent starting point when diving into the world of machine learning for the first time. In combination, all the tecniques they teach has let us reduce our error from 16,171 to 14,978. ","01794734":"Next we'll put into practice our first option.","d1a786d0":"Again, our model has improved, this time by applying Principal Component Analysis.\n# Model Explainability\nAfter all this process, it would be convenient to know what the model is actually doing, i.e. how much weight it \"assigns\" to each feature. This will let us know weather some of them are useful or do not contribute at all.\n### Permutation importance\nWe'll begin with this tecnique, which measures how a model is affected when a column gets all its values randomly sorted.","8f2b1f64":"From this, we see that:\n* Surface features,as we could expect, have a clear relationship with our target variable\n* Some chronological data could be important for our future final model\n\nThis has sense in the case of \"1stFlrSF\", as one of the most important qualities of a building is its available space. We see a positive linear relationship when we plot this feature and \"SalePrice\"","ade20a99":"For instance, we see:\n1. PC1 simply found the \"size\" linear relationship\n2. PC2 mainly compares houses with a large first floor and basement areas to those ones which also include a second floor but have a smaller lower area\n\nIt'll be interesting to make features from the relationships which seem useful.\n","4b53a4a7":"As expected, the overall quality was the one which mostly affected the model when shuffled. But also, good news! One of the columns added in the feature engineering process, \"2ndFlrProp\", takes the second place.\nWe will further investigate how this feature affects the model using another tecnique.\n### Partial Dependence Plots\nPartial plots let us see how different values of a feature make the prediction change. As mentioned before, we'll use \"2ndFlrProp\" in this case.","9bea8336":"### Let's start encoding\nAfter that, the next step is to encode categorical features. An interesting approach is to one-hot-encode those ones with fewer categories and use an ordinal encoder for the rest, but we should also figure out wether using only OneHotEncoder isn't the best option.","45adba40":"However, regardless what MI scores tell us about MoSold, it's not that easy to see the pattern visually. It will be better to start analysing these features' relationship.\n### Principal Component Analysis\nPCA is a efficient method to extract these relationships. First of all, we'll get the principal components. After that, we are going to create new features apllying mathematical transforms.\nA good starting point is passing surface features through the PCA algorithm. To do so, we begin by standardizing them.","266dbc74":"Now that we know which features do contain missing values, we have to classify them by the *type of absence*. Why? Because, depending on data description, a missing value could mean the surveyor truly missed something or he left values blank purposely, for instance.\n\nTo do so, we'll compare these features with similar ones. Since the plotting process would span a considerable length, it won't be included.","4621d92b":"# Introduction\nThroughout this notebook, we'll aply the main tools provided by Kaggle's free courses, as well as some other techniques. The idea is to go through a step-by-step engineering process, focusing on the different sections a Data Science project implies.\n\nIn this case, I'll be using the \"House Prices - Advanced Regression Techniques\" dataset, which is intended to be an adequate starting point for begginers. Note that, since this dataset has detailed descriptions of its features and it's used in many of the courses, we'll skip the Exploratory Data Analysis step.\n\n**Disclaimer:** I'm not an expert and I don't pretend to be so. As a beginner, I do this as a way of putting knowledge into practice. In fact, constructive criticism is useful and welcome!","c63fd3b4":"# Data Cleaning\nDue to all the features we have, it will be useful (and necessary) to check every column containing missing values, then checking which is the best imputation method for each of them. In the case of features related to surface, for instance, we should impute 0. For categorical ones, however, we should check whether NaN values mean the abscense of something.\n\nBefore imputing, we must get the train and test subsets so that the model doesn't overfit.","5cbb5c10":"Since our mixed approach worked slightly better, we'll use it for the rest of the notebook.\n# Feature Engineering\nAfter having covered some of the basics, it's time to find relationships between features and create new ones. We could begin by having a look at features' Mutual Information.","bbe50f22":"As wee see, many features have a positive relationship with the SHAP values, except obvious cases like \"BsmtUnfSf\" (basement unfinished surface). But what we care about the most is the importance of each one of them.","579dfa75":"# Encoding\nFirst of all, a model fed with raw features will serve as a baseline. We first define a function to get mae easily. This will come in handy throughout the notebook.","0e780360":"This tells us that the predictions will be positively and almost linearly related to the feature values, although there is a slight depression between 50000 and 100000.\nIn the end, however, this doesn't really give us valuable information. We'll take one last step.\n### SHAP values\nSHAP values on a summary plot, as seen through the tutorials, will show us a wide view of the features' effect on the model."}}