{"cell_type":{"7e7da892":"code","bec9890d":"code","d1ea43c3":"code","46222564":"code","4b36dc0d":"code","6ac806b3":"code","9e711ea2":"code","59de5a92":"code","68b9171b":"code","41ded2ab":"code","b63baeca":"code","5e374970":"code","d82705f6":"code","83d7f26c":"code","8ed9cbd1":"code","382cf8ab":"code","bda5adf0":"code","2c20ff33":"code","de16c76f":"code","fbc0895a":"code","4ffa33e2":"code","a4bfda0a":"code","81f55090":"code","6cf704e1":"code","dcb84f8c":"code","aca9ec06":"code","af4b7936":"code","4bf2c394":"code","ab92a08d":"code","147a8203":"code","403f239e":"code","06bbb03d":"code","57a21da3":"code","22889755":"code","4dbbaaa5":"code","449a1d6e":"code","1648270c":"code","51c157f4":"code","7f26d7d4":"code","fb889fee":"code","7f1a659f":"code","aab280c5":"code","824249f5":"code","ac4305b8":"code","e4943e12":"code","70636a81":"code","b3302844":"code","f5c00a40":"code","e768a64e":"code","3a7e0077":"code","9e1dbb92":"markdown","e6cb4862":"markdown","1fb2287d":"markdown","78964c34":"markdown","5087fff7":"markdown","530200a0":"markdown","acf55a40":"markdown","e8a060e7":"markdown","c993aafa":"markdown","65e88240":"markdown","67fc1f97":"markdown","52feb3f6":"markdown","750f007c":"markdown","e51957da":"markdown","3a34017b":"markdown","ca12ad85":"markdown","22f17521":"markdown","5a472e74":"markdown","1090268e":"markdown","300fd248":"markdown","f03c811d":"markdown","89053adc":"markdown","64252107":"markdown","d9fa439d":"markdown","be8f56f0":"markdown","323e7d6c":"markdown","9c5618dc":"markdown","e5be4c63":"markdown","a977fb3d":"markdown","b7cdee15":"markdown","5c9ed094":"markdown","ccc902f7":"markdown","7dc4e829":"markdown","d654db32":"markdown","b4923321":"markdown","757fbb6a":"markdown","4e144755":"markdown","f98dcac6":"markdown","8d42c1db":"markdown","7a782eee":"markdown","f1bf2e94":"markdown","37089867":"markdown","a077b878":"markdown","263d84dc":"markdown","f806b2ea":"markdown","9edc96f3":"markdown","6ed95c11":"markdown","5c4c244e":"markdown","ec99c63e":"markdown","971dc26e":"markdown"},"source":{"7e7da892":"#importing relevant libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nSEED = 42\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport warnings\nwarnings.filterwarnings('ignore')","bec9890d":"#Data Collection \ndf = pd.read_csv(r'..\/input\/diabetes-prediction\/Diabetes.csv')\n#Displaying the first five columns\ndf.head()","d1ea43c3":"#Checking whether null value is present or not\ndf.info()\ndf.isnull().sum()","46222564":"#Printing target variable 'Outcome' & Visualizing it\ndf = pd.read_csv(r'..\/input\/diabetes-prediction\/Diabetes.csv')\nprint(df.Outcome.value_counts())\ndf['Outcome'].value_counts().plot(kind='bar').set_title('Diabetes Outcome')","4b36dc0d":"#Data Pre-processing\ndf.describe()","6ac806b3":"#Pictorial representation of the features before conversion","9e711ea2":"p = df.hist(figsize = (20,20))","59de5a92":"df.Glucose.replace(0, np.nan, inplace=True)\ndf.Glucose.replace(np.nan, df['Glucose'].median(), inplace=True)\ndf.BloodPressure.replace(0, np.nan, inplace=True)\ndf.BloodPressure.replace(np.nan, df['BloodPressure'].median(), inplace=True)\ndf.SkinThickness.replace(0, np.nan, inplace=True)\ndf.SkinThickness.replace(np.nan, df['SkinThickness'].median(), inplace=True)\ndf.Insulin.replace(0, np.nan, inplace=True)\ndf.Insulin.replace(np.nan, df['Insulin'].median(), inplace=True)\ndf.BMI.replace(0, np.nan, inplace=True)\ndf.BMI.replace(np.nan, df['BMI'].median(), inplace=True)","68b9171b":"#Pictorial representation of the features after conversion","41ded2ab":"p = df.hist(figsize = (20,20))","b63baeca":"#Displaying the shape of the dataset after cleaning\nX, y = df.drop('Outcome', axis=1), df['Outcome']\nprint(X.shape, y.shape)","5e374970":"g = sns.heatmap(df.corr(),cmap=\"BrBG\",annot=False)","d82705f6":"p=sns.pairplot(df, hue = 'Outcome')","83d7f26c":"#The above graph has been put in a tabular format\ndf.corr()","8ed9cbd1":"from sklearn.ensemble import RandomForestClassifier\nSEED = 42\n\nX, y = df.drop('Outcome', axis=1), df['Outcome']\nrfc = RandomForestClassifier(random_state=SEED, n_estimators=100)\nrfc_model = rfc.fit(X, y)\n(pd.Series(rfc_model.feature_importances_, index=X.columns)\n    .nlargest(8)\n    .plot(kind='barh', figsize=[8,4])\n    .invert_yaxis())\nplt.yticks(size=15)\nplt.title('Top Features derived by Random Forest', size=20)","382cf8ab":"#Dropping the Target Variable\nX = df.drop('Outcome', axis=1)   \ny = df['Outcome']","bda5adf0":"# Splitting data to 80:20 ratio for train\/test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)\n\n#Printing the shape of the dataset after dropping 'Outcome'\nprint('X_train', X_train.shape)\nprint('y_train', y_train.shape)\nprint('X_test', X_test.shape)\nprint('y_test', y_test.shape)","2c20ff33":"#Importing Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nclassifier = DecisionTreeClassifier()\nclassifier = classifier.fit(X_train,y_train)\n\n#Prediction Variable\ny_pred = classifier.predict(X_test)","de16c76f":"#Confusion Matrix\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score,f1_score\nresult = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(result)\nresult1 = classification_report(y_test, y_pred)\nprint(\"Classification Report:\",)\nprint (result1)\nresult2 = accuracy_score(y_test,y_pred)\nresult3 = precision_score(y_test,y_pred)\nresult4 = recall_score(y_test,y_pred)\nresult5 = f1_score(y_test,y_pred)\n\n#Printing the Accuracy of the Classifier\nprint(\"Accuracy of Decision Tree Classifier is\",result2)\nprint(\"Precision of Decision Tree Classifier is\",result3)\nprint(\"Recall of Decision Tree Classifier is\",result4)\nprint(\"F1 of Decision Tree Classifier is\",result5)","fbc0895a":"import seaborn as sns\nsns.heatmap(result, annot=True)","4ffa33e2":"#Importing Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nclassifier = RandomForestClassifier(n_estimators=50)\nclassifier.fit(X_train, y_train)\n\n#Prediction Variable\ny_pred = classifier.predict(X_test)","a4bfda0a":"#Confusion Matrix\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score,precision_score, recall_score,f1_score\nresult = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(result)\nresult1 = classification_report(y_test, y_pred)\nprint(\"Classification Report:\",)\nprint (result1)\nresult2 = accuracy_score(y_test,y_pred)\nresult3 = precision_score(y_test,y_pred)\nresult4 = recall_score(y_test,y_pred)\nresult5 = f1_score(y_test,y_pred)\n\n#Printing the Accuracy of the Classifier\nprint(\"Accuracy of Random Forest Classifier is\",result2)\nprint(\"Precision of Random Forest Classifier is\",result3)\nprint(\"Recall of Random Forest Classifier is\",result4)\nprint(\"F1 of Random Forest Classifier is\",result5)","81f55090":"import seaborn as sns\nsns.heatmap(result, annot=True)","6cf704e1":"#Importing Logisitic Regression Classifier\nfrom sklearn import linear_model\nfrom sklearn import metrics","dcb84f8c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nclassifier = linear_model.LogisticRegression()\nclassifier.fit(X_train, y_train)\n\n#Prediction Variable\ny_pred = classifier.predict(X_test)","aca9ec06":"#Confusion Matrix\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score,precision_score, recall_score,f1_score\nresult = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(result)\nresult1 = classification_report(y_test, y_pred)\nprint(\"Classification Report:\",)\nprint (result1)\nresult2 = accuracy_score(y_test,y_pred)\nresult3 = precision_score(y_test,y_pred)\nresult4 = recall_score(y_test,y_pred)\nresult5 = f1_score(y_test,y_pred)\n\n#Printing the Accuracy of the Classifier\nprint(\"Accuracy of Logistic Regression is\",result2)\nprint(\"Precision of Logistic Regression is\",result3)\nprint(\"Recall of Logistic Regression is\",result4)\nprint(\"F1 of Logistic Regression is\",result5)","af4b7936":"import seaborn as sns\nsns.heatmap(result, annot=True)","4bf2c394":"#Importing the kNeighbors Classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 8)\nclassifier.fit(X_train, y_train)\n\n#Prediction Variable\ny_pred = classifier.predict(X_test)","ab92a08d":"#Confusion Matrix\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score,precision_score, recall_score,f1_score\nresult = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(result)\nresult1 = classification_report(y_test, y_pred)\nprint(\"Classification Report:\",)\nprint (result1)\nresult2 = accuracy_score(y_test,y_pred)\nresult3 = precision_score(y_test,y_pred)\nresult4 = recall_score(y_test,y_pred)\nresult5 = f1_score(y_test,y_pred)\n\n#Printing the Accuracy of the Classifier\nprint(\"Accuracy of k-Nearest Neighbour is\",result2)\nprint(\"Precision of k-Nearest Neighbour is\",result3)\nprint(\"Recall of k-Nearest Neighbour is\",result4)\nprint(\"F1 of k-Nearest Neighbour is\",result5)","147a8203":"import seaborn as sns\nsns.heatmap(result, annot=True)","403f239e":"#Importing Gaussian Naive Bayes Classifier\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\n#Prediction Variable\ny_pred = classifier.predict(X_test)","06bbb03d":"#Confusion Matrix\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nresult = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(result)\nresult1 = classification_report(y_test, y_pred)\nprint(\"Classification Report:\",)\nprint (result1)\nresult2 = accuracy_score(y_test,y_pred)\nresult3 = precision_score(y_test,y_pred)\nresult4 = recall_score(y_test,y_pred)\nresult5 = f1_score(y_test,y_pred)\n\n#Printing the Accuracy of the Classifier\nprint(\"Accuracy of Gaussian Naive Bayes is\",result2)\nprint(\"Precision of Gaussian Naive Bayes is\",result3)\nprint(\"Recall of Gaussian Naive Bayes is\",result4)\nprint(\"F1 of Gaussian Naive Bayes is\",result5)","57a21da3":"import seaborn as sns\nsns.heatmap(result, annot=True)","22889755":"models = []\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('LR', linear_model.LogisticRegression()))\nmodels.append(('DT', DecisionTreeClassifier()))\nmodels.append(('GNB', GaussianNB()))\nmodels.append(('RF', RandomForestClassifier()))","4dbbaaa5":"names = []\nscores = []\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    scores.append(accuracy_score(y_test, y_pred))\n    names.append(name)\ntr_split = pd.DataFrame({'Name': names, 'Accuracy Score': scores})\nprint(tr_split)","449a1d6e":"names = []\nscores = []\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    scores.append(precision_score(y_test, y_pred))\n    names.append(name)\ntr_split = pd.DataFrame({'Name': names, 'Precision Score': scores})\nprint(tr_split)","1648270c":"names = []\nscores = []\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    scores.append(recall_score(y_test, y_pred))\n    names.append(name)\ntr_split = pd.DataFrame({'Name': names, 'Recall Score': scores})\nprint(tr_split)","51c157f4":"names = []\nscores = []\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    scores.append(f1_score(y_test, y_pred))\n    names.append(name)\ntr_split = pd.DataFrame({'Name': names, 'F1 Score': scores})\nprint(tr_split)","7f26d7d4":"# Random forest\nmodelRF= RandomForestClassifier(n_estimators=100,random_state=0)\nmodelRF.fit(X_train,y_train)\ny_pred_prob_rf = modelRF.predict_proba(X_test)[:,1]\nfpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_pred_prob_rf)\n\n# Gaussian Naive Bayes\nmodelNB= GaussianNB()\nmodelNB.fit(X_train,y_train)\ny_pred_prob_nb = modelNB.predict_proba(X_test)[:,1]\nfpr_nb, tpr_nb, thresholds_nb = roc_curve(y_test, y_pred_prob_nb)\n\n# Logistic regression\nmodelLR = linear_model.LogisticRegression()\nmodelLR.fit(X_train,y_train)\ny_pred_prob_lr = modelLR.predict_proba(X_test)[:,1]\nfpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, y_pred_prob_lr)\n\n# KNN\nmodelKNN = KNeighborsClassifier(n_neighbors=3)\nmodelKNN.fit(X_train,y_train)\ny_pred_prob_KNN = modelKNN.predict_proba(X_test)[:,1]\nfpr_KNN, tpr_KNN, thresholds_KNN = roc_curve(y_test, y_pred_prob_KNN)\n\n# Decision Tree\nmodelTree=DecisionTreeClassifier()\nmodelTree.fit(X_train,y_train)\ny_pred_prob_Tree = modelTree.predict_proba(X_test)[:,1]\nfpr_Tree, tpr_Tree, thresholds_Tree = roc_curve(y_test, y_pred_prob_Tree)","fb889fee":"from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# generate two class dataset\nX, y = make_classification(n_samples=1000, n_classes=2, n_features=20, random_state=27)\n\n# split into train-test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=27)\n\nfrom sklearn.metrics import roc_curve\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)","7f1a659f":"import matplotlib.pyplot as plt\nplt.style.use('seaborn')\n\n# plot roc curves\nplt.plot(fpr_rf, tpr_rf, linestyle='-',color='green',label='Random Forest')\nplt.plot(fpr_nb, tpr_nb, linestyle='-',color='pink', label='Gaussian Naive Bayes')\nplt.plot(fpr_lr, tpr_lr, linestyle='-',color='orange', label='Logistic Regression')\nplt.plot(fpr_KNN, tpr_KNN, linestyle='-',color='purple', label='K Nearest Neighbor')\nplt.plot(fpr_Tree, tpr_Tree, linestyle='-',color='yellow', label='Decision Tree')\n\n\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve for the algorithms')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\n\nplt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();","aab280c5":"#Importing KFold Cross Validation\nfrom sklearn.model_selection import KFold, cross_val_score","824249f5":"names = []\nscores = []\nfor name, model in models:\n    \n    kfold = KFold(n_splits=10, random_state=10) \n    score = cross_val_score(model, X, y, cv=kfold, scoring='accuracy').mean()\n    \n    names.append(name)\n    scores.append(score)\n    \nkf_cross_val = pd.DataFrame({'Name': names, 'CV Score': scores})\nprint(kf_cross_val)","ac4305b8":"axis = sns.barplot(x = 'Name', y = 'CV Score', data = kf_cross_val)\naxis.set(xlabel='Classifier', ylabel='Accuracy')\nfor p in axis.patches:\n    height = p.get_height()\n    axis.text(p.get_x() + p.get_width()\/2, height + 0.005, '{:1.4f}'.format(height), ha=\"center\") \n    \nplt.show()","e4943e12":"#To find whether the model is Overfitting\n\n#Accuracy Scores of Training Data\nnames = []\nscores = []\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    scores.append(accuracy_score(y_test, y_pred))\n    names.append(name)\ntr_split = pd.DataFrame({'Name': names, 'Accuracy Score': scores})\nprint(tr_split)\n\n#Accuracy Scores of Validation Data\nnames = []\nscores = []\nfor name, model in models:\n    \n    kfold = KFold(n_splits=10, random_state=10) \n    score = cross_val_score(model, X, y, cv=kfold, scoring='accuracy').mean()\n    \n    names.append(name)\n    scores.append(score)\nkf_cross_val = pd.DataFrame({'Name': names, 'CV Score': scores})\nprint(kf_cross_val)","70636a81":"print(modelRF, 'accuracy score is')\nprint('Training: {:.2f}%'.format(100*model.score(X_train, y_train)))  # score uses accuracy\naccuracy      = model.score(X_test, y_test)\nprint('Test set: {:.2f}%'.format(100*accuracy))","b3302844":"print('Precision: {:.4f},   Recall: {:.4f},   F1 Score: {:.4f}'.format(\nprecision_score(y_test, y_pred), recall_score(y_test, y_pred), f1_score(y_test, y_pred)))\nprint(modelRF, 'confusion matrix: \\n', confusion_matrix(y_test, y_pred))","f5c00a40":"import seaborn as sns\nsns.heatmap(result, annot=True)\nplt.title('Winning Model - Random Forest Classifier')\nplt.xlabel('Prediction')\nplt.ylabel('Actual')","e768a64e":"#Writing the csv file\ndf.to_csv('C:\\\\Users\\\\riyar\\\\Desktop\\Clean_Diabetes.csv', index=False)","3a7e0077":"df.to_csv","9e1dbb92":"Precision Score","e6cb4862":"DATA SOURCE","1fb2287d":"The dataset consists of multiple (independent) variables of the medical predictor and one target\u00a0(dependent) variable, Outcome. The number of pregnancies the patient has had, their BMI, insulin level , age, and so on are some of the independent variables.\n\n","78964c34":"The dataset consists of a target variable 'Outcome' which consists of discrete values such as '0' and '1'.\nTherefore, the Machine Learning type is Supervised Learning and the Algorithm to be used is Classification algorithms.","5087fff7":"LOGISTIC REGRESSION","530200a0":"-> Decision Tree\n-> Random Forest\n-> Logistic Regression\n-> k-Nearest Neighbor\n-> Gaussian Naive Bayes","acf55a40":"0 = Non-diabetic,\n1 = Diabetic","e8a060e7":"OBJECTIVE","c993aafa":"The 8 medical predictor features are:\n\u00b7 Pregnancies: Number of times pregnant\n\u00b7 Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n\u00b7 BloodPressure: Diastolic blood pressure (mm Hg)\n\u00b7 SkinThickness: Triceps skin fold thickness (mm)\n\u00b7 Insulin: 2-Hour serum insulin (mu U\/ml)\n\u00b7 BMI: Body mass index (weight in kg\/(height in m)\u00b2)\n\u00b7 DiabetesPedigreeFunction: Diabetes pedigree function\n\u00b7 Age: Age (years)\n","65e88240":"TRAINING AND TESTING SPLIT","67fc1f97":"THANK YOU!","52feb3f6":"FEATURE IMPORTANCE USING RANDOM FOREST CLASSIFIER","750f007c":"This dataset is originally from the Diabetes and Digestive and Kidney Disorders National Institute. Based on certain diagnostic measurements used in the dataset, the purpose of the dataset is to diagnostically predict whether or not a patient has diabetes. Several limits have been imposed on the choosing of these instances from a broader database. In fact, all the patients here are women of Pima Indian descent who are at least 21 years old.\n    \n","e51957da":"STEP 5 - EVALUATING THE MODELS","3a34017b":"Recall Score","ca12ad85":"RANDOM FOREST CLASSIFIER","22f17521":"STEP 4 - TRAINING THE MODELS","5a472e74":"STEP 3 - CHOOSING MODEL","1090268e":"kNN - k-NEAREST NEIGHBOR","300fd248":"THE WINNING MODEL","f03c811d":"From the above graph, we can find that 'Glucose' and 'BMI' are the most important features for prediction.","89053adc":"Depicting the 'Outcome' variable in the form of a pairplot visualization.","64252107":"Since diabetes is quite prevalent among the people, make sure you have a balanced diet i.e. reduce the intake of sugar (Glucose levels) and maintain your body weight i.e. BMI (Body Mass Index) in order to live a healthy life :)","d9fa439d":"We can see that the Training accuracy scores and CV scores do not differ much, so therfore, we can conclude by saying that the model is neither Overfitting or Underfitting.","be8f56f0":"Since we have '0' values in our dataset we need to replace the '0' values. This is done by converting the 0 values to NAN and then converting the NAN to the Median values.","323e7d6c":"STEP 1 - DATA COLLECTION","9c5618dc":"Depicting all the features using a heatmap.","e5be4c63":"DATA CLEANING","a977fb3d":"Accuracy Score","b7cdee15":"DECISION TREE CLASSIFIER","5c9ed094":"METRICS USED","ccc902f7":"Cross Validation","7dc4e829":"The dataset has been downloaded from Kaggle.\nhttps:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database\nThere are 768 observations with 8 medical predictor features (input) and 1 target variable (output 0 for 'non-diabetic' and 1 for 'diabetic')\n","d654db32":"# DIABETES ONSET PREDICTION","b4923321":"MODEL PERFORMANCE","757fbb6a":"Reciever Operator Characteristics (ROC)","4e144755":"Step 7 - MODEL COMPARISON","f98dcac6":"Accuracy Scores of Training & Validation","8d42c1db":"Step 6 - VALIDATION OF MODELS","7a782eee":"PROBLEM IDENTIFICATION","f1bf2e94":"PURPOSE\n    ","37089867":"GAUSSIAN NAIVE BAYES","a077b878":"F1 Score","263d84dc":"EXPECTED OUTCOME\n","f806b2ea":"Outcome:","9edc96f3":"CONCLUSION","6ed95c11":"From the above graph, we can find that the area under the curve for Random Forest Classifier is large compared to the other models.","5c4c244e":"A metabolic disease that induces elevated blood sugar is diabetes mellitus, also known as diabetes. To be processed or used for nutrition, the hormone insulin transfers sugar from the blood into the cells. The metabolic characteristics of Pima Indians with type 2 diabetes are obesity, insulin resistance, insulin secretory deficiency and elevated levels of endogenous glucose production, which are the clinical features that characterise this condition in most populations.\n    \n","ec99c63e":"Random Forest Classifier is the winning model with ","971dc26e":"STEP 2 - DATA PREPARATION"}}