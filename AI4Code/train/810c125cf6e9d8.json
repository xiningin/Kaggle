{"cell_type":{"e469cec9":"code","9b15b155":"code","ee056ed8":"code","4469af33":"code","7603911f":"code","5fad85a9":"code","715edec7":"code","ae320250":"code","23408828":"code","aa9fb282":"code","e8b16112":"code","087332b1":"code","0825c8c1":"code","e5df0ab9":"code","10cc6fd1":"code","29d566ec":"code","9fe76783":"code","5da5ab33":"code","0afa7773":"code","e2c95f98":"code","da2db4da":"code","29edb1a0":"code","05c985f4":"code","13230f09":"code","00a28aaa":"code","cb1a6612":"code","148f8fd3":"code","a7b16688":"code","32880d7b":"code","df3cbea1":"code","da00f332":"code","5d21cad5":"code","be143774":"code","56421b29":"code","fca5d834":"code","f9c7bc00":"code","5a3c24b1":"code","2b802c85":"code","9960f534":"code","37ba7d00":"code","5cf471cf":"markdown","b0c11748":"markdown","1d4f1de0":"markdown","61a871a2":"markdown","cdcc51c5":"markdown","00aefa22":"markdown","0efc5cb9":"markdown","36edec93":"markdown","11632994":"markdown","f4666ed3":"markdown","0abc135c":"markdown","ee3115bb":"markdown","86f3de49":"markdown","3372b508":"markdown"},"source":{"e469cec9":"!ls ..\/input\/","9b15b155":"!conda install -y -c openbabel openbabel \nimport openbabel\n","ee056ed8":"#!rm -r \/opt\/conda\/lib\/python3.6\/site-packages\/lightgbm\n#!git clone --recursive https:\/\/github.com\/Microsoft\/LightGBM\n#!apt-get install -y -qq libboost-all-dev    ","4469af33":"#%%bash\n#cd LightGBM\n#rm -r build\n#mkdir build\n#cd build\n#cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=\/usr\/local\/cuda\/lib64\/libOpenCL.so -DOpenCL_INCLUDE_DIR=\/usr\/local\/cuda\/include\/ ..\n#make -j$(nproc)","7603911f":"#!cd LightGBM\/python-package\/;python3 setup.py install --precompile","5fad85a9":"#!mkdir -p \/etc\/OpenCL\/vendors && echo \"libnvidia-opencl.so.1\" > \/etc\/OpenCL\/vendors\/nvidia.icd\n#!rm -r LightGBM","715edec7":"import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\nfrom sklearn import metrics\nfrom sklearn import linear_model\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom IPython.display import HTML\nimport json\nimport altair as alt\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nalt.renderers.enable('notebook')\n\nos.environ[\"TZ\"]=\"Europe\/Paris\"\ntime.tzset()","ae320250":"# from https:\/\/www.kaggle.com\/artgor\/artgor-utils\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n    \n","23408828":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsub = pd.read_csv('..\/input\/sample_submission.csv')","aa9fb282":"print(f'There are {train.shape[0]} rows in train data.')\nprint(f'There are {test.shape[0]} rows in test data.')\n\nprint(f\"There are {train['molecule_name'].nunique()} distinct molecules in train data.\")\nprint(f\"There are {test['molecule_name'].nunique()} distinct molecules in test data.\")\nprint(f\"There are {train['atom_index_0'].nunique()} unique atoms.\")\nprint(f\"There are {train['type'].nunique()} unique types.\")","e8b16112":"train.head()","087332b1":"train.describe()","0825c8c1":"atom_count = train['atom_index_0'].value_counts().reset_index().rename(columns={'atom_index_0': 'count', 'index': 'atom_index_0'})\nchart1 = alt.Chart(atom_count).mark_bar().encode(\n    x=alt.X(\"atom_index_0:N\", axis=alt.Axis(title='atom_index_0')),\n    y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n    tooltip=['atom_index_0', 'count']\n).properties(title=\"Counts of atom_index_0\", width=350).interactive()\n\natom_count = train['atom_index_1'].value_counts().reset_index().rename(columns={'atom_index_1': 'count', 'index': 'atom_index_1'})\nchart2 = alt.Chart(atom_count).mark_bar().encode(\n    x=alt.X(\"atom_index_1:N\", axis=alt.Axis(title='atom_index_1')),\n    y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n    tooltip=['atom_index_1', 'count']\n).properties(title=\"Counts of atom_index_1\", width=350).interactive()\n\ntype_count = train['type'].value_counts().reset_index().rename(columns={'type': 'count', 'index': 'type'})\nchart3 = alt.Chart(type_count).mark_bar().encode(\n    x=alt.X(\"type:N\", axis=alt.Axis(title='type')),\n    y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n    tooltip=['type', 'count']\n).properties(title=\"Counts of type\", width=350).interactive()\n\nhist_df = pd.cut(train['scalar_coupling_constant'], 20).value_counts().sort_index().reset_index().rename(columns={'index': 'bins'})\nhist_df['bins'] = hist_df['bins'].astype(str)\nchart4 = alt.Chart(hist_df).mark_bar().encode(\n    x=alt.X(\"bins:O\", axis=alt.Axis(title='Target bins')),\n    y=alt.Y('scalar_coupling_constant:Q', axis=alt.Axis(title='Count')),\n    tooltip=['scalar_coupling_constant', 'bins']\n).properties(title=\"scalar_coupling_constant histogram\", width=400).interactive()\n\n\n(chart1 | chart2) & (chart3 | chart4)","e5df0ab9":"fig, ax = plt.subplots(figsize = (18, 8))\nplt.subplot(1, 2, 1);\nplt.hist(train['scalar_coupling_constant'], bins=20);\nplt.title('Basic scalar_coupling_constant histogram');\nplt.subplot(1, 2, 2);\nsns.violinplot(x='type', y='scalar_coupling_constant', data=train);\nplt.title('Violinplot of scalar_coupling_constant by type');","10cc6fd1":"fig, ax = plt.subplots(figsize = (20, 12))\nfor i, t in enumerate(train['type'].unique()):\n    train_type = train.loc[train['type'] == t]\n    bad_atoms_0 = list(train_type['atom_index_0'].value_counts(normalize=True)[train_type['atom_index_0'].value_counts(normalize=True) < 0.01].index)\n    bad_atoms_1 = list(train_type['atom_index_1'].value_counts(normalize=True)[train_type['atom_index_1'].value_counts(normalize=True) < 0.01].index)\n    bad_atoms = list(set(bad_atoms_0 + bad_atoms_1))\n    train_type = train_type.loc[(train_type['atom_index_0'].isin(bad_atoms_0) == False) & (train_type['atom_index_1'].isin(bad_atoms_0) == False)]\n    G = nx.from_pandas_edgelist(train_type, 'atom_index_0', 'atom_index_1', ['scalar_coupling_constant'])\n    plt.subplot(2, 4, i + 1);\n    nx.draw(G, with_labels=True);\n    plt.title(f'Graph for type {t}')","29d566ec":"structures = pd.read_csv('..\/input\/structures.csv')\n\ndef map_atom_info(df, atom_idx):\n    df = pd.merge(df, structures, how = 'left',\n                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n                  right_on = ['molecule_name',  'atom_index'])\n    \n    df = df.drop('atom_index', axis=1)\n    df = df.rename(columns={'atom': f'atom_{atom_idx}',\n                            'x': f'x_{atom_idx}',\n                            'y': f'y_{atom_idx}',\n                            'z': f'z_{atom_idx}'})\n    return df\n\ntrain = map_atom_info(train, 0)\ntrain = map_atom_info(train, 1)\n\ntest = map_atom_info(test, 0)\ntest = map_atom_info(test, 1)\n\natom_count=structures.groupby(['molecule_name','atom']).size().unstack(fill_value=0)\ntrain=pd.merge(train,atom_count, how = 'left', left_on  = 'molecule_name', right_on = 'molecule_name')\ntest=pd.merge(test,atom_count, how = 'left', left_on  = 'molecule_name', right_on = 'molecule_name')","9fe76783":"train.head()","5da5ab33":"%%time\nobConversion = openbabel.OBConversion()\nobConversion.SetInFormat(\"xyz\")\n\nstructdir='..\/input\/structures\/'\nmols=[]\nmols_files=os.listdir(structdir)\nmols_index=dict(map(reversed,enumerate(mols_files)))\nfor f in mols_index.keys():\n    mol = openbabel.OBMol()\n    obConversion.ReadFile(mol, structdir+f) \n    mols.append(mol)\n    ","0afa7773":"def Atoms(molname,AtomId1,AtomId2):\n    mol=mols[mols_index[molname+'.xyz']]\n    return mol, mol.GetAtomById(AtomId1), mol.GetAtomById(AtomId2)\n\ndef SecondAtom(bond,FirstAtom):\n    if FirstAtom.GetId()==bond.GetBeginAtom().GetId(): return bond.GetEndAtom()\n    else: return bond.GetBeginAtom()\n\ndef Angle2J(molname,AtomId1,AtomId2,debug=False):\n    mol,firstAtom,lastAtom=Atoms(molname,AtomId1,AtomId2)\n    if debug: print (mol.GetFormula())\n    if debug: print(firstAtom.GetType(),firstAtom.GetId(),':',lastAtom.GetType(),lastAtom.GetId())\n    for b in openbabel.OBAtomBondIter(firstAtom): # all bonds for first atom\n      secondAtom=SecondAtom(b,firstAtom)\n      lastBond=secondAtom.GetBond(lastAtom)\n      if lastBond: # found!\n        if debug: print('middle',secondAtom.GetId(),secondAtom.GetType())\n        return firstAtom.GetAngle(secondAtom,lastAtom)\n\nAngle2J('dsgdb9nsd_000003',1,2,debug=True) #water","e2c95f98":"def Torsion3J(molname,AtomId1,AtomId2,debug=False):\n    mol,firstAtom,lastAtom=Atoms(molname,AtomId1,AtomId2)\n    if debug: print (molname, mol.GetFormula())\n    if debug: print(firstAtom.GetType(),firstAtom.GetId(),':',lastAtom.GetType(),lastAtom.GetId())\n    for b in openbabel.OBAtomBondIter(firstAtom): # all bonds for first atom\n      secondAtom=SecondAtom(b,firstAtom)\n      for b2 in openbabel.OBAtomBondIter(secondAtom): # all bonds for second atom \n        thirdAtom=SecondAtom(b2,secondAtom)\n        lastBond=thirdAtom.GetBond(lastAtom)\n        if lastBond: # found!\n          if debug: print(secondAtom.GetType(),secondAtom.GetId(),'<->',thirdAtom.GetType(),thirdAtom.GetId())\n          return mol.GetTorsion(firstAtom,secondAtom,thirdAtom,lastAtom)\n          \nTorsion3J('dsgdb9nsd_000007',2,5,debug=True) #methanol","da2db4da":"train['bonds']=train['type'].str[0].astype(int)\ntest['bonds']=test['type'].str[0].astype(int)\n\ntrain_p_0 = train[['x_0', 'y_0', 'z_0']].values\ntrain_p_1 = train[['x_1', 'y_1', 'z_1']].values\ntest_p_0 = test[['x_0', 'y_0', 'z_0']].values\ntest_p_1 = test[['x_1', 'y_1', 'z_1']].values\n\ntrain['dist'] = np.linalg.norm(train_p_0 - train_p_1, axis=1)\ntest['dist'] = np.linalg.norm(test_p_0 - test_p_1, axis=1)\ntrain['abs_dist']=np.linalg.norm(train_p_0-train_p_1,axis=1,ord=1)\ntest['abs_dist']=np.linalg.norm(test_p_0-test_p_1,axis=1,ord=1)","29edb1a0":"def dist12(name='xy',a='x',b='y'):\n    train_p_0=train[[a+'_0',b+'_0']].values\n    train_p_1=train[[a+'_1',b+'_1']].values\n    test_p_0=test[[a+'_0',b+'_0']].values\n    test_p_1=test[[a+'_1',b+'_1']].values\n    \n    train[name] = np.linalg.norm(train_p_0 - train_p_1, axis=1)\n    test[name] = np.linalg.norm(test_p_0 - test_p_1, axis=1)\n    train['abs_'+name]= np.linalg.norm(train_p_0-train_p_1,axis=1,ord=1)\n    test['abs_'+name]= np.linalg.norm(test_p_0-test_p_1,axis=1,ord=1)","05c985f4":"dist12('dist_xy','x','y')\ndist12('dist_xz','x','z')\ndist12('dist_yz','y','z')","13230f09":"train.head(5)","00a28aaa":"fig, ax = plt.subplots(figsize = (18, 8))\nplt.subplot(1, 2, 1);\nplt.hist(train['dist_xz'], bins=20);\nplt.title('Basic dist histogram');\nplt.subplot(1, 2, 2);\nsns.violinplot(x='type', y='dist_xz', data=train);\nplt.title('Violinplot of dist_xz by type');","cb1a6612":"train['dist_to_type_mean'] = train['dist'] \/ train.groupby('type')['dist'].transform('mean')\ntest['dist_to_type_mean'] = test['dist'] \/ test.groupby('type')['dist'].transform('mean')\n\ntrain['dist_to_type_std'] = train['dist'] \/ train.groupby('type')['dist'].transform('std')\ntest['dist_to_type_std'] = test['dist'] \/ test.groupby('type')['dist'].transform('std')\n\ntrain['dist_to_type_mean_xy'] = train['dist_xy'] \/ train.groupby('type')['dist_xy'].transform('mean')\ntest['dist_to_type_mean_xy'] = test['dist_xy'] \/ test.groupby('type')['dist_xy'].transform('mean')\n\ntrain['dist_to_type_mean_xz'] = train['dist_xz'] \/ train.groupby('type')['dist_xz'].transform('mean')\ntest['dist_to_type_mean_xz'] = test['dist_xz'] \/ test.groupby('type')['dist_xz'].transform('mean')\n\ntrain['dist_to_type_mean_yz'] = train['dist_yz'] \/ train.groupby('type')['dist_yz'].transform('mean')\ntest['dist_to_type_mean_yz'] = test['dist_yz'] \/ test.groupby('type')['dist_yz'].transform('mean')","148f8fd3":"train.head()","a7b16688":"#%%time\n#### get atom or mol in train?\n######train['sp']=train.apply(lambda row: Atoms(row.molecule_name, row.atom_index_0, row.atom_index_1)[2],axis=1) # second atom is C or N for bond 1\n","32880d7b":"def create_features(df):\n    df['molecule_couples'] = df.groupby('molecule_name')['id'].transform('count')\n    df['molecule_dist_mean'] = df.groupby('molecule_name')['dist'].transform('mean')\n    df['molecule_dist_min'] = df.groupby('molecule_name')['dist'].transform('min')\n    df['molecule_dist_max'] = df.groupby('molecule_name')['dist'].transform('max')\n    df['atom_0_couples_count'] = df.groupby(['molecule_name', 'atom_index_0'])['id'].transform('count')\n    df['atom_1_couples_count'] = df.groupby(['molecule_name', 'atom_index_1'])['id'].transform('count')\n    \n    df[f'molecule_atom_index_0_x_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['x_1'].transform('std')\n    df[f'molecule_atom_index_0_y_1_mean'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('mean')\n    df[f'molecule_atom_index_0_y_1_mean_diff'] = df[f'molecule_atom_index_0_y_1_mean'] - df['y_1']\n    df[f'molecule_atom_index_0_y_1_mean_div'] = df[f'molecule_atom_index_0_y_1_mean'] \/ df['y_1']\n    df[f'molecule_atom_index_0_y_1_max'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('max')\n    df[f'molecule_atom_index_0_y_1_max_diff'] = df[f'molecule_atom_index_0_y_1_max'] - df['y_1']\n    df[f'molecule_atom_index_0_y_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('std')\n    df[f'molecule_atom_index_0_z_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['z_1'].transform('std')\n    df[f'molecule_atom_index_0_dist_mean'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('mean')\n    df[f'molecule_atom_index_0_dist_mean_diff'] = df[f'molecule_atom_index_0_dist_mean'] - df['dist']\n    df[f'molecule_atom_index_0_dist_mean_div'] = df[f'molecule_atom_index_0_dist_mean'] \/ df['dist']\n    df[f'molecule_atom_index_0_dist_max'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('max')\n    df[f'molecule_atom_index_0_dist_max_diff'] = df[f'molecule_atom_index_0_dist_max'] - df['dist']\n    df[f'molecule_atom_index_0_dist_max_div'] = df[f'molecule_atom_index_0_dist_max'] \/ df['dist']\n    df[f'molecule_atom_index_0_dist_min'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('min')\n    df[f'molecule_atom_index_0_dist_min_diff'] = df[f'molecule_atom_index_0_dist_min'] - df['dist']\n    df[f'molecule_atom_index_0_dist_min_div'] = df[f'molecule_atom_index_0_dist_min'] \/ df['dist']\n    df[f'molecule_atom_index_0_dist_std'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('std')\n    df[f'molecule_atom_index_0_dist_std_diff'] = df[f'molecule_atom_index_0_dist_std'] - df['dist']\n    df[f'molecule_atom_index_0_dist_std_div'] = df[f'molecule_atom_index_0_dist_std'] \/ df['dist']\n    df[f'molecule_atom_index_1_dist_mean'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('mean')\n    df[f'molecule_atom_index_1_dist_mean_diff'] = df[f'molecule_atom_index_1_dist_mean'] - df['dist']\n    df[f'molecule_atom_index_1_dist_mean_div'] = df[f'molecule_atom_index_1_dist_mean'] \/ df['dist']\n    df[f'molecule_atom_index_1_dist_max'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('max')\n    df[f'molecule_atom_index_1_dist_max_diff'] = df[f'molecule_atom_index_1_dist_max'] - df['dist']\n    df[f'molecule_atom_index_1_dist_max_div'] = df[f'molecule_atom_index_1_dist_max'] \/ df['dist']\n    df[f'molecule_atom_index_1_dist_min'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('min')\n    df[f'molecule_atom_index_1_dist_min_diff'] = df[f'molecule_atom_index_1_dist_min'] - df['dist']\n    df[f'molecule_atom_index_1_dist_min_div'] = df[f'molecule_atom_index_1_dist_min'] \/ df['dist']\n    df[f'molecule_atom_index_1_dist_std'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('std')\n    df[f'molecule_atom_index_1_dist_std_diff'] = df[f'molecule_atom_index_1_dist_std'] - df['dist']\n    df[f'molecule_atom_index_1_dist_std_div'] = df[f'molecule_atom_index_1_dist_std'] \/ df['dist']\n    df[f'molecule_atom_1_dist_mean'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('mean')\n    df[f'molecule_atom_1_dist_min'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('min')\n    df[f'molecule_atom_1_dist_min_diff'] = df[f'molecule_atom_1_dist_min'] - df['dist']\n    df[f'molecule_atom_1_dist_min_div'] = df[f'molecule_atom_1_dist_min'] \/ df['dist']\n    df[f'molecule_atom_1_dist_std'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('std')\n    df[f'molecule_atom_1_dist_std_diff'] = df[f'molecule_atom_1_dist_std'] - df['dist']\n    df[f'molecule_bonds_dist_std'] = df.groupby(['molecule_name', 'bonds'])['dist'].transform('std')\n    df[f'molecule_bonds_dist_std_diff'] = df[f'molecule_bonds_dist_std'] - df['dist']\n    df[f'molecule_type_dist_mean'] = df.groupby(['molecule_name', 'type'])['dist'].transform('mean')\n    df[f'molecule_type_dist_mean_diff'] = df[f'molecule_type_dist_mean'] - df['dist']\n    df[f'molecule_type_dist_mean_div'] = df[f'molecule_type_dist_mean'] \/ df['dist']\n    df[f'molecule_type_dist_max'] = df.groupby(['molecule_name', 'type'])['dist'].transform('max')\n    df[f'molecule_type_dist_min'] = df.groupby(['molecule_name', 'type'])['dist'].transform('min')\n    df[f'molecule_type_dist_std'] = df.groupby(['molecule_name', 'type'])['dist'].transform('std')\n    df[f'molecule_type_dist_std_diff'] = df[f'molecule_type_dist_std'] - df['dist']\n\n    return df\n","df3cbea1":"%%time\ntrain = create_features(train)\ntest = create_features(test)\n\ntrain=reduce_mem_usage(train)\ntest=reduce_mem_usage(test)","da00f332":"# memory usage\nimport sys\ndef sizeof_fmt(num, suffix='B'):\n    ''' By Fred Cirera, after https:\/\/stackoverflow.com\/a\/1094933\/1870254'''\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f%s%s\" % (num, unit, suffix)\n        num \/= 1024.0\n    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n\nfor name, size in sorted(((name, sys.getsizeof(value)) for name,value in locals().items()),\n                         key= lambda x: -x[1])[:10]:\n    print(\"{:>30}: {:>8}\".format(name,sizeof_fmt(size)))","5d21cad5":"def metric(df, preds):\n    df[\"prediction\"] = preds\n    maes = []\n    for t in df.type.unique():\n        y_true = df[df.type==t].scalar_coupling_constant.values\n        y_pred = df[df.type==t].prediction.values\n        mae = np.log(metrics.mean_absolute_error(y_true, y_pred))\n        maes.append(mae)\n    return np.mean(maes)","be143774":"def group_mean_log_mae(y_true, y_pred, groups, floor=1e-9):\n    maes = (y_true-y_pred).abs().groupby(groups).mean()\n    return np.log(maes.map(lambda x: max(x, floor))).mean()","56421b29":"for f in [ 'atom_0', 'atom_1']:\n    lbl = LabelEncoder()\n    lbl.fit(list(train[f].values) )\n    train[f] = lbl.transform(list(train[f].values))\n    test[f] = lbl.transform(list(test[f].values))","fca5d834":"def train_model(X, X_test, y, params, folds, model_type='lgb', plot_feature_importance=False, model=None):\n\n    oof = np.zeros(len(X))\n    print(X.shape,X_test.shape)\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print('Fold', fold_n, 'started at', time.ctime())\n      \n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = 2000, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',\n                    verbose=500, early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)\n            \n        if model_type == 'rcv':\n            model = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0), scoring='neg_mean_absolute_error', cv=3)\n            model.fit(X_train, y_train)\n            print(model.alpha_)\n\n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = mean_absolute_error(y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict(X_test).reshape(-1,)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = mean_absolute_error(y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict(X_test).reshape(-1,)\n        \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] \/= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction\n    \n    else:\n        return oof, prediction, model","f9c7bc00":"params = {\n          'num_leaves': 128,\n          'min_child_samples': 50,\n          'objective': 'regression',\n          'max_depth': 9,\n          'learning_rate': 0.2,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 1,\n          \"subsample\": 0.9,\n          \"bagging_seed\": 11,\n          \"metric\": 'mae',\n          \"verbosity\": -1,\n          'reg_alpha': 0.1302650970728192,\n          'reg_lambda': 0.3603427518866501,\n          'colsample_bytree': 1.0,\n          'device' : 'cpu',\n          'gpu_platform_id' : 0,\n          'gpu_device_id' : 0,\n          'gpu_use_dp': False\n         }","5a3c24b1":"prediction_lgb=pd.DataFrame(test.shape[0],index=test['id'],columns=['scalar_coupling_constant'],dtype=float)\noof_lgb=pd.DataFrame(train.shape[0],index=train['id'],columns=['oof'],dtype=float)\n\nfor t in train['type'].unique():\n    print(f'Training of type {t}')\n    b=int(t[0]) # current bond for this type\n    print('Predicting J=',b)\n    X=train[train.type==t]\n    if (b==1):\n        X['sp']=X.apply(lambda row: Atoms(row.molecule_name, row.atom_index_0, row.atom_index_1)[2].GetHyb(),axis=1) # second atom is C or N for bond 1\n    if (b==2):\n        X['Angle']=X.apply(lambda row: Angle2J(row.molecule_name , row.atom_index_0, row.atom_index_1),axis=1) \n    if (b==3):\n        X['Torsion']=X.apply(lambda row: Torsion3J(row.molecule_name , row.atom_index_0, row.atom_index_1),axis=1) \n        X['cosT']=np.cos(np.deg2rad(X['Torsion']))\n        X['cos2T']=np.cos(2*np.deg2rad(X['Torsion']))\n    y = X['scalar_coupling_constant']\n    ids_train=X['id']\n    X = X.drop(['id','type', 'molecule_name', 'scalar_coupling_constant','bonds'], axis=1)\n    \n    X_test = test[test.type==t]\n    if (b==1): \n        X_test['sp']=X_test.apply(lambda row: Atoms(row.molecule_name, row.atom_index_0, row.atom_index_1)[2].GetHyb(),axis=1) # second atom is C or N for bond 1\n    if (b==2):\n        X_test['Angle']=X_test.apply(lambda row: Angle2J(row.molecule_name , row.atom_index_0, row.atom_index_1),axis=1) \n    if (b==3):\n        X_test['Torsion']=X_test.apply(lambda row: Torsion3J(row.molecule_name , row.atom_index_0, row.atom_index_1),axis=1)  \n        X_test['cosT']=np.cos(np.deg2rad(X_test['Torsion']))\n        X_test['cos2T']=np.cos(2*np.deg2rad(X_test['Torsion']))\n    ids_test=X_test['id']    \n    X_test=X_test.drop(['id', 'type', 'molecule_name','bonds'], axis=1)\n    \n    n_fold = 5\n    folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)\n    \n    fold_oof_lgb, fold_prediction_lgb, feature_importance = train_model(X=X,X_test=X_test,y=y,params=params,folds=folds, model_type='lgb', plot_feature_importance=True)\n    \n    prediction_lgb.loc[ids_test,'scalar_coupling_constant']=fold_prediction_lgb\n    oof_lgb.loc[ids_train,'oof']=fold_oof_lgb\n      ","2b802c85":"#'categorical_feature' : \"name:type, atom_0, atom_1\",","9960f534":"sub=sub.drop(['scalar_coupling_constant'],axis=1).merge(prediction_lgb,on='id')\nsub.to_csv('submission.csv', index=False)\nsub.head()","37ba7d00":"plot_data = pd.DataFrame(train['scalar_coupling_constant'])\nplot_data.index.name = 'id'\nplot_data['yhat'] = oof_lgb['oof']\nplot_data['type'] = train['type']\n\ndef plot_oof_preds(ctype, llim, ulim):\n        plt.figure(figsize=(6,6))\n        sns.scatterplot(x='scalar_coupling_constant',y='yhat',\n                        data=plot_data.loc[plot_data['type']==ctype,\n                        ['scalar_coupling_constant', 'yhat']]);\n        plt.xlim((llim, ulim))\n        plt.ylim((llim, ulim))\n        plt.plot([llim, ulim], [llim, ulim])\n        plt.xlabel('scalar_coupling_constant')\n        plt.ylabel('predicted')\n        plt.title(f'{ctype}', fontsize=18)\n        plt.show()\n\nplot_oof_preds('1JHC', 0, 250)\nplot_oof_preds('1JHN', 0, 100)\nplot_oof_preds('2JHC', -50, 50)\nplot_oof_preds('2JHH', -50, 50)\nplot_oof_preds('2JHN', -25, 25)\nplot_oof_preds('3JHC', -25, 100)\nplot_oof_preds('3JHH', -20, 20)\nplot_oof_preds('3JHN', -15, 15)","5cf471cf":"## Data loading and overview","b0c11748":"importing libraries","1d4f1de0":"## Basic model\n\nI'll use the function for metric calculation from this kernel: https:\/\/www.kaggle.com\/abhishek\/competition-metric","61a871a2":"Now the graphs are much more clear!","cdcc51c5":"## General information\n\nThis kernel is created using data of `Predicting Molecular Properties` competition.\nWe have information about atom couples in molecules and need to predict `scalar_coupling_constant` between these atoms.\n\n![](http:\/\/www.et.byu.edu\/~rowley\/VLEfinal\/methane_dimer.gif)\n\nIn this kernel I'll do EDA and will try some approaches to modelling.\n\n*Work still in progress*\n\n\n","00aefa22":"## Better network graphs\nBut there is a little problem: as we saw earlier, there are atoms which are very rare, as a result graphs will be skewed due to them. Now I'll drop atoms for each type which are present in less then 1% of connections","0efc5cb9":"Fork from **Andrew Lukyanenko**\n\nThis kernel is still a draft.\n\nUses OpenBabel API. \nIdea from https:\/\/www.kaggle.com\/c\/champs-scalar-coupling\/discussion\/93928#latest-546589\n\nWith Openbabel it's easy to navigate through bonds, so I added sp, 2J angle and 3J dihedral angle.\n\nSome basic info here https:\/\/yvesrubin.files.wordpress.com\/2011\/03\/coupling-constants-for-1h-and-13c-nmr.pdf\n\nWhat to do next?\nIf you are a python fan, you certainly can improve my code!\n\nIf you are a chemist, you may extract more useful info from openbabel.\n\nif you aren in ML, extracting *everything* from openbabel and create a data file might work. I also did a learning for each bond, but not sure it's good.","36edec93":"It seems that the values are quite different for different types!","11632994":"**Creating mol reference with OpenBabel**","f4666ed3":"## Feature engineering\n\nFor now I'll use a basic approach to feature engineering.\n\nI'll use this useful kernel:\nhttps:\/\/www.kaggle.com\/seriousran\/just-speed-up-calculate-distance-from-benchmark","0abc135c":"So in out main data files we have information about moleculas and pairs of atoms\n- test set in ~2 times smaller that train set;\n- we have 29 unique atoms types and 8 coupling types;","ee3115bb":"## Plotting network graphs by type\n\nWe have molecules, atom pairs, so this means data, which is interconnected. Network graphs should be useful to visualize such data!","86f3de49":"So, everyone uses this distance feature, let's have a look at it!","3372b508":"There are many interesting things here:\n- among first atoms there is a little number of atoms with index lower than 7 or higher than 24;\n- among second atoms there is a little number of atoms with index higher than 24. Also index with atom with index 9 in quite rare;\n- coupling types are unevenly distributed. There are 3 very popular, 3 quite rare and 2 with medium frequency;\n- target variable has a bimodal distribution;\n- different coupling types have really different values of target variable. Maybe it would make sense to build separate models for each of them;"}}