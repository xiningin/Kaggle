{"cell_type":{"c6cea60c":"code","008cc414":"code","3a4123ac":"code","b3829e62":"code","2e6ce3a6":"code","ea7af176":"code","c436e485":"code","5f997327":"code","e591d104":"code","b48cc25d":"code","9b3a06e1":"code","e38e1740":"code","fc4c067b":"code","270dab12":"code","9a42c856":"code","b995c951":"code","ffb48c79":"code","179b21c9":"code","8617c0ab":"code","8f68c1de":"code","0d5bdfe7":"code","a65bfad8":"code","ae83b661":"code","095ea047":"code","4fd68ad0":"code","9343400f":"code","1329d9e5":"code","c7699d49":"code","93825d73":"code","a8cf889a":"code","7439c25d":"code","3321e7a2":"code","bb4dba15":"code","de91b3b3":"code","e9786d65":"code","9c421666":"code","0ab347c0":"code","f884b907":"code","0d63745e":"code","60e6b318":"code","69d22f1c":"code","5bc529cb":"code","410f9dd4":"code","d2fc09d3":"code","e3541963":"markdown","0fa9aa28":"markdown","77ae2b76":"markdown","bec4b350":"markdown","d651816c":"markdown","0cf6c5e8":"markdown","70497659":"markdown","e516cb87":"markdown","7e1ae0ce":"markdown","463d4546":"markdown","750c2c9f":"markdown","63cd0dbe":"markdown","cc71afd2":"markdown","df0f0a20":"markdown","67046042":"markdown","11ecf6c8":"markdown","d41854b6":"markdown","ba033d71":"markdown"},"source":{"c6cea60c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns # statistical data visualization\n\n#-- plotly\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n#--\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","008cc414":"data = pd.read_csv('..\/input\/jm1.csv')","3a4123ac":"data.info() #informs about the data (memory usage, data types etc.)","b3829e62":"data.head() #shows first 5 rows","2e6ce3a6":"data.tail() #shows last 5 rows","ea7af176":"data.sample(10) #shows random rows (sample(number_of_rows))","c436e485":"data.shape #shows the number of rows and columns","5f997327":"data.describe() #shows simple statistics (min, max, mean, etc.)","e591d104":"defects_true_false = data.groupby('defects')['b'].apply(lambda x: x.count()) #defect rates (true\/false)\nprint('False : ' , defects_true_false[0])\nprint('True : ' , defects_true_false[1])","b48cc25d":"trace = go.Histogram(\n    x = data.defects,\n    opacity = 0.75,\n    name = \"Defects\",\n    marker = dict(color = 'green'))\n\nhist_data = [trace]\nhist_layout = go.Layout(barmode='overlay',\n                   title = 'Defects',\n                   xaxis = dict(title = 'True - False'),\n                   yaxis = dict(title = 'Frequency'),\n)\nfig = go.Figure(data = hist_data, layout = hist_layout)\niplot(fig)\n","9b3a06e1":"data.corr() #shows coveriance matrix","e38e1740":"f,ax = plt.subplots(figsize = (15, 15))\nsns.heatmap(data.corr(), annot = True, linewidths = .5, fmt = '.2f')\nplt.show()","fc4c067b":"trace = go.Scatter(\n    x = data.v,\n    y = data.b,\n    mode = \"markers\",\n    name = \"Volume - Bug\",\n    marker = dict(color = 'darkblue'),\n    text = \"Bug (b)\")\n\nscatter_data = [trace]\nscatter_layout = dict(title = 'Volume - Bug',\n              xaxis = dict(title = 'Volume', ticklen = 5),\n              yaxis = dict(title = 'Bug' , ticklen = 5),\n             )\nfig = dict(data = scatter_data, layout = scatter_layout)\niplot(fig)\n\n#two attributes with high correlation v-b > just about 1","270dab12":"data.isnull().sum() #shows how many of the null","9a42c856":"trace1 = go.Box(\n    x = data.uniq_Op,\n    name = 'Unique Operators',\n    marker = dict(color = 'blue')\n    )\nbox_data = [trace1]\niplot(box_data)","b995c951":"def evaluation_control(data):    \n    evaluation = (data.n < 300) & (data.v < 1000 ) & (data.d < 50) & (data.e < 500000) & (data.t < 5000)\n    data['complexityEvaluation'] = pd.DataFrame(evaluation)\n    data['complexityEvaluation'] = ['Succesful' if evaluation == True else 'Redesign' for evaluation in data.complexityEvaluation]","ffb48c79":"evaluation_control(data)\ndata","179b21c9":"data.info()","8617c0ab":"data.groupby(\"complexityEvaluation\").size() #complexityEvalution rates (Succesfull\/redisgn)","8f68c1de":"# Histogram\ntrace = go.Histogram(\n    x = data.complexityEvaluation,\n    opacity = 0.75,\n    name = 'Complexity Evaluation',\n    marker = dict(color = 'darkorange')\n)\nhist_data = [trace]\nhist_layout = go.Layout(barmode='overlay',\n                   title = 'Complexity Evaluation',\n                   xaxis = dict(title = 'Succesful - Redesign'),\n                   yaxis = dict(title = 'Frequency')\n)\nfig = go.Figure(data = hist_data, layout = hist_layout)\niplot(fig)","0d5bdfe7":"from sklearn import preprocessing\n\nscale_v = data[['v']]\nscale_b = data[['b']]\n\nminmax_scaler = preprocessing.MinMaxScaler()\n\nv_scaled = minmax_scaler.fit_transform(scale_v)\nb_scaled = minmax_scaler.fit_transform(scale_b)\n\ndata['v_ScaledUp'] = pd.DataFrame(v_scaled)\ndata['b_ScaledUp'] = pd.DataFrame(b_scaled)\n\ndata","a65bfad8":"scaled_data = pd.concat([data.v , data.b , data.v_ScaledUp , data.b_ScaledUp], axis=1)\nscaled_data","ae83b661":"data.info()","095ea047":"from sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn import model_selection\n\nX = data.iloc[:, :-10].values  #Select related attribute values for selection\nY = data.complexityEvaluation.values   #Select classification attribute values","4fd68ad0":"Y","9343400f":"#Parsing selection and verification datasets\nvalidation_size = 0.20\nseed = 7\nX_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size = validation_size, random_state = seed)","1329d9e5":"#Creation of Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()","c7699d49":"#Calculation of ACC value by K-fold cross validation of NB model\nscoring = 'accuracy'\nkfold = model_selection.KFold(n_splits = 10, random_state = seed)\ncv_results = model_selection.cross_val_score(model, X_train, Y_train, cv = kfold, scoring = scoring)","93825d73":"cv_results","a8cf889a":"msg = \"Mean : %f - Std : (%f)\" % (cv_results.mean(), cv_results.std())\nmsg","7439c25d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\n#Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n#Accuracy score\nfrom sklearn.metrics import accuracy_score\nprint(\"ACC: \",accuracy_score(y_pred,y_test))","3321e7a2":"    sel_loc = data['loc']\n    sel_b = data['b']\n    selected_data = pd.concat([sel_loc, sel_b], axis=1)\n    selected_data\n    #data selected for selection","bb4dba15":"selected_data.describe() #shows simple statistics (min, max, mean, etc.)","de91b3b3":"selected_data.corr() #shows coveriance matrix","e9786d65":"#Scatter Plot\ntrace = go.Scatter(\n    x = data['loc'],\n    y = data.b,\n    mode = \"markers\",\n    name = \"Line of Code - Bug\",\n    marker = dict(color = 'darkmagenta'),\n    text = \"Bug (b)\")\n\nscatter_data = [trace]\nscatter_layout = dict(title = 'Line of Code - Bug',\n              xaxis = dict(title = 'Line of Code', ticklen = 5),\n              yaxis = dict(title = 'Bug' , ticklen = 5),\n             )\nfig = dict(data = scatter_data, layout = scatter_layout)\niplot(fig)","9c421666":"Y = selected_data['b'].values  \nX = selected_data['loc'].values  \nX = X.reshape(-1,1)\n#Select the X and Y values for selection\nY","0ab347c0":"#Parsing selection and verification datasets\nfrom sklearn.model_selection import train_test_split  \nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0) ","f884b907":"#Creation of Linear Regression model\nfrom sklearn.linear_model import LinearRegression  \nmodel = LinearRegression()  \nmodel.fit(X_train, y_train)  ","0d63745e":"# Intercept & Coef\nprint(\"Intercept :\", model.intercept_)  \nprint(\"Coef :\", model.coef_)","60e6b318":"X_test","69d22f1c":"y_pred = model.predict(X_test) ","5bc529cb":"# New data (real , estimated)\nnew_data = pd.DataFrame({'real': y_test, 'estimated': y_pred})  \nnew_data","410f9dd4":"#The nearest line of all values in the model\nplt.scatter(X_train, y_train, color = 'red')\nmodelin_tahmin_ettigi_y = model.predict(X_train)\nplt.plot(X_train, modelin_tahmin_ettigi_y, color = 'black')\nplt.title('Line of Code - Bug', size = 15)  \nplt.xlabel('Line of Code')  \nplt.ylabel('Bug')  \nplt.show() ","d2fc09d3":"#The results of the model. (This uses the Least squares method and the Root mean square error methods)\n#In general, as these values are calculated as the mean value and the difference difference, it is considered that the model has better estimation ability as it approaches 0.\nfrom sklearn import metrics   \nprint('Mean Squared Error (MSE):', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))  ","e3541963":">*  **Feature Extraction**","0fa9aa28":">* **Naive Bayes**","77ae2b76":"> * **Scatter Plot **","bec4b350":"> **Data Discovery & Visualization**","d651816c":">*When we look at the values, the fact that the values are close to zero shows us that the model has good predictive ability.*","0cf6c5e8":"> * **Data Normalization  (Min-Max Normalization)**","70497659":"> *No missing value. *\n\n> *No data cleaning needed because the data is all important.*","e516cb87":"*Showing all information when clicking on plot (min, max, q1, q2, etc.).*","7e1ae0ce":"> * **Heatmap**","463d4546":">**Model Selection**","750c2c9f":"> * **Covariance**\n\nCovariance is a measure of the directional relationship between the returns on two risky assets. A positive covariance means that asset returns move together while a negative covariance means returns move inversely.","63cd0dbe":"*- THE END -*","cc71afd2":"> * **Histogram**","df0f0a20":"> *The light color in the heat map indicates that the covariance is high. (Ex. \"v-b\" , \"v-n\", etc.)*\n\n> *The dark color in the heat map indicates that the covariance is low. (Ex. \"loc-l\" , \"l-d\", etc.)*","67046042":"> * **Linear Regression**","11ecf6c8":"> **About this Software Defect Prediction Dataset**\n \nThis is a Promise data set made publicly available in order to encourage repeatable, verifiable, refutable, and\/or improvable predictive models of software engineering.\n\n>***Attribute Information:***\n 1. loc                                    : numeric % McCabe's line count of code\n 2. v(g)                                  : numeric % McCabe \"cyclomatic complexity\"\n 3. ev(g)                                : numeric % McCabe \"essential complexity\"\n 4. iv(g)                                 : numeric % McCabe \"design complexity\"\n 5. n                                      : numeric % Halstead total operators + operands\n 6. v                                       : numeric % Halstead \"volume\"\n 7. l                                        : numeric % Halstead \"program length\"\n 8. d                                      : numeric % Halstead \"difficulty\"\n 9. i                                        : numeric % Halstead \"intelligence\"\n 10. e                                     : numeric % Halstead \"effort\"\n 11. b                                      : numeric % Halstead \n 12. t                                      : numeric % Halstead's time estimator\n 13. lOCode                          : numeric % Halstead's line count\n 14. lOComment                  : numeric % Halstead's count of lines of comments\n 15. lOBlank                          : numeric % Halstead's count of blank lines\n 16. lOCodeAndComment  : numeric\n 17. uniq_Op                          : numeric % unique operators\n 18. uniq_Opnd                     : numeric % unique operands\n 19. total_Op                         : numeric % total operators\n 20. total_Opnd                    : numeric % total operands\n 21. branchCount                 : numeric % of the flow graph\n 22. defects                          : {false,true} % module has\/has not one or more reported defects","d41854b6":"> **Data Preprocessing**","ba033d71":">*  **Outlier Detection (Box Plot)**"}}