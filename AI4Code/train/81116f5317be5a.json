{"cell_type":{"2d617a1e":"code","742b3550":"code","83260905":"code","44f0e5cf":"code","21ae9dd6":"code","3115414f":"code","544c2a2e":"code","130c5622":"code","6eab457e":"code","c14e90bd":"code","a26c2256":"code","224b7a6e":"code","e04b91a4":"code","68067c37":"code","cd10cdb1":"code","309a474e":"code","365069a3":"code","ae74d4e1":"code","4737efe9":"code","8a8518df":"code","ca12fc06":"code","0ca9c669":"code","09e8bb6b":"code","893459f3":"code","0949fc4c":"code","61ec036f":"markdown","f1c141a7":"markdown","e1391daf":"markdown","b95e71d8":"markdown","817c3505":"markdown","5c4d73f1":"markdown","d22e48fd":"markdown","43ca8a6e":"markdown"},"source":{"2d617a1e":"import numpy as np \nimport pandas as pd \nimport os\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.decomposition import PCA\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport time","742b3550":"import sys\nsys.path.append('..\/input\/stratified')\nfrom ml_stratifiers import MultilabelStratifiedKFold","83260905":"test_df = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ntrain_df = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_target_df = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\nsub = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\n\ntarget_cols = train_target_df.columns[1:]\nN_TARGETS = len(target_cols)\nprint(train_df.shape)","44f0e5cf":"GENES = [col for col in train_df.columns if col.startswith('g-')]\nCELLS = [col for col in train_df.columns if col.startswith('c-')]","21ae9dd6":"# GENES\nn_comp = 50\n\ndata = pd.concat([pd.DataFrame(train_df[GENES]), pd.DataFrame(test_df[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_df.shape[0]]; test2 = data2[-test_df.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_df = pd.concat((train_df, train2), axis=1)\ntest_df = pd.concat((test_df, test2), axis=1)","3115414f":"train_df.shape","544c2a2e":"#CELLS\nn_comp = 15\n\ndata = pd.concat([pd.DataFrame(train_df[CELLS]), pd.DataFrame(test_df[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_df.shape[0]]; test2 = data2[-test_df.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_df = pd.concat((train_df, train2), axis=1)\ntest_df = pd.concat((test_df, test2), axis=1)","130c5622":"train_df.shape","6eab457e":"from sklearn.feature_selection import VarianceThreshold\n\ntrain_copy = train_df\nvar_thresh = VarianceThreshold(0.8)  #<-- Update\ndata = train_df.append(test_df)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n#array\u3067\u51fa\u529b\u3055\u308c\u308b\n#\u3069\u3046\u3044\u3046\u7279\u5fb4\u91cf\u304c\u9078\u3070\u308c\u305f\u304b\u308f\u304b\u3089\u306a\u3044\n\ntrain_df_transformed = data_transformed[ : train_df.shape[0]]\ntest_df_transformed = data_transformed[-test_df.shape[0] : ]\n\n\ntrain_df = pd.DataFrame(train_df[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_df = pd.concat([train_df, pd.DataFrame(train_df_transformed)], axis=1)\n\n\ntest_df = pd.DataFrame(test_df[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_df = pd.concat([test_df, pd.DataFrame(test_df_transformed)], axis=1)\n\ntrain_df.shape","c14e90bd":"#num_dict = {}\n#for i in np.arange(0,868):\n#    num_dict[i] = f'{i}'\n#train_df = train_df.rename(columns=num_dict)\n#test_df = test_df.rename(columns=num_dict)","a26c2256":"search_row = dict(train_copy.iloc[0,4:])\ncol_rela = {}\nfor i in np.arange(0,868):\n    for k, v in search_row.items():\n        if train_df[i][0] == v:\n            col_rela[i] = k\ntrain_df = train_df.rename(columns=col_rela)\ntest_df = test_df.rename(columns=col_rela)","224b7a6e":"train_df","e04b91a4":"SEED = 1234\nEPOCHS = 28\nBATCH_SIZE = 128\nFOLDS = 5\nREPEATS = 5\nLR = 0.0005\nN_TARGETS = len(target_cols)","68067c37":"def seed_everything(seed):\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)","cd10cdb1":"def multi_log_loss(y_true, y_pred):\n    losses = []\n    for col in y_true.columns:\n        losses.append(log_loss(y_true.loc[:, col], y_pred.loc[:, col]))\n    return np.mean(losses)","309a474e":"def preprocess_df(data):\n    data['cp_type'] = (data['cp_type'] == 'trt_cp').astype(int)\n    data['cp_dose'] = (data['cp_dose'] == 'D2').astype(int)\n    return data","365069a3":"x_train = preprocess_df(train_df.drop(columns=\"sig_id\"))\nx_test =preprocess_df(test_df.drop(columns=\"sig_id\"))\ny_train = train_target_df.drop(columns=\"sig_id\")\nN_FEATURES = x_train.shape[1]","ae74d4e1":"#x_train = np.asarray(x_train)\n#x_test = np.asarray(x_test)\n#y_train = np.asarray(y_train)","4737efe9":"#VarianceThershold\u306e\u6642\u306f\u5fc5\u8981\nx_train = x_train.astype({'cp_time':int})\nx_test = x_test.astype({'cp_time':int})","8a8518df":"def create_model():\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(N_FEATURES),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    #tf.keras.layers.Dropout(0.4),\n    #tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"relu\")),  \n    #tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(N_TARGETS, activation=\"sigmoid\"))\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = LR), loss='binary_crossentropy', metrics=[\"accuracy\"])\n    return model","ca12fc06":"def build_train(resume_models = None, repeat_number = 0, folds = 5, skip_folds = 0):\n    \n    models = []\n    oof_preds = y_train.copy()\n    \n\n    kfold = KFold(folds, shuffle = True)\n    #kfold = MultilabelStratifiedKFold(n_splits=folds)\n    # stratified\u306e\u6642\u306fX=x_train,y=y_train\n    for fold, (train_ind, val_ind) in enumerate(kfold.split(x_train)):\n        print('\\n')\n        print('-'*50)\n        print(f'Training fold {fold + 1}')\n        \n        cb_lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'binary_crossentropy', factor = 0.4, patience = 2, verbose = 1, min_delta = 0.0001, mode = 'auto')\n        checkpoint_path = f'repeat:{repeat_number}_Fold:{fold}.hdf5'\n        cb_checkpt = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True, save_weights_only = True, mode = 'min')\n\n        model = create_model()\n        model.fit(x_train.values[train_ind],\n              y_train.values[train_ind],\n              validation_data=(x_train.values[val_ind], y_train.values[val_ind]),\n              callbacks = [cb_lr_schedule, cb_checkpt],\n              epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2\n             )\n        model.load_weights(checkpoint_path)\n        oof_preds.loc[val_ind, :] = model.predict(x_train.values[val_ind])\n        models.append(model)\n        print('train:')\n        print(list(zip(model.metrics_names, model.evaluate(x_train.values[train_ind], y_train.values[train_ind], verbose=0, batch_size=32))))\n        print('val:')\n        print(list(zip(model.metrics_names, model.evaluate(x_train.values[val_ind], y_train.values[val_ind], verbose=0, batch_size=32))))\n\n    return models, oof_preds","0ca9c669":"model = create_model()\nmodel.summary()","09e8bb6b":"start = time.time()\nmodels = []\noof_preds = []\n# seed everything\nseed_everything(SEED)\nfor i in range(REPEATS):\n    m, oof = build_train(repeat_number = i, folds=FOLDS)\n    models = models + m\n    oof_preds.append(oof)\n#\u4e00\u56de\u5c02\u7528\n#m, oof = build_train(repeat_number = i, folds=FOLDS)\n#models = models + m\n#oof_preds.append(oof)\n\nfinish = time.time()-start\nprint(finish)","893459f3":"models[1].predict(x_test)","0949fc4c":"test_preds = sub.copy()\ntest_preds[target_cols] = 0\nfor model in models:\n    test_preds.loc[:,target_cols] += model.predict(x_test)\ntest_preds.loc[:,target_cols] \/= len(models)\ntest_preds.loc[x_test['cp_type'] == 0, target_cols] = 0\ntest_preds.to_csv('submission.csv', index=False)","61ec036f":"create new features of 'c-' with PCA","f1c141a7":"create new features of 'g-' with PCA","e1391daf":"### \u30ab\u30e9\u30e0\u5217\u540d\u3092\u5165\u308c\u308b","b95e71d8":"Basic setup","817c3505":"Refference:\n[Drug MoA: TF Keras Starter](https:\/\/www.kaggle.com\/ravy101\/drug-moa-tf-keras-starter)  \n[Kernel Logistic Regression](https:\/\/www.kaggle.com\/gogo827jz\/kernel-logistic-regression-one-for-206-targets)  \n","5c4d73f1":"Encode Categoricals to binary","d22e48fd":"g-\u5217\u3068c-\u5217\u3092\u53d6\u308a\u51fa\u3059","43ca8a6e":"Define Model Architecture"}}