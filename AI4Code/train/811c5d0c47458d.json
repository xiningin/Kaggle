{"cell_type":{"eb6f4b3b":"code","4268a6c7":"code","dc58244d":"code","605c2e45":"code","10c1ae94":"code","b7a0d671":"code","7e1c0073":"code","beed6c2a":"code","3583da08":"code","abc84893":"code","68c46891":"code","851f68e0":"code","f869381d":"code","56cfad61":"code","bcc76bf3":"code","b52636d9":"code","4b56c633":"code","160d8536":"code","b93bc33a":"code","19c5335b":"code","6af1d5a6":"code","14ca7fe3":"code","75237354":"markdown","be4db426":"markdown","f52d501e":"markdown","7fe0c8ae":"markdown","c5d0a44b":"markdown","cbf9d031":"markdown","dbbb1447":"markdown","b4923210":"markdown"},"source":{"eb6f4b3b":"import numpy as np \nfrom tqdm import tqdm\nimport cv2\nimport os\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport itertools\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras import layers\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import EarlyStopping\nfrom keras.preprocessing.image import ImageDataGenerator\ninit_notebook_mode(connected=True)\nRANDOM_SEED = 123","4268a6c7":"TRAIN_DIR = ('..\/input\/facial-recognition-dataset\/Training\/Training\/')\nTEST_DIR = ('..\/input\/facial-recognition-dataset\/Testing\/Testing\/')","dc58244d":"def load_data(dir_path, IMG_SIZE):\n   \n    X = []\n    y = []\n    i = 0\n    labels = dict()\n    for path in tqdm(sorted(os.listdir(dir_path))):\n        if not path.startswith('.'):\n            labels[i] = path\n            for file in os.listdir(dir_path + path):\n                if not file.startswith('.'):\n                    img = cv2.imread(dir_path + path + '\/' + file)\n                    img = img.astype('float32') \/ 255\n                    resized = cv2.resize(img, IMG_SIZE, interpolation = cv2.INTER_AREA)\n                    X.append(resized)\n                    y.append(i)\n            i += 1\n    X = np.array(X)\n    y = np.array(y)\n    print(f'{len(X)} images loaded from {dir_path} directory.')\n    return X, y, labels","605c2e45":"IMG_SIZE= (48, 48)","10c1ae94":"X_train, y_train, train_labels = load_data(TRAIN_DIR, IMG_SIZE)","b7a0d671":"train_labels","7e1c0073":"X_test, y_test, test_labels = load_data(TEST_DIR,IMG_SIZE)","beed6c2a":"def plot_samples(X, y, labels_dict, n=50):\n   \n    for index in range(len(labels_dict)):\n        imgs = X[np.argwhere(y == index)][:n]\n        j = 10\n        i = int(n\/j)\n\n        plt.figure(figsize=(10,3))\n        c = 1\n        for img in imgs:\n            plt.subplot(i,j,c)\n            plt.imshow(img[0])\n\n            plt.xticks([])\n            plt.yticks([])\n            c += 1\n        plt.suptitle(labels_dict[index])\n        plt.show()","3583da08":"plot_samples(X_train, y_train, train_labels, 10)","abc84893":"from keras.utils.np_utils import to_categorical\n\nY_train = to_categorical(y_train, num_classes=6)\nY_train.shape","68c46891":"Y_test = to_categorical(y_test, num_classes=6)\nY_test.shape","851f68e0":"from keras.applications.vgg16 import VGG16\n\nbase_model = VGG16(\n        weights=None,\n        include_top=False, \n        input_shape=IMG_SIZE + (3,)\n    )\n\nbase_model.summary()","f869381d":"NUM_CLASSES = 6\n\nmodel = Sequential()\nmodel.add(base_model)\nmodel.add(Flatten())\nmodel.add(Dense(1000, activation=\"relu\"))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(NUM_CLASSES, activation=\"softmax\"))\n","56cfad61":"def deep_model(model, X_train, Y_train, epochs, batch_size):\n   \n    model.compile(\n    loss='binary_crossentropy',\n    optimizer=RMSprop(learning_rate=1e-4),\n    metrics=['accuracy'])\n    \n    history = model.fit(X_train\n                       , Y_train\n                       , epochs=epochs\n                       , batch_size=batch_size\n                       , verbose=1)\n    return history","bcc76bf3":"epochs = 40\nbatch_size = 64\n\nhistory = deep_model(model, X_train, Y_train, epochs, batch_size)\n","b52636d9":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \n    plt.figure(figsize = (6,6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    cm = np.round(cm,2)\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","4b56c633":"# Validate on test set\n\npredictions = model.predict(X_test)\ny_pred = [np.argmax(probas) for probas in predictions]\n\n\naccuracy = accuracy_score(y_test, y_pred)\nprint('Test Accuracy = %.2f' % accuracy)\n\nconfusion_mtx = confusion_matrix(y_test, y_pred) \ncm = plot_confusion_matrix(confusion_mtx, classes = list(test_labels.items()), normalize=False)","160d8536":"from keras.preprocessing.image import ImageDataGenerator\n\ndatagen_test = ImageDataGenerator(rescale = 1.\/255)\npred_generator = datagen_test.flow_from_directory(TEST_DIR,\n                                                 target_size = (48,48),\n                                                 color_mode = \"grayscale\",\n                                                 batch_size = batch_size,\n                                                 class_mode = \"categorical\",\n                                                 shuffle=False)","b93bc33a":"new_predictions = model.predict(X_test)\ny_pred = [np.argmax(probas) for probas in new_predictions]\ny_pred = [test_labels[k] for k in y_pred]","19c5335b":"filenames = pred_generator.filenames\nactual_class = [test_labels[h] for h in pred_generator.classes]","6af1d5a6":"import pandas as pd\n\npred_result = pd.DataFrame({\"Filename\":filenames,\n                           \"Predictions\":y_pred,\n                           \"Actual Values\":actual_class})\n\npred_result.head()","14ca7fe3":"from random import randint\n\nl = len(filenames)\nbase_path = TEST_DIR\nfor i in range(10):  # 10 images\n    \n    rnd_number = randint(0,l-1)\n    filename,pred_class,actual_class = pred_result.loc[rnd_number]\n    \n    img_path = os.path.join(base_path,filename)\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n    plt.title(\"Predicted Class: {} {} Actual Class: {}\".format(pred_class,'\\n',actual_class))\n    plt.show()\n    pass","75237354":"Load data","be4db426":"# **Model Building**\n\nVGG16 is a convolution neural net (CNN ) architecture which was used to win ILSVR(Imagenet) competition in 2014. It is considered to be one of the excellent vision model architecture till date. Most unique thing about VGG16 is that instead of having a large number of hyper-parameter they focused on having convolution layers of 3x3 filter with a stride 1 and always used same padding and maxpool layer of 2x2 filter of stride 2. It follows this arrangement of convolution and max pool layers consistently throughout the whole architecture. In the end it has 2 FC(fully connected layers) followed by a softmax for output. The 16 in VGG16 refers to it has 16 layers that have weights. This network is a pretty large network and it has about 138 million (approx) parameters.\n\n![](https:\/\/miro.medium.com\/max\/940\/1*3-TqqkRQ4rWLOMX-gvkYwA.png)","f52d501e":"# **Introduction**\n\nOne of the important ways humans display emotions is through facial expressions. Facial expression recognition is one of the most powerful, natural and immediate means for human beings to communicate their emotions and intensions. Humans can be in some circumstances restricted from showing their emotions, such as hospitalized patients, or due to deficiencies; hence, better recognition of other human emotions will lead to effective communication. Automatic human emotion recognition has received much attention recently with the introduction of IOT and smart environments at hospitals, smart homes and smart cities. Intelligent personal assistants (IPAs), such as Siri, Alexia, Cortana and others, use natural language processing to communicate with humans, but when augmented with emotions, it increases the level of effective communication and human-level intelligence.\n\n![](https:\/\/ars.els-cdn.com\/content\/image\/1-s2.0-S235291482030201X-gr1.jpg)\n\nA convolutional neural network was used in our system to obtain improved facial emotion detection as it is applied to other computer fields such as face recognition and object detection. In addition, predictions are based on information given at a particular time.\n\nFig shows the network structure that is used for emotion detection using facial landmarks. This network takes an input image and attempts to predict the output emotion.\n\n![](https:\/\/ars.els-cdn.com\/content\/image\/1-s2.0-S235291482030201X-gr6.jpg)","7fe0c8ae":"**Encoding Classes**\n\nUsing the method to_categorical(), a numpy array (or) a vector which has integers that represent different categories, can be converted into a numpy array (or) a matrix which has binary values and has columns equal to the number of categories in the data.","c5d0a44b":"# **Load Data and Plot Samples**","cbf9d031":"**Let's try our model and make predictions**","dbbb1447":"**Confusion Matrix**\n\nAdditionally, the sensitivity, specificity, F-score, and accuracy are calculated for each class by using the following calculated confusion matrix Fig for the six classes for emotion detection. Each class is used against all classes in order to find those performance factors related to it.\n\n","b4923210":"**Import libraries**"}}