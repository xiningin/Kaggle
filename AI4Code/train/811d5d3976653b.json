{"cell_type":{"a4aaeb0b":"code","95d59610":"code","7f93ae62":"code","8320cb85":"code","93dba070":"code","3cd00d32":"code","e4c42503":"code","bc7852a4":"code","1ecce1bc":"code","305ec5f6":"code","3c78c907":"code","baab0633":"code","5ac88bcc":"code","79537679":"code","9fd8ea9a":"code","e2ba2d63":"code","47bf9b11":"code","850acc3e":"code","ea912298":"code","2e130a9f":"code","d70298e2":"code","0cdf4893":"code","0cfeaf3d":"code","ef68cef0":"code","0e1deb7f":"code","b1e00c54":"code","0f3ceec7":"code","064410f7":"code","925a431d":"code","bf756775":"code","be3d3bd4":"code","8eb49f84":"code","a4f8e209":"code","5834e4d3":"code","328c0eb8":"code","4720c8d1":"code","ae275617":"code","141f5ee1":"code","b36f6eb8":"code","2e0ca161":"code","3584d1cb":"markdown","f3c763a5":"markdown","8209d8cb":"markdown","4c509097":"markdown","a6d9eb49":"markdown","99d4dcfc":"markdown","1d19ffd3":"markdown","fecc506f":"markdown","7259d5cc":"markdown","4443b2b7":"markdown","5d7978ad":"markdown","912a8f48":"markdown","66c1e41e":"markdown","3df997f2":"markdown","0a8c0fdf":"markdown","f7934810":"markdown","6578a849":"markdown","1014ba28":"markdown","c1fcde05":"markdown","de6bd31e":"markdown","35ce8897":"markdown","6848a6c4":"markdown","e5a4c19c":"markdown","34905d8d":"markdown","c0a59822":"markdown","ec7fcaf6":"markdown","9b274b3e":"markdown"},"source":{"a4aaeb0b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","95d59610":"column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'PRICE']\nboston = pd.read_csv('\/kaggle\/input\/boston-house-prices\/housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)","7f93ae62":"boston.head()","8320cb85":"boston.shape","93dba070":"boston.isnull().sum()","3cd00d32":"corr = boston.corr()\ncorr.shape","e4c42503":"boston.hist(figsize=(10, 10), bins=20)\nplt.show()","bc7852a4":"boston.describe()","1ecce1bc":"plt.figure(figsize=(20,10))\nplt.boxplot(boston)\nplt.show()","305ec5f6":"plt.figure(figsize=(20, 10))\nsns.heatmap(corr,  annot=True, cmap='twilight_r')","3c78c907":"from sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\ncolumn_sels = ['ZN', 'INDUS', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'LSTAT']\nX = boston.loc[:,column_sels]\ny = boston['PRICE']\nfig, axs = plt.subplots(ncols=4, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor i, k in enumerate(column_sels):\n    sns.regplot(y=y, x=X[k], ax=axs[i])\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","baab0633":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 4)","5ac88bcc":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, y_train)","79537679":"coeffcients = pd.DataFrame([X_train.columns,lr.coef_]).T\ncoeffcients = coeffcients.rename(columns={0: 'Zmienna', 1: 'Wsp\u00f3\u0142czynnik kierunkowy'})\ncoeffcients","9fd8ea9a":"print('Wyraz wolny: ', lr.intercept_)","e2ba2d63":"y_pred = lr.predict(X_train)","47bf9b11":"print('R^2:',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","850acc3e":"plt.scatter(y_train, y_pred)\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices\")\nplt.title(\"Prices do Predicted prices\")\nplt.show()","ea912298":"y_test_pred = lr.predict(X_test)","2e130a9f":"acc_linreg = metrics.r2_score(y_test, y_test_pred)\nprint('R^2:', acc_linreg)\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_test_pred))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_test_pred))\nprint('MSE:',metrics.mean_squared_error(y_test, y_test_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))","d70298e2":"lr.score(X_test,y_test)","0cdf4893":"train_errors = []\ntest_error = []","0cfeaf3d":"from sklearn.metrics import mean_squared_error\ndef plot_learning_curve(model,X,y):\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 4)\n\n    for m in range(1, len(X_train)):\n        model.fit(X_train[:m],y_train[:m])\n        y_train_pred = model.predict(X_train[:m])\n        y_test_pred = model.predict(X_test[:m])\n        train_errors.append(metrics.r2_score(y_train[:m], y_train_pred))\n        test_error.append(metrics.r2_score(y_test[:m], y_test_pred))\n        \n    plt.plot(np.sqrt(train_errors),\"r-+\",linewidth=2,label=\"train\")\n    plt.plot(np.sqrt(test_error),\"b-\",linewidth=3,label=\"test\")\n    plt.xlabel('Rozmiar zestawu ucz\u0105cego')\n    plt.ylabel('B\u0142\u0105d R^2')\n    plt.title('Krzywa uczenia')\n    plt.legend() \n    plt.show()\n","ef68cef0":"lr = LinearRegression()\nplot_learning_curve(lr,X_train,y_train)   ","0e1deb7f":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)","b1e00c54":"models = []\nmodels.append( ('Ridge', Ridge()) )\nmodels.append( ('Lasso', Lasso()) )","0f3ceec7":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nimport joblib\nfor name, model in models:\n pipelined_model = Pipeline([('minmax', MinMaxScaler()),('pca', PCA(n_components = 3)),(name, model)])\n pipelined_model.fit(rescaledX, y_train)\n y_hat = pipelined_model.predict(X_test)\n RMSE = np.sqrt(mean_squared_error(y_test, y_hat))\n print('Model: ', name, ' | RMSE: ', RMSE)\n print('----------------')\n joblib.dump(pipelined_model, '{}_model.pkl'.format(name))","064410f7":" from sklearn.svm import SVC\n from sklearn.preprocessing import StandardScaler\n from sklearn.datasets import make_classification\n from sklearn.model_selection import train_test_split\n from sklearn.pipeline import Pipeline","925a431d":"X, y = make_classification(random_state=0) \nX_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0)\npipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC(C=2.0, kernel='rbf',gamma='scale'))])\npipe.fit(X_train, y_train)\nPipeline(steps=[('scaler', StandardScaler()), ('svc', SVC())])\npipe.score(X_test, y_test)","bf756775":"X, y = make_classification(random_state=0) \nX_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0)\npipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC(C=0.4, kernel='linear'))])\npipe.fit(X_train, y_train)\nPipeline(steps=[('scaler', StandardScaler()), ('svc', SVC())])\npipe.score(X_test, y_test)","be3d3bd4":"models = []\nmodels.append( ('Ridge', Ridge()) )\nmodels.append( ('Lasso', Lasso()) )\nmodels.append( ('svc-rbf', SVC(C=2.0, kernel='rbf',gamma='scale')))\nmodels.append( ('svc-linear', SVC(C=0.4, kernel='linear')))             ","8eb49f84":"for name, model in models:\n pipelined_model = Pipeline([('minmax', MinMaxScaler()),('pca', PCA(n_components = 3)),(name, model)])\n pipelined_model.fit(X_train, y_train)\n y_hat = pipelined_model.predict(X_test)\n RMSE = np.sqrt(mean_squared_error(y_test, y_hat))\n print('Model: ', name, ' | RMSE: ', RMSE)\n print('----------------')\n joblib.dump(pipelined_model, '{}_model.pkl'.format(name))","a4f8e209":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\nlogistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,random_state=0)\ndistributions = dict(C=uniform(loc=0, scale=4),penalty=['l2', 'l1'])# l1 lasso l2 ridge\nclf = RandomizedSearchCV(logistic, distributions, random_state=0)\nsearch = clf.fit(X_train,y_train)\nscore=clf.score(X_train,y_train)\nsearch.best_params_","5834e4d3":"lr_new=LogisticRegression(C=2.195254015709299,penalty='l1', solver='liblinear')\nlr_new.fit(X_train,y_train)\nprint(\"score\",lr_new.score(X_test,y_test))","328c0eb8":"models.append( ('RandomizedSearchCV', RandomizedSearchCV(logistic, distributions, random_state=0)) )","4720c8d1":"for name, model in models:\n pipelined_model = Pipeline([('minmax', MinMaxScaler()),('pca', PCA(n_components = 3)),(name, model)])\n pipelined_model.fit(X_train, y_train)\n y_hat = pipelined_model.predict(X_test)\n RMSE = np.sqrt(mean_squared_error(y_test, y_hat))\n print('Model: ', name, ' | RMSE: ', RMSE)\n print('----------------')\n joblib.dump(pipelined_model, '{}_model.pkl'.format(name))","ae275617":"from sklearn.ensemble import GradientBoostingRegressor\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmodel = GradientBoostingRegressor(random_state=7, n_estimators=400)\nmodel.fit(rescaledX, y_train)\n# transform the validation dataset\nrescaledValidationX = scaler.transform(X_test)\npredictions = model.predict(rescaledValidationX)\nprint(mean_squared_error(y_test, predictions))","141f5ee1":"print('R^2:',metrics.r2_score(y_test, predictions))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, predictions))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:',metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, predictions)))","b36f6eb8":"models.append( ('GradientBoostingRegressor', GradientBoostingRegressor(random_state=7, n_estimators=400)) )\nmodels.append( ('LinearRegression', LinearRegression()) )","2e0ca161":"for name, model in models:\n pipelined_model = Pipeline([('minmax', MinMaxScaler()),('pca', PCA(n_components = 3)),(name, model)])\n pipelined_model.fit(X_train, y_train)\n y_hat = pipelined_model.predict(X_test)\n RMSE = np.sqrt(mean_squared_error(y_test, y_hat))\n print('Model: ', name, ' | RMSE: ', RMSE)\n print('----------------')\n joblib.dump(pipelined_model, '{}_model.pkl'.format(name))","3584d1cb":"* CRIM - wska\u017anik przest\u0119pczo\u015bci na mieszka\u0144ca wed\u0142ug miast,\n* ZN - udzia\u0142 teren\u00f3w mieszkalnych przeznaczonych pod dzia\u0142ki powy\u017cej 25 tys. Mkw.,\n* INDUS - udzia\u0142 niedetalicznych akr\u00f3w biznesowych przypadaj\u0105cych na miasto,\n* CHAS - zmienna zast\u0119pcza Charles River (1 je\u015bli trakt ogranicza rzek\u0119; 0 w przeciwnym razie),\n* NOX - st\u0119\u017cenie tlenk\u00f3w azotu (cz\u0105stki na 10 mln),\n* RM - \u015brednia liczba izb w mieszkaniu,\n* AGE - odsetek lokali okupowanych przez w\u0142a\u015bcicieli zbudowanych przed 1940 rokiem, \n* DIS - wa\u017cone odleg\u0142o\u015bci do pi\u0119ciu centr\u00f3w pracy w Bostonie,\n* RAD - wska\u017anik dost\u0119pno\u015bci do autostrad radialnych,\n* TAX - pe\u0142nowarto\u015bciowa stawka podatku od nieruchomo\u015bci na 10000 USD, \n* PTRATIO - stosunek liczby uczni\u00f3w do nauczycieli wed\u0142ug miast,\n* B - 1000 (Bk - 0,63) ^ 2 gdzie Bk to odsetek os\u00f3b czarnosk\u00f3rych w mie\u015bcie, \n* LSTAT \u2013 odsetek os\u00f3b o ni\u017cszym statusie,\n* PRICE- Mediana warto\u015bci dom\u00f3w zajmowanych przez w\u0142a\u015bcicieli w tysi\u0105cach dolar\u00f3w","f3c763a5":"**Krzywe uczenia**","8209d8cb":"Na podstawie wynik\u00f3w podstawowoego b\u0142\u0119du \u015bredniokwadratowego widzimy, \u017ce model z najlepszym wynikiem to GradientBoostingRegressor","4c509097":"Standaryzacja danych","a6d9eb49":"**Podstawowy b\u0142ad \u015bredniokwadratowy dla r\u00f3\u017cnych metod**","99d4dcfc":"Po obejrzeniu wykres\u00f3w zdecydowa\u0142am si\u0119 na usuni\u0119cie jeszcze zmiennej B, RAD, CRIM","1d19ffd3":"**Model**\n* Regresja liniowa","fecc506f":"**Regularyzowane modele liniowe**","7259d5cc":"**Wyregulowanie modelu**","4443b2b7":"Lepszy wynik modelu dostajemy dla j\u0105dra linear.","5d7978ad":"Zdecydowa\u0142am si\u0119 na tworzenie modelu z wykorzystaniem tylko zmiennych ZN, INDUS, RM, AGE, DIS, TAX, PTRATIO, LSTAT.\nUsune\u0142am zmienne CHAS i NOX ze wzgl\u0119du na wysok\u0105 korelacj\u0119.","912a8f48":"**Model SVC**\n\nRozwa\u017cmay model SVC z dwoma r\u00f3\u017cnymi rodzajami j\u0105dra linear i rbf.","66c1e41e":"**Dzielimy nasze dane na dwa zbiory:**","3df997f2":"**Wgranie danych**","0a8c0fdf":"Metoda siatki","f7934810":"Dane zawiraj\u0105 14 zmiennych i 506 rekord\u00f3w","6578a849":"* MAE - \u015aredni b\u0142\u0105d bezwzgl\u0119dny reprezentuje \u015bredni\u0105 bezwzgl\u0119dnej r\u00f3\u017cnicy mi\u0119dzy rzeczywistymi i przewidywanymi warto\u015bciami w zbiorze danych. Mierzy \u015bredni\u0105 reszt w zbiorze danych.\n* MSE - B\u0142\u0105d \u015bredniokwadratowy reprezentuje \u015bredni\u0105 kwadrat\u00f3w r\u00f3\u017cnicy mi\u0119dzy oryginalnymi i przewidywanymi warto\u015bciami w zestawie danych. Mierzy wariancj\u0119 reszt.\n* RMSE - Podstawowy b\u0142\u0105d \u015bredniokwadratowy to pierwiastek kwadratowy b\u0142\u0119du \u015bredniokwadratowego. Mierzy odchylenie standardowe reszt.\n* R^2 - Wsp\u00f3\u0142czynnik determinacji lub R-kwadrat reprezentuje proporcj\u0119 wariancji zmiennej zale\u017cnej, kt\u00f3ra jest wyja\u015bniona przez model regresji liniowej. Jest to wynik bez skali, tj. Niezale\u017cnie od tego, czy warto\u015bci s\u0105 ma\u0142e czy du\u017ce, warto\u015b\u0107 R kwadrat b\u0119dzie mniejsza ni\u017c jeden.\n* Adjusted R^2 - Skorygowany R kwadrat jest zmodyfikowan\u0105 wersj\u0105 R-kwadrat i jest dostosowany do liczby niezale\u017cnych zmiennych w modelu i zawsze b\u0119dzie mniejszy lub r\u00f3wny R\u00b2.\n\n\n","1014ba28":"**Wyniki dzia\u0142ania modelu**","c1fcde05":"**Pradykcja na zbiorze testowym**","de6bd31e":"**Macierz korelacji**","35ce8897":"Regresja Lasso i regresja grzbietowa","6848a6c4":"Nie zawieraj\u0105 rzadnych brak\u00f3w","e5a4c19c":"Parametry modelu","34905d8d":"* X \u2013 zmienne bez zmiennej celu, zbi\u00f3r pos\u0142u\u017cy nam do uczenia naszego modelu\n* y \u2013 warto\u015bci do uczenia modelu zmiennej celu","c0a59822":"**Predyktor maszyny wektor\u00f3w no\u015bnych**","ec7fcaf6":"**Podsumowanie**","9b274b3e":"Widzimy, \u017ce dla danych po standaryzacji lepszy wynik dostajemy przy regresji lasso."}}