{"cell_type":{"a286d616":"code","34c475fe":"code","a63be822":"code","06064392":"code","2ddce08b":"code","a470a7b7":"code","a66ea316":"code","d26ef59a":"code","94a36cf8":"code","d3908e14":"code","7a1583d4":"code","7f857e42":"code","49322d3d":"markdown","b88f232d":"markdown","43133048":"markdown"},"source":{"a286d616":"# thanks for RAHUL D SHETTY\u2018s Wonderful job\n# https:\/\/www.kaggle.com\/rahuldshetty\/word-spelling-correction-with-lstm","34c475fe":"import pandas as pd\nimport numpy as np\nimport random\nimport torch as t\nprint(t.__version__)\nimport tensorflow as tf\nprint(tf.__version__)\nimport re\nfrom tqdm.notebook import  tqdm\nimport gc\ngc.collect()","a63be822":"# 1- \u8bad\u7ec3\u96c6\u751f\u6210\nclass GeneratData:\n    def __init__(self, test_size=0.3, embedding_func=None):\n        self.test_size = test_size\n        self.embedding_func = embedding_func\n        self.__char_int_map()\n    \n    def __char_int_map(self):\n        self.char_set = [chr(i) for i in range(ord('a'), ord('z')+1)] + '0 1 2 3 4 5 6 7 8 9'.split() + ['\\t', '\\n', '#']\n        self.char2int = dict(zip(self.char_set, range(len(self.char_set))))\n        self.int2char = dict(zip(range(len(self.char_set)), self.char_set))\n\n    def _load_data(self):\n        file_path = '..\/input\/english-word-frequency\/unigram_freq.csv'\n        orignal_df = pd.read_csv(file_path)\n        orignal_df['word'] = orignal_df['word'].apply(self._word_process)\n        # \u53ea\u62ff\u53d6str\u7684\n        orignal_df['need'] = orignal_df['word'].map(lambda x: type(x) == type('a'))\n        return orignal_df.loc[orignal_df['need'], 'word'].tolist()\n\n    def _word_process(self, word):\n        \"\"\"\n        \u5c06\u6570\u636e\u6e05\u6d17\u5e72\u51c0\n        \"\"\"\n        try:\n            word = word.lower()\n            word = re.sub(r'[^0-9a-zA-Z]', '', word)\n            return word\n        except Exception as e:\n            return word\n    \n    def generate(self, thresh=0.2):\n        \"\"\"\n        \u8bad\u7ec3\u6570\u636e\n        1- \u751f\u6210 input1 input2\n        2- embedding\n        \n        2021-10-17 \u589e\u52a0\u6570\u636e\u6269\u589e\n        \"\"\"\n        lines = self._load_data()\n        test_size = int(len(lines) * self.test_size)\n        input1_list, input2_list = [], []\n        input1_max_len, input2_max_len = 0, 0\n        for w in tqdm(lines[:-test_size]):\n            # \u6bcf\u4e2a\u5355\u8bcd\u6269\u589e5\u4e2a\u7ed3\u679c\n            len_w = len(w)\n            range_n = np.random.choice([1, 1, 1, 1, 3])\n            for _ in range(range_n):\n                # \u9488\u5bf9\u77ed\u5355\u8bcd\n                if len_w >= 5:\n                    input2_word = f'\\t{w}\\n'\n                    input1_word = self.gen_gibberish(w, thresh=thresh)\n                    input1_list.append(input1_word)\n                    input2_list.append(input2_word)\n\n                    input1_max_len = max(input1_max_len, len(input1_word))\n                    input2_max_len = max(input2_max_len, len(input2_word))\n\n        # 2- embedding\n        return self.word_embedding(input1_list, input2_list, input1_max_len, input2_max_len)\n\n\n    def word_embedding(self, input1_list, input2_list, input1_max_len, input2_max_len):\n        \"\"\"\n        \u5f53\u6ca1\u6709\u63d0\u4f9bembedding\u7684\u65b9\u6cd5\u7684\u65f6\u5019\uff0c\n        \u91c7\u7528\u6700\u7b80\u5355\u7684\u5b57\u6bcd\u4f4d\u7f6e\u53ca\u51fa\u73b0\u5219\u6807\u8bb0\u4e3a1\uff0c \u5426\u5219\u6807\u8bb0\u4e3a0\u3002 \u4fbf\u4e8e\u540e\u9762\u4e00\u4e2a\u4e00\u4e2a\u5b57\u6bcd\u9884\u6d4b\u7684\u65f6\u5019\u62bd\u53d6\u5b57\u6bcd\n        \"\"\"\n        samples_count = len(input1_list)\n        input1_encode_data = np.zeros((samples_count, input1_max_len, len(self.char_set)), dtype='float64')\n        input2_decode_data = np.zeros((samples_count, input2_max_len, len(self.char_set)), dtype='float64')\n        target_data = np.zeros((samples_count, input2_max_len, len(self.char_set)), dtype='float64')\n\n        # \u5c06\u77e9\u9635\u586b\u5145\u4e0a\u6570\u636e \u67d0\u4e2a\u5b57\u6bcd\u51fa\u73b0\u4e00\u6b21\u5219\u6807\u8bb0\u589e\u52a01\n        for num_idx, (inp1_w, inp2_w) in tqdm(enumerate(zip(input1_list, input2_list))):\n            for w_idx, chr_tmp in enumerate(inp1_w):\n                input1_encode_data[num_idx, w_idx, self.char2int[chr_tmp]] = 1\n\n            for w_idx, chr_tmp in enumerate(inp2_w):\n                input2_decode_data[num_idx, w_idx, self.char2int[chr_tmp]] = 1\n                if w_idx > 0: # \u9884\u6d4b\u8d77\u59cb\u7b26\u540e\u7684\n                    target_data[num_idx, w_idx - 1, self.char2int[chr_tmp]] = 1\n        \n        del input1_list, input2_list, input1_max_len, input2_max_len\n        gc.collect()\n        return input1_encode_data, input2_decode_data, target_data\n\n    def gen_gibberish(self, eng_word, thresh=0.2):\n        \"\"\"\n        \u751f\u6210\u9519\u8bef\u5355\u8bcd\n        20211017\u589e\u52a0\u4fee\u6b63\uff1a\u9488\u5bf9\u8f83\u77ed\u5355\u8bcd\n        \"\"\"\n        max_times = len(eng_word) * thresh\n        times = int(random.randrange(1, len(eng_word)) * thresh) if max_times >= 2 else random.randrange(0, 2)\n        while times != 0:\n            times -= 1\n            val = random.randrange(0, 10)\n            idx = random.randrange(2, len(eng_word))\n            insert_index = random.randrange(0, len(self.char_set))\n            if val <=3 : # delete\n                eng_word = eng_word[:idx] + eng_word[idx+1:]\n            elif val <= 5: # add\n                eng_word = eng_word[:idx] + self.char_set[insert_index] + eng_word[idx:]\n            else: # replace\n                eng_word = eng_word[:idx] + self.char_set[insert_index] + eng_word[idx+1:]\n        \n        return eng_word\n\n","06064392":"\n# \u5b9a\u4e49\u7ea0\u6b63\u6a21\u578b\n## \u7f16\u7801 -> \u89e3\u7801\nfrom tensorflow.keras.layers import Dense, LSTM, Input\nfrom tensorflow.keras import Model\n\ndef de_right_word_tf2(lstm_units, out_dims, encode_max_len, decode_max_len, lr=0.001):\n    encoder_lstm = LSTM(lstm_units, return_state=True)\n    # \u9700\u8981\u5c06\u5404\u4e2a\u9690\u5c42\u7684\u7ed3\u679c\u4f5c\u4e3a\u4e0b\u4e00\u5c42\u7684\u8f93\u5165\u65f6\uff0c\u9009\u62e9\u8bbe\u7f6e return_sequences=True \n    decoder_lstm = LSTM(lstm_units, return_state=True, return_sequences=True)\n    fc = Dense(out_dims, activation='softmax')\n\n    input_1 = Input(shape=(None, encode_max_len))\n    encode_out, encode_h, encode_c = encoder_lstm(input_1)\n    input_2 = Input(shape=(None, decode_max_len))\n    decode_out, decode_h, decode_c = decoder_lstm(input_2, initial_state=[encode_h, encode_c])\n    predict_out = fc(decode_out)\n    model = Model([input_1, input_2], predict_out)\n    opt = tf.keras.optimizers.RMSprop(lr=lr, rho=0.98, epsilon=1e-06)\n    model.compile(\n        optimizer=opt,\n        loss=['categorical_crossentropy']\n    )\n    return model\n\n\n\n## \u7531\u4e8e\u9884\u6d4b\u7684word \u4e0d\u77e5\u9053\u4f55\u65f6\u7ed3\u675f\uff0c \u6240\u4ee5\u6211\u4eec\u9700\u8981\u5bf9\u8f93\u5165\u7684\u503c\u8fdb\u884c\u4e0d\u65ad\u7684\u4fee\u6b63\uff0c\u76f4\u5230\u9884\u6d4b\u5230\u672b\u5c3e\u7b26\u4e3a\u6b62\n## \u6240\u4ee5\u6211\u4eec\u5bf9\u4e8e\u4e00\u4e2a\u5168\u65b0\u7684\u8f93\u5165\uff0c\u8fdb\u884c\u9884\u6d4b\u7684\u65f6\u5019\u9700\u8981\u5148\u4f7f\u5f97input2 \u8f93\u5165\u4e3a \u4e00\u4e2a\u5168\u7a7a\u7684\u5355\u8bcd\u77e9\u9635\ndef predict(m, input1_test):\n    input2_orign = np.zeros((1, 36, 39))\n    input2_orign[:, 0, g.char2int['\\t']] = 1\n\n    input_word = ''\n    pred_word = ''\n    \n    for idx in range(input2_orign.shape[1] - 1): # max_encode_len\n        p_tmp =  m.predict([tf.constant(input1_test), tf.constant(input2_orign)])\n        # update input\n        input2_w_idx = np.argmax(p_tmp[:, idx, :], axis=1)[0]\n        input2_orign[:, idx+1, :] = p_tmp[:, idx, :]\n#         input2_orign[:, idx+1, input2_w_idx] = 1\n        \n        input1_w_idx = np.argmax(input1_test[:, idx, :], axis=1)[0]\n        pred_word += g.int2char[input2_w_idx]\n        input_word += g.int2char[input1_w_idx]\n#         print(f'[{idx}] input_word: {input_word},  pred_word : {pred_word}' )\n\n        if (pred_word[-1] == '\\n'):\n            break\n    print(f'[{idx}] input_word: {input_word},  pred_word : {pred_word}' )\n    return pred_word\n\n\ndef word2tensor(word):\n    \"\"\"\n    \u5f53\u6ca1\u6709\u63d0\u4f9bembedding\u7684\u65b9\u6cd5\u7684\u65f6\u5019\uff0c\n    \u91c7\u7528\u6700\u7b80\u5355\u7684\u5b57\u6bcd\u4f4d\u7f6e\u53ca\u51fa\u73b0\u5219\u6807\u8bb0\u4e3a1\uff0c \u5426\u5219\u6807\u8bb0\u4e3a0\u3002 \u4fbf\u4e8e\u540e\u9762\u4e00\u4e2a\u4e00\u4e2a\u5b57\u6bcd\u9884\u6d4b\u7684\u65f6\u5019\u62bd\u53d6\u5b57\u6bcd\n    \"\"\"\n    char_set = [chr(i) for i in range(ord('a'), ord('z')+1)] + '0 1 2 3 4 5 6 7 8 9'.split() + ['\\t', '\\n', '#']\n    char2int = dict(zip(char_set, range(len(char_set))))\n    # int2char = dict(zip(range(len(char_set)), char_set))\n    input1_encode_data = np.zeros((1, 34, len(char_set)), dtype='float64')\n\n    # \u5c06\u77e9\u9635\u586b\u5145\u4e0a\u6570\u636e \u67d0\u4e2a\u5b57\u6bcd\u51fa\u73b0\u4e00\u6b21\u5219\u6807\u8bb0\u589e\u52a01\n    for w_idx, chr_tmp in enumerate(list(word)):\n        if w_idx == 34:\n            break\n        input1_encode_data[0, w_idx, char2int[chr_tmp]] = 1\n\n    return input1_encode_data\n\n\ndef word_correct(m, word):\n    input1_encode_data = word2tensor(word)\n    return predict(m, input1_encode_data)","2ddce08b":"g = GeneratData(test_size=0.4)\ninput1_encode_data, input2_decode_data, target_data = g.generate()","a470a7b7":"m = de_right_word_tf2(256, 39, 39, 39)\n# m.summary()\nhis_ = m.fit([tf.constant(input1_encode_data), tf.constant(input2_decode_data)], tf.constant(target_data),\n    epochs=500,\n    batch_size=64,\n    validation_split=0.2\n)","a66ea316":"m2 = de_right_word_tf2(256, 39, 39, 39, lr=0.001)\n\n# torch.optim.lr_scheduler.ReduceLROnPlateau \nscheduler = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', mode='min', \n    patience=10, min_delta=0.001,\n    factor=0.8, verbose=1,\n    min_lr=1e-5\n)\nhis_2 = m2.fit([tf.constant(input1_encode_data), tf.constant(input2_decode_data)], tf.constant(target_data),\n    epochs=500,\n    batch_size=256, # 128,\n    validation_split=0.2,\n    callbacks=[scheduler]\n)","d26ef59a":"import matplotlib.pyplot as plt\nplt.plot(his_.history['loss'], label='without-schedule')\n# plt.plot(his_2.history['loss'], label='with-schedule')\nplt.legend()\nplt.show()","94a36cf8":"# \u7531\u4e8e\u8f93\u5165\u8f93\u51fa\u90fd\u662f\u6709\u503c\u7684\u6240\u6709\u53ef\u4ee5\u76f4\u63a5\u9884\u6d4b\u51fa\u6240\u6709\u503c\np = m.predict([tf.constant(input1_encode_data[1:2, :, :]), tf.constant(input2_decode_data[1:2, :, :])])\nprint(p)\n\ninput1_test = input1_encode_data[10:11, :, :]\npredict(m, input1_test)","d3908e14":"for  i in range(1, 50):\n    print('--'*23, f'[{i}]', '--'*23)\n    input1_test = input1_encode_data[i:i+1, :, :]\n    predict(m, input1_test)\n#     predict(m2, input1_test)","7a1583d4":"# word2tensor('hellp')[0, 4]\n\nword_correct(m, 'wanderful') ","7f857e42":"word2tensor('cabb').shape, word2tensor('cabb').sum()","49322d3d":"## Improved Model\n### 1- add schedule - reduce learning rate when the val_loss not reduce lasted 10 epochs\nFrom the before model train, we find that the val_loss almost not reduce after 200 epochs. So Our target is to reduce learning rate to reduce the val_loss\n\n> `tf.keras.callbacks.ReduceLROnPlateau`  \nmonitor\uff1afocus metric -> val_loss  \nfactor\uff1anew_lr = lr * factor  \npatience\uff1amonitor not reduce lasted n epochs.  \nmode\uff1a{auto,min,max}; min: monitor stop reduce Then reduce lr;max: monitor stop increase Then reduce lr;auto find the direction    \nmin_delta\uff1amin delta of monitor.  \ncooldown\uff1anumber of epochs to wait before resuming normal operation after lr has been reduced.  \nmin_lr\uff1alower bound on the learning rate.","b88f232d":"## ","43133048":"# Model Train"}}