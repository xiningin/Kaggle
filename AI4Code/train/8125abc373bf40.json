{"cell_type":{"e67c0707":"code","41072ac4":"code","f69b9375":"code","a50d85de":"code","ba419b9e":"code","f1a54d43":"code","7ee7464b":"code","ee6ac223":"code","6349e8c4":"code","a0177f32":"code","733ce9a6":"code","bc646e3d":"code","827f465b":"code","ee02726d":"code","78df034a":"code","335112ad":"code","371d201a":"code","1c19920b":"code","da2d415e":"code","180dce19":"code","38dc5ec6":"code","22a3a81d":"code","695b1dec":"code","ebd1b695":"code","4dc11d63":"code","9a2c47c8":"code","34b7b0ce":"code","d107cc9f":"code","c7331c6a":"code","1e145474":"code","9700d5f2":"code","29e15fd5":"code","6e4d9a47":"code","7b85a714":"code","b62bfa67":"code","36669686":"code","2e94cd94":"code","30650066":"code","1f5eb7e9":"code","f0709071":"code","52b874f8":"code","f6e3f948":"code","d7ceda45":"code","83e9a53a":"code","695adfba":"code","41bba107":"code","dadda212":"code","62611bcc":"code","3982fe2e":"code","60d34cba":"code","cd8727b0":"code","6be228c0":"code","ae3acf77":"code","49ddc762":"code","7e07578a":"code","5f785e69":"code","16a3f0b1":"code","ffdea865":"code","c894cb3e":"code","a1777f0c":"code","1dd3280d":"code","b1136cae":"code","7d40f285":"code","59e9bd80":"code","f04e9c0f":"code","aaca1558":"code","fa9631bb":"code","44acbcf7":"markdown","f413802f":"markdown","a01dda1f":"markdown","ce75e685":"markdown","e909b430":"markdown","395e86d6":"markdown","97a80326":"markdown","376f6373":"markdown","50670067":"markdown","390bcbc3":"markdown","744f8d42":"markdown","7d16e6b1":"markdown","974d76fd":"markdown","bb536c3f":"markdown","343583bc":"markdown","1919d568":"markdown","727d604f":"markdown","40acfbfc":"markdown","7a1186f8":"markdown","b0db87c9":"markdown","c053394d":"markdown","02b804c9":"markdown","47ebf314":"markdown","04b409c5":"markdown","33335b42":"markdown","c74d4b2c":"markdown","eb0caccf":"markdown","65790a5f":"markdown","d892c89d":"markdown","5fa9b30f":"markdown","a58bc14e":"markdown","0a4f42d3":"markdown","9ce214f8":"markdown","3021d2ad":"markdown","f6b55a33":"markdown","ae14ec6b":"markdown","5c11d478":"markdown","a631afac":"markdown","dc0fe5de":"markdown","5ab231b1":"markdown","0009e3e9":"markdown","8bd56b5d":"markdown","7bb4a317":"markdown","ae7f0056":"markdown","ba79f0dd":"markdown","4cb29319":"markdown","2a247d41":"markdown","f5206958":"markdown","d44f46ac":"markdown","f24ce98c":"markdown","be04c85c":"markdown","e4111474":"markdown","9dfa257e":"markdown","6d74df2d":"markdown","4a41f3a9":"markdown","b43c896d":"markdown","98e684a2":"markdown","b87a44b6":"markdown","cfa6f52a":"markdown","11040ec5":"markdown","fa3d9513":"markdown","c5773c5f":"markdown","3e4d563f":"markdown","3b0fbc8f":"markdown","c174257b":"markdown","e71809b1":"markdown","9b27ce8b":"markdown","7b66c1a9":"markdown","ec925a3d":"markdown","be1ae3ff":"markdown","2df550be":"markdown","54f3b972":"markdown","465d4765":"markdown","9aa9017e":"markdown","2dffa680":"markdown","2f8ac0e4":"markdown","78f9cb7d":"markdown","7df2d5ad":"markdown"},"source":{"e67c0707":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","41072ac4":"import csv, json\nimport pandas as pd\nimport numpy as np\ntrain_file = {}\nwith open('..\/input\/house-prices-advanced-regression-techniques\/train.csv') as f:\n  csvReader = csv.DictReader(f)\n  for rows in csvReader:\n    id = rows['Id']\n    train_file[id] = rows\n\ntest_file = {}\nwith open('..\/input\/house-prices-advanced-regression-techniques\/test.csv') as f:\n  csvReader = csv.DictReader(f)\n  for rows in csvReader:\n    id = rows['Id']\n    test_file[id] = rows","f69b9375":"with open('train.json', 'w') as f:\n  f.write(json.dumps(train_file, indent=4))\nwith open('test.json', 'w') as f:\n  f.write(json.dumps(test_file, indent=4))","a50d85de":"with open(\"train.json\", \"rb\") as f:\n    train_file = json.load(f)\nwith open('train.json', 'r') as f:\n  for i in range(1*84):\n    print(i, \"\\t\", repr(f.readline()))","ba419b9e":"with open(\"test.json\", \"rb\") as f:\n    test_file = json.load(f)\nwith open('test.json', 'r') as f:\n  for i in range(1*83):\n    print(i, \"\\t\", repr(f.readline()))","f1a54d43":"type(train_file)","7ee7464b":"type(test_file)","ee6ac223":"train_file.keys()","6349e8c4":"test_file.keys()","a0177f32":"train_file['1'].keys()","733ce9a6":"test_file['1461'].keys()","bc646e3d":"print(\"The training dataset is\", os.path.getsize('..\/input\/house-prices-advanced-regression-techniques\/train.csv') \/ 1e6, \"MB\")","827f465b":"print(\"The test dataset is\", os.path.getsize('..\/input\/house-prices-advanced-regression-techniques\/test.csv') \/ 1e6, \"MB\")","ee02726d":"with open('..\/input\/house-prices-advanced-regression-techniques\/train.csv') as f:\n    print(f)","78df034a":"train_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', na_filter=True, encoding='UTF-8')\nnumeric_cols = train_data.select_dtypes(include=[np.number]).columns.tolist()\nfor col in numeric_cols:\n  train_data[col].fillna(0, inplace=True)\n#cat_cols = train_data.columns.tolist()\n#for col in numeric_cols:\n#  cat_cols.remove(col)\n#for col in cat_cols:\n#    train_data[col].fillna('None', inplace=True)\ntrain_data.head()","335112ad":"test_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', na_filter=True, encoding='UTF-8')\nnumeric_cols = test_data.select_dtypes(include=[np.number]).columns.tolist()\nfor col in numeric_cols:\n  test_data[col].fillna(0, inplace=True)\n#cat_cols = test_data.columns.tolist()\n#for col in numeric_cols:\n#  cat_cols.remove(col)\n#for col in cat_cols:\n#    test_data[col].fillna('None', inplace=True)\ntest_data.head()","371d201a":"train_data['SalePrice'].describe()","1c19920b":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm, skew, spearmanr\nsns.distplot(train_data['SalePrice'])","da2d415e":"print(\"Skewness of train data: %f\" % train_data['SalePrice'].skew())\nsns.distplot(np.log1p(train_data['SalePrice']), fit=norm)","180dce19":"train_data.corr()","38dc5ec6":"f, ax = plt.subplots(figsize=(12, 9))\nmask = np.triu(np.ones_like(train_data.corr(), dtype=bool))\nmap = sns.heatmap(train_data.corr(), vmin=-1, vmax=1, center=0, mask=mask, cmap=sns.dark_palette((260, 75, 60), input=\"husl\"), square=True)\nmap.set_xticklabels(map.get_xticklabels(), rotation=45, horizontalalignment='right')","22a3a81d":"corr = train_data.corr()\nhighest_correlations = corr.index[abs(corr['SalePrice']) > 0.5]\nmask = np.triu(np.ones_like(train_data[highest_correlations].corr(), dtype=bool))\nplt.figure(figsize=(10, 10))\ng = sns.heatmap(train_data[highest_correlations].corr(), annot=True, mask=mask, cmap=\"RdYlGn\")","695b1dec":"corr['SalePrice'].sort_values(ascending=False)","ebd1b695":"cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train_data[cols])","4dc11d63":"train_data['YearBuilt'].describe()","9a2c47c8":"train_data['YearRemodAdd'].describe()","34b7b0ce":"features = ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'MasVnrArea','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', \n            'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n            'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\ndef detect_outliers():\n  outliers = []\n  for feature in features:\n    subset = []\n    index = 0\n    threshold = 3\n    array = np.array([x for x in train_data[feature].tolist() if isinstance(x, int)])\n    mean = np.mean(array)\n    std = np.std(array)\n\n    for x in array:\n      z_score = (x - mean)\/std\n      feature_pair = {'Feature': feature, 'Home_ID': index, 'Value': x}\n      if np.abs(z_score) > threshold:\n        subset.append(feature_pair)\n      index = index+1\n    outliers.append(subset)\n  return outliers\n\noutliers = detect_outliers()\nfor subset in outliers:\n  index = 0\n  if len(subset) > 0: \n    print(\"Feature: \" + str(subset[index]['Feature']) + \", Proportion of outliers: \" + str(len(subset)\/1460 * 100) + \"%\")\n  index = index+1","d107cc9f":"fig, axes = plt.subplots(ncols=5, nrows=2, figsize=(16, 4))\naxes = np.ravel(axes)\n#col_name = ['GrLivArea','TotalBsmtSF','1stFlrSF','BsmtFinSF1','LotArea']\ncol_name = ['GrLivArea', 'TotalBsmtSF', '1stFlrSF', 'BsmtFinSF1', 'LotArea']\nfor i, c in zip(range(5), col_name):\n    train_data.plot.scatter(ax=axes[i], x=c, y='SalePrice', sharey=True, colorbar=False, c='r')\n\n# delete outliers\nprint(train_data.shape)\ntrain = train_data[train_data['GrLivArea'] < 4500]\ntrain = train_data[train_data['LotArea'] < 100000]\ntrain = train_data[train_data['TotalBsmtSF'] < 3000]\ntrain = train_data[train_data['1stFlrSF'] < 2500]\ntrain = train_data[train_data['BsmtFinSF1'] < 2000]\n\nprint(train.shape)\n\nfor i, c in zip(range(5,10), col_name):\n    train.plot.scatter(ax=axes[i], x=c, y='SalePrice', sharey=True, colorbar=False, c='b')","c7331c6a":"train_data.drop('Id', inplace=True, axis=1)\ntrain_data.head()","1e145474":"test_data.drop('Id', inplace=True, axis=1)\ntest_data.head()","9700d5f2":"corr = train_data.corr()\nhighest_correlations = corr.index[abs(corr['SalePrice']) > 0.5]\ntrain_data[highest_correlations]","29e15fd5":"quality_pivot = train.pivot_table(index='OverallQual',\n                                  values='SalePrice', aggfunc=np.median)\nquality_pivot.plot(kind='bar', color='blue')\nplt.xlabel('Overall Quality')\nplt.ylabel('Median Sale Price')\nplt.xticks(rotation=0)\nplt.show()","6e4d9a47":"# Histogram and normal probability plot\nfrom scipy import stats\nsns.distplot(train_data['SalePrice'], fit = norm)\nfig = plt.figure()\nres = stats.probplot(train_data['SalePrice'],plot = plt)","7b85a714":"categoricals = train.select_dtypes(exclude=[np.number])\ncategoricals.describe()","b62bfa67":"n = categoricals\nfor c in n.columns:\n    print('{:<14}'.format(c), train[c].unique())","36669686":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\ncols = ('MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',\n       'SaleType', 'SaleCondition')\n\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    if train_data[c].notnull().all():\n      lbl = LabelEncoder() \n      lbl.fit(list(train_data[c].values)) \n      train_data[c] = lbl.transform(list(train_data[c].values))\n      lbl.fit(list(test_data[c].values)) \n      test_data[c] = lbl.transform(list(test_data[c].values))","2e94cd94":"train_data","30650066":"test_data","1f5eb7e9":"features # Numeric features","f0709071":"def detect_and_remove_outliers(inline_delete= True):\n    global train_data\n    outliers = []\n    cnt = 0\n    min_percentile = 0.001\n    max_percentile = 0.999\n    rows = int(np.ceil(len(features)\/2))\n    for row in range (0, rows):\n      for col in range (0, 2):\n          # df_outliers = outlier_detection_using_percentile(features[cnt])\n          # Outlier detection using percentile\n          min_thresold, max_thresold = train_data[features[cnt]].quantile([min_percentile, max_percentile])\n          df_outliers = train_data[(train_data[features[cnt]] < min_thresold) | (train_data[features[cnt]] > max_thresold)]\n\n          # Updaing list of outliers\n          outliers = outliers + df_outliers.index.tolist()\n\n          if inline_delete: \n              # Drop the outliers inline\n              train_data = train_data.drop(df_outliers.index.tolist())\n              train_data.reset_index(drop = True, inplace = True)\n          cnt = cnt + 1\n          if cnt >= len(features):\n              break\n    unique_outliers= list(set(outliers))\n    \n    if inline_delete == False: \n        # Drop the unique outliers from final list\n        train_data = train_data.drop(unique_outliers)\n        train_data.reset_index(drop = True, inplace = True)\n              \ndetect_and_remove_outliers(inline_delete= False)\ntrain_data","52b874f8":"total = train_data.isnull().sum().sort_values(ascending=False)\npercent = (train_data.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nnull_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nnull_data.head(20)","f6e3f948":"train_data = train_data.dropna(thresh=0.7*len(train_data), axis=1)\ntest_data = test_data.dropna(thresh=0.7*len(test_data), axis=1)\n\ntrain_data = train_data.fillna(train_data.mean())\ntest_data = test_data.fillna(test_data.mean())","d7ceda45":"train_data.head()","83e9a53a":"test_data.head()","695adfba":"remaining = [c for c in train_data.columns if train_data[c].isnull().any()]\nfor c in remaining:\n      lbl = LabelEncoder() \n      lbl.fit(list(train_data[c].values)) \n      train_data[c] = lbl.transform(list(train_data[c].values))\n      lbl.fit(list(test_data[c].values)) \n      test_data[c] = lbl.transform(list(test_data[c].values))","41bba107":"train_data.head()","dadda212":"test_data.head()","62611bcc":"from statsmodels.stats.outliers_influence import variance_inflation_factor \nfrom statsmodels.tools.tools import add_constant\n\ndef get_highest_vif_feature(df, thresh=5):\n    '''\n      Calculates VIF each feature in a pandas dataframe\n    A constant must be added to variance_inflation_factor or the results will be incorrect\n\n    :param df: the pandas dataframe containing only the predictor features, not the response variable\n    :param thresh: the max VIF value before the feature is removed from the dataframe\n    :return: dataframe with features removed\n    '''\n   \n    const = add_constant(df)\n    print(f'Shape of data after adding const column: {const.shape}')\n    cols = const.columns\n    \n    # Calculating VIF for each feature\n    vif_df = pd.Series([ (variance_inflation_factor(const.values, i)) for i in range(const.shape[1]) ], index= const.columns).to_frame()\n    \n    vif_df = vif_df.sort_values(by=0, ascending=False).rename(columns={0: 'VIF'})\n    vif_df = vif_df.drop('const')\n    vif_df = vif_df[vif_df['VIF'] > thresh]\n\n    if vif_df.empty:\n        print('DataFrame is empty!')\n        return None\n    else:\n        print(f'\\nFeatures above VIF threshold: {vif_df.to_dict()}')       \n        # Feature with highest VIF value\n        return list(vif_df.index)[0]\n        print(f'Lets delete the feature with highest VIF value: {list(vif_df.index)[0]}')\n\n\n# Selecting only numeric features\nprint(f'Shape of input data: {train_data.shape}')\nnumeric_feats = train_data.dtypes[train_data.dtypes != \"object\"].index\nprint(f\"Calculating VIF for {len(numeric_feats)} numerical features\")\n\ndf_numeric = train_data[numeric_feats]\nprint(f'Shape of df_numeric: {df_numeric.shape}')\n    \nfeature_to_drop = None\nfeature_to_drop_list = []\nwhile True:\n    feature_to_drop = get_highest_vif_feature(df_numeric, thresh=5)\n    print(f'feature_to_drop: {feature_to_drop}')\n    if feature_to_drop is None:\n        print('No more features to drop!')\n        break\n    else:\n        feature_to_drop_list.append(feature_to_drop)\n        df_numeric = df_numeric.drop(feature_to_drop, axis=1)\n        print(f'Feature {feature_to_drop} droped from df_numeric')\n\nprint(f'\\nfeature_to_drop_list: {feature_to_drop_list}')","3982fe2e":"print(f'Shape of training data= {train_data.shape}')\ntrain_data = train_data.drop(['LowQualFinSF'], axis= 1) # Default drop axis is 0 i.e. rows \ntrain_data.reset_index(drop = True, inplace = True)\nprint(f'Shape of training data= {train_data.shape}')","60d34cba":"#Lets check the count of numerical and categorical features\ncat_feats = train_data.dtypes[train_data.dtypes == \"object\"].index\nnumeric_feats = train_data.dtypes[train_data.dtypes != \"object\"].index\nprint(f\"Number of categorical features: {len(cat_feats)}, Numerical features: {len(numeric_feats)}\")\n\nskew_features = train_data[numeric_feats].apply(lambda x: stats.skew(x)).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew': skew_features})\n\nprint(f'Skew in numerical features. Shape of skewness: {skewness.shape}')\nskewness.head(10)\n\n# todo add histo and probability plot of skewed features","cd8727b0":"train_data['TotalSF'] = train_data['TotalBsmtSF'] + train_data['1stFlrSF'] + train_data['2ndFlrSF']\ntrain_data['TotalSF1'] = train_data['BsmtFinSF1'] + train_data['BsmtFinSF2'] + train_data['1stFlrSF'] +train_data['2ndFlrSF']\n\ntrain_data['YrBltAndRemod']= train_data['YearBuilt'] + train_data['YearRemodAdd']\n\ntrain_data['TotalBathrooms'] = (train_data['FullBath'] + (0.5 * train_data['HalfBath']) +\n                               train_data['BsmtFullBath'] + (0.5 * train_data['BsmtHalfBath']))\n\ntrain_data['TotalPorchSF'] = (train_data['OpenPorchSF'] + train_data['3SsnPorch'] +\n                              train_data['EnclosedPorch'] +train_data['ScreenPorch'] +\n                              train_data['WoodDeckSF'])\n\nprint(f'Shape train_data: {train_data.shape}')","6be228c0":"test_data['TotalSF'] = test_data['TotalBsmtSF'] + test_data['1stFlrSF'] + test_data['2ndFlrSF']\ntest_data['TotalSF1'] = test_data['BsmtFinSF1'] +test_data['BsmtFinSF2'] + test_data['1stFlrSF'] +test_data['2ndFlrSF']\n\ntest_data['YrBltAndRemod']= test_data['YearBuilt'] + test_data['YearRemodAdd']\n\ntest_data['TotalBathrooms'] = (test_data['FullBath'] + (0.5 * test_data['HalfBath']) +\n                              test_data['BsmtFullBath'] + (0.5 * test_data['BsmtHalfBath']))\n\ntest_data['TotalPorchSF'] = (test_data['OpenPorchSF'] + test_data['3SsnPorch'] +\n                             test_data['EnclosedPorch'] +test_data['ScreenPorch'] +\n                            test_data['WoodDeckSF'])\n\nprint(f'Shape test_data: {test_data.shape}')","ae3acf77":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler","49ddc762":"minMaxScaler = MinMaxScaler()\nY_train = train_data['SalePrice']\ntrain_data = train_data.drop(['SalePrice'], axis=1)\ntrain_data = minMaxScaler.fit_transform(train_data)","7e07578a":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(train_data, Y_train, test_size=0.2, random_state=42)","5f785e69":"x_train.shape","16a3f0b1":"x_test.shape","ffdea865":"y_train.shape","c894cb3e":"y_test.shape","a1777f0c":"x_train","1dd3280d":"x_test","b1136cae":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nimport statsmodels.api as sm\n\n\n# define models to test:\nbase_models = [(\"LinearRegression\",      LinearRegression()),                     #LinearRegression\n               (\"Poly\",      PolynomialFeatures(degree=2, include_bias=False)),]","7d40f285":"models_data = {'intercept':{}, 'r_sq':{}}","59e9bd80":"model_name = 'LinearRegression'\nmodel = LinearRegression()\nmodel.fit(x_train, y_train)\nr_sq = model.score(x_train, y_train)\n\n#models_data['coefficients'][model_name] = model.coef_\nmodels_data['intercept'][model_name] = model.intercept_\nmodels_data['r_sq'][model_name] = r_sq\nprint(\"Accuracy of Linear Regression:\", r_sq * 100, \"%\")","f04e9c0f":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import make_scorer, r2_score\nfrom sklearn.model_selection import GridSearchCV, ShuffleSplit\n\ncv_sets = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 0)\nregressor = DecisionTreeRegressor()\nparams = {'max_depth': [1,2,3,4,5,6,7,8,9,10], 'max_features': [10, 12, 14, 16, 18, 20, 22, 24], 'splitter': ['best', 'random'], 'criterion': ['mse', 'friedman_mse', 'mae']}\n\ndef func_metric(y, theta):\n  return r2_score(y, theta)\n\n# Estimate the parameter(s) with Grid Search and obtain the best model\nscoring_fnc = make_scorer(func_metric)\ngrid = GridSearchCV(estimator=regressor, param_grid=params, scoring=scoring_fnc, cv=cv_sets)\ngrid = grid.fit(x_train, y_train)\nbest_model = grid.best_estimator_","aaca1558":"best_model","fa9631bb":"best_model.fit(x_train, y_train)\nbest_model.predict(x_train)\nscore = best_model.score(x_train, y_train)\nprint(\"Accuracy of Decision Tree Regressor: \" + str(score * 100) + \"%\")","44acbcf7":"#### Data Normalization","f413802f":"**Description:** The train and test files contains rows in which each row represents all of the features for each home. Each row is represented as a dictionary so we can conclude that the train and test files are a list of dictionaries. The key for each dictionary is the id of the home and the value represents all of the features that are observed for each home.","a01dda1f":"### *1. Compiling the data*","ce75e685":"And these are the transformed datasets.","e909b430":"* Since area related features are very important to determine the house price, we will create a new feature by the name 'TotalSF' by adding 'TotalBsmtSF', '1stFlrSF' and '2ndFlrSF'.\n* Similarly we will create one more new feature by name 'TotalSF1' by adding 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', '1stFlrSF' and '2ndFlrSF'. Here 'BsmtFinSF1' and 'BsmtFinSF2' represent finished square feet of all area, that's why we are creating a separate feature using it.\n* Create new feature 'YrBltAndRemod' by adding 'YearBuilt' and 'YearRemodAdd'\n* Create new feature 'TotalBathrooms' by adding all the bathrooms in the house.\n* Create new feature 'TotalPorchSF' by adding all porch area.","395e86d6":"## 2.2 Problem Formulation","97a80326":"#### 1. Split the Data\n\nWe will split the train data into a train subset and test subset, so we can evaluate and test our models.","376f6373":"We will remove the columns that have more than 70% of missing data.","50670067":"### *Introduction*\nHousing prices reflect important information of the state of our economy. When housing prices rise, it corresponds to an increase in spending and borrowing from both individuals and businesses which will help improve the economy. Forming an accurate predicion of the value of a house is crucial when, for example, rennovating a home can significantly improve the value of the home relative to the cost of said reconnvation. This corresponds to a lucrative investment for the homeowner which leads to more spending.\n\nAs such, the problem we seek to address is one that deals with predicting the value of a house. We intend to use advanced regression techniques combined with extensive feature engineering to address this problem. By devising techniques that lead to accurate predictions, we can identify what qualities drastically impact the value of a home.","390bcbc3":"###### **Structure:** \n\nThe aforementioned analyses above illustrate the structure of our data. Both the train and test datasets yield a recursive format in which the keys of the top level json object represents the unique id of each home. The next level represents the final level of the recursive structure in which the keys of the next level denote the explanatory features that influence the value of the home.","744f8d42":"##### *Downloading the Data*\n\nWe want to retrieve the data so we can thoroughly analyze the architecture of the data.\n\nEach file is represented as an empty directory and stores each row in the csv file into each directory respectively. ","7d16e6b1":"Now we need to select a model that will best evaluate our data. Two of the models we have selected is LinearRegression and Decision Tree Regressor.\n\n**Linear Regression:** Linear Regression can serve as an effective model because it models the relationship between two variables by assuming there is a strong linear relationship between the two variables. In the case of SalePrice, we deduced earlier that there were many features in our that had a strong linear relationship with SalePrice. As such, we believe that Linear Regression can be very useful for modeling this problem.\n\n**Decision Tree Regressor:** Decision Tree Regressor was also considered for this problem because it evaluates the data by asking a series of questions related to the features within the dataset to predict the target value by splitting the data into smaller subsets. Because there are numerous features of a home and we have eliminated the outliers within the data, we believe that this model can perform accurate decisions that can predict the SalePrice of our data.","974d76fd":"#### Getting the Data\n\nFirst, we upload the data as done previously in Assignment 2 and access the directory in which the data was uploaded so it can be retrieved and compiled. This was done by mounting our shared google drive and uploading the specific folder containing the train and test datasets to the google drive.","bb536c3f":"Transforming and engineering features Here we are using label encoding. Label encoding refers to transforming the word labels into numerical form so that the algorithms can understand how to operate on them.","343583bc":"**Linear Regression:** This fits a linear model with specific coefficients to minimize the residual sum of squares between the estimated targets and the observed targets within the dataset. This is called Ordinary Least Squares. It is a statistical technique used to measure the variance within the dataset.\n\n**Decision Tree Regressor:** This model normally uses mean squared error whenever it decides to split a node into two or more child nodes. This means that a node is evaluated as the average of the squared difference between the estimated values and the observed values. However, as we observe later on, the model in question actually uses friedman mean squared error which measures the impurity of a node before it is removed.","1919d568":"### B. Features \n\nWe will extract necessary features within the train data and eliminate any redundant data. This ensures that our model will perform faster and will only extrapolate the saleprice based on relevant data.","727d604f":"##### **Decision Tree Regressor**\n\nWe implement the Decision Tree Regressor and train it on our train subset. ","40acfbfc":"The granularity of the data is observed by analyzing the correlation between every feature in relation to the sale price. This way, we extract the most impactful features for our data visualizations.","7a1186f8":"After extracting the most relevant features, we can plot all of the pairs of features that have the highest correlation between eachother.","b0db87c9":"###### **Faithfulness:**\n","c053394d":"##### **Model Evaluation**","02b804c9":"To analyze the data in greater detail, we must visualize the data. We do so by creating a data frame that encapsulates data from the train dataset.","47ebf314":"We want to delete the outliers in the data so our dataset will be more representative of the problem. Outliers aren't appropriate for predicting the saleprice because they fall outside of the norm.","04b409c5":"#### 1. Missing Data\n\nWe first analyze the missing data within our dataset, so that we can filter the redundancy out.","33335b42":"### *4. Sampling*\n\nCompared to the data population, we believe that was used was cluster sampling collected through administrative data in order to retrieve the data. The data was derived from an administrative source with some variables stripped away for usability. ","c74d4b2c":"## 1.2 Data Transformation and EDA","eb0caccf":"The train and test files are then converted to a json file such that the data can be thoroughly analyzed in the *Exploring the data* section","65790a5f":"Before we even begin to load the data, it often helps to analyze the high-level structure:\n\n*How much data do I have?*\n\n*How is it formatted?*","d892c89d":"#### Adding New Features","5fa9b30f":"##### C. *How big is the data?*\n","a58bc14e":"##### **Loss Function**","0a4f42d3":"**Data Population**\n\nThe Kaggle regression problem represents a sample of the Data Population. The Data Population represents all homes throughout the world.","9ce214f8":"##### *D. Data Visualization*\n","3021d2ad":"### *3. Observations*\n\nThe data we obtained is representative of the problem because we can infer which features are most relevant to analyze and only a small proportion of the data contains outliers. According to our visualizations, we can observe the linear relationship between two different features and how they relate to the target value i.e the sale price of homes that we are trying to predict.\n\nTo accurately predict the value of a home using this dataset, we would assume that the variables that influence the final sale price remain true across different regions. If that assumption holds true, we can extrapolate the data and results collected and apply it to different datasets involving sales of residential property.","f6b55a33":"##### *B. Structure*\n","ae14ec6b":"The train and test files both represent dictionary containing the data of each house encoded in the datasets.","5c11d478":"We import the train dataset from train.csv to create our train dataframe.","a631afac":"###### **Temporality:**\n\nEach home was sold between 2006 to 2010. The earliest home was built in 1872 with the newest home being built in 2010. The earliest rennovation was made in 1950 with the latest being 2010. As such, some homes within the dataset represent the value of a modern home within Ames, Iowa whereas some do not as some homes were built early and were either not rennovated, but rennovated many years ago. Naturally homes that were built\/rennovated very recently would be homes we would naturally consider above other homes.","dc0fe5de":"#### 2. Model Selection\n\nNow we need to select models that we believe would be useful in predicting our data.","5ab231b1":"The contents of the json files are shown below:","0009e3e9":"The graph is skewed, so we must address it by transforming the data using logarithm.","8bd56b5d":"We will check for non-numeric features to encode the data.","7bb4a317":"### C. Modeling","ae7f0056":"**Structure**\n\nThe structure of data we used is the rectangular data.","ba79f0dd":"And then we can visualize some of the outliers.","4cb29319":"First, we analyze the structure of our data. We do this by delving deep into the recursive structure of each file and analyze the keys of the json objects at the top level.\n\n**Note**: This represents the the unique identifier for each home.","2a247d41":"## 1.1 Stage-1","f5206958":"#### 2. Multicollinear Features\n\nNext, we will remove multicollinear features i.e features that have the highest correlations between eachother by analyzing the VIF score. \n\n**Note:** The VIF score is a metric that is used to measure the strength of correlation between 2 independent variables.","d44f46ac":"We often like to start the analysis by getting a rough estimate of the size of the data. This will help inform the tools we use and how we view the data. If it is relatively small, we might use a text editor or a spreadsheet to look at the data. If it is larger, we might jump to more programmatic exploration or even use distributed computing tools.\n\nHowever here we will use python tools to probe the file.","f24ce98c":"### *2. Data Wrangling*","be04c85c":"We first observe the faithfulness of the dataset by filtering out all of the outliers within the dataset. Focusing on outliers, defined by Gladwell as people who do not fit into our normal understanding of achievement. Outliers deals with exceptional people, especially those who are smart, rich, and successful, and those who operate at the extreme outer edge of what is statistically plausible. An outlier is a data point that is distant from other similar points. They may be due to variability in the measurement or may indicate experimental errors. If possible, outliers should be excluded from the data set. We'll do a quick analysis through the standard deviation of 'SalePrice' and a set of scatter plots.","e4111474":"### *A. Transformations*\n\nWe must prepare and transform the data so that it can be trained using a regression model. Not only would this eliminate redundant data, but it will also transform the data into a set that only contains numerical data. Otherwise, it cannot be processed by the model.","9dfa257e":"## 2.3 Data ","6d74df2d":"**References:** \n\nhttps:\/\/www.kaggle.com\/satishgunjal\/advanced-reg-techniques-linear-models-top-6\nhttps:\/\/www.kaggle.com\/akuei0419\/housepricepredict02\nhttps:\/\/www.kaggle.com\/nikkisharma536\/house-prediction-dealing-with-outlier","4a41f3a9":"\n\nNow that we have obtained the data we want to understand its:\n\n**Structure** -- the \"shape\" of a data file\n\n**Granularity** -- how fine\/coarse is each datum\n\n**Scope** -- how (in)complete is the data\n\n**Temporality** -- how is the data situated in time\n\n**Faithfulness** -- how well does the data capture \"reality\"","b43c896d":"#### JSON","98e684a2":"We will encode the remaining columns that still retain missing data.","b87a44b6":"First, we want to read and interpret the file to analyze the file encoding.\n\n**Note:** Not understanding the encoding of the file will perturb some of the column's values and cause it to return NaN instead. As such, pandas.read_csv takes in the parameter, encoding, to ensure that file encoding is taken into account when retrieving the data and return the correct results.","cfa6f52a":"##### **Linear Regression Model**\n\nWe implement the linear regression model and train it on our train subset.","11040ec5":"Then we do the same for the keys at the next level.","fa3d9513":"**Representation**\n\nWe did initially overlook the limitations of the data. Because the data only observes homes from 2006 to 2008 in only one specific area, it is quite limited in scope and is not exactly representative of the Data Population.","c5773c5f":"# Project 2-Stage 2","3e4d563f":"###### **Scope:**\n\nThere are 2390 different records with each record in the dataset representing the sale of an individual residential property in Ames, Iowa. There were originally a total of 3970 different records within the given timeframe, but some were filtered to only include residential sales, thus only leaving us with 2390 different records to work with.","3b0fbc8f":"#### Numeric Feature Scaling","c174257b":"Normalization plays a significant role in data preprocessing. Normalization will transform the data into a matrix of values ranging from 0 to 1 without distorting any values or losing information. Some models can't operate effectively without applying some sort of normalization on the data.\n\nMinMaxScaler will be applied because it transforms the data by scaling each feature to a specific range.","e71809b1":"We need the features that have the highest correlation with the sale price.","9b27ce8b":"**Collaborators**: Nazim Zerrouki and Shrustishree Sumanth","7b66c1a9":"**References:**\n\nhttps:\/\/www.kaggle.com\/satishgunjal\/advanced-reg-techniques-linear-models-top-6\nhttps:\/\/www.kaggle.com\/nikkisharma536\/house-prediction-dealing-with-outlier\nhttps:\/\/machinelearningmastery.com\/standardscaler-and-minmaxscaler-transforms-in-python\/\nhttps:\/\/towardsdatascience.com\/machine-learning-project-predicting-boston-house-prices-with-regression-b4e47493633d","ec925a3d":"##### *A. Exploring the data*","be1ae3ff":"Because the ID attribute does not contain any useful information that can be used to predict the saleprice, we will drop the attribute.","2df550be":"In order to give every feature the same importance we perform feature scaling. There are many techniques like Min-Max Scalar, Robust Scalar etc. to do feature scaling.\nBefore we can finalize any scaling technique lets check the skewness of our numeric features. Skewness is the measure of degree of asymmetry of a distribution.\n* skewness = 0 : normally distributed.\n* skewness > 0 : more weight in the left tail of the distribution.\n* skewness < 0 : more weight in the right tail of the distribution.","54f3b972":"###### **Granularity:**\n\nWe can analyze the granularity of our data by creating data visualizations that illustrate the correlation between specific features in relation to the sale price. This way, we can create data visualizations that are most relevant to the problem.\n\nFirst, we take a look at the statistics of the sales prices of homes because the sale price is the target value we want to predict.","465d4765":"Every property in the dataset could be thoroughly observed using its correlation matrix. The data was also imported properly to ensure that no data is missing. \n\n**Note:** However as we observed, there is a very small proportion of outliers in the dataset when observing features with numerical data. Thus, there is only a very small proportion of data that is unreliable.","9aa9017e":"# Project 2-Stage 1\n\nProject 2 was selected which deals with analyzing house prices using advanced regression techniques. We will be using techniques learned from Assignments 1 and 2 such as working with pandas DataFrame API in order to complete stage 1 of the project.\n\n**Objective:** Transform the data into something that can be thoroughly analyzed and extract relevant results from i.e Data Wrangling.","2dffa680":"**Accounts:**  nazimz@uw.edu and ssumanth@uw.edu","2f8ac0e4":"We analyze the distribution in the sale price of all homes in the train dataset.","78f9cb7d":"We import the test dataset from test.csv to create our test dataframe.","7df2d5ad":"### *Data Population*\n\nThe dataset provided by the Kaggle competition via the data_description.txt file contains a list of homes found in Ames, Iowes which was created by Dean De Cock. The dataset is comprised of 2390 different sales of residential properties from 2006 to 2010. The sale price of the home is the target value which is influenced by 79 explanatory features. Each record denotes the details of a particular house involved in a sale of residential property. "}}