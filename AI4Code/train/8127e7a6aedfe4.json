{"cell_type":{"2f7ff2c1":"code","92856f72":"code","1492d6d8":"code","950c4123":"code","73acd1cf":"code","553e0eb6":"code","83ce4387":"code","806c52b8":"code","9246b001":"code","72400bad":"code","abbfda29":"code","01e1c94f":"code","a222e286":"code","a5b57751":"code","b96305f8":"code","2a0dff68":"code","c9fca134":"code","16e6b4b6":"code","8eb43a97":"code","8453f408":"code","888308e8":"code","73a2d062":"code","09203d34":"code","62416471":"code","2f82103d":"markdown","8d0cb129":"markdown","af85d916":"markdown","6839cb6d":"markdown"},"source":{"2f7ff2c1":"import warnings\nwarnings.filterwarnings('ignore')","92856f72":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","1492d6d8":"Bill_Auth_data = pd.read_csv('..\/input\/bill_authentication\/bill_authentication.csv')\nBill_Auth_data.head()","950c4123":"Bill_Auth_data.info()","73acd1cf":"Bill_Auth_data = Bill_Auth_data.sample(frac=1.0).reset_index(drop=True)\nBill_Auth_data.head()","553e0eb6":"Bill_Auth_data.describe()","83ce4387":"Bill_Auth_data.iloc[:,0:-1].plot(kind='hist',subplots=True,layout=(2,2),figsize=(15,6));","806c52b8":"Bill_Auth_data.iloc[:,0:-1].plot(kind='box',subplots=True,layout=(2,2),figsize=(15,6));","9246b001":"sns.pairplot(data=Bill_Auth_data,hue='Class');","72400bad":"sns.heatmap(Bill_Auth_data.corr(),annot=True);","abbfda29":"from sklearn.preprocessing import RobustScaler\n\nX = Bill_Auth_data.drop('Class',axis=1)\nY = Bill_Auth_data['Class']\n\nRS = RobustScaler()\nX = RS.fit_transform(X)\n","01e1c94f":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2,random_state=10)\n","a222e286":"from sklearn.ensemble import RandomForestClassifier\n\nRFC = RandomForestClassifier()\nRFC.fit(X_train,Y_train)\n\nY_pred = RFC.predict(X_test)","a5b57751":"from sklearn import metrics\n\nprint(metrics.accuracy_score(Y_test,Y_pred))\nprint(metrics.confusion_matrix(Y_test,Y_pred))\nprint(metrics.classification_report(Y_test,Y_pred))","b96305f8":"accuracy_list =[]\n\nfor n_est in range(50,201,10):\n  RFC = RandomForestClassifier(n_estimators=n_est)\n  RFC.fit(X_train,Y_train)\n  accuracy_list.append(round(metrics.accuracy_score(Y_test,RFC.predict(X_test))*100,2))\n\n","2a0dff68":"plt.figure(figsize=(12, 6))  \nplt.plot(range(50, 201, 10), accuracy_list, color='red', \n         linestyle='dashed', marker='o', markerfacecolor='blue', \n         markersize=10)\nplt.xticks(range(50, 201, 10))\nplt.xlabel('n_estimator Value')\nplt.ylabel('Accuracy');","c9fca134":"Petrol_Cons_data = pd.read_csv('..\/input\/petrol-consumption\/petrol_consumption.csv')\nPetrol_Cons_data.head()","16e6b4b6":"Petrol_Cons_data.info()","8eb43a97":"Petrol_Cons_data.describe()","8453f408":"sns.heatmap(Petrol_Cons_data.corr(),annot=True);","888308e8":"X = Petrol_Cons_data.drop('Petrol_Consumption', axis=1)  \nY = Petrol_Cons_data['Petrol_Consumption']  \n\n  \nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=10)  \n","73a2d062":"from sklearn.ensemble import RandomForestRegressor\nRFR=RandomForestRegressor(n_estimators=90,random_state=0)\nRFR.fit(X_train,Y_train)\n\nY_pred=RFR.predict(X_test)\n","09203d34":"from sklearn import metrics\nimport numpy as np\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_pred))  \nprint('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))","62416471":"Petrol_Cons_data.Petrol_Consumption.mean()\/10","2f82103d":"# Random Forest","8d0cb129":"# Random Forest Regressor","af85d916":"Rnadom Forest is a made from collection of decision trees. \n\n1> Pick N random records from the dataset, **call it Bootstraped Dataset**. <br>\n2> Build a decision tree based on these N records. <br>\n3> Choose the number of trees you want in your algorithm \n   and repeat steps 1 and 2. <br>\n4> In case of a **regression problem**, <font color='green'> for a new record, each tree in the forest predicts a value for Y (output). The final value can be calculated by taking the average of all the values predicted by all the trees in forest.<\/font> <br> \nOr, in case of a **classification problem**, <font color='green'> each tree in the forest predicts the category to which the new record belongs. Finally, the new record is assigned to the category that wins the majority vote. <\/font><br>\n5> Data that is left out in bootstrapped data is called as 'Out-Of-Bag' data.\n\n![](https:\/\/drive.google.com\/uc?export=view&id=1T9eWQNh1xTuGgFv1HpKO3TWyJsyOPRrh)\n\n\n\n","6839cb6d":"# Random Forest Classifier"}}