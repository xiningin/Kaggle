{"cell_type":{"56939f64":"code","b73ae443":"code","a9aa0699":"code","63cf4719":"code","9d0ac8ad":"code","e72b3e57":"code","0768844d":"code","142bd1a7":"code","91c2375d":"code","853b8097":"code","757aa0ea":"code","af5bce36":"code","929f87a5":"code","66928587":"code","039dfdca":"code","82da220c":"code","28588af9":"code","9cd039bb":"code","31ff30c9":"markdown"},"source":{"56939f64":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b73ae443":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import GridSearchCV, cross_val_score","a9aa0699":"df=pd.read_csv(\"\/kaggle\/input\/hitters\/Hitters.csv\")\ndf.head()","63cf4719":"# Numerik ve kategorik de\u011fi\u015fkenler (Numeric and categorical variables)\n\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","9d0ac8ad":"# Ayk\u0131r\u0131 De\u011ferler Analizi (Outlier Analysis)\n\ndef outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\nfor col in num_cols:\n    print(col, check_outlier(df, col))\n    \nfor col in num_cols:\n    if check_outlier(df, col):\n        replace_with_thresholds(df, col)","e72b3e57":" # Eksik De\u011fer Analizi (Missing Values Analysis)\n    \ndf.isnull().sum()","0768844d":"df.dropna(inplace=True)","142bd1a7":"# Kategorik de\u011fi\u015fkenlerin label encoder ile say\u0131salla\u015ft\u0131r\u0131lmas\u0131 (Digitizing categorical variables with label encoder)\n\ndef label_encoder(dataframe, binary_col):\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe\n\n\nbinary_cols = [col for col in df.columns if df[col].dtype not in [int, float]\n               and df[col].nunique() == 2]\n\nfor col in binary_cols:\n    df = label_encoder(df, col)","91c2375d":"df.head()","853b8097":"# \u00d6zellik \u00c7\u0131kar\u0131m\u0131 (Feature Extraction)\n\ndf[\"New_best\"] = df[\"Hits\"] * df[\"CRBI\"] * df[\"PutOuts\"]\ndf[\"New_hit_success\"] = df[\"CHits\"] \/ df[\"CAtBat\"]\ndf[\"New_Asis-Div\"] = df[\"Division\"] * df[\"CRuns\"]\ndf[\"New_CRun-CtBt\"] = df[\"CRuns\"] * df[\"CAtBat\"]\ndf[\"New_Run-At\"] = (df[\"Runs\"]) * df[\"AtBat\"]\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","757aa0ea":"# RobustScaler (Feature Scaling)\n\nnew_num_cols = [col for col in num_cols if col not in [\"Salary\", \"Years\"]]\n\nrs = RobustScaler()\ndf[new_num_cols] = rs.fit_transform(df[new_num_cols])\n\ndf.head()","af5bce36":"# Ba\u011f\u0131ml\u0131 ve ba\u011f\u0131ms\u0131z de\u011fi\u015fkenler (Dependent and independent variables)\n\nX = df.drop('Salary', axis=1)\ny = df[[\"Salary\"]]\ny = y.values.ravel()","929f87a5":"# Hiperparametre \u00f6ncesi modellerin RMSE de\u011ferleri (RMSE values of pre-hyperparameter models)\n\nmodels = [('LR', LinearRegression()),\n          (\"Ridge\", Ridge()),\n          (\"Lasso\", Lasso()),\n          (\"ElasticNet\", ElasticNet()),\n          ('KNN', KNeighborsRegressor()),\n          ('CART', DecisionTreeRegressor()),\n          ('RF', RandomForestRegressor()),\n          ('SVR', SVR()),\n          ('GBM', GradientBoostingRegressor()),\n          (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n          (\"LightGBM\", LGBMRegressor())\n          ]\n\nfor name, regressor in models:\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")","66928587":"# Otomatik Hiperparametre Optimizasyonu (Automated Hyperparameter Optimization)\n\nrf_params = {\"max_depth\": [3,5,7 ,None],\n             \"max_features\": [2, 5, 7, \"auto\"],\n             \"min_samples_split\": [2, 8, 15],\n             \"n_estimators\": [100, 200, 500]}\n\nlightgbm_params = {\"learning_rate\": [0.01, 0.1, 0.15],\n                   \"n_estimators\": [490, 500, 510],\n                   \"colsample_bytree\": [0.4, 0.5, 0.6]}\n\ngbm_params = {\"learning_rate\": [0.001, 0.01, 0.1],\n              \"max_depth\": [3,5, 8],\n              \"n_estimators\": [490, 500, 510],\n              \"subsample\": [0.4, 0.5, 0.6]}\n\nregressors = [(\"RF\", RandomForestRegressor(warm_start=True), rf_params),\n              ('LightGBM', LGBMRegressor(), lightgbm_params),\n              (\"GBM\", GradientBoostingRegressor(), gbm_params)]\n\nbest_models = {}\n\nfor name, regressor, params in regressors:\n    print(f\"########## {name} ##########\")\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=3, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n\n    gs_best = GridSearchCV(regressor, params, cv=3, n_jobs=-1, verbose=False).fit(X, y)\n\n    final_model = regressor.set_params(**gs_best.best_params_)\n    rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=3, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE (After): {round(rmse, 4)} ({name}) \")\n\n    print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n\n    best_models[name] = final_model","039dfdca":"# Stacking & Ensemble Learning\n\nvoting_reg = VotingRegressor(estimators=[(\"LightGBM\", best_models[\"LightGBM\"]),\n                                         (\"GBM\", best_models[\"GBM\"]),\n                                         ('RF', best_models[\"RF\"])]).fit(X, y)\n\nnp.mean(np.sqrt(-cross_val_score(voting_reg, X, y, cv=10, scoring=\"neg_mean_squared_error\")))","82da220c":"# Yeni Bir G\u00f6zlem \u0130\u00e7in Tahmin (Prediction for a New Observation)\n\nrandom_user = X.sample(1, random_state=45)\nvoting_reg.predict(random_user)","28588af9":"# En iyi modelin RMSE de\u011feri (Best model's score)\n\ngb_model = GradientBoostingRegressor(random_state=17)\ngb_best_params ={'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 510, 'subsample': 0.4}\n\ngb_final = gb_model.set_params(**gb_best_params).fit(X, y)\n\nnp.mean(np.sqrt(-cross_val_score(gb_final, X, y, cv=10, scoring=\"neg_mean_squared_error\")))","9cd039bb":"# \u00d6nemli De\u011fi\u015fkenler (Feature Importance)\n\ndef plot_importance(model, features, num=len(X)):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False), color=\"navy\")\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n\nplot_importance(gb_final, X)","31ff30c9":"![](https:\/\/www.turhost.com\/blog\/wp-content\/uploads\/2020\/11\/kapak-6.jpg)"}}