{"cell_type":{"0d4fd8fe":"code","7881c186":"code","2bef257d":"code","f8b07a16":"code","3ac40fd3":"code","5428c4ee":"code","8ff05587":"code","3ede0048":"code","72128700":"code","8df4ecac":"code","d49cdc95":"code","c3591551":"markdown"},"source":{"0d4fd8fe":"from __future__ import print_function\nimport keras\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.regularizers import l2\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline","7881c186":"# Defining the parameters\nbatch_size = 32\nnum_classes = 10\nepochs = 75","2bef257d":"# Splitting the data between train and test\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')","f8b07a16":"# plotting some random 10 images\n\nclass_names = ['airplane','automobile','bird','cat','deer',\n               'dog','frog','horse','ship','truck']\n\nfig = plt.figure(figsize=(8,3))\nfor i in range(num_classes):\n    ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[])\n    idx = np.where(y_train[:]==i)[0]\n    features_idx = x_train[idx,::]\n    img_num = np.random.randint(features_idx.shape[0])\n    im = (features_idx[img_num,::])\n    ax.set_title(class_names[i])\n    plt.imshow(im)\nplt.show()","3ac40fd3":"# Convert class vectors to binary class matrices.\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)","5428c4ee":"# Printing sample data\nprint(y_train[:10])","8ff05587":"model = Sequential()\nmodel.add(Conv2D(32, (3, 3), padding='same',\n                 input_shape=x_train.shape[1:]))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(128, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512,kernel_regularizer=l2(0.01)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))","3ede0048":"# summary of the model\nprint(model.summary())","72128700":"# compile\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='sgd',\n              metrics=['accuracy'])\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\n\n# Normalizing the input image\nx_train \/= 255\nx_test \/= 255\n","8df4ecac":"# Training the model\nhistory = model.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=epochs,\n              validation_data=(x_test, y_test),\n              shuffle=True)","d49cdc95":"# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.grid()\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.grid()\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","c3591551":"# **Experiment-VI:**\n\nAdd a new convolutional layer to the network."}}