{"cell_type":{"387867df":"code","6566c0d7":"code","eab75160":"code","ccb2354f":"code","6b84a7d5":"code","58fdd2b2":"code","4122578a":"code","06eeca4e":"code","3ede3a1c":"code","80f1738b":"code","ce597d6e":"code","0f6af4dd":"code","57429429":"code","96d1533c":"code","8cc1d277":"code","52ce5bc7":"code","92acb932":"code","8e5a9c5b":"code","c642b0bd":"code","149459eb":"code","56986409":"markdown","38907716":"markdown","52bb2ba3":"markdown","2de6d4df":"markdown"},"source":{"387867df":"!apt install -y build-essential swig curl\n!curl https:\/\/raw.githubusercontent.com\/automl\/auto-sklearn\/master\/requirements.txt | xargs -n 1 -L 1 pip install\n!pip install auto-sklearn","6566c0d7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport autosklearn.classification\nfrom sklearn.model_selection import train_test_split\nimport sklearn\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eab75160":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')","ccb2354f":"pid = test_data[\"PassengerId\"]","6b84a7d5":"def preprocess_titanic(data):\n    from sklearn.impute import KNNImputer\n    from sklearn.preprocessing import OrdinalEncoder\n    from sklearn.preprocessing import MinMaxScaler\n    from scipy import stats\n    \n    del data['Name']\n    del data['PassengerId']\n    \n    data = data[data[\"Embarked\"].notna()]\n    data['Cabin'][data['Cabin'].isna()] = 'NaN'\n    \n    ord_enc = OrdinalEncoder()\n    ord_enc = ord_enc.fit(data[['Ticket', 'Cabin']])\n    data[['Ticket', 'Cabin']] = ord_enc.transform(data[['Ticket', 'Cabin']])\n    \n    data = pd.get_dummies(data, columns=['Sex', 'Embarked'])\n    \n    knn_imputer = KNNImputer(n_neighbors=5)\n    data = pd.DataFrame(knn_imputer.fit_transform(data), columns=data.columns)\n    \n    data = data[(np.abs(stats.zscore(data['Fare'])) < 2.4)]\n    \n    variance = np.var(data)\n    highvar_cols = [col for col in data.columns if variance[col] > 2]\n\n    minmax_scal = MinMaxScaler(feature_range=(0.0,1.0))\n    minmax_scal = minmax_scal.fit(data[highvar_cols])\n    data[highvar_cols] = minmax_scal.transform(data[highvar_cols])\n    return data","58fdd2b2":"def preprocess_titanic_test(data):\n    from sklearn.impute import KNNImputer\n    from sklearn.preprocessing import OrdinalEncoder\n    from sklearn.preprocessing import MinMaxScaler\n    from scipy import stats\n    \n    del data['Name']\n    del data['PassengerId']\n    \n    data['Cabin'][data['Cabin'].isna()] = 'NaN'\n    \n    ord_enc = OrdinalEncoder()\n    ord_enc = ord_enc.fit(data[['Ticket', 'Cabin']])\n    data[['Ticket', 'Cabin']] = ord_enc.transform(data[['Ticket', 'Cabin']])\n    \n    data = pd.get_dummies(data, columns=['Sex', 'Embarked'])\n    \n    knn_imputer = KNNImputer(n_neighbors=5)\n    data = pd.DataFrame(knn_imputer.fit_transform(data), columns=data.columns)\n        \n    variance = np.var(data)\n    highvar_cols = [col for col in data.columns if variance[col] > 2]\n\n    minmax_scal = MinMaxScaler(feature_range=(0.0,1.0))\n    minmax_scal = minmax_scal.fit(data[highvar_cols])\n    data[highvar_cols] = minmax_scal.transform(data[highvar_cols])\n    return data","4122578a":"train_data = preprocess_titanic(train_data)\ntest_data = preprocess_titanic_test(test_data)","06eeca4e":"train_data.head()","3ede3a1c":"test_data.head()","80f1738b":"X = train_data\ny = X['Survived']\ndel X['Survived']","ce597d6e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","0f6af4dd":"automl = autosklearn.classification.AutoSklearnClassifier(\n    time_left_for_this_task=600,\n    per_run_time_limit=60,\n    n_jobs=4,\n    seed=42\n)","57429429":"automl.fit(X_train, y_train)","96d1533c":"y_hat = automl.predict(X_test)\nprint(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_hat))","8cc1d277":"print(automl.show_models())","52ce5bc7":"predictions = automl.predict(test_data)","92acb932":"print(len(pid), len(predictions))\npreds = pd.DataFrame({\"PassengerId\": pid, \"Survived\":predictions})","8e5a9c5b":"preds['Survived'] = preds['Survived'].astype(int)\npreds.head()","c642b0bd":"preds[preds['Survived']!=0].head()","149459eb":"preds.to_csv(\"titanic_submission.csv\", index=False)","56986409":"# Model training and validation","38907716":"# Preprocessing\n\nfrom: https:\/\/www.kaggle.com\/mateus558\/titanic-preprocessing","52bb2ba3":"# Libraries importing and configuration","2de6d4df":"# Saving results for submission"}}