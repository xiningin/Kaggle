{"cell_type":{"f8f58dea":"code","062591cf":"code","131ac518":"code","681e4639":"code","76625cf7":"code","9d244e90":"code","1fe96778":"code","3a4e635e":"code","eacf78ea":"code","2a6f0a68":"code","459700b0":"code","3a1eaf8b":"code","3633dee5":"code","33acea58":"code","9a96bf8c":"code","6df2c8dc":"code","7edeb9e8":"code","7389ccd6":"code","809dd999":"code","81b25403":"code","9fd6b245":"code","7f785fc2":"code","cfb20079":"code","b3621207":"code","e425a1b8":"code","3f4b31ba":"code","93a6c6db":"code","fede944e":"code","6f3b0028":"code","9db83015":"code","968919c9":"code","e1c3fb2a":"code","552be7d8":"code","173d9e8d":"code","7c1c245a":"code","44f72ff9":"code","0fddd43c":"code","000de882":"code","302da1c1":"code","bbedb4bc":"code","63d29a92":"code","08fd36cd":"code","94bda48c":"code","f733dd9f":"code","251c53df":"code","ac146de7":"code","9844f075":"code","3e836f40":"code","0d01f46b":"code","8e4d2bf4":"code","bfc42c10":"code","fbafbcbc":"code","a91ad577":"code","498415d7":"code","f64d8de3":"code","b9156cdc":"code","9915d857":"code","cb05b08c":"code","980a701f":"code","74e91507":"code","145552f0":"code","23379737":"code","df40c3de":"code","0b15bcef":"code","6af9b32e":"code","564998ef":"code","cf91b739":"code","da120f9c":"code","0f4b77bc":"code","e8c618c8":"code","34e4560a":"code","ad568bc7":"markdown","89d34dd2":"markdown","41a880ab":"markdown","5c029d15":"markdown","32fc590e":"markdown","daa4af1d":"markdown","6f49628d":"markdown","5af7ee19":"markdown","ce85fb47":"markdown","d2c23144":"markdown","5abb971e":"markdown","5a039752":"markdown","49b075a9":"markdown","d5871a6b":"markdown","14083e46":"markdown","8c792164":"markdown","25bb361a":"markdown","6bc9f0f5":"markdown","ea9af55f":"markdown","6559f663":"markdown","c862cab2":"markdown","0c83b53c":"markdown","d3fff948":"markdown","ad0664dd":"markdown","987f84e2":"markdown","3f44d094":"markdown","665e5faf":"markdown","e45a669d":"markdown","8b63ed76":"markdown","6f0e07b6":"markdown","ecf78d7f":"markdown","3e3f3528":"markdown","ae467c09":"markdown","9eed6c3e":"markdown","364baf59":"markdown","21fa01e5":"markdown","a07d8334":"markdown","2f57f4d6":"markdown","a4ef288b":"markdown","cd753965":"markdown","d31feb5b":"markdown","7ca4c6d3":"markdown","163ea19c":"markdown","76e9b664":"markdown","ba75705a":"markdown","85345c5a":"markdown","f1340f07":"markdown","189262d4":"markdown","9d1c5903":"markdown","b2981cbc":"markdown","e1f78aa4":"markdown"},"source":{"f8f58dea":"import os\nimport json\nimport numpy as np \nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nimport re\nfrom IPython.display import display\nfrom tqdm import tqdm\nfrom collections import Counter\nimport ast\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly as py\nimport plotly.graph_objs as go\nimport gensim\nfrom gensim import corpora, models, similarities\nimport logging\nimport tempfile\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom collections import OrderedDict\nimport seaborn as sns\nimport pyLDAvis.gensim\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ninit_notebook_mode(connected=True) #do not miss this line\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\nimport seaborn as sb\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom textblob import TextBlob\nimport scipy.stats as stats\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.manifold import TSNE\n\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\noutput_notebook()\n\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom sklearn.metrics import accuracy_score, f1_score\nfrom tqdm import tqdm_notebook as tqdm\nfrom Levenshtein import ratio as levenshtein_distance\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text\n\nfrom scipy import spatial","062591cf":"## Common Variables for Notebook \nROOT = '\/kaggle\/input\/google-quest-challenge\/'\n\n## load the data \ndf_train = pd.read_csv(ROOT+'train.csv')\ndf_test = pd.read_csv(ROOT+'test.csv')\ndf_sub = pd.read_csv(ROOT+'sample_submission.csv')","131ac518":"#Looking data format and types\nprint(df_train.info())\nprint(df_test.info())\nprint(df_sub.info())","681e4639":"#Some Statistics\ndf_train.describe()","76625cf7":"df_sub.describe()","9d244e90":"#Take a look at the data\ndf_train.head()","1fe96778":"df_train[\"question_title\"].head()","3a4e635e":"q = df_train[\"question_title\"].to_list()\nfor i in range(5):\n    print('Question title '+str(i+1)+': '+q[i])","eacf78ea":"q = df_train[\"question_body\"].to_list()\nfor i in range(5):\n    print('==> Question body '+str(i+1)+': '+q[i])\n    print('****************************************************************************************************')\n    print('****************************************************************************************************')","2a6f0a68":"q = df_train[\"answer\"].to_list()\nfor i in range(5):\n    print('==> Answer '+str(i+1)+': '+q[i])","459700b0":"#defining the figure size of our graphic\nplt.figure(figsize=(12,5))\n\n#Plotting the result\nsns.countplot(x='category', data=df_train, palette=\"hls\")\nplt.xlabel(\"category\", fontsize=16) #seting the xtitle and size\nplt.ylabel(\"Count\", fontsize=16) # Seting the ytitle and size\nplt.title(\"Category Name Count\", fontsize=20) \nplt.xticks(rotation=45)\nplt.show()","3a1eaf8b":"df_train['host_type'] = df_train.host.apply(lambda x: x.split('.')[0])","3633dee5":"#defining the figure size of our graphic\nplt.figure(figsize=(12,8))\n\n#Plotting the result\nsns.countplot(x='host_type', data=df_train, palette=\"hls\")\nplt.xlabel(\"host_type\", fontsize=16) #seting the xtitle and size\nplt.ylabel(\"Count\", fontsize=16) # Seting the ytitle and size\nplt.title(\"Host Type Name Count\", fontsize=20) \nplt.xticks(rotation=45)\nplt.show()","33acea58":"## Check the scoring for questions\nall_train_columns = list(df_train.columns)\nquestion_answer_cols = all_train_columns[:11]\nquestion_target_cols = all_train_columns[11:32]\nanswer_target_cols  = all_train_columns[32:41]\n\n## Check target scoring for question\ndf_train[question_target_cols].loc[0]","9a96bf8c":"## Check target scoring for answer\ndf_train[answer_target_cols].loc[0]","6df2c8dc":"print('There is '+str(len(set(df_train['question_user_name'].to_list())))+' unique user asked a questions')\nprint('There is '+str(len(set(df_train['answer_user_name'].to_list())))+' unique user answer a questions')","7edeb9e8":"## What is the distribution of all question ranking columns \n\ndf_train[question_target_cols]","7389ccd6":"## lets see some distributions of questions targets\nplt.figure(figsize=(10, 5))\n\nsns.distplot(df_train[question_target_cols[0]], hist= False , rug= False ,kde=True, label =question_target_cols[0],axlabel =False )\nsns.distplot(df_train[question_target_cols[1]], hist= False , rug= False,label =question_target_cols[1],axlabel =False)\nsns.distplot(df_train[question_target_cols[2]], hist= False , rug= False,label =question_target_cols[2],axlabel =False)\nsns.distplot(df_train[question_target_cols[3]], hist= False , rug= False,label =question_target_cols[3],axlabel =False)\nsns.distplot(df_train[question_target_cols[4]], hist= False , rug= False,label =question_target_cols[4],axlabel =False)\nplt.show()","809dd999":"## lets see some distributions of answer targets\nplt.figure(figsize=(10, 5))\n\nsns.distplot(df_train[answer_target_cols[0]], hist= False , rug= False ,kde=True, label =answer_target_cols[0],axlabel =False )\nsns.distplot(df_train[answer_target_cols[1]], hist= False , rug= False,label =answer_target_cols[1],axlabel =False)\n#sns.distplot(train[answer_target_cols[2]], hist= False , rug= False,label =answer_target_cols[2],axlabel =False)\n#sns.distplot(train[answer_target_cols[3]], hist= False , rug= False,label =answer_target_cols[3],axlabel =False)\nsns.distplot(df_train[answer_target_cols[4]], hist= False , rug= False,label =answer_target_cols[4],axlabel =False)\nplt.show()\n## Removed two columns as value was quite high and other graphs were not visible .","81b25403":"# Lets see how the mean value of one target feature for questions changes based on category\nfor idx in range(20):\n    df = df_train.groupby('category')[question_target_cols[idx]].mean()\n        \n    fig, axes = plt.subplots(1, 1, figsize=(10,10))\n    axes.set_title(question_target_cols[idx])\n    df.plot(label=question_target_cols[idx])\n    plt.show()","9fd6b245":"html_tags = ['<P>', '<\/P>', '<Table>', '<\/Table>', '<Tr>', '<\/Tr>', '<Ul>', '<Ol>', '<Dl>', '<\/Ul>', '<\/Ol>', \\\n             '<\/Dl>', '<Li>', '<Dd>', '<Dt>', '<\/Li>', '<\/Dd>', '<\/Dt>']\nr_buf = ['It', 'is', 'are', 'do', 'does', 'did', 'was', 'were', 'will', 'can', 'the', 'a', 'of', 'in', 'and', 'on', \\\n         'what', 'where', 'when', 'which'] + html_tags\n\ndef clean(x):\n    x = x.lower()\n    for r in r_buf:\n        x = x.replace(r, '')\n    x = re.sub(' +', ' ', x)\n    return x\n\nbin_question_tokens = ['it', 'is', 'are', 'do', 'does', 'did', 'was', 'were', 'will', 'can']\nstop_words = text.ENGLISH_STOP_WORDS.union([\"book\"])\n\ndef predict(json_data, annotated=False):\n    # Parse JSON data\n    candidates = json_data['long_answer_candidates']\n    candidates = [c for c in candidates if c['top_level'] == True]\n    doc_tokenized = json_data['document_text'].split(' ')\n    question = json_data['question_text']\n    question_s = question.split(' ') \n    if annotated:\n        ann = json_data['annotations'][0]\n\n    # TFIDF for the document\n    tfidf = TfidfVectorizer(ngram_range=(1,1), stop_words=stop_words)\n    tfidf.fit([json_data['document_text']])\n    q_tfidf = tfidf.transform([question]).todense()\n\n    # Find the nearest answer from candidates\n    distances = []\n    scores = []\n    i_ann = -1\n    for i, c in enumerate(candidates):\n        s, e = c['start_token'], c['end_token']\n        t = ' '.join(doc_tokenized[s:e])\n        distances.append(levenshtein_distance(clean(question), clean(t)))\n        \n        t_tfidf = tfidf.transform([t]).todense()\n        score = 1 - spatial.distance.cosine(q_tfidf, t_tfidf)\n        \n#         score = 0\n        \n#         for w in doc_tokenized[s:e]:\n#             if w in q_s:\n#                 score += 0.1\n\n        scores.append(score)\n\n    # Format results\n#     ans = candidates[np.argmin(distances)]\n    ans = candidates[np.argmax(scores)]\n    if np.max(scores) < 0.2:\n        ans_long = '-1:-1'\n    else:\n        ans_long = str(ans['start_token']) + ':' + str(ans['end_token'])\n    if question_s[0] in bin_question_tokens:\n        ans_short = 'YES'\n    else:\n        ans_short = ''\n        \n    # Preparing data for debug\n    if annotated:\n        ann_long_text = ' '.join(doc_tokenized[ann['long_answer']['start_token']:ann['long_answer']['end_token']])\n        if ann['yes_no_answer'] == 'NONE':\n            if len(json_data['annotations'][0]['short_answers']) > 0:\n                ann_short_text = ' '.join(doc_tokenized[ann['short_answers'][0]['start_token']:ann['short_answers'][0]['end_token']])\n            else:\n                ann_short_text = ''\n        else:\n            ann_short_text = ann['yes_no_answer']\n    else:\n        ann_long_text = ''\n        ann_short_text = ''\n        \n    ans_long_text = ' '.join(doc_tokenized[ans['start_token']:ans['end_token']])\n    if len(ans_short) > 0 or ans_short == 'YES':\n        ans_short_text = ans_short\n    else:\n        ans_short_text = '' # Fix when short answers will work\n                    \n    return ans_long, ans_short, question, ann_long_text, ann_short_text, ans_long_text, ans_short_text","7f785fc2":"reindexed_data = df_train['question_body']\nreindexed_data1 = df_train['answer']","cfb20079":"# Define helper functions\ndef get_top_n_words(n_top_words, count_vectorizer, text_data):\n    '''\n    returns a tuple of the top n words in a sample and their \n    accompanying counts, given a CountVectorizer object and text sample\n    '''\n    vectorized_headlines = count_vectorizer.fit_transform(text_data.values)\n    vectorized_total = np.sum(vectorized_headlines, axis=0)\n    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)\n    word_values = np.flip(np.sort(vectorized_total)[0,:],1)\n    \n    word_vectors = np.zeros((n_top_words, vectorized_headlines.shape[1]))\n    for i in range(n_top_words):\n        word_vectors[i,word_indices[0,i]] = 1\n\n    words = [word[0].encode('ascii').decode('utf-8') for \n             word in count_vectorizer.inverse_transform(word_vectors)]\n\n    return (words, word_values[0,:n_top_words].tolist()[0])","b3621207":"count_vectorizer = CountVectorizer(stop_words='english')\nwords, word_values = get_top_n_words(n_top_words=25,\n                                     count_vectorizer=count_vectorizer, \n                                     text_data=reindexed_data)\n\nfig, ax = plt.subplots(figsize=(10,4))\nax.bar(range(len(words)), word_values);\nax.set_xticks(range(len(words)));\nax.set_xticklabels(words, rotation='vertical');\nax.set_title('Top words in headlines dataset (excluding stop words)');\nax.set_xlabel('Word');\nax.set_ylabel('Number of occurences');\nplt.show()","e425a1b8":"count_vectorizer = CountVectorizer(stop_words='english')\nwords, word_values = get_top_n_words(n_top_words=25,\n                                     count_vectorizer=count_vectorizer, \n                                     text_data=reindexed_data1)\n\nfig, ax = plt.subplots(figsize=(10,4))\nax.bar(range(len(words)), word_values);\nax.set_xticks(range(len(words)));\nax.set_xticklabels(words, rotation='vertical');\nax.set_title('Top words in headlines dataset (excluding stop words)');\nax.set_xlabel('Word');\nax.set_ylabel('Number of occurences');\nplt.show()","3f4b31ba":"tagged_headlines = [TextBlob(reindexed_data[i]).pos_tags for i in range(reindexed_data.shape[0])]","93a6c6db":"tagged_headlines_df = pd.DataFrame({'tags':tagged_headlines})\n\nword_counts = [] \npos_counts = {}\n\nfor headline in tagged_headlines_df[u'tags']:\n    word_counts.append(len(headline))\n    for tag in headline:\n        if tag[1] in pos_counts:\n            pos_counts[tag[1]] += 1\n        else:\n            pos_counts[tag[1]] = 1\n            \nprint('Total number of words: ', np.sum(word_counts))\nprint('Mean number of words per question: ', np.mean(word_counts))","fede944e":"y = stats.norm.pdf(np.linspace(0,14,50), np.mean(word_counts), np.std(word_counts))\n\nfig, ax = plt.subplots(figsize=(8,4))\nax.hist(word_counts, bins=range(1,14), density=True);\nax.plot(np.linspace(0,14,50), y, 'r--', linewidth=1);\nax.set_title('Headline word lengths');\nax.set_xticks(range(1,14));\nax.set_xlabel('Number of words');\nplt.show()","6f3b0028":"pos_sorted_types = sorted(pos_counts, key=pos_counts.__getitem__, reverse=True)\npos_sorted_counts = sorted(pos_counts.values(), reverse=True)\n\nfig, ax = plt.subplots(figsize=(14,4))\nax.bar(range(len(pos_counts)), pos_sorted_counts);\nax.set_xticks(range(len(pos_counts)));\nax.set_xticklabels(pos_sorted_types);\nax.set_title('Part-of-Speech Tagging for questions Corpus');\nax.set_xlabel('Type of Word');","9db83015":"small_count_vectorizer = CountVectorizer(stop_words='english', max_features=40000)\nsmall_text_sample = reindexed_data.sample(n=500, random_state=0).values\n\nprint('Questions before vectorization: {}'.format(small_text_sample[123]))\n\nsmall_document_term_matrix = small_count_vectorizer.fit_transform(small_text_sample)\n\nprint('Questions after vectorization: \\n{}'.format(small_document_term_matrix[123]))","968919c9":"#number of topics\nn_topics = 5","e1c3fb2a":"lsa_model = TruncatedSVD(n_components=n_topics)\nlsa_topic_matrix = lsa_model.fit_transform(small_document_term_matrix)","552be7d8":"# Define helper functions\ndef get_keys(topic_matrix):\n    '''\n    returns an integer list of predicted topic \n    categories for a given topic matrix\n    '''\n    keys = topic_matrix.argmax(axis=1).tolist()\n    return keys\n\ndef keys_to_counts(keys):\n    '''\n    returns a tuple of topic categories and their \n    accompanying magnitudes for a given list of keys\n    '''\n    count_pairs = Counter(keys).items()\n    categories = [pair[0] for pair in count_pairs]\n    counts = [pair[1] for pair in count_pairs]\n    return (categories, counts)","173d9e8d":"lsa_keys = get_keys(lsa_topic_matrix)\nlsa_categories, lsa_counts = keys_to_counts(lsa_keys)","7c1c245a":"# Define helper functions\ndef get_top_n_words(n, keys, document_term_matrix, count_vectorizer):\n    '''\n    returns a list of n_topic strings, where each string contains the n most common \n    words in a predicted category, in order\n    '''\n    top_word_indices = []\n    for topic in range(n_topics):\n        temp_vector_sum = 0\n        for i in range(len(keys)):\n            if keys[i] == topic:\n                temp_vector_sum += document_term_matrix[i]\n        temp_vector_sum = temp_vector_sum.toarray()\n        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n        top_word_indices.append(top_n_word_indices)   \n    top_words = []\n    for topic in top_word_indices:\n        topic_words = []\n        for index in topic:\n            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n            temp_word_vector[:,index] = 1\n            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n        top_words.append(\" \".join(topic_words))         \n    return top_words","44f72ff9":"top_n_words_lsa = get_top_n_words(10, lsa_keys, small_document_term_matrix, small_count_vectorizer)\n\nfor i in range(len(top_n_words_lsa)):\n    print(\"Topic {}: \".format(i+1), top_n_words_lsa[i])","0fddd43c":"top_3_words = get_top_n_words(3, lsa_keys, small_document_term_matrix, small_count_vectorizer)\nlabels = ['Topic {}: \\n'.format(i) + top_3_words[i] for i in lsa_categories]\n\nfig, ax = plt.subplots(figsize=(8,4))\nax.bar(lsa_categories, lsa_counts);\nax.set_xticks(lsa_categories);\nax.set_xticklabels(labels);\nax.set_ylabel('Number of questions');\nax.set_title('LSA topic counts');\nplt.show()","000de882":"tsne_lsa_model = TSNE(n_components=2, perplexity=50, learning_rate=100, \n                        n_iter=2000, verbose=1, random_state=0, angle=0.75)\ntsne_lsa_vectors = tsne_lsa_model.fit_transform(lsa_topic_matrix)","302da1c1":"# Define helper functions\ndef get_mean_topic_vectors(keys, two_dim_vectors):\n    '''\n    returns a list of centroid vectors from each predicted topic category\n    '''\n    mean_topic_vectors = []\n    for t in range(n_topics):\n        articles_in_that_topic = []\n        for i in range(len(keys)):\n            if keys[i] == t:\n                articles_in_that_topic.append(two_dim_vectors[i])    \n        \n        articles_in_that_topic = np.vstack(articles_in_that_topic)\n        mean_article_in_that_topic = np.mean(articles_in_that_topic, axis=0)\n        mean_topic_vectors.append(mean_article_in_that_topic)\n    return mean_topic_vectors","bbedb4bc":"colormap = np.array([\n    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\",\n    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\" ])\ncolormap = colormap[:n_topics]","63d29a92":"top_3_words_lsa = get_top_n_words(3, lsa_keys, small_document_term_matrix, small_count_vectorizer)\nlsa_mean_topic_vectors = get_mean_topic_vectors(lsa_keys, tsne_lsa_vectors)\n\nplot = figure(title=\"t-SNE Clustering of {} LSA Topics\".format(n_topics), plot_width=700, plot_height=700)\nplot.scatter(x=tsne_lsa_vectors[:,0], y=tsne_lsa_vectors[:,1], color=colormap[lsa_keys])\n\nfor t in range(n_topics):\n    label = Label(x=lsa_mean_topic_vectors[t][0], y=lsa_mean_topic_vectors[t][1], \n                  text=top_3_words_lsa[t], text_color=colormap[t])\n    plot.add_layout(label)\n    \nshow(plot)","08fd36cd":"lda_model = LatentDirichletAllocation(n_components=n_topics, learning_method='online', \n                                          random_state=0, verbose=0)\nlda_topic_matrix = lda_model.fit_transform(small_document_term_matrix)","94bda48c":"lda_keys = get_keys(lda_topic_matrix)\nlda_categories, lda_counts = keys_to_counts(lda_keys)","f733dd9f":"top_n_words_lda = get_top_n_words(10, lda_keys, small_document_term_matrix, small_count_vectorizer)\n\nfor i in range(len(top_n_words_lda)):\n    print(\"Topic {}: \".format(i+1), top_n_words_lda[i])","251c53df":"top_3_words = get_top_n_words(3, lda_keys, small_document_term_matrix, small_count_vectorizer)\nlabels = ['Topic {}: \\n'.format(i) + top_3_words[i] for i in lda_categories]\n\nfig, ax = plt.subplots(figsize=(10,4))\nax.bar(lda_categories, lda_counts);\nax.set_xticks(lda_categories);\nax.set_xticklabels(labels);\nax.set_title('LDA topic counts');\nax.set_ylabel('Number of questions');","ac146de7":"tsne_lda_model = TSNE(n_components=2, perplexity=50, learning_rate=100, \n                        n_iter=2000, verbose=1, random_state=0, angle=0.75)\ntsne_lda_vectors = tsne_lda_model.fit_transform(lda_topic_matrix)","9844f075":"top_3_words_lda = get_top_n_words(3, lda_keys, small_document_term_matrix, small_count_vectorizer)\nlda_mean_topic_vectors = get_mean_topic_vectors(lda_keys, tsne_lda_vectors)\n\nplot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), plot_width=600, plot_height=600)\nplot.scatter(x=tsne_lda_vectors[:,0], y=tsne_lda_vectors[:,1], color=colormap[lda_keys])\n\nfor t in range(n_topics):\n    label = Label(x=lda_mean_topic_vectors[t][0], y=lda_mean_topic_vectors[t][1], \n                  text=top_3_words_lda[t], text_color=colormap[t])\n    plot.add_layout(label)\n\nshow(plot)","3e836f40":"# Preparing a corpus for analysis and checking the first 5 entries\ncorpus=[]\n\ncorpus = df_train['question_body'].to_list()\n\ncorpus[:5]","0d01f46b":"TEMP_FOLDER = tempfile.gettempdir()\nprint('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","8e4d2bf4":"# removing common words and tokenizing\n# google-quest-challenge\nstoplist = stopwords.words('english') + list(punctuation) + list(\"([)]?\") + [\")?\"]\n\ntexts = [[word for word in str(document).lower().split() if word not in stoplist] for document in corpus]\n\ndictionary = corpora.Dictionary(texts)\ndictionary.save(os.path.join(TEMP_FOLDER, 'google-quest-challenge.dict'))  # store the dictionary,","bfc42c10":"corpus = [dictionary.doc2bow(text) for text in texts]\ncorpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'google-quest-challenge.mm'), corpus) ","fbafbcbc":"tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model","a91ad577":"corpus_tfidf = tfidf[corpus]  # step 2 -- use the model to transform vectors","498415d7":"#I will try 15 topics\ntotal_topics = 15\n\nlda = models.LdaModel(corpus, id2word=dictionary, num_topics=total_topics)\ncorpus_lda = lda[corpus_tfidf] # create a double wrapper over the original corpus: bow->tf","f64d8de3":"lda.show_topics(total_topics,5)","b9156cdc":"data_lda = {i: OrderedDict(lda.show_topic(i,25)) for i in range(total_topics)}","9915d857":"df_lda = pd.DataFrame(data_lda)\ndf_lda = df_lda.fillna(0).T\nprint(df_lda.shape)","cb05b08c":"df_lda","980a701f":"g=sns.clustermap(df_lda.corr(), center=0, standard_scale=1, cmap=\"OrRd\", metric='cosine', linewidths=.75, figsize=(12, 12))\nplt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\nplt.show()\n#plt.setp(ax_heatmap.get_yticklabels(), rotation=0)  # For y axis","74e91507":"pyLDAvis.enable_notebook()\npanel = pyLDAvis.gensim.prepare(lda, corpus_lda, dictionary, mds='tsne')\npanel","145552f0":"train = df_train.copy()\ntest = df_test.copy()","23379737":"target_cols = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']","df40c3de":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD","0b15bcef":"tfidf = TfidfVectorizer(ngram_range=(1, 3))\ntsvd = TruncatedSVD(n_components = 50)\nquestion_title = tfidf.fit_transform(train[\"question_title\"].values)\nquestion_title_test = tfidf.transform(test[\"question_title\"].values)\nquestion_title = tsvd.fit_transform(question_title)\nquestion_title_test = tsvd.transform(question_title_test)\n\nquestion_body = tfidf.fit_transform(train[\"question_body\"].values)\nquestion_body_test = tfidf.transform(test[\"question_body\"].values)\nquestion_body = tsvd.fit_transform(question_body)\nquestion_body_test = tsvd.transform(question_body_test)\n\nanswer = tfidf.fit_transform(train[\"answer\"].values)\nanswer_test = tfidf.transform(test[\"answer\"].values)\nanswer = tsvd.fit_transform(answer)\nanswer_test = tsvd.transform(answer_test)","6af9b32e":"train[\"len_user_name\"]= train.question_user_name.apply(lambda x : len(x.split()))\ntest[\"len_user_name\"]= test.question_user_name.apply(lambda x : len(x.split()))","564998ef":"train[\"cat_host\"]= train[\"category\"]+train[\"host\"]+str(train[\"len_user_name\"])\ntest[\"cat_host\"]= test[\"category\"]+test[\"host\"]+str(test[\"len_user_name\"])\n\n\ncategory_means_map = train.groupby(\"len_user_name\")[target_cols].mean().T.to_dict()\ncategory_te = train[\"len_user_name\"].map(category_means_map).apply(pd.Series)\ncategory_te_test = test[\"len_user_name\"].map(category_means_map).apply(pd.Series)","cf91b739":"train_features = np.concatenate([question_title, question_body, answer#, category_te.values\n                                ], axis = 1)\ntest_features = np.concatenate([question_title_test, question_body_test, answer_test#, category_te_test.values\n                               ], axis = 1)","da120f9c":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom sklearn.model_selection import KFold\nfrom keras.callbacks.callbacks import EarlyStopping\nfrom scipy.stats import spearmanr\n\nnum_folds = 5\nfold_scores = []\nkf = KFold(n_splits = num_folds, shuffle = True, random_state = 42)\ntest_preds = np.zeros((len(test_features), len(target_cols)))\nfor train_index, val_index in kf.split(train_features):\n    train_X = train_features[train_index, :]\n    train_y = train[target_cols].iloc[train_index]\n    \n    val_X = train_features[val_index, :]\n    val_y = train[target_cols].iloc[val_index]\n    \n    model = Sequential([\n        Dense(128, input_shape=(train_features.shape[1],)),\n        Activation('relu'),\n        Dense(64),\n        Activation('relu'),\n        Dense(len(target_cols)),\n        Activation('sigmoid'),\n    ])\n    es = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy')\n    \n    model.fit(train_X, train_y, epochs = 50, validation_data=(val_X, val_y), callbacks = [es])\n    preds = model.predict(val_X)\n    overall_score = 0\n    for col_index, col in enumerate(target_cols):\n        overall_score += spearmanr(preds[:, col_index], val_y[col].values).correlation\/len(target_cols)\n        print(col, spearmanr(preds[:, col_index], val_y[col].values).correlation)\n    fold_scores.append(overall_score)\n\n    test_preds += model.predict(test_features)\/num_folds\n    \nprint(fold_scores)","0f4b77bc":"sub = df_sub.copy()","e8c618c8":"for col_index, col in enumerate(target_cols):\n    sub[col] = test_preds[:, col_index]","34e4560a":"sub.to_csv(\"submission.csv\", index = False)","ad568bc7":"**This step will take more than 1 hour, you can skeep it if you want**","89d34dd2":"Thus we have our (very high-rank and sparse) training data, small_document_term_matrix, and can now actually implement a clustering algorithm. Our choice will be either Latent Semantic Analysis or Latent Dirichilet Allocation. Both will take our document-term matrix as input and yield an $n \\times N$ topic matrix as output, where $N$ is the number of topic categories (which we supply as a parameter). For the moment, we shall take this to be 5 like categories number.","41a880ab":"Taking the $\\arg \\max$ of each headline in this topic matrix will give the predicted topics of each headline in the sample. We can then sort these into counts of each topic.","5c029d15":"However, this does not provide a great point of comparison with other clustering algorithms. In order to properly contrast LSA with LDA we instead use a dimensionality-reduction technique called $t$-SNE, which will also serve to better illuminate the success of the clustering process.","32fc590e":"**Findings**\n\nIt's clear that most questions are about graphic design, more than 1200.","daa4af1d":"## Preprocessing\n\nThe only preprocessing step required in our case is feature construction, where we take the sample of text questions and represent them in some tractable feature space. In practice, this simply means converting each string to a numerical vector. This can be done using the CountVectorizer object from SKLearn, which yields an $n\u00d7K$ document-term matrix where $K$ is the number of distinct words across the $n$ headlines in our sample (less stop words and with a limit of max_features).","6f49628d":"**How many unique users asked questions ?**\n\n**How many unique users answered questions ?**","5af7ee19":"**However, these topic categories are in and of themselves a little meaningless. In order to better characterise them, it will be helpful to find the most frequent words in each.**","ce85fb47":"# Exploring the data:","d2c23144":"## Let's check and count hosts catecories","5abb971e":"# Creating a transformation\n\n**The transformations are standard Python objects, typically initialized by means of a training corpus:**\n\n**Different transformations may require different initialization parameters; in case of TfIdf, the \u201ctraining\u201d consists simply of going through the supplied corpus once and computing document frequencies of all its features. Training other models, such as Latent Semantic Analysis or Latent Dirichlet Allocation, is much more involve**","5a039752":"## In the previous cells, I created a corpus of documents represented as a stream of vectors. To continue, lets use that corpus, with the help of Gensim.\n\n**Gensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning. Gensim is implemented in Python and Cython. Wikipedia**\n\n","49b075a9":"**From now on, tfidf is treated as a read-only object that can be used to apply a transformation to a whole corpus:**\n","d5871a6b":"## Part-of-Speech Tagging for questions Corpus","14083e46":"**We now repeat this process using LDA instead of LSA. LDA is instead a generative probabilistic process, designed with the specific goal of uncovering latent topic structure in text corpora.**","8c792164":"### Topic Modelling\n\nWe now apply a clustering algorithm to the headlines corpus in order to study the topic, as well as how it has evolved through time. To do so, we first experiment with a small subsample of the dataset in order to determine which of the two potential clustering algorithms is most appropriate \u2013 once this has been ascertained, we then scale up to a larger portion of the available data.","25bb361a":"# TO BE CONTINUED...","6bc9f0f5":"## Count category","ea9af55f":"**I thinks it's very important to do topic modeling and understand the content of each question and answer. The reason is that there is some question out of meaning and are not related to the category so they will get little answer and sometimes some blaming**\n\n**So we will start with some Topic Modeling Algorithms**","6559f663":"# LDA with an other way of visualisation","c862cab2":"**First we develop a list of the top words used across all questions, giving us a glimpse into the core vocabulary of the source data. Stop words are omitted here to avoid any trivial conjunctions, prepositions, etc.**","0c83b53c":"**Once again, we take the $\\arg \\max$ of each entry in the topic matrix to obtain the predicted topic category for each question. These topic categories can then be characterised by their most frequent words.**","d3fff948":"# Check the scoring for questions","ad0664dd":"# LDA:\n\n**Latent Dirichlet Allocation, LDA is yet another transformation from bag-of-words counts into a topic space of lower dimensionality. LDA is a probabilistic extension of LSA (also called multinomial PCA), so LDA\u2019s topics can be interpreted as probability distributions over words. These distributions are, just like with LSA, inferred automatically from a training corpus. Documents are in turn interpreted as a (soft) mixture of these topics (again, just like with LSA).**","987f84e2":"**Let's check the answers**","3f44d094":"**However, in order to properly compare LDA with LSA, we again take this topic matrix and project it into two dimensions with $t$-SNE.**","665e5faf":"Next we generate a histogram of headline word lengths, and use part-of-speech tagging to understand the types of words used across the corpus. This requires first converting all headline strings to TextBlobs and calling the pos_tags method on each, yielding a list of tagged words for each headline. A complete list of such word tags is available [here](https:\/\/www.clips.uantwerpen.be\/pages\/MBSP-tags)","e45a669d":"## I didn't used text vectors yet, I just tried to visualize them.","8b63ed76":"# Note\n\n**Transformations always convert between two specific vector spaces. The same vector space (= the same set of feature ids) must be used for training as well as for subsequent vector transformations. Failure to use the same input feature space, such as applying a different string preprocessing, using different feature ids, or using bag-of-words input vectors where TfIdf vectors are expected, will result in feature mismatch during transformation calls and consequently in either garbage output and\/or runtime exceptions.**","6f0e07b6":"Evidently, this is a bit a of a failed result. We have failed to reach any great degree of separation across the topic categories, and it is difficult to tell whether this can be attributed to the LSA decomposition or instead the $t$-SNE dimensionality reduction process. Let's move forward and try another clustering technique.","ecf78d7f":"## Let's check the answers now","3e3f3528":"**It's clear that the most questions are technical and IT questions**","ae467c09":"Let's start by experimenting with LSA. This is effectively just a truncated singular value decomposition of a (very high-rank and sparse) document-term matrix, with only the $r=$n_topics largest singular values preserved.","9eed6c3e":"## By focusing in thr URL and host of each question, it's clearly that there is different specified host link to each category, such as:\n\n**photo**\n\n**rpg**\n\n**electronics**\n\n**judaims**\n\n**graphicdesign**\n\n**...**","364baf59":"Now that we have reduced these n_topics-dimensional vectors to two-dimensional representations, we can then plot the clusters using Bokeh. Before doing so however, it will be useful to derive the centroid location of each topic, so as to better contextualise our visualisation.","21fa01e5":"![](http:\/\/2.bp.blogspot.com\/-Igasbf2fhm0\/XdhmHtZxQ2I\/AAAAAAAAGVk\/260Uwxiex3IPMwFRiJA4zpsYBTT9Ia3rQCK4BGAYYCw\/s1600\/image.png)\n","a07d8334":"# Latent Dirichilet Allocation (LDA)","2f57f4d6":"These figures are taked from this kernel https:\/\/www.kaggle.com\/phoenix9032\/get-started-with-your-questions-eda","a4ef288b":"# Latent Semantic Analysis (LSA)","cd753965":"**Let's take a look to question_body**","d31feb5b":"**This is a much better result! Controlling for $t$-SNE, it would seem that LDA has had much more succcess than LSA in separating out the topic categories. For this reason, LDA appears the more appropriate algorithm when we scale up the clustering process.**","7ca4c6d3":"**Welcome to my Google QUEST Q&A Labeling!**\n\n**This kernel will provide a analysis through the Google QUEST Q&A Labeling to improving automated understanding of complex question answer content**","163ea19c":"## Latent Dirichlet Allocation (LDA)\n\nTopic modeling is a type of statistical modeling for discovering the abstract \u201ctopics\u201d that occur in a collection of documents. **Latent Dirichlet Allocation (LDA)** is an example of topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions.\n\nHere we are going to apply LDA to a set of questions\/answers and split them into topics. Let\u2019s get started!","76e9b664":"Thus we have converted our initial small sample of headlines into a list of predicted topic categories, where each category is characterised by its most frequent words. The relative magnitudes of each of these categories can then be easily visualised though use of a bar chart.","ba75705a":"## It's clear that the rest of the question titles are hided because of the question length, I will put the column in a list to check the first 100 questions","85345c5a":"# Topic Modeling","f1340f07":"**Show first n important word in the topics:**\n","189262d4":"# BASELINE TAKEN FROM RYCHES KERNEL","9d1c5903":"**All that remains is to plot the clustered questions. Also included are the top three words in each cluster, which are placed at the centroid for that topic.**","b2981cbc":"# First look at the data","e1f78aa4":"# Check target scoring for answer"}}