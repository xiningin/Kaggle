{"cell_type":{"6a13a9f6":"code","0aec7465":"code","77d540da":"code","212d48fe":"code","b0d3b827":"code","ea74e88d":"code","9dbc98b9":"code","73979322":"code","6b161528":"code","cb73ba28":"code","a6afa43e":"code","7ca7280c":"code","248e325b":"code","e425eedf":"code","82533b5f":"code","8b987eb5":"code","8007ee78":"code","111d7fc5":"code","2b4672ab":"code","3ee66ab4":"code","a208c6f3":"code","ef572f25":"code","188c0c72":"code","d16cd541":"code","50263847":"code","3337a002":"code","5efdab80":"code","d67dc1fb":"code","c4a7cfee":"code","8ad143e9":"code","0e930dcf":"code","38d47f9a":"code","14fe2866":"code","25f4fe22":"code","5f47a289":"code","ac9ea45a":"code","9ff78a25":"code","d43b254e":"code","fbac05d4":"code","3573def7":"code","933dccab":"code","974b3a60":"code","bc7f67d3":"code","e4c355cf":"code","757be527":"code","8d225dbf":"code","fb0115cc":"code","8440d323":"code","6ba6295a":"code","ab8e27f3":"code","c8041e26":"markdown","c1074f06":"markdown","9468010e":"markdown","c48fe24d":"markdown","56de752f":"markdown","70f245a4":"markdown","d665093a":"markdown","ec9baa31":"markdown","bbe3fbd9":"markdown","7b6e6826":"markdown","438fc56e":"markdown","d172ea41":"markdown","44f572e6":"markdown","54e4dc40":"markdown","61aa1dc1":"markdown","489fbebb":"markdown","b428b37f":"markdown","42d9bad0":"markdown","b92b8a3c":"markdown","039b127d":"markdown","b3e96778":"markdown","eba7351f":"markdown","38f4f31b":"markdown","b6e4acf7":"markdown","99edf5a1":"markdown","a5bcc64d":"markdown","76a93f50":"markdown"},"source":{"6a13a9f6":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom pandas.api.types import is_numeric_dtype, is_object_dtype\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\n\n\nimport xgboost as xgb\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0aec7465":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntraindf = train.copy()\ntestdf = test.copy()\ndf = pd.concat([traindf,testdf])","77d540da":"train.head()","212d48fe":"test.head()","b0d3b827":"print(\"Train Data:\",train.shape[0],\"observations and\", train.shape[1], \"features\" )\nprint(\"Test Data:\",test.shape[0],\"observations and\", test.shape[1], \"features\" )","ea74e88d":"train_id = train[\"Id\"]\ntest_id = test[\"Id\"]\ntrain.drop(\"Id\" , axis = 1 , inplace = True)\ntest.drop(\"Id\" , axis = 1 , inplace = True)","9dbc98b9":"df.drop([\"SalePrice\"], axis = 1, inplace = True)","73979322":"features = df.drop(\"Id\",1)\nfeatures.describe().T","6b161528":"col_null_df = pd.DataFrame(columns = ['Column', 'Type', 'Total NaN', '%'])\ncol_null = features.columns[features.isna().any()].to_list()\nL = len(features)\nfor col in col_null:\n    T = 0\n    if is_numeric_dtype(features[col]):\n        T = \"Numerical\"  \n    else:\n        T = \"Categorical\"\n    nulls = len(features[features[col].isna() == True][col])   \n    col_null_df = col_null_df.append({'Column': col,'Type': T,'Total NaN': nulls,'%': (nulls \/ L)*100}, ignore_index=True)\ncol_null_df.sort_values(by=\"%\", ascending=False)","cb73ba28":"df.isna().sum().nlargest(35)","a6afa43e":"some_miss_columns = [\"PoolQC\",\"MiscFeature\",\"Alley\",\"Fence\",\"FireplaceQu\",\"GarageType\",\"GarageFinish\",\"GarageQual\",\"GarageCond\",\n                  \"BsmtQual\",\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\"MasVnrType\",\"MSSubClass\"]\n\nfor i in some_miss_columns :\n        df[i].fillna(\"None\" , inplace = True)","7ca7280c":"df[\"Functional\"] = df[\"Functional\"].fillna(\"Typ\")","248e325b":"some_miss_columns2 = [\"MSZoning\", \"BsmtFullBath\", \"BsmtHalfBath\", \"Utilities\",\"MSZoning\",\n                      \"Electrical\", \"KitchenQual\", \"SaleType\",\"Exterior1st\", \"Exterior2nd\",\"MasVnrArea\"]\nfor i in some_miss_columns2:\n    df[i].fillna(df[i].mode()[0], inplace = True)","e425eedf":"some_miss_columns3 = [\"GarageYrBlt\", \"GarageArea\", \"GarageCars\",\"BsmtFinSF1\",\"BsmtFinSF2\",\"BsmtUnfSF\",\"TotalBsmtSF\"]\nfor i in some_miss_columns3 :\n    df[i] = df[i].fillna(0)","82533b5f":"df[\"LotFrontage\"] = df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","8b987eb5":"df.isna().sum().nlargest(3)","8007ee78":"sns.distplot(train[\"SalePrice\"])","111d7fc5":"sns.distplot(np.log1p(train[\"SalePrice\"]) , color = \"g\")","2b4672ab":"train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\ny = train.SalePrice.values","3ee66ab4":"Nm = [\"MSSubClass\",\"MoSold\",\"YrSold\"]\nfor col in Nm:\n    df[col] = df[col].astype(str)","a208c6f3":"lbe = LabelEncoder()\nencodecolumns = (\"FireplaceQu\",\"BsmtQual\",\"BsmtCond\",\"ExterQual\",\"ExterCond\",\"HeatingQC\",\"GarageQual\",\n                \"GarageCond\",\"PoolQC\",\"KitchenQual\",\"BsmtFinType1\",\"BsmtFinType2\",\"Functional\",\"Fence\",\n                \"BsmtExposure\",\"GarageFinish\",\"LandSlope\",\"LotShape\",\"PavedDrive\",\"Street\",\"Alley\",\n                \"CentralAir\",\"MSSubClass\",\"OverallCond\",\"YrSold\",\"MoSold\")\nfor i in encodecolumns :\n    lbe.fit(list(df[i].values))\n    df[i] = lbe.transform(list(df[i].values))","ef572f25":"df = pd.get_dummies(df)\ndf.head()","188c0c72":"Train = df[:train.shape[0]]\nTest = df[train.shape[0]:]\nprint(Train.shape)\nprint(Test.shape)","d16cd541":"X_train, X_test, y_train, y_test = train_test_split(Train, y, test_size=0.2, random_state=7)","50263847":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\nprint(y.shape)","3337a002":"model_xgb = xgb.XGBRegressor()\nparameters_xgb = {\"n_estimators\" : [5, 10, 15, 20, 50, 100, 500]}\ngrid_xgb = GridSearchCV(model_xgb, parameters_xgb, verbose=1 , scoring = \"r2\")\ngrid_xgb.fit(X_train, y_train)\n\nprint(\"Best LinReg Model: \" + str(grid_xgb.best_estimator_))\nprint(\"Best Score: \" + str(grid_xgb.best_score_))","5efdab80":"xgb = grid_xgb.best_estimator_\nxgb.fit(X_train, y_train)\nxgb_pred = xgb.predict(X_test)\nr2_xgb = r2_score(y_test, xgb_pred)\nrmse_xgb = np.sqrt(mean_squared_error(y_test, xgb_pred))\nprint(\"R^2 Score: \" + str(r2_xgb))\nprint(\"RMSE Score: \" + str(rmse_xgb))","d67dc1fb":"scores_xgb = cross_val_score(xgb, X_train, y_train, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_xgb)))","c4a7cfee":"model_gbm = GradientBoostingRegressor()\nparameters_gbm = {\"n_estimators\" : [5, 10, 15, 20, 50, 100, 500, 1000, 1500]}\ngrid_gbm = GridSearchCV(model_gbm, parameters_gbm, verbose=1 , scoring = \"r2\")\ngrid_gbm.fit(X_train, y_train)\n\nprint(\"Best LinReg Model: \" + str(grid_gbm.best_estimator_))\nprint(\"Best Score: \" + str(grid_gbm.best_score_))","8ad143e9":"gbm = grid_gbm.best_estimator_\ngbm.fit(X_train, y_train)\ngbm_pred = gbm.predict(X_test)\nr2_gbm = r2_score(y_test, gbm_pred)\nrmse_gbm = np.sqrt(mean_squared_error(y_test, gbm_pred))\nprint(\"R^2 Score: \" + str(r2_gbm))\nprint(\"RMSE Score: \" + str(rmse_gbm))","0e930dcf":"scores_gmb = cross_val_score(gbm, X_train, y_train, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_gmb)))","38d47f9a":"linreg = LinearRegression()\nparameters_lin = {}\ngrid_linreg = GridSearchCV(linreg, parameters_lin, verbose=1 , scoring = \"r2\")\ngrid_linreg.fit(X_train, y_train)\n\nprint(\"Best LinReg Model: \" + str(grid_linreg.best_estimator_))\nprint(\"Best Score: \" + str(grid_linreg.best_score_))","14fe2866":"linreg = grid_linreg.best_estimator_\nlinreg.fit(X_train, y_train)\nlin_pred = linreg.predict(X_test)\nr2_lin = r2_score(y_test, lin_pred)\nrmse_lin = np.sqrt(mean_squared_error(y_test, lin_pred))\nprint(\"R^2 Score: \" + str(r2_lin))\nprint(\"RMSE Score: \" + str(rmse_lin))","25f4fe22":"scores_lin = cross_val_score(linreg, X_train, y_train, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_lin)))","5f47a289":"dtr = DecisionTreeRegressor()\nparameters_dtr = {\"criterion\" : [\"mse\", \"friedman_mse\", \"mae\"],\n                  \"max_features\" : [\"auto\", \"log2\"], \n                  \"min_samples_split\" : [2, 3, 5, 10, 25, 50]}\ngrid_dtr = GridSearchCV(dtr, parameters_dtr, verbose=1, scoring=\"r2\")\ngrid_dtr.fit(X_train, y_train)\n\nprint(\"Best DecisionTreeRegressor Model: \" + str(grid_dtr.best_estimator_))\nprint(\"Best Score: \" + str(grid_dtr.best_score_))","ac9ea45a":"dtr = grid_dtr.best_estimator_\ndtr.fit(X_train, y_train)\ndtr_pred = dtr.predict(X_test)\nr2_dtr = r2_score(y_test, dtr_pred)\nrmse_dtr = np.sqrt(mean_squared_error(y_test, dtr_pred))\nprint(\"R^2 Score: \" + str(r2_dtr))\nprint(\"RMSE Score: \" + str(rmse_dtr))","9ff78a25":"scores_dtr = cross_val_score(dtr, X_train, y_train, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_dtr)))","d43b254e":"rf = RandomForestRegressor()\nparemeters_rf = {\"criterion\" : [\"mse\", \"mae\"],\n                 \"n_estimators\" : [5, 10, 15, 20],\n                \"min_samples_split\" : [2, 3, 5, 10] }\ngrid_rf = GridSearchCV(rf, paremeters_rf, verbose=1, scoring=\"r2\")\ngrid_rf.fit(X_train, y_train)\n\nprint(\"Best RandomForestRegressor Model: \" + str(grid_rf.best_estimator_))\nprint(\"Best Score: \" + str(grid_rf.best_score_))","fbac05d4":"rf = grid_rf.best_estimator_\nrf.fit(X_train, y_train)\nrf_pred = rf.predict(X_test)\nr2_rf = r2_score(y_test, rf_pred)\nrmse_rf = np.sqrt(mean_squared_error(y_test, rf_pred))\nprint(\"R^2 Score: \" + str(r2_rf))\nprint(\"RMSE Score: \" + str(rmse_rf))","3573def7":"scores_rf = cross_val_score(rf, X_train, y_train, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_rf)))","933dccab":"ad = AdaBoostRegressor()\nparemeters_ad = {\"n_estimators\" : [5, 10, 15, 20, 50, 100, 1000]}\ngrid_ad = GridSearchCV(ad, paremeters_ad, verbose=1, scoring=\"r2\")\ngrid_ad.fit(X_train, y_train)\n\nprint(\"Best AdaBoostRegressor Model: \" + str(grid_ad.best_estimator_))\nprint(\"Best Score: \" + str(grid_ad.best_score_))","974b3a60":"ad = grid_ad.best_estimator_\nad.fit(X_train, y_train)\nad_pred = ad.predict(X_test)\nr2_ad = r2_score(y_test, rf_pred)\nrmse_ad = np.sqrt(mean_squared_error(y_test, ad_pred))\nprint(\"R^2 Score: \" + str(r2_ad))\nprint(\"RMSE Score: \" + str(rmse_ad))","bc7f67d3":"scores_ad = cross_val_score(ad, X_train, y_train, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_ad)))","e4c355cf":"model_performances = pd.DataFrame({\n    \"Model\" : [\"XGB Regressor\", \"Gradient Boosting Regressor\", \"Linear Regression\", \"Decision Tree Regressor\", \"Random Forest Regressor\", \"Adaboost Regressor\"],\n    \"Best Score\" : [grid_xgb.best_score_, grid_gbm.best_score_, grid_linreg.best_score_, grid_dtr.best_score_, grid_rf.best_score_, grid_ad.best_score_],\n    \"R Squared\" : [str(r2_xgb)[0:5], str(r2_gbm)[0:5], str(r2_lin)[0:5], str(r2_dtr)[0:5], str(r2_rf)[0:5], str(r2_ad)[0:5]],\n    \"RMSE\" : [str(rmse_xgb)[0:8], str(rmse_gbm)[0:8], str(rmse_lin)[0:8], str(rmse_dtr)[0:8], str(rmse_rf)[0:8], str(rmse_ad)[0:8]],\n    \"Cross Validation\" : [str(np.mean(scores_xgb)), str(np.mean(scores_gmb)), str(np.mean(scores_lin)),str(np.mean(scores_dtr)),str(np.mean(scores_rf)),str(np.mean(scores_ad))]\n})\nmodel_performances.round(4)\n\nprint(\"Sorted by Best Score:\")\nmodel_performances.sort_values(by=\"Best Score\", ascending=False)","757be527":"print(\"Sorted by R Squared:\")\nmodel_performances.sort_values(by=\"R Squared\", ascending=False)","8d225dbf":"print(\"Sorted by RMSE:\")\nmodel_performances.sort_values(by=\"RMSE\", ascending=True)","fb0115cc":"print(\"Sorted by Cross Validation:\")\nmodel_performances.sort_values(by=\"Cross Validation\", ascending=False)","8440d323":"gbm.fit(X_train, y_train)\nxgb.fit(X_train, y_train)\nrf.fit(X_train, y_train)","6ba6295a":"ensemble = np.exp(gbm.predict(Test))*0.45 + np.exp(xgb.predict(Test))*0.35 + np.exp(rf.predict(Test))*0.25\nensemble","ab8e27f3":"submission = pd.DataFrame({\n        \"Id\": test_id,\n        \"SalePrice\": ensemble\n    })\n\nsubmission.to_csv(\"prices.csv\", index=False)\nsubmission.head(5)","c8041e26":"\n\nChecking performance of base models by evaluating the cross-validation RMSLE error.\n","c1074f06":"**Log Transform for SalePrice**\n\nSo we should apply logarithmic transformation to our target variable, so that  the regression models will work better.","9468010e":"Turning numeric dates and classes into strings","c48fe24d":"XGB","56de752f":"\n\nChecking performance of base models by evaluating the cross-validation RMSLE error.\n","70f245a4":"Here we can see PoolQC, MiscFeature, Alley, Fence, FireplaceQu, LotFrontage are features with a large amouunt of missing values. We can also see that other variables have some missing values that we need to fill in.","d665093a":"Finished Filling in all of our NaN values and not its time to look at our Prediction Variable","ec9baa31":"Random Forest","bbe3fbd9":"GBM","7b6e6826":"\n\nChecking performance of base models by evaluating the cross-validation RMSLE error.\n","438fc56e":"**Label Encoder**\n\nConvert this kind of categorical text data into model-understandable numerical data, we use the Label Encoder class.","d172ea41":"Linear Regression","44f572e6":"From the plot we can see the target prediction variable SalePrice is not distributed normally. This may reduce the performance of regression models because some of them assume normal distribution.\n\nSo we should log transform.","54e4dc40":"AdaBoost","61aa1dc1":"**Dummy Variables**\n\nIn statistics and econometrics, particularly in regression analysis, a dummy variable is one that takes only the value 0 or 1 to indicate the absence or presence of some categorical effect that may be expected to shift the outcome.","489fbebb":"Now compiling all the scores and comparing the models.","b428b37f":"\n\nChecking performance of base models by evaluating the cross-validation RMSLE error.\n","42d9bad0":"DecisionTree Regression","b92b8a3c":"**Spliting train and test data for models**\n\nSplit DF into Train and Test and then again from train test split","039b127d":"For this competiton, we are given a data set of 1460 homes, with different class types: float, integer, and categorical. We are tasked with building a regression model to estimate a home's sale price.\n\nWe will be doing this by using XGB, GradientBoosting, Linear Regression, Decision Tree, RandomForest and AdaBoost","b3e96778":"**Filling missing values**\n\nThe features which are categorical with missing values can be replaced with \"None\". \nThis is also the same for PoolQC, Fence and FireplaceQu, Even though it says NaN it implies that the house does not contain that feature.\n","eba7351f":"Dropping SalePrice as we are going to Predict the values for it.","38f4f31b":"**Model Selection and Predictions**\n","b6e4acf7":"\n\nChecking performance of base models by evaluating the cross-validation RMSLE error.\n","99edf5a1":"\n\nChecking performance of base models by evaluating the cross-validation RMSLE error.\n","a5bcc64d":"From this we see GBM > XGB > RF > LR > ADA > DT.\nBut the parameters were not optimised.\n\nNow we create our own ensemble by taking top 3","76a93f50":"**Calculating percentage of missing values.**\n\nNot all data is always complete, We are checking to see if there are any missing values"}}