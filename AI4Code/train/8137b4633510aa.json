{"cell_type":{"abe8ca4f":"code","4c14f742":"code","ec33c574":"code","bc5007fc":"code","7734e075":"code","b881168c":"code","5a12f99a":"code","5dad1c97":"code","0edb9de1":"code","cdbabd22":"code","c16b0c99":"code","977a5e72":"code","4b58277d":"code","5f69baee":"code","b9991daf":"code","3e4fea43":"markdown","444812e6":"markdown","90b9090e":"markdown","36fcb126":"markdown","9bb9621e":"markdown"},"source":{"abe8ca4f":"import os\nimport glob\nfrom joblib import Parallel, delayed\nimport pandas as pd\nimport numpy as np\nimport scipy as sc\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 300)\nfrom tqdm import tqdm\nimport time","4c14f742":"# \u7d30\u304b\u3044\u95a2\u6570\n# data directory\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'","ec33c574":"# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test\n\n# Read train and test\ntrain, test = read_train_test()","bc5007fc":"#stocklist = train['stock_id'].unique()\nstocklist = [0,1] #\u8efd\u91cf\u5316\u306e\u305f\u3081\u30012\u3064\u306e\u307f\nlen(stocklist)","7734e075":"#\u73fe\u5728\u4fa1\u683c\u306e\u8a08\u7b97\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap","b881168c":"pricelist = ['bid_price1','ask_price1','bid_price2','ask_price2','wap1','wap2']\ncolumns = ['stock_id','time_id']\nfor price in pricelist:\n    element = [f'{price}_{sec}' for sec in range(0,600)]\n    columns.extend(element)","5a12f99a":"def get_time_alldata(b_df,stock_id,time_id,columns):\n    tmp = pd.DataFrame(index=range(0,600)).rename_axis('seconds_in_bucket')\n    tmp = pd.concat([tmp,b_df[b_df['time_id'] == time_id].set_index('seconds_in_bucket')],axis=1)\n    tmp = tmp.fillna(method='ffill').fillna(method='bfill').reset_index()\n    tmp_row = []\n    for price in pricelist:\n        tmp_row.extend(tmp[price].to_list())\n    tmp_row = [stock_id,time_id] + tmp_row\n    tmp_df = pd.DataFrame(tmp_row, index=columns).T\n    return tmp_df","5dad1c97":"all_stock_df = pd.DataFrame()\nfor stock_id in tqdm(stocklist):\n    b_df = pd.read_parquet(data_dir+f'book_train.parquet\/stock_id={stock_id}')\n    b_df['wap1'] = calc_wap1(b_df)\n    b_df['wap2'] = calc_wap2(b_df)\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(get_time_alldata)(b_df,stock_id,time_id,columns) for time_id in b_df['time_id'].unique())\n    df = pd.concat(df, ignore_index = True)\n    all_stock_df = pd.concat([all_stock_df, df],ignore_index=True)\ndel df,b_df","0edb9de1":"from sklearn.decomposition import PCA\n#pca_feats_std = all_stock_df.filter(like=pricelist[1],axis=1).apply(lambda x: (x-x.mean())\/x.std(), axis=0)\nn_components=20\npca = PCA(n_components=n_components,random_state=57)\n#pca_feats = pca_feats_df.drop(['stock_id', 'time_id'], axis=1)\n#\u6a19\u6e96\u5316\u3057\u3066\u3082\u3057\u306a\u304f\u3066\u3082\u826f\u3044\npca_feats_std = all_stock_df.drop(['stock_id', 'time_id'], axis=1).apply(lambda x: (x-x.mean())\/x.std(), axis=0)\npca.fit(pca_feats_std)\nfeature = pca.transform(pca_feats_std)\nfeature = pd.DataFrame(feature, columns=[\"PC{}\".format(x + 1) for x in range(n_components)])\ndel pca_feats_std\n\n#\u5bc4\u4e0e\u7387\ndisplay(pd.DataFrame(pca.explained_variance_ratio_, index=[\"PC{}\".format(x + 1) for x in range(n_components)]).head())\n#\u56fa\u6709\u5024\ndisplay(pd.DataFrame(pca.explained_variance_, index=[\"PC{}\".format(x + 1) for x in range(n_components)]).head())","cdbabd22":"feature.loc[:,['stock_id','time_id']] = all_stock_df[['stock_id','time_id']]","c16b0c99":"#\u672c\u6765\u306a\u3089train\u306bjoin\u3057\u305f\u65b9\u304c\u826f\u3044\u304c\u3001\u4eca\u306f\u6b20\u640d\u304c\u51fa\u308b\u305f\u3081leftjoin\u306b\u3057\u3066\u3044\u308b\ntrain = train[train['stock_id'].isin(stocklist)]\ntrain = train.merge(all_stock_df,how='left',on=['stock_id','time_id'])\ntrain = train.merge(feature, how='left', on=['stock_id','time_id'])","977a5e72":"def rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","4b58277d":"# Hyperparammeters (just basic)\nparams = {\n  'objective': 'rmse',  \n  'boosting_type': 'gbdt',\n  'num_leaves': 100,\n  'n_jobs': -1,\n  'learning_rate': 0.1,\n  'feature_fraction': 0.8,\n  'bagging_fraction': 0.8,\n  'verbose': -1\n}","5f69baee":"# Split features and target\nx = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\n#x_test = test.drop(['row_id', 'time_id'], axis = 1)\n# Transform stock id to a numeric value\nx['stock_id'] = x['stock_id'].astype(int)\n#x_test['stock_id'] = x_test['stock_id'].astype(int)\n\n# Create out of folds array\noof_predictions = np.zeros(x.shape[0])\n# Create test array to store predictions\n#test_predictions = np.zeros(x_test.shape[0])\n# Create a KFold object\nkfold = KFold(n_splits = 5, random_state = 66, shuffle = True)\n# Iterate through each fold\nmodellist = []\nfor fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n    print(f'Training fold {fold + 1}')\n    x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n    y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n    # Root mean squared percentage error weights\n    train_weights = 1 \/ np.square(y_train)\n    val_weights = 1 \/ np.square(y_val)\n    train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n    val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n    model = lgb.train(params = params, \n                      train_set = train_dataset, \n                      valid_sets = [train_dataset, val_dataset], \n                      num_boost_round = 10000, \n                      early_stopping_rounds = 50, \n                      verbose_eval = 50,\n                      feval = feval_rmspe)\n    modellist.append(model)\n    # Add predictions to the out of folds array\n    oof_predictions[val_ind] = model.predict(x_val)\n    # Predict the test set\n    #test_predictions += model.predict(x_test) \/ 5\n\nrmspe_score = rmspe(y, oof_predictions)\nprint(f'Our out of folds RMSPE is {rmspe_score}')","b9991daf":"PC_cols = train.filter(like='PC',axis=1).columns.to_list()\n# Split features and target\nx = train.drop(['row_id', 'target', 'time_id']+PC_cols, axis = 1)\ny = train['target']\n#x_test = test.drop(['row_id', 'time_id'], axis = 1)\n# Transform stock id to a numeric value\nx['stock_id'] = x['stock_id'].astype(int)\n#x_test['stock_id'] = x_test['stock_id'].astype(int)\n\n# Create out of folds array\noof_predictions = np.zeros(x.shape[0])\n# Create test array to store predictions\n#test_predictions = np.zeros(x_test.shape[0])\n# Create a KFold object\nkfold = KFold(n_splits = 5, random_state = 66, shuffle = True)\n# Iterate through each fold\nmodellist = []\nfor fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n    print(f'Training fold {fold + 1}')\n    x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n    y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n    # Root mean squared percentage error weights\n    train_weights = 1 \/ np.square(y_train)\n    val_weights = 1 \/ np.square(y_val)\n    train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n    val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n    model = lgb.train(params = params, \n                      train_set = train_dataset, \n                      valid_sets = [train_dataset, val_dataset], \n                      num_boost_round = 10000, \n                      early_stopping_rounds = 50, \n                      verbose_eval = 50,\n                      feval = feval_rmspe)\n    modellist.append(model)\n    # Add predictions to the out of folds array\n    oof_predictions[val_ind] = model.predict(x_val)\n    # Predict the test set\n    #test_predictions += model.predict(x_test) \/ 5\n\nrmspe_score = rmspe(y, oof_predictions)\nprint(f'Our out of folds RMSPE is {rmspe_score}')","3e4fea43":"## \u30e2\u30c7\u30eb\u4f5c\u6210","444812e6":"## PCA","90b9090e":"## PCA\u306a\u3057","36fcb126":"## Train\u30c7\u30fc\u30bf\u306e\u4f5c\u6210","9bb9621e":"## PCA\u3042\u308a"}}