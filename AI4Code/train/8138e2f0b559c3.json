{"cell_type":{"54b614c7":"code","98676e0c":"code","c7dc0aaf":"code","8b248879":"code","47370551":"code","b871a0bc":"code","20426d38":"code","e39654d8":"code","b6f28804":"code","49915fe2":"code","c4361b9d":"code","f041007a":"code","a12773a2":"code","2e2ab690":"code","7cee36d6":"code","648c96fb":"code","6c417ed1":"code","66d6b668":"code","8c797ada":"code","d28f256e":"code","7819c880":"code","9bbdfdd4":"markdown","5af8ebad":"markdown","65a9777c":"markdown"},"source":{"54b614c7":"import pandas as pd\nimport re","98676e0c":"df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ndf.head()","c7dc0aaf":"# select particular columns\ndf=df[['text','target']]\ndf.head()","8b248879":"test=df['text'].sample(5).to_list()\ntest","47370551":"import nltk\nfrom nltk.corpus import stopwords\n\nstop_words = stopwords.words('english')","b871a0bc":"stop_words[:10]","20426d38":"\"\"\"\n->  Text cleaning \n        * Remove urls,hastags,usernames etc\n        * Remove stopwords like ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves'] etc .,\n\"\"\"\n\n\ndef remove_stopwords(text):\n    text = ' '.join([i for i in text.split(' ') if i not in stop_words ])\n    return text\n\n\ndef clean_text(text):\n    text = re.sub('https?:\/\/[A-Za-z0-9.\/]*','', text) # Remove https..(URL)\n    text = re.sub('RT @[\\w]*:','', text) # Removed RT \n    text = re.sub('@[A-Za-z0-9_]+', '', text) # Removed @mention\n    text = re.sub('#', '', text) # hastag into text\n    text = re.sub('&amp; ','',text) # Removed &(and) \n    text = re.sub(\"[^a-zA-Z0-9_']\",' ',text) # remove punctuation\n    text = re.sub('[0-9]*','', text) # Removed digits\n   \n    text = text.strip().lower()\n    text = remove_stopwords(text)\n    return text\n\nfor i in test:\n    print(i)\n    print(\"-\"*10)\n    print(\"clean_text \\n\")\n    print(clean_text(i))\n    print(\"\\n\")\n    print(\"*\"*10)","e39654d8":"from nltk.stem import WordNetLemmatizer\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = WordNetLemmatizer()\n\ndef lemmatize_text(text):\n    return ' '.join([lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)])\n\nlemmatize_text(\"Apples and oranges are similar. Boots and hippos aren't.\")\n","b6f28804":"df['clean_text'] = df['text'].apply(clean_text).apply(lemmatize_text)\ndf_test['text'] = df_test['text'].apply(clean_text).apply(lemmatize_text)\ndf['text']=df['clean_text']\ndf=df[['text','target']]\ndf.head()","49915fe2":"df['target'].value_counts()","c4361b9d":"df.shape","f041007a":"\n# Term Frequency \u2013 Inverse Document\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\ncv=CountVectorizer() ","a12773a2":"\ncount_vectorizer = CountVectorizer(stop_words=stop_words)\ntrain_vectors = count_vectorizer.fit_transform(df['text']).toarray()\n\ntest_vectors = count_vectorizer.fit_transform(df_test['text']).toarray()","2e2ab690":"\nfrom collections import Counter \nwords=Counter(count_vectorizer.vocabulary_)\nwords.most_common(5)","7cee36d6":"tf_idf_ngram = TfidfVectorizer(ngram_range=(1,2))\ntf_idf_ngram.fit(df.text)\nx_train_tf_bigram = tf_idf_ngram.transform(df.text)\nx_test_tf_bigram = tf_idf_ngram.transform(df_test.text)","648c96fb":"tf_idf_words=Counter(tf_idf_ngram.vocabulary_)\ntf_idf_words.most_common(5)","6c417ed1":"x_train_tf_bigram.shape,x_test_tf_bigram.shape","66d6b668":"from sklearn.model_selection import train_test_split\n\nX = x_train_tf_bigram\ny = df.target.values\n\n# Train-Test Splitting\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","8c797ada":"from sklearn.linear_model import LogisticRegression\n\nclf_LR = LogisticRegression(C=2,dual=True, solver='liblinear',random_state=0)\nclf_LR.fit(X_train,y_train)\n\ny_pred_LR = clf_LR.predict(X_test)","d28f256e":"from sklearn.metrics import accuracy_score , confusion_matrix, f1_score\naccuracy = accuracy_score(y_test, y_pred_LR) * 100\nf1score = f1_score(y_test, y_pred_LR) * 100\n\nprint(f\"Logistic Regression Accuracy: {round(accuracy,2)} %\")\nprint(f\"Logistic Regression F1 Score: {round(f1score,2)} %\")","7819c880":"submission =pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission['target']=clf_LR.predict(x_test_tf_bigram)\nsubmission.to_csv(\"\/kaggle\/submission.csv\",index=False)\nsubmission.head()","9bbdfdd4":"# Term Frequency \u2013 Inverse Document\n\n\n    - Term Frequency: This summarizes how often a given word appears within a document.\n    - Inverse Document Frequency: This downscales words that appear a lot across documents.\n\n","5af8ebad":"# Simple Text Classifications","65a9777c":"# CountVectorizer"}}