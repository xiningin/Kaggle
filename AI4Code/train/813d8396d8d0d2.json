{"cell_type":{"57e12a38":"code","6fcd2fab":"code","47ad7156":"code","66bd6adc":"code","c582fbf7":"code","8b66dc47":"code","11196ad3":"code","85b407c6":"code","7c1f3379":"code","ba61d809":"code","5e7515e2":"code","1625a110":"markdown","c5a3bee5":"markdown","e62a685c":"markdown","479ca769":"markdown","1548b434":"markdown","5a1a5c6a":"markdown","95f5d32c":"markdown","df786604":"markdown","5c594ac8":"markdown","09dcbbca":"markdown","ab0b8f95":"markdown","38e242d7":"markdown","25cac6da":"markdown","847d540a":"markdown","b0ca7f7d":"markdown","45734068":"markdown","23836d50":"markdown","7eefb81b":"markdown","dce6330f":"markdown","d3df712b":"markdown","fd8d4663":"markdown"},"source":{"57e12a38":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport missingno\nnetflix_data = pd.read_csv('\/kaggle\/input\/netflix-shows\/netflix_titles.csv')\nnetflix_data.info()","6fcd2fab":"missingno.bar(netflix_data,figsize=(12,5))","47ad7156":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel \n\ntf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\ntfidf_matrix = tf.fit_transform(netflix_data['description'])","66bd6adc":"cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix) \nresults = {}\nfor idx, row in netflix_data.iterrows():\n    similar_indices = cosine_similarities[idx].argsort()[:-100:-1] \n    similar_items = [(cosine_similarities[idx][i], netflix_data['show_id'][i]) for i in similar_indices] \n    results[row['show_id']] = similar_items[1:]","c582fbf7":"def item(id):  \n    return netflix_data.loc[netflix_data['show_id'] == id]['title'].tolist()[0].split(' - ')[0] \n\n# Just reads the results out of the dictionary.def \ndef recommend(item_id, num):\n    print(\"Recommending \" + str(num) + \" products similar to \" + item(item_id) + \"...\")   \n    print(\"-------\")    \n    recs = results[item_id][:num] \n    for rec in recs: \n        print(\"Recommended: \" + item(rec[1]) + \" (score:\" +      str(rec[0]) + \")\")","8b66dc47":"recommend('s1305',6)","11196ad3":"!pip install -U sentence-transformers","85b407c6":"from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('paraphrase-distilroberta-base-v1')","7c1f3379":"import numpy as np\ndescriptions = netflix_data['description'].tolist()\n# print(descriptions)\ndes_embeddings = []\nfor i,des in enumerate(descriptions):\n    des_embeddings.append(model.encode(des))\n    ","ba61d809":"import numpy as np\ndes_embeddings = np.load('..\/input\/netflix-descriptions-bert-embeddings\/descriptions_embeddings.npy')","5e7515e2":"import torch\nfrom sentence_transformers import SentenceTransformer, util\n\ndef recommend(query):\n    #Compute cosine-similarities with all embeddings \n    query_embedd = model.encode(query)\n    cosine_scores = util.pytorch_cos_sim(query_embedd, des_embeddings)\n    top5_matches = torch.argsort(cosine_scores, dim=-1, descending=True).tolist()[0][1:6]\n    return top5_matches\n\nid = 's1305'\nquery_show_des = netflix_data.loc[netflix_data['show_id'] == id]['description'].to_list()[0]\nrecommendded_results = recommend(query_show_des)\n\nfor index in recommendded_results:\n    print(netflix_data.iloc[index,:])\n\n","1625a110":"With SentenceTransformer('paraphrase-distilroberta-base-v1') we define which sentence transformer model we like to load. In this example, we load paraphrase-distilroberta-base-v1, which is a DistilBERT-base-uncased model fine tuned on a large dataset of paraphrase sentences.","c5a3bee5":"<center> <h3><b> Please upvote if you liked this approach. In case of improvements or suggestion write it in comments. <\/b><\/h3> <\/center> ","e62a685c":"* **Items (also known as documents)**:\nThe entities a system recommends.\n* **Query (also known as context)**:\nThe information a system uses to make recommendations.\n* **Embedding**:\nA mapping from a discrete set (in this case, the set of queries, or the set of items to recommend) to a vector space called the embedding space. Many recommendation systems rely on learning an appropriate embedding representation of the queries and items","479ca769":"> Find Embeddings for all show descriptions in dataset.","1548b434":"* **Content based Filtering**: Uses similarity between items to recommend items similar to what the user likes.\n* **Collaborative Filtering**: Uses similarities between queries and items simultaneously to provide recommendations.\n\nHere we are going to use description column for recommending movie. since we are recommending based on what the user has watched this comes under content based filtering","5a1a5c6a":"Term Frequency-inverse document frequency (or TF-idf) is an established technique for scoring document similarity based on the importance of the words that they share.","95f5d32c":"**<h4>Cosine Similarity<h4>**It computes the L2-normalized dot product of vectors. This is called cosine similarity, because Euclidean (L2) normalization projects the vectors onto the unit sphere, and their dot product is then the cosine of the angle between the points denoted by the vectors.This kernel is a popular choice for computing the similarity of documents represented as tf-idf vectors.","df786604":"<center><h3><b>A Recommender System that recommends movies based on movie description.<\/b><\/h3><\/center>","5c594ac8":"<h3>Check for missing data in description column<h3> ","09dcbbca":"> **From above results we can see that the recommendations for chef's table were not close to the show's theme, so there is a need to improve the embeddings representation. We now use sentence transformers to respresent descriptions of show.**","ab0b8f95":"<h3>This framework provides an easy method to compute dense vector representations for sentences, paragraphs, and images. The models are based on transformer networks like BERT \/ RoBERTa \/ XLM-RoBERTa etc.<\/h3>","38e242d7":"<h4>For a query show id lets find the top five shows with highest cosine similarity.<\/h4>","25cac6da":"<h2> Types of Recommendation Engines <\/h2>","847d540a":"# **TF-IDF Based**","b0ca7f7d":"# **Sentence Transformer Based**","45734068":"<html>\n<style>\nh2 {\n  text-align: center;\n} <\/style>\n<h2 text-align:center > Recommendation Systems - Terminology<\/h2>\n<\/html>","23836d50":"Above results show that the top 5 recommendations of the show are:\n\n1. **Chef's Table: France**\n2. **Rotten**\n3. **The Mind of a Chef**\n4. **Chef's Table: BBQ**\n5. **The Chef Show**\n\nThe recommendations are now more close to the query.","7eefb81b":"<h3>Load csv into Pandas Dataframe<h3>","dce6330f":"<center><img src = \"https:\/\/wallpapercave.com\/wp\/wp5063342.png\" width=\"650\"><\/center>","d3df712b":"<h4> <b> *** Run the below cell only to find embeddings for each description. ***<\/b> <br> I have added the embeddings as npy file seperately skip to next cell.<\/h4>","fd8d4663":"<h3>From above barplot there is no misisng data in description column, so no need of any missing data handling.<h3>"}}