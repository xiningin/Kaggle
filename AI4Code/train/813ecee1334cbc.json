{"cell_type":{"b7516591":"code","597b1479":"code","6d0fbacf":"code","410f5d1e":"code","1e93faf8":"code","3315b9e0":"code","89d46112":"code","ca3524e8":"code","76c776c9":"code","6862b23f":"code","dc203cfe":"code","5e4844f6":"code","37e9c44b":"code","7b544245":"code","90fa27fa":"code","6765bf8c":"code","255928da":"code","5e8d89c1":"code","f90fed4d":"code","a1137db8":"code","a48a6027":"code","86183a12":"code","66ff668d":"code","54b45a9a":"code","4fa0fab7":"code","298c3e13":"code","ca74f4dd":"code","73ce2483":"code","64292693":"code","c85594ee":"code","3080c165":"code","4000a609":"code","20010ce1":"code","1c72a62c":"code","25602047":"code","52b58c49":"code","0a67de9f":"code","2be52a56":"code","469be692":"markdown","1a8aad32":"markdown","6e1d9e91":"markdown","eba0c05b":"markdown","58163623":"markdown","4ff94c4a":"markdown","648a4683":"markdown","50e35589":"markdown","d2b09f90":"markdown","768b0b23":"markdown","b860b836":"markdown","65a8551d":"markdown","d39a36cf":"markdown","7138cef0":"markdown","f1ecba51":"markdown","f3fbd09b":"markdown","1f0d871f":"markdown","665dd2fc":"markdown","01b574d1":"markdown","48484deb":"markdown"},"source":{"b7516591":"import numpy as np\nimport pandas as pd\nimport os\nimport json\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n%matplotlib inline \nfrom wordcloud import WordCloud, STOPWORDS\nfrom joblib import Parallel, delayed\nimport tqdm\nimport jieba\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, f1_score, recall_score","597b1479":"data_df = pd.read_csv(\"..\/input\/chinese-official-daily-news-since-2016\/chinese_news.csv\")","6d0fbacf":"print(f\"Rows: {data_df.shape[0]}, Cols: {data_df.shape[1]}\")","410f5d1e":"data_df.head()","1e93faf8":"print(f\"Samples with content null: {data_df.loc[data_df['content'].isnull()].shape[0]}\")","3315b9e0":"print(f\"Samples with headline null: {data_df.loc[data_df['headline'].isnull()].shape[0]}\")","89d46112":"data_df = data_df.loc[~data_df['content'].isnull()]","ca3524e8":"print(f\"New data shape: {data_df.shape}\")","76c776c9":"!wget https:\/\/github.com\/adobe-fonts\/source-han-sans\/raw\/release\/SubsetOTF\/SourceHanSansCN.zip\n!unzip -j \"SourceHanSansCN.zip\" \"SourceHanSansCN\/SourceHanSansCN-Regular.otf\" -d \".\"\n!rm SourceHanSansCN.zip\n!ls","6862b23f":"import matplotlib.font_manager as fm\nfont_path = '.\/SourceHanSansCN-Regular.otf'\nprop = fm.FontProperties(fname=font_path)","dc203cfe":"def plot_count(feature, title, df, font_prop=prop, size=1):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    ax.set_xticklabels(ax.get_xticklabels(), fontproperties=font_prop);\n    plt.show()    ","5e4844f6":"plot_count('tag', 'tag (all data)', font_prop=prop, df=data_df,size=1.2)","37e9c44b":"data_df['datetime'] = data_df['date'].apply(lambda x: pd.to_datetime(x))\ndata_df['year'] = data_df['datetime'].dt.year\ndata_df['month'] = data_df['datetime'].dt.month\ndata_df['dayofweek'] = data_df['datetime'].dt.dayofweek","7b544245":"def jieba_cut(x, sep=' '):\n    return sep.join(jieba.cut(x, cut_all=False))\n\nprint('raw', data_df['headline'][0])\nprint('cut', jieba_cut(data_df['headline'][0], ', '))","90fa27fa":"%%time\ndata_df['headline_cut'] = Parallel(n_jobs=4)(\n    delayed(jieba_cut)(x) for x in tqdm.tqdm_notebook(data_df['headline'].values)\n)","6765bf8c":"%%time\ndata_df['content_cut'] = Parallel(n_jobs=4)(\n    delayed(jieba_cut)(x) for x in tqdm.tqdm_notebook(data_df['content'].values)\n)","255928da":"prop = fm.FontProperties(fname=font_path, size=20)","5e8d89c1":"\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, font_path=font_path, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        font_path=font_path,\n        max_words=50,\n        max_font_size=40, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(10,10))\n    plt.axis('off')\n    if title: \n        prop = fm.FontProperties(fname=font_path)\n        fig.suptitle(title, fontsize=40, fontproperties=prop)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","f90fed4d":"show_wordcloud(data_df['headline_cut'], font_path, title = 'Prevalent words in headline, all data')","a1137db8":"data_df.tag.unique()","a48a6027":"data_tag_df = data_df.loc[data_df.tag=='\u8be6\u7ec6\u5168\u6587']\nshow_wordcloud(data_tag_df['headline_cut'], font_path, title = 'Prevalent words in headline, tag=\u8be6\u7ec6\u5168\u6587')","86183a12":"data_tag_df = data_df.loc[data_df.tag=='\u56fd\u5185']\nshow_wordcloud(data_tag_df['headline_cut'], font_path, title = 'Prevalent words in headline, tag=\u56fd\u5185')","66ff668d":"data_tag_df = data_df.loc[data_df.tag=='\u56fd\u9645']\nshow_wordcloud(data_tag_df['headline_cut'], font_path, title = 'Prevalent words in headline, tag=\u56fd\u9645')","54b45a9a":"train_df, test_df = train_test_split(data_df, test_size = 0.2, random_state = 42) ","4fa0fab7":"print(f\"train: {train_df.shape}, test: {test_df.shape}\")","298c3e13":"plot_count('tag', 'tag (train)', font_prop=prop, df=train_df,size=1.2)","ca74f4dd":"plot_count('tag', 'tag (test)', font_prop=prop, df=test_df,size=1.2)","73ce2483":"train_df.head()","64292693":"def count_vect_feature(feature, df, max_features=5000):\n    start_time = time.time()\n    cv = CountVectorizer(max_features=max_features,\n                             ngram_range=(1, 1),\n                             stop_words='english')\n    X_feature = cv.fit_transform(df[feature])\n    print('Count Vectorizer `{}` completed in {} sec.'.format(feature, round(time.time() - start_time,2)))\n    return X_feature, cv","c85594ee":"X_headline, cv = count_vect_feature('headline_cut', train_df, 20000)","3080c165":"X_content, cv = count_vect_feature('content_cut', train_df, 30000)","4000a609":"target =  'tag'\nX = X_content\ny = train_df[target].values\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size = 0.2, random_state = 42) ","20010ce1":"train_X.shape, valid_X.shape, train_y.shape, valid_y.shape","1c72a62c":"%%time\nclf_svc = SVC(kernel='linear')\nclf_svc = clf_svc.fit(train_X, train_y)","25602047":"def show_confusion_matrix(valid_y, predicted, size=1, font_prop=prop, trim_labels=False):\n    mat = confusion_matrix(valid_y, predicted)\n    plt.figure(figsize=(4*size, 4*size))\n    f, ax = plt.subplots(1,1, figsize=(4*size,4*size))\n    sns.set()\n    target_labels = np.unique(valid_y)\n    if(trim_labels):\n        target_labels = [x[0:70] for x in target_labels]\n    sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n                xticklabels=target_labels,\n                yticklabels=target_labels\n               )\n    ax.set_xticklabels(ax.get_xticklabels(), fontproperties=font_prop);\n    ax.set_yticklabels(ax.get_yticklabels(), fontproperties=font_prop);\n    plt.xlabel('true label')\n    plt.ylabel('predicted label')\n    plt.show()","52b58c49":"predicted_valid = clf_svc.predict(valid_X)\nprediction_acc = np.mean(predicted_valid == valid_y)\nprediction_f1_score = f1_score(valid_y, predicted_valid, average='weighted')\nprediction_recall = recall_score(valid_y, predicted_valid, average='weighted')\nprint(\"Valid:\\n========================================================\")\nprint(f\"Feature: {target} \\t| Prediction accuracy: {prediction_acc}\")\nprint(f\"Feature: {target} \\t| Prediction F1-score: {prediction_f1_score}\")\nprint(f\"Feature: {target} \\t| Prediction recall: {prediction_recall}\")\nshow_confusion_matrix(valid_y, predicted_valid, font_prop=prop,size=1.5)\nprint(classification_report(valid_y, predicted_valid))","0a67de9f":"%%time\nclf_nb = MultinomialNB(fit_prior='true')\nclf_nb = clf_nb.fit(train_X, train_y)","2be52a56":"predicted_valid = clf_nb.predict(valid_X)\nprediction_acc = np.mean(predicted_valid == valid_y)\nprediction_f1_score = f1_score(valid_y, predicted_valid, average='weighted')\nprediction_recall = recall_score(valid_y, predicted_valid, average='weighted')\nprint(\"Valid:\\n========================================================\")\nprint(f\"Feature: {target} \\t| Prediction accuracy: {prediction_acc}\")\nprint(f\"Feature: {target} \\t| Prediction F1-score: {prediction_f1_score}\")\nprint(f\"Feature: {target} \\t| Prediction recall: {prediction_recall}\")\nshow_confusion_matrix(valid_y, predicted_valid, font_prop=prop,size=1.5)\nprint(classification_report(valid_y, predicted_valid))","469be692":"## MultinomialNB model","1a8aad32":"# Model","6e1d9e91":"## Load data\n\nThe data contains news articles in Chinese simplified.","eba0c05b":"## Glimpse the data","58163623":"Let's drop the rows will null content. We will not include in the analysis the samples with articles without content.","4ff94c4a":"### Model validation","648a4683":"## Target distribution","50e35589":"We show now the most frequent groups of ideograms grouped by target value (or tag).","d2b09f90":"Then we load the font from the downloaded resource.","768b0b23":"## Date info extraction\n\n\nWe extract from the data the year, month and day of week.","b860b836":"## SVC Model\n\n\nWe use first a SVC model (with linear kernel).\n\n\n### Model fit","65a8551d":"# Split data to train-test","d39a36cf":"# Text preprocessing","7138cef0":"# Data exploration\n\n","f1ecba51":"# Introduction\n\n\nIn this Kernel we introduce a general ML pipeline for text classification, with focus on what is specific for Chinese text.\n\n\n# Analysis preparation\n\n## Load packages\n\nMost of the packages are usual ones used for simple NLP and classification; in this case, we are also imported **jieba**, a package for Chinese language basic NLP.\n","f3fbd09b":"# Conclusion\n\n\nWe used two different models: MultinomialNB (based on Naive Bayes) and SCV (based on SVM).\n\nSVC model performed better, with weighted and macro average scores for performance, recall and f1-score (and corresponding scores per class) better than for MultinomialNB scores.","1f0d871f":"## Cut phrases in ideograms groups\n\nChinese does not have flexionary forms and also does not use spaces between ideograms to mark separate words. In the same time, some concepts are using 2 ore more ideograms in a sequence. The reader will `cut` in mind during reading the sequences of ideograms in groups, corresponding to different concepts, based on context. \nWe will use **jieba** library to separate the ideograms in groups.","665dd2fc":"We apply now the above defined function to the whole dataset. We are doing this for both the content and headline features.","01b574d1":"## Visualization utility\n\n\nWe download the fonts to display the Chinese characters.","48484deb":"After we cut the ideograms sequences in groups (each corresponding to one concept - or token) we represent the most frequenty used with wordclouds."}}