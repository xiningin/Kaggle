{"cell_type":{"2d6e3b5d":"code","69cdc6cf":"code","c0055330":"code","b352f323":"code","bd29ee98":"code","d66a35cb":"code","8b0422ed":"code","b4f9076c":"code","74a5d950":"code","0c4d1c7f":"code","2ba909ab":"code","823f25f0":"code","0c8b5168":"code","43a6bbd6":"code","3e654d3b":"code","bdb20a57":"code","5422fd96":"code","6af8bea8":"code","15bb9723":"code","4478e738":"code","8e6836b1":"code","748fba60":"code","529ae1f3":"code","dc238cfd":"code","a941e31d":"code","779fd69f":"code","edaf50f2":"code","aa874924":"code","d33a72a2":"code","c35b6e87":"code","862794aa":"code","b263a5c8":"code","861067ee":"code","32e74ceb":"code","ad5d32ed":"code","95a86ae6":"code","3f971188":"code","d7f64ccb":"code","e9be2298":"code","192ae312":"code","3da9e128":"code","12f2e141":"code","345e46f1":"code","92b4dc7a":"code","ec9a3884":"code","bc267eaf":"code","37dfce0f":"code","f5066ee6":"code","4165fb66":"code","218a9359":"code","1e32c1af":"code","0426a478":"code","6985ae1b":"code","2329b38c":"code","cbe87c62":"code","00a3f392":"code","0f67ae5a":"code","daa95ff5":"code","3e14b09a":"code","57c613fa":"code","a0cd83b0":"code","08b952a7":"code","af91bce4":"code","0ed82528":"markdown","8cb1196c":"markdown","aef95851":"markdown","ea800f24":"markdown","4a3b7bb8":"markdown","d175e881":"markdown"},"source":{"2d6e3b5d":"import gc\nimport os\nimport logging\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score , roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nwarnings.filterwarnings('ignore')","69cdc6cf":"PATH = '..\/input\/santander-customer-transaction-prediction\/'","c0055330":"# Load the train and test data files.\n\n%time\ntrain_df = pd.read_csv(PATH +'train.csv')\ntest_df = pd.read_csv(PATH + 'test.csv')","b352f323":"# Data Exploration\n\ntrain_df.shape , test_df.shape","bd29ee98":"# Glimpse train and test dataset\n\ntrain_df.head()","d66a35cb":"test_df.head()","8b0422ed":"def missing_data(data):\n    \n    total = data.isnull().sum()\n    percent = (data.isnull().sum() \/ data.isnull().count() * 100 )\n    tt = pd.concat([total , percent] , axis = 1 , keys = ['Total','Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['types'] = types\n    return np.transpose(tt)","b4f9076c":"%time\nmissing_data(train_df)","74a5d950":"# Check Test dataset\n\n%time\nmissing_data(test_df)","0c4d1c7f":"%time\ntrain_df.describe()","2ba909ab":"%time\ntest_df.describe()","823f25f0":"# standard deviation is relatively large for both train and test variable data;\n# min, max, mean, sdt values for train and test data looks quite close;\n# mean values are distributed over a large range.","0c8b5168":"# ScatterPlot for train and test Dataset\n\ndef plot_feature_scatter(df1 , df2 ,features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig , ax = plt.subplots(4,4,figsize = (14,14))\n\n    for feature in features:\n        i += 1\n        plt.subplot(4,4,i)\n        plt.scatter(df1[feature], df2[feature],marker = '+')\n        plt.xlabel(feature , fontsize = 9)\n    plt.show();","43a6bbd6":"# Show just 5% of the data\n\nfeatures = ['var_0','var_1','var_2','var_3','var_4','var_5','var_6','var_7',\n           'var_8','var_9','var_10','var_11','var_12','var_13','var_14','var_15']\n\nplot_feature_scatter(train_df[::20],test_df[::20],features)","3e654d3b":"# Check The distribution of target value\n\nsns.countplot(train_df['target'],palette = 'Set3')","bdb20a57":"print('There are {}% target values with 1'.format(100 * train_df['target'].value_counts()[1] \/ train_df.shape[0]))","5422fd96":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();","6af8bea8":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nfeatures = train_df.columns.values[2:102]\nplot_feature_distribution(t0 , t1 , '0','1',features)","15bb9723":"# next\n\nfeatures  = train_df.columns.values[102:202]\nplot_feature_distribution(t0 , t1 , '0','1',features)","4478e738":"#Distribution of data \n\nfeatures = train_df.columns.values[2:102]\nplot_feature_distribution(train_df , test_df , 'train','test',features)","8e6836b1":"# Next\n\nfeatures = train_df.columns.values[102:202]\nplot_feature_distribution(train_df , test_df , 'train','test',features)","748fba60":"# Check the distribution of the mean values per row in the train and test set.\n# axis = 1 = row\nplt.figure(figsize = (16,6))\nfeatures = train_df.columns.values[2:202]\nplt.title('Distribution of mean values per row in the train and test set')\nsns.distplot(train_df[features].mean(axis = 1),color = 'green',kde = True , bins = 120 , label = 'train')\nsns.distplot(test_df[features].mean(axis = 1),color = 'blue',kde = True,bins = 120 , label = 'test')\nplt.legend()\nplt.show()","529ae1f3":"# Check the distribution of the mean values per columns in the train and test set.\n# axis = 0  = columns\n\nplt.figure(figsize =(16,6))\nplt.title('Distriution of mean values per column in the train and test set')\nsns.distplot(train_df[features].mean(axis = 0),color = 'magenta',kde = True,bins = 120 , label = 'train')\nsns.distplot(test_df[features].mean(axis = 0),color ='darkblue',kde = True , bins = 120 , label = 'test')\nplt.legend()\nplt.show();","dc238cfd":"# Distribution of standard deviation of values per row for train and test dataset\n\nplt.figure(figsize = (16,6))\nplt.title('Distribution of std values per row in the train and test set')\nsns.distplot(train_df[features].std(axis = 1),color = 'black',kde = True,bins = 120 , label = 'train')\nsns.distplot(test_df[features].std(axis = 1),color = 'red', kde = True  , bins =120 , label = 'test')\nplt.legend()\nplt.show();","a941e31d":"# Distribution of the standard deviation of values per columns in the train and test dataset.\n\nplt.figure(figsize = (16,6))\nplt.title('Distribution of std values per column in the train and test set')\nsns.distplot(train_df[features].std(axis = 0),color = 'blue',kde = True ,bins = 120, label = 'train')\nsns.distplot(test_df[features].std(axis = 0 ),color = 'green',kde = True , bins = 120 , label = 'test')\nplt.legend()\nplt.show()","779fd69f":"# Check the distribution of the mean value per row in the train dataset , grouped by value of target\n\nt0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize = (16,6))\nplt.title('Distribution of mean values per row in the train set')\nsns.distplot(t0[features].mean(axis = 1),color = 'red',kde = True , bins = 120 , label = 'target = 0')\nsns.distplot(t1[features].mean(axis = 1),color = 'blue',kde = True,bins = 120 , label = 'target = 1')\nplt.legend()\nplt.show();","edaf50f2":"# Check the distribution of the mean value per column , grouped by value of target.\n\nplt.figure(figsize = (16,6))\nplt.title('distribution of mean values per column in the train set')\nsns.distplot(t0[features].mean(axis = 0),color = 'green',kde = True , bins = 120,label = 'target = 0')\nsns.distplot(t1[features].mean(axis = 0),color = 'blue',kde = True , bins = 120 , label = 'target = 1')\nplt.legend()\nplt.show();","aa874924":"# Check the distribution of min per row in the train and test set\n\nplt.figure(figsize = (16,6))\nfeatures = train_df.columns.values[2:202]\nplt.title('Distribution of min values per row in the train and test set')\nsns.distplot(train_df[features].min(axis = 1),color = 'red',kde = True , bins = 120 , label = 'train')\nsns.distplot(test_df[features].min(axis = 1),color = 'orange',kde = True , bins = 120 , label = 'test')\nplt.legend()\nplt.show();","d33a72a2":"# check the distribtuion of min per column in the train and test set\n\nplt.figure(figsize =(16,6))\nfeatures = train_df.columns.values[2:202]\nplt.title('Distribution of min values per column in the train and test set')\nsns.distplot(train_df[features].min(axis = 0),color = 'magenta',kde = True , bins = 120, label = 'train')\nsns.distplot(test_df[features].min(axis = 0),color = 'darkblue',kde = True , bins =120,label = 'test')\nplt.legend()\nplt.show()","c35b6e87":"# Check the distribution of max values per rows for train and test set\nplt.figure(figsize = (16,6))\nfeatures = train_df.columns.values[2:202]\nplt.title('Distribution of max values per row in the train and test set')\nsns.distplot(train_df[features].max(axis = 1),color = 'brown',kde = True , bins =120 , label = 'train')\nsns.distplot(test_df[features].max(axis = 1),color = 'yellow',kde = True , bins =120 , label = 'test')\nplt.legend()\nplt.show()","862794aa":"# Check max distribution on columns for train and test set\n\nplt.figure(figsize = (16,6))\nfeatures = train_df.columns.values[2:202]\nplt.title('Distribution of max values per column in the train and test set')\nsns.distplot(train_df[features].max(axis = 0), color = 'blue',kde = True , bins = 120 , label = 'train')\nsns.distplot(test_df[features].max(axis = 0),color = 'red',kde = True , bins =120 , label = 'test')\nplt.legend()\nplt.show()","b263a5c8":"# Check distribution of min values per row in tran set, separated on the values of target(0 and 1)\n\nt0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize = (16,6))\nplt.title('Distribution of min values per row in the train set')\nsns.distplot(t0[features].min(axis = 1),color = 'orange',kde = True, bins =120 , label ='train = 0')\nsns.distplot(t1[features].min(axis = 1),color = 'darkblue',kde = True , bins =120 , label = 'train = 1')\nplt.legend()\nplt.show();","861067ee":"# Check distribution of min values per row in train set\n\nplt.figure(figsize = (16,6))\nplt.title('Distribution of min values per column in the train set')\nsns.distplot(t0[features].min(axis = 0),color = 'red',kde = True , bins =120 , label = 'target =0')\nsns.distplot(t1[features].min(axis = 0),color = 'blue',kde = True , bins = 120 , label = 'target = 1')\nplt.legend()\nplt.show();","32e74ceb":"# Check distribution of max values per rown in the train set\n\nplt.figure(figsize = (16,6))\nplt.title('Distribution of max values per row in the train set')\nsns.distplot(t0[features].max(axis = 1),color = 'gold',kde = True, bins = 120, label = 'target = 0')\nsns.distplot(t1[features].max(axis = 1),color = 'darkblue',kde = True , bins = 120 ,label = 'target = 1')\nplt.legend()\nplt.show();","ad5d32ed":"# Check distribution of max values per columns in the train set\n\nplt.figure(figsize = (16,6))\nplt.title('Distribution of max values per column in the train set')\nsns.distplot(t0[features].max(axis = 0),color = 'red',kde = True , bins = 120 , label = 'target = 0')\nsns.distplot(t1[features].max(axis = 0),color = 'blue',kde = True , bins =120 , label = 'target  = 1')\nplt.legend()\nplt.show();","95a86ae6":"# Check distribution of skewness calculated per rows in train and test sets.","3f971188":"plt.figure(figsize = (16,6))\nplt.title('Distribution of skew per row in the train and test set')\nsns.distplot(train_df[features].skew(axis = 1),color = 'red',kde = True , bins = 120,label = 'train')\nsns.distplot(train_df[features].skew(axis = 1),color = 'orange',kde = True , bins = 120 ,label = 'test')\nplt.legend()\nplt.show()","d7f64ccb":"# See first the distribution of skewness calculated per columns\n\nplt.figure(figsize = (16,6))\nplt.title('Distribution of skew per column in the train and test set')\nsns.distplot(train_df[features].skew(axis = 0),color = 'magenta',kde = True , bins = 120 , label = 'train')\nsns.distplot(test_df[features].skew(axis = 0), color = 'darkblue',kde = True , bins = 120 , label = 'test')\nplt.legend()\nplt.show()","e9be2298":"# Check distribution of kurtosis values per rows and columns\n","192ae312":"plt.figure(figsize = (16,6))\nplt.title('Distribution of kurtosis per row in the train and test set')\nsns.distplot(train_df[features].kurtosis(axis = 1),color = 'darkblue',kde = True , bins = 120, label = 'train')\nsns.distplot(test_df[features].kurtosis(axis  = 1),color = 'yellow',kde = True , bins = 120 , label = 'test')\nplt.legend()\nplt.show()","3da9e128":"# Check the distribution of kurtosis calculated per columns in train and test sets\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis per column in the train and test set\")\nsns.distplot(train_df[features].kurtosis(axis=0),color=\"magenta\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].kurtosis(axis=0),color=\"green\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","12f2e141":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew values per row in the train set\")\nsns.distplot(t0[features].skew(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].skew(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","345e46f1":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew values per column in the train set\")\nsns.distplot(t0[features].skew(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].skew(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","92b4dc7a":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis values per row in the train set\")\nsns.distplot(t0[features].kurtosis(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].kurtosis(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","ec9a3884":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis values per column in the train set\")\nsns.distplot(t0[features].kurtosis(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].kurtosis(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","bc267eaf":"%%time\ncorrelations = train_df[features].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]\ncorrelations.head(10)","37dfce0f":"correlations.tail(10)","f5066ee6":"correlations.head(10)","4165fb66":"%%time\nfeatures = train_df.columns.values[2:202]\nunique_max_train = []\nunique_max_test = []\nfor feature in features:\n    values = train_df[feature].value_counts()\n    unique_max_train.append([feature, values.max(), values.idxmax()])\n    values = test_df[feature].value_counts()\n    unique_max_test.append([feature, values.max(), values.idxmax()])","218a9359":"# top 15 max of duplicate values per train set\nnp.transpose((pd.DataFrame(unique_max_test, columns=['Feature', 'Max duplicates', 'Value'])).\\\n            sort_values(by = 'Max duplicates', ascending=False).head(15))","1e32c1af":"# Feature Engineering\n\n%time\n\nidx = features = train_df.columns.values[2:202]\nfor df in [test_df , train_df]:\n    df['sum'] = df[idx].sum(axis = 1)\n    df['min'] = df[idx].min(axis = 1)\n    df['max'] = df[idx].max(axis = 1)\n    df['mean'] = df[idx].mean(axis = 1)\n    df['std'] = df[idx].std(axis = 1)\n    df['skew'] = df[idx].skew(axis = 1)\n    df['kurt'] = df[idx].kurtosis(axis = 1)\n    df['med'] = df[idx].median(axis = 1)","0426a478":"# new created features\n\ntrain_df[train_df.columns[202:]].head()","6985ae1b":"test_df[test_df.columns[201:]].head()","2329b38c":"def plot_new_feature_distribtuion(df1,df2 , label1,label2 , features):\n    \n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig , ax = plt.subplots(2,4,figsize = (18,8))\n    \n    for feature in features:\n        i += 1\n        plt.subplot(2,4,i)\n        sns.kdeplot(df1[feature],bw = 0.5 , label = label1)\n        sns.kdeplot(df2[feature],bw = 0.5 , label = label2)\n        \n        plt.xlabel(feature,fontsize = 11)\n        locs , labels = plt.xticks()\n        plt.tick_params(axis = 'x',which = 'major',labelsize = 8)\n        plt.tick_params(axis = 'y',which = 'major',labelsize = 8)\n        ","cbe87c62":"# Check the distribution of these new engineered features.\nt0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nfeatures = train_df.columns.values[202:]\nplot_new_feature_distribtuion(t0 , t1 , 'target:0','target:1',features)","00a3f392":"# show the distribution of new features value\n\nfeatures = train_df.columns.values[202:]\nplot_new_feature_distribtuion(train_df , test_df,'train','test',features)","0f67ae5a":"# Rounded features\n\nfeatures = [c for c in train_df.columns if c not in ['ID_code','target']]\nfor feature in features:\n    train_df['r2_' + feature] = np.round(train_df[feature],2)\n    test_df['r2_' + feature] = np.round(test_df[feature],2)\n    train_df['r1_' + feature] = np.round(train_df[feature],2)\n    test_df['r1' + feature] = np.round(train_df[feature],2)","daa95ff5":"# How many features we have now.\n\nprint('Train and test columns: {}{}'.format(len(train_df.columns), len(test_df.columns)))","3e14b09a":"# Model\n# We drop the ID and target to form the features list\n\nfeatures = [c for c in train_df.columns if c not in['ID_code','target']]\ntarget = train_df['target']","57c613fa":"# Define the hyperparametrs for the model\n\nparam = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.4,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': 1\n}","a0cd83b0":"# run model\n\nfolds = StratifiedKFold(n_splits = 10 , shuffle = False , random_state = 44000)\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_ , (trn_idx , val_idx) in enumerate(folds.split(train_df.values,target.values)):\n    print('Fold {}'.format(fold_))\n    trn_data = lgb.Dataset(train_df.iloc[trn_idx][features],label = target.iloc[trn_idx])\n    val_data = lgb.Dataset(train_df.iloc[val_idx][features],label = target.iloc[val_idx])\n    \n    num_round = 1000000\n    clf = lgb.train(param , trn_data , num_round , valid_sets = [trn_data , val_data],verbose_eval = 1000,early_stopping_rounds= 3000)\n    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features],num_iteration = clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df['Feature'] = features\n    fold_importance_df['importance'] = clf.feature_importance()\n    fold_importance_df['fold'] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df,fold_importance_df],axis = 0)\n    \n    predictions += clf.predict(test_df[features],num_iteration = clf.best_iteration) \/ folds.n_splits\n    \n    print('Cv score:{:<8.5f}'.format(roc_auc_score(target , oof)))\n    ","08b952a7":"# Check The feature importance\n\ncols = (feature_importance_df[['feature','importance']].groupby('Feature').mean().sort_values(by = 'importance',ascending = False)[:150].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize = (14,28))\nsns.barplot(x = 'importance' , y = 'Feature',data = best_features.sort_values(by = 'importance',ascending = False))\nplt.title('Features importance (averaged \/ folds)')\nplt.tight_layout()\nplt.savefig('Fi.png')","af91bce4":"# Submission\n\nsub_df = pd.DataFrame({'ID_code':test_df['ID_code'].values})\nsub_df['target'] = predictions\nsub_df.to_csv('submission.csv',index = False)","0ed82528":"**The train and test seems to be well ballanced with respect with distribution of the numeric variables.**","8cb1196c":"**It is the degree of distortion from the symmetrical bell curve or the normal distribution. It measures the lack of symmetry in data distribution.\nIt differentiates extreme values in one versus the other tail. A symmetrical distribution will have a skewness of 0.\nThere are two types of Skewness: Positive and Negative**","aef95851":"**Kurtosis\nKurtosis is all about the tails of the distribution \u2014 not the peakedness or flatness. It is used to describe the extreme values in one versus the other tail. It is actually the measure of outliers present in the distribution.**","ea800f24":"![](https:\/\/miro.medium.com\/max\/600\/1*nj-Ch3AUFmkd0JUSOW_bTQ.jpeg)","4a3b7bb8":"![](http:\/\/miro.medium.com\/max\/371\/1*Nqu07THa7APRTOF7kaVr5Q.jpeg)","d175e881":"**The data is unbalanced with respect with target value.**"}}