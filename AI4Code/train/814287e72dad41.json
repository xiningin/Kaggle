{"cell_type":{"c1df841d":"code","202a78b5":"code","a2da1ca2":"code","64bf2cb6":"code","b3b0c55e":"code","50223c92":"code","6e5ece8e":"code","75113d1d":"code","ece05c8a":"markdown","ea65cb77":"markdown","b675e162":"markdown","29824d42":"markdown","756644ea":"markdown","0a861b4b":"markdown","245dd558":"markdown","fa2441ed":"markdown"},"source":{"c1df841d":"import pymc3 as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-darkgrid')","202a78b5":"# Initialize random number generator\nnp.random.seed(123)\n\n# True parameter values\nalpha, sigma = 1, 1\nbeta = [1, 2.5]\n\n# Size of dataset\nsize = 1000\n\n# Predictor variable\nX1 = np.random.randn(size)\nX2 = np.random.randn(size) * 0.2\n\n# Simulate outcome variable\nY = alpha + beta[0]*X1 + beta[1]*X2 + np.random.randn(size)*sigma","a2da1ca2":"fig, axes = plt.subplots(1, 2, sharex=True, figsize=(10,4))\naxes[0].scatter(X1, Y)\naxes[1].scatter(X2, Y)\naxes[0].set_ylabel('Y'); axes[0].set_xlabel('X1'); axes[1].set_xlabel('X2');","64bf2cb6":"basic_model = pm.Model()\n\nwith basic_model:\n\n    # Priors for unknown model parameters\n    alpha = pm.Normal('alpha', mu=0, sigma=100)\n    beta = pm.Normal('beta', mu=0, sigma=100, shape=2)\n    sigma = pm.HalfNormal('sigma', sigma=100)\n\n    # Expected value of outcome\n    mu = alpha + beta[0]*X1 + beta[1]*X2\n\n    # Likelihood (sampling distribution) of observations\n    Y_obs = pm.Normal('Y_obs', mu=mu, sigma=sigma, observed=Y)","b3b0c55e":"with basic_model:\n    # draw 5000 posterior samples\n    trace = pm.sample(5000)","50223c92":"pm.traceplot(trace);","6e5ece8e":"pm.summary(trace).round(2)","75113d1d":"import seaborn as sns\nplt.figure(figsize=(9,7))\nsns.jointplot(trace['beta'][:,0], trace['beta'][:,1], kind=\"hex\", color=\"#4CB391\")\nplt.xlabel(\"beta[0]\")\nplt.ylabel(\"beta[1]\");\nplt.show()\n\nplt.figure(figsize=(9,7))\nsns.jointplot(trace['alpha'], trace['sigma'], kind=\"hex\", color=\"#4CB391\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"sigma\");\nplt.show()","ece05c8a":"There is a built-in summary function as well.","ea65cb77":"Here is a quick plot of our data.","b675e162":"Here are some 2-d plots of joint probability distributions.","29824d42":"Let's create some data for our regression.  Our true values are:\n* $\\alpha = 1$\n* $\\sigma = 1$\n* $\\beta = [1, 2.5]$\n\nOur outcome variable is:\n$$ Y = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 + N(0,\\sigma).$$","756644ea":"**Bayesian Linear Regression in PYMC3 Tutorial**\n\nThis is a very simple tutorial on using PYMC3 for Bayesian linear regression, with an estimation of the posterior probability distributions.  It was adopted from the PYMC3 getting started documentation [https:\/\/docs.pymc.io\/notebooks\/getting_started.html](https:\/\/docs.pymc.io\/notebooks\/getting_started.html).","0a861b4b":"Now we build the model.\n\nThe prior distributions are:\n* $\\alpha \\sim \\mathcal{N}(\\mu=0,\\sigma=10)$\n* $\\beta[i] \\sim \\mathcal{N}(\\mu=0,\\sigma=10)$, where $i=1,2$\n* $\\sigma \\sim \\textrm{half-normal}(\\sigma=1)$\n\nWe define the unobserved variable rate, which is a function of the year:\n\\begin{equation}\n  \\mu = \\alpha + \\beta[0]*X1 + \\beta[1]*X2\n\\end{equation}\n\nOur likelihood function in Bayes theorem is a Poisson distribution on the number of disasters, each year.  This can be written as:\n\\begin{equation}\n  \\text{Y}\\sim \\mathcal{N}(\\mu=\\mu,\\sigma=\\sigma).\n\\end{equation}","245dd558":"Now we take 5000 random MCMC samples.  The defualt PYMC3 sampler is the Hamiltonian MC No U-Turn Sampler (NUTS), which is almost always a good choice.","fa2441ed":"The traceplot is the standard good way to view the posterior probability distributions along with theMCMC samples."}}