{"cell_type":{"01ccceb2":"code","401d6402":"code","0b0cdc8b":"code","fa648532":"code","eef27970":"code","0461b6d9":"code","befd5aeb":"code","3a65d3a3":"code","b29764bf":"code","8d00521f":"code","2b03a797":"code","809b87a1":"code","5e0dcf3a":"code","cacc5750":"code","48ca0c91":"code","916c3365":"code","3454886d":"code","e457a037":"code","5bf11b1f":"code","70487789":"code","60a36bd6":"code","4e56df39":"code","b1eb6ae7":"code","27e49b63":"code","6d21b02a":"code","8656d809":"code","d2bf9693":"code","392f5875":"code","3eadc2d8":"code","f7fc7b61":"code","17d732e4":"code","1461f7b4":"code","7587e690":"code","4a8ad9c3":"code","1ad76e54":"markdown","14bd2514":"markdown","c80b9f50":"markdown","729b1f37":"markdown","7d74cfdd":"markdown","6bc2b6ba":"markdown","d2218a22":"markdown","15054f32":"markdown","7bf6508b":"markdown","d1938225":"markdown","4a358238":"markdown","e69cf8ab":"markdown","8bf6f6f8":"markdown","c748f323":"markdown","9ff69c15":"markdown","063dc9ca":"markdown","06ce54dc":"markdown","8f3c36ac":"markdown","6829d722":"markdown","2f95e8fc":"markdown","5ac245a5":"markdown","c86f9f0c":"markdown","ebd2d767":"markdown","62466260":"markdown","0494d4c9":"markdown","acfb5614":"markdown","24fb1ca9":"markdown","cf705ab0":"markdown","8f36e595":"markdown","6a69bd16":"markdown","e7f28cfd":"markdown"},"source":{"01ccceb2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","401d6402":"data=pd.read_csv(\"..\/input\/pulsar_stars.csv\")","0b0cdc8b":"data.info()","fa648532":"data = data.rename(columns={' Mean of the integrated profile':\"mean_integrated_profile\",\n       ' Standard deviation of the integrated profile':\"std_deviation_integrated_profile\",\n       ' Excess kurtosis of the integrated profile':\"kurtosis_integrated_profile\",\n       ' Skewness of the integrated profile':\"skewness_integrated_profile\", \n        ' Mean of the DM-SNR curve':\"mean_dm_snr_curve\",\n       ' Standard deviation of the DM-SNR curve':\"std_deviation_dm_snr_curve\",\n       ' Excess kurtosis of the DM-SNR curve':\"kurtosis_dm_snr_curve\",\n       ' Skewness of the DM-SNR curve':\"skewness_dm_snr_curve\",\n       })","eef27970":"data.head()","0461b6d9":"f,ax=plt.subplots(figsize=(15,15))\nsns.heatmap(data.corr(),annot=True,linecolor=\"blue\",fmt=\".2f\",ax=ax)\nplt.show()","befd5aeb":"g = sns.pairplot(data, hue=\"target_class\",palette=\"husl\",diag_kind = \"kde\",kind = \"scatter\")","3a65d3a3":"y = data[\"target_class\"].values\nx_data = data.drop([\"target_class\"],axis=1)\nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","b29764bf":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=1)","8d00521f":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\nlr_prediction = lr.predict(x_test)","2b03a797":"from sklearn.metrics import mean_squared_error\nmse_lr=mean_squared_error(y_test,lr_prediction)\n\nfrom sklearn.metrics import confusion_matrix,classification_report\ncm_lr=confusion_matrix(y_test,lr_prediction)\ncm_lr=pd.DataFrame(cm_lr)\ncm_lr[\"total\"]=cm_lr[0]+cm_lr[1]\ncr_lr=classification_report(y_test,lr_prediction)\n","809b87a1":"from sklearn.metrics import cohen_kappa_score\ncks_lr= cohen_kappa_score(y_test, lr_prediction)\n\n","5e0dcf3a":"score_and_mse={\"model\":[\"logistic regression\"],\"Score\":[lr.score(x_test,y_test)],\"Cohen Kappa Score\":[cks_lr],\"MSE\":[mse_lr]}\nscore_and_mse=pd.DataFrame(score_and_mse)\n","cacc5750":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors =13) # n_neighbors = k\nknn.fit(x_train,y_train)\nknn_prediction = knn.predict(x_test)","48ca0c91":"score_list = []\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","916c3365":"mse_knn=mean_squared_error(y_test,knn_prediction)\ncm_knn=confusion_matrix(y_test,knn_prediction)\ncm_knn=pd.DataFrame(cm_knn)\ncr_knn=classification_report(y_test,knn_prediction)\ncm_knn[\"total\"]=cm_knn[0]+cm_knn[1]","3454886d":"from sklearn.metrics import cohen_kappa_score\ncks_knn= cohen_kappa_score(y_test, knn_prediction)\n","e457a037":"score_and_mse = score_and_mse.append({'model': \"knn classification\",\"Score\":knn.score(x_test,y_test),\"Cohen Kappa Score\":cks_knn,\"MSE\":mse_knn}, ignore_index=True)","5bf11b1f":"from sklearn.svm import SVC\nsvm=SVC(random_state=1)\nsvm.fit(x_train,y_train)\nsvm_prediction=svm.predict(x_test)","70487789":"mse_svm=mean_squared_error(y_test,svm_prediction)\nsvm_cm=confusion_matrix(y_test,svm_prediction)\ncm_svm=pd.DataFrame(svm_cm)\ncm_svm[\"total\"]=cm_svm[0]+cm_svm[1]\n\ncr_svm=classification_report(y_test,svm_prediction)\ncks_svm= cohen_kappa_score(y_test, svm_prediction)\n","60a36bd6":"score_and_mse = score_and_mse.append({'model': \"svm classification\",\"Score\":svm.score(x_test,y_test),\"Cohen Kappa Score\":cks_svm,\"MSE\":mse_svm}, ignore_index=True)","4e56df39":"from sklearn.naive_bayes import GaussianNB\nnb=GaussianNB()\nnb.fit(x_train,y_train)\nprediction_nb=nb.predict(x_test)","b1eb6ae7":"nb_mse=mean_squared_error(y_test,prediction_nb)\nnb_cm=confusion_matrix(y_test,prediction_nb)\nnb_cm=pd.DataFrame(nb_cm)\nnb_cm[\"total\"]=nb_cm[0]+nb_cm[1]\n\ncr_nb=classification_report(y_test,prediction_nb)\ncks_nb= cohen_kappa_score(y_test, prediction_nb)\n","27e49b63":"score_and_mse = score_and_mse.append({'model': \"naive bayes classification\",\"Score\":nb.score(x_test,y_test),\"Cohen Kappa Score\":cks_nb,\"MSE\":nb_mse}, ignore_index=True)","6d21b02a":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nprediction_dt=dt.predict(x_test)","8656d809":"dt_mse=mean_squared_error(y_test,prediction_dt)\ndt_cm=confusion_matrix(y_test,prediction_dt)\ndt_cm=pd.DataFrame(dt_cm)\ndt_cm[\"total\"]=dt_cm[0]+dt_cm[1]\n\ncr_dt=classification_report(y_test,prediction_dt)\ncks_dt= cohen_kappa_score(y_test, prediction_dt)\n","d2bf9693":"score_and_mse = score_and_mse.append({'model': \"decision tree classification\",\"Score\":dt.score(x_test,y_test),\"Cohen Kappa Score\":cks_dt, \"MSE\":dt_mse}, ignore_index=True)","392f5875":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=100,random_state=1)\nrf.fit(x_train,y_train)\n\nprediction_rf=rf.predict(x_test)","3eadc2d8":"rf_mse=mean_squared_error(y_test,prediction_rf)\nrf_cm=confusion_matrix(y_test,prediction_rf)\nrf_cm=pd.DataFrame(rf_cm)\nrf_cm[\"total\"]=rf_cm[0]+rf_cm[1]\n\ncr_rf=classification_report(y_test,prediction_rf)\ncks_rf= cohen_kappa_score(y_test, prediction_rf)","f7fc7b61":"score_and_mse = score_and_mse.append({'model': \"random forest classification\",\"Score\":rf.score(x_test,y_test),\"Cohen Kappa Score\":cks_rf,\"MSE\":rf_mse}, ignore_index=True)","17d732e4":"print('Classification report for Logistic Regression: \\n',cr_lr)\nprint('Classification report for KNN Classification: \\n',cr_knn)\nprint('Classification report for SVM Classification: \\n',cr_svm)\nprint('Classification report for Naive Bayes Classification: \\n',cr_nb)\nprint('Classification report for Decision Tree Classification: \\n',cr_dt)\nprint('Classification report for Random Forest Classification: \\n',cr_rf)","1461f7b4":"f, axes = plt.subplots(2, 3,figsize=(18,12))\ng1 = sns.heatmap(cm_lr,annot=True,fmt=\".1f\",cmap=\"flag\",cbar=False,ax=axes[0,0])\ng1.set_ylabel('y_true')\ng1.set_xlabel('y_head')\ng1.set_title(\"Logistic Regression\")\ng2 = sns.heatmap(cm_knn,annot=True,fmt=\".1f\",cmap=\"flag\",cbar=False,ax=axes[0,1])\ng2.set_ylabel('y_true')\ng2.set_xlabel('y_head')\ng2.set_title(\"KNN Classification\")\ng3 = sns.heatmap(cm_svm,annot=True,fmt=\".1f\",cmap=\"flag\",ax=axes[0,2])\ng3.set_ylabel('y_true')\ng3.set_xlabel('y_head')\ng3.set_title(\"SVM Classification\")\ng4 = sns.heatmap(nb_cm,annot=True,fmt=\".1f\",cmap=\"flag\",cbar=False,ax=axes[1,0])\ng4.set_ylabel('y_true')\ng4.set_xlabel('y_head')\ng4.set_title(\"Naive Bayes Classification\")\ng5 = sns.heatmap(dt_cm,annot=True,fmt=\".1f\",cmap=\"flag\",cbar=False,ax=axes[1,1])\ng5.set_ylabel('y_true')\ng5.set_xlabel('y_head')\ng5.set_title(\"Decision Tree Classification\")\ng6 = sns.heatmap(rf_cm,annot=True,fmt=\".1f\",cmap=\"flag\",ax=axes[1,2])\ng6.set_ylabel('y_true')\ng6.set_xlabel('y_head')\ng6.set_title(\"Random Forest Classification\")\n\n","7587e690":"from sklearn.metrics import roc_curve\nfpr_lr, tpr_lr, thresholds = roc_curve(y_test, lr_prediction)\nplt.plot([0, 1], [0, 1], 'k--',color=\"grey\")\nplt.plot(fpr_lr, tpr_lr,color=\"red\")\nplt.title('Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')","4a8ad9c3":"score_and_mse","1ad76e54":"It is the ratio of number of correct predictions to the total number of input samples.\n\nAccuracy= Number of Correct Predictions\/ Total Number of Predictions Made","14bd2514":"**Cohen Kappa Score**","c80b9f50":"The first pulsar was observed on November 28, 1967, by Jocelyn Bell Burnell and Antony Hewish. They observed pulses separated by 1.33 seconds that originated from the same location in the sky, and kept to sidereal time. In looking for explanations for the pulses, the short period of the pulses eliminated most astrophysical sources of radiation, such as stars, and since the pulses followed sidereal time, it could not be man-made radio frequency interference.(source=Wikipedia)","729b1f37":"**Receiver Operating Characteristic(ROC) Curve**","7d74cfdd":"In this kernel, I will explain whether a star is a pulsar star with supervised learning machine learning algorithms.","6bc2b6ba":"<a id=\"6\"><\/a> <br>\nDECISION TREE CLASSIFICATION","d2218a22":"In a Receiver Operating Characteristic (ROC) curve the true positive rate (Sensitivity) is plotted in function of the false positive rate (100-Specificity) for different cut-off points. \n\nEach point on the ROC curve represents a sensitivity\/specificity pair corresponding to a particular decision threshold.\n\nA test with perfect discrimination (no overlap in the two distributions) has a ROC curve that passes through the upper left corner (100% sensitivity, 100% specificity). \n\nTherefore the closer the ROC curve is to the upper left corner, the higher the overall accuracy of the test (Zweig & Campbell, 1993).","15054f32":"**Confusion Matrix**","7bf6508b":"A confusion matrix is a summary of prediction results on a classification problem.\n\nPositive (P) : Observation is positive (for example: is a Pulse Star).\n\nNegative (N) : Observation is not positive (for example: is not a Pulse Star).\n\nTrue Positive (TP) : Observation is positive, and is predicted to be positive.\n\nFalse Negative (FN) : Observation is positive, but is predicted negative.\n\nTrue Negative (TN) : Observation is negative, and is predicted to be negative.\n\nFalse Positive (FP) : Observation is negative, but is predicted positive.","d1938225":"Now Let's look at the 5 entries at the top of the data set","4a358238":" <a id=\"5\"><\/a> <br>\n NAIVE BAYES CLASSIFICATION","e69cf8ab":"**Classification Report**","8bf6f6f8":"<a id=\"8\"><\/a> <br>\nEVALUATING A CLASSIFICATION MODEL","c748f323":"![](https:\/\/usercontent2.hubstatic.com\/14277725_f520.jpg)","9ff69c15":"<a id=\"3\"><\/a> <br>\nK-NEAREST NEIGHBOUR(KNN) CLASSIFICATION ","063dc9ca":"**Accuracy (Score) **","06ce54dc":"Kappa is similar to Accuracy score, but it takes into account the accuracy that would have happened anyway through random predictions.\n\nIt is a measure of how well the classifier actually performs. In other words, if there is a big difference between accuracy and null error rate, a model will have a high Kappa score.\n\n\nCohen Kappa only serves to make comparisons between two classifiers, if there are more than two classifiers, Fleiss's Kappa is used.\n\nKappa = (Observed Accuracy - Expected Accuracy) \/ (1 - Expected Accuracy)","8f3c36ac":"And following pairplots show correlations between features with classes","6829d722":"<a id=\"7\"><\/a> <br>\nRANDOM FOREST CLASSIFICATION","2f95e8fc":"Following heatmap shows correlation between features. \n\nThere is a high positive correlation between following features:\n- Excess kurtosis of the integrated profile - Skewness of the integrated profile (0.95)\n- Mean of the DM-SNR curve - Standard deviation of the DM-SNR curve(0.80)\n- Excess kurtosis of the DM-SNR curve - Skewness of the DM-SNR curve (0.92)\n\n\nThere is a high negative correlation between following features:\n- Mean of the integrated profile - Excess kurtosis of the integrated profile (-0.87)\n- Mean of the integrated profile - Skewness of the integrated profile (-0.74)\n- Standard deviation of the DM-SNR curve - Excess kurtosis of the DM-SNR curve (-0.81)","5ac245a5":"We have 9 features and looks like there are no nan values. However features names are a little bit untidy. I will change them.","c86f9f0c":"<a id=\"1\"><\/a> <br>\nDATA ANALYSIS ","ebd2d767":"Pulsars are a rare type of Neutron star that produce radio emission detectable here on Earth. They are of considerable scientific interest as probes of space-time, the inter-stellar medium, and states of matter .  Neutron stars are very dense, and have short, regular rotational periods. This produces a very precise interval between pulses that ranges from milliseconds to seconds for an individual pulsar. Pulsars are believed to be one of the candidates for the source of ultra-high-energy cosmic rays.","62466260":" PREDICTING A PULSAR STAR","0494d4c9":"Mean Squared Error(MSE) is quite similar to Mean Absolute Error, the only difference being that MSE takes the average of the square of the difference between the original values and the predicted values.","acfb5614":"The classification report averages include : \n\n**Precision:** The ratio of the total number of correctly classified positive examples by the total number of predicted positive examples. TP\/TP+FP\n\n**Recall:** The ratio of the total number of correctly classified positive examples divide to the total number of positive examples. TP\/TP+FN\n\n**F-measure:** 2 x Recall x Precision \/ Recall+ Precision\n\n**micro average:** averaging the total true positives, false negatives and false positives, \n\n**macro average:** averaging the unweighted mean per label, \n\n**weighted average:** averaging the support-weighted mean per label ","24fb1ca9":"<a id=\"4\"><\/a> <br>\nSUPPORT VECTOR MACHINE(SVM) CLASSIFICATION","cf705ab0":"<a id=\"2\"><\/a> <br>\nLOGISTIC REGRESSION","8f36e595":"### **CONTENT :**\n\n1. [DATA ANALYSIS](#1)\n2. [LOGISTIC REGRESSION](#2)\n3. [K-NEAREST NEIGHBOUR(KNN) CLASSIFICATION](#3) \n4. [SUPPORT VECTOR MACHINE(SVM) CLASSIFICATION](#4)\n5. [NAIVE BAYES CLASSIFICATION](#5)\n6. [DECISION TREE CLASSIFICATION](#6)\n7. [RANDOM FOREST CLASSIFICATION](#7)\n8. [EVALUATING A CLASSIFICATION MODEL](#8)","6a69bd16":"\n\n![](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/evaluate\/confusion_matrix_files\/confusion_matrix_1.png)","e7f28cfd":"**Mean Squared Error(MSE)**"}}