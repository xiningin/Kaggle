{"cell_type":{"88885ae5":"code","94cc6aa7":"code","03df4c5a":"code","cd1e6f8b":"code","766d5132":"code","f7f670b0":"code","f985b661":"code","8fd7fbb6":"code","9efb3d3b":"code","77170643":"code","78304803":"code","53f62dd5":"code","8bf487e5":"code","15f19824":"code","fb108818":"code","1a401f82":"code","61fc6de1":"code","18434811":"markdown","8407adb5":"markdown","0f86f768":"markdown","fe09ab49":"markdown","341d94bd":"markdown","038bfe96":"markdown","aeba8bd4":"markdown","e8c70b78":"markdown","e938dc4e":"markdown","d8a50945":"markdown","07829581":"markdown","2248d1ca":"markdown","a0e99010":"markdown","c982fae9":"markdown"},"source":{"88885ae5":"import plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os","94cc6aa7":"prices = np.loadtxt('..\/input\/binance-bitcoin-futures-price-10s-intervals\/prices_btc_Jan_11_2020_to_May_22_2020.txt', dtype=float)","03df4c5a":"len(prices)","cd1e6f8b":"fig = go.Figure(data=go.Scatter(y=prices[-10000:]))\nfig.show()\n\ndef saludo():\n    print(\"Hola amigas\")\n    \nsaludo()","766d5132":"def buy(btc_price, btc, money):\n    if(money != 0):\n        btc = (1 \/ btc_price ) * money\n        money = 0\n    return btc, money\n\n\ndef sell(btc_price, btc, money):\n    if(btc != 0):\n        money = btc_price * btc\n        btc = 0\n    return btc, money\n\n\ndef wait(btc_price, btc, money):\n    # do nothing\n    return btc, money","f7f670b0":"np.random.seed(1)\n\n# set of actions that the user could do\nactions = { 'buy' : buy, 'sell': sell, 'wait' : wait}\n\nactions_to_nr = { 'buy' : 0, 'sell' : 1, 'wait' : 2 }\nnr_to_actions = { k:v for (k,v) in enumerate(actions_to_nr) }\n\nnr_actions = len(actions_to_nr.keys())\nnr_states = len(prices)\n\n# q-table = reference table for our agent to select the best action based on the q-value\nq_table = np.random.rand(nr_states, nr_actions)","f985b661":"def get_reward(before_btc, btc, before_money, money):\n    reward = 0\n    if(btc != 0):\n        if(before_btc < btc):\n            reward = 1\n    if(money != 0):\n        if(before_money < money):\n            reward = 1\n            \n    return reward","8fd7fbb6":"def choose_action(state):\n    if np.random.uniform(0, 1) < eps:\n        return np.random.randint(0, 2)\n    else:\n        return np.argmax(q_table[state])","9efb3d3b":"def take_action(state, action):\n    return actions[nr_to_actions[action]](prices[state], btc, money)","77170643":"def act(state, action, theta):\n    btc, money = theta\n    \n    done = False\n    new_state = state + 1\n    \n    before_btc, before_money = btc, money\n    btc, money = take_action(state, action)\n    theta = btc, money\n    \n    reward = get_reward(before_btc, btc, before_money, money)\n    \n    if(new_state == nr_states):\n        done = True\n    \n    return new_state, reward, theta, done","78304803":"reward = 0\nbtc = 0\nmoney = 100\n\ntheta = btc, money","53f62dd5":"# exploratory\neps = 0.3\n\nn_episodes = 20\nmin_alpha = 0.02\n\n# learning rate for Q learning\nalphas = np.linspace(1.0, min_alpha, n_episodes)\n\n# discount factor, used to balance immediate and future reward\ngamma = 1.0","8bf487e5":"rewards = {}\n\nfor e in range(n_episodes):\n    \n    total_reward = 0\n    \n    state = 0\n    done = False\n    alpha = alphas[e]\n    \n    while(done != True):\n\n        action = choose_action(state)\n        next_state, reward, theta, done = act(state, action, theta)\n        \n        total_reward += reward\n        \n        if(done):\n            rewards[e] = total_reward\n            print(f\"Episode {e + 1}: total reward -> {total_reward}\")\n            break\n        \n        q_table[state][action] = q_table[state][action] + alpha * (reward + gamma *  np.max(q_table[next_state]) - q_table[state][action])\n\n        state = next_state","15f19824":"plt.ylabel('Total Reward')\nplt.xlabel('Episode')\nplt.plot([rewards[e] for e in rewards.keys()])","fb108818":"state = 0\nacts = np.zeros(nr_states)\ndone = False\n\nwhile(done != True):\n\n        action = choose_action(state)\n        next_state, reward, theta, done = act(state, action, theta)\n        \n        acts[state] = action\n        \n        total_reward += reward\n        \n        if(done):\n            break\n            \n        state = next_state","1a401f82":"buys_idx = np.where(acts == 0)\nwait_idx = np.where(acts == 2)\nsell_idx = np.where(acts == 1)","61fc6de1":"plt.figure(figsize=(15,15))\nplt.plot(buys_idx[0], prices[buys_idx], 'bo', markersize=2)\nplt.plot(sell_idx[0], prices[sell_idx], 'ro', markersize=2)\nplt.plot(wait_idx[0], prices[wait_idx], 'yo', markersize=2)","18434811":"### Functions to buy, sell and wait","8407adb5":"Load price data","0f86f768":"### Goal\n\nThe goal of this notebook is to showcase the use of Q-learning in order to buy and sell in order to maximize the money\/btc amount.\n\nThis is an controlled environment - **DO NOT USE ON REAL BITCOIN**\n\n**Don't forget to up-vote in order to support me from developing new kernels (notebooks)**","fe09ab49":"### Conclusion\n\nEven if we trained for only 20 episodes, we can see clear how the Q learning works in reinforcement learning.<br>\nWe got a Q network 2D array that holds the best action for each state plus we also use exploratory using (eps) in order to try new actions, if the actions get better rewards, we add them to the Q table by increasing the q table actions using the formula :<br>\n> q_table[state][action] + alpha * (reward + gamma *  np.max(q_table[next_state]) - q_table[state][action])","341d94bd":"### Definition\n\nQ-learning is an off policy reinforcement learning algorithm that seeks to find the best action to take given the current state. It\u2019s considered off-policy because the q-learning function learns from actions that are outside the current policy, like taking random actions, and therefore a policy isn\u2019t needed. More specifically, q-learning seeks to learn a policy that maximizes the total reward.","038bfe96":"# Q learning - Reinforcement Learning","aeba8bd4":"### Functions to get rewards and act upon action","e8c70b78":"## Retrieve data","e938dc4e":"#### Steps for Q-network learning\n\nHere are the 3 basic steps:\n\n- Agent starts in a state=0 takes an action and receives a reward\n- Agent selects action by referencing Q-table with highest value (max) OR by random (epsilon, \u03b5)\n- Update q-values","d8a50945":"### Plot the Results","07829581":"### Training the Q table","2248d1ca":"## Reinforcement Learning","a0e99010":"### Learning Analysis","c982fae9":"### Create actions, states tables"}}