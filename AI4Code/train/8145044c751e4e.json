{"cell_type":{"db945357":"code","a553a442":"code","7f40b696":"code","68c3d150":"code","70a21116":"code","24978898":"code","b4669481":"code","90f4a6fb":"code","e25b33d1":"code","980b65a8":"code","ec0121d4":"code","a0c795df":"code","4ec7d0ee":"code","8022b722":"code","54493c3a":"code","988ebdef":"code","0f2ddff4":"code","76ac533d":"code","21ed133f":"code","424d47d0":"code","f2730443":"code","e46d73ce":"code","08d244c1":"code","a8668b08":"code","b71b71e6":"code","f8c850c8":"markdown","cd5c66bd":"markdown","58d6aed2":"markdown","caa9bd66":"markdown","01e6c26e":"markdown","25d03ebe":"markdown","a82d5016":"markdown","e6a01d2d":"markdown","6d2a29ff":"markdown","5a003cde":"markdown","bc242356":"markdown","d887b8cb":"markdown","6d907266":"markdown","46e2d180":"markdown","4214462e":"markdown","8c6e6326":"markdown"},"source":{"db945357":"! pip install pandas-profiling","a553a442":"from __future__ import print_function\n%matplotlib inline\nimport os\nimport warnings\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as image\nimport pandas as pd\nimport pandas_profiling\nplt.style.use(\"ggplot\")\nwarnings.simplefilter(\"ignore\")","7f40b696":"plt.rcParams['figure.figsize'] = (12,8)","68c3d150":"hr = pd.read_csv(\"..\/input\/employee-data\/employee_data.csv\")\nhr.head()","70a21116":"hr.shape","24978898":"hr.profile_report(title = 'Data Report')","b4669481":"pd.crosstab(hr.salary, hr.quit).plot(kind= 'bar')\nplt.title('Turnover Frequency on Salary Bracket')\nplt.xlabel('Salary')\nplt.ylabel('Frequency of TurnOver')\nplt.show();","90f4a6fb":"pd.crosstab(hr.department, hr.quit).plot(kind= 'bar')\nplt.title('Turnover Frequency on Department')\nplt.xlabel('Department')\nplt.ylabel('Frequency of TurnOver')\nplt.show();","e25b33d1":"cat_vars = ['department', 'salary']\nfor var in cat_vars:\n    cat_list = pd.get_dummies(hr[var], prefix = var)\n    hr = hr.join(cat_list)","980b65a8":"hr.head()","ec0121d4":"hr.drop(columns = ['department', 'salary'], axis = 1, inplace = True)","a0c795df":"hr.head()","4ec7d0ee":"! pip install yellowbrick","8022b722":"from yellowbrick.target import ClassBalance\nplt.style.use(\"ggplot\")\nplt.rcParams['figure.figsize'] = (12,8)","54493c3a":"visualizer = ClassBalance(labels = ['stayed', 'quit']).fit(hr.quit)\nvisualizer.show();","988ebdef":"x = hr.loc[:, hr.columns != 'quit']\ny = hr.quit","0f2ddff4":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 0, \n                                                   test_size = 0.2, stratify = y)","76ac533d":"from sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import export_graphviz # display the tree within a Jupyter notebook\nfrom IPython.display import SVG\nfrom graphviz import Source\nfrom IPython.display import display\nfrom ipywidgets import interactive, IntSlider, FloatSlider, interact\nimport ipywidgets\nfrom IPython.display import Image\nfrom subprocess import call\nimport matplotlib.image as mpimg","21ed133f":"@interact\ndef plot_tree_rf(crit=['gini', 'entropy'],\n                 split=['best','random'],\n                 depth=IntSlider(min=1,max=30,value=2, continuous_update=False),\n                 min_split=IntSlider(min=2,max=5,value=2,continuous_update=False),\n                 min_leaf=IntSlider(min=1,max=5,value=1, continuous_update=False)):\n    estimator = DecisionTreeClassifier(random_state = 0,\n                                      criterion = crit,\n                                     splitter = split,\n                                      max_depth = depth,\n                                      min_samples_split = min_split,\n                                      min_samples_leaf= min_leaf)\n    estimator.fit(x_train,y_train)\n    print('Descision Tree Training Accuracy:{:0.3f}' .format(accuracy_score(y_train, estimator.predict(x_train))))\n    print('Descision Tree Testing Accuracy:{:0.3f}'.format(accuracy_score(y_test, estimator.predict(x_test))))\n                                                   \n    graph = Source(tree.export_graphviz(estimator,out_file = None,\n                                        feature_names = x_train.columns,\n                                        class_names = ['stayed', 'quit'],\n                                        filled = True))\n                                                           \n     \n    display(Image(data = graph.pipe(format = 'png')))\n                                     ","424d47d0":"@interact\ndef plot_tree_rf(crit=['gini', 'entropy'],\n                 bootstrap=['best','random'],\n                 depth=IntSlider(min=1,max=30,value=2, continuous_update=False),\n                 forests=IntSlider(min=1,max=200,value=100,continuous_update=False),\n                 min_split=IntSlider(min=2,max=5,value=2, continuous_update=False),\n                 min_leaf=IntSlider(min=1,max=5,value=1, continuous_update=False)):\n    \n\n    estimator = RandomForestClassifier(random_state = 1,\n                                      criterion = crit,\n                                      bootstrap = bootstrap,\n                                      n_estimators = forests,\n                                      max_depth = depth,\n                                      min_samples_split = min_split,\n                                      min_samples_leaf= min_leaf,\n                                      n_jobs = -1,\n                                      verbose = False).fit(x_train, y_train)\n    \n    print('Random Forest Training Accuracy:{:0.3f}' .format(accuracy_score(y_train, estimator.predict(x_train))))\n    print('Random Forest Testing Accuracy:{:0.3f}'.format(accuracy_score(y_test, estimator.predict(x_test))))\n    num_tree = estimator.estimators_[0]\n    print('\\Visualizing Tree:', 0)\n    graph = Source(tree.export_graphviz(num_tree,\n                                        out_file = None,\n                                        feature_names = x_train.columns,\n                                        class_names = ['stayed', 'quit'],\n                                        filled = True))\n                                                           \n     \n    display(Image(data = graph.pipe(format = 'png')))\n                                     ","f2730443":"from yellowbrick.model_selection import FeatureImportances\nplt.rcParams['figure.figsize'] = (12,8)\nplt.style.use(\"ggplot\")","e46d73ce":"from yellowbrick.classifier import ROCAUC","08d244c1":"rf = RandomForestClassifier(bootstrap = 'True', class_weight=None, criterion='gini', max_depth=5,\n            max_features= 'auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0,n_estimators = 100, n_jobs=-1,\n            oob_score=False, random_state=1,\n            verbose = False,\n            warm_start = False)\n\nvisualizer = ROCAUC(rf, classes=[\"stayed\", \"quit\"])\n\nvisualizer.fit(x_train, y_train)        # Fit the training data to the visualizer\nvisualizer.score(x_test, y_test)        # Evaluate the model on the test data\nvisualizer.poof();","a8668b08":"## Plotting Code ##\ndt = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n            splitter='best')\n\nvisualizer = ROCAUC(dt, classes=[\"stayed\", \"quit\"])\n\nvisualizer.fit(x_train, y_train)        # Fit the training data to the visualizer\nvisualizer.score(x_test, y_test)        # Evaluate the model on the test data\nvisualizer.poof();","b71b71e6":"from sklearn.linear_model import LogisticRegressionCV\n\nlogit = LogisticRegressionCV(random_state=1, n_jobs=-1,max_iter=500,\n                             cv=10)\n\nlr = logit.fit(x_train, y_train)\n\nprint('Logistic Regression Accuracy: {:.3f}'.format(accuracy_score(y_test, lr.predict(x_test))))\n\nvisualizer = ROCAUC(lr, classes=[\"stayed\", \"quit\"])\n\nvisualizer.fit(x_train, y_train)        # Fit the training data to the visualizer\nvisualizer.score(x_test, y_test)        # Evaluate the model on the test data\nvisualizer.poof();","f8c850c8":"### Task 9: Feature Importance and Evaluation Metrics\n---","cd5c66bd":"### Optional: Comparison with Logistic Regression Classifier\n---","58d6aed2":"### Exploratory Data Analysis\n---","caa9bd66":"About:\n \n - Decision trees are non-parametric models which can model arbitrarily complex relations between inputs and outputs, without any a priori assumption\n \n- Decision trees handle numeric and categorical variables\n\n- They implement feature selection, making them robust to noisy features (to an extent)\n\n- Robust to outliers or errors in labels\n\n- Easily interpretable by even non-ML practioners.","01e6c26e":"### Task 6 & 7: Build an Interactive Decision Tree Classifier\n---","25d03ebe":"### Import Libraries\n---","a82d5016":"### Task 8: Build an Interactive Random Forest Classifier\n---","e6a01d2d":"- Data is a finite set $$\\mathbb{L}=\\{(x_i,y_i)|i=0, ..., N-1\\}$$\nwhere $x_i \\in X = X_1 \\times ... \\times X_p$ and $y_i \\in y$ are randomly drawn from $P_{X,Y}.$\n\nE.g., $(x_i,y_i)=((\\text{salary = low, department = sales, ...}),\\text{quit = 1})$\n\n- The goal is to find a model $\\varphi_\\mathbb{L}: X \\mapsto y$ minimizing $$\\text{Err}(\\varphi_\\mathbb{L}) = \\mathbb{E}_{X,Y}\\{L(Y, \\varphi_\\mathbb{L}(X))\\}.$$","6d2a29ff":"Supervised learning: \n- The inputs are random variables $X = X_1, ..., X_p$;\n- The output is a random variable $Y.$","5a003cde":"Although randomization increases bias, it is possible to get a reduction in variance of the ensemble. Random forests are one of the most robust machine learning algorithms for a variety of problems.\n\n- Randomization and averaging lead to a reduction in variance and improve accuracy\n- The implementations are parallelizable\n- Memory consumption and training time can be reduced by bootstrapping\n- Sampling features and not solely sampling examples is crucial to improving accuracy","bc242356":"![partition](assets\/images\/partition-feature-space.png)\n\n- Decision trees generally have low bias but have high variance.\n- We will solve the high variance problem in Task 8.","d887b8cb":"### Task 3: Encode Categorical Features\n---","6d907266":"<h2 align=\"center\">Predict Employee Churn with Decision Trees and Random Forests<\/h2>","46e2d180":"### Task 5: Create Training and Test Sets\n---","4214462e":"#### Decision trees: partitioning the feature space:","8c6e6326":"### Task 4: Visualize Class Imbalance\n---"}}