{"cell_type":{"d79bf778":"code","c04c2abb":"code","40417d4d":"code","199f0e35":"code","c37c2054":"code","a4707850":"code","5bce2437":"code","d72ec5fe":"code","7ba5e020":"code","30ac7be4":"code","5c69d4ea":"code","05845dbf":"code","3c9d9a81":"code","c5184d58":"code","6e64b37b":"code","bfaff154":"code","db80ba7c":"code","1277ac1b":"code","5aa5e476":"code","2914d9b5":"code","d294b090":"code","e4e0b97d":"code","c3be4647":"code","f269b9a3":"code","a9367e33":"code","a89ac246":"code","2c921255":"code","c0396603":"code","95a60e2c":"code","11de0fac":"code","9ab5a7f6":"code","2ca8d1d5":"code","702a0224":"code","96022e71":"code","109e5e40":"code","0fb42c00":"code","20035c59":"code","334cd56e":"code","18324313":"code","3f4befd6":"code","8dbf565a":"code","5630d9eb":"code","bc23f48a":"code","08e96405":"code","b3f6e579":"markdown","baa70600":"markdown","fcf70441":"markdown","cdedd576":"markdown","d65ec8ed":"markdown","0ff1a8b5":"markdown","b1a55ce3":"markdown","4cc2af65":"markdown","fcc93856":"markdown","d384392b":"markdown","1f602419":"markdown","2c43ffcf":"markdown","39349de2":"markdown","efb976cb":"markdown","dba5e1e5":"markdown","3305a293":"markdown","b1526237":"markdown","8c18bd74":"markdown","e71fdf37":"markdown","1e6bf6a7":"markdown"},"source":{"d79bf778":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\ncolor_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('ggplot')","c04c2abb":"train = pd.read_csv(\"..\/input\/song-popularity-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/song-popularity-prediction\/test.csv\")\nss = pd.read_csv(\"..\/input\/song-popularity-prediction\/sample_submission.csv\")\n\ntrain[\"isTrain\"] = True\ntest[\"isTrain\"] = False\n\ntt = pd.concat([train, test]).reset_index(drop=True).copy()","40417d4d":"FEATURES = [\n    \"song_duration_ms\",\n    \"acousticness\",\n    \"danceability\",\n    \"energy\",\n    \"instrumentalness\",\n    \"key\",\n    \"liveness\",\n    \"loudness\",\n    \"audio_mode\",\n    \"speechiness\",\n    \"tempo\",\n    \"time_signature\",\n    \"audio_valence\",\n]","199f0e35":"train.shape, test.shape, tt.shape","c37c2054":"ncounts = pd.DataFrame([train.isna().mean(), test.isna().mean()]).T\nncounts = ncounts.rename(columns={0: \"train_missing\", 1: \"test_missing\"})\n\nncounts.query(\"train_missing > 0\").plot(\n    kind=\"barh\", figsize=(8, 5), title=\"% of Values Missing\"\n)\nplt.show()","a4707850":"nacols = [\n    \"song_duration_ms\",\n    \"acousticness\",\n    \"danceability\",\n    \"energy\",\n    \"instrumentalness\",\n    \"key\",\n    \"liveness\",\n    \"loudness\",\n]","5bce2437":"tt[\"n_missing\"] = tt[nacols].isna().sum(axis=1)\ntrain[\"n_missing\"] = train[nacols].isna().sum(axis=1)\ntest[\"n_missing\"] = test[nacols].isna().sum(axis=1)","d72ec5fe":"tt[\"n_missing\"].value_counts().plot(\n    kind=\"bar\", title=\"Number of Missing Values per Sample\"\n)","7ba5e020":"tt.query(\"n_missing == 6\")","30ac7be4":"cat_features = [\"key\", \"audio_mode\"]\ntt.groupby(\"audio_mode\")[\"n_missing\"].mean()","5c69d4ea":"tt.groupby(\"time_signature\")[\"n_missing\"].agg(['mean','count'])","05845dbf":"train.groupby(\"song_popularity\")[\"n_missing\"].mean()","3c9d9a81":"tt_missing_tag_df = tt[nacols].isna()\ntt_missing_tag_df.columns = \\\n    [f\"{c}_missing\" for c in tt_missing_tag_df.columns]","c5184d58":"tt = pd.concat([tt, tt_missing_tag_df], axis=1)","6e64b37b":"from sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.metrics import roc_auc_score\n\nlr = LogisticRegressionCV(scoring=\"accuracy\")\n\nX = tt.query(\"isTrain\")[\n    [\n        \"song_duration_ms_missing\",\n        \"acousticness_missing\",\n        \"danceability_missing\",\n        \"energy_missing\",\n        \"instrumentalness_missing\",\n        \"key_missing\",\n        \"liveness_missing\",\n        \"loudness_missing\",\n    ]\n]\n\ny = tt.query(\"isTrain\")[\"song_popularity\"]\n\nlr.fit(X, y)\nlr.score(X, y)\n\npreds = lr.predict_proba(X)[:, 0]\n\nroc_auc_score(y, preds)","bfaff154":"# use_missing (default)\n# or zero_as_missing are params that can be used \nimport lightgbm as lgb\nlgbm_params = {\n    'objective': 'regression',\n    'metric': 'auc',\n    'verbose': -1,\n    'boost_from_average': False,\n    'min_data': 1,\n    'num_leaves': 2,\n    'learning_rate': 1,\n    'min_data_in_bin': 1,\n#     'use_missing': False,\n#     'zero_as_missing': True\n}\n\nmodel = lgb.LGBMClassifier(params=lgbm_params)","db80ba7c":"tt.shape, tt.dropna(axis=0).shape","1277ac1b":"tt.shape, tt.dropna(axis=1).shape","5aa5e476":"# Dropna based only on a subset of columns\n_ = train.dropna(subset=['song_duration_ms'])","2914d9b5":"# Fill with a default value\ntt['song_duration_ms'].fillna(-999).head(5)","d294b090":"# Impute with mean\ntt[\"song_duration_ms_mean_imp\"] = tt[\"song_duration_ms\"].fillna(\n    tt[\"song_duration_ms\"].mean()\n)\n\ntt.loc[tt['song_duration_ms'].isna()] \\\n    [[\"song_duration_ms\", \"song_duration_ms_mean_imp\"]] \\\n    .head(5)","e4e0b97d":"# Impute with median\n\ntt[\"song_duration_ms_median_imp\"] = tt[\"song_duration_ms\"].fillna(\n    tt[\"song_duration_ms\"].median()\n)\n\ntt.loc[tt['song_duration_ms'].isna()] \\\n    [[\"song_duration_ms\",\n      \"song_duration_ms_mean_imp\",\n     \"song_duration_ms_median_imp\"]] \\\n    .head(5)","c3be4647":"ts_data = pd.DataFrame(index=[x \/ 5 for x in range(100)])\nts_data[\"data\"] = np.sin(ts_data.index)\nts_data[\"data_missing\"] = ts_data[\"data\"] \\\n    .sample(frac=0.9, random_state=529)\nts_data[\"data_ffill\"] = ts_data[\"data_missing\"].ffill()\nts_data[\"data_mean_fill\"] = ts_data[\"data_missing\"].fillna(\n    ts_data[\"data_missing\"].mean()\n)","f269b9a3":"ts_data.plot(style=\".-\",\n             figsize=(15, 5),\n             title='Time Series Imputation')\nplt.show()","a9367e33":"sd_mean_map = tt.groupby(\"audio_mode\")[\"song_duration_ms\"] \\\n    .mean().to_dict()\nsd_mean_map","a89ac246":"sd_mean_series = tt['audio_mode'].map(sd_mean_map)","2c921255":"tt[\"song_duration_ms_mean_audio_mode\"] = \\\n    tt[\"song_duration_ms\"].fillna(sd_mean_series)","c0396603":"tt.query('song_duration_ms_missing == True') \\\n    [['id','audio_mode','song_duration_ms_mean_audio_mode']].head(5)","95a60e2c":"from sklearn.impute import SimpleImputer\nimptr = SimpleImputer(strategy=\"mean\", add_indicator=False)","11de0fac":"train['song_duration_ms'].mean()","9ab5a7f6":"ax = train['danceability'] \\\n    .plot(kind='hist', bins=50,\n          title='distribution of song_duration')\nax.axvline(train['danceability'].mean(),\n           color='black')\nax.axvline(train['danceability'].median(),\n           color='orange')","2ca8d1d5":"# Fit \/ Transform on train, transform only on val\/test\ntr_imp = imptr.fit_transform(train[FEATURES])\ntest_imp = imptr.transform(test[FEATURES])","702a0224":"# For kaggle competition you can kind of cheat by fitting on all data\ntt_impute = imptr.fit_transform(tt[FEATURES])\ntt_simple_impute = pd.DataFrame(tt_impute, columns=FEATURES)\ntt_simple_impute.head()","96022e71":"from sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn.impute import IterativeImputer","109e5e40":"%%time\nit_imputer = IterativeImputer(max_iter=10)\ntrain_iterimp = it_imputer.fit_transform(train[FEATURES])\ntest_iterimp = it_imputer.transform(test[FEATURES])\ntt_iterimp = it_imputer.fit_transform(tt[FEATURES])\n\n# Create train test imputed dataframe\ntt_iter_imp_df = pd.DataFrame(tt_iterimp, columns=FEATURES)","0fb42c00":"# Save this off to use later\ntt_iter_imp_df.to_parquet(\"tt_iterative_imp.parquet\")","20035c59":"%%time\nfrom sklearn.impute import KNNImputer\n\nknn_imptr = KNNImputer(n_neighbors=1)\ntrain_knnimp = knn_imptr.fit_transform(train[FEATURES])\ntest_knnimp = knn_imptr.transform(test[FEATURES])\ntt_knnimp = knn_imptr.fit_transform(tt[FEATURES])\ntt_imp = pd.DataFrame(tt_knnimp, columns=FEATURES)\n\n# Create KNN Train\/Test imputed dataframe\nknn_imp_df = pd.DataFrame(tt_imp, columns=FEATURES)","334cd56e":"# Save this off to use later\nknn_imp_df.to_parquet(\"tt_knn_imp.parquet\")","18324313":"# !rm -r kuma_utils\n!git clone https:\/\/github.com\/analokmaus\/kuma_utils.git","3f4befd6":"import sys\nsys.path.append(\"kuma_utils\/\")\nfrom kuma_utils.preprocessing.imputer import LGBMImputer","8dbf565a":"%%time\nlgbm_imtr = LGBMImputer(n_iter=100, verbose=True)\n\ntrain_lgbmimp = lgbm_imtr.fit_transform(train[FEATURES])\ntest_lgbmimp = lgbm_imtr.transform(test[FEATURES])\ntt_lgbmimp = lgbm_imtr.fit_transform(tt[FEATURES])\ntt_imp = pd.DataFrame(tt_lgbmimp, columns=FEATURES)\n\n# Create LGBM Train\/Test imputed dataframe\nlgbm_imp_df = pd.DataFrame(tt_imp, columns=FEATURES)","5630d9eb":"tt_lgbm_imp = pd.concat([tt[[\"id\", \"isTrain\",\n                             \"song_popularity\"]],\n                         tt_lgbmimp], axis=1)","bc23f48a":"tt_lgbm_imp.to_parquet(\"tt_lgbm_imp.parquet\")","08e96405":"fig, axs = plt.subplots(2, 2, figsize=(8, 8))\naxs = axs.flatten()\n\ntt_simple_impute['song_duration_ms'] \\\n    .plot(kind='hist',\n          bins=50,\n          ax=axs[0],\n          title='Simple Impute',\n         color=color_pal[0])\n\ntt_iter_imp_df['song_duration_ms'] \\\n    .plot(kind='hist',\n          bins=50,\n          ax=axs[1],\n          title='Iterative Impute',\n         color=color_pal[1])\n\nknn_imp_df['song_duration_ms'] \\\n    .plot(kind='hist',\n          bins=50,\n          ax=axs[2],\n          title='KNN Impute',\n         color=color_pal[2])\n\ntt_lgbm_imp['song_duration_ms'] \\\n    .plot(kind='hist',\n          bins=50,\n          ax=axs[3],\n          title='LGBM Impute',\n         color=color_pal[3])\nplt.show()","b3f6e579":"## KNN Imputer\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.KNNImputer.html\n\nImputation for completing missing values using k-Nearest Neighbors.","baa70600":"# Level 4: LightGBM Imputer!!\n\n![img](https:\/\/i.imgur.com\/i0vWAcc.jpg)\n\n<!-- ![img](https:\/\/avatars.githubusercontent.com\/u\/9374781?v=4) -->\n\nShoutout to analokmaus:\n<div> <img src=\"https:\/\/avatars.githubusercontent.com\/u\/9374781?v=4\" alt=\"Drawing\" style=\"height: 100px;\"\/><\/div>\n\nhttps:\/\/github.com\/analokmaus\/kuma_utils\/blob\/master\/preprocessing\/imputer.py","fcf70441":"# THE END!\n\nI hope you've learned something new about how to deal with missing values!","cdedd576":"# Level 0: Do Nothing!\nhttps:\/\/lightgbm.readthedocs.io\/en\/latest\/Advanced-Topics.html\n- Tree Type Models like LightGBM and XGBoost can work with NA values\n\n- Other types of regression or neural networks will require some sort of imputation.","d65ec8ed":"# Handling with Missing Values\n\nIn this notebook we will discuss some techniques that can be used to deal with missing values in tabular data.\n\nThis notebook was presented live on Abhishek Thakur's youtube channel. \n- Watch here: https:\/\/www.youtube.com\/watch?v=EYySNJU8qR0\n- My twitch channel: [www.twitch.tv\/medallionstallion_](https:\/\/www.twitch.tv\/medallionstallion_)\n- My youtube channel: https:\/\/www.youtube.com\/channel\/UCxladMszXan-jfgzyeIMyvw\n\n<div> <img src=\"https:\/\/i.imgur.com\/SkG4HMS.jpg\" alt=\"Drawing\" style=\"height: 350px;\"\/><\/div> ","0ff1a8b5":"# Which Method is Best?\n- Try and check on cross validation!","b1a55ce3":"# Protip:\n- Try to predict the target using only missing value indicators as features","4cc2af65":"## Protip: filling for time series data.\n- `ffill()` and `bfill()`","fcc93856":"# Check The Imputation Distribution","d384392b":"# Level 1: Drop 'em\n\n![img](https:\/\/i.imgur.com\/hmqbGOo.jpg)\n\n- Drop all rows with missing values (axis=0)\n    - This doesn't work on test because we can't just not predict those values\n- Drop all missing columns:\n    - This leaves us with much less features to use","1f602419":"# Quick Missing Values EDA\n## What are the counts of missing values in train vs. test?","2c43ffcf":"## Do we see an imbalance in missing values when splitting by other features?","39349de2":"## GroupBy Fill\n- Use the average value grouped by a different feature\n- As an example we will use the `audio_mode` feature","efb976cb":"## How many missing values per observation?","dba5e1e5":"## SimpleImputer\n- Impute with the mean value, median, and just a single value\n- `add_indicator` will add the indicator column","3305a293":"# Level 2: Pandas Imputation\n![img](https:\/\/i.imgur.com\/Qas2RHT.jpg)\n- Fill missing values using `fillna`\n- `ffill` and `bfill`\n- `groupby` fills","b1526237":"# Level 3: Sklearn Imputation\n![img](https:\/\/i.imgur.com\/eVKTU5p.jpg)\n\n- `SimpleImputer` Similar to pandas `fillna`\n- `IterativeImputer`\n- `KNNImputer`\n\nUsing SKlearn is good because it provides a `fit` and `transform` method. This allows us to fit on the training and transform on both the train validation.\n\nIn real world situations you will want to `fit` and `transform` *within* your cross validation loop to ensure no leakage.\n\n**Protip: in kaggle competitions we sometimes know all of the features for the test set. So it may be advantageous to fit on all the data.** This doesn't work for \"code\" competitions because we can't see the test data before out model predicts.","8c18bd74":"# Before we start.... why are the missing values there?\n\nExample causes of missing data:\n- Sensor data where the sensor went offline.\n- Survey data where some questions were not answered.\n- A Kaggle competition where the host wants to make the problem hard :D\n\nYou need to understand *Why* you have missing values before deciding on the approach for dealing with them.","e71fdf37":"# Prep - Create Tag Columns with Missing Indicators","1e6bf6a7":"# Iterative Imputer!\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.IterativeImputer.html\n\nMultivariate imputer that estimates each feature from all the others.\n\nA strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion.\n\nUses by default the `BayesianRidge` model to impute\n\n- *Protip* : We want to `fit` and `predict` on all columns! This is because the model is using all features to help fill the missing values."}}