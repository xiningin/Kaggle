{"cell_type":{"5ae0c6ca":"code","72570c4b":"code","fa0c41da":"code","dc15667d":"code","4e5332e5":"code","6d944141":"code","e3704494":"code","1d9998c4":"code","5b9ae272":"code","ee76f6e3":"code","18f2d712":"code","2e95cab0":"code","93ca584c":"code","d01ce95d":"markdown","070de21e":"markdown","922a9bda":"markdown","7a6e947b":"markdown","5d456b52":"markdown","3955575c":"markdown","57fb785a":"markdown","5bdc03f6":"markdown","19a3b89f":"markdown","7b2910e5":"markdown","0817827a":"markdown","717ba1dc":"markdown","0dc681fb":"markdown","e54da484":"markdown"},"source":{"5ae0c6ca":"from tqdm import tqdm \nimport numpy as np \n\nembeddings_index = {}\nEMBEDDING_FILE = '..\/input\/fatsttext-common-crawl\/crawl-300d-2M\/crawl-300d-2M.vec'\nf = open(EMBEDDING_FILE, encoding=\"utf8\")\nfor line in tqdm(f):\n    values = line.split()\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[values[0]] = coefs\nf.close()","72570c4b":"from sklearn.metrics.pairwise import linear_kernel\nfrom nltk.corpus import stopwords \nimport pandas as pd\nimport string, os\nimport json\n\nignorewords = [\"year\", \"experience\", \"full-time\", \"part-time\", \"part\", \"time\", \"full\", \"university\", \"college\", \"degree\", \"major\"]\nstopwords = stopwords.words('english')\nnumbs  = \"0123456789\"\n\n\"\"\" function to cleanup the text \"\"\"\ndef _cleanup(text):\n    text = text.lower()\n    text = \" \".join([c for c in text.split() if c not in stopwords])\n    for c in string.punctuation:\n        text = text.replace(c, \" \")\n    text = \" \".join([c for c in text.split() if c not in stopwords])\n    words = []\n    for wrd in text.split():\n        if len(wrd) <= 2: \n            continue\n        if wrd in ignorewords:\n            continue\n        words.append(wrd)\n    text = \" \".join(words)    \n    return text\n\n\"\"\" function to clean the filename and obtain the job role title\"\"\"\ndef _clean(fname):\n    for num in numbs: \n        fname = fname.split(num)[0].strip()\n    return fname.title()\n\nresults = []\nbase_path = \"..\/input\/data-science-for-good-city-of-los-angeles\/cityofla\/CityofLA\/Job Bulletins\/\"\nfor fname in os.listdir(base_path):\n    if fname == \"POLICE COMMANDER 2251 092917.txt\":\n        continue\n\n    txt = open(base_path + fname).read()\n    lines = txt.split(\"\\n\")\n    start = 0\n    rel_lines = []\n    for i, l in enumerate(lines):\n        if 'requirement' in l.lower():\n            start = i\n            break\n    for i, l in enumerate(lines[start+1:]):\n        if \"substituted\" in l.lower():\n            break\n        if l.isupper():\n            break\n        rel_lines.append(l)\n    req1 = \" \".join(rel_lines)\n    req = _cleanup(req1)\n    d = {'cleaned' : req, 'original' : req1, 'title' : _clean(fname)}\n    results.append(d)\n    \ndata = pd.DataFrame(results)[['title','original','cleaned']]\ndata.head()","fa0c41da":"\"\"\" function to generate document vector by aggregating word embeddings \"\"\"\ndef generate_doc_vectors(s):\n    words = str(s).lower().split() \n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        if w in embeddings_index:\n            M.append(embeddings_index[w])\n    v = np.array(M).sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    return v \/ np.sqrt((v ** 2).sum())\n\nreq_vectors = []\nfor i,r in data.iterrows():\n    req_vectors.append(generate_doc_vectors(r['cleaned']))","dc15667d":"_interactions = linear_kernel(req_vectors, req_vectors)\n\n_edges = {}\nfor idx, row in data.iterrows():\n    similar_indices = _interactions[idx].argsort()[:-100:-1]\n    similar_items = [(_interactions[idx][i], data['title'][i]) for i in similar_indices]\n    _edges[row['title']] = similar_items[:20]","4e5332e5":"\"\"\" function to identify the implicit links \"\"\"\ndef get_treedata(k, threshold, limit):\n    k = k.title()\n\n    txt = \"\"\"<h3><font color=\"#aa42f4\">Requirement Texts: <\/font><\/h3>\"\"\"\n    txt += \"<h3><font color='#196ed6'>\" + k + \"<\/font><\/h3>\"\n    req = data[data['title'] == k]['original'].iloc(0)[0]\n    if len(req) > 350:\n        req = req[:350] + \" ...\"\n    txt += \"<p><b>Requirements: <\/b>\" + req + \"<\/p>\"\n\n    treedata = {\"name\" : k, \"children\" : [], \"color\" : '#97f4e3', \"size\":25, \"exp\" : \"\"}\n    edges = _edges[k]\n    edges = [_ for _ in edges if _[1] != k]\n    edges = [_ for _ in edges if _[0] >= threshold]\n    ignore = ['principal', \"chief\", \"director\", \"supervisor\"]\n    counter = 0\n    for i, edge in enumerate(edges):\n        if any(upper in edge[1].lower() for upper in ignore):\n            continue\n        d = {\"name\" : edge[1], \"children\" : [], \"color\" : \"red\", \"size\":15, \"exp\" : edge[0]}\n        treedata['children'].append(d)\n        counter += 1\n        if counter == limit:\n            break\n        txt += \"<h3><font color='#f93b5e'>\" + edge[1] + \"(Context Similarity: \"+str(round(edge[0], 2))+\")<\/font><\/h3>\"\n        req1 = data[data['title'] == edge[1]]['original'].iloc(0)[0]\n        if len(req1) > 350:\n            req1 = req1[:350] + \" ...\"\n        txt += \"<b>Requirements: <\/b>\" + req1 + \"\"\n    return treedata, txt","6d944141":"from IPython.core.display import display, HTML, Javascript\nimport IPython.display\n\n\"\"\" function to generate required javascript and HTML for the visualization \"\"\"\ndef _get_js(treedata, idd):\n    html = \"\"\"<style>  \n        .node circle {\n          fill: #fff;\n          stroke: steelblue;\n          stroke-width: 3px;\n        }\n        .node text { font: 12px sans-serif; }\n        .node--internal text {\n          text-shadow: 0 1px 0 #fff, 0 -1px 0 #fff, 1px 0 0 #fff, -1px 0 0 #fff;\n        }\n        .link {\n          fill: none;\n          stroke: #ccc;\n          stroke-width: 2px;\n        }\n    <\/style>\n    <svg height='340' id='\"\"\"+idd+\"\"\"' width=\"760\"><\/svg>\"\"\"\n\n    js=\"\"\"require.config({\n        paths: {\n            d3: \"https:\/\/d3js.org\/d3.v4.min\"\n        }\n    });\n    require([\"d3\"], function(d3) {\n        var treeData =\"\"\"+json.dumps(treedata)+\"\"\";\n\n        \/\/ set the dimensions and margins of the diagram\n        var margin = {top: 40, right: 90, bottom: 50, left: 90},\n            width = 660 - margin.left - margin.right,\n            height = 290 - margin.top - margin.bottom;\n\n        \/\/ declares a tree layout and assigns the size\n        var treemap = d3.tree()\n            .size([width, height]);\n\n        \/\/  assigns the data to a hierarchy using parent-child relationships\n        var nodes = d3.hierarchy(treeData);\n\n        \/\/ maps the node data to the tree layout\n        nodes = treemap(nodes);\n\n        \/\/ append the svg obgect to the body of the page\n        \/\/ appends a 'group' element to 'svg'\n        \/\/ moves the 'group' element to the top left margin\n        var svg = d3.select('#\"\"\"+idd+\"\"\"').append(\"svg\")\n              .attr(\"width\", width + margin.left + margin.right)\n              .attr(\"height\", height + margin.top + margin.bottom),\n            g = svg.append(\"g\")\n              .attr(\"transform\",\n                    \"translate(\" + margin.left + \",\" + margin.top + \")\");\n\n        \/\/ adds the links between the nodes\n        var link = g.selectAll(\".link\")\n            .data( nodes.descendants().slice(1))\n          .enter().append(\"path\")\n            .attr(\"class\", \"link\")\n            .attr(\"d\", function(d) {\n               return \"M\" + d.x + \",\" + d.y\n                 + \"C\" + d.x + \",\" + (d.y + d.parent.y) \/ 2\n                 + \" \" + d.parent.x + \",\" +  (d.y + d.parent.y) \/ 2\n                 + \" \" + d.parent.x + \",\" + d.parent.y;\n               });\n\n        \/\/ adds each node as a group\n        var node = g.selectAll(\".node\")\n            .data(nodes.descendants())\n          .enter().append(\"g\")\n            .attr(\"class\", function(d) { \n              return \"node\" + \n                (d.children ? \" node--internal\" : \" node--leaf\"); })\n            .attr(\"transform\", function(d) { \n              return \"translate(\" + d.x + \",\" + d.y + \")\"; });\n\n        \/\/ adds the circle to the node\n        node.append(\"image\")\n        .attr(\"xlink:href\", function(d) { return \"https:\/\/image.flaticon.com\/icons\/png\/512\/306\/306473.png\" })\n        .attr(\"x\", function(d) { return -15;})\n        .attr(\"y\", function(d) { return -15;})\n        .attr(\"height\", 30)\n        .attr(\"width\", 30);\n\n        \/\/ adds the text to the node\n        node.append(\"text\")\n          .attr(\"dy\", \".35em\")\n          .attr(\"y\", function(d) { return d.children ? -20 : 20; })\n          .style(\"text-anchor\", \"middle\")\n          .text(function(d) { return d.data.name; })\n          .attr(\"transform\", \"rotate(-10)\" );        \n    });\"\"\"\n    \n    return html, js\n\ndef _implicit(title, idd, threshold=0.88, limit = 4):\n    treedata, txt = get_treedata(title, threshold, limit)\n    h, js = _get_js(treedata, idd)\n    h = display(HTML(h))\n    j = IPython.display.Javascript(js)\n    IPython.display.display_javascript(j)\n    display(HTML(txt))","e3704494":"_implicit(\"Senior Administrative Clerk\", idd=\"a8\", threshold = 0.75, limit = 6)","1d9998c4":"_implicit(\"Chief Benefits Analyst\", idd=\"a1\")","5b9ae272":"_implicit(\"Ems Nurse Practitioner Supervisor\", idd=\"a2\")","ee76f6e3":"_implicit(\"HELICOPTER MECHANIC SUPERVISOR\", idd=\"a3\")","18f2d712":"_implicit(\"General Automotive Supervisor\", idd=\"a4\")","2e95cab0":"_implicit(\"Steam Plant Maintenance Supervisor\", idd=\"a5\")","93ca584c":"_implicit(\"Wastewater Treatment Electrician Supervisor\", idd=\"a6\")","d01ce95d":"### 7.7 Wastewater Treatment Electrician Supervisor","070de21e":"<div id=\"2\"><\/div>\n## <font color=\"#ff3fb5\">2. Load and Clean the Requirements Text  <\/font>  \n\nNext, we load and perform text clenaing on the requirement texts of job bulletins. We can either load directly from files, or use the structured file generated in Kernel 1. ","922a9bda":"## <font color=\"#ff3fb5\">End Notes<\/font>\n\nThis was the last kernel of my submission. Hope you have gone through all the other parts. If there are any questions about my entire solution, please feel free to share them in the comments. If you liked it upvote. I really enjoyed working on this competition. Other Links: <a href='https:\/\/www.kaggle.com\/shivamb\/1-description-structuring-engine-cola'>Part 1<\/a> | <a href='https:\/\/www.kaggle.com\/shivamb\/2-encourage-diversity-reduce-bias-cola'>Part 2<\/a> | <a href='https:\/\/www.kaggle.com\/shivamb\/3-impact-of-ctl-content-tone-language-cola'>Part 3<\/a> | <a href='https:\/\/www.kaggle.com\/shivamb\/4-promotional-pathway-discoverability-cola'>Part 4 <\/a> | <a href='https:\/\/www.kaggle.com\/shivamb\/5-implicit-promotional-pathways-discoverability\/'> Part 5 <\/a>   \n\nThanks. \n\n-- Shivam","7a6e947b":"<div id=\"4\"><\/div>\n## <font color=\"#ff3fb5\">4. Compute Contextual Similarity between the Requirements <\/font>  \n\nNext, we perform the pairwise similarity between the vectors. For this purpose, I have used [linear_kernel](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.pairwise.linear_kernel.html) from the python's sklearn package. The output of this snippet is a matrix which stores the similarity between any two vectors in the entire dataset. Additionally, I have also iterated for every job class, and identified top 20 most contextually similar classes. ","5d456b52":"### 7.6 Steam Plant Maintenance Supervisor","3955575c":"<div align='center'><font size=\"6\" color=\"#ff3fb5\">Data Science For Good : CoLA<\/font><\/div>\n<div align='center'><font size=\"4\" color=\"#ff3fb5\">A Complete Pipeline for Structuring, Analysis and Recommendation<\/font><\/div>\n<div align='center'><font size=\"3\" color=\"#ff3fb5\">Improve Hiring Process and Decisions<\/font><\/div>\n<hr>\n\n<p style='text-align:justify'><b>Key Objectives:<\/b> Keeping these challenges in mind, an ideal solution for the City of Los Angeles has following key objectives: Develop an nlp framework to accurately structurize the job descriptions. Develop an analysis framework to identify the implict bias in the text and encourage diversity. Develop a system which can clearly identify the promotion pathways within the organization.<\/p>\n\n<b>My Submission:<\/b> Following are parts of Kernels Submissions in order:<br>\n\n<ul>\n    <li><a href=\"https:\/\/www.kaggle.com\/shivamb\/1-description-structuring-engine-cola\">Part 1: Job Bulletin Structuring Engine - City of Los Angeles <\/a>  <\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/shivamb\/2-encourage-diversity-reduce-bias-cola\">Part 2: Encourage Diversity and Remove Unconsious Bias from Job Bulletins - A Deep Analysis<\/a>  <\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/shivamb\/3-impact-of-ctl-content-tone-language-cola\">Part 3: Impact of Content, Tone, and Language : CTL Analysis for CoLA<\/a>  <\/li> \n    <li><a href=\"https:\/\/www.kaggle.com\/shivamb\/4-promotional-pathway-discoverability-cola\">Part 4: Increasing the Discoverability of Promotional Pathways (Explicit)  <\/a><\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/shivamb\/4-promotional-pathway-discoverability-cola\">Part 5: Implicit Promotional Pathways Discoverability<\/a><\/li><\/ul>\n<div align='center'><font size=\"5\" color=\"#ff3fb5\">Part 5: Implicit - Promotional Pathways Discoverability<\/font><\/div>\n<div align='center'>Other Parts: <a href='https:\/\/www.kaggle.com\/shivamb\/1-description-structuring-engine-cola'>Part 1<\/a> | <a href='https:\/\/www.kaggle.com\/shivamb\/2-encourage-diversity-reduce-bias-cola'>Part 2<\/a> | <a href='https:\/\/www.kaggle.com\/shivamb\/3-impact-of-ctl-content-tone-language-cola'>Part 3<\/a> | <a href='https:\/\/www.kaggle.com\/shivamb\/4-promotional-pathway-discoverability-cola'>Part 4<\/a> | <a href='https:\/\/www.kaggle.com\/shivamb\/5-implicit-promotional-pathways-discoverability'>Part 5<\/a> <\/div>\n<p style='text-align:justify'>In the last kernel, I explored the method to identify and visualize promotional pathways which are mentioned explicitly in the job requirements. In this kernel, I have made an attempt to identify the promotional pathways using the contextual anlaysis of job requirements (implict). I have shared the methodology below: <\/p>       \n\n**Methodology:**   \n> 1. From the structured data, obtain the complete requirement text and perform basic text cleaning.   \n> 2. Represent every requirement text as a vector in which the context and semantics are preserved. I have used Pre-Trained Word Embeddings using [fasttext](https:\/\/fasttext.cc\/) for this purpose.  \n> For every job class requirement, representing it as a vector is very helpful as the word embedding vectors can be used to identify other job classes which shares similar requirements. In cases when a job class is not mentioned explicitly in the requirment, this method can be used to identify implict links.   \n> 3. Compute a contextual similarity matrix which gives similarity scores of one class with the others.  \n> 4. Use the similarity matrix to identify possible candidates. Filter them using a dictionary of seniority levels and flexible ngram matching to ensure that parent job class is actually linked to a child job class. \n\nThe overview of the methodology is shown in the following process flow diagram. \n\n<br>\n![](https:\/\/i.imgur.com\/WDzTuSQ.png)\n<br>\n\nThere are two parts in this method:   \n\nA: Pre-Processing Stage : Compute requirement context vectors, and context similarity matrix   \nB: Identification Stage : Finding the implicit links using similarity scores, dictionary, and ngram matching. \n\nFollowing are the contents of the kernel: \n\n## <font color=\"#ff3fb5\">Contents:<\/font>  \n\n<a href='#1'>1. Load Pre-Trained Word Embeddings<\/a>   \n<a href='#2'>2. Load and Clean the Requirements Text Data<\/a>   \n<a href='#3'>3. Convert Requirements to Requirements Context Vectors<\/a>   \n<a href='#4'>4. Compute Contextual Similarity Matrix<\/a>   \n<a href='#5'>5. Identify and Filter the Implicit Links<\/a>   \n<a href='#6'>6. Write the Visualization Functions<\/a>   \n<a href='#7'>7. Examples<\/a>   \n\n<div id=\"1\"><\/div>\n## <font color=\"#ff3fb5\">1. Load Pre-Trained Word Embeddings<\/font>  \n\nA popular idea in modern machine learning is to represent words by vectors (also called word embeddings). These vectors capture hidden information about a language, like word analogies or semantic. Let's load the 2M word embedding vectors in a python object from fasttext dataset. ","57fb785a":"### 7.4 Helicopter Mechanic Supervisor","5bdc03f6":"In this example as well, we see that specific job class names are not mentioned in the requirement texts however the work to be done or the responsibilities are almost similar. Hence their exist an implict link between the two classes. \n\n### 7.5 General Automotive Supervisor","19a3b89f":"<div id=\"5\"><\/div>\n## <font color=\"#ff3fb5\">5. Identify the Implicit Connections between requirements <\/font>  \n\nNow, we need to ensure that the parent class is one of the higher classes (director, principle, chief, senior etc) and child class is one of the lower class. In this function, formatting of links is also performed which makes it easier for visualization purposes.","7b2910e5":"> - The requirement texts of these job roles asks for similar work experience, hence employees who are retirement plana manager roles can also become analysts after spending considerable amount of experience required. \n\n### 7.3 EMS Nurse Practitioner Supervisor","0817827a":"<div id=\"3\"><\/div>\n## <font color=\"#ff3fb5\">3. Convert Requirements to Requirement Context Vector <\/font>  \n\nNow, we will write a function to convert the requirement text into a vector form. This vector maintains the content, context, and semantics of the original text. The idea of generating a document vector from word vectors is to perform simple aggregation.","717ba1dc":"<div id=\"7\"><\/div>\n## <font color=\"#ff3fb5\">7. Some examples <\/font>  \n\nNow, let's look at some examples of job classes and the implicit links between them. \n\n### 7.1 Senior Administrative Clerk","0dc681fb":"> - In this example of \"Senior Administrative Clerk\" we can observe the position requires 1 year of clerical experience. We see that in related jobs which have contextually similar requirements also asks for similar experience. Secretary - 1 year of clerical experience, Accounting clerk - 2 years of clerical work, and so on. \n> - This means that the employees who are serving as these positions and have spent enough experience, are eligible to be promoted as Senior Administrative Clerk role. \n\n### 7.2 Chief Benefits Analyst","e54da484":"> - The methodology works for most of the classes but at the same time it might gives wrong results for some classes. This is possibly because of similarity of words which are not important to a role (example - experience, applicant etc.)  \n\n<div id=\"6\"><\/div>\n## <font color=\"#ff3fb5\">6. Visualizing the Implict Links <\/font>  \n\nIn the next cell, I have shared the code to visualize the links. "}}