{"cell_type":{"1626b608":"code","ee4704c5":"code","5b116e5a":"code","beb7fa04":"code","d8074761":"code","a554a072":"code","2ea7cccd":"code","98a8a9f8":"code","dd40f69e":"code","dcdb2a83":"code","15fedcdf":"code","c704e239":"code","4baa617c":"code","6290f3b4":"code","a7fd52f2":"code","5cc61744":"code","6252b09a":"code","d817dd61":"code","6fb9c8f1":"code","e5348e3a":"code","05b67757":"code","81ee6e44":"code","24554d3e":"code","d4055098":"code","1636198d":"code","bb6de45c":"code","3953286f":"code","6d79914d":"markdown","fb620620":"markdown","ca83db14":"markdown","f83b8ee9":"markdown","bfcde9dc":"markdown","a71d0f01":"markdown","e28b2882":"markdown","c103180b":"markdown","aa556e0e":"markdown","00e8780e":"markdown","fe0d003b":"markdown","ad9356b2":"markdown"},"source":{"1626b608":"# Default kaggle imports\n\nimport numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ee4704c5":"# Useful imports necessary for our analysis\n\nimport torch\nimport torchtext\nfrom torchtext import data\nfrom torch.utils.data import DataLoader\n\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchtext.data.utils import ngrams_iterator\n\nimport spacy # I'll be making use of spacy for text preprocessing\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nimport string, re\n\nfrom torch.autograd import Variable\nimport time\nimport copy\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data.dataset import random_split\n\nfrom sklearn.model_selection import train_test_split\nfrom torchtext.vocab import Vectors, GloVe\nfrom matplotlib.pyplot import plot, hist, xlabel, legend\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nimport logging\nimport warnings\nlogging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt='%H:%M:%S', level=logging.INFO)\nwarnings.filterwarnings(\"ignore\")","5b116e5a":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\n\ntrain.drop_duplicates(subset=['text'], inplace=True)\nprint('Training Set Shape = {}'.format(train.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(train.memory_usage().sum() \/ 1024**2))\nprint('Test Set Shape = {}'.format(test.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(test.memory_usage().sum() \/ 1024**2))\nprint('\\n\\tFirst five rows of our data:\\n')\nprint(train.head())","beb7fa04":"# probing the keyword and location columns\nmissing_cols = ['keyword', 'location']\nfig, ax = plt.subplots(ncols=2, figsize=(15, 6))\n\nsns.barplot(x=train[missing_cols].isnull().sum().index, y=train[missing_cols].isnull().sum().values, ax=ax[0])\nsns.barplot(x=test[missing_cols].isnull().sum().index, y=test[missing_cols].isnull().sum().values, ax=ax[1])\n\nax[0].set_ylabel('Missing Value Count', size=15, labelpad=20)\nax[0].tick_params(axis='x', labelsize=15)\nax[0].tick_params(axis='y', labelsize=15)\nax[1].tick_params(axis='x', labelsize=15)\nax[1].tick_params(axis='y', labelsize=15)\n\nax[0].set_title(\"Train Set\", fontsize=13)\nax[1].set_title(\"Test Set\", fontsize=13)\n\nplt.show()","d8074761":"null_train = train.isnull().sum() * 100 \/ len(train)\nnull_test = test.isnull().sum() * 100\/ len(test)\n\npd.concat([null_train, null_test], axis=1).rename(columns={0:'train missing %', 1:'test missing %'}).head()","a554a072":"# fill up all missing values\nfor df in [train, test]:\n    for col in ['keyword', 'location']:\n        df[col].fillna(f'no_{col}', inplace=True)","2ea7cccd":"print(f'Unique values in keyword = {train[\"keyword\"].nunique()} (Training) - {test[\"keyword\"].nunique()} (Test)')\nprint(f'Unique values in location = {train[\"location\"].nunique()} (Training) - {test[\"location\"].nunique()} (Test)')","98a8a9f8":"train['target_mean'] = train.groupby('keyword')['target'].transform('mean')\n\nfig = plt.figure(figsize=(8, 72), dpi=100)\n\nsns.countplot(y=train.sort_values(by='target_mean', ascending=False)['keyword'],\n              hue=train.sort_values(by='target_mean', ascending=False)['target'])\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()\n\ntrain.drop(columns=['target_mean'], inplace=True)","dd40f69e":"# viewing the distribution of the target variables\n\nfig, axes = plt.subplots(ncols=2, figsize=(17, 4), dpi=100)\nplt.tight_layout()\n\ntrain.groupby('target').count()['id'].plot(kind='pie', ax=axes[0], labels=['Not Disaster (57%)', 'Disaster (43%)'])\nsns.countplot(x=train['target'], hue=train['target'], ax=axes[1])\n\naxes[0].set_ylabel('')\naxes[1].set_ylabel('')\naxes[1].set_xticklabels(['Not Disaster (4342)', 'Disaster (3271)'])\naxes[0].tick_params(axis='x', labelsize=15)\naxes[0].tick_params(axis='y', labelsize=15)\naxes[1].tick_params(axis='x', labelsize=15)\naxes[1].tick_params(axis='y', labelsize=15)\n\naxes[0].set_title('Target Distribution in Training Set', fontsize=13)\naxes[1].set_title('Target Count in Training Set', fontsize=13)\n\nplt.show()","dcdb2a83":"# viewing the word cloud to see most prominent words\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train[\"text\"], title=\"Word Cloud of Questions\")","15fedcdf":"from collections import defaultdict\ntrain1_df = train[train[\"target\"]==1]\ntrain0_df = train[train[\"target\"]==0]\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), '#bccbde')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), '#c2dde6')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of sincere questions\", \n                                          \"Frequent words of insincere questions\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')","c704e239":"freq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), '#ff414e')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), '#12343b')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent bigrams of sincere questions\", \n                                          \"Frequent bigrams of insincere questions\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='#f7f7f7', title=\"Bigram Count Plots\")\npy.iplot(fig, filename='word-plots')","4baa617c":"freq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), '#393f4d')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), '#1d1e22')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04, horizontal_spacing=0.2,\n                          subplot_titles=[\"Frequent trigrams of sincere questions\", \n                                          \"Frequent trigrams of insincere questions\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Count Plots\")\npy.iplot(fig, filename='word-plots')","6290f3b4":"df_train = train['text']\ndf_test = test['text']","a7fd52f2":"# Text preprocessing with spacy\n\ndef clean_text(text):\n    return text.strip().lower()\n\npunctuations = string.punctuation\nnlp = spacy.load('en')\nstop_words = STOP_WORDS\nparser = English()\n\ndef spacy_preprocessor(sentence):\n    \"\"\"Tokenize, Lemmatize, Remove Stopwords\"\"\"\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n    result = ' '.join(mytokens)\n    return result\n\ndf_train = df_train.apply(lambda x: spacy_preprocessor(x))\ndf_test = df_test.apply(lambda x: spacy_preprocessor(x))","5cc61744":"train['text'] = df_train\ntest['text'] = df_test","6252b09a":"X = train[[col for col in train.columns if not col == 'target']]\ny = train['target']\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=20)\n\ntrain_data = pd.concat([X_train, y_train], axis=1)\nvalid_data = pd.concat([X_valid, y_valid], axis=1)\n\n# save them\n!mkdir preprocessed_data\ntrain_data.to_csv('preprocessed_data\/train.csv', index=False)\nvalid_data.to_csv('preprocessed_data\/valid.csv', index=False)\ntest.to_csv('preprocessed_data\/test.csv', index=False)","d817dd61":"is_cuda = torch.cuda.is_available()\nprint(f\"Cuda Status on the system is {is_cuda}\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","6fb9c8f1":"# choose a fixed length to process the string\nfix_length = 17\ntext = data.Field(tokenize=\"spacy\",\n                  pad_first=True)\n\n# define train, validation and test sets\ntrain_data = data.TabularDataset(path=\"preprocessed_data\/train.csv\",\n                                 format=\"csv\",\n                                 fields=[\n                                         ('id', data.Field()),\n                                         ('keyword', text),\n                                         ('location', data.Field()),\n                                         ('text', text),\n                                         ('target', data.Field())],\n                                 skip_header=True)\n\nvalid_data = data.TabularDataset(path=\"preprocessed_data\/valid.csv\",\n                                 format=\"csv\",\n                                 fields=[\n                                         ('id', data.Field()),\n                                         ('keyword', text),\n                                         ('location', data.Field()),\n                                         ('text', text),\n                                         ('target', data.Field())],\n                                 skip_header=True)\n\ntest_data = data.TabularDataset(path=\"preprocessed_data\/test.csv\",\n                                format=\"csv\",\n                                fields=[\n                                        ('id', data.Field()),\n                                        ('keyword', text),\n                                        ('location', data.Field()),\n                                        ('text', text),\n                                        ],\n                                skip_header=True)\n\ntext.build_vocab(train_data, valid_data)","e5348e3a":"VOCAB_SIZE = len(text.vocab)\nNGRAMS = 2\nBATCH_SIZE = 8\nEMBED_DIM = 32\nNUM_CLASS = 2","05b67757":"class TextSentiment(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_class):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n        self.fc = nn.Linear(embed_dim, num_class)\n        # initialize the weights\n        self.init_weights()\n        \n    def init_weights(self):\n        initrange = 0.5\n        self.embedding.weight.data.uniform_(-initrange, initrange)\n        self.fc.weight.data.uniform_(-initrange, initrange)\n        self.fc.bias.data.zero_()\n        \n    def forward(self, text, offsets):\n        embedded = self.embedding(text, offsets)\n        return self.fc(embedded)","81ee6e44":"model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUM_CLASS).to(device)","24554d3e":"def generate_batch(batch):\n    label = torch.tensor([int(entry.target[0]) for entry in batch])\n    _text = []\n    for entry in batch:\n        _entry = []\n        for t in entry.text:\n            _entry.append(text.vocab.stoi[t])\n        _text.append(torch.tensor(_entry,dtype=torch.long))\n    offsets = [0] + [len(entry) for entry in _text]\n    # torch.Tensor.cumsum returns the cumulative sum\n    # of elements in the dimension dim.\n    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n    _text = torch.cat(_text)\n    return _text, offsets, label","d4055098":"def train_func(sub_train_):\n    # Train the model\n    train_loss = 0\n    train_acc = 0\n    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n    for i, (text, offsets, cls) in enumerate(data):\n        optimizer.zero_grad()\n        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n        output = model(text, offsets)\n        loss = criterion(output, cls)\n        train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        train_acc += (output.argmax(1) == cls).sum().item()\n    # Adjust the learning rate\n    scheduler.step()\n    return train_loss \/ len(sub_train_), train_acc \/ len(sub_train_)\n\ndef test(data_):\n    loss = 0\n    acc = 0\n    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n    for text, offsets, cls in data:\n        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n        with torch.no_grad():\n            output = model(text, offsets)\n            loss = criterion(output, cls)\n            loss += loss.item()\n            acc += (output.argmax(1) == cls).sum().item()\n    return loss \/ len(data_), acc \/ len(data_)","1636198d":"N_EPOCHS = 5\nmin_valid_loss = float('inf')\n\ncriterion = torch.nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=1.0)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n\ntrain_len = int(len(train_data) * 0.95)\nsub_train_, sub_valid_ = random_split(train_data, [train_len, len(train_data) - train_len])\n\nfor epoch in range(N_EPOCHS):\n    start_time = time.time()\n    train_loss, train_acc = train_func(sub_train_)\n    valid_loss, valid_acc = test(sub_valid_)\n    \n    secs = int(time.time() - start_time)\n    mins = secs \/ 60\n    secs = secs % 60\n    \n    print(f'Epoch: {epoch + 1}, | time in {mins} minutes and {secs} seconds')\n    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')","bb6de45c":"def predict(_text, model, vocab, ngrams):\n    if len(_text) == 0:\n        return 0\n    with torch.no_grad():\n        _text = [vocab.stoi[token] for token in ngrams_iterator(_text, ngrams)]\n        output = model(torch.tensor(_text), torch.tensor([0]))\n        return output.argmax(1).item()\n\nmodel = model.to('cpu')\npredictions = [predict(entry.text, model, text.vocab, NGRAMS) for entry in test_data]\ntweet_id = [entry.id[0] for entry in test_data]","3953286f":"output = pd.DataFrame({'id': tweet_id, 'target': predictions})\noutput.to_csv('my_submission.csv', index=False)","6d79914d":"### Cardinality & Target Distribution\n\nLocations are not automatically generated, they are user inputs. This means that they were not automatically sent by the device using services like,*android location feature*, *gps*, *ip addresses* and others but a text field was created and the users typed in their location.\nThat's why location is very dirty and there are too many unique values in it. E.g. users in Nigeria could have put their location as: **Nigeria, 9ja, NGA**, e.t.c. This would cause their locations to be treated as different locations. It shouldn't be used as a feature.\n\nKeywords are important because some words can only be used in one context. Keywords have different counts and target means. keyword can be used as a feature by itself or as a word added to the text. Every single keyword in training set exists in test set. If training and test set are from the same sample, it is also possible to use target encoding on keyword.","fb620620":"**Trigrams**","ca83db14":"**Bigrams**","f83b8ee9":"# Model Building ","bfcde9dc":"we can see from above, the train and test set both have similar percentage of missing in keyword and location features. It would therefore be reasonable to infer that they were taken from the same source","a71d0f01":"# Imports and Data Reading","e28b2882":"# Introduction\n\nSpecial thanks to\n1. https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-full-cleaning\n2. https:\/\/www.kaggle.com\/cristophersfr\/twitter-disaster-problem-with-pytorch\n3. https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc\n\nAnd also, the official pytorch tutorial https:\/\/pytorch.org\/tutorials\/beginner\/text_sentiment_ngrams_tutorial.html\n\nfor such wonderful kernels \/ documentation. I really learnt a lot from them. This analysis contains sample code from them, please check them out, and upvote (where applicable)\n\n\n**NB: DO NOT BOTHER ABOUT THOSE WHO MADE PERFECT SCORES. THE LABELS ARE LEAKED ONLINE. JUST FOCUS ON LEARNING AND SHARING. There would always be other NLP competitions where you can use the skills you learn here**","c103180b":"# Exploratory Data Analysis\n\n1. Spot missing values, and make creative inferences from them.","aa556e0e":"**OBSERVATIONS**\n\n1. Most common words that appear in both categories are very different.\n2. Insincere questions tend to contain words that show violence or war.","00e8780e":"### NGRAMS distribution","fe0d003b":"### Missing Values","ad9356b2":"**Unigrams**"}}