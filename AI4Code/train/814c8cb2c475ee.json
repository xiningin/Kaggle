{"cell_type":{"2ccd8842":"code","a8ba4aa0":"code","0205311d":"code","4ce2c987":"code","a66ba920":"code","59193f8e":"code","5af51484":"code","cf3465b8":"code","46201d62":"code","3a2aaff6":"code","7075c732":"code","e82c265b":"code","c2b34a10":"code","f0dd16d8":"code","8d381a72":"code","1ed0df05":"code","336c9e5e":"code","cbd940ca":"code","f28fa9da":"code","3ab076ff":"code","aa705b63":"code","a8859f89":"code","de972e48":"code","3c79f472":"code","2e015af7":"code","b0da7d55":"code","2d6d12ed":"code","383daaed":"code","e6f780ab":"code","cc752ab7":"code","e6f8e5cc":"code","ea480eee":"code","aa5233aa":"code","e3fdadc6":"code","e20e058b":"code","3aa0c765":"code","3dc4dc8b":"code","e54dd5e5":"code","d23763c7":"code","6f795633":"code","0d13efb9":"code","dcb5a81d":"code","00518c48":"code","d31d0409":"code","c359e35b":"code","d2497bb3":"code","6dea3e52":"code","d04e381c":"code","59e458d3":"code","edde87c3":"code","0e080b82":"code","094200bf":"code","63050a22":"code","5626e589":"code","570a049b":"code","d832837f":"code","71211b94":"code","4991bd71":"code","82d518e3":"code","0570f58c":"code","c7b5d853":"code","64c9c096":"code","df8e40c9":"markdown","f3e26170":"markdown","6b322863":"markdown","15756047":"markdown","3b3a5d9e":"markdown","1661fc86":"markdown","4ea581af":"markdown","02731493":"markdown","936def8e":"markdown","16bc6aa7":"markdown","0ec1ea62":"markdown","3412ff82":"markdown","d3a6d835":"markdown","1081c1ce":"markdown","f9f16855":"markdown","5e68bf2e":"markdown","87c053ef":"markdown","254bbf39":"markdown","6f21f510":"markdown","f52b470e":"markdown","7e1a2de8":"markdown","28404e2e":"markdown","69cb7e9b":"markdown","06b2ee79":"markdown","0a5aedad":"markdown","e7b7d3d6":"markdown","d871d124":"markdown","448bf4f8":"markdown","d22d93c4":"markdown","9736608e":"markdown"},"source":{"2ccd8842":"import numpy as np\nfrom numpy import random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\n\npd.set_option('display.max_columns', 200)\n\n","a8ba4aa0":"creditDataFrame = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\", encoding = 'unicode_escape')\nprint(creditDataFrame.shape)\nprint(creditDataFrame.size)","0205311d":"creditDataFrame.info()","4ce2c987":"creditDataFrame.head()","a66ba920":"creditDataFrame.drop(columns= ['Time'], inplace=True)","59193f8e":"stdScaler = StandardScaler()\ncreditDataFrame[['Amount']] = stdScaler.fit_transform(creditDataFrame[['Amount']])\ncreditDataFrame.head()","5af51484":"print(creditDataFrame['Class'].value_counts()\/len(creditDataFrame)*100)\nprint(creditDataFrame['Class'].value_counts())\n\nsns.countplot(creditDataFrame['Class'])","cf3465b8":"def getRandomNonFraudData(df):\n    random.seed(100)\n    nonFraudDF = df[df['Class'] == 0]\n    numbers = random.choice(len(nonFraudDF), size=492, replace=False)\n    return nonFraudDF.iloc[numbers]","46201d62":"nonFraudDF = getRandomNonFraudData(creditDataFrame)\nfraudDF = creditDataFrame[creditDataFrame['Class'] == 1]\nmergedDF = pd.concat([nonFraudDF, fraudDF])\nmergedDF.head()","3a2aaff6":"mergedDF.shape","7075c732":"plt.figure(figsize  = (15,30))\nfor i in enumerate(mergedDF.columns.drop('Class')):\n    plt.subplot(10, 3, i[0]+1)\n    sns.distplot(mergedDF[i[1]])","e82c265b":"plt.figure(figsize  = (15,30))\nfor i in enumerate(mergedDF.columns.drop('Class')):\n    plt.subplot(10, 3, i[0]+1)\n    sns.boxplot(data=mergedDF, x=i[1])","c2b34a10":"def treatOutliers(col, df):\n    q4 = df[col].quantile(0.95)\n    df[col][df[col] >=  q4] = q4\n    \n    q1 = df[col].quantile(0.05)\n    df[col][df[col] <=  q1] = q1\n    \n    return df","f0dd16d8":"columns = mergedDF.columns.drop('Class')\nfor i in enumerate(columns):\n    mergedDF = treatOutliers(i[1], mergedDF)","8d381a72":"## We can see outliers are treated well and data is much better condition now.\n\nplt.figure(figsize  = (15,30))\nfor i in enumerate(mergedDF.columns.drop('Class')):\n    plt.subplot(10, 3, i[0]+1)\n    sns.boxplot(data=mergedDF, x=i[1])","1ed0df05":"corr = mergedDF.corr()\ncorr = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\ncorr_df = corr.unstack().reset_index()\ncorr_df.columns = ['Variable1', 'Variable2', 'Correlation']\ncorr_df.dropna(subset = ['Correlation'], inplace=True)\ncorr_df['Correlation'] = round(corr_df['Correlation'].abs(), 2)\ncorr_df.sort_values(by = 'Correlation', ascending=False).head(10)","336c9e5e":"plt.figure(figsize  = (25,15))\nsns.heatmap(mergedDF.corr(), annot=True, cmap='RdYlGn')","cbd940ca":"train_set, test_set = train_test_split(mergedDF, \n                                       train_size=0.7, \n                                       stratify=mergedDF.Class, \n                                       shuffle = True, \n                                       random_state=100)","f28fa9da":"print(train_set.shape)\nprint(test_set.shape)","3ab076ff":"# Futher divide the dataset in X_train and y_train\ny_train = train_set.pop('Class')\nX_train = train_set","aa705b63":"logreg = LogisticRegression()\nrfe = RFE(logreg, n_features_to_select=15 )\nrfe = rfe.fit(X_train, y_train)","a8859f89":"#useful columns according to rfe\nuseful_cols = X_train.columns[rfe.support_]\nuseful_cols","de972e48":"# Not useful columns according to rfe\nX_train.columns[~rfe.support_]","3c79f472":"X_train_rfe = X_train[useful_cols]\nX_train_rfe.head()","2e015af7":"def checkVIF():\n    vif = pd.DataFrame()\n    vif['Feaures'] = X_train_sm.columns\n    vif['VIF'] = [VIF(X_train_sm.values, i) for i in range(X_train_sm.shape[1])]\n    vif = vif.sort_values(by = 'VIF', ascending=False)\n    return vif","b0da7d55":"# Logistic Regression Model\nX_train_sm = sm.add_constant(X_train_rfe)\nlm = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nlm.fit().summary()","2d6d12ed":"checkVIF()","383daaed":"X_train_sm = X_train_sm.drop(columns=['V27'])\nlm = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nlm.fit().summary()","e6f780ab":"checkVIF()","cc752ab7":"X_train_sm = X_train_sm.drop(columns=['V12'])\nlm = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nlm.fit().summary()","e6f8e5cc":"checkVIF()","ea480eee":"X_train_sm = X_train_sm.drop(columns=['V24'])\nlm = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nlm.fit().summary()","aa5233aa":"checkVIF()","e3fdadc6":"X_train_sm = X_train_sm.drop(columns=['V22'])\nlm = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nlm = lm.fit()\nlm.summary()","e20e058b":"checkVIF()","3aa0c765":"X_train_sm = X_train_sm.drop(columns=['V15'])\nlm = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nlm = lm.fit()\nlm.summary()","3dc4dc8b":"checkVIF()","e54dd5e5":"X_train_sm = X_train_sm.drop(columns=['V14'])\nlm = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nlm = lm.fit()\nlm.summary()","d23763c7":"checkVIF()","6f795633":"X_train_sm = X_train_sm.drop(columns=['V10'])\nlm = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nlm = lm.fit()\nlm.summary()","0d13efb9":"checkVIF()","dcb5a81d":"y_train_pred = lm.predict(X_train_sm)\ny_train_pred = y_train_pred.values.reshape(-1)","00518c48":"y_train_pred_final = pd.DataFrame({'Class': y_train.values, 'Class_Prob': y_train_pred})\ny_train_pred_final['id'] = y_train.index\ny_train_pred_final.head()","d31d0409":"y_train_pred_final['Class_Predicted'] = y_train_pred_final.Class_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","c359e35b":"def drawRoc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()","d2497bb3":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Class, y_train_pred_final.Class_Prob, \n                                         drop_intermediate = False )","6dea3e52":"drawRoc(y_train_pred_final.Class, y_train_pred_final.Class_Prob)","d04e381c":"numbers = [float(x)\/10 for x in range(10)]\n\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Class_Prob.map(lambda x: 1 if x > i else 0)\n\ny_train_pred_final.head()","59e458d3":"cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Class, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n    \nprint(cutoff_df)","edde87c3":"cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","0e080b82":"y_train_pred_final['final_predicted'] = y_train_pred_final.Class_Prob.map(lambda x: 1 if x > 0.37 else 0)\ny_train_pred_final.head()","094200bf":"y_train_pred_final['final_predicted'].value_counts()","63050a22":"cm = metrics.confusion_matrix(y_train_pred_final.Class, y_train_pred_final.final_predicted)\ncm","5626e589":"accuracy = metrics.accuracy_score(y_train_pred_final.Class, y_train_pred_final.final_predicted)*100\n\ntn, fp, fn, tp = metrics.confusion_matrix(y_train_pred_final.Class, y_train_pred_final.final_predicted).ravel()\nspecificity = tn \/ (tn + fp)*100\n\nrecall = metrics.recall_score(y_train_pred_final.Class, y_train_pred_final.final_predicted)*100\n\nprecision = metrics.precision_score(y_train_pred_final.Class, y_train_pred_final.final_predicted)*100\n\nf1_score = metrics.f1_score(y_train_pred_final.Class, y_train_pred_final.final_predicted)*100\n\nprint(\"Accuracy: {0} %\".format(round(accuracy, 2)))\nprint(\"Specificity: {0} %\".format(round(specificity, 2)))\nprint(\"Recall: {0} %\".format(round(recall, 2)))\nprint(\"Precision: {0} %\".format(round(precision, 2)))\nprint(\"F1-Score: {0} %\".format(round(f1_score, 2)))","570a049b":"print(metrics.classification_report(y_train_pred_final.Class, y_train_pred_final.final_predicted))","d832837f":"y_test = test_set.pop('Class')\nX_test = test_set","71211b94":"cols = X_train_sm.columns\nX_test_final = X_test[cols.drop('const')]\n\nX_test_final = sm.add_constant(X_test_final)\nX_test_final.head()","4991bd71":"y_test_pred = lm.predict(X_test_final)\ny_test_pred = y_test_pred.values.reshape(-1)","82d518e3":"y_test_pred_final = pd.DataFrame({'Class': y_test.values, 'Class_Prob': y_test_pred})\ny_test_pred_final['id'] = y_test.index\ny_test_pred_final.head()","0570f58c":"y_test_pred_final['Class_Predicted'] = y_test_pred_final.Class_Prob.map(lambda x: 1 if x > 0.38 else 0)\ny_test_pred_final.head()","c7b5d853":"metrics.confusion_matrix(y_test_pred_final.Class, y_test_pred_final.Class_Predicted)","64c9c096":"accuracy = metrics.accuracy_score(y_test_pred_final.Class, y_test_pred_final.Class_Predicted)*100\n\ntn, fp, fn, tp = metrics.confusion_matrix(y_test_pred_final.Class, y_test_pred_final.Class_Predicted).ravel()\nspecificity = tn \/ (tn + fp)*100\n\nrecall = metrics.recall_score(y_test_pred_final.Class, y_test_pred_final.Class_Predicted)*100\n\nprecision = metrics.precision_score(y_test_pred_final.Class, y_test_pred_final.Class_Predicted)*100\n\nf1_score = metrics.f1_score(y_test_pred_final.Class, y_test_pred_final.Class_Predicted)*100\n\nprint(\"Accuracy: {0} %\".format(round(accuracy, 2)))\nprint(\"Specificity: {0} %\".format(round(specificity, 2)))\nprint(\"Recall: {0} %\".format(round(recall, 2)))\nprint(\"Precision: {0} %\".format(round(precision, 2)))\nprint(\"F1-Score: {0} %\".format(round(f1_score, 2)))","df8e40c9":"## Model Creation","f3e26170":"## Data Cleaning and Preparation\n### Remove Time Column\n- Time column is not useful for model traning","6b322863":"### Train the Model","15756047":"# Final Conclusion:\n\n- This Logistic regression model has high level of Accuracy and highly Sensitive to Fraud transaction.\n- This model has high level of Sensitivity and it is predicting on the test data very accrately\n- All the metrics values are equivalent (with in 5% of range) for both train and test data which shows that this model is very efficient in detecting the fradulent credit card transaction.\n\n### On Train Data:\n\n| Metrics | Score |\n| :- | :- |\n| Accuracy | 92.59 % |\n| Specificity | 93.31 % |\n| Recall \/ Sensitivity | 91.86 % |\n| Precision | 93.22 % |\n| F1-Score | 92.53 % |\n\n\n### On Test Data:\n| Metrics | Score |\n| :- | :- |\n| Accuracy | 90.2 % |\n| Specificity | 89.86 % |\n| Recall \/ Sensitivity | 90.54 % |\n| Precision | 89.93 % |\n| F1-Score | 90.24 % |\n","3b3a5d9e":"### Choosing the Random Cut-Off probability 0.5","1661fc86":"## Conclusion\n\n#### On the train data\n- All the Metrics are giving the high score\n- Model's accuracy is High\n- Model's Recall\/Sensitivity is also good around 94.71%\n- This model seems to be working excellently on the traning Data","4ea581af":"### Top 10 highly corelated variables\n- Below are Top 10 highly co-related values\n- Variables are highly co-related with each other and redundant\n- We will have to choose right and independent variables while feature selection.","02731493":"### Manual Feature Elimination","936def8e":"#### Remove the column with High P-Value\n- These columns needs to be removed because these are very in-significant variables in the data","16bc6aa7":"## Prediction on the Train data","0ec1ea62":"## Get Confusion Matrix","3412ff82":"## Model Evaluation on the Test Data","d3a6d835":"### Finding Optimal Cutoff Point","1081c1ce":"### Feature selection using RFE","f9f16855":"## Model Evaluation\n#### Only Accuracy can't tell the performance of the model, we need other Matrics like Precision and Recall to understand actual performance of the model.\n","5e68bf2e":"#### Optimal cut off probability is that prob where we get balanced sensitivity and specificity","87c053ef":"### Select only useful columns","254bbf39":"### From the curve above, it seems 0.37 (slightly less than 0.4) is the optimal point to take it as a cutoff probability.","6f21f510":"## Most of the columns except Amount column are already scaled by StandardScaler method\n#### However we still have to scale the Amount column","f52b470e":"### Treating the outliers\n- There are two ways to treat the outliers\n  - Delete those rows\n  - Impute the values based on inter quartile range (IQR)\n- I will use *soft outlier treatment* to avoid any data loss.\n- [Q1, Q3] which is lower quantile threshold 5% and Higher quantile threshold 95%","7e1a2de8":"### **Check Data Imbalance for target variable 'class' (fraud or non fraud)**\n- This data is quite imbalanced\n- Around 99.8 percent transactions are genuine transactions\n- Around 0.17 (492) percent are fraudlent transactions","28404e2e":"## Treating the imbalance of data\n#### I will be using Random Stratified sampling to treat the imbalance of data.\n#### For this, I will be using random indices to get the non fraud data and will concat for fraud data so that we have 50-50% of distribution of the data for both classes","69cb7e9b":"### Divide Data in Train and Test set\n- We should have the statified split based on target variable so that we have proper distribution of data","06b2ee79":"#### Calculate VIF values to check co-relation between independent variables","0a5aedad":"## Exploratory Data Analysis\n- All the columns are normally distributed around mean=0 which is the result of standard scaling.\n- if Standard scaling would not have already done then dist plot could have given real picture of the data. (Anyways)\n- 'Amount' column distribution seems to be quite skewed and gives the indication of outliers present in the data\n- Some of the other columns are also quite skewed which can be clearly seen from the distrbution of the data.","e7b7d3d6":"### Features look good now, Low VIF and Low P-Value for all the variables.","d871d124":"# Credit Card Fraud Detection Model\n- Due to privacy reasons, data doesn't have actual columns names except transaction time and amount.\n- But still we can build a Machine learning model on the data provided.\n- I will build a Logistic Regression model to detect fraudlent transactions.\n- Special thanks to Kaggle.com to provide such useful platform to Data Engineers and Scientists.\n\n### This model is based on Supervised learning because labels are provided for fraud and non fraud transactions\n\n### Steps taken to build the Model\n\n![image.png](attachment:image.png)","448bf4f8":"### Remove the columns with very High VIF\n- these variables need to be removed to fix Multi colinearity in the data","d22d93c4":"## Finding Outliers\n- We can clearly see the outliers most of the columns.\n- We will have to treat the outlier because they may influence and dominate while building the models\n- Those outlier may not present in the test set which can impact the overall accuracy of the model.","9736608e":"### Scaling the amount column\n- We can use either Standard Scaler or MinMaxScaler\n- Since StandardScaler is used for scaling the other columns so I will use the same for Amount"}}