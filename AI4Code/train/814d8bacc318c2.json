{"cell_type":{"a45723ac":"code","d67ca3ca":"code","705baf00":"code","abcdb866":"code","c3bd89c3":"code","c3d40651":"code","1caa0bf0":"code","2eaf40af":"code","76286e00":"code","90a46bbc":"code","da7c3f6b":"code","a00c093a":"markdown","c959952d":"markdown","5687bfdf":"markdown","d773d4a0":"markdown","4f093a8d":"markdown","36c753fe":"markdown","ca820295":"markdown","7f6e54b9":"markdown","b6b181a0":"markdown","4a5bab0a":"markdown","f3ee47d1":"markdown"},"source":{"a45723ac":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2 #deal with images\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm\nnp.random.seed(42)\n\ntraining = pd.read_csv('..\/input\/training_set.csv')\nmeta_training = pd.read_csv(\"..\/input\/training_set_metadata.csv\")\nmerged = training.merge(meta_training, on = \"object_id\")","d67ca3ca":"###recurrent plot\n\ndef sigmoid(x):\n    '''\n    Returns the sigmoid of a value\n    '''\n    return 1\/(1+np.exp(-x))\n\ndef R_matrix(signal, eps):\n    '''\n    Given a time series (signal) and an epsilon,\n    return the Recurrent Plot matrix\n    '''\n    R = np.zeros((signal.shape[0], signal.shape[0]))\n    for i in range(R.shape[0]):\n        for j in range(R.shape[1]):\n            R[i][j] = np.heaviside((eps - abs(signal[i] - signal[j])),1)\n    return R\n\n#using sigmoid rather than heaviside\n#because in this dataset the epsilon parameter needs to\n#change from object to object and therefore should be learned as well\ndef R_matrix_modified(signal):\n    '''\n    Given a time series (signal) and an epsilon,\n    return the modified Recurrent Plot matrix\n    using sigmoid rather than heaviside\n    '''\n    R = np.zeros((signal.shape[0], signal.shape[0]))\n    for i in range(R.shape[0]):\n        for j in range(R.shape[1]):\n            R[i][j] = sigmoid((abs(signal[i] - signal[j])))\n    return R\n\ndef create_objects_dict(merged_dataset):\n    '''\n    Input: dataset containing both training data and metadata\n    Creates a dictionary using each object as keys and\n    one R matrix for each passband in that object\n    '''\n    objects = {}\n    for obj in tqdm(np.unique(merged_dataset.object_id)):\n        R_passbands = []\n        for passband in np.unique(merged_dataset.passband):\n            obj_flux = merged_dataset[(merged_dataset.object_id == obj) & (merged_dataset.passband == passband)].flux.values\n            R_passbands.append(R_matrix_modified(obj_flux))\n        objects[obj] = (np.asarray(R_passbands), max(merged_dataset[merged_dataset.object_id == obj].target))\n    return objects\n\ndef get_minmax_shapes(obj_R_matrices):\n    '''\n    Given an R matrix, get the min and max width \n    to be used to crop and let all images from a given\n    object be of the same size so they can be concatenated\n    '''\n    min_length = 0\n    max_length = 0\n    for passband in np.unique(merged.passband):\n        if passband == 0:\n            length = len(obj_R_matrices[passband])\n            min_length = length\n            max_length = length\n        else:\n            length = len(obj_R_matrices[passband])\n            min_length = min(min_length, length)\n            max_length = max(max_length, length)\n    return (min_length, max_length)\n\ndef crop_obj_plots(objects):\n    '''\n    Accepts a dictionary where each key is a different object\n    and each value is a tuple - one slot with a list of R matrices and \n    the other with the target value (object class)\n    '''\n    for obj in tqdm(objects.keys()):\n        min_len, max_len = get_minmax_shapes(objects[obj][0])\n        for passband in np.unique(merged.passband):\n            objects[obj][0][passband] = objects[obj][0][passband][:min_len, :min_len]\n    return objects\n\nobjects = create_objects_dict(merged)\ncropped_objects = crop_obj_plots(objects)","705baf00":"cropped_objects[730][0][3].shape\nplt.imshow(cropped_objects[730][0][0])","abcdb866":"from collections import Counter\n\nshapes = []\nfor key in tqdm(cropped_objects.keys()):\n    shapes.append(cropped_objects[key][0][0].shape[0])\nplt.hist(shapes, bins = 50)\nCounter(shapes)","c3bd89c3":"import math\ncropped_2 = np.copy(cropped_objects).item()\nfor key in tqdm(cropped_2.keys()):\n    shape = cropped_2[key][0][0].shape[0]\n    if shape < 11:\n        for passband in np.unique(merged.passband):\n            #how much we will increase the border\n            increaseBorder = abs(shape-11)\/2\n            cropped_2[key][0][passband] = cv2.copyMakeBorder(src = cropped_2[key][0][passband],\n                                                             top = math.ceil(increaseBorder), \n                                                             left = math.ceil(increaseBorder),\n                                                             bottom = round(increaseBorder),\n                                                             right = round(increaseBorder),\n                                                             borderType = cv2.BORDER_REFLECT)\n    elif shape>11 and shape < 25:\n        for passband in np.unique(merged.passband):\n            cropped_2[key][0][passband] = cropped_2[key][0][passband][:-(shape-11), :-(shape-11)]\n            \n    \n    elif shape >= 50 and shape < 57:\n        for passband in np.unique(merged.passband):\n            increaseBorder57 = abs(shape-57)\/2\n            cropped_2[key][0][passband] = cv2.copyMakeBorder(src = cropped_2[key][0][passband],\n                                                             top = math.ceil(increaseBorder57), \n                                                             left = math.ceil(increaseBorder57),\n                                                             bottom = round(increaseBorder57),\n                                                             right = round(increaseBorder57),\n                                                             borderType = cv2.BORDER_REFLECT)\n    else:\n        continue","c3d40651":"shapes = []\nfor key in tqdm(cropped_2.keys()):\n    shapes.append(cropped_2[key][0][0].shape[0])\nplt.hist(shapes, bins = 50)\nCounter(shapes)","1caa0bf0":"objects = list(cropped_2.keys())\ninput_images = list()\nlabels = list()\nfor key in tqdm(objects):\n    if cropped_2[key][0][0].shape[0] == 57:\n        img = np.stack((cropped_2[key][0][0],\n                        cropped_2[key][0][1],\n                        cropped_2[key][0][2],\n                        cropped_2[key][0][3],\n                        cropped_2[key][0][4]), axis = -1)  \n                                           \n        input_images.append(np.expand_dims(img, axis = 0))\n        labels.append(cropped_2[key][1])                                                        \ninput_images = np.vstack(input_images)     \ninput_images.shape","2eaf40af":"#LabelBinarizer and train-test split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras.utils import np_utils\n\n\ntrain_fraction = 0.8\n\nencoder = LabelBinarizer()\ny = encoder.fit_transform(labels)\nx = input_images\n\ntrain_tensors, test_tensors, train_targets, test_targets =\\\n    train_test_split(x, y, train_size = train_fraction, random_state = 42)\n\nval_size = int(0.5*len(test_tensors))\n\nval_tensors = test_tensors[:val_size]\nval_targets = test_targets[:val_size]\ntest_tensors = test_tensors[val_size:]\ntest_targets = test_targets[val_size:]\n","76286e00":"from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D\nfrom keras.layers import Dropout, Flatten, Dense, LeakyReLU\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.models import Sequential\nfrom tensorflow import set_random_seed\n\nset_random_seed(42)\n\nearly_stopping = EarlyStopping(monitor = 'val_loss', patience = 10)\ncheckpointer = ModelCheckpoint(filepath='weights.hdf5', \n                               verbose=1, save_best_only=True)\nmodel = Sequential()\nmodel.add(Conv2D(filters = 16, kernel_size = 4, padding = 'same', activation = 'relu', input_shape = (None, None,5)))\nmodel.add(Conv2D(filters = 16, kernel_size = 4, padding = 'same', activation = 'relu'))\nmodel.add(Conv2D(filters = 16, kernel_size = 4, padding = 'same', activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(MaxPooling2D(pool_size = 2)) \n\nmodel.add(Conv2D(filters = 32, kernel_size = 4, padding = 'same', activation = 'relu')) \nmodel.add(Conv2D(filters = 32, kernel_size = 4, padding = 'same', activation = 'relu')) \nmodel.add(Conv2D(filters = 32, kernel_size = 4, padding = 'same', activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(MaxPooling2D(pool_size = 2)) \n\nmodel.add(Conv2D(filters = 64, kernel_size = 4, padding = 'same', activation = 'relu')) \nmodel.add(Conv2D(filters = 64, kernel_size = 4, padding = 'same', activation = 'relu')) \nmodel.add(Conv2D(filters = 64, kernel_size = 4, padding = 'same', activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(GlobalMaxPooling2D()) \n\nmodel.add(Dense(256, activation = 'linear'))\nmodel.add(Dense(128, activation = 'linear'))\nmodel.add(Dense(14, activation = 'softmax'))\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nepochs = 100\nmodel.fit(train_tensors, train_targets, \n          validation_data=(val_tensors, val_targets),\n          epochs=epochs, batch_size=80, verbose=1, callbacks = [early_stopping, checkpointer])","90a46bbc":"model.load_weights('weights.hdf5')\n\ncell_predictions =  [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n\ntest_accuracy = 100*np.sum(np.array(cell_predictions)==np.argmax(test_targets, axis=1))\/len(cell_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)","da7c3f6b":"#manipulations to get our images into the proper dimensions (done before with shape 57)\nobjects = list(cropped_2.keys())\nsmall_labels = list()\nsmall_input_images = list()\nfor key in tqdm(objects):\n    if cropped_2[key][0][0].shape[0] == 11:\n        img = np.stack((cropped_2[key][0][0],\n                        cropped_2[key][0][1],\n                        cropped_2[key][0][2],\n                        cropped_2[key][0][3],\n                        cropped_2[key][0][4]), axis = -1)  \n                                           \n        small_input_images.append(np.expand_dims(img, axis = 0))\n        small_labels.append(cropped_2[key][1])                                                        \nsmall_input_images = np.vstack(small_input_images)     \nsmall_input_images.shape\n\n#predictions\ncell_predictions =  [np.argmax(model.predict(np.expand_dims(small_image, axis=0))) for small_image in small_input_images]\n\nsmall_encoder = LabelBinarizer()\nsmall_labels_encoded = encoder.fit_transform(small_labels)\ntest_accuracy = 100*np.sum(np.array(cell_predictions)==np.argmax(small_labels_encoded, axis=1))\/len(cell_predictions)\nprint('Accuracy in small images: %.4f%%' % test_accuracy)","a00c093a":"Now we found the second problem and the key source of information loss: we have lots of different shapes, most around 10-12. The most complete images (longer observations) are just a small portion of the dataset. In order to properly work with the dataset, I will try to get most of them into the same shape. From my experience, the very small images are not very good predictors, so I'll just use the larger ones. Now I'll put all images with shapes (50,50) or larger  into shape (57,57). ","c959952d":"The model's accuracy for a much smaller test set is awfully low, around 13%-15%. There are at least two possible ways out of this problem (I'm not sure if any would actually work. Just some hypothesis here):\n- Train the model using just the smaller images and try to generalize to the larger ones\n- Train the model using both large and small images\n\nI'll leave this open for more experienced data scientists to discuss if they want to :) Meanwhile I'll keep working on how to solve this generalization issue and update here if I have an Eureka moment.","5687bfdf":"Now we can stack all the images with the same shape to form our input.","d773d4a0":"We then train the model. The input_shape needs to be (None, None, 5) rather than (57, 57, 5) - that we know will be our training input shapes - because we want to test the model in images of different sizes as well (remember the huge amount of images of size 11x11?). If we don't set input_shape's width and height to None, our convolutional layers will throw an error. ","4f093a8d":"Now let's check how the shapes look like.","36c753fe":"Note that I'm not considering any weights for any particular class here. The loss function that gets the best results so far is categorical crossentropy. By doing so, I'm getting an accuracy of around 60% in the classification of the larger images (objects that have observations for a longer period of time). Making the predictions on the images of shape (11x11), we got:","ca820295":"Now each object has a 6-channel image. They all look like the image below:","7f6e54b9":"Hey everyone! I've seen that people have started using NNs for a while now, but I didn't see people using CNNs. Indeed, it is pretty unusual, at least to me, to use a kind of network that performs so well on image data to treat time series. However, I'm completely in love with this kind of NN and I started looking for ways to use it here and I found [this paper](https:\/\/arxiv.org\/pdf\/1710.00886.pdf). There are several issues that we have to deal with along the preprocessing and evaluation steps but I believe that this could be a good starting point for those who want to try using CNNs. I prepared this notebook as an introduction in the topic and necessary preprocessing in case someone wants to investigate it deeper. IMHO, the greatest advantage of this method - and other DL techniques - is that we don't need worry about feature extraction.\n\nFirst of all, we'll need to load the data. ","b6b181a0":"We have lots of images with the same shape and therefore we can use them in our CNN. Here I'll just use images with shape 57x57x5.  \n\nThe following cell is to split between train, validation and test samples and also binarize the labels. ","4a5bab0a":"The basic idea described in the paper is that we can transform a 1-dimensional time series into a 2-dimensional image by calculating its recurrences, that is, revealing in which points the series returns to a previous value.  This is done by using the equation:\n\n$$ R_{i,j} = \\theta(\\epsilon - || s_i - s_j ||)$$\n\nwhere a $s$ are the states (time series values), ||.|| is the norm  and $\\epsilon$ is a threshold distance and $\\theta$ is the Heaviside (step) function. \n\nBelow are the functions I used to create a dictionary of such R matrices, where each key is a different object. You'll notice that I preferred to use a slightly different approach by using a sigmoid rather than Heaviside step function. I did so because since we are dealing with several different objects, it turns out that each one has a different \"optimal\" $\\epsilon$ in order to create a proper image. So for simplicity I opted to use sigmoid because it doesn't need tunning for each image.\n\nAfter each R matrix for each object and each passband is made, there is another issue that we need to worry about.  Since each object has 6 passbands, we need a way to put all of those images - one for each passband -  into a single image. My solution was to stack them up , in some kind of resemblance to a RGB image that has 3 channels. Here we will have 6 channels instead. The problem is: it is not guaranteed that each passband has the same number of observations, resulting in images of different sizes. If we want to stack them, they need to have the same shape. To solve that, I cropped all of them to the minimum size possible for that object - and here is where we begin to lose information. For instance, if an object has R matrices of shapes (30x30), (50x50) and (48x48), I'd shape them all to (30x30) so they can be stacked. This is done by `crop_obj_plots` function.","f3ee47d1":"Now let's the distribution of shapes across all images."}}