{"cell_type":{"004fbf04":"code","e98a6eae":"code","cfb3f3c1":"code","08c2b328":"code","4ff4188d":"code","bd88942f":"code","7d730061":"code","23f9bfa1":"code","69f9f78d":"code","addcdbe4":"code","f9681b90":"code","f4a5d839":"code","04353618":"code","83aa5027":"code","b8ead54e":"code","8ca452f6":"code","bccf1547":"code","fdaaf89a":"code","15124bba":"code","d3921ab3":"code","a19c780b":"code","d5bbf1f2":"code","6a67baa3":"code","495ecd17":"code","dc624a7f":"code","b55b39af":"code","c99845ef":"code","93474729":"code","85a51d18":"code","6c7d5f4f":"code","bac306fd":"code","79d46e4d":"code","d5388442":"code","db46c1fb":"code","95cf8797":"code","679937d1":"code","88c97241":"code","c0c16bfa":"code","d458e9c7":"code","05cce21e":"code","a089fc9a":"code","eb423916":"code","3937018d":"code","8e440902":"code","5a5b469b":"code","0f46b054":"code","f951c57b":"code","649ff73f":"code","c748ec24":"code","a0a61b24":"code","24979b52":"code","cfc6c2f5":"code","d3d9fe59":"code","92623e29":"code","865b41ef":"code","b7887f03":"code","c35fe29a":"code","9d35792f":"code","3dd2c061":"code","d231fde4":"code","6fd5ff92":"code","eea9f66b":"code","082de47c":"code","0f28d04e":"code","e000e4cf":"code","3a47db83":"code","f6174a5e":"code","b1e36973":"code","bfb5c8e0":"code","82550a19":"code","1f7d3b4f":"code","e5b89de1":"code","1157fb66":"code","d6d8e01c":"code","0789b20e":"code","7699780d":"code","fad18248":"code","db008cde":"code","05aafab0":"code","05a3d35f":"markdown","2a408b35":"markdown","daab9a0a":"markdown","d2b3257d":"markdown","13f6493e":"markdown","cea04239":"markdown","90387a19":"markdown","40e8560c":"markdown","9733860f":"markdown","ff8f8bf8":"markdown","9adee576":"markdown","036e7c3f":"markdown","1394e011":"markdown","2becf1bb":"markdown","2850fb2d":"markdown","4f27f33e":"markdown","dd82b303":"markdown","70a79807":"markdown","f28e6c09":"markdown","6d38a741":"markdown","f98e2d43":"markdown","1c03079c":"markdown","5f6d3257":"markdown","f0a3b89a":"markdown","2b65c97f":"markdown","68e88e7c":"markdown","cb153b45":"markdown","133b2dc2":"markdown","5a25febb":"markdown","d4cfb89a":"markdown","8314d52f":"markdown","e585cfcd":"markdown","768e0997":"markdown","4e26c141":"markdown","65a6728a":"markdown"},"source":{"004fbf04":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as st\nimport scipy.stats as stat\nfrom scipy.stats import chi2\n","e98a6eae":"Origin = pd.read_csv(\"\/kaggle\/input\/food-fact\/foodfact.csv\",sep='\\t',encoding='utf-8', low_memory=False)","cfb3f3c1":"# The dataset is quit big, so let's check that we have uploaded all rows and columns. \n\nnum_lines = sum(1 for line in open(\"\/kaggle\/input\/food-fact\/foodfact.csv\", encoding='utf-8'))\nmessage = (f\"Nombre de lignes dans le fichier (en comptant l'ent\u00eate): {num_lines}\\n\"\nf\"Nombre d'instances dans le dataframe: {Origin.shape[0]}\")\nprint(message)","08c2b328":"print('The initial dataset includes {:,} rows '.format(Origin.shape[0]) + 'and {} columns'.format(Origin.shape[1]))","4ff4188d":"#Let's have an overview of the dataset, with random rows\nOrigin.sample(5)","bd88942f":"#Let's get info about the dataset\nOrigin.info()","7d730061":"#let's list the different types of values in the dataset\nOrigin.dtypes","23f9bfa1":"#Do we have Nans in the dataset ? \n#True = yes\n#False = no \nprint(Origin.isnull().values.any())","69f9f78d":"#Let's learn more about the dataset and it's statistiques\nOrigin.describe()","addcdbe4":"#note vt : int\u00e9grer la fonction qui split les columns quanti et quali","f9681b90":"#Let's list the columns of the dataset\nOrigin.columns.tolist()","f4a5d839":"#note vt : int\u00e9grer la facon de classer comme dans P5\/6 ou7","04353618":"#display potential exactly similar lines\nOrigin.duplicated().sum()","83aa5027":"#Among the 1648410 lines in the dataset, a duplicate has crept in. \n#Let\u2019s check the content.\nOrigin[Origin.duplicated(keep=False)]","b8ead54e":"#We have confirmed that this line is an exact dupplicata,\n#The barcode is exactly the same.\n#Or a barcode represents a single product and therefore cannot\n#be assigned to 2 different products. We can delete one of these lines.\nOrigin2 = Origin.drop_duplicates(inplace=False) #False pour permettre le nouveau nom du dataset","8ca452f6":"print(f\"Shape of the dataset before cleaning {Origin.shape}\")\nprint(f\"Shape of the dataset after cleaning {Origin2.shape}\")\nprint(f\"We have deleted {Origin.shape[1]- Origin2.shape[1]} columns\")\nprint(f\"We have deleted {Origin.shape[0]- Origin2.shape[0]} rows\")","bccf1547":"#According to the describe function, there is a lot of data, and apparently a lot of NaN.\n#The dataset represents both quantitative and qualitative data.\n#These variables appear to be represented under several headings, with different suffixes.\n#First, let\u2019s look at the distribution of these missing values\npourcentage_nan = Origin2.isnull().sum().sort_values(ascending=False) \/ Origin2.shape[0] * 100","fdaaf89a":"#Let's visualize the Nan of the dataset\nplt.figure(figsize=(10,5))\nsns.displot(pourcentage_nan, bins=100, kde=False)\nplt.xlabel(\"missing values in %\")\nplt.ylabel(\"Nb of columns\") \nplt.title(\"Distribution of missing values by column of the source file Open Food Fact (in %) \")","15124bba":"#Now let's visualize these informations\n\ndef msv1(Origin2, thresh=60, color='black', edgecolor='black', width=25, height=8):\n    \n    plt.figure(figsize=(width,height))\n    percentage=(Origin2.isnull().mean())*100\n    percentage.sort_values(ascending=False).plot.bar(color=color, edgecolor=edgecolor)\n    plt.axhline(y=thresh, color='b', linestyle='-')\n    plt.title('Number of missing values per column', fontsize=20, weight='bold' )\n    plt.text(len(Origin2.isnull().sum()\/len(Origin2))\/1.7, thresh + 7, 'Columns with more than %s%s missing values' %(thresh, '%'), fontsize=20,weight='bold', color='black',\n         ha='right' ,va='top')\n    plt.text(len(Origin2.isnull().sum()\/len(Origin2))\/1.7, thresh - 5, 'Columns with less than %s%s missing values' %(thresh, '%'), fontsize=20,weight='bold', color='black',\n         ha='right' ,va='top')\n    plt.xlabel('Columns = Variables', size=20, weight='bold')\n    plt.ylabel('Missing values per columns (in %)', size=18, weight='bold')\n    plt.yticks(weight ='bold')\n    \n    return plt.show()","d3921ab3":"msv1(Origin2,60, color=('pink'))","a19c780b":"#note vt : rendre le visuel + lisible \/ changer couleurs ","d5bbf1f2":"# These 2 graphs confirm the low fill rate of the dataset.\n# It was decided to deal with this problem with sufficiently completed data\n#our 60\/40 ratio has already been predefined\n# The threshold technique is applied to remove the columns.\n\nselection = 60\n\ndef split_nan_60(dataframe,threshold):\n    # rows with no values (in %) \n    s = dataframe.isnull().mean(axis=0).apply(lambda x: x*100)\n    # we keep the columns if we are less than 60% of null values \n    cols_splitees = s[s<= threshold].index\n    return cols_splitees\n\n# Filter columns according to the threshold chosen \nOrigin_60 = Origin2[split_nan_60(Origin2,selection)]\n\nprint(\"Number of initial columns (=variables) : \" + str(Origin.shape[1]))\nprint(\"Number of columns (=variables) with less than \"+ str(selection)+\"% NaNs: \" + str(Origin_60.shape[1]))\nprint(Origin_60.columns)","6a67baa3":"print(f\"Shape of the dataset before cleaning {Origin.shape}\")\nprint(f\"Shape of the dataset after nettoyage {Origin_60.shape}\")\nprint(f\"We have deleted {Origin.shape[1]- Origin_60.shape[1]} columns\")\nprint(f\"We have deleted {Origin.shape[0]- Origin_60.shape[0]} rows\")","495ecd17":"#We had previously listed the list of columns (=variables) of this dataset.\n#We may have noticed that some columns have the same names but with suffixes\n#different. We wonder about the similarity between these columns.\n#Let\u2019s take an example:\n\nlist_columns = ['categories', 'categories_tags', 'categories_en']\nOrigin_60[Origin_60[list_columns].notnull().any(axis=1)][['product_name']+ list_columns][:20:3]","dc624a7f":"columns_to_remove = []\nfor s in Origin_60.columns:\n    if \"_en\" in s: \n        t = s.replace('_en', '')\n        u = s.replace('_en', '_tags')\n        print(\"{:<20} 'no suffix' -> {} ; '_tags' suffix -> {}\".format(s,\n                                t in Origin_60.columns, u in Origin_60.columns))\n        if t in Origin_60.columns: columns_to_remove.append(t)\n        if u in Origin_60.columns: columns_to_remove.append(u)\nOrigin_dropsuf = Origin_60.drop(columns_to_remove, axis = 1, inplace = False)\n\n\n#we made this choice to keep the _en as the labels are mostly in English in the world.","b55b39af":"print(f\"Shape of the dataset before the cleaning {Origin.shape}\")\nprint(f\"Shape of the dataset after cleaning {Origin_dropsuf.shape}\")\nprint(f\"We have deleted {Origin.shape[1]- Origin_dropsuf.shape[1]} columns\")\nprint(f\"We have deleted {Origin.shape[0]- Origin_dropsuf.shape[0]} rows\")","c99845ef":"#look at blank lines for quantitative variables\ncomposant = []\nfor s in Origin_dropsuf.columns:\n    if '_100g' in s: composant.append(s)\nsubset_columns = Origin_dropsuf[composant]\nprint('empty _100g raws: {}'.format(subset_columns.isnull().all(axis=1).sum()))\n#___________________\n# then we delete them\nOrigin_row = Origin_dropsuf[subset_columns.notnull().any(axis=1)]","93474729":"print(f\"Shape of the dataset before cleaning {Origin.shape}\")\nprint(f\"Shape of the dataset after cleaning {Origin_row.shape}\")\nprint(f\"We have deleted {Origin.shape[1]- Origin_row.shape[1]} columns\")\nprint(f\"We have deleted {Origin.shape[0]- Origin_row.shape[0]} rows\")","85a51d18":"#Let's list the remaining columns\nOrigin_row.columns.tolist()","6c7d5f4f":"Clean100= Origin_row[(Origin_row['energy_100g']<100)&(Origin_row['fat_100g']<100)&(Origin_row['saturated-fat_100g']<100)&(Origin_row['carbohydrates_100g']<100)&(Origin_row['sugars_100g']<100)&(Origin_row['proteins_100g']<100)&(Origin_row['salt_100g']<100)&(Origin_row['sodium_100g']<100)]\nClean100.head()\n\n\n#on does not remove energy-kcal_100g which can go up to 3700 KJ for example.","bac306fd":"print(f\"Shape of the dataset before cleaning {Origin.shape}\")\nprint(f\"Shape of the dataset after cleaning {Clean100.shape}\")\nprint(f\"We have deleted {Origin.shape[1]- Clean100.shape[1]} columns\")\nprint(f\"We have deleted {Origin.shape[0]- Clean100.shape[0]} rows\")","79d46e4d":"#Let\u2019s start with a general overview of the different food categories to be analyzed\n\nfig, ax = plt.subplots(1,1,figsize=(25,10))\nfig= sns.barplot(x='index',y=\"main_category_en\", data=Clean100['main_category_en'].value_counts().reset_index().head(10))\nax.set(ylabel = 'Frequency', xlabel='Category', title='World foods by main categories (frequency of occurrence - update May 2021')\nplt.show(fig)","d5388442":"fig, ax = plt.subplots(1,1,figsize=(5,10))\nfig= sns.barplot(x='categories_en',y=\"index\", data=Clean100['categories_en'].value_counts().reset_index().head(10))\nax.set(ylabel = 'Category', xlabel='Frequency', title='World foods by category (frequency of occurrence - update May 2021')\nplt.show(fig)","db46c1fb":"fig, ax = plt.subplots(1,1,figsize=(25,10))\nfig= sns.barplot(x='index',y=\"pnns_groups_1\", data=Clean100['pnns_groups_1'].value_counts().reset_index().head(10))\nax.set(ylabel = 'Frequency', xlabel='pnns_groups_1', title='World Foods by Pnns 1 Score (frequency of occurrence - update May 2021')\nplt.show(fig)","95cf8797":"magasins=Clean100['brands'].value_counts().head(15).to_frame()\nmagasins_visu = magasins.style.background_gradient(cmap='Blues')\nmagasins_visu","679937d1":"pays=Clean100['countries_en'].value_counts().head().to_frame()\npays_visu = pays.style.background_gradient(cmap='Blues')\npays_visu","88c97241":"sns.boxplot(Clean100.carbohydrates_100g)","c0c16bfa":"sns.boxplot(Clean100.carbohydrates_100g)","d458e9c7":"#For each category, a sub-sample (subset) is created that contains only the current category transactions.\n#The histogram is also displayed to bring the 3 measurements into perspective.\n\nfor cat in Clean100['pnns_groups_1'].unique():\n    subset = Clean100[Clean100.pnns_groups_1 == cat] # Cr\u00e9ation du sous-\u00e9chantillon\n    print(\"-\"*20)\n    print(cat)\n    print(\"moy:\\n\",subset['carbohydrates_100g'].mean())              #permet de conclure sur la tendance centrale\n    print(\"med:\\n\",subset['carbohydrates_100g'].median())           #permet de conclure sur la tendance centrale\n    print(\"mod:\\n\",subset['carbohydrates_100g'].mode())           #permet de conclure sur la tendance centrale\n    print(\"var:\\n\",subset['carbohydrates_100g'].var(ddof=0))       #permet de conclure sur la dispersion et l ecart type\n    print(\"ect:\\n\",subset['carbohydrates_100g'].std(ddof=0))       #permet de conclure sur la dispersion et l ecart type\n    print(\"skw:\\n\",subset['carbohydrates_100g'].skew())          #permet de conclure pour la mesure de forme\n    print(\"kur:\\n\",subset['carbohydrates_100g'].kurtosis())       #permet de conclure pour la mesure de forme\n    subset[\"carbohydrates_100g\"].hist() # Cr\u00e9e l'histogramme\n    plt.show() # Affiche l'histogramme\n    subset.boxplot(column=\"carbohydrates_100g\", vert=False)           #dispersion et forme        \n    plt.show()                                            #dispersion et forme","05cce21e":"#lorenz curve\n#There is one size segment n for each individual, plus an additional 0 ordinate segment.\n#The first segment starts at 0-1\/n, and the last one ends at 1+1\/n.\n\nAliment_sain = Clean100[Clean100['carbohydrates_100g']<=100]     \nSain = -Aliment_sain['carbohydrates_100g'].values\nn = len(Sain)\nlorenz = np.cumsum(np.sort(Sain)) \/ Sain.sum()\nlorenz = np.append([0],lorenz) # La courbe de Lorenz commence \u00e0 0\n\nplt.axes().axis('equal')\nxaxis = np.linspace(0-1\/n,1+1\/n,n+1) \nplt.plot(xaxis,lorenz,drawstyle='steps-post')\nplt.show()","a089fc9a":"#pearson between 2 quantitative variables\nSante_corr=Clean100.corr()\nf,ax=plt.subplots(figsize=(10,7))\nsns.heatmap(Sante_corr, cmap='viridis')\nplt.title(\"Correlation between remaining variables\", \n          weight='bold', \n          fontsize=18)\nplt.xticks(weight='bold')\nplt.yticks(weight='bold')\n\nplt.show()","eb423916":"#To calculate the Pearson coefficient and covariance\nmalbouffe = Clean100[Clean100.carbohydrates_100g<100]   \n\nprint(st.pearsonr(malbouffe[\"sugars_100g\"],-malbouffe[\"carbohydrates_100g\"])[0])\nprint(np.cov(malbouffe[\"sugars_100g\"],-malbouffe[\"carbohydrates_100g\"],ddof=0)[1,0])","3937018d":"#scatter plot\n\nplt.figure(figsize=(14,5))\n\n\ntaille_classe = 10 # class size for discretization \n\ngroupes = [] # will receive the aggregated data to display\n\n# units are calculated from 0 to the maximum balance by size increments_class\ntranches = np.arange(0, max(malbouffe[\"fat_100g\"]), taille_classe)\ntranches += taille_classe\/2 # half class size slices are shifted\nindices = np.digitize(malbouffe[\"fat_100g\"], tranches) # links each balance to its class number\n\nfor ind, tr in enumerate(tranches): # for each slice, ind receives the slice number and tr the slice in question\n    montants = -malbouffe.loc[indices==ind,\"carbohydrates_100g\"] # selection of individuals in the ind\n\n    if len(montants) > 0:\n        g = {\n            'valeurs': montants,\n            'centre_classe': tr-(taille_classe\/2),\n            'taille': len(montants),\n            'quartiles': [np.percentile(montants,p) for p in [25,50,75]]\n        }\n        groupes.append(g)\n\n# visualization of the boxplots\nplt.boxplot([g[\"valeurs\"] for g in groupes],\n            positions= [g[\"centre_classe\"] for g in groupes], # x-axis of boxplots\n            showfliers= False, # outliers are not taken into account\n            widths= taille_classe*0.6, # graphic width of the boxplots\n            \n                        \n)\n\n\n# display of the headcount of each class\nfor g in groupes:\n    plt.text(g[\"centre_classe\"],20,\"(n={})\".format(g[\"taille\"]),horizontalalignment='center',verticalalignment='top')  \n    \nplt.show()\n\n# display of quartiles\nfor n_quartile in range(3):\n    plt.plot([g[\"centre_classe\"] for g in groupes],\n             [g[\"quartiles\"][n_quartile] for g in groupes])\nplt.show()","8e440902":"#ANOVA : let's compare 1 quantitative variable with a qualitative variable\n\n\nX = \"pnns_groups_1\" # qualitative\nY = \"carbohydrates_100g\" # quantitative\n\n# We only keep the variable carbohydrates_100g. \nsous_echantillon = malbouffe[malbouffe[\"carbohydrates_100g\"]<100].copy()\n\n# Ensure final result will be in +\nsous_echantillon[\"carbohydrates_100g\"] = -sous_echantillon[\"carbohydrates_100g\"]\n\n# In order to reduce the size of the file, focus on the main categories:\nsous_echantillon = sous_echantillon[sous_echantillon[\"pnns_groups_1\"] != \"BOUFFE\"] ","5a5b469b":"#Let's visualize : \n\nmodalites = sous_echantillon[X].unique()\ngroupes = []\nfor m in modalites:\n    groupes.append(sous_echantillon[sous_echantillon[X]==m][Y])\n\n# Propri\u00e9t\u00e9s graphiques   \nmedianprops = {'color':\"black\"}\nmeanprops = {'marker':'o', 'markeredgecolor':'black',\n            'markerfacecolor':'firebrick'}\n    \nplt.boxplot(groupes, labels=modalites, showfliers=False, medianprops=medianprops, \n            vert=False, patch_artist=True, showmeans=True, meanprops=meanprops)\nplt.show()","0f46b054":"#Chi2 : let's compare two qualitative variables\n#pnns_groups_1 & Countries_en\nX = \"countries_en\"   #variable 1 \nY = \"pnns_groups_1\"  #variable 2 ","f951c57b":"#Contingency Table\ncontingency_table=pd.crosstab(malbouffe[\"countries_en\"],malbouffe[\"pnns_groups_1\"])\nprint('contingency_table :-\\n',contingency_table)","649ff73f":"Observed_Values = contingency_table.values \nprint(\"Valeurs observ\u00e9es :-\\n\",Observed_Values)","c748ec24":"b=stat.chi2_contingency(contingency_table)\nExpected_Values = b[3]\nprint(\"Expected values  :-\\n\",Expected_Values)","a0a61b24":"#Degree of Freedoms\nno_of_rows=len(contingency_table.iloc[0:2,0])\nno_of_columns=len(contingency_table.iloc[0,0:2])\ndf=(no_of_rows-1)*(no_of_columns-1)\nprint(\"#Degree of Freedoms:-\",df)","24979b52":"#significance level : 5%\nalpha=0.05","cfc6c2f5":"#chi-square statistic - \u03c72\nchi_square=sum([(o-e)**2.\/e for o,e in zip(Observed_Values,Expected_Values)])\nchi_square_statistic=chi_square[0]+chi_square[1]\nprint(\"chi-square statistic:-\",chi_square_statistic)","d3d9fe59":"#Critical Value\ncritical_value=chi2.ppf(q=1-alpha,df=df)\nprint('Critical Value:',critical_value)","92623e29":"#p-value\np_value=1-chi2.cdf(x=chi_square_statistic,df=df)\nprint('p-value:',p_value)","865b41ef":"print('RECAPITULATIF:')\n\nprint('significance level: ',alpha)\nprint('Degree of Freedoms: ',df)\nprint('chi-square statistic:',chi_square_statistic)\nprint('Critical value:',critical_value)\nprint('p-value:',p_value)","b7887f03":"#Let's compare the CHI 2 with the critical value and the p-value, of the probability to have  \n#to get CHI2>5194.289564659622 (chi_square_statistic)\n\nif chi_square_statistic>=critical_value:\n    print(\"We reject the hypothesis H0, that is to say that there is a link between the two variables\")\nelse:\n    print(\"We consider the hypothesis H0, that is to say that there is no link between the two variables.\")\n    \nif p_value<=alpha:\n    print(\"We reject the hypothesis H0, that is to say that there is a link between the two variables\")\nelse:\n    print(\"We consider the hypothesis H0, that is to say that there is no link between the two variables.\")","c35fe29a":"def count_words(malbouffe, colonne = 'categories_en'):\n    list_words = set()\n    for word in malbouffe[colonne].str.split(','):\n        if isinstance(word, float): continue\n        list_words = set().union(word, list_words)       \n    print(\"Number of sub-category in the category '{}': {}\".format(colonne, len(list_words)))\n    return list(list_words)","9d35792f":"list_countries = count_words(malbouffe, 'countries_en')","3dd2c061":"# Start with one review:\n#text = malbouffe.countries_en[1]\n\n# Create and generate a word cloud image:\n#wordcloud = WordCloud().generate(text)\n\n# Display the generated image:\n#plt.imshow(wordcloud, interpolation='bilinear')\n#plt.axis(\"off\")\n#plt.show()","d231fde4":"# Do we have missing values ? \nprint(malbouffe.isnull().values.any())","6fd5ff92":"#For quantitative variables, we replace missing values with their median.\nSante_med = malbouffe.fillna(malbouffe.median(), inplace=False)","eea9f66b":"print(Sante_med.isnull().sum())","082de47c":"#For qualitative variables, replace with a keyword.\nSante_med['product_name'].fillna(\"TBD\", inplace=True)    \nSante_med['brands'].fillna(\"TBD\", inplace=True)    \nSante_med['brands_tags'].fillna(\"TBD\", inplace=True)  \nSante_med['categories_en'].fillna(\"TBD\", inplace=True)  \nSante_med['countries_en'].fillna(\"TBD\", inplace=True)  \nSante_med['ingredients_text'].fillna(\"TBD\", inplace=True)  \nSante_med['pnns_groups_1'].fillna(\"TBD\", inplace=True) \nSante_med['main_category_en'].fillna(\"TBD\", inplace=True)  \nSante_med['image_url'].fillna(\"TBD\", inplace=True)  \nSante_med['image_small_url'].fillna(\"TBD\", inplace=True)  \nSante_med['image_ingredients_url'].fillna(\"TBD\", inplace=True)  \nSante_med['image_ingredients_small_url'].fillna(\"TBD\", inplace=True)  \nSante_med['image_nutrition_url'].fillna(\"TBD\", inplace=True)  \nSante_med['image_ingredients_url'].fillna(\"TBD\", inplace=True)  \nSante_med['image_nutrition_small_url'].fillna(\"TBD\", inplace=True)","0f28d04e":"#Check the NaN \nprint(Sante_med.isnull().sum())","e000e4cf":"#Let\u2019s check if we have Outliers in the processed dataset\nSante_med.select_dtypes(include=float).plot(kind='box', subplots=True, title='Outliers des variables', figsize=(20,20), layout=(6,4))\n\nplt.show()","3a47db83":"Carbo = Sante_med['carbohydrates_100g'].sort_values(ascending=False)\nprint(Carbo)","f6174a5e":"#we see that the values are between 0 and 99.9 . This is consistent with the notion of 100g.\n#So there is a wide distribution of values of this variable, but no outliers to remove.\n\nSugar = Sante_med['sugars_100g'].sort_values(ascending=False)\nprint(Sugar)","b1e36973":"def display_circles(pcs, n_comp, pca, axis_ranks, labels=None, label_rotation=0, lims=None):\n    for d1, d2 in axis_ranks: # We display the first 3 factorial plans, so the first 6 components\n        if d2 < n_comp:\n\n            # initialization of the figure\n            fig, ax = plt.subplots(figsize=(7,6))\n\n            # determination of chart boundaries\n            if lims is not None :\n                xmin, xmax, ymin, ymax = lims\n            elif pcs.shape[1] < 30 :\n                xmin, xmax, ymin, ymax = -1, 1, -1, 1\n            else :\n                xmin, xmax, ymin, ymax = min(pcs[d1,:]), max(pcs[d1,:]), min(pcs[d2,:]), max(pcs[d2,:])\n\n            # arrows display\n            # if there are more than 30 arrows, the triangle is not displayed at the end of the arrows\n            if pcs.shape[1] < 30 :\n                plt.quiver(np.zeros(pcs.shape[1]), np.zeros(pcs.shape[1]),\n                   pcs[d1,:], pcs[d2,:], \n                   angles='xy', scale_units='xy', scale=1, color=\"grey\")\n                # (voir la doc : https:\/\/matplotlib.org\/api\/_as_gen\/matplotlib.pyplot.quiver.html)\n            else:\n                lines = [[[0,0],[x,y]] for x,y in pcs[[d1,d2]].T]\n                ax.add_collection(LineCollection(lines, axes=ax, alpha=.1, color='black'))\n            \n            # display of variable names \n            if labels is not None:  \n                for i,(x, y) in enumerate(pcs[[d1,d2]].T):\n                    if x >= xmin and x <= xmax and y >= ymin and y <= ymax :\n                        plt.text(x, y, labels[i], fontsize='14', ha='center', va='center', rotation=label_rotation, color=\"blue\", alpha=0.5)\n            \n            # circle display\n            circle = plt.Circle((0,0), 1, facecolor='none', edgecolor='b')\n            plt.gca().add_artist(circle)\n\n            # definition of graph boundaries\n            plt.xlim(xmin, xmax)\n            plt.ylim(ymin, ymax)\n        \n            # display of horizontal and vertical lines\n            plt.plot([-1, 1], [0, 0], color='grey', ls='--')\n            plt.plot([0, 0], [-1, 1], color='grey', ls='--')\n\n            # name of the axes, with the percentage of inertia explained\n            plt.xlabel('F{} ({}%)'.format(d1+1, round(100*pca.explained_variance_ratio_[d1],1)))\n            plt.ylabel('F{} ({}%)'.format(d2+1, round(100*pca.explained_variance_ratio_[d2],1)))\n\n            plt.title(\"Cercle des corr\u00e9lations (F{} et F{})\".format(d1+1, d2+1))\n            plt.show(block=False)\n        \ndef display_factorial_planes(X_projected, n_comp, pca, axis_ranks, labels=None, alpha=1, illustrative_var=None):\n    for d1,d2 in axis_ranks:\n        if d2 < n_comp:\n \n            # initialization of the figure       \n            fig = plt.figure(figsize=(7,6))\n        \n            # # points display\n            if illustrative_var is None:\n                plt.scatter(X_projected[:, d1], X_projected[:, d2], alpha=alpha)\n            else:\n                illustrative_var = np.array(illustrative_var)\n                for value in np.unique(illustrative_var):\n                    selected = np.where(illustrative_var == value)\n                    plt.scatter(X_projected[selected, d1], X_projected[selected, d2], alpha=alpha, label=value)\n                plt.legend()\n\n            # point labels display\n            if labels is not None:\n                for i,(x,y) in enumerate(X_projected[:,[d1,d2]]):\n                    plt.text(x, y, labels[i],\n                              fontsize='14', ha='center',va='center') \n                \n            # determination of chart boundaries\n            boundary = np.max(np.abs(X_projected[:, [d1,d2]])) * 1.1\n            plt.xlim([-boundary,boundary])\n            plt.ylim([-boundary,boundary])\n        \n            # display of horizontal and vertical lines\n            plt.plot([-100, 100], [0, 0], color='grey', ls='--')\n            plt.plot([0, 0], [-100, 100], color='grey', ls='--')\n\n            # name of the axes, with the percentage of inertia explained\n            plt.xlabel('F{} ({}%)'.format(d1+1, round(100*pca.explained_variance_ratio_[d1],1)))\n            plt.ylabel('F{} ({}%)'.format(d2+1, round(100*pca.explained_variance_ratio_[d2],1)))\n\n            plt.title(\"Projection des individus (sur F{} et F{})\".format(d1+1, d2+1))\n            plt.show(block=False)\n\ndef display_scree_plot(pca):\n    scree = pca.explained_variance_ratio_*100\n    plt.bar(np.arange(len(scree))+1, scree)\n    plt.plot(np.arange(len(scree))+1, scree.cumsum(),c=\"red\",marker='o')\n    plt.xlabel(\"rang de l'axe d'inertie\")\n    plt.ylabel(\"pourcentage d'inertie\")\n    plt.title(\"Eboulis des valeurs propres\")\n    plt.show(block=False)","bfb5c8e0":"n_comp = 3","82550a19":"#selection of quantitative variables to be analysed - columns to be taken into account in the CPA\nSanteacp = Sante_med[[\"fat_100g\",\n                      \"sugars_100g\",\n                      \"carbohydrates_100g\",\n                      ]]","1f7d3b4f":"#on converti les donn\u00e9es en object array numpy\nX = Santeacp.values\nX","e5b89de1":"#this type of object does not retain column names, they are saved differently:\nnames = Santeacp[\"carbohydrates_100g\"]  \nfeatures = Santeacp.columns","1157fb66":"#centering and data reduction\nX_scaled = preprocessing.StandardScaler().fit_transform(X)","d6d8e01c":"#Calculation of the main components\npca = decomposition.PCA(n_components=n_comp)    #n_comp (=how many max components have been defined)\npca.fit(X_scaled) #then the data is provided for this object to calculate the main components","0789b20e":"#Clean Values Slide\ndisplay_scree_plot(pca)","7699780d":"#circle of correlations\npcs = pca.components_","fad18248":"display_circles(pcs,n_comp,pca,[(0,1),(2,3),(4,5)], labels = np.array(features)) \n                \n#numbers are the different factorial plans desired\n#labels is the names of individuals\n            ","db008cde":"#individual projection\nX_projected = pca.transform(X_scaled)\ndisplay_factorial_planes(X_projected, n_comp, pca, [(0,1),(2,3),(4,5)], labels = np.array(names))","05aafab0":"Santeacp.to_csv('PSant\u00e9_02_notebookexploration2.csv',index=False)","05a3d35f":"<a name=\"2.4\"><\/a>\n    2.4 columns details","2a408b35":"<a name=\"5\"><\/a>\n# 5 - ACP","daab9a0a":"To understand the type of results obtained, it is necessary to understand where the data comes from (what types of stores?). If they are large stores or bulk stores, the results may vary (on product diversity, as well as on labelling and packaging methods).","d2b3257d":"\u2695\ufe0f**Design an application for public health**\ud83c\udf45\n\n*Project for CentraleSupElec *\n\n--------------\n\n**Mission**\n\nThe French Public Health Agency has launched a call for projects to find innovative ideas for food-related applications. For this project, we want you to participate and propose an application idea. \n\n\n- 1) Process the dataset to identify variables relevant to future treatments. Automate these treatments to avoid repeating these operations.\n- 2) Throughout the analysis, generate vi sualizations to better understand the data. Perform a univariate analysis for each interesting variable, in order to synthesize its behavior.\n- 3) Confirm or deny assumptions using multivariate analysis. Perform appropriate statistical tests to verify the significance of the results.\n- 4) Develop an application idea. Identify arguments justifying the feasibility (or not) of the application from Open Food Facts data.\n- 5) Write an exploration report and pitch your idea during the project defense.\n\n\n\n**Dataset:**\n\nThe Open Food Fact dataset is available on the official website, [here](https:\/\/world.openfoodfacts.org\/).\nThe variables are defined at this address ([click](https:\/\/world.openfoodfacts.org\/data\/data-fields.txt))\n\nThe fields are separated into four sections:\n- General information on the product file: name, date of modification, etc.\n- A set of tags: product category, location, origin, etc.\n- The ingredients of the products and their possible additives.\n- Nutritional information: amount in grams of a nutrient per 100 grams of the product.\n\n\n\n\ud83c\udfaf **Competencies assessed:**\n\n\n- Perform cleaning operations on structured data\n- Communicate results using legible and relevant graphic representations\n- Perform multivariate statistical analysis\n- Perform univariate statistical analysis\n\n\n**Plan:**\n* [0 - Selected purpose](#0)\n* [1 - Upload the environment](#1)\n    * [1.1 - Import of the packages](#1.1)\n    * [1.2 - Parameters](#1.2)\n* [2 - Dataset discovery](#2)\n    * [2.1 - Basics](#2.1)\n    * [2.2 - NaN](#2.2)\n    * [2.3 - Statistics](#2.3)\n    * [2.4 - Columns details](#2.4)\n    * [2.5 - conclusion](#2.5)\n* [3 - EDA](#3)\n    * [3.1 - Duplicates](#3.1)\n    * [3.2 - Missing values](#3.2)\n    * [3.3 - Qualitatives features - filters](#3.3)\n    * [3.4 - Quantitatives features - filters](#3.4)\n    * [3.5 - Processing features _100g](#3.5)\n* [4 - Analyses](#4)\n    * [4.1 - Univariate analysis](#4.1)\n    * [4.2 - bivariates analysis](#4.2)\n    * [4.3 - NaN & outliers](#4.3)\n* [5 - ACP](#5)\n* [6 - Save](#6)","13f6493e":"<a name=\"2.1\"><\/a>\n    2.1 basics","cea04239":"<a name=\"2.2\"><\/a>\n    2.2 nan","90387a19":"> There are mostly qualitative variables","40e8560c":"<a name=\"4.3\"><\/a>\n    4.3 - NaN & Outliers","9733860f":"> a country filter can be performed during an in-depth analysis. Especially since the calculation below shows us a strong inconsistency: 310 countries instead of 197 official.","ff8f8bf8":"<a name=\"6\"><\/a>\n# 6 - Save","9adee576":"<a name=\"2.5\"><\/a>\n\n\ud83d\udca1 **Conclusions** :\n\n- This data set consists of 1'648'410 rows (=products) and 184 columns (=variables). The file size is colossal (memory usage: 2.3+ GB) but very random filling (according to the description function).\n\n- It will therefore be necessary to sort the data according to the filling rate in order to define which variables are exploitable.\n\n- We see that some columns seem redundant (same title, but with _tag or _en or _t or _datetime). These variable variants are described on the website: https:\/\/world.openfoodfacts.org\/data\/data-fields.txt\n\n- It will therefore be necessary to establish whether a sorting of the columns is possible to lighten\/ ventilate the dataset to move to the exploratory phase.\n\n- According to the describe function, we also see inconsistencies: Among other things: some quantitative columns whose maximum value should be 100g are (largely) higher than this number.\n\n- These rows and columns should therefore be carefully sorted.","036e7c3f":"Next steps : \n- include interesting ressources : \nhttps:\/\/www.kaggle.com\/petegore\/1-2-pre-filtering-french-dataset\nand\nhttps:\/\/www.kaggle.com\/michaelfumery\/openfoodfacts-data-cleaning","1394e011":"> the conclusion is widely confirmed: there is a link between the geographical origin and the quality of the product","2becf1bb":"<a name=\"4.1\"><\/a>\n    4.1 - Univariates analysis","2850fb2d":"<a name=\"2.3\"><\/a>\n    2.3 statistics","4f27f33e":"Understand the link between the quality of our food and carbohydrates in our food.","dd82b303":"These are mostly stores that are found in the French territory. Let\u2019s see if there is a correlation.","70a79807":"<a name=\"2\"><\/a>\n# 2 - Dataset discovery","f28e6c09":"<a name=\"3.4\"><\/a>\n    3.4 Quantitatives variables - filters","6d38a741":"<a name=\"1.1\"><\/a>\n    1.1 Import of the packages","f98e2d43":"<a name=\"1.2\"><\/a>\n    1.2 parameters","1c03079c":"<a name=\"3.1\"><\/a>\n    3.1 duplicates","5f6d3257":"**We can see 2 types of data:**\n\n****Ingredients****\n\n****Food information and its industrial distribution****\n\n* Information on barcodes that track products and allow traceability of items :\n\n'url','code','emb_codes', 'emb_codes_tags'\n\n* Information on the display of foodstuffs : \n\n'image_url', 'image_small_url', 'image_ingredients_url', 'image_ingredients_small_url', 'image_nutrition_url','image_nutrition_small_url'\n\n* Information about the person who entered the product information :\n\n'creator'\n\n* Information on food-related dates : \n\n'created_t', 'created_datetime','last_modified_t','last_modified_datetime'\n\n* Information on names and means of identification of foodstuffs : \n\n'product_name', 'abbreviated_product_name', 'generic_name'\n\n* Information on categories grouping food into categories : \n\n'categories', 'categories_tags', 'categories_en'\n\n* Information on the origins of the product : \n\n'origins', 'origins_tags', 'origins_en'\n\n* Information on product packaging, and unit packaging : \n\n'packaging', 'packaging_tags', 'packaging_text', 'first_packaging_code_geo', 'serving_size', 'serving_quantity','quantity'\n\n* Information on labels (organic, etc.) : \n\n'labels', 'labels_tags', 'labels_en'\n\n* Information on the stores that sell and reference the products : \n\n'brands', 'brands_tags','purchase_places', 'stores', 'brand_owner'\n\n* Information on countries, cities and states that offer registered products :\n\n'countries', 'countries_tags', 'countries_en', 'cities', 'cities_tags', 'states', 'states_tags', 'states_en'\n\n* Information on additives in food : \n\n'additives_n', 'additives', 'additives_tags', 'additives_en'\n\n* Information on food production sites : \n\n'manufacturing_places', 'manufacturing_places_tags'\n\n* Evaluation of product quality and its level of transformation : \n\n 'nutrition-score-fr_100g', 'nutrition-score-uk_100g, 'nutriscore_score', 'nutriscore_grade','nova_group', 'pnns_groups_1', 'pnns_groups_2'\n\n* Information on the presence\/absence or risk of palm oil ; \n\n'ingredients_from_palm_oil_n', 'ingredients_from_palm_oil', 'ingredients_from_palm_oil_tags', 'ingredients_that_may_be_from_palm_oil_n', 'ingredients_that_may_be_from_palm_oil', 'ingredients_that_may_be_from_palm_oil_tags'\n\n* Information on the presence of traces of certain ingredients, related to the risk of allergies : \n\n'ingredients_text', 'allergens', 'allergens_en', 'traces', 'traces_tags', 'traces_en', 'no_nutriments'\n\n* Information on general categories of articles : \n\n'main_category', 'main_category_en'","f0a3b89a":"<a name=\"1\"><\/a>\n# 1 - Upload the environment","2b65c97f":"we see that the majority of the food offered in the world is not very healthy. In bivariate analysis we can make the link between the countries","68e88e7c":"<a name=\"3.3\"><\/a>\n    3.3 Qualitatives variables - filters","cb153b45":"<a name=\"3\"><\/a>\n# 3 - EDA","133b2dc2":"> \"carbohydrates_100g\" & \"sugars_100g\" are correlated","5a25febb":"<a name=\"3.5\"><\/a>\n    3.5 - processing of features with _100g","d4cfb89a":"<a name=\"4\"><\/a>\n4 - Analysis","8314d52f":"<a name=\"3.2\"><\/a>\n    3.2 missing values","e585cfcd":"<a name=\"4.2\"><\/a>\n    4.2 - bivariates analysis","768e0997":"The dataset provided offers different variables to assess food quality.\nTo answer our problem, we will analyze the variable \"carbohydrates\" as well as the variables PNNS 1 & 2","4e26c141":"There are many columns whose contents are completely empty (+80 columns to += 100% missing values).\n\nThere are less than 20 columns that are 100% complete (0% missing values).\n\nThe lack of usable data is striking on this graph. We could keep the fifteen columns with 0 missing values, but the analysis would be very limited.\nIt was decided to extend the acceptable limit to 60% of missing values (i.e., 40% of completed values).\n\nonclusion: we will be able to remove many columns right now by taking 60\/40 as the ratio. Because you need enough data to exploit, but you need a minimum of data to analyze by columns.\n\nWe therefore delete all columns that have 60% more missing values","65a6728a":"<a name=\"0\"><\/a>\n# 0 - Selected purpose"}}