{"cell_type":{"b4825250":"code","36adc563":"code","8464e3ed":"code","953d2532":"code","981a8852":"code","ee1f44a3":"code","d6cd0cd0":"code","5b4661f7":"code","6a8b8db2":"code","6233c040":"code","cf83864d":"code","c7d20933":"code","0c0f8c73":"markdown","bdabe800":"markdown","d7d0f228":"markdown","2a6d203f":"markdown","b8c0b273":"markdown","21df5e7f":"markdown","2a234395":"markdown","4a44d98e":"markdown","1902b7f3":"markdown","c564b406":"markdown","569d4a9e":"markdown"},"source":{"b4825250":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom numpy.random import seed\nseed(17)\n\ndf = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","36adc563":"def getPreprocessor(scaler=StandardScaler()):\n    # Define feature column categories by column type\n    categorical_cols = df.select_dtypes(include=['object','category']).columns.to_list()\n    numeric_cols = df.select_dtypes(include='number').columns.to_list()\n\n    # Remove the target column (SalePrice) from our feature list\n    numeric_cols.remove('SalePrice')\n\n    # Preprocessing for numerical data\n    numerical_transformer = Pipeline(steps=[\n        ('scaler', scaler),\n        ('imputer', SimpleImputer(strategy='mean'))\n\n    ])\n\n    # Preprocessing for categorical data\n    categorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))#, categories=onehot_categories))\n    ])\n\n    # Bundle preprocessing for numerical and categorical data\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('numeric', numerical_transformer, numeric_cols),\n            ('categorical', categorical_transformer, categorical_cols)\n        ])\n    \n    return preprocessor","8464e3ed":"# Grab target as y, remove target from X\ntrain_test = df.copy()\ny = train_test.SalePrice\nX = train_test.drop(columns=['SalePrice'])\n\n# Split into train, test\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, train_size=0.8, random_state = 17)","953d2532":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\ndef baseline_model(\n    layer_sizes: tuple = (200, 100), \n    optimizer: str ='adam',  \n    activation: str ='relu', \n    input_dim: int = 286, \n    verbose: bool =False):\n    \"\"\"\n    Creates an NN model with the following structure:\n    \n    input (length input_dim) \n    --> layer 0, with layer_sizes[0] neurons \n    --> layer 1, with layer_sizes[1] neurons \n    --> ... \n    --> output\n    \"\"\"\n    model = Sequential()\n    \n    # Add input layer\n    if verbose:\n        print(\"making layer {} with {} neurons\".format(0, layer_sizes[0]))\n    model.add(Dense(layer_sizes[0], \n                    input_dim=input_dim, \n                    kernel_initializer='normal', \n                    activation=activation))\n    \n    # Add hidden layers\n    for i in range(1, len(layer_sizes)):\n        if verbose:\n            print(\"making layer {} with {} neurons\".format(i, layer_sizes[i]))\n        model.add(Dense(layer_sizes[i], \n                        kernel_initializer='normal', \n                        activation=activation))\n\n    # Add final layer to ensure output is 1 dimensional\n    model.add(Dense(1, kernel_initializer='normal'))\n    # Compile model\n    model.compile(loss='mean_squared_error', optimizer=optimizer)\n    return model","981a8852":"# Define pipeline, combining preprocessor and model\npreprocessor = getPreprocessor()\nmodel = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                                ('model', model)])\n\n# Use pipeline to predict on validation data, plot validation vs predictions for the heck of it.\npipeline.fit(train_X,train_y)\npreds = pipeline.predict(val_X)\nplt.plot(preds, val_y, 'o')\nplt.xlabel(\"predictions\")\nplt.ylabel(\"actuals\")\nplt.show()","ee1f44a3":"history = model.fit(x=preprocessor.fit_transform(train_X), \n                    validation_data=(preprocessor.transform(val_X), val_y), \n                    y=train_y)\n\nplt.plot(history.history['loss'][2:], label='training data loss')\nplt.plot(history.history['val_loss'][2:], label='validation data los')\nplt.legend()\nplt.show()","d6cd0cd0":"val_loss = history.history['val_loss']\nprint(\"The validation loss is minimzed at epoch {}\".format(np.asarray(val_loss).argmin()))","5b4661f7":"from sklearn.metrics import mean_squared_error\n\n# taken from https:\/\/machinelearningmastery.com\/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling\/\n\ndef repeated_evaluation(train_X, train_y, val_X, val_y, scaler, n_repeats=10):\n    \"\"\"\n    Evaluates a model using preprocessing scaling scaler, by training the data n_repeats times.\n    \"\"\"\n    # Definte the model\n    model = KerasRegressor(build_fn=baseline_model, epochs=5, batch_size=5, verbose=0)\n    pipeline = Pipeline(steps=[('preprocessor', getPreprocessor(scaler)),\n                                ('model', model)])\n    \n    # Test the model by training it many times and recording the MSE each time.\n    results = list()\n    for n in range(n_repeats):\n        pipeline.fit(train_X,train_y)\n        # minimzation loss functions are negative so that you're still trying to maximize; \n        # have to multiply by -1 to make positive again\n        mse = -1 * pipeline.score(val_X, val_y) \n        results.append(mse)\n        \n    return results","6a8b8db2":"no_scaler = repeated_evaluation(train_X, train_y, val_X, val_y, scaler=None)\nprint(\"No scaling eval complete!\")\nminmax = repeated_evaluation(train_X, train_y, val_X, val_y, scaler=MinMaxScaler())\nprint(\"Minmax eval complete!\")\nstandard = repeated_evaluation(train_X, train_y, val_X, val_y, scaler=StandardScaler())\nprint(\"Standard eval complete!\")\n\nprint('Unscaled: %.3f (%.3f)' % (np.mean(no_scaler), np.std(no_scaler)))\nprint('MinMax: %.3f (%.3f)' % ( np.mean(standard), np.std(standard)))\nprint('Standardized: %.3f (%.3f)' % (np.mean(minmax), np.std(minmax)))\n\n# plot results\nresults = [no_scaler, minmax, standard]\nlabels = ['unscaled', 'minmax', 'standardized']\nplt.boxplot(results, labels=labels)\nplt.ylabel('Mean Standard Error')\nplt.show()","6233c040":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n# uses some of https:\/\/machinelearningmastery.com\/grid-search-hyperparameters-deep-learning-models-python-keras\/\n\n# RandomizedSearchCV tuning parameters\noptimizer = ['adam', 'rmsprop', 'adagrad']\nactivation = ['relu', 'sigmoid', 'tanh']\nlayer_sizes = [(200, 100), (400,), (100,100), (200, 200), (200, 200, 200)]\nepochs = [10, 20, 30, 40, 50]\nbatch_size = [16, 32, 64, 128, 256]\nparam_dist = dict(optimizer=optimizer, \n                  activation=activation,\n                 layer_sizes=layer_sizes,\n                 batch_size = batch_size,\n                 epochs = epochs)\n\n# Create a grid, make a preprocessor + grid pipeline\n# n_jobs = -1 means use the maximum number of cores available\ngrid = RandomizedSearchCV(estimator=model, \n                          param_distributions=param_dist, \n                          n_iter=100, \n                          scoring = 'neg_mean_squared_error', \n                          verbose=1, \n                          n_jobs=1)\ngrid_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                               ('grid', grid)])\n\n# Fit the pipeline, which runs the grid\npipeline_result = grid_pipeline.fit(train_X,train_y)\ngrid_result = pipeline_result.named_steps['grid']\n\n# Summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nrank = grid_result.cv_results_['rank_test_score']\nfor mean, stdev, param, rank in zip(means, stds, params, rank):\n    if rank <= 10:\n        print(f\"RANK {rank:.0f}: {mean:.1f} ({stdev:.1f}) with: {param}\")","cf83864d":"# GridSearchCV tuning parameters\noptimizer = ['adam', 'adagrad', 'rmsprop']\nactivation = ['relu']\nlayer_sizes = [(100,100), (200, 100), (200, 200), (200, 200, 200)]\nepochs = [20, 30, 40, 50]\nbatch_size = [16, 32, 64]\nparam_grid = dict(optimizer=optimizer, \n                  activation=activation,\n                 layer_sizes=layer_sizes,\n                 batch_size = batch_size,\n                 epochs = epochs)\n\n# Create a grid, make a preprocessor + grid pipeline\ngrid = GridSearchCV(estimator=model, \n                    param_grid=param_grid, \n                    scoring = 'neg_mean_squared_error', \n                    verbose=1, \n                    n_jobs=1)\ngrid_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                               ('grid', grid)])\n\n# Fit the pipeline, which runs the grid\npipeline_result = grid_pipeline.fit(train_X,train_y)\ngrid_result = pipeline_result.named_steps['grid']\n\n# Summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nrank = grid_result.cv_results_['rank_test_score']\nfor mean, stdev, param, rank in zip(means, stds, params, rank):\n    if rank <= 10:\n        print(f\"RANK {rank:.0f}: {mean:.1f} ({stdev:.1f}) with: {param}\")","c7d20933":"# Create predictions to be submitted!\nfinal_predictions = pipeline_result.predict(test)\npd.DataFrame({'Id': test.Id, 'SalePrice': final_predictions}).to_csv('Keras.csv', index =False)  \nprint(\"Done :D\")","0c0f8c73":"## What's in This Notebook\n\nThis notebook is a walkthrough of basic deep learning practices in [Keras](https:\/\/keras.io\/), a user-friendly wrapper API for Tensorflow and Theano. \n\nShoutout to the following references, which I use throughout! Happy modeling \ud83d\ude0a\n - [Machine Learning Mastery: Regression Tutorial with Keras](https:\/\/machinelearningmastery.com\/regression-tutorial-keras-deep-learning-library-python\/) for constructing basic Keras models\n - [Google's Machine Learning Crash Course](https:\/\/developers.google.com\/machine-learning\/crash-course\/introduction-to-neural-networks\/playground-exercises) for a great visualization tool for tuning NNs\n - [A Simple Guide to Hyperparameter Tuning in NNs](https:\/\/towardsdatascience.com\/simple-guide-to-hyperparameter-tuning-in-neural-networks-3fe03dad8594) for thinking through which parameters to tune in an NN\n - [A Guide to an Efficient Way to Build NN Architectures](https:\/\/towardsdatascience.com\/a-guide-to-an-efficient-way-to-build-neural-network-architectures-part-i-hyper-parameter-8129009f131b) for using hyperas to tune NNs, if you want something more descriptive than gridsearch\n - [Optimal Pipeline Best Practices](https:\/\/medium.com\/@DrGabrielHarris\/python-how-scikit-learn-0-20-optimal-pipeline-and-best-practices-dc4dd94d2c09) for how to write clean pipeline code\n - [GridSearch Pipelines](https:\/\/stackoverflow.com\/questions\/43366561\/use-sklearns-gridsearchcv-with-a-pipeline-preprocessing-just-once) for putting gridsearch in a pipeline and avoiding preprocessing multiple times","bdabe800":"We care about where the test loss is lowest, since if the train loss is going down but the test loss isn't that's a sign of overfitting! Let's see where the validation loss is at its best.","d7d0f228":"If we wanted to use this model as part of an [sklearn pipeline](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html), we could do the following:","2a6d203f":"Great, so it looks like the standard scaling option will be best here!\n\nWhile it's cool to see those boxplots, there's a few problems with this technique: it takes way to much time, and it'll get more and more complicated to write code for it as we add in more parameters to test.\n\nInstead, we'll use sklearn's [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html) and [RandomizedSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html) to more efficiently run through parameter choices. GridSearch will test every possible parameter combination, whereas RandomizedSearch will randomly select parameter combinations and run as many trials as you tell it to. GridSearch is more thorough but takes way longer, so which one to use depends on your computational power and desire for accuracy. Here we'll use a combination of both. First, we'll try a wide variety of options with RandomizedSearchCV.\n\nNote on the pipeline structure here: Rather than use a normal pipeline of preprocessor + model, this is a [preprocessor + grid pipeline](https:\/\/stackoverflow.com\/questions\/43366561\/use-sklearns-gridsearchcv-with-a-pipeline-preprocessing-just-once). The reason for the change is that this means the preprocessing is done only once, which if wasn't the case, would lead to different onehotencodings per each cross validation set. If you want to test parameters in both the model and preprocessor at the same time, you'll need to retrain every time, in which case you'll want to use a preprocessor + model pipeline and gridesearch afterwards!","b8c0b273":"Now we can put the training data through our transformer! We split the data into a train and test set (since the final test set isn't given to us, realistically we're splitting into train and validation sets here. One for training, one for parameter tuning!).","21df5e7f":"## Import and Clean Data\n\nThis data cleaning code is taken from my earlier notebook on [how to process the housing price dataset using a pipeline and random forrest regressor](https:\/\/www.kaggle.com\/dinasinclair\/housing-prices-from-basics-to-random-forest). If you haven't looked at this dataset yet, I'd recommend either looking at that earlier notebook or do some exploration on the dataset directly for more context! If you want to take the initial dataset processing as a given and go directly to the neural net part, the processing I do is:\n - Standard scaling\n - Impute numeric values with their mean\n - Impute categoric values with their mode\n - Onehot encode categoric values\n \nall of which are defined in a ColumnTransformer Pipeline.\n ","2a234395":"## Validating a Keras Neural Net\n\nHow good is our model? How many epochs will it take for the model to fit well without overfitting? Keras automatically records how the model is scoring per epoch, and we access that below. The history['loss'] is the loss score of the model on the training data, whereas history['val_loss'] is the loss score on the validation data.\n\nWe don't use a pipeline here (just the preprocessor) so that we can access the history parameters.","4a44d98e":"## Tuning a Keras Neural Net\n\nSince this neural net is initialized randomly (each weight is sampled from a normal distribution, as per the [kernel_initializer](https:\/\/keras.io\/initializers\/)) each training of the model will give a slightly different result. Thus, in an ideal world, we'd train each possible parameter set several times to get a more accurate idea of how well the model does. \n\nThe code below tests a first possible parameter choice: what kind of scaling to use in the preprocessing step. For each of the three choices (No scaling, MinMax Scaling, Standar Scaling) we fit the model 20 times, and then display the results to see which scaler fares best.","1902b7f3":"Since the best results use the relu activation, it seems safe to choose that in our gridsearch. Similarly, since there are few top results using 10 epochs or the (400,) neural net structure, so we'll drop those. Since there weren't that many good hits from the large batch sizes and those are the most heavy computationally, we'll drop the 128 and 256 batch sizes as well.","c564b406":"## Submit Solutions\n\nGridsearch keeps the best scoring parameters as its final fit, so we can go ahead and use the same pipeline to predict on the test set.","569d4a9e":"## Define a Keras Neural Net\n\n[Keras sequential models](https:\/\/keras.io\/getting-started\/sequential-model-guide\/) are made up of layers of neurons. After initializing the model, you progressively add layers to it via model.add(). The first layer you add is closest to the input, and thus you need to pass that layer the dimensions of the input. Layers you add next add on to the network until the final layer, which will return an output.\n\nThe key features here are the number of neurons per layer, the number of layers, the [optimizer](https:\/\/keras.io\/optimizers\/) used, and the [activation](https:\/\/keras.io\/activations\/) used.\n\nTo make iterating on model parameters easier, the baseline_model function below creates a model given an optimizer, a list of network layers (each with its own number of neurons) and an activation.\n\nAs a final note, the weights are initialized in the model via a normal distribution, as indicated by kernel_initializer='normal'. Other initialization styles are listed [here](https:\/\/keras.io\/initializers\/).\n"}}