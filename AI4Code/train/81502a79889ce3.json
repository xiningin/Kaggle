{"cell_type":{"517a8186":"code","69332719":"code","e77860b8":"code","6d30b328":"code","918c9509":"code","0c538133":"code","3e14b96f":"code","2ede6b07":"code","6e8b08fc":"code","8567f0cc":"code","a5249f14":"code","d99759bf":"code","de2b3746":"code","35a83f68":"code","8a1fc235":"code","92303e39":"code","74aa4986":"code","83ec772b":"code","0cddfdc8":"code","17d0d305":"code","065b31ec":"markdown","f7d853e5":"markdown","de7b630c":"markdown","bbf845a2":"markdown","b310827f":"markdown","be8eead3":"markdown","f0b6a78d":"markdown","c9c867a5":"markdown","1095bdb4":"markdown","ef221470":"markdown","99801397":"markdown","faa6cef3":"markdown","b1f1bd28":"markdown","6aa89820":"markdown","6bddc40a":"markdown","c0cf5389":"markdown","4f7b2294":"markdown","bf874fa7":"markdown","c4e76b79":"markdown","a6d605f3":"markdown"},"source":{"517a8186":"import torch\nimport torch.nn as nn\nimport time\nimport math\nimport sys\nsys.path.append(\"..\/input\/\")\nimport d2lzhpytorch as d2l\n(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","69332719":"def one_hot(x, n_class, dtype=torch.float32):\n    result = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)  # shape: (n, n_class)\n    result.scatter_(1, x.long().view(-1, 1), 1)  # result[i, x[i, 0]] = 1\n    return result\n    \nx = torch.tensor([0, 2])\nx_one_hot = one_hot(x, vocab_size)\nprint(x_one_hot)\nprint(x_one_hot.shape)\nprint(x_one_hot.sum(axis=1))","e77860b8":"def to_onehot(X, n_class):\n    return [one_hot(X[:, i], n_class) for i in range(X.shape[1])]\n\nX = torch.arange(10).view(2, 5)\ninputs = to_onehot(X, vocab_size)\nprint(len(inputs), inputs[0].shape)","6d30b328":"num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n# num_inputs: d\n# num_hiddens: h, \u9690\u85cf\u5355\u5143\u7684\u4e2a\u6570\u662f\u8d85\u53c2\u6570\n# num_outputs: q\n\ndef get_params():\n    def _one(shape):\n        param = torch.zeros(shape, device=device, dtype=torch.float32)\n        nn.init.normal_(param, 0, 0.01)\n        return torch.nn.Parameter(param)\n\n    # \u9690\u85cf\u5c42\u53c2\u6570\n    W_xh = _one((num_inputs, num_hiddens))\n    W_hh = _one((num_hiddens, num_hiddens))\n    b_h = torch.nn.Parameter(torch.zeros(num_hiddens, device=device))\n    # \u8f93\u51fa\u5c42\u53c2\u6570\n    W_hq = _one((num_hiddens, num_outputs))\n    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device))\n    return (W_xh, W_hh, b_h, W_hq, b_q)","918c9509":"def rnn(inputs, state, params):\n    # inputs\u548coutputs\u7686\u4e3anum_steps\u4e2a\u5f62\u72b6\u4e3a(batch_size, vocab_size)\u7684\u77e9\u9635\n    W_xh, W_hh, b_h, W_hq, b_q = params\n    H, = state\n    outputs = []\n    for X in inputs:\n        H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)\n        Y = torch.matmul(H, W_hq) + b_q\n        outputs.append(Y)\n    return outputs, (H,)","0c538133":"def init_rnn_state(batch_size, num_hiddens, device):\n    return (torch.zeros((batch_size, num_hiddens), device=device), )","3e14b96f":"print(X.shape)\nprint(num_hiddens)\nprint(vocab_size)\nstate = init_rnn_state(X.shape[0], num_hiddens, device)\ninputs = to_onehot(X.to(device), vocab_size)\nparams = get_params()\noutputs, state_new = rnn(inputs, state, params)\nprint(len(inputs), inputs[0].shape)\nprint(len(outputs), outputs[0].shape)\nprint(len(state), state[0].shape)\nprint(len(state_new), state_new[0].shape)","2ede6b07":"def grad_clipping(params, theta, device):\n    norm = torch.tensor([0.0], device=device)\n    for param in params:\n        norm += (param.grad.data ** 2).sum()\n    norm = norm.sqrt().item()\n    if norm > theta:\n        for param in params:\n            param.grad.data *= (theta \/ norm)","6e8b08fc":"def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\n                num_hiddens, vocab_size, device, idx_to_char, char_to_idx):\n    state = init_rnn_state(1, num_hiddens, device)\n    output = [char_to_idx[prefix[0]]]   # output\u8bb0\u5f55prefix\u52a0\u4e0a\u9884\u6d4b\u7684num_chars\u4e2a\u5b57\u7b26\n    for t in range(num_chars + len(prefix) - 1):\n        # \u5c06\u4e0a\u4e00\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u4f5c\u4e3a\u5f53\u524d\u65f6\u95f4\u6b65\u7684\u8f93\u5165\n        X = to_onehot(torch.tensor([[output[-1]]], device=device), vocab_size)\n        # \u8ba1\u7b97\u8f93\u51fa\u548c\u66f4\u65b0\u9690\u85cf\u72b6\u6001\n        (Y, state) = rnn(X, state, params)\n        # \u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u5165\u662fprefix\u91cc\u7684\u5b57\u7b26\u6216\u8005\u5f53\u524d\u7684\u6700\u4f73\u9884\u6d4b\u5b57\u7b26\n        if t < len(prefix) - 1:\n            output.append(char_to_idx[prefix[t + 1]])\n        else:\n            output.append(Y[0].argmax(dim=1).item())\n    return ''.join([idx_to_char[i] for i in output])","8567f0cc":"predict_rnn('\u5206\u5f00', 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,\n            device, idx_to_char, char_to_idx)","a5249f14":"def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n                          vocab_size, device, corpus_indices, idx_to_char,\n                          char_to_idx, is_random_iter, num_epochs, num_steps,\n                          lr, clipping_theta, batch_size, pred_period,\n                          pred_len, prefixes):\n    if is_random_iter:\n        data_iter_fn = d2l.data_iter_random\n    else:\n        data_iter_fn = d2l.data_iter_consecutive\n    params = get_params()\n    loss = nn.CrossEntropyLoss()\n\n    for epoch in range(num_epochs):\n        if not is_random_iter:  # \u5982\u4f7f\u7528\u76f8\u90bb\u91c7\u6837\uff0c\u5728epoch\u5f00\u59cb\u65f6\u521d\u59cb\u5316\u9690\u85cf\u72b6\u6001\n            state = init_rnn_state(batch_size, num_hiddens, device)\n        l_sum, n, start = 0.0, 0, time.time()\n        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)\n        for X, Y in data_iter:\n            if is_random_iter:  # \u5982\u4f7f\u7528\u968f\u673a\u91c7\u6837\uff0c\u5728\u6bcf\u4e2a\u5c0f\u6279\u91cf\u66f4\u65b0\u524d\u521d\u59cb\u5316\u9690\u85cf\u72b6\u6001\n                state = init_rnn_state(batch_size, num_hiddens, device)\n            else:  # \u5426\u5219\u9700\u8981\u4f7f\u7528detach\u51fd\u6570\u4ece\u8ba1\u7b97\u56fe\u5206\u79bb\u9690\u85cf\u72b6\u6001\n                for s in state:\n                    s.detach_()\n            # inputs\u662fnum_steps\u4e2a\u5f62\u72b6\u4e3a(batch_size, vocab_size)\u7684\u77e9\u9635\n            inputs = to_onehot(X, vocab_size)\n            # outputs\u6709num_steps\u4e2a\u5f62\u72b6\u4e3a(batch_size, vocab_size)\u7684\u77e9\u9635\n            (outputs, state) = rnn(inputs, state, params)\n            # \u62fc\u63a5\u4e4b\u540e\u5f62\u72b6\u4e3a(num_steps * batch_size, vocab_size)\n            outputs = torch.cat(outputs, dim=0)\n            # Y\u7684\u5f62\u72b6\u662f(batch_size, num_steps)\uff0c\u8f6c\u7f6e\u540e\u518d\u53d8\u6210\u5f62\u72b6\u4e3a\n            # (num_steps * batch_size,)\u7684\u5411\u91cf\uff0c\u8fd9\u6837\u8ddf\u8f93\u51fa\u7684\u884c\u4e00\u4e00\u5bf9\u5e94\n            y = torch.flatten(Y.T)\n            # \u4f7f\u7528\u4ea4\u53c9\u71b5\u635f\u5931\u8ba1\u7b97\u5e73\u5747\u5206\u7c7b\u8bef\u5dee\n            l = loss(outputs, y.long())\n            \n            # \u68af\u5ea6\u6e050\n            if params[0].grad is not None:\n                for param in params:\n                    param.grad.data.zero_()\n            l.backward()\n            grad_clipping(params, clipping_theta, device)  # \u88c1\u526a\u68af\u5ea6\n            d2l.sgd(params, lr, 1)  # \u56e0\u4e3a\u8bef\u5dee\u5df2\u7ecf\u53d6\u8fc7\u5747\u503c\uff0c\u68af\u5ea6\u4e0d\u7528\u518d\u505a\u5e73\u5747\n            l_sum += l.item() * y.shape[0]\n            n += y.shape[0]\n\n        if (epoch + 1) % pred_period == 0:\n            print('epoch %d, perplexity %f, time %.2f sec' % (\n                epoch + 1, math.exp(l_sum \/ n), time.time() - start))\n            for prefix in prefixes:\n                print(' -', predict_rnn(prefix, pred_len, rnn, params, init_rnn_state,\n                    num_hiddens, vocab_size, device, idx_to_char, char_to_idx))","d99759bf":"num_epochs, num_steps, batch_size, lr, clipping_theta = 250, 35, 32, 1e2, 1e-2\npred_period, pred_len, prefixes = 50, 50, ['\u5206\u5f00', '\u4e0d\u5206\u5f00']","de2b3746":"train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n                      vocab_size, device, corpus_indices, idx_to_char,\n                      char_to_idx, True, num_epochs, num_steps, lr,\n                      clipping_theta, batch_size, pred_period, pred_len,\n                      prefixes)","35a83f68":"train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n                      vocab_size, device, corpus_indices, idx_to_char,\n                      char_to_idx, False, num_epochs, num_steps, lr,\n                      clipping_theta, batch_size, pred_period, pred_len,\n                      prefixes)","8a1fc235":"rnn_layer = nn.RNN(input_size=vocab_size, hidden_size=num_hiddens)\nnum_steps, batch_size = 35, 2\nX = torch.rand(num_steps, batch_size, vocab_size)\nstate = None\nY, state_new = rnn_layer(X, state)\nprint(Y.shape, state_new.shape)","92303e39":"class RNNModel(nn.Module):\n    def __init__(self, rnn_layer, vocab_size):\n        super(RNNModel, self).__init__()\n        self.rnn = rnn_layer\n        self.hidden_size = rnn_layer.hidden_size * (2 if rnn_layer.bidirectional else 1) \n        self.vocab_size = vocab_size\n        self.dense = nn.Linear(self.hidden_size, vocab_size)\n\n    def forward(self, inputs, state):\n        # inputs.shape: (batch_size, num_steps)\n        X = to_onehot(inputs, vocab_size)\n        X = torch.stack(X)  # X.shape: (num_steps, batch_size, vocab_size)\n        hiddens, state = self.rnn(X, state)\n        hiddens = hiddens.view(-1, hiddens.shape[-1])  # hiddens.shape: (num_steps * batch_size, hidden_size)\n        output = self.dense(hiddens)\n        return output, state","74aa4986":"def predict_rnn_pytorch(prefix, num_chars, model, vocab_size, device, idx_to_char,\n                      char_to_idx):\n    state = None\n    output = [char_to_idx[prefix[0]]]  # output\u8bb0\u5f55prefix\u52a0\u4e0a\u9884\u6d4b\u7684num_chars\u4e2a\u5b57\u7b26\n    for t in range(num_chars + len(prefix) - 1):\n        X = torch.tensor([output[-1]], device=device).view(1, 1)\n        (Y, state) = model(X, state)  # \u524d\u5411\u8ba1\u7b97\u4e0d\u9700\u8981\u4f20\u5165\u6a21\u578b\u53c2\u6570\n        if t < len(prefix) - 1:\n            output.append(char_to_idx[prefix[t + 1]])\n        else:\n            output.append(Y.argmax(dim=1).item())\n    return ''.join([idx_to_char[i] for i in output])","83ec772b":"model = RNNModel(rnn_layer, vocab_size).to(device)\npredict_rnn_pytorch('\u5206\u5f00', 10, model, vocab_size, device, idx_to_char, char_to_idx)","0cddfdc8":"def train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\n                                corpus_indices, idx_to_char, char_to_idx,\n                                num_epochs, num_steps, lr, clipping_theta,\n                                batch_size, pred_period, pred_len, prefixes):\n    loss = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    model.to(device)\n    for epoch in range(num_epochs):\n        l_sum, n, start = 0.0, 0, time.time()\n        data_iter = d2l.data_iter_consecutive(corpus_indices, batch_size, num_steps, device) # \u76f8\u90bb\u91c7\u6837\n        state = None\n        for X, Y in data_iter:\n            if state is not None:\n                # \u4f7f\u7528detach\u51fd\u6570\u4ece\u8ba1\u7b97\u56fe\u5206\u79bb\u9690\u85cf\u72b6\u6001\n                if isinstance (state, tuple): # LSTM, state:(h, c)  \n                    state[0].detach_()\n                    state[1].detach_()\n                else: \n                    state.detach_()\n            (output, state) = model(X, state) # output.shape: (num_steps * batch_size, vocab_size)\n            y = torch.flatten(Y.T)\n            l = loss(output, y.long())\n            \n            optimizer.zero_grad()\n            l.backward()\n            grad_clipping(model.parameters(), clipping_theta, device)\n            optimizer.step()\n            l_sum += l.item() * y.shape[0]\n            n += y.shape[0]\n        \n\n        if (epoch + 1) % pred_period == 0:\n            print('epoch %d, perplexity %f, time %.2f sec' % (\n                epoch + 1, math.exp(l_sum \/ n), time.time() - start))\n            for prefix in prefixes:\n                print(' -', predict_rnn_pytorch(\n                    prefix, pred_len, model, vocab_size, device, idx_to_char,\n                    char_to_idx))","17d0d305":"num_epochs, batch_size, lr, clipping_theta = 250, 32, 1e-3, 1e-2\npred_period, pred_len, prefixes = 50, 50, ['\u5206\u5f00', '\u4e0d\u5206\u5f00']\ntrain_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\n                            corpus_indices, idx_to_char, char_to_idx,\n                            num_epochs, num_steps, lr, clipping_theta,\n                            batch_size, pred_period, pred_len, prefixes)","065b31ec":"### \u56f0\u60d1\u5ea6\n\n\u6211\u4eec\u901a\u5e38\u4f7f\u7528<span class=\"graffiti-highlight graffiti-id_kasti5n-id_zonxdw3\"><i><\/i>\u56f0\u60d1\u5ea6<\/span>\uff08perplexity\uff09\u6765\u8bc4\u4ef7\u8bed\u8a00\u6a21\u578b\u7684\u597d\u574f\u3002\u56de\u5fc6\u4e00\u4e0b[\u201csoftmax\u56de\u5f52\u201d](..\/chapter_deep-learning-basics\/softmax-regression.ipynb)\u4e00\u8282\u4e2d\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\u7684\u5b9a\u4e49\u3002\u56f0\u60d1\u5ea6\u662f\u5bf9\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\u505a\u6307\u6570\u8fd0\u7b97\u540e\u5f97\u5230\u7684\u503c\u3002\u7279\u522b\u5730\uff0c\n\n* \u6700\u4f73\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u603b\u662f\u628a\u6807\u7b7e\u7c7b\u522b\u7684\u6982\u7387\u9884\u6d4b\u4e3a1\uff0c\u6b64\u65f6\u56f0\u60d1\u5ea6\u4e3a1\uff1b\n* \u6700\u574f\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u603b\u662f\u628a\u6807\u7b7e\u7c7b\u522b\u7684\u6982\u7387\u9884\u6d4b\u4e3a0\uff0c\u6b64\u65f6\u56f0\u60d1\u5ea6\u4e3a\u6b63\u65e0\u7a77\uff1b\n* \u57fa\u7ebf\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u603b\u662f\u9884\u6d4b\u6240\u6709\u7c7b\u522b\u7684\u6982\u7387\u90fd\u76f8\u540c\uff0c\u6b64\u65f6\u56f0\u60d1\u5ea6\u4e3a\u7c7b\u522b\u4e2a\u6570\u3002\n\n\u663e\u7136\uff0c\u4efb\u4f55\u4e00\u4e2a\u6709\u6548\u6a21\u578b\u7684\u56f0\u60d1\u5ea6\u5fc5\u987b\u5c0f\u4e8e\u7c7b\u522b\u4e2a\u6570\u3002\u5728\u672c\u4f8b\u4e2d\uff0c\u56f0\u60d1\u5ea6\u5fc5\u987b\u5c0f\u4e8e\u8bcd\u5178\u5927\u5c0f`vocab_size`\u3002\n\n### \u5b9a\u4e49\u6a21\u578b\u8bad\u7ec3\u51fd\u6570\n\n\u8ddf\u4e4b\u524d\u7ae0\u8282\u7684\u6a21\u578b\u8bad\u7ec3\u51fd\u6570\u76f8\u6bd4\uff0c\u8fd9\u91cc\u7684\u6a21\u578b\u8bad\u7ec3\u51fd\u6570\u6709\u4ee5\u4e0b\u51e0\u70b9\u4e0d\u540c\uff1a\n\n1. \u4f7f\u7528\u56f0\u60d1\u5ea6\u8bc4\u4ef7\u6a21\u578b\u3002\n2. \u5728\u8fed\u4ee3\u6a21\u578b\u53c2\u6570\u524d\u88c1\u526a\u68af\u5ea6\u3002\n3. \u5bf9\u65f6\u5e8f\u6570\u636e\u91c7\u7528\u4e0d\u540c\u91c7\u6837\u65b9\u6cd5\u5c06\u5bfc\u81f4\u9690\u85cf\u72b6\u6001\u521d\u59cb\u5316\u7684\u4e0d\u540c\u3002\n\n<span class=\"graffiti-highlight graffiti-id_kasti5n-id_f9noday\"><i><\/i>\u4ee3\u7801\u8bb2\u89e3<\/span>","f7d853e5":"## \u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7684\u7b80\u4ecb\u5b9e\u73b0\n\n### \u5b9a\u4e49\u6a21\u578b\n\n\u6211\u4eec\u4f7f\u7528Pytorch\u4e2d\u7684`nn.RNN`\u6765\u6784\u9020\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u3002\u5728\u672c\u8282\u4e2d\uff0c\u6211\u4eec\u4e3b\u8981\u5173\u6ce8`nn.RNN`\u7684\u4ee5\u4e0b\u51e0\u4e2a<span class=\"graffiti-highlight graffiti-id_k40p6v8-id_h5roii2\"><i><\/i>\u6784\u9020\u51fd\u6570\u53c2\u6570<\/span>\uff1a\n\n* `input_size` - The number of expected features in the input x\n* `hidden_size` \u2013 The number of features in the hidden state h\n* `nonlinearity` \u2013 The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'\n* `batch_first` \u2013 If True, then the input and output tensors are provided as (batch_size, num_steps, input_size). Default: False\n\n\u8fd9\u91cc\u7684`batch_first`\u51b3\u5b9a\u4e86\u8f93\u5165\u7684\u5f62\u72b6\uff0c\u6211\u4eec\u4f7f\u7528\u9ed8\u8ba4\u7684\u53c2\u6570`False`\uff0c\u5bf9\u5e94\u7684\u8f93\u5165\u5f62\u72b6\u662f (num_steps, batch_size, input_size)\u3002\n\n`forward`\u51fd\u6570\u7684<span class=\"graffiti-highlight graffiti-id_k40p6v8-id_sqkpcqa\"><i><\/i>\u53c2\u6570<\/span>\u4e3a\uff1a\n\n* `input` of shape (num_steps, batch_size, input_size): tensor containing the features of the input sequence. \n* `h_0` of shape (num_layers * num_directions, batch_size, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.\n\n`forward`\u51fd\u6570\u7684<span class=\"graffiti-highlight graffiti-id_k40p6v8-id_yaaryhb\"><i><\/i>\u8fd4\u56de\u503c<\/span>\u662f\uff1a\n\n* `output` of shape (num_steps, batch_size, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t.\n* `h_n` of shape (num_layers * num_directions, batch_size, hidden_size): tensor containing the hidden state for t = num_steps.\n\n\u73b0\u5728\u6211\u4eec\u6784\u9020\u4e00\u4e2a`nn.RNN`\u5b9e\u4f8b\uff0c\u5e76\u7528\u4e00\u4e2a\u7b80\u5355\u7684<span class=\"graffiti-highlight graffiti-id_k40p6v8-id_clitci9\"><i><\/i>\u4f8b\u5b50<\/span>\u6765\u770b\u4e00\u4e0b\u8f93\u51fa\u7684\u5f62\u72b6\u3002","de7b630c":"\u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a\u5b8c\u6574\u7684\u57fa\u4e8e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7684\u8bed\u8a00\u6a21\u578b\u3002\n\n<span class=\"graffiti-highlight graffiti-id_m47aq8s-id_2egnshr\"><i><\/i>\u4ee3\u7801\u8bb2\u89e3<\/span>","bbf845a2":"\u6211\u4eec\u5148<span class=\"graffiti-highlight graffiti-id_157mdwx-id_1pn0b6r\"><i><\/i>\u6d4b\u8bd5<\/span>\u4e00\u4e0b`predict_rnn`\u51fd\u6570\u3002\u6211\u4eec\u5c06\u6839\u636e\u524d\u7f00\u201c\u5206\u5f00\u201d\u521b\u4f5c\u957f\u5ea6\u4e3a10\u4e2a\u5b57\u7b26\uff08\u4e0d\u8003\u8651\u524d\u7f00\u957f\u5ea6\uff09\u7684\u4e00\u6bb5\u6b4c\u8bcd\u3002\u56e0\u4e3a\u6a21\u578b\u53c2\u6570\u4e3a\u968f\u673a\u503c\uff0c\u6240\u4ee5\u9884\u6d4b\u7ed3\u679c\u4e5f\u662f\u968f\u673a\u7684\u3002","b310827f":"\u4f7f\u7528\u6743\u91cd\u4e3a\u968f\u673a\u503c\u7684\u6a21\u578b\u6765\u9884\u6d4b\u4e00\u6b21\u3002","be8eead3":"\u7c7b\u4f3c\u7684\uff0c\u6211\u4eec\u9700\u8981\u5b9e\u73b0\u4e00\u4e2a\u9884\u6d4b\u51fd\u6570\uff0c\u4e0e\u524d\u9762\u7684\u533a\u522b\u5728\u4e8e\u524d\u5411\u8ba1\u7b97\u548c\u521d\u59cb\u5316\u9690\u85cf\u72b6\u6001\u3002","f0b6a78d":"\u63a5\u4e0b\u6765\u91c7\u7528\u76f8\u90bb\u91c7\u6837\u8bad\u7ec3\u6a21\u578b\u5e76\u521b\u4f5c\u6b4c\u8bcd\u3002","c9c867a5":"# \u5faa\u73af\u795e\u7ecf\u7f51\u7edc\n\n\u672c\u8282\u4ecb\u7ecd<span class=\"graffiti-highlight graffiti-id_m8ksq1l-id_hp33wrv\"><i><\/i>\u5faa\u73af\u795e\u7ecf\u7f51\u7edc<\/span>\uff0c\u4e0b\u56fe\u5c55\u793a\u4e86\u5982\u4f55\u57fa\u4e8e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u8bed\u8a00\u6a21\u578b\u3002\u6211\u4eec\u7684\u76ee\u7684\u662f\u57fa\u4e8e\u5f53\u524d\u7684\u8f93\u5165\u4e0e\u8fc7\u53bb\u7684\u8f93\u5165\u5e8f\u5217\uff0c\u9884\u6d4b\u5e8f\u5217\u7684\u4e0b\u4e00\u4e2a\u5b57\u7b26\u3002\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5f15\u5165\u4e00\u4e2a\u9690\u85cf\u53d8\u91cf$H$\uff0c\u7528$H_{t}$\u8868\u793a$H$\u5728\u65f6\u95f4\u6b65$t$\u7684\u503c\u3002$H_{t}$\u7684\u8ba1\u7b97\u57fa\u4e8e$X_{t}$\u548c$H_{t-1}$\uff0c\u53ef\u4ee5\u8ba4\u4e3a$H_{t}$\u8bb0\u5f55\u4e86\u5230\u5f53\u524d\u5b57\u7b26\u4e3a\u6b62\u7684\u5e8f\u5217\u4fe1\u606f\uff0c\u5229\u7528$H_{t}$\u5bf9\u5e8f\u5217\u7684\u4e0b\u4e00\u4e2a\u5b57\u7b26\u8fdb\u884c\u9884\u6d4b\u3002\n\n![%E5%9B%BE%E7%89%87.png](attachment:%E5%9B%BE%E7%89%87.png)\n\n## \u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7684\u6784\u9020\n\n\u6211\u4eec\u5148\u770b\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7684<span class=\"graffiti-highlight graffiti-id_m8ksq1l-id_3yln3za\"><i><\/i>\u5177\u4f53\u6784\u9020<\/span>\u3002\u5047\u8bbe$\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}$\u662f\u65f6\u95f4\u6b65$t$\u7684\u5c0f\u6279\u91cf\u8f93\u5165\uff0c$\\boldsymbol{H}_t  \\in \\mathbb{R}^{n \\times h}$\u662f\u8be5\u65f6\u95f4\u6b65\u7684\u9690\u85cf\u53d8\u91cf\uff0c\u5219\uff1a\n\n$$\\boldsymbol{H}_t = \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh}  + \\boldsymbol{b}_h).$$\n\n\u5176\u4e2d\uff0c$\\boldsymbol{W}_{xh} \\in \\mathbb{R}^{d \\times h}$\uff0c$\\boldsymbol{W}_{hh} \\in \\mathbb{R}^{h \\times h}$\uff0c$\\boldsymbol{b}_{h} \\in \\mathbb{R}^{1 \\times h}$\uff0c$\\phi$\u51fd\u6570\u662f\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u3002\u7531\u4e8e\u5f15\u5165\u4e86$\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh}$\uff0c$H_{t}$\u80fd\u591f\u6355\u6349\u622a\u81f3\u5f53\u524d\u65f6\u95f4\u6b65\u7684\u5e8f\u5217\u7684\u5386\u53f2\u4fe1\u606f\uff0c\u5c31\u50cf\u662f\u795e\u7ecf\u7f51\u7edc\u5f53\u524d\u65f6\u95f4\u6b65\u7684\u72b6\u6001\u6216\u8bb0\u5fc6\u4e00\u6837\u3002\u7531\u4e8e$H_{t}$\u7684\u8ba1\u7b97\u57fa\u4e8e$H_{t-1}$\uff0c\u4e0a\u5f0f\u7684\u8ba1\u7b97\u662f\u5faa\u73af\u7684\uff0c\u4f7f\u7528\u5faa\u73af\u8ba1\u7b97\u7684\u7f51\u7edc\u5373\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08recurrent neural network\uff09\u3002\n\n\u5728\u65f6\u95f4\u6b65$t$\uff0c\u8f93\u51fa\u5c42\u7684\u8f93\u51fa\u4e3a\uff1a\n\n$$\\boldsymbol{O}_t = \\boldsymbol{H}_t \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q.$$\n\n\u5176\u4e2d$\\boldsymbol{W}_{hq} \\in \\mathbb{R}^{h \\times q}$\uff0c$\\boldsymbol{b}_q \\in \\mathbb{R}^{1 \\times q}$\u3002\n\n\n## \u4ece\u96f6\u5f00\u59cb\u5b9e\u73b0\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\n\n\u6211\u4eec\u5148\u5c1d\u8bd5\u4ece\u96f6\u5f00\u59cb\u5b9e\u73b0\u4e00\u4e2a\u57fa\u4e8e\u5b57\u7b26\u7ea7\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u8fd9\u91cc\u6211\u4eec\u4f7f\u7528\u5468\u6770\u4f26\u7684\u6b4c\u8bcd\u4f5c\u4e3a\u8bed\u6599\uff0c\u9996\u5148\u6211\u4eec\u8bfb\u5165\u6570\u636e\uff1a","1095bdb4":"### \u521d\u59cb\u5316\u6a21\u578b\u53c2\u6570\n\n<span class=\"graffiti-highlight graffiti-id_6nii7n3-id_4tnzas2\"><i><\/i>\u4ee3\u7801\u8bb2\u89e3<\/span>","ef221470":"\u6211\u4eec\u6bcf\u6b21\u91c7\u6837\u7684\u5c0f\u6279\u91cf\u7684\u5f62\u72b6\u662f\uff08\u6279\u91cf\u5927\u5c0f, \u65f6\u95f4\u6b65\u6570\uff09\u3002\u4e0b\u9762\u7684\u51fd\u6570\u5c06\u8fd9\u6837\u7684\u5c0f\u6279\u91cf\u53d8\u6362\u6210\u6570\u4e2a\u5f62\u72b6\u4e3a\uff08\u6279\u91cf\u5927\u5c0f, \u8bcd\u5178\u5927\u5c0f\uff09\u7684\u77e9\u9635\uff0c\u77e9\u9635\u4e2a\u6570\u7b49\u4e8e\u65f6\u95f4\u6b65\u6570\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u65f6\u95f4\u6b65$t$\u7684\u8f93\u5165\u4e3a$\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}$\uff0c\u5176\u4e2d$n$\u4e3a\u6279\u91cf\u5927\u5c0f\uff0c$d$\u4e3a\u8bcd\u5411\u91cf\u5927\u5c0f\uff0c\u5373one-hot\u5411\u91cf\u957f\u5ea6\uff08\u8bcd\u5178\u5927\u5c0f\uff09\u3002\n\n<span class=\"graffiti-highlight graffiti-id_eaelzjl-id_s9id9jl\"><i><\/i>\u4ee3\u7801\u8bb2\u89e3<\/span>","99801397":"### \u8bad\u7ec3\u6a21\u578b\u5e76\u521b\u4f5c\u6b4c\u8bcd\n\n\u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u8bad\u7ec3\u6a21\u578b\u4e86\u3002\u9996\u5148\uff0c\u8bbe\u7f6e\u6a21\u578b\u8d85\u53c2\u6570\u3002\u6211\u4eec\u5c06\u6839\u636e\u524d\u7f00\u201c\u5206\u5f00\u201d\u548c\u201c\u4e0d\u5206\u5f00\u201d\u5206\u522b\u521b\u4f5c\u957f\u5ea6\u4e3a50\u4e2a\u5b57\u7b26\uff08\u4e0d\u8003\u8651\u524d\u7f00\u957f\u5ea6\uff09\u7684\u4e00\u6bb5\u6b4c\u8bcd\u3002\u6211\u4eec\u6bcf\u8fc750\u4e2a\u8fed\u4ee3\u5468\u671f\u4fbf\u6839\u636e\u5f53\u524d\u8bad\u7ec3\u7684\u6a21\u578b\u521b\u4f5c\u4e00\u6bb5\u6b4c\u8bcd\u3002","faa6cef3":"\u4e0b\u9762\u91c7\u7528\u968f\u673a\u91c7\u6837\u8bad\u7ec3\u6a21\u578b\u5e76\u521b\u4f5c\u6b4c\u8bcd\u3002","b1f1bd28":"### one-hot\u5411\u91cf\n\n\u6211\u4eec\u9700\u8981\u5c06\u5b57\u7b26\u8868\u793a\u6210\u5411\u91cf\uff0c\u8fd9\u91cc\u91c7\u7528one-hot\u5411\u91cf\u3002\u5047\u8bbe\u8bcd\u5178\u5927\u5c0f\u662f$N$\uff0c\u6bcf\u6b21\u5b57\u7b26\u5bf9\u5e94\u4e00\u4e2a\u4ece$0$\u5230$N-1$\u7684\u552f\u4e00\u7684\u7d22\u5f15\uff0c\u5219\u8be5\u5b57\u7b26\u7684\u5411\u91cf\u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a$N$\u7684\u5411\u91cf\uff0c\u82e5\u5b57\u7b26\u7684\u7d22\u5f15\u662f$i$\uff0c\u5219\u8be5\u5411\u91cf\u7684\u7b2c$i$\u4e2a\u4f4d\u7f6e\u4e3a$1$\uff0c\u5176\u4ed6\u4f4d\u7f6e\u4e3a$0$\u3002\u4e0b\u9762\u5206\u522b\u5c55\u793a\u4e86\u7d22\u5f15\u4e3a0\u548c2\u7684one-hot\u5411\u91cf\uff0c\u5411\u91cf\u957f\u5ea6\u7b49\u4e8e\u8bcd\u5178\u5927\u5c0f\u3002\n\n<span class=\"graffiti-highlight graffiti-id_kryzk9x-id_8dcjyhw\"><i><\/i>\u4ee3\u7801\u8bb2\u89e3<\/span>","6aa89820":"\u63a5\u4e0b\u6765\u5b9e\u73b0\u8bad\u7ec3\u51fd\u6570\uff0c\u8fd9\u91cc\u53ea\u4f7f\u7528\u4e86\u76f8\u90bb\u91c7\u6837\u3002\n\n<span class=\"graffiti-highlight graffiti-id_3bm44z0-id_abdo8cr\"><i><\/i>\u4ee3\u7801\u8bb2\u89e3<\/span>","6bddc40a":"\u51fd\u6570init_rnn_state\u521d\u59cb\u5316\u9690\u85cf\u53d8\u91cf\uff0c\u8fd9\u91cc\u7684\u8fd4\u56de\u503c\u662f\u4e00\u4e2a\u5143\u7ec4\u3002\n\n<span class=\"graffiti-highlight graffiti-id_ap3wbu6-id_m2ba8i7\"><i><\/i>\u4ee3\u7801\u8bb2\u89e3<\/span>","c0cf5389":"### \u5b9a\u4e49\u6a21\u578b\n\n\u51fd\u6570`rnn`\u7528\u5faa\u73af\u7684\u65b9\u5f0f\u4f9d\u6b21\u5b8c\u6210\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u8ba1\u7b97\u3002\n\n<span class=\"graffiti-highlight graffiti-id_ge2je0t-id_eecwtjn\"><i><\/i>\u4ee3\u7801\u8bb2\u89e3<\/span>","4f7b2294":"### \u5b9a\u4e49\u9884\u6d4b\u51fd\u6570\n\n\u4ee5\u4e0b\u51fd\u6570\u57fa\u4e8e\u524d\u7f00`prefix`\uff08\u542b\u6709\u6570\u4e2a\u5b57\u7b26\u7684\u5b57\u7b26\u4e32\uff09\u6765\u9884\u6d4b\u63a5\u4e0b\u6765\u7684`num_chars`\u4e2a\u5b57\u7b26\u3002\u8fd9\u4e2a\u51fd\u6570\u7a0d\u663e\u590d\u6742\uff0c\u5176\u4e2d\u6211\u4eec\u5c06\u5faa\u73af\u795e\u7ecf\u5355\u5143`rnn`\u8bbe\u7f6e\u6210\u4e86\u51fd\u6570\u53c2\u6570\uff0c\u8fd9\u6837\u5728\u540e\u9762\u5c0f\u8282\u4ecb\u7ecd\u5176\u4ed6\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u65f6\u80fd\u91cd\u590d\u4f7f\u7528\u8fd9\u4e2a\u51fd\u6570\u3002\n\n<span class=\"graffiti-highlight graffiti-id_qz5cf4d-id_jssinja\"><i><\/i>\u4ee3\u7801\u8bb2\u89e3<\/span>","bf874fa7":"\u505a\u4e2a\u7b80\u5355\u7684<span class=\"graffiti-highlight graffiti-id_cnjj602-id_rv36ncp\"><i><\/i>\u6d4b\u8bd5<\/span>\u6765\u89c2\u5bdf\u8f93\u51fa\u7ed3\u679c\u7684\u4e2a\u6570\uff08\u65f6\u95f4\u6b65\u6570\uff09\uff0c\u4ee5\u53ca\u7b2c\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\u5c42\u8f93\u51fa\u7684\u5f62\u72b6\u548c\u9690\u85cf\u72b6\u6001\u7684\u5f62\u72b6\u3002","c4e76b79":"### \u88c1\u526a\u68af\u5ea6\n\n\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u4e2d\u8f83\u5bb9\u6613\u51fa\u73b0\u68af\u5ea6\u8870\u51cf\u6216\u68af\u5ea6\u7206\u70b8\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u7f51\u7edc\u51e0\u4e4e\u65e0\u6cd5\u8bad\u7ec3\u3002\u88c1\u526a\u68af\u5ea6\uff08clip gradient\uff09\u662f\u4e00\u79cd\u5e94\u5bf9\u68af\u5ea6\u7206\u70b8\u7684\u65b9\u6cd5\u3002\u5047\u8bbe\u6211\u4eec\u628a\u6240\u6709\u6a21\u578b\u53c2\u6570\u7684\u68af\u5ea6\u62fc\u63a5\u6210\u4e00\u4e2a\u5411\u91cf $\\boldsymbol{g}$\uff0c\u5e76\u8bbe\u88c1\u526a\u7684\u9608\u503c\u662f$\\theta$\u3002\u88c1\u526a\u540e\u7684\u68af\u5ea6\n\n$$ \\min\\left(\\frac{\\theta}{\\|\\boldsymbol{g}\\|}, 1\\right)\\boldsymbol{g}$$\n\n\u7684$L_2$\u8303\u6570\u4e0d\u8d85\u8fc7$\\theta$\u3002\n\n<span class=\"graffiti-highlight graffiti-id_zrzqvc3-id_qxl3pwc\"><i><\/i>\u4ee3\u7801\u8bb2\u89e3<\/span>","a6d605f3":"\u8bad\u7ec3\u6a21\u578b\u3002"}}