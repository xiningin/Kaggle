{"cell_type":{"e9b0bc8c":"code","f77da070":"code","537036fc":"code","35b43ac1":"code","2b4c7fe1":"code","7712fb25":"code","ffb69f21":"code","a0604d01":"code","09e1e1c0":"code","64a541a9":"code","21bd8f51":"code","6e7f9063":"code","8ba613a7":"code","3deecf6c":"code","73d7ae83":"code","e2cb97b3":"code","ab7d502e":"code","425bf959":"markdown","f0dd23f3":"markdown","a9e6206f":"markdown","d01586d1":"markdown","ae3faa25":"markdown","4aad5e6b":"markdown","b2b6bc59":"markdown","731ad8af":"markdown"},"source":{"e9b0bc8c":"import os\nimport tensorflow as tf\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport PIL\nimport random\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D, Dense, MaxPool2D, Flatten, LeakyReLU, BatchNormalization, Dropout, Input\n\n%matplotlib inline\n\nbase_path = '\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/'\ntest_path = os.path.join(base_path,'test')\ntrain_path = os.path.join(base_path,'train')\nval_path = os.path.join(base_path, 'val')\n\ntry:\n    os.mkdir(\"\/kaggle\/working\/models\")\nexcept:\n    pass","f77da070":"def visualize_images(dir_path):\n        images = os.listdir(dir_path)\n        images = [img for img in images if img.endswith('jpeg')]\n        fig = plt.figure(figsize=(24,24))\n        for i in range(3):\n            for j in range(3):\n                img = random.choice(images)\n                img = PIL.Image.open(os.path.join(dir_path,img))\n                axobj = fig.add_subplot(3, 3, i * 3 + j + 1)\n                axobj.imshow(img)","537036fc":"visualize_images(os.path.join(test_path, 'PNEUMONIA'))","35b43ac1":"visualize_images(os.path.join(test_path, 'NORMAL'))","2b4c7fe1":"IMAGE_SIZE=(224,224)\nBATCH_SIZE=64\n\ntrain_datagen  = ImageDataGenerator(\n    rescale=1.\/255,\n    zoom_range=0.2,\n    horizontal_flip=True\n)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_path,\n    target_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='binary')\n\ntest_datagen  = ImageDataGenerator(\n    rescale=1.\/255,\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_path,\n    target_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='binary')","7712fb25":"simple_model_leaky = tf.keras.models.Sequential([\n    Input(shape=(224, 224, 3,)),\n    \n    Conv2D(64, 5, 2),\n    LeakyReLU(),\n    MaxPool2D(2),\n    #BatchNormalization(),\n    #Dropout(),\n\n    Conv2D(128, 3, 2),\n    LeakyReLU(),\n    MaxPool2D(2),\n    #BatchNormalization(),\n    #Dropout(),\n\n    Conv2D(256, 3, 1, padding='same'),\n    LeakyReLU(),\n    MaxPool2D(2),\n    #BatchNormalization(),\n    #Dropout(),\n\n    Conv2D(512, 3, 1, padding='same'),\n    LeakyReLU(),\n    MaxPool2D(2),\n    #BatchNormalization(),\n    #Dropout(),\n    \n    Flatten(),\n    Dense(128),\n    LeakyReLU(),\n    Dense(64),\n    LeakyReLU(),\n    Dense(1, activation='sigmoid')    \n])\n\nsimple_model_leaky.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n\nsimple_model_leaky_bachnorm = tf.keras.models.Sequential([\n    Input(shape=(224, 224, 3,)),\n    \n    Conv2D(64, 5, 2),\n    LeakyReLU(),\n    MaxPool2D(2),\n    BatchNormalization(),\n    Dropout(0.2),\n\n    Conv2D(128, 3, 2),\n    LeakyReLU(),\n    MaxPool2D(2),\n    BatchNormalization(),\n    Dropout(0.2),\n\n    Conv2D(256, 3, 1, padding='same'),\n    LeakyReLU(),\n    MaxPool2D(2),\n    BatchNormalization(),\n\n    Conv2D(512, 3, 1, padding='same'),\n    LeakyReLU(),\n    MaxPool2D(2),\n    BatchNormalization(),\n    \n    Flatten(),\n    Dense(128),\n    LeakyReLU(),\n    Dense(64),\n    LeakyReLU(),\n    Dense(1, activation='sigmoid')    \n])\n\nsimple_model_leaky_bachnorm.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')","ffb69f21":"def prepare_model(model, input_shape=(224, 224, 3), optimizer='adam'):\n    pre_model = model(input_shape=input_shape,\n                 include_top=False,\n                 weights='imagenet')\n    \n    for layer in pre_model.layers:\n        layer.trainable = False\n        \n    last_out = pre_model.layers[-1].output\n    x = Flatten()(last_out) \n    x = Dense(512, activation='relu')(x)\n    x = Dense(1, activation='sigmoid')(x)\n    \n    model = tf.keras.Model(pre_model.input, x)\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics='accuracy')\n    \n    return model   ","a0604d01":"def train_model(model, name, train, test, epochs=50):\n    callbacks =[] \n    callbacks.append(tf.keras.callbacks.EarlyStopping(patience=5))\n    callbacks.append(tf.keras.callbacks.ModelCheckpoint(os.path.join('\/kaggle\/working\/models', name), save_best_only=True))\n    \n    history = model.fit(train, validation_data=test, epochs=epochs, callbacks=callbacks)\n    \n    return model, name, history","09e1e1c0":"models_to_prepare = [\n    (tf.keras.applications.inception_v3.InceptionV3, 'inception'),\n    (tf.keras.applications.ResNet50, 'resnet'),\n    (tf.keras.applications.vgg16.VGG16, 'vgg'),\n    (tf.keras.applications.xception.Xception, 'xception'),\n]\n\nmodels = [(prepare_model(model[0]), model[1]) for model in models_to_prepare]\n\nmodels += [\n    (simple_model_leaky_bachnorm, 'leaky_bachnorm'),\n    (simple_model_leaky, 'leaky'),\n]","64a541a9":"trained_models = []\nhistories = []\nfor model in models:\n    model, name, history = train_model(model[0], model[1], train_generator, test_generator, epochs=50) \n    trained_models.append(model)\n    histories.append((name, history))","21bd8f51":"def present_training_results(histories):\n    length = len(histories)\n    fig = plt.figure(figsize=(24, 4 * length))\n    for i, j in enumerate(histories):\n        name, history = j\n        history = history.history\n        for k, key in enumerate(history.keys()):\n            axobj = fig.add_subplot(length, 4, 4 * i + k + 1)\n            axobj.plot(history[key], label=(key + '_' + name))\n            axobj.legend()\n            if 'acc' in key:\n                axobj.set_ylim((0.5 ,1))","6e7f9063":"present_training_results(histories)","8ba613a7":"for j in histories:\n    name, history = j\n    history = history.history\n    print(name, max(history['val_accuracy']))","3deecf6c":"val_datagen  = ImageDataGenerator(\n    rescale=1.\/255,\n)\n\nval_generator = test_datagen.flow_from_directory(\n    val_path,\n    target_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='binary')","73d7ae83":"def evaluate_model(name, data):\n    model = tf.keras.models.load_model(os.path.join('\/kaggle\/working\/models', name))\n    model.evaluate(data)","e2cb97b3":"evaluate_model('leaky', val_generator)","ab7d502e":"evaluate_model('vgg', val_generator)","425bf959":"### Now that train results are known let's evaluate 2 the best model on val data","f0dd23f3":"- inception 0.8862179517745972\n- resnet 0.8814102411270142\n- vgg 0.9262820482254028\n- xception 0.8958333134651184\n- leaky_bachnorm 0.8060897588729858\n- leaky 0.9214743375778198","a9e6206f":"### Visualize some training images","d01586d1":"<img src=\"https:\/\/idsb.tmgrup.com.tr\/ly\/uploads\/images\/2020\/11\/12\/71822.jpg\" alt=\"drawing\" height=\"600\" width=\"600\"\/>\n\n# **Pneumonia is an inflammatory condition of the lung primarily affecting the small air sacs known as alveoli.**\n### Symptoms typically include some combination of productive or dry cough, chest pain, fever and difficulty breathing.The severity of the condition is variable. Pneumonia is usually caused by infection with viruses or bacteria, and less commonly by other microorganisms. Identifying the responsible pathogen can be difficult. Diagnosis is often based on symptoms and physical examination. Chest X-rays, blood tests, and culture of the sputum may help confirm the diagnosis.\n\n### In this notebook I will try to automate the diagnosis task by trying multiple CNN architectures.","ae3faa25":"### Images have different sizes and it will have to be unified","4aad5e6b":"### Try mulitple models to see which gives the best acc for a validation set\n- Simple model with 4 conv layers (do some experimests, different activations, add dropout, add batchnorm, adjust learning rate etc)\n- Xception tranfer learning\n- ResNet tranfer learning\n- VGG16 transfer learning\n- Inception transfer learning","b2b6bc59":"# Conclusions\n\n### With a very simple approach final model (VGG) gained almost 94% accuracy on validation data. As a next step to improve the result, layers on top of vgg could be modified(I could add another layer, or experiment with number of neurons) and learning rate could be tuned.\n\n### I also believe that experimenting with own architecture DNN could bring even better results. In the next version of this notebook I will try to build such network by trying to find optimal actication function, learning rate and better architecutre.\n\nEvery feedback is more than welcome. \n\n","731ad8af":"### Create data generators"}}