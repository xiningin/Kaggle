{"cell_type":{"7b02aabe":"code","592c2bc4":"code","0e100169":"code","8cef5389":"code","1c44aded":"code","a4491092":"code","17a170f3":"code","a3dc16d7":"code","b04d72a0":"code","9f0c2e55":"code","7f6e2113":"code","4bf23a74":"code","e6adb641":"code","5773bac6":"code","b9387566":"code","cdbb21a3":"code","4003521f":"code","a01daf97":"code","93af4713":"code","a9f770d7":"code","bd3c24bb":"code","cc82d8c9":"code","2150f9e0":"code","533fc2e1":"code","3f456042":"markdown","a986d1d7":"markdown","0b975c30":"markdown","8394517b":"markdown","0dff7e36":"markdown","41af63cf":"markdown","aa9e9073":"markdown","481cb8cf":"markdown","cc4f4ee6":"markdown"},"source":{"7b02aabe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","592c2bc4":"from kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\n\n(market_train_df, news_train_df) = env.get_training_data()\n","0e100169":"#beginning data exploration\n\n#check columns\ncol_list_1 = market_train_df.columns\ncol_list_2 = news_train_df.columns\n\nprint(\"market_train has {} rows and {} columns. \\nnews_train has {} rows and {} columns.\"\\\n      .format(market_train_df.shape[0], market_train_df.shape[1], news_train_df.shape[0], news_train_df.shape[1]))\n\ncol_list_1 = [i for i in market_train_df.columns.values]\ncol_list_2 = [i for i in news_train_df.columns.values]\nprint(\"Columns of market_train_df:\\n\", \",\\n\".join(a for a in col_list_1), \\\n      \"\\n\\n\\nColumns of news_train_df:\\n\",\",\\n\".join(a for a in col_list_2) ,\"\\n\\n\")\n\n#check if the particular column has NaN\/empty data\nmarket_cols_hasnan = {column:market_train_df[column].isnull().any() for column in col_list_1}\nnews_cols_hasnan = {column:news_train_df[column].isnull().any() for column in col_list_2}\ncols_null_market = [i for i in market_cols_hasnan.keys() if market_cols_hasnan.get(i)]\ncols_null_news = [i for i in news_cols_hasnan.keys() if news_cols_hasnan.get(i)]\nprint(\"Columns with NaN in market_train_df:\\n\",cols_null_market,\"\\n\\t There are \",len(cols_null_market),\" columns with null.\")\nprint(\"Columns with NaN in news_train_df:\\n\",cols_null_news,\"\\n\\t There are \",len(cols_null_news),\" columns with null.\")\n","8cef5389":"#print a single row of each for visual familiarity of the data\nprint(\"First row of market_train_df\\n\",market_train_df.iloc[0])\nprint(\"First row of news_train_df\\n\",news_train_df.iloc[0])\n","1c44aded":"#makes copies of the 2 dataframes \na_market_train_df = market_train_df.copy()\na_news_train_df = news_train_df.copy()\n","a4491092":"\ndef list_cols_type(df, verbose=True): #list all column names with their datatypes\n    s = set(zip([e for e in df.columns.values],[df[i].dtype.name for i in df.columns.values ]))\n    for a in s:\n        print(a)\n    return s \n\nprint(\"market_dataframe_follows:\")\nlist_cols_type(a_market_train_df)\nprint(\"\\nnews_dataframe_follows:\")\nlist_cols_type(a_news_train_df)\n","17a170f3":"def list_categorical_columns(df, verbose=True):\n    #names of different dtypes that may be \"categorical\" or \"categoricalish\"(sorry)\n    list_categorical_dtype_names = ['datetime64[ns, UTC]', 'category', 'object']\n    cat_col_names = [(i, df[i].dtype.name) for i in df.columns.values if df[i].dtype.name in list_categorical_dtype_names]\n    if(verbose):\n        print(cat_col_names)\n    return cat_col_names\n\nprint(\"market df follows:\")\nlist_categorical_columns(a_market_train_df)\nprint(\"\\nnews df follows:\")\nlist_categorical_columns(a_news_train_df)","a3dc16d7":"#check the null values per column in the 2 dataframes\n#this is to help decide if the column should be dropped or nulls autopopulated by the proxy value\n\n#type(a_market_train_df.groupby('time')) #>>pandas.core.groupby.groupby.DataFrameGroupBy\n\ndef list_cols_with_null(df):\n    A = [a for a in df.columns.values if df[a].isnull().sum()>0]\n    #TODO: Try to do the following using dictionary comprehension\n    B = {} #Dictionary holding (column_name:percentage of null values)\n    for a in A:\n        num_null = df[a].isnull().sum()\n        total_count = len(df[a])\n        b = float(num_null\/total_count)\n        B[a] = b\n    return B\n\nA = list_cols_with_null(a_market_train_df)\nB = list_cols_with_null(a_news_train_df)\n\nprint(\"market_df column null percentages:\")\nprint(A)\nprint(\"\\nnews_df column null percentages:\")\nprint(B)","b04d72a0":"#Join the dataframes into a combined dataframe - 0\n\n#perform the necessary cleaning to the market & news dataframes\n#merge them into a single dataframe thereafter\ndef clean_data_fields(market_df, news_df, verbose=False):\n    #the timestamps are string, convert them to int\n    market_df['time'] = market_df.time.dt.strftime(\"%Y%m%d\").astype(int)#convert datetime to simple int\n    news_df['time'] = news_df.time.dt.strftime(\"%Y%m%d\").astype(int)\n    #news df has a list of 'assetCodes', create a column 'assetCode' with the first assetCode in assetCodes\n    #drop the column assetCodes thereafter\n    news_df['assetCode'] = news_df['assetCodes'].map(lambda x: list(eval(x))[0])#from: https:\/\/www.kaggle.com\/rabaman\/0-64-in-100-lines\n    news_df.drop(['assetCodes'], axis=1, inplace=True)\n    #market df has opening and closing prices, create a column with the average prices\n    market_df['average'] = (market_df['close'] + market_df['open'])\/2\n    #columns to be dropped from the market & news dataframes\n    cols_dropped_market = ['assetName','universe']\n                        #['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10',\\\n                        #'returnsOpenPrevMktres10','universe'] #,'returnsOpenNextMktres10'\n    cols_dropped_news   = ['noveltyCount12H','noveltyCount24H','noveltyCount3D','noveltyCount5D','noveltyCount7D',\\\n                       'volumeCounts12H','volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D', \\\n                       'sourceTimestamp', 'sourceId', 'takeSequence', 'headline', 'firstCreated', 'provider', \\\n                       'audiences', 'firstMentionSentence', 'wordCount', 'headlineTag', 'bodySize', 'companyCount',\\\n                       'marketCommentary','sentenceCount', 'subjects', 'assetName']#last 2 added\n    #drop columns\n    if(verbose):\n        print(\"a_market_train_df has {} columns, pre-drop\".format(market_df.shape[1]))\n        print(\"a_news_train_df has {} columns, pre-drop\".format(news_df.shape[1]))\n    market_df = market_df.drop(cols_dropped_market, axis=1)\n    news_df = news_df.drop(cols_dropped_news, axis=1)\n    if(verbose):\n        print(\"a_market_train_df has {} columns, post-drop\".format(market_df.shape[1]))\n        print(\"a_news_train_df has {} columns, post-drop\".format(news_df.shape[1]))\n        print(\"---\")\n        print(\"a_market_train_df has {:,} rows and {} columns.\".format(market_df.shape[0],market_df.shape[1]))\n        print(\"a_news_train_df has {:,} rows and {} columns.\".format(news_df.shape[0],news_df.shape[1]))\n    #aggregate news_df and \n    news_df = news_df.groupby(['time', 'assetCode'], sort=False).aggregate(np.mean).reset_index()\n    #merge merket & news dfs into a simgle df\n    #unified_df = pd.merge(market_df, news_df, how=\"inner\", on=['time', 'assetCode'], copy=False)\n    unified_df = pd.merge(market_df, news_df, how=\"left\", on=['time', 'assetCode'], copy=False)\n    return unified_df\n\nunified_df = clean_data_fields(a_market_train_df, a_news_train_df)\nprint(\"done.\")\n\n\n    ","9f0c2e55":"import gc\n#clear mem\ndel a_market_train_df, market_train_df, news_train_df, a_news_train_df\n\ngc.collect()","7f6e2113":"#check if unified df has any null fields\nprint(list_cols_with_null(unified_df))\n#get list of columns with null\n#A = [a for a in unified_df.columns.values if unified_df[a].isnull().sum()>0]\nprint(unified_df.shape)\nprint(unified_df.iloc[0])","4bf23a74":"import gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#explore time column a bit, of the unified\/merged DataFrame\ntime_unique = unified_df['time'].nunique()\nnum_rows = unified_df['time'].shape[0]\nprint(\"The Time column in unified_df has a total of {} rows which contain {} unique values\".format(num_rows, time_unique))\n\n","e6adb641":"num_rows = unified_df.shape[0]\nnum_rows_train = int(0.7 * num_rows) #\nnum_rows_test = num_rows - num_rows_train\nprint(\"There should be {} training rows and {} val. rows \".format(num_rows_train, num_rows_test))\n\nu_train_df = unified_df.iloc[:num_rows_train,:]\nu_test_df = unified_df.iloc[num_rows_train:,:]\nprint(\"Training set has {} rows while val set has {} rows\".format(u_train_df.shape[0], u_test_df.shape[0] ))","5773bac6":"import gc\n#check shapes of derived dataframes to ensure all seems well\nprint(u_train_df.shape)\nprint(u_test_df.shape)\nprint(u_train_df.shape[0]+u_test_df.shape[0])\nprint(unified_df.shape)\n\n#delete unified_df to save memory\ndel unified_df\ngc.collect()\nprint(\"done.\")","b9387566":"#Split the derived datasets into training and target col.sets.\n#strip off y which is 'returnsOpenNextMktres10'\ny_label = 'returnsOpenNextMktres10' #training target\ny_train_df = u_train_df[y_label]\ny_test_df = u_test_df[y_label]\nu_train_df.drop(columns=y_label, inplace=True)\nu_test_df.drop(columns=y_label, inplace=True)\nprint(\" u_train_df.shape == {}, y_train_df.shape == {}\".format(u_train_df.shape, y_train_df.shape))\nprint(\" u_test_df.shape == {}, y_test_df.shape == {}\".format(u_test_df.shape, y_test_df.shape))\n","cdbb21a3":"#train a model\n#import lightgbm as lgb\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nimport matplotlib.pyplot as plt\nfrom itertools import chain\n\n%matplotlib inline\n\ncategorical_cols = ['assetCode']\ncategorical_cols_index = [1]\ncategorical_cols_index_str = '1'\n\nu_train_df_ = u_train_df.copy()\n\n#x_train_1_ = x_train_1.copy()\n#x_train_2_ = x_train_2.copy()\n#\n#x_train_1_.drop(categorical_cols,axis=1,inplace=True)\n#x_train_2_.drop(categorical_cols,axis=1,inplace=True)","4003521f":"#get training columns list\nu_train_df_.columns.tolist()\n#x_train_1_.columns.tolist()\ntraining_columns = [\n 'close',\n 'open',\n 'returnsClosePrevRaw1',\n 'returnsOpenPrevRaw1',\n 'returnsClosePrevMktres1',\n 'returnsOpenPrevMktres1',\n 'returnsClosePrevRaw10',\n 'returnsOpenPrevRaw10',\n 'returnsClosePrevMktres10',\n 'returnsOpenPrevMktres10',\n 'average',\n 'urgency',\n 'relevance',\n 'sentimentClass',\n 'sentimentNegative',\n 'sentimentNeutral',\n 'sentimentPositive',\n 'sentimentWordCount']","a01daf97":"import time\nimport lightgbm as lgb\n#%%time\nlgb_params = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary', #'binary' 'regression'\n    'metric': {'binary_logloss'}, # 'l2', 'l1', 'logloss'\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'seed': 42,\n    'verbose': 50\n}\ntime_start = time.time()\n\n#below is without categorical data\n#check if any categorical data except 'assetCode'\n#print(\"1---\")\ntry:\n    #rerunning this cell becomes a problem as attempt is made to drop already dropped columns...\n    #hence this try catch block\n    u_train_df.drop(columns=['assetCode'], inplace=True)\nexcept KeyError:\n    print(\"There was a KeyError at u_train_df\")\ntry:\n    #rerunning this cell becomes a problem as attempt is made to drop already dropped columns...\n    #hence this try catch block\n    u_test_df.drop(columns=['assetCode'], inplace=True)\nexcept KeyError:\n    print(\"There was a KeyError at u_test_df\")\nlist_categorical_columns(u_train_df)\n#print(\"2---\")\nx_train = lgb.Dataset(u_train_df.values, y_train_df,feature_name=u_train_df.columns.tolist())\n#print(\"3---\")\nx_valid = lgb.Dataset(u_test_df.values, y_test_df,feature_name=u_test_df.columns.tolist(), reference=x_train)\nevals_result = {}#store evaluation results\n#print(\"4---\")\nmodel01 = lgb.train(lgb_params, x_train, num_boost_round=1000, \\\n                    valid_sets=[x_train, x_valid], valid_names=['eval1','eval2'], \\\n                    evals_result=evals_result, verbose_eval=50 )\ntime_end   = time.time()","93af4713":"print(\"The work took {:2.4f} minutes\".format((time_end-time_start)\/60))","a9f770d7":"\n_ = lgb.plot_metric(evals_result)\n_ = lgb.plot_importance(model01)\n","bd3c24bb":"#Submission Process\n# You can only iterate through a result from `get_prediction_days()` once\n# so be careful not to lose it once you start iterating.\ndays = env.get_prediction_days()\nprint(\"done.\")","cc82d8c9":"def clean_data_fields_forPreds(market_df, news_df, verbose=False):\n    #the timestamps are string, convert them to int\n    #already int date only#market_df['time'] = market_df.time.dt.strftime(\"%Y%m%d\").astype(int)#convert datetime to simple int\n    #already int date only#news_df['time'] = news_df.time.dt.strftime(\"%Y%m%d\").astype(int)\n    market_df['time'] = market_df.time.astype(int)#convert datetime to simple int\n    news_df['time'] = news_df.time.astype(int)#convert datetime to simple int\n    \n    if 'assetCodes' in news_df.columns.values:\n        #news df has a list of 'assetCodes', create a column 'assetCode' with the first assetCode in assetCodes\n        #drop the column assetCodes thereafter\n        news_df['assetCode'] = news_df['assetCodes'].map(lambda x: list(eval(x))[0])#from: https:\/\/www.kaggle.com\/rabaman\/0-64-in-100-lines\n        news_df.drop(['assetCodes'], axis=1, inplace=True)\n    \n    \n    #market df has opening and closing prices, create a column with the average prices\n    market_df['average'] = (market_df['close'] + market_df['open'])\/2\n    #columns to be dropped from the market & news dataframes\n    cols_dropped_market = ['assetName'] #,'universe'\n                        #['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10',\\\n                        #'returnsOpenPrevMktres10','universe'] #,'returnsOpenNextMktres10'\n    cols_dropped_news   = ['noveltyCount12H','noveltyCount24H','noveltyCount3D','noveltyCount5D','noveltyCount7D',\\\n                       'volumeCounts12H','volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D', \\\n                       'sourceTimestamp', 'sourceId', 'takeSequence', 'headline', 'firstCreated', 'provider', \\\n                       'audiences', 'firstMentionSentence', 'wordCount', 'headlineTag', 'bodySize', 'companyCount',\\\n                       'marketCommentary','sentenceCount', 'subjects', 'assetName']#last 2 added\n    #drop columns\n    if(verbose):\n        print(\"a_market_train_df has {} columns, pre-drop\".format(market_df.shape[1]))\n        print(\"a_news_train_df has {} columns, pre-drop\".format(news_df.shape[1]))\n    market_df = market_df.drop(cols_dropped_market, axis=1)\n    news_df = news_df.drop(cols_dropped_news, axis=1)\n    if(verbose):\n        print(\"a_market_train_df has {} columns, post-drop\".format(market_df.shape[1]))\n        print(\"a_news_train_df has {} columns, post-drop\".format(news_df.shape[1]))\n        print(\"---\")\n        print(\"a_market_train_df has {:,} rows and {} columns.\".format(market_df.shape[0],market_df.shape[1]))\n        print(\"a_news_train_df has {:,} rows and {} columns.\".format(news_df.shape[0],news_df.shape[1]))\n    #aggregate news_df and \n    news_df = news_df.groupby(['time', 'assetCode'], sort=False)\\\n                     .aggregate(np.mean)\\\n                     .reset_index()\n    #merge merket & news dfs into a simgle df\n    #unified_df = pd.merge(market_df, news_df, how=\"inner\", on=['time', 'assetCode'], copy=False)\n    unified_df = pd.merge(market_df, news_df, how=\"left\", on=['time', 'assetCode'], copy=False)\n    return unified_df\nprint(\"done.\")","2150f9e0":"#print(x_train_1_.columns.values,\"\\n\",market_obs_df.columns.values,\"\\n\",news_obs_df.columns.values,\"\\n\")\n#list_col_allowed = x_train_1_.columns.values\nlist_col_allowed = u_train_df.columns.values\ndef cols_needing_dropping(list_cols):\n    return [a for a in list_cols if a not in list_col_allowed]\ntarget_cols = ['returnsOpenNextMktres10']\n#current_df = clean_data_fields_forPreds(market_obs_df, news_obs_df, verbose=False)\nprint(\"done\")","533fc2e1":"for market_obs_df, news_obs_df, predictions_template_df in days:\n    current_df = clean_data_fields_forPreds(market_obs_df, news_obs_df, verbose=False)\n    current_df_ = current_df[list_col_allowed]\n    print(current_df.shape,\"-----\",current_df_.shape)\n    predictions = ((model01.predict(current_df_) * 2) - 1)\n    predictions_df = pd.DataFrame({'assetCode':current_df['assetCode'], '_confidence':predictions})\n    predictions_template_df = predictions_template_df.merge(predictions_df, how='left')\\\n                                                     .drop('confidenceValue', axis=1)\\\n                                                     .fillna(0)\\\n                                                     .rename(columns={'_confidence':'confidenceValue'})\n    print(predictions_df.head())\n    print(predictions_template_df.head())\n    env.predict(predictions_template_df)    \nenv.write_submission_file()\n","3f456042":"Beginning...\n\n\"From every dingy basement on every dingy street<br>\nEvery dragging hand clap over every dragging beat<br>\nThat's just the beat of time the beat that must go on<br>\nIf you've been trying for years we already heard your song\"<br>\n                                                                - lyrics of the song 'Death or Glory', The Clash","a986d1d7":"Sources::\nhttps:\/\/www.kaggle.com\/bguberfain\/a-simple-model-using-the-market-and-news-data\nhttp:\/\/mlexplained.com\/2018\/01\/05\/lightgbm-and-xgboost-explained\/\nhttps:\/\/martin-thoma.com\/pandas-merge-join-concatenate\/\nhttps:\/\/chartio.com\/resources\/tutorials\/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe\/\nhttps:\/\/docs.python.org\/3.1\/library\/re.html\nhttps:\/\/pyformat.info\/\nhttps:\/\/stackoverflow.com\/questions\/15769246\/pythonic-way-to-print-list-items\nhttps:\/\/docs.python.org\/3\/tutorial\/datastructures.html\nhttps:\/\/docs.python.org\/3\/tutorial\/datastructures.html#dictionaries\nhttps:\/\/stackoverflow.com\/questions\/29530232\/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe\nhttps:\/\/stackoverflow.com\/questions\/14507591\/python-dictionary-comprehension\nhttps:\/\/www.digitalocean.com\/community\/tutorials\/understanding-list-comprehensions-in-python-3\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.drop.html\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/merging.html\nhttps:\/\/ipython.org\/ipython-doc\/3\/interactive\/magics.html\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.merge.html\nhttps:\/\/www.kaggle.com\/jagangupta\/memory-optimization-and-eda-on-entire-dataset\nhttps:\/\/www.kaggle.com\/kunalkotian\/easily-load-train-csv-w-o-crash-save-feather-file\nhttps:\/\/stackoverflow.com\/questions\/39662149\/pandas-extract-date-and-time-from-timestamp\nhttps:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters-Tuning.html\nhttps:\/\/github.com\/Microsoft\/LightGBM\/blob\/master\/examples\/python-guide\/simple_example.py\nhttps:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html\nhttps:\/\/www.kaggle.com\/bguberfain\/a-simple-model-using-the-market-and-news-data\nhttps:\/\/www.kaggle.com\/artgor\/eda-feature-engineering-and-everything\nhttps:\/\/www.kaggle.com\/richardgg93\/two-sigma-news-first-try\nhttps:\/\/www.kaggle.com\/chocozzz\/two-sigma-news-simple-eda-prophet-nlp\nhttps:\/\/www.kaggle.com\/smasar\/tutorial-timeseriesapproach\nhttps:\/\/www.kaggle.com\/archermo\/twosigma-day1\nhttps:\/\/www.kaggle.com\/dansbecker\/using-categorical-data-with-one-hot-encoding\nhttps:\/\/github.com\/Microsoft\/LightGBM\/blob\/master\/docs\/Advanced-Topics.rst\nhttps:\/\/github.com\/Microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst#categorical_feature\nhttps:\/\/www.kaggle.com\/ogrellier\/good-fun-with-ligthgbm\/code\nhttps:\/\/stackoverflow.com\/questions\/47370240\/multiclass-classification-with-lightgbm\nhttps:\/\/towardsdatascience.com\/catboost-vs-light-gbm-vs-xgboost-5f93620723db\nhttps:\/\/lightgbm.readthedocs.io\/en\/latest\/Python-API.html\nhttps:\/\/www.avanwyk.com\/an-overview-of-lightgbm\/\nhttp:\/\/www.chioka.in\/differences-between-the-l1-norm-and-the-l2-norm-least-absolute-deviations-and-least-squares\/\nhttps:\/\/www.kaggle.com\/rabaman\/0-64-in-100-lines\nhttp:\/\/dylan-chen.com\/model\/lightgbm-tutorial\/\nhttps:\/\/www.kaggle.com\/dmdm02\/complete-eda-voting-lightgbm\nhttps:\/\/www.kaggle.com\/kazuokiriyama\/tuning-hyper-params-in-lgbm-achieve-0-66-in-lb\n\n\n","0b975c30":"from sklearn.model_selection import train_test_split\n\n#verbose=True\n#split the unified df into different sets for validation\nx_train ,x_test = train_test_split(unified_df,test_size=0.2,random_state=42) #Leave 20% of data for final testing\nx_train_1 ,x_train_2 = train_test_split(x_train,test_size=0.3,random_state=42) #Split training set to 70% & 30% split\nprint(\"x_train has {:,} rows and {} cols\".format(x_train.shape[0], x_train.shape[1]))\nprint(\"x_test has {:,} rows and {} cols\".format(x_test.shape[0], x_test.shape[1]))\nprint(\"x_train_1 has {:,} rows and {} cols\".format(x_train_1.shape[0], x_train_1.shape[1]))\nprint(\"x_train_2 has {:,} rows and {} cols\".format(x_train_2.shape[0], x_train_2.shape[1]))\n#strip off y which is 'returnsOpenNextMktres10'\ny_label = 'returnsOpenNextMktres10' #training target\ny_train_1 = x_train_1[y_label]\ny_train_2 = x_train_2[y_label]\nx_train_1.drop(columns=y_label, inplace=True)\nx_train_2.drop(columns=y_label, inplace=True)\nprint(\"---\\n x_train_1.shape == {}, y_train_1.shape == {}\".format(x_train_1.shape, y_train_1.shape))\n","8394517b":"(market_obs_df, news_obs_df, predictions_template_df) = next(days)\n","0dff7e36":"#print(current_df_.shape)\n#print(current_df_.head())\n#env.write_submission_file()\nprint(market_obs_df.columns.values)\n#list(set(market_obs_df['assetCode']) & set(news_obs_df['assetCode']))\nprint(len(list(set(market_obs_df['assetCode']))))\nprint(len(list(set(news_obs_df['assetCode']))))\n#print(len(list( set(market_obs_df['assetCode', 'time']) & set(news_obs_df['assetCode', 'time'])  )))","41af63cf":"predictions = ((model01.predict(current_df_) * 2) - 1)\npredictions_df = pd.DataFrame({'assetCode':current_df['assetCode'], '_confidence':predictions})\npredictions_template_df = predictions_template_df.merge(predictions_df, how='left')\\\n                                                 .drop('confidenceValue', axis=1)\\\n                                                 .fillna(0)\\\n                                                 .rename(columns={'_confidence':'confidenceValue'})\nprint(predictions_df.head())\nprint(predictions_template_df.head())\nenv.predict(predictions_template_df)","aa9e9073":"print(current_df.columns.values)\ncurrent_df_ = current_df[list_col_allowed]\nprint(current_df_.columns.values)","481cb8cf":"pred_y = model01.predict(x_train_2_)#(df.iloc[test_index])\n#pred_y[pred_y >= 0.5] = 1\n#pred_y[pred_y < 0.5] = 0\n#print(x_train_2_.shape,\"...\", pred_y.shape)\nprint(y_train_2.iloc[102])\nprint(pred_y[102])\n#print(y_train_2==pred_y)\n","cc4f4ee6":"import time\nimport lightgbm as lgb\n#%%time\nlgb_params = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary', #'binary' 'regression'\n    'metric': {'binary_logloss'}, # 'l2', 'l1', 'logloss'\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'seed': 42,\n    'verbose': 50\n}\ntime_start = time.time()\n\n#x_train = lgb.Dataset(x_train_1_.values, y_train_1,feature_name=categorical_cols,categorical_feature=categorical_cols,free_raw_data=False )            \n#x_valid = lgb.Dataset(x_train_2_.values, y_train_2,feature_name=categorical_cols,categorical_feature=categorical_cols,free_raw_data=False ) \n\n#below is without categorical data\n#check if any categorical data except 'assetCode'\n#print(\"1---\")\ntry:\n    #rerunning this cell becomes a problem as attempt is made to drop already dropped columns...\n    #hence this try catch block\n    x_train_1_.drop(columns=['assetCode'], inplace=True)\n    x_train_2_.drop(columns=['assetCode'], inplace=True)\nexcept KeyError:\n    print(\"There was a KeyError\")\nlist_categorical_columns(x_train_1_)\nlist_categorical_columns(x_train_2_)\n#print(\"2---\")\nx_train = lgb.Dataset(x_train_1_.values, y_train_1,feature_name=x_train_1_.columns.tolist())#, free_raw_data=False )            \n#x_train = lgb.Dataset(x_train_1_.values, y_train_1,feature_name=training_columns)#, free_raw_data=False )            \n#print(\"3---\")\nx_valid = lgb.Dataset(x_train_2_.values, y_train_2,feature_name=x_train_2_.columns.tolist(), reference=x_train)#,free_raw_data=False ) \n#x_valid = lgb.Dataset(x_train_2_.values, y_train_2,feature_name=training_columns, reference=x_train)#,free_raw_data=False ) \nevals_result = {}#store evaluation results\n#print(\"4---\")\nmodel01 = lgb.train(lgb_params, x_train, num_boost_round=1000, \\\n                    valid_sets=[x_train, x_valid], valid_names=['eval1','eval2'], \\\n                    evals_result=evals_result, verbose_eval=50 )\n\n\n'''\n#print(\"5---\")\nmodel01 = lgb.train(lgb_params, x_train, num_boost_round=1000, \\\n                    valid_sets=[x_train], valid_names=['eval1'], \\\n                    evals_result=evals_result )\nmodel01 = lgb.train(lgb_params, x_train, num_boost_round=1000, \\\n                    valid_sets=[x_train, x_valid], valid_names=['eval1','eval2'], \\\n                    evals_result=evals_result )\nmodel01 = lgb.train(lgb_params, x_train, num_boost_round=1000, \\\n                    valid_sets=[x_train, x_valid], valid_names=['eval1','eval2'], \\\n                    evals_result=evals_result )\n'''\ntime_end   = time.time()"}}