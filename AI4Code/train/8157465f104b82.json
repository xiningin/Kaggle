{"cell_type":{"2f0132f9":"code","53f3cd1d":"code","54d93b08":"code","6ded6ae7":"code","7f2aa3f5":"code","e72a2b55":"code","61a8f344":"code","d0649d7c":"code","aae38f05":"code","c59760e2":"code","2ae4bf3f":"code","8fbcf1f0":"code","dc2406a0":"code","ec3405f4":"code","fefc593a":"code","b754336f":"code","8fa9971b":"code","a7ae625d":"code","c4a47df4":"code","bbef7962":"code","c1a63025":"code","472702a6":"code","81b687bf":"code","7fd8de20":"code","3f26aead":"code","717f3706":"code","9a1ad66c":"code","cde0a9e4":"code","d310d01b":"code","198c0c81":"code","10821af4":"code","bc3ea693":"code","09aa1530":"markdown","7aca7740":"markdown","f1310b46":"markdown","8d5760b2":"markdown","f79e4463":"markdown","c70098ae":"markdown","1bd7186e":"markdown","9e484e0e":"markdown","de2b9b87":"markdown","73083b86":"markdown","e39bdb06":"markdown","150e3bd3":"markdown","909dc505":"markdown","1506aeb8":"markdown"},"source":{"2f0132f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, Normalizer, StandardScaler, KBinsDiscretizer\nfrom sklearn.impute import SimpleImputer\n#based on https:\/\/ashishtrada.medium.com\/data-preprocessing-using-scikitlearn-39fe3420e7fd","53f3cd1d":"df = pd.read_csv('..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv')\ndf","54d93b08":"df.info()","6ded6ae7":"df['neighbourhood_group'].value_counts()","7f2aa3f5":"le = LabelEncoder()\ndf['neighbourhood_group'] = le.fit_transform(df['neighbourhood_group'])","e72a2b55":"df['neighbourhood_group'].value_counts()","61a8f344":"le.classes_","d0649d7c":"df['room_type'].value_counts()","aae38f05":"one_hot = OneHotEncoder()\ntransformed_df = one_hot.fit_transform(df['room_type'].values.reshape(-1,1)).toarray()","c59760e2":"one_hot.categories_","2ae4bf3f":"transformed_df = pd.DataFrame(transformed_df, columns = ['Entire home\/apt', 'Private room', 'Shared room'])","8fbcf1f0":"transformed_df.head()","dc2406a0":"transformed_df.iloc[1,]","ec3405f4":"df['room_type'][1]","fefc593a":"temp_data = df.drop(['name', 'host_name', 'neighbourhood', 'latitude', 'longitude', 'room_type','last_review','reviews_per_month'], axis = 1, inplace = False)","b754336f":"temp_data","8fa9971b":"le_2 = LabelEncoder()\ntemp_data['neighbourhood_group'] = le.fit_transform(temp_data['neighbourhood_group'])","a7ae625d":"temp_data","c4a47df4":"normalizer = Normalizer()","bbef7962":"normalized_df = normalizer.transform(temp_data)","c1a63025":"pd.DataFrame(normalized_df, columns= temp_data.columns)","472702a6":"standard_scaler = StandardScaler()","81b687bf":"standardized_df = standard_scaler.fit_transform(temp_data)","7fd8de20":"pd.DataFrame(standardized_df, columns = temp_data.columns)","3f26aead":"df['reviews_per_month'].isnull().sum()","717f3706":"imputer = SimpleImputer(missing_values = np.nan, strategy='most_frequent')","9a1ad66c":"reviews_col = imputer.fit_transform(df['reviews_per_month'].values.reshape(-1,1))","cde0a9e4":"pd.DataFrame(reviews_col).isnull().sum()","d310d01b":"df['reviews_per_month'].isnull().sum()","198c0c81":"temp_data.head(10)","10821af4":"trans = KBinsDiscretizer(n_bins = 3, encode = 'ordinal', strategy='kmeans')\nnew_df = trans.fit_transform(temp_data)","bc3ea693":"pd.DataFrame(new_df,columns=temp_data.columns)","09aa1530":"![](https:\/\/miro.medium.com\/max\/481\/0*0AxBqEG7UwJgcogh.png)","7aca7740":"based on https:\/\/ashishtrada.medium.com\/data-preprocessing-using-scikitlearn-39fe3420e7fd","f1310b46":"<h3><b>(4) Imputing Missing Values<\/b><\/h3>\n\nHandling missing values is an important task that every data scientist must have to do. We can handle missing values in two ways.\n\n<p>(1) Remove the data (whole row) which have missing values.<\/p>\n<p>(2) Add the values by using some strategies or using Imputer.<\/p>\n\nWe can remove the missing values when the ratio of the number of missing values and a total number of values is low. So in this particular situation, we can remove missing values using dropna() in pandas.\n\nIf the ratio is high so we have to Impute the values.\n\nThankfully <b>Scikit Learn gives the SimpleImputer Class<\/b> Which will help us to fill values in missing values. It replaces the NaN values with a specified placeholder.","8d5760b2":"In real-world data is not available on the same scale. Data columns will always have different scales. So to make all the columns in one scale we can use normalization methods. Normalization will convert the whole dataset into one scale. With the Help of Normalization, we can increase the computational speed and we can also detect outliers in the dataset by normalizing the data.\n\nMinMaxScaler is one type of Normalizer will normalize the data by using minimum and maximum value of particular feature column.\nThis technique is to re-scale features with a distribution value between 0 and 1. For every feature, the minimum value of that feature gets transformed into 0, and the maximum value gets transformed into 1. The general equation is shown below:\n","f79e4463":"<h3><b>(5) Discreatization<\/b><\/h3>\n\n\nDiscretization is the process of putting values into buckets so that there are a limited number of possible states. Basically, in simple terms, it will convert continuous numerical features into categorical columns.\n\nWhen we have a lot of possibilities in the data range and it is difficult to classify the data at that time we group the continuous variable into one group. This converting feature methodology is called Discretization.\n\nThere are 3 types of Discretization available in Sci-kit learn.\n\n<p>(1) <b>Quantile<\/b> Discretization Transform<\/p>\n<p>(2) <b>Uniform<\/b> Discretization Transform<\/p>\n<p>(3) <b>KMeans<\/b> Discretization Transform<\/p>\n","c70098ae":"<p>There are a lot of preprocessing methods but we will mainly focus on the following methodologies:<\/p>\n<p>(1) Encoding the Data<\/p>\n<p>(2) Normalization<\/p>\n<p>(3) Standardization<\/p>\n<p>(4) Imputing the Missing Values<\/p>\n<p>(5) Discretization<\/p>\n\n<h2><b>Dataset information<\/b><\/h2>\n\n<p>For this tutorial, we are using the \u2018New York City Airbnb Open Data\u2019. For downloading data set click here. Here is the information about the dataset in the Notebook.<\/p>","1bd7186e":"<b>(1.1) Label Encoder<\/b>\n    \n<p> In Machine Learning, We will have more than one category in the dataset that to convert those categories into numerical features we can use a Label encoder. Label Encoder will assign a unique number to each category.","9e484e0e":"![](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/268833\/611395\/New_York_City_.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20211218%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20211218T155354Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=4326e871f33455379703fe73ab82acb6568f03a62977d4bf706c89b5e5b978c12e1232d3184df71011b311b68cee80e45067c11a5a4f86330023e556a4063c6c587b149adb979d5393cf82135ed759995621a315f9b7dd3213c9529d4d9cd419f3e56e29fb4cac3167240f407973973351f932d3f3b6385bc43e2102e578a9f0f86ba8e41c1b087a90b8966d8575933309aefa58fdd59473d63b0b6885b3a94044e6e584f6c54b877de2d0233a54cdc855ca83a5b2813a90c83505168151a82add086cee308019f514629d013d166bdaf7b0bf5a24c9d676711750cf75cde2a6096fabdf14d68aabc4b48b79ec26b1a70bf673a52d9552986ae17988b811f0cc)","de2b9b87":"<h3><b>(1) Encoding the Data<\/b>","73083b86":"<h3><b>(2) Normalization<\/b>","e39bdb06":"<b>(4.1) Imputer<\/b>","150e3bd3":"![](https:\/\/miro.medium.com\/max\/576\/0*Z62Y9V7jU4jVIVhi.png)","909dc505":"<h3><b>(3) Standardization<\/b><\/h3>\n\nStandardization is also one type of normalizer that will ensure that transformed data have <b>mean = 0 & standard deviation = 1<\/b>.\nThis is very useful for optimization algorithms like gradient descent. Data will have a value near zero so it will help to increase computational speed. Rescaling is also used for distance-based algorithms like KNN.\n\nStandardScaler is also known as Z-Score which is used for outlier detection.","1506aeb8":"<b>(1.2) One Hot Encoder<\/b>\n\n<p>One hot encoder does the same things but in a different way. Label Encoder initializes the particular number but one hot encoder will assign a whole new column to particular categories. So if you have 3 categories in the column then one hot encoder will add 3 more columns to your dataset.\n    \n"}}