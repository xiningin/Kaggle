{"cell_type":{"6c66bde9":"code","b17d9318":"code","6101b600":"code","2ba67467":"code","c8310534":"code","18bb95da":"code","212cb43d":"code","706e6422":"code","dd1f51c8":"code","b098d359":"code","afbc8228":"code","9aa7fdec":"code","13859f96":"code","75e9b3cb":"code","22c65374":"code","dde3bbcf":"code","e51fd6fe":"code","58c64ddf":"code","45407bd0":"code","0e02f516":"code","77483de2":"code","c89d865d":"code","ade36ee6":"code","767f44c4":"code","2ca196e5":"code","d406b641":"code","7951b64f":"code","963dd692":"code","03420a85":"code","ff9d3f66":"code","42cc627b":"code","a7528b6a":"code","54edf8b8":"code","9c6f7f44":"code","95a195f9":"code","26197e58":"code","56216f91":"code","7d03b792":"code","a41bdbd6":"code","683b370c":"code","e6f36f0c":"code","2e54281b":"code","6f3bfcb7":"code","a75649f7":"code","8774d242":"code","5d3bd63c":"code","77b5bb15":"markdown","ad5aafe1":"markdown","5a0d946e":"markdown","519ec40c":"markdown","4410c376":"markdown","5be1b3e4":"markdown"},"source":{"6c66bde9":"#GENERAL\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\n#PATH PROCESS\nimport os\nimport os.path\nfrom pathlib import Path\nimport glob\n#IMAGE PROCESS\nfrom PIL import Image\nfrom keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\nfrom keras.preprocessing import image\nfrom skimage.feature import hessian_matrix, hessian_matrix_eigvals\nfrom scipy.ndimage.filters import convolve\nfrom skimage import data, io, filters\nimport skimage\nfrom skimage.morphology import convex_hull_image, erosion\nfrom IPython.display import HTML\nimport matplotlib.animation as animation\nimport PIL\nfrom IPython import display\nfrom io import BytesIO\n#SCALER & TRANSFORMATION\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras import regularizers\nfrom sklearn.preprocessing import LabelEncoder\n#ACCURACY CONTROL\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n#OPTIMIZER\nfrom keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\n#MODEL LAYERS\nfrom tensorflow.keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN, LSTM, GlobalAveragePooling2D, SeparableConv2D,\\\nZeroPadding2D, Convolution2D, ZeroPadding2D, Conv2DTranspose,ReLU, UpSampling2D, Concatenate, Conv2DTranspose, ReLU, LeakyReLU\nfrom keras import models\nfrom keras import layers\nimport tensorflow as tf\nfrom keras.applications import VGG16,VGG19,inception_v3\nfrom keras import backend as K\nfrom keras.utils import plot_model\nfrom keras.models import load_model\nfrom keras import backend\nfrom tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, decode_predictions, preprocess_input\nfrom keras.preprocessing.image import img_to_array\n#SKLEARN CLASSIFIER\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import ElasticNetCV\n#IGNORING WARNINGS\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\",category=DeprecationWarning)\nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)","b17d9318":"COLOR_PATH = Path(\"..\/input\/landscape-image-colorization\/landscape Images\/color\")\nGRAY_PATH = Path(\"..\/input\/landscape-image-colorization\/landscape Images\/gray\")","6101b600":"COLOR_LIST = list(COLOR_PATH.glob(r\"*jpg\"))\nGRAY_LIST = list(GRAY_PATH.glob(r\"*jpg\"))\n\nCOLOR_LIST = sorted(COLOR_LIST)\nGRAY_LIST = sorted(GRAY_LIST)","2ba67467":"print(len(COLOR_LIST))\nprint(len(GRAY_LIST))","c8310534":"COLOR_SERIES = pd.Series(COLOR_LIST,name=\"COLOR\").astype(str)\nGRAY_SERIES = pd.Series(GRAY_LIST,name=\"GRAY\").astype(str)","18bb95da":"MAIN_DATA = pd.concat([COLOR_SERIES,GRAY_SERIES],axis=1)","212cb43d":"MAIN_DATA","706e6422":"EXAMPLE_COLOR = cv2.cvtColor(cv2.imread(MAIN_DATA[\"COLOR\"][3]),cv2.COLOR_BGR2RGB)\nEXAMPLE_GRAY = cv2.cvtColor(cv2.imread(MAIN_DATA[\"GRAY\"][3]),cv2.COLOR_BGR2RGB)\n\nfigure,axis = plt.subplots(1,2,figsize=(14,14))\n\naxis[0].set_xlabel(EXAMPLE_COLOR.shape)\naxis[0].set_title(\"COLOR\")\n\naxis[0].imshow(EXAMPLE_COLOR)\n\naxis[1].set_xlabel(EXAMPLE_GRAY.shape)\naxis[1].set_title(\"GRAY\")\n\naxis[1].imshow(EXAMPLE_GRAY)\n\nplt.tight_layout()\nplt.show()","dd1f51c8":"EXAMPLE_COLOR = cv2.cvtColor(cv2.imread(MAIN_DATA[\"COLOR\"][13]),cv2.COLOR_BGR2RGB)\nEXAMPLE_GRAY = cv2.cvtColor(cv2.imread(MAIN_DATA[\"GRAY\"][13]),cv2.COLOR_BGR2RGB)\n\nfigure,axis = plt.subplots(1,2,figsize=(14,14))\n\naxis[0].set_xlabel(EXAMPLE_COLOR.shape)\naxis[0].set_title(\"COLOR\")\n\naxis[0].imshow(EXAMPLE_COLOR)\n\naxis[1].set_xlabel(EXAMPLE_GRAY.shape)\naxis[1].set_title(\"GRAY\")\n\naxis[1].imshow(EXAMPLE_GRAY)\n\nplt.tight_layout()\nplt.show()","b098d359":"EXAMPLE_COLOR = cv2.cvtColor(cv2.imread(MAIN_DATA[\"COLOR\"][55]),cv2.COLOR_BGR2RGB)\nEXAMPLE_GRAY = cv2.cvtColor(cv2.imread(MAIN_DATA[\"GRAY\"][55]),cv2.COLOR_BGR2RGB)\n\nfigure,axis = plt.subplots(1,2,figsize=(14,14))\n\naxis[0].set_xlabel(EXAMPLE_COLOR.shape)\naxis[0].set_title(\"COLOR\")\n\naxis[0].imshow(EXAMPLE_COLOR)\n\naxis[1].set_xlabel(EXAMPLE_GRAY.shape)\naxis[1].set_title(\"GRAY\")\n\naxis[1].imshow(EXAMPLE_GRAY)\n\nplt.tight_layout()\nplt.show()","afbc8228":"EXAMPLE_COLOR = cv2.cvtColor(cv2.imread(MAIN_DATA[\"COLOR\"][1155]),cv2.COLOR_BGR2RGB)\nEXAMPLE_GRAY = cv2.cvtColor(cv2.imread(MAIN_DATA[\"GRAY\"][1155]),cv2.COLOR_BGR2RGB)\n\nfigure,axis = plt.subplots(1,2,figsize=(14,14))\n\naxis[0].set_xlabel(EXAMPLE_COLOR.shape)\naxis[0].set_title(\"COLOR\")\n\naxis[0].imshow(EXAMPLE_COLOR)\n\naxis[1].set_xlabel(EXAMPLE_GRAY.shape)\naxis[1].set_title(\"GRAY\")\n\naxis[1].imshow(EXAMPLE_GRAY)\n\nplt.tight_layout()\nplt.show()","9aa7fdec":"EXAMPLE_COLOR = cv2.cvtColor(cv2.imread(MAIN_DATA[\"COLOR\"][7000]),cv2.COLOR_BGR2RGB)\nEXAMPLE_GRAY = cv2.cvtColor(cv2.imread(MAIN_DATA[\"GRAY\"][7000]),cv2.COLOR_BGR2RGB)\n\nfigure,axis = plt.subplots(1,2,figsize=(14,14))\n\naxis[0].set_xlabel(EXAMPLE_COLOR.shape)\naxis[0].set_title(\"COLOR\")\n\naxis[0].imshow(EXAMPLE_COLOR)\n\naxis[1].set_xlabel(EXAMPLE_GRAY.shape)\naxis[1].set_title(\"GRAY\")\n\naxis[1].imshow(EXAMPLE_GRAY)\n\nplt.tight_layout()\nplt.show()","13859f96":"COLOR_ARRAY_LIST = []\nGRAY_ARRAY_LIST = []","75e9b3cb":"REDUCE_DATA = MAIN_DATA[:3000]","22c65374":"REDUCE_DATA","dde3bbcf":"for x_color,x_gray in zip(REDUCE_DATA.COLOR,REDUCE_DATA.GRAY):\n    \n    READING_COLOR = cv2.cvtColor(cv2.imread(x_color),cv2.COLOR_BGR2RGB)\n    READING_GRAY = cv2.cvtColor(cv2.imread(x_gray),cv2.COLOR_BGR2RGB)\n    \n    RESIZED_COLOR = cv2.resize(READING_COLOR,(150,150))\n    RESIZED_GRAY = cv2.resize(READING_GRAY,(150,150))\n    \n    FLOAT_COLOR = RESIZED_COLOR.astype(np.float32) \/ 255.\n    FLOAT_GRAY = RESIZED_GRAY.astype(np.float32) \/ 255.\n    \n    COLOR_ARRAY_LIST.append(img_to_array(FLOAT_COLOR))\n    GRAY_ARRAY_LIST.append(img_to_array(FLOAT_GRAY))","e51fd6fe":"print(\"WHEN IT IS ARRAY COLOR SHAPE: \",np.shape(np.array(COLOR_ARRAY_LIST)))\nprint(\"WHEN IT IS ARRAY GRAY SHAPE: \",np.shape(np.array(GRAY_ARRAY_LIST)))","58c64ddf":"COLOR_FIT_DATA = np.array(COLOR_ARRAY_LIST)\nGRAY_FIT_DATA = np.array(GRAY_ARRAY_LIST)","45407bd0":"print(COLOR_FIT_DATA.shape)\nprint(GRAY_FIT_DATA.shape)","0e02f516":"print(COLOR_FIT_DATA.dtype)\nprint(GRAY_FIT_DATA.dtype)","77483de2":"COMPILE_LOSS = \"mae\"\nCOMPILE_OPTIMIZER = RMSprop(lr=0.0001,decay=1e-8)\nCOMPILE_METRIC = [\"accuracy\"]\nOUTPUT_CLASS = 3\n\nCOMPILE_FUNC = LeakyReLU()","c89d865d":"Reduce_Model = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\",factor=0.1,patience=3)\n\nCheckpoint_Model = tf.keras.callbacks.ModelCheckpoint(monitor=\"val_accuracy\",\n                                                      save_best_only=True,\n                                                      save_weights_only=True,\n                                                      filepath=\".\/modelcheck\")","ade36ee6":"Encoder_G = Sequential()\nEncoder_G.add(Conv2D(32,(2,2),kernel_initializer = 'he_normal',padding = \"same\",use_bias = True))\nEncoder_G.add(BatchNormalization())\nEncoder_G.add(COMPILE_FUNC)\n#\nEncoder_G.add(Conv2D(64,(2,2),kernel_initializer = 'he_normal',padding = \"same\",use_bias = True))\nEncoder_G.add(BatchNormalization())\nEncoder_G.add(COMPILE_FUNC)\n#\nEncoder_G.add(Conv2D(128,(2,2),kernel_initializer = 'he_normal',padding = \"same\",use_bias = True))\nEncoder_G.add(BatchNormalization())\nEncoder_G.add(COMPILE_FUNC)\n\nEncoder_G.add(Conv2D(256,(2,2),kernel_initializer = 'he_normal',padding = \"same\",use_bias = True))\nEncoder_G.add(BatchNormalization())\nEncoder_G.add(COMPILE_FUNC)\n\n\nDecoder_G = Sequential()\nDecoder_G.add(Conv2DTranspose(128,(2,2),padding = \"same\",use_bias = True))\nDecoder_G.add(COMPILE_FUNC)\n#\nDecoder_G.add(Conv2DTranspose(64,(2,2),padding = \"same\",use_bias = True))\nDecoder_G.add(COMPILE_FUNC)\n#\nDecoder_G.add(Conv2DTranspose(32,(2,2),padding = \"same\",use_bias = True))\nDecoder_G.add(COMPILE_FUNC)\n#\nDecoder_G.add(Conv2D(OUTPUT_CLASS,(2,2),padding = \"same\",use_bias = True))\nDecoder_G.add(COMPILE_FUNC)","767f44c4":"Auto_Encoder = Sequential([Encoder_G,Decoder_G])\nAuto_Encoder.compile(loss=COMPILE_LOSS,optimizer=COMPILE_OPTIMIZER,metrics=COMPILE_METRIC)","2ca196e5":"Model_AutoEncoder = Auto_Encoder.fit(GRAY_FIT_DATA,COLOR_FIT_DATA,epochs=50,callbacks=[Checkpoint_Model,Reduce_Model],batch_size=50)","d406b641":"Prediction_IMAGES_SEEN = Auto_Encoder.predict(GRAY_FIT_DATA[:5])","7951b64f":"figure,axis = plt.subplots(1,2,figsize=(10,10))\nprediction_img_number = 1\n\nOriginal_Img = GRAY_FIT_DATA[prediction_img_number]\nPredict_Image_AE = Prediction_IMAGES_SEEN[prediction_img_number]\n\naxis[0].imshow(Original_Img)\naxis[0].set_xlabel(Original_Img.shape)\naxis[0].set_ylabel(Original_Img.size)\naxis[0].set_title(\"INPUT\")\n\n\naxis[1].imshow(Predict_Image_AE)\naxis[1].set_xlabel(Predict_Image_AE.shape)\naxis[1].set_ylabel(Predict_Image_AE.size)\naxis[1].set_title(\"AUTO ENCODER OUTPUT\")\n\nplt.show()","963dd692":"figure,axis = plt.subplots(1,2,figsize=(10,10))\nprediction_img_number = 2\n\nOriginal_Img = GRAY_FIT_DATA[prediction_img_number]\nPredict_Image_AE = Prediction_IMAGES_SEEN[prediction_img_number]\n\naxis[0].imshow(Original_Img)\naxis[0].set_xlabel(Original_Img.shape)\naxis[0].set_ylabel(Original_Img.size)\naxis[0].set_title(\"INPUT\")\n\n\naxis[1].imshow(Predict_Image_AE)\naxis[1].set_xlabel(Predict_Image_AE.shape)\naxis[1].set_ylabel(Predict_Image_AE.size)\naxis[1].set_title(\"AUTO ENCODER OUTPUT\")\n\nplt.show()","03420a85":"EXAMPLE_IMAGE = cv2.cvtColor(cv2.imread(MAIN_DATA[\"GRAY\"][3002]),cv2.COLOR_BGR2RGB)\nRESIZED_EXAMPLE = cv2.resize(EXAMPLE_IMAGE,(150,150))\nFLOAT_EXAMPLE = RESIZED_EXAMPLE.astype(np.float32) \/ 255.\n\nRESHAPE_EXAMPLE = FLOAT_EXAMPLE.reshape(1,FLOAT_EXAMPLE.shape[0],FLOAT_EXAMPLE.shape[1],FLOAT_EXAMPLE.shape[2])\n\nPrediction_IMAGES_NON_SEEN = Auto_Encoder.predict(RESHAPE_EXAMPLE)\n\nfigure,axis = plt.subplots(1,2,figsize=(18,18))\n\naxis[0].imshow(EXAMPLE_IMAGE)\naxis[0].set_xlabel(EXAMPLE_IMAGE.shape)\naxis[0].set_ylabel(EXAMPLE_IMAGE.size)\naxis[0].set_title(\"INPUT\")\n\n\naxis[1].imshow(Prediction_IMAGES_NON_SEEN[0])\naxis[1].set_xlabel(Prediction_IMAGES_NON_SEEN[0].shape)\naxis[1].set_ylabel(Prediction_IMAGES_NON_SEEN[0].size)\naxis[1].set_title(\"AUTO ENCODER OUTPUT\")\n\nplt.show()","ff9d3f66":"EXAMPLE_IMAGE = cv2.cvtColor(cv2.imread(MAIN_DATA[\"GRAY\"][3052]),cv2.COLOR_BGR2RGB)\nRESIZED_EXAMPLE = cv2.resize(EXAMPLE_IMAGE,(150,150))\nFLOAT_EXAMPLE = RESIZED_EXAMPLE.astype(np.float32) \/ 255.\n\nRESHAPE_EXAMPLE = FLOAT_EXAMPLE.reshape(1,FLOAT_EXAMPLE.shape[0],FLOAT_EXAMPLE.shape[1],FLOAT_EXAMPLE.shape[2])\n\nPrediction_IMAGES_NON_SEEN = Auto_Encoder.predict(RESHAPE_EXAMPLE)\n\nfigure,axis = plt.subplots(1,2,figsize=(18,18))\n\naxis[0].imshow(EXAMPLE_IMAGE)\naxis[0].set_xlabel(EXAMPLE_IMAGE.shape)\naxis[0].set_ylabel(EXAMPLE_IMAGE.size)\naxis[0].set_title(\"INPUT\")\n\n\naxis[1].imshow(Prediction_IMAGES_NON_SEEN[0])\naxis[1].set_xlabel(Prediction_IMAGES_NON_SEEN[0].shape)\naxis[1].set_ylabel(Prediction_IMAGES_NON_SEEN[0].size)\naxis[1].set_title(\"AUTO ENCODER OUTPUT\")\n\nplt.show()","42cc627b":"EXAMPLE_IMAGE = cv2.cvtColor(cv2.imread(MAIN_DATA[\"GRAY\"][4052]),cv2.COLOR_BGR2RGB)\nRESIZED_EXAMPLE = cv2.resize(EXAMPLE_IMAGE,(150,150))\nFLOAT_EXAMPLE = RESIZED_EXAMPLE.astype(np.float32) \/ 255.\n\nRESHAPE_EXAMPLE = FLOAT_EXAMPLE.reshape(1,FLOAT_EXAMPLE.shape[0],FLOAT_EXAMPLE.shape[1],FLOAT_EXAMPLE.shape[2])\n\nPrediction_IMAGES_NON_SEEN = Auto_Encoder.predict(RESHAPE_EXAMPLE)\n\nfigure,axis = plt.subplots(1,2,figsize=(18,18))\n\naxis[0].imshow(EXAMPLE_IMAGE)\naxis[0].set_xlabel(EXAMPLE_IMAGE.shape)\naxis[0].set_ylabel(EXAMPLE_IMAGE.size)\naxis[0].set_title(\"INPUT\")\n\n\naxis[1].imshow(Prediction_IMAGES_NON_SEEN[0])\naxis[1].set_xlabel(Prediction_IMAGES_NON_SEEN[0].shape)\naxis[1].set_ylabel(Prediction_IMAGES_NON_SEEN[0].size)\naxis[1].set_title(\"AUTO ENCODER OUTPUT\")\n\nplt.show()","a7528b6a":"EXAMPLE_IMAGE = cv2.cvtColor(cv2.imread(MAIN_DATA[\"GRAY\"][6152]),cv2.COLOR_BGR2RGB)\nRESIZED_EXAMPLE = cv2.resize(EXAMPLE_IMAGE,(150,150))\nFLOAT_EXAMPLE = RESIZED_EXAMPLE.astype(np.float32) \/ 255.\n\nRESHAPE_EXAMPLE = FLOAT_EXAMPLE.reshape(1,FLOAT_EXAMPLE.shape[0],FLOAT_EXAMPLE.shape[1],FLOAT_EXAMPLE.shape[2])\n\nPrediction_IMAGES_NON_SEEN = Auto_Encoder.predict(RESHAPE_EXAMPLE)\n\nfigure,axis = plt.subplots(1,2,figsize=(18,18))\n\naxis[0].imshow(EXAMPLE_IMAGE)\naxis[0].set_xlabel(EXAMPLE_IMAGE.shape)\naxis[0].set_ylabel(EXAMPLE_IMAGE.size)\naxis[0].set_title(\"INPUT\")\n\n\naxis[1].imshow(Prediction_IMAGES_NON_SEEN[0])\naxis[1].set_xlabel(Prediction_IMAGES_NON_SEEN[0].shape)\naxis[1].set_ylabel(Prediction_IMAGES_NON_SEEN[0].size)\naxis[1].set_title(\"AUTO ENCODER OUTPUT\")\n\nplt.show()","54edf8b8":"EXAMPLE_IMAGE = cv2.cvtColor(cv2.imread(MAIN_DATA[\"GRAY\"][6192]),cv2.COLOR_BGR2RGB)\nRESIZED_EXAMPLE = cv2.resize(EXAMPLE_IMAGE,(150,150))\nFLOAT_EXAMPLE = RESIZED_EXAMPLE.astype(np.float32) \/ 255.\n\nRESHAPE_EXAMPLE = FLOAT_EXAMPLE.reshape(1,FLOAT_EXAMPLE.shape[0],FLOAT_EXAMPLE.shape[1],FLOAT_EXAMPLE.shape[2])\n\nPrediction_IMAGES_NON_SEEN = Auto_Encoder.predict(RESHAPE_EXAMPLE)\n\nfigure,axis = plt.subplots(1,2,figsize=(18,18))\n\naxis[0].imshow(EXAMPLE_IMAGE)\naxis[0].set_xlabel(EXAMPLE_IMAGE.shape)\naxis[0].set_ylabel(EXAMPLE_IMAGE.size)\naxis[0].set_title(\"INPUT\")\n\n\naxis[1].imshow(Prediction_IMAGES_NON_SEEN[0])\naxis[1].set_xlabel(Prediction_IMAGES_NON_SEEN[0].shape)\naxis[1].set_ylabel(Prediction_IMAGES_NON_SEEN[0].size)\naxis[1].set_title(\"AUTO ENCODER OUTPUT\")\n\nplt.show()","9c6f7f44":"TARGET_PATH = \"..\/input\/mkemal-ataturk-image\/ata.jpg\"\n\nEXAMPLE_IMAGE = cv2.cvtColor(cv2.imread(TARGET_PATH),cv2.COLOR_BGR2RGB)\nRESIZED_EXAMPLE = cv2.resize(EXAMPLE_IMAGE,(150,150))\nFLOAT_EXAMPLE = RESIZED_EXAMPLE.astype(np.float32) \/ 255.\n\nRESHAPE_EXAMPLE = FLOAT_EXAMPLE.reshape(1,FLOAT_EXAMPLE.shape[0],FLOAT_EXAMPLE.shape[1],FLOAT_EXAMPLE.shape[2])\n\nPrediction_IMAGES_NON_SEEN = Auto_Encoder.predict(RESHAPE_EXAMPLE)\n\nfigure,axis = plt.subplots(1,2,figsize=(18,18))\n\naxis[0].imshow(EXAMPLE_IMAGE)\naxis[0].set_xlabel(EXAMPLE_IMAGE.shape)\naxis[0].set_ylabel(EXAMPLE_IMAGE.size)\naxis[0].set_title(\"INPUT\")\n\n\naxis[1].imshow(Prediction_IMAGES_NON_SEEN[0])\naxis[1].set_xlabel(Prediction_IMAGES_NON_SEEN[0].shape)\naxis[1].set_ylabel(Prediction_IMAGES_NON_SEEN[0].size)\naxis[1].set_title(\"AUTO ENCODER OUTPUT\")\n\nplt.show()","95a195f9":"TARGET_PATH = \"..\/input\/mkemal-ataturk-image\/ata2.jpg\"\n\nEXAMPLE_IMAGE = cv2.cvtColor(cv2.imread(TARGET_PATH),cv2.COLOR_BGR2RGB)\nRESIZED_EXAMPLE = cv2.resize(EXAMPLE_IMAGE,(150,150))\nFLOAT_EXAMPLE = RESIZED_EXAMPLE.astype(np.float32) \/ 255.\n\nRESHAPE_EXAMPLE = FLOAT_EXAMPLE.reshape(1,FLOAT_EXAMPLE.shape[0],FLOAT_EXAMPLE.shape[1],FLOAT_EXAMPLE.shape[2])\n\nPrediction_IMAGES_NON_SEEN = Auto_Encoder.predict(RESHAPE_EXAMPLE)\n\nfigure,axis = plt.subplots(1,2,figsize=(18,18))\n\naxis[0].imshow(EXAMPLE_IMAGE)\naxis[0].set_xlabel(EXAMPLE_IMAGE.shape)\naxis[0].set_ylabel(EXAMPLE_IMAGE.size)\naxis[0].set_title(\"INPUT\")\n\n\naxis[1].imshow(Prediction_IMAGES_NON_SEEN[0])\naxis[1].set_xlabel(Prediction_IMAGES_NON_SEEN[0].shape)\naxis[1].set_ylabel(Prediction_IMAGES_NON_SEEN[0].size)\naxis[1].set_title(\"AUTO ENCODER OUTPUT\")\n\nplt.show()","26197e58":"TARGET_PATH = \"..\/input\/ataturk-image\/ata3.jpg\"\n\nEXAMPLE_IMAGE = cv2.cvtColor(cv2.imread(TARGET_PATH),cv2.COLOR_BGR2RGB)\nRESIZED_EXAMPLE = cv2.resize(EXAMPLE_IMAGE,(150,150))\nFLOAT_EXAMPLE = RESIZED_EXAMPLE.astype(np.float32) \/ 255.\n\nRESHAPE_EXAMPLE = FLOAT_EXAMPLE.reshape(1,FLOAT_EXAMPLE.shape[0],FLOAT_EXAMPLE.shape[1],FLOAT_EXAMPLE.shape[2])\n\nPrediction_IMAGES_NON_SEEN = Auto_Encoder.predict(RESHAPE_EXAMPLE)\n\nfigure,axis = plt.subplots(1,2,figsize=(18,18))\n\naxis[0].imshow(EXAMPLE_IMAGE)\naxis[0].set_xlabel(EXAMPLE_IMAGE.shape)\naxis[0].set_ylabel(EXAMPLE_IMAGE.size)\naxis[0].set_title(\"INPUT\")\n\n\naxis[1].imshow(Prediction_IMAGES_NON_SEEN[0])\naxis[1].set_xlabel(Prediction_IMAGES_NON_SEEN[0].shape)\naxis[1].set_ylabel(Prediction_IMAGES_NON_SEEN[0].size)\naxis[1].set_title(\"AUTO ENCODER OUTPUT\")\n\nplt.show()","56216f91":"TARGET_PATH = \"..\/input\/ataturk-image\/ata4.jpg\"\n\nEXAMPLE_IMAGE = cv2.cvtColor(cv2.imread(TARGET_PATH),cv2.COLOR_BGR2RGB)\nRESIZED_EXAMPLE = cv2.resize(EXAMPLE_IMAGE,(150,150))\nFLOAT_EXAMPLE = RESIZED_EXAMPLE.astype(np.float32) \/ 255.\n\nRESHAPE_EXAMPLE = FLOAT_EXAMPLE.reshape(1,FLOAT_EXAMPLE.shape[0],FLOAT_EXAMPLE.shape[1],FLOAT_EXAMPLE.shape[2])\n\nPrediction_IMAGES_NON_SEEN = Auto_Encoder.predict(RESHAPE_EXAMPLE)\n\nfigure,axis = plt.subplots(1,2,figsize=(18,18))\n\naxis[0].imshow(EXAMPLE_IMAGE)\naxis[0].set_xlabel(EXAMPLE_IMAGE.shape)\naxis[0].set_ylabel(EXAMPLE_IMAGE.size)\naxis[0].set_title(\"INPUT\")\n\n\naxis[1].imshow(Prediction_IMAGES_NON_SEEN[0])\naxis[1].set_xlabel(Prediction_IMAGES_NON_SEEN[0].shape)\naxis[1].set_ylabel(Prediction_IMAGES_NON_SEEN[0].size)\naxis[1].set_title(\"AUTO ENCODER OUTPUT\")\n\nplt.show()","7d03b792":"def saving_frames(save_path,frame_list,fps=32,size=(150,150)):\n    \n    frame_list = np.array(frame_list) * 255.\n    frame_list = frame_list.astype(np.uint8)\n    Writing_Function = cv2.VideoWriter(save_path,cv2.VideoWriter_fourcc(*\"MJPG\"),fps,size)\n    \n    for frames in frame_list:\n        Writing_Function.write(cv2.cvtColor(frames,cv2.COLOR_BGR2RGB))\n        \n    Writing_Function.release()","a41bdbd6":"def displaying_video(source):\n    figure = plt.figure(figsize=(15,15))\n\n    Image_List = []\n    for indexing in source:\n        \n        Read_IMG = plt.imshow(indexing, animated=True)\n        plt.axis('off')\n        Image_List.append([Read_IMG])\n\n    Animation_Func = animation.ArtistAnimation(figure, Image_List, interval=50, repeat_delay=1000)\n    plt.close()\n    return Animation_Func","683b370c":"TEST_VIDEO_PATH = \"..\/input\/ataturk-video\/9cv5i9.mp4\"\n\nVideo_Capturing = cv2.VideoCapture(TEST_VIDEO_PATH)\n    \nFrames_List = []\n    \nwhile Video_Capturing.isOpened():\n        \n    _,frame = Video_Capturing.read()\n        \n    if _ != True:\n        break\n            \n    if Video_Capturing.isOpened():\n            \n        Transformed_Image = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n        Resized_Image = cv2.resize(Transformed_Image,(150,150))\n        Target_Image = Resized_Image \/ 255.\n        Frames_List.append(Target_Image)\n            \nVideo_Capturing.release()","e6f36f0c":"print(len(Frames_List))","2e54281b":"print(Frames_List[0].shape)","6f3bfcb7":"plt.figure(figsize=(11,11))\n\nplt.imshow(Frames_List[0])\nplt.axis(\"off\")\nplt.show()","a75649f7":"PREDICT_TARGET_LIST = []\n\nfor x_target_image in Frames_List:\n\n    FLOAT_EXAMPLE = x_target_image.astype(np.float32)\n\n    RESHAPE_EXAMPLE = FLOAT_EXAMPLE.reshape(1,FLOAT_EXAMPLE.shape[0],FLOAT_EXAMPLE.shape[1],FLOAT_EXAMPLE.shape[2])\n\n    Prediction_IMAGES_NON_SEEN = Auto_Encoder.predict(RESHAPE_EXAMPLE)\n    \n    Prediction_IMAGES_NON_SEEN = Prediction_IMAGES_NON_SEEN.reshape(Prediction_IMAGES_NON_SEEN.shape[1],Prediction_IMAGES_NON_SEEN.shape[2],Prediction_IMAGES_NON_SEEN.shape[3])\n    \n    PREDICT_TARGET_LIST.append(Prediction_IMAGES_NON_SEEN)","8774d242":"plt.figure(figsize=(11,11))\n\nplt.imshow(PREDICT_TARGET_LIST[0])\nplt.axis(\"off\")\nplt.show()","5d3bd63c":"HTML(displaying_video(PREDICT_TARGET_LIST).to_html5_video())","77b5bb15":"### DATA","ad5aafe1":"# IMAGE PROCESS","5a0d946e":"### VISION CONTROL","519ec40c":"# PACKAGES AND LIBRARIES","4410c376":"# DATA AND PATH PROCESS","5be1b3e4":"# AUTO-ENCODER MODEL"}}