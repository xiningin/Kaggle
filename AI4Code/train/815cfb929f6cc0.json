{"cell_type":{"f0dab121":"code","77228897":"code","37c071b5":"code","ba382cf3":"code","17dd32bc":"code","7f6fa628":"code","3bca7988":"code","de497836":"code","6e3a79c9":"code","ddbbd5aa":"code","7719e05a":"code","4d1c1491":"code","613e76a8":"code","a0ad6085":"code","9976ded7":"code","51ddd6e6":"code","1d732653":"code","e6535194":"code","8a44a326":"code","d8a299b9":"code","75cc6534":"code","9c089bb8":"code","e22a7def":"code","53c47fc2":"code","a404c5ae":"code","ecab6094":"code","66c0b1a3":"code","7b292f8a":"code","dfe700a1":"code","24a12fad":"code","7b65e72f":"code","c6494fab":"code","5172654f":"code","0700500b":"code","dd0dcccd":"code","cccb3e67":"code","e1582e71":"code","991a082f":"code","c1fe1ab8":"code","3dfa13d3":"markdown","ad251b49":"markdown","cf04d30b":"markdown","4140851b":"markdown","37f4e848":"markdown","0320bb86":"markdown","03ab2bad":"markdown","9d1e0a04":"markdown","4bbb3990":"markdown","3ee4a2df":"markdown","f930a954":"markdown","4ebf1446":"markdown","5fcc0600":"markdown","41d4e226":"markdown","167a00b3":"markdown","5991a9c2":"markdown","7a264446":"markdown","377771bd":"markdown","c42b02ee":"markdown","4a5d60cf":"markdown","7444eec0":"markdown","d3bd8683":"markdown","d1cbbda4":"markdown","8decf0db":"markdown","083fd561":"markdown","bce39140":"markdown","e0882733":"markdown","407cb7bc":"markdown","d1c4bd00":"markdown","a7357b5f":"markdown","6b51e6f3":"markdown","cd185d2d":"markdown","0f1d4a11":"markdown"},"source":{"f0dab121":"import pandas as pd\nimport numpy as np\nimport sklearn\nimport re\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# \uc2a4\ud0dc\ud0b9\uc744 \uc704\ud574 5\uac1c\uc758 ML\ubaa8\ub378\uc744 \uc0ac\uc6a9\nfrom sklearn.ensemble import (RandomForestClassifier,AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.cross_validation import KFold\n\n# target directory for submission file\ntarget_dir = '\/kaggle\/working\/'","77228897":"# plotly\ub97c \uc774\uc6a9\ud55c \uc2dc\uac01\ud654\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)  # \uce90\uae00 \ubc0f \ucf54\ub7a9\uc5d0\uc11c \uc0ac\uc6a9\uac00\ub2a5\ud558\ub3c4\ub85d \uc14b\uc5c5\n\nimport plotly.graph_objs as go\nimport plotly.tools as tls","37c071b5":"%cd ..\/input\/titanic","ba382cf3":"def load_dataset():\n    \"\"\"\n    \ub370\uc774\ud130\uc14b \ub85c\ub4dc\n    \"\"\"\n    train = pd.read_csv(\"train.csv\")\n    test  = pd.read_csv(\"test.csv\")\n    \n    PassengerId = test[\"PassengerId\"]\n    \n    display(train.head(n=2), \"\\n\", train.shape, test.shape)\n    display(train.info())\n    display(test.info())\n    \n    full_data = [train, test]\n    \n    return train, test, PassengerId, full_data\n\n# \ub370\uc774\ud130\uc14b\ub85c\ub4dc \ntrain, test, PassengerId, full_data = load_dataset()","17dd32bc":"# Age\ud53c\ucc98\ub97c 5\uac1c\uc758 \ubc94\uc8fc\ud615 \ub370\uc774\ud130\ub85c \ub098\ub208 \uacb0\uacfc\ub974 \ube48\ub3c4\uc218 \ud655\uc778\npd.cut(train['Age'], 5).value_counts()","7f6fa628":"# Fare\ud53c\ucc98\ub97c 4\uac1c\uc758 \ubc94\uc8fc\ud615 \ub370\uc774\ud130\ub85c \ub098\ub208 \uacb0\uacfc\ub97c \ube48\ub3c4\uc218 \ud655\uc778\npd.cut(train['Fare'], 4).value_counts()","3bca7988":"full_data = [train, test]\ntrain[\"Name_length\"] = train[\"Name\"].apply(len)\ntest[\"Name_length\"] = test[\"Name\"].apply(len)\n\n# Cabin owns?\ntrain[\"Has_Cabin\"] = train[\"Cabin\"].apply(lambda x: 0 if type(x) ==float else 1)\ntest[\"Has_Cabin\"] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n\n# New feature - 10 FamilySize\nfor dataset in full_data:\n    dataset[\"FamilySize\"] = dataset[\"Parch\"] + dataset[\"SibSp\"] + 1\n\n# New Feature derived from FamilySize feature.\nfor dataset in full_data:\n    dataset[\"IsAlone\"] = 0\n    dataset.loc[dataset[\"FamilySize\"] ==1, \"IsAlone\"] = 1\n    \n# Removed null value from Embarked feature\nfor dataset in full_data:\n    dataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(\"S\")\n    \n# Remove null value from Fare feature and create a new feature  - Fare_Band\nfor dataset in full_data:\n    dataset[\"Fare\"] = dataset[\"Fare\"].fillna(train[\"Fare\"].median())\n    \ntrain[\"CategoricalFare\"] = pd.cut(train[\"Fare\"], 4)\n\n# Create a new feature CategoricalAge\n# fill NaN value for Age feature into random value\nfor dataset in full_data:\n    avg_age = dataset[\"Age\"].mean()\n    std_age = dataset[\"Age\"].std()\n    age_null_count = dataset[\"Age\"].isnull().sum()\n    age_null_random_list = np.random.randint(avg_age - std_age, avg_age + std_age, age_null_count)\n    dataset[\"Age\"][np.isnan(dataset[\"Age\"])] = age_null_random_list\n    dataset[\"Age\"] = dataset[\"Age\"].astype(int)\n    \ntrain[\"CategoricalAge\"] = pd.cut(train[\"Age\"], 5)  # Age\ud53c\ucc98\uc758 \uac12\uc744 5\uac1c\uc758 \uad6c\uac04\uc73c\ub85c \ubd84\ud560\n\n# Title\ud53c\ucc98\uc5d0 \ub300\ud55c \ucc98\ub9ac \ndef get_title(name):\n    \"\"\"\n    \uc2b9\uc120\uc790\uc758 \uc774\ub984\uc744 \uc778\uc790\ub85c \uc874\uce6d\uc744 \ub9ac\ud134\n    \"\"\"\n    title = None\n    title_search = re.search(\"([a-zA-Z]+)\\.\", name)\n    if title_search:\n        title = title_search.group(1)\n    return title\n\nfor dataset in full_data:\n    dataset[\"Title\"] = dataset[\"Name\"].apply(get_title)\n    \nfor dataset in full_data:\n    title_list = dataset[\"Title\"].value_counts().index.tolist()\n    # ['Mr','Miss','Mrs','Master',     'Dr',  'Rev',  'Major','Col',  'Mlle', 'Lady','Countess','Ms', 'Don', 'Mme', 'Jonkheer','Sir','Capt']\n    dataset[\"Title\"] = dataset[\"Title\"].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Mlle\", \"Miss\")\n    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Ms\", \"Miss\")\n    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Mme\", \"Mrs\")\n\nfor dataset in full_data:\n    dataset[\"Sex\"] = dataset[\"Sex\"].map({\"female\": 0, \"male\":1})\n    \n    mapping_titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset[\"Title\"] = dataset[\"Title\"].map(mapping_titles)\n    dataset[\"Title\"] = dataset[\"Title\"].fillna(0)\n    \n    # Embarked mapping\n    dataset[\"Embarked\"] = dataset[\"Embarked\"].map({\"S\": 0, \"C\": 1, \"Q\": 2}).astype(int)\n    \n    # Age mapping\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4    \n    \n    # Fare mapping\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)","de497836":"# feature drop\ndrop_features = [\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\", \"SibSp\"]\ntrain = train.drop(drop_features, axis = 1)\ntrain = train.drop([\"CategoricalFare\", \"CategoricalAge\"], axis = 1)\ntest  = test.drop(drop_features,  axis = 1)","6e3a79c9":"train.info()","ddbbd5aa":"colormap = plt.cm.RdBu\ny_position = 1.05\nplt.figure(figsize = (14, 12))\n\nplt.title(\"Person Correlation of features\", y=y_position, size = 15)\nsns.heatmap(train.astype(float).corr(), linewidth=0.1, vmax=1.0, square=True, cmap=colormap, linecolor=\"white\", annot=True)","7719e05a":"data = train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Embarked', u'FamilySize', u'Title']]\ng = sns.pairplot(data = data, hue=\"Survived\", palette=\"seismic\", size = 1.5, diag_kind=\"kde\", diag_kws=dict(shade=True), plot_kws=dict(s=5))\n#g.set(xticklabels=[])","4d1c1491":"data  =train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Embarked', u'FamilySize', u'Title']]\ng = sns.pairplot(data = data, hue='Survived', palette='seismic', size = 1.5, diag_kind='kde', diag_kws=dict(shade=True), plot_kws=dict(s=10))\ng.set(xticklabels=[])  # X\ucd95\uc758 \ub208\uae08 \uc2a4\ucf00\uc77c\uc744 \ubcf4\uc5ec\uc8fc\uc9c0 \uc54a\ub294\uad70","613e76a8":"version = sklearn.__version__\nversionChk = True\n\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0\nNFOLDS = 5\n\nif version.split(\".\")[1] == \"20\":\n    from sklearn.model_selection import KFold # 0.20 Use this API\n    kf = KFold(n_splits = NFOLDS, random_state=SEED)\nelse:\n    from sklearn.cross_validation import KFold # 0.18 will be removed\n    kf = KFold(n_train, n_folds = NFOLDS, random_state = SEED)\n    versionChk = False\ndisplay(ntrain, ntest)","a0ad6085":"class SklearnHelper(object):\n    \"\"\"\n    \uc0ac\uc774\ud0b7\ub7f0\uc758 \uac1d\uccb4\ub97c \uc0c1\uc18d\ubc1b\uace0, \uc0ac\uc774\ud0b7\ub7f0\uc5d0\uc11c \uc81c\uacf5\ud558\ub294 \uba54\uc18c\ub4dc\ub97c \uad6c\ud604\ud558\uc5ec \n    \ud559\uc2b5, \uc608\uce21, \ud53c\ucc98 \uc911\uc694\ub3c4 \uad6c\ud604\n    \"\"\"\n    def __init__(self, clf, seed, params):\n        params[\"random_state\"] = seed\n        self.clf = clf(**params)\n        \n    def get_name(self):\n        return self.clf.__class__.__name__\n    \n    def train(self, X_train, y_train):\n        self.clf.fit(X_train, y_train)\n        \n    def predict(self, X):\n        \"\"\"\n        \uc778\uc790 - \ud14c\uc2a4\ud2b8\uc138\ud2b8\n        \"\"\"\n        return self.clf.predict(X)\n    \n    def feature_importance(self, X, y):\n        \"\"\"\n        \ud53c\ucc98\uc758 \uc911\uc694\ub3c4\ub97c \ucd9c\ub825\n        X - \ud53c\ucc98 \ub370\uc774\ud130\n        y - \ub808\uc774\ube14 \ub370\uc774\ud130 \n        \"\"\"\n        model = self.clf\n        model.fit(X, y)\n        display(model.feature_importances_)\n        return model.feature_importances_","9976ded7":"def loop_get_oof(clfs, X_train, y_train, X_test):\n    \"\"\"\n    Out Of Fold Prediction\ub97c \ubaa8\ub378\uc758 \uac2f\uc218\ub9cc\ud07c \ubc18\ubcf5 \uc218\ud589\n    \"\"\"\n    result = dict()\n    \n    for clf in clfs:\n        display(clf.get_name())\n        oof_train,oof_test = get_oof(clf, X_train, y_train, X_test)\n        result[clf] = (oof_train, oof_test)\n    return result","51ddd6e6":"oof_train = np.zeros((891,))\ndisplay('oof_train  :', oof_train.shape)\n\noof_test  = np.zeros((481, ))\ndisplay('oof_test  :', oof_test.shape)\n\noof_test_skf = np.empty((5, 481))\ndisplay('oof_test_skf  :', oof_test_skf.shape, oof_test_skf)","1d732653":"def get_oof(clf, X_train, y_train, X_test):\n    \"\"\"\n    OOF Prediction\n    \"\"\"\n    oof_train = np.zeros((ntrain, )) # 891\n    oof_test  = np.zeros((ntest,))   # 418\n    oof_test_skf = np.empty((NFOLDS, ntest))\n    \n    # scikitlearn\ubc84\uc804 0.18\uacfc 0.20\ubc84\uc804\uc5d0\uc11c \uc9c0\uc6d0\ud558\ub294 API\uac00 \ucc98\ub9ac\ubc29\uc2dd\uc774 \ub2e4\ub984\n    for i, (train_index, test_index) in enumerate(kf.split(X_train,y_train) if versionChk else kf):\n        X_tr = X_train[train_index]\n        y_tr = y_train[train_index]\n        X_te = X_train[test_index]\n        \n        clf.train(X_tr, y_tr)\n        \n        # \uac80\uc99d\ud3f4\ub4dc\ub97c \uc774\uc6a9\ud574 \uc608\uce21\ud55c \uacb0\uacfc\ub97c \ud559\uc2b5\ub370\uc774\ud130\ub85c \uc0dd\uc131\n        oof_train[test_index] = clf.predict(X_te)\n        \n        # \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc138\ud2b8\ub97c \uc774\uc6a9\ud574 \uc608\uce21\ud55c \uacb0\uacfc\ub97c \ud14c\uc2a4\ud2b8\ub370\uc774\ud130\ub85c\uc0dd\uc131\n        oof_test_skf[i,:] = clf.predict(X_test)\n        \n    oof_test[:] = oof_test_skf.mean(axis = 0) #\ucd5c\uc885 \ud3c9\uade0\uac12\uc744 OOF\ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\ub85c \uc0dd\uc131\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","e6535194":"#hyper parameter for RandomForestClassifier\nrf_params = {\n    \"n_jobs\": -1,\n    \"n_estimators\":500,\n    \"warm_start\": True,  # \uac1d\uccb4 \uc0dd\uc131\ud55c \uac83\uc744 \uc7ac\uc0ac\uc6a9,\n    #\"max_feature\":0.2,\n    \"max_depth\":6,\n    \"min_samples_leaf\": 2,\n    \"max_features\": \"sqrt\",\n    \"verbose\": 0\n}\n\n# hyper parameter for ExtraTreeClassifier\next_params = {\n    \"n_jobs\": -1,\n    \"n_estimators\": 500,\n    #\"max_features\": 0.2,\n    \"max_depth\": 6,\n    \"min_samples_leaf\": 2,\n    \"max_features\": \"sqrt\",\n    \"verbose\": True\n}\n\n#hyper parameter for AdaBoostClassifer\nada_params = {\n    \"n_estimators\": 500,\n    \"learning_rate\": 0.75\n}\n\n#hyper parameter for GradientBoost\ngb_params = {\n    'n_estimators': 500,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# hyper parameter for SVC\nsvc_params = {\n    'kernel' : 'linear',\n    'C' : 0.025\n}","8a44a326":"# Create 5 objects that represent our 4 models\nrf = SklearnHelper(clf = RandomForestClassifier, seed = SEED, params = rf_params)\net = SklearnHelper(clf = ExtraTreesClassifier, seed = SEED, params = ext_params)\nada = SklearnHelper(clf = AdaBoostClassifier, seed = SEED, params = ada_params)\ngb  = SklearnHelper(clf = GradientBoostingClassifier, seed = SEED, params = gb_params)\nsvm = SklearnHelper(clf = SVC, seed = SEED, params = svc_params)","d8a299b9":"# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train[\"Survived\"].ravel() # ravel()\uc73c\ub85c pd.Series\ud0c0\uc785\uc73c\ub85c 1\ucc28\uc6d0 ndarray\ud0c0\uc785\uc73c\ub85c \ubcc0\ud658, \ud0c0\uac9f\uac12\uc744 1\ucc28\uc6d0\ubc30\uc5f4\ub85c ML\ubaa8\ub378\uc5d0 \ud559\uc2b5\uc2dc \uc774\ub807\uac8c \ub4e4\uc5b4\uac10\ntrain = train.drop([\"Survived\"], axis=1, errors='ignore')\n\n# ML\ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ud0a4\uae30 \uc704\ud574\uc11c\ub294 ndarray\ub85c \ubcc0\ud658\uc774 \ud544\uc694\nX_train = train.values  #  DataFrame\uc744 ndarray\ub85c \ubcc0\ud658  - (891, 11)   \nX_test  = test.values   #  DataFrame\ub97c ndarray\ub85c \ubcc0\ud658  - (481, 11)","75cc6534":"# Create our OOF train and test predictions. These base results will be used as new features\nclfs = [rf, et, ada, gb, svm]\nresult = loop_get_oof(clfs, X_train, y_train, X_test)","9c089bb8":"for k, v in result.items():\n    clf_name = k.clf.__class__.__name__\n    if clf_name ==\"RandomForestClassifier\":\n        print(\"OOF Train : \", v[0][1:5])\n        print(\"OOF Test : \", v[1][1: 5])","e22a7def":"def get_oof_train_test(clf):\n    \"\"\"\n    OOF\uacb0\uacfc \ub9ac\ud134 \ubc1b\uc740 \uacb0\uacfc\uac12\uc5d0\uc11c \ud544\uc694\ub85c \ud558\ub294 \ubd84\ub958\uae30\uc758 \ucd5c\uc885 \ud559\uc2b5 \ubc0f \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc138\ud2b8 \n    \uacb0\uacfc \uac00\uc838\uc624\uae30\n    \"\"\"\n    r0 = [v[0] for k, v in result.items() if k.clf.__class__.__name__ == clf]\n    r1 = [v[1] for k, v in result.items() if k.clf.__class__.__name__ == clf]\n    \n    return r0, r1  ","53c47fc2":"rf_oof_train, rf_oof_test = get_oof_train_test('RandomForestClassifier')\net_oof_train, et_oof_test = get_oof_train_test('ExtraTreesClassifier')\nada_oof_train, ada_oof_test = get_oof_train_test('AdaBoostClassifier')\ngb_oof_train, gb_oof_test = get_oof_train_test('GradientBoostingClassifier')\nsvm_oof_train, svm_oof_test = get_oof_train_test('SVC')","a404c5ae":"rf_feature = rf.feature_importance(X_train, y_train)\net_feature = et.feature_importance(X_train, y_train)\nada_feature = ada.feature_importance(X_train, y_train)\ngb_feature = gb.feature_importance(X_train, y_train)","ecab6094":"rf_feature = rf.clf.feature_importances_\net_feature = et.clf.feature_importances_\nada_feature = ada.clf.feature_importances_\ngb_feature = gb.clf.feature_importances_","66c0b1a3":"rf_feature, et_feature, ada_feature, gb_feature","7b292f8a":"cols = train.columns.values\ndata = {\n    \"feature\": cols, \n    \"Ranfom Forest Feature Importances\": rf_feature,\n    \"Extra Tree Feature Imortances\": et_feature,\n    \"AdaBoost Feature Importances\": ada_feature,\n    \"Gradient Boost Feature Imortances\": gb_feature\n}\n\nfeature_dataframe = pd.DataFrame(data)\ndisplay(feature_dataframe)","dfe700a1":"feature_dataframe.info()","24a12fad":"# \ubd84\ub958\uae30\ubcc4\ub85c \ud53c\ucc98 \uc911\uc694\ub3c4\ub97c \uadf8\ub9ac\uae30\uc704\ud568\n# ['Ranfom Forest Feature Importances','Extra Tree Feature Imortances', 'AdaBoost Feature Importances', 'Gradient Boost Feature Imortances']\ncols = feature_dataframe.columns.values[1:].tolist()\n\nfor col in cols:\n    trace = go.Scatter(\n        y = feature_dataframe[col].values,\n        x = feature_dataframe[\"feature\"].values,\n        mode = \"markers\",\n        marker = dict(\n            sizemode=\"diameter\",\n            sizeref = 1,\n            size = 25,\n            #       size= feature_dataframe['AdaBoost feature importances'].values,\n            #color = np.random.randn(500), #set color equal to a variable\n            color = feature_dataframe[col].values,\n            colorscale=\"Portland\",\n            showscale=True\n        ),\n        text = feature_dataframe[\"feature\"].values\n    )\n    data = [trace]\n    layout  = go.Layout(autosize = True, title = col, hovermode='closest', \n                        #     xaxis= dict(\n                        #         title= 'Pop',\n                        #         ticklen= 5,\n                        #         zeroline= False,\n                        #         gridwidth= 2,\n                        #     ),\n                        yaxis = dict(\n                            title=\"Feature Importances\",\n                            ticklen =5,\n                            gridwidth=2\n                        ),\n                        showlegend = False\n                       )\n    fig = go.Figure(data = data , layout=layout)\n    py.iplot(fig, filename=\"Scatter2010\")","7b65e72f":"feature_dataframe[\"mean\"] = feature_dataframe.mean(axis = 1)\nfeature_dataframe.head()","c6494fab":"y = feature_dataframe[\"mean\"].values # \ubd84\ub958\uae30\ubcc4 \ud53c\ucc98\uc911\uc694\ub3c4\uc758 \ud3c9\uade0\uac12\nx = feature_dataframe[\"feature\"].values # \ud53c\ucc98\uba85\n\n# data region\ndata = [\n    go.Bar(\n        x = x,\n        y = y,\n        marker = dict(\n            color=feature_dataframe[\"mean\"].values,\n            colorscale=\"Portland\",\n            showscale=True,\n            reversescale=False  # False\uc774\uba74 \ud53c\ucc98\uc911\uc694\ub3c4\uac00 \ub192\uc740 \ud53c\ucc98\uac00 \uc0c9\uae54\uc774 \ubc1d\uc740 \uc0c9 \uacc4\uc5f4\ub85c \uc2dc\uc791, True\uc774\uba74 \uc774\uc640 \ubc18\ub300\uc784\n        ),\n        opacity=0.6\n    )\n]\n\n#\uadf8\ub798\ud504\uc758 \ubc30\uce58 \ubc0f \ub208\uae08 \nlayout = go.Layout(\n    autosize=True,\n    title = \"BarPlot of Mean Feature Importance\",\n    hovermode=\"Closest\",\n    #xaxis=dict(\n     #   title=\"Feature Mean\",\n     #   ticklen=5,\n     #   zeroline=False,\n     #   gridwidth=2\n    #),\n    yaxis=dict(\n        title=\"Feature Importances\",\n        ticklen=5,\n        gridwidth=2        \n    ),\n    showlegend=False\n)\n\nfig = go.Figure(data = data, layout=layout)\npy.iplot(fig, filename=\"barplot_featureimportance\")","5172654f":"display(type(rf_oof_train[0]))\ndisplay(rf_oof_train[0].shape)","0700500b":"data = {\n        \"RandomForest\": rf_oof_train[0].ravel(),\"ExtraTrees\": et_oof_train[0].ravel(),\"AdaBoost\": ada_oof_train[0].ravel(),\"GradientBoost\": gb_oof_train[0].ravel()\n    }\nbase_predicitons_train = pd.DataFrame(data)\nbase_predicitons_train.head()","dd0dcccd":"data = [\n    go.Heatmap(\n        z = base_predicitons_train.astype(float).corr().values,\n        x = base_predicitons_train.columns.values,\n        y = base_predicitons_train.columns.values,\n        colorscale=\"Viridis\",\n        showscale=True,\n        reversescale=True  # \uc0c9\uae54\uc774 \ubc1d\uc744\uc218\ub85d \ud53c\ucc98\uc758 \uc601\ud5a5\ub3c4\ub294 \uc801\uc74c\uc744 \uc758\ubbf8\n    )\n]\npy.iplot(data, filename=\"labelled-heatmap\")","cccb3e67":"# \ub9ac\ud134\ud55c \ud55c \ud0c0\uc785\uc774 \ub9ac\uc2a4\ud2b8\uc548\uc5d0 \ubc30\uc5f4\ub85c \ub4e4\uc5b4\uac00\uc788\uc73c\ub2c8\uae4c...ndarray\ud0c0\uc785\uc73c\ub85c \ud574\uc57c \ud558\ub2c8 \uc778\ub371\uc2a4 [0]\uc744 \ucde8\ud55c \uac83.\nX_train = np.concatenate((et_oof_train[0], rf_oof_train[0], ada_oof_train[0], gb_oof_train[0], svm_oof_train[0]), axis = 1)\nX_test  = np.concatenate((et_oof_test[0], rf_oof_test[0], ada_oof_test[0], gb_oof_test[0], svm_oof_test[0]), axis= 1)","e1582e71":"gbm = XGBClassifier(n_estimators=2000,\n                   learning_rate=0.02,\n                   max_depth=4,\n                   min_child_weight=2,\n                   gamma=0.9,\n                   subsample=0.8,\n                   colspample_bytree=0.8,\n                   objective=\"binary:logistic\",\n                   nthreads=-1,\n                   scale_pos_weight=1)\ngbm.fit(X_train, y_train)\npredictions = gbm.predict(X_test)","991a082f":"StackingSubmission = pd.DataFrame(\n    {\n                \"PassengerId\": PassengerId,\n                \"Survived\": predictions\n    }\n)\ndisplay(StackingSubmission.head(n=3))","c1fe1ab8":"StackingSubmission.to_csv(target_dir + \"StackingSubmission.csv\", index = False)","3dfa13d3":"**Output of the First level Predictions** \n\n\ud559\uc2b5 \ubc0f \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\ub97c 5\uac1c\uc758 ML\ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ud0a4\uace0, OOF Prediction\uc744 \uc218\ud589","ad251b49":"# Generating our Base First-Level Models \n\n\uba3c\uc800 \uc559\uc0c1\ube14\uc758 \uccab\ubc88\uc9f8 \ub2e8\uacc4\ub85c 5\uac1c\uc758 ML\ubaa8\ub378\uc744 \ub300\uc0c1\uc73c\ub85c \ud55c\ub2e4.\n\n 1. Random Forest classifier\n 2. Extra Trees classifier\n 3. AdaBoost classifer\n 4. Gradient Boosting classifer\n 5. Support Vector Machine","cf04d30b":"**\uc0ac\uc774\ud0b7\ub7f0\uc758 \uac1d\uccb4\ub97c \uc0c1\uc18d\ubc1b\uc544 \ud559\uc2b5\ubc0f \uc608\uce21 \uac1d\uccb4 \uc0dd\uc131**","4140851b":"**\ud53c\ucc98 \uc911\uc694\ub3c4\ub97c DataFrame\ud615\ud0dc\ub85c \uc0dd\uc131**","37f4e848":"### Out-of-Fold Predictions\n\n\ud559\uc2b5 \uc138\ud2b8\uc5d0\ub9cc \ucd5c\uc801\ud654\ub418\ub294 \uc624\ubc84\ud53c\ud305\uc744 \ubc29\uc9c0\ud558\uae30 \uc704\ud55c OOF Predict\uad6c\ud604","0320bb86":"**Interactive feature importances via Plotly scatterplots**\n\n* Potly\ub97c \uc774\uc6a9\ud574 Scatter\uadf8\ub9ac\uae30\n* [plotly tutorial for Python](https:\/\/plotly.com\/python\/)","03ab2bad":"** Correlation Heatmap of the Second Level Training set**","9d1e0a04":"# Feature Exploration, Engineering and Cleaning \n\n\ubc94\uc8fc\ud615 \ud53c\ucc98\uc5d0 \ub300\ud574\uc11c \uc22b\uc790\ud615\uc73c\ub85c \uc778\ucf54\ub529, \ud53c\ucc98 \uc5d4\uc9c0\ub2c8\uc5b4\ub9c1, EDA\ubc0f \ub370\uc774\ud130 \uc815\uc81c \uc9c4\ud589","4bbb3990":"# \uc559\uc0c1\ube14 \ubc0f \uc2a4\ud0dc\ud0b9 \uc18c\uac1c\n\n\uc774 \uce90\uae00 \ub178\ud2b8\ubd81\uc740 \uc2a4\ud0dc\ud0b9\uc73c\ub85c \uc54c\ub824\uc9c4 \uc559\uc0c1\ube14 \uae30\ubc95\uc744 \uc18c\uac1c\ud558\uace0\uc790 \ud569\ub2c8\ub2e4.\n\n\uc559\uc0c1\ube14 \uae30\ubc95\uc740 \uc880 \ub35c \ub5a8\uc5b4\uc9c0\ub294 \uc5ec\ub7ec \ub300\uc758 \ubd84\ub958\uae30\uac00 \ud559\uc2b5 \ub370\uc774\ud130\ub97c \ud559\uc2b5 \ubc0f \uc608\uce21\ud558\uc5ec \ub098\uc628 \uc608\uce21 \uacb0\uacfc\ub97c \ub2e4\uc218\uacb0\ub85c \uacb0\uc815\ud558\ub294 \ubc29\uc2dd\uacfc \ub3d9\uc77c\ud55c \ubd84\ub958\uae30\n\n\ub97c \uc0ac\uc6a9\ud558\uc9c0\ub9cc \uac01\uac01\uc758 \ud559\uc2b5 \ub370\uc774\ud130\ub97c \uc0d8\ud50c\ub9c1\uc744 \ub2ec\ub9ac\ud558\uc5ec \uc608\uce21\ud55c \uacb0\uacfc\ub97c \ud3c9\uade0\uac12\uc73c\ub85c \uacb0\uc815\ud558\ub294 \ubc29\uc2dd\uc774 \uc788\ub2e4.\n\n\uc2a4\ud0dc\ud0b9\uae30\ubc95\uc740 \uac01\uac01\uc758 \ubd84\ub958\uae30\uac00 \ud559\uc2b5 \ud6c4 \uc608\uce21\ud55c \uacb0\uacfc\ub97c \ubccf\uc9da \uc313\ub4ef\uc774 \ucc28\uace1\ucc28\uace1 \uc313\uc544\uc11c \uc774\ub97c \ub2e4\uc2dc \ucd5c\uc885 \uba54\ud0c0 \ubd84\ub958\uae30\uac00 \ub2e4\uc2dc \ud55c\ubc88 \ud559\uc2b5 \ubc0f \uc608\uce21\ud558\ub294 \uae30\ubc95.\n\n\ud574\ub2f9 \ub178\ud2b8\ubd81\uc740 [Stacking Starter][1]: by Faron  , [arthurtok][2] : by arthurtok \uc5d0 \uc758\ud574 \uc4f0\uc5ec\uc9c4 \uce90\uae00 \ub178\ud2b8\ubd81\uc744 \ucc38\uace0\ud588\uc2b5\ub2c8\ub2e4.\n\n\ub610\ud55c \ucd5c\uadfc\uc758 [Introduction to Ensembling\/Stacking in Python][3] \uc744 \ubc14\ud0d5\uc73c\ub85c \uc791\uc131\ub41c \uce90\uae00\uc784\uc744 \ubc1d\ud799\ub2c8\ub2e4. \n\n\ud53c\ucc98\uc5d4\uc9c0\ub2c8\uc5b4\ub9c1\uc5d0 \ub300\ud55c \uc88b\uc740 \uce90\uae00\uc740 [Titanic Best Working Classfier][4] \uc774 \uae00\uc744 \ucc38\uace0\ud558\uc154\ub3c4 \uc88b\uc2b5\ub2c8\ub2e4.\n\n\"\ucc45\uc744 \uc77d\ub294\ub2e4\ub294 \uac83\uc740 \ub2e8\uc21c\ud788 \ud65c\uc790\ub97c \uc77d\ub294\ub2e4\ub294 \uac83\uc774 \uc544\ub2c8\ub77c , \uadf8 \uc2dc\ub300\ub97c \uc77d\ub294\uac83\uc774\ub2e4\"\n\n  [1]: https:\/\/www.kaggle.com\/mmueller\/allstate-claims-severity\/stacking-starter\/run\/390867\n  [2]: https:\/\/www.kaggle.com\/arthurtok\/titanic\/simple-stacking-with-xgboost-0-808\n  [3]: https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python\n  [4]: https:\/\/www.kaggle.com\/sinakhorami\/titanic\/titanic-best-working-classifier","3ee4a2df":"**Feature importances generated from the different classifiers**\n\n* feature_importances_\ub97c \ud1b5\ud574\uc11c \ud53c\ucc98\uc758 \uc911\uc694\ub3c4 \ubcf4\uae30","f930a954":"* \uc704\uc758 \uacb0\uacfc\uac12\uc744 \ubcc0\uc218\uc5d0 \ud560\ub2f9 - \uc704\uc758 \ud615\ud0dc\ub85c \ubc14\ub85c \ud074\ub798\uc2a4\uac1d\uccb4 \ubc14\ub85c \ud638\ucd9c\ud574\ubd04.","4ebf1446":"## \uc2dc\uac01\ud654 ","5fcc0600":"# \uc559\uc0c1\ube14 \ubc0f \uc2a4\ud0dc\ud0b9 \ubaa8\ub378\n\n* \uc2a4\ud0dc\ud0b9 \uc559\uc0c1\ube14 \ubaa8\ub378 ","41d4e226":"### XGBoost\ub97c \uc774\uc6a9\ud55c \uba54\ud0c0\ubaa8\ub378 \ud559\uc2b5 \ubc0f \uc608\uce21\n\nXGBoost\ub97c \ucd5c\uc885 \uba54\ud0c0\ubaa8\ub378 \ubd84\ub958\uae30\ub85c \uc0ac\uc6a9 \ud560 \uac83\uc774\uace0 \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \uacf5\uc2dd \uc0ac\uc774\ud2b8\ub97c \ucc38\uc870\ud558\uae30 \ubc14\ub78d\ub2c8\ub2e4.\n\n[official Documentation][1].\n\n  [1]: https:\/\/xgboost.readthedocs.io\/en\/latest\/","167a00b3":"### Helpers via Python Classes\n\nsklearnHelper\ud074\ub798\uc2a4\ub97c \uc0dd\uc131\ud558\uc5ec \ud559\uc2b5 \ubc0f \uc608\uce21\uc744 \uc218\ud589\ud558\ub294 \uba54\uc18c\ub4dc \uc0dd\uc131","5991a9c2":"* Scatter Plot using Plotly","7a264446":"**First-level output as new features**","377771bd":"**Plotly Barplot of Average Feature Importances**\n\n\ud53c\ucc98 \uc911\uc694\ub3c4\uc758 \ud3c9\uade0\uc744 \ubc14\ucc28\ud2b8\ub85c \ud45c\uc2dc","c42b02ee":"**Parameters**\n\n**n_jobs** : \ud559\uc2b5\uc2dc\uc5d0 \uc0ac\uc6a9\ud560 CPU\ucf54\uc5b4 \uc218.  -1\uc758 \uc758\ubbf8\ub294 \ubaa8\ub4e0 CPU\ucf54\uc544\ub97c \ub2e4 \uc0ac\uc6a9\n\n**n_estimators** : ML\ubaa8\ub378 \ubd84\ub958\uae30\uc758 \uc218 ( \ub514\ud3f4\ud2b8\ub294 10)\n\n**max_depth** : \ud2b8\ub9ac\uc758 \ucd5c\ub300 \uae4a\uc774 \uc218(\ud06c\uac8c \uc124\uc815\ub420\uc218\ub85d \uc624\ubc84\ud53c\ud305\uc774 \uc704\ud5d8\uc774 \uc874\uc7ac\ud568\uc744 \uc720\ub150)\n\n**verbose** : \ud559\uc2b5\uc774 \uc9c4\ud589\ub420 \ub3d9\uc548 \ucd9c\ub825\uacb0\uacfc\ub97c \ubcf4\uace0 \uc2f6\uc744 \uacbd\uc6b0 \uc124\uc815( 0 - \uacb0\uacfc\ub97c \ubcf4\uc774\uc9c0 \uc54a\uc74c, 1- \uacb0\uacfc\ubcf4\uae30)","4a5d60cf":"### \uacb0\ub860\n\n\uc2a4\ud0dc\ud0b9 \ubc0f \uc559\uc0c1\ube14 \uae30\ubc95\uc5d0 \ub300\ud55c \uc88b\uc740 \uc790\ub8cc \ucc38\uc870 \n\nwebsite MLWave: [Kaggle Ensembling Guide][1]. \n\n[1]: http:\/\/mlwave.com\/kaggle-ensembling-guide\/","7444eec0":"**Pearson Correlation Heatmap**\n\n\ud53c\ucc98\uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ud55c\ubc88 \ubcf4\ub3c4\ub85d \ud558\uc790","d3bd8683":"* Feature Importance\uac12\uc758 \ud3c9\uade0\uac12\uc744 \uc0c8\ub85c\uc6b4 \ud53c\ucc98\ub85c \uc0dd\uc131","d1cbbda4":"\ubc94\uc8fc\ud615 \ubcc0\uc218(categorical variables)\uc5d0\uc11c \uc778\uc0ac\uc774\ud2b8\ub97c \ucc3e\uc544 \uc0c8\ub85c\uc6b4 \ud30c\uc0dd\ubcc0\uc218\ub97c \uc0dd\uc131\ud558\ub294 \uac83\uc774 \ud53c\ucc98 \uc5d4\uc9c0\ub2c8\uc5b4\ub9c1\uc774\ub77c\uace0 \ud560 \uc218 \uc788\ub2e4.\n\n**\ud53c\ucc98 \uc5d4\uc9c0\ub2c8\uc5b4\ub9c1**\n\n\ud53c\ucc98 \uc5d4\uc9c0\ub2c8\uc5b4\ub9c1\uc5d0 \ub300\ud574\uc11c \uc544\ub798 \ucf00\uae00\uc758 \ub178\ud2b8\ubd81\uc744 \ucc38\uace0\ud558\uc2dc\uba74 \uc88b\uc744 \uac83 \uac19\uc2b5\ub2c8\ub2e4.\n\n[Titanic Best Working Classfier][1] : by Sina\n\n  [1]: https:\/\/www.kaggle.com\/sinakhorami\/titanic\/titanic-best-working-classifier","8decf0db":"**\uac01\uac01\uc758 \ubd84\ub958\uae30\uc5d0 \ub300\ud55c \ud559\uc2b5 \ubc0f \ud14c\uc2a4\ud2b8 \uacb0\uacfc\uac12**","083fd561":"**\uacb0\uacfc\ub97c \ub2f4\uace0\uc788\ub294 \ubcc0\uc218\uc5d0\uc11c \ucd5c\uc885\ud559\uc2b5 \ubc0f \ud14c\uc2a4\ud2b8\ub370\uc774\ud130\uc14b \ucd94\ucd9c**","bce39140":"# Second-Level Predictions from the First-level Output","e0882733":"**[ML OOF Prediciton](https:\/\/techblog-history-younghunjo1.tistory.com\/142)**","407cb7bc":"**\ucd5c\uc885 \uacb0\uacfc \uc81c\ucd9c \ud30c\uc77c**","d1c4bd00":"**Takeaway from the Plots**\n\nOne thing that the Person Correlation plot can tell us is that there are not too many features strongly correlated with one another. \n\nThis is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant of superfluous data in out training set and we are happy that each feature carries with it some unique information.\n\nHere are two most correlated features are thart of Famil Size and Parch(Parents and Children).\n\nFamilySize\uc640 \uac00\uc7a5 \uc5f0\uad00\uc774 \uc788\ub294 \ud53c\ucc98\ub294 Parch(Parents and Children)\uc774\ub2e4. \n\n\ud788\ud2b8\ub9f5\uc744 \ubcf4\uba74, \uac15\ud55c \uc0c1\uad00\uad00\uacc4\ub97c \uac16\ub294 \ud53c\ucc98\ub4e4\uc774 \uadf8\ub807\uac8c \ub9ce\uc774 \ubcf4\uc774\uc9c0 \uc54a\ub294\ub2e4\ub294 \uc810\uc774\ub2e4. ML\ubaa8\ub378\uc744 \ud559\uc2b5\ud558\uae30\uc5d0\ub294 \uc88b\uc740 \ub370\uc774\ud130\uc758 \ubd84\ud3ec\ub77c\uace0 \ud560\uc218\uc788\ub2e4.(\uc911\ubcf5\ub418\ub294 \ud53c\ucc98\uac00 \uc801\uace0, \ud559\uc2b5\uc138\ud2b8\uc5d0 \uacfc\uc801\ud569\ub418\ub294 \uc0ac\ub840\uac00 \uc904\uc5b4\ub4ec)\n\n\n**Pairplots**\n\nPairplots\ub294 \ud55c \ud53c\ucc98\uc640 \ub2e4\ub978 \ud53c\ucc98\uc640\uc758 \ub370\uc774\ud130 \ubd84\ud3ec\ub3c4\ub97c \ud655\uc778\ud558\ub294 \uac83\n\n[PairPlot Document](https:\/\/seaborn.pydata.org\/generated\/seaborn.pairplot.html?highlight=pairplot#seaborn.pairplot)","a7357b5f":"XGBoost \ub300\ud45c\uc801\uc778 \ud30c\ub77c\ubbf8\ud130:\n\n**max_depth** : \ud2b8\ub9ac\uc758 \uae4a\uc774 \uc81c\uc5b4(related to the machine learning model overfitting)\n\n**gamma** : \uc624\ubc84\ud53c\ud305\uc744 \uc704\ud55c \uc81c\uc5b4\uac12\n\n**eta** : learning_rate\uc640 \uc720\uc0ac\ud558\ub2e4\uace0 \ubcf4\uba74 \ub428","6b51e6f3":"1\ucc28 base estimator\uc5d0\uc11c \ub098\uc628 \ud559\uc2b5 \ubc0f \ud14c\uc2a4\ud2b8 \uc138\ud2b8\ub97c \ubcd1\ud569\ud574 \uba54\ud0c0 \uba38\uc2e0 \ub7ec\ub2dd \ubaa8\ub378\uc5d0\uc11c \uc0ac\uc6a9","cd185d2d":"**Creating NumPy arrays out of our train and test sets**\n\n\ud559\uc2b5 \ubc0f \ud14c\uc2a4\ud2b8 \uc138\ud2b8\ub85c \ubd80\ud130 \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ud0ac \ub370\uc774\ud130\uc138\ud2b8 \uc0dd\uc131","0f1d4a11":"# Load library"}}