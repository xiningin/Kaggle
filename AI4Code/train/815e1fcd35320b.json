{"cell_type":{"7cb46a66":"code","241bddde":"code","09ab2152":"code","de6c4dad":"code","d7557ee1":"code","b930eb13":"code","17386870":"code","435dea52":"code","cafc3c27":"code","6d02e7f5":"code","0dcda21c":"code","0614ec01":"code","5708150a":"code","03a0f5b6":"code","52f9d298":"code","a4fed9ca":"code","cc1b63a3":"code","50bbca73":"code","5b0b0a62":"code","e499d2f3":"code","cb49e884":"code","e8d56459":"code","1d81f33f":"code","ec552033":"code","814cda79":"code","ce7378da":"code","039d24ae":"code","25f916af":"code","c77b9592":"code","d7aa5c81":"code","7668bfbf":"code","e5175d3a":"code","7f8b914e":"markdown","7cd74b3c":"markdown","86b892f0":"markdown","637bda6f":"markdown","bd4ad173":"markdown","fd393bd7":"markdown","ffa930c8":"markdown","351852e3":"markdown","fe4072ab":"markdown","0238ca73":"markdown","0849fa9c":"markdown","4320b015":"markdown","e2d8184b":"markdown","59e5f084":"markdown","58d847b6":"markdown","279ca151":"markdown","5eba0d7f":"markdown","49c10ef7":"markdown"},"source":{"7cb46a66":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport nltk\nfrom nltk.stem.wordnet import WordNetLemmatizer \nimport string\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\nimport os\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","241bddde":"# Load the data\ndata = pd.read_csv('..\/input\/train.csv')\ndata.head()","09ab2152":"print(\"There is {} messages.\".format(len(data)))","de6c4dad":"classes = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\noccurence = []\nprint(\"\\n{:^15} | {:^15} | {:^5}\".format(\"Class\", \"Occurrence\", \"%\"))\nprint(\"*\"*42)\nfor clas in classes:\n    print(\"{:15} | {:>15} | {:^5.2f}\".format(clas, \n                                             data[clas].value_counts()[1], \n                                             data[clas].value_counts()[1]*100\/len(data)\n                                            )\n         )\n    occurence.append(data[clas].value_counts()[1])","d7557ee1":"plt.figure(figsize=(10, 6))\nplt.bar(classes, occurence)\nplt.title(\"Number of comments per category\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('category', fontsize=12)","b930eb13":"data['all'] = data[classes].sum(axis=1)\ndata['any'] = data['all'].apply(lambda x:1 if x>0 else 0)\ndata.head()","17386870":"in_classes = data['all'].value_counts()\nprint(\"\\n{:^10} | {:^10} | {:^6}\".format(\"# Classes\", \"# Comment\", \"%\"))\nprint(\"*\"*33)\nfor idx in range(7):\n    print(\"{:10} | {:>10} | {:>6.2f}\".format(idx, \n                                             in_classes[idx], \n                                             in_classes[idx]*100\/len(data)\n                                            )\n         )\nprint(\"*\"*33)\nprint(\"{:^10} | {:>10} | {:>6}\".format(\"\", len(data), \"100.00\"))","435dea52":"df = pd.DataFrame(in_classes.values)","cafc3c27":"ax = df.plot.bar(stacked=True, figsize=(10, 6), legend=False)\nax.set_ylabel('# of Occurrences', fontsize=12)\nax.set_xlabel('# of classes', fontsize=12)\nax.set_title(\"# of messages per # of classes associated\")","6d02e7f5":"# toxic\ndata[data['toxic']==1].iloc[1,1]","0dcda21c":"# severe_toxic\ndata[data['severe_toxic']==1].iloc[2,1]","0614ec01":"# obscene\ndata[data['obscene']==1].iloc[3,1]","5708150a":"# threat\ndata[data['threat']==1].iloc[4,1]","03a0f5b6":"# insult\ndata[data['insult']==1].iloc[5,1]","52f9d298":"# identity_hate\ndata[data['identity_hate']==1].iloc[6,1]","a4fed9ca":"lens = data['comment_text'].str.len()\nlens.head()","cc1b63a3":"# Statistics:\nprint('Minimum : ', lens.min())\nprint('Maximum : ', lens.max())\nprint('Median : ', lens.median())","50bbca73":"# horizontal boxplot\nplt.figure(figsize=(15,4))\nplt.boxplot(lens, 0, 'gD', 0, showmeans=True)\n# The length of comment text is varying a lot. There is a lot of outlier.","5b0b0a62":"# Split data using stratifying variable \"all\" to take into account the imbalanced data throw calsses\ndatatrain, datatest = train_test_split(data, test_size=0.2, stratify=data[\"all\"], random_state=42)","e499d2f3":"# Here we create a list of noisy entities\nuseless_words = nltk.corpus.stopwords.words(\"english\") + list(string.punctuation) + [\"\\'m\"] + [\"\\'s\"] + [\"\\'\\'\"] + [\"``\"] + [\"n\\'t\"] + [\"ca\"]","cb49e884":"lem = WordNetLemmatizer()\ndef clean_data(txt):\n    txt = nltk.word_tokenize(txt.lower())\n    txt = [word for word in txt if not word in useless_words]\n    txt = [lem.lemmatize(w, \"v\") for w in txt]\n    return ' '.join(word for word in txt)","e8d56459":"# datatest['comment_text'] = datatest['comment_text'].apply(lambda x:clean_data(x))\n# datatrain['comment_text'] = datatrain['comment_text'].apply(lambda x:clean_data(x))\n\ndatatest['comment_text'] = datatest['comment_text'].apply(lambda x:clean_data(x))\ndatatrain['comment_text'] = datatrain['comment_text'].apply(lambda x:clean_data(x))","1d81f33f":"def ROC_curve_plot(datatest, prediction, classes, figure_title):\n    # Compute ROC curve and ROC area for each class\n    nbr_classes = len(classes)\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n\n    y = np.zeros(nbr_classes*len(datatest))\n    y_hat = np.zeros(nbr_classes*len(datatest))\n\n    for idx,clas in enumerate(classes):\n        print('... Processing {}'.format(clas))\n        print('Cofusion Matrix:\\n', confusion_matrix(datatest[clas], prediction[:,idx]))\n        fpr[clas], tpr[clas], _ = roc_curve(datatest[clas], prediction[:,idx])\n        roc_auc[clas] = auc(fpr[clas], tpr[clas])\n\n        y[idx*len(datatest):(idx+1)*len(datatest)] = datatest[clas].values\n        y_hat[idx*len(datatest):(idx+1)*len(datatest)] = prediction[:,idx]\n        \n    # Compute average ROC curve and ROC area\n    fpr[\"all\"], tpr[\"all\"], _ = roc_curve(y, y_hat)\n    roc_auc[\"all\"] = auc(fpr[\"all\"], tpr[\"all\"])\n    \n    plt.figure(figsize=(10,10))\n    for i in [\"all\"] + classes:\n        plt.plot(fpr[i], tpr[i], label='{0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.xlabel('False Positive Rate', fontsize=12)\n    plt.ylabel('True Positive Rate' , fontsize=12)\n    plt.title(figure_title,           fontsize=12)\n    plt.legend(loc=\"lower right\",     fontsize=12)\n    plt.show()","ec552033":"NB_pipeline = Pipeline([\n                        ('tfidf', TfidfVectorizer()),\n                        ('clf', OneVsRestClassifier(MultinomialNB(fit_prior=True, class_prior=None))),\n                       ])\n\nNB_pipeline.fit(datatrain['comment_text'], datatrain[classes])\nprediction = NB_pipeline.predict(datatest['comment_text'])\n\nROC_curve_plot(datatest, prediction, classes, 'ROC curve : Naive Bayes Classifier')","814cda79":"SVC_pipeline = Pipeline([\n                         ('tfidf', TfidfVectorizer()),\n                         ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n                        ])\n\nSVC_pipeline.fit(datatrain['comment_text'], datatrain[classes])\nprediction = SVC_pipeline.predict(datatest['comment_text'])\n\nROC_curve_plot(datatest, prediction, classes, 'ROC curve : Linear SVC Classifier')","ce7378da":"LogReg_pipeline = Pipeline([\n                            ('tfidf', TfidfVectorizer()),\n                            ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1)),\n                           ])\n\nLogReg_pipeline.fit(datatrain['comment_text'], datatrain[classes])\nprediction = LogReg_pipeline.predict(datatest['comment_text'])\n\nROC_curve_plot(datatest, prediction, classes, 'ROC curve : Logistic Regression Classifier')","039d24ae":"# from sklearn.ensemble import RandomForestClassifier\n# RandomForest_pipeline = Pipeline([\n#                             ('tfidf', TfidfVectorizer()),\n#                             ('clf', OneVsRestClassifier(RandomForestClassifier(), n_jobs=1)),\n#                            ])\n\n# RandomForest_pipeline.fit(datatrain['comment_text'], datatrain[classes])\n# prediction = RandomForest_pipeline.predict(datatest['comment_text'])\n\n# ROC_curve_plot(datatest, prediction, classes, 'ROC curve : Random Forest Classifier')","25f916af":"from xgboost import XGBClassifier\nXGBoost_pipeline = Pipeline([\n                            ('tfidf', TfidfVectorizer()),\n                            ('clf', OneVsRestClassifier(XGBClassifier(), n_jobs=1)),\n                           ])\n\nXGBoost_pipeline.fit(datatrain['comment_text'], datatrain[classes])\nprediction = XGBoost_pipeline.predict(datatest['comment_text'])\n\nROC_curve_plot(datatest, prediction, classes, 'ROC curve : XGBoost Classifier')","c77b9592":"### Decision tree classifier","d7aa5c81":"from sklearn.tree import DecisionTreeClassifier\nDecisionTree_pipeline = Pipeline([\n                            ('tfidf', TfidfVectorizer()),\n                            ('clf', OneVsRestClassifier(DecisionTreeClassifier())),\n                           ])\n\nDecisionTree_pipeline.fit(datatrain['comment_text'], datatrain[classes])\nprediction = DecisionTree_pipeline.predict(datatest['comment_text'])\n\nROC_curve_plot(datatest, prediction, classes, 'ROC curve : Decision Tree Classifier')","7668bfbf":"### Multi-layer Perceptron","e5175d3a":"# from sklearn.neural_network import MLPClassifier\n# MLPClassifier_pipeline = Pipeline([\n#                             ('tfidf', TfidfVectorizer()),\n#                             ('clf', OneVsRestClassifier(MLPClassifier())),\n#                            ])\n\n# MLPClassifier_pipeline.fit(datatrain['comment_text'], datatrain[classes])\n# prediction = MLPClassifier_pipeline.predict(datatest['comment_text'])\n\n# ROC_curve_plot(datatest, prediction, classes, 'ROC curve : Multi-layer Perceptron Classifier')","7f8b914e":"                                        Area Under the ROC Curve Table","7cd74b3c":"#### Lexicon Normalization\n* Stemming:  Stemming is a rudimentary rule-based process of stripping the suffixes (\u201cing\u201d, \u201cly\u201d, \u201ces\u201d, \u201cs\u201d etc) from a word.\n* Lemmatization: Lemmatization, on the other hand, is an organized & step by step procedure of obtaining the root form of the word, it makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).","86b892f0":"### Logistic Regression","637bda6f":"## Natural Language Processing (NLP)","bd4ad173":"### Naive Bayes","fd393bd7":"### Text Preprocessing\n#### Cleaning data (Noise Removal)\nAny piece of text which is not relevant to the context of the data and the end-output can be specified as the noise.\n\nFor example \u2013 language stopwords (commonly used words of a language \u2013 is, am, the, of, in etc), URLs or links, social media entities (mentions, hashtags), punctuations and industry specific words. This step deals with removal of all types of noisy entities present in the text.\n\nA general approach for noise removal is to prepare a dictionary of noisy entities, and iterate the text object by tokens (or by words), eliminating those tokens which are present in the noise dictionary.","ffa930c8":"# Toxic Comment Classification Challenge\n**Identify and classify toxic online comments**\n\nThe data is a collection of comment text that has been classified throw six classes.\nThe competition consist on predicting negative online behaviours, like toxic comments (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion). So the goal is to create a classification model that can perform the highest accuracy.","351852e3":"### Text to Features (Feature Engineering on text data)\nTo analyse a preprocessed data, it needs to be converted into features. Depending upon the usage, text features can be constructed using assorted techniques \u2013 Syntactical Parsing, Entities \/ N-grams \/ word-based features, Statistical features, and word embeddings.\n#### Term Frequency \u2013 Inverse Document Frequency (TF \u2013 IDF)\nTF-IDF is a weighted model commonly used for information retrieval problems. It aims to convert the text documents into vector models on the basis of occurrence of words in the documents without taking considering the exact ordering. For Example \u2013 let say there is a dataset of N text documents, In any document \u201cD\u201d, TF and IDF will be defined as \u2013\n\nTerm Frequency (TF) \u2013 TF for a term \u201ct\u201d is defined as the count of a term \u201ct\u201d in a document \u201cD\u201d\n\nInverse Document Frequency (IDF) \u2013 IDF for a term is defined as logarithm of ratio of total documents available in the corpus and number of documents containing the term T.","fe4072ab":"**Comment text behavior**\n\nLet's look at the length of the comment text","0238ca73":"**Examples of toxic message**","0849fa9c":"### XGBoost","4320b015":"### Random Forest","e2d8184b":"**Remarks**\n\n9.58% of the messages are considered toxic and 1% are considered severe toxic. \n\nBut a message can belong to more than one class so let's take a look:","59e5f084":"**Comment classes**\n\nLet's look at the different classes and how many comment by class. That is clear we have an imbalanced data throw calsses. When we encounter such problems, we are bound to have difficulties solving them with standard algorithms. Conventional algorithms are often biased towards the majority class, not taking the data distribution into consideration. In the worst case, minority classes are treated as outliers and ignored. For some cases, such as fraud detection or cancer prediction, we would need to carefully configure our model or artificially balance the dataset, for example by undersampling or oversampling each class.\n\nHowever, in our case of learning imbalanced data, the majority classes might be of our great interest. It is desirable to have a classifier that gives high prediction accuracy over the majority class, while maintaining reasonable accuracy for the minority classes. Therefore, we will leave it as it is.","58d847b6":"**Remarks**\n\n3.99% of messages belong only to one class and 2.18% belong to two classes. \n\nThere is 31 messages that belong to all classes.","279ca151":"### LinearSVC","5eba0d7f":"| Classifier | ALL   |Toxic  | Severe Toxic   |Obscene  | Threat   |Insult  | Identity Hate   |\n|------|------|------|------|------|------|------|------|\n|   Naive Bayes | 0.55|   0.58  | 0.50|   0.55  | 0.50|   0.52  | 0.50|\n|   LinearSVC  | 0.81|   0.84  | 0.64|   0.86  | 0.57|   0.78  | 0.62|\n|   Logistic Regression  | 0.77|   0.80  | 0.62|   0.82  | 0.53|   0.75  | 0.58|\n|   Random Forest  | 0.72|   0.74  | 0.53|   0.77  | 0.51|   0.70  | 0.53|\n|   XGBoost  | 0.71|   0.71  | 0.55|   0.78  | 0.55|   0.70  | 0.57|\n|   Decision tree classifier  | 0.82|   0.83  | 0.61|   0.88  | 0.61|   0.79  | 0.67|\n|   Multi-layer Perceptron  | 0.81|   0.84  | 0.64|   0.86  | 0.61|   0.78  | 0.66|","49c10ef7":"## Analyse the data"}}