{"cell_type":{"768e0a42":"code","1e5a1898":"code","4d328690":"code","c2ba681d":"code","b1567ba3":"code","f6baba3b":"code","04694597":"code","5a4bc9a0":"code","d5f88c81":"code","69e1b3a7":"code","4d7d8ab1":"code","faaf95fa":"code","62a35a6f":"code","cf401424":"code","a76df397":"code","e19fdaf0":"code","e73463f4":"code","d2a76ab1":"code","5d5f13e4":"code","3ae2c64e":"code","f87da64e":"code","508aa0c7":"code","9e5e28bb":"code","c371f692":"code","e79cdc51":"code","9e8a787e":"code","b9b7a5ea":"code","b8c4cf2d":"code","8a272585":"code","470cd0a7":"code","be3141ef":"code","641e582a":"code","10591999":"code","137e96d7":"code","d70e60cf":"code","ff67f593":"markdown","d2293cfd":"markdown","27bfa622":"markdown","c2607dd0":"markdown","193dbc0e":"markdown","35485d6d":"markdown","5237946a":"markdown","e7c45661":"markdown","f45fa069":"markdown","02309d14":"markdown","450db09b":"markdown","44900b2b":"markdown","b8d0854a":"markdown","39f4861b":"markdown","dd692aa3":"markdown","4de0f6d9":"markdown","b6405f8a":"markdown","bc91fa43":"markdown","5d07ac2b":"markdown"},"source":{"768e0a42":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1e5a1898":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sklearn","4d328690":"bbc_text = pd.read_csv('..\/input\/bbc-fulltext-and-category\/bbc-text.csv')\nbbc_text","c2ba681d":"bbc_text.category.unique()","b1567ba3":"bbc_text.category = bbc_text.category.map({'tech':0, 'business':1, 'sport':2, 'entertainment':3, 'politics':4})\nbbc_text.category.unique()","f6baba3b":"bbc_text.info()","04694597":"bbc_text.shape","5a4bc9a0":"# bbc_news = bbc_text.values\n\nX = bbc_text.text\ny = bbc_text.category\n\n#split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, random_state = 1)\nprint(X_train)\nprint(y_train)","d5f88c81":"# countVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvec = CountVectorizer(stop_words = 'english')","69e1b3a7":"# fit the vectorizer on the training data\n\nvec.fit(X_train)\nprint(len(vec.get_feature_names()))\nvec.vocabulary_","4d7d8ab1":"# another way of representing the features\nX_transformed = vec.transform(X_train)\nX_transformed","faaf95fa":"print(X_transformed)","62a35a6f":"X_transformed.toarray()","cf401424":"# convert X_transformed to sparse matrix, just for readability.\npd.DataFrame(X_transformed.toarray(), columns= [vec.get_feature_names()])","a76df397":"# for test data\nX_test_transformed = vec.transform(X_test)\nX_test_transformed","e19fdaf0":"print(X_test_transformed)","e73463f4":"# convert X_transformed to sparse matrix, just for readability\npd.DataFrame(X_test_transformed.toarray(), columns= [vec.get_feature_names()])","d2a76ab1":"from sklearn.linear_model import LogisticRegression\n\nlogit = LogisticRegression()\nlogit.fit(X_transformed, y_train)","5d5f13e4":"# fit\nlogit.fit(X_transformed,y_train)\n\n# predict class\ny_pred_class = logit.predict(X_test_transformed)\n\n# predict probabilities\ny_pred_proba = logit.predict_proba(X_test_transformed)","3ae2c64e":"# printing the overall accuracy\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test, y_pred_class)","f87da64e":"confusion = metrics.confusion_matrix(y_test, y_pred_class)\nprint(confusion)\nTN = confusion[0, 0]\nFP = confusion[0, 1]\nFN = confusion[1, 0]\nTP = confusion[1, 1]","508aa0c7":"sensitivity = TP \/ float(FN + TP)\nprint(\"sensitivity\",sensitivity)\n\nspecificity = TN \/ float(TN + FP)\nprint(\"specificity\",specificity)","9e5e28bb":"print(\"PRECISION SCORE :\",metrics.precision_score(y_test, y_pred_class, average = 'micro'))\nprint(\"RECALL SCORE :\", metrics.recall_score(y_test, y_pred_class, average = 'micro'))\nprint(\"F1 SCORE :\",metrics.f1_score(y_test, y_pred_class, average = 'micro'))","c371f692":"from sklearn.naive_bayes import MultinomialNB\n\nnb = MultinomialNB()\nnb.fit(X_transformed, y_train)","e79cdc51":"# fit\nnb.fit(X_transformed,y_train)\n\n# predict class\ny_pred_class = nb.predict(X_test_transformed)\n\n# predict probabilities\ny_pred_proba = nb.predict_proba(X_test_transformed)","9e8a787e":"# printing the overall accuracy\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test, y_pred_class)","b9b7a5ea":"# confusion matrix\nmetrics.confusion_matrix(y_test, y_pred_class)\n# help(metrics.confusion_matrix)","b8c4cf2d":"confusion = metrics.confusion_matrix(y_test, y_pred_class)\nprint(confusion)\nTN = confusion[0, 0]\nFP = confusion[0, 1]\nFN = confusion[1, 0]\nTP = confusion[1, 1]","8a272585":"sensitivity = TP \/ float(FN + TP)\nprint(\"sensitivity\",sensitivity)\n\nspecificity = TN \/ float(TN + FP)\nprint(\"specificity\",specificity)","470cd0a7":"print(\"PRECISION SCORE :\",metrics.precision_score(y_test, y_pred_class, average = 'micro'))\nprint(\"RECALL SCORE :\", metrics.recall_score(y_test, y_pred_class, average = 'micro'))\nprint(\"F1 SCORE :\",metrics.f1_score(y_test, y_pred_class, average = 'micro'))","be3141ef":"s1 = ['FIR against Delhi Minorities Commission chairman for inflammatory content on social media']\nvec1 = vec.transform(s1).toarray()\nprint('Headline:' ,s1)\nprint(str(list(nb.predict(vec1))[0]).replace('0', 'TECH').replace('1', 'BUSINESS').replace('2', 'SPORTS').replace('3','ENTERTAINMENT').replace('4','POLITICS'))","641e582a":"relabel = {'0': 'tech', '1': 'business', '2': 'sport', '3': 'entertainment', '4': 'politics'}","10591999":"s2 = ['Need to restart economy but with caution: Yogi Adityanath at E-Agenda AajTak']\nvec2 = vec.transform(s2).toarray()\nprint('Headline:' ,s2)\nprint(str(list(nb.predict(vec2))[0]).replace('0', 'TECH').replace('1', 'BUSINESS').replace('2', 'SPORTS').replace('3','ENTERTAINMENT').replace('4','POLITICS'))","137e96d7":"s3 = ['2 doctors attacked in Andhra Pradesh Vijayawada']\nvec3 = vec.transform(s3).toarray()\nprint('Headline:', s3)\nprint(str(list(nb.predict(vec3))[0]).replace('0', 'TECH').replace('1', 'BUSINESS').replace('2', 'SPORTS').replace('3','ENTERTAINMENT').replace('4','POLITICS'))","d70e60cf":"s4 = ['If I bat for an hour, you\u2019ll see a big one: How Dravid spelt doom for Pak']\nvec4 = vec.transform(s4).toarray()\nprint('Headline:', s4)\nprint(str(list(nb.predict(vec4))[0]).replace('0', 'TECH').replace('1', 'BUSINESS').replace('2', 'SPORTS').replace('3','ENTERTAINMENT').replace('4','POLITICS'))","ff67f593":"We dont use sparse matrix while model building as it unnecessarily creates a dimensionality expansion, where other than a single position all postion carry zero value","d2293cfd":"## Model Evaluation Logistic Regression","27bfa622":"### Logistic Regression","c2607dd0":"------------------------------------------------------------------------------------------------------------","193dbc0e":"Lets now build a Naive Bayes model, and see if we get any better results","35485d6d":"## Importing Libraries","5237946a":"We will use the BBC headline news text, labeled in 5 categories, i.e., 'Tech', 'Sports', 'Business', 'Entertainment', and 'Politics', and train our model with Logistic Regression and Naive Bayes.\n\nFinally we will try using some random out of the dataset headlines to test whether our model correctly classifies them into respective label class.","e7c45661":"# BBC News Headlines Classification Model","f45fa069":"#### Our Naive Bayes model is performing pretty well on random News Healines out of the dataset !","02309d14":"There are a totla of 5 classes of news that we have here in our dataset. As our model would require it in numeric form, lets map it to numeric form.","450db09b":"### Naive Bayes","44900b2b":"Both the Logistic Regression as well as the Naive Bayes model offer similar performance. We will go ahead choosing Naive bayes as our final model","b8d0854a":"   >Lets choose random news headlines from the internet, and see if our model perform well in classifying them","39f4861b":"### Train Test Split","dd692aa3":"## Building the model","4de0f6d9":"## Model Evaluation Naive Bayes","b6405f8a":"## Data Reading and Understanding","bc91fa43":"## Test from random data outside the dataset","5d07ac2b":"### Creating the Bag of Words Representation\n\nWe now have to convert the data into a format which can be used for training the model. We'll use the **bag of words representation** for each sentence (document).\n\nImagine breaking X in individual words and putting them all in a bag. Then we pick all the unique words from the bag one by one and make a dictionary of unique words. \n\nThis is called **vectorization of words**. We have the class ```CountVectorizer()``` in scikit learn to vectorize the words. \n\nWe will also use the `stop_words` in english to clear our data of stop words.\n"}}