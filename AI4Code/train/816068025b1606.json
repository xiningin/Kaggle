{"cell_type":{"85a28d4c":"code","1fca78c8":"code","73ebb370":"code","80e63c8e":"code","d3ba7247":"code","2262209a":"code","bfe03798":"code","d8e9f9bb":"code","10a8f73e":"code","138adafe":"code","b6d6fe4d":"code","b39a898b":"code","0d82b971":"code","8da94a2e":"markdown","5aa178fc":"markdown","797bc398":"markdown","95414982":"markdown","8cdebf8d":"markdown","1748454f":"markdown","bd8ee06a":"markdown","f09da285":"markdown","da2890d2":"markdown","9eec3892":"markdown","15c9d60b":"markdown","06cf61e6":"markdown","808e712f":"markdown","73ea7f81":"markdown"},"source":{"85a28d4c":"# Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","1fca78c8":"# Read the file\ndata = pd.read_csv(\"..\/input\/voice.csv\")\nprint(data.info())","73ebb370":"data.label = [1 if each == \"female\" else 0 for each in data.label]\nprint(data.info())","80e63c8e":"y = data.label.values\nx_data = data.drop([\"label\"],axis=1)","d3ba7247":"\"\"\"\n(x - min(x))\/(max(x)-min(x))\n\"\"\"\nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values","2262209a":"# %% train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)","bfe03798":"print(x_train.shape, y_train.shape)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(x_train.shape, y_train.shape)","d8e9f9bb":"def initial_w_b(dimension):    \n    w = np.full((dimension,1), 0.01) # (20,1)\n    b = 0.0\n    return w,b\n\ndef sigmoid(z):\n    y_head = 1 \/ (1+np.exp(-z))\n    return y_head\n\nprint(sigmoid(0)) # result must be 0.5 (test)","10a8f73e":"def propagation_forward_backward(w, b, x_train, y_train):\n    # Forward propagation\n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    loss = -(1-y_train)*np.log(1-y_head) - y_train*np.log(y_head) # formula, y:y_train\n    cost = (np.sum(loss))\/x_train.shape[1]  # x_train.shape[1] is for scaling\n    \n    # Backward Propagation\n    derivative_weight = ( np.dot(x_train,((y_head-y_train).T)) )\/x_train.shape[1] # x_train.shape[1] is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 # x_train.shape[1] is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","138adafe":"def update(w, b, x_train, y_train, learning_rate, number_of_iteration):\n    cost_list = []\n    cost_plot = []\n    for i in range(number_of_iteration):\n        # (1) make Forward and Backward propagation\n        # (2) find the Cost and Gradients\n        cost, gradients = propagation_forward_backward(w,b,x_train,y_train)\n        cost_list.append(cost)   \n        # Update:\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"] \n        \n        # This section is just for plotting.\n        if i % 10 == 0:\n            cost_plot.append(cost)\n            print (\"Cost after iteration {0} : {1}\".format(i, cost))\n            \n    # update (learn) the parameters; weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    \n    plt.plot(cost_plot)\n    plt.xticks(rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    return parameters, gradients, cost_list","b6d6fe4d":"def prediction(w, b, x_test):\n    # x_test is an input for the forward propagation\n    y_head_test = sigmoid( np.dot(w.T , x_test) + b )\n    Y_prediction = np.zeros( (1, x_test.shape[1]) )\n    \n    for i in range(y_head_test.shape[1]):\n        if y_head_test[0,i] <= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n        \n    # if y_head_test is bigger than 0.5, our prediction is one (Y_prediction=1),\n    # if y_head_test is smaller than 0.5, our prediction is zero (Y_prediction=0)\n    return Y_prediction","b39a898b":"def logistic_regression (x_train, y_train, x_test, y_test, learning_rate ,  number_of_iteration):\n    # initialize\n    dimension = x_train.shape[0] # 20\n    w, b = initial_w_b(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,number_of_iteration)\n    \n    y_prediction_test = prediction(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, number_of_iteration = 1000)    \n","0d82b971":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver='lbfgs')\n# solver : str, {\u2018newton-cg\u2019, \u2018lbfgs\u2019, \u2018liblinear\u2019, \u2018sag\u2019, \u2018saga\u2019}, optional (default=\u2019liblinear\u2019).\n# Algorithm to use in the optimization problem.\n# If we do not specify it, this error will be occured:\n# FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning. FutureWarning)\nlr.fit(x_train.T,y_train.T)\n\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))","8da94a2e":"<a id=\"1\"><\/a> <br>\n# ** 1. Preparing the Logistic Regression Model**\n<a id=\"1.1\"><\/a> <br>\n**1.1. Normalization (ML)**\n<br>\nSome values on the features may be much more or less than the others, so it may cause to ignore their effect. Therefore, we will normalize the data (fix the values between the 0 and 1). After the normalization; min(x) will be 0, max(x) will be 1.","5aa178fc":"<a id=\"1.2\"><\/a> <br>\n**1.2. Train and Test Splits of the Data**\n\nTrain the model with the \"sklearn\" library.<br>\n%80 of the data is for training,<br>\n%20 of the data is for test.\n\nSo, we set \"test_size\" as 0.2.","797bc398":"<a id=\"3\"><\/a>\n# ** 3. Logistic Regression with Sklearn**","95414982":"Loss function:\n\n![](https:\/\/image.ibb.co\/eC0JCK\/duzeltme.jpg)","8cdebf8d":"Weights will have the size of the feature. We will multiply them (w) with every sample.<br>\nOur data has 3168 sample and 20 features.<br>","1748454f":"<a id=\"1.6\"><\/a> <br>\n** 1.6. Prediction**","bd8ee06a":"<a id=\"1.5\"><\/a> <br>\n** 1.5. Update (Learning) the Parameters**","f09da285":"1. [Preparing the Logistic Regression Model](#1)\n    1. [Normalization](#1.1)\n    1. [Train and Test Splits of the Data](#1.2)\n    1. [Initial Values of the Model](#1.3)\n    1. [Forward and Backward Propagation](#1.4)\n    1. [Update (Learning) the Parameters](#1.5)\n    1. [Prediction](#1.6)\n1. [Logistic Regression](#2)\n1. [Logistic Regression with Sklearn](#3)","da2890d2":"<a id=\"1.4\"><\/a> <br>\n** 1.4. Forward and Backward Propagation**","9eec3892":"\"label\" column's data type is object, but we want to see int (0 or 1).","15c9d60b":"voice.csv data shows the gender result; female voice or male voice. <br>\nhttps:\/\/www.kaggle.com\/duyguay12\/voice-gender-recognition-by-voice","06cf61e6":"<a id=\"1.3\"><\/a> <br>\n** 1.3. Initial Values of the Model**\n\nInitialization for the weight and bias.\n","808e712f":"<a id=\"2\"><\/a>\n# ** 2. Logistic Regression**","73ea7f81":"Now \"label\" column's data type is int.\n<br>\n1 : Female <br>\n0 : Male\n\nX axis means the features and Y axis means the result, 1 or 0."}}