{"cell_type":{"fc39d98c":"code","0d0adc98":"code","7044b03c":"code","f92d2647":"code","c0e351e5":"code","2c43a55d":"code","16e24fcb":"code","2d0eef25":"code","e3351ced":"code","905e4e95":"code","e04a8527":"code","674087b6":"code","fdebbeaf":"code","5ce98892":"code","16afb3bf":"code","f8224821":"code","9f08e1c9":"code","56c048c3":"code","478e3a59":"code","5fae512b":"code","f00afe54":"code","9c2674ce":"code","5ec60ffb":"code","f062f815":"code","c591bf9c":"code","9cdce3b7":"code","a5bcc5bf":"code","689c74b4":"code","b6411331":"code","6a0b9068":"code","13e48a75":"code","f56fb6b9":"code","764c403f":"code","73d69504":"code","e982c8ec":"code","e1d209ac":"code","a9e58499":"code","31cdfabb":"code","b311850b":"code","f1d758cd":"code","cc224d59":"code","323fc31e":"code","83232c34":"code","ba4c1cc5":"code","25285622":"markdown","ae01b31c":"markdown","c9c5a228":"markdown","431806d4":"markdown"},"source":{"fc39d98c":"#data analysis\nimport numpy as np\nimport pandas as pd\nimport re\n\n#visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set(style=\"ticks\", color_codes=True)\n\n#machine learning\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, Imputer, StandardScaler, Normalizer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression","0d0adc98":"train_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')\ncombine_data = [train_data, test_data]\nprint(train_data.info())\nprint(test_data.info())","7044b03c":"#numeric data\ntrain_data.describe()","f92d2647":"#categorical data\ntrain_data.describe(include=['O'])","c0e351e5":"#view first 5 rows\nprint(train_data.head())","2c43a55d":"#view Name feature, top 5 rows\nprint(train_data.Name.head())","16e24fcb":"#Survival rate per Class. \"1\" class passengers have highest survival rate.\ntrain_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index = False).mean()","2d0eef25":"#Survival rate Gender-wise. \n#Females-  74.2% have survived. \n#Males- 18.89% have survived.\ntrain_data[['Sex', 'Survived']].groupby(['Sex'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)","e3351ced":"#compared to singles (no siblings and spouse), small families have higher survival rate\n#Lagre families didnt survive\ntrain_data[['SibSp', 'Survived']].groupby(['SibSp'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)","905e4e95":"#Same observation as \"SibSp\"\n#Small families have higher survival rate, compared to passengers travelling alone and larger families\ntrain_data[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending = False)","e04a8527":"#\"SibSp\" and \"Parch\" both indicate the family size. So let us have single feature \"Familysize\"  \n#and drop these 2 features\ntrain_data['Familysize'] = train_data['Parch'] + train_data['SibSp'] + 1\ntest_data['Familysize'] = test_data['Parch'] + test_data['SibSp'] + 1\n\ntrain_data = train_data.drop(['Parch', 'SibSp'], axis = 1)\ntest_data = test_data.drop(['Parch', 'SibSp'], axis = 1)\ncombine_data = [train_data, test_data]","674087b6":"#\"Cabin\" has lot of missing values. 204\/891 are available. It doesnt help in analysis. So, drop it.\n#\"Ticket\" value is unique for 681\/891 records. Alpha numeric ticket number, shared by 210 records. \n#No visible pattern. So drop these two columns it doesnt help in analysis.\n\ntrain_data = train_data.drop(['Cabin', 'Ticket'], axis = 1)\ntest_data = test_data.drop(['Cabin', 'Ticket'], axis = 1)\n\ncombine_data = [train_data, test_data]","fdebbeaf":"# Define function to extract titles from passenger names\ndef extract_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\n# Create a new feature Title, containing the titles of passenger names\nfor dataset in combine_data:\n    dataset['Title'] = dataset['Name'].apply(extract_title)\n\n#Check if the Titles are properly aligned as per the Sex of the passenger\npd.crosstab(train_data['Title'], train_data['Sex'])\n\n#Title feature will help in data analysis. So, retain it.\n","5ce98892":"#Some rare titles can be grouped together, which have very few ( 2-3) passengers mapped.\nfor dataset in combine_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","16afb3bf":"#Survival rate based on Title\ntrain_data[['Title', 'Survived']].groupby(['Title'], as_index = False).mean()\n\n#Name feature can be dropped, as we have extracted the Title and this is not used further for analysis.\n#PassengerId is just a sequential numbering for the data records. Can be droppped\ntrain_data = train_data.drop(['Name', 'PassengerId'], axis = 1)\ntest_data = test_data.drop(['Name', 'PassengerId'], axis = 1)\ncombine_data = [train_data, test_data]","f8224821":"title_mapping = {'Mr' : 1, 'Miss' : 2, 'Master' : 3, 'Mrs' : 4, 'Rare' : 5}\nfor dataset in combine_data:\n    dataset['Titlecode'] = dataset['Title'].map(title_mapping)\n    dataset['Titlecode'] = dataset['Title'].fillna(0)    \ntrain_data['Titlecode'] = train_data['Title'].map(title_mapping)\ntest_data['Titlecode'] = test_data['Title'].map(title_mapping)\ncombine_data = [train_data, test_data]","9f08e1c9":"#Most of the passengers who have paid extremely higher price (Fare>80) have survived\n#Most of the Passengers who have paid extremely lower price (Fare<20) have nor survived\n# ore Females have paid higher Fare\ngrid = sns.FacetGrid(train_data, col='Survived', hue = \"Sex\")\ngrid.map(plt.hist, 'Fare', alpha=.5, bins = 6)\ngrid.add_legend();","56c048c3":"# Comparitively more Female pssengers have paid higher Fare\n# Age >65 senior citizens are Male passengers\n# oldest Male passenger has survived.\n# High Fare paying passengers have survived.\ngrid = sns.FacetGrid(train_data, col = 'Survived', hue='Sex')\ngrid.map(plt.scatter, 'Fare','Age')\ngrid.add_legend();","478e3a59":"#Third class (Pclass = 3) embarked in either \"S\" or \"Q\"\n#First class (Pclass = 1) embarked mostly in \"C\".\ngrid = sns.FacetGrid(train_data, hue='Embarked')\ngrid.map(plt.hist, 'Pclass',alpha = 0.9, bins=5)\ngrid.add_legend()","5fae512b":"#First class passengers (Pclass = 1) have mostly embarked in \"C\". Had good survival rate.\n#Third class passengers(PClass = 3) embarked in \"S\"have suffered the most.\ngrid = sns.FacetGrid(train_data, col = 'Survived', hue='Embarked')\ngrid.map(plt.hist, 'Pclass',alpha = 0.9, bins=5)\ngrid.add_legend()","f00afe54":"#Singles and larger families (Familysize>4) have least survival rate\ngrid = sns.FacetGrid(train_data, col = 'Survived', hue='Sex')\ngrid.map(plt.hist, 'Familysize',alpha = 0.6, bins=11)\ngrid.add_legend()","9c2674ce":"#Singles and larger families travelled in third class (Pclass = 3)\n#Smaller families and fewer singles travelled in first or second class\nsns.catplot(y=\"Familysize\", hue=\"Pclass\", kind=\"count\", edgecolor=\".6\",data=train_data)","5ec60ffb":"#Survival rate Familysize-wise. \n#Smaller size families have higher survival rate, together. Large families have perished together.\ntrain_data[['Familysize', 'Survived']].groupby(['Familysize'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)","f062f815":"#Males- 18.89% have survived.\ntrain_data[['Sex', 'Survived']].groupby(['Sex'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)\n\npd.crosstab(train_data['Embarked'], train_data['Sex'])","c591bf9c":"#Larger families (Familysize > 4) have embarked in \"S\". Have least survival rate\npd.crosstab(train_data['Familysize'], [train_data['Survived'], train_data['Embarked']])","9cdce3b7":"#Females, minor Passengers embarked in \"C\" and \"Q\" have mostly survived.\npd.crosstab(train_data['Title'], [train_data['Survived'], train_data['Embarked']])","a5bcc5bf":"#Almost all the children, Females travelling in first and second class have Survived. \n#However, the children travelling in third class (embarked in \"S\") have been penalised along with their parents.\n#So families sink or survive together.\npd.crosstab(train_data['Title'], [train_data['Survived'], train_data['Pclass']])","689c74b4":"grid = sns.FacetGrid(train_data, col = 'Survived', row = 'Pclass', height = 2.2, aspect = 2 )\ngrid.map(plt.hist, 'Age', bins = 10)","b6411331":"grid = sns.FacetGrid(train_data, row = 'Embarked')\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex',palette = 'deep')\ngrid.add_legend()","6a0b9068":"grid = sns.FacetGrid(train_data, row = 'Embarked', col = 'Survived')\ngrid.map(sns.barplot, 'Sex', 'Fare')","13e48a75":"guessed_ages = np.zeros(5)\nguessed_ages","f56fb6b9":"#Impute the missing \"Age\" data. \n#Find the Median age per Title and use that to impute missing data\nfor dataset in combine_data:   \n    for i in range(0,5):        \n        guess_age = dataset[(dataset['Titlecode'] == i+1)]['Age'].dropna()\n        guess_age = guess_age.median()\n        guess_age = int( guess_age\/0.5 + 0.5 ) * 0.5 \n        guessed_ages[i] = guess_age\n      \n    for i in range(0,5):          \n        dataset.loc[ (dataset.Age.isnull()) & (dataset.Titlecode == i+1), 'Age'] = guessed_ages[i]\n    \n    dataset['Age'] = dataset['Age'].astype(int)   ","764c403f":"#Impute the missing \"Embarked\" data\n#889 records have not null values. two records have missing values. \n#These can be the Mode of the dataset.\ntrain_data['Embarked'].count() \nfreq_embarked = train_data['Embarked'].mode()[0]\n#print(freq_embarked)\ntrain_data.loc[(train_data.Embarked.isnull()), 'Embarked'] = freq_embarked\n","73d69504":"test_data['Fare'].fillna(test_data['Fare'].dropna().median(), inplace=True)","e982c8ec":"#place the passengers in the bins based on their Age. \n#\"pd.cut\" divides the data based on the Age, into 5 bins\n#train_data['AgeBins'] --> holds array-like object representing respective Age bin for each passenger\n\n\n#Also, place the passengers in the bins based on the Fare.\n#\"pd.qcut\" divides the data based on ticker fare, into ordered 5 bins.\n#train_data['FareBin'] --> holds array-like object representing respective Fare bin for each passenger\nfor dataset in combine_data:\n    dataset['AgeBin'] = pd.cut(dataset['Age'], 5)\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)","e1d209ac":"# Mapping Fare\nfor dataset in combine_data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\ntrain_data.head()","a9e58499":"# Mapping Age\nfor dataset in combine_data:\n    dataset.loc[ dataset['Age'] <= 16, 'Age']  = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 \n    dataset['Age'] = dataset['Age'].astype(int)\ntrain_data.head()","31cdfabb":"train_data = train_data.drop(['Title','AgeBin','FareBin'], axis = 1)\ntest_data = test_data.drop(['Title','AgeBin','FareBin'], axis = 1)\ncombine_data = [train_data, test_data]\n\ntrain_data.head()","b311850b":"for dataset in combine_data:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)","f1d758cd":"#there is a missing 'Sex' value in test_data\nfor dataset in combine_data:\n    dataset['Sex'] = dataset['Sex'].map({'male' : 0, 'female' : 1}).astype(int)\n\ntrain_data.head()","cc224d59":"#Now that all the categorical features are converted into numeric, \n#Sex --> Already one hot encoded #Age and Fare -->Ordinal data #Embarked, Pclass, Titlecode --> need to one hot encode\nfeatures = train_data[['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Familysize', 'Titlecode']]\ntarget = train_data['Survived']\npreprocess = make_column_transformer(    \n    (OneHotEncoder(sparse=False), ['Pclass', 'Sex', 'Embarked','Titlecode']), \n    remainder= StandardScaler())\n\npreprocess.transformers\ntrain_x = preprocess.fit_transform(features)\ntrain_y = target.values\n\ntrain_x[1]\n\n#Preprocess and transform test features\nfeatures_test = test_data[['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Familysize', 'Titlecode']]\npreprocess = make_column_transformer(    \n    (OneHotEncoder(sparse=False), ['Pclass', 'Sex', 'Embarked','Titlecode']), \n    remainder= StandardScaler())\npreprocess.transformers\ntest_x = preprocess.fit_transform(features_test)\n#model = make_pipeline(preprocess, LogisticRegression())\n#model.fit(train_x, train_y)\n#print(\"logistic regression score: %f\" % model.score(test_x, test_y))\n","323fc31e":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nimport keras\n#from keras.optimizers import SGD\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\nimport graphviz","83232c34":"#creating the model\ndef build_classifier(optimizer):  \n    #initialize ANN\n    classifier = Sequential()\n    #Adding the input parameters and first layer to ANN\n    classifier.add(Dense(units = 20, \n                         kernel_initializer = 'uniform', \n                         activation = 'relu', \n                         input_dim = 16))\n    \n    #Adding the dropout layer\n    classifier.add(Dropout(0.5))\n    \n    #Adding the second layer to ANN\n    classifier.add(Dense(units = 60, \n                         kernel_initializer = 'uniform', \n                         activation = 'relu'))\n    \n    #Adding the dropout layer\n    classifier.add(Dropout(0.5))\n    \n    #Adding the output layer that is binary\n    classifier.add(Dense(units = 1, \n                         kernel_initializer = 'uniform', \n                         activation = 'sigmoid'))\n    \n    classifier.summary()\n    \n    #with the scalar sigmoid output on a binary classification problem, use the binary cross entropy loss function\n    classifier.compile(optimizer = optimizer, \n                       loss = 'binary_crossentropy', \n                       metrics = ['accuracy'] )\n    \n    return classifier\n\n#Use KerasClassifier\nclassifier = KerasClassifier(build_fn = build_classifier)\n\n#create a dictionary for the hyper parameters\nparameters = {'batch_size' : [60, 30],\n               'nb_epoch' : [30, 50],\n               'optimizer' : ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']}\n\n# 'optimizer' : ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']}\n#GridSearchCV implements fit and predict\ngrid_search = GridSearchCV(estimator = classifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 3)\n\ngrid_search = grid_search.fit(train_x, train_y)\nbest_estimator = grid_search.best_estimator_\nbest_parameters = grid_search.best_params_\nbest_score = grid_search.best_score_\n\n","ba4c1cc5":"print(\"best_estimator %s\" %best_estimator)\nprint(\"best_parameters %s\" %best_parameters)\nprint(\"best_score %f\" %best_score)","25285622":"This is my my first kernel in kaggle to solve Titanic dataset.\nWould appreciate your feedback and comments. And don't forget to Like if you like it!","ae01b31c":"First look at the data shows that - Training has 891 records and Test has 418.\nThere are missing values in  Age, Cabin and Embarked.\nLets impute the missing values\n\nFor Age, let us substitute the mean of the Age grouped by sex, pclass, embarked","c9c5a228":"Categorical features - Sex, Embarked, Survived\nOrdinal - Pclass\nNumerical features -\n    Discrete - SibSp, Parch\n    Continous - Age, Fare","431806d4":"**Solution Approach that I have followed**\n1. Describe and visualize the data\n2. Impute any missing values\n3. Convert any Categorical features into numeric, one-hot encode.\n4. Find Co-relation among all the features and the solution goal\n5. Feature engineer and create\/ change\/ convert the features  \n6. Drop\/ discard any feature that is not contributing to the analysis.\n7. Check for any incorrect data, outliers\n"}}