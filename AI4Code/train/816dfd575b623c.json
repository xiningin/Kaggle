{"cell_type":{"408ba4a3":"code","f0bedaa9":"code","d69a68ea":"code","2332ea11":"code","939761ec":"code","b3895c8e":"code","ea9651f9":"code","f4e7484b":"code","8aae50e6":"code","6751e433":"code","7fe9893d":"code","8faf5741":"code","625cacdd":"code","374418b7":"code","1b7b1b6a":"code","942d0338":"code","d3ef3ca6":"code","4dac1b23":"code","46c728b1":"code","3fa85bd7":"code","50a7a466":"code","1b063c65":"code","b5813467":"code","2d3536c6":"code","3ca32c43":"code","edd91a9b":"code","3988ce4a":"code","f6ccee01":"code","6ea530f5":"code","b491f762":"code","ff162274":"code","863d396e":"code","f38add95":"code","d03e6fc4":"code","060a69c2":"code","d475a990":"code","ee014d86":"code","501f83cc":"code","4a317837":"code","c8ba3529":"code","5aa620a2":"code","c829bdce":"code","e4c23117":"code","d03464b2":"code","99d682ac":"code","9ed4ff50":"code","7fa5627c":"code","dc0a81a0":"code","cf4cc556":"code","140e9c31":"code","c916f302":"code","c3e3d15a":"code","ed2d2e9d":"code","f74165f3":"code","8ce365ff":"code","cd720bef":"code","1b0a5ecd":"markdown","2dcd7a95":"markdown","3f0f41d2":"markdown","804c8d0b":"markdown","d623d5a6":"markdown","9d264451":"markdown","fbefb42a":"markdown","b39cc4a4":"markdown","53f4010e":"markdown","e1c23697":"markdown","6551bc71":"markdown","620129ea":"markdown","23f43156":"markdown","590b6eff":"markdown","f454aadf":"markdown","18a0f8c8":"markdown","0108f146":"markdown","6cbe1bc7":"markdown","11b29f49":"markdown","795f6474":"markdown","0797e05e":"markdown","3e451960":"markdown","d30ed757":"markdown","062b20be":"markdown","8ae3725d":"markdown","43392888":"markdown","76c15d50":"markdown","90730449":"markdown","02ef40f6":"markdown","9d7f7ca2":"markdown","f28c856b":"markdown","deab1acb":"markdown","491965f3":"markdown","5e011e9a":"markdown","793cf7ba":"markdown","dbe13e44":"markdown","bed40e84":"markdown","815a52dd":"markdown","fba548af":"markdown","93ebe10e":"markdown","6ad15040":"markdown","8fa0c35a":"markdown","53e4cb95":"markdown","ffb0750b":"markdown","5ca53d3f":"markdown","c38fc101":"markdown","c9f59d00":"markdown","c7673565":"markdown","67339ade":"markdown","bbf46b64":"markdown","ebd2a948":"markdown","8afa6308":"markdown","275439f7":"markdown","d94a3b3e":"markdown","ff78f2bd":"markdown","de4c1d54":"markdown","2706f433":"markdown","2009262c":"markdown","de11db20":"markdown","ce4b5930":"markdown"},"source":{"408ba4a3":"# load the pandas library\nimport pandas as pd\n\n# load the data\ndemographic = pd.read_csv('..\/input\/national-health-and-nutrition-examination-survey\/demographic.csv', index_col=False)\nlabs = pd.read_csv('..\/input\/national-health-and-nutrition-examination-survey\/labs.csv', index_col=False)\nquestionnaire = pd.read_csv('..\/input\/national-health-and-nutrition-examination-survey\/questionnaire.csv', index_col=False)","f0bedaa9":"demographic.shape","d69a68ea":"demographic.head()","2332ea11":"# get only gender and age columns\ndemographic = demographic[['SEQN', 'RIDAGEYR', 'RIAGENDR']]\n\n# change the names of columns\ndemographic.rename(columns={'SEQN': 'ID', 'RIDAGEYR': 'Age', 'RIAGENDR': 'Gender'}, inplace=True)\n\ndemographic.head()","939761ec":"# Load the visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.countplot(demographic['Gender'], label='Count', palette='husl')\nplt.title(\"Distrubiton of Gender\")\nplt.show()","b3895c8e":"male = (demographic['Gender']==1).sum()\nfemale = (demographic['Gender']==2).sum()\n\nprint(\"There are {} male attenders\".format(male))\nprint(\"There are {} female attenders\".format(female))","ea9651f9":"print(\"There are {} different ages\".format(len(demographic['Age'].unique())))","f4e7484b":"labs.shape","8aae50e6":"labs.head()","6751e433":"#\u00a0Get the columns of id and B12\nlabs = labs[['SEQN', 'LBDB12']]\n\n# Change the names of columns\nlabs.rename(columns={'SEQN': 'ID', 'LBDB12': 'B12'}, inplace=True)\n\nlabs.head()","7fe9893d":"labs['B12'].dtype","8faf5741":"labs['B12'].isnull().sum()","625cacdd":"labs['B12'].describe()","374418b7":"questionnaire.shape","1b7b1b6a":"questionnaire.head()","942d0338":"# Get the columns\nquestionnaire = questionnaire[['SEQN', 'DPQ010', 'DPQ020', 'DPQ030', 'DPQ040', 'DPQ050', 'DPQ060', 'DPQ070', 'DPQ080', 'DPQ090', 'DPQ100', 'SLQ050', 'SLQ060', 'IND235']]\n\n# Change the name of id column\nquestionnaire.rename(columns={'SEQN': 'ID'}, inplace=True)\n\nquestionnaire.head()","d3ef3ca6":"questionnaire['DPQ020'].unique()","4dac1b23":"questionnaire['SLQ050'].unique()","46c728b1":"# categorize the data\ndemographic['Age'] = pd.cut(demographic['Age'], bins=[20, 30, 40, 50, 60, 70, 80], labels=[1, 2, 3, 4, 5, 6], include_lowest=True)\n\ndemographic.head()","3fa85bd7":"# visualize the distrubiton of age\nsns.countplot(demographic['Age'], palette='husl')\nplt.ylabel('Count')\nplt.title('Distrubiton of Age')\nplt.show()","50a7a466":"# Drop the records with null value in column B12\nlabs.dropna(axis=0, inplace=True)\n\nlabs.shape","1b063c65":"# Categorize the data\nlabs['B12'] = pd.cut(labs['B12'], bins=[0, 300, 950, 27000], labels=[1, 2, 3])\n\nlabs.head()\n","b5813467":"# Visualize the distribution of B12 values\nsns.countplot(labs['B12'], label='Count', palette='husl')\nplt.title('Distribution of B12')\nplt.show()","2d3536c6":"labs['B12'].value_counts()","3ca32c43":"data = labs.merge(demographic, on=\"ID\").merge(questionnaire, on=\"ID\")\n\ndata.head()","edd91a9b":"data.drop('ID', axis=1, inplace=True)\n\ndata.head()","3988ce4a":"data.hist(figsize=(15, 10),grid=False)\nplt.show()","f6ccee01":"data.info()","6ea530f5":"data.drop('DPQ100', axis=1, inplace=True)\ndata.head()","b491f762":"data.info()","ff162274":"data.dropna(subset=['DPQ010', 'DPQ020', 'DPQ030', 'DPQ040', 'DPQ050', 'DPQ060', 'DPQ070', 'DPQ080', 'DPQ090'], axis=0, inplace=True)\ndata.info()","863d396e":"data['IND235'].describe()","f38add95":"data['IND235'].replace({77: 7, 99: 7, None: 7}, inplace=True)\n","d03e6fc4":"data[['DPQ010', 'DPQ020', 'DPQ030', 'DPQ040', 'DPQ050', 'DPQ060', 'DPQ070', 'DPQ080', 'DPQ090', 'SLQ050', 'SLQ060']].replace({7: 0, 9: 0})\ndata.info()","060a69c2":"data['B12'] = data['B12'].astype(int)\ndata['Age'] = data['Age'].astype(int)","d475a990":"# load the numpy library\nimport numpy as np\n\n# graph size\nplt.figure(figsize=(15, 10))\n\n# to get the lower diagonal of the correlation matrix, using the numpy library\nmask = np.triu(data.iloc[:, 1:].corr())\n\nsns.heatmap(data.iloc[:, 1:].corr(), annot=True, mask=mask)\n\n# change the direction of ticks on y axis\nplt.yticks(rotation=0)\n\nplt.show()","ee014d86":"#\u00a0load the libraries\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.utils.extmath import density\n\nfrom sklearn import metrics\n\n# split the data into features and target\nX = data.iloc[:, 1:15]\ny = data.iloc[:, 0]\n\n# split the data into test and train\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True) # 70% training and 30% test","501f83cc":"from time import time\n# Benchmark classifier\ndef benchmark(clf, X_train, y_train, X_test, y_test):\n    print('_' * 80)\n    print(\"Training: \")\n    print(clf)\n    t0 = time()\n    clf.fit(X_train, y_train)\n    train_time = time() - t0\n    print(\"train time: %0.3fs\" % train_time)\n\n    t0 = time()\n    pred = clf.predict(X_test)\n    test_time = time() - t0\n    print(\"test time:  %0.3fs\" % test_time)\n\n    score = metrics.accuracy_score(y_test, pred)\n    print(\"accuracy:   %0.3f\" % score)\n\n    if hasattr(clf, 'coef_'):\n        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n        print(\"density: %f\" % density(clf.coef_))\n\n    print(\"classification report:\")\n    print(metrics.classification_report(y_test, pred))\n\n    print(\"confusion matrix:\")\n    print(metrics.confusion_matrix(y_test, pred))\n\n    clf_descr = str(clf).split('(')[0]\n    return clf_descr, score, train_time, test_time","4a317837":"#\u00a0Different algorithms\nresults = []\nfor clf, name in (\n        (tree.DecisionTreeClassifier(), \"Decision Tree\"),\n        (RandomForestClassifier(n_estimators=100), \"Random Forest\"),\n        (GaussianNB(), \"Gauissian Naive Bayes\"),\n        (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n        (LogisticRegression(), \"Logistic Regression\"),\n        (svm.SVC(), \"Support Vector Machines\"),\n    ):\n    print('=' * 80)\n    print(name)\n    results.append(benchmark(clf, X_train, y_train, X_test, y_test))","c8ba3529":"# plots\n\nindices = np.arange(len(results))\n\nresults = [[x[i] for x in results] for i in range(4)]\n\nclf_names, score, training_time, test_time = results\ntraining_time = np.array(training_time) \/ np.max(training_time)\ntest_time = np.array(test_time) \/ np.max(test_time)\n\nplt.figure(figsize=(12, 8))\nplt.title(\"Score\")\nplt.barh(indices, score, .2, label=\"score\", color='navy')\nplt.barh(indices + .3, training_time, .2, label=\"training time\",\n         color='c')\nplt.barh(indices + .6, test_time, .2, label=\"test time\", color='darkorange')\nplt.yticks(())\nplt.legend(loc='best')\nplt.subplots_adjust(left=.25)\nplt.subplots_adjust(top=.95)\nplt.subplots_adjust(bottom=.05)\n\nfor i, c in zip(indices, clf_names):\n    plt.text(-.3, i, c)\n\nplt.show()","5aa620a2":"from sklearn.model_selection import GridSearchCV\n\nforest = RandomForestClassifier(random_state=1, n_estimators = 10, min_samples_split = 1)\n\nn_estimators = [100, 300, 500, 800, 1200]\nmax_depth = [5, 8, 15, 25, 30]\nmin_samples_split = [2, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10] \n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n              min_samples_split = min_samples_split, \n             min_samples_leaf = min_samples_leaf)\n\ngridF = GridSearchCV(forest, hyperF, cv = 3, verbose = 1, \n                      n_jobs = -1)\nbestF = gridF.fit(X_train, y_train)","c829bdce":"# best parameters on model\ngridF.best_params_","e4c23117":"gridF.refit\npreds = gridF.predict(X_test)\nprobs = gridF.predict_proba(X_test)\n\n# accuracy score\nnp.mean(preds == y_test)","d03464b2":"print(metrics.classification_report(y_test, preds, target_names=['Low', 'Normal', 'High']))","99d682ac":"balanced_data = data.copy()\n\nbalanced_data[balanced_data['B12'] == 2] = balanced_data[balanced_data['B12'] == 2].sample(1000, random_state=42)\nbalanced_data = balanced_data[balanced_data['B12'].notnull()]\nbalanced_data['B12'].value_counts()","9ed4ff50":"# split the data into features and target\nX_balanced = balanced_data.iloc[:, 1:15]\ny_balanced = balanced_data.iloc[:, 0]\n\n# split the data into test and train\nX_train_balanced, X_test_balanced, y_train_balanced, y_test_balanced = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, shuffle=True) # 70% training and 30% test","7fa5627c":"results_balanced = []\nfor clf, name in (\n        (tree.DecisionTreeClassifier(), \"Decision Tree\"),\n        (RandomForestClassifier(n_estimators=100), \"Random Forest\"),\n        (GaussianNB(), \"Gauissian Naive Bayes\"),\n        (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n        (LogisticRegression(), \"Logistic Regression\"),\n        (svm.SVC(), \"Support Vector Machines\"),\n    ):\n    print('=' * 80)\n    print(name)\n    results_balanced.append(benchmark(clf, X_train_balanced, y_train_balanced, X_test_balanced, y_test_balanced))","dc0a81a0":"# make plots\n\nindices = np.arange(len(results_balanced))\n\nresults_balanced = [[x[i] for x in results_balanced] for i in range(4)]\n\nclf_names, score, training_time, test_time = results_balanced\ntraining_time = np.array(training_time) \/ np.max(training_time)\ntest_time = np.array(test_time) \/ np.max(test_time)\n\nplt.figure(figsize=(12, 8))\nplt.title(\"Score\")\nplt.barh(indices, score, .2, label=\"score\", color='navy')\nplt.barh(indices + .3, training_time, .2, label=\"training time\",\n         color='c')\nplt.barh(indices + .6, test_time, .2, label=\"test time\", color='darkorange')\nplt.yticks(())\nplt.legend(loc='best')\nplt.subplots_adjust(left=.25)\nplt.subplots_adjust(top=.95)\nplt.subplots_adjust(bottom=.05)\n\nfor i, c in zip(indices, clf_names):\n    plt.text(-.3, i, c)\n\nplt.show()","cf4cc556":"gridF_balanced = GridSearchCV(forest, hyperF, cv = 3, verbose = 1, \n                      n_jobs = -1)\nbestF_balanced = gridF_balanced.fit(X_train_balanced, y_train_balanced)","140e9c31":"gridF_balanced.best_params_","c916f302":"gridF_balanced.refit\npreds = gridF_balanced.predict(X_test_balanced)\nprobs = gridF_balanced.predict_proba(X_test_balanced)\n\n# accuracy score\nnp.mean(preds == y_test_balanced)","c3e3d15a":"print(metrics.classification_report(y_test_balanced, preds, target_names=['Low', 'Normal', 'High']))","ed2d2e9d":"model = GaussianNB()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n\nprint(\"Accuracy score is {}\".format(metrics.accuracy_score(y_test, pred)))\nprint(metrics.classification_report(y_test, pred, target_names=['Low', 'Normal', 'High']))","f74165f3":"rslt = metrics.confusion_matrix(y_test, pred)\nsns.heatmap(pd.DataFrame(rslt, index=['Low Predicted', 'Normal Predicted', 'High Predicted'], columns=['Actual Low', 'Actual Normal', 'Actual High']), annot=True, fmt='d', cmap='Blues')\nplt.title('Unbalanced Data Confision Matrix')\nplt.show()","8ce365ff":"model.fit(X_train_balanced, y_train_balanced)\npred_balanced = model.predict(X_test_balanced)\n\nprint(\"Accuracy score for balanced data is {}\".format(metrics.accuracy_score(y_test_balanced, pred_balanced)))\nprint(metrics.classification_report(y_test_balanced, pred_balanced, target_names=['Low', 'Normal', 'High']))","cd720bef":"rslt_balanced = metrics.confusion_matrix(y_test_balanced, pred_balanced)\nsns.heatmap(pd.DataFrame(rslt_balanced, index=['Low Predicted', 'Normal Predicted', 'High Predicted'], columns=['Actual Low', 'Actual Normal', 'Actual High']), annot=True, fmt='d', cmap='Blues')\nplt.title('Balanced Data Confision Matrix')\nplt.show()","1b0a5ecd":"\n\n*   We can say that columns that starts with DPQ have positive correlation and higher values compared to others.\n*   Columns starts with SLQ are not highly correlated and have positive correlation but with all other columns except gender, have negative correlation (SLQ050 column also have negative correlation with gender)\n*   We can see that total monthly income is not really correlated with other features.\n\n","2dcd7a95":"## **Data Preparation**<a class=\"anchor\" id=\"data_preparation\"><\/a>","3f0f41d2":"Now our dataset is ready.","804c8d0b":"\n\n*   1: Yes\n*   2: No\n*   9: Don't know\n\n","d623d5a6":"## **Table of Content**\n\n[Problem](#problem)   \n[Data Understanding](#data_understanding)   \n[Data Preparation](#data_preparation)   \n[Modeling](#modeling)   \n[Evaluation](#evaluation)   \n[References](#references)   \n","9d264451":"Recall is ability of a classification model to identify all relevant instances.\nWe can see that recall is only good for normal B12 values. Model is not very successful for low and high.\n\nPrecision is ability of a classification model to return only relevant instances.\nOur model is so successful at normal values but not very successful at low and high values.\n","fbefb42a":"As we can see from the graph, people generally have normal B12 value. But this also says that the data is unbalanced. This may make the model to tend to category 2.","b39cc4a4":"When we compare the two results, we can see that the accuracy rate is better for the unbalanced data set, but when we look at the classification report, we can see that the balanced data set is better.","53f4010e":"This dataset has more features compared to others. Has 953 features. But as mentioned above, only columns that related to mental health and psychology will be selected.","e1c23697":"Since 'hyperparameters' cannot be made in the Gaussian Naive Bayes algorithm, we cannot use Grid Search. Instead, we try the Grid Search algorithm to see if we can get better results in Random Forest, the best model after Naive Bayes.","6551bc71":"We have seen above that there are many more participants with \"Normal\" B12 in our data. Therefore, we will make the categories more balanced and reconstruct our model. There are about 530 data in other categories. We select 1000 data from the normal category. The reason for this is that we do not want to lose too much data.","620129ea":"Confision matrix shows that, model tends to predict normal values more. The reason is data is unbalanced and normal values are much more than others. We can see that, model predicts 80 values as normal but actually low or high.\n","23f43156":"### For Unbalanced Data","590b6eff":"Age column with non-categorical values will not mean anything to the model. Values need to be categorized and will be as below.<br>\n\n*   1: 20 - 29\n*   2: 30 - 39\n*   3: 40 - 49\n*   4: 50 - 59\n*   5: 60 - 69\n*   6: 70 - 80\n\n","f454aadf":"There is 5316 records after dropping the null records.","18a0f8c8":"There are 47 features and 10175 records in the demographic dataset. The model will be used gender and age from this dataset.<br>RIAGENDR represents gender and RIDAGEYR represents age.","0108f146":"\n*   0: Not at all\n*   1: Several days\n*   2: More than half the days\n*   3: Nearly every day\n*   7: Refused\n*   9: Don't know\n*   And some missing data\n\n","6cbe1bc7":"Let's invest the values of columns","11b29f49":"The average is 6.95 ~ 7. Also in the dataset, 77 represents 'refused' and 99 represents 'don't know'. Will fill these values with 7 also.","795f6474":"Id is redundant anymore. Will drop it.","0797e05e":"# **The Effect of Psychology on B12**\n\n#### \u0130rem Dereli\n\nThis project aims to detect a person's B12 level by looking him\/her psychology and mental health.\n","3e451960":"Convert the age and B12 columns from category to int.","d30ed757":"We can say that, people generally answers the questions as 'not at all'(0).","062b20be":"### Inspect the labs dataset.\n","8ae3725d":"### Inspecting the data","43392888":"## **Problem** <a class=\"anchor\" id=\"problem\"><\/a>","76c15d50":"## **Data Understanding**<a class=\"anchor\" id=\"data_understanding\"><\/a>","90730449":"### Inspect the demographic dataset.","02ef40f6":"Accuracy rate is lower than unbalanced data. Again, considering the accuracy rate, f1score, recall and precision values, we can see that the Gaussian Naive Bayes algorithm works better.","9d7f7ca2":"When comparing the accuracy of the models, two algorithms with the highest accuracy are Logistic Regression and Support Vector Classifier, but when we consider 'precision', 'recall' and 'f1score' values, it is not very successful models, only 'Normal' value B12 prediction we can see what model did. Therefore, when we compare these two models, we will use the Gaussian Naive Bayes algorithm in our project, whose accuracy rate is not very different, but also predicts other categories.","f28c856b":"B12 values are stored as float.\n\n","deab1acb":"## **References**<a class=\"anchor\" id=\"references\"><\/a>","491965f3":"Also the DPQ and SLQ features have refused as 7 and don't know as 9 values. Will replace these values with most occurred value 0 (not at all).","5e011e9a":"## **Evaluation**<a class=\"anchor\" id=\"evaluation\"><\/a>","793cf7ba":"One of the column that will be selected is DPQ020. Asks the people who attends 'Feeling down, depressed or hopeless?' Let's invest this column. All the other columns that related to mental health are same (columns that starts with DPQ).","dbe13e44":"4497 records have missing B12 value.\n","bed40e84":"In the labs dataset, there are 424 features. Needed one is the only value of B12. And B12 values will also be categorized as below.<br>\n\n*   Low = 1: 0 - 300\n*   Normal = 2: 301 - 950\n*   High = 3: 950 - 27000\n\n","815a52dd":"As with the unbalanced data set, our accuracy rate increased by about 6% but had a negative impact on our classification report.","fba548af":"Now the result with balanced data shows that model not tend to predict normal values more. It predicts all values but because of accuracy is bad, it predicts more wrongly.\n\n---\n","93ebe10e":"The distrubiton of ages are nearly close. We can say that most participation is from age between 20 and 29. And worst participation is from age between 70 and 80.\n\n---\n\n","6ad15040":"Psychology and mental health problems are really common in this century. And the lack of vitamin B12 causes some other big health problems like anemia, forgetfulness, neurologic, gastrointestinal problems, etc. Psychology problems can occur for many reasons such as childhood traumas, private life, family problems, work-life, and so on.<br> B12 of a person can be low, normal, or high. Depending on the medical articles, the lower limit of B12 is 200pg\/ml and should not be lower than 300pg\/ml. Smaller than 300pg\/ml values are really low levels for B12 and this can cause a person to stay at the hospital for days. But doctors suggest using oral supplements if B12 is lower than 500pg\/ml.\n\n\n---\n\n\nAs mentioned above, the result of the B12 level will be either low, medium, or high. And this is a classification problem with three categories.","8fa0c35a":"Recall is ability of a classification model to identify all relevant instances.\nRecall of balanced data looks better than recall of unbalanced data. High values are now having a good percentage of recall, normal is still good. Low could be better.\n\nPrecision is ability of a classification model to return only relevant instances.\nPrecision of balanced data is much better than precision of unbalanced data. All categories have now acceptable precisions.\n","53e4cb95":"Other column will be selected is related to sleep disorders and has categorical values too.","ffb0750b":"Benchmark classifier is a function that makes us able to compare the different algorithms.","5ca53d3f":"Merge the demographics, labs and questionnaire parts.","c38fc101":"Maximum B12 value in the dataset is 26801 and minimum is 18. We can say that people have normal B12 value averagely.\n\n---\n\n","c9f59d00":"Records that has missing values of B12 will be dropped because the model will predict the rank of B12.","c7673565":"There is a total of 5316 records in the dataset. But there are some null values for some features. Especially, the DPQ100 feature has a lot of null records compared to others. Putting average value to these records makes the model work badly, so we will drop this column from the dataset.","67339ade":"After dropping the column, there is 13 features and still have null records. Features related to mental health (starts with DPQ) are more important compared to others. So, they cannot have a missing value. Records that have a missing value on the mental health features will be dropped from the dataset.","bbf46b64":"### With Balanced Data","ebd2a948":"Now, we have 4870 records and still have missing values on the IND235 feature. Because the DPQ features are important, will not drop these records from the dataset and will fill the missing values with the average values.","8afa6308":"The reason that dataset have a nan value on age column is, laboratory dataset only have records that person aged 20 between 80 as mentioned above. Data outside the age of 20 and 80 will have nan value.\n\n\n\n\n\n","275439f7":"## **Modeling**<a class=\"anchor\" id=\"modeling\"><\/a>","d94a3b3e":"The 'National Health and Nutrition Examination Survey(NHANES)' is online dataset repository at www.cdc.gov. Starting from 1999, there are total of 11 datasets, one for each two years. In this project, selected years are 2013-2014 depending on the large amount of B12 data. There is a total of 14,332 persons who attended the survey. \n\nThe dataset contains 5 parts; demographics, examinations, dietary, laboratory, and questionnaire. Demographics, laboratory, and questionnaire parts will be used in this project. \n\nThe demographics part contains individual, family, and household information about person. Gender and age will be taken.\n\nThe laboratory part contains blood values and only value of B12 will be taken. Laboratory part is for people that aged 20 between 80.\n\nAnd lastly, the questionnaire part contains a mental health screener, relationship, family, and sample person questions. Questions related to psychology, mental health, and related to psychology such as household income, sleep disorders will be used in the model.","ff78f2bd":"In the questionnaire dataset, columns that will be selected are listed below.\n\n*   **DPQ010**: Have little interest in doing things\n*   **DPQ020**: Feeling down, depressed or hopeless\n*   **DPQ030**: Trouble sleeping or sleeping too much\n*   **DPQ040**: Feeling tired or having little energy\n*   **DPQ050**: Poor appetite or overeating\n*   **DPQ060**: Feeling bad about yourself\n*   **DPQ070**: Trouble concentrating on things\n*   **DPQ080**: Moving or speaking slowly or too fast\n*   **DPQ090**: Thought you would be better off dead\n*   **DPQ100**: How difficult have these problems made it for you to do your work, take care of things at home, or get along with people?\n<br><br>\n*   **SLQ050**: Ever told doctor had trouble sleeping?\n*   **SLQ060**: Ever told by doctor have sleep disorder?\n<br><br>\n*   **IND235**: Monthly family income\n\n","de4c1d54":"\n\n*   https:\/\/github.com\/Jean-njoroge\/Breast-cancer-risk-prediction\n*   https:\/\/dizziness-and-balance.com\/disorders\/central\/b12.html\n","2706f433":"In gender column, 1 represents male and 2 represents female.\n<br>Visualization of distribution of gender","2009262c":"As a result of Grid Search, we see that the accuracy rate increased by 3%, but caused a negative effect on the classification report. We see that it cannot predict low and high B12 categories (1 and 3). Therefore, we continue with the Naive Bayes algorithm.","de11db20":"There are 424 features and 9813 records in the dataset. B12 will be used only from this dataset.<br>LBDB12 represents B12 value.","ce4b5930":"### Inspect the questionnaire dataset."}}