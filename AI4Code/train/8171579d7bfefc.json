{"cell_type":{"11f0dbbc":"code","f246f849":"code","eabfef06":"code","059e0e90":"code","a49e1983":"code","99c24535":"code","f66d27a3":"code","1c9afd05":"code","90de9b21":"code","71ae600c":"code","52ebd2a4":"code","41528c2e":"markdown","534456a5":"markdown","d04960f9":"markdown","865e1e48":"markdown","08dfc788":"markdown","bdde77ea":"markdown","d1a5b5d9":"markdown","88804a74":"markdown"},"source":{"11f0dbbc":"# importing libraries:\nimport sqlite3                      # to save\/ load the .sqlite files and perform SQL operations\nimport pandas as pd                 # dataframe ops\nimport numpy as np                  # array ops\nfrom IPython.display import display # to view dataframe in a tabular format\n\n# creating the connect object to connect with the database:\ncon = sqlite3.connect('..\/input\/amazon-fine-food-reviews\/database.sqlite')\n\n# getting the values that are positive or negative and avoiding the ambiguous\/ neutral (Score=3) reviews:\nfiltered_data = pd.read_sql_query(\"\"\"\nSELECT\n    *\nFROM \n    Reviews\nWHERE \n    Score <> 3;\n\"\"\", con)\n\n# change Score from numbers to ratings as follows:\n# if Score > 3 then 'Positive' rating\n# if Score < 3 then 'Negative' rating, thus, eliminating Score = 3 cases:\ndef rate(x):\n    if x < 3:\n        return \"Negative\"\n    return \"Positive\"\n\n# replacing the numbers in the Score column with \"Positive\"\/ \"Negative\" values:\npositive_negative = filtered_data['Score']\npositive_negative = positive_negative.map(rate)\nfiltered_data['Score'] = positive_negative","f246f849":"# Cleaning the data: de-duplicating\n\nsorted_data = filtered_data.sort_values('ProductId', axis=0, inplace=False, ascending=True)\nfinal = sorted_data.drop_duplicates(['UserId', 'ProfileName', 'Time','Text'], inplace=False, keep='first')\n\n# there's also a scenario where helpfulness numerator is greater than helpfulness denominator which doesn't make any sense.\n# because HelpfulnessNumerator is no. of YES (helpful reviews)\n# and HelpfulnessDenominator is [no. of YES + no. of NO (not helpful reviews)]\nprint(\"Removing the below rows:\\n\", pd.read_sql_query(\"\"\"\nSELECT \n    *\nFROM \n    Reviews\nWHERE \n    HelpfulnessNumerator > HelpfulnessDenominator;\n\"\"\", con))\n\n# we thus only keep the rows where helpfulness numerator is less than or equal to the helpfulness denominator:\nfinal = final[final.HelpfulnessNumerator <= final.HelpfulnessDenominator]\n\n# resetting the index because many of the rows are deleted and their corresponding indices are missing:\nfinal = final.reset_index(drop=True)\ndisplay(final)  # display the table in tabular format\n\n# printing shape of the filtered\/ modified data:\nprint(\"Shape of the dataframe: \", final.shape)\n\n# can convert to interactive table using beakerx but not recommended as it takes long time since the dataset is huge:\n#table = TableDisplay(final)\n#print(table)\ncon.close()","eabfef06":"# importing gensim.models to implement Word2Vec:\nimport gensim\nfrom gensim import models\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\n\n# in the filename, 300 stands for 300 dimensions:\n# loading the model, this is a very high resource consuming task: \ng_trained_model = KeyedVectors.load_word2vec_format('..\/input\/googles-trained-word2vec-model-in-python\/GoogleNews-vectors-negative300.bin', binary=True)\nprint(\"Model loaded successfully: \", type(g_trained_model))","059e0e90":"# Just testing the model:\nprint('Most similar to the word \"pizza\"', g_trained_model.most_similar('pizza'))","a49e1983":"# import statements:\nimport re   # to search for html tags, punctuations & special characters\n\n# importing gensim.models to implement Word2Vec:\nimport gensim\nfrom gensim import models\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\n\n# Remove HTML tags - getting all the HTML tags and replacing them with blank spaces:\ndef cleanhtml(sentence):\n    clean_text = re.sub('<.*?>', ' ', sentence)\n    return clean_text\n\n# Remove punctuations & special characters - getting all the punctuations and replacing them with blank spaces:\ndef cleanpunc(sentence):\n    clean_text = re.sub(r'[@#$%\\^&\\*+=]', r'', sentence) # removing special characters\n    clean_text = re.sub(r'[,.;\\'\"\\-\\!?:\\\\\/|\\[\\]{}()]', r' ', clean_text) # removing punctuations\n    return clean_text\n\nfinal_clean_sentences = []\n\nfor sentence in final['Text'].values:\n    sentence = cleanhtml(sentence)\n    sentence = cleanpunc(sentence)\n    clean_sentence = []\n    \n    # for each word in the sentence, if it is alphabetic, we append it to the new list\n    for word in sentence.split():\n        if word.isalpha():\n            clean_sentence.append(word.lower())\n    \n    # for each review in the 'Text' column, we create a list of words that appear in that sentence and store it in another list. \n    # basically, a list of lists - because that's how the model takes the input while training:\n    final_clean_sentences.append(clean_sentence)\n    \nprint(\"Sentence cleaning completed!\")\ndel clean_sentence, sorted_data, filtered_data, positive_negative","99c24535":"# below imports are used only to show the progress bar:\nimport tqdm\nimport time\n\nvectored_sentences = []\nnot_converted = set()  # used to keep track of how many words are not converted to vector\nprint(\"Average Word2Vec calculations started...\")\n\n# its ok to ignore tqdm_notebook in the for loop below, just kept it for ETA:\nfor sentence in final_clean_sentences:\n    vec_sent = np.zeros(300)        # since the size of g_trained_model is 300\n    for word in sentence:\n        if word in g_trained_model:\n            vectored_word = g_trained_model.wv[word]\n            vec_sent += vectored_word\n        else:\n             not_converted.add(word)\n                \n    vec_sent\/=len(sentence)\n    vectored_sentences.append(vec_sent)\n\nprint(\"\\nAvg. Word2Vec calculations completed!\")\nprint(\"First element of Avg. Word2Vec vectored sentences:\\n\", vectored_sentences[0])\nprint(\"-\"*60)\ndel g_trained_model","f66d27a3":"# Separating 'X' & 'y':\nscore = list(final['Score'])\nx = pd.DataFrame(vectored_sentences)\nprint(\"X shape: \", x.shape)\ny = pd.DataFrame(score, columns=['rating'])\ny.loc[y['rating']=='Positive', 'rating'] = 1\ny.loc[y['rating']=='Negative', 'rating'] = 0\nprint(y.shape)\nprint(y.dtypes)\n\n# Here y has the dtypes as 'object' which can be used with fit() method of any ML algorithm and thus need to convert it to int object\ny=y.astype('int')\nprint(y.dtypes)\n\n# Standardization:\nfrom sklearn.preprocessing import StandardScaler\nx_scaler = StandardScaler()\nx = x_scaler.fit_transform(x)\nprint(x)\n#print(x_scaler.transform(y))\n\n# Imputation\/ Handling the missing values:\nfrom sklearn.impute import SimpleImputer\nx_impute = SimpleImputer(missing_values=np.nan, strategy='mean')\nx = x_impute.fit_transform(x)\nprint(x)\n\n# Creating training & testing datasets:\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.33,random_state=0)\nprint(\"x_train\", x_train.shape)\nprint(\"x_test\", x_test.shape)\nprint(\"y_train\", y_train.shape)\nprint(\"y_test\", y_test.shape)\ndel final, x_scaler, x_impute","1c9afd05":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\n# Testing the model's performance using 10-fold cross-validation:\nLR = LogisticRegression(penalty='l1', random_state=0, solver='saga', n_jobs=-1)\nlr_score = cross_val_score(LR, x_train, y_train.values.ravel(), cv=10, scoring='accuracy').mean()\nprint(\"Using Logistic Regression, can give the generalization accuracy of: ~\", lr_score)","90de9b21":"# Implementing Logistic Regression:\nLR = LogisticRegression(penalty='l2', random_state=0, solver='saga', n_jobs=-1)\nLR = LR.fit(x_train, y_train.values.ravel())\nlr_preds = LR.predict(x_test)","71ae600c":"# Checking with confusion matrix:\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, lr_preds)\nimport seaborn as sns\nprint(\"Confusion matrix:\")\nsns.heatmap(data=cm, annot=True)","52ebd2a4":"TP = cm[0][0]\nFP = cm[0][1]\nFN = cm[1][0]\nTN = cm[1][1]\n\nprint(\"True positives: \", TP)\nprint(\"True negatives: \", TN)\nprint(\"False positives: \", FP)\nprint(\"False negatives: \", FN)\n\nacc = (TP+TN)\/(TP+TN+FP+FN)\nprecision = TP\/ (TP+FP)\nrecall = TP\/ (TP+FN)\nf1_score = 2*precision*recall\/ (precision + recall)\n\nprint(\"\\n=========Final performance evaluation=========\")\nprint(\"Accuracy: \", acc)\nprint(\"Precision: \", precision)\nprint(\"Recall: \", recall)\nprint(\"F-1 Score: \", f1_score)\nprint(\"==============================================\")","41528c2e":"### Removing the duplicates:\n\nIt is necessary to remove duplicates on order to get unbiased results for the analysis of the data:\nHere, we sort ProductID and then remove the duplicates except the first occurrence.\nIn this way, we preserve the first occurrence of the duplicate data and remove other occurrences.","534456a5":"### Final Performance evaluation:","d04960f9":"Preparing\/ pre-processing the dataset:","865e1e48":"### Using Word2Vec model to perform vectorization:\nGoogle's trained model can be downloaded from - https:\/\/drive.google.com\/open?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n\nAlso available on Kaggle - https:\/\/www.kaggle.com\/umbertogriffo\/googles-trained-word2vec-model-in-python, can simply add the data in the notebook","08dfc788":"### Cleaning up the sentences\/ removing noise:","bdde77ea":"Performing the vectorization using the trained model:","d1a5b5d9":"### Applying Logistic Regression:","88804a74":"### Loading libraries & the dataset:"}}