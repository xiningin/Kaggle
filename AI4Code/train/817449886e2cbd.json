{"cell_type":{"03881d28":"code","180f2c28":"code","2fc5f2e9":"code","d7736433":"code","3e67c6ff":"code","430b31bb":"code","ff6cb2e5":"code","a84d903a":"code","6e51fb3d":"code","2fc708ec":"code","155c6fe7":"code","a5136252":"code","831c368c":"markdown","9ac1f6d3":"markdown","d5bdeb8f":"markdown"},"source":{"03881d28":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","180f2c28":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\ndef normalize(df):\n    df_num = df.select_dtypes(include=[np.float, np.int])\n    scaler = StandardScaler()\n    df[list(df_num.columns)] = scaler.fit_transform(df[list(df_num.columns)])\n\n# get data \nufc_pp_data = pd.read_csv(\"\/kaggle\/input\/ufcdata\/preprocessed_data.csv\")\n\n# normalize all except one-hots, non-ints, non-floats\n# backup weight_class_* , B_Stance_*, R_Stance_*\nufc_pp_one_hots = ufc_pp_data.filter(regex=(\"weight_class_*|R_Stance_*|B_Stance_*\"))\n\n# disclude columns above\nufc_pp_no_hots = ufc_pp_data.drop(ufc_pp_one_hots.columns, axis=1)\n\n# normalize\nnormalize(ufc_pp_no_hots)\n\n# recombine the one-hots\nufc_pp_normalized = pd.concat([ufc_pp_no_hots, ufc_pp_one_hots], axis=1)\n\n# seperate the data\ny = ufc_pp_normalized['Winner']\nX = ufc_pp_normalized.drop(columns = 'Winner')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=43)\n\n# fit model\nmodel = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=43)\nmodel.fit(X_train, y_train)\n\ny_preds = model.predict(X_test)\n\n# test model\nprint(\"OOB, Accuracy\\n\", model.oob_score_, accuracy_score(y_test, y_preds))","2fc5f2e9":"import category_encoders as ce\n\n# get data, going to need the original data to revert the one hot encoding\nufc_raw_data = pd.read_csv(\"\/kaggle\/input\/ufcdata\/data.csv\")\n\n# normalize all except one-hots, non-ints, non-floats\n# backup weight_class_* , B_Stance_*, R_Stance_*\nufc_pp_one_hots = ufc_pp_data.filter(regex=(\"weight_class_*|R_Stance_*|B_Stance_*\"))\n\n# disclude columns above\nufc_pp_no_hots = ufc_pp_data.drop(ufc_pp_one_hots.columns, axis=1)\n\n# normalize\nnormalize(ufc_pp_no_hots)\n        \n# reverse the stance column\nufc_rev_one_hots = pd.DataFrame({})\n\nufc_pp_weight_class = ufc_pp_data.filter(regex=(\"weight_class_*\"))\nufc_rev_one_hots[\"weight_class\"] = ufc_pp_weight_class.idxmax(1)\n\nufc_pp_R_Stance = ufc_pp_data.filter(regex=(\"R_Stance_*\"))\nufc_rev_one_hots[\"R_Stance\"] = ufc_pp_R_Stance.idxmax(1)\n\nufc_pp_B_Stance = ufc_pp_data.filter(regex=(\"B_Stance_*\"))\nufc_rev_one_hots[\"B_Stance\"] = ufc_pp_B_Stance.idxmax(1)\n\n# binary encode the categories\nencoder = ce.BinaryEncoder(cols=ufc_rev_one_hots.columns.tolist())\nufc_cat_binary = encoder.fit_transform(ufc_rev_one_hots)\n\n# combine the binary categories with data\nufc_pp_normalized = pd.concat([ufc_pp_no_hots, ufc_cat_binary], axis=1)\n\n# seperate the data\ny = ufc_pp_normalized['Winner']\nX = ufc_pp_normalized.drop(columns = 'Winner')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=43)\n\n# fit model\nmodel = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=43)\nmodel.fit(X_train, y_train)\n\ny_preds = model.predict(X_test)\n\n# test model\nprint(\"OOB, Accuracy\\n\", model.oob_score_, accuracy_score(y_test, y_preds))","d7736433":"import seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nmpl.style.use('seaborn')\n\n# backup weight_class_* , B_Stance_*, R_Stance_*\nufc_pp_one_hots = ufc_pp_data.filter(regex=(\"weight_class_*|R_Stance_*|B_Stance_*\"))\n\n# disclude columns above\nufc_pp_no_hots = ufc_pp_data.drop(ufc_pp_one_hots.columns, axis=1)\n\n# reverse the stance column\nufc_rev_one_hots = pd.DataFrame({})\n\nufc_pp_weight_class = ufc_pp_data.filter(regex=(\"weight_class_*\"))\nufc_rev_one_hots[\"weight_class\"] = ufc_pp_weight_class.idxmax(1)\n\nufc_pp_R_Stance = ufc_pp_data.filter(regex=(\"R_Stance_*\"))\nufc_rev_one_hots[\"R_Stance\"] = ufc_pp_R_Stance.idxmax(1)\n\nufc_pp_B_Stance = ufc_pp_data.filter(regex=(\"B_Stance_*\"))\nufc_rev_one_hots[\"B_Stance\"] = ufc_pp_B_Stance.idxmax(1)\n\n# combine the binary categories with data\nufc_pp_no_hots = pd.concat([ufc_pp_no_hots, ufc_rev_one_hots], axis=1)\n\nufc_pp_no_hots.info(verbose=True)","3e67c6ff":"# ax = sns.distplot(ufc_pp_data.Winner)\nax = sns.countplot(ufc_pp_no_hots.Winner)\nplt.show()\n\nvalue_counts = ufc_pp_no_hots.Winner.value_counts()\nprint(value_counts \/ (value_counts[0] + value_counts[1]))\n\nax = sns.countplot(ufc_pp_no_hots.B_Stance)\nplt.show()\nax = sns.countplot(ufc_pp_no_hots.R_Stance)\nplt.show()","430b31bb":"ax = sns.distplot(ufc_pp_no_hots.R_Reach_cms)\nplt.show()\nax = sns.distplot(ufc_pp_no_hots.B_Reach_cms)\nplt.show()","ff6cb2e5":"reach_diff = ufc_pp_no_hots.R_Reach_cms - ufc_pp_no_hots.B_Reach_cms\n\n#winner_reach_diff.sort_values('')\nax = sns.distplot(reach_diff)\nplt.show()\n\n# reach_diff = reach_diff.to_frame()\n# reach_diff = reach_diff.rename(columns= {0: 'reach_diff'})\nwinner_reach_diff = pd.concat([reach_diff, ufc_pp_no_hots.Winner], axis=1)\nwinner_reach_diff = winner_reach_diff.rename(columns= {0: 'reach_diff'})\nwinner_reach_diff = winner_reach_diff.sort_values('reach_diff')","a84d903a":"# what effect does reach difference have? \nsns.catplot(x=\"reach_diff\", y=\"Winner\", kind=\"box\", data=winner_reach_diff);","6e51fb3d":"from sklearn import datasets\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom hyperopt import tpe, hp, fmin\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_curve, auc\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\n# normalize generic dataframes inplace\n# df: pandas dataframe\ndef normalize(df):\n    df_num = df.select_dtypes(include=[np.float, np.int])\n    scaler = StandardScaler()\n    df[list(df_num.columns)] = scaler.fit_transform(df[list(df_num.columns)])\n\n# from Yacin Nouri, https:\/\/www.kaggle.com\/ynouri\/random-forest-k-fold-cross-validation\ndef compute_roc_auc(index, clf):\n    y_predict = clf.predict_proba(X.iloc[index])[:,1]\n    fpr, tpr, thresholds = roc_curve(y.iloc[index], y_predict)\n    auc_score = auc(fpr, tpr)\n    return fpr, tpr, auc_score    \n\n# load the data\nufc_pp_data = pd.read_csv(\"\/kaggle\/input\/ufcdata\/preprocessed_data.csv\")\nnormalize(ufc_pp_data)\ncriterions = ['gini', 'entropy']\n\ny = ufc_pp_data['Winner'].astype('category').cat.codes\nX = ufc_pp_data.drop(columns = 'Winner')\n\ndef objective_func(args):\n    # return the average of the roc performance\n    \n    clf = RandomForestClassifier(n_estimators=args['params']['estimators'],\n    criterion=args['params']['criterion'],\n    max_depth=args['params']['max_depth'],\n    min_samples_split=args['params']['min_samples_split'],\n    min_samples_leaf=args['params']['min_samples_leaf'],\n    min_weight_fraction_leaf=0,\n    max_features='auto',\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    bootstrap=True,\n    oob_score=False,\n    n_jobs=-1,\n    random_state=43,\n    verbose=0,\n    warm_start=False,\n    class_weight='balanced')\n\n    cv = StratifiedKFold(n_splits=5, random_state=123, shuffle=True)\n    results = pd.DataFrame(columns=['training_score', 'test_score'])\n    fprs, tprs, scores = [], [], []\n        \n    for (train, test), i in zip(cv.split(X, y), range(5)):\n        clf.fit(X.iloc[train], y.iloc[train])\n        _, _, auc_score_train = compute_roc_auc(train, clf)\n        fpr, tpr, auc_score = compute_roc_auc(test, clf)\n        scores.append((auc_score_train, auc_score))\n        fprs.append(fpr)\n        tprs.append(tpr)\n    \n    # return the average performance on the test splits\n    total = 0\n    for i in scores:\n        total += i[1]\n        \n    # minimize average inaccuracy across 5 folds\n    return 1 - total \/ len(scores)\n\nspace = {'params': {'estimators': hp.choice('estimators',range(50,250)),\n        'criterion':hp.choice('criterion',criterions),\n        'max_depth':hp.choice('max_depth',range(2,10)),\n        'min_samples_split':hp.uniform('min_samples_split',0,0.99),\n       'min_samples_leaf':hp.uniform('min_samples_leaf',0,0.49)}\n        }\n\nbest_classifier = fmin(objective_func,space,\n                        algo=tpe.suggest,max_evals=500)\n\nprint(best_classifier)","2fc708ec":"# this is one result that was found\nbest_classifier = {'criterion': 0, 'estimators': 71, 'max_depth': 6, 'min_samples_leaf': 2.0576639506899924e-05, 'min_samples_split': 0.0007085464011934425}","155c6fe7":"# from Yacin Nouri, https:\/\/www.kaggle.com\/ynouri\/random-forest-k-fold-cross-validation\ndef plot_roc_curve(fprs, tprs):\n    \"\"\"Plot the Receiver Operating Characteristic from a list\n    of true positive rates and false positive rates.\"\"\"\n    \n    # Initialize useful lists + the plot axes.\n    tprs_interp = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n    f, ax = plt.subplots(figsize=(14,10))\n    \n    # Plot ROC for each K-Fold + compute AUC scores.\n    for i, (fpr, tpr) in enumerate(zip(fprs, tprs)):\n        tprs_interp.append(np.interp(mean_fpr, fpr, tpr))\n        tprs_interp[-1][0] = 0.0\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        ax.plot(fpr, tpr, lw=1, alpha=0.3,\n                 label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n        \n    # Plot the luck line.\n    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n             label='Luck', alpha=.8)\n\n    # Plot the mean ROC.\n    mean_tpr = np.mean(tprs_interp, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    std_auc = np.std(aucs)\n    ax.plot(mean_fpr, mean_tpr, color='b',\n             label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n             lw=2, alpha=.8)\n    \n    # Plot the standard deviation around the mean ROC.\n    std_tpr = np.std(tprs_interp, axis=0)\n    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n                     label=r'$\\pm$ 1 std. dev.')\n    \n    # Fine tune and show the plot.\n    ax.set_xlim([-0.05, 1.05])\n    ax.set_ylim([-0.05, 1.05])\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.set_title('Receiver operating characteristic')\n    ax.legend(loc=\"lower right\")\n    plt.show()\n    return (f, ax)\n\n# construct model with parameters\nclf = RandomForestClassifier(n_estimators=best_classifier['estimators'],\n    criterion=criterions[best_classifier['criterion']],\n    max_depth=best_classifier['max_depth'],\n    min_samples_split=best_classifier['min_samples_split'],\n    min_samples_leaf=best_classifier['min_samples_leaf'],\n    min_weight_fraction_leaf=0,\n    max_features='auto',\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    bootstrap=True,\n    oob_score=False,\n    n_jobs=-1,\n    random_state=43,\n    verbose=0,\n    warm_start=False,\n    class_weight='balanced')\n\n# with the best parameters found, let's visualize its AUC curve\ncv = StratifiedKFold(n_splits=5, random_state=123, shuffle=True)\nresults = pd.DataFrame(columns=['training_score', 'test_score'])\nfprs, tprs, scores = [], [], []\n    \nfor (train, test), i in zip(cv.split(X, y), range(5)):\n    clf.fit(X.iloc[train], y.iloc[train])\n    _, _, auc_score_train = compute_roc_auc(train, clf)\n    fpr, tpr, auc_score = compute_roc_auc(test, clf)\n    scores.append((auc_score_train, auc_score))\n    fprs.append(fpr)\n    tprs.append(tpr)\n\nplot_roc_curve(fprs, tprs);\npd.DataFrame(scores, columns=['AUC Train', 'AUC Test'])","a5136252":"\nclf.predict(X.iloc[0:10])","831c368c":"### Again, but with binary encodings on stance","9ac1f6d3":"We observe that the average accuracy is about 69-70%, I test the classifier below to ensure that it does not simply output the dominant winner class (RED with win rate of 2 in 3 matches)","d5bdeb8f":"I play around with Random Forest hyper-parameter optimization here, as well as some visualizations."}}