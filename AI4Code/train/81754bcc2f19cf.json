{"cell_type":{"018e0593":"code","5906e6c8":"code","c4f52170":"code","8430fca6":"code","b52b64a5":"code","e76891b3":"code","7a693a7a":"code","975b5329":"code","afbd2845":"code","679b5c8c":"code","a4a3f449":"code","99389541":"code","59ee5da5":"code","3eac1ff1":"code","af05f236":"code","a35582ab":"code","02023922":"code","3b41b9c8":"code","4bed6a24":"code","a8f38102":"markdown","89f5403c":"markdown","06d80991":"markdown","b53cc3bc":"markdown","b6028462":"markdown","503e21f5":"markdown","a8bacb1a":"markdown","4058101f":"markdown","e1048816":"markdown","e2b75cf1":"markdown","b1e68d86":"markdown","3cb5d34a":"markdown"},"source":{"018e0593":"import seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.callbacks import LearningRateScheduler\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import (\n    Conv2D,\n    Dense,\n    Dropout,\n    Flatten,\n    MaxPooling2D\n)\nimport kerastuner as kt\nfrom kerastuner.tuners import RandomSearch\nfrom tensorflow.keras import layers","5906e6c8":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","c4f52170":"train.head(10)","8430fca6":"test.head(10)","b52b64a5":"# Checking missing values\nsns.heatmap(train.isnull())\n","e76891b3":"sns.heatmap(test.isnull())","7a693a7a":"X_train=train.iloc[:,1:]\nY_train=train.iloc[:,0]\ng=sns.countplot(Y_train)\n","975b5329":"X_train=X_train\/255.0\ntest=test\/255.0","afbd2845":"\nX_train = X_train.values.reshape(-1, 28, 28, 1)\ntest = test.values.reshape(-1, 28, 28, 1)","679b5c8c":"X_train.shape","a4a3f449":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(X_train,Y_train,test_size=0.2)","99389541":"\ny_train = to_categorical(y_train, num_classes=10)\ny_test = to_categorical(y_test, num_classes=10)\n","59ee5da5":"y_train.shape","3eac1ff1":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(x_train)","af05f236":"INPUT_SHAPE = (28,28,1)\nNUM_CLASSES = 10","a35582ab":"def build_model(hp):  \n  model = keras.Sequential([\n    keras.layers.Conv2D(\n        filters=hp.Int('conv_1_filter', min_value=32, max_value=128, step=16),\n        kernel_size=hp.Choice('conv_1_kernel', values = [3,5]),\n        activation='relu',\n        input_shape=(28,28,1)\n    ),\n    keras.layers.Conv2D(\n        filters=hp.Int('conv_2_filter', min_value=32, max_value=64, step=16),\n        kernel_size=hp.Choice('conv_2_kernel', values = [3,5]),\n        activation='relu'\n    ),\n    \n    \n    \n  \n      \n    keras.layers.Conv2D(\n        filters=hp.Int('conv_2_filter', min_value=32, max_value=64, step=16),\n        kernel_size=hp.Choice('conv_2_kernel', values = [3,5]),\n        activation='relu'\n    ),\n    \n   \n      \n    keras.layers.Flatten(),\n    keras.layers.Dense(\n        units=hp.Int('dense_1_units', min_value=32, max_value=128, step=16),\n        activation='relu'\n    ),\n    keras.layers.Dense(10, activation='softmax')\n  ])\n  \n  model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n  \n  return model","02023922":"tuner = RandomSearch(\n    build_model,\n    objective='val_accuracy',\n    max_trials=5,\n    executions_per_trial=3,\n    directory='project',\n    project_name='Air Quality Index')","3b41b9c8":"\ntuner.search_space_summary()","4bed6a24":"batch_size = 64\ntuner.search(datagen.flow(x_train,y_train, batch_size=batch_size),\n             epochs=3,\n             validation_data=(x_test, y_test),\n             steps_per_epoch=x_train.shape[0] \/\/ 128\n            )","a8f38102":"Checking the null value in the dataset.Using seaborn to plot if there is any null values ","89f5403c":"Encoding our outing into 10 differents categories","06d80991":"Train and test images (28px x 28px) has been stock into pandas.Dataframe as 1D vectors of 784 values. We reshape all data to 28x28x1 3D matrices.\n\nKeras requires an extra dimension in the end which correspond to channels. MNIST images are gray scaled so it use only one channel. For RGB images, there is 3 channels, we would have reshaped 784px vectors to 28x28x3 3D matrices.","b53cc3bc":"\n# Keras Tuner\n\nInput Shape (28,28,1)\n3 layers have min_value 32 and 128 with relu activation.\nYou can add maxpooling and dropout for better result.\nthis Notebook is just to get your hand on Keras Tuner","b6028462":"Importing Dataset","503e21f5":"Spliting the data into train and validation.","a8bacb1a":"These images are very small. Also, if you go look at images of what these numbers look like (sorry for not including examples in here), you'll see that all of the numbers are fairly standard: right side up, not mirrored, not too tilted, etc. Thus, we shouldn't need to do too much to avoid overfitting; we'll just shift things around, rotate slightly, and zoom a bit.\nNow we do data augumentation.It is a technique to increase the size of the dataset by applying various rotations and various other factors so that model can train on more images to get high accuracy.","4058101f":"# Making a simple CNN model to train and on the MINST digits dataset using Keras Tuner.\n\nIn this notebook i am implementing Keras Tuner.It will gives us optimal hyperparamenter.This is into to keras tuner you can modify tuner to get maximum accuracy.Keras Tuner make your work easy.This is a small notebook so you can focus on Keras Tuner.\n\n\nKeras Tuner is an easy-to-use, distributable hyperparameter optimization framework that solves the pain points of performing a hyperparameter search. Keras Tuner makes it easy to define a search space and leverage included algorithms to find the best hyperparameter values. Keras Tuner comes with Bayesian Optimization, Hyperband, and Random Search algorithms built-in, and is also designed to be easy for researchers to extend in order to experiment with new search algorithms.\n\nImporting Libraries","e1048816":"As seen above, these are datasets of pixels by image, with each row representing a seperate image of 784 pixels (likely 28 high X 28 wide). The training set contains 42000 images, while the testing set contains 28000. This appears to be monohromatic, as there is only one layer. Were they not black and white, we could expect three different sets of values for each pixel, representing RGB","e2b75cf1":"# Thanks a lot for having a look at this notebook. If you found this notebook useful, Do Upvote.\nIf you have forked the kernel and not upvoted yet, then show the support by upvoting :)\n\nPlease leave you constructive criticism and suggestion in comments below!!","b1e68d86":"Checking that the dataset we have is balanced or not.Since the data is well balanced upon the 10 categories given to us so we don't need to upsample or downsample the classes.","3cb5d34a":"Pretty standard configuration: 0 is black, 255 is white. We should adjust the values so they fall between 0 and 1, which makes it easier for the model to handle later. We should also reshape and re-type the data to the required format. Let's wrap it all up in a function.\nSo we normailize the data so that we get values between 0 and 1 so model can converge faster."}}