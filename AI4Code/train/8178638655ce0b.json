{"cell_type":{"683c2af7":"code","5281a902":"code","105fbde9":"code","d8dc0116":"code","23dd5e3e":"code","dae9867b":"code","f913b964":"code","8945cb2c":"code","268b042d":"code","58aab657":"code","5dd18ed1":"code","7045c399":"code","f487f759":"code","d19357ed":"code","4fde9732":"code","55ed2d22":"code","5fc77d64":"code","3f46ac42":"code","6c981060":"code","3a430e44":"code","12d59736":"markdown","58318076":"markdown","ef8b82fc":"markdown","f86765f5":"markdown","0dfe73d8":"markdown","8e7e2db6":"markdown","ecc89376":"markdown","a14c4b2e":"markdown","2926f6b1":"markdown","b9278b55":"markdown"},"source":{"683c2af7":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n","5281a902":"# load datasets\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","105fbde9":"train.head()","d8dc0116":"test.head()","23dd5e3e":"train_len = len(train)\ntest_copy = test.copy()","dae9867b":"total = train.append(test)\ntotal.isnull().sum()","f913b964":"total[total.Fare.isnull()]","8945cb2c":"total['Fare'].fillna(value = total[total.Pclass==3]['Fare'].median(), inplace = True)","268b042d":"total['Title'] = total['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\nplt.figure(figsize=(8,6))\nsns.countplot(x= \"Title\",data = total)\nplt.xticks(rotation='45')\nplt.show()","58aab657":"# Replacing rare titles with more common ones\nmapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\ntotal.replace({'Title': mapping}, inplace=True)","5dd18ed1":"# fill the missing value for Age column with median of its title\ntitles = list(total.Title.unique())\nfor title in titles:\n    age = total.groupby('Title')['Age'].median().loc[title]\n    total.loc[(total.Age.isnull()) & (total.Title == title),'Age'] = age","7045c399":"# add family size as a feature\ntotal['Family_Size'] = total['Parch'] + total['SibSp']","f487f759":"total['Last_Name'] = total['Name'].apply(lambda x: str.split(x, \",\")[0])\ntotal['Fare'].fillna(total['Fare'].mean(), inplace=True)\n\ndefault_survival_rate = 0.5\ntotal['Family_Survival'] = default_survival_rate\n\nfor grp, grp_df in total[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n    \n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                total.loc[total['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin==0.0):\n                total.loc[total['PassengerId'] == passID, 'Family_Survival'] = 0\n\nprint(\"Number of passengers with family survival information:\", \n      total.loc[total['Family_Survival']!=0.5].shape[0])","d19357ed":"for _, grp_df in total.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    total.loc[total['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    total.loc[total['PassengerId'] == passID, 'Family_Survival'] = 0\n                        \nprint(\"Number of passenger with family\/group survival information: \" \n      +str(total[total['Family_Survival']!=0.5].shape[0]))\n","4fde9732":"# add fare bins\ntotal['Fare_Bin'] = pd.qcut(total['Fare'], 5,labels=False)\n# add age bins\ntotal['Age_Bin'] = pd.qcut(total['Age'], 4,labels=False)","55ed2d22":"# convert Sex to catergorical value\ntotal.Sex.replace({'male':0, 'female':1}, inplace = True)\n\n# only select the features we want\nfeatures = ['Survived','Pclass','Sex','Family_Size','Family_Survival','Fare_Bin','Age_Bin']\ntotal = total[features]","5fc77d64":"# split total to train and test set\ntrain = total[:train_len]\n# set Survied column as int\nx_train = train.drop(columns = ['Survived'])\ny_train = train['Survived'].astype(int)\n\nx_test = total[train_len:].drop(columns = ['Survived'])","3f46ac42":"# Scaling features\nscaler = StandardScaler()\nscaler.fit(x_train)\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)","6c981060":"clf = KNeighborsClassifier()\nparams = {'n_neighbors':[6,8,10,12,14,16,18,20],\n         'leaf_size':list(range(1,50,5))}\n\ngs = GridSearchCV(clf, param_grid= params, cv = 5,scoring = \"roc_auc\",verbose=1)\ngs.fit(x_train, y_train)\nprint(gs.best_score_)\nprint(gs.best_estimator_)\nprint(gs.best_params_)","3a430e44":"preds = gs.predict(x_test)\npd.DataFrame({'PassengerId': test_copy['PassengerId'], 'Survived': preds}).to_csv('submission.csv', index = False)\n","12d59736":"# Load datasets","58318076":"# Model Building\nSome of the most common hyperparameters are:\n* n_neighbors\n* weights which can be set to either \u2018uniform\u2019, where each neighbor within the boundary carries the same weight or \u2018distance\u2019 where closer points will be more heavily weighted toward the decision. Note that when weights = 'distance' the class with the highest number in the boundary may not \u201cwin the vote\u201d.","ef8b82fc":"## Feature Scailing","f86765f5":"# Data Preprocess\n* Combine train and test set\n* Fill the missing value for Fare column with median\n* Extract title from name, fill the missing value for Age column according to title's median\n","0dfe73d8":"It\u2019s important to set verbose so you\u2019ll get feedback on the model and know how long it may take to finish. kNN can take a long time to complete as it measures the individual distances for each point in the test set.","8e7e2db6":"# K-nearest neighbours algorithm\n> k-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, normalizing the training data can improve its accuracy dramatically.\nBoth for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. \n\n> The best choice of k depends upon the data; generally, larger values of k reduces effect of the noise on the classification,but make boundaries between classes less distinct. A good k can be selected by various heuristic techniques (see hyperparameter optimization). The special case where the class is predicted to be the class of the closest training sample (i.e. when k = 1) is called the nearest neighbor algorithm.","ecc89376":"# Grid Search\n> Grid search is essentially an optimization algorithm which lets you select the best parameters for your optimization problem from a list of parameter options that you provide, hence automating the 'trial-and-error' method. It is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set.","a14c4b2e":"# Train and Test set","2926f6b1":"# Import Libraries","b9278b55":"# *Please upvote the kernel if you found it insightful!*"}}