{"cell_type":{"39c4f545":"code","0d3bd611":"code","8a22da97":"code","a5d7f00e":"code","6af5b9e8":"code","31572a35":"code","1c56d1c1":"code","8cf83404":"code","d722e133":"code","dedb9d23":"code","b39f037a":"code","e7e3f182":"code","0b9cb1d9":"code","798b53e5":"code","4ea08ebb":"code","b94a9d75":"code","5d5b7bcf":"code","aec8ebd8":"code","5eade4b2":"code","96c8256c":"markdown","2f80eadd":"markdown","837e3465":"markdown","29e1ed02":"markdown","7faea692":"markdown","6d708001":"markdown","3da445f4":"markdown","e151bebe":"markdown","2cec5392":"markdown","9a3bd80c":"markdown","fb3d3a9e":"markdown","2726ea70":"markdown","391860d1":"markdown","a53c853b":"markdown","c7d6956c":"markdown","7a6556d4":"markdown","0e642652":"markdown","be305b1a":"markdown","dd436e2f":"markdown"},"source":{"39c4f545":"import pandas\nimport pandas as pd\ndf = pd.read_csv('..\/input\/blobs-reg\/blobs_reg.csv')         #create a dataframe named 'data' from 'blobs.csv' file\nprint(df.head())","0d3bd611":"X =  df[['feature1', 'feature2']].values                                    #extract feature1 and feature2 values\ny =  df['target'].values                                    #extract target values\n\nassert X.shape == (5000, 2)\nassert y.shape == (5000, )","8a22da97":"import matplotlib.pyplot as plt\nimport matplotlib.colors\n%matplotlib inline\ncolors=['blue','green']\ncmap = matplotlib.colors.ListedColormap(colors)\n#Plot the figure\nplt.figure()\nplt.title('Non-linearly separable classes')\nplt.scatter(X[:,0], X[:,1], c=y,\n           marker= 'o', s=50,cmap=cmap,alpha = 0.5 )\nplt.show()","a5d7f00e":"X_data = X.T\ny_data = y.reshape(1, -1)\n\nassert X_data.shape == (2, 5000)\nassert y_data.shape == (1, 5000)","6af5b9e8":"layer_dims = [2, 25, 25, 1]","31572a35":"import tensorflow as tf","1c56d1c1":"def placeholders(num_features):\n    A_0 = tf.placeholder(dtype = tf.float64, shape = ([num_features,None]))\n    Y = tf.placeholder(dtype = tf.float64, shape = ([1,None]))\n    return A_0,Y","8cf83404":"def initialize_parameters_deep(layer_dims):\n    tf.set_random_seed(1)\n    L = len(layer_dims)\n    parameters = {}\n    for l in range(1,L):\n        parameters['W' + str(l)] = tf.get_variable(\"W\" + str(l), shape=[layer_dims[l], layer_dims[l-1]], dtype = tf.float64,\n                                   initializer=tf.contrib.layers.xavier_initializer())\n                                   \n        parameters['b' + str(l)] = tf.get_variable(\"b\"+ str(l), shape = [layer_dims[l], 1], dtype= tf.float64, initializer= tf.zeros_initializer() )\n        \n    return parameters","d722e133":"initialize_parameters_deep(layer_dims)","dedb9d23":"def linear_forward_prop(A_prev,W,b, activation):\n    Z =   tf.add(tf.matmul(W, A_prev), b)    \n    if activation == \"sigmoid\":\n        A = Z\n    elif activation == \"relu\":\n        A =  tf.nn.relu(Z) \n    \n    return A","b39f037a":"def l_layer_forwardProp(A_0, parameters, drop_out = False):\n    A = A_0\n    L = len(parameters)\/\/2                               #number of layers\n    for l in range(1,L):                                 \n        A_prev = A\n        \n        A = linear_forward_prop(A_prev,parameters['W' + str(l)],parameters['b' + str(l)], \"relu\")                                           \n        #call linear_forward_prop to return the output from current layer\n        \n        if drop_out:                                    #check if dropout == True, if true apply dropout to current layer's output.\n            A =  tf.nn.dropout(x = A, keep_prob = 0.8)                                   \n            # call tensoflow's droupout function to deactivate output A, set keep_prob = 0.8\n    A = linear_forward_prop(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\" )   # return output from final layer.\n    return A","e7e3f182":"def final_cost(Z_final, Y , parameters, regularization = False, lambd = 0):\n    \n    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=Z_final,labels=Y)\n    if regularization:\n        \n        reg_term = 0                               \n        L = len(parameters)\/\/2                     \n        for l in range(1,L+1):\n            #use tensorflow's l2 regularization to calculate regularization term for each later and\n            #sum it up to previous layer's regularization term\n            regularization_term = tf.nn.l2_loss(parameters['W'+str(l)])\n            \n\n            reg_term += regularization_term\n        \n        #multiply lambd\/2 to reg_term to add it to original cost\n        cost += tf.multiply(reg_term, lambd\/2)\n        \n    return tf.reduce_mean(cost)","0b9cb1d9":"def deep_net(X_train,Y_train, layer_dims, learning_rate, num_iter, regularization = False, lambd = 0, drop_out = False):\n    tf.reset_default_graph()                \n    num_features = layer_dims[0]\n    \n    A_0, Y =  placeholders(num_features)                    \n    #call placeholder function to initialize placeholders A_0 and Y\n    parameters = initialize_parameters_deep(layer_dims)                 \n    #Initialse Weights and bias using initialize_parameters_deep() with layer_dims as parameters  \n    Z_final =  l_layer_forwardProp(A_0, parameters, drop_out)\n    #call the function l_layer_forwardProp() to define the final output\n    \n    # call final_cost() function to return the cost that has to be minimized during gradient descent\n    cost = final_cost(Z_final, Y , parameters, regularization, lambd)\n    \n    #call tensorflow's gradient descent optimizer function with minimize cost  \n    train_net =   tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n    \n    init = tf.global_variables_initializer()\n    costs = []\n    with tf.Session() as sess:\n        sess.run(init)\n        for i in range(num_iter):\n            _,c = sess.run([train_net, cost], feed_dict={A_0: X_train, Y: Y_train})\n            if i % 100 == 0:\n                costs.append(c)\n            if i % 1000 == 0:\n                print(c)\n        with open('output.txt', 'w') as file:\n            file.write(\"cost = %f \"  % costs[-1])\n        plt.ylim(min(costs)+0.1 ,max(costs), 4, 0.01)\n        plt.xlabel(\"epoches per 100\")\n        plt.ylabel(\"cost\")\n        plt.plot(costs)\n        plt.show()\n        params = sess.run(parameters)\n    return params","798b53e5":"def predict(A_0, parameters):\n    with tf.Session() as sess:\n        Z = l_layer_forwardProp(A_0, parameters, drop_out= False)\n        A = sess.run(tf.round(tf.sigmoid(Z)))\n    return A","4ea08ebb":"import numpy as np\ndef plot_decision_boundary1( X, y, model):\n    plt.clf()\n    # Set min and max values and give it some padding\n    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1   \n    colors=['blue','green']\n    cmap = matplotlib.colors.ListedColormap(colors)   \n    h = 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    # Predict the function value for the whole grid\n    A = model(np.c_[xx.ravel(), yy.ravel()])\n    A = A.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.contourf(xx, yy, A, cmap=\"spring\")\n    plt.ylabel('x2')\n    plt.xlabel('x1')\n    plt.scatter(X[0, :], X[1, :], c=y, s=8,cmap=cmap)\n    plt.title(\"Decision Boundary for learning rate:\")\n    plt.show()","b94a9d75":"parameters = deep_net(X_data,y_data, layer_dims, learning_rate=0.01, num_iter=10000, regularization = False, lambd = 0, drop_out = False) \n\nplot_decision_boundary1(X_data,y,lambda x: predict(x.T,parameters))","5d5b7bcf":"parameters = deep_net(X_data,y_data, layer_dims, learning_rate=0.01, num_iter=10000, regularization = True, lambd = 0.02, drop_out = False) \n\nplot_decision_boundary1(X_data,y,lambda x: predict(x.T,parameters))","aec8ebd8":"parameters = deep_net(X_data,y_data, layer_dims, learning_rate=0.01, num_iter=10000, regularization = False, lambd = 0, drop_out = True) \n\nplot_decision_boundary1(X_data,y,lambda x: predict(x.T,parameters))","5eade4b2":"parameters = deep_net(X_data,y_data, layer_dims, learning_rate=0.01, num_iter=10000, regularization = True, lambd = 0.02, drop_out = True) \n\nplot_decision_boundary1(X_data,y,lambda x: predict(x.T,parameters))","96c8256c":"Run the cell below to define the method to predict outputof the model for given input and parameters.The code has been written for you\nNote that we are not applying droupout during prediction.","2f80eadd":"## Weight and Bias Initialization\nDefine function named initialize_parameters_deep() to initialize weights and bias for each layer\n- Use tf.get_variable to initialise weights and bias, set datatype as float64\n- Make sure you are using xavier initialization for weigths and initialize bias to zeros\n- Parameters - layer_dims\n- Returns - dictionary of weights and bias","837e3465":"## FP for entire network\nDefine forward propagation for entire network as l_layer_forward()\n- Parameters: A_0(input data), parameters(dictionary of weights and bias), dropout (boolean)\n- returns: A(output from final layer)  \nIf dropout = True, deactivate the layers's output with probability value equal to 0.8.  \nuse tensoflow's droupout function to apply dropout.","29e1ed02":"- The data is provided as file named 'blobs.csv'.\n- The data has two features feature1 and feature2 and one targer variable which is a binary value\n- Using pandas read the csv file and assign the resulting dataframe to variable 'df'   \n- for example if file name is 'xyz.csv' read file as **pd.read_csv('xyz.csv')**\n- Packages to import: **pandas** (to read csv file)","7faea692":"## define the model to train the network\n- parameters:\n  - X_train, Y_train: input and target data\n  - layer_dims: network configuration\n  - learning_rate\n  - num_iter: number of epoches\n  - regularization (boolean): If true apply regularization\n  - lambd: regularization parameter ($\\lambda$)\n  - dropout (boolean): if true apply dropout\n- return: dictionary of trained parameters","6d708001":"## Introduction\n- In this handson you will be building an optimized network for binary classification.\n- You will incorporate regularization and dropout techniques in order to generalize the model's prediction.","3da445f4":"## Visualise Train Data\n- Run the below cell to visualize the data in x-y plane. (visualization code has been written for you)\n- The blue spots corresponds to target value 0 and green spots corresponds to target value 1","e151bebe":"import tensorflow package as tf","2cec5392":"- In order to feed the network the input has to be of **shape (number of features, number of samples)** and target should be of shape **(1, number of samples)**\n- Transpose X and assign it to variable 'X_data'\n- reshape y to have shape (1, number of samples) and assign to variable 'y_data'","9a3bd80c":"## Define the cost function\n- parameters:\n  - Z_final: output fro final layer\n  - Y: actual output\n  - parameters: dictionary of weigths and bias\n  - regularization : boolean\n  - lambd: regularization parameter\n- First define the original cost using tensoflow's sigmoid_cross_entropy function\n- If **regularization == True** add regularization term to original cost function","fb3d3a9e":"If you are able to train the network and visualize the decision boundary you can see that model's prediction has defined the boundary","2726ea70":"# Forward Propagation for Current Layer\nDefine functon named linear_forward_prop() to define forward propagation for a given layer.\n- parameters: A_prev(output from previous layer), W(weigth matrix of current layer), b(bias vector for current layer),activation(type of activation to be used for out of current layer)  \n- returns: A(output from the current layer)\n- Use relu activation for hidden layers and for final output layer return the output unactivated i.e if activation is sigmoid","391860d1":"## DNN Visualisation Results:\n\nBuild the model with no dropout and regualrization,   \nassign **learning_rate = 0.01** and **num_iter = 10000**","a53c853b":"## Define Layers\nDefine the network dimension to have two input features, two hidden layers with 25 nodes each, one output node at final layer. ","c7d6956c":"## Preparing Data\n- Extract feature1 and feature2 values from dataframe 'df' and assign it to variable 'X'\n- Extract target variable 'traget' and assign it to variable 'y'.  \nHint:\n - Use .values to exract values from dataframe","7a6556d4":"## define the method to plot the decision boundary.","0e642652":"## Placeholders\nDefine a function named placeholders to return two placeholders one for input data as A_0 and one for output data as Y.\n- Set the datatype of placeholders as float64\n- parameters - num_features\n- Returns - A_0 with shape (num_feature, None) and Y with shape(1,None)","be305b1a":"Train the model with dropout  \nassign **learning_rate = 0.01, num_iter = 10000 , regularization = False, drop_out = True**","dd436e2f":"Now build the model with regularization.  \nassign **learning_rate = 0.01, num_iter = 10000 , regularization = True, lambd = 0.02**"}}