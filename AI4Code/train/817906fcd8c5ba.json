{"cell_type":{"6496b6e4":"code","46d11e40":"code","ddc070bb":"code","6eecc5eb":"code","349ab68f":"code","1bc6491e":"code","49bb895d":"code","8d2fcc26":"code","46c56222":"code","3df8ebf2":"code","35cc8064":"code","3943557b":"code","c834199b":"code","32fb5fa8":"code","5daa4916":"code","2ec18d67":"code","76a96131":"code","4ca86fa1":"code","70e35b23":"code","1d53ac23":"code","a8f3b9be":"code","6538c882":"code","1157243b":"code","c421b5d0":"code","62f8528d":"code","7bdbc31f":"code","6f50147f":"code","fffa5e5f":"code","10cb43dd":"code","db0b0425":"code","186889de":"code","851c57d7":"code","3c58e482":"code","56420cde":"code","c7663699":"code","131afc72":"code","2ae2f26d":"code","cc59615c":"code","e0a2b56a":"code","54913692":"code","67123ff5":"code","9e6608f7":"code","efdef562":"code","27aa1605":"code","ef3d20ca":"code","e4a5d641":"code","d14c90b4":"code","988ea1e3":"code","8148d65c":"code","836258ed":"code","fa8b1f0c":"code","fffcfa6c":"code","eab0812f":"code","3066b2f9":"code","b83d3bce":"code","f97c4882":"code","d90907e9":"code","a36bf9c8":"code","9bbba1ec":"code","a40d3dcf":"code","6f0ac64e":"code","4601d848":"code","ea1d57f5":"code","abe6ec4a":"code","cb87c56b":"code","4f311edc":"code","cad196bf":"code","2771f055":"code","a0bb06a0":"code","c510acb4":"code","2258be4f":"code","8b98fc28":"code","0676452c":"code","d21353f3":"code","bae7ac5c":"code","2ec1191b":"code","ae63b81c":"code","0c7e9f7f":"code","6087f25a":"code","699667c0":"code","3abdcfa9":"code","ccad6ac6":"code","3c142ff3":"code","3ca5f1b3":"code","56014c6e":"code","5ffe64c0":"code","81b0871b":"code","f67adf32":"markdown","51945f2b":"markdown","e7a23652":"markdown","651c1e95":"markdown","77d097a0":"markdown","7f1abf9d":"markdown","2a12d7ee":"markdown","21887555":"markdown","7227634b":"markdown","37bf7fa1":"markdown","88190770":"markdown","388dbbe5":"markdown","451bb773":"markdown","375c1901":"markdown","8e49736e":"markdown"},"source":{"6496b6e4":"#Installation of required libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn import neighbors\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing","46d11e40":"#Reading the dataset\ndf = pd.read_csv(\"\/kaggle\/input\/hitters\/Hitters.csv\")\ndf.head()","ddc070bb":"#Feature information\ndf.info()","6eecc5eb":"#Summary information about features\ndf.describe().T","349ab68f":"df.shape","1bc6491e":"# Number of unique observations in variables\ndf.nunique()","49bb895d":"#How many missing values?\ndf.isnull().sum()","8d2fcc26":"#Eksik verilere bak\u0131\u015f\ndf[df[\"Salary\"].isnull()].head()","46c56222":"df[\"League\"].value_counts()","3df8ebf2":"df[\"NewLeague\"].value_counts()","35cc8064":"df[\"Division\"].value_counts()","3943557b":"#Distribution of dependent variable\nsns.distplot(df.Salary);","c834199b":"# Access to the correlation of the data set was provided. What kind of relationship is examined between the variables. \n# If the correlation value is> 0, there is a positive correlation. While the value of one variable increases, the value of the other variable also increases.\n# Correlation = 0 means no correlation.\n# If the correlation is <0, there is a negative correlation. While one variable increases, the other variable decreases. \n# When the correlations are examined, there are 2 variables that act as a positive correlation to the Salary dependent variable.\n# These variables are CRBI and CRuns. As these increase, Salary (Salary) variable increases.\ndf.corr()","32fb5fa8":"# Correlation matrix graph of the data set\nf, ax = plt.subplots(figsize= [20,15])\nsns.heatmap(df.corr(), annot=True, fmt=\".2f\", ax=ax, cmap = \"magma\" )\nax.set_title(\"Correlation Matrix\", fontsize=20)\nplt.show()","5daa4916":"# The variables related to their careers were divided into career years and new values were created in the data set by obtaining average values.\ndf[\"AvgCAtBat\"]=df[\"CAtBat\"]\/df[\"Years\"]\ndf[\"AvgCHits\"]=df[\"CHits\"]\/df[\"Years\"]\ndf[\"AvgCHmRun\"]=df[\"CHmRun\"]\/df[\"Years\"]\ndf[\"AvgCRuns\"]=df[\"CRuns\"]\/df[\"Years\"]\ndf[\"AvgCRBI\"]=df[\"CRBI\"]\/df[\"Years\"]\ndf[\"AvgCWalks\"]=df[\"CWalks\"]\/df[\"Years\"]\n\ndf.head()","2ec18d67":"#Based on some trials and correlation results, we subtract variables that do not contribute to the model from our data set.\ndf = df.drop(['AtBat','Hits','HmRun','Runs','RBI','Walks',\"PutOuts\",'Assists','Errors',\n              'League','NewLeague',\"Division\"],axis=1)\n","76a96131":"df.info()","4ca86fa1":"# Eksik g\u00f6zlemlerin g\u00f6rselle\u015ftirilmesi i\u00e7in missingno k\u00fct\u00fcphanesi kullan\u0131larak eksik g\u00f6zlemler g\u00f6rselle\u015ftirilmi\u015ftir.\nimport missingno as msno\nmsno.bar(df);","70e35b23":"#We fill in the missing observations with the KNN method.\nfrom sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors = 4)\ndf_filled = imputer.fit_transform(df)","1d53ac23":"df = pd.DataFrame(df_filled,columns = df.columns)\ndf.head()","a8f3b9be":"#We conduct a stand alone observation review for the salary variable\n#We suppress contradictory values\nQ1 = df.Salary.quantile(0.25)\nQ3 = df.Salary.quantile(0.75)\nIQR = Q3-Q1\nlower = Q1 - 1.5*IQR\nupper = Q3 + 1.5*IQR\ndf.loc[df[\"Salary\"] > upper,\"Salary\"] = upper","6538c882":"#We determine outliers between all variables with the Lof method\nfrom sklearn.neighbors import LocalOutlierFactor\nlof =LocalOutlierFactor(n_neighbors= 10)\nlof.fit_predict(df)","1157243b":"df_scores = lof.negative_outlier_factor_","c421b5d0":"np.sort(df_scores)[0:30]","62f8528d":"#We choose the threshold value according to lof scores\nth = np.sort(df_scores)[7]\nth","7bdbc31f":"#We delete those that are higher than the threshold\ndf = df[df_scores > th]","6f50147f":"df.shape","fffa5e5f":"models = []\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('SVR', SVR()))\nmodels.append(('MLP',MLPRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('GBM', GradientBoostingRegressor()))\nmodels.append((\"XGBoost\", XGBRegressor()))\nmodels.append((\"LightGBM\", LGBMRegressor()))\nmodels.append((\"CatBoost\", CatBoostRegressor(verbose = False)))","10cb43dd":"y = df[\"Salary\"]\nX = df.drop(\"Salary\", axis=1)\ncols = X.columns\ncols","db0b0425":"#Feature Selection\n#Wrapper Method\n#Backward Elimination\n#https:\/\/towardsdatascience.com\/feature-selection-with-pandas-e3690ad8504b\nimport statsmodels.api as sm\ncols = list(X.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = X[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(y,X_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(selected_features_BE)","186889de":"y = df[\"Salary\"]\nX = df[selected_features_BE]","851c57d7":"scaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.20, \n                                                    random_state=46)","3c58e482":"for name, model in models:\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n        msg = \"%s: (%f)\" % (name, rmse)\n        print(msg)","56420cde":"knn_params = {\"n_neighbors\": np.arange(1,30,1)}\n\nknn_model = KNeighborsRegressor()\n\nknn_cv_model = GridSearchCV(knn_model, knn_params, cv = 10).fit(X_train, y_train)","c7663699":"knn_cv_model.best_params_","131afc72":"knn_tuned = KNeighborsRegressor(**knn_cv_model.best_params_).fit(X_train, y_train)","2ae2f26d":"y_pred = knn_tuned.predict(X_test)\nknn_tuned_score = np.sqrt(mean_squared_error(y_test, y_pred))\nknn_tuned_score","cc59615c":"#nonlinear\nsvr_model = SVR() \n\nsvr_params = {\"C\": [0.01, 0.1,0.3,0.5,0.8,1,5, 10, 50, 100,500,1000,10000]}\n\nsvr_cv_model = GridSearchCV(svr_model, svr_params, cv = 10, n_jobs = -1, verbose =  2).fit(X_train, y_train)","e0a2b56a":"svr_cv_model.best_params_","54913692":"svr_tuned = SVR(**svr_cv_model.best_params_).fit(X_train, y_train)","67123ff5":"y_pred = svr_tuned.predict(X_test)\nsvr_tuned_score = np.sqrt(mean_squared_error(y_test, y_pred))\nsvr_tuned_score","9e6608f7":"cart_params = {\"max_depth\": [2,3,4,5,6,8,10,20,30,50, 100, 500, 1000,5000,10000],\n              \"min_samples_split\": [2,5,10,20,30,50,100,500,1000,5000,10000]}","efdef562":"cart_model = DecisionTreeRegressor()","27aa1605":"cart_cv_model = GridSearchCV(cart_model, cart_params, cv = 10).fit(X_train, y_train)","ef3d20ca":"cart_cv_model.best_params_","e4a5d641":"cart_tuned = DecisionTreeRegressor(**cart_cv_model.best_params_).fit(X_train, y_train)","d14c90b4":"y_pred = cart_tuned.predict(X_test)\ncart_tuned_score = np.sqrt(mean_squared_error(y_test, y_pred))\ncart_tuned_score","988ea1e3":"rf_params = {\"max_depth\": [5,10,None],\n            \"max_features\": [2,5,10],\n            \"n_estimators\": [100, 500, 1000],\n            \"min_samples_split\": [2,10,30]}","8148d65c":"rf_model = RandomForestRegressor(random_state = 42).fit(X_train, y_train)","836258ed":"rf_cv_model = GridSearchCV(rf_model, rf_params, cv = 10, n_jobs = -1, verbose = 2).fit(X_train, y_train)","fa8b1f0c":"rf_cv_model.best_params_","fffcfa6c":"rf_tuned = RandomForestRegressor(max_depth=30,\n            max_features=3,\n            n_estimators=1000,\n            min_samples_split=3).fit(X_train, y_train)","eab0812f":"y_pred = rf_tuned.predict(X_test)\nrf_tuned_score = np.sqrt(mean_squared_error(y_test, y_pred))\nrf_tuned_score","3066b2f9":"Importance = pd.DataFrame({'Importance':rf_tuned.feature_importances_*100}, \n                          index = cols)\n\n\nImportance.sort_values(by = 'Importance', \n                       axis = 0, \n                       ascending = True).plot(kind = 'barh', \n                                              color = 'r', )\n\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","b83d3bce":"gbm_params = {\"learning_rate\": [0.001,0.1,0.01],\n             \"max_depth\": [3,5,8],\n             \"n_estimators\": [200,500,1000],\n             \"subsample\": [1,0.5,0.8]\n             }","f97c4882":"gbm_model = GradientBoostingRegressor()","d90907e9":"gbm_cv_model = GridSearchCV(gbm_model, \n                            gbm_params, \n                            cv = 10, \n                            n_jobs=-1, \n                            verbose = 2).fit(X_train, y_train)","a36bf9c8":"gbm_cv_model.best_params_","9bbba1ec":"gbm_tuned = GradientBoostingRegressor(**gbm_cv_model.best_params_).fit(X_train, y_train)","a40d3dcf":"y_pred = gbm_tuned.predict(X_test)\ngbm_tuned_score = np.sqrt(mean_squared_error(y_test, y_pred))\ngbm_tuned_score","6f0ac64e":"Importance = pd.DataFrame({'Importance':gbm_tuned.feature_importances_*100}, \n                          index = cols)\n\n\nImportance.sort_values(by = 'Importance', \n                       axis = 0, \n                       ascending = True).plot(kind = 'barh', \n                                              color = 'r', )\n\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","4601d848":"xgb_params = {\"learning_rate\": [0.1,0.01,1],\n             \"max_depth\": [2,5,8],\n             \"n_estimators\": [100,500,1000],\n             \"colsample_bytree\": [0.3,0.6,1]}","ea1d57f5":"xgb = XGBRegressor()","abe6ec4a":"xgb_cv_model  = GridSearchCV(xgb,xgb_params, cv = 10, n_jobs = -1, verbose = 2).fit(X_train, y_train)","cb87c56b":"xgb_cv_model.best_params_","4f311edc":"xgb_tuned = XGBRegressor(**xgb_cv_model.best_params_).fit(X_train, y_train)","cad196bf":"y_pred = xgb_tuned.predict(X_test)\nxgb_tuned_score = np.sqrt(mean_squared_error(y_test, y_pred))\nxgb_tuned_score","2771f055":"Importance = pd.DataFrame({'Importance':xgb_tuned.feature_importances_*100}, \n                          index = cols)\n\n\nImportance.sort_values(by = 'Importance', \n                       axis = 0, \n                       ascending = True).plot(kind = 'barh', \n                                              color = 'r', )\n\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","a0bb06a0":"lgb_model = LGBMRegressor()","c510acb4":"lgbm_params = {\"learning_rate\": [0.01, 0.1, 1],\n              \"n_estimators\": [200,1000,10000],\n              \"max_depth\": [2,5,10],\n              \"colsample_bytree\": [1,0.5,0.3]}","2258be4f":"lgbm_cv_model = GridSearchCV(lgb_model, \n                             lgbm_params, \n                             cv = 10, \n                             n_jobs = -1, \n                             verbose =2).fit(X_train, y_train)","8b98fc28":"lgbm_cv_model.best_params_","0676452c":"lgbm_tuned = LGBMRegressor(learning_rate=0.01,\n              n_estimators=300,\n              max_depth=5,\n              colsample_bytree=1).fit(X_train, y_train)","d21353f3":"y_pred = lgbm_tuned.predict(X_test)\nlgbm_tuned_score = np.sqrt(mean_squared_error(y_test, y_pred))\nlgbm_tuned_score","bae7ac5c":"Importance = pd.DataFrame({'Importance':lgbm_tuned.feature_importances_*100}, \n                          index = cols)\n\n\nImportance.sort_values(by = 'Importance', \n                       axis = 0, \n                       ascending = True).plot(kind = 'barh', \n                                              color = 'r', )\n\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","2ec1191b":"catb_model = CatBoostRegressor(verbose = False)","ae63b81c":"catb_params = {\"iterations\": [500,1000,10000],\n              \"learning_rate\": [0.01,0.1,1],\n              \"depth\": [2,6,10]}","0c7e9f7f":"catb_cv_model = GridSearchCV(catb_model, \n                           catb_params, \n                           cv = 5, \n                           n_jobs = -1, \n                           verbose = 2).fit(X_train, y_train)","6087f25a":"catb_cv_model.best_params_","699667c0":"catb_tuned = CatBoostRegressor(iterations=670,\n              learning_rate=0.01,\n              depth=6,verbose=False).fit(X_train, y_train)","3abdcfa9":"y_pred = catb_tuned.predict(X_test)\ncatb_tuned_score = np.sqrt(mean_squared_error(y_test, y_pred))\ncatb_tuned_score","ccad6ac6":"mlp = MLPRegressor()","3c142ff3":"mlp_params = {\"alpha\": [0.1, 0.01, 0.02, 0.001, 0.0001], \n             \"hidden_layer_sizes\": [(10,20), (5,5), (100,100), (1000,100,10)]}","3ca5f1b3":"mlp_cv_model = GridSearchCV(mlp, mlp_params, cv = 10, verbose = 2, n_jobs = -1).fit(X_train, y_train)","56014c6e":"mlp_tuned = MLPRegressor(**mlp_cv_model.best_params_).fit(X_train, y_train)","5ffe64c0":"y_pred = mlp_tuned.predict(X_test)\nmlp_tuned_score = np.sqrt(mean_squared_error(y_test, y_pred))\nmlp_tuned_score","81b0871b":"index = [\"KNN_tuned\",\"SVR_tuned\",\"MLP_tuned\",\"CART_tuned\",\"RF_tuned\",\"GBM_tuned\",\"XGB_tuned\",\"LGBM_tuned\",\"CATB_tuned\"]\ntuned_score_df = pd.DataFrame({\"Tuned Score\":[knn_tuned_score,svr_tuned_score,mlp_tuned_score,cart_tuned_score,\n                                              rf_tuned_score,gbm_tuned_score,xgb_tuned_score,lgbm_tuned_score,catb_tuned_score]})\ntuned_score_df.index = index\ntuned_score_df","f67adf32":"# 3. First Results","51945f2b":"### The results at the end of the notebook are as in the tables below. The best results were obtained with the RF model.\n\n![1.PNG](attachment:1.PNG)","e7a23652":"# Salary Predict with Nonlinear Regression Models in Hitters Dataset\n\n### This dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.\n\n#### In this notebook, We will use Knn, Svr, CART, RF, GBM, XGBoost, LigthGBM, CatB from nonlinear models.\n\n- A data frame with 322 observations of major league players on the following 20 variables.\n- AtBat Number of times at bat in 1986\n- Hits Number of hits in 1986\n- HmRun Number of home runs in 1986\n- Runs Number of runs in 1986\n- RBI Number of runs batted in in 1986\n- Walks Number of walks in 1986\n- Years Number of years in the major leagues\n- CAtBat Number of times at bat during his career\n- CHits Number of hits during his career\n- CHmRun Number of home runs during his career\n- CRuns Number of runs during his career\n- CRBI Number of runs batted in during his career\n- CWalks Number of walks during his career\n- League A factor with levels A and N indicating player\u2019s league at the end of 1986\n- Division A factor with levels E and W indicating player\u2019s division at the end of 1986\n- PutOuts Number of put outs in 1986\n- Assists Number of assists in 1986\n- Errors Number of errors in 1986\n- Salary 1987 annual salary on opening day in thousands of dollars\n- NewLeague A factor with levels A and N indicating player\u2019s league at the beginning of 1987","651c1e95":"## 4.9 MLP model tuning","77d097a0":"## 4.8 CatBoost model tuning","7f1abf9d":" ## 4.1 KNN model tuning","2a12d7ee":"## 4.3 CART model tuning","21887555":"# 1. Data Understanding","7227634b":"## 4.6 XGBoost model tuning","37bf7fa1":"## 4.7 LightGBM model tuning","88190770":"## 4.2 SVR model tuning","388dbbe5":"# 2. Data Preprocessing","451bb773":"# 4. Model Tuning","375c1901":"## 4.4 RF model tuning","8e49736e":"## 4.5 GBM model tuning"}}