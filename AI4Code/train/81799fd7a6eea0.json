{"cell_type":{"719e2c74":"code","0071b951":"code","918a535a":"code","f1bea11c":"code","e8c4b560":"code","a07e4358":"code","cd790a48":"code","c7a133fc":"code","b1d127ff":"code","c4c18c84":"code","c7ea9c01":"code","ff3466a3":"code","5a17ccd1":"code","4ce5842a":"code","9c4f2b9b":"code","737d35aa":"code","0769700d":"code","3182ba1d":"code","e5c13240":"code","378e64de":"code","3f74c98a":"code","8cec166b":"code","6e347ea2":"code","eaf64e94":"code","aefe329c":"code","ba4da7a4":"code","f9f90b0a":"code","2e4fc00a":"code","468a9a2e":"code","d92fc1a5":"markdown","fd9e1397":"markdown","c8b11cf8":"markdown","8d23f860":"markdown","e1b4007b":"markdown","f7378249":"markdown","6f5c716f":"markdown","38e3723a":"markdown","86f09cfd":"markdown","b5bf1e5c":"markdown","ed55f5b6":"markdown","87864160":"markdown","346e244c":"markdown","b40c9451":"markdown","350367d5":"markdown","c660b08c":"markdown","7ce36e61":"markdown","7d675fb9":"markdown","24b04225":"markdown"},"source":{"719e2c74":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, FunctionTransformer\nfrom sklearn.decomposition import KernelPCA, PCA\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import Ridge, Lasso, LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.pipeline import Pipeline\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nimport tensorflow as tf\nimport keras_tuner as kt\nfrom tqdm.notebook import tqdm","0071b951":"import_pipe = lambda df: pd.read_csv(df).pipe(\n                                lambda df: df.drop(['id', 'pubchem_id'], axis = 1)\n                            )","918a535a":"train_path = '..\/input\/cap-4611-2021-fall-assignment-3\/train.csv'\nsubmission_path = '..\/input\/cap-4611-2021-fall-assignment-3\/eval.csv'","f1bea11c":"df = import_pipe(train_path)","e8c4b560":"df.describe()","a07e4358":"plt.xkcd()\ncomponents = 100\npca = PCA(n_components=components).fit(df.drop('Eat', axis=1))\n\npca_trans = pca.transform(df.drop('Eat', axis=1))\npca_trans_var = np.var(pca_trans, axis=0)\npca_trans_var_ratio  = pca_trans_var \/ np.sum(pca_trans_var)\npca_trans_var_cum = np.cumsum(pca_trans_var_ratio)\n\nplt.plot(list(range(components)), pca_trans_var_ratio, \n         label=\"How much each Component Explains\")\nplt.plot(list(range(components)), pca_trans_var_cum, \n         label=\"Cumulative Explination\")\nplt.title('Matplotlib has an xkcd format!!!')\nplt.legend()","cd790a48":"components = 100\nkpca = KernelPCA(n_components=components,kernel='cosine').fit(\n    df.drop('Eat', axis=1).iloc[np.random.randint(0, len(df), size=4500)]\n)\n\nkpca_trans = pca.transform(df.drop('Eat', axis=1))\nkpca_trans_var = np.var(kpca_trans, axis=0)\nkpca_trans_var_ratio  = kpca_trans_var \/ np.sum(kpca_trans_var)\nkpca_trans_var_cum = np.cumsum(kpca_trans_var_ratio)\n\nplt.plot(list(range(100)), kpca_trans_var_ratio, \n         label=\"How much each Component Explains\")\nplt.plot(list(range(100)), kpca_trans_var_cum, \n         label=\"Cumulative Explination\")\nplt.title('PCA but with an oscilating function, maybe there are harmonics in this?')\nplt.legend()","c7a133fc":"components_to_show = 15\npca_reduction = pca.transform(df.drop('Eat', axis=1))\nkpca_reduction = kpca.transform(df.drop('Eat', axis=1))\n\nfig = plt.figure(figsize=(components_to_show,2))\nax = fig.subplots(2, components_to_show, sharey=True)\n\nfor i in range(components_to_show):\n    ax[0][i].scatter(pca_reduction[:, i], df['Eat'],s=0.1, alpha=0.2)\n    ax[1][i].scatter(kpca_reduction[:, i], df['Eat'],s=0.1, alpha=0.2)","b1d127ff":"# feature engineering train test split\nX_train, X_test, y_train, y_test = train_test_split(\n    df.drop('Eat', axis=1), \n    df['Eat'], \n    test_size=0.2, \n    random_state=42)\n# well retrain our pca's to prevent data leakage\npca = PCA(n_components=11).fit(X_train)\nkpca = KernelPCA(n_components=11,kernel='cosine').fit(\n    X_train.iloc[np.random.randint(0, X_train.shape[0], size=4500)]\n)\n_scaler = StandardScaler()\ndef pca_reduce(df, train=False):\n#     if train:\n#         _scaler.fit(df)\n#     df[df == 0] = np.nan\n#     df = _scaler.transform(df)\n#     df[df == np.nan] = 0\n#     df[df == np.inf] = 0\n    tmp = np.empty((df.shape[0], 22))\n    tmp[:, :11] = pca.transform(df)\n    tmp[:, 11:] = kpca.transform(df)\n    return tmp\n\nX_train = pca_reduce(X_train, True)\nX_test = pca_reduce(X_test)","c4c18c84":"regressors = [Ridge, Lasso, LinearRegression, SVR, DecisionTreeRegressor,\n             RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor]\nregressor_names = ['Ridge', 'Lasso', 'LinearRegression', 'SVR', 'DecisionTreeRegressor',\n             'RandomForestRegressor', 'GradientBoostingRegressor', 'ExtraTreesRegressor']","c7ea9c01":"pd.DataFrame(np.array([\n        cross_val_score(\n            reg(), \n            X_train, \n            y_train, \n            cv=5, \n            scoring=lambda est, x, y: np.sqrt(mse(y, est.predict(x))),\n            n_jobs=-1\n        )\n    for reg in tqdm(regressors)\n    ]).T, \n    columns=regressor_names\n).describe()","ff3466a3":"trained_regressors = [reg().fit(X_train, y_train) for reg in tqdm(regressors)]","5a17ccd1":"def add_regressor_features(df):\n    tmp = np.empty((df.shape[0], df.shape[1] + len(trained_regressors)))\n    tmp[:, :df.shape[1]] = df\n    tmp[:, df.shape[1]:] = np.array([reg.predict(df) for reg in trained_regressors]).T\n    return tmp\nX_train = add_regressor_features(X_train)\nX_test = add_regressor_features(X_test)","4ce5842a":"feature_engineering_pipe = Pipeline([\n    ('PCA', FunctionTransformer(pca_reduce)),\n    ('Regressors', FunctionTransformer(add_regressor_features))\n])\nfeature_engineering_pipe","9c4f2b9b":"X_train, X_test, y_train, y_test = train_test_split(\n    df.drop('Eat', axis=1), \n    df['Eat'], \n    test_size=0.2, \n    random_state=42)\nnp.all(feature_engineering_pipe.fit_transform(X_train) == add_regressor_features(pca_reduce(X_train)))","737d35aa":"X_train = feature_engineering_pipe.fit_transform(X_train)\nX_test = feature_engineering_pipe.transform(X_test)","0769700d":"import tensorflow as tf\nimport keras_tuner as kt\nmse = tf.keras.losses.MeanSquaredError()\nrmse = tf.keras.metrics.RootMeanSquaredError()","3182ba1d":"X_train, X_test, y_train, y_test = train_test_split(\n    df.drop('Eat', axis=1), \n    df['Eat'], \n    test_size=0.2, \n    random_state=420)\nmodel = Sequential()\nmodel.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], activation='sigmoid'))\nmodel.add(Dense(500, activation='sigmoid'))\nmodel.add(Dense(500, activation='sigmoid'))\nmodel.add(Dense(1))\nmodel.compile(loss=mse\n              , optimizer='nadam',\n             metrics=[tf.keras.metrics.RootMeanSquaredError()])\nhist = model.fit(X_train, y_train, epochs=200, batch_size=128, validation_split=0.2)\npd.DataFrame(hist.history)[['root_mean_squared_error', 'val_root_mean_squared_error']].plot()","e5c13240":"model.evaluate(X_test, y_test, batch_size=100, return_dict=True)","378e64de":"X_train, X_test, y_train, y_test = train_test_split(\n    df.drop('Eat', axis=1), \n    df['Eat'], \n    test_size=0.2, \n    random_state=42)\ndef model_builder(hp):\n    model = Sequential()\n    activations = ['sigmoid', 'relu', 'elu', 'selu', 'tanh']\n    hp_activation = hp.Choice('input_activation', values=activations)\n    model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], activation=hp_activation))\n    for i in range(hp.Int('Layers', min_value=0, max_value=10, step=1)):\n        hp_units = hp.Int(f'layer_{i}_size', min_value=32, max_value=1024, step=32)\n        hp_activation = hp.Choice(f'layer_{i}_activation', values=activations)\n        model.add(Dense(units=hp_units, activation=hp_activation))\n#     hp_units = hp.Int('units_2', min_value=32, max_value=1024, step=32)\n#     model.add(Dense(units=hp_units, activation='sigmoid'))\n#     hp_units = hp.Int('units_3', min_value=32, max_value=1024, step=32)\n#     model.add(Dense(units=hp_units, activation='sigmoid'))\n    \n    model.add(Dense(1))\n    \n    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5])\n    hp_optimizer = hp.Choice('optimizer', values=\n                             ['A',\n                             'N'])\n    opt = tf.keras.optimizers.Adam(learning_rate=hp_learning_rate) if hp_optimizer == 'A' else tf.keras.optimizers.Nadam(learning_rate=hp_learning_rate)\n    model.compile(loss=mse,\n                  optimizer=opt,\n                  metrics=[tf.keras.metrics.RootMeanSquaredError()]\n                 )\n    return model\n\nstop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\ntuner = kt.Hyperband(\n    model_builder,\n    objective=kt.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n    max_epochs=50,\n    factor=3,\n    overwrite=True)\ntuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n# tuner.search(X_train, y_train, epochs=50, validation_split=0.2)\nbest_model = tuner.get_best_models()[0]","3f74c98a":"best_model.summary()","8cec166b":"hist = best_model.fit(X_train, y_train, epochs=200, batch_size=128, validation_split=0.2)\npd.DataFrame(hist.history)[['root_mean_squared_error', 'val_root_mean_squared_error']].plot()","6e347ea2":"best_model.evaluate(X_test, y_test, batch_size=100, return_dict=True)","eaf64e94":"X_train, X_test, y_train, y_test = train_test_split(\n    df.drop('Eat', axis=1), \n    df['Eat'], \n    test_size=0.2, \n    random_state=42)\nmodel = Sequential()\nmodel.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], activation='sigmoid'))\n# model.add(Dropout(0.25))\nmodel.add(Dense(400, activation='sigmoid'))\n# model.add(Dropout(0.15))\nmodel.add(Dense(100, activation='sigmoid'))\n# model.add(Dropout(0.25))\n# model.add(Dense(500, activation='sigmoid'))\nmodel.add(Dense(1))\nopt = tf.keras.optimizers.Nadam(learning_rate=0.0001)\nmodel.compile(loss=mse\n              , optimizer=opt,\n             metrics=[tf.keras.metrics.RootMeanSquaredError()])\nhist = model.fit(X_train, y_train, epochs=1000, batch_size=1024, validation_split=0.2)\npd.DataFrame(hist.history)[['root_mean_squared_error', 'val_root_mean_squared_error']].plot()\n# y_train.dtype","aefe329c":"model.evaluate(X_test, y_test, batch_size=100, return_dict=True)","ba4da7a4":"pred = np.array(best_model.predict(import_pipe(submission_path)))\nprint(list(pred.flatten()))","f9f90b0a":"submission = pd.read_csv('..\/input\/cap-4611-2021-fall-assignment-3\/sample_submission.csv')\n# submission","2e4fc00a":"submission['Eat'] = pred\nsubmission.describe()","468a9a2e":"submission.to_csv('.\/pred.csv', index=False)","d92fc1a5":"**Jesus Christ on a Bike that has more dimensions in it then a season of Rick and Morty**<br>\nand unfortunatly, it looks like they are all being used so we cant just drop sections of data like we might by croping and image.","fd9e1397":"This is pretty good, beter then i was expecting in fact. We could probably get even better <br>\nscores with some hyperperamiter tuning, but that would take some time and i dont think its <br>\nworth it for the feature engineering phase. These already meet our requirements of producing <br>\nconsistantly close results for our network to improve on so we can probably leave the rest to <br>\nneural net. Worst case, we know where to come back to for further improvements.","c8b11cf8":"# Feature Engineering\nso now that we have our dataset reduced to a measly 22 dimensions, lets start adding back to that.<br>\npython does have a quantom espresso package that we can call and calculate the answeres<br>\nas \"feature engineering\", but i have no idea how to use it with this data so well do things <br>\nthe old fashioned way and make our own features. we already have a kernal acting on the <br>\ndata, so now lets try making some features with machine learning. Ultimatly we want to <br>\nuse a neural net to make predictions, but that doesnt mean traditional ml wont me effective. <br>\nlets first use some classic algos from sklearn on our reduced dataset and take their results <br>\nas new features. after that, we can also make neural nets and have their output feed <br>\ninto our final model too. Each model we make probably wont be all that great to start with, <br>\nbut they will be consistantly wrong and in uncorrilated ways. this will be perfact for a <br>\nneural net to look at and improve uppon.","8d23f860":"# Pipeline\nBefore we go any further, lets package this as a pipeline so we dont miss any steps later","e1b4007b":"# Network 1","f7378249":"# Dimensionality Reduction\/EDA\nthere is no practical way for us to analyse all of those columns of data manualy<br>\nso lets see if we can describe them in less columns first.","6f5c716f":"By the way, on assignment 2, i got 5 points off for misspelling something in a markdown section. If that is realy something that counts for points, then im quite frankley impressed with myself for only misspelling the one thing considdering kaggle doesnt have spellcheck. Anyway im not going to be writing all my markdown in a document editor this time, so i appolagise in advance and hope you will be linient.","38e3723a":"# Submission","86f09cfd":"# Predictions","b5bf1e5c":"# Imports","ed55f5b6":"So here we have a plot of each component with respect to the answer for both pca reductions, <br>\nand as you can see, both start off showing some relationship that may help our<br>\nalgo, but as we go through more components that relationship dwindels until <br>\nno new information seems to be added. This is in line with what we know from <br>\nour earlier plots about how much each component explains. We can also see that <br>the top plots (linear PCA) are different from the ones on bottom (cosine kernal PCA).<br>\nEvidently our earlier charts were correct and in both cases, 11 dimensions seems to be a good number to use.","87864160":"So im putting this here instead of making new code cells, but **WHAT IN THE HYDROFLUORIC FUCK!?!?!?!**, I spent all this time on engineering a great feature set<br>\nthat represents the data well and doesnt have too many features that might reduce the networks effectiveness and what happens? The damn thing wont go below 0.5 <br>\nfor RMSE. When i change it to just the raw data, it instantly goes to 0.35, what the cinnamon toasted hell?","346e244c":"# Import the Data\nHere we will make a pipeline to import our data, <br>\nanything we want to be done to our data while its being imported can be added here.","b40c9451":"If we look at the orange line, it seems like the frist 10ish <br>\ncomponents is enough to enough to explain most of the data. <br>\nnow its probably a coincadance that string theory is 11-dimensional, <br>\nand i dont have the domain knowlage to confirm or deny that <br>\nbut it doesnt matter either way.<br>\n11 dimensions explains most of the data and nothing will stop me <br>\nfrom claiming its because of string theory. So in accordance with stirng theory,<br> we will be using the first 11 principal components of our quantom dataset<br>\nfor our models to learn from.","350367d5":"that looks basicly the same as linear PCA, either i was right about 11 dimensions,<br>or i did something wrong in generating my graphs. lets generate some more graphs to find out.","c660b08c":"# Network 2\nHyperperamiter tuning","7ce36e61":"According to some youtube physics that i wont claim to understand, but will still believe with the blind faith of a puppy who smells peanut butter; not all particles nessiserily exibit all properties in quantom mechanics, and 0's are sometimes used to denote a value that doesnt exist rather then exists and is 0. This is some grade A level bullshit, and quite frankley i dont know how to deal with it properly. Instead, im going assume that all or most of the 0's dont exist and try to do anything that would be affected deferently by them without them, and then add them back in, like ghosts fading in and out of existance, or my gpa.","7d675fb9":"# Neural Nets\n~~Alright, so now we have a relativley small dataset with a bunch of usefull features, <br>\nlets throw some neural nets at it.~~","24b04225":"# Network 3"}}