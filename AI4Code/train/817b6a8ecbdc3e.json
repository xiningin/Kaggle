{"cell_type":{"5d5a1053":"code","71a13a63":"code","d03f4268":"code","9664ff44":"code","15ce5733":"code","455ef342":"code","6383b9cf":"code","291724f2":"code","06bd024a":"code","069e094f":"code","cb6884a8":"code","61379581":"code","3e669d5b":"code","f885426a":"code","b6cff8e8":"code","c97c64b4":"code","3e1639e2":"code","7cbe57a2":"code","5a8236fd":"code","93afbb6a":"code","ba2ae0f2":"code","051af5fa":"code","11c68f0d":"code","9d936a2c":"code","6000097e":"code","b08dac68":"code","49abfb44":"markdown","206924ae":"markdown","9b5e420f":"markdown","13a87701":"markdown","ad38b524":"markdown","dd946ad8":"markdown","608477ce":"markdown","1117e9f1":"markdown","dc96d36a":"markdown","47bff5ec":"markdown","05065562":"markdown","cd081f90":"markdown","66653e81":"markdown","75619e7f":"markdown","cf4927e2":"markdown","e0729ba1":"markdown","7e210dc6":"markdown","1fdee96b":"markdown","f1a84fe4":"markdown","10391e77":"markdown"},"source":{"5d5a1053":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nimport pandas as pd\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport joblib\nimport glob\nfrom tqdm import tqdm_notebook as tqdm\n\nimport torch\nimport cv2\nfrom PIL import Image\n\nimport torchvision.transforms as transforms\nimport torchvision.models as pretrained_models\nimport os\n#import pretrainedmodels\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport torchvision\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nfrom tqdm import trange\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics\nfrom sklearn import preprocessing\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","71a13a63":"## utils\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","d03f4268":"def find_start_end(data: np.ndarray):\n    \"\"\"\n    Calculates start and end of real demand data. Start is an index of first non-zero and\n    end index of non-zero\n\n    :param data: Time series, shape [n_items, n_days]\n    :return : start_idx, end_idx\n    \"\"\"\n\n    n_items = data.shape[0]\n    n_days = data.shape[1]\n    \n    start_idx = np.full(n_items, -1, dtype=np.int32)\n    end_idx = np.full(n_items, -1, dtype=np.int32)\n\n    for item in range(n_items):\n        # scan from start to the end\n        for day in range(n_days):\n            if not np.isnan(data[item, day]) and data[item, day] > 0:\n                start_idx[item] = day \n                break\n        # reverse scan, from end to start\n        for day in range(n_days - 1, -1, -1):\n            if not np.isnan(data[item, day]) and data[item, day] > 0:\n                end_idx[item] = day\n                break\n\n    return start_idx, end_idx\n\ndef read_x(df, start, end) -> pd.DataFrame:\n    \"\"\"\n    Gets source data from start to end data, Any data can be None\n    :param df -> dataframe\n    :param start -> start day\n    :param end -> end day\n\n    :returns -> df \n    \"\"\"\n    if start and end:\n        return df.loc[:, start:end]\n    elif end:\n        return df.loc[:, end]\n    else:\n        return df\n\ndef prepare_data(df , start, end, valid_threshold):\n    \"\"\"\n    Reads source data, calculates start and end of each series, drops bad series, calculates log1p(demand)\n    :param df\n    :param start: start date of effective time interval, can be None to start from beginning\n    :param end: end dae of effective time interval, can be None to return all data\n    :param valid_threshold: minimal ratio of series real length to entire (end-start) interval.Series dropped if\n    ratio is less then threshold\n    :return: tuple(log1p(series), series start, series end)\n    \"\"\"\n\n    df = read_x(df, start, end)\n    starts, ends = find_start_end(df.values)\n\n    # boolean mask for bad (too short) series\n    page_mask = (ends - starts) \/ df.shape[1] < valid_threshold\n\n    print(\"Masked %d pages from %d\" % (page_mask.sum(), len(df)))\n    inv_mask = ~page_mask\n    df = df[inv_mask]\n    \n    return np.log1p(df), starts[inv_mask], ends[inv_mask]\n    #return df, starts[inv_mask], ends[inv_mask]\n\ndef normalize(values: np.ndarray):\n    return (values - values.mean()) \/ np.std(values)\n\n\ndef single_autocorr(series, lag):\n    \"\"\"\n    Autocorrelation for single data series\n    :param series: traffic series\n    :param lag: lag, days\n    :return:\n    \"\"\"\n    s1 = series[lag:]\n    s2 = series[:-lag]\n    ms1 = np.mean(s1)\n    ms2 = np.mean(s2)\n    ds1 = s1 - ms1\n    ds2 = s2 - ms2\n    divider = np.sqrt(np.sum(ds1 * ds1)) * np.sqrt(np.sum(ds2 * ds2))\n    return np.sum(ds1 * ds2) \/ divider if divider != 0 else 0\n\ndef batch_autocorr(data, lag, starts, ends, threshold, backoffset=0):\n    \"\"\"\n    Calculate autocorrelation for batch (many time series at once)\n    :param data: Time series, shape [n_pages, n_days]\n    :param lag: Autocorrelation lag\n    :param starts: Start index for each series\n    :param ends: End index for each series\n    :param threshold: Minimum support (ratio of time series length to lag) to calculate meaningful autocorrelation.\n    :param backoffset: Offset from the series end, days.\n    :return: autocorrelation, shape [n_series]. If series is too short (support less than threshold),\n    autocorrelation value is NaN\n    \"\"\"\n    n_series = data.shape[0]\n    n_days = data.shape[1]\n    max_end = n_days - backoffset\n    corr = np.empty(n_series, dtype=np.float64)\n    support = np.empty(n_series, dtype=np.float64)\n    for i in range(n_series):\n        series = data[i]\n        end = min(ends[i], max_end)\n        real_len = end - starts[i]\n        support[i] = real_len\/lag\n        if support[i] > threshold:\n            series = series[starts[i]:end]\n            c_365 = single_autocorr(series, lag)\n            c_364 = single_autocorr(series, lag-1)\n            c_366 = single_autocorr(series, lag+1)\n            # Average value between exact lag and two nearest neighborhs for smoothness\n            corr[i] = 0.5 * c_365 + 0.25 * c_364 + 0.25 * c_366\n        else:\n            corr[i] = np.NaN\n    return corr #, support","9664ff44":"def encode_id_features(id_features):\n    \n    df = id_features.set_index(\"id\")\n    df[\"id\"] = df.index.values\n\n    #final_df = id\n\n    #item_id\n    item_id = pd.get_dummies(df[\"item_id\"], drop_first=False, prefix=\"item_id\")\n    item_id = (item_id - item_id.mean()) \/ item_id.std()\n\n    # dept_id\n    dept_id = pd.get_dummies(df[\"dept_id\"], drop_first=False, prefix=\"dept_id\")\n    dept_id = (dept_id - dept_id.mean()) \/ dept_id.std()\n\n    # cat_id\n    cat_id = pd.get_dummies(df[\"cat_id\"], drop_first=False, prefix=\"cat_id\")\n    cat_id = (cat_id - cat_id.mean()) \/ cat_id.std()\n\n    # store_id\n    store_id = pd.get_dummies(df[\"store_id\"], drop_first=False, prefix=\"store_id\")\n    store_id = (store_id - store_id.mean()) \/ store_id.std()\n\n    # state_id\n    state_id = pd.get_dummies(df[\"state_id\"], drop_first=False, prefix=\"state_id\")\n    state_id = (state_id - state_id.mean()) \/ state_id.std()\n\n\n    # encoded_id_df\n    final_df = pd.merge(item_id,dept_id, how=\"left\", left_index=True, right_index=True)\n    final_df = pd.merge(final_df,cat_id, how=\"left\", left_index=True, right_index=True)\n    final_df = pd.merge(final_df,store_id, how=\"left\", left_index=True, right_index=True)\n    final_df = pd.merge(final_df,state_id, how=\"left\", left_index=True, right_index=True)\n\n\n\n    #print(f\"item_id : {item_id.shape} , dept_id : {dept_id.shape} , cat_id : {cat_id.shape} , store_id : {store_id.shape}, state_id : {state_id.shape}\")\n\n    print(f\"encoded_id_df : {final_df.shape}\")\n\n    return final_df","15ce5733":"def create_seles_features(sales_df):\n\n    id_features = sales_df[[\"id\",\"item_id\", \"dept_id\", \"cat_id\", \"store_id\",\"state_id\"]]\n\n    sales_df.drop(columns=[\"item_id\", \"dept_id\", \"cat_id\", \"store_id\",\"state_id\"],inplace=True)\n\n    df = sales_df.set_index(\"id\")\n\n    df, starts, ends = prepare_data(df, start=None, end=None, valid_threshold=0.0)\n\n    train_days_range = pd.date_range('29-01-2011',periods=1913)\n    valid_days_range = pd.date_range('25-04-2016',periods=28)\n    test_days_range = pd.date_range('23-05-2016',periods=28)\n\n    df.columns = df.columns.to_series().apply(lambda x: int(x.split(\"_\")[-1]))\n\n    date_start, date_end = train_days_range[0] , train_days_range[-1]\n    features_end = valid_days_range[-1]\n\n    print(f\"date_satart : {date_start} , date_end : {date_end}, features_end : {features_end}\")\n    \n    encoded_id_features = encode_id_features(id_features)\n\n    item_popularity = df.median(axis=1)\n    item_popularity = (item_popularity - item_popularity.mean()) \/ item_popularity.std()\n\n    # Yearly(annual) autocorrelation\n    raw_year_autocorr = batch_autocorr(df.values, 365, starts, ends, 1.5, 0)\n    year_unknown_pct = np.sum(np.isnan(raw_year_autocorr))\/len(raw_year_autocorr)\n    # Normalise all the things\n    year_autocorr = normalize(np.nan_to_num(raw_year_autocorr))\n\n    # Quarterly autocorrelation\n    raw_quarter_autocorr = batch_autocorr(df.values, int(round(365.25\/4)), starts, ends, 2, 0)\n    quarter_unknown_pct = np.sum(np.isnan(raw_quarter_autocorr)) \/ len(raw_quarter_autocorr)  # type: float\n    # Normalise all the things\n    quarter_autocorr = normalize(np.nan_to_num(raw_quarter_autocorr))\n\n    # Monthly autocorrelation\n    raw_month_autocorr = batch_autocorr(df.values, int(round(30)), starts, ends, 2, 0)\n    month_unknown_pct = np.sum(np.isnan(raw_month_autocorr)) \/ len(raw_month_autocorr)  # type: float\n    # Normalise all the things\n    month_autocorr = normalize(np.nan_to_num(raw_month_autocorr))\n\n    # Weekly autocorrelation\n    raw_week_autocorr = batch_autocorr(df.values, int(round(7)), starts, ends, 2, 0)\n    week_unknown_pct = np.sum(np.isnan(raw_week_autocorr)) \/ len(raw_week_autocorr)  # type: float\n    # Normalise all the things\n    week_autocorr = normalize(np.nan_to_num(raw_week_autocorr))\n\n    print(\"Percent of undefined autocorr = yearly:%.3f, quarterly:%.3f\" % (year_unknown_pct, quarter_unknown_pct))\n\n    final_df = pd.DataFrame({\n        \"item_popularity\" : item_popularity,\n        \"year_autocorr\" : year_autocorr,\n        \"quarter_autocorr\": quarter_autocorr,\n        \"month_autocorr\" : month_autocorr,\n        \"week_autocorr\" : week_autocorr\n    })\n\n    final_df.index = df.index.values\n\n    final_df = pd.merge(final_df,encoded_id_features, how=\"left\", left_index=True, right_index=True)\n\n    print(\"id_features : \", final_df.shape)\n\n    extra_features = {\n        \"train_days_range\": train_days_range,\n        \"valid_days_range\": valid_days_range,\n        \"test_days_range\": test_days_range,\n        \"starts\" : starts,\n        \"ends\" : ends\n    }\n    \n    return final_df, df, extra_features","455ef342":"%%time\nsales_train_validation = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv\")\nsales_train_validation = reduce_mem_usage(sales_train_validation)\nid_features_df, demand_df, extra_features_dict = create_seles_features(sales_train_validation)","6383b9cf":"def create_date_features(df):\n    \n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n\n    #df.set_index(\"date\", inplace=True)\n\n    # week day\n    week_period = 7 \/ (2 * np.pi)\n    dow_norm = df[\"wday\"].values \/ week_period\n    wday_cos = np.cos(dow_norm)\n    wday_sin = np.sin(dow_norm)\n    \n    \n    # month\n    month_period = 12 \/ (2 * np.pi)\n    dow_norm = df[\"month\"].values \/ month_period\n    month_cos = np.cos(dow_norm)\n    month_sin = np.sin(dow_norm)\n\n    #print(df[\"date\"])\n\n    # day\n    day_period = 31 \/ (2 * np.pi)\n    dow_norm = df[\"date\"].dt.day \/ day_period\n    day_cos = np.cos(dow_norm)\n    day_sin = np.sin(dow_norm)\n    \n\n    # month\n    #month = pd.get_dummies(df[\"month\"], drop_first=False, prefix=\"month\")\n    #month = (month - month.mean()) \/ month.std()\n\n    # day\n    #day = pd.get_dummies(df[\"date\"].dt.day, drop_first=False, prefix=\"date\")\n    #day = (day - day.mean()) \/ day.std()\n\n\n    # event_name_1\n    event_name_1 = pd.get_dummies(df[\"event_name_1\"], drop_first=False, dummy_na=True, prefix=\"event_name_1\")\n    event_name_1 = (event_name_1 - event_name_1.mean()) \/ event_name_1.std()\n\n    # event_type_1\n    event_type_1 = pd.get_dummies(df[\"event_type_1\"], drop_first=False, dummy_na=True, prefix=\"event_type_1\")\n    event_type_1 = (event_type_1 - event_type_1.mean()) \/ event_type_1.std()\n\n    # event_name_2\n    event_name_2 = pd.get_dummies(df[\"event_name_2\"], drop_first=False, dummy_na=True, prefix=\"event_name_2\")\n    event_name_2 = (event_name_2 - event_name_2.mean()) \/ event_name_2.std()\n\n    # event_type_2\n    event_type_2 = pd.get_dummies(df[\"event_type_2\"], drop_first=False, dummy_na=True, prefix=\"event_type_2\")\n    event_type_2 = (event_type_2 - event_type_2.mean()) \/ event_type_2.std()\n\n    snap_CA = df[\"snap_CA\"].values\n    snap_TX = df[\"snap_TX\"].values\n    snap_WI = df[\"snap_WI\"].values\n\n    final_df = pd.DataFrame({\n        \"date\":df[\"date\"],\n        \"wday_cos\": wday_cos,\n        \"wday_sin\": wday_sin,\n        \"month_cos\": month_cos,\n        \"month_sin\": month_sin,\n        \"day_cos\": day_cos,\n        \"day_sin\": day_sin,\n        \"snap_CA\": snap_CA,\n        \"snap_TX\": snap_TX,\n        \"snap_WI\": snap_WI\n    })\n\n    #final_df = pd.merge(final_df,month, how=\"left\", left_index=True, right_index=True)\n    #final_df = pd.merge(final_df,day, how=\"left\", left_index=True, right_index=True)\n    final_df = pd.merge(final_df,event_name_1, how=\"left\", left_index=True, right_index=True)\n    final_df = pd.merge(final_df,event_type_1, how=\"left\", left_index=True, right_index=True)\n    final_df = pd.merge(final_df,event_name_2, how=\"left\", left_index=True, right_index=True)\n    final_df = pd.merge(final_df,event_type_2, how=\"left\", left_index=True, right_index=True)\n    \n    final_df.set_index(\"date\", inplace=True)\n    #dow = np.stack([wday_cos, wday_sin, month_cos, month_sin, day_cos, day_sin, snap_CA, snap_TX, snap_WI], axis=-1)\n\n    return final_df","291724f2":"%%time\ncalendar = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/calendar.csv\")\ncalendar = reduce_mem_usage(calendar)\ncalendar_features_df = create_date_features(calendar)","06bd024a":"def lag_indexes(begin, end):\n    \"\"\"\n    Calculates indexes for 3, 6, 9, 12 months backward lag for the given date range\n    :param begin: start of date range\n    :param end: end of date range\n    :return : List of 4 Series, one for each lag. For each Series, index is date in range(begin, end),value is an index\n    of target (lagged) date in a same Series. If target date is out of (begin, end) range, index is -1\n    \"\"\"\n\n    dr = pd.date_range(begin, end)\n\n    #key is date, value is day index\n    base_index = pd.Series(np.arange(0, len(dr)), index=dr)\n\n    def lag(offset):\n        dates = dr - offset\n\n        date_lag = []\n        for d in dates:\n            if d in base_index.index:\n                date_lag.append(base_index.loc[d])\n            else:\n                date_lag.append(-1)\n        return pd.Series(data=np.array(date_lag).astype(np.int16), index=dr)\n    \n    return [lag(pd.DateOffset(months=m)) for m in (1, 3, 6, 9, 12)]","069e094f":"lag_idx = lag_indexes('2011-01-29', '2016-05-22')\n\nlag_idx_df = pd.DataFrame({\n    \"month_1\": lag_idx[0],\n    \"month_3\": lag_idx[1],\n    \"month_6\": lag_idx[2],\n    \"month_9\": lag_idx[3],\n    \"month_12\": lag_idx[4],\n})\nlag_index = lag_idx_df.values","cb6884a8":"id_features_df.shape, demand_df.shape, calendar_features_df.shape","61379581":"def fill_nan_before_starts(data: np.ndarray, starts):\n\n    n_items = data.shape[0]\n    n_days = data.shape[1]\n    \n    new_data = data.copy()\n\n    for item in range(n_items):\n        data[item, 0:starts[item]] = -1\n    \n    return data\n\ndemand_original = fill_nan_before_starts(demand_df.values, extra_features_dict[\"starts\"])","3e669d5b":"def convert_pytorch_tensors(df):\n    df_tensor = torch.tensor(df.values)\n    df_indexs = df.index.values\n    return df_tensor, df_indexs\n\nid_tensor, id_idx = convert_pytorch_tensors(id_features_df)\ndemand_tensor, demand_idx = convert_pytorch_tensors(demand_df)\ncalender_tensor, calender_idx = convert_pytorch_tensors(calendar_features_df)","f885426a":"# make train data set for pytorch data loaders\ntrain_df = pd.DataFrame(demand_original, columns = np.arange(1, 1914))\ntrain_df[\"id\"] = id_idx\ntrain_df[\"id_index\"] = np.arange(len(id_idx))\ntrain_df = pd.melt(train_df, id_vars=[\"id\", \"id_index\"], value_name=\"demand\", var_name=\"day\")\ntrain_df = train_df[train_df.demand >= 0]","b6cff8e8":"train_days = np.arange(1856, (1856 - 366), -28)\nvalid_days_df = train_df[train_df.day == 1885]\ntrain_days_df = train_df[train_df.day.isin(train_days)]","c97c64b4":"train_days_df","3e1639e2":"class M5Dataset:\n\n    def __init__(self, df, hparams):\n        \n        self.id = df.id.values\n        self.id_index = df.id_index.values\n        self.day = df.day.values\n\n        self.train_window = hparams[\"train_window\"]\n        self.predict_window = hparams[\"predict_window\"]\n        \n\n    def __len__(self):\n        return len(self.id)\n\n    def __getitem__(self, item):\n\n        id = self.id[item]\n        id_index = self.id_index[item]\n        day = self.day[item]-1 # because day index starts from 1\n\n        id_features = id_tensor[id_index].unsqueeze(0)\n\n        # train_x\n        calendar_train = calender_tensor[day-self.train_window:day, :]\n        id_train = torch.repeat_interleave(id_features, repeats=self.train_window, dim=0)\n        \n        demand_train = demand_tensor[id_index, day-self.train_window:day]\n        lag_index_train = lag_index[day-self.train_window:day]\n        lag_train = demand_tensor[id_index, lag_index_train]\n\n        mean = torch.mean(demand_train)\n        std = torch.std(demand_train)\n\n        norm_demand_train = ((demand_train - mean) \/ std ) if std !=0 else demand_train\n        norm_lag_train = (lag_train - mean) \/ std if std !=0 else lag_train\n        \n        # fingerprint\n        fingerprint = torch.cat([norm_demand_train.unsqueeze(1), norm_lag_train], dim=1)\n        \n        \n        train_x = torch.cat([id_train, calendar_train, fingerprint.double()], dim=1)\n        \n        \n        # train_y\n        calendar_predict = calender_tensor[day:day+self.predict_window, :]\n        id_predict = torch.repeat_interleave(id_features, repeats=self.predict_window, dim=0)\n        \n        demand_predict = demand_tensor[id_index, day:day+self.predict_window]\n        norm_demand_predict = (demand_predict - mean) \/ std\n        \n        lag_index_predict = lag_index[day:day+self.predict_window]\n        lag_predict = demand_tensor[id_index, lag_index_predict]\n        norm_lag_predict = (lag_predict - mean) \/ std if std !=0 else lag_predict\n        \n        train_y = torch.cat([id_predict, calendar_predict, norm_lag_predict.double()], dim=1)\n        \n\n        return {\n            \"encoder_input\":train_x,\n            \"fingerprint\":fingerprint,\n            \"previous_y\":norm_demand_train,\n            \"decoder_input\":train_y,\n            \"prediction_demand\" : demand_predict,\n            \"mean\":mean,\n            \"std\":std\n        }","7cbe57a2":"# Example\n\nhparams = {\n    \"train_window\":100,\n    \"predict_window\":28\n}\n\nm5d = M5Dataset(valid_days_df,hparams)","5a8236fd":"out = m5d.__getitem__(1)\n\nout[\"encoder_input\"].shape, out[\"decoder_input\"].shape , out[\"prediction_demand\"].shape, out[\"fingerprint\"].shape","93afbb6a":"class Encoder(nn.Module):\n    def __init__(self, hparams):\n        super(Encoder, self).__init__()\n\n        #print(hparams[\"encoder_input_size\"])\n\n        self.RNN = nn.RNN(\n                    input_size=hparams[\"encoder_input_size\"],\n                    hidden_size=hparams[\"encoder_hidden_size\"], \n                    num_layers=hparams[\"encoder_rnn_layers\"],\n                    batch_first=True,\n                    #dropout = hparams[\"encoder_dropout\"] if hparams[\"encoder_dropout\"] else 0\n                )\n        \n        self.compressed_readout = nn.Linear(hparams[\"encoder_hidden_size\"], hparams[\"attention_depth\"])\n        \n        \n    def forward(self, X):\n        \n        rnn_out, rnn_state = self.RNN(X)\n\n        encoder_state = rnn_state.squeeze(0)\n\n        encoder_readout = F.selu(self.compressed_readout(rnn_out))\n\n        return rnn_out, rnn_state, encoder_readout\n\ndef convert_cudnn_state_v2(h_state, hparams,c_state=None, dropout=1.0):\n    \"\"\"\n    Converts RNN state tensor from cuDNN representation to TF RNNCell compatible representation.\n    :param h_state: tensor [num_layers, batch_size, depth]\n    :param c_state: LSTM additional state, should be same shape as h_state\n    :return: TF cell representation matching RNNCell.state_size structure for compatible cell\n    \"\"\"\n    def squeeze(seq):\n        return tuple(seq) if len(seq) > 1 else seq[0]\n\n    # cases:\n    # decoder_layer = encoder_layers, straight mapping\n    # encoder_layers > decoder_layers : get outputs of upper encoder layers\n    # encoder_layers < deocder_layers : feed encoder outputs to lower decoder layers, feed zeros to top layers\n\n    h_layers = torch.unbind(h_state)\n\n    if hparams[\"encoder_rnn_layers\"] >= hparams[\"decoder_rnn_layers\"]:\n        return squeeze(h_layers[hparams[\"encoder_rnn_layers\"] - hparams[\"decoder_rnn_layers\"]:])\n    else:\n        lower_inputs = [h_layers]\n        upper_inputs = [torch.zeros_like(h_layers[0]) for _ in\n                        range(hparams[\"decoder_rnn_layers\"] - hparams[\"encoder_rnn_layers\"])]\n        #print(len(lower_inputs))\n        #print(upper_inputs)\n        return squeeze(lower_inputs + upper_inputs)\n\nclass FingerPrint(nn.Module):\n    def __init__(self, hparams):\n        super(FingerPrint, self).__init__()\n\n        #print(hparams[\"fingerprint_in_channels\"])\n\n        self.c11 = nn.Conv1d(in_channels=hparams[\"fingerprint_in_channels\"] ,out_channels=16,kernel_size=7, padding=7\/\/2)\n        self.c12 = nn.Conv1d(in_channels=16,out_channels=16, kernel_size=3, padding=3\/\/2)\n        self.pool1 = nn.MaxPool1d(2, 2)\n\n        self.c21 = nn.Conv1d(in_channels=16,out_channels=32,kernel_size=3, padding=3\/\/2)\n        self.c22 = nn.Conv1d(in_channels=32,out_channels=32,kernel_size=3, padding=3\/\/2)\n        self.pool2 = nn.MaxPool1d(2, 2)\n\n        self.c31 = nn.Conv1d(in_channels=32,out_channels=64,kernel_size=3, padding=3\/\/2)\n        self.c32 = nn.Conv1d(in_channels=64,out_channels=64,kernel_size=3, padding=3\/\/2)\n        self.pool3 = nn.MaxPool1d(2, 2)\n\n        self.fc_1 = nn.Linear(in_features=hparams[\"fingerprint_fc1_in_features\"], out_features=512)\n        self.fc_2 = nn.Linear(in_features=512, out_features=hparams[\"fingerprint_fc2_out_features\"])\n\n    def forward(self,x):\n        # x -> [bs, time, features] -> [bs, features, time]\n        x = x.transpose(2,1)\n\n        x = F.relu(self.c11(x))\n        x = F.relu(self.c12(x))\n        x = self.pool1(x)\n\n        x = F.relu(self.c21(x))\n        x = F.relu(self.c22(x))\n        x = self.pool2(x)\n\n        x = F.relu(self.c31(x))\n        x = F.relu(self.c32(x))\n        x = self.pool3(x)\n\n        dims = x.shape\n\n        x = x.reshape(-1, dims[1] * dims[2])\n\n        x = F.selu(self.fc_1(x))\n\n        out_encoder = F.selu(self.fc_2(x))\n\n        return out_encoder\n\n\nclass AttnReadOut(nn.Module):\n    def __init__(self, hparams):\n        super(AttnReadOut, self).__init__()\n\n        self.hparams = hparams\n\n        self.att_focus = nn.Linear(hparams[\"fingerprint_fc2_out_features\"], \n                                   hparams[\"attn_window\"] * hparams[\"attn_heads\"])\n    \n    def forward(self, enc_readout, fingerprint):\n        # enc_readout : [bs, time, att_f]\n        # fingerprint : [bs, features]\n\n        # en_readout : [bs, time, features] -> [attn_f, time, bs]\n        enc_readout = enc_readout.transpose(2,0)\n\n        # [batch(readout_depth), width, channels] -> [batch, height=1, width, channels]\n        inp = enc_readout.unsqueeze(1)\n\n\n        # attn_window = train_window - predict_window + 1\n        # [batch, attn_window * n_heads]\n        filter_logits = self.att_focus(fingerprint)\n\n        # [batch, attn_window * n_heads] -> [batch, attn_window, n_heads]\n        filter_logits = filter_logits.reshape(-1, self.hparams[\"attn_window\"], self.hparams[\"attn_heads\"])\n\n        attns_max = filter_logits \/ filter_logits.sum(dim=1, keepdim=True)\n        # [batch, attn_window, n_heads] -> [width(attn_window), channels(batch), n_heads]\n        attns_max = attns_max.transpose(1,0)\n\n        # [width(attn_window), channels(batch), n_heads] -> [height(1), widht(attn_windw), channels(batch), multiplier(n_heads)]\n        attn_filter = attns_max.unsqueeze(0)\n\n        # [bs, H, W, chanels] -> [bs, in_c, H, W]\n        inp = inp.permute(0, 3, 1, 2)\n        # [out_channels, in_channels\/groups, H, W] -> [H, W, C, n_heads]\n        attn_filter = attn_filter.permute(2, 3, 0, 1)\n\n        cin = inp.shape[1]\n\n        # [batch(readout_depth), height=1, widht=n_days, channels=batch] -> [batch(readout_depth), height=1, widht=predict_window, channels=batch*n_heads]\n        averaged = F.conv2d(inp, attn_filter, padding=0, groups=cin)\n\n        # [batch, channels=readout_depth*n_neads, height=1, width=predict_window] -> [batch, height=1, width=predict_window,channels=readout_depth*n_neads,]\n        averaged = averaged.permute(0, 2, 3, 1)\n        \n        # [batch, height=1, width=predict_window, channels=readout_depth*n_neads] -> [batch(depth), predict_window, batch*n_heads]\n        attn_features = averaged.squeeze(1)\n\n        # [batch(depth), predict_window, batch*n_heads] -> [batch*n_heads, predict_window, depth]\n        attn_features = attn_features.permute(2, 1, 0)\n\n        # [batch * n_heads, predict_window, depth] -> n_heads * [batch, predict_window, depth]\n        heads = [attn_features[head_no::self.hparams[\"attn_heads\"]] for head_no in range(self.hparams[\"attn_heads\"])]\n\n        # n_heads * [batch, predict_window, depth] -> [batch, predict_window, depth*n_heads]\n        result = torch.cat(heads, dim=-1)\n\n        # attn_diag = torch.unstack(attns_max, dim=-1)\n\n        return result, None\n\nclass Decoder(nn.Module):\n    def __init__(self, hparams):\n        super(Decoder, self).__init__()\n\n        self.hparams = hparams\n        \n        if hparams[\"decoder_rnn_layers\"] > 1:\n            pass\n        else:\n            self.cell = nn.GRUCell(input_size=hparams[\"decoder_input_size\"],\n                              hidden_size=hparams[\"decoder_hidden_size\"]\n                              )\n            \n        self.projected_output = nn.Linear(in_features=hparams[\"decoder_fc_in_features\"], out_features=1)\n\n    \n\n    def forward(self, encoder_state, attn_features, prediction_inputs, previous_y):\n        \n        predict_days = self.hparams[\"predict_window\"]\n\n        # [batch_size, time, input_depth] -> [time, batch_size, input_depth]\n        inputs_by_time = prediction_inputs.permute(1,0,2)\n\n        # Return raw outputs for RNN loss calcula\n        targets_ta = []\n        outputs_ta = []\n\n        #print(previous_y.shape)\n        prev_output = previous_y.unsqueeze(-1)\n\n        prev_state = encoder_state\n\n        for time in range(inputs_by_time.shape[0]):\n            \n            # RNN inputs for current step\n            features = inputs_by_time[time]\n\n            # [batch, predict_window, readout_depth * n_heads] -> [batch, readout_depth * n_heads]\n            if attn_features is not None:\n                # [batch_size, 1] + [batch_size,  input_depth]\n                attn = attn_features[:, time, :]\n                # Append previous predicted value + attention vector to input features\n                #print(attn.shape)\n                #print(prev_output.shape)\n                #print(features.shape)\n                next_input = torch.cat([prev_output, features, attn], dim=1)\n            else:\n                # append previous predicted values to input features\n                next_input = torch.cat([prev_output, features], dim=1)\n        \n            # Run RNN cell\n            #print(next_input.shape)\n            output = self.cell(next_input, prev_state)\n\n            # Make prediction from RNN outputs\n            projected_output = self.projected_output(output)\n\n            targets_ta.append(projected_output)\n            outputs_ta.append(output)\n\n            prev_output = projected_output\n            prev_state = output\n\n    \n        # Get final tensors from buffer list\n        targets = torch.stack(targets_ta)\n        raw_outputs = torch.stack(outputs_ta)\n\n        # [time, batch_size, 1] -> [time, batch_size]\n        targets = targets.squeeze(-1)\n\n        return targets , raw_outputs\n\nclass Encoder_Decoder(nn.Module):\n    def __init__(self, hparams):\n        super(Encoder_Decoder, self).__init__()\n\n        self.hparams = hparams\n\n        # Encoder\n        self.encoder = Encoder(hparams)\n\n        # Fingerprint\n        self.fingerprint = FingerPrint(hparams)\n\n        # AttnReadOut\n        self.attn = AttnReadOut(hparams)\n\n        # Decoder\n        self.decoder = Decoder(hparams)\n\n    def forward(self, encoder_input, fingerprint_input, decoder_inputs,previous_y):\n        \n        # Encoder\n        encoder_outputs, rnn_state, encoder_readout = self.encoder(encoder_input)\n        encoder_state = convert_cudnn_state_v2(rnn_state, self.hparams)\n\n        # Fingerprint\n        fingerprint = self.fingerprint(fingerprint_input)\n\n        # AttnReadOut\n        attn_features, attn_weights = self.attn(encoder_readout, fingerprint)\n\n        # Decoder\n        decoder_targets, decoder_outputs = self.decoder(encoder_state, attn_features, decoder_inputs, previous_y)\n\n        return encoder_outputs, decoder_outputs, decoder_targets","ba2ae0f2":"def rnn_stability_loss(rnn_output, beta):\n    \"\"\"\n    REGULARIZING RNNS BY STABILIZING ACTIVATIONS\n    https:\/\/arxiv.org\/pdf\/1511.08400.pdf\n    :param rnn_output: [time, batch, features]\n    :return: loss value\n    \"\"\"\n\n    if beta == 0.0:\n        return 0.0\n    # [batch, time, features] -> [time, batch, features]\n    rnn_output = rnn_output.transpose(1,0)\n    # [time, batch, features] -> [time, batch]\n    l2 = torch.sqrt(torch.sum((rnn_output ** 2), dim=-1))\n    #[time, batch] - []\n    return beta * torch.mean((l2[1:] - l2[:-1]) ** 2)\n\ndef rnn_activation_loss(rnn_output, beta):\n    \"\"\"\n    REGULARIZING RNNS BY STABILIZING ACTIVATIONS\n    https:\/\/arxiv.org\/pdf\/1511.08400.pdf\n    :param rnn_output: [time, batch, features]\n    :return: loss value\n\n    (extra_out['hiddens'].norm(dim=-1) -\n                     args.norm_stabilizer_fixed_point).pow(2).mean())\n    \"\"\"\n    # [batch, time, features] -> [time, batch, features]\n    rnn_output = rnn_output.transpose(1,0)\n\n    if beta == 0.0:\n        return 0.0\n    return rnn_output.norm(dim=-1).pow(2).mean() * beta\n\ndef decoder_predictions(decoder_targets, mean, std):\n    \"\"\"\n    Converts normalized prediction values to log1p(demand) e.g. reverts normalization\n    :param decoder_targets: Decoder targets, shape [n_days, batch]\n    :mean : mean of batch [batch, mean_values]\n    :std : std of batch [batch, std_values] \n    \"\"\"\n\n    # [n_days, batch] -> [batch, n_days]\n    batch_targets = decoder_targets.transpose(1,0)\n    batch_mean = mean.unsqueeze(-1)\n    batch_std = std.unsqueeze(-1)\n\n    return batch_targets * batch_std + batch_mean\n\ndef smape_loss(true, predicted):\n    \"\"\"\n    Differentiable SMAPE loss\n    true : True values\n    predicted : Predicted values\n    \"\"\"\n    epsilon = 0.1 # Smoothing factor, helps SMAPE to be well-behaved near zero\n    true_o = torch.expm1(true)\n    pred_o = torch.expm1(predicted)\n    summ = torch.max(torch.abs(true_o) + torch.abs(pred_o) + epsilon, 0.5 + epsilon)\n    smape = torch.abs(pred_o - true_o) \/ summ * 2.0\n\n    return smape\n\ndef criterion(predictions, true_y, encoder_output, decoder_output):\n\n    enc_stab_loss = rnn_stability_loss(encoder_output, beta=0.5)\n    enc_activation_loss = rnn_activation_loss(encoder_output, beta=0.5)\n\n    dec_stab_loss = rnn_stability_loss(decoder_output, beta=0.5)\n    dec_activation_loss = rnn_activation_loss(decoder_output, beta=0.5)\n\n    mse_loss = nn.MSELoss()(true_y, predictions)\n\n    return mse_loss #+ enc_stab_loss + dec_stab_loss +enc_activation_loss + dec_activation_loss","051af5fa":"# %load cocob.py\nimport torch.optim as optim\nimport torch\n\n# https:\/\/github.com\/anandsaha\/nips.cocob.pytorch\/blob\/master\/cocob_report.ipynb\n###########################################################################\n# Training Deep Networks without Learning Rates Through Coin Betting\n# Paper: https:\/\/arxiv.org\/abs\/1705.07795\n#\n# NOTE: This optimizer is hardcoded to run on GPU, needs to be parametrized\n###########################################################################\n\nclass COCOBBackprop(optim.Optimizer):\n    \n    def __init__(self, params, alpha=100, epsilon=1e-8):\n        \n        self._alpha = alpha\n        self.epsilon = epsilon\n        defaults = dict(alpha=alpha, epsilon=epsilon)\n        super(COCOBBackprop, self).__init__(params, defaults)\n        \n    def step(self, closure=None):\n        \n        loss = None\n        \n        if closure is not None:\n            loss = closure()\n            \n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n        \n                grad = p.grad.data\n                state = self.state[p]\n                \n                if len(state) == 0:\n                    state['gradients_sum'] = torch.zeros_like(p.data).cuda().float()\n                    state['grad_norm_sum'] = torch.zeros_like(p.data).cuda().float()\n                    state['L'] = self.epsilon * torch.ones_like(p.data).cuda().float()\n                    state['tilde_w'] = torch.zeros_like(p.data).cuda().float()\n                    state['reward'] = torch.zeros_like(p.data).cuda().float()\n                    \n                gradients_sum = state['gradients_sum']\n                grad_norm_sum = state['grad_norm_sum']\n                tilde_w = state['tilde_w']\n                L = state['L']\n                reward = state['reward']\n                \n                zero = torch.cuda.FloatTensor([0.])\n                \n                L_update = torch.max(L, torch.abs(grad))\n                gradients_sum_update = gradients_sum + grad\n                grad_norm_sum_update = grad_norm_sum + torch.abs(grad)\n                reward_update = torch.max(reward - grad * tilde_w, zero)\n                new_w = -gradients_sum_update\/(L_update * (torch.max(grad_norm_sum_update + L_update, self._alpha * L_update)))*(reward_update + L_update)\n                p.data = p.data - tilde_w + new_w\n                tilde_w_update = new_w\n                \n                state['gradients_sum'] = gradients_sum_update\n                state['grad_norm_sum'] = grad_norm_sum_update\n                state['L'] = L_update\n                state['tilde_w'] = tilde_w_update\n                state['reward'] = reward_update\n\n        return loss","11c68f0d":"def train_model(model, train_loader, epoch, optimizer, scheduler, DEVICE):\n    model.train()\n\n    total_loss = 0\n\n    t = tqdm(train_loader)\n    for i, d in enumerate(train_loader):\n\n        encoder_input = d[\"encoder_input\"].float().to(DEVICE)\n        fingerprint = d[\"fingerprint\"].float().to(DEVICE)\n        decoder_input = d[\"decoder_input\"].float().to(DEVICE)\n        previous_y = d[\"previous_y\"][:,-1].float().to(DEVICE)\n\n        prediction_demand = d[\"prediction_demand\"].float().to(DEVICE)\n\n        #print(prediction_demand.shape)\n\n        mean = d[\"mean\"].float().to(DEVICE)\n        std = d[\"std\"].float().to(DEVICE)\n\n        optimizer.zero_grad()\n\n        encoder_outputs, decoder_outputs, decoder_targets = model(encoder_input, fingerprint, decoder_input,previous_y)\n\n        predictions = decoder_predictions(decoder_targets, mean, std)\n\n        prediction_demand = prediction_demand.squeeze(-1)\n\n        loss = criterion(predictions, prediction_demand, encoder_outputs, decoder_outputs)\n\n        total_loss += loss\n\n        loss.backward()\n        optimizer.step()\n        \n        t.set_description(f'Epoch {epoch+1} : Loss: %.4f'%(total_loss\/(i+1)))\n\n        if i % int(len(t)\/10) == 0:\n            print(f'Epoch {epoch+1|i} : Loss: %.4f'%(total_loss\/(i+1)))\n\n\ndef valid_model(model, valid_loader, epoch, scheduler, DEVICE):\n    model.eval()\n\n    total_loss = 0\n    RMSE_list = []\n\n    #t = tqdm(valid_loader)\n    with torch.no_grad():\n        for i, d in enumerate(valid_loader):\n\n            encoder_input = d[\"encoder_input\"].float().to(DEVICE)\n            fingerprint = d[\"fingerprint\"].float().to(DEVICE)\n            decoder_input = d[\"decoder_input\"].float().to(DEVICE)\n            previous_y = d[\"previous_y\"][:,-1].float().to(DEVICE)\n\n            prediction_demand = d[\"prediction_demand\"].float().to(DEVICE)\n\n            mean = d[\"mean\"].float().to(DEVICE)\n            std = d[\"std\"].float().to(DEVICE)\n\n            encoder_outputs, decoder_outputs, decoder_targets = model(encoder_input, fingerprint, decoder_input,previous_y)\n\n            predictions = decoder_predictions(decoder_targets, mean, std)\n\n            prediction_demand = prediction_demand.squeeze(-1)\n\n            #print(prediction_demand)\n\n            loss = criterion(predictions, prediction_demand, encoder_outputs, decoder_outputs)\n\n            total_loss += loss\n\n            output = predictions.cpu().numpy()\n            predict = prediction_demand.cpu().numpy()\n\n            for pred, real in zip(output, predict):\n                #print(\"pred : \",pred)\n                #print(\"expm1 : \",np.expm1(pred))\n                rmse = sklearn.metrics.mean_squared_error(np.expm1(real), np.expm1(pred), squared=False)\n                RMSE_list.append(rmse)\n\n            #if i == 1:\n            #    break\n        #print(total_loss\/i+1)\n\n        #t.set_description(f'Epoch {epoch+1} : Loss: %.4f'%(total_loss\/(i+1)))\n\n    print(f\" Valid RMSE : %.4f\"%(np.mean(RMSE_list)))\n\n    return np.mean(RMSE_list)","9d936a2c":"hparams = dict(\n    encoder_input_size = 3136,\n    encoder_hidden_size = 128,\n    encoder_rnn_layers = 1,\n    attention_depth = 64,\n    fingerprint_in_channels = 6,\n    fingerprint_fc1_in_features = 768,\n    fingerprint_fc2_out_features = 16, \n    #attn_window = train_window - predict_window + 1,\n    attn_heads = 1,\n    decoder_rnn_layers=1,\n    decoder_input_size = 3200,\n    decoder_hidden_size = 128, \n    decoder_fc_in_features = 128,\n    predict_window=28,\n    train_window=100,\n    attn_window = 100 - 28 + 1,\n)","6000097e":"TRAIN_BATCH_SIZE = 128\nVALID_BATCH_SIZE = 128\n\ntrain_dataset = M5Dataset(train_days_df, hparams)\n    \ntrain_loader = torch.utils.data.DataLoader(\n        dataset=train_dataset,\n        batch_size= TRAIN_BATCH_SIZE,\n        shuffle=False,\n        num_workers=4,\n        drop_last=False\n    )\n\nvalid_dataset = M5Dataset(valid_days_df, hparams)\n\nvalid_loader = torch.utils.data.DataLoader(\n        dataset=valid_dataset,\n        batch_size= VALID_BATCH_SIZE,\n        shuffle=False,\n        num_workers=4,\n        drop_last=False\n    )","b08dac68":"DEVICE = \"cuda\"\nEPOCHS = 2\nstart_e = 0\n\nmodel = Encoder_Decoder(hparams)\nmodel.to(DEVICE)\n#model.load_state_dict(torch.load(\"CA_2_web_model_0_rmse_4.097639083862305.pth\"))\n\noptimizer = COCOBBackprop(model.parameters())#torch.optim.Adam(model.parameters(), lr=0.001)\n\nfor epoch in range(start_e, EPOCHS):\n    train_model(model, train_loader, epoch, optimizer, scheduler=None, DEVICE=DEVICE)\n    rmse = valid_model(model, valid_loader, epoch, scheduler=None, DEVICE=DEVICE)\n    torch.save(model.state_dict(), f\"sample_28_model_last_28_{epoch}_rmse_{rmse}.pth\")","49abfb44":"### Hparams","206924ae":"### Train & Valid & Test split\n\n- Using last one year for traing\n- and using sliding window for train data\n- for simple i am taking last one year of demand","9b5e420f":"## Problem Formulation\n\n- State -> Store -> Category -> Depoartment -> Item\n- Total Stores -> 10\n- For one Store -> 3049\n- For one Item -> 1913 days\n- For one Item -> Category -> Deportment\n-  We need to predict 28d \"Demand\" for every Item in Every Store","13a87701":"- we can see train_days_df each id have starting day 1492","ad38b524":"\n### Hi... Frineds I tryed different types of NN based Models but no luck, then i read Web-Traffic-Implementation, The winner of this compitation provided code with good documentation, Then i decided to implement his idea for this competition.\n\n### The solution of this competition in Tensorflow. Honestly i am not good with TF. Then i think can i implement this in Pytorch. Then i start reading his solution documentation and go through each and every line try to understand miss something again debug and google it try to implement those in Pytorch... It takes 1week to understand and implement model in pytorch.\n\n### At last i got his flow and implemented his model in pytorch, When i did that i feel very happy.\n\n### With is model i manage to get LB:0.58 ","dd946ad8":"### Encoder Decoder Model\n\n- Model has two main parts: encoder and decoder.\n\n\n![encoder-decoder.png](attachment:encoder-decoder.png)","608477ce":"### COCOB optimizer","1117e9f1":"### Calender Features","dc96d36a":"## Acknowledgements\n\n1. [Web Traffic Time Series Forecasting](https:\/\/www.kaggle.com\/c\/web-traffic-time-series-forecasting\/data)\n\n2. [Web Traffic Time Series Forecasting 1st place solution](https:\/\/github.com\/Arturus\/kaggle-web-traffic)\n\n3. [Encoder Decoder Pytorch](https:\/\/www.kaggle.com\/gopidurgaprasad\/m5-encoder-decoder-pytorch)\n\n4. [EDA + LSTM Pytorch](https:\/\/www.kaggle.com\/gopidurgaprasad\/m5-forecasting-eda-lstm-pytorch-modeling)\n\n","47bff5ec":"### Encode Id Features\n\n- We have 10 Stores in each Store we have 3049 Itema os total items we need to encode is 10 * 3049 = 30490\n\n- In web-traffic-implementation what he did was take each web-page and extract page features like (site, country, agent, etc) from urls then encoded as a page vector or think like embedding for page.\n\n- Same idea i am following hear, total we have 30490 items each iteam have there categories like (item_id, dept_id, cat_id, store_id, state_id)\n\n- From those categories we uniquely identify the items","05065562":"### Utils","cd081f90":"### Feature Engineering\n\n- Demand : Raw values transformed by log1p() to get more-or-less normal intra-series values distribution, instead of skewed one.\n- year-to-year autocorrelation, quarter-to-quarter autocorrelation - to capture yearly and quarterly seasonality strength.\n- page popularity - High Demand and low Demand items have different demand change patterns, this feature (median of demand) helps to capture demand scale. This scale information is lost in a demand feature, because each demand series independently normalized to zero mean and unit variance.\n","66653e81":"### Loss","75619e7f":"### Lag Features\n\n- Calculates indexes fro 1, 3, 6, 9, 12 months backward lag for the given date range","cf4927e2":"### Dataset for Pytorch Data Loaders\n\n![sliding%20window.png](attachment:sliding%20window.png)\n\n- we are using sliding window for model training\n- we need to create train & valid & test dataset for model training\n- the idea to load data using pytorch data loaders like this\n    - we have 30490 items each item contain 1913 days of demand","e0729ba1":"### dataframe convert to Pytorch tensors","7e210dc6":"### Train & Evalution Functions","1fdee96b":"### Create DataLoader\n\n- every time using indexs of item and indexs of day we are taking train window and predict widow data","f1a84fe4":"### Find Start Index\n\n- Calculates start of real demand data. Start is an index of first non-zero and end index of non-zero","10391e77":"### Data Preparation"}}