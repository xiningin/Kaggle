{"cell_type":{"8bb066d7":"code","64cb9088":"code","63fbf5e0":"code","49902bea":"code","7ff376a3":"code","ad8980db":"code","39c05331":"code","e009b66d":"code","fa457e7c":"code","5c7984d2":"code","2f21a9ec":"code","3657836d":"code","0779777c":"markdown"},"source":{"8bb066d7":"import transformers\nfrom transformers import AutoModel,AutoTokenizer,get_cosine_schedule_with_warmup, AutoConfig, AdamW\nimport pandas as pd\nimport os\nimport gc\nimport sys\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.nn.functional import mse_loss\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import RandomSampler, SequentialSampler\nfrom time import time\nfrom tqdm import tqdm\nimport warnings\nfrom pathlib import Path\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import mean_squared_error","64cb9088":"class configuration:\n    tokenizer_path = '..\/input\/robertalarge'\n    clrp_data_path= '..\/input\/commonlitreadabilityprize'\n    pretrained_model_path = '..\/input\/clrp-trained-robertalarge\/robertalarge_clrp_model'\n    output_path='.\/clrp-robertalarge-modelweights'\n    RunningOnKaggle=True\n    epochs = 3\n    batch_size = 4\n    device = 'cuda'\n    base_seed = 42\n    max_len = 256\n    lr = 2e-5\n    wd = 0.01\n    eval_schedule = [(float('inf'), 16), (0.5, 8), (0.49, 4), (0.48, 2), (0.47, 1), (0, 0)]\n    num_folds=5\n    base_seed=42\n    max_length = 256\n    train_batch_size = 4\n    val_batch_size = 16\n    num_warmup_steps=50\n    evaluation_stop_level=120","63fbf5e0":"scaler = torch.cuda.amp.GradScaler() \nDEVICE = torch.device(configuration.device if torch.cuda.is_available() else 'cpu')\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)","49902bea":"train = pd.read_csv(configuration.clrp_data_path + \"\/train.csv\")\ntest = pd.read_csv(configuration.clrp_data_path + \"\/test.csv\")","7ff376a3":"models_dir = Path(configuration.output_path)\nif configuration.RunningOnKaggle==True:\n   models_dir.mkdir(exist_ok=True)","ad8980db":"def seed_everything(seed=configuration.base_seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \ndef get_data_loaders(data, fold):\n    \n    x_train = data.loc[data.fold != fold, 'excerpt'].tolist()\n    y_train = data.loc[data.fold != fold, 'target'].values\n    x_val = data.loc[data.fold == fold, 'excerpt'].tolist()\n    y_val = data.loc[data.fold == fold, 'target'].values\n    \n    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n    \n    encoded_train = tokenizer.batch_encode_plus(\n        x_train, \n        add_special_tokens=True, \n        return_attention_mask=True, \n        padding='max_length', \n        truncation=True,\n        max_length=MAX_LENGTH, \n        return_tensors='pt'\n    )\n    \n    encoded_val = tokenizer.batch_encode_plus(\n        x_val, \n        add_special_tokens=True, \n        return_attention_mask=True, \n        padding='max_length', \n        truncation=True,\n        max_length=MAX_LENGTH, \n        return_tensors='pt'\n    )\n    \n    dataset_train = TensorDataset(\n        encoded_train['input_ids'],\n        encoded_train['attention_mask'],\n        torch.tensor(y_train)\n    )\n    dataset_val = TensorDataset(\n        encoded_val['input_ids'],\n        encoded_val['attention_mask'],\n        torch.tensor(y_val)\n    )\n    \n    dataloader_train = DataLoader(\n        dataset_train,\n        sampler = RandomSampler(dataset_train),\n        batch_size=configuration.train_batch_size\n    )\n\n    dataloader_val = DataLoader(\n        dataset_val,\n        sampler = SequentialSampler(dataset_val),\n        batch_size=configuration.val_batch_size\n    )\n\n    return dataloader_train, dataloader_val","39c05331":"#create folds\nseed_everything()\nx=train.index.to_list()\nrand_idx=random.sample(x, len(x))\ntrain.loc[:,'fold'] = pd.cut(rand_idx, bins=configuration.num_folds,labels=False)\ntarget = train.target.to_numpy()\n","e009b66d":"def convert_examples_to_features(text, tokenizer, max_len):\n\n    tok = tokenizer.encode_plus(\n        text, \n        max_length=max_len, \n        truncation=True,\n        padding='max_length',\n    )\n    return tok\n\n\nclass CLRPDataset(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        self.data = data\n        self.excerpts = self.data.excerpt.tolist()\n        if not is_test:\n            self.targets = self.data.target.tolist()\n            \n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n            excerpt = self.excerpts[item]\n            label = self.targets[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.float),\n            }\n        else:\n            excerpt = self.excerpts[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }","fa457e7c":"class AttentionHead(nn.Module):\n    def __init__(self, h_size, hidden_dim=512):\n        super().__init__()\n        self.W = nn.Linear(h_size, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        \n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\nclass CLRPModel(nn.Module):\n    def __init__(self,transformer,config):\n        super(CLRPModel,self).__init__()\n        self.h_size = config.hidden_size\n        self.transformer = transformer\n        self.head = AttentionHead(self.h_size)\n        self.linear = nn.Linear(self.h_size, 1)\n              \n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer(input_ids, attention_mask)\n        x = self.head(transformer_out.last_hidden_state)\n        x = self.linear(x)\n        return x\n","5c7984d2":"def create_optimizer(model):\n    named_parameters = list(model.named_parameters())    \n    \n    roberta_parameters = named_parameters[:389]    \n    attention_parameters = named_parameters[391:395]\n    regressor_parameters = named_parameters[395:]\n        \n    attention_group = [params for (name, params) in attention_parameters]\n    regressor_group = [params for (name, params) in regressor_parameters]\n\n    parameters = []\n    parameters.append({\"params\": attention_group})\n    parameters.append({\"params\": regressor_group})\n\n    for layer_num, (name, params) in enumerate(roberta_parameters):\n        weight_decay = 0.0 if \"bias\" in name else 0.01\n\n        lr = configuration.lr\n\n  \n        parameters.append({\"params\": params,\n                           \"weight_decay\": weight_decay,\n                           \"lr\": lr})\n\n    return optim.AdamW(parameters)","2f21a9ec":"class AvgCounter:\n    def __init__(self):\n        self.reset()\n        \n    def update(self, loss, n_samples):\n        self.loss += loss * n_samples\n        self.n_samples += n_samples\n        \n    def avg(self):\n        return self.loss \/ self.n_samples\n    \n    def reset(self):\n        self.loss = 0\n        self.n_samples = 0\n\nclass EvaluationScheduler:\n    def __init__(self, evaluation_schedule, penalize_factor=1, max_penalty=8):\n        self.evaluation_schedule = evaluation_schedule\n        self.evaluation_interval = self.evaluation_schedule[0][1]\n        self.last_evaluation_step = 0\n        self.prev_loss = float('inf')\n        self.penalize_factor = penalize_factor\n        self.penalty = 0\n        self.prev_interval = -1\n        self.max_penalty = max_penalty\n\n    def step(self, step):\n        # should we to make evaluation right now\n        if step >= self.last_evaluation_step + self.evaluation_interval:\n            self.last_evaluation_step = step\n            return True\n        else:\n            return False\n        \n            \n    def update_evaluation_interval(self, last_loss):\n        # set up evaluation_interval depending on loss value\n        cur_interval = -1\n        for i, (loss, interval) in enumerate(self.evaluation_schedule[:-1]):\n            if self.evaluation_schedule[i+1][0] < last_loss < loss:\n                self.evaluation_interval = interval\n                cur_interval = i\n                break\n            \n        self.prev_loss = last_loss\n        self.prev_interval = cur_interval\n        \n          \n        \ndef make_dataloader(data, tokenizer, is_train=True):\n    dataset = CLRPDataset(data, tokenizer=tokenizer, max_len=configuration.max_len)\n    if is_train:\n        sampler = RandomSampler(dataset)\n    else:\n        sampler = SequentialSampler(dataset)\n\n    batch_dataloader = DataLoader(dataset, sampler=sampler, batch_size=configuration.batch_size, pin_memory=True)\n    return batch_dataloader\n                   \n            \nclass Trainer:\n    def __init__(self, train_dl, val_dl, model, optimizer, scheduler, criterion, model_num):\n        self.train_dl = train_dl\n        self.val_dl = val_dl\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.device = configuration.device\n        self.batches_per_epoch = len(self.train_dl)\n        self.criterion = criterion\n        self.model_num = model_num\n                \n    def run(self):\n        record_info = {\n            'train_loss': [],\n            'val_loss': [],\n        }\n        \n        best_val_loss = float('inf')\n        evaluation_scheduler = EvaluationScheduler(configuration.eval_schedule)\n        train_loss_counter = AvgCounter()\n        step = 0\n        last_best_loss_update=0\n        for epoch in range(configuration.epochs):\n            \n            print(f'Epoch: {epoch+1}\/{configuration.epochs}')\n            start_epoch_time = time()\n            \n            for batch_num, batch in enumerate(self.train_dl):\n                train_loss = self.train(batch)\n#                 print(f'{epoch+1}#[{step+1}\/{len(self.train_dl)}]: train loss - {train_loss.item()}')\n\n                train_loss_counter.update(train_loss, len(batch))\n                record_info['train_loss'].append((step, train_loss.item()))\n\n                if evaluation_scheduler.step(step):\n                    val_loss = self.evaluate()\n                    \n                    record_info['val_loss'].append((step, val_loss.item()))        \n                    print(f'\\t\\t{epoch+1}#[{batch_num+1}\/{self.batches_per_epoch}]: train loss - {train_loss_counter.avg()} | val loss - {val_loss}',)\n                    train_loss_counter.reset()\n\n                    if val_loss < best_val_loss:\n                        best_val_loss = val_loss\n                        last_best_loss_update=0\n                        print(f\"\\t\\tVal loss decreased from {best_val_loss} to {val_loss}\")\n                        torch.save(self.model, f'{configuration.output_path}\/model_{self.model_num}.bin')\n                    else:\n                        last_best_loss_update+=1\n                    if last_best_loss_update>=configuration.evaluation_stop_level:\n                          print(f\"no update to best loss in last { last_best_loss_update} steps; exiting fold\")  \n                          break\n                    evaluation_scheduler.update_evaluation_interval(val_loss.item())\n                    gc.collect()    \n\n                step += 1\n            end_epoch_time = time()\n            print(f'The epoch took {end_epoch_time - start_epoch_time} sec..')\n            gc.collect()\n        return record_info, best_val_loss\n            \n\n    def train(self, batch):\n        self.model.train()\n        sent_id, mask, labels = batch['input_ids'].to(self.device), batch['attention_mask'].to(self.device), batch['label'].to(self.device), \n        self.model.zero_grad() \n        preds = self.model(sent_id, mask)\n        train_loss = self.criterion(preds, labels.unsqueeze(1))\n        \n        train_loss.backward()\n        self.optimizer.step()\n        self.scheduler.step()\n        return torch.sqrt(train_loss)\n\n    def evaluate(self):\n        self.model.eval()\n        val_loss_counter = AvgCounter()\n\n        for step,batch in enumerate(self.val_dl):\n            sent_id, mask, labels = batch['input_ids'].to(self.device), batch['attention_mask'].to(self.device), batch['label'].to(self.device)\n            with torch.no_grad():\n                preds = self.model(sent_id, mask)\n                loss = self.criterion(preds,labels.unsqueeze(1))\n                val_loss_counter.update(torch.sqrt(loss), len(labels))\n            gc.collect()\n        return val_loss_counter.avg()\n    \n    \ndef mse_loss(y_true,y_pred):\n\n    return nn.functional.mse_loss(y_true,y_pred)","3657836d":"seed_everything(configuration.base_seed)\nbest_scores = []\ngc.collect()\n#for model_num in range(configuration.num_folds): \nfor model_num in [3,4]: \n    print(f'Fold {model_num+1} ')\n\n    tokenizer = AutoTokenizer.from_pretrained(configuration.tokenizer_path)\n    config = AutoConfig.from_pretrained(configuration.pretrained_model_path)\n    config.update({\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n            }) \n\n    train_dl = make_dataloader(train[train.fold!=model_num], tokenizer)\n    val_dl = make_dataloader(train[train.fold==model_num], tokenizer, is_train=False)\n\n    transformer = AutoModel.from_pretrained(configuration.pretrained_model_path, config=config)  \n    model = CLRPModel(transformer, config)\n    model = model.to(configuration.device)\n    optimizer = create_optimizer(model)\n    \n    scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_training_steps=configuration.epochs * len(train_dl),\n            num_warmup_steps=configuration.num_warmup_steps)  \n\n    criterion = mse_loss\n\n    trainer = Trainer(train_dl, val_dl, model, optimizer, scheduler, criterion, model_num)\n    record_info, best_val_loss = trainer.run()\n    best_scores.append(best_val_loss)    \n    \n    steps, train_losses = list(zip(*record_info['train_loss']))\n\n    steps, val_losses = list(zip(*record_info['val_loss']))\n    gc.collect()\nprint('Best val losses:', best_scores)\n#print('Avg val loss:', np.array(best_scores).mean())\n!date '+%A %W %Y %X' > execution_time","0779777c":"**Solution Overview:**\n\nTrain Roberta-Base and RobertaLarge models on the contest data along with supplmemental sources similar to that data.  Fine tune the models using cross-validation folds. Inference weights all 10 models (two trained models * five fine-tuned models [five folds] per model) equally.\n\n**Notebook Sequence:**\n* [Train Roberta Base Model](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-train-robertabase-maskedlm-model)\n* [Train Roberta Large Model](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-train-robertalarge-masked-lm-model\/)\n* [Fine Tune Trained Roberta-Base Model](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-finetune-trained-robertabase)\n* [Fine Tune Trained Roberta Large Model -- **This Notebook**](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-finetune-trained-robertalarge)\n* [Inference Notebook](https:\/\/www.kaggle.com\/charliezimmerman\/clrp-inference-robertabase-robertalarge-ensemble)\n\n**This Notebook influenced by:**\n\n* [https:\/\/www.kaggle.com\/chamecall\/clrp-finetune-single-roberta-base?scriptVersionId=68893027](https:\/\/www.kaggle.com\/chamecall\/clrp-finetune-single-roberta-base?scriptVersionId=68893027)\n* [https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-finetune](https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-finetune)"}}