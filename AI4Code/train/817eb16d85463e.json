{"cell_type":{"f1ec7e50":"code","7ac2d872":"code","7679d668":"code","201f9a12":"code","429814a0":"code","e50ef4b5":"code","ad23a60e":"code","52e76398":"code","2f37ef96":"code","bb705d12":"code","0885b4b9":"code","e7ab7cc8":"code","367d114f":"code","602310da":"code","0ff7e591":"code","eb46317f":"code","d240f535":"code","798fee35":"code","9edd137d":"code","2e99248f":"code","d249909c":"markdown","b869d736":"markdown","485ba120":"markdown","1dec7ced":"markdown","1516d151":"markdown"},"source":{"f1ec7e50":"import os\nimport math\nimport string\nimport random\nimport time\nimport wandb\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nfrom tqdm import tqdm\nimport cv2\nimport numpy as np\nfrom torchvision import transforms","7ac2d872":"class Hparams():\n    def __init__(self):\n\n        # SETS OF CHARACTERS\n        #self.cyrillic = ['PAD', 'SOS', ' ', '!', '\"', '%', '(', ')', ',', '-', '.', '\/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '[', ']', '\u00ab', '\u00bb', '\u0410', '\u0411', '\u0412', '\u0413', '\u0414', '\u0415', '\u0416', '\u0417', '\u0418', '\u0419', '\u041a', '\u041b', '\u041c', '\u041d', '\u041e', '\u041f', '\u0420', '\u0421', '\u0422', '\u0423', '\u0424', '\u0425', '\u0426', '\u0427', '\u0428', '\u0429', '\u042d', '\u042e', '\u042f', '\u0430', '\u0431', '\u0432', '\u0433', '\u0434', '\u0435', '\u0436', '\u0437', '\u0438', '\u0439', '\u043a', '\u043b', '\u043c', '\u043d', '\u043e', '\u043f', '\u0440', '\u0441', '\u0442', '\u0443', '\u0444', '\u0445', '\u0446', '\u0447', '\u0448', '\u0449', '\u044a', '\u044b', '\u044c', '\u044d', '\u044e', '\u044f', '\u0451', 'EOS']\n        self.chars = []\n        # CHARS TO REMOVE\n        self.del_sym = []\n            \n        self.lr = 0.001\n        self.batch_size = 64\n        self.hidden = 512\n        self.enc_layers = 2\n        self.dec_layers = 2\n        self.nhead = 4\n        self.dropout = 0.0\n        \n        # IMAGE SIZE\n        self.width = 256\n        self.height = 64\n\n\nrandom.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nhp = Hparams()","7679d668":"# text to array of indicies\ndef text_to_labels(s, char2idx):\n    return [char2idx['SOS']] + [char2idx[i] for i in s if i in char2idx.keys()] + [char2idx['EOS']]\n\n# convert images and labels into defined data structures\ndef process_data(image_dir, labels_dir, ignore=[]):\n    \"\"\"\n    params\n    ---\n    image_dir : str\n      path to directory with images\n    labels_dir : str\n      path to tsv file with labels\n    returns\n    ---\n    img2label : dict\n      keys are names of images and values are correspondent labels\n    chars : list\n      all unique chars used in data\n    all_labels : list\n    \"\"\"\n\n    chars = []\n    img2label = dict()\n\n    raw = open(labels_dir, 'r', encoding='utf-8').read()\n    temp = raw.split('\\n')\n    for t in temp:\n        try:\n            x = t.split(',')\n            flag = False\n            for item in ignore:\n                if item in x[1]:\n                    flag = True\n            if flag == False:\n                img2label[image_dir + x[0]] = x[1]\n                for char in x[1]:\n                    if char not in chars:\n                        chars.append(char)\n        except:\n            print('ValueError:', x)\n            pass\n\n    all_labels = sorted(list(set(list(img2label.values()))))\n    chars.sort()\n    chars = ['PAD', 'SOS'] + chars + ['EOS']\n\n    return img2label, chars, all_labels\n\n# MAKE TEXT TO BE THE SAME LENGTH\nclass TextCollate():\n    def __call__(self, batch):\n        x_padded = []\n        max_y_len = max([i[1].size(0) for i in batch])\n        y_padded = torch.LongTensor(max_y_len, len(batch))\n        y_padded.zero_()\n\n        for i in range(len(batch)):\n            x_padded.append(batch[i][0].unsqueeze(0))\n            y = batch[i][1]\n            y_padded[:y.size(0), i] = y\n\n        x_padded = torch.cat(x_padded)\n        return x_padded, y_padded\n\n\n# TRANSLATE INDICIES TO TEXT\ndef labels_to_text(s, idx2char):\n    \"\"\"\n    params\n    ---\n    idx2char : dict\n        keys : int\n            indicies of characters\n        values : str\n            characters\n    returns\n    ---\n    S : str\n    \"\"\"\n    S = \"\".join([idx2char[i] for i in s])\n    if S.find('EOS') == -1:\n        return S\n    else:\n        return S[:S.find('EOS')]\n\n\n# COMPUTE CHARACTER ERROR RATE\ndef char_error_rate(p_seq1, p_seq2):\n    \"\"\"\n    params\n    ---\n    p_seq1 : str\n    p_seq2 : str\n    returns\n    ---\n    cer : float\n    \"\"\"\n    p_vocab = set(p_seq1 + p_seq2)\n    p2c = dict(zip(p_vocab, range(len(p_vocab))))\n    c_seq1 = [chr(p2c[p]) for p in p_seq1]\n    c_seq2 = [chr(p2c[p]) for p in p_seq2]\n    return editdistance.eval(''.join(c_seq1),\n                             ''.join(c_seq2)) \/ max(len(c_seq1), len(c_seq2))\n\n\n# RESIZE AND NORMALIZE IMAGE\ndef process_image(img):\n    \"\"\"\n    params:\n    ---\n    img : np.array\n    returns\n    ---\n    img : np.array\n    \"\"\"\n    w, h, _ = img.shape\n    new_w = hp.height\n    new_h = int(h * (new_w \/ w))\n    img = cv2.resize(img, (new_h, new_w))\n    w, h, _ = img.shape\n\n    img = img.astype('float32')\n\n    new_h = hp.width\n    if h < new_h:\n        add_zeros = np.full((w, new_h - h, 3), 255)\n        img = np.concatenate((img, add_zeros), axis=1)\n\n    if h > new_h:\n        img = cv2.resize(img, (new_h, new_w))\n\n    return img\n\n\n# GENERATE IMAGES FROM FOLDER\ndef generate_data(img_paths):\n    \"\"\"\n    params\n    ---\n    names : list of str\n        paths to images\n    returns\n    ---\n    data_images : list of np.array\n        images in np.array format\n    \"\"\"\n    data_images = []\n    for path in tqdm(img_paths):\n        img = cv2.imread(path)\n        try:\n            img = process_image(img)\n            data_images.append(img.astype('uint8'))\n        except:\n            print('ERROR:',path)\n            #img = process_image(img)\n    return data_images","201f9a12":"# store list of images' names (in directory) and does some operations with images\nclass TextLoader(torch.utils.data.Dataset):\n    def __init__(self, images_name, labels, char2idx, idx2char, eval=False):\n        \"\"\"\n        params\n        ---\n        images_name : list\n            list of names of images (paths to images)\n        labels : list\n            list of labels to correspondent images from images_name list\n        char2idx : dict\n        idx2char : dict\n        \"\"\"\n        self.images_name = images_name\n        self.labels = labels\n        self.char2idx = char2idx\n        self.idx2char = idx2char\n        self.eval = eval\n        self.transform = transforms.Compose([\n            transforms.ToPILImage(),\n            #p.torch_transform(),  # random distortion and shear\n            # transforms.Resize((int(hp.height *1.05), int(hp.width *1.05))),\n            # transforms.RandomCrop((hp.height, hp.width)),\n            # transforms.ColorJitter(contrast=(0.5,1),saturation=(0.5,1)),\n            #transforms.RandomRotation(degrees=(-9, 9), fill=255),\n            # transforms.RandomAffine(10 ,None ,[0.6 ,1] ,3 ,fillcolor=255),\n            transforms.transforms.GaussianBlur(3, sigma=(0.1, 1.9)),\n            transforms.ToTensor()\n        ])\n\n    def _transform(self, X):\n        j = np.random.randint(0, 3, 1)[0]\n        if j == 0:\n            return self.transform(X)\n        if j == 1:\n            return tt(ld(vignet(X)))\n        if j == 2:\n            return tt(ld(un(X)))\n        \n\n    def __getitem__(self, index):\n        img = self.images_name[index]\n        if not self.eval:\n            img = self.transform(img)\n            img = img \/ img.max()\n            img = img ** (random.random() * 0.7 + 0.6)\n        else:\n            img = np.transpose(img, (2, 0, 1))\n            img = img \/ img.max()\n\n        label = text_to_labels(self.labels[index], self.char2idx)\n        return (torch.FloatTensor(img), torch.LongTensor(label))\n\n    def __len__(self):\n        return len(self.labels)","429814a0":"img2label, chars, all_words = process_data(\"..\/input\/handwriting-recognition\/train_v2\/train\/\", \"..\/input\/handwriting-recognition\/written_name_train_v2.csv\")\nX_train, y_train = [], []\nhp.chars = chars\nchar2idx = {char: idx for idx, char in enumerate(hp.chars)}\nidx2char = {idx: char for idx, char in enumerate(hp.chars)}\nitems = list(img2label.items())[1000:190000] # a dataset is too big for size of 256x64, so, it needs to be trained by part.\nrandom.shuffle(items)\nfor i, item in enumerate(items):\n    X_train.append(item[0])\n    y_train.append(item[1])\n\nX_train = generate_data(X_train)","e50ef4b5":"train_dataset = TextLoader(X_train, y_train, char2idx, idx2char, eval=False)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True,\n                                           batch_size=4, pin_memory=True,\n                                           drop_last=True, collate_fn=TextCollate())\nepochs, best_eval_loss_cer = 0, float('inf')","ad23a60e":"from torchvision import models\nclass TransformerModel(nn.Module):\n    def __init__(self, bb_name, outtoken, hidden, enc_layers=1, dec_layers=1, nhead=1, dropout=0.1, pretrained=False):\n        # \u0437\u0434\u0435\u0441\u044c \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, resnet50\n        super(TransformerModel, self).__init__()\n        self.backbone = models.__getattribute__(bb_name)(pretrained=pretrained)\n        self.backbone.fc = nn.Conv2d(2048, int(hidden\/2), 1)\n\n        self.pos_encoder = PositionalEncoding(hidden, dropout)\n        self.decoder = nn.Embedding(outtoken, hidden)\n        self.pos_decoder = PositionalEncoding(hidden, dropout)\n        self.transformer = nn.Transformer(d_model=hidden, nhead=nhead, num_encoder_layers=enc_layers,\n                                          num_decoder_layers=dec_layers, dim_feedforward=hidden * 4, dropout=dropout,\n                                          activation='relu')\n\n        self.fc_out = nn.Linear(hidden, outtoken)\n        self.src_mask = None\n        self.trg_mask = None\n        self.memory_mask = None\n        \n        print('backbone: {}'.format(type(self.backbone)))\n        print('layers: {}'.format(enc_layers))\n        print('heads: {}'.format(nhead))\n        print('dropout: {}'.format(dropout))\n\n    def generate_square_subsequent_mask(self, sz):\n        mask = torch.triu(torch.ones(sz, sz), 1)\n        mask = mask.masked_fill(mask == 1, float('-inf'))\n        return mask\n\n    def make_len_mask(self, inp):\n        return (inp == 0).transpose(0, 1)\n\n    def forward(self, src, trg):\n        '''\n        params\n        ---\n        src : Tensor [64, 3, 64, 256] : [B,C,H,W]\n            B - batch, C - channel, H - height, W - width\n        trg : Tensor [13, 64] : [L,B]\n            L - max length of label\n        '''\n        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n            self.trg_mask = self.generate_square_subsequent_mask(len(trg)).to(trg.device) \n        x = self.backbone.conv1(src)\n\n        x = self.backbone.bn1(x)\n        x = self.backbone.relu(x)\n        x = self.backbone.maxpool(x)\n        x = self.backbone.layer1(x)\n        x = self.backbone.layer2(x)\n        x = self.backbone.layer3(x)\n        x = self.backbone.layer4(x) # [64, 2048, 2, 8] : [B,C,H,W]\n            \n        x = self.backbone.fc(x) # [64, 256, 2, 8] : [B,C,H,W]\n        x = x.permute(0, 3, 1, 2) # [64, 8, 256, 2] : [B,W,C,H]\n        x = x.flatten(2) # [64, 8, 512] : [B,W,CH]\n        x = x.permute(1, 0, 2) # [8, 64, 512] : [W,B,CH]\n        \n        src_pad_mask = self.make_len_mask(x[:, :, 0])\n        src = self.pos_encoder(x) # [8, 64, 512]\n        trg_pad_mask = self.make_len_mask(trg)\n        trg = self.decoder(trg)\n        trg = self.pos_decoder(trg)\n\n        output = self.transformer(src, trg, src_mask=self.src_mask, tgt_mask=self.trg_mask,\n                                  memory_mask=self.memory_mask,\n                                  src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=trg_pad_mask,\n                                  memory_key_padding_mask=src_pad_mask) # [13, 64, 512] : [L,B,CH]\n        output = self.fc_out(output) # [13, 64, 92] : [L,B,H]\n\n        return output\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        self.scale = nn.Parameter(torch.ones(1))\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(\n            0, d_model, 2).float() * (-math.log(10000.0) \/ d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.scale * self.pe[:x.size(0), :]\n        return self.dropout(x) ","52e76398":"def train(model, optimizer, criterion, iterator):\n    \"\"\"\n    params\n    ---\n    model : nn.Module\n    optimizer : nn.Object\n    criterion : nn.Object\n    iterator : torch.utils.data.DataLoader\n    returns\n    ---\n    epoch_loss \/ len(iterator) : float\n        overall loss\n    \"\"\"\n    model.train()\n    epoch_loss = 0\n    counter = 0\n    for src, trg in iterator:\n        counter += 1\n        if counter % 5000 == 0:\n            print('[', counter, '\/', len(iterator), ']')\n        if torch.cuda.is_available():\n          src, trg = src.cuda(), trg.cuda()\n\n        optimizer.zero_grad()\n        output = model(src, trg[:-1, :])\n\n        loss = criterion(output.view(-1, output.shape[-1]), torch.reshape(trg[1:, :], (-1,)))\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n\n    return epoch_loss \/ len(iterator)","2f37ef96":"model = TransformerModel('resnet50', len(hp.chars), hidden=hp.hidden, enc_layers=2, dec_layers=2,   \n                         nhead=4, dropout=0.0).to(device)","bb705d12":"model.load_state_dict(torch.load('..\/input\/weights-rnn\/model (5).pt'))","0885b4b9":"optimizer = optim.SGD(model.parameters(), lr=hp.lr)\ncriterion = nn.CrossEntropyLoss(ignore_index=char2idx['PAD'])\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\nepochs = 50\nfor epoch in range(epochs):\n    epoch_loss = train(model, optimizer, criterion, train_loader)\n    print(f'--- epoch {epoch+1} ---')\n    print(f'loss: {epoch_loss}')","e7ab7cc8":"torch.save(model.state_dict(), '.\/model.pt')","367d114f":"import gc\ndel X_train, y_train, train_loader, train_dataset\ngc.collect()","602310da":"img2label, chars, all_words = process_data(\"..\/input\/handwriting-recognition\/test_v2\/test\/\", \"..\/input\/handwriting-recognition\/written_name_test_v2.csv\")\nX_test, y_test = [], []\nitems = list(img2label.items())[1:] # skip header\nfor i, item in enumerate(items):\n    X_test.append(item[0])\n    y_test.append(item[1])\n\nX_test = generate_data(X_test)","0ff7e591":"test_dataset = TextLoader(X_test, y_test, char2idx, idx2char, eval=False)\ntest_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True,\n                                           batch_size=1, pin_memory=True,\n                                           drop_last=True, collate_fn=TextCollate())","eb46317f":"preds = []\nfor src, trg in tqdm(test_loader):\n    #src = torch.FloatTensor(img).unsqueeze(0)\n    if torch.cuda.is_available():\n        src = src.cuda()\n    x = model.backbone.conv1(src)\n    x = model.backbone.bn1(x)\n    x = model.backbone.relu(x)\n    x = model.backbone.maxpool(x)\n\n    x = model.backbone.layer1(x)\n    x = model.backbone.layer2(x)\n    x = model.backbone.layer3(x)\n    x = model.backbone.layer4(x)\n\n    x = model.backbone.fc(x)\n    x = x.permute(0, 3, 1, 2).flatten(2).permute(1, 0, 2)\n    memory = model.transformer.encoder(model.pos_encoder(x))\n\n    p_values = 1\n    out_indexes = [char2idx['SOS'], ]\n    for i in range(100):\n        trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(device)\n        output = model.fc_out(model.transformer.decoder(model.pos_decoder(model.decoder(trg_tensor)), memory))\n\n        out_token = output.argmax(2)[-1].item()\n        p_values = p_values * torch.sigmoid(output[-1, 0, out_token]).item()\n        out_indexes.append(out_token)\n        if out_token == char2idx['EOS']:\n            break\n\n    pred = labels_to_text(out_indexes[1:], idx2char)\n    true = ''.join([ idx2char[ch] for ch in trg.transpose(0,1)[0].numpy()])[3:-3]\n    preds.append({'predicted_transcript': pred, 'true_transcript' : true, 'p_values': p_values})","d240f535":"examples = []\ncounter = 0\nshow_n = 10\nfor image, trans in test_loader:\n    if counter == show_n:\n        break\n    examples.append([image,trans])\n    counter += 1","798fee35":"import matplotlib.pyplot as plt\nplt.imshow(examples[0][0][0].permute(1,2,0).numpy())\nprint(''.join([ idx2char[ch] for ch in examples[0][1].transpose(0,1)[0].numpy()])[3:-3])","9edd137d":"!pip install editdistance","2e99248f":"import editdistance\nN = len(preds)\nwer = 0\ncer = 0\nfor item in preds:\n    if item['true_transcript'] != item['predicted_transcript']:\n        wer += 1\n        cer += char_error_rate(item['predicted_transcript'], item['true_transcript'])\n\ncharacter_accuracy = 1 - (cer \/ N)\nstring_accuracy = 1 - (wer \/ N)\n\nprint(character_accuracy)\nprint(string_accuracy)","d249909c":"# CONFIG","b869d736":"# TRANSFORMER MODEL","485ba120":"# TRAIN","1dec7ced":"# LOAD AND PREPARE TRAIN DATASET","1516d151":"# TEST"}}