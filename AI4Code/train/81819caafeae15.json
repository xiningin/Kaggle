{"cell_type":{"3d1891ab":"code","a0ce1619":"code","bb097b0b":"code","ff95dac4":"code","5a748df5":"code","53131ce3":"code","948e7762":"code","d42baa76":"code","adc27849":"code","9b74a4c4":"code","f1d91cfa":"code","b8900c86":"code","e55a8ce1":"code","c614a7b1":"code","dea4c815":"code","4a8bead1":"code","919198fb":"code","11e03cc8":"markdown","58151f75":"markdown","350fa7ee":"markdown","1963e042":"markdown","f75da26b":"markdown","d043f943":"markdown","86030509":"markdown","56a6cfe7":"markdown","02fbf7c0":"markdown","ebf3961c":"markdown","3a8eca17":"markdown","a5becf8f":"markdown","825cd3c7":"markdown","2157b91d":"markdown","a3946efd":"markdown","2c80a5a1":"markdown","af220f14":"markdown","8fba75cf":"markdown","d9b069dc":"markdown","e3d508ef":"markdown","9bc9de3c":"markdown","81ed317f":"markdown","721f883b":"markdown","bc0b7d02":"markdown","d963f4ee":"markdown","ade3a26f":"markdown"},"source":{"3d1891ab":"# WIP: todo load a video, \n# detect bouding box\n# run a collision model?\n# TODO: It seems there are labels for both videos and images.\n# There are endzone and sideline types of videos.\n#\u00a0Easiest way to build a model? \n#\u00a0Detr model using Pytorch Lightning?\n# EfficientDet model? Yes, based on the following: https:\/\/github.com\/rwightman\/efficientdet-pytorch\n# Add link to streamlit app later? => https:\/\/share.streamlit.io\/yassinealouini\/nfl-impact-streamlit\/app.py\n# Use create_model from effdet.factory.\n#\u00a0There is a problem with making effdet work. How to fix?\n#\u00a0Maybe use YOLO5 instead of EfficientDet?","a0ce1619":"import pandas as pd\nfrom pathlib import Path\nimport cv2\nfrom PIL import Image\nimport matplotlib.pylab as plt\n%matplotlib inline ","bb097b0b":"IMAGE_LABELS_PATH = \"..\/input\/nfl-impact-detection\/image_labels.csv\"\nTRAIN_LABELS_PATH = \"..\/input\/nfl-impact-detection\/train_labels.csv\"\nTRAIN_TRACKING_PATH = \"..\/input\/nfl-impact-detection\/train_player_tracking.csv\"\nTEST_TRACKING_PATH = \"..\/input\/nfl-impact-detection\/test_player_tracking.csv\"","ff95dac4":"\u00ecmg_labels_df = pd.read_csv(IMAGE_LABELS_PATH)","5a748df5":"\u00ecmg_labels_df.sample(2).T","53131ce3":"IMG_ID = \"58084_000728_Sideline_frame0989.jpg\"\nIMG_PATH = Path(\"..\/input\/nfl-impact-detection\/images\/\") \/ IMG_ID","948e7762":"bboxes = labels_df.loc[lambda df: df[\"image\"] == IMG_ID, [\"left\", \"top\", \"width\", \"height\"]].values","d42baa76":"\n\nimg = cv2.imread(IMG_PATH.as_posix())\n\nfor x, y, w, h in bboxes:\n    x1 = x\n    y1 = y\n    x2 = x + w\n    y2 = y + h\n    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n\n    \nheight, width, channels = img.shape\n\n\nwidth_to_height = int(width \/ height)\n\n\n\nfig, ax = plt.subplots(1, 1, figsize=(20 * width_to_height, 20))\nax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))","adc27849":"train_labels_df = pd.read_csv(TRAIN_LABELS_PATH)","9b74a4c4":"train_labels_df.sample(2).T","f1d91cfa":"tracking_df = pd.read_csv(TRAIN_TRACKING_PATH)","b8900c86":"tracking_df.sample(2).T","e55a8ce1":"TRAINING_VIDEOS_FOLDER = Path(\"..\/input\/nfl-impact-detection\/train\/\")\nVIDEO_PATHS = TRAINING_VIDEOS_FOLDER.glob(\"*.mp4\")","c614a7b1":"\n\ndef play(video_path):\n    cap = cv2.VideoCapture(video_path)\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        yield plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))","dea4c815":"player = play(next(VIDEO_PATHS).as_posix())","4a8bead1":"for _ in range(5):\n    next(player)","919198fb":"# there are also images. Explore these. How are they related ? ","11e03cc8":"#\u00a0Other labels?","58151f75":"That's neat!","350fa7ee":"# Image inputs","1963e042":"##\u00a0Image labels","f75da26b":"Well, we have a bunch of training videos. So let's explore some of these.\n","d043f943":"One question we will try to answer later is: is there an overlap between the different labels?","86030509":"In order to understand the competition metric, we first need to make a small detour by IoU for object detection.\n\n\nWhat is IoU?\n\n\nThis stands for intersection over union. \n\nWe will compute it for two bounding boxes: the true bounding box and the detected one.\n\nIf the detection is pecfect, both bounding boxes will overlap and the IoU score will be 1\nOn the other hand if the detection is very bad, there won't be any intersection and the score will be 0.\n\n","56a6cfe7":"Here are few things to keep in mind:\n    \n* 9 frames would be considered when scoring: 4 before, the current frame, and the 4 next ones.\n* One helmet can obscure another one so some predictions can be assigned to one or more ground truths.\n\n\n\nHere is an example to make things more clear.\n\nTODO: Take a video with bounding boxes, make some predictions, and compute the metric.","02fbf7c0":"For more details, check this IoU guide: https:\/\/www.pyimagesearch.com\/2016\/11\/07\/intersection-over-union-iou-for-object-detection\/\nAlso, check this script: https:\/\/www.kaggle.com\/nvnnghia\/evaluation-metrics","ebf3961c":"![nfl_impact_metric.png](attachment:nfl_impact_metric.png)","3a8eca17":"## Video inputs","a5becf8f":"#\u00a0Let's start with the labels","825cd3c7":"Let's analyze the competition metric. \n\n\nSeen from afar, it looks like a classic detection metric with the IoU. Let's have a closer look on the description page: ","2157b91d":"It seems there are labels for two types of files: \n    \n    \n* **videos** => these are in the `train_labels.csv`. \n* **images** => these are in the `image_labels.csv`. ","a3946efd":"These are localted in the `image_labels.csv` file.","2c80a5a1":"#\u00a0Detect than 3D localize.","af220f14":"# What about inputs? ","8fba75cf":"Here we have much more information compared to the image labels: \n    \n    \n* \n* \n* \n* ","d9b069dc":"So now, I image we have 3 objects to detect and we computed the IoU score for these three and we get these values: \n    \n\n- Object 1: IoU 0.6\n- Object 2: IoU 0.7\n- Object 3: IoU 0.8\n\n\nIf we set the threshold for true positive to 0.5 let's say, then:\n\n- Object 1: True positive\n- Object 2: True positive\n- Object 3: True positive\n\nThen, the F1 score at this threshold is thus: 1. \n\nIf we set the threshold this time to 0.6, then the F1 score is lower since object 1 is no longer a true positive but\nnow a False negative.","e3d508ef":"In addition to the usual sample_submission.csv file, there is one last type of labels: \n    \n- train_player_tracking.csv\n- test_player_tracking.csv\n\n\nWhat are these about?","9bc9de3c":"## Video labels","81ed317f":"These are localted in the `train_labels.csv` file.","721f883b":"There are of two kinds: videos and images. ","bc0b7d02":"## IoU and F1 for object detection","d963f4ee":"## Evaluation metric","ade3a26f":"So what are we looking at?\n\n\n* `image`: the id of an image, we will open one just afterwards.\n* `label`: how many are there?\n\nThe, the remaining `4` columns form a bounding box by providing a **top-left corner**, a **width** and a **height**. \n\nAgain, we will overlay a bounding box on top of an image.\n\nLet's do this."}}