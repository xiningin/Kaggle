{"cell_type":{"db7e6a64":"code","696e6ca2":"code","0e1e1efd":"code","d73154ef":"code","dc2d6bd4":"code","533b3f98":"code","912e336a":"code","02f6aaac":"code","bfce69d9":"code","b488ea94":"code","19322ba9":"code","a1380a59":"code","cb8e78b1":"code","aa8768b3":"code","77a86224":"code","63a9d11c":"code","49771373":"code","97ebe8ed":"code","70012ec8":"markdown","2fedff65":"markdown","ed9ab1b8":"markdown","a40ff5c3":"markdown","84c32600":"markdown"},"source":{"db7e6a64":"import numpy as np \nimport pandas as pd ","696e6ca2":"train = pd.read_csv('..\/input\/train.csv')","0e1e1efd":"train = train[:10000]","d73154ef":"import spacy\nimport re\nfrom nltk.corpus import stopwords\nnlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\nstops = stopwords.words(\"english\")\ndef normalize(comm, lowercase, remove_stopwords):\n    text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', comm)\n    comment = re.sub(r'(\\W)\\1+', r'\\1', text)\n    if lowercase:\n        comment = comment.lower()\n    comment = nlp(comment)\n    lemmatized = list()\n    for word in comment:\n        lemma = word.lemma_.strip()\n        if lemma:\n            if not remove_stopwords or (remove_stopwords and lemma not in stops):\n                lemmatized.append(lemma)\n    docs = nlp.pipe(lemmatized, batch_size=1000, n_threads=4)\n    return [' '.join([x.lemma_ for x in doc if x.is_alpha]) for doc in docs]","dc2d6bd4":"x_train_lemmatized = train.comment_text.apply(normalize, lowercase=True, remove_stopwords=True)","533b3f98":"train['comment_text'] = x_train_lemmatized\ntrain.head()","912e336a":"def reg(corpus):\n    corpus1 = []\n    for i in corpus:\n        corpus1.append(' '.join(i))\n    return corpus1\ntrain['comment_text'] = reg(train.comment_text)\ntrain.head()","02f6aaac":"toxics = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']","bfce69d9":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nx_train, x_test, y_train, y_test = train_test_split(train, train[toxics], test_size=0.3, shuffle=True)\nx_train.head()","b488ea94":"# \u0437\u0430\u0434\u0430\u0435\u043c \u0432\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0442\u043e\u0440 tf-idf\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_df=0.87,\n               smooth_idf=True, use_idf=True, max_features=300000)","19322ba9":"from sklearn.decomposition import TruncatedSVD\nfrom sklearn.pipeline import Pipeline","a1380a59":"# \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c SVD \u0434\u043b\u044f \u0441\u043d\u0438\u0436\u0435\u043d\u0438\u044f \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438  \n\nsvd_model = TruncatedSVD(n_components=3,\n                         algorithm='randomized',\n                         n_iter=10)\n\n# pipeline: tf-idf + SVD\nsvd_transformer = Pipeline([('tfidf', vect), \n                            ('svd', svd_model)])\n\nsvd_train = svd_transformer.fit_transform(x_train['comment_text'])\nsvd_test = svd_transformer.transform(x_test['comment_text'])\n\n# \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c svd \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0434\u043b\u044f \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0439 \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0432 \u043c\u043e\u0434\u0435\u043b\u044c ","cb8e78b1":"# \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430 \n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\n\nsubmiss_1 = x_test['id']\nlogreg = LogisticRegression(C=12, max_iter=10000, dual=True)\n\nsubmiss_1 = pd.DataFrame.from_dict({'id': x_test['id']})\nfor i in toxics:\n    logreg.fit(svd_train, y_train[i])\n    \n    submiss_1.loc[:,i] = logreg.predict_proba(svd_test)[:,1]\nsubmiss_1.head()","aa8768b3":"#\u0434\u0435\u043b\u0430\u0435\u043c \u0432\u044b\u0432\u043e\u0434, \u0447\u0442\u043e \u043d\u0430 \u043d\u0430\u0448\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u043c\u0435\u0442\u043e\u0434\u0438\u043a\u0430 tf-idf + tSVD \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043b\u0430\u0442\u0435\u043d\u0442\u043d\u043e-\u0441\u0435\u043c\u0430\u043d\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u043d\u0435 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e\u0439 \n#\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043e\u0431\u044b\u0447\u043d\u044b\u0439 tf-idf + LogReg \u0434\u0430\u0435\u0442 \u043b\u0443\u0447\u0448\u0435\u0435 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e\n\nscores = []\nfor i in toxics:\n    scores.append(roc_auc_score(y_test[i], submiss_1[i]))\nnp.mean(scores)","77a86224":"new = []\nfor i in train['comment_text']:\n    new.append(i.split())","63a9d11c":"from gensim import corpora\ndictionary = corpora.Dictionary(new)\ncorpus = [dictionary.doc2bow(text) for text in new]\nimport pickle\npickle.dump(corpus, open('corpus.pkl', 'wb'))\ndictionary.save('dictionary.gensim')","49771373":"# \u0437\u0430\u0434\u0430\u0435\u043c \u0447\u0438\u0441\u043b\u043e \u0442\u0435\u043c:  7 \n\nimport gensim\nNUM_TOPICS = 7\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\nldamodel.save('model7.gensim')\ntopics = ldamodel.print_topics(num_words=4)\nfor topic in topics:\n    print(topic)","97ebe8ed":"# \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c 7 \u0442\u0435\u043c \u043d\u0430 \u043d\u0430\u0448\u0435\u0439 \u043a\u043e\u043b\u043b\u0435\u043a\u0446\u0438\u0438 \u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0430\u0440\u0438\u0435\u0432 \n\ndictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\ncorpus = pickle.load(open('corpus.pkl', 'rb'))\nlda = gensim.models.ldamodel.LdaModel.load('model7.gensim')\nimport pyLDAvis.gensim\nlda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\npyLDAvis.display(lda_display)","70012ec8":"\u041f\u0435\u0440\u0435\u0434 \u043f\u0435\u0440\u0435\u0445\u043e\u0434\u043e\u043c \u043a \u0442\u0435\u0445\u043d\u0438\u043a\u0430\u043c \u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u0440\u043e\u0432\u0435\u0441\u0442\u0438 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e \u0442\u0435\u043a\u0441\u0442\u0430. \u041e\u043d\u0430 \u0432\u043a\u043b\u044e\u0447\u0430\u0435\u0442 \u0432 \u0441\u0435\u0431\u044f: \n\n1. \u0443\u0434\u0430\u043b\u0435\u043d\u0438\u0435 \u043b\u0438\u0448\u043d\u0438\u0445 \u0441\u0438\u043c\u0432\u043e\u043b\u043e\u0432  \n2. \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u0441\u043b\u043e\u0432 \u043a \u043d\u0438\u0436\u043d\u0435\u043c\u0443 \u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0443 \n3. \u0443\u0434\u0430\u043b\u0435\u043d\u0438\u0435 \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\n4. \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044f ","2fedff65":"\u041f\u0435\u0440\u0435\u0445\u043e\u0434\u0438\u043c \u043a \u0438\u043b\u043b\u044e\u0441\u0442\u0440\u0430\u0446\u0438\u0438 LSA - \u043b\u0430\u0442\u0435\u043d\u0442\u043d\u043e-\u0441\u0435\u043c\u0430\u043d\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0430\u043d\u0430\u043b\u0438\u0437\u0430. ","ed9ab1b8":"\u0422\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\n\n\u0414\u0430\u043d\u043d\u044b\u0439 kernel \u043f\u043e\u0441\u0432\u044f\u0449\u0435\u043d \u0432\u043e\u043f\u0440\u043e\u0441\u0443 \u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f - \u0437\u0430\u0434\u0430\u0447\u0435, \u0437\u0430\u043a\u043b\u044e\u0447\u0430\u044e\u0449\u0435\u0439\u0441\u044f \u0432 \u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u0435 \u0442\u0435\u043a\u0441\u0442\u043e\u0432 \u0432 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0438 \u0441 \u0438\u0445 \u0442\u0435\u043c\u043e\u0439. \u0422\u0435\u0445\u043d\u0438\u043a\u0438 LSA \u0438 LDA \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u043c\u0438 \u043f\u0440\u0438 \u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u043c \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0438, \u0438 \u0438\u043c\u0435\u043d\u043d\u043e \u043e\u043d\u0438 \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u044e\u0442\u0441\u044f \u0432 \u043c\u043e\u0435\u0439 \u0440\u0430\u0431\u043e\u0442\u0435. ","a40ff5c3":"LDA - \u041b\u0430\u0442\u0435\u043d\u0442\u043d\u043e\u0435 \u0440\u0430\u0437\u043c\u0435\u0449\u0435\u043d\u0438\u0435 \u0414\u0438\u0440\u0438\u0445\u043b\u0435. \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u043e\u0434\u043d\u043e\u0439 \u0438\u043b\u0438 \u0431\u043e\u043b\u0435\u0435 \u0442\u0435\u043c, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u044b. ","84c32600":"\u0414\u043b\u044f \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0430\u0446\u0438\u0438 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u043e\u0432 \u0440\u0430\u0431\u043e\u0442\u044b LSA\/LDA \u043d\u0435\u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043f\u043e\u043b\u043d\u044b\u0439 \u043d\u0430\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0434\u043b\u044f \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u0438 \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0435\u0433\u043e \u0447\u0430\u0441\u0442\u044c. "}}