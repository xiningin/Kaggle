{"cell_type":{"9897e16e":"code","13cbef88":"code","f8e0e878":"code","0b567aeb":"code","8d6d9bff":"code","330e3d01":"code","84b97e28":"code","95d2531d":"code","feaf12a4":"code","dc91a98a":"code","7ac0015e":"code","b443335c":"code","8cf937ef":"code","519ecd89":"markdown","c99105ee":"markdown","b23cb65e":"markdown","6b80a3f2":"markdown","2697f6c1":"markdown","3df91214":"markdown","c6aab2a1":"markdown","3c12906c":"markdown","b10ec354":"markdown"},"source":{"9897e16e":"import gc\n\nfrom tqdm import tqdm\n\nimport pandas as pd\nimport numpy as np\nimport janestreet as jane\nfrom sklearn.ensemble import RandomForestClassifier as RF\nfrom sklearn.metrics import accuracy_score, recall_score, roc_auc_score, precision_score\n\nimport matplotlib.pyplot as plt\n\nimport multiprocessing\nfrom joblib import delayed, Parallel, parallel_backend\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","13cbef88":"%%time\n\ntrain = pd.read_csv(\"..\/input\/jane-street-market-prediction\/train.csv\")\ntrain = train.loc[train[\"weight\"] != 0,]\ntrain[\"Y\"] = (train[\"weight\"]*train[\"resp\"] > 0).astype(\"int\")\n\ntrainCV = train.iloc[0:100000,:]\n\ntrain.ndim# dimensions of data frame\ntrain.size #rows\ntrain.shape#rows columns\ntrain.head(n=5)\n\ndate = train[\"date\"]\ndateCV = date[0:100000] ","f8e0e878":"impFeat = pd.read_csv(\"..\/input\/market-prediction-feature-importance-lofo-lgbm\/importanceResults.csv\")\nfeatVecGen = impFeat.iloc[0:60,0]","0b567aeb":"impFeat = impFeat.iloc[0:20,:]\n\nplt.figure(figsize=(10,7))\nplt.barh(impFeat.iloc[:,0], \n         impFeat.iloc[:,1], \n         xerr=impFeat.iloc[:,2], \n         align='center',\n         color= \"green\")\nplt.xlabel('Importance')\nplt.title('LOFO Feature Importance')\n\nplt.show()","8d6d9bff":"%%time\n\nX, XCV = train.loc[:,train.columns.isin(featVecGen)], trainCV.loc[:,train.columns.isin(featVecGen)]\nY, YCV = train.loc[:,train.columns.str.contains(\"Y\")], trainCV.loc[:,train.columns.str.contains(\"Y\")]","330e3d01":"del train, trainCV\ngc.collect()","84b97e28":"def models(k):\n    \"\"\"This function generates models in a list\"\"\"\n    \n    RFC = RF(\n            n_jobs = -1,\n            random_state = k,\n            n_estimators = 70,\n            max_depth = 12,\n            max_samples = 0.75,\n            min_samples_leaf = 3\n            )\n    \n    return RFC\n\nhelp(models)","95d2531d":"%%time\n\nD=0\n\nfolds = 10\ndenom = max(dateCV+1)\/folds\nshift = 4\n\nprint(\"total folds: \", folds - shift*2)\n\nAccRFC = []\nAucRFC = []\nRecRFC = []\nPreRFC = []\nRFCs = []\n\nk=0\n\nfor D, k in zip(list(range(0,folds-shift*2+1)), list(range(0,folds-shift*2+1))):\n    \n    #create time variant indices\n    startTr = ((dateCV)\/(denom)).astype(\"int\") >= D\n    endTr = ((dateCV)\/(denom)).astype(\"int\") < D+shift\n    train_index = startTr & endTr\n    \n    startTe = ((dateCV)\/(denom)).astype(\"int\") >= D+shift\n    endTe = ((dateCV)\/(denom)).astype(\"int\") < D+shift*2\n    test_index = startTe & endTe\n    print(\"D: \", D, \"D+shift: \", D+shift)\n    \n    #train test split\n    X_train, X_test = XCV.loc[train_index,:], XCV.loc[test_index,:]\n    y_train, y_test = YCV.loc[train_index,:], YCV.loc[test_index,:]\n\n    #learn\n    RFC = models(k)\n    RFC.fit(X_train.fillna(value=-999), y_train) \n    \n    #store fitted models\n    RFCs.append(RFC)\n    \n    #predict\n    R = RFCs[k]\n    PredRFC = R.predict(X_test.fillna(value=-999))\n\n    #store accuracy\n    AccRFC.append(accuracy_score(y_test, PredRFC))\n    \n    #store AUC\n    AucRFC.append(roc_auc_score(y_test, PredRFC))\n    \n    #Recall\n    RecRFC.append(recall_score(y_test, PredRFC))\n    \n    #Precision\n    PreRFC.append(precision_score(y_test, PredRFC))","feaf12a4":"print(\"Accuracy Random Forest is: \", AccRFC)\nprint(\"Auc Roc Random Forest is: \", AucRFC)\nprint(\"Recall Random Forest is: \", RecRFC)\nprint(\"Precision Random Forest is: \", PreRFC)","dc91a98a":"No2plt = 10\n\nRFImpMean = (RFCs[0].feature_importances_ + RFCs[1].feature_importances_ + RFCs[2].feature_importances_)\/3\n\nImportances = np.c_[list(X_train.columns), list(RFImpMean)]#np.c_ == cbind()\nind = list(np.lexsort((Importances[:,0],Importances[:,1])))\nImportances = Importances[list(reversed(ind))]\n\nFeatures = Importances[0:No2plt,0]\ny_pos = np.arange(No2plt)\nImportance = np.round(Importances[0:No2plt,1].astype(\"float32\"), 4)\n\nI = np.c_[list(RFCs[0].feature_importances_), \n          list(RFCs[1].feature_importances_), \n          list(RFCs[2].feature_importances_)]\n\n\nmaxI = np.amax(I, 1)\nminI = np.amin(I, 1)\nerror = maxI - minI \nerror = error[0:No2plt]\n\n\nplt.figure(figsize=(12,7))\nplt.barh(Features, \n         Importance, \n         xerr=error, \n         align='center',\n         color = \"green\")\nplt.xlabel('Importance')\nplt.title('RF Gini Feature Importance')\n\nplt.show()","7ac0015e":"del X_train, y_train, y_test, PredRFC, AccRFC, impFeat, RFC, RFCs, dateCV, date, X_test\ngc.collect() ","b443335c":"%%time\n\nRFC = models(10)\nRFC.fit(X.fillna(value=-999), Y) \ngc.collect()\n","8cf937ef":"%%time\n\ncores = multiprocessing.cpu_count()\n\nenv = jane.make_env() \niter_test = env.iter_test() \n\n        \nfor (test_df, sample_prediction_df) in tqdm(iter_test):\n    \n    if test_df[\"weight\"].item() == 0:\n        sample_prediction_df.action = 0\n    else:\n        X_test = test_df[featVecGen].fillna(value=-999)\n        sample_prediction_df.action = RFC.predict(X_test)\n    \n    env.predict(sample_prediction_df)","519ecd89":"Define models","c99105ee":"Read in important feature names","b23cb65e":"Creating targets and features","6b80a3f2":"Removing train dataset","2697f6c1":"Reading the data, creating the target","3df91214":"CV is separated by time. It predicts future values, stores the model in al list and assesses the quality by accuracy.\n\nRandom Forest can not interpolate trends. It needs stationary data.\nLogit is able to interpolate. Therefore, it is a good supplement.","c6aab2a1":"### Final learning","3c12906c":"Initializing environment, preparing data and submit.","b10ec354":"Define function for normalization and subsetting"}}