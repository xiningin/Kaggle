{"cell_type":{"4de518cb":"code","cae362f0":"code","fae786f6":"code","8e01822f":"code","ca78fa21":"code","a8894d40":"code","16e3034c":"code","82b1ba10":"code","e7864c37":"code","dbb9863b":"code","8ea7e791":"code","515837cc":"code","daf8e6d0":"code","8606a9c6":"code","b32dbd16":"code","f9026c30":"code","019b4bae":"code","cfe7485b":"code","e986b757":"code","ebd1e38d":"code","0d685cb1":"code","97c8288b":"code","652fa47e":"code","9c43384b":"markdown","a2536de2":"markdown","c3d10c8f":"markdown","565bb634":"markdown","ab2ea7c1":"markdown","d89165ad":"markdown","6b292c68":"markdown","e676b250":"markdown","23518cc6":"markdown","58b13c4e":"markdown","e99108fe":"markdown","1aa2ddf9":"markdown","d9799eb9":"markdown","b914163e":"markdown","d7d6bf52":"markdown"},"source":{"4de518cb":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter('ignore')\nimport plotly.express as px\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\nimport plotly.graph_objects as go\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.model_selection import train_test_split,KFold, GroupKFold, StratifiedKFold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.utils import class_weight\nfrom mlxtend.classifier import EnsembleVoteClassifier\nimport xgboost as xgb ","cae362f0":"# Initialize local variables\n\nSEED = 1992\nTARGET = ['target']\nSAMPLE = 2000 #For Visualization purposees","fae786f6":"# Import the data and prepare for the analysis. I have used sampling for speed up purposes. You can remove it :)\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')\n\n#Prepare train and test dataset\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)\n\n#Checking the null Data\nnull_data = (train.isna().sum().sort_values(ascending=False) \/ len(train) * 100) \nfig, ax = plt.subplots(1,1,figsize=(35, 7)) \nax.bar(null_data.index, 100, color='#dadada', width=0.6) \nbar = ax.bar(null_data.index,null_data, width=0.6) \nax.bar_label(bar, fmt='%.01f %%') \nax.spines.left.set_visible(False) \nax.set_yticks([]) \nax.set_title('Null Data Ratio', fontweight='bold') \nplt.show()\n\ntrain.describe().drop('count').T\\\n        .style.bar(subset=['mean'])\\\n        .background_gradient(subset=['std'])\\\n        .background_gradient(subset=['50%'])\\\n        .background_gradient(subset=['max'])","8e01822f":"# Combining all df to make the same preprocessing.\n\n#Encode the string data\ndef label_encoder(c):\n    le = LabelEncoder()\n    return le.fit_transform(c)\n\n\nall_df = pd.concat([train, test]).reset_index(drop=True)\nall_df[TARGET]=label_encoder(all_df[TARGET])\ntrain_last_id = train.shape[0]\n\nX = all_df.drop(TARGET,axis=1)\ny = all_df[TARGET]\nprint('Feature dataset format:\\t{}\\nTarget dataset format:\\t{}\\nTrain dataset size:\\t{}'.format(X.shape,y.shape,train_last_id))","ca78fa21":"# Check the Class distribution\npx.histogram(y, x=TARGET,title='Class Weight Histogram')","a8894d40":"# Calculate the classes weights\nclass_ratio = class_weight.compute_class_weight('balanced', y.target.unique(), y.target)\nprint(class_ratio)","16e3034c":"\nX_scaled = StandardScaler().fit_transform(X)\n\n# Presentation of scaled variables\nprint('Not Scaled Values:\\n{}\\n\\nScaled Values:\\n{}'.format(X[:1].values,X_scaled[:1]))\n","82b1ba10":"#Creation of covariation matrix and checkig if variables are internally covariated\n\ncorr = X.corr().abs()\n\ndata = go.Heatmap(\n        z=corr.values,\n        x=corr.index.values,\n        y=corr.columns.values,\n        zmin=0,\n        zmax=0.05)\n\nlayout = go.Layout(title='Covariance matrix - Internal noise detection',\n                   yaxis_autorange='reversed',\n                   xaxis_showgrid=False,\n                   yaxis_showgrid=False\n                  ) \n\nfig = go.Figure(data=data,layout=layout) \nfig.show()","e7864c37":"features = X_scaled.T\ncov_matrix = np.cov(features,bias=False)\nvalues, eig_vectors = np.linalg.eig(cov_matrix)\nvalues[:10]","dbb9863b":"explained_variances = []\nfor i in range(len(values)):\n    explained_variances.append(values[i] \/ np.sum(values))\n \nprint(np.sum(explained_variances), explained_variances)","8ea7e791":"projected_1 = X_scaled.dot(eig_vectors.T[0])\nprojected_2 = X_scaled.dot(eig_vectors.T[1])\nprojected_3 = X_scaled.dot(eig_vectors.T[2])\nres = pd.DataFrame(projected_1, columns=['PC1'])\nres['PC2'] = projected_2\nres['PC3'] = projected_3 # You can create as many vectors you want\nres['Y'] = y\n\n# In the output you will get the vectors as new fetures. You can use it for the training\nres.head()\n","515837cc":"#  Plot the sample vectors\nSC_DATA = res[:train_last_id].sample(SAMPLE)\nSC_DATA[\"Y\"] = SC_DATA[\"Y\"].astype(str)\n\nfig = px.scatter(SC_DATA, x='PC1', y='PC2',color='Y',title='PC1 vs PC2 vectors plot',opacity=0.9)\nfig.show()\nfig = px.scatter(SC_DATA, x='PC2', y='PC3',color='Y',title='PC2 vs PC3 vectors plot',opacity=0.9)\nfig.show()","daf8e6d0":" X_scaled.dot(eig_vectors)","8606a9c6":"fig = px.scatter_3d(res[:train_last_id].sample(SAMPLE), x='PC1', y='PC2',z='PC3',color='Y',size_max=10,opacity=0.6,title='PC1 PC2 PC3 vector 3d plot')\nfig.show()","b32dbd16":"#Feature Selection\nX_to_select = X[:train_last_id]\ny_to_select = y[:train_last_id]","f9026c30":"# Crate selector instance\nestimator = CatBoostClassifier(verbose=False)\nselector = SelectFromModel(estimator=estimator).fit(X_to_select, y_to_select)\n\n# Calculate stimator coeficient The treshold will be used to keep\/discard the variables for LogisticRegression\n#selector.estimator_.coef_[0]","019b4bae":"# Get selector result\nselector_results = np.unique(selector.get_support(),return_counts=True)\n\nprint('#'*50)\nprint(\"After Variable Selection for PCA:\\n\\tVariables keeped:\\t{}\\n\\tVariables removed:\\t{}\\n\\tImportance threshold:\\t{}\"\\\n      .format(selector_results[1][1],selector_results[1][0],selector.threshold_))\nprint('#'*50)\n\n# Apply the selection\nX_transformed = selector.transform(X_to_select)","cfe7485b":"# Combine everything together\ndef PCA(X , num_components):\n     \n    # Part 1 Scaling the data\n    scaledX = StandardScaler().fit_transform(X)\n     \n    #Creation of covariance matrix\n    cov_mat = np.cov(scaledX , rowvar = False)\n     \n    #Calculate Eigenvectors and Eigenvalues wrap function - Hermitian or symmetric matrix\n    '''\n    Hermitian matix (Dirac matrix):\n    1,0,0,0\n    0,1,0,0\n    0,0,-1.0\n    0,0,0,-1\n    \n    Symmetric matrix:\n    1,a,b\n    a,2,c\n    b,c,3\n    \n    np.linalg.eigh will solve both cases - no need to make exceptions\n    '''\n    eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)\n     \n    #Descending order sort\n    sorted_index = np.argsort(eigen_values)[::-1] # sorting the same shape\n    sorted_eigenvalue = eigen_values[sorted_index]\n    sorted_eigenvectors = eigen_vectors[:,sorted_index]\n     \n    #Subset from the matrxc\n    eigenvector_sub = sorted_eigenvectors[:,0:num_components]\n     \n    #Step-6\n    X_reduced = np.dot(eigenvector_sub.transpose() , scaledX.transpose() ).transpose()\n    \n    # Create Labels automaticaly\n    column_labels = []\n    for i in range(num_components):\n        column_labels.append('PC_{}'.format(i+1))\n    \n    \n    return X_reduced,column_labels\n\nmat_reduced,col_labels = PCA(X_transformed,4)\n#Creating DataFrame of reduced Dataset\nPCA_DF = pd.DataFrame(mat_reduced , columns = col_labels)\n \n#Concat with target\nPCA_DF = pd.concat([PCA_DF , y] , axis = 1)\n","e986b757":"lgbm_params = {\n    'n_estimators': 45000,\n    'objective' : 'multiclass',\n    'metric' : 'multi_logloss',\n    'random_state': SEED,\n    'learning_rate': 0.02,\n    'min_child_samples': 150,\n    'reg_alpha': 750,\n    'reg_lambda': 9e-2,\n    'num_leaves': 20,\n    'max_depth': 3,#16\n    'colsample_bytree': 0.15,\n    'subsample': 0.7,\n    'subsample_freq': 2,\n    'max_bin': 240,\n    'device':'gpu',\n    'class_weights':class_ratio\n}\n\ncols = list(PCA_DF.columns)\ncols.remove(\"target\")\n\nPCA_train = PCA_DF[:train_last_id]\nPCA_test = PCA_DF[train_last_id:]","ebd1e38d":"def KFoldTraining(classifier):\n    test_preds = None\n    train_rmse = 0\n    val_rmse = 0\n    n_splits = 5\n\n    classifier = classifier\n    \n\n    kf = KFold(n_splits = n_splits , shuffle = True , random_state = 42)\n    for fold, (tr_index , val_index) in enumerate(kf.split(PCA_train[cols].values , PCA_train['target'].values)):\n\n        print(\"-\" * 50)\n        print(f\"Fold {fold + 1}\")\n\n        x_train,x_val = PCA_DF[cols].values[tr_index] , PCA_train[cols].values[val_index]\n        y_train,y_val = PCA_DF['target'].values[tr_index] , PCA_train['target'].values[val_index]\n\n        eval_set = [(x_val, y_val)]\n\n        model = classifier\n        model.fit(x_train, y_train, eval_set = eval_set, verbose = 1000)\n\n        train_preds = model.predict(x_train)\n        train_rmse += mean_squared_error(y_train ,train_preds , squared = False)\n        print(\"Training RMSE : \" , mean_squared_error(y_train ,train_preds , squared = False))\n\n        val_preds = model.predict(x_val)\n        val_rmse += mean_squared_error(y_val , val_preds , squared = False)\n        print(\"Validation RMSE : \" , mean_squared_error(y_val , val_preds , squared = False))\n\n        if test_preds is None:\n            test_preds = model.predict_proba(PCA_test[cols].values)\n        else:\n            test_preds += model.predict_proba(PCA_test[cols].values)\n\n    print(\"-\" * 200)\n    print(\"Average Training RMSE : \" , train_rmse \/ n_splits)\n    print(\"Average Validation RMSE : \" , val_rmse \/ n_splits)\n\n    test_preds \/= n_splits\n    return test_preds","0d685cb1":"LGBM_classifier = LGBMClassifier(**lgbm_params)\n#XGB_classifier = xgb.XGBClassifier(eval_metric='mlogloss')#Can be optimized :)\n\n\nCB_pred = KFoldTraining(LGBM_classifier)\n#XGB_pred = KFoldTraining(XGB_classifier) I have to make the balanced training on XGB - Feature soon\ntest_preds = CB_pred.copy() # Next I will make model ensembing","97c8288b":"sample_submission['Class_1']=test_preds[:,0]\nsample_submission['Class_2']=test_preds[:,1]\nsample_submission['Class_3']=test_preds[:,2]\nsample_submission['Class_4']=test_preds[:,3]\nsample_submission.head()","652fa47e":"sample_submission.to_csv(\"PCA_sample2.csv\",index=False)","9c43384b":"Data Looks highly unbalanced thus the class balance method needs to be adjusted.","a2536de2":"<h1 style=\"background-color:#6B6B6B; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\">Covariance matrix by numpy.cov <\/h1>\n\n\n[Documentation](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.cov.html)<br>\nEstimate a covariance matrix, given data and weights.\n\nCovariance indicates the level to which two variables vary together. If we examine N-dimensional samples, X = [x_1, x_2, ... x_N]^T, then the covariance matrix element C_{ij} is the covariance of x_i and x_j. The element C_{ii} is the variance of x_i.","c3d10c8f":"<h1 style=\"background-color:#A50034; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 4. Prediction setup<\/h1>\nNow I will create the simple pipeline of using the catboost model to make the prediction basing on the PCA components","565bb634":"<h1 style=\"background-color:#A50034; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 2. Initial Setup-Data Preparaton <\/h1>\n","ab2ea7c1":"<h1 style=\"background-color:#A50034; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\"> Notebook development plan<\/h1>\n\n# If you like the notebook I will be happy. This motivate me to develop it :) <br>\n# Plans\n- More visualization\n- PCA ensembling\n- The ideas in comments<br>\n\nLets connect on [LinkedIn](https:\/\/www.linkedin.com\/in\/marcinstasko\/?locale=en_US) :)","d89165ad":"<h1 style=\"background-color:#6B6B6B; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\">Basics Terms<\/h1>\nTo undrstand the PCA we need some basic knowledge about the following terms<br>\n**Variance** = Spread of data around the Mean<br>\n<br>\n**Covariance** = Relation direction beween two random variables<br>\n![image.png](attachment:f3452d29-054f-4e12-bc64-e631d92de010.png)![image.png](attachment:5006a5fc-e71b-42f6-adea-ab9eedcdf0ec.png)\n<br>\n**Correlation** = Measure that shows us when one variable is changining how another variables\/s results are affected.\n![image.png](attachment:8de03666-5d40-46e9-bee7-86c95e39f90b.png)!","6b292c68":"As we can see the data is not internally correlated thus all variables can be used in features selecton. Internal correlation may leads to strong correlation and covariation signals making other relations underated","e676b250":"<h1 style=\"background-color:#A50034; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 3. PCA Pipeline - Step by step how it works<\/h1>\n\nTo begin the PCA analysis you have to ensure that the Eigendecomposition will be not affected by the scale of the values ","23518cc6":"<h1 style=\"background-color:#6B6B6B; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\">Let`s combine everything together <\/h1>\n\nAll these tutorial wrapped up into single functon I hope I have presented the PCA analysis in the easy way :)<br>\nWe will start from the feature selecton. For this purposes we will use the custom model as selector","58b13c4e":"<h1 style=\"background-color:#A50034; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 3b. PCA Pipeline - PCA Visualization<\/h1>\n","e99108fe":"<h1 style=\"background-color:#A50034; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 3a. PCA Pipeline - Features Selection<\/h1>\nWe will make Levene's significnce test to check equality of variances. Non-signifant p-value = variances are indeed euqal and there is no difference in variances of both groups.\nBasically we want to get rid of features where nothing interesting is happening (denoising of low variance features)\n","1aa2ddf9":"<h1 style=\"background-color:#A50034; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 1. Introduction <\/h1>\n\nI wanted to create this notebook because this is good chance to play with PCA. Instead of using prepared fuctions I wanted o make it from scratch explaining how it works. You can play with it and if you like upvote:) <br>\nMy personal goal is to make the notebooks prettier instead of .py code :) ","d9799eb9":"<h1 style=\"background-color:#6B6B6B; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\">Submission Creation <\/h1>\n","b914163e":"Ensemble the 2 classifiers and make final prediction by averaging the predictions made by 2 models","d7d6bf52":"<h1 style=\"background-color:#6B6B6B; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\">Model Ensembling<\/h1>"}}