{"cell_type":{"4c10f1b6":"code","77b6693d":"code","4b0a079f":"code","89df783e":"code","dcbef8bc":"code","adbc1b43":"code","837a9b8a":"code","26756cce":"code","f652fbb0":"code","18011cb0":"code","14f3ea3d":"code","2e2a35f5":"code","161b758f":"code","3c966b71":"code","f2038fc9":"code","ba3d4a73":"code","68cef60f":"code","f015fd8d":"markdown","fe5a3281":"markdown","a86447b9":"markdown","23410d5d":"markdown","d2d8cf9e":"markdown","d10664bd":"markdown","d2233337":"markdown","9a6fa998":"markdown","8fbdaf64":"markdown","9512331e":"markdown","306b1b00":"markdown","f4c2a117":"markdown","95c91797":"markdown","42db7bf0":"markdown","67c989b7":"markdown","5423e64c":"markdown","fc17cd2c":"markdown"},"source":{"4c10f1b6":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import RepeatedKFold\n\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain['train'] = True\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ntest['train'] = False\n\nalldata = train.append(test).copy()\n\n# Continuous features\nalldata['FamilyMembers'] = alldata['SibSp'] + alldata['Parch']\nalldata['AdjFare'] = alldata.groupby('Ticket')['Fare'].transform(lambda x: x\/len(x))\n# Categorical features\nalldata['Female'] = alldata['Sex'].map({'male': 0, 'female': 1}).astype(np.int8)\nalldata['Embarked'] = alldata['Embarked'].map({'S':2, 'C':1, 'Q': 0, np.NaN: 2}).astype(np.int8) # Missing values assigned to majority class (Southampton)\nalldata['Pclass_cat'] = (alldata['Pclass']-1).astype(np.int8)\nalldata['Male3rd'] = (alldata['Sex'].map({'male': 1, 'female': 0}) * alldata['Pclass'].map({3:1, 1:0, 2:0})).astype(np.int8)\nalldata['Adult'] = (alldata['Age']>16).astype(np.int8)\nalldata['Adult'].values[alldata['Age'].isna()] = 1 # Looking at titles of people, I suspect those with missing age are mostly adult\nalldata['MissingAge'] = (train['Age'].isna()*1).astype(np.int8)\nalldata['NonAdult1st2nd'] = (alldata['Adult'] * alldata['Pclass'].map({3:0, 1:1, 2:1})).astype(np.int8)\nalldata['Female1st2nd'] = (alldata['Female'] * alldata['Pclass'].map({3:0, 1:1, 2:1})).astype(np.int8)\n\n\n# Imputation of age\nalldata['Age'] = alldata.groupby(['Pclass', 'Female'])['Age'].transform(lambda x: x.fillna(x.median()))\n\n# Family member by age interaction\nalldata['FamilyAge'] = alldata['FamilyMembers'] + alldata['Age']\/60\n# Taking a guess as to which passengers are parents\nalldata['father'] = 1 * (alldata['Age']>=18) * (alldata['Parch']>0) * (alldata['Sex']=='male')\nalldata['mother'] = 1 * (alldata['Age']>=18) * (alldata['Parch']>0) * (alldata['Sex']=='female')\nalldata['parent'] = alldata['father'] + alldata['mother']\n\n\nalldata['title_type2'] = [ any([title in Name for title in ['Capt.', 'Col.', 'Major.', 'Rev.']]) for Name in alldata['Name']]\nalldata['title_type1'] = [ any([title in Name for title in ['Master.', 'Mme.', 'Dona.', 'Countess.', 'Lady.', 'Miss.', 'Mlle.']]) for Name in alldata['Name']]\n\nalldata['title_type'] = alldata['title_type1']*1 + alldata['title_type2']*2\n\nalldata['AgeGroup'] = 1 * (alldata['Age']<=2) + 1 * (alldata['Age']<=6) + 1 * (alldata['Age']<=17) + 1 * (alldata['Age']<=60)\n\n# Lists of features to be used later\ncontinuous_features = ['Pclass', 'Age', 'SibSp', 'Parch', 'FamilyMembers', 'AdjFare', 'FamilyAge']\ndiscrete_features = ['Female', 'Pclass_cat', 'Male3rd', 'Embarked', 'Adult', 'NonAdult1st2nd', \n                     'Female1st2nd', 'MissingAge', 'father', 'mother', 'parent', 'title_type', 'AgeGroup']\nids_of_categorical = [0,1,2,3,4,5,6,7,8,9,10,11,12]\n\ntrain = alldata[alldata['train']==True]\ntest = alldata[alldata['train']==False]\n","77b6693d":"pd.options.display.max_colwidth = 250\npd.options.display.max_columns = 50\ntrain.head(20)","4b0a079f":"import optuna.integration.lightgbm as lgb\nimport optuna\n\nrkf = RepeatedKFold(n_splits=10, n_repeats=10, random_state=42)\n\nparams = {\n        \"objective\": \"binary\",\n        \"metric\": \"binary_error\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",                \n        \"seed\": 42\n    }\n\nX = np.array( train[discrete_features + continuous_features] )    \ny = np.array( train['Survived'] ).flatten()\n\nstudy_tuner = optuna.create_study(direction='minimize')\ndtrain = lgb.Dataset(X, label=y)\n\n# Suppress information only outputs - otherwise optuna is \n# quite verbose, which can be nice, but takes up a lot of space\noptuna.logging.set_verbosity(optuna.logging.WARNING) \n\n# Run optuna LightGBMTunerCV tuning of LightGBM with cross-validation\ntuner = lgb.LightGBMTunerCV(params, \n                            dtrain, \n                            categorical_feature=ids_of_categorical,\n                            study=study_tuner,\n                            verbose_eval=False,                            \n                            early_stopping_rounds=250,\n                            time_budget=19800, # Time budget of 5 hours, we will not really need it\n                            seed = 42,\n                            folds=rkf,\n                            num_boost_round=10000,\n                            callbacks=[lgb.reset_parameter(learning_rate = [0.005]*200 + [0.001]*9800) ] #[0.1]*5 + [0.05]*15 + [0.01]*45 + \n                           )\n\ntuner.run()","89df783e":"print(tuner.best_params)\n# Classification error\nprint(tuner.best_score)\n# Or expressed as accuracy\nprint(1.0-tuner.best_score)","dcbef8bc":"# Set-up a temporary set of best parameters that we will use as a starting point below.\n# Note that optuna will complain about values on the edge of the search space, so we move \n# such values a tiny little bit inside the search space.\ntmp_best_params = tuner.best_params\nif tmp_best_params['feature_fraction']==1:\n    tmp_best_params['feature_fraction']=1.0-1e-9\nif tmp_best_params['feature_fraction']==0:\n    tmp_best_params['feature_fraction']=1e-9\nif tmp_best_params['bagging_fraction']==1:\n    tmp_best_params['bagging_fraction']=1.0-1e-9\nif tmp_best_params['bagging_fraction']==0:\n    tmp_best_params['bagging_fraction']=1e-9  ","adbc1b43":"import lightgbm as lgb\ndtrain = lgb.Dataset(X, label=y)\n\n# We will track how many training rounds we needed for our best score.\n# We will use that number of rounds later.\nbest_score = 999\ntraining_rounds = 10000\n\n# Declare how we evaluate how good a set of hyperparameters are, i.e.\n# declare an objective function.\ndef objective(trial):\n    # Specify a search space using distributions across plausible values of hyperparameters.\n    param = {\n        \"objective\": \"binary\",\n        \"metric\": \"binary_error\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",                \n        \"seed\": 42,\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 512),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.1, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.1, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 0, 15),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 100),\n        'seed': 1979\n    }\n    \n    # Run LightGBM for the hyperparameter values\n    lgbcv = lgb.cv(param,\n                   dtrain,\n                   categorical_feature=ids_of_categorical,\n                   folds=rkf,\n                   verbose_eval=False,                   \n                   early_stopping_rounds=250,                   \n                   num_boost_round=10000,                    \n                   callbacks=[lgb.reset_parameter(learning_rate = [0.005]*200 + [0.001]*9800) ]\n                  )\n    \n    cv_score = lgbcv['binary_error-mean'][-1] + lgbcv['binary_error-stdv'][-1]\n    if cv_score<best_score:\n        training_rounds = len( list(lgbcv.values())[0] )\n    \n    # Return metric of interest\n    return cv_score\n\n# Suppress information only outputs - otherwise optuna is \n# quite verbose, which can be nice, but takes up a lot of space\noptuna.logging.set_verbosity(optuna.logging.WARNING) \n\n# We search for another 4 hours (3600 s are an hours, so timeout=14400).\n# We could instead do e.g. n_trials=1000, to try 1000 hyperparameters chosen \n# by optuna or set neither timeout or n_trials so that we keep going until \n# the user interrupts (\"Cancel run\").\nstudy = optuna.create_study(direction='minimize')  \nstudy.enqueue_trial(tmp_best_params)\nstudy.optimize(objective, timeout=14400) \n","837a9b8a":"optuna.visualization.plot_optimization_history(study)","26756cce":"optuna.visualization.plot_slice(study)","f652fbb0":"optuna.visualization.plot_param_importances(study)","18011cb0":"print(study.best_params)","14f3ea3d":"# Classification error\nprint(study.best_value)\n# Or expressed as accuracy\nprint(1.0-study.best_value)","2e2a35f5":"best_params = {\n    \"objective\": \"binary\",\n    \"metric\": \"binary_error\",\n    \"verbosity\": -1,\n    \"boosting_type\": \"gbdt\",\n    \"seed\": 42} \nbest_params.update(study.best_params)\nbest_params\n","161b758f":"lgbfit = lgb.train(best_params,\n                   dtrain,\n                   categorical_feature=ids_of_categorical,\n                   verbose_eval=False,                   \n                   num_boost_round=training_rounds)","3c966b71":"X_test = np.array( test[discrete_features + continuous_features] )    ","f2038fc9":"test['Survived'] = np.round(lgbfit.predict(X_test)).astype(np.int8)\nsubmission = test[['PassengerId', 'Survived']]","ba3d4a73":"submission","68cef60f":"submission.to_csv('submission.csv', index=False)","f015fd8d":"**optuna** also provides some nice visualizations for your optimizaton study. I show a few ones here, but the **optuna** [documentation](https:\/\/optuna.readthedocs.io\/en\/v1.0.0\/reference\/visualization.html) has the full list with examples of what each plot looks like. In practice, we might optimize for a bit and then look at these plots to narrow down the search space - for example to bagging fractions between 0.75 and 0.95, lower feature fractions, lower values of the minimum number of child samples and number of leaves from 100 to 300. We can iterate that process to potentially arrive at increasingly better solutions that may outperform just running **optuna** longer.\n\nAs we can see, the gains from more optimizations plateau somewhat after a number of trials, which does not mean that we would not try to squeeze for the very last few decimals on Kaggle, but perhaps in a practical application there is not much point in going beyond, say, a few hundred or thousand trials with different hyperparameters.","fe5a3281":"# Train a tuned model that we will submit to the leaderboard","a86447b9":"# Visualizing the hyperparameter optimization","23410d5d":"Now let's search more broadly with the results that the automatic tuner provided as a starting point. Annoyingly optuna does not have an automatic way of continuing the study we started. So, we re-run the best parameters we found using the **enqueue_trial** function, which allows us to force the search to automatically look as our best guesses (in this case just one based on the tuner we used above). \n\nNote that how we could optimize the mean binary error, but here I choose to minimize the mean binary error + the standard deviation of the binary errors across CV folds. This aims to favor hyperparameter settings that produce consistently good results.\n\nHow will optuna now select values? We use the TPE (Tree-structured Parzen Estimator) algorithm (this is the default setting, which we could explictly select using **sampler=TPESampler** in the **study.optimize()** call). For more on TPE, you can read this [NeurIPS paper](https:\/\/papers.nips.cc\/paper\/4443-algorithms-for-hyper-parameter-optimization.pdf) or e.g. this more accessible [blog post](https:\/\/towardsdatascience.com\/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f). TPE at each step constructs an approximate surrogate model for how the performance of LightGBM depends on the hyperparameters based on what performance we have got for the values we tried, so far (note: technically, we re-write this differently using Bayes rule, but let's skip those details). It then tries new hyperparameter values to test based on this model, which are picked based on which values are expected to improve our metric of interest the most.","d2d8cf9e":"# What does this notebook try to do?\nI use the well-known dataset on whether we can predict what passengers survive the Titanic disaster to illustrate hyperparameter tuning. What I discuss:\n* basics of how [**LightGBM**](https:\/\/lightgbm.readthedocs.io\/en\/latest\/) and other tree-boosting approaches (like e.g. [xgboost](https:\/\/xgboost.readthedocs.io\/en\/latest\/)) work\n* hyperparameter tuning using the [**optuna** package](https:\/\/optuna.readthedocs.io\/en\/stable\/)\n\nWhy do I primarily use LightGBM? Mostly, due to its popularity on Kaggle, which it primarily owes to its speed, which allows more extensive hyperparameter tuning due to faster iteration. Obviously, this is less of a consideration on a small dataset such as the Titanic data, but on larger data this becomes a consideration.\n\nWhat I **do not** try to do\n* extensive feature engineering\/generating very insightful features, but rather hyperparameter tuning given a set of features\n* stacking or blending multiple models to improve what a single model can achieve\n* achieving a really good score on the leaderboard (see the two points above)\n* explaining model predictions using [**SHAP**](https:\/\/github.com\/slundberg\/shap) (SHapley Additive exPlanations)\n\nI avoid these things, because I primarily aim to achieve a clear code and explanations for hyperparameter tuning without complex code and lengthy feature engineering getting in the way.","d10664bd":"# Load the data\nFirst, we load the data and derive some basic features.\n\nNote, that we did not necessarily have to impute missing data e.g. for passenger age, because LightGBM can automatically handle missing values by assigning them a split-direction at each split (as xgboost does). However, it is plausible that the imputation I used should do better, because a missing value may not have the same implication for passenger class, age and gender.","d2233337":"Now we take the predicted probabilities from LightGBM and then round them (below 0.5 we predict 0, at or above 0.5 we predict 1):","9a6fa998":"Now let's actually train it.","8fbdaf64":"The achieved CV-score is","9512331e":"Here are our interim results after the automatic tuner.","306b1b00":"# Create test set predictions","f4c2a117":"Now create a dictionary with the best parameter values so that we can use those in training a final model below.","95c91797":"Now we save a *submission.csv* file, but without the index column - this is the expected format, as illustrated in the example submission file (*gender_submission.csv*).","42db7bf0":"# Results of hyperparameter optimization\n\nThis tuning strategy in the previous section gets us to these hyper-parameters for LightGBM:","67c989b7":"This is what the data then look like:","5423e64c":"So, our predictions now look like this:","fc17cd2c":"# Hyperparameter optimization with cross-validation\nPreviously, we used an absurdly simple model for illustration purposes, now, let's take this seriously and create a single LightGBM model of appropriate complexity. For that, we turn to hyperparameter optimization using the **optuna** package.\n\n**optuna** comes with a generic ability to tune hyperparameters for any machine learning algorithm, but specifically for LightGBM there is an intergration via the **LightGBMTunerCV** function. This function implements a sensible hyperparameter tuning strategy that is known to be sensible for LightGBM by tuning the following parameters in order:\n* feature_fraction\n* num_leaves\n* bagging_fraction and bagging_freq\n* feature_fraction (again)\n* regularization factors (i.e. 'lambda_l1' and 'lambda_l2')\n* min_child_samples\n\nWe can either entirely rely on this tuner, or additionally run some further hyperparameter search thereafter. Here we will do the latter. Arguably, you could even skip the automatic LightGBM tuner, if you have enough of a time budget, while it is particularly attractive if you have not so much time.\n\nHow do we tune? In each training, we actually optimize the log-loss, but we pick the hyperparameters that optimize the metric of interest. For the Titanic dataset on Kaggle that's accuracy, but you might argue that accuracy is not such a great metric to use, because it's not a proper scoring function, and only \"cares\" about correct (survival predicted and in truth survived, or non-survival predicted and in truth did not survive) or wrong (non-survival predicted and in truth survived, or survival predicted and in truth did not survive). That leads to a rather sparse signal. I.e. a predicted probability of 0.51 is just as good as 0.999, as long as it is above 0.5 - or whatever other threshold you choose to use to predict that a passenger survived - and as long as a passenger survives. In any case, we will maximize accuracy - or in fact, minimze binary error (=1-accuracy), which is completely equivalent.\n\nFor a full list of parameters we can tweak and objective functions\/metrics, see the [LightGBM documentation](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html). Note that you should typically use the main named for a metric, e.g. 'l1' and not an alias such as 'MAE', because optuna and LightGBM seen to interact on unfortunate ways - at least in optuna version 2.2.0 with LigthGBM version 2.3.1 (used in this notebook) - if you don't.\n\nSince this is a pretty small dataset, we can afford to operate with really low learning rates in LightGBM, which then require more trees, but tend to increase performance.\n\nWe will use 10-fold cross-validation with random splits, but depending on how the split of training and test data was done you might want to do something different. This particular approach may be particularly relevant if the training-test split was simply done randomly (like this cross-validation scheme)."}}