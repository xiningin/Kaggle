{"cell_type":{"743f0ba2":"code","498d9fe2":"code","2dfffe5f":"code","b82a5899":"code","cf5a0ccc":"code","eb5120fe":"code","2ad69bd3":"code","247a4a5b":"code","a958a207":"code","f177b8f2":"code","61e864b8":"code","f4d77adc":"code","2bfdb7c0":"code","794e8ae6":"code","588f780e":"code","e7d05ce9":"code","9c749374":"code","e23b0e1f":"code","88d2337d":"code","93ef1f26":"code","2d780aa2":"code","c6e1afe4":"code","b139c9d9":"code","4880fc1c":"code","fe8f5f11":"code","5b56ef02":"code","3297fbdb":"code","fd79d2b4":"markdown","4c7e16a6":"markdown","02f60e97":"markdown","8f619e47":"markdown","b110bccc":"markdown","800b9204":"markdown","61eac397":"markdown","6eb0f277":"markdown","55285758":"markdown","08e6689d":"markdown"},"source":{"743f0ba2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","498d9fe2":"%matplotlib inline\nimport matplotlib \nimport matplotlib.pyplot as plt","2dfffe5f":"dt_1 = pd.read_csv('\/kaggle\/input\/wine-quality-dataset\/train.csv')\nprint(\"Print first 5 subjects\")\nprint(dt_1.head())\nprint(\"\")\nprint(\"Basic descriptive statistics for all features\")\nprint(dt_1.describe())\nprint(\"\")\nprint(\"Feaure attributes\")\nprint(dt_1.info())","b82a5899":"dt_white = dt_1[dt_1['kind']=='white']\ndt_red = dt_1[dt_1['kind']=='red']","cf5a0ccc":"def Hist_p(dataset):\n    dataset.iloc[:, 1:].hist(bins=20, figsize=(20, 10))\n    plt.show()\n    \nprint('White wine histogram')    \nHist_p(dt_white)\nprint(' ')    \nprint('Red wine histogram')    \nHist_p(dt_red)","eb5120fe":"print('White wine correlation map') \ncorr_white = dt_white.corr()\ncorr_white.style.background_gradient(cmap='coolwarm')\n","2ad69bd3":"print('Red wine correlation map') \ncorr_red = dt_red.corr()\ncorr_red.style.background_gradient(cmap='coolwarm')","247a4a5b":"White_X = dt_white.iloc[:, 2:13]\nWhite_y = dt_white.iloc[:, 13]\n\nRed_X = dt_red.iloc[:, 2:13]\nRed_y = dt_red.iloc[:, 13]\n\n#print(White_X.head())\n#print(White_y.head())\n#print(Red_X.head())\n#print(Red_y.head())","a958a207":"from sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef Dtree(X, y, label):\n    clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n    scores = cross_val_score(clf, X, y, cv=5)\n    print(label)\n    print(\"Decision Tree Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n    #print(scores.mean())\n    #Default score for Decision Tree: Mean accuracy of self.predict(X) wrt. y. \n    #accuracy = # of correct \/ # of prediction\n\ndef Rfrst(X, y, label):    \n    clrf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n    scores = cross_val_score(clrf, X, y, cv=5)\n    print(label)\n    print(\"Random Forest Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n    #Default score for Random Forest: Mean accuracy of self.predict(X) wrt. y. \n","f177b8f2":"Dtree(White_X, White_y, \"White wine\")\nRfrst(White_X, White_y, \"White wine\")","61e864b8":"Dtree(Red_X, Red_y, \"Red wine\")\nRfrst(Red_X, Red_y, \"Red wine\")","f4d77adc":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 250, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","2bfdb7c0":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)# Fit the random search model\n\ndef RANDOM_T(X, y):\n    rf_random.fit(X, y)\n    ","794e8ae6":"RANDOM_T(White_X, White_y)\nrf_random.best_params_","588f780e":"clrf_grid = RandomForestClassifier(n_estimators=63, min_samples_split=5, min_samples_leaf= 2, max_features= 'auto', max_depth=70, random_state=0, bootstrap= False)\nscores = cross_val_score(clrf_grid, White_X, White_y, cv=5)\nprint(\"White wine Grid Random Forest Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n#Default score for Random Forest: Mean accuracy of self.predict(X) wrt. y. ","e7d05ce9":"RANDOM_T(Red_X, Red_y)","9c749374":"rf_random.best_params_","e23b0e1f":"clrf_grid = RandomForestClassifier(n_estimators=250, min_samples_split=5, min_samples_leaf= 1, max_features= 'sqrt', max_depth=100, random_state=0, bootstrap= True)\nscores = cross_val_score(clrf_grid, Red_X, Red_y, cv=5)\nprint(\"Red wine Grid Random Forest Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n#Default score for Random Forest: Mean accuracy of self.predict(X) wrt. y. ","88d2337d":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold","93ef1f26":"xgb = XGBClassifier()\n#kfold = KFold(n_splits=5, random_state=7)\ndef XGBs(X, y, label):\n    results = cross_val_score(xgb, X, y, cv=5)\n    print(label)\n    print(\"XGBoost Accuracy: %.2f (+\/- %.2f)\" % (results.mean(), results.std()* 2))","2d780aa2":"XGBs(White_X, White_y, \"White wine\")\nXGBs(Red_X, Red_y, \"Red wine\")","c6e1afe4":"dt_test = pd.read_csv('\/kaggle\/input\/wine-quality-dataset\/test.csv')\nprint(\"Print first 5 subjects\")\nprint(dt_test.head())","b139c9d9":"dtt_white = dt_test[dt_test['kind']=='white']\ndtt_red = dt_test[dt_test['kind']=='red']\n","4880fc1c":"dtt_white_X=dtt_white.iloc[:, 2:13]\n#dtt_white_X.head()\ndtt_red_X=dtt_red.iloc[:, 2:13]","fe8f5f11":"#for white\nclrf_gridw = RandomForestClassifier(n_estimators=63, min_samples_split=5, min_samples_leaf= 2, max_features= 'auto', max_depth=70, random_state=0, bootstrap= False)\nclrf_gridw.fit(White_X, White_y)\nresult_white = clrf_gridw.predict(dtt_white_X)\n\n#for red\nclrf_gridr = RandomForestClassifier(n_estimators=250, min_samples_split=5, min_samples_leaf= 1, max_features= 'sqrt', max_depth=100, random_state=0, bootstrap= True)\nclrf_gridr.fit(Red_X, Red_y)\nresult_red = clrf_gridw.predict(dtt_red_X)","5b56ef02":"White_submitt=pd.DataFrame({\"Id\": dtt_white['Id'], \"quality\": result_white})\nRed_submitt=pd.DataFrame({\"Id\": dtt_red['Id'], \"quality\": result_red})","3297fbdb":"SJ_submit = pd.concat([White_submitt, Red_submitt]).sort_index()\nSJ_submit\npd.DataFrame(SJ_submit).to_csv(\"submit_SJ.csv\", index=False)","fd79d2b4":"# 1. Quick look of the data","4c7e16a6":"# 3.2 Conduct Random hyperparameter grid for Random Forest","02f60e97":"# 3. Construct Model\n# 3.1 Decision Tree and Random Forest","8f619e47":"## I think it's reasonable to separate White and Red wine","b110bccc":"# Data science explorer - project 004 Wine Quality","800b9204":"# 4. XGBoost","61eac397":"# 2. Create the datasets for modeling","6eb0f277":"#### There is no variable with missing value. No need to do the imputation.\n#### Explanation of all the variables\n\n1. Id\n2. kind: White or Red wine\n3. fixed acidity: The predominant fixed acids found in wines are tartaric, malic, citric, and succinic.\n4. volatile acidity\uff08\u63ee\u767c\u9178\uff09: Volatile acidity refers to the steam distillable acids present in wine, primarily acetic acid but also lactic, formic, butyric, and propionic acids.\n5. citric acid\uff08\u6ab8\u6aac\u9178\uff09: Citric acid is a weak organic acid that has the chemical formula C6H8O7. It occurs naturally in citrus fruits.\n6. residual sugar: Residual Sugar (or RS) is from natural grape sugars leftover in a wine after the alcoholic fermentation finishes. It\u2019s measured in grams per liter. \n7. chlorides\uff08\u6c2f\u5316\u7269\uff09:\n8. free sulfur dioxide: Free sulfur dioxide is a measure of the amount of SO2 that is not bound to other molecules, and is used to calculate molecular SO2.\n9. total sulfur dioxide: Total sulfur dioxide (SO2) is a measure of both the free and bound forms of SO2\n10. density\n11. pH\n12. sulphates\uff08\u786b\u9178\u9e7d\uff09:\n13. alcohol \n14. quality","55285758":"## It seems like except for alcohol, all the variables are not highly correlated with quality","08e6689d":"# 5. Apply Grid Random Forest on Test data"}}