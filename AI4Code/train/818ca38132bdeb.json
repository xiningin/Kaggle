{"cell_type":{"1afe4d5e":"code","5e274170":"code","2a481510":"code","ad4b1126":"code","6e632729":"code","20d622ba":"code","78ed71a6":"code","31377a1b":"code","613dac87":"code","7a6bb924":"code","59a75e73":"code","62575c79":"code","90e9570b":"code","6579a403":"code","cd5c4233":"code","1041e20b":"code","7f71536f":"code","ca80b279":"code","5da4fdb7":"code","c0580439":"code","f100c1dc":"code","6188ce1a":"code","7388d6b9":"code","166fde52":"code","1cccf59f":"code","f077b5e2":"code","a9b4a7c1":"code","bb5ab42f":"code","6baf0c56":"code","f939091b":"code","ec006048":"code","27c24b2f":"code","72550ed5":"code","bb2f6463":"code","7d85c28e":"code","9d480290":"code","d4882c19":"code","948a91dc":"code","acf78b79":"code","16242d17":"code","3e42a36e":"code","d7ec2100":"code","dba428f7":"code","02716c51":"code","3b9bc22c":"code","99beea8c":"markdown","1f56c083":"markdown","cd58f852":"markdown","0ef6d2d1":"markdown","f01a80b5":"markdown","51a63810":"markdown","94951032":"markdown","ba1f4650":"markdown","c6733e81":"markdown","2bbb723b":"markdown","300cfe6e":"markdown","b263dcdd":"markdown","18b38e23":"markdown","9381af3e":"markdown","a9952b1f":"markdown","95d540c0":"markdown","8dfc730b":"markdown","3e509e4d":"markdown","7184059d":"markdown","db05bdaa":"markdown","d3245c95":"markdown","01753d4a":"markdown","928ec3f8":"markdown","bd469e4b":"markdown","f5d63de0":"markdown","2a6f0b55":"markdown","ce990c8f":"markdown","d7c8ec3c":"markdown","aa63cf28":"markdown","e5586b24":"markdown","6c0a1e20":"markdown","447248c7":"markdown"},"source":{"1afe4d5e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5e274170":"### Libraries\n\n# Base\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom plotly.offline import iplot\n\n# Missing Values\n\nimport missingno as msno\n\n\n# Warnings\n\npd.set_option(\"display.float_format\",lambda x: \"%.5f\" % x)\npd.set_option(\"display.max_rows\",None)\npd.set_option(\"display.max_columns\",None)\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\n\n\n# Modeling\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport xgboost\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n# Metrics & Evaluation\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import *","2a481510":"# Read the Data\ndata = pd.read_csv(\"\/kaggle\/input\/hitters-baseball-data\/Hitters.csv\")\ndf = data.copy()\ndf.head()","ad4b1126":"def check_df(dataframe):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(3))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(3))\n","6e632729":"check_df(df)","20d622ba":"print(\"Num of Object Variables:\", df.select_dtypes(object).shape[1])\nprint(\"Num of Integer Variables:\", df.select_dtypes(\"integer\").shape[1])\nprint(\"Num of Float Variables:\", df.select_dtypes(\"float\").shape[1])\n","78ed71a6":"missings = df.isna().sum()\nmissing_df = (pd.concat([missings.rename('Missing Values'),\n                     missings.div(len(df)).rename('Missing ratio')],axis = 1)\n                                           .loc[missings.ne(0)])\nmissing_df","31377a1b":"df[df.Salary.isnull()==True].head()","613dac87":"msno.bar(df);","7a6bb924":"# Descriptive Analysis\ndf.describe([0.05,0.25,0.50,0.75,0.95,0.99]).T","59a75e73":"sns.boxplot(x = df[\"Salary\"])\nplt.show()","62575c79":"#To determine the threshold value for outliers\n\ndef outlier_thresholds(dataframe, col_name):\n    quartile1 = dataframe[col_name].quantile(0.05)\n    quartile3 = dataframe[col_name].quantile(0.95)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit","90e9570b":"# Threshold values are determined for the target variable. \n    \nlow_limit,up_limit = outlier_thresholds(df, \"Salary\")\nprint(\"Low Limit : {0}  Up Limit : {1}\".format(low_limit,up_limit))\n","6579a403":"def replace_with_thresholds(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if low_limit > 0:\n        dataframe.loc[(dataframe[col_name] < low_limit), col_name] = low_limit\n        dataframe.loc[(dataframe[col_name] > up_limit), col_name] = up_limit\n    else:\n        dataframe.loc[(dataframe[col_name] > up_limit), col_name] = up_limit\n","cd5c4233":"# numerical variables\nnum_cols = [col for col in df.columns if df[col].dtypes != \"O\"]\nprint('\\n',num_cols)","1041e20b":"#Outlier analysis for numerical variables\nfor col in num_cols:\n    replace_with_thresholds(df, col)\n","7f71536f":"# Outliers in the dependent variable have been eliminated\nsns.boxplot(x = df[\"Salary\"])\nplt.show()\n","ca80b279":"def num_plot(df, cat_length = 10, remove = [\"Id\"], hist_bins = 10, figsize = (17,4)):\n    \n    num_col = [col for col in df.columns if df[col].dtypes != \"O\" \n                and len(df[col].unique()) >= cat_length]\n    \n    if len(remove) > 0:\n        num_col = [x for x in num_col if (x not in remove)]\n           \n    for i in num_col:\n        fig, axes = plt.subplots(1, 3, figsize = figsize)\n        df.hist(str(i), bins = hist_bins, ax=axes[0])\n        df.boxplot(str(i),  ax=axes[1], vert=False);\n        try: \n            sns.kdeplot(np.array(df[str(i)]))\n        except: ValueError\n        \n        axes[1].set_yticklabels([])\n        axes[1].set_yticks([])\n        axes[0].set_title(i + \" | Histogram\")\n        axes[1].set_title(i + \" | Boxplot\")\n        axes[2].set_title(i + \" | Density\")\n        plt.show()\n        \n        \nnum_plot(df, cat_length = 16, remove = [\"Id\"], hist_bins = 10, figsize = (20,4))","5da4fdb7":"# correlation analysis\ndf.corr(method=\"spearman\")\n","c0580439":"# correlation analysis before feature engineering\n\nplt.figure(figsize=(20,10))\nsns.heatmap(df.corr(),annot=True ,cmap=\"twilight_shifted_r\");","f100c1dc":"def cart_feature_gen(model_type, dataframe, X, y, suffix = None):\n    # Model Type\n    if model_type == \"reg\":\n        from sklearn.tree import DecisionTreeRegressor\n        model = DecisionTreeRegressor()\n    elif model_type == \"class\":\n        from sklearn.tree import DecisionTreeClassifier\n        model = DecisionTreeClassifier()\n    else:\n        print(\"Give a model type! model_type argument should be equal to 'reg' or 'class'\")\n        return None\n    \n    # Remove NaN\n    temp = dataframe[[X,y]].dropna()\n    \n    # Fit a tree\n    rules = model.fit(temp[[X]], temp[y])\n  \n    # First Decision Rule\n    print(X)\n    print(\"Threshold:\", rules.tree_.threshold[0])\n    print(\"Range:\", \"[\"+str(dataframe[X].min())+\" - \"+str(dataframe[X].max()) +\"]\", \"\\n\")\n    if suffix == None:\n        new_colname = \"DTREE_\"+X.upper()\n    else:\n        new_colname = \"DTREE_\"+suffix+\"_\"+X.upper()\n    dataframe[new_colname] = np.where(dataframe[X] <= rules.tree_.threshold[0], 1, 0)   \n    ","6188ce1a":"# I added the variables with the highest values as a result of the correlation analysis to a list.\n\ntime_cols = [\"AtBat\", \"CAtBat\", \"Hits\", \"CHits\", \"HmRun\", \"CHmRun\", \"Runs\", \"CRuns\",\"RBI\",\"CRBI\",\"Walks\",\"CWalks\"]\n\nfor i in time_cols:\n  cart_feature_gen(model_type=\"reg\", dataframe=df, X=i, y=\"Salary\", suffix = \"TIME\")\n","7388d6b9":"# Correlation analysis of numerical variables was performed.\n\ndef find_correlation(dataframe, numeric_cols, corr_limit=0.60):\n    high_correlations = []\n    low_correlations = []\n    for col in numeric_cols:\n        if col == \"Salary\":\n            pass\n        else:\n            correlation = dataframe[[col, \"Salary\"]].corr().loc[col, \"Salary\"]\n            print(col, correlation)\n            if abs(correlation) > corr_limit:\n                high_correlations.append(col + \": \" + str(correlation))\n            else:\n                low_correlations.append(col + \": \" + str(correlation))\n    return low_correlations, high_correlations\n\n\nlow_corrs, high_corrs = find_correlation(df, num_cols)\n","166fde52":"sns.scatterplot(x= df['CRuns'], y=df.Salary);","1cccf59f":"sns.scatterplot(x= df['CRBI'], y=df.Salary);","f077b5e2":"#New variables were created with the most appropriate variables according to their proportions.\n\ndf['AtBat_new'] = df['AtBat'] \/ df['CAtBat']\ndf['Hits_new'] = df['Hits'] \/ df['CHits']\ndf['HmRun_new'] = df['HmRun'] \/ df['CHmRun']\ndf['Runs_new'] = df['Runs'] \/ df['CRuns']\ndf['RBI_new'] = df['RBI'] \/ df['CRBI']\ndf['Walks_new'] = df['Walks'] \/ df['CWalks']\n\ndf['New_Year'] = pd.cut(x=df['Years'], bins=[0, 7, 14, 40], \n                        labels=[\"New\", \"Experienced\", \"Highly Experienced\"]).astype(\"O\")\n\ndf['New_Assists'] = pd.qcut(x=df['Assists'], q=3, \n                            labels=[\"High\", \"Medium\", \"Low\"]).astype(\"O\")\n","a9b4a7c1":"# There is no proportionality between mistakes and salary\n\nsns.distplot (df[\"Salary\"])\nplt.xlabel(\"Salary\")\nplt.ylabel(\"Errors\")\nplt.title(\"Errors vs Salary\", size=(15), color=\"blue\")\nplt.show ();","bb5ab42f":"check_df(df)","6baf0c56":"# Variables with 2 categories\nbin_cols=[col for col in df.columns if len(df[col].unique())==2 and df[col].dtypes==\"O\"]\nbin_cols","f939091b":"# connection between variables\n\ngrid = sns.FacetGrid(df, col='League', row='Division', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'CHits', alpha=.5, bins=20)\ngrid.add_legend();","ec006048":"#relation of variables to target variables\nfor i in [\"League\", \"Division\", \"NewLeague\"]:\n    plt.figure(figsize = (10,4))\n    p = sns.boxplot(x = i, y = \"Salary\", data = df)\n    for item in p.get_xticklabels():\n        item.set_rotation(90)","27c24b2f":"for col in bin_cols:\n     label_encoder = LabelEncoder()\n     label_encoder.fit(list(df[col].values))\n     df[col] = label_encoder.transform(list(df[col].values))","72550ed5":"df.head()","bb2f6463":"check_df(df)","7d85c28e":"onehot_cols = [col for col in df.columns if 10 >= len(df[col].unique()) > 2]\n\ndf = pd.get_dummies(df, columns=onehot_cols, drop_first=True)\n\ndf.head()\n","9d480290":"# correlation with the final state of the variables\nplt.figure(figsize=(45,45))\ncorr=df.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(df.corr(), mask=mask, cmap='coolwarm', vmax=.3, center=0,\n            square=True, linewidths=.5,annot=True)\nplt.show()","d4882c19":"def minmax_scaler(dataframe, col_names, feature_range=(0,1)):\n    minmax_scaler = preprocessing.MinMaxScaler(feature_range=feature_range)\n    col_names=[col for col in df.columns if col not in df[\"Salary\"]]\n    dataframe[col_names] = minmax_scaler.fit_transform(dataframe[col_names])\n    return dataframe\n\nminmax_scaler(df, num_cols, feature_range=(0,1))\n","948a91dc":"#Salary is an target variable, it should not be filled in.\n\nmsno.bar(df);","acf78b79":"df.dropna(inplace=True)","16242d17":"def check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\ncheck_outlier(df, \"Salary\")","3e42a36e":"from sklearn.model_selection import train_test_split\n\ny=df[\"Salary\"]\nX=df.drop(\"Salary\", axis=1)\n\nX_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.20,random_state=42)\n\n# Test Error\nlgb_model = LGBMRegressor().fit(X_train, y_train)\ny_pred = lgb_model.predict(X_test)\n\n# Train Error\nlgb_model = LGBMRegressor().fit(X_train, y_train)\ny_pred1 = lgb_model.predict(X_train)\n\n# Parameters\nlgb_model = LGBMRegressor()\n\nlgb_params = {\"learning_rate\": [0.01, 0.1],\n               \"n_estimators\": [500, 1000],\n               \"max_depth\": [3, 5, 8],\n               \"colsample_bytree\": [1, 0.8, 0.5]}\n\n# CV\nlgb_cv_model = GridSearchCV(lgb_model,\n                             lgb_params,\n                             cv=5,\n                             n_jobs=-1,\n                             verbose=2).fit(X_train, y_train)\n\nlgb_cv_model.best_params_\n\n# LGB TUNED\nlgb_tuned = LGBMRegressor(**lgb_cv_model.best_params_).fit(X_train, y_train)\ny_pred = lgb_tuned.predict(X_test)\n\n\nprint(\"\")\nprint(\"Test RMSE:\", \"{:,.2f}\".format(np.sqrt(mean_squared_error(y_test, y_pred))))\nprint(\"Train RMSE:\", \"{:,.2f}\".format(np.sqrt(mean_squared_error(y_train, y_pred1))), \"\\n\")\nprint(\"Train MAE:\", \"{:,.2f}\".format(mean_absolute_error(y_train, y_pred1)))\nprint(\"Test MAE:\", \"{:,.2f}\".format(mean_absolute_error(y_test, y_pred)), \"\\n\")\nprint(\"Train R^2:\", \"{:,.2f}\".format(r2_score(y_train, y_pred1)))\nprint(\"Test R^2:\", \"{:,.2f}\".format(r2_score(y_test, y_pred)))\n","d7ec2100":"# Faeture Importance\ndef plot_importance(model, features, num=len(X), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')\n\nplot_importance(lgb_tuned, X_test)\n","dba428f7":"rf_model = RandomForestRegressor(random_state=42).fit(X_train, y_train)\n\n# Train Error\ny_pred2 = rf_model.predict(X_train)\nnp.sqrt(mean_squared_error(y_train, y_pred2))\n\n# Test Error\ny_pred3= rf_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred3))\n\nrf_params = {\"max_depth\": [5, 8, None],\n             \"max_features\": [3, 5, 15],\n             \"n_estimators\": [200, 500],\n             \"min_samples_split\": [2, 5, 8]}\nrf_cv_model = GridSearchCV(rf_model, rf_params, cv=5, n_jobs=-1, verbose=1).fit(X_train, y_train)\nrf_cv_model.best_params_\n\n# RF TUNED\nrf_tuned = RandomForestRegressor(**rf_cv_model.best_params_).fit(X_train, y_train)\ny_pred3 = rf_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred3))\n","02716c51":"# Feature Importance\n\ndef plot_importance(model, features, num=len(X), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')\n\nplot_importance(rf_tuned, X_test)\n\n","3b9bc22c":"def train_model(m,name):\n    model = m\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    mse=np.sqrt(mean_squared_error(y_test, y_pred)) \n    print(\" mse {0} mse {1} \".format(name,mse))\n\ntrain_model(DecisionTreeRegressor(),\"Decision Tree Regressor\")\ntrain_model(GradientBoostingRegressor(), \"Gradient Boosting Regressor\")\ntrain_model(XGBRegressor(n_estimators=600),\"XGBoost Regressor\")\n","99beea8c":"Making large changes to the outlier can corrupt the data. So let's give the interval balanced.","1f56c083":"    \n<p><img style=\"float: top;margin: max-width:1000px\" src=\"    \nhttps:\/\/i.gifer.com\/552Q.gif\"><\/p>","cd58f852":"* One-Hot Encoding is the process of creating dummy variables. Every unique value in the category will be added as a feature.\n\n* Dummy Variable Trap is a scenario in which variables are highly correlated to each other. Multicollinearity occurs where there is a dependency between the independent features. So, in order to overcome the problem of multicollinearity, one of the dummy variables has to be dropped.\n\n* Apply One-Hot Encoding when the categorical feature is **not ordinal**.","0ef6d2d1":"<a id='ldata'><\/a>\n <a id = \"15\"><\/a><br>\n\n<div>  \n<h3><center style=\"background-color:#C39BD3; color:white;\"><strong>LIGHT GBM \ud83d\udcda<\/strong><\/center><\/h3>\n<\/div>","f01a80b5":"\n<a id = \"6\"><\/a><br>\n\n<div>  \n<h3><center style=\"background-color:#C39BD3; color:white;\"><strong>Importing Libraries \ud83d\udcda<\/strong><\/center><\/h3>\n<\/div>","51a63810":"<font color = '#F1C40F'>\n\n <a id = \"11\"><\/a><br>\n## New Variables\n    ","94951032":"* A machine can only understand the numbers. Categorical encoding is a process of converting categories to numbers.\n\n* Usually preferred for 2-category variables. Because it can break scaling in more than two variables.(Nominal-Ordinal)\n\n* This method is not only for category variables.Numeric variables can also be transformed into categorical variables.\n\n* Assigns 1 to the first value it sees.\n\n* Apply Label Encoding when the categorical feature is **ordinal**.\n","ba1f4650":"Two variables with high correlation.","c6733e81":"<div>  \n<h3><center style=\"background-color:#C39BD3; color:white;\"><strong>Content <\/strong><\/center><\/h3>\n<\/div>","2bbb723b":"# <a id='ldata'><\/a>\n <a id = \"17\"><\/a><br>\n\n<div>  \n<h3><center style=\"background-color:#C39BD3; color:white;\"><strong>OTHERS \ud83d\udcda<\/strong><\/center><\/h3>\n<\/div>","300cfe6e":"<p><img style=\"float: top;margin: max-width:500px\" src=\"https:\/\/media.giphy.com\/media\/LrRtjpasjgtOTjO2Pn\/source.gif\"><\/p>\n","b263dcdd":"<font color = '#F1C40F'>\n\n<a id = \"8\"><\/a><br>\n## Data Understanding","18b38e23":"<img style=\"float: margin:1000px 50px 50px 1px; max-width:500px\" src=\"https:\/\/gousfbulls.com\/images\/2011\/3\/21\/BKNFLKGKQSCXEWR.20110321184030.jpg?width=1024&height=576&mode=crop\">\n\n\n                1986- Oracle New Orleans","9381af3e":"### <a id = \"1\"><\/a><br>\n### Description\nMajor League Baseball Data from the 1986 and 1987 seasons.\n\n <a id = \"2\"><\/a><br>\n### Format\nA data frame with 322 observations of major league players on the following 20 variables.\n\nAtBat: Number of times at bat in 1986\n\nHits: Number of hits in 1986\n\nHmRun: Number of home runs in 1986\n\nRuns: Number of runs in 1986\n\nRBI: Number of runs batted in in 1986\n\nWalks: Number of walks in 1986\n\nYears: Number of years in the major leagues\n\nCAtBat: Number of times at bat during his career\n\nCHits: Number of hits during his career\n\nCHmRun: Number of home runs during his career\n\nCRuns: Number of runs during his career\n\nCRBI: Number of runs batted in during his career\n\nCWalks: Number of walks during his career\n\nLeague: A factor with levels A and N indicating player's league at the end of 1986\n\nDivision: A factor with levels E and W indicating player's division at the end of 1986\n\nPutOuts: Number of put outs in 1986\n\nAssists: Number of assists in 1986\n\nErrors: Number of errors in 1986\n\nSalary: 1987 annual salary on opening day in thousands of dollars\n\nNewLeague: A factor with levels A and N indicating player's league at the beginning of 1987\n\n <a id = \"3\"><\/a><br>\n### Source\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.\n <a id = \"4\"><\/a><br>\n### References\nGames, G., Witten, D., Hastie, T., and Tibshirani, R. (2013) An Introduction to Statistical Learning with applications in R, www.StatLearning.com, Springer-Verlag, New York\n <a id = \"5\"><\/a><br>\n### Examples\nsummary(Hitters)\n","a9952b1f":"\n<p><img style=\"float: top;margin: max-width:1000px\" src=\"https:\/\/metshotcorner.com\/wp-content\/uploads\/2016\/05\/2016.05.25.jpg\"><\/p>","95d540c0":"<a id='ldata'><\/a>\n <a id = \"16\"><\/a><br>\n\n<div>  \n<h3><center style=\"background-color:#C39BD3; color:white;\"><strong>RANDOM FOREST \ud83d\udcda<\/strong><\/center><\/h3>\n<\/div>","8dfc730b":"<font color = '#C39BD3'>\n    \n\n\n1. [DESCRIPTION](#1)\n* [Format](#2)\n* [Source](#3)\n* [References](#4)\n* [Examples](#5)\n2. [Importing Libraries](#6)\n3. [FEATURE ENGINEERING](#7)\n* [Data Understanding](#8)\n* [Data Preprocessing](#9)\n* [Outliers](#10)\n* [New Variables](#11)\n* [Label Encoding](#12)\n* [One-Hot Encoding](#13)\n* [Min-Max Scaler](#14)\n4. [LightGBM](#15)\n5. [Random Forest](#16)\n6. [Others](#17)\n","3e509e4d":"<img style=\"float: margin:1000px 50px 50px 1px; max-width:500px\" src=\"http:\/\/mulvanesports.com\/pictures\/articles\/5424_534_article_4-13-2020.jpg\">\n\n\n\n               1986 State Baseball Champions, A Look Back","7184059d":"* Random forest is a Supervised Learning algorithm which uses ensemble learning method for classification and regression.\n\n* Random forest is a bagging technique and not a boosting technique. The trees in random forests are run in parallel. There is no interaction between these trees while building the trees.\n\n* A random forest is a meta-estimator which aggregates many decision trees, with some helpful modifications.\n\n**NOTE:** Random forests have been observed to overfit for some datasets with noisy classification\/regression tasks.\n","db05bdaa":"<font color = '#F1C40F'>\n\n<a id = \"14\"><\/a><br>\n##  Min-Max Scaler","d3245c95":"Before creating a new variable, let's create a CART model and do variable analysis.","01753d4a":"<font color = '#F1C40F'>\n\n<a id = \"9\"><\/a><br>\n## Data Preprocessing\n    ","928ec3f8":"<font color = '#F1C40F'>\n\n<a id = \"13\"><\/a><br>\n##  One-Hot Encoding","bd469e4b":"<div>\n    <h1><center style=\"background-color:#C39BD3; color:white;\"> HITTERS BASEBALL DATA<\/center><\/h1>\n<\/div>","f5d63de0":"<font color = '#F1C40F'>\n\n<a id = \"12\"><\/a><br>\n##  Label Encoding","2a6f0b55":"<a id='ldata'><\/a>\n <a id = \"7\"><\/a><br>\n<div>  \n<h3><center style=\"background-color:#C39BD3; color:white;\"><strong>FEATURE ENGINEERING \ud83d\udcda<\/strong><\/center><\/h3>\n<\/div>","ce990c8f":"* This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n\n* default value is between 0-1.\n\n* Affected by the max value in the data.\n\n* This disadvantage can be turned off by outliers corrections.","d7c8ec3c":"<p><img style=\"float: top;margin: max-width:750px\" src=\"https:\/\/i.imgur.com\/h9AkgFx.gif\">\n\n","aa63cf28":"**Random Forests** and **LightGbm** has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing.I will still throw out the small amount of outlier in the data.\n\n\nSo what if we tried filling it instead of deleting it? \nFor missing data, the filling method is appropriate.But here the missing variable is the target variable.It is not a good idea to fill in the target variable. Why should I fill in the variable that we are trying to find the result?","e5586b24":"* Light GBM is a gradient boosting framework that uses tree based learning algorithm.\n\n* Light GBM is prefixed as \u2018Light\u2019 because of its high speed. \n \n* Light GBM can handle the large size of data and takes lower memory to run. \n \n* LGBM also supports GPU learning and thus data scientists are widely using LGBM for data science application development.\n  \n* The most important parameters: learning rate, max_depth, min_data_in_leaf,bagging_fraction,min_gain_to_split etc.(According to the situation, the best should be found by trying.)","6c0a1e20":"* **Feature engineering** is a process of using domain knowledge to create\/extract new features from a given dataset by using data mining techniques.\n* Used to improve performance.\n* Better explains the model for Machine Learning.\n* Creating new features to enhance the model.\n\n* You can derive new variables with the help of variables in the data.\n* To do this, one can first examine the correlation between variables.\n* To do this, one can first examine the correlation between variables.\n* So the potential of the data increases.\n\n**BUT;**\n\nUnnecessary variables should not be created.Do not deviate from purpose.We want to explain the data better.\nWe don't want to complicate. \n\nDid the newly created variables work? Check feature importance.\n","447248c7":"<font color = '#F1C40F'>\n\n<a id = \"10\"><\/a><br>\n## Outliers"}}