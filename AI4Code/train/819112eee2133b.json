{"cell_type":{"6a9a0fb7":"code","bb42a586":"code","e0f39dff":"code","c5326458":"code","aa5e28bd":"code","0896cd8f":"code","c87dbbed":"code","af6fd1f3":"code","c359232e":"code","3a0fd90c":"code","fdc13ad7":"code","3ae0c318":"code","13ba9e68":"code","dc6110d3":"code","79fcc9d7":"code","73cacbc6":"code","8183e7d2":"code","c0171d1c":"code","f6ec1443":"code","0df3e16f":"code","2202a68f":"code","ef6c7647":"markdown","50c410e2":"markdown","15430d83":"markdown","5be95e52":"markdown","daf21d14":"markdown","532d671b":"markdown","79721f33":"markdown","92f833ff":"markdown","400e55b4":"markdown","980425f0":"markdown","8d8610dd":"markdown","1ce7e0a1":"markdown","de72d179":"markdown","b9bdf60a":"markdown","5edbbd48":"markdown","b7f6cd80":"markdown","969607f6":"markdown"},"source":{"6a9a0fb7":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nuser_credential = user_secrets.get_gcloud_credential()\nuser_secrets.set_tensorflow_credential(user_credential)","bb42a586":"from kaggle_datasets import KaggleDatasets\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('cifar10dataset')","e0f39dff":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import (Dense, Conv2D, Flatten, Dropout, BatchNormalization, Activation, SeparableConv2D)\nfrom tensorflow.keras.optimizers import Adam, SGD\nimport matplotlib.pyplot as plt","c5326458":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","aa5e28bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0896cd8f":"# The training and validation tfrecord have same feature set \n# i.e., image in bytes and corresponding labels in int64 format\n\nPATH = GCS_DS_PATH\nAUTO = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 128 * 4\nCATEGORIES = ['airplane','automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n\ntrain_features = {\n    'image':tf.io.FixedLenFeature([], dtype=tf.string),\n    'label':tf.io.FixedLenFeature([], dtype=tf.int64),\n}\n\n# The test tfrecord features same as above with change instead of label we have id of that image.\ntest_features = {\n    'image':tf.io.FixedLenFeature([], dtype=tf.string),\n    'id':tf.io.FixedLenFeature([], dtype=tf.int64),\n}\n\n\ndef load_train_example(example):\n    example = tf.io.parse_single_example(example, train_features)\n    image = tf.io.parse_tensor(example['image'], out_type=tf.float32)[:32, :32, :3] # image is a serialized tensor.\n    image = tf.reshape(tf.clip_by_value(image, 0, 1), shape=(32, 32, 3)) # As it is serialized we need to reshape.\n    return image, tf.one_hot(example['label'], depth=10) # labels needed to be one-hot encoded.\n\ndef load_test_example(example):\n    example = tf.io.parse_single_example(example, test_features)\n    image = tf.io.parse_tensor(example['image'], out_type=tf.float32)[:32, :32, :3]\n    image = tf.reshape(tf.clip_by_value(image, 0, 1), shape=(32, 32, 3))\n    return image, example['id']\n    \ntrain_data = tf.data.TFRecordDataset(PATH + \"\/preprocessed_train_data.tfrecord\",\n                                     compression_type='GZIP')\nval_data = tf.data.TFRecordDataset(PATH + \"\/preprocessed_val_data.tfrecord\",\n                                   compression_type='GZIP')\n\ntrain_data = train_data.map(load_train_example, num_parallel_calls=AUTO)\n# Shuffle the train data and batch it. Also don't forget to prefetch to avoid bottleneck performance.\ntrain_data = train_data.shuffle(buffer_size=100000, seed=1).batch(BATCH_SIZE).prefetch(AUTO)\n\nval_data = val_data.map(load_train_example, num_parallel_calls=AUTO).cache().shuffle(buffer_size=100000, seed=2)\nval_data = val_data.batch(BATCH_SIZE).prefetch(AUTO)\n\ntest_data = tf.data.TFRecordDataset(PATH + '\/test_data.tfrecord')\ntest_data =test_data.map(load_test_example, num_parallel_calls=AUTO)\ntest_data = test_data.batch(BATCH_SIZE).prefetch(AUTO)","c87dbbed":"def plot_images(data, labels, grid, has_categories=True):\n    _, axes = plt.subplots(grid[0], grid[1], figsize=(20,15), gridspec_kw={'hspace':.01, 'wspace':.01})\n    if len(labels.shape) > 1:\n        print(\"Converting One-Hot encoded labels to Categorical labels\")\n        labels = tf.argmax(labels, axis=1)\n    for img, label, ax in zip(data, labels, axes.ravel()):\n        ax.imshow(img)\n        ax.axis('off')\n        if has_categories:\n            ax.set_title(CATEGORIES[int(label)])\n        else:\n            ax.set_title(int(label))","af6fd1f3":"for i in train_data:\n    sample1 = i\n    break","c359232e":"for i in val_data:\n    sample2 = i\n    break","3a0fd90c":"for i in test_data:\n    sample3 = i\n    break","fdc13ad7":"# 32 images = 4 rows * 8 columns\nplot_images(data=sample1[0], labels=sample1[1], grid=[4, 8])","3ae0c318":"# 32 images = 4 rows * 8 columns\nplot_images(data=sample2[0], labels=sample2[1], grid=[4, 8])","13ba9e68":"# 32 images = 4 rows * 8 columns\nplot_images(data=sample3[0], labels=sample3[1], grid=[4, 8], has_categories=False)","dc6110d3":"from tqdm import tqdm\n\ndata = None\nfor i in tqdm(val_data):\n    data = pd.concat([data, pd.DataFrame(np.argmax(i[1].numpy(), axis=1))], axis=0)\n    \nprint(data[0].value_counts())\nprint()\n\ndata = None\nfor i in tqdm(train_data):\n    data = pd.concat([data, pd.DataFrame(np.argmax(i[1].numpy(), axis=1))], axis=0)\n    \nprint(data[0].value_counts())\n\ndel data","79fcc9d7":"def get_model():\n    with strategy.scope():\n        ki = 'glorot_normal'\n        activation = tf.nn.leaky_relu\n\n        inputs = tf.keras.Input(shape=(32, 32, 3))\n        x = Conv2D(filters=16, kernel_size=(1,1), strides=(1,1), padding='same', \n                activation=None, kernel_initializer=ki)(inputs)\n        x = BatchNormalization()(x)\n        x = Activation(activation)(x)\n        x = Conv2D(filters=16, kernel_size=(3,3), strides=(1,1), padding='same', \n                activation=None, kernel_initializer=ki)(x)\n        x = BatchNormalization()(x)\n        x1 = Activation(activation)(x)\n\n\n\n        x = Conv2D(filters=32, kernel_size=(1,1), strides=(1,1), padding='same', \n                activation=None, kernel_initializer=ki)(x1)\n        x = BatchNormalization()(x)\n        x = Activation(activation)(x)\n        x = Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding='same', \n                activation=None, kernel_initializer=ki)(x)\n        x = BatchNormalization()(x)\n\n\n\n        # skip connection - 1\n        x1 = Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding='same', \n                activation=None, kernel_initializer=ki)(x1)\n        x1 = BatchNormalization()(x1)\n        x1 = Activation(activation)(x + x1)\n\n\n\n        x = Conv2D(filters=64, kernel_size=(1,1), strides=(1,1), padding='same', \n                activation=None, kernel_initializer=ki)(x1)\n        x = BatchNormalization()(x)\n        x = Activation(activation)(x)\n        x = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same', \n                activation=None, kernel_initializer=ki)(x)\n        x = BatchNormalization()(x)\n        \n        \n        \n        # Recalibrate Filters - 1\n        aux = SeparableConv2D(filters=32, kernel_size=(32,32), strides=(1,1), padding='valid', \n                activation=None, kernel_initializer=ki)(x)\n        shape = tf.shape(aux) + (0, 0, 0, 32)\n        aux = BatchNormalization()(aux)\n        aux = Activation(activation)(aux)\n        aux = Flatten()(aux)\n        aux = Dense(4, activation=None)(aux)\n        aux = Activation(activation)(aux)\n        aux = Dense(64, activation='softmax')(aux)\n        x *= tf.reshape(aux, shape=shape)\n\n\n\n        # skip connection - 2\n        x1 = Conv2D(filters=64, kernel_size=(1,1), strides=(1,1), padding='same', \n                activation=None, kernel_initializer=ki)(x1)\n        x1 = BatchNormalization()(x1)\n        x1 = Activation(activation)(x + x1)\n\n\n\n        x = Conv2D(filters=128, kernel_size=(1,1), strides=(1,1), padding='same', \n                activation=None, kernel_initializer=ki)(x1)\n        x = BatchNormalization()(x)\n        x = Activation(activation)(x)\n        x = Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), padding='same', \n                activation=None, kernel_initializer=ki)(x)\n        x = BatchNormalization()(x)\n        \n        \n                        \n        # Recalibrate Filters - 2\n        aux = SeparableConv2D(filters=64, kernel_size=(32,32), strides=(1,1), padding='valid', \n                activation=None, kernel_initializer=ki)(x)\n        shape = tf.shape(aux) + (0, 0, 0, 64)\n        aux = BatchNormalization()(aux)\n        aux = Activation(activation)(aux)\n        aux = Flatten()(aux)\n        aux = Dense(8, activation=None)(aux)\n        aux = Activation(activation)(aux)\n        aux = Dense(128, activation='softmax')(aux)\n        x *= tf.reshape(aux, shape=shape)\n\n\n\n        # skip connection - 3\n        x1 = Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), padding='same', \n                activation=None, kernel_initializer=ki)(x1)\n        x1 = BatchNormalization()(x1)\n        x1 = Activation(activation)(x + x1)\n\n\n\n        x = Conv2D(filters=64, kernel_size=(1,1), strides=(1,1), padding='same', \n                activation=None, kernel_initializer=ki)(x1)\n        x = BatchNormalization()(x)\n        x = Activation(activation)(x)         \n        x = Conv2D(filters=64, kernel_size=(32,32), strides=(1,1), padding='valid', \n                activation=None, kernel_initializer=ki)(x)\n        x = BatchNormalization()(x)\n        x = Activation(activation)(x)\n\n\n\n        x = Flatten()(x)    \n        output = Dense(10, activation='softmax')(x)\n\n        model = tf.keras.models.Model(inputs=[inputs], outputs=[output])\n\n        model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), \n                    optimizer=tf.keras.optimizers.Adam(learning_rate=.005), \n                    metrics=['accuracy'])\n\n    return model\n\nmodel = get_model()","73cacbc6":"model.summary()","8183e7d2":"tf.keras.utils.plot_model(model)","c0171d1c":"# Model Checkpoint to store only model which has highest validation accuracy\nmodel_cb = tf.keras.callbacks.ModelCheckpoint('cifar10_model1.h5', save_best_only=True, verbose=1)\n# Learning Rate Scheduler to update learning rate.\n\n# I have set patience to 2 which means that it will wait for 2 consecutive epochs to improve validation loss.\n# if it fails then learning rate is reduced by factor of .6.\nlr_cb = tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=.6, min_delta=.005, verbose=1)\n\nhistory = model.fit(train_data, validation_data = val_data, epochs=35, \n                    callbacks=[model_cb, lr_cb]) ","f6ec1443":"y_test = model.predict(test_data, verbose=1)\ny_test = tf.argmax(y_test, axis=1)","0df3e16f":"for i in test_data:\n    sample = i\n    break","2202a68f":"plot_images(sample[0], y_test[:BATCH_SIZE], grid=(4,8))","ef6c7647":"### So more generalized approach is to use a Convolution Layer instead of Dense Layer. That's all you need to know.","50c410e2":"The *preprocessed_val_data* and *preprocessed_train_data* are both compressed with \"GZIP\" compression type. So we need to add compression_type=\"GZIP\" in TFRecordDataset call.","15430d83":"# Fun Part: Model Architecture","5be95e52":"# Training and Validating Model","daf21d14":"But you can suggest me any other architectures, if you have one in mind. One with highest validation accuracy will be added in next version with your name.","532d671b":"# Observe Data","79721f33":"All the files are TFRecordDataset files as you can see from extension. ","92f833ff":"### NOTE -  Even after having large dataset to train on, the CNN model converges very rapidly due to skip connections and recalibrating layers.","400e55b4":"# Further Notes","980425f0":"## Note that we are not using Dense layer after the last Convolution layer instead we are using a Convolution layer, which acts as a Dense layer. \n\n\n## Why Convolution instead of Dense Layer?? Well put is this way  if you have images of resolution more that 32 x 32 and you plan to use this model, it will not complain about image resolution, just change the *shape* in Input Layer, and you will be good to go.\n\n\n## But instead if you use a Dense Layer then by changing resolution it will not compile because Dense Layer has fixed no. of weights, while in case of Convolution layer there are no fixed weights there are only fixed sized kernels.","8d8610dd":"I have used many architectures with skip connections, ensemble architectures, and all sort of architectures. But this is how far I've reached in getting accuracy.","1ce7e0a1":"### Check out the no. of instances in every class","de72d179":"## For example let us consider images of 32 x 32 x 3 resolution and images of resolution 128 x 128 x 3.\n\n\n1. Image of resolution 32 x 32 x 3. \n        \n   Final Output Size of last Convolution Layer (Here the Convolution Layer of skip-connection - 3) = (8, 8, 128). \n   \n   It means height is 8, width is 8, and channels\/filters are 128.\n   Now you Flatten this Convolution Layer and you get total neurons = 8 * 8 * 128 = 8192.\n   So Dense layer will have weight matrix of [8192 * 10].\n\n\n\n2. Image of Resolution 128 x 128 x 3\n    \n    But now you decided to use this model to predict images of size 128 x 128 x 3. So again the output of last convolution layer will be = 32 * 32 * 128 = 131072. So it assumes that the Dense layer has weight matrix of [131072 * 10]. But wait that's not true!!. So you will receive an error during compilation.\n    \n    SO YOU NEED TO TRAIN YOUR MODEL AGAIN WITH 128 X 128 X 3 IMAGES. I KNOW ITS REALLY PAINFUL!!","b9bdf60a":"# Testing Model","5edbbd48":"## **NOTE** - The id of images in the test data is not strictly increasing it has the pattern which was used to extract images from it's .7z file. So Sorry for that. But it contains all 3,00,000 images","b7f6cd80":"## So that's it folks.","969607f6":"# Loading Data and Dependencies"}}