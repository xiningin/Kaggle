{"cell_type":{"4a16a59d":"code","7098579d":"code","83c61c96":"code","088429b5":"code","e38dbca3":"code","05f8b148":"code","c1fc7088":"code","66480edf":"code","0b837d14":"code","31e1fc2a":"code","69822c01":"code","c16fec93":"code","4e92fa18":"code","a44466ce":"code","8569d00a":"code","5542e8f2":"code","b49edf54":"code","ea294496":"code","951b26fa":"code","ce1c8b99":"code","8a30f303":"code","5feabc5d":"code","17e27b7d":"code","53fda166":"code","b242aefe":"code","10decbc9":"code","0b51fe49":"code","e1698455":"code","5b38b0ff":"code","c0028688":"code","d2169028":"code","1f6d02fc":"code","ad6e1b33":"code","e4450b5b":"code","2796c6f0":"code","62551cca":"code","6e5741be":"code","9cce11b4":"code","855eb100":"code","aeb631ce":"code","f3b8d3e2":"code","1ce10faa":"code","eea73934":"code","9efd2ebd":"code","3259d3d3":"code","b67806ac":"code","ed9ca78e":"code","21fe9918":"code","7beadc30":"code","b5073346":"code","b8cfa631":"code","dec928ec":"code","bd262634":"code","af28aefc":"code","1c785e09":"code","fc621b27":"code","b85b281b":"code","7e4f88ae":"code","db8586a5":"code","b205a60a":"code","37f110c3":"code","f2848e30":"code","828cc1f5":"code","3dc8009b":"code","a818dcb1":"code","1a6dbef2":"code","76e144ee":"code","f767e032":"code","cd7e6c4c":"code","242475de":"code","c7f6fcef":"code","4ed2084b":"code","b94af3b1":"code","30b3d45c":"code","d701ccf7":"code","d6a64a83":"code","1ad135e3":"code","f9cefb99":"code","b8bf819e":"code","b9131535":"code","f5285e7f":"code","6617ca53":"code","ee138105":"code","5b2f3876":"code","56e3c091":"code","ffb87bd7":"code","7420fe2e":"code","10af6ccb":"code","3a8c61dd":"code","636a8164":"code","df086ce3":"code","fdc6106b":"code","bd17ac57":"markdown","e75d22d9":"markdown","6c962c65":"markdown","2d2e928b":"markdown","cdbc0f73":"markdown","def148b9":"markdown","91b69aaf":"markdown","7e5321e3":"markdown","66a122a9":"markdown","b959a5ad":"markdown","4e897262":"markdown","11e5d4ec":"markdown","e4ae2c80":"markdown","6cd44262":"markdown","f8c3eb47":"markdown","9a38352f":"markdown","004a19e3":"markdown","d0dad44b":"markdown","c5b679b8":"markdown","d0a63e43":"markdown","079f147c":"markdown","d1edbc4f":"markdown","2de66143":"markdown","401ff421":"markdown","3192d4d4":"markdown","0647e2a9":"markdown","05aa21e6":"markdown","0d12ac6e":"markdown","b89bb01f":"markdown","0eed1dc4":"markdown","ab210cf4":"markdown","a68a8ecc":"markdown","c6699eff":"markdown","76ca1d45":"markdown","8e8ac461":"markdown","30aef1d1":"markdown","785e579e":"markdown","52e9b6a3":"markdown","75e4c849":"markdown","244b03c9":"markdown","66d0545e":"markdown","b49b6f6c":"markdown","178ca6c6":"markdown","68cf5e08":"markdown","acc83349":"markdown","2979d6fe":"markdown","559d9a56":"markdown","c2b6fd75":"markdown","1856bf6d":"markdown","5119b69c":"markdown","f6d64d71":"markdown","fd777aa7":"markdown","24cc4dd4":"markdown","eca80895":"markdown","f5f94097":"markdown","d099bece":"markdown","4222bf9d":"markdown","e5622f67":"markdown","9d604819":"markdown","7c690275":"markdown","8d5ef7e5":"markdown","70aa11ae":"markdown","a93cf7cf":"markdown","935a2dcf":"markdown","81ba6f32":"markdown","9ee7b4ae":"markdown","92c4df50":"markdown","0be6a0d4":"markdown","ebfdf3ab":"markdown","b5724455":"markdown","3f1fb638":"markdown","6bb1dec4":"markdown","07e471b0":"markdown","f4da0299":"markdown","9c65e2eb":"markdown","b4151c22":"markdown","8b24cdbb":"markdown","7a642134":"markdown","9daa7d2c":"markdown","87144974":"markdown","a9281e9c":"markdown","fc141ed1":"markdown","381a5f93":"markdown","6d82f117":"markdown","8cd444e3":"markdown","13c13a17":"markdown","ece46362":"markdown","79615bb3":"markdown","866a8b4d":"markdown","a9976aef":"markdown","a114b1c4":"markdown","3f0cf86f":"markdown","11016720":"markdown","910f6966":"markdown","4d0db1d4":"markdown","7e2985a5":"markdown","0f4886d5":"markdown","56ab5cd2":"markdown","80a7b654":"markdown","f97100a0":"markdown","4a66ad2b":"markdown","430137b5":"markdown","d8a3a987":"markdown","7e506406":"markdown","59ad3579":"markdown","2f398131":"markdown","06f679fe":"markdown","5311f99f":"markdown","9776b720":"markdown","b9795669":"markdown","e52e49ab":"markdown","130e953b":"markdown","3f2ae46e":"markdown","e380b83e":"markdown"},"source":{"4a16a59d":"import math, re, os, time\nimport datetime\nimport tensorflow as tf\nimport numpy as np\nfrom collections import namedtuple, Counter\nimport json\nfrom matplotlib import pyplot as plt\nfrom matplotlib import gridspec\nimport itertools \nfrom kaggle_datasets import KaggleDatasets\nimport sklearn\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\nimport pandas as pd\n\nprint(\"Tensorflow version \" + tf.__version__)\n\nfrom tensorflow.keras.applications import Xception\nfrom tensorflow.keras.applications import DenseNet121, DenseNet169, DenseNet201\nfrom tensorflow.keras.applications import ResNet50V2, ResNet101V2, ResNet152V2\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.applications import InceptionResNetV2\n\n# Only for tensorflow 2.3\n# from tensorflow.keras.applications import EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, EfficientNetB4, EfficientNetB5, EfficientNetB6, EfficientNetB7\n\n!pip install -q efficientnet\nimport efficientnet.tfkeras as efn\n\nMODEL_CLASSES = {\n    'Xception': Xception,\n    'DenseNet121': DenseNet121,\n    'DenseNet169': DenseNet169,\n    'DenseNet201': DenseNet201,\n    'ResNet50V2': ResNet50V2,\n    'ResNet101V2': ResNet101V2,\n    'ResNet152V2': ResNet152V2,\n    'InceptionV3': InceptionV3,\n    'InceptionResNetV2': InceptionResNetV2,\n    'EfficientNetB0': efn.EfficientNetB0,\n    'EfficientNetB1': efn.EfficientNetB1,\n    'EfficientNetB2': efn.EfficientNetB2,\n    'EfficientNetB3': efn.EfficientNetB3,\n    'EfficientNetB4': efn.EfficientNetB4,\n    'EfficientNetB5': efn.EfficientNetB5,\n    'EfficientNetB6': efn.EfficientNetB6,\n    'EfficientNetB7': efn.EfficientNetB7,\n}\n\nimport gc\ngc.enable()","7098579d":"# Detect hardware, return appropriate distribution strategy\n\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","83c61c96":"GCS_DS_PATH = KaggleDatasets().get_gcs_path('tpu-getting-started')\nprint(f'GCS_DS_PATH = {GCS_DS_PATH}\\n')\n\n# you can list the bucket with \"!gsutil ls $GCS_DS_PATH\"\n!gsutil ls $GCS_DS_PATH","088429b5":"# At size `512`, a GPU will run out of memory, so we use the TPU.\n# For GPU training, please select 224 x 224 px image size.\nIMAGE_SIZE = [192, 192] \n\nGCS_PATH_SELECT = { # available image sizes\n    192: GCS_DS_PATH + '\/tfrecords-jpeg-192x192',\n    224: GCS_DS_PATH + '\/tfrecords-jpeg-224x224',\n    331: GCS_DS_PATH + '\/tfrecords-jpeg-331x331',\n    512: GCS_DS_PATH + '\/tfrecords-jpeg-512x512'\n}\n# Select the dataset containing the size we chose above\nGCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/train\/*.tfrec')\nVALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/val\/*.tfrec')\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/test\/*.tfrec') # predictions on this dataset should be submitted for the competition","e38dbca3":"CLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']                                                                                                                                               # 100 - 103","05f8b148":"print(f\"number of flower classes: {len(CLASSES)}\")","c1fc7088":"# numpy and matplotlib defaults\nnp.set_printoptions(threshold=15, linewidth=80)\n\ndef batch_to_numpy_images_and_labels(data):\n    \n    if type(data) == tuple:\n        images, labels = data\n    else:\n        images = data\n        labels = None\n    \n    numpy_images = images.numpy()\n    \n    numpy_labels = [None for _ in enumerate(numpy_images)]\n    if labels is not None:\n        numpy_labels = labels.numpy()\n        if numpy_labels.dtype == object: # binary string in this case, these are image ID strings\n            numpy_labels = [None for _ in enumerate(numpy_images)]\n    \n    # If no labels, only image IDs, return None for labels (this is the case for test data)\n    return numpy_images, numpy_labels\n\ndef title_from_label_and_target(label, correct_label):\n    if correct_label is None:\n        return '{} {}'.format(CLASSES[label], label) , True\n    correct = (label == correct_label)\n    return \"{} [{}{}{}]\".format(\n        '{} {}'.format(CLASSES[label], label),\n        'OK' if correct else 'NO',\n        u\"\\u2192\" if not correct else '',\n        '{} {}'.format(CLASSES[correct_label], correct_label) if not correct else ''\n    ), correct\n\ndef display_one_flower(image, title, subplot, red=False, titlesize=16):\n    plt.subplot(*subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    if len(title) > 0:\n        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize\/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize\/1.5))\n    return (subplot[0], subplot[1], subplot[2]+1)\n    \ndef display_batch_of_images(databatch, predictions=None):\n    \"\"\"This will work with:\n    display_batch_of_images(images)\n    display_batch_of_images(images, predictions)\n    display_batch_of_images((images, labels))\n    display_batch_of_images((images, labels), predictions)\n    \"\"\"\n    \n    # data\n    images, labels = batch_to_numpy_images_and_labels(databatch)\n    if labels is None:\n        labels = [None for _ in enumerate(images)]\n        \n    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n    rows = int(math.sqrt(len(images)))\n    cols = len(images)\/\/rows\n        \n    # size and spacing\n    FIGSIZE = 13.0\n    SPACING = 0.1\n    subplot=(rows,cols,1)\n    if rows < cols:\n        plt.figure(figsize=(FIGSIZE,FIGSIZE\/cols*rows))\n    else:\n        plt.figure(figsize=(FIGSIZE\/rows*cols,FIGSIZE))\n    \n    # display\n    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n        title = '' if label is None else '{} {}'.format(CLASSES[label], label)\n        correct = True\n        if predictions is not None:\n            title, correct = title_from_label_and_target(predictions[i], label)\n        dynamic_titlesize = FIGSIZE*SPACING\/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n    \n    #layout\n    plt.tight_layout()\n    if label is None and predictions is None:\n        plt.subplots_adjust(wspace=0, hspace=0)\n    else:\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    plt.show()\n    plt.close()\n\ndef get_title(label, prediction):\n\n    title = '' if label is None else '{} {}'.format(CLASSES[label], label)\n    correct = True\n    if prediction is not None:\n        title, correct = title_from_label_and_target(prediction, label)\n    return title, correct\n\ndef display_one_flower_ax(image, label, prediction, ax, red=False, titlesize=16):\n\n    title, correct = get_title(label, prediction)\n    red = not correct\n    \n    ax.axis('off')\n    ax.imshow(image)\n    if len(title) > 0:\n        ax.set_title(title, fontsize=int(titlesize) if not red else int(titlesize \/ 1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize \/ 1.5))        \n        \ndef display_pairs_of_image_batch(databatch, databatch_2=None, predictions=None, predictions_2=None, ds_name_1=None, ds_name_2=None):\n    \"\"\"This will work with:\n    display_batch_of_images(images)\n    display_batch_of_images(images, predictions)\n    display_batch_of_images((images, labels))\n    display_batch_of_images((images, labels), predictions)\n    \"\"\"\n\n    nb_databatch = 1\n    if databatch_2:\n        nb_databatch = 2\n    \n    # data\n    images, labels = batch_to_numpy_images_and_labels(databatch)\n    if labels is None:\n        labels = [None for _ in enumerate(images)]\n\n    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n    rows = int(math.sqrt(len(images)))\n    cols = len(images) \/\/ rows        \n\n    gs0 = gridspec.GridSpec(1, nb_databatch)\n    gs00 = gridspec.GridSpecFromSubplotSpec(rows, cols, subplot_spec=gs0[0])\n    \n    if databatch_2:\n        images_2, labels_2 = batch_to_numpy_images_and_labels(databatch_2)\n        if labels_2 is None:\n            labels_2 = [None for _ in enumerate(images_2)]\n        gs01 = gridspec.GridSpecFromSubplotSpec(rows, cols, subplot_spec=gs0[1]) \n\n    # size and spacing\n    FIGSIZE = 24.0\n    SPACING = 0.10\n    subplot=(rows, cols, 1)\n\n    if rows < cols:\n        fig = plt.figure(figsize=(FIGSIZE, FIGSIZE \/ nb_databatch \/ cols * rows))\n    else:\n        fig = plt.figure(figsize=(FIGSIZE \/ rows * cols, FIGSIZE \/ nb_databatch))\n    \n    if ds_name_1 and ds_name_2:\n        fig.suptitle('2 batch of images. [Left: {}]   vs.   [Right: {}]'.format(ds_name_1, ds_name_2), y=-0.05, verticalalignment='bottom', fontsize=24)\n    elif ds_name_1:\n        fig.suptitle('1 batch of images from {}'.format(ds_name_1), y=-0.05, verticalalignment='bottom', fontsize=24)\n        \n    dynamic_titlesize = FIGSIZE * SPACING \/ max(rows, 2 * cols) * 40 + 3 # magic formula tested to work from 1x1 to 10x10 images          \n        \n    # display\n    for row, col in itertools.product(range(rows), range(cols)):\n        \n        idx = row * cols + col\n\n        image = images[idx]\n        label = labels[idx]\n        prediction = None if predictions is None else predictions[idx]\n        ax = fig.add_subplot(gs00[row, col])\n        display_one_flower_ax(image, label, prediction, ax, titlesize=dynamic_titlesize)\n        \n        if databatch_2:\n            image = images_2[idx]\n            label = labels_2[idx]\n            prediction = None if predictions_2 is None else predictions_2[idx]\n            ax = fig.add_subplot(gs01[row, col])\n            display_one_flower_ax(image, label, prediction, ax, titlesize=dynamic_titlesize)\n\n    #layout\n    plt.tight_layout()\n    \n    if label is None and predictions is None:\n        plt.subplots_adjust(wspace=SPACING \/ 2, hspace=SPACING \/ 2)\n    else:\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    \n    plt.show()\n    plt.close()","66480edf":"raw_dataset = tf.data.TFRecordDataset(TRAINING_FILENAMES)\nraw_dataset","0b837d14":"serialized_example = next(iter(raw_dataset))\n\nprint('A serialized example looks like:\\n\\n' + str(serialized_example)[:100] + '...' * 5 + str(serialized_example)[-100:] + '\\n')\nprint(type(serialized_example))","31e1fc2a":"example = tf.train.Example()\nexample.ParseFromString(serialized_example.numpy())\nprint(str(example)[:300] + ' ...')","69822c01":"def decode_image(image_data):\n    \"\"\"\n    Args:\n        image_data: A `tf.string` obtained from `tf.io.encode_jpeg()`.\n    \"\"\"\n    \n    # image is now of type `tf.uint8`\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    \n    # convert image to floats in [0, 1] range\n    image = tf.cast(image, tf.float32) \/ 255.0  \n    \n    # explicit size needed for TPU\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) \n    \n    return image\n\ndef read_labeled_tfrecord(example):\n    \n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    parsed_example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(parsed_example['image'])\n    label = tf.cast(parsed_example['class'], tf.int32)\n    \n    return image, label # returns a dataset of (image, label) pairs\n\ndef read_unlabeled_tfrecord(example):\n    \n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n    }\n    parsed_example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(parsed_example['image'])\n    idnum = parsed_example['id']\n    \n    return image, idnum # returns a dataset of (image, id) pairs","c16fec93":"parsed_example = read_labeled_tfrecord(serialized_example)\nprint('A parsed example looks like\\n\\n' + str(parsed_example)[:200] + '\\n...')","4e92fa18":"def load_dataset(filenames, labeled=True, ordered=False):\n    \"\"\"Read from TFRecords.\n    \n    For optimal performance, reading from multiple files at once and disregarding data order (if `ordered=False`).\n\n    Order does not matter since we will be shuffling the data anyway (for training dataset).\n    \"\"\"\n\n    options = tf.data.Options()\n    if not ordered:\n        # disable order, increase speed\n        options.experimental_deterministic = False\n\n    # Read in an automatically interleaving way from multiple tfrecord files.\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=tf.data.experimental.AUTOTUNE)\n    \n    # Uses data as soon as it streams in, rather than in its original order.\n    dataset = dataset.with_options(options) \n    \n    # parse and return a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    dataset = dataset.map(\n        read_labeled_tfrecord if labeled else read_unlabeled_tfrecord,\n        num_parallel_calls=tf.data.experimental.AUTOTUNE,\n    )\n    \n    return dataset\n\ndef get_training_dataset(batch_size, shuffle_buffer_size, repeat_dataset=False, ordered=False, drop_remainder=True):\n    \"\"\"\n    Set `shuffle_buffer_size` to `1` to have no shuffling.\n    \"\"\"\n    \n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True, ordered=ordered)\n    \n    # Repeat the training dataset. We will determine the number of steps (or updates) later for 1 training epoch.\n    if repeat_dataset:\n        dataset = dataset.repeat()\n    \n    # Shuffling\n    if not ordered:\n        dataset = dataset.shuffle(shuffle_buffer_size)\n    \n    # Batching\n    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n    \n    # prefetch next batch while training (autotune prefetch buffer size)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    \n    return dataset\n\ndef get_validation_dataset(batch_size):\n    \n    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=True)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    \n    return dataset\n\ndef get_test_dataset(batch_size):\n    \n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=True)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) \n    \n    return dataset\n\ndef count_data_items(filenames):\n    # For this flower dataset, the number of data items is written in the name of .tfrec files.\n    # For example, `flowers00-230.tfrec` means 230 data items in it.\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nORIGINAL_NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\nNUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n\nprint('Original Dataset:\\n\\n{} training images\\n{} validation images\\n{} unlabeled test images'.format(ORIGINAL_NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))","a44466ce":"ds = get_training_dataset(batch_size=3, shuffle_buffer_size=1)\nds","8569d00a":"batch = next(iter(ds))\nprint('The batch is a {} with {} components.'.format(type(batch).__name__, len(batch)))\nprint('\\nThe 1st compoent is a {} with shape {}'.format(type(batch[0]).__name__, batch[0].shape))\nprint('The 2nd compoent is a {} with shape {}\\n'.format(type(batch[1]).__name__, batch[1].shape))\n\nprint('The 2nd compoent looks like')\nbatch[1]","5542e8f2":"ds = get_test_dataset(batch_size=3)\nds","b49edf54":"batch = next(iter(ds))\nprint('The batch is a {} with {} components.'.format(type(batch).__name__, len(batch)))\nprint('\\nThe 1st compoent is a {} with shape {}'.format(type(batch[0]).__name__, batch[0].shape))\nprint('The 2nd compoent is a {} with shape {}'.format(type(batch[1]).__name__, batch[1].shape))\n\nprint('\\nThe 2nd compoent looks like')\nbatch[1]","ea294496":"# Peek the training data\ntrain_dataset = get_training_dataset(batch_size=16, shuffle_buffer_size=1, ordered=True, drop_remainder=False)\ntrain_iter = iter(train_dataset)","951b26fa":"# run this cell again for next set of images\nbatch = next(train_iter)\ndisplay_batch_of_images(batch)","ce1c8b99":"# peek the validation data\nvalid_dataset = get_validation_dataset(batch_size=16)\nvalid_iter = iter(valid_dataset)","8a30f303":"# run this cell again for next set of images\ndisplay_batch_of_images(next(valid_iter))","5feabc5d":"# peek the test data\ntest_dataset = get_test_dataset(batch_size=16)\ntest_iter = iter(test_dataset)","17e27b7d":"# run this cell again for next set of images\ndisplay_batch_of_images(next(test_iter))","53fda166":"print('Original Dataset:\\n\\ntraining images: {}\\nvalidation images: {}\\ntest images {}'.format(ORIGINAL_NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))","b242aefe":"@tf.autograph.experimental.do_not_convert\ndef get_label_counting(labeled_dataset):\n    \n    c = Counter()\n    labels = []\n    for batch in labeled_dataset.map(lambda image, label: label, num_parallel_calls=tf.data.experimental.AUTOTUNE):\n        labels.append(batch)\n    \n    labels = tf.concat(labels, axis=0).numpy()\n    c.update(labels)\n\n    return labels, c","10decbc9":"train_labels, train_counter = get_label_counting(train_dataset)\nvalid_labels, valid_counter = get_label_counting(valid_dataset)","0b51fe49":"def plot_label_dist(labels, dist_1, dist_2, dist_label_1, dist_label_2, title=''):\n    \n    x = np.arange(len(labels)) # the label locations\n    width = 0.4 # the width of the bars\n\n    fig, ax = plt.subplots(figsize=(15, 5))\n    rects1 = ax.bar(x - width \/ 2, dist_1, width, label=dist_label_1)\n    rects2 = ax.bar(x + width \/ 2, dist_2, width, label=dist_label_2)\n\n    # Add some text for labels, title and custom x-axis tick labels, etc.\n    ax.set_ylabel('portion in dataset')\n    ax.set_title(title)\n    ax.set_xticks(x)\n    ax.set_xticklabels([str(x) if x % 5 in [0] else '' for x in range(len(labels))] )\n    ax.legend()\n\n    plt.show()\n    plt.close()","e1698455":"labels = list(range(len(CLASSES)))\ndist_train = [train_counter[x] \/ ORIGINAL_NUM_TRAINING_IMAGES for x in labels]\ndist_valid = [valid_counter[x] \/ NUM_VALIDATION_IMAGES for x in labels]    \n    \nhalf = len(labels) \/\/ 2\nplot_label_dist(\n    labels[:half],\n    dist_train[:half],\n    dist_valid[:half],\n    'Train',\n    'Valid',\n    title='Label distribution in Train\/Valid datasets: Labels 0-{}'.format(half - 1)\n)\n\nplot_label_dist(\n    labels[half:],\n    dist_train[half:],\n    dist_valid[half:],\n    'Train',\n    'Valid',    \n    title='Label distribution in Train\/Valid datasets: Labels {}-{}'.format(half, len(labels) - 1)\n)","5b38b0ff":"print(\"labels in the original training dataset, sorted by occurrence\\n\")\nprint(\"pairs of (class id, counting)\\n\")\nprint(train_counter.most_common())","c0028688":"def get_num_to_repeat_for_class(class_id, target_counting):\n    \"\"\"Compute the (ideal) number of times a training example with\n       label `class_id` should repeat in order to get a dataset where\n       each class occur `target_counting` times.\n       \n    The return value is a float number. The actual number to repeat will\n    be determined in the function `get_nums_to_repeat` in a randomized way,\n    in order to make a better approximation.\n       \n    Args:\n    \n        class_id: int, the id of a class.\n        target_counting: int, the targeted occurrence number.\n        \n    Returns:\n        A float, the number of times an example with label `class_id` to repeat.\n    \"\"\"\n    \n    # Use the counter computed in `Label distribution` subsection`.\n    counting = train_counter[class_id]\n    \n    # No need to repeat for a class having already the desired occurrecne.\n    if counting >= target_counting:\n        return 1.0\n    \n    num_to_repeat = target_counting \/ counting\n    \n    return num_to_repeat\n\ndef get_nums_to_repeat(target_counting):\n    \"\"\"Compute a tabel that stores the results of `get_num_to_repeat_for_class`\n       for every class and for the given `target_counting`.\n    \n    Args:\n        target_counting: int, the targeted occurrence number.\n    \n    Returns:\n        table: A `tf.lookup.StaticHashTable`.\n        d: A dictionary storing the same information as `table`.\n    \"\"\"\n    \n    keys = range(len(CLASSES))\n    values = [get_num_to_repeat_for_class(x, target_counting) for x in keys]\n\n    keys_tensor = tf.constant(keys)\n    vals_tensor = tf.constant(values)\n    \n    table_initializers = tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor)\n    table = tf.lookup.StaticHashTable(table_initializers, default_value=0.0)\n    \n    d = {k: v for k, v in zip(keys, values)}\n\n    return table, d\n\ndef get_num_to_repeat_for_example(example, table):\n    \"\"\"Compute the actual number of times a training example will repeat\n       in order to get a dataset where each class occur <approximately> \n       N times with N being a pre-defined number that is used for constructing\n       `table`.\n\n    Args:\n        example: A tuple of 2 tensors, which is a labeled training example and\n            represented as (image, label).\n                          \n        tabel: A tf.lookup.StaticHashTable, as obtained from `get_nums_to_repeat`.\n                          \n    Returns:\n        A tf.int64 scalar tensor, the number of times `example` will repeat.\n    \"\"\"\n    \n    image, label = example\n\n    num_to_repeat = table.lookup(label)    \n    \n    # This part is deterministic.\n    num_to_repeat_integral = tf.cast(int(num_to_repeat), tf.float32)\n    \n    # With a probability `residue`, we allow `example` to repeat one more time.\n    residue = num_to_repeat - num_to_repeat_integral\n    num_to_repeat = num_to_repeat_integral + tf.cast(tf.random.uniform(shape=()) <= residue, tf.float32)\n    \n    return tf.cast(num_to_repeat, tf.int64)","d2169028":"_, d = get_nums_to_repeat(782)\nd = sorted(d.items(), key=lambda x: x[1], reverse=True)\n\nprint('pair of (class id, num to repeat)\\n')\nfor x in d:\n    print(x)","1f6d02fc":"def get_oversampled_training_dataset(\n        target_counting, batch_size, shuffle_buffer_size,\n        repeat_dataset=False, ordered=False,\n        oversample=True, augmentation_fn=None, probability=1.0\n    ):\n    \"\"\"\n    Construct an oversampled dataset in which each class occurs approximately\n    `target_counting` times.\n    \n    (Special) Args:\n    \n        target_counting: int, the target occurrence.\n        oversampe: bool, if to use oversampling. If `False`, no oversampliing and\n            the arguement `target_counting` has no effect.\n        augmentation_fn: A funtion used to map the dataset for data augmentation.\n        probability: float, the probability to perform the augmentation\n        \n    Returns:\n        A tf.data.Dataset.\n    \"\"\"\n    \n    table, d = get_nums_to_repeat(target_counting)\n    \n    nb_examples = ORIGINAL_NUM_TRAINING_IMAGES\n    \n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True, ordered=ordered)\n\n    if oversample:\n        \n        # This is only approximation, but good enough.\n        nb_examples = int(sum([train_counter[k] *  v for k, v in d.items()]))\n        \n        dataset = dataset.flat_map(\n            lambda image, label: tf.data.Dataset.from_tensors((image, label)).repeat(get_num_to_repeat_for_example((image, label), table))\n        )\n        \n    if repeat_dataset:\n        dataset = dataset.repeat()\n \n    if not ordered:\n        if not shuffle_buffer_size:\n            shuffle_buffer_size = nb_examples\n        dataset = dataset.shuffle(shuffle_buffer_size)\n    \n    dataset = dataset.batch(batch_size, drop_remainder=True)\n    \n    if augmentation_fn:\n        probability = tf.constant(probability, dtype=tf.float32)\n        dataset = dataset.map(\n            lambda images, labels: augmentation_fn(images, labels, probability=probability),\n            num_parallel_calls=tf.data.experimental.AUTOTUNE\n        )\n        \n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    \n    return dataset, nb_examples","ad6e1b33":"oversampled_train_dataset, _ = get_oversampled_training_dataset(target_counting=782, batch_size=16, shuffle_buffer_size=1, repeat_dataset=False, ordered=True, oversample=True, augmentation_fn=None)\n\n_, oversampled_train_counter = get_label_counting(oversampled_train_dataset)\n\nprint('Oversampled training dataset:\\ntraining images: {}\\n'.format(sum(oversampled_train_counter.values())))\n\nprint(\"labels in the oversampled training dataset, sorted by occurrence: pairs of (label_id, label_counting)\\n\")\nprint(oversampled_train_counter.most_common())\n\nprint('\\n' + 'averaged number of occurrences: ', np.array(list(oversampled_train_counter.values())).mean())","e4450b5b":"dist_train_oversampled = np.array([oversampled_train_counter[x] for x in labels]) \/ sum(oversampled_train_counter.values())\n\nhalf = len(labels) \/\/ 2\nplot_label_dist(\n    labels[:half],\n    dist_train[:half],\n    dist_train_oversampled[:half],\n    'original',\n    'oversampled',\n    title='Label distribution in train datasets with\/without oversampling: Labels 0-{}'.format(half - 1)\n)\n\nplot_label_dist(\n    labels[half:],\n    dist_train[half:],\n    dist_train_oversampled[half:],\n    'original',\n    'oversampled',    \n    title='Label distribution in train datasets with\/without oversampling: Labels {}-{}'.format(half, len(labels) - 1)\n)","2796c6f0":"# Peek the oversampled training data\ntrain_iter = iter(train_dataset)\noversampled_train_iter = iter(oversampled_train_dataset)\n\ndisplay_pairs_of_image_batch(next(train_iter), next(oversampled_train_iter), ds_name_1='original dataset', ds_name_2='oversampled dataset')","62551cca":"class Flower_Trainer:\n    \n    def __init__(self, batch_size_per_replica=16, prediction_batch_size_per_replica=64, shuffle_buffer_size=1, oversample=False, target_counting=1, grad_acc_steps=1, augmentation_fn=None, probability=1.0, log_interval=1):\n    \n        self.batch_size_per_replica = batch_size_per_replica\n        self.prediction_batch_size_per_replica = prediction_batch_size_per_replica\n        \n        self.batch_size = batch_size_per_replica * strategy.num_replicas_in_sync\n        self.prediction_batch_size = prediction_batch_size_per_replica * strategy.num_replicas_in_sync\n\n        self.grad_acc_steps = grad_acc_steps\n        self.update_size = self.batch_size * self.grad_acc_steps\n        \n        self.shuffle_buffer_size = shuffle_buffer_size\n        self.oversample = oversample\n        self.target_counting = target_counting\n        \n        self.augmentation_fn = augmentation_fn\n        \n        self.train_ds, self.nb_examples_approx = get_oversampled_training_dataset(\n            self.target_counting, self.update_size, self.shuffle_buffer_size,\n            repeat_dataset=True, ordered=False,\n            oversample=self.oversample, augmentation_fn=self.augmentation_fn,\n            probability=probability\n        )\n\n        self.updates_per_epoch = self.nb_examples_approx \/\/ self.update_size        \n        \n        self.valid_ds = get_validation_dataset(self.prediction_batch_size)\n        self.test_ds = get_test_dataset(self.prediction_batch_size)\n        \n        self.log_interval = log_interval\n         \n    def train(self, train_name, model_name, epochs, start_lr, max_lr, end_lr, warmup, lr_scaling, optimized_loop=False, verbose=False):\n        \n        update_steps = epochs * self.updates_per_epoch\n        warmup_steps = int(update_steps * warmup)\n        \n        model, loss_fn, optimizer, gradient_accumulator, metrics = get_model(model_name, update_steps, warmup_steps, start_lr, max_lr, end_lr, lr_scaling, verbose=verbose)\n        \n        dist_train_1_epoch_optimized, dist_train_1_epoch_normal, dist_predict_step  = get_routines(\n            model, loss_fn, optimizer, gradient_accumulator, metrics, self.batch_size_per_replica, self.update_size, self.grad_acc_steps, self.updates_per_epoch\n        )\n\n        dist_train_1_epoch = dist_train_1_epoch_normal\n        if optimized_loop:\n             dist_train_1_epoch = dist_train_1_epoch_optimized\n        \n        train_fn = get_train_fn(dist_train_1_epoch, dist_predict_step, loss_fn, metrics, log_interval=self.log_interval)\n        history, valid_labels, valid_preds = train_fn(train_name, epochs, self.train_ds, self.valid_ds, self.test_ds, self.updates_per_epoch)\n        \n        return history, valid_labels, valid_preds","6e5741be":"class WarmupLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n\n    def __init__(self, backend_schedule, start_lr, max_lr, end_lr, opt_steps, warmup_steps, lr_scaling):\n\n        self.start_lr = start_lr\n        self.max_lr = max_lr\n        self.end_lr = end_lr\n        self.opt_steps = tf.cast(opt_steps, tf.float32)\n        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n        self.lr_scaling = tf.cast(lr_scaling, tf.float32)\n        self.backend_lr = backend_schedule\n\n        self.warmup_incremental = (self.max_lr - self.start_lr) \/ tf.math.reduce_max([self.warmup_steps, 1.0]) * tf.cast(self.warmup_steps > 0.0, tf.float32)\n\n    def __call__(self, step):\n\n        is_warmup = tf.cast(step < self.warmup_steps, tf.float32)\n        warmup_lr = self.warmup_incremental * step + self.start_lr\n        decay_lr = self.backend_lr(step - self.warmup_steps)\n        lr = (1.0 - is_warmup) * decay_lr + is_warmup * warmup_lr\n\n        return lr * self.lr_scaling","9cce11b4":"def plot_lr_schedule(lr_schedule, n_steps):\n\n    steps = [i for i in range(n_steps)]\n    lrs = [lr_schedule(x) for x in steps]\n    plt.plot(steps, lrs)\n    print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(lrs[0], max(lrs), lrs[-1])) \n    plt.show()\n    \ndef plot_lr_schedule_pair(lr_schedule_1, lr_schedule_2, n_steps):\n    \n    steps = [i for i in range(n_steps)]\n    \n    plt.figure(figsize=(11.0, 6.0 \/ 2))\n    \n    lrs = [lr_schedule_1(x) for x in steps]\n    plt.subplot(1, 2, 1)\n    plt.plot(steps, lrs)\n    plt.title('original lr', fontsize=14, color='black', fontdict={'verticalalignment':'center'}, pad=12.0)\n          \n    lrs = [lr_schedule_2(x) for x in steps]\n    plt.subplot(1, 2, 2)\n    plt.plot(steps, lrs)\n    plt.title('warmup lr', fontsize=14, color='black', fontdict={'verticalalignment':'center'}, pad=12.0)\n    \n    plt.show()\n    \n    \nopt_steps, start_lr, max_lr, end_lr, lr_scaling = 1000, 1e-7, 1e-5, 1e-6, 1\n\nwarmup_steps = 0\nbackend_lr = tf.keras.optimizers.schedules.ExponentialDecay(\n    max_lr, opt_steps - warmup_steps, decay_rate=(end_lr \/ max_lr),\n)\nlr_rate1 = WarmupLearningRateSchedule(backend_lr, start_lr, max_lr, end_lr, opt_steps, warmup_steps, lr_scaling)\n\nwarmup_steps = 0\nbackend_lr = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=max_lr, decay_steps=(opt_steps - warmup_steps), end_learning_rate=end_lr, power=1.0\n)\nlr_rate3 = WarmupLearningRateSchedule(backend_lr, start_lr, max_lr, end_lr, opt_steps, warmup_steps, lr_scaling)","855eb100":"warmup_steps = 200\nbackend_lr = tf.keras.optimizers.schedules.ExponentialDecay(\n    max_lr, opt_steps - warmup_steps, decay_rate=(end_lr \/ max_lr),\n)\nlr_rate2 = WarmupLearningRateSchedule(backend_lr, start_lr, max_lr, end_lr, opt_steps, warmup_steps, lr_scaling)","aeb631ce":"plot_lr_schedule_pair(lr_rate1, lr_rate2, opt_steps)","f3b8d3e2":"warmup_steps = 200\nbackend_lr = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=max_lr, decay_steps=(opt_steps - warmup_steps), end_learning_rate=end_lr, power=1.0\n)\nlr_rate4 = WarmupLearningRateSchedule(backend_lr, start_lr, max_lr, end_lr, opt_steps, warmup_steps, lr_scaling)","1ce10faa":"plot_lr_schedule_pair(lr_rate3, lr_rate4, opt_steps)","eea73934":"def get_model(model_name, update_steps, warmup_steps, start_lr, max_lr, end_lr, lr_scaling, verbose=False):\n\n    with strategy.scope():\n\n        model_class = MODEL_CLASSES[model_name]\n        pretrained_model = model_class(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n        \n        # False = transfer learning, True = fine-tuning\n        pretrained_model.trainable = True \n\n        model = tf.keras.Sequential([\n            pretrained_model,\n            tf.keras.layers.Dropout(rate=0.05),\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dense(len(CLASSES))\n        ])\n        \n        if verbose:\n            model.summary()\n\n        backend_lr = tf.keras.optimizers.schedules.ExponentialDecay(\n            max_lr, opt_steps - warmup_steps, decay_rate=(end_lr \/ max_lr),\n        )\n        lr_rate = WarmupLearningRateSchedule(backend_lr, start_lr, max_lr, end_lr, update_steps, warmup_steps, lr_scaling)\n        \n        # Instiate an optimizer with a learning rate schedule\n        optimizer = tf.keras.optimizers.Adam(lr_rate)\n\n        # Only `NONE` and `SUM` are allowed, and it has to be explicitly specified.\n        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n\n        # Instantiate metrics\n        metrics = {\n            'train loss': tf.keras.metrics.Sum(),\n            'train acc': tf.keras.metrics.SparseCategoricalAccuracy(),\n            'valid acc': tf.keras.metrics.SparseCategoricalAccuracy()\n        }\n        \n        gradient_accumulator = None\n\n        return model, loss_fn, optimizer, gradient_accumulator, metrics","9efd2ebd":"ds = get_training_dataset(batch_size=9, shuffle_buffer_size=1)\ndist_ds = strategy.experimental_distribute_dataset(ds)","3259d3d3":"dist_batch = next(iter(dist_ds))\nprint('The distributed batch is a {} with {} components.'.format(type(dist_batch).__name__, len(dist_batch)))\nprint('\\nThe 1st compoent is a {}'.format(type(dist_batch[0]).__name__))\nprint('The 2nd compoent is a {}'.format(type(dist_batch[1]).__name__))","b67806ac":"dist_batch[1]","ed9ca78e":"print('The values contained inside dist_batch[0] (which is a `{}` object) are packed in a {} with {} components.\\n'.format(type(dist_batch[0]).__name__, type(dist_batch[0].values).__name__, len(dist_batch[0].values)))\nprint('The 1st component in `dist_batch[0].values` is a {} with shape {}'.format(type(dist_batch[0].values[0]).__name__, dist_batch[0].values[0].shape))\nprint('The 4th component in `dist_batch[0].values` is a {} with shape {}'.format(type(dist_batch[0].values[4]).__name__, dist_batch[0].values[4].shape))\nprint('The last component in `dist_batch[0].values` is a {} with shape {}'.format(type(dist_batch[0].values[-1]).__name__, dist_batch[0].values[-1].shape))","21fe9918":"print('The values contained inside dist_batch[1] (which is a `{}` object) are packed in a {} with {} components.\\n'.format(type(dist_batch[1]).__name__, type(dist_batch[1].values).__name__, len(dist_batch[1].values)))\nprint('The first component in `dist_batch[1].values` is a {} with shape {}'.format(type(dist_batch[1].values[0]).__name__, dist_batch[1].values[0].shape))\nprint('The 4th component in `dist_batch[1].values` is a {} with shape {}'.format(type(dist_batch[1].values[4]).__name__, dist_batch[1].values[4].shape))\nprint('The last component in `dist_batch[1].values` is a {} with shape {}'.format(type(dist_batch[1].values[-1]).__name__, dist_batch[1].values[-1].shape))","7beadc30":"@tf.function\ndef dummy_run(images, labels):\n    \n    images = images + 1\n    labels = labels * 0\n    \n    return images, labels\n    \ndummy_images, dummy_labels = strategy.run(dummy_run, args=dist_batch)\ndummy_labels","b5073346":"strategy.experimental_local_results(dummy_labels)","b8cfa631":"tf.concat(strategy.experimental_local_results(dummy_labels), axis=0)","dec928ec":"def get_routines(model, loss_fn, optimizer, gradient_accumulator, metrics, batch_size_per_replica, update_size, grad_acc_steps, updates_per_epoch):\n\n    def train_1_forward_backward(images, labels):\n\n        with tf.GradientTape() as tape:\n\n            logits = model(images, training=True)\n            # Remember that we use the `SUM` reduction when we define the loss object.\n            loss = loss_fn(labels, logits) \/ update_size\n\n        grads = tape.gradient(loss, model.trainable_variables)\n        \n        # shape = [batch_size_per_replica]\n        preds = tf.cast(tf.math.argmax(logits, axis=-1), dtype=tf.int32)\n\n        # update metrics\n        metrics['train loss'].update_state(loss)\n        metrics['train acc'].update_state(labels, logits)\n\n        return grads, preds\n\n    def train_1_update(images, labels):\n        \"\"\"\n        gradient accumulation.\n        \"\"\"\n        \n        accumulated_grads = [tf.zeros_like(var, dtype=tf.float32) for var in model.trainable_variables]\n        \n        # Used for collecting the predictions.\n        preds = tf.zeros_like(labels)\n        \n        for idx in tf.range(grad_acc_steps):\n\n            # Take the 1st `batch_size_per_replica` examples.\n            _images = images[:batch_size_per_replica]\n            _labels = labels[:batch_size_per_replica]\n\n            # Get the gradients\n            grads, _preds = train_1_forward_backward(_images, _labels)\n            preds = tf.concat([preds[batch_size_per_replica:], _preds], axis=0)\n\n            # accumulated the gradients\n            accumulated_grads = [x + y for x, y in zip(accumulated_grads, grads)]\n\n            # Move the leading part to the end, so the shape is not changed.\n            images = tf.concat([images[batch_size_per_replica:], _images], axis=0)\n            labels = tf.concat([labels[batch_size_per_replica:], _labels], axis=0)\n            \n        # Update the model's parameters.\n        optimizer.apply_gradients(zip(accumulated_grads, model.trainable_variables))\n\n        return labels, preds\n        \n    @tf.function\n    def dist_train_step(dist_batch):\n        \n        labels, preds = strategy.run(train_1_update, args=dist_batch)        \n        \n        return labels, preds\n        \n    def dist_train_1_epoch(data_iter):\n        \"\"\"\n        Iterating outside `tf.function`.\n        \"\"\"\n        \n        labels = tf.zeros(shape=[updates_per_epoch * update_size], dtype=tf.int32)\n        preds = tf.zeros(shape=[updates_per_epoch * update_size], dtype=tf.int32)\n        \n        for _ in range(updates_per_epoch):\n            \n            _labels, _preds = dist_train_step(next(data_iter))\n            \n            # these are tuples of tensors\n            _labels = strategy.experimental_local_results(_labels)\n            _preds = strategy.experimental_local_results(_preds)\n            \n            # convert each to a single tensor\n            _labels = tf.concat(_labels, axis=0)\n            _preds = tf.concat(_preds, axis=0)\n            \n            # collect the results\n            labels = tf.concat([labels[update_size:], _labels], axis=0)\n            preds = tf.concat([preds[update_size:], _preds], axis=0)\n            \n        return labels, preds\n        \n    @tf.function\n    def dist_train_1_epoch_optimized(data_iter):\n        \"\"\"\n        Iterating inside `tf.function` to optimized training time.\n        \"\"\"\n\n        labels = tf.zeros(shape=[updates_per_epoch * update_size], dtype=tf.int32)\n        preds = tf.zeros(shape=[updates_per_epoch * update_size], dtype=tf.int32)        \n        \n        for _ in tf.range(updates_per_epoch):\n            \n            _labels, _preds = dist_train_step(next(data_iter))\n            \n            # tuple of tensors\n            _labels = strategy.experimental_local_results(_labels)\n            _preds = strategy.experimental_local_results(_preds)\n            \n            # to a single tensor\n            _labels = tf.concat(_labels, axis=0)\n            _preds = tf.concat(_preds, axis=0)           \n            \n            # collect\n            labels = tf.concat([labels[update_size:], _labels], axis=0)\n            preds = tf.concat([preds[update_size:], _preds], axis=0)\n            \n        return labels, preds\n            \n    def predict_step(images):\n\n        logits = model(images, training=False)\n        return logits\n\n    @tf.function\n    def dist_predict_step(images):\n\n        logits = strategy.run(predict_step, [images])\n        return logits\n\n    return dist_train_1_epoch_optimized, dist_train_1_epoch, dist_predict_step","bd262634":"def save_results(train_name, history, valid_labels, valid_preds, test_idx, test_preds):\n\n    with open(f'history-{train_name}.json', 'w', encoding='UTF-8') as fp:\n        json.dump(history, fp, indent=4, ensure_ascii=False)\n\n    with open(f'valid-labels-{train_name}.json', 'w', encoding='UTF-8') as fp:\n        json.dump(valid_labels, fp, indent=4, ensure_ascii=False)\n\n    with open(f'valid-preds-{train_name}.json', 'w', encoding='UTF-8') as fp:\n        json.dump(valid_preds, fp, ensure_ascii=False)\n\n    with open(f'test-preds-{train_name}.json', 'w', encoding='UTF-8') as fp:\n        json.dump(test_preds, fp, indent=4, ensure_ascii=False)\n        \n    submission = pd.DataFrame(test_idx, columns=['id'])\n    submission['label'] = test_preds\n    submission.to_csv(f'submission-{train_name}.csv', index=False)\n    \ndef print_metrics(history, epochs, log_interval):\n    \n    epoch = len(history) - 1\n    \n    if epoch in [0, epochs-1] or (epoch + 1) % log_interval == 0:\n\n        print('epoch: {}'.format(epoch + 1))\n        print('elapsed: {}\\n'.format(history[epoch]['train timing']))\n\n        print('train loss: {}'.format(history[epoch]['train loss']))\n        print('train acc: {}'.format(history[epoch]['train acc']))                \n        print('train recall: {}'.format(history[epoch]['train recall']))\n        print('train precision: {}'.format(history[epoch]['train precision']))\n        print('train f1: {}\\n'.format(history[epoch]['train f1']))           \n\n        print('valid loss: {}'.format(history[epoch]['valid loss']))\n        print('valid acc: {}'.format(history[epoch]['valid acc']))        \n        print('valid recall: {}'.format(history[epoch]['valid recall']))\n        print('valid precision: {}'.format(history[epoch]['valid precision']))\n        print('valid f1: {}'.format(history[epoch]['valid f1']))\n        \n        print('-' * 40)    ","af28aefc":"def get_train_fn(dist_train_1_epoch, dist_predict_step, loss_fn, metrics, log_interval=1):\n\n    def predict_fn(dist_image_ds):\n\n        all_logits = []\n        for images in dist_image_ds:\n\n            # PerReplica object\n            logits = dist_predict_step(images)\n\n            # Tuple of tensors\n            logits = strategy.experimental_local_results(logits)\n\n            # tf.Tensor\n            logits = tf.concat(logits, axis=0)\n\n            all_logits.append(logits)\n\n        # tf.Tensor\n        logits = tf.concat(all_logits, axis=0)\n        preds = tf.math.argmax(logits, axis=-1)\n\n        return logits, preds\n\n    def valid_fn(dist_image_ds, labels, epoch):\n\n        logits, preds = predict_fn(dist_image_ds)\n\n        loss = loss_fn(labels, logits) \/ NUM_VALIDATION_IMAGES\n\n        # update metrics\n        metrics['valid acc'].update_state(labels, logits)\n\n        # get metrics\n        acc = metrics['valid acc'].result()\n\n        recall = sklearn.metrics.recall_score(labels, preds, average='macro')\n        precision = sklearn.metrics.precision_score(labels, preds, average='macro')\n        f1 = sklearn.metrics.f1_score(labels, preds, average='macro')\n        \n        # reset metrics\n        metrics['valid acc'].reset_states()\n        \n        return {'loss': float(loss), 'acc': float(acc), 'recall': recall, 'precision': precision, 'f1': f1, 'preds': preds}\n    \n    def train_fn(train_name, epochs, train_ds, valid_ds, test_ds, updates_per_epoch):    \n        \n        valid_image_ds = valid_ds.map(lambda image, label: image)   \n        test_image_ds = test_ds.map(lambda image, idx: image)\n    \n        train_dist_ds = strategy.experimental_distribute_dataset(train_ds)        \n        valid_dist_image_ds = strategy.experimental_distribute_dataset(valid_image_ds)\n        test_dist_image_ds = strategy.experimental_distribute_dataset(test_image_ds)\n        \n        train_data_iter = iter(train_dist_ds)\n        \n        valid_label_ds = valid_ds.map(lambda image, label: label)\n        valid_labels = next(iter(valid_label_ds.unbatch().batch(NUM_VALIDATION_IMAGES)))        \n        \n        test_idx_ds = test_ds.map(lambda image, idx: idx)\n        test_idx = next(iter(test_idx_ds.unbatch().batch(NUM_TEST_IMAGES)))\n      \n        history = {}\n        valid_preds = {}\n    \n        for epoch in range(epochs):\n            \n            s = datetime.datetime.now()\n\n            labels, preds = dist_train_1_epoch(train_data_iter)\n\n            # get metrics\n            train_loss = metrics['train loss'].result() \/ updates_per_epoch\n            train_acc = metrics['train acc'].result()\n\n            # reset metrics\n            metrics['train loss'].reset_states()\n            metrics['train acc'].reset_states()\n            \n            recall = sklearn.metrics.recall_score(labels, preds, average='macro')\n            precision = sklearn.metrics.precision_score(labels, preds, average='macro')\n            f1 = sklearn.metrics.f1_score(labels, preds, average='macro')\n            \n            e = datetime.datetime.now()\n            elapsed = (e - s).total_seconds()\n            \n            valid_results = valid_fn(valid_dist_image_ds, valid_labels, epoch)\n            \n            history[epoch] = {\n                'train loss': float(train_loss),\n                'train acc': float(train_acc),\n                'train recall': recall,\n                'train precision': precision,\n                'train f1': f1,                \n                'valid loss': valid_results['loss'],\n                'valid acc': valid_results['acc'],\n                'valid recall': valid_results['recall'],\n                'valid precision': valid_results['precision'],\n                'valid f1': valid_results['f1'],\n                'train timing': elapsed\n            }\n            valid_preds[epoch] = valid_results['preds'].numpy().tolist()\n            \n            print_metrics(history, epochs, log_interval)\n            \n        _, test_preds = predict_fn(test_dist_image_ds)\n        \n        valid_labels = valid_labels.numpy().tolist()\n        test_preds = test_preds.numpy().tolist()\n        test_idx = test_idx.numpy().tolist()\n        \n        save_results(train_name, history, valid_labels, valid_preds, test_idx, test_preds)\n        \n        return history, valid_labels, valid_preds\n                \n    return train_fn","1c785e09":"model_name = 'EfficientNetB7'\n# model_name = 'Xception'\nepochs = 30\nlr_scaling = 8\nlog_interval = 10","fc621b27":"trainer = Flower_Trainer(\n    batch_size_per_replica=16, prediction_batch_size_per_replica=64, shuffle_buffer_size=None,\n    oversample=False, target_counting=1, grad_acc_steps=1, augmentation_fn=None, log_interval=log_interval\n)","b85b281b":"def print_config(trainer):\n\n    print('use oversampling: {}'.format(trainer.oversample))\n    \n    if trainer.oversample:\n        print('target counting of each class for oversampling {}: '.format(trainer.target_counting))    \n    \n    print('(approximated) nb. of training examples used: {}'.format(trainer.nb_examples_approx))\n    \n    print('per replica batch size for training: {}'.format(trainer.batch_size_per_replica))\n    print('batch size for training: {}'.format(trainer.batch_size))    \n    print('gradient accumulation steps: {}'.format(trainer.grad_acc_steps))\n    print('update size: {}'.format(trainer.update_size))\n    print('updates per epoch: {}'.format(trainer.updates_per_epoch))\n    \n    print('per replica batch size for prediction: {}'.format(trainer.prediction_batch_size_per_replica))\n    print('batch size for prediction: {}'.format(trainer.prediction_batch_size))\n\n    print('use data augmentation: {}'.format(trainer.augmentation_fn is not None))","7e4f88ae":"print_config(trainer)","db8586a5":"history_1, valid_labels, valid_preds = trainer.train(train_name='original', model_name=model_name, epochs=epochs, start_lr=1e-5, max_lr=1e-5, end_lr=1e-5, warmup=0.2, lr_scaling=1, optimized_loop=False, verbose=True)","b205a60a":"def plot_history(history, desc=''):\n    \n    fig, _ = plt.subplots(figsize=(25, 14.28))\n    # fig, _ = plt.subplots(figsize=(17.25, 10))\n    # plt.tight_layout()\n\n    if desc:\n        fig.suptitle('{}'.format(desc), fontsize=24, y=0.95)\n    \n    xs = range(1, len(history) + 1)\n\n    subplot = (2, 2, 1)\n    ax = plt.subplot(*subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(xs, [history[k]['train loss'] for k in history])\n    ax.plot(xs, [history[k]['valid loss'] for k in history])\n    ax.set_title('model loss', fontsize=20)\n    ax.set_xlabel('epoch', fontsize=16)\n    ax.set_ylabel('loss', fontsize=16)\n    ax.legend(['train loss', 'valid loss'], fontsize=16)\n\n    subplot = (2, 2, 2)\n    ax = plt.subplot(*subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(xs, [history[k]['train acc'] for k in history])\n    ax.plot(xs, [history[k]['valid acc'] for k in history])\n    ax.set_title('model accuracy', fontsize=20)\n    ax.set_xlabel('epoch', fontsize=16)\n    ax.set_ylabel('accuracy', fontsize=16)\n    ax.legend(['train acc', 'valid acc'], fontsize=16)\n\n    subplot = (2, 2, 3)\n    ax = plt.subplot(*subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(xs, [history[k]['train recall'] for k in history])\n    ax.plot(xs, [history[k]['train precision'] for k in history])\n    ax.plot(xs, [history[k]['train f1'] for k in history])\n    ax.set_title('train - recall, precision, f1', fontsize=20)\n    ax.set_xlabel('epoch', fontsize=16)\n    ax.set_ylabel('train metrics', fontsize=16)\n\n    ax.legend(['train recall', 'train precision', 'train f1'], fontsize=16)\n\n    subplot = (2, 2, 4)\n    ax = plt.subplot(*subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(xs, [history[k]['valid recall'] for k in history])\n    ax.plot(xs, [history[k]['valid precision'] for k in history])\n    ax.plot(xs, [history[k]['valid f1'] for k in history])\n    ax.set_title('valid - recall, precision, f1', fontsize=20)\n    ax.set_xlabel('epoch', fontsize=16)\n    ax.set_ylabel('valid metrics', fontsize=16)    \n    \n    ax.legend(['valid recall', 'valid precision', 'valid f1'], fontsize=16)    \n    \ndef plot_history_pair(history_1, history_2, desc_1='', desc_2='', short_desc_1='history 1', short_desc_2='history 2'):\n    \n    nb_epochs_1 = len(history_1)\n    nb_epochs_2 = len(history_2)\n    \n    # extend by the last epoch\n    if nb_epochs_1 < nb_epochs_2:\n        for epoch in range(nb_epochs_1, nb_epochs_2):\n            history_1[epoch] = history_1[nb_epochs_1 - 1]\n    elif nb_epochs_1 > nb_epochs_2:\n        for epoch in range(nb_epochs_2, nb_epochs_1):\n            history_2[epoch] = history_2[nb_epochs_2 - 1]        \n    \n    fig, _ = plt.subplots(figsize=(25, 14.28))\n    # fig, _ = plt.subplots(figsize=(17.25, 10))\n    # plt.tight_layout()\n    \n    if desc_1 and desc_2:\n        fig.suptitle('{}   vs.   {}'.format(desc_1, desc_2), fontsize=24, y=0.95)\n    elif desc_1:\n        fig.suptitle('{}'.format(desc_1), fontsize=24, y=0.95)\n\n    xs = range(1, len(history_1) + 1)\n\n    subplot = (2, 2, 1)\n    ax = plt.subplot(*subplot)\n    ax.set_facecolor('#F8F8F8')\n\n    ax.plot(xs, [history_2[k]['train loss'] for k in history_2], color='b')\n    ax.plot(xs, [history_2[k]['valid loss'] for k in history_2], color='g')     \n\n    ax.plot([], [], linestyle='--', color='k')     \n    ax.plot([], [], linestyle='-', color='k')       \n    \n    ax.plot(xs, [history_1[k]['train loss'] for k in history_1], linestyle='--', color='b')\n    ax.plot(xs, [history_1[k]['valid loss'] for k in history_1], linestyle='--', color='g')\n    \n    ax.set_title('model loss', fontsize=20)\n    ax.set_xlabel('epoch', fontsize=16)\n    ax.set_ylabel('loss', fontsize=16)\n    ax.legend(['train loss', 'valid loss', short_desc_1, short_desc_2], fontsize=16)\n\n    subplot = (2, 2, 2)\n    ax = plt.subplot(*subplot)\n    ax.set_facecolor('#F8F8F8')\n    \n    ax.plot(xs, [history_2[k]['train acc'] for k in history_2], color='b')\n    ax.plot(xs, [history_2[k]['valid acc'] for k in history_2], color='g')     \n    \n    ax.plot([], [], linestyle='--', color='k')     \n    ax.plot([], [], linestyle='-', color='k')     \n    \n    ax.plot(xs, [history_1[k]['train acc'] for k in history_1], linestyle='--', color='b')\n    ax.plot(xs, [history_1[k]['valid acc'] for k in history_1], linestyle='--', color='g')\n    \n    ax.set_title('model accuracy', fontsize=20)\n    ax.set_xlabel('epoch', fontsize=16)\n    ax.set_ylabel('accuracy', fontsize=16)\n    ax.legend(['train acc', 'valid acc', short_desc_1, short_desc_2], fontsize=16)\n\n    subplot = (2, 2, 3)\n    ax = plt.subplot(*subplot)\n    ax.set_facecolor('#F8F8F8')\n\n    ax.plot(xs, [history_2[k]['train recall'] for k in history_2], color='b')\n    ax.plot(xs, [history_2[k]['train precision'] for k in history_2], color='g')\n    ax.plot(xs, [history_2[k]['train f1'] for k in history_2], color='r')    \n\n    ax.plot([], [], linestyle='--', color='k')     \n    ax.plot([], [], linestyle='-', color='k')     \n    \n    ax.plot(xs, [history_1[k]['train recall'] for k in history_1], linestyle='--', color='b')\n    ax.plot(xs, [history_1[k]['train precision'] for k in history_1], linestyle='--', color='g')\n    ax.plot(xs, [history_1[k]['train f1'] for k in history_1], linestyle='--', color='r')\n    \n    ax.set_title('train - recall, precision, f1', fontsize=20)\n    ax.set_xlabel('epoch', fontsize=16)\n    ax.set_ylabel('train metrics', fontsize=16)\n\n    ax.legend(['train recall', 'train precision', 'train f1', short_desc_1, short_desc_2], fontsize=16)\n    \n    subplot = (2, 2, 4)\n    ax = plt.subplot(*subplot)\n    ax.set_facecolor('#F8F8F8')\n    \n    ax.plot(xs, [history_2[k]['valid recall'] for k in history_2], color='b')\n    ax.plot(xs, [history_2[k]['valid precision'] for k in history_2], color='g')\n    ax.plot(xs, [history_2[k]['valid f1'] for k in history_2], color='r')    \n\n    ax.plot([], [], linestyle='--', color='k')     \n    ax.plot([], [], linestyle='-', color='k')      \n    \n    ax.plot(xs, [history_1[k]['valid recall'] for k in history_1], linestyle='--', color='b')\n    ax.plot(xs, [history_1[k]['valid precision'] for k in history_1], linestyle='--', color='g')\n    ax.plot(xs, [history_1[k]['valid f1'] for k in history_1], linestyle='--', color='r')\n    \n    ax.set_title('valid - recall, precision, f1', fontsize=20)\n    ax.set_xlabel('epoch', fontsize=16)\n    ax.set_ylabel('valid metrics', fontsize=16)\n\n    ax.legend(['valid recall', 'valid precision', 'valid f1', short_desc_1, short_desc_2], fontsize=16)","37f110c3":"plot_history(history_1, desc='original training')","f2848e30":"history_2, valid_labels, valid_preds = trainer.train(train_name='optimized loop', model_name=model_name, epochs=epochs, start_lr=1e-5, max_lr=1e-5, end_lr=1e-5, warmup=0.2, lr_scaling=1, optimized_loop=True, verbose=False)","828cc1f5":"def compare_training_time(history1, history2, title1, title2):\n\n    avg1 = sum([history1[k]['train timing'] for k in history1 if k != 0]) \/ (len(history1) - 1)\n    avg2 = sum([history2[k]['train timing'] for k in history2 if k != 0]) \/ (len(history2) - 1)\n\n    print('Training time per epoch\\n')\n    print('  for the 1st epoch')\n    print(f'    {title1}: {history1[0][\"train timing\"]}')\n    print(f'    {title2}: {history2[0][\"train timing\"]}\\n')\n    print('  for the remaining epoch')\n    print(f'    {title1}: {avg1}')\n    print(f'    {title2}: {avg2}')","3dc8009b":"compare_training_time(history_1, history_2, 'usual training loop', 'optimized training loop')","a818dcb1":"plot_history_pair(history_1, history_2, desc_1='[usual training loop]', desc_2='[optimized training loop]', short_desc_1='usual loop', short_desc_2='optimized loop')","1a6dbef2":"trainer = Flower_Trainer(\n    batch_size_per_replica=16, prediction_batch_size_per_replica=64, shuffle_buffer_size=None,\n    oversample=False, target_counting=1, grad_acc_steps=8, augmentation_fn=None, log_interval=log_interval\n)","76e144ee":"print_config(trainer)","f767e032":"history_3, valid_labels, valid_preds = trainer.train(train_name='grad. accumulation 8', model_name=model_name, epochs=epochs, start_lr=1e-5, max_lr=1e-5, end_lr=1e-5, warmup=0.2, lr_scaling=1, optimized_loop=True, verbose=False)","cd7e6c4c":"compare_training_time(history_2, history_3, 'no gradient accumulation', 'gradient accumulation steps 8')","242475de":"plot_history_pair(history_2, history_3, desc_1='[no gradient accumulation]', desc_2='[gradient accumulation steps 8]', short_desc_1='no grad. accumulation', short_desc_2='grad. accumulation 8')","c7f6fcef":"history_4, valid_labels, valid_preds = trainer.train(train_name='grad. accumulation + lr. x8', model_name=model_name, epochs=epochs, start_lr=1e-5, max_lr=1e-5, end_lr=1e-5, warmup=0.2, lr_scaling=lr_scaling, optimized_loop=True, verbose=False)","4ed2084b":"plot_history_pair(history_3, history_4, desc_1='lr scaling 1', desc_2='lr scaling 8', short_desc_1='grad. accumulation lr. x1', short_desc_2='grad. accumulation lr. x8')","b94af3b1":"plot_history_pair(history_2, history_4, desc_1='[no gradient accumulation]', desc_2='[gradient accumulation steps 8 + lr. x8]', short_desc_1='no. grad. accumulation', short_desc_2='grad. accumulation lr. x8')","30b3d45c":"trainer = Flower_Trainer(\n    batch_size_per_replica=16, prediction_batch_size_per_replica=64, shuffle_buffer_size=None,\n    oversample=True, target_counting=100, grad_acc_steps=8, augmentation_fn=None, log_interval=log_interval\n)","d701ccf7":"print_config(trainer)","d6a64a83":"history_5, valid_labels, valid_preds = trainer.train(train_name='oversampling N=100', model_name=model_name, epochs=epochs, start_lr=1e-5, max_lr=1e-5, end_lr=1e-5, warmup=0.2, lr_scaling=lr_scaling, optimized_loop=True, verbose=False)","1ad135e3":"plot_history_pair(history_4, history_5, desc_1='[no oversampling]', desc_2='[oversampling N=100]', short_desc_1='no oversampling', short_desc_2='oversamp. N=100')","f9cefb99":"trainer = Flower_Trainer(\n    batch_size_per_replica=16, prediction_batch_size_per_replica=64, shuffle_buffer_size=None,\n    oversample=True, target_counting=300, grad_acc_steps=8, augmentation_fn=None, log_interval=log_interval\n)","b8bf819e":"print_config(trainer)","b9131535":"history_6, valid_labels, valid_preds = trainer.train(train_name='oversampling N=300', model_name=model_name, epochs=epochs, start_lr=1e-5, max_lr=1e-5, end_lr=1e-5, warmup=0.2, lr_scaling=lr_scaling, optimized_loop=True, verbose=False)","f5285e7f":"plot_history_pair(history_5, history_6, desc_1='[oversampling N=100]', desc_2='[oversampling N=300]', short_desc_1='oversamp. N=100', short_desc_2='oversamp. N=300')","6617ca53":"plot_history_pair(history_4, history_6, desc_1='[no oversampling]', desc_2='[oversampling N=300]', short_desc_1='no oversampling', short_desc_2='oversamp. N=300')","ee138105":"epochs_reduced = int(round(epochs \/ (trainer.nb_examples_approx \/ ORIGINAL_NUM_TRAINING_IMAGES)))\nhistory_7, valid_labels, valid_preds = trainer.train(train_name='oversampling 300 + epochs {}'.format(epochs_reduced), model_name=model_name, epochs=epochs_reduced, start_lr=1e-5, max_lr=1e-5, end_lr=1e-5, warmup=0.2, lr_scaling=lr_scaling, optimized_loop=True, verbose=False)","5b2f3876":"plot_history_pair(history_4, history_7, desc_1=f'[no oversampling + {epochs} epochs]', desc_2=f'[oversampling 300 + {epochs_reduced} epochs]', short_desc_1='no oversampling', short_desc_2=f'oversamp. epoch {epochs_reduced}')","56e3c091":"import IPython\nIPython.display.IFrame(\"\/\/jsfiddle.net\/dFrHS\/1\/embedded\/result,js,html,css\", width=700, height=500)","ffb87bd7":"def random_4_points_2D_batch(height, width, batch_size, probability=1.0):\n    \"\"\"Generate `batch_size * 4` random 2-D points.\n    \n    Each 4 points are inside a rectangle with the same center as the above rectangle\n    but with side length being approximately 1.5 times. This choice is to avoid the\n    image being transformed too disruptively.\n\n    Each point is created first by making it close to the corresponding corner points\n    determined by the rectangle, i.e [0, 0], [0, width], [height, width] and [height, 0]\n    respectively. Then the 4 points are randomly shifted module 4 and randomly flipped.\n    \n    Args:\n        height: 0-D tensor, height of a reference rectangle.\n        width: 0-D tensor, width of a reference rectangle.\n        batch_size: 0-D tensor, the number of 4 points to be generated.\n        probability: 0-D tensor, the probability to use perspective transformation.\n        \n    Returns:\n        points: 3-D tensor of shape [batch_size, 4, 2]\n    \"\"\"\n\n    probability = tf.constant(probability, dtype=tf.float32)\n    \n    sy = height \/\/ 4\n    sx = width \/\/ 4\n        \n    h, w = height, width\n    \n    # Each has shape [batch_size]\n    y1 = tf.random.uniform(minval = -sy, maxval = sy, shape=[batch_size], dtype=tf.int32)\n    x1 = tf.random.uniform(minval = -sx, maxval = sx, shape=[batch_size], dtype=tf.int32)\n\n    y2 = tf.random.uniform(minval = -sy, maxval = sy, shape=[batch_size], dtype=tf.int32)\n    x2 = tf.random.uniform(minval = 3 * sx, maxval = 5 * sx, shape=[batch_size], dtype=tf.int32)\n\n    y3 = tf.random.uniform(minval = 3 * sy, maxval = 5 * sy, shape=[batch_size], dtype=tf.int32)\n    x3 = tf.random.uniform(minval = 3 * sx, maxval = 5 * sx, shape=[batch_size], dtype=tf.int32)    \n\n    y4 = tf.random.uniform(minval = 3 * sy, maxval = 5 * sy, shape=[batch_size], dtype=tf.int32)\n    x4 = tf.random.uniform(minval = -sx, maxval = sx, shape=[batch_size], dtype=tf.int32)\n        \n    # shape = [4, 2, batch_size]\n    _points = tf.convert_to_tensor([[y1, x1], [y2, x2], [y3, x3], [y4, x4]])\n    \n    # shape = [batch_size, 4, 2]\n    #     Each _points[i, :, :] consists of 4 points\n    #         [y1, x1], [y2, x2], [y3, x3], [y4, x4],\n    #     with xj, yj as scalars this time.\n    _points = tf.transpose(_points, perm=[2, 0, 1])\n    \n    # shape = [4, 2]\n    _standard_points = tf.constant([[0, 0], [0, width], [height, width], [height, 0]], dtype=tf.int32)\n    # shape = [batch_size, 4, 2]\n    _standard_points = tf.broadcast_to(_standard_points[tf.newaxis, :, :], shape=[batch_size, 4, 2])\n    \n    # If to use perspective transformation\n    # shape = [batch_size, 1, 1]\n    do_perspective = tf.cast(tf.random.uniform(shape=[batch_size]) < probability, dtype=tf.int32)[:, tf.newaxis, tf.newaxis]\n    \n    # shape = [batch_size, 4, 2]\n    _points = do_perspective * _points + (1 - do_perspective) * _standard_points\n    \n    # ----------------------------------------\n    # Trick to get random rotations\n    \n    # shape = [batch_size]\n    # shift degree\n    shift_degree = tf.random.uniform(minval=0, maxval=4, shape=[batch_size], dtype=tf.int32)\n    \n    # shape = [batch_size, 4]\n    # Each `_indices[i, :]` is [0, 1, 2, 3] + a random integer in [0, 1, 2, 3]\n    # This shifts the indices of the 4 points, which corresponds to a rotation.\n    _indices = shift_degree[:, tf.newaxis] + tf.range(4, dtype=tf.int32)[tf.newaxis, :]\n    _indices = tf.math.floormod(_indices, 4)\n\n    # shape = [batch_size, 4, 2]\n    # Each `indices[i, :, 0]` is [i, i, i, i]\n    # Each `indices[i, :, :]` is [[i, k1], [i, k2] [i, k3], [i, k4]] where [k0, k1, k2, k3] is `_indices[i]`.\n    _indices = tf.stack(\n        [\n            tf.broadcast_to(\n                tf.range(batch_size, dtype=tf.int32)[:, tf.newaxis],\n                shape=[batch_size, 4]\n            ),\n            _indices\n        ],\n        axis=2\n    )\n\n    # Obtain a tensor `new _points` of shape [batch_size, 4, 2], where\n    # `new _points[i, j] = _points[indices[i, j]]`\n    \n    # shape = [batch_size, 4, 2]\n    _points = tf.gather_nd(_points, _indices)\n      \n    # ----------------------------------------\n    # Trick to get random reflections\n    \n    # All has shape [4]\n    # no reflection\n    reflection_0 = tf.constant([0, 1, 2, 3], dtype=tf.int32)\n    # flip up\/down\n    reflection_1 = tf.constant([3, 2, 1, 0], dtype=tf.int32)\n    # flip left\/right\n    reflection_2 = tf.constant([1, 0, 3, 2], dtype=tf.int32)\n    \n    # shape = [3, 4]\n    reflections = tf.stack([reflection_0, reflection_1, reflection_2], axis=0)\n    \n    # shape = [batch_size, 3]\n    reflection_types = tf.cast(\n        tf.one_hot(\n            tf.random.uniform(\n                minval=0, maxval=3, shape=[batch_size], dtype=tf.int32\n            ),\n            3\n        ),\n        dtype=tf.int32\n    )\n\n    # shape = [batch_size, 4]\n    selected_reflections = tf.linalg.matmul(reflection_types, reflections)\n        \n    # shape = [batch_size, 4, 2]\n    _indices = tf.stack(\n        [\n            tf.broadcast_to(\n                tf.range(batch_size, dtype=tf.int32)[:, tf.newaxis],\n                shape=[batch_size, 4]\n            ),\n            selected_reflections\n        ],\n        axis=2\n    )\n            \n    # shape = [batch_size, 4, 2]\n    _points = tf.gather_nd(_points, _indices)\n    \n    # ----------------------------------------\n    \n    return _points\n\n\ndef random_4_point_transform_2D_batch(images, probability=1.0):\n    \"\"\"Apply 4 point transformation on 2-D images `images` with randomly\n       generated 4 points on target spaces.\n    \n    On source space, the 4 points are the corner points, i.e [0, 0], [0, width],\n    [height, width] and [height, 0].\n    \n    On target space, the 4 points are randomly generated by `random_4_points_2D_batch()`.\n    \"\"\"\n\n    batch_size, height, width = images.shape[:3]\n\n    # 4 corner points in source image\n    # shape = [batch_size, 4, 2]\n    src_pts = tf.convert_to_tensor([[0, 0], [0, width], [height, width], [height, 0]])\n    src_pts = tf.broadcast_to(src_pts, shape=[batch_size, 4, 2])\n\n    # 4 points in target image\n    # shape = [batch_size, 4, 2]\n    tgt_pts = random_4_points_2D_batch(height, width, batch_size, probability=probability)\n    \n    tgt_images = four_point_transform_2D_batch(images, src_pts, tgt_pts)\n\n    return tgt_images\n\n\ndef four_point_transform_2D_batch(images, src_pts, tgt_pts):\n    \"\"\"Apply 4 point transformation determined by `src_pts` and `tgt_pts`\n       on 2-D images `images`.\n    \n    Args:\n        images: 3-D tensor of shape [batch_size, height, width], or 4-D tensor\n            of shape [batch_size, height, width, channels]\n        src_pts: 3-D tensor of shape [batch_size, 4, 2]\n        tgt_pts: 3-D tensor of shape [batch_size, 4, 2]\n        \n    Returns:\n        A tensor with the same shape as `images`.\n    \"\"\"\n    \n    src_to_tgt_mat = get_src_to_tgt_mat_2D_batch(src_pts, tgt_pts)\n    \n    tgt_images = transform_by_perspective_matrix_2D_batch(images, src_to_tgt_mat)\n    \n    return tgt_images\n\n\ndef transform_by_perspective_matrix_2D_batch(images, src_to_tgt_mat):\n    \"\"\"Transform 2-D images by prespective transformation matrices\n    \n    Args:\n        images: 3-D tensor of shape [batch_size, height, width], or 4-D tensor of\n            shape [batch_size, height, width, channels]\n        src_to_tgt_mat: 3-D tensor of shape [batch_size, 3, 3]. This is the\n            transformation matrix mapping the source space to the target space.\n        \n    Returns:\n        A tensor with the same shape as `image`.        \n    \"\"\"\n\n    batch_size, height, width = images.shape[:3]\n\n    # shape = (3, 3)\n    tgt_to_src_mat = tf.linalg.inv(src_to_tgt_mat)\n        \n    # prepare y coordinates\n    # shape = [height * width]\n    ys = tf.repeat(tf.range(height), width) \n    \n    # prepare x coordinates\n    # shape = [height * width]\n    xs = tf.tile(tf.range(width), [height])\n\n    # prepare indices in target space\n    # shape = [2, height * width]\n    tgt_indices = tf.stack([ys, xs], axis=0)\n    \n    # Change to projective coordinates in the target space by adding ones\n    # shape = [3, height * width]\n    tgt_indices_homo = tf.concat([tgt_indices, tf.ones(shape=[1, height * width], dtype=tf.int32)], axis=0)\n    \n    # Get the corresponding projective coordinate in the source space\n    # shape = [batch_size, 3, height * width]\n    src_indices_homo = tf.linalg.matmul(tgt_to_src_mat, tf.cast(tgt_indices_homo, dtype=tf.float64))\n    \n    # normalize the projective coordinates\n    # shape = [batch_size, 3, height * width]\n    src_indices_normalized = src_indices_homo[:, :3, :] \/ src_indices_homo[:, 2:, :]\n    \n    # Get the affine coordinate by removing ones\n    # shape = [batch_size, 2, height * width]\n    src_indices_affine = tf.cast(src_indices_normalized, dtype=tf.int32)[:, :2, :]\n    \n    # Mask the points outside the range\n    # shape = [batch_size, height * width]\n    y_mask = tf.logical_and(src_indices_affine[:, 0] >= 0, src_indices_affine[:, 0] <= height - 1)\n    x_mask = tf.logical_and(src_indices_affine[:, 1] >= 0, src_indices_affine[:, 1] <= width - 1)\n    mask = tf.logical_and(y_mask, x_mask)\n    \n    # clip the coordinates\n    # shape = [batch_size, 2, height * width]\n    src_indices = tf.clip_by_value(src_indices_affine, clip_value_min=0, clip_value_max=[[height - 1], [width - 1]])\n    \n    # Get a collection of (y_coord, x_coord)\n    # shape = [batch_size, height * width, 2]\n    src_indices = tf.transpose(src_indices, perm=[0, 2, 1])\n    \n    # shape = [batch_size, height * width, channels]\n    tgt_images = tf.gather_nd(images, src_indices, batch_dims=1)\n    \n    # Set pixel to 0 by using the mask\n    tgt_images = tgt_images * tf.cast(mask[:, :, tf.newaxis], tf.float32)\n    \n    # reshape to [height, width, channels]\n    tgt_images = tf.reshape(tgt_images, images.shape)\n\n    return tgt_images\n\n\ndef get_src_to_tgt_mat_2D_batch(src_pts, tgt_pts):\n    \"\"\"Get the perspective transformation matrix from the source space to the target space,\n       which maps the 4 source points to the 4 target points.\n    \n    Args:\n        src_pts: 3-D tensor of shape [batch_size, 4, 2]\n        tgt_pts: 3-D tensor of shape [batch_size, 4, 2]\n        \n    Returns:\n        3-D tensor of shape [batch_size, 3, 3]\n    \"\"\"\n    \n    src_pts = tf.cast(src_pts, tf.int32)\n    tgt_pts = tf.cast(tgt_pts, tf.int32)\n    \n    # The perspective transformation matrix mapping basis vectors and (1, 1, 1) to `src_pts`\n    # shape = [batch_size, 3, 3]\n    src_mat = get_transformation_mat_2D_batch(src_pts)\n    \n    # The perspective transformation matrix mapping basis vectors and (1, 1, 1) to `tgt_pts`\n    # shape = [batch_size, 3, 3]\n    tgt_mat = get_transformation_mat_2D_batch(tgt_pts)\n    \n    # The perspective transformation matrix mapping `src_pts` to `tgt_pts`\n    # shape = [batch_size, 3, 3]\n    src_to_tgt_mat = tf.linalg.matmul(tgt_mat, tf.linalg.inv(src_mat))\n    \n    return src_to_tgt_mat\n  \n    \ndef get_transformation_mat_2D_batch(four_pts):\n    \"\"\"Get the perspective transformation matrix from a space to another space,\n       which maps the basis vectors and (1, 1, 1) to the 4 points defined by `four_pts`.\n    \n    Args:\n        four_pts: 3-D tensor of shape [batch_size, 4, 2]\n        \n    Returns:\n        3-D tensor of shape [batch_size, 3, 3]        \n    \"\"\"\n    \n    batch_size = four_pts.shape[0]\n    \n    # Change to projective coordinates by adding ones\n    # shape = [batch_size, 3, 4]\n    pts_homo = tf.transpose(tf.concat([four_pts, tf.ones(shape=[batch_size, 4, 1], dtype=tf.int32)], axis=-1), perm=[0, 2, 1])\n    \n    pts_homo = tf.cast(pts_homo, tf.float64)\n    \n    # Find `scalars` such that: src_pts_homo[:, 3:] * scalars == src_pts_homo[:, 3:]\n    # shape = [batch_size 3, 3]\n    inv_mat = tf.linalg.inv(pts_homo[:, :, :3])\n    # shape = [batch_size, 3, 1]\n    scalars = tf.linalg.matmul(inv_mat, pts_homo[:, :, 3:])\n    \n    # Get the matrix transforming unit vectors to the 4 source points\n    # shape = [batch_size, 3, 3]    \n    mat = tf.transpose(tf.transpose(pts_homo[:, :, :3], perm=[0, 2, 1]) * scalars, perm=[0, 2, 1])\n    \n    return mat\n\ndef perspective_transform(images, labels, probability=1.0):\n    \"\"\"\n    This is the method used for dataset transformation (random perspective transformation on images).\n    \"\"\"\n        \n    transformed_images = random_4_point_transform_2D_batch(images, probability=probability)\n    \n    return transformed_images, labels","7420fe2e":"train_dataset = get_training_dataset(batch_size=16, shuffle_buffer_size=1, ordered=True)\ntrain_iter = iter(train_dataset)\n\ntransformed_train_dataset = train_dataset.map(lambda images, labels: perspective_transform(images, labels, probability=0.8))\ntransformed_train_iter = iter(transformed_train_dataset)","10af6ccb":"# run this cell again for next set of images\nbatch = next(train_iter)\ntransformed_batch = next(transformed_train_iter)\n\ndisplay_pairs_of_image_batch(batch, transformed_batch, ds_name_1='original dataset', ds_name_2='transformed dataset')","3a8c61dd":"trainer = Flower_Trainer(\n    batch_size_per_replica=16, prediction_batch_size_per_replica=64, shuffle_buffer_size=None,\n    oversample=True, target_counting=300, grad_acc_steps=8, augmentation_fn=perspective_transform,\n    probability=0.35, log_interval=log_interval\n)","636a8164":"history_8, valid_labels, valid_preds = trainer.train(train_name='with data augmentation', model_name=model_name, epochs=epochs, start_lr=1e-5, max_lr=1e-5, end_lr=1e-5, warmup=0.2, lr_scaling=lr_scaling, optimized_loop=True, verbose=False)","df086ce3":"plot_history_pair(history_6, history_8, desc_1=f'[without data augmentation]', desc_2=f'[with data augmentation]', short_desc_1='no data aug.', short_desc_2=f'data aug.')","fdc6106b":"print('results of the last epoch\\n')\n\nprint('    without data augmentation:\\n')\n\nprint('        valid recall:', history_6[len(history_6) - 1]['valid recall'])\nprint('        valid precision:', history_6[len(history_6) - 1]['valid precision'])\nprint('        valid f1:', history_6[len(history_6) - 1]['valid f1'])\n\nprint('\\n    with data augmentation:\\n')\n\nprint('        valid recall:', history_8[len(history_8) - 1]['valid recall'])\nprint('        valid precision:', history_8[len(history_8) - 1]['valid precision'])\nprint('        valid f1:', history_8[len(history_8) - 1]['valid f1'])","bd17ac57":"### Train with oversampled dataset\n\nLet's see what's the impact of oversampling.","e75d22d9":"# TPU or GPU detection<a id='hardware-detection'><\/a>\n\nIn order to use `TPU`, we use `TPUClusterResolver` for some initialization which is necessary to connect to the remote cluster and initialize cloud TPUs.\n\n1. When using TPU on Kaggle, you don't need to specify arguments for `TPUClusterResolver`.\n2. However, on Google Compute Engine, you need to do things like\n```\n    # The name you gave to the TPU to use\n    TPU_WORKER = 'my-tpu-name'\n    # or you can also specify the grpc path directly\n    # TPU_WORKER = 'grpc:\/\/xxx.xxx.xxx.xxx:8470'\n    \n    # The zone you chose when you created the TPU to use on GCP.\n    ZONE = 'europe-west4-a'\n    \n    # The name of the GCP project where you created the TPU to use on GCP.\n    PROJECT = 'my-tpu-project'\n    \n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_WORKER, zone=ZONE, project=PROJECT)\n```\n\nAlthough the tf documentation says it is the project name for the argument `project`, it is actually the `Project ID` you should specify when you check on the GCP project dashboard page.\n\n**References**:\n\n1. [Guide - Use TPUs](https:\/\/www.tensorflow.org\/guide\/tpu#tpu_initialization)\n\n2. [Doc - TPUClusterResolver](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/cluster_resolver\/TPUClusterResolver)","6c962c65":"# Conclusion<a id='conclusion'><\/a>","2d2e928b":"It is very clear that the oversampled training dataset we just created is super balanced. The argument `target_counting` is set to $782$, and in the oversampled datasets, the number of occurrences for all classes is inside the range from $750$ to $800$ with a mean very close to $782$.\n\nIt is also important to keep in mind that, althoug we have created a super balanced training dataset, the label distribution is now much more different from the label distribution in the validation dataset.","cdbc0f73":"#### Remark","def148b9":"### Plot label distribution","91b69aaf":"# Oversampling<a id='oversampling'><\/a>\n\nHere are 2 common techniques for dealing with imbalanced data:\n\n* Class weights\n    \n    This makes the classifier weight more on those examples with labels in the minority classes. \n\n\n* Oversampling\n\n   The idea is to extend the dataset by repeating the examples in the minority classes.\n   \nHere is a quote from [Classification on imbalanced data](https:\/\/www.tensorflow.org\/tutorials\/structured_data\/imbalanced_data):\n  \n> If the training process were considering the whole dataset on each gradient update, this oversampling would be basically identical to the class weighting.  \n> But when training the model batch-wise, as you did here, the oversampled data provides a smoother gradient   signal: Instead of each positive example being shown in one batch with a large weight, they're shown in many different batches each time with a small weight.\n\nIn this notebook, we decide to focus on applying oversampling and compare it to training without oversampling.\nThe comparison between class weights and oversampling could be found in the reference.\n\n**References:** \n\n1. [Tutorial - Classification on imbalanced data](https:\/\/www.tensorflow.org\/tutorials\/structured_data\/imbalanced_data)","7e5321e3":"With learning rate scaled by $8$ while using a gradient accumulation stpes $8$, the results on the validation dataset are better than not using gradient accumulation. From now on, we will use this training configuration.","66a122a9":"With learning rate scaled by $8$ while using a gradient accumulation stpes $8$, we have much better results than no learning rate scaling.","b959a5ad":"### Train with data augmentation","4e897262":"## Create a trainer to make the experminents easier\n\nThe next cell has nothing related to tensorflow \/ TPU. It just contains some stuffs that make our experminents and presentation easier. The actual model definition and training process are defined in the subsequent cells.\n\nYou can check it if you want to know how our training works in a high level.","11e5d4ec":"#### Plot history","e4ae2c80":"### Collect the return values<a id='collecting'><\/a>\n\nThe results of [strategy.run](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/Strategy#run) are also \ndistributed values, just like distributed batches as its inputs. For each return value, we can use [strategy.experimental_local_results](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/Strategy#experimental_local_results) to obtain a tuple of tensors from all replicas, and use [tf.concat](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/concat) to aggregate them into a single tensor.\nWe use this method to collect the labels and model predictions during training, and use them to calculate different metrics after each epoch.\n\n**References**\n1. [Doc - strategy.experimental_local_results](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/Strategy#experimental_local_results)","6cd44262":"# Training<a id='training'><\/a>\n\nWith all the efforts made so far, the training is very easy!","f8c3eb47":"#### Info about a test dataset","9a38352f":"## Working with tf.data.Dataset<a id='working-with-datasets'><\/a>\n\nWith the above parsing methods defined, we can define how to load the dataset with more options and further apply shuffling, bacthing, etc. In particular:\n\n\n* Use `num_parallel_reads` in `tf.data.TFRecordDataset` to read files in parallel.\n* Set `tf.data.Options.experimental_deterministic=False` and use it to get a new dataset that ignores the order of elements.\n* Use `num_parallel_calls` in `tf.data.Dataset.map()` method to have parallel processing.\n* Use `tf.data.Dataset.prefetch()` to allow later batches to be prepared while the current batch is being processed.\n\nThe parallel processing and prefetching are particular important when working with TPU. Since TPU can process batches very quickly, the dataset pipeline should be able to provide data for TPU efficiently, otherwise the TPU will be idle.\n\n**References**:\n1. [Guide - tf.data: Build TensorFlow input pipelines](https:\/\/www.tensorflow.org\/guide\/data)\n2. [Guide - Better performance with the tf.data API](https:\/\/www.tensorflow.org\/guide\/data_performance)\n3. [Doc - Dataset](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset)","004a19e3":"#### no oversampling vs. oversampling $N=300$","d0dad44b":"## Step 1: Review the number of occurrence of each class\n\nWe use the counter `train_counter` computed in the [Label distribution](#label-distribution) subsection.","c5b679b8":"### Linear learning rate with warmup\n\nWarmup is commonly used in learning rate schedule where we start training a model with a much smaller learning rate and increase it during the first few epochs\/steps until the initial learning rate is used.\n\nIntuitively, it allows a model to adjust itself less before it becomes more familiar with the dataset. For adaptive optimisers like Adam, warmup also allows the optimizers to compute bettere statistics of the gradients.\n\nHere we present a very simple way that turns any learnning rate schedule without warmup into a version that uses warmup. This is only for educational purpose, and we will use a constant learning rate later in this notebook.","d0a63e43":"### Shape invariance\n\nThere are a few subtleties when working with TPUs: it requires the shapes of tensors to be fixed inside a loop. Also, the indices used for a tensor slice have to be compile-time constant.\n\nHowever, when we aggregate the results, their shapes increase along the batch dimension. In order to overcome these restriction, we use the following trick:\n\n```\n    # To keep the shape invariant\n    preds = tf.zeros(shape=[... predefined known shape ...])\n\n    for _ in tf.range(...):\n\n        # results from distributed computation\n        _preds = dist_train_step(next(data_iter))\n        \n        # these are tuples of tensors\n        _preds = strategy.experimental_local_results(_preds)\n        \n        # convert each to a single tensor\n        _preds = tf.concat(_preds, axis=0)\n        \n        # collect the results\n        preds = tf.concat([preds[update_size:], _preds], axis=0)\n        \n    return preds\n```\nBasically, it just puts the results `_preds` at the end of `preds` each time, and discards a few elements at the beginning of `pred` to keep the shape invariant. This trick is also used for the gradient accumulation implemented in `train_1_update`, but operated on the input batches instead of the output batches.","079f147c":"#### print configuration ","d1edbc4f":"#### Train with gradient accumulation steps 8 + learning rate scaling 8","2de66143":"#### no oversampling vs. oversampling $N=300$ with fewer epochs","401ff421":"# Define the model and training process<a id='define-model'><\/a>\n\nAfter the input pipeline is defined, we are ready to see how to create models and how to train them using TPU.","3192d4d4":"#### A remark about using tf.lookup.StaticHashTable\n\nHere we use `tf.lookup.StaticHashTable` instead of a python dictionary because the dataset transformations involve tensorflow tensors which are not hashable.","0647e2a9":"## Number of dataset examples","05aa21e6":"For a batch in the distribute dataset, we also have a tuple of 2 compoents as in the original dataset, but each component is a `PerReplica` objeect rather than a tensor.","0d12ac6e":"## Implement perspective transformation in TensorFlow (batch)<a id='imp-pers-trans'><\/a>","b89bb01f":"#### oversampling $N=100$ vs. oversampling $N=300$","0eed1dc4":"From the above figures, we see the images do repeat in the oversampled dataset. Since we set `ordered=True` when we created the previous datasets, there is no shuffling. Therefore, the oversampled dataset often has a lot of same images in the same batch. When we create the actual training dataset, we will shuffle it and the same images will usually appear in different batches.\n\nHowever, even with shuffling, having exactly the same images appear a lot of times is still not desired, because it is equivalent to training with more epochs, and a model will be very likely overfitted.\n\nIn image classification tasks, data augmentation is a natural approach and a commn technique to improve results and avoid overfitting. With data augmentation, the same images will be transformed to different (but related) images, which is an important step to take, in particular for an oversampled dataset. We will discuss this later in section [Data augmentation - Perspective transformation](#perspective-transformation).","ab210cf4":"We can aggregate the results into a single tensor by using [tf.concat](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/concat)","a68a8ecc":"From the above label distribution plots for training and validation datasets, we see that the distributions are almost identical. However, the labels are very imbalanced. This leads to the next section - Oversampling - to deal with such case.","c6699eff":"## Parse the raw dataset<a id='parse-dataset'><\/a>\n\nThe general recipe to parse the string tensors in the raw dataset is like:\n\n1. Create a description of the features. For example:\n\n```\n    feature_description = {    \n        'feature0': tf.io.FixedLenFeature([], tf.int64),\n        'feature1': tf.io.FixedLenFeature([], tf.string),\n        'feature2': tf.io.FixedLenFeature([], tf.float32),\n        ...\n    }\n```\n\n2. Define a parsing function by using `tf.io.parse_single_example` and the defined feature description.\n```\n    def _parse_function(example):\n        \"\"\"\n        Args:\n            example: A string tensor representing a `tf.train.Example`.\n        \"\"\"\n\n        # Parse `example`.\n        parsed_example = tf.io.parse_single_example(example, feature_description)\n        \n        return parsed_example\n```\n\n3. Map the raw dataset by `_parse_function`.\n```\ndataset = raw_dataset.map(_parse_function)\n```\n\nIn the following cell, we apply the above recipe to our flower classification datasets.\nThe parsed images are `tf.string`, which are then decoded with `tf.image.decode_jpeg`.\n\n**References**:\n1. [Tutorial - TFRecord and tf.Example](https:\/\/www.tensorflow.org\/tutorials\/load_data\/tfrecord)\n\n2. [Doc - TFRecordDataset](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/TFRecordDataset)\n\n3. [Doc - tf.io.decode_jpeg](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/io\/decode_jpeg)\n\n4. [Doc - tf.io.encode_jpeg](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/io\/encode_jpeg)","76ca1d45":"## Check an oversampled dataset\n\nNow, we create an oversampled dataset with `target_counting=782`, i.e. each class occurs the same time as the most frequent class in the original training dataset. We verify the results to make sure that the above codes work as expected.\n\nAgain, this oversampled dataset is only used for the visualizations and the analysis. We will create the actual oversampled dataset for training later.","8e8ac461":"#### Compare training with \/ without gradient accumulation","30aef1d1":"#### Exponential Decay","785e579e":"### Loss calculation<a id='loss-calculation'><\/a>\n\nWhen a batch is distributed to the replicas by calling [strategy.run](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/Strategy#run), each replica receives a part of the batch and\ncalculates the loss values separately. It **SHOULD NOT** calculate the average of the per example losses on the (partial) batch it recevies. This is because :\n1. The gradients calculated on each replica will be synced across the replicas - they are summed before the optimizer applies the gradients to update the model's parameters.\n2. If we use the averaged per examples loss to compute the graident on each replica, the final graident applied by the optimizer will correspond to the sum of these averaged per examples losses on the different replicas.\n3. However, the optimizer should apply the gradient obtained from the averaged per examples loss over the whole distributed batch.<a id=\"point-3\"><\/a>\n4. We have already seen that [each replica might receive different number of examples](#look-perreplica-object). Therefore it is impossible, in general, to obtain the averaged per example loss over the whole distributed batch from [3.](#point-3) by simply divide it by the number of replicas.\n5. So on each replica, we calculate the sum of per examples losses divided by the batch size of the whole distributed batch, which will give the optimizer the correct gradients to apply.\n\nIn this notebook, since we use gradient accumulation, each replica receives several batches before the optimizer applying the graidents, we divide the sum of per examples losses by the update size (i.e. the number of examples used for one parameter update) rather than by the size of a single distributed batch.\n\n**References**\n\n1. [Tutorial - Custom training with tf.distribute.Strategy - Define the loss function](https:\/\/www.tensorflow.org\/tutorials\/distribute\/custom_training#define_the_loss_function)","52e9b6a3":"#### oversampled dataset with $N = 300$, but with fewer epochs\n\nWhen we train a model with oversampling, it is unfair to compare to the training without oversampling by looking at the same epochs, because oversampling has more training examples in each epoch.\n\nFor oversampling with $N=300$, we have about $33300$ examples in one epoch, which is about $2.6$ times the number of original training example (which is $12753$). Let's reduce the number of epochs by a similar factor when training with oversampling.","75e4c849":"## Play with perspective transformation in JavaScript<a id='play-pers-trans'><\/a>\n\nThe code here is from the [javascript implementation](http:\/\/jsfiddle.net\/dFrHS\/1\/) which I found through the discussion in [Computing a projective transformation](https:\/\/math.stackexchange.com\/a\/339033\/33138).\n\nYou can move the 4 corners to play with perspective transformation!","244b03c9":"### Testing samples","66d0545e":"### Get labels and counting","b49b6f6c":"### Train with the original dataset","178ca6c6":"# Configuration<a id='configuration'><\/a>","68cf5e08":"We get a tuple of tensors after calling [strategy.experimental_local_results](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/Strategy#experimental_local_results).","acc83349":"#### without data augmentation vs. with data augmentation","2979d6fe":"## Define the routines<a id='define-routines'><\/a>\n\nWith the above discussions, we are ready to define the routines used for training, validation and prediction. The following code should be clear now, except for the loss calculation.","559d9a56":"### compare label distributions","c2b6fd75":"### Check a batch in the distributed dataset\n[Previously](#check-batch), we checked a batch in the training dataset, which is a tuple containing 2 tensors.\nOne is a batch of images, and the other one is a batch of labels.\nLet's check what we get when we distribute our flower training dataset.\n\n<a id=\"check-dist-batch\"><\/a>\nLet's create a dataset of batch size $9$.","1856bf6d":"### PerReplica objects in distributed datasets\n\nThe distributed datasets (when working with TPU) contains objects of type [tensorflow.python.distribute.values.PerReplica](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/v2.3.0\/tensorflow\/python\/distribute\/values.py#L361), which is a subclass of [tf.distribute.DistributedValues](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/DistributedValues) that is the base class for representing distributed values.","5119b69c":"#### Check strategy.experimental_local_results","f6d64d71":"#### no oversampling vs. oversampling $N=100$","fd777aa7":"## Distributed computation<a id='distributed-computation'><\/a>\n\nFor each distributed batch (which contains `PerReplica` objects as discussed in [Distributed dataset](#distributed-dataset)) produced by a distributed dataset, we use [strategy.run](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/Strategy#run) to perform a distributed computation on different TPU replicas, each processes a part of the batch.\n\n```\n    @tf.function\n    def dist_step(dist_batch):\n        strategy.run(replica_fn, args=dist_batch)\n        \n    for dist_batch in dist_ds:\n        dist_step(dist_batch)\n```\n\nHere `replica_fn` is a function that is going to be run on each replica, and it should work with tensors, not with `PerReplica` objects.\nYou define the operations (for example, forward pass, compute loss values and gradients, etc.) to peform just like witout using TPU. \nWhen working with `TPU`, either [strategy.run](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/Strategy#run) have to be called inside [tf.function](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/function) or the replica function have to be annotated with [tf.function](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/function). For example:\n\n```\n    @tf.function\n    def replica_fn(batch):\n        \n        model(batch)\n        ...\n        \n    for dist_batch in dist_ds:\n        strategy.run(replica_fn, args=dist_batch)\n```\n\nThe above code snippet is a high level concept, and `replica_fn` doesn't necessary receive a single argument. In our case, the original dataset yields tuples of tensors, a distributed batch is also a tuple of `PerReplica` objects, and `replica_fn` actually receives the unpacked version of a tuple of tensors as arguments.\n\nIf a dataset yield a single tensor, you can do things like \n```\n    @tf.function\n    def replica_fn(batch):\n        \n        tensor0 (, ... tensorN) = batch\n        model(tensor0, ... tensorN)\n\n    strategy.run(replica_fn, args=(dist_batch,))\n```\nwhere `replica_fn` expects a single tensor as arugment. Even if a dataset yields tuples of tensors, the above code still works, but `replica_fn` expects a single tuple of tensors as argument.","24cc4dd4":"#### Linear Decay","eca80895":"Our perspecitve transformation as data augmentation helps the model to perform slightly better on the validation dataset.\nHowever, this might depends on a lot of factors, including the model architecture, model size, image size, etc. Also, our perspective transformation introduces black regions in the images, which might be another factor that affects the model performance. We leave the readers to explore different image augmentation methods by themselves. See the references for some notebooks about data augmentation.\n\nWith [TensorFlow 2.3](https:\/\/blog.tensorflow.org\/2020\/07\/whats-new-in-tensorflow-2-3.html), we have the new layer [tf.keras.layers.experimental.preprocessing.RandomRotation](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/experimental\/preprocessing\/RandomRotation) that can perform random rotation for us!\n\n**References**<a id='ref-aug'><\/a>\n1. [CutMix and MixUp on GPU\/TPU](https:\/\/www.kaggle.com\/cdeotte\/cutmix-and-mixup-on-gpu-tpu)\n2. [Rotation Augmentation GPU\/TPU - [0.96+]](https:\/\/www.kaggle.com\/cdeotte\/rotation-augmentation-gpu-tpu-0-96)\n3. [Make Chris Deotte's data augmentation faster](https:\/\/www.kaggle.com\/yihdarshieh\/make-chris-deotte-s-data-augmentation-faster)\n4. [batch implementation of more data augmentations](https:\/\/www.kaggle.com\/yihdarshieh\/batch-implementation-of-more-data-augmentations)\n5. [GridMask data augmentation with tensorflow](https:\/\/www.kaggle.com\/xiejialun\/gridmask-data-augmentation-with-tensorflow)\n6. [Flower with TPUs - Advanced augmentations](https:\/\/www.kaggle.com\/dimitreoliveira\/flower-with-tpus-advanced-augmentations)","f5f94097":"The returned labels is a `PerReplica` object.","d099bece":"### Look a PerReplica object<a id=\"look-perreplica-object\"><\/a>\nHere is the second component `dist_batch[1]`. It contains tensors, each of them is a batch of labels that will be processed on a different replica. They have different batch dimensions: $0$, $1$ and $2$, but their sum is $9$ which is the batch size we used to create [the dataset](#check-dist-batch).\n\nSimilarly, `dist_batch[0]` contains tensors which are batch of images.","4222bf9d":"#### Visualization utilities\ndata -> pixels, nothing of much interest for the machine learning practitioner in this section.","e5622f67":"#### Info about a train dataset","9d604819":"#### learning rate scaling: x1 vs. x8\nFor gradient accumulation rate $8$, let's see the impact of scaling learning rate by $8$.","7c690275":"## Put the routines together\n\nWe are at the final step before the real traning! Here we use the above routines to define the highest level of the training, validation and testing processes, including printing some information and saving the results.","8d5ef7e5":"#### Tensorflow 2.3 and EfficientNet\n\nWith `TensorFlow 2.3` released recently, it is easier to import `EfficientNet` models. For example\n\n```\nfrom tensorflow.keras.applications import EfficientNetB7\n```","70aa11ae":"#### Check an element in the raw dataset","a93cf7cf":"#### All the classes","935a2dcf":"### some statistics (for $N=782$)","81ba6f32":"### Loss classes and reduction<a id='loss-functions'><\/a>\n\nAccording to the tutorial [Custom training with tf.distribute.Strategy - Define the loss function](https:\/\/www.tensorflow.org\/tutorials\/distribute\/custom_training#define_the_loss_function), if a loss class in the module [tf.keras.losses](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/losses) is used, like [SparseCategoricalCrossentropy](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/losses\/SparseCategoricalCrossentropy) in this notebook, we have to specify `NONE` or `SUM` for the parameter `reduction` when working with [tf.distribute.Strategy](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/Strategy). The default value `AUTO` and the usually used `SUM_OVER_BATCH_SIZE` are disallowed when working with tf.distribute.Strategy. \n\nOn each replica, after the per example losses are summed, it should be divided by the global bacth size rather than the number of examples processed by a single replica. By global bacth, it means a batch of examples that is distributed to different replicas. Tensorflow only allows `NONE` or `SUM` reductions to make the users explicitly think about the correct and desired reduction in their distributed case. We will show how to deal with loss values in [Loss calculation](#loss-calculation).\n\n**References**\n\n1. [Tutorial - Custom training with tf.distribute.Strategy - Define the loss function](https:\/\/www.tensorflow.org\/tutorials\/distribute\/custom_training#define_the_loss_function)","9ee7b4ae":"# Competition data access<a id='data-access'><\/a>\nTPUs read data directly from Google Cloud Storage (GCS). This Kaggle utility will copy the dataset to a GCS bucket co-located with the TPU.  Once done, use `!ls \/kaggle\/input\/` to list attached datasets.\n\n> Tips: If you have multiple datasets attached to the notebook, you should pass the name of a specific dataset to the `get_gcs_path()` function. (Here, the name of the dataset is the name of the directory it is mounted in.)","92c4df50":"## Step 2: Determine how many times an example in a class should repeat\n\nThe objective of our approach is to have a (more) balanced dataset, where the number of occurrence of each class is much closer to those of other classes than it is in the original training dataset.\n\nFrom the above counting, we see that the majority class is class 67 which occurs 782 times, while the minority classes are class 44, 34 and 6, which occur 18 times each.  \n\nFor a given number $N$, we will construct a new dataset where each class occurs approximately $N$ times. For example, if we specify $N = 782$, the new dataset will have about $782$ examples for each class, which is clearly balanced.\n\nFor the flexbility of our experiments, we allow $N$ to be lower, say, `100` or `300`. These still give imbalanced dataset, but less imbalanced than the original dataset.\n\nGiven a such number $N$ and a class $y$, for any training example $\\mathbb{x}$ in the original dataset with label $y$, we now determine the number of times the example $\\mathbb{x}$ should repeat in order to obtain a dataset having the property discussed in the prevous paragraphs.\nAt a first attempt, this will be a float number, which is the ideal value. Based on it, the actual number of times the example $\\mathbb{x}$ will repeat is determined in a randomized way to make the number of occurrences of each class roughly $N$. For example, if examples in class $y$ should repeat $2.7$ times, then they will repeat $2$ times with a probability $0.3$ and $3$ times with a probability $0.7$.","0be6a0d4":"### original \/ oversampled datasets - samples side by side \n\nLet's look a few sample batches in the original dataset and in the oversampled dataset.","ebfdf3ab":"# Dataset functions<a id='dataset-functions'><\/a>","b5724455":"Even trained with much fewer epochs, oversampling with $N=300$ (11 epochs) outperforms training without oversampling (30 epochs) on `recall` and `f1` scores.\n\nHowever, this is not very stable across different runs. Sometimes, their results are very close.","3f1fb638":"### Let's check the number of times an example should repeat\n\nHere, we take $N = 782$.","6bb1dec4":"# Simple EDA<a id='simple-eda'><\/a>\n\nIn this EDA section, we create datasets of batch size 16. These are only used for visualizations and for getting some statistics about the datasets. Later, we will create actual datasets for training, validation and testing.","07e471b0":"## Distributed dataset<a id='distributed-dataset'><\/a>\n\nWith an input pipeline [tf.data.Dataset](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset), we use  [strategy.experimental_distribute_dataset](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/Strategy#experimental_distribute_dataset) to turn it into a distributed dataset, which produces `per-replica` values (which are objects of type [PerReplica](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/v2.3.0\/tensorflow\/python\/distribute\/values.py#L361)) when iterating over it. For example, \n\n```\n    ds = (... something that is a `tf.data.Dataset` ...)\n    dist_ds = strategy.experimental_distribute_dataset(ds)\n```","f4da0299":"### Train with optimized loop\n\nNow let's train with optimized loop. The model performance will be the same, but the training time will be reduced.","9c65e2eb":"### what if I don't know how to define the feature description for a raw dataset ...\n\nIf you are the author who created the TFRecord files, you definitely know how to define the feature description to parse the raw dataset.\n\nOtherwise, you can use like\n\n```\n    example = tf.train.Example()\n    example.ParseFromString(serialized_example.numpy())\n```\n\nto check the information. You will get something like\n\n```\n    features {\n      feature {\n        key: \"class\"\n        value {\n          int64_list {\n            value: 57\n          }\n        }\n      }\n      feature {\n        key: \"id\"\n        value {\n          bytes_list {\n            value: \"338ab7bac\"\n          }\n        }\n      }\n      feature {\n        key: \"image\"\n        value {\n          bytes_list {\n            value: .......\n```\nThis should give you enough information to define the feature description.","b4151c22":"#### compare to training without gradient accumulationo again","8b24cdbb":"# Import","7a642134":"### Train with gradient accumulation","9daa7d2c":"# Table of Contents\n\n1. [TPU or GPU detection](#hardware-detection)\n2. [Competition data access](#data-access)\n3. [Configuration](#configuration)\n4. [Dataset functions](#dataset-functions)\n  * [Read from TFRecord files - raw dataset](#read-files)\n  * [Parse the raw dataset](#parse-dataset)\n  * [Working with tf.data.Dataset](#working-with-datasets)\n5. [Simple EDA](#simple-eda)\n  * [Dataset visualizations](#visualizations)\n  * [Label distribution](#label-distribution)\n6. [Oversampling](#oversampling)\n7. [Define the model and training process](#define-model)\n * [Model, metric and optimizer](#objects)\n     - [Loss classes and reduction](#loss-functions)\n * [Distributed dataset](#distributed-dataset)\n * [Distributed computation](#distributed-computation)\n    - [Optimized loops](#optimized-loops)\n * [Define the routines](#define-routines)\n    - [Loss calculation](#loss-calculation)\n    - [Collect the return values](#collecting) \n8. [Training](#training)\n9. [Data augmentation - Perspective transformation](#perspective-transformation)\n  * [Preview the effect of perspective transformation](#preview)\n  * [Play with perspective transformation in JavaScript](#play-pers-trans)\n  * [Implement perspective transformation in TensorFlow (batch)](#imp-pers-trans)\n  * [Visualize perspective transformation](#visu-pers-trans)\n10. [Conclusion](#conclusion)  ","87144974":"#### A train batch","a9281e9c":"<center><img src=\"https:\/\/raw.githubusercontent.com\/dimitreOliveira\/MachineLearning\/master\/Kaggle\/Flower%20Classification%20with%20TPUs\/banner.png\" width=\"1000\"><\/center>\n<br>\n<center><h1>A detailed guide to custom training with TPUs - Flower Classification<\/h1><\/center>\n<br>\n\n### In this notebook, we will go through, step by step, training models with TPUs in a custom way. These includes:\n\n* use tf.data.Dataset as input pipeline\n* perform a custom training loop\n* correctly define loss function\n* make the custom training loop even faster\n* gradient accumulation with TPUs\n* apply oversampling to deal with imbalanced data\n* Have fun with a special data augmentaion - Perspective transformation\n\nThis kernel is based on the following kernels with my own extension (I keep some code in these 2 notebooks):\n1. [Getting started with 100+ flowers on TPU](https:\/\/www.kaggle.com\/mgornergoogle\/getting-started-with-100-flowers-on-tpu) - by Martin G\u00f6rner.\n\n2. [Custom Training Loop with 100+ flowers on TPU](https:\/\/www.kaggle.com\/mgornergoogle\/custom-training-loop-with-100-flowers-on-tpu) - by Martin G\u00f6rner.\n","fc141ed1":"The above results confirm that the training time is reduced when the optimized loop is used. Except for the first epoch, the trining speed is about 2x faster! For the 1st epoch, since a computation graph is compilled, it always takes more time to finish.\n\nFrom now on, we will perform training only with the optimized loop.","381a5f93":"## Step 3. A method to get oversampled datasets\n\nNow we are ready to define a method that can return oversampled datasets.\n\nThere is an argument `augmentation_fn` that won't be used for now. After we define our own data augmentation method in [Data augmentation - Perspective transformation](#perspective-transformation), we can pass it to `augmentation_fn` to make the oversampled dataset having more diversity.","6d82f117":"#### Train without optimized loop","8cd444e3":"#### A test batch","13c13a17":"In this notebook, we went through a custom training with TPUs. We learned about datase pipeline, oversampling, distributed dataset and distributed computation. We also saw how to define the loss values correctly, how to collect the return values from TPUs and a way to optimize the training time. A minimal implementation of gradient accumulation is provided for working with TPUs. We also showed a unusual data augmentation - perspective transformation. Finally, we performed several training and compared their results.\n\nI hope you enjoy reading this notebook and learn something new!","ece46362":"### Finally, the implementation of our routines","79615bb3":"The performances are almost identical for training with \/ without optimized loop, which is expected.","866a8b4d":"Using oversampling, we obtains better results than without using oversampling, especially for the `recall` score (and therefore also for the `f1` score).","a9976aef":"### Check what a batch looks like<a id=\"check-batch\"><\/a>","a114b1c4":"In this notebook, we only run each configuration once. Ideally, each configuration should be run multiple times and the averaged results are used for comparison. Due to the 3 hours TPU time limit on Kaggle, this is not feasible for the model we use in this notebook.\n\nHowever, most of the conclusions in this notebook are stable and won't be different in another run.","3f0cf86f":"#### oversampled dataset with $N = 300$.\n\nLet's increase the number of occurrences of each class.","11016720":"### visualize the learning rate schedule\n\nLet's use the above code to turn learning rate schedules to use warmup and visualize them.","910f6966":"#### Compare training with \/ without optimized loop","4d0db1d4":"#### Preview the effect of perspective transformation<a id='preview'><\/a>\n\n![inbox_1533864_7d71919df88ad547a96aca9b3a7557d6___results___9_0.png](attachment:inbox_1533864_7d71919df88ad547a96aca9b3a7557d6___results___9_0.png)","7e2985a5":"## Model, metric and optimizer<a id='objects'><\/a>\n\nIn order to use TPU, or tensorflow distribute strategy in general, some objects have to be created inside the strategy's scope.\nHere is the rule of thumb:\n\n* Anything that creates variables that will be used in a distributed way must be created inside `strategy.scope()`.\n* This includes:\n  - model creation\n  - optimizer\n  - metrics\n  - sometimes, checkpoint restore\n  - any custom code that creates distributed variables\n* Once a variable is created inside a strategy's scope, it captures the strategy's information, and you can use it outside the strategy's scope.\n* Unless using a high level API like `model.fit()`, define things inside the strategy's scope won't automatically distribute the computation. This will be discussed in [Distributed computation](#distributed-computation).\n\nInside the scope, everything is defined in a way just like without using distribute strategy. There is however a particularity about the loss function, see [Loss classes and reduction](#loss-functions).\n\nIn the next cell, we define the learning rate and the loss object inside the scope, but it's not mandatory.\n\n**References:**\n1. [Doc - TPUStrategy - scope](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/experimental\/TPUStrategy#scope)\n2. [Tutorial - Custom training with TPUs](https:\/\/colab.research.google.com\/github\/tensorflow\/tpu\/blob\/master\/tools\/colab\/custom_training.ipynb#scrollTo=s_suB7CZNw5W)","0f4886d5":"### Optimized loops<a id='optimized-loops'><\/a>\n\nIn [Distributed computation](#distributed-computation), we show a way to iterate the distributed dataset:\n\n```\n    for dist_batch in dist_ds:\n        dist_step(dist_batch)\n```\nEvery step in the loop, which calls [strategy.run](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/Strategy#run), will have a communication between the local VM (in our case, the Kaggle VM) and the remote TPU worker(s).\n\nHowever, you can iterate the distributed dataset inside a `tf.function`, like\n```        \n    @tf.function\n    def dist_run_on_dataset(dist_ds):\n    \n        for dist_batch in dist_ds:\n            dist_step(dist_batch)\n            \n    dist_process_dataset(dist_ds)\n```\nThis way, the whole operations over the dataset is compiled into a graph is sent to the remote TPU worker(s) for execution. This will reduce the running time and avoid TPUs to be idle waiting for data from the local VM. See [TPU: extreme optimizations](https:\/\/www.kaggle.com\/c\/flower-classification-with-tpus\/discussion\/135443) for a good benchmark by [Martin G\u00f6rner](https:\/\/www.kaggle.com\/mgornergoogle).\n\n\n\nIn this notebook, we use a fixed number of training steps, so we can also use\n```        \n    @tf.function\n    def dist_process_dataset(dist_ds_iter):\n    \n        for _ in tf.range(n_stes):\n            dist_step(next(dist_ds_iter))\n            \n    dist_ds_iter = iter(dist_ds)\n    dist_process_dataset(dist_ds_iter)\n```\n\n**References**\n\n* [Tutorial - Iterating inside a tf.function](https:\/\/www.tensorflow.org\/tutorials\/distribute\/custom_training#iterating_inside_a_tffunction)\n\n* [Tutorial - Using iterators](https:\/\/www.tensorflow.org\/tutorials\/distribute\/custom_training#using_iterators)\n\n* [Kaggle discussion - TPU: extreme optimizations](https:\/\/www.kaggle.com\/c\/flower-classification-with-tpus\/discussion\/135443)\n\n* [Kaggle notebook - Custom Training Loop with 100+ flowers on TPU](https:\/\/www.kaggle.com\/mgornergoogle\/custom-training-loop-with-100-flowers-on-tpu#Optimized-custom-training-loop)","56ab5cd2":"### Training samples","80a7b654":"### Validation samples","f97100a0":"### Access PerReplica's content\n\nFor a `PerReplica` object, you can use the property `values` to access its content. It turns out to be a tuple. The number of its components is the number of replicas `strategy.num_replicas_in_sync`.","4a66ad2b":"We use image size $192$ in this notebook to reduce the running time for the demonstration.","430137b5":"#### Fix some hyperparameters\nIn our experiment, we fix the model to be `EfficientNetB7` and the number of epochs to be $30$. The image size is $192$ in order to reduce the running time. With larger image size, we could get better results.","d8a3a987":"## Visualize perspective transformation<a id='visu-pers-trans'><\/a>","7e506406":"We use \n```\n@tf.autograph.experimental.do_not_convert\n```\nto tell tensorflow not to convert the function `get_label_counting`, otherwise we get the following warning\n> WARNING: AutoGraph could not transform <function get_label_counting.<locals>.<lambda> at 0x7f0690681830> and will run it as-is.\n\nThis is OK, because this function is not used in our input pipeline, so not converting it to graph won't slow down the pipeline.","59ad3579":"## Label distribution<a id=\"label-distribution\"><\/a>","2f398131":"Compared to oversampling with $N=100$, we get better results when training by oversampling with $N=300$.","06f679fe":"Here is how we parse a serialized example in this flower classification dataset and obtain an image.","5311f99f":"## Dataset visualizations<a id='visualizations'><\/a>\n### Let's look some samples from the train\/validation\/test datasets.","9776b720":"The training time is further but slightly reduced. However, depending on the model size and the number of training epochs, it could be a significant amount.","b9795669":"#### Train with gradient accumulation steps 8\n\nNow, let's accumulate the gradients 8 times before updating the model parameters.","e52e49ab":"With gradient accumulation, the model performs worse than the training without graident accumulation. This is because the number of times that the model parameters are updated is fewer (8 times fewer here). One can either train with more epochs or with a larger learning rate.\n\nHere, we try to scale the learning rate.","130e953b":"## Read from TFRecord files - raw dataset<a id='read-files'><\/a>\n\nHere we use `tf.data.TFRecordDataset` to read some TFRecord files and peek the content.\n\nThe simplest way is to specify a list of filenames (paths) of TFRecord files to it.\nIt is a subclass of `tf.data.Dataset`.\n\nThe raw dataset contains `tf.train.Example` messages, and when iterated over it, we get scalar string tensors.","3f2ae46e":"# Data augmentation - Perspective transformation<a id=perspective-transformation><\/a>\n\nThis is originally implemented in my notebook [perspective transformation](https:\/\/www.kaggle.com\/yihdarshieh\/perspective-transformation?scriptVersionId=29866403), which followed a discussion in [Computing a projective transformation](https:\/\/math.stackexchange.com\/a\/339033\/33138) that has a [javascript implementation](http:\/\/jsfiddle.net\/dFrHS\/1\/).\n\nWe will skip the introduction to perspective transformation here. Basically, you can think it as a transformation obtained by mapping 4 points in a source space to 4 points in a target space. That's why it is also called `4 points transformation`. It also includes rotations and flipping.\n\nThere are other notebooks implementing different data augmentations. See [the references here](#ref-aug).","e380b83e":"#### oversampled dataset with $N = 100$."}}