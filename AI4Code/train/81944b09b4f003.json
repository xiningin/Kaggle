{"cell_type":{"f678bcad":"code","0aa46b5f":"code","dd29c4fb":"code","9d12cf64":"code","b6875ca9":"code","c42a4396":"code","7fb6926a":"code","513bd291":"code","b7b46c1e":"code","abcba450":"code","30c902f6":"code","03fa5668":"code","c7a94453":"markdown","becd97ea":"markdown","09252651":"markdown","17ec1ba6":"markdown","c13779f6":"markdown","d5f74cdc":"markdown","9166534b":"markdown"},"source":{"f678bcad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0aa46b5f":"dataset = pd.read_csv(\"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")\ndataset.describe()","dd29c4fb":"# Splitting predictors and target\nx = dataset.drop(columns=[\"DEATH_EVENT\"])\ny = dataset[\"DEATH_EVENT\"]","9d12cf64":"# Scaling input\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaler.fit(x)\n\nx = scaler.fit_transform(x)\n\n# Splitting into train\/test\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n\nprint(\"x_train Shape : \", x_train.shape)\nprint(\"x_test Shape  : \", x_test.shape)\nprint(\"y_train Shape : \", y_train.shape)\nprint(\"y_test Shape  : \", y_test.shape)","b6875ca9":"# Model building\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n\nnn_model = MLPClassifier(random_state=0)\nnn_model.fit(x_train, y_train)\n\ny_pred = nn_model.predict(x_test)\n\nprint(\"Accuracy score  : {:.4f}\".format(accuracy_score(y_pred, y_test)))\nprint(\"Precision score : {:.4f}\".format(precision_score(y_pred, y_test)))\nprint(\"Recall score    : {:.4f}\".format(recall_score(y_pred, y_test)))\nprint(\"F1 score        : {:.4f}\".format(f1_score(y_pred, y_test)))\nprint(\"AUC ROC score   : {:.4f}\".format(roc_auc_score(y_pred, y_test)))\nprint(\"\\n\", classification_report(y_pred, y_test))","c42a4396":"from sklearn.model_selection import GridSearchCV\n\nactivation_fn = [\"identity\", \"relu\", \"logistic\", \"tanh\"] # activation function\nsolver = [\"lbfgs\", \"adam\", \"sgd\"] # optimizer\nalpha = [0.0001, 0.05] # Ridge regression's alpha\nlearning_rate = list(['constant','adaptive'])\nhidden_layer_sizes = list([(50,50,50), (50,100,50), (100,)]) # different sizes of hidden layers\n\n\nparam_grid = dict(\n    activation = activation_fn,\n    solver = solver,\n    alpha = alpha,\n    learning_rate = learning_rate,\n    hidden_layer_sizes = hidden_layer_sizes\n)\n\nmlp = MLPClassifier(max_iter=100)\nclf = GridSearchCV(mlp, param_grid=param_grid, \n                   cv=10, \n                   scoring='roc_auc',\n                   n_jobs=-1, verbose=2\n                  )\nclf.fit(x_train, y_train)\nclf.best_params_","7fb6926a":"mlpc_final = MLPClassifier(activation = 'tanh',\n                           alpha = 0.05,\n                           hidden_layer_sizes = (50, 50, 50),\n                           learning_rate = 'adaptive',\n                           solver = 'adam',\n                           random_state = 0\n                          )\nmlpc_final.fit(x_train, y_train)\ny_pred = mlpc_final.predict(x_test)\n\nprint(\"Accuracy score  : {:.4f}\".format(accuracy_score(y_pred, y_test)))\nprint(\"Precision score : {:.4f}\".format(precision_score(y_pred, y_test)))\nprint(\"Recall score    : {:.4f}\".format(recall_score(y_pred, y_test)))\nprint(\"F1 score        : {:.4f}\".format(f1_score(y_pred, y_test)))\nprint(\"AUC ROC score   : {:.4f}\".format(roc_auc_score(y_pred, y_test)))\nprint(\"\\n\", classification_report(y_pred, y_test))","513bd291":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_pred, y_test)\n\nsns.heatmap((cm\/np.sum(cm) * 100),\n            annot = True,\n            fmt = \".2f\",\n            cmap = \"Greens\"\n           )","b7b46c1e":"from sklearn.utils import resample\n\nall_accuracy_scores = []\n\nfor i in range(0, 101): # repeat bootstrap sampling 100 times\n    x_boot = resample(dataset, replace=True)\n    oob = dataset[~dataset.apply(tuple,1).isin(x_boot.apply(tuple,1))]\n    \n    mlpc_boot = MLPClassifier(activation = 'tanh',\n                              alpha = 0.05,\n                              hidden_layer_sizes = (50, 50, 50),\n                              learning_rate = 'adaptive',\n                              solver = 'adam',\n                              random_state = 0\n                             )\n    mlpc_boot.fit(x_boot.drop(columns=[\"DEATH_EVENT\"]), x_boot[\"DEATH_EVENT\"])\n    boot_pred = mlpc_boot.predict(oob.drop(columns=[\"DEATH_EVENT\"]))\n    \n    all_accuracy_scores.append(accuracy_score(boot_pred, oob[\"DEATH_EVENT\"]))\n\nprint(\"Mean accuracy score  : {:.4f}\".format(np.mean(all_accuracy_scores)))","abcba450":"from imblearn.over_sampling import SMOTE\n\nsms = SMOTE(random_state=0)\n\nx_res, y_res = sms.fit_sample(x, y)\nx_train, x_test, y_train, y_test = train_test_split(x_res, y_res, test_size=0.2, random_state=42)\n\nprint(\"x_train Shape : \", x_train.shape)\nprint(\"x_test Shape  : \", x_test.shape)\nprint(\"y_train Shape : \", y_train.shape)\nprint(\"y_test Shape  : \", y_test.shape)","30c902f6":"mlpc_model = MLPClassifier(activation=\"relu\",\n                            alpha=0.05,\n                            hidden_layer_sizes= (100,),\n                            learning_rate= 'adaptive',\n                            solver= 'adam',\n                            random_state=42)\nmlpc_model.fit(x_train, y_train)\ny_pred = mlpc_model.predict(x_test)\n\nprint('Accuracy Score: {:.4f}'.format(accuracy_score(y_test, y_pred)))\nprint('SVC f1-score  : {:.4f}'.format(f1_score(y_pred, y_test)))\nprint('SVC precision : {:.4f}'.format(precision_score(y_pred, y_test)))\nprint('SVC recall    : {:.4f}'.format(recall_score(y_pred, y_test)))\nprint(\"\\n\",classification_report(y_pred, y_test))","03fa5668":"cm = confusion_matrix(y_pred, y_test)\n\nsns.heatmap((cm\/np.sum(cm) * 100),\n            annot = True,\n            fmt = \".2f\",\n            cmap = \"Oranges\"\n           )","c7a94453":"## Visuallizing confusion matrix","becd97ea":"## Model building","09252651":"### Parameters tuning with Grid search (using AUC ROC as scoring method)","17ec1ba6":"## Using the tuned parameters","c13779f6":"## Using SMOTE method (over sampling)","d5f74cdc":"## Read and preprocess data","9166534b":"## Using boostrap sampling"}}