{"cell_type":{"b1f5a112":"code","55c89981":"code","f5c7908f":"code","0e0f46b8":"code","01acd226":"code","80b7c58d":"code","95d5bd57":"code","8ecddd1c":"code","806cec77":"code","d9dc479f":"code","19df2718":"code","c415b0d2":"code","1b000124":"code","6fa3db52":"code","ecad3674":"markdown","b6952c5b":"markdown","fac10aa4":"markdown","ad8e4efe":"markdown","4b85cecb":"markdown","e9213fe5":"markdown","d12f8988":"markdown","957d3bd0":"markdown"},"source":{"b1f5a112":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport tqdm\nimport torch\nimport joblib\nimport transformers\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\n\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import AutoTokenizer, AdamW, get_cosine_schedule_with_warmup\nfrom transformers import AutoModel, AutoConfig\n\nfrom sklearn import model_selection","55c89981":"class Config:\n    def __init__(self):\n        self.TARGET_COLS = [\"target\"]\n        self.MAX_LEN = 256\n        self.CHECKPOINT = \"..\/input\/clrp-itpt-roberta-base\/clrp-itpt-model-roberta-base\"\n        self.TOKENIZER_CHECKPOINT = \"roberta-base\"\n        self.EPOCHS = 3\n        self.TRAIN_BATCH_SIZE = 16\n        self.EVAL_BATCH_SIZE = 16\n        self.LR = 5e-5\n        self.DEVICE = \"cuda\"\n        self.EVAL_INTERVAL = 20\n        self.LOG_INTERVAL = 20\n        self.FOLDS = 5\n        self.WD = 0.01\n\nconfig = Config()","f5c7908f":"raw_train = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\nraw_test = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\n\ntrain_df = raw_train.copy(deep=True)\ntrain_df.loc[:, \"excerpt\"] = train_df.excerpt.map(lambda x: x.replace(\"\\n\", \"\"))\n# sum(train_df.excerpt.str.contains(\"\\n\"))\n\nnum_bins = int(np.floor(np.log2(len(train_df))))\nprint(f\"Num bins : {num_bins}\")\n\ntrain_df.loc[:, \"bins\"] = pd.cut(\n    train_df[\"target\"], bins=num_bins, labels=False\n)\n\nkf = model_selection.StratifiedKFold(n_splits=config.FOLDS)\n\ntrain_df[\"fold\"] = -1\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X=train_df, y=train_df.bins.values)):\n    train_df.loc[valid_idx, \"fold\"] = fold\n\ntrain_df.head()","0e0f46b8":"train_df.bins.hist()","01acd226":"excerpts = train_df[\"excerpt\"].copy(deep=True)","80b7c58d":"class CLRPDataset(Dataset):\n    def __init__(self, data, checkpoint, max_length: int = 256, is_test: bool = False):\n        self.excerpts = data.excerpt.values.tolist()\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        self.max_len = max_length\n        self.targets = data.target.values.tolist()\n        self.is_test = is_test\n        \n    def __getitem__(self, idx):\n        item = self.tokenizer(self.excerpts[idx], max_length=self.max_len,\n                             return_tensors=\"pt\", truncation=True, padding=\"max_length\")\n        if self.is_test:\n            return {\n                \"input_ids\": torch.tensor(item[\"input_ids\"], dtype=torch.long).squeeze(0),\n                \"attention_mask\": torch.tensor(item[\"attention_mask\"], dtype=torch.long).squeeze(0)\n            }\n        else:\n            target = self.targets[idx]\n            return {\n                \"input_ids\": torch.tensor(item[\"input_ids\"], dtype=torch.long).squeeze(0),\n                \"attention_mask\": torch.tensor(item[\"attention_mask\"], dtype=torch.long).squeeze(0),\n                \"label\": torch.tensor(target, dtype=torch.float).squeeze(0)\n            }\n\n    def __len__(self):\n        return len(self.targets)","95d5bd57":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n    \nclass CLRPModel(nn.Module):\n    def __init__(self,path):\n        super(CLRPModel, self).__init__()\n        self.roberta = AutoModel.from_pretrained(path)  \n        self.config = AutoConfig.from_pretrained(path)\n        self.head = AttentionHead(self.config.hidden_size,self.config.hidden_size)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(self.config.hidden_size,1)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        x = self.roberta(input_ids=input_ids, attention_mask=attention_mask)[0]\n        x = self.head(x)\n        x = self.dropout(x)\n        x = self.linear(x)\n        loss = None\n        if labels is not None:\n            loss = loss_fn(x, labels)\n        return (loss, x) if loss is not None else x","8ecddd1c":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.val = 0\n        self.count = 0\n        self.max = 0\n        self.min = 0\n        self.avg = 0\n        self.sum = 0\n    \n    def update(self, val, n=1):\n        self.val = val\n        self.count += n\n        self.sum += val*n\n        self.avg = self.sum \/ self.count\n        if val > self.max: self.max = val\n        if val < self.min: self.min = val\n\n\ndef loss_fn(outputs, targets):\n    outputs = outputs.view(-1)\n    targets = targets.view(-1)\n    return torch.sqrt(nn.MSELoss()(outputs, targets))\n","806cec77":"class Trainer:\n\n    def __init__(self, model, log_interval, eval_interval, epochs, \n                optimizer, lr_scheduler, model_dir):\n        self.model = model\n        self.optimizer = optimizer\n        self.lr_scheduler = lr_scheduler\n        self.epochs = epochs\n        self.log_interval = log_interval\n        self.eval_interval = eval_interval\n        self.model_dir = model_dir\n        self.evaluator = Evaluator(self.model)\n\n\n    def train(self, train_loader, valid_loader, result_dict, fold):\n        result_dict[\"best_valid_loss\"] = 9999\n        for epoch in range(self.epochs):\n            result_dict[\"epoch\"] = epoch\n            result_dict = self._train_loop_for_one_epoch(\n                epoch=epoch,\n                train_loader=train_loader,\n                valid_loader=valid_loader,\n                result_dict=result_dict\n            )\n        \n        return result_dict\n\n    def _train_loop_for_one_epoch(self, epoch, train_loader, valid_loader, result_dict):\n        losses = AverageMeter()\n        for batch_idx, batch in enumerate(train_loader):\n            input_ids = batch[\"input_ids\"].to(config.DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(config.DEVICE)\n            label = batch[\"label\"].to(config.DEVICE)\n            self.model = self.model.to(config.DEVICE)\n\n            loss = self._train_loop_for_one_step(\n                input_ids,\n                attention_mask,\n                label\n            )\n            losses.update(loss.item())\n\n            if batch_idx % self.log_interval == 0:\n                print(f\"Epoch={epoch}, Avg Loss={losses.avg}, Batch Idx={batch_idx}\")\n                if \"train_loss\" not in result_dict.keys(): result_dict[\"train_loss\"] = list()\n                result_dict[\"train_loss\"].append(losses.avg)\n                print(\"--------Training Results Summary------\")\n                print(f\"Epoch: {epoch}, train_loss: {losses.avg}\")\n\n            if batch_idx % self.eval_interval == 0:\n                result_dict = self.evaluator.evaluate(\n                    valid_loader=valid_loader,\n                    result_dict=result_dict,\n                    epoch=epoch\n                )\n                if result_dict[\"valid_loss\"][-1] <= result_dict[\"best_valid_loss\"]:\n                    print(f\"Train loss: {result_dict['train_loss'][-1]}, Valid loss: {result_dict['valid_loss'][-1]}\")\n                    print(f\"Valid loss decreased from {result_dict['best_valid_loss']} to {result_dict['valid_loss'][-1]}\")\n                    result_dict[\"best_valid_loss\"] = result_dict[\"valid_loss\"][-1]\n                    \n                    print(f\"Saving model state dict in {self.model_dir}....\")\n                    torch.save(self.model.state_dict(), f'{self.model_dir}\/model-fold-{fold}_dict')\n\n        return result_dict\n    \n    def _train_loop_for_one_step(self, input_ids, attention_mask, label):\n        self.model.train()\n        self.optimizer.zero_grad()\n        loss, logits = self.model(input_ids, attention_mask, label)\n        loss.backward()\n        self.optimizer.step()\n        if self.lr_scheduler:\n            self.lr_scheduler.step()\n        return loss","d9dc479f":"class Evaluator:\n\n    def __init__(self, model):\n        self.model = model\n\n    def evaluate(self, epoch, valid_loader, result_dict):\n        losses = AverageMeter()\n        with torch.no_grad():\n            for batch_idx, batch in enumerate(valid_loader):\n                input_ids = batch[\"input_ids\"].to(config.DEVICE)\n                attention_mask = batch[\"attention_mask\"].to(config.DEVICE)\n                label = batch[\"label\"].to(config.DEVICE)\n                self.model = self.model.to(config.DEVICE)\n\n                loss = self._eval_loop_for_one_step(\n                    input_ids,\n                    attention_mask,\n                    label\n                )\n                losses.update(loss.item())\n            print(\"----------Validation Results Summary---------\")\n            print(f\"Epoch: {epoch}, valid_loss: {losses.avg}\")\n            if \"valid_loss\" not in result_dict.keys(): result_dict[\"valid_loss\"] = list()\n            result_dict[\"valid_loss\"].append(losses.avg)\n\n        return result_dict\n\n    def _eval_loop_for_one_step(self, input_ids, attention_mask, label):\n        self.model.eval()\n        loss, logits = self.model(input_ids, attention_mask, label)\n        return loss","19df2718":"def run(df, fold, model_dir):\n\n    xtrain = df[df[\"fold\"] != fold]\n    xvalid = df[df [\"fold\"] == fold]\n    dtrain = CLRPDataset(xtrain, config.TOKENIZER_CHECKPOINT, config.MAX_LEN)\n    dvalid = CLRPDataset(xvalid, config.TOKENIZER_CHECKPOINT, config.MAX_LEN)\n\n    train_loader = DataLoader(\n        dtrain,\n        batch_size=config.TRAIN_BATCH_SIZE,\n        shuffle=True\n    )\n    valid_loader = DataLoader(\n        dvalid,\n        batch_size=config.EVAL_BATCH_SIZE,\n        shuffle=True\n    )\n\n    model = CLRPModel(path=config.CHECKPOINT)\n    \n    optimizer = AdamW(model.parameters(),\n                      lr=config.LR,\n                      weight_decay=config.WD\n                )\n    \n    num_training_steps = config.EPOCHS*len(train_loader)\n    lr_scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps\n    )\n    \n    print(f\"Fold: {fold}\")\n    print(f\"Total Epochs: {config.EPOCHS}, Train Dataset length: {len(dtrain)}, Train Data Loader length: {len(train_loader)}\")\n    print(f\"Total Epochs: {config.EPOCHS}, Valid Dataset length: {len(dvalid)}, Valid Data Loader length: {len(valid_loader)}\")\n\n    print(f\"Num training steps: {num_training_steps}\")\n\n    result_dict = {}\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        lr_scheduler=lr_scheduler,\n        epochs=config.EPOCHS,\n        log_interval=config.LOG_INTERVAL,\n        eval_interval=config.EVAL_INTERVAL,\n        model_dir=model_dir\n    )\n    result_dict = trainer.train(\n        train_loader=train_loader,\n        valid_loader=valid_loader,\n        result_dict=result_dict,\n        fold=fold\n    )\n    joblib.dump(result_dict, f\".\/result_dict-fold-{fold}\")","c415b0d2":"import torch\nimport gc\n\nfor fold in range(config.FOLDS):\n    torch.cuda.empty_cache()\n    gc.collect()\n    run(train_df, fold, \".\")","1b000124":"df = train_df.copy(deep=True)\nxtrain = df[df[\"fold\"] == fold]\nxvalid = df[df [\"fold\"] != fold]\ndtrain = CLRPDataset(xtrain, config.TOKENIZER_CHECKPOINT, config.MAX_LEN)\ndvalid = CLRPDataset(xvalid, config.TOKENIZER_CHECKPOINT, config.MAX_LEN)","6fa3db52":"xvalid.shape","ecad3674":"### Dataset","b6952c5b":"### Main App","fac10aa4":"### Trainer","ad8e4efe":"### Evaluator","4b85cecb":"### Metrics","e9213fe5":"### Create Folds","d12f8988":"### Model\n","957d3bd0":"### Config"}}