{"cell_type":{"ba25534f":"code","efd8bd2d":"code","c551bb44":"code","e8c3dcb0":"code","075b5196":"code","76fcf0e1":"code","b7ea33cd":"code","8113cd0a":"code","67b5c263":"code","eebc40bf":"code","9609dcc3":"code","cc95ac3d":"code","22dba781":"code","214c28e6":"code","37074c5f":"code","d6dca02b":"code","1ab09259":"code","66a83e22":"code","eef72a2d":"code","0b5a061b":"code","70397176":"code","3dd9f225":"code","2e5dddcc":"code","fbbc8e0d":"code","0e57d73e":"code","6fd364bb":"code","7f9edfd4":"code","174ebf2b":"code","f9ad9c34":"code","20a21572":"code","c3c4c26b":"code","960723b2":"code","0d6b1ece":"markdown","cdaca6ab":"markdown","6c4c9060":"markdown","04640c26":"markdown","9dc58ac1":"markdown","1d3573e3":"markdown","c3544b17":"markdown","0d3c6059":"markdown","9cb3b4e1":"markdown","4829b819":"markdown","917e337e":"markdown","3a288835":"markdown","5db4734d":"markdown","b2deab8b":"markdown","d49f2486":"markdown","f019e60c":"markdown","45affbac":"markdown","070fb426":"markdown","323209d8":"markdown","28daadb6":"markdown","08856081":"markdown","3cdc62d9":"markdown","429e1fb5":"markdown"},"source":{"ba25534f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\n\n# Input data files are available in the \"..\/input\/\" directory.\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","efd8bd2d":"metadf = pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")\nmetadf = metadf.drop_duplicates()\nmetadf","c551bb44":"DATA_DIR = \"\/kaggle\/input\/cord19-vaccines-and-therapeutics-google-results\/\"\ngoogle_files = glob.glob(DATA_DIR + \"4*.csv\")\ngoogle_files","e8c3dcb0":"google_files_df = {}\nfor filename in google_files:\n    print(filename)\n    df = pd.read_csv(filename)\n    #remove unnecessary or empty columns\n    df = df.drop(columns=['QueryDate', \"Type\", \"DOI\", \"ISSN\", \"CitationURL\", \"Volume\", \"Issue\", \"StartPage\", \"EndPage\", \"ECC\", \"CitesPerAuthor\", \"AuthorCount\"])\n    # remove all empty rows\n    df = df.dropna(how='all')\n    # rename title coumns\n    df = df.rename(columns={\"Title\": \"title\"}, errors=\"raise\")\n    # only take first 400\n    df = df[:400]\n    # string matching for keys\n    result_index = filename.find('results\/')\n    google_files_df[filename[result_index+8:-4]] = df\ngoogle_files_df","075b5196":"def get_score(rank_col):\n    score = 1 - (rank_col\/len(rank_col))\n    return score","76fcf0e1":"for current_item in google_files_df.items():\n    current_query, current_df = current_item\n    current_df['score from rank'] = get_score(current_df['GSRank'])\ngoogle_files_df","b7ea33cd":"matched_df = {}\nfor current_item in google_files_df.items():\n    current_query, current_df = current_item\n    matched_df[current_query] = pd.merge(current_df, metadf, on=\"title\")\nmatched_df","8113cd0a":"x = matched_df['400Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers'].loc[1]\nx","67b5c263":"for i, current_item in enumerate(matched_df.items()):\n    current_query, current_df = current_item\n    print(\"#######Common articles for \", current_query)\n    #matched_df[current_query] = pd.merge(current_df, metadf, on=\"title\")\n    for j, compare_item in enumerate(matched_df.items()):\n        #if not i==j:\n        compare_query, compare_df = compare_item\n        merged_df = current_df.merge(compare_df, on=['title'], how='inner', indicator=True)\n        print(compare_query[2:40], \"--\", len(merged_df))","eebc40bf":"from os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport matplotlib.pyplot as plt","9609dcc3":"for i, current_item in enumerate(matched_df.items()):\n    current_query, tempdf = current_item\n    tit_text = \" \".join(str(tit) for tit in tempdf.title)\n    abs_text = \" \".join(str(abs) for abs in tempdf.Abstract)\n    text = tit_text + \" \" + abs_text \n    print(current_query)\n    # Create and generate a word cloud image:=\n    # lower max_font_size, change the maximum number of word and lighten the background:\n    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n    fig = plt.figure(figsize=(10,5))\n    #fig.title(current_query)\n    # Display the generated image:\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()","cc95ac3d":"vacc_df = matched_df[\"420vaccine\"]\nvacc_df[\"class\"] = 1\nther_df = matched_df[\"410therapeutics\"]\nther_df[\"class\"] = 2","22dba781":"#positive samples\nconcat_google_df = pd.concat([vacc_df, ther_df], axis=0, ignore_index=True)\nconcat_google_df = concat_google_df.drop_duplicates(subset='title', keep=\"first\")\nconcat_google_df","214c28e6":"#negative samples\ngoogle_files_neg = glob.glob(DATA_DIR + \"NEGATIVE*.csv\")\n\ngoogle_df_neg = []\nfor filename in google_files_neg:\n    df = pd.read_csv(filename)\n    google_df_neg.append(df)\n\nconcat_google_df_neg = pd.concat(google_df_neg, axis=0, ignore_index=True)\nconcat_google_df_neg = concat_google_df_neg.drop_duplicates(subset=\"title\")\nconcat_google_df_neg[\"class\"] = 0\nconcat_google_df_neg\n","37074c5f":"# find comman samples between postive and negative samples\nmerged_df = concat_google_df.merge(concat_google_df_neg, on=['title'], \n                   how='inner', indicator=True, suffixes=('', '_y'))\nmerged_df.drop(list(merged_df.filter(regex='_y$')), axis=1, inplace=True)\nmerged_df","d6dca02b":"#removing the common articles from the nagative samples\nwithout_commom_df = pd.concat([merged_df, concat_google_df_neg])\nwithout_commom_df = without_commom_df.drop_duplicates(keep=False, subset=\"title\")\nwithout_commom_df =  without_commom_df.drop(columns=[\"_merge\"])\nwithout_commom_df\n","1ab09259":"#split into train test\ndef split_traintest(df):\n  #shuffle\n  df = df.sample(frac=1)\n  #first 20%\n  index = int(len(df)*0.2)\n  print(len(df))\n  print(index)\n  test_df = df.iloc[:index]\n  train_df = df.iloc[index:]\n  return test_df, train_df\n\nther_test, ther_train = split_traintest(ther_df)\nvacc_test, vacc_train = split_traintest(vacc_df)\nneg_test, neg_train = split_traintest(without_commom_df)","66a83e22":"def save_csv(df_list, name):\n  final_df = pd.concat([df_list[0],df_list[1], df_list[2]])\n  final_df.to_csv(\"GOOGLE_CLASSIFIED_samples_\"+name+'.csv')\n  print(len(final_df))\n\nsave_csv([ther_test, vacc_test, neg_test], \"test\")\nsave_csv([ther_train, vacc_train, neg_train], \"train\")\n","eef72a2d":"!pip install -q tensorflow_gpu>=2.0","0b5a061b":"import tensorflow as tf\nprint(tf.__version__)","70397176":"!pip install -q ktrain","3dd9f225":"import pandas as pd\nimport numpy as np\nimport glob","2e5dddcc":"test = pd.read_csv(\"GOOGLE_CLASSIFIED_samples_test.csv\")\ntrain = pd.read_csv(\"GOOGLE_CLASSIFIED_samples_train.csv\")","fbbc8e0d":"test = test.fillna(\"\")\ntrain = train.fillna(\"\")\n\ntest[\"text\"] = test[\"title\"] +\";\"+ test[\"abstract\"] +\";\"+ test[\"journal\"]\ntrain[\"text\"] = train[\"title\"] +\";\"+ train[\"abstract\"] +\";\"+ train[\"journal\"]\n\n\nprint('size of training set: %s' % (len(train)))\nprint('size of validation set: %s' % (len(test)))\n\nx_train = train.text.to_list()\ny_train = train[\"class\"].to_list()\nx_test = test.text.to_list()\ny_test = test[\"class\"].to_list()\n\nfor x in x_train[:10]:\n  print(x)\n\nprint(y_train[:10])","0e57d73e":"import ktrain\nfrom ktrain import text","6fd364bb":"MODEL_NAME = 'allenai\/scibert_scivocab_uncased' #'distilbert-base-uncased'\nt = text.Transformer(MODEL_NAME, maxlen=500, classes=['0', '1', '2'])\ntrn = t.preprocess_train(x_train, y_train)\nval = t.preprocess_test(x_test, y_test)\nmodel = t.get_classifier()\nlearner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)","7f9edfd4":"learner.fit_onecycle(5e-5, 4)","174ebf2b":"learner.validate(class_names=t.get_classes())","f9ad9c34":"predictor = ktrain.load_predictor(DATA_DIR+'GOOGLE_scibert_predictor\/GOOGLE_scibert_predictor')","20a21572":"metadf = metadf.fillna(\"\")\n\nmetadf[\"text\"] = metadf[\"title\"] +\";\"+ metadf[\"abstract\"] +\";\"+ metadf[\"journal\"]\n\nmetadf[\"class\"] = \"\"\nmetadf[\"0\"] = 0\nmetadf[\"1\"] = 0\nmetadf[\"2\"] = 0\n\nmetadf","c3c4c26b":"# test on 1 sample\npredictor.predict_proba(metadf.text.iloc[0])","960723b2":"i=0\nfor index, row in metadf.iterrows():\n    probs = predictor.predict_proba(row[\"text\"])\n    classif = predictor.predict(row[\"text\"])\n    metadf.loc[index, \"class\"] = classif\n    metadf.loc[index, \"0\"] = probs[0]\n    metadf.loc[index, \"1\"] = probs[1]\n    metadf.loc[index, \"2\"] = probs[2]\n    if i%200 == 0:\n        print(index)\n    #if i%500 == 0:\n        # uncomment to save after every 500 samples\n        #metadf_diff.to_csv(\"GOOGLE_CLASSIFIED_metadata_diff_\"+str(i)+'.csv')\n    i = i+1\n\nmetadf_diff\n#metadf_diff.to_csv(\"GOOGLE_CLASSIFIED_metadata_\"+\"final\"+'.csv')","0d6b1ece":"# Part 3: Training SciBERT on the Google extracted labels to get labels for entire dataset","cdaca6ab":"The nagative samples are extracted by taking a completely seperate query from the 9 given challenges of the CORD-19 challenge which are semantically very different from \"vaccines and therapeutics\" to make sure that the negative samples do not contain any information that should be a part of the positive samples.\n\nThey are extracted and matched in the same manner as the postive samples.","6c4c9060":"### part 3-a: load\/install","04640c26":"# Using Novel Language Models and Web scraping to Effectively Identify Articles related to Therapeutics and Vaccines\n * Team: MD-Lab, ASU\n * Author: Jitesh Pabla, Email: jpabla1@asu.edu, Kaggle ID: jiteshpabla\n * Team Members: Rishab Banerjee, Hong Guan, Ashwin Karthik Ambalavanan, Mihir Parmar, Murthy Devarakonda\n * Email ID: loccapollo@gmail.com, hguan6@asu.edu, aambalav@asu.edu, mparmar3@asu.edu, Murthy.Devarakonda@asu.edu\n * Kaggle ID: loccapollo, hongguan, ashwinambal96, mihir3031, murthydevarakonda\n * This is a Team Submission\n * Here are the links to our teams Kernels:\n     - https:\/\/www.kaggle.com\/jiteshpabla\/classifying-cord-19-articles-using-elasticbert\/edit\n     - https:\/\/www.kaggle.com\/ashwinambal96\/scibert-based-article-identification\n     - https:\/\/www.kaggle.com\/hongguan\/micro-scorers-for-covid-19-open-challenge\/\n     - https:\/\/www.kaggle.com\/loccapollo\/lexicon-based-similarity-scoring-with-bert-biobert\n     - https:\/\/www.kaggle.com\/mihir3031\/bert-sts-for-searching-relevant-research-papers\n     - The final ensembling that combines everything together: http:\/\/https:\/\/www.kaggle.com\/hongguan\/ensemble-model-for-covid-19-open-challenge\/","9dc58ac1":"The \"cord19-vaccines-and-therapeutics\" dataset is extracted from Google scholar using the [Publish or Perish tool](https:\/\/harzing.com\/resources\/publish-or-perish) by using the queries like:\n - \"vaccine\"\n - \"therapeutics\"\n - \"Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers\"\n - etc.\n along with the keyword \"coronavirus\"","1d3573e3":"load the metadata file and remove duplicates (if any)","c3544b17":"Using word cloud to visualize the common wordsin the articles of various queries","0d3c6059":"Give class=1 to vaccine; class=2 for therpeutics; and class=0 for other","9cb3b4e1":"splitting the samples equally into train (80%) and test data (20%)","4829b819":"### (Option 2) load the pretrained model from the dataset files","917e337e":"# Part2: creating training and testing dataset","3a288835":"# part1: Extracting Articles from Google","5db4734d":"### part 3-c: do the final predictions","b2deab8b":"## Analysis","d49f2486":"Perform a merge on the \"title\" column the metadata and the Google search results data to find the common articles","f019e60c":"load all the files from the google search results dataset into a dictionary of dataframes (key being the query; value being the dataframe)","45affbac":"View a single row","070fb426":"### part 3-b: train\/load model\n### (Option 1) finetuning the SciBERT model on the dataset","323209d8":"# Introduction\nThis repository deals with the \"cord19-vaccines-and-therapeutics\" dataset which is based on the [\"What do we know about vaccines and therapeutics?\"](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/tasks?taskId=561) task of the COVID-19 Open Research Dataset Challenge (CORD-19).","28daadb6":"The analysis shows that there is not many common articles between \"vaccine\" and other long queries like \"Approaches to evaluate risk for enhanced disease after vaccination\" etc. The same goes for \"therapeutics\" query.\nSince the long queries are very complex and specific we decide to classify the articles into the \"vaccines\" and \"therapeutics\" categories first.","08856081":"## Code","3cdc62d9":"Find the number of common articles in each dataframe (per query dataframe) to guage how much overlap of information exists between each query","429e1fb5":"Generate a normalized score from 0-1 based on the ranks using the formula:\n```\n1 - (rank\/total)\n```\nwhere\n- rank = rank of article\n- total = total number of articles"}}