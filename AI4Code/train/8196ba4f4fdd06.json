{"cell_type":{"4e520456":"code","160515d1":"code","315321de":"code","597284c1":"code","a14ec3dd":"code","fb829719":"code","b30ba215":"code","6b9d74a7":"markdown","0c1b5332":"markdown","87f7f641":"markdown","d1385827":"markdown","b7fd9513":"markdown","d20085dc":"markdown"},"source":{"4e520456":"import pandas as pd\npd.set_option('display.max_columns', 225)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.multiclass import unique_labels\n\nprint('Setup Complete')","160515d1":"file_path = '..\/input\/cat-in-the-dat\/train.csv'\ntrain_raw = pd.read_csv(file_path, index_col='id')\n\nfile_path = '..\/input\/cat-in-the-dat\/test.csv'\ntest_raw = pd.read_csv(file_path, index_col='id')\n\nz = train_raw.ord_5.unique()\n\nfeature_select = 'month'\n\nq = train_raw[feature_select].unique()\nr = test_raw[feature_select].unique()\nq_not_r = []\nr_not_q = []\nboth = []\nt = 0\n\nfor val in q:\n    if val in r:\n        both = both + [val]\n    else:\n        q_not_r = q_not_r + [val]\n\nfor val in r:\n    if val in q:\n        t = t + 1\n    else:\n        r_not_q = r_not_q + [val]\n\nq = train_raw.ord_5.unique()\nq = sorted(q)\nfive = {}\nx = 0\nfor val in q:\n    five[val] = x\n    x = x + 1\n#print(five)\n\ndef ord_five_a(val):\n    return five.get(val)\n\n\n\nq = train_raw.ord_3.unique()\nq = sorted(q)\nthree = {}\nx = 0\nfor val in q:\n    three[val] = x\n    x = x + 1\ndef ord_three_a(val):\n    return three.get(val)\n\n\n\n\nq = train_raw.ord_4.unique()\nq = sorted(q)\nfour = {}\nx = 0\nfor val in q:\n    four[val] = x\n    x = x + 1\n    \ndef ord_four_a(val):\n    return four.get(val)\n\nfeatures = ['bin_0', 'bin_1', 'bin_3', 'bin_4', 'nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5','nom_6', 'nom_7', 'nom_8', 'nom_9', \n            'ord_0', 'ord_1', 'ord_2','ord_3', 'ord_4', 'ord_5', 'day', 'month']\n\ndef hexx(val):\n    return int(val, 16)\n\ndef ord_five(val):\n    return ord_alph(val[0])\n\ndef ord_fiveish(val):\n    return ord_alph(val[1])\n\ndef ord_alph(val):\n#    if val.dtype == \"int\":\n#        return val\n    val = val.lower()\n    if val == 'a':\n        return 1\n    if val == 'b':\n        return 2\n    if val == 'c':\n        return 3\n    if val == 'd':\n        return 4\n    if val == 'e':\n        return 5\n    if val == 'f':\n        return 6\n    if val == 'g':\n        return 7\n    if val == 'h':\n        return 8\n    if val == 'i':\n        return 9\n    if val == 'j':\n        return 10\n    if val == 'k':\n        return 11\n    if val == 'l':\n        return 12\n    if val == 'm':\n        return 13\n    if val == 'n':\n        return 14\n    if val == 'o':\n        return 15\n    if val == 'p':\n        return 16\n    if val == 'q':\n        return 17\n    if val == 'r':\n        return 18\n    if val == 's':\n        return 19\n    if val == 't':\n        return 20\n    if val == 'u':\n        return 21\n    if val == 'v':\n        return 22\n    if val == 'w':\n        return 23\n    if val == 'x':\n        return 24\n    if val == 'y':\n        return 25\n    if val == 'z':\n        return 26\n    else:\n        return 0\n    \ndef ord_five_b(val):\n    if val[0] == val[0].lower():\n        val_0 = ord_alph(val[0]) + 26\n    else:\n        val_0 = ord_alph(val[0])\n    if val[1] == val[1].lower():\n        val_1 = ord_alph(val[1]) + 26\n    else:\n        val_1 = ord_alph(val[1])\n    val_1 = val_1\/100\n    return(val_0 + val_1)\n\ndef ord_two(val):\n    if val == 'Cold':\n        return 2\n    if val == 'Hot':\n        return 4\n    if val == 'Lava Hot':\n        return 6\n    if val == 'Boiling Hot':\n        return 5\n    if val == 'Freezing':\n        return 1\n    if val == 'Warm':\n        return 3\n    else:\n        return 0\n\ndef ord_one(val):\n    if val == 'Grandmaster':\n        return 1\n    if val == 'Expert':\n        return 3\n    if val == 'Novice':\n        return 5\n    if val == 'Contributor': \n        return 4\n    if val == 'Master':\n        return 2\n    else:\n        return 0\n    \ndef nom_four(val):\n    if val == 'Bassoon':\n        return 1\n    if val == 'Piano':\n        return 2\n    if val == 'Theremin':\n        return 3\n    if val == 'Oboe':\n        return 4\n    else:\n        return 0\n    \ndef nom_three(val):\n    if val == 'Finland':\n        return 1\n    if val == 'Russia':\n        return 2\n    if val == 'Canada':\n        return 3\n    if val == 'Costa Rica':\n        return 4\n    if val == 'China':\n        return 5\n    if val == 'India':\n        return 6\n    else:\n        return 0\n    \ndef nom_two(val):\n    if val == 'Snake':\n        return 1\n    if val == 'Hamster':\n        return 2\n    if val == 'Lion':\n        return 3\n    if val == 'Cat':\n        return 4\n    if val == 'Dog':\n        return 5\n    if val == 'Axolotl':\n        return 6\n    else:\n        return 0\n    \ndef nom_one(val):\n    if val == 'Triangle':\n        return 1\n    if val == 'Trapezoid':\n        return 2\n    if val == 'Polygon':\n        return 3\n    if val == 'Square':\n        return 4\n    if val == 'Star':\n        return 5\n    if val == 'Circle':\n        return 6\n    else:\n        return 0\n    \ndef nom_zero(val):\n    if val == 'Green' or val == 0:\n        return 0\n    if val == 'Blue' or val == 1:\n        return 1\n    if val == 'Red' or val == 2:\n        return 2\n    else:\n        return 3\n    \ndef bin_three(val):\n    if val == 'T' or val == 1:\n        return 1\n    if val == 'F' or val == 2:\n        return 2\n    else:\n        return 0\n    \ndef bin_four(val):\n    if val == 'y' or val == 1:\n        return 1\n    if val == 'N' or val == 2:\n        return 2\n    else:\n        return 0\n    \ntrain_raw['nom_0'] = pd.Series([nom_zero(x) for x in train_raw.nom_0], index=train_raw.index)\ntrain_raw['nom_1'] = pd.Series([nom_one(x) for x in train_raw.nom_1], index=train_raw.index)\ntrain_raw['nom_2'] = pd.Series([nom_two(x) for x in train_raw.nom_2], index=train_raw.index)\ntrain_raw['nom_3'] = pd.Series([nom_three(x) for x in train_raw.nom_3], index=train_raw.index)\ntrain_raw['nom_4'] = pd.Series([nom_four(x) for x in train_raw.nom_4], index=train_raw.index)\ntrain_raw['bin_3'] = pd.Series([bin_three(x) for x in train_raw.bin_3], index=train_raw.index)\ntrain_raw['bin_4'] = pd.Series([bin_four(x) for x in train_raw.bin_4], index=train_raw.index)\ntrain_raw['ord_1'] = pd.Series([ord_one(x) for x in train_raw.ord_1], index=train_raw.index)\ntrain_raw['ord_2'] = pd.Series([ord_two(x) for x in train_raw.ord_2], index=train_raw.index)\ntrain_raw['ord_3'] = pd.Series([ord_alph(x) for x in train_raw.ord_3], index=train_raw.index)\ntrain_raw['ord_4'] = pd.Series([ord_alph(x) for x in train_raw.ord_4], index=train_raw.index)\n#train_raw['ord_5_plus'] = pd.Series([x for x in train_raw.ord_5], index=train_raw.index)\n#train_raw['ord_5_plus'] = pd.Series([ord_alph(x[1]) for x in train_raw.ord_5_plus], index=train_raw.index)\ntrain_raw['ord_5'] = pd.Series([ord_five_b(x) for x in train_raw.ord_5], index=train_raw.index)\ntrain_raw['nom_5'] = pd.Series([hexx(x) for x in train_raw.nom_5], index=train_raw.index)\ntrain_raw['nom_6'] = pd.Series([hexx(x) for x in train_raw.nom_6], index=train_raw.index)\ntrain_raw['nom_7'] = pd.Series([hexx(x) for x in train_raw.nom_7], index=train_raw.index)\ntrain_raw['nom_8'] = pd.Series([hexx(x) for x in train_raw.nom_8], index=train_raw.index)\ntrain_raw['nom_9'] = pd.Series([hexx(x) for x in train_raw.nom_9], index=train_raw.index)\ntrain_raw.sample(5)   \n\n#train_raw['ord_5'] = pd.Series([ord_five(x) for x in train_raw.ord_5], index=train_raw.index)\n#train_raw['ord_5_plus'] = pd.Series([x for x in train_raw.ord_5], index=train_raw.index)","315321de":"from sklearn.model_selection import KFold\nfrom sklearn import metrics\n\nkf = KFold(n_splits=6)\n\ncols = ['Model', 'Mean train score', 'Train standard deviation', 'Mean test score', 'Test standard deviation']\nmodel_performance = pd.DataFrame(columns=cols)\n\ndef kfold_eval_model(model, X_, metric=metrics.accuracy_score, update_df=True, printing=True):\n    \"\"\"simple function to test out a model for this problem\n    statement in a single line of code,\n    prints out the f1 score for training and testing data\n    based on results from an n-fold cross validation\n    saves results to a dataframe.\n    will round predictions for regression to use for f1 evaluation.\n    Takes an sklearn model as the first argument,\n    second argument determines if the performance dataframe is updated\"\"\"\n    global model_performance\n    train_f1 = np.array([], dtype='float')\n    test_f1 = np.array([], dtype='float')\n    for train_index, test_index in kf.split(X_):\n#         print(train_index)\n        model.fit(X_.iloc[train_index], y.iloc[train_index])\n        score_1 = metric(y.iloc[train_index], abs((model.predict(X_.iloc[train_index])).round()))\n        score_2 =  metric(y.iloc[test_index], abs((model.predict(X_.iloc[test_index])).round()))\n        train_f1 = np.append(train_f1, score_1)\n        test_f1 = np.append(test_f1, score_2)\n        train_stats = (train_f1.mean(), train_f1.std())\n        test_stats = (test_f1.mean(), test_f1.std())\n    if printing:\n        print(model.__class__.__name__)\n        print('Average train', metric.__name__, 'across', kf.get_n_splits(X),' folds:', train_stats[0], 'with a standard diviation of',train_stats[1])    \n        print('Average test', metric.__name__, 'across', kf.get_n_splits(X),' folds:', test_stats[0], 'with a standard diviation of', test_stats[1])\n    if update_df:\n        s = np.array([[model.__class__.__name__, train_stats[0],  train_stats[1], test_stats[0],  test_stats[1]]])\n        s = pd.DataFrame(s)\n        s.columns = cols\n        model_performance = model_performance.append(s)","597284c1":"from sklearn.preprocessing import PolynomialFeatures\npfe = PolynomialFeatures(degree=2)\n\ny = train_raw.target\nX = train_raw.drop('target', axis=1)\nX_2 = pd.DataFrame(pfe.fit_transform(X))\nX_2 = X.merge(X_2, right_index=True, left_index=True)","a14ec3dd":"from sklearn import tree\nmodel_performance = pd.DataFrame(columns=cols)\ndt = tree.DecisionTreeClassifier(max_depth=5) \nfor num in range(20, 210, 10):\n    dt.__class__.__name__ = str(num) + ' features'\n    kfold_eval_model(dt, X_2[range(num)], printing=False) \ndisplay(model_performance)","fb829719":"from sklearn import tree\nmodel_performance = pd.DataFrame(columns=cols)\ndt = tree.DecisionTreeClassifier(max_depth=15) \nfor num in range(20, 210, 10):\n    dt.__class__.__name__ = str(num) + ' features'\n    kfold_eval_model(dt, X_2[range(num)], printing=False) \ndisplay(model_performance)","b30ba215":"from sklearn import tree\nmodel_performance = pd.DataFrame(columns=cols)\ndt = tree.DecisionTreeClassifier(max_depth=30) \nfor num in range(20, 210, 10):\n    dt.__class__.__name__ = str(num) + ' features'\n    kfold_eval_model(dt, X_2[range(num)], printing=False) \ndisplay(model_performance)","6b9d74a7":"k-fold is used to determine the accuracy of the model with a greater degree of confidence. I reused the code for this function from a previous project: https:\/\/github.com\/Neil-Kloper\/u2n6lk8eWLzVfJ3M\/blob\/main\/Apziva_eval_problem_Final.ipynb\n\nThe output from this is a dataframe that stores the average accuracy for the models, as well as the standard deviation of accuracy between folds.","0c1b5332":"Next, I will repeat the same expirement, this time using 15 for the max depth.","87f7f641":"I will use polynomial feature extraction to generate additional features so that the added features are not wholely without value. I will run through multiple iterations of multiple features and max depths for the decision trees.","d1385827":"On reddit someone had asked the question: \"[D] the more features we have, the more accurate is a decision tree (always)?\" and there were a few educated guesses as to the outcome on training data (assuming for a fixed depth tree). The goal of this notebook is to determine if training accuracy will improve or decrease as more features are added. The secondary objective is to see what change in accuracy one could recieve on validation given the same circumstances.\n\nI chose the Cat in the dat challenge dataset as it is sufficiently large to believe the results should generalize well and because I could reuse the data prepairing code I already wrote from when I tried this competition.\n\n\nOriginal thread: https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/lfj0r5\/d_the_more_features_we_have_the_more_accurate_is\/\n\nThe first codeblock handles the data preprocessing, the cell was hidden to keep the notebook length down, feel free to unhide it and look it over should you choose. Basically, all ordinal features were label encoded based on order, all non ordinal categorical features were randomly label encoded (tree based algorithms can make sence of these features so long as cardinality isn't extreme), and the hexidecial values were returned to base 10.","b7fd9513":"Finally I will repeat the test with a max depth of 30","d20085dc":"Here I test the Decision Trees performance given a max depth of 5"}}