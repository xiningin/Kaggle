{"cell_type":{"8badc0b3":"code","db63c47f":"code","fef7011b":"code","f8804d70":"code","07d0c03d":"code","145bef46":"code","1fbb122c":"code","56ed1839":"code","281b2fdf":"code","1e37fb46":"code","12dda68f":"code","bcb24e96":"code","e611af35":"code","326302d8":"code","0861339d":"code","bdd8529a":"code","9f12d08e":"markdown","3fb20a72":"markdown","f4f54e06":"markdown","574ef359":"markdown","ce3f847f":"markdown","c71f7b42":"markdown","25f0819f":"markdown","cde0499e":"markdown","63f7dd39":"markdown","080b92de":"markdown","babc6c58":"markdown","a3da2fd1":"markdown"},"source":{"8badc0b3":"a = input('Name or enrollment no. :- ')","db63c47f":"# Importing all modules or library\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split, cross_val_score","fef7011b":"input_file = '..\/input\/navie-bayes-example-data\/data_multivar_nb.txt' # Giving the file path to read our data\n\n# Load data from input file\ndata = np.loadtxt(input_file, delimiter=',')\n\n# Here the speration of the data is done {X, y}\nX, y = data[:, :-1], data[:, -1]","f8804d70":"# Have already import the GaussianNB in importing part\n# From navie bayes, which is from sklearn\n# Create Navies Bayes Classifier\nclassifier = GaussianNB()\n\n# Train the Classifier\nclassifier.fit(X,y)","07d0c03d":"# Predict the values for training data\ny_pred = classifier.predict(X)\n\n# Compute accuracy\naccuracy = 100.0 * (y == y_pred).sum() \/ X.shape[0]\nprint(\"Accuracy of Naive Bayes classifier =\", round(accuracy, 2), \"%\")","145bef46":"# Defining the Visualizer \ndef visualize_classifier(classifier, X, y):\n    # Define the minimum and maximum values for X and Y\n    # that will be used in the mesh grid\n    min_x, max_x = X[:, 0].min() - 1.0, X[:, 0].max() + 1.0\n    min_y, max_y = X[:, 1].min() - 1.0, X[:, 1].max() + 1.0\n\n    # Define the step size to use in plotting the mesh grid \n    mesh_step_size = 0.01\n\n    # Define the mesh grid of X and Y values\n    x_vals, y_vals = np.meshgrid(np.arange(min_x, max_x, mesh_step_size), np.arange(min_y, max_y, mesh_step_size))\n\n    # Run the classifier on the mesh grid\n    output = classifier.predict(np.c_[x_vals.ravel(), y_vals.ravel()])\n\n    # Reshape the output array\n    output = output.reshape(x_vals.shape)\n\n    # Create a plot\n    plt.figure()\n\n    # Choose a color scheme for the plot \n    plt.pcolormesh(x_vals, y_vals, output, cmap=plt.cm.gray)\n\n    # Overlay the training points on the plot \n    plt.scatter(X[:, 0], X[:, 1], c=y, s=75, edgecolors='black', linewidth=1, cmap=plt.cm.Paired)\n\n    # Specify the boundaries of the plot\n    plt.xlim(x_vals.min(), x_vals.max())\n    plt.ylim(y_vals.min(), y_vals.max())\n\n    # Specify the ticks on the X and Y axes\n    plt.xticks((np.arange(int(X[:, 0].min() - 1), int(X[:, 0].max() + 1), 1.0)))\n    plt.yticks((np.arange(int(X[:, 1].min() - 1), int(X[:, 1].max() + 1), 1.0)))\n    plt.title(f'{a}')\n    plt.show()","1fbb122c":"# Visualize the performance of the classifier\nvisualize_classifier(classifier, X, y)","56ed1839":"# Cross validation \n\n# Split data into training and test data \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n\n#Creating a classifier\nclassifier_new = GaussianNB()\n\n#Train the classifier\nclassifier_new.fit(X_train, y_train)\n\n# Predict the test data\ny_test_pred = classifier_new.predict(X_test)","281b2fdf":"# compute accuracy of the classifier\naccuracy = 100.0 * (y_test == y_test_pred).sum() \/ X_test.shape[0]\nprint(\"Accuracy of the new classifier =\", round(accuracy, 2), \"%\")\n\n# Visualize the performance of the classifier\nvisualize_classifier(classifier_new, X_test, y_test)\n\nprint('Predicted output :-', y_test_pred)","1e37fb46":"# Scoring functions\n\nnum_folds = 3\n\naccuracy_values = cross_val_score(classifier, X, y, scoring='accuracy', cv=num_folds)\nprint(\"Accuracy: \" + str(round(100*accuracy_values.mean(), 2)) + \"%\")\n\nprecision_values = cross_val_score(classifier, X, y, scoring='precision_weighted', cv=num_folds)\nprint(\"Precision: \" + str(round(100*precision_values.mean(), 2)) + \"%\")\n\nrecall_values = cross_val_score(classifier, X, y, scoring='recall_weighted', cv=num_folds)\nprint(\"Recall: \" + str(round(100*recall_values.mean(), 2)) + \"%\")\n\nf1_values = cross_val_score(classifier, X, y, scoring='f1_weighted', cv=num_folds)\nprint(\"F1: \" + str(round(100*f1_values.mean(), 2)) + \"%\")","12dda68f":"# Q1 solution:\nimport pandas as pd\ndf = pd.read_csv('..\/input\/navie-bayes-example-data\/data_multivar_nb.txt', header = None)","bcb24e96":"# Removing one class from the data \ndf = df[df[2] != 3]\n\n# now putting all the values together \nx_1 = df[0]\nx_2 = df[1]\nX = list(zip(x_1, x_2))\nX = np.array(X)\ny = df[2].to_numpy()","e611af35":"# Cross validation \n\n# Split data into training and test data \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n\n#Creating a classifier\nclassifier_new_1 = GaussianNB()\n\n#Train the classifier\nclassifier_new_1.fit(X_train, y_train)\n# Predict the test data\ny_test_pred = classifier_new_1.predict(X_test)","326302d8":"# compute accuracy of the classifier\naccuracy = 100.0 * (y_test == y_test_pred).sum() \/ X_test.shape[0]\nprint(\"Accuracy of the new classifier =\", round(accuracy, 2), \"%\")\n\n# Visualize the performance of the classifier\nvisualize_classifier(classifier_new_1, X_test, y_test)\n\nprint('Predicted output :-', y_test_pred)","0861339d":"# Cross validation \n\n# Split data into training and test data \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)\n\n#Creating a classifier\nclassifier_new_2 = GaussianNB()\n\n#Train the classifier\nclassifier_new_2.fit(X_train, y_train)\n# Predict the test data\ny_test_pred = classifier_new_1.predict(X_test)","bdd8529a":"# compute accuracy of the classifier\naccuracy = 100.0 * (y_test == y_test_pred).sum() \/ X_test.shape[0]\nprint(\"Accuracy of the new classifier =\", round(accuracy, 2), \"%\")\n\n# Visualize the performance of the classifier\nvisualize_classifier(classifier_new_1, X_test, y_test)\n\nprint('Predicted output :-', y_test_pred)","9f12d08e":"### Here you can see that wrong classification is done by the classifier","3fb20a72":"# In simple Terms:\n\n## Mathematics for Navie Bayes:\n\n* **The first part of training a naive bayes classifier is to identify the number of classes that you have.**\n* **You will create a probability for each class.**\n\n>**$P(D_{0})$ is the probability that the input belong to class 0.**\n\n>**$P(D_{1})$ is the probability that the input belong to class 1.**\n\n>**$P(D_{2})$ is the probability that the input belong to class 2.**\n\n**Use the formulas as follows and store the values in a dictionary:**\n\n$$P(D_{0}) = \\frac{D_{0}}{D}\\tag{1}$$\n\n$$P(D_{1}) = \\frac{D_{1}}{D}\\tag{2}$$\n\n$$P(D_{2}) = \\frac{D_{2}}{D}\\tag{3}$$\n\n- **Where D is our data which we are using, $D_0$ is Class 0 data, $D_1$ is Class 1 data, $D_2$ is Class 2 data.**","f4f54e06":"### Classifier model socer is looking good....\n\n### Thank you and If you like... please UpVote this notebook..\ud83d\udc4d\n\n## Refernces:-\n\n> https:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html\n\n> https:\/\/www.cs.unb.ca\/~hzhang\/publications\/FLAIRS04ZhangH.pdf","574ef359":"#### Now the classifier is prefectly classifing our data","ce3f847f":"{**Note:- We can also use PANDAS (Data-science) library, this library is mostly used for reading, analysis and manipulation of data** \n\n>```\nimport pandas as pd\ndata = pd.read_csv('<file path>', header = None)\n    \nby this we can load the ``.txt`` file in it, by we are using ``.read_csv``}","c71f7b42":"## Gaussian Navies Bayes\n\n`GaussianNB` implements the Gaussian Navies Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian:\n\n$$P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)$$\n\nThe parameters $\\sigma_y$ and $\\mu_y$ are estimated using maximum likelihood","25f0819f":"### In below section we are actually dividing our data into to main parts which are:\n\n> **1. Trainig data , `{X_train, y_train}`**\n\n> **2. Testing data , `(X_test, y_test)`**\n\n\n### Now by using `train_test_split` we can split the data as:\n\n>**For Training data is about `80%` which is `0.80`**\n\n>**For Testing data is about `20%` which is `0.2`**\n\n**We can change the size of training and testing data set, by just varing ``test_size``**","cde0499e":"# Navie Bayes Algorithm \n\n**Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes\u2019 theorem with the \u201cnaive\u201d assumption of conditional independence between every pair of features given the value of the class variable. Bayes\u2019 theorem states the following relationship, given class variable $y$ and dependent feature vector $x_1$ through $x_n$:**\n\n**$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots, x_n \\mid y)}\n                                 {P(x_1, \\dots, x_n)}$$**\n                                 \n**Using the naive conditional independence assumption that**\n\n$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}\n                                 {P(x_1, \\dots, x_n)}$$\n                                 \n\n**for all $i$, this relationship is simplified to:**\n\n$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}\n                                 {P(x_1, \\dots, x_n)}$$\n\n\n**Since $P(x_1,..., x_n)$ is constant given the input, we can use the following classification rule:**\n$$\\begin{align}\\begin{aligned}P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)\\\\\\Downarrow\\\\\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y),\\end{aligned}\\end{align}$$\n\n**and we can use Maximum A Posteriori(MAP) estimation to estimate $P(y)$ and $P(x_i | y)$, the former is then the relative frequency of class $y$ in the training set.**\n\n**The different Navies Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $P(x_i | y)$.**\n\n**In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering. They require a small amount of training data to estimate the necessary parameters. (For theoretical reasons why naive Bayes works well, and on which types of data it does, see the references below.)**\n\n**Naive Bayes learners and classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality.**\n\n**On the flip side, although naive Bayes is known as a decent classifier, it is known to be a bad estimator, so the probability outputs from ``predict_proba`` are not to be taken too seriously.**","63f7dd39":"## Modules used for this:\n\n1. **`numpy` :- Numerical Python used for mathematical operations and scientific calculations.**\n\n2. **`matplotlib` :- A visualization tool used for plotting purpose.**\n\n3. **`sklearn` :- Scikit-learn is a ML library, used to apply different ML algorithm, Which are already defined in it**.","080b92de":"## For practice Purpose:-\n\n> **Q1). Perform the same Experiment by removing class 3 data from the \u201cdata_multivar.txt\u201d file.**\n\n> **Q2). Perform the same experiment by changing test size to 30% from 25%. Evaluate the results.**","babc6c58":"### Q1. solution:- ","a3da2fd1":"### Q2. Solution:- "}}