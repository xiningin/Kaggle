{"cell_type":{"f19e6334":"code","e9798a38":"code","9c4020fa":"code","dde5d622":"code","efe9085f":"code","af245e79":"code","f3bc4f2f":"code","c1b1dc0c":"code","e549c7ae":"code","b4e3c80d":"code","b132b09e":"code","da270038":"code","441752ac":"code","caadacc1":"code","280172a3":"markdown","9ade1aed":"markdown"},"source":{"f19e6334":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e9798a38":"data_path = \"\/kaggle\/input\/simple-preprocessing\/\"","9c4020fa":"import pickle\n\nwith open(data_path+\"\/df_train.pkl\", \"rb\") as myfile:\n    df_train = pickle.load(myfile)","dde5d622":"with open(data_path+\"\/df_test.pkl\", \"rb\") as myfile:\n    df_test = pickle.load(myfile)","efe9085f":"#How many iterations of Hyperopt search to run\n#if you enable GPU below with use_gpu=True (and in kernel), this can be much higher. \n# But Kaggle limits GPU use so much, I use CPU here :) (or maybe just a quickie..)\nN_ROUNDS_CATB = 25\n#This is just the N folds for the final train\/predict after running the hyperopt search\nN_FOLDS = 10","af245e79":"df_train.columns","f3bc4f2f":"df_test.columns","c1b1dc0c":"target = df_train[\"target\"]\n","e549c7ae":"df_train.drop(\"target\", inplace=True, axis=1)","b4e3c80d":"from optimizers import *\n\ncatb_opt = CatboostOptimizer()\ncatb_opt.n_trials = N_ROUNDS_CATB\ncatb_opt.n_folds = N_FOLDS\ncatb_opt.use_gpu = True\ncatb_opt.cat_features = categorical_indices(df_train)\nX_cols = df_train.columns\ncatb_results = catb_opt.classify_binary(X_cols, df_train, df_test, target)\n","b132b09e":"submission = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/sample_submission.csv\")\nsubmission.head()","da270038":"submission[\"target\"] = catb_results.predictions[:, 1]\nsubmission.head()","441752ac":"submission.to_csv(\"submission_catb.csv\", index=False)","caadacc1":"!head submission_catb.csv","280172a3":"# Hello\n\n This is just a simple kernel so I can demonstrate utility scripting. For the [utility scripting competition](https:\/\/www.kaggle.com\/general\/109651). This runs a hyperopt search on Catboost, creates predictions, outputs feature importances, etc...\n \n Used scripts:\n \n - [Optimizer utils](https:\/\/www.kaggle.com\/donkeys\/optimizer-utils)\n - [Optimizers](https:\/\/www.kaggle.com\/donkeys\/optimizers) built on those utils\n\nNeeds some work but feedback and improvements are welcome.. :)","9ade1aed":"How many optimization iterations to run:"}}