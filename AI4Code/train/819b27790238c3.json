{"cell_type":{"68caf183":"code","3fd25ee5":"code","546b56f5":"code","af5d7fc9":"code","19a77cdf":"code","e84a9809":"code","dd2ae558":"code","bd88fa27":"code","ef7e8154":"code","308ff6b7":"code","cc411a57":"code","dda924a3":"code","22f33316":"code","5026e05c":"code","fb84bc91":"code","0c75845b":"code","192ffac1":"code","fd97800c":"code","5dd0c80f":"code","897d902d":"code","40ec3577":"code","5d132f21":"code","9d8b2d45":"markdown","f715f4fa":"markdown","21de5e29":"markdown","664a84c0":"markdown"},"source":{"68caf183":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3fd25ee5":"##\u3069\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u3082\u4f7f\u3046\u57fa\u672c\u7684\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\n#Basic library \n# Reading files from directory\nimport os\nimport pickle\n \n# Data manipulation & analysis\nimport pandas as pd\npd.set_option('display.max_columns',100)\npd.set_option('display.max_rows', 500)\nimport datetime as dt\n \nimport numpy as np\nimport scipy\n\n# Visualisation\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n \n # \u5b9f\u884c\u306b\u95a2\u4fc2\u306a\u3044\u8b66\u544a\u3092\u7121\u8996\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom tqdm import tqdm\nimport gc\nimport json\nimport math\n\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import accuracy_score,roc_auc_score,log_loss\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\n\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nimport category_encoders as ce\nimport plotly.express as px\n\nfrom lightgbm import LGBMClassifier\nimport lightgbm","546b56f5":"#\u5b9f\u884c\u74b0\u5883\u304c\u3069\u3053\u306e\u968e\u5c64\u306b\u3042\u308b\u304b\u3001\u30d1\u30b9\u3092\u691c\u7d22\nimport os\nprint(os.getcwd())","af5d7fc9":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")","19a77cdf":"train.drop(['id'],axis=1).describe().T.style.bar(subset=['mean'],color=px.colors.qualitative.G10[1]).background_gradient(subset=['std'],cmap='Blues').background_gradient(subset=['50%'],cmap='BuGn')","e84a9809":"test.drop(['id'],axis=1).describe().T.style.bar(subset=['mean'],color=px.colors.qualitative.G10[1]).background_gradient(subset=['std'],cmap='Blues').background_gradient(subset=['50%'],cmap='BuGn')","dd2ae558":"def diff_color(x):\n    color = 'red' if x<0 else ('green' if x > 0 else 'black')\n    return f'color: {color}'\n\n(train.describe() - test.describe())[test.columns].T.iloc[1:,1:].style\\\n        .bar(subset=['mean', 'std'], align='mid', color=['#d65f5f', '#5fba7d'])\\\n        .applymap(diff_color, subset=['min', 'max'])","bd88fa27":"display(train.info())\ndisplay(test.info())","ef7e8154":"fig, ax = plt.subplots(figsize=(12, 6))\nsns.countplot(x='target', data=train)\nax.set_title('Target Distribution')","308ff6b7":"train.columns","cc411a57":"#--------------------------------\n#\u6570\u5024\u306e\u7279\u5fb4\u91cf\u3000\u203b\u4e0a\u7d1a\u8005\u306f\u3001\u7279\u5fb4\u91cf\u306e\u30ea\u30b9\u30c8\u3092\u4f5c\u6210\u3057\u3066\u3044\u308b\u3002\n#------------------------------\nfeatures_cat = ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4',\n       'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9',\n       'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14',\n       'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19',\n       'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24',\n       'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29',\n       'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34',\n       'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39',\n       'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44',\n       'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49',\n       'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54',\n       'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59',\n       'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64',\n       'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69',\n       'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74',],\nfeatures_target = ['target'] ","dda924a3":"train","22f33316":"##--------------------------------------------\n#\u30ab\u30c6\u30b4\u30eafeature  \u3000\u30e9\u30d9\u30eb\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\n#---------------------------------------------\nfor feature in features_target :\n    le = LabelEncoder()\n    le.fit(train[feature])\n    train[feature] = le.transform(train[feature])","5026e05c":"# \u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u7279\u5fb4\u91cf\u3068\u76ee\u7684\u5909\u6570\u306b\u5206\u3051\u308b  \u6b63\u89e3\u30e9\u3079\u30eb\u3092\u8a18\u5165\ntrain_x = train.drop(['target'], axis=1)\ntrain_y = train['target']\n# \u5909\u6570Id\u3092\u9664\u5916\u3059\u308b\ntrain_x = train_x.drop(['id'], axis=1)\ntest_x = test.drop(['id'], axis=1)","fb84bc91":"test_x","0c75845b":"## Trial \u95a2\u6570\u3067\u6700\u9069\u5316\u3092\u63a2\u7d22\n##\u8a08\u7b97\u91cf\u3092\u6e1b\u3089\u3059\u305f\u3081\u306b\u3001Hold-out\u6cd5\u30672:8\u306b\u5206\u5272\n##\u30d1\u30e9\u30e1\u30fc\u30bf\u63a2\u7d22\u306f\u91cd\u8981\u30d1\u30e9\u30e1\u30fc\u30bf\u3092suggest_int(\u6574\u6570\u3067\u5168\u90e8\u63a2\u7d22) , \u305d\u306e\u4ed6\u3092suggest_categorical\uff08\u30ea\u30b9\u30c8\u5185\u3092\u9078\u629e\uff09\n\ndef objective(trial,data=train_x,target=train_y):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.2,random_state=71)\n    params = {\"objective\": \"multiclass\",\n        'reg_alpha' : trial.suggest_loguniform('reg_alpha' , 1e-2 , 1000),\n        'reg_lambda' : trial.suggest_loguniform('reg_lambda' , 1e-1 , 10000),\n        'num_leaves' : trial.suggest_int('num_leaves' , 42, 42),#FIX\n        #'num_leaves' : trial.suggest_int('num_leaves' , 3 , 400),\n        'learning_rate' : trial.suggest_float('learning_rate' , 0.03 , 0.03),#FIX\n        'max_depth' : trial.suggest_int('max_depth' , 3 , 80),\n        'n_estimators' : trial.suggest_int('n_estimators' , 1 ,20000),\n        'min_child_samples' : trial.suggest_int('min_child_samples' , 40 , 100),\n        'min_child_weight' : trial.suggest_loguniform('min_child_weight' ,0.0004586402479388673 , 0.0004586402479388673),#FIX\n        #'min_child_weight' : trial.suggest_loguniform('min_child_weight' , 1e-5 , 1),\n        #'subsample' : trial.suggest_float('subsample' , 0.01 , 1.0), \n        'subsample' : trial.suggest_float('subsample' , 0.84327 , 0.84327), #FIX\n        'colsample_bytree' : trial.suggest_float('colsample_bytree' , 0.234 , 0.234),#FIX\n        #'colsample_bytree' : trial.suggest_float('colsample_bytree' , 0.01 , 1),\n            }\n    model = LGBMClassifier(**params)  \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],eval_metric='multi_logloss',early_stopping_rounds=50,verbose=False)\n        \n    preds_opt = model.predict_proba(test_x)\n \n\n    log_loss_multi = log_loss(test_y, preds_opt)\n    return log_loss_multi","192ffac1":"import optuna\nfrom sklearn.model_selection import train_test_split\n\nfrom lightgbm import LGBMClassifier\nOPTUNA_OPTIMIZATION = True\n\nstudy = optuna.create_study(direction='minimize') # or maximize \/ \"minimize\"\nstudy.optimize(objective, n_trials=35)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","fd97800c":"if OPTUNA_OPTIMIZATION:\n    display(optuna.visualization.plot_optimization_history(study))\n    display(optuna.visualization.plot_slice(study))","5dd0c80f":"#Number of finished trials: 35 \n#Best trial: {'reg_alpha': 16.724382543126165, 'reg_lambda': 4.4252351797809535, 'num_leaves': 42, 'learning_rate': 0.03, 'max_depth': 73, 'n_estimators': 6290, 'min_child_samples': 47, 'min_child_weight': 0.0004586402479388673, 'subsample': 0.84327, 'colsample_bytree': 0.234}","897d902d":"##############################\n#######     LGBMClassifier by tunner\n################################\n\ntrain_oof_lgbm_0 = np.zeros((len(train_x), 9))\ntemp_test = np.zeros((len(test_x), 9))\n\n\nlgbm_params = study.best_trial.params\n\n#{'task': 'train',\n#        'objective': 'multiclass', # \u76ee\u7684 : \u591a\u30af\u30e9\u30b9\u5206\u985e \n#        'num_class': 4, # \u30af\u30e9\u30b9\u6570 : 4\n#        'metric': 'multi_logloss', # \u8a55\u4fa1\u6307\u6a19  \n#        # \u4ed6\u306b\u306f'multi_logloss'\u306a\u3069 }\n\n\nNUM_FOLDS = 10\nkf =  StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=137)\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train_x, train_y))):\n        print(f'Fold {f+1}')\n        train_df = train_x.iloc[train_ind].reset_index(drop=True)\n        val_df = train_x.iloc[val_ind].reset_index(drop=True)\n        train_target = train_y.iloc[train_ind].reset_index(drop=True)\n        val_target = train_y.iloc[val_ind].reset_index(drop=True)\n\n        model = LGBMClassifier(**lgbm_params)\n        model =  model.fit(train_df, train_target,eval_set=[(val_df,val_target)],early_stopping_rounds=50,verbose=False)\n        \n        temp_oof = model.predict_proba(val_df)\n        print(log_loss(val_target, temp_oof))\n        train_oof_lgbm_0[val_ind] = temp_oof\n\n        temp_test += model.predict_proba(test_x)\n        test_preds_lgbm_0 = temp_test\/NUM_FOLDS\n\n        \nprint('All_logloss',log_loss(train_y, train_oof_lgbm_0))\nnp.save('train_oof_lgbm_0', train_oof_lgbm_0 ) #for validation\nnp.save('test_preds_lgbm_0',test_preds_lgbm_0 ) #for submission","40ec3577":"#\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u7528\ntest_preds = test_preds_lgbm_0\n\ntest_preds = np.clip(test_preds,0.002500, 0.941)","5d132f21":"##### \u63d0\u51fa\u7528\u30d5\u30a1\u30a4\u30eb\u306e\u4f5c\u6210 \u30d8\u30c3\u30c0\u30fc\u7121\u8a2d\u306e\u8a2d\u5b9a\nsubmission = pd.DataFrame( test_preds)\nsubmission.columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9']\nsubmission['id'] = test['id']\nsubmission = submission[['id', 'Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9']]\n\nsubmission.to_csv(\"submission_lgbm_0.csv\", index=False)\ndisplay(submission.head(), submission.tail())","9d8b2d45":"![\u30ad\u30e3\u30d7\u30c1\u30e32.PNG](attachment:19a60ec0-8220-47d1-a795-8bdcb516c52c.PNG)","f715f4fa":"# importance\u3092\u8868\u793a\u3059\u308b\nimport shap\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(test_x)\nshap.summary_plot(shap_values, train_x)","21de5e29":"The first step in kaggle is to get better results in an easy way.\n\nKaggle\u306e\u7b2c\u4e00\u6b69\u306f\u697d\u306f\u65b9\u6cd5\u3067\u3088\u308a\u826f\u3044\u7d50\u679c\u3092\u3048\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002\n\nLightGBM+ optuna\u3000is a very good tool for first trail.\n\nLightGBM+ optuna \u306f\u521d\u3081\u306e\u624b\u306b\u306f\u3068\u3066\u3082\u826f\u3044\u30c4\u30fc\u30eb\u306e\u4e00\u3064\u3067\u3059\u3002\n\nRepeat to optimazed (Parametr FIX) , Public Score is UP step by step !!\n\n**Public Score\u3000 1.75345 \u21921.74909\u3000\u21921.74906\u3000\u21921.74901\u3000\u21921.74880**\n\n\u6700\u9069\u5316\u3092\u7e70\u308a\u8fd4\u3059\u3053\u3068\u3067\u3001Public Score\u306f\u6bb5\u3005\u3068\u4e0a\u6607\u3057\u3066\u884c\u304d\u307e\u3059\u3002\n\nI recommend that you use this Easy-Way study for your next\u3000deep exploration.\n\n\u3053\u306e\u65b9\u6cd5\u3092\u4f7f\u3063\u3066\u6b21\u306e\u6df1\u3044\u63a2\u7d22\u3092\u3059\u308b\u3053\u3068\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002\n\nDon't forget to vote !!","664a84c0":"![\u30ad\u30e3\u30d7\u30c1\u30e3.PNG](attachment:b8094ecc-edec-45d4-b663-21fdf668bfd9.PNG)"}}