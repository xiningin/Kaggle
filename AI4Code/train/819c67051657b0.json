{"cell_type":{"bd54a2dd":"code","aafd2a76":"code","7a3000f2":"code","5b501b67":"code","0d9538ef":"code","fe3d6981":"markdown","93f48171":"markdown","cb170b23":"markdown","534d7ed8":"markdown","0a9da4e5":"markdown","70d155f3":"markdown","87b20639":"markdown","8faac0b2":"markdown","bdd05166":"markdown"},"source":{"bd54a2dd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nprint(\"Check project files\")\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nprint(\"---------------------------------------------------------------------\\n\\n\\n\")\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\ndf = pd.read_csv(\"..\/input\/online-retail-ii-data-set-from-ml-repository\/Year 2010-2011.csv\", encoding= 'unicode_escape')\ndf.head()","aafd2a76":"# import matplotlib.pyplot as plt\nimport numpy as np\n\ndf.info()\nprint(\"\\n\\n\\n---------------------------------------------------------------------\\n\\n\\n\")\ndf.describe().T\n\n# Plot\n\n# plt.scatter(np.arange(len(df.Invoice)), df.Invoice, c=('r', 'b'), alpha= 0.5) \n# plt.title('quantity of products purchased') \n# plt.xlabel('x') \n# plt.ylabel('y') \n# plt.show()\n\n\n\n","7a3000f2":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nprint(\"--------------------------------------------------------------Price\\n\")\nmean_ = df['Price'].mean()\nstd_ = df['Price'].std()\nmin_ = df['Price'].min()\nmax_ = df['Price'].max()\nQ1_ = df['Price'].quantile(q=0.25)\nQ3_ = df['Price'].quantile(q=0.75)\nprint(f\"Min: {min_}\\nQ1: {Q1_}\\nMax: {max_}\\nQ3: {Q3_}\\nMean: {mean_}\\nStd: {std_}\\n\")\n\nprint(\"--------------------------------------------------------------Quantity\\n\")\nmean_ = df['Quantity'].mean()\nstd_ = df['Quantity'].std()\nmin_ = df['Quantity'].min()\nmax_ = df['Quantity'].max()\nQ1_ = df['Quantity'].quantile(q=0.25)\nQ3_ = df['Quantity'].quantile(q=0.75)\nprint(f\"Min: {min_}\\nQ1: {Q1_}\\nMax: {max_}\\nQ3: {Q3_}\\nMean: {mean_}\\nStd: {std_}\\n\")\n\n# categorical variables have blank printed data?\nlis = [pd.isna(i) for i in df if df[i].dtypes == 'object']\nprint(f\"CHECK: does it have Nan value --> {any(lis)}\")\n","5b501b67":"# Preprocessing\n\ndf.drop(df[df['StockCode'] == 'POST'].index, inplace=True) # POST = cargo\ndf = df[~df['Invoice'].str.contains(\"C\", na=False)] # invoice = C ==> sale canceled\ndf = df[df['Quantity'] > 0]\ndf = df[df['Price'] > 0]\n\nPQ1_ = df['Price'].quantile(q=0.25)\nPQ3_ = df['Price'].quantile(q=0.75)\nQQ1_ = df['Quantity'].quantile(q=0.25)\nQQ3_ = df['Quantity'].quantile(q=0.75)\n# Outlier tolerance calculated.\np_interquantile = PQ3_ - PQ1_\nq_interquantile = QQ3_ - QQ1_\n\np_up_limit = PQ3_ + 1.5 * p_interquantile\np_low_limit = PQ1_ - 1.5 * p_interquantile\n\nq_up_limit = QQ3_ + 1.5 * q_interquantile\nq_low_limit = QQ1_ - 1.5 * q_interquantile\n\ndf.loc[(df['Quantity'] < q_low_limit ), 'Quantity'] = q_low_limit\ndf.loc[(df['Quantity'] > q_up_limit ), 'Quantity'] = q_up_limit\n\ndf.loc[(df['Price'] < p_low_limit ), 'Price'] = p_low_limit\ndf.loc[(df['Price'] > p_up_limit ), 'Price'] = p_up_limit\n\n","0d9538ef":"#RFM**\nfrom mlxtend.frequent_patterns import apriori, association_rules\n\n##################################################\n# Desc       product1 product2 .. poduct(n)      #\n#                                                #\n# Invoice                                        #\n# id1                                            #\n# id2                                            #\n# ...                                            #\n# id(n)                                          #\n##################################################\n\ndf = df[df['Country'] == 'Germany']\ndf = df.groupby(['Invoice','StockCode'])['Quantity'].sum().unstack().fillna(0).applymap(lambda x: 1 if x > 0 else 0)\nfrequent_itemsets = apriori(df, min_support=0.01, use_colnames=True)\nrules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.01) # confidence: Probability of receiving object A also receives object B\nprint(rules)\n\n# df = df[df['Country']=='Germany']\n# df.groupby(['Invoice','StockCode'])['Quantity'].sum().unstack().fillna(0).applymap(lambda x: 1 if x > 0 else 0)\n# frequent_itemsets = apriori(df, min_support=0.01, use_colnames=True)\n\n# rules = association_rules(frequent_itemsets, metric=\"support\", min_threshold=0.01)\n\n\nsorted_rules = rules.sort_values(\"lift\", ascending=False) # Lift: By how many times does buying object A increase the rate of purchasing object B?\nproduct_id = 22492\nfor i, product in enumerate(sorted_rules[\"antecedents\"]):\n    #print(f\"\\n\\nantecedents products: {product}\\n\\n\")\n    for j in list(product):\n        if j == product_id:\n            recommendation_list.append(list(sorted_rules.iloc[i][\"consequents\"])[0]) # index 0 but there may be more than one product that I can recommend.\n\n# So ..\n\ndef arl_recommender(rules_df, product_id, rec_count=1):\n    sorted_rules = rules_df.sort_values(\"lift\", ascending=False)\n    recommendation_list = []\n    for i, product in sorted_rules[\"antecedents\"].items():\n        for j in list(product):\n            if j == product_id:\n                recommendation_list.append(list(sorted_rules.iloc[i][\"consequents\"]))\n    recommendation_list = list({item for item_list in recommendation_list for item in item_list}) # set singularizes..\n    return recommendation_list[:rec_count]\n\n#I wrote a method that can make multiple suggestions.\n\n\n\n#if u want view product of id --> u can use check function.\n\n#check \n# def check_id(stock_code):\n#     product_name = df[df['StockCode'] == stock_code ][['Description']].value[0].tolist()\n#print(check_id(df, 23049))\n\nprint(arl_recommender(rules, product_id = 22492, rec_count = 1))\nprint(arl_recommender(rules, product_id = 22492, rec_count = 2))","fe3d6981":"<div id=\"exploration\"><h1> <center> \ud83d\udcda DATA EXPLORATION <\/center><\/div>","93f48171":" <h6><b>Source:<\/b><\/h6>\n\nDr. Daqing Chen, Course Director: MSc Data Science. chend '@' lsbu.ac.uk, School of Engineering, London South Bank University, London SE1 0AA, UK.\n\n\n <h6><b>Data Set Information:<\/b><\/h6>\n\nThis Online Retail II data set contains all the transactions occurring for a UK-based and registered, non-store online retail between 01\/12\/2009 and 09\/12\/2011.The company mainly sells unique all-occasion gift-ware. Many customers of the company are wholesalers.\n\n <h5><b>Attribute Information:<\/b><\/h5>\n\n* **InvoiceNo:** Invoice number. Nominal. A 6-digit integral number uniquely assigned to each transaction. If this code starts with the letter 'c', it indicates a cancellation.\n* **StockCode:** Product (item) code. Nominal. A 5-digit integral number uniquely assigned to each distinct product.\n* **Description:** Product (item) name. Nominal.\n* **Quantity:** The quantities of each product (item) per transaction. Numeric.\n* **InvoiceDate:** Invice date and time. Numeric. The day and time when a transaction was generated.\n* **UnitPrice:** Unit price. Numeric. Product price per unit in sterling (\u00c2\u00a3).\n* **CustomerID:** Customer number. Nominal. A 5-digit integral number uniquely assigned to each customer.\n* **Country:** Country name. Nominal. The name of the country where a customer resides.","cb170b23":"\n<h1>RFM - Recency Frequency Monetary<\/h1>\n\nRFM is a method used for analyzing customer value. It is commonly used in database marketing and direct marketing and has received particular attention in retail and professional services industries.\n\nRFM stands for the three dimensions:\n\n<ul> \n<li> <p> <b>R<\/b>ecency \u2013 How recently did the customer purchase? <\/p> <\/li> \n<li> <p> <b>F<\/b>requency \u2013 How often do they purchase? <\/p> <\/li> \n<li> <p> <b>M<\/b>onetary Value \u2013 How much do they spend? <\/p> <\/li> \n<\/ul>\n\nCustomer purchases may be represented by a table with columns for the customer name, date of purchase and purchase value. One approach to RFM is to assign a score for each dimension on a scale from 1 to 10. The maximum score represents the preferred behavior and a formula could be used to calculate the three scores for each customer. For example, a service-based business could use these calculations:\n\nRecency = the maximum of \"10 \u2013 the number of months that have passed since the customer last purchased\" and 1\nFrequency = the maximum of \"the number of purchases by the customer in the last 12 months (with a limit of 10)\" and 1\nMonetary = the highest value of all purchases by the customer expressed as a multiple of some benchmark value\nAlternatively, categories can be defined for each attribute. For instance, Recency might be broken into three categories: customers with purchases within the last 90 days; between 91 and 365 days; and longer than 365 days. Such categories may be derived from business rules or using data mining techniques to find meaningful breaks.\n\nOnce each of the attributes has appropriate categories defined, segments are created from the intersection of the values. If there were three categories for each attribute, then the resulting matrix would have twenty-seven possible combinations (one well-known commercial approach uses five bins per attributes, which yields 125 segments). Companies may also decide to collapse certain subsegments, if the gradations appear too small to be useful. The resulting segments can be ordered from most valuable (highest recency, frequency, and value) to least valuable (lowest recency, frequency, and value). Identifying the most valuable RFM segments can capitalize on chance relationships in the data used for this analysis. For this reason, it is highly recommended that another set of data be used to validate the results of the RFM segmentation process. Advocates of this technique point out that it has the virtue of simplicity: no specialized statistical software is required, and the results are readily understood by business people. In the absence of other targeting techniques, it can provide a lift in response rates for promotions.","534d7ed8":"<div id=\"preprocessing\"><h1> <center> \ud83d\udcda PREPROCESSING <\/center><\/div>","0a9da4e5":"<div id=\"rfm\"><h1> <center> \ud83d\udcda RFM <\/center><\/div>","70d155f3":"<div id=\"imported\"><h1><center> \ud83d\udcda IMPORT LIBRARY AND DATA <\/center><\/div>","87b20639":"<div class=\"container\" style=\"width: 1600px\">\n    <div class=\"alert alert-block\" style=\"background-color: #dfc6f6\"> <span style=\"color:#383838\">Steps to Follow<\/span>\n  <ul class=\"list-group list-group-flush\">\n    <a href=\"https:\/\/www.kaggle.com\/seymasa\/rfm-recency-frequency-monetary\/notebook#imported\"><li class=\"list-group-item\" \n        style=\"overflow:hidden; \n               background-color:#cccccc;\n              position: relative;\n                  display: block;\n              padding: 10px 15px;\n          background-color: #fff;\n          border: 1px solid #ddd;\">\n        <center>Import library and data<\/center<\/li><\/a>\n    <a href=\"https:\/\/www.kaggle.com\/seymasa\/rfm-recency-frequency-monetary\/notebook#exploration\"><li class=\"list-group-item\" style=\"overflow:hidden;  \n              position: relative;\n                  display: block;\n              padding: 10px 15px;\n              margin-bottom: -1px;\n          background-color: #fff;\n          border: 1px solid #ddd;\">\n        <center>Data Exploration<\/center><\/li><a\/>\n    <a href=\"https:\/\/www.kaggle.com\/seymasa\/rfm-recency-frequency-monetary\/notebook#preprocessing\"><li class=\"list-group-item\"style=\"overflow:hidden;  \n              position: relative;\n                  display: block;\n              padding: 10px 15px;\n              margin-bottom: -1px;\n          background-color: #fff;\n          border: 1px solid #ddd;\">\n        <center>Preprocessing<\/center><\/li><\/a>\n    <a href=\"https:\/\/www.kaggle.com\/seymasa\/rfm-recency-frequency-monetary\/notebook#rfm\"><li class=\"list-group-item\"style=\"overflow:hidden;  \n              position: relative;\n                  display: block;\n              padding: 10px 15px;\n              margin-bottom: -1px;\n          background-color: #fff;\n          border: 1px solid #ddd;\">\n        <center>RFM<\/center><\/li><\/a>\n  <\/ul>\n    <\/div>\n<\/div>","8faac0b2":"<div class=\"alert alert-block alert-warning\">\n  <p style=\"color:#5925e5\">* \ud83d\udccc Numeric types should be checked. If there are outliers, outlier analysis should be performed.\n    Categorical types should be checked, if there are null values, they should be removed.* <\/p>  \n<\/div>","bdd05166":"![RFM](https:\/\/cdn.dribbble.com\/users\/2295279\/screenshots\/6297810\/predictiveanalisis_rfm_4x.png?compress=1&resize=1800x1000)"}}