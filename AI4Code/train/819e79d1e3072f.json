{"cell_type":{"b2e5ae83":"code","d417da40":"code","d8d03ee0":"code","1bcdcd98":"code","0c06af8c":"code","e0d9b60f":"code","7d19dd88":"code","aa80fbdb":"code","db927126":"code","d957eefc":"code","16d7626f":"code","9e763f52":"code","a7bbac0e":"code","0b569a94":"code","d98d6b1f":"code","fe879197":"code","6724fcc9":"code","b0378a93":"code","5a462821":"code","01a3dac3":"code","a8bc868c":"code","a3ce0813":"code","5f81e72a":"markdown","067317a9":"markdown","6b8f50e6":"markdown","469dc9a8":"markdown","e3023bb4":"markdown","d434d169":"markdown","39e3f0fb":"markdown"},"source":{"b2e5ae83":"import os\nprint(\"There are following directories and files in this dataset\")\nprint(*list(os.listdir(\"..\/input\/rfcx-species-audio-detection\")),sep = \"\\n\")","d417da40":"import glob\ntrain_audio_tfrec_format = glob.glob('..\/input\/rfcx-species-audio-detection\/tfrecords\/train\/*.tfrec')\nlen(train_audio_tfrec_format)","d8d03ee0":"import glob\ntrain_audio_flac_format = glob.glob('..\/input\/rfcx-species-audio-detection\/train\/*.flac')\nlen(train_audio_flac_format)","1bcdcd98":"import pandas as pd","0c06af8c":"!pip install --upgrade light-the-torch\n!ltt install torch torchvision torchaudio\n!pip install --upgrade git+https:\/\/github.com\/fastaudio\/fastaudio.git\nimport pkg_resources\ndef placeholder(x): raise pkg_resources.DistributionNotFound\npkg_resources.get_distribution = placeholder\n\nfrom fastaudio.all import *","e0d9b60f":"import fastai\nfrom fastai.vision.all import *","7d19dd88":"path = Path('..\/input\/rfcx-species-audio-detection')\naudio_path = Path('..\/input\/rfcx-species-audio-detection\/train')\ntraining_data_file = Path('..\/input\/rfcx-species-audio-detection\/train_tp.csv')\nsample_submission_file = Path('..\/input\/rfcx-species-audio-detection\/sample_submission.csv')","aa80fbdb":"train_df = pd.read_csv(training_data_file)","db927126":"print(\"Size of Training data \\n\", train_df.shape)\nprint(\"----------------------------------------------------------\")\nprint(\"\\nFirst few samples of data are \\n\",train_df.head())","d957eefc":"train_df.info()","16d7626f":"train_df.species_id.nunique()","9e763f52":"train_df['species_id'].value_counts()","a7bbac0e":"train_df.head()","0b569a94":"import matplotlib.pyplot as plt\nfrom plotly.offline import iplot","d98d6b1f":"train_dff = pd.read_csv(training_data_file)\ntrain_dff['species_id'].value_counts().sort_values().iplot(title = 'Number of samples present in the training data for each type of species',kind='bar',color='red',yTitle='Number of samples',xTitle = 'species_id')","fe879197":"print(\"Size of Training data before removing the duplicate recording_id  \\n\", train_df.shape)","6724fcc9":"train_df.drop_duplicates(subset =\"recording_id\", keep = False, inplace = True)","b0378a93":"print(\"Size of Training data after removing the duplicate recording_id  \\n\", train_df.shape)","5a462821":"print(\"Training data before removing any column  \\n\",)\ntrain_df.head()","01a3dac3":"train_df = train_df.drop(['songtype_id','t_min', 'f_min','t_max','f_max'], axis=1)\ntrain_df['species_id'] = train_df['species_id'].astype(str)","a8bc868c":"print(\"Training data after removing the above specified columns  \\n\",)\ntrain_df.head()","a3ce0813":"train_files = get_audio_files(audio_path)\naudio = AudioTensor.create(train_files[0])\naudio.show()","5f81e72a":"# Analysing the data","067317a9":"# Understanding the dataset\nLet us print the dataset","6b8f50e6":"# Importing necessary libraries\n\nNote: We are using Fastai version 2 which is the latest version \n(Not previous version of Fastai- which is version 1)","469dc9a8":"# Selecting a subset of data for training purpose\n","e3023bb4":"# Defining the variables and assigning the paths for this notebook\n","d434d169":"# Choosing the data for model training\n**We will now count the number of audio files availabe to us in the following directories:\n","39e3f0fb":"Let us print the number of data samples with each output category"}}