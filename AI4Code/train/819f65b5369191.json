{"cell_type":{"37c56e6a":"code","8ebd68db":"code","c6ef6cfa":"code","36b8f8fc":"code","c096890a":"code","2c05427a":"code","e71651b7":"code","95ffb7d3":"code","765bafbf":"code","a77ba173":"code","8f07e936":"code","01d4b30a":"code","fe0b7ff9":"code","6a24874e":"code","1f6326cd":"code","8cbe0a3d":"code","e6078280":"code","eb57398a":"code","fe402d5e":"code","3c2ed187":"code","83856af7":"code","5b1fb66d":"code","09faaa29":"code","2063191c":"code","e61a1efa":"code","f34e9020":"code","7640bf30":"code","a69f4c08":"code","6b6225e8":"code","c564b1a4":"code","0b4fdc30":"code","c1b818fe":"code","bee9d837":"code","1cbde4ec":"code","9c23a0a8":"code","c0f84404":"code","21e04796":"code","4d15bc0a":"code","78ce9dab":"code","68b6520f":"markdown","bd3cc5ef":"markdown","2f092f13":"markdown","3b842b2a":"markdown","0faadcad":"markdown","1c9a037d":"markdown","7ce2ecbd":"markdown","53568204":"markdown","3016eb10":"markdown","cc1da9ff":"markdown","3554bb9e":"markdown","8ddf42a3":"markdown","9d4876c9":"markdown","d33a95ac":"markdown","14df471c":"markdown","94c02fdc":"markdown","f0c67604":"markdown"},"source":{"37c56e6a":"# Imports\n\n# Visualisations\nimport matplotlib.pyplot as plt \nimport matplotlib\n%matplotlib inline\n\n# Warnings\nimport warnings\nwarnings.filterwarnings(action = 'ignore')\n\n# Data exploration\nimport pandas as pd\n\n# Numerical\nimport numpy as np\n\n# Random\nnp.random.seed(11)\n\n# Files in dataset\nimport os\nprint(os.listdir(\"..\/input\"))","8ebd68db":"# Import data frame formed in part I of NYC crimes kernel\ncrimes = pd.read_csv(\"..\/input\/crimes_df.csv\")\ncrimes.info()","c6ef6cfa":"# Find values with NaN in PATROL_BORO column, extract them and save as a new data frame.\nprint(\"Name of the PATROL BORO: \\n\", crimes['PATROL_BORO'].value_counts(dropna = False), sep = '')   # check if there any NaNs\npatrol_boro_nan = crimes[crimes['PATROL_BORO'].isnull()]   # df with PATROL_BORO NaNs only\npatrol_boro_nan.drop('PATROL_BORO', axis = 1)   # delete PATROL_BORO column","36b8f8fc":"# Create df without PATROL_BORO NaN values, to split in sets\ndf_p_b = crimes.dropna(subset = ['PATROL_BORO'], axis = 0).reset_index()   # reset_index() is crucial here\n# Sanity check\ndf_p_b['PATROL_BORO'].value_counts(dropna = False)","c096890a":"# Split data in train and test set. Use StratifiedShuffleSplit to make low (lower than splitting with purely random values)\nfrom sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 11)\nfor train_index, test_index in split.split(df_p_b, df_p_b['PATROL_BORO']):\n    strat_train_set = df_p_b.loc[train_index]\n    strat_test_set = df_p_b.loc[test_index]","2c05427a":"# Check values in test set\n#print(strat_test_set['PATROL_BORO'].value_counts(normalize = True))","e71651b7":"# Create df with crimes\/incidents labels of train set, and drop PATROL_BORO column (maybe not necessary to drop column, because I pick categorical columns manually)\ncrimes_labels = strat_train_set['PATROL_BORO'].copy().to_frame()\ncrimes = strat_train_set.drop('PATROL_BORO', axis = 1)","95ffb7d3":"# Quick plot for data check\ncrimes.plot(kind = 'scatter', x = 'Longitude', y = 'Latitude', marker = 'o', alpha = 0.08, figsize = (16,12));","765bafbf":"# Select categories to feed the model, all numerical without index and one categoroical.\n# To be honest I didn`t wonder much time what to select from categorical series, but BORO_NM should be a perfect match\ncrimes_num = crimes.select_dtypes(include = [np.number]).drop('index', axis = 1)\ncrimes_cat = crimes['BORO_NM']","a77ba173":"# Deal with numerical NaNs \nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")\nimputer.fit(crimes_num)\nimputer.transform(crimes_num)","8f07e936":"# Encode crimes labels, use OneHotEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nonehot_encoder = OneHotEncoder(sparse = False)\ncrimes_labels_1hot = onehot_encoder.fit_transform(crimes_labels)\nprint(crimes_labels_1hot.shape)\ncrimes_labels_1hot","01d4b30a":"# Write a selector \nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y = None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].values","fe0b7ff9":"# Make pipelines for numerical and categorical attributes\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_attribs = list(crimes_num)\ncat_attribs = ['BORO_NM']\n\nnum_pipeline = Pipeline([\n    ('selector', DataFrameSelector(num_attribs)),\n    ('imputer', SimpleImputer(strategy=\"median\")),\n    ('std_scaler', StandardScaler()),\n])\n\ncat_pipeline = Pipeline([\n    ('selector', DataFrameSelector(cat_attribs)),\n    ('cat_encoder', OneHotEncoder(sparse=False)), \n])","6a24874e":"# Create one pipeline for the whole process\nfrom sklearn.pipeline import FeatureUnion\nfull_pipeline = FeatureUnion(transformer_list=[\n    ('num_pipeline', num_pipeline),\n    ('cat_pipeline', cat_pipeline),\n])","1f6326cd":"# Encode values using full_pipeline\ncrimes_prepared = full_pipeline.fit_transform(crimes)\nprint(crimes_prepared.shape)\ncrimes_prepared","8cbe0a3d":"# Linear regression model\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(crimes_prepared, crimes_labels_1hot) #return to crimes_labels_encoded","e6078280":"from sklearn.metrics import mean_squared_error\ncrimes_predictions = lin_reg.predict(crimes_prepared)\nlin_mse = mean_squared_error(crimes_labels_1hot, crimes_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","eb57398a":"from sklearn.metrics import mean_absolute_error\n\nlin_mae = mean_absolute_error(crimes_labels_1hot, crimes_predictions)\nlin_mae","fe402d5e":"# Decsision tree regressor model\nfrom sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor(random_state=11)\ntree_reg.fit(crimes_prepared, crimes_labels_1hot)","3c2ed187":"# Don't use code from this cell to predict labels. Data overfitted - too good to be true.\n# Uncomment below to check rsme.\n\n# crimes_predictions = tree_reg.predict(crimes_prepared)\n# tree_mse = mean_squared_error(crimes_labels_1hot, crimes_predictions)\n# tree_rmse = np.sqrt(tree_mse)\n# print(\"Decision Tree Regressor: rmse:\", tree_rmse) ","83856af7":"# Better option, cross validation\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, crimes_prepared, crimes_labels_1hot, scoring = 'neg_mean_squared_error', cv = 10)\ntree_rmse_scores = np.sqrt(-scores)","5b1fb66d":"# Display all scores\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())","09faaa29":"# Execute display_scores(scores) function\ndisplay_scores(tree_rmse_scores)","2063191c":"# Compute the same scores for Linear Regression\nlin_scores = cross_val_score(lin_reg, crimes_prepared, crimes_labels_1hot, scoring = 'neg_mean_squared_error', cv = 10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","e61a1efa":"# Random forset Regressor model\nfrom sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor(random_state=11)\nforest_reg.fit(crimes_prepared, crimes_labels_1hot)\n\ncrimes_predictions = forest_reg.predict(crimes_prepared)\nforest_mse = mean_squared_error(crimes_labels_1hot, crimes_predictions)\nforest_rmse = np.sqrt(forest_mse)\nprint(\"Random Forest Regressor -> rmse:\", forest_rmse)","f34e9020":"# Compute cross_val_score for Random Forest Regressor\nfrom sklearn.model_selection import cross_val_score\nforest_scores = cross_val_score(forest_reg, crimes_prepared, crimes_labels_1hot, scoring='neg_mean_squared_error', cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","7640bf30":"# Grid search using RFR\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [3, 4, 5, 6]},\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n]\nforest_reg = RandomForestRegressor(random_state = 11)\ngrid_search = GridSearchCV(forest_reg, param_grid, cv = 5, scoring = 'neg_mean_squared_error', \n                          return_train_score = True)\ngrid_search.fit(crimes_prepared, crimes_labels_1hot)\nprint(\"Grid search best parameters: \", grid_search.best_params_)\nprint(\"Grid search best estimator: \", grid_search.best_estimator_)","a69f4c08":"# Evaluation scores\nprint(\"Evaluation scores\")\ncvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print(np.sqrt(-mean_score), params)","6b6225e8":"# Randomized search on RFR\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n    'n_estimators': randint(low = 1, high = 200),\n    'max_features': randint(low = 1, high = 8),\n}\n\nforest_reg = RandomForestRegressor(random_state = 11)\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions = param_distribs,\n                               n_iter = 10, cv = 5, scoring = 'neg_mean_squared_error', random_state = 11)\nrnd_search.fit(crimes_prepared, crimes_labels_1hot)","c564b1a4":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","0b4fdc30":"# Check most important attributes\ncat_encoder = cat_pipeline.named_steps['cat_encoder']\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse = True)","c1b818fe":"# Evaluate model on test set\nfinal_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop('PATROL_BORO', axis = 1)\ny_test = strat_test_set['PATROL_BORO'].copy().to_frame()\n\n# Second step - OneHotEncoder, enoding integers to sparse matrix as an output, if (sparse = False) array as an output\nfrom sklearn.preprocessing import OneHotEncoder\ncat_encoder = OneHotEncoder(sparse = False)\ny_test_encoded_oh = cat_encoder.fit_transform(y_test)\ny_test_encoded_oh\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_predictions\nfinal_mse = mean_squared_error(y_test_encoded_oh, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nprint(\"Final score:\", final_rmse)","bee9d837":"# Find PATROL_BORO NaN values. Evaluate final model on patrol_boro_nan data frame\nX_to_find = full_pipeline.transform(patrol_boro_nan)\nNaNs_found = final_model.predict(X_to_find)\nNaNs_found[:5]","1cbde4ec":"# Decode values\n# decode one hot oncoder\none_hot_decode = cat_encoder.inverse_transform(NaNs_found)\none_hot_decode[:5]","9c23a0a8":"# Make data frame of founded NaNs and fix index\nfound = pd.DataFrame(one_hot_decode, columns = ['PATROL_BORO'], index = patrol_boro_nan.index)\nfound[:5]","c0f84404":"# Read original data frame\ncrimes_original = pd.read_csv(\"..\/input\/crimes_df.csv\")\ncrimes_original['PATROL_BORO'].value_counts(dropna = False)","21e04796":"# Fill crimes_original PATROL_BORO NaNs with found values\nfor index in crimes_original['PATROL_BORO'], found['PATROL_BORO']:\n    crimes_original['PATROL_BORO'].loc[crimes_original['PATROL_BORO'].isnull()] = found['PATROL_BORO']","4d15bc0a":"# Check\ncrimes_original.info()","78ce9dab":"# Write df to csv\ncrimes_original.to_csv('crimes_NYC.csv', index = False)","68b6520f":"# NYPD Complaint - Filling NaNs with Random Forest Regressor for Beginners\n### Data updated 7 June 2018\n\n### Notebook created - *2018-24-08*\n### Version - 1\n\n## **This kernel is continuation of previous one**\n\n### **List of kernels:**\n### 1. Previous, introductory notebook - [here](https:\/\/www.kaggle.com\/mihalw28\/nyc-crimes-2018-data-cleaning-part-i)\n### 2. This one - [here](https:\/\/www.kaggle.com\/mihalw28\/nyc-crimes-2018-random-forest-regressor-nans)","bd3cc5ef":"<a id=\"2\"><\/a> <br>\n### First Things First","2f092f13":"## Introduction\n\nThis notebook is continuation of kernel [NYC Crimes 2018  - data cleaning, part I](https:\/\/www.kaggle.com\/mihalw28\/nyc-crimes-2018-data-cleaning-part-i). Like previous one, this kernel take up basic\ndata science skills like data cleaning and implementing regression to fill empty values. I found many inspirations and ideas for this notebook in *Hands-On Machine Learning with Scikit_Learn and TensorFlow*  book written by [Aurelion Geron](https:\/\/twitter.com\/aureliengeron). Any comments about kernel errors and better solutions are welcome. ","3b842b2a":"### **Grid search**","0faadcad":"Time to select and train machine learning model.","1c9a037d":"### **Random forest**","7ce2ecbd":"<a id=\"3\"><\/a> <br>\n## Import data and prepare for machine learning algorithms","53568204":"## Activities I am planning to perform in this kernel\n\n\n### [FILL NAN VALUES IN PATROL_BORO COLUMN:](#1)\n1. [First things first](#2)\n2. [Import data & prepare for machine learning algorithms](#3)\n3. [Train models](#4)\n4. [Fine-tune](#5)\n5. [Evaluation and results](#6)\n","3016eb10":"### **Randomized search**","cc1da9ff":"<a id=\"1\"><\/a> <br>\n# Fill NaN values in PATROL_BORO column","3554bb9e":"The next step is to write a custom transformer to automatically feed a pipeline with selected numerical or categorical attributes. Source [here](https:\/\/github.com\/ageron\/handson-ml\/blob\/master\/02_end_to_end_machine_learning_project.ipynb).","8ddf42a3":"<a id=\"6\"><\/a> <br>\n## **Evaluation and results**","9d4876c9":"<a id=\"4\"><\/a> <br>\n## Train models","d33a95ac":"### **Linear Regression**","14df471c":"<a id=\"5\"><\/a> <br>\n## **Fine-tune model**","94c02fdc":"### **Decision Tree**","f0c67604":"Application of SimpleImputer to fill NaNs in numerical values is a bit useless here, because there aren't any NaNs. Nevertheless, it is very usefull if the data set was not cleaned up before."}}