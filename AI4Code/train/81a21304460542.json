{"cell_type":{"94fb0102":"code","d0dec879":"code","146bd9aa":"code","596f2952":"code","7d7c68c3":"code","49a47f7e":"code","0241fd43":"code","99aa39ed":"code","1ec17797":"code","202aaf75":"code","4437f750":"code","b1e535ff":"code","db98dd9a":"code","8d6b0e1f":"code","a00e54a1":"code","6713021b":"code","89e62472":"code","0e63b00a":"code","79a9af72":"code","cc470733":"code","4cef9462":"code","a96ed19a":"code","d13e3365":"code","a45a6646":"code","3fd77595":"code","bb4b473c":"code","7741b912":"code","2126003b":"code","f28b4c2e":"code","1701da19":"code","89dd1fc2":"code","28898604":"code","d2221202":"code","0cab20bb":"code","cf963bf1":"markdown","d5519517":"markdown","d5195031":"markdown","4dbf443c":"markdown","8fb3be21":"markdown","2c8efb42":"markdown","3416cc3d":"markdown","3d73a6cc":"markdown","0bf62a53":"markdown","fb91ca52":"markdown","b4577cc7":"markdown"},"source":{"94fb0102":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier","d0dec879":"#printing the first 5 rows\ndf=pd.read_csv('..\/input\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndf.head()","146bd9aa":"#checking for null values in the dataset\ndf.isnull().sum()\n#No null values in the dataset","596f2952":"#datatypes of all the fields in the dataset\ndf.dtypes","7d7c68c3":"correlation_df = df.corr()\n# The below correlation coefficients is NaN for Employee Count and Standard Hours Fields\n#This may be because of the zero variance in those fields\nemployee_count_var = df[\"EmployeeCount\"].var() #this is 0\nstandard_hours_var = df[\"StandardHours\"].var() #this is 0","49a47f7e":"#Hence we drop these 2 rows\nnew_df = df.drop([\"EmployeeCount\",\"StandardHours\"],axis = 1)","0241fd43":"new_df.head()","99aa39ed":"correlation_new_df = new_df.corr()\ncorrelation_new_df","1ec17797":"#The above given matrix can also be drawn on MatplotLib for better visual Interpretation\n\nsns.set()\nf, ax = plt.subplots(figsize=(10, 8))\ncorr = new_df.corr()\nsns.heatmap(corr, mask=np.zeros_like(corr), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)\nplt.show()","202aaf75":"#After removing the strongly correlated variables\ndf_numerical = new_df[['Age','DailyRate','DistanceFromHome','Education',\n                       'EnvironmentSatisfaction', 'HourlyRate',                     \n                       'JobInvolvement', 'JobLevel','MonthlyRate',\n                       'JobSatisfaction',\n                       'RelationshipSatisfaction', \n                       'StockOptionLevel',\n                        'TrainingTimesLastYear','WorkLifeBalance']].copy()\ndf_numerical.head()","4437f750":"df_numerical = abs(df_numerical - df_numerical.mean())\/df_numerical.std()  \ndf_numerical.head()","b1e535ff":"df_categorical = new_df[['Attrition', 'BusinessTravel','Department',\n                       'EducationField','Gender','JobRole',\n                       'MaritalStatus',\n                       'Over18', 'OverTime']].copy()\ndf_categorical.head()","db98dd9a":"df_categorical[\"Over18\"].value_counts()\n#Since all values are Y, we can drop this column","8d6b0e1f":"df_categorical = df_categorical.drop([\"Over18\"],axis = 1)","a00e54a1":"# We now Label Encode the Attrition data \nlbl = LabelEncoder()\nlbl.fit(['Yes','No'])\ndf_categorical[\"Attrition\"] = lbl.transform(df_categorical[\"Attrition\"])\ndf_categorical.head()","6713021b":"# We create dummies for the remaining categorical variables\n\ndf_categorical = pd.get_dummies(df_categorical)\ndf_categorical.head()","89e62472":"#Now we finally join both the numerical and categorical dataframes for model evaluation\n\nfinal_df = pd.concat([df_numerical,df_categorical], axis= 1)\nfinal_df.head()\n","0e63b00a":"X = final_df.drop(['Attrition'],axis= 1)\ny = final_df[\"Attrition\"]\n","79a9af72":"\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.4,random_state = 4)","cc470733":"lr = LogisticRegression(solver = 'liblinear',random_state = 0) #Since this a small dataset, we use liblinear solver and Regularization strength as\n# default i.e C = 1.0\nlr.fit(X_train,y_train)","4cef9462":"y_pred_lr = lr.predict(X_test)","a96ed19a":"accuracy_score_lr = accuracy_score(y_pred_lr,y_test)\naccuracy_score_lr \n#Logistic Regression shows 85.7 percent accuracy\n","d13e3365":"dtree = DecisionTreeClassifier(criterion='entropy',max_depth = 4,random_state = 0)","a45a6646":"dtree.fit(X_train,y_train)","3fd77595":"y_pred_dtree = dtree.predict(X_test)\n","bb4b473c":"accuracy_score_dtree = accuracy_score(y_pred_dtree,y_test)\naccuracy_score_dtree","7741b912":"rf = RandomForestClassifier(criterion = 'gini',random_state = 0)\nrf.fit(X_train,y_train)","2126003b":"y_pred_rf = rf.predict(X_test)\naccuracy_score_rf = accuracy_score(y_pred_rf,y_test)\naccuracy_score_rf","f28b4c2e":"sv = svm.SVC(kernel= 'linear',gamma =2)\nsv.fit(X_train,y_train)","1701da19":"y_pred_svm = sv.predict(X_test)\naccuracy_score_svm = accuracy_score(y_pred_svm,y_test)\naccuracy_score_svm","89dd1fc2":"knn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train,y_train)\n","28898604":"y_pred_knn = knn.predict(X_test)\naccuracy_score_knn = accuracy_score(y_pred_knn,y_test)\naccuracy_score_knn","d2221202":"scores = [accuracy_score_lr,accuracy_score_dtree,accuracy_score_rf,accuracy_score_svm,accuracy_score_knn]\nscores = [i*100 for i in scores]\nalgorithm  = ['Logistic Regression','Decision Tree','Random Forest','SVM', 'K-Means']\nindex = np.arange(len(algorithm))\nplt.bar(index, scores)\nplt.xlabel('Algorithm', fontsize=10)\nplt.ylabel('Accuracy Score', fontsize=5)\nplt.xticks(index, algorithm, fontsize=10, rotation=30)\nplt.title('Accuracy scores for each classification algorithm')\nplt.ylim(80,100)\nplt.show()    ","0cab20bb":"feat_importances = pd.Series(rf.feature_importances_, index=X.columns)\nfeat_importances = feat_importances.nlargest(20)\nfeat_importances.plot(kind='barh')\nplt.show()","cf963bf1":"Now from the above plot, we can see that YearsAtCompany,YearsInCurrentRole,YearsSinceLastPromotion and YearsWtihCurrManager\nare closely related, Hence we drop those columns.\nAlso, we'll drop the following rows based on their High Correlation coefficient from the plot:\n1. TotalWorkingYears\n2. PercentSalaryHike\n3. PerformanceRating\n4. NumCompaniesWorked\n5. MonthlyIncome\n\n\n","d5519517":"Now we use RandomForestClassifier","d5195031":"Based on the above graph, Random Forest and SVM shows the highest accuracy score.\n\nWe now calculate feature importances for RandomForest Classifier","4dbf443c":"\nNow we use DecisionTreeClassifier for classifying our input data","8fb3be21":"As we have got our numerical data set up for modelling, We need to do the same for the categorical data.\nWe create dummies for the categorical data and remove any redundant rows.","2c8efb42":"Now we finally have to split our data into training and test set. First, we allot the input and the output variables\n","3416cc3d":"Now we have a wide variety of classification algorithms to choose from, and we'll select the one with the highest model accuracy. We start with Logistic Regression\n\n","3d73a6cc":"KNN Algorithm","0bf62a53":" Support vector Machines","fb91ca52":"Findings : Monthly Rate affects the Attrition the most","b4577cc7":"Now we can scale our numerical features either by StandardScaler or by using math operations\n"}}