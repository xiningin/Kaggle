{"cell_type":{"e858b4bc":"code","78c01857":"code","478d2126":"code","efba87a6":"code","92b47f57":"code","73f5a9e6":"code","e9ce8d58":"code","0e3742eb":"code","760357e0":"code","e4bed33c":"code","6a0375ad":"code","e91e8b49":"code","b5a1ba8e":"code","7ab54f3d":"code","796a706f":"code","3391dcc8":"code","08fc03c7":"code","05c8c87c":"markdown","ba1fa450":"markdown","f30c8c96":"markdown","11fbb1cb":"markdown","75b6033d":"markdown","6f82a5e3":"markdown","209560fa":"markdown","0a8ab0bf":"markdown","d5ed86e0":"markdown"},"source":{"e858b4bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","78c01857":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","478d2126":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef visualize_null_data(df,Title):\n    miss = df.isnull().sum()\/len(df)\n    miss = miss[miss> 0]*100\n    miss.sort_values(inplace=True)\n    print(miss)\n\n    #visualising missing values\n    miss = miss.to_frame()\n    miss.columns = ['count']\n    miss.index.names = ['Name']\n    miss['Name'] = miss.index\n\n    #plot the missing value count\n    sns.set(style=\"whitegrid\", color_codes=True)\n    sns.barplot(x = 'Name', y = 'count', data=miss)\n    plt.xticks(rotation = 90)\n    plt.title(Title)\n    plt.show()","efba87a6":"visualize_null_data(train_data,\"Titanic Training Data - Null Value %\")","92b47f57":"train_data.loc[(train_data.Sex=='male') & (train_data['Age'].isnull()),'Age']=train_data[train_data.Sex=='male']['Age'].median()\ntrain_data.loc[(train_data.Sex=='female') & (train_data['Age'].isnull()),'Age']=train_data[train_data.Sex=='female']['Age'].median()\ntrain_data['Embarked'].fillna(train_data['Embarked'].mode()[0],inplace=True)","73f5a9e6":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","e9ce8d58":"visualize_null_data(test_data,\"Titanic Test Data Null Values %\")","0e3742eb":"test_data.loc[(test_data.Sex=='male') & (test_data['Age'].isnull()),'Age']=test_data[test_data.Sex=='male']['Age'].median()\ntest_data.loc[(test_data.Sex=='female') & (test_data['Age'].isnull()),'Age']=test_data[test_data.Sex=='female']['Age'].median()\ntest_data['Fare']=test_data['Fare'].fillna(test_data['Fare'].mean())","760357e0":"def build_model(df,y):\n    X = pd.get_dummies(df)\n\n    model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n    model.fit(X, y)\n    \n    return X, model","e4bed33c":"from sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX, model = build_model(train_data[features],y)","6a0375ad":"#Exploring explainable\nimport lime\nfrom lime import lime_tabular\n\ninterpretor=lime_tabular.LimeTabularExplainer(\n    training_data=np.array(X),\n    feature_names=X.columns,\n    mode='classification'\n)\n\nForExplainableData=X.copy()\nForExplainableData.insert(0,'Survived',y)","e91e8b49":"ForExplainableData.iloc[[18, 869]]","b5a1ba8e":"exp=interpretor.explain_instance(\n    data_row=ForExplainableData.iloc[18][1:],\n    predict_fn=model.predict_proba\n)\nexp.show_in_notebook(show_table=True)","7ab54f3d":"exp=interpretor.explain_instance(\n    data_row=ForExplainableData.iloc[869][1:],\n    predict_fn=model.predict_proba\n)\nexp.show_in_notebook(show_table=True)","796a706f":"features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\",\"Age\",\"Embarked\",\"Fare\"]\nX, model = build_model(train_data[features],y)\ntrain_predictions2 = model.predict(X)\ntrain_predictions2[18]\n\ninterpretor=lime_tabular.LimeTabularExplainer(\n    training_data=np.array(X),\n    feature_names=X.columns,\n    mode='classification'\n)\n\nForExplainableData=X.copy()\nForExplainableData.insert(0,'Survived',y)","3391dcc8":"exp=interpretor.explain_instance(\n    data_row=ForExplainableData.iloc[18][1:],\n    predict_fn=model.predict_proba\n)\nexp.show_in_notebook(show_table=True)","08fc03c7":"exp=interpretor.explain_instance(\n    data_row=ForExplainableData.iloc[869][1:],\n    predict_fn=model.predict_proba\n)\nexp.show_in_notebook(show_table=True)","05c8c87c":"Passenger 18 (As we don't have passenger name, representing 18th row as Passenger 18) not survived & Passenger 869 survived.  Now let us try to interpret for both of the rows using LIME.\n\nAs we notice in the above only difference between both records are:\n1. Passenger 18 is Female & Parch is 0\n2. Passenger 869 is Male & Parch is 1","ba1fa450":"Passenger 18 is predicted as survived mainly because this passenger was Female. But our prediction is not correct.","f30c8c96":"# We have reached Core Part now\n\nLIME stands for Local Interpretable Model-agnostic Explanations. It was developed by Marco Ribeiro in 2016. It helps in explaining predictions of Machine Learning models.\n\nLet us pass our input data (X) and the feature names to LimeTabularExplainer method.","11fbb1cb":"* Let us read the training & test data\n* Identify the missing data in both datasets\n* Impute them\n* Run the Random Forest Classifier (RFC) in the above data","75b6033d":"Passenger 869 is predicted as not survived mainly because this passenger was Male and travelled in 3rd class. But our prediction is not correct.\n\nThis explicitly explains to us that above features are not sufficient.  Let us add additional features \"Age\",\"Embarked\",\"Fare\".\n\nLet us rebuild the model with these additional features and run the interpretor.","6f82a5e3":"As we see in the above additional factors that we have added like Age, Fare and Embarked fields helped us to predict the correct Survival status.\n\nHope you liked the above analysis.\n\n## If you have liked please upvote!!!\n\nThis is continued in Part 3. Please click [here](https:\/\/www.kaggle.com\/rajamykaggle\/titanic-explainable-ml-code-part3) to go to Part 3.","209560fa":"# **Explainable Machine Learning**\n\nDear All\n\nThis is Part 2 of my Explainable Machine Learning Series.  If you have not gone through Part 1, kindly go through the same by clicking [Explainable ML Part1](https:\/\/www.kaggle.com\/rajamykaggle\/titanic-explainable-ml-code-part1).\n\nIf you like this, ***kindly upvote that motivates*** me to continue writing and sharing.\n\nOne of the **key challenges in the Machine Learning** ***is the explainability of the ML model***. When we show the outcome of the model, how customer will understand what are the factors that are influencing the final outcome (i.e. prediction done by ML Model).\n\nCustomer could see this as a black box.  For example, let us say our ML model is predicting the end customer who will subscribe for a particular banking product.  When we show our ML Model result to our customer, their question could be, out of the given factors (dependent variables), which are the influencing factors that will help us to decide whether a particular end customer will subscribe to a product or not.\n\nAs a data scientist how do we answer to this question?\n\n**Explainable AI helps to answer above question.**\n\nWe will continue where we have left in Part-1.","0a8ab0bf":"Let us identify two records where RFC has not done the prediction correctly.  After analyzing the records, identified these two rows 18 & 869 to explain further.\n\nLets look into that:","d5ed86e0":"As we see in the above additional factors that we have added like Age, Fare and Embarked fields helped us to predict the correct Survival status."}}