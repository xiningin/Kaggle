{"cell_type":{"c6253e99":"code","fbfc877a":"code","e4515631":"code","7684b394":"code","0e1324e9":"code","8e2a0025":"code","0bf372fb":"code","1b966cda":"code","8d6427de":"code","72825345":"code","eb788f31":"code","69be7a41":"code","b2fe3e80":"code","dc06c944":"code","aab15460":"code","59a7ee7d":"code","54a10865":"code","d3555750":"code","a72064bd":"code","7940de7f":"code","8c4b4014":"code","ad1eb688":"code","197e7296":"code","fb2dd12c":"markdown","0080dff9":"markdown","6c360206":"markdown","5d6b1873":"markdown","c95adcbc":"markdown","f7407135":"markdown","78c5bab6":"markdown","7983740b":"markdown","2ffefbe1":"markdown","fecc2c07":"markdown","9a498730":"markdown","5c71ff3d":"markdown","07f7f477":"markdown","11074202":"markdown","41c4b0cb":"markdown","0150dbd9":"markdown","fb8daddd":"markdown","8f2e7b4c":"markdown","7befa4cb":"markdown","e18d29b0":"markdown","7e3da171":"markdown","6d92df34":"markdown","85f87599":"markdown","bdd7b891":"markdown","f32e50f6":"markdown","893094e6":"markdown","733837c5":"markdown","e6f7cb21":"markdown","0c334731":"markdown"},"source":{"c6253e99":"# Python version\nimport sys\nprint('Python: {}'.format(sys.version))\n# numpy\nimport numpy\nprint('numpy: {}'.format(numpy.__version__))\n# pandas\nimport pandas\nprint('pandas: {}'.format(pandas.__version__))\n# matplotlib\nimport matplotlib\nprint('matplotlib: {}'.format(matplotlib.__version__))\n# seaborn\nimport seaborn\nprint('seaborn: {}'.format(seaborn.__version__))\n# scikit-learn\nimport sklearn\nprint('sklearn: {}'.format(sklearn.__version__))","fbfc877a":"# Display plots within notebook\n%matplotlib inline\n\n# Import required python libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap  # For making our own cmap\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n# Classification Algorithms - \nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Hide FutureWarnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","e4515631":"# set seaborn for attractive plots\nsns.set(style='darkgrid')","7684b394":"# Load dataset into dataframe\npath = '..\/input\/Iris.csv'\nfeature_labels = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\niris_df = pd.read_csv(path)\n\n#Drop Id column and change column names to desired names in above list\niris_df = iris_df.drop(columns='Id')\niris_df.columns = feature_labels\n\n# keep unique classes\nclasses = iris_df['class'].unique()","0e1324e9":"iris_df.shape","8e2a0025":"iris_df.head(5)","0bf372fb":"iris_df.info()","1b966cda":"iris_df.describe()","8d6427de":"plt.figure(figsize=(12,8))\nfor i in range(1, len(feature_labels)):\n    plt.subplot(2, 2, i)\n    sns.distplot(iris_df[feature_labels[i-1]], kde=False)\n    \nplt.subplots_adjust(wspace=0.2, hspace=0.3)","72825345":"plt.figure(figsize=(6,5))\nsns.boxplot(data=iris_df)","eb788f31":"# Boxplot variant of violinplot makes, its same plot as violinplots as below but in box format\n# plt.figure(figsize=(12,10))\n# for i in range(1, len(feature_labels)):\n#     plt.subplot(2, 2, i)\n#     sns.boxplot(x='class', y=feature_labels[i-1], data=iris_df)\n    \n# plt.subplots_adjust(wspace=0.2, hspace=0.2)","69be7a41":"# Violinplot variant of boxplot\n# plt.figure(figsize=(6,5))\n# sns.violinplot(data=iris_df)","b2fe3e80":"plt.figure(figsize=(12,8))\nfor i in range(1, len(feature_labels)):\n    plt.subplot(2, 2, i)\n    sns.violinplot(x='class', y=feature_labels[i-1], data=iris_df)\n    \nplt.subplots_adjust(wspace=0.2, hspace=0.3)","dc06c944":"sns.pairplot(iris_df, hue='class')","aab15460":"test_size = 0.2\nseed = 7\n\nfeatures_train, features_test, labels_train, labels_test = train_test_split(np.float64(iris_df.values[:, 0:4]), iris_df['class'], test_size=0.2, random_state=seed)","59a7ee7d":"scaler = StandardScaler()\nscaler.fit(features_train)\nscaled_features_train = scaler.transform(features_train)\nscaled_features_test = scaler.transform(features_test)\n\n#### Visualise Boxplot for each scaled feature\nscaled_df = pd.DataFrame(data=np.array(scaled_features_train), columns=feature_labels[0:4])\n# scaled_df[feature_labels[4]] = le.inverse_transform(labels_train)\nplt.figure(figsize=(6, 5))\nsns.boxplot(data=scaled_df)","54a10865":"# SelectKBest using f_classif (ANOVA Test) as score function\nbestfeatures = SelectKBest(score_func=f_classif, k=2)\nfit = bestfeatures.fit(scaled_features_train, labels_train)\n\nscores_df = pd.DataFrame(data=fit.scores_)\ncolumns_df = pd.DataFrame(data=iris_df.columns)\n\nfeature_scores_df = pd.concat([columns_df,scores_df],axis=1)\nfeature_scores_df.columns = ['Features','Score']\n\n# print(feature_scores_df.nlargest(4,'Score'))\n\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nsns.barplot(x='Score', y='Features', order=feature_scores_df.nlargest(4,'Score')['Features'], data=feature_scores_df, palette=sns.cubehelix_palette(n_colors=4, reverse=True))\nplt.subplot(1, 2, 2)\nsns.heatmap(iris_df.corr(), annot=True, cmap=sns.cubehelix_palette(start=0, as_cmap=True))","d3555750":"transformed_features_train = scaled_features_train[:, [2, 3]]\ntransformed_features_test = scaled_features_test[:, [2, 3]]","a72064bd":"clf_names = ['NB', 'SVC', 'KNN', 'CART', 'BAG', 'RF', 'AB']\n\nmodels = [\n    ('NB', GaussianNB()),\n    ('SVC', SVC(C=1000, kernel='rbf', gamma=0.05)),\n    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n    ('CART', DecisionTreeClassifier(min_samples_split=20, min_samples_leaf=5)),\n    ('BAG', BaggingClassifier(n_estimators=100)),\n    ('RF', RandomForestClassifier(n_estimators=100, min_samples_split=20, min_samples_leaf=5)),\n    ('AB', AdaBoostClassifier(n_estimators=100))\n]","7940de7f":"scoring = 'accuracy'\n\ncv_results = []\nfor name, clf in models:\n    kfold = KFold(n_splits=10, random_state=seed)\n    cv_score = cross_val_score(clf, transformed_features_train, labels_train, cv=kfold, scoring=scoring)\n    cv_results.append(cv_score)\n    print('{}: {}, ({})' .format(name, cv_score.mean(), cv_score.std()))","8c4b4014":"plt.figure(figsize=(6, 5))\nsns.boxplot(data=pd.DataFrame(data=np.array(cv_results).transpose(), columns=clf_names))\n\n# #### Alternate choice for boxplot - univariateplot -> boxplot + violinplot\n# plt.figure(figsize=(15, 5))\n# plt.subplot(1, 2, 1)\n# sns.boxplot(data=pd.DataFrame(data=np.array(cv_results).transpose(), columns=clf_names))\n\n# plt.subplot(1, 2, 2)\n# sns.violinplot(data=pd.DataFrame(data=np.array(cv_results).transpose(), columns=clf_names))","ad1eb688":"x = transformed_features_train[:, [0]].ravel()\ny = transformed_features_train[:, [1]].ravel()\n\nle = LabelEncoder()\nle.fit(classes)\n\n# Encode all classes of Iris Flower species to values [0, 1, 2] to plot contour\ntarget_labels_encoded = le.transform(iris_df['class'].ravel())\nlabels_train_encoded = le.transform(labels_train)\n\n# color sequence\nc = labels_train_encoded\n\nmodels_ = models.copy()\ntrained_models = [(name, clf.fit(np.array(transformed_features_train), c)) for name, clf in models_]\n\ntitles = ('Input Data',\n          'Gaussian NB',\n          'SVM with RBF kernel',\n          'KNN',\n          'Decision Tree',\n          'Bagging',\n          'Random Forest',\n          'AdaBoost',\n         )\n\ndef mesh_grid(x, y, h=0.02):\n    x_min, x_max = x.min(), x.max()\n    y_min, y_max = y.min(), y.max()\n    \n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    \n    return xx, yy\n\ndef plot_contour(ax, clf, xx, yy, **params):\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    return ax.contourf(xx, yy, Z, **params)\n\nxx, yy = mesh_grid(x, y)\n\n# custom cmap\nmy_cmap = ListedColormap(sns.color_palette().as_hex()[0:3])\n\nfig, sub = plt.subplots(2, 4, figsize=(20, 8))\n\nsub[0, 0].scatter(iris_df.values[:, [2]].ravel(), iris_df.values[:, [3]].ravel(), c=target_labels_encoded, cmap=my_cmap, s=20)\nsub[0, 0].set_title(titles[0])\n\nfor clf, title, ax in zip(trained_models, titles[1:], sub.flatten()[1:]):\n        plot_contour(ax, clf[1], xx, yy, cmap=my_cmap, alpha=1)\n        ax.scatter(x, y, c=c, cmap=my_cmap, s=25, edgecolor='k')\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xlabel('Petal length')\n        ax.set_ylabel('Petal width')\n        ax.set_xticks(())\n        ax.set_yticks(())\n        ax.set_title(title)\n\nplt.show()","197e7296":"# Selecting and predicting using KNN\nclf = models[2][1]\nclf.fit(transformed_features_train, labels_train)\npredictions = clf.predict(transformed_features_test)\n\nprint(accuracy_score(labels_test, predictions))\nprint(confusion_matrix(labels_test, predictions))\nprint(classification_report(labels_test, predictions))","fb2dd12c":"### 6.3.1 Boxplot","0080dff9":"# 2. Load Dataset","6c360206":"# 1. Import Libraries","5d6b1873":"# 4. Data Visualisation\n\nNow, lets visualise data  by -\n1. <B>Univariate plot<\/B> to better understand each feature.\n2. <B>Multivariate plot<\/B> to understand relationship between features.","c95adcbc":"# 3. Summarise Data\n\nLets take a quick look at the data by - \n1. Dimension.\n2. Peeping at the data itself.\n3. Info of the dataframe.\n4. Looking at the statistics of data.","f7407135":"## 4.1 Univariate Plot\n\n1. Histplot<br \/> \n2. Boxplot<br \/> \n3. Violinplot<br \/> ","78c5bab6":"# 7. Make Prediction\n\nKNN is a quite simple algorithm and has also perfomed well in validation. Now lets see how it performs on out test set.<br\/> <br\/>\nTesting our classifier will give us an independent final check on the selected best model.<br\/><br\/>\nWe can run the KNN directly on the test set and summarize the results as accuracy score, confusion matrix and classification report.","7983740b":"## 4.2 Multivariate Plot\n\n1. Pairplot","2ffefbe1":"## 3.4 Statistics of data","fecc2c07":"Based on our SelectKBest scores and Correlation Matrix, we'll select and 2 best features to build our classifier, i.e, 'petal-length' and 'petal-width'.","9a498730":"We can see that our classifier has an accuracy of 90% on the testing set. The confusion matrix tells us that it made three errors and the classification report gives the result of classification for each class. The overall result seems good.","5c71ff3d":"### 4.1.2 Boxplot","07f7f477":"## 3.2 Peeping at the data itself","11074202":"## 3.1 Dimension of data","41c4b0cb":"### 4.2.1 Pairplot","0150dbd9":"### 5.2.1 Feature Scaling","fb8daddd":"## 5.2 Feature Engineering\/Preprocessing\n\nThis step involvs - <br\/>\n1. Feature Scaling.<br\/>\n2. PCA.<br\/>\n3. Feature Selection.<br\/>\n\n","8f2e7b4c":"## 6.2 Validation\n\nLets use cross validation to estimate accuracy of all our predictive models using <B>10-fold cross validator<\/B> with <B>accuracy<\/B> scoring.<br\/><br\/>\nA 10-fold cv will divide our training set into 9 set for training our classifiers and retain 1 set for validation\/testing. It will repeat this for all(10) possible sets of train and validate data set and will calculate the average accuracy over accuracy for each train-test set.","7befa4cb":"## 3.3 Info of data","e18d29b0":"# 6. Evaluate Some Algorithms\n\nNow, its time to build and evaluate some supervised leaning classifiers, by - \n1. Build classifier.\n2. Validation.\n3. Select best classifier.","7e3da171":"# 5. Prepare Data\n\nNow, after having a close look at the data, lets perpare it for classification.<br\/>\nWe will prepare data by following steps - \n1. Test-Train Split.\n2. Feature Engineering.","6d92df34":"## 6.1 Build Classifiers\n\nWe will build 7 classifiers, namely - \n1. Gaussian Naive Bayes (NB).\n2. SVM (SVC).\n3. KNN (KNN).\n4. Decision Tree (CART).\n5. Bagging (BAG).\n6. Random Forest (RF).\n7. AdaBoost (AB).","85f87599":"## Check version of libraries","bdd7b891":"### 6.3.2 Classifier' Contour Plots","f32e50f6":"### 4.1.1 Histplot","893094e6":"## 6.3 Select Best Classifier\n\nWe have an estimate accuracy for all our classifiers. We can see that the SVC and CART outperformes every other classifier with an accuracy of about 97.5%. This makes us little suspicious to overfitting which will be confirmed when predicting.<br\/>\nFor now lets - \n1. Compare our CV results for each classifier using boxplot.\n2. Compare classifier using contour plot (Just for fun!).","733837c5":"### 5.2.3 Feature Selection\n\nNow, lets select two features (mainly because we want to visualise our classifier).<br\/><br\/>\nFeature selection is quite beneficial as it - \n1. Reduces overfitting.\n2. Improves accuracy.\n3. Reduces training time.\n<br\/><br\/>\nWe might not need feature selection on this dataset as we have only 4 features, but since I want to visualize my trained classifiers, I'm gonna select two features.<br\/>\n\nThere are many ways to decide which features to use, Univariate Selection, Feature Importance, Correlation matrix etc. But we're gonna visualise and use SelectKBest(Univariate Selection) and Correlation matrix(its just a bonus!).","e6f7cb21":"## 5.1 Train-Test Split\n\nSplitting dataset into training and testing as - 80% for training and 20% for testing our classifers.","0c334731":"### 4.1.3 Violinplot\n\nIt is similar to boxplot, but it also gives us <B>kernel density plot<\/B>.<br\/>\nIt consists boxplot in the center of kernel density plot."}}