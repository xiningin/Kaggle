{"cell_type":{"e7a49212":"code","31e6edcb":"code","a6d7e180":"code","2882010c":"code","6499e721":"code","b863deaa":"code","3b39e427":"code","c30afc19":"code","448c591b":"code","eda4e4d7":"code","a0dac31b":"code","e8592fbe":"code","c88bebd4":"code","d0120df5":"code","1058d6d3":"code","0ad118dc":"code","0b5b6625":"code","c5665829":"markdown","267e93aa":"markdown"},"source":{"e7a49212":"%matplotlib inline\nimport xgboost as xgb\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nimport gc\nimport matplotlib.pyplot as plt\nimport shap\n\n# load JS visualization code to notebook\nshap.initjs()\nxgb.__version__","31e6edcb":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a6d7e180":"train = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv')\nsub = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/sample_submission.csv')","2882010c":"train.head()","6499e721":"train.shape","b863deaa":"columns = test.columns[1:]\ncolumns\n","3b39e427":"target = train['target'].values\n","c30afc19":"cat_features = columns[:19]\ncat_features","448c591b":"def label_encode(train_df, test_df, column):\n    le = LabelEncoder()\n    new_feature = \"{}_le\".format(column)\n    le.fit(train_df[column].unique().tolist() + test_df[column].unique().tolist())\n    train_df[new_feature] = le.transform(train_df[column])\n    test_df[new_feature] = le.transform(test_df[column])\n    return new_feature","eda4e4d7":"cat_cols = [col for col in columns if 'cat' in col]\ncont_cols = [col for col in columns if 'cont' in col]","a0dac31b":"le_cols = []\nfor feature in cat_cols:\n    le_cols.append(label_encode(train, test, feature))","e8592fbe":"columns = le_cols + cont_cols","c88bebd4":"xgb_params=  {'learning_rate': 0.005,\n              'objective': 'binary:logistic',\n              'eval_metric': 'auc',\n                'tree_method': 'gpu_hist',\n                'predictor': 'gpu_predictor',\n                'gpu_id': 0,\n                'max_bin': 623,\n                'max_depth': 10,\n                'alpha': 0.5108154566815425,\n                'gamma': 1.9276236172849432,\n                'reg_lambda': 11.40999855634382,\n                'colsample_bytree': 0.705851334291963,\n                'subsample': 0.8386116751473301,\n                'min_child_weight': 2.5517043283716605}","d0120df5":"test = xgb.DMatrix(test[columns])","1058d6d3":"train_oof = np.zeros((train[columns].shape[0],))\ntest_preds = 0\ntrain_oof_shap = np.zeros((train[columns].shape[0],train[columns].shape[1]+1))\ntest_preds_shap = 0\ntrain_oof_shap.shape","0ad118dc":"%%time\nNUM_FOLDS = 10\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train, target))):\n        #print(f'Fold {f}')\n        train_df, val_df = train.iloc[train_ind][columns], train.iloc[val_ind][columns]\n        train_target, val_target = target[train_ind], target[val_ind]\n        \n        train_df = xgb.DMatrix(train_df, label=train_target)\n        val_df = xgb.DMatrix(val_df, label=val_target)\n        \n        model =  xgb.train(xgb_params, train_df, 7000)\n        temp_oof = model.predict(val_df)\n        temp_oof_shap = model.predict(val_df, pred_contribs=True)\n        temp_test = model.predict(test)\n        temp_test_shap = model.predict(test, pred_contribs=True)\n\n        train_oof[val_ind] = temp_oof\n        test_preds += temp_test\/NUM_FOLDS\n        \n        train_oof_shap[val_ind, :] = temp_oof_shap\n        test_preds_shap += temp_test_shap\/NUM_FOLDS\n        \n        \n        print(roc_auc_score(val_target, temp_oof))\n        \nprint(roc_auc_score(target, train_oof))","0b5b6625":"np.save('train_oof', train_oof)\nnp.save('test_preds', test_preds)\nnp.save('train_oof_shap', train_oof_shap)\nnp.save('test_preds_shap', test_preds_shap)","c5665829":"In this notebook we'll explore feature importance using SHAP values. SHAP values are the most mathematically consistent way for getting feature importances, and they work particulalry nicely with the tree-based models. Unfortunately, calculating SHAP values is an extremely resource intensive process. However, starting with XGBoost 1.3 it is possible to calcualte these values on GPUs, which speeds up the process by a factor of 20X - 50X compared to calculating the same on a CPU. Furthermore, it is also possible to calculate SHAP values for feature interactions. The GPU speedup for those is even more dramatic - it takes a few minutes, as opposed to days or even longer on a CPU.","267e93aa":"Let's applay label encoder to the categorical features.\n"}}