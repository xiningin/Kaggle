{"cell_type":{"a0b7b4fa":"code","6968e38c":"code","1c57f135":"code","a6c508e3":"code","71d3c2b5":"code","8692a524":"code","6f70d42c":"code","e401f908":"code","a4d84712":"code","031c9e27":"code","d2506b5b":"code","d4d0f423":"code","a45c4479":"code","92a6a2f9":"code","628e8154":"markdown","1736e0d3":"markdown","155ead2d":"markdown","19c2f96f":"markdown","037caff9":"markdown","6ff8639f":"markdown","f977acd5":"markdown","2397d6e2":"markdown","450ac51f":"markdown","3755ca1b":"markdown","8f6833ba":"markdown","2a96fe99":"markdown"},"source":{"a0b7b4fa":"import pandas as pd\nimport numpy as np\n\nspam_data = pd.read_csv('..\/input\/spam-data\/spam.csv')\nspam_data=spam_data.dropna()\nspam_data['target'] = np.where(spam_data['target']=='spam',1,0)\nspam_data","6968e38c":"from sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(spam_data['text'], \n                                                    spam_data['target'], \n                                                    random_state=0)","1c57f135":"def answer_one():\n    return (len(spam_data[spam_data['target']==1])\/len(spam_data))*100\nprint(round(answer_one(),1),'% of the data classified as spam' )","a6c508e3":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef answer_two():\n    vect=CountVectorizer().fit(X_train)\n    #vocab=vect.get_feature_names()\n    [(len(i),i) for i in vect.get_feature_names()]\n    return str(sorted([(len(i),i) for i in vect.get_feature_names()],reverse=True)[0][1])\nprint('The Longest word in the vocabulary is:  ',answer_two())","71d3c2b5":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_extraction.text import CountVectorizer\ndef answer_three():\n    vect=CountVectorizer().fit(X_train)\n    X_train_transformed=vect.transform(X_train)\n    model=MultinomialNB(alpha=0.1).fit(X_train_transformed,y_train)\n    predictions=model.predict(vect.transform(X_test))\n    auc=roc_auc_score(y_test,predictions)\n    return auc\n\nprint('Accuracy of the Model :',round(answer_three(),2) )","8692a524":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ndef answer_four():\n    tf_vect=TfidfVectorizer().fit(X_train)\n    X_train_transformed=tf_vect.transform(X_train)\n    tfid_sorted=X_train_transformed.max(0).toarray()[0].argsort()\n    feature_names=np.array(tf_vect.get_feature_names())\n    smallest_20=pd.DataFrame(feature_names[tfid_sorted[:20]])\n    return  (pd.Series(feature_names[tfid_sorted[:20]]) , pd.Series(feature_names[tfid_sorted[:-21:-1]]).sort_values()) \nanswer_four()","6f70d42c":"def answer_five():\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.naive_bayes import MultinomialNB\n    from sklearn.metrics import roc_auc_score\n    tf_vect=TfidfVectorizer(min_df=3).fit(X_train)\n    X_train_transformed=tf_vect.transform(X_train)\n    nb_classifier=MultinomialNB(alpha=0.1)\n    model=nb_classifier.fit(X_train_transformed,y_train)\n    predictions=model.predict(tf_vect.transform(X_test))\n    return roc_auc_score(y_test,predictions)\nprint('Model Score: ', answer_five())","e401f908":"def answer_six():\n    spam_data['ave_len']=pd.Series(len(spam_data['text'][i]) for i in range(0,len(spam_data)))\n    np.where(spam_data['target']==0,spam_data['ave_len'],0).sum()\n    return (np.where(spam_data['target']==0,spam_data['ave_len'],0).sum()\/len(spam_data[spam_data['target']==0]),np.where(spam_data['target']==1,spam_data['ave_len'],0).sum()\/len(spam_data[spam_data['target']==1]))\nprint('Average number of characters for not spam documents : ',answer_six()[0],\n     '\\nAverage number of characters for spam documents    :  ',answer_six()[1])","a4d84712":"def add_feature(X, feature_to_add):\n    \"\"\"\n    Returns sparse feature matrix with added feature.\n    feature_to_add can also be a list of features.\n    \"\"\"\n    from scipy.sparse import csr_matrix, hstack\n    return hstack([X, csr_matrix(feature_to_add).T], 'csr')","031c9e27":"from sklearn.svm import SVC\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import roc_auc_score \ndef answer_seven():\n    tfid_vect=TfidfVectorizer(min_df=5).fit(X_train)\n    X_train_transformed=tfid_vect.transform(X_train)\n    X_train_textlen_added=add_feature(X_train_transformed,pd.Series(len(pd.DataFrame(X_train).iloc[i]) for i in range(0,len(X_train))))\n    svc_model=SVC(C=10000)\n    model=svc_model.fit(X_train_textlen_added,y_train)\n    predictions=model.predict(add_feature(tfid_vect.transform(X_test),pd.Series(len(pd.DataFrame(X_test).iloc[i]) for i in range(0,len(X_test)))))\n    return roc_auc_score(y_test,predictions)\nprint('Model Score :',answer_seven())","d2506b5b":"def answer_eight():\n    import re\n    spam_data['digits_per_doc']=pd.Series([ len(re.findall(r'\\d',spam_data.loc[i,'text'])) for i in range(0,len(spam_data))])\n    return np.where(spam_data['target']==0,spam_data['digits_per_doc'],0).sum()\/len(spam_data[spam_data['target']==0]),np.where(spam_data['target']==1,spam_data['digits_per_doc'],0).sum()\/len(spam_data[spam_data['target']==1])\nprint('Average number of digits for not spam documents : ',answer_eight()[0],\n     '\\nAverage number of digits for spam documents    :  ',answer_eight()[1])","d4d0f423":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import roc_auc_score\nimport re\ndef answer_nine():\n    vect=TfidfVectorizer(min_df=5,ngram_range=(1,3)).fit(X_train)\n    X_train_transformed=vect.transform(X_train)\n    X_train_len_dig_added=add_feature(add_feature(X_train_transformed,pd.Series(len(pd.DataFrame(X_train).iloc[i]) for i in range(0,len(X_train)))),pd.Series([len(re.findall(r'\\d',pd.DataFrame(X_train).iloc[i][0])) for i in range(0,len(X_train))]))\n    log_reg=SVC(C=100)\n    model=log_reg.fit(X_train_len_dig_added,y_train)\n    predictions=model.predict(add_feature(add_feature(vect.transform(X_test),pd.Series(len(pd.DataFrame(X_test).iloc[i]) for i in range(0,len(X_test)))),pd.Series([len(re.findall(r'\\d',pd.DataFrame(X_test).iloc[i][0])) for i in range(0,len(X_test))])))\n    \n    return roc_auc_score(y_test,predictions)\nprint('Model Score :',round(answer_nine(),2))","a45c4479":"import re\ndef answer_ten():\n    spam_data['len_of_nonword']=pd.Series([len(re.findall(r'\\W',spam_data.loc[i,'text'])) for i in range(0,len(spam_data))])\n    not_spam_ave=np.where(spam_data['target']==0,spam_data['len_of_nonword'],0).sum()\/len(spam_data[spam_data['target']==0])\n    spam_ave=np.where(spam_data['target']==1,spam_data['len_of_nonword'],0).sum()\/len(spam_data[spam_data['target']==1])\n    return (not_spam_ave,spam_ave)\nprint('--Average number of non-word characters-- ','\\nNot spam Documents:',answer_ten()[0],'\\nSpam Documents    :',answer_ten()[1])","92a6a2f9":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.naive_bayes import MultinomialNB\nimport re\ndef answer_eleven():\n    vect=CountVectorizer(min_df=5,ngram_range=(2,5),analyzer='char_wb').fit(X_train)\n    X_train_vectorized=vect.transform(X_train)\n    length_of_doc=pd.Series(len(pd.DataFrame(X_train).iloc[i]) for i in range(0,len(X_train)))\n    digit_count=pd.Series([len(re.findall(r'\\d',pd.DataFrame(X_train).iloc[i][0])) for i in range(0,len(X_train))])\n    non_word_char_count=pd.Series([len(re.findall(r'\\W',pd.DataFrame(X_train).iloc[i][0])) for i in range(0,len(X_train))])\n    X_train_all3=add_feature(add_feature(add_feature(X_train_vectorized,length_of_doc),digit_count),non_word_char_count)\n    log_reg=MultinomialNB()\n    model=log_reg.fit(X_train_all3,y_train)\n    X_test_len_dig_added=add_feature(add_feature(vect.transform(X_test),pd.Series(len(pd.DataFrame(X_test).iloc[i]) for i in range(0,len(X_test)))),pd.Series([len(re.findall(r'\\d',pd.DataFrame(X_test).iloc[i][0])) for i in range(0,len(X_test))]))\n    X_test_all3_added=add_feature(X_test_len_dig_added,pd.Series([len(re.findall(r'\\W',pd.DataFrame(X_test).iloc[i][0])) for i in range(0,len(X_test))]))\n    predictions=model.predict(X_test_all3_added)\n    auc_score=roc_auc_score(y_test,predictions)\n    feature_names=np.array(vect.get_feature_names())\n    feature_names=np.append(feature_names,['length_of_doc', 'digit_count', 'non_word_char_count'])\n    sorted_coef=model.coef_[0].argsort()\n    return  (auc_score,list(feature_names[sorted_coef[:10]]), list(feature_names[sorted_coef[:-11:-1]]))\nprint('Model Score: ',round(answer_eleven()[0],2))","628e8154":"### What is the average number of non-word characters (anything other than a letter, digit or underscore) per document for not spam and spam documents?","1736e0d3":"### Fit and transform the training data `X_train` using a Count Vectorizer with default parameters and fit the a multinominal Naive Bayes Classifier.\n\nNext, fit a fit a multinomial Naive Bayes classifier model with smoothing `alpha=0.1`. Find the area under the curve (AUC) score using the transformed test data.","155ead2d":"### Question 6\n\nWhat is the average length of documents (number of characters) for not spam and spam documents?\n\n*This function should return a tuple (average length not spam, average length spam).*","19c2f96f":"### What percentage of the documents in `spam_data` are spam?","037caff9":"### Fit and transform the training data X_train using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **5**.\n\nUsing this document-term matrix and an additional feature, **the length of document (number of characters)**, fit a Support Vector Classification model with regularization `C=10000`. Then compute the area under the curve (AUC) score using the transformed test data.","6ff8639f":"### Fit and transform the training data `X_train` using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **5** and using **word n-grams from n=1 to n=3** (unigrams, bigrams, and trigrams).\n\nUsing this document-term matrix and the following additional features:\n* the length of document (number of characters)\n* **number of digits per document**\n\nfit a Logistic Regression model with regularization `C=100`. Then compute the area under the curve (AUC) score using the transformed test data.","f977acd5":"### Fit and transform the training data `X_train` using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **3**.\n\nThen fit a multinomial Naive Bayes classifier model with smoothing `alpha=0.1` and compute the area under the curve (AUC) score using the transformed test data.","2397d6e2":"### What is the average number of digits per document for not spam and spam documents?","450ac51f":"### Fit and transform the training data `X_train` using a Tfidf Vectorizer with default parameters.\n\n\nWhat 20 features have the smallest tf-idf and what 20 have the largest tf-idf?\n    Put these features in a two series where each series is sorted by tf-idf value and then alphabetically by feature name. The index of the series should be the feature name, and the data should be the tf-idf.\n    The series of 20 features with smallest tf-idfs should be sorted smallest tfidf first, the list of 20 features with largest tf-idfs should be sorted largest first. ","3755ca1b":"## Explore text message data and create models to predict if a message is spam or not. ","8f6833ba":"### Question 11\n\nFit and transform the training data X_train using a Count Vectorizer ignoring terms that have a document frequency strictly lower than **5** and using **character n-grams from n=2 to n=5.**\n\nTo tell Count Vectorizer to use character n-grams pass in `analyzer='char_wb'` which creates character n-grams only from text inside word boundaries. This should make the model more robust to spelling mistakes.\n\nUsing this document-term matrix and the following additional features:\n* the length of document (number of characters)\n* number of digits per document\n* **number of non-word characters (anything other than a letter, digit or underscore.)**\n\nfit a Logistic Regression model with regularization C=100. Then compute the area under the curve (AUC) score using the transformed test data.\n\nAlso **find the 10 smallest and 10 largest coefficients from the model** and return them along with the AUC score in a tuple.\n\nThe list of 10 smallest coefficients should be sorted smallest first, the list of 10 largest coefficients should be sorted largest first.\n\nThe three features that were added to the document term matrix should have the following names should they appear in the list of coefficients:\n['length_of_doc', 'digit_count', 'non_word_char_count']\n\n*This function should return a tuple `(AUC score as a float, smallest coefs list, largest coefs list)`.*","2a96fe99":"### Fit Count Vectorizer with default parameters.\n\nWhat is the longest token in the vocabulary?"}}