{"cell_type":{"72e9c3fa":"code","5ee8c4ed":"code","d1805581":"code","a11d9804":"code","28b9e1b0":"code","0c0a3815":"code","0b86645c":"code","43244b84":"code","52c34d81":"code","5375e572":"code","8b14247a":"code","a3d90507":"code","3652c944":"code","9a731db0":"code","ac21b1b1":"code","d0aeee5f":"code","71fc1d35":"code","9be1968e":"code","32aedfcb":"code","7c623f86":"code","83472de6":"code","6d1fd0ef":"code","87b05299":"code","2a414c3e":"code","0f51ee2f":"code","3e2abe2b":"code","f0ab4837":"code","dd7396fe":"code","5f86cc4f":"code","04c716ed":"code","c618eab6":"code","d02b5b3c":"code","5cbe9fad":"code","2e4a7a9e":"code","6ec85c72":"code","d7ab54f5":"code","b8c103c3":"code","5dd0305b":"code","336e4469":"code","51337b7a":"code","3c1be140":"code","1c11ad42":"code","4699517e":"code","c9ae5fea":"code","96c5ba55":"code","a0d7aaaf":"code","99cd7803":"code","f785f3cc":"code","90a07157":"code","7a95f5af":"code","a5db629a":"markdown","8bdde598":"markdown","c652646e":"markdown","0e5f521d":"markdown","35c7dd07":"markdown","cdadcc9a":"markdown","cc883c80":"markdown","4acdaa16":"markdown","bc5077d1":"markdown","d3df6fa9":"markdown","ef0e0fe5":"markdown","398bd814":"markdown"},"source":{"72e9c3fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5ee8c4ed":"!pip install transformers","d1805581":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","a11d9804":"import transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup, BertForSequenceClassification\nimport torch\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom collections import defaultdict\nfrom textwrap import wrap\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n","28b9e1b0":"sns.set(style='whitegrid', palette='muted', font_scale=1.2)\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\nrcParams['figure.figsize'] = 10, 6\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)","0c0a3815":"sns.countplot(train.target)\nplt.xlabel('train target')","0b86645c":"PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'","43244b84":"tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)","52c34d81":"sample_txt = 'When was I last outside? I am stuck at home for 2 weeks.'\ntokens = tokenizer.tokenize(sample_txt)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(f' Sentence: {sample_txt}')\nprint(f'   Tokens: {tokens}')\nprint(f'Token IDs: {token_ids}')","5375e572":"print(tokenizer.sep_token, tokenizer.sep_token_id)\nprint(tokenizer.cls_token, tokenizer.cls_token_id)\nprint(tokenizer.pad_token, tokenizer.pad_token_id)\nprint(tokenizer.unk_token, tokenizer.unk_token_id)","8b14247a":"# All of that work can be done using the encode_plus() method\nencoding = tokenizer.encode_plus(\n  text=sample_txt,\n  max_length=32,           # max length of sentence \n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=False,\n  padding='max_length',\n  return_attention_mask=True,\n  return_tensors='pt',  # Return PyTorch tensors\n)\nencoding.keys()","a3d90507":"# The token ids are now stored in a Tensor and padded to a length of 32\nprint(len(encoding['input_ids'][0]))\nencoding['input_ids'][0]","3652c944":"# The attention mask has the same length:\nprint(len(encoding['attention_mask'][0]))\nencoding['attention_mask']","9a731db0":"# We can inverse the tokenization to have a look at the special tokens\nprint(tokenizer.convert_ids_to_tokens(encoding['input_ids'][0]))","ac21b1b1":"# Choosing max Sequence Length for our data set \n# BERT works with fixed-length sequences. We\u2019ll use a simple strategy to choose the max length.\nall_tweets = np.concatenate([train.text.values, test.text.values])\n\ntoken_lens = []\nfor txt in all_tweets:\n  tokens = tokenizer.encode(txt, max_length=512)\n  token_lens.append(len(tokens))","d0aeee5f":"# plot the distribution\nsns.distplot(token_lens)\nplt.xlim([0, 256]);\nplt.xlabel('Token count')","71fc1d35":"MAX_LEN = max(token_lens)\nMAX_LEN","9be1968e":"class TweetDataset(Dataset):\n  def __init__(self, text, targets, tokenizer, max_len):\n    self.text = text\n    self.targets = targets\n    self.tokenizer = tokenizer\n    self.max_len = max_len\n    \n  def __len__(self):\n    return len(self.text)\n\n\n  def __getitem__(self, item):\n    text = str(self.text[item])\n    target = self.targets[item]\n    encoding = self.tokenizer.encode_plus(\n      text,\n      add_special_tokens=True,\n      max_length=self.max_len,\n      return_token_type_ids=False,\n      padding='max_length',\n      truncation=True,\n      return_attention_mask=True,\n      return_tensors='pt',\n    )\n    \n    \n    return {\n      'text': text,\n      'input_ids': encoding['input_ids'].flatten(),\n      'attention_mask': encoding['attention_mask'].flatten(),\n      'targets': torch.tensor(target, dtype=torch.long)\n    }","32aedfcb":"df_train, df_val = train_test_split(\n  train,\n  test_size=0.1,\n  random_state=RANDOM_SEED\n)\n\ndf_test = test\ndf_train.shape, df_val.shape, df_test.shape","7c623f86":"BATCH_SIZE = 16","83472de6":"def create_data_loader_train(df, tokenizer, max_len, batch_size):\n  dataset = TweetDataset(\n    text=df.text.to_numpy(),\n    targets=df.target.to_numpy(),\n    tokenizer=tokenizer,\n    max_len=max_len\n  )\n\n  return DataLoader(\n    dataset,\n    batch_size=batch_size,\n    num_workers=2\n  )","6d1fd0ef":"class TweetDatasetTest(Dataset):\n  def __init__(self, text, tokenizer, max_len):\n    self.text = text\n    self.tokenizer = tokenizer\n    self.max_len = max_len\n    \n  def __len__(self):\n    return len(self.text)\n\n\n  def __getitem__(self, item):\n    text = str(self.text[item])\n    encoding = self.tokenizer.encode_plus(\n      text,\n      add_special_tokens=True,\n      max_length=self.max_len,\n      return_token_type_ids=False,\n      padding='max_length',\n      truncation=True,\n      return_attention_mask=True,\n      return_tensors='pt'\n    )\n    \n    \n    return {\n      'text': text,\n      'input_ids': encoding['input_ids'].flatten(),\n      'attention_mask': encoding['attention_mask'].flatten()\n    }","87b05299":"def create_data_loader_test(df, tokenizer, max_len, batch_size):\n  dataset = TweetDatasetTest(\n    text=df.text.to_numpy(),\n    tokenizer=tokenizer,\n    max_len=max_len\n  )\n\n  return DataLoader(\n    dataset,\n    batch_size=batch_size,\n    num_workers=2\n  )","2a414c3e":"CUSTOM_MAX_LEN = 60\ntrain_data_loader = create_data_loader_train(df_train, tokenizer, CUSTOM_MAX_LEN, BATCH_SIZE)\nval_data_loader = create_data_loader_train(df_val, tokenizer, CUSTOM_MAX_LEN, BATCH_SIZE)\ntest_data_loader = create_data_loader_test(df_test, tokenizer, CUSTOM_MAX_LEN, BATCH_SIZE)","0f51ee2f":"import warnings\nwarnings.filterwarnings(\"ignore\")\ndata = next(iter(train_data_loader))\ndata.keys()","3e2abe2b":"print(data['input_ids'].shape)\nprint(data['attention_mask'].shape)\nprint(data['targets'].shape)","f0ab4837":"bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)","dd7396fe":"# return last state of encoder and pooled output \n# The last_hidden_state is a sequence of hidden states of the last layer of the model. \n# Obtaining the pooled_output is done by applying the BertPooler on last_hidden_state\n\n# We have the hidden state for each of our 32 tokens (the length of our example sequence).\n# 768 is the number of hidden units in the feedforward-networks\n\n# https:\/\/huggingface.co\/docs\/transformers\/model_doc\/bert#bertmodel\n\n# https:\/\/stackoverflow.com\/questions\/65132144\/bertmodel-transformers-outputs-string-instead-of-tensor\noutput = bert_model(\n  input_ids=encoding['input_ids'],\n  attention_mask=encoding['attention_mask']\n)\n\nprint(type(output))\nprint(output.keys())\nprint(bert_model.config.hidden_size)\n","5f86cc4f":"print(output['last_hidden_state'].shape)","04c716ed":"print(output['pooler_output'].shape)","c618eab6":"import torch\nimport torch.nn as nn\n\nif torch.cuda.is_available():       \n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","d02b5b3c":"class SentimentClassifier(nn.Module):\n  def __init__(self, n_classes, p):\n    super(SentimentClassifier, self).__init__()\n    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n    self.drop = nn.Dropout(p)\n    # torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n    self.softmax = nn.Softmax(dim=1)\n\n    \n  def forward(self, input_ids, attention_mask):\n    output_dict = self.bert(\n      input_ids=input_ids,\n      attention_mask=attention_mask\n    )\n    output = self.drop(output_dict['pooler_output'])\n    output = self.out(output)\n    return self.softmax(output)","5cbe9fad":"N_CLASSES = 2\nP = 0.2\nmodel = SentimentClassifier(N_CLASSES, P)\nmodel = model.to(device)","2e4a7a9e":"# data = next(iter(train_data_loader))\nprint(len(data['text']))\n# data is consisted of 16 tweets ","6ec85c72":"input_ids = data['input_ids'].to(device)\nattention_mask = data['attention_mask'].to(device)","d7ab54f5":"print(input_ids.shape)\nprint(attention_mask.shape)","b8c103c3":"# inject input ids and attention mask into Sentiment Classifier \n# this returns the probablity for 0 or 1 for each tweet in data \nmodel(input_ids, attention_mask)","5dd0305b":"EPOCHS = 10\noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\ntotal_steps = len(train_data_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)\nloss_fn = nn.CrossEntropyLoss().to(device)","336e4469":"def train_epoch(\n      model,\n      data_loader,\n      loss_fn,\n      optimizer,\n      device,\n      scheduler,\n      n_examples\n):\n      model = model.train()\n      losses = []\n      correct_predictions = 0\n        \n      for data in data_loader:\n            input_ids = data[\"input_ids\"].to(device)\n            attention_mask = data[\"attention_mask\"].to(device)\n            targets = data[\"targets\"].to(device)\n\n            # inject inputs ids and attention mask into bert model \n            outputs = model(\n              input_ids=input_ids,\n              attention_mask=attention_mask\n            )\n\n            # return prediction \n            _, preds = torch.max(outputs, dim=1)\n\n            # calculate loss \n            loss = loss_fn(outputs, targets)\n\n            # calculate correct prediction \n            correct_predictions += torch.sum(preds == targets)\n            \n            # append each loss\n            losses.append(loss.item())\n            \n            # back propagation \n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            # updata parameters \n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n            \n      return correct_predictions.double() \/ n_examples, np.mean(losses)","51337b7a":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n  model = model.eval()\n  losses = []\n  correct_predictions = 0\n  with torch.no_grad():\n    for data in data_loader:\n      input_ids = data[\"input_ids\"].to(device)\n      attention_mask = data[\"attention_mask\"].to(device)\n      targets = data[\"targets\"].to(device)\n      outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n      )\n        \n      _, preds = torch.max(outputs, dim=1)\n      loss = loss_fn(outputs, targets)\n      correct_predictions += torch.sum(preds == targets)\n      losses.append(loss.item())\n        \n  return correct_predictions.double() \/ n_examples, np.mean(losses)","3c1be140":"%%time\nhistory = defaultdict(list)\nbest_accuracy = 0\n\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}\/{EPOCHS}')\n    print('-' * 10)\n    \n    train_acc, train_loss = train_epoch(\n                            model,\n                            train_data_loader,\n                            loss_fn,\n                            optimizer,\n                            device,\n                            scheduler,\n                            len(df_train)\n                        )\n    \n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    \n    val_acc, val_loss = eval_model(\n                            model,\n                            val_data_loader,\n                            loss_fn,\n                            device,\n                            len(df_val)\n                        )\n    \n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n    print()\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    \n    if val_acc > best_accuracy:\n        torch.save(model.state_dict(), 'best_model_state.bin')\n        best_accuracy = val_acc","1c11ad42":"plt.plot(history['train_acc'], label='train accuracy')\nplt.plot(history['val_acc'], label='validation accuracy')\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1])","4699517e":"test_data = next(iter(test_data_loader))\nprint(test_data['text'])","c9ae5fea":"print(test_data['input_ids'])","96c5ba55":"print(test_data['attention_mask'])","a0d7aaaf":"def get_predictions(model, data_loader):\n  model = model.eval()\n  tweets = []\n  predictions = []\n  prediction_probs = []\n  \n  with torch.no_grad():\n    for data in data_loader:\n      texts = data[\"text\"]\n      input_ids = data[\"input_ids\"].to(device)\n      attention_mask = data[\"attention_mask\"].to(device)\n      \n      outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n      )\n    \n      _, preds = torch.max(outputs, dim=1)\n      tweets.extend(texts)\n      predictions.extend(preds)\n      prediction_probs.extend(outputs)\n      \n  predictions = torch.stack(predictions).cpu()\n  prediction_probs = torch.stack(prediction_probs).cpu()\n  \n  return tweets, predictions, prediction_probs","99cd7803":"_, y_pred, _ = get_predictions(model, test_data_loader)","f785f3cc":"print(len(y_pred.tolist()))","90a07157":"submission['target'] = y_pred.tolist()\nsubmission.head()","7a95f5af":"submission.to_csv('submission.csv', index=False)","a5db629a":"Create PyTorch dataset","8bdde598":"Build Sentiment Classifier","c652646e":"How do we come up with all hyperparameters? The BERT authors have some recommendations for fine-tuning:\n\n* Batch size: 16, 32\n* Learning rate (Adam): 5e-5, 3e-5, 2e-5\n* Number of epochs: 2, 3, 4","0e5f521d":"Predictions","35c7dd07":"Training the model should look familiar, except for two things.\n\nThe scheduler gets called every time a batch is fed to the model. \n\nWe\u2019re avoiding exploding gradients by clipping the gradients of the model using clipgrad_norm.","cdadcc9a":"(https:\/\/www.analyticsvidhya.com\/blog\/2021\/05\/all-you-need-to-know-about-bert\/#:~:text=%5BCLS%5D%20is%20a%20special%20classification,segment%20embeddings%20for%20each%20token)","cc883c80":"Train Model \n\nTo reproduce the training procedure from the BERT paper, we\u2019ll use the AdamW optimizer provided by Hugging Face. \n\nIt corrects weight decay, so it\u2019s similar to the original paper. We\u2019ll also use a linear scheduler with no warmup steps","4acdaa16":"Bert Tokenizer: \n* Add special tokens to separate sentences and do classification\n* Pass sequences of constant length (introduce padding)\n* Create array of 0s (pad token) and 1s (real token) called attention mask","bc5077d1":"![bert-tokens.PNG](attachment:c30e8f11-f363-4841-8858-bff8efc1d790.PNG)","d3df6fa9":"Bert Model","ef0e0fe5":"Reference:\n\nMany thanks to Venelin Valkov for the great tutorial ([1], [2], [3]): \n\n[1] https:\/\/curiousily.com\/posts\/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python\/\n\n[2] https:\/\/www.youtube.com\/watch?v=Osj0Z6rwJB4\n\n[3] https:\/\/www.youtube.com\/watch?v=8N-nM3QW7O0&t=733s\n\nhttps:\/\/www.kaggle.com\/swarnabha\/pytorch-text-classification-torchtext-lstm\n\nhttps:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n\nhttps:\/\/www.kaggle.com\/vishalsiram50\/fine-tuning-bert-88-accuracy\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2021\/05\/all-you-need-to-know-about-bert\/#:~:text=%5BCLS%5D%20is%20a%20special%20classification,segment%20embeddings%20for%20each%20token.","398bd814":"https:\/\/curiousily.com\/posts\/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python\/\n\nSpecial Tokens:\n\n[SEP] - marker for ending of a sentence\n\n[CLS] - we must add this token to the start of each sentence, so BERT knows we\u2019re doing classification\n\n[PAD] - There is also a special token for padding\n\nBERT understands tokens that were in the training set. Everything else can be encoded using the [UNK] (unknown) token"}}