{"cell_type":{"cc403376":"code","7b3a7f81":"code","0711ad6f":"code","034bd46d":"code","880d9a92":"code","bcd81f9f":"code","9b482d1d":"code","34d27d01":"code","dd42f556":"code","e0ccf1ca":"code","9f0179c9":"code","b072b7e2":"code","bc3038e4":"code","ea36d32c":"code","8c2c3388":"code","8d790a14":"code","c53eae53":"code","d41882a8":"code","05079f4b":"code","9ebbb452":"code","f31ba937":"code","688aba73":"code","7bbb97e6":"code","da3d1c90":"code","3d63b89b":"code","544d9318":"code","9845a5dd":"code","c4cb3655":"code","9f041c77":"code","b0a45cf0":"code","cb599c50":"code","a5a754cf":"code","d24350f7":"code","42edbcb6":"code","6901972d":"markdown","8fd6bc2a":"markdown","5badcd2b":"markdown","81ee36a6":"markdown","3d4aaff5":"markdown","af2d8dd2":"markdown","e64c28b2":"markdown","6e3c4a6b":"markdown","c586df93":"markdown","3bc14b31":"markdown","acd302ab":"markdown","7f398b12":"markdown","91b28987":"markdown","f1b07c7a":"markdown","74c15a69":"markdown","5f687b5d":"markdown","cff7addd":"markdown","61ed262d":"markdown","8ba4d815":"markdown","cafb49ba":"markdown","158604f4":"markdown","e4abe6c8":"markdown","0b13aa7f":"markdown","a8671971":"markdown","b0c57c0a":"markdown","5d06a597":"markdown","f75d4961":"markdown","cfdaa7fe":"markdown","893368c9":"markdown"},"source":{"cc403376":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7b3a7f81":"import matplotlib.pyplot as plt #visualization library in Python for 2D plots of arrays\nimport seaborn as sns #interactive visualization library\nimport xgboost as xgb #uses a gradient boosting framework\nfrom sklearn.model_selection import GridSearchCV #implements a \u201cfit\u201d and a \u201cscore\u201d method\nfrom sklearn.model_selection import cross_val_score #cross-validation\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.ensemble import GradientBoostingRegressor#it allows for the optimization of arbitrary differentiable loss functions\nfrom sklearn.preprocessing import StandardScaler#for scalling\nfrom sklearn.metrics import r2_score,mean_squared_error# for scoring\n","0711ad6f":"train= pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","034bd46d":"print(train.shape)\nprint(test.shape)\n\n#print(train.info())\nprint(\"number of Non_catgorical feature\",train.dtypes[train.dtypes != \"object\"].count())\nprint(\"number of catgorical feature\",train.dtypes[train.dtypes == \"object\"].count())","880d9a92":"train.describe()","bcd81f9f":"plt.figure(figsize=(18,12))\nsns.heatmap(train.corr(),annot=True, )","9b482d1d":"# Features most correlated with SalePrice\ncorr_matrix_pearson=train.corr()\ntop_correlated = corr_matrix_pearson.nlargest(10, 'SalePrice')[\n    'SalePrice'].index\ncorr_matrix_pearson_top_correlated = corr_matrix_pearson.loc[top_correlated, top_correlated]\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix_pearson_top_correlated, cbar=True,\n            annot=True, square=True, fmt='.2f', annot_kws={'size': 12})\nplt.show()","34d27d01":"sns.set_style(\"whitegrid\")\ncols = corr_matrix_pearson_top_correlated['SalePrice'][corr_matrix_pearson_top_correlated['SalePrice']>0.6].index\nplt.figure(figsize=(8,6))\nsns.pairplot(train[cols], size = 2)","dd42f556":"plt.figure()\nsns.pairplot(train,x_vars=[\"GrLivArea\"], y_vars='SalePrice', size=5)#,kind='scatter' or 'reg')\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.title(\"GrLivArea vs SalePrice\")\nplt.show()","e0ccf1ca":"plt.figure(figsize=(16,8))\nsns.distplot(train.SalePrice)\nplt.xlabel(\"SalePrice\")\nplt.ylabel(\"count\")\nplt.title(\"Sale_Price_distribution\")","9f0179c9":"plt.figure()\nsns.pairplot(train,x_vars=[\"GrLivArea\"], y_vars=['SalePrice'], hue=\"SaleCondition\",size=5)#,kind='scatter' or 'reg')\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.title(\"GrLivArea vs SalePrice\")\nplt.show()","b072b7e2":"plt.figure()\nsns.pairplot(train,x_vars=[\"GrLivArea\"], y_vars=['SalePrice'],size=5)#,kind='scatter' or 'reg')\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.title(\"GrLivArea vs SalePrice\")\nplt.show()","bc3038e4":"train.drop(train[(train[\"GrLivArea\"]>4000) & (train['SalePrice']<300000)].index, inplace=True)","ea36d32c":"plt.figure()\nsns.pairplot(train,x_vars=[\"GrLivArea\"], y_vars=['SalePrice'],size=5)#,kind='scatter' or 'reg')\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.title(\"GrLivArea vs SalePrice\")\nplt.show()","8c2c3388":"features_all = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'], test.loc[:,'MSSubClass':'SaleCondition']))\nfeatures_all.shape","8d790a14":"print(\"train dataset have missing values:\",np.sum(train.isnull().sum()))\nprint(\"test dataset have missing values:\",np.sum(test.isnull().sum()))\nprint(\"feature_all have missing values:\",np.sum(features_all.isnull().sum()))\n\ndef count_missing_data(df):\n    \"\"\" Counts the missing values for each features\n        and display the Total and the Percentage\n    \"\"\"\n    # Calculates the total number of missing values for each feature\n    total = df.isnull().sum().sort_values(ascending=False)\n    \n    # Calculates the percentage of missing values for each features\n    percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    \n    # Combines the percentage and totals\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    \n    # Only returns data that have missing values\n    return missing_data[missing_data['Percent']>0]\n\nprint(\"Number of features with missing values: {}\".format(len(count_missing_data(features_all))))\n\ncount_missing_data(features_all)","c53eae53":"def fillna_numerica_average(df):\n    \"\"\" Fills the numeric features that contain \n        NaN with the average in those columns\n    \"\"\"\n    # Get features that contain missing values\n    features_with_nan = count_missing_data(df).index \n    \n    # Get numeric features that contain missing values\n    numeric_missing_features = df[features_with_nan].dtypes[df.dtypes != \"object\"].index \n    \n    # Fill each missing values for the numeric features with the corresponding median\n    for feature in numeric_missing_features:\n        df[feature].fillna(df[feature].median(), inplace=True)\n        \nfillna_numerica_average(features_all)","d41882a8":"# Shows the missing values again, but seeing as we have filled all numerical features, the categorical features are left\nlist_missing_data = count_missing_data(features_all)\nprint(\"Number of features with missing values: {}\".format(len(list_missing_data)))\nprint(\"Categorical features\")\nlist_missing_data","05079f4b":"# #log transform the target:\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","9ebbb452":"plt.figure(figsize=(16,8))\nsns.distplot(train.SalePrice)\nplt.xlabel(\"SalePrice\")\nplt.ylabel(\"count\")\nplt.title(\"Sale_Price_distribution\")","f31ba937":"features_all = pd.get_dummies(features_all)","688aba73":"# Top 20 features with largest positive skew\nimport scipy.stats as stats\nskewness = train.select_dtypes(include=np.number).apply(lambda x: stats.skew(x, nan_policy='propagate'))\nskewness = pd.DataFrame(skewness.sort_values(ascending=False))\nskewness.columns = ['Skew']\nskewness.head(20)","7bbb97e6":"numeric_feature_names = features_all.dtypes[features_all.dtypes != \"object\"].index\n\n# # Selecting only the numeric features\nfrom scipy.stats import skew\n\n# # Calculates the skewedness of the features and then gets the features with a skewedness above a certain threshold\nskewed_features = features_all[numeric_feature_names].apply(lambda x: skew(x, nan_policy='propagate'))\nskewed_features = skewed_features[skewed_features > 0.70].index\n\nfeatures_all[skewed_features] = np.log1p(features_all[skewed_features])","da3d1c90":"scaler = StandardScaler()\nscaler.fit(features_all[numeric_feature_names])\nscaled = scaler.transform(features_all[numeric_feature_names])","3d63b89b":"from sklearn.feature_selection import VarianceThreshold\nvariance = VarianceThreshold(0.01)\nfeatures_all = variance.fit_transform(features_all)","544d9318":"target = train['SalePrice']\nX_train = features_all[:train.shape[0]]\ntest_data = features_all[train.shape[0]:]","9845a5dd":"# Gradient Boosting\ngradient = GradientBoostingRegressor(n_estimators = 3000, min_samples_leaf = 15, learning_rate = 0.05, max_features = 'sqrt',\n                                      max_depth = 3, min_samples_split = 10, loss = 'huber')\ngradient.fit(X_train, target)\n\n# XGBoost\nxgb = xgb.XGBRegressor(n_estimators = 30000, colsample_bytree = 0.2, gamma = 0.0,  reg_lambda = 0.6,  min_child_weight = 1.5,\n                  reg_alpha = 0.9, learning_rate = 0.01, max_depth = 4, subsample = 0.2, seed = 42, silent = 1)\nxgb.fit(X_train, target)","c4cb3655":"print('XGBoost CV r2: {}'.format(cross_val_score(xgb, X_train,target,cv=5,scoring='r2')))\nprint('Gradient CV r2: {}'.format(cross_val_score(gradient, X_train,target,cv=5,scoring='r2')))","9f041c77":"rmse_xgb=np.sqrt(-cross_val_score(xgb, X_train,target,cv=5,scoring='neg_mean_squared_error'))\nrmse_gradient=np.sqrt(-cross_val_score(gradient, X_train,target,cv=5,scoring='neg_mean_squared_error'))\n\nprint('XGBoost CV RMSE: {}'.format(rmse_xgb))\nprint('rmse_gradient CV RMSE: {}'.format(rmse_gradient))","b0a45cf0":"# Gradient Boosting\ny_pred_gradient = gradient.predict(X_train)\n\n# XGBoost\ny_pred_xgb = xgb.predict(X_train)","cb599c50":"# Averaging scores using weights\ny_pred = (0.70*y_pred_xgb + 0.30*y_pred_gradient)","a5a754cf":"# XGBoost\nimport xgboost as xgb\n\nxgb = xgb.XGBRegressor(n_estimators = 30000, colsample_bytree = 0.2, gamma = 0.0,  reg_lambda = 0.6,  min_child_weight = 1.5,\n                       reg_alpha = 0.9, learning_rate = 0.01, max_depth = 4, subsample = 0.2, seed = 42, silent = 1)\nxgb.fit(X_train, target)\ny_pred_xgb = xgb.predict(test_data)\n\n# Gradient boosting\ngradient = GradientBoostingRegressor(n_estimators = 3000, min_samples_leaf = 15, learning_rate = 0.05, max_features = 'sqrt',\n                                      max_depth = 3, min_samples_split = 10, loss = 'huber', random_state = 42)\ngradient.fit(X_train, target)\ny_pred_gradient = gradient.predict(test_data)","d24350f7":"y_pred = (.80*y_pred_xgb +.20*y_pred_gradient)","42edbcb6":"solution = pd.DataFrame({\"id\":test.Id, \"SalePrice\": np.expm1(y_pred)})\nsolution.to_csv(\"submission2.csv\", index = False)","6901972d":"### Combining Features\n\nThe data that was given was split into a test and train part. However, if I would want to transform categorical labels to numbers and I would do that only for the train set, then there might exist some levels\/labels in the test set which we do not know. Therefore, I merge the test and train data so that I can get dummies (which I will do later) and create numbers for all labels. Furthermore, now I can also look for missing values in the test set.","8fd6bc2a":"### Missing Value\nobserved many features, many don't have any values and are reported as NaN. In this dataset I find it highly unlikely that it was possible to report a value for everyone of the 81 features. Thus, I will create a list of the names of the columns that have missing values (solely based on NaN). This list can then be used to decide on what to do with those values.","5badcd2b":"Observation\n\nThere's a lot going on the in the correlation matrix. We see that there are some clusters that are highly correlated like GarageArea and GarageCars, which makes sense since having a garage that can hold more cars would likely also have a greater area. It is possible that there's some multicollinearity which affects the results.\n\nNext, let's take a look at the features that are highly correlated with each other and look at those in more detail. I will select features that have a pearson's correlation of 10 or more","81ee36a6":"Although we now can clearly see the relationships, I also notice that a lot of features have some heteroscedasticity and some are not normally distributed. It might be worth to look into those features in more detail","3d4aaff5":"# Data Collection","af2d8dd2":"first Will see the correlation\n\nI will focus on the numeric features and see in what way they are related to the target variable. To do this, a correlation matrix is often the first step just to see what is exactly going on with the data.\nAfter that, I will look into some individual plots showing more on the relationship between that feature and the target.","e64c28b2":"# Modelling \nXGBoost\n\nXGBoost is very similar to a gradient booster in that it follows gradient boosting. XGBoost uses a more regularized model formalization to control over-fitting, which gives it better performance.\n\nGradient Booster\n\nThe gradient booster basically takes many weak predictive models and boosts it into a stronger one, which represents an ensemble of weak models. Often, the weak model can be any model that works a little better than a random guess.","6e3c4a6b":"Exploration of the target variable \"SalePrice\" and plot its distribution to gain some information & later we will be work on input variable.\n\nwe can see that the target variable \"SalePrice\" has a skewed distribution. Because I will be predicting a continuous feature, I will be using regressors which perform better if the target variable has a normal distribution with little to no skewdness. Thus, it is important to remember that in a later stage the data needs to be normalized.","c586df93":"# Model evaluation\nFor int\/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used.","3bc14b31":"If you found this notebook helpful or you just liked it , some upvotes would be very much appreciated - That will keep me motivated to update it on a regular basis ,Thanks","acd302ab":"# Data Processing\nWhile EDA-Displot we have observed that Clearly, the distribution of target variable is skewed to the right, which means that we have to transform the current distribution to one that has a normal distribution. Below we will simply do that by computing the log(1+x) of all data points within the target variable.","7f398b12":"### Scaling Numerical Features","91b28987":"There seems to be 2 extreme outliers on the bottom right, really large houses that sold for really cheap. Seeing the trends in the plot above I believe these two points to be outliers and not representative for the entire dataset. Thus, my initial conclusion is to delete those outliers","f1b07c7a":"now plot seems to look a lot better and more representative of the data. However, we also see the shape of a cone in the plot, which may suggest that there is something going on with the distribution of the variables.\n\nThere are probably others outliers in the training data. However, removing all them may affect badly our models if ever there were also outliers in the test data. That's why , instead of removing them all, we will just manage to make some of our models robust on them.\n\nNext, I'll take a look at the distribution of the target variable.","74c15a69":"Based on the prediction scores, I devised a ensemble of multiple predictors using certain weights.","5f687b5d":"# Model Submission with test set","cff7addd":"# Model Prediction","61ed262d":"I will fill the missing values for the Numerical features with the median using the function below.\n\n","8ba4d815":"### Transforming Categorical Features\nIf I would simply label the categorical features, then that would imply some meaning in the distances between data points, that is not the case. Thus, I choose to make dummy features of all categorical features so that they will displayed as having some category or not.","cafb49ba":"Things I tried, but didn't work...\n\nI've tried some different methods for filling in the missing values. So far, simply taking the median of numeric features and taking the median of categoricals gave me the best score.\n\nI will list here what I have tried and my view on why it didn't work.\n\nFilling the missing values for numerical features, for example GarageYrBlt, with 0.\n\nSince the corresponding feature \"GarageCars\" has the value 0 for the missing values in GarageYrBlt, this led me to think that there is no Garage for that house and GarageYrBlt cannot be filled with the median. However, 0 is technically also not correct, since no garage was built in the year 0. Dropping would make sense, but it lowered my score compared to filling it with the median. I may be overfitting the data or there actually may be a garage, but it cannot hold any cars.\n\nFilling the missing values for categorical features, for example GarageType, with \"No Garage\" Similarly, some categorical features were missing and using the mode didn't help the score, which assumes there are not other categories than is listed in the features. Since it didn't help, there might be more categories, which is why I figured that the missing values for GarageType meant that there was no garage. So I filled it with \"No Garage\", which also didn't help my score. Overfitting might have happened, but it may also be that there actually is a Garage","158604f4":"#### Variance Threshold\nHere, I'm selecting features for which most values are the same. If most values are the same for a certain feature, then it only adds noise and will likely make our model overfit the data. Thus, we drop the features that have a low variance (which significantly improved my score).","e4abe6c8":"Observation\n\nThere is some multicollinearity within the \"Garage\" features.\nSalePrice is higly correlated with OverallQual and GrLivArea\nSome people have bought a house without a garage and built the garage in later, which is why the correlation isn't 1\nNext, I will look at all the features that have a high correlation with SalePrice\n\nThe overall quality of the house has the highest correlation with the sale price.\nThe Garage seems to be an important feature, as well as the number of floors and whether it has a basement or not.\nAlthough we have a lot of numbers telling us a lot of things, it would be more intuitive if the data was actually visualized.\nThe type of relation is not clear purely based on pearson's r, thus I will use a pairplot to plot all features that are highly correlated with SalePrice.","0b13aa7f":"# Train Test Split\nBecause we concatenated the data in an earlier stage, we can simply use the shape of the train data to mark where we have added the test data so that we can split all_features into X_train and X_test. That way, we have the same data that was in the training dataset and in the testing dataset.","a8671971":"# Feature Engineering\nBelow points are observed during the exploration that I need to work on:\n\nThere were some outliers\nThe target feature is not normally distributed\nSome features showed heteroscedasticity\nMulticollinearity\nAlthough multicollinearity was found in some features, I did not find any improvement in the scores if I deleted those features. Therefore, I simply let them be.\nFurthermore, I will be looking at:\n\nVariance of features\nMissing Values\n","b0c57c0a":"The graph doesn't give much information. It may be then that the salecondition doens't influence the relationship between living area and saleprice. The main conclusion is that most houses seem to be of condition \"Normal\". Perhaps we see something different for neighborhood.\n\n","5d06a597":"There are two things that you should see immediately:\n\nThere are two outliers on the far right (which we will deal with at a later stage)\nThere is some heteroscedasticity going on (which we will also deal with later)","f75d4961":"# Data Visuallization & EDA","cfdaa7fe":"This already looks a lot better! Next, it would be wise to do the same for the independent variables that have a high skewedness.","893368c9":"### Outliers\nIn this machine learning task, it will be likely that I'll be using some form of regression analysis. These kind of methods are very sensitive to outliers, which may decrease the models accuracy. Therefore, I'll be looking for any outliers and remove them if neccesary.\n\n\n"}}