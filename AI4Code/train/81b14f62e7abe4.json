{"cell_type":{"3af1d46e":"code","dbf963b4":"code","135bf3e2":"code","89c54e34":"code","4a0d53a3":"code","e07096c8":"code","0aa0f42d":"code","8fb2ca3f":"code","2f2ca04f":"code","37795f60":"code","125df77a":"code","e1b5954a":"code","e5c4a10d":"code","c84536f5":"code","298db755":"code","208ce491":"code","78cc84b1":"code","b1632449":"code","3326ccc8":"code","39eaec0d":"code","3a1d53a3":"code","82cf6616":"code","f55ba4c8":"code","dc34430b":"code","8ddc9f09":"code","b90c4bb9":"code","f9d34042":"code","e47cced0":"code","5660cdda":"code","67e56c52":"code","6e3af977":"code","694956a4":"code","2b96123b":"code","1295ab59":"code","072b28a4":"code","58d9cbc7":"code","f0b9e6f8":"code","a0ae6c16":"code","771cf843":"code","8c43e4f8":"code","cb286f70":"code","f702a764":"code","82a70813":"code","f3015af8":"code","871dc0a3":"code","551133fc":"code","27864b4b":"code","78a9abfa":"code","ac4ad41f":"code","c8490aa3":"code","3a1a88aa":"code","ec1a9a0d":"code","867819d5":"code","68afcfc1":"code","65380434":"code","b4130ef0":"code","9c2b4c0e":"code","37f069ac":"code","7963202c":"code","7c5badf9":"code","5d4e5dde":"code","04b5dd45":"code","02751fe2":"code","c738b3a8":"code","f5312cd6":"code","1ae4262a":"code","a8819d46":"code","9b805734":"code","69dafad6":"code","72bf6064":"code","83a5535c":"code","cc7caf91":"code","d0cb971a":"code","ee6a3781":"code","e5f341b8":"code","e04e4078":"code","0458c0f7":"code","0544ae7d":"code","24de444c":"code","657d4947":"code","9dc83ae1":"code","3f2c5151":"code","de7ddfef":"code","b7302d02":"code","6a057bd8":"code","9274c87b":"code","e600fab1":"code","a9902031":"code","f01bdaa2":"code","99ed5444":"code","5ac502b9":"code","bffd0703":"markdown","2c3f3a9e":"markdown","95935eb7":"markdown","4f989cc8":"markdown","afc8f8c4":"markdown","1048e66e":"markdown","3377958b":"markdown","bf4010de":"markdown","09d4b8c3":"markdown","69a2c978":"markdown","441eed02":"markdown","34405a0b":"markdown","74e28bf4":"markdown","e3320233":"markdown","6490af5b":"markdown","e4eb0d88":"markdown","18043d67":"markdown","d41e6258":"markdown","16546c7f":"markdown","ad18d451":"markdown","b3b6baa0":"markdown","7496d496":"markdown","c6e8b10a":"markdown","23d17e55":"markdown"},"source":{"3af1d46e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","dbf963b4":"# pd.read_csv('..\/input\/sample_submission.csv')","135bf3e2":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set() # setting seaborn default for plots\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","89c54e34":"train.head(10)","4a0d53a3":"test.head()","e07096c8":"train.shape","0aa0f42d":"test.shape","8fb2ca3f":"train.info()","2f2ca04f":"test.info()","37795f60":"train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","125df77a":"train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e1b5954a":"train[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e5c4a10d":"train[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c84536f5":"train.corr()","298db755":"plt.figure(figsize=(12, 12))\nsns.heatmap(train.corr(), linewidths=0.1, vmax=0.5, cmap=plt.cm.OrRd, linecolor='white', annot=True)\nplt.show()","208ce491":"train.isnull().sum()","78cc84b1":"test.isnull().sum()","b1632449":"# \uc0b0 \uc0ac\ub78c\uacfc \uc8fd\uc740 \uc0ac\ub78c\uc758 \uac01 \ud53c\uccd0 \ud2b9\uc9d5 \ud655\uc778\ndef bar_chart(feature):\n    survived = train[train['Survived'] == 1][feature].value_counts() # Survived \uac12\uc774 1\uc778 \ud589\ub4e4\uc758 feature\uc5f4 \uc218 count\n    dead = train[train['Survived'] == 0][feature].value_counts() # Survived \uac12\uc774 0\uc778 \ud589\ub4e4\uc758 feature\uc5f4 \uc218 count\n    df = pd.DataFrame([survived, dead]) # \uc0b0\uc0ac\ub78c\uacfc \uc8fd\uc740\uc0ac\ub78c\uc73c\ub85c \ub098\ub204\uc5b4\uc11c DataFrame\uc73c\ub85c \uc800\uc7a5\n    df.index = ['Survived', 'Dead'] # index\uba85 \uc9c0\uc815\n    df.plot(kind='bar', stacked=True, figsize=(10, 5)) # bar chart \uadf8\ub9ac\uae30\n    plt.show()","3326ccc8":"bar_chart('Sex')","39eaec0d":"bar_chart('Pclass')","3a1d53a3":"bar_chart('SibSp')","82cf6616":"bar_chart('Parch')","f55ba4c8":"bar_chart('Embarked')","dc34430b":"train.head(10)","8ddc9f09":"train_test_data = [train, test] # train data\uc640 test data \uacb0\ud569\n\n# train_test_data\uc758 Name\ud544\ub4dc\uc5d0\uc11c Title\uc744 \ubf51\uc74c\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Name'].str.extract('([A-Za-z]+)\\.', expand=False)","b90c4bb9":"# train data\uc758 \ud0c0\uc774\ud2c0 \uc885\ub958 \ubc0f \uc778\uc6d0 \ud655\uc778\ntrain['Title'].value_counts()","f9d34042":"# test data\uc758 \ud0c0\uc774\ud2c0 \uc885\ub958 \ubc0f \uc778\uc6d0 \ud655\uc778\ntest['Title'].value_counts()","e47cced0":"# \ud0c0\uc774\ud2c0\ubcc4\ub85c Mr\ub294 0, Miss\ub294 1, Mrs\ub294 2, \uadf8 \uc678 \ub098\uba38\uc9c0\ub294 3\uc73c\ub85c \ub9e4\ud551\n\ntitle_mapping = {\"Mr\":0, \"Miss\":1, \"Mrs\":2,\n                 \"Master\":3, \"Dr\":3, \"Rev\":3, \"Major\":3, \"Col\":3, \"Mlle\":3, \"Lady\":3,\n                 \"Mme\":3, \"Sir\":3, \"Jonkheer\":3, \"Ms\":3, \"Capt\":3, \"Don\":3, \"Dona\":3,\n                \"Countess\":3}\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)","5660cdda":"train.head(10)","67e56c52":"bar_chart('Title')","6e3af977":"# \ub370\uc774\ud130\uc14b \uc911 \ud544\uc694\uc5c6\ub294 \ud53c\uccd0 \uc0ad\uc81c\ntrain.drop('Name', axis=1, inplace=True)\ntest.drop('Name', axis=1, inplace=True)","694956a4":"train.head()","2b96123b":"sex_mapping = {\"male\":0, \"female\":1}\nfor dataset in train_test_data:\n    dataset['Sex'] = dataset['Sex'].map(sex_mapping)","1295ab59":"bar_chart('Sex')","072b28a4":"# Age \ud544\ub4dc\uc758 NaN\uac12\uc744 Title \uadf8\ub8f9\ubcc4\uc758 \ub098\uc774 \uc911\uac04\uac12\uc73c\ub85c \ucc44\uc6c0\ntrain[\"Age\"].fillna(train.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\ntest[\"Age\"].fillna(test.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)","58d9cbc7":"# train data\uc758 \ub098\uc774\uc5d0 \ub530\ub978 \uc0dd\uc0ac \ud655\uc778\nfacet = sns.FacetGrid(train, hue=\"Survived\", aspect=4)\nfacet.map(sns.kdeplot, 'Age', shade=True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend()\nplt.show()","f0b9e6f8":"# \ub098\uc774\ub300\ubcc4 \uc0dd\uc0ac \ud655\uc778\nfacet = sns.FacetGrid(train, hue=\"Survived\", aspect=4)\nfacet.map(sns.kdeplot, 'Age', shade=True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend()\nplt.xlim(0, 20)","a0ae6c16":"facet = sns.FacetGrid(train, hue=\"Survived\", aspect=4)\nfacet.map(sns.kdeplot, 'Age', shade=True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend()\nplt.xlim(20, 30)","771cf843":"facet = sns.FacetGrid(train, hue=\"Survived\", aspect=4)\nfacet.map(sns.kdeplot, 'Age', shade=True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend()\nplt.xlim(30, 40)","8c43e4f8":"facet = sns.FacetGrid(train, hue=\"Survived\", aspect=4)\nfacet.map(sns.kdeplot, 'Age', shade=True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend()\nplt.xlim(40, 60)","cb286f70":"facet = sns.FacetGrid(train, hue=\"Survived\", aspect=4)\nfacet.map(sns.kdeplot, 'Age', shade=True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend()\nplt.xlim(60)","f702a764":"# \ub098\uc774\ub300\uc5d0 \ub530\ub77c \uadf8\ub8f9 \ub098\ub214\nfor dataset in train_test_data:\n    dataset.loc[dataset['Age'] <= 16, 'Age'] = 0,\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 26), 'Age'] = 1,\n    dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 36), 'Age'] = 2,\n    dataset.loc[(dataset['Age'] > 36) & (dataset['Age'] <= 62), 'Age'] = 3,\n    dataset.loc[dataset['Age'] > 62, 'Age'] = 4","82a70813":"train.head()","f3015af8":"bar_chart('Age')","871dc0a3":"Pclass1 = train[train['Pclass'] == 1]['Embarked'].value_counts()\nPclass2 = train[train['Pclass'] == 2]['Embarked'].value_counts()\nPclass3 = train[train['Pclass'] == 3]['Embarked'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class', '2nd class', '3rd class']\ndf.plot(kind='bar', stacked=True, figsize=(10, 5))","551133fc":"for dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')","27864b4b":"train['Embarked'].isnull().sum()","78a9abfa":"embarked_mapping = {\"S\":0, \"C\":1, \"Q\":2}\nfor dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)","ac4ad41f":"# \ub4f1\uae09\ubcc4 \uc911\uac04\uac12\uc744 NaN\uac12\uc5d0 \ub123\uc5b4\uc90c\ntrain[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\ntest[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)","c8490aa3":"facet = sns.FacetGrid(train, hue=\"Survived\", aspect=4)\nfacet.map(sns.kdeplot, 'Fare', shade=True)\nfacet.set(xlim=(0, train['Fare'].max()))\nfacet.add_legend()\n\nplt.show()","3a1a88aa":"for dataset in train_test_data:\n    dataset.loc[dataset['Fare'] <= 17, 'Fare'] = 0,\n    dataset.loc[(dataset['Fare'] > 17) & (dataset['Fare'] <= 30), 'Fare'] = 2,\n    dataset.loc[(dataset['Fare'] > 30) & (dataset['Fare'] <= 100), 'Fare'] = 4,\n    dataset.loc[dataset['Fare'] > 100, 'Fare'] = 6","ec1a9a0d":"train.head()","867819d5":"train.Cabin.value_counts()","68afcfc1":"for dataset in train_test_data:\n    dataset['Cabin'] = dataset['Cabin'].str[:1]","65380434":"Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts()\nPclass2 = train[train['Pclass']==2]['Cabin'].value_counts()\nPclass3 = train[train['Pclass']==3]['Cabin'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class', '2nd class', '3rd class']\ndf.plot(kind = 'bar', stacked=True, figsize=(10, 5))","b4130ef0":"train.Cabin.value_counts()","9c2b4c0e":"# scaling\n# \uba38\uc2e0 \ub7ec\ub2dd \ubaa8\ub378\uc740 \uac12\uc758 \ucc28\uc774\uac00 \ud074 \uc218\ub85d \ub354 \ud070 \uc758\ubbf8\ub97c \ubd80\uc5ec\ud558\uae30 \ub54c\ubb38\uc5d0 \uac12\uc744 \uc2a4\ucf00\uc77c\ub9c1 \ud574\uc90c\ncabin_mapping = {\"A\":0, \"B\":0.7, \"C\":1.4, \"D\":2.1, \"E\":2.8, \"F\":3.5, \"G\":4.2, \"T\":4.9}\nfor dataset in train_test_data:\n    dataset['Cabin'] = dataset['Cabin'].map(cabin_mapping)","37f069ac":"train[\"Cabin\"].fillna(train.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)\ntest[\"Cabin\"].fillna(train.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)","7963202c":"train.head(10)","7c5badf9":"train[\"FamilySize\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\ntest[\"FamilySize\"] = test[\"SibSp\"] + test[\"Parch\"] + 1","5d4e5dde":"facet = sns.FacetGrid(train, hue=\"Survived\", aspect=4)\nfacet.map(sns.kdeplot, 'FamilySize', shade=True)\nfacet.set(xlim=(0, train['FamilySize'].max()))\nfacet.add_legend()\nplt.xlim(0)","04b5dd45":"family_mapping = {1:0, 2:0.5, 3:1.0, 4:1.5, 5:2.0, 6:2.5, 7:3.0, 8:3.5, 9:4.0, 10:4.5, 11:5.0}\nfor dataset in train_test_data:\n    dataset['FamilySize'] = dataset['FamilySize'].map(family_mapping)","02751fe2":"corr = train.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","c738b3a8":"train.head()","f5312cd6":"# \ud544\uc694\uc5c6\ub294 \ud56d\ubaa9 drop\nfeatures_drop = ['Ticket', 'SibSp', 'Parch', 'Cabin']\ntrain.drop(features_drop, axis=1, inplace=True)\ntest = test.drop(features_drop, axis=1)\ntrain = train.drop(['PassengerId'], axis=1)","1ae4262a":"# \ubaa8\ub378 \uc785\ub825\ub370\uc774\ud130 \uad6c\uc131\uc744 \uc704\ud55c train_data \uc14b \uad6c\uc131\n#train_data = \uc785\ub825 , target = \ucd9c\ub825\ntrain_data = train.drop('Survived', axis=1)\ntarget = train['Survived']\n\ntrain_data.shape, target.shape","a8819d46":"train_data.head(10)","9b805734":"test = test.drop(['PassengerId'], axis=1)","69dafad6":"test.head(10)","72bf6064":"plt.figure(figsize=(8, 8))\nsns.heatmap(train.corr(), linewidths=0.1, vmax=0.5, cmap=plt.cm.gist_heat, linecolor='white', annot=True)\nplt.show()","83a5535c":"import tensorflow as tf\ntf.set_random_seed(777)\n\ntrain_x = train_data\ndf = pd.DataFrame(target) # \uc0b0\uc0ac\ub78c\uacfc \uc8fd\uc740\uc0ac\ub78c\uc73c\ub85c \ub098\ub204\uc5b4\uc11c DataFrame\uc73c\ub85c \uc800\uc7a5\ndf.columns = ['Survived'] # index\uba85 \uc9c0\uc815\ntrain_y = df\ntest_x = test\ntest_y = pd.read_csv('..\/input\/gender_submission.csv')\ndf = pd.DataFrame(test_y['Survived'])\ndf.columns = ['Survived']\ntest_y = df","cc7caf91":"train_x.shape, train_y.shape, test_x.shape, test_y.shape","d0cb971a":"X = tf.placeholder(tf.float32, shape=[None, 7])\nY = tf.placeholder(tf.float32, shape=[None, 1])\n\nW = tf.get_variable(\"W\", shape=[7, 1], initializer=tf.contrib.layers.xavier_initializer())\nb = tf.Variable(tf.random_normal([1]), name='bias')\nH = tf.sigmoid(tf.matmul(X, W) + b)\n\ncost = -tf.reduce_mean(Y*tf.log(H) + (1-Y) * tf.log(1-H))\n\ntrain = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n\npredicted = tf.cast(H > 0.5, dtype=tf.float32)\naccuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))","ee6a3781":"import time\nstartTime = time.time()\nacc_nn = 0.0\nnn_predicted_result = []\nfeature = train_data.columns.tolist()\nWeight = []\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    \n    for step in range(10001):\n        cost_val, _ = sess.run([cost, train], feed_dict={X:train_x, Y:train_y})\n        if step % 1000 == 0:\n            print(step, cost_val)\n                  \n    print('-----------------------------')\n    print('train_data = ', len(train_x), 'test_data = ', len(test_x))\n    Weight = sess.run(W)\n    \n    for i in range(len(feature)):\n        print('W', i, '=', Weight[i], ', feature =', feature[i])\n\n    h, c, y, a = sess.run([H, predicted, Y, accuracy], feed_dict={X:test_x, Y:test_y})\n    \nacc_nn = round(a * 100, 2)\nprint(\"accuracy : \", acc_nn)\nnn_predicted_result = c\nnn_predicted_result = nn_predicted_result.reshape([-1])\nnn_predicted_result = nn_predicted_result.astype(int)\n\nendTime = time.time()\nprint(endTime - startTime, \" sec\")","e5f341b8":"Weight = abs(Weight)\n\nfrom pandas import Series\n\nWeight = np.reshape(Weight, [-1])\n\ntensorflow_feat_imp = Series(Weight, index = feature)","e04e4078":"plt.figure(figsize=(10, 10))\ntensorflow_feat_imp.sort_values(ascending=True).plot.barh()\nplt.xlabel('Weight importance')\nplt.ylabel('Feature')\nplt.show()","0458c0f7":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","0544ae7d":"#Logistic Regression \ubaa8\ub378\n\nlogreg = LogisticRegression()\nlogreg.fit(train_x, train_y)\nY_pred = logreg.predict(test_x)\nacc_log = round(logreg.score(train_x, train_y) * 100 , 2)\nacc_log","24de444c":"# Support Vector Machines \ubaa8\ub378\n\nsvc = SVC()\nsvc.fit(train_x, train_y)\nY_pred = svc.predict(test_x)\nacc_svc = round(svc.score(train_x, train_y) * 100, 2)\nacc_svc","657d4947":"#K Neighbors Classifier \ubaa8\ub378\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(train_x, train_y)\nY_pred = knn.predict(test_x)\nacc_knn = round(knn.score(train_x, train_y) * 100, 2)\nacc_knn","9dc83ae1":"# Gaussian Naive Bayes \ubaa8\ub378\n\ngaussian = GaussianNB()\ngaussian.fit(train_x, train_y)\nY_pred = gaussian.predict(test_x)\nacc_gaussian = round(gaussian.score(train_x, train_y) * 100, 2)\nacc_gaussian","3f2c5151":"# Perceptron \ubaa8\ub378\n\nperceptron = Perceptron()\nperceptron.fit(train_x, train_y)\nY_pred = perceptron.predict(test_x)\nacc_perceptron = round(perceptron.score(train_x, train_y) * 100, 2)\nacc_perceptron","de7ddfef":"# Linear SVC \ubaa8\ub378\nlinear_svc = LinearSVC()\nlinear_svc.fit(train_x, train_y)\nY_pred = linear_svc.predict(test_x)\nacc_linear_svc = round(linear_svc.score(train_x, train_y) * 100, 2)\nacc_linear_svc","b7302d02":"# Stochastic Gradient Descent \ubaa8\ub378\n\nsgd = SGDClassifier()\nsgd.fit(train_x, train_y)\nY_pred = sgd.predict(test_x)\nacc_sgd = round(sgd.score(train_x, train_y) * 100, 2)\nacc_sgd","6a057bd8":"# Decision Tree \ubaa8\ub378\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(train_x, train_y)\nY_pred = decision_tree.predict(test_x)\nacc_decision_tree = round(decision_tree.score(train_x, train_y) * 100, 2)\nacc_decision_tree","9274c87b":"# Random Forest \ubaa8\ub378\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(train_x, train_y)\nY_pred = random_forest.predict(test_x)\nacc_random_forest = round(random_forest.score(train_x, train_y) * 100, 2)\nacc_random_forest","e600fab1":"# \uba38\uc2e0\ub7ec\ub2dd \ud074\ub798\uc2a4 \uc608\uce21 \uc54c\uace0\ub9ac\uc998\uc5d0 \ub530\ub978 \uc815\ud655\ub3c4 \ud3c9\uac00\n\nmodels = pd.DataFrame({\n    'Model' : ['Support Vector Machines', 'KNN', 'Logistic Regression', 'Random Forest', 'Naive Bayes', 'Perceptron', 'Stochastic Gradient Decent', 'Linear SVC', 'Decision Tree', 'tensorflow_model'],\n    'Score' : [acc_svc, acc_knn, acc_log, acc_random_forest, acc_gaussian, acc_perceptron, acc_sgd, acc_linear_svc, acc_decision_tree, acc_nn]\n})\nmodels.sort_values(by='Score', ascending=False)","a9902031":"rf_feature_importance = random_forest.feature_importances_\nrf_feat_imp = Series(rf_feature_importance, index = feature)\n\nplt.figure(figsize=(10, 10))\nrf_feat_imp.sort_values(ascending=True).plot.barh()\nplt.xlabel('Feature importance')\nplt.ylabel('Feature')\nplt.show()","f01bdaa2":"submission = pd.read_csv('..\/input\/gender_submission.csv')\nsubmission.head()","99ed5444":"# \uc608\uce21 \uacb0\uacfc csv\ub85c \uc800\uc7a5\n\ntest = pd.read_csv('..\/input\/test.csv')\nsubmission = pd.DataFrame({\n    \"PassengerId\":test[\"PassengerId\"],\n    \"Survived\":nn_predicted_result\n})","5ac502b9":"submission.to_csv(\".\/submission.csv\", header=True, index=False)","bffd0703":"<p>Child : 0<br\/>\nYoung : 1<br\/>\nAdult : 2<br\/>\nmid-age : 3<br\/>\nsenior : 4\n<\/p>","2c3f3a9e":"<h3>7. FamilySize<\/h3>","95935eb7":"<h4>- Load data and analysis<\/h4>","4f989cc8":"<h2>Feature Engineering<\/h2>\n<p>feature\ub4e4\uc744 vector\ub85c \ub9cc\ub4dc\ub294 \uacfc\uc815 <br\/>\ndata\ub97c \uc22b\uc790\ub85c \ub9cc\ub4e4\uc5b4\uc8fc\ub294 \uac83 <br\/>\nNaN = Not a Number<\/p>","afc8f8c4":"Queenstown\uc5d0\uc11c \ud0d1\uc2b9\ud55c \uc2b9\uac1d\uc774 \uc0dd\uc874 \ud655\ub960\uc774 \ub0ae\ub2e4","1048e66e":"\ubd80\ubaa8\uc790\uc2dd \uc5c6\uc774 \ud63c\uc790 \ud0d4\uc744 \uacbd\uc6b0 \uc0dd\uc874 \ud655\ub960\uc774 \ub0ae\ub2e4.","3377958b":"<h3>Lost Information<\/h3>\n<br\/>\n\n<p>\n    dataframe.info()\ub97c \ud1b5\ud574 Age, Cabin\uc774 \ube44\uc5b4\uc788\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc74c (train\uc5d0\uc11c Embarked\ub3c4 \ube60\uc838\uc788\uc74c)<br \/>\n    dataframe.isnull().sum()\uc744 \ud1b5\ud574\uc11c null\uc778 \uac12\uc774 \uba87\ud589\uc778\uc9c0 \ud655\uc778 \uac00\ub2a5\n<\/p>","bf4010de":"Southampton\uc5d0\uc11c\uc758 \ud0d1\uc2b9\uac1d\uc774 \ubaa8\ub4e0 \ub4f1\uae09 \uc88c\uc11d\uc5d0\uc11c 50% \uc774\uc0c1\uc744 \ucc28\uc9c0\ud568.<br\/>\n\ub530\ub77c\uc11c NaN \ub370\uc774\ud130\ub97c S\ub85c \ucc44\uc6b4\ub2e4.","09d4b8c3":"\uc5ec\uc790\uac00 \ub0a8\uc790\ubcf4\ub2e4 \uc0dd\uc874\ud560 \uac00\ub2a5\uc131\uc774 \ub192\ub2e4.","69a2c978":"<h4>- Data visualization<\/h4>","441eed02":"<h3>Problem analysis<\/h3>\n\n<p> In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.<\/p><br \/>\n<p> RMS \ud0c0\uc774\ud0c0\ub2c9 \uce68\ubab0 \uc0ac\uace0 \uc2dc \uad6c\uba85\uc815\uc774 \ud0d1\uc2b9 \uc2b9\uac1d\ubcf4\ub2e4 \uc801\uc5c8\uae30 \ub54c\ubb38\uc5d0 \uc77c\ubd80 \uc0ac\ub78c\ub4e4\ub9cc \uc0dd\uc874\ud560 \uc218 \uc788\uc5c8\ub2e4.<br \/>\n\uc2b9\uac1d \uc911 \uc5b4\ub5a4 \uadf8\ub8f9\uc758 \uc0ac\ub78c\ub4e4\uc740 \ub2e4\ub978 \uadf8\ub8f9\ub4e4\ubcf4\ub2e4 \uc0dd\uc874\ud560 \ud655\ub960\uc774 \ub192\uc558\ub2e4. ex)\uc5ec\uc131, \uc544\uc774\ub4e4, \uc0c1\ub958\uce35<br \/>\n\ud0c0\uc774\ud0c0\ub2c9\ud638 \ud0d1\uc2b9 \uc2b9\uac1d \ub370\uc774\ud130\ub97c \ud559\uc2b5\ud558\uc5ec \ud0d1\uc2b9\uc790 \uc815\ubcf4\uac00 \ub4e4\uc5b4\uc654\uc744 \ub54c \uc0dd\uc874 \ud639\uc740 \uc0ac\ub9dd\ud560 \ud655\ub960\uc744 \uc608\uce21\ud55c\ub2e4.<\/p>","34405a0b":"<h2>Reference<\/h2>\n\n- <a href=\"https:\/\/www.kaggle.com\/chapagain\/titanic-solution-a-beginner-s-guide\/notebook\">Mukesh ChapagainTitanic Solution: A Beginner's Guide<\/a>\n- <a href=\"https:\/\/ahmedbesbes.com\/how-to-score-08134-in-titanic-kaggle-challenge.html\">How to score 0.8134 in Titanic Kaggle Challenge<\/a>\n- <a href=\"https:\/\/olegleyz.github.io\/titanic_factors.html\">Titanic: factors to survive<\/a>\n- <a href=\"https:\/\/www.youtube.com\/watch?v=aqp_9HV58Ls&list=PLVNY1HnUlO25B-8Gwn1mS35SD0yMHh147&index=2\">Minsuk Heo youtube lecture<\/a>\n\n<h3>Version 4\uc758 output\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2f5\uc548\uc744 \uc81c\ucd9c\ud558\uc600\uc2b5\ub2c8\ub2e4.<\/h3>","74e28bf4":"<h3>5. Fare<\/h3>","e3320233":"<h3>3. Age<\/h3>\n<p>NaN\uc778 \ub370\uc774\ud130\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uae30\uc5d0 \uc8fc\uc758\ud574\uc57c \ud55c\ub2e4.<br\/>\n(Missing Data)<br\/><br\/>\n<h4>\ubc29\ubc95 1. NaN \uc774\uc678 \uc804\uccb4 \ud0d1\uc2b9\uac1d\ub4e4\uc758 \ub098\uc774\uc758 \ud3c9\uade0 \ud639\uc740 \uc911\uac04\uac12\uc744 \ub300\uc785<\/h4><\/p>\n\n- NaN\uc778 \ub370\uc774\ud130\uc758 \uc131\ubcc4(\ud0c0\uc774\ud2c0)\uc744 \ub098\ub220 \uc131\ubcc4\ubcc4 \ud0d1\uc2b9\uac1d\ub4e4\uc758 \ub098\uc774\uc758 \uc911\uac04\uac12\uc744 \ub300\uc785","6490af5b":"Mr \ub0a8\uc131\uc740 \uc0dd\uc874 \ud655\ub960\uc774 \uc801\uace0, Miss, Mrs \uc5ec\uc131\uc740 \uc0dd\uc874 \ud655\ub960\uc774 \ub192\uc74c. (\ub354\uc774\uc0c1 Name \ud544\ub4dc\ub294 \ud544\uc694\uac00 \uc5c6\uc73c\ubbc0\ub85c \ud53c\uccd0 \uc0ad\uc81c)","e4eb0d88":"---------------------------------------------------------------------------------------------","18043d67":"1\ub4f1\uae09 \uc11d\uc758 \uc0ac\ub78c\uc740 \uc0dd\uc874 \ud655\ub960\uc774 \ub192\uace0 3\ub4f1\uae09 \uc11d\uc758 \uc0ac\ub78c\uc740 \uc0dd\uc874 \ud655\ub960\uc774 \ub0ae\ub2e4.","d41e6258":"<h3>4.Embarked<\/h3>","16546c7f":"<h3>Data Dictionary<\/h3>\n<br\/>\n\n- survived : 0 = No(Death), 1 = Yes(Alive)\n- pclass : Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd (1 > 2 > 3)\n- sibsp : siblings \/ spouses (\ud615\uc81c\uc790\ub9e4, \ubc30\uc6b0\uc790\uac00 \ud568\uaed8 \ud0d1\uc2b9\ud55c \uc218) (\ud63c\uc790\uba74 0)\n- parch : parent \/ children (\ubd80\ubaa8\ub2d8, \uc790\uc2dd\uc774 \ud568\uaed8 \ud0d1\uc2b9\ud55c \uc218) (\ud63c\uc790\uba74 0)\n- cabin : \uac1d\uc2e4 \ubc88\ud638\n- embarked : \ud0d1\uc2b9 \uc120\ucc29\uc7a5 C = Cherbourg, Q = Queenstown, S = Southampton\n<br \/><br \/>\n<p>train data\ub294 12\uac1c\uc758 \uc5f4\uc744 \uac16\uace0 test data\ub294 \uc608\uce21\ud558\ub824\ub294 Survived \uc5f4\uc740 \uc81c\uac70\ub418\uc5b4 11\uac1c\uc758 \uc5f4\uc744 \uac16\ub294\ub2e4.<\/p>","ad18d451":"<h3>2. Sex<\/h3>\n<p> vector\ub85c \ubcc0\uacbd <\/p>","b3b6baa0":"<h3>1. Name (English honorifics)<\/h3>\n<p>\uc790\uc2dd \uc5ec\ubd80, \uacc4\uae09 \ub4f1\uc744 \uc54c \uc218 \uc788\ub294 \uc9c0\ud45c\uc77c \uac83\uc774\ub77c \uac00\uc815<\/p>","7496d496":"\ud615\uc81c\uc790\ub9e4 \uc5c6\uc774 \ud63c\uc790 \ud0d4\uc744 \uacbd\uc6b0 \uc0dd\uc874 \ud655\ub960\uc774 \ub0ae\ub2e4.","c6e8b10a":"<h3>6. Cabin<\/h3>","23d17e55":"# Titanic: Machine Learning from Disaster\n<h3> Predict survival on the Titanic <\/h3>"}}