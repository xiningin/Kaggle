{"cell_type":{"b7d1fe7e":"code","bc17d0a5":"code","9d31705f":"code","6bf75b3b":"code","8bb40a36":"code","f3a65d9c":"code","00df8fe0":"code","04d2a90a":"code","e301d698":"code","754efd69":"code","bca8eeaa":"code","58d578c1":"code","20560caa":"code","b8ac6bdd":"code","3b8de771":"code","828873a8":"code","9c77133b":"code","b7031f5f":"code","e65ef04d":"code","4dbe3eb1":"code","9263199a":"code","ffe060d7":"code","27772f20":"code","babd0eed":"code","a4fec708":"code","4a64c3ff":"markdown","aefa6d0e":"markdown","b2037be8":"markdown","747c7b23":"markdown","f3377bbf":"markdown","52c32c69":"markdown","03d38d67":"markdown","bf8862c0":"markdown"},"source":{"b7d1fe7e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report, confusion_matrix \nfrom sklearn.model_selection import train_test_split","bc17d0a5":"df_sky = pd.read_csv(\"..\/input\/Skyserver_SQL2_27_2018 6_51_39 PM.csv\", dtype={'class':'category'})\ndf_sky.head()","9d31705f":"df_sky.drop(['objid','specobjid'],axis =1,inplace=True)\ndf_sky.head()","6bf75b3b":"counts = df_sky.groupby(['class'])['class'].count().plot(kind='bar')","8bb40a36":"import seaborn as sns\ncorr = df_sky.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns)","f3a65d9c":"plt.figure(figsize=(20,20))\nx = 0\nfor column in df_sky.columns:\n    if column not in ['rerun','class']:\n        x = x + 1\n        plt.subplot(4,4,x)\n        df_sky.groupby('class')[column].plot.kde()\n        plt.title(column)\n        plt.legend()","00df8fe0":"col = [i for i in df_sky.columns if i not in ['class']]\nfrom sklearn.preprocessing import LabelEncoder\nX_train = df_sky[col].apply(LabelEncoder().fit_transform)\nX_train.head()","04d2a90a":"col = ['class']\nfrom sklearn.preprocessing import LabelEncoder\nY_train = df_sky[col].apply(LabelEncoder().fit_transform)\nY_train.head()","e301d698":"Y_train.shape","754efd69":"X_train=df_sky.drop('class',axis=1).values","bca8eeaa":"print(\"X_train shape:\", X_train.shape)\nprint(\"Y_train shape:\", Y_train.shape)","58d578c1":"import keras\nY_train = keras.utils.to_categorical(Y_train, 3)\nprint(\"Y_train shape:\", Y_train.shape)","20560caa":"#X_train=train.drop('class',axis=1).values\n\n#Y_train=train['class'].values\n#Y_train=Y_train.reshape((10000,1))\n\n#print(\"X_train shape:\", X_train.shape)\n#print(\"Y_train shape:\", Y_train.shape)","b8ac6bdd":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X_train,Y_train, test_size=0.2, random_state=42)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","3b8de771":"import tensorflow as tf\ndef layer(output_dim,input_dim,inputs,activation=None):\n    W = tf.Variable(tf.random_normal([input_dim, output_dim]))\n    b = tf.Variable(tf.random_normal([1, output_dim]))\n    XWb = tf.matmul(inputs, W)+b\n    if activation is None:\n        outputs = XWb\n    else:\n        outputs = activation(XWb)\n    return outputs","828873a8":"X = tf.placeholder(\"float\", [None, 15])\nh1 = layer(300,15,X,activation=tf.nn.relu)\ny_predict = layer(3, 300, h1, activation=None)\ny_label = tf.placeholder(\"float\", [None, 3]) ","9c77133b":"loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_predict,labels=y_label))","b7031f5f":"optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss_function)","e65ef04d":"correct_prediction = tf.equal(tf.argmax(y_label,1),tf.argmax(y_predict,1))","4dbe3eb1":"accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))","9263199a":"import math\ndef batches(batch_size, features,labels):\n    sample_size = len(features)\n    for start_i in range(0, sample_size, batch_size):\n        end_i = start_i + batch_size\n        batch1 = features[start_i:end_i]\n        batch2 = labels[start_i:end_i]\n    return batch1,batch2","ffe060d7":"trainEpochs = 20\nbatchSizes = 100\ntotalBatchs = int(10000\/batchSizes)\n\nepoch_list = []\nloss_list = []\nacc_list = []\nfrom time import time\nstartTime = time()\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())","27772f20":"for epoch in range(trainEpochs):\n    for i in range(totalBatchs):\n        batch_x, batch_y = batches(batchSizes, x_train, y_train)\n        sess.run(optimizer,feed_dict={X: batch_x, y_label: batch_y})\n    loss,acc = sess.run([loss_function,accuracy],feed_dict={X: x_test,y_label: y_test})\n    epoch_list.append(epoch)\n    loss_list.append(loss)\n    acc_list.append(acc)\n    print(\"Train Epoch:\", '%02d' % (epoch+1), \"Loss=\", \"{:.9f}\".format(loss), \"Acc=\", acc)\nduration = time() - startTime\nprint(\"Train Finished takes:\", duration)","babd0eed":"%matplotlib inline\nfig = plt.gcf()\nfig.set_size_inches(10,6)\nplt.plot(epoch_list, acc_list, label='MES')\nplt.ylabel('acc')\nplt.xlabel('epoch')\nplt.legend(['acc'], loc='upper left')","a4fec708":"%matplotlib inline\nfig = plt.gcf()\nfig.set_size_inches(10,6)\nplt.plot(epoch_list, loss_list, label='MES')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['loss'], loc='upper left')","4a64c3ff":"# 1. Import Data","aefa6d0e":"## Correlation Study","b2037be8":"## Density graph\nPlots density graphs, in which you can verify the distribution of the data.","747c7b23":"Now the data format of feature & label are suitable for MLP model.\n\n## Data split to training & testing sets","f3377bbf":"# 3. Data Preprocess\nIt separates the DataFrame into training and test pieces, in which the 'class' column is taken from the DataFrame. y is the class corresponding to the DataFrame X.\n## Label Encoding","52c32c69":"Here we initialize the database and remove two columns that will not make a difference to our classification, the column 'objid' and 'specobjid' which are object identifiers.","03d38d67":"# 4. Model Training","bf8862c0":"# 2. Data Exploration\n## Analyzing data\nNumber of samples per class. We notice that the quantity of Quasar is much smaller than the other classses."}}