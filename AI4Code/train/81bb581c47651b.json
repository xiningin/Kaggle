{"cell_type":{"c106e426":"code","0f3f959c":"code","3ed9cc87":"code","a4e82752":"code","8e1f1e90":"code","c8e3feef":"code","b434342d":"code","482421d3":"code","d7097ef0":"code","41af8b54":"code","3b1ae1ea":"code","35811316":"code","4b313f05":"code","3e5672ea":"code","f1c3bcd7":"code","28db32bb":"code","1805017b":"code","4f333ec2":"code","0798bdd2":"code","d9d71071":"code","65741ac0":"code","d781db83":"code","7b7379f1":"code","1b2c7d36":"code","a97ee798":"code","4b8468e7":"code","1e8d6a9a":"code","0237ed52":"code","b3b3c591":"code","ae1d1796":"code","1735d555":"code","47801e72":"code","30cb3b1a":"code","de51bd78":"code","4743bf19":"code","713564bc":"code","8d5e4c91":"code","bd0ea7bf":"code","a388e6d4":"code","14564f8d":"code","12b0ef4d":"code","8a0859ee":"code","340710e4":"code","3fe1d7de":"code","1152de34":"code","e6d8cfdc":"code","684374a3":"code","f158d964":"code","f46870c7":"code","8e397026":"code","c4880d16":"code","e70e10fa":"code","a9ea35bb":"code","752dca49":"code","e7b1f9ec":"code","0504b923":"code","3616d022":"code","4bf6ddd3":"code","64e91990":"code","35fe3cfe":"code","060da661":"code","afde5f97":"code","5197daff":"code","ff9f4d3e":"code","deef5edd":"code","1ca68512":"code","52e7037b":"code","3d70e236":"code","6fce91c8":"code","7a97b936":"code","64816890":"code","4ef12e8d":"code","7922a4eb":"code","c70a013b":"code","25ac668f":"code","e258fd0d":"code","7cb270e3":"code","427139ca":"code","997326d1":"code","bd541b1b":"code","37177eef":"markdown","053a3ca1":"markdown","1af00c27":"markdown","1cda0dcf":"markdown","5c29014e":"markdown","51836464":"markdown","7b82c9a5":"markdown","2e36ab15":"markdown","5ba439b3":"markdown","16354046":"markdown","f83aa47f":"markdown","d9f9ca62":"markdown","7044a0be":"markdown","7e8180d0":"markdown","2a27bfc4":"markdown","d9b238a0":"markdown","e3e490a9":"markdown","cd8b7221":"markdown","e3412c75":"markdown","a6c72bcd":"markdown","049e888e":"markdown","7efdd518":"markdown","34c6f9e6":"markdown","6cd14c21":"markdown","65abaa34":"markdown","af728957":"markdown","d7281dea":"markdown","de59f11b":"markdown","21276e4d":"markdown","f82cba3b":"markdown","b4228015":"markdown","fcdefec8":"markdown","dee0d250":"markdown","e3d276bd":"markdown","c3ca2264":"markdown","95e33b82":"markdown","62695bd6":"markdown","deb5dea4":"markdown","4a39a6be":"markdown","542c41e1":"markdown","ad545244":"markdown","6958868d":"markdown","323453ff":"markdown","2f4782c5":"markdown","86f87d86":"markdown","b6c7d999":"markdown","daf07ad0":"markdown","83854bc4":"markdown","75656cfd":"markdown","f9bf4dc2":"markdown","17265a7c":"markdown","47319e90":"markdown"},"source":{"c106e426":"pip install autocorrect","0f3f959c":"pip install geonamescache","3ed9cc87":"import pandas as pd\n    \nimport autocorrect\nfrom autocorrect import Speller\nimport string\nfrom nltk.tokenize import word_tokenize\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom nltk.stem import PorterStemmer\nimport re\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport spacy\nnlp = spacy.load('en_core_web_lg')\nimport unicodedata\nfrom itertools import chain \nfrom collections import Counter\nfrom textblob import TextBlob\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom IPython.display import FileLink\nimport geonamescache\nimport seaborn as sns","a4e82752":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8e1f1e90":"train = pd.read_csv(\"\/kaggle\/input\/train-csv\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/test-scv\/test.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/sample-sub\/sample_submission.csv\")\nids = test['id']","c8e3feef":"train.drop(['id', 'location'], axis = 1).duplicated().sum()","b434342d":"train = train.drop_duplicates(subset = ['keyword', 'text', 'target']).reset_index(drop=True)","482421d3":"def visual(num_var, target_var):\n    '''Function that takes a numerical variable and a categorical variable and returns the histogram of the distribution\n    of the numerical variable and two histograms of the distributions of the numerical variable filtered by the levels\n    of the categorical variable(in this case the 'target variable').\n    '''\n    \n    fig, ((ax0, ax1, ax2)) = plt.subplots(nrows=3, ncols=1)\n    n_bins = 20\n    colors = ['green']\n    ax0.hist(train[num_var], n_bins, density=True, histtype='bar', color=colors, \n             label=colors)\n    ax0.set_title('Total ' + ' '.join(num_var.split('_')).capitalize())\n\n    ax1.hist(train[num_var][train[target_var] == 0], n_bins, density=True, histtype='bar', stacked=True)\n    ax1.set_title(' '.join(num_var.split('_')).capitalize() +' in True comments')\n\n    ax2.hist(train[num_var][train[target_var] == 1], n_bins,  density=True, histtype='bar', stacked=True)\n    ax2.set_title(' '.join(num_var.split('_')).capitalize() +' False comments')\n\n    fig.tight_layout()\n    return plt.show()\n\ndef split_count(text):\n    '''The function that takes a string as an input and returns the number of emojis in the string as an output.\n    '''\n\n    emoji_list = []\n    data = regex.findall(r'\\X', text)\n    for word in data:\n        if any(char in emoji.UNICODE_EMOJI for char in word):\n            emoji_list.append(word)\n\n    return emoji_list\n\ndef replace_elem(text):\n    '''Function that separates two or more capitalized words written together and replaces problem specific elements\n    from the string:\n    'http...' - 'link'\n    '#', '@...', '&amp' - ''\n    \n    '''\n    text = ' '.join(['link' if i.startswith(('http')) else i for i in text.split()]).strip()\n    text = text.replace('#', ' ').replace('  ', ' ')\n    text = ' '.join([''.join([' ' + i if i.isupper() else i for i in e]).strip() if (len(e) >1 and e[0].isupper() \n                                                                                     and e[1].islower()) else e for e in text.split()])\n    text = text.replace('  ', ' ').strip()\n    text_clean = ' '.join(['' if i.startswith(('@', '&amp')) else i for i in text.split()]).strip()\n    \n    return text_clean\n\ndef strip_accents(text):\n    '''Function that eliminates accents from the string.\n    '''\n    \n    text = unicodedata.normalize('NFD', text)\\\n           .encode('ascii', 'ignore')\\\n           .decode(\"utf-8\")\n\n    return str(text)\n\ndef ent_replace(string):\n    '''Function that takes a string as an input and returns the string with entities replaced with the corresponding labels \n    from Spacy package.\n    '''\n    doc = nlp(string)\n    for ent in doc.ents:\n        string = string.replace(ent.text, ent.label_)\n    return string.lower()\n\ndef reduce_lengthening(text):\n    '''Function that eliminates more than two repeat letters from a word.\n    '''\n    pattern = re.compile(r\"(.)\\1{2,}\")\n    return pattern.sub(r\"\\1\\1\", text)\n\nspell = Speller(lang='en')\nresult = string.punctuation \ndef text_prepro(text):\n    '''Function that performs text preprocessing like spelling correction, 'cleaning' a string from stop words, digits, \n    accents, one letter word; entities replacement, stemming, etc.\n    '''\n    \n    text = reduce_lengthening(text)\n    stop_words = set(stopwords.words('english'))\n    new_stopwords = [\"i'm\", \"im\", \"there's\", \"that's\", \"wasnt\", 'as', 'thus']\n    new_stopwords_list = stop_words.union(new_stopwords)\n    text = replace_elem(text)\n    text = ent_replace(text)\n    text = ''.join([l if l not in result else ' ' for l in text])\n    text = text.lower()\n    text = spell(text)\n    word_tokens = word_tokenize(text) \n    filtered_sentence = [w for w in word_tokens if w not in new_stopwords_list and len(w) > 1]\n    #lemmatizer=WordNetLemmatizer()\n    porter = PorterStemmer()\n    stemmed_words = [porter.stem(word) for word in filtered_sentence if word.isalpha() and len(word) > 1]\n    stem_string = ' '.join(stemmed_words)\n    stemmed_string = strip_accents(stem_string)\n  \n    return stemmed_string\n\ndef spacy_counter(df, type_, text_column, target_column):\n    '''Functions that return a number of parts of speech or entetis from a string.\n    '''\n    def partofSpeachRec(text,tag):\n        doc = nlp(text)\n        pos = [token.pos_ for token in doc]\n        return pos.count(tag)\n    def entetiesRecog(text, tag):\n        doc = nlp(text)\n        labels = [ent.label_ for ent in doc.ents]\n        return labels.count(tag)\n    \n    if type_ == 'part_of_speech':\n        aux_list = ['ADJ', 'ADP', 'ADV', 'AUX', 'CONJ', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON',\n                       'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X', 'SPACE']\n        function = partofSpeachRec\n    elif type_ == 'entety_label':\n        aux_list = ['PERSON', 'ORG', 'GPE', 'NORP', 'FAC', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART','LAW', 'LANGUAGE', \n                    'DATE','TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']\n        function = entetiesRecog\n    for i in aux_list:\n        df[i] = df[text_column].apply(lambda x: function(x, i))\n        real_mean = df[df[target_column] == 0][i].mean()\n        fake_mean = df[df[target_column] == 1][i].mean()\n        mean_diff = abs(real_mean - fake_mean)\n        \n        real_median = df[df[target_column] == 0][i].median()\n        fake_median = df[df[target_column] == 1][i].median()\n        \n        print(i)\n        print(\"Mean no. of %s in real and fake comments are %.2f and %.2f respectively with the difference of %.2f\"%(\n            i, real_mean, fake_mean, mean_diff))\n        print(\"Median no. of %s in real and fake comments are %.2f and %.2f respectively\"%(i, real_median, fake_median))\n\n    return df\n\ndef part_speech(string, tag):\n    '''Function that takes a string as an input and returns the number of specified part of the spech from it.\n    '''\n    doc = nlp(string)\n    token_list = [token.pos_ for token in doc]\n    return token_list.count(tag)\n\ndef ent(string, tag):\n    '''Function that takes a string as an input and returns the number of specified entety from it.\n    '''\n    doc = nlp(string)\n    ent_list = [ent.label_ for ent in doc.ents]\n    return ent_list.count(tag)\n\ndef train_and_predict(alpha, x_train, y_train, x_test, y_test):\n    '''Function that takes as an input the value of alpha and returns the corresponding f1_score of a Multinomial model.\n    '''\n    nb_classifier = MultinomialNB(alpha=alpha)\n    nb_classifier.fit(x_train, y_train)\n    pred = nb_classifier.predict(x_test)\n    score = f1_score(y_test, pred, average='micro')\n    return score\n\ndef text_vectorizer(text_column, ngram_range, x_set, add_col_list = None, add_keyword = None, min_df = 0.001, max_features = None,\n                    stand_sc = False):\n    '''Function that vectorizes a text column of the x_set with the option of adding additional columns of numeric values.\n    It also returns the columns of the features names.\n    '''\n    vectorizer = CountVectorizer(max_features = max_features, min_df = min_df, ngram_range = ngram_range)\n    X_bow = vectorizer.fit_transform(x_set[text_column])\n    X_bow_df = pd.DataFrame(X_bow.toarray()) \n    X_bow_df.columns = vectorizer.get_feature_names()\n    if add_col_list == None:\n        final_df = X_bow_df.copy()\n\n        if add_keyword == None:\n            final_df = final_df\n        else:\n            x_set[add_keyword].index = final_df.index\n            final_df = pd.concat([final_df, x_set[add_keyword].reset_index(drop = True)], axis = 1) \n            \n    else:\n        final_df = pd.concat([X_bow_df, x_set[add_col_list].reset_index(drop = True)], axis = 1)\n        if add_keyword == None:\n            final_df = final_df\n        else:\n            final_df = pd.concat([final_df, x_set[add_keyword].reset_index(drop = True)], axis = 1) \n\n    if stand_sc:\n        columns_aux = final_df.columns\n        scaler = MinMaxScaler()\n        scaler.fit(final_df)\n        final_df = scaler.transform(final_df)\n        final_df = pd.DataFrame(final_df)\n        final_df.columns = columns_aux\n    return final_df, final_df.columns\n\ndef col_replacer(columns, df):\n    '''Function that fills a data frame with columns in case the are missing in the data frame and puts them in the\n    right order according to the argument 'columns'. \n    '''\n    cols = columns\n    for i in columns:\n        if i not in df:\n            df[i] = 0\n    df = df[cols]   \n    return df\n\ndef predictor(model, df):\n    '''Function that takes as an input a test data frame and a model and creates a Kaggle submission format data frame \n    with the predictions.'''\n    \n    sub_array = model.predict(df)\n    \n    if sub_array.ndim == 2:\n        sub_array = [int(round(i[0])) for i in sub_array]\n        sub = pd.DataFrame()\n        sub['id'] = ids\n        sub['target'] = sub_array\n    else:\n        sub = pd.DataFrame()\n        sub['id'] = ids\n        sub['target'] = pd.Series(sub_array)\n    return sub\n\ndef descr_stats(df, num_col, contr_col):\n    '''Function that takes a data frame, one of its numerical columns and a categorical column and returns the descriptive statistics of the\n    total of the numerical variable and the numerical variable filtered by the levels of the categorical variable.'''\n    \n    a = df[num_col].describe().to_frame()\n    col_list = ['total']\n    num_lev = len(df[contr_col].value_counts().index)\n    for i in range(num_lev):\n           b = df[num_col][df[contr_col] == i].describe().to_frame()\n           col_list.append(contr_col + '_' + str(i))\n           a = pd.concat([a,b], axis = 1)\n           a.columns = col_list\n    return a\n\ndef string_iterator(string):\n    string = ' '.join([i.capitalize() for i in string.split()]).replace(',', '').replace('.', '')\n    #maximum = max(range(len(string.split())))\n    aux_list = []\n    for i in range(len(string.split())):\n        new_string = ' '.join(string.split()[i:])\n        new_maximum = max(range(len(new_string.split())))\n        new_minimum = min(range(len(new_string.split())))\n        number = new_minimum + 1\n        while number <= new_maximum +1:\n            aux_list.append(' '.join(new_string.split()[new_minimum: number]))\n            new_minimum +1\n            number +=1\n\n    return aux_list\n\ngc = geonamescache.GeonamesCache()\ncontry_dict = gc.get_countries()\ncontries_dict = {}\nfor i in contry_dict.keys():\n    item_list = []\n    sub_dict = contry_dict[i]\n    name = sub_dict['name']\n    item_list.append(name.lower())\n    capital = sub_dict['capital']\n    item_list.append(capital.lower())\n    iso = sub_dict['iso']\n    item_list.append(iso.lower())\n    iso3 = sub_dict['iso3']\n    item_list.append(iso3.lower())\n    fips = sub_dict['fips']\n    item_list.append(fips.lower())\n    sub_dict = {name: item_list}\n    contries_dict.update(sub_dict)\n\nus_states_dct = gc.get_us_states()\n\ndef replaceMultiple(mainString, toBeReplaces, newString):\n  \n    for elem in toBeReplaces :\n        \n        if elem in mainString.lower():\n            \n            mainString = mainString.lower().replace(elem, newString)\n    \n    return  mainString\n\ndef country_search(text):\n    \n    text = reduce_lengthening(text)\n    text = text.translate(text.maketrans('', '', string.punctuation))\n    text = ''.join([i.lower() for i in text if not i.isdigit()]).strip()\n    text = replaceMultiple(text, ['nyc', 'los angeles', 'las vegas', 'usa', 'san francisco', 'california', 'ny', 'us', 'san diego'], 'United States')\n    text = replaceMultiple(text, ['england', 'wales', 'ireland', 'scotland', 'manchester', 'scotland', 'uk', 'liverpool'], 'United Kingdom')\n    text = replaceMultiple(text, ['waterloo on'], 'Canada')\n    text = text.replace('brasil', 'brazil')\n    aux_list = []\n    for i in contries_dict.keys():\n        if (i.lower() in text.lower()) and (i.lower() != 'jersey' and i.lower() != 'georgia' and 'indian' not in text.lower()):\n            aux_list.append(i)\n    if len(aux_list) > 1:\n        aux_list = aux_list[-1]\n    elif len(aux_list) == 0:\n        \n        for i in us_states_dct.keys():\n            us_state = us_states_dct[i]['name']\n            #us_state_code = us_states_dct[i]\n            if (us_state.lower() in text) or (i in text.upper().split()):\n                aux_list.append('United States')\n                \n        if len(aux_list) > 1:\n            aux_list = aux_list[0]\n        elif len(aux_list) == 0:\n            \n            for i in contries_dict.keys():\n                capital = contries_dict[i][1]\n                if (capital != '' and capital.lower() in text and capital.lower() != 'victoria') or (i.lower() in text and i.lower() != 'jersey'):\n                    aux_list.append(i)\n            if len(aux_list) > 1:\n                aux_list = aux_list[-1]\n            elif len(aux_list) == 0:\n                for word in string_iterator(text):\n                    if len(gc.get_cities_by_name(word.capitalize())) > 0 and word.capitalize() != 'Of':\n                        for i, o in gc.get_cities_by_name(word.capitalize())[0].items():\n                            ff =  [i for i in o.values()]\n                            if len(ff) > 0:\n                                contr_code =  ff[4]\n                                for i in contries_dict.keys():\n                                    if contr_code.lower() in contries_dict[i]:\n                                        aux_list.append(i)\n\n\n                     \n                if len(aux_list) > 1:\n                    aux_list = aux_list[-1]\n                elif len(aux_list) == 0:\n                    aux_list.append('NOT_DEFINED')\n        \n    return ''.join(aux_list)\n\n        \n","d7097ef0":"train['location'] = train['location'].fillna('missing')\ntest['location'] = test['location'].fillna('missing')","41af8b54":"plt.figure(figsize=(9,6))\nsns.countplot(y=train.location, order = train.location.value_counts().iloc[:25].index)\nplt.title('Top 15 locations')\nplt.show()","3b1ae1ea":"train['country'] = train.location.apply(country_search)\ntest['country'] = test.location.apply(country_search)","35811316":"plt.figure(figsize=(9,6))\nsns.countplot(y=train.country, order = train.country.value_counts().iloc[:30].index)\nplt.title('Top 30 locations')\nplt.show()","4b313f05":"train['country'] = np.where(((train['country'] == 'NOT_DEFINED') & (train['location'] == 'missing')), 'EMPTY',train['country'])\ntest['country'] = np.where(((test['country'] == 'NOT_DEFINED') & (test['location'] == 'missing')), 'EMPTY',test['country'])","3e5672ea":"plt.figure(figsize=(9,6))\nsns.countplot(y=train.country[train['target'] == 1], order = train.country.value_counts().iloc[:20].index)\nplt.title('Top 15 locations')\nplt.show()","f1c3bcd7":"train['country'] = np.where(train['country'] == 'EMPTY', 'empty', \n                            np.where(train['country'] == 'United States', 'usa',\n                                    np.where(train['country'] == 'NOT_DEFINED', 'not_valid',\n                                            np.where(train['country'] == 'United Kingdom', 'uk',\n                                                            np.where(train['country'] == 'India', 'india',\n                                                                    np.where(train['country'] == 'Nigeria', 'nigeria',\n                                                                            'others'))))))\n\ntest['country'] = np.where(test['country'] == 'EMPTY', 'empty', \n                            np.where(test['country'] == 'United States', 'usa',\n                                    np.where(test['country'] == 'NOT_DEFINED', 'not_valid',\n                                            np.where(test['country'] == 'United Kingdom', 'uk',\n                                                            np.where(test['country'] == 'India', 'india',\n                                                                    np.where(test['country'] == 'Nigeria', 'nigeria',\n                                                                            'others'))))))","28db32bb":"plt.figure(figsize=(6,3))\nsns.countplot(y=train.country, order = train.country.value_counts().iloc[:].index)\nplt.title('Grouped Locations')\nplt.show()","1805017b":"num_fake = train.groupby(['country'])['target'].sum()\nnum_tot = train.groupby(['country'])['country'].count()\nproportion = num_fake\/num_tot\ndf_countries = pd.DataFrame()\ndf_countries['prop_fake_new'] = proportion\ndf_countries.sort_values(by=['prop_fake_new'], ascending = False)","4f333ec2":"dummies_train = pd.get_dummies(train['country'],prefix = 'country', drop_first=True)\ndummies_test = pd.get_dummies(test['country'],prefix = 'country', drop_first=True)\ntrain = pd.concat([train, dummies_train], axis = 1)\ntest = pd.concat([test, dummies_test], axis = 1)","0798bdd2":"# Train data treatment:\n\ntrain['comment_length'] = [len(word_tokenize(item)) for item in train['text']]\n# Creates a list of lists, containing the tokens from text column.\ntokens = [word_tokenize(item) for item in train['text']]\n\ntrain['words_number'] = [len([word for word in item if word.isalpha()]) for item in tokens]\n\ntrain['punctuation_number'] = [len([word for word in item if not word.isalnum()]) for item in tokens]\n\ntrain['digits_number'] = [len([word for word in item if word.isdigit()]) for item in tokens]\n\ntrain['mean_word_length'] = train['text'].apply(lambda x: sum([len(i) for i in x if x.split()])\/len(x.split()))\n\ntrain['spaces_number'] = train['text'].apply(lambda x: len(x.split()))\n\ntrain['upper_letters_number'] = train['text'].apply(lambda x: len([i for i in x if i.isupper()]))\n\ntrain['ats_number'] = train['text'].apply(lambda x: len([i for i in x if i == '@']))\n\ntrain['hash_number'] = train['text'].apply(lambda x: len([i for i in x if i == '#']))\n\n\n# Test data treatment:\n\ntest['comment_length'] = [len(word_tokenize(item)) for item in test['text']]\n# Creates a list of lists, containing the tokens from text column.\ntokens_ = [word_tokenize(item) for item in test['text']]\n\ntest['words_number'] = [len([word for word in item if word.isalpha()]) for item in tokens_]\n\ntest['punctuation_number'] = [len([word for word in item if not word.isalnum()]) for item in tokens_]\n\ntest['digits_number'] = [len([word for word in item if word.isdigit()]) for item in tokens_]\n\ntest['mean_word_length'] = test['text'].apply(lambda x: sum([len(i) for i in x if x.split()])\/len(x.split()))\n\ntest['spaces_number'] = test['text'].apply(lambda x: len(x.split()))\n\ntest['upper_letters_number'] = test['text'].apply(lambda x: len([i for i in x if i.isupper()]))\n\ntest['ats_number'] = test['text'].apply(lambda x: len([i for i in x if i == '@']))\n\ntest['hash_number'] = test['text'].apply(lambda x: len([i for i in x if i == '#']))","d9d71071":"visual('comment_length', 'target')","65741ac0":"descr_stats(train, 'comment_length', 'target')","d781db83":"visual('words_number', 'target')","7b7379f1":"descr_stats(train, 'words_number', 'target')","1b2c7d36":"visual('punctuation_number', 'target')","a97ee798":"descr_stats(train, 'punctuation_number', 'target')","4b8468e7":"visual('digits_number', 'target')","1e8d6a9a":"descr_stats(train, 'digits_number', 'target')","0237ed52":"visual('mean_word_length', 'target')","b3b3c591":"descr_stats(train, 'mean_word_length', 'target')","ae1d1796":"visual('spaces_number', 'target')","1735d555":"descr_stats(train, 'spaces_number', 'target')","47801e72":"visual('upper_letters_number', 'target')","30cb3b1a":"descr_stats(train, 'upper_letters_number', 'target')","de51bd78":"visual('ats_number', 'target')","4743bf19":"descr_stats(train, 'ats_number', 'target')","713564bc":"train_aux =  pd.concat([train[list(test.columns)],test], axis = 0)\ntrain_aux.drop_duplicates(subset='text', inplace= True)\nkw_dict = dict(zip(train_aux['text'][train_aux['keyword'].notna()],train_aux['keyword'][train_aux['keyword'].notna()]))\n\n# Train data treatment:\ntrain['keyword'] = train['keyword'].fillna(train['text'].map(kw_dict))\ntrain['keyword'].fillna('', inplace = True)\ntrain['keyword'] = train['keyword'].apply(lambda x: x.replace('%20', ''))\n\n# Test data treatment:\ntest['keyword'] = test['keyword'].fillna(train['text'].map(kw_dict))\ntest['keyword'].fillna('', inplace = True)\ntest['keyword'] = test['keyword'].apply(lambda x: x.replace('%20', ''))\n\nfull = pd.concat([train, test], axis = 0)","8d5e4c91":"cloud_comment_words = WordCloud(background_color=\"black\").generate(\n    ' '.join(list(train['keyword'][train['target'] == 0])))\nplt.imshow(cloud_comment_words, interpolation='bilinear') \nplt.axis(\"off\")\nplt.show()","bd0ea7bf":"cloud_comment_words = WordCloud(background_color=\"black\").generate(\n    ' '.join(list(train['keyword'][train['target'] == 1])))\nplt.imshow(cloud_comment_words, interpolation='bilinear') \nplt.axis(\"off\")\nplt.show()","a388e6d4":"len(full['keyword'].unique())","14564f8d":"full['keyword'].unique()[:25]","12b0ef4d":"porter = PorterStemmer()\nfull['keyword'] = full['keyword'].apply(porter.stem)\n    ","8a0859ee":"len(full['keyword'].unique())","340710e4":"key_word_full = pd.get_dummies(full['keyword'],prefix = 'kw_', drop_first=True)\nkey_word_cols = [i for i in key_word_full.columns]\nkey_word_full['id'] = full['id']","3fe1d7de":"train = pd.merge(train, key_word_full,\n                how = 'left',\n                on = 'id')\n\ntest = pd.merge(test, key_word_full,\n                how = 'left',\n                on = 'id')","1152de34":"# Train data treatment:\ntrain['text_modif'] = train['text'].apply(text_prepro)\n\n# Test data treatment:\ntest['text_modif'] = test['text'].apply(text_prepro)\n","e6d8cfdc":"word_freq = Counter(chain.from_iterable(list(train['text_modif'].str.split())))\nmost_common = word_freq.most_common()\n\nword_freq_t1 = Counter(chain.from_iterable(list(train['text_modif'][train['target'] == 1].str.split())))\nmost_common_t1 = word_freq_t1.most_common()\n\nword_freq_t0 = Counter(chain.from_iterable(list(train['text_modif'][train['target'] == 0].str.split())))\nmost_common_t0 = word_freq_t0.most_common()","684374a3":"most_common_t1[:25]","f158d964":"most_common_t0[:25]","f46870c7":"spacy_counter(train, 'part_of_speech', 'text', 'target')","8e397026":"#Train data treatment:\ntrain['text'] = train['text'].apply(replace_elem)\ntrain['NOUN'] = train['text'].apply(lambda x: part_speech(x,'NOUN'))\ntrain['PROPN'] = train['text'].apply(lambda x: part_speech(x,'PROPN'))\ntrain['PRON'] = train['text'].apply(lambda x: part_speech(x,'PRON'))\ntrain['ADP'] = train['text'].apply(lambda x: part_speech(x,'ADP'))\n\n#Te data treatment:\ntest['text'] = test['text'].apply(replace_elem)\ntest['NOUN'] = test['text'].apply(lambda x: part_speech(x,'NOUN'))\ntest['PROPN'] = test['text'].apply(lambda x: part_speech(x,'PROPN'))\ntest['PRON'] = test['text'].apply(lambda x: part_speech(x,'PRON'))\ntest['ADP'] = test['text'].apply(lambda x: part_speech(x,'ADP'))","c4880d16":"# Train data treatment:\nstop_words = set(stopwords.words('english'))\nnew_stopwords = [\"i'm\", \"im\", \"there's\", \"that's\", \"wasnt\"]\nnew_stopwords_list = stop_words.union(new_stopwords)\n\ntrain['text'] = train['text'].apply(lambda x: ' '.join([i for i in x.lower().split() if (i.isalpha() and i not in new_stopwords_list)]))\ntrain['commenet_polar'] = train['text'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\ntrain['commenet_polar'] = np.where(((train['commenet_polar'] <= 1) & (train['commenet_polar'] >= 0.5)), 3,\n                                  np.where(((train['commenet_polar'] < 0.5) & (train['commenet_polar'] >= 0)), 2,\n                                           np.where(((train['commenet_polar'] < 0) & (train['commenet_polar'] >= -0.5)), 1, 0)))\n\ntrain['commenet_subject'] = train['text'].apply(lambda x: TextBlob(str(x)).sentiment.subjectivity)\n\n# Test data treatment:\ntest['text'] = test['text'].apply(lambda x: ' '.join([i for i in x.lower().split() if (i.isalpha() and i not in new_stopwords_list)]))\ntest['commenet_polar'] = test['text'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\ntest['commenet_polar'] =  np.where(((test['commenet_polar'] <= 1) & (test['commenet_polar'] >= 0.5)), 3,\n                                  np.where(((test['commenet_polar'] < 0.5) & (test['commenet_polar'] >= 0)), 2,\n                                           np.where(((test['commenet_polar'] < 0) & (test['commenet_polar'] >= -0.5)), 1, 0)))\n\ntest['commenet_subject'] = test['text'].apply(lambda x: TextBlob(str(x)).sentiment.subjectivity)","e70e10fa":"visual('commenet_polar', 'target')","a9ea35bb":"visual('commenet_subject', 'target')","752dca49":"X = train[['country_india', 'country_nigeria', 'country_not_valid', 'country_others', 'country_uk', 'country_usa',\n           'comment_length','words_number', 'punctuation_number', 'digits_number', 'mean_word_length', \n           'spaces_number', 'upper_letters_number', 'ats_number', 'hash_number', 'NOUN', 'PROPN', 'PRON', 'ADP',\n           'commenet_polar', 'commenet_subject']] \n\ny = train['target']\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(len(X)-1).plot(kind='barh')\nplt.show()","e7b1f9ec":"train.to_csv(\"\/kaggle\/master.csv\")","0504b923":"master = pd.read_csv(\"\/kaggle\/master.csv\")","3616d022":"ad_vars = [ 'country_india', 'country_nigeria', 'country_not_valid', 'country_others', 'country_uk', 'country_usa',\n           'comment_length','words_number', 'punctuation_number', 'digits_number', 'mean_word_length', \n           'spaces_number', 'upper_letters_number', 'ats_number', 'hash_number', 'NOUN', 'PROPN', 'PRON', 'ADP',\n           'commenet_polar', 'commenet_subject']\ntot_cols = ad_vars + key_word_cols + ['text_modif']","4bf6ddd3":"master = master[master.text_modif.notna()]","64e91990":"X = master[tot_cols]\ny = master.target","35fe3cfe":"matr_lr, cols_lr = text_vectorizer(text_column = 'text_modif', ngram_range = (1,3), x_set = X, add_col_list = ad_vars,\n                                   add_keyword = key_word_cols, \n                                   min_df = 0.001, stand_sc = False)\n\nX_train, X_test, y_train, y_test = train_test_split(matr_lr, y, test_size=0.06, \n                                                    stratify = y, random_state=42)","060da661":"log_reg = LogisticRegression(C=0.05).fit(X_train, y_train)\n\nlog_preds = log_reg.predict(X_test)\n\nprint('F1 of logistic regression reg 0.01:', f1_score(y_test, log_preds, average='micro'))","afde5f97":"matr_mn, cols_mn = text_vectorizer(text_column = 'text_modif', ngram_range = (1,1), x_set = X, add_col_list = None,\n                                   add_keyword = None, \n                                   min_df = 0.001, stand_sc = False)\n\nX_train, X_test, y_train, y_test = train_test_split(matr_mn, y, test_size=0.06, \n                                                    stratify = y, random_state=42)","5197daff":"alphas = np.arange(0, 1, 0.1)\n\nfor alpha in alphas:\n    print('Alpha: ', alpha)\n    print('Score: ', train_and_predict(alpha, X_train, y_train, X_test, y_test))\n    print()\n","ff9f4d3e":"clf = MultinomialNB(alpha = 0.2)\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\nprint('F1 of Multinomial NB: ', f1_score(y_test, predictions, average='micro'))","deef5edd":"class_labels = clf.classes_\n\nfeature_names = cols_mn\n\n# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\nfeat_with_weights = sorted(zip(clf.coef_[0], feature_names))\n\n# Print the first class label and the top 20 feat_with_weights entries\nprint('Top 20 lowest values (less predictive features)')\nprint()\nprint(class_labels[0], feat_with_weights[:20])\nprint()\nprint('*******************************************************************************************************************************')\nprint()\n# Print the second class label and the bottom 20 feat_with_weights entries\nprint('top 20 highest values (highest predictive features)')\nprint()\nprint(class_labels[1], feat_with_weights[-20:])\n","1ca68512":"matr_nr, cols_nr = text_vectorizer(text_column = 'text_modif', ngram_range = (1,2), x_set = X, add_col_list = ad_vars,\n                                   add_keyword = key_word_cols,  max_features = None, min_df = 0.001, stand_sc = True)\n\nX_train, X_test, y_train, y_test = train_test_split(matr_nr, y, test_size=0.06, stratify = y, random_state=42)\n","52e7037b":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.callbacks import Callback\nfrom sklearn.metrics import f1_score\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.layers import BatchNormalization\nfrom keras import optimizers\n\nns = X_train.shape[0] # number of samples in training data set.\nno = 1                # number of output neurons.\nni = X_train.shape[1] # number of input neurons\na = 6                 # an arbitrary scaling factor usually 2-10.\nnh = ns\/(a**(ni + no))# number of hidden layer neurons\n\n# Creates a model given an activation learning rate and number of layers.\n\ndef create_model(drop_out, learning_rate, number_of_layers=1):\n    \n    model = Sequential()\n    model.add(Dense(ni, input_dim= ni, activation='relu'))\n    model.add(Dropout(drop_out))\n    if number_of_layers == 2:\n        model.add(Dense(nh, activation='relu'))\n        model.add(Dropout(drop_out))\n    model.add(Dense(1, activation='sigmoid'))\n    sgd = optimizers.Adamax(beta_1 = 0.4, beta_2 = 0.999)\n    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n    \n    return model\n\n","3d70e236":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Create a KerasClassifier\nmodel = KerasClassifier(build_fn = create_model)\n\n# Define the parameters to try out\nparams = {'batch_size': [60, 100, 200, 300,  500, 600], \n          'epochs': [50, 100, 300], \n          'number_of_layers': [1,2],\n          'drop_out': [ 0.2, 0.5],\n          'learning_rate': [0.1, 0.01, 0.001, 0.0001]}\n\n# Create a grid search cv object passing in the parameters to try\nsg = GridSearchCV(model, param_grid = params, cv = 3, scoring = 'f1_micro')\n\nsg.fit(X_train, y_train) ","6fce91c8":"#{'batch_size': 500, 'drop_out': 0.2, 'epochs': 200, 'learning_rate': 0.0001, 'number_of_layers': 1}\nprint(sg.best_params_)","7a97b936":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.callbacks import Callback\nfrom sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n\n# Create a Sequential model\nmodel = Sequential()\n\nmodel.add(Dense(ni, input_dim=ni, activation='relu'))\nmodel.add(Dropout(0.1))\n#model.add(Dense(no, activation='relu'))\n#model.add(Dropout(0.2))\n\nmodel.add(Dense(1, activation='sigmoid'))\nsgd = optimizers.Adamax(learning_rate=0.0001, beta_1 = 0.9, beta_2 = 0.999)\nmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\nmodel.summary()","64816890":"monitor_val_acc = EarlyStopping(monitor='val_accuracy', patience = 10)\n#Save the best model as best_banknote_model.hdf5\nmodelCheckpoint = ModelCheckpoint('Nnr_model.hdf5', save_best_only=True)\nhistory = model.fit(X_train, y_train, epochs=100, \n                    callbacks=[modelCheckpoint,monitor_val_acc],\n                    batch_size=200,\n                    validation_data = (X_test, y_test))\n","4ef12e8d":"plt.figure()\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\n\nplt.title('Model accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Test'])\nplt.show()","7922a4eb":"plt.figure()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'])\nplt.show()","c70a013b":"predictions = model.predict(X_test)\n\npredictions = [round(i[0]) for i in predictions]\nprint('F1 of Keras Net Work: ', f1_score(y_test, predictions, average='micro'))","25ac668f":"test_lr, cols = text_vectorizer(text_column = 'text_modif', ngram_range = (1,3), x_set = test, add_col_list = ad_vars,\n                                   add_keyword = key_word_cols, \n                                   min_df = 0.001, stand_sc = False)\ntest_lr = col_replacer(columns = cols_lr, df = test_lr)","e258fd0d":"lr = predictor(model = log_reg, df = test_lr)\nlr.to_csv('Log_reg.csv', index=False)\nFileLink(r'Log_reg.csv')","7cb270e3":"test_mn, cols1 = text_vectorizer(text_column = 'text_modif', ngram_range = (1,1), x_set = test, add_col_list = None,\n                                   add_keyword = key_word_cols,\n                                   max_features = None, min_df = 0.001, stand_sc = False)\n\ntest_mn = col_replacer(columns = cols_mn, df = test_mn)","427139ca":"mn = predictor(model = clf, df = test_mn)\nmn.to_csv('mult_nom.csv', index=False)\nFileLink(r'mult_nom.csv')","997326d1":"test_nr, cols2 = text_vectorizer(text_column = 'text_modif', ngram_range = (1,2), x_set = test, add_col_list = ad_vars,\n                                   add_keyword = key_word_cols,  max_features = None, min_df = 0.001, stand_sc = True)\n\ntest_nr = col_replacer(columns = cols_nr, df = test_nr)","bd541b1b":"from keras.models import load_model\nnnr_model = load_model('Nnr_model.hdf5')\nnr = predictor(model = nnr_model, df = test_nr)\nnr.to_csv('neural_net.csv', index=False)\nFileLink(r'neural_net.csv')","37177eef":"*To check the frequency of the words we are going to use the Collection from Counter.*","053a3ca1":"*Let's train a logistic regression model and calculate its F1 score.*","1af00c27":"*I am going to group the location variable by countries using the function country search.*","1cda0dcf":"False comments seem to have more upper case letters on average (10.8 while true comments mean is 9.2), but the true comments are more spread\nwith the standard deviation of 10.94 which is higher than in the case of false commnets (9.6), which could be explained by the extreme outliers.","5c29014e":"*Now if we take a look at the unique values of the keyword column of the full data set we can spot that some of the topics have the same root, like 'bloody' and 'blood', and therefore can be stemmed to reduce the number of the unique values of the keyword.*","51836464":"*Let's explore the model and print out the most and the least important features.*","7b82c9a5":"**Ats_number.**","2e36ab15":"*Let's create some structural features to describe the comment length, the number of digits, words, upper case letters, punctuations,\nspaces, ads, hashes, mean word length in a comment and visualize them.*","5ba439b3":"*While we can observe quite similar destributions when comes to the mean (4,56 and 4.77), first, second and third percentiles (2,4,6 and 3,4,6) in the cases of true and false comments respectively, true comments tend to have more extreme outliers.*","16354046":"**Punctuation_number.**","f83aa47f":"*A unigram vectorizer on the text data and key words variables.*","d9f9ca62":"*A data frame based on (1,2) gramm vectorized text using 1000 most frequent words, with key words and created varibales.\nThe data has been scaled as well.*","7044a0be":"**Upper_letters_number.**","7e8180d0":"*On average the false comments contain more words than the true once with the mean\nof 14 and standard deviation of 4.9 while the true comments have the mean of 13.7 and standard deviation 6.*","2a27bfc4":"**Mean_word_length.**","d9b238a0":"**Spaces_number.**","e3e490a9":"# Modelling part.\n","cd8b7221":"*Appart from a great number of missing values, there are several values that represent the same thing such as 'New York, NY' and 'New York', \n'United Kingdom' and 'UK', etc.*","e3412c75":"*Applying Logistic Regression Model to the test data.*","a6c72bcd":"*The majority of the comments don't have digits in them, but in the case of the false comments the mean number is higher than in the case of true comments.*","049e888e":"# Logistic Regression.","7efdd518":"# Feature selection part.\n","34c6f9e6":"*Exploring the model.*","6cd14c21":"*Applying Multinomial Model to the test data.*","65abaa34":"*Content Features: Content features are based on comments\u2019 textual aspects and include polarity (the average positive or negative feelings expressed a comment), subjectivity (a score of whether a tweet is objective or subjective).*\n*But before we should get rid of the hash tags, ats, etc.*","af728957":"**Comment_length.**","d7281dea":"*Let's take a look at a key word variable. Some of them are missing.\nSo we are going to create a dictionary with texts as keys and keywords as values and replace the missing values with the keywords of the same texts since we know that some texts are repeated.\nThen I will visualize the rusults.*","de59f11b":"#  Multinomial model.","21276e4d":"*As a conclusion of this project I must say there hasn't been much difference in the performance of the three models. On average they have 0.80\nF1 scroe on the test and 0.79 on the validation data. I started with the avrege score of 0.76 and managed to improve it by using text prepro -\ncessing tecniques such as spelling corection, entity labeling and chosing stemming over lematization.*","f82cba3b":"*On average false comments tend to have more lengthy words than the true once with the mean of 7.4, first, second and third percentiles of 6.4, 7.3 and 8.2 respectively and the maximum of 20.16.*","b4228015":"*Even though there are some really long comments among the true once, on average the false once are longer with the mean\nof 19.22 and standard deviation of 6 while the true comments have the mean of 18.6 and standard deviation 7.4.*","fcdefec8":"*Grid Search for the best parameters.*","dee0d250":"Next, we will compute the mean number of different parts of speech and name enteties used in fake and real comments and compare the values. If there is a remarkable difference, then there is a good chance that using the created feature in fake news detector will improve its performance.","e3d276bd":"**Key_word.**","c3ca2264":"#  Functions.\n*Here some problem specific functions were created.*","95e33b82":"*Bag of words.\nUsing (1,3) n-gramm baw vectores and new features.*","62695bd6":"*Now as the data is cleaner we can see that apparently Nigeria and India seem to have high level of fake tweets coming from them.*","deb5dea4":"# Location.","4a39a6be":"True comments have more ats than the false once which can be explained by the fact that there are more discussions about true news than the false once.","542c41e1":"**Digits_number.**","ad545244":"*Feature importance is an inbuilt class that comes with Tree Based Classifiers, we will be using Extra Tree Classifier for extracting the top n features for the dataset.*","6958868d":"*There are 72 repeated cases in the train dataset we should get rid of.*","323453ff":"**Words_number.**","2f4782c5":"# Neural NetWork model.","86f87d86":"*Now let's dummify the keyword variable and create key_word_full data frame with the dummies of the keyword and the id of each comment, and then merge it with train and test data sets on 'id'.*","b6c7d999":"###### 1. It looks like the number of nouns (NOUN with the mean difference equal to 0.69), proper names (PROPN with the mean difference equal to 0.74), pronouns (PRON with the mean difference equal to 0.56) and adposition (ADP with the mean difference equal to 0.43) might do do some good to our model. Let's take a look at the number of parts of speech in the comments.","daf07ad0":"*Applying Neural Net Model to the test data.*","83854bc4":"The distributions are quite similar, though the true comments seem to be more spread.","75656cfd":"# Feature engeneering part.\n","f9bf4dc2":"*Let's do some text cleaning.*","17265a7c":"*Here we are going to iterate over a range of values of alpha to optimize the model.*","47319e90":"*Looking at the word clouds of true and false comments it\u00b4s clear that latter fall into more contentious topics like wreckage, derailment, etc., and since it's human labeled it could be useful to add the dummies of the keywords to as predictors as well.*"}}