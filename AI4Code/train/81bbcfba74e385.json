{"cell_type":{"2d36870b":"code","a875cdd8":"code","d920c9f4":"code","844b2b58":"code","7f6d43cf":"code","9cf39620":"code","31f7719e":"code","6bfa94c9":"code","4d8da31a":"code","d2ffa76b":"code","ca9fb775":"code","02d8e4d5":"code","a7c99ced":"code","cdff1ec2":"code","305a6bfb":"code","c8936ee9":"markdown","954d638c":"markdown","5c0508fb":"markdown","0a30585b":"markdown","97032263":"markdown","4980b1c7":"markdown","21755d96":"markdown","f7817ef3":"markdown"},"source":{"2d36870b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a875cdd8":"!pip install node2vec\n!pip install datasketch\n","d920c9f4":"import networkx as nx\nimport json\nfrom collections import Counter\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.offline import plot\nfrom node2vec import Node2Vec\nfrom datasketch import MinHash\nimport networkx as nx\nimport numpy as np\nfrom sklearn.cluster import OPTICS","844b2b58":"class TMinHash:\n    def __init__(self, K, seed=1073):\n        self.M = MinHash(num_perm=K, seed=seed)\n\n    def fit(self, data):\n        for sh in data:\n            for p in sh.split(\"\/\"):\n                self.M.update(p.encode('utf8'))\n\n        return self.M\n    \ndef load_graphs(filename):\n    fid = open(filename, \"r\")\n    call_graphs = fid.readlines()\n    call_graphs_json = json.loads(call_graphs[0])\n    behavior_graphs = list()\n\n    for g in call_graphs_json:\n        one_graph = nx.Graph()\n        for node in g['call_graph']:\n            # source node\n            one_graph.add_node(node['toId'])  \n            # destination node\n            one_graph.add_node(node['fromId'])   \n            # edge\n            one_graph.add_edge(node['fromId'], node['toId'])\n        behavior_graphs.append(one_graph)\n    return behavior_graphs\n\ndef graph_to_minhash_key(graph: nx.Graph, K=2):\n    edge_list = list(map(lambda x: x[0]+\"_\"+x[1], list(graph.edges)))\n    tm = TMinHash(seed=1073, K=K)\n    res = tm.fit(edge_list)\n    key = \"|\".join(list(map(lambda x: str(x), sorted(res.hashvalues))))\n    return key\n\ndef merge_graphs(master_graph: nx.Graph, graph_list: list):\n    for g in graph_list:\n        master_graph.add_nodes_from(g.nodes)\n        master_graph.add_edges_from(g.edges)\n    return master_graph\n\ndef semantically_related(api_id, api_id_name_dict, model, num_related=2):\n    api_name = api_id_name_dict[api_id]\n    related =  model.most_similar(api_id, topn=num_related)\n    related_apis = list(map(lambda x: (x[0], api_id_name_dict[x[0]], x[1]), related))\n    return (api_name, related_apis)\n\ndef graph2vec_edge_arithmetic(graph: nx.Graph, n2vmodel):\n    # Graphs can have multiple connected components. Sometimes there can be trivial connected components, \n    # meaning they are just nodes eithout any edges.\n    # In cases where we have trivial connected components we perform only node addition.\n    # For non-trivial connected components each edge is taken. embeddings for the nodes in the edge are multiplied to get an edge vector.\n    # These edge vectors are summed to get the vector for the connected component\n    \n    sum_vec = None\n    count = 0\n    \n    # for non trivial connected components\n    for e in list(graph.edges):\n        edgevec = np.multiply(n2vmodel.wv.get_vector(e[0]),n2vmodel.wv.get_vector(e[1]))\n        norm = np.linalg.norm(edgevec)\n        edgevec = edgevec if norm == 0 else edgevec\/norm\n        sum_vec = edgevec if sum_vec is None else np.add(sum_vec, edgevec)\n        count += 1\n    # getting all nodes that have a zero degree. These nodes a part of the trivial connected components\n    node_degrees = list(map(lambda x: (x, graph.degree(x)), list(graph.nodes)))\n    node_zero_degrees = list(filter(lambda x: x[1] == 0 , node_degrees))\n    for (n, d) in node_zero_degrees:\n        nodevec = n2vmodel.wv.get_vector(n)\n        norm = np.linalg.norm(nodevec)\n        nodevec = nodevec if norm == 0 else nodevec\/norm\n        sum_vec = nodevec if sum_vec is None else np.add(sum_vec, nodevec)\n        count += 1\n    try:\n        graph_vector = sum_vec\/count\n        return graph_vector \n    except:\n        print(sum_vec, count, len(graph.nodes), len(graph.edges))\n        return None\n\ndef graph2vec_node_arithmetic(graph: nx.Graph, n2vmodel):\n    # Graphs can have multiple connected components. Sometimes there can be trivial connected components, \n    # meaning they are just nodes eithout any edges.\n    # In cases where we have trivial connected components we perform only node addition.\n    # For non-trivial connected components each edge is taken. embeddings for the nodes in the edge are multiplied to get an edge vector.\n    # These edge vectors are summed to get the vector for the connected component\n    \n    sum_vec = None\n    count = 0\n    \n    for n in list(graph.nodes):\n        nodevec = n2vmodel.wv.get_vector(n)\n        norm = np.linalg.norm(nodevec)\n        nodevec = nodevec if norm == 0 else nodevec\/norm\n        sum_vec = nodevec if sum_vec is None else np.add(sum_vec, nodevec)\n        count += 1\n    try:\n        graph_vector = sum_vec\/count\n        return graph_vector \n    except:\n        print(sum_vec, count, len(graph.nodes), len(graph.edges))\n        return None","7f6d43cf":"supervised_graphs = load_graphs('\/kaggle\/input\/api-access-behaviour-anomaly-dataset\/supervised_call_graphs.json')\nevaluation_graphs = load_graphs('\/kaggle\/input\/api-access-behaviour-anomaly-dataset\/remaining_call_graphs.json')","9cf39620":"print('number of graphs in supervised datset: ', len(supervised_graphs))\nprint('number of graphs in evaluation datset: ', len(evaluation_graphs))","31f7719e":"graph_keys = list(map(lambda g: graph_to_minhash_key(g, K=2), supervised_graphs))\ngraph_hist = Counter(graph_keys)\nprint(\"supervised graphs\")\nprint(\"Number of graphs: \",sum(list(map(lambda x: x[1], list(graph_hist.items())))), \"Number of distinct graphs :\", len(graph_hist))\n\ngraph_keys = list(map(lambda g: graph_to_minhash_key(g, K=2), evaluation_graphs))\ngraph_hist = Counter(graph_keys)\nprint(\"evaluation graphs\")\nprint(\"Number of graphs: \",sum(list(map(lambda x: x[1], list(graph_hist.items())))), \"Number of distinct graphs :\", len(graph_hist))","6bfa94c9":"# merge all graphs into a single graph. Note that we have 2 different soures of information namely E and F. \n# Although the API eco-system is different for both we can actually build a single graph (forest of graphs) and\n# compute the random walks. These walks will be different for nodes of E and F.\n\nthe_graph = nx.Graph()\nthe_graph = merge_graphs(the_graph, supervised_graphs)\nthe_graph = merge_graphs(the_graph, evaluation_graphs)","4d8da31a":"node2vec = Node2Vec(the_graph, dimensions=20, walk_length=8, num_walks=100, workers=2)","d2ffa76b":"model = node2vec.fit(window=4, min_count=0)","ca9fb775":"# add index to graphs so that we can get the graph from cluster-id\nsupervised_graphs_idx = list(zip(range(0, len(supervised_graphs)), supervised_graphs))\n\nvector_list = list()\nfor (idx, g) in supervised_graphs_idx:\n    graph_vector = graph2vec_edge_arithmetic(g, model)\n    vector_list.append(graph_vector)\n# filter empty graphs\nvector_list_edge_arithmetic = list(filter(lambda x: x is not None, vector_list))","02d8e4d5":"# add index to graphs so that we can get the graph from cluster-id\nsupervised_graphs_idx = list(zip(range(0, len(supervised_graphs)), supervised_graphs))\n\nvector_list = list()\nfor (idx, g) in supervised_graphs_idx:\n    graph_vector = graph2vec_node_arithmetic(g, model)\n    vector_list.append(graph_vector)\n# filter empty graphs\nvector_list_node_arithmetic = list(filter(lambda x: x is not None, vector_list))","a7c99ced":"# clustering using OPTICS\nopt_edge = OPTICS()\nopt_edge.fit(np.array(vector_list_edge_arithmetic))\nopt_node = OPTICS()\nopt_node.fit(np.array(vector_list_node_arithmetic))","cdff1ec2":"labels = list(opt_edge.labels_)\ncluster_density = Counter(labels)\noutlier_membership = cluster_density[-1]\nnumber_of_clusters = len(cluster_density)\nprint(\"outlier membership: \", outlier_membership, \"number of clusters: \", number_of_clusters)","305a6bfb":"labels = list(opt_node.labels_)\ncluster_density = Counter(labels)\noutlier_membership = cluster_density[-1]\nnumber_of_clusters = len(cluster_density)\nprint(\"outlier membership: \", outlier_membership, \"number of clusters: \", number_of_clusters)","c8936ee9":"One key observation is that the number of clusters generated with embeddings is much smaller than the number of clusters generated when we used MinHash based graph keys. Secondly the number of datapoints falling into outlier clusters is much higher when we used embeddings.","954d638c":"### Approach-1\n1. Edges from each graph are collected\n2. Embeddings of nodes of each edge are multiplied to get edge vectors\n3. Edge vectors of all the edges are summed and normalized\n4. Thus each graph now becomes a vector\n5. These vectors are clustered using OPTICS","5c0508fb":"## <span style=color:blue> Loading the dataset <\/span>","0a30585b":"# <span style=color:blue> API Call-Graph Analytics <\/span>\nIn this notebook we perform graph analytics using the API call graphs dataset. We have two datasets called supervised_call_graphs.json and remaining_call_graphs.json. Each of these datasets are json files containing an **id** field and **call_graph** field. The **call_graph** field conntains edges of a graph each marked with sa source node and a destination node. Each of these graphs is a real API call graph interaction captured from production environment of two diverse distributed microservices-driven applications. Do read more about the datasets in the dataset description.","97032263":"## Graph clustering using node embeddings\nAnother approach to graph clustering is to convert graphs into embedding. A simple approach is to first compute node embeddings and then perform arithmetic on the embeddings of all the nodes in a graph to generate a graph embedding. The graph embedding can then be used to perform graph clustering","4980b1c7":"## <span style=color:blue> Helper Functions <\/span>","21755d96":"## <span style=color:blue> Graph processing <\/span>\nGraphs can be processed to generate various insights and answer questions such as\n- How many APIs are actively accessed by users? --> This is nothing but the node count\n- Is there semantic relationship between nodes? --> This is solved by computing node embeddings\n- Are all users' API accesses similar. If not how many different access patterns are there? --> This is solved using graph clustering\n\n### Graph Clustering\n\nIf graphs could be converted to keys, a simple groupBy operation would result in graph clustering. One trick is to convert graphs into minhashes and use the hash values as keys. The minhash permutations determine how graph similarity will be computed. Here is one such approach to determine the number of distinct graphs.","f7817ef3":"### Approach-2\n1. Embeddings of nodes are summed and normalized\n2. Thus each graph now becomes a vector\n3. These vectors are clustered using OPTICS"}}