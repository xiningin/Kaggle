{"cell_type":{"feaf5bf1":"code","dac5aabe":"code","fefe1306":"code","6d268512":"code","ffe723cb":"code","df17881f":"code","6c8cc5cf":"code","b9687758":"code","da72d350":"code","f7d27663":"code","292a0f77":"code","ed1f9cc2":"code","c4fa1ccf":"code","2bd35b33":"code","c499f5f8":"code","1e49c225":"markdown","853a07a5":"markdown","41645133":"markdown","55fe5440":"markdown","3d2cee13":"markdown","95b8ff36":"markdown","40b4e5a8":"markdown","d943cf8c":"markdown","d12a5484":"markdown","0302ffa6":"markdown","4ddcc125":"markdown","7483f887":"markdown"},"source":{"feaf5bf1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n","dac5aabe":"df = pd.read_csv('..\/input\/50-startups\/50_Startups.csv')\ndf.head()","fefe1306":"df.info()","6d268512":"df.shape","ffe723cb":"X = df.iloc[:,:-1] #independent\ny = df.iloc[:,4] #dependent","df17881f":"states = pd.get_dummies(X['State'],drop_first=True)","6c8cc5cf":"states.head()","b9687758":"X=X.drop('State',axis=1)","da72d350":"X","f7d27663":"X=pd.concat([X,states],axis=1)","292a0f77":"X","ed1f9cc2":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)","c4fa1ccf":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train,y_train)","2bd35b33":"y_pred = regressor.predict(X_test)","c499f5f8":"from sklearn.metrics import r2_score\nscore = r2_score(y_test,y_pred)","1e49c225":"![image.png](attachment:6ee8a4b8-45ec-44e9-9e18-de64c57c9149.png)","853a07a5":"# Splitting the dataset into the Training set and Test set\n**Importing the Libraries and Applying Cross Validation with 80% data as Training Data and 20% as Test Data.**","41645133":"![image.png](attachment:f23e1abf-c5c9-48db-8a2e-3ab9d9b38bfb.png)","55fe5440":"# R2 SCORE FUNCTION\n**Now we will use the R2 Score function to compare our models accuracy. R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. It may also be known as the coefficient of determination.**","3d2cee13":"**Importing the dataset**","95b8ff36":"# Fitting Multiple Linear Regression to the Training set\n\n**The Linear Regression equation would look like \u2014 \u2014 > y=b(0)+b(1)x(1)+b(2)x(2)+b(3)x(3)+b(4)D(1)+b(5)D(2)+b(6)D(3)\u2026b(n+3)D(m-1)\nImporting the Linear Regression Class**","40b4e5a8":"**Predicting the Test set results**","d943cf8c":"**For storing the independent & dependent features in X & y we use\u2026**","d12a5484":"# Implementing Multiple Linear Regression on 50_Startups Dataset","0302ffa6":"# HERE I HAVE EVEN ATTACHED AN IMAGE THAT GIVES THE INSIGHT OF ACTUALLY WHICH FORMULA WORKS BEHIND THIS R2 SCORE FUNCTION. TAKE A NOTE THAT IF THE VALUE OF R2 TENDS TO 1 OUR MODEL WORKS WELL.","4ddcc125":"# Avoiding the Dummy Variable Trap\n**The Linear Regression equation would look like \u2014> y=b(0)+b(1)x(1)+b(2)x(2)+b(3)x(3)+b(4)D(1)+b(5)D(2)+b(6)D(3)\u2026b(n+3)D(m-1)\nHere D(1)\u2026D(m-1) are the m dummy variable\u2019s which we had defined earlier in LabelEncoder and OneHotEncoder\nWell if you are sharp enough you might have noticed that the even though there are m dummy variables we have excluded the last dummy variable D(m)\nThe reason to that is a concept called Dummy Variable Trap in Machine Learning\u2026and to avoid that we must always exclude the last Dummy Variable\nIf you are more interested then feel free to research a bit on Dummy Variable Trap!!**","7483f887":"**Importing the libraries\u2026**"}}