{"cell_type":{"f9771d96":"code","51f37fad":"code","d01f91e7":"code","dc66ab39":"code","bd993141":"code","2c851751":"code","b576b9ca":"code","15edd380":"code","328b855f":"code","bd231267":"code","809d7b72":"code","c3fbf304":"code","691fbbdb":"code","82631801":"code","348c1154":"code","649fb2b3":"code","c53a0a12":"code","7c2c196c":"code","3f7905e8":"code","d867bead":"code","5fb7b81d":"markdown","79b2db57":"markdown","0c4847b9":"markdown","83d6eb8b":"markdown","0b4551c4":"markdown","134a12ad":"markdown","0130c812":"markdown","3d9c282a":"markdown","2a07b354":"markdown","4eb2b0d8":"markdown","43b49e7f":"markdown","d8dfa263":"markdown","1e9df304":"markdown","a9f9113d":"markdown"},"source":{"f9771d96":"#loading dataset\nimport pandas as pd\nimport numpy as np\n#visualisation\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n#EDA\nfrom collections import Counter\nimport pandas_profiling as pp\n# data preprocessing\nfrom sklearn.preprocessing import StandardScaler\n# data splitting\nfrom sklearn.model_selection import train_test_split\n# data modeling\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n#ensembling\nfrom mlxtend.classifier import StackingCVClassifier","51f37fad":"data = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')\ndata.head()","d01f91e7":"data.info()","dc66ab39":"data.isnull().sum()","bd993141":"data.describe()","2c851751":"pp.ProfileReport(data)","b576b9ca":"y = data[\"target\"]\nX = data.drop('target',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","15edd380":"print(y_test.unique())\nCounter(y_train)","328b855f":"m1 = 'Logistic Regression'\nlr = LogisticRegression()\nmodel = lr.fit(X_train, y_train)\nlr_predict = lr.predict(X_test)\nlr_conf_matrix = confusion_matrix(y_test, lr_predict)\nlr_acc_score = accuracy_score(y_test, lr_predict)\nprint(\"confussion matrix\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Logistic Regression:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test,lr_predict))","bd231267":"\nm2 = 'Naive Bayes'\nnb = GaussianNB()\nnb.fit(X_train,y_train)\nnbpred = nb.predict(X_test)\nnb_conf_matrix = confusion_matrix(y_test, nbpred)\nnb_acc_score = accuracy_score(y_test, nbpred)\nprint(\"confussion matrix\")\nprint(nb_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Naive Bayes model:\",nb_acc_score*100,'\\n')\nprint(classification_report(y_test,nbpred))","809d7b72":"m3 = 'Random Forest Classfier'\nrf = RandomForestClassifier(n_estimators=20, random_state=2,max_depth=5)\nrf.fit(X_train,y_train)\nrf_predicted = rf.predict(X_test)\nrf_conf_matrix = confusion_matrix(y_test, rf_predicted)\nrf_acc_score = accuracy_score(y_test, rf_predicted)\nprint(\"confussion matrix\")\nprint(rf_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Random Forest:\",rf_acc_score*100,'\\n')\nprint(classification_report(y_test,rf_predicted))","c3fbf304":"m4 = 'Extreme Gradient Boost'\nxgb = XGBClassifier(learning_rate=0.01, n_estimators=25, max_depth=15,gamma=0.6, subsample=0.52,colsample_bytree=0.6,seed=27, \n                    reg_lambda=2, booster='dart', colsample_bylevel=0.6, colsample_bynode=0.5)\nxgb.fit(X_train, y_train)\nxgb_predicted = xgb.predict(X_test)\nxgb_conf_matrix = confusion_matrix(y_test, xgb_predicted)\nxgb_acc_score = accuracy_score(y_test, xgb_predicted)\nprint(\"confussion matrix\")\nprint(xgb_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Extreme Gradient Boost:\",xgb_acc_score*100,'\\n')\nprint(classification_report(y_test,xgb_predicted))","691fbbdb":"m5 = 'K-NeighborsClassifier'\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_train, y_train)\nknn_predicted = knn.predict(X_test)\nknn_conf_matrix = confusion_matrix(y_test, knn_predicted)\nknn_acc_score = accuracy_score(y_test, knn_predicted)\nprint(\"confussion matrix\")\nprint(knn_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of K-NeighborsClassifier:\",knn_acc_score*100,'\\n')\nprint(classification_report(y_test,knn_predicted))","82631801":"m6 = 'DecisionTreeClassifier'\ndt = DecisionTreeClassifier(criterion = 'entropy',random_state=0,max_depth = 6)\ndt.fit(X_train, y_train)\ndt_predicted = dt.predict(X_test)\ndt_conf_matrix = confusion_matrix(y_test, dt_predicted)\ndt_acc_score = accuracy_score(y_test, dt_predicted)\nprint(\"confussion matrix\")\nprint(dt_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of DecisionTreeClassifier:\",dt_acc_score*100,'\\n')\nprint(classification_report(y_test,dt_predicted))","348c1154":"m7 = 'Support Vector Classifier'\nsvc =  SVC(kernel='rbf', C=2)\nsvc.fit(X_train, y_train)\nsvc_predicted = svc.predict(X_test)\nsvc_conf_matrix = confusion_matrix(y_test, svc_predicted)\nsvc_acc_score = accuracy_score(y_test, svc_predicted)\nprint(\"confussion matrix\")\nprint(svc_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Support Vector Classifier:\",svc_acc_score*100,'\\n')\nprint(classification_report(y_test,svc_predicted))","649fb2b3":"imp_feature = pd.DataFrame({'Feature': ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n       'exang', 'oldpeak', 'slope', 'ca', 'thal'], 'Importance': xgb.feature_importances_})\nplt.figure(figsize=(10,4))\nplt.title(\"barplot Represent feature importance \")\nplt.xlabel(\"importance \")\nplt.ylabel(\"features\")\nplt.barh(imp_feature['Feature'],imp_feature['Importance'],color = 'rgbkymc')\nplt.show()","c53a0a12":"lr_false_positive_rate,lr_true_positive_rate,lr_threshold = roc_curve(y_test,lr_predict)\nnb_false_positive_rate,nb_true_positive_rate,nb_threshold = roc_curve(y_test,nbpred)\nrf_false_positive_rate,rf_true_positive_rate,rf_threshold = roc_curve(y_test,rf_predicted)                                                             \nxgb_false_positive_rate,xgb_true_positive_rate,xgb_threshold = roc_curve(y_test,xgb_predicted)\nknn_false_positive_rate,knn_true_positive_rate,knn_threshold = roc_curve(y_test,knn_predicted)\ndt_false_positive_rate,dt_true_positive_rate,dt_threshold = roc_curve(y_test,dt_predicted)\nsvc_false_positive_rate,svc_true_positive_rate,svc_threshold = roc_curve(y_test,svc_predicted)\n\n\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,5))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(lr_false_positive_rate,lr_true_positive_rate,label='Logistic Regression')\nplt.plot(nb_false_positive_rate,nb_true_positive_rate,label='Naive Bayes')\nplt.plot(rf_false_positive_rate,rf_true_positive_rate,label='Random Forest')\nplt.plot(xgb_false_positive_rate,xgb_true_positive_rate,label='Extreme Gradient Boost')\nplt.plot(knn_false_positive_rate,knn_true_positive_rate,label='K-Nearest Neighbor')\nplt.plot(dt_false_positive_rate,dt_true_positive_rate,label='Desion Tree')\nplt.plot(svc_false_positive_rate,svc_true_positive_rate,label='Support Vector Classifier')\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.legend()\nplt.show()","7c2c196c":"model_ev = pd.DataFrame({'Model': ['Logistic Regression','Naive Bayes','Random Forest','Extreme Gradient Boost',\n                    'K-Nearest Neighbour','Decision Tree','Support Vector Machine'], 'Accuracy': [lr_acc_score*100,\n                    nb_acc_score*100,rf_acc_score*100,xgb_acc_score*100,knn_acc_score*100,dt_acc_score*100,svc_acc_score*100]})\nmodel_ev","3f7905e8":"colors = ['red','green','blue','gold','silver','yellow','orange',]\nplt.figure(figsize=(12,5))\nplt.title(\"barplot Represent Accuracy of different models\")\nplt.xlabel(\"Accuracy %\")\nplt.ylabel(\"Algorithms\")\nplt.bar(model_ev['Model'],model_ev['Accuracy'],color = colors)\nplt.show()","d867bead":"scv=StackingCVClassifier(classifiers=[xgb,knn,svc],meta_classifier= svc,random_state=42)\nscv.fit(X_train,y_train)\nscv_predicted = scv.predict(X_test)\nscv_conf_matrix = confusion_matrix(y_test, scv_predicted)\nscv_acc_score = accuracy_score(y_test, scv_predicted)\nprint(\"confussion matrix\")\nprint(scv_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of StackingCVClassifier:\",scv_acc_score*100,'\\n')\nprint(classification_report(y_test,scv_predicted))","5fb7b81d":"## **ML models**\n\nHere I take different machine learning algorithm and try to find algorithm which predict accurately.\n\n1. Logistic Regression\n2. Naive Bayes\n3. Random Forest Classifier\n4. Extreme Gradient Boost\n5. K-Nearest Neighbour\n6. Decision Tree\n7. Support Vector Machine\n","79b2db57":"# **Conclusion**\n\n1) Extreme Gradient Boost gives the best Accuracy compared to other models.\n\n2) Exercise induced angina,Chest pain is major symptoms of heart attack.\n\n3) Ensembling technique increase the accuracy of the model.\n\n**Feel free to ask any question related to this topic. I'm happy to answer. If you like my work please upvote.**\n\n**HAPPY LEARNING :-)**","0c4847b9":"## Packages Required","83d6eb8b":"## **EDA**","0b4551c4":"### **Missing Value Detection**","134a12ad":"## **About Heart Disease**\n\n> Cardiovascular disease or heart disease describes a range of conditions that affect your heart. Diseases under the heart disease umbrella include blood vessel diseases, such as coronary artery disease. From WHO statistics every year 17.9 million dying from heart disease. The medical study says that human life style is the main reason behind this heart problem. Apart from this there are many key factors which warns that the person may\/maynot getting chance of heart disease.\n\n> <img style=\"float: centre;\" src=\"https:\/\/www.canwelivebetter.bayer.com\/sites\/default\/files\/2018-05\/NEW_Heartbeat_looping_GIF_NORMAL_0.gif\" width=\"600px\"\/>\n\n> From the dataset if we create suitable machine learning technique which classify the heart disease more accurately, it is very helpful to the health organisation as well as patients. ","0130c812":"## Table of Contents\n1) Import Packages\n\n2) EDA\n\n3) Preparing ML models\n\n4) Models evaluation\n\n5) Ensembling\n\n6) Conclusion","3d9c282a":"# **HEART DISEASE ANALYSIS** ","2a07b354":"## **About the Data set**\n> This dataset gives the information realated to heart disease. Dataset contain 13 columns, target is the class variable which is affected by other 12 columns. Here the aim is to classify the target variable to (disease\\non disease) using different machine learning algorithm and findout which algorithm suitable for this dataset.\n\n> **Attribute Information**\n> * Age (age in years)\n> * Sex (1 = male; 0 = female)\n> * CP (chest pain type)\n> * TRESTBPS (resting blood pressure (in mm Hg on admission to the hospital))\n> * CHOL (serum cholestoral in mg\/dl)\n> * FPS (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n> * RESTECH (resting electrocardiographic results)\n> * THALACH (maximum heart rate achieved)\n> * EXANG (exercise induced angina (1 = yes; 0 = no))\n> * OLDPEAK (ST depression induced by exercise relative to rest)\n> * SLOPE (the slope of the peak exercise ST segment)\n> * CA (number of major vessels (0-3) colored by flourosopy)\n> * THAL (3 = normal; 6 = fixed defect; 7 = reversable defect)\n> * TARGET (1 or 0)","4eb2b0d8":"## **Ensembling**\n\n> In order to increase the accuracy of the model we use ensembling. Here we use stacking technique.\n\n### About Stacking \n\n> Stacking or Stacked Generalization is an ensemble machine learning algorithm. It uses a meta-learning algorithm to learn how to best combine the predictions from two or more base machine learning algorithms. The base level often consists of different learning algorithms and therefore stacking ensembles are often heterogeneous.The stacking ensemble is illustrated in the figure below\n\n> <img style=\"float: centre;\" src=\"https:\/\/mlfromscratch.com\/content\/images\/2020\/01\/image-2.png\" width=\"400px\"\/>","43b49e7f":"**Before applying algorithm we should check whether the data is equally splitted or not, because if data is not splitted equally it will cause for data imbalacing problem**","d8dfa263":"# **Model Evaluation**","1e9df304":"## **Model prepration**","a9f9113d":"### **Descriptive statistics**"}}