{"cell_type":{"6fbe892d":"code","b06679ab":"code","dc51da1b":"code","106de5d4":"code","f6ac4199":"code","218e5752":"code","bc962f14":"code","0f684711":"markdown"},"source":{"6fbe892d":"import re\nimport os\nimport gc\ngc.enable()\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport math\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport logging\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import *\nfrom sklearn.linear_model import *\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import *\nfrom tensorflow_addons.optimizers import AdamW, RectifiedAdam\nfrom transformers import AutoTokenizer, AutoConfig, TFAutoModel, TFFunnelBaseModel\ntf.get_logger().setLevel(logging.ERROR)\nfrom glob import glob","b06679ab":"train = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsub = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\nmean_no_url = train[train.url_legal.isnull()].target.mean()\nmean_url    = train[train.url_legal.notnull()].target.mean()\n\nprint(f'Mean w\/o url_legal: {mean_no_url:.5f}')\nprint(f'Mean w\/  url_legal: {mean_url:.5f}\\n')\n\ntrain['pred'] = train.url_legal.apply(lambda x: mean_no_url if type(x) == float else mean_url)\n\ntrain.iloc[train.url_legal.isnull().index].pred = mean_no_url\ntrain.iloc[train.url_legal.notnull().index].pred = mean_url\nhas_no_url = train[train.url_legal.isnull()]\nhas_url    = train[train.url_legal.notnull()]\nprint(f'RMSE w\/ mixed means: {mean_squared_error(train.target, train.pred, squared=False):.5f}')\nprint(f'RMSE w\/ simple mean: {mean_squared_error(train.target, np.full(len(train), train.target.mean()), squared=False):.5f}\\n')\nprint(f'RMSE w\/o url_legal mean: {mean_squared_error(has_no_url.target, has_no_url.pred, squared=False):.5f}')\nprint(f'RMSE w\/  url_legal mean: {mean_squared_error(has_url.target, has_url.pred, squared=False):.5f}')\nsns.displot(train[train.url_legal.isnull()].target);\nsns.displot(train[train.url_legal.notnull()].target);","dc51da1b":"# Configurations\nBATCH_SIZE = 4\nSEED = 1\nVERBOSE = 2\nFOLDS = 5\nMAX_LEN = 256\nAUTO = tf.data.AUTOTUNE","106de5d4":"# Get the trained model we want to use\nMODEL = '..\/input\/download-distilbert-base-uncased'\nTOKENIZER = AutoTokenizer.from_pretrained(MODEL)\nmodel_paths = sorted(glob('..\/input\/clr-exp387\/*.h5'))","f6ac4199":"# Function to seed everything\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n\n# This function tokenize the text according to a transformers model tokenizer\ndef regular_encode(texts, tokenizer, maxlen = MAX_LEN):\n    enc_di = tokenizer.batch_encode_plus(\n        texts,\n        padding = 'max_length',\n        truncation = True,\n        max_length = maxlen,\n    )\n    \n    return np.array(enc_di['input_ids'])\n\n# This function encode our test sentences\ndef encode_test(x_test, MAX_LEN, tokenizer=None):\n    x_test = regular_encode(x_test.tolist(), tokenizer, maxlen = MAX_LEN)\n    return x_test\n\n# Function to build our model\ndef build_base_model(max_len, MODEL):\n    transformer = TFAutoModel.from_pretrained(MODEL)\n    input_word_ids = tf.keras.layers.Input(shape = (max_len, ), dtype = tf.int32, name = 'input_word_ids')\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    output = tf.keras.layers.Dense(1, activation = 'sigmoid', dtype = 'float32')(cls_token)\n    model = tf.keras.models.Model(inputs = [input_word_ids], outputs = [output])\n    return model\n\n# Function to train and evaluate our model\ndef evaluate(MODEL, MAX_LEN, TOKENIZER):\n    df = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\n    test_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n    x_test = test_df['excerpt']\n    x_test = encode_test(x_test, MAX_LEN, tokenizer=TOKENIZER)\n    \n    seed_everything(SEED)\n    predictions = np.zeros(len(test_df))\n    \n    for fold, model_path in enumerate(model_paths):\n        print('\\n')\n        print('-'*50)\n        print(f'Training fold {fold + 1}')\n        K.clear_session()\n        model = build_base_model(MAX_LEN, MODEL)\n        model.load_weights(model_path)\n        predictions += model.predict(x_test)[:,0] \/ FOLDS\n        del model\n\n    sub = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n    sub.target = predictions\n    sub.to_csv('has_url.csv', index=False)\n    return sub","218e5752":"%%time\ntest = evaluate(MODEL, MAX_LEN, TOKENIZER)","bc962f14":"test = pd.merge(test, pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv'), on=['id'], how='left')\ntest.target = test.target.map(lambda x: mean_url if x > 0.5 else mean_no_url)\ntest[['id', 'target']].to_csv('submission.csv', index=False)\ntest.head(7)","0f684711":"### Calculate RMSE using mixed means"}}