{"cell_type":{"6f7d38eb":"code","2c2e1ecf":"code","c3c24666":"code","2f23dfac":"code","a5fd42e3":"code","150400bc":"code","c7940822":"code","6d7104ca":"code","47499714":"code","1bb145ab":"code","f3fdee79":"code","66a9daf7":"code","8e80ea9f":"code","41bf1035":"code","b63b82cc":"code","9618c7d8":"markdown","6b402e34":"markdown","9119c9f8":"markdown","88f36bd1":"markdown","c549d970":"markdown","edc2b788":"markdown","1295a5e9":"markdown","c0a8c203":"markdown","084db9f4":"markdown","2137d733":"markdown","2b54fa48":"markdown"},"source":{"6f7d38eb":"!nvidia-smi","2c2e1ecf":"!pip install spwk-cartesius","c3c24666":"import wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n\n    user_secrets = UserSecretsClient()\n    WANDB_KEY = user_secrets.get_secret(\"WANDB_KEY\")\nexcept:\n    WANDB_KEY = None\n\nwandb.login(key=WANDB_KEY)","2f23dfac":"from cartesius.data import PolygonDataset\n\n\ntrain_data = PolygonDataset(\n    x_range=[-50, 50],          # Range for the center of the polygon (x)\n    y_range=[-50, 50],          # Range for the center of the polygon (y)\n    avg_radius_range=[1, 10],   # Average radius of the generated polygons. Here it will either generate polygons with average radius 1, or 10\n    n_range=[6, 8, 11],         # Number of points in the polygon. here it will either generate polygons with 6, 8 or 11 points\n)","a5fd42e3":"import matplotlib.pyplot as plt\nfrom cartesius.utils import print_polygon\n\n\ndef disp(*polygons):\n    plt.clf()\n    for p in polygons:\n      print_polygon(p)\n    plt.gca().set_aspect(1)\n    plt.axis(\"off\")\n    plt.show()\n\n\npolygon, labels = train_data[0]\ndisp(polygon)\nprint(labels)","150400bc":"from shapely.geometry import Polygon\nimport torch\nfrom cartesius.tokenizers import Tokenizer\n\nPAD_COORD = (0, 0)\n\nclass TransformerTokenizer(Tokenizer):\n    \"\"\"Tokenizer for Transformer model.\n    \n    This is a basic tokenizer, used with Transformer model. It just uses the coordinates\n    of the polygon and pad them appropriately.\n    \n    Args:\n        max_seq_len (int): Maximum sequence length. An exception will be raised if you\n            try to tokenize a polygon with more points than this.\n    \"\"\"\n\n    def __init__(self, max_seq_len, *args, **kwargs):  # pylint: disable=unused-argument\n        super().__init__()\n\n        self.max_seq_len = max_seq_len\n\n    def tokenize(self, polygons):\n        poly_coords = [list(p.boundary.coords) if isinstance(p, Polygon) else list(p.coords) for p in polygons]\n        pad_size = max(len(p_coords) for p_coords in poly_coords)\n\n        if pad_size > self.max_seq_len:\n            raise RuntimeError(f\"Polygons are too big to be tokenized ({pad_size} > {self.max_seq_len})\")\n\n        masks = []\n        tokens = []\n        for p_coords in poly_coords:\n            m = [1 if i < len(p_coords) else 0 for i in range(pad_size)]\n            p = p_coords + [PAD_COORD for _ in range(pad_size - len(p_coords))]\n\n            masks.append(m)\n            tokens.append(p)\n\n        return {\n            \"polygon\": torch.tensor(tokens),\n            \"mask\": torch.tensor(masks, dtype=torch.bool),\n        }","c7940822":"from torch import nn\n\n\nclass Transformer(nn.Module):\n    \"\"\"Basic Transformer implementation for Cartesius.\n    \n    Args:\n        d_model (int): Dimension for the Transformer Encoder Layer.\n        max_seq_len (int): Maximum sequence length.\n        n_heads (int): Number of attention heads for the Transformer Encoder Layer.\n        d_ff (int): Hidden size of the FF network in the Transformer Encoder Layer.\n        dropout (float): Dropout for the Transformer Encoder Layer.\n        activation (str): Activation function to use in the Transformer Encoder Layer.\n        n_layers (int): Number of layers in the Transformer Encoder.\n    \"\"\"\n\n    def __init__(self, d_model, max_seq_len, n_heads, d_ff, dropout, activation, n_layers):\n        super().__init__()\n\n        # Embeddings\n        self.coord_embeds = nn.Linear(2, d_model, bias=False)\n        self.position_embeds = nn.Embedding(max_seq_len, d_model)\n\n        # Transformer encoder\n        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model,\n                                                    nhead=n_heads,\n                                                    dim_feedforward=d_ff,\n                                                    dropout=dropout,\n                                                    activation=activation,\n                                                    batch_first=True)\n        self.encoder = nn.TransformerEncoder(encoder_layers, n_layers)\n\n    def forward(self, polygon, mask):\n        batch_size, seq_len, _ = polygon.size()\n        device = polygon.device\n\n        # Embed polygon's coordinates\n        coord_emb = self.coord_embeds(polygon)\n        pos_emb = self.position_embeds(torch.arange(seq_len, device=device).repeat((batch_size, 1)))\n        emb = coord_emb + pos_emb\n\n        # Encode polygon\n        hidden = self.encoder(emb, src_key_padding_mask=~mask)\n\n        # Extract a representation for the whole polygon\n        poly_feat = hidden[:, 0, :]\n        return poly_feat","6d7104ca":"# First, let's define our hyperparameters\nPROJECT_NAME = \"Cartesius\"\nSAVE_DIR = \"results\"\n\nSEED = 1234\n\nD_MODEL = 128\nMAX_SEQ_LEN = 256\nN_HEADS = 8\nD_FF = 256\nDROPOUT = 0\nACTIVATION = \"gelu\"\nN_LAYERS = 3\nTASK_DROPOUT = 0.1\n\nWATCH_MODEL = True\nMAX_TIME = \"00:12:00:00\"   # Maximum 12h of training\n\nGRAD_CLIP = 40\nAUTO_LR_FIND = True\nLR = 3e-4\n\nN_RANGE = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 50]\nTRANSFORMS = [\"norm_pos\", \"norm_static_scale\"]\nBATCH_SIZE = 64\nN_BATCH_PER_EPOCH = 1000","47499714":"import pytorch_lightning as pl\n\n# Set seed\npl.seed_everything(SEED, workers=True)","1bb145ab":"# Define our tokenizer + model\ntokenizer = TransformerTokenizer(max_seq_len=MAX_SEQ_LEN)\n\nencoder = Transformer(\n    d_model=D_MODEL,\n    max_seq_len=MAX_SEQ_LEN,\n    n_heads=N_HEADS,\n    d_ff=D_FF,\n    dropout=DROPOUT,\n    activation=ACTIVATION,\n    n_layers=N_LAYERS,\n)","f3fdee79":"from cartesius.data import PolygonDataModule\nfrom cartesius import PolygonEncoder\nfrom cartesius.tasks import TASKS\n\n# Create the tasks we will train our model on\ntasks = {n: t(d_model=D_MODEL, task_dropout=TASK_DROPOUT) for n, t in TASKS.items()}\n\n# Create the PL modules for training\nmodel = PolygonEncoder(tasks, encoder, lr=LR)\ndata = PolygonDataModule(tasks, tokenizer, n_range=N_RANGE, transforms=TRANSFORMS, batch_size=BATCH_SIZE,\n                         n_batch_per_epoch=N_BATCH_PER_EPOCH)","66a9daf7":"from pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n\n# Create the trainer\nwandb_logger = pl.loggers.WandbLogger(project=PROJECT_NAME, config={\n    \"seed\": SEED,\n    \"d_model\": D_MODEL,\n    \"max_seq_len\": MAX_SEQ_LEN,\n    \"n_heads\": N_HEADS,\n    \"d_ff\": D_FF,\n    \"dropout\": DROPOUT,\n    \"activation\": ACTIVATION,\n    \"n_layers\": N_LAYERS,\n    \"task_dropout\": TASK_DROPOUT,\n    \"grad_clip\": GRAD_CLIP,\n    \"auto_lr_find\": AUTO_LR_FIND,\n    \"lr\": LR,\n})\nif WATCH_MODEL:\n    wandb_logger.watch(model, log=\"all\")\nmc = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", filename=\"{step}-{val_loss:.4f}\")\ntrainer = pl.Trainer(\n    gpus=1,\n    logger=wandb_logger,\n    callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=True), mc],\n    gradient_clip_val=GRAD_CLIP,\n    max_time=MAX_TIME,\n    auto_lr_find=AUTO_LR_FIND,\n    default_root_dir=SAVE_DIR,\n    num_sanity_val_steps=-1,\n)","8e80ea9f":"# Train !\ntrainer.tune(model, datamodule=data)\ntrainer.fit(model, datamodule=data)","41bf1035":"# Test !\nckpt = mc.best_model_path\n_ = trainer.test(model, datamodule=data, ckpt_path=ckpt)","b63b82cc":"!cp submission.csv ..\/","9618c7d8":"## 0.2 Install `cartesius` package","6b402e34":"# 3. The model : Transformer\n\nNow we just have to define a model that takes as inputs the tensor representing a polygon, and encode it into a polygon representation.\n\nWe will use a basic Transformer model for this, and extract the representation of the first token as the polygon representation.","9119c9f8":"Let's check how the generated polygon look like.","88f36bd1":"In this notebook, we will :\n\n* See how we can represent a polygon as a tensor \n* Implement a basic Transformer model\n* Train our Transformer on randomly generated polygons\n* Submit the results for the Cartesius competition","c549d970":"# 5. Testing & Submission","edc2b788":"## 0.3 Login with `wandb`\n\nWe use `wandb` to keep track of experiments and compare runs.\n\nYou need to set the secret `WANDB_KEY` in order to login to `wandb`. You can get your `wandb` key by visiting `https:\/\/wandb.ai\/authorize`.","1295a5e9":"# 2. The tokenizer\n\nPolygons are defined by a list of Points, and each point is defined as a XY coordinates.\n\nBut we need to define a way to represent this into a **tensor**. We should also ensure several samples can be **batched** into one tensor.\n\nSo let's define a `Tokenizer` class that will takes care of that :","c0a8c203":"# 1. The data\n\nWe will be using `cartesius` package to randomly generate polygons, and train our model on this data.","084db9f4":"# 0. Setup\n\n## 0.1 Ensure GPU is accessible\n\nIf you cannot see the GPU, on the right tab, go to `Settings` and set `Accelerator` to `GPU`.\n\nAlso ensure `Internet` is enabled.","2137d733":"# 4. Training\n\nNow that we defined our model, it's time to train it !\n\nWe will use the Pytorch-lightning module provided in `cartesius` to train the model.\n\nAfter training, the model will run on the test set and write the predictions in a file. ","2b54fa48":"_Note : You can rerun this cell several time, everytime a different polygon is generated._"}}