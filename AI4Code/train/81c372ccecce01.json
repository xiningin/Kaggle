{"cell_type":{"01c2a9f3":"code","9939aeb9":"code","b1ae709d":"code","84c2b112":"code","aa77fe30":"code","98819302":"code","a2be6cd1":"code","866ed490":"code","3747bcf7":"code","4d3c5b21":"code","adb4f669":"code","32d49f6e":"code","70c22b2e":"code","5cbb642c":"code","89423d7f":"code","620befb8":"code","0e1f8dbf":"code","6f97e850":"code","2fc11608":"code","c15e76c8":"code","07ea0ffc":"code","c4fc1edb":"code","7446112d":"code","4c991226":"code","afa2f1bf":"code","4ec01b8f":"code","f8966eb0":"code","2ea89cef":"code","3b0ff405":"code","88d2464e":"code","e6695cb4":"code","603f51c2":"code","4d0fce61":"code","4d006663":"code","ecf0f516":"code","a18802b4":"code","df543f49":"code","d98f9b42":"code","d0f47d0b":"code","806ec87f":"code","b881963a":"code","08b02498":"code","1d10e2ce":"code","2d6961f5":"code","f74eb131":"markdown","75d57391":"markdown","1e231384":"markdown","2b69f0ff":"markdown","8617fbf2":"markdown","1fe9afc1":"markdown","4284c4fc":"markdown","05357e10":"markdown","da442ec9":"markdown","ee5dd82e":"markdown","432cc816":"markdown","5dcd12e8":"markdown","dc7101d7":"markdown","728226ed":"markdown","f1ae64c6":"markdown","9e682cc9":"markdown","8d6613d3":"markdown","2691006c":"markdown","0f589daa":"markdown","13eecb08":"markdown"},"source":{"01c2a9f3":"#Importing Libararies\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings('ignore')","9939aeb9":"# Reading the data\ntrain_data=pd.read_csv('..\/input\/titanic\/train.csv')\ntrain_data.head()","b1ae709d":"train_data.shape","84c2b112":"train_data.describe()","aa77fe30":"# checking the percentage of missing values\nround(100*train_data.isnull().sum()\/train_data.shape[0],2)\n","98819302":"# Dropping columns that are not required for analysis\ntrain_data.drop(['Name','Ticket', 'Cabin'], axis=1, inplace=True)","a2be6cd1":"# Imputing the NaN values in Age with the median value\ntrain_data['Age'].fillna(train_data['Age'].median(), inplace=True)","866ed490":"# Only 2 values n Embarked column are missing and hence the corresponding rows can be dropped\ntrain_data= train_data.dropna()\ntrain_data.shape","3747bcf7":"# checking the percentage of missing values\nround(100*train_data.isnull().sum()\/train_data.shape[0],2)","4d3c5b21":"train_data.columns","adb4f669":"# Definning a function to plot barchart\nimport math\ndef func_bar(*args):                        \n   \n    m=math.ceil(len(args)\/2)  # getting the length f arguments to determine the shape of subplots                   \n    \n    fig,axes = plt.subplots(m,2,squeeze=False, figsize = (16, 6*m))\n    ax_li = axes.flatten()       # flattening the numpy array returned by subplots\n    i=0\n    for col in args:\n        \n        sns.countplot(x=col, data=train_data,ax=ax_li[i], order = train_data[col].value_counts().index)\n        ax_li[i].set_title(col)\n        plt.tight_layout()\n        i=i+1","32d49f6e":"bar_list=['Survived', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\nfunc_bar(*bar_list)","70c22b2e":"survived=(sum(train_data['Survived'])\/len(train_data['Survived'].index))*100\nsurvived","5cbb642c":"# Checking the distribution of Age and fare\nfig,axes = plt.subplots(1,2,squeeze=False, figsize = (16,6))\n#plt.subplot(121)\nsns.boxplot(y=train_data['Age'],ax=axes[0,0])\nplt.yscale('log')\nsns.boxplot(y=train_data['Fare'],ax=axes[0,1])\nplt.yscale('log')\n","89423d7f":"# function to plot stacked bar charts\nimport math\nfrom textwrap import wrap\ndef func_stk(*args):                        \n   \n    m=math.ceil(len(args)\/2)  # getting the length f arguments to determine the shape of subplots                   \n    \n    fig,axes = plt.subplots(m,2,squeeze=False, figsize = (16, 6*m))\n    ax_li = axes.flatten()       # flattening the numpy array returned by subplots\n    \n    i=0\n    for col in args:\n        gf=train_data.groupby(col)['Survived'].value_counts(normalize=True)*100       #grouping by target and finding the percentage value of target and non target category\n        gf=gf.unstack().plot(kind='bar',width=0.5, stacked=True,ax=ax_li[i])  # plotting a stcked bar chart\n        ax_li[i].legend(['Dead','Survived'],bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.) # changing the legend \n        gf.set_ylabel('Percentage')\n        i+=1\n        ","620befb8":"func_stk(*bar_list[1:])","0e1f8dbf":"# function to plot stackd histogram\nimport math\nfrom textwrap import wrap\ndef func_hist(*args):                        \n   \n    m=len(args)  # getting the length f arguments to determine the shape of subplots                   \n    \n    fig,axes = plt.subplots(m,1,squeeze=False, figsize = (16, 6*m))\n    ax_li = axes.flatten()       # flattening the numpy array returned by subplots\n    \n    i=0\n    for col in args:\n        train_data.pivot(columns='Survived')[col].plot(kind = 'hist', bins=10,rwidth=0.95,stacked=True,ax=ax_li[i])\n        ax_li[i].legend(['Dead','Survived'],bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.) # changing the legend \n        ax_li[i].set_xlabel(col)\n        i=i+1","6f97e850":"func_hist(*['Age','Fare'])","2fc11608":"#Converting sex column to numerical values \n# Assign 1 for female and 0 for male\ntrain_data['Sex']=train_data[['Sex']].apply(lambda x :x.map({'female':1,'male':0}))","c15e76c8":"#Perform OneHot Encoding for embarked column\nemb = pd.get_dummies(train_data['Embarked'], drop_first=True)\ntrain_data.drop('Embarked',axis=1, inplace=True)\ntrain_data=pd.concat([train_data,emb],axis=1)\ntrain_data.head()","07ea0ffc":"# Scaling the \"Age\" and \"Fare\" columns\nscaler = MinMaxScaler()\ntrain_data[[\"Age\",\"Fare\"]] = scaler.fit_transform(train_data[[\"Age\",\"Fare\"]])\ntrain_data.head()","c4fc1edb":"#Determing the correlation between variables\nplt.figure(figsize = (12, 8))\nsns.heatmap(train_data.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","7446112d":"# Putting feature variable to X\nX = train_data.drop(['PassengerId','Survived'], axis=1)\nX.head()","4c991226":"# Putting response variable to y\ny = train_data['Survived']\ny.head()","afa2f1bf":"# Logistic regression model\nlgm1 = sm.GLM(y,(sm.add_constant(X)), family = sm.families.Binomial())\nlgm1.fit().summary()","4ec01b8f":"logreg = LogisticRegression()\nrfe = RFE(logreg, 5)             # running RFE with 13 variables as output\nrfe = rfe.fit(X, y)","f8966eb0":"list(zip(X.columns, rfe.support_, rfe.ranking_))","2ea89cef":"X = train_data.drop(['PassengerId','Survived','Q'], axis=1)\n# Logistic regression model\nlgm2 = sm.GLM(y,(sm.add_constant(X)), family = sm.families.Binomial())\nlgm2.fit().summary()","3b0ff405":"X = train_data.drop(['PassengerId','Survived','Q','Parch'], axis=1)\n# Logistic regression model\nlgm3 = sm.GLM(y,(sm.add_constant(X)), family = sm.families.Binomial())\nlgm3.fit().summary()","88d2464e":"X = train_data.drop(['PassengerId','Survived','Q','Parch','Fare'], axis=1)\n\nX_sm = sm.add_constant(X[X.columns])\n\nlgm4 = sm.GLM(y,X_sm, family = sm.families.Binomial())\nres = lgm4.fit()\nres.summary()\n","e6695cb4":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","603f51c2":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","4d0fce61":"# Predicting the survival chance on train data\ny_train_pred = res.predict(X_sm)\ny_train_pred[:10]","4d006663":"thresh= 0.65  # fixed by trial and error\ny_train_pred_val = y_train_pred.apply(lambda x: 1 if x > thresh else 0)\n\ny_train_pred_final = pd.DataFrame({'Survived': y,'Predicted':y_train_pred_val})\ny_train_pred_final['PassengerId'] = train_data['PassengerId'].astype(int)\ncol=[\"PassengerId\",\"'Predicted'\"]\ny_train_pred_final.head()","ecf0f516":"y_train_pred_final.head()","a18802b4":"#Confusion matrix\nfrom sklearn import metrics\nconfusion = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final.Predicted )\nprint(confusion)","df543f49":"#Checking Accuracy\nprint(round(metrics.accuracy_score(y_train_pred_final.Survived, y_train_pred_final.Predicted),4))","d98f9b42":"# Reading test data\ntest_data=pd.read_csv('..\/input\/titanic\/test.csv')\ntest_data.head()","d0f47d0b":"#Converting sex column to numerical values \n# Assign 1 for female and 0 for male\ntest_data['Sex']=test_data[['Sex']].apply(lambda x :x.map({'female':1,'male':0}))\n#Perform OneHot Encoding for embarked column\nemb = pd.get_dummies(test_data['Embarked'], drop_first=True)\ntest_data.drop('Embarked',axis=1, inplace=True)\ntest_data=pd.concat([test_data,emb],axis=1)\ntest_data.head()","806ec87f":"# Scaling the test data\ntest_data[[\"Age\",\"Fare\"]] = scaler.transform(test_data[[\"Age\",\"Fare\"]])\ntest_data.head()","b881963a":"X_test = test_data[['Pclass','Sex','Age','SibSp','S']]\nX_test_sm = sm.add_constant(X_test)","08b02498":"y_pred = res.predict(X_test_sm)\ny_pred[:10]","1d10e2ce":"thresh= 0.65     \ny_pred_val = y_pred.apply(lambda x: 1 if x > thresh else 0)\n\ny_pred_final = pd.DataFrame({'Survived':y_pred_val})\ny_pred_final['PassengerId'] = test_data['PassengerId'].astype(int)\ncol=[\"PassengerId\",\"Survived\"]\ny_pred_final=y_pred_final.reindex()\ny_pred_final.set_index('PassengerId')\ny_pred_final.head()","2d6961f5":"cols = list(y_pred_final)\ncols[1], cols[0] = cols[0], cols[1]\ny_pred_final=y_pred_final.loc[:,cols]\ny_pred_final.set_index('PassengerId',inplace=True)\ny_pred_final.head()","f74eb131":"## Data Preparation","75d57391":"## Model Evaluation","1e231384":"### Bivariate Analysis","2b69f0ff":"Since the variable Q is found to have a very high p value and a lower rank in RFE, it is dropped and the model is rebuilt","8617fbf2":"# TITANIC : MACHINE LEARNING FROM  DISASTER","1fe9afc1":"By inspection,we can drop the columns that do not have any correlation on the survival rate. Passenger Id,Name and Ticket columns are dropped as they will not contribute to the survival rate. Since the percentage of missing values in the Cabin column is 77%, it is also dropped.  .","4284c4fc":"Variable Parch with a p values of 0.461 is dropped and model is rebuit","05357e10":"#### Scaling the numerical columns","da442ec9":"***From the above analysis, it is clear that during rescue operations, children, female and people with higher economic status were given more prefernce than others***","ee5dd82e":"1. Kids and passengers above 48 years survived more than  passengers in other age groups\n2. Passengers who paid more than $50 for tickets survived more than those who paid less","432cc816":"## Prediction ","5dcd12e8":"## Exploratory Data Analysis","dc7101d7":"## Data Cleaning","728226ed":"***Following are prople who have survived most likely:\n\n    1. Passengers who travelld in first class\n    2. Female passengers\n    3. Passengers who travelled with 2 or less siblings survived more than those who travelled with 3 or more siblings\n    4. Passengers who travelled as a family of 3 or less survived more than the others\n    5. Passengers who embarked at Cherbourg port survived more thann those who embarked at other ports\n    \n","f1ae64c6":"# Model buiding","9e682cc9":"### Univariate Analysis","8d6613d3":" From the statistical analysis, it is found that all variables are significant and hence the model lgm4 is used for prediction","2691006c":"##### Analysing the dataframe","0f589daa":"variable 'Fare' is also found to have a high p value and hence it is  also dropped","13eecb08":"*** We observe that : \n       1. There is an imbalance in the number of .....\n       2. More number of passengers travelled in third class ticket\n       3. Around 2\/3 of the passengers were male\n       4. Most of the passengers were accompanied by no or 1 sibling\n       5. most of the passengers travelled alone\n       6. Most of the passengers embarked at ............ while a few had embarked at Q...\n       "}}