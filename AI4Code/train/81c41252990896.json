{"cell_type":{"81f6c6fa":"code","c2ed7093":"code","de708310":"code","f61ca708":"code","636dea7e":"code","0b6e15b3":"code","5aa4f627":"code","0d0af4a1":"code","83a48b71":"code","bc9a5f01":"code","5d4dc7fa":"code","83840d08":"code","16a16f99":"code","fae27aab":"code","fd6c293a":"code","b94fe690":"code","72c66405":"code","f9de5bc9":"markdown","6d1bf7ca":"markdown","3eff5d80":"markdown","23bd8673":"markdown","419f7718":"markdown","50162fcb":"markdown","92dd3575":"markdown","c3e0fbc6":"markdown","3ce26059":"markdown","2f010077":"markdown","0a36a0a8":"markdown","60841782":"markdown","8b218c79":"markdown","ba94eaf5":"markdown","dacec8e6":"markdown"},"source":{"81f6c6fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# for visualization\nimport seaborn as sns \nimport matplotlib.pyplot as plt \n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport warnings\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c2ed7093":"data= pd.read_csv('..\/input\/voice.csv') # Let's load the data first as \"data\"","de708310":"data.head(10) # First 10 data  ","f61ca708":"f, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(data.corr(), annot=True, linewidths=0.5,linecolor=\"red\", fmt= '.1f',ax=ax)\nplt.show()","636dea7e":"data.info()","0b6e15b3":"data.label= [1 if (each == 'male') else 0 for each in data.label] \n\ny= data.label.values\nx_data= data.drop([\"label\"], axis=1)","5aa4f627":"x= (x_data - np.min(x_data))\/(np.max(x_data) - np.min(x_data)).values  # We scale numbers into 0 and 1 ","0d0af4a1":"from sklearn.model_selection import train_test_split \n\n# Random state will divide data to always same equality to 42 number. Its means anytime you dive to data its will  give to same result\nx_train, x_test, y_train, y_test= train_test_split(x,y, test_size=0.2, random_state= 42)\n\nx_train= x_train.T\nx_test= x_test.T\ny_train= y_train.T\ny_test= y_test.T","83a48b71":"def init_weights_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w, b","bc9a5f01":"def sigmoid(z):\n    y_head = 1 \/ (1 + np.exp(-z))\n    return y_head","5d4dc7fa":"# Forward propagation steps:\n# find z = w.T*x+b\n# y_head = sigmoid(z)\n# loss(error) = loss(y,y_head)\n# cost = sum(loss)\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # Forward Propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    \n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    # Cost is summary of all losses\n    cost = (np.sum(loss))\/x_train.shape[1] # x_train.shape[1] is count of all samples\n    # Divide to sample size because of scaling\n    \n    # Backward Propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] \n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 \n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost, gradients","83840d08":"def update_weights_bias(w,b,x_train,y_train,learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    for i in range(number_of_iterarion):\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        w = w - learning_rate * gradients[\"derivative_weight\"] \n        b = b - learning_rate * gradients[\"derivative_bias\"]   \n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","16a16f99":"def predict(w,b,x_test):\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","fae27aab":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iterations):\n    dimension = x_train.shape[0]\n    w, b = init_weights_bias(dimension)\n\n    parameters,gradients,cost_list = update_weights_bias(w,b,x_train,y_train,learning_rate,num_iterations)\n\n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return y_prediction_test #Estimates for Complex Matrix\n    \ny_predict = logistic_regression(x_train, y_train, x_test, y_test, learning_rate=1, num_iterations=300)","fd6c293a":"predict = []\nfor i in range(0,1):\n    for each in y_predict[i]:\n        predict.append(int(each))","b94fe690":"# Total predicted datas by gender\nmale=0\nfemale=0\nfor i in range(y_predict.shape[1]):\n    if y_predict[0][i] == 0:\n        male= male+1\n    else:\n        female= female+1\n        \nx=[\"Male\", \"Female\"]\ny=[male, female]\n\nplt.figure(figsize=(10,10))\nsns.barplot(x=x, y=y, palette = sns.cubehelix_palette(len(x)))\nplt.xlabel(\"Genders\")\nplt.ylabel(\"Data By Genders\")\nplt.show()\n\n","72c66405":"# We'll see how many data are predicted correctly. \ntrue_predict = 0\nfalse_predict = 0\nfor x in range(len(predict)):\n    for y in range(x,len(y_test)):\n        if (predict[x] == y_test[y]):\n            true_predict = true_predict +1\n            break\n        else:\n            false_predict = false_predict +1\n            break\n            \n# Visualization\nx_Axis = [\"True\",\"False\"]\ny_Axis = [true_predict,false_predict]\n\nplt.figure(figsize=(15,15))\nsns.barplot(x=x_Axis,y=y_Axis,palette = sns.cubehelix_palette(len(x_Axis)))\nplt.xlabel(\"Gender Class\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Male and Female\")\nplt.show()            ","f9de5bc9":"<a id=\"read_data\"><\/a> \n ## READ DATA","6d1bf7ca":" <a id=\"update\"> <\/a>\n ## UPDATING PARAMETERS(LEARNING)","3eff5d80":"<a id=\"viz\"><\/a>\n## VISUALIZATION OF RESULTS \nIf we stack true and false predicts, we can easy visualize this values. ","23bd8673":"<a id=\"sigmoid\"> <\/a> \n## IMPLEMENTING SIGMOID FUNCTION\n\n![](http:\/\/cdn-images-1.medium.com\/max\/2000\/1*JHWL_71qml0kP_Imyx4zBg.png)\n\n Looking at the graph, we can see that the given a number n, the sigmoid function would map that number between 0 and 1. \n As the value of n gets larger, the value of the sigmoid function gets closer and closer to 1 and as n gets smaller, the value of the sigmoid function is get closer and closer to 0.","419f7718":" <a id=\"normalization\"><\/a> \n ## NORMALIZATION","50162fcb":"<a id=\"int\"> <\/a> \n## INITIALIZING PARAMETERS","92dd3575":"<a id=\"split\"><\/a> \n ## TRAIN TEST SPLIT\n  By using sklearn library we are spliting the data to %20 test and %80 train. ","c3e0fbc6":"<a id=\"con\"><\/a>\n## CONVERTING FEATURE TO INT\n As we can see \"label\" is \"object\". We can't use \"object\" for classifaciton problems. The Logistic Algortihm is has to be 2 situation. For this dataset our situatins is \"man\" or \"woman\" and there outputs has to be 0 or 1, so lets convert to \"int\". ","3ce26059":"<a id=\"prediction\"> <\/a> \n## PREDICTION","2f010077":"## INTRODUCTION\n\nIn this project, we will implement logistic regression algrotithm without Sklearn Logistic Rregression library. \n\n*  [Reading csv](#read_data)\n*  [Checking first 10 data for look features](#data_head)\n*  [Checking correlation between features](#cor)\n*  [Checking data types](#data_info)\n*  [Converting the feature to int](#convert)\n*  [Normalization](#normalization)\n*  [Splitting data to train and test](#split)\n*  [In\u0131talizing parameters](#int)\n*  [Implementing sigmoid function](#sigmoid)\n*  [Implementing forward propagation](#forward)\n*  [Updating parameters](#update)\n*  [Prediction](#prediction)\n*  [Logistic Regression](#log)\n*  [Visualization of results](#viz)\n\nSource: https:\/\/www.kaggle.com\/kanncaa1\/deep-learning-tutorial-for-beginners","0a36a0a8":"<a id=\"log\"> <\/a> \n## LOGISTIC REGRESSION","60841782":"<a id=\"data_head\"><\/a> \n ## CHECKING FIRST 10 DATA FOR LOOK FEATURES","8b218c79":"<a id=forward> <\/a> \n## FORWARD PROPAGATION\n\nThe cost or loss function has an important job in that it must faithfully distill all aspects of the model down into a single number in such a way that improvements in that number are a sign of a better model.\n\n![](http:\/\/image.ibb.co\/dFCR3H\/6.jpg)","ba94eaf5":"<a id=\"cor\"><\/a> \n ## CHECKING CORELLATION BETWEEN FEATURES","dacec8e6":"<a id=\"data_info\"><\/a> \n## CHECKING DATA TYPES"}}