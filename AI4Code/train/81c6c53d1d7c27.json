{"cell_type":{"3210ff73":"code","eb9bac0c":"code","164cbdcc":"code","19652b59":"code","1418f2ea":"code","891d91f5":"code","7dc33dd4":"code","ffddb6ae":"code","4129315e":"code","9998e280":"code","a5385528":"code","1948f58b":"code","ea85f9e9":"code","6f4b1a7e":"code","39a9f585":"code","2618b8a7":"code","e02c360f":"code","62517ed6":"code","f4e332b3":"code","12cf3be5":"code","d50f1d7b":"code","84877aec":"code","fe4071e7":"code","941d0c87":"code","ebba0b52":"code","1a903f1c":"code","0cba2d3e":"code","6df4f019":"code","b3b34240":"code","664590bd":"code","dd57ae8c":"code","2c53ec16":"code","f00102a6":"code","f0a29bcc":"code","326922b9":"code","9a836e9f":"code","0cd7ea88":"code","afea6a29":"code","24f4d901":"code","db9c7111":"code","f4be1ee5":"code","1e1505a0":"code","6f1a8437":"code","d505bc29":"code","b9a6629e":"code","119b4dc7":"code","70069afa":"code","fed775b0":"code","a560108b":"code","774b9d2e":"code","fd6aa8f5":"code","0e800c03":"code","5f137b93":"code","4ee56582":"code","24b1ab50":"code","1ea8a43a":"code","53d8beca":"code","f8cfe138":"code","5d7af24f":"code","efb5dcba":"code","6fab4e8c":"code","1c800450":"code","998e2f79":"code","7c2eaee6":"code","2b176044":"code","3b03c3f2":"code","f073b759":"code","9bae07b5":"code","0cdfb9e8":"code","73fceefb":"code","12be41eb":"code","2a251f9e":"code","4fe1bddd":"code","556cd6a3":"code","4f56a3b0":"code","1503370f":"code","6754f141":"code","9d9f6e58":"code","3d884024":"markdown","be95b1da":"markdown","6446736e":"markdown","b8620355":"markdown","13727ace":"markdown","ec67ea35":"markdown","e3e7901c":"markdown","9fed1ba9":"markdown","b401b5f0":"markdown","b89d67c8":"markdown","da63e34c":"markdown","0286ffc4":"markdown","4e83d6cc":"markdown","2b2958a1":"markdown","a3c5b14c":"markdown","43461e89":"markdown","ce930486":"markdown","7c42eeb7":"markdown","f7d56007":"markdown","1ee5f8ea":"markdown","29c9c3c9":"markdown","bc437668":"markdown","c45ad481":"markdown","7704e774":"markdown","31cbf21f":"markdown","aae60e70":"markdown","e98800ec":"markdown","c6135084":"markdown","bfead8a2":"markdown","4f28a1c9":"markdown","a9d02bd8":"markdown","ff8cd9ae":"markdown","a9aab72c":"markdown","0d6e9ebd":"markdown","eb007530":"markdown","9f46af5e":"markdown","98914d88":"markdown","ab647e7d":"markdown","84131216":"markdown","243e8671":"markdown","787707ea":"markdown","8f245992":"markdown","9809c195":"markdown","152b39de":"markdown","e267229b":"markdown","57139470":"markdown","455be7fd":"markdown","3c5765af":"markdown","9cbc0de2":"markdown","4b18b3de":"markdown"},"source":{"3210ff73":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nfrom scipy.stats import norm\nfrom scipy import stats\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# sklearn_processing\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder,OrdinalEncoder\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer,IterativeImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score,GridSearchCV\nfrom sklearn.metrics import classification_report,confusion_matrix,make_scorer,f1_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest,f_regression\n\n# sklearn_algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","eb9bac0c":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","164cbdcc":"data = train.copy()\ndata.info()","19652b59":"data.head(10)","1418f2ea":"data.isna().sum()","891d91f5":"data.describe()","7dc33dd4":"# Visualizing how many null values.\ndata_null = data.isna().sum()\ndata_null.plot.bar()","ffddb6ae":"# Noisiness & Outliers check\ndata_num = data.select_dtypes('number')\nplt.figure(figsize=(20,10))\nsns.boxplot(data=data_num)","4129315e":"# data distribution:\nfor col in ['Age','Fare']:\n    plt.figure(figsize=(5,5))\n    sns.distplot(data_num[col],bins=50,kde = True)","9998e280":"sns.pairplot(data_num)","a5385528":"# Feature correlation:\ncorr = data_num.corr()\nsns.heatmap(corr,annot=True)","1948f58b":"\n# Gaussian boundaries for Age only since it is normally distributed\nupper_boundary = data['Age'].mean()+3*data['Age'].std()\nlower_boundary = data['Age'].mean()-3*data['Age'].std()\nprint('For Age: the upper boundary is '+str(upper_boundary))\nprint('For Age: the lower boundary is '+str(lower_boundary))\n\nprint('-------------------------------------')\n# IQR to calculate boundaries:\nfor col in ['Age','Fare']:\n    IQR = data[col].quantile(0.75)-data[col].quantile(0.25)\n    lower_bridge = data[col].quantile(0.25)-(IQR*1.5)\n    upper_bridge = data[col].quantile(0.75)+(IQR*1.5)\n    print('for ',str(col),' the lower bridge is ' + str(lower_bridge))\n    print('for ',str(col),' the upper bridge is ' + str(upper_bridge))\n    \nprint('-------------------------------------')\n# Extreme outliers\nfor col in ['Age','Fare']:\n    IQR = data[col].quantile(0.75)-data[col].quantile(0.25)\n    lower_bridge = data[col].quantile(0.25)-(IQR*3)\n    upper_bridge = data[col].quantile(0.75)+(IQR*3)\n    print('for ',str(col),' the lower bridge is ' + str(lower_bridge))\n    print('for ',str(col),' the upper bridge is ' + str(upper_bridge))\n    ","ea85f9e9":"# Age will be fixed by Gaussian boundaries but Fare will be fixed by Extreme\ndata.Age[data.Age>=73]=73\ndata.Fare[data.Fare>=100]=100","6f4b1a7e":"data.describe()","39a9f585":"plt.figure(figsize=(20,10))\nsns.boxplot(data=data.select_dtypes('number'))","2618b8a7":"data['Cabin'].value_counts()\n","e02c360f":"\ndata['Embarked'].value_counts()","62517ed6":"s_imputer = SimpleImputer(strategy='most_frequent')\nI_imputer = IterativeImputer()\ndef missing_values(data):\n    data['Cabin'].fillna('none',inplace= True)\n    data['Age'] = I_imputer.fit_transform(data[['Age']])\n    data[[col for col in data.columns if col not in ['Age','Cabin']]] = s_imputer.fit_transform(data[[col for col in data.columns if col not in ['Age','Cabin']]])\n    return\nmissing_values(data)\ndata.info()","f4e332b3":"passenger_Id = data['PassengerId']\ny_train = data['Survived']","12cf3be5":"data.describe(include=['O'])","d50f1d7b":"def feature_extraction(data):\n    data.drop(['Ticket','Cabin'],axis=1,inplace =True)\n    name = pd.Series(data['Name']).str.split(',',expand=True)\n    name_ = pd.Series(name[1]).str.split('.',expand=True)\n    data.Name = name_[0]\n    return\nfeature_extraction(data)\ndata","84877aec":"data[['Name', 'Survived']].groupby(['Name'], as_index=False).mean().sort_values(by='Survived', ascending=False)","fe4071e7":"data.drop(['Survived','PassengerId'],axis=1, inplace = True)","941d0c87":"data.Name.value_counts()","ebba0b52":"# Replacing Rare names by grouping them as one category\ndef replace_rare(data):\n    data['Name'].replace([' Lady', ' the Countess',' Capt', ' Col',\n                                         ' Don', ' Major', ' Sir', ' Jonkheer', ' Dona'], ' Rare',inplace=True)\n\n    data['Name'].replace(' Mlle', ' Miss',inplace=True)\n    data['Name'].replace(' Ms', ' Miss',inplace=True)\n    data['Name'].replace(' Mme', ' Mrs',inplace=True)\n    print(data.Name.value_counts())\n    return\nreplace_rare(data)","1a903f1c":"data","0cba2d3e":"# categorical encoding\nohe = OneHotEncoder(handle_unknown='ignore')\ncol_transformer = make_column_transformer((OrdinalEncoder(categories= [['male','female'],['S','C','Q']]),\n                                        ['Sex','Embarked']),remainder = 'passthrough')\ndef categorical_encoding(data_):\n    global data\n\n    cat_name = ohe.fit_transform(data_['Name'].to_numpy().reshape(-1, 1)).toarray()\n    ohe_df = pd.DataFrame(cat_name, columns=ohe.get_feature_names())\n    data_ = pd.concat([data_, ohe_df], axis=1).drop(['Name'], axis=1)\n\n    data__ = col_transformer.fit_transform(data_)\n    data = pd.DataFrame(data__, columns=['Sex','Embarked']+[col for col in data_.columns if col not in ['Sex','Embarked']]) \n    \n    return data\n\n######## OneHotEncoding for All\n\n#col_transformer = make_column_transformer((OneHotEncoder(handle_unknown='ignore'),\n #                                          ['Sex','Name','Embarked']),remainder = 'passthrough')\n#def categorical_encoding(data_):\n #   global data\n  #  data__ = col_transformer.fit_transform(data_)\n   # data = pd.DataFrame(data__) \n    #data.columns = [0 ,1, 2,3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 'Age' ,14 ,15, 'Fare']\n    #return data\n    \ncategorical_encoding(data)","6df4f019":"# Transform Features (Normality)\ncols = ['Age' ,'Fare']\nfor col in cols:\n    fig = plt.figure()\n\n    sns.distplot(data[col], fit=norm);\n    fig = plt.figure()\n    stats.probplot(data[col], plot=plt)","b3b34240":"def transform_data(data):\n    data['Fare'], parameters = stats.boxcox(data['Fare'].replace(0,1))\n    data['Fare'], parameters = stats.boxcox(data['Fare'].replace(0,1))\n    return\ntransform_data(data)\nfor col in cols:\n    fig = plt.figure()\n\n    sns.distplot(data[col], fit=norm);\n    fig = plt.figure()\n    stats.probplot(data[col], plot=plt)","664590bd":"# Feature Scaling for Age attribute\nstd_scaler = StandardScaler()\ndata.Age = std_scaler.fit_transform(data.Age.to_numpy().reshape(-1,1))\n","dd57ae8c":"def processing_pipeline(test):\n    \n    def missing_values_(test):\n        test['PassengerId'] , test['Survived'] = passengerid , y_train\n        test['Cabin'].fillna('none',inplace= True)\n        test['Age'] = I_imputer.transform(test[['Age']])\n        test[[col for col in test.columns if col not in ['Age','Cabin']]] = s_imputer.transform(test[[col for col in test.columns if col not in ['Age','Cabin']]])\n        test.drop(['PassengerId','Survived'],axis = 1, inplace=True)\n        return\n\n    missing_values_(test)\n    \n    feature_extraction(test)\n\n    replace_rare(test)\n\n    def categorical_encoding(test_):\n        global test\n\n        cat_name = ohe.transform(test_['Name'].to_numpy().reshape(-1, 1)).toarray()\n        ohe_df = pd.DataFrame(cat_name, columns=ohe.get_feature_names())\n        test_ = pd.concat([test_, ohe_df], axis=1).drop(['Name'], axis=1)\n\n        data__ = col_transformer.transform(test_)\n        test = pd.DataFrame(data__, columns=['Sex','Embarked']+[col for col in test_.columns if col not in ['Sex','Embarked']]) \n    \n        return \n\n    categorical_encoding(test)\n    \n    def transform_data():\n        global test\n        test['Fare'], parameters = stats.boxcox(test['Fare'].replace(0,1))\n        test['Fare'], parameters = stats.boxcox(test['Fare'].replace(0,1))\n        return\n    \n    transform_data()\n    \n    def std():\n        global test\n        test.Age = std_scaler.transform(test.Age.to_numpy().reshape(-1,1))\n        return\n    std()\n    return test","2c53ec16":"logistic_model = LogisticRegression()\nlogistic_model.fit(data,y_train)\ny_pred_logistic = logistic_model.predict(data)","f00102a6":"def scoring(y_train,y_pred):\n    print('The classification Report:\\n ', classification_report(y_train,y_pred))\n    print('--------------------------------------------')\n    print('The Confusion Matrix:\\n ', confusion_matrix(y_train,y_pred))\n    return\nscoring(y_train,y_pred_logistic)","f0a29bcc":"def score(y_train, y_pred):\n    print(classification_report(y_train, y_pred)) \n    print('----------------------------------------------------------')\n    print(confusion_matrix(y_train, y_pred))\n    print('----------------------------------------------------------')\n\n    return f1_score(y_train,y_pred)\n    ","326922b9":"logistic_score = cross_val_score(logistic_model,data,y_train,scoring= make_scorer(score), cv=5)","9a836e9f":"# SVC\nsvc_model = SVC()\nsvc_model.fit(data,y_train)\ny_pred_scv = svc_model.predict(data)\n\n# LinearSVC\nlinearsvc_model = LinearSVC()\nlinearsvc_model.fit(data,y_train)\ny_pred_linearscv = linearsvc_model.predict(data)","0cd7ea88":"# SVC\nscoring(y_train,y_pred_scv)\n\n# LinearSVC\nscoring(y_train,y_pred_linearscv)","afea6a29":"# SVC\nsvc_score = cross_val_score(svc_model,data,y_train,scoring= make_scorer(score), cv=5)\nprint('==========================================')\n# LinearSVC\nlinearsvc_score = cross_val_score(linearsvc_model,data,y_train,scoring= make_scorer(score), cv=5)","24f4d901":"forest_model = RandomForestClassifier()\nforest_model.fit(data,y_train)\ny_pred_forest = forest_model.predict(data)","db9c7111":"scoring(y_train,y_pred_forest)","f4be1ee5":"forest_score = cross_val_score(forest_model,data,y_train,scoring= make_scorer(score), cv=5)","1e1505a0":"kn_model = KNeighborsClassifier()\nkn_model.fit(data,y_train)\ny_pred_kn = kn_model.predict(data)","6f1a8437":"scoring(y_train,y_pred_kn)","d505bc29":"kn_score = cross_val_score(kn_model,data,y_train,scoring= make_scorer(score), cv=5)","b9a6629e":"naive_model = GaussianNB()\nnaive_model.fit(data,y_train)\ny_pred_naive = naive_model.predict(data)","119b4dc7":"scoring(y_train,y_pred_naive)","70069afa":"naive_score = cross_val_score(naive_model,data,y_train,scoring= make_scorer(score), cv=5)","fed775b0":"perceptron_model = Perceptron()\nperceptron_model.fit(data,y_train)\ny_pred_perceptron = perceptron_model.predict(data)","a560108b":"scoring(y_train,y_pred_perceptron)","774b9d2e":"perceptron_score = cross_val_score(perceptron_model,data,y_train,scoring= make_scorer(score), cv=5)","fd6aa8f5":"sgd_model = SGDClassifier()\nsgd_model.fit(data,y_train)\ny_pred_sgd = sgd_model.predict(data)","0e800c03":"scoring(y_train,y_pred_sgd)","5f137b93":"sgd_score = cross_val_score(sgd_model,data,y_train,scoring= make_scorer(score), cv=5)","4ee56582":"tree_model = DecisionTreeClassifier()\ntree_model.fit(data,y_train)\ny_pred_tree = tree_model.predict(data)","24b1ab50":"scoring(y_train,y_pred_tree)","1ea8a43a":"tree_score = cross_val_score(tree_model,data,y_train,scoring= make_scorer(score), cv=5)","53d8beca":"logistic_pipeline = Pipeline([('selector',SelectKBest(f_regression)), ('model',LogisticRegression(random_state = 42,max_iter=1000))])\n\nlogistic_grid= GridSearchCV( estimator = logistic_pipeline, param_grid = {'selector__k':[12,13,14] , \n        'model__C':[0.1,1,2],'model__solver':['liblinear'],'model__penalty':['l1','l2']}, n_jobs=-1, scoring = make_scorer(score), cv=5, verbose=3)","f8cfe138":"logistic_grid.fit(data,y_train)\nprint('the best parameters : ',logistic_grid.best_params_)\nprint('the best score = ', logistic_grid.best_score_)","5d7af24f":"svc_pipeline = Pipeline([('selector',SelectKBest(f_regression)), ('model',SVC(random_state = 42))])\n\nsvc_grid= GridSearchCV( estimator = svc_pipeline, param_grid = {'selector__k':[12,13,14] , \n        'model__C':[1,0.5],'model__kernel':['sigmoid','linear','rbf','poly']}, n_jobs=-1, scoring = make_scorer(score), cv=5, verbose=3)","efb5dcba":"svc_grid.fit(data,y_train)\nprint('the best parameters : ',svc_grid.best_params_)\nprint('the best score = ', svc_grid.best_score_)","6fab4e8c":"forest_pipeline = Pipeline([('selector',SelectKBest(f_regression)), ('model',RandomForestClassifier(random_state = 42))])\n\nforest_grid= GridSearchCV( estimator = forest_pipeline, param_grid = {'selector__k':[11,12,13] , \n        'model__n_estimators':np.arange(40,81,20),'model__max_depth':[5,7,9],\n        'model__min_samples_split':[5,7,10],'model__max_features':[8,9,10]}, n_jobs=-1, scoring = make_scorer(score), cv=7, verbose=3)","1c800450":"forest_grid.fit(data,y_train)\nprint('the best parameters : ',forest_grid.best_params_)\nprint('the best score = ', forest_grid.best_score_)","998e2f79":"kn_pipeline = Pipeline([('selector',SelectKBest(f_regression)), ('model',KNeighborsClassifier())])\n\nkn_grid= GridSearchCV( estimator = kn_pipeline, param_grid = {'selector__k':[12,13,14] , \n        'model__n_neighbors':np.arange(5,15,2)}, n_jobs=-1, scoring = make_scorer(score), cv=5, verbose=3)","7c2eaee6":"kn_grid.fit(data,y_train)\nprint('the best parameters : ',kn_grid.best_params_)\nprint('the best score = ', kn_grid.best_score_)","2b176044":"tree_pipeline = Pipeline([('selector',SelectKBest(f_regression)), ('model',DecisionTreeClassifier())])\n\ntree_grid= GridSearchCV( estimator = tree_pipeline, param_grid = {'selector__k':[10,11,12] , \n        'model__max_depth':[5,7,10],'model__min_samples_split':[5,7,10]}, n_jobs=-1, scoring = make_scorer(score), cv=8, verbose=3)","3b03c3f2":"tree_grid.fit(data,y_train)\nprint('the best parameters : ',tree_grid.best_params_)\nprint('the best score = ', tree_grid.best_score_)","f073b759":"test","9bae07b5":"test.info()","0cdfb9e8":"passengerid = test['PassengerId']\ntest.drop('PassengerId',axis=1,inplace=True)","73fceefb":"processing_pipeline(test)","12be41eb":"model = forest_grid.best_estimator_","2a251f9e":"y_predicted = model.predict(test)","4fe1bddd":"sub = pd.DataFrame()\nsub['PassengerId'] = passengerid\nsub['Survived'] = y_predicted\nsub.to_csv('submission.csv',index=False)","556cd6a3":"model_logisitc = logistic_grid.best_estimator_\nmodel_svc      = svc_grid.best_estimator_\nmodel_forest   = forest_grid.best_estimator_\nmodel_kn       = kn_grid.best_estimator_\nmodel_tree     = tree_grid.best_estimator_","4f56a3b0":"y_predicted_logistic = model_logisitc.predict(test)\ny_predicted_svc      = model_svc.predict(test)\ny_predicted_forest   = model_forest.predict(test)\ny_predicted_kn       = model_kn.predict(test)\ny_predicted_tree     = model_tree.predict(test)","1503370f":"y_pred = stats.mode([y_predicted_logistic,y_predicted_svc,y_predicted_forest,y_predicted_kn,y_predicted_tree])","6754f141":"y_pred[0].T.shape","9d9f6e58":"sub_ensemble = pd.DataFrame()\nsub_ensemble['PassengerId'] = passengerid\nsub_ensemble['Survived'] = y_pred[0].T\nsub_ensemble.to_csv('submission_ensemble.csv',index=False)","3d884024":"## Exploration Summary:\n* **5 features are object type** ... Name is TEXT....Sex is Ordinal categorical .... Ticket is text ..... Cabin & Embarked are nominal categorical.\n* **For NAN values**             ... Age has 177 over 891 .... Cabin has 687 over 891.... Embarked has 2 over 891.\n* **for outliers**               ... it may be exist in Age and Highly  exist in Fare.\n* **for distribution type**      ... most of the features are right skewed.","be95b1da":"### 2. SVC & LinearSVC Model","6446736e":"# Fine-Tune the models","b8620355":"Model Evaluation","13727ace":"#### Frame the problem\n* the objective of the project is to create a model that predicts which passengers survived the Titanic shipwreck. \n* the model will be Supervised, Offline.\n* the performance will be measured by Confusion Matrix,roc,auc,accuracy.\n","ec67ea35":"### 5. DecisionTreeClassifier Model","e3e7901c":"# Final Model for Test set","9fed1ba9":"#  Data Exploration","b401b5f0":"#### Cross validation Score","b89d67c8":"### the most promising model \n**RandomForestClassifier**","da63e34c":"### 7.SGDClassifier","0286ffc4":"\n#### Cross validation Score","4e83d6cc":"### 1. Logistic Regression Model","2b2958a1":"### 4. KNeighborsClassifier Model","a3c5b14c":"### Ensemble Model\nhere will be predict based on voting from the 5 promising models","43461e89":"# Training models","ce930486":"#### Cross validation Score","7c42eeb7":"Model Evaluation","f7d56007":"# Importing lib","1ee5f8ea":"#### Cross validation Score","29c9c3c9":"Model Evaluation","bc437668":"### 5.GaussianNB","c45ad481":"#### Cross validation Score","7704e774":"# Load Data","31cbf21f":"Model Evaluation","aae60e70":"Model Evaluation","e98800ec":"# Data Preparation","c6135084":"Model Evaluation","bfead8a2":"### 8.DecisionTreeClassifier","4f28a1c9":"### 6.Perceptron","a9d02bd8":"### 3. RandomForestClassifier Model","ff8cd9ae":"### Final Model ","a9aab72c":"#### Cross validation Score","0d6e9ebd":"Model Evaluation","eb007530":"### Working on Test data processing","9f46af5e":"###  4.KNeighborsClassifier","98914d88":"#### Cross validation Score","ab647e7d":"## Preparation Summary:\n* **missing_values** Function\n* **Feature_extraction** Function\n* **replace_rare** Function\n* **categorical_encoding** Function\n* **transform_data** Function\n* **Feature Scaling** Code\n* **Processing_pipeline** Function","84131216":"Model Evaluation","243e8671":"##### fixing outliers in Age & Fare:\n","787707ea":"### Feature Scaling","8f245992":"### 2. SVC Model","9809c195":"#### Cross validation Score","152b39de":"### Making Processing PipeLine for test and new data","e267229b":"##### Impute Missing Values","57139470":"### The most promising models are: \n1. LogisticRegression Model\n2. SVC Model \n3. RandomForestClassifier Model\n4. KNeighborsClassifier Model\n5. DecisionTreeClassifier Model","455be7fd":"### 1. LogisticRegression Model","3c5765af":"### Feature Engineering","9cbc0de2":"### data cleaning","4b18b3de":"### 3.RandomForestClassifier Model"}}