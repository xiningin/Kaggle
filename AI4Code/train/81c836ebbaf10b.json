{"cell_type":{"2dcba298":"code","bb3f2b42":"code","b581bab5":"code","091016df":"code","1c035220":"code","d25a899a":"code","890eff1c":"code","603056a3":"code","cfaed5b6":"code","6419be13":"code","0010fde9":"code","e7cf7b64":"code","e6c3ccb9":"code","72671bce":"code","e4057bc5":"code","e5fa8d33":"code","43feb352":"code","2ca22021":"code","aa58abba":"code","2c8492c9":"code","e2fed565":"code","1de39fca":"code","4c2fe7dd":"code","69fec8e3":"code","d778fcc3":"code","95d8d861":"code","5574c6ba":"code","f4511f87":"code","c359f487":"code","8402dab8":"code","4db8ee72":"code","5ba53184":"code","2d87d1e2":"code","6a5a7756":"code","ada31c1e":"code","709eb6ef":"code","d3cfef57":"code","71fc017a":"code","7d2a7bab":"code","e15b96c0":"code","0848ddde":"code","647e44be":"code","c3d7cbb7":"code","649ff3b4":"code","94c2c756":"markdown","8a90362a":"markdown","0b9cd801":"markdown","b536bb7f":"markdown","36e2551c":"markdown","e9e16713":"markdown","2c39d5ae":"markdown","de2422d3":"markdown","d956d0f0":"markdown","1f313068":"markdown","8aad7383":"markdown","f830e87f":"markdown","2106db40":"markdown","efd1003d":"markdown","2866242a":"markdown","381d873b":"markdown","578dbb68":"markdown","e40b7b32":"markdown","9bf885ff":"markdown","8961763e":"markdown","9aa88171":"markdown","85a0ea5a":"markdown","9ae702a8":"markdown","52a03e30":"markdown","0a24a872":"markdown","12d19863":"markdown","2c43e18f":"markdown"},"source":{"2dcba298":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bb3f2b42":"# classifier models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# modules to handle data\nimport pandas as pd\nimport numpy as np\n\n# visualization tools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","b581bab5":"# load data\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n","091016df":"\n# save PassengerId for final submission\npassengerId = test.PassengerId\n\n# merge train and test\ntitanic = train.append(test, ignore_index=True)","1c035220":"# create indexes to separate data later on\ntrain_idx = len(train)\ntest_idx = len(titanic) - len(test)","d25a899a":"titanic.head()","890eff1c":"# get info on features\ntitanic.info()","603056a3":"# PassengerId can be removed from data for now\ntitanic.drop('PassengerId', axis=1, inplace=True)","cfaed5b6":"# create a new feature to extract title names from the Name column\ntitanic['Title'] = titanic.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\n\n# view the newly created feature\ntitanic.head()","6419be13":"# show count of titles\nprint(\"There are {} unique titles.\".format(titanic.Title.nunique()))\n\n# show unique titles\nprint(\"\\n\", titanic.Title.unique())","0010fde9":"# normalize the titles\nnormalized_titles = {\n    \"Capt\":       \"Officer\",\n    \"Col\":        \"Officer\",\n    \"Major\":      \"Officer\",\n    \"Jonkheer\":   \"Royalty\",\n    \"Don\":        \"Royalty\",\n    \"Sir\" :       \"Royalty\",\n    \"Dr\":         \"Officer\",\n    \"Rev\":        \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Dona\":       \"Royalty\",\n    \"Mme\":        \"Mrs\",\n    \"Mlle\":       \"Miss\",\n    \"Ms\":         \"Mrs\",\n    \"Mr\" :        \"Mr\",\n    \"Mrs\" :       \"Mrs\",\n    \"Miss\" :      \"Miss\",\n    \"Master\" :    \"Master\",\n    \"Lady\" :      \"Royalty\"\n}","e7cf7b64":"# map the normalized titles to the current titles\ntitanic.Title = titanic.Title.map(normalized_titles)\n\n# view value counts for the normalized titles\nprint(titanic.Title.value_counts())","e6c3ccb9":"# group by Sex, Pclass, and Title\ngrouped = titanic.groupby(['Sex','Pclass', 'Title'])\n\n# view the median Age by the grouped features\ngrouped.Age.median()","72671bce":"# apply the grouped median value on the Age NaN\ntitanic.Age = grouped.Age.apply(lambda x: x.fillna(x.median()))\n\n# view changes\ntitanic.info()","e4057bc5":"\n# fill Cabin NaN with U for unknown\ntitanic.Cabin = titanic.Cabin.fillna('U')","e5fa8d33":"# find most frequent Embarked value and store in variable\nmost_embarked = titanic.Embarked.value_counts().index[0]\n\n# fill NaN with most_embarked value\ntitanic.Embarked = titanic.Embarked.fillna(most_embarked)","43feb352":"\n# fill NaN with median fare\ntitanic.Fare = titanic.Fare.fillna(titanic.Fare.median())\n\n# view changes\ntitanic.info()","2ca22021":"# view the percentage of those that survived vs. those that died in the Titanic\ntitanic.Survived.value_counts(normalize=True)","aa58abba":"# group by sex\ngroup_by_sex = titanic.groupby('Sex')\n\n# survival rate by sex\ngroup_by_sex.Survived.mean()","2c8492c9":"# group by passenger class and sex\ngroup_class_sex = titanic.groupby(['Pclass', 'Sex'])\n\n# survival rates by class and sex\ngroup_class_sex.Survived.mean()","e2fed565":"# plot by Survivded, Sex\n_ = sns.factorplot(x='Sex', col='Survived', data=titanic, kind='count')","1de39fca":"\n# plot by Pclass, Sex, Survived\n_ = sns.factorplot(x='Pclass', hue='Sex', col='Survived', data=titanic, kind='count')","4c2fe7dd":"# get stats on all the features\ntitanic.describe()","69fec8e3":"# size of families (including the passenger)\ntitanic['FamilySize'] = titanic.Parch + titanic.SibSp + 1","d778fcc3":"# map first letter of cabin to itself\ntitanic.Cabin = titanic.Cabin.map(lambda x: x[0])\n\n# view normalized count\ntitanic.Cabin.value_counts(normalize=True)","95d8d861":"titanic.head()","5574c6ba":"# Convert the male and female groups to integer form\ntitanic.Sex = titanic.Sex.map({\"male\": 0, \"female\":1})","f4511f87":"# create dummy variables for categorical features\npclass_dummies = pd.get_dummies(titanic.Pclass, prefix=\"Pclass\")\ntitle_dummies = pd.get_dummies(titanic.Title, prefix=\"Title\")\ncabin_dummies = pd.get_dummies(titanic.Cabin, prefix=\"Cabin\")\nembarked_dummies = pd.get_dummies(titanic.Embarked, prefix=\"Embarked\")","c359f487":"\n# concatenate dummy columns with main dataset\ntitanic_dummies = pd.concat([titanic, pclass_dummies, title_dummies, cabin_dummies, embarked_dummies], axis=1)\n\n# drop categorical fields\ntitanic_dummies.drop(['Pclass', 'Title', 'Cabin', 'Embarked', 'Name', 'Ticket'], axis=1, inplace=True)\n\ntitanic_dummies.head()","8402dab8":"# create train and test data\ntrain = titanic_dummies[ :train_idx]\ntest = titanic_dummies[test_idx: ]\n\n# convert Survived back to int\ntrain.Survived = train.Survived.astype(int)","4db8ee72":"# create X and y for data and target values\nX = train.drop('Survived', axis=1).values\ny = train.Survived.values","5ba53184":"# create array for test set\nX_test = test.drop('Survived', axis=1).values\n#X_test","2d87d1e2":"\n# create param grid object\nlog_params = dict(\n    C = np.logspace(-5, 8, 15),\n    penalty = ['l1', 'l2']\n)","6a5a7756":"\n# instantiate logistic regressor\nlog = LogisticRegression()\n\n# load param grid and log model into GridSearcCV\nlogreg_cv = GridSearchCV(estimator=log, param_grid=log_params, cv=5)\n\n# fit model\nlogreg_cv.fit(X, y)","ada31c1e":"\n# Print the tuned parameters and score\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \nprint(\"Best score is {}\".format(logreg_cv.best_score_))","709eb6ef":"# predict on test set for submission\nlog_pred = logreg_cv.predict(X_test)","d3cfef57":"# create param grid object\nforrest_params = dict(\n    max_depth = [n for n in range(9, 14)],\n    min_samples_split = [n for n in range(4, 11)],\n    min_samples_leaf = [n for n in range(2, 5)],\n    n_estimators = [n for n in range(10, 60, 10)],\n)","71fc017a":"# instantiate Random Forest model\nforrest = RandomForestClassifier()","7d2a7bab":"# build and fit model\nforest_cv = GridSearchCV(estimator=forrest, param_grid=forrest_params, cv=5)\nforest_cv.fit(X, y)","e15b96c0":"# print(\"Best score: {}\".format(forest_model.best_score_))\n# print(\"Optimal params: {}\".format(forest_model.best_estimator_))\nprint(\"Best score: {}\".format(forest_cv.best_score_))\nprint(\"Optimal params: {}\".format(forest_cv.best_estimator_))","0848ddde":"# random forrest prediction on test set\nforrest_pred = forest_cv.predict(X_test)\nprint(forrest_pred)","647e44be":"# dataframe with predictions\nkaggle = pd.DataFrame( {'PassengerId': passengerId, 'Survived': forrest_pred} )","c3d7cbb7":"# ensure df is in the right format\nkaggle.head(10)","649ff3b4":"\n# save to csv\nfilename = 'submit.csv'\nkaggle.to_csv(filename, index=False)","94c2c756":"In preparation for our modeling, we will convert some of the categorical variables into numbers.","8a90362a":"\nThe social status is pretty clear on your chances of surviving.\n\nLastly, let's get a feel for some of the other metrics.","0b9cd801":"After viewing the unique Titles that were pulled, we see that we have 18 different titles but we will want to normalize these a bit so that we can generalize a bit more. To do this, we will create a dictionary that maps the 18 titles to 6 broader categories and then map that dictionary back to the Title feature.","b536bb7f":"\n**I. Data Wrangling**\n\nLike in most cases with any data science project, we are likely to encounter dirty or missing data and will need to do some wrangling before we can really do anything else.","36e2551c":"\n**IV. Modeling**\n\nNow we can begin with the modeling portion. First we need to convert the data from a dataframe to an array of numbers.","e9e16713":"\nIn this step, we concatenate all the dummy variable columns together and drop all the remaining categorical columns.","2c39d5ae":"\nIII. Feature Engineering\u00b6\nNow, let's go ahead and create a few new features from the data. The first feature we will look at building is FamilySize. This is important to look at because we want to see if having a large or small family affected someone's chances of survival.\n\nThe relevant features that will help with this are Parch (number of parents\/children aboard) and SibSp (number of siblings\/spouses aboard). We combine the Parch and SibSp features and add 1 as well as we want to count the passenger for each observation.","de2422d3":"We were able to fill in the gaps for Age and now have 1309 records.\n\nThe next feature we will take a look at is Cabin and this one will be pretty simple. We will just fill in the NaNs with \"U\" for unknown.","d956d0f0":"\n**II. Exploratory Data Analysis\u00b6\n\nNow we have a full and clean dataset that we can perform some exploratory analysis on. Probably the first thing we want to take a look at is what was the breakdown of those that died (0) vs those that survived (1) the Titanic?**","1f313068":"As expected, those passengers with a title of \"Miss\" tend to be younger than those titled \"Mrs\". Also, it looks like we have some age variability amongst the different passenger classes as well as between the sexes, so this should help us more accurately estimate the missing ages for the observations that do not have an age recorded.","8aad7383":"Additionally, from looking at the features, it looks like we can just drop PassengerId from the dataset all together since it isn't really a helpful feature, but rather simply a row identifier.","f830e87f":"Looks like the Random Forrest Classifier got us a little higher score of ~84% given the paramters we fed into the model. Given the higher result, we will use this model to run a prediction on the test set (X_test) and submit our predictions to Kaggle!","2106db40":"**Kaggle provides a test and a train dataset. The training data provides a Survived column which shows a 1 if the passenger survived and a 0 if they did not. This is ultimately the feature we are trying to predict so the test set will not have this column.**\n\n> ****> I don\u2019t like doing things twice, I first loaded the data into a train and test variable and then created a titanic variable where I appended the test to the train so that I can create new features to both data sets at the same time. I also created an index for each train and test so that I can separate them out later into their respective train and test.","efd1003d":"Logistic Regression Model\n\nThe first model we will try is a Logistic Regression model which is a binary classifier algorithm. We will be using GridSearchCV to fit our model by specifying a few paramters and return the best possible combination of those parameters.","2866242a":"\nAs we had already seen, the proportion of men to women that died was dramatically in favor of women. Less than 100 women died in comparison to well over 400 men.\n\nLet's look at the graph broken down by Pclass.","381d873b":"\nIt appears that 1st class females had an incredible 97% survival rate while 1st class males only still had a 37% chance of survival. Even though you only had a 37% chance of surviving as a 1st class male, you still were almost 3 times more likely to survive than a 3rd class male who had the lowest survival rate amongst sex and class at 13.5%.\n\nTo get a better understanding for this, let's take a look at these figures visually with some graphs. We will first take a look at the Survived feature by Sex.","578dbb68":"**The first thing I want to do is parse out the Name column and extract the title's of each person's name so we can group the data according to a person's title. This will allow us to more accurately estimate other features in the next few steps. Technically this is more of a \"feature engineering\" step but it will help out in the data wrangling process so we include it here first.**","e40b7b32":"\nLooks like only 38% of people onboard the Titanic managed to survive its fateful voyage.\n\nLet's dig a little deeper and view survival chances by sex.","9bf885ff":"Titanic Kaggle Challenege - My First Project","8961763e":"**\nRandom Forest Model**\n\nThe best score using logistic regression was ~82% which wasn't bad. But let's see how we can fare with a Random Forrest Classifier algorithm instead.","9aa88171":"The Cabin feature itself as it stands now doesn't really provide all that useful information. However, we can extract the first letter and in that way, we would have a grouped set of cabins that could potentially reveal any effect on survival.","85a0ea5a":"\nV. Prepparing for Kaggle\nLastly, we put together the PassengerId which we saved in a variable in the beginning along with out prediction results into a dataframe and export it as a csv file.","9ae702a8":"For our next step, we are going to assume that their is a relationship between a person's age and their title since it makes sense that someone that is younger is more likely to be a titled a \"Miss\" vs a \"Mrs\".\n\nWith this in mind, we will group the data by Sex, Pclass, and Title and then view the median age for the grouped classes.","52a03e30":"Finally, as our last step before modeling, we separate the combined dataset into its train and test versions once again.","0a24a872":"**\nFor those of use who are familiar with the fateful story of the Titanic or who have seen the movie, we know that women and children had priority for life boats so these numbers aren't all that surprising. Even knowing that, it is still quite astounding to see that almost 75% of women survived the sinking of the Titanic while only 19% of men did.\n\n\nFinally, let's break it down even further by Passenger Class and Sex.**","12d19863":"This shows us all the features (or columns) in the data frame along with the count of non-null values. Looking at the RangeIndex we see that there are 1309 total entries, but the Age, Cabin, Embarked, Fare, and Survived have less than that, suggesting that those columns have some null, or NaN, values. This is a dirty dataset and we either need to drop the rows with NaN values or fill in the gaps by leveraging the data in the dataset to estimate what those values could have been. We will choose the latter and try to estimate those values and fill in the gaps rather than lose observations. However, one thing to note is that the Survived feature will not require us to fill in the gaps as the count of 891 represents the labels from the train data. Remember that we are trying to predict the Survived column and so the test set does not have this column at all.\n\nEven though this is technically the \u201cData Wrangling\u201d section, before we do any data wrangling and address any missing values, I first want to create a Title feature which simply extracts the honorific from the Name feature. Simply put, an honorific is the title or rank of a given person such as \u201cMrs\u201d or \u201cMiss\u201d. The following code takes a value like \u201cBraund, Mr. Owen Harris\u201d from the Name column and extracts \u201cMr\u201d:","2c43e18f":"Then we load the test set to the variable X_test."}}