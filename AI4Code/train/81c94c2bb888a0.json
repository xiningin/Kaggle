{"cell_type":{"e73bd77f":"code","695db911":"code","50fcf263":"code","10ac4c53":"code","d8c51ffd":"code","22c30ae5":"code","1633336d":"code","0d8c6abc":"code","578b47e4":"code","4bb38c03":"code","6001ee6b":"code","ca813f5e":"code","6a036122":"code","d4ac8818":"code","b49c33be":"code","805420aa":"code","b2756f2f":"code","623fc5c0":"code","fcb7ec59":"code","32cb1710":"code","6fcd0f45":"code","fe80dc01":"code","327b2397":"code","d0308989":"code","219ca79a":"code","3057b7c4":"code","f0b99e69":"code","93740bf6":"code","e6d1ef5e":"code","120733b8":"code","3bce315e":"code","3c1e36c4":"code","39fcc192":"code","ffb73d54":"markdown","6495b6e6":"markdown","79c26c7d":"markdown","7e5fadc4":"markdown","03e75e6d":"markdown","6e805d60":"markdown","7f7c3cc5":"markdown","274f7650":"markdown","2aa399ca":"markdown","c71c1b47":"markdown","83f25d70":"markdown","0a05ae37":"markdown","15530540":"markdown","f382d6bd":"markdown","32c1773f":"markdown","3c49c50a":"markdown","98f6a4be":"markdown","db47327e":"markdown"},"source":{"e73bd77f":"from sklearn.model_selection import train_test_split\nimport pandas as pd \nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Create scaler: scaler\n#scaler = StandardScaler()","695db911":"ls ..\/input","50fcf263":"titanic = pd.read_csv('..\/input\/train.csv')\ntitanic.head(5)","10ac4c53":"titanic.groupby('Sex').Survived.value_counts()","d8c51ffd":"titanic.groupby(['Pclass','Sex']).Survived.value_counts()","22c30ae5":"id = pd.crosstab([titanic.Pclass, titanic.Sex], titanic.Survived)\nid","1633336d":"titanic.dtypes","0d8c6abc":"for cat in ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']:\n    print(\"Number of levels in category '{0}': \\b {1:2d} \".format(cat, titanic[cat].unique().size))","578b47e4":"for cat in ['Sex', 'Embarked']:\n    print(\"Levels for category '{0}': {1}\".format(cat, titanic[cat].unique()))","4bb38c03":"titanic['Sex'] = titanic['Sex'].map({'male':0,'female':1})\ntitanic['Embarked'] = titanic['Embarked'].map({'S':0,'C':1,'Q':2})","6001ee6b":"titanic = titanic.fillna(-999)\npd.isnull(titanic).any()","ca813f5e":"pd.crosstab(titanic.Embarked, titanic.Survived)","6a036122":"# specifies the parameters of our graphs\nfig = plt.figure(figsize=(18,6), dpi=1600) \nalpha=alpha_scatterplot = 0.2 \nalpha_bar_chart = 0.55\n\n# let's plot many different shaped graphs together \nax1 = plt.subplot2grid((2,3),(0,0))\n# plots a bar graph of those who surived vs those who did not.               \ntitanic.Survived.value_counts().plot(kind='bar', alpha=alpha_bar_chart)\nax1.set_xlim(-1, 2)\n# puts a title on our graph\nplt.title(\"Distribution of Survival, (1 = Survived)\")    \n\nplt.subplot2grid((2,3),(0,1))\nplt.scatter(titanic.Survived, titanic.Age, alpha=alpha_scatterplot)\n# sets the y axis lable\nplt.ylabel(\"Age\")\n# formats the grid line style of our graphs                          \nplt.grid(b=True, which='major', axis='y')  \nplt.title(\"Survival by Age,  (1 = Survived)\")\n\nax3 = plt.subplot2grid((2,3),(0,2))\ntitanic.Pclass.value_counts().plot(kind=\"barh\", alpha=alpha_bar_chart)\nax3.set_ylim(-1, len(titanic.Pclass.value_counts()))\nplt.title(\"Class Distribution\")\n\nplt.subplot2grid((2,3),(1,0), colspan=2)\n# plots a kernel density estimate of the subset of the 1st class passangers's age\ntitanic.Age[titanic.Pclass == 1].plot(kind='kde')    \ntitanic.Age[titanic.Pclass == 2].plot(kind='kde')\ntitanic.Age[titanic.Pclass == 3].plot(kind='kde')\n # plots an axis lable\nplt.xlabel(\"Age\")    \nplt.title(\"Age Distribution within classes\")\n# sets our legend for our graph.\nplt.legend(('1st Class', '2nd Class','3rd Class'),loc='best') \n\nax5 = plt.subplot2grid((2,3),(1,2))\ntitanic.Embarked.value_counts().plot(kind='bar', alpha=alpha_bar_chart)\nax5.set_xlim(-1, len(titanic.Embarked.value_counts()))\n# specifies the parameters of our graphs\nplt.title(\"Passengers per boarding location\")","d4ac8818":"titanic_new = titanic.drop(['Name','Ticket','Cabin','class'], axis=1)","b49c33be":"np.isnan(titanic_new).any()","805420aa":"titanic_new.info()","b2756f2f":"titanic_class = titanic['class'].values","623fc5c0":"training_indices, validation_indices = training_indices, testing_indices = train_test_split(titanic.index, stratify = titanic_class, train_size=0.75, test_size=0.25)\ntraining_indices.size, validation_indices.size","fcb7ec59":"tpot = TPOTClassifier(generations=999,verbosity=5,population_size=9990,n_jobs=10)\ntpot.fit(titanic_new[training_indices], titanic_class[training_indices])","32cb1710":"tpot.score(titanic_new[validation_indices], titanic.loc[validation_indices, 'class'].values)","6fcd0f45":"tpot.export('tpot_titanic_pipeline.py')","fe80dc01":"%load tpot_titanic_pipeline.py\n","327b2397":"# Read in the submission dataset\ntitanic_sub = pd.read_csv('..\/input\/test.csv')\ntitanic_sub.describe()","d0308989":"for var in ['Cabin']: #,'Name','Ticket']:\n    new = list(set(titanic_sub[var]) - set(titanic[var]))\n    titanic_sub.loc[titanic_sub[var].isin(new), var] = -999","219ca79a":"titanic_sub['Sex'] = titanic_sub['Sex'].map({'male':0,'female':1})\ntitanic_sub['Embarked'] = titanic_sub['Embarked'].map({'S':0,'C':1,'Q':2})","3057b7c4":"titanic_sub = titanic_sub.fillna(-999)\npd.isnull(titanic_sub).any()","f0b99e69":"from sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\nSubCabinTrans = mlb.fit([{str(val)} for val in titanic['Cabin'].values]).transform([{str(val)} for val in titanic_sub['Cabin'].values])\ntitanic_sub = titanic_sub.drop(['Name','Ticket','Cabin'], axis=1)","93740bf6":"# Form the new submission data set\ntitanic_sub_new = np.hstack((titanic_sub.values,SubCabinTrans))","e6d1ef5e":"np.any(np.isnan(titanic_sub_new))","120733b8":"# Ensure equal number of features in both the final training and submission dataset\nassert (titanic_new.shape[1] == titanic_sub_new.shape[1]), \"Not Equal\"","3bce315e":"# Generate the predictions\nsubmission = tpot.predict(titanic_sub_new)","3c1e36c4":"# Create the submission file\nfinal = pd.DataFrame({'PassengerId': titanic_sub['PassengerId'], 'Survived': submission})\nfinal.to_csv('submission.csv', index = False)","39fcc192":"final.shape","ffb73d54":"The data set has 5 categorical variables which contain non-numerical values: Name, Sex, Ticket, Cabin and Embarked.","6495b6e6":"To begin our analysis, we need to divide our training data into training and validation sets. The validation set is just to give us an idea of the test set error. The model selection and tuning is entirely taken care of by TPOT, so if we want to, we can skip creating this validation set.","79c26c7d":"# Data Analysis using TPOT","7e5fadc4":"## Make predictions on the submission data","03e75e6d":"Let's have a look at the generated code. As we can see, the random forest classifier performed the best on the given dataset out of all the other models that TPOT currently evaluates on. If we ran TPOT for more generations, then the score should improve further.","6e805d60":"Let's check the number of levels that each of the five categorical variables have.","7f7c3cc5":"There we go! We have successfully generated the predictions for the 418 data points in the submission dataset, and we're good to go ahead to submit these predictions on Kaggle.","274f7650":"# Data Exploration","2aa399ca":"While calling MultiLabelBinarizer for the submission data set, we first fit on the training set again to learn the levels and then transform the submission dataset values. This further ensures that only those levels that were present in the training dataset are transformed. If new levels are still found in the submission dataset then it will return an error and we need to go back and check our earlier step of replacing new levels with the placeholder value.","c71c1b47":" Sex and Embarked have few levels. Let's find out what they are.","83f25d70":"Let's code these levels manually into numerical values. For nan i.e. the missing values, let's simply replace them with a placeholder value (-999). In fact, we perform this replacement for the entire data set.","0a05ae37":"Finally we store the class labels, which we need to predict, in a separate variable.","15530540":"\n\nAfter that, we proceed to calling the fit, score and export functions on our training dataset. To get a better idea of how these functions work, refer the TPOT documentation [here](http:\/\/epistasislab.github.io\/tpot\/api\/).\n\n\n","f382d6bd":"Drop the unused features from the dataset.","32c1773f":"We then carry out the data munging steps as done earlier for the training dataset.","3c49c50a":"The most important step here is to check for new levels in the categorical variables of the submission dataset that are absent in the training set. We identify them and set them to our placeholder value of '-999', i.e., we treat them as missing values. This ensures training consistency, as otherwise the model does not know what to do with the new levels in the submission dataset.","98f6a4be":"Survived vs Embarked","db47327e":"# Data Munging"}}