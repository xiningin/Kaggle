{"cell_type":{"3ad3ac62":"code","ee2d87d5":"code","3a28f904":"code","f0b3c410":"code","656f605c":"code","e8b869a3":"code","c2968edd":"code","a437a17a":"code","d1214e61":"code","2c56ce66":"code","d3288b29":"code","ed8f6f9e":"code","453ab1aa":"code","dddb37f8":"code","8e5dc014":"code","c1b4f33f":"code","65687641":"code","59f9dcfd":"code","2a6bc029":"code","2099087c":"code","cf7f768b":"code","16b11ef5":"code","3ddbc5f5":"code","99d09e36":"code","a7353c5c":"code","c1795156":"code","ef9eda09":"code","0dc0e459":"code","d692c23a":"code","e948f23c":"code","0561db47":"code","269d1461":"code","fad15132":"markdown","dc3a0c15":"markdown","a2e05f13":"markdown","43c66a24":"markdown","d86d76d3":"markdown","61cf353d":"markdown","aa1ca5ac":"markdown","7b61ab87":"markdown","39f6326a":"markdown","7e6eff0a":"markdown","e8d82c01":"markdown","32fdff97":"markdown"},"source":{"3ad3ac62":"!pip install torchaudio > \/dev\/null\n!pip install inflect==4.1.0  > \/dev\/null\n!pip install toml > \/dev\/null\n!pip install unidecode==1.1.1 > \/dev\/null\n!pip install soundfile > \/dev\/null\n!pip install num2words==0.5.10 > \/dev\/null\n!git clone https:\/\/github.com\/NVIDIA\/DeepLearningExamples > \/dev\/null\n\ndef replace_import_apex(path):\n    !cp '{path}' '.\/tmp.py'\n    fin = open('.\/tmp.py', \"rt\")\n    fout = open(path, \"wt\")\n    for line in fin:\n        fout.write(line.replace('from apex import amp', '#from apex import amp'))\n    fin.close()\n    fout.close()\n    !rm '.\/tmp.py'\n\nreplace_import_apex('.\/DeepLearningExamples\/PyTorch\/SpeechRecognition\/Jasper\/model.py')\nreplace_import_apex('.\/DeepLearningExamples\/PyTorch\/SpeechRecognition\/Jasper\/parts\/features.py')","ee2d87d5":"import sys\nimport random\nimport num2words\nimport os\nfrom datetime import datetime\nimport time\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torchaudio\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom torch.utils.data import Dataset, DataLoader\n\n\nSEED = 42\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)","3a28f904":"marking = pd.read_csv('..\/input\/russian-numbers-asr\/train.csv')\nmarking['text_number'] = marking['number'].apply(lambda x: num2words.num2words(x, lang='ru'))\n\ndef get_stratify_group(row):\n    stratify_group = row['gender']\n    stratify_group += '_' + str(len(row['text_number'].split()))\n    return stratify_group\n\nmarking['stratify_group'] = marking.apply(get_stratify_group, axis=1)\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\nmarking.loc[:, 'fold'] = 0\nfor fold_number, (train_index, val_index) in enumerate(skf.split(X=marking.index, y=marking['stratify_group'])):\n    marking.loc[marking.iloc[val_index].index, 'fold'] = fold_number\n\nmarking.head()","f0b3c410":"import re\n\nclass CharLabels(object):\n\n    def __init__(self):\n        self.chars = [\n            ' ', '\u0430', '\u0431', '\u0432', '\u0433', '\u0434', '\u0435', '\u0451', '\u0436', '\u0437', '\u0438',\n            '\u0439', '\u043a', '\u043b', '\u043c', '\u043d', '\u043e', '\u043f', '\u0440', '\u0441', '\u0442', '\u0443',\n            '\u0444', '\u0445', '\u0446', '\u0447', '\u0448', '\u0449', '\u044a', '\u044b', '\u044c', '\u044d', '\u044e', \n            '\u044f', '<blank>',\n        ]\n        self.index = {c: i for i, c in enumerate(self.chars)}\n        self.re_compile_clean_text = re.compile(r'[^' + ''.join(self.chars[1:-1])+ r'\\s]')\n\n    def __len__(self):\n        return len(self.chars)\n\n    def __call__(self, sentence):\n        targets = []\n        for c in sentence.strip().lower():\n            targets.append(self.index[c])\n        return targets\n    \n    def postprocess_indexes(self, preds, max_repeat=1):\n        result = []\n        last_value = -1\n        repeated = 0\n        for pred in preds:\n            if pred == last_value:\n                repeated += 1\n            if pred != last_value:\n                repeated = 0\n                last_value = pred\n            if repeated >= max_repeat:\n                continue\n            result.append(pred)\n        return result\n\n    def get_text(self, indexes):\n        indexes = self.postprocess_indexes(indexes)\n        chars = []\n        for index in indexes:\n            if index < len(self.chars) - 1:\n                chars.append(self.chars[index])\n        return ''.join(chars)\n\n    def clean_text(self, text):\n        return re.sub(self.re_compile_clean_text, '', text.lower())","656f605c":"CHAR_LABELS = CharLabels()\n\nCHAR_LABELS.get_text(CHAR_LABELS(marking.iloc[0]['text_number'])), CHAR_LABELS(marking.iloc[0]['text_number'])[:10]","e8b869a3":"LABELS_COUNT = len(CHAR_LABELS)\nTRAIN_PATH = '..\/input\/russian-numbers-asr\/train-sr16k\/train-sr16k'\n\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, paths, texts, audio_augs=None):\n        super().__init__()\n        self.paths = paths\n        self.texts = texts\n        self.audio_augs = audio_augs\n\n    def __getitem__(self, idx):\n        path = self.paths[idx].split('\/')[-1]\n        waveform, sample_rate = torchaudio.load(f'{TRAIN_PATH}\/{path}', normalization=True)\n        \n        ####################\n        # Audio Augs: TODO #\n        if self.audio_augs:\n            waveform = self.audio_augs(waveform)\n        ####################\n\n        text = self.texts[idx]\n\n        return {\n            'waveform': waveform.squeeze(0),\n            'labels': torch.tensor(CHAR_LABELS(text), dtype=torch.int),\n            'text': text,\n            'path': path,\n        }\n\n    def __len__(self) -> int:\n        return self.paths.shape[0]","c2968edd":"def add_noise(waveform):\n    noise = waveform.clone().normal_(0.0, 0.005)\n    return waveform + noise","a437a17a":"fold_number = 0\n\ntrain_dataset = DatasetRetriever(\n    paths=marking[marking['fold'] != fold_number]['path'].values,\n    texts=marking[marking['fold'] != fold_number]['text_number'].values,\n    audio_augs=add_noise\n)\n\nvalidation_dataset = DatasetRetriever(\n    paths=marking[marking['fold'] == fold_number]['path'].values,\n    texts=marking[marking['fold'] == fold_number]['text_number'].values,\n)","d1214e61":"from torch.nn.utils.rnn import pad_sequence\n\ndef collate_audio(batch):\n    waveforms = []\n    labels = []\n    texts = []\n    paths = []\n    bs = len(batch)\n    waveform_sizes = torch.empty(bs, dtype=torch.int)\n    label_sizes = torch.empty(bs, dtype=torch.int)\n    for i, sample in enumerate(batch):\n        waveforms.append(sample['waveform'])\n        labels.append(sample['labels'])\n        waveform_sizes[i] = sample['waveform'].shape[0]\n        label_sizes[i] = sample['labels'].shape[0]\n        texts.append(sample['text'])\n        paths.append(sample['path'])\n\n    return {\n        'waveforms': pad_sequence(waveforms, batch_first=True),\n        'labels': pad_sequence(labels, batch_first=True, padding_value=LABELS_COUNT-1),\n        'waveform_sizes': waveform_sizes,\n        'label_sizes': label_sizes,\n        'texts': texts,\n        'paths': paths,\n    }","2c56ce66":"train_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    sampler=RandomSampler(train_dataset),\n    batch_size=16,\n    pin_memory=False,\n    drop_last=True,\n    num_workers=2,\n    collate_fn=collate_audio,\n)","d3288b29":"for batch in train_loader:\n    break\n    \nbatch['waveforms'].shape, batch['labels'].shape","ed8f6f9e":"batch['waveform_sizes'], batch['label_sizes']","453ab1aa":"batch['texts']","dddb37f8":"import sys\n\nsys.path.insert(0, '.\/DeepLearningExamples\/PyTorch\/SpeechRecognition\/Jasper')\n\nimport toml\nfrom collections import OrderedDict\nfrom model import Jasper, JasperEncoderDecoder, JasperEncoder, AudioPreprocessing, CTCLossNM\n\n\nmodel_toml = '..\/input\/pretrained-jasper10x5dr-nomask\/jasper10x5dr_nomask.toml'\nmodel_definition = toml.load(model_toml)\n\nmodel_definition['labels']['labels'] = CHAR_LABELS.chars\nmodel_definition['encoder']['convmask'] = True\nmodel_definition['normalize_transcripts'] = False\nmodel_definition['input_eval']['normalize_transcripts'] = False\nmodel_definition['input']['normalize_transcripts'] = False\n\n\nfeaturizer_config = model_definition['input_eval']\nfeaturizer_config[\"optimization_level\"] = 3","8e5dc014":"criterion = CTCLossNM(num_classes=LABELS_COUNT)\nmodel = Jasper(feature_config=featurizer_config, jasper_model_definition=model_definition, feat_in=1024, num_classes=LABELS_COUNT)","c1b4f33f":"checkpoint = torch.load(f'..\/input\/pretrained-jasper10x5dr-nomask\/jasper_fp16.pt')\npreprocessed_checkpoint = OrderedDict()\nfor key in checkpoint['state_dict'].keys():\n    if 'jasper_decoder' in key:\n        continue\n    preprocessed_checkpoint[key] = checkpoint['state_dict'][key]\n\nmodel.load_state_dict(preprocessed_checkpoint, strict=False)\nmodel = model.cuda()","65687641":"audio_preprocessor = AudioPreprocessing(**featurizer_config)","59f9dcfd":"def freeze_until(net, param_name):\n    found_name = False\n    for name, params in net.named_parameters():\n        if name == param_name:\n            found_name = True\n        params.requires_grad = found_name\n\nfreeze_until(model, 'jasper_encoder.encoder.10.conv.0.weight')","2a6bc029":"from sklearn import metrics\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","2099087c":"def levenshtein_distance(first, second):\n    \"\"\"\n    Compute Levenshtein distance between two array-like objects\n    \"\"\"\n    distance = [[0 for _ in range(len(second) + 1)] for _ in range(len(first) + 1)]\n    for i in range(len(first) + 1):\n        for j in range(len(second) + 1):\n            if i == 0:\n                distance[i][j] = j\n            elif j == 0:\n                distance[i][j] = i\n            else:\n                diag = distance[i - 1][j - 1] + (first[i - 1] != second[j - 1])\n                upper = distance[i - 1][j] + 1\n                left = distance[i][j - 1] + 1\n                distance[i][j] = min(diag, upper, left)\n    return distance[len(first)][len(second)]\n\ndef calculate_metrics(targets, outputs):\n    ser, wer = [], []\n    for output, target in zip(\n        outputs.argmax(axis=2).cpu().numpy(),\n        targets.cpu().numpy()\n    ):\n        target_text = CHAR_LABELS.get_text(target)\n        output_text = CHAR_LABELS.get_text(output)\n        ser.append(levenshtein_distance(\n            output_text, \n            target_text\n        ) \/ len(target_text))\n        wer.append(levenshtein_distance(\n            output_text.split(), \n            target_text.split(),\n        ) \/ len(target_text.split()))\n    return np.mean(ser), np.mean(wer)","cf7f768b":"import warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nclass Fitter:\n    \n    def __init__(self, model, audio_preprocessor, criterion, device, config):\n        self.config = config\n        self.epoch = 0\n        \n        self.base_dir = '.\/'\n        self.log_path = f'{self.base_dir}\/log.txt'\n        self.best_summary_loss = 10**5\n\n        self.model = model\n        self.audio_preprocessor = audio_preprocessor\n        self.criterion = criterion\n        \n        self.device = device\n\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ] \n\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n        self.log(f'Fitter prepared. Device is {self.device}')\n\n    def fit(self, train_loader, validation_loader):\n        for e in range(self.config.n_epochs):\n            if self.config.verbose:\n                lr = self.optimizer.param_groups[0]['lr']\n                timestamp = datetime.utcnow().isoformat()\n                self.log(f'\\n{timestamp}\\nLR: {lr}')\n\n            t = time.time()\n            summary_loss, summary_ser, summary_wer = self.train_one_epoch(train_loader)\n            \n            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, ser: {summary_ser.avg:.5f}, wer: {summary_wer.avg:.5f}, time: {(time.time() - t):.5f}')\n\n            t = time.time()\n            summary_loss, summary_ser, summary_wer = self.validation(validation_loader)\n\n            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, ser: {summary_ser.avg:.5f}, wer: {summary_wer.avg:.5f}, time: {(time.time() - t):.5f}')\n\n            self.model.eval()\n            self.save(f'{self.base_dir}\/last-checkpoint.bin')\n            \n            if summary_loss.avg < self.best_summary_loss:\n                self.best_summary_loss = summary_loss.avg\n                self.save(f'{self.base_dir}\/best-checkpoint.bin')\n\n            if self.config.validation_scheduler:\n                self.scheduler.step(metrics=summary_loss.avg)\n\n            self.epoch += 1\n\n    def validation(self, val_loader):\n        self.model.eval()\n        self.audio_preprocessor.eval();\n        summary_loss = AverageMeter()\n        summary_wer = AverageMeter()\n        summary_ser = AverageMeter()\n        t = time.time()\n        for step, batch in enumerate(val_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Val Step {step}\/{len(val_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, ser: {summary_ser.avg:.5f}, wer: {summary_wer.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            with torch.no_grad():\n                t_audio_signal_t = batch['waveforms'].to(self.device)\n                t_a_sig_length_t = batch['waveform_sizes'].to(self.device)\n                t_transcript_t = batch['labels'].to(self.device)\n                t_transcript_len_t = batch['label_sizes'].to(self.device)\n                \n                t_processed_signal_t, t_processed_sig_length_t = self.audio_preprocessor(t_audio_signal_t, t_a_sig_length_t)\n                t_log_probs_t, t_encoded_len_t = self.model.forward((t_processed_signal_t, t_processed_sig_length_t))\n\n                loss = self.criterion(log_probs=t_log_probs_t, targets=t_transcript_t, input_length=t_encoded_len_t, target_length=t_transcript_len_t)\n                \n                batch_size = t_a_sig_length_t.shape[0]\n                \n                ser, wer = calculate_metrics(t_transcript_t, t_log_probs_t)\n                summary_ser.update(ser, batch_size)\n                summary_wer.update(wer, batch_size)\n                summary_loss.update(loss.detach().item(), batch_size)\n\n        return summary_loss, summary_ser, summary_wer\n\n    def train_one_epoch(self, train_loader):\n        self.model.train()\n        self.audio_preprocessor.train();\n        summary_loss = AverageMeter()\n        summary_wer = AverageMeter()\n        summary_ser = AverageMeter()\n        t = time.time()\n        for step, batch in enumerate(train_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Train Step {step}\/{len(train_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, ser: {summary_ser.avg:.5f}, wer: {summary_wer.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n\n            t_audio_signal_t = batch['waveforms'].to(self.device)\n            t_a_sig_length_t = batch['waveform_sizes'].to(self.device)\n            t_transcript_t = batch['labels'].to(self.device)\n            t_transcript_len_t = batch['label_sizes'].to(self.device)\n            \n            self.optimizer.zero_grad()\n            \n            t_processed_signal_t, t_processed_sig_length_t = self.audio_preprocessor(t_audio_signal_t, t_a_sig_length_t)\n            t_log_probs_t, t_encoded_len_t = self.model.forward((t_processed_signal_t, t_processed_sig_length_t))\n            \n            loss = self.criterion(log_probs=t_log_probs_t, targets=t_transcript_t, input_length=t_encoded_len_t, target_length=t_transcript_len_t)\n\n            batch_size = t_a_sig_length_t.shape[0]\n\n            loss.backward()\n\n            self.optimizer.step()\n            \n            with torch.no_grad():\n                ser, wer = calculate_metrics(t_transcript_t, t_log_probs_t)\n                summary_ser.update(ser, batch_size)\n                summary_wer.update(wer, batch_size)\n                summary_loss.update(loss.detach().item(), batch_size)\n\n            if self.config.step_scheduler:\n                self.scheduler.step()\n\n        return summary_loss, summary_ser, summary_wer\n    \n    def save(self, path):\n        self.model.eval()\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'best_summary_loss': self.best_summary_loss,\n            'epoch': self.epoch,\n        }, path)\n\n    def load(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_summary_loss = checkpoint['best_summary_loss']\n        self.epoch = checkpoint['epoch'] + 1\n        \n    def log(self, message):\n        if self.config.verbose:\n            print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')","16b11ef5":"class TrainGlobalConfig:\n    num_workers = 2\n    batch_size = 16 \n    n_epochs = 10\n    lr = 0.001\n\n    # -------------------\n    verbose = True\n    verbose_step = 1\n    # -------------------\n\n    # --------------------\n    step_scheduler = True  # do scheduler.step after optimizer.step\n    validation_scheduler = False  # do scheduler.step after validation stage loss\n\n    SchedulerClass = torch.optim.lr_scheduler.OneCycleLR\n    scheduler_params = dict(\n        max_lr=0.001,\n        epochs=n_epochs,\n        steps_per_epoch=int(len(train_dataset) \/ batch_size),\n        pct_start=0.1,\n        anneal_strategy='cos', \n        final_div_factor=10**5\n    )\n\n#     SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n#     scheduler_params = dict(\n#         mode='min',\n#         factor=0.5,\n#         patience=1,\n#         verbose=False, \n#         threshold=0.0001,\n#         threshold_mode='abs',\n#         cooldown=0, \n#         min_lr=1e-8,\n#         eps=1e-08\n#     )\n    # --------------------","3ddbc5f5":"def run_training():\n    device = torch.device('cuda:0')\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        sampler=RandomSampler(train_dataset),\n        batch_size=TrainGlobalConfig.batch_size,\n        pin_memory=False,\n        drop_last=True,\n        num_workers=TrainGlobalConfig.num_workers,\n        collate_fn=collate_audio,\n    )\n    \n    val_loader = torch.utils.data.DataLoader(\n        validation_dataset, \n        batch_size=TrainGlobalConfig.batch_size,\n        num_workers=TrainGlobalConfig.num_workers,\n        shuffle=False,\n        sampler=SequentialSampler(validation_dataset),\n        pin_memory=False,\n        collate_fn=collate_audio,\n    )\n\n    fitter = Fitter(\n        model=model.to(device),\n        audio_preprocessor=audio_preprocessor.to(device),\n        criterion=criterion,\n        device=device, \n        config=TrainGlobalConfig,\n    )\n    fitter.fit(train_loader, val_loader)","99d09e36":"run_training()","a7353c5c":"model.load_state_dict(torch.load(f'best-checkpoint.bin')['model_state_dict'])","c1795156":"from tqdm import tqdm\n\ndef run_inference(model, audio_preprocessor, val_loader):\n    result = {\n        'reference': [],\n        'prediction': [],\n        'path': [],\n    }\n    model.eval();\n    audio_preprocessor.eval();\n    for batch in tqdm(val_loader, total=len(val_loader)):\n        with torch.no_grad():\n            t_audio_signal_t = batch['waveforms'].cuda()\n            t_a_sig_length_t = batch['waveform_sizes'].cuda()\n            result['reference'].extend(batch['texts'])\n            result['path'].extend(batch['paths'])\n\n            t_processed_signal_t, t_processed_sig_length_t = audio_preprocessor(t_audio_signal_t, t_a_sig_length_t)\n            t_log_probs_t, t_encoded_len_t = model.forward((t_processed_signal_t, t_processed_sig_length_t))\n            \n            for output in t_log_probs_t.argmax(axis=2).cpu().numpy():\n                output_text = CHAR_LABELS.get_text(output)\n                result['prediction'].append(output_text)\n\n    return pd.DataFrame(result)","ef9eda09":"val_loader = torch.utils.data.DataLoader(\n    validation_dataset, \n    batch_size=TrainGlobalConfig.batch_size,\n    num_workers=TrainGlobalConfig.num_workers,\n    shuffle=False,\n    sampler=SequentialSampler(validation_dataset),\n    pin_memory=False,\n    collate_fn=collate_audio,\n)\n\nresult = run_inference(model, audio_preprocessor, val_loader)","0dc0e459":"result.to_csv(f'prediction-{fold_number}.csv', index=False)\nresult.head()","d692c23a":"ser, wer = [], []\nfor prediction, reference in tqdm(zip(result['prediction'], result['reference']), total=result.shape[0]):\n    ser.append(levenshtein_distance(\n        prediction, \n        reference\n    ) \/ len(reference))\n    wer.append(levenshtein_distance(\n        prediction.split(), \n        reference.split(),\n    ) \/ len(reference.split()))","e948f23c":"print('-'*10 + f'fold-{fold_number}' + '-'*10)\nprint(f'[SER]: {np.mean(ser):.5f}')\nprint(f'[WER]: {np.mean(wer):.5f}')\nprint('-'*26)","0561db47":"mistakes = result[result['reference'] != result['prediction']]\nmistakes","269d1461":"import IPython.display as ipd\nipd.Audio(f'{TRAIN_PATH}\/{mistakes.iloc[-1][\"path\"]}' )","fad15132":"# Data Splitting ","dc3a0c15":"# Dataset Retriever","a2e05f13":"# Pretrained Jasper\n\n\nThe Jasper model is an end-to-end neural acoustic model for automatic speech recognition (ASR) that provides near state-of-the-art results on LibriSpeech among end-to-end ASR models without any external data. \n\nsource: https:\/\/github.com\/NVIDIA\/DeepLearningExamples\/tree\/master\/PyTorch\/SpeechRecognition\/Jasper\n\n<img src=\"https:\/\/github.com\/NVIDIA\/DeepLearningExamples\/blob\/master\/PyTorch\/SpeechRecognition\/Jasper\/images\/jasper_model.png?raw=true\" width=\"500\" align=\"left\"\/>","43c66a24":"# Training","d86d76d3":"# Char Labels","61cf353d":"# Dependencies","aa1ca5ac":"# Inference","7b61ab87":"# [Transfer Learning Jasper] Russian Numbers\n\n\nIt is a toy project about ASR model for russian language on small dataset. \n\nAuthors: [@shonenkov](https:\/\/www.kaggle.com\/shonenkov) & [@artemsolomin](https:\/\/www.kaggle.com\/artemsolomin)\n\nFor students of [Sirius](https:\/\/sochisirius.ru\/)","39f6326a":"# Thank you for reading!","7e6eff0a":"# Fold Evaluation","e8d82c01":"# Metrics","32fdff97":"Improvements with waveform augs by [@galinakaleeva](https:\/\/www.kaggle.com\/galinakaleeva), that gives best metrics (see diff version 3 & version 4)"}}