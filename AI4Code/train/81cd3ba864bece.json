{"cell_type":{"2c5ce780":"code","8343f963":"code","90c9fc65":"code","9855d991":"code","3c5c7579":"code","ba23858b":"code","3ee74bb4":"code","c8de428f":"code","49eb0e0a":"code","9ce7ffd5":"code","f3040baa":"code","12c865ff":"code","47c1048a":"code","5e5e4039":"code","752308eb":"code","2e239310":"code","daf2bc93":"code","6b679ae7":"code","db3e6044":"code","05d8669f":"code","6b268c08":"code","a7e6552c":"code","1fa3f549":"code","aaf22773":"code","6c59654f":"code","46710f85":"code","bffbc7bf":"code","d7205b61":"code","a0db876e":"code","60d00c82":"code","da5268f0":"code","27719942":"code","472126ea":"code","2fbd83b3":"code","498d31ab":"code","03afdeae":"code","155fbd97":"code","b6826236":"code","0c299a11":"code","1bc16029":"code","7a49fe3d":"code","3c463172":"code","38fe3aee":"code","08cb63be":"code","7508d5c1":"code","54d4e7eb":"code","5a53c762":"code","74af5ae4":"code","6447d97e":"code","472bd272":"code","b0ffe37c":"code","4001bdd4":"code","80205503":"code","b908feaa":"code","a26de1ff":"code","3890814a":"code","31002123":"code","5713ef79":"code","4621d697":"code","14e2a20f":"code","0e2ec49f":"code","102eb23e":"code","f370fd79":"code","425290c2":"code","dc7c9cdb":"code","4f487720":"code","d835aec3":"code","9236df27":"code","e28f246c":"code","94db27c8":"code","2b613b44":"code","b6554a6b":"markdown","5b2403be":"markdown","ba226ce6":"markdown","e66e944c":"markdown","0591b823":"markdown","d28fa207":"markdown","ae44212a":"markdown","cb79b132":"markdown","57c1c6c6":"markdown","a95670a7":"markdown","7e6242cf":"markdown","3926e188":"markdown","e11bb362":"markdown","e2bc883d":"markdown","349bfc06":"markdown","2e740f3c":"markdown","108b3c96":"markdown","008fa041":"markdown","80774377":"markdown","5db1d209":"markdown","b50c2870":"markdown","eb4fc3a0":"markdown","b561ee0a":"markdown","8a770b10":"markdown","72bea83c":"markdown","de2ba479":"markdown","d9f79a42":"markdown","ae335fcf":"markdown","0877b39d":"markdown","38d6642c":"markdown","4137efd6":"markdown","67656d61":"markdown","141ab2be":"markdown","26bec890":"markdown","fd9bfc53":"markdown","7abe1485":"markdown","8086cd51":"markdown","3a688607":"markdown","3ad224cb":"markdown","4af09d4f":"markdown","ee42744b":"markdown","fbe35c84":"markdown","d8633c0e":"markdown","85ece50e":"markdown","75f52b5d":"markdown","38ef1c45":"markdown","927bb1ba":"markdown","bbcbf1b8":"markdown","75b73fff":"markdown","e384898f":"markdown","cf59244b":"markdown"},"source":{"2c5ce780":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom collections import Counter\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n\nsns.set(style='white', context='notebook', palette='deep')\n","8343f963":"# Load data\n##### Load train and Test set\n\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nIDtest = test[\"PassengerId\"]","90c9fc65":"# Outlier detection \n\ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])","9855d991":"train.loc[Outliers_to_drop] # Show the outliers rows","3c5c7579":"# Drop outliers\ntrain = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","ba23858b":"## Join train and test datasets in order to obtain the same number of features during categorical conversion\ntrain_len = len(train)\ndataset =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","3ee74bb4":"# Fill empty and NaNs values with NaN\ndataset = dataset.fillna(np.nan)\n\n# Check for Null values\ndataset.isnull().sum()","c8de428f":"# Infos\ntrain.info()\ntrain.isnull().sum()","49eb0e0a":"train.head()","9ce7ffd5":"train.dtypes","f3040baa":"### Summarize data\n# Summarie and statistics\ntrain.describe()","12c865ff":"# Correlation matrix between numerical values (SibSp Parch Age and Fare values) and Survived \ng = sns.heatmap(train[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","47c1048a":"# Explore SibSp feature vs Survived\ng = sns.factorplot(x=\"SibSp\",y=\"Survived\",data=train,kind=\"bar\", size = 6 , \npalette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","5e5e4039":"# Explore Parch feature vs Survived\ng  = sns.factorplot(x=\"Parch\",y=\"Survived\",data=train,kind=\"bar\", size = 6 , \npalette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","752308eb":"# Explore Age vs Survived\ng = sns.FacetGrid(train, col='Survived')\ng = g.map(sns.distplot, \"Age\")\n\n","2e239310":"# Explore Age distibution \ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 0) & (train[\"Age\"].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 1) & (train[\"Age\"].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","daf2bc93":"dataset[\"Fare\"].isnull().sum()","6b679ae7":"#Fill Fare missing values with the median value\ndataset[\"Fare\"] = dataset[\"Fare\"].fillna(dataset[\"Fare\"].median())","db3e6044":"# Explore Fare distribution \ng = sns.distplot(dataset[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","05d8669f":"# Apply log to Fare to reduce skewness distribution\ndataset[\"Fare\"] = dataset[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)","6b268c08":"g = sns.distplot(dataset[\"Fare\"], color=\"b\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","a7e6552c":"g = sns.barplot(x=\"Sex\",y=\"Survived\",data=train)\ng = g.set_ylabel(\"Survival Probability\")","1fa3f549":"train[[\"Sex\",\"Survived\"]].groupby('Sex').mean()","aaf22773":"# Explore Pclass vs Survived\ng = sns.factorplot(x=\"Pclass\",y=\"Survived\",data=train,kind=\"bar\", size = 6 , \npalette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","6c59654f":"# Explore Pclass vs Survived by Sex\ng = sns.factorplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train,\n                   size=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","46710f85":"dataset[\"Embarked\"].isnull().sum()","bffbc7bf":"#Fill Embarked nan values of dataset set with 'S' most frequent value\ndataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(\"S\")","d7205b61":"# Explore Embarked vs Survived \ng = sns.factorplot(x=\"Embarked\", y=\"Survived\",  data=train,\n                   size=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","a0db876e":"# Explore Pclass vs Embarked \ng = sns.factorplot(\"Pclass\", col=\"Embarked\",  data=train,\n                   size=6, kind=\"count\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Count\")","60d00c82":"# Explore Age vs Sex, Parch , Pclass and SibSP\ng = sns.factorplot(y=\"Age\",x=\"Sex\",data=dataset,kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"Sex\",hue=\"Pclass\", data=dataset,kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"Parch\", data=dataset,kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"SibSp\", data=dataset,kind=\"box\")","da5268f0":"# convert Sex into categorical value 0 for male and 1 for female\ndataset[\"Sex\"] = dataset[\"Sex\"].map({\"male\": 0, \"female\":1})","27719942":"g = sns.heatmap(dataset[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(),cmap=\"BrBG\",annot=True)","472126ea":"# Filling missing value of Age \n\n## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n# Index of NaN age rows\nindex_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n\nfor i in index_NaN_age :\n    age_med = dataset[\"Age\"].median()\n    age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        dataset['Age'].iloc[i] = age_pred\n    else :\n        dataset['Age'].iloc[i] = age_med\n\n","2fbd83b3":"g = sns.factorplot(x=\"Survived\", y = \"Age\",data = train, kind=\"box\")\ng = sns.factorplot(x=\"Survived\", y = \"Age\",data = train, kind=\"violin\")","498d31ab":"dataset[\"Name\"].head()","03afdeae":"# Get Title from Name\ndataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\ndataset[\"Title\"] = pd.Series(dataset_title)\ndataset[\"Title\"].head()","155fbd97":"g = sns.countplot(x=\"Title\",data=dataset)\ng = plt.setp(g.get_xticklabels(), rotation=45) ","b6826236":"# Convert to categorical values Title \ndataset[\"Title\"] = dataset[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndataset[\"Title\"] = dataset[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ndataset[\"Title\"] = dataset[\"Title\"].astype(int)","0c299a11":"g = sns.countplot(dataset[\"Title\"])\ng = g.set_xticklabels([\"Master\",\"Miss\/Ms\/Mme\/Mlle\/Mrs\",\"Mr\",\"Rare\"])","1bc16029":"g = sns.factorplot(x=\"Title\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_xticklabels([\"Master\",\"Miss-Mrs\",\"Mr\",\"Rare\"])\ng = g.set_ylabels(\"survival probability\")","7a49fe3d":"# Drop Name variable\ndataset.drop(labels = [\"Name\"], axis = 1, inplace = True)","3c463172":"# Create a family size descriptor from SibSp and Parch\ndataset[\"Fsize\"] = dataset[\"SibSp\"] + dataset[\"Parch\"] + 1","38fe3aee":"g = sns.factorplot(x=\"Fsize\",y=\"Survived\",data = dataset)\ng = g.set_ylabels(\"Survival Probability\")","08cb63be":"# Create new feature of family size\ndataset['Single'] = dataset['Fsize'].map(lambda s: 1 if s == 1 else 0)\ndataset['SmallF'] = dataset['Fsize'].map(lambda s: 1 if  s == 2  else 0)\ndataset['MedF'] = dataset['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndataset['LargeF'] = dataset['Fsize'].map(lambda s: 1 if s >= 5 else 0)","7508d5c1":"g = sns.factorplot(x=\"Single\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.factorplot(x=\"SmallF\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.factorplot(x=\"MedF\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.factorplot(x=\"LargeF\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")","54d4e7eb":"# convert to indicator values Title and Embarked \ndataset = pd.get_dummies(dataset, columns = [\"Title\"])\ndataset = pd.get_dummies(dataset, columns = [\"Embarked\"], prefix=\"Em\")","5a53c762":"dataset.head()","74af5ae4":"dataset[\"Cabin\"].head()","6447d97e":"dataset[\"Cabin\"].describe()","472bd272":"dataset[\"Cabin\"].isnull().sum()","b0ffe37c":"dataset[\"Cabin\"][dataset[\"Cabin\"].notnull()].head()","4001bdd4":"# Replace the Cabin number by the type of cabin 'X' if not\ndataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin'] ])","80205503":"g = sns.countplot(dataset[\"Cabin\"],order=['A','B','C','D','E','F','G','T','X'])","b908feaa":"g = sns.factorplot(y=\"Survived\",x=\"Cabin\",data=dataset,kind=\"bar\",order=['A','B','C','D','E','F','G','T','X'])\ng = g.set_ylabels(\"Survival Probability\")\n","a26de1ff":"dataset = pd.get_dummies(dataset, columns = [\"Cabin\"],prefix=\"Cabin\")","3890814a":"dataset[\"Ticket\"].head()","31002123":"## Treat Ticket by extracting the ticket prefix. When there is no prefix it returns X. \n\nTicket = []\nfor i in list(dataset.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket.append(\"X\")\n        \ndataset[\"Ticket\"] = Ticket\ndataset[\"Ticket\"].head()\n","5713ef79":"dataset = pd.get_dummies(dataset, columns = [\"Ticket\"], prefix=\"T\")","4621d697":"# Create categorical values for Pclass\ndataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\ndataset = pd.get_dummies(dataset, columns = [\"Pclass\"],prefix=\"Pc\")","14e2a20f":"# Drop useless variables \ndataset.drop(labels = [\"PassengerId\"], axis = 1, inplace = True)","0e2ec49f":"dataset.head()","102eb23e":"#Let's separate the train dataset and test\n\ntrain_data = dataset[:train_len]\ntest_data = dataset[train_len:]\ntest_data.drop(labels=[\"Survived\"],axis = 1,inplace=True)","f370fd79":"## Separate train features and label \n\ntrain_data[\"Survived\"] = train_data[\"Survived\"].astype(int)\n\n#now, let's convert the data to numpy arrays\n#we could use the pandas as_matrix() method, but we received a warning\n#FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n#x_test = test_data.as_matrix()\n#x_train = train_data.drop(['label'], axis=1).as_matrix()\n#y_train = train_data['label'].as_matrix()\n\nx_test = test_data.values\nx_train = train_data.drop(labels = [\"Survived\"],axis = 1).values\ny_train = train_data[\"Survived\"].values\n\nx_test.shape, x_train.shape, y_train.shape","425290c2":"# We split the known data into train and validation sets\n#let's choose 20% for validation set\nfrom sklearn.model_selection import train_test_split\nrandom_seed = 23\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.11, random_state=random_seed)","dc7c9cdb":"import xgboost as xgb\ndef xgb_model(train_data, train_label, test_data, test_label):\n    clf = xgb.XGBClassifier(learning_rate=0.1,\n                            max_depth=8,\n                            min_child_weight=2,\n                            n_estimators=100,\n                            nthread=1,\n                            subsample=0.6)#0.6000000000000001\n    clf.fit(train_data, train_label, eval_metric='auc', verbose=True,\n            eval_set=[(test_data, test_label)], early_stopping_rounds=100)\n    y_pre = clf.predict(test_data)\n\n    return clf\n\n","4f487720":"print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)","d835aec3":"clf=xgb_model(x_train,y_train,x_val, y_val)","9236df27":"y_test = clf.predict(x_test)\nprint(y_test.shape)","e28f246c":"y_test","94db27c8":"x_test.shape","2b613b44":"y_test = pd.Series(clf.predict(x_test), name=\"Survived\")\n\nresults = pd.concat([IDtest,y_test],axis=1)\n\nresults.to_csv(\"predictions.csv\",index=False)","b6554a6b":"### 5.3 Cabin","5b2403be":"# Titanic Top 4% with ensemble modeling\n### **Yassine Ghouzam, PhD**\n#### 13\/07\/2017\n\n* **1 Introduction**\n* **2 Load and check data**\n    * 2.1 load data\n    * 2.2 Outlier detection\n    * 2.3 joining train and test set\n    * 2.4 check for null and missing values\n* **3 Feature analysis**\n    * 3.1 Numerical values\n    * 3.2 Categorical values\n* **4 Filling missing Values**\n    * 4.1 Age\n* **5 Feature engineering**\n    * 5.1 Name\/Title\n    * 5.2 Family Size\n    * 5.3 Cabin\n    * 5.4 Ticket\n* **6 Modeling**\n    * 6.1 Simple modeling\n        * 6.1.1 Cross validate models\n        * 6.1.2 Hyperparamater tunning for best models\n        * 6.1.3 Plot learning curves\n        * 6.1.4 Feature importance of the tree based classifiers\n    * 6.2 Ensemble modeling\n        * 6.2.1 Combining models\n    * 6.3 Prediction\n        * 6.3.1 Predict and Submit results\n    ","ba226ce6":"Since we have one missing value , i decided to fill it with the median value which will not have an important effect on the prediction.","e66e944c":"We detect 10 outliers. The 28, 89 and 342 passenger have an high Ticket Fare \n\nThe 7 others have very high values of SibSP.","0591b823":"Skewness is clearly reduced after the log transformation","d28fa207":"It seems that passengers having a lot of siblings\/spouses have less chance to survive\n\nSingle passengers (0 SibSP) or with two other persons (SibSP 1 or 2) have more chance to survive\n\nThis observation is quite interesting, we can consider a new feature describing these categories (See feature engineering)","ae44212a":"## 2. Load and check data\n### 2.1 Load data","cb79b132":"Factorplots of family size categories show that Small and Medium families have more chance to survive than single passenger and large families.","57c1c6c6":"No difference between median value of age in survived and not survived subpopulation. \n\nBut in the violin plot of survived passengers, we still notice that very young passengers have higher survival rate.","a95670a7":"## 4. Filling missing Values\n### 4.1 Age\n\nAs we see, Age column contains 256 missing values in the whole dataset.\n\nSince there is subpopulations that have more chance to survive (children for example), it is preferable to keep the age feature and to impute the missing values. \n\nTo adress this problem, i looked at the most correlated features with Age (Sex, Parch , Pclass and SibSP).","7e6242cf":"It seems that passenger coming from Cherbourg (C) have more chance to survive.\n\nMy hypothesis is that the proportion of first class passengers is higher for those who came from Cherbourg than Queenstown (Q), Southampton (S).\n\nLet's see the Pclass distribution vs Embarked","3926e188":"Only Fare feature seems to have a significative correlation with the survival probability.\n\nIt doesn't mean that the other features are not usefull. Subpopulations in these features can be correlated with the survival. To determine this, we need to explore in detail these features","e11bb362":"### 2.4 check for null and missing values","e2bc883d":"### 2.2 Outlier detection","349bfc06":"Age distribution seems to be a tailed distribution, maybe a gaussian distribution.\n\nWe notice that age distributions are not the same in the survived and not survived subpopulations. Indeed, there is a peak corresponding to young passengers, that have survived. We also see that passengers between 60-80 have less survived. \n\nSo, even if \"Age\" is not correlated with \"Survived\", we can see that there is age categories of passengers that of have more or less chance to survive.\n\nIt seems that very young passengers have more chance to survive.","2e740f3c":"# **NOTE:** I have forked the Yassine Ghouzam kerner, in order to use his data preprocessing analysis","108b3c96":"It is clearly obvious that Male have less chance to survive than Female.\n\nSo Sex, might play an important role in the prediction of the survival.\n\nFor those who have seen the Titanic movie (1997), I am sure, we all remember this sentence during the evacuation : \"Women and children first\". ","008fa041":"Indeed, the third class is the most frequent for passenger coming from Southampton (S) and Queenstown (Q), whereas Cherbourg passengers are mostly in first class which have the highest survival rate.\n\nAt this point, i can't explain why first class has an higher survival rate. My hypothesis is that first class passengers were prioritised during the evacuation due to their influence.","80774377":"#### Age","5db1d209":"## 6. MODELING: Beginning of own ML architecture","b50c2870":"As we can see, Fare distribution is very skewed. This can lead to overweigth very high values in the model, even if it is scaled. \n\nIn this case, it is better to transform it with the log function to reduce this skew. ","eb4fc3a0":"Age and Cabin features have an important part of missing values.\n\n**Survived missing values correspond to the join testing dataset (Survived column doesn't exist in test set and has been replace by NaN values when concatenating the train and test set)**","b561ee0a":"I join train and test datasets to obtain the same number of features during categorical conversion (See feature engineering).","8a770b10":"When we superimpose the two densities , we cleary see a peak correponsing (between 0 and 5) to babies and very young childrens.","72bea83c":"\"Women and children first\" \n\nIt is interesting to note that passengers with rare title have more chance to survive.","de2ba479":"The correlation map confirms the factorplots observations except for Parch. Age is not correlated with Sex, but is negatively correlated with Pclass, Parch and SibSp.\n\nIn the plot of Age in function of Parch, Age is growing with the number of parents \/ children. But the general correlation is negative.\n\nSo, i decided to use SibSP, Parch and Pclass in order to impute the missing ages.\n\nThe strategy is to fill Age with the median age of similar rows according to Pclass, Parch and SibSp.","d9f79a42":"There is 17 titles in the dataset, most of them are very rare and we can group them in 4 categories.","ae335fcf":"The Name feature contains information on passenger's title.\n\nSince some passenger with distingused title may be preferred during the evacuation, it is interesting to add them to the model.","0877b39d":"#### Fare","38d6642c":"### 5.4 Ticket","4137efd6":"The first letter of the cabin indicates the Desk, i choosed to keep this information only, since it indicates the probable location of the passenger in the Titanic.","67656d61":"### 3.2 Categorical values\n#### Sex","141ab2be":"## 3. Feature analysis\n### 3.1 Numerical values","26bec890":"Because of the low number of passenger that have a cabin, survival probabilities have an important standard deviation and we can't distinguish between survival probability of passengers in the different desks. \n\nBut we can see that passengers with a cabin have generally more chance to survive than passengers without (X).\n\nIt is particularly true for cabin B, C, D, E and F.","fd9bfc53":"#### SibSP","7abe1485":"The passenger survival is not the same in the 3 classes. First class passengers have more chance to survive than second class and third class passengers.\n\nThis trend is conserved when we look at both male and female passengers.","8086cd51":"## 1. Introduction\n\nThis is my first kernel at Kaggle. I choosed the Titanic competition which is a good way to introduce feature engineering and ensemble modeling. Firstly, I will display some feature analyses then ill focus on the feature engineering. Last part concerns modeling and predicting the survival on the Titanic using an voting procedure. \n\nThis script follows three main parts:\n\n* **Feature analysis**\n* **Feature engineering**\n* **Modeling**","3a688607":"It could mean that tickets sharing the same prefixes could be booked for cabins placed together. It could therefore lead to the actual placement of the cabins within the ship.\n\nTickets with same prefixes may have a similar class and survival.\n\nSo i decided to replace the Ticket feature column by the ticket prefixe. Which may be more informative.","3ad224cb":"#### Pclass","4af09d4f":"#### Parch","ee42744b":"The family size seems to play an important role, survival probability is worst for large families.\n\nAdditionally, i decided to created 4 categories of family size.","fbe35c84":"### 5.2 Family size\n\nWe can imagine that large families will have more difficulties to evacuate, looking for theirs sisters\/brothers\/parents during the evacuation. So, i choosed to create a \"Fize\" (family size) feature which is the sum of SibSp , Parch and 1 (including the passenger).","d8633c0e":"At this stage, we have 22 features.","85ece50e":"The Cabin feature column contains 292 values and 1007 missing values.\n\nI supposed that passengers without a cabin have a missing value displayed instead of the cabin number.","75f52b5d":"Since we have two missing values , i decided to fill them with the most fequent value of \"Embarked\" (S).","38ef1c45":"Age distribution seems to be the same in Male and Female subpopulations, so Sex is not informative to predict Age.\n\nHowever, 1rst class passengers are older than 2nd class passengers who are also older than 3rd class passengers.\n\nMoreover, the more a passenger has parents\/children the older he is and the more a passenger has siblings\/spouses the younger he is.","927bb1ba":"### 2.3 joining train and test set","bbcbf1b8":"Small families have more chance to survive, more than single (Parch 0), medium (Parch 3,4) and large families (Parch 5,6 ).\n\nBe carefull there is an important standard deviation in the survival of passengers with 3 parents\/children ","75b73fff":"#### Embarked","e384898f":"## 5. Feature engineering\n### 5.1 Name\/Title","cf59244b":"Since outliers can have a dramatic effect on the prediction (espacially for regression problems), i choosed to manage them. \n\nI used the Tukey method (Tukey JW., 1977) to detect ouliers which defines an interquartile range comprised between the 1st and 3rd quartile of the distribution values (IQR). An outlier is a row that have a feature value outside the (IQR +- an outlier step).\n\n\nI decided to detect outliers from the numerical values features (Age, SibSp, Sarch and Fare). Then, i considered outliers as rows that have at least two outlied numerical values."}}