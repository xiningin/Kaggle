{"cell_type":{"a49e4cdc":"code","35ecba74":"code","70221350":"code","9ce69d35":"code","bcf71f3a":"code","bac5a8e1":"code","7b3385e5":"code","92995e1c":"code","813721bc":"code","7569a16e":"code","1817e83c":"code","27e8f13d":"code","09291569":"code","ca19f117":"code","07f5d889":"code","0ff31192":"code","cd94ba96":"code","78e26511":"code","06d9c8d1":"code","4f89496d":"code","cd440c4c":"code","b90b3b38":"code","27054f46":"code","45bcffbe":"code","269b6af1":"code","a3df6fce":"code","0afb4fb0":"code","b36de85d":"code","8f955c78":"markdown","5407bcea":"markdown","7f668378":"markdown","e7bd49ae":"markdown","e55fd036":"markdown","fea21544":"markdown","0b9598e7":"markdown","d598c2d3":"markdown","9b2d3e02":"markdown","3687c85d":"markdown","3fd1582c":"markdown","4b0fca47":"markdown","92b65f7c":"markdown","db9b2fd4":"markdown","2c1742c8":"markdown","11a505e8":"markdown","b7597384":"markdown","013d0eb3":"markdown","ae9c2026":"markdown","c8d86a39":"markdown","277a8c86":"markdown","efc77988":"markdown","e3cc9b3a":"markdown"},"source":{"a49e4cdc":"import pandas as pd\nimport numpy as np","35ecba74":"import matplotlib.pyplot as plt","70221350":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier","9ce69d35":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import precision_recall_fscore_support","bcf71f3a":"from sklearn.model_selection import train_test_split","bac5a8e1":"from sklearn.preprocessing import StandardScaler as ss\nfrom sklearn.decomposition import PCA","7b3385e5":"data = pd.read_csv(\"..\/input\/data.csv\")\ndata.info()","92995e1c":"data.head()","813721bc":"data.isnull().sum()","7569a16e":"data.drop(['id','Unnamed: 32'], axis=1, inplace=True)","1817e83c":"y = data.diagnosis\ny.unique()\n\nX = data.drop(['diagnosis'], axis=1)","27e8f13d":"y = y.map(lambda x: 1 if x=='M' else 0)\ny.tail(10)","09291569":"scaler = ss()\nscaled = scaler.fit_transform(X)\n","ca19f117":"pca = PCA()\nx_pca = pca.fit_transform(scaled)\n\npca.explained_variance_ratio_.cumsum()","07f5d889":"X = scaled[:, 0:10]","0ff31192":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,\n                                                    shuffle = True)\n","cd94ba96":"dt = DecisionTreeClassifier()\nrf = RandomForestClassifier(n_estimators=100)\nxg = XGBClassifier()\ngbm = GradientBoostingClassifier()\netc = ExtraTreesClassifier(n_estimators=100)\nknc = KNeighborsClassifier()","78e26511":"dt1 = dt.fit(x_train,y_train)\nrf1 = rf.fit(x_train,y_train)\nxg1 = xg.fit(x_train,y_train)\ngbm1 = gbm.fit(x_train,y_train)\netc1 = etc.fit(x_train,y_train)\nknc1 = knc.fit(x_train,y_train)","06d9c8d1":"y_pred_dt = dt1.predict(x_test)\ny_pred_rf = rf1.predict(x_test)\ny_pred_xg= xg1.predict(x_test)\ny_pred_gbm= gbm1.predict(x_test)\ny_pred_etc= etc1.predict(x_test)\ny_pred_knc= knc1.predict(x_test)","4f89496d":"y_pred_dt_prob = dt1.predict_proba(x_test)\ny_pred_rf_prob = rf1.predict_proba(x_test)\ny_pred_xg_prob = xg1.predict_proba(x_test)\ny_pred_gbm_prob= gbm1.predict_proba(x_test)\ny_pred_etc_prob = etc1.predict_proba(x_test)\ny_pred_knc_prob= knc1.predict_proba(x_test)\n","cd440c4c":"cm_dt = confusion_matrix(y_test,y_pred_dt)\ncm_rf = confusion_matrix(y_test,y_pred_rf)\ncm_xg = confusion_matrix(y_test,y_pred_xg)\ncm_gbm = confusion_matrix(y_test,y_pred_gbm)\ncm_etc = confusion_matrix(y_test,y_pred_etc)\ncm_knc = confusion_matrix(y_test,y_pred_knc)\n\ncms = [cm_dt, cm_rf, cm_xg, cm_gbm, cm_etc, cm_knc]\nclassifiers = [\"Decision Tree\",\"Random Forest\",\"XGBoost\",\"GBM\",\"Exrta Trees\",\"KNeighbors\"]\n\ndef plot_confusion_matrix(cms):\n   \n    fig = plt.figure(figsize=(16,12))\n    plt.subplots_adjust( hspace=0.5, wspace=0.1)\n    for i in range(6):\n        j = i+1\n        ax = fig.add_subplot(2,3,j)\n        plt.imshow(cms[i], interpolation='nearest', cmap=plt.cm.Purples)\n\n        classNames = ['B','M']\n\n        plt.ylabel('Actual', size='large')\n\n        plt.xlabel('Predicted', size='large')\n\n        tick_marks = np.arange(len(classNames))\n        plt.xticks(tick_marks, classNames, size='x-large')\n\n        plt.yticks(tick_marks, classNames, size='x-large')\n\n        s = [['TN','FP'], ['FN', 'TP']]\n    \n    \n        plt.text(-0.23,0.05, str(s[0][0])+\" = \"+str(cms[i][0][0]), size='x-large')\n        plt.text(0.8,0.05, str(s[0][1])+\" = \"+str(cms[i][0][1]), size='x-large')\n        plt.text(-0.23,1.05, str(s[1][0])+\" = \"+str(cms[i][1][0]), size='x-large')\n        plt.text(0.8,1.05, str(s[1][1])+\" = \"+str(cms[i][1][1]), size='x-large')\n        plt.title(classifiers[i], fontsize=15)\n\nplot_confusion_matrix(cms)\n","b90b3b38":"print(accuracy_score(y_test,y_pred_dt))\nprint(accuracy_score(y_test,y_pred_rf))\nprint(accuracy_score(y_test,y_pred_xg))\nprint(accuracy_score(y_test,y_pred_gbm))\nprint(accuracy_score(y_test,y_pred_etc))\nprint(accuracy_score(y_test,y_pred_knc))","27054f46":"print(accuracy_score(y_test,y_pred_dt))\nprint(accuracy_score(y_test,y_pred_rf))\nprint(accuracy_score(y_test,y_pred_xg))\nprint(accuracy_score(y_test,y_pred_gbm))\nprint(accuracy_score(y_test,y_pred_etc))\nprint(accuracy_score(y_test,y_pred_knc))","45bcffbe":"precision_recall_fscore_support(y_test, y_pred_dt)\nprecision_recall_fscore_support(y_test, y_pred_rf)\nprecision_recall_fscore_support(y_test, y_pred_xg)\nprecision_recall_fscore_support(y_test, y_pred_gbm)\nprecision_recall_fscore_support(y_test, y_pred_etc)\nprecision_recall_fscore_support(y_test, y_pred_knc)","269b6af1":"fpr_dt, tpr_dt, thresholds = roc_curve(y_test,\n                                 y_pred_dt_prob[: , 1],\n                                 pos_label= 1\n                                 )\n\nfpr_rf, tpr_rf, thresholds = roc_curve(y_test,\n                                 y_pred_rf_prob[: , 1],\n                                 pos_label= 1\n                                 )\n\nfpr_xg, tpr_xg, thresholds = roc_curve(y_test,\n                                 y_pred_xg_prob[: , 1],\n                                 pos_label= 1\n                                 )\n\nfpr_gbm, tpr_gbm,thresholds = roc_curve(y_test,\n                                 y_pred_gbm_prob[: , 1],\n                                 pos_label= 1\n                                 )\n\nfpr_etc, tpr_etc,thresholds = roc_curve(y_test,\n                                 y_pred_etc_prob[: , 1],\n                                 pos_label= 1\n                                 )\n\nfpr_knc, tpr_knc,thresholds = roc_curve(y_test,\n                                 y_pred_knc_prob[: , 1],\n                                 pos_label= 1\n                                 )","a3df6fce":"print(\"Decision Tree: \",auc(fpr_dt,tpr_dt))\nprint(\"Random Forest: \",auc(fpr_rf,tpr_rf))\nprint(\"GBM: \",auc(fpr_gbm,tpr_gbm))\nprint(\"XGBoost: \",auc(fpr_xg,tpr_xg))\nprint(\"Extra Trees : \",auc(fpr_etc,tpr_etc))\nprint(\"KNeighbors: \",auc(fpr_knc,tpr_knc))\n","0afb4fb0":"fig = plt.figure(figsize=(12,10))   # Create window frame\nax = fig.add_subplot(111)   # Create axes\n\n#8.1 Connect diagonals\nax.plot([0, 1], [0, 1], ls=\"--\")  # Dashed diagonal line\n\n#8.2 Labels \nax.set_xlabel('False Positive Rate')  # Final plot decorations\nax.set_ylabel('True Positive Rate')\nax.set_title('ROC curve for models')\n\n#8.3 Set graph limits\nax.set_xlim([0.0, 1.0])\nax.set_ylim([0.0, 1.0])\n\n#8.4 Plot each graph now\nax.plot(fpr_dt, tpr_dt, label = \"dt\")\nax.plot(fpr_rf, tpr_rf, label = \"rf\")\nax.plot(fpr_xg, tpr_xg, label = \"xg\")\nax.plot(fpr_gbm, tpr_gbm, label = \"gbm\")\nax.plot(fpr_etc, tpr_etc, label = \"etc\")\nax.plot(fpr_knc, tpr_knc, label = \"knc\")\n\n#8.5 Set legend and show plot\nax.legend(loc=\"lower right\")\nplt.show()","b36de85d":"performance = pd.DataFrame({ \"classifiers\":[\"dt\",\"rf\",'xg','gbm','etc','knc'],\n\"Accuracy\": [accuracy_score(y_test,y_pred_dt),accuracy_score(y_test,y_pred_rf),accuracy_score(y_test,y_pred_xg),accuracy_score(y_test,y_pred_gbm),accuracy_score(y_test,y_pred_etc),accuracy_score(y_test,y_pred_knc)],\n\"Precision\": [precision_score(y_test,y_pred_dt),precision_score(y_test,y_pred_rf),precision_score(y_test,y_pred_xg),precision_score(y_test,y_pred_gbm),precision_score(y_test,y_pred_etc),precision_score(y_test,y_pred_knc)],\n\"AUC\":[auc(fpr_dt,tpr_dt),auc(fpr_rf,tpr_rf),auc(fpr_xg,tpr_xg),auc(fpr_gbm,tpr_gbm),auc(fpr_etc,tpr_etc),auc(fpr_knc,tpr_knc)],\n\"Recall\":[recall_score(y_test,y_pred_dt),recall_score(y_test,y_pred_rf),recall_score(y_test,y_pred_xg),recall_score(y_test,y_pred_gbm),recall_score(y_test,y_pred_etc),recall_score(y_test,y_pred_knc)],\n\"f1_score\":[f1_score(y_test,y_pred_dt),f1_score(y_test,y_pred_rf),f1_score(y_test,y_pred_xg),f1_score(y_test,y_pred_gbm),f1_score(y_test,y_pred_etc),f1_score(y_test,y_pred_knc)]})\n\n\nplt.clf()\n\nvals= np.array(performance.iloc[0, 1:].values.tolist())\nfor i in range(1,6):\n    vals = np.append(vals,performance.iloc[i, 1:].values.tolist())\n\n\nbarWidth = 0.15\nplt.figure(figsize=(16,6))\n# set height of bar\nbars1 = performance.Accuracy\nbars2 = performance.Precision\nbars3 = performance.Recall\nbars4 = performance.AUC\nbars5 = performance.f1_score\n \n# Set position of bar on X axis\nr1 = np.arange(len(bars1))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\nr4 = [x + barWidth for x in r3]\nr5 = [x + barWidth for x in r4]\n \n# Make the plot\nplt.bar(r1, bars1, color='lightgreen', width=barWidth, edgecolor='white', label='Accuracy')\nplt.bar(r2, bars2, color='lightblue', width=barWidth, edgecolor='white', label='Precision')\nplt.bar(r3, bars3, color='pink', width=barWidth, edgecolor='white', label='Recall')\nplt.bar(r4, bars4, color='orange', width=barWidth, edgecolor='white', label='AUC')\nplt.bar(r5, bars5, color='yellow', width=barWidth, edgecolor='white', label='F1 Score')\n \n# Add xticks on the middle of the group bars\nplt.xlabel('Performance Metrics', fontweight='bold', fontsize=20)\nplt.xticks([r + barWidth +0.1 for r in range(len(bars1))], ['DT', 'RF', 'XG', 'GBP', 'ETC', 'KNC'],fontweight='bold', fontsize=10)\n\n# Create legend & Show graphic\nplt.legend()\nplt.show()\n\n","8f955c78":"### <font color=purple>1.4 For performance measures<\/font>","5407bcea":"### <font color=purple>4.2 Get probability values<\/font>","7f668378":"### <font color=purple>2.4 Map values in ' y ' (target) from 'M' and 'B' to 1 and 0<\/font>","e7bd49ae":"### Read, analyse and preprocess dataset. Then split it into train and test datasets.\n### <font color=purple>2.1 Read dataset<\/font>","e55fd036":"### <font color=purple>2.2 Drop any column not needed<\/font>","fea21544":"### <font color=purple>2.5 Scale all numerical features in X  using sklearn's StandardScaler class<\/font>","0b9598e7":"### <font color=purple>3.2 Train the models.<\/font>","d598c2d3":"### <font color=purple>1.5 For splitting the data<\/font>","9b2d3e02":"### <font color=purple>5.1 Draw Confusion matrix<\/font>\n","3687c85d":"### Calculate the performance of each of these models by calculating:\n#### a) Accuracy,<br>b) Precision & Recall,<br>c) F1 score,<br>d) AUC\n### <font color=purple>6.1 Calculate accuracy<\/font>","3fd1582c":"### Importing Libraries\n### <font color=purple>1.1 For reading and manipulating data<\/font>","4b0fca47":"### <font color=purple>7.0 ROC data.<\/font>","92b65f7c":"### <font color='purple'>8.1 Plot ROC curve<\/font>","db9b2fd4":"### Perform modeling on (x_train, y_train) using -\n#### RandomForestClassifier,<br>GradientBoostingClassifier,<br>XGBClassifier,<br>DecisionTreeClassifier,<br>ExtraTreesClassifier,<br>KNeighborsClassifier\t\t\t\t\t\t\t\t\t\t\t\t\n### <font color=purple>3.1 Create default classifiers<\/font>","2c1742c8":"### <font color=purple>2.7 Spliting X, y into train and test datasets in the ratio of 80:20 using sklearn's train_test_split function. We get: x_train, x_test, y_train, y_test.<\/font>","11a505e8":"### <font color=purple>2.3 Segregate dataset into predictors (X) and target (y)<\/font>","b7597384":"### <font color=purple>1.2 For plotting<\/font>","013d0eb3":"### <font color=purple>1.3 For modelling<\/font>","ae9c2026":"### <font color=purple>4.1 Make predictions on test (x_test) for each one of the models. Compare the output of predictions in each case with actual (y_test)<\/font>","c8d86a39":"### <font color=purple>6.2 Calculate precision, recall and F1 score<\/font>","277a8c86":"### <font color=purple>1.6 For data preprocessing<\/font>","efc77988":"### <font color=purple>2.6 Performing PCA on numeric features, X. Using sklearn's PCA class. Only retaining as many principal components (PCs) as explain 95% variance. <\/font>","e3cc9b3a":"### <font color=purple>6.3 Get AUC values.<\/font>"}}