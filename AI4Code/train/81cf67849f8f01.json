{"cell_type":{"00ceaa16":"code","3465b129":"code","26d2b2a2":"code","323b57d6":"code","16fb0a58":"code","ddb09de3":"code","53ac11db":"code","cde8c5a0":"code","f7bb5dcb":"code","6a1e08ad":"code","feb1abc8":"code","f2fb0eb1":"code","0d92b13d":"code","f2cd445e":"code","a542f3e8":"code","3fb614f3":"code","35ecdcab":"code","cc54a877":"code","54be0222":"code","a6524e91":"code","9143ff4e":"code","28f02e38":"code","fd69c016":"code","17fdfcc6":"code","e1da1c67":"code","9ce5f97f":"code","06fba80c":"code","1a04bc1d":"code","e8de5cd7":"code","5ce6f673":"code","d85ba2a3":"code","a17753d1":"code","5d8a790a":"code","88914e65":"code","f91effd0":"code","b61d467c":"code","5ce9d000":"code","3e235649":"code","fdd3bc10":"code","0b78e33c":"code","7a7b1cf1":"code","690c31bb":"code","83dafb39":"code","d85ce64f":"code","90549d70":"code","37b53252":"code","f38839ea":"code","c61e3907":"code","4d8d8ed7":"code","71f9002f":"code","6ee7384d":"markdown","4b263233":"markdown","bc691683":"markdown","9155f976":"markdown"},"source":{"00ceaa16":"%config Completer.use_jedi = False","3465b129":"import numpy as np\nimport pandas as pd\nimport os\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold","26d2b2a2":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport transformers\nimport random\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nscaler = torch.cuda.amp.GradScaler()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndevice","323b57d6":"SEED = 20210520\n\ndef random_seed():\n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True","16fb0a58":"train = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","ddb09de3":"train.head(7)","53ac11db":"test","cde8c5a0":"tokenizer = transformers.BertTokenizer.from_pretrained('..\/input\/huggingface-bert\/bert-base-uncased\/')","f7bb5dcb":"test['excerpt'].iloc[0]","6a1e08ad":"sentence_length = []\n\nfor sentence in tqdm(train['excerpt']):\n    token_words = tokenizer.encode_plus(sentence)['input_ids']\n    sentence_length.append(len(token_words))\n    \nprint('maxlength:', max(sentence_length))","feb1abc8":"sample_tokenize = tokenizer.encode_plus(train['excerpt'][0])\nsample_tokenize","f2fb0eb1":"tokenizer.decode(sample_tokenize['input_ids'])","0d92b13d":"pad_sentence = tokenizer.encode_plus(\n    train['excerpt'][0],\n    add_special_tokens = True,\n    max_length = 314,\n    pad_to_max_length = True,\n    truncation = True\n)","f2cd445e":"tokenizer.decode(pad_sentence['input_ids'])","a542f3e8":"train_data = train.sort_values('target').reset_index(drop=True)\ntrain_data","3fb614f3":"train_data['kfold'] = train_data.index % 5\ntrain_data","35ecdcab":"p_train_data = train_data[train_data['kfold']!=0].reset_index(drop=True)\np_val_data = train_data[train_data['kfold']==0].reset_index(drop=True)","cc54a877":"class BERTDataset(Dataset):\n    def __init__(self, sentences, targets):\n        self.sentences = sentences\n        self.targets = targets\n        \n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, idx):\n        sentence = self.sentences[idx]\n        \n        encode_sentence = tokenizer.encode_plus(\n                                sentence,\n                                add_special_tokens = True,\n                                max_length = 314,\n                                pad_to_max_length = True,\n                                truncation = True,\n                                return_attention_mask = True\n                        )\n        \n        ids = torch.tensor(encode_sentence['input_ids'], dtype=torch.long)\n        mask = torch.tensor(encode_sentence['attention_mask'], dtype=torch.long)\n        token_type_ids = torch.tensor(encode_sentence['token_type_ids'], dtype=torch.long)\n        \n        target = torch.tensor(self.targets[idx], dtype=torch.float)\n        \n        return {\n            'ids': ids,\n            'mask': mask,\n            'token_type_ids': token_type_ids,\n            'targets': target\n        }","54be0222":"train_dataset = BERTDataset(p_train_data['excerpt'], p_train_data['target'])\nval_dataset = BERTDataset(p_val_data['excerpt'], p_val_data['target'])","a6524e91":"train_dataset[0]","9143ff4e":"train_batch = 8\nval_batch = 32","28f02e38":"train_dataloader = DataLoader(train_dataset, batch_size=train_batch, shuffle=True, num_workers=8, pin_memory=True)\nval_dataloader = DataLoader(val_dataset, batch_size=val_batch, shuffle=False, num_workers=8, pin_memory=True)\n\nfor a in train_dataloader:\n    print(a)\n    break","fd69c016":"model = transformers.BertForSequenceClassification.from_pretrained('..\/input\/huggingface-bert\/bert-base-uncased\/', num_labels=1)\nmodel.to(device)","17fdfcc6":"for train_encode in train_dataloader:\n    ids = train_encode['ids'].to(device)\n    mask = train_encode['mask'].to(device)\n    \n    output = model(ids, mask)\n    break\noutput","e1da1c67":"print(output['logits'].squeeze(-1))\nprint(output['logits'].squeeze(-1).shape)","9ce5f97f":"from transformers import AdamW\nLR = 2e-5\noptimizer = AdamW(model.parameters(), LR, betas=(0.9, 0.999), weight_decay=1e-2)","06fba80c":"from transformers import get_linear_schedule_with_warmup\n\nepochs = 20\n\ntrain_steps = int(len(p_train_data)\/train_batch*epochs)\nprint(train_steps)\n\nnum_steps = int(train_steps*0.1)\n\nscheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)","1a04bc1d":"# \u5b66\u7fd2\u7387\u306e\u63a8\u79fb\n# le = []\n# train_dataloader = DataLoader(train_dataset,\n#                               batch_size = train_batch,\n#                               shuffle = True,\n#                               num_workers = 8,\n#                               pin_memory = True\n#                              )\n\n# for epoch in tqdm(range(epochs)):\n#     for train_dl in train_dataloader:\n#         le.append(scheduler.get_last_lr())\n#         scheduler.step()\n# x = np.arange(len(le))\n# plt.plot(x, le)","e8de5cd7":"def loss_fn(output, target):\n    loss = nn.MSELoss()\n    return torch.sqrt(loss(output, target))","5ce6f673":"def training(train_dataloader, model, optimizer, scheluder):\n    model.train()\n    torch.backends.cudnn.benchmark = True\n    \n    all_preds = []\n    all_targets = []\n    losses = []\n\n    for train_dl in train_dataloader:\n\n        optimizer.zero_grad()\n        \n        with torch.cuda.amp.autocast():\n            ids = train_dl['ids'].to(device, non_blocking=True)\n            mask = train_dl['mask'].to(device, non_blocking=True)\n            \n            output = model(ids, mask)\n            output = output['logits'].squeeze(-1)\n            \n            target = train_dl['targets'].to(device, non_blocking=True)\n            \n            loss = loss_fn(output, target)\n            pred = output.detach().cpu().numpy()\n\n            losses.append(loss.item())\n            all_preds.append(pred)\n            all_targets.append(target.detach().squeeze(-1).cpu().numpy())\n            \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        scheduler.step()\n\n        del loss, ids, mask\n    \n    all_preds = np.concatenate(all_preds)\n    all_targets = np.concatenate(all_targets)\n    \n    losses = np.mean(losses)\n    train_rme_loss = np.sqrt(mean_squared_error(all_targets, all_preds))\n    \n    return losses, train_rme_loss\n    ","d85ba2a3":"def validating(val_dataloader, model):\n    model.eval()\n    \n    all_preds = []\n    all_targets = []\n    losses = []\n    \n    for val_dl in val_dataloader:\n        with torch.no_grad():\n            ids = val_dl['ids'].to(device)\n            mask = val_dl['mask'].to(device)\n            \n            output = model(ids, mask)\n            output = output['logits'].squeeze(-1)\n            \n            target = val_dl['targets'].to(device)\n            \n            loss = loss_fn(output, target)\n            \n            losses.append(loss.item())\n            all_preds.append(output.detach().cpu().numpy())\n            all_targets.append(target.detach().squeeze(-1).cpu().numpy())\n            \n            del loss, ids, mask\n            \n    all_preds = np.concatenate(all_preds)\n    all_targets = np.concatenate(all_targets)\n    \n    losses = np.mean(losses)\n    \n    val_rme_loss = np.sqrt(mean_squared_error(all_targets, all_preds))\n    \n    return all_preds, losses, val_rme_loss","a17753d1":"train","5d8a790a":"p_train = train_data[train_data['kfold']!=0].reset_index(drop=True)\np_val = train_data[train_data['kfold']==0].reset_index(drop=True)\n\ntrain_batch = 16\nval_batch = 32\n\ntrain_dataset = BERTDataset(p_train['excerpt'], p_train['target'])\nval_dataset = BERTDataset(p_val['excerpt'], p_val['target'])\n\ntrain_dataloader = DataLoader(train_dataset,\n                               batch_size = train_batch,\n                               shuffle = True,\n                               num_workers = 4,\n                               pin_memory = True)\n\nval_dataloader = DataLoader(val_dataset,\n                           batch_size = val_batch,\n                           shuffle = False,\n                           num_workers = 4,\n                           pin_memory = True)\n\nmodel = transformers.BertForSequenceClassification.from_pretrained('..\/input\/huggingface-bert\/bert-base-uncased\/', num_labels=1)\nmodel.to(device)\n\nlearning_rate = 2e-5\noptimizer = AdamW(model.parameters(), learning_rate, betas=(0.9, 0.999), weight_decay=1e-2)\n\ntrain_steps = int(len(p_train)\/train_batch*epochs)\nnum_steps = int(train_steps*0.1)\n\nscheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)","88914e65":"train_losses = []\nval_losses = []\nbest_score = None\n\ntrain_scores = []\nval_scores = []\n\nfor epoch in range(epochs):\n    print(f'Epochs: {epoch+1}\/{epochs}')\n\n    train_loss, train_score = training(train_dataloader, model, optimizer, scheduler)\n    train_losses.append(train_loss)\n    train_scores.append(train_score)\n    \n    preds, val_loss, val_score = validating(val_dataloader, model)\n    val_losses.append(val_loss)\n    val_scores.append(val_score)\n    \n    print(f'train loss {train_loss}, train RME {train_score}')\n    print(f'validation loss {val_loss}, validation RME {val_score}')\n    \n    if best_score is None:\n        best_score = val_score\n        \n        print('Save first model')\n        \n        state = {\n            'state_dict': model.state_dict(),\n            'optimizer_dict': optimizer.state_dict(),\n            'best_score': best_score\n        }\n        torch.save(state, 'model0.pth')\n        \n    elif best_score > val_score:\n        best_score = val_score\n        \n        print('found better point')\n        \n        state = {\n            'state_dict': model.state_dict(),\n            'optimizer_dict': optimizer.state_dict(),\n            'best_score': best_score\n        }\n        torch.save(state, 'model0.pth')\n    \n    else:\n        pass","f91effd0":"plt.scatter(p_val['target'], preds)","b61d467c":"x = np.arange(epochs)\nplt.plot(x, train_losses)\nplt.plot(x, val_losses)","5ce9d000":"x = np.arange(epochs)\nplt.plot(x, train_scores)\nplt.plot(x, val_scores)","3e235649":"best_scores = []\nbest_scores.append(best_score)","fdd3bc10":"for k in range(1, 5):\n    p_train = train_data[train_data['kfold']!=k].reset_index(drop=True)\n    p_val = train_data[train_data['kfold']==k].reset_index(drop=True)\n    \n    train_dataset = BERTDataset(p_train['excerpt'], p_train['target'])\n    val_dataset = BERTDataset(p_val['excerpt'], p_val['target'])\n\n    train_dataloader = DataLoader(train_dataset,\n                                   batch_size = train_batch,\n                                   shuffle = True,\n                                   num_workers = 4,\n                                   pin_memory = True)\n\n    val_dataloader = DataLoader(val_dataset,\n                               batch_size = val_batch,\n                               shuffle = False,\n                               num_workers = 4,\n                               pin_memory = True)\n\n    model = transformers.BertForSequenceClassification.from_pretrained('..\/input\/huggingface-bert\/bert-base-uncased\/', num_labels=1)\n    model.to(device)\n\n    learning_rate = 2e-5\n    optimizer = AdamW(model.parameters(), learning_rate, betas=(0.9, 0.999), weight_decay=1e-2)\n\n    train_steps = int(len(p_train)\/train_batch*epochs)\n    num_steps = int(train_steps*0.1)\n\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)\n    \n    train_losses = []\n    val_losses = []\n    best_score = None\n\n    train_scores = []\n    val_scores = []\n\n    for epoch in range(epochs):\n        print(f'Epochs: {epoch+1}\/{epochs}')\n\n        train_loss, train_score = training(train_dataloader, model, optimizer, scheduler)\n        train_losses.append(train_loss)\n        train_scores.append(train_score)\n\n        preds, val_loss, val_score = validating(val_dataloader, model)\n        val_losses.append(val_loss)\n        val_scores.append(val_score)\n\n        print(f'train loss {train_loss}, train RME {train_score}')\n        print(f'validation loss {val_loss}, validation RME {val_score}')\n\n        if best_score is None:\n            best_score = val_score\n\n            print('Save first model')\n\n            state = {\n                'state_dict': model.state_dict(),\n                'optimizer_dict': optimizer.state_dict(),\n                'best_score': best_score\n            }\n            torch.save(state, f'model{k}.pth')\n\n        elif best_score > val_score:\n            best_score = val_score\n\n            print('found better point')\n\n            state = {\n                'state_dict': model.state_dict(),\n                'optimizer_dict': optimizer.state_dict(),\n                'best_score': best_score\n            }\n            torch.save(state, f'model{k}.pth')\n\n        else:\n            pass\n    \n    best_scores.append(best_score)","0b78e33c":"best_scores","7a7b1cf1":"print(f'score: {np.mean(best_scores)}')","690c31bb":"import gc\ndel train_dataset, val_dataset, train_dataloader, val_dataloader, model, optimizer, scheduler\n_ = gc.collect()","83dafb39":"class BERTinfDataset(Dataset):\n    def __init__(self, sentences):\n        self.sentences = sentences\n\n        \n    def __len__(self):\n        return len(self.sentences)\n    \n    \n    def __getitem__(self, idx):\n        sentence = self.sentences[idx]\n        \n        encode_sentence = tokenizer.encode_plus(\n                                sentence,\n                                add_special_tokens = True,\n                                max_length = 314,\n                                pad_to_max_length = True,\n                                truncation = True\n                        )\n        \n        ids = torch.tensor(encode_sentence['input_ids'], dtype=torch.long)\n        mask = torch.tensor(encode_sentence['attention_mask'], dtype=torch.long)\n        token_type_ids = torch.tensor(encode_sentence['token_type_ids'], dtype=torch.long)\n\n        return {\n            'ids': ids,\n            'mask': mask,\n            'token_type_ids': token_type_ids\n        }","d85ce64f":"test_dataset = BERTinfDataset(test['excerpt'])\ntest_batch = 32\n\ntest_dataloader = DataLoader(test_dataset,\n                            batch_size = test_batch,\n                            shuffle = False,\n                            num_workers = 4,\n                            pin_memory = True)\n\nmodel = transformers.BertForSequenceClassification.from_pretrained('..\/input\/huggingface-bert\/bert-base-uncased\/', num_labels=1)","90549d70":"pthes = [os.path.join('.\/', s) for s in os.listdir('.\/') if '.pth' in s]\npthes","37b53252":"def predicting(test_dataloader, model, pthes):\n    all_preds = []\n    \n    for pth in pthes:\n        state = torch.load(pth)\n        model.load_state_dict(state['state_dict'])\n        model.to(device)\n        model.eval()\n        \n        preds = []\n        all_val_loss = 0\n        \n        with torch.no_grad():\n            for test_dl in test_dataloader:\n                ids = test_dl['ids'].to(device)\n                mask = test_dl['mask'].to(device)\n                token_type = test_dl['token_type_ids'].to(device)\n                \n                output = model(ids, mask)\n                output = output['logits'].squeeze(-1)\n                \n                preds.append(output.cpu().numpy())\n\n        preds = np.concatenate(preds)\n        all_preds.append(preds)\n        \n    return all_preds","f38839ea":"all_preds = predicting(test_dataloader, model, pthes)\ndf = pd.DataFrame(all_preds).T\ndf","c61e3907":"mean_submission = df.mean(axis=1)\nmean_submission","4d8d8ed7":"submit_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\nsubmit_df['target'] = mean_submission\nsubmit_df","71f9002f":"submit_df.to_csv('submission.csv', index=False)","6ee7384d":"# preprocess","4b263233":"# inference","bc691683":"Training","9155f976":"# EDA"}}