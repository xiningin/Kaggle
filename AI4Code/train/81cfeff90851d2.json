{"cell_type":{"8ae297ce":"code","65cf1966":"code","27a51cc2":"code","ac93d058":"code","1ee50135":"code","9b2f9b8f":"code","7467b025":"code","5514cbfe":"code","6f580e46":"code","48cd88af":"code","2918ba8c":"code","2e5a6df6":"code","b404427e":"code","31c94722":"code","ea329cdb":"code","0a6f08b2":"code","494cb0a6":"code","59ba1205":"code","a3927f55":"code","12f3cee5":"markdown"},"source":{"8ae297ce":"import pandas as pd\nd = pd.read_csv('..\/input\/nfl-impact-detection\/test_player_tracking.csv')\nIS_PRIVATE = d.shape != (19269, 12)\nprint(IS_PRIVATE)\n\nIS_PRIVATE = True","65cf1966":"if IS_PRIVATE:\n    !pip install ..\/input\/nfl-lib\/timm-0.1.26-py3-none-any.whl\n    !tar xfz ..\/input\/nfl-lib\/pkgs.tgz\n    # for pytorch1.6\n    cmd = \"sed -i -e 's\/ \\\/ \/ \\\/\\\/ \/' timm-efficientdet-pytorch\/effdet\/bench.py\"\n    !$cmd","27a51cc2":"import sys\nsys.path.insert(0, \"timm-efficientdet-pytorch\")\nsys.path.insert(0, \"omegaconf\")\n\nimport torch\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom glob import glob\nimport pandas as pd\nimport gc\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchEval\nfrom effdet.efficientdet import HeadNet\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nDATA_ROOT_PATH = 'test_images'\nSEED = 42\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseed_everything(SEED)","ac93d058":"#################\n# SET CONSTANTS\n#################\n\nDETECTION_THRESHOLD = 0.4\nDETECTOR_FILTERING_THRESHOLD = 0.3","1ee50135":"def mk_images(video_name, video_labels, video_dir, out_dir, only_with_impact=True):\n    video_path=f\"{video_dir}\/{video_name}\"\n    video_name = os.path.basename(video_path)\n    vidcap = cv2.VideoCapture(video_path)\n    if only_with_impact:\n        boxes_all = video_labels.query(\"video == @video_name\")\n        print(video_path, boxes_all[boxes_all.impact == 1.0].shape[0])\n    else:\n        print(video_path)\n    frame = 0\n    while True:\n        it_worked, img = vidcap.read()\n        if not it_worked:\n            break\n        frame += 1\n        if only_with_impact:\n            boxes = video_labels.query(\"video == @video_name and frame == @frame\")\n            boxes_with_impact = boxes[boxes.impact == 1.0]\n            if boxes_with_impact.shape[0] == 0:\n                continue\n        img_name = f\"{video_name}_frame{frame}\"\n        image_path = f'{out_dir}\/{video_name}'.replace('.mp4',f'_{frame}.png')\n        _ = cv2.imwrite(image_path, img)","9b2f9b8f":"if IS_PRIVATE:\n    out_dir = DATA_ROOT_PATH\n    if not os.path.exists(out_dir):\n        !mkdir -p $out_dir\n        video_dir = '\/kaggle\/input\/nfl-impact-detection\/test'\n        uniq_video = [path.split('\/')[-1] for path in glob(f'{video_dir}\/*.mp4')]\n        for video_name in uniq_video:\n            mk_images(video_name, pd.DataFrame(), video_dir, out_dir, only_with_impact=False)","7467b025":"def get_valid_transforms():\n    return A.Compose([\n            A.Resize(height=512, width=512, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.0)","5514cbfe":"class DatasetRetriever(Dataset):\n    def __init__(self, image_ids, transforms=None):\n        super().__init__()\n        self.image_ids = image_ids\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        image = cv2.imread(f'{DATA_ROOT_PATH}\/{image_id}', cv2.IMREAD_COLOR).copy().astype(np.float32)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        if self.transforms:\n            sample = {'image': image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n        return image, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","6f580e46":"def load_net(checkpoint_path):\n    config = get_efficientdet_config('tf_efficientdet_d5')\n    net = EfficientDet(config, pretrained_backbone=False)\n    config.num_classes = 2\n    config.image_size=512\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n    checkpoint = torch.load(checkpoint_path)\n    net.load_state_dict(checkpoint['model_state_dict'])\n    net = DetBenchEval(net, config)\n    net.eval();\n    return net.cuda()\nif IS_PRIVATE:\n    net = load_net('..\/input\/nfl-models\/\/best-checkpoint-002epoch.bin')","48cd88af":"dataset = DatasetRetriever(\n    image_ids=np.array([path.split('\/')[-1] for path in glob(f'{DATA_ROOT_PATH}\/*.png')]),\n    transforms=get_valid_transforms()\n)\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ndata_loader = DataLoader(\n    dataset,\n    batch_size=16,\n    shuffle=False,\n    num_workers=4,\n    drop_last=False,\n    collate_fn=collate_fn\n)","2918ba8c":"def make_predictions(images, score_threshold=0.5):\n    images = torch.stack(images).cuda().float()\n    box_list = []\n    score_list = []\n    with torch.no_grad():\n        det = net(images, torch.tensor([1]*images.shape[0]).float().cuda())\n        for i in range(images.shape[0]):\n            boxes = det[i].detach().cpu().numpy()[:,:4]    \n            scores = det[i].detach().cpu().numpy()[:,4]   \n            label = det[i].detach().cpu().numpy()[:,5]\n            # useing only label = 2\n            indexes = np.where((scores > score_threshold) & (label == 2))[0]\n            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n            box_list.append(boxes[indexes])\n            score_list.append(scores[indexes])\n    return box_list, score_list\nimport matplotlib.pyplot as plt","2e5a6df6":"#check prediction\n\ncnt = 0\nfor images, image_ids in data_loader:\n    box_list, score_list = make_predictions(images, score_threshold=DETECTION_THRESHOLD)\n    for i in range(len(images)):\n        sample = images[i].permute(1,2,0).cpu().numpy()\n        boxes = box_list[i].astype(np.int32).clip(min=0, max=511)\n        scores = score_list[i]\n        if len(scores) >= 1:\n            fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n            sample = cv2.resize(sample , (int(1280), int(720)))\n            for box,score in zip(boxes,scores):\n                box[0] = box[0] * 1280 \/ 512\n                box[1] = box[1] * 720 \/ 512\n                box[2] = box[2] * 1280 \/ 512\n                box[3] = box[3] * 720 \/ 512\n                cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (1, 0, 0), 3)\n            ax.set_axis_off()\n            ax.imshow(sample);\n            cnt += 1\n    if cnt >= 10:\n        break","b404427e":"result_image_ids = []\nresults_boxes = []\nresults_scores = []\nfor images, image_ids in data_loader:\n    box_list, score_list = make_predictions(images, score_threshold=DETECTION_THRESHOLD)\n    for i, image in enumerate(images):\n        boxes = box_list[i]\n        scores = score_list[i]\n        image_id = image_ids[i]\n        boxes[:, 0] = (boxes[:, 0] * 1280 \/ 512)\n        boxes[:, 1] = (boxes[:, 1] * 720 \/ 512)\n        boxes[:, 2] = (boxes[:, 2] * 1280 \/ 512)\n        boxes[:, 3] = (boxes[:, 3] * 720 \/ 512)\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        boxes = boxes.astype(np.int32)\n        boxes[:, 0] = boxes[:, 0].clip(min=0, max=1280-1)\n        boxes[:, 2] = boxes[:, 2].clip(min=0, max=1280-1)\n        boxes[:, 1] = boxes[:, 1].clip(min=0, max=720-1)\n        boxes[:, 3] = boxes[:, 3].clip(min=0, max=720-1)\n        result_image_ids += [image_id]*len(boxes)\n        results_boxes.append(boxes)\n        results_scores.append(scores)","31c94722":"box_df = pd.DataFrame(np.concatenate(results_boxes), columns=['left', 'top', 'width', 'height'])\ntest_df = pd.DataFrame({'scores':np.concatenate(results_scores), 'image_name':result_image_ids})\ntest_df = pd.concat([test_df, box_df], axis=1)\n\ntest_df = test_df[test_df.scores > DETECTOR_FILTERING_THRESHOLD]\ntest_df.shape","ea329cdb":"#gameKey,playID,view,video,frame,left,width,top,height\n#57590,3607,Endzone,57590_003607_Endzone.mp4,1,1,1,1,1\ntest_df['gameKey'] = test_df.image_name.str.split('_').str[0].astype(int)\ntest_df['playID'] = test_df.image_name.str.split('_').str[1].astype(int)\ntest_df['view'] = test_df.image_name.str.split('_').str[2]\ntest_df['frame'] = test_df.image_name.str.split('_').str[3].str.replace('.png','').astype(int)\ntest_df['video'] = test_df.image_name.str.rsplit('_',1).str[0] + '.mp4'\ntest_df = test_df[[\"gameKey\",\"playID\",\"view\",\"video\",\"frame\",\"left\",\"width\",\"top\",\"height\"]]\ntest_df","0a6f08b2":"#################\n# FILTER\n#################\n\n\ndropIDX = []\nfor keys in test_df.groupby(['gameKey', 'playID']).size().to_dict().keys():\n    tmp_df = test_df.query('gameKey == @keys[0] and playID == @keys[1]')\n    \n    for index, row in tmp_df.iterrows():\n            \n        currentFrame = row['frame']\n\n        bboxCount1 = tmp_df.query('view == \"Sideline\" and abs(frame - @currentFrame) <= 0').shape[0]\n        bboxCount2 = tmp_df.query('view == \"Endzone\" and abs(frame - @currentFrame) <= 0').shape[0]\n        if bboxCount1 != bboxCount2:\n            dropIDX.append(index)","494cb0a6":"test_df = test_df.drop(index = dropIDX).reset_index(drop = True)","59ba1205":"# clearing working dir\n# be careful when running this code on local environment!\n# !rm -rf *\n!mv * \/tmp\/","a3927f55":"import nflimpact\nenv = nflimpact.make_env()\n\nif IS_PRIVATE:\n    env.predict(test_df) # df is a pandas dataframe of your entire submission file\nelse:\n    sub = pd.read_csv('..\/input\/nfl-impact-detection\/sample_submission.csv')\n    env.predict(sub)","12f3cee5":"#### This notebook is based on https:\/\/www.kaggle.com\/its7171\/2class-object-detection-inference\n#### Here is another version with filtering: https:\/\/www.kaggle.com\/artkulak\/2class-object-detection-inference-with-filtering\n\nA day ago I released a notebook with filter to remove some of the False Positives by leaving only predictions which are present in both \"Endzone\" and \"Sideline\" views. Here is one more filtering idea which is similar but achieves a bit higher public  LB score. Don't forget to properly validate your solutions, before adding those postprocessing extra steps to your pipelines.\n\n##### Please upvote if this was helpful to you. Pressing \"fork\" is one click, pressing \"upvote\" is just one extra click which shouldn't take a lot of your time :)"}}