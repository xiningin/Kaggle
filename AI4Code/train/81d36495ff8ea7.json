{"cell_type":{"5a5f60a6":"code","56796617":"code","d9bc03ab":"code","756e26e1":"code","ac40c77e":"code","4659e258":"code","cdb4bbdf":"code","6ea615a3":"code","d8d20e0d":"code","7e474978":"code","5660ff17":"code","3fda56bf":"code","5560dcc9":"code","107eaebb":"code","41436b1f":"code","86664745":"code","2f6253ea":"code","44138664":"code","a91bb125":"code","9c414933":"code","90c77ae9":"code","977f1c5b":"code","8b90149c":"code","1c2d8ff5":"code","2b520659":"code","81f65ef5":"code","9b198424":"code","9d5bf118":"code","01e02f7a":"code","923b1c8d":"code","cc368c2e":"code","54f056e8":"code","6dcad0b5":"code","f5fc1812":"code","acd4b084":"code","58e4f2c1":"code","79edf297":"code","44f89f46":"code","c76f183a":"code","d330af6c":"code","e3b6b67e":"code","ea9bce4a":"markdown","d98e621d":"markdown","d4aaadf8":"markdown","a75c6410":"markdown","bab3b40e":"markdown","131f2e1e":"markdown","11c10345":"markdown","72099430":"markdown","a0d015b9":"markdown","2bf23598":"markdown","8192ffad":"markdown","191aaaec":"markdown","2eea2f4e":"markdown","9cc87c32":"markdown","39a6da16":"markdown","a4b3439a":"markdown","c1eb2474":"markdown","738b5a25":"markdown","eb887465":"markdown","99ae007f":"markdown","b6ab5be0":"markdown","8ed8764f":"markdown","e02131f7":"markdown","b41dc52f":"markdown","ab2213fb":"markdown","4d4becae":"markdown"},"source":{"5a5f60a6":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport statistics\nimport math\nimport random\n\nfrom numpy.linalg import inv\nfrom scipy.linalg import cholesky\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import preprocessing\nfrom sklearn.impute import SimpleImputer","56796617":"train = pd.read_csv(\"..\/input\/train.csv\")\ntrain.head()","d9bc03ab":"train.shape","756e26e1":"total = train.isnull().sum().sort_values(ascending = False)\npercent = (train.isnull().sum() \/ train.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total, percent], \n                         axis = 1, \n                         keys = ['Total', 'Percent'])\nmissing_data.head(20)","ac40c77e":"COLUMNS_drop = (missing_data[missing_data['Total'] > 1]).index\ntrain = train.drop(labels = COLUMNS_drop, axis = 1)\ntrain.shape","4659e258":"INDEX_drop = train.loc[train['Electrical'].isnull()].index\ntrain = train.drop(labels = INDEX_drop, axis = 0)\ntrain.shape","cdb4bbdf":"COLUMNS = train.columns\nCOLUMNS_res = \"\"\nCOLUMNS_num = train._get_numeric_data().columns\nCOLUMNS_cat = list(set(COLUMNS) - set(COLUMNS_num))\nprint('Number of numeric columns: {}'.format(len(COLUMNS_num)))\nprint('Number of categorical columns: {}'.format(len(COLUMNS_cat)))","6ea615a3":"train = train[COLUMNS_num]\ntrain.shape","d8d20e0d":"corrmat = train.corr()\ncorrmat = corrmat.rename_axis(None).rename_axis(None, axis = 1)\ncorrmat = corrmat.stack().reset_index()\ncorrmat.columns = ['var_1', 'var_2', 'correlation']\ncorrmat = corrmat[corrmat['correlation'] != 1]\ncorrmat.sort_values(by = 'correlation', ascending = False).head(15)","7e474978":"COLUMNS_drop = ['GarageArea', 'TotRmsAbvGrd', 'TotalBsmtSF', '2ndFlrSF']\ntrain = train.drop(labels = COLUMNS_drop, axis = 1)\ntrain.shape","5660ff17":"X = train.iloc[:, 1:(train.shape[1]-1)].values\ny = train.iloc[:, train.shape[1]-1].values","3fda56bf":"def lpy_X(y, X, g = len(y), nu0 = 1):\n    \n    n = X.shape[0]\n    p = X.shape[1]\n    s20 = sum(sm.OLS(y, X).fit().resid**2) \/ sm.OLS(y, X).fit().df_resid\n    \n    if p == 0:\n        Hg = 0\n        s20 = statistics.mean(y**2)\n    \n    elif p > 0:\n        X_T = np.transpose(X)\n        Hg = (g\/(g+1)) * np.dot(np.dot(X, inv(np.dot(X_T, X))), X_T)\n        \n    y_T = np.transpose(y)\n    identity_mat = np.diag(np.repeat(1, n))\n    i_H = identity_mat - Hg\n    SSRg = np.dot(np.dot(y_T, i_H), y)\n    \n    return (-0.5\n            * (n * np.log(np.pi) + p * np.log(1+g) + (nu0+n)*np.log(nu0*s20+SSRg) - nu0*np.log(nu0*s20))\n            + math.lgamma((nu0+n)\/2)\n            + math.lgamma(nu0\/2))","5560dcc9":"z = np.repeat(1, X.shape[1])\nlpy_c = lpy_X(y = y, X = X[:, z == 1])\nS = 100\nZ = np.zeros([S, X.shape[1]], dtype = int)","107eaebb":"for s in np.arange(0, S, 1):\n        \n    for j in pd.Series(np.arange(0,29,1)).sample(29, replace = False).values:\n        \n        zp = z.copy()\n        zp[j] = 1 - zp[j]\n        lpy_p = lpy_X(y, X[:, zp == 1])\n        r = (lpy_p - lpy_c)*(-1)**(zp[j] == 0)\n        z[j] = np.random.binomial(n=1, p=(1\/(1+np.exp(-r))), size=1)\n        \n        if z[j] == zp[j]:\n            lpy_c = lpy_p\n    \n    Z[s,:] = z  \n    \n    # Display sampling process by printing a single dot for each step\n    print('.', end = '')\n    \n    if s == (S-1):\n        print('Done!')","41436b1f":"ps = pd.Series([tuple(i) for i in Z])\ncounts = ps.value_counts(normalize = True)\ncounts[0:5]","86664745":"COLUMNS = np.array(train.columns)\nCOLUMNS = np.delete(COLUMNS, [0, 30])\nCOLUMNS = COLUMNS[np.array(counts.index[0]) == 1]\nCOLUMNS","2f6253ea":"train = pd.concat([train[['Id', 'SalePrice']], train[COLUMNS]], axis = 1)\ntrain.head()","44138664":"neighbor = pd.read_csv('..\/input\/train.csv')[['Id', 'Neighborhood']]\ntrain = pd.merge(left = train, right = neighbor, left_on = 'Id', right_on = 'Id', how = 'left')\nprint(train.shape)","a91bb125":"_ = sns.catplot(x = 'Neighborhood', y = 'SalePrice', kind = 'box', data = train)\n_ = plt.xticks(rotation = 90)\n_ = plt.title('Boxplot of house prices by locations')\nplt.show()","9c414933":"X_COLUMNS = COLUMNS\n\n# preprocessing scale function shows warning if data type is int64\ntemp_df = train[X_COLUMNS].astype(np.float64)\n\nNeighborhood_list = np.unique(train['Neighborhood'])\nm = len(Neighborhood_list)\nX_list = []\n\nfor i in np.arange(0,m,1):\n    temp = temp_df.loc[train['Neighborhood'] == Neighborhood_list[i]]\n    # centering, not scaling\n    temp = preprocessing.scale(temp.values,\n                               with_mean = True,\n                               with_std = False)\n    # adding intercept. do this after centering, otherwise intercept will be 0\n    temp = sm.add_constant(temp)\n    X_list.append(temp)\n    \nX = X_list","90c77ae9":"temp_df = train['SalePrice']\ny_list = []\n\nfor i in np.arange(0, len(Neighborhood_list),1):\n    y_list.append(temp_df.loc[train['Neighborhood'] == Neighborhood_list[i]])\n    \ny = y_list","977f1c5b":"def rmvnorm(n, mu, Sigma):\n    E = np.random.normal(0, 1, n*len(mu))\n    return np.dot(E, cholesky(Sigma, lower = False)) + mu","8b90149c":"def rwish(nu0, S0):\n    sS0 = cholesky(S0, lower = False)\n    Z = np.dot(np.random.normal(0, 1, rwish_mean*rwish_var.shape[0]).reshape(rwish_mean, rwish_var.shape[0]), sS0)\n    S = np.dot(np.transpose(Z), Z)\n    return S","1c2d8ff5":"m = len(Neighborhood_list)\np = X[0].shape[1]\nBETA_LS = np.zeros([m, p])\nS2_LS = np.zeros(m)\n\nfor i in np.arange(0,m,1):\n    \n    # fit OLS to each group\n    results = sm.OLS(y[i], X[i]).fit()\n    \n    # calculate parameters\n    beta = results.params\n    \n    # calculate sample variance\n    RSS = sum(results.resid ** 2)\n    df = results.df_resid\n    sample_variance = RSS\/df\n    \n    # store outputs\n    BETA_LS[i] = beta\n    S2_LS[i] = sample_variance  ","2b520659":"p = X[0].shape[1]\ntheta = pd.DataFrame(BETA_LS).apply(statistics.mean, axis = 0)\nmu0 = pd.DataFrame(BETA_LS).apply(statistics.mean, axis = 0)\nnu0 = 1\ns2 = statistics.mean(S2_LS[np.isfinite(S2_LS)])\ns20 = statistics.mean(S2_LS[np.isfinite(S2_LS)])\neta0 = p + 2\nSigma = np.cov(BETA_LS, rowvar = False)\nS0 = np.cov(BETA_LS, rowvar = False)\nL0 = np.cov(BETA_LS, rowvar = False)\nBETA = BETA_LS\niL0 = inv(L0)\niSigma = inv(Sigma)\n\nN = np.zeros(m)\nfor i in np.arange(0,m,1):\n    N[i] = len(X[i])","81f65ef5":"# MCMC setting and storing\nrandom.seed(0)\nS = 5000\nS2_b = np.zeros(S)\nTHETA_b = np.zeros([S, p])\nSigma_ps = np.zeros([p, p])\nBETA_ps = BETA * 0\nSIGMA_PS = np.zeros([S, p * p])\nBETA_pp = np.zeros([S, p])","9b198424":"for s in np.arange(0,S,1):\n\n    # update beta_j\n    for j in np.arange(0,m,1):\n        Vj = inv(iSigma + np.dot(np.transpose(X[j]), X[j])\/s2)\n        Ej = np.dot(Vj, np.dot(iSigma, theta) + np.dot(np.transpose(X[j]), y[j])\/s2)\n        BETA[j] = rmvnorm(1, Ej, Vj)\n\n    # update theta\n    Lm = inv(iL0 + m * iSigma)\n    mum = np.dot(Lm, np.dot(iL0, mu0) + np.dot(iSigma, pd.DataFrame(BETA).apply(sum, axis = 0)))\n    theta = rmvnorm(1, mum, Lm)\n\n    # update Sigma\n    rwish_mean = eta0 + m\n    rwish_var = inv(S0 + np.dot(np.transpose(BETA - theta), BETA - theta))\n    iSigma = rwish(rwish_mean, rwish_var)\n\n    # update s2\n    RSS = 0\n    for j in np.arange(0,m,1):\n        RSS = RSS + sum((y[j] - np.dot(X[j], BETA[j])) ** 2)\n    s2 = 1\/np.random.gamma(shape = (nu0 + sum(N))\/2, scale = 1\/((nu0*s20+RSS)\/2), size = 1)\n\n    # store results\n    S2_b[s] = s2\n    THETA_b[s] = theta\n    Sigma_ps = Sigma_ps + inv(iSigma)\n    BETA_ps = BETA_ps + BETA\n    SIGMA_PS[s] = np.matrix.flatten(inv(iSigma))\n    BETA_pp[s] = rmvnorm(1, theta, inv(iSigma))\n    \n    # Display sampling process by printing a single dot\n    if (s % 100 == 0):\n        print('.', end = '')\n        \n    if s == (S-1):\n        print('Done!')","9d5bf118":"neighborhood_list_df = pd.DataFrame(Neighborhood_list,\n                                    columns = ['Neighborhood'])\ncoef = pd.DataFrame(BETA_ps\/S)\ncoef = pd.concat([neighborhood_list_df, coef], \n                 axis = 1)\n\n# Rename columns\nCOLUMNS_coef = COLUMNS.copy()\nCOLUMNS_coef = COLUMNS_coef + '_coef'\nCOLUMNS_coef = np.append(['Neighborhood', 'Intercept_coef'], COLUMNS_coef)\ncoef.columns = COLUMNS_coef\n\ncoef","01e02f7a":"temp_y = np.matrix(y[0]).transpose()\ntemp_x = np.matrix(X[0])\ntemp_yx = pd.DataFrame(np.concatenate((temp_y, temp_x), axis = 1))\ntemp_yx['Neighborhood'] = Neighborhood_list[0]\n\nm = len(Neighborhood_list)\n\nfor i in range(1, m):\n    temp_y = np.matrix(y[i]).transpose()\n    temp_x = np.matrix(X[i])\n    temp = pd.DataFrame(np.concatenate((temp_y, temp_x), axis = 1))\n    temp['Neighborhood'] = Neighborhood_list[i]\n    temp_yx = pd.concat([temp_yx, temp], axis = 0)\n    \n# Rename columns\nCOLUMNS_pred = COLUMNS.copy()\nCOLUMNS_pred = np.append(['SalePrice', 'Intercept'], COLUMNS_pred)\nCOLUMNS_pred = np.append(COLUMNS_pred, 'Neighborhood')\ntemp_yx.columns = COLUMNS_pred\n    \ntrain_pred_df = temp_yx.merge(coef, on = 'Neighborhood', how = 'left')","923b1c8d":"train_pred_df.head()","cc368c2e":"# the 2nd argument of np.delete specifies the element position in array that you wan to drop.\nCOLUMNS_pred = np.delete(COLUMNS_pred, 0)\nCOLUMNS_pred = np.delete(COLUMNS_pred, len(COLUMNS_pred)-1)\ntrain_pred_df['SalePrice_pred'] = 0\n\nfor i in range(len(COLUMNS_pred)):\n    coefs = COLUMNS_pred[i] + '_coef'\n    train_pred_df['SalePrice_pred'] = train_pred_df['SalePrice_pred'] + train_pred_df[coefs] * train_pred_df[COLUMNS_pred[i]]","54f056e8":"np.round(train_pred_df[['SalePrice', 'SalePrice_pred']].head(), decimals = 0)","6dcad0b5":"MSPE_train_BDA = statistics.mean((train_pred_df['SalePrice'] - train_pred_df['SalePrice_pred'])**2)\nprint('MSPE in training data by Bayesian model selection and hierarchical regression: {:,}'.format(round(MSPE_train_BDA)))","f5fc1812":"test = pd.read_csv(\"..\/input\/test.csv\")\ntest.shape","acd4b084":"COLUMNS_pred = np.delete(COLUMNS_pred, 0)\nCOLUMNS_pred = np.append(['Id', 'Neighborhood'], COLUMNS_pred)\nX_test = test[COLUMNS_pred]\nprint(X_test.shape)\nprint(X_test.isnull().sum())","58e4f2c1":"# imputing missing values\nCOLUMNS_missing = COLUMNS_pred[X_test.isnull().sum() > 0]\ntest_temp = test[COLUMNS_missing]\nimp_mean = SimpleImputer(missing_values = np.nan, strategy = 'mean')\ntest_temp = imp_mean.fit_transform(test_temp)\ntest_temp = pd.DataFrame(test_temp, columns = COLUMNS_missing)\nprint(test_temp.shape)\nprint(test_temp.isnull().sum())","79edf297":"COLUMNS_nonmiss = COLUMNS_pred[X_test.isnull().sum() == 0]\nX_test = X_test[COLUMNS_nonmiss]\nX_test = pd.concat([X_test, test_temp], axis = 1)","44f89f46":"train = pd.read_csv(\"..\/input\/train.csv\")\n\nfor col in COLUMNS:\n    \n    mean = train.groupby('Neighborhood')[col].mean()\n    mean_df = pd.DataFrame(data = {'Neighborhood':mean.index, 'mean':mean.values})\n    X_test = pd.merge(left = X_test, right = mean_df, on = 'Neighborhood', how = 'left')\n    X_test[col] = X_test[col] - X_test['mean'] # update original values with scaled values\n    X_test = X_test.drop(columns = ['mean'])\n    \nX_test = pd.merge(left = X_test, right = coef, on = 'Neighborhood', how = 'left')","c76f183a":"COLUMNS_pred = np.append('Intercept', COLUMNS)\nX_test['SalePrice'] = 0\nX_test['Intercept'] = 1\n\nfor i in range(len(COLUMNS_pred)):\n    coefs = COLUMNS_pred[i] + '_coef'\n    X_test['SalePrice'] = X_test['SalePrice'] + X_test[coefs] * X_test[COLUMNS_pred[i]]","d330af6c":"np.round(X_test['SalePrice'].describe(), decimals = 0)","e3b6b67e":"np.round(train['SalePrice'].describe(), decimals = 0)","ea9bce4a":"One of the numeric column is our response \"SalePrice\". We put all the numeric variables into variable selection process. We consider only numeric variables in the selection.","d98e621d":"# Missing data","d4aaadf8":"# Data for hierarchical regression model","a75c6410":"# Functions for Markov chain Monte Carlo (MCMC)","bab3b40e":"# MCMC setup","131f2e1e":"# Parameters estimated MCMC","11c10345":"# Drop highly correlated variable","72099430":"We remove one of variables which has high correlation. It enables to solve inverse matrix calculation and prevents multicolinearity.","a0d015b9":"# Prior values","2bf23598":"# Training data","8192ffad":"The following is the estimated predictors' coefficient by MCMC.","191aaaec":"# Imputation missing values in test data","2eea2f4e":"# Gibbs sampler","9cc87c32":"# Data for Bayesian model selection","39a6da16":"# Prediction in test data","a4b3439a":"# Bayesian Data Analysis of House Prices","c1eb2474":"We apply Bayesian model selection and hierarchical regression model to estimate house prices. Model selection is performed on numerical variables, and hierarchy is set by house location (\"Neighborhood\").","738b5a25":"Distribution of SalePrice in training set, which is not hugely different from prediction in test set.","eb887465":"When we built train dataset for MCMC, we scaled predictors, ie deducted predictor values by means of each predictor. So we need to deduct test predictor values also.","99ae007f":"# Likelihood and Posterior values (MCMC part)","b6ab5be0":"# Training accuracy","8ed8764f":"# Package","e02131f7":"# Function for Bayesian model selection","b41dc52f":"Distribution of predicted SalePrice in test set.","ab2213fb":"# Training accuracy","4d4becae":"Predict house prices."}}