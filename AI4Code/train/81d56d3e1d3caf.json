{"cell_type":{"7e3dcdb5":"code","931c60ed":"code","0a56d56e":"code","f6563db4":"code","e127bd72":"code","7182fc73":"code","cbf0b78d":"code","325b9d1d":"code","69e9dc88":"code","11fd1cba":"code","cc9418af":"code","9de08eef":"code","acaf71f9":"code","365fd1a8":"code","aeb58177":"code","fda09270":"code","17454f5c":"code","38fb6d13":"code","2b1b1534":"code","aa3eadeb":"code","c8eef292":"code","82607be3":"code","55db6030":"markdown","2f192258":"markdown","4a5e4772":"markdown","5ec913b5":"markdown","acf01ce2":"markdown","2f62cd61":"markdown","ee190bc1":"markdown"},"source":{"7e3dcdb5":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n","931c60ed":"data=pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\n","0a56d56e":"data.head()","f6563db4":"data.shape","e127bd72":"data.isnull().sum()","7182fc73":"data.describe()","cbf0b78d":"data.loc[(data.SkinThickness<5)& (data.Outcome==0), 'SkinThickness']=int(data[(data.Outcome==0)]['SkinThickness'].mean())\ndata.loc[(data.SkinThickness<5)& (data.Outcome==1), 'SkinThickness']=int(data[(data.Outcome==1)]['SkinThickness'].mean())","325b9d1d":"data.loc[(data.Insulin==0)& (data.Outcome==0), 'Insulin']=int(data[(data.Outcome==0)]['Insulin'].mean())\ndata.loc[(data.Insulin==0)& (data.Outcome==1), 'Insulin']=int(data[(data.Outcome==1)]['Insulin'].mean())","69e9dc88":"data.columns","11fd1cba":"sns.countplot(data=data ,x=\"Outcome\",hue=\"Outcome\")\nplt.title(\"Womans have diabetes\")","cc9418af":"fig, ax=plt.subplots(figsize=(5,5))\nsns.boxplot(y=\"Age\",x='Outcome',hue='Outcome',data=data)\nplt.title(\" Age \")","9de08eef":"for i in data.columns:\n    plt.figsize=(12,10)\n    plt.hist(data[i])\n    plt.title(i)\n    plt.show()","acaf71f9":"sns.lineplot(data=data ,x='Age',hue='Age',y=\"Glucose\")","365fd1a8":"sns.pairplot(x_vars=[\"Glucose\",\"Pregnancies\",\"BMI\"],y_vars=\"Age\",hue=\"Outcome\",data=data)","aeb58177":"sns.pairplot(data=data,hue=\"Outcome\")","fda09270":"corr_matrix=data.corr()\ncorr_matrix","17454f5c":"import seaborn as sns\n#get correlations of each features in dataset\ncorr_matrix = data.corr()\ntop_corr_features = corr_matrix.index\nplt.figure(figsize=(8,6))\n#plot heat map\ng=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","38fb6d13":"X=np.array(data[[\"Pregnancies\",\"BloodPressure\",\"Glucose\",\"SkinThickness\",\"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\"]])\ny=np.array(data.Outcome)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\n","2b1b1534":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nlog_clf = LogisticRegression()\nrnd_clf = RandomForestClassifier()\nsvm_clf = SVC()\ntree_clf = DecisionTreeClassifier()\nknn_clf= KNeighborsClassifier()\nbgc_clf=BaggingClassifier()\ngbc_clf=GradientBoostingClassifier()\nabc_clf= AdaBoostClassifier()\n\nvoting_clf = VotingClassifier(\nestimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf), ('tree', tree_clf),('knn', knn_clf),('bg', bgc_clf),\n            ('gbc', gbc_clf),('abc', abc_clf)],voting='hard')\nvoting_clf.fit(x_train, y_train)\n\nfrom sklearn.metrics import accuracy_score\nfor clf in  (log_clf, rnd_clf, svm_clf,tree_clf,knn_clf,bgc_clf,gbc_clf,abc_clf,voting_clf):\n    clf.fit(x_train,y_train)\n    y_pred = clf.predict(x_test)\n    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))","aa3eadeb":"from sklearn.ensemble import GradientBoostingClassifier\ngbc=GradientBoostingClassifier(random_state=0)\ngbc.fit(x_train,y_train)\n#predict x_test values\npred=gbc.predict(x_test)\n#print accuracy for algorithm\nprint(\"Accuracy for GradientBoosting data: \",gbc.score(x_test,y_test))\n# 0.8779888739049218","c8eef292":"#import classification_report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,pred))","82607be3":"from sklearn.metrics import roc_curve,auc\ny_pred_proba = gbc.predict_proba(x_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test,pred)\nauc_gbc = auc(fpr, tpr)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='Gradient Boosting (auc = %0.3f)'% auc_gbc )\nplt.xlabel('Tpr')\nplt.xlabel('Fpr')\nplt.title('Gradient Boosting ROC curve')\nplt.legend()\nplt.show()","55db6030":"# Model Evaluation Metrics","2f192258":"# If need learn  more about Removing outliers \nCheck this kemel:https:\/\/www.kaggle.com\/akhileshdkapse\/starter-guide-eda-acc-87-precision-92\/notebook#Removing-outliers-!","4a5e4772":"# Removing outliers","5ec913b5":"# Voting Classifier","acf01ce2":"# Build model use  Logistic Regression","2f62cd61":"# data visualization","ee190bc1":"# Load Necessary Libraries "}}