{"cell_type":{"047d3c72":"code","ebd2e27c":"code","c595afa9":"code","667551dc":"code","aabfce91":"code","ae18b5b3":"code","fac71d2c":"code","dde3a9e1":"code","ea4ace37":"code","655c927b":"code","0971d8fc":"code","82d5f7cd":"code","2c21e040":"code","fb3e98bb":"code","35417a27":"markdown","8444214b":"markdown","c12203b8":"markdown"},"source":{"047d3c72":"from PIL import Image, ImageOps\nimport cv2\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport torch\n\nimport os\nimport sys","ebd2e27c":"os.mkdir('.\/utils')\nsys.path.append('.\/utils')","c595afa9":"from shutil import copyfile\ncopyfile(src = \"..\/input\/utils-inf\/data_augumentation.py\", dst = \".\/utils\/data_augumentation.py\")\ncopyfile(src = \"..\/input\/utils-inf\/dataloader.py\", dst = \".\/utils\/dataloader.py\")\ncopyfile(src = \"..\/input\/utils-inf\/pspnet.py\", dst = \".\/utils\/pspnet.py\")\n\nfrom dataloader import make_datapath_list, DataTransform","667551dc":"rootpath = \"..\/input\/sartorius-cell-instance-segmentation\/\"\nval_anno_list = make_datapath_list(\n    rootpath=rootpath)","aabfce91":"from pspnet import PSPNet\n\nnet = PSPNet(n_classes=1)\n\nstate_dict = torch.load(\"..\/input\/weights\/pspnet50_40.pth\",\n                        map_location={'cuda:0': 'cpu'})\nnet.load_state_dict(state_dict)","ae18b5b3":"sample_sub_df = pd.read_csv('..\/input\/sartorius-cell-instance-segmentation\/sample_submission.csv')\nsample_sub_df","fac71d2c":"test_id_list = list(sample_sub_df['id'].values)\ntest_id_list","dde3a9e1":"def rle_decode(mask_rle, shape, color=1):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height, width, channels) of array to return \n    color: color for the mask\n    Returns numpy array (mask)\n\n    '''\n    s = mask_rle.split()\n    \n    starts = list(map(lambda x: int(x) - 1, s[0::2]))\n    lengths = list(map(int, s[1::2]))\n    ends = [x + y for x, y in zip(starts, lengths)]\n    \n    img = np.zeros((shape[0] * shape[1], shape[2]), dtype=np.float32)\n            \n    for start, end in zip(starts, ends):\n        img[start : end] = color\n    \n    return img.reshape(shape)\n\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","ea4ace37":"color_mean = (1.0, 1.0, 1.0)\ncolor_std = (1.0, 1.0, 1.0)","655c927b":"def make_test_mask(test_id):\n\n    image_file_path = f\"..\/input\/sartorius-cell-instance-segmentation\/test\/{test_id}.png\"\n\n    img = Image.open(image_file_path)\n    img = img.convert(\"RGB\")\n    img_width, img_height = img.size\n\n    transform = DataTransform(\n        input_size=520, color_mean=color_mean, color_std=color_std)\n\n    anno_file_path = val_anno_list[0]\n    anno_class_img = Image.open(anno_file_path)  \n    anno_class_img = anno_class_img.convert(\"L\")\n    anno_class_img = ImageOps.invert(anno_class_img)\n    anno_class_img = anno_class_img.quantize()\n    p_palette = anno_class_img.getpalette()\n    phase = \"val\"\n    img, anno_class_img = transform(phase, img, anno_class_img)\n\n    net.eval()\n    x = img.unsqueeze(0) \n    outputs = net(x)\n    y = outputs[0]\n\n    y = y.detach().numpy()[0][0]\n    anno_class_img = Image.fromarray(np.uint8(y), mode=\"P\")\n    anno_class_img = anno_class_img.resize((img_width, img_height), Image.NEAREST)\n    anno_class_img.putpalette(p_palette)\n\n    anno_class_img = anno_class_img.convert('I')\n    n = np.array(anno_class_img).astype(np.uint8)\n    n = np.clip(n, 0, 1)\n\n    test_mask = rle_encode(n)\n\n    sample_sub_df.loc[sample_sub_df['id'] == test_id, \"predicted\"] = test_mask\n    \n    return","0971d8fc":"for test_id in test_id_list:\n    make_test_mask(test_id)","82d5f7cd":"sample_sub_df.to_csv('submission.csv', index=False)","2c21e040":"def plot_masks(image_id, colors=True):\n    labels = sample_sub_df[sample_sub_df[\"id\"] == image_id][\"predicted\"].tolist()\n\n    if colors:\n        mask = np.zeros((520, 704, 3))\n        for label in labels:\n            mask += rle_decode(label, shape=(520, 704, 3), color=np.random.rand(3))\n    else:\n        mask = np.zeros((520, 704, 1))\n        for label in labels:\n            mask += rle_decode(label, shape=(520, 704, 1))\n    mask = mask.clip(0, 1)\n\n    image = cv2.imread(f\"..\/input\/sartorius-cell-instance-segmentation\/test\/{image_id}.png\")\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    plt.figure(figsize=(16, 32))\n    plt.subplot(3, 1, 1)\n    plt.imshow(image)\n    plt.axis(\"off\")\n    plt.subplot(3, 1, 2)\n    plt.imshow(image)\n    plt.imshow(mask, alpha=0.5)\n    plt.axis(\"off\")\n    plt.subplot(3, 1, 3)\n    plt.imshow(mask)\n    plt.axis(\"off\")\n    \n    plt.show();","fb3e98bb":"plot_masks(\"7ae19de7bc2a\", colors=False)","35417a27":"# Training notebook is [here](https:\/\/www.kaggle.com\/kurokia\/semantic-segmentation-by-pspnet-train).","8444214b":"Copyright (c) 2019 Yutaro Ogawa\n\nReleased under the MIT license https:\/\/github.com\/YutaroOgawa\/pytorch_advanced\/blob\/master\/LICENSE","c12203b8":"# Since the evaluation method for this competition is \"instance segmentation\", this notebook cannot be used directly for submission.\nPlease use this notebook as a reference for semantic segmentation.\n\nThe codes in this notebook refer to https:\/\/github.com\/YutaroOgawa\/pytorch_advanced\/tree\/master\/3_semantic_segmentation, https:\/\/www.kaggle.com\/inversion\/run-length-decoding-quick-start and https:\/\/www.kaggle.com\/ihelon\/cell-segmentation-run-length-decoding\n\nPlease upvote the notebooks."}}