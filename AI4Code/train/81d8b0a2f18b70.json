{"cell_type":{"47cdc01f":"code","f5201721":"code","f9323a5f":"code","351291d8":"code","c2246c5e":"code","7f1e6310":"code","0ea90221":"code","d7820b07":"code","fc0e14e7":"code","14c12710":"code","b37afba9":"code","e62ff197":"code","ae1f5ce8":"code","cfb06c83":"code","5475a978":"code","e0e2cd36":"code","00646083":"code","2aee24f6":"code","12845d10":"code","bb64753b":"code","d8f53d3a":"code","a9cc767c":"code","6a2927ca":"code","1dceeb67":"code","3bd30a42":"code","235344a1":"code","ddb8a5ec":"code","12ccbead":"code","0fb4d90e":"code","65af15c6":"code","15143cc2":"code","29131f91":"code","5e184561":"code","9510c9d9":"code","8d2e04cc":"code","c5885422":"code","d7b389f8":"code","1e0d176a":"code","e553e064":"markdown","94f30f9b":"markdown","4d12d1f6":"markdown","9ffc4e5d":"markdown","6e149688":"markdown","37e35f5d":"markdown","ad4b047a":"markdown","e72064c2":"markdown","2dbf23f0":"markdown","77bb4401":"markdown","6044263c":"markdown","be9d2b3c":"markdown","737ec540":"markdown","fc360c94":"markdown","d38aad2c":"markdown","9701f9b6":"markdown","8a769764":"markdown","da5bbd78":"markdown","24da942e":"markdown","9e3d20eb":"markdown","fd2f5c8e":"markdown","99871af2":"markdown","51b509af":"markdown","76ff180e":"markdown","cc9239f8":"markdown","d3cc9df6":"markdown","3162288a":"markdown","b0656224":"markdown","e48244ba":"markdown"},"source":{"47cdc01f":"import pandas as pd\nfrom pandas.api.types import CategoricalDtype\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom xgboost import XGBRegressor\nfrom sklearn.feature_selection import mutual_info_regression\nfrom category_encoders import MEstimateEncoder\nimport optuna\nimport numpy as np","f5201721":"pd.set_option('display.max_rows', 80)\nmetric = 'RMSLE'","f9323a5f":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')\nprint(df_train.shape)\nprint(df_test.shape)","351291d8":"df = pd.concat([df_train, df_test])","c2246c5e":"for col in df.select_dtypes(['object']):\n    print(f'{col}:\\n{df[col].unique()}\\n')","7f1e6310":"df['Exterior2nd'] = df['Exterior2nd'].replace({'Wd Shng': 'WdShing', 'CmentBd': 'CemntBd', 'Brk Cmn': 'BrkComm'})","0ea90221":"df['MSZoning'] = df['MSZoning'].replace({'C (all)': 'C'})\ndf['Neighborhood'] = df['Neighborhood'].replace({'NAmes': 'Names'})\ndf['BldgType'] = df['BldgType'].replace({'2fmCon': '2FmCon', 'Duplex': 'Duplx', 'Twnhs': 'TwnhsI'})","d7820b07":"df.select_dtypes('number').aggregate(['min', 'max', 'mean']).T","fc0e14e7":"df['GarageYrBlt'] = df['GarageYrBlt'].where(df['GarageYrBlt'] <= 2010, df['YearBuilt'])","14c12710":"def clean(df):\n    df['Exterior2nd'] = df['Exterior2nd'].replace({'Wd Shng': 'WdShing', 'CmentBd': 'CemntBd', 'Brk Cmn': 'BrkComm'})\n    df['MSZoning'] = df['MSZoning'].replace({'C (all)': 'C'})\n    df['Neighborhood'] = df['Neighborhood'].replace({'NAmes': 'Names'})\n    df['BldgType'] = df['BldgType'].replace({'2fmCon': '2FmCon', 'Duplex': 'Duplx', 'Twnhs': 'TwnhsI'})\n    df['GarageYrBlt'] = df['GarageYrBlt'].where(df['GarageYrBlt'] <= 2010, df['YearBuilt'])\n    return df","b37afba9":"# The nominal (unordered) categorical features\nnominal_categories = {\n    \"MSSubClass\": CategoricalDtype(categories=['None', 20, 30, 40, 45, 50, 60, 70, 75, 80, 85, 90, 120, 150, 160, 180, 190]),\n    \"MSZoning\": CategoricalDtype(categories=['None', 'A', 'C', 'FV', 'I', 'RH', 'RL', 'RP', 'RM']),\n    \"Street\": CategoricalDtype(categories=['None', 'Grvl', 'Pave']),\n    \"Alley\": CategoricalDtype(categories=['None', 'Grvl', 'Pave']),\n    \"LandContour\": CategoricalDtype(categories=['None', 'Lvl', 'Bnk', 'HLS', 'Low']), # ordinal?\n    \"LotConfig\": CategoricalDtype(categories=['None', 'Inside', 'Corner', 'CulDSac', 'FR2', 'FR3']), # ordinal?\n    \"Neighborhood\": CategoricalDtype(categories=['None', 'Blmngtn', 'Blueste', 'BrDale', 'BrkSide', 'ClearCr', 'CollgCr', 'Crawfor', 'Edwards', 'Gilbert', 'IDOTRR', 'MeadowV', 'Mitchel', 'Names', 'NoRidge', 'NPkVill', 'NridgHt', 'NWAmes', 'OldTown', 'SWISU', 'Sawyer', 'SawyerW', 'Somerst', 'StoneBr', 'Timber', 'Veenker']),\n    \"Condition1\": CategoricalDtype(categories=['None', 'Artery', 'Feedr', 'Norm', 'RRNn', 'RRAn', 'PosN', 'PosA', 'RRNe', 'RRAe']),\n    \"Condition2\": CategoricalDtype(categories=['None', 'Artery', 'Feedr', 'Norm', 'RRNn', 'RRAn', 'PosN', 'PosA', 'RRNe', 'RRAe']),\n    \"BldgType\": CategoricalDtype(categories=['None', '1Fam', '2FmCon', 'Duplx', 'TwnhsE', 'TwnhsI']),\n    \"HouseStyle\": CategoricalDtype(categories=['None', '1Story', '1.5Fin', '1.5Unf', '2Story', '2.5Fin', '2.5Unf', 'SFoyer', 'SLvl']),\n    \"RoofStyle\": CategoricalDtype(categories=['None', 'Flat', 'Gable', 'Gambrel', 'Hip', 'Mansard', 'Shed']),\n    \"RoofMatl\": CategoricalDtype(categories=['None', 'ClyTile', 'CompShg', 'Membran', 'Metal', 'Roll', 'Tar&Grv', 'WdShake', 'WdShngl']),\n    \"Exterior1st\": CategoricalDtype(categories=['None', 'AsbShng', 'AsphShn', 'BrkComm', 'BrkFace', 'CBlock', 'CemntBd', 'HdBoard', 'ImStucc', 'MetalSd', 'Other', 'Plywood', 'PreCast', 'Stone', 'Stucco', 'VinylSd', 'Wd Sdng', 'WdShing']),\n    \"Exterior2nd\": CategoricalDtype(categories=['None', 'AsbShng', 'AsphShn', 'BrkComm', 'BrkFace', 'CBlock', 'CemntBd', 'HdBoard', 'ImStucc', 'MetalSd', 'Other', 'Plywood', 'PreCast', 'Stone', 'Stucco', 'VinylSd', 'Wd Sdng', 'WdShing']),\n    \"MasVnrType\": CategoricalDtype(categories=['None', 'BrkCmn', 'BrkFace', 'CBlock', 'Stone']),\n    \"Foundation\": CategoricalDtype(categories=['None', 'BrkTil', 'CBlock', 'PConc', 'Slab', 'Stone', 'Wood']),\n    \"Heating\": CategoricalDtype(categories=['None', 'Floor', 'GasA', 'GasW', 'Grav', 'OthW', 'Wall']),\n    \"GarageType\": CategoricalDtype(categories=['None', '2Types', 'Attchd', 'Basment', 'BuiltIn', 'CarPort', 'Detchd']),\n    \"MiscFeature\": CategoricalDtype(categories=['None', 'Elev', 'Gar2', 'Othr', 'Shed', 'TenC']),\n    \"SaleType\": CategoricalDtype(categories=['None', 'WD', 'CWD', 'VWD', 'New', 'COD', 'Con', 'ConLw', 'ConLI', 'ConLD', 'Oth']),\n    \"SaleCondition\": CategoricalDtype(categories=['None', 'Normal', 'Abnorml', 'AdjLand', 'Alloca', 'Family', 'Partial'])\n}\n\n# The ordinal (ordered) categorical features \nfive_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\nten_levels = list(range(1,11))\nordered_levels = {\n    \"OverallQual\": ten_levels,\n    \"OverallCond\": ten_levels,\n    \"ExterQual\": five_levels,\n    \"ExterCond\": five_levels,\n    \"BsmtQual\": five_levels,\n    \"BsmtCond\": five_levels,\n    \"HeatingQC\": five_levels,\n    \"KitchenQual\": five_levels,\n    \"FireplaceQu\": five_levels,\n    \"GarageQual\": five_levels,\n    \"GarageCond\": five_levels,\n    \"PoolQC\": five_levels,\n    \"LotShape\": [\"IR3\", \"IR2\", \"IR1\", \"Reg\"],\n    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"Functional\": [\"Sal\", \"Sev\", \"Maj2\", \"Maj1\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"],\n    \"CentralAir\": [\"N\", \"Y\"],\n    \"Electrical\": [\"FuseP\", \"FuseF\", \"Mix\", \"FuseA\", \"SBrkr\"],\n    \"Fence\": [\"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n}\n\n# Add None level for missing values\nordered_levels = {key: ['None'] + value for key, value in ordered_levels.items()}","e62ff197":"def encode(df):\n    # Nominal categories\n    for col, cats in nominal_categories.items():\n        df[col] = df[col].astype(cats)\n    # Ordinal categories\n    for col, levels in ordered_levels.items():\n        df[col] = df[col].astype(CategoricalDtype(levels, ordered=True))\n    return df\n\ndf = encode(df)","ae1f5ce8":"nan_by_col = df.isnull().sum().drop(labels='SalePrice')\ncols_with_nan = nan_by_col[nan_by_col > 0]\nprint(f'There are {len(cols_with_nan)} columns with missing values:\\n{cols_with_nan.sort_values(ascending=False)}')\nprint(f'Total number of missing entries: {nan_by_col.sum()}')","cfb06c83":"cols_with_typical = ['ExterQual', 'ExterCond', 'HeatingQC', 'KitchenQual']\ndf[cols_with_typical] = df[cols_with_typical].fillna('TA')\ndf['Functional'] = df['Functional'].fillna('Typ')\ndf['Electrical'] = df['Electrical'].fillna('FuseA')","5475a978":"print(df['Utilities'].value_counts())\ndf['Utilities'] = df['Utilities'].fillna('AllPub')","e0e2cd36":"categorical_columns = df.select_dtypes('category').columns\ndf[categorical_columns] = df[categorical_columns].fillna('None')","00646083":"# We have to remove the target column (SalePrice) from the list of numerical columns before imputing\nnumerical_columns = df.select_dtypes('number').columns.drop('SalePrice')\ndf[numerical_columns] = df[numerical_columns].fillna(0)","2aee24f6":"def impute(df):\n    nan_by_col = df.isnull().sum().drop(labels='SalePrice')\n    cols_with_nan = nan_by_col[nan_by_col > 0]\n\n    cols_with_typical = ['ExterQual', 'ExterCond', 'HeatingQC', 'KitchenQual']\n    df[cols_with_typical] = df[cols_with_typical].fillna('TA')\n    df['Functional'] = df['Functional'].fillna('Typ')\n    df['Electrical'] = df['Electrical'].fillna('FuseA')\n\n    df['Utilities'] = df['Utilities'].fillna('AllPub')\n\n    categorical_columns = df.select_dtypes('category').columns\n    df[categorical_columns] = df[categorical_columns].fillna('None')\n\n    numerical_columns = df.select_dtypes('number').columns.drop('SalePrice')\n    df[numerical_columns] = df[numerical_columns].fillna(0)\n    return df","12845d10":"def load_data():\n    # Read data\n    df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\n    df_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')\n    # Merge the splits so we can process them together\n    df = pd.concat([df_train, df_test])\n    # Preprocessing\n    df = clean(df)\n    df = encode(df)\n    df = impute(df)\n    # Reform splits\n    df_train = df.loc[df_train.index, :]\n    df_test = df.loc[df_test.index, :]\n    return df_train, df_test","bb64753b":"df_train, df_test = load_data()","d8f53d3a":"def score_dataset(X, y, model=XGBRegressor(), metric='MAE'):\n    # Label encoding for categoricals\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    if (metric == 'RMSLE'):\n        y = np.log(y)\n    score = cross_val_score(\n        model, X, y, cv=5, scoring=\"neg_mean_squared_error\",\n    )\n    score = -1 * score.mean()\n    if (metric == 'RMSLE'):\n        score = np.sqrt(score)\n    return score","a9cc767c":"X_train = df_train.copy()\ny_train = X_train.pop(\"SalePrice\")\n\nbaseline_score = score_dataset(X_train, y_train, metric=metric)\nprint(f\"Baseline score: {baseline_score:.5f} {metric}\")","6a2927ca":"def make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","1dceeb67":"X_train = df_train.copy()\ny_train = X_train.pop(\"SalePrice\")\n\nmi_scores = make_mi_scores(X_train, y_train)\nmi_scores.round(3)","3bd30a42":"def drop_uninformative(df, mi_scores):\n    return df.loc[:, mi_scores > 0.0]","235344a1":"X_train = df_train.copy()\ny_train = X_train.pop(\"SalePrice\")\nX_train = drop_uninformative(X_train, mi_scores)\nscore_dataset(X_train, y_train, metric=metric)","ddb8a5ec":"X_train = df_train.copy()\ny_train = X_train.pop(\"SalePrice\")","12ccbead":"def label_encode(df):\n    X = df.copy()\n    for col in X.select_dtypes([\"category\"]):\n        X[col] = X[col].cat.codes\n    return X","0fb4d90e":"def mathematical_transforms(df):\n    X = pd.DataFrame()\n    X[\"LivLotRatio\"] = df['GrLivArea'] \/ df['LotArea']\n    X[\"Spaciousness\"] = (df['1stFlrSF'] + df['2ndFlrSF']) \/ df['TotRmsAbvGrd']\n    return X","65af15c6":"def interactions(df):\n    X = pd.get_dummies(df.BldgType, prefix=\"Bldg\")\n    X = X.mul(df.GrLivArea, axis=0)\n    return X","15143cc2":"def counts(df):\n    X = pd.DataFrame()\n    X[\"PorchTypes\"] = df[[\n        \"WoodDeckSF\",\n        \"OpenPorchSF\",\n        \"EnclosedPorch\",\n        \"3SsnPorch\",\n        \"ScreenPorch\",\n    ]].gt(0.0).sum(axis=1)\n    return X","29131f91":"def group_transforms(df):\n    X = pd.DataFrame()\n    X[\"MedNhbdArea\"] = df.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"median\")\n    return X","5e184561":"def pca_inspired(df):\n    X = pd.DataFrame()\n    X[\"Feature1\"] = df.GrLivArea + df.TotalBsmtSF\n    X[\"Feature2\"] = df.YearRemodAdd * df.TotalBsmtSF\n    return X","9510c9d9":"class CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=5)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) \/ len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","8d2e04cc":"def create_features(df_train, df_test=None):\n    X_train = df_train.copy()\n    y_train = X_train.pop(\"SalePrice\")\n    mi_scores = make_mi_scores(X_train, y_train)\n    # Combine splits if test data is given\n    #\n    # If we're creating features for test set predictions, we should\n    # use all the data we have available. After creating our features,\n    # we'll recreate the splits.\n    if df_test is not None:\n        X_test = df_test.copy()\n        X_test.pop(\"SalePrice\")\n        X_train = pd.concat([X_train, X_test])\n\n    X_train = drop_uninformative(X_train, mi_scores)\n    X_train = X_train.join(mathematical_transforms(X_train))\n    #X_train = X_train.join(interactions(X_train))\n    X_train = X_train.join(counts(X_train))\n    X_train = X_train.join(group_transforms(X_train))\n    X_train = X_train.join(pca_inspired(X_train))\n    X_train = label_encode(X_train)\n\n    # Reform splits\n    if df_test is not None:\n        X_test = X_train.loc[df_test.index, :]\n        X_train.drop(df_test.index, inplace=True)\n\n    # Target encoding\n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    X_train = X_train.join(encoder.fit_transform(X_train, y_train, cols=[\"MSSubClass\"]))\n    if df_test is not None:\n        X_test = X_test.join(encoder.transform(X_test))\n        return X_train, X_test\n    else:\n        return X_train\n\ndf_train, df_test = load_data()\nX_train = create_features(df_train)\ny_train = df_train[\"SalePrice\"]\nscore_dataset(X_train, y_train, metric=metric)","c5885422":"X_train = create_features(df_train)\ny_train = df_train[\"SalePrice\"]\n\nxgb_params = dict(\n    max_depth=6,           # maximum depth of each tree - try 2 to 10\n    learning_rate=0.01,    # effect of each tree - try 0.0001 to 0.1\n    n_estimators=1000,     # number of trees (that is, boosting rounds) - try 1000 to 8000\n    min_child_weight=1,    # minimum number of houses in a leaf - try 1 to 10\n    colsample_bytree=0.7,  # fraction of features (columns) per tree - try 0.2 to 1.0\n    subsample=0.7,         # fraction of instances (rows) per tree - try 0.2 to 1.0\n    reg_alpha=0.5,         # L1 regularization (like LASSO) - try 0.0 to 10.0\n    reg_lambda=1.0,        # L2 regularization (like Ridge) - try 0.0 to 10.0\n    num_parallel_tree=1,   # set > 1 for boosted random forests\n)\n\nxgb = XGBRegressor(**xgb_params)\nscore_dataset(X_train, y_train, xgb, metric=metric)","d7b389f8":"def objective(trial):\n    xgb_params = dict(\n        max_depth=trial.suggest_int(\"max_depth\", 2, 20),\n        learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n        n_estimators=trial.suggest_int(\"n_estimators\", 1000, 8000),\n        min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 10),\n        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n        subsample=trial.suggest_float(\"subsample\", 0.2, 1.0),\n        reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n        reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n    )\n    xgb = XGBRegressor(**xgb_params)\n    return score_dataset(X_train, y_train, xgb, metric=metric)\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=50)\nxgb_params = study.best_params","1e0d176a":"X_train, X_test = create_features(df_train, df_test)\ny_train = df_train[\"SalePrice\"]\n\nxgb = XGBRegressor(**xgb_params)\n# XGB minimizes MSE, but competition loss is RMSLE\n# So, we need to log-transform y to train and exp-transform the predictions\nxgb.fit(X_train, np.log(y_train))\npredictions = np.exp(xgb.predict(X_test))\noutput = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})\noutput.to_csv('my_submission.csv', index=False)","e553e064":"The *Exterior2nd* column contains some typos. Let's fix them.","94f30f9b":"## Imputation\nLet's first look at how many missing values we have and the columns where they're located.","4d12d1f6":"3 of the numerical columns in the data (*MSSubClass*, *OverallQual*, and *OverallCond*) are actually categorical.\n\nWe define the nominal and ordinal features' categories and levels according to the data description.","9ffc4e5d":"For the rest of the categorical columns, we'll just impute with *\\\"None\\\"*.","6e149688":"## Encoding","37e35f5d":"# Feature Selection","ad4b047a":"We will impute with the TA\/Typical\/Average\/Normal level whenever the column has that level but lacks a *NA\/None* level.","e72064c2":"All that's left are numerical columns. We will impute 0 for these.","2dbf23f0":"This function lets us drop any features with an MI score of 0.","77bb4401":"And now we can convert the categorical columns to their respective categorical types.","6044263c":"We will use optuna to optimize our XGBoost regressor's hyperparameters.","be9d2b3c":"To find out which features are more useful for predicting the target variable, we calculate the MI (Mutual Information) score for each one of them.","737ec540":"We'll concatenate training and test data to preprocess them together.","fc360c94":"# Read and Preprocess Data","d38aad2c":"There are values that are not consistent with the provided *data_description.txt* file. Let's fix these too.","9701f9b6":"# Import Libraries","8a769764":"Let's set some defaults.","da5bbd78":"# Feature Engineering","24da942e":"## Inspect Values of Numerical Columns\nLet's look at the range of numerical values to see if there are any problems.","9e3d20eb":"## Create Final Feature Set","fd2f5c8e":"Finally, we compile all of these steps in a function for later reuse.","99871af2":"## Inspect Values of Categorical Columns","51b509af":"The data has 80 columns (plus the target column we're trying to predict - *SalePrice*).","76ff180e":"# Train Model and Create Submission","cc9239f8":"All of these steps are summarized in this function.","d3cc9df6":"# Establish a Baseline Score","3162288a":"All the values of the *Utilities* column are equal to *AllPub* except one. So we'll impute with that.","b0656224":"We notice that the *GarageYrBlt* column which describes the year the garage was built has impossible values (as high as 2207).\n\nLet's replace those values with the year the house was built.","e48244ba":"# Hyperparameter Tuning"}}