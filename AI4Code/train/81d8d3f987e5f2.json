{"cell_type":{"e0ea3141":"code","0a7975fa":"code","4fd6666c":"code","8dfdb80b":"code","b355dd9c":"code","63bc0354":"code","fdf734f4":"code","a53cb9c5":"code","448eecf8":"code","e3385e2e":"code","d35e2a1a":"code","f7bfe355":"code","7df428a6":"code","651d0ade":"code","e1b28073":"code","5afdb0f4":"code","70be3bef":"code","2e6835b5":"code","c58aa4d1":"code","65875ce2":"code","df7289f8":"code","2b5998a8":"code","92111e09":"code","b8025376":"code","041c8945":"code","eeb6fa25":"code","c48a7b7f":"code","f639f082":"code","711a2666":"code","19d8c9e3":"code","9961d838":"code","52a25987":"code","fbbda105":"code","4152b76a":"code","c705c93f":"code","54b31266":"code","09e8559b":"code","51e38ed7":"code","7717ca62":"code","d7f8fd4d":"code","0e160c7d":"code","44e6776c":"code","c9a9535a":"code","0ff5682a":"code","5f837e21":"code","e3dca3a6":"code","b73e3469":"code","36518389":"code","18e5ab04":"code","62683f8a":"code","d80221fc":"code","81485599":"code","2bd449ee":"code","37c48d70":"code","2929cd6a":"code","2e4d93f7":"markdown","08827aba":"markdown","7b3d0d21":"markdown","bc3c0f46":"markdown","371d7d32":"markdown","2d1c673a":"markdown","21fcc384":"markdown","2ff263dd":"markdown","071c97e8":"markdown","8cb28a90":"markdown","c177b3b6":"markdown","d4081b9f":"markdown","8a1b1326":"markdown","ccf6b0ad":"markdown","f630056e":"markdown","1d1294a5":"markdown","e5cfdd3e":"markdown","efdad3f5":"markdown","efe8daae":"markdown","ce220b32":"markdown","42fc5d18":"markdown","0b799c18":"markdown","bb75823a":"markdown"},"source":{"e0ea3141":"## Importing the basic libraries\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline","0a7975fa":"## Importind the data into the system for futher analysis\nprint(os.listdir(\"..\/input\"))\ncol = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\nhousing=pd.read_csv('..\/input\/housing.csv',delim_whitespace=True, names=col)","4fd6666c":"housing.head()","8dfdb80b":"## Check the spread of the data and identify potential outliers at glance\nhousing.describe()","b355dd9c":"# Verify the type and categories of data and is there any missing values present that we need to take care\nhousing.info()","63bc0354":"housing.hist(bins=20,figsize=(15,15))","fdf734f4":"fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in housing.items():\n    sns.boxplot(y=k, data=housing, ax=axs[index])\n    index += 1","a53cb9c5":"CRIM_data=pd.DataFrame(housing.iloc[:,0])\nprint(CRIM_data.describe())\nCRIM_data.hist()\nplt.title(\"Original CRIM Data\")\nmin_cutoff=3.613524-3*8.601545\nprint(\"Minimum Cut off Value\",min_cutoff)\nmax_cutoff=3.613524+3*8.601545\nprint(\"Maximum Cut Off Value\",max_cutoff)\n# Values beyond 29.4 must be dropped becasue they are potential outliers so dropping these 4 values\nhousing_df = housing.drop(housing[housing.CRIM>29.4].index)\nprint(\"Final shape of the dataset\",housing_df.shape)","448eecf8":"CRIM_data_new=pd.DataFrame(housing_df.iloc[:,0])\nCRIM_data_new.hist()\nplt.xlabel(\"Value of Criminal Rates\")\nplt.title(\"New CRIM Data\")","e3385e2e":"ZN_data=pd.DataFrame(housing_df.iloc[:,1])\nprint(ZN_data.describe())\nZN_data.hist()","d35e2a1a":"min_cutoff=11.546185-3*23.464449\nprint(\"Minimum Cut off Value\",min_cutoff)\nmax_cutoff=11.546185+3*23.464449\nprint(\"Maximum Cut Off Value\",max_cutoff)\nhousing_df_N = housing_df.drop(housing_df[housing.ZN>=82].index)\nCRIM_data_new=pd.DataFrame(housing_df_N.iloc[:,1])\nCRIM_data_new.hist()","f7bfe355":"housing_df_N['INDUS'].hist()\nprint(housing_df_N['INDUS'].describe())","7df428a6":"housing_df_N['MEDV'].hist()","651d0ade":"housing_df_N['MEDV'].describe()","e1b28073":"lower_limit=22.370248-3*8.802114\nupper_limit=22.370248+3*8.802114\nprint(upper_limit,lower_limit) ## No action been been taken due to small data size\nhousing_df_M = housing_df_N.drop(housing_df_N[housing_df_N.MEDV>48.77659].index)","5afdb0f4":"housing_df_M['B'].hist()","70be3bef":"housing_df_M['B'].describe()","2e6835b5":"upper_limit=358.160764+3*88.205368\nlower_limit=358.160764-3*88.205368\nprint(\"Upper cut off value\",upper_limit)\nprint(\"Lower Cut off Value\",lower_limit) ## I will come back later on this based on model under fitting and over fitting","c58aa4d1":"housing_df_O = housing_df_M.drop(housing_df_M[housing_df_M.B<92].index)","65875ce2":"housing_df_O['B'].hist()","df7289f8":"housing_df_O['NOX'].hist()","2b5998a8":"housing_df_O['NOX'].describe()","92111e09":"housing_df_O['RM'].describe()","b8025376":"housing_df_O['AGE'].hist()","041c8945":"housing_df_O['AGE'].unique()\nhousing_df_P = housing_df_O.drop(housing_df_O[housing_df_O.MEDV>48.77659].index)","eeb6fa25":"housing_df_P.shape","c48a7b7f":"housing_df_O['DIS'].describe()","f639f082":"housing_df_P['LSTAT'].describe()","711a2666":"lower_limit=12.522477-3*6.699438\nupper_limit=12.522477+3*6.699438\nprint(\"Lower Limit\",lower_limit)\nprint(\"Upper Limit\",upper_limit)","19d8c9e3":"housing_df_Q = housing_df_P.drop(housing_df_P[housing_df_P.MEDV>32.620791].index) ## rm the value>32.620791","9961d838":"housing_df_Q.shape","52a25987":"plt.figure(figsize = (15,12))\nsns.heatmap(data=housing_df_Q.corr(), annot=True,linewidths=.8,cmap='Blues')","fbbda105":"sns.pairplot(housing_df_Q,x_vars=[\"CRIM\",\"ZN\",\"INDUS\"],y_vars =[\"MEDV\"], kind=\"reg\",height=6)","4152b76a":"sns.pairplot(housing_df_Q,x_vars=[\"NOX\",\"RM\",\"AGE\"],y_vars =[\"MEDV\"], kind=\"reg\",height=6)","c705c93f":"sns.pairplot(housing_df_Q,x_vars=[\"DIS\",\"RAD\",\"TAX\"],y_vars =[\"MEDV\"], kind=\"reg\",height=6)","54b31266":"sns.pairplot(housing_df_Q,x_vars=[\"PTRATIO\",\"B\",\"LSTAT\"],y_vars =[\"MEDV\"], kind=\"reg\",height=6)","09e8559b":"plt.figure(figsize = (9,9))\nsns.heatmap(data=housing_df_Q.corr(), annot=True,linewidths=.8,cmap='Blues')","51e38ed7":"housing_df_Q.head()","7717ca62":"import numpy as np\ndef split_train_test(data,test_ratio):\n    shuffled_indicies=np.random.permutation(len(data))\n    test_set_size=int(len(data)*test_ratio)\n    test_indicies=shuffled_indicies[:test_set_size]\n    train_indicies=shuffled_indicies[test_set_size:]\n    return data.iloc[test_indicies],data.iloc[train_indicies]\ntest_set,train_set=split_train_test(housing_df_Q,0.2)","d7f8fd4d":"def training_and_testing_set(test_set,train_set):\n    test_set_x=test_set.iloc[:,:-1]\n    test_set_y=test_set.iloc[:,-1]\n    train_set_x=train_set.iloc[:,:-1]\n    train_set_y=train_set.iloc[:,-1]\n    return test_set_x,test_set_y,train_set_x,train_set_y\ntest_set_x,test_set_y,train_set_x,train_set_y=training_and_testing_set(test_set,train_set)\nprint(test_set_x.shape,test_set_y.shape,train_set_x.shape,train_set_y.shape)","0e160c7d":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nlin_reg=LinearRegression()\nlin_reg.fit(train_set_x,train_set_y)\ny_pred=lin_reg.predict(test_set_x)\nmserr=mean_squared_error(test_set_y,y_pred)\nprint(\"Mean Squared Error\",mserr)\nroot_mean_squared_error=np.sqrt(mserr)\nprint(\"Root Mean Squared Error\",root_mean_squared_error)","44e6776c":"import statsmodels.formula.api as sm","c9a9535a":"X=housing_df_Q.iloc[:,:-1].values\ny=housing_df_Q.iloc[:,-1].values","0ff5682a":"X.shape","5f837e21":"X1=np.append(arr=np.ones((403, 1),int).astype(int),values = X,axis =1)","e3dca3a6":"# soring the optimal dataset in X_opt\nX_opt = X1[:,[0,1,2,3,4,5,6,7,8,9,10,11,12,13]]\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()","b73e3469":"X_opt = X1[:,[0,1,2,5,6,8,9,10,11,13]]\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()","36518389":"X_opt = X1[:,[0,1,5,6,8,9,10,11,13]]\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()","18e5ab04":"X_train, X_test, y_train, y_test = train_test_split(X_opt,y,test_size = 0.3,random_state = 1)\nregressor.fit(X_train,y_train)","62683f8a":"y_pred = regressor.predict(X_test)\n\nprint(\"RMSE: %.2f\"% np.sqrt(((y_pred - y_test) ** 2).mean()))","d80221fc":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=200,random_state=0)","81485599":"rf.fit(train_set_x,train_set_y)","2bd449ee":"y_pred1=rf.predict(test_set_x)","37c48d70":"mserr=mean_squared_error(test_set_y,y_pred1)\nprint(\"Mean Squared Error\",mserr)\nroot_mean_squared_error=np.sqrt(mserr)\nprint(\"Root Mean Squared Error\",root_mean_squared_error)","2929cd6a":"from sklearn.metrics import r2_score\nr2 = r2_score(test_set_y,y_pred1)\nr2","2e4d93f7":"# 2 Exploratory Data analysis","08827aba":"**Analysis From above step:**\n> This reflect  that data is quite clean and most of the variables are float and few of them are integers, no imputations or filling up values required.","7b3d0d21":"** Linear Regression Using Backward Elimination **","bc3c0f46":"**B variable**","371d7d32":"\nThis dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Mass. It was obtained from the StatLib archive (http:\/\/lib.stat.cmu.edu\/datasets\/boston) The dataset is small in size with only 506 cases and 14 features. The details about this dataset available on -https:\/\/www.cs.toronto.edu\/~delve\/data\/boston\/bostonDetail.html The purpose of the database is to leverage the available data to predict the prices of houses in Boston using machine learning algorithms.\nThis note book contains\n1. Data Exploration \n2. Data visualization\n3. Data Preprocessing\n4. Hyper Tuning of the parameters\n5. Model Builing using Various techniques\n6 .Result Prediction\n\n","2d1c673a":"**2. ZN Variable**** Observation","21fcc384":"From heat map we have observed that TAX ,RAD ,CRIM are highly correlated and CHAS , PTRATIO and B are quite less correlated\nwrt .to target variable RM positively and LSAT is negatively correlated and hence their impact\n### In case prediction is not up to mark need to remove one of these highly correlated variable","2ff263dd":"**Observations:**\n**CRIM Variable**\n\nIt has very short data bandwidth and correlation is not that segnificant being +0.39 which makes it an important parameter for prediction, but it have quite heavy outliers that needs to be handled carefully.","071c97e8":"**Regression Model Using Linear Regression**","8cb28a90":"1. [\"CRIM\",\"ZN\",\"INDUS\"] features are reflect  weak correlation with MDEV variable\n2. CRIM has negative correlation with prices. The areas with lower rate of crime has high prices and vice versa.and there are few areas are with high crime rate else most of the areas has lower crime rate.\n3. ZN - proportiion residential land zoned --this feature posses positive correlation with prices. More the residelntial land zone more higher the housing prices.And also it shows that threr are a large group of area has low prportion of residential land zone.\n 4. INDUS- proportion of non retail bussiness acre per town - it shows negative correlation. As the proportion decreases the housing prices lowers.","c177b3b6":"**Conclusion for CRIM Data variable:** Inspite of having a very small size data, it very important to remove the potential outlier from the data set, Incase it is not effecting to much target varibles, if it is further analysis is required, I will remove the excess data +\/- 3 standard Deviation from the mean\n    ","d4081b9f":"5. ** MDEV**\nTarget variable","8a1b1326":"** Linear Regression Using Random Forest**","ccf6b0ad":"3. **CHAS Variable**\nThis is a categorical variablle and has only 2 distinct values, and this column is already dummified so no need to take any action upon it","f630056e":" 1. NOX- Nitric oxide concentration - it shows negatve correaltion .We can see the areas with high cocentration has lower housing prices. lower the pollution of air , higher the housing prices.\n 2. RM- Avg number of room per house. -  it is obivious that as number of rooms increases the area of house increase and prices will be more. the sme trend we can see in the plot. Also it has strong correlation with prices as compared to other parameters we saw upto now.\n 3. AGE- proportion of owner-occupied units built prior to 1940 - this feature shows negative correlation . the older the property lower the housing prices . we can seee the dataset has slighlty more number of old houses.","1d1294a5":"##Let's explore each of the variables and see how are they affecting each i'e check correlation for each of the varibles by plotting box plot and analysing how does the shape of variables are and the test way to visualize is to look at the box plot for each of the variables","e5cfdd3e":"It seems that many variables like Age,B,MEDV,RAD ..etc are caped, It seems this sample size might not be representative of sample space, ","efdad3f5":"Till now we checked the individual relations with feature variable. And almost all variable shows linear relationship with target variable. Not every feature has strong correlation. Eventually we will find out which are not the good features for prediction. As the target variable has continous values we wil go for regression techniques. First lets try with Multiple linear regression.\n\nThe preassumption for Linear regression is that the features used for moel should not be correlated . From scatter plots above we checked linear relationship of individual feature with target variable.\nLets check the correlation of all features with each other using corrrelaton matrix","efe8daae":" 1. DIS-    weighted distances to five Boston employment centres - This feature shows Positive correlation with housing prices. the areas near to the employment centres\/ work places has high prices which is obivious trend we do see in housing prices.\n 2. RAD-index of accessibility to radial highways- there is negative correlation with prices.Alos we can see that a lot of areas has low index and few areas has high index for highway accessibility. Mthe plot shows people do not prefer houses near the highways.\n 3.TAX- full-value property-tax rate per USD 10,000 -Tax Rate shows negative correlation with housing prices. Higher the property tax lower the prices in that area. the housing prics are high where the propery taxes are low. people prefer areas with lower property tax.","ce220b32":"4. **INDUS Variable**\n     Since dataset is extremely small so thinking not to remove the outliers, but when dataset is large it is advisible to remove few outliers from the data to make the mode fit","42fc5d18":"** Considering the continous value of the housing price targer variable this above problem is considered to be a regression task**","0b799c18":"We can see there is improvment in regressor performance after elimination of features. there is improvment in RMSE values And there is still scope of improvment.\n\nLets try out the Random forest regression model on data. because the LSTAT feature shows some curvilinear nature and few shows direct correation. The dataset is combination of Linear and non linear Features.","bb75823a":"1. PTRATIO- pupil-teacher ratio by town - this feature has negative correltion with housing prices. \n2. B - proportion of blacks by town - It shows positive correlaton with prices.\n3.LSTAT-LSTAT - % lower status of the population - This shows strong postive correation . it does not show direct correltion. the plot shows somewhat curvilinear nature."}}