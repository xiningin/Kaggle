{"cell_type":{"bc2de6d5":"code","4755a22d":"code","9b71eda4":"code","6fa0c723":"code","2bcdba3e":"code","d8e1a801":"code","d48cb03d":"code","5320d5f1":"code","66bb89fe":"code","8c1197bc":"code","f14c250d":"code","6f3ef89a":"code","202f4578":"code","87520e4b":"code","17fa295d":"code","d0e6f606":"code","3f29c3a2":"code","75c7c6d9":"code","3d9b093e":"code","ec6e5072":"code","171f827f":"code","0bf4758c":"code","3a3d1174":"markdown"},"source":{"bc2de6d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4755a22d":"!pip install transformers\n\nimport time\nimport sys\nimport copy\nimport torch \nimport numpy as np\nfrom scipy.sparse import *\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport pyarrow as pa\n\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\nimport torch.nn.functional as F\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import Dataset,DataLoader\nfrom transformers import DistilBertConfig,DistilBertTokenizer,DistilBertModel\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelBinarizer\n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")","9b71eda4":"# data = pd.read_csv('..\/input\/vn-sentiment\/data_sure - data.csv')\ndata = pd.read_csv('..\/input\/data-csv\/convert_cmt.csv')","6fa0c723":"data","2bcdba3e":"data['new'] = list(pd.get_dummies(data['nam_rate']).get_values())","d8e1a801":"data['content'] = data['convert_content'].values.astype('str')","d48cb03d":"data","5320d5f1":"## Feature engineering to prepare inputs for BERT....\n# Y = train[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].astype(float)\n# X = train['comment_text']\n\nX = data['convert_content']\nY = data['new']\n\ntrain, test = train_test_split(data, stratify=data['nam_rate'], test_size=0.2, random_state=42)","66bb89fe":"X_train = train['content'].values\nX_test = test['content'].values\ny_train = train['nam_rate'].values - 1\ny_test = test['nam_rate'].values - 1 \n# y_train_new = np.zeros((len(y_train), 5))\n# y_test_new = np.zeros((len(y_test), 5))\n","8c1197bc":"y_train","f14c250d":"# for i, y in enumerate(y_train):\n#     y_train_new[i] = y\n    \n# for i, y in enumerate(y_test):\n#     y_test_new[i] = y","6f3ef89a":"def accuracy_thresh(y_pred, y_true, thresh:float=0.4, sigmoid:bool=True):\n    \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n    if sigmoid: y_pred = y_pred.sigmoid()\n#     return ((y_pred>thresh)==y_true.byte()).float().mean().item()\n    return np.mean(((y_pred>thresh).float()==y_true.float()).float().cpu().numpy(), axis=1).sum()\n#Expected object of scalar type Bool but got scalar type Double for argument #2 'other'","202f4578":"config = DistilBertConfig(vocab_size=30522, dim=768,dropout=0.1,num_labels=5, n_layers=15, n_heads=12, hidden_dim=3072)","87520e4b":"class DistilBertForSequenceClassification(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.num_labels = config.num_labels\n\n        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased')\n        self.pre_classifier = nn.Linear(config.hidden_size, config.hidden_size)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        self.dropout = nn.Dropout(config.seq_classif_dropout)\n\n        nn.init.xavier_normal_(self.classifier.weight)\n\n    def forward(self, input_ids=None, attention_mask=None, head_mask=None, labels=None):\n        distilbert_output = self.distilbert(input_ids=input_ids,\n                                            attention_mask=attention_mask,\n                                            head_mask=head_mask)\n        hidden_state = distilbert_output[0]                    \n        pooled_output = hidden_state[:, 0]                   \n        pooled_output = self.pre_classifier(pooled_output)   \n        pooled_output = nn.ReLU()(pooled_output)             \n        pooled_output = self.dropout(pooled_output)        \n        logits = self.classifier(pooled_output) \n        return F.softmax(logits)","17fa295d":"max_seq_length = 60\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n\n\nclass text_dataset(Dataset):\n    def __init__(self,x,y, transform=None):\n        \n        self.x = x\n        self.y = y\n        self.transform = transform\n        \n    def __getitem__(self,index):\n        \n        tokenized_comment = tokenizer.tokenize(self.x[index])\n        \n        if len(tokenized_comment) > max_seq_length:\n            tokenized_comment = tokenized_comment[:max_seq_length]\n            \n        ids_review  = tokenizer.convert_tokens_to_ids(tokenized_comment)\n\n        padding = [0] * (max_seq_length - len(ids_review))\n        \n        ids_review += padding\n        \n        assert len(ids_review) == max_seq_length\n        \n        #print(ids_review)\n        ids_review = torch.tensor(ids_review)\n        \n        hcc = self.y[index] # toxic comment        \n#         list_of_labels = [torch.from_numpy(hcc)]\n        \n        \n        return ids_review, hcc\n    \n    def __len__(self):\n        return len(self.x)\n ","d0e6f606":"X_train[3]","3f29c3a2":"tokenizer.tokenize(X_train[3])\n","75c7c6d9":"text_dataset(X_train, y_train).__getitem__(6)[0]   ### Testing index 6 to see output","3d9b093e":"batch_size = 32\n\n\ntraining_dataset = text_dataset(X_train,y_train)\n\ntest_dataset = text_dataset(X_test,y_test)\n\ndataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=False),\n                   'val':torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n                   }\ndataset_sizes = {'train':len(X_train),\n                'val':len(X_test)}\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = DistilBertForSequenceClassification(config)\nmodel.to(device)\n\nprint(device)\n","ec6e5072":"from tqdm import *\nfrom sklearn.metrics import f1_score\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=2):\n    \n    train_losses = []\n    valid_losses = []\n    avg_train_losses = []\n    avg_valid_losses = []\n    train_lost_iter = []\n    valid_lost_iter = []\n    best_val_score = 1000\n\n    dataloader_train = dataloaders_dict['train']\n    dataloader_valid = dataloaders_dict['val']\n    \n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        train_pre = []\n        valid_pre = []\n\n        with tqdm(total=len(dataloader_train)) as pbar:\n            for step, (ids_review, hcc) in enumerate(dataloader_train):\n                optimizer.zero_grad()\n                ids_review = ids_review.to(device)\n                hcc = hcc.to(device)\n                outputs = model(ids_review)\n                train_pre += list(np.argmax(outputs.cpu().detach().numpy(), axis=1))\n                loss = criterion(outputs, hcc.long())\n                loss.backward()\n                optimizer.step()\n                train_losses.append(loss.item())\n                train_lost_iter.append(loss.item())\n                pbar.update()\n\n        model.eval()\n        with tqdm(total=len(dataloader_valid)) as pbar:\n            for step, (ids_review, hcc) in enumerate(dataloader_valid):\n                ids_review = ids_review.to(device)\n                hcc = hcc.to(device)\n                outputs = model(ids_review)\n                loss = criterion(outputs, hcc.long())\n                valid_losses.append(loss.item())\n                valid_lost_iter.append(loss.item())\n                valid_pre += list(np.argmax(outputs.cpu().detach().numpy(), axis=1))\n\n                pbar.update()\n                \n        train_pre = np.array(train_pre).reshape(-1,)\n        valid_pre = np.array(valid_pre).reshape(-1,)\n        train_f1 = f1_score(y_train, train_pre, average='weighted')\n        valid_f1 = f1_score(y_test, valid_pre, average='weighted')\n        print(train_f1, valid_f1)\n\n        \n        scheduler.step()\n        # print training\/validation statistics \n        # calculate average loss over an epoch\n        train_loss = np.average(train_losses)\n        valid_loss = np.average(valid_losses)\n        avg_train_losses.append(train_loss)\n        avg_valid_losses.append(valid_loss)\n        \n        if valid_f1 < best_val_score:\n            best_val_score = valid_f1\n            torch.save(model.state_dict(), 'distilbert_model_weights.pth')\n            \n        epoch_len = len(str(num_epochs))        \n        print_msg = (f'[{epoch:>{epoch_len}}\/{num_epochs:>{epoch_len}}] ' +\n                     f'train_loss: {train_loss:.5f} ' +\n                     f'valid_loss: {valid_loss:.5f}')\n        print(print_msg)\n        # clear lists to track next epoch\n        train_losses = []\n        valid_losses = []\n        \n    return model\n \nprint('done')","171f827f":"lrlast = .001\nlrmain = 3e-5\n#optim1 = torch.optim.Adam(\n#    [\n#        {\"params\":model.parameters,\"lr\": lrmain},\n#        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n#       \n#   ])\n\noptim1 = torch.optim.Adam(model.parameters(),lrmain)\n\noptimizer_ft = optim1\ncriterion = nn.CrossEntropyLoss()\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.4)","0bf4758c":"model_ft1 = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=10)","3a3d1174":"## Make_Predictions"}}