{"cell_type":{"45492c9e":"code","095f4227":"code","b41d478c":"code","9ba199e1":"code","2911f845":"code","4764ece9":"code","220bd042":"code","46f39579":"code","8f134a8f":"code","a41bfaee":"code","430afae4":"code","e1c4325c":"code","577c0147":"code","c598864b":"code","7886be83":"code","1c9e625a":"code","5e971f0e":"code","c5cd3446":"markdown","53eb963f":"markdown","917d4b81":"markdown","75772a58":"markdown","802de3f9":"markdown","26c689d1":"markdown","8deacb99":"markdown"},"source":{"45492c9e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","095f4227":"import numpy as np \nimport pandas as pd \nimport os\nimport gc\nimport logging\nimport datetime\nimport warnings\nimport pickle\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nimport time","b41d478c":"from tensorflow.compat.v1.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom tensorflow.compat.v1.keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, Flatten\nfrom tensorflow.compat.v1.keras.layers import Conv1D, MaxPooling1D\nfrom tensorflow.compat.v1.keras.preprocessing import text, sequence\nfrom tensorflow.compat.v1.keras.losses import binary_crossentropy\nfrom tensorflow.compat.v1.keras import backend as K\nimport tensorflow.compat.v1.keras.layers as L\nfrom tensorflow.compat.v1.keras import initializers, regularizers, constraints, optimizers, layers\nfrom tensorflow.compat.v1.keras.layers import Layer\nfrom tensorflow.compat.v1.keras.models import Model\nfrom tensorflow.compat.v1.keras.optimizers import Adam\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.compat.v1.keras.preprocessing.text import Tokenizer\nfrom tensorflow.compat.v1.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.compat.v1.keras.utils import plot_model\nfrom tensorflow.compat.v1.keras.callbacks import TensorBoard","9ba199e1":"def get_coefs(word, *arr):\n    ''' get word and its weight vector from embeddings'''\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    ''' load weights from two embeddings in dict'''\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)","2911f845":"def build_embedding_matrix(word_index, path):\n    '''\n        credits to: https:\/\/www.kaggle.com\/christofhenkel\/keras-baseline-lstm-attention-5-fold\n        buiid embedding matrix for comment text\n    '''\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, EMB_MAX_FEAT))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n        except:\n            embedding_matrix[i] = embeddings_index[\"unknown\"]\n            \n    del embedding_index\n    gc.collect()\n    return embedding_matrix","4764ece9":"def tokenizer_text(train, test):\n    '''\n        credits go to: https:\/\/www.kaggle.com\/tanreinama\/simple-lstm-using-identity-parameters-solution\/ \n        tokenize the train and test comment, filter out some special characters\n    '''\n\n    punct = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'\n    punct += '\u00a9^\u00ae` <\u2192\u00b0\u20ac\u2122\u203a \u2665\u2190\u00d7\u00a7\u2033\u2032\u00c2\u2588\u00bd\u00e0\u2026\u201c\u2605\u201d\u2013\u25cf\u00e2\u25ba\u2212\u00a2\u00b2\u00ac\u2591\u00b6\u2191\u00b1\u00bf\u25be\u2550\u00a6\u2551\u2015\u00a5\u2593\u2014\u2039\u2500\u2592\uff1a\u00bc\u2295\u25bc\u25aa\u2020\u25a0\u2019\u2580\u00a8\u2584\u266b\u2606\u00e9\u00af\u2666\u00a4\u25b2\u00e8\u00b8\u00be\u00c3\u22c5\u2018\u221e\u2219\uff09\u2193\u3001\u2502\uff08\u00bb\uff0c\u266a\u2569\u255a\u00b3\u30fb\u2566\u2563\u2554\u2557\u25ac\u2764\u00ef\u00d8\u00b9\u2264\u2021\u221a'\n    tokenizer = Tokenizer(filters=punct) \n    tokenizer.fit_on_texts(list(train[COMMENT_TEXT_COL]))\n    # it is wordindex dictionary so every word gets a unique integer value. \n    # 0 is reserved for padding. So lower integer means more frequent word\n    word_index = tokenizer.word_index\n    # it takes each word in the text and \n    # replaces it with its corresponding integer value from the word_index dictionary\n    X_train = tokenizer.texts_to_sequences(list(train[COMMENT_TEXT_COL]))\n    X_test = tokenizer.texts_to_sequences(list(test[COMMENT_TEXT_COL]))\n    # used to ensure that all sequences in a list have the same length. \n    # By default this is done by padding 0 in the beginning of each sequence\n    X_train = pad_sequences(X_train, maxlen=MAX_LEN)\n    X_test = pad_sequences(X_test, maxlen=MAX_LEN)\n    \n    return X_train, X_test, word_index","220bd042":"def build_embeddings(word_index):\n    '''\n        build embedding matrix for each of the embedding and combine them\n    '''\n    embedding_matrix = np.concatenate(\n        [build_embedding_matrix(word_index, f) for f in EMB_PATHS], axis=-1) \n    return embedding_matrix","46f39579":"def load_data():\n    '''load input data'''\n    train = pd.read_csv(os.path.join(JIGSAW_PATH,'train.csv'), index_col='id')\n    test = pd.read_csv(os.path.join(JIGSAW_PATH,'test.csv'), index_col='id')\n    y_train = np.where(train['target'] >= 0.5, True, False) * 1\n    X_train, X_test, word_index = tokenizer_text(train, test)\n    embedding_matrix = build_embeddings(word_index)\n    del train,test\n    gc.collect()\n    return X_train,y_train, X_test, word_index, embedding_matrix","8f134a8f":"# declar model parameters and embedding related variables\nEMB_MAX_FEAT = 300\nMAX_LEN = 220\nBATCH_SIZE = 512\nNUM_EPOCHS = 2\nCOMMENT_TEXT_COL = 'comment_text'\nEMB_PATHS = [\n    '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec',\n    '..\/input\/glove840b300dtxt\/glove.840B.300d.txt'\n]\nJIGSAW_PATH = '..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/'","a41bfaee":"def build_model(embedding_matrix, fold_n=0):\n    '''\n    function to define architecture of CNN based text classification model\n    '''\n    inp = Input(shape = (MAX_LEN,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(inp)\n    # Spatial dropout works same function as Dropout, \n    # however it drops entire 1D feature maps instead of individual elements\n    x = SpatialDropout1D(rate=0.2)(x)\n    # add conv layers\n    x = Conv1D(filters=128, kernel_size=2, activation='relu', padding='same')(x)\n    x = MaxPooling1D(pool_size=5, padding='same')(x)\n    x = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(x)\n    x = MaxPooling1D(pool_size=5, padding='same')(x)\n    x = Conv1D(filters=128, kernel_size=4, activation='relu', padding='same')(x)\n    x = MaxPooling1D(pool_size=5, padding='same')(x)\n    x = Conv1D(filters=128, kernel_size=5, activation='relu', padding='same')(x)\n    x = MaxPooling1D(pool_size=5, padding='same')(x)    \n    x = Flatten()(x)\n    x = Dropout(rate=0.1)(Dense(units=128, activation='relu') (x))\n    result = Dense(units=1, activation=\"sigmoid\")(x)\n    \n    model = Model(inputs=inp, outputs=result)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n    return model","430afae4":"def run_model(X, y,X_test, embedding_matrix, word_index):\n    '''\n        function to run CNN based text classification model. It \n        evaluate the test dataset based on averaged out prediction given\n        by 5 fold training.\n    '''\n    predictions = np.zeros((len(X_test), 1))\n    n_fold = 5\n    # tensor board callback to store logs\n    tensorboard_callback = TensorBoard(\"logs\")\n    # early stopping criterion\n    early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n    # define stratified K fold \n    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=11)\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        # separate train and validation data\n        X_train, X_valid = X[train_index], X[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n        # build model\n        model = build_model(embedding_matrix, fold_n)\n        # model checkpoints to save model at each fold\n        file_path = f\"best_model_fold_{fold_n}.hdf5\"\n        check_point = ModelCheckpoint(file_path, monitor=\"val_loss\", verbose=1,save_best_only=True, mode=\"min\")\n        # training\n\n        model.fit(\n            X_train,y_train,\n            batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=2,\n            validation_data=(X_valid, y_valid),\n            callbacks=[tensorboard_callback,early_stop,check_point] \n        )\n\n        predictions+=model.predict(X_test, batch_size=2048)\n        print(predictions)\n        del model\n        gc.collect()    \n    # average the predictions as per number of folds training   \n    preds = predictions\/n_fold\n    return preds","e1c4325c":"def submit(sub_preds):\n    ''' \n        function to create predictions in acceptable format for jigsaw competetion\n    '''\n    submission = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv', index_col='id')\n    submission['prediction'] = sub_preds\n    submission.reset_index(drop=False, inplace=True)\n    submission.to_csv('submission.csv', index=False)","577c0147":"# import data\nX_train, y_train,X_test, word_index,embedding_matrix = load_data()\nmodel = build_model(embedding_matrix)\nmodel.summary()","c598864b":"# plot model\nplot_model(model, to_file='model_plot.png',show_layer_names=True)","7886be83":"del model\ngc.collect()","1c9e625a":"sub_preds = run_model(X_train, y_train,X_test, embedding_matrix, word_index)\nsubmit(sub_preds)","5e971f0e":"# Load the extension and start TensorBoard\n\n%load_ext tensorboard\n%tensorboard --logdir logs","c5cd3446":"<h3>1-D Convolutions over text <\/h3>\n\nGiven a sequence of words $w_{1:n}=w_1,\u2026,w_n$\n, where each is associated with an embedding vector of dimension d. A 1D convolution of width-k is the result of moving a sliding-window of size k over the sentence, and applying the same convolution filter or kernel to each window in the sequence, i.e., a dot-product between the concatenation of the embedding vectors in a given window and a weight vector u, which is then often followed by a non-linear activation function g.","53eb963f":"### Import Libraries","917d4b81":"## Text-CNN model","75772a58":"### Data pre-processing function","802de3f9":"Here, we have used deep CNN model with 4 Conv1D layers where each conv. layer is followed by a maxpooling layer","26c689d1":"In the case of NLP tasks, i.e., when applied to text instead of images, we have a 1 dimensional array representing the text. Here the architecture of the ConvNets is changed to 1D convolutional-and-pooling operations.\n\nOne of the most typically tasks in NLP where ConvNet are used is sentence classification, that is, classifying a sentence into a set of pre-determined categories by considering n\n-grams, i.e. it\u2019s words or sequence of words, or also characters or sequence of characters.","8deacb99":"### CNN Model"}}