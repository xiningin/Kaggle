{"cell_type":{"8bab95c1":"code","e56e3b72":"code","a715ff7b":"code","0498ab09":"code","1800728e":"code","74a838b5":"code","ad1b8cb1":"code","631e96c5":"code","8342bca1":"code","c6593d88":"code","845beae8":"code","c9f42d6d":"code","338af69f":"code","77306730":"code","56a2a62d":"code","d2e376f5":"code","187d63d8":"code","3ac0431f":"code","a5e6eaf6":"code","72fe31a4":"code","306462d0":"code","9bf987bf":"code","0a9cf453":"code","fb9bf39c":"code","e1ae61b6":"code","2ed8e634":"code","32684b11":"code","c8af4970":"code","83623845":"code","e67cbd31":"code","b885a77e":"code","31a379f0":"code","8cb6bb21":"code","434f9c93":"code","42eb4b07":"code","37d300c1":"code","cbbfad88":"code","ce8e82aa":"code","be9f7909":"code","36ab6b73":"code","96df191f":"code","f3a8d827":"code","a7d0cf00":"code","63c6f912":"code","2575193e":"code","3df02dee":"code","e3f21cae":"code","a20012e2":"code","e58c3138":"code","5c296672":"code","49f6c628":"code","d70f6a52":"code","42eedf5d":"markdown","39ba7edf":"markdown","7eba685b":"markdown","b5048018":"markdown","c776d9f6":"markdown","75b00c69":"markdown","16ee240c":"markdown","af195776":"markdown"},"source":{"8bab95c1":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport time\nimport warnings\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nfrom scipy.sparse import hstack\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\n#from sklearn.cross_validation import StratifiedKFold \nfrom collections import Counter, defaultdict\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport math\nfrom sklearn.metrics import normalized_mutual_info_score\nfrom sklearn.ensemble import RandomForestClassifier\nwarnings.filterwarnings(\"ignore\")\nimport nltk\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e56e3b72":"df_tab = pd.read_csv('\/kaggle\/input\/msk-redefining-cancer-treatment\/training_variants.zip')\ndf_tab.head()","a715ff7b":"df_tab.shape","0498ab09":"# Count of Unique Values of Gene\ndf_tab[\"Gene\"].value_counts()","1800728e":"# Count of Unique Values of Variation\ndf_tab[\"Variation\"].value_counts()","74a838b5":"# Count of Unique Values of Class\ndf_tab[\"Class\"].value_counts()","ad1b8cb1":"df_tab.isnull().sum()","631e96c5":"df_text =pd.read_csv(\"\/kaggle\/input\/msk-redefining-cancer-treatment\/training_text.zip\",sep=\"\\|\\|\",engine=\"python\",names=[\"ID\",\"TEXT\"],skiprows=1)\ndf_text.head()","8342bca1":"df_text.shape","c6593d88":"df_merge = pd.merge(df_tab,df_text, on=\"ID\",how=\"left\")\ndf_merge.head()","845beae8":"# lowercase column names\ndf_merge.columns = df_merge.columns.str.lower()","c9f42d6d":"df_merge.isnull().sum()","338af69f":"df_merge[df_merge.isnull().any(axis=1)]","77306730":"# Fill NaN values with Gene & Variation \ndf_merge.loc[df_merge[\"text\"].isnull(),\"text\"] = df_merge[\"gene\"] + \" \" + df_merge[\"variation\"]","56a2a62d":"df_merge.isnull().sum()","d2e376f5":"# lowercase \ndf_merge[\"text\"] = df_merge[\"text\"].apply(lambda x: \" \".join(i.lower() for i in x.split()))\ndf_merge.head()","187d63d8":"# remove punctuations\ndf_merge[\"text\"] = df_merge[\"text\"].str.replace(\"[^\\w\\s]\",\"\")\ndf_merge.head()","3ac0431f":"# remove numeric values\ndf_merge[\"text\"] = df_merge[\"text\"].str.replace(\"\\d\",\"\")\ndf_merge.head()","a5e6eaf6":"# remove stopwords: the,a,an etc.\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\nsw = stopwords.words(\"english\")","72fe31a4":"df_merge[\"text\"] = df_merge[\"text\"].apply(lambda x: \" \".join(i for i in x.split() if i not in sw))\ndf_merge.head()","306462d0":"nltk.download(\"punkt\")\nimport textblob\nfrom textblob import TextBlob","9bf987bf":"# an example:\nTextBlob(df_merge[\"text\"][1]).words","0a9cf453":"df_merge[\"text\"] = df_merge[\"text\"].apply(lambda x: TextBlob(x).words)\ndf_merge.head()","fb9bf39c":"from textblob import Word\nnltk.download(\"wordnet\")","e1ae61b6":"df_merge[\"text\"] = df_merge[\"text\"].apply(lambda x: \" \".join(Word(word).lemmatize() for word in x))\ndf_merge.head()","2ed8e634":"df_merge.to_pickle(\"clean_text.pkl\")","32684b11":"train_x, test_x, train_y, test_y = model_selection.train_test_split(df_merge[[\"gene\",\"variation\",\"text\"]],\n                                                                    df_merge[\"class\"], \n                                                                    random_state = 7,\n                                                                    stratify=df_merge[\"class\"],\n                                                                    test_size=0.2)","c8af4970":"train_x.shape, test_x.shape, train_y.shape, test_y.shape","83623845":"train_x.head()","e67cbd31":"tf_idf_word_vectorizer = TfidfVectorizer()\ntf_idf_word_vectorizer.fit(train_x.text)","b885a77e":"tf_idf_word_vectorizer.get_feature_names()[0:5]","31a379f0":"len(tf_idf_word_vectorizer.get_feature_names())","8cb6bb21":"x_train_tf_idf_word = tf_idf_word_vectorizer.transform(train_x.text)\nx_test_tf_idf_word = tf_idf_word_vectorizer.transform(test_x.text)","434f9c93":"loj = LogisticRegression()\nloj_model = loj.fit(x_train_tf_idf_word, train_y)","42eb4b07":"from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics","37d300c1":"accuracy = model_selection.cross_val_score(loj_model, \n                                           x_test_tf_idf_word, \n                                           test_y, \n                                           cv = 10).mean()\n\nprint(\"Count Vectors Do\u011fruluk Oran\u0131:\", accuracy)","cbbfad88":"train_x.head()","ce8e82aa":"train_y.head()","be9f7909":"test_x.head()","36ab6b73":"df_merge.head()","96df191f":"df_merge.shape","f3a8d827":"df_merge.dtypes","a7d0cf00":"gene_enc = pd.get_dummies(df_merge[\"gene\"],drop_first=True)","63c6f912":"variation_enc = pd.get_dummies(df_merge[\"variation\"],drop_first=True)","2575193e":"df_enc = pd.concat([gene_enc,variation_enc],axis=1)","3df02dee":"df_enc2 = pd.concat([df_merge[\"class\"],df_enc],axis=1)\ndf_enc2.head()","e3f21cae":"train_x, test_x, train_y, test_y = model_selection.train_test_split(df_enc2.drop(\"class\",axis=1),\n                                                                    df_enc2[\"class\"], \n                                                                    random_state = 7,\n                                                                    stratify=df_merge[\"class\"],\n                                                                    test_size=0.2)","a20012e2":"loj = LogisticRegression()\nloj_model = loj.fit(train_x, train_y)\naccuracy = model_selection.cross_val_score(loj_model, \n                                           test_x, \n                                           test_y, \n                                           cv = 10).mean()\n\nprint(\"Count Vectors Do\u011fruluk Oran\u0131:\", accuracy)","e58c3138":"tf_idf_word_vectorizer = TfidfVectorizer()\ntext_enc = tf_idf_word_vectorizer.fit_transform(df_merge.text)","5c296672":"text_enc_df = pd.DataFrame(text_enc.toarray())","49f6c628":"df_enc3 = pd.concat([df_enc2,text_enc_df],axis=1)\ndf_enc3.head()","d70f6a52":"# train_x, test_x, train_y, test_y = model_selection.train_test_split(df_enc3.drop(\"class\",axis=1),\n#                                                                     df_enc3[\"class\"], \n#                                                                     random_state = 7,\n#                                                                     stratify=df_merge[\"class\"],\n#                                                                     test_size=0.2)","42eedf5d":"## Tokenization\n* Split sentence to words","39ba7edf":"## Split Dataset ","7eba685b":"## Merge Datasets","b5048018":"## Reading Text Dataset and EDA","c776d9f6":"## Clean Text Column","75b00c69":"## TF-IDF","16ee240c":"## Reading Gene Dataset and EDA","af195776":"## Lemmatization\n* Create root of the word"}}