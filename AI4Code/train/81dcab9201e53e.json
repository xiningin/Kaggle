{"cell_type":{"ec0d20e7":"code","5404a5e1":"code","e10ef4f9":"code","b4123f2f":"code","bc7e93f8":"code","3e027922":"code","5a693700":"code","fdd29709":"code","5badfaf2":"code","0a8b364b":"code","0065dbd1":"code","6f3e38cd":"code","2baafd42":"code","1aa0145d":"code","686b2477":"code","98c3f15a":"code","2d5a7074":"code","2fc69ddc":"code","c2dfdd99":"code","cd876c92":"code","de500ce6":"code","4ce8120e":"code","d80d5307":"code","08fd9d3b":"code","926a58a9":"markdown","abc0e00e":"markdown","95662f82":"markdown","a9e0b036":"markdown","7004a8de":"markdown","1acccfd8":"markdown","8da126ef":"markdown","efe693ad":"markdown","28dda6f0":"markdown","c0c080f1":"markdown","ed0d18fc":"markdown","6bca8082":"markdown","7451ac40":"markdown","e7e8bfa2":"markdown","f227d609":"markdown","6795dea7":"markdown","deefdb0a":"markdown","b78c82be":"markdown","788b6f5d":"markdown","61135981":"markdown","fa7101b7":"markdown","a9683dc3":"markdown"},"source":{"ec0d20e7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline","5404a5e1":"import torch\nimport torch.nn as nn # \u6df1\u5c64\u5b66\u7fd2\u3067\u7528\u3044\u308b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u306e\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u304c\u683c\u7d0d\nimport torch.nn.functional as F\n# from torch.autograd import Variable # \u81ea\u52d5\u5fae\u5206\u7528\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.utils import make_grid","e10ef4f9":"print(torch.__version__)","b4123f2f":"print(os.listdir('..\/input\/'))","bc7e93f8":"class FashionMNIST(Dataset):\n    \"\"\"\n    X : pandas.DataFrame (images)\n    y : pandas.Series (labels)\n    transform : list of data augmentation\n    \"\"\"\n    def __init__(self, X, y=None, transform=None):\n        self.X = X\n        self.y = y\n        self.transform = transform\n    \n    def __getitem__(self, index):\n        # PyTorch automatically divide image data by 255 when its data type is np.uint8 \n        # (np.uint8 : unchanged sign value [0 ~ 255])\n        image = self.X.iloc[index, :].values.astype(np.uint8).reshape((28, 28, 1)) # (height, width, channels)\n        \n        if self.transform is not None:\n            image = self.transform(image)\n            \n        if self.y is not None: # training\n            return image, self.y.iloc[index]\n        else: # prediction\n            return image\n        \n    def __len__(self):\n        return len(self.X.index)","3e027922":"train_df = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\ntest_df = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')\n\nprint('train csv : {}'.format(train_df.shape))\nprint('test csv : {}'.format(test_df.shape))","5a693700":"test_df.drop(labels=['label'], axis=1, inplace=True)\nprint('test csv : {}'.format(test_df.shape))","fdd29709":"X_train, X_valid, y_train, y_valid = \\\n        train_test_split(train_df.iloc[:, 1:], train_df.iloc[:, 0], test_size=1\/6, random_state=0)\n\nX_test = test_df\n\nprint('data shape on training images : {}'.format(X_train.shape))\nprint('data shape on training labels : {}'.format(y_train.shape))\nprint('data shape on validation images : {}'.format(X_valid.shape))\nprint('data shape on validation labels : {}'.format(y_valid.shape))\nprint('data shape on test images : {}'.format(X_test.shape))","5badfaf2":"transform=transforms.Compose([\n    transforms.ToTensor() # PIL or numpy.ndarray \u3092\u53d7\u3051\u53d6\u308aTonsor\u578b\u306b\u5909\u63db\u3059\u308b\u3002\n])\n\ntrain_dataset = FashionMNIST(X_train, y_train, transform=transform)\n# \u691c\u8a3c\u7528\u30fb\u30c6\u30b9\u30c8\u7528\u306fTensor\u578b\u306b\u5909\u63db\u3059\u308b\u3060\u3051\u3067\nvalid_dataset = FashionMNIST(X_valid, y_valid, transform=transform)\ntest_dataset = FashionMNIST(X_test, transform=transform)","0a8b364b":"img, lab = train_dataset.__getitem__(0) # index=0 \u306e\u753b\u50cf\u3092\u51fa\u529b\nprint(img.shape)\nprint(lab.shape)","0065dbd1":"img.numpy().transpose((1, 2, 0)).reshape((28, 28))\n\nplt.imshow(img.numpy().reshape((28, 28)), cmap='gray')\nplt.title(f'label is {lab}');\nplt.axis('off');","6f3e38cd":"train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)","2baafd42":"train_iter = iter(train_loader)\nimages, labels = next(train_iter)\n\nprint('data shape on train loader images : {}'.format(images.size()))\nprint('data shape on train loader labels : {}'.format(labels.size()))","1aa0145d":"print(images[0])","686b2477":"grid = make_grid(images, nrow=8)\n\nfig, ax = plt.subplots(figsize=(15, 10))\nax.imshow(grid.numpy().transpose((1, 2, 0)))\nax.axis('off');","98c3f15a":"test_iter = iter(test_loader)\nimages = next(test_iter)\n\ngrid = make_grid(images, nrow=8)\n\nfig, ax = plt.subplots(figsize=(15, 10))\nax.imshow(grid.numpy().transpose((1, 2, 0)))\nax.axis('off');","2d5a7074":"class MLP(nn.Module):\n    \n    def __init__(self):\n        super(MLP, self).__init__()\n        \n        self.layers = nn.Sequential(\n            nn.Linear(28*28*1, 100),\n            nn.ReLU(),\n            nn.Linear(100, 10)\n        )\n        \n    def forward(self, x):\n        x = x.view(x.size(0), -1) # convert data shape (64, 1, 28, 28) --> (64, 1*28*28) = [64, 784]\n        x = self.layers(x)\n        return x","2fc69ddc":"model = MLP()\nprint(model)","c2dfdd99":"optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nloss_fn = nn.CrossEntropyLoss()","cd876c92":"mean_train_losses = []\nmean_valid_losses = []\nepochs = 20\n\nfor epoch in range(epochs):\n    # \u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u30e2\u30fc\u30c9\u306b\u5909\u66f4(\u52fe\u914d\u8a08\u7b97\u3084\u30d1\u30e9\u30e1\u30fc\u30bf\u66f4\u65b0\u3092\u884c\u3046)\n    model.train()\n    \n    train_losses = []\n    valid_losses = []\n    for i, (images, labels) in enumerate(train_loader):        \n        \n        # zero the parameters gradient\n        optimizer.zero_grad()\n        \n        # feedforward -> backward -> optimize\n        outputs = model(images)\n        loss = loss_fn(outputs, labels)\n        train_losses.append(loss.data)\n        loss.backward()\n        optimizer.step()\n        \n        if (i * 128) % (128 * 100) == 0:\n            print(f'{i * 128} \/ 50000')\n    \n    # 1epoch\u6bce\u306b\u63a8\u8ad6\u3092\u884c\u3046\u3002\n    # \u30e2\u30c7\u30eb\u3092\u63a8\u8ad6\u30e2\u30fc\u30c9\u306b\u5909\u66f4\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for i, (images, labels) in enumerate(valid_loader):\n\n            # feedforward -> loss\n            outputs = model(images)\n            loss = loss_fn(outputs, labels)\n            valid_losses.append(loss.data)\n\n            _, predicted = torch.max(outputs.data, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n        \n    mean_train_losses.append(np.mean(train_losses))\n    mean_valid_losses.append(np.mean(valid_losses))\n    \n    print('epoch : {}, train loss : {:.4f}, valid loss : {:.4f}, val acc : {:.2f}%'\\\n          .format(epoch+1, np.mean(train_losses),\\\n                           np.mean(valid_losses), 100*correct\/total))","de500ce6":"fig, ax = plt.subplots(figsize=(15, 10))\nax.plot(mean_train_losses, label='train')\nax.plot(mean_valid_losses, label='valid')\nlines, labels = ax.get_legend_handles_labels()\nax.legend(lines, labels, loc='best')","4ce8120e":"model.eval()\ntest_preds = torch.LongTensor()\n\nfor i, images in enumerate(test_loader):\n\n    outputs = model(images)\n\n    pred = outputs.max(1, keepdim=True)[1]\n    test_preds = torch.cat((test_preds, pred), dim=0)","d80d5307":"out_df = pd.DataFrame()\nout_df['ID'] = np.arange(1, len(X_test.index)+1)\nout_df['label'] = test_preds.numpy()\n\nout_df.head()","08fd9d3b":"out_df.to_csv('submission.csv', index=None)","926a58a9":"PyTorch\u3067\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4f5c\u6210\u306f\u3001\u524d\u51e6\u7406\u3082\u4e00\u7dd2\u306b\u51e6\u7406\u3092\u884c\u3044\u307e\u3059\u3002\n\n\u305d\u306e\u969b\u306e\u524d\u51e6\u7406\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b**torchvision.transforms.Compose**\u95a2\u6570\u3092\u4f7f\u7528\u3057\u3066\u5b9a\u7fa9\u3057\u307e\u3059\u3002\n\n```python\nclass Compose(object):\n    \"\"\"Composes several transforms together.\n    Args:\n        transforms (list of ``Transform`` objects): list of transforms to compose.\n    Example:\n        >>> transforms.Compose([\n        >>>     transforms.CenterCrop(10),\n        >>>     transforms.ToTensor(),\n        >>> ])\n    \"\"\"\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img):\n        for t in self.transforms:\n            img = t(img)\n        return img\n\n```\n\n\u5143\u306e\u30b3\u30fc\u30c9\u3092\u898b\u308c\u3070\u308f\u304b\u308b\u3088\u3046\u306b\u3001Compose\u3067\u306f\u30ea\u30b9\u30c8\u3067\u5b9a\u7fa9\u3057\u305f\u524d\u51e6\u7406\u3092\u5b9a\u7fa9\u3057\u305f\u9806\u756a\u306b\u9069\u7528\u3055\u305b\u3066\u3044\u304f\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\n\n\u3053\u306e\u305f\u3081\u524d\u51e6\u7406\u3092\u884c\u3046\u9806\u756a\u304c\u975e\u5e38\u306b\u5927\u5207\u3067\u3059\u3002\uff08\u5165\u529b\u306b\u5408\u308f\u305b\u3066\u578b\u3092\u5909\u3048\u308b\u5fc5\u8981\u304c\u3042\u308b\u305f\u3081\uff09","abc0e00e":"## DataLoader\u3092\u5b9a\u7fa9\u3059\u308b","95662f82":"Dataset\u3092\u6e21\u3059\u3053\u3068\u3067\u30df\u30cb\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u306e\u30c7\u30fc\u30bf\u3092\u8fd4\u3059Iterable\u306a\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u8fd4\u3057\u3066\u304f\u308c\u307e\u3059\u3002","a9e0b036":"## \u640d\u5931\u95a2\u6570\u306a\u3069\u306e\u5b9a\u7fa9","7004a8de":"## \u30c7\u30fc\u30bf\u3092\u8a13\u7df4\u7528\u30fb\u691c\u8a3c\u7528\u306b\u5206\u5272\u3059\u308b","1acccfd8":"## \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4f5c\u6210","8da126ef":"\u3053\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e2d\u3092\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002","efe693ad":"\u5b9f\u969b\u306b\u4e2d\u8eab\u3092\u898b\u3066\u307f\u308b\u3068\u3001\u8f1d\u5ea6\u304c0\uff5e255\u306b\u8abf\u6574\u3055\u308c\u305f\u753b\u50cf\u30c7\u30fc\u30bf\u3060\u3068\u3044\u3046\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002","28dda6f0":"PyTorch\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u306f\u78ba\u8a8d\u3092\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n- \u73fe\u6642\u70b9\uff082018\u5e748\u670812\u65e5\uff09: 0.4.0","c0c080f1":"\u4e88\u6e2c\u3059\u308b\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u4e2d\u8eab\u3082\u78ba\u8a8d\u3057\u3066\u304a\u304d\u307e\u3059\u3002","ed0d18fc":"\u3067\u306f\u5b9f\u969b\u306b\u3053\u306e\u30a4\u30c6\u30ec\u30fc\u30bf\u306e\u4e2d\u8eab\u3092\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002","6bca8082":"\u3069\u3061\u3089\u3082PyTorch\u306e\u30c7\u30fc\u30bf\u578b\u306b\u5909\u63db\u3055\u308c\u3066\u3044\u307e\u3059\u3002","7451ac40":"## PyTorch\u3067\u306e\u30c7\u30fc\u30bf\u306e\u6e96\u5099","e7e8bfa2":"## \u5b66\u7fd2","f227d609":"## \u30e2\u30c7\u30eb\u306e\u69cb\u7bc9","6795dea7":"## \u5927\u4f1a\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f","deefdb0a":"PyTorch\u3067\u306f\u30d0\u30fc\u30b8\u30e7\u30f3\u3054\u3068\u306b\u8a18\u6cd5\u306a\u3069\u304c\u82e5\u5e72\u5909\u5316\u3059\u308b\u306e\u3067\u3001\u73fe\u5728\u4f7f\u7528\u3057\u3066\u3044\u308b\u30d1\u30c3\u30b1\u30fc\u30b8\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u78ba\u8a8d\u306f\u5fc5\u9808\u3067\u3059\u3002","b78c82be":"\u4ee5\u4e0b\u304c\u516c\u5f0f\u306e**Dataset**\u30af\u30e9\u30b9\u3067\u3059\u3002\n\n```python\nclass Dataset(object):\n    \"\"\"An abstract class representing a Dataset.\n    All other datasets should subclass it. All subclasses should override\n    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n    supporting integer indexing in range from 0 to len(self) exclusive.\n    \"\"\"\n\n    def __getitem__(self, index):\n        raise NotImplementedError\n\n    def __len__(self):\n        raise NotImplementedError\n\n    def __add__(self, other):\n        return ConcatDataset([self, other])\n```\n\n\u30ab\u30b9\u30bf\u30e0\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f5c\u6210\u3059\u308b\u969b\u306f\u3001\u3053\u306e\u30af\u30e9\u30b9\u3092\u7d99\u627f\u3057\u3066\u4ee5\u4e0b\u306e\u9805\u76ee\u3092\u4e0a\u66f8\u304d\u3057\u307e\u3059\u3002\n\n- init() : \u521d\u671f\u6761\u4ef6\uff08CSV\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f\u3001\u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406\u3001...\uff09\n- len() : \u30b5\u30f3\u30d7\u30eb\u6570\u3092\u51fa\u529b\n- getitem() : \u4efb\u610f\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3067\u306e\u30c7\u30fc\u30bf\u3068\u30e9\u30d9\u30eb\u3092\u51fa\u529b\n\n\u3067\u306f\u3053\u306e\u30af\u30e9\u30b9\u3092\u7d99\u627f\u3057\u3066\u4eca\u5927\u4f1a\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f5c\u6210\u3057\u3066\u3044\u304d\u307e\u3059\u3002","788b6f5d":"PyTorch\u3067\u306f**Dataset**\u3068**DataLoader**\u3068\u3044\u3046\u30d1\u30c3\u30b1\u30fc\u30b8\u3067\u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406\u3084\u30d0\u30c3\u30c1\u51e6\u7406\u3092\u884c\u3044\u307e\u3059\u3002\n\n\u6d41\u308c\u3068\u3057\u3066\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3001\n\n1. **\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080Dataset\u30af\u30e9\u30b9\u3092\u5b9a\u7fa9**\n2. **Dataset\u3092\u30af\u30e9\u30b9\u3092\u901a\u3057\u3066\u524d\u51e6\u7406**\n3. **DataLoader**\u306b\u6e21\u3057\u3066\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3054\u3068\u306b\u30c7\u30fc\u30bf\u3092\u53d6\u5f97","61135981":"PyTorch\u3067\u306e\u30e2\u30c7\u30eb\u69cb\u7bc9\u306f**nn.Module**\u3092\u7d99\u627f\u3057\u3066\u884c\u3044\u307e\u3059\u3002","fa7101b7":"PyTorch\u306b\u306f\u30df\u30cb\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3092\u542b\u3081\u305f4\u6b21\u5143Tensor\u3092\u53ef\u8996\u5316\u3055\u305b\u308b\u95a2\u6570**make_grid**\u304c\u5b58\u5728\u3057\u3066\u3044\u307e\u3059\u3002\n\n```python\nmake_grid(tensor, nrow=8, padding=2, normalize=False, range=None, scale_each=False, pad_value=0)\n```","a9683dc3":"\u4e0a\u8a18\u306e\u3088\u3046\u306bDataset\u30af\u30e9\u30b9\u3092\u4e0a\u66f8\u304d\u3057\u307e\u3057\u305f\u3002\n\n\u6ce8\u610f\u70b9\u3068\u3057\u3066\u306fPyTorch\u3067\u306f**Tensor**\u578b\u3067\u30c7\u30fc\u30bf\u3092\u53d6\u308a\u6271\u3046\u70b9\u3067\u3059\u3002\n\nTensor\u578b\u3067\u306e\u753b\u50cf\u3067\u30c7\u30fc\u30bf\u306e\u5f62\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u8a2d\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\n- Tensor\u578b\uff1a**4 dimensional shape** ( batch_size, channels, height, width )\n\nTensor\u578b\u306b\u5909\u63db\u3059\u308b\u969b\u306b\u306ftorchvision\u306b\u5b58\u5728\u3059\u308b**ToTensor()**\u3092\u4f7f\u7528\u3057\u307e\u3059\u304c\u3001\u3082\u3068\u3082\u3068\u306e\u30c7\u30fc\u30bf\u306f\u901a\u5e38\u306e ( height, width, channels ) \u3067\u3042\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\n\u516c\u5f0f\u306eToTensor\u30b3\u30fc\u30c9\n\n```python\nclass ToTensor(object):\n    \"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n    Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n    \"\"\"\n\n    def __call__(self, pic):\n        \"\"\"\n        Args:\n            pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n        Returns:\n            Tensor: Converted image.\n        \"\"\"\n        return F.to_tensor(pic)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '()'\n```"}}