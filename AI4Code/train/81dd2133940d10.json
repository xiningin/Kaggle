{"cell_type":{"0162d291":"code","938d9fdb":"code","43e27eb9":"code","057862c0":"code","f39547d4":"code","58d9cac2":"code","374a9bee":"code","c2cfcc30":"code","cef53510":"code","e8aa6a44":"code","e058e37a":"code","9a65ebda":"code","4ccd118d":"code","abea3684":"code","8c8db9ac":"code","503da4ec":"code","ca7d4c5d":"code","9ab9c2fb":"code","ea9c6314":"code","ff4ff297":"code","4bcda785":"code","6cb35481":"code","afa9eccc":"code","05b6bcbc":"code","19fc59fb":"code","96b64a61":"code","c6009a5a":"code","14ec10f7":"code","5ac1730a":"code","ace15a83":"code","4d65c6f2":"code","c9fc1afd":"code","5360332e":"code","6d33dab9":"markdown","7c687c50":"markdown","e6694253":"markdown","a8ede792":"markdown","a0a05e1e":"markdown","2a593d7b":"markdown","a49a702b":"markdown"},"source":{"0162d291":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","938d9fdb":"import spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing, svm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport string\nimport nltk\nnlp = spacy.load('en_core_web_sm')","43e27eb9":"train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntrain_df.head()","057862c0":"train_df.isna().sum()","f39547d4":"train_df_size = train_df.shape[0] * 0.9\ntrain_df_size","58d9cac2":"#train_df = train_df[:6850]\n#val_df = train_df[6850:]","374a9bee":"keywords_dis = train_df[train_df['target'] == 1].keyword.value_counts().sort_values(ascending=False)\nkeywords_dis","c2cfcc30":"keywords_nondis = train_df[train_df['target'] == 0].keyword.value_counts().sort_values(ascending=False)\nkeywords_nondis ","cef53510":"sns.barplot(y = (keywords_dis \/ keywords_nondis).index, x = (keywords_dis - keywords_nondis))\nsns.set(rc={'figure.figsize':(40,120)})","e8aa6a44":"train_df.keyword.value_counts().sort_values(ascending=False)","e058e37a":"sns.barplot(y = keywords_dis.index[:40], x = keywords_dis[:40])\nsns.set(rc={'figure.figsize':(20, 10)})","9a65ebda":"sns.barplot(y = keywords_nondis.index[:40], x = keywords_nondis[:40])\nsns.set(rc={'figure.figsize':(20,10)})","4ccd118d":"train_df['text'] = train_df['text'].str.lower()","abea3684":"train_df['text'] = train_df['text'].str.replace('[^\\w\\s]','')\ntrain_df['text'].head()","8c8db9ac":"#Importing stopwords from nltk library\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))\n# Function to remove the stopwords\ndef stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n# Applying the stopwords to 'text_punct' and store into 'text_stop'\ntrain_df[\"text\"] = train_df[\"text\"].apply(stopwords)","503da4ec":"# Checking the first 10 most frequent words\nfrom collections import Counter\ncnt = Counter()\nfor text in train_df[\"text\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncnt.most_common(10)\n\n# Removing the frequent words\nfreq = set([w for (w, wc) in cnt.most_common(10)])\n# function to remove the frequent words\ndef freqwords(text):\n    return \" \".join([word for word in str(text).split() if word not \nin freq])\n# Passing the function freqwords\ntrain_df[\"text\"] = train_df[\"text\"].apply(freqwords)","ca7d4c5d":"# remove old style retweet text \"RT\"\ntrain_df['text'] = train_df['text'].str.replace(r'^RT[\\s]+', '')\n\n# remove hyperlinks\ntrain_df['text'] = train_df['text'].str.replace(r'https?:\/\/[^\\s\\n\\r]+', '')\ntrain_df['text'] = train_df['text'].str.replace(r'#', '')","9ab9c2fb":"from bs4 import BeautifulSoup\n#Function for removing html\ndef html(text):\n    return BeautifulSoup(text, \"lxml\").text\n# Passing the function to 'text_rare'\ntrain_df['text'] = train_df['text'].apply(html)","ea9c6314":"train_df['lemmatized'] = train_df[\"text\"].apply(lambda row: \" \".join([w.lemma_ for w in nlp(row)  if not w.is_stop]))","ff4ff297":"from nltk.tokenize import word_tokenize\ndef count_tweets(tweets, ys):\n    results = {}\n    l = tweets.to_list()\n    for tweet, y in zip(l, ys):\n        for word in word_tokenize(tweet):\n            pair = (word, y)\n            if pair in results:\n                results[pair] += 1\n            else:\n                results[pair] = 1\n    return results","4bcda785":"freqs = count_tweets(train_df['lemmatized'],train_df['target'])","6cb35481":"def naive_bayes(freqs, train_x, train_y):\n    '''\n    calculates loglikelihood and logprior to later use in prediction\n    '''\n    loglikelihood = {}\n    logprior = 0\n    \n    vocab = set(word for word, y in freqs.keys())\n    V = len(vocab)\n    N_pos = N_neg = 0\n    \n    for word, y in freqs.keys():\n        if y > 0:\n            N_pos += freqs.get((word, y))\n        else:\n            N_neg += freqs.get((word, y))\n            \n    for word in vocab:                       # counts log likelihoods with laplacian smoothing\n        freq_pos = freqs.get((word, 1), 0)\n        freq_neg = freqs.get((word, 0), 0)\n        p_pos = (freq_pos + 1) \/ (N_pos + V)\n        p_neg = (freq_neg + 1) \/ (N_neg + V)\n        loglikelihood[word] = np.log(p_pos) - np.log(p_neg)\n    \n    D = train_x.shape[0]\n    D_pos = sum(train_y)\n    D_neg = D - D_pos\n    \n    logprior = np.log(D_pos) - np.log(D_neg)\n    \n    return logprior, loglikelihood","afa9eccc":"train_df['lemmatized'].shape[0]","05b6bcbc":"logprior, loglikelihood = naive_bayes(freqs, train_df['lemmatized'], train_df['target'])","19fc59fb":"test_df['text'] = test_df['text'].str.lower()\n\ntest_df['text'] = test_df['text'].apply(html)\n\ntest_df['text'] = test_df['text'].str.replace('[^\\w\\s]','')\n\ntest_df[\"text\"] = test_df[\"text\"].apply(stopwords)\n\n# Checking the first 10 most frequent words\nfrom collections import Counter\ncnt = Counter()\nfor text in test_df[\"text\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncnt.most_common(10)\n\n# Removing the frequent words\nfreq = set([w for (w, wc) in cnt.most_common(10)])\n# function to remove the frequent words\ndef freqwords(text):\n    return \" \".join([word for word in str(text).split() if word not \nin freq])\n# Passing the function freqwords\ntest_df[\"text\"] = test_df[\"text\"].apply(freqwords)\ntest_df[\"text\"].head()\n\n# remove old style retweet text \"RT\"\ntest_df['text'] = test_df['text'].str.replace(r'^RT[\\s]+', '')\n\n# remove hyperlinks\ntest_df['text'] = test_df['text'].str.replace(r'https?:\/\/[^\\s\\n\\r]+', '')\ntest_df['text'] = test_df['text'].str.replace(r'#', '')\n\ntest_df['lemmatized'] = test_df[\"text\"].apply(lambda row: \" \".join([w.lemma_ for w in nlp(row)  if not w.is_stop]))","96b64a61":"train_df.tail()","c6009a5a":"def naive_bayes_predictor(tweet, logprior, loglikelihood):\n    word_test = word_tokenize(tweet)\n    \n    p = 0\n    \n    p += logprior\n    \n    for word in word_test:\n        if word in loglikelihood:\n        \n            p += loglikelihood[word]\n    return p","14ec10f7":"def naive_bayes_test(test_x, logprior, loglikelihood, naive_bayes_predictor=naive_bayes_predictor):\n    \n    y_hat = []\n    accuracy = 0\n    \n    l = test_x.to_list()\n    for tweet in l:\n        if naive_bayes_predictor(tweet, logprior, loglikelihood) > 0:\n            y_hat_i = 1\n        else:\n            y_hat_i = 0\n        y_hat.append(y_hat_i)\n    \n    #error = (abs(len(y_hat) - sum(y_hat == test_y.to_list()))) \/ len(y_hat)\n    #accuracy = 1 - error\n    \n    return y_hat","5ac1730a":"submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","ace15a83":"submission['target'] = naive_bayes_test(test_df['lemmatized'], logprior, loglikelihood)","4d65c6f2":"submission.head()","c9fc1afd":"submission.to_csv('submission.csv', index=False)","5360332e":"test_df.head()","6d33dab9":"##  barplot for all keywords was too big and words couldn't be differentiated so I limited only top 40","7c687c50":"## now do same for test data","e6694253":"## First let's make some eda","a8ede792":"## Implementing Naive Bayes. For detailed description, see my [model-only notebook](https:\/\/www.kaggle.com\/avtochakhnashvili\/handwritten-naive-bayes)","a0a05e1e":"## most used keywords are different for disaster and non disaster samples which will help algorithm to differentiate","2a593d7b":"## despite many missing values, location can be useful for future versions. Tweets from the same location aboout the same keywords can be used","a49a702b":"## preprocess the text data"}}