{"cell_type":{"6b8a9ab6":"code","4e6fb014":"code","6217319b":"code","44decf9f":"code","1d2a9156":"code","ab4857fb":"code","042be0d8":"code","32c74327":"code","9082e012":"code","c7a7b0b8":"code","0300d222":"code","2ddcf98f":"code","2ec5d9eb":"code","51eb9291":"code","985cfdc7":"code","fa01ce9e":"code","e448031d":"code","0b5c38c3":"code","d42f27c0":"code","15471192":"code","83f22dba":"code","bdf18abc":"code","1187dec2":"code","4e268ee3":"markdown","359b3995":"markdown","d49638f0":"markdown","1606b0e5":"markdown","54d5db74":"markdown","48627516":"markdown","023037a0":"markdown","112dbebd":"markdown","d6bc05c8":"markdown","57be58b3":"markdown","33eb62e4":"markdown","12dc5765":"markdown","73314b16":"markdown","b2607c21":"markdown"},"source":{"6b8a9ab6":"!pip install \/kaggle\/input\/adjusttext\n!pip install \/kaggle\/input\/bioinfokit","4e6fb014":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# for feature importance study\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom pdpbox import pdp\nimport shap\n\n# for PCA\nfrom bioinfokit.visuz import cluster\nfrom sklearn.decomposition import PCA\n\n# ML\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBRegressor\nimport os\nimport tensorflow as tf\nimport keras\nfrom tensorflow.keras import models, layers, callbacks\nimport tensorflow_addons as tfa\nfrom keras.utils.vis_utils import plot_model\nfrom keras import backend as K\nimport gc\n\n# Reproducability\ndef set_seed(seed = 0):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    print('*** --- Set seed \"%i\" --- ***' %seed)\n\n# Custom theme\nplt.style.use('fivethirtyeight')\n\nfigure = {'dpi': '200'}\nfont = {'family': 'serif'}\ngrid = {'linestyle': ':', 'alpha': .9}\naxes = {'titlecolor': 'black', 'titlesize': 20, 'titleweight': 'bold',\n        'labelsize': 12, 'labelweight': 'bold'}\n\nplt.rc('font', **font)\nplt.rc('figure', **figure)\nplt.rc('grid', **grid)\nplt.rc('axes', **axes)\n\nmy_colors = ['#DC143C', '#FF1493', '#FF7F50', '#FFD700', '#32CD32', \n             '#4ddbff', '#1E90FF', '#663399', '#708090']\n\ncaption = \"\u00a9 maksymshkliarevskyi\"\n\n# Show our custom palette\nsns.palplot(sns.color_palette(my_colors))\nplt.title('Custom palette')\nplt.text(6.9, 0.75, caption, size = 8)\nplt.show()","6217319b":"train = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv', \n                    index_col = 0)\ntest = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv', \n                   index_col = 0)\nss = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')","44decf9f":"train.describe().T.style.background_gradient(subset = ['count'], cmap = 'viridis') \\\n    .bar(subset = ['mean', '50%'], color = my_colors[6]) \\\n    .bar(subset = ['std'], color = my_colors[0])","1d2a9156":"test.describe().T.style.background_gradient(subset = ['count'], cmap = 'viridis') \\\n    .bar(subset = ['mean', '50%'], color = my_colors[6]) \\\n    .bar(subset = ['std'], color = my_colors[0])","ab4857fb":"dtypes = train.dtypes.value_counts().reset_index()\n\nplt.figure(figsize = (12, 1))\nplt.title('Data types\\n')\nplt.barh(str(dtypes.iloc[0, 0]), dtypes.iloc[0, 1],\n         label = str(dtypes.iloc[0, 0]), color = my_colors[4])\nplt.legend(loc = 'upper center', ncol = 3, fontsize = 13,\n           bbox_to_anchor = (0.5, 1.45), frameon = False)\nplt.yticks('')\nplt.text(85, -0.9, caption, size = 8)\nplt.show()","042be0d8":"loss = train.loss\n\ntrain.drop(['loss'], axis = 1, inplace = True)","32c74327":"# Concatenate train and test datasets\nall_data = pd.concat([train, test], axis = 0)\n\n# columns with missing values\ncols_with_na = all_data.isna().sum()[all_data.isna().sum() > 0].sort_values(ascending = False)\ncols_with_na","9082e012":"print('Train data')\nfig = plt.figure(figsize = (15, 120))\nfor idx, i in enumerate(train.columns):\n    fig.add_subplot(np.ceil(len(train.columns)\/4), 4, idx+1)\n    train.iloc[:, idx].hist(bins = 20)\n    plt.title(i)\nplt.text(10, -8000, caption, size = 12)\nplt.show()","c7a7b0b8":"fig = plt.figure(figsize = (15, 5))\nloss.hist(bins = 20)\nplt.title('loss')\nplt.text(37, -12000, caption, size = 12)\nplt.show()","0300d222":"print('Test data')\nfig = plt.figure(figsize = (15, 120))\nfor idx, i in enumerate(test.columns):\n    fig.add_subplot(np.ceil(len(test.columns)\/4), 4, idx+1)\n    test.iloc[:, idx].hist(bins = 20)\n    plt.title(i)\nplt.text(10, -8000, caption, size = 12)\nplt.show()","2ddcf98f":"corr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\nplt.figure(figsize = (15, 15))\nplt.title('Corelation matrix')\nsns.heatmap(corr, mask = mask, cmap = 'Spectral_r', linewidths = .5)\nplt.text(106, 106, caption, size = 8)\nplt.show()","2ec5d9eb":"corr = test.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\nplt.figure(figsize = (15, 15))\nplt.title('Corelation matrix')\nsns.heatmap(corr, mask = mask, cmap = 'Spectral_r', linewidths = .5)\nplt.text(106, 106, caption, size = 8)\nplt.show()","51eb9291":"# Create data sets for training (85%) and validation (15%)\nX_train, X_valid, y_train, y_valid = train_test_split(train, loss, \n                                                      test_size = 0.15,\n                                                      random_state = 0)","985cfdc7":"def root_mean_squared_error(y_true, y_pred):\n    from sklearn.metrics import mean_squared_error\n    from math import sqrt\n    return sqrt(mean_squared_error(y_true, y_pred))","fa01ce9e":"# The basic model\nparams = {'random_state': 0,\n          'predictor': 'gpu_predictor',\n          'tree_method': 'gpu_hist',\n          'eval_metric': 'logloss'}\n\nmodel = XGBRegressor(**params)\n\nmodel.fit(X_train, y_train, verbose = False)\n\npreds = model.predict(X_valid)\nprint('Valid RMSE of the basic model: {}'.format(root_mean_squared_error(y_valid, preds)))","e448031d":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_valid)\n\nshap.summary_plot(shap_values, X_valid)","0b5c38c3":"def root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true))) \n\ndef create_model(shape = train.shape[1]):\n    inputs = layers.Input(shape = (shape))\n\n    hidden = layers.Dropout(0.25)(inputs)\n    hidden = tfa.layers.WeightNormalization(layers.Dense(units = 128, activation = 'relu'))(hidden)\n\n    output = layers.Dropout(0.25)(layers.Concatenate()([inputs, hidden]))\n    output = tfa.layers.WeightNormalization(layers.Dense(units = 64, activation = 'relu'))(output) \n\n    output = layers.Dropout(0.25)(layers.Concatenate()([inputs, hidden, output]))\n    output = tfa.layers.WeightNormalization(layers.Dense(units = 32, activation = 'relu'))(output) \n    output = layers.Dense(1)(output)\n\n    model = keras.Model(inputs = inputs, outputs = output, name = \"res_nn_model\")\n\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.003),\n                  loss = root_mean_squared_error)\n    return model","d42f27c0":"model = create_model()\n\nplot_model(model, to_file = 'model_plot.png', show_shapes = True, show_layer_names = True)","15471192":"scaler = StandardScaler()\ntrain = scaler.fit_transform(train)\ntest = scaler.transform(test)\n\n# Create data sets for training (85%) and validation (15%)\nX_train, X_valid, y_train, y_valid = train_test_split(train, np.float32(loss), \n                                                      test_size = 0.15,\n                                                      random_state = 0)","83f22dba":"set_seed()\n\nmodel = create_model()\n\nearly_stopping = callbacks.EarlyStopping(\n    patience = 7,\n    min_delta = 0.00001,\n    restore_best_weights = True,\n    monitor = \"val_loss\")\nplateau = callbacks.ReduceLROnPlateau(\n    factor = 0.6,                                     \n    patience = 3,                                   \n    min_delt = 0.00001,                                \n    verbose = 0) \n\nhistory = model.fit(X_train, y_train,\n                    batch_size = 64,\n                    epochs = 30,\n                    validation_data = (X_valid, y_valid),\n                    callbacks = [early_stopping, plateau],\n                    verbose = 0)\n\nmin_loss = round(np.min(history.history['val_loss']), 5)\nmin_loss_epoch = np.argmin(history.history['val_loss']) + 1\nprint('Best valid RMSE at the epoch #{} : {}'.format(min_loss_epoch, min_loss))\n\nfig, ax = plt.subplots(figsize = (20, 4))\nsns.lineplot(x = history.epoch, y = history.history['loss'])\nsns.lineplot(x = history.epoch, y = history.history['val_loss'])\nax.set_title('Learning Curve (Loss) (Best value: {})'.format(min_loss))\nax.set_ylabel('Loss')\nax.set_xlabel('Epoch')\nax.legend(['train', 'test'], loc='best')\nplt.show()\n\ndel X_train, X_valid, y_train, y_valid, train\ngc.collect()","bdf18abc":"ss.loss = model.predict(test, batch_size = 8)\nss","1187dec2":"ss.to_csv('submission.csv', index = False)","4e268ee3":"<h2 style='color:white; background:#663399; border:0'><center>WORK IN PROGRESS...<\/center><\/h2>\n\n[**Back to the start**](#section-start)","359b3995":"First, let's load the data and take a look at basic statistics.","d49638f0":"<a id=\"section-8\"><\/a>\n<h2 style='color:white; background:#008294; border:0'><center>Test prediction<\/center><\/h2>\n\n[**Back to the table of contents**](#section-start)\n\nLet's take a look at our predictions and prepare them for submission.","1606b0e5":"We should also look at the correlation between features.","54d5db74":"Before starting the training process, let's visualize our architecture.","48627516":"As in the previous competitions, our data has no missing values. Now, let's look at the feature distributions.","023037a0":"We have 250000 training and 150000 test observations. All our data is in float32 format.\n\nBefore we continue, let's pull the target feature into the separate variable.","112dbebd":"All features are weakly correlated.\n\n<h2 style='color:white; background:#663399; border:0'><center>XGBRegressor Baseline<\/center><\/h2>\n\n[**Back to the start**](#section-start)\n\nAt first, we'll train a very simple basic XGBRegressor.","d6bc05c8":"It's important to see if our data has missing values.","57be58b3":"<h2 style='color:white; background:#663399; border:0'><center>EDA<\/center><\/h2>\n\n[**Back to the start**](#section-start)","33eb62e4":"<h1 style='color:white; background:#663399; border:0'><center> TPS-Aug: EDA, Baselines (XGB, Keras NN)<\/center><\/h1>\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/26480\/logos\/header.png?t=2021-04-09-00-57-05)\n\n<a id=\"section-start\"><\/a>\n\nThe goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with calculating the loss associated with a loan defaults. Although the features are anonymized, they have properties relating to real-world features.\n\nFor this competition, you will be predicting a `target loss` based on a number of feature columns given in the data. The ground truth loss is integer valued, although predictions can be continuous.\n\n### See also my previous TPS works:\n- [TPS-July: EDA, Baseline Analysis (XGBRegressor)](https:\/\/www.kaggle.com\/maksymshkliarevskyi\/tps-july-eda-baseline-analysis-xgbregressor)\n- [TPS-Jun: starting point (EDA, Baseline, CV)](https:\/\/www.kaggle.com\/maksymshkliarevskyi\/tps-jun-starting-point-eda-baseline-cv)","12dc5765":"The loss of the NN is slightly fewer. But we used very simple sighting models. There is still a lot of work to be done with the selection of optimal parameters.","73314b16":"Wow, f60 feature has very big values.","b2607c21":"<h2 style='color:white; background:#663399; border:0'><center>Keras NN Baseline<\/center><\/h2>\n\n[**Back to the start**](#section-start)\n\nIn this step, we'll train our baseline keras NN. We'll use the residual network from this excellent notebook [tabular residual network](https:\/\/www.kaggle.com\/oxzplvifi\/tabular-residual-network).\n\nLet's define architecture."}}