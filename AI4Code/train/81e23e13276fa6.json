{"cell_type":{"1a7db0c7":"code","161e5217":"code","bd3a0525":"code","110da8c5":"code","51cb85e3":"code","e4917936":"code","22945306":"code","01e2654e":"code","1095a66e":"code","eb442f11":"code","5017b965":"code","715671cb":"code","f3b938c4":"code","60a770b6":"code","42bc9e2b":"code","bdfc62d6":"code","3784f5f2":"code","027d3396":"code","bb006ae9":"code","2ac60765":"code","b1fc9483":"code","a1633b39":"code","0f9e05b2":"code","6f1aae58":"markdown","7e6cf584":"markdown","8fad8c58":"markdown","2165e06a":"markdown","f2548be1":"markdown","49e06c11":"markdown","4c6dff76":"markdown","d08a176c":"markdown","daba0c0a":"markdown","d4f4a4ac":"markdown","6892e053":"markdown","a63a1dba":"markdown","b1bb16bd":"markdown","a6809700":"markdown","bceaa23b":"markdown","96d637a8":"markdown","60ca2ead":"markdown","0527af37":"markdown","06fcfb4f":"markdown","832d9f30":"markdown","c05ecf02":"markdown","9d91ef30":"markdown","965b5026":"markdown"},"source":{"1a7db0c7":"#Standard imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n#to show the large number of features in this dataset\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","161e5217":"import h2o\nfrom h2o.automl import H2OAutoML\nfrom h2o.estimators.random_forest import H2ORandomForestEstimator\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\nfrom h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator\nfrom h2o.grid.grid_search import H2OGridSearch\n# Number of threads, nthreads = -1, means use all cores on your machine\n# max_mem_size is the maximum memory (in GB) to allocate to H2O\nh2o.init(nthreads = -1, max_mem_size = 16)","bd3a0525":"#Based on this great kernel https:\/\/www.kaggle.com\/arjanso\/reducing-dataframe-memory-size-by-65\ndef reduce_mem_usage(df):\n    start_mem_usg = df.memory_usage().sum() \/ 1024**2 \n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in df.columns:\n        if df[col].dtype != object:  # Exclude strings      \n            # make variables for Int, max and min\n            IsInt = False\n            mx = df[col].max()\n            mn = df[col].min()            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(df[col]).all(): \n                NAlist.append(col)\n                df[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = df[col].fillna(0).astype(np.int64)\n            result = (df[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < 65535:\n                        df[col] = df[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        df[col] = df[col].astype(np.uint32)\n                    else:\n                        df[col] = df[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)    \n            # Make float datatypes 32 bit\n            else:\n                df[col] = df[col].astype(np.float32)\n\n    return df, NAlist","110da8c5":"# import Dataset\ntrain_identity= pd.read_csv(\"..\/input\/train_identity.csv\", index_col='TransactionID')\ntrain_transaction= pd.read_csv(\"..\/input\/train_transaction.csv\", index_col='TransactionID')\ntest_identity= pd.read_csv(\"..\/input\/test_identity.csv\", index_col='TransactionID')\ntest_transaction = pd.read_csv('..\/input\/test_transaction.csv', index_col='TransactionID')","51cb85e3":"# Creat our train & test dataset\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)","e4917936":"#Delete the imports to free up memory\ndel train_identity,train_transaction,test_identity, test_transaction","22945306":"train, NAlist = reduce_mem_usage(train)","01e2654e":"test, NAlist = reduce_mem_usage(test)","1095a66e":"train.head()","eb442f11":"train.shape","5017b965":"hf_train = h2o.H2OFrame(train)\nhf_test = h2o.H2OFrame(test)\n#encode the binary repsonse as a factor\nhf_train['isFraud'] = hf_train['isFraud'].asfactor()","715671cb":"# Partition data into 70%, 15%, 15% chunks\n# Setting a seed will guarantee reproducibility\n\nsplits = hf_train.split_frame(ratios=[0.7, 0.15], seed=1)  \n\ntrain_x = splits[0]\nvalid = splits[1]\ntest_x = splits[2]","f3b938c4":"print(train_x.nrow)\nprint(valid.nrow)\nprint(test_x.nrow)","60a770b6":"y = 'isFraud'\nx = list(hf_train.columns)\nx.remove(y)","42bc9e2b":"# Number of CV folds (to generate level-one data for stacking)\nnfolds = 5","bdfc62d6":"my_gbm = H2OGradientBoostingEstimator(distribution=\"bernoulli\",\n                                      ntrees=10,\n                                      max_depth=3,\n                                      min_rows=2,\n                                      learn_rate=0.2,\n                                      nfolds=nfolds,\n                                      fold_assignment=\"Modulo\",\n                                      keep_cross_validation_predictions=True,\n                                      seed=1)\nmy_gbm.train(x=x, y=y, training_frame=train_x)","3784f5f2":"my_rf = H2ORandomForestEstimator(ntrees=100,\n                                 nfolds=nfolds,\n                                 fold_assignment=\"Modulo\",\n                                 keep_cross_validation_predictions=True,\n                                 seed=1)\nmy_rf.train(x=x, y=y, training_frame=train_x)","027d3396":"ensemble = H2OStackedEnsembleEstimator(model_id=\"my_ensemble_binomial\",\n                                       base_models=[my_gbm, my_rf])\nensemble.train(x=x, y=y, training_frame=train_x)","bb006ae9":"stack_test = ensemble.model_performance(test_x)","2ac60765":"perf_gbm_test = my_gbm.model_performance(test_x)\nperf_rf_test = my_rf.model_performance(test_x)\nbaselearner_best_auc_test = max(perf_gbm_test.auc(), perf_rf_test.auc())\nstack_auc_test = stack_test.auc()\nprint(\"Best Base-learner Test AUC:  {0}\".format(baselearner_best_auc_test))\nprint(\"Ensemble Test AUC:  {0}\".format(stack_auc_test))","b1fc9483":"# Generate predictions on test set \npred = ensemble.predict(hf_test)","a1633b39":"sample_submission = pd.read_csv('..\/input\/sample_submission.csv')\nsample_submission.shape","0f9e05b2":"sample_submission['isFraud'] = pred['p1'].as_data_frame().values\nsample_submission.to_csv('h2o_automl_submission_3.csv', index=False)","6f1aae58":"### Converting the pandas dataframe (memory reduced) to H2OFrame","7e6cf584":"Notice that split_frame() uses approximate splitting not exact splitting (for efficiency), so these are not exactly 70%, 15% and 15% of the total rows.","8fad8c58":"# Compare to base learner performance on the test set","2165e06a":"### Train a stacked ensemble using the GBM and RF above","f2548be1":"This kernel acts as a baseline for H2o AutoML stacked ensembles'. One can further build high performance models by adding\/removing base learners, hyperparameter tuning etc.\n\n**Please upvote the kernel if you found it helpful.**","49e06c11":"### Setting the target and predictor variables\n\nIn H2O, we use y to designate the response variable and x to designate the list of predictor columns.","4c6dff76":"### Memory reduction for train","d08a176c":"Also, encoding the response variable to factor is important bacause otherwise H2O will asssume it as numeric and will train a regression model instead of a classifiaction model.","daba0c0a":"<img src='https:\/\/image.slidesharecdn.com\/joeamsautoml-171107063815\/95\/using-h2o-automl-for-kaggle-competitions-10-638.jpg'><\/img>","d4f4a4ac":"Eval ensemble performance on the test data","6892e053":"The size of train dataset reduced by almost 65% after the operation (for the sake of simplicity, size reduction logs have not been printed)","a63a1dba":"### Partition data","b1bb16bd":"# Loading the H2O library and Starting a local H2O cluster (on your machine)","a6809700":"### Memory reduction for Test","bceaa23b":"### Train and cross-validate a RF","96d637a8":"# Creating a stacked Emsemble Model","60ca2ead":"### The objective of this kernel is to detect fraudulant transactions taking place on a ecommerce site using H2O AutoML\n\n### This kernel helps provide a fair understanding of the H2O's AutoML and it's syntactic nitty gritties. ","0527af37":"# H2O AutoML tutorial on IEEE-CIS Fruad detection compitition","06fcfb4f":"# Data Prep","832d9f30":"# 1. Generate a 2-model stacked ensemble (GBM + RF)**\n\n### Train and cross-validate a GBM","c05ecf02":"The size of train dataset reduced by almost 76% after the operation (for the sake of simplicity, size reduction logs have not been printed)","9d91ef30":"We shall first import the data in a pandas dataframe, carry out a memory reduction operation on the dataset (pandas dataframe) and then load it in a H2OFrame.","965b5026":"The default number of trees in an H2O Random Forest is 50, here, we have taken ntrees=100 so this RF will be twice as big as the default. Usually increasing the number of trees in an RF will increase performance as well. "}}