{"cell_type":{"345d0b60":"code","f3425783":"code","c5860361":"code","efda0ee7":"code","150da60e":"code","0d160040":"code","45c7e51a":"code","21c60548":"code","f2da19fb":"code","ee1146e4":"code","23066036":"code","c9ec337c":"code","7a524b7a":"code","0f8b7c6b":"code","d309c941":"code","5b035db8":"code","a0dad87b":"code","5505094d":"code","7e62ca31":"code","e1ac32d4":"code","0cbd6cb0":"code","ffd68435":"code","cd723356":"code","75d75707":"code","aee0e957":"code","9a0f2685":"code","7abefb72":"code","a4d9f9fe":"code","6a638ff6":"code","28181d6d":"code","0623e569":"code","0a1f58fa":"code","f91215d6":"code","7ff23a8b":"code","991f1dc0":"code","bb27026e":"code","b3037e2e":"code","cff695f7":"code","dc8d6a17":"code","a1229611":"code","8aa8f3a3":"markdown","39601806":"markdown","1f5ec59e":"markdown","df2a8d3e":"markdown","b30a5c5a":"markdown","f825f70f":"markdown","aaeac688":"markdown","5ce360ad":"markdown","a8d8aff3":"markdown","04e91cc8":"markdown","caa61f76":"markdown","c454ebd1":"markdown","30f5e8d2":"markdown","3e81093c":"markdown","31818dbd":"markdown","07bb0c8c":"markdown","72a54294":"markdown","d514e8a2":"markdown","fdc3e82a":"markdown","0c853113":"markdown","8da3a5bc":"markdown","b9c63ff2":"markdown","461285fc":"markdown","0140b07b":"markdown","18366f4a":"markdown","44fdfaca":"markdown","b034fa0b":"markdown","0920462d":"markdown","f1aaf499":"markdown","04234e6e":"markdown","9d64c4ef":"markdown","81446ede":"markdown","72c2dbcb":"markdown","376cf1f5":"markdown","63c45c50":"markdown","c3e7a754":"markdown","750c18d7":"markdown","cbaba0e6":"markdown","54175c76":"markdown","467ecfc4":"markdown","1dfcad9e":"markdown","e2f29714":"markdown","34b322d9":"markdown","dfa4efb4":"markdown","413327d0":"markdown","2b9cf505":"markdown","37c8b9bd":"markdown","84a3f539":"markdown","2148f1cf":"markdown"},"source":{"345d0b60":"def evalBinaryClassifier(model, x, y, labels=['Positives','Negatives']):\n    '''\n    source: https:\/\/towardsdatascience.com\/how-to-interpret-a-binary-logistic-regressor-with-scikit-learn-6d56c5783b49\n    Visualize the performance of  a Logistic Regression Binary Classifier.\n    \n    Displays a labelled Confusion Matrix, distributions of the predicted\n    probabilities for both classes, the ROC curve, and F1 score of a fitted\n    Binary Logistic Classifier. Author: gregcondit.com\/articles\/logr-charts\n    \n    Parameters\n    ----------\n    model : fitted scikit-learn model with predict_proba & predict methods\n        and classes_ attribute. Typically LogisticRegression or \n        LogisticRegressionCV\n    \n    x : {array-like, sparse matrix}, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples\n        in the data to be tested, and n_features is the number of features\n    \n    y : array-like, shape (n_samples,)\n        Target vector relative to x.\n    \n    labels: list, optional\n        list of text labels for the two classes, with the positive label first\n        \n    Displays\n    ----------\n    3 Subplots\n    \n    Returns\n    ----------\n    F1: float\n    '''\n    #model predicts probabilities of positive class\n    p = model.predict_proba(x)\n    if len(model.classes_)!=2:\n        raise ValueError('A binary class problem is required')\n    if model.classes_[1] == 1:\n        pos_p = p[:,1]\n    elif model.classes_[0] == 1:\n        pos_p = p[:,0]\n    \n    #FIGURE\n    plt.figure(figsize=[15,4])\n    \n    #1 -- Confusion matrix\n    cm = confusion_matrix(y,model.predict(x))\n    plt.subplot(131)\n    ax = sns.heatmap(cm, annot=True, cmap='Blues', cbar=False, \n                annot_kws={\"size\": 14}, fmt='g')\n    cmlabels = ['True Negatives', 'False Positives',\n              'False Negatives', 'True Positives']\n    for i,t in enumerate(ax.texts):\n        t.set_text(t.get_text() + \"\\n\" + cmlabels[i])\n    plt.title('Confusion Matrix', size=15)\n    plt.xlabel('Predicted Values', size=13)\n    plt.ylabel('True Values', size=13)\n      \n    #2 -- Distributions of Predicted Probabilities of both classes\n    df = pd.DataFrame({'probPos':pos_p, 'target': y})\n    plt.subplot(132)\n    plt.hist(df[df.target==1].probPos, density=True, bins=25,\n             alpha=.5, color='green',  label=labels[0])\n    plt.hist(df[df.target==0].probPos, density=True, bins=25,\n             alpha=.5, color='red', label=labels[1])\n    plt.axvline(.5, color='blue', linestyle='--', label='Boundary')\n    plt.xlim([0,1])\n    plt.title('Distributions of Predictions', size=15)\n    plt.xlabel('Positive Probability (predicted)', size=13)\n    plt.ylabel('Samples (normalized scale)', size=13)\n    plt.legend(loc=\"upper right\")\n    \n    #3 -- ROC curve with annotated decision point\n    fp_rates, tp_rates, _ = roc_curve(y,p[:,1])\n    roc_auc = auc(fp_rates, tp_rates)\n    plt.subplot(133)\n    plt.plot(fp_rates, tp_rates, color='green',\n             lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], lw=1, linestyle='--', color='grey')\n    #plot current decision point:\n    tn, fp, fn, tp = [i for i in cm.ravel()]\n    plt.plot(fp\/(fp+tn), tp\/(tp+fn), 'bo', markersize=8, label='Decision Point')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate', size=13)\n    plt.ylabel('True Positive Rate', size=13)\n    plt.title('ROC Curve', size=15)\n    plt.legend(loc=\"lower right\")\n    plt.subplots_adjust(wspace=.3)\n    plt.show()\n    #Print and Return the F1 score\n    tn, fp, fn, tp = [i for i in cm.ravel()]\n    precision = tp \/ (tp + fp)\n    recall = tp \/ (tp + fn)\n    F1 = 2*(precision * recall) \/ (precision + recall)\n    printout = (\n        f'Precision: {round(precision,2)} | '\n        f'Recall: {round(recall,2)} | '\n        f'F1 Score: {round(F1,2)} | '\n    )\n    print(printout)\n    return F1","f3425783":"conda install gxx_linux-64 gcc_linux-64 swig","c5860361":"import h2o\nh2o.init(ip=\"localhost\", port=54323)","efda0ee7":"import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")","150da60e":"\ntrain_data = h2o.import_file(\"\/kaggle\/input\/cat-in-the-dat\/train.csv\")\ntest_data = h2o.import_file(\"\/kaggle\/input\/cat-in-the-dat\/test.csv\")","0d160040":" test_id = h2o.import_file('\/kaggle\/input\/cat-in-the-dat\/test.csv')['id']","45c7e51a":"\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator","21c60548":"glm_fit1 = H2OGeneralizedLinearEstimator(family='binomial', model_id='glm_fit1')","f2da19fb":"train_data[\"target\"] = train_data[\"target\"].asfactor()","ee1146e4":"train, valid, test = train_data.split_frame(ratios=[0.7, 0.15], seed=42)  \ny = 'target'\nx = list(train_data.columns)","23066036":"id_var = 'id'\nx.remove(id_var)  #remove the response","c9ec337c":"x.remove(y)  #remove the response\nprint(x)","7a524b7a":"glm_fit1.train(x=x, y=y, training_frame=train)","0f8b7c6b":"glm_fit2 = H2OGeneralizedLinearEstimator(family='binomial', model_id='glm_fit2', lambda_search=True,balance_classes = True)\nglm_fit2.train(x=x, y=y, training_frame=train, validation_frame=valid)","d309c941":"glm_perf1 = glm_fit1.model_performance(test)\nglm_perf2 = glm_fit2.model_performance(test)","5b035db8":"\nprint (glm_perf1.gini())\nprint (glm_perf2.gini())","a0dad87b":"\nprint (glm_fit2.gini(train=True))\nprint (glm_fit2.gini(valid=True))","5505094d":"\nfrom h2o.estimators.random_forest import H2ORandomForestEstimator","7e62ca31":"\nrf_fit1 = H2ORandomForestEstimator(model_id='rf_fit1',   seed=1)\n","e1ac32d4":"rf_fit1.train(x=x, y=y, training_frame=train,validation_frame=valid)","0cbd6cb0":"rf_fit2 = H2ORandomForestEstimator(model_id='rf_fit2', ntrees=100,   seed=1)\nrf_fit2.train(x=x, y=y, training_frame=train,validation_frame=valid)","ffd68435":"rf_perf1 = rf_fit1.model_performance(test)\nrf_perf2 = rf_fit2.model_performance(test)","cd723356":"\nprint(rf_perf1.gini())\nprint(rf_perf2.gini())","75d75707":"rf_fit3 = H2ORandomForestEstimator(model_id='rf_fit3', seed=1, nfolds=5)\nrf_fit3.train(x=x, y=y, training_frame=train)","aee0e957":"print( rf_fit3.gini(xval=True))","9a0f2685":"\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator","7abefb72":"\ngbm_fit1 = H2OGradientBoostingEstimator(model_id='gbm_fit1',   seed=1)\ngbm_fit1.train(x=x, y=y, training_frame=train, validation_frame=valid)","a4d9f9fe":"gbm_fit2 = H2OGradientBoostingEstimator(model_id='gbm_fit2', ntrees=500,   seed=1)\ngbm_fit2.train(x=x, y=y, training_frame=train,validation_frame=valid)","6a638ff6":"# Now let's use early stopping to find optimal ntrees\n\ngbm_fit3 = H2OGradientBoostingEstimator(model_id='gbm_fit3', \n                                        ntrees=1000, \n                                        score_tree_interval=5,     #used for early stopping\n                                        stopping_rounds=3,         #used for early stopping\n                                        stopping_metric='AUC',     #used for early stopping\n                                        stopping_tolerance=0.0005, #used for early stopping\n                                        seed=1)\n# The use of a validation_frame is recommended with using early stopping\ngbm_fit3.train(x=x, y=y, training_frame=train, validation_frame=valid)","28181d6d":"# Let's try XGBOOSTING\nfrom h2o.estimators import H2OXGBoostEstimator\nparam = {\n      \"model_id\": 'gbm_fit4'\n    , \"ntrees\" : 100\n    , \"max_depth\" : 10\n    , \"learn_rate\" : 0.02\n    , \"sample_rate\" : 0.7\n    , \"col_sample_rate_per_tree\" : 0.9\n    , \"min_rows\" : 5\n    , \"seed\": 4241\n    , \"score_tree_interval\": 100\n}\ngbm_fit4 = H2OXGBoostEstimator(**param)\ngbm_fit4.train(x=x, y=y, training_frame=train, validation_frame=valid)","0623e569":"gbm_perf1 = gbm_fit1.model_performance(test)\ngbm_perf2 = gbm_fit2.model_performance(test)\ngbm_perf3 = gbm_fit3.model_performance(test)\ngbm_perf4 = gbm_fit4.model_performance(test)","0a1f58fa":"\nprint (gbm_perf1.gini())\nprint (gbm_perf2.gini())\nprint (gbm_perf3.gini())\nprint (gbm_perf4.gini())","f91215d6":"# Import H2O DL:\nfrom h2o.estimators.deeplearning import H2ODeepLearningEstimator","7ff23a8b":"# Initialize and train the DL estimator:\n\ndl_fit1 = H2ODeepLearningEstimator(model_id='dl_fit1',   seed=1,  balance_classes = True)\ndl_fit1.train(x=x, y=y, training_frame=train,validation_frame=valid)","991f1dc0":"dl_fit2 = H2ODeepLearningEstimator(model_id='dl_fit2', \n                                   epochs=50, \n                                   hidden=[10,10], \n                                   stopping_rounds=0,  #disable early stopping\n                                   seed=1,\n                                   balance_classes = True)\ndl_fit2.train(x=x, y=y, training_frame=train,validation_frame=valid)\n","bb27026e":"dl_fit3 = H2ODeepLearningEstimator(model_id='dl_fit3', \n                                   epochs=500, \n                                   hidden=[10,10],\n                                   score_interval=1,          #used for early stopping\n                                   stopping_rounds=50,         #used for early stopping\n                                   stopping_metric='AUC',     #used for early stopping\n                                   stopping_tolerance=0.0005, #used for early stopping\n                                   seed=1,  \n                                   balance_classes = True)\ndl_fit3.train(x=x, y=y, training_frame=train, validation_frame=valid)","b3037e2e":"dl_perf1 = dl_fit1.model_performance(test)\ndl_perf2 = dl_fit2.model_performance(test)\ndl_perf3 = dl_fit3.model_performance(test)","cff695f7":"# Retreive test set AUC\nprint (dl_perf1.gini())\nprint (dl_perf2.gini())\nprint( dl_perf3.gini())","dc8d6a17":"test_pred = gbm_fit4.predict(test_id) # test","a1229611":"test_pred","8aa8f3a3":"Train a GBM with more trees Next we will increase the number of trees used in the GBM by setting ntrees=500. The default number of trees in an H2O GBM is 50, so this GBM will trained using ten times the default. Increasing the number of trees in a GBM is one way to increase performance of the model, however, you have to be careful not to overfit your model to the training data by using too many trees. To automatically find the optimal number of trees, you must use H2O's early stopping functionality. This example will not do that, however, the following example will.","39601806":"# General Findinds\n\n- Is data synthetic? by [cpmpml](https:\/\/www.kaggle.com\/cpmpml)\n\nsource: https:\/\/www.kaggle.com\/c\/cat-in-the-dat\/discussion\/105713\n\n\n- Encoding cyclical features using sin and cos transformation by [gogo827jz](https:\/\/www.kaggle.com\/gogo827jz)\nsource: https:\/\/www.kaggle.com\/c\/cat-in-the-dat\/discussion\/105610\n\n- CATEGORICAL MATERIAL MUST READ by (brunhs)[https:\/\/www.kaggle.com\/brunhs]\n\nsource: https:\/\/www.kaggle.com\/c\/cat-in-the-dat\/discussion\/105512\n\n- CATEGORICAL MATERIAL SURVEY\ud83d\udc31 & Deduplication & Record Linkage. by [caesarlupum](https:\/\/www.kaggle.com\/caesarlupum)\n\nsource: https:\/\/www.kaggle.com\/c\/cat-in-the-dat\/discussion\/111930\n\n\n","1f5ec59e":"Train a default GLM We first create an object of class, \"H2OGeneralizedLinearEstimator\".","df2a8d3e":"# Load files","b30a5c5a":"### Random Forest\n\nH2O's Random Forest (RF) is implements a distributed version of the standard Random Forest algorithm and variable importance measures.","f825f70f":"Compare model performance Let's compare the performance of the three GBMs that were just trained.","aaeac688":"# Initialize and train the GBM estimator\n","5ce360ad":"# Retreive test set AUC","a8d8aff3":"Compare model performance Again, we will compare the model performance of the three models using a test set and AUC.","04e91cc8":"# Compare test AUC to the training AUC and validation AUC","caa61f76":"\n# About this Competition\n![](http:\/\/img08.deviantart.net\/3e2f\/i\/2016\/121\/7\/8\/beerus__god_of_destruction_by_liloutehcat-da0wye6.png)\n\n> #### In this competition, you will be predicting the probability [0, 1] of a binary target column.\n\nThe data contains binary features (bin_*), nominal features (nom_*), ordinal features (ord_*) as well as (potentially cyclical) day (of the week) and month features. The string ordinal features ord_{3-5} are lexically ordered according to string.ascii_letters.\nSince the purpose of this competition is to explore various encoding strategies, the data has been simplified in that (1) there are no missing values, and (2) the test set does not contain any unseen feature values (See this). (Of course, in real-world settings both of these factors are often important to consider!)\n\n#### Files\n- train.csv - the training set\n- test.csv - the test set; you must make predictions against this data\n- sample_submission.csv - a sample submission file in the correct format","c454ebd1":"> #### Inspired by:\n- https:\/\/www.kaggle.com\/felipeleiteantunes\/h2o-ai-from-linear-models-to-deep-learning (upvote this !) Not only useful but also valuable","30f5e8d2":"# Import H2O RF","3e81093c":"Now that glm_fit1 object is initialized, we can train the model:","31818dbd":"Train a DL with early stopping This example will use the same model parameters as dl_fit2, however, we will turn on early stopping and specify the stopping criterion. We will also pass a validation set, as is recommended for early stopping.","07bb0c8c":"\n1. # Instructions to download:\n> #### http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/downloading.html\n\n1. # Documentation:\n> #### https:\/\/h2o-release.s3.amazonaws.com\/h2o\/rel-turan\/4\/docs-website\/h2o-py\/docs\/intro.html\n\n1. # A booklet:\n> #### http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/booklets\/PythonBooklet.pdf\n\n1. # A presentation:\n> #### https:\/\/pt.slideshare.net\/0xdata\/intro-to-h2o-in-python-data-science-la\n\nAnd many more questions:\n<html>\n<body>\n\n<p><font size=\"5\" color=\"Blue\">\nIf you find this kernel useful or interesting, please don't forget to upvote the kernel =)\n<\/font><\/p>\n\n<\/body>\n<\/html>\n\n","72a54294":"# Retreive test set AUC","d514e8a2":"# Initialize the RF estimator\n","fdc3e82a":"# Load the H2O library and start up the H2O cluter locally on your machine","0c853113":"# Let's try XGBOOSTING","8da3a5bc":"# Import H2O DL","b9c63ff2":"# Top kernels\n\n-  ### \ud83d\udc31 Cat with Null Importance - Target Permutation by @CaesarLupum\n\nSource: https:\/\/www.kaggle.com\/caesarlupum\/cat-with-null-importance-target-permutation\n\n- ###  An Overview of Encoding Techniques by @shahules\n\nSource: https:\/\/www.kaggle.com\/shahules\/an-overview-of-encoding-techniques\n\n-  ### EDA & Feat Engineering - Encode & Conquer by @kabure\n\nSource: https:\/\/www.kaggle.com\/kabure\/eda-feat-engineering-encode-conquer\n\n- ###  Why Not Logistic Regression? by @peterhurford\n\nSource: https:\/\/www.kaggle.com\/peterhurford\/why-not-logistic-regression\n\n-  ### OH my Ca by @superant\n\nSource: https:\/\/www.kaggle.com\/superant\/oh-my-cat\n\n- ###  Entity embeddings to handle categories by @abhishek\n\nSource: https:\/\/www.kaggle.com\/abhishek\/entity-embeddings-to-handle-categories\n\n- ###  2nd place Solution - Categorical FE Callenge by @adaubas\n\nSource: https:\/\/www.kaggle.com\/adaubas\/2nd-place-solution-categorical-fe-callenge\n\n- ###  \ud83d\udc31 CatComp - Simple Target Encoding by @CaesarLupum\n\nSource: https:\/\/www.kaggle.com\/caesarlupum\/catcomp-simple-target-encoding\n\n-  ### Handling Categorical Variables:Encoding & Modeling by @vikassingh1996\n\nSource: https:\/\/www.kaggle.com\/vikassingh1996\/handling-categorical-variables-encoding-modeling\n\n-  ### R GLMNET by @ccccat\n\nSource: https:\/\/www.kaggle.com\/ccccat\/r-glmnet\n\n-  ### Exploring CATegorical encodings  by @artgor\n\nSource: https:\/\/www.kaggle.com\/artgor\/exploring-categorical-encodings\n\n- ### CatBoost Baseline with Feature Importance by @gogo827jz\n\nSource: https:\/\/www.kaggle.com\/gogo827jz\/catboost-baseline-with-feature-importance\n","461285fc":"# Retreive test set AUC","0140b07b":"# Categorical Feature Encoding Challenge\n[Crisl\u00e2nio Mac\u00eado](https:\/\/medium.com\/sapere-aude-tech) -  Last Update in March, 07th, 2021\n\n\n- [**Github**](https:\/\/github.com\/crislanio)\n- [**Linkedin**](https:\/\/www.linkedin.com\/in\/crislanio\/)\n- [**Medium**](https:\/\/medium.com\/sapere-aude-tech)\n- [**Quora**](https:\/\/www.quora.com\/profile\/Crislanio)\n- [**Ensina.AI**](https:\/\/medium.com\/ensina-ai\/an%C3%A1lise-dos-dados-abertos-do-governo-federal-ba65af8c421c)\n- [**Hackerrank**](https:\/\/www.hackerrank.com\/crislanio_ufc?hr_r=1)\n- [**Blog**](https:\/\/medium.com\/@crislanio.ufc)\n- [**Personal Page**](https:\/\/crislanio.wordpress.com\/about)\n- [**Twitter**](https:\/\/twitter.com\/crs_macedo)\n\n----------\n----------\n","18366f4a":"# Initialize and train the DL estimator\n","44fdfaca":"## Final","b034fa0b":"Train a default GBM First we will train a basic GBM model with default parameters. GBM will infer the response distribution from the response encoding if not specified explicitly through the distribution argument. A seed is required for reproducibility.","0920462d":"H2OGeneralizedLinearEstimator","f1aaf499":"To evaluate the cross-validated AUC, do the following:","04234e6e":"# Import H2O GBM","9d64c4ef":"![](https:\/\/miro.medium.com\/max\/5120\/1*hSDIm8k315XGjxZ9gqnhvA.jpeg)","81446ede":"# Now let's use early stopping to find optimal ntrees\n","72c2dbcb":"Cross-validate performance Rather than using held-out test set to evaluate model performance, a user may wish to estimate model performance using cross-validation. Using the RF algorithm (with default model parameters) as an example, we demonstrate how to perform k-fold cross-validation using H2O. No custom code or loops are required, you simply specify the number of desired folds in the nfolds argument. Since we are not going to use a test set here, we can use the original (full) dataset, which we called data rather than the subsampled train dataset. Note that this will take approximately k (nfolds) times longer than training a single RF model, since it will train k models in the cross-validation process (trained on n(k-1)\/k rows), in addition to the final model trained on the full training_frame dataset with n rows.","376cf1f5":"Train a default DL First we will train a basic DL model with default parameters. DL will infer the response distribution from the response encoding if not specified explicitly through the distribution argument. H2O's DL will not be reproducbible if run on more than a single core, so in this example, the performance metrics below may vary slightly from what you see on your machine. In H2O's DL, early stopping is enabled by default, so below, it will use the training set and default stopping parameters to perform early stopping.","63c45c50":"#### Train a GLM with lambda search\nNext we will do some automatic tuning by passing in a validation frame and setting lambda_search = True. Since we are training a GLM with regularization, we should try to find the right amount of regularization (to avoid overfitting). The model parameter, lambda, controls the amount of regularization in a GLM model and we can find the optimal value for lambda automatically by setting lambda_search = True and passing in a validation frame (which is used to evaluate model performance using a particular value of lambda).","c3e7a754":"Train an RF with more trees Next we will increase the number of trees used in the forest by setting ntrees = 100. The default number of trees in an H2O Random Forest is 50, so this RF will be twice as big as the default. Usually increasing the number of trees in an RF will increase performance as well. Unlike Gradient Boosting Machines (GBMs), Random Forests are fairly resistant (although not free from) overfitting by increasing the number of trees. See the GBM example below for additional guidance on preventing overfitting using H2O's early stopping functionality.","750c18d7":"Train a GBM with early stopping We will again set ntrees = 500, however, this time we will use early stopping in order to prevent overfitting (from too many trees). All of H2O's algorithms have early stopping available, however, with the exception of Deep Learning, it is not enabled by default. There are several parameters that should be used to control early stopping. The three that are generic to all the algorithms are: stopping_rounds, stopping_metric and stopping_tolerance. The stopping metric is the metric by which you'd like to measure performance, and so we will choose AUC here. The score_tree_interval is a parameter specific to Random Forest and GBM. Setting score_tree_interval=5 will score the model after every five trees. The parameters we have set below specify that the model will stop training after there have been three scoring intervals where the AUC has not increased more than 0.0005. Since we have specified a validation frame, the stopping tolerance will be computed on validation AUC rather than training AUC.","cbaba0e6":"Now that rf_fit1 object is initialized, we can train the model:","54175c76":"Train and a default RF First we will train a basic Random Forest model with default parameters. Random Forest will infer the response distribution from the \nresponse encoding. A seed is required for reproducibility. :\n","467ecfc4":"test_pred","1dfcad9e":"### Import H2O GLM","e2f29714":"#### H2O Machine Learning\n> Now that we have prepared the data, we can train some models. We will start by training a single model from each of the H2O supervised algos:\n\n- Generalized Linear Model (GLM)\n- Random Forest (RF)\n- Gradient Boosting Machine (GBM)\n- Deep Learning (DL)\n- Generalized Linear Model (GLM)\n\nLet's start with a basic binomial Generalized Linear Model (GLM). By default, H2O's GLM uses a regularized, elastic net model.","34b322d9":"# Import Libraries","dfa4efb4":"\n<html>\n<body>\n\n<p><font size=\"5\" color=\"purple\">If you like my kernel please consider upvoting it<\/font><\/p>\n<p><font size=\"4\" color=\"purple\">Don't hesitate to give your suggestions in the comment section<\/font><\/p>\n\n<\/body>\n<\/html>\n","413327d0":"Compare model performance Let's compare the performance of the two RFs that were just trained.","2b9cf505":"Train a DL with new architecture and more epochs Next we will increase the number of epochs used in the GBM by setting epochs=20 (the default is 10). Increasing the number of epochs in a deep neural net may increase performance of the model, however, you have to be careful not to overfit your model. To automatically find the optimal number of epochs, you must use H2O's early stopping functionality. Unlike the rest of the H2O algorithms, H2O's DL will use early by default, so we will first turn it off in the next example by setting stopping_rounds=0, for comparison.","37c8b9bd":"Evaluate model performance\n\nLet's compare the performance of the two GLMs that were just trained.","84a3f539":"# Retreive test set AUC","2148f1cf":"# Deep Learning\nH2O's Deep Learning algorithm is a multilayer feed-forward artificial neural network. It can also be used to train an autoencoder, however, in the example below we will train a standard supervised prediction model"}}