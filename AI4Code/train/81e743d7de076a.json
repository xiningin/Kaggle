{"cell_type":{"01145717":"code","97850736":"code","2f665b83":"code","db627d03":"code","97c6ecf8":"code","fd3d5dd9":"code","4d337a63":"code","93e8e2b8":"code","01d1d26e":"code","b3cac3fa":"code","c91a1e0b":"code","d9691a04":"code","611d5e63":"code","6e7e116d":"code","fa714cb0":"code","4c0d4404":"markdown","3a35b5bf":"markdown","db3a9f8d":"markdown","21a9ab7f":"markdown","63f095a6":"markdown","88aa704f":"markdown","0fdf8979":"markdown","a5b679ac":"markdown","194f080d":"markdown","a9bdf941":"markdown","74cb87df":"markdown","4e92ceab":"markdown","bf694436":"markdown"},"source":{"01145717":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","97850736":"Data = pd.read_csv(\"\/kaggle\/input\/heart-disease-prediction-using-logistic-regression\/framingham.csv\")\nData.info()\n","2f665b83":"Data.drop(['education'], axis=1, inplace=True)\nData.isnull().sum()\n\nData.dropna(axis = 0, inplace = True)\nprint(Data.shape[0])\nData.isnull().sum()\n","db627d03":"Y = Data.TenYearCHD.values\nX = Data.drop(['TenYearCHD'], axis = 1)","97c6ecf8":"sc= StandardScaler()\nX = sc.fit_transform(X)\n","fd3d5dd9":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\nX_train = X_train.T\nX_test = X_test.T\nY_train = Y_train.T\nY_test = Y_test.T\n\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","4d337a63":"###Initialize the weights and bias\ndef initialize_W_b_with_zeros(num_features):\n    w = np.zeros(shape = (num_features,1))\n    b = 0\n    return w,b","93e8e2b8":"### Sigmoid Function\n\ndef sigmoid(z):\n    s = 1\/(1+ np.exp(-z))\n    \n    return s","01d1d26e":"#Forward and Backward propagation function \ndef propagate(w,b, X,Y):\n    \n    m = X.shape[1]\n    z = np.dot(w.T, X) + b\n    A = sigmoid(z)\n    \n    loss =  - (Y * np.log(A) + (1-Y) * np.log( 1-A) )\n    cost=  np.sum(loss)\/m\n    \n    dw = (1 \/ m) * np.dot(X, (A-Y).T)\n    db = (1 \/ m) * np.sum(A-Y)\n    \n    gradient= {\"dw\": dw,\n             \"db\": db}\n    \n    return gradient, cost","b3cac3fa":"def update(w,b, X,Y, num_iterations, learning_rate):\n    \n    costs = []\n    \n    for i in range( num_iterations ):\n        gradient, cost = propagate(w,b, X,Y)\n        \n        dw = gradient['dw']\n        db = gradient['db']\n        \n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        \n        if i % 20 == 0:\n            costs.append(cost)\n            \n    parameters = {\"w\": w,\n                 \"b\": b}\n    \n    gradient= {\"dw\": dw,\n             \"db\": db}\n    \n    return parameters, gradient, costs","c91a1e0b":"def predict( w,b,X):\n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0], 1)\n    \n    A = sigmoid( np.dot(w.T , X) + b)\n    \n    for i in range(A.shape[1]):\n        if A[:,i] > 0.5 :\n              Y_prediction[:,i] = 1 \n      \n    return Y_prediction\n    ","d9691a04":"def Logistic_Regression_model(X_train, X_test, Y_train, Y_test,num_iterations, learning_rate ):\n    num_features = X_train.shape[0]\n    w,b = initialize_W_b_with_zeros(num_features)\n    parameters, gradient, costs = update(w,b, X_train,Y_train, num_iterations, learning_rate)\n    \n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n    \n    Y_Test_Predict = predict(w,b, X_test)\n    \n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_Test_Predict - Y_test)) * 100))\n\n    \n    Dictionary = {\"Prediction \": Y_Test_Predict,\n                \"Weight\": w,\n                \"Bias\" :b,\n                \"Cost Function\" : costs}\n    \n    return Dictionary","611d5e63":"Dictionary = Logistic_Regression_model(X_train, X_test, Y_train, Y_test, num_iterations = 1000, learning_rate = 0.10 )\n","6e7e116d":"# Plot learning curve (with costs)\nimport matplotlib.pyplot as plt\ncosts = np.squeeze(Dictionary['Cost Function'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations')\nplt.title(\"Cost Reduction\")\nplt.show()","fa714cb0":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train.T,Y_train.T)\nprint(\"test accuracy {}\".format(lr.score(X_test.T,Y_test.T)))  ","4c0d4404":"# Logistic Regression\n**Mathematical expression of the algorithm**:\n\nFor one example $x^{(i)}$:\n$$z^{(i)} = w^T x^{(i)} + b $$\n$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$ \n$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})$$\n\nThe cost is computed by taking the summation of all training examples:\n$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})$$","3a35b5bf":"# Propagation\nFor forward propagation we compute\n\n$A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n\nthen compute the cost function:\n\n$J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n\nThen, for backward propagation, we compute the gradients dw and db by taking the derivatives.\n\n$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$","db3a9f8d":"For ensuring the accuracy of our model, the Logistic Regression classifier of Sklearn is given.","21a9ab7f":"Here, we defined sigmoif function.\n\n$sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$","63f095a6":"# Prediction\n\nAfter creating the logistic regression model, we use test set for predicting the output.\n\n$\\hat{Y} = A = \\sigma(w^T X + b)$\n\nWe take an numpy array of m columns for m data samples to predict the output $\\hat{Y}$ ","88aa704f":"# Logistic Regression Model\nIt is the final function integrating all the functions. We take the number of features present in the dependent variable which is X_Train in this case. the number of features is 14 for this dataset. So we initialize weights and biases of that diemnsion. \n\nThe model uses training set to learn the optimized weights and biases which then we use for test set. finally, we calculate the test accuracy.\n\n","0fdf8979":"Here, we are dropping the Education feature, since it is not relevant with weither the patient will get affected with CHD or not.\n\nAfter that we are dropping the rows containing null values since they dont contain all the feature information regarding the specific patient.","a5b679ac":"# Update the weights and biases\n\nHere, we update the weights and biases after every iteration of the propagation. we record the updated weights for the next iteration and also try to minimize the cost at each iteration.","194f080d":"At first, we initialize the weights and bias as vectors of zeros.","a9bdf941":"# Heart Disease Prediction using Logistic Regression\nThis notebook is created for classifying binary classification of heart disease i.e. whether the patient has the 10 year risk of coronary heart disease or not. Since logistic regression is used to model binary dependent variable, i used it to estimate the probabilities of the problem.\n# Dataset\nhttps:\/\/www.kaggle.com\/dileep070\/heart-disease-prediction-using-logistic-regression\n\n","74cb87df":"test accuracy: 84.4 %","4e92ceab":"Now that the data is ready , we have to only bring them in standardized form using Scikit learn StandardScaler Class.","bf694436":"Here, we are creating the independent variable X and dependent variable Y from the dataframe. "}}