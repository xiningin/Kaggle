{"cell_type":{"f25803e6":"code","fd337de2":"code","baed72e3":"code","cf84c8d0":"code","591cdcc3":"code","0f36dd8d":"code","43d3fe82":"code","96f8cd5e":"code","b0ada6c1":"code","61c992ca":"code","11873eae":"code","27cdc848":"code","bf4f9b65":"code","0773346b":"code","e11df70e":"code","a1e4c635":"code","2fad5c5d":"code","596e9086":"code","58c4d9e1":"code","0a03b875":"code","125587fe":"code","7fca2bb0":"code","d39b46d9":"code","6b87d779":"markdown","7fe46d41":"markdown","d5a0f856":"markdown","56674c96":"markdown","3e131494":"markdown","7afad132":"markdown","c78e1d82":"markdown"},"source":{"f25803e6":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.cluster import DBSCAN\nfrom nltk.corpus import stopwords\nfrom spacy.matcher import Matcher \nfrom collections import  Counter\nimport matplotlib.pyplot as plt\nfrom spacy.tokens import Span \nimport tensorflow_hub as hub\n#from rake_nltk import Rake\nimport tensorflow as tf\nimport pyLDAvis.gensim\nfrom tqdm import tqdm\nimport seaborn as sns\nimport networkx as nx\nimport pandas as pd\nimport numpy as np\nimport pyLDAvis\nimport gensim\nimport spacy\nimport os\nimport gc\nimport re\n\nfrom scipy.spatial import distance\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nimport nltk\n\nimport gensim.corpora as corpora\nfrom gensim import models\nfrom gensim.utils import simple_preprocess\nfrom gensim.test.utils import common_texts\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom gensim import similarities\n\n\n\n!pip install -U sentence-transformers\n\n# Library from here: https:\/\/github.com\/UKPLab\/sentence-transformers\nfrom sentence_transformers import SentenceTransformer\n#pd.describe_option('display')\npd.options.display.max_seq_items = 2500\npd.set_option('display.max_columns', None)  # or 1000\npd.set_option('display.max_rows', None)  # or 1000\npd.set_option('display.max_colwidth', -1)","fd337de2":"# Cleaned dataset from this kernel: https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv\n!ls \/kaggle\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/","baed72e3":"CLEAN_DATA_PATH = \"..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/\"\n\npmc_df = pd.read_csv(CLEAN_DATA_PATH + \"clean_pmc.csv\")\nbiorxiv_df = pd.read_csv(CLEAN_DATA_PATH + \"biorxiv_clean.csv\")\ncomm_use_df = pd.read_csv(CLEAN_DATA_PATH + \"clean_comm_use.csv\")\nnoncomm_use_df = pd.read_csv(CLEAN_DATA_PATH + \"clean_noncomm_use.csv\")\n","cf84c8d0":"\npapers_df_el = pd.concat([pmc_df,\n                       biorxiv_df,\n                       comm_use_df,\n                       noncomm_use_df], axis=0).reset_index(drop=True)","591cdcc3":"papers_df_el['combined']=papers_df_el['title']+ ' '+papers_df_el['abstract']","0f36dd8d":"papers_df_el.head(1)","43d3fe82":"papers_df_el.dropna(inplace=True)\npapers_df_el = papers_df_el.drop_duplicates(subset=['title'], keep=False)\npapers_combined_el = papers_df_el['combined'].str.lower().tolist()","96f8cd5e":"nltk.download('stopwords')\nstop_words = stopwords.words('english')\nstop_words.extend(['also', 'may', 'however', 'could',\"''\",'=','.','(',')','abstract', 'found', 'using','used','result','including','based','although','among','two','three','one','or','use'])\n\ndef common_words_graph(df,col, n):\n\n    corpus=[]\n    lem=WordNetLemmatizer()\n    new= df[col].dropna().str.split()\n    new=new.values.tolist()\n    corpus=[lem.lemmatize(word.lower()) for i in new for word in i if word not in stop_words]\n    corpus=[word for word in corpus if word not in stop_words]\n    counter=Counter(corpus).most_common()[:n]\n    \n    top_words = [x[0] for x in counter]\n    top_counts = [x[1] for x in counter]\n    \n    \n    plt.figure(figsize=(9,7))\n    sns.barplot(x=top_counts,y=top_words)\n    plt.title('Top '+str(n)+' words in '+col)\n    plt.show()\n","b0ada6c1":"common_words_graph(papers_df_el, 'title',20)","61c992ca":"common_words_graph(papers_df_el, 'abstract',20)","11873eae":"def clean_word_round1(text):\n    text = text.lower()\n    text = text.replace('(',\"\")\n    text = text.replace(')',\"\")\n    text = text.replace('=',\"\")\n    text = text.replace('-',\"\")\n    text= text.split()\n    #lem=WordNetLemmatizer()\n    #corpus=[lem.lemmatize(word.lower()) for word in text]\n    corpus=' '.join([word for word in text if word not in stop_words])\n    return corpus","27cdc848":"c_combined_words= papers_df_el['combined'].apply(clean_word_round1).tolist()","bf4f9b65":"def show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color = 'white',\n        max_words = 200,\n        max_font_size = 40, \n        scale = 3,\n        random_state = 42\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize = (20, 20))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize = 20)\n        fig.subplots_adjust(top = 2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\n    ","0773346b":"show_wordcloud(c_combined_words, title = 'Title and abstract')","e11df70e":"# just check the words\ncv = CountVectorizer()\ncombined_matrix = cv.fit_transform(c_combined_words)\n\ncombined =cv.get_feature_names()\ncombined_key_words_df = pd.DataFrame(combined_matrix.sum(axis =0).T,index = combined, columns =['count'] )\n\nfig, ax = plt.subplots(figsize = (10,10))\ncombined_key_words_df.sort_values(by ='count', ascending = False).iloc[:20].sort_values('count', ascending=True).plot(kind='barh', ax = ax)","a1e4c635":"# key_words \ncommon_words = ['ncov','covid','cov','sars','coronavirus','medical care','health care']\n\n#1. Resources to support skilled nursing facilities and long term care facilities.\nkey_words_dic = {1 : ['skilled nursing','long term care','ltc', 'resource','facilities','facility','health care'],\n\n#2. Mobilization of surge medical staff to address shortages in overwhelmed communities\n 2 : ['mobilization', 'mobilize','mobilization', 'surge', 'medical', 'medical staff', 'medical professional'\n                 'overwhelmed communities','overwhelmed','communities','shortage','lack','shortfall'],\n\n#3. Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with\/without other organ failure \u2013 particularly for viral etiologies\n 3 : ['age-adjusted mortality','age-adjusted','mortality data','acute respiratory', 'distress syndrome',\n                 'ards', 'organ failure','etiologies','etiology','senior','respiratory','mortality','viral etiologies'],\n\n#4. Extracorporeal membrane oxygenation (ECMO) outcomes data of COVID-19 patients\n4 : ['extracorporeal', 'membrane','oxygenation','ecmo', 'extracorporeal membrane oxgenation','covid-19','patients','patients'],\n\n#5. Outcomes data for COVID-19 after mechanical ventilation adjusted for age'\n5 : ['mechanical','ventilation', 'age', 'adjusted for age', 'mechanical ventilation adjusted for age'],\n\n#6. Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest'\n6 : ['frequency','manifestations', 'manifestation', 'extrapulmonary','course of extrapulmonary', 'extrapulmonary manifestation'\n                 'cardiomyopathy', 'cardiac arrest', 'cardiac'],\n\n#7. Application of regulatory standards (e.g., EUA, CLIA) and ability to adapt care to crisis standards of care level.\n7 : ['application of regulatroy standard','regulatroy standard', 'eua','clia', 'ability', 'care','crisis standard', 'care level'],\n\n#8. Approaches for encouraging and facilitating the production of elastomeric respirators, which can save thousands of N95 masks.\n8 : ['approach','encourage','facilitate','elastomeric respirator','elastomeric', 'respiratory','n95', 'mask'],\n\n#9. Best telemedicine practices, barriers and faciitators, and specific actions to remove\/expand them within and across state boundaries.\n9 : ['telemedicine','telemedicine practices','barrier', 'action', 'boundaries','boundary','remove','expand'],\n\n#10. Guidance on the simple things people can do at home to take care of sick people and manage disease.\n10 : ['guidance', 'home','take care', 'sick people', 'diesease'],\n\n#11. Oral medications that might potentially work.\n11 : ['oral', 'mediation', 'oral medication'],\n\n#12. Use of AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in a way that could not be done manually.\n12 : ['ai', 'real time', 'intervention', 'risk factor', 'factor', 'delivery'],\n#13. Best practices and critical challenges and innovative solutions and technologies in hospital flow and organization, workforce protection, workforce allocation, community-based support resources, payment, and supply chain management to enhance capacity, efficiency, and outcomes.\n13 : ['practice','challenge','critical','innovative','solution', 'techonology', 'hospital flow', 'hospital', 'organization','workforce','protection', 'community-based support resource'],\n\n#14. Efforts to define the natural history of disease to inform clinical care, public health interventions, infection prevention control, transmission, and clinical trials\n14 : ['effort','natural history', 'clinical care','clinical', 'public health intervention', 'infection', 'prevention', 'control','transmission','trial'],\n\n#15. Efforts to develop a core clinical outcome set to maximize usability of data across a range of trials\n15 : ['effort', 'clinical','outcome', 'maximize','usability','trial'],\n\n#16. Efforts to determine adjunctive and supportive interventions that can improve the clinical outcomes of infected patients (e.g. steroids, high flow oxygen)\n16 : ['adjunctive', 'supportive', 'intervention', 'clinical','improve', 'outcome', 'infected','patient', 'steroid','oxyen']}","2fad5c5d":"papers_df_el['c_combined'] = papers_df_el['combined'].apply(clean_word_round1)","596e9086":"def count_key_words(row,num): \n    total_count = 0\n    \n    for word in key_words_dic[num]:\n        counts = len(re.findall(word, row))\n        total_count += counts\n    return total_count","58c4d9e1":"for i in range(1, 17):\n    col = 'q_'+str(i)\n    papers_df_el[col] = papers_df_el['c_combined'].apply(lambda x: count_key_words(x,num=i))","0a03b875":"count_df_el=papers_df_el[['q_1','q_2','q_3','q_4',\n          'q_5','q_6','q_7','q_8',\n          'q_9','q_10','q_11','q_12',\n          'q_13','q_14','q_15','q_16']]","125587fe":"\ncount_df_el","7fca2bb0":"papers_df_el['title'].loc[40]","d39b46d9":"papers_df_el.loc[40]","6b87d779":"# Count how many key words from every article's combined abstract and title fall into sub-questions' key word list","7fe46d41":"# Visualize the frequency of words occured in papers' Title & Abstract in relation to the scale of text size. The bigger font size, the higher frequency.","d5a0f856":"### To gain quick data insights, we constructed a Keywords-Article Matrix as an overview approach to get a profile and a preliminary evaluation of the dataset.","56674c96":"# List top 20 words occured in papers' Abstract","3e131494":"# List top words occured in papers' Title & Abstract","7afad132":"# List top 20 words occured in papers' Title ","c78e1d82":"Load DataFrame of Cleaned Documents"}}