{"cell_type":{"03592f29":"code","07310564":"code","e80d61da":"code","b706b835":"code","a67b80a1":"code","5843600a":"code","29dd345a":"code","d5103d20":"code","f5f2da75":"code","7f401aa9":"code","47d4bf20":"code","1e05bf0d":"code","ab82f92b":"code","9e910062":"code","c8d82332":"code","f697fe33":"code","8e472b25":"code","28fdd2e6":"code","cf5d9c28":"markdown","0e6726a3":"markdown","c7dd09e1":"markdown","954b4f6f":"markdown","e3f65284":"markdown","39555a11":"markdown","66ad49be":"markdown","a2fc763d":"markdown","7afd5ba3":"markdown","c6a4b100":"markdown","d5986283":"markdown","e2018f6c":"markdown","4c37280d":"markdown","80c0f3bb":"markdown"},"source":{"03592f29":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","07310564":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import VarianceThreshold, mutual_info_classif, SelectKBest\nfrom sklearn.ensemble import ExtraTreesClassifier","e80d61da":"data = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')","b706b835":"data.head()","a67b80a1":"data.shape","5843600a":"data.dropna(axis=0, inplace=True)\ndata.reset_index(drop=True, inplace=True)","29dd345a":"data.shape","d5103d20":"X, y = data.drop('claim', axis=1) , data['claim']\nX_train, X_test, y_train, y_test =  train_test_split(X,y, test_size=.3 , random_state=4)","f5f2da75":"var_threshold = VarianceThreshold(threshold=0)\nvar_threshold.fit(X_train)","7f401aa9":"constant_column = [column for column in X_train.columns if column not in X_train.columns[var_threshold.get_support()]]\nprint([feature for feature in constant_column])","47d4bf20":"def finding_correlation(data, threshold):\n    correlated_columns = set()\n    correlation_matrix = X.corr()\n    for i in range(correlation_matrix.shape[0]):\n        for j in range(i):\n            if abs(correlation_matrix.iloc[i,j]) > threshold:\n                column_name = correlation_matrix.columns[i]\n                correlated_columns.add(column_name)\n    return correlated_columns","1e05bf0d":"print(finding_correlation(X_train, .7))\nprint(finding_correlation(X_train, .9))","ab82f92b":"mutual_info = mutual_info_classif(X_train, y_train)\n\nmutual_info = pd.Series(mutual_info)\nmutual_info.index = X_train.columns\nmutual_info.sort_values(ascending=False)","9e910062":"select_k_features  = SelectKBest(mutual_info_classif, k=50)\nselect_k_features.fit(X_train, y_train)\nselect_k_features","c8d82332":"X_train.columns[select_k_features.get_support()]","f697fe33":"from sklearn.ensemble import ExtraTreesClassifier\ntree_classifier = ExtraTreesClassifier()\ntree_classifier.fit(X_train, y_train)","8e472b25":"tree_classifier.feature_importances_","28fdd2e6":"feature_importance = pd.Series(tree_classifier.feature_importances_, index=X_train.columns)\nfeature_importance","cf5d9c28":"<a id=\"info\"><\/a>\n**Using  Mutual Information** | [WebLink](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.mutual_info_classif.html)\n\nSelecting feature using Mutual Information for discrete target variable","0e6726a3":"![1-Feature-Selection-Method.png](attachment:c041382e-8e09-40db-a1dc-374ef8591542.png)","c7dd09e1":"You can sort above series by feature_importances_ and select the top n feature that you want to include in your model based on importance.","954b4f6f":"<a id=\"cor\"><\/a>\n**Droping Feature with High Correlation**\n\nUsing Pearson correlation","e3f65284":"### Dropping constant feature","39555a11":"**This kernel just includes different approaches for selecting features, if you find this useful please provide you constructive feedback to improve or to add any other methods.**\n\nIn the next version I'll be adding \n\n1. Forward Selection\n2. Backward Selection\n3. Filter vs Wrapper method\n4. Xgboost and other ensemble technique ","66ad49be":"And as we can see there are no feature which are highly correlated, so it good to go! But if you features in set you can drop them.","a2fc763d":"<a id=\"vr\"><\/a>\n**Variance Threshold** |  [WebLink](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.VarianceThreshold.html)\n\nRemoving features with low variance","7afd5ba3":"* [Variance Threshold](#vr)\n* [Droping Feature with High Correlation](#cor)  \n* [Using Information Gain](#info) \n* [Using Feature importance](#impo)","c6a4b100":"**Feature selection is an important technique in machine learning to get the best features that \nare suitable for business problem and following differnt methods for selecting features.**","d5986283":"<a id=\"impo\"><\/a>\n**Using Feature importance** | [WebLink](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html)\n\nFor calculating feature importance you can use any classifier such as RandomForest, ExtraTree, and Xgboost classifier etc.","e2018f6c":"Here as we can see, the Mutual information from every feature and based on that we can select k features, i.e. top 10 or 20 or any number of features we can select depend of the available data. <br>\nHere I am selecting top **50 features**.","4c37280d":"> There are no feature with contant variance in this data set.","80c0f3bb":"# **Methods for feature selection**"}}