{"cell_type":{"9fb7e5ac":"code","c9634f30":"code","c3096f25":"code","86304f0a":"code","839116af":"code","4ca0b016":"code","fca3bee3":"code","e0f2de4e":"code","047b0865":"code","ed84962f":"code","63ee08ea":"code","5123941a":"code","a43c9384":"code","007c8633":"code","d39516c2":"code","87a4dfea":"code","45ae57d1":"code","477bf0c5":"code","07b0f6c4":"code","c21e142a":"code","884aee38":"code","c6ebe037":"code","8befdcd9":"code","dac105a9":"code","29ff1e54":"code","588437e0":"code","80a5a070":"code","fd7d5335":"code","d7979343":"code","afa66d3c":"code","ea60c924":"code","c7b0edad":"code","8fbe16cd":"code","47bf6f91":"code","ff9ba502":"code","e3926984":"code","f2bb720c":"code","0ce46392":"code","e5f6a652":"code","86dd0775":"code","1f9eee85":"code","22a56b1f":"code","21622e52":"code","20d61213":"code","9a105ae1":"code","5135c755":"code","ee18d8d8":"code","87932d4a":"code","442991e0":"code","38661e55":"code","e5fdfc58":"code","8492fac3":"code","210238a5":"code","e6233378":"code","f3f3a464":"code","3f789d64":"code","b5919dbb":"code","f5a92a88":"code","d2fa337a":"code","720a0f1b":"code","95842d0b":"code","be96da65":"markdown","85fb0685":"markdown","9f621eb3":"markdown","b9920c90":"markdown","643a13d9":"markdown","9977644a":"markdown","b3517f62":"markdown","4880ae78":"markdown","110ce843":"markdown","6a2f7bae":"markdown","42e7b9f4":"markdown","aaeaac4a":"markdown","ec37c3f0":"markdown","456fa757":"markdown","f489995a":"markdown","80ed62d8":"markdown","57c10380":"markdown","81062902":"markdown","1a16672c":"markdown","c75bfbd2":"markdown","f7992d36":"markdown","d95e5523":"markdown","5069aa34":"markdown","13c876c4":"markdown","a13f36d2":"markdown","ce12c722":"markdown","ad4a393a":"markdown","f052a045":"markdown","abf55206":"markdown","7b247c32":"markdown","323db044":"markdown","7168524d":"markdown","934e0751":"markdown","70d7841e":"markdown","d8e0d3ab":"markdown","57f66cda":"markdown","f837e186":"markdown","ef1be557":"markdown","55b6562e":"markdown","db547a52":"markdown","9f74f071":"markdown","e54e2303":"markdown","9bd3c7a1":"markdown","dcf8d5ee":"markdown","7c2ccfb7":"markdown","9e8be7ce":"markdown","d6d9619b":"markdown","91b28880":"markdown","f4da634e":"markdown","8ab779d0":"markdown","0ac58e20":"markdown","18cbdca8":"markdown","0ba01fb0":"markdown","84b51cf5":"markdown","11af41cc":"markdown","9485270d":"markdown","4e7c9a72":"markdown","fc329eb2":"markdown","dbd71271":"markdown","5ad0360a":"markdown","2f7542eb":"markdown","53bce77c":"markdown","be245388":"markdown","ed6a0e36":"markdown","4427af7a":"markdown","6e74a009":"markdown","826f1939":"markdown","58e58cfd":"markdown","bf196a54":"markdown","86551a1c":"markdown","4ea84fff":"markdown","80aacbc0":"markdown","9bc15606":"markdown","8d1dcfa0":"markdown","92229d40":"markdown","c8b55b15":"markdown","eac58bed":"markdown","87b04451":"markdown","d35ca74e":"markdown","e8876964":"markdown","f19b63b3":"markdown"},"source":{"9fb7e5ac":"target = 'area'","c9634f30":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')\n\nimport statsmodels.api as sm\nfrom statsmodels.compat import lzip\nimport statsmodels.stats.api as sms\nfrom statsmodels.formula.api import ols\nfrom scipy.stats import zscore\nfrom statsmodels.stats.stattools import durbin_watson\nfrom sklearn.model_selection import train_test_split,KFold\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_selection import RFECV\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nfrom sklearn.linear_model import LinearRegression,RidgeCV,LassoCV,ElasticNetCV","c3096f25":"# path = 'forestfires.csv'\npath = \"..\/input\/forest-fires-data-set\/forestfires.csv\"\ndf = pd.read_csv(path)\n\ndf.shape","86304f0a":"df.dtypes","839116af":"df.describe().T","4ca0b016":"df.isna().sum().sum()","fca3bee3":"plt.rcParams[\"figure.figsize\"] = 9,5","e0f2de4e":"plt.figure(figsize=(16,5))\nprint(\"Skew: {}\".format(df[target].skew()))\nprint(\"Kurtosis: {}\".format(df[target].kurtosis()))\nax = sns.kdeplot(df[target],shade=True,color='g')\nplt.xticks([i for i in range(0,1200,50)])\nplt.show()","047b0865":"ax = sns.boxplot(df[target])","ed84962f":"# Outlier points\ny_outliers = df[abs(zscore(df[target])) >= 3 ]\ny_outliers","63ee08ea":"dfa = df.drop(columns=target)\ncat_columns = dfa.select_dtypes(include='object').columns.tolist()\nnum_columns = dfa.select_dtypes(exclude='object').columns.tolist()\n\ncat_columns,num_columns","5123941a":"# analyzing categorical columns\nplt.figure(figsize=(16,10))\nfor i,col in enumerate(cat_columns,1):\n    plt.subplot(2,2,i)\n    sns.countplot(data=dfa,y=col)\n    plt.subplot(2,2,i+2)\n    df[col].value_counts(normalize=True).plot.bar()\n    plt.ylabel(col)\n    plt.xlabel('% distribution per category')\nplt.tight_layout()\nplt.show()    ","a43c9384":"plt.figure(figsize=(18,40))\nfor i,col in enumerate(num_columns,1):\n    plt.subplot(8,4,i)\n    sns.kdeplot(df[col],color='g',shade=True)\n    plt.subplot(8,4,i+10)\n    df[col].plot.box()\nplt.tight_layout() \nplt.show()\nnum_data = df[num_columns]\npd.DataFrame(data=[num_data.skew(),num_data.kurtosis()],index=['skewness','kurtosis'])","007c8633":"print(df['area'].describe(),'\\n')\nprint(y_outliers)","d39516c2":"# a categorical variable based on forest fire area damage\n# No damage, low, moderate, high, very high\ndef area_cat(area):\n    if area == 0.0:\n        return \"No damage\"\n    elif area <= 1:\n        return \"low\"\n    elif area <= 25:\n        return \"moderate\"\n    elif area <= 100:\n        return \"high\"\n    else:\n        return \"very high\"\n\ndf['damage_category'] = df['area'].apply(area_cat)\ndf.head()","87a4dfea":"cat_columns","45ae57d1":"for col in cat_columns:\n    cross = pd.crosstab(index=df['damage_category'],columns=df[col],normalize='index')\n    cross.plot.barh(stacked=True,rot=40,cmap='hot')\n    plt.xlabel('% distribution per category')\n    plt.xticks(np.arange(0,1.1,0.1))\n    plt.title(\"Forestfire damage each {}\".format(col))\nplt.show()","477bf0c5":"plt.figure(figsize=(20,40))\nfor i,col in enumerate(num_columns,1):\n    plt.subplot(10,1,i)\n    if col in ['X','Y']:\n        sns.swarmplot(data=df,x=col,y=target,hue='damage_category')\n    else:\n        sns.scatterplot(data=df,x=col,y=target,hue='damage_category')\nplt.show()","07b0f6c4":"selected_features = df.drop(columns=['damage_category','day','month']).columns\nselected_features","c21e142a":"sns.pairplot(df,hue='damage_category',vars=selected_features)\nplt.show()","884aee38":"out_columns = ['area','FFMC','ISI','rain']","c6ebe037":"df = pd.get_dummies(df,columns=['day','month'],drop_first=True)","8befdcd9":"print(df[out_columns].describe())\nnp.log1p(df[out_columns]).skew(), np.log1p(df[out_columns]).kurtosis()","dac105a9":"# FFMC and rain are still having high skew and kurtosis values, \n# since we will be using Linear regression model we cannot operate with such high values\n# so for FFMC we can remove the outliers in them using z-score method\nmask = df.loc[:,['FFMC']].apply(zscore).abs() < 3\n\n# Since most of the values in rain are 0.0, we can convert it as a categorical column\ndf['rain'] = df['rain'].apply(lambda x: int(x > 0.0))\n\ndf = df[mask.values]\ndf.shape","29ff1e54":"out_columns.remove('rain')\ndf[out_columns] = np.log1p(df[out_columns])","588437e0":"df[out_columns].skew()","80a5a070":"# we will use this dataframe for building our ML model\ndf_ml = df.drop(columns=['damage_category']).copy()","fd7d5335":"X = df.drop(columns=['area','damage_category'])\ny = df['area']","d7979343":"X_constant = sm.add_constant(X)\n\n# Build OLS model\nlin_reg = sm.OLS(y,X_constant).fit()\nlin_reg.summary()","afa66d3c":"import scipy.stats as stats\nimport pylab\n\n# get an instance of Influence with influence and outlier measures \nst_resid = lin_reg.get_influence().resid_studentized_internal\nstats.probplot(st_resid,dist=\"norm\",plot=pylab)\nplt.show()","ea60c924":"# return fstat and p-value\nsm.stats.diagnostic.linear_rainbow(lin_reg)","c7b0edad":"# The mean expected value around 0, it implies linearity is preserved\nlin_reg.resid.mean()","8fbe16cd":"def linearity_test(model, y):\n    '''\n    Function for visually inspecting the assumption of linearity in a linear regression model.\n    It plots observed vs. predicted values and residuals vs. predicted values.\n    \n    Args:\n    * model - fitted OLS model from statsmodels\n    * y - observed values\n    '''\n    fitted_vals = model.predict()\n    resids = model.resid\n\n    fig, ax = plt.subplots(1,2,figsize=(15,5))\n\n    sns.regplot(x=fitted_vals, y=y, lowess=True, ax=ax[0], line_kws={'color': 'red'})\n    ax[0].set_title('Observed vs. Predicted Values', fontsize=16)\n    ax[0].set(xlabel='Predicted', ylabel='Observed')\n\n    sns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[1], line_kws={'color': 'red'})\n    ax[1].set_title('Residuals vs. Predicted Values', fontsize=16)\n    ax[1].set(xlabel='Predicted', ylabel='Residuals')\n    \nlinearity_test(lin_reg, y) \nplt.tight_layout()","47bf6f91":"sns.distplot(lin_reg.resid,fit=stats.norm)\nplt.text(4,0.5,f\"Skewness: {round(lin_reg.resid.skew(),2)}\",fontsize=15)\nplt.show()","ff9ba502":"sm.qqplot(lin_reg.resid,line ='r')\njb = [round(n,2) for n in stats.jarque_bera(lin_reg.resid)]\nplt.text(-2,4,f\"Jarque bera: {jb}\",fontsize=15)\nplt.show()","e3926984":"sms.het_goldfeldquandt(lin_reg.resid, lin_reg.model.exog)","f2bb720c":"model = lin_reg\nfitted_vals = model.predict()\nresids = model.resid\nresids_standardized = model.get_influence().resid_studentized_internal\n\nfig, ax = plt.subplots(1,2)\n\nsns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[0], line_kws={'color': 'red'})\nax[0].set_title('Predicted vs Residuals', fontsize=16)\nax[0].set(xlabel='Predicted Values', ylabel='Residuals')\n\nsns.regplot(x=fitted_vals, y=np.sqrt(np.abs(resids_standardized)), lowess=True, ax=ax[1], line_kws={'color': 'red'})\nax[1].set_title('Scale-Location', fontsize=16)\nax[1].set(xlabel='Predicted Values', ylabel='sqrt(abs(Residuals))')\n\nname = ['F statistic', 'p-value']\ntest = sms.het_goldfeldquandt(model.resid, model.model.exog)\nlzip(name, test)\nplt.tight_layout()","0ce46392":"import statsmodels.tsa.api as smt\n# Confidence intervals are drawn as a cone. \n# By default, this is set to a 95% confidence interval, \n# suggesting that correlation values outside of this code are very likely a correlation \n# and not a statistical fluke\nacf = smt.graphics.plot_acf(lin_reg.resid, lags=50 , alpha=0.05)\nacf.show()","e5f6a652":"plt.figure(figsize =(16,10))\n\nsns.heatmap(df.corr(),annot=True,cmap='YlGnBu',fmt=\".2f\",cbar=False)\nplt.show()","86dd0775":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = [variance_inflation_factor(X_constant.values, i) for i in range(X_constant.shape[1])]\npd.DataFrame({'vif': vif[1:]}, index=X.columns).sort_values(by=\"vif\",ascending=False)","1f9eee85":"lr = LinearRegression()\nlr.fit(X, y)\n\nprint(f'Intercept: {lr.intercept_}')\nprint(f'R^2 score: {lr.score(X, y)}')\npd.DataFrame({\"Coefficients\": lr.coef_}, index=X.columns)","22a56b1f":"X = df.drop(columns=['area','damage_category'])\ny = df['area']","21622e52":"def check_stats(X,y):\n    vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    print(pd.DataFrame({'vif': vif}, index=X.columns).sort_values(by=\"vif\",ascending=False)[:10])\n    lin_reg = sm.OLS(y,X).fit()\n    print(lin_reg.summary())\ncheck_stats(X,y)","20d61213":"X.drop(columns=['FFMC'],inplace=True)\n# check_stats(X,y)","9a105ae1":"X.drop(columns=['Y'],inplace=True)\n# check_stats(X,y)","5135c755":"X.drop(columns=['month_jul'],inplace=True)\n# check_stats(X,y)","ee18d8d8":"X.drop(columns=['day_thu'],inplace=True)\n# check_stats(X,y)","87932d4a":"X.drop(columns=['day_mon'],inplace=True)\n# check_stats(X,y)","442991e0":"X.drop(columns=['month_aug'],inplace=True)\ncheck_stats(X,y)","38661e55":"X_m, y_m = df_ml.drop(columns=[target]), df_ml[target]","e5fdfc58":"# RFECV is a variant with inbuilt Cross validation\nmodel = LinearRegression()\nselector = RFECV(model,cv=5)\nselector = selector.fit(X_m, y_m)\nprint(f\"Out of {len(X_m.columns)} features, best number of features {selector.n_features_}\")\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score\")\nplt.plot(range(1, len(X_m.columns) + 1), selector.grid_scores_)\nplt.show()","8492fac3":"# In our stats method we found that the intercept was not relevant \n# Let's try that feature out in our ML model\nmodel = LinearRegression(fit_intercept=False)\nselector = RFECV(model,cv=5)\nselector = selector.fit(X_m, y_m)\nprint(f\"Out of {len(X_m.columns)} features, best number of features {selector.n_features_}\")\n\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score\")\nplt.plot(range(1, len(X_m.columns) + 1), selector.grid_scores_)\nprint(X_m.columns[selector.support_].values)\nplt.show()","210238a5":"mask = selector.support_\nprint(f\"Best features according to RFE {X_m.columns[mask].values}\")\n\nX_m1 = X_m.iloc[:,mask]\n# We could have used train test split or cross validation strategies\n# for scoring the model but in order to compare with the stats model \n# we will use the whole data\nmodel1 = LinearRegression().fit(X_m1,y_m)\nprint(f\"R2 Score: {model1.score(X_m1,y_m)}\")","e6233378":"model = LinearRegression(fit_intercept=False)\nsfs1 = sfs(model,k_features=20,forward=True,scoring='r2',cv=5)\nsfs1.fit(X_m,y_m)\nfig = plot_sfs(sfs1.get_metric_dict())\nplt.title('Forward Selection')\nplt.grid()\nplt.show()","f3f3a464":"print(sfs1.k_features, sfs1.k_feature_names_,sep=\"\\n\")","3f789d64":"index = list(sfs1.k_feature_idx_)\nX_m1 = X_m.iloc[:,index]\nmodel1 = LinearRegression().fit(X_m1,y_m)\nprint(f\"R2 Score: {model1.score(X_m1,y_m)}\")","b5919dbb":"model = LinearRegression(fit_intercept=False)\nsfs1 = sfs(model,k_features=6,forward=False,scoring='r2',cv=5)\nsfs1.fit(X_m,y_m)\nfig = plot_sfs(sfs1.get_metric_dict())\nplt.title('Backward Selection')\nplt.grid(True)\nplt.show()","f5a92a88":"index = list(sfs1.k_feature_idx_)\nprint(f\"Best features according to RFE: {X_m.columns[index]}\")\n\nX_m1 = X_m.iloc[:,index]\nmodel1 = LinearRegression().fit(X_m1,y_m)\nprint(f\"R2 Score: {model1.score(X_m1,y_m)}\")","d2fa337a":"# higher the alpha value, more restriction on the coefficients; \n# lower the alpha > more generalization, coefficients are barely\nrr = RidgeCV(cv=5,fit_intercept=False) \nrr.fit(X_m, y_m)\nrr.score(X_m,y_m)","720a0f1b":"rr.alpha_","95842d0b":"plt.plot(rr.coef_,alpha=0.7,marker='*',markersize=10,color='red',label=r'Ridge; $\\alpha =10$') \nplt.grid(True)\nplt.xticks(range(0,28,1))\nplt.legend()\nplt.show()","be96da65":"### Independent columns ","85fb0685":"## 4. No Autocorrelation","9f621eb3":"\n<img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcQ78MxgdAcvdPa-45oY67MVwA0P3AVrVCtJO_6nYytQv9jJXjxF\" \/>\n\nHomoscedacity: If the residuals are symmetrically distributed across the trend , then it is called as homoscedacious. \n\nHeteroscedacity: If the residuals are not symmetric across the trend, then it is called as heteroscedacious.\n\n\n**Goldfeld-Quandt test for Homoscedasticity**\n\nH0 = constant variance among residuals (Homoscedacity)\n\nHa = Heteroscedacity.","b9920c90":"---","643a13d9":"**RMSE** \n\nRMSE is the most popular evaluation metric used in regression problems. It follows an assumption that error are unbiased and follow a normal distribution.\n\nFurther read: [Analyticsvidhya](https:\/\/www.analyticsvidhya.com\/blog\/2019\/08\/11-important-model-evaluation-error-metrics\/)","9977644a":"---","b3517f62":"## Feature Selection techniques\n\nThe following can be used for selecting relevant features for model building\n\n1. **Using Pearson Correlation**\n2. **Wrapper method** \n    1. Forward Selection: Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.\n    1. Backward Elimination: In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.\n    1. Recursive Feature elimination: It is a greedy optimization algorithm which aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination.\n3. **Embedded method**, lasso is one such method which penalizes features based on feature importance, making lesss important feature to 0.","4880ae78":"**Test for normality: Jarque Bera**\n\nFor a good model, the residuals should be normally distributed.\nThe higher the value of Jarque Bera test, the lesser the residuals are normally distributed.\n\nThe Jarque\u2013Bera test is a goodness-of-fit test of whether sample data \nhave the skewness and kurtosis matching a normal distribution.\n\n> Jarque-Bera (JB):\t107.018\n\nThe jarque bera test tests whether the sample data has the skewness and kurtosis matching a normal distribution.\n\nNote that this test generally works good for large enough number of data samples(>2000) as the test statistics asymptotically has a chi squared distribution with degrees 2 of freedom.\n\n> Our dataframe length, 517\n\n**Null hypothesis (H0)** - Residuals are normally distributed","110ce843":"### Numerical Columns","6a2f7bae":"# Objective \n\nForest fires help in the natural cycle of woods' growth and replenishment. They Clear dead trees, leaves, and competing vegetation from the forest floor, so new plants can grow. Remove weak or disease-ridden trees, leaving more space and nutrients for stronger trees.\n\n\nBut when fires burn too hot and uncontrollable or when they\u2019re in the \u201cwildland-urban interface\u201d (the places where woodlands and homes or other developed areas meet), they can be damaging and life threatning.\n\n\nIn this kernel, our aim is to predict the burned area (`area`) of forest fires, in the northeast region of Portugal. Based on the the spatial, temporal, and weather variables where the fire is spotted. \n\nThis prediction can be used for calculating the forces sent to the incident and deciding the urgency of the situation.\n\nFurther read:\n1. [Mylandplan](https:\/\/mylandplan.org\/content\/good-and-bad-forest-fires)\n2. [KNIME](https:\/\/www.knime.com\/knime-applications\/forest-fire-prediction)","42e7b9f4":"Outliers, Skewness and kurtosis (high positive or negative) was observed in the following columns:\n1. FFMC\n2. ISI\n3. rain","aaeaac4a":"**Few observations:**\n\n- The data is highly skewed with a value of +12.84 and huge kurtosis value of 194.\n\n- It even tells you that majority of the forest fires do not cover a large area, most of the damaged area is under 50 hectares of land.\n\n- We can apply tranformation to fix the skewnesss and kurtosis, however we will have to inverse transform before submitting the output.\n\n- Outlier Check: There are 4 outlier instances in our area columns but the questions is should we drop it or not? (Will get back to this in the outlier treatment step)","ec37c3f0":"---","456fa757":"---","f489995a":"---","80ed62d8":"### Numerical columns","57c10380":"The desired outcome of plots is that points are symmetrically distributed around a diagonal line in the former plot or around horizontal line in the latter one. \n\n> - By observing  the plots the linearity assumption is not there \n- Adding new features might result in linearity of model \n- Also, transforming the feature from non-linear to linear using various data transformation techniques can help.","81062902":"# Missing value treatment","1a16672c":"---","c75bfbd2":"# Exploratory Data Analysis\n   We will try out the following analysis on our dataset\n   - Univariate \n   - Bivariate \n   - Multivariate","f7992d36":"## 2. Normality of the residuals","d95e5523":"---","5069aa34":"## 1. Linearity of residuals\n","13c876c4":"### Categorical columns ","a13f36d2":"## Bivariate analysis with our target variable","ce12c722":"---","ad4a393a":"### Backward Selection","f052a045":"---","abf55206":"---","7b247c32":"## Univariate analysis\n\n","323db044":"However, the above outliers are not error values so we cannot remove it. \n\nIn order to minimize the effect of outliers in our model we will transform the above features. \n\n**Ref:** https:\/\/humansofdata.atlan.com\/2018\/03\/when-delete-outliers-dataset\/","7168524d":"---","934e0751":"**Expectation Mean of residual is zero**","70d7841e":"---","d8e0d3ab":"### Ridge","57f66cda":"# Load and describe data","f837e186":"# Statistical approach\n\n## Checking assumptions for linear regression in statistics\n\n1. Linearity of model\n    \n2. Normality of residuals\n\n3. Homoscedasticity\n\n4. No Autocorrelation\n\n5. Multicollinearity ","ef1be557":"- Previously we had observed that `August` and `September` had the most number of forest fires. And from the above plot of `month`, we can understand few things\n    - Most of the fires in August were low (< 1 hectare).\n    - The very high damages(>100 hectares) happened in only 3 months - august,july and september.\n \n- Regarding fire damage per day, nothing much can be observed. Except that, there were no ` very high` damaging fires on Friday and on Saturdays it has been reported most.","55b6562e":"**Building model with the best features and checking the R2 score for the same**","db547a52":"---","9f74f071":"---","e54e2303":"---","9bd3c7a1":"- Data transformations like `log,root,inverse,exponential`,etc","dcf8d5ee":"**Rainbow test**","7c2ccfb7":"<img src='https:\/\/upload.wikimedia.org\/wikipedia\/commons\/d\/d8\/Deerfire_high_res_edit.jpg' width='1200px'\/>","9e8be7ce":"Multicollineariy arises when one independent variable can be linearly predicted by others with a substantial level of accuracy.\n\n<img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcR7jSx3cImz0dmtPHejjtlJAU8MwhK0mjbZxc7Wvu_aE4PkCMYO\"\/>","d6d9619b":"# Define the metrics","91b28880":"> There is multicollinearity present between some features where vif >5.\n- We can even use PCA to reduce features to a smaller set of uncorrelated components.\n- To deal with multicollinearity we should iteratively remove features with high values of VIF.\n","f4da634e":"### Let's begin with the target variable, `Area`","8ab779d0":"---","0ac58e20":"1. It is interesting to see that abnormally high number of the forest fires occur in the month of `August`\nand `September`.\n\n2. In the case of day, the days `Friday` to `Monday` have higher proportion of cases. (However, no strong indicators)","18cbdca8":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-1\">Objective<\/a><\/span><\/li><li><span><a href=\"#Define-the-metrics\" data-toc-modified-id=\"Define-the-metrics-2\">Define the metrics<\/a><\/span><\/li><li><span><a href=\"#Dependencies\" data-toc-modified-id=\"Dependencies-3\">Dependencies<\/a><\/span><\/li><li><span><a href=\"#Load-and-describe-data\" data-toc-modified-id=\"Load-and-describe-data-4\">Load and describe data<\/a><\/span><\/li><li><span><a href=\"#Missing-value-treatment\" data-toc-modified-id=\"Missing-value-treatment-5\">Missing value treatment<\/a><\/span><\/li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-6\">Exploratory Data Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Univariate-analysis\" data-toc-modified-id=\"Univariate-analysis-6.1\">Univariate analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Let's-begin-with-the-target-variable,-Area\" data-toc-modified-id=\"Let's-begin-with-the-target-variable,-Area-6.1.1\">Let's begin with the target variable, <code>Area<\/code><\/a><\/span><\/li><li><span><a href=\"#Independent-columns\" data-toc-modified-id=\"Independent-columns-6.1.2\">Independent columns<\/a><\/span><\/li><li><span><a href=\"#Categorical-columns\" data-toc-modified-id=\"Categorical-columns-6.1.3\">Categorical columns<\/a><\/span><\/li><li><span><a href=\"#Numerical-Columns\" data-toc-modified-id=\"Numerical-Columns-6.1.4\">Numerical Columns<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Bivariate-analysis-with-our-target-variable\" data-toc-modified-id=\"Bivariate-analysis-with-our-target-variable-6.2\">Bivariate analysis with our target variable<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Categorical-columns\" data-toc-modified-id=\"Categorical-columns-6.2.1\">Categorical columns<\/a><\/span><\/li><li><span><a href=\"#Numerical-columns\" data-toc-modified-id=\"Numerical-columns-6.2.2\">Numerical columns<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Multivariate-analysis\" data-toc-modified-id=\"Multivariate-analysis-6.3\">Multivariate analysis<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Outlier-treatment\" data-toc-modified-id=\"Outlier-treatment-7\">Outlier treatment<\/a><\/span><\/li><li><span><a href=\"#Preparing-the-data-for-modelling\" data-toc-modified-id=\"Preparing-the-data-for-modelling-8\">Preparing the data for modelling<\/a><\/span><\/li><li><span><a href=\"#Linear-Regression\" data-toc-modified-id=\"Linear-Regression-9\">Linear Regression<\/a><\/span><\/li><li><span><a href=\"#Statistical-approach\" data-toc-modified-id=\"Statistical-approach-10\">Statistical approach<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Checking-assumptions-for-linear-regression-in-statistics\" data-toc-modified-id=\"Checking-assumptions-for-linear-regression-in-statistics-10.1\">Checking assumptions for linear regression in statistics<\/a><\/span><\/li><li><span><a href=\"#1.-Linearity-of-residuals\" data-toc-modified-id=\"1.-Linearity-of-residuals-10.2\">1. Linearity of residuals<\/a><\/span><\/li><li><span><a href=\"#2.-Normality-of-the-residuals\" data-toc-modified-id=\"2.-Normality-of-the-residuals-10.3\">2. Normality of the residuals<\/a><\/span><\/li><li><span><a href=\"#3.-Homoscedasticity\" data-toc-modified-id=\"3.-Homoscedasticity-10.4\">3. Homoscedasticity<\/a><\/span><\/li><li><span><a href=\"#4.-No-Autocorrelation\" data-toc-modified-id=\"4.-No-Autocorrelation-10.5\">4. No Autocorrelation<\/a><\/span><\/li><li><span><a href=\"#5.-Multicollinearity\" data-toc-modified-id=\"5.-Multicollinearity-10.6\">5. Multicollinearity<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Machine-learning-approach\" data-toc-modified-id=\"Machine-learning-approach-11\">Machine learning approach<\/a><\/span><\/li><li><span><a href=\"#Improving-Stats-model\" data-toc-modified-id=\"Improving-Stats-model-12\">Improving Stats model<\/a><\/span><\/li><li><span><a href=\"#Improving-ML-model\" data-toc-modified-id=\"Improving-ML-model-13\">Improving ML model<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Selection-techniques\" data-toc-modified-id=\"Feature-Selection-techniques-13.1\">Feature Selection techniques<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#RFE\" data-toc-modified-id=\"RFE-13.1.1\">RFE<\/a><\/span><\/li><li><span><a href=\"#Forward-Selection\" data-toc-modified-id=\"Forward-Selection-13.1.2\">Forward Selection<\/a><\/span><\/li><li><span><a href=\"#Backward-Selection\" data-toc-modified-id=\"Backward-Selection-13.1.3\">Backward Selection<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Regularization\" data-toc-modified-id=\"Regularization-13.2\">Regularization<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Ridge\" data-toc-modified-id=\"Ridge-13.2.1\">Ridge<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><\/ul><\/div>","0ba01fb0":"### Categorical columns","84b51cf5":"## Regularization\n1. Lasso\n2. Ridge\n3. ElasticNet\n","11af41cc":"# Dependencies","9485270d":"**Dropping columns to improve accuracy:**\n    \nBy checking high Variance inflation factor and p-value we will decide whether to keep the column or drop it.\n\n> R^2 = 1 - SSE(Sum of Square of Residuals)\/SST (Sum of square Total)\n\nJust by dropping constant we got a huge bump in adjusted R2 from `2.5%` to `40.6%`.","4e7c9a72":"> - By observing the above data we can say that there is positive autocorrelation is present , we can reduce it by using fine tuning our parameters\n- We can even use Generalize Least Squares (GLS) model","fc329eb2":"We had observed outliers in the following columns:\n1. area \n2. FFMC\n2. ISI\n3. rain","dbd71271":"# Improving ML model","5ad0360a":"## Multivariate analysis","2f7542eb":"# Preparing the data for modelling\nThing which we can cover here\n- Encoding the categorical columns ","53bce77c":"**In this kernel we will be working on Linear regression using both Statistical and Machine learning approach.**\n\n<img src=\"\/\/i.imgflip.com\/3hzd4f.jpg\"\/>\n\n\n**Difference between statistical and machine learning approach**\n\n- Machine learning produces **predictions**.  As far as I can tell, it is not very good at drawing conclusions about general principles based on a set of observations.\n- Statistical estimation lets the practitioner make **inferences** (conclusions about a larger set of phenomena based on the observation of a smaller set of phenomena.)  For example, in a regression model the practitioner can estimate the effect of a one unit change in an independent variable X on a dependent variable y.\n\nFurther read: [Quora](https:\/\/www.quora.com\/When-do-you-use-machine-learning-vs-statistical-regression)","be245388":"# Improving Stats model","ed6a0e36":"# Linear Regression","4427af7a":"# Outlier treatment","6e74a009":"### Forward Selection","826f1939":"Similarly, you can continue to optimize the model.\n\nOur Prob (F-statistic) has improved from 0.0558 to 2.20e-48. As the value is less than 0.05, the model becomes more significant.","58e58cfd":"## 5. Multicollinearity ","bf196a54":"---","86551a1c":"> The p-value is 0 which simply means we can reject out NULL hypothesis.\nWe can fix that by\n- Removing the outliers in the data\n- Fixing the Non-linearity in our dependent or target feature\n- Removing the bias, the bias might be contributing to the non-normality. ","4ea84fff":"---","80aacbc0":"---","9bc15606":"**Linearity can be measured by two methods:**\n\n- Plot the observed values Vs predicted values` and plot the `Residual Vs predicted values` and see the linearity of residuals. \n-  Rainbow test","8d1dcfa0":"---","92229d40":"### RFE","c8b55b15":"---","eac58bed":"# Machine learning approach","87b04451":"- **Null hypothesis (H0):** The Null hypothesis is that the regression is correctly modeled as linear.\n- **Alternate hypothesis(H1)**: The model is non-linear","d35ca74e":">- To identify homoscedasticity in the plots, the placement of the points should be equally distributed, random, no pattern (increase\/decrease in values of residuals) should be visible and a flat red line.\n- In the plots we can see there are no paticular patterns and P-Values is also greater than 0.05 ,so we can say that there is homoscedasticity.\n- Outliers can make it Heteroscedacious, Transforming (log or Box cox, if > 0) the dependent or independent variables can help fix it.\n\n[Reference](https:\/\/datascienceplus.com\/how-to-detect-heteroscedasticity-and-rectify-it\/)","e8876964":"Autocorrelation measures the relationship between a variable's current value and its past values.\n\n**Test for autocorrelation : Durbin- Watson Test**\n\nIt's test statistic value ranges from 0-4. If the value is between \n- 0-2, it's known as Positive Autocorrelation.\n- 2-4, it is known as Negative autocorrelation.\n- exactly 2, it means No Autocorrelation.\n\nFor a good linear model, it should have low or no autocorrelation.\n\n```python\nfrom statsmodels.stats.stattools import durbin_watson\ndurbin_watson(lin_reg.resid)\n```\n\n> In our case, Durbin-Watson: 0.979","f19b63b3":"## 3. Homoscedasticity\n"}}