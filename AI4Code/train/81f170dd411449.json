{"cell_type":{"74ce0f44":"code","73fd48e5":"code","b619d6d0":"code","41c9cb1a":"code","65883059":"code","09d473f5":"code","300c1381":"code","a4e202e7":"code","435d2270":"code","76a1dfe8":"code","72b9e0c2":"code","8b8ea041":"code","0f7e06c6":"code","6e6de13d":"code","07cc4862":"code","5d5cf773":"code","d364f97e":"code","f0f96620":"code","c7c27ebf":"code","35ba50f1":"code","334a2183":"code","7952b16d":"code","9bcce04f":"code","42849965":"code","88dcd053":"code","d691ce1e":"code","30f072e6":"code","02e74eb0":"code","3bc663f3":"code","63cb430b":"code","614c8591":"code","4b9f21b2":"code","391795c4":"code","89d68784":"code","76ca1c34":"code","a1b28d54":"code","b00da727":"code","be89f567":"code","1006052b":"code","90bf73fe":"code","52d58138":"code","370fe1f6":"code","52f76189":"code","7e823d72":"code","f0143a6e":"code","2561a980":"code","60f62def":"code","24cb5ece":"code","63b29b04":"code","69711b0c":"code","38c815fa":"code","72d694fd":"code","60cea9a5":"code","b92bff10":"code","b694297a":"code","94364ab3":"code","12e5df4c":"code","417973f0":"code","0de12cda":"code","81b2b3b4":"code","e11b4124":"code","cd84e09b":"code","ee514720":"code","c03ad405":"code","98791092":"code","cba6b3b5":"code","bf5fddd6":"code","290fa1e7":"code","5dfce0ef":"code","794e6bc4":"code","473e87a2":"code","8625b5e8":"code","abf6a75b":"markdown","d1e1e6e5":"markdown","aea54939":"markdown","47732be4":"markdown","8f7504cb":"markdown","f5d66ea3":"markdown","772fb7b4":"markdown","7339dfb9":"markdown","5f6c95f0":"markdown","64011fda":"markdown","b29ba12c":"markdown","8a48ffc8":"markdown","d2c5ddd7":"markdown","09c08ced":"markdown","5553fc51":"markdown","41df5b02":"markdown","30e90bbe":"markdown","91ca08a6":"markdown","b22f2a78":"markdown","75524cba":"markdown","70a37b18":"markdown","036c8807":"markdown","6d0708b1":"markdown","41d54dfb":"markdown","5caf8369":"markdown","4fe239fb":"markdown","4574cab6":"markdown","1cfe3e0b":"markdown","525fe9bb":"markdown","23794168":"markdown"},"source":{"74ce0f44":"!pip install scorecardpy","73fd48e5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scorecardpy as sc\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns","b619d6d0":"df = pd.read_csv(\"..\/input\/default-of-credit-card-clients-dataset\/UCI_Credit_Card.csv\")\ndf = df.drop('ID', axis = 1)\ndf.head()","41c9cb1a":"df.info()","65883059":"df[['SEX', 'EDUCATION', 'MARRIAGE']].describe()","09d473f5":"plt.subplots_adjust(left = 0, bottom = 0, right = 1.5, top = 1.5, wspace = 0.3, hspace = 0.3)\nfont = {'family' : 'Times New Roman', 'weight' : 'normal', 'size' : 12}\n\nplt.subplot(2, 2, 1)\ndf.SEX.value_counts().plot(kind = 'bar')\nplt.xlabel(\"SEX\", font)\nplt.ylabel(\"Number\", font)\n\nplt.subplot(2, 2, 2)\ndf.MARRIAGE.value_counts().plot(kind = 'bar')\nplt.xlabel(\"MARRIAGE\", font)\nplt.ylabel(\"Number\", font)\n\nplt.subplot(2, 2, 3)\ndf.EDUCATION.value_counts().plot(kind = 'bar')\nplt.xlabel(\"EDUCATION\", font)\nplt.ylabel(\"Number\", font)","300c1381":"df[['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']].describe()","a4e202e7":"def draw_histograms(df, variables, nrows, ncols, nbins):\n    fig = plt.figure()\n    for i, varname in enumerate(variables):\n        ax = fig.add_subplot(nrows, ncols, i + 1)\n        df[varname].hist(bins = nbins, ax = ax)\n        ax.set_title(varname)\n    fig.tight_layout()\n    plt.show()\n\npay = df[['PAY_0','PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']]\ndraw_histograms(pay, pay.columns, 2, 3, 10)","435d2270":"df[['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']].describe()","76a1dfe8":"def draw_histograms(df, variables, nrows, ncols, nbins):\n    fig = plt.figure()\n    for i, varname in enumerate(variables):\n        ax = fig.add_subplot(nrows, ncols, i + 1)\n        df[varname].hist(bins = nbins, ax = ax)\n        ax.set_title(varname)\n    fig.tight_layout()\n    plt.show()\n\nbills = df[['BILL_AMT1','BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']]\ndraw_histograms(bills, bills.columns, 2, 3, 10)","72b9e0c2":"df[['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']].describe()","8b8ea041":"def draw_histograms(df, variables, nrows, ncols, nbins):\n    fig = plt.figure()\n    for i, varname in enumerate(variables):\n        ax = fig.add_subplot(nrows, ncols, i + 1)\n        df[varname].hist(bins = nbins, ax = ax)\n        ax.set_title(varname)\n    fig.tight_layout()\n    plt.show()\n\npay_amt = df[['PAY_AMT1','PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']]\ndraw_histograms(pay_amt, pay_amt.columns, 2, 3, 10)","0f7e06c6":"df.LIMIT_BAL.describe()","6e6de13d":"df.LIMIT_BAL.hist()","07cc4862":"df.AGE.describe()","5d5cf773":"df.AGE.hist()","d364f97e":"edu_ano = (df.EDUCATION == 5) | (df.EDUCATION == 6) | (df.EDUCATION == 0)\ndf.loc[edu_ano, 'EDUCATION'] = 4\ndf.EDUCATION.value_counts()","f0f96620":"df.loc[df.MARRIAGE == 0, 'MARRIAGE'] = 3\ndf.MARRIAGE.value_counts()","c7c27ebf":"plt.subplots_adjust(left = 0, bottom = 0, right = 1.2, top = 0.6, wspace = 0.5, hspace = 0.5)\n\nplt.subplot(1, 2, 1)\ndf.MARRIAGE.value_counts().plot(kind = 'bar')\nplt.xlabel(\"MARRIAGE\", font)\nplt.ylabel(\"Number\", font)\n\nplt.subplot(1, 2, 2)\ndf.EDUCATION.value_counts().plot(kind = 'bar')\nplt.xlabel(\"EDUCATION\", font)\nplt.ylabel(\"Number\", font)","35ba50f1":"fil = (df.PAY_0 == -2) | (df.PAY_0 == -1) | (df.PAY_0 == 0)\ndf.loc[fil, 'PAY_0'] = 0\nfil = (df.PAY_2 == -2) | (df.PAY_2 == -1) | (df.PAY_2 == 0)\ndf.loc[fil, 'PAY_2'] = 0\nfil = (df.PAY_3 == -2) | (df.PAY_3 == -1) | (df.PAY_3 == 0)\ndf.loc[fil, 'PAY_3'] = 0\nfil = (df.PAY_4 == -2) | (df.PAY_4 == -1) | (df.PAY_4 == 0)\ndf.loc[fil, 'PAY_4'] = 0\nfil = (df.PAY_5 == -2) | (df.PAY_5 == -1) | (df.PAY_5 == 0)\ndf.loc[fil, 'PAY_5'] = 0\nfil = (df.PAY_6 == -2) | (df.PAY_6 == -1) | (df.PAY_6 == 0)\ndf.loc[fil, 'PAY_6'] = 0\npay = df[['PAY_0','PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']]\ndraw_histograms(pay, pay.columns, 2, 3, 10)","334a2183":"df = df.rename(columns = {'default.payment.next.month': 'Default', 'PAY_0': 'PAY_1'})\ndf.head()","7952b16d":"bins = sc.woebin(df, y = 'Default', \n                 min_perc_fine_bin = 0.05,     # How many bins to cut initially into\n                 min_perc_coarse_bin = 0.05,   # Minimum percentage per final bin\n                 stop_limit = 0.1,             # Minimum information value \n                 max_num_bin = 8,              # Maximum number of bins\n                 method = 'tree')\nsc.woebin_plot(bins)","9bcce04f":"df_drop = df.drop(['PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', \n                   'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'SEX', 'MARRIAGE', 'AGE', 'EDUCATION'], axis = 1)\ndf_drop.head()","42849965":"def PAY_1_bin(PAY_1):\n    if PAY_1 < 1:\n        return 0\n    elif PAY_1 == 1:\n        return 1\n    elif PAY_1 >= 2:\n        return 2\n\ndf_drop.PAY_1 = df_drop.PAY_1.apply(PAY_1_bin)","88dcd053":"def PAY_2_bin(PAY_2):\n    if PAY_2 < 2:\n        return 0\n    elif PAY_2 >= 2:\n        return 1\n\ndf_drop.PAY_2 = df_drop.PAY_2.apply(PAY_2_bin)","d691ce1e":"def PAY_3_bin(PAY_3):\n    if PAY_3 < 2:\n        return 0\n    elif PAY_3 >= 2:\n        return 1\n\ndf_drop.PAY_3 = df_drop.PAY_3.apply(PAY_3_bin)","30f072e6":"def PAY_4_bin(PAY_4):\n    if PAY_4 < 1:\n        return 0\n    elif PAY_4 >= 1:\n        return 1\n\ndf_drop.PAY_4 = df_drop.PAY_4.apply(PAY_4_bin)","02e74eb0":"def PAY_5_bin(PAY_5):\n    if PAY_5 < 2:\n        return 0\n    elif PAY_5 >= 2:\n        return 1\n\ndf_drop.PAY_5 = df_drop.PAY_5.apply(PAY_5_bin)","3bc663f3":"def PAY_6_bin(PAY_6):\n    if PAY_6 < 2:\n        return 0\n    elif PAY_6 >= 2:\n        return 1\ndf_drop.PAY_6 = df_drop.PAY_6.apply(PAY_6_bin)","63cb430b":"def PAY_AMT1_bin(PAY_AMT1):\n    if PAY_AMT1 < 1000:\n        return 0\n    elif (PAY_AMT1 >= 1000) & (PAY_AMT1 < 4000):\n        return 1\n    elif (PAY_AMT1 >= 4000) & (PAY_AMT1 < 18000):\n        return 2\n    elif PAY_AMT1 >= 18000:\n        return 3\n\ndf_drop.PAY_AMT1 = df_drop.PAY_AMT1.apply(PAY_AMT1_bin)","614c8591":"def PAY_AMT2_bin(PAY_AMT2):\n    if PAY_AMT2 < 1000:\n        return 0\n    elif (PAY_AMT2 >= 1000) & (PAY_AMT2 < 2000):\n        return 1\n    elif (PAY_AMT2 >= 2000) & (PAY_AMT2 < 5000):\n        return 2\n    elif (PAY_AMT2 >= 5000) & (PAY_AMT2 < 16000):\n        return 3\n    elif PAY_AMT2 >= 16000:\n        return 4\n\ndf_drop.PAY_AMT2 = df_drop.PAY_AMT2.apply(PAY_AMT2_bin)","4b9f21b2":"def PAY_AMT3_bin(PAY_AMT3):\n    if PAY_AMT3 < 1000:\n        return 0\n    elif (PAY_AMT3 >= 1000) & (PAY_AMT3 < 3000):\n        return 1\n    elif (PAY_AMT3 >= 3000) & (PAY_AMT3 < 5000):\n        return 2\n    elif (PAY_AMT3 >= 5000) & (PAY_AMT3 < 17000):\n        return 3\n    elif PAY_AMT3 >= 17000:\n        return 4\n\ndf_drop.PAY_AMT3 = df_drop.PAY_AMT3.apply(PAY_AMT3_bin)","391795c4":"def LIMIT_BAL_bin(LIMIT_BAL):\n    if LIMIT_BAL < 50000:\n        return 0\n    elif (LIMIT_BAL >= 50000) & (LIMIT_BAL < 150000):\n        return 1\n    elif (LIMIT_BAL >= 150000) & (LIMIT_BAL < 250000):\n        return 2\n    elif LIMIT_BAL >= 250000:\n        return 3\n    \ndf_drop.LIMIT_BAL = df_drop.LIMIT_BAL.apply(LIMIT_BAL_bin)","89d68784":"X = df_drop.drop('Default', axis = 1)\ny = df_drop.Default \nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.3, random_state = 0, stratify = y)","76ca1c34":"# Compute the correlation matrix\ncorr = Xtrain.corr()\ncorr = np.abs(corr)\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize = (11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap = True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask = mask, cmap = cmap, vmax = 1, center = 0,\n            square = True, linewidths = 0.5, cbar_kws = {\"shrink\": .5})","a1b28d54":"corr","b00da727":"from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc","be89f567":"# Calculate performance measures from scratch\n# TP: true postives \n# TN: true negatives \n# FP: False positives \n# FN: False negatives\ndef compute_performance(yhat, y):\n    # First, get tp, tn, fp, fn\n    tn, fp, fn, tp = confusion_matrix(y,yhat).ravel()\n\n    print(f\"tp: {tp} tn: {tn} fp: {fp} fn: {fn}\")\n    \n    # Accuracy\n    acc = (tp + tn) \/ (tp + tn + fp + fn)\n    \n    # Precision\n    # \"Of the ones I labeled +, how many are actually +?\"\n    precision = tp \/ (tp + fp)\n    \n    # Recall\n    # \"Of all the + in the data, how many do I correctly label?\"\n    recall = tp \/ (tp + fn)    \n    \n    # Sensitivity\n    # \"Of all the + in the data, how many do I correctly label?\"\n    sensitivity = recall\n    \n    # Specificity\n    # \"Of all the - in the data, how many do I correctly label?\"\n    specificity = tn \/ (fp + tn)\n    \n    # Print results\n    \n    print(\"Accuracy:\",round(acc,3),\"Recall:\",round(recall,3),\"Precision:\",round(precision,3),\n          \"Sensitivity:\",round(sensitivity,3),\"Specificity:\",round(specificity,3))","1006052b":"from sklearn.linear_model import LogisticRegression\nLOGREG = LogisticRegression(solver = 'lbfgs',penalty = 'none',max_iter = 10000)\nlr_all = LOGREG.fit(Xtrain, ytrain)\nlr_all.coef_","90bf73fe":"ytest_hat_all = lr_all.predict(Xtest)\nprobs_test = lr_all.predict_proba(Xtest)\ncompute_performance(ytest_hat_all, ytest)","52d58138":"fpr, tpr, thresholds = roc_curve(ytest, \n                                 probs_test[:,1])\nax = sns.lineplot(fpr,tpr)\nax.set(xlabel = \"FPR\",ylabel = \"TPR\")\nauc(fpr,tpr)","370fe1f6":"from sklearn.linear_model import LogisticRegressionCV\n\nlogregCV = LogisticRegressionCV(penalty = 'l1', # Type of penalization l1 = lasso, l2 = ridge\n                                     Cs = 10,\n                                     tol = 0.0001, # Tolerance for parameters\n                                     cv = 3,\n                                     fit_intercept = True, # Use constant?\n                                     class_weight = 'balanced', # Weights, see below\n                                     random_state = 0, # Random seed\n                                     max_iter = 1000, # Maximum iterations\n                                     verbose = 1, # Show process. 1 is yes.\n                                     solver = 'liblinear',\n                                     n_jobs = 8,\n                                     # warm_start=False, # Train anew or start from previous weights. For repeated training.\n                                     refit = True\n                                    )","52f76189":"logregCV.fit(X = Xtrain, # All rows and from the second var to end\n           y = ytrain # The target\n          )","7e823d72":"logregCV.C_","f0143a6e":"logreg = LogisticRegression(penalty = 'l1', # Type of penalization l1 = lasso, l2 = ridge\n                                     tol = 0.0001, # Tolerance for parameters\n                                     C = 0.00077426, # Penalty constant, see below\n                                     fit_intercept = True, \n                                     class_weight = 'balanced', # Weights, see below\n                                     random_state = 0, # Random seed\n                                     max_iter = 1000, # Maximum iterations\n                                     verbose = 1, \n                                     solver = 'liblinear',\n                                     warm_start = False \n                                    )","2561a980":"logreg.fit(X = Xtrain, # All rows and from the second var to end\n           y = ytrain # The target\n          )","60f62def":"coef_df = pd.concat([pd.DataFrame({'column': Xtrain.columns}), \n                    pd.DataFrame(np.transpose(logreg.coef_))],\n                    axis = 1\n                   )\n\ncoef_df","24cb5ece":"logreg.intercept_","63b29b04":"pred_class_test = logreg.predict(Xtest)\nprobs_test = logreg.predict_proba(Xtest)","69711b0c":"compute_performance(pred_class_test, ytest)","38c815fa":"confusion_matrix_log = confusion_matrix(ytest, pred_class_test)\n \n# Turn matrix to percentages\nconfusion_matrix_log = confusion_matrix_log.astype('float') \/ confusion_matrix_log.sum(axis = 1)[:, np.newaxis]\n \n# Turn to dataframe\ndf_cm = pd.DataFrame(\n        confusion_matrix_log, index = ['Not Default', 'Default'], columns = ['Not Default', 'Default'], \n)\n \n# Parameters of the image\nfigsize = (5,5)\nfontsize = 10\n \n# Create image\nfig = plt.figure(figsize = figsize)\nheatmap = sns.heatmap(df_cm, annot = True, fmt = '.2f',linecolor = \"Darkblue\", cmap = \"Blues\",\n            yticklabels = ['Not Default', 'Default'],)\n \n# Make it nicer\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation = 0, \n                             ha = 'right', fontsize = fontsize)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation = 45,\n                             ha = 'right', fontsize = fontsize)\n \n# Add labels\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\n \n# Plot!\nplt.show()","72d694fd":"from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc\nfpr, tpr, thresholds = roc_curve(ytest, \n                                 probs_test[:,1])\nax = sns.lineplot(fpr,tpr)\nax.set(xlabel = \"FPR\",ylabel = \"TPR\")\nauc(fpr,tpr)","60cea9a5":"ytrain.value_counts()","b92bff10":"16355\/4645","b694297a":"from xgboost import XGBClassifier\n#Define the classifier.\nXGB = XGBClassifier(max_depth = 3,                 # Depth of each tree\n                            learning_rate = 0.1,            # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n                            n_estimators = 100,             # How many trees to use, the more the better, but decrease learning rate if many used.\n                            verbosity = 1,                  # If to show more errors or not.\n                            objective = 'binary:logistic',  # Type of target variable.\n                            booster = 'gbtree',             # What to boost. Trees in this case.\n                            n_jobs = 8,                     # Parallel jobs to run. Set your processor number.\n                            gamma = 0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n                            subsample = 0.632,              # Subsample ratio. Can set lower\n                            colsample_bytree = 1,           # Subsample ratio of columns when constructing each tree.\n                            colsample_bylevel = 1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n                            colsample_bynode = 1,           # Subsample ratio of columns when constructing each split.\n                            reg_alpha = 1,                  # Regularizer for first fit. alpha = 1, lambda = 0 is LASSO.\n                            reg_lambda = 0,                 # Regularizer for first fit.\n                            scale_pos_weight = 3.52099,           # Balancing of positive and negative weights.\n                            base_score = 0.5,               # Global bias. Set to average of the target rate.\n                            random_state = 0,        # Seed\n                            missing = None                  # How are nulls encoded?\n                            )","94364ab3":"# Define the parameters. Play with this grid!\nparam_grid = dict({'n_estimators': [100, 150, 200],\n                   'max_depth': [2, 3, 4],\n                 'learning_rate' : [0.01, 0.05, 0.1, 0.15]\n                  })","12e5df4c":"from sklearn.model_selection import GridSearchCV\n\n# Define grid search object.\nGridXGB = GridSearchCV(XGB,        # Original XGB. \n                       param_grid,          # Parameter grid\n                       cv = 3,              # Number of cross-validation folds.  \n                       scoring = 'recall', # How to rank outputs.\n                       n_jobs = 8,          # Parallel jobs. -1 is \"all you have\"\n                       refit = False,       # If refit at the end with the best. We'll do it manually.\n                       verbose = 1          # If to show what it is doing.\n                      )","417973f0":"GridXGB.fit(Xtrain,ytrain)","0de12cda":"GridXGB.best_params_.get('max_depth')","81b2b3b4":"GridXGB.best_params_.get('learning_rate')","e11b4124":"GridXGB.best_params_.get('n_estimators')","cd84e09b":"# Create XGB with best parameters.\nXGB = XGBClassifier(max_depth = GridXGB.best_params_.get('max_depth'), # Depth of each tree\n                            learning_rate = GridXGB.best_params_.get('learning_rate'), # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n                            n_estimators = GridXGB.best_params_.get('n_estimators'), # How many trees to use, the more the better, but decrease learning rate if many used.\n                            verbosity = 1,                  # If to show more errors or not.\n                            objective = 'binary:logistic',  # Type of target variable.\n                            booster = 'gbtree',             # What to boost. Trees in this case.\n                            n_jobs = 8,                     # Parallel jobs to run. Set your processor number.\n                            gamma = 0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n                            subsample = 0.632,              # Subsample ratio. Can set lower\n                            colsample_bytree = 1,           # Subsample ratio of columns when constructing each tree.\n                            colsample_bylevel = 1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n                            colsample_bynode = 1,           # Subsample ratio of columns when constructing each split.\n                            reg_alpha = 1,                  # Regularizer for first fit. alpha = 1, lambda = 0 is LASSO.\n                            reg_lambda = 0,                 # Regularizer for first fit.\n                            scale_pos_weight = 3.52099,     # Balancing of positive and negative weights.\n                            base_score = 0.5,               # Global bias. Set to average of the target rate.\n                            random_state = 0,        # Seed\n                            missing = None                  # How are nulls encoded?\n                            )","ee514720":"XGB.fit(Xtrain, ytrain)","c03ad405":"# Plot variable importance\nimportances = XGB.feature_importances_\nindices = np.argsort(importances)[::-1] \n\nf, ax = plt.subplots(figsize = (3, 8))\nplt.title(\"Variable Importance\")\nsns.set_color_codes(\"pastel\")\nsns.barplot(y = [Xtrain.columns[i] for i in indices], x = importances[indices], \n            label=\"Total\", color = \"b\")\nax.set(ylabel = \"Variable\",\n       xlabel = \"Variable Importance (Entropy)\")\nsns.despine(left = True, bottom = True)","98791092":"XGBClassTest = XGB.predict(Xtest)\nxg_probs_test = XGB.predict_proba(Xtest)\ncompute_performance(XGBClassTest, ytest)","cba6b3b5":"confusion_matrix_xg = confusion_matrix(ytest, XGBClassTest)\n \n# Turn matrix to percentages\nconfusion_matrix_xg = confusion_matrix_xg.astype('float') \/ confusion_matrix_xg.sum(axis=1)[:, np.newaxis]\n \n# Turn to dataframe\ndf_cm = pd.DataFrame(\n        confusion_matrix_xg, index=['Not Default', 'Default'], columns=['Not Default', 'Default'], \n)\n \n# Parameters of the image\nfigsize = (5,5)\nfontsize=10\n \n# Create image\nfig = plt.figure(figsize=figsize)\nheatmap = sns.heatmap(df_cm, annot=True, fmt='.2f',linecolor=\"Darkblue\", cmap=\"Blues\",\n            yticklabels=['Not Default', 'Default'],)\n \n# Make it nicer\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, \n                             ha='right', fontsize=fontsize)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45,\n                             ha='right', fontsize=fontsize)\n \n# Add labels\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\n \n# Plot!\nplt.show()","bf5fddd6":"fpr, tpr, thresholds = roc_curve(ytest, \n                                 xg_probs_test[:,1])\nax=sns.lineplot(fpr,tpr)\nax.set(xlabel=\"FPR\",ylabel=\"TPR\")\nauc(fpr,tpr)","290fa1e7":"from sklearn.ensemble import RandomForestClassifier\nRF_default = RandomForestClassifier(n_estimators=210, # Number of trees to train\n                       criterion = 'entropy', # How to train the trees. Also supports gini.\n                       max_depth = None, # Max depth of the trees. Not necessary to change.\n                       min_samples_split = 2, # Minimum samples to create a split.\n                       min_samples_leaf = 0.0001, # Minimum samples in a leaf. Accepts fractions for %. This is 0.1% of sample.\n                       min_weight_fraction_leaf = 0.0, # Same as above, but uses the class weights.\n                       max_features = 'auto', # Maximum number of features per split (not tree!) by default is sqrt(vars)\n                       max_leaf_nodes = None, # Maximum number of nodes.\n                       min_impurity_decrease = 0.0001, # Minimum impurity decrease. This is 10^-4.\n                       bootstrap = False, # If sample with repetition. For large samples (>100.000) set to false.\n                       oob_score = False,  # If report accuracy with non-selected cases.\n                       n_jobs = 8, # Parallel processing. Set to the number of cores you have. Watch your RAM!!\n                       random_state = 250886749, # Seed\n                       verbose = 1, # If to give info during training. Set to 0 for silent training.\n                       warm_start = False, # If train over previously trained tree.\n                       class_weight = 'balanced' # Balance the classes.\n                    )\nRF_fit = RF_default.fit(Xtrain, ytrain)","5dfce0ef":"RF_predict = RF_fit.predict(Xtest)\nRF_predict_prob = RF_fit.predict_proba(Xtest)","794e6bc4":"compute_performance(RF_predict, ytest)","473e87a2":"confusion_matrix_xg = confusion_matrix(ytest, RF_predict)\n \n# Turn matrix to percentages\nconfusion_matrix_xg = confusion_matrix_xg.astype('float') \/ confusion_matrix_xg.sum(axis=1)[:, np.newaxis]\n \n# Turn to dataframe\ndf_cm = pd.DataFrame(\n        confusion_matrix_xg, index=['Not Default', 'Default'], columns=['Not Default', 'Default'], \n)\n \n# Parameters of the image\nfigsize = (5,5)\nfontsize = 10\n \n# Create image\nfig = plt.figure(figsize = figsize)\nheatmap = sns.heatmap(df_cm, annot = True, fmt = '.2f',linecolor = \"Darkblue\", cmap=\"Blues\",\n            yticklabels = ['Not Default', 'Default'],)\n \n# Make it nicer\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation = 0, \n                             ha = 'right', fontsize = fontsize)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation = 45,\n                             ha = 'right', fontsize = fontsize)\n \n# Add labels\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\n \n# Plot!\nplt.show()","8625b5e8":"fpr, tpr, thresholds = roc_curve(ytest, \n                                 RF_predict_prob[:,1])\nax = sns.lineplot(fpr,tpr)\nax.set(xlabel = \"FPR\",ylabel = \"TPR\")\nauc(fpr,tpr)","abf6a75b":"* ### Load Dataset","d1e1e6e5":"# University of Western Ontario CS 9114A Introduction to Data Science Final Project\n## Credit Card Fraud Detection\n\n#### *By Xuanzhi Huang, Wanyue Xin, Tongchen Yi*","aea54939":"Then we do the same thing for MARRIAGE = 0.","47732be4":"From PAY_0 to PAY_6, all present an undocumented label -2 and 0. According to UCI, 1, 2, 3, ..., 8 are the months of delay, and -1 indicates 'pay duly'. Here it is reasonable to label 0 and -2 \"pay duly\", as 0 means no delay, and -2 can mean payment in advance. To make it more understandable, We will change PAY = -2, -1 to PAY = 0, which means there is no delay time, in the data cleaning part.","8f7504cb":"Here we use the function **woebin** in package **scorecardpy** to calculate WOE and IV of each feature, and then drop some useless variables as well as bin the rest in a proper way. The results are shown in the plots below.","f5d66ea3":"We stratifiedly split the dataset into training set and test set according to the distribution of \"Default\" label.","772fb7b4":"We set IV = 0.1 as a threshold and drop variables with IV under it.","7339dfb9":"## 4. Feature Engineering","5f6c95f0":"## 3. Data Cleaning\n","64011fda":"We choose dataset [\"Default of Credit Card Clients\"](https:\/\/archive.ics.uci.edu\/ml\/datasets\/default+of+credit+card+clients) from UCI for our final project. We would use some classification algorithms to predict whether a client would default or not.","b29ba12c":"We can find two bothering and confusing attribute names in the dataset, so here we change them for convenience.","8a48ffc8":"Here are the attribute information from UCI.\n* **ID:** ID of each client\n* **LIMIT_BAL:** Amount of given credit in NT dollars (includes individual and family\/supplementary credit)\n* **SEX:** Gender (1 = male, 2 = female)\n* **EDUCATION:** (1 = graduate school, 2 = university, 3 = high school, 4 = others)\n* **MARRIAGE:** Marital status (1 = married, 2 = single, 3 = others)\n* **AGE:** Age (year)\n* **PAY_0:** Repayment status in September, 2005 (-1 = pay duly, 1 = payment delay for one month, 2 = payment delay for two months, ... 8 = payment delay for eight months, 9 = payment delay for nine months and above)\n* **PAY_2:** Repayment status in August, 2005 (scale same as above)\n* **PAY_3:** Repayment status in July, 2005 (scale same as above)\n* **PAY_4:** Repayment status in June, 2005 (scale same as above)\n* **PAY_5:** Repayment status in May, 2005 (scale same as above)\n* **PAY_6:** Repayment status in April, 2005 (scale same as above)\n* **BILL_AMT1:** Amount of bill statement in September, 2005 (NT dollar)\n* **BILL_AMT2:** Amount of bill statement in August, 2005 (NT dollar)\n* **BILL_AMT3:** Amount of bill statement in July, 2005 (NT dollar)\n* **BILL_AMT4:** Amount of bill statement in June, 2005 (NT dollar)\n* **BILL_AMT5:** Amount of bill statement in May, 2005 (NT dollar)\n* **BILL_AMT6:** Amount of bill statement in April, 2005 (NT dollar)\n* **PAY_AMT1:** Amount of previous payment in September, 2005 (NT dollar)\n* **PAY_AMT2:** Amount of previous payment in August, 2005 (NT dollar)\n* **PAY_AMT3:** Amount of previous payment in July, 2005 (NT dollar)\n* **PAY_AMT4:** Amount of previous payment in June, 2005 (NT dollar)\n* **PAY_AMT5:** Amount of previous payment in May, 2005 (NT dollar)\n* **PAY_AMT6:** Amount of previous payment in April, 2005 (NT dollar)\n* **default.payment.next.month:** Default payment (1 = yes, 0 = no)","d2c5ddd7":"## 1. Preliminaries","09c08ced":"We take a look at their distribution again.","5553fc51":"As mentioned in the data exploration (categorical variables) part, we label undocumented\/unlabeled education level \"others\", i.e. change their values to 4.","41df5b02":"* ### Attribute Information","30e90bbe":"It is shown that all the numerical variables are skewed, which may affect our prediction. ","91ca08a6":"Fortunately, there is no missing value in the dataset. Then take a look at the description and distribution of each variable to find out if there exist anomalous data. We will divide variables into categorical ones and numerical ones and observe them respectively.\n#### 1) Categorical Variables\n\nThere are three categorical variables in the dataset, including SEX, MARRIAGE, and EDUCATION. Take a look at their description and distribution.","b22f2a78":"## 2. Data Exploration","75524cba":"* ## XGBoost\n\nWe choose the best hyperparameters by Grid Search, and then fit and make prediction.","70a37b18":"As mentioned before in the data exploration (numerical variables) part, change PAY == -1 and -2 to 0.","036c8807":"## 4. Model Training","6d0708b1":"* ## Random Forest","41d54dfb":"Best parameter: n_estimators = 200, max_depth = 3, learning_rate = 0.1","5caf8369":"* ## Logistic\n\nWe choose the best hyperparameters by Cross Validation, and then fit and make prediction.","4fe239fb":"As the first step, find out if there are missing or anomalous data.","4574cab6":"Then we cut the rest variables into bins given by the graphs.","1cfe3e0b":"The best penalty constant is 0.00077426.","525fe9bb":"According to the attribute information given by UCI, SEX = 1 means \"male\", while SEX = 2 indicates \"female\"; MARRIGE = 1 means \"married\", 2 means \"single\", and 3 indicates \"others\", such as \"divorced\"; EDUCATION = 1 indicates \"graduate school\", 2 means \"university\", 3 means \"high school\", and 4 indicates \"others\", like \"primary school\". From the description tables and barplots above, we can find some undocumented labels (MARRIAGE = 0, EDUCATION = 0) and some unknown labels (EDUCATION = 5, EDUCATION = 6). All of these can be safely categorized as \"others\", as the number of them is small and thus will not affect our prediction that much even if we categorize them wrongly. Besides, it is difficult and too compliated to build some other models to work out these undocumented or unknown labels. We will deal with these labels in the data cleaning part.\n\n#### 2) Numerical Variables\n\nWe then observe the description tables and histograms of numerical variables.","23794168":"* ### Data Distribution & Description"}}