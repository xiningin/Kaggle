{"cell_type":{"58e030fb":"code","3aa432a2":"code","1884bad2":"code","8f385c87":"code","543d608a":"code","753a57c6":"code","3ea47c8a":"code","9dd3a365":"code","6571f458":"code","33ccf4eb":"code","01aee8f7":"code","471b5a0e":"code","3e634d6e":"code","02bb0115":"code","ba2f4d2c":"code","d9d6854a":"code","8f41eddb":"code","5332a958":"code","34b770d8":"code","d2de333f":"code","4b622b3d":"code","beb0d979":"code","1a6c40f4":"code","0a93e027":"code","a2266727":"code","a0ca68e3":"code","9373587a":"code","907eb6e2":"code","e0dadd8a":"code","6dabd571":"code","93807d4b":"code","5e7e195d":"code","417d63b1":"code","b26d6eca":"code","4445390b":"code","d4fbfcc1":"code","95fb3e0a":"code","48feb7d5":"code","f7c1f319":"markdown","04670cb0":"markdown","3a27027e":"markdown","57a3c9e5":"markdown","df066057":"markdown","ad21d13d":"markdown","a18f13f4":"markdown","ec6fac53":"markdown","51ead605":"markdown","20ae2bde":"markdown","29c32076":"markdown","030290fd":"markdown","7080e604":"markdown","f9befc06":"markdown","80a4b1ea":"markdown"},"source":{"58e030fb":"from kaggle.competitions import twosigmanews\nimport catboost as catb\nfrom catboost import CatBoostClassifier\nfrom datetime import datetime, date\nimport gc\nimport lightgbm as lgb\nimport multiprocessing\nfrom multiprocessing import Pool\nimport numpy as np\nimport pandas as pd\nfrom resource import getrusage, RUSAGE_SELF\nfrom sklearn import model_selection\nfrom xgboost import XGBClassifier","3aa432a2":"np.random.seed(1) # I don't know if using numpy random seeding helps in reproducing results but I do it anyways to be safe","1884bad2":"global STARTED_TIME\nSTARTED_TIME = datetime.now()\n\n# It's better to use cpu_count from the system - who knows what happens during test phase\nglobal N_THREADS\nN_THREADS = multiprocessing.cpu_count() * 2 \n\nprint(f'N_THREADS: {N_THREADS}')","8f385c87":"# FILTERDATE - start date for the train data\nFILTERDATE = date(2007, 1, 1)\n\n# SAMPLEDATE - use it for sampling and fast sanity check of scripts. Since I am generating too many features limiting data amount will help in not crashing\n# In production, it would be better to cherry pick useful features and discard non important features. And then use more data again.\nSAMPLEDATE = None\nSAMPLEDATE = date(2008, 1, 30)","543d608a":"global N_WINDOW, BASE_FEATURES\n\nN_WINDOW = np.sort([5, 10, 20, 252])\n\n# Features for lags calculation\nBASE_FEATURES = [\n    'returnsOpenPrevMktres10',\n    'returnsOpenPrevRaw10',\n    'open',\n    'close']","753a57c6":"# Tracking time and memory usage\nglobal MAXRSS\nMAXRSS = getrusage(RUSAGE_SELF).ru_maxrss\ndef using(point=\"\"):\n    global MAXRSS, STARTED_TIME\n    print(str(datetime.now()-STARTED_TIME).split('.')[0], point, end=' ')\n    max_rss = getrusage(RUSAGE_SELF).ru_maxrss\n    if max_rss > MAXRSS:\n        MAXRSS = max_rss\n    print(f'max RSS {MAXRSS\/1024\/1024:.1f}Gib')\n    gc.collect()","3ea47c8a":"global FILLNA\nFILLNA = -99999\n\newm = pd.Series.ewm\n\ndef generate_features_for_df_by_assetCode(df_by_code):\n    prevlag = 1\n    for window in N_WINDOW:\n        rolled = df_by_code[BASE_FEATURES].shift(prevlag).rolling(window=window)\n        df_by_code = df_by_code.join(rolled.mean().add_suffix(f'_window_{window}_mean'))\n        df_by_code = df_by_code.join(rolled.median().add_suffix(f'_window_{window}_median'))\n        df_by_code = df_by_code.join(rolled.max().add_suffix(f'_window_{window}_max'))\n        df_by_code = df_by_code.join(rolled.min().add_suffix(f'_window_{window}_min'))\n        for col in BASE_FEATURES: # not sure if this can be optimized without using for loop but I only know how to calculate exponentially moving averages like this\n            df_by_code[col + f'_window_{window}_ewm'] = ewm(df_by_code[col], span=window).mean().add_suffix(f'_window_{window}_ewm')\n    return df_by_code.fillna(FILLNA)\n\ndef generate_features(df):\n    global BASE_FEATURES, N_THREADS\n    all_df = []\n    df_codes = df.groupby('assetCode')\n    df_codes = [df_code[1][['time','assetCode'] + BASE_FEATURES] for df_code in df_codes]\n    pool = Pool(N_THREADS)\n    all_df = pool.map(generate_features_for_df_by_assetCode, df_codes)\n    new_df = pd.concat(all_df)\n    new_df.drop(BASE_FEATURES,axis=1,inplace=True)\n    pool.close()\n    return new_df","9dd3a365":"# The following functions are used for initialization and expanding of numpy arrays\n# for storing historical data of all assets.\n\n# It helps to have very fast lags creation.\n\n# Initialization of history array\ndef initialize_values(items=5000, features=4, history=15):\n    return np.ones((items, features, history))*np.nan\n\n# Expanding of history array for new assets\ndef expand_history_array(history_array, items=100):\n    return np.concatenate([history_array, initialize_values(items, history_array.shape[1], history_array.shape[2])])\n\n# codes dictionary maps assetCode to the index in the history array\n# if we found new assetCode - we have to store it and expand history\ndef get_index_by_assetCode(assetCode):\n    global code2array_idx, history\n    try: \n        return code2array_idx[assetCode]\n    except KeyError:\n        code2array_idx[assetCode] = len(code2array_idx)\n        if len(code2array_idx) > history.shape[0]:\n            history = expand_history_array(history, 100)\n        return code2array_idx[assetCode]\n\n# list2codes returns numpy array of indices of assetCodes in history array(for each day)\ndef codes_list2idx_array(codes_list):\n    return np.array([get_index_by_assetCode(assetCode) for assetCode in codes_list])","6571f458":"env = twosigmanews.make_env()","33ccf4eb":"(market_train_df, news_train_df) = env.get_training_data()","01aee8f7":"# Memory limit 16GB leaves me no choice but to drop some columns early on.\n# Kernel crashes when all memory is used up.\nmarket_train_df = market_train_df.drop(['universe'], axis = 1)\nnews_train_df = news_train_df.drop(['sourceTimestamp', 'sourceId', 'headline', 'takeSequence', \n                                    'provider', 'subjects', 'audiences', 'bodySize', \n                                    'companyCount', 'headlineTag', 'marketCommentary', 'sentenceCount', 'wordCount',\n                                    'relevance', 'sentimentWordCount', 'noveltyCount12H', 'noveltyCount24H',\n                                    'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H',\n                                    'volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D',\n                                    'volumeCounts7D'], axis = 1)\n# uncomment if you are not using news data\n# del news_train_df\nusing('Data loaded')","471b5a0e":"def process_time(df):\n    df['time'] = df['time'].dt.date\n    return df\n\nmarket_train_df = process_time(market_train_df)\nnews_train_df = process_time(news_train_df)","3e634d6e":"# Dataframe filtering\nprint('DF Filtering')\nmarket_train_df = market_train_df.loc[market_train_df['time']>=FILTERDATE]\nnews_train_df = news_train_df.loc[news_train_df['time']>=FILTERDATE]\n\nif SAMPLEDATE is not None:\n    market_train_df = market_train_df.loc[market_train_df['time']<=SAMPLEDATE]  \n    news_train_df = news_train_df.loc[news_train_df['time']<=SAMPLEDATE]  \nusing('Done')","02bb0115":"def add_market_mean_col(market_df):\n    daily_market_mean_df = market_df.groupby('time').mean()\n    daily_market_mean_df = daily_market_mean_df[['volume', 'close']]\n    merged_df = market_df.merge(daily_market_mean_df, left_on='time',\n                                right_index=True, suffixes=(\"\",'_market_mean'))\n    merged_df['volume\/volume_market_mean'] = merged_df['volume'] \/ merged_df['volume_market_mean']\n    merged_df['close\/close_market_mean'] = merged_df['close'] \/ merged_df['close_market_mean']\n    return merged_df.reset_index(drop = True)\n\nBASE_FEATURES = BASE_FEATURES + ['volume', 'volume\/volume_market_mean', 'close\/close_market_mean']\n\nmarket_train_df = add_market_mean_col(market_train_df)\nmarket_train_df.head(3)","ba2f4d2c":"market_train_df.tail(3) # check different days have different market mean","d9d6854a":"def generate_open_close_ratio(df):\n    df['open\/close'] = df['open'] \/ df['close']\n    \nBASE_FEATURES = BASE_FEATURES + ['open\/close']\n\ngenerate_open_close_ratio(market_train_df)\nusing('Done')","8f41eddb":"open_raw_cols = ['returnsOpenPrevRaw1', 'returnsOpenPrevRaw10']\nclose_raw_cols = ['returnsClosePrevRaw1', 'returnsClosePrevRaw10']\n\ndef raw_features_to_ratio_features(df):\n    for col in open_raw_cols:\n        df[col + '\/open' ] = df[col] \/ df['open']\n    for col in close_raw_cols:\n        df[col + '\/close'] = df[col] \/ df['close']\n\nBASE_FEATURES = BASE_FEATURES + ['returnsClosePrevRaw1\/close', 'returnsClosePrevRaw10\/close', 'returnsOpenPrevRaw1\/open', 'returnsOpenPrevRaw10\/open']\n\nraw_features_to_ratio_features(market_train_df)\nmarket_train_df.head(3)","5332a958":"origlen = len(market_train_df)\nprint(market_train_df.loc[market_train_df['open\/close'] >= 3][['open', 'close']])\nprint(market_train_df.loc[market_train_df['open\/close'] <= 0.3][['open', 'close']])\nmarket_train_df = market_train_df.loc[market_train_df['open\/close'] < 3]\nmarket_train_df = market_train_df.loc[market_train_df['open\/close'] > 0.3]\nprint(origlen - len(market_train_df), \"row deleted\")\nusing('Done')","34b770d8":"target = 'returnsOpenNextMktres10'\ngenerated_non_feature_cols = [] # Use it to filter out generated cols that turns out not so useful after analyzing feature importances\nnon_feature_cols = ['assetCode', 'assetName', target, 'time', 'time_x', 'volume_y',] + generated_non_feature_cols","d2de333f":"new_df = generate_features(market_train_df)\nusing('Done')","4b622b3d":"market_train_df = pd.merge(market_train_df, new_df, how = 'left', on = ['time', 'assetCode'])\ndel new_df\nusing('Done')","beb0d979":"# label encoding of assetCode\ndef encode_assetCode(market_train):\n    global code2array_idx\n    market_train['assetCodeT'] = market_train['assetCode'].map(code2array_idx)\n    market_train = market_train.dropna(axis=0)\n    return market_train\n\ncode2array_idx = dict(\n    zip(market_train_df.assetCode.unique(), np.arange(market_train_df.assetCode.nunique()))\n)\n\nmarket_train_df = encode_assetCode(market_train_df)","1a6c40f4":"def merge_with_news_data(market_df, news_df):\n    news_df['firstCreated'] = news_df.firstCreated.dt.hour\n    news_df['assetCodesLen'] = news_df['assetCodes'].map(lambda x: len(eval(x)))\n    news_df['asset_sentiment_count'] = news_df.groupby(['assetName', 'sentimentClass'])['firstCreated'].transform('count')\n    # I don't use assetCode for joining key, but use assetName. One news row has multiple assetCode so they don't match nicely with market data\n    # Also remember assetCodes with same assetName are related assets (normal stock and preferred stock from same company e.g.)\n    kcol = ['time', 'assetName']\n    news_df = news_df.groupby(kcol, as_index=False).mean()\n    market_df = pd.merge(market_df, news_df, how='left', on=kcol, suffixes=(\"\", \"_news\"))\n    return market_df\n\nmarket_train_df = merge_with_news_data(market_train_df, news_train_df)\ndel news_train_df\nusing(\"Merged news data\")\nmarket_train_df.head(3)","0a93e027":"# this cell only need it for preparing to predict, uncomment to prepare to predict.\n# history stores information for all assets, required features\n\n# history = initialize_values(len(code2array_idx), len(BASE_FEATURES), np.max(N_WINDOW)+1)\n\n# # Get the latest information for assets\n# latest_events = market_train_df.groupby('assetCode').tail(np.max(N_WINDOW)+1)\n# # but we may have different informations size for different assets\n# latest_events_size = latest_events.groupby('assetCode').size()","a2266727":"# Filling the history array, this cell only need it for preparing to predict, uncomment to prepare to predict.\n\n# for s in latest_events_size.unique():\n#     for i in range(len(BASE_FEATURES)):\n#         # l is a Dataframe with assets with the same history size for each asset\n#         l = latest_events[\n#             latest_events.assetCode.isin(latest_events_size[latest_events_size==s].index.values)\n#         ].groupby('assetCode')[BASE_FEATURES[i]].apply(list)\n\n#         # v is a 2D array contains history information of feature RETURN_FEATURES[i] \n#         v = np.array([k for k in l.values])\n\n#         # r contains indexes (in the history array) of all assets\n#         r = codes_list2idx_array(l.index.values)\n\n#         # Finally, filling history array\n#         history[r, i, -s:] = v\n#         del l, v, r\n\n# del latest_events, latest_events_size\n# using('Done')","a0ca68e3":"y = market_train_df[target] >= 0\ny = y.values.astype(int)\nfcol = [c for c in market_train_df if c not in non_feature_cols]\n\nX = market_train_df[fcol]\nprint(\"len:\", len(X.columns))\nfor col in X.columns:\n    print(col, end = ', ')\nX = X.values\n\n# Scaling of X values, Tree based models shouldn't need scaling tho?\nmaxs = np.max(X, axis=0)\nrng = maxs - np.min(X, axis=0)\nX = 1 - ((maxs - X) \/ rng)\n\n# Sanity check\nassert X.shape[0] == y.shape[0]\n\nX_train, X_val, y_train, y_val = model_selection.train_test_split(X, y, test_size=0.25, random_state=99)\n\ndel market_train_df, X, y\nusing('done')","9373587a":"# Train lgbm\n\nlgb_train_data = lgb.Dataset(X_train, label = y_train)\nlgb_test_data = lgb.Dataset(X_val, label = y_val)\n\nparams = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'learning_rate': 0.19016805202090095,\n        'num_leaves': 2583,\n        'min_data_in_leaf': 213,\n        'num_iteration': 172,\n        'max_bin': 220,\n        'seed': 42,\n    }\n\ngbm = lgb.train(params,\n                lgb_train_data,\n                valid_sets=lgb_test_data,\n                early_stopping_rounds=5,\n                verbose_eval=30,\n            )\n\ndel lgb_train_data, lgb_test_data\nusing('Training done')","907eb6e2":"# Train catboost\n\ntrain_pool = catb.Pool(X_train, y_train)\nvalidate_pool = catb.Pool(X_val, y_val)\n\ncatb_model = CatBoostClassifier(\n    iterations = 300,\n    random_seed=42,\n)\n\ncatb_model.fit(\n    X_train, y_train.astype(int),\n    eval_set=(X_val, y_val),\n    verbose=50,\n    plot=False\n)","e0dadd8a":"# Train XGB\nxgb = XGBClassifier(random_state=42) \neval_set = [(X_val, y_val)] \nxgb.fit(X_train, y_train, eval_metric=\"logloss\", early_stopping_rounds=5, eval_set=eval_set, verbose=True)","6dabd571":"print(\"total features:\", len(fcol), \", average:\", 100\/len(fcol))","93807d4b":"def show_feature_importances(feature_importances):\n    total_feature_importances = sum(feature_importances)\n    assert len(feature_importances) == len(fcol) # sanity check\n    for score, feature_name in sorted(zip(feature_importances, fcol), reverse=True):\n        print('{}: {}'.format(feature_name, score\/total_feature_importances * 100))","5e7e195d":"# lgbm importances split\nshow_feature_importances(gbm.feature_importance(importance_type='split'))","417d63b1":"# Cat boost feature importance\nshow_feature_importances(catb_model.get_feature_importance(train_pool))","b26d6eca":"# XGB feature importance\nshow_feature_importances(xgb.feature_importances_)","4445390b":"# Another standard for deciding feature importance\n# lgbm importances gain\n# show_feature_importances(gbm.feature_importance(importance_type='gain'))","d4fbfcc1":"def get_non_important_features(feature_importances, threshold):\n    total_feature_importances = sum(feature_importances)\n    assert len(feature_importances) == len(fcol) # sanity check\n    return [feature_name for score, feature_name in sorted(zip(feature_importances, fcol), reverse=True) if ((score * 100) \/ total_feature_importances)  < threshold]\n\nnon_features = get_non_important_features(gbm.feature_importance(importance_type='split'), threshold = 0.1)\nprint(len(non_features))\nnon_features","95fb3e0a":"# Other standards for deciding feature importance\n# print(get_non_important_features(gbm.feature_importance(importance_type='gain'), threshold = 0.1))\n# print(get_non_important_features(catb_model.get_feature_importance(train_pool), threshold = 0.1))\n# print(get_non_important_features(xgb.feature_importances_, threshold = 0.1))","48feb7d5":"using('finished!')","f7c1f319":"Having too many features don't always help. It can make it harder for models to train or sometimes lead them to overfit. Let's get some list of features whose importance is way below average(0.37%). Use percentage threshold to set the bar. Whopping 123 features have less than 0.1% importance. Remember you can add these colums to `generated_non_feature_cols` list and run the kernel again to use only important enough columns.","04670cb0":"I am not sure if it is a good thing to use assetCode as a feature, because it is like an id column and we are putting our model in a risk of overfitting.\nIf model memorize too much about assetCode and the characteristics of each asset changes a lot in testing phase, it wouldn't be good. \n(e.g think about case a company changes strategy\/ceo, reduces outstanding shares etc, it will be very different from training data even if the code is the same)\nSo it might be good thing to not use 'assetCodeT' as a feature to avoid overfitting.","3a27027e":"Most day traders would consider how much an asset price appreciated \/ depreciated on a single trading day is an important factor in trading.","57a3c9e5":"Not related to feature generations but I want to get rid of outliers (too much variations of price in one day). Among the deleted rows, opening price 999.99 seems that it is a dummy value that was filled.","df066057":"Start training of 3 different models. By no means this can be used as benchmark result because only lgbm is tuned and the others mostly use default paramters. Still it's interesting that XGB stops training so early on. With current parameters, lgbm works the best in terms of low loss.","ad21d13d":"A lot of news data columns are deleted but let's make use of what's left. I think sentiment related columns are most important anyways but who knows.","a18f13f4":"Raw return features themselves are not that meaningful (unlike market residual features) because 1 dollar appreciation of an asset which has price of 5 and that of 100 have very different implications. Raw return features ratio to 'open', 'close' might be more useful.","ec6fac53":"This kernel only focuses on various experimental features generation, checking their importances, how to filter out non-important features. Not all the features generated are useful. However, it is easy to exclude features using `generated_non_feature_cols` variable. I can imagine there can be many more interesting features than I experimented here. What were some of your favorite features that you created? Let me know.\nThank you for reading.","51ead605":"We are going to be end up generating too many features. `generated_non_feature_cols` will be used to filter out unnecessary columns before training starts. (filter by `non_feature_cols` but adding columns to `generated_non_feature_cols` works because `non_feature_cols` includes `generated_non_feature_cols`.)","20ae2bde":"Now let's check the feature importance. When we have too many features, I find it it's not so useful to draw graph because feature names are so crammed and hard to read. So I print features in the order of importance and in terms of percentage of total feature importance. It is intereting to see each model views feature importances slightly differently.","29c32076":"The first feature I think is interesting is ratio of a single asset's 'volume', 'close' to mean value of 'market'. Because all assets have different average volumes and closing price (I think in this regard, either one of open or close is usually enough). If single asset's 'volume'\/'close''s ratio changes in relation to 'market_mean' it would be a possible indicator that implies an asset is oversold \/ overbought. Thus, we will include these ratios to base features.","030290fd":"Generate features with the usual window statics (mean, median, max, min, exponentially weighted mean).","7080e604":"Now that training is finished, let's check how many features we have and what percentage of contribution each feature makes on average. If some features are contributing below average you might consider getting rid of those features.","f9befc06":"In this competition, I tried to come up with a lot of useful features to improve my model. Unfortunately, I didn't know how to create features quickly so I could not use many features in the end because the more feature I have, the more it will take to create them. Vadim kindly shared a [kernel](https:\/\/www.kaggle.com\/nareyko\/fast-lags-calculation-concept-using-numpy-arrays\/comments#454588) that shows how to create features very fast. Using this code, it is possible to make use of many features. So, I wanted to experiment with a lot of features that might be useful.\nCredits for efficient feature creation algorithms goes to [Vadim](https:\/\/www.kaggle.com\/nareyko). Other kernels that inspired me for ideas include [simple quant features kernel](https:\/\/www.kaggle.com\/youhanlee\/simple-quant-features-using-python), [eda kernel](https:\/\/www.kaggle.com\/qqgeogor\/eda-script-67) and [another eda with nice visualization](https:\/\/www.kaggle.com\/artgor\/eda-feature-engineering-and-everything)","80a4b1ea":"Split training and validation data set."}}