{"cell_type":{"c416c61c":"code","3483fcee":"code","0f4160dc":"code","5ef61ee9":"code","7c933da3":"code","9d39c714":"code","23eeb6d2":"code","a20eb08e":"code","b95ebdb8":"code","e82ba22a":"code","dc221f3d":"code","0147341e":"code","f81f260b":"code","8f26a69f":"code","1edae6b8":"code","1da5750d":"markdown","6ac17587":"markdown","fe14c780":"markdown","65df9b9e":"markdown","eeb356d3":"markdown","ebb63a31":"markdown","6294596d":"markdown","4129c815":"markdown","14406195":"markdown","d2d80bf2":"markdown","e1792fb6":"markdown","1956914f":"markdown"},"source":{"c416c61c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\n%matplotlib inline\nimport os\nimport re\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3483fcee":"df = pd.read_json('..\/input\/Sarcasm_Headlines_Dataset.json', lines=True)\ndf.head()\n","0f4160dc":"df = df[['headline','is_sarcastic']]\ndf.head()","5ef61ee9":"sns.countplot(df.is_sarcastic)\nplt.xlabel('Label')\nplt.title('Sarcasm vs Non-sarcasm')\n","7c933da3":"df['headline'] = df['headline'].apply(lambda x: x.lower())\ndf['headline'] = df['headline'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n","9d39c714":"for idx,row in df.iterrows():\n    row[0] = row[0].replace('rt',' ')\n    \nmax_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(df['headline'].values)\nX = tokenizer.texts_to_sequences(df['headline'].values)\nX = pad_sequences(X)","23eeb6d2":"Y = pd.get_dummies(df['is_sarcastic']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","a20eb08e":"embed_dim = 128\nlstm_out = 196\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])","b95ebdb8":"from IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nSVG(model_to_dot(model).create(prog='dot', format='svg'))","e82ba22a":"batch_size = 32\nhistory = model.fit(X_train, Y_train, epochs = 25, batch_size=batch_size, verbose = 2)","dc221f3d":"validation_size = 1500\n\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","0147341e":"# summarize history for accuracy\nplt.plot(history.history['acc'])\n#plt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.savefig('model_accuracy.png')\n# summarize history for loss\nplt.plot(history.history['loss'])\n#plt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.savefig('model_loss.png')","f81f260b":"pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\nfor x in range(len(X_validate)):\n    \n    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n   \n    if np.argmax(result) == np.argmax(Y_validate[x]):\n        if np.argmax(Y_validate[x]) == 0:\n            neg_correct += 1\n        else:\n            pos_correct += 1\n       \n    if np.argmax(Y_validate[x]) == 0:\n        neg_cnt += 1\n    else:\n        pos_cnt += 1\n\n\n\nprint(\"Sarcasm_acc\", pos_correct\/pos_cnt*100, \"%\")\nprint(\"Non-Sarcasm_acc\", neg_correct\/neg_cnt*100, \"%\")","8f26a69f":"headline = ['Chowkidar hi chor hai']\nheadline = tokenizer.texts_to_sequences(headline)\nheadline = pad_sequences(headline, maxlen=29, dtype='int32', value=0)\n\nsentiment = model.predict(headline,batch_size=1,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"Non-sarcastic\")\nelif (np.argmax(sentiment) == 1):\n    print(\"Sarcasm\")","1edae6b8":"#Save the model\n# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")","1da5750d":"Pretty decent for a begginner huh?\n\nLet me try it on a popular headline","6ac17587":"### Splitting data to train & test","fe14c780":"### A graph of accuracy & Loss vs Epoch","65df9b9e":"- ### Checking for normalization","eeb356d3":"- ### Getting rid of unwanted data","ebb63a31":"# Data Pre-Processing\n- Getting rid of unwanted data\n- Checking up if the data is normalized or not\n- Filtering Headlines\n- Defining max features\n- Vectorize & Convert text for input\n- Splitting data to train & test","6294596d":"- ### Filtering headlines \n- So that only valid headlines remain & ","4129c815":"# Defining the LSTM RNN Model\nWith softmax activation","14406195":"Let's check the accuracy and score now","d2d80bf2":"#### Let's check it's score on sarcastic & non-sarcastic headlines respectively","e1792fb6":"#### Training the model","1956914f":"- ### Defining Max features \n- ### using Tokenizer to vectorize and convert text into Sequences \n so that the Network can deal with it as input"}}