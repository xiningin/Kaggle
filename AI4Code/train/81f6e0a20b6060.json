{"cell_type":{"40f1ef03":"code","07ca98c4":"code","78918db1":"code","affad201":"code","9869a93a":"code","1daac918":"code","2d43a0ba":"code","93718c21":"code","b2cb9b72":"code","62e69647":"code","7c424ab6":"code","b15219a8":"code","d865d897":"code","8c650552":"code","38244a94":"code","573817bd":"code","7b370627":"code","0ea98d81":"code","c37f80ad":"code","6a83d97d":"code","29bb0236":"code","2615e414":"code","d0d31d96":"code","1119c965":"code","b4bc96dd":"code","fe92b488":"code","3255e233":"code","45d60ad2":"code","5ce5b071":"code","0c18e4b7":"code","f27aa0b8":"code","d8999b91":"code","634a02cc":"code","bb9c9b15":"code","5c74ba01":"code","d34cb2ce":"code","91316f0d":"code","858ef639":"code","a07e167c":"code","4dd5ae12":"code","586debb7":"code","0e1a3a24":"code","c2e0437f":"code","8e39840e":"code","248eeda4":"code","14854f36":"code","037cdd0e":"code","5b67cef1":"code","9edd10af":"code","7c35e054":"code","df89ef64":"code","dc3a0cc0":"code","330c134c":"code","2be20cd8":"code","6c9bfbfb":"code","813910c6":"code","b8d403ac":"code","0394e438":"code","6a245157":"code","c83b9e05":"code","a4409435":"code","85835b92":"code","53c8237a":"code","9b6910b8":"code","c55c67db":"code","51decc92":"code","df4bd885":"code","eff4f11e":"code","fabb43b7":"code","5cf292a2":"markdown","6ffffad6":"markdown","921e9592":"markdown","85d54d67":"markdown","b64ff5f8":"markdown","5baf0d86":"markdown","4d92ff95":"markdown","d1eb5513":"markdown","c1fb1e7f":"markdown"},"source":{"40f1ef03":"pip install category-encoders","07ca98c4":"pip install optuna","78918db1":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom category_encoders import TargetEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler\nimport optuna   \nfrom sklearn.model_selection import train_test_split\nimport sklearn\nimport xgboost as xgb \nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom scipy import stats as s","affad201":"#define the train and test path\ntrain_path=\"..\/input\/jobathon-analytics-vidhya-health-insurance\/Train.csv\"\ntest_path=\"..\/input\/jobathon-analytics-vidhya-health-insurance\/Test.csv\"","9869a93a":"#read the train data\ntrain_data=pd.read_csv(train_path)\ntrain_data.head()","1daac918":"#read the test data\ntest_data=pd.read_csv(test_path)\ntest_data.head()","2d43a0ba":"#check for the numerical and categorical columns\nnumerical_columns=[]\ncategorical_columns=[]\nfor col in train_data.columns:\n    if train_data[col].dtype=='O':\n        categorical_columns.append(col)\n    else:\n        numerical_columns.append(col)","93718c21":"train_data[numerical_columns].describe()","b2cb9b72":"#checking the missing values count in each columns\nfor col in train_data.columns:\n  print(col+\" : \"+str(train_data[col].isna().sum()))","62e69647":"#checking the unique values count in each columns\nfor col in train_data.columns:\n  print(col+\" : \"+str(train_data[col].nunique()))","7c424ab6":"#set the palette for seaborn library\nsns.set_palette(\"hls\", 8)","b15219a8":"sns.countplot(train_data[\"Response\"])\nplt.title('Target Count {0: No Interest, 1: Interested}',weight='bold', fontsize=10)","d865d897":"train_data.Response.value_counts()","8c650552":"sns.scatterplot(x=train_data['Upper_Age'],y=train_data['Reco_Policy_Premium'])\nplt.title('Upper_Age VS Reco_Policy_Premium',weight='bold', fontsize=10)","38244a94":"sns.scatterplot(x=train_data['Lower_Age'],y=train_data['Reco_Policy_Premium'])\nplt.title('Lower_Age VS Reco_Policy_Premium',weight='bold', fontsize=10)","573817bd":"def count_plot(variable):\n    # get feature\n    var = train_data[variable]\n    varValue = var.value_counts()\n    # visualize\n    plt.figure(figsize = (15,3))\n    sns.countplot(var)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable,weight='bold', fontsize=10)\n    plt.show()\n    print('\\n')\n    print(\"{}: \\n {}\".format(variable,varValue))","7b370627":"for i in categorical_columns:\n    count_plot(i)","0ea98d81":"sns.heatmap(train_data[numerical_columns].corr(), annot = True, fmt = \".2f\")\nplt.show()","c37f80ad":"g = sns.FacetGrid(train_data, col = \"Response\")\ng.map(sns.distplot, \"Upper_Age\", bins = 25)\nplt.show()","6a83d97d":"g = sns.FacetGrid(train_data, col = \"Response\")\ng.map(sns.distplot, \"Lower_Age\", bins = 25)\nplt.show()","29bb0236":"#Filling the Holding_policy_type column with 0\ntrain_data[\"Holding_Policy_Type\"]= train_data[\"Holding_Policy_Type\"].fillna(0)\ntest_data[\"Holding_Policy_Type\"]= test_data[\"Holding_Policy_Type\"].fillna(0)","2615e414":"#Filling the Health Indicator column with X0\ntrain_data[\"Health Indicator\"]= train_data[\"Health Indicator\"].fillna('X0')\ntest_data[\"Health Indicator\"]= test_data[\"Health Indicator\"].fillna('X0')","d0d31d96":"#Filling the Holding_Policy_Duration column with 0\ntrain_data[\"Holding_Policy_Duration\"]= train_data[\"Holding_Policy_Duration\"].fillna(0)\ntest_data[\"Holding_Policy_Duration\"]= test_data[\"Holding_Policy_Duration\"].fillna(0)","1119c965":"#checking for any more nan values in train set\ntrain_data.isna().sum()","b4bc96dd":"#checking for any more nan values in test set\ntest_data.isna().sum()","fe92b488":"#drop the ID columns from both Train and Test set as it wont have any dependency on the target variable\ntrain_data.drop(\"ID\",axis=1,inplace=True)\ntest_data.drop(\"ID\",axis=1,inplace=True)","3255e233":"#creating a feature indicating that the customer is new\ntrain_data[\"new_cust\"]=np.where(np.logical_and((train_data[\"Holding_Policy_Duration\"]==0),(train_data[\"Holding_Policy_Type\"]==0)),1,0)\ntest_data[\"new_cust\"]=np.where(np.logical_and((test_data[\"Holding_Policy_Duration\"]==0),(test_data[\"Holding_Policy_Type\"]==0)),1,0)","45d60ad2":"#Removing C in the City code and making this feature as a category \ntrain_data['City_Code'] = train_data['City_Code'].apply(lambda x: x[1:])\ntrain_data['City_Code'] = train_data['City_Code'].astype('category')\ntrain_data['City_Code'] = train_data['City_Code'].cat.codes\ntest_data['City_Code'] = test_data['City_Code'].apply(lambda x: x[1:])\ntest_data['City_Code'] = test_data['City_Code'].astype('category')\ntest_data['City_Code'] = test_data['City_Code'].cat.codes","5ce5b071":"#Create Accom_Is combining accomodation type and is spouse to find how many married couples have rented and owned property\ntrain_data[\"Accom_Is\"]=train_data[\"Accomodation_Type\"]+\"_\"+train_data[\"Is_Spouse\"]\ntest_data[\"Accom_Is\"]=test_data[\"Accomodation_Type\"]+\"_\"+test_data[\"Is_Spouse\"]","0c18e4b7":"#Create Reco_Accom indicating the combination of joint account holders having owned and rented property\ntrain_data[\"Reco_Accom\"]=train_data[\"Accomodation_Type\"]+\"_\"+train_data[\"Reco_Insurance_Type\"]\ntest_data[\"Reco_Accom\"]=test_data[\"Accomodation_Type\"]+\"_\"+test_data[\"Reco_Insurance_Type\"]","f27aa0b8":"#Create Reco_IsSpouse indicating the combination of account holders with spouse having owned and rented property\ntrain_data[\"Reco_IsSpouse\"]=train_data[\"Is_Spouse\"]+\"_\"+train_data[\"Reco_Insurance_Type\"]\ntest_data[\"Reco_IsSpouse\"]=test_data[\"Is_Spouse\"]+\"_\"+test_data[\"Reco_Insurance_Type\"]","d8999b91":"#Create the Age difference feature indicating difference between upper and lower Age\ntrain_data[\"Age_diff\"]=train_data[\"Upper_Age\"]-train_data[\"Lower_Age\"]\ntest_data[\"Age_diff\"]=test_data[\"Upper_Age\"]-test_data[\"Lower_Age\"]\n#Create the family feature indicating there are more than one member in the family if the age difference is above 0\ntrain_data[\"Family\"]=np.where(train_data[\"Age_diff\"]>0,1,0)\ntest_data[\"Family\"]=np.where(test_data[\"Age_diff\"]>0,1,0)","634a02cc":"#one hot encoding the columns with two unique values\ntrain_data[\"Accomodation_Type\"]=np.where(train_data[\"Accomodation_Type\"]==\"Owned\",1,0)\ntest_data[\"Accomodation_Type\"]=np.where(test_data[\"Accomodation_Type\"]==\"Owned\",1,0)\ntrain_data[\"Reco_Insurance_Type\"]=np.where(train_data[\"Reco_Insurance_Type\"]==\"Joint\",1,0)\ntest_data[\"Reco_Insurance_Type\"]=np.where(test_data[\"Reco_Insurance_Type\"]==\"Joint\",1,0)\ntrain_data[\"Is_Spouse\"]=np.where(train_data[\"Is_Spouse\"]==\"Yes\",1,0)\ntest_data[\"Is_Spouse\"]=np.where(test_data[\"Is_Spouse\"]==\"Yes\",1,0)","bb9c9b15":"#Replacing '14+' in Duration feature to '15'\ntrain_data[\"Holding_Policy_Duration\"]= (train_data[\"Holding_Policy_Duration\"].replace('14+','15')).astype(float)\ntest_data[\"Holding_Policy_Duration\"]= (test_data[\"Holding_Policy_Duration\"].replace('14+','15')).astype(float)","5c74ba01":"#Taking the top 10 common city codes in the data set and one hot encoding it\ntopcity_10=train_data.City_Code.value_counts().sort_values(ascending=False).head(10).index\nlst_10=list(topcity_10)\nfor categories in lst_10:\n    train_data[\"City_code:\"+str(categories)]=np.where(train_data['City_Code']==categories,1,0)\n    test_data[\"City_code:\"+str(categories)]=np.where(test_data['City_Code']==categories,1,0)","d34cb2ce":"#Taking the top 10 common region codes in the data set and one hot encoding it\ntopregion_10=train_data.Region_Code.value_counts().sort_values(ascending=False).head(10).index\nlst_10=list(topregion_10)\nfor categories in lst_10:\n    train_data[\"Region_code:\"+str(categories)]=np.where(train_data['Region_Code']==categories,1,0)\n    test_data[\"Region_code:\"+str(categories)]=np.where(test_data['Region_Code']==categories,1,0)","91316f0d":"#One hot encoding the Holding Policy Type \ntrain_data[\"Holding_Policy_Type\"] = train_data[\"Holding_Policy_Type\"].astype(\"str\")\ntest_data[\"Holding_Policy_Type\"] = test_data[\"Holding_Policy_Type\"].astype(\"str\")\ndummies_HPT = pd.get_dummies(train_data[\"Holding_Policy_Type\"], prefix = 'HPT',drop_first=True)\ntrain_data = pd.concat([train_data, dummies_HPT], axis = 1)\ndummies_HPT = pd.get_dummies(test_data[\"Holding_Policy_Type\"], prefix = 'HPT',drop_first=True)\ntest_data = pd.concat([test_data, dummies_HPT], axis = 1)","858ef639":"#Renaming the Health Indicator column\ntrain_data.rename(columns = {'Health Indicator':'H_I'}, inplace = True) \ntest_data.rename(columns = {'Health Indicator':'H_I'}, inplace = True) ","a07e167c":"#Taking the log of Policy Premium\ntrain_data[\"log_Premium\"]=np.log(train_data[\"Reco_Policy_Premium\"])\ntest_data[\"log_Premium\"]=np.log(test_data[\"Reco_Policy_Premium\"])","4dd5ae12":"#binning the Lower and upper age to create the upper and lower age groups feature\nbins = [15,21,30,40,55,75]\nlabels = ['young','adult','middle-aged','old','very_old']\ntrain_data['Upper_age_group'] = pd.cut(train_data['Upper_Age'],bins = bins, labels = labels)\ntest_data['Upper_age_group'] = pd.cut(test_data['Upper_Age'],bins = bins, labels = labels)\ntrain_data['Lower_age_group'] = pd.cut(train_data['Lower_Age'],bins = bins, labels = labels)\ntest_data['Lower_age_group'] = pd.cut(test_data['Lower_Age'],bins = bins, labels = labels)","586debb7":"#New feature buy taking difference between the Upper and policy Duration\ntrain_data[\"Insurance_taking_Age\"]=np.where((train_data[\"new_cust\"]==1),0,train_data[\"Upper_Age\"]-train_data[\"Holding_Policy_Duration\"])\ntest_data[\"Insurance_taking_Age\"]=np.where((test_data[\"new_cust\"]==1),0,test_data[\"Upper_Age\"]-test_data[\"Holding_Policy_Duration\"])","0e1a3a24":"#Binning the Holding Duration\nlabels= [\"not_customer\",\"new_customer\",\"old_customer\",\"valued_customer\",\"Very_valued_customer\"]\nbins=[-1.0,0.0,5.0,10.0,14.0,15.0]\ntrain_data[\"Cust_Acq\"] = pd.cut(train_data[\"Holding_Policy_Duration\"] , labels= labels, bins=bins)\ntest_data[\"Cust_Acq\"] = pd.cut(test_data[\"Holding_Policy_Duration\"] , labels= labels, bins=bins)","c2e0437f":"#Converting the features to str\ntrain_data['Upper_age_group']=train_data['Upper_age_group'].astype(str)\ntest_data['Upper_age_group']=test_data['Upper_age_group'].astype(str)\ntrain_data['Lower_age_group']=train_data['Lower_age_group'].astype(str)\ntest_data['Lower_age_group']=test_data['Lower_age_group'].astype(str)\ntrain_data[\"Cust_Acq\"]=train_data[\"Cust_Acq\"].astype(str)\ntest_data[\"Cust_Acq\"]=test_data[\"Cust_Acq\"].astype(str)","8e39840e":"#Binning the Policy Premium\nlabels= [\"below-10k\",\"10k-30k\",\"above-30k\"]\nbins=[2000,10000,30000,50000]\ntrain_data[\"Premium_category\"] = pd.cut(train_data[\"Reco_Policy_Premium\"] , labels= labels, bins=bins)\ntest_data[\"Premium_category\"] = pd.cut(test_data[\"Reco_Policy_Premium\"] , labels= labels, bins=bins)","248eeda4":"#One hot encoding the Premium category\ndummies_HPT = pd.get_dummies(train_data[\"Premium_category\"], prefix = 'PRE_CAT',drop_first=True)\ntrain_data = pd.concat([train_data, dummies_HPT], axis = 1)\ndummies_HPT = pd.get_dummies(test_data[\"Premium_category\"], prefix = 'PRE_CAT',drop_first=True)\ntest_data = pd.concat([test_data, dummies_HPT], axis = 1)\ntrain_data.drop(\"Premium_category\",axis=1,inplace=True)\ntest_data.drop(\"Premium_category\",axis=1,inplace=True)","14854f36":"#Target Encoding the Region and City Code\nte = TargetEncoder()\ntrain_data['Region_Code_encoding'] = te.fit_transform(train_data['Region_Code'].astype(str), train_data['Response'])\ntest_data['Region_Code_encoding'] = te.transform(test_data['Region_Code'].astype(str))\nte = TargetEncoder()\ntrain_data['City_Code_encoding'] = te.fit_transform(train_data['City_Code'].astype(str), train_data['Response'])\ntest_data['City_Code_encoding'] = te.transform(test_data['City_Code'].astype(str))","037cdd0e":"#Removing the Type,City and region codes columns\ntrain_data.drop([\"Holding_Policy_Type\",\"Region_Code\",\"City_Code\"],axis=1,inplace=True)\ntest_data.drop([\"Holding_Policy_Type\",\"Region_Code\",\"City_Code\"],axis=1,inplace=True)","5b67cef1":"#selecting the categorical features\ncategoricals_features=[]\nfor col in train_data:\n  if train_data[col].dtypes==\"O\":\n    categoricals_features.append(col)","9edd10af":"print(categoricals_features)","7c35e054":"#Label Encoding the Categorical variables\n\nprint('Transform all String features to category.\\n')\nfor usecol in categoricals_features:\n    colcount = train_data[usecol].value_counts().index[0]\n    train_data[usecol] = train_data[usecol].fillna(colcount)\n    test_data[usecol]  = test_data[usecol].fillna(colcount)\n    \n    train_data[usecol] = train_data[usecol].astype('str')\n    test_data[usecol] = test_data[usecol].astype('str')\n    \n    #Fit LabelEncoder\n    le = LabelEncoder().fit(\n            np.unique(train_data[usecol].unique().tolist()+\n                      test_data[usecol].unique().tolist()))\n\n    #At the end 0 will be used for dropped values\n    train_data[usecol] = le.transform(train_data[usecol])+1\n    test_data[usecol]  = le.transform(test_data[usecol])+1\n    \n    train_data[usecol] = train_data[usecol].replace(np.nan, -1).astype('int')\n    test_data[usecol]  = test_data[usecol].replace(np.nan , -1).astype('int')","df89ef64":"train_data.head()","dc3a0cc0":"#Separating the target variable\nX= train_data.loc[:, train_data.columns != 'Response']\ny = train_data.loc[:, train_data.columns == \"Response\"]","330c134c":"X.shape,test_data.shape","2be20cd8":"#Upsampling using the SMOTE\nprint(\"Before UpSampling, counts of label '1': {}\".format(sum(y.Response==1)))\nprint(\"Before UpSampling, counts of label '0': {} \\n\".format(sum(y.Response==0)))\n\nsm = SMOTE(sampling_strategy = 1 ,k_neighbors = 5, random_state=1)   \nX_smote, y = sm.fit_resample(X, y)\n\n\nprint(\"After UpSampling, counts of label '1': {}\".format(sum(y.Response==1)))\nprint(\"After UpSampling, counts of label '0': {} \\n\".format(sum(y.Response==0)))","6c9bfbfb":"#scaling using standard scaler\nsc = StandardScaler()\nX_smote = sc.fit_transform(X_smote)\ntest_data = sc.transform(test_data)","813910c6":"X_smote = pd.DataFrame(X_smote,columns=X.columns)\ntest_data=pd.DataFrame(test_data,columns=X.columns)","b8d403ac":"X_smote.head()","0394e438":"test_data.head()","6a245157":"X_smote.drop([\"City_Code_encoding\",\"Region_Code_encoding\",],axis=1,inplace=True)\ntest_data.drop([\"City_Code_encoding\",\"Region_Code_encoding\"],axis=1,inplace=True)","c83b9e05":"x_train, x_valid, y_train, y_valid = train_test_split(X_smote, y, test_size=0.2, random_state = 42)","a4409435":"dtrain = xgb.DMatrix(x_train, label=y_train)\ndvalid = xgb.DMatrix(x_valid, label=y_valid)","85835b92":"def objective(trial):\n    \n    # params specifies the XGBoost hyperparameters to be tuned\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 150, 3000),\n        'max_depth': trial.suggest_int('max_depth', 10, 20),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.01, .1),\n        'subsample': trial.suggest_uniform('subsample', 0.50, 1),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.50, 1),\n        'gamma': trial.suggest_int('gamma', 0, 10),\n        'tree_method': 'gpu_hist',  \n        'objective': 'binary:logistic'\n    }\n    \n    bst = xgb.train(params, dtrain)\n    preds = bst.predict(dvalid)\n\n    pred_labels = np.rint(preds)\n    # trials will be evaluated based on their accuracy on the test set\n    accuracy = sklearn.metrics.accuracy_score(y_valid, pred_labels)\n    return accuracy","53c8237a":"#hyperparameter tuning using optuna\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective,n_trials=10) ","9b6910b8":"print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))","c55c67db":"best_params = study.best_trial.params\nbest_params['tree_method'] = 'gpu_hist'      #gpu_hist is really fast\nbest_params['objective'] = 'binary:logistic'","51decc92":"#training the model in 5 fold and 2 repeats\nN_FOLDS = 5\nN_REPEAT = 2\n\ndef training(n_repeat = 1, n_folds = 5):\n    models = []\n    F1_scores = []\n    kfold = StratifiedKFold(n_folds, shuffle = True)\n    \n    for fold, (train_index, test_index) in enumerate(kfold.split(X_smote,y), 1):\n        print('-'*85)\n        print(f'Repeat {n_repeat}, Fold {fold}')\n        \n        X_train = X_smote.values[train_index]\n        y_train = y.values[train_index].ravel()\n        X_test = X_smote.values[test_index]\n        y_test = y.values[test_index].ravel()\n        \n        model = xgb.XGBClassifier(**best_params)\n        model.fit(X_train, y_train)\n        \n        y_pred = model.predict(X_test)\n        f1 = roc_auc_score(y_test,model.predict_proba(X_test)[:,1])\n        print(f'AUC: {f1}')\n        print(classification_report(y_test, y_pred, labels=[0,1]))\n        \n        models.append(model)\n        F1_scores.append(f1)\n    return models, np.mean(F1_scores)\n\nmodels = []\nmean_f1s = []\n\nfor i in range(1, N_REPEAT+1):\n    m, f = training(i, N_FOLDS)\n    print('-'*85)\n    models = models + m\n    mean_f1s.append(f)","df4bd885":"X_smote.shape,test_data.shape","eff4f11e":"#prediction of 10 models and taking the mean\npred = np.array([])\npred1=models[0].predict_proba(test_data.values)[:,1]\npred2=models[1].predict_proba(test_data.values)[:,1]\npred3=models[2].predict_proba(test_data.values)[:,1]\npred4=models[3].predict_proba(test_data.values)[:,1]\npred5=models[4].predict_proba(test_data.values)[:,1]\npred6=models[5].predict_proba(test_data.values)[:,1]\npred7=models[6].predict_proba(test_data.values)[:,1]\npred8=models[7].predict_proba(test_data.values)[:,1]\npred9=models[8].predict_proba(test_data.values)[:,1]\npred10=models[9].predict_proba(test_data.values)[:,1]\n\n\nfor i in range(0,len(test_data.values)):\n    pred = np.append(pred, np.mean([pred1[i], pred2[i], pred3[i], pred4[i], pred5[i],\n                                   pred6[i], pred7[i], pred8[i], pred9[i], pred10[i]\n                                  ]))","fabb43b7":"#creating the submission file\ntest_new = pd.read_csv(test_path)\npd.DataFrame({\n    'ID': test_new['ID'].values,\n    'Response': pred\n             }).to_csv('Final_submission.csv',index=False)","5cf292a2":"# **Model Building**","6ffffad6":"## **Importing libraries and Reading Dataset**","921e9592":"**Removing columns based on the adversarial validation**","85d54d67":"# **Scaling the Dataset**","b64ff5f8":"# **Balancing the dataset**","5baf0d86":"# **Feature Engineering**","4d92ff95":"# **EDA**","d1eb5513":"Hence, the train data is skewed we need to do oversampling to make the dataset balanced.","c1fb1e7f":"# **Filling the missing values**"}}