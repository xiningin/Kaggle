{"cell_type":{"83d0f175":"code","d9abf210":"code","6612f71a":"code","0f78e3df":"code","4f392611":"code","900e578c":"code","04f9a98a":"code","07fe853c":"code","297ba1ef":"code","b90522e6":"code","bb181f0b":"code","952fc450":"code","4743cfeb":"code","0f6bfdf4":"code","c99f073b":"code","d926afde":"code","13c355d2":"code","326c5859":"code","709376f0":"code","c366dfa4":"code","7756bd9e":"code","1931dcad":"code","dfdd6d0b":"code","1ca81dca":"code","ffc65d0f":"code","e6dc2dd4":"code","44764c55":"code","806e6407":"code","4952472f":"code","4ec68e0d":"code","f7e1ca78":"code","41365418":"code","fe541afa":"code","b960a586":"code","a9ec67fb":"code","91e06f7a":"code","2581c389":"code","1612658c":"code","9ad75ca1":"code","79ec9323":"code","ef47fa38":"code","f2140a2a":"code","62f1ff81":"code","7bfd521a":"code","22c99aa2":"markdown","845bd1c0":"markdown","0b13b2f6":"markdown","c51da67a":"markdown","36d374b3":"markdown","92766486":"markdown","6e0125c6":"markdown","d46c9998":"markdown","56435706":"markdown","12a04613":"markdown","c9ed0481":"markdown","c6e86f19":"markdown","4745c659":"markdown","99681b24":"markdown","03a5571e":"markdown","84371dda":"markdown","6f9e5ad2":"markdown","b04820bf":"markdown","3255f1e8":"markdown","b1441219":"markdown"},"source":{"83d0f175":"\"\"\"Importing libraries\"\"\"\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport time\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nimport xgboost as xgb\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.metrics import f1_score\n\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping","d9abf210":"\"\"\"Let's load the data files\"\"\"\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","6612f71a":"\"\"\"Reading train data\"\"\"\nprint(train.shape)\ntrain.head()","0f78e3df":"\"\"\"Reading test data\"\"\"\nprint(test.shape)\ntest.head()","4f392611":"\"\"\"reading submission file\"\"\"\nsub.head()","900e578c":"xtrain, xvalid, ytrain, yvalid = train_test_split(train.text, \n                                                  train.target,\n                                                  random_state=42, \n                                                  test_size=0.1, shuffle=True)","04f9a98a":"print(xtrain.shape)\nprint(xvalid.shape)","07fe853c":"%%time\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\n# Applying the cleaning function to both test and training datasets\nxtrain = xtrain.apply(lambda x: clean_text(x))\nxvalid = xvalid.apply(lambda x: clean_text(x))\nxtrain.head(3)","297ba1ef":"%%time\ntokenizer1 = nltk.tokenize.WhitespaceTokenizer()\ntokenizer2 = nltk.tokenize.TreebankWordTokenizer()\ntokenizer3 = nltk.tokenize.WordPunctTokenizer()\ntokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntokenizer5 = nltk.tokenize.TweetTokenizer()\n\n# appling tokenizer5\nxtrain = xtrain.apply(lambda x: tokenizer5.tokenize(x))\nxvalid = xvalid.apply(lambda x: tokenizer5.tokenize(x))\nxtrain.head(3)","b90522e6":"%%time\ndef remove_stopwords(text):\n    \"\"\"\n    Removing stopwords belonging to english language\n    \n    \"\"\"\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n\n\nxtrain = xtrain.apply(lambda x : remove_stopwords(x))\nxvalid = xvalid.apply(lambda x : remove_stopwords(x))","bb181f0b":"%%time\ndef combine_text(list_of_text):\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\nxtrain = xtrain.apply(lambda x : combine_text(x))\nxvalid = xvalid.apply(lambda x : combine_text(x))","952fc450":"# Stemmer\nstemmer = nltk.stem.PorterStemmer()\n\n# Lemmatizer\nlemmatizer=nltk.stem.WordNetLemmatizer()\n\n# Appling Lemmatizer\nxtrain = xtrain.apply(lambda x: lemmatizer.lemmatize(x))\nxvalid = xvalid.apply(lambda x: lemmatizer.lemmatize(x))","4743cfeb":"# Appling CountVectorizer()\ncount_vectorizer = CountVectorizer()\nxtrain_vectors = count_vectorizer.fit_transform(xtrain)\nxvalid_vectors = count_vectorizer.transform(xvalid)","0f6bfdf4":"# Appling TFIDF\ntfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2), norm='l2')\nxtrain_tfidf = tfidf.fit_transform(xtrain)\nxvalid_tfidf = tfidf.transform(xvalid)","c99f073b":"# Fitting a simple Logistic Regression on TFIDF\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_tfidf, ytrain)\n#scores = model_selection.cross_val_score(clf, train_tfidf, ytrain, cv=5, scoring=\"f1\")\n\npredictions = clf.predict(xvalid_tfidf)\nprint('simple Logistic Regression on TFIDF')\nprint (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))","d926afde":"# Fitting a simple Logistic Regression on CountVec\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_vectors, ytrain)\n#scores = model_selection.cross_val_score(clf, xtrain_vectors, ytrain_vectors, cv=5, scoring=\"f1\")\n\npredictions = clf.predict(xvalid_vectors)\nprint('simple Logistic Regression on CountVectorizer')\nprint (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))","13c355d2":"# Fitting a LinearSVC on TFIDF\nclf = SVC()\nclf.fit(xtrain_tfidf, ytrain)\n\npredictions = clf.predict(xvalid_tfidf)\nprint('SVC on TFIDF')\nprint (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))","326c5859":"# Fitting a LinearSVC on CountVec\nclf = SVC()\nclf.fit(xtrain_vectors, ytrain)\n\npredictions = clf.predict(xvalid_vectors)\nprint('SVC on CountVectorizer')\nprint (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))","709376f0":"# Fitting a MultinomialNB on TFIDF\nclf = MultinomialNB()\nclf.fit(xtrain_tfidf, ytrain)\n\npredictions = clf.predict(xvalid_tfidf)\nprint('MultinomialNB on TFIDF')\nprint (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))","c366dfa4":"# Fitting a MultinomialNB on CountVec\nclf = MultinomialNB()\nclf.fit(xtrain_vectors, ytrain)\n\npredictions = clf.predict(xvalid_vectors)\nprint('MultinomialNB on CountVectorizer')\nprint (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))","7756bd9e":"# Fitting a simple xgboost on TFIDF\nclf = xgb.XGBClassifier(max_depth=5, n_estimators=300, colsample_bytree=0.8, \n                        subsample=0.5, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_tfidf.tocsc(), ytrain)\npredictions = clf.predict(xvalid_tfidf.tocsc())\n\nprint('XGBClassifier on TFIDF')\nprint (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))","1931dcad":"# Fitting a simple xgboost on CountVec\nclf = xgb.XGBClassifier(max_depth=5, n_estimators=300, colsample_bytree=0.8, \n                        subsample=0.5, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_vectors, ytrain)\n\npredictions = clf.predict(xvalid_vectors)\nprint('XGBClassifier on CountVectorizer')\nprint (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))","dfdd6d0b":"'''Create a function to tune hyperparameters of the selected models.'''\nseed = 44\ndef grid_search_cv(model, params):\n    global best_params, best_score\n    grid_search = GridSearchCV(estimator = model, param_grid = params, cv = 5, \n                             verbose = 3,\n                             scoring = 'f1', n_jobs = -1)\n    grid_search.fit(xtrain_vectors, ytrain)\n    best_params = grid_search.best_params_\n    best_score = grid_search.best_score_\n    \n    return best_params, best_score","1ca81dca":"'''Define hyperparameters of Logistic Regression.'''\nLR_model = LogisticRegression()\n\nLR_params = {'penalty':['l1', 'l2'],\n             'C': np.logspace(0.1, 1, 4, 8 ,10)}\n\ngrid_search_cv(LR_model, LR_params)\nLR_best_params, LR_best_score = best_params, best_score\nprint('LR best params:{} & best_score:{:0.5f}' .format(LR_best_params, LR_best_score))","ffc65d0f":"'''Define hyperparameters of Logistic Regression.'''\nSVC_model = SVC()\n\nSVC_params = {'kernel':[ 'linear', 'rbf', 'sigmoid'],\n             'C': np.logspace(0.1, 1,10)}\n\ngrid_search_cv(SVC_model, SVC_params)\nSVC_best_params, SVC_best_score = best_params, best_score\nprint('SVC best params:{} & best_score:{:0.5f}' .format(SVC_best_params, SVC_best_score))","e6dc2dd4":"'''Define hyperparameters of Logistic Regression.'''\nNB_model = MultinomialNB()\n\nNB_params = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n\ngrid_search_cv(NB_model, NB_params)\nNB_best_params, NB_best_score = best_params, best_score\nprint('NB best params:{} & best_score:{:0.5f}' .format(NB_best_params, NB_best_score))","44764c55":"#'''For XGBC, the following hyperparameters are usually tunned.'''\n#'''https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html'''\n\n#XGB_model = XGBClassifier(\n#            n_estimators=500,\n#            verbose = True)\n\n\n#XGB_params = {'max_depth': (2, 5),\n#               'reg_alpha':  (0.01, 0.4),\n#               'reg_lambda': (0.01, 0.4),\n#               'learning_rate': (0.1, 0.4),\n#               'colsample_bytree': (0.3, 1),\n#               'gamma': (0.01, 0.7),\n#               'num_leaves': (2, 5),\n#               'min_child_samples': (1, 5),\n#              'subsample': [0.5, 0.8],\n#              'random_state':[seed]}\n\n#grid_search_cv(XGB_model, XGB_params)\n#XGB_best_params, XGB_best_score = best_params, best_score\n#print('XGB best params:{} & best_score:{:0.5f}' .format(XGB_best_params, XGB_best_score))","806e6407":"\"\"\"Load the Glove vectors in a dictionay\"\"\"\nembeddings_index={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embeddings_index[word]=vectors\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","4952472f":"\"\"\" Function Creates a normalized vector for the whole sentence\"\"\"\ndef sent2vec(s):\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stopwords.words('english')]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(200)\n    return v \/ np.sqrt((v ** 2).sum())","4ec68e0d":"# create sentence vectors using the above function for training and validation set\n# create glove features\nxtrain_glove = np.array([sent2vec(x) for x in tqdm(xtrain)])\nxvalid_glove = np.array([sent2vec(x) for x in tqdm(xvalid)])","f7e1ca78":"# Shape of data after embedding\nxtrain_glove.shape,  xvalid_glove.shape","41365418":"# Fitting a simple xgboost on glove features\nclf = xgb.XGBClassifier(max_depth=8, n_estimators=300, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_glove, ytrain)\n\npredictions = clf.predict(xvalid_glove)\nprint('XGBClassifier on GloVe featur')\nprint (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))","fe541afa":"\"\"\"scale the data before any neural net\"\"\"\nscl = preprocessing.StandardScaler()\nxtrain_glove_scl = scl.fit_transform(xtrain_glove)\nxvalid_glove_scl = scl.transform(xvalid_glove)","b960a586":"\"\"\"create a simple 2 layer sequential neural net\"\"\"\nmodel = Sequential()\n\nmodel.add(Dense(200, input_dim=200, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\n# compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","a9ec67fb":"model.fit(xtrain_glove_scl, y=ytrain, batch_size=64, \n          epochs=10, verbose=1, \n          validation_data=(xvalid_glove_scl, yvalid))","91e06f7a":"predictions = model.predict(xvalid_glove_scl)\npredictions = np.round(predictions).astype(int)\nprint('2 layer sequential neural net on GloVe Feature')\nprint (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))","2581c389":"# using keras tokenizer here\ntoken = text.Tokenizer(num_words=None)\nmax_len = 80\n\ntoken.fit_on_texts(list(xtrain) + list(xvalid))\nxtrain_seq = token.texts_to_sequences(xtrain)\nxvalid_seq = token.texts_to_sequences(xvalid)\n\n# zero pad the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n\nword_index = token.word_index\nprint('Number of unique words:',len(word_index))","1612658c":"#create an embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(word_index) + 1, 200))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","9ad75ca1":"# A simple LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     200,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1000, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1000, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","79ec9323":"# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n\nmodel.fit(xtrain_pad, y=ytrain, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid), callbacks=[earlystop])","ef47fa38":"predictions = model.predict(xvalid_pad)\npredictions = np.round(predictions).astype(int)\n\nprint('simple LSTM')\nprint (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))","f2140a2a":"# A simple LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     200,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(GRU(100, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1000, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1000, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","62f1ff81":"# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n\nmodel.fit(xtrain_pad, y=ytrain, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid), callbacks=[earlystop])","7bfd521a":"predictions = model.predict(xvalid_pad)\npredictions = np.round(predictions).astype(int)\nprint('simple GRU')\nprint (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))","22c99aa2":"Waoh!! LSTM model perform as expected. Best score till now!!.\n\n## GRU <a id=\"6.3\"><\/a>","845bd1c0":"# References <a id=\"7\"><\/a>\n1. https:\/\/www.kaggle.com\/vikassingh1996\/simple-model-feat-nlp-disaster-tweets-lb-0-80572\n2. https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n3. https:\/\/www.kaggle.com\/abhishek\/approaching-almost-any-nlp-problem-on-kaggle\n\n## Give me your feedback and if you find my kernel helpful please UPVOTE will be appreciated","0b13b2f6":"After hyperparameter tuning, we can see that countvector data give us improved result. Before our best score was **0.73515 (LR)** and now after gridsearch our score is **0.75783 (NB)**.\n\nI am not optimizing XGBoost hyperparameters becuase its  is time consuming. but you can try optimization on all the models for better score.","c51da67a":"When we compare the previous xgboost results with XGBClassifier on GloVe feature result we can see that score has been increase by 0.06. we can further improve by tuning of parameters.\n\n# Deep Learning <a id=\"6\"><\/a>\n\n## Sequential Neural Net <a id=\"6.1\"><\/a>","36d374b3":"Bad performance by svc on this dataset!!\n\n## MultinomialNB <a id=\"3.3\"><\/a>","92766486":"##  2.1 Text Cleaning <a id=\"2.1\"><\/a>","6e0125c6":"It seens like XGBoost perform worst than other! but that is not correct. I haven't done any hyperparameter optimizations yet. Let's do it.\n\n# Grid Search <a id=\"4\"><\/a>\nNow let's add Grid Search to all the models with the hopes of optimizing their hyperparameters and thus improving their accuracy. Are the default model parameters the best bet? Let's find out.","d46c9998":"## 2.4 Token normalization <a id=\"2.4\"><\/a>\nToken normalisation means converting different tokens to their base forms. This can be done either by:\n\n* Stemming : removing and replacing suffixes to get to the root form of the word, which is called the stem for instance cats - cat, wolves - wolv\n* Lemmatization : Returns the base or dictionary form of a word, which is known as the lemma\n\n[source](https:\/\/www.google.com\/search?q=not+least+but+last&oq=not+least&aqs=chrome.7.69i57j0l7.22351j0j1&sourceid=chrome&ie=UTF-8)","56435706":"# 3.Building Models <a id=\"3\"><\/a>\n\n## Logistic Regression <a id=\"3.1\"><\/a>","12a04613":"## 2.3 Remove StopWord <a id=\"2.3\"><\/a>\n[Documentation](https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/)\n","c9ed0481":"## 2.5 Transforming tokens to a vector <a id=\"2.5\"><\/a>\n[Countvectorizer Features](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html)\n\n[TFIDF Features](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html)\n\n*[Reading](http:\/\/www.tfidf.com\/)*","c6e86f19":"## LSTM <a id=\"6.2\"><\/a>\nFor LSTM modeling we need to tokensize the text data:","4745c659":"Hmm!! We just improved our first model by 0.01.\n\nBut, we can find different scores by playing with the parameters of count, tfidf and model.\n\n[about f1_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html)\n\nLet's try SVC and Naives Bayes Classifier\n\n## SVC <a id=\"3.2\"><\/a>","99681b24":"Nice!! 2 layer sequential neural net on GloVe Feature perform better than xgboost","03a5571e":"Good!! but the logistic regression on counts is still better than other ancient models we try here. Let's jump into XGBoost model.\n\n## XGBoost <a id=\"3.4\"><\/a>\n\n[Parameters](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html)","84371dda":"# 2. Data Preprocessing <a id=\"2\"><\/a>","6f9e5ad2":"# Different Approaches To NLP Problems\n\n<img src=\"https:\/\/s3.amazonaws.com\/codecademy-content\/courses\/NLP\/Natural_Language_Processing_Overview.gif\" height=\"500\" width=\"500\">\n[Image Source](https:\/\/s3.amazonaws.com\/codecademy-content\/courses\/NLP\/Natural_Language_Processing_Overview.gif)\n\n# CONTEXT \n* [Importing packages and Reading Data](#1)\n* [Data Preprocessing](#2)\n    * [Text Cleaning](#2.1)\n    * [Tokenizer](#2.2)\n    * [Remove StopWord](#2.3)\n    * [Token normalization](#2.4)\n    * [Transforming tokens to a vector](#2.5)\n* [Building Model](#3)\n    * [Logistic Regression](#3.1)\n    * [SVC](#3.2)\n    * [MultinomialNB](#3.3)\n    * [XGBoost](#3.4)\n* [Grid Search](#4)\n* [GloVe](#5)\n* [Deep Learning](#6)\n    * [Sequential Neural Net](#6.1) \n    * [LSTM](#6.2)\n    * [GRU](#6.3)\n* [References](#7)\n\n\n# Intro\nIn this notebook, I just want to try different approaches, methods and modeles to solve NLP problems. We are going to start with very basic model and feature engineering and then improve it using different other models. We will also use deep neural networks and see how its perform compare to others. Last but not least we will try emsembling.\n\n# Data\n**Each sample in the train and test set has the following information:**\n\n* The text of a tweet\n* A keyword from that tweet (although this may be blank!)\n* The location the tweet was sent from (may also be blank)\n\n# What am I predicting?\nYou are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n\n**Files**\n* train.csv - the training set\n* test.csv - the test set\n* sample_submission.csv - a sample submission file in the correct format\n**Columns**\n* id - a unique identifier for each tweet\n* text - the text of the tweet\n* location - the location the tweet was sent from (may be blank)\n* keyword - a particular keyword from the tweet (may be blank)\n* target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)","b04820bf":"# GloVe for Vectorization <a id=\"5\"><\/a>\nHere we will use GloVe pretrained corpus model to represent our words.It is available in 3 varieties :50D ,100D and 200 Dimentional.We will try 200 D here.","3255f1e8":"## 2.2 Tokenizer <a id=\"2.2\"><\/a>\n[Documentation](https:\/\/www.nltk.org\/api\/nltk.tokenize.html****)","b1441219":"# 1. Importing packages and Reading Data <a id=\"1\"><\/a>"}}