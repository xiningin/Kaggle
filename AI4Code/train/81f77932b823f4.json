{"cell_type":{"990ec4db":"code","3b8b7a50":"code","139bfce5":"code","a3d0aadb":"code","a4f1c8c1":"code","4312fab2":"code","071bd738":"code","422e0ae8":"code","c35a1192":"code","5c160d5d":"code","fdae2037":"code","5b3e9936":"code","bd2474fb":"code","6f9dc4e9":"code","8538ae38":"code","ce4bd3ff":"markdown","c4532eff":"markdown","2d476d4f":"markdown","534bcdae":"markdown","072e5295":"markdown","cf72ac4c":"markdown","76c35633":"markdown"},"source":{"990ec4db":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport os\nimport cv2\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","3b8b7a50":"import pycuda.autoinit\nimport pycuda.driver as cuda\nfrom pycuda.compiler import SourceModule","139bfce5":"base_path = '..\/input\/fruit-recognition\/train\/train\/'","a3d0aadb":"kiwis = list(os.walk(base_path + 'Kiwi'))[0][2]\nlemons = list(os.walk(base_path + 'Lemon'))[0][2]\n\nkiwis_lemons = (kiwis, list(os.walk(base_path + 'Kiwi'))[0][0]) + (lemons, list(os.walk(base_path + 'Lemon'))[0][0])\n\nlabels = np.array([[1, 0]] * len(kiwis) + [[0, 1]] * len(lemons))","a4f1c8c1":"features = []\n\nfor kiwi in kiwis_lemons[0]:\n    features.append(cv2.resize(cv2.imread(kiwis_lemons[1] + '\/' + kiwi, cv2.IMREAD_GRAYSCALE), (28, 28)))\n    \n    \nfor lemon in kiwis_lemons[2]:\n    features.append(cv2.resize(cv2.imread(kiwis_lemons[3] + '\/' + lemon, cv2.IMREAD_GRAYSCALE), (28, 28)))\n    \nfeatures = np.array(features)\nfeatures = features.reshape(len(features), 28, 28)","4312fab2":"# normalize\nfeatures = features \/ 255.0","071bd738":"X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.33, random_state=42)","422e0ae8":"forward_mod = SourceModule(\"\"\"\n__global__ void f_conv_before_act(float input[28][28], float preact[6][24][24], float weight[6][5][5])\n{\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    const int N = 5*5*6*24*24;\n\n    for (int n = N * index \/ stride; n < N * (index+1) \/ stride; ++n) {\n        int i = n;\n        const int i1 = ((i \/= 1    ) % 5);\n        const int i2 = ((i \/= 5    ) % 5);\n        const int i3 = ((i \/= 5    ) % 6);\n        const int i4 = ((i \/= 6    ) % 24);\n        const int i5 = ((i \/= 24    ) % 24);\n\n        atomicAdd(&preact[i3][i4][i5], weight[i3][i1][i2] * input[i4 + i1][i5 + i2]);\n    }\n}\n\n__global__ void f_conv_bias(float preact[6][24][24], float bias[6])\n{\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    const int N = 6*24*24;\n\n    for (int n = N * index \/ stride; n < N * (index+1) \/ stride; ++n) {\n        int i = n;\n        const int i1 = ((i \/= 1    ) % 6);\n        const int i2 = ((i \/= 6    ) % 24);\n        const int i3 = ((i \/= 24    ) % 24);\n\n        preact[i1][i2][i3] += bias[i1];\n    }\n}\n\n__device__ float sigmoid(float x)\n{\n    return 1 \/ (1 + exp(-x));\n}\n\n__global__ void activation_function(float *input, float *output, const int N)\n{\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    for (int i = N * index \/ stride; i < N * (index+1) \/ stride; ++i) {\n        output[i] = sigmoid(input[i]);\n    }\n}\n\n__global__ void f_soft_before_act(float input[6][24][24], float preact[6][6][6], float weight[1][4][4])\n{\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    const int N = 4*4*6*6*6;\n\n    for (int n = N * index \/ stride; n < N * (index+1) \/ stride; ++n) {\n        int i = n;\n        const int i1 = ((i \/= 1    ) % 4);\n        const int i2 = ((i \/= 4    ) % 4);\n        const int i3 = ((i \/= 4    ) % 6);\n        const int i4 = ((i \/= 6    ) % 6);\n        const int i5 = ((i \/= 6    ) % 6);\n\n        atomicAdd(&preact[i3][i4][i5], weight[0][i1][i2] * input[i3][i4 * 4 + i1][i5 * 4 + i2]);\n    }\n}\n\n__global__ void f_soft_bias(float preact[6][6][6], float bias[1])\n{\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    const int N = 6*6*6;\n\n    for (int n = N * index \/ stride; n < N * (index+1) \/ stride; ++n) {\n        int i = n;\n        const int i1 = ((i \/= 1    ) % 6);\n        const int i2 = ((i \/= 6    ) % 6);\n        const int i3 = ((i \/= 6    ) % 6);\n\n        preact[i1][i2][i3] += bias[0];\n    }\n}\n\n__global__ void f_final_before_act(float input[6][6][6], float preact[2], float weight[2][6][6][6])\n{\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    const int N = 2*6*6*6;\n\n    for (int n = N * index \/ stride; n < N * (index+1) \/ stride; ++n) {\n        int i = n;\n        const int i1 = ((i \/= 1    ) % 2);\n        const int i2 = ((i \/= 2    ) % 6);\n        const int i3 = ((i \/= 6    ) % 6);\n        const int i4 = ((i \/= 6    ) % 6);\n\n        atomicAdd(&preact[i1], weight[i1][i2][i3][i4] * input[i2][i3][i4]);\n    }\n}\n\n__global__ void f_final_bias(float preact[2], float bias[2])\n{\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    const int N = 2;\n\n    for (int i = N * index \/ stride; i < N * (index+1) \/ stride; ++i) {\n        preact[i] += bias[i];\n    }\n}\n\n\"\"\")","c35a1192":"conv_before_act = forward_mod.get_function(\"f_conv_before_act\")\nconv_bias = forward_mod.get_function(\"f_conv_bias\")\nactivation_function = forward_mod.get_function(\"activation_function\")\nsoft_before_act = forward_mod.get_function(\"f_soft_before_act\")\nsoft_bias = forward_mod.get_function(\"f_soft_bias\")\nfinal_before_act = forward_mod.get_function(\"f_final_before_act\")\nfinal_bias = forward_mod.get_function(\"f_final_bias\")","5c160d5d":"backprop_mod = SourceModule(\"\"\"\n__device__ float sigmoid(float x)\n{\n    return 1 \/ (1 + exp(-x));\n}\n\n__global__ void grad_final(float d_weight[2][6][6][6], float d_preact[2], float p_output[6][6][6])\n{\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    const int N = 2*6*6*6;\n\n    for (int n = N * index \/ stride; n < N * (index+1) \/ stride; ++n) {\n        int i = n;\n        const int i1 = ((i \/= 1    ) % 2);\n        const int i2 = ((i \/= 2    ) % 6);\n        const int i3 = ((i \/= 6    ) % 6);\n        const int i4 = ((i \/= 6    ) % 6);\n\n        d_weight[i1][i2][i3][i4] = d_preact[i1] * p_output[i2][i3][i4];\n    }\n}\n\nconst static float learning_rate = 1.0E-03f;\n\n__global__ void bias_final(float bias[2], float d_preact[2])\n{\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    const int N = 2;\n\n    for (int i = N * index \/ stride; i < N * (index+1) \/ stride; ++i) {\n        bias[i] += learning_rate * d_preact[i];\n    }\n}\n\n__global__ void output_soft(float d_output[6][6][6], float n_weight[2][6][6][6], float nd_preact[2])\n{\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    const int N = 2*6*6*6;\n\n    for (int n = N * index \/ stride; n < N * (index+1) \/ stride; ++n) {\n        int i = n;\n        const int i1 = ((i \/= 1    ) % 2);\n        const int i2 = ((i \/= 2    ) % 6);\n        const int i3 = ((i \/= 6    ) % 6);\n        const int i4 = ((i \/= 6    ) % 6);\n\n        atomicAdd(&d_output[i2][i3][i4], n_weight[i1][i2][i3][i4] * nd_preact[i1]);\n    }\n}\n\n__global__ void before_act_soft(float d_preact[6][6][6], float d_output[6][6][6], float preact[6][6][6])\n{\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    const int N = 6*6*6;\n\n    for (int n = N * index \/ stride; n < N * (index+1) \/ stride; ++n) {\n        int i = n;\n        const int i1 = ((i \/= 1    ) % 6);\n        const int i2 = ((i \/= 6    ) % 6);\n        const int i3 = ((i \/= 6    ) % 6);\n\n        const float o = sigmoid(preact[i1][i2][i3]);\n\n        d_preact[i1][i2][i3] = d_output[i1][i2][i3] * o * (1 - o);\n    }\n}\n\n__global__ void grad_soft(float d_weight[1][4][4], float d_preact[6][6][6], float p_output[6][24][24])\n{\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    const int N = 1*4*4*6*6*6;\n    const float d = pow(6.0f, 3.0f);\n\n    for (int n = N * index \/ stride; n < N * (index+1) \/ stride; ++n) {\n        int i = n;\n        const int i1 = ((i \/= 1    ) % 1);\n        const int i2 = ((i \/= 1    ) % 4);\n        const int i3 = ((i \/= 4    ) % 4);\n        const int i4 = ((i \/= 4    ) % 6);\n        const int i5 = ((i \/= 6    ) % 6);\n        const int i6 = ((i \/= 6    ) % 6);\n\n        atomicAdd(&d_weight[i1][i2][i3], d_preact[i4][i5][i6] * p_output[i4][i5 * 4 + i2][i6 * 4 + i3]);\n    }\n}\n\n__global__ void bias_soft(float bias[1], float d_preact[6][6][6])\n{\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    const int N = 6*6*6;\n    const float d = pow(6.0f, 3.0f);\n\n    for (int n = N * index \/ stride; n < N * (index+1) \/ stride; ++n) {\n        int i = n;\n        const int i1 = ((i \/= 1    ) % 6);\n        const int i2 = ((i \/= 6    ) % 6);\n        const int i3 = ((i \/= 6    ) % 6);\n\n        atomicAdd(&bias[0], learning_rate * d_preact[i1][i2][i3] \/ d);\n    }\n}\n\n__global__ void output_conv(float d_output[6][24][24], float n_weight[1][4][4], float nd_preact[6][6][6])\n{\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    const int N = 1*4*4*6*6*6;\n\n    for (int n = N * index \/ stride; n < N * (index+1) \/ stride; ++n) {\n        int i = n;\n        const int i1 = ((i \/= 1    ) % 1);\n        const int i2 = ((i \/= 1    ) % 4);\n        const int i3 = ((i \/= 4    ) % 4);\n        const int i4 = ((i \/= 4    ) % 6);\n        const int i5 = ((i \/= 6    ) % 6);\n        const int i6 = ((i \/= 6    ) % 6);\n\n        atomicAdd(&d_output[i4][i5 * 4 + i2][i6 * 4 + i3], n_weight[i1][i2][i3] * nd_preact[i4][i5][i6]);\n    }\n}\n\n__global__ void before_act_conv(float d_preact[6][24][24], float d_output[6][24][24], float preact[6][24][24])\n{\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    const int N = 6*24*24;\n\n    for (int n = N * index \/ stride; n < N * (index+1) \/ stride; ++n) {\n        int i = n;\n        const int i1 = ((i \/= 1    ) % 6);\n        const int i2 = ((i \/= 6    ) % 24);\n        const int i3 = ((i \/= 24    ) % 24);\n\n        const float o = sigmoid(preact[i1][i2][i3]);\n\n        d_preact[i1][i2][i3] = d_output[i1][i2][i3] * o * (1 - o);\n    }\n}\n\n__global__ void grad_conv(float d_weight[6][5][5], float d_preact[6][24][24], float p_output[28][28])\n{\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    const int N = 6*5*5*24*24;\n    const float d = pow(24.0f, 2.0f);\n\n    for (int n = N * index \/ stride; n < N * (index+1) \/ stride; ++n) {\n        int i = n;\n        const int i1 = ((i \/= 1    ) % 6);\n        const int i2 = ((i \/= 6    ) % 5);\n        const int i3 = ((i \/= 5    ) % 5);\n        const int i4 = ((i \/= 5    ) % 24);\n        const int i5 = ((i \/= 24    ) % 24);\n\n        atomicAdd(&d_weight[i1][i2][i3], d_preact[i1][i4][i5] * p_output[i4 + i2][i5 + i3] \/ d);\n    }\n}\n\n__global__ void bias_conv(float bias[6], float d_preact[6][24][24])\n{\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    const int N = 6*24*24;\n    const float d = pow(24.0f, 2.0f);\n\n    for (int n = N * index \/ stride; n < N * (index+1) \/ stride; ++n) {\n        int i = n;\n        const int i1 = ((i \/= 1    ) % 6);\n        const int i2 = ((i \/= 6    ) % 24);\n        const int i3 = ((i \/= 24    ) % 24);\n\n        atomicAdd(&bias[i1], learning_rate * d_preact[i1][i2][i3] \/ d);\n    }\n}\n\n__global__ void apply_gradient(float *output, float *grad, const int N)\n{\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    for (int i = N * index \/ stride; i < N * (index+1) \/ stride; ++i) {\n        output[i] += learning_rate * grad[i];\n    }\n}\n\n\"\"\")","fdae2037":"grad_final = backprop_mod.get_function(\"grad_final\")\nbias_final = backprop_mod.get_function(\"bias_final\")\noutput_soft = backprop_mod.get_function(\"output_soft\")\nbefore_act_soft = backprop_mod.get_function(\"before_act_soft\")\ngrad_soft = backprop_mod.get_function(\"grad_soft\")\nbias_soft = backprop_mod.get_function(\"bias_soft\")\noutput_conv = backprop_mod.get_function(\"output_conv\")\nbefore_act_conv = backprop_mod.get_function(\"before_act_conv\")\ngrad_conv = backprop_mod.get_function(\"grad_conv\")\nbias_conv = backprop_mod.get_function(\"bias_conv\")\napply_gradient = backprop_mod.get_function(\"apply_gradient\")","5b3e9936":"error_mod = SourceModule(\"\"\"\n__global__ void calc_error(float *error, float *output, float *label, const int N)\n{\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    for (int i = N * index \/ stride; i < N * (index+1) \/ stride; ++i) {\n        error[i] = label[i] - output[i];\n    }\n}\n\"\"\")","bd2474fb":"calc_error = error_mod.get_function(\"calc_error\")","6f9dc4e9":"np.random.seed(3243234242)\n\nL1_preact = np.zeros((6, 24, 24)).astype(np.float32)\nL1_weight = np.random.randn(6,5,5).astype(np.float32)\nL1_bias = np.random.randn(6).astype(np.float32)\nL1_O = np.int32(3456)\n\nL1_opt = np.zeros((6, 24, 24)).astype(np.float32)\nL2_preact = np.zeros((6, 6, 6)).astype(np.float32)\nL2_weight = np.random.randn(1, 4, 4).astype(np.float32)\nL2_bias = np.random.randn(1).astype(np.float32)\nL2_opt = np.zeros((6, 6, 6)).astype(np.float32)\nL2_O = np.int32(216)\n\nL3_preact = np.zeros((2)).astype(np.float32)\nL3_weight = np.random.randn(2,6,6,6).astype(np.float32)\nL3_bias = np.zeros((2)).astype(np.float32)\nL3_O = np.int32(2)\n\nR = np.zeros((2)).astype(np.float32)","8538ae38":"for epoch in tqdm(range(500)):\n    \n    for idx in range(len(X_train)):\n\n        x = X_train[idx].astype(np.float32)\n        y = y_train[idx].astype(np.float32)\n\n        # Forward\n        conv_before_act(cuda.In(x), cuda.InOut(L1_preact), cuda.In(L1_weight), block=(64,1,1), grid=(64,1))\n        conv_bias(cuda.In(L1_preact), cuda.InOut(L1_bias), block=(64,1,1), grid=(64,1))\n        activation_function(cuda.In(L1_preact), cuda.InOut(L1_opt), L1_O, block=(64,1,1), grid=(64,1))\n\n        soft_before_act(cuda.In(L1_opt), cuda.InOut(L2_preact), cuda.In(L2_weight), block=(64,1,1), grid=(64,1))\n        soft_bias(cuda.In(L2_preact), cuda.Out(L2_bias), block=(64,1,1), grid=(64,1))\n        activation_function(cuda.In(L2_preact), cuda.InOut(L2_opt), L2_O, block=(64,1,1), grid=(64,1))\n\n        final_before_act(cuda.In(L2_opt), cuda.InOut(L3_preact), cuda.In(L3_weight), block=(64,1,1), grid=(64,1))\n        final_bias(cuda.In(L3_preact), cuda.InOut(L3_bias), block=(64,1,1), grid=(64,1))\n        activation_function(cuda.In(L3_preact), cuda.InOut(R), L3_O, block=(64,1,1), grid=(64,1))\n\n        L3_bp_preact = np.zeros((2,)).astype(np.float32)\n        L3_bp_weight = np.zeros((2,6,6,6)).astype(np.float32)\n        L2_bp_opt = np.zeros((6,6,6)).astype(np.float32)\n        L2_bp_preact = np.zeros((6,6,6)).astype(np.float32)\n        L2_bp_weight = np.zeros((1,4,4)).astype(np.float32)\n        L1_bp_opt = np.zeros((6,24,24)).astype(np.float32)\n        L1_bp_preact = np.zeros((6,24,24)).astype(np.float32)\n        L1_bp_weight = np.zeros((6,5,5)).astype(np.float32)\n\n        # Backward\n        calc_error(cuda.Out(L3_bp_preact), cuda.In(R), cuda.In(y), np.int32(2), block=(1,1,1), grid=(2,1))\n\n        grad_final(cuda.Out(L3_bp_weight), cuda.In(L3_bp_preact), cuda.In(L2_opt), block=(64,1,1), grid=(64,1))\n        bias_final(cuda.InOut(L3_bias), cuda.In(L3_bp_preact), block=(64,1,1), grid=(64,1))\n\n        output_soft(cuda.InOut(L2_bp_opt), cuda.In(L3_weight), cuda.In(L3_bp_preact), block=(64,1,1), grid=(64,1))\n        before_act_soft(cuda.InOut(L2_bp_preact), cuda.In(L2_bp_opt), cuda.In(L2_preact), block=(64,1,1), grid=(64,1))\n        grad_soft(cuda.InOut(L2_bp_weight), cuda.In(L2_bp_preact), cuda.In(L1_opt), block=(64,1,1), grid=(64,1))\n        bias_soft(cuda.InOut(L2_bias), cuda.In(L2_bp_preact), block=(64,1,1), grid=(64,1))\n\n        output_conv(cuda.InOut(L1_bp_opt), cuda.In(L2_weight), cuda.In(L2_bp_preact), block=(64,1,1), grid=(64,1))\n        before_act_conv(cuda.InOut(L1_bp_preact), cuda.In(L1_bp_opt), cuda.In(L1_preact), block=(64,1,1), grid=(64,1))\n        grad_conv(cuda.InOut(L1_bp_weight), cuda.In(L1_bp_preact), cuda.In(x), block=(64,1,1), grid=(64,1))\n        bias_conv(cuda.InOut(L1_bias), cuda.In(L1_bp_preact), block=(64,1,1), grid=(64,1))\n\n        apply_gradient(cuda.InOut(L3_weight), cuda.In(L3_bp_weight), np.int32(216 * 2), block=(64,1,1), grid=(64,1))\n        apply_gradient(cuda.InOut(L2_weight), cuda.In(L2_bp_weight), np.int32(16 * 1), block=(64,1,1), grid=(64,1))\n        apply_gradient(cuda.InOut(L1_weight), cuda.In(L1_bp_weight), np.int32(25 * 6), block=(64,1,1), grid=(64,1))","ce4bd3ff":"# CUDA Backward","c4532eff":"# CUDA Error Function","2d476d4f":"# Prepare Data","534bcdae":"# Kiwi vs Lemon using CUDA","072e5295":"# Parameters","cf72ac4c":"# Training","76c35633":"# CUDA Forward"}}