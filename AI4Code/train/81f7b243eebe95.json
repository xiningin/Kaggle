{"cell_type":{"764ea6ee":"code","9ff508ab":"code","ed378992":"code","900b272d":"code","744db817":"code","6813060f":"code","773f49e1":"code","501eec01":"code","4c7e58ed":"code","82ab2502":"code","a0f5c425":"code","6ed0989c":"code","5bb1b732":"code","1c3fb6be":"code","a0e71e8a":"code","af2d7541":"code","b3ae94f1":"code","6cbd4948":"code","6ec7ce09":"code","0a365c5b":"code","ff373f2c":"code","19cec114":"code","51fc5419":"code","8561d210":"code","d766da52":"code","543e0d3a":"code","27322b82":"markdown","bd161b18":"markdown","22a3a17e":"markdown","82b28952":"markdown","9d708642":"markdown","c19cf4a6":"markdown","ccc0e1e2":"markdown","a544b8a8":"markdown","890d8ffa":"markdown","00e79c8a":"markdown","242cc46b":"markdown","c4e799be":"markdown","34817a64":"markdown","272a4044":"markdown","88b33009":"markdown","1d689d32":"markdown","b2f2285c":"markdown","43b86174":"markdown","0771b776":"markdown","0848f31d":"markdown","ec054197":"markdown","0e65909a":"markdown","f76eb13e":"markdown","3f162380":"markdown"},"source":{"764ea6ee":"import sys\nprint(sys.executable)","9ff508ab":"import os\nimport glob\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\n\nimport cv2\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\nprint(\"Python version used = \", sys.version)\nprint(\"Numpy version used = \", np.__version__)\nprint(\"OpenCV version used = \", cv2.__version__)\nprint(\"pydicom version used = \", pydicom.__version__)","ed378992":"train = pd.DataFrame(pd.read_csv(\"..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train.csv\"))\n\nprint(\"Shape of dataframe = \", train.shape)","900b272d":"train.head()","744db817":"train.info()","6813060f":"train.isna().sum().to_frame().rename(columns = {0 : \"NaN_count\"}).style.background_gradient(cmap = \"copper\")","773f49e1":"map_name_to_id = {\n    \"Aortic enlargement\" : 0,\n    \"Atelectasis\" : 1,\n    \"Calcification\" : 2,\n    \"Cardiomegaly\" : 3,\n    \"Consolidation\" : 4,\n    \"ILD\" : 5,\n    \"Infiltration\" : 6,\n    \"Lung Opacity\" : 7,\n    \"Nodule\/Mass\" : 8,\n    \"Other lesion\" : 9,\n    \"Pleural effusion\" : 10,\n    \"Pleural thickening\" : 11,\n    \"Pneumothorax\" : 12,\n    \"Pulmonary fibrosis\" : 13,\n    \"No Finding(healthy)\" : 14\n}","501eec01":"train.class_name.unique(), len(train.class_name.unique())","4c7e58ed":"label_count = dict()\nfor label in tqdm(train.class_id.values) : \n    if label not in label_count : \n        label_count[label] = 1\n    else:\n        label_count[label] += 1\n\nlabels = [\"Aortic Enlargement\", \"Atelectasis\", \"Calcification\", \"Cardiomegaly\", \"Consolidation\", \"ILD\", \"Infiltration\", \"Lung Opacity\", \"Nodule\/Mass\",\n         \"Other lesion\", \"Pleural effusion\", \"Pleural thickening\", \"Pneumothorax\", \"Pulmonary fibrosis\", \"No finding\"]\ncounts = [label_count[0], label_count[1], label_count[2], label_count[3], label_count[4], label_count[5], label_count[6], label_count[7], label_count[8],\n         label_count[9], label_count[10], label_count[11], label_count[12], label_count[13], label_count[14]]\nexplode = [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n\nfig, ax = plt.subplots(figsize = (20, 12))\nax.pie(counts, explode = explode, labels = labels, shadow = True, startangle = 90)\nax.axis(\"equal\")","82ab2502":"train.rad_id.unique()","a0f5c425":"r_count = dict()\nfor rad_id in train.rad_id.values : \n    if rad_id in r_count : \n        r_count[rad_id] += 1\n    else:\n        r_count[rad_id] = 1\n\nrad_ids = ['R11', 'R7', 'R10', 'R9', 'R17', 'R3', 'R8', 'R6', 'R5', 'R4', 'R2', 'R16', 'R1', 'R15', 'R13', 'R12', 'R14']\ncounts = [r_count['R11'], r_count['R7'], r_count['R10'], r_count['R9'], r_count['R17'], r_count['R3'], r_count['R8'], r_count['R6'],\n         r_count['R5'], r_count['R4'], r_count['R2'], r_count['R16'], r_count['R1'], r_count['R15'], r_count['R13'], r_count['R12'], r_count['R14']]\nexplode = [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n\nfig, ax = plt.subplots(figsize = (20, 12))\nax.pie(counts, explode = explode, labels = rad_ids, shadow = True, startangle = 90)\nax.axis(\"equal\")","6ed0989c":"def dicom2numpy(path, voi_lut = True, fix_monochrome = True) : \n    dicom = pydicom.read_file(path)\n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n    if voi_lut == True : \n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n    \n    if fix_monochrome == True and dicom.PhotometricInterpretation == \"MONOCHROME1\" : \n        data = np.amax(data) - data\n    \n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    \n    return data","5bb1b732":"sample_image = dicom2numpy(\"..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train\/000434271f63a053c4128a0ba6352c7f.dicom\")\nprint(\"Shape = \", sample_image.shape)\n\nplt.figure(figsize = (20, 12))\nplt.imshow(sample_image)\nplt.grid(False)\nplt.title(\"Sample Image\", fontsize = 16)","1c3fb6be":"plt.figure(figsize = (20, 20))\n\nplt.subplot(1,3,1)\nsample_image = dicom2numpy(\"..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train\/000434271f63a053c4128a0ba6352c7f.dicom\")\nplt.imshow(sample_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Sample Image\", fontsize = 18)\n\nplt.subplot(1,3,2)\nsample_image = dicom2numpy(\"..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train\/000434271f63a053c4128a0ba6352c7f.dicom\", voi_lut = False)\nplt.imshow(sample_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Sample Image + VOI_LUT = False\", fontsize = 18)\n\nplt.subplot(1,3,3)\nsample_image = dicom2numpy(\"..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train\/000434271f63a053c4128a0ba6352c7f.dicom\", fix_monochrome = False)\nplt.imshow(sample_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Sample Image + Fix Monochrome = False\", fontsize = 18)\n","a0e71e8a":"print(pydicom.read_file(\"..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train\/000434271f63a053c4128a0ba6352c7f.dicom\"))","af2d7541":"train_image_ids = train.image_id.unique() # one person might have multiple diseases.\nprint(\"Number of unique IDS = \", len(train_image_ids))","b3ae94f1":"rows = []\ncolumns = []\nsex = []\nfor pat_id in tqdm(train_image_ids) : \n    path = \"..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train\/\"+pat_id+\".dicom\"\n    dicom_file = pydicom.read_file(path, stop_before_pixels = True)\n    rows.append(dicom_file.Rows)\n    columns.append(dicom_file.Columns)\n    sex.append(dicom_file.PatientSex)\n\nadditional_metadata = pd.DataFrame({\n    \"image_id\" : train_image_ids,\n    \"rows\" : rows,\n    \"columns\" : columns,\n    \"sex\" : sex\n})\n\nprint(\"Shape of additional metadata frame = \", additional_metadata.shape)\nadditional_metadata.head()","6cbd4948":"male_count = len(additional_metadata[additional_metadata[\"sex\"] == \"M\"])\nfemale_count = len(additional_metadata[additional_metadata[\"sex\"] == \"F\"])\n\nprint(\"Male : Female Ratio = \", male_count \/ female_count )","6ec7ce09":"additional_metadata.sex.unique()","0a365c5b":"plt.figure(figsize = (12, 8))\nsns.countplot(additional_metadata[\"sex\"], palette = \"dark\")\nplt.grid(True)\nplt.axis('on')\nplt.title(\"Gender Count\", fontsize = 18)","ff373f2c":"plt.figure(figsize = (20, 12))\nx = additional_metadata[\"rows\"]\ny = additional_metadata[\"columns\"]\nplt.scatter(x, y, cmap = \"plasma\", label = \"Training Images\")\nplt.title(\"Shape Analysis Of Training Images\", fontsize = 18)\nplt.xlabel(\"Number Of Rows\", fontsize = 18)\nplt.ylabel(\"Number Of Columns\", fontsize = 18)\nplt.grid(True)\nplt.axis('on')\nplt.legend()","19cec114":"plt.figure(figsize = (12, 8))\nx = additional_metadata[\"rows\"]\ny = additional_metadata[\"columns\"]\nsns.distplot(x * y, kde = True, color = \"brown\")\nplt.xlabel(\"pixel count\", fontsize = 16)\nplt.title(\"Pixel Count Analysis\", fontsize = 18)\nplt.grid(True)\nplt.axis(\"on\")","51fc5419":"train.head()","8561d210":"df = train[train[\"class_id\"] != 14]\n\nimages = []\nimage_ids = df.image_id.values\nclass_ids = df.class_id.unique()\n\n# map label id to a random color(distinct for each class)\ncolor_mapping = dict()\nfor class_id in class_ids : \n    color_code = [random.randint(0, 255) for i in range(3)]\n    color_mapping[class_id] = color_code\n\nbox_thickness = 3\nscale = 4 # to scale the axes by this factor as images are real huge.\n\nfor i in tqdm(range(6)) : \n    image_id = np.random.choice(image_ids)\n    image_path = f\"..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train\/{image_id}.dicom\"\n    image = dicom2numpy(image_path)\n    image = cv2.resize(image, None, fx = 1\/scale, fy = 1\/scale)\n    \"\"\"\n    dsize is required param but if you still want the resize method to calculate the dsize for you then you may pass the param as None.\n    \"\"\"\n    image = np.stack([image, image, image], axis = -1)\n    \n    bounding_boxes = df.loc[df[\"image_id\"] == image_id, [\"x_min\", \"y_min\", \"x_max\", \"y_max\"]].values\/scale\n    \"\"\"\n    as we previously scaled the axes, it makes sense to scale these values too.\n    \"\"\"\n    labels = df.loc[df[\"image_id\"] == image_id, [\"class_id\"]].values.squeeze()\n    \n    for label_id, box in zip(labels, bounding_boxes) : \n        color = color_mapping[label_id]\n        image = cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color, box_thickness)\n    image = cv2.resize(image, (500, 500))\n    images.append(image)\n\nplt.figure(figsize = (20, 20))\nfor n in range(6) : \n    plt.subplot(3, 2, n+1)\n    annotated_image = images[n]\n    plt.imshow(annotated_image, cmap = \"gray\")\n    plt.grid(False)\n    plt.axis('off')\nplt.tight_layout()","d766da52":"def plot_selected(class_name) :\n    class_id = map_name_to_id[class_name]\n    df = train[train[\"class_id\"] == class_id]\n    images = []\n    image_ids = df.image_id.values\n    color_mapping = [random.randint(0, 255) for i in range(3)]\n    box_thickness = 3\n    scale = 4\n    \n    for i in tqdm(range(6)) :\n        image_id = np.random.choice(image_ids)\n        image_path = f\"..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train\/{image_id}.dicom\"\n        image = dicom2numpy(image_path)\n        image = cv2.resize(image, None, fx = 1\/scale, fy = 1\/scale)\n        \"\"\"\n        dsize is required param but if you still want the resize method to calculate the dsize for you then you may pass the param as None.\n        \"\"\"\n        image = np.stack([image, image, image], axis = -1)\n    \n        bounding_boxes = df.loc[df[\"image_id\"] == image_id, [\"x_min\", \"y_min\", \"x_max\", \"y_max\"]].values\/scale\n        \"\"\"\n        as we previously scaled the axes, it makes sense to scale these values too.\n        \"\"\"\n        \n        for box in bounding_boxes :\n            image = cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color_mapping, box_thickness)\n        image = cv2.resize(image, (500, 500))\n        images.append(image)\n\n    plt.figure(figsize = (20, 20))\n    for n in range(6) : \n        plt.subplot(3, 2, n+1)\n        annotated_image = images[n]\n        plt.imshow(annotated_image, cmap = \"gray\")\n        plt.title(class_name, fontsize = 16)\n        plt.grid(False)\n        plt.axis('off')\n    plt.tight_layout()   ","543e0d3a":"for class_name in map_name_to_id : \n    if class_name != \"No Finding(healthy)\" : \n        print(f\"Samples of {class_name} images\")\n        plot_selected(class_name)","27322b82":"# DICOM to Numpy Tensor : \n\nInsights taken from  : **[raddar notebook](https:\/\/www.kaggle.com\/raddar\/convert-dicom-to-np-array-the-correct-way)**\n\nRaw dicom data is not actually linearly convertable to \"human-friendly\" png\/jpg. In fact, most of DICOM's store pixel values in exponential scale.\n\nSo in order to get jpg\/png we need to apply some transformations. DICOM metadata stores information how to make such \"human-friendly\" transformations.\n\n## Fix Monochrome : \n\nRegarding fix_monochrome, we use that since .dcm images contains many shades of grey and black in it. Hence, to bring down it to the same level (normalize), we do this. It helps in getting better insights from the medical images. Also, MONOCHROME2 images have intensities inverted vs MONOCHROME1. One goes from 0=air to XXXX=bone, while the other goes from 0=bone to XXXX=air. Hence the operation `data = np.amax(data) - data` is needed.","bd161b18":"So, we have significant missing values in our dataset. Let's see the exact NaN count per feature. \n\nDifferent colormaps available can be found here : [matplotlib_colormaps](https:\/\/matplotlib.org\/3.1.0\/tutorials\/colors\/colormaps.html)","22a3a17e":"## A Note On Dataset Feature - Class ID: \n\nFollowing are the class names and ids which are used on the metadata dataframe.\n\n* 0 - Aortic enlargement\n* 1 - Atelectasis\n* 2 - Calcification\n* 3 - Cardiomegaly\n* 4 - Consolidation\n* 5 - ILD\n* 6 - Infiltration\n* 7 - Lung Opacity\n* 8 - Nodule\/Mass\n* 9 - Other lesion\n* 10 - Pleural effusion\n* 11 - Pleural thickening\n* 12 - Pneumothorax\n* 13 - Pulmonary fibrosis\n* 14 - No Finding(healthy)","82b28952":"This is quite a healthy ratio between both genders. However, before closing the book on this one, let's cross confirm whether there are other genders too, in the dicom dataset.","9d708642":"**References** : \n\n* [Building Neural Network for Medical Imaging using Deep Learning in Tensorflow (Part 1)](https:\/\/medium.com\/@verma.chandan\/building-neural-network-for-medical-imaging-using-deep-learning-in-tensorflow-part-1-ab993b7fb04f)\n* [Understanding DICOMs](https:\/\/towardsdatascience.com\/understanding-dicoms-835cd2e57d0b)\n* [DISCUSSION THREAD] : [Doubts With Dicom](https:\/\/www.kaggle.com\/c\/vinbigdata-chest-xray-abnormalities-detection\/discussion\/211855#1157539)","c19cf4a6":"Majority of images have rows in range [2500, 3000] and columns = [2000, 3000].","ccc0e1e2":"# Class Name : \n\nThough we know there are 14 + 1(no finding) classes into which images are classified, and a single image may be diagnosed with multiple diseases, hence it becomes imperative to at least check whether the labels are available altogether in one entry(*we have to separate them if that's the case*), or the entry is repeated in the dataframe, having new label corresponding to it, till all the classes it's been diagnosed with are covered.","a544b8a8":"# A Note ON DICOM File Format : \n\nDICOM stands for **Digital Imaging and Communications in Medicine**. It is a standard, internationally accepted format to view, store, retrieve and share medical images. DICOM conforms to set protocols to maintain accuracy of information relayed through medical images. \n\nAny DICOM medical image consists of two parts \u2014 **a header and the actual image itself**. \n\n![image.png](attachment:image.png)\n\n* The header consists of data that describes the image, the most important being patient data. This includes the patient\u2019s demographic information such as the patient\u2019s name, age, gender, and date of birth. \n* The header may also give information on image characteristics such as acquisition parameters, pixel intensity, matrix size, and dimensions of the image. All info in DICOM(.dcm) files are provided using **separate tags**. \n\n16 bit DICOM images have values ranging from -32768 to 32768 while 8-bit grey-scale images store values from 0 to 255. The value ranges in DICOM images are useful as they correlate with the Hounsfield Scale which is a quantitative scale for describing radio-density (or a way of viewing different tissues densities).\n\n## Hounsfield Units : \n\nThe Hounsfield Units (HU) make up the grayscale in medical CT imaging. **It is a scale from black to white of 4096 values (12 bit) and ranges from -1024 HU to 3071 HU (zero is also a value). It is defined by the following:**\n\n*-1024 HU is black and represents air (in the lungs). 0 HU represents water (since we consist mostly out of water, there is a large peak here). 3071 HU is white and represents the densest tissue in a human body, tooth enamel. All other tissues are somewhere within this scale; fat is around -100 HU, muscle around 100 HU and bone spans from 200 HU (trabecular\/spongeous bone) to about 2000 HU (cortical bone).**\n\nMetal implants typically have very high Hounsfield units. Therefore, they are attributed the maximum value in typical 12-bit CT scans (3071).\n\n## DICOM LUT : \n\n* **Modality LUT** : A \"Modality LUT \" allows the transformation of manufacturer-dependent pixel values into manufacturer-independent pixel values (e.g., Hounsfield units for CT images). \n* **VOI LUT** : A \"VOI LUT\" allows the transformation of the modality pixel values into pixel values that are meaningful for print or display. This transformation is applied after any \"Modality LUT\".","890d8ffa":"# Radiologist Contribution Imbalance Study\n\nAs we have several radiologists labeling each image, it might be helpful to know whether there is existing of an imbalance in their respective work. Let's have a look at that.","00e79c8a":"Indeed, there are. Let's have a look at the count of them all.","242cc46b":"# Bounding Box Visualization","c4e799be":"# Label Count : ","34817a64":"# Shape Analysis","272a4044":"Extracting Metadata from this.","88b33009":"Also, let's look at the metadata header of DICOM as well.","1d689d32":"# Visualize Each Class Of Disease","b2f2285c":"So, yes there is heavy imbalance when contribution of radiologists are concerned. R10, R9 and R8 collectively dominate the overall space!","43b86174":"## Image Pixel Encapsulation\n\nHigher pixel count directly corresponds to quality and size of the image. Let's have a look at that too.","0771b776":"Trying switching off VOI_LUT and FIX_MONOCHROME","0848f31d":"We are only interested in visualizing bounding boxes. However, a good fraction of the bulk is healthy. So, it seems wise to drop it for this visualization purpose, as no bounding box exists for these cases. In dataframe, all coordinates are marked by NaN, indicating this fact.","ec054197":"# <span style=\"color:Green;\"> VinBigData - Exploratory Data Analysis <\/span>\n**Book One**\n\n<blockquote> The day healthcare can fully embrace AI is the day we can have a revolution in terms of cutting costs and improving care. ~ Fei-Fei Li<\/blockquote>\n\n![image.png](attachment:image.png)","0e65909a":"So, we don't have a scenario where labels are merged like *\"Aortic enlargmenet| Lung opacity\"*, which we would have to separate otherwise.","f76eb13e":"# Gender Analysis","3f162380":"**Inference** : \n\nIn terms of skewness, it's quite a number. We can observe nearly 45%+ cases thankfully healthy(no-finding), however the diseased ones, the distribution of diseases is skewed. From overall perspective too, the imbalance is high enough to ring danger alarms! Down the line, this will have to be addressed."}}