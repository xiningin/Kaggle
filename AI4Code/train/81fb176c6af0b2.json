{"cell_type":{"30a9207b":"code","5e7baa79":"code","d536a2ab":"code","9a5e2b82":"code","04d984f6":"code","c3f75d03":"code","5d5fe1fc":"code","92d6be67":"code","9e060654":"code","b0335716":"code","18cd0515":"code","39c97e9b":"code","2b089f48":"code","159823d6":"code","c986b597":"code","732eb203":"code","e2417d9a":"code","2923aebb":"code","4f9fa21e":"code","8140686d":"code","89b9fe15":"code","d386ef9d":"code","3d6dedd1":"code","26d36c81":"code","e1ffdd5a":"code","30719c11":"code","f9b098a8":"code","5e988a8e":"code","c6401c69":"code","d373be01":"code","81e8da28":"code","8b3f9f87":"code","c5d89e9f":"code","2cb7bc28":"code","64532c98":"code","84d12f0b":"code","9e2a7703":"code","f7bc8e24":"code","553230b1":"code","997d2a6f":"code","34b70d0d":"code","95ae95d7":"code","cbc0284a":"code","e04790c6":"code","cfa124d1":"code","bf2fe169":"code","9f1d3811":"code","b1c5f6aa":"code","cd3d045f":"code","bcdba1d0":"code","81f64d33":"code","3bbed101":"code","4a54c078":"code","c518ec2b":"code","a8b9f308":"code","e37c95be":"code","125f07f7":"code","0d8edb52":"code","a57c3206":"code","d685e4ed":"code","da733e36":"code","18efa981":"code","543575c0":"code","b79375f0":"code","7aad7253":"code","334d2c16":"code","81823880":"code","5ff2e3c9":"code","7d5b7f55":"code","01a8e229":"code","0d237737":"code","c9b5189a":"code","91ddb0af":"code","97f5b2f6":"code","f3695a10":"code","7038fb5b":"code","16474b69":"code","2ae608f5":"code","45c8c6b0":"code","5978be82":"code","bf8c83a0":"code","410f0beb":"code","c3750f86":"code","25117f7f":"code","fce6731a":"code","5240ac34":"code","c90e3694":"code","68161fc7":"code","d2ebc19b":"code","2a7c5689":"code","af2f7919":"code","38fcba9d":"code","c6680e56":"code","93587003":"code","83af0d6e":"code","2d8fa3ce":"code","5ef126db":"code","b1d46091":"code","7588fa94":"code","cd2a349b":"code","3fc6a58f":"code","99854695":"code","c5452e3d":"code","ec68b247":"code","1dae86cc":"code","5279b0fa":"code","a04f6c78":"code","8a982c18":"code","d2286889":"markdown","e32ef50a":"markdown","55e6da9b":"markdown","e53fbfb0":"markdown","501c516a":"markdown","97efb481":"markdown","0f97aa4b":"markdown","a1b2f3a4":"markdown","2afac0dd":"markdown","73b07358":"markdown","02d0dd2d":"markdown","e232f8de":"markdown","d1559898":"markdown","0ad278b3":"markdown","21f5d350":"markdown","d6c0c858":"markdown","692ac10c":"markdown","0fdc0b1a":"markdown","781c1043":"markdown","9d21b229":"markdown","c460007e":"markdown","cb50123f":"markdown","a43af6e7":"markdown","8a723812":"markdown","6c0d6f92":"markdown","09f4ca75":"markdown","ef07c0b6":"markdown","c9731115":"markdown","80051e89":"markdown"},"source":{"30a9207b":"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nimport seaborn as sns \nimport scipy.stats as stats\nfrom scipy.stats import norm\n!pip install researchpy\nimport researchpy as rp\n\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n# Import libraries","5e7baa79":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndiabetes = pd.read_csv('\/kaggle\/input\/diabetes-dataset\/diabetes2.csv')\n# import diabetes dataset as a pandas dataframe","d536a2ab":"diabetes.sample(5)\n# Looking at some random samples we can look at an overview of data","9a5e2b82":"diabetes.describe()\n# Looking at the five number summary statistics of variables since they are numeric. This can be confirmed using .dtypes","04d984f6":"diabetes.dtypes\n# In correct format","c3f75d03":"diabetes.isnull().sum()\n# No missing values are present. However from the above functions we can see that Glucose, BloodPressure, SkinThickness, Insulin\n# and BMI have min values of 0 which are not real clinical values. Lets look at counts. A value of 0 for Pregnancies is a real value.","5d5fe1fc":"print(\"Number of 0's for Glucose:\", diabetes['Glucose'].isin([0]).sum())\nprint(\"Number of 0's for Blood Pressure:\", diabetes['BloodPressure'].isin([0]).sum())\nprint(\"Number of 0's for Skin Thickness:\", diabetes['SkinThickness'].isin([0]).sum())\nprint(\"Number of 0's for Insulin:\", diabetes['Insulin'].isin([0]).sum())\nprint(\"Number of 0's for BMI:\", diabetes['BMI'].isin([0]).sum())","92d6be67":"glucose = diabetes['Glucose'].isin([0]).sum() \/ 768*100\nprint(\"Percentage of missing data for Glocuse: %.2f\" % glucose)\n\nbloodpressure = diabetes['BloodPressure'].isin([0]).sum() \/ 768*100\nprint(\"Percentage of missing data for Blood Pressure: %.2f\" % bloodpressure)\n\nskinthickness = diabetes['SkinThickness'].isin([0]).sum() \/ 768*100,\nprint(\"Percentage of missing data for Skin Thickness: %.2f\" % skinthickness)\n\ninsulin = diabetes['Insulin'].isin([0]).sum() \/ 768*100\nprint(\"Percentage of missing data for Insulin: %.2f\" % insulin)\n\nbmi = diabetes['BMI'].isin([0]).sum() \/ 768*100\nprint(\"Percentage of missing data for BMI: %.2f\" % bmi)\n\n\n# We can see we are missing between 0.65%-48.6% of data for these variables, even though the isnull function showed us no \n# missing data as NaN.","9e060654":"diabetes_clean = diabetes.copy()\n# Make copy of data set before making minor wrangling operations as we don't want to edit our row data","b0335716":"diabetes_clean['Glucose'] = diabetes_clean['Glucose'].replace(0,diabetes['Glucose'].mean())\ndiabetes_clean['BloodPressure'] = diabetes_clean['BloodPressure'].replace(0,diabetes['BloodPressure'].mean())\ndiabetes_clean['SkinThickness'] = diabetes_clean['SkinThickness'].replace(0,diabetes['SkinThickness'].mean())\ndiabetes_clean['Insulin'] = diabetes_clean['Insulin'].replace(0,diabetes['Insulin'].mean())\ndiabetes_clean['BMI'] = diabetes_clean['BMI'].replace(0,diabetes['BMI'].mean())\n\n# Replacing 0 values in these columns with mean","18cd0515":"diabetes_clean['Pregnancies'].values[diabetes_clean['Pregnancies'] > 0] = 1\n# Changing number of pregancies to a binary variable. Where any pregnancy > 0 is returned a value of 1. Since no pregnancies is\n# already encoded as 0 this does not need to be changed","39c97e9b":"diabetes_clean.describe()","2b089f48":"diabetes_clean.to_csv('diabetes_clean_03042021.csv')\n# Exporting cleaned data to csv file so that it can be used in the second Jupyter Notebook","159823d6":"sns.pairplot(diabetes_clean, diag_kind='kde', hue='Outcome'); \n# Looking at a high level overview of the data separated out by outcome, i.e. 1 = diabetes and 0 = no diabetes\n# Lets look at number of patients with diabetes and some of these visuals in more detail","c986b597":"diabetes_clean['Outcome'].value_counts()","732eb203":"diabetes_clean['Pregnancies'].value_counts()","e2417d9a":"# What percentage of Hispanics identify as each race?\nPercentDiabetes = (268\/768)*100\n# 34.9 % of patients have diabtes\nPercentDiabetes","2923aebb":"base_color = sns.color_palette()[0]\nsns.countplot(data = diabetes_clean, x = 'Outcome', color = base_color);\n# Where 1 is where a patient has diabetes","4f9fa21e":"plt.figure(figsize = [10, 10])\nsns.heatmap(diabetes_clean.corr(), annot = True, fmt = '.3f', cmap = 'vlag_r', center = 0);\n# Returns a heatmap with Pearson correlation values\n# Some interesting correlations including age and number of pregencies\n# Lets look in a bit more detail using a linear regression in Seaborn","8140686d":"sns.regplot(data=diabetes_clean, x='Insulin', y='Glucose')\nplt.xlabel('Insulin')\nplt.ylabel('Glucose');\n# You can see the result of replacing values for insulin ","89b9fe15":"# Comparison of values between values and CI\n\n# Further info on CI here https:\/\/towardsdatascience.com\/a-complete-guide-to-confidence-interval-and-examples-in-python-ff417c5cb593","d386ef9d":"diabetes_clean.groupby(\"Outcome\").agg({\"Glucose\":[np.mean, np.std, np.size]})","3d6dedd1":"# Diabetes patients\nglucose_diabetes_mean = 142.159\nglucose_diabetes_std = 29.545\nglucose_diabetes_n = 268\nglucose_diabetes_se = glucose_diabetes_std \/ np.sqrt(glucose_diabetes_n) # standard error\n\nglucose_diabetes_lcb = glucose_diabetes_mean - 1.96* glucose_diabetes_se  #lower limit of the CI\nglucose_diabetes_ucb = glucose_diabetes_mean + 1.96* glucose_diabetes_se  #upper limit of the CI\n# Z = 1.96 for 95% CI\n\n\n\n# Non-diabetes patients\nglucose_nodiabetes_mean = 110.705\nglucose_nodiabetes_std = 24.715\nglucose_nodiabetes_n = 500\nglucose_nodiabetes_se = glucose_nodiabetes_std \/ np.sqrt(glucose_nodiabetes_n) # standard error\n\nglucose_nodiabetes_lcb = glucose_nodiabetes_mean - 1.96* glucose_nodiabetes_se  #lower limit of the CI\nglucose_nodiabetes_ucb = glucose_nodiabetes_mean + 1.96* glucose_nodiabetes_se  #upper limit of the CI","26d36c81":"# Diabetes patients\nprint(\"Glucose diabetes patients LCB: %.2f\" % glucose_diabetes_lcb)\nprint(\"Glucose diabetes patients UCB: %.2f\" % glucose_diabetes_ucb)\n\n# Non-diabetes patients\nprint(\"Glucose non-diabetes patients LCB: %.2f\" % glucose_nodiabetes_lcb)\nprint(\"Glucose non-diabetes patients UCB: %.2f\" % glucose_nodiabetes_ucb)","e1ffdd5a":"df_glucose = diabetes_clean[['Glucose', 'Outcome']]","30719c11":"df_glucose[\"Outcome\"].replace({1: \"Diabetes\", 0: \"NotDiabetes\"}, inplace=True)\ndf_glucose\n# Need to convert 1 and 0 in Outcome column to Diabetes and NotDiabetes respectively for further hypothesis testing","f9b098a8":"rp.ttest(group1= df_glucose['Glucose'][df_glucose['Outcome'] == 'Diabetes'], group1_name= \"Diabetes\",\n         group2= df_glucose['Glucose'][df_glucose['Outcome'] == 'NotDiabetes'], group2_name= \"NotDiabetes\")\n# independent t-test using researchpy\n# further information: https:\/\/www.pythonfordatascience.org\/independent-samples-t-test-python\/","5e988a8e":"stats.ttest_ind(df_glucose['Glucose'][df_glucose['Outcome'] == 'Diabetes'],\n                df_glucose['Glucose'][df_glucose['Outcome'] == 'NotDiabetes'])\n# independent t-test using scipy.stats","c6401c69":"mu, std = norm.fit(df_glucose['Glucose'])\nplt.figure(figsize = [12, 8])\nsns.distplot(df_glucose['Glucose'], hist=True, kde=True, \n             bins=25, color = 'b', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 2});\nplt.title(\"Fit results: $\\mu$ = %.2f,  $\\sigma$ = %.2f\" % (mu, std), size=15)\nplt.xlabel(\"Glucose\", size=15)\nplt.ylabel(\"Density\", size=15);","d373be01":"diabetes_clean.groupby(\"Outcome\").agg({\"BloodPressure\":[np.mean, np.std, np.size]})","81e8da28":"# Diabetes patients\nbloodpressure_diabetes_mean = 74.95\nbloodpressure_diabetes_std = 12.01\nbloodpressure_diabetes_n = 268\nbloodpressure_diabetes_se = bloodpressure_diabetes_std \/ np.sqrt(bloodpressure_diabetes_n) # standard error\n\nbloodpressure_diabetes_lcb = bloodpressure_diabetes_mean - 1.96* bloodpressure_diabetes_se  #lower limit of the CI\nbloodpressure_diabetes_ucb = bloodpressure_diabetes_mean + 1.96* bloodpressure_diabetes_se  #upper limit of the CI\n# Z = 1.96 for 95% CI\n\n# Non-diabetes patients\nbloodpressure_nodiabetes_mean = 70.81\nbloodpressure_nodiabetes_std = 11.93\nbloodpressure_nodiabetes_n = 500\nbloodpressure_nodiabetes_se = bloodpressure_nodiabetes_std \/ np.sqrt(bloodpressure_nodiabetes_n) # standard error\n\nbloodpressure_nodiabetes_lcb = bloodpressure_nodiabetes_mean - 1.96* bloodpressure_nodiabetes_se  #lower limit of the CI\nbloodpressure_nodiabetes_ucb = bloodpressure_nodiabetes_mean + 1.96* bloodpressure_nodiabetes_se  #upper limit of the CI","8b3f9f87":"# Diabetes patients\nprint(\"Blood Pressure diabetes patients LCB: %.2f\" % bloodpressure_diabetes_lcb)\nprint(\"Blood Pressure diabetes patients UCB: %.2f\" % bloodpressure_diabetes_ucb)\n\n# Non-diabetes patients\nprint(\"Blood Pressure non-diabetes patients LCB: %.2f\" % bloodpressure_nodiabetes_lcb)\nprint(\"Blood Pressure non-diabetes patients UCB: %.2f\" % bloodpressure_nodiabetes_ucb)","c5d89e9f":"df_bloodpressure = diabetes_clean[['BloodPressure', 'Outcome']]","2cb7bc28":"df_bloodpressure[\"Outcome\"].replace({1: \"Diabetes\", 0: \"NotDiabetes\"}, inplace=True)\ndf_bloodpressure","64532c98":"rp.ttest(group1= df_bloodpressure['BloodPressure'][df_bloodpressure['Outcome'] == 'Diabetes'], group1_name= \"Diabetes\",\n         group2= df_bloodpressure['BloodPressure'][df_bloodpressure['Outcome'] == 'NotDiabetes'], group2_name= \"NotDiabetes\")\n# independent t-test using researchpy","84d12f0b":"stats.ttest_ind(df_bloodpressure['BloodPressure'][df_bloodpressure['Outcome'] == 'Diabetes'],\n                df_bloodpressure['BloodPressure'][df_bloodpressure['Outcome'] == 'NotDiabetes'])\n# independent t-test using scipy.stats","9e2a7703":"mu, std = norm.fit(df_bloodpressure['BloodPressure'])\nplt.figure(figsize = [12, 8])\nsns.distplot(df_bloodpressure['BloodPressure'], hist=True, kde=True, \n             bins=25, color = 'b', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 2});\nplt.title(\"Fit results: $\\mu$ = %.2f,  $\\sigma$ = %.2f\" % (mu, std), size=15)\nplt.xlabel(\"Blood Pressure\", size=15)\nplt.ylabel(\"Density\", size=15);","f7bc8e24":"diabetes_clean.groupby(\"Outcome\").agg({\"SkinThickness\":[np.mean, np.std, np.size]})","553230b1":"# Diabetes patients\nskinthickness_diabetes_mean = 28.907\nskinthickness_diabetes_std = 10.290\nskinthickness_diabetes_n = 268\nskinthickness_diabetes_se = skinthickness_diabetes_std \/ np.sqrt(skinthickness_diabetes_n) # standard error\n\nskinthickness_diabetes_lcb = skinthickness_diabetes_mean - 1.96* skinthickness_diabetes_se  #lower limit of the CI\nskinthickness_diabetes_ucb = skinthickness_diabetes_mean + 1.96* skinthickness_diabetes_se  #upper limit of the CI\n# Z = 1.96 for 95% CI\n\n# Non-diabetes patients\nskinthickness_nodiabetes_mean = 25.373\nskinthickness_nodiabetes_std = 9.030\nskinthickness_nodiabetes_n = 500\nskinthickness_nodiabetes_se = bloodpressure_nodiabetes_std \/ np.sqrt(bloodpressure_nodiabetes_n) # standard error\n\nskinthickness_nodiabetes_lcb = skinthickness_nodiabetes_mean - 1.96* skinthickness_nodiabetes_se  #lower limit of the CI\nskinthickness_nodiabetes_ucb = skinthickness_nodiabetes_mean + 1.96* skinthickness_nodiabetes_se  #upper limit of the CI","997d2a6f":"# Diabetes patients\nprint(\"Skin Thickness diabetes patients LCB: %.2f\" % skinthickness_diabetes_lcb)\nprint(\"Skin Thickness diabetes patients UCB: %.2f\" % skinthickness_diabetes_ucb)\n\n# Non-diabetes patients\nprint(\"Skin Thickness non-diabetes patients LCB: %.2f\" % skinthickness_nodiabetes_lcb)\nprint(\"Skin Thickness non-diabetes patients UCB: %.2f\" % skinthickness_nodiabetes_ucb)","34b70d0d":"df_skinthickness = diabetes_clean[['SkinThickness', 'Outcome']]","95ae95d7":"df_skinthickness[\"Outcome\"].replace({1: \"Diabetes\", 0: \"NotDiabetes\"}, inplace=True)\ndf_skinthickness","cbc0284a":"rp.ttest(group1= df_skinthickness['SkinThickness'][df_skinthickness['Outcome'] == 'Diabetes'], group1_name= \"Diabetes\",\n         group2= df_skinthickness['SkinThickness'][df_skinthickness['Outcome'] == 'NotDiabetes'], group2_name= \"NotDiabetes\")\n# independent t-test using researchpy","e04790c6":"stats.ttest_ind(df_skinthickness['SkinThickness'][df_skinthickness['Outcome'] == 'Diabetes'],\n                df_skinthickness['SkinThickness'][df_skinthickness['Outcome'] == 'NotDiabetes'])\n# independent t-test using scipy.stats","cfa124d1":"mu, std = norm.fit(df_skinthickness['SkinThickness'])\nplt.figure(figsize = [12, 8])\nsns.distplot(df_skinthickness['SkinThickness'], hist=True, kde=True, \n             bins=25, color = 'b', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 2});\nplt.title(\"Fit results: $\\mu$ = %.2f,  $\\sigma$ = %.2f\" % (mu, std), size=15)\nplt.xlabel(\"Skin Thickness\", size=15)\nplt.ylabel(\"Density\", size=15);","bf2fe169":"diabetes_clean.groupby(\"Outcome\").agg({\"Insulin\":[np.mean, np.std, np.size]})","9f1d3811":"# Diabetes patients\ninsulin_diabetes_mean = 141.426\ninsulin_diabetes_std = 112.047\ninsulin_diabetes_n = 268\ninsulin_diabetes_se = insulin_diabetes_std \/ np.sqrt(insulin_diabetes_n) # standard error\n\ninsulin_diabetes_lcb = insulin_diabetes_mean - 1.96* insulin_diabetes_se  #lower limit of the CI\ninsulin_diabetes_ucb = insulin_diabetes_mean + 1.96* insulin_diabetes_se  #upper limit of the CI\n# Z = 1.96 for 95% CI\n\n# Non-diabetes patients\ninsulin_nodiabetes_mean = 106.457\ninsulin_nodiabetes_std = 78.561\ninsulin_nodiabetes_n = 500\ninsulin_nodiabetes_se = insulin_nodiabetes_std \/ np.sqrt(insulin_nodiabetes_n) # standard error\n\ninsulin_nodiabetes_lcb = insulin_nodiabetes_mean - 1.96* insulin_nodiabetes_se  #lower limit of the CI\ninsulin_nodiabetes_ucb = insulin_nodiabetes_mean + 1.96* insulin_nodiabetes_se  #upper limit of the CI","b1c5f6aa":"# Diabetes patients\nprint(\"Insulin diabetes patients LCB: %.2f\" % insulin_diabetes_lcb)\nprint(\"Insulin Thickness diabetes patients UCB: %.2f\" % insulin_diabetes_ucb)\n\n# Non-diabetes patients\nprint(\"Insulin Thickness non-diabetes patients LCB: %.2f\" % insulin_nodiabetes_lcb)\nprint(\"Insulin Thickness non-diabetes patients UCB: %.2f\" % insulin_nodiabetes_ucb)","cd3d045f":"df_insulin = diabetes_clean[['Insulin', 'Outcome']]","bcdba1d0":"df_insulin[\"Outcome\"].replace({1: \"Diabetes\", 0: \"NotDiabetes\"}, inplace=True)\ndf_insulin","81f64d33":"rp.ttest(group1= df_insulin['Insulin'][df_insulin['Outcome'] == 'Diabetes'], group1_name= \"Diabetes\",\n         group2= df_insulin['Insulin'][df_insulin['Outcome'] == 'NotDiabetes'], group2_name= \"NotDiabetes\")\n# independent t-test using researchpy","3bbed101":"stats.ttest_ind(df_insulin['Insulin'][df_insulin['Outcome'] == 'Diabetes'],\n                df_insulin['Insulin'][df_insulin['Outcome'] == 'NotDiabetes'])\n# independent t-test using scipy.stats","4a54c078":"mu, std = norm.fit(df_insulin['Insulin'])\nplt.figure(figsize = [12, 8])\nsns.distplot(df_insulin['Insulin'], hist=True, kde=True, \n             bins=25, color = 'b', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 2});\nplt.title(\"Fit results: $\\mu$ = %.2f,  $\\sigma$ = %.2f\" % (mu, std), size=15)\nplt.xlabel(\"Insulin\", size=15)\nplt.ylabel(\"Density\", size=15);","c518ec2b":"diabetes_clean.groupby(\"Outcome\").agg({\"BMI\":[np.mean, np.std, np.size]})","a8b9f308":"# Diabetes patients\nbmi_diabetes_mean = 35.381\nbmi_diabetes_std = 6.596\nbmi_diabetes_n = 268\nbmi_diabetes_se = bmi_diabetes_std \/ np.sqrt(bmi_diabetes_n) # standard error\n\nbmi_diabetes_lcb = bmi_diabetes_mean - 1.96* bmi_diabetes_se  #lower limit of the CI\nbmi_diabetes_ucb = bmi_diabetes_mean + 1.96* bmi_diabetes_se  #upper limit of the CI\n# Z = 1.96 for 95% CI\n\n# Non-diabetes patients\nbmi_nodiabetes_mean = 30.880\nbmi_nodiabetes_std = 6.503\nbmi_nodiabetes_n = 500\nbmi_nodiabetes_se = bmi_nodiabetes_std \/ np.sqrt(bmi_nodiabetes_n) # standard error\n\nbmi_nodiabetes_lcb = bmi_nodiabetes_mean - 1.96* bmi_nodiabetes_se  #lower limit of the CI\nbmi_nodiabetes_ucb = bmi_nodiabetes_mean + 1.96* bmi_nodiabetes_se  #upper limit of the CI","e37c95be":"# Diabetes patients\nprint(\"BMI diabetes patients LCB: %.2f\" % bmi_diabetes_lcb)\nprint(\"BMI diabetes patients UCB: %.2f\" % bmi_diabetes_ucb)\n\n# Non-diabetes patients\nprint(\"BMI non-diabetes patients LCB: %.2f\" % bmi_nodiabetes_lcb)\nprint(\"BMI Thickness non-diabetes patients UCB: %.2f\" % bmi_nodiabetes_ucb)","125f07f7":"df_bmi = diabetes_clean[['BMI', 'Outcome']]","0d8edb52":"df_bmi[\"Outcome\"].replace({1: \"Diabetes\", 0: \"NotDiabetes\"}, inplace=True)\ndf_bmi","a57c3206":"rp.ttest(group1= df_bmi['BMI'][df_bmi['Outcome'] == 'Diabetes'], group1_name= \"Diabetes\",\n         group2= df_bmi['BMI'][df_bmi['Outcome'] == 'NotDiabetes'], group2_name= \"NotDiabetes\")\n# independent t-test using researchpy","d685e4ed":"stats.ttest_ind(df_bmi['BMI'][df_bmi['Outcome'] == 'Diabetes'],\n                df_bmi['BMI'][df_bmi['Outcome'] == 'NotDiabetes'])\n# independent t-test using scipy.stats","da733e36":"mu, std = norm.fit(df_bmi['BMI'])\nplt.figure(figsize = [12, 8])\nsns.distplot(df_bmi['BMI'], hist=True, kde=True, \n             bins=25, color = 'b', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 2});\nplt.title(\"Fit results: $\\mu$ = %.2f,  $\\sigma$ = %.2f\" % (mu, std), size=15)\nplt.xlabel(\"BMI\", size=15)\nplt.ylabel(\"Density\", size=15);","18efa981":"diabetes_clean.groupby(\"Outcome\").agg({\"DiabetesPedigreeFunction\":[np.mean, np.std, np.size]})","543575c0":"# Diabetes patients\ndpf_diabetes_mean = 0.5505\ndpf_diabetes_std = 0.3723\ndpf_diabetes_n = 268\ndpf_diabetes_se = dpf_diabetes_std \/ np.sqrt(dpf_diabetes_n) # standard error\n\ndpf_diabetes_lcb = dpf_diabetes_mean - 1.96* dpf_diabetes_se  #lower limit of the CI\ndpf_diabetes_ucb = dpf_diabetes_mean + 1.96* dpf_diabetes_se  #upper limit of the CI\n# Z = 1.96 for 95% CI\n\n# Non-diabetes patients\ndpf_nodiabetes_mean = 0.4297\ndpf_nodiabetes_std = 0.29908\ndpf_nodiabetes_n = 500\ndpf_nodiabetes_se = dpf_nodiabetes_std \/ np.sqrt(dpf_nodiabetes_n) # standard error\n\ndpf_nodiabetes_lcb = dpf_nodiabetes_mean - 1.96* dpf_nodiabetes_se  #lower limit of the CI\ndpf_nodiabetes_ucb = dpf_nodiabetes_mean + 1.96* dpf_nodiabetes_se  #upper limit of the CI","b79375f0":"# Diabetes patients\nprint(\"Diabetes Pedigree Function diabetes patients LCB: %.3f\" % dpf_diabetes_lcb)\nprint(\"Diabetes Pedigree Function diabetes patients UCB: %.3f\" % dpf_diabetes_ucb)\n\n# Non-diabetes patients\nprint(\"Diabetes Pedigree Function non-diabetes patients LCB: %.3f\" % dpf_nodiabetes_lcb)\nprint(\"Diabetes Pedigree Function patients UCB: %.3f\" % dpf_nodiabetes_ucb)","7aad7253":"df_dpf = diabetes_clean[['DiabetesPedigreeFunction', 'Outcome']]","334d2c16":"df_dpf[\"Outcome\"].replace({1: \"Diabetes\", 0: \"NotDiabetes\"}, inplace=True)\ndf_dpf","81823880":"rp.ttest(group1= df_dpf['DiabetesPedigreeFunction'][df_dpf['Outcome'] == 'Diabetes'], group1_name= \"Diabetes\",\n         group2= df_dpf['DiabetesPedigreeFunction'][df_dpf['Outcome'] == 'NotDiabetes'], group2_name= \"NotDiabetes\")\n# independent t-test using researchpy","5ff2e3c9":"stats.ttest_ind(df_dpf['DiabetesPedigreeFunction'][df_dpf['Outcome'] == 'Diabetes'],\n                df_dpf['DiabetesPedigreeFunction'][df_dpf['Outcome'] == 'NotDiabetes'])\n# independent t-test using scipy.stats","7d5b7f55":"mu, std = norm.fit(df_dpf['DiabetesPedigreeFunction'])\nplt.figure(figsize = [12, 8])\nsns.distplot(df_dpf['DiabetesPedigreeFunction'], hist=True, kde=True, \n             bins=25, color = 'b', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 2});\nplt.title(\"Fit results: $\\mu$ = %.2f,  $\\sigma$ = %.2f\" % (mu, std), size=15)\nplt.xlabel(\"Diabetes Pedigree Function\", size=15)\nplt.ylabel(\"Density\", size=15);","01a8e229":"diabetes_clean.groupby(\"Outcome\").agg({\"Age\":[np.mean, np.std, np.size]})","0d237737":"# Diabetes patients\nage_diabetes_mean = 37.067\nage_diabetes_std = 10.968\nage_diabetes_n = 268\nage_diabetes_se = age_diabetes_std \/ np.sqrt(age_diabetes_n) # standard error\n\nage_diabetes_lcb = age_diabetes_mean - 1.96* age_diabetes_se  #lower limit of the CI\nage_diabetes_ucb = age_diabetes_mean + 1.96* age_diabetes_se  #upper limit of the CI\n# Z = 1.96 for 95% CI\n\n# Non-diabetes patients\nage_nodiabetes_mean = 31.190\nage_nodiabetes_std = 1.667\nage_nodiabetes_n = 500\nage_nodiabetes_se = age_nodiabetes_std \/ np.sqrt(age_nodiabetes_n) # standard error\n\nage_nodiabetes_lcb = age_nodiabetes_mean - 1.96* age_nodiabetes_se  #lower limit of the CI\nage_nodiabetes_ucb = age_nodiabetes_mean + 1.96* age_nodiabetes_se  #upper limit of the CI","c9b5189a":"# Diabetes patients\nprint(\"Age diabetes patients LCB: %.2f\" % age_diabetes_lcb)\nprint(\"Age diabetes patients UCB: %.2f\" % age_diabetes_ucb)\n\n# Non-diabetes patients\nprint(\"Age non-diabetes patients LCB: %.2f\" % age_nodiabetes_lcb)\nprint(\"Age Thickness non-diabetes patients UCB: %.2f\" % age_nodiabetes_ucb)","91ddb0af":"df_age = diabetes_clean[['Age', 'Outcome']]","97f5b2f6":"df_age[\"Outcome\"].replace({1: \"Diabetes\", 0: \"NotDiabetes\"}, inplace=True)\ndf_age","f3695a10":"rp.ttest(group1= df_age['Age'][df_age['Outcome'] == 'Diabetes'], group1_name= \"Diabetes\",\n         group2= df_age['Age'][df_age['Outcome'] == 'NotDiabetes'], group2_name= \"NotDiabetes\")\n# independent t-test using researchpy","7038fb5b":"stats.ttest_ind(df_age['Age'][df_age['Outcome'] == 'Diabetes'],\n                df_age['Age'][df_age['Outcome'] == 'NotDiabetes'])\n# independent t-test using scipy.stats","16474b69":"mu, std = norm.fit(df_age['Age'])\nplt.figure(figsize = [12, 8])\nsns.distplot(df_age['Age'], hist=True, kde=True, \n             bins=25, color = 'b', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 2});\nplt.title(\"Fit results: $\\mu$ = %.2f,  $\\sigma$ = %.2f\" % (mu, std), size=15)\nplt.xlabel(\"Age\", size=15)\nplt.ylabel(\"Density\", size=15);","2ae608f5":"diabetes['Intercept'] = 1\n# Adding intercept\n\ny = diabetes['Outcome']\nX = diabetes.drop('Outcome', axis=1)\n# definine X and y\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n# Splitting the data so 20% is for testing\n\nmodel = LogisticRegression(solver='liblinear')\n# instantiate the model \n\nmodel.fit(X_train, y_train)\ny_predict = model.predict(X_test)\nmodel_score = model.score(X_test, y_test)\n\nprint(model_score)","45c8c6b0":"plot_confusion_matrix(model, X_test, y_test, cmap=plt.cm.Blues);\n# Confusion matrix","5978be82":"accuracy = metrics.accuracy_score(y_test, y_predict)\nprint(\"Accuracy: %.3f\" % accuracy)\n\nprecision = metrics.precision_score(y_test, y_predict)\nprint(\"Precision: %.3f\" % precision)\n\nrecall = metrics.recall_score(y_test, y_predict)\nprint(\"Recall: %.3f\" % recall)\n\nf1 = metrics.f1_score(y_test, y_predict)\nprint(\"F1 Score: %.3f\" % f1)","bf8c83a0":"y_pred_proba = model.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\n\nplt.figure(figsize=(12,8))\nplt.plot(fpr,tpr,label=\"Logistic Regression (Sensitivity = %0.3f)\" % auc)\nplt.legend(loc=4)\nplt.xlabel('False Positive Rate', size = 20)\nplt.ylabel('True Positive Rate', size = 20)\nplt.title(\"Receiver Operatinng Characteristics\", size = 25)\nplt.plot([0, 1], [0, 1],'r--')\nplt.legend(loc=4, fontsize='xx-large')\nplt.show()\n# AUC score for LR model is 0.874","410f0beb":"print(metrics.classification_report(y_test,y_predict))","c3750f86":"coeff = list(model.coef_[0])\nlabels = list(X_test.columns)\nfeatures = pd.DataFrame()\nfeatures['Features'] = labels\nfeatures['importance'] = coeff\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures['positive'] = features['importance'] > 0\nfeatures.set_index('Features', inplace=True)\nfeatures.importance.plot(kind='barh', figsize=(11, 6),color = features.positive.map({True: 'blue', False: 'red'}))\nplt.xlabel('Importance');","25117f7f":"diabetes_clean['Intercept'] = 1\n# Adding intercept\n\ny = diabetes_clean['Outcome']\nX = diabetes_clean.drop('Outcome', axis=1)\n# definine X and y\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n# Splitting the data so 20% is for testing\n\nmodel = LogisticRegression(solver='liblinear')\n# instantiate the model \n\nmodel.fit(X_train, y_train)\ny_predict = model.predict(X_test)\nmodel_score = model.score(X_test, y_test)\n\nprint(model_score)","fce6731a":"plot_confusion_matrix(model, X_test, y_test, cmap=plt.cm.Blues);\n# Confusion matrix","5240ac34":"accuracy = metrics.accuracy_score(y_test, y_predict)\nprint(\"Accuracy: %.3f\" % accuracy)\n\nprecision = metrics.precision_score(y_test, y_predict)\nprint(\"Precision: %.3f\" % precision)\n\nrecall = metrics.recall_score(y_test, y_predict)\nprint(\"Recall: %.3f\" % recall)\n\nf1 = metrics.f1_score(y_test, y_predict)\nprint(\"F1 Score: %.3f\" % f1)","c90e3694":"y_pred_proba = model.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\n\nplt.figure(figsize=(12,8))\nplt.plot(fpr,tpr,label=\"Logistic Regression (Sensitivity = %0.3f)\" % auc)\nplt.legend(loc=4)\nplt.xlabel('False Positive Rate', size = 20)\nplt.ylabel('True Positive Rate', size = 20)\nplt.title(\"Receiver Operatinng Characteristics\", size = 25)\nplt.plot([0, 1], [0, 1],'r--')\nplt.legend(loc=4, fontsize='xx-large')\nplt.show()\n# AUC score for LR model is 0.834","68161fc7":"print(metrics.classification_report(y_test,y_predict))","d2ebc19b":"coeff = list(model.coef_[0])\nlabels = list(X_test.columns)\nfeatures = pd.DataFrame()\nfeatures['Features'] = labels\nfeatures['importance'] = coeff\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures['positive'] = features['importance'] > 0\nfeatures.set_index('Features', inplace=True)\nfeatures.importance.plot(kind='barh', figsize=(11, 6),color = features.positive.map({True: 'blue', False: 'red'}))\nplt.xlabel('Importance');","2a7c5689":"diabetes_LR = diabetes_clean.copy()\n# Make copy of data set before making minor wrangling operations as we don't want to edit our row data","af2f7919":"diabetes_LR.drop(['Pregnancies', 'BloodPressure','SkinThickness','Insulin'], axis = 1, inplace=True)\n\n# Dropping all columns except DiabetesPedigreeFunction, BMI, Glucose and Age are key variables we will \n# do another LR with only these parameters.\n","38fcba9d":"diabetes_LR","c6680e56":"diabetes_LR.describe()","93587003":"# Dont need to add intercept since we have it\n\ny = diabetes_LR['Outcome']\nX = diabetes_LR.drop('Outcome', axis=1)\n# definine X and y\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=0)\n# Splitting the data so 20% is for testing\n\nmodel = LogisticRegression(solver='liblinear')\n# instantiate the model \n\nmodel.fit(X_train, y_train)\ny_predict = model.predict(X_test)\nmodel_score = model.score(X_test, y_test)\n\nprint(model_score)","83af0d6e":"plot_confusion_matrix(model, X_test, y_test, cmap=plt.cm.Blues);\n# Confusion matrix","2d8fa3ce":"accuracy = metrics.accuracy_score(y_test, y_predict)\nprint(\"Accuracy: %.3f\" % accuracy)\n\nprecision = metrics.precision_score(y_test, y_predict)\nprint(\"Precision: %.3f\" % precision)\n\nrecall = metrics.recall_score(y_test, y_predict)\nprint(\"Recall: %.3f\" % recall)\n\nf1 = metrics.f1_score(y_test, y_predict)\nprint(\"F1 Score: %.3f\" % f1)","5ef126db":"y_pred_proba = model.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\n\nplt.figure(figsize=(12,8))\nplt.plot(fpr,tpr,label=\"Logistic Regression (Sensitivity = %0.3f)\" % auc)\nplt.legend(loc=4)\nplt.xlabel('False Positive Rate', size = 20)\nplt.ylabel('True Positive Rate', size = 20)\nplt.title(\"Receiver Operatinng Characteristics\", size = 25)\nplt.plot([0, 1], [0, 1],'r--')\nplt.legend(loc=4, fontsize='xx-large')\nplt.show()\n# AUC score for LR model is 0.860","b1d46091":"print(metrics.classification_report(y_test,y_predict))","7588fa94":"coeff = list(model.coef_[0])\nlabels = list(X_test.columns)\nfeatures = pd.DataFrame()\nfeatures['Features'] = labels\nfeatures['importance'] = coeff\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures['positive'] = features['importance'] > 0\nfeatures.set_index('Features', inplace=True)\nfeatures.importance.plot(kind='barh', figsize=(11, 6),color = features.positive.map({True: 'blue', False: 'red'}))\nplt.xlabel('Importance');","cd2a349b":"diabetes_LR.describe()","3fc6a58f":"from sklearn.model_selection import GridSearchCV\n\nmodel = LogisticRegression()\nparameters = {'penalty': ['l1','l2'], 'C': [0.001,0.01,0.1,1,10,100,1000],\n             'max_iter' : [10,100,500]} \n\nbest_model = GridSearchCV(model, parameters)","99854695":"best_model.fit(X_train, y_train)","c5452e3d":"model = best_model.fit(X_train, y_train)\ny_predict = model.predict(X_test)\nmodel_score = model.score(X_test, y_test)\n\nprint(model_score)","ec68b247":"accuracy = metrics.accuracy_score(y_test, y_predict)\nprint(\"Accuracy: %.3f\" % accuracy)\n\nprecision = metrics.precision_score(y_test, y_predict)\nprint(\"Precision: %.3f\" % precision)\n\nrecall = metrics.recall_score(y_test, y_predict)\nprint(\"Recall: %.3f\" % recall)\n\nf1 = metrics.f1_score(y_test, y_predict)\nprint(\"F1 Score: %.3f\" % f1)","1dae86cc":"plot_confusion_matrix(model, X_test, y_test, cmap=plt.cm.Blues);\n# Confusion matrix","5279b0fa":"y_pred_proba = model.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\n\nplt.figure(figsize=(12,8))\nplt.plot(fpr,tpr,label=\"Logistic Regression (Sensitivity = %0.3f)\" % auc)\nplt.legend(loc=4)\nplt.xlabel('False Positive Rate', size = 20)\nplt.ylabel('True Positive Rate', size = 20)\nplt.title(\"Receiver Operatinng Characteristics\", size = 25)\nplt.plot([0, 1], [0, 1],'r--')\nplt.legend(loc=4, fontsize='xx-large')\nplt.show()\n# AUC score for LR model is 0.861","a04f6c78":"print(metrics.classification_report(y_test,y_predict))","8a982c18":"patient = [[180, 18, 2.1, 30, 1]] # Glucose, BMI, DiabetesPedigreeFunction, Age, Intercept\nbest_model.predict(patient)\n\n# Predicting from patient records that a patient has diabetes from clinical observations","d2286889":"# Diabetes and Logistic regression\n\ndata downloaded from [Kaggle](https:\/\/www.kaggle.com\/kandij\/diabetes-dataset)","e32ef50a":"### (a) Initial Logistic regression model","55e6da9b":"## Gathering data and overview","e53fbfb0":"### (b) Blood Pressure","501c516a":"### (h) Age","97efb481":"### (c) Logistic Regression - Further Iteration","0f97aa4b":"Since DiabetesPedigreeFunction, BMI, Glucose and Age are key variables we will \ndo another LR with only these parameters.","a1b2f3a4":"## Confidence values and hypothesis testing","2afac0dd":"### (d) Insulin","73b07358":"### (c) Skin Thickness","02d0dd2d":"### (d) Logistic Regression - Hyperparameter optimization","e232f8de":"**Table of Content**\n- [Dataset overview](#Dataset-overview)\n- [Logisitc regression overview](#Logistic-regression-overview)\n- [Gathering data and overview](#Gathering-data-and-overview)\n- [Data cleaning](#Data-cleaning)\n- [Data visualisation](#Data-visualisation)\n- [Confidence values and hypothesis testing](#Confidence-values-and-hypothesis-testing)\n    - [(a) Glucose](#(a)-Glucose)\n    - [(b) Blood Pressure](#(b)-Blood-Pressure)\n    - [(c) Skin Thickness](#(c)-Skin-Thickness)\n    - [(d) Insulin](#(d)-Insulin)\n    - [(e) BMI](#(e)-BMI)\n    - [(f) Diabetes Pedigree Function](#(f)-Diabetes-Pedigree-Function)\n    - [(h) Age](#(h)-Age)\n- [Logistic regression](#Logistic-regression)\n    - [(a) Initial Logistic regression model](#(a)-Initial-Logistic-regression-model)\n    - [(b) Logistic Regression using cleaned diabetes dataset](#(b)-Logistic-Regression-using-cleaned-diabetes-dataset)\n    - [(c) Logistic Regression - Further Iteration](#(c)-Logistic-Regression---Further-Iteration)\n    - [(d) Logistic Regression - Hyperparameter optimization](#(d)-Logistic-Regression---Hyperparameter-optimization)\n","d1559898":"There is evidence to suggest that the average blood pressure for diabetics M=74.95 (95% CI: 73.51, 76.39) was higher than non-diabetics, M=70.81 (95% CI: 69.76, 71.81); t(766)= 4.57, p < 0.000 at the 95% confidence level (using independent t-test)","0ad278b3":"## Logistic regression","21f5d350":"For this iteration of LR mode the signiciant parameters will be used which were identified from previous model.","d6c0c858":"For this final logistic regression model the four significant variables will be used, but some hyperparameters will be optisimised to try and improve the performance of the model in terms of diabetes prediction.","692ac10c":"## Logistic regression overview","0fdc0b1a":"## Data visualisation","781c1043":"There is evidence to suggest that the average blood glucsoe concentration for diabetics M=142.15 (95% CI: 138.61, 145.71) was higher than non-diabetics, M=110.71 (95% CI: 108.53, 112.87); t(766)= 15.67, p < 0.000 at the 95% confidence level (using independent t-test)","9d21b229":"### (e) BMI","c460007e":"### (b) Logistic Regression using cleaned diabetes dataset","cb50123f":"For a comparion to further LR models, this LR model will be completed on the diabetes dataset that contained the missing data and also had pregnancies as a discrete variable.","a43af6e7":"## Dataset overview","8a723812":"## Data cleaning","6c0d6f92":"A binary logistic regression model will be implemented using Python in Jupyter Notebooks with an example of predicting if a patient has diabetes mellitus using binary logistic regression. Diabetes mellitus is a series of metabolic disorders where there is hyperglycemia (i.e., high blood glucose levels) which can be broadly divided into type 1 diabetes (patients cannot produce insulin, mainly affecting juveniles), type 2 diabetes (patients have insulin resistance) and gestational diabetes (female patients who while pregnant can be less susceptive to insulin). Type 2 diabetes is the most prevalent form of diabetes. \n\nThe dataset used was the Pima Indian Diabetes dataset from Machine Learning Repository (originally from National Institute of Diabetes and Digestive and Kidney Disease) which contains 8 medical diagnostic attributes and one target variable (i.e, Outcome) of 768 female patients with 34.9% having diabetes (268 patients). The variance for insulin for both categories was quite high. An independent t-test was used to compare the values between diabetic and non-diabetic patients showing that differences occurred for all 8 independent variables. For example, there is evidence to suggest that the average blood glucose concentration for diabetics mean=142.2 mg\/dl (95% CI: 138.6, 145.7) was higher than non-diabetics, mean=110.7 mg\/dl (95% CI: 108.5, 112.9); t(766)= 15.67, p < 0.001 at the 95% confidence level. Before an algorithm like logistic regression can be implemented data quality must be checked.\n","09f4ca75":"Out of all the different iteriations of LR models in this dataset, this one using the missing data reported as values 0 actually gives the best metrics. Different variables are showed as being significant which contributed to the model the most. However, while these metrics are favourable, they do no realistially represent clinical values, especially considering the variables skin thickness and insulin had a high rate of missing values (circa 45%)","ef07c0b6":"### (f) Diabetes Pedigree Function","c9731115":"### (a) Glucose","80051e89":"Machine learning techniques can be generally divided into either supervised or unsupervised methods. Unsupervised learning techniques mainly involve clustering and regression such as principal component analysis (PCA) for data dimensionality reduction. These unsupervised techniques do not require labeled data so that the model output is predicted or classified based on patterns on the input data. \n\nIn comparison, supervised techniques require required the data to be labelled in addition to being split into training and testing datasets. Supervised learning techniques mainly deal with classification (training a model to predict what animal is in a photo) or regression (predicting the price of a house based on number of rooms, neighborhood, area, etc.). However, in the case of supervised classification algorithms, they cannot predict an animal if it was not present during model training. Data is split so that the model can be evaluated on data it has not seen before (i.e., testing data). After a model has been fitted a confusion matrix can be generated to evaluate the model in addition to other metrics.\n\nA commonly supervised regression technique is linear regression where an independent variable (or explanatory) is used to estimate dependent variable (i.e., hours of study to estimate grades). This analogy can be extended to multiple independent variables are used to estimate a dependent variable in multiple linear regression (MLS) (i.e., hours of study, extracurricular activity, number of days sick used to predict grades). The output of MLS is a continuous value (i.e., price of house or grades), in comparison, logistic regression returns the probability of a binary outcome (for example is an email spam or not).\n\nLogistic regression (or logit regression) is a supervised classification algorithm (despite regression in the name) that is used to estimate the probability that an occurrence belongs to a classification (e.g. spam folder of email) where a threshold probability of >50% the model will predict that the occurrence belongs to the positive class (denoted as 0, i.e. normal email) or else predicts that the classification belongs to the negative class (denoted as 1, i.e. spam). The logistic function (denoted as \u03c3) is a sigmoid function with the output bound between 0 and 1.\n\nLogistic regression finds many applications, such as in healthcare settings to predict the likelihood that a cancer is benign or cancerous from nth variables. A database of patients with several variables is split into two with the training dataset being used to train the model while the test dataset is used for model evaluation. These variables can be measured for a new patient to predict if cancer is benign or cancerous. While this type of logistic regression is known as binary logistic regression, it can also be used to predict membership to more than two categories (i.e., is a person married, single or divorced) then multinomial logistic regression can be used.\nIn order to use logistic regression, there are several general assumptions of the model;\n- The true conditional probabilities are a logistic function of the independent variables.\n- Independent variables are measured without error. \n- Observations are independent.\n- Errors are binomially distributed.\n- Independent variables are not linear combinations of each other.\n- No extraneous variables are included, and no important variables are omitted. \n\nWhile a lot of data include nominal type data, logistic regression cannot use these variables for modelling (including linear regression too). To overcome this, dummy variable can be computed. One of the dummy variables of a data type can be removed so that perfect multicollinearity can be avoided. Multicollinearity can also affect the estimate of the dependent variables if the correlation coefficient of several independent variables is high (>0.8) and effects the interpretation of independent coefficients of the logistic regression model."}}