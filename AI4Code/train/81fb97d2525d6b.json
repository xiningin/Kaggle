{"cell_type":{"5894b122":"code","9e424de9":"code","3e7806ef":"code","2a2b2632":"code","3b310a1e":"code","332b1f7f":"code","08366434":"code","2ce7a1dc":"code","13972758":"code","00321c2b":"code","b3ad35cf":"code","c7532957":"code","8c06f298":"code","a83d9285":"code","d7ce4104":"code","abaa426c":"code","b2f09018":"markdown","49cf14ad":"markdown","f4c0bbea":"markdown","d83787b7":"markdown","682c06e8":"markdown","58132128":"markdown","47e0569f":"markdown","8eecf480":"markdown","7201d2d6":"markdown","83fb5bff":"markdown","aeb6ee00":"markdown","be482adb":"markdown","fab7afa1":"markdown","105b8c0e":"markdown","d51900e4":"markdown","3942a726":"markdown","2f20956e":"markdown"},"source":{"5894b122":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9e424de9":"# import the libraries\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\nimport pandas as pd\n\n# load the data\ndigits = datasets.load_digits()\npd = datasets.load_digits()\nprint('Digits dictionary content \\n{}'.format(digits.keys()))  # and see whai it is\n","3e7806ef":"images_and_labels = list(zip(digits.images, digits.target))\nfor index, (image, label) in enumerate(images_and_labels[:4]):\n    plt.subplot(2, 4, index + 1)\n    plt.axis('off')\n    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    plt.title('Training: %i' % label)","2a2b2632":"images_and_labels = list(zip(digits.images, digits.target))\nfor index, (data, label) in enumerate(images_and_labels[:4]):\n    imgdim=int(np.sqrt(digits.data[index].shape[0]))\n    img=np.reshape(digits.data[index],(imgdim,imgdim))\n    plt.subplot(2, 4, index + 1)\n    plt.axis('off')\n    plt.imshow(img, cmap=plt.cm.gray_r, interpolation='nearest')\n    plt.title('Training: %i' % label)","3b310a1e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(digits.data,digits.target,test_size=0.25)\nprint('Training data and target sizes: \\n{}, {}'.format(X_train.shape,y_train.shape))\nprint('Test data and target sizes: \\n{}, {}'.format(X_test.shape,y_test.shape))","332b1f7f":"from sklearn import tree\nclass_tree=tree.DecisionTreeClassifier()\nclass_tree.fit(X_train, y_train)","08366434":"from sklearn.metrics import accuracy_score, classification_report\ny_pred = class_tree.predict(X_test)\nprint(\"Accuracy of model = %2f%%\" % (accuracy_score(y_test, y_pred )*100))","2ce7a1dc":"from sklearn.linear_model import LogisticRegression\n\n\nclass_logistic = LogisticRegression()\nclass_logistic.fit(X_train, y_train)\n\ny_pred = class_logistic.predict(X_test)\nprint(\"Accuracy of model = %2f%%\" % (accuracy_score(y_test, y_pred )*100))","13972758":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\n\n#input data normalization\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train_scaled=scaler.transform(X_train)\nX_test_scaled=scaler.transform(X_test)\n\n\nmlp = MLPClassifier()  #default\n# Train the classifier with the traning data\nmlp.fit(X_train_scaled,y_train)\ny_pred = mlp.predict(X_test_scaled)\nprint(\"Accuracy of model = %2f%%\" % (accuracy_score(y_test, y_pred )*100))","00321c2b":"from sklearn.naive_bayes import MultinomialNB\n \nclassifier_naive = MultinomialNB()\nclassifier_naive.fit(X_train, y_train)\ny_pred  = classifier_naive.predict(X_test)\nprint(\"Accuracy of model = %2f%%\" % (accuracy_score(y_test, y_pred )*100))","b3ad35cf":"from sklearn import neighbors\nknn=neighbors.KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred  = knn.predict(X_test)\nprint(\"Accuracy of model = %2f%%\" % (accuracy_score(y_test, y_pred )*100))","c7532957":"from sklearn import svm\nsvm_classifier = svm.SVC(gamma=0.001)  # as is in https:\/\/scikit-learn.org\/stable\/auto_examples\/classification\/plot_digits_classification.html\n#fit to the trainin data\nsvm_classifier.fit(X_train,y_train)\ny_pred = svm_classifier.predict(X_test)\n\nprint(\"Accuracy of model = %2f%%\" % (accuracy_score(y_test, y_pred )*100))","8c06f298":"from sklearn.metrics import  classification_report\nfrom sklearn import metrics\nprint(\"Classification report for classifier %s:\\n%s\\n\"\n      % (svm_classifier, metrics.classification_report(y_test, y_pred)))","a83d9285":"print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, y_pred))","d7ce4104":"from sklearn.metrics import confusion_matrix\nimport itertools\n#function to plot the Confusion Matrix using matplotlib\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.rcParams['figure.figsize'] = (12,8) # set the plot size\n    plt.rcParams['font.size']= (24)\n    plt.tight_layout()\n    \ntarget_names=digits.target_names \ncnf_matrix=metrics.confusion_matrix(y_test, y_pred)\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=target_names,\n                      title='Confusion matrix, without normalization')\nplt.show()","abaa426c":"import dill as pickle\n\n# Use pickle to save model for next usage.\nfilename = 'svmModel.pk'\nwith open('.\/'+filename, 'wb') as file:\n    pickle.dump(svm_classifier, file) \n    \n# # Use pickle to save model for next usage.\n#filename = 'svmModel.pk'\n#with open('.\/'+filename, 'wb') as file:\n#svm_classifier=pickle.load(file)\n\n#y_pred = svm_classifier.predict(X_test)  #don't need to train it anymore\n\n","b2f09018":"Around 85% without any parametrers tunning. Not bad, but let's test the accuracy of a ** [Logistic Regression](https:\/\/en.wikipedia.org\/wiki\/Logistic_regression)**","49cf14ad":"Let's see how it looks like. We can do it two ways. Using the **images** themselves","f4c0bbea":"This dataset is inside the sklearn and is packed in a dictionary. Let's  start  and see what this dataset contais: ","d83787b7":"Are you a computing specialist?  \nWhat if you could create an algorithm to identify hand written digits, with a great precision? How many lines of code you'll need to recognize digit patterns , from zero to nine?  \nAI can do all the hard work.  We just need to feed  the system with the proper data. \nFollowing the steps below, you'll be able to solve this problem using ML and also evaluate the algorithm to choose the best classifier. \nThis  digits dataset is a ready to use small part  of a much bigger dataset.","682c06e8":"99% plus! Not bad at all. This is the best classifier so far.  Lets try our last one.  **[Support Vector Machine](https:\/\/en.wikipedia.org\/wiki\/Support_vector_machine)** or SVM","58132128":"Well, we already have our model, how can we save it for further use?\nPickle is the answer. ","47e0569f":"Let's see the accuracy of our classifier.","8eecf480":"**What does it mean?**\nprecision: is a fraction of relevant instance among the retrieved instances and is defined as: \n\n \n\nprecision=tp \/ (tp + fp) \n\n \n\nrecall: is the fraction of relevant instances that have been retrieved over total relevant instances in the image, and is defined as \n\n \n\nRecall= tp \/ (tp + fn) \n\n \n\ntp= true positives \n\nfp= false positives \n\nfn= false negatives \n\n \n\n \n\n \n\nf1-score: a mesure of accuracy. Considers precision and recall and the general formula is: \n\n \n\nF1-score= 2 X (precision X recall) \/ (precision + recall) \n\n \n\nReferences:  https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall \n\nhttps:\/\/en.wikipedia.org\/wiki\/F1_score \n\n\n\n\nAnd, to complete our analysis, let's see the confusion matrix. The right predictions are in the main diagonal. For every value on the diagonal, the **numbers in the same line** are the **false positives** \nAnd those in the **same column** are the **false negatives** \n\nhttps:\/\/en.wikipedia.org\/wiki\/Confusion_matrix \n","7201d2d6":"The first classifier will be the **[Decision tree](https:\/\/en.wikipedia.org\/wiki\/Decision_tree)**\n","83fb5bff":"97%, wow! Lets try another one, to see if we can make it even better.  Lets try  **[Neural Network](https:\/\/en.wikipedia.org\/wiki\/Artificial_neural_network)**\n\n\nCheck **[scikit-learn Multi-layer Perceptron](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPClassifier.html)**\n\nIn this classifier, the input data is normalized.  In theory, it's not necessary to normalize numeric x-data (also called independent data). However, practice has shown that when numeric x-data values are normalized, neural network training is often more efficient, which leads to a better predictor","aeb6ee00":"Not so good, but this classifier has many advantages as low computational cost.  \nLet's try the **[K-Nearest Neighbors](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm)** classifier","be482adb":"We found:\n\n* The target names \u2013 all target names we can find. \n* Data \u2013 the 8X8 images, all flattened in to a 1-d vector of size 64X1  \n* The target of each image, for supervised learning \n* Images the 8x8 images itself.  \n* DESCR \u2013 The information of this dataset, how it was created and where, etc. ","fab7afa1":"Well, after this brief investigation, let\u00b4s train some  classifiers and see how it goes.  \nTo do this, we have to split the dataset in two parts. The first part will be the Train dataset and the second part will be the Test dataset.  \nApplying our classifier to the Test dataset wich we previously know the target, will let us  know how good is it. \nTo make it easier, we'll use the train_test_split  library, where we can set the test_size parameter. ","105b8c0e":"We have a Winner! Lets investigate the performance.","d51900e4":"But wait! Data science is also about a good presentation of results. Let's improve our Confusion Matrix Visualization","3942a726":"Or using **data**. In this case we need to reshape the data. Note the results are the same","2f20956e":"This is getting better !  Now, we gonna try the **[Multinomial naive Bayes](https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier#Multinomial_naive_Bayes)**  classifier."}}