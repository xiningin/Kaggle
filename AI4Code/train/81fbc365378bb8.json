{"cell_type":{"b120f6ba":"code","1da8adb2":"code","ec58e353":"code","8a6213d1":"code","b1d9b617":"code","c7479950":"code","8f9ff1d5":"code","7ef5ad8d":"code","977fec90":"code","85b4f024":"code","76bb810c":"code","646f1b07":"code","1efcf701":"code","4380bbdc":"code","7226e457":"code","29c396aa":"code","80ce6f48":"code","94b18f05":"code","a90df440":"code","7b1f0e75":"code","67d2ca78":"code","b7b717e7":"code","bd69b98a":"code","976fd496":"code","58c145f4":"code","b6dba55c":"code","a8138e04":"code","1bfb68ae":"code","c472e80f":"code","53a6956d":"code","8c2d266a":"code","bca6207a":"code","9e45487a":"markdown","50ad8f0c":"markdown","34cc8d9f":"markdown","6a66f805":"markdown","19dcd8f5":"markdown","536cae7a":"markdown","78897d5e":"markdown","4c4e3b99":"markdown","ca4f8b1b":"markdown","f550ccaa":"markdown","4c12fbcb":"markdown","e45a55bc":"markdown","607f893d":"markdown"},"source":{"b120f6ba":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nfrom sklearn.preprocessing import RobustScaler, Imputer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.manifold import TSNE\npd.set_option('display.max_columns', 200) # This allow us to see every column","1da8adb2":"data = pd.read_csv(\"..\/input\/equipfails\/equip_failures_training_set.csv\")  # Read Data into a Pandas DataFrame for Analysis","ec58e353":"data.describe(include = 'all')","8a6213d1":"def clean_data(df):\n    df.replace('na', np.nan, inplace = True) #Replacing \"na\" with Numpy's Nan for Computation\n    df = df.astype(float)\n    r_c = []\n    for i in range(df.shape[1]): #Removes features with more than 50% of the samples missing \n        if df[df.columns[i]].count() > (0.5*df.shape[0]):\n            r_c.append(df.columns[i])\n    imp_data = Imputer(strategy = 'median').fit_transform(df[r_c]) #Imputs missing data with median value of corresponding feature\n    ret_data = pd.DataFrame(imp_data, columns = r_c)\n    return ret_data, r_c #The Index kept are stored for use when readying the deployment dataset (i.e the test dataset)","b1d9b617":"data_fresh, index_in = clean_data(data)","c7479950":"#The Total Data is split into features (X) and a target set, the feature set correponds to a matrix of every eligible sensor reading\n#y corresponds to a failure (1) or normal conditions(0)\nX = data_fresh.iloc[:,2:] \ny = np.array(data_fresh.iloc[:,1])","8f9ff1d5":"def scale(df,method): #This function scales a dataset by the method set by the user\n    scale_d = method.fit_transform(df)\n    return scale_d","7ef5ad8d":"robust = RobustScaler(quantile_range=(2.5,97.5)) #Defining the Robust Scaler","977fec90":"X_scale = scale(X,robust) #Scaling Feature Set","85b4f024":"var = VarianceThreshold()","76bb810c":"var.fit(X_scale) ","646f1b07":"var_feat = var.get_support() #This gets all index with non-zero variance, we stored it for use for the readying the deployment dataset","1efcf701":"X_new_feat = X_scale[:,var_feat] #Extract all features with non-zero variance","4380bbdc":"X_train, X_test, y_train, y_test = train_test_split(X_new_feat,y, test_size = 0.3, random_state = 0, stratify = y)","7226e457":"def train_model(X_train,X_test,y_train,y_test, model): # This function trains the model\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_test)\n    return ('The accuracy score is: ' + str(model.score(X_test, y_test)) +\n            ' The F1 score is: ' + str(f1_score(y_test, y_hat)))","29c396aa":"from sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression","80ce6f48":"#Initiate our models, all hyperparameters used were based on extensive GridSearch and trial and error\nlr = LogisticRegression()\nrfc = RandomForestClassifier(n_estimators=200, n_jobs = -1, max_features = 15)\nxgb = XGBClassifier(n_jobs=-1)","94b18f05":"train_model(X_train, X_test, y_train, y_test, lr)","a90df440":"train_model(X_train, X_test, y_train, y_test, rfc)","7b1f0e75":"train_model(X_train, X_test, y_train, y_test, xgb)","67d2ca78":"test_data = pd.read_csv(\"..\/input\/equipfails\/equip_failures_test_set.csv\")","b7b717e7":"test_d = test_data.iloc[:,1:] #Select all features except id","bd69b98a":"test_in = test_d[index_in[2:]] #Filter based on features","976fd496":"test_in.replace('na',np.nan, inplace = True) #Replace na with Nan as was done with TRAIN data","58c145f4":"test_imp = Imputer(strategy = 'median').fit_transform(test_in) #Replace missing values with with median of feature","b6dba55c":"test_final = test_imp[:,var_feat]","a8138e04":"test_final = robust.fit_transform(test_final) #Scale data using Robust scaler","1bfb68ae":"rfc.fit(X_new_feat, y)","c472e80f":"y_pred = rfc.predict(test_final) # We chose the Random Forest Classifier as we believe its less likely to overfit compared to Random Forest","53a6956d":"export = test_data[['id']]","8c2d266a":"export['target'] = y_pred","bca6207a":"print(export)","9e45487a":"# Import Test Data and Refine the feature the same way as was done with the TRAIN set","50ad8f0c":"# From the above 2-D plot we can see that most of the faulty data most exist at the edge of the 2D space, obviously this a very low dimensional representation of the data and we believe that in higher dimensional space the delineations are clearer","34cc8d9f":"# To ensure we dont have a \"static\" feature in the dataset we remove every feature with zero variance using the Sklearn VarianceThreshold function","6a66f805":"# We did perform some dimensionality reduction techniques to try to reduce the dimensions to aid the performance of the models but this generally lead to poorer results so we decided to stick with high dimensional data for training","19dcd8f5":"# Import Relevant Libraries","536cae7a":"## We split our data into a Train and Test set to ensure we are not overfitting on our dataset","78897d5e":"# ConocoPhillips Sensor Challenge -- OIL MEN Submission","4c4e3b99":"# The data is a bit high dimensional, at least high dimensional enough for us not to be able to visualize it using conventional methods. We used the tSNE method to reduce the data to a 2-dimensional space visualization","ca4f8b1b":"# We selected the Robust scaler which centers each feature (subtracts by its means) and divides by a user selected quantile range of that feature. The purpose of this is to ensure our scaling proedure is not affected by extreme values at either end which are likely to occur in sensor data","f550ccaa":"# Scaling Data is an important step in the ML workflow as it removes the effect of dissimilar scales and is generally best practice","4c12fbcb":"# Everything looks weird in the .describe() DataFrame, after taking a look at the data we found out that some numerical values where saves as strings and the 'na' string ultimately affects the computation","e45a55bc":"# So we write a function that does all the cleaning (replaces the 'na' with the Numpy Nan Object), the function also removes all features (sensor reading), which have less than half of the total samples available (60,000). It also replace all missing data with the median value of its correspnding feature","607f893d":"# The models selected were based on the high dimensionality of the dataset and non-linearity, we believed that the random forest and Gradient boost were the best models to control bias and variance of the model. The SVC was considered but took too long to train. The Logistic Regression was used as something of a base model"}}