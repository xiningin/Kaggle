{"cell_type":{"3cbd2a2c":"code","2d8ab834":"code","d1d0193c":"code","8d57113e":"code","7e062ecb":"code","c07a6941":"code","4fc31ea8":"code","9739c030":"code","cafbe257":"code","8cd050cc":"code","757558c4":"code","c8a3132a":"code","f9d4923b":"code","a18d3371":"code","ddfd8089":"code","976f7e57":"code","a575209c":"code","dccc9922":"code","b2418dab":"code","f107fdeb":"code","96bf5d48":"code","9b3d3ddb":"code","7763ab71":"code","cf75dc34":"code","f4b707f0":"code","b92fe5cf":"code","f49548c4":"code","91bf53f2":"code","3e4e051b":"markdown","d0cd995b":"markdown","594589bc":"markdown","d8d72a00":"markdown","d5d43a81":"markdown","b66b05cc":"markdown","f6598e95":"markdown","f31030cc":"markdown","1fa04481":"markdown","fc814392":"markdown","c4337558":"markdown","0fb324fd":"markdown","532a74ca":"markdown","2a0fd70a":"markdown","858e2b63":"markdown","08a729a1":"markdown","0613dcde":"markdown","a4102ddd":"markdown","e8216f81":"markdown","c11bd254":"markdown","88d1a238":"markdown","c93a9017":"markdown","439774ff":"markdown","a3f7b3cf":"markdown","6df48660":"markdown","6823a85a":"markdown","ecafbcf3":"markdown","6c20766e":"markdown","5fde1b1f":"markdown","26d07c6e":"markdown","cad02eba":"markdown","073f3dbf":"markdown","746fd876":"markdown","c09497f3":"markdown","40893b59":"markdown","9ae7a936":"markdown","6705587e":"markdown","6a67eba5":"markdown","03bd1072":"markdown","3977fb01":"markdown","7f5ef1e1":"markdown","eb320e2d":"markdown"},"source":{"3cbd2a2c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2d8ab834":"import numpy as np \nimport pandas as pd\n\n# Graphs\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Modelling Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import metrics\n\n# Model Evaluation\nfrom sklearn.model_selection import cross_val_score,GridSearchCV\n\n# Other\nimport warnings\nwarnings.filterwarnings('ignore')","d1d0193c":"df_train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndf_gender = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\nprint(f\"Train data consist of {df_train.shape[0]} rows and {df_train.shape[1]} columns\\\n      \\nTest data consist of {df_test.shape[0]} rows and {df_test.shape[1]} columns\\\n      \\nGender submission data consist of {df_gender.shape[0]} rows and {df_gender.shape[1]} columns\")","8d57113e":"df_train.sample(10)","7e062ecb":"def missingValuesAndTaypes(data):\n    df = pd.DataFrame(data.isnull().sum(),columns=[\"Count of Missing Values\"])\n    df[\"Rate of Missing Values\"] = df[\"Count of Missing Values\"] \/ data.shape[0]\n    df[\"Types\"] = [data[i].dtypes for i in data.columns]\n    return df.sort_values(by=\"Count of Missing Values\",ascending=False)\n\nmissingValuesAndTaypes(df_train)","c07a6941":"if \"Cabin\" in df_train.columns:\n    df_train = df_train.drop(columns=\"Cabin\")\n    df_c = pd.DataFrame({\"Columns\":df_train.columns})\nelse:\n    df_c = pd.DataFrame({\"Columns\":df_train.columns})\ndf_c","4fc31ea8":"df_train = df_train[df_train.Embarked.isnull() != True]\ndf_train['Age'] = df_train.groupby([\"Sex\", \"Pclass\"])[\"Age\"].apply(lambda x: x.fillna(x.median()))\nmissingValuesAndTaypes(df_train)","9739c030":"pd.DataFrame(df_train.describe())","cafbe257":"categoricalVariables = [\"Survived\",\"Pclass\",\"Sex\",\"Embarked\"]\nfor i in categoricalVariables:\n    print(pd.DataFrame(df_train[i].value_counts()))","8cd050cc":"sns.violinplot( x=df_train.Survived, y=df_train.Pclass)\nplt.show()","757558c4":"sns.violinplot( x=df_train.Survived, \n               y=df_train.Sex,\n               linewidth=0)\nplt.show()","c8a3132a":"sns.violinplot( x=df_train.Survived, \n               y=df_train.Age,\n               linewidth=0)\nplt.show()","f9d4923b":"g = sns.factorplot(x=\"SibSp\",y=\"Survived\",data=df_train,\n                   kind=\"bar\", height = 5, aspect= 1.6, \n                   palette = \"hls\")\ng.set_ylabels(\"Probability(Survive)\", fontsize=15)\ng.set_xlabels(\"SibSp Number\", fontsize=15)\nplt.show()","a18d3371":"g = sns.factorplot(x=\"Parch\",y=\"Survived\",data=df_train,\n                   kind=\"bar\", height = 5, aspect= 1.6, \n                   palette = \"hls\")\ng.set_ylabels(\"Probability(Survive)\", fontsize=15)\ng.set_xlabels(\"Parch Number\", fontsize=15)\nplt.show()","ddfd8089":"sns.violinplot( x=df_train.Survived, \n               y=df_train.Embarked,\n               linewidth=0)\nplt.show()","976f7e57":"sns.violinplot( x=df_train.Survived, \n               y=df_train.Fare,\n               linewidth=0)\nplt.show()","a575209c":"df_train[\"FSize\"] = df_train[\"Parch\"] + df_train[\"SibSp\"] + 1\npd.DataFrame(pd.crosstab(df_train.FSize, df_train.Survived))","dccc9922":"if \"Sex_male\" not in df_train.columns:\n    df_train = pd.get_dummies(df_train, columns=[\"Sex\",\"Embarked\"],drop_first=True)\ndf_train.head()","b2418dab":"plt.figure(figsize=(15,12))\nplt.title('Correlation of Features for Train Set')\nsns.heatmap(df_train[[\"Pclass\", \"Age\",\"Fare\",\"FSize\",\"Sex_male\",\"Embarked_Q\",\"Embarked_S\",\"Survived\"]].astype(float).corr(),vmax=1.0,  annot=True)\nplt.show()","f107fdeb":"target = df_train.Survived\npredictors = df_train[[\"Pclass\", \"Age\",\"Fare\",\"FSize\",\"Sex_male\",\"Embarked_Q\",\"Embarked_S\"]]","96bf5d48":"predictors = (predictors-predictors.min())\/(predictors.max()-predictors.min())\npredictors.head()","9b3d3ddb":"# Train and test splitting\nfeatures = predictors.columns\nx_train,x_test,y_train,y_test = train_test_split(predictors,target,test_size=0.25, random_state=0)\nfor i in [x_train,x_test,y_train,y_test ]:\n    print(i.shape)","7763ab71":"dtc = DecisionTreeClassifier()\ndtc.fit(x_train,y_train)\ny_pred_dtc = dtc.predict(x_test)\n\ndtc_cm = confusion_matrix(y_test,y_pred_dtc)\nprint(\"Decision Tree Confusion Matrix\",dtc_cm)\ndtc_acc = accuracy_score(y_test,y_pred_dtc)\nprint(\"Decision Tree Accuracy\",dtc_acc)\n\nimportances_dtc = dtc.feature_importances_\nindices_dtc = np.argsort(importances_dtc)\n\nplt.title('Decision Tree Feature Importances')\nplt.barh(range(len(indices_dtc)), importances_dtc[indices_dtc], color='b', align='center')\nplt.yticks(range(len(indices_dtc)), [features[i] for i in indices_dtc])\nplt.xlabel('Relative Importance')\nplt.show()","cf75dc34":"rfc = RandomForestClassifier()\nrfc.fit(x_train,y_train)\ny_pred_rfc = rfc.predict(x_test)\n\nrfc_cm = confusion_matrix(y_test,y_pred_rfc)\nprint(\"Random Forrest Confusion Matrix\",rfc_cm)\nrfc_acc = accuracy_score(y_test,y_pred_rfc)\nprint(\"Random Forrest Accuracy\",rfc_acc)\n\nimportances_rfc = rfc.feature_importances_\nindices_rfc = np.argsort(importances_rfc)\n\nplt.title('Random Forrest Feature Importances')\nplt.barh(range(len(indices_rfc)), importances_rfc[indices_rfc], color='b', align='center')\nplt.yticks(range(len(indices_rfc)), [features[i] for i in indices_rfc])\nplt.xlabel('Relative Importance')\nplt.show()","f4b707f0":"xgboast = XGBClassifier()\nxgboast.fit(x_train, y_train)\nprint(\"Xgboast Classifier Accuracy\",xgboast.score(x_test,y_test))\n\nimportances_xgboast = xgboast.feature_importances_\nindices_xgboast = np.argsort(importances_xgboast)\n\nplt.title('Xgboost Classifier Feature Importances')\nplt.barh(range(len(indices_xgboast)), importances_xgboast[indices_xgboast], color='b', align='center')\nplt.yticks(range(len(indices_xgboast)), [features[i] for i in indices_xgboast])\nplt.xlabel('Relative Importance')\nplt.show()","b92fe5cf":"models = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('RFC', RandomForestClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\nmodels.append(('xgboast', XGBClassifier()))\n\n# evaluate each model in turning kfold results\nresults_boxplot = []\nnames = []\nresults_mean = []\nresults_std = []\np,t = predictors.values, target.values\nfor name, model in models:\n    cv_results = cross_val_score(model, p,t, cv=10)\n    results_boxplot.append(cv_results)\n    results_mean.append(cv_results.mean())\n    results_std.append(cv_results.std())\n    names.append(name)\nalgorithm_table = pd.DataFrame({\"Algorithm\":names,\n                                \"Accuracy Mean\":results_mean,\n                                \"Accuracy\":results_std})\nalgorithm_table.sort_values(by=\"Accuracy Mean\",ascending=False)","f49548c4":"fig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results_boxplot)\nax.set_xticklabels(names)\nplt.show()","91bf53f2":"#Grid Seach for XGboast\nparams = {\n        'min_child_weight': [1, 2, 3],\n        'gamma': [1.9, 2, 2.1, 2.2],\n        'subsample': [0.4,0.5,0.6],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3,4,5]\n        }\ngd_sr = GridSearchCV(estimator=XGBClassifier(),\n                     param_grid=params,\n                     scoring='accuracy',\n                     cv=5,\n                     n_jobs=1\n                     )\ngd_sr.fit(predictors, target)\nbest_parameters = gd_sr.best_params_\nbest_result = gd_sr.best_score_\n\nprint(\"Best result:\", best_result)\npd.DataFrame({\"Parameter\":[i for i in best_parameters.keys()],\n              \"Best Values\":[i for i in best_parameters.values()]})\n","3e4e051b":"**boxplot algorithm comparison**","d0cd995b":"Passengers boarding from Southampton seem to be less likely to survive.","594589bc":"Since only 2 values are missing in the variable named X, we can delete the lines belonging to this value.","d8d72a00":"## 7. Model Preparation <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","d5d43a81":"**Decision Tree**","b66b05cc":"**Survived and Parch**","f6598e95":"**Survived and Embarked**","f31030cc":"## 1. Importing libraries <a class=\"anchor\" id=\"1\"><\/a>\n[Back to Table of Contents](#0.1)","1fa04481":"Convert to dummy variable","fc814392":"**Survived and Passenger Class**","c4337558":"## 4. Analysis of Numerical Variables with Descriptive Statistics <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","0fb324fd":"## 6. Explanation of the Relationship Between Target (Survived) and Other Variables <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","532a74ca":"**Survived and Gender (Sex)**","2a0fd70a":"**New Variable**\nWe derive a family variable by adding the x and y variables and the passenger itself.","858e2b63":"Values of Cabin that 77 percent of them are missing. Therefore, this column is removed from the data.","08a729a1":"**Random Forrest**","0613dcde":"**Survived and Age**","a4102ddd":"**Xgboost Classifier**","e8216f81":"Targetin variable is survivors and other variables are predictors. Let's look at the relationship between the target variable and other variables according to the type of the variable.","c11bd254":"**Correlation Anaylsis**","88d1a238":"We can see a high standard deviation in the survival with 3 parents\/children person's\nAlso that small families (1~2) have more chance to survival than single or big families","c93a9017":"## 5. Analysis of Categorical Variables with Descriptive Statistics <a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","439774ff":"**Survived and Fare**","a3f7b3cf":"The scale distribution of categorical data in the data is given as above.","6df48660":"* **PassengerId:** This column cannot be evaluated numerically. Because it is an ID\n* **Survived:** That is target variable in dataset. It has the feature of being a flag.\n* **Pclass:** It is categorical variable. So it cannot be evaluated numerically\n* **Age:** Since the average age of the people on this ship and the median values are almost close, a normal distribution can be predicted.\n* **SibSp:** As it is understood from the average, one in every two people is seen as a close siblings or spouses on the ship.\n* **Parch:** As it is understood from the average, one in every three people is seen as a close parents \/ children on the ship.\n* **Fare:** It is seen that there is a difference between average and median value in terms of wages and standrt deviation is high.","6823a85a":"All data importing from csv convert to pandas DataFrame object","ecafbcf3":"One or two, siblings\/spouses are more likely to survive.","6c20766e":"## 10. Parameter Tuning for Best Modelling <a class=\"anchor\" id=\"10\"><\/a>\n[Back to Table of Contents](#0.1)","5fde1b1f":"The distribution shows that women survive more than men.","26d07c6e":"## 9. Model Evaluation <a class=\"anchor\" id=\"9\"><\/a>\n[Back to Table of Contents](#0.1)","cad02eba":"It seems that there is an almost even distribution between the classes for the survivors. On the other hand, it is seen that those who do not survive have more people traveling in low class.","073f3dbf":"Selecting target and predictors","746fd876":"## 3. Handle Missing Values and Columns Types <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","c09497f3":"## 8. Modelling and Feature Importances <a class=\"anchor\" id=\"8\"><\/a>\n[Back to Table of Contents](#0.1)","40893b59":"It turns out that passengers who have paid higher tickets are more likely to survive.","9ae7a936":"<a class=\"anchor\" id=\"0.1\"><\/a>\n\n## Table of Contents\n\n1. [Importing libraries](#1)\n1. [Review and Preparation of Data](#2)\n1. [Handle Missing Values and Columns Types](#3)\n1. [Analysis of Numerical Variables with Descriptive Statistics](#4)\n1. [Analysis of Categorical Variables with Descriptive Statistics](#5)\n1. [Explanation of the Relationship Between Target (Survived) and Other Variables](#6)\n1. [Model Preparation](#7)\n1. [Modelling and Feature Importances](#8)\n1. [Model Evaluation](#9)\n1. [Parameter Tuning for Best Modelling](#10)","6705587e":"## 2. Review and Preparation of Data <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","6a67eba5":"Age of the survival probability of survival decreases with age in relation to said to have increased relatively.","03bd1072":"Prepare configuration for cross validation test harnessall models including a list","3977fb01":"**Normalization for data modelling**","7f5ef1e1":"Sample train data review","eb320e2d":"**Survived and siblings \/ spouses**"}}