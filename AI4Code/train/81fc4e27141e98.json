{"cell_type":{"04f08971":"code","446a77f7":"code","9267b16b":"code","145a4f4b":"code","513fe498":"code","36467124":"code","611a4dd9":"code","b0d1b282":"code","00a611fe":"code","f96f2628":"code","36b98abc":"code","f3504c22":"code","ecff1eab":"code","5e6897e0":"code","27fa924f":"code","77e5ca63":"code","e605a4ef":"code","e3e59d5d":"code","f2453bd0":"code","c8af49b2":"code","5e82f487":"code","8b1d9ada":"code","cdf2431c":"code","40fdf9f7":"code","f9e09abc":"code","212b2d87":"code","3411546c":"code","965f76bc":"code","1fbb21e7":"code","0c918627":"code","07aeff48":"code","3e31e21c":"code","6d1983ea":"code","b9665f06":"code","1452657e":"code","74ec55f2":"markdown","ea12c2d9":"markdown","19e056af":"markdown","ec796058":"markdown","13f1d2ec":"markdown","5e6101f8":"markdown","66b7a2f2":"markdown","3522b546":"markdown","4e6ab6ed":"markdown","b07f828f":"markdown","b31a5c75":"markdown","592bc597":"markdown","10120b51":"markdown","0e4b6891":"markdown","b8e45af1":"markdown","e6f6b1da":"markdown","ba1e64d0":"markdown","dc6574df":"markdown","4f04a1cd":"markdown","c2e2a1be":"markdown"},"source":{"04f08971":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","446a77f7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","9267b16b":"train=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest_df=test.copy()\n\n# train.head()\ntest.head()","145a4f4b":"train.info()","513fe498":"train.isnull().sum()","36467124":"train.columns","611a4dd9":"test.columns","b0d1b282":"train.drop(columns=['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\ntest.drop(columns= ['Name', 'Ticket', 'Cabin'], axis=1, inplace= True)\n# train.head()\ntest.head()","00a611fe":"train['Age'].fillna(train['Age'].median(), inplace=True)\ntrain['Embarked'].fillna(train['Embarked'].mode()[0], inplace=True)\n\ntrain.isnull().sum()\n","f96f2628":"test['Age'].fillna(test['Age'].median(), inplace=True)\ntest['Fare'].fillna(test['Fare'].mean(), inplace=True)\ntest['Embarked'].fillna(test['Embarked'].mode()[0], inplace=True)\ntest.isnull().sum()","36b98abc":"train['Survived'].value_counts()","f3504c22":"train['Pclass'].value_counts()","ecff1eab":"train['Sex'].value_counts()","5e6897e0":"train['SibSp'].value_counts()","27fa924f":"train['Parch'].value_counts()","77e5ca63":"train['Embarked'].value_counts()","e605a4ef":"sns.countplot(x='Survived', data= train)","e3e59d5d":"sns.countplot(x='Sex', data= train)","f2453bd0":"sns.countplot(x='Survived', hue='Sex', data= train)","c8af49b2":"sns.countplot(x='Survived', hue='Pclass', data= train)","5e82f487":"sns.boxplot(x='Survived', y= 'Age', hue='Sex', data= train)","8b1d9ada":"sns.boxplot(x='Pclass', y= 'Fare', data= train)","cdf2431c":"train.plot(kind='box')","40fdf9f7":"cols= ['Age', 'SibSp', 'Parch', 'Fare']\n\ntrain[cols]= train[cols].clip(lower= train[cols].quantile(0.15), upper= train[cols].quantile(0.85), axis=1)\n\ntrain.drop(columns=['Parch'], axis=1, inplace=True)\ntrain.plot(kind='box', figsize= (10,8)) \n# no outliers ","f9e09abc":"test.plot(kind='box', figsize= (10,8))\n# there are outliers","212b2d87":"test[cols]= test[cols].clip(lower= test[cols].quantile(0.15), upper= test[cols].quantile(0.85), axis=1)\n\ntest.drop(columns=['Parch'], axis=1, inplace=True)\ntest.plot(kind='box', figsize= (10,8))  \n# no outliers","3411546c":"train1= pd.get_dummies(train, columns=['Pclass', 'Sex', 'Embarked' ], drop_first= True)\ntest1= pd.get_dummies(test, columns=['Pclass', 'Sex', 'Embarked' ], drop_first= True)","965f76bc":"train2=pd.concat([train,train1],axis=1)\ntest2=pd.concat([test,test1],axis=1)","1fbb21e7":"y_train= train2['Survived']\nX_train= train2.drop(['PassengerId','Survived'],axis=1)\n\n# y_test= test2['Survived']\n# X_test= test2.drop(['PassengerId','Survived'],axis=1)","0c918627":"from sklearn.preprocessing import StandardScaler\nss= StandardScaler()\nfeatures= ['Age', 'SibSp', 'Fare']\n\nX_train[features]= ss.fit_transform(X_train[features])\n\nX_test[features]= ss.fit_transform(X_test[features])\n# X_train.head()\nX_test.head()","07aeff48":"X_train=X_train.drop(['Pclass','Sex','Embarked'],axis=1)\n# X_test=X_test.drop(['Pclass','Sex','Embarked'],axis=1)","3e31e21c":"\n# from sklearn.model_selection import train_test_split\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)","6d1983ea":"#Using little bit of Hyperparameter Tuning\nfrom sklearn.ensemble import RandomForestClassifier\nclf=RandomForestClassifier(n_estimators=500)\nclf.fit(X_train,y_train)","b9665f06":"predictions= clf.predict(X_test)\nclf.score(X_test, y_test)","1452657e":"submission= pd.DataFrame(data=predictions)\nprint(submission.head())\nfilename= 'titanic_prediction1.csv'\nsubmission.to_csv(filename,index=False)","74ec55f2":"### 10. Handling Categorical Data using Get_Dummies()\n#### We use *'drop_first'* to avoid **Dummy Trap**","ea12c2d9":"# Titanic - Solution - Easiest Way\n\n\n### I used following techniques in this notebook\n1. Loading Data using Pandas\n2. Checking varible type for each column\n3. Checking number of nulls in each column\n4. Finding column in Dataset\n5. Drop useless columns\n6. Handling Missing value with Median and Mode\n7. Checking occurance of each category\n8. Data Visualiztion using Matplotlib\n9. Checking Ouliers\n10. Handling Categorical Data using Get_Dummies()\n11. Concatenating the Original Dataset & the One after creating Dummies\n12. Seggregating X & y.\n13. Preprocessing Numeric Data using StandardScaler\n14. Dropping useless columns after we get_dummies()\n15. Splitting using train_test_split\n16. Using Random Forest as ML model\n17. Predicting & Scoring the Trained Model","19e056af":"### 11. Concatenating the Original Dataset & the One after creating Dummies*(get_dummies() creates a new DF containing JUST the dummies, MOST People get wrong here)*","ec796058":"### 17. Predicting & Scoring the Trained Model","13f1d2ec":"### 4. Finding column in Dataset","5e6101f8":"### 2. Checking varible type for each column","66b7a2f2":"### 16. Using Random Forest as ML model","3522b546":"### 15. Splitting using train_test_split","4e6ab6ed":"### 9. Checking Ouliers","b07f828f":"### 13. Preprocessing Numeric Data using StandardScaler","b31a5c75":"#### If you like this notebook, give an upvote. Any suggestions or comments are appreciated.","592bc597":"### 3. Checking number of nulls in each column","10120b51":"### 18. Saving the output in a file","0e4b6891":"### 6. Handling Missing value with Median and Mode\n","b8e45af1":"### **1. Loading Data using Pandas**","e6f6b1da":"### 12. Splitting X & y","ba1e64d0":"### 5. Drop useless columns","dc6574df":"### 8. Data Visualiztion using Matplotlib","4f04a1cd":"### 14. Dropping useless columns after we get_dummies()","c2e2a1be":"### 7. Checking occurance of each category"}}