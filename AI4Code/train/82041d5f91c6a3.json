{"cell_type":{"55f9c652":"code","5a73eec5":"code","62504586":"code","e14ce4e7":"code","6d722a49":"code","b8b28121":"code","c2b85e63":"code","3e01874a":"code","ab6bed2d":"code","0f538687":"code","67f51f6f":"code","e3605a85":"code","58dec821":"code","afbcd117":"code","5c399ca3":"code","fbad8c74":"markdown","2ffb206a":"markdown","259732f9":"markdown","dd19ab8e":"markdown","a1bde498":"markdown","0cf1e900":"markdown","dd7cc06b":"markdown"},"source":{"55f9c652":"# installations\n!pip install datasets\n!pip install ipywidgets\n!pip install regex\n!pip install negspacy\n!pip install vaderSentiment\n\n!python -m spacy download en_core_web_lg\n!python -m spacy download en_core_web_sm","5a73eec5":"# imports\nfrom datasets import load_dataset\nimport pandas as pd\nimport numpy as np\nimport regex as re\nimport spacy\n\nimport nltk\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.tokenize import word_tokenize \nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\nfrom datasets import load_dataset\n\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer","62504586":"%%time\n## PREPROCESSING FUNCTIONS ##\n\n# Setting stopword list\nstop_words = set(stopwords.words('english'))\n\n# Stemmization prerequisits\nps = PorterStemmer()\n\n# Lemmatization prerequisits\nlem = WordNetLemmatizer()\n\ndef tokenization(df: pd.DataFrame):\n    ''' Takes a pandas dataframe as argument and returns it after tokenizing its sentences.'''\n    df['text'] = df['text'].apply(word_tokenize)\n\ndef stemmization(df: pd.DataFrame):\n    ''' Takes a pandas dataframe as argument and returns it after stemming its vocabulary.\n        The input text must be already tokenized.'''\n    df['text'] = df['text'].apply(lambda tokenized_text: [ps.stem(token) for token in tokenized_text])\n\ndef lemmatization(df: pd.DataFrame):\n    ''' Takes a pandas dataframe as argument and returns it after lemmatazing its vocabulary.\n        The input text must be already tokenized.'''\n    df['text'] = df['text'].apply(lambda tokenized_text: [lem.lemmatize(token) for token in tokenized_text])","e14ce4e7":"%%time\n# dataframes creation\ndataset = load_dataset('imdb', split='train')\ndf = dataset.to_pandas()\nprint(\"> dataset loaded\")\ndisplay(df)","6d722a49":"%%time\n# Tokenization\n\ndf_tokenized = df.copy()\ntokenization(df_tokenized)","b8b28121":"%%time\n# Lemmatization\n\ndf_lemma = df_tokenized.copy()\nlemmatization(df_lemma)","c2b85e63":"%%time\n# Stemmization\n\ndf_stem = df_tokenized.copy()\nstemmization(df_stem)","3e01874a":"display(\"tokenized:\", df_tokenized.head(3), \"lemmatized:\", df_lemma.head(3), \"stemmed:\", df_stem.head(3))","ab6bed2d":"# feature extraction functions\n\ndef hasWord(tokens, wordsToMatch):\n    for tok in tokens:\n        t = tok.lower()\n        for w in wordsToMatch:\n            if t == w:\n                return 1\n    return 0\n\ndef countMatch(tokens, wordsToMatch):\n    n = 0\n    for tok in tokens:\n        t = tok.lower()\n        for w in wordsToMatch:\n            if t == w:\n                n += 1\n                continue\n    return n\n\ndef hasNo(tokens):\n    return hasWord(tokens, ['no', 'not'])\n\ndef nFirstPronoun(tokens):\n    return countMatch(tokens, ['i', 'we'])\n\ndef nSecondPronoun(tokens):\n    return countMatch(tokens, ['you'])\n\ndef hasExclam(tokens):\n    return hasWord(tokens, ['!'])\n\ndef logWordCount(tokens):\n    return np.log(len(tokens))\n\n\nanalyzer = SentimentIntensityAnalyzer()\nposThreshold = 0.6\nnegThreshold = 0.6\n\ndef nPositive(tokens):\n    n = 0\n    for token in tokens:\n        vs = analyzer.polarity_scores(token)\n        if vs['pos'] > posThreshold:\n            n += 1\n    return n\n\ndef nNegative(tokens):\n    n = 0\n    for token in tokens:\n        vs = analyzer.polarity_scores(token)\n        if vs['neg'] > negThreshold:\n            n += 1\n    return n","0f538687":"%%time\n# applying feature extraction functions to create a new DataFrame\n\ndef extract_text_features(df):\n    tokens = df[\"text\"]\n    df[\"hasNo\"] = tokens.apply(hasNo)\n    print(\"> hasNo\")\n    df[\"nFirstPronoun\"] = tokens.apply(nFirstPronoun)\n    print(\"> nFirstPronoun\")\n    df[\"nSecondPronoun\"] = tokens.apply(nSecondPronoun)\n    print(\"> nSecondPronoun\")\n    df[\"hasExclam\"] = tokens.apply(hasExclam)\n    print(\"> hasExclam\")\n    df[\"logWordCount\"] = tokens.apply(logWordCount)\n    print(\"> logWordCount\")\n    df[\"nPositive\"] = tokens.apply(nPositive)\n    print(\"> nPositive\")\n    df[\"nNegative\"] = tokens.apply(nNegative)\n    print(\"> nNegative\")\n    return df\n\n\ndf_tokenized = extract_text_features(df_tokenized)\ndf_stem = extract_text_features(df_stem)\ndf_lemma = extract_text_features(df_lemma)\n\nprint()\ndisplay(\"tokenized:\", df_tokenized.head(3), \"lemmatized:\", df_lemma.head(3), \"stemmed:\", df_stem.head(3))\n","67f51f6f":"%%time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\nX_tokenized = df_tokenized[[\"hasNo\", \"nFirstPronoun\", \"nSecondPronoun\", \"hasExclam\", \"logWordCount\", \"nPositive\", \"nNegative\"]]\nX_lemma = df_lemma[[\"hasNo\", \"nFirstPronoun\", \"nSecondPronoun\", \"hasExclam\", \"logWordCount\", \"nPositive\", \"nNegative\"]]\nX_stem = df_stem[[\"hasNo\", \"nFirstPronoun\", \"nSecondPronoun\", \"hasExclam\", \"logWordCount\", \"nPositive\", \"nNegative\"]]\ny = df_tokenized[\"label\"]\n\n","e3605a85":"%%time\n\n# applying a train-predict-scoring pipeline to our 3 datasets\n\nlegend = [\"tokenized\", \"lemmatized\", \"stemmed\"]\n\nfor i, X in enumerate([X_tokenized, X_lemma, X_stem]):\n    # train\/text split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n\n    # training a logistic regression model\n    model = LogisticRegression(random_state=42, penalty='l2', solver='liblinear')\n    model.fit(X_train, y_train)\n    \n    # making predictions and showing the scores\n    y_pred = model.predict(X_test)\n    print(legend[i] + \":\")\n    print(classification_report(y_test, y_pred))","58dec821":"def uniqueWordsRatio(tokens):\n    if len(tokens) == 0:\n        return 0\n    tokens_set = set(tokens)    \n    return float(len(tokens_set)) \/ float(len(tokens))\n\ndef avgWordLength(tokens):\n    if len(tokens) == 0:\n        return 0\n    avg = 0.0\n    for tok in tokens:\n        avg += len(tok)\n    return avg \/ float(len(tokens))\n\n\ndef extract_more_text_features(df):\n    tokens = df[\"text\"]\n    df[\"uniqueWordsRatio\"] = tokens.apply(uniqueWordsRatio)\n    df[\"avgWordLength\"] = tokens.apply(avgWordLength)\n    return df","afbcd117":"df_tokenized = extract_more_text_features(df_tokenized)\ndf_lemma = extract_more_text_features(df_lemma)\ndf_stem = extract_more_text_features(df_stem)\n\nprint()\ndisplay(\"tokenized:\", df_tokenized.head(3), \"lemmatized:\", df_lemma.head(3), \"stemmed:\", df_stem.head(3))\n","5c399ca3":"train_columns = [\"hasNo\", \"nFirstPronoun\", \"nSecondPronoun\", \"hasExclam\", \"logWordCount\",\n                 \"nPositive\", \"nNegative\", \"uniqueWordsRatio\", \"avgWordLength\"]\n\nX_tokenized = df_tokenized[train_columns]\nX_lemma = df_lemma[train_columns]\nX_stem = df_stem[train_columns]\ny = df_tokenized[\"label\"]\n\n# applying a train-predict-scoring pipeline to our 3 datasets\n\nlegend = [\"tokenized\", \"lemmatized\", \"stemmed\"]\n\nfor i, X in enumerate([X_tokenized, X_lemma, X_stem]):\n    # train\/text split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n\n    # training a logistic regression model\n    model = LogisticRegression(random_state=42, penalty='l2', solver='liblinear')\n    model.fit(X_train, y_train)\n    \n    # making predictions and showing the scores\n    y_pred = model.predict(X_test)\n    print(legend[i] + \":\")\n    print(classification_report(y_test, y_pred))","fbad8c74":"We are first extracting these features :\n\n- \"**hasNo**\" : 1 if \"no\" or \"not\" appear in the doc, 0 otherwise\n- \"**nFirstPronoun**\" : The count of first pronouns in the document\n- \"**nSecondPronoun**\" : The count of second pronouns in the document\n- \"**hasExclam**\" : 1 if \"!\" is in the document, 0 otherwise\n- \"**logWordCount**\" : log(word count in the document)\n- \"**nPositive**\" : Number of words in the document which are in the positive lexicon from VADER sentiment\n- \"**nNegative**\" : \"Number of words in the document which are in the negative lexicon from VADER sentiment","2ffb206a":"# 4 - improving preprocessing","259732f9":"# 1 - Preprocesing ","dd19ab8e":"We can observe that the precision, recall, and f1-score are the same for df_tokenized and df_lemma, and slightly lower for df_stemmed.","a1bde498":"# 2 - Feature extraction","0cf1e900":"## 3 - Logistic regression\n\nLet's train a logistic regression model on our columns","dd7cc06b":"Applying the logistic regression again"}}