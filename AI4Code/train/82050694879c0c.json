{"cell_type":{"6da0214b":"code","94ddf07a":"code","df509619":"code","646a9f46":"code","895364c2":"code","0cd6eb60":"code","84027565":"code","2b98695c":"code","165bb2b0":"code","6a5df556":"code","cfdb5385":"code","d9870518":"code","8022caa7":"code","15a4744a":"code","263356ef":"code","eb2225ee":"code","c3c30017":"code","08b4f896":"code","4ca840fb":"code","8c5c9c13":"code","bcd58f8e":"code","c2a1fd5d":"code","1dff51e1":"code","de1b5421":"code","274c49e9":"code","da771b96":"code","f5f3a562":"code","92e30aac":"code","195451dc":"code","a7732924":"code","9dcba721":"code","22fcd47a":"code","df2428ba":"code","66b40ce0":"code","2e36b59c":"code","016f1791":"markdown","85bf66c5":"markdown","5b28acfb":"markdown","d65905f2":"markdown","87021b40":"markdown","1a1934b3":"markdown","ab4740e1":"markdown","1d3477ea":"markdown","e56c16ff":"markdown","94fd9ec5":"markdown","8b30f436":"markdown","721a63a2":"markdown","a35c733f":"markdown","41956ec1":"markdown","ea9e1e1b":"markdown","b4d1ad40":"markdown","52029c4d":"markdown","dc718cb9":"markdown","1dd43e50":"markdown","24706684":"markdown","a20f49da":"markdown","a329418e":"markdown","835a0483":"markdown","aa179394":"markdown","d2e5fc56":"markdown","f59b69b9":"markdown","32cbd5bc":"markdown","8be5da49":"markdown","3ddd664a":"markdown","d9f35f96":"markdown","72f79fe7":"markdown"},"source":{"6da0214b":"import os\nimport sys\nimport librosa\nimport librosa.display\nimport librosa.feature\nimport numpy as np\nimport pandas as pd\nimport plotly.express as xp\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport IPython.display as ipd\nfrom sklearn.preprocessing import minmax_scale\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","94ddf07a":"train = pd.read_csv('..\/input\/birdsong-recognition\/train.csv')\ntrain.head()","df509619":"train.info()","646a9f46":"missing = train.isna().sum().sort_values(ascending=False)","895364c2":"missing = missing[missing != 0]\nxp.bar(x=missing.index, y=missing, text=missing, title='Missing Values by Feature', labels={'x':'Features', 'y':'Quantity'})","0cd6eb60":"counts = train['ebird_code'].value_counts()","84027565":"xp.bar(x=counts.index, y=counts, title='Species Distribution (by ebird code)', labels={'x':'Ebird Code', 'y':'Quantity'})","2b98695c":"# split datetime into separate dataframe\ndatetime = train[['date', 'time']]\ndatetime.date = pd.to_datetime(datetime.date, errors='coerce').dropna()\ndatetime['hour'] = pd.to_numeric(datetime.time.str.split(':', expand=True)[0], errors='coerce')","165bb2b0":"ax1 = datetime.date.value_counts().sort_values().plot(figsize=(10,6), title='Recordings by Date')\n\nax1.set_xlabel('Date')\nax1.set_ylabel('Quantity')\nplt.show()","6a5df556":"ax2 = datetime['hour'].value_counts().sort_index().plot(figsize=(10,6), title='Recordings by Time', kind='bar', figure=plt.figure())\n\nax2.set_xlabel('Hour')\nax2.set_ylabel('Quantity')\nplt.show()","cfdb5385":"ax3 = train['recordist'].value_counts().sort_values(ascending=False).head(20).sort_values().plot(figsize=(10, 6), title='Recordings by Recordist', figure=plt.figure(), kind='barh', fontsize=9)\n\nax3.set_xlabel('Hour')\nax3.set_ylabel('Quantity')\nplt.tight_layout()\nplt.show()","d9870518":"counts = train['country'].value_counts().sort_values(ascending=False).head(10).sort_values()","8022caa7":"xp.bar(y=counts.index, x=counts, title='Number of Recordings by Country', labels={'y':'Country', 'x':'Quantity'}, orientation='h')","15a4744a":"coords = train.groupby(['latitude', 'longitude'], as_index=False)['ebird_code'].agg('count')\ncoords = coords[coords.latitude != 'Not specified']\ncoords = coords[coords.longitude != 'Not specified']\nxp.scatter_geo(lat=coords['latitude'], lon=coords['longitude'], title='Recording Locations')","263356ef":"bird_codes = train.ebird_code.unique()[:5]\n\naudio = []\nfor bird in range(len(bird_codes)):\n    filename = train[train['ebird_code'] == bird_codes[bird]]['filename'].iloc[0]\n    path = os.path.join('..\/input\/birdsong-recognition\/train_audio\/', bird_codes[bird], filename)\n    \n    # wave plot\n    plt.figure(figsize=(15,10))\n    plt.subplot(len(bird_codes), 1, bird+1)\n    data, srate = librosa.load(path)\n    librosa.display.waveplot(data, sr=srate)\n    plt.gca().set_title(bird_codes[bird])\n    plt.xticks([],[])\n    plt.xlabel('')\n    plt.show()\n    \n    # audio display\n    audio = ipd.Audio(path)\n    ipd.display(audio)","eb2225ee":"data, srate = librosa.load('..\/input\/birdsong-recognition\/train_audio\/ameavo\/XC99571.mp3')","c3c30017":"# plot waveform as refresher\nplt.figure(figsize=(15,5))\nlibrosa.display.waveplot(data, sr=srate)\nplt.gca().set_title('ameavo')\nplt.xticks([],[])\nplt.xlabel('')\nplt.show()","08b4f896":"autocorrelation = librosa.autocorrelate(data, max_size=5000)","4ca840fb":"plt.figure(figsize=(15,5))\nplt.plot(autocorrelation)\nplt.gca().set_title('Autocorrelation by Lag Time')\nplt.xlabel('Lag')\nplt.show()","8c5c9c13":"spectrogram = librosa.stft(data)","bcd58f8e":"plt.figure(figsize=(20,10))\nlibrosa.display.specshow(librosa.amplitude_to_db(abs(spectrogram)), sr=srate, x_axis='time', y_axis='hz')\nplt.xlabel('Time', fontsize=20)\nplt.ylabel('Frequency Band')\nplt.colorbar()\nplt.title('Spectrogram', fontsize=20)\nplt.show()","c2a1fd5d":"chroma = librosa.feature.chroma_stft(data, sr=srate)","1dff51e1":"plt.figure(figsize=(20,10))\nlibrosa.display.specshow(chroma, x_axis='time', y_axis='chroma')\nplt.xlabel('Time', fontsize=20)\nplt.ylabel('Chroma Value', fontsize=20)\nplt.colorbar()\nplt.clim(0,1)\nplt.title('Chromagram', fontsize=20)\nplt.show()","de1b5421":"centroid = librosa.feature.spectral_centroid(data)[0]","274c49e9":"plt.figure(figsize=(15,5))\nlibrosa.display.waveplot(data, sr=srate)\nplt.plot(librosa.frames_to_time(range(len(centroid))), minmax_scale(centroid), color='g')\nplt.gca().set_title('Spectral Centroid by Frame')\nplt.xlabel('Frame')\nplt.show()","da771b96":"bandwidth = librosa.feature.spectral_bandwidth(data, sr=srate)[0]","f5f3a562":"plt.figure(figsize=(15,5))\nlibrosa.display.waveplot(data, sr=srate)\nplt.plot(librosa.frames_to_time(range(len(bandwidth))), minmax_scale(bandwidth))\nplt.gca().set_title('Spectral Bandwidth by Time')\nplt.xlabel('Time')\nplt.show()","92e30aac":"contrast = librosa.feature.spectral_contrast(data, sr=srate)","195451dc":"plt.figure(figsize=(20,10))\nlibrosa.display.specshow(contrast, x_axis='time')\nplt.xlabel('Time', fontsize=20)\nplt.colorbar()\nplt.title('Spectral Contrast', fontsize=20)\nplt.ylabel('Frequency Band', fontsize=20)\nplt.show()","a7732924":"flatness = librosa.feature.spectral_flatness(data)","9dcba721":"plt.figure(figsize=(20,10))\nlibrosa.display.specshow(flatness, x_axis='time')\nplt.xlabel('Time', fontsize=20)\nplt.colorbar()\nplt.clim(0,1)\nplt.title('Spectral Flatness', fontsize=20)\nplt.ylabel('Frequency Band', fontsize=20)\nplt.show()","22fcd47a":"rolloff = librosa.feature.spectral_rolloff(data, sr=srate)[0]","df2428ba":"plt.figure(figsize=(15,5))\nlibrosa.display.waveplot(data, sr=srate)\nplt.plot(librosa.frames_to_time(range(len(rolloff))), minmax_scale(rolloff))\nplt.gca().set_title('Spectral Bandwidth by Time')\nplt.xlabel('Time')\nplt.show()","66b40ce0":"mfcc = librosa.feature.mfcc(data, sr=srate, n_mfcc=30)","2e36b59c":"plt.figure(figsize=(20,10))\nlibrosa.display.specshow(minmax_scale(mfcc, axis=1), x_axis='time')\nplt.xlabel('Time', fontsize=20)\nplt.colorbar()\nplt.clim(0,1)\nplt.title('Mel-Frequency Cepstral Coefficients', fontsize=20)\nplt.show()\n\nprint()\nprint('MFCCs calculated: %d' % mfcc.shape[0])","016f1791":"- Pitch seems to hover around 2000 to 3500 Hz most of the time\n- Some spikes to 5500-7000 Hz","85bf66c5":"- Most recordings taken between 6AM and 12PM\n- Gradual decrease as the day moves on from 8AM","5b28acfb":"<a id=\"Header\"><\/a>\n# Exploratory Data Analysis","d65905f2":"<a id=\"MissingValues\"><\/a>\n### Missing Values\n\nWe need to check to see if any of our relevant features are missing values and, if so, what to do about that.","87021b40":"<a id=\"Autocorrelation\"><\/a>\n### Autocorrelation\n\nAutocorrelation compares a signal with a lagged version of itself. It's main purpose is to find repeated patterns in a sample that might be hidden by noise.","1a1934b3":"- highest contrast occurs in edge frequency bands","ab4740e1":"- pure noise portions of sample are higher in bandwidth\n\n<a id=\"Contrast\"><\/a>\n#### Spectral Contrast\n\nSpectral contrast compares the max and min frequency values for each frequency band at a point in time. Thus, spectral contrast gives a robust measure of relative spectral characteristics.","1d3477ea":"<a id=\"Species\"><\/a>\n### Species\nLet's explore the distribution of the bird species among samples. We'll use the ebird code instead of the full species name.","e56c16ff":"<a id=\"Environment\"><\/a>\n## Environment","94fd9ec5":"<a id=\"Flatness\"><\/a>\n#### Spectral Flatness\n\nSpectral flatness compares the arithmetic and geometric means of the power spectrum. It is most often used to identify and separate tones versus noise.","8b30f436":"<a id=\"Thanks\"><\/a>\n# Thank You for Reading!\n\nI am still very much new to data science, and I'm jumping in head-first. This is meant as a learning experience to help me learn some signal processing and audio classification techniques as well as a simple EDA and FE for those who aren't well-versed in audio processing. I invite any and all constructive feedback!\n\nThanks again! Hope this is helpful to you.","721a63a2":"<a id=\"MFCC\"><\/a>\n### MFCC\n\nMel-Frequency Cepstral Coefficients are a collection of coefficients that together give a representation of the overall spectral envelope of a signal. Probably the most common and important feature of audio signal processing in machine learning.","a35c733f":"- Majority of recordings made by only two people","41956ec1":"<a id=\"Recordists\"><\/a>\n### Recordists\nWho recorded the samples? This could be important as certain recordists may have a particular interest in certain birds.","ea9e1e1b":"- Maximum value of .2 at points\n- Low noise in general\n\n<a id=\"Rolloff\"><\/a>\n#### Spectral Rolloff\n\nSpectral rolloff is the frequency under which a specified percentage of the energy lies","b4d1ad40":"<a id=\"Chromagram\"><\/a>\n### Chromagram\n\nThe Chromagram is a visual representation of a signal's chroma feature. The chroma feature at any point in time is the intensity for each chroma value in the set {C, C\u266f, D, D\u266f, E , F, F\u266f, G, G\u266f, A, A\u266f, B}. These values are the rows of the chromagram.","52029c4d":"- Majority of recordings taken in the past decade\n- Interesting spike around 2003\n- Cyclical spikes after 2013","dc718cb9":"- Exactly 100 samples for about half of species in question\n- Redhead is minimum at 9 samples","1dd43e50":"<a id=\"Spectral\"><\/a>\n### Spectral Features\n\n<a id=\"Centroid\"><\/a>\n#### Spectral Centroid\n\nSpectral Centroid is a measurement of the \"center of gravity\" of the signal and is a common metric of timbre in a sound sample. It's essentially the dominant frequency at each point.","24706684":"<a id=\"Spectrogram\"><\/a>\n### Spectrogram\n\nThe spectrogram is a visual representation of a signal's spectrum of frequencies over time. ","a20f49da":"<a id=\"Location\"><\/a>\n### Location\nCertain birds only inhabit certain areas. Therefore, we need to take location into account.","a329418e":"# Table of Contents\n\n* [Exploratory Data Analysis](#Header)\n    - [Metadata](#Metadata)\n        - [Missing Values](#MissingValues)\n        - [Species](#Species)\n        - [Date and Time](#DateTime)\n        - [Recordists](#Recordists)\n        - [Location](#Location)\n* [Audio Feature Extraction](#AudioFeatureExtraction)\n    - [Waveform](#Waveform)\n    - [Autocorrelation](#Autocorrelation)\n    - [Spectrogram](#Spectrogram)\n    - [Chromagram](#Chromagram)\n    - [Spectral](#Spectral)\n        - [Centroid](#Centroid)\n        - [Bandwidth](#Bandwidth)\n        - [Contrast](#Contrast)\n        - [Flatness](#Flatness)\n        - [Rolloff](#Rolloff)\n    - [MFCC](#MFCC)\n\n* [Afterword](#Thanks)","835a0483":"# Resources\nHere are the major resources that I used while doing my research.\n\n- [Autocorrelation Wiki](https:\/\/en.wikipedia.org\/wiki\/Autocorrelation)\n- [Sanket Doshi - Music Feature Extraction in Python](https:\/\/towardsdatascience.com\/extract-features-of-music-75a3f9bc265d)\n- [Spectral Features (IPython Notebook)](https:\/\/musicinformationretrieval.com\/spectral_features.html#:~:text=Spectral%20contrast%20considers%20the%20spectral,difference%20in%20each%20frequency%20subband.)","aa179394":"<a id=\"Bandwidth\"><\/a>\n#### Spectral Bandwidth\n\nSpectral bandwidth represents the range between the lowest and highest frequency bands of the signal at a certain time.","d2e5fc56":"- Vast majority of data comes from North America, specifically from USA\n- Very little data from Africa and Asia","f59b69b9":"<a id=\"Features\"><\/a>\n## Features\n\nAfter doing some research on audio signal classification, I have come up with the following features to extract from the audio files:\n\n- [Waveform](#Waveform)\n- [Autocorrelation](#Autocorrelation)\n- [Spectrogram](#Spectrogram)\n- [Chromagram](#Chromagram)\n- [Spectral](#Spectral)\n    - [Centroid](#Centroid)\n    - [Bandwidth](#Bandwidth)\n    - [Contrast](#Contrast)\n    - [Flatness](#Flatness)\n    - [Rolloff](#Rolloff)\n- [MFCC](#MFCC)\n\nWe'll do a sample feature extraction of bird code 'ameavo' as an example. (filename XC99571.mp3)\n\n<a id=\"Waveform\"><\/a>\n### Waveform","32cbd5bc":"<a id=\"AudioFeatureExtraction\"><\/a>\n# Audio Feature Extraction\n\n<a id=\"Sample\"><\/a>\n## Sample\nFirst, let's take 5 audio samples from the first 5 birds. We'll look at the waveforms and listen to the songs.","8be5da49":"<a id=\"Metadata\"><\/a>\n## Metadata\n\nThe train.csv file contains the metadata of the recording sample for that entry. From that metadata, we get these relevant features:\n\n- ebird_code\n- date\/time\n- location\n- recordist\n- filename","3ddd664a":"- autocorrelation very quickly falls off reaching almost 0 after a lag of about 500","d9f35f96":"- Luckily, none of our relevant data is missing","72f79fe7":"<a id=\"DateTime\"><\/a>\n### Date and Time\nThe date and time of the recording could have an impact on which bird is making the call. Some birds may usually call only at certain times, and some birds are only in certain locations during certain times of the year."}}