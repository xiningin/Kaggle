{"cell_type":{"acadcdcd":"code","d906a5d2":"code","2b033181":"code","432e9c18":"code","eef4a322":"code","134490f5":"code","1d894dae":"code","534bdc54":"code","a2daa6c1":"code","b8c28c73":"code","2209ede6":"code","3f5c57a4":"code","a6c651d9":"code","b7bff9ad":"code","173d0ff7":"code","f3b406eb":"code","dc33a531":"code","3e459630":"code","14d6adaf":"code","89f62044":"code","d33458a4":"code","b4a2304d":"code","ad004e9c":"code","aaa8f2cc":"code","31491ef0":"code","6b229eab":"code","8c02c18e":"code","e354cde0":"code","29b7dcd4":"code","e36a6836":"code","c0abd8da":"code","4afb315a":"code","91dee5a6":"code","1d2dd12b":"code","df8cdd34":"code","8f5d450e":"code","51f53346":"code","b943d0d0":"code","fbe8423f":"code","924f48dc":"code","5031fa48":"code","b066295a":"code","2ba6e1d7":"code","091afc1a":"code","d8b67cac":"code","385d59b3":"code","7773d78a":"code","97a441d2":"code","04df6dab":"code","4fce9507":"code","bbdc7da3":"code","b2d0553c":"code","0c2b1cf5":"code","5d7677f0":"code","bf177682":"markdown","296bbc7b":"markdown","9ec40341":"markdown","73099e00":"markdown","64dd360c":"markdown","9e94a58f":"markdown","b1612838":"markdown","6d60200d":"markdown","bc114e3b":"markdown","88345e66":"markdown","6ae71eb0":"markdown","7ba9c25e":"markdown","3c6ccc9d":"markdown","141aae68":"markdown","6321e952":"markdown","a62b3bdc":"markdown","75ed1be1":"markdown","2f738a39":"markdown","88a27330":"markdown","51f7e404":"markdown","e6277ab0":"markdown","150be047":"markdown","cfec0782":"markdown","0162edba":"markdown","5f6dcd61":"markdown","0d69aa94":"markdown","ef6395a6":"markdown"},"source":{"acadcdcd":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport plotly.express as px \nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport nltk\nimport string\nimport seaborn as sns \nfrom collections import Counter\nimport operator\nfrom math import ceil\nimport re\nfrom transformers import BertTokenizer,BertModel\nimport torch \nfrom torch import nn\nfrom torch.utils.data import DataLoader,TensorDataset\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Embedding,LSTM,Dense,Bidirectional,\\\nDropout,BatchNormalization,Input,Conv1D,MaxPool1D,Flatten\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.losses import mse\nfrom tensorflow.keras.callbacks import EarlyStopping , ModelCheckpoint\nimport tensorflow as tf\nimport pickle\nimport shutil \nimport random \nimport os \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\nimport math","d906a5d2":"SEED = 42\n\ndef random_seed(SEED):\n    \n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n\nrandom_seed(SEED)","2b033181":"# Load the train datas\ndata = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")","432e9c18":"# Let's take a look on the training datas :\ndata.head()","eef4a322":"# Let's take a look on test datas.\ntest = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ntest.head()","134490f5":"# words_count.\n\ndata[\"words_count\"] = data[\"excerpt\"].map(lambda x : len(x.split()))\n\n# unique_words_count\n\ndata[\"unique_words_count\"] = data[\"excerpt\"].map(lambda x : len(set(str(x).lower().split())))\n\n# stop_words_count \n\ndata[\"stop_words_count\"] = data[\"excerpt\"].map(lambda x : len([elt for elt in str(x).lower().split() if\\\n                                                              elt in nltk.corpus.stopwords.words(\"english\")]))\n# characters_count \n\ndata[\"characters_count\"] = data[\"excerpt\"].map(lambda x : len(str(x)))\n\n#mean_words_length\n\ndata[\"mean_words_length\"] = data[\"excerpt\"].map(lambda x : np.mean([len(elt) for elt in \\\n                                                                    x.split()]))\n\n#punctuation_count \n\ndata[\"punctuation_count\"] = data[\"excerpt\"].map(lambda x : len([elt for elt in str(x).split()\\\n                                                               if elt in string.punctuation]))\n","1d894dae":"# transform the target values to categorical variable,which represent the rate complexity \n# of reading passages for Quantile1 - Quantile4\ndata[\"target_bins\"] = pd.cut(data[\"target\"].values,[data[\"target\"].min()-1,data[\"target\"].quantile(0.25),\\\n                                                    data[\"target\"].quantile(0.5),data[\"target\"].quantile(0.75),data[\"target\"].quantile(1)],right=True,labels=[f\"Q{i}\" for i in range(1,5)])","534bdc54":"meta_features = [\"words_count\",\"unique_words_count\",\"stop_words_count\",\"characters_count\",\\\n                \"mean_words_length\",\"punctuation_count\"]\ntarget_bins = data[\"target_bins\"].unique()\nfig, ax = plt.subplots(3,2,figsize=(20,30))\nfor i,meta in enumerate(meta_features) :\n    for grade in target_bins :\n        mask = data[\"target_bins\"]== grade\n        sns.distplot(data[mask][meta],ax=ax[i\/\/2,i%2],label=grade,kde=False)\n        ax[i\/\/2,i%2].set_title(f\"{meta} distributions per grade\",fontsize=13)\n        ax[i\/\/2,i%2].set_xlabel(\" \")\n        ax[i\/\/2,i%2].tick_params(axis=\"x\",labelsize=13)\n        ax[i\/\/2,i%2].tick_params(axis=\"y\",labelsize=13)\n        ax[i\/\/2,i%2].legend()\nplt.show()","a2daa6c1":"# Investigate correlation between meta features and the target fature.\nfig = plt.figure(figsize=(20,5))\ncorrelation = pd.DataFrame({feature: data[\"target\"].corr(data[feature]) for feature in \\\n                           meta_features},index=[\"target\"])\nsns.heatmap(correlation,annot=True,square=True,cmap='BrBG',linewidths=0.5,\\\n            cbar_kws={\"shrink\": .5})\nplt.title(\"Correlation of Meta features with target\")","b8c28c73":"# Distribution of each words in our corpus.\ncorpus = []\nfor tx in data[\"excerpt\"].values :\n    corpus.extend(str(tx).lower().split())\ncounter = Counter(corpus)\ncounter = dict(counter)","2209ede6":"def check_embedding_coverage(embedding,counter):\n    \"\"\"This function allow to inspect the coverage of our corpus by the embedding word,\n       represented in GloVe technic. \n       \n       @param embedding(dict) : a dict of the words with their embedding coefficients in the used \n                                embedding technic.\n       @param counter(dict) : a dict of the words distributions in the corpus that we would \n                              examinate.\n       \n       @return coverage_text (int): the coverage of our corpus by the terms used in the \n                                    introduced embedding technic.\n       @return coverage_vocab(int) : the coverage of the vocab used in the corpus by the terms used\n                                in the introduced embedding technic.\n       @return oov (list) : ordered list of words,that not represented in the embedding technic \n                            terms.\n    \"\"\"\n    oov_words = {}\n    covered_words = {}\n    n_coov = 0\n    n_oov = 0\n    for word in counter :\n        try :\n            covered_words[word] = embedding[word]\n            n_coov += counter[word]\n        except :\n            oov_words[word] = counter[word]\n            n_oov += counter[word]\n    coverage_text = (n_coov\/(n_coov+n_oov)) * 100 \n    coverage_vocab = (len(covered_words)\/len(counter)) *100\n    oov = sorted(oov_words.items(),key=operator.itemgetter(1),reverse=True)\n    \n    return coverage_text,coverage_vocab,oov","3f5c57a4":"# load Glove embedding terms and weights.\nglove_embedding_file =  \"..\/input\/gloves-embedding-weights\/glove.6B.300d.txt\"\nglove_embedding = {}\nf = open(glove_embedding_file)\nfor l in f :\n    content = l.split()\n    glove_embedding[content[0]] = np.asarray(content[1:])","a6c651d9":"coverage_text,coverage_vocab,oov = check_embedding_coverage(glove_embedding,counter)\nprint(f\"Glove embedding cover {ceil(coverage_vocab)} % of vocabulary and {ceil(coverage_text)} % of text in our training corpus\")","b7bff9ad":"test_corpus = []\nfor tx in test[\"excerpt\"].values :\n    test_corpus.extend(tx.lower().split())\ntest_counter = Counter(test_corpus)","173d0ff7":"coverage_text,coverage_vocab,oov = check_embedding_coverage(glove_embedding,test_counter)\nprint(f\"Glove embedding cover {ceil(coverage_vocab)} % of vocabulary and {ceil(coverage_text)} % of text in our test corpus\")","f3b406eb":"def clean_text(excerpt):\n    punctuations = \".,?!;\\(\\\":-)\u2018\"\n    extrait = excerpt\n    for p in punctuations : \n      extrait = extrait.replace(p,f\" {p} \")\n    extrait = re.sub(r\"'s\",\" is \",extrait)\n    extrait = extrait.replace(\"i'm\",\"I'm\")\n    extrait = extrait.replace(\"don't\",\"do not\")\n    extrait = extrait.replace(\"didn't\",\"did not\")\n    extrait = extrait.replace(\"can't\",\"cannot\")\n    extrait = extrait.replace(\"i'll\",\"I will\")\n    extrait = extrait.replace(\"wouldn't\",\"would not\")\n    extrait = extrait.replace(\"i've\",\"I have\")\n    extrait = re.sub(r\"i've\",\"I have\",extrait)\n    extrait = extrait.replace(\"won't\",\"will not\")\n    extrait = extrait.replace(\"couldn't\",\"could not\")\n    extrait = extrait.replace(\"wasn't\",\"was not\")\n    extrait = extrait.replace(\"you'll\",\"you will\")\n    extrait = extrait.replace(\"isn't\",\"is not\")\n    extrait = extrait.replace(\"you're\",\"you are\")\n    extrait = extrait.replace(\"hadn't\",\"had not\")\n    extrait = extrait.replace(\"you've\",\"you have\")\n    extrait = extrait.replace(\"doesn't\",\"does not\")\n    extrait = extrait.replace(\"haven't\",\"have not\")\n    extrait = extrait.replace(\"they're\",\"they are\")\n    extrait = extrait.replace(\"we're\",\"we are\")\n    #extrait = re.sub(r\"(\/s+)i(\/s+)\",\"I\",excerpt)\n    #extrait = re.sub(r\"don't\",\"do not\",extrait)\n    #extrait = re.sub(r\"i'm\",\"I'm\",extrait)\n    #extrait = re.sub(r\"man's\",\"man is\",extrait)\n    #extrait = re.sub(r\"it's\",\"it is\",extrait)\n    #extrait = re.sub(r\"didn't\",\"did not\",extrait)\n    #extrait = re.sub(r\"can't\",\"cannot\",extrait)\n    #extrait = re.sub(r\"earth's\",\"earth is\",extrait)\n    #extrait = re.sub(r\"father's\",\"father is\",extrait)\n    #extrait = re.sub(r\"i'll\",\"I will\",extrait)\n    #extrait = re.sub(r\"i've\",\"I have\",extrait)\n    #extrait = re.sub(r\"i\\'\",r\"I'\",extrait)\n    #extrait = re.sub(r\"children\\'s\",\"children is\",extrait)\n    \n    return extrait ","dc33a531":"# clean the train datas\ndata[\"cleaned_excerpt\"] = data[\"excerpt\"].map(clean_text)","3e459630":"corpus = []\nfor tx in data[\"cleaned_excerpt\"].values :\n    corpus.extend(str(tx).lower().split())\ncounter = Counter(corpus)\ncoverage_text,coverage_vocab,oov = check_embedding_coverage(glove_embedding,counter)\nprint(f\"Glove embedding cover {ceil(coverage_vocab)} % of vocabulary and {ceil(coverage_text)} % of text in our cleaned training corpus\")","14d6adaf":"# clean the test datas.\ntest[\"cleaned_excerpt\"] = test[\"excerpt\"].map(clean_text)","89f62044":"test_corpus = []\nfor tx in test[\"cleaned_excerpt\"].values :\n    test_corpus.extend(str(tx).lower().split())\ncounter = Counter(test_corpus)\ncoverage_text,coverage_vocab,oov = check_embedding_coverage(glove_embedding,counter)\nprint(f\"Glove embedding cover {ceil(coverage_vocab)} % of vocabulary and {ceil(coverage_text)} % of text in our cleaned training corpus\")","d33458a4":"# split datas to train and test datas.\ndata_tr,data_ts,ytr,yts = train_test_split(data,data[\"target\"],test_size=\\\n                                  0.2,stratify=data[\"target_bins\"].values,random_state=42)","b4a2304d":"lstm_tokenizer = Tokenizer(27892,oov_token=\"<OOV>\")\nlstm_tokenizer.fit_on_texts(data[\"cleaned_excerpt\"].values)\nword_index = lstm_tokenizer.word_index","ad004e9c":"def preprocessing_data(txs,tokenizer,max_len):\n    x = tokenizer.texts_to_sequences(txs)\n    x = pad_sequences(x,maxlen=max_len,padding=\"post\",truncating=\"post\")\n    return x","aaa8f2cc":"# plot the sequence length distribution.\nlengths = [len(seq.split()) for seq in data[\"cleaned_excerpt\"].values]\nplt.hist(lengths,bins=30)\nplt.title(\"Distribution of sequence length in our training dataset\",size=15,color=\"red\")\nplt.show()","31491ef0":"input_length = 280 ","6b229eab":"embedding_matrix = np.zeros((len(word_index)+1,300))\nfor word,i in lstm_tokenizer.word_index.items() :\n    if glove_embedding.get(word) is not None:\n        embedding_matrix[i,:] = glove_embedding.get(word)\n   ","8c02c18e":"embedding_layer = Embedding(len(word_index)+1,300,weights=[embedding_matrix],input_length=\\\n                           input_length,trainable=False)","e354cde0":"def lstm_model() :\n    model = Sequential([embedding_layer,Bidirectional(LSTM(64,return_sequences=True)),\\\n                   Bidirectional(LSTM(32,return_sequences=True)),Dense(1)])\n    model.compile(loss=\"mse\",optimizer=\"adam\",metrics=\"mse\")\n    return model ","29b7dcd4":"checkpoint_lstm = '.\/lstm\/checkpoint'\nearly_stopping = EarlyStopping(patience=20,min_delta=0.1,monitor=\"val_loss\")\ncheck_point = ModelCheckpoint(checkpoint_lstm,monitor=\"val_loss\",save_weights_only=True,\\\n                             save_best_only=True,mode=\"min\")","e36a6836":"st = StratifiedKFold(n_splits=5)\nlstm_models = []\nfig,ax = plt.subplots(1,5,figsize=(15,10))\nfor i,(tr_index,val_index) in enumerate(st.split(data_tr,data_tr[\"target_bins\"])):\n    tr_x,val_x = data_tr.reset_index(drop=True).loc[tr_index,\"cleaned_excerpt\"].values, data_tr.reset_index(drop=True).loc[val_index,\"cleaned_excerpt\"].values\n    tr_y,val_y = data_tr.reset_index(drop=True).loc[tr_index,\"target\"].values,data_tr.reset_index(drop=True).loc[val_index,\"target\"].values\n    tr_x = preprocessing_data(tr_x,lstm_tokenizer,input_length)\n    val_x = preprocessing_data(val_x,lstm_tokenizer,input_length)\n    model = lstm_model()\n    his = model.fit(tr_x,tr_y,validation_data=(val_x,val_y),batch_size=32,callbacks=[check_point,\\\n                                                                                    early_stopping],epochs=100)\n    model.load_weights(checkpoint_lstm)\n    lstm_models.append(model)\n    ax[i].plot(np.arange(1,len(his.history[\"loss\"])+1),his.history[\"loss\"],label=\"loss\")\n    ax[i].plot(np.arange(1,len(his.history[\"loss\"])+1),his.history[\"val_loss\"],label=\"val_loss\")\n    ax[i].legend()\n    ax[i].set_ylim([0,1])\n    ax[i].set_xlabel(\"epochs\")\n    ax[i].set_ylabel(\"loss\")\n    ax[i].set_title(\"Loss Monitoring over epoch for train and validation datas\",size=15)","c0abd8da":"xts_seq = preprocessing_data(data_ts[\"cleaned_excerpt\"].values,lstm_tokenizer,input_length)\nYts = np.mean(np.concatenate([np.mean(model.predict(xts_seq),axis=1) for model in lstm_models],axis=1),axis=1)\nMSE = mean_squared_error(yts,Yts)\nprint(f\"The RMSE error of the lstm model = {math.sqrt(MSE)}\")","4afb315a":"# prepare first prediction \nX = test[\"cleaned_excerpt\"].values\nX = preprocessing_data(X,lstm_tokenizer,input_length)\nprediction = np.mean(np.concatenate([np.mean(model.predict(X),axis=1) for model in lstm_models],axis=1),axis=1)","91dee5a6":"submission = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\nsubmission[\"target\"] = prediction \nsubmission.to_csv(\"submission.csv\",index=False)","1d2dd12b":"# Build cnn model.\ndef build_cnn_model():\n  inp = Input(shape=(input_length,))\n  emb = embedding_layer(inp)\n  conv1 = Conv1D(128,2,activation=\"relu\")(emb)\n  maxp1 = MaxPool1D(2)(conv1)\n  conv2 = Conv1D(128,2,activation=\"relu\")(maxp1)\n  maxp2 = MaxPool1D(2)(conv2)\n  conv3 = Conv1D(128,2,activation=\"relu\")(maxp2)\n  maxp3 = MaxPool1D(2)(conv3)\n  #conv4 = Conv1D(16,2,activation=\"relu\")(maxp3)\n  #maxp4 = MaxPool1D(2)(conv4)\n  fl = Flatten()(maxp3)\n  drop = Dropout(0.2)(fl)\n  batch = BatchNormalization()(drop)\n  out = Dense(60)(batch)\n  drop = Dropout(0.2)(out)\n  batch = BatchNormalization()(drop)\n  out = Dense(30)(batch)\n  drop = Dropout(0.2)(out)\n  batch = BatchNormalization()(drop)\n  out = Dense(1)(batch)\n  cnn_model = tf.keras.models.Model(inputs=[inp],outputs=[out])\n  cnn_model.compile(loss=\"mse\",optimizer=\"adam\",metrics=\"mse\")\n  return cnn_model","df8cdd34":"checkpoint_cnn = '.\/cnn\/checkpoint'\ncheck_point = ModelCheckpoint(checkpoint_cnn,monitor=\"val_loss\",save_weights_only=True,\\\n                             save_best_only=True,mode=\"min\")","8f5d450e":"cnn_models = []\nfig,ax = plt.subplots(1,5,figsize=(35,10))\nfor i ,(tr_index,val_index) in enumerate(st.split(data_tr,data_tr[\"target_bins\"])) :\n    x_tr,x_val = data_tr.reset_index(drop=True).loc[tr_index,\"cleaned_excerpt\"].values,data_tr.reset_index(drop=True).loc[val_index,\"cleaned_excerpt\"].values\n    y_tr,y_val = data_tr.reset_index(drop=True).loc[tr_index,\"target\"].values,data_tr.reset_index(drop=True).loc[val_index,\"target\"].values\n    \n    x_tr = preprocessing_data(x_tr,lstm_tokenizer,input_length)\n    x_val =preprocessing_data(x_val,lstm_tokenizer,input_length)\n    md = build_cnn_model()\n    cnn_hist = md.fit(x_tr,y_tr,validation_data=(x_val,y_val),epochs=100,batch_size=100,\\\n                     callbacks=[check_point])\n    md.load_weights(checkpoint_cnn)\n    cnn_models.append(md)\n    ax[i].plot(np.arange(1,101),cnn_hist.history[\"loss\"],label=\"loss\")\n    ax[i].plot(np.arange(1,101),cnn_hist.history[\"val_loss\"],label=\"val_loss\")\n    ax[i].legend()\n    ax[i].set_xlabel(\"epochs\",size=12)\n    ax[i].set_ylabel(\"loss\",size=12)\n    ax[i].set_title(\"Monitoring loss over epochs for train and validation datas.\",size=15)\nplt.show()","51f53346":"# Evaluate cnn models in test datas \nY_cnn = np.mean(np.concatenate([cnn_model.predict(xts_seq) for cnn_model in cnn_models],axis=1),axis=1)\nCnn_MSE = mean_squared_error(yts,Y_cnn)\nprint(f\"The RMSE error for CNN model = {math.sqrt(Cnn_MSE)}\")","b943d0d0":"# Prepare CNN submission.\ncnn_prediction = np.mean(np.concatenate([cnn_model.predict(X) for cnn_model in cnn_models],axis=1),axis=1)\ncnn_submission = submission\ncnn_submission[\"target\"] = cnn_prediction\n#cnn_submission.to_csv(\"submission.csv\",index=False)","fbe8423f":"coeff = []\nfor i in range(1,11):\n    coeff.append(i * 0.1)","924f48dc":"result_blended = []\nbest_coef_cnn = 0\nbest_rest = float(\"inf\")\nfor i in range(10) :\n    lstm_prediction = np.mean(np.concatenate([np.mean(model.predict(xts_seq),axis=1) for model in lstm_models],axis=1),axis=1)\n    prediction_cnn = np.mean(np.concatenate([cnn_model.predict(xts_seq) for cnn_model in cnn_models],axis=1),axis=1)\n    pred = coeff[i] * prediction_cnn + (1-coeff[i]) * lstm_prediction\n    #res = coeff[i] * cnn_model.evaluate(xts_seq,yts)[0] + (1-coeff[i]) * (mean_squared_error(np.mean(model.predict(xts_seq),axis=1),yts))\n    res = math.sqrt(mean_squared_error(yts,pred))\n    result_blended.append(res)\n    if res < best_rest :\n        best_coef_cnn = coeff[i]\n        best_rest = res","5031fa48":"plt.plot(coeff,result_blended)","b066295a":"best_coef_cnn # best coefficient to adopt for cnn model ","2ba6e1d7":"best_rest # RMSE OF BLENDED MODEL ","091afc1a":"blended_pred = 0.6 * cnn_prediction + 0.4 * prediction","d8b67cac":"blended_submission = submission\nblended_submission[\"target\"] = cnn_prediction\nblended_submission.to_csv(\"submission.csv\",index=False)","385d59b3":"def preprocessing_for_bert(tokenizer,text,max_len):\n    tokens_ids = []\n    mask_attention = []\n    for tx in text :\n        tok = tokenizer.encode_plus(tx,add_special_tokens=True,padding=\"max_length\",max_length=\\\n                              max_len,truncation=True)\n        tokens_ids.append(tok.get(\"input_ids\"))\n        mask_attention.append(tok.get(\"attention_mask\"))\n    \n    inp_ids = torch.tensor(tokens_ids)\n    mask_attention = torch.tensor(mask_attention)\n    \n    return inp_ids,mask_attention","7773d78a":"class BertRegressor(nn.Module):\n    def __init__(self,bert_base,freeze_layer=True):\n        super(BertRegressor,self).__init__()\n        self.bert = bert_base\n        self.dense = nn.Linear(768,120)\n        self.dropout = nn.Dropout(p=0.3)\n        self.batch_n = nn.BatchNorm1d(120)\n        self.freeze = freeze_layer\n        self.regressor = nn.Linear(120,1)\n        \n        if  freeze_layer :\n            for param in self.bert.parameters():\n                param.requires_grad = False\n    def forward(self,input_ids,mask_attention):\n        hidden_states = self.bert(input_ids,mask_attention)[0][:,0,:]\n        out = self.dropout(hidden_states)\n        out = self.dense(out)\n        out = self.batch_n(out)\n        out = self.dropout(out)\n        out = self.regressor(out)\n        return out ","97a441d2":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse :\n    print(\"No GPU available, using CPU instead\")\n    device = torch.device(\"cpu\")","04df6dab":"with open(\"..\/input\/bert-model-training\/bert_layer\",\"rb\") as f :\n    bert_base = pickle.load(f)","4fce9507":"bregressor = BertRegressor(bert_base)\nbregressor.load_state_dict(torch.load(\"..\/input\/bert-model-training\/bert_model\"),strict = False)\nbregressor.to(device)","bbdc7da3":"with open(\"..\/input\/bert-model-training\/tokenizer\",\"rb\") as f :\n    tokenizer = pickle.load(f)","b2d0553c":"batch_size = 8 \ntest_id,test_mask = preprocessing_for_bert(tokenizer,test[\"cleaned_excerpt\"].values,input_length)\ntest_dataset = TensorDataset(test_id,test_mask)\ntest_dataloader = DataLoader(test_dataset,batch_size=batch_size)","0c2b1cf5":"bert_prediction = []\nbregressor.eval()\nfor datas in test_dataloader :\n    inp_id,mask_att = (t.to(device) for t in datas)\n    with torch.no_grad() :\n        output = bregressor(inp_id,mask_att)\n        output = output.cpu().numpy()\n    bert_prediction.extend([elt[0] for elt in output])","5d7677f0":"b_submission = submission\nb_submission[\"target\"] = bert_prediction\n#b_submission.to_csv(\"submission.csv\",index=False)","bf177682":"We will do text cleaning based in the gloves embedding technic.","296bbc7b":"#### <b> LSTM Model  <\/b>:","9ec40341":"# <center> CommonLit Readability Prize\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/25914\/logos\/header.png?t=2021-04-01-15-58-06)","73099e00":"# 3. Modeling:","64dd360c":"#### <b> Blended CNN and LSTM model  <\/b>:","9e94a58f":"## <font color=red> We will come back , upvote and leave your comments to enrich the work.","b1612838":"# 0.Import Librairies :","6d60200d":"hist_comb = comb_model.fit(Xtr[0],ytr,validation_data=(Xts[0],yts),epochs=100,batch_size=32,\\\n                          callbacks=[combined_check_point])","bc114e3b":"xtr,xts,ytr,yts = train_test_split(data.loc[:,[\"cleaned_excerpt\",\"mean_words_length\",\\\n                                         \"characters_count\"]],data[\"target\"].values,test_size=0.2,random_state=42,\\\n                                   stratify=data[\"target_bins\"].values)","88345e66":"#### <b> Data preparation  <\/b>:","6ae71eb0":"best_combined = \".\/best_combined.pth\"\ncombined_check_point = ModelCheckpoint(best_combined,monitor=\"val_loss\",mode=\"min\",\\\n                                       save_weights_only=True,save_best_only=True)","7ba9c25e":"### <font color=#FF15A8> Meta Features :","3c6ccc9d":"# Build Combined model \n# This model will try to include all features and structure , which can affect the target.\n\ncnn_input = Input(shape=(input_length,))\nemb = embedding_layer(cnn_input)\nlstm_input = Bidirectional(LSTM(64,return_sequences=True))(emb)\nlstm = Bidirectional(LSTM(32,return_sequences=True))(lstm_input)\nconv_1 = Conv1D(120,2,activation=\"relu\")(lstm)\npool_1 = MaxPool1D(2)(conv_1)\nconv_2 = Conv1D(120,2,activation=\"relu\")(pool_1)\npool_2 = MaxPool1D(2)(conv_2)\nconv_3 = Conv1D(120,2,activation=\"relu\")(pool_2)\npool_3 = MaxPool1D(2)(conv_3)\nfl = Flatten()(pool_3)\n#lstm_input = Bidirectional(LSTM(64,return_sequences=True))(emb)\n#lstm = Bidirectional(LSTM(32,))(lstm_input)\n#meta_features_inp = Input(shape=(2,))\n#x = tf.keras.layers.concatenate([fl,lstm],axis=1)\ndrop = Dropout(0.2)(fl)\nbatch_n = BatchNormalization()(drop)\nout = Dense(60)(batch_n)\ndrop = Dropout(0.2)(out)\nbatch_n = BatchNormalization()(drop)\nout = Dense(30)(batch_n)\n#meta_features_inp = Input(shape=(2,))\ndrop = Dropout(0.2)(out)\nbatch_n = BatchNormalization()(drop)\nout = Dense(1)(batch_n)\n\n#meta_features_inp = BatchNormalization()(meta_features_inp)\n#x = tf.keras.layers.concatenate([out,meta_features_inp],axis=1)\n#drop = Dropout(0.2)(out)\n#batch_n = BatchNormalization()(drop)\n#out = Dense(1)(batch_n)\ncomb_model = tf.keras.models.Model(inputs=[cnn_input],outputs=out)\ncomb_model.compile(loss=\"mse\",optimizer = tf.keras.optimizers.Adam(lr=5e-5),\\\n            metrics=\"mse\")","141aae68":"#### <b> CNN Model  <\/b>:","6321e952":"### <font color=#FF15A8> LSTM +CNN :","a62b3bdc":"### <font color=#FF15A8> Embedding and Text Cleaning :","75ed1be1":"==> The table above , confirm the previous results which conclude that characters_count and mean_words_length could bring a valuable informations about our target. ","2f738a39":"comb_model.summary()","88a27330":"# 2.Explorations Datas Analysis (EDA):","51f7e404":"#### <b> Bert model  <\/b>:","e6277ab0":"Xtr = pre_processing_for_combined_model(xtr,lstm_tokenizer,input_length)\nXts = pre_processing_for_combined_model(xts,lstm_tokenizer,input_length)","150be047":"plt.plot(np.arange(1,101),hist_comb.history[\"loss\"],label=\"train_loss\")\nplt.plot(np.arange(1,101),hist_comb.history[\"val_mse\"],label=\"val_loss\")\nplt.legend(loc=\"best\")\nplt.xlim([1,100])\nplt.ylim([0,1])\nplt.title(\"Loss monitoring over epochs\")","cfec0782":"We will made the following cleaning in order to enhance coverage embedding of text and vocab :\n- The most common type of words that require cleaning , have punctuations at first or in the end of the word.THe words don't have embedding because the trailing punctuations. Punctuations : . , ? ! ; \" (\n- Typos and slang are corrected , and informal abbreviation are written in their long form.","0162edba":"#### <b>  Combined Model <\/b>:","5f6dcd61":"==> The meta features distributions per grade complexity of excerpt , show the following statements :\n\n- Routhly same  distribution of words_count,unique_words_count,stop_words_count and punctuation_count per grade : This can allow us to conclude that theses features don't have informations about the target.\n- characters_count and mean_words_length features , have apparently a differents distribution per grade, which confirm that theses features could bring informations about our target.  \n\nWe will hereunder investigate theses results by dressing the correlation of theses features with the target feature.\n","0d69aa94":"def pre_processing_for_combined_model(x,tokenizer,max_len):\n    x_text = x.iloc[:,0].values\n    x_tex_seq = tokenizer.texts_to_sequences(x_text)\n    x_tex_seq = pad_sequences(x_tex_seq,maxlen=max_len,padding=\"post\",truncating=\"post\")\n    \n    x_series = x.iloc[:,[1,2]].values\n    \n    return x_tex_seq , x_series","ef6395a6":"# 1.Import Datas:"}}