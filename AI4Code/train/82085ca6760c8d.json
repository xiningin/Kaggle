{"cell_type":{"8c07b4ea":"code","cbf30a9b":"code","0cf00db1":"code","abfcc133":"code","75076b0e":"code","cf422e91":"code","055676a5":"code","7a8f1dfc":"code","1120974e":"code","acff327c":"code","a507ed1f":"code","f915a089":"code","aa22321f":"code","7e5ee0da":"code","6a4581f8":"code","e018345b":"code","35c4b359":"code","0f55ad24":"code","9da2f043":"code","bb71f16b":"code","f7da6a3e":"code","3e321385":"code","fd034b00":"code","9ce281c4":"code","fe65f48d":"code","e1c92164":"code","a4833b51":"markdown","0612197c":"markdown","b8054ce8":"markdown","02439b82":"markdown","4c8f9f88":"markdown","7e3dd4b1":"markdown","dbe25f6a":"markdown","1d0af301":"markdown","ef2cfb61":"markdown","e8ed2130":"markdown","e0dca7c7":"markdown","796035df":"markdown","69da9378":"markdown","c0f09a3b":"markdown","2de34f32":"markdown","f7d7543c":"markdown","e654a303":"markdown","6e64e952":"markdown","d504e661":"markdown","f81c555c":"markdown","ee8196cb":"markdown","9a9ddafc":"markdown","c8ec2940":"markdown","c2ff58ca":"markdown","b21f40ec":"markdown","ac1139bc":"markdown","1dbad0a9":"markdown","7c50248c":"markdown","0837b841":"markdown","05d75baa":"markdown","6e156534":"markdown","b17fad12":"markdown"},"source":{"8c07b4ea":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Rectangle\nimport matplotlib.patches as patches\nimport matplotlib.cm as cm\nimport matplotlib.colors as mc\n\nimport seaborn as sns\n\n#For calculating  similarities and distances\nfrom scipy.spatial.distance import pdist, squareform\n\nfrom scipy.interpolate import interp1d\n\n#For dimension reduction\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nimport copy\n\nfrom IPython.core.display import display, HTML\nimport math\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport matplotlib.path as mpath\n\nimport warnings\n#Don't show warnings in the final K\nwarnings.filterwarnings('ignore')\n\nPath = mpath.Path\n\n#Load the multi-choice answers into a DataFrame\nmulti_df = pd.read_csv('..\/input\/kaggle-survey-2018\/' + 'multipleChoiceResponses.csv', low_memory=False, header=[0,1])\n\n#Store the Question ColumnName-QuestionText tuples\nquestions = pd.DataFrame(list(zip(multi_df.columns.get_level_values(0),multi_df.columns.get_level_values(1))))\n\n#Drop the long descriptions, we don't need them\nmulti_df.columns = multi_df.columns.droplevel(1)\n\n#Drop the personal questions\nPERSONAL_QUESTIONS = ['Time from Start to Finish (seconds)','Q1','Q1_OTHER_TEXT','Q2','Q3']\nmulti_df.drop(PERSONAL_QUESTIONS, axis=1, inplace=True)\n\n#Helper function to get multiple columns at once\ndef get_cols_with_prefix(df, prefix):\n    return [col for col in df.columns.values if col.startswith(prefix)]\n\n#Let's give these frequently used columns aliases:\nC_TITLE = 'Q6'\n#TODO: add Q6_original, Q24_Buckets\n\n#Drop those who didn't complete their title information\nmulti_df.dropna(subset=[C_TITLE], inplace=True)\n\n#These should be transformed to 1\/0\nBINARY_COLUMNS = (\n    get_cols_with_prefix(multi_df, \"Q11_Part\")+\n    get_cols_with_prefix(multi_df, \"Q13_Part\")+\n    get_cols_with_prefix(multi_df, \"Q14_Part\")+\n    get_cols_with_prefix(multi_df, \"Q15_Part\")+\n    get_cols_with_prefix(multi_df, \"Q16_Part\")+\n    get_cols_with_prefix(multi_df, \"Q19_Part\")+\n    get_cols_with_prefix(multi_df, \"Q21_Part\")+\n    get_cols_with_prefix(multi_df, \"Q27_Part\")+\n    get_cols_with_prefix(multi_df, \"Q28_Part\")+\n    get_cols_with_prefix(multi_df, \"Q29_Part\")+\n    get_cols_with_prefix(multi_df, \"Q30_Part\")\n)\n\n# Convert the Binary Columns\nmulti_df[BINARY_COLUMNS] = multi_df[BINARY_COLUMNS].notnull().astype(int)\n\n#These should be imputed when not available, then scaled\n#WARNING! after imputation the values won't add up to 100%!\nVALUE_COLUMNS = ['Q34_Part_1','Q34_Part_2','Q34_Part_3','Q34_Part_4','Q34_Part_5','Q34_Part_6']\n\n#Remove missing Value columns\nmulti_df.dropna(subset=VALUE_COLUMNS, inplace=True)\n#Convert percent values to 0-1 range\nmulti_df[VALUE_COLUMNS]=multi_df[VALUE_COLUMNS]\/100\n\n#Replace Q12 - Primary tool values - Convert to shorter values\nprimary_tool_map = {\n    'Cloud-based data software & APIs (AWS, GCP, Azure, etc.)':'Cloud',\n    'Basic statistical software (Microsoft Excel, Google Sheets, etc.)':'BasicStat',\n    'Local or hosted development environments (RStudio, JupyterLab, etc.)':'DevEnv',\n    'Advanced statistical software (SPSS, SAS, etc.)':'AdvancedStat',\n    'Other':'Other',\n    'Business intelligence software (Salesforce, Tableau, Spotfire, etc.)':'BI',\n    'nan':np.nan\n}\nmulti_df['Q12_MULTIPLE_CHOICE'] = multi_df['Q12_MULTIPLE_CHOICE'].apply(lambda tool: primary_tool_map.get(str(tool), tool))\ndummies = pd.get_dummies(multi_df[\"Q12_MULTIPLE_CHOICE\"],prefix=\"Q12_MC_Part\")\nmulti_df = pd.concat([multi_df, dummies], axis=1)\n\n#Replace Q17 - Most used Programming Language - Create dummy columns\ndummies = pd.get_dummies(multi_df[\"Q17\"],prefix=\"Q17_Part\")\nmulti_df = pd.concat([multi_df, dummies], axis=1)\n\n#Replace Q20 - Most used ML tool - Create dummy columns\ndummies = pd.get_dummies(multi_df[\"Q20\"],prefix=\"Q20_Part\")\nmulti_df = pd.concat([multi_df, dummies], axis=1)\n\n#Replace Q22 - DataViz tools - Create dummy columns\ndummies = pd.get_dummies(multi_df[\"Q22\"],prefix=\"Q22_Part\")\nmulti_df = pd.concat([multi_df, dummies], axis=1)\n\n#Replace Q23 - Time spent coding - Convert to 0-1 range\ntime_spent_map = {\n    '0% of my time': 0,\n    '1% to 25% of my time':1,\n    '25% to 49% of my time':2,\n    '50% to 74% of my time': 3,\n    '75% to 99% of my time':4,\n    '100% of my time': 5,\n    'nan':0\n}\nmulti_df['Q23'] = multi_df['Q23'].apply(lambda tool: time_spent_map.get(str(tool), tool))\/5\n\n#Replace Q26 - Do you consider yourself a Data Scientist? - Convert to 0-1 range\nself_ds_map = {\n    'Definitely yes':4,\n    'Probably yes':3,\n    'Maybe':2,\n    'Probably not':1,\n    'Definitely not':0,\n    'nan':np.nan\n}\nmulti_df['Q26'] = multi_df['Q26'].apply(lambda tool: self_ds_map.get(str(tool), tool))\/4\n\n#Replace Q48 - Black box\nblack_box_map = {\n'I am confident that I can explain the outputs of most if not all ML models':4,\n'I am confident that I can understand and explain the outputs of many but not all ML models':3,\n'I do not know; I have no opinion on the matter':2,\n'I view ML models as \"black boxes\" but I am confident that experts are able to explain model outputs':1,\n'Yes, most ML models are \"black boxes\"':0\n}\nmulti_df['Q48'] = multi_df['Q48'].apply(lambda tool: black_box_map.get(str(tool), tool))\/4\n\n#Merge some role groups\nroles_map = {\n'Research Assistant':'Research Scientist',\n'Principal Investigator':'Research Scientist',\n\n'DBA\/Database Engineer':'Data Engineer',\n\n'Marketing Analyst':'Data Analyst',\n'Data Journalist':'Data Analyst',\n'Business Analyst':'Data Analyst',\n'Salesperson':'Data Analyst',\n\n'Student':np.nan,\n'Not employed':np.nan,\n'Consultant':\"Other\",\n'Other':np.nan,\n'Product\/Project Manager':np.nan,\n'Chief Officer':np.nan,\n'Manager':np.nan,\n'Developer Advocate': np.nan,\n}\n\n#Replace values in the map, keep unmapped values unchanged\nmulti_df['Q6_original']=multi_df['Q6']\nmulti_df['Q6'] = multi_df['Q6'].apply(lambda role: roles_map.get(role, role))\n\nmulti_df['Q8'].unique()\nexperience_map = {\n    '0-1':'0',\n    '1-2':'1',\n    '2-3':'2',\n    '3-4':'3',\n    '4-5':'4',\n    '5-10':'5',\n    '10-15':'10',\n    '15-20':'15',\n    '20-25':'20',\n    '25-30':'25',\n    '30 +':'30',\n    'nan':np.nan\n}\nmulti_df['Q8_Buckets'] = multi_df['Q8'].apply(lambda tool: experience_map.get(str(tool), tool))\nmulti_df['Q8_Buckets'].unique()\nmulti_df['Q8_Buckets'] = multi_df['Q8_Buckets'].astype(float)\n\nmulti_df['Q24'].unique()\ncode_experience_map = {\n    'I have never written code but I want to learn':'1',\n    '< 1 year':'1',\n    '1-2 years':'2',\n    '3-5 years':'3',\n    '5-10 years':'4',\n    '10-20 years':'5',\n    '20-30 years':'5',\n    '30-40 years':'5',\n    '40+ years':'5',\n    'I have never written code and I do not want to learn':np.nan,\n    'nan':np.nan\n}\n\nQ24_bucket_labels = {\n\"1\":\"Intern\",\n\"2\":\"Junior\",\n\"3\":\"Middle\",\n\"4\":\"Senior\",\n\"5\":\"Veteran\"\n}\nmulti_df['Q24_Buckets'] = multi_df['Q24'].apply(lambda tool: code_experience_map.get(str(tool), tool))\nmulti_df['Q24_Buckets'].unique()\nmulti_df['Q24_Buckets'] = multi_df['Q24_Buckets'].astype(float)","cbf30a9b":"arrow_svg = \"\"\"\n<svg version=\"1.1\" id=\"Arrow_down\" xmlns=\"http:\/\/www.w3.org\/2000\/svg\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\" x=\"0px\" y=\"0px\"\n viewBox=\"0 0 20 20\" enable-background=\"new 0 0 20 20\" xml:space=\"preserve\">\n<path d=\"M10,17.5L3.5,11H7V3h6v8h3.5L10,17.5z\"\/>\n\"\"\"\n\nhtml = \"\"\"\n<style>\n    .mv-title-container{\n        display:flex;\n        flex-direction:column;\n        align-items:center;\n        justify-content:center;\n        height:auto;\n    }\n    .mv-title-container h1{\n        white-space:pre-wrap;\n        text-align:center;\n    }\n    \n    .mv-title-keyword{\n        color:#5DADE2;\n    }\n    .mv-title-arrow{\n        color: #5DADE2;\n    }\n    \n    .mv-title-arrow svg{\n        fill:#5DADE2;\n        stroke:none;\n        width:100px;\n    }\n<\/style>\n\n<div class=\"mv-title-container\">\n    <h1><span>If you have completed the Kaggle ML & DS Survey 2018, <\/span>\n<span class=\"mv-title-keyword\">one of these dots is You.<\/span><\/h1>\n        \n\n    <div class=\"mv-title-arrow\">\"\"\"+arrow_svg+\"\"\"<\/div>\n<\/div>\n\"\"\"\n\ndisplay(HTML(html))","0cf00db1":"#Give each of the original titles a color\ncolor_map_original = {\n    'Consultant':'black', \n    'Other':'black',  \n    'Data Journalist':'black', \n    'Not employed':'black', \n    \n    'Student':'lightgreen', \n    \n    'Research Assistant':'green', \n    'Research Scientist':'green', \n    'Principal Investigator':'green', \n    \n    'Data Analyst':'orange', \n    'Business Analyst':'orange', \n    'Marketing Analyst':'orange', \n    'Salesperson':'orange', \n    \n    'Data Engineer':'purple',\n    'DBA\/Database Engineer':'purple',\n    \n    'Developer Advocate':'blue',  \n    'Software Engineer':'blue', \n    \n    'Data Scientist':'red', \n    \n    'Chief Officer':'lightgray', \n    'Manager':'lightgray', \n    'Product\/Project Manager':'lightgray', \n    \n    'Statistician':'yellow', \n    \n}\n\n#Give each of the new titles a color\ncolors = [\n    'red', #Data Scientist\n    'green',  #Data Analyst\n    'blue', #Software Engineer\n    'orange', #Research Scientist\n    'yellow',#Statistician\n    'purple', #Data Engineer\n    'lemonchiffon',  #Other \n]\n\n\ncolor_maps = {\n    'red':'Reds', #Data Scientist\n    'green':'Greens',  #Data Analyst\n    'blue':'Blues', #Software Engineer\n    'orange':'Oranges', #Research Scientist\n    'purple':'Purples', #Data Engineer\n     'gray':'Greys',#Statistician,\n    'yellow':'Oranges' #Other\n}\n\n#Keep only columns which are potentially good predictors of the role\nrelevant_columns_for_roles_reduction = (\n    get_cols_with_prefix(multi_df, \"Q11_Part\")+ #primary activities\n    get_cols_with_prefix(multi_df, \"Q16_Part\")+ #programming languages used\n    get_cols_with_prefix(multi_df, \"Q34_Part\")+ #time spent with activities (smaller than 1)\n    get_cols_with_prefix(multi_df, \"Q12_MC_Part\")+ #primary tool\n    get_cols_with_prefix(multi_df, \"Q28_Part\")+ #machine learning products\n    get_cols_with_prefix(multi_df, \"Q30_Part\")+ #big data products\n    get_cols_with_prefix(multi_df, \"Q17_Part\")+ #primary language\n    [\"Q23\" ] #time spent coding (smaller than 1)\n)\nroles_df = multi_df[relevant_columns_for_roles_reduction].copy()\n\n#Exclude the null values when computing the LDA\nset_roles_titles = multi_df[C_TITLE].dropna()\nset_roles_df = roles_df[multi_df[C_TITLE].notnull()]\n\ncentroids = set_roles_df.groupby(set_roles_titles).mean()\n\nset_roles_original_titles = multi_df[\"Q6_original\"].dropna()\nset_roles_original_df = roles_df[multi_df[\"Q6_original\"].notnull()]\ncentroids_original = set_roles_original_df.groupby(set_roles_original_titles).mean()\n\nlda = LinearDiscriminantAnalysis(n_components=2, priors = set_roles_titles.value_counts(normalize=True).values)\n\ndrA = lda.fit_transform(set_roles_df, set_roles_titles)\n\nprincipalDf = pd.DataFrame(data = drA\n             , columns = ['principal component 1', 'principal component 2'], index=set_roles_df.index)\nfinalDf = pd.concat([principalDf, set_roles_titles], axis = 1)\n\nfig = plt.figure(figsize = (16,10))\nax = fig.add_subplot(1,1,1) \n#ax.set_xlabel(PRINCIPAL_COMP_1_NAME, fontsize = 15)\n#ax.set_ylabel(PRINCIPAL_COMP_2_NAME, fontsize = 15)\nax.set_title('The Data Science Community', fontsize = 20)\n\ntargets_colors = [\n    (\"Other\",\"gray\"),\n    (\"Research Scientist\",\"green\"),\n    (\"Software Engineer\",\"blue\"),\n    ('Data Scientist',\"red\"),\n    ('Data Analyst', \"orange\"),\n    (\"Data Engineer\",\"purple\"),\n    (\"Statistician\",\"yellow\")\n]\n\ntarget_order= {\n    'Data Scientist':0, \n    'Data Analyst':1, \n    'Software Engineer':2,\n    'Research Scientist':3, \n    'Statistician':4,\n    'Data Engineer':5, \n    \"Other\":6\n}\n\nfor target, color in targets_colors:\n    indicesToKeep = finalDf[C_TITLE] == target\n    a = 0.4\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               ,alpha=a\n               , s = 15)\n    \nhandles, labels = ax.get_legend_handles_labels()\n# sort both labels and handles by labels\nlabels, handles = zip(*sorted(zip([target for target, color in  targets_colors], handles), key=lambda t: target_order[t[0]]))\nlegend = ax.legend(handles, labels)\n\n#legend = ax.legend(targets)\nlegend.get_frame().set_edgecolor(\"white\")\n\nax.set_xticks([])\nax.set_yticks([])\n\n\n\nmin_x, max_x = ax.get_xlim()\nmin_y, max_y = ax.get_ylim()\n\nx_offset = 0.01\ny_offset = 0.05\nax.text(min_x-x_offset, max_y,\"More Math\", ha=\"right\", va=\"top\", rotation=90, fontsize=14)\nax.text(min_x-x_offset, min_y,\"More Engineering\", ha=\"right\", va=\"bottom\", rotation=90, fontsize=14)\n\nax.text(min_x, min_y-y_offset,\"More Business\", ha=\"left\", va=\"top\", fontsize=14, color=\"black\")\nax.text(max_x, min_y-y_offset,\"More Code\", ha=\"right\", va=\"top\", fontsize=14, color=\"black\")\nplt.setp(ax.spines.values(), color=\"gray\")\n\n# Hide the right and left spines\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines[\"top\"].set_color(\"gray\")\nax.spines[\"right\"].set_color(\"gray\")","abfcc133":"sizes = multi_df.groupby(\"Q6_original\").size()\nmin_size = sizes.min()\nmax_size = sizes.max()\n\n\nPRINCIPAL_COMP_1_NAME = \"Business \u2194 Code\"\nPRINCIPAL_COMP_2_NAME = \"Engineering \u2194 Math\"\n\n\nfig = plt.figure(figsize = (10,10))\nax = fig.add_subplot(1,1,1) \n#ax.set_xlabel(PRINCIPAL_COMP_1_NAME, fontsize = 15)\n#ax.set_ylabel(PRINCIPAL_COMP_2_NAME, fontsize = 15)\nax.set_title('Cluster Centroids', fontsize = 20)\ntargets = np.setdiff1d(multi_df[\"Q6_original\"].dropna().unique(),[\"Other\"])\ndot_size_min = 100\ndot_size_max = 1000\n\nfor target in targets:\n    vector = pd.DataFrame(centroids_original.loc[target]).T\n    size_p = (sizes[target] - min_size) \/(max_size - min_size)\n    size = dot_size_min + size_p * (dot_size_max - dot_size_min)\n    coords = lda.transform(vector)[0]\n    x = [coords[0]]\n    y =  [coords[1]]\n    radius = (size \/ 3.14) ** 0.5\n    ax.scatter(x,y, c = color_map_original[target],alpha=0.4, s = size)\n    ax.annotate(target, \n                xy = (x[0], y[0]), \n                xytext = (0,2 + radius), \n                textcoords='offset points', \n                horizontalalignment=\"center\")\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\n\nmin_x, max_x = ax.get_xlim()\nmin_y, max_y = ax.get_ylim()\n\nx_offset = 0.14\ny_offset = 0.14\nax.text(min_x-x_offset, max_y,\"More Math\", ha=\"right\", va=\"top\", rotation=90, fontsize=14)\nax.text(min_x-x_offset, min_y,\"More Engineering\", ha=\"right\", va=\"bottom\", rotation=90, fontsize=14)\n\nax.text(min_x, min_y-y_offset,\"More Business\", ha=\"left\", va=\"top\", fontsize=14, color=\"black\")\nax.text(max_x, min_y-y_offset,\"More Code\", ha=\"right\", va=\"top\", fontsize=14, color=\"black\")\nplt.setp(ax.spines.values(), color=\"gray\")\n\n# Hide the right and left spines\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n#ax.spines[\"top\"].set_color(\"gray\")\n#ax.spines[\"right\"].set_color(\"gray\")","75076b0e":"relevant_columns_for_individual_roles=(\n    get_cols_with_prefix(multi_df, \"Q11_Part\")+ #primary activities\n    get_cols_with_prefix(multi_df, \"Q12_MC_Part\")+ #primary tool\n    [\"Q23\" ]+ #time spent coding (smaller than 1)\n    [\"Q26\" ]+ #are you a Data Scientist?\n    get_cols_with_prefix(multi_df, \"Q34_Part\")+ #time spent with activities (smaller than 1)\n    get_cols_with_prefix(multi_df, \"Q17_Part\")+ #primary language\n    [\"Q48\"]  \n)\n\npositive_color=\"#117bd7\"\nnegative_color=\"orange\"\n\n# Drop the Other values from the Dataframe\nmulti_df.loc[(multi_df[C_TITLE]==\"Other\"),C_TITLE]=np.nan\nindividual_roles_df = multi_df[relevant_columns_for_individual_roles].copy()\nset_roles_titles = multi_df[C_TITLE].dropna()\nset_individual_roles_df = individual_roles_df[multi_df[C_TITLE].notnull()]\nglobal_mean = set_individual_roles_df.mean()\nindividual_centroids = set_individual_roles_df.groupby(set_roles_titles).mean()\n\nrelevant_q11_columns = [\n    'Q11_Part_1',\n    'Q11_Part_2',\n    'Q11_Part_3',\n    'Q11_Part_4',\n    'Q11_Part_5',\n    \"Q23\"\n]\nq11_column_labels = [\"Analyze Data\",\"Build ML Service\",\"Build Infrastructure\",\"Explore Applying ML\",\"Research ML\",\"Write Code\"]\nq11_centroids = individual_centroids[relevant_q11_columns]\nq11_centroids.columns=q11_column_labels\nq11_global_mean = global_mean[relevant_q11_columns]\nq11_global_mean.index = q11_column_labels\nq11_data = (q11_centroids - q11_global_mean).T\nq11_data_ranks = q11_data.rank(axis=1)\n\ndef visualize_role_differences(data, ranks, global_mean, global_x_max):\n    row_height = 1\n\n    # plot it\n    f, axs  = plt.subplots(7,8, gridspec_kw = {'width_ratios':[0.8]+[1]*7,'hspace':0,'wspace':0.35}, figsize=(16,row_height * len(data)))\n\n\n\n    #rows\n    for i in range(0,6):\n\n        # Row Labels\n        ax = axs[i,0]\n        ax.grid(False)\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n        # Hide the right and left spines\n        ax.spines['right'].set_visible(False)\n        ax.spines['left'].set_visible(False)\n        for spine in ax.spines.values():\n            spine.set_visible(False)\n            spine.set_edgecolor('lightgray')\n\n        ax.text(1.5, 0.5,data.index[i], ha=\"right\", va=\"center\")\n        ax.set_xlim(left=-1, right=1)\n        ax.set_ylim(bottom=0, top=1)\n\n        # Mean charts\n        ax = axs[i,1]\n\n        if (i == 0):\n            ax.set_title(\"Overall Mean\")\n\n        ax.grid(False)\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n        # Hide the right and left spines\n        ax.spines['right'].set_visible(False)\n        ax.spines['left'].set_visible(False)\n        for spine in ax.spines.values():\n                spine.set_edgecolor('lightgray')\n        value = global_mean[i]\n\n        rect = patches.Rectangle((0,0.1),value,0.8,linewidth=1,edgecolor='black',facecolor=\"gray\",alpha=0.5)\n        ax.add_patch(rect)\n        pct = int(round(value * 100))\n        ax.text(value + 0.05*global_x_max, 0.5,\"{}%\".format(pct), ha=\"left\", va=\"center\")\n        ax.set_xlim(xmin=0, xmax=global_x_max)\n        ax.set_ylim(ymin=0, ymax=1)\n\n        #columns\n        for j in range(2,8):\n            #[row,column]\n            ax = axs[i,j]\n            if (i == 0):\n                ax.set_title(data.columns[j-2])\n            # Hide the right and left spines\n            ax.spines['right'].set_visible(False)\n            ax.spines['left'].set_visible(False)\n\n            value = data.values[i,j-2]\n            rank = ranks.values[i,j-2]\n            color = positive_color if value > 0 else negative_color\n            offset = +1 if value > 0 else -1\n            sign = \"+\" if value > 0 else \"\"\n            rect = patches.Rectangle((0,0.1),value,0.8,linewidth=1,edgecolor='black',facecolor=color,alpha=0.7)\n            ax.add_patch(rect)\n            if (rank == 1 or rank == 6):\n                circle = patches.Ellipse((offset * -0.1 * global_x_max,0.5),\n                                         width=0.05*global_x_max,\n                                         height=0.1,\n                                         linewidth=1,\n                                         edgecolor='black',\n                                         facecolor=color,\n                                         alpha=0.7\n                                         )\n                ax.add_patch(circle)\n\n            pct = int(round(value * 100))\n            alignment = \"left\" if value > 0 else \"right\"\n            ax.text(value + offset * 0.01, 0.5,\"{}{}%\".format(sign, pct), ha=alignment, va=\"center\")\n            ax.plot([0,0], [1,0],linestyle=\"--\",linewidth=1,color=\"gray\")\n            ax.set_xlim(xmin=-0.5*global_x_max, xmax=0.5*global_x_max)\n            ax.set_ylim(ymin=0, ymax=1)\n            ax.grid(False)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            for spine in ax.spines.values():\n                spine.set_edgecolor('lightgray')\n\n        ax = axs[6,0]\n        ax.grid(False)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        ax.spines['left'].set_visible(False)\n        ax.spines['bottom'].set_visible(False)\n\n        ax = axs[6,1]\n        ax.grid(False)\n        ax.set_xlim(left=0, right=1)\n        ax.set_ylim(bottom=0, top=1)\n        ax.spines['right'].set_visible(False)\n        ax.spines['left'].set_visible(False)\n        ax.spines['bottom'].set_visible(False)\n        ax.tick_params(top=True,\n                           bottom=False,\n                           right=False,\n                           left=False,\n                           labeltop=False,\n                           labelbottom=False,\n                           labelright=False,\n                           labelleft=False,\n                           colors=\"gray\",\n                           direction=\"in\")\n        for spine in ax.spines.values():\n                spine.set_edgecolor('gray')\n        ax.text(0, 0.6,\"0%\", ha=\"center\", va=\"bottom\", fontsize=8, fontweight=100, color=\"gray\")\n        ax.text(0.5, 0.6,\"{}%\".format(int(50*global_x_max)), ha=\"center\", va=\"bottom\", fontsize=8, fontweight=100, color=\"gray\")\n        ax.text(1, 0.6,\"{}%\".format(int(100*global_x_max)), ha=\"center\", va=\"bottom\", fontsize=8, fontweight=100, color=\"gray\")\n\n        #columns\n        for j in range(2,8):\n            ax = axs[6,j]\n            ax.set_xlim(left=-0.5*global_x_max, right=0.5*global_x_max)\n            ax.set_ylim(bottom=0, top=1)\n            ax.tick_params(top=True,\n                           bottom=False,\n                           right=False,\n                           left=False,\n                           labeltop=False,\n                           labelbottom=False,\n                           labelright=False,\n                           labelleft=False,\n                           colors=\"gray\",\n                           direction=\"in\")\n            for spine in ax.spines.values():\n                spine.set_edgecolor('gray')\n\n            ax.spines['right'].set_visible(False)\n            ax.spines['left'].set_visible(False)\n            ax.spines['bottom'].set_visible(False)\n            \n            ax.set_xticks([-0.5*global_x_max,0,0.5*global_x_max])\n            ax.set_yticks([])\n        \n            ax.text(-0.5*global_x_max, 0.6,\"-{}%\".format(int(50*global_x_max)), ha=\"center\", va=\"bottom\", fontsize=8, fontweight=0, color=\"gray\")\n            ax.text(0, 0.6,\"0%\", ha=\"center\", va=\"bottom\", fontsize=8, fontweight=0, color=\"gray\")\n            ax.text(0.5*global_x_max, 0.6,\"+{}%\".format(int(50*global_x_max)), ha=\"center\", va=\"bottom\",fontsize=8, fontweight=0, color=\"gray\")","cf422e91":"visualize_role_differences(q11_data,q11_data_ranks, q11_global_mean, 1)","055676a5":"relevant_q34_columns = [\n    'Q34_Part_1',\n    'Q34_Part_2',\n    'Q34_Part_3',\n    'Q34_Part_4',\n    'Q34_Part_5',\n    'Q34_Part_6'\n]\nq34_column_labels = [\n    \"Gathering Data\",\n    \"Cleaning Data\",\n    \"Visualizing Data\",\n    \"Building Model\",\n    \"Running Model\",\n    \"Analyzing Data\"]\nq34_centroids = individual_centroids[relevant_q34_columns]\nq34_centroids.columns=q34_column_labels\nq34_global_mean = global_mean[relevant_q34_columns]\nq34_global_mean.index = q34_column_labels\n\nq34_data = (q34_centroids - q34_global_mean).T\n\nq34_data_ranks = q34_data.rank(axis=1)\n\nvisualize_role_differences(q34_data,q34_data_ranks, q34_global_mean, 0.25)","7a8f1dfc":"relevant_q12_columns = [\n    'Q12_MC_Part_AdvancedStat',\n    'Q12_MC_Part_BI',\n    'Q12_MC_Part_BasicStat',      \n    'Q12_MC_Part_Cloud',\n    'Q12_MC_Part_DevEnv',\n    'Q12_MC_Part_Other'\n]\n\nq12_column_labels = [\n    \"Advanced Statistics\",\n    \"Business Intelligence\",\n    \"Basic Statistics\",\n    \"Cloud Computing\",\n    \"Dev Environment\",\n    \"Other\"\n]\n\nq12_centroids = individual_centroids[relevant_q12_columns]\nq12_centroids.columns=q12_column_labels\n\nq12_global_mean = global_mean[relevant_q12_columns]\nq12_global_mean.index = q12_column_labels\n\nq12_data = (q12_centroids - q12_global_mean).T\n\nq12_data_ranks = q12_data.rank(axis=1)\n\nvisualize_role_differences(q12_data,q12_data_ranks, q12_global_mean, 1)","1120974e":"def negative_count(g):\n    pcts = g.value_counts(normalize=True, sort = False).sort_index()\n    return pcts.iloc[0]+pcts.iloc[1]+pcts.iloc[2]\/2\n    \ndef positive_count(g):\n    pcts = g.value_counts(normalize=True,sort = False).sort_index()\n    return pcts.iloc[3]+pcts.iloc[4]+pcts.iloc[2]\/2\n\nq26_value_labels = [\n    'Definitely not',\n    'Probably not',\n    'Maybe',\n    'Probably yes',\n    'Definitely yes'    \n]\nq26_global_mean = set_individual_roles_df[\"Q26\"].value_counts(normalize=True).sort_index()\nq26_mean_by_role = set_individual_roles_df.groupby(set_roles_titles)[\"Q26\"].value_counts(normalize=True).sort_index().unstack()\nq26_mean_by_role.columns = q26_value_labels\nq26_sentiment_count = set_individual_roles_df.groupby(set_roles_titles)[\"Q26\"].agg([positive_count, negative_count])\nq26_data = pd.concat([q26_mean_by_role,q26_sentiment_count], axis=1).sort_values(by=[\"negative_count\"])\nq26_data.name = \"Do you consider yourself to be a data scientist?\"\n\nq48_value_labels = [\n    'Most models are black boxes',\n    'I do not know \/ No opinion',\n    'Experts can explain them',\n    'I can explain many',\n    'I can explain most'\n]\nq48_global_mean = set_individual_roles_df[\"Q48\"].value_counts(normalize=True).sort_index()\nq48_mean_by_role = set_individual_roles_df.groupby(set_roles_titles)[\"Q48\"].value_counts(normalize=True).sort_index().unstack()\nq48_mean_by_role.columns = q48_value_labels\nq48_sentiment_count = set_individual_roles_df.groupby(set_roles_titles)[\"Q48\"].agg([positive_count, negative_count])\nq48_data = pd.concat([q48_mean_by_role,q48_sentiment_count], axis=1).sort_values(by=[\"negative_count\"])\nq48_data.name = \"Do you consider Machine Learning models to be black boxes?\"\n\n\nsentiment_color_map=[\"#363299\",\"#117bd7\",\"#25b4a7\",\"#86bf76\",\"#f9d229\"]\n\n\ndef visualize_opinions(data):\n    row_height = 0.5\n\n    norm = mc.Normalize(vmin=-data.iloc[0][\"negative_count\"], vmax=data.iloc[0][\"positive_count\"], clip=True)\n    mapper = cm.ScalarMappable(norm=norm, cmap=cm.viridis)\n\n\n\n\n    # plot it\n    f, axs  = plt.subplots(6,3, \n                           gridspec_kw = {'width_ratios':[1,7,2],'hspace':0,'wspace':0.35}, \n                           figsize=(15,row_height * len(data)))\n\n    ax = axs[0,1]\n    ax.set_title(data.name, loc=\"center\")\n\n    #rows\n    for i in range(6):\n        # Row labels\n        ax = axs[i,0]\n\n        ax.grid(False)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        for spine in ax.spines.values():\n            spine.set_visible(False)\n        ax.text(1, 0.5,data.index[i], ha=\"right\", va=\"center\", fontsize=14)\n        ax.set_xlim(xmin=-1, xmax=1)\n        ax.set_ylim(ymin=0, ymax=1)\n\n        #Charts\n        ax = axs[i,1]\n        ax.grid(True)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        for spine in ax.spines.values():\n            spine.set_visible(False)\n            ax.set_xlim(xmin=-1, xmax=1)\n            ax.set_ylim(ymin=0, ymax=1)\n\n        subset = data.iloc[i]\n        start =  - subset[\"negative_count\"]\n        for j in range(5):\n            color = sentiment_color_map[j]\n            value = subset.iloc[j]\n            #color = mapper.to_rgba(start)\n            rect = patches.Rectangle((start,0.1),value,0.8,linewidth=1,edgecolor='black',facecolor=color,alpha=0.9)\n            ax.add_patch(rect)\n            start += value\n\n\n        ax = axs[i,2]\n        ax.grid(True)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        for spine in ax.spines.values():\n            spine.set_visible(False)\n            ax.set_xlim(xmin=0, xmax=1)\n            ax.set_ylim(ymin=0, ymax=1)\n\n\n    ax = axs[5,1]\n    ax.spines['bottom'].set_visible(True)\n    ax.spines['bottom'].set_edgecolor('gray')\n    ax.set_xticks([-1,0,1])\n    ax.grid(False)\n    ax.tick_params(top=False, \n\n                           bottom=True, \n                           right=False, \n                           left=False, \n                           labeltop=False,\n                           labelbottom=False,\n                           labelright=False,\n                           labelleft=False,\n                           colors=\"gray\",\n                           direction=\"out\")\n    ax.text(-1,  -0.6,\"-100%\", ha=\"center\", va=\"bottom\", fontsize=8, fontweight=0, color=\"gray\")\n    ax.text(0,  -0.6,\"0%\", ha=\"center\", va=\"bottom\", fontsize=8, fontweight=0, color=\"gray\")\n    ax.text(1, -0.6,\"+100%\", ha=\"center\", va=\"bottom\",fontsize=8, fontweight=0, color=\"gray\")\n\n    ax = axs[5,2]\n    ax.set_xlim(xmin=0, xmax=10)\n    ax.set_ylim(ymin=0, ymax=1)\n    for i in range(5):\n        y = 3 + i *  0.6\n        ax.text(1, y ,data.columns[i], ha=\"left\", va=\"center\", fontsize=12, fontweight=\"normal\", color=\"black\")\n        ax.text(0, y+0.04 ,\"\u25a0\", ha=\"left\", va=\"center\", fontsize=12, fontweight=\"normal\", color=sentiment_color_map[i])\n\n    plt.show()","acff327c":"visualize_opinions(q48_data)","a507ed1f":"visualize_opinions(q26_data)","f915a089":"drA = lda.transform(set_roles_original_df)\nprincipalDf = pd.DataFrame(data = drA\n             , columns = ['principal component 1', 'principal component 2'], index=set_roles_original_df.index)\nfinalDf = pd.concat([principalDf, set_roles_original_titles], axis = 1)\n\n\nmapped_centroids = finalDf.groupby(\"Q6_original\").median()\n\nself_assessment = multi_df.groupby(multi_df[\"Q6_original\"])[\"Q26\"].mean()\nself_assessment.index.names=[\"Role\"]\nself_assessment.name = \"Self Assessment\"\n\ndistances = pdist(centroids_original, metric='cosine')\ndist_matrix = squareform(distances)\n\ncentroid_distances = pd.DataFrame(dist_matrix, index = mapped_centroids.index, columns=mapped_centroids.index)\ncentroid_distances.index.names = ['Role']\n\ndata_scientist_similarity = centroid_distances.loc[\"Data Scientist\"]\ndata_scientist_similarity.name=\"Similarity\"\n\nbubble_plot_data = pd.concat([self_assessment,data_scientist_similarity], axis=1, sort=True)\n\nmin_x, min_y = bubble_plot_data.min(axis=0)\nmax_x, max_y = bubble_plot_data.max(axis=0)\nbubble_plot_data_normalized = bubble_plot_data.copy()\nbubble_plot_data_normalized[\"Self Assessment\"] = ((bubble_plot_data[\"Self Assessment\"] - min_x)\/(max_x-min_x))\nbubble_plot_data_normalized[\"Similarity\"] = 1- (bubble_plot_data[\"Similarity\"] - min_y)\/(max_y-min_y)\n\nfig = plt.figure(figsize = (10,10))\nax = fig.add_subplot(1,1,1) \nax.plot([-2,2], [-2,2],linestyle=\"--\",linewidth=1,color=\"lightgray\", zorder=-10)\nax.set_xlim(-0.1,1.1)\nax.set_ylim(-0.1,1.1)\nax.set_xlabel(\"Self Assessment\", fontsize = 15, ha=\"right\", x = 1)\nax.set_ylabel(\"Workflow Similarity\", fontsize = 15, ha=\"right\", y=1)\nax.set_title('Are you a Data Scientist?', fontsize = 20)\ntargets = np.setdiff1d(multi_df[\"Q6_original\"].unique(),[\"Other\"])\ndot_size_min = 100\ndot_size_max = 1000\n#min_y, max_y = ax.get_xlim()\n#ax.set_ylim(max_y, min_y)\nfor target in targets:\n    size_p = (sizes[target] - min_size) \/(max_size - min_size)\n    size = dot_size_min + size_p * (dot_size_max - dot_size_min)\n    coords = bubble_plot_data_normalized\n    x = bubble_plot_data_normalized.loc[target,\"Self Assessment\"]\n    y =  bubble_plot_data_normalized.loc[target,\"Similarity\"]\n    radius = (size \/ 3.14) ** 0.5\n    ax.scatter([x],[y], c = color_map_original[target],alpha=0.4, s = size)\n    ax.spines['right'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n    ax.annotate(target, \n                xy = (x, y), \n                xytext = (0,2 + radius), \n                textcoords='offset points', \n                horizontalalignment=\"center\")","aa22321f":"questions_df = pd.DataFrame(questions)\nquestions_df.columns=[\"Q\", \"Question\"]\nquestions_df.set_index(\"Q\", inplace=True)\n\ndef get_skill_from_column_name(questions, column):\n    q = questions_df.loc[column,\"Question\"]\n    sp = q.split(\" - Selected Choice - \")\n    skills = sp[1] if len(sp)>1 else column\n    return skills\n\ndef get_skill_dict_with_prefix(df, questions, prefix):\n    return {get_skill_from_column_name(questions, column):column for column in get_cols_with_prefix(df, prefix )}\n\nskills_ide = get_skill_dict_with_prefix(multi_df, questions, \"Q13_Part\")\nskills_notebook = get_skill_dict_with_prefix(multi_df, questions, \"Q14_Part\")\nskills_cloud_service = get_skill_dict_with_prefix(multi_df, questions, \"Q15_Part\")\nskills_prog_lang = get_skill_dict_with_prefix(multi_df, questions, \"Q16_Part\")\nskills_ml_framework = get_skill_dict_with_prefix(multi_df, questions, \"Q19_Part\")\nskills_dataviz = get_skill_dict_with_prefix(multi_df, questions, \"Q21_Part\")\nskills_cloud_products = get_skill_dict_with_prefix(multi_df, questions, \"Q27_Part\")\nskills_ml_products = get_skill_dict_with_prefix(multi_df, questions, \"Q28_Part\")\nskills_database = get_skill_dict_with_prefix(multi_df, questions, \"Q29_Part\")\nskills_big_data_products = get_skill_dict_with_prefix(multi_df, questions, \"Q30_Part\")\n\nskill_categories={\n    'IDE': skills_ide,\n    'Notebook':skills_notebook,\n    'Cloud Service':skills_cloud_service,\n    'Language':skills_prog_lang,\n    'ML Framework':skills_ml_framework,\n    'DataViz':skills_dataviz,\n    'Cloud Product':skills_cloud_products,\n    'ML Product':skills_ml_products,\n    'Database':skills_database,\n    'Big Data':skills_big_data_products\n}\n\nskill_counts =[multi_df[list(skill_category.values())].sum(axis=1) for name, skill_category  in skill_categories.items()]\nskill_counts_df = pd.concat(skill_counts, axis=1)\nskill_counts_df.columns=list(skill_categories.keys())\n\nstacked_skills_data = skill_counts_df.groupby(multi_df[\"Q24_Buckets\"]).mean()\nzeroes = pd.DataFrame([0]*10).T\nzeroes.columns = stacked_skills_data.columns\nstacked_skills_data = pd.concat([zeroes,stacked_skills_data])\nstacked_skills_data.index = [\"Novice\"] + list(Q24_bucket_labels.values())","7e5ee0da":"\n\ncolor_scheme = [\n    \"#fdd021\", #candlelight\n     \"#5e9b9e\", #malibu  \n    \"#80746d\", #smalt \n    \"#66ccff\", #malibu \n    \"#de9602\",#tangerine\n    \"#cdcdcd\", #silver\n    \n    \"#0099cc\", #pacificblue\n    \"#fef266\",#parisdaisy\n    \"#cfbcae\", #softamber   \n    \"#003399\" #smalt \n]\n\nbucket_count = len(stacked_skills_data)\nskill_category_count = len(stacked_skills_data.columns)\n\nfig = plt.figure(figsize = (12,8))\nax = fig.add_subplot(1,1,1) \nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.set_xticks(np.arange(bucket_count))\nax.set_xticklabels(stacked_skills_data.index)\nax.stackplot(np.arange(bucket_count), stacked_skills_data.T.values, colors=color_scheme, labels = stacked_skills_data.columns, alpha=0.8)\nax.tick_params(labelrotation =0)\n\nprimary_tick_color = \"black\"\nsecondary_tick_color = \"gray\"\ndot_size = 0.18\n#totals\nfor i in range(bucket_count):\n    total = stacked_skills_data.iloc[i,:].sum()\n    total_text = \"%.1f\" % total\n    ax.text(i,  total+1.1,\"{}\".format(total_text), ha=\"center\", va=\"center\", fontsize=14, fontweight=\"bold\", color=primary_tick_color)\n\n    \nfor i in range(bucket_count-1):\n    for j in range(skill_category_count):\n        value = stacked_skills_data.iloc[i+1,j]\n        diff = value - stacked_skills_data.iloc[i,j]\n        sign = \"+\" if diff>0 else \"\"\n        diff_text = \"%.1f\" % diff\n        value_text = \"%.1f\" % value\n        color = color_scheme[j]\n        y = stacked_skills_data.iloc[i+1,:j].sum() + value\/2\n        circle = patches.Ellipse((i+1, y), width=dot_size,\n                                         height=7 * dot_size,\n                                         linewidth=1,\n                                         edgecolor=color,\n                                         facecolor='white',\n                                         alpha=1,\n                                         zorder=j+10\n                                         )\n        ax.add_patch(circle)\n        ax.text(i+1,  y,\"{}\".format(value_text), ha=\"center\", va=\"center\", fontsize=10, fontweight=\"bold\", color=color, zorder=j+10)\n\nhandles, labels = ax.get_legend_handles_labels()\nlegend = ax.legend(handles[::-1], labels[::-1])\n\nlegend.get_frame().set_edgecolor(\"white\")\nax.set_xlabel(\"Experience\", fontsize = 15,  ha=\"right\", x = 1)\nax.set_ylabel(\"Number of Skills\", fontsize = 15,  ha=\"right\", y = 1)\nax.set_title('Number of Tech Skills per Experience Level', fontsize = 20, y=1.08)\nplt.show()\n\n","6a4581f8":"all_skill_categories_flat = { column: label \n                         for categ_name, skill_category in skill_categories.items() \n                         for  label, column in skill_category.items() }\n\ntotal_individual_skills = multi_df[list(all_skill_categories_flat)]\ntotal_individual_skills.columns = list(all_skill_categories_flat.values())\ntotal_skill_counts = total_individual_skills.sum().sort_values(ascending=False)\n\n#These are obviously not valid Skills\nnan_values_to_filter = [\"I have not used any cloud providers\",\"None\",\"Other\"]\n\ntotal_skill_pcts = total_skill_counts\/len(multi_df)\n\njobs_df = pd.read_json(\"..\/input\/it-jobs\/jobs.json\", orient=\"index\")\nall_job_skills = set(item for category_list in jobs_df[\"categories\"].values\n              for item in category_list )\nall_categories = jobs_df[\"categories\"]\nall_skills = [item for category_list in all_categories.values\n              for item in category_list ]\n\nsimilar_skills_to_merge = [\n    [\"spark\",\"apache-spark\"],\n    [\"python\",\"python-3.x\"],\n    [\"c++\",\"c\"],\n    [\"c#\",\".net\",\"asp.net\"],\n    [\"javascript\",'typescript'],\n    ['sql-server','sql-servertsql','sql-server-2012','sql-server-2008-r2','sql-server-2008','microsoft-sql-server','ms-sql-server'],\n    [\"postgresql\",'postgresql-9.4','postgresql-9.6','postgressql'],\n    ['reactjs','redux'],\n    [\"oracle\",\"oracle-database\",\"oracle11g\",\"oracle12c\"],\n    [\"angular\",\"angularjs\"],\n    ['amazon-web-services','aws'] ,\n    ['java','java-ee']\n]\n\ndef merge_similar_skills(df, skill_lists ):\n    \n    for skill_list in skill_lists:\n        target_column = skill_list[0]\n        dropped_columns = skill_list[1:]\n        for dropped_column in dropped_columns:\n            if (dropped_column in df.columns):\n                df[target_column] = df[target_column] | df[dropped_column]\n                df.drop(dropped_column, axis=1, inplace=True)\n        \n\n\nmlb = MultiLabelBinarizer()\njobs_df_dummies = pd.DataFrame(mlb.fit_transform(jobs_df[\"categories\"]),\n                          columns=mlb.classes_,\n                          index=jobs_df.index)\n\n#Merge these columns to match the skills in the Survey\n\nmerge_similar_skills(jobs_df_dummies, similar_skills_to_merge)\n\njob_total_skill_counts = jobs_df_dummies.sum()\njob_total_skill_pcts = (job_total_skill_counts\/len(jobs_df_dummies)).sort_values(ascending=False)","e018345b":"svg_graph = {}\n\nsvg_graph[1] = \"\"\"\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg version=\"1.1\" id=\"Layer_1\" xmlns=\"http:\/\/www.w3.org\/2000\/svg\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\" x=\"0px\" y=\"0px\"\n viewBox=\"0 0 91.7 91.7\" style=\"enable-background:new 0 0 91.7 91.7;\" xml:space=\"preserve\">\n<g id=\"CIRCLE\">\n<path class=\"st0\" d=\"M38.6,46c0-4,3.3-7.3,7.3-7.3s7.3,3.3,7.3,7.3s-3.3,7.3-7.3,7.3S38.6,50,38.6,46L38.6,46z\"\/>\n<\/g>\n<\/svg>\n\"\"\"\n\nsvg_graph[2]=\"\"\"\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg version=\"1.1\" id=\"Layer_1\" xmlns=\"http:\/\/www.w3.org\/2000\/svg\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\" x=\"0px\" y=\"0px\"\n viewBox=\"0 0 91.7 91.7\" style=\"enable-background:new 0 0 91.7 91.7;\" xml:space=\"preserve\">\n<g id=\"LINE_7_\">\n<line class=\"st0\" x1=\"46\" y1=\"38.3\" x2=\"46\" y2=\"53.7\"\/>\n<\/g>\n<g id=\"CIRCLE_1_\">\n<path class=\"st0\" d=\"M38.6,61c0-4,3.3-7.3,7.3-7.3s7.3,3.3,7.3,7.3S50,68.3,46,68.3S38.6,65.1,38.6,61L38.6,61z\"\/>\n<\/g>\n<g id=\"CIRCLE_2_\">\n<path class=\"st0\" d=\"M38.6,31c0-4,3.3-7.3,7.3-7.3s7.3,3.3,7.3,7.3S50,38.3,46,38.3S38.6,35,38.6,31L38.6,31z\"\/>\n<\/g>\n<\/svg>\n\"\"\"\n\nsvg_graph[3]=\"\"\"\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg version=\"1.1\" id=\"Layer_1\" xmlns=\"http:\/\/www.w3.org\/2000\/svg\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\" x=\"0px\" y=\"0px\"\n viewBox=\"0 0 91.7 91.7\" style=\"enable-background:new 0 0 91.7 91.7;\" xml:space=\"preserve\">\n<g id=\"LINE\">\n<line class=\"st0\" x1=\"42.2\" y1=\"38\" x2=\"34.5\" y2=\"51.2\"\/>\n<\/g>\n<g id=\"LINE_1_\">\n<line class=\"st0\" x1=\"38.2\" y1=\"57.6\" x2=\"53.5\" y2=\"57.6\"\/>\n<\/g>\n<g id=\"LINE_2_\">\n<line class=\"st0\" x1=\"49.5\" y1=\"38\" x2=\"57.2\" y2=\"51.2\"\/>\n<\/g>\n<g id=\"CIRCLE_3_\">\n<path class=\"st0\" d=\"M38.5,31.6c0-4,3.3-7.3,7.3-7.3s7.3,3.3,7.3,7.3s-3.3,7.3-7.3,7.3S38.5,35.7,38.5,31.6L38.5,31.6z\"\/>\n<\/g>\n<g id=\"CIRCLE_4_\">\n<path class=\"st0\" d=\"M23.5,57.6c0-4,3.3-7.3,7.3-7.3s7.3,3.3,7.3,7.3s-3.3,7.3-7.3,7.3S23.5,61.6,23.5,57.6L23.5,57.6z\"\/>\n<\/g>\n<g id=\"CIRCLE_5_\">\n<path class=\"st0\" d=\"M53.5,57.6c0-4,3.3-7.3,7.3-7.3s7.3,3.3,7.3,7.3s-3.3,7.3-7.3,7.3S53.5,61.6,53.5,57.6L53.5,57.6z\"\/>\n<\/g>\n<\/svg>\n\n\"\"\"\n\nsvg_graph[4]=\"\"\"\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg version=\"1.1\" id=\"Layer_1\" xmlns=\"http:\/\/www.w3.org\/2000\/svg\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\" x=\"0px\" y=\"0px\"\n viewBox=\"0 0 91.7 91.7\" style=\"enable-background:new 0 0 91.7 91.7;\" xml:space=\"preserve\">\n<g id=\"LINE_3_\">\n<line class=\"st0\" x1=\"29.8\" y1=\"51\" x2=\"40.6\" y2=\"61.9\"\/>\n<\/g>\n<g id=\"LINE_4_\">\n<line class=\"st0\" x1=\"61.9\" y1=\"51\" x2=\"51\" y2=\"61.9\"\/>\n<\/g>\n<g id=\"LINE_5_\">\n<line class=\"st0\" x1=\"29.8\" y1=\"40.6\" x2=\"40.6\" y2=\"29.8\"\/>\n<\/g>\n<g id=\"LINE_6_\">\n<line class=\"st0\" x1=\"61.9\" y1=\"40.6\" x2=\"51\" y2=\"29.8\"\/>\n<\/g>\n<g id=\"CIRCLE_6_\">\n<path class=\"st0\" d=\"M38.5,67c0-4,3.3-7.3,7.3-7.3s7.3,3.3,7.3,7.3s-3.3,7.3-7.3,7.3S38.5,71.1,38.5,67L38.5,67z\"\/>\n<\/g>\n<g id=\"CIRCLE_7_\">\n<path class=\"st0\" d=\"M17.3,45.8c0-4,3.3-7.3,7.3-7.3s7.3,3.3,7.3,7.3s-3.3,7.3-7.3,7.3S17.3,49.9,17.3,45.8L17.3,45.8z\"\/>\n<\/g>\n<g id=\"CIRCLE_8_\">\n<path class=\"st0\" d=\"M59.7,45.8c0-4,3.3-7.3,7.3-7.3s7.3,3.3,7.3,7.3s-3.3,7.3-7.3,7.3S59.7,49.9,59.7,45.8L59.7,45.8z\"\/>\n<\/g>\n<g id=\"CIRCLE_9_\">\n<path class=\"st0\" d=\"M38.5,24.6c0-4,3.3-7.3,7.3-7.3s7.3,3.3,7.3,7.3s-3.3,7.3-7.3,7.3S38.5,28.7,38.5,24.6L38.5,24.6z\"\/>\n<\/g>\n<\/svg>\n\n\"\"\"\n\nsvg_graph[5]=\"\"\"\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg version=\"1.1\" id=\"Layer_1\" xmlns=\"http:\/\/www.w3.org\/2000\/svg\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\" x=\"0px\" y=\"0px\"\n viewBox=\"0 0 91.7 91.7\" style=\"enable-background:new 0 0 91.7 91.7;\" xml:space=\"preserve\">\n<g id=\"LINE_8_\">\n<line class=\"st0\" x1=\"30\" y1=\"51\" x2=\"40.8\" y2=\"61.9\"\/>\n<\/g>\n<g id=\"LINE_9_\">\n<line class=\"st0\" x1=\"62.1\" y1=\"51\" x2=\"51.2\" y2=\"61.9\"\/>\n<\/g>\n<g id=\"LINE_10_\">\n<line class=\"st0\" x1=\"30\" y1=\"40.6\" x2=\"40.8\" y2=\"29.8\"\/>\n<\/g>\n<g id=\"LINE_11_\">\n<line class=\"st0\" x1=\"62.1\" y1=\"40.6\" x2=\"51.2\" y2=\"29.8\"\/>\n<\/g>\n<g id=\"LINE_12_\">\n<line class=\"st0\" x1=\"46\" y1=\"59.7\" x2=\"46\" y2=\"53.2\"\/>\n<\/g>\n<g id=\"LINE_13_\">\n<line class=\"st0\" x1=\"32.1\" y1=\"45.8\" x2=\"38.7\" y2=\"45.8\"\/>\n<\/g>\n<g id=\"LINE_14_\">\n<line class=\"st0\" x1=\"53.4\" y1=\"45.8\" x2=\"59.9\" y2=\"45.8\"\/>\n<\/g>\n<g id=\"LINE_15_\">\n<line class=\"st0\" x1=\"46\" y1=\"38.5\" x2=\"46\" y2=\"31.9\"\/>\n<\/g>\n<g id=\"CIRCLE_10_\">\n<path class=\"st0\" d=\"M38.7,67c0-4,3.3-7.3,7.3-7.3s7.3,3.3,7.3,7.3s-3.3,7.3-7.3,7.3S38.7,71.1,38.7,67L38.7,67z\"\/>\n<\/g>\n<g id=\"CIRCLE_11_\">\n<path class=\"st0\" d=\"M17.5,45.8c0-4,3.3-7.3,7.3-7.3s7.3,3.3,7.3,7.3s-3.3,7.3-7.3,7.3S17.5,49.9,17.5,45.8L17.5,45.8z\"\/>\n<\/g>\n<g id=\"CIRCLE_12_\">\n<path class=\"st0\" d=\"M59.9,45.8c0-4,3.3-7.3,7.3-7.3s7.3,3.3,7.3,7.3s-3.3,7.3-7.3,7.3S59.9,49.9,59.9,45.8L59.9,45.8z\"\/>\n<\/g>\n<g id=\"CIRCLE_13_\">\n<path class=\"st0\" d=\"M38.7,24.6c0-4,3.3-7.3,7.3-7.3s7.3,3.3,7.3,7.3s-3.3,7.3-7.3,7.3S38.7,28.7,38.7,24.6L38.7,24.6z\"\/>\n<\/g>\n<g id=\"CIRCLE_14_\">\n<path class=\"st0\" d=\"M38.7,45.8c0-4,3.3-7.3,7.3-7.3s7.3,3.3,7.3,7.3s-3.3,7.3-7.3,7.3S38.7,49.9,38.7,45.8L38.7,45.8z\"\/>\n<\/g>\n<\/svg>\n\n\"\"\"\n\nsvg_graph[6]=\"\"\"\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg version=\"1.1\" id=\"Layer_1\" xmlns=\"http:\/\/www.w3.org\/2000\/svg\" xmlns:xlink=\"http:\/\/www.w3.org\/1999\/xlink\" x=\"0px\" y=\"0px\"\n viewBox=\"0 0 91.7 91.7\" style=\"enable-background:new 0 0 91.7 91.7;\" xml:space=\"preserve\">\n<g id=\"LINE_16_\">\n<line class=\"st0\" x1=\"38.9\" y1=\"43.6\" x2=\"26.9\" y2=\"39.7\"\/>\n<\/g>\n<g id=\"LINE_17_\">\n<line class=\"st0\" x1=\"41.5\" y1=\"51.8\" x2=\"34.2\" y2=\"61.9\"\/>\n<\/g>\n<g id=\"LINE_18_\">\n<line class=\"st0\" x1=\"45.8\" y1=\"38.5\" x2=\"45.8\" y2=\"26\"\/>\n<\/g>\n<g id=\"LINE_19_\">\n<line class=\"st0\" x1=\"25.9\" y1=\"33.1\" x2=\"39.9\" y2=\"23\"\/>\n<\/g>\n<g id=\"LINE_20_\">\n<line class=\"st0\" x1=\"27.6\" y1=\"60.8\" x2=\"22.2\" y2=\"44.4\"\/>\n<\/g>\n<g id=\"LINE_21_\">\n<line class=\"st0\" x1=\"52.8\" y1=\"43.6\" x2=\"64.7\" y2=\"39.7\"\/>\n<\/g>\n<g id=\"LINE_22_\">\n<line class=\"st0\" x1=\"65.8\" y1=\"33.1\" x2=\"51.8\" y2=\"23\"\/>\n<\/g>\n<g id=\"LINE_23_\">\n<line class=\"st0\" x1=\"64.1\" y1=\"60.8\" x2=\"69.4\" y2=\"44.4\"\/>\n<\/g>\n<g id=\"LINE_24_\">\n<line class=\"st0\" x1=\"50.1\" y1=\"51.8\" x2=\"57.5\" y2=\"61.9\"\/>\n<\/g>\n<g id=\"LINE_25_\">\n<line class=\"st0\" x1=\"54.5\" y1=\"67.8\" x2=\"37.2\" y2=\"67.8\"\/>\n<\/g>\n<g id=\"CIRCLE_15_\">\n<path class=\"st0\" d=\"M38.5,45.8c0-4,3.3-7.3,7.3-7.3s7.3,3.3,7.3,7.3s-3.3,7.3-7.3,7.3S38.5,49.9,38.5,45.8L38.5,45.8z\"\/>\n<\/g>\n<g id=\"CIRCLE_16_\">\n<path class=\"st0\" d=\"M12.6,37.4c0-4,3.3-7.3,7.3-7.3s7.3,3.3,7.3,7.3S24,44.8,20,44.8S12.6,41.5,12.6,37.4L12.6,37.4z\"\/>\n<\/g>\n<g id=\"CIRCLE_17_\">\n<path class=\"st0\" d=\"M38.5,18.6c0-4,3.3-7.3,7.3-7.3s7.3,3.3,7.3,7.3S49.9,26,45.8,26S38.5,22.7,38.5,18.6L38.5,18.6z\"\/>\n<\/g>\n<g id=\"CIRCLE_18_\">\n<path class=\"st0\" d=\"M22.5,67.8c0-4,3.3-7.3,7.3-7.3s7.3,3.3,7.3,7.3s-3.3,7.3-7.3,7.3S22.5,71.9,22.5,67.8L22.5,67.8z\"\/>\n<\/g>\n<g id=\"CIRCLE_19_\">\n<path class=\"st0\" d=\"M64.4,37.4c0-4,3.3-7.3,7.3-7.3s7.3,3.3,7.3,7.3s-3.3,7.3-7.3,7.3S64.4,41.5,64.4,37.4L64.4,37.4z\"\/>\n<\/g>\n<g id=\"CIRCLE_20_\">\n<path class=\"st0\" d=\"M54.5,67.8c0-4,3.3-7.3,7.3-7.3s7.3,3.3,7.3,7.3s-3.3,7.3-7.3,7.3S54.5,71.9,54.5,67.8L54.5,67.8z\"\/>\n<\/g>\n<\/svg>\n\n\"\"\"","35c4b359":"skill_requirement_pcts = jobs_df_dummies.sum(axis=1).value_counts(normalize=True)\nhtml = \"\"\"\n<style>\n.mv-kpi-container{\ndisplay:flex;\njustify-content: center;\n}\n.mv-kpi{\n    padding:10px;\n    -webkit-box-shadow: 0px 5px 10px 0px rgba(0,0,0,0.28);\n    -moz-box-shadow: 0px 5px 10px 0px rgba(0,0,0,0.28);\n    box-shadow: 0px 5px 10px 0px rgba(0,0,0,0.28);\n    margin:5px;\n    display:flex;\n    flex-direction:column;\n    justify-content:center;\n    align-items:center;\n    text-align:center;\n}\n.mv-kpi-value{\n    color:#5DADE2;\n    font-size:25px;\n    font-weight:bold;\n    display:block;\n    width:100%;\n    padding:5px 5px 20px 5px;\n   \n}\n.mv-kpi-value-large{\n  color: #5DADE2;\n    font-size:80px;\n    font-weight:bold;\n    display:block;\n    width:100%;\n    padding:5px;\n  \n}\n\n.mv-kpi-value-large svg{\n    width:100px;\n}\n\n.mv-kpi-value-large svg g{\nstroke: #5DADE2;\nstroke-width:3px;\nfill:none;\n}\n<\/style>\n\"\"\"\nhtml += \"<div class='mv-kpi-container'>\"\nfor i in range(skill_requirement_pcts.size):\n    pct = int(skill_requirement_pcts.iloc[i]*100)\n    count = skill_requirement_pcts.index[i]\n    html+=\"<div class='mv-kpi'><span class='mv-kpi-value-large'>\"+svg_graph[skill_requirement_pcts.index[i]]+\"<\/span><span class='mv-kpi-value'>{}%<\/span><\/div>\".format( pct)\nhtml+= \"<\/div>\"\ndisplay(HTML(html))","0f55ad24":"#These terms are too generic, should be filtered\nfilter_demand_values = [\n    \"machine-learning\",\n    \"bigdata\",\n    \"cloud\",\n    \"sysadmin\",\n    \"agile\",\n    \"algorithm\",\n    \"nosql\",\n    \"database\",\n    \"data-science\",\n    \"rest\",\n    \"deep-learning\",\n    \"artificial-intelligence\",\n    \"web-services\",\n    \"testing\",\n    \"computer-vision\",\n    \"qa\",\n    \"security\",\n    \"automation\",\n    \"design\",\n    \"microservices\",\n    \"devops\",\n    \"data-warehouse\",\n    'statistics',\n    'nlp',\n    'etl',\n    'data-mining',\n    'data',\n    'neural-network',\n    'time-series',\n    'data-modeling',\n    'user-interface',\n    'json',\n    'apache',\n    'continuous-integration',\n    'api',\n    'automated-tests',\n    'project-management',\n    'user-experience',\n    'sdlc',\n    'analytics',\n    'xml'\n]\n\nrenamed_survey_skills = {\n    'Amazon Web Services (AWS)':'AWS'\n}\n\nfrom_job_skill_to_survey_skill_map = {\n    \"jupyter\":\"Jupyter\/IPython\",\n    \"jupyter notebook\":\"Jupyter\/IPython\",\n    \"r-studio\":\"RStudio\",\n    \"visual-studio-code\":\"Visual Studio Code\",\n    \"atom\":\"Atom\",\n    \"atom-ide\":\"Atom\",\n    \"matlab\":\"MATLAB\",\n    \"visual-studio\":\"Visual Studio\",\n    \"notepad++\":\"Notepad++\",\n    \"sublime-text\":\"Sublime Text\",\n    \"vim\":\"Vim\",\n    \"intellij\":\"IntelliJ\",\n    \"intellij-idea\":\"IntelliJ\",\n    \"floydhub\":\"Floydhub\",\n    \"google-cloud-platform\":\"Google Cloud Platform (GCP)\",\n    'amazon-web-services':'AWS', \n    'aws': 'AWS',\n    'azure': 'Microsoft Azure',\n    'microsoft-azure': 'IBM Cloud',\n    'r': 'R',\n    'sql': 'SQL',\n    'bash': 'Bash',\n    'java': 'Java',\n    'javascript':'Javascript\/Typescript', \n    'typescript': 'Javascript\/Typescript',\n    'vb': 'Visual Basic\/VBA',\n    'vba':'Visual Basic\/VBA', \n    'visual-basic': 'Visual Basic\/VBA',\n    'c':'C\/C++', \n    'c++': 'C\/C++',\n    'matlab': 'MATLAB',\n    'scala': 'Scala',\n    'julia': 'Julia',\n    'go': 'Go',\n    'c#, .net': 'C#\/.NET',\n    'php': 'PHP',\n    'ruby': 'Ruby',\n    'sas, sass': 'SAS',\n    'scikit-learn': 'Scikit-Learn',\n    'tensorflow': 'TensorFlow',\n    'keras': 'Keras',\n    'pytorch': 'PyTorch',\n    'h2o': 'H20',\n    'caret': 'Caret',\n    'prophet': 'Prophet',\n    'caffe':\"Caffe\",\n    'caffe2': 'Caffe',\n    'ggplot': 'ggplot2',\n    'matplotlib': 'Matplotlib',\n    'shiny': 'Shiny',\n    'd3.js': 'D3',\n    'plotly': 'Plotly',\n    'leaflet': 'Leaflet',\n    'lattice': 'Lattice',\n    'amazon-ec2': 'AWS Elastic Compute Cloud (EC2)',\n    'google-compute-engine': 'Google Compute Engine',\n    'aws-elastic-beanstalk': 'AWS Elastic Beanstalk', \n    'elastic-beanstalk': 'AWS Elastic Beanstalk',\n    'google-app-engine': 'Google App Engine',\n    'aws-lambda': 'AWS Lambda',\n    'google-cloud-functions': 'Google Cloud Functions',\n    'azure-container-service': 'Azure Container Service',\n    'azure-functions': 'Azure Functions',\n    'azure-eventhub': 'Azure Event Grid',\n    'ibm-cloud': 'IBM Cloud Virtual Servers',\n    'google-cloud-speech-api': 'Google Cloud Speech-to-text API',\n    'rekognition-api': 'Amazon Rekognition',\n    'google-cloud-vision-api': 'Google Cloud Vision API',\n    'google-cloud-natural-language': 'Google Cloud Natural Language API',\n    'amazon-lex': 'Amazon Lex',\n    'domino': 'Domino Datalab',\n    'cloudera': 'Cloudera',\n    'azure-machine-learning': 'Azure Machine Learning Workbench',\n    'cortana-intelligence': 'Azure Cortana Intelligence Suite',\n    'ibm-watson-cognitive': 'IBM Watson Studio',\n    'azure-cognitive-services': 'Azure Cognitive Services',\n    'aws-rds': 'AWS Relational Database Service',\n    'amazon-aurora': 'AWS Aurora',\n    'amazon-rds-aurora': 'AWS Aurora', \n    'aws-aurora': 'AWS Aurora',\n    'google-cloud-sql': 'Google Cloud SQL', \n    'google-cloud-sql-for-postgresql': 'Google Cloud SQL',\n    'amazon-dynamodb': 'AWS DynamoDB',\n    'google-cloud-datastore': 'Google Cloud Datastore',\n    'google-cloud-bigtable': 'Google Cloud Bigtable',\n    'amazon-simpledb': 'AWS SimpleDB',\n    'microsoft-sql-server': 'Microsoft SQL Server',\n    'ms-sql-server': 'Microsoft SQL Server',\n    'sql-server': 'Microsoft SQL Server', \n    'sql-server-2008': 'Microsoft SQL Server', \n    'sql-server-2008-r2': 'Microsoft SQL Server',\n    'sql-server-2012': 'Microsoft SQL Server',\n    'sql-servertsql': 'Microsoft SQL Server',\n    'mysql': 'MySQL',\n    'postgresql': 'PostgresSQL', \n    'postgresql-9.4': 'PostgresSQL', \n    'postgresql-9.6': 'PostgresSQL',\n    'postgressql': 'PostgresSQL',\n    'sqlite': 'SQLite',\n    'oracle': 'Oracle Database',\n    'oracle-database': 'Oracle Database',\n    'oracle11g': 'Oracle Database',\n    'oracle12c': 'Oracle Database',\n    'ms-access, access': 'Microsoft Access',\n    'nexus': 'NexusDB',\n    'azure-cosmos-db': 'Azure Cosmos DB', \n    'azure-cosmosdb': 'Azure Cosmos DB',\n    'azure-sql-database': 'Azure SQL Database',\n    'azure-database-for-postgresql': 'Azure Database for PostgreSQL',\n    'google-cloud-dataflow': 'Google Cloud Dataflow',\n    'amazon-kinesis': 'AWS Kinesis',\n    'google-cloud-pub\/sub': 'Google Cloud Pub\/Sub',\n    'google-cloud-pubsub': 'Google Cloud Pub\/Sub',\n    'amazon-athena': 'AWS Athena',\n    'amazon-redshift': 'AWS Redshift',\n    'google-bigquery': 'Google BigQuery',\n    'google-bigquery-data-transfer-service': 'Google BigQuery',\n    'teradata': 'Teradata',\n    'snowflake': 'Snowflake', \n    'snowflake-datawarehouse': 'Snowflake',\n    'databricks': 'Databricks',\n    'azure-stream-analytics': 'Azure Stream Analytics',\n\n    'python':'Python',\n    'r':'R',\n    'sql':'SQL',\n    'hadoop':'Hadoop',\n    'apache-spark':'Apache Spark',\n    'java':'Java',\n    'scala':'Scala',\n    'tensorflow':'TensorFlow',\n    'amazon-web-services':'AWS',\n\n    'pyspark':'PySpark',\n    'pandas':'Pandas',\n    'spark':'Apache Spark',\n    'numpy':'Numpy',\n\n    'linux':'Linux',\n    'tableau':'Tableau',\n    'http':'HTTP',\n    'cassandra':'Cassandra',\n    'git':'Git',\n    'mongodb':'MongoDB',\n    'hive':'Hive',\n\n    'html':'HTML'   ,\n    'elasticsearch':'Elasticsearch',\n    'reactjs':'React.js',\n    'docker':'Docker',\n    'apache-kafka':'Apache Kafka',\n    'salesforce':'Salesforce',\n    'node.js':'Node.js',\n    \".net\":'.NET',\n    'asp.net':'.NET',\n    'ruby-on-rails':'Ruby on Rails',\n    'kubernetes':'Kubernetes',\n    'spring':'Spring',\n    'angularjs':'Angular.js',\n    \"angular\":'Angular.js',\n    'apex':'APEX',\n    'css':'CSS',\n    'mapreduce':'MapReduce',\n    'redis':'Redis',\n    'user-interface':'UI',\n    'hbase':'Apache HBase',\n    \"c#\":\"C#\/.NET\",\n    'clojure':'Clojure',\n    'redux':'Redux',\n    'jenkins':'Jenkins',\n    'powerbi':'Power BI',\n    'jira':'Jira',\n    'windows':'Windows',\n    'unix':'Unix',\n    'vb.net':'VB.net',\n    'ansible':'Ansible'\n}\n\n\n\n\nx_supply_axis=-0.75\nx_demand_axis = 0.75\ncount = 50\n\ndemand = job_total_skill_pcts[~job_total_skill_pcts.index.isin(filter_demand_values)]\nsupply = total_skill_pcts[~total_skill_pcts.index.isin(nan_values_to_filter)]\n\ndemand_labels = [from_job_skill_to_survey_skill_map.get(l,l) for l in demand.index] \n\nfig, ax = plt.subplots(figsize=(10, 0.7*count))\n\n\nx_supply = [x_supply_axis]*count\ny_supply = list(range(count))\n\nx_demand = [x_demand_axis]*count\ny_demand = list(range(count))\n\n\nsupply_values = [math.ceil(supply.iloc[i]*100) for i in range(count)]\ndemand_values = [math.ceil(demand.iloc[i]*100) for i in range(count)]\n\narea_coef = 5000\narea_supply = (supply*area_coef).values\narea_demand = (demand*area_coef).values\n\nsupply_radius = [((size \/ 3.14) ** 0.5)\/200 for size in area_supply]\ndemand_radius = [((size \/ 3.14) ** 0.5)\/200 for size in area_demand]\n\ncenter_offset = 0.18\n\n#x_supply = [x_supply[i] - supply_radius[i] + center_offset for i in range(count)]\n#x_demand = [x_demand[i] + demand_radius[i] - center_offset for i in range(count)]\n\nexisting_color=\"gray\"\nmissing_color=\"whitesmoke\"\n\ntext_offset=0.04 #horizontal offset between bubble and text\nsupply_colors = []\ndemand_colors = []\ntext_vertical_offset = 0.16 #vertical offset between bubble center and text center\nfor i in range(count):\n    supply_label =supply.index[i]\n    demand_label=demand_labels[i]\n    supply_value=supply_values[i]\n    demand_value=demand_values[i]\n    x = x_supply_axis - supply_radius[i] -text_offset\n    y = i-text_vertical_offset\n    ax.text(x, y, supply_label, horizontalalignment=\"right\", verticalalignment=\"center\", fontweight=\"bold\")\n    x = x_supply_axis - supply_radius[i]-text_offset\n    y = i+text_vertical_offset\n    ax.text(x, y ,\"{}%\".format(supply_value) , horizontalalignment=\"right\", verticalalignment=\"center\")\n    \n    x = x_demand_axis + demand_radius[i]+text_offset\n    y = i-text_vertical_offset\n    ax.text(x,y , demand_label, horizontalalignment=\"left\",verticalalignment=\"center\",fontweight=\"bold\")\n    x = x_demand_axis + demand_radius[i]+text_offset\n    y = i+text_vertical_offset\n    ax.text(x, y, \"{}%\".format(demand_value), horizontalalignment=\"left\",verticalalignment=\"center\")\n    supply_colors.append(\"#117bd7\" if supply_label in demand_labels else \"gray\")\n    demand_colors.append(\"orange\" if demand_label in supply.index else \"gray\")\n    \n    line_start_offset = 0.01\n    line_offset = 0.8\n    if supply_label in demand_labels:\n        target_index = demand_labels.index(supply_label)\n        c = existing_color if supply_label in demand_labels[:count] else missing_color\n        vertices = [(x_supply_axis+line_start_offset + supply_radius[i], i), \n              (x_supply_axis+line_offset, i), \n              (x_demand_axis-line_offset, target_index), \n              (x_demand_axis-line_start_offset - demand_radius[target_index], target_index)]\n        curves = [Path.MOVETO, Path.CURVE4,Path.CURVE4, Path.CURVE4]\n        ax.add_patch(patches.PathPatch( Path(vertices,curves), fc=\"none\", transform=ax.transData, color=\"white\", linewidth=7))\n        ax.add_patch(patches.PathPatch( Path(vertices,curves), fc=\"none\", transform=ax.transData, color=c, linewidth=4))\n        \n    if (demand_label in supply.index) and not (demand_label in supply.index[:count]):\n        c= missing_color\n        target_index = list(supply.index).index(demand_label)\n        vertices= [(x_supply_axis+line_start_offset + supply_radius[target_index], target_index), \n              (x_supply_axis+line_offset, target_index), \n              (x_demand_axis-line_offset, i), \n              (x_demand_axis-line_start_offset - demand_radius[i], i)]\n        curves = [Path.MOVETO, Path.CURVE4,Path.CURVE4, Path.CURVE4]\n        zorder = -100\n        ax.add_patch(patches.PathPatch(Path(vertices,curves), \n                                        fc=\"none\", \n                                        transform=ax.transData, \n                                        color=\"white\", \n                                        linewidth=7,\n                                        zorder=zorder))\n        ax.add_patch(patches.PathPatch(Path(vertices,curves), \n                                        fc=\"none\", \n                                        transform=ax.transData, \n                                        color=c, \n                                        linewidth=4,\n                                        zorder=zorder))\n\n\n\nax.scatter(x_supply, y_supply, s=area_supply, alpha=0.5, c=supply_colors)\nax.scatter(x_demand, y_demand, s=area_demand, alpha=0.5, c=demand_colors)\nax.grid(False)\nax.set_xticks([])\nax.set_yticks([])\nax.spines['left'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nax.set_xlim(-1.5, 1.5)\nax.set_ylim(count, -1)\nax.set_title(\"Supply and Demand on the Job Market\")\nplt.show()","9da2f043":"plots = [\n    [(\"Data Scientist\",\"Scien\"),\n    (\"Software Engineer\",\"Developer\")],\n    [(\"Data Analyst\",\"Analyst\"),\n    (\"Data Engineer\",\"Data Engineer\")]\n]\nfig, axarr = plt.subplots(2,2, sharex=True, figsize=(8*2, 0.7*count))\nfor pi in range(2):\n    row = plots[pi]\n    for pj in range(2):\n        role, keyword = row[pj]\n        mask = jobs_df[\"title\"].str.contains(keyword)\n        masked_dummy_counts = jobs_df_dummies[mask].sum()\n        masked_job_total_skill_pcts = (masked_dummy_counts\/len(jobs_df_dummies[mask])).sort_values(ascending=False)\n\n        supply_mask = multi_df[C_TITLE]==role\n        supply_masked_dummy_counts = total_individual_skills[supply_mask].sum()\n        masked_supply_total_skill_pcts = (supply_masked_dummy_counts\/len(total_individual_skills[supply_mask])).sort_values(ascending=False)\n\n\n        count = 20\n\n        ax = axarr[pi,pj]\n\n        demand = masked_job_total_skill_pcts[~masked_job_total_skill_pcts.index.isin(filter_demand_values)]\n        supply = masked_supply_total_skill_pcts[~masked_supply_total_skill_pcts.index.isin(nan_values_to_filter)]\n        demand_labels = [from_job_skill_to_survey_skill_map.get(l,l) for l in demand.index]\n        supply_labels = [renamed_survey_skills.get(l,l) for l in supply.index]\n        #fig, ax = plt.subplots(figsize=(8, 0.7*count))\n\n        x_supply = [x_supply_axis]*count\n        y_supply = list(range(count))\n\n        x_demand = [x_demand_axis]*count\n        y_demand = list(range(count))\n\n\n        supply_values = [math.ceil(supply.iloc[i]*100) for i in range(count)]\n        demand_values = [math.ceil(demand.iloc[i]*100) for i in range(count)]\n\n        area_coef = 500\n        area_supply = (supply*area_coef).values\n        area_demand = (demand*area_coef).values\n\n        supply_radius = [((size \/ 3.14) ** 0.5)\/200 for size in area_supply]\n        demand_radius = [((size \/ 3.14) ** 0.5)\/200 for size in area_demand]\n\n        center_offset = 0.18\n\n\n        existing_color=\"gray\"\n        missing_color=\"whitesmoke\"\n\n        text_offset=0.04 #horizontal offset between bubble and text\n        supply_colors = []\n        demand_colors = []\n        text_vertical_offset = 0.16 #vertical offset between bubble center and text center\n        for i in range(count):\n            supply_label =supply_labels[i]\n            demand_label=demand_labels[i]\n            supply_value=supply_values[i]\n            demand_value=demand_values[i]\n            x = x_supply_axis - supply_radius[i] -text_offset\n            y = i-text_vertical_offset\n            ax.text(x, y, supply_label, horizontalalignment=\"right\", verticalalignment=\"center\", fontweight=\"bold\")\n            x = x_supply_axis - supply_radius[i]-text_offset\n            y = i+text_vertical_offset\n            ax.text(x, y ,\"{}%\".format(supply_value) , horizontalalignment=\"right\", verticalalignment=\"center\")\n\n            x = x_demand_axis + demand_radius[i]+text_offset\n            y = i-text_vertical_offset\n            ax.text(x,y , demand_label, horizontalalignment=\"left\",verticalalignment=\"center\",fontweight=\"bold\")\n            x = x_demand_axis + demand_radius[i]+text_offset\n            y = i+text_vertical_offset\n            ax.text(x, y, \"{}%\".format(demand_value), horizontalalignment=\"left\",verticalalignment=\"center\")\n            supply_colors.append(\"#117bd7\" if supply_label in demand_labels else \"gray\")\n            demand_colors.append(\"orange\" if demand_label in supply.index else \"gray\")\n\n            line_start_offset = 0.04\n            line_offset = 0.8\n            if supply_label in demand_labels:\n                target_index = demand_labels.index(supply_label)\n                c = existing_color if supply_label in demand_labels[:count] else missing_color\n                vertices = [(x_supply_axis+line_start_offset + supply_radius[i], i), \n                      (x_supply_axis+line_offset, i), \n                      (x_demand_axis-line_offset, target_index), \n                      (x_demand_axis-line_start_offset - demand_radius[target_index], target_index)]\n                curves = [Path.MOVETO, Path.CURVE4,Path.CURVE4, Path.CURVE4]\n                ax.add_patch(patches.PathPatch( Path(vertices,curves), fc=\"none\", transform=ax.transData, color=\"white\", linewidth=4))\n                ax.add_patch(patches.PathPatch( Path(vertices,curves), fc=\"none\", transform=ax.transData, color=c, linewidth=2))\n\n            if (demand_label in supply.index) and not (demand_label in supply.index[:count]):\n                c= missing_color\n                target_index = list(supply.index).index(demand_label)\n                vertices= [(x_supply_axis+line_start_offset + supply_radius[target_index], target_index), \n                      (x_supply_axis+line_offset, target_index), \n                      (x_demand_axis-line_offset, i), \n                      (x_demand_axis-line_start_offset - demand_radius[i], i)]\n                curves = [Path.MOVETO, Path.CURVE4,Path.CURVE4, Path.CURVE4]\n                zorder = -100\n                ax.add_patch(patches.PathPatch(Path(vertices,curves), \n                                                fc=\"none\", \n                                                transform=ax.transData, \n                                                color=\"white\", \n                                                linewidth=4,\n                                                zorder=zorder))\n                ax.add_patch(patches.PathPatch(Path(vertices,curves), \n                                                fc=\"none\", \n                                                transform=ax.transData, \n                                                color=c, \n                                                linewidth=2,\n                                                zorder=zorder))\n\n\n\n        ax.scatter(x_supply, y_supply, s=area_supply, alpha=0.5, c=supply_colors)\n        ax.scatter(x_demand, y_demand, s=area_demand, alpha=0.5, c=demand_colors)\n        ax.grid(False)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.spines['left'].set_visible(False)\n        ax.spines['bottom'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        ax.set_xlim(-1.5, 1.5)\n        ax.set_ylim(count, -1)\n        ax.set_title(role)\n\nplt.show()\n","bb71f16b":"mapped_categories = jobs_df[\"categories\"].apply(lambda row: [from_job_skill_to_survey_skill_map.get(skill, skill) for skill in row])\n\nskill_counts_per_experience_bucket = skill_counts_df.groupby([multi_df[C_TITLE],multi_df[\"Q24_Buckets\"]]).mean().astype(int)\n\ndef get_average_skill_count_by_title_and_exp(job_title, experience_level):\n    by_job_title = skill_counts_per_experience_bucket.loc[job_title]\n    columns = skill_counts_per_experience_bucket.columns\n    return by_job_title.loc[experience_level] if experience_level in by_job_title.index else pd.Series(np.zeros(len(columns)),index = columns)\n\ndef get_total_skill_popularity(job_title, experience_level):\n    df = pd.DataFrame(total_individual_skills[(multi_df[C_TITLE]==job_title) & (multi_df[\"Q24_Buckets\"]==experience_level)].mean())\n    df[\"Category\"] = [cat for cat, category_list in skill_categories.items() for skill in category_list]\n    return df\n\ndef get_most_probable_skill_stack(job_title, experience_level):\n    avg_skill_count = get_average_skill_count_by_title_and_exp( job_title, experience_level)\n    skill_popularity = get_total_skill_popularity(job_title,experience_level)\n    stack = {}\n    for category in avg_skill_count.index:\n        get_count = avg_skill_count[category]\n        if get_count == 0:\n            stack[category] = []\n        else:\n            mask = (skill_popularity[\"Category\"]==category)& ~(skill_popularity.index.isin(nan_values_to_filter))\n            stack[category]=list( skill_popularity[mask].sort_values(by=0, ascending=False).head(get_count).index)\n    return stack\n\ndef get_category_counts_until_next_level(job_title, experience_level):\n    avg_skill_count = get_average_skill_count_by_title_and_exp( job_title, experience_level)\n    avg_skill_count_next = get_average_skill_count_by_title_and_exp( job_title, experience_level+1)\n    diff = avg_skill_count_next-avg_skill_count\n    diff[diff < 0] = 0\n    return diff.astype(int)\n\ndef get_employability_from_stack(stack):\n    all_skills_from_stack = {skill for category, category_list in stack.items() for skill in category_list}\n    match_count = mapped_categories.apply(lambda skills: len(set(skills)&all_skills_from_stack))\n    match_points = match_count * (match_count+1)\/2\n    return match_points.sum()\n\ndef fill_stack(stack, counts):\n    \n\n    #counts = The number of skills to add to each category\n    \n    \n    #set up a DF with zero counts\n    skill_counts = pd.DataFrame(list(all_skill_categories_flat.values()), columns=[\"Skill\"])\n    skill_counts[\"Count\"] = 0\n    skill_counts.set_index(\"Skill\", inplace=True)\n    \n    for category_list in jobs_df[\"categories\"].values:\n        all_skills_from_stack = {skill for category, category_list in stack.items() for skill in category_list}\n        match_count = len(set(all_skills_from_stack)&set(category_list))\n        of_total_count = len(set(category_list))\n        is_matched = match_count > 0\n        for skill in category_list:\n                mapped_name = from_job_skill_to_survey_skill_map.get(skill,skill)\n                if not mapped_name in skill_counts.index.values:\n                    continue\n                else:\n                    #The job was already matched, now we increase the fill\n                    if not is_matched:\n                        skill_counts.loc[mapped_name,\"Count\"] = skill_counts.loc[mapped_name,\"Count\"] + match_count+1 #skill_counts.loc[mapped_name,\"Count\"] + 1\n                    else:\n                        skill_counts.loc[mapped_name,\"Count\"] = skill_counts.loc[mapped_name,\"Count\"] + match_count+1 #skill_counts.loc[mapped_name,\"Count\"] + 1\/len(category_list)\n\n    skill_counts[\"Category\"]= [cat for cat, category_list in skill_categories.items() for skill in category_list]\n    skill_counts\n    new_stack = copy.deepcopy(stack)\n    for category in counts.index:\n        count = counts[category]\n        picked = skill_counts[(skill_counts[\"Category\"]==category)&\n                              ~(skill_counts.index.isin(nan_values_to_filter))&\n                              ~(skill_counts.index.isin(all_skills_from_stack ))\n                             ].sort_values(by=\"Count\", ascending=False).head(count).index.values\n        new_stack[category].extend(picked)\n    \n    return new_stack\n\nstack_css = \"\"\"\n    <style>\n        .mv-stack-container{\n            display:flex;\n            height:700px;\n            width:100%;\n        }\n        .mv-stack-symbol{\n            display:flex;\n            height:100%;\n            align-items:center;\n            justify-content:center;\n            font-size:30px;\n            font-weight:bold;\n            color:#5DADE2;\n        }\n        .mv-stack-categories{\n            display:flex;\n            height:100%;\n            align-items:flex-start;\n            justify-content:center;\n            padding:20px;\n            width:30%;\n            flex-grow:1;\n        }\n        .mv-stack-unknown{\n            max-width:240px;\n            align-items:center;\n        }\n        .mv-stack-table{\n            display:table;\n        }\n        .mv-stack-row{\n            \n            padding:5px 0px;\n            display: flex;\n            flex-direction: column;\n            align-items: flex-start;\n        }\n        \n        .mv-stack-row-title{\n            font-size: 20px;\n            font-weight: bold;\n            color: #5DADE2;\n            padding: 10px 0px;\n        }\n        .mv-stack-row-metric{\n            font-size: 30px;\n            font-weight: bold;\n            color: #5DADE2;\n            padding: 10px 0px;\n        }\n        \n        .mv-stack-row.mv-stack-unknown{\n            display:table-row;\n        }\n        .mv-stack-cell{\n            display:table-cell;\n            \n        }\n        .mv-stack-cell-title{\n            display:table-cell;\n            text-align:right;\n            font-weight:bold;\n            font-size:12px;\n            margin-bottom: -4px;\n        }\n        \n        .mv-stack-cell-title.mv-stack-unknown{\n            padding:5px;\n        \n        }\n        .mv-stack-skill{\n            background:#5DADE2;\n            color:white;\n            border-radius:3px;\n            padding: 2px 4px;\n            margin:0px 4px 2px 0px;\n            font-size:12px;\n            white-space:nowrap;\n            display:inline-block;\n            line-height:1.2rem;\n        }\n        \n        .mv-stack-skill-empty{\n            background:darkgray;\n        }\n        .mv-stack-skill-unknown{\n            background:black;\n            color:white;\n        }\n        \n    <\/style>\n    \"\"\"\n\nskill_category_color_scheme = {list(skill_categories.keys())[i]:color_scheme[i] for i in range(len(color_scheme))}\n\nfrom IPython.core.display import display, HTML\n\n\ndef visualize_stack(before_stack_name, before_stack, after_stack_name, after_stack, category_counts):\n    html = stack_css\n    html+= \"<div class='mv-stack-container'>\"\n    html+=visualize_single_stack(before_stack, before_stack_name)\n    html+=visualize_new_skills(category_counts)\n    html+=visualize_single_stack(after_stack, after_stack_name)    \n    html+= \"<\/div>\"\n    \n    display(HTML(html))\n\ndef visualize_new_skills(category_counts):\n    html=  \"\"\"  <div class='mv-stack-symbol'>+<\/div>\n                <div class='mv-stack-categories mv-stack-unknown'>\n                    <div class='mv-stack-table'>\"\"\"\n    for i in range(len(category_counts)):\n        if category_counts[i]==0: \n            continue\n        html+=  \"<div class='mv-stack-row mv-stack-unknown'>\"\n        html+=  \"    <div class='mv-stack-cell-title mv-stack-unknown'>{}<\/div>\".format(category_counts.index[i])\n        html+=  \"    <div class='mv-stack-cell'>\"\n        for j in range(category_counts[i]):\n            html+=  \"    <span class='mv-stack-skill mv-stack-skill-unknown'>?<\/span>\"      \n        html+=  \"    <\/div>\"      \n        html+=  \"<\/div>\"               \n    html+=  \"\"\"\n            <\/div>\n        <\/div>\n        <div class='mv-stack-symbol'>=<\/div>\"\"\"\n    return html\n\ndef visualize_single_stack(stack, title):\n    score = int(get_employability_from_stack(stack))\n    html=\"\"\"<div class='mv-stack-categories'>\n            <div class='mv-stack-table'>\n                <div class='mv-stack-row-title'>{}<\/div>\n                <div class='mv-stack-row-metric'>{} points<\/div>\n            \"\"\".format(title, score)\n    for category, skills in stack.items():\n        #color = skill_category_color_scheme[category]\n        html+=\"\"\"\n        <div class='mv-stack-row'>\n        <div class='mv-stack-cell-title'>{}<\/div>\n        <div class='mv-stack-cell' >\"\"\".format(category)\n        if not len(skills):\n            html+=\"<span class='mv-stack-skill mv-stack-skill-empty' >{}<\/span>\".format(\"None\")\n        else:\n            for skill in sorted(skills):\n                html+=\"<span class='mv-stack-skill' >{}<\/span>\".format(skill)\n                    \n        html+=   \"\"\"<\/div> <\/div> \"\"\"\n    \n    html+=  \"\"\"\n            <\/div>\n        <\/div>\n        \"\"\"\n    return html\n    \ndef visualize_role_options(job_title, level, before_label, after_label):\n    before_stack = get_most_probable_skill_stack(job_title, level)\n    counts = get_category_counts_until_next_level(job_title,level)\n    after_stack = fill_stack(before_stack,counts)\n\n    visualize_stack(before_label,\n                    before_stack, \n                    after_label,\n                    after_stack, \n                    counts)\n    \ndef visualize_stacks(stacks, titles):\n    html = stack_css\n    html+= \"<div class='mv-stack-container'>\"\n    for i in range(len(stacks)):\n        html+=visualize_single_stack(stacks[i], titles[i])\n    html+= \"<\/div>\"\n    \n    display(HTML(html))","f7da6a3e":"all_skill_categories_flat_filtered = {column:skill for column, skill in all_skill_categories_flat.items() if skill not in nan_values_to_filter}\nskills_studied = all_skill_categories_flat_filtered\np_skill = pd.DataFrame(multi_df[list(skills_studied.keys())].mean())\np_skill.index = list(skills_studied.values())\n\np_skill.index.name = \"From\"\np_skill.columns=[\"p\"]\nskill_pairs = multi_df[list(skills_studied.keys())]\nskill_pairs.columns = list(skills_studied.values())\np_skill_pair = skill_pairs.T.dot(skill_pairs)\/len(skill_pairs)\np_skill_pair = pd.DataFrame(p_skill_pair.unstack())\np_skill_pair.index.rename([\"From\",\"To\"], inplace=True)\np_skill_pair.columns=[\"p_both\"]\n\nfrom  itertools import combinations\n\ncombo = list(combinations(range(len(p_skill.index)),2))\ni1 = [i1 for i1,i2 in combo]\ni2 = [i2 for i1,i2 in combo]\nassoc_rules_data= pd.DataFrame({'From':p_skill.index[i1],'To':p_skill.index[i2],'p_from':p_skill.iloc[i1,0].values,'p_to':p_skill.iloc[i2,0].values})\n\nassoc_rules = pd.DataFrame(assoc_rules_data)\nassoc_rules.set_index([\"From\",\"To\"], inplace=True)\n\np_skill_pair = p_skill_pair[~p_skill_pair.index.duplicated(keep='first')]\nassoc_rules= assoc_rules[~assoc_rules.index.duplicated(keep='first')]\n\nassoc_rules = pd.concat([assoc_rules, p_skill_pair], sort=True, axis=1)\n#assoc_rules[\"support\"] = p_skill_pair.values\nassoc_rules[\"confidence\"]= assoc_rules[\"p_both\"]\/assoc_rules[\"p_to\"]\nassoc_rules[\"lift\"] = assoc_rules[\"p_both\"]\/(assoc_rules[\"p_from\"]*assoc_rules[\"p_to\"])\nassoc_rules.loc[assoc_rules[\"confidence\"]==1,\"lift\"]=np.nan\n\nassoc_rules_not_null = assoc_rules.dropna()\n#Drop identical and inverted rules\nassoc_rules_not_null_filtered =assoc_rules_not_null[\n    (assoc_rules_not_null.index.get_level_values(0)!=assoc_rules_not_null.index.get_level_values(1))&\n    (assoc_rules_not_null[\"p_from\"]>assoc_rules_not_null[\"p_to\"])\n]\n\n#Limit input count\ninput_count = 2\n\n#Limit output count\noutput_count = 8\n\nassoc_rules_not_null_grouped = assoc_rules_not_null_filtered.groupby(\n    assoc_rules_not_null_filtered.index.get_level_values(1), as_index=False\n).apply(lambda g: g.sort_values(by=\"confidence\", ascending=False).head(input_count)).reset_index(0, drop=True)\n\nassoc_rules_not_null_top = assoc_rules_not_null_grouped.groupby(\n    assoc_rules_not_null_grouped.index.get_level_values(0), as_index=False\n).apply(lambda g: g.sort_values(by=\"confidence\", ascending=False).head(output_count)).reset_index(0, drop=True)\n\n\nselected_assoc_rules = assoc_rules_not_null_top[\n    (assoc_rules_not_null_top[\"p_both\"]>0.05)&\n    (assoc_rules_not_null_top[\"lift\"]>0.2)\n].sort_values(by=\"confidence\", ascending=False).head(100)\n\n\nall_selected_nodes = set(list(selected_assoc_rules.index.get_level_values(0)) + list(selected_assoc_rules.index.get_level_values(1)))\nall_nodes = [(index, {'size':p_skill.loc[index][\"p\"].mean()}) for index in all_selected_nodes]\noutput_counts = selected_assoc_rules.groupby(selected_assoc_rules.index.get_level_values(0)).size()\ninput_counts = selected_assoc_rules.groupby(selected_assoc_rules.index.get_level_values(1)).size()\n","3e321385":"import networkx as nx\n\nplt.figure(figsize=(25,15))\nG = nx.DiGraph()\nG.add_nodes_from(all_nodes)\n\n\nfor row in selected_assoc_rules.reset_index().values:\n    G.add_edge(str(row[0]),str(row[1]),weight = row[6])\n\n    \nroots = {n: list(G.predecessors(n)) for n,attr in G.nodes(data=\"size\")}\nroot_names =  {n:n if len(predecessors)==0 else predecessors[-1] for n,predecessors in roots.items()}\nroot_sizes = {n: G.nodes[n][\"size\"] if len(predecessors)==0 else  G.nodes[predecessors[-1]][\"size\"] for n,predecessors in roots.items()}\nimport operator\n\nnodes_sorted = sorted({n:attr[\"size\"] for (n, attr) in all_nodes}.items(), key=operator.itemgetter(1), reverse=False)\nroots_sorted = [n for n, size in nodes_sorted if n in root_names.values()]\n\nall_nodes_names = sorted([n for (n, attr) in all_nodes])\npos_initial = {n:(input_counts[n] if n in input_counts else 0,all_nodes_names.index(n)\/100) for (n, attr) in all_nodes}\n#pos_initial = {n:(-attr[\"size\"],all_nodes_names.index(n)\/100) for (n, attr) in all_nodes}\npos_initial = {n:(-output_counts[n]*attr[\"size\"] if n in output_counts else 0,all_nodes_names.index(n)\/100) for (n, attr) in all_nodes}\n\npos_initial = {n:(\n     -roots_sorted.index(root_names[n])\/10,\n    attr[\"size\"]*10\n) for (n, attr) in all_nodes}\n\npos = nx.spring_layout(G,weight='weight',k=5, pos=pos_initial, iterations=0)\n\nnode_sizes = node_size=[attr[\"size\"]*1000 for (n, attr) in all_nodes]\n\nnx.draw_networkx_nodes(G, pos, node_size =node_sizes , node_color=\"#5DADE2\")\n\nweights = [w for (f,t,w) in G.edges(data=\"weight\")]\nmin_w = min(weights)\nmax_w = max(weights)\n\n\n\nedge_intensity = [0.5 + ((w - min_w)\/(max_w-min_w))*3 for w in weights]\n\nnx.draw_networkx_edges(G, pos,arrows=True,arrowsize=15, arrowstyle='-|>', \n                       width =edge_intensity,\n                       edge_color=\"darkgray\",\n                       edge_cmap = plt.cm.Greys)\ny_offset=0.000\nx_offset = -0.005\n\n#These are too long\nreplace_labels = {\n    'AWS Elastic Compute Cloud (EC2)':'AWS EC2',\n    'AWS Relational Database Service':'AWS RDS',\n    'Amazon Web Services (AWS)':'AWS',\n    'Google Cloud Platform (GCP)':'Google Cloud Platform'\n}\nlabels = dict((n, replace_labels.get(n,n)) for n in G.nodes())\n\nlabel_pos = {node:[coords[0]+x_offset,coords[1]+y_offset] for node, coords in pos.items()}\nnx.draw_networkx_labels(G, label_pos,labels = labels,horizontalalignment = \"right\", font_size=14)\nplt.axis('off')\nplt.show()","fd034b00":"stacks = [get_most_probable_skill_stack(\"Data Scientist\", i) for i in range(1,5)]\ntitles = [title for title in Q24_bucket_labels.values()]\nvisualize_stacks(stacks[1:4], titles[1:4])","9ce281c4":"stacks = [get_most_probable_skill_stack(\"Data Analyst\", i) for i in range(1,5)]\ntitles = [title for title in Q24_bucket_labels.values()]\nvisualize_stacks(stacks[1:5], titles[1:4])","fe65f48d":"visualize_role_options(\"Data Engineer\",1,\"Junior Data Engineer\",\"Mid-Level Data Engineer\" )","e1c92164":"before_stack = get_most_probable_skill_stack(\"Data Scientist\", 0)\ncounts = get_average_skill_count_by_title_and_exp(\"Data Scientist\",4)\nafter_stack = fill_stack(before_stack,counts)\n\nvisualize_stack(\"Data Science Newbie\",\n                before_stack, \n                \"Senior Data Scientist\",\n                after_stack, \n                counts)","a4833b51":"#### Observations\n* The average score of a **Mid-level Data Engineer** on Kaggle is **2198**.  On the other hand the skills picked based on actual demands would score **2304**.\n* **Java** is clearly the highest scoring tech added to the new stack.\n* The addition of **PostgresSQL** and **AWS Redshift** are excellent choices for a **Data Engineer**.\n* Adding [D3](https:\/\/d3js.org\/) to the stack opens the possibilities in terms of data visualization. \n* Deep learning with [Keras](https:\/\/keras.io\/) is also an in-demand skill.","0612197c":"#### Observations:\n \n * **Statisticians** seem to consider themselves **Data Scientists** despite the differences highlighted in the charts above.\n * **Data Engineers** on the other hand seem to be unaware of how similar their job is to one of a **Data Scientist**.","b8054ce8":"# Choose the right skills\n\nIn the following lines I'm going to dig into data about the **Data Science** job market. I combined the Kaggle Survey Data with a slice of a larger data set I'm currently working on, costisting of more than 1500 Data-related job postings with technology-stack information. The postings were scraped from StackOverflow over the past 2 years. The only information I've kept is the title and the technology stacks. Comparing this set to the survey results gave me the perfect opportunity to explore how the community fullfills the skill demands of the data-job market.\n\n### Evolution of a Data Scientist\n\n**Data Science** is a booming, rapidly evolving field, most of the associated technologies and frameworks are recent and haven't matured yet. For example [Tensorflow](https:\/\/www.tensorflow.org\/), a leader in symbolic math and neural-network technology was released only 3 years ago. Given the extremely dynamic nature of the **Data Science** technology stacks, I'm going to focus only on developers under 10 years of experience. I believe that above this level technology stacks play a less and less important role in employability, the expectation of knowing a specific set of languages and technologies gives way to less measurable skills such as architecture, management, domain specific experience, and so on.\n\nSo let's see for example how the skill stack of a **Data Scientist** evolves over time. I assumed a \"blank-page\" start, with zero skills, but this is rarely the case. Usually people are already familiar with a few programming languages before starting using the code for data analysis.\n\nI created 5 groups based on the individual's experience:\n* Intern - less than 1 year\n* Junior - 1-2 years\n* Mid-level - 3-5 years\n* Senior - 5-10 years\n* Veteran - more than 10 years","02439b82":"#### Tool usage\nWhat kind of tools and software do they use?\n> Q12 - What is the primary tool that you use to analyze data?","4c8f9f88":"#### Observations:\n\n* Between **Junior** and **Middle** level the most important improvement is the learning of a second Database provider.\n* Between **Middle** level and **Senior** level learning **R** opens up the possibilities.","7e3dd4b1":"# Conclusions\n\n* Pick the right role. If you like writing code, become a **Software Engineer**, if you enjoy math, become a **Statistician**, if you are a DevOps kind of person, do **Data Engineering**, if you like data visualization and doing EDA, be a **Data Analyst**. If you want all these, become a **Data Scientist**!\n* Pick the right skills. **Python**, **Java** and **SQL** are in high demand. **Git**, **Docker** and **Linux** are essential. **ScikitLearn**, **Tensorflow** and **Keras** are the leaders  ML libraries. **Hadoop**, **AWS** and **Spark** can't be overlooked. \n","dbe25f6a":"### Now let's see a Data Analyst","1d0af301":"#### Observations:\n* A quick glance on the data reveals that more than 88% of all job descriptions require at least 3 different skills in the requirements. More than 60% mention 5 different terms the applicant is expected to be familiar with.\n\nTime is limited so we have to decide carefully in which skills to invest it to maximize our chances on the job market. The next section intends to help us select these skills by analysing their frequency in the data set.","ef2cfb61":"#### Data pipeline\n\nNow let's see how much of their time each step of the data pipeline is taking. \n\n> Q34 - During a data science project, approximately what proportion of your time is devoted to the following activities?","e8ed2130":"#### Observations\n*  Most of the identified associations seem to be valid prerequisite relationships, for example **Seaborn** is built as an abstraction over the **Matplotlib** API and there is no **Shiny** without **RStudio**. Others, for example **Tensorflow** - **Caffe** are competing relationships where the more popular alternative (in this case **Tensorflow**) is usually already explored before learning the less popular one.\n* [Xgboost](https:\/\/xgboost.readthedocs.io\/en\/latest\/), [lightgbm](https:\/\/github.com\/Microsoft\/LightGBM) and [catboost](https:\/\/github.com\/catboost\/catboost) are advanced ensemble tools which can wait until you feel limited by the possibilities offered by [ScikitLearn](https:\/\/scikit-learn.org\/stable\/).","e0dca7c7":"### What are they doing each day?\n\n\nLet's explore these roles by work methodology. For the first 5 categories I calculated the percent of people considering that particular acitivity important. \n\n> Q11 - Which activities make up an important of your role at work?\n\nFor the importance of code writing the answers were given as percents, so I used them directly.\n\n>Q23: Approximately what percent of your time at work or school is spent actively coding?\n\nThe colored bars represent the deviation from the mean measured in percent points. The little dots next to the boxes indicate that the value is the maximum (blue) or minimum(red) in the row in question.\n\n#### Main activities\n","796035df":"#### Observations:\n* The marketing & business roles form a cluster group in the center left area of the chart, not far from the closely related **Management** clusters, which use the reports and analyses provided by the **Data Analysts** to making data driven business decisions.\n* The academical roles, namely **Principal Investigator**, **Research Investigator** and **Research Scientist** form a tight cluster group in the top center part of the chart. They are the most similar to the **Data Scientist** role from a technical point of view.\n* The **Student** cluster lies close to the other academic roles, with an observable offset towards the more practical, engineering-related clusters.\n* **Database Engineer** lies further than expected from the **Data Engineer** cluster. The reason might be that these roles consist of a lot of querying, report generation and data maintenance which is more closely related to what the Business cluster group does. \n* **Student**, and **Chief Officer** and **Consultant** are the real Jack-of-All-Trades roles in the **Data Science** community. I imagine these **Chief Officers** as former **Data Scientist** now leading a **AI** or **Big Data**-driven startups.","69da9378":"### Learn Python. No excuses.\n\nI visualized the top 50 technologies by percent of users familiar with the technology in question (Supply) and percent of job requirements mentioning it (Demand). Technologies which appear in both lists are connected with gray ribbons.\nThe gray-colored skills are the ones missing from either the Supply or the Demand column. \n\nI decided not to include some of the terms which I found too general, but nonetheless these concepts are among the top 50 most important things required by employers. In order of importance: `Machine Learning`, `Big Data`, `Cloud`, `Sys Admin`, `Agile`, `Algorithm`, `NoSQL`, `Database`, `Data Science`, `REST`, `Deep Learning`, `Artificial Intelligence`, `Web-Services`, `Testing`, `Computer Vision`, `QA`, `Security`, `Automation`, `Design`, `Microservices`, `DevOps`, `Data Warehouse`, `NLP`, `Statistics`, `ETL`, `Data`, `Neural Network`, `Time Series`, `Data Modeling`, `UI`, `JSON`, `Apache`, `CI` and `API`.\n","c0f09a3b":"# The skills they need\n\nTo find out the other part of the story, let's take a look at the requirements of Data-related job postings. The original data set I used consists of over 30.000 entries. Selecting the ones releveant to the **Data Science** field wasn't a straightforward process. I tried to limit the selection to those containing **Machine Learning**, **Data Science** or **Big Data** related terms. The filtered results can be found in the dataset I published on **Kaggle**. Here is a breakdown of the percentages of numbers of requirements:","2de34f32":"#### Observations:\n* Between **Junior** and **Middle** level **Tensorflow** gives the biggest boost.\n* Between **Middle** and **Senior** level the addition of **R** seems to be the improvement, rising the score above 1500.","f7d7543c":"#### Observations\n* These two questions perfectly capture the essence of the roles.  Most roles have a single strong-point, **Data Scientist** being the exception. They give above-average importance to each of the activities and are clear leaders in both building and exploring ML solutions. This supports the preconception that Data Scientist are expected to be well versed in multiple domains.","e654a303":"### Data Scientist Technology Stacks\n\nNow let's take a look at the most probable stacks at each experience level. To measure the employability chances of the stack we will use a simple metric: we get **one point** for knowing the first skill of a requirement set, **two points** for the second, **three** for the third and so on. This way deeper stacks will get a proper boost. The final score is the sum of all points a candidate receives for each of the job requirements.\n\nOf course deep knowledge of a handful of technologies will always beat superficial familiarity with several, but depth is much harder to measure than breadth so I will stick to comparing stacks based on the number skills matching a requirement.  ","6e64e952":"### Black-box models?\nNah! You just haven't studied enough!","d504e661":"#### Observations:\n* The average score of a **Senior Data Scientist** on Kaggle is **1735**. A same sized stack picked freely based on skill demands scores **2469**.\n* The core of the stack are **Python**, **Java** and **SQL** . These three cover more than 50% of all job requirements. \n* [Amazon Web Services](https:\/\/aws.amazon.com\/) also gives an excellent boost to this stack. \n* [D3](https:\/\/d3js.org\/), [Matplotlib](https:\/\/matplotlib.org\/index.html) and [Shiny](https:\/\/shiny.rstudio.com\/) cover the most important dataviz tools in **Javascript**, **Python** and **R**. \n* **PostgreSQL** and **MySQL** are the most popular open-source Databases, which maximize our chances in this domain. ","f81c555c":"#### Observations\n* The learning curve is very steep at the beginning but reaches a plateau at Senior level.\n* It's interesting to note that **Programming Language** is one category in which knowledge doesn't decline with age. Most of the languages used in Data Science have been around for a very long time. [SQL (1974)](https:\/\/en.wikipedia.org\/wiki\/SQL), [Python (1991)](https:\/\/en.wikipedia.org\/wiki\/Python_(programming_language), [Java (1995)](https:\/\/en.wikipedia.org\/wiki\/Java_(programming_language).","ee8196cb":"# Choosing the right role\n\nIf you are a student or just recently started your career and are still unsure about the technology skills to target or if you are an experienced data scientist looking for ways to improve your chances on the job market, the first step is to identify all possibilities and explore the differences between them. The roles I personally consider most relevant are:\n\n* **Data Scientist**\n* **Data Analyst**\n* **Software Engineer**\n* **Research Scientist**\n* **Data Engineer**\n* **Statistician**\n\n#### Mapping the community\n\nI cleaned up the data by imputing missing values, transforming to dummy variables and normalizing numerical values. I then used [LDA](https:\/\/sebastianraschka.com\/Articles\/2014_python_lda.html) to perform supervised [dimensionality reduction](https:\/\/idyll.pub\/post\/dimensionality-reduction-293e465c2a3443e8941b016d\/) and map the resulting feature vectors to a 2 dimensional space while maximing the separation of the clusters.  The feature set I used for LDA is based on questions which I felt are the best predictors of a person's daily activity at work. \n\nThe interpretation of the resulting principal components and naming of the axes is highly subjective because it is the weighted combination of multiple features and can be too complex to describe in simple words. Based on the resulting positions of the various groups I like to think of **PC1(horizontal)** as a **Business (left) vs.  Code (right)** axis and of **PC2 (vertical)** as an **Engineering (bottom) vs. Research (top)** axis.\n\n### Where are these role situated in relation to each-other?\n\nAs you probably noticed on the first image, the clusters are clearly overlapping, the definitions of these roles are extremely fuzzy. For example the **Data Analyst** and **Business Analyst** roles share most of the same characteristics with slight differences in the application of the conclusions resulting from the analysis.  To avoid cluttering \/ having too many categories, in some of the charts I merged some of the less-represented groups:\n* (Student, Not employed, Consultant, Other, Developer Advocate) \u2192 drop\n* (Project Manager, Chief Officer) \u2192 Manager\n* (Research Assistant, Principal Investigator) \u2192 Research Scientist\n* (Data Journalist, Business Analyst) \u2192 Business Analyst\n* (Database Engineer) \u2192 Data Engineer\n\nDespite the overlaps, the positions of the cluster centroids reveal some interesting patterns. The size of the bubbles represent the size of the group.\n","9a9ddafc":"### How far am I from being a Data Scientist?\n\nLet's take the **Data Scientist** role as a reference and explore the way other roles relate to it. The answers to Question Nr.26 helps us estimate each individual's self assessment:\n\n> Q26 - Do you consider yourself to be a data scientist?","c8ec2940":"### Learning order\n\nNobody in their right mind would start learning programming by working with **MapReduce** or **Hadoop**. Yout first need to learn the ABC before starting writing a novel, they say. So if there is a natural order in learning these skills, can we reveal this underlying sequentiality from the data?\n\n[Association Rules](https:\/\/en.wikipedia.org\/wiki\/Association_rule_learning) might give us some answers. The goal of **Association Rule Mining** is the identification of patterns of cooccurrences between items based on their individual occurrence probability vs. their simultaneous occurrence probability. If **RStudio** for example appears in almost every stack where **R** is present, but the opposite is not true, we could presume that **R** is a prerequisite for **RStudio**. \nHere is a graph of the associations identified above a predefined confidence threshold, the weight of the edges represent the confidence, the size of the nodes and the vertical position both represent the occurrence frequency of that specific skill.","c2ff58ca":"### The Kaggle Data Science Survey\n\n\nIf you missed it, don't worry, you can join us next year. With over 20,000 respondents, The **Kaggle ML & DS 2018** is one of, if not the most important and extensive surveys ever conducted about  the state of the Data Science community. The data set contains answers to over 50 questions, many of them multi-choice or even free text, so there's a huge playground for data lovers to explore.\n\n\n# Let's get employed!\n\nThe theme of this study is **employability**, so most of my exploration will focus on a - rather large - subset of the community: those employed and currently active in one of the major roles of the Data Science field.\n\n>  **employability**, noun\n>\n> Being in possession of skills which are in high demand on the job market.\n> \n> <sub><cite>(my personal definition)<\/cite><\/sub>\n\n\nIn the first part of this kernel I will use a selection of the survey data to answer questions like:\n\n * **Which are the most important roles in the Data Science field? **\n\n* **What are the individual characteristics of these roles? **\n\n* **What sets them apart, makes them different? **\n\n* **Which tech skills are most required for each of them? **\n\nI think the answers to these questions are especially relevant to students, to the unemployed and to anyone planning a career transition or simply ways to improve themselves. Hopefully the reader of this kernel doesn't share the following opinion:\n\n> \"I'm a student, don't ask these questions please\"\n>\n> <sub><cite>Responder #27 in Free Form Responses<\/cite><\/sub>\n\n\nIn  the second part I will focus on technology stacks specific to each role, comparing the supply and demand for them on the job market and identifying the most useful skills to possess.\n\n\n\n","b21f40ec":"#### Observations\n* **Statisticians** are leaders in advanced statistics by a far margin.\n* **Research Scientists** are leaders in usage of other types of Software Packages, these are probably the domain-specific software they use for bioinformatics, physics, math.","ac1139bc":"#### Observations:\n* The cleaning of the data is the most important part of the data pipeline. **Research Scientist ** and **Software Engineer** are the lucky ones who get their data served on a silver plate. They are also the ones who spend the least time analysing it.","1dbad0a9":"#### Observations:\n    \n* Learn `Python`. No excuses. [Pandas](https:\/\/pandas.pydata.org\/) and [Numpy](https:\/\/www.numpy.org\/), its most important data-wrangling libraries also appear on the top 50 list.\n* Most **IDE**s, notebooks and data visualization libraries are never mentioned in job requirements. \n* On the other hand [Hadoop](https:\/\/hadoop.apache.org\/), [Spark](https:\/\/spark.apache.org\/) and [Elasticsearch](https:\/\/www.elastic.co\/) are some of the most sought-after technologies on the job market, but were not mentioned in the Survey. Other **Big Data** technologies which appear in the top 50 are: [Hive](https:\/\/hive.apache.org\/), [Cassandra](http:\/\/cassandra.apache.org\/), [Kubernetes](https:\/\/kubernetes.io\/) and [Kafka](https:\/\/kafka.apache.org\/).\n* **Java** is important, but its importance in the global ranking may be exaggerated by the large proportion of **Software-Engineering** jobs in the dataset. The same with **Javascript**, **React** and **Angular**. These jobs were picked because they form in a way part of the data-pipeline, but they tend to over-inflate the presence of some techs in the list.\n* **OS** usage was not surveyed, but it's important to note that **Linux** is in demand, mentioned in over 5% of all data-related job requirements.\n* [Tableau](https:\/\/www.tableau.com\/) was not mentioned in the survey among the data-visualization tools, even though based on **Google Trends**, [it is more than 2 times more popular](https:\/\/trends.google.com\/trends\/explore?geo=US&q=learn%20tableau,learn%20matplotlib) than the most popular **Python** dataviz tool, **Matplotlib**. Its language-agnostic, versatile, easy to learn and it's on the rise. \n* **Git** and [Docker](https:\/\/www.docker.com\/) are in my opinion indispensable tools. Even though they appear only in the bottom half of the list, they should be among the first skills to aquire for a developer.","7c50248c":"Observations:\n* Notice that the order of the roles is exactly the same for both answers. Being able to deeply understanding **Machine Learning** models is an essential skill for **Data Scientists**.\n* The **Data Scientists** on the left side of the 0 axis are the self-doubters. They picked the **Data Scientist** title when they were asked the first time, but began to question their true identity when facing the question for the second time: \"Really? You? Look at yourself! You consider yourself to be a data scientist?\"\n\nWe can estimate each role's similarity to the **Data Scientist** role using our LDA feature-space by calculating the [Euclidean distance](https:\/\/tekmarathon.com\/2015\/11\/15\/different-similaritydistance-measures-in-machine-learning\/) between the cluster centroids.\n\nNow that we have a general idea of what each role entails and have seen the way they seen themselves, let's plot against each other the \"Self Assessed\", perceived similarity and the actual similarity between each role and the **Data Scientist** role.\n\nThe size of the bubble represents the size of the group, the X axis represents the distance of the group to the **Data Scientist** role measured by self-assessment (Q26 - Are you a Data Scientist?), the Y axis represents the distance of the group to the **Data Scientist** role.","0837b841":"#### Observations\n\n* **Statisticians** are the leaders in having the highest number of both the *know-it-all* and *I-know-nothing types*.\n* Almost every **Data Scientist** has an opinion on how opaque or black ML models are, somewhat surprisingly **Data Analysts** are the least intrigued by the question.","05d75baa":"### The Royal Flush of Data Science Technology Stacks\n\nWhat happens if we start with a clean slate and get the choose all skills from scratch to fill the stack of a **Senior Data Scientist**?","6e156534":" #### Skill importance by role\nThe importance of each skill varies between the roles, so while it's useful to have an overview, breaking down the supply and demand by job title gives us a more valuable insight:","b17fad12":"### Let's play a game!\n\nLet's start the Game as a **Junior Data Engineer** with a moderate number of skills. We know the average tech stack size of a person on the next level (**Mid-level**), so let's pick the same number of skills from each category and try to maximize our employability. We will then compare the results of our picks to the average score of a person on the next level."}}