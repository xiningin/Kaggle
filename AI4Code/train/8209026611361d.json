{"cell_type":{"f49fb9f6":"code","fe6d09b7":"code","6772a449":"code","7336e778":"code","03d9d464":"code","aa553a6e":"code","72905b6a":"code","a917e8cd":"code","c49e5bfb":"code","cbe6fd35":"markdown","e5a817a1":"markdown","7bbb85e2":"markdown","438fc2c0":"markdown","065494f8":"markdown","6064fe7e":"markdown","ce7fdd29":"markdown","fba7a229":"markdown"},"source":{"f49fb9f6":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","fe6d09b7":"seed = 1234\nnp.random.seed(seed)\ntf.random.set_seed(seed)","6772a449":"# Model \/ data parameters\nnum_classes = 10\ninput_shape = (28, 28, 1)\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n# Scale images to the [0, 1] range\nx_train = x_train.astype(\"float32\") \/ 255\nx_test = x_test.astype(\"float32\") \/ 255\n# Make sure images have shape (28, 28, 1)\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\nprint(\"x_train shape:\", x_train.shape)\nprint(x_train.shape[0], \"train samples\")\nprint(x_test.shape[0], \"test samples\")\n\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)","7336e778":"model = keras.Sequential(\n    [\n        keras.Input(shape=input_shape),\n        layers.Conv2D(32, kernel_size=(3, 3), activation=tf.math.sin, kernel_initializer=\"he_uniform\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Conv2D(64, kernel_size=(3, 3), activation=tf.math.sin, kernel_initializer=\"he_uniform\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Flatten(),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation=\"softmax\"),\n    ]\n)\n\nmodel.summary()","03d9d464":"batch_size = 128\nepochs = 15\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)","aa553a6e":"score = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])","72905b6a":"# Model \/ data parameters\nnum_classes = 10\ninput_shape = (32, 32, 1)\n\n\n# The data, split between train and test sets:\n(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n\n# Scale images to the [0, 1] range\nx_train = x_train.astype(\"float32\") \/ 255\nx_test = x_test.astype(\"float32\") \/ 255\n\nprint(\"x_train shape:\", x_train.shape)\nprint(x_train.shape[0], \"train samples\")\nprint(x_test.shape[0], \"test samples\")\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)","a917e8cd":"model = keras.models.Sequential()\nmodel.add(layers.Conv2D(32,\n                 (3, 3),\n                 padding='same',\n                 kernel_initializer=\"he_uniform\",\n                 activation=tf.math.sin,\n                 input_shape=x_train.shape[1:]))\nmodel.add(layers.Conv2D(32,\n                 (3, 3),\n                 kernel_initializer=\"he_uniform\",\n                 activation=tf.math.sin))\nmodel.add(layers.MaxPooling2D(pool_size=(2, 2)))\nmodel.add(layers.Dropout(0.25))\n\nmodel.add(layers.Conv2D(64,\n                 (3, 3),\n                 padding='same',\n                 kernel_initializer=\"he_uniform\",\n                 activation=tf.math.sin))\nmodel.add(layers.Conv2D(64,\n                 (3, 3),\n                 kernel_initializer=\"he_uniform\",\n                 activation=tf.math.sin))\nmodel.add(layers.MaxPooling2D(pool_size=(2, 2)))\nmodel.add(layers.Dropout(0.25))\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512, kernel_initializer=\"he_uniform\", activation=tf.math.sin))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(num_classes, activation=\"softmax\"))\n\nmodel.summary()","c49e5bfb":"epochs=25\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)","cbe6fd35":"This example has been taken from the old keras [website](https:\/\/keras.io\/examples\/cifar10_cnn\/) and the following modifications were done:\n\n1. Change activation function from `relu` to `sin`\n2. Change initializer from `glorot_uniform` to `he_uniform`","e5a817a1":"A new [paper](https:\/\/arxiv.org\/pdf\/2006.09661.pdf) that was recently preseneted in CVPR 2020 proposed to use `sine` activation function as compared to `relu`. As shown here in the [video](https:\/\/www.youtube.com\/watch?time_continue=3&v=Q2fLWGBeaiI&feature=emb_logo), the representations learned by the `sine` activations are remarkable as compared to any other activation. The paper also suggests that using `sine` activation, we get better convergence.\n\nTo this end, I thought to put up a few small scale experiments to check the validity of the claims. Here I am going to demonstrate the usage on `MNSIT` and `CIFAR` dataset.\n\n**Note:** I am doing some more experimentation, so keep an eye on the noetbooks section for more on it. Also, blogpost coming soon as well.","7bbb85e2":"**Conclusion:** \n\nThis network performed far better than the original one with `relu` activations. This network achieved much lower loss `(~0.25 vs ~0.26)` on the test set. The test accuracy is also much better `(~99 vs ~991xx)`","438fc2c0":"**Conclusion:** \n\nEven without any `augmentation`, this network achieved the same validation accuracy `(~74%-75%)` whereas heavy augmentation is used in the original implementation. Although you can argue that with augmentation the network would take much more time to generalize as in the case of the original implementation, I would say that same holds for `overfitting`. The network isn't that bad in this case. ","065494f8":"## Final Thoughts\n\n1. As per the paper, it seems we are surely having faster convergence. Though I am running a few more tests and will report more results soon.\n2. I think `sine` activation is the first **true** competitor of `relu`.\n3. The best part is that if this holds for other experiments as well, the code changes are negligible.\n\n\n**Please upvote if you liked the kernel. People think I do nothing on Kagggle except bragging, your votes will help.**","6064fe7e":"## CIFAR-10","ce7fdd29":"## MNIST","fba7a229":"This example has been taken from the Keras [website](https:\/\/keras.io\/examples\/vision\/mnist_convnet\/) and the following modifications were done:\n\n1. Change activation function from `relu` to `sin`\n2. Change initializer from `glorot_uniform` to `he_uniform`"}}