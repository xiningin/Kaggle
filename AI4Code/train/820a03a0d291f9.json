{"cell_type":{"210e158b":"code","21aacdb1":"code","700bb742":"code","290b8777":"code","076335cf":"code","2b72626f":"code","8884b116":"code","22e3ef09":"code","d8d08404":"code","8b544268":"code","139d4cbe":"code","d66556fe":"code","2cf1ee48":"code","03bb96a7":"code","32e45964":"code","12fb3ba7":"code","ef0119fd":"code","b61ff8e5":"code","e47b1d66":"code","3e8b2732":"markdown","3b3b6b1d":"markdown","1cb1d086":"markdown","5e0589f3":"markdown","13dab2cb":"markdown"},"source":{"210e158b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","21aacdb1":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col='PassengerId')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv', index_col='PassengerId')\ncols = train_data.columns\n# to make this notebook's output identical at every run\nnp.random.seed(42)","700bb742":"train_data.head()","290b8777":"# Drop non-relevant columns\ntrain_data = train_data.drop(['Name','Ticket'], axis = 1)\ntest_data = test_data.drop(['Name','Ticket'], axis = 1)","076335cf":"train_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","2b72626f":"train_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","8884b116":"best_score = float(\"inf\")\nbest_model = None\n\nX = train_data.drop('Survived',axis = 1)\nY = train_data.Survived","22e3ef09":"# Too missing data in Cabin cols\n# Drop non-relevant columns\nX = X.drop(['Cabin'], axis = 1)\ntest_data = test_data.drop(['Cabin'], axis = 1)","d8d08404":"X.info()","8b544268":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\n\ndef Model(model):\n    num_pipeline = Pipeline(steps=[\n            ('imputer', SimpleImputer(strategy=\"median\")),\n            ('scaler', StandardScaler()),\n            ])\n\n    cat_pipeline = Pipeline(steps=[\n        ('imputer',SimpleImputer(strategy=\"most_frequent\")),\n        ('one_hot_encoder', OneHotEncoder(sparse=False)),\n        ])\n\n    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n    categorical_features = X.select_dtypes(include=['object']).columns\n\n    preprocessor = ColumnTransformer([\n    (\"num_pipeline\", num_pipeline, numeric_features),\n    (\"cat_pipeline\", cat_pipeline, categorical_features),\n    ])\n\n    return Pipeline(steps=[\n                          ('preprocessor', preprocessor),\n                          ('model', model)])\n\n\n\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size=0.20, random_state=1)\nprint(len(X_train), \"train +\", len(X_valid), \"valid +\", len(test_data), \"test\")","139d4cbe":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\nmodel_name = [\"Logistic regression\", \n              \"Naive Bayes\",\n              \"Stochastic Gradient Descent \", \n              \"K-Nearest Neighbours\",\n              \"Decision Tree\",\n              \"Random Forest\",\n              \"Support Vector Machine\",\n              \"XGBoost\"]\n\nmodels = [\n    LogisticRegression(random_state=0),\n    GaussianNB(),\n    SGDClassifier(max_iter=1000, tol=1e-3),\n    KNeighborsClassifier(n_neighbors=2),\n    DecisionTreeClassifier(random_state=0),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    SVC(gamma='auto'),\n    XGBClassifier()]","d66556fe":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\n\ni = 0\nfor modele in models:\n    model = Model(modele)\n#     1 -Train test split \n#     model.fit(X_train, Y_train)   \n#     prediction_train = model.predict(X_train)\n#     prediction_valid = model.predict(X_valid)\n\n    \n    print('Model ',model_name[i])\n#     2- Cross validation\n    skfold = StratifiedKFold(n_splits=10)\n    scores = cross_val_score(model, X, Y, cv=skfold)\n    train_accuracy = round(scores.mean()* 100, 2) #, scores.std() * 2\n    print(\"Accuracy: %.2f%%\" % (train_accuracy))\n    \n#     predictions = cross_val_predict(model, X_valid, Y_valid, cv=5)\n#     print(\"Validation: \", round(model.score(X_valid, Y_valid) * 100, 2)) #accuracy_score(Y_valid, prediction_valid))\n    \n    print('-'*40)\n    i+=1\n    ","2cf1ee48":"from sklearn.model_selection import GridSearchCV\n\nmodel_name = [\"Support Vector Machine\"]\n# Set the parameters by cross-validation\n# defining parameter range \nparam_grid = {'model__C': [0.1, 1, 10, 100],  \n              'model__gamma': [1, 0.1, 0.01, 0.001], \n              'model__kernel': ['rbf','poly', 'linear']}  \n\nclf = GridSearchCV(\n        Model(SVC()), \n        param_grid, \n        scoring='accuracy', \n        cv=5,#skfold,\n        verbose = 3\n    )\nclf.fit(X, Y)","03bb96a7":"best_result = clf.best_score_\nprint(\"Best accuracy:\",round(best_result*100,2),'%')\n\nprint(\"Best parameters set found on development set:\")\nprint()\nprint(clf.best_params_)","32e45964":"from sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\n\n# Set the parameters by cross-validation\n# defining parameter range \nparam_grid = {\n        'model__min_child_weight': [1, 5, 10],\n        'model__gamma': [0.5, 1, 1.5, 2, 5],\n        'model__subsample': [0.6, 0.8, 1.0],\n        'model__colsample_bytree': [0.6, 0.8, 1.0],\n        'model__max_depth': [3, 4, 5]\n             }  \n\nclf = GridSearchCV(\n        Model(xgb.XGBClassifier()), \n        param_grid, \n        scoring='accuracy', \n        cv=5,#skfold,\n        verbose = 3,\n        refit=True\n    )\nclf.fit(X, Y)","12fb3ba7":"best_result = clf.best_score_\nprint(\"Best accuracy:\",round(best_result*100,2),'%')\n\nprint(\"Best parameters set found on development set:\")\nprint()\nprint(clf.best_params_)","ef0119fd":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Set the parameters by cross-validation\n# defining parameter range \nparam_grid = {\n        'model__n_estimators': [120, 140],\n        'model__max_depth': [30,50],\n        'model__min_samples_split': [2,3],\n        'model__min_samples_split': [3,5],\n        'model__class_weight': [{0:1, 1:1},{0:1,1:5},{0:1,1:3},'balanced']\n             }  \n\nclf = GridSearchCV(\n        Model(RandomForestClassifier()), \n        param_grid, \n        scoring='accuracy', \n        cv=5,#skfold,\n        verbose = 3,\n        refit=True\n    )\nclf.fit(X, Y)","b61ff8e5":"best_result = clf.best_score_\nprint(\"Best accuracy:\",round(best_result*100,2),'%')\n\nprint(\"Best parameters set found on development set:\")\nprint()\nprint(clf.best_params_)","e47b1d66":"best_model = Model(xgb.XGBClassifier(colsample_bytree= 0.8, gamma= 0.5, max_depth= 5, min_child_weight= 5, subsample= 0.8))\nbest_model.fit(X, Y)\n\npredicted_classes = best_model.predict(test_data)\n\noutput = pd.DataFrame({'PassengerId': test_data.index,\n                       'Survived': predicted_classes})\n\n# you could use any filename. We choose submission here\noutput.to_csv('submission.csv', index=False)\n","3e8b2732":"# Check for correlations","3b3b6b1d":"# Model Random Forest","1cb1d086":"# Model  Support Vector Machine","5e0589f3":"# Model XGBoost","13dab2cb":"# Final Model"}}