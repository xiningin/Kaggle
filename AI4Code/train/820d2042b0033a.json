{"cell_type":{"0899a0d4":"code","d2ed4013":"code","541f5182":"code","c73b11be":"code","e9e047da":"code","4b842881":"code","dd65f389":"code","664c25e4":"code","b58670be":"code","6f4a40df":"code","65388fcc":"code","94cd435e":"code","7c8e03e1":"code","dc1dddc1":"code","8fc01fae":"code","8b5f16ab":"code","4fe54f16":"code","c5d08e47":"code","0475df46":"code","71f997c0":"code","c56871a5":"code","6272f002":"code","2cacd714":"code","f7804ac1":"code","6395e7eb":"code","6a390eb1":"code","4c73b1ad":"code","be8e7268":"code","c613d9fe":"code","4d0a1168":"code","40905568":"code","6b6b87fa":"code","8794a0f2":"code","59aa8e3d":"code","aada51ba":"code","85660252":"code","fe91f0a4":"code","9ac03a8b":"code","774e6d87":"code","6483525e":"code","5677e146":"code","bb7f02ee":"code","d679803c":"code","9ce392d6":"code","aae3c4d7":"code","8ee78a89":"code","03a76f64":"code","5343e0d8":"code","a462a4dc":"code","69bdf9d4":"code","324d0aae":"code","452cf009":"code","8dcec96c":"code","9f97df09":"markdown","ef1fa7a2":"markdown","210df50f":"markdown","35804051":"markdown","5a888720":"markdown","aabdc8e7":"markdown","dff73a72":"markdown","d218ef8a":"markdown","b8df109e":"markdown"},"source":{"0899a0d4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d2ed4013":"pip install tensorflow==2.0.0-rc1","541f5182":"import tensorflow as tf\n\nfrom tensorflow import keras\n","c73b11be":"import numpy as np\nimport pandas as pd\n\n\nprint(tf.__version__)","e9e047da":"imdb = keras.datasets.imdb\n\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)","4b842881":"print(\"Training entries: {}, labels: {}\".format(len(train_data), len(train_labels)))","dd65f389":"print(train_data[0])","664c25e4":"len(train_data[0]), len(train_data[1])","b58670be":"# A dictionary mapping words to an integer index\nword_index = imdb.get_word_index()\n\n# The first indices are reserved\nword_index = {k:(v+3) for k,v in word_index.items()}\nword_index[\"<PAD>\"] = 0\nword_index[\"<START>\"] = 1\nword_index[\"<UNK>\"] = 2  # unknown\nword_index[\"<UNUSED>\"] = 3\n\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])","6f4a40df":"decode_review(train_data[0])","65388fcc":"train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n                                                        value=word_index[\"<PAD>\"],\n                                                        padding='post',\n                                                        maxlen=256)\n\ntest_data = keras.preprocessing.sequence.pad_sequences(test_data,\n                                                       value=word_index[\"<PAD>\"],\n                                                       padding='post',\n                                                       maxlen=256)","94cd435e":"len(train_data[0]), len(train_data[1])","7c8e03e1":"print(train_data[0])","dc1dddc1":"# input shape is the vocabulary count used for the movie reviews (10,000 words)\nvocab_size = 10000\n\nmodel = keras.Sequential()\nmodel.add(keras.layers.Embedding(vocab_size, 16))\nmodel.add(keras.layers.GlobalAveragePooling1D())\nmodel.add(keras.layers.Dense(16, activation=tf.nn.relu))\nmodel.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\nmodel.summary()","8fc01fae":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])","8b5f16ab":"x_val = train_data[:10000]\npartial_x_train = train_data[10000:]\n\ny_val = train_labels[:10000]\npartial_y_train = train_labels[10000:]","4fe54f16":"history = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=40,\n                    batch_size=512,\n                    validation_data=(x_val, y_val),\n                    verbose=1)","c5d08e47":"results = model.evaluate(test_data, test_labels)\n\nprint(results)","0475df46":"history_dict = history.history\nhistory_dict.keys()","71f997c0":"import matplotlib.pyplot as plt\n\nacc = history_dict['acc']\nval_acc = history_dict['val_acc']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","c56871a5":"plt.clf()   # clear figure\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","6272f002":"# input shape is the vocabulary count used for the movie reviews (10,000 words)\nvocab_size = 10000\n\nmodel1 = keras.Sequential()\n#model1.add(keras.layers.Embedding(vocab_size, 16))\nmodel1.add(keras.layers.GlobalAveragePooling1D())\nmodel1.add(keras.layers.Dense(16, activation=tf.nn.relu))\nmodel1.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\nmodel1.summary()","2cacd714":"# input shape is the vocabulary count used for the movie reviews (10,000 words)\nvocab_size = 10000\n\nmodel1 = keras.Sequential()\nmodel1.add(keras.layers(vocab_size, 16))\nmodel1.add(keras.layers.GlobalAveragePooling1D())\nmodel1.add(keras.layers.Dense(16, activation=tf.nn.relu))\nmodel1.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\nmodel1.summary()","f7804ac1":"# input shape is the vocabulary count used for the movie reviews (10,000 words)\nvocab_size = 10000\n\nmodel2 = keras.Sequential()\nmodel2.add(keras.layers.Embedding(vocab_size, 4))\nmodel2.add(keras.layers.GlobalAveragePooling1D())\nmodel2.add(keras.layers.Dense(4, activation=tf.nn.relu))\nmodel2.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\nmodel2.summary()","6395e7eb":"model2.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])","6a390eb1":"history2 = model2.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=40,\n                    batch_size=512,\n                    validation_data=(x_val, y_val),\n                    verbose=1)","4c73b1ad":"results2 = model2.evaluate(test_data, test_labels)\n\nprint(results)","be8e7268":"history_dict2 = history2.history\nhistory_dict2.keys()","c613d9fe":"import matplotlib.pyplot as plt\n\nacc = history_dict2['acc']\nval_acc = history_dict2['val_acc']\nloss = history_dict2['loss']\nval_loss = history_dict2['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","4d0a1168":"plt.clf()   # clear figure\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","40905568":"# input shape is the vocabulary count used for the movie reviews (10,000 words)\nvocab_size = 10000\n\nmodel3 = keras.Sequential()\nmodel3.add(keras.layers.Embedding(vocab_size, 512))\nmodel3.add(keras.layers.GlobalAveragePooling1D())\nmodel3.add(keras.layers.Dense(512, activation=tf.nn.relu))\nmodel3.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\nmodel3.summary()","6b6b87fa":"model3.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])","8794a0f2":"history3 = model3.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=40,\n                    batch_size=512,\n                    validation_data=(x_val, y_val),\n                    verbose=1)","59aa8e3d":"results3 = model3.evaluate(test_data, test_labels)\n\nprint(results)","aada51ba":"history_dict3 = history3.history\nhistory_dict3.keys()","85660252":"import matplotlib.pyplot as plt\n\nacc = history_dict3['acc']\nval_acc = history_dict3['val_acc']\nloss = history_dict3['loss']\nval_loss = history_dict3['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","fe91f0a4":"plt.clf()   # clear figure\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","9ac03a8b":"vocab_size = 15000\n\nmodelV1 = keras.Sequential()\nmodelV1.add(keras.layers.Embedding(vocab_size, 16))\nmodelV1.add(keras.layers.GlobalAveragePooling1D())\nmodelV1.add(keras.layers.Dense(16, activation=tf.nn.relu))\nmodelV1.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\nmodelV1.summary()","774e6d87":"modelV1.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])","6483525e":"x_val = train_data[:10000]\npartial_x_train = train_data[10000:]\n\ny_val = train_labels[:10000]\npartial_y_train = train_labels[10000:]","5677e146":"historyV1 = modelV1.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=40,\n                    batch_size=512,\n                    validation_data=(x_val, y_val),\n                    verbose=1)","bb7f02ee":"resultsV1 = modelV1.evaluate(test_data, test_labels)\n\nprint(results)","d679803c":"history_dictV1 = historyV1.history\nhistory_dictV1.keys()","9ce392d6":"import matplotlib.pyplot as plt\n\nacc = history_dictV1['acc']\nval_acc = history_dictV1['val_acc']\nloss = history_dictV1['loss']\nval_loss = history_dictV1['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","aae3c4d7":"plt.clf()   # clear figure\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n","8ee78a89":"imdb = keras.datasets.imdb\n\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=17000)","03a76f64":"vocab_size = 17000\n\nmodelV2 = keras.Sequential()\nmodelV2.add(keras.layers.Embedding(vocab_size, 16))\nmodelV2.add(keras.layers.GlobalAveragePooling1D())\nmodelV2.add(keras.layers.Dense(16, activation=tf.nn.relu))\nmodelV2.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\nmodelV2.summary()","5343e0d8":"modelV2.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])","a462a4dc":"historyV2 = modelV2.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=40,\n                    batch_size=512,\n                    validation_data=(x_val, y_val),\n                    verbose=1)","69bdf9d4":"#resultsV2 = modelV2.evaluate(test_data, test_labels)\n\n#print(results)","324d0aae":"history_dictV2 = historyV2.history\nhistory_dictV2.keys()","452cf009":"import matplotlib.pyplot as plt\n\nacc = history_dictV2['acc']\nval_acc = history_dictV2['val_acc']\nloss = history_dictV2['loss']\nval_loss = history_dictV2['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","8dcec96c":"plt.clf()   # clear figure\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","9f97df09":"without embadding word , the TFModuleWrapper object which using for make gragh not callable","ef1fa7a2":"# Model without Embedding Layer","210df50f":"**with 4 size**","35804051":"# without Embedding word","5a888720":"# with different word embedding size\n# different heddin unit","aabdc8e7":"\n# with different vocabulary size","dff73a72":"# Original Model","d218ef8a":"**with 512 size**","b8df109e":"# As we show, the embedding layer support model to be bulit and without it ,  model will not work"}}