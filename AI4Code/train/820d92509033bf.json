{"cell_type":{"b276a44c":"code","30db25bc":"code","c9b17010":"code","fe3cc982":"code","f0c84038":"code","8b1623c3":"code","ec8eb54c":"code","12aa2c53":"code","c585af0a":"code","7401fd14":"code","e614373d":"code","f2a66cbd":"code","edfb9d1a":"code","273705b9":"code","d0d7fa0d":"code","326510fa":"code","f998c52d":"code","6a070825":"code","c64af9bc":"code","105ad1aa":"code","156ee9bb":"code","bab96230":"code","8c1a4be9":"code","c5f16997":"code","f463db6e":"code","bf0c12c5":"code","92b7c650":"code","6ea217ee":"code","45191a0e":"code","d995d0dc":"code","c7211781":"code","79bf091f":"code","d64df89b":"code","65e3d516":"code","a634c947":"code","9add5811":"code","a65559f5":"code","f2caa8a5":"code","5db58a81":"code","2cb71a6e":"code","31a95024":"code","a398f3fe":"code","108618ec":"code","29bfa7e9":"code","3847f548":"code","9eb9e60a":"code","c4688416":"code","ebc839a7":"code","ab72cd1d":"code","eb908c69":"code","20b75bb4":"code","6b0453d9":"code","ec64e4e3":"markdown","e0398cd6":"markdown","502a2229":"markdown","da9ea3cf":"markdown","d6c680e6":"markdown","287da3b4":"markdown","f08df81a":"markdown","ebc73f62":"markdown","fbc90c7e":"markdown","2ab291a2":"markdown","549d065f":"markdown"},"source":{"b276a44c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import f1_score\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\n%matplotlib inline","30db25bc":"train = pd.read_csv('..\/input\/seleksidukungaib\/train.csv')\ntest = pd.read_csv('..\/input\/seleksidukungaib\/test.csv')","c9b17010":"train.shape, test.shape","fe3cc982":"train.head(10)","f0c84038":"train.isnull().sum()","8b1623c3":"train.drop(['idx','userId','date_collected'],axis=1).corr()","ec8eb54c":"train['date_collected'].equals(train['date'])","12aa2c53":"sns.distplot(train['max_recharge_trx'],kde=False,rug=True)","c585af0a":"sns.distplot(train['average_topup_trx'],kde=False,rug=True)","7401fd14":"sns.distplot(train['average_transfer_trx'],kde=False,rug=True)","e614373d":"sns.distplot(train['num_transfer_trx'],kde=False,rug=True)","f2a66cbd":"sns.countplot(x='isVerifiedPhone',hue='isChurned',data=train,palette='RdBu_r')","edfb9d1a":"noPhoneVerified = len(train[train['isVerifiedPhone']==0].index)\nprint(noPhoneVerified)\nprint(noPhoneVerified\/len(train))","273705b9":"sns.countplot(x='isVerifiedEmail',hue='isChurned',data=train,palette='RdBu_r')","d0d7fa0d":"sns.countplot(x='isUpgradedUser',hue='isChurned',data=train,palette='RdBu_r')","326510fa":"sns.countplot(x='blocked',hue='isChurned',data=train,palette='RdBu_r')","f998c52d":"len(train[train['blocked']==1].index)\/len(train.index)","6a070825":"sns.countplot(x='premium',hue='isChurned',data=train,palette='RdBu_r')","c64af9bc":"sns.countplot(x='super',hue='isChurned',data=train,palette='RdBu_r')","105ad1aa":"len(train[train['super']==True].index)\/len(train.index)","156ee9bb":"sns.countplot(x='userLevel',hue='isChurned',data=train,palette='RdBu_r')","bab96230":"sns.countplot(x='pinEnabled',hue='isChurned',data=train,palette='RdBu_r')","8c1a4be9":"sns.distplot(train['average_recharge_trx'])","c5f16997":"sns.distplot(train['total_transaction'])","f463db6e":"sns.countplot(x='isActive',hue='isChurned',data=train,palette='RdBu_r')","bf0c12c5":"#Some Analysis\n# -column date_collected and column date are exactly the same, I will drop either one\n# -column random_number has no explicitly meaningful information and it has very low correlation towars all others column according to correlation matrix so I will drop it\n# -there are a lot of column contains null value, i will impute some of them which contains a lot of null value and have high correlation towards column isChurned which are\n#  column 'max_reharge_trx','average_topup_trx','total_transaction', and 'average_topup_trx'. Meanwhile for columns that contain very few null value, i will just fill null with 0\n# -Value in 'average_transfer_trx' is either 0 or null, and furthermore using data visualization value for 'num_topup_trx' is constant 0 and since the num of tranfer is 0 \n#  it makes sense that max_transfer_trx and min_topup_trx are all constant 0. So instead of impute 'average_transfer_trx', i will just drop all 'transfer' related column\n# -Based on correlation matrix and data visualization column 'isVerifiedPhone','super','blocked','isActive','isUpgradedUser' is almost constant (has a very low variance)\n#  and furthermore all of them doesn't have very high correlation towards column 'isChurned' so I will drop them.\n# -UserId can be replace with idx which is also a unique identification so i will drop column userId","92b7c650":"#Imputation Function\ndef imputeMaxRecharge(data):\n    MinRecharge = data[0]\n    AvgRecharge = data[1]\n    MaxRecharge = data[2]\n    if pd.isnull(MaxRecharge):\n        return AvgRecharge + 2.5 *(AvgRecharge-MinRecharge) \n    return MaxRecharge\ndef imputeAverageTopUpTrx(data):\n    minTopUp = data[0]\n    averageTopUp = data[1]\n    maxTopUp = data[2]\n    if pd.isnull(averageTopUp):   \n        return (maxTopUp + minTopUp)\/2\n    return averageTopUp\ndef imputeTotalTransaction(data):\n    avgRecharge = data[0]\n    avgTopUp = data[1]\n    totalTransaction = data[2]\n    if pd.isnull(totalTransaction):\n        return avgRecharge + avgTopUp\n    return totalTransaction","6ea217ee":"train['max_recharge_trx'] = train[['min_recharge_trx','average_recharge_trx','max_recharge_trx']].apply(imputeMaxRecharge,axis=1)\ntrain['average_topup_trx'] = train[['min_topup_trx','average_topup_trx','max_topup_trx']].apply(imputeAverageTopUpTrx,axis=1)\ntrain['total_transaction'] = train[['average_recharge_trx','average_topup_trx','total_transaction']].apply(imputeTotalTransaction,axis=1)","45191a0e":"train.drop(['isActive','isUpgradedUser','random_number','date'],axis=1,inplace=True)\ntrain.drop(['isVerifiedPhone','super','blocked','userId'],axis=1,inplace=True)\ntrain.drop(['min_transfer_trx','average_transfer_trx','max_transfer_trx','num_transfer_trx'],axis=1,inplace=True)","d995d0dc":"train.fillna(0,inplace=True)","c7211781":"columns = list(filter(lambda x: x != 'isChurned',list(train.columns.values)))\ntest = test[columns]\nprint(list(test.columns))\nprint(list(train.columns))","79bf091f":"data = pd.concat([train,test],ignore_index=True)","d64df89b":"date = ['date_collected']\nbin = ['premium','pinEnabled']\ncol = date + bin\nle = LabelEncoder()\nfor i in col: \n    data[i] = le.fit_transform(list(data[i].values))","65e3d516":"# Optional step for some algorithm\nscaled_feature = ['num_recharge_trx','min_recharge_trx','average_recharge_trx','max_recharge_trx','num_topup_trx','min_topup_trx','average_topup_trx','max_topup_trx','num_transaction','total_transaction']\ndata[scaled_feature] = StandardScaler().fit_transform(data[scaled_feature])","a634c947":"train = data[~data.isChurned.isnull()]\ntest = data[data.isChurned.isnull()]","9add5811":"#Splitting train dataset for analysis purpose\nX_train_train, X_train_test, y_train_train, y_train_test = train_test_split(train.drop(['isChurned','idx'],axis=1), \n                                                    train['isChurned'], test_size=0.30, \n                                                    random_state=101)","a65559f5":"model = LogisticRegression()\nmodel.fit(X_train_train,y_train_train)\ntrain_pred = model.predict(X_train_test)\nprint(confusion_matrix(y_train_test,train_pred))\nprint(classification_report(y_train_test,train_pred))","f2caa8a5":"#KNN\n# error_rate = []\n# for i in range(1,25):\n#     model = KNeighborsClassifier(n_neighbors=i)\n#     model.fit(X_train_train,y_train_train)\n#     pred_i = model.predict(X_train_test)\n#     error_rate.append(np.mean(pred_i != y_train_test))\n# plt.figure(figsize=(10,6))\n# plt.plot(range(1,25),error_rate,color='blue', linestyle='dashed', marker='o',markerfacecolor='red', markersize=10)\n# plt.title('Error Rate vs. K Value')\n# plt.xlabel('K')\n# plt.ylabel('Error Rate') ","5db58a81":"# With visualization above, i found that with n_neighbors=22 result in the least error_rate \nmodel = KNeighborsClassifier(n_neighbors=22)\nmodel.fit(X_train_train,y_train_train)\ntrain_pred = model.predict(X_train_test)\nprint(confusion_matrix(y_train_test,train_pred))\nprint(classification_report(y_train_test,train_pred))","2cb71a6e":"params = {'n_estimators':[100,200,300,500,800],'max_depth': [10,20,30],'min_samples_split':[2,5,10],'min_samples_leaf':[2,5,10] }","31a95024":"# grid_search = RandomizedSearchCV(estimator= RandomForestClassifier(),param_distributions = params, n_iter = 30,cv=3,verbose=2)\n# grid_search.fit(X_train_train,y_train_train)\n# grid_search.best_params_","a398f3fe":"model = RandomForestClassifier(n_estimators=300,max_depth=30,min_samples_split=5,min_samples_leaf=5)\nmodel.fit(X_train_train,y_train_train)\ntrain_pred = model.predict(X_train_test)\nprint(confusion_matrix(y_train_test,train_pred))\nprint(classification_report(y_train_test,train_pred))","108618ec":"params = {'n_estimators': [100,200,300,500,800],'gamma': [1,10,50,100],'max_depth': [3, 6,10],'learning_rate': [0.1, 0.01, 0.05]}","29bfa7e9":"# grid_search =GridSearchCV(estimator= xgb.XGBClassifier(),param_grid=params,cv=3,verbose=2)\n# grid_search.fit(X_train_train,y_train_train)\n# grid_search.best_params_","3847f548":"#XGBoosting\nmodel = xgb.XGBClassifier(gamma=1,learning_rate=0.05,max_depth=6,n_estimators=800)\nmodel.fit(X_train_train,y_train_train)\ntrain_pred = model.predict(X_train_test)\nprint(confusion_matrix(y_train_test,train_pred))\nprint(classification_report(y_train_test,train_pred))","9eb9e60a":"X_train = train.drop(['isChurned','idx'],axis=1)\ny_train = train['isChurned']","c4688416":"#Logistic Regression\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\npred = model.predict(test.drop(['isChurned','idx'],axis=1))","ebc839a7":"#KNN\n# model = KNeighborsClassifier(n_neighbors=22)\n# model.fit(X_train,y_train)\n# pred = model.predict(test.drop(['isChurned','idx'],axis=1))","ab72cd1d":"#Random Forest\n# model = RandomForestClassifier(n_estimators=300,max_depth=30,min_samples_split=5,min_samples_leaf=5)\n# model.fit(X_train,y_train)\n# pred = model.predict(test.drop(['isChurned','idx'],axis=1))","eb908c69":"#XG Boosting\n# model = xgb.XGBClassifier(gamma=1,learning_rate=0.05,max_depth=6,n_estimators=800)\n# model.fit(X_train,y_train)\n# pred = model.predict(test.drop(['isChurned','idx'],axis=1))","20b75bb4":"submission = pd.DataFrame({'idx':test['idx'],'isChurned':pred.astype(int)})\nsubmission.to_csv('submission13.csv',index=False)","6b0453d9":"submission","ec64e4e3":"# XGBoosting + GridSearch","e0398cd6":"# Explore Data,Data Analysis, Data Visualization","502a2229":"# KNN","da9ea3cf":"# Logistic Regression","d6c680e6":"# Handle null value","287da3b4":"# Import Library","f08df81a":"# Modelling","ebc73f62":"# Random Forest + GridSearch","fbc90c7e":"# Data Preprocessing","2ab291a2":"# Conclusion\nFrom the classifation report, algorithm xgboosting and random forest gives the highest f1-score but submission result gives highest score when i use logistic regression and random forest so I will use logistic regression as my final model","549d065f":"# Encode non-numeric feature"}}