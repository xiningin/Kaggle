{"cell_type":{"7cfab0ad":"code","2f81d82e":"code","7071a7b4":"code","bb72f99a":"code","fe04f8e9":"code","8116181c":"code","a1b6ed4a":"code","7101849b":"code","cbbe2e56":"code","6bd06b17":"code","f6054708":"code","f3ea7141":"code","42ec6cb8":"code","e6d714a5":"code","dc929582":"code","8d346da0":"code","7f725f43":"code","d1a98bbf":"code","ae0b7d17":"code","adebbfbb":"code","a207a294":"code","7ec9516d":"code","3091b4ad":"code","8fccfe26":"code","3ca9e63f":"code","cf89f3ff":"code","881f0b8f":"code","622f1127":"code","29303c72":"code","6d5f1de4":"code","cd0de49f":"code","d7dba107":"code","b769f350":"code","87572050":"code","73d8eadf":"code","7a950586":"code","9f727b1c":"code","3a714bfa":"code","c728a333":"code","27910dc6":"code","83630423":"code","1921f86d":"code","b9930933":"code","09c97a99":"code","f60aadce":"code","c19343c7":"code","81db3ebc":"code","1942d9cb":"code","3b17f570":"code","0c2b51bd":"code","41b17ff6":"code","d82f7400":"code","05cfade1":"code","24273f3a":"code","20447f92":"code","2b3e48af":"code","5ebc0d16":"code","9799349f":"code","df108a8e":"code","7a2d4df6":"code","49127d7d":"code","1bd3aec3":"code","44b7cec5":"code","a4c65914":"code","db22b6a6":"code","5579188e":"code","0fa11815":"code","3785b114":"code","57cb9b8c":"code","ba744b2a":"code","eb80c0fc":"code","156e748f":"code","6ecbad1f":"code","0f99deb7":"code","cd9188a8":"code","bfa468e0":"code","07dc1043":"code","739931a6":"code","d4d9e29f":"code","8c1f714e":"code","cb026234":"markdown","5235ad5b":"markdown","b1580050":"markdown","a246327b":"markdown","87ce7191":"markdown","01cbef64":"markdown","d4d5b747":"markdown","048b3778":"markdown","9faab306":"markdown","406b076f":"markdown","baacc139":"markdown","b56b8675":"markdown","c2248cc1":"markdown","7039ef56":"markdown","01d98da6":"markdown","6f83c30a":"markdown","48d318ef":"markdown","d0fb9089":"markdown","a6f608f9":"markdown","d6aff66d":"markdown","d06f5671":"markdown","1cffed7b":"markdown","63003073":"markdown","052f1d73":"markdown","4f4b89e2":"markdown","94500b94":"markdown","e5f4c18f":"markdown","6a79f72f":"markdown","a4248ce7":"markdown","a6ee2148":"markdown","5e08e3e5":"markdown","944f28ac":"markdown","2e3a44ee":"markdown","19e68696":"markdown","2c360462":"markdown","a4c865de":"markdown","47bce764":"markdown","b32229ec":"markdown","815e5947":"markdown","c88e928e":"markdown","531c8772":"markdown","f2804e56":"markdown","027ad854":"markdown","c903d9e6":"markdown","a3038530":"markdown","b60d7170":"markdown","f010fcab":"markdown","a8f83164":"markdown","f8eeefde":"markdown","22b0437f":"markdown","04d596cd":"markdown","c47fbf8d":"markdown","b7ef9915":"markdown","0a3ec7b3":"markdown"},"source":{"7cfab0ad":"# data management\nimport pandas as pd\nimport numpy as np\n\n# visualization\nfrom pylab import*\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\n\n# preprocessing\nimport sklearn as sk\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer\n\n# clusters models\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering, DBSCAN\nfrom sklearn import metrics\nfrom sklearn.metrics import silhouette_score, silhouette_samples\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.cluster.hierarchy import dendrogram, linkage","2f81d82e":"data = pd.read_csv(\"..\/input\/ccdata\/CC GENERAL.csv\")","7071a7b4":"data.shape","bb72f99a":"data.head(3)","fe04f8e9":"features = data.columns[1:]","8116181c":"data.info()","a1b6ed4a":"data[features].describe()","7101849b":"data.nunique()","cbbe2e56":"data.isna().sum()","6bd06b17":"print(data[data.CREDIT_LIMIT.isna()].shape[0],' clientes')\nprint(\"{0:.2f}%\".format(100*data[data.CREDIT_LIMIT.isna()].shape[0]\/data.shape[0]))","f6054708":"data[data.CREDIT_LIMIT.isna()]","f3ea7141":"data.CREDIT_LIMIT.describe()","42ec6cb8":"print('Customers with zero credit limit:' , data[data.CREDIT_LIMIT==0].shape[0])","e6d714a5":"data_aux = data[(data.PURCHASES_TRX==0)&(data.CASH_ADVANCE_TRX>0)][['CASH_ADVANCE','CASH_ADVANCE_TRX','CREDIT_LIMIT']]\nprint(data_aux.describe())\ndata_aux.head()","dc929582":"print(data[data.MINIMUM_PAYMENTS.isna()].shape[0],' clientes')\nprint(\"{0:.2f}%\".format(100*data[data.MINIMUM_PAYMENTS.isna()].shape[0]\/data.shape[0]))","8d346da0":"data[data.MINIMUM_PAYMENTS.isna()].head(7)","7f725f43":"data[(data.PAYMENTS==0)].shape[0] == data[(data.PAYMENTS==0)&(data.MINIMUM_PAYMENTS.isna())].shape[0]","d1a98bbf":"data[(data.MINIMUM_PAYMENTS.isna())&(data.PRC_FULL_PAYMENT==0)].shape[0] == data[data.MINIMUM_PAYMENTS.isna()].shape[0]","ae0b7d17":"data.MINIMUM_PAYMENTS.describe()","adebbfbb":"def detect_col_outliers(ls_data):\n     # z_score and filter\n\n    mean = np.mean(ls_data)\n    std = np.std(ls_data)\n   \n    return [i for i in ls_data if np.abs(i-mean) > 4*std]","a207a294":"features_outliers = ['BALANCE','PURCHASES','ONEOFF_PURCHASES','INSTALLMENTS_PURCHASES','CASH_ADVANCE','CASH_ADVANCE_TRX','PURCHASES_TRX','CREDIT_LIMIT','PAYMENTS','MINIMUM_PAYMENTS']\nfor name_col in features_outliers:\n    rtdo = detect_col_outliers(data[name_col])\n    print('-'*50)\n    print(name_col)\n    print('# values outlier: ', len(rtdo))\n    print('{0:.2f}% of the total data'.format(100*len(rtdo)\/data.shape[0]))","7ec9516d":"plt.figure(figsize=(15,10))\nsns.boxplot(data=data[features])\nplt.xticks(rotation=90)","3091b4ad":"nr_rows = len(features_outliers)\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nfor r, col in enumerate(features_outliers):\n    sns.distplot(data[col], ax = axs[r][0]).set_title('Original')\n    sns.distplot(np.sqrt(data[col].tolist()), ax = axs[r][1]).set_title(\"Root Square\")\n    sns.distplot(np.log1p(data[col]), ax = axs[r][2]).set_title('log(1+x)')\nplt.tight_layout()    \nplt.show()  ","8fccfe26":"int_cols = data[features].select_dtypes(include=['int']).columns\nint_cols","3ca9e63f":"for col in int_cols:\n    print(data[col].value_counts().sort_values(ascending=False))\n    print('-'*30)","cf89f3ff":"data[int_cols].hist(figsize=(15,8))\nplt.tight_layout()","881f0b8f":"#Using Pearson Correlation\nplt.figure(figsize=(12,10))\ncorr_m = data[features].corr()\nsns.heatmap(corr_m, annot=True, cmap=plt.cm.Reds).set_title('Correlation Matrix')\nplt.show()","622f1127":"cor_purchases = abs(corr_m[\"PURCHASES\"])\ncor_purchases[cor_purchases>0.5].sort_values(ascending=False)","29303c72":"print('{0:.2f}%'.format(100*sum(data.PURCHASES == data.ONEOFF_PURCHASES + data.INSTALLMENTS_PURCHASES)\/data.shape[0]))","6d5f1de4":"data[data.PURCHASES != data.ONEOFF_PURCHASES + data.INSTALLMENTS_PURCHASES].head()","cd0de49f":"sns.pairplot(data[['PURCHASES','ONEOFF_PURCHASES','INSTALLMENTS_PURCHASES']],\n             markers=\"+\",\n             kind='reg',\n             diag_kind=None, \n             height=4)","d7dba107":"sns.pairplot(data[['CASH_ADVANCE_FREQUENCY','CASH_ADVANCE_TRX']],\n             markers=\"+\",\n             kind='reg',\n             height=4)","b769f350":"features = data.columns[1:]\nfeatures_group1 = ['BALANCE','ONEOFF_PURCHASES','INSTALLMENTS_PURCHASES','CASH_ADVANCE','CASH_ADVANCE_TRX','PURCHASES_TRX','PAYMENTS','CREDIT_LIMIT','MINIMUM_PAYMENTS']\nfeatures_group2 = list(set(features)-set(features_group1))","87572050":"# using median in columns with outliers \ng1_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('log', FunctionTransformer(np.log1p)),\n    #('scaler', MinMaxScaler(feature_range=(0, 1)))\n    ('scaler', StandardScaler())\n    ])\n\n# using median in columns without outliers \ng2_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n    ])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('group1', g1_transformer, features_group1),\n        ('group2', g2_transformer, features_group2),\n        ])","73d8eadf":"preprocessor.fit(data) \nnp_data = preprocessor.transform(data) \nprint(np_data[np.isnan(np_data)])\ndf_data = pd.DataFrame(np_data, columns=features_group1+features_group2)\nprint(df_data.isna().sum())\nprint(df_data.shape)\ndf_data.head(6)","7a950586":"#to check StandardScaler\ndf_data.describe()","9f727b1c":"# to check outliers\nplt.figure(figsize=(15,10))\nsns.boxplot(data=df_data)\nplt.xticks(rotation=90)","3a714bfa":"data_range = data.copy()","c728a333":"columns=['BALANCE', 'PURCHASES', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'CREDIT_LIMIT', 'PAYMENTS', 'MINIMUM_PAYMENTS']\n\nfor c in columns:\n    \n    Range=c+'_RANGE'\n    data_range[Range]=0        \n    data_range.loc[((data[c]>0)&(data[c]<=500)),Range]=1\n    data_range.loc[((data[c]>500)&(data[c]<=1000)),Range]=2\n    data_range.loc[((data[c]>1000)&(data[c]<=3000)),Range]=3\n    data_range.loc[((data[c]>3000)&(data[c]<=5000)),Range]=4\n    data_range.loc[((data[c]>5000)&(data[c]<=10000)),Range]=5\n    data_range.loc[((data[c]>10000)),Range]=6","27910dc6":"columns=['BALANCE_FREQUENCY', 'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY', 'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY', 'PRC_FULL_PAYMENT']\n\nfor c in columns:  \n\n    Range=c+'_RANGE'\n    data_range[Range]=0\n    for i in range(10):\n        data_range.loc[((data[c]>i*0.1)&(data[c]<=(i+1)*0.1)), Range]=i+1","83630423":"columns=['PURCHASES_TRX', 'CASH_ADVANCE_TRX']  \n\nfor c in columns:\n    \n    Range=c+'_RANGE'\n    data_range[Range]=0\n    data_range.loc[((data[c]>0)&(data[c]<=5)),Range]=1\n    data_range.loc[((data[c]>5)&(data[c]<=10)),Range]=2\n    data_range.loc[((data[c]>10)&(data[c]<=15)),Range]=3\n    data_range.loc[((data[c]>15)&(data[c]<=20)),Range]=4\n    data_range.loc[((data[c]>20)&(data[c]<=30)),Range]=5\n    data_range.loc[((data[c]>30)&(data[c]<=50)),Range]=6\n    data_range.loc[((data[c]>50)&(data[c]<=100)),Range]=7\n    data_range.loc[((data[c]>100)),Range]=8","1921f86d":"data_range.drop(['CUST_ID', 'BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES',\n       'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE',\n       'PURCHASES_FREQUENCY',  'ONEOFF_PURCHASES_FREQUENCY',\n       'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY',\n       'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS',\n       'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT' ], axis=1, inplace=True)","b9930933":"len(data.columns), len(data_range.columns)","09c97a99":"data_range.head()","f60aadce":"data_range.describe()","c19343c7":"plt.figure(figsize=(15,10))\nsns.boxplot(data=data_range)\nplt.xticks(rotation=90)","81db3ebc":"features_group3 = ['INSTALLMENTS_PURCHASES_RANGE','MINIMUM_PAYMENTS_RANGE','ONEOFF_PURCHASES_FREQUENCY_RANGE','CASH_ADVANCE_FREQUENCY_RANGE','PRC_FULL_PAYMENT_RANGE','CASH_ADVANCE_TRX_RANGE']\nfeatures_group4 = list(set(data_range.columns)-set(features_group3))","1942d9cb":"# using median in columns with outliers \ng1_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('log', FunctionTransformer(np.log1p)),\n    #('scaler', MinMaxScaler(feature_range=(0, 1)))\n    ('scaler', StandardScaler())\n    ])\n\n# using median in columns without outliers \ng2_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n    ])\n\npreprocessor2 = ColumnTransformer(\n    transformers=[\n        ('group1', g1_transformer, features_group3),\n        ('group2', g2_transformer, features_group4),\n        ])","3b17f570":"data_range.columns","0c2b51bd":"preprocessor2.fit(data_range) \nnp_data_range = preprocessor2.transform(data_range) ","41b17ff6":"print(np_data_range[np.isnan(np_data_range)])\ndf_data2 = pd.DataFrame(np_data_range, columns=features_group3+features_group4)\nprint(df_data2.isna().sum())\nprint(df_data2.shape)\ndf_data2.head(6)","d82f7400":"df_data.describe()","05cfade1":"plt.figure(figsize=(15,10))\nsns.boxplot(data=df_data)\nplt.xticks(rotation=90)","24273f3a":"pca = PCA(n_components=2)\npca.fit(np_data)","20447f92":"data_pca = pca.transform(np_data)\nplt.figure(figsize=(8,6))\nplt.scatter(np_data[:,0],np_data[:,1])\nplt.xlabel('First principal component')\nplt.ylabel('Second Principal Component')","2b3e48af":"print(pca.noise_variance_)\nprint(pca.explained_variance_ratio_)","5ebc0d16":"Sum_of_squared_distances = []\nK = range(1, 20)\nfor k in K:\n    km = KMeans(n_clusters=k, \n                init='k-means++',\n                max_iter=400, \n                n_init=80, \n                random_state=0).fit(np_data)\n    Sum_of_squared_distances.append(km.inertia_)\n\nplt.figure(figsize=(10,10))\nplt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","9799349f":"silhouette_scores = [] \nK = range(2, 20)\n\nfor k in K:\n    km = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=45).fit_predict(np_data)\n    scr = silhouette_score(np_data, km)\n    silhouette_scores.append(scr)\n    print(\"For n_clusters =\", k, \"The average silhouette_score is :\", scr)\nplt.plot(K, silhouette_scores, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Method For Optimal k')\nplt.show()\n\n","df108a8e":"K = range(2,10)\n\nfor k in K:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    ax1.set_xlim([-0.1, 1])\n    ax1.set_ylim([0, len(np_data) + (k + 1) * 10])\n\n    clusterer = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=45)\n    cluster_labels = clusterer.fit_predict(np_data)\n\n    silhouette_avg = silhouette_score(np_data, cluster_labels)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(np_data, cluster_labels)\n\n    y_lower = 10\n    for i in range(k):\n        # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) \/ k)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels \/ ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) \/ k)\n    pca = PCA(n_components=2)\n    pca.fit(np_data)\n    X = pca.transform(np_data)\n\n    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors, edgecolor='k')\n\n    # Labeling the clusters\n    pca_centers = pca.transform(clusterer.cluster_centers_)\n    # Draw white circles at cluster centers\n    ax2.scatter(pca_centers[:, 0], pca_centers[:, 1], marker='o',\n                c=\"white\", alpha=1, s=200, edgecolor='k')\n\n    for i, c in enumerate(pca_centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n                    s=50, edgecolor='k')\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st principal feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd principal feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % k),\n                 fontsize=14, fontweight='bold')\n\nplt.show()","7a2d4df6":"km = KMeans(n_clusters=6, \n            init='k-means++',\n            max_iter=400, \n            n_init=80, \n            random_state=0)\n\nkm_pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('km', km)])\n\nkm_pipe.fit(data)","49127d7d":"labels = km.labels_","1bd3aec3":"clusters = pd.concat([data, pd.DataFrame({'CLUSTER':labels})], axis=1)\nclusters.head()","44b7cec5":"clusters.CLUSTER.value_counts()","a4c65914":"clusters.CLUSTER.hist(figsize=(10, 8))\nplt.tight_layout()","db22b6a6":"# save clusters to csv\nclusters.to_csv('Clusters_CreditCards_Kmeans.csv')","5579188e":"for c in clusters:\n    grid= sns.FacetGrid(clusters, col='CLUSTER')\n    grid.map(plt.hist, c)","0fa11815":"clusters.groupby(['CLUSTER']).mean()","3785b114":"dist = 1 - cosine_similarity(np_data)\n\npca = PCA(2)\npca.fit(dist)\nX_PCA = pca.transform(dist)\nX_PCA.shape","57cb9b8c":"x, y = X_PCA[:, 0], X_PCA[:, 1]\n\ncolors = {0: 'red',\n          1: 'blue',\n          2: 'green', \n          3: 'yellow', \n          4: 'orange',  \n          5:'purple'}\n\nnames = {0: 'high level of income and high credit limit who take cash in advance', \n         1: 'low level of income. Not Frequent purchases', \n         2: 'who purchases mostly in installments', \n         3: 'They purchase mostly in one-go with a high frequency. the percent of full payment paid is low (debtors)', \n         4: 'do not spend much money and who accept large amounts of cash advances but not frequently',\n         5: 'High spenders who take more cash in advance'}\n  \ndf = pd.DataFrame({'x': x, 'y':y, 'label':labels}) \ngroups = df.groupby('label')\n\nfig, ax = plt.subplots(figsize=(20, 13)) \n\nfor name, group in groups:\n    ax.plot(group.x, group.y, marker='o', linestyle='', ms=5,\n            color=colors[name],label=names[name], mec='none')\n    ax.set_aspect('auto')\n    ax.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off')\n    ax.tick_params(axis= 'y',which='both',left='off',top='off',labelleft='off')\n    \nax.legend()\nax.set_title(\"Customers Segmentation based on their Credit Card usage bhaviour.\")\nplt.show()","ba744b2a":"preprocessor.fit(data) \nnp_data = preprocessor.transform(data) ","eb80c0fc":"siliuette_list_hierarchical = []\nfor cluster in range(2,10):\n    for linkage_method in ['ward', 'average','single']:\n        agglomerative = AgglomerativeClustering(linkage=linkage_method, affinity='euclidean',n_clusters=cluster).fit_predict(np_data)\n        sil_score = metrics.silhouette_score(np_data, agglomerative, metric='euclidean')\n        siliuette_list_hierarchical.append((cluster, sil_score, linkage_method))\n        \ndf_hierarchical = pd.DataFrame(siliuette_list_hierarchical, columns=['cluster', 'sil_score','linkage_method'])\ndf_hierarchical.sort_values('sil_score', ascending=False)","156e748f":"Z_avg = linkage(np_data, 'average')\n\nplt.figure(figsize=(15,10))\ndendrogram(Z_avg, leaf_rotation=90, p=5, color_threshold=20, leaf_font_size=10, truncate_mode='level')\nplt.axhline(y=125, color='r', linestyle='--')\nplt.show()","6ecbad1f":"Z_ward = linkage(np_data, 'ward')\n\nplt.figure(figsize=(15,10))\ndendrogram(Z_ward, leaf_rotation=90, p=5, color_threshold=20, leaf_font_size=10, truncate_mode='level')\nplt.axhline(y=125, color='r', linestyle='--')\nplt.show()","0f99deb7":"Z_ward = linkage(np_data, 'single')\n\nplt.figure(figsize=(15,10))\ndendrogram(Z_ward, leaf_rotation=90, p=15, color_threshold=20, leaf_font_size=10, truncate_mode='level')\nplt.axhline(y=125, color='r', linestyle='--')\nplt.show()","cd9188a8":"hierarchical = AgglomerativeClustering(n_clusters=2, linkage='average')","bfa468e0":"pipe_hierar = Pipeline(steps=[\n                              ('preprocessor', preprocessor),\n                              ('hierarchical', hierarchical)]\n                       )\n\npipe_hierar.fit(data)","07dc1043":"clusters_hierar = pd.concat([data, pd.DataFrame({'CLUSTER':hierarchical.labels_})], axis=1)\nclusters_hierar.head()","739931a6":"clusters_hierar.to_csv('Clusters_CreditCard_Hierarchical.csv')","d4d9e29f":"clusters_hierar.groupby('CLUSTER').mean()","8c1f714e":"clusters_hierar.CLUSTER.value_counts()","cb026234":"# MODELS","5235ad5b":"#### N\u00b0Clusters for K-means: Elbow Method","b1580050":"\n1. **CUST_ID** : Identification of Credit Card holder\n2. **BALANCE** : Balance amount left in their account to make purchases\n3.**BALANCE_FREQUENCY** : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n4. **PURCHASES** : Amount of purchases made from account\n5. **ONEOFF_PURCHASES** : Maximum purchase amount done in one-go\n6. **INSTALLMENTS_PURCHASES** : Amount of purchase done in installment\n7. **CASH_ADVANCE** : Cash in advance given by the user\n8. **PURCHASES_FREQUENCY** : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n9. **ONEOFF_PURCHASES_FREQUENCY** : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n10. **PURCHASES_INSTALLMENTS_FREQUENCY** : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n11. **CASH_ADVANCE_FREQUENCY** : How frequently the cash in advance being paid\n12. **CASH_ADVANCE_TRX** : Number of Transactions made with \"Cash in Advanced\"\n13. **PURCHASES_TRX** : Numbe of purchase transactions made\n14. **CREDIT_LIMIT** : Limit of Credit Card for user\n15. **PAYMENTS** : Amount of Payment done by user\n16. **MINIMUM_PAYMENTS** : Minimum amount of payments made by user\n17. **PRC_FULL_PAYMENT** : Percent of full payment paid by user\n18. **TENURE** : Tenure of credit card service for user\n\nCUSTOMER_ID will not be taken into account as a model variable because (in my point of view) it doesn't give information about customer behavior.\n","a246327b":"### Correlation Analysis","87ce7191":"### Set clusters","01cbef64":"I didn't find anything special regarding the missing values of the column, so I will use the median to fill.","d4d5b747":"**CONCLUSION OF OUTLIERS:** The columns with outliers problems are 10: \n* BALANCE,\n* PURCHASES,\n* ONEOFF_PURCHASES, \n* INSTALLMENTS_PURCHASES, \n* CASH_ADVANCE, \n* CASH_ADVANCE_TRX, \n* PURCHASE_TRX, \n* CREDIT_LIMIT, \n* PAYMENTS and \n* MINIMUM_PAYMENTS\n\nand for these variables I think it is appropiate to apply a logarithmic transformation.","048b3778":"We see that is not a good option to fill with zero. \n\nTaking into account the characteristics of the customer 15349, I look for special values in the CREDIT_LIMIT column for customers without purchases but with cash advances. ","9faab306":"When PAYMENTS = 0, the value of MINIMUM_PAYMENTS is always NaN:","406b076f":"## Basic Data Analysis - Overview","baacc139":"### Outliers","b56b8675":"**CONCLUSIONS OF MISSING VALUES:** Fill the missing values in CREDIT_LIMIT and MINIMUM_PAYMENTS with the median of the column.","c2248cc1":"Another way to deal with outliers is to make ranges.","7039ef56":"The estimated noise covariance is not a good value, so we cannot rely on the shape of the data using the visual of PCA in 2 dimensions.","01d98da6":"### Discrete variables","6f83c30a":"We can see from the table above, that variables the following variables have outliers:\n* BALANCE,\n* PURCHASES,\n* ONEOFF_PURCHASES, \n* INSTALLMENTS_PURCHASES, \n* CASH_ADVANCE, \n* CASH_ADVANCE_TRX, \n* PURCHASE_TRX, \n* CREDIT_LIMIT, \n* PAYMENTS and \n* MINIMUM_PAYMENTS\n\nA data point is an outlier if any of the two following conditions apply:\n1. data point that falls outside of 1.5 times of an interquartile range above the 3rd quartile and below 1st quartile.\n2. data point that falls outside of 3 or 4 standard deviations,\n","48d318ef":"The dendogram can be hard to read when the original observation matrix from which the linkage is derived is large. Truncation is used to condense the dendrogram.\n\nI'm going to plot with different parameters to see the best option.","d0fb9089":"![image.png](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcRhdKUnlaTWstr6eoGVrHV6iDhOLr4ZhBXValerAT4vUfbWXrgA&usqp=CAU)\n\nimage from: https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcRhdKUnlaTWstr6eoGVrHV6iDhOLr4ZhBXValerAT4vUfbWXrgA&usqp=CAU","a6f608f9":"**Missing values**","d6aff66d":"### Interpretation of clusters","d06f5671":"Below is a toy example to illustrate how the algorithm works.\n\n![image.png](https:\/\/stanford.edu\/~cpiech\/cs221\/img\/kmeansViz.png)\n\n\nImage: https:\/\/stanford.edu\/~cpiech\/cs221\/img\/kmeansViz.png","1cffed7b":"to a model with better-defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores:\n\n$a$: The mean distance between a sample and all other points in the same class.\n\n$b$: The mean distance between a sample and all other points in the next nearest cluster.\n\nThe Silhouette Coefficient is for a single sample is then given as:\n\n$$s=\\dfrac{b\u2212a}{max(a,b)}$$\n \nTo find the optimal value of k for KMeans, loop through 1..n for n_clusters in KMeans and calculate Silhouette Coefficient for each sample.\n\nA higher Silhouette Coefficient indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.","63003073":"### Missing Values","052f1d73":"### Preprocessing - Extra Miscellaneous","4f4b89e2":"#### N\u00b0 Clusters for K-means: Silhouette Coefficient Method:","94500b94":"**MINIMUM_PAYMENTS**","e5f4c18f":"# CLUSTERING PROBLEM\n\nPurpose: The goal is to develop a customer segmentation model to define a credit card company's marketing strategy.\n\nModel Class: *Unsupervised*\n\nModel Type: *Clustering*\n\nEdit Date: 4\/8\/2020\n\nCluster Models Include:\n- K-Means\n- Hierarchical\n\nResources:\n* https:\/\/afnan.io\/2017-10-31\/using-k-means-clustering-in-scikit-learn\/\n* https:\/\/www.kaggle.com\/ainslie\/credit-card-data-clustering-analysis\/data\n* https:\/\/lab.pwc.com\/automation\/details\/3850\n* https:\/\/scikit-learn.org\/stable\/auto_examples\/compose\/plot_column_transformer_mixed_types.html\n\nData:\nhttps:\/\/www.kaggle.com\/ainslie\/credit-card-data-clustering-analysis\/data\n\nThe dataset summarizes the usage behavior of about 9000 active credit card holders during 6 months.\nThe file is at a customer level with 18 behavioral variables.","6a79f72f":"## PCA data","a4248ce7":"**z_scr method**","a6ee2148":"**CONCLUSIONS OF CORRELATIONS:** Taking into account the high correlation between PURCHASES and ONEOFF_PURCHASES, and the interpretation of the variables in the problem, I think PURCHASES can be remove or not from the variables to include in the models.","5e08e3e5":"# DEPENDENCIES\n\nLoad the dependencies for model development. Current package requirements include:\n* Sklearn\n* Pandas\n* Numpy\n* Scipy\n* Matplotlib","944f28ac":"**CASH_ADVANCE analysis**","2e3a44ee":"The idea behind elbow method is to run k-means clustering on a given dataset for a range of values of k (e.g k=1 to 10), for each value of k, calculate sum of squared errors (SSE).\n\nCalculate the mean distance between data points and their cluster centroid. Increasing the number of clusters(K) will always reduce the distance to data points, thus decrease this metric, to the extreme of reaching zero when K is as same as the number of data points. **So the goal is to choose a small value of k that still has a low SSE.**","19e68696":"We observe above that there are not significant  differences with respect to the complete data. So, in this case, I decided to fill with the median value.","2c360462":"## Analysis of Preprocessing","a4c865de":"# Data","47bce764":"## Hierarchical","b32229ec":"* PURCHASES has a higher lever of correlation with ONEOFF_PURCHASES.\n* CASH_ADVANCE_TRX has a higher lever of correlation with CASH_ADVANCE_FREQUENCY.\n* PURCHASES_TRX has a good level of correlation with INSTALLMENTS_PURCHASES, PURCHASES_FREQUENCY.\n* BALANCE has a negative correlation with PRC_FULL_PAYMENT","815e5947":"## K-means","c88e928e":"Hierarchical clustering starts by treating each observation as a separate cluster. Then, it repeatedly executes the following two steps: \n1. identify the two clusters that are closest together, and\n2. merge the two most similar clusters. This iterative process continues until all the clusters are merged together.","531c8772":"The Silhouette coefficient, between -1 and 1, gives an indication of how close each point in one cluster is to points in the neighbouring clusters. Values close to 1 are furthest from other clusters whereas negative points overlap with others. In an ideal situation we would expect all the points of a cluster to have Silhouette coefficients close to 1. ","f2804e56":"![image](https:\/\/3.bp.blogspot.com\/-TQYHVkgesMg\/WbTcMIOuquI\/AAAAAAAAD3Y\/dY4YpxJ3OhU5VGppwcrS6j-ewvlddxSjwCLcBGAs\/s1600\/hcust.PNG)\n\nimage from: https:\/\/3.bp.blogspot.com\/-TQYHVkgesMg\/WbTcMIOuquI\/AAAAAAAAD3Y\/dY4YpxJ3OhU5VGppwcrS6j-ewvlddxSjwCLcBGAs\/s1600\/hcust.PNG","027ad854":"Since one possibility is to fill the missing values with zero, I analyze if there are clients with Credit limit equal to zero. ","c903d9e6":"**PURCHASE analysis**","a3038530":"**Columns transformation**","b60d7170":"When MINIMUM_PAYMENTS is NaN, the value of PRC_FULL_PAYMENT is always zero:","f010fcab":"**IQR method**","a8f83164":"<img src=\"https:\/\/dpzbhybb2pdcj.cloudfront.net\/rhys\/v-7\/Figures\/CH17_FIG_2_MLR.png\" width=\"400\">\n\nimage: https:\/\/dpzbhybb2pdcj.cloudfront.net\/rhys\/v-7\/Figures\/CH17_FIG_2_MLR.png","f8eeefde":"**CREDIT_LIMIT**","22b0437f":"## Exploratory Data Analisys (EDA)","04d596cd":"### N\u00b0 of clusters - Visualization","c47fbf8d":"***Cluster 0***  People with high level of income (balance) and high credit limit who take cash in advance.\n\n***Cluster 1*** People with low level of income. Not Frequent purchases.\n\n***Cluster 2*** Low balance but the balance gets updated frequently ie. more no. of transactions. They purchase mostly in installments\n\n***Cluster 3*** They purchase mostly in one-go with a high frequency. the percent of full payment paid is low (debtors).\n\n***Cluster 4***: People with a medium level of income who don't spend much money and who accept large amounts of cash advances but not frequently.\n\n***Cluster 5*** High spenders with high credit limit who make expensive purchases and take more cash in advance","b7ef9915":"## Preprocessing","0a3ec7b3":"### Set 2 clusters"}}