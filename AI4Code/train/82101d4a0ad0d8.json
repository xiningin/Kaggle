{"cell_type":{"6b51fc44":"code","1fb7cc07":"code","dcc52f81":"code","32f0fb2a":"code","37c938d0":"code","635fda01":"code","c60e79f9":"code","6a41a0f2":"code","bc1e00cd":"code","c336baa5":"code","95b9f126":"code","1819e4da":"code","193e1a51":"code","12901ba7":"code","ef52468f":"code","f3928248":"code","606504bc":"code","2c150eaf":"code","4aaadd1a":"code","4903e242":"code","48bc04d1":"code","c287530b":"code","c972ac03":"code","136b9217":"code","63525249":"code","28081665":"code","e02967bc":"code","61ab9ba2":"code","d299651c":"code","83b39d2f":"code","cdf89152":"code","7d1a40df":"code","c0d99bcd":"code","12fdd33d":"code","03e6d3eb":"code","a7f0d682":"code","a325b8b9":"code","a785a8c8":"code","8b8fc431":"code","210c6f19":"code","d286d4d7":"code","751d978b":"code","42552e5d":"code","01e70623":"code","25c1c12b":"code","aba8d48e":"code","d10bc30c":"code","549812ce":"code","811e4d81":"code","0c6fe3fc":"code","e6f377b6":"code","2957dd82":"code","79f0541c":"code","8557edd3":"code","3ebb2c48":"code","6ea00ce9":"code","e8866b4c":"markdown","9ca044e3":"markdown","1b081548":"markdown","b5ceb76f":"markdown","b424f447":"markdown","440c6457":"markdown","52eb7890":"markdown","9e0a9c5b":"markdown","793367a8":"markdown","16e90b9b":"markdown","fa6c0152":"markdown","e40b6af2":"markdown","89151539":"markdown","4bbd373e":"markdown","0dc938cb":"markdown","119d3d5c":"markdown","95aa9bde":"markdown","19b2c893":"markdown","a24e5246":"markdown","ff2325f2":"markdown","87597535":"markdown","af5e94d0":"markdown","d8068c88":"markdown","f83fe624":"markdown","c10edb55":"markdown","218f1ce0":"markdown","ceb0464d":"markdown","930e4696":"markdown","c9edb220":"markdown","7427771a":"markdown","55888d26":"markdown"},"source":{"6b51fc44":"!pip install jcopml","1fb7cc07":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfrom jcopml.pipeline import num_pipe, cat_pipe\nfrom jcopml.utils import save_model, load_model\nfrom jcopml.plot import plot_missing_value\nfrom jcopml.feature_importance import mean_score_decrease","dcc52f81":"df = pd.read_csv(\"..\/input\/tweet-sentiment-indonesian-president-candidates-14\/20200802_sentiment_data.csv\", encoding='latin1')\ndf.head()","32f0fb2a":"# plot missing value\nplot_missing_value(df, return_df=True)","37c938d0":"# drop too missing values data\ndf.drop(columns=[\"ID\", \"Tweet_ID\", \"Date\", \"Aplikasi\", \"Lokasi User\", \"Source\", \"Topic\", \"Remark\"], inplace=True)","635fda01":"plot_missing_value(df, return_df=True)","c60e79f9":"print('the data is ready to be engineered')\ndf.head(10)","6a41a0f2":"# total of words of old df\ntotal_words_old_df = df.Isi_Tweet.apply(lambda x: len(x.split(\" \"))).sum()\nprint(total_words_old_df)","bc1e00cd":"# import nltk \nimport re\n# from nltk.tokenize import word_tokenize, sent_tokenize\n# from nltk.corpus import stopwords\n# from nltk.corpus import wordnet\n# from nltk.stem import WordNetLemmatizer\n# from nltk.stem import PorterStemmer\n# from string import punctuation\n\n# STOPWORDS = stopwords.words(\"indonesian\") + stopwords.words(\"english\")\nSTOPWORDS = ['ada', 'adalah', 'adanya', 'adapun', 'agak', 'agaknya', 'agar', 'akan', 'akankah', 'akhir', 'akhiri', 'akhirnya', 'aku', 'akulah', 'amat', 'amatlah', 'anda', 'andalah', 'antar', 'antara', 'antaranya', 'apa', 'apaan', 'apabila', 'apakah', 'apalagi', 'apatah', 'artinya', 'asal', 'asalkan', 'atas', 'atau', 'ataukah', 'ataupun', 'awal', 'awalnya', 'bagai', 'bagaikan', 'bagaimana', 'bagaimanakah', 'bagaimanapun', 'bagi', 'bagian', 'bahkan', 'bahwa', 'bahwasanya', 'baik', 'bakal', 'bakalan', 'balik', 'banyak', 'bapak', 'baru', 'bawah', 'beberapa', 'begini', 'beginian', 'beginikah', 'beginilah', 'begitu', 'begitukah', 'begitulah', 'begitupun', 'bekerja', 'belakang', 'belakangan', 'belum', 'belumlah', 'benar', 'benarkah', 'benarlah', 'berada', 'berakhir', 'berakhirlah', 'berakhirnya', 'berapa', 'berapakah', 'berapalah', 'berapapun', 'berarti', 'berawal', 'berbagai', 'berdatangan', 'beri', 'berikan', 'berikut', 'berikutnya', 'berjumlah', 'berkali-kali', 'berkata', 'berkehendak', 'berkeinginan', 'berkenaan', 'berlainan', 'berlalu', 'berlangsung', 'berlebihan', 'bermacam', 'bermacam-macam', 'bermaksud', 'bermula', 'bersama', 'bersama-sama', 'bersiap', 'bersiap-siap', 'bertanya', 'bertanya-tanya', 'berturut', 'berturut-turut', 'bertutur', 'berujar', 'berupa', 'besar', 'betul', 'betulkah', 'biasa', 'biasanya', 'bila', 'bilakah', 'bisa', 'bisakah', 'boleh', 'bolehkah', 'bolehlah', 'buat', 'bukan', 'bukankah', 'bukanlah', 'bukannya', 'bulan', 'bung', 'cara', 'caranya', 'cukup', 'cukupkah', 'cukuplah', 'cuma', 'dahulu', 'dalam', 'dan', 'dapat', 'dari', 'daripada', 'datang', 'dekat', 'demi', 'demikian', 'demikianlah', 'dengan', 'depan', 'di', 'dia', 'diakhiri', 'diakhirinya', 'dialah', 'diantara', 'diantaranya', 'diberi', 'diberikan', 'diberikannya', 'dibuat', 'dibuatnya', 'didapat', 'didatangkan', 'digunakan', 'diibaratkan', 'diibaratkannya', 'diingat', 'diingatkan', 'diinginkan', 'dijawab', 'dijelaskan', 'dijelaskannya', 'dikarenakan', 'dikatakan', 'dikatakannya', 'dikerjakan', 'diketahui', 'diketahuinya', 'dikira', 'dilakukan', 'dilalui', 'dilihat', 'dimaksud', 'dimaksudkan', 'dimaksudkannya', 'dimaksudnya', 'diminta', 'dimintai', 'dimisalkan', 'dimulai', 'dimulailah', 'dimulainya', 'dimungkinkan', 'dini', 'dipastikan', 'diperbuat', 'diperbuatnya', 'dipergunakan', 'diperkirakan', 'diperlihatkan', 'diperlukan', 'diperlukannya', 'dipersoalkan', 'dipertanyakan', 'dipunyai', 'diri', 'dirinya', 'disampaikan', 'disebut', 'disebutkan', 'disebutkannya', 'disini', 'disinilah', 'ditambahkan', 'ditandaskan', 'ditanya', 'ditanyai', 'ditanyakan', 'ditegaskan', 'ditujukan', 'ditunjuk', 'ditunjuki', 'ditunjukkan', 'ditunjukkannya', 'ditunjuknya', 'dituturkan', 'dituturkannya', 'diucapkan', 'diucapkannya', 'diungkapkan', 'dong', 'dua', 'dulu', 'empat', 'enggak', 'enggaknya', 'entah', 'entahlah', 'guna', 'gunakan', 'hal', 'hampir', 'hanya', 'hanyalah', 'hari', 'harus', 'haruslah', 'harusnya', 'hendak', 'hendaklah', 'hendaknya', 'hingga', 'ia', 'ialah', 'ibarat', 'ibaratkan', 'ibaratnya', 'ibu', 'ikut', 'ingat', 'ingat-ingat', 'ingin', 'inginkah', 'inginkan', 'ini', 'inikah', 'inilah', 'itu', 'itukah', 'itulah', 'jadi', 'jadilah', 'jadinya', 'jangan', 'jangankan', 'janganlah', 'jauh', 'jawab', 'jawaban', 'jawabnya', 'jelas', 'jelaskan', 'jelaslah', 'jelasnya', 'jika', 'jikalau', 'juga', 'jumlah', 'jumlahnya', 'justru', 'kala', 'kalau', 'kalaulah', 'kalaupun', 'kalian', 'kami', 'kamilah', 'kamu', 'kamulah', 'kan', 'kapan', 'kapankah', 'kapanpun', 'karena', 'karenanya', 'kasus', 'kata', 'katakan', 'katakanlah', 'katanya', 'ke', 'keadaan', 'kebetulan', 'kecil', 'kedua', 'keduanya', 'keinginan', 'kelamaan', 'kelihatan', 'kelihatannya', 'kelima', 'keluar', 'kembali', 'kemudian', 'kemungkinan', 'kemungkinannya', 'kenapa', 'kepada', 'kepadanya', 'kesampaian', 'keseluruhan', 'keseluruhannya', 'keterlaluan', 'ketika', 'khususnya', 'kini', 'kinilah', 'kira', 'kira-kira', 'kiranya', 'kita', 'kitalah', 'kok', 'kurang', 'lagi', 'lagian', 'lah', 'lain', 'lainnya', 'lalu', 'lama', 'lamanya', 'lanjut', 'lanjutnya', 'lebih', 'lewat', 'lima', 'luar', 'macam', 'maka', 'makanya', 'makin', 'malah', 'malahan', 'mampu', 'mampukah', 'mana', 'manakala', 'manalagi', 'masa', 'masalah', 'masalahnya', 'masih', 'masihkah', 'masing', 'masing-masing', 'mau', 'maupun', 'melainkan', 'melakukan', 'melalui', 'melihat', 'melihatnya', 'memang', 'memastikan', 'memberi', 'memberikan', 'membuat', 'memerlukan', 'memihak', 'meminta', 'memintakan', 'memisalkan', 'memperbuat', 'mempergunakan', 'memperkirakan', 'memperlihatkan', 'mempersiapkan', 'mempersoalkan', 'mempertanyakan', 'mempunyai', 'memulai', 'memungkinkan', 'menaiki', 'menambahkan', 'menandaskan', 'menanti', 'menanti-nanti', 'menantikan', 'menanya', 'menanyai', 'menanyakan', 'mendapat', 'mendapatkan', 'mendatang', 'mendatangi', 'mendatangkan', 'menegaskan', 'mengakhiri', 'mengapa', 'mengatakan', 'mengatakannya', 'mengenai', 'mengerjakan', 'mengetahui', 'menggunakan', 'menghendaki', 'mengibaratkan', 'mengibaratkannya', 'mengingat', 'mengingatkan', 'menginginkan', 'mengira', 'mengucapkan', 'mengucapkannya', 'mengungkapkan', 'menjadi', 'menjawab', 'menjelaskan', 'menuju', 'menunjuk', 'menunjuki', 'menunjukkan', 'menunjuknya', 'menurut', 'menuturkan', 'menyampaikan', 'menyangkut', 'menyatakan', 'menyebutkan', 'menyeluruh', 'menyiapkan', 'merasa', 'mereka', 'merekalah', 'merupakan', 'meski', 'meskipun', 'meyakini', 'meyakinkan', 'minta', 'mirip', 'misal', 'misalkan', 'misalnya', 'mula', 'mulai', 'mulailah', 'mulanya', 'mungkin', 'mungkinkah', 'nah', 'naik', 'namun', 'nanti', 'nantinya', 'nyaris', 'nyatanya', 'oleh', 'olehnya', 'pada', 'padahal', 'padanya', 'pak', 'paling', 'panjang', 'pantas', 'para', 'pasti', 'pastilah', 'penting', 'pentingnya', 'per', 'percuma', 'perlu', 'perlukah', 'perlunya', 'pernah', 'persoalan', 'pertama', 'pertama-tama', 'pertanyaan', 'pertanyakan', 'pihak', 'pihaknya', 'pukul', 'pula', 'pun', 'punya', 'rasa', 'rasanya', 'rata', 'rupanya', 'saat', 'saatnya', 'saja', 'sajalah', 'saling', 'sama', 'sama-sama', 'sambil', 'sampai', 'sampai-sampai', 'sampaikan', 'sana', 'sangat', 'sangatlah', 'satu', 'saya', 'sayalah', 'se', 'sebab', 'sebabnya', 'sebagai', 'sebagaimana', 'sebagainya', 'sebagian', 'sebaik', 'sebaik-baiknya', 'sebaiknya', 'sebaliknya', 'sebanyak', 'sebegini', 'sebegitu', 'sebelum', 'sebelumnya', 'sebenarnya', 'seberapa', 'sebesar', 'sebetulnya', 'sebisanya', 'sebuah', 'sebut', 'sebutlah', 'sebutnya', 'secara', 'secukupnya', 'sedang', 'sedangkan', 'sedemikian', 'sedikit', 'sedikitnya', 'seenaknya', 'segala', 'segalanya', 'segera', 'seharusnya', 'sehingga', 'seingat', 'sejak', 'sejauh', 'sejenak', 'sejumlah', 'sekadar', 'sekadarnya', 'sekali', 'sekali-kali', 'sekalian', 'sekaligus', 'sekalipun', 'sekarang', 'sekarang', 'sekecil', 'seketika', 'sekiranya', 'sekitar', 'sekitarnya', 'sekurang-kurangnya', 'sekurangnya', 'sela', 'selain', 'selaku', 'selalu', 'selama', 'selama-lamanya', 'selamanya', 'selanjutnya', 'seluruh', 'seluruhnya', 'semacam', 'semakin', 'semampu', 'semampunya', 'semasa', 'semasih', 'semata', 'semata-mata', 'semaunya', 'sementara', 'semisal', 'semisalnya', 'sempat', 'semua', 'semuanya', 'semula', 'sendiri', 'sendirian', 'sendirinya', 'seolah', 'seolah-olah', 'seorang', 'sepanjang', 'sepantasnya', 'sepantasnyalah', 'seperlunya', 'seperti', 'sepertinya', 'sepihak', 'sering', 'seringnya', 'serta', 'serupa', 'sesaat', 'sesama', 'sesampai', 'sesegera', 'sesekali', 'seseorang', 'sesuatu', 'sesuatunya', 'sesudah', 'sesudahnya', 'setelah', 'setempat', 'setengah', 'seterusnya', 'setiap', 'setiba', 'setibanya', 'setidak-tidaknya', 'setidaknya', 'setinggi', 'seusai', 'sewaktu', 'siap', 'siapa', 'siapakah', 'siapapun', 'sini', 'sinilah', 'soal', 'soalnya', 'suatu', 'sudah', 'sudahkah', 'sudahlah', 'supaya', 'tadi', 'tadinya', 'tahu', 'tahun', 'tak', 'tambah', 'tambahnya', 'tampak', 'tampaknya', 'tandas', 'tandasnya', 'tanpa', 'tanya', 'tanyakan', 'tanyanya', 'tapi', 'tegas', 'tegasnya', 'telah', 'tempat', 'tengah', 'tentang', 'tentu', 'tentulah', 'tentunya', 'tepat', 'terakhir', 'terasa', 'terbanyak', 'terdahulu', 'terdapat', 'terdiri', 'terhadap', 'terhadapnya', 'teringat', 'teringat-ingat', 'terjadi', 'terjadilah', 'terjadinya', 'terkira', 'terlalu', 'terlebih', 'terlihat', 'termasuk', 'ternyata', 'tersampaikan', 'tersebut', 'tersebutlah', 'tertentu', 'tertuju', 'terus', 'terutama', 'tetap', 'tetapi', 'tiap', 'tiba', 'tiba-tiba', 'tidak', 'tidakkah', 'tidaklah', 'tiga', 'tinggi', 'toh', 'tunjuk', 'turut', 'tutur', 'tuturnya', 'ucap', 'ucapnya', 'ujar', 'ujarnya', 'umum', 'umumnya', 'ungkap', 'ungkapnya', 'untuk', 'usah', 'usai', 'waduh', 'wah', 'wahai', 'waktu', 'waktunya', 'walau', 'walaupun', 'wong', 'yaitu', 'yakin', 'yakni', 'yang', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '\/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n","c336baa5":"for i in range(len(df)):\n    print(df.Isi_Tweet[i])\n    if i == 10:\n        break","95b9f126":"print(\"The length of dataframe is\", len(df), \"rows\")","1819e4da":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(\"\\n\", \" \", text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = text.split()\n    text = \" \".join(text)\n    return text\n\ndf1 = df.Isi_Tweet.apply(str).apply(lambda x:clean_text(x))","193e1a51":"for i in range(len(df1)):\n    print(df1[i])\n    if i == 10:\n        break","12901ba7":"print(\"The length of dataframe is\", len(df1), \"rows\")","ef52468f":"df1_clean_text = []\nfor i in range(len(df1)):\n    x = df1[i]\n    # x_sent_token = sent_tokenize(x)\n    # x_sent_token\n    x_word_tokens = word_tokenize(x)\n    # x_word_tokens\n#     print(x)\n    \n#     print(df1[i])\n    \n    # punctuation removal\n    x_word_tokens_removed_punctuations = [w for w in x_word_tokens if w not in punctuation]\n#     print(x_word_tokens_removed_punctuations, \"punctuation\")\n    \n    # numeric removal\n    x_word_tokens_removed_punctuations = [w for w in x_word_tokens_removed_punctuations if w.isalpha()]\n#     print(x_word_tokens_removed_punctuations, \"numeric\")\n    \n    # stopwords removal\n    x_word_tokens_removed_punctuation_removed_sw = [w for w in x_word_tokens_removed_punctuations if w not in sw]\n#     print(x_word_tokens_removed_punctuation_removed_sw, \"stopwords\")\n\n    # rejoining the words into one string\/sentence as inputted before being tokenized\n    x_word_tokens_removed_punctuation_removed_sw = \" \".join(x_word_tokens_removed_punctuation_removed_sw)\n#     print(x_word_tokens_removed_punctuation_removed_sw)\n    \n    df1_clean_text.append(x_word_tokens_removed_punctuation_removed_sw)","f3928248":"# text vs processed text\nfor i,j in zip(df1[0:10], df1_clean_text[0:10]):\n    print(i)\n    print(j)\n    print()","606504bc":"# list (df1_clean_text) to series (df1_clean_text_series)\n\n# list\nprint(type(df1_clean_text))\nprint(len(df1_clean_text))\n\n# converting list to pandas series\ndf1_clean_text_series = pd.Series(df1_clean_text)\n\nprint(type(df1_clean_text_series))\nprint(len(df1_clean_text_series))","2c150eaf":"# new series\ndf1_clean_text_series.head()","4aaadd1a":"# new df\ndf['Isi_Tweet'] = df1_clean_text_series\ndf.head(10)","4903e242":"df.info()","48bc04d1":"df.to_csv(\"preprocessed_df.csv\")","c287530b":"# total of words of old df vs new df \n\ntotal_words_new_df = df.Isi_Tweet.apply(lambda x: len(x.split(\" \"))).sum()\n\nprint(\"old df: \", total_words_old_df, \"words\")\nprint(\"new df: \", total_words_new_df, \"words\")\nprint(\"text processing has reduced the number of words by\", round((total_words_old_df-total_words_new_df)\/total_words_old_df*100), \"%\")","c972ac03":"X = df.Isi_Tweet\ny = df.Sentimen\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","136b9217":"# Check the imbalance dataset\ny_train.value_counts() \/ len(y_train) *100\n\n# the train data is more or less balance","63525249":"X_train.head(), X_test.head(), y_train.head(), y_test.head()","28081665":"X_train.head(11)","e02967bc":"import seaborn as sns\nimport matplotlib.pyplot as plt","61ab9ba2":"y_train.shape, y_test.shape","d299651c":"sns.set(style=\"darkgrid\")\nsns.countplot(x=y_train)\nplt.title(\"y_train\");","83b39d2f":"sns.set(style=\"darkgrid\")\nsns.countplot(x=y_test)\nplt.title(\"y_test\");","cdf89152":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom jcopml.tuning import random_search_params as rsp\nfrom jcopml.tuning import random_search_params as gsp\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer ","7d1a40df":"pipeline = Pipeline([\n    ('prep', TfidfVectorizer(tokenizer=word_tokenize, ngram_range=(1, 3))),\n    ('algo', LogisticRegression(solver='lbfgs', n_jobs=-1, random_state=42))\n])\n\nmodel_logreg_tfidf = RandomizedSearchCV(pipeline, rsp.logreg_params, cv=5, n_iter=50, n_jobs=-1, verbose=1, random_state=42)\nmodel_logreg_tfidf.fit(X_train, y_train)\n\nprint(model_logreg_tfidf.best_params_)\nprint(model_logreg_tfidf.score(X_train, y_train), model_logreg_tfidf.best_score_, model_logreg_tfidf.score(X_test, y_test))","c0d99bcd":"pipeline = Pipeline([\n    ('prep', CountVectorizer(tokenizer=word_tokenize, ngram_range=(1, 3))),\n    ('algo', LogisticRegression(solver='lbfgs', n_jobs=-1, random_state=42))\n])\n\nmodel_logreg_bow = RandomizedSearchCV(pipeline, rsp.logreg_params, cv=5, n_iter=50, n_jobs=-1, verbose=1, random_state=42)\nmodel_logreg_bow.fit(X_train, y_train)\n\nprint(model_logreg_bow.best_params_)\nprint(model_logreg_bow.score(X_train, y_train), model_logreg_bow.best_score_, model_logreg_bow.score(X_test, y_test))","12fdd33d":"# save model\nsave_model(model_logreg_bow, \"logreg_bow_pipeline_sentiment_checker.pkl\")","03e6d3eb":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom jcopml.tuning import random_search_params as rsp\nfrom jcopml.tuning import random_search_params as gsp\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer ","a7f0d682":"pipeline = Pipeline([\n    ('prep', CountVectorizer(tokenizer=word_tokenize, ngram_range=(1, 3))),\n    ('algo', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n])\n\n\nparameter = {\n    'algo__loss': ['hinge', 'log', 'modified_huber', 'perceptron'],\n    'algo__penalty': ['l2', 'l1', 'elasticnet'],\n    'algo__alpha': [0.0001, 0.0002, 0.0003], \n    'algo__max_iter': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n    'algo__tol': [0.0001, 0.0002, 0.0003]\n}\n# model = GridSearchCV(pipeline, parameter, cv=5, n_jobs=-1, verbose=1)\nmodel_sgd_bow = RandomizedSearchCV(pipeline, parameter, cv=50, n_jobs=-1, verbose=1)\nmodel_sgd_bow.fit(X_train, y_train)\n\n\nprint(model_sgd_bow.best_params_)\nprint(model_sgd_bow.score(X_train, y_train), model_sgd_bow.best_score_, model_sgd_bow.score(X_test, y_test))","a325b8b9":"pipeline = Pipeline([\n    ('prep', TfidfVectorizer(tokenizer=word_tokenize, ngram_range=(1, 3))),\n    ('algo', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n])\n\n\nparameter = {\n    'algo__loss': ['hinge', 'log', 'modified_huber', 'perceptron'],\n    'algo__penalty': ['l2', 'l1', 'elasticnet'],\n    'algo__alpha': [0.0001, 0.0002, 0.0003], \n    'algo__max_iter': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n    'algo__tol': [0.0001, 0.0002, 0.0003]\n}\nmodel_sgd_tfidf = RandomizedSearchCV(pipeline, parameter, cv=50, n_jobs=-1, verbose=1, random_state=42)\nmodel_sgd_tfidf.fit(X_train, y_train)\n\n\nprint(model_sgd_tfidf.best_params_)\nprint(model_sgd_tfidf.score(X_train, y_train), model_sgd_tfidf.best_score_, model_sgd_tfidf.score(X_test, y_test))","a785a8c8":"text = ['kemenangan jokowi membawa berkah besar', 'sayang prabowo tidak menang', 'menang menang', 'kalah menang']\nmodel_sgd_bow.predict(text)","8b8fc431":"# save model to joblib\nfrom joblib import dump\n\ndump(model_sgd_bow, filename=\"sentiment_prediction.joblib\")","210c6f19":"# load the joblib\nfrom joblib import load\n\nmodel_joblib = load(\".\/sentiment_prediction.joblib\")","d286d4d7":"# predict with model_joblib\nmodel_joblib.predict(text)","751d978b":"X = df.Isi_Tweet\ny = df.Sentimen\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","42552e5d":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom jcopml.tuning import random_search_params as rsp\nfrom jcopml.tuning import random_search_params as gsp\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer ","01e70623":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler","25c1c12b":"from sklearn.compose import ColumnTransformer","aba8d48e":"df.head()","d10bc30c":"# create a TF-IDF vectorizer object\ntfidf_vectorizer = TfidfVectorizer(lowercase= True, max_features=1000)\n\n# fit the object with the training data tweets\ntfidf_vectorizer.fit(X_train)","549812ce":"https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/how-to-deploy-machine-learning-model-flask\/","811e4d81":"pipeline.fit(X_train.Isi_Tweet, y_train.Sentimen)","0c6fe3fc":"dump(model_sgd_bow, filename=\"sentiment_classification.joblib\")\n\nmodel_joblib = load(\".\/sentiment_classification.joblib\")","e6f377b6":"# Common Packages\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfrom jcopml.pipeline import num_pipe, cat_pipe\nfrom jcopml.utils import save_model, load_model\nfrom jcopml.plot import plot_missing_value\nfrom jcopml.feature_importance import mean_score_decrease\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\n# Preprocessing Packages\nimport nltk \nimport re\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom string import punctuation\n\n# Training Packages\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom jcopml.tuning import random_search_params as rsp\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n\n\n# Import Data\ndf = pd.read_csv(\"..\/input\/tweet-sentiment-indonesian-president-candidates-14\/Capres2014-2.0 (1).csv\", encoding='latin1')\n\n\n# Text Preprocessing\n# 1) Normalization to Lower Case & Removing unimportant characters\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(\"\\n\", \" \", text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = text.split()\n    text = \" \".join(text)\n    return text\n\ndf1 = df.Isi_Tweet.apply(str).apply(lambda x:clean_text(x))\n\n# 2) Sentence & Word Tokenization; Punctuation and Words Removal\nfor i in range(len(df1)):\n    x = df1[i]\n    x_word_tokens = word_tokenize(x)\n    \n    # punctuation removal\n    x_word_tokens_removed_punctuations = [w for w in x_word_tokens if w not in punctuation]\n    \n    # numeric removal\n    x_word_tokens_removed_punctuations = [w for w in x_word_tokens_removed_punctuations if w.isalpha()]\n    \n    # stopwords removal\n    x_word_tokens_removed_punctuation_removed_sw = [w for w in x_word_tokens_removed_punctuations if w not in sw]\n\n    # rejoining the words into one string\/sentence as inputted before being tokenized\n    x_word_tokens_removed_punctuation_removed_sw = \" \".join(x_word_tokens_removed_punctuation_removed_sw)\n    \n    df1_clean_text.append(x_word_tokens_removed_punctuation_removed_sw)\n\n# converting list to pandas series\ndf1_clean_text_series = pd.Series(df1_clean_text)\n\n# new df\ndf['Isi_Tweet'] = df1_clean_text_series\ndf.head(10)\n\n\n# Dataset Splitting\u00b6\nX = df.Isi_Tweet\ny = df.Sentimen\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n\n\n# Training Process\npipeline = Pipeline([\n    ('prep', CountVectorizer(tokenizer=word_tokenize, ngram_range=(1, 3))),\n    ('algo', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n])\n\nparameter = {\n    'algo__loss': ['hinge', 'log', 'modified_huber', 'perceptron'],\n    'algo__penalty': ['l2', 'l1', 'elasticnet'],\n    'algo__alpha': [0.0001, 0.0002, 0.0003], \n    'algo__max_iter': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n    'algo__tol': [0.0001, 0.0002, 0.0003]\n}\n\nmodel_sgd_bow = RandomizedSearchCV(pipeline, parameter, cv=50, n_jobs=-1, verbose=1)\nmodel_sgd_bow.fit(X_train, y_train)\n\n# print(model_sgd_bow.best_params_)\n# print(model_sgd_bow.score(X_train, y_train), model_sgd_bow.best_score_, model_sgd_bow.score(X_test, y_test))","2957dd82":"# Common Packages\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfrom jcopml.pipeline import num_pipe, cat_pipe\nfrom jcopml.utils import save_model, load_model\nfrom jcopml.plot import plot_missing_value\nfrom jcopml.feature_importance import mean_score_decrease\n\n# Training Packages\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom jcopml.tuning import random_search_params as rsp\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n\n\n# Import Data\ndf = pd.read_csv(\"..\/input\/tweet-sentiment-indonesian-president-candidates-14\/Capres2014-2.0 (1).csv\", encoding='latin1')\n\n\n# Dataset Splitting\nX = df.Isi_Tweet\ny = df.Sentimen\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n\n\n# Training Process\npipeline = Pipeline([\n    ('prep', CountVectorizer(tokenizer=word_tokenize, ngram_range=(1, 3))),\n    ('algo', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n])\n\nparameter = {\n    'algo__loss': ['hinge', 'log', 'modified_huber', 'perceptron'],\n    'algo__penalty': ['l2', 'l1', 'elasticnet'],\n    'algo__alpha': [0.0001, 0.0002, 0.0003], \n    'algo__max_iter': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n    'algo__tol': [0.0001, 0.0002, 0.0003]\n}\n\nmodel_sgd_bow = RandomizedSearchCV(pipeline, parameter, cv=50, n_jobs=-1, verbose=1)\nmodel_sgd_bow.fit(X_train, y_train)\n\n# print(model_sgd_bow.best_params_)\n# print(model_sgd_bow.score(X_train, y_train), model_sgd_bow.best_score_, model_sgd_bow.score(X_test, y_test))","79f0541c":"\n\ndf_data = df[[\"Isi_Tweet\", \"Sentimen\"]]\n    \n# dataset splitting\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nX = df_data[\"Isi_Tweet\"]\ny = df_data[\"Sentimen\"]\n\ncv = TfidfVectorizer()\nX = cv.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n\n# training\nfrom sklearn.linear_model import SGDClassifier\nclf = SGDClassifier(tol=0.0003, loss='modified_huber', penalty='l2', alpha=0.0002, random_state=42, max_iter=5)\nclf.fit(X_train, y_train)","8557edd3":"clf.score(X_test, y_test)","3ebb2c48":"# save model\nfrom jcopml.utils import save_model","6ea00ce9":"save_model(clf, \"sgd_tfidf_wo_pipeline_sentiment_checker.pkl\")","e8866b4c":"##### Sample Text","9ca044e3":"##### resource:\n\n- Lemmatization: https:\/\/www.geeksforgeeks.org\/python-lemmatization-with-nltk\/\n- Stemming: https:\/\/www.geeksforgeeks.org\/python-stemming-words-with-nltk\/ <br>  https:\/\/www.machinelearningplus.com\/nlp\/lemmatization-examples-python\/\n- Replacing values in DataFrame: https:\/\/www.python-course.eu\/pandas_replacing_values.php\n- Converting list into pandas series: https:\/\/www.geeksforgeeks.org\/creating-a-pandas-series-from-lists\/","1b081548":"# Code","b5ceb76f":"## Logistic Regression","b424f447":"# PART 2: NOT USING PIPELINE","440c6457":"#### Sanitation Check","52eb7890":"##### y_test dataset","9e0a9c5b":"##### Display text samples","793367a8":"#### Deployment Code with Pipeline - it is not recommended to do training on the deployment","16e90b9b":"# Linear SVM\n\nsource: https:\/\/towardsdatascience.com\/multi-class-text-classification-model-comparison-and-selection-5eb066197568#:~:text=Linear%20Support%20Vector%20Machine%20is,the%20best%20text%20classification%20algorithms.&text=We%20achieve%20a%20higher%20accuracy,5%25%20improvement%20over%20Naive%20Bayes.","fa6c0152":"## Feature Engineering - Basic Text Processing","e40b6af2":"#### Code on app.py - MLApplications","89151539":"##### TFIDF","4bbd373e":"# Training","0dc938cb":"##### BoW","119d3d5c":"##### 2) Sentence & Word Tokenization; Punctuation and Words Removal ","95aa9bde":"#### Convert to joblib","19b2c893":"# Dataset Splitting","a24e5246":"# Convert Model to joblib for deployment","ff2325f2":"#### Bag of Words - CountVectorizer","87597535":"### Text Processing","af5e94d0":"# Import Data","d8068c88":"# PART 1 - WITH PIPELINE","f83fe624":"##### Display text samples","c10edb55":"##### y_train dataset","218f1ce0":"#### Full Code of Sentiment Analysis","ceb0464d":"#### TF-IDF","930e4696":"##### joining the series into dataframe","c9edb220":"##### 1) Normalization to Lower Case & Removing \"https: ...\"","7427771a":"# Visualize the Target Label","55888d26":"## Training"}}