{"cell_type":{"9d071918":"code","0664b973":"code","5aa137cc":"code","0da7a772":"code","c1fcfb91":"code","8a0dcef8":"code","84017bee":"code","792200a9":"code","5860bf50":"code","e1835ae5":"code","71e32f47":"code","507e1d29":"code","119d5399":"code","898f763e":"code","b886fde4":"code","7c30db46":"code","3a455955":"code","5968344c":"code","dedb2fba":"code","92f6fc45":"code","f8119e0c":"code","ce979f1f":"code","45880565":"code","a883f0b7":"code","a194060d":"code","33fcf6d9":"code","13d2a20a":"markdown","f0ad7b42":"markdown","0354ecc9":"markdown","a27e0181":"markdown"},"source":{"9d071918":"!conda install -y mpi4py \n!pip -qq install deepspeed","0664b973":"SAMPLE = False # set True for debugging","5aa137cc":"!pip install seqeval -qq # evaluation metrics for training (not the competition metric)\n!pip install --upgrade wandb -qq # experiment tracking","0da7a772":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","c1fcfb91":"# CONFIG\n\nEXP_NUM = 4\ntask = \"ner\"\nmodel_checkpoint = \"allenai\/longformer-base-4096\"\nmax_length = 1024\nstride = 128\nmin_tokens = 6\nmodel_path = f'{model_checkpoint.split(\"\/\")[-1]}-{EXP_NUM}'\n\n# TRAINING HYPERPARAMS\nBS = 10\nGRAD_ACC = 2\nLR = 5e-5\nWD = 0.01\nWARMUP = 0.1\nN_EPOCHS = 5","8a0dcef8":"import pandas as pd\n\n# read train data\ntrain = pd.read_csv('..\/input\/feedback-prize-2021\/train.csv')\ntrain.head(1)","84017bee":"# check unique classes\nclasses = train.discourse_type.unique().tolist()\nclasses","792200a9":"# setup label indices\n\nfrom collections import defaultdict\ntags = defaultdict()\n\nfor i, c in enumerate(classes):\n    tags[f'B-{c}'] = i\n    tags[f'I-{c}'] = i + len(classes)\ntags[f'O'] = len(classes) * 2\ntags[f'Special'] = -100\n    \nl2i = dict(tags)\n\ni2l = defaultdict()\nfor k, v in l2i.items(): \n    i2l[v] = k\ni2l[-100] = 'Special'\n\ni2l = dict(i2l)\n\nN_LABELS = len(i2l) - 1 # not accounting for -100","5860bf50":"# some helper functions\n\nfrom pathlib import Path\n\npath = Path('..\/input\/feedback-prize-2021\/train')\n\ndef get_raw_text(ids):\n    with open(path\/f'{ids}.txt', 'r') as file: data = file.read()\n    return data","e1835ae5":"# group training labels by text file\n\ndf1 = train.groupby('id')['discourse_type'].apply(list).reset_index(name='classlist')\ndf2 = train.groupby('id')['discourse_start'].apply(list).reset_index(name='starts')\ndf3 = train.groupby('id')['discourse_end'].apply(list).reset_index(name='ends')\ndf4 = train.groupby('id')['predictionstring'].apply(list).reset_index(name='predictionstrings')\n\ndf = pd.merge(df1, df2, how='inner', on='id')\ndf = pd.merge(df, df3, how='inner', on='id')\ndf = pd.merge(df, df4, how='inner', on='id')\ndf['text'] = df['id'].apply(get_raw_text)\n\ndf.head()","71e32f47":"# debugging\nif SAMPLE: df = df.sample(n=100).reset_index(drop=True)","507e1d29":"# we will use HuggingFace datasets\nfrom datasets import Dataset, load_metric\n\nds = Dataset.from_pandas(df)\ndatasets = ds.train_test_split(test_size=0.1, shuffle=True, seed=42)\ndatasets","119d5399":"from transformers import AutoTokenizer\n    \ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)","898f763e":"# Not sure if this is needed, but in case we create a span with certain class without starting token of that class,\n# let's convert the first token to be the starting token.\n\ne = [0,7,7,7,1,1,8,8,8,9,9,9,14,4,4,4]\n\ndef fix_beginnings(labels):\n    for i in range(1,len(labels)):\n        curr_lab = labels[i]\n        prev_lab = labels[i-1]\n        if curr_lab in range(7,14):\n            if prev_lab != curr_lab and prev_lab != curr_lab - 7:\n                labels[i] = curr_lab -7\n    return labels\n\nfix_beginnings(e)","b886fde4":"# tokenize and add labels\ndef tokenize_and_align_labels(examples):\n\n    o = tokenizer(examples['text'], truncation=True, padding=True, return_offsets_mapping=True, max_length=max_length, stride=stride, return_overflowing_tokens=True)\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = o[\"overflow_to_sample_mapping\"]\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = o[\"offset_mapping\"]\n    \n    o[\"labels\"] = []\n\n    for i in range(len(offset_mapping)):\n                   \n        sample_index = sample_mapping[i]\n\n        labels = [l2i['O'] for i in range(len(o['input_ids'][i]))]\n\n        for label_start, label_end, label in \\\n        list(zip(examples['starts'][sample_index], examples['ends'][sample_index], examples['classlist'][sample_index])):\n            for j in range(len(labels)):\n                token_start = offset_mapping[i][j][0]\n                token_end = offset_mapping[i][j][1]\n                if token_start == label_start: \n                    labels[j] = l2i[f'B-{label}']    \n                if token_start > label_start and token_end <= label_end: \n                    labels[j] = l2i[f'I-{label}']\n\n        for k, input_id in enumerate(o['input_ids'][i]):\n            if input_id in [0,1,2]:\n                labels[k] = -100\n\n        labels = fix_beginnings(labels)\n                   \n        o[\"labels\"].append(labels)\n        \n    return o","7c30db46":"tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True,batch_size=20000, remove_columns=datasets[\"train\"].column_names)","3a455955":"tokenized_datasets","5968344c":"# we will use auto model for token classification\nfrom transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n\nmodel = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=N_LABELS)","dedb2fba":"###### DECLARING DEEPSPEED CONDFIG ##################\n\nds_config_dict = {\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"betas\": \"auto\",\n            \"eps\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\"\n        }\n    },\n\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": True\n        },\n        \"allgather_partitions\": True,\n        \"allgather_bucket_size\": 2e8,\n        \"overlap_comm\": True,\n        \"reduce_scatter\": True,\n        \"reduce_bucket_size\": 5e8,\n        \"contiguous_gradients\": True\n    },\n\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 2000,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": False\n}","92f6fc45":"model_name = model_checkpoint.split(\"\/\")[-1]\nargs = TrainingArguments(\n    f\"{model_name}-finetuned-{task}\",\n    evaluation_strategy = \"epoch\",\n    logging_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate=LR,\n    per_device_train_batch_size=BS,\n    per_device_eval_batch_size=BS,\n    num_train_epochs=N_EPOCHS,\n    weight_decay=WD,\n   # report_to='wandb', \n    gradient_accumulation_steps=GRAD_ACC,\n    warmup_ratio=WARMUP,\n    fp16 = True,\n    \n    #### THE ONLY CHANGE YOU NEED TO MAKE TO USE DEEPSPEED ########\n    deepspeed=ds_config_dict\n)","f8119e0c":"from transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(tokenizer)","ce979f1f":"# this is not the competition metric, but for now this will be better than nothing...\n\nmetric = load_metric(\"seqeval\")","45880565":"import numpy as np\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [i2l[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [i2l[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }","a883f0b7":"trainer = Trainer(\n    model,\n    args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics, \n)","a194060d":"trainer.train()","33fcf6d9":"trainer.save_model(model_path)","13d2a20a":"## End\n\nI'll appreciate every upvote or comment!","f0ad7b42":"## Model and Training","0354ecc9":"## Data Preprocessing","a27e0181":"# Introduction\n\nAs we all know , this is a hardware heavy competition and training at a higher MAX_LEN and a larger model does the trick , but all those with limited hardware like me have been struggling to do so . DeepSpeed Library allows one to Train Large models with bigger batch sizes on smaller GPUs , This notebook integrates DeepSpeed with HF Trainer and most of it is a replica of the wonderful Baseline prepared by Darek [Here](https:\/\/www.kaggle.com\/thedrcat\/feedback-prize-huggingface-baseline-training) \n\n\n<b>Please Note that in this notebook I show an example to use DeepSpeed and its features to train LongFormer Base with a batch_size of 10 on a 16GB card . We can train even larger batch size or bigger sequences and utilize more juice out of a 16Gb card if we have more CPU memory but since kaggle gives us only 16GB of memory I was only able to demonstrate use of Stage2 ZeRO Optimizer given by DEEPSPEED<\/b>\n\nI am able to train longformer large on a bs of 6 on a 24GB RTX 6000 card on [Jarvislabs.ai](https:\/\/cloud.jarvislabs.ai\/) using DeepSpeed and HF Trainer . Here is an [article](https:\/\/jarvislabs.ai\/blogs\/deepspeed) which describes in detail to make full use of DeepSpeed \n\n\nHope you all are able to use your GPU's more efficiently , thanks for reading"}}