{"cell_type":{"a3af1d6b":"code","53c2f551":"code","3a43cd08":"code","c58ac043":"code","a470fe9e":"code","06b84b79":"code","cf76081a":"markdown","cec6c961":"markdown","5f5d1f29":"markdown"},"source":{"a3af1d6b":"!pip install kaggle-environments --upgrade -q","53c2f551":"%%writefile submission.py\n\nimport random\nimport sys\n\nclass CopyLastOpponentMoveUnlessWin:\n    def __init__(self, retry_winrate=5\/6, retry_max_step=1000, verbose=True):\n        self.retry_winrate  = retry_winrate\n        self.retry_max_step = retry_max_step or sys.maxsize \n        self.verbose        = verbose\n        \n        self.last_reward = 0\n        self.last_action = 0\n        self.last_winrate = { \"count\": 0, \"reward\": 0 }\n\n    def __call__(self, obs, conf):\n        return self.agent(obs, conf)\n\n    def winrate(self) -> float:\n        winrate = self.last_winrate['reward'] \/ max(1, self.last_winrate['count'])\n        return winrate\n    \n    def print_winrate(self):\n        winrate = self.winrate()\n        print( f\"winrate {self.last_action:02d} = {self.last_winrate['reward']:2d} \/ {max(1, self.last_winrate['count']):2d} = {winrate:.2f}\" )\n        \n    \n    # observation   {'remainingOverageTime': 60, 'agentIndex': 1, 'reward': 0, 'step': 0, 'lastActions': []}\n    # configuration {'episodeSteps': 2000, 'actTimeout': 0.25, 'runTimeout': 1200, 'banditCount': 100, 'decayRate': 0.97, 'sampleResolution': 100}\n    def agent(self, obs, conf) -> int:\n        # print('observation', obs)\n        # print('configuration', conf)\n                \n        # First round doesn't have lastActions \n        if obs.step > 0 and len(obs.lastActions): \n            self.last_winrate['count'] += 1\n            \n            action = None\n            \n            # Stop doing retry near the end of the game\n            if obs.step <= self.retry_max_step:            \n                # Copy opponent move unless win\n                if obs.reward > self.last_reward:\n                    self.last_winrate['reward'] += 1\n                    action = self.last_action\n\n                # If we have found a bandit with a high winrate, keep trying it unless over retry_max_step\n                elif self.winrate() >= self.retry_winrate:\n                    action = self.last_action\n            \n            # Else copy opponent action and reset stats\n            if action is None:            \n                self.last_winrate = { \"count\": 0, \"reward\": 0 }\n                opponentIndex  = (obs.agentIndex + 1) % len(obs.lastActions)\n                opponentAction = obs.lastActions[opponentIndex]\n                action         = opponentAction\n        else:\n            # When in doubt, be random\n            action = random.randrange(conf.banditCount) \n\n        self.last_action = action = int(action or 0) % conf.banditCount\n        self.last_reward = obs.reward\n        \n        if self.verbose:\n            self.print_winrate()\n\n        return action\n\n    \ninstance = CopyLastOpponentMoveUnlessWin()\ndef kaggle_agent(obs, conf):\n    return instance.agent(obs, conf)","3a43cd08":"%run submission.py","c58ac043":"import random\ndef random_agent(observation, configuration):\n    return random.randrange(configuration.banditCount)","a470fe9e":"from kaggle_environments import make, evaluate\n\n# env = make(\"mab\", debug=True, configuration={\"episodeSteps\": 50})\nenv = make(\"mab\", debug=True)\n\nenv.reset()\n# env.run([\n#     CopyLastOpponentMoveUnlessWin(retry_winrate=5\/6, retry_max_step=1000), \n#     CopyLastOpponentMoveUnlessWin(retry_winrate=1,   retry_max_step=2000),\n# ])\nenv.run([\"submission.py\", random_agent])\nenv.render(mode=\"ipython\", width=500, height=500)","06b84b79":"import numpy as np\nfrom joblib import Parallel, delayed\n\nresults = np.array(Parallel(-1)([\n    delayed(evaluate)(\"mab\", [\"submission.py\", random_agent])\n    for n in range(10)\n])).reshape(-1,2)\n\nprint('results\\n', results)\nprint('mean', np.mean(results, axis=0))","cf76081a":"# Rock Paper Candy - Copy Opponent Move Unless Win\n\nThis follows on from the Copy Opponent Move strategy, but will repeatedly try any machine that\nproduces a win payout, and only copy the opponent move after a loss \n- https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-candy-copy-opponent-move\/","cec6c961":"# Further Reading\n\nThis notebook is part of a series exploring the Santa2020 Candy Cane competition\n- [Rock Paper Candy - Copy Opponent Move](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-candy-copy-opponent-move)\n- [Rock Paper Candy - Copy Opponent Move Unless Win](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-candy-copy-opponent-move-unless-win)\n- [Candy Cane - Multi-Armed Bandit](https:\/\/www.kaggle.com\/jamesmcguigan\/candy-cane-multi-armed-bandit)\n- [Candy Cane - Optimized UCB](https:\/\/www.kaggle.com\/jamesmcguigan\/candy-cane-optimized-ucb)\n- [Candy Cane - Random Agent](https:\/\/www.kaggle.com\/jamesmcguigan\/candy-cane-random-agent)\n\nI also created an agents comparison notebook to compare the relative strengths of public agents:\n- [Santa 2020 - Agents Comparison](https:\/\/www.kaggle.com\/jamesmcguigan\/santa-2020-agents-comparison\/)","5f5d1f29":"Turns out we can beat random bot!"}}