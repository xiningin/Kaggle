{"cell_type":{"93f372b0":"code","8774357a":"code","1733e103":"code","148bffc2":"code","327cea62":"code","b98a5bdc":"code","1b9807ef":"code","87e400f2":"code","3b99586f":"code","f53c8ef6":"code","7d00f4e4":"code","4d657d73":"code","e55a9fc7":"code","6e08bfff":"code","94c78b03":"code","55583707":"code","db5f58ff":"code","3533a285":"code","11a321ce":"code","c2b86234":"code","f0769822":"code","eab20337":"code","34e38c8e":"code","e207832a":"code","c7910237":"code","ecee920e":"code","fac2f7dd":"code","8000aa35":"code","1a2a6784":"code","42a8e641":"code","a66c5130":"code","85f4d1c0":"code","ed5b9a08":"code","e50e38e8":"code","2f08ea59":"code","78314703":"code","15a650be":"code","7174a4c5":"code","fc916eab":"code","10571498":"code","4d8b7692":"code","1d5c638a":"code","50f1d4a3":"code","61ed0920":"code","1a549bff":"code","9d54bfce":"code","29060dab":"code","896bad09":"markdown","5a095b57":"markdown","f299a22e":"markdown","3e0dbb23":"markdown","0e53a4f9":"markdown","ca7715de":"markdown","8dbd2818":"markdown"},"source":{"93f372b0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8774357a":"train = pd.read_csv('..\/input\/hackerearth-ml-solving-the-citizens-grievances\/dataset\/train.csv')\ntrain","1733e103":"train['sharepointid'].apply(lambda x: str(x)[:3])\ntrain['sharepointid']","148bffc2":"train = train.drop(['decisiondate','application','docname','ecli','introductiondate','country.name','itemid','languageisocode','originatingbody_type','originatingbody_name','sharepointid','documentcollectionid=CASELAW' ,'documentcollectionid=JUDGMENTS','documentcollectionid=CHAMBER','documentcollectionid=ENG','documentcollectionid=COMMITTEE','documentcollectionid=GRANDCHAMBER','typedescription'],axis = 1)","327cea62":"test = pd.read_csv('..\/input\/hackerearth-ml-solving-the-citizens-grievances\/dataset\/test.csv')","b98a5bdc":"test = test.drop(['decisiondate','application','docname','ecli','introductiondate','country.name','itemid','languageisocode','originatingbody_type','originatingbody_name','sharepointid','documentcollectionid=CASELAW' ,'documentcollectionid=JUDGMENTS','documentcollectionid=CHAMBER','documentcollectionid=ENG','documentcollectionid=COMMITTEE','documentcollectionid=GRANDCHAMBER','typedescription'],axis = 1)","1b9807ef":"from sklearn import preprocessing \nlabel_encoder = preprocessing.LabelEncoder() \ntrain['country.alpha2']= label_encoder.fit_transform(train['country.alpha2']) \ntest['country.alpha2']= label_encoder.transform(test['country.alpha2']) ","87e400f2":"db=pd.get_dummies(train['doctypebranch'],drop_first=True)\ndb.head()","3b99586f":"db=pd.get_dummies(test['doctypebranch'],drop_first=True)\ndb.head()","f53c8ef6":"train=pd.concat([train,db],axis=1)","7d00f4e4":"test=pd.concat([test,db],axis=1)","4d657d73":"train = train.drop(['doctypebranch'],axis=1)\ntest = test.drop(['doctypebranch'],axis=1)","e55a9fc7":"cols = [c for c in train.columns if c.lower()[:5] != 'issue']","6e08bfff":"train = train[cols]","94c78b03":"col = [c for c in test.columns if c.lower()[:5] != 'issue']\ntest = test[col]","55583707":"cols = [c for c in train.columns if c.lower()[:7] != 'parties']\ntrain = train[cols]","db5f58ff":"col = [c for c in test.columns if c.lower()[:7] != 'parties']\ntest = test[col]","3533a285":"cols = [c for c in train.columns if c.lower()[:10] != 'respondent']\ntrain = train[cols]","11a321ce":"col = [c for c in test.columns if c.lower()[:10] != 'respondent']\ntest = test[col]","c2b86234":"train['judgementdate'] = pd.to_datetime(train['judgementdate'])\ntrain = train.assign(\n               day=train.judgementdate.dt.day,\n               month=train.judgementdate.dt.month,\n               year=train.judgementdate.dt.year)\ntrain.head()\ntrain = train.drop(['judgementdate'],axis = 1)","f0769822":"test['judgementdate'] = pd.to_datetime(test['judgementdate'])\ntest = test.assign(\n               day=test.judgementdate.dt.day,\n               month=test.judgementdate.dt.month,\n               year=test.judgementdate.dt.year)\ntest.head()\ntest = test.drop(['judgementdate'],axis = 1)","eab20337":"train['kpdate'] = pd.to_datetime(train['kpdate'])\ntrain = train.assign(\n               day_k=train.kpdate.dt.day,\n               month_k=train.kpdate.dt.month,\n               year_k=train.kpdate.dt.year)\ntrain.head()\ntrain = train.drop(['kpdate'],axis = 1)","34e38c8e":"test['kpdate'] = pd.to_datetime(test['kpdate'])\ntest = test.assign(\n               day_k=test.kpdate.dt.day,\n               month_k=test.kpdate.dt.month,\n               year_k=test.kpdate.dt.year)\ntest.head()\ntest = test.drop(['kpdate'],axis = 1)","e207832a":"so=pd.get_dummies(train['separateopinion'],drop_first=True)\nso.head()","c7910237":"train = train.drop(['separateopinion'],axis=1)","ecee920e":"train=pd.concat([train,so],axis=1)","fac2f7dd":"so=pd.get_dummies(test['separateopinion'],drop_first=True)","8000aa35":"test = test.drop(['separateopinion'],axis=1)","1a2a6784":"test=pd.concat([test,so],axis=1)","42a8e641":"train","a66c5130":"train_y = train['importance']","85f4d1c0":"train_x = train.drop(['appno' ,'importance' ],axis=1)","ed5b9a08":"params={\n \"learning_rate\"    : [ 0.10, 0.15, 0.20] ,\n \"max_depth\"        : [ 5, 6, 7 ,8],\n \"min_child_weight\" : [ 1, 3, 5 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5],  \n}","e50e38e8":"## Hyperparameter optimization using RandomizedSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nimport xgboost\nfrom sklearn.metrics import precision_score,recall_score,accuracy_score,f1_score,roc_auc_score","2f08ea59":"classifier=xgboost.XGBClassifier(n_estimators = 1000,nthread=1,objective = 'binary:logistic')","78314703":"random_search=RandomizedSearchCV(classifier,param_distributions=params,n_iter=5,scoring='accuracy',n_jobs=4,cv=5,verbose=3)","15a650be":"random_search.fit(train_x, train_y)","7174a4c5":"random_search.best_params_","fc916eab":"idno = test['appno']","10571498":"test = test.drop(['appno'],axis=1)","4d8b7692":"pred = random_search.predict(test)","1d5c638a":"pred","50f1d4a3":"df = pd.DataFrame()","61ed0920":"df['appno'] = idno","1a549bff":"df['importance'] = pred","9d54bfce":"df","29060dab":"df.to_csv('file.csv',index=False) ","896bad09":"***If you like the notebook please drop a like***","5a095b57":"***Dropping issue , parties , respondent columns***","f299a22e":"***Splitting judgementdate feature into 3 features of day,month,year***","3e0dbb23":"***Dropping unwanted features***","0e53a4f9":"***HyperParameter Tunning***","ca7715de":"Most of the time in an unbiased dataset, xgboost works well.\nYou can try SMOTEboost or Catboost techniques also.\n","8dbd2818":"***Encoding some feature values***"}}