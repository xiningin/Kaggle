{"cell_type":{"b2a1f85d":"code","5d34dbd2":"code","0fc28151":"code","1cbcb5ea":"code","04e1a4b8":"code","f45ac66e":"code","9fb0cf24":"code","538207e2":"code","da712198":"code","1b26c93c":"markdown","6c3ea419":"markdown","2996eddf":"markdown","e43a2fdb":"markdown","8c854dc3":"markdown","91e1fce2":"markdown","d0b2339f":"markdown","dcc954ea":"markdown","7a9b2a7f":"markdown","a5355e06":"markdown","10e83972":"markdown"},"source":{"b2a1f85d":"START, END = '\u00f7', '\u25a0'\nmax_features, max_len = 2500, 15","5d34dbd2":"import pickle\ntokenizer = pickle.load(open('..\/input\/tokenization\/tokenizer_{}.pkl'.format(max_features), 'rb'))","0fc28151":"tokenizer.word_index['trump'], tokenizer.word_index[START], tokenizer.word_index[END], tokenizer.word_index[';']","1cbcb5ea":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nimport numpy as np\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers import Input, Dense, Bidirectional, GRU, Embedding, Dropout, LSTM\nfrom keras.layers import concatenate, SpatialDropout1D, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.models import Model, Sequential\n\nmodel = Sequential()\n\n# Embedding and GRU\nmodel.add(Embedding(max_features, 250, trainable=False))\nmodel.add(SpatialDropout1D(0.5))\nmodel.add(Bidirectional(LSTM(100)))\n\n# Output layer\nmodel.add(Dense(max_features, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.load_weights('..\/input\/lm-reuters-e-50-d-200k-f-2500\/model50.h5')","04e1a4b8":"def sample(preds, temp=1.0):\n    \"\"\"\n    Sample next word given softmax probabilities, using temperature.\n    \n    Taken and modified from:\n    https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/lstm_text_generation.py\n    \"\"\"\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) \/ temp\n    preds = np.exp(preds) \/ np.sum(np.exp(preds))\n    preds = np.random.multinomial(1, preds, 1)\n    return np.argmax(preds)","f45ac66e":"from collections import defaultdict\n\ndef read_file(fname):\n    \"\"\"Read contents of file\"\"\"\n    f = open(fname, 'rb')\n    contents = pickle.load(f)\n    f.close()\n    return contents\n\ndef read_stats():\n    \"\"\"Contains a dictionary of dictionaries\n    {bigrams: next_word_counts}\"\"\"\n    trigram_pos = read_file('..\/input\/reuters-part-of-speech-extraction\/trigrams_pos.pkl')\n    \n    return trigram_pos\n\ntrigram_pos = read_stats()","9fb0cf24":"\"\"\"When sampling from the distribution, we do not know which word is being\nsampled, only its index. We need a way to go from index to word. Unfortunately,\nthe tokenizer class only contains a dictionary of {word: index} items. We will\nreverse that dictionary to get {index: word} items. That way, going from\nindices to words is much faster.\"\"\"\nidx_to_words = {value: key for key, value in tokenizer.word_index.items()}\n\n\ndef process_input(text):\n    \"\"\"Tokenize and pad input text\"\"\"\n    tokenized_input = tokenizer.texts_to_sequences([text])[0]\n    return pad_sequences([tokenized_input], maxlen=max_len-1)\n\n\ndef generate_text(input_text, model, temp=1.0):\n    \"\"\"Takes some input text and feeds it to the model (after processing it).\n    Then, samples a next word and feeds it back into the model until the end\n    token is produced.\n    \n    :input_text: A string or list of strings to be used as a generation seed.\n    :model:      The model to be used during generation.\n    :temp:       A float that adjusts how 'volatile' predictions are. A higher\n                 value increases the chance of low-probability predictions to\n                 be picked.\"\"\"\n    if type(input_text) is str:\n        sent = input_text\n    else:\n        sent = ' '.join(input_text)\n    \n    tokenized_input = process_input(input_text)\n    \n    while True:\n        preds = model.predict(tokenized_input, verbose=0)[0]\n        pred_idx = sample(preds, temp=temp)\n        pred_word = idx_to_words[pred_idx]\n        \n        if pred_word == END:\n            if len(sent.split()) <= 5:\n                continue\n            return sent\n        \n        sent += ' ' + pred_word\n        tokenized_input = process_input(sent)\n        \n        if len(sent.split()) > 25:\n            return sent","538207e2":"import spacy\nnlp = spacy.load('en_core_web_sm')\n\ndef perplexity(sent, P=trigram_pos):\n    \"\"\"Compute perplexity for sentence, given P. The lower the better.\"\"\"\n    parsed = nlp(sent)\n    sent = [START, START] + [doc.pos_ for doc in parsed] + [END, END]\n    n = len(sent)\n    \n    inv_prob = 0\n    for i in range(2, len(sent)):\n        w1, w2, w3 = sent[i-2], sent[i-1], sent[i]\n        if w3 not in P[(w1, w2)]:\n            # If the word is not found in the distribution,\n            # add a penalty of 5.\n            inv_prob += 5\n        else:\n            inv_prob += np.log(1 \/ P[(w1, w2)][w3])\n    \n    return np.power(inv_prob, 1\/n)","da712198":"results = []\nwhile True:\n    text = generate_text(START, model, temp=0.75)[2:]\n    if perplexity(text) < 1.275:\n        results.append((text, perplexity(text)))\n    \n    if len(results) == 10:\n        break\n\nresults = sorted(results, key=lambda x:x[1])\nfor r in results:\n    print(r)","1b26c93c":"In the generation function, we are given an input text. We will tokenize this input and pad it so that it is in the correct form. Then, we will feed the tokenized input into the model to compute the probability distribution of next words. We will sample from this distribution and add the selected word to the generated sentence. Then, we will feed the generated sentence in its whole to the network and generate the next word (alternatively, we can only feed part of the generated sentence). We repeat this proess until we generate the end token ('\u25a0').","6c3ea419":"## Results","2996eddf":"To pick headlines, we are going to use the perplexity score, as computed below. ","e43a2fdb":"We can now start generating text, using the starting symbol as input.","8c854dc3":"First we are going to load the pre-trained tokenizer.","91e1fce2":"## Headline Generation\n\nIn this kernel we will pick samples to use in a survey. To accomplish that, we are going to generate samples and pick the best ones according to their [perplexity score](https:\/\/en.wikipedia.org\/wiki\/Perplexity). The perplexity score is calculated over a simple Part-of-Speech distribution. To speed up the process, we are going to load a pre-trained model (50 epochs; 200k samples; 2500 words).","d0b2339f":"Finally, we are going to generate the headlines. We will keep generating headlines until we reach 10 samples that are below a certain perplexity threshold.","dcc954ea":"Next, we are going to read our POS distribution, computed over the Reuters corpus.","7a9b2a7f":"## Text Generation\n\nWith the model at hand, we are ready to start generating text. We are going to feed the starting character, '\u00f7', to the model and then continuously sample and input generated text until we reach the end character, '\u25a0'.\n\nIn our sampling function, we are going to adjust the softmax probabilities by temperature.","a5355e06":"Next, we are going to load our pre-trained model.","10e83972":"The tokenizer is basically a large dictionary of `{word: index}` items. We are going to print indices for a random word and the two special tokens (to make sure they are included in the tokenizer)."}}