{"cell_type":{"3256aa80":"code","dd99e22d":"code","8a619798":"code","4221af6c":"code","4016968d":"code","d5fd73a4":"code","3eb86b26":"code","f9b1c431":"code","dac2e22e":"code","f64dcc7a":"code","7b925563":"code","05745494":"code","191e2c58":"code","0b029159":"code","5b4662d6":"code","65af5fbf":"code","48b53e7a":"code","46ce8747":"code","08f81a97":"code","934a5286":"code","bc91365e":"markdown","5642c05e":"markdown","ffb4048d":"markdown","30c33eb6":"markdown","ced816ca":"markdown","dcc5ed6f":"markdown","7f8407db":"markdown","c61a4d3c":"markdown","3ad1a2a6":"markdown","e90afe7f":"markdown","e5afc8df":"markdown","37297a3a":"markdown","0ab7b715":"markdown","7ce71a7f":"markdown","09f9b10f":"markdown","53035bac":"markdown","ddfc1378":"markdown","cb30d46f":"markdown"},"source":{"3256aa80":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","dd99e22d":"import numpy as np\nimport pandas as pd \nimport string\nimport re\nfrom string import digits\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nfrom matplotlib import pyplot as plt","8a619798":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nprint(\"\\nTrain data: \\n\",train.head())\nprint(\"\\nTest data: \\n\",test.head())","4221af6c":"train_data=train.drop(train.columns[0], axis=1) \ntest_data=test\nprint(train_data.head())\nprint(test_data.head())","4016968d":"train_comments=train_data.iloc[:,0]\ntest_comments=test_data.iloc[:,1]\n\n#saving index to separate them later\ntrain_comments_index=train_comments.index\ntest_comments_index=test_comments.index\n\nframes = [train_comments, test_comments]\ncomments = pd.concat(frames, ignore_index=True)\n\n\nlabels=train_data.iloc[:,1:]\n\nprint(\"Train Comments Shape: \",train_comments.shape)\nprint(\"Test Comments Shape: \",test_comments.shape)\nprint(\"Comments Shape after Merge: \",comments.shape)\nprint(\"Comments are: \\n\",comments.head())\nprint(\"\\nLabels are: \\n\", labels.head())","d5fd73a4":"c=comments.str.translate(str.maketrans(' ', ' ', string.punctuation))\nc.head()","3eb86b26":"c=c.str.translate(str.maketrans(' ', ' ', '\\n'))\nc=c.str.translate(str.maketrans(' ', ' ', digits))\nc.head()","f9b1c431":"c=c.apply(lambda tweet: re.sub(r'([a-z])([A-Z])',r'\\1 \\2',tweet))\nc.head()","dac2e22e":"c=c.str.lower()\nc.head()","f64dcc7a":"c=c.str.split()\nc.head()","7b925563":"stop = set(stopwords.words('english'))\nc=c.apply(lambda x: [item for item in x if item not in stop])\nc.head()    ","05745494":"from tqdm import tqdm\nlemmatizer = WordNetLemmatizer()\ncom=[]\nfor y in tqdm(c):\n    new=[]\n    for x in y:\n        z=lemmatizer.lemmatize(x)\n        z=lemmatizer.lemmatize(z,'v')\n        new.append(z)\n    y=new\n    com.append(y)","191e2c58":"clean_data=pd.DataFrame(np.array(com), index=comments.index,columns={'comment_text'})\nclean_data['comment_text']=clean_data['comment_text'].str.join(\" \")\nprint(clean_data.head())\ntrain_clean_data=clean_data.loc[train_comments_index]\ntest_clean_data=clean_data.drop(train_comments_index,axis=0).reset_index(drop=True)\nprint(\"PreProcessed Train Data : \",train_clean_data.head(5))\nprint(\"PreProcessed Test Data : \",test_clean_data.head(5))\nframes=[train_clean_data,labels]\ntrain_result = pd.concat(frames,axis=1)\nframes=[test.iloc[:,0],test_clean_data]\ntest_result = pd.concat(frames,axis=1)\nprint(train_result.head())\nprint(test_result.head())","0b029159":"temp_df=train_result.iloc[:,2:-1]\ncorr=temp_df.corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(corr,\n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values, annot=True)","5b4662d6":"tf_idf = TfidfVectorizer(max_features=50000, min_df=2)\ntfidf_train = tf_idf.fit_transform(train_result['comment_text'])\ntfidf_test = tf_idf.transform(test_result['comment_text'])\n# import pickle\n# pickle.dump(tf_idf.vocabulary_,open(\"feature.pkl\",\"wb\"))","65af5fbf":"from keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nmodel = Sequential()\nmodel.add(Dense(100,activation='relu',input_shape=(50000,)))\nmodel.add(Dense(100,activation='relu',input_shape=(50000,)))\nmodel.add(Dense(6,activation='sigmoid'))\nmodel.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])\n","48b53e7a":"model.fit(tfidf_train, train_result[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values)\n","46ce8747":"y_pred = model.predict(tfidf_test)","08f81a97":"dict = {\n    'id': test_result.id.values,\n    'toxic' : y_pred[:,0],\n    'severe_toxic' : y_pred[:,1],\n    'obscene':y_pred[:,2],\n    'threat':y_pred[:,3],\n    'insult':y_pred[:,4],\n    'identity_hate':y_pred[:,5]\n}\nans = pd.DataFrame(dict)\nans\nans.to_csv('Submit1.csv',index=False)","934a5286":"s = input()\nc = s.translate(str.maketrans(' ', ' ', string.punctuation))\nc = c.translate(str.maketrans(' ', ' ', '\\n'))\nc = c.translate(str.maketrans(' ', ' ', digits))\nc = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', c)\nc = c.lower()\nc = c.split()\nstop = set(stopwords.words('english'))\nc = [item for item in c if item not in stop]\nfrom tqdm import tqdm\nlemmatizer = WordNetLemmatizer()\ncom = []\nfor y in tqdm(c):\n    new = []\n    for x in y:\n        z = lemmatizer.lemmatize(x)\n        z = lemmatizer.lemmatize(z, 'v')\n        new.append(z)\n    y = new\n    com.append(y)\nclean = \"\"\nfor i in com:\n    t = ''\n    clean += t.join(i) + \" \"\ntest = tf_idf.transform(np.array([clean]))\ny_pred = model.predict(test)\npred = pd.DataFrame(\n{\n    'label':labels.columns,\n    'probability':y_pred[0]\n})\n# print(train.columns)\nprint(pred)\n","bc91365e":"## Try and classify your comment","5642c05e":"## Neural network implementation : Building the model","ffb4048d":"## Drop the null values","30c33eb6":"## Removing '\\n' and digits","ced816ca":"## Read the data into a DataFrame","dcc5ed6f":"## Import necessary packages","7f8407db":"## Remove Punctuation","c61a4d3c":"## Predic the probability of each label in the test dataset","3ad1a2a6":"## Save the output as csv file","e90afe7f":"## Lemmatized form is in array form. Convert it to DataFrame using stored index.","e5afc8df":"## Remove Stop Words","37297a3a":"## Convert Word to Base Form or Lematize","0ab7b715":"## Convert a collection of raw documents to a matrix of TF-IDF features","7ce71a7f":"## Are the labels inter-related?","09f9b10f":"## Convert to lowercase","53035bac":"## Split each sentence using delimiter","ddfc1378":"## Fit the training data using the vectorized matrix","cb30d46f":"## Split combined words \nExample - Convert 'Whoareyou' to 'Who are you'."}}