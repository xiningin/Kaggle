{"cell_type":{"43ee1193":"code","1d2019b8":"code","7f49b7db":"code","8c3cf714":"code","73d347d6":"code","b4c8cbc5":"code","8bb43181":"code","d0154994":"code","03d16e9b":"code","e31203e1":"code","b9b3b1a7":"code","75dcf907":"code","a3cbc42a":"code","51f956f8":"code","d21f3c0f":"code","495d6923":"code","4b6f267b":"code","4cfab646":"code","b87d444c":"code","8ecc73e7":"code","cdd4739d":"code","22eb8dcf":"code","d3e16e5a":"code","60fa7979":"code","9564e167":"code","a71f4f19":"code","50e2b99d":"code","e973b5ac":"markdown","f60dfac8":"markdown","2dca5306":"markdown","f3158b61":"markdown","92506a53":"markdown","0bfbcd7f":"markdown","7878347a":"markdown","77e944ee":"markdown","6ca11f9d":"markdown","7456aed6":"markdown","9e6e4a20":"markdown","170fc3d3":"markdown","e717366f":"markdown","7da9aeab":"markdown"},"source":{"43ee1193":"# Read basic libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))","1d2019b8":"# Read the training data\ndata = pd.read_csv('..\/input\/drugsComTrain_raw.csv')","7f49b7db":"# Show the head of the data\ndata.head()","8c3cf714":"# Create labels based on the original article: Gr\u00e4sser et al. (2018)\nr = data['rating']\nlabels = -1*(r <= 4) + 1*(r >= 7)\n# Add the label column to the data\ndata['label'] = labels\n# Check the new data\ndata.head()","73d347d6":"# Check ratings to labels conversion\nimport matplotlib.pyplot as plt\ndata.plot(x = 'rating', y = 'label', kind = 'scatter')\nplt.show()","b4c8cbc5":"# Plot distribution of labels\ndata.hist(column = 'label', bins = np.arange(-1, 3), align = 'left');","8bb43181":"data['review_length'] = data['review'].apply(len)\ndata['review_length'].describe()","d0154994":"data.hist('review_length', bins = np.arange(0, 1500, 100));\nplt.title('Distribution of the reviews lengths')\nplt.xlabel('Review length')\nplt.ylabel('Count')\nplt.show()","03d16e9b":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Common settings for all models\nWORDS = 1000\nLENGTH = 100\nN = 10000\nDEPTH = 32\n\n# Read a part of the reviews and create training sequences (x_train)\nsamples = data['review'].iloc[:N]\ntokenizer = Tokenizer(num_words = WORDS)\ntokenizer.fit_on_texts(samples)\nsequences = tokenizer.texts_to_sequences(samples)\nx_train = pad_sequences(sequences, maxlen = LENGTH)","e31203e1":"from keras.utils import to_categorical\n\n# Convert the labels to one_hot_category values\none_hot_labels = to_categorical(labels[:N], num_classes = 3)","b9b3b1a7":"# Check the training and label sets\nx_train.shape, one_hot_labels.shape","75dcf907":"# We use the same plotting commands several times, so create a function for that purpose\ndef plot_history(history):\n    \n    f, ax = plt.subplots(1, 2, figsize = (16, 7))\n    \n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(1, len(acc) + 1)\n\n    plt.sca(ax[0])\n    plt.plot(epochs, acc, 'bo', label='Training acc')\n    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n\n    plt.sca(ax[1])\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\n    plt.show()","a3cbc42a":"# Similarly create a function for model training, for demonstration purposes we use constant values\ndef train_model(model, x, y, e = 10, bs = 32, v = 1, vs = 0.25):\n    h = model.fit(x, y, epochs = e, batch_size = bs, verbose = v, validation_split = vs)\n    return h","51f956f8":"from keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense\n\n# First model: Embedding layer -> Flatten -> Dense classifier\nm0 = Sequential()\nm0.add(Embedding(WORDS, DEPTH, input_length = LENGTH)) \nm0.add(Flatten())\nm0.add(Dense(32, activation = 'relu'))\nm0.add(Dense(3, activation = 'softmax'))\nm0.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['acc'])\nm0.summary()","d21f3c0f":"# Train the first model and plot the history\nh0 = train_model(m0, x_train, one_hot_labels)\nplot_history(h0)","495d6923":"from keras.layers import LSTM\n\n# Second model: Embedding -> LSTM -> Dense classifier\nm1 = Sequential()\nm1.add(Embedding(WORDS, DEPTH, input_length = LENGTH))\nm1.add(LSTM(DEPTH))\nm1.add(Dense(3, activation = 'softmax'))\nm1.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['acc'])\nm1.summary()","4b6f267b":"# Train the second model and plot the history\nh1 = train_model(m1, x_train, one_hot_labels)\nplot_history(h1)","4cfab646":"from keras.layers import GRU\n\n# Third model: Embedding -> GRU -> Dense classifier\nm2 = Sequential()\nm2.add(Embedding(WORDS, DEPTH, input_length = LENGTH))\nm2.add(GRU(LENGTH))\nm2.add(Dense(3, activation = 'softmax'))\nm2.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['acc'])\nm2.summary()","b87d444c":"# Train the third model and plot the history\nh2 = train_model(m2, x_train, one_hot_labels)\nplot_history(h2)","8ecc73e7":"# Fourth model: Embedding -> GRU with dropouts -> Dense classifier\nm3 = Sequential()\nm3.add(Embedding(WORDS, DEPTH, input_length = LENGTH))\nm3.add(GRU(DEPTH, dropout = 0.2, recurrent_dropout = 0.2))\nm3.add(Dense(3, activation = 'softmax'))\nm3.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['acc'])\nm3.summary()","cdd4739d":"# Train and plot the history\nh3 = train_model(m3, x_train, one_hot_labels)\nplot_history(h3)","22eb8dcf":"# Fifth model: Embedding -> Stack of GRU layers -> Dense classifier\nm4 = Sequential()\nm4.add(Embedding(WORDS, DEPTH, input_length = LENGTH))\nm4.add(GRU(DEPTH, dropout = 0.1, recurrent_dropout = 0.5, return_sequences = True))\nm4.add(GRU(DEPTH, activation = 'relu', dropout = 0.1, recurrent_dropout = 0.5))\nm4.add(Dense(3, activation = 'softmax'))\nm4.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['acc'])\nm4.summary()","d3e16e5a":"# Train and plot the history\nh4 = train_model(m4, x_train, one_hot_labels)\nplot_history(h4)","60fa7979":"from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n\n# Sixth model: Embedding -> Conv1D & MaxPooling1D -> Dense classifier\nm5 = Sequential()\nm5.add(Embedding(WORDS, DEPTH, input_length = LENGTH))\nm5.add(Conv1D(DEPTH, 7, activation = 'relu'))\nm5.add(MaxPooling1D(5))\nm5.add(Conv1D(DEPTH, 7, activation = 'relu'))\nm5.add(GlobalMaxPooling1D())\nm5.add(Dense(3, activation = 'softmax'))\nm5.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['acc'])\nm5.summary()","9564e167":"# Train and plot the history\nh5 = train_model(m5, x_train, one_hot_labels)\nplot_history(h5)","a71f4f19":"# Seventh model: Embedding -> 2 x Conv1D + MaxPooling -> GRU -> Dense\nm6 = Sequential()\nm6.add(Embedding(WORDS, DEPTH, input_length = LENGTH))\nm6.add(Conv1D(DEPTH, 5, activation = 'relu'))\nm6.add(MaxPooling1D(5))\nm6.add(Conv1D(DEPTH, 7, activation = 'relu'))\nm6.add(GRU(DEPTH, dropout = 0.1, recurrent_dropout = 0.5))\nm6.add(Dense(3, activation = 'softmax'))\nm6.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['acc'])\nm6.summary()","50e2b99d":"# Train and plot the history\nh6 = train_model(m6, x_train, one_hot_labels)\nplot_history(h6)","e973b5ac":"# Embedding, LSTM, GRU and Conv1D\nCognitive Systems for Health Technology Applications<br>\n8.3.2019, Sakari Lukkarinen<br>\n[Helsinki Metropolia University of Applied Sciences](https:\/\/www.metropolia.fi\/en)\n","f60dfac8":"## Data preparation","2dca5306":"## Example 5 - Embedding and stack of GRUs","f3158b61":"## Example 2 - Embedding and LSTM","92506a53":"## Example 4 - Embedding and GRU with dropout","0bfbcd7f":"## Example 6 - Embedding and Conv1D","7878347a":"## Check the shapes","77e944ee":"## Convert reviews to padded sequences","6ca11f9d":"## Example 3 - Embedding and GRU","7456aed6":"## Convert labels to one-hot-categories","9e6e4a20":"## Example 1 - Embedding and Flatten","170fc3d3":"## Objectives\nThe aim of this Notebook is to show how to use keras embedding layers together with LSTM, GRU and Conv1D layers to classify review converted to sequences. \n\nThe examples are following the structure given in book: Chollet, Deep Learning with Python, Ch. 6. Deep learning for text and sequences. The examples are not optimized but the aim is to show how to configure the model architectures.","e717366f":"## Helper functions","7da9aeab":"## Example 7 - Embedding and mixed Conv1D and GRU"}}