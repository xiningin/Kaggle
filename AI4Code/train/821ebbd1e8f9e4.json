{"cell_type":{"fd52ff4b":"code","0d02c4e8":"code","cb76632e":"code","adac66b7":"code","f9c3f408":"code","e5f897ed":"code","c231a54a":"code","6eeaf5b8":"code","38441d5e":"code","e952b87e":"code","703283e7":"code","7afb6638":"code","a9dacf05":"code","46201018":"code","bb0ec025":"code","97a0b143":"code","bbeed116":"code","a6d2618f":"code","ae76fa03":"code","64c94f25":"code","e097d460":"code","1f32a0a9":"code","8ea428dc":"code","67d552cc":"code","cfa759a0":"code","aed19ede":"code","4c28f1b9":"code","6e286448":"code","d3e0aead":"code","d1ca77f2":"code","f511c81d":"code","37a6b363":"code","d472fa62":"code","c43cd317":"code","eca89c52":"code","8ac8d2ac":"code","2e58af41":"code","e021748f":"code","960968f8":"code","f18bc4a4":"code","1cd4139c":"code","f3ec0143":"code","2d1bbed4":"code","147b1091":"code","d97e27ed":"code","47802bf3":"code","af3ac8c8":"code","e1c4c487":"code","1ffe0b6f":"code","3ebc9efe":"code","5a59fb9c":"code","07de556b":"code","b7f7dfbe":"markdown","4ce366bb":"markdown","2bc86a97":"markdown","27e724a1":"markdown","83af3f92":"markdown","e13b7bd8":"markdown","09f25c64":"markdown","3911563d":"markdown","7a89ef72":"markdown","8330d41b":"markdown","dbbea007":"markdown","80f85377":"markdown","79cf5bdc":"markdown","f40b4d6b":"markdown","1a20fb3f":"markdown","5ece32de":"markdown","217e2410":"markdown","1ede204d":"markdown","a1d990f8":"markdown","9a62e37a":"markdown","68050360":"markdown","4ad6128e":"markdown","3bdb6e08":"markdown","c1c418c2":"markdown","663c91a3":"markdown","d22e3c2a":"markdown","a51f3db6":"markdown","20e234f5":"markdown","e5476193":"markdown","32cc98b2":"markdown","879dc0d9":"markdown","6dcd0315":"markdown","51820daf":"markdown","98192dc3":"markdown","fc601363":"markdown","f0499e53":"markdown","61a11aea":"markdown","5babd1dc":"markdown","5b9d9a2e":"markdown","04bf0e6b":"markdown","d13ee86a":"markdown","a424ee5a":"markdown","b605f3ce":"markdown","ae58334c":"markdown","33299bbf":"markdown","d4e52ed7":"markdown","7d6e287a":"markdown","d48dbe48":"markdown","1bb7c2c1":"markdown","b2d75f22":"markdown","bfa9a55a":"markdown","289f1af7":"markdown","5a3fa9d6":"markdown","5d5f23ae":"markdown","b806ea59":"markdown","9c8a26cc":"markdown","e629e438":"markdown","f5bfc851":"markdown","ebcd1dc8":"markdown","7717b790":"markdown","0fb07602":"markdown","fd74ea71":"markdown","90234638":"markdown","601a85b2":"markdown","1db2ab1a":"markdown"},"source":{"fd52ff4b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats","0d02c4e8":"data = pd.read_csv('\/kaggle\/input\/pokemon\/Pokemon.csv')","cb76632e":"data","adac66b7":"# Know the size of the data uaing .shape function \nprint('The size of the data is',data.shape[0],'rows and ', data.shape[1],'columns and the shape is ',data.shape)","f9c3f408":"data.columns","e5f897ed":"data.describe()","c231a54a":"data.head()","6eeaf5b8":"data.tail(7)","38441d5e":"# to see a specific row \ndata.iloc[150]","e952b87e":"# examinig punch of rows \ndata.iloc[50:55]","703283e7":"# five random rows from the the data  \ndata.sample(5)","7afb6638":"data['Legendary'].value_counts()","a9dacf05":"result = data['Legendary'].value_counts()\nseries = pd.Series(\n    np.array([result[1],result[0]]),\n    index = ['Legandary', 'NonLegendary']\n                )\nprint(series)","46201018":"# first we know what is the maximum attack value \ndata['Attack'].max()\n","bb0ec025":"# now we query the whole record using the data we had from the previous \ndata.query(\"Attack == 190\")","97a0b143":"# display all the legendary pocemons \ndata.query('Legendary == True')","bbeed116":"print('Maximum value of HP')\nprint(data['HP'].max())\nprint('----------------------')\nprint('Mimimum value of HP')\nprint(data['HP'].min())\n","a6d2618f":"# or u can easily use describe() function that we used before on that specific feature \ndata['Attack'].describe()","ae76fa03":"# We can easily use describe() function that we used before on that specific feature \ndata['Attack'].describe()","64c94f25":"frame = data.groupby('Generation').mean()\nframe","e097d460":"# we will plot the distripution of the attack feature using seaborn library \nsns.distplot(data['Attack']);","1f32a0a9":"# look at the values of skewness and kurtosis of the prevoius graph \nprint(\"Skewness: %f\" % data['Attack'].skew())\nprint(\"Kurtosis: %f\" % data['Attack'].kurt())","8ea428dc":"sns.distplot(data['HP']);","67d552cc":"print(\"Skewness: %f\" % data['HP'].skew())\nprint(\"Kurtosis: %f\" % data['HP'].kurt())","cfa759a0":"var2 = 'Attack'\nvar1 = 'Total'\npd.concat([data[var2], data[var1]], axis=1)\ndata.plot.scatter(x=var1, y=var2, ylim=(0,200));","aed19ede":"var1 = 'Generation'\nvar2 = 'Total'\npd.concat([data[var1], data[var2]], axis=1)\nfig = sns.boxplot(x=var1, y=var2, data=data)","4c28f1b9":"var1 = 'Speed'\nvar2 = 'Total'\npd.concat([data[var1], data[var2]], axis=1)\nfig = sns.boxplot(x=var1, y=var2, data=data)","6e286448":"corrmat = data.corr()\ncorrmat","d3e0aead":"f, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=0.9, square=True);","d1ca77f2":"f, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=0.9, square=True,annot=True);","f511c81d":"sns.set()\ncols = [ 'Total', 'HP', 'Attack', 'Defense','Sp. Atk', 'Sp. Def', 'Speed' ,'Generation']\nsns.pairplot(data[cols], size = 1.5) \nplt.show();","37a6b363":"# see the count of every record in that col \ndata['#'].value_counts()","d472fa62":"# see the count of every record in the name columne \ndata['Name'].value_counts()","c43cd317":"sns.distplot(data['#'])","eca89c52":"var = 'Attack'\nvar0 = '#'\npd.concat([data[var0], data[var]], axis=1)\ndata.plot.scatter(x=var, y=var0, ylim=(0,800),xlim=(0,180));","8ac8d2ac":"var = 'HP'\nvar0 = '#'\npd.concat([data[var0], data[var]], axis=1)\ndata.plot.scatter(x=var, y=var0, ylim=(0,800),xlim=(0,150));","2e58af41":"data = data.drop('#',axis=1)\ndata.head()","e021748f":"data.columns","960968f8":"# see how many null value in every column \nAll = data.isnull().sum().sort_values(ascending=False)\nAll","f18bc4a4":"percentage = (data.isnull().sum()\/data.isnull().count()).sort_values(ascending=False)\npercentage","1cd4139c":"missing_data = pd.concat([All, percentage], axis=1, keys=['All', 'percentage'])\nmissing_data.head(10)","f3ec0143":"New_data = data.drop('Type 2' , axis = 1)\nNew_data.head()","2d1bbed4":"# deleting all the rows with at least one null value \nt1 = data.dropna()\n","147b1091":"All = t1.isnull().sum().sort_values(ascending=False)\nAll","d97e27ed":"#deleting all the rows that has all the values in them null  \nt2 = data.dropna(how = 'all')","47802bf3":"All = t2.isnull().sum().sort_values(ascending=False)\nAll","af3ac8c8":"# filling the missing valuse in type 2 col witth a value of the previous row \ntemp  = data['Type 2'].fillna(method='ffill')\ntemp","e1c4c487":"missing = temp.isnull().sum()\nmissing","1ffe0b6f":"sns.distplot(data['HP']);","3ebc9efe":"sns.distplot(data['Defense'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(data['Defense'], plot=plt)","5a59fb9c":"#applying log transformation\ndata['Defense'] = np.log(data['Defense'])","07de556b":"sns.distplot(data['Defense'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(data['Defense'], plot=plt)","b7f7dfbe":"Now we will see the description of the data , the mean std and count of the columns etc .. ","4ce366bb":"We can see that its fitted in a better way after applying the log transformation","2bc86a97":"Now will see the least 7 rows ","27e724a1":"dropna() function has many other usages but we are not covering them all because they are all alike so quik look at the function documentation will give us every thing","83af3f92":"Dealing with missing data is out of our scope but will take a prief look of what to be done here ","e13b7bd8":"I am curuios to know which among them has the highst attack value ","09f25c64":"Here you can see that the destripution of the data is not that bad and it can really be dealt with this way but will apply the log transformation to normaliaze our data as follows","3911563d":"There is also a lot of other function to deal with missing data \nlike fillna","7a89ef72":"Here we can see that the relation between the generation and the Total score is almost linear and the means nor the out liers changed alot ","8330d41b":"Now will see some statistics about that graph","dbbea007":"As we see no missing values were found in the temp , we can pass a constant also to the method fillna and will but that constant in every null value","80f85377":"Well it turned out that this col has some repititve values but the majority is unique , but that tells us that it is not an index .. maybe it is an index and the records are repeated so lets find out","79cf5bdc":"We can not say that we explored the dataset very well and know everything boutit without lookin at the missing data and the outliers in you dataset","f40b4d6b":"Now we will see some specifics of a specific feature ","1a20fb3f":"**Skewness** is a measure of the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point. so if skewness is zero means that the data is perfectly semetric higher or lower values other than zero means that te data is skewwed right or left respectivly .\n\n**Kurtosis** is a measure of whether the data are heavy tailed or light tailed relative to a normal distribution . \nthe standard normal distribution has a kurtosis of 0 positive kurtosis indicates a \"heavy-tailed\" distribution and negative kurtosis indicates a \"light tailed\" distribution.\nif You still confused you can take a look here it is a 5 minutes read \nhttps:\/\/www.itl.nist.gov\/div898\/handbook\/eda\/section3\/eda35b.htm\n","5ece32de":"**Now we can read your data and see any portion ywe like to inspect and also see some visualization that help us to understad our data in better way**","217e2410":"We can see that 386 missing values in the column Type 2 an nothing else is misssing","1ede204d":"another way is to use interpolate() method and pass the interpolation methed like linear for example and the method interpolates the missing values for us , we can also add limit to our interpolation etc .. for more information -----> https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.16.2\/generated\/pandas.DataFrame.interpolate.html","a1d990f8":"And here you can the percentage of the missing values in each coloumn","9a62e37a":"The aggregate function \"groupby\" enables us to ctegorise your data and deal with it as chunks instead of individual records as follows","68050360":"Remeber the describe function we used earlier ? we can envoke it in a specific column as well ","4ad6128e":"We may also use the function loc instead of iloc to include the last row which in the previous case is 55 ","3bdb6e08":"And that was an over all view that gives you more confedince and makes it more easy for you to see the relationships,\nand in the diagonal we can see a histogram representaion of distripution of that feature ","c1c418c2":"Now we may need to take a look in the middle at a specific row or a group of rows ","663c91a3":"You can aslo add parameter to the head(x) to get the first x rows ","d22e3c2a":"Now if we want to make it in a fancy way we would use series to store the values with indector index ","a51f3db6":"**Normality** - When we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely on this . In this exercise we'll just check univariate normality for 'Defense' , but it helps. Another detail to take into account is that in big samples (>200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems so that's the main reason why we are doing this analysis.\n\n","20e234f5":"Now will see another form of visualizing the relations between the features which is box blot \n\nbriefly  50% of the data will be in the colored box and 25% on the above and 25 are below \n\nfor more explanation about box blots ","e5476193":"Here you can see that most of the vlues are between 0 and 150 but the graph continues to 250 so that could be some kind of outliers","32cc98b2":"The privious plot shows the the relation between the Total which represents the X axis and Attack which represents the Y aixis  \nand we can see that the line x=y will just fit in the middle so its almost a linear relationship with some outliers . outliers ??!! hang on a moment , will discuss it in a bit","879dc0d9":"Ok it is time to look at the actual records of our data set , we will start with the first few rows ","6dcd0315":"No missing data and type 2 col still exists","51820daf":"At the first look the data is huge to examine at once . first we will se the size of the data","98192dc3":"Next we import our data and read it using pandas lib . actually there is a variety of built in functions for reading data in pandas like read_csv() , read_json() , read_xlsx() , and more and u choose what ever u like accordind to the format of ur data set","fc601363":"# Now time to see some Graphs and visualizations","f0499e53":"Now we may need to look at random sample from the data to spot any irruglarity and also to feel more confedint ","61a11aea":"# For starters we will examine our data statistically ","5babd1dc":"Now we can discuss outliers , outliers is the record that does not belong to the crowd , in other words when we see the distripution of a data we can see the outliers at the very left and right side of the grapgh . and it is really dangerous if we did not handle them and deal with the properly for example : if we have a fature Age in you data set and it varies from 5 to 1000 you should but a huge quiestion mark here as it affects the mean and the skewness , etc .. and that will affect the result of any thing you do with your data , as they always say garpage data in ---> garpage result out . we can deal with them by butting a limit for a specific feature like the age for example could be only from 0 to 100 or the gender can only be male or female etc ...","5b9d9a2e":"Visualizing the realtions bettween the features does give us a good intuition about waht is going on between them but we are not yet confedint , we dont have a specific value to rely on . right ? ","04bf0e6b":"You can drop the entire column that has a large amount of missing data","d13ee86a":"We can import our data from our local host or as raw data from a wep page\n","a424ee5a":"I dont know whether u have noticed that or not but that column with label # is looks like its an index that starts with 1 . is it ? lets find out","b605f3ce":"We can see that # does not exist any more","ae58334c":"Now we will take a look at our data ","33299bbf":"Now we have the exact numbers of how the columns are correlated , as we can see in our data its relatively easy to read the above matrix but in real world it can get missey and you will be lost between the numbers so here is a visual aid ","d4e52ed7":"First things first -> missing data","7d6e287a":"We can see that we used query function here wich takes a valid query as input , any valid query ","d48dbe48":"Maybe you dont want to drop the entire column and need only to drop the rows from ur dataFrame","1bb7c2c1":"It did not delete the missing data from type 2 because the entire record is not null","b2d75f22":"\nSo far we saw thae distripution of the features alone next will see the relations between the features","bfa9a55a":"While i was exploring the dataset with you i noticed something wierd !! the column # feels like its here by mistake .. no strong corelation with other feaures , the name itself is not an indecator of any thing . what does it do here ?!!","289f1af7":"Thats is the end of our tutorial. peice","5a3fa9d6":"Yes , thats exactly what we are looking for both statistical and visual indecation at the same outbut","5d5f23ae":"OK , we will take a look at these columns ","b806ea59":"Here also you can see the aproximate linear relation between the speed and the total and the average speed per total alos encoded on the graph represented by the box plot ","9c8a26cc":"# Breif Intro to missing data and outliers","e629e438":"So as it turned out that specific feature does not have any relationshib between the other ones as the plots shows random noise , almost constant ngeligable correlation between other features , so we will delete that column and will continue our exploration","f5bfc851":"Good , intuitive but again no numbers !!","ebcd1dc8":"# Extra knowledge","7717b790":"First things first what is data explorationn and why is it important ? \ndata exploration is the precoss of reviewing the data to uncover initial patterns, characteristics and points of interest . and it is important to help us to know our data , understand it and make some assumptions and maybe test these assumptions . further more it helps to reveal some fatal errors , or in another words data corruption . like huge amount of missing data or unrealstic records or irrelevant data feaures etc .. and with some good data exploration and data cleaning , we are almost done with machine learning task unless you are willing to implement the model from the scratch . this will be a different story . now we only are intersted in data exploration and data cleaning will be out of our scope but we will beak and have a taste about what data cleaning is . enough talking and lets start .\n","0fb07602":"Before we jump into coding we will see a list of the content covered here . \n* Exploring the data in a statistical way \n* Seeing some data visulization and graphs \n* Inroducing missing data \n* Optional section ","fd74ea71":"So at the beginig we will import the important libraries that we are going to need throw our tutorial ","90234638":"***Data Exploration***","601a85b2":"I am curious to know how many pocemon is legandary ! what about you ? ","1db2ab1a":"There is no repetetive names , # is not an index maybe it has smth to do with the other columns , has realtions with them or tells us any extra information ?? will find that out in a bit"}}