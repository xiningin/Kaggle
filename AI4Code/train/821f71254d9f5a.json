{"cell_type":{"b2b78794":"code","c16eadfc":"code","cf163806":"code","cb974025":"code","879c1f10":"code","8ff6d75c":"code","3d2d5598":"code","9105b195":"code","59700831":"code","bc524f38":"code","252e3ecc":"code","561afefb":"code","7a94ba92":"code","5a8db983":"code","b8d4c068":"code","ae0fd73b":"code","711ba1d2":"code","52fee03e":"code","90e651b5":"code","8a291ea3":"code","c41160a0":"code","fb27ee2b":"code","525e0045":"code","e884362a":"code","5046d28c":"code","03a2d2a1":"code","49bc3201":"code","07db1dab":"code","519a5a60":"code","50c05d1a":"code","c209f89d":"code","653e2406":"code","e85b96e5":"code","b495319d":"code","88727079":"code","10d32978":"code","6eae0e2b":"code","bc869165":"code","62a32327":"code","bb459eaa":"code","d2b41be1":"code","37452f80":"markdown","b4b69940":"markdown","b50a5d1e":"markdown","86315096":"markdown","552e8e37":"markdown","0a6ad951":"markdown","50a946e4":"markdown","28cfe608":"markdown","b712801d":"markdown","74341e40":"markdown","5bcd4382":"markdown","e3dd8a23":"markdown","72b8c3f8":"markdown","83285f8d":"markdown","a342b01b":"markdown","74812435":"markdown","f2b41cf9":"markdown","71168fc2":"markdown"},"source":{"b2b78794":"#!pip -q install http:\/\/download.pytorch.org\/whl\/cu92\/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n#!pip -q install torchvision","c16eadfc":"## https:\/\/stackoverflow.com\/questions\/49853303\/how-to-install-pydot-graphviz-on-google-colab?rq=1\n#!pip -q install graphviz \n#!apt-get install graphviz -qq\n#!pip -q install pydot","cf163806":"#!pip -q install \"dask[complete]\"","cb974025":"## https:\/\/medium.com\/@iphoenix179\/running-cuda-c-c-in-jupyter-or-how-to-run-nvcc-in-google-colab-663d33f53772\n## https:\/\/developer.nvidia.com\/cuda-90-download-archive?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1704&target_type=deblocal\n#!apt update -qq;\n#!wget https:\/\/developer.nvidia.com\/compute\/cuda\/9.0\/Prod\/local_installers\/cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb\n#!mv cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64.deb  \n#!dpkg -i cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64.deb\n#!apt-key add \/var\/cuda-repo-9-0-local\/7fa2af80.pub\n#!apt-get update -qq;\n#!apt-get install cuda gcc-5 g++-5 -y -qq;\n#!ln -s \/usr\/bin\/gcc-5 \/usr\/local\/cuda\/bin\/gcc;\n#!ln -s \/usr\/bin\/g++-5 \/usr\/local\/cuda\/bin\/g++;\n### !apt install cuda-9.2;","879c1f10":"## http:\/\/alisonrowland.com\/articles\/installing-pycuda-via-pip\n## https:\/\/codeyarns.com\/2015\/07\/31\/pip-install-error-with-pycuda\/\n#import os\n#PATH = os.environ[\"PATH\"]\n#os.environ[\"PATH\"] = \"\/usr\/local\/cuda-9.2\/bin:\/usr\/local\/cuda\/bin:\" + PATH\n#os.environ[\"LD_LIBRARY_PATH\"] = \"\/usr\/local\/cuda\/lib64\"\n#os.environ[\"CUDA_ROOT\"] = \"\/usr\/local\/cuda\/\"","8ff6d75c":"#!pip -q install --ignore-installed pycuda","3d2d5598":"import numpy as np\nimport pandas as pd\nfrom multiprocessing import Pool, Process\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport gc\ngc.enable()","9105b195":"import multiprocessing as mp\nmp.cpu_count()","59700831":"def function(lst):\n    arr = np.zeros_like(lst)\n    for i in range(lst.shape[0]):\n        for j in range(lst.shape[1]):\n            arr[i][j] = lst[i][j] ** 2\n    return arr\n\narray = np.random.randint(1, 9, (2**10, 10000))\ndata = np.array_split(array, 2)","bc524f38":"data[0].shape","252e3ecc":"%%time\nwith Pool(2) as p: # Only 2 processes will be called at a time, others will be in a queue.\n    res = p.map(function, data)\n    p.close()\n    p.join()","561afefb":"%%time\nprocesses = []\nfor i in range(2): # You can make as many processes as you like, they all will be working together until finished.\n    p = Process(target=function, args=(data[i],))\n    processes.append(p)\n    p.start()\n  \nfor p in processes: p.join()","7a94ba92":"from sklearn.datasets import make_regression\nX, y = make_regression()\n\ndf = pd.DataFrame(X)\ndf.head()","5a8db983":"dfs = [df.iloc[i*25:i*25+25, 0] for i in range(4)]\nwith Pool(4) as p:\n    res = p.map(np.exp, dfs)\nfor i in range(4): df.iloc[i*25:i*25+25, 0] = res[i]\ndf.head()","b8d4c068":"def function(lst):\n    arr = np.zeros_like(lst)\n    for i in range(lst.shape[0]):\n        arr[i] = lst[i] ** 2\n    return arr","ae0fd73b":"import time\n\ndef serial(n):\n    times = []\n    size = []\n    for i in range(n):\n        s = 10**(i+1)\n        size.append(s)\n        lst = np.random.randint(1, 7, (s,))\n        st = time.time()\n        res = function(lst)\n        en = time.time()\n        times.append(en-st)\n    return times, size\n\ndef parallel(n):\n    times = []\n    size = []\n    for i in range(n):\n        s = 10**(i+1)\n        size.append(s)\n        lst = np.random.randint(1, 7, (s,))\n        splitted = np.split(lst, 2)\n        with Pool(2) as p:\n            st = time.time()\n            res = p.map(function, splitted)\n            en = time.time()\n        times.append(en-st)\n    return times, size\n\ndef parallel2(n):\n    times = []\n    size = []\n    for i in range(n):\n        s = 10**(i+1)\n        size.append(s)\n        lst = np.random.randint(1, 7, (s,))\n        splitted = np.split(lst, 2)\n        processes = []\n        for i in range(2):\n            p = Process(target=function, args=(splitted[i],))\n            processes.append(p)\n        st = time.time()\n        for p in processes: p.start()\n        for p in processes: p.join()\n        en = time.time()\n        times.append(en-st)\n    return times, size","711ba1d2":"t1, s1 = serial(7)","52fee03e":"t2, s2 = parallel(7)","90e651b5":"t3, s3 = parallel2(7)","8a291ea3":"plt.plot(s1, t1, \"o-\", label=\"Serial\")\nplt.plot(s2, t2, \"o-\", label=\"Pool\")\nplt.plot(s3, t3, \"o-\", label=\"Process\")\nplt.legend()\nplt.xlabel(\"Number of elements:\")\nplt.ylabel(\"Time (sec):\")\nplt.show()\n# Our task is not that complex, results here may vary unexpectedly.","c41160a0":"from threading import Thread as trd\nimport queue\nq = queue.Queue()","fb27ee2b":"def function(lst):\n    arr = np.zeros_like(lst)\n    for i in range(lst.shape[0]):\n        for j in range(lst.shape[0]):\n            arr[i][j] = lst[i][j] * lst[i][j]\n    return arr\n\narray = np.random.randint(1, 10, (1000, 10000))\ndata = np.array_split(array, 2)","525e0045":"%%time\nres = function(array)","e884362a":"%%time\n# By using Queue this way you can get result of function without\n# modifying your function.\nt1 = trd(target=lambda q, args1: q.put(function(args1)), args=(q, data[0]))\nt2 = trd(target=lambda q, args1: q.put(function(args1)), args=(q, data[1]))\n\nt1.start()\nt2.start()\n\nt1.join()\nt2.join()\n\nres1 = q.get()\nres2 = q.get()","5046d28c":"q.empty()","03a2d2a1":"from dask import delayed as delay\n\n@delay\ndef add(x, y): return x+y\n@delay\ndef sq(x): return x**2\n@delay\ndef sum(x): \n    sum=0\n    for i in range(len(x)): sum+=x[i]\n    return sum","49bc3201":"inputs = list(np.arange(1, 11))\n\nres = [sq(n) for n in inputs]\nres = [add(n, m) for n, m in zip(res[::2], res[1::2])]\nres = sum(res)","07db1dab":"res.visualize()","519a5a60":"res.compute()","50c05d1a":"import torch.multiprocessing as mp_\nmp = mp_.get_context('spawn')","c209f89d":"a = torch.zeros((1000, 1000))\nb = torch.zeros_like(a).cuda()","653e2406":"def func(arr):\n    for i in range(arr.shape[0]):\n        for j in range(arr.shape[1]):\n            arr[i][j] += (i+j)\n            arr[i][j] *= arr[i][j]\n    return arr","e85b96e5":"%%time\nres = func(a)","b495319d":"%%time\nres = func(b)","88727079":"import torch.multiprocessing as mp_\nmp = mp_.get_context('spawn')","10d32978":"from sklearn.datasets import make_classification\nfrom torch.utils.data import DataLoader, TensorDataset\nX, y = make_classification(n_samples=100000, )\n\ndataset = TensorDataset(torch.FloatTensor(X), torch.DoubleTensor(y))\ndata_loader = DataLoader(dataset, batch_size=8)","6eae0e2b":"n_in = 20; n_out = 1        \n        \nmodel = nn.Sequential(nn.Linear(n_in, 15),\n                      nn.ReLU(),\n                      nn.Linear(15, 10),\n                      nn.ReLU(),\n                      nn.Linear(10, 5),\n                      nn.ReLU(),\n                      nn.Linear(5, n_out),\n                      nn.Sigmoid())\n\nmodel.share_memory() # Required for 'fork' method to work","bc869165":"optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, nesterov=True)\nloss_fun = torch.nn.BCELoss()","62a32327":"import torch.nn as nn\ndef train(model):\n    for data, labels in data_loader:\n        optimizer.zero_grad()\n        loss_fn(model(data), labels).backward()\n        optimizer.step()  # This will update the shared parameters","bb459eaa":"processes = []\nfor i in range(4): # No. of processes\n    p = mp.Process(target=train, args=(model,))\n    p.start()\n    processes.append(p)\nfor p in processes: p.join()","d2b41be1":"sum=0\nfor data, labels in data_loader:\n    with torch.no_grad():\n        res = model(data)\n        res[res>=0.7] = 1\n        res[res<0.7] = 0\n        sum += (res.numpy()!=labels.float().numpy()).diagonal().sum()\n    \nsum\/100000","37452f80":"### Pycuda:","b4b69940":"# Import","b50a5d1e":"# Initial ","86315096":"This goes along with my Medium post. All posts are here:\n1. [Speed Up your Algorithms Part 1\u200a-\u200aPyTorch](https:\/\/medium.com\/r\/?url=https%3A%2F%2Ftowardsdatascience.com%2Fspeed-up-your-algorithms-part-1-pytorch-56d8a4ae7051)\n1. [Speed Up your Algorithms Part 2\u200a-\u200aNumba](https:\/\/medium.com\/r\/?url=https%3A%2F%2Ftowardsdatascience.com%2Fspeed-up-your-algorithms-part-2-numba-293e554c5cc1)\n1. [Speed Up your Algorithms Part 3\u200a-\u200aParallelization](https:\/\/towardsdatascience.com\/speed-up-your-algorithms-part-3-parallelization-4d95c0888748)\n1. [Speed Up your Algorithms Part 4\u200a-\u200aDask](https:\/\/towardsdatascience.com\/speeding-up-your-algorithms-part-4-dask-7c6ed79994ef)\n\n## Index\n\n\n1. [Pool and Process](#poolpr)\n1. [Threading](#thread)\n1. [Dask](#dask)\n1. [torch.multiprocessing](#torch)\n1. [Pycuda](#pycuda)","552e8e37":"### Training Model using multiple processes:","0a6ad951":"# 2. Threading <a id=\"thread\"><\/a>","50a946e4":"### Process:","28cfe608":"### Pool:","b712801d":"### Dask:","74341e40":"# 3. Dask <a id=\"dask\"> <\/a>","5bcd4382":"### What can we do with a DataFrame with Processes?","e3dd8a23":"`Dask` is a parallel processing library for Data Scientists and Machine Learning Enthusiasts to get most out of their devices. For full intro to Dask look here.","72b8c3f8":"# 4. torch.multiprocessing <a id=\"torch\"><\/a>","83285f8d":"# 5. Pycuda (Optional) <a id=\"pycuda\"><\/a>\n\nPycuda's `Source Module` is not working here. To check out basic kernel calling of Pycuda, go [here](https:\/\/nbviewer.jupyter.org\/github\/PuneetGrov3r\/MediumPosts\/blob\/master\/SpeedUpYourAlgorithms\/3%29%20Prallelization.ipynb#5.-Pycuda-(Optional)).","a342b01b":"# 1. Pool and Process <a id=\"poolpr\"> <\/a>","74812435":"### Torch:","f2b41cf9":"### Graphing:","71168fc2":"So if you have some parallelizable function to be applied to this DataFrame, we can split the DataFrame into pieces and apply that fuction to those pieces separately in parallel."}}