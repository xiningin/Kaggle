{"cell_type":{"7fc4a0b3":"code","ca226ea2":"code","ed6ca775":"code","fc3a4346":"code","be461f41":"code","2f573ee3":"code","80c67ead":"code","251ef7f9":"code","3beb7525":"code","6e16a25f":"code","859f5d22":"code","0ca71e7d":"markdown","69f1ccd5":"markdown","97ae2fbb":"markdown","94adc0a2":"markdown","dba83a23":"markdown","aabbdfe5":"markdown","b9e91479":"markdown","0d4c7bf4":"markdown","85dde897":"markdown","ed7e0635":"markdown"},"source":{"7fc4a0b3":"!pip install googletrans","ca226ea2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport json\nimport os\nimport torch\nfrom tqdm import tqdm,tqdm_notebook\nfrom IPython.core.display import display, HTML\nfrom googletrans import Translator\nimport gc\n# Input data files are available in the \"..\/input\/\" directory.\n#####################################################################################\n#thanks for your work vasuji https:\/\/www.kaggle.com\/vasuji\/i-covid19-nlp-data-parsing\n#####################################################################################\n\ndatafiles = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        ifile = os.path.join(dirname, filename)\n        if ifile.split(\".\")[-1] == \"json\":\n            datafiles.append(ifile)","ed6ca775":"id2abstract = []\nfor file in datafiles[:10000]:\n    with open(file,'r')as f:\n        doc = json.load(f)\n    id = doc['paper_id'] \n    abstract = ''\n    try:\n        for item in doc['abstract']:\n            abstract = abstract + item['text']\n            \n        id2abstract.append({id:abstract})\n    except KeyError:\n        None\n    \nprint (\"finish load all paper's abstract\")    ","fc3a4346":"from transformers import BertForQuestionAnswering\nfrom transformers import BertTokenizer\nimport torch\ndevice = torch.device(\"cuda\")\nmodel_QA = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\ntokenizer_QA = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\nmodel_QA = model_QA.to(device)\nmodel_QA = model_QA.eval()\n\nfrom transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n# see ``examples\/summarization\/bart\/evaluate_cnn.py`` for a longer example\nmodel = BartForConditionalGeneration.from_pretrained('bart-large-cnn')\ntokenizer = BartTokenizer.from_pretrained('bart-large-cnn')","be461f41":"def reconstructText(tokens, start=0, stop=-1):\n    tokens = tokens[start: stop]\n    if '[SEP]' in tokens:\n        sepind = tokens.index('[SEP]')\n        tokens = tokens[sepind+1:]\n    txt = ' '.join(tokens)\n    txt = txt.replace(' ##', '')\n    txt = txt.replace('##', '')\n    txt = txt.strip()\n    txt = \" \".join(txt.split())\n    txt = txt.replace(' .', '.')\n    txt = txt.replace('( ', '(')\n    txt = txt.replace(' )', ')')\n    txt = txt.replace(' - ', '-')\n    txt_list = txt.split(' , ')\n    txt = ''\n    nTxtL = len(txt_list)\n    if nTxtL == 1:\n        return txt_list[0]\n    newList =[]\n    for i,t in enumerate(txt_list):\n        if i < nTxtL -1:\n            if t[-1].isdigit() and txt_list[i+1][0].isdigit():\n                newList += [t,',']\n            else:\n                newList += [t, ', ']\n        else:\n            newList += [t]\n    return ''.join(newList)\n\ndef extract_sentence(abstract,answer):\n    abstract = abstract.lower()\n    answer = answer.lower()\n    split_byans = abstract.split(answer)\n    if len(split_byans)==1:\n        return split_byans[0][split_byans[0].rfind(\". \")+1:]+\" \"+answer\n    else: \n        return split_byans[0][split_byans[0].rfind(\". \")+1:]+\" \"+answer+split_byans[1][:split_byans[1].find(\". \")+1]\n    \ndef Add_contaner(text,index,query,Summary_text,Summary_text_TW,Summary_text_CN,Summary_text_JP,Summary_text_KO,ans_pd):\n    text = text.replace(\"demo\",\"demo\"+index).replace(\"en_s\",\"en_s\"+index).replace(\"tw_s\",\"tw_s\"+index).replace(\"cn_s\",\"cn_s\"+index).replace(\"jp_s\",\"jp_s\"+index).replace(\"ko_s\",\"ko_s\"+index).replace(\"topextract\",\"topextract\"+index)\n    text = text.format(query,Summary_text,Summary_text_TW,Summary_text_CN,Summary_text_JP,Summary_text_KO,ans_pd)\n    return text\n","2f573ee3":"def Summary_Model(pd,count,model):\n    ### Top confident answer to article\n    total_abstract=''\n    for i in range(len(pd[:count])):\n        abss, ans = pd.loc[i,['abstract_by_ans','Answer']].values\n        total_abstract+=(abss+\".\")\n    ARTICLE_TO_SUMMARIZE = total_abstract\n\n    ### Token the article, if larger than 1024, then split the article\n    tokens= tokenizer.tokenize(ARTICLE_TO_SUMMARIZE)\n    max_seq_length = 1024\n    longer = 0\n    all_tokens=[]\n    if len(tokens)>1024:\n        for i in range(0,len(tokens),max_seq_length):\n            tokens_a = tokens[i:i+max_seq_length]\n            one_token = tokenizer.batch_encode_plus([tokens_a], max_length=1024, return_tensors='pt')\n            all_tokens.append(one_token)\n    \n    \n    Summary_text = []\n    \n    ## decode the model output as summary text\n    def decode_text(sum_ids):\n        text =''\n        for g in sum_ids:\n            text = text+tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n        return text\n    \n    ## Summary model\n    model = model.to(device)\n    model.eval()\n    Summary_text = ''\n    for inputs in all_tokens:\n        summary_ids = model.generate(inputs['input_ids'].to(device), num_beams=2, max_length=1000, early_stopping=True)\n        Summary_text = Summary_text+\" \"+decode_text(summary_ids) \n    \n    if Summary_text == '':\n        Summary_text = \"Can't find summary of answer\"\n    #print (Summary_text)\n    ## Translate to zh-TW, zh-CN, JP, KO\n    translator = Translator()\n    Summary_text_TW = translator.translate(Summary_text,dest='zh-tw').text\n    Summary_text_CN = translator.translate(Summary_text,dest='zh-cn').text\n    Summary_text_JP = translator.translate(Summary_text,dest='ja').text\n    Summary_text_KO = translator.translate(Summary_text,dest='ko').text\n    return Summary_text,Summary_text_TW,Summary_text_CN,Summary_text_JP,Summary_text_KO","80c67ead":"def ANS_Model(ques,count):\n    question = ques\n    answer_all=[]\n    cons_all = []\n    ID_list = []\n    abstract_list = []\n    for i in range(len(id2abstract[:count])):\n        document = list(id2abstract[i].values())[0]\n        ID = list(id2abstract[i].keys())[0]\n        nWords = len(document.split()) ## check how many tokens\n        input_ids_all = tokenizer_QA.encode(question, document) ## Encode the document to ids\n        tokens_all = tokenizer_QA.convert_ids_to_tokens(input_ids_all)\n        if len(input_ids_all) > 512:\n            nFirstPiece = int(np.ceil(nWords\/2))\n            docSplit = document.split()\n            docPieces = [' '.join(docSplit[:nFirstPiece]), ' '.join(docSplit[nFirstPiece:])]\n            input_ids = [tokenizer_QA.encode(question, dp) for dp in docPieces]\n        else:\n            input_ids = [input_ids_all]   \n\n        answers = []\n        cons = []\n        for iptIds in input_ids:\n            tokens = tokenizer_QA.convert_ids_to_tokens(iptIds)\n            sep_index = iptIds.index(tokenizer_QA.sep_token_id)\n            num_seg_a = sep_index + 1\n            num_seg_b = len(iptIds) - num_seg_a\n            segment_ids = [0]*num_seg_a + [1]*num_seg_b\n            assert len(segment_ids) == len(iptIds)\n            number_ids = len(segment_ids)\n\n            if number_ids < 512:\n                start_scores, end_scores = model_QA(torch.tensor([iptIds]).to(device), \n                                         token_type_ids=torch.tensor([segment_ids]).to(device))\n            else:\n                start_scores, end_scores = model_QA(torch.tensor([iptIds[:512]]).to(device), \n                                         token_type_ids=torch.tensor([segment_ids[:512]]).to(device))\n            \n            start_scores = start_scores[:,1:-1]\n            end_scores = end_scores[:,1:-1]\n            answer_start = torch.argmax(start_scores)\n            answer_end = torch.argmax(end_scores)\n            answer = reconstructText(tokens, answer_start, answer_end+2)\n\n            if answer.startswith('. ') or answer.startswith(', '):\n                answer = answer[2:]\n\n            c = start_scores[0,answer_start].item()+end_scores[0,answer_end].item()\n            answers.append(answer)\n            cons.append(c)\n\n        maxC = max(cons)\n        iMaxC = [i for i, j in enumerate(cons) if j == maxC][0]\n        confidence = cons[iMaxC]\n        answer = answers[iMaxC]\n\n        sep_index = tokens_all.index('[SEP]')\n        full_txt_tokens = tokens_all[sep_index+1:]\n\n        abs_returned = reconstructText(full_txt_tokens)\n        if answer!=\"\":\n            answer_all.append(answer)\n            cons_all.append(confidence)\n            ID_list.append(ID)\n            abstract_list.append(document)\n            \n    ans_pd = pd.DataFrame({\"PaperID\":ID_list,'abstract':abstract_list,\"Answer\":answer_all,\"Confident\":cons_all})\n    gc.collect()\n    extrac_list = []\n    for i in range(len(ans_pd)):\n        abss, ans = ans_pd.loc[i,['abstract','Answer']].values\n        extract_sen = extract_sentence(abss,ans)\n        extrac_list.append(extract_sen)\n    ans_pd['abstract_by_ans'] = extrac_list\n    ans_pd = ans_pd.sort_values(by=['Confident'],ascending=False).reset_index(drop=True)\n    Summary_text_EN,Summary_text_TW,Summary_text_CN,Summary_text_JP,Summary_text_KO = Summary_Model(ans_pd,100,model)\n    gc.collect()\n    return ans_pd,Summary_text_EN,Summary_text_TW,Summary_text_CN,Summary_text_JP,Summary_text_KO","251ef7f9":"from tqdm import tqdm_notebook\nall_tasks_count = 0 \ndef topic_all_display(topic, ques, count_papers):\n    global all_tasks_count\n    HTML_Question= \"\"\"\n    <div>\n      <button type=\"button\" class=\"btn btn-info\" data-toggle=\"collapse\" data-target=\"#demo\">{}<\/button>\n      <div id=\"demo\" class=\"collapse\">\n        <nav class=\"navbar navbar-light\" style=\"background-color: #e3f2fd;width: 350px\">\n            <div>\n                <ul class=\"nav navbar-nav nav-tabs\">\n                    <li class=\"active\"><a data-toggle=\"tab\" href=\"#en_s\">EN<\/a><\/li>\n                    <li><a data-toggle=\"tab\" href=\"#tw_s\">ZH-TW<\/a><\/li>\n                    <li><a data-toggle=\"tab\" href=\"#cn_s\">ZH-CN<\/a><\/li>\n                    <li><a data-toggle=\"tab\" href=\"#jp_s\">JP<\/a><\/li>\n                    <li><a data-toggle=\"tab\" href=\"#ko_s\">KO<\/a><\/li>\n                <\/ul>\n            <\/div>\n        <\/nav>\n        <h1>Summary : <\/h1>\n        <div class=\"tab-content\">\n            <div id=\"en_s\" class=\"tab-pane fade in active\">\n                <p>{}<\/p>\n            <\/div>\n            <div id=\"tw_s\" class=\"tab-pane fade\">\n                <p>{}<\/p>\n            <\/div>\n            <div id=\"cn_s\" class=\"tab-pane fade\">\n                <p>{}<\/p>\n            <\/div>\n            <div id=\"jp_s\" class=\"tab-pane fade\">\n                <p>{}<\/p>\n            <\/div>\n            <div id=\"ko_s\" class=\"tab-pane fade\">\n                <p>{}<\/p>\n            <\/div>\n        <\/div>\n        <br>\n        <button type=\"button\" class=\"btn-warning\" data-toggle=\"collapse\" data-target=\"#topextract\">Top Condfident Papers<\/button>\n        <div id=\"topextract\" class=\"collapse\">\n        {}\n        <\/div>\n      <\/div>\n    <\/div>\"\"\"\n    \n\n    topic_header = \"\"\"\n    <div>\n      <button type=\"button\" class=\"btn\" data-toggle=\"collapse\" data-target=\"#topic_id\" style=\"font-size:20px\">&#8226 topic<\/button>\n      <div id=\"topic_id\" class=\"collapse\">\n      {}\n      <\/div>\n    <\/div>\"\"\"\n\n    topic_result = \"\"\n    for index,i in enumerate(ques):\n        question = i\n        ans_pd,Summary_text_EN,Summary_text_TW,Summary_text_CN,Summary_text_JP,Summary_text_KO = ANS_Model(question,len(id2abstract[:count_papers]))\n        ans_pd_html = ans_pd.drop(['abstract'],axis = 1)[:10].to_html(render_links=True, escape=False)\n        topic_result = topic_result+Add_contaner(HTML_Question,str(all_tasks_count),i,Summary_text_EN,Summary_text_TW,Summary_text_CN,Summary_text_JP,Summary_text_KO,ans_pd_html)\n        all_tasks_count+=1\n        \n    topic_header = topic_header.replace(\"topic_id\",topic.replace(\" \",\"\").replace(\"?\",\"\")).replace(\",\",\"\").replace(\"topic\",topic).format(topic_result)\n    #HTML_String = HTML_Header+topic_header\n    #display(HTML(HTML_String))\n    return topic_header","3beb7525":"task_dic={}\n\nall_tasks=[\n    'What is known about transmission, incubation, and environmental stability?',\n    'What do we know about COVID-19 risk factors?',\n    'What do we know about virus genetics, origin, and evolution?',\n    'What do we know about vaccines and therapeutics?',\n    'What has been published about medical care?',\n    'What do we know about non-pharmaceutical interventions?',\n    'What do we know about diagnostics and surveillance?',\n    'What has been published about ethical and social science considerations?',\n    'What has been published about information sharing and inter-sectoral collaboration?'\n]\n\n#1 task\nquestion_list = []\nquestion_list.append(\"What is known about transmission, incubation, and environmental stability for the 2019-nCoV\")\nquestion_list.append(\"What do we know about natural history, transmission, and diagnostics for the 2019-nCoV\")\nquestion_list.append(\"What have we learned about infection prevention and control for the 2019-nCoV\")\nquestion_list.append(\"What is the range of incubation periods for the 2019-nCoV in humans\")\nquestion_list.append(\"How does temperature and humidity affect the tramsmission of 2019-nCoV\")\nquestion_list.append(\"How long can 2019-nCoV remain viable on inanimate, environmental, or common surfaces\")\nquestion_list.append(\"What types of inanimate or environmental surfaces affect transmission, survival, or  inactivation of 2019-nCov\")\nquestion_list.append(\"What is tools and studies to monitor phenotypic change and potential adaptation of the virus\")\ntask_dic[all_tasks[0]] = question_list\n\n\n#2 task\nquestion_list = []\nquestion_list.append(\"What risk factors contribute to the severity of 2019-nCoV\")\nquestion_list.append(\"How does hypertension affect patients\")\nquestion_list.append(\"How does heart disease affect patients\")\nquestion_list.append(\"How does copd affect patients\")\nquestion_list.append(\"How does smoking affect 2019-nCoV patients\")\nquestion_list.append(\"How does pregnancy affect patients\")\nquestion_list.append(\"What are the case fatality rates for 2019-nCoV patients\")\nquestion_list.append(\"What is the case fatality rate in Italy\")\nquestion_list.append(\"What public health policies prevent or control the spread of 2019-nCoV\")\ntask_dic[all_tasks[1]] = question_list\n\n#3 task\nquestion_list = []\nquestion_list.append(\"Can animals transmit 2019-nCoV\")\nquestion_list.append(\"What do we know about the virus origin and management measures at the human-animal interface\")\nquestion_list.append(\"What animal did 2019-nCoV come from\")\nquestion_list.append(\"What real-time genomic tracking tools exist\")\nquestion_list.append(\"What regional genetic variations (mutations) exist\")\nquestion_list.append(\"What effors are being done in asia to prevent further outbreaks\")\ntask_dic[all_tasks[2]] = question_list\n\n#4 task\nquestion_list = []\nquestion_list.append(\"What do we know about vaccines and therapeutics\")\nquestion_list.append(\"What has been published concerning research and development and evaluation efforts of vaccines and therapeutics\")\nquestion_list.append(\"What drugs or therapies are being investigated\")\nquestion_list.append(\"What clinical trials for hydroxychloroquine have been completed\")\nquestion_list.append(\"What antiviral drug clinical trials have been completed\")\nquestion_list.append(\"Are anti-inflammatory drugs recommended\")\ntask_dic[all_tasks[3]] = question_list\n\n\n#5 task\nquestion_list = []\nquestion_list.append(\"What has been published about medical care for 2019-nCoV\")\nquestion_list.append(\"What has been published concerning surge capacity and nursing homes for 2019-nCoV\")\nquestion_list.append(\"What has been published concerning efforts to inform allocation of scarce resources for 2019-nCoV\")\nquestion_list.append(\"What do we know about the clinical characterization and management of the 2019-nCoV\")\nquestion_list.append(\"How does extracorporeal membrane oxygenation affect 2019-nCoV patients\")\nquestion_list.append(\"What telemedicine and cybercare methods are most effective\")\nquestion_list.append(\"How is artificial intelligence being used in real time health delivery\")\nquestion_list.append(\"What adjunctive or supportive methods can help patients\")\ntask_dic[all_tasks[4]] = question_list\n\n#6 task\nquestion_list = []\nquestion_list.append(\"Which non-pharmaceutical interventions limit tramsission\")\nquestion_list.append(\"What are most important barriers to compliance\")\ntask_dic[all_tasks[5]] = question_list\n\n#7 task\nquestion_list = []\nquestion_list.append(\"What diagnostic tests (tools) exist or are being developed to detect 2019-nCoV\")\nquestion_list.append(\"What is being done to increase testing capacity or throughput\")\nquestion_list.append(\"What point of care tests are exist or are being developed\")\nquestion_list.append(\"What is the minimum viral load for detection\")\nquestion_list.append(\"What markers are used to detect or track COVID-19\")\ntask_dic[all_tasks[6]] = question_list\n\n\n#8 task\nquestion_list = []\nquestion_list.append('What collaborations are happening within the research community')\ntask_dic[all_tasks[7]] = question_list\n\n#9 task\nquestion_list = []\nquestion_list.append(\"What are the major ethical issues related pandemic outbreaks\")\nquestion_list.append(\"How do pandemics affect the physical and\/or psychological health of doctors and nurses\")\nquestion_list.append(\"What strategies can help doctors and nurses cope with stress in a pandemic\")\nquestion_list.append(\"What factors contribute to rumors and misinformation\")\ntask_dic[all_tasks[8]] = question_list\n","6e16a25f":"#ques = [\"What do we know about COVID19 risk factors\",\"What is COVID19\"]\n#ans_pd,Summary_text_EN,Summary_text_TW,Summary_text_CN,Summary_text_JP,Summary_text_KO = ANS_Model(ques,len(id2abstract[:100]))","859f5d22":"HTML_Header=\"\"\"\n    <!DOCTYPE html>\n    <html>\n    <head>\n      <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n      <link rel=\"stylesheet\" href=\"https:\/\/maxcdn.bootstrapcdn.com\/bootstrap\/3.4.1\/css\/bootstrap.min.css\">\n      <script src=\"https:\/\/ajax.googleapis.com\/ajax\/libs\/jquery\/3.4.1\/jquery.min.js\"><\/script>\n      <script src=\"https:\/\/maxcdn.bootstrapcdn.com\/bootstrap\/3.4.1\/js\/bootstrap.min.js\"><\/script>\n    <\/head>\n    <body> \"\"\"\n\ntopic_body = \"\"\nfor i in task_dic:\n    task = i\n    question_list = task_dic[task]\n    topic_body += topic_all_display(task,question_list,3000) # Only read 3000 papers to in the kaggle run-time \nAll_HTML = HTML_Header + topic_body\ndisplay(HTML(All_HTML))","0ca71e7d":"# COVID-19 : Find all task answer by reading all Papers (Multiple Language)\n## Technique used\n* Bert QA Model (Pretrained by SQuAD dataset)\n* BART summary Model\n* Python Google translate package\n* HTML for visualize result\n\n## All Flow\n1. Using QA Model, read all paper's abastract than find answer for all tasks\n2. Concatenate Top 50 confident answers to be article, and using Summary model to write summary of answers\n3. Translate multiple language by google translate\n4. Write HTML to show summary of all'papers answer for all tasks.\n\n## Display iteraction\n![image](https:\/\/i.ibb.co\/9hQBVsy\/ezgif-4-ab3acd981194.gif)","69f1ccd5":"## Display all Answer Summary (Multiple Language)\n* Display EN, zh-TW, zh-CN, JP, KO language","97ae2fbb":"## Define all task \n* https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/tasks\n* Ref : https:\/\/www.kaggle.com\/dirktheeng\/anserini-bert-squad-for-semantic-corpus-search","94adc0a2":"## Load Paper's Abstract \n* Only load 10000 paper for reduce memory and run-time","dba83a23":"## Summary Model function","aabbdfe5":"## Load QA & Summary model\n* QA Using bert pretrained model by SQuAD, https:\/\/github.com\/google-research\/bert\n* Summary using BART pretrained model.","b9e91479":"## Define All function that will used","0d4c7bf4":"## Load modules","85dde897":"## HTML Display function","ed7e0635":"## QA Model function"}}