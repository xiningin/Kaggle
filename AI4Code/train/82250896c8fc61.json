{"cell_type":{"3004adf2":"code","844b5d99":"code","93936350":"code","d8fa64e6":"code","ecab2c65":"code","baf94b37":"code","62e3a0b3":"code","a5eae502":"code","131c1ab7":"code","f8385c07":"code","97a2df8a":"code","3aa15614":"markdown","97152c91":"markdown","d15c7bd9":"markdown","536135f2":"markdown","6a006c0b":"markdown","341ed460":"markdown","fa2e7073":"markdown","5d69a214":"markdown","0af6114a":"markdown","51acdf96":"markdown","4af48c42":"markdown"},"source":{"3004adf2":"import plotly\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as exp\nimport plotly.graph_objects as go\n\nimport pandas as pd\nimport numpy as np\n","844b5d99":"calendar = \"\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv\"\nsales_train_validation = \"\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv\"\nsell_prices = \"\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv\"\n\n\ncalendar_ = pd.read_csv(calendar, delimiter=\",\")\nsales_train_validation_ = pd.read_csv(sales_train_validation, delimiter=\",\")\nsell_prices_ = pd.read_csv(sell_prices, delimiter=\",\")","93936350":"print(calendar_.shape)\ncalendar_.head()","d8fa64e6":"print(sales_train_validation_.shape)\nsales_train_validation_.head()","ecab2c65":"print(sell_prices_.shape)\nsell_prices_.head()","baf94b37":"groups = sales_train_validation_.groupby(['cat_id'])\ncounts_dict = {}\nfor name, group in groups:\n    counts_dict[name] = len(group)\ndf = pd.DataFrame(counts_dict.items(), columns=['category', 'value'])\nfig = exp.pie(df, values='value', names='category', title='Category wise sales of items')\nfig.show()","62e3a0b3":"groups = sales_train_validation_.groupby(['state_id'])\ncounts_dict = {}\nfor name, group in groups:\n    counts_dict[name] = len(group)\ndf = pd.DataFrame(counts_dict.items(), columns=['category', 'value'])\nfig = exp.pie(df, values='value', names='category', title='State wise sales of items')\nfig.show()","a5eae502":"date_dict = pd.Series(calendar_.date.values,index=calendar_.d).to_dict()\ndates = list(date_dict.values())[0:1913] # we have sales data for 1913 days \n\ndef plot_time_series(row):\n    \n    daily_sales = row.iloc[6:].values\n    \n    \n    df = pd.DataFrame(\n    {'daily sales': daily_sales,\n     'date': dates,\n    })\n\n    fig = exp.line(df,x=\"date\",y=\"daily sales\")\n    \n    fig.update_layout(\n    title={\n        'text': \"Daily Unit Sales of a particular item from 2011 - 2016\",\n        'y':1,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    xaxis_title=\"Year\",\n    yaxis_title=\"No. of Items Sold\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"#7f7f7f\"\n    ))\n    \n    fig.show()\n\nplot_time_series(sales_train_validation_.iloc[1,:])","131c1ab7":"# plot a random sample of 10 points\n\ndf_sample = sales_train_validation_.sample(n=10, replace=False)\ndf_sample_ = pd.DataFrame(columns = ['item id','daily sales','date'])\n\ncolors = (exp.colors.sequential.Plasma)\n\ndef plot_sample(df_sample):\n    \n    fig = go.Figure()\n    count = 0\n    \n    for index,row in df_sample.iterrows():\n        \n        \n        item_id = row[1]\n        daily_sales = row.iloc[6:].values\n           \n        fig.add_trace(go.Scatter(x=dates, y=daily_sales,\n                    mode='lines+markers',\n                    name=item_id, marker = dict(color=colors[count])))\n        \n        \"\"\" uncomment to plot a sample of more than 10 points\n        fig.add_trace(go.Scatter(x=dates, y=daily_sales,\n               mode='lines+markers',\n               name=item_id))\"\"\"\n        \n        count+=1\n    \n    fig.update_layout(\n    autosize=False,\n    width=1200,\n    height=500, title={\n        'text': \"Daily Unit Sales of items from 2011 - 2016\",\n        'y':1,\n        'x':0.3,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    xaxis_title=\"Year\",\n    yaxis_title=\"No. of Items Sold\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"#7f7f7f\"\n    ))\n    fig.show()\n\nplot_sample(df_sample)        \n        \n        ","f8385c07":"### Moving Average\n\ndef plot_moving_avg_series(row):\n    \n    item_id = row.iloc[1]\n    \n    daily_sales_ = row.iloc[6:].values\n    daily_sales = list(map(int, daily_sales_))\n    \n    df = pd.DataFrame(\n    {'daily sales': daily_sales,\n     'date': dates,\n    })\n    \n    rolling = df.rolling(window=30)\n    moving_avg = rolling.mean()\n    \n    fig = go.Figure()\n    \n    fig.add_trace(go.Scatter(x=dates, y=daily_sales,mode='lines+markers',name=item_id))\n    \n    fig.add_trace(go.Scatter(x=dates, y=moving_avg['daily sales'],mode='lines+markers',name=item_id))\n\n    \n    fig.update_layout(\n    autosize=False,\n    width=1500,\n    height=500,\n    title={\n        'text': \"Daily Unit Sales of a particular item from 2011 - 2016\",\n        'y':1,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    xaxis_title=\"Year\",\n    yaxis_title=\"No. of Items Sold\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"#7f7f7f\"\n    ))\n    \n    fig.show()\n    \nplot_moving_avg_series(sales_train_validation_.iloc[1,:])","97a2df8a":"def plot_moving_avg_sample(df_sample):\n    \n    fig = go.Figure()\n    \n    for index,row in df_sample.iterrows():\n        \n        \n        item_id = row[1]\n        \n        daily_sales_ = row.iloc[6:].values\n        daily_sales = list(map(int, daily_sales_))\n    \n        df = pd.DataFrame(\n            {'daily sales': daily_sales,\n             'date': dates,\n            })\n    \n        rolling = df.rolling(window=30)\n        moving_avg = rolling.mean()\n\n        \n        fig.add_trace(go.Scatter(x=dates, y=moving_avg['daily sales'],\n               mode='lines+markers',\n               name=item_id))\n        \n    fig.update_layout(\n    autosize=False,\n    width=1200,\n    height=500,\n    title={\n        'text': \"Daily Unit Sales of a sample from 2011 - 2016\",\n        'y':1,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n    xaxis_title=\"Year\",\n    yaxis_title=\"No. of Items Sold\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"#7f7f7f\"\n    ))\n    \n    fig.show()\n\nplot_moving_avg_sample(df_sample)","3aa15614":"### References\n- https:\/\/mk0mcompetitiont8ake.kinstacdn.com\/wp-content\/uploads\/2020\/02\/M5-Competitors-Guide_Final-1.pdf\n- https:\/\/www.kaggle.com\/headsortails\/back-to-predict-the-future-interactive-m5-eda\n- https:\/\/www.kaggle.com\/robikscube\/m5-forecasting-starter-data-exploration ","97152c91":"### Decomposition of a time series\n\nA time series can be defined by decomposing it into four main elements:\n\n- Trend: Long-term movements- increasing\/decreing trend.\n- Seasonal Effect: Calendar-related cyclical fluctuations.\n- Cycles: Business cycle-related fluctuations.\n- Residuals: Random or systematic fluctuations\n\n### Questions\n\n- Seasonality: Any repititive pattern observed in the sales of various items over the years?\n- Fluctuaton in the sales during an event\/ promotions\/ specific time period?","d15c7bd9":"## State wise sales","536135f2":"### Key Observations\n\n- Looking at the plots, we observe that most of the values are zeroes. It's noisy. There are too many fluctuations in this data.\n  \n  Can we say that the demand for these products is **intermittent**? How can we determine this statistically?\n  Intermittent time series have a large number of values that are zero. When an item has several periods of zero demand, the demand is said to be intermittent. In cases other than zero, the demand is erratic. Read more at:\n      - https:\/\/www.lancaster.ac.uk\/pg\/waller\/pdfs\/Intermittent_Demand_Forecasting.pdf\n      - https:\/\/www.researchgate.net\/publication\/237019869_Intermittent_demand_forecasts_with_neural_networks\n      - https:\/\/robjhyndman.com\/papers\/foresight.pdf","6a006c0b":"## Data Organization\n\n- **Calendar.csv**: Contains the dates on which the products are sold.\n    - date, wm_yr_wk, weekday, wday, month, year, d\n    - event_name_1, event_type_1, event_name_2, event_type_2: Certain events which may affect the sales\n    - snap_CA, snap_TX, snap_WI: \n    \n    \n\n- **sales_train_validation.csv**: Contains the unit sales data for each product for 1913 days.\n    - ***id, item_id***\n    - ***cat_id:***  3, hobbies, household, food\n    - ***dept_id***:  7 departments,  2 for hobbies, 2 for household, 3 for foods\n    - ***state_id***:  3 states, CA, WI, TX\n    - ***store_id***:  10 stores, CA: 4, TX: 3, WI: 3\n    - ***d_1, d_2, ... d_1912, d_1913***:  Daily unit sales data for each item belonging to a particular category\n      (cat_id), from a department (dept_id), sold at some store with id as store_id, in a particular state identified by state_id.\n\n\n- **sell_prices.csv**: Contanins the price of each product that was sold in different stores and the date it was sold.\n","341ed460":"## Category wise sales","fa2e7073":"Given the unit sales of various products from Walmart, forecast daily sales for 28 days into the future(from the last day for which the data is available) .\n\nThe data is from three states (California, Texas, and Wisconsin) and spans over a period of 5 years from 2011 - 2016. For each item being sold, we have information on the department it's from, its category, and the store details selling the item. There are 3049 items belonging to 3 categories and 7 departments, being sold in 10 stores in 3 states.\nApart from the above, we also have some other variables such as prices of the product for the week, and special events.\n","5d69a214":"## Time Series Forecasting:\n\n- https:\/\/otexts.com\/fpp2\/intro.html\n- https:\/\/robjhyndman.com\/papers\/forecastingoverview.pdf","0af6114a":"## Plotting the time series data","51acdf96":"##  Goal","4af48c42":"#### Data Smoothing using moving averages.\n\nSince our time series data is noisy, we can smooth it using moving averages.\nMoving average removes the random fluctuations\/ variation in data by taking the average of a fixed window of observations, over the entire time series. Here we consider a window size of 30.\n"}}