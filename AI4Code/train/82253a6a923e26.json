{"cell_type":{"87ea45cf":"code","bca3f760":"code","d6046c97":"code","37dda125":"code","ee68153f":"code","fd4b7b30":"code","1b2fa890":"code","617b03fa":"code","cdbc3289":"code","9bab5e11":"code","35abf341":"code","867c5336":"code","12c20a4b":"code","4acc3a02":"code","953b21ec":"code","f652113b":"code","94fbd9cc":"code","66343b53":"code","9b3fb6ea":"code","8baba848":"code","e87efaa6":"code","88bc7b3c":"code","38bc1a8e":"code","3a4ca2fb":"code","d1ef12be":"code","5ec055ac":"code","72ba1c72":"code","4bdec192":"code","85c8a90e":"code","dea7014a":"code","d72093d2":"code","2b45749c":"code","cb193084":"code","6a479119":"code","f66f1565":"code","42e79cc1":"code","6493b016":"code","441fb0ec":"code","4f2371a0":"code","2147ef76":"code","3e4116ef":"code","f9e3e2a9":"code","57491e70":"code","11d913d9":"code","647a1020":"code","4ab85df2":"code","cae73fe9":"code","2d7e3aa4":"code","84499706":"code","0c6237a7":"code","2d0bbd97":"code","dbc94000":"code","76582850":"code","65c8f021":"code","c13ffa8e":"code","fed97fd4":"code","20a4a688":"code","8ea7bb98":"code","cc75423f":"code","e3e27845":"code","0ea15268":"code","38afd2c0":"code","27fe3329":"code","97e9c85c":"code","93303817":"code","120aca2d":"code","be38157e":"code","2de756ae":"code","6c1c247c":"code","b0bff415":"code","a97141b6":"code","b20b7b6a":"code","e5c8cb81":"markdown","450a4bb3":"markdown","68de2f47":"markdown","3420ca76":"markdown","45f2eb8a":"markdown","c2fb78b5":"markdown","04de48cd":"markdown","e4446eae":"markdown","ee1dd117":"markdown","0f9020ba":"markdown","5528753a":"markdown","01ef7800":"markdown","978dc2e4":"markdown","a39d46d0":"markdown","84f5cbcd":"markdown","4e44370d":"markdown"},"source":{"87ea45cf":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', None)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n%matplotlib inline\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bca3f760":"df=pd.read_csv('\/kaggle\/input\/housesalesprediction\/kc_house_data.csv')","d6046c97":"df.shape","37dda125":"df.columns","ee68153f":"plt.figure(figsize=(6,4), dpi=100)\nplt.scatter(x=df['sqft_living'], y=df['price'], marker='.')","fd4b7b30":"#from above plot we can see there is one extreme outlier which has highest living area but the price is very low. I can remove it for usual scenarios.\n#we will check the index of the outlier row\ndf[df['sqft_living']>12000]","1b2fa890":"#we will drop the index 12777\ndf.drop(index=12777, axis=0, inplace=True)","617b03fa":"f, axes = plt.subplots(1, 2, figsize=(20, 5), sharex=False)\n\n\nplt.figure(figsize=(8,5), dpi=100)\nsns.distplot(df['price'] , fit=norm, ax=axes[0]);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df['price'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\naxes[0].legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\naxes[0].set_ylabel('Frequency')\naxes[0].set_title('SalePrice distribution')\n\n#Get also the QQ-plot\n# fig = plt.figure(figsize=(8,5), dpi=100)\nres = stats.probplot(df['price'], plot=axes[1])","cdbc3289":"log_price = np.log1p(df[\"price\"])\n\nf, axes = plt.subplots(1, 2, figsize=(20, 5), sharex=False)\n\n\nplt.figure(figsize=(8,5), dpi=100)\nsns.distplot(log_price , fit=norm, ax=axes[0]);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df['price'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\naxes[0].legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\naxes[0].set_ylabel('Frequency')\naxes[0].set_title('SalePrice distribution')\n\n#Get also the QQ-plot\n# fig = plt.figure(figsize=(8,5), dpi=100)\nres = stats.probplot(log_price, plot=axes[1])","9bab5e11":"df['log_price']=np.log1p(df['price'])\ndf.head()","35abf341":"df_null_val=df.isnull().sum()\/df.shape[0]*100.0\ndf_null_val","867c5336":"sns.heatmap(df.isnull(), yticklabels=False, cbar='viridis')","12c20a4b":"plt.subplots(figsize=(5,5), dpi=100)\nsns.heatmap(df.corr(), cmap='viridis', square=True)","4acc3a02":"# for i in df.columns:\n#     print(i, df[i].nunique())\ncol_uniqueness_df=pd.DataFrame([{'column':i, 'unique_values':df[i].nunique()} for i in df.columns])\n\ncol_uniqueness_df","953b21ec":"from sklearn.preprocessing import LabelEncoder\n\nfor c in list(col_uniqueness_df[col_uniqueness_df['unique_values']<20]['column']):\n    l=LabelEncoder()\n    l.fit(list(df[c].values))\n    df[c]=l.transform(list(df[c].values))","f652113b":"df.head()","94fbd9cc":"df['total_sqft']=df['sqft_living']+df['sqft_lot']+df['sqft_above']+df['sqft_living15']+df['sqft_lot15']\nplt.scatter(x=df['total_sqft'], y=df['price'], marker='.')","66343b53":"from itertools import combinations\nfrom scipy.stats.stats import pearsonr\n\n\nfeature_list=['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_living15', 'sqft_lot15']\nfor i in range(2,len(feature_list)+1):\n    combinations_object = combinations(feature_list, i)\n    for set_obj in list(combinations_object):\n        added_vector=np.zeros(df.shape[0])\n        for col in set_obj:\n            list_vector=np.array(list(df[col]))\n            added=np.add(added_vector,list_vector)\n            added_vector=added\n        if pearsonr(added_vector, np.array(df['log_price']))[0]>0.6:\n            print (set_obj)\n            print (pearsonr(added_vector, np.array(df['log_price'])))\n            ","9b3fb6ea":"df['new_sqft_feature']=df['sqft_living']+df['sqft_above']+df['sqft_living15']","8baba848":"numeric_feats = df.dtypes[df.dtypes != \"object\"].index\n# numeric_feats\n\n\n# Check the skew of all numerical features\nskewed_feats = df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head","e87efaa6":"new_df=df.copy()\nskewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    if feat not in ['price','log_price']:\n        new_df[feat] = boxcox1p(new_df[feat], lam)","88bc7b3c":"numeric_feats = df.dtypes[df.dtypes != \"object\"].index\n# numeric_feats\n\n\n# Check the skew of all numerical features\nskewed_feats = new_df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head","38bc1a8e":"new_df.head()","3a4ca2fb":"all_dummy=pd.get_dummies(new_df.drop(columns=['id','date'], axis=1))\nall_dummy.head()","d1ef12be":"new_df.dtypes","5ec055ac":"import xgboost\nreg=xgboost.XGBRegressor()\nreg","72ba1c72":"n_estimators=[200,500,1000,1200, 1300, 1400, 1500]\nmax_depth=[2,3,5,10,15]\nbooster=['gbtree','gblinear']\nlearning_rate=[0.05, 0.1, 0.15, 0.2]\nmin_child_weight=[1,2,3,4,5]\n\nhyperparameter_grid={\n    'n_estimators':n_estimators  ,\n    'max_depth':max_depth  ,\n    'booster':booster  ,\n    'learning_rate': learning_rate  ,\n    'min_child_weight':   min_child_weight\n}","4bdec192":"# new_df.groupby('yr_built').count()\nnew_df.head(2)\n# df.groupby('zipcode').count()","85c8a90e":"from sklearn.model_selection import train_test_split\n\nX=new_df[list( set(new_df.columns) - set(['id','date','price', 'lat', 'long', 'log_price', 'total_sqft', 'waterfront', 'view', ]) )]\nY=new_df['log_price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)","dea7014a":"reg.fit(X_train, y_train)\ny_pred=reg.predict(X_test)","d72093d2":"pd.set_option('display.float_format', lambda x: '%.3f' % x)\n\npred_ds=[]\nfor i in range(len(y_pred)):\n    pred_ds.append({'y_actual':list(y_test)[i], 'y_predicted':list(y_pred)[i]})\n\npred_df=pd.DataFrame(pred_ds)\npred_df['actual_price']=pred_df['y_actual'].apply(lambda x: np.expm1(x))\npred_df['predicted_price']=pred_df['y_predicted'].apply(lambda x: float(np.expm1(x)))\npred_df.head()","2b45749c":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nprint (sqrt(mean_squared_error(y_test, y_pred, squared=False)))","cb193084":"from sklearn.model_selection import RandomizedSearchCV\n\n\nrandomcv=RandomizedSearchCV(\n            estimator=reg,\n            param_distributions=hyperparameter_grid,\n            cv=10, n_iter=50,\n            scoring='neg_mean_absolute_error', n_jobs=8,\n            verbose=5,\n            return_train_score=True,\n            random_state=42\n)","6a479119":"# randomcv.fit(X,Y)","f66f1565":"best_xgb=randomcv.best_estimator_\n# best_xgb\n# randomcv.__dict__\nprint(randomcv.best_score_)","42e79cc1":"\ny_pred=best_xgb.predict(X_test)\n\nbest_xgb_pred_ds=[]\nfor i in range(len(y_pred)):\n    best_xgb_pred_ds.append({'y_actual':list(y_test)[i], 'y_predicted':list(y_pred)[i]})\n\nbest_xgb_pred_df=pd.DataFrame(best_xgb_pred_ds)\nbest_xgb_pred_df['actual_price']=best_xgb_pred_df['y_actual'].apply(lambda x: np.expm1(x))\nbest_xgb_pred_df['predicted_price']=best_xgb_pred_df['y_predicted'].apply(lambda x: float(np.expm1(x)))\nbest_xgb_pred_df.head()","6493b016":"print (sqrt(mean_squared_error(y_test, y_pred, squared=False)))","441fb0ec":"plt.figure(figsize=(10,10), dpi=100)\nplt.scatter(best_xgb_pred_df['actual_price'], best_xgb_pred_df['predicted_price'], marker='.')","4f2371a0":"plt.figure(figsize=(10,10), dpi=100)\nplt.scatter(best_xgb_pred_df['actual_price'], best_xgb_pred_df['predicted_price'], marker='.')","2147ef76":"import xgboost\nreg=xgboost.XGBRegressor()","3e4116ef":"df.head(2)","f9e3e2a9":"df.columns","57491e70":"x_cols=['bedrooms', 'bathrooms', 'sqft_living',\n       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n       'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'sqft_living15', 'sqft_lot15']","11d913d9":"X=df[x_cols]\nY=df['price']","647a1020":"from sklearn.model_selection import train_test_split","4ab85df2":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)","cae73fe9":"from sklearn.feature_selection import SelectKBest, f_regression\n# configure to select all features\nfs = SelectKBest(score_func=f_regression, k='all')\n# learn relationship from training data\nfs.fit(X_train, y_train)\n# transform train input data\nX_train_fs = fs.transform(X_train)\n# transform test input data\nX_test_fs = fs.transform(X_test)","2d7e3aa4":"...\n# what are scores for the features\nfor i in range(len(fs.scores_)):\n    print('Feature %d: %f' % (i, fs.scores_[i]))\n# plot the scores\nplt.bar([i for i in range(len(fs.scores_))], fs.scores_)\nplt.show()","84499706":"X_train_fs","0c6237a7":"X_train_fs.shape","2d0bbd97":"reg.fit(X_train_fs, y_train)","dbc94000":"pred=reg.predict(X_test_fs)","76582850":"type(y_test)","65c8f021":"pred_list=[]\ny_test_list=list(y_test)\nfor i in range(len(pred)):\n    pred_list.append({'prediction':pred[i], 'actual':y_test_list[i], 'diff':pred[i]-y_test_list[i]})\npred_df=pd.DataFrame(pred_list)\n\npred_df","c13ffa8e":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nprint (sqrt(mean_squared_error(y_test, pred, squared=False)))","fed97fd4":"sns.heatmap(X.corr())","20a4a688":"X.corr()","8ea7bb98":"df.corr()","cc75423f":"corr=df.corr()","e3e27845":"corr[corr]","0ea15268":"# high_tgt_corr_cols=corr['price']\n# for r, idx in corr[corr['price']>0.35].iterrows():\n#     print (idx)\n\ncorr[corr['price']>0.35].index","38afd2c0":"high_corr_cols=list(corr[corr['price']>0.35].index)\n\nhigh_corr_cols.remove('price')\n\nX=df[high_corr_cols]\nY=df['price']","27fe3329":"list(corr[corr['price']>0.35].index).remove('price')","97e9c85c":"X.head()","93303817":"reg_2=xgboost.XGBRegressor()\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\nreg_2.fit(X_train, y_train)\npred=reg_2.predict(X_test)\n\npred_list=[]\ny_test_list=list(y_test)\nfor i in range(len(pred)):\n    pred_list.append({'prediction':pred[i], 'actual':y_test_list[i], 'diff':pred[i]-y_test_list[i]})\npred_df=pd.DataFrame(pred_list)\n\npred_df","120aca2d":"from sklearn.model_selection import RandomizedSearchCV\n\n","be38157e":"print (sqrt(mean_squared_error(y_test, pred, squared=False)))","2de756ae":"X.head()","6c1c247c":"df.shape","b0bff415":"from sklearn.feature_selection import SelectKBest, f_regression\n# configure to select all features\nfs = SelectKBest(score_func=f_regression, k='all')\n# learn relationship from training data\nfs.fit(X_train, y_train)\n# transform train input data\nX_train_fs = fs.transform(X_train)\n# transform test input data\n# X_test_fs = fs.transform(X_test)","a97141b6":"X_train.shape","b20b7b6a":"X_train_fs.shape","e5c8cb81":"# We will add new feature by adding 'sqft_living', 'sqft_above', 'sqft_living15' as this has the highest correlation","450a4bb3":"# check how many nulls present in each column\n\n**for filling null values with simple imputation:**\n* use mean() for numericals\n* use mode() for categorical","68de2f47":"# The data is little deviated from normal distribution, hence we would try to normalize this. \n# Reason being linear models tend to work better with Normally distributed data\n**We can take log of the price variable to normalize the distribution**","3420ca76":"# trying to reduce the skewness in feature set","45f2eb8a":"# Below is the relation of act vs pred with only X_train fitted model","c2fb78b5":"# check data correlation\nThis will be a good measure to check high correlation between two features, if we have high correlation among features we can exclude any one to simplify model","04de48cd":"# Since data looks quite close to it's normal now, we will apply the same filter to te price data and substitute the same in the dataframe","e4446eae":"# Below is the correlation betweem act vs pred with model trained in whole data set and tested with X_test","ee1dd117":"# from the above plot it seems the new feature might not be a good addition to the feature set, we will check for other sqft feature combinations.If any combination is having a more than .6 correlation with price we will pick it","0f9020ba":"# CHeck skewness in numerical features","5528753a":"# First train and check on default settings","01ef7800":"# We will cehck again for skewness","978dc2e4":"# check unique ness of the columns to check if any numerical variable is actually a categorical variable","a39d46d0":"# Plot the nulls count in a heatmap\nwe don't have any nulls so the below is just as an example code block","84f5cbcd":"# removing the unwanted columns from the feature set","4e44370d":"# a new feature can be created as total sqft of the property by adding up all the sqft measure features as below, then we can check the correlation of the new feature with price"}}