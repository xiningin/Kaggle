{"cell_type":{"fcd7ea73":"code","d62c11c7":"code","009752ca":"code","8ad8d4f3":"code","e89fb6a1":"code","ae19eac2":"code","71deb034":"code","6a5fdcaf":"code","9088261b":"code","50973252":"code","ad58c8d7":"code","32b9335e":"code","8137ff9e":"code","e1cd462a":"code","62e7bce4":"code","bd71f7f5":"code","c0e9af6b":"code","8b229c98":"code","d7e0593a":"code","5cb3e75c":"code","b10323dd":"markdown","506eba14":"markdown","93de20a1":"markdown","4ad2edd1":"markdown","22402fca":"markdown","9d890a76":"markdown","8ba4079c":"markdown","6ee95ebf":"markdown","3e7c0cfc":"markdown","00f5f08a":"markdown","db4b2e5c":"markdown"},"source":{"fcd7ea73":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d62c11c7":"train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\ntrain.head()","009752ca":"train.shape","8ad8d4f3":"test.head()","e89fb6a1":"test.shape","ae19eac2":"# save the labels into a variable\nlabel = train['label']\n\n# Drop the label feature and store the pixel data\ndata = train.drop(\"label\", axis=1)","71deb034":"print(data.shape)\nprint(label.shape)","6a5fdcaf":"# display or plot a number\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nplt.figure(figsize=(7,7))\nidx = 1\n\ngrid_data = data.iloc[idx].as_matrix().reshape(28, 28)\nplt.imshow(grid_data, interpolation=\"none\", cmap=\"gray\")\nplt.show()\n\nprint(label[idx])","9088261b":"#before doing dimensionality reduction we must perform data-preprocessing for better understandable format\n#We can utilise Eigenvalues and Eigenvectors to reduce the dimension space\n# Data-preprocessing: Standardizing the data\n\nfrom sklearn.preprocessing import StandardScaler\nstandardized_data = StandardScaler().fit_transform(data)\nprint(standardized_data.shape)","50973252":"#find the co-variance matrix which is : A^T * A\nsample_data = standardized_data\n\n# matrix multiplication using numpy\ncovar_matrix = np.matmul(sample_data.T , sample_data)\n\n\nprint(\"the shape of co-variance matrix= \", covar_matrix.shape)\n","ad58c8d7":"#eigenvector does not change direction in a transformation\n# finding the top two eigen-values and corresponding eigen-vectors \n# for projecting onto a 2-Dim space.\n\nfrom scipy.linalg import eigh\n\n\n# the parameter 'eigvals' is defined (low value to heigh value) \n# eigh function will return the eigen values in asending order\n#top eigen values \n\nvalues, vectors = eigh(covar_matrix, eigvals=(782, 783))\n\n\nprint(\"Shape of eigen vectors = \",vectors.shape)\n# converting the eigen vectors into (2,d) shape for easyness of further computations\nvectors = vectors.T\n\nprint(\"Updated shape of eigen vectors = \",vectors.shape)\n# here the vectors[1] represent the eigen vector corresponding 1st principal eigen vector\n# here the vectors[0] represent the eigen vector corresponding 2nd principal eigen vector","32b9335e":"# projecting the original data  on the plane \n#formed by two principal eigen vectors by vector-vector multiplication.\n\nimport matplotlib.pyplot as plt\nnew_coordinates = np.matmul(vectors, sample_data.T)\n\nprint (\" resultanat new data points' shape \", vectors.shape, \"X\", sample_data.T.shape,\" = \", new_coordinates.shape)","8137ff9e":"import pandas as pd\n\n#appending label to the 2nd projected data\nnew_coordinates = np.vstack((new_coordinates, label)).T\n\n#creating a new data frame for ploting the labeled points\ndataframe = pd.DataFrame(data=new_coordinates, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\nprint(dataframe.head())","e1cd462a":"#ploting the 2d data points with seaborn\nimport seaborn as sns\nsns.FacetGrid(dataframe, hue=\"label\", height=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()\n#overlapping is lot and pca does not do good job\n#tsne does a lot better ","62e7bce4":"#initializing the pca\n\nfrom sklearn import decomposition\npca = decomposition.PCA()","bd71f7f5":"#configuring the parameters\n#the number of components = 2\n\npca.n_components = 2\npca_data=  pca.fit_transform(sample_data)\n\n#pca_reduced will contain the 2-d projects of simple data\nprint(\"shape of pca_reduced.shape\", pca_data.shape)","c0e9af6b":"import warnings\nwarnings.filterwarnings(\"ignore\")","8b229c98":"# attaching the label for each 2-d data point \npca_data = np.vstack((pca_data.T, label)).T\n\n# creating a new data fram which help us in ploting the result data\npca_df = pd.DataFrame(data=pca_data, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\nsns.FacetGrid(pca_df, hue=\"label\", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()\n#just as same its kinda slighlty rotated 90 degrees","d7e0593a":"#pca for dimensionality reduction (not for visulaization)\n\npca.n_components = 784\npca_data = pca.fit_transform(sample_data)\n\n#nothing but eigen values\npercentage_var_explained = pca.explained_variance_\/ np.sum(pca.explained_variance_)\ncum_var_explained = np.cumsum(percentage_var_explained)\n\n#plot the pca spectrum\nplt.figure(1, figsize=(6, 4))\n\nplt.clf()\nplt.plot(cum_var_explained, linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('cumulative_explained_variance')\nplt.show()\n\n#if we take 200-dimesions , approx . 90% of variance is explained","5cb3e75c":"from sklearn.manifold import TSNE\n\nmodel = TSNE(n_components=2, random_state=0)\n# configuring the parameteres\n# the number of components = 2\n\ntsne_data=  model.fit_transform(sample_data)\n\ntsne_data = np.vstack((tsne_data.T, label)).T\ntsne_df = pd.DataFrame(tsne_data, columns = (\"Dim_1\", \"Dim_2\", \"label\"))\n\n#plotting the result of tsne\nsns.FacetGrid(tsne_df, hue='label', size=6).map(plt.scatter, \"Dim_1\", 'Dim_2').add_legend()\nplt.show()","b10323dd":"**What is t-SNE?**\n\nt-Distributed Stochastic Neighbor Embedding (t-SNE) is an unsupervised, non-linear technique primarily used for data exploration and visualizing high-dimensional data","506eba14":"* it tries to save local structure rather then global structure \n* tsne tries to find nearest points or neighbours in both dimesions in high and low and try to embed in lower dimension (uses gaussian in high and cauchy in lower)\n* it uses probability for finding similar points (neighbors)\n* its much better for non-linear shapes \n* not very interpretable, must run multiple times, random shapes don't mean nothing , perplexity is like neighbours\n* can take a lot of time to run\n* clusters are not interpretable \n* it create new features from old ones","93de20a1":"* first we standardize our dataset so our mean will be 0 and variance would be 1\n* after standardization we get the covariance-matrix..\n* from the covariance matrix we find our eigen-values and eigen-vector\n* top eigen values correspond to eigen vectors and these vectors explain maximum variance so we select those only","4ad2edd1":"* features that explain the maximum variance are principal components or eigen-vectors\n* we select only top eigen-vectors and discard whatever left\n* it tries to save global structure\n* it tries to find the direction and project old points into that direction which explain maximum variance(spread)\n* its good for linear shape \n* interpretability not good can get messy with multiple dimensions works fine with less dimensions\n* does not create new features","22402fca":"in t-sne we should always try for multiple perplexity and multiple iteration until its not get properly stable shape so","9d890a76":"** PCA for dimensionality redcution**","8ba4079c":"**2D Visualization using PCA**","6ee95ebf":"What is Dimensionality Reduction?\n\nDimensionality reduction is simply, the process of reducing the dimension of your feature set. \n\nPCA (Principal Component Analysis) : Popularly used for dimensionality reduction in continuous data, PCA rotates and projects data along the direction of increasing variance. The features with the maximum variance are the principal components.\n","3e7c0cfc":"# ** PCA using Scikit-Learn**","00f5f08a":"we should standardized the variables before applying PCA because it will give more emphasis to those variables having higher variances than to those variables with very low variances while identifying the right principle component.","db4b2e5c":"a covariance matrixa is a square matrix giving the covariance between each pair of elements of a given random vector. In the matrix diagonal there are variances, i.e., the covariance of each element with itself.\n\n\u201cCovariance\u201d indicates the direction of the linear relationship between variables. \u201cCorrelation\u201d on the other hand measures both the strength and direction of the linear relationship between two variables"}}