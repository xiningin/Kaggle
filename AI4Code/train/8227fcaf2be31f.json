{"cell_type":{"495703f7":"code","4ec1a597":"code","cd3e030a":"code","669e3e7d":"code","1f05d479":"code","8b6041af":"code","83b138a0":"code","a9910beb":"code","186b43e9":"code","639bdad9":"code","540d6d4e":"code","8708aa71":"code","2bc64142":"code","e3179684":"code","5dc2a036":"code","5fbe6f7f":"code","e712df26":"code","24ab6147":"code","bb6f0d77":"code","8a21fa7a":"code","9f2dbe84":"code","ff17767d":"code","2962bc89":"code","fc4cee05":"code","4e446f1f":"code","3ff498f2":"code","0e8673b8":"code","9592ba16":"code","79ac4173":"code","8a185df4":"code","204e6c14":"code","0d3eeb07":"code","e16a47ac":"code","c13b3284":"code","bc617634":"code","8865d042":"code","93a4593c":"code","75d22af6":"code","edacccb1":"code","a71b5f7a":"code","a6a09403":"code","d6d8aea8":"code","f9f63a39":"code","a6016d43":"code","ad3ef0ec":"code","3af2c819":"code","6cb32cec":"code","b5f8eb7a":"code","c6c3bc83":"code","16ad410d":"code","bfb9fc89":"code","5950ecad":"markdown","8555478b":"markdown","3f6e3698":"markdown","cd49f563":"markdown","f8660c65":"markdown","ba41b84f":"markdown","aec6ac45":"markdown","72d808da":"markdown","0adb40cb":"markdown","7ac31b51":"markdown","ea35752d":"markdown","251cff46":"markdown","9b9745bc":"markdown","a425e767":"markdown","559cab8f":"markdown","8fbc3326":"markdown","94c4a766":"markdown","f5441829":"markdown","9f5d3fbe":"markdown"},"source":{"495703f7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport os \nimport re\nfrom keras.utils import to_categorical \nfrom sklearn.model_selection import train_test_split\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport seaborn as sns\n\nfrom nltk.stem import *\nfrom sklearn.preprocessing import LabelEncoder\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nnltk.download(\"stopwords\")","4ec1a597":"os.listdir(\"..\/input\/covid-19-nlp-text-classification\") # list of items in that directory","cd3e030a":"train = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\", encoding='latin-1') # read the csv\ntrain.head() # first 5 row","669e3e7d":"test = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv\", encoding='latin-1') # test dataset\ntest.head() # test dataset first 5 row","1f05d479":"train[\"Sentiment\"].unique() # unique sentiment","8b6041af":"train.info() # info of training dataset","83b138a0":"train.shape # shape of train data","a9910beb":"train[\"Sentiment\"].value_counts() # sentiment cateorical sum","186b43e9":"train[\"OriginalTweet\"] = train[\"OriginalTweet\"].apply(lambda x: x.lower()) # lower the text","639bdad9":"train[\"OriginalTweet\"][3] # originaltweet no. 3","540d6d4e":"def hash_tag(sent): # for #tag\n    hashes = re.findall(r\"#(\\w+)\", sent)\n    return \" \".join(hashes)\n\ntrain[\"hash_tag\"] = train[\"OriginalTweet\"].apply(lambda x: hash_tag(x)) # create column for hash tag\n\ndef web_link(link):\n    web = re.findall(r'https?:\/\/\\S+', link)\n    return \" \".join(web)\ntrain[\"web_link\"] = train[\"OriginalTweet\"].apply(lambda x: web_link(x)) # create a column for weblinks","8708aa71":"train.head()","2bc64142":"def original_tweet(data):\n    processed = data.str.replace(r\"#(\\w+)\", \"\") # replace #tag\n    processed = processed.str.replace(r'https?:\/\/\\S+', \" \") # replace url\n    processed = processed.str.replace(r\"\\r\\r\\n\", \"\") # replace \\r\\n\\r\n    processed = processed.str.replace(\".\", \"\") # Replace dot\n    processed = processed.str.replace(r'^\\s+|\\s+?$', '') # Remove lead and trail space\n    processed = processed.str.replace(r'\\s+', ' ') # Remove whitespace\n    processed = processed.str.replace(\"'\", \"o\")\n    processed = processed.str.replace(r'[^\\w\\d\\s]', '') # remove punctuation\n    processed = processed.str.replace(r'[0-9]', '') #remove number\n    return processed.str.lower()","e3179684":"clean_train = original_tweet(train[\"OriginalTweet\"])\nclean_test = original_tweet(test[\"OriginalTweet\"])","5dc2a036":"clean_test[4]","5fbe6f7f":"location = train[\"Location\"].value_counts()[:20]\nplt.subplots(figsize=(12,10))\nsns.barplot(y=location.index, x=location, palette=\"deep\", data=train)","e712df26":"train.shape # train csv shape","24ab6147":"train[\"Location\"].value_counts() # location value counts","bb6f0d77":"train[\"Sentiment\"].value_counts() # Sentiment value counts","8a21fa7a":"train[\"Location\"] = train[\"Location\"].fillna(\"Unknown\") # Fill the location gap by Unknown","9f2dbe84":"Ex_neg = train[train[\"Sentiment\"]==\"Extremely Negative\"]\nEX_NEG = Ex_neg[Ex_neg[\"Location\"] != \"Unknown\"]","ff17767d":"EX_NEG[EX_NEG[\"Location\"]==\"I try to be politics free but will comment on Trump\/GOP policy as the news is dominating tape.\"] # find false location","2962bc89":"EX_NEG.drop(104, axis=0, inplace=True) # drop location","fc4cee05":"stop_words = set(stopwords.words(\"english\")) # stopwords\n\nclean_train = clean_train.apply(lambda x:\" \".join(term for term in x.split() if term not in stop_words)) # clean train\nclean_test = clean_test.apply(lambda x:\" \".join(term for term in x.split() if term not in stop_words)) # clean test","4e446f1f":"wn = WordNetLemmatizer() # Using Lemmatization\n\nclean_train = clean_train.apply(lambda x:\" \".join([wn.lemmatize(word) for word in x.split()]))\nclean_test = clean_test.apply(lambda x:\" \".join([wn.lemmatize(word) for word in x.split()]))","3ff498f2":"clean_train[2]","0e8673b8":"le = LabelEncoder()\ntrain[\"Sentiment\"] = le.fit_transform(train[\"Sentiment\"])\ntrain","9592ba16":"max_seq_length = np.max(clean_train.apply(lambda tweet: len(tweet))) # max sequence length\nmax_seq_length","79ac4173":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(clean_train)\n\nvocab_length = len(tokenizer.word_index) + 1","8a185df4":"vocab_size = vocab_length\nembedding_dim = 16\nmax_length = max_seq_length\ntrunc_type = \"pre\"\noov_token = \"<OOV>\"\npad_type = \"pre\"","204e6c14":"train_inputs = tokenizer.texts_to_sequences(clean_train)\ntrain_inputs = pad_sequences(train_inputs, maxlen=max_seq_length, padding=pad_type)\n\ntest_inputs = tokenizer.texts_to_sequences(clean_test)\ntest_inputs = pad_sequences(test_inputs, maxlen=max_seq_length, padding=pad_type)","0d3eeb07":"train_inputs","e16a47ac":"seed = 42\n\nX = train_inputs\ny = train[\"Sentiment\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed, test_size=0.3)","c13b3284":"X_train","bc617634":"y_train = to_categorical(y_train, 5, dtype =\"uint8\")\ny_test = to_categorical(y_test, 5, dtype =\"uint8\")","8865d042":"y_train.shape # shape of y_train","93a4593c":"train[\"Sentiment\"].unique()","75d22af6":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(256, return_sequences=True)),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(5, activation='softmax')\n])\n# opt = tf.keras.optimizers.Adam(learning_rate=0.01)\nmodel.compile(loss='categorical_crossentropy',optimizer=\"adam\",metrics=['accuracy'])","edacccb1":"model.summary() # summary of the model","a71b5f7a":"num_epochs = 20\nhistory = model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_test, y_test))","a6a09403":"le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))","d6d8aea8":"le_name_mapping","f9f63a39":"predicted = model.predict_classes(X_test)\npredicted","a6016d43":"test_data = list(clean_test)","ad3ef0ec":"labels = ['Extremely Negative', 'Extremely Positive', 'Negative', 'Neutral', 'Positive']\n# forming new sentences for testing, feel free to experiment\n# sentence 1 is bit sarcastic, whereas sentence two is a general statment.\nnew_senti = []\nfor i in range(len(test_data)):\n    new_sentence = [f\"{test_data[i]}\"]\n\n    # Converting the sentences to sequences using tokenizer\n    new_sequences = tokenizer.texts_to_sequences(new_sentence)\n    # padding the new sequences to make them have same dimensions\n    new_padded = pad_sequences(new_sequences, maxlen = max_length,\n                               padding = \"pre\",\n                               truncating = trunc_type)\n\n    new_padded = np.array(new_padded)\n\n    new_senti.append(labels[np.argmax(model.predict(new_padded))])","3af2c819":"test[\"new_sentiment\"] = new_senti","6cb32cec":"test","b5f8eb7a":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(accuracy))\n\nplt.plot(epochs, accuracy, \"b\", label=\"trainning accuracy\")\nplt.plot(epochs, val_accuracy, \"r\", label=\"validation accuracy\")\nplt.legend()\nplt.show()\n\nplt.plot(epochs, loss, \"b\", label=\"trainning loss\")\nplt.plot(epochs, val_loss, \"r\", label=\"validation loss\")\nplt.legend()\nplt.show()","c6c3bc83":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(np.argmax(y_test, 1), predicted)\nimport seaborn as sns\nsns.heatmap(cm, annot=True)","16ad410d":"e = model.layers[0]\nweights = e.get_weights()[0]\nprint(weights.shape)\nreverse_word_index = dict([(value, key) for (key, value) in tokenizer.word_index.items()])","bfb9fc89":"import io\n\nout_v = io.open(\"vecs.tsv\", \"w\", encoding=\"utf-8\")\nout_m = io.open(\"meta.tsv\", \"w\", encoding=\"utf-8\")\n\nfor word_num in range(1, vocab_size):\n    word = reverse_word_index[word_num]\n    embeddings = weights[word_num]\n    out_m.write(word + \"\\n\")\n    out_v.write(\"\\t\".join([str(x) for x in embeddings]) + \"\\n\")\nout_v.close()\nout_m.close()","5950ecad":"# visualization","8555478b":"# confusion matrix","3f6e3698":"> ** Stemming and Lemmatization are Text Normalization (or sometimes called Word Normalization) techniques in the field of Natural Language Processing that are used to prepare text, words, and documents for further processing. Stemming and Lemmatization have been studied, and algorithms have been developed in Computer Science since the 1960's. (datacamp)**\n\n# Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.","cd49f563":"# test csv","f8660c65":"# plot","ba41b84f":"## Read train files","aec6ac45":"# list of files","72d808da":"# apply","0adb40cb":"# check","7ac31b51":"# prediction","ea35752d":"# model building","251cff46":"## define some parameter","9b9745bc":"# categorical encoding","a425e767":"![no-tweet-blink-cursor.gif](attachment:no-tweet-blink-cursor.gif)","559cab8f":"## Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types \u2013 word, character, and subword (n-gram characters) tokenization.","8fbc3326":"# Train test split","94c4a766":"# label Encoding","f5441829":"# import all dependencies","9f5d3fbe":"# train model"}}