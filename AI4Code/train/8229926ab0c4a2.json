{"cell_type":{"d0ad96f1":"code","b0ca9db1":"code","d0329383":"code","a95d5692":"code","2024737b":"code","23e68d0c":"code","e6d4de68":"code","65ba8e66":"code","6e81ba37":"code","56c52c4c":"code","dd075572":"code","c0e2974e":"code","c5ea5402":"code","30fb6f15":"code","79d576cb":"code","97347cca":"markdown","263f8f31":"markdown","cfacb862":"markdown","02c9d79b":"markdown","59314deb":"markdown","6f2a234e":"markdown"},"source":{"d0ad96f1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b0ca9db1":"import os\nimport numpy as np\nimport h5py\nimport json\nimport torch\n#from scipy.misc import imread, imresize\nfrom skimage.io import imread\nfrom skimage.transform import resize as imresize\n\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom random import seed, choice, sample\n\n\n\ndef init_embedding(embeddings):\n    \"\"\"\n    Fills embedding tensor with values from the uniform distribution.\n\n    :param embeddings: embedding tensor\n    \"\"\"\n    bias = np.sqrt(3.0 \/ embeddings.size(1))\n    torch.nn.init.uniform_(embeddings, -bias, bias)\n\n\ndef load_embeddings(emb_file, word_map):\n    \"\"\"\n    Creates an embedding tensor for the specified word map, for loading into the model.\n\n    :param emb_file: file containing embeddings (stored in GloVe format)\n    :param word_map: word map\n    :return: embeddings in the same order as the words in the word map, dimension of embeddings\n    \"\"\"\n\n    # Find embedding dimension\n    with open(emb_file, 'r') as f:\n        emb_dim = len(f.readline().split(' ')) - 1\n\n    vocab = set(word_map.keys())\n\n    # Create tensor to hold embeddings, initialize\n    embeddings = torch.FloatTensor(len(vocab), emb_dim)\n    init_embedding(embeddings)\n\n    # Read embedding file\n    print(\"\\nLoading embeddings...\")\n    for line in open(emb_file, 'r'):\n        line = line.split(' ')\n\n        emb_word = line[0]\n        embedding = list(map(lambda t: float(t), filter(lambda n: n and not n.isspace(), line[1:])))\n\n        # Ignore word if not in train_vocab\n        if emb_word not in vocab:\n            continue\n\n        embeddings[word_map[emb_word]] = torch.FloatTensor(embedding)\n\n    return embeddings, emb_dim\n\n\ndef clip_gradient(optimizer, grad_clip):\n    \"\"\"\n    Clips gradients computed during backpropagation to avoid explosion of gradients.\n\n    :param optimizer: optimizer with the gradients to be clipped\n    :param grad_clip: clip value\n    \"\"\"\n    for group in optimizer.param_groups:\n        for param in group['params']:\n            if param.grad is not None:\n                param.grad.data.clamp_(-grad_clip, grad_clip)\n\n\ndef save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer,\n                    bleu4, is_best):\n    \"\"\"\n    Saves model checkpoint.\n\n    :param data_name: base name of processed dataset\n    :param epoch: epoch number\n    :param epochs_since_improvement: number of epochs since last improvement in BLEU-4 score\n    :param encoder: encoder model\n    :param decoder: decoder model\n    :param encoder_optimizer: optimizer to update encoder's weights, if fine-tuning\n    :param decoder_optimizer: optimizer to update decoder's weights\n    :param bleu4: validation BLEU-4 score for this epoch\n    :param is_best: is this checkpoint the best so far?\n    \"\"\"\n    state = {'epoch': epoch,\n             'epochs_since_improvement': epochs_since_improvement,\n             'bleu-4': bleu4,\n             'encoder': encoder,\n             'decoder': decoder,\n             'encoder_optimizer': encoder_optimizer,\n             'decoder_optimizer': decoder_optimizer}\n    filename = 'checkpoint_' + data_name + '.pth.tar'\n    torch.save(state, '..\/'+filename)\n    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n    if is_best:\n        torch.save(state, '..\/BEST_' + filename)\n\n\nclass AverageMeter(object):\n    \"\"\"\n    Keeps track of most recent, average, sum, and count of a metric.\n    \"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n\ndef adjust_learning_rate(optimizer, shrink_factor):\n    \"\"\"\n    Shrinks learning rate by a specified factor.\n\n    :param optimizer: optimizer whose learning rate must be shrunk.\n    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n    \"\"\"\n\n    print(\"\\nDECAYING learning rate.\")\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = param_group['lr'] * shrink_factor\n    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n\n\ndef accuracy(scores, targets, k):\n    \"\"\"\n    Computes top-k accuracy, from predicted and true labels.\n\n    :param scores: scores from the model\n    :param targets: true labels\n    :param k: k in top-k accuracy\n    :return: top-k accuracy\n    \"\"\"\n\n    batch_size = targets.size(0)\n    _, ind = scores.topk(k, 1, True, True)\n    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n    correct_total = correct.view(-1).float().sum()  # 0D tensor\n    return correct_total.item() * (100.0 \/ batch_size)\n","d0329383":"import torch\nfrom torch.utils.data import Dataset\nimport h5py\nimport json\nimport os\n\n\nclass CaptionDataset(Dataset):\n    \"\"\"\n    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n    \"\"\"\n\n    def __init__(self, data_folder, data_name, split, transform=None):\n        \"\"\"\n        :param data_folder: folder where data files are stored\n        :param data_name: base name of processed datasets\n        :param split: split, one of 'TRAIN', 'VAL', or 'TEST'\n        :param transform: image transform pipeline\n        \"\"\"\n        self.split = split\n        assert self.split in {'TRAIN', 'VAL', 'TEST'}\n\n        # Open hdf5 file where images are stored\n        self.h = h5py.File(os.path.join(data_folder, self.split + '_IMAGES_' + data_name + '.hdf5'), 'r')\n        self.imgs = self.h['images']\n\n        # Captions per image\n        self.cpi = self.h.attrs['captions_per_image']\n\n        # Load encoded captions (completely into memory)\n        with open(os.path.join(data_folder, self.split + '_CAPTIONS_' + data_name + '.json'), 'r') as j:\n            self.captions = json.load(j)\n\n        # Load caption lengths (completely into memory)\n        with open(os.path.join(data_folder, self.split + '_CAPLENS_' + data_name + '.json'), 'r') as j:\n            self.caplens = json.load(j)\n\n        # PyTorch transformation pipeline for the image (normalizing, etc.)\n        self.transform = transform\n\n        # Total number of datapoints\n        self.dataset_size = len(self.captions)\n\n    def __getitem__(self, i):\n        # Remember, the Nth caption corresponds to the (N \/\/ captions_per_image)th image\n        img = torch.FloatTensor(self.imgs[i \/\/ self.cpi] \/ 255.)\n        if self.transform is not None:\n            img = self.transform(img)\n\n        caption = torch.LongTensor(self.captions[i])\n\n        caplen = torch.LongTensor([self.caplens[i]])\n\n        if self.split is 'TRAIN':\n            return img, caption, caplen\n        else:\n            # For validation of testing, also return all 'captions_per_image' captions to find BLEU-4 score\n            all_captions = torch.LongTensor(\n                self.captions[((i \/\/ self.cpi) * self.cpi):(((i \/\/ self.cpi) * self.cpi) + self.cpi)])\n            return img, caption, caplen, all_captions\n\n    def __len__(self):\n        return self.dataset_size\n","a95d5692":"import torch\nfrom torch import nn\nimport torchvision\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass Encoder(nn.Module):\n    \"\"\"\n    Encoder.\n    \"\"\"\n\n    def __init__(self, encoded_image_size=14):\n        super(Encoder, self).__init__()\n        self.enc_image_size = encoded_image_size\n\n        vgg19 = torchvision.models.vgg19(pretrained=True)  # pretrained ImageNet VGG19\n\n        # Remove linear and pool layers (since we're not doing classification)\n        modules = list(vgg19.children())[0][:-1]\n        self.vgg19 = nn.Sequential(*modules)\n\n        self.fine_tune()\n\n    def forward(self, images):\n        \"\"\"\n        Forward propagation.\n\n        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n        :return: encoded images\n        \"\"\"\n        out = self.vgg19(images)  # (batch_size, 512, image_size\/32, image_size\/32)\n        out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 512)\n        return out\n\n    def fine_tune(self, fine_tune=True):\n        \"\"\"\n        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n\n        :param fine_tune: Allow?\n        \"\"\"\n        for p in self.vgg19.parameters():\n            p.requires_grad = False\n        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n        # for c in list(self.vgg19.children())[5:]:\n        #     for p in c.parameters():\n        #         p.requires_grad = fine_tune\n\n\nclass Attention(nn.Module):\n    \"\"\"\n    Attention Network.\n    \"\"\"\n\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        \"\"\"\n        :param encoder_dim: feature size of encoded images\n        :param decoder_dim: size of decoder's RNN\n        :param attention_dim: size of the attention network\n        \"\"\"\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n        #self.relu = nn.ReLU()\n        self.tanh = nn.Tanh()\n        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n\n    def forward(self, encoder_out, decoder_hidden):\n        \"\"\"\n        Forward propagation.\n\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n        :return: attention weighted encoding, weights\n        \"\"\"\n        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n        #att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n        att = self.full_att(self.tanh(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n        alpha = self.softmax(att)  # (batch_size, num_pixels)\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n\n        return attention_weighted_encoding, alpha\n\n\nclass DecoderWithAttention(nn.Module):\n    \"\"\"\n    Decoder.\n    \"\"\"\n\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=512, dropout=0.5):\n        \"\"\"\n        :param attention_dim: size of attention network\n        :param embed_dim: embedding size\n        :param decoder_dim: size of decoder's RNN\n        :param vocab_size: size of vocabulary\n        :param encoder_dim: feature size of encoded images\n        :param dropout: dropout\n        \"\"\"\n        super(DecoderWithAttention, self).__init__()\n\n        self.encoder_dim = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.decoder_dim = decoder_dim\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n\n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n\n        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n        self.dropout = nn.Dropout(p=self.dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n        self.init_weights()  # initialize some layers with the uniform distribution\n\n    def init_weights(self):\n        \"\"\"\n        Initializes some parameters with values from the uniform distribution, for easier convergence.\n        \"\"\"\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n\n    def load_pretrained_embeddings(self, embeddings):\n        \"\"\"\n        Loads embedding layer with pre-trained embeddings.\n\n        :param embeddings: pre-trained embeddings\n        \"\"\"\n        self.embedding.weight = nn.Parameter(embeddings)\n\n    def fine_tune_embeddings(self, fine_tune=True):\n        \"\"\"\n        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n\n        :param fine_tune: Allow?\n        \"\"\"\n        for p in self.embedding.parameters():\n            p.requires_grad = fine_tune\n\n    def init_hidden_state(self, encoder_out):\n        \"\"\"\n        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n        :return: hidden state, cell state\n        \"\"\"\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n        \"\"\"\n        Forward propagation.\n\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n        \"\"\"\n\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n\n        # Flatten image\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n\n        # Sort input data by decreasing lengths; why? apparent below\n        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n        encoder_out = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n\n        # Embedding\n        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n\n        # Initialize LSTM state\n        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n\n        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n        # So, decoding lengths are actual lengths - 1\n        decode_lengths = (caption_lengths - 1).tolist()\n\n        # Create tensors to hold word predicion scores and alphas\n        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n\n        # At each time-step, decode by\n        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n                                                                h[:batch_size_t])\n            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n            attention_weighted_encoding = gate * attention_weighted_encoding\n            h, c = self.decode_step(\n                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:batch_size_t, t, :] = preds\n            alphas[:batch_size_t, t, :] = alpha\n\n        return predictions, encoded_captions, decode_lengths, alphas, sort_ind","2024737b":"import time\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nfrom torch import nn\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom tqdm import tqdm as tqdm\n\n# Data parameters\ndata_folder = '\/kaggle\/input\/data_processing_first_step'  # folder with data files saved by create_input_files.py\ndata_name = 'flickr8k_5_cap_per_img_5_min_word_freq'  # base name shared by data files\n\n# Model parameters\nemb_dim = 512  # dimension of word embeddings\nattention_dim = 512  # dimension of attention linear layers\ndecoder_dim = 512  # dimension of decoder RNN\ndropout = 0.5\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\ncudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n\n# Training parameters\nstart_epoch = 0\nepochs = 120 #120  # number of epochs to train for (if early stopping is not triggered)\nepochs_since_improvement = 0  # keeps track of number of epochs since there's been an improvement in validation BLEU\nbatch_size = 64\nworkers = 1  # for data-loading; right now, only 1 works with h5py\nencoder_lr = 1e-4  # learning rate for encoder if fine-tuning\ndecoder_lr = 4e-4  # learning rate for decoder\ngrad_clip = 5.  # clip gradients at an absolute value of\nalpha_c = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper\nbest_bleu4 = 0.  # BLEU-4 score right now\nprint_freq = 100  # print training\/validation stats every __ batches\nfine_tune_encoder = False  # fine-tune encoder?\ncheckpoint = None  # path to checkpoint, None if none\n\n\ndef main_train():\n    \"\"\"\n    Training and validation.\n    \"\"\"\n\n    global best_bleu4, epochs_since_improvement, checkpoint, start_epoch, fine_tune_encoder, data_name, word_map\n\n    # Read word map\n    word_map_file = os.path.join(data_folder, 'WORDMAP_' + data_name + '.json')\n    with open(word_map_file, 'r') as j:\n        word_map = json.load(j)\n\n    # Initialize \/ load checkpoint\n    if checkpoint is None:\n        decoder = DecoderWithAttention(attention_dim=attention_dim,\n                                       embed_dim=emb_dim,\n                                       decoder_dim=decoder_dim,\n                                       vocab_size=len(word_map),\n                                       dropout=dropout)\n        decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n                                             lr=decoder_lr)\n        encoder = Encoder()\n        encoder.fine_tune(fine_tune_encoder)\n        encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n                                             lr=encoder_lr) if fine_tune_encoder else None\n\n    else:\n        checkpoint = torch.load(checkpoint)\n        start_epoch = checkpoint['epoch'] + 1\n        epochs_since_improvement = checkpoint['epochs_since_improvement']\n        best_bleu4 = checkpoint['bleu-4']\n        decoder = checkpoint['decoder']\n        decoder_optimizer = checkpoint['decoder_optimizer']\n        encoder = checkpoint['encoder']\n        encoder_optimizer = checkpoint['encoder_optimizer']\n        if fine_tune_encoder is True and encoder_optimizer is None:\n            encoder.fine_tune(fine_tune_encoder)\n            encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n                                                 lr=encoder_lr)\n\n    # Move to GPU, if available\n    decoder = decoder.to(device)\n    encoder = encoder.to(device)\n\n    # Loss function\n    criterion = nn.CrossEntropyLoss().to(device)\n\n    # Custom dataloaders\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n    train_loader = torch.utils.data.DataLoader(\n        CaptionDataset(data_folder, data_name, 'TRAIN', transform=transforms.Compose([normalize])),\n        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n    val_loader = torch.utils.data.DataLoader(\n        CaptionDataset(data_folder, data_name, 'VAL', transform=transforms.Compose([normalize])),\n        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n\n    # Epochs\n    for epoch in range(start_epoch, epochs):\n\n        # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n        if epochs_since_improvement == 20:\n            break\n        if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n            adjust_learning_rate(decoder_optimizer, 0.8)\n            if fine_tune_encoder:\n                adjust_learning_rate(encoder_optimizer, 0.8)\n\n        # One epoch's training\n        train(train_loader=train_loader,\n              encoder=encoder,\n              decoder=decoder,\n              criterion=criterion,\n              encoder_optimizer=encoder_optimizer,\n              decoder_optimizer=decoder_optimizer,\n              epoch=epoch)\n    \n        # One epoch's validation\n        recent_bleu4 = validate(val_loader=val_loader,\n                                encoder=encoder,\n                                decoder=decoder,\n                                criterion=criterion)\n\n        # Check if there was an improvement\n        is_best = recent_bleu4 > best_bleu4\n        best_bleu4 = max(recent_bleu4, best_bleu4)\n\n        if not is_best:\n            epochs_since_improvement += 1\n            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n        else:\n            epochs_since_improvement = 0\n\n        # Save checkpoint\n        save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer,\n                        decoder_optimizer, recent_bleu4, is_best)\n\n\ndef train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n    \"\"\"\n    Performs one epoch's training.\n\n    :param train_loader: DataLoader for training data\n    :param encoder: encoder model\n    :param decoder: decoder model\n    :param criterion: loss layer\n    :param encoder_optimizer: optimizer to update encoder's weights (if fine-tuning)\n    :param decoder_optimizer: optimizer to update decoder's weights\n    :param epoch: epoch number\n    \"\"\"\n\n    decoder.train()  # train mode (dropout and batchnorm is used)\n    encoder.train()\n\n    batch_time = AverageMeter()  # forward prop. + back prop. time\n    data_time = AverageMeter()  # data loading time\n    losses = AverageMeter()  # loss (per word decoded)\n    top5accs = AverageMeter()  # top5 accuracy\n\n    start = time.time()\n\n    # Batches\n    for i, (imgs, caps, caplens) in enumerate(tqdm(train_loader)):\n        data_time.update(time.time() - start)\n\n        # Move to GPU, if available\n        imgs = imgs.to(device)\n        caps = caps.to(device)\n        caplens = caplens.to(device)\n\n        # Forward prop.\n        imgs = encoder(imgs)\n        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n\n        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n        targets = caps_sorted[:, 1:]\n\n        # Remove timesteps that we didn't decode at, or are pads\n        # pack_padded_sequence is an easy trick to do this\n        scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n        targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n\n        # Calculate loss\n        loss = criterion(scores, targets)\n\n        # Add doubly stochastic attention regularization\n        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n\n        # Back prop.\n        decoder_optimizer.zero_grad()\n        if encoder_optimizer is not None:\n            encoder_optimizer.zero_grad()\n        loss.backward()\n\n        # Clip gradients\n        if grad_clip is not None:\n            clip_gradient(decoder_optimizer, grad_clip)\n            if encoder_optimizer is not None:\n                clip_gradient(encoder_optimizer, grad_clip)\n\n        # Update weights\n        decoder_optimizer.step()\n        if encoder_optimizer is not None:\n            encoder_optimizer.step()\n\n        # Keep track of metrics\n        top5 = accuracy(scores, targets, 5)\n        losses.update(loss.item(), sum(decode_lengths))\n        top5accs.update(top5, sum(decode_lengths))\n        batch_time.update(time.time() - start)\n\n        start = time.time()\n\n        # Print status\n        if i % print_freq == 0:\n            print('Epoch: [{0}][{1}\/{2}]\\t'\n                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n                                                                          batch_time=batch_time,\n                                                                          data_time=data_time, loss=losses,\n                                                                          top5=top5accs))\n\n\ndef validate(val_loader, encoder, decoder, criterion):\n    \"\"\"\n    Performs one epoch's validation.\n\n    :param val_loader: DataLoader for validation data.\n    :param encoder: encoder model\n    :param decoder: decoder model\n    :param criterion: loss layer\n    :return: BLEU-4 score\n    \"\"\"\n    decoder.eval()  # eval mode (no dropout or batchnorm)\n    if encoder is not None:\n        encoder.eval()\n\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top5accs = AverageMeter()\n\n    start = time.time()\n\n    references = list()  # references (true captions) for calculating BLEU-4 score\n    hypotheses = list()  # hypotheses (predictions)\n\n    # explicitly disable gradient calculation to avoid CUDA memory error\n    # solves the issue #57\n    with torch.no_grad():\n        # Batches\n        for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n\n            # Move to device, if available\n            imgs = imgs.to(device)\n            caps = caps.to(device)\n            caplens = caplens.to(device)\n\n            # Forward prop.\n            if encoder is not None:\n                imgs = encoder(imgs)\n            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n\n            # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n            targets = caps_sorted[:, 1:]\n\n            # Remove timesteps that we didn't decode at, or are pads\n            # pack_padded_sequence is an easy trick to do this\n            scores_copy = scores.clone()\n            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n\n            # Calculate loss\n            loss = criterion(scores, targets)\n\n            # Add doubly stochastic attention regularization\n            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n\n            # Keep track of metrics\n            losses.update(loss.item(), sum(decode_lengths))\n            top5 = accuracy(scores, targets, 5)\n            top5accs.update(top5, sum(decode_lengths))\n            batch_time.update(time.time() - start)\n\n            start = time.time()\n\n            if i % print_freq == 0:\n                print('Validation: [{0}\/{1}]\\t'\n                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n                                                                                loss=losses, top5=top5accs))\n\n            # Store references (true captions), and hypothesis (prediction) for each image\n            # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n            # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n\n            # References\n            allcaps = allcaps[sort_ind]  # because images were sorted in the decoder\n            for j in range(allcaps.shape[0]):\n                img_caps = allcaps[j].tolist()\n                img_captions = list(\n                    map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<pad>']}],\n                        img_caps))  # remove <start> and pads\n                references.append(img_captions)\n\n            # Hypotheses\n            _, preds = torch.max(scores_copy, dim=2)\n            preds = preds.tolist()\n            temp_preds = list()\n            for j, p in enumerate(preds):\n                temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads\n            preds = temp_preds\n            hypotheses.extend(preds)\n\n            assert len(references) == len(hypotheses)\n\n        # Calculate BLEU-4 scores\n        bleu4 = corpus_bleu(references, hypotheses)\n\n        print(\n            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n'.format(\n                loss=losses,\n                top5=top5accs,\n                bleu=bleu4))\n\n    return bleu4","23e68d0c":"main_train()","e6d4de68":"import numpy as np","65ba8e66":"# Natural Language Toolkit: Machine Translation\n#\n# Copyright (C) 2001-2020 NLTK Project\n# Author: Uday Krishna <udaykrishna5@gmail.com>\n# URL: <http:\/\/nltk.org\/>\n# For license information, see LICENSE.TXT\n\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import wordnet\nfrom itertools import chain, product\nimport numpy as np\n\n\ndef _generate_enums(hypothesis, reference, preprocess=str.lower):\n    \"\"\"\n    Takes in string inputs for hypothesis and reference and returns\n    enumerated word lists for each of them\n\n    :param hypothesis: hypothesis string\n    :type hypothesis: str\n    :param reference: reference string\n    :type reference: str\n    :preprocess: preprocessing method (default str.lower)\n    :type preprocess: method\n    :return: enumerated words list\n    :rtype: list of 2D tuples, list of 2D tuples\n    \"\"\"\n    hypothesis_list = list(enumerate(preprocess(hypothesis).split()))\n    reference_list = list(enumerate(preprocess(reference).split()))\n    return hypothesis_list, reference_list\n\n\ndef exact_match(hypothesis, reference):\n    \"\"\"\n    matches exact words in hypothesis and reference\n    and returns a word mapping based on the enumerated\n    word id between hypothesis and reference\n\n    :param hypothesis: hypothesis string\n    :type hypothesis: str\n    :param reference: reference string\n    :type reference: str\n    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n             enumerated unmatched reference tuples\n    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n    \"\"\"\n    hypothesis_list, reference_list = _generate_enums(hypothesis, reference)\n    return _match_enums(hypothesis_list, reference_list)\n\n\n\ndef _match_enums(enum_hypothesis_list, enum_reference_list):\n    \"\"\"\n    matches exact words in hypothesis and reference and returns\n    a word mapping between enum_hypothesis_list and enum_reference_list\n    based on the enumerated word id.\n\n    :param enum_hypothesis_list: enumerated hypothesis list\n    :type enum_hypothesis_list: list of tuples\n    :param enum_reference_list: enumerated reference list\n    :type enum_reference_list: list of 2D tuples\n    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n             enumerated unmatched reference tuples\n    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n    \"\"\"\n    word_match = []\n    for i in range(len(enum_hypothesis_list))[::-1]:\n        for j in range(len(enum_reference_list))[::-1]:\n            if enum_hypothesis_list[i][1] == enum_reference_list[j][1]:\n                word_match.append(\n                    (enum_hypothesis_list[i][0], enum_reference_list[j][0])\n                )\n                (enum_hypothesis_list.pop(i)[1], enum_reference_list.pop(j)[1])\n                break\n    return word_match, enum_hypothesis_list, enum_reference_list\n\n\ndef _enum_stem_match(\n    enum_hypothesis_list, enum_reference_list, stemmer=PorterStemmer()\n):\n    \"\"\"\n    Stems each word and matches them in hypothesis and reference\n    and returns a word mapping between enum_hypothesis_list and\n    enum_reference_list based on the enumerated word id. The function also\n    returns a enumerated list of unmatched words for hypothesis and reference.\n\n    :param enum_hypothesis_list:\n    :type enum_hypothesis_list:\n    :param enum_reference_list:\n    :type enum_reference_list:\n    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n             enumerated unmatched reference tuples\n    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n    \"\"\"\n    stemmed_enum_list1 = [\n        (word_pair[0], stemmer.stem(word_pair[1])) for word_pair in enum_hypothesis_list\n    ]\n\n    stemmed_enum_list2 = [\n        (word_pair[0], stemmer.stem(word_pair[1])) for word_pair in enum_reference_list\n    ]\n\n    word_match, enum_unmat_hypo_list, enum_unmat_ref_list = _match_enums(\n        stemmed_enum_list1, stemmed_enum_list2\n    )\n\n    enum_unmat_hypo_list = (\n        list(zip(*enum_unmat_hypo_list)) if len(enum_unmat_hypo_list) > 0 else []\n    )\n\n    enum_unmat_ref_list = (\n        list(zip(*enum_unmat_ref_list)) if len(enum_unmat_ref_list) > 0 else []\n    )\n\n    enum_hypothesis_list = list(\n        filter(lambda x: x[0] not in enum_unmat_hypo_list, enum_hypothesis_list)\n    )\n\n    enum_reference_list = list(\n        filter(lambda x: x[0] not in enum_unmat_ref_list, enum_reference_list)\n    )\n\n    return word_match, enum_hypothesis_list, enum_reference_list\n\n\ndef stem_match(hypothesis, reference, stemmer=PorterStemmer()):\n    \"\"\"\n    Stems each word and matches them in hypothesis and reference\n    and returns a word mapping between hypothesis and reference\n\n    :param hypothesis:\n    :type hypothesis:\n    :param reference:\n    :type reference:\n    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n    :type stemmer: nltk.stem.api.StemmerI or any class that\n                   implements a stem method\n    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n             enumerated unmatched reference tuples\n    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n    \"\"\"\n    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n    return _enum_stem_match(enum_hypothesis_list, enum_reference_list, stemmer=stemmer)\n\n\n\ndef _enum_wordnetsyn_match(enum_hypothesis_list, enum_reference_list, wordnet=wordnet):\n    \"\"\"\n    Matches each word in reference to a word in hypothesis\n    if any synonym of a hypothesis word is the exact match\n    to the reference word.\n\n    :param enum_hypothesis_list: enumerated hypothesis list\n    :param enum_reference_list: enumerated reference list\n    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n    :type wordnet: WordNetCorpusReader\n    :return: list of matched tuples, unmatched hypothesis list, unmatched reference list\n    :rtype:  list of tuples, list of tuples, list of tuples\n\n    \"\"\"\n    word_match = []\n    for i in range(len(enum_hypothesis_list))[::-1]:\n        hypothesis_syns = set(\n            chain(\n                *[\n                    [\n                        lemma.name()\n                        for lemma in synset.lemmas()\n                        if lemma.name().find(\"_\") < 0\n                    ]\n                    for synset in wordnet.synsets(enum_hypothesis_list[i][1])\n                ]\n            )\n        ).union({enum_hypothesis_list[i][1]})\n        for j in range(len(enum_reference_list))[::-1]:\n            if enum_reference_list[j][1] in hypothesis_syns:\n                word_match.append(\n                    (enum_hypothesis_list[i][0], enum_reference_list[j][0])\n                )\n                enum_hypothesis_list.pop(i), enum_reference_list.pop(j)\n                break\n    return word_match, enum_hypothesis_list, enum_reference_list\n\n\ndef wordnetsyn_match(hypothesis, reference, wordnet=wordnet):\n    \"\"\"\n    Matches each word in reference to a word in hypothesis if any synonym\n    of a hypothesis word is the exact match to the reference word.\n\n    :param hypothesis: hypothesis string\n    :param reference: reference string\n    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n    :type wordnet: WordNetCorpusReader\n    :return: list of mapped tuples\n    :rtype: list of tuples\n    \"\"\"\n    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n    return _enum_wordnetsyn_match(\n        enum_hypothesis_list, enum_reference_list, wordnet=wordnet\n    )\n\n\n\ndef _enum_allign_words(\n    enum_hypothesis_list, enum_reference_list, stemmer=PorterStemmer(), wordnet=wordnet\n):\n    \"\"\"\n    Aligns\/matches words in the hypothesis to reference by sequentially\n    applying exact match, stemmed match and wordnet based synonym match.\n    in case there are multiple matches the match which has the least number\n    of crossing is chosen. Takes enumerated list as input instead of\n    string input\n\n    :param enum_hypothesis_list: enumerated hypothesis list\n    :param enum_reference_list: enumerated reference list\n    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n    :type wordnet: WordNetCorpusReader\n    :return: sorted list of matched tuples, unmatched hypothesis list,\n             unmatched reference list\n    :rtype: list of tuples, list of tuples, list of tuples\n    \"\"\"\n    exact_matches, enum_hypothesis_list, enum_reference_list = _match_enums(\n        enum_hypothesis_list, enum_reference_list\n    )\n\n    stem_matches, enum_hypothesis_list, enum_reference_list = _enum_stem_match(\n        enum_hypothesis_list, enum_reference_list, stemmer=stemmer\n    )\n\n    wns_matches, enum_hypothesis_list, enum_reference_list = _enum_wordnetsyn_match(\n        enum_hypothesis_list, enum_reference_list, wordnet=wordnet\n    )\n\n    return (\n        sorted(\n            exact_matches + stem_matches + wns_matches, key=lambda wordpair: wordpair[0]\n        ),\n        enum_hypothesis_list,\n        enum_reference_list,\n    )\n\n\ndef allign_words(hypothesis, reference, stemmer=PorterStemmer(), wordnet=wordnet):\n    \"\"\"\n    Aligns\/matches words in the hypothesis to reference by sequentially\n    applying exact match, stemmed match and wordnet based synonym match.\n    In case there are multiple matches the match which has the least number\n    of crossing is chosen.\n\n    :param hypothesis: hypothesis string\n    :param reference: reference string\n    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n    :type wordnet: WordNetCorpusReader\n    :return: sorted list of matched tuples, unmatched hypothesis list, unmatched reference list\n    :rtype: list of tuples, list of tuples, list of tuples\n    \"\"\"\n    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n    return _enum_allign_words(\n        enum_hypothesis_list, enum_reference_list, stemmer=stemmer, wordnet=wordnet\n    )\n\n\n\ndef _count_chunks(matches):\n    \"\"\"\n    Counts the fewest possible number of chunks such that matched unigrams\n    of each chunk are adjacent to each other. This is used to caluclate the\n    fragmentation part of the metric.\n\n    :param matches: list containing a mapping of matched words (output of allign_words)\n    :return: Number of chunks a sentence is divided into post allignment\n    :rtype: int\n    \"\"\"\n    i = 0\n    chunks = 1\n    while i < len(matches) - 1:\n        if (matches[i + 1][0] == matches[i][0] + 1) and (\n            matches[i + 1][1] == matches[i][1] + 1\n        ):\n            i += 1\n            continue\n        i += 1\n        chunks += 1\n    return chunks\n\n\ndef single_meteor_score(\n    reference,\n    hypothesis,\n    preprocess=str.lower,\n    stemmer=PorterStemmer(),\n    wordnet=wordnet,\n    alpha=0.9,\n    beta=3,\n    gamma=0.5,\n):\n    \"\"\"\n    Calculates METEOR score for single hypothesis and reference as per\n    \"Meteor: An Automatic Metric for MT Evaluation with HighLevels of\n    Correlation with Human Judgments\" by Alon Lavie and Abhaya Agarwal,\n    in Proceedings of ACL.\n    http:\/\/www.cs.cmu.edu\/~alavie\/METEOR\/pdf\/Lavie-Agarwal-2007-METEOR.pdf\n\n\n    >>> hypothesis1 = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n\n    >>> reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'\n\n\n    >>> round(single_meteor_score(reference1, hypothesis1),4)\n    0.7398\n\n        If there is no words match during the alignment the method returns the\n        score as 0. We can safely  return a zero instead of raising a\n        division by zero error as no match usually implies a bad translation.\n\n    >>> round(meteor_score('this is a cat', 'non matching hypothesis'),4)\n    0.0\n\n    :param references: reference sentences\n    :type references: list(str)\n    :param hypothesis: a hypothesis sentence\n    :type hypothesis: str\n    :param preprocess: preprocessing function (default str.lower)\n    :type preprocess: method\n    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n    :type wordnet: WordNetCorpusReader\n    :param alpha: parameter for controlling relative weights of precision and recall.\n    :type alpha: float\n    :param beta: parameter for controlling shape of penalty as a\n                 function of as a function of fragmentation.\n    :type beta: float\n    :param gamma: relative weight assigned to fragmentation penality.\n    :type gamma: float\n    :return: The sentence-level METEOR score.\n    :rtype: float\n    \"\"\"\n    enum_hypothesis, enum_reference = _generate_enums(\n        hypothesis, reference, preprocess=preprocess\n    )\n    translation_length = len(enum_hypothesis)\n    reference_length = len(enum_reference)\n    matches, _, _ = _enum_allign_words(enum_hypothesis, enum_reference, stemmer=stemmer)\n    matches_count = len(matches)\n    try:\n        precision = float(matches_count) \/ translation_length\n        recall = float(matches_count) \/ reference_length\n        fmean = (precision * recall) \/ (alpha * precision + (1 - alpha) * recall)\n        chunk_count = float(_count_chunks(matches))\n        frag_frac = chunk_count \/ matches_count\n    except ZeroDivisionError:\n        return 0.0\n    penalty = gamma * frag_frac ** beta\n    return (1 - penalty) * fmean\n\n\n\ndef meteor_score(\n    references,\n    hypothesis,\n    preprocess=str.lower,\n    stemmer=PorterStemmer(),\n    wordnet=wordnet,\n    alpha=0.9,\n    beta=3,\n    gamma=0.5,\n):\n    \"\"\"\n    Calculates METEOR score for hypothesis with multiple references as\n    described in \"Meteor: An Automatic Metric for MT Evaluation with\n    HighLevels of Correlation with Human Judgments\" by Alon Lavie and\n    Abhaya Agarwal, in Proceedings of ACL.\n    http:\/\/www.cs.cmu.edu\/~alavie\/METEOR\/pdf\/Lavie-Agarwal-2007-METEOR.pdf\n\n\n    In case of multiple references the best score is chosen. This method\n    iterates over single_meteor_score and picks the best pair among all\n    the references for a given hypothesis\n\n    >>> hypothesis1 = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n    >>> hypothesis2 = 'It is to insure the troops forever hearing the activity guidebook that party direct'\n\n    >>> reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'\n    >>> reference2 = 'It is the guiding principle which guarantees the military forces always being under the command of the Party'\n    >>> reference3 = 'It is the practical guide for the army always to heed the directions of the party'\n\n    >>> round(meteor_score([reference1, reference2, reference3], hypothesis1),4)\n    0.7398\n\n        If there is no words match during the alignment the method returns the\n        score as 0. We can safely  return a zero instead of raising a\n        division by zero error as no match usually implies a bad translation.\n\n    >>> round(meteor_score(['this is a cat'], 'non matching hypothesis'),4)\n    0.0\n\n    :param references: reference sentences\n    :type references: list(str)\n    :param hypothesis: a hypothesis sentence\n    :type hypothesis: str\n    :param preprocess: preprocessing function (default str.lower)\n    :type preprocess: method\n    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n    :type wordnet: WordNetCorpusReader\n    :param alpha: parameter for controlling relative weights of precision and recall.\n    :type alpha: float\n    :param beta: parameter for controlling shape of penalty as a function\n                 of as a function of fragmentation.\n    :type beta: float\n    :param gamma: relative weight assigned to fragmentation penality.\n    :type gamma: float\n    :return: The sentence-level METEOR score.\n    :rtype: float\n    \"\"\"\n    return max(\n        [\n            single_meteor_score(\n                reference,\n                hypothesis,\n                stemmer=stemmer,\n                wordnet=wordnet,\n                alpha=alpha,\n                beta=beta,\n                gamma=gamma,\n            )\n            for reference in references\n        ]\n    )\n\ndef corpus_meteor(references, hypothesis):\n    \"\"\"\n    references: list of list string.\n    hypothesis: list of string.\n    \"\"\"\n    scores = np.zeros((len(hypothesis)))\n    for i, (ref, hypo) in enumerate(zip(references, hypothesis)):\n        scores[i] = meteor_score(ref, hypo)\n    score = scores.mean()\n    return score","6e81ba37":"import torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nfrom nltk.translate.bleu_score import corpus_bleu\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\n# Parameters\ndata_folder = '\/kaggle\/input\/data_processing_first_step'  # folder with data files saved by create_input_files.py\ndata_name = 'flickr8k_5_cap_per_img_5_min_word_freq'  # base name shared by data files\ncheckpoint = '..\/BEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar'  # model checkpoint\nword_map_file = '\/kaggle\/input\/data_processing_first_step\/WORDMAP_flickr8k_5_cap_per_img_5_min_word_freq.json'  # word map, ensure it's the same the data was encoded with and the model was trained with\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\ncudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n\n# Load model\ncheckpoint = torch.load(checkpoint)\ndecoder = checkpoint['decoder']\ndecoder = decoder.to(device)\ndecoder.eval()\nencoder = checkpoint['encoder']\nencoder = encoder.to(device)\nencoder.eval()\n\n# Load word map (word2ix)\nwith open(word_map_file, 'r') as j:\n    word_map = json.load(j)\nrev_word_map = {v: k for k, v in word_map.items()}\nvocab_size = len(word_map)\n# Normalization transform\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n\ndef evaluate(beam_size):\n    \"\"\"\n    Evaluation\n\n    :param beam_size: beam size at which to generate captions for evaluation\n    :return: BLEU-4 score\n    \"\"\"\n    # DataLoader\n    loader = torch.utils.data.DataLoader(\n        CaptionDataset(data_folder, data_name, 'TEST', transform=transforms.Compose([normalize])),\n        batch_size=1, shuffle=True, num_workers=1, pin_memory=True)\n\n    # TODO: Batched Beam Search\n    # Therefore, do not use a batch_size greater than 1 - IMPORTANT!\n\n    # Lists to store references (true captions), and hypothesis (prediction) for each image\n    # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n    # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n    references = list()\n    hypotheses = list()\n\n    references_meteor = list()\n    hypotheses_meteor = list()\n\n    # For each image\n    for i, (image, caps, caplens, allcaps) in enumerate(\n            tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size))):\n        k = beam_size\n\n        # Move to GPU device, if available\n        image = image.to(device)  # (1, 3, 256, 256)\n\n        # Encode\n        encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n        enc_image_size = encoder_out.size(1)\n        encoder_dim = encoder_out.size(3)\n\n        # Flatten encoding\n        encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n\n        # We'll treat the problem as having a batch size of k\n        encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n\n        # Tensor to store top k previous words at each step; now they're just <start>\n        k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n\n        # Tensor to store top k sequences; now they're just <start>\n        seqs = k_prev_words  # (k, 1)\n\n        # Tensor to store top k sequences' scores; now they're just 0\n        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n\n        # Lists to store completed sequences and scores\n        complete_seqs = list()\n        complete_seqs_scores = list()\n\n        # Start decoding\n        step = 1\n        h, c = decoder.init_hidden_state(encoder_out)\n\n        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n        while True:\n\n            embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n\n            awe, _ = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n\n            gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n            awe = gate * awe\n\n            h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n\n            scores = decoder.fc(h)  # (s, vocab_size)\n            scores = F.log_softmax(scores, dim=1)\n\n            # Add\n            scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n\n            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n            if step == 1:\n                top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n            else:\n                # Unroll and find top scores, and their unrolled indices\n                top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n\n            # Convert unrolled indices to actual indices of scores\n            prev_word_inds = top_k_words \/ vocab_size  # (s)\n            next_word_inds = top_k_words % vocab_size  # (s)\n\n            # Add new words to sequences\n            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n\n            # Which sequences are incomplete (didn't reach <end>)?\n            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n                               next_word != word_map['<end>']]\n            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n\n            # Set aside complete sequences\n            if len(complete_inds) > 0:\n                complete_seqs.extend(seqs[complete_inds].tolist())\n                complete_seqs_scores.extend(top_k_scores[complete_inds])\n            k -= len(complete_inds)  # reduce beam length accordingly\n\n            # Proceed with incomplete sequences\n            if k == 0:\n                break\n            seqs = seqs[incomplete_inds]\n            h = h[prev_word_inds[incomplete_inds]]\n            c = c[prev_word_inds[incomplete_inds]]\n            encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n\n            # Break if things have been going on too long\n            if step > 50:\n                break\n            step += 1\n\n        i = complete_seqs_scores.index(max(complete_seqs_scores))\n        seq = complete_seqs[i]\n\n        # References\n        img_caps = allcaps[0].tolist()\n        img_captions = list(\n            map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}],\n                img_caps))  # remove <start> and pads\n        references.append(img_captions)\n\n\n        # Hypotheses\n        sentence = [w for w in seq if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}]\n        hypotheses.append(sentence)\n        \n        # meteor\n        references_meteor.append(' '.join([ ' '.join([rev_word_map[w] for w in cap]) for cap in img_captions]))\n        hypotheses_meteor.append(' '.join([rev_word_map[w] for w in sentence]))\n\n        assert len(references) == len(hypotheses)\n    \n    # Calculate BLEU-4 scores\n    bleu4 = corpus_bleu(references, hypotheses)\n    bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n    bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n    bleu3 = corpus_bleu(references, hypotheses, weights=(1\/3, 1\/3, 1\/3, 0))\n    meteor = corpus_meteor(references_meteor, hypotheses_meteor)\n\n    return bleu1, bleu2, bleu3, bleu4, meteor\n","56c52c4c":"beam_size = 10\nblue1, blue2, blue3, blue4, meteor = evaluate(beam_size)\n#blue1, blue2, blue3, blue4 = evaluate(beam_size)\nprint(\"BLUE-1: \", blue1)\nprint(\"BLUE-2: \", blue2)\nprint(\"BLUE-3: \", blue3)\nprint(\"BLUE-4: \", blue4)\nprint(\"meteor: \", meteor)","dd075572":"beam_size = 5\nblue1, blue2, blue3, blue4, meteor = evaluate(beam_size)\n#blue1, blue2, blue3, blue4 = evaluate(beam_size)\nprint(\"BLUE-1: \", blue1)\nprint(\"BLUE-2: \", blue2)\nprint(\"BLUE-3: \", blue3)\nprint(\"BLUE-4: \", blue4)\nprint(\"meteor: \", meteor)","c0e2974e":"beam_size = 4\nblue1, blue2, blue3, blue4, meteor = evaluate(beam_size)\n#blue1, blue2, blue3, blue4 = evaluate(beam_size)\nprint(\"BLUE-1: \", blue1)\nprint(\"BLUE-2: \", blue2)\nprint(\"BLUE-3: \", blue3)\nprint(\"BLUE-4: \", blue4)\nprint(\"meteor: \", meteor)","c5ea5402":"beam_size = 3\nblue1, blue2, blue3, blue4, meteor = evaluate(beam_size)\n#blue1, blue2, blue3, blue4 = evaluate(beam_size)\nprint(\"BLUE-1: \", blue1)\nprint(\"BLUE-2: \", blue2)\nprint(\"BLUE-3: \", blue3)\nprint(\"BLUE-4: \", blue4)\nprint(\"meteor: \", meteor)","30fb6f15":"beam_size = 2\nblue1, blue2, blue3, blue4, meteor = evaluate(beam_size)\n#blue1, blue2, blue3, blue4 = evaluate(beam_size)\nprint(\"BLUE-1: \", blue1)\nprint(\"BLUE-2: \", blue2)\nprint(\"BLUE-3: \", blue3)\nprint(\"BLUE-4: \", blue4)\nprint(\"meteor: \", meteor)","79d576cb":"beam_size = 1\nblue1, blue2, blue3, blue4, meteor = evaluate(beam_size)\n#blue1, blue2, blue3, blue4 = evaluate(beam_size)\nprint(\"BLUE-1: \", blue1)\nprint(\"BLUE-2: \", blue2)\nprint(\"BLUE-3: \", blue3)\nprint(\"BLUE-4: \", blue4)\nprint(\"meteor: \", meteor)","97347cca":"## **Datasets**","263f8f31":"## **Model** ","cfacb862":"## **Eval**","02c9d79b":"## **Utils**","59314deb":"## **Train**","6f2a234e":"**Meteor score**"}}