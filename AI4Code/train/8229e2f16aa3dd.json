{"cell_type":{"ba245b10":"code","e258ea45":"code","a329894e":"code","705e53a1":"code","bb9be584":"code","1227bd32":"code","c22597fd":"code","d01261be":"code","949b8690":"code","c942b93e":"code","051983c1":"code","dccfbf4c":"code","693194c0":"code","ee507ad2":"code","a48ec1aa":"code","bf98370c":"code","4d720040":"code","40bbf926":"code","38239db4":"code","228ee6fc":"code","1e22c580":"code","2bf558f3":"code","c4cc5f7e":"code","49c32964":"code","f8331594":"code","4310b2c1":"code","90331970":"code","ae0015cc":"code","6aeaa717":"code","a0dc5f5d":"code","fc31b5f8":"code","78e11f74":"code","83b2d6ca":"code","0a010fc9":"code","7074d41b":"code","249338e3":"code","af468de9":"code","98635cb3":"code","715eedd7":"code","107a0b0b":"code","5b69a4f4":"code","06954e44":"code","484ad5cb":"code","746b3369":"code","f1df9617":"code","7be0fc24":"code","7e56dc38":"code","7b742fea":"code","64586bd6":"code","e0456888":"code","111f512b":"code","47a0b8d8":"code","2c4d1ff8":"code","028ab770":"code","4f80e13f":"code","d2c18234":"code","30fa78d7":"code","e23befe2":"code","89c92b6b":"code","b91ca462":"code","cf572109":"code","cc465165":"code","f1df4287":"code","cac56a58":"code","d6d6f656":"code","e37e2553":"code","df320e58":"code","431a005c":"markdown","dbffea07":"markdown","4ba8dd77":"markdown","b24e434c":"markdown","5ee75aa9":"markdown","438aa872":"markdown","0ae70898":"markdown","3cdc614f":"markdown","f5de0ebb":"markdown","a9aadfbd":"markdown","a3f66800":"markdown","8f8430db":"markdown","7539c722":"markdown","6f382d72":"markdown","24c4cf6a":"markdown","a9b12204":"markdown","549856b5":"markdown","211f44cd":"markdown","f0d3a74e":"markdown","14765568":"markdown","6c6c8d0d":"markdown","5468863a":"markdown","2769b302":"markdown","87f3485d":"markdown","f95dc682":"markdown","9ef45f01":"markdown","11913a17":"markdown","86b68707":"markdown","e6faf5ed":"markdown","06f186ef":"markdown","4d8a136c":"markdown","a6e34e37":"markdown","7733e63a":"markdown","eea2c855":"markdown","7837a500":"markdown","701a466b":"markdown","4a365da4":"markdown","59848696":"markdown","d108bcc9":"markdown","12f546be":"markdown","a51caddb":"markdown","71b90afc":"markdown","e1e6cd9e":"markdown","b11f21a6":"markdown","b8530e4d":"markdown","9ec895f4":"markdown","df310e0f":"markdown","d04a567b":"markdown","0462094e":"markdown"},"source":{"ba245b10":"# Initialize Notebook\n\nimport numpy as np\nimport scipy as sc\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statistics as stat\n%matplotlib inline\nsns.set_style('whitegrid')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, Normalizer\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, KFold\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier","e258ea45":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","a329894e":"train.shape, test.shape","705e53a1":"train.describe()","bb9be584":"test.describe()","1227bd32":"train.describe(include=['O'])","c22597fd":"test.describe(include=['O'])","d01261be":"train.head()","949b8690":"f,ax = plt.subplots(3,4,figsize=(20,16))\nsns.countplot('Pclass',data=train,ax=ax[0,0])\nsns.countplot('Sex',data=train,ax=ax[0,1])\nsns.boxplot(x='Pclass',y='Age',data=train,ax=ax[0,2])\nsns.countplot('SibSp',hue='Survived',data=train,ax=ax[0,3],palette='husl')\nsns.distplot(train['Fare'].dropna(),ax=ax[2,0],kde=False,color='b')\nsns.countplot('Embarked',data=train,ax=ax[2,2])\n\nsns.countplot('Pclass',hue='Survived',data=train,ax=ax[1,0],palette='husl')\nsns.countplot('Sex',hue='Survived',data=train,ax=ax[1,1],palette='husl')\nsns.distplot(train[train['Survived']==0]['Age'].dropna(),ax=ax[1,2],kde=False,color='r',bins=5)\nsns.distplot(train[train['Survived']==1]['Age'].dropna(),ax=ax[1,2],kde=False,color='g',bins=5)\nsns.countplot('Parch',hue='Survived',data=train,ax=ax[1,3],palette='husl')\nsns.swarmplot(x='Pclass',y='Fare',hue='Survived',data=train,palette='husl',ax=ax[2,1])\nsns.countplot('Embarked',hue='Survived',data=train,ax=ax[2,3],palette='husl')\n\nax[0,0].set_title('Total Passengers by Class')\nax[0,1].set_title('Total Passengers by Gender')\nax[0,2].set_title('Age Box Plot By Class')\nax[0,3].set_title('Survival Rate by SibSp')\nax[1,0].set_title('Survival Rate by Class')\nax[1,1].set_title('Survival Rate by Gender')\nax[1,2].set_title('Survival Rate by Age')\nax[1,3].set_title('Survival Rate by Parch')\nax[2,0].set_title('Fare Distribution')\nax[2,1].set_title('Survival Rate by Fare and Pclass')\nax[2,2].set_title('Total Passengers by Embarked')\nax[2,3].set_title('Survival Rate by Embarked')","c942b93e":"train['Cabin'].value_counts().head()","051983c1":"g = sns.FacetGrid(col='Embarked',data=train)\ng.map(sns.pointplot,'Pclass','Survived','Sex',palette='viridis',hue_order=['male','female'])\ng.add_legend()","dccfbf4c":"f,ax = plt.subplots(1,2,figsize=(15,3))\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis',ax=ax[0])\nsns.heatmap(test.isnull(),yticklabels=False,cbar=False,cmap='viridis',ax=ax[1])","693194c0":"# Set Figure Size\nfig = plt.figure(figsize=(15,5))\n\n# 1st Subplot\nax = fig.add_subplot(1, 2, 1)\n\n# Box Plot for Age by Pclass for Train Data\nax.set_title('Train Dataset')\n\n# Settings to display median values\nbox_plot_train = sns.boxplot(x='Pclass',y='Age',data=train)\nax_train = box_plot_train.axes\nlines_train = ax_train.get_lines()\ncategories_train = ax_train.get_xticks()\n\nfor cat in categories_train:\n    # Median line is the 4th line in a range of 6 lines:\n    # 0: 25th percentile, 1: 75th percentile, 2: lower whisker, 3: upper whisker, 4: median, 5: upper extreme value\n    y = round(lines_train[4+cat*6].get_ydata()[0],1) \n\n    ax_train.text(cat, y, f'{y}', ha='center', va='center', fontweight='bold',\n                  size=10, color='white', bbox=dict(facecolor='#445A64'))\n\n# 2nd Subplot\nax = fig.add_subplot(1, 2, 2)\n\n# Box Plot for Age by Pclass for Test Data\nax.set_title('Test Dataset')\n\n# Settings to display median values\nbox_plot_test = sns.boxplot(x='Pclass',y='Age',data=test)\nax_test = box_plot_test.axes\nlines_test = ax_test.get_lines()\ncategories_test = ax_test.get_xticks()\n\nfor cat in categories_test:\n    # Median line is the 4th line in a range of 6 lines:\n    # 0: 25th percentile, 1: 75th percentile, 2: lower whisker, 3: upper whisker, 4: median, 5: upper extreme value\n    y = round(lines_test[4+cat*6].get_ydata()[0],1) \n\n    ax_test.text(cat, y, f'{y}', ha='center', va='center', fontweight='bold',\n                  size=10, color='white', bbox=dict(facecolor='#445A64'))\n","ee507ad2":"test.groupby('Pclass')['Age'].median()","a48ec1aa":"# Histograms for Age\n\n# Set Figure Size\nfig = plt.figure(figsize=(15,5))\n\n# 1st Subplot\nax = fig.add_subplot(1, 2, 1)\n\n# Histogram for Age: Train Dataset\nax.set_title('Train Dataset')\n\nsns.distplot(train['Age'].dropna(), kde=True, bins=5)\n\n# 2nd Subplot\nax = fig.add_subplot(1, 2, 2)\n\n# Histogram for Age: Test Dataset\nax.set_title('Test Dataset')\n\nsns.distplot(test['Age'].dropna(), kde=True, bins=5)\n","bf98370c":"def fill_age_train(cols):\n    Age = cols[0]\n    PClass = cols[1]\n    \n    if pd.isnull(Age):\n        if PClass == 1:\n            return 37\n        elif PClass == 2:\n            return 29\n        else:\n            return 24\n    else:\n        return Age\n\ndef fill_age_test(cols):\n    Age = cols[0]\n    PClass = cols[1]\n    \n    if pd.isnull(Age):\n        if PClass == 1:\n            return 42\n        elif PClass == 2:\n            return 26.5\n        else:\n            return 24\n    else:\n        return Age","4d720040":"train['Age'] = train[['Age','Pclass']].apply(fill_age_train,axis=1)\ntest['Age'] = test[['Age','Pclass']].apply(fill_age_test,axis=1)","40bbf926":"test['Fare'].fillna(stat.mode(test['Fare']),inplace=True)\ntrain['Embarked'].fillna('S',inplace=True)\ntrain['Cabin'].fillna('No Cabin',inplace=True)\ntest['Cabin'].fillna('No Cabin',inplace=True)","38239db4":"f,ax = plt.subplots(1,2,figsize=(15,3))\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis',ax=ax[0])\nsns.heatmap(test.isnull(),yticklabels=False,cbar=False,cmap='viridis',ax=ax[1])","228ee6fc":"train.drop('Ticket',axis=1,inplace=True)\ntest.drop('Ticket',axis=1,inplace=True)","1e22c580":"train.head()","2bf558f3":"# Combine Dataset 1st for Feature Engineering\n\ntrain['IsTrain'] = 1\ntest['IsTrain'] = 0\ndf = pd.concat([train,test])","c4cc5f7e":"# Scaler Initiation\n\nscaler = MinMaxScaler()","49c32964":"df['Title'] = df['Name'].str.split(', ').str[1].str.split('.').str[0]\ndf['Title'].value_counts()","f8331594":"df['Title'].replace('Mme','Mrs',inplace=True)\ndf['Title'].replace(['Ms','Mlle'],'Miss',inplace=True)\ndf['Title'].replace(['Dr','Rev','Col','Major','Dona','Don','Sir','Lady','Jonkheer','Capt','the Countess'],'Others',inplace=True)\ndf['Title'].value_counts()","4310b2c1":"df.drop('Name',axis=1,inplace=True)\ndf.head()","90331970":"sns.distplot(df['Age'],bins=5)","ae0015cc":"df['AgeGroup'] = df['Age']\ndf.loc[df['AgeGroup']<=19, 'AgeGroup'] = 0\ndf.loc[(df['AgeGroup']>19) & (df['AgeGroup']<=30), 'AgeGroup'] = 1\ndf.loc[(df['AgeGroup']>30) & (df['AgeGroup']<=45), 'AgeGroup'] = 2\ndf.loc[(df['AgeGroup']>45) & (df['AgeGroup']<=63), 'AgeGroup'] = 3\ndf.loc[df['AgeGroup']>63, 'AgeGroup'] = 4","6aeaa717":"sns.countplot(x='AgeGroup',hue='Survived',data=df[df['IsTrain']==1],palette='husl')","a0dc5f5d":"df.drop('Age',axis=1,inplace=True)\ndf.head()","fc31b5f8":"df['FamilySize'] = df['SibSp'] + df['Parch'] + 1 #himself\ndf['IsAlone'] = 0\ndf.loc[df['FamilySize']==1, 'IsAlone'] = 1","78e11f74":"# Visual Inspection of Survival Rates\n\nf,ax = plt.subplots(1,2,figsize=(15,6))\nsns.countplot(df[df['IsTrain']==1]['FamilySize'],hue=train['Survived'],ax=ax[0],palette='husl')\nsns.countplot(df[df['IsTrain']==1]['IsAlone'],hue=train['Survived'],ax=ax[1],palette='husl')\n","83b2d6ca":"df.drop(['SibSp','Parch','FamilySize'],axis=1,inplace=True)\ndf.head()","0a010fc9":"df.head()","7074d41b":"df['Deck'] = df['Cabin']\ndf.loc[df['Deck']!='No Cabin','Deck'] = df[df['Cabin']!='No Cabin']['Cabin'].str.split().apply(lambda x: np.sort(x)).str[0].str[0]\ndf.loc[df['Deck']=='No Cabin','Deck'] = 'N\/A'","249338e3":"sns.countplot(x='Deck',hue='Survived',data=df[df['IsTrain']==1],palette='husl')","af468de9":"df.loc[df['Deck']=='N\/A', 'Deck'] = 0\ndf.loc[df['Deck']=='G', 'Deck'] = 1\ndf.loc[df['Deck']=='F', 'Deck'] = 2\ndf.loc[df['Deck']=='E', 'Deck'] = 3\ndf.loc[df['Deck']=='D', 'Deck'] = 4\ndf.loc[df['Deck']=='C', 'Deck'] = 5\ndf.loc[df['Deck']=='B', 'Deck'] = 6\ndf.loc[df['Deck']=='A', 'Deck'] = 7\ndf.loc[df['Deck']=='T', 'Deck'] = 0","98635cb3":"df.drop('Cabin',axis=1,inplace=True)\ndf.head()","715eedd7":"df[['Fare','Pclass','Deck']] = scaler.fit_transform(df[['Fare','Pclass','Deck']])","107a0b0b":"df.head()","5b69a4f4":"def process_dummies(df,cols):\n    for col in cols:\n        dummies = pd.get_dummies(df[col],prefix=col,drop_first=True)\n        df = pd.concat([df.drop(col,axis=1),dummies],axis=1)\n    return df","06954e44":"df = process_dummies(df,['Embarked','Sex','Title','AgeGroup'])","484ad5cb":"df.head()","746b3369":"dataset = df[df['IsTrain']==1]\ndataset.drop(['IsTrain','PassengerId'],axis=1,inplace=True)\nholdout = df[df['IsTrain']==0]\ntest_id = holdout['PassengerId']\nholdout.drop(['IsTrain','PassengerId','Survived'],axis=1,inplace=True)","f1df9617":"int(np.sum(dataset['Survived'])), dataset.shape[0]","7be0fc24":"df.to_csv('titanic_dataset_preprocessed.csv',index=False)","7e56dc38":"X = dataset.drop(['Survived'],axis=1)\ny = dataset['Survived'].astype('int')\n#X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=101)","7b742fea":"%%time\n\nmodel = RandomForestClassifier()\nsplits = 5\nkf = KFold(n_splits=splits,shuffle=True,random_state=101)\nscore = 0\ntrain_indices, validation_indices = [],[]\ntotal_score = []\nfor curr_train_indices, curr_validation_indices in kf.split(X):\n    result = model.fit(X.iloc[curr_train_indices], y.iloc[curr_train_indices])\n    curr_score = result.score(X.iloc[curr_validation_indices],y.iloc[curr_validation_indices])\n    print(curr_score)\n    total_score.append(curr_score)\n        \n    if(curr_score > score):\n        score = curr_score\n        train_indices = curr_train_indices\n        validation_indices = curr_validation_indices\nprint('Best Score: ',score)\nprint('Average Score: ', sum(total_score)\/splits)\n","64586bd6":"%%time\n\nparam_grid = [{'n_estimators':[10,20,50,100,200], 'max_depth':[1,2,3,4,5,7,10,None], 'max_features':[3,5,7,9,10,'auto']}]\ngrid = GridSearchCV(model, param_grid, n_jobs=-1) # n_jobs = -1 to use all processors\ngrid.fit(X, y)","e0456888":"grid.best_params_, grid.best_score_","111f512b":"predictions = grid.predict(holdout)","47a0b8d8":"fullpredictions = grid.predict(X)\nlen(fullpredictions)","2c4d1ff8":"# Confusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\n\nconf_mat = confusion_matrix(y, fullpredictions)\nprint(conf_mat)","028ab770":"# Accuracy, Recall and Precision\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\n\naccuracy = accuracy_score(y, fullpredictions)\nrecall = recall_score(y, fullpredictions)\nprecision = precision_score(y, fullpredictions)\n\nprint('Accuracy: ', '{:.0%}'.format(accuracy))\nprint('Recall: ', '{:.0%}'.format(recall))\nprint('Precision: ', '{:.0%}'.format(precision))","4f80e13f":"# Model\n\nforest = RandomForestClassifier(max_depth=7, max_features=10, n_estimators=20, random_state=101)\nforest.fit(X, y)\n","d2c18234":"# Feature DataFrame\n\nlist(X.columns)","30fa78d7":"# Feature Importance (Raw)\n\nforest.feature_importances_.tolist()","e23befe2":"# Define Function for Random Forest Feature Importance\n\ndef RF_FeatureImportance(df, model):\n    \"\"\"\n    df: input DataFrame with features, excluding column to be predicted\n    model: input fitted RandomForest model\n    \"\"\"\n    importancedf = pd.DataFrame(list(zip(df.columns.tolist(), model.feature_importances_.tolist())),\n                         columns=['Features','Importance'])\n    importancedf.sort_values(by=['Importance'], inplace=True, ascending=False)\n    importancedf['Importance'] = importancedf['Importance'].map(lambda x: '{:.0%}'.format(x))\n    return importancedf\n","89c92b6b":"# Random Forest Feature Importance\n\nRF_FeatureImportance(X,forest)\n","b91ca462":"# Check Correlation Heatmap for Feature Importance Analysis\n\ncut_off = 0.3\ncorr_mat = X.corr() # Insert DataFrame X\ncorr_mat[(corr_mat > -cut_off) & (corr_mat < cut_off) ] = np.nan # Use NaN instead of 0 so that they will not be shown in the heatmap\nplt.figure(figsize=(25.6,16))\n\nsns.set_style(style = 'whitegrid')\nmask = np.zeros_like(corr_mat, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\ncmap = sns.diverging_palette(h_neg=240, h_pos=10, s=80, l=55, n=9, \n                             center='light', as_cmap=True)\nsns.heatmap(corr_mat, annot=True, cmap='magma',\n            mask=mask, fmt='.2f', linewidths=0.5,\n            square=True, vmin=-1, vmax=1, \n            annot_kws={\"size\": 15});\n","cf572109":"RF_FeatureImportance(X,forest)","cc465165":"# Spearman's Correlation (Raw)\n\nfrom scipy.stats import spearmanr\n\nspearman, pvalue = spearmanr(X)\nspearman\n","f1df4287":"# Display Spearman's Correlations in DataFrame\n\nspearmandf = pd.DataFrame(spearman, columns=X.columns, index=X.columns)\nspearmandf\n","cac56a58":"# Spearman's Correlation Heatmap\n\ncut_off = 0.3\ncorr_mat = spearmandf # Insert DataFrame for Spearman's Correlation, from above\ncorr_mat[(corr_mat > -cut_off) & (corr_mat < cut_off) ] = np.nan # Use NaN instead of 0 so that they will not be shown in the heatmap\nplt.figure(figsize=(25.6,16))\n\nsns.set_style(style = 'whitegrid')\nmask = np.zeros_like(corr_mat, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\ncmap = sns.diverging_palette(h_neg=240, h_pos=10, s=80, l=55, n=9, \n                             center='light', as_cmap=True)\nsns.heatmap(corr_mat, annot=True, cmap='coolwarm',\n            mask=mask, fmt='.2f', linewidths=0.5,\n            square=True, vmin=-1, vmax=1, \n            annot_kws={\"size\": 15});\n","d6d6f656":"# Define Function for Random Forest Tree Visualization\n\ndef RF_TreeViz(df, y, model, n):\n    \"\"\"\n    df: input DataFrame with features, excluding column to be predicted\n    y: input Series with target variable\/ classification which is being predicted\n    model: input fitted RandomForest model\n    n: tree number in the Forest\n    \"\"\"\n    \n    # Extract one tree, n\n    estimator = model.estimators_[n]\n\n    from sklearn.tree import export_graphviz\n\n    # Export tree as dot file\n    export_graphviz(estimator, out_file='tree.dot', feature_names = list(df.columns), class_names = list(y.name),\n                    rounded = True, proportion = False, precision = 2, filled = True)\n\n    # Convert dot file to png using system command (requires graphviz)\n    from subprocess import call\n    call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\n    # Display in Jupyter Notebook\n    from IPython.display import Image, display\n    display(Image(filename = 'tree.png'))\n","e37e2553":"# Visualize A Random Forest Tree\n\nRF_TreeViz(X, y, forest, 10) # Recall that forest is our fitted RandomForestClassifier model and here, I randomly selected tree # 10\n","df320e58":"submission = pd.DataFrame({\n    'PassengerId': test_id,\n    'Survived': predictions\n})\n\nsubmission.to_csv('submission.csv',index=False)","431a005c":"In this section, we will look at 2 things: our RandomForestClassifier model's feature importance and visualize one sample tree from the Forest. They should help demystify the Forest, which will give us full confidence that **this is not a black box model**.","dbffea07":"Now, let's visualize one tree from our RandomForestClassifier model:","4ba8dd77":"**Conclusion**\n\nWith the analysis above, we tie **G.3 Model Performance** and **G.4 Model Explainability** sections together, and conclude that our *RandomForestClassifier model* (with best hyperparameters specified in the **G.2 Model Creation** section) produced predictions with *90% accuracy*, contributed by the following *features: 93% importance* from *Fare, Pclass, Deck, Titles, Sex and IsAlone*.\n","b24e434c":"**A. DESCRIPTIVE STATISTICS**\n\nAll right, using a few lines of code, let's try to describe the data using descriptive statistics:","5ee75aa9":"**E.4 Feature Engineering: Cabin -> Deck**","438aa872":"**D.2 Filling Missing Values in Fare, Cabin and Embarked**","0ae70898":"**E. FEATURE ENGINEERING**\n\nAs mentioned in the Summary section above, we will work on **Name, Age, SibSP & Parch, Fare, Cabin**: let's do this!","3cdc614f":"Well, now we can clearly see the survival rate based on passenger's Deck","f5de0ebb":"**C. SUMMARY**\n\nThis is a summary of our dataset and plan:\n* **Survived:**\n    * The value we should predict using test dataset. It is numerical with binary value 0 (Dead) and 1 (Survived)\n    \n* **Pclass:**\n    * The data type is categorical, level of measurement is qualitative->ordinal, since the level seems like 1>2>3.\n    * Since this is an ordinal, maybe we should **scale its value from 0 to 1** so we can keep the level information from this variable.\n    \n* **Name:**\n    * The data type is categorical, level of measurement is qualitative->nominal.\n    * We should include this variable in **Feature Engineering** process to extract the title value which maybe could improve our prediction result.\n    \n* **Sex:**\n    * The data type is categorical, level of measurement is qualitative->nominal.\n    * Since this is a categorical, maybe we should change the value to binary value 0 for male and 1 for female. We'll do this on **Data Preparation** process.\n    \n* **Age:**\n    * The data type is numerical->continuous with level of measurement quantitative->ratio.\n    * We should fill the **missing values**\n    * For a more meaningful analysis on age, it's better to change the level of measurement to quantitative->interval, by grouping the ages (maybe 1. Children\/ Teenagers, 2. Young Adults, 3. Adults, 4. Middle-Aged and 5. Seniors) in the **Feature Engineering** process.\n    * Ages have right-skewed distributions so median is a better measure of central tendency than mean\n    \n* **SibSp & Parch:**\n    * The data type is numerical, level of measurement is quantitative->ratio.\n    * Passenger with Sibling\/Spouse, or have parent\/children aboard, have higher survival rate than passenger which is alone!\n    * So I'll create a new feature based on this attribute called 'is_alone', I'll do this on **Feature engineering** process.\n    \n* **Ticket:**\n    * *Let's drop this for now.*\n    \n* **Fare:**\n    * The data type is numerical->continuous with level of measurement is quantitative->ratio.\n    * There is 1 missing value in test dataset\n    * Since it seems there is an increasing level of passenger based on the Fare, I'll just scale the value of this variable from 0 to 1 in **Feature Engineering** process.\n    \n* **Cabin:**\n    * The data type is categorical, level of measurement is qualitative->ordinal, since the level seems like A>B>C>D..\n    * Some passenger have multiple cabin listed.\n    * There are many **missing values** on this attributes, I'll fill it with 'No Cabin' string.\n    * For each passenger, I'll just try to create a new feature called **'Deck'** with first letter from the Cabin as its value on **Feature Engineering** process.\n    * If passenger have multiple deck listed, I'll just use the higher class deck (ex: A and D, I'll just use A as the value)\n    * Because this variable is ordinal, I'll further change the letter value to be scaled from 0 to 1 in **Feature Engineering** process\n    \n* **Embarked:**\n    * The data type is categorical, level of measurement is qualitative->nominal.\n    * Since this is a categorical, maybe we should **get dummies** of this variable.\n    * There are 2 missing values in training dataset","a9aadfbd":"**D. DEALING WITH MISSING VALUES**\n\nFrom the summary above, we should fill missing values in **Age**, 1 value for **Fare** in test, and 2 values for **Embarked** in training. So, let's do this.\n\nHang on, let's check the missing values using heatmap:","a3f66800":"**E.3 Feature Engineering: SibSp & Parch -> IsAlone**","8f8430db":"**E.2 Feature Engineering: Age -> AgeGroup**","7539c722":"I'll divide the ages to 5 categories: Children (<=19), Young Adults (>19, <=30), Adults (>30, <=45), Middle Aged (>45, <=63), Seniors (>63), \n\nwith: **Children = 0, Young Adults = 1, Adults = 2, Middle Aged = 3, Seniors = 4**","6f382d72":"**F.2 Balancing Dataset**\n\nIn this part, I'll try to balance the prior of classes in train dataset to become 50% of survived passengers, and 50% not survived.\n\n*Update: Not used in this latest update since it lowered the submission score. But we surely need to balance the dataset in some other cases to get better predictions.*\n\nUse StratifiedKFold to address class imbalance (if any) <br>\nSee: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html","24c4cf6a":"**G.2 Model Creation**\n\nCross validate the model (K = 10 folds): 10 is a high number; very computationally expensive so I tried with just 5 (default) instead <br>\nScore = accuracy of the model: (TP + TN) \/ Total Sample\n\nSee: https:\/\/towardsdatascience.com\/cross-validation-430d9a5fee22 <br>\n**Stratify** data if you want to address class imbalance especially in cases like fraud detection, defaults, spam vs genuine mails, etc.\n","a9b12204":"**G.1 Splitting the Dataset**","549856b5":"**F. FINAL DATA PREPARATION**\n\nWe now have all the features. Finally, on data preprocessing, we need to create dummies for categorical data and produce a new DataFrame. The features to preprocess are: **Embarked, Sex, Title, AgeGroup**.","211f44cd":"* There are high correlations in these 2 sets of features: (Fare, Pclass, Deck) and (Titles and Sex) <br>\n* Feature importance strips out correlation e.g. if 2 features are 100% correlated, the importance of one of them will be shown as 0 (assuming additive, with correlation deducted\/ stripped out) <br>\n* *If correlations are not stripped out*, feature importance will sum to >100%, which then becomes counter-intuitive <br>\n\n* So among these 2 sets: (Titles and Sex): 50% and (Fare, Pclass, Deck): 42%, the high correlations should be taken into account when interpreting their feature importance <br>\n\n* It is also interesting to note that IsAlone has some positive correlation with Sex_Male and Title_Mr, which is very intuitive, so IsAlone is likely to be a very important feature, vis-\u00e0-vis Titles and Sex - so **do not misinterpret that IsAlone: 1% is not important**  <br>\n* AgeGroup_1.0: Young Adults (>19, <=30) has weak positive correlation with Pclass => AgeGroup_1.0\/ Young Adults' feature importance is very likely >1% surface value, given the correlation with Pclass's 12% importance","f0d3a74e":"From the figures above, we can see that if a passenger has family onboard, **the survival rate will increase to approximately 50%.**\n\nSince we already have the information using IsAlone feature only, *we'll just drop SibSp, Parch, and FamilySize*","14765568":"Information from visualizations above:\n* Female passengers who embarked from C and Q have high survival rate\n* Female passengers who embarked from Queenstown in Class 1 and 2 all survived!\n* Male passengers who embarked from Queenstown in Class 1 and 2 have the lowest survival rate!","6c6c8d0d":"This is a binary classification problem (Survived or Not) so a confusion matrix is the perfect tool to describe the performance of our classification model:","5468863a":"For these rare title, we'll convert them to 'Others', except **Mme** will be converted to Mrs, **Ms and Mlle** to Miss","2769b302":"**H. SUBMISSION**\n\nFinally, let's submit our results:","87f3485d":"**G. PREDICTION (MODELLING)**","f95dc682":"**D.1 Filling Missing Values in Age**\n\nLet's look at the box plots 1st:","9ef45f01":"This notebook is forked from **Astandri K**'s original notebook (100% credit to him):\nhttps:\/\/www.kaggle.com\/astandrik\/journey-from-statistics-eda-to-prediction\n\nI added a few points below to enhance my personal understanding so this notebook is meant as a supplement to **Astandri K**'s work above:\n* **D.1 Filling Missing Values in Age**: added box plots for median, and emphasized the use of median as a more appropriate measure of central tendency, given the age distribution skew\n* **G.3 Model Performance**: added Confusion Matrix, Accuracy, Recall and Precision scores for greater clarity on measuring model performance, which are applicable for a binary classification problem such as this\n* **G.4 Model Explainability**: added this section, given the current high focus on this topic in data science\n\nFeel free to click \"Run All\" to reproduce this notebook's analysis for yourself - reproducibility is another important aspect of data science nowadays","11913a17":"Average score makes more sense if you want to compare multiple models. <br>\n\nRandomForestClassifier's score: returns the mean accuracy on the given test\/ validation data and labels. This is a subset accuracy, which is a harsh metric, since you require each label to be correctly predicted for each given test\/ validation data. <br>\n\nThis explains why the score here is lower than the total score in the **G.3 Model Performance** section below.","86b68707":"The Spearman's correlation heatmap above pretty much paints the same picture as Pearson's correlation heatmap. <br>\n\nWe can distill 2 additional observations though:\n* IsAlone has some negative monotonic relationship with Fare: this establishes IsAlone's relationship with the (Fare, Pclass, Deck) set, on top of the linear relationship with Sex_Male and Title_Mr, shown by Pearson's correlation (1st) heatmap. Therefore, IsAlone is likely to be an even more important feature than before. <br>\n* AgeGroup_1.0: Young Adults (>19, <=30) has weak negative monotonic relationship with Fare, on top of its weak positive linear relationship with Pclass => this is likely to marginally increase its feature importance. The observations are intuitive: Young Adults likely couldn't afford tickets with high Fare, and therefore ended up in higher Pclass e.g. 3rd class instead of 1st class.\n","e6faf5ed":"Now we have new information - some passengers have multiple cabin listed.\nFor each passenger, I'll just try to create a new feature called **'Deck'** with the first letter from the Cabin as its value.\nIf a passenger have multiple deck listed, I'll just use the higher class deck (ex: A and D, I'll just use A as the value.)\n\nThanks to this discussion: https:\/\/www.kaggle.com\/c\/titanic\/discussion\/4693\n\n\"First class had the top decks (A-E), second class (D-F), and third class (E-G). It also makes sense that the people towards the top (higher decks, higher pclass) more likely survived, because they were closer to the lifeboats.\"","06f186ef":"From the box plots above, we can fill missing Ages with:\n* Train: PClass 1 = 37, PClass 2 = 29, PClass 3 = 24\n* Test: PClass 1 = 42, PClass 2 = 26.5, PClass 3 = 24\n\nAnd from the histograms above, Ages clearly have right-skewed distributions so median is a better measure of central tendency than mean. This is very similar to income distributions, which are also skewed to the right, such that mean will overstate the feature's \"centralness\" while mode will understate the centralness.","4d8a136c":"**G.3 Model Performance**\n\nLet's make full predictions so that we can analyze the confusion matrix holistically below:","a6e34e37":"**A.1 Numerical Attributes**\n\nFrom the codes above, we obtain the descriptive statistics for numerical attributes: **What do we see here?**\n1. **Survived**: The sample mean of this training data is 0,38, which could means *only about that percentage of passengers survived from titanic accident*\n\n2. **Pclass** (Passenger Class:Tthere are 3 class of passenger. At Q2(50%) and Q3(75%) we could see the value is 3, which could means *there are minimum 50% (or more) passengers which is 3rd class passengers*. It seems logical since lower class usually have cheaper ticket prize, and more quota for that class\n\n3. **Age**: From train and test data, the count values seems different from the others. yes, **Age attribute contains missing values**. Another useful information, the mean\/average age on training data is 29 years old, which is 1 years older than the median value of the mean (30 mean and 27 median on test dataset), so what does it mean?\n    \n    It means the distributions of age values have **right skew**, which we expect some outliers in the *higher age value* (on the right size ofthe axis. As we can see, on the training and test dataset max value is 80 and 76 respectively.\n    \n4. **SibSp and Parch**: These attributes indicate number of SIblings or spouses, and Parent or Children number aboard. From the mean value, seems *majority of the passengers is alone (neither have SibSp or Parch)*. It is interesting that we see the maximum value have 8 SibSp and 9 ParCh, *maybe the oldest person brought his\/her entire family on the ship*\n\n5. **Fare**: There are huge difference between mean and median value of this attributes, which is logical. *Many passengers from 3rd class which always have lower Fare*, on the other hand, we have so high value on max of Fare here, which seems an outlier that affect the average of this attributes (**again, right skew**). **Fare attribute contain 1 missing value on test dataset**","7733e63a":"**B. EXPLORATORY DATA ANALYSIS (EDA)**","eea2c855":"> Some useful information:\n* Clearly, we can see most passengers are in class 3, which have least survival probability here\n* From Sex attribute, we can see total male Passengers is almost 2 times of female passengers, but lower survival probability *maybe male passengers tend to save their lady first?*\n* From the figure above, we can try to input missing Ages by class:\n    * Pclass 1, median Age is approximately = 37\n    * Pclass 2, median Age is approximately = 29\n    * Pclass 3, median Age is approximately = 24\n* Ages have right-skewed distributions so median is a better measure of central tendency than mean\n* It seems that passengers with Sibling\/Spouse or have Parent\/Children aboard, have higher survival rates than passengers who are alone!","7837a500":"The correlation analysis above is based on Pearson's correlation, which is a measure most people are familiar with. Pearson's correlation measures the strength of the **linear** relationship between variables. <br>\nFor non-linear relationships, Pearson's correlation is insufficient. Let's run a Spearman's correlation analysis, to supplement Pearson's correlation. <br>\n\nSee excellent notes here for more details on the 2 correlation measures' differences:\nhttp:\/\/www.statstutor.ac.uk\/resources\/uploaded\/spearmans.pdf\n\nTo digress, these 2 correlations are examples of measures of concordance (agreement.) In other cases, other measures of concordance can be considered e.g. Kendall's Tau.\n","701a466b":"**A.2 Categorical Attributes** <br>\nNow, we're dealing with categorical attributes. From the describe method above, we get additional information:\n1. **Name**: All names are unique (nothing special,) *but they contain title* - maybe we can perform feature engineering later to produce a new attribute (Title) which could improve model performance\n\n2.  **Sex**: Or *gender*. Consists of 2 categories - male and female. In both training and test datasets, male have higher frequency (approximately 60:40.)\n\n3.  **Ticket**: There are many unique values for this attributes - maybe we'll just drop this attribute for now and include it for future research\n\n4. **Cabin**: Many **missing values** here (*204 filled from 891 possible* on training dataset and *91 filled from 418 possible* on test dataset). *Maybe some passengers*, which we already know, 3rd class or low-Fare passengers, **do not have Cabin**.\n\n5. **Embarked**: There are **2 missing values** on training dataset. From the train and test datasets, we know that most passengers embarked from S (*what is \"S\" anyway?*)","4a365da4":"**G.4 Model Explainability**","59848696":"**Plot Charts**","d108bcc9":"# **NOTEBOOK OUTLINE** <br>\n\n# A. Descriptive Statistics <br>\n# B. Exploratory Data Analysis (EDA) <br>\n# C. Summary and Feature Selection <br>\n# D. Dealing with Missing Values <br>\n# E. Feature Engineering <br>\n# F. Final Data Preparation <br>\n# G. Prediction (Modelling) <br>","12f546be":"**Notes:**\n\nLike machine learning, I am still and always learning - feel free to leave a comment below if you have any feedback.\n\nThank you for reading my long journey and happy Kaggling :)\n\n* Update 2018-01-11(2): Checked Embarked C graph which seems the color getting inversed between male and female. Solved using **Hue Order** parameter in seaborn pointplot.\n* Update 2018-01-11: Added GridSearchCV and change the classifier to RandomForestClassifier, *LB goes up again to 0.78*\n* Update 2018-01-10(4): trying to use KFold Cross Validation to train the model. Used SVM as the classifier.\n* Update 2018-01-10(3): **Removed Balancing Dataset**, submission score going up again *0.77990* yeay :D, but still more works to do.\n* Update 2018-01-10(2): Fixed some ordinal features from getting dummies to be scaled instead to keep the level information from ordinal features. *LB dropped to: 0.73684* T.T\n* Update 2018-01-10: Fixed filling test dataset (AGE) with information from test dataset (I used information from train dataset before.\n* Update 2018-01-09(2): Added sub section in **F. Final Data Preparation**, Balancing Dataset before doing any prediction. Submission score dropped to *0.77033*. Too bad :'D\n* Update 2018-01-09: Commented **train_test_split** since I am not really using it now. I'll try to learn how to use it properly in sklearn machine learning convention. Maybe if you have any good reference, please kindly share it to me :)","a51caddb":"**F.1 Divide df to train dataset and holdout for final testing purpose**\n\nBefore we continue to the prediction section, let's divide our data again to **dataset** (formerly train data) and **holdout** (formerly test data):","71b90afc":"![](https:\/\/www.statisticshowto.com\/wp-content\/uploads\/2014\/02\/pearson-mode-skewness.jpg)","e1e6cd9e":"342 out of 891 passengers (38%) survived in this dataset","b11f21a6":"class_one_total = int(np.sum(dataset['Survived']))\nclass_zero_counter = 0\nindices_to_remove = []\n\nfor i in range(dataset.shape[0]):\n    if(dataset['Survived'].iloc[i] == 0):\n        class_zero_counter += 1\n        if(class_zero_counter > class_one_total):\n            indices_to_remove.append(i)\n\n#dataset.drop(dataset.index[indices_to_remove],inplace=True)","b8530e4d":"**IMPORT DATASET**\n\n1st of all, let's import the dataset:","9ec895f4":"**E.1 Feature Engineering: Name -> Title**","df310e0f":"**ALL CLEARED NOW!** Ready for feature engineering, after we drop Ticket:","d04a567b":"**E.5 Feature Engineering -> Scaling**\n\nIn this part, any features with level of measurement nominal -> ordinal would be scaled from 0 to 1: they are **Fare, Pclass, and Deck**:","0462094e":"True Negative: 522 <br>\nFalse Positive: 27 <br>\nTrue Positive: 280 <br>\nFalse Negative: 62 <br>\n\nAccuracy = (TP+TN) \/ Total = 90%, note that this total accuracy\/ score is greater than the (validation subset) scores in the **G.2 Model Creation** section above - reason explained in that section <br>\nRecall = TP \/ (TP+FN) = 82% of actual survivors predicted <br>\nPrecision = TP \/ (TP+FP) = 91% of positives predicted are actual survivors <br>\nPositives predicted = (27+280) \/ 891 = 34% (90% Accuracy) - not bad compared to the class imbalance below: <br>\n\n342 out of 891 passengers (38%) survived so there is some class imbalance. <br>\nHowever, *this class imbalance is not extreme enough to cause an accuracy paradox* e.g. a case of 1% or less death rate would have meant that even a random pick (without modelling) would result in 99%+ survival prediction --> this phenomenon is known as an accuracy paradox.\n"}}