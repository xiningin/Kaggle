{"cell_type":{"1e543c0f":"code","9dd55851":"code","ad09cb4d":"code","b30d9a78":"code","9391a9b5":"code","b2875683":"code","453006fe":"code","cfe8deb2":"code","51638e24":"code","2f79d69b":"code","ad5b214c":"code","0a6cf4e2":"code","0cdcaaed":"code","394c16b3":"code","de09a00e":"code","3e8a012e":"code","4be9e3bb":"code","7bba9f07":"code","acf47457":"code","99475fc9":"markdown","2816b2ff":"markdown","6fae388a":"markdown","f3d36dab":"markdown","566495d2":"markdown","ef3dd735":"markdown","eeced297":"markdown","76a9928b":"markdown","6d3fc117":"markdown","886ae622":"markdown","35e4febc":"markdown","f89279b3":"markdown","e203770b":"markdown"},"source":{"1e543c0f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9dd55851":"# import our bq_helper package\nimport bq_helper ","ad09cb4d":"from google.cloud import bigquery","b30d9a78":"# create a helper object for our bigquery dataset\nhacker_news = bq_helper.BigQueryHelper(active_project= \"bigquery-public-data\", \n                                       dataset_name = \"hacker_news\")","9391a9b5":"from google.cloud import bigquery\n\n# Create a \"Client\" object\nclient = bigquery.Client()\n\n# Construct a reference to the \"hacker_news\" dataset\ndataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"comments\" table\ntable_ref = dataset_ref.table(\"comments\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the \"comments\" table\nclient.list_rows(table, max_results=5).to_dataframe()","b2875683":"# print a list of all the tables in the hacker_news dataset\nhacker_news.list_tables()","453006fe":"# print information on all the columns in the \"full\" table\n# in the hacker_news dataset\nhacker_news.table_schema(\"full\")","cfe8deb2":"hacker_news.head(\"full\")","51638e24":"# preview the first ten entries in the by column of the full table\nhacker_news.head(\"full\", selected_columns=\"by\", num_rows=10)","2f79d69b":"# this query looks in the full table in the hacker_news\n# dataset, then gets the score column from every row where \n# the type column has \"job\" in it.\nquery = \"\"\"SELECT score\n            FROM `bigquery-public-data.hacker_news.full`\n            WHERE type = \"job\" \"\"\"\n\n# check how big this query will be\nhacker_news.estimate_query_size(query)","ad5b214c":"# only run this query if it's less than 100 MB\nhacker_news.query_to_pandas_safe(query, max_gb_scanned=0.1)","0a6cf4e2":"# check out the scores of job postings (if the \n# query is smaller than 1 gig)\njob_post_scores = hacker_news.query_to_pandas_safe(query)","0cdcaaed":"# Query to select countries with units of \"ppm\"\nfirst_query = \"\"\"\n              SELECT parent\n              FROM `bigquery-public-data.hacker_news.full`\n              WHERE type = 'comment'\n            \"\"\"\n\n\n\n# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nfirst_query_job = client.query(first_query, job_config=safe_config)\n\n# API request - run the query, and return a pandas DataFrame\nfirst_results = first_query_job.to_dataframe()\n","394c16b3":"first_results.head()","de09a00e":"query_improved = \"\"\"\n                 SELECT ranking, COUNT(1) AS NumRank \n                 FROM `bigquery-public-data.hacker_news.comments`\n                 GROUP BY ranking\n                 HAVING COUNT(1) > 10\n                 \"\"\"\n## AS means the name the data column vill have, parent er the column it will coint\n## (COUNT(1)) We can use GROUP BY to group together rows that have the same value in \n##the Animal column, while using COUNT() to find out how many ID's we have in each group.\n## HAVING is used in combination with GROUP BY to ignore groups that don't meet certain criteria.\n## Here the COUNT cant be bigger 10.\n\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_improved, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\nimproved_df = query_job.to_dataframe()\n\n# Print the first five rows of the DataFrame\nimproved_df.head()","3e8a012e":"query_improved = \"\"\"\n                SELECT id, ranking, deleted\n                FROM `bigquery-public-data.hacker_news.comments`\n                ORDER BY id\n                 \"\"\"\n##ORDER BY is usually the last clause in your query, and it sorts the results returned by the rest of your query.\n\n\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_improved, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\nimproved_df = query_job.to_dataframe()\n\n# Print the first five rows of the DataFrame\nimproved_df.head()","4be9e3bb":"query_improved = \"\"\"\n                 WITH time AS \n                 (\n                     SELECT dead AS dead\n                     FROM `bigquery-public-data.hacker_news.comments`\n                 )\n                 SELECT COUNT(1) AS transactions,\n                        dead\n                 FROM time\n                 GROUP BY dead\n                 ORDER BY dead\n                 \"\"\"\n\n##A common table expression (or CTE) is a temporary table that you return\n##within your query. CTEs are helpful for splitting your queries into readable \n##chunks, and you can write queries against them.\n\n# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query_improved, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\ntransactions_by_date = query_job.to_dataframe()\n\n# Print the first five rows\ntransactions_by_date.head()","7bba9f07":"#Note the repos and dataset used in this code is not downloaded to this kernel, so it will note work\n## However it vill show the concept if ever needed.\n\nquery = \"\"\"\n        SELECT L.license, COUNT(1) AS number_of_files\n        FROM `bigquery-public-data.github_repos.sample_files` AS sf\n        INNER JOIN `bigquery-public-data.github_repos.licenses` AS L \n            ON sf.repo_name = L.repo_name\n        GROUP BY L.license\n        ORDER BY number_of_files DESC\n        \"\"\"\n##In the query, ON determines which column in each table to use to combine the \n##tables. Notice that since the ID column exists in both tables, we have to clarify\n##which one to use. We use p.ID to refer to the ID column from the pets table, and \n##o.Pet_ID refers to the Pet_ID column from the owners table.\n\n##In general, when you're joining tables, it's a good habit to specify which table\n##each of your columns comes from. That way, you don't have to pull up the schema \n##every time you go back to read the query.\n\n##The type of JOIN we're using today is called an INNER JOIN. That means that a \n##row will only be put in the final output table if the value in the columns you're \n##using to combine them shows up in both the tables you're joining. For example, if\n##Tom's ID number of 4 didn't exist in the pets table, we would only get 3 rows \n##back from this query. There are other types of JOIN, but an INNER JOIN is very\n##widely used, so it's a good one to start with.\n\n\n# Set up the query (cancel the query if it would use too much of \n# your quota, with the limit set to 10 GB)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query, job_config=safe_config)\n\n# API request - run the query, and convert the results to a pandas DataFrame\nfile_count_by_license = query_job.to_dataframe()","acf47457":"# save our dataframe as a .csv \njob_post_scores.to_csv(\"job_post_scores.csv\")","99475fc9":"## GROUP BY","2816b2ff":"- Avoid using the asterisk (*) in your queries.\n- For initial exploration, look at just part of the table instead of the whole thing.\n- Be cautious about joining tables.\n- Don't rely on LIMIT","6fae388a":"## SELECT FROM","f3d36dab":"## ORDER BY & Dates","566495d2":"## JOIN","ef3dd735":"## Check the data","eeced297":"# Run a query","76a9928b":"# Common mistakes","6d3fc117":"## WITH & AS","886ae622":"# Note none of the below code is written by me, it is just notes for SQL. The content of this kernel comes from courses on kaggle and other notebooks!\n","35e4febc":"# Save the data from your query as a .csv","f89279b3":"### Dont try to take the full dataset and make it to a .csv file since it is to large. And it will take up all the 5 TB to fast. ","e203770b":"# Check the size"}}