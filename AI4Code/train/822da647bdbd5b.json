{"cell_type":{"ec6fe47d":"code","870b25b0":"code","e0d453c4":"code","894051c5":"code","50d5d88a":"code","acbd36af":"code","473a5b7a":"code","81ed034d":"code","64e1130d":"code","ccde267a":"code","be0824d8":"code","077851d9":"code","8906dc63":"code","4fa55539":"code","a5cbb3ab":"code","bc7fb040":"code","feee1acd":"code","f36cb8f8":"code","56030cf4":"code","9714a490":"code","243c52a9":"code","f154c068":"code","37aaa572":"code","2de48a59":"code","e257a6c4":"code","36b8546f":"code","3143554d":"code","bb71ad51":"code","92cc71c1":"code","92a41b7c":"code","872852ed":"code","d8d6ddf2":"code","79b4f827":"code","496c45f0":"code","bc1eef27":"code","0d3062da":"code","4b22df41":"code","c5e4c0dc":"code","aabb0515":"markdown","a250ce33":"markdown","38572060":"markdown","a28805ed":"markdown","07490eec":"markdown","c439ed6e":"markdown","b6f7c658":"markdown","30d33d5b":"markdown","93a58d74":"markdown","176bdf1e":"markdown","87ca2571":"markdown","f6a19e48":"markdown","44b523d3":"markdown","e05288d9":"markdown","b8520c52":"markdown","49867105":"markdown","bd76fc63":"markdown","4ffafd55":"markdown","ba590f8f":"markdown","00171541":"markdown","e0a3e9a8":"markdown","54b4a2fb":"markdown","45ebbe08":"markdown","a35dde79":"markdown","def635ea":"markdown","5e506723":"markdown","1bfa8007":"markdown","2d4f6f53":"markdown","399bc18e":"markdown","0bbad752":"markdown","cdfb803b":"markdown","900af4a3":"markdown"},"source":{"ec6fe47d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","870b25b0":"train = pd.read_csv('..\/input\/titanic-machine-learning-from-disaster\/train.csv')","e0d453c4":"train.head()","894051c5":"train.info()","50d5d88a":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","acbd36af":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',data=train,palette='RdBu_r')","473a5b7a":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Sex',data=train,palette='RdBu_r')","81ed034d":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Pclass',data=train,palette='rainbow')","64e1130d":"sns.countplot(x='SibSp',data=train)","ccde267a":"import cufflinks as cf\ncf.go_offline()","be0824d8":"train['Fare'].iplot(kind='hist',bins=30,color='yellow')","077851d9":"plt.figure(figsize=(12, 7))\nsns.boxplot(x='Pclass',y='Age',data=train,palette='winter')","8906dc63":"def impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n\n        if Pclass == 1:\n            return 37\n\n        elif Pclass == 2:\n            return 29\n\n        else:\n            return 24\n\n    else:\n        return Age","4fa55539":"train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)","a5cbb3ab":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","bc7fb040":"train.drop('Cabin',axis=1,inplace=True)","feee1acd":"train.head()","f36cb8f8":"sex = pd.get_dummies(train['Sex'],drop_first=True) #replacing male female with either male or not male\nembark = pd.get_dummies(train['Embarked'],drop_first=True) #same for embarked","56030cf4":"train = pd.concat([train,sex,embark],axis=1)","9714a490":"train.head()","243c52a9":"train.drop(['Sex','Embarked','Name','Ticket'],axis=1,inplace=True)","f154c068":"train.head()","37aaa572":"train.tail()","2de48a59":"train.drop(['PassengerId'],axis = 1, inplace = True)","e257a6c4":"train.head()","36b8546f":"from sklearn.model_selection import train_test_split","3143554d":"X_train, X_test, y_train, y_test = train_test_split(train.drop('Survived',axis=1), \n                                                    train['Survived'], test_size=0.40, \n                                                    random_state=101)","bb71ad51":"from sklearn.linear_model import LogisticRegression","92cc71c1":"logmodel = LogisticRegression(max_iter=1000)\n","92a41b7c":"logmodel.fit(X_train,y_train)","872852ed":"predictions = logmodel.predict(X_test)","d8d6ddf2":"from sklearn.metrics import classification_report","79b4f827":"print(classification_report(y_test,predictions))","496c45f0":"from sklearn.metrics import confusion_matrix","bc1eef27":"cm = confusion_matrix(y_test,predictions)\nprint(cm)","0d3062da":"from sklearn.metrics import accuracy_score","4b22df41":"accuracy_score(y_test, predictions,normalize=True)","c5e4c0dc":"TruePositive = cm[1,1]\nprint(TruePositive)\nTrueNegetive = cm[0,0]\nprint(TrueNegetive)\nFalsePositive = cm[0,1]\nprint(FalsePositive)\nFalseNegetive = cm[1,0]\nprint(FalseNegetive)","aabb0515":"**Now we can remove columns Name, Sex, Ticket, Embarked**","a250ce33":"**So, Most of them who not Survived were Male, and from who survived most of the are Female.**","38572060":"**And Most of the 3rd class Passengers didn't survive. and from the survival list, Most of them from 1st Class passenger.**","a28805ed":"**So, Most of the Passengers were Alone, and around 200 people have partners, may be with their spouse.**","07490eec":"**We can Drop PassengerId also.**","c439ed6e":"**Lets check for Passengers with spouse or siblings.**","b6f7c658":"So Our Model is doing well.","30d33d5b":"**and add new sex and embark output in train dataset.**","93a58d74":"**Majority of people spent 10 to 30 USD of that time.**","176bdf1e":"We can check precision,recall,f1-score using classification report!","87ca2571":"**So the Age Column has no Null any more. As there is so much Null in Cabin column , we will Drop the column.**","f6a19e48":"**Now Check for the Nulls again.**","44b523d3":"Let's start by splitting our data into a training set and test set","e05288d9":"**Lets check the confusion matrix also,**","b8520c52":"## Training and Predicting","49867105":"Let's evaluate our model!","bd76fc63":"We want to fill in missing age data instead of just dropping the missing age data rows. One way to do this is by filling in the mean age of all the passengers (imputation). However we can be smarter about this and check the average age by passenger class.","4ffafd55":"## Evaluation","ba590f8f":"**We only use Logistic Regression **","00171541":"# Titanic Dataset - with Python and Logistic Regression\n\nFor this lecture we will be working with the Titanic Data Set from Kaggle. This is a very famous data set and very often is a students first step in machine learning! \n\nWe will be trying to predict a classification- survival or deceased.\nLets begin our understanding of implementing Logistic Regression in Python for classification.\n\n## Import Libraries\nLets import some libraries to get started!","e0a3e9a8":"## Train Test Split","54b4a2fb":"# Exploratory Data Analysis\n\nLet's begin some exploratory data analysis! We'll start by checking out missing data!\n\n## Missing Data\n\nWe can use seaborn to create a simple heatmap to see where we are missing data!","45ebbe08":"## Converting Categorical Features\n\nWe'll need to convert categorical features to dummy variables using pandas! Otherwise our machine learning algorithm won't be able to directly take in those features as inputs.","a35dde79":"# Building a Logistic Regression model","def635ea":"**Our Dataset is clean.**","5e506723":"### Accuracy","1bfa8007":"**Let's take a look how much people spent.**","2d4f6f53":"**Passenger Id is also just indexing. lets check**","399bc18e":"**In Cabin column There are so many nulls In Age Column there are also some significant Nulls.**","0bbad752":"### Import the Dataset","cdfb803b":"**Let's continue on by visualizing some more of the data! Check out the video for full explanations over these plots, this code is just to serve as reference.**","900af4a3":"# Data Cleaning"}}