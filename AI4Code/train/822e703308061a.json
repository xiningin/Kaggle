{"cell_type":{"9d496b6c":"code","c91c1a25":"code","143c5107":"code","1e4b87f3":"code","85c79f28":"code","afbc1cf1":"code","7ded686e":"code","041bde7d":"code","cf076418":"code","ba2413f4":"code","04268d74":"code","f9db22c8":"code","820d2564":"code","1cff91df":"code","315301b0":"code","3389cb12":"code","62144365":"code","83fd20bc":"code","1b72e594":"code","28d92f52":"code","66515226":"code","9917412a":"code","148a42bd":"code","46d13eda":"code","9d7a1668":"code","ebdd2a99":"code","f8ed4f46":"code","bef2cf1f":"code","b810372f":"code","776dfefa":"code","93e37e4c":"code","dd88b60f":"code","bb915e91":"code","a0e3db62":"code","d134a92d":"code","acdc2588":"code","56dd7d89":"code","b3d97a4d":"code","99d10ad7":"code","6c608d12":"code","38539e16":"code","d9a8b212":"code","afa6d191":"code","f64cb742":"markdown","bcfebb86":"markdown","d7fbd02f":"markdown","5bbe9aa2":"markdown","afa56de2":"markdown","a031a0c1":"markdown","ed555db7":"markdown","39524f7f":"markdown","60960e1c":"markdown","6c4a2b8f":"markdown","8041b5b2":"markdown","4e93377b":"markdown","0e32ed65":"markdown","8af41161":"markdown","a51d3b74":"markdown","8de60640":"markdown","656b8c2f":"markdown","3c14a93d":"markdown","f2ed1fdb":"markdown","f2cdc6ea":"markdown","a50ebff3":"markdown","96ccfbbf":"markdown","3f28162e":"markdown","bf98780a":"markdown","54f83dde":"markdown"},"source":{"9d496b6c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\ndf = pd.read_csv('..\/input\/train.csv')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c91c1a25":"df.head()","143c5107":"df.info()","1e4b87f3":"sns.heatmap(df.isnull(), yticklabels = False, cbar = False, cmap = 'viridis')","85c79f28":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',data=df,palette='RdBu_r')","afbc1cf1":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Sex',data=df,palette='RdBu_r')","7ded686e":"sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Pclass',data=df,palette='rainbow')","041bde7d":"df['Age'].hist(bins=30,color='darkred',alpha=0.7)","cf076418":"sns.boxplot(x='Pclass',y='Age',data=df,palette='winter')","ba2413f4":"def impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age) :\n        \n        if Pclass == 1 :\n            return 37\n        if Pclass == 2 :\n            return 29\n        else :\n            return 24\n        \n    else : \n        return Age\n    \ndf['Age'] = df[['Age','Pclass']].apply(impute_age, axis =1)","04268d74":"sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')","f9db22c8":"df.drop('Cabin',axis =1, inplace = True)","820d2564":"sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')","1cff91df":"df.info()","315301b0":"sex = pd.get_dummies(df['Sex'],drop_first=True)\nembark = pd.get_dummies(df['Embarked'],drop_first=True)","3389cb12":"df.drop(['Sex','Embarked','Name','Ticket'],axis=1,inplace=True)","62144365":"df = pd.concat([df,sex,embark],axis=1)","83fd20bc":"df.head()","1b72e594":"from sklearn.linear_model import LogisticRegression","28d92f52":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df.drop('Survived',axis=1), \n                                                    df['Survived'], test_size=0.30, \n                                                    random_state=42)","66515226":"logmodel = LogisticRegression()\nlogmodel.fit(X_train, y_train)","9917412a":"predictions = logmodel.predict(X_test)\n\nfrom sklearn import metrics\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nprint('The accuracy of the Logistic Regression is',metrics.accuracy_score(predictions, y_test))","148a42bd":"print(classification_report(y_test,predictions))\nprint(confusion_matrix(y_test, predictions))","46d13eda":"from sklearn.svm import SVC","9d7a1668":"SVCmodel = SVC()\nSVCmodel.fit(X_train, y_train)","ebdd2a99":"predictionsSVC = SVCmodel.predict(X_test)\nprint('The accuracy of the Support Vector Machine is',metrics.accuracy_score(predictionsSVC, y_test))","f8ed4f46":"print(classification_report(y_test,predictionsSVC))\nprint(confusion_matrix(y_test, predictionsSVC))","bef2cf1f":"param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']}","b810372f":"from sklearn.model_selection import GridSearchCV","776dfefa":"grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)","93e37e4c":"grid.fit(X_train,y_train)","dd88b60f":"grid.best_params_","bb915e91":"grid_predictions = grid.predict(X_test)\nprint('The accuracy of the Support Vector Machine with Grid Search is',metrics.accuracy_score(grid_predictions, y_test))","a0e3db62":"print(classification_report(y_test,grid_predictions))\nprint(confusion_matrix(y_test, grid_predictions))","d134a92d":"from sklearn.neighbors import KNeighborsClassifier","acdc2588":"knn = KNeighborsClassifier(n_neighbors = 1)\nknn.fit(X_train,y_train)","56dd7d89":"KNNpredictions = knn.predict(X_test)","b3d97a4d":"print('The accuracy of KNN with 1 neighbor is',metrics.accuracy_score(KNNpredictions, y_test))","99d10ad7":"error_rate = []\n\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","6c608d12":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","38539e16":"knn2 = KNeighborsClassifier(n_neighbors = 7)\nknn2.fit(X_train,y_train)\n","d9a8b212":"KNN2predictions = knn2.predict(X_test)","afa6d191":"print('The accuracy of KNN with 7 neighbors is',metrics.accuracy_score(KNN2predictions, y_test))","f64cb742":"An explanation : \n\nAs you can see, we have converted the variables 'sex' and 'embark' into dummy variables.\n\nWe have also dropped the initial columns (sex, embark, name, ticket).\n\nThis is because name and ticket are variables that intuitively do not affect survival, and they can't be converted to dummy variables as well.\n\nWe then join the newly made 'sex' and 'embark' columns (showing dummy variables) back into the dataframe. \n\n","bcfebb86":"**SLIGHTLY BETTER! But nowhere near Logistic Regression ;)**","d7fbd02f":"**STEP 6**\n\nBuilding and executing a Logistic Regression Model.\n","5bbe9aa2":"**STEP 5**\n\nConverting Categorical Variables into dummy variables\n\nTo put it simply, categorical variables like sex = male\/ female are written in words, and very unfriendly to our ML model.Our model can't process words as training data.\n\nWe are going to convert these variables into numbers 1 & 0 for our model to process with ease.","afa56de2":"As you can see, SVM has a much lousier predictive value in this instance. This might be due to the nature of the data and its suitability to fit into SVMs.  However, before we give up, we have a few tricks up our sleeve to see if we can boost the effectivness of SVMs. \n\nWhat we are going to do is run something called a Grid Search. What this does is run through many possible combinations that make up the parameters of this SVM model, and find the most effective one. \n\nParameters are basically settings in a model that affects the model's behaviour. They are not directly a source fo data\/ training example, but rather, contro lthe way the model processes data, such as prioritizing certain features over others to make a prediction. ","a031a0c1":"As the heatmap shows, we have successfully filled up the appropriate missing values under the Age column.\n\nWe will now drop the Cabin column as it has too many missing pieces of data.","ed555db7":"**STEP 6a. ** : Import Logistic Regression Model from Sci-Kit Learn\n\nSci-Kit Learn (aka sklearn) is a machine learning and data science library with pre-built prediction models. To code the regression formula as a full function using numpy is tedious and beyond the scope of this tutorial.\n\nRemember, the goal of our model is one of classification - to predict which class of survival passengers in the test data will fall into.","39524f7f":"**STEP 8**\n\n**K Nearest Neighbours (KNN)**\n\nFinally, we shall attempt to a KNN model on this data set and see if it yields good results. Following the same procedures as before :","60960e1c":"Now, just like he SVM before, a poor result is not the end of the world! We can once again adjust the settings of a model (in this case the n_neighbors) to get an optimal result, \n\n**Before we proceed, do be reminded that KNN is not a parametric model! So we can't exactly call the n_neighbors a 'parameter'.**\n\nNow, let's plot a graph showing the error rate of the model against the K value (ie the n_neighbors).\n\nFrom here, we can find the K value with the lowest error rates! \n\nThis also shows the importance of visualisation when it comes to data science. ","6c4a2b8f":"**STEP 7**\n\n**Support Vector Machines (SVM)**\n\nWe shall run similar steps to STEP 6 above, but this time, importing and using SVMs in the place of Logistic Regression, to see if it has a higher predictive value. ","8041b5b2":"**STEP 2**\n\nCheck for missing data. Looks abit complex here, but nothing to worry about. I simply used seaborn, a python visualisation library, to run a heat map on missing values. The rest of the code is for aesthetic purposes.","4e93377b":"Notice we has to remove the 'Survived' column from our 'X' as it is the label of the training examples, and not a feature\/variable itself.","0e32ed65":"**STEP 1**\n\nImport the necessary libraries into this notebook\/console\n\nA notebook\/console is just a place where you can type, execute and edit code. Think of it as an environment that everything else takes place in.\n\nLibraries are exactly what they mean - pre made sets of code that are readily available for use. Without libraries, you'll have to write alot more code than you do now!\n\nNotice that I also imported a dataset (ie the 'train') data to train our models. I labelled this 'df'.","8af41161":"**NOT TOO SHABBY! About an 80% prediction rate. ;)**","a51d3b74":"**THANKS FOR TUNING IN AND SEE YOU IN MY NEXT KERNEL! **","8de60640":"**STEP 6c. ** : Fit data into Model\n\nFirst we instantise the model as a function. This just means we can use other functions like '.fit' on it to fit our training data.","656b8c2f":"And now, it has come to the time to explain the reasons behind the results. \n\nFirst off, KNN did poorly in this instance as it is traditionally an unsupervised learning algorithm. However, because we are feeding our model labels for the training data (in the form of \"Survived\"), our problem is one suited for Supervised Learning, Simply classifying the passengers based on their features without a reference label can place emphasis on classification leaning towards other traits and group characteristics over survivability (our label).  While I have seen other kernels pull off >80% predictions with KNN, I can't seem to replicate the results.  Several sites have also shown the KNN classifier to be effective for the titanic dataset, so if anyone knows why, I would love to hear your answer.\n\nSVMs didn't do much better as well. SVMs have their foundations in a technique called the \"kernel trick\", an act of using a kernel to preserve linearity between variables and labels. However, most SVMs require further engineering of the kernel, and various techniques to choose the appropriate hyperparamters to allow for sufficient generalisation performance. \n\nIf you are interested, feel free to learn more about the various ML models here : \n\nhttp:\/\/web.mit.edu\/6.034\/wwwbob\/svm-notes-long-08.pdf [](http:\/\/web.mit.edu\/6.034\/wwwbob\/svm-notes-long-08.pdf)\n\nhttps:\/\/medium.com\/@adi.bronshtein\/a-quick-introduction-to-k-nearest-neighbors-algorithm-62214cea29c7 [](https:\/\/medium.com\/@adi.bronshtein\/a-quick-introduction-to-k-nearest-neighbors-algorithm-62214cea29c7)\n\nA great book to read on ML\/DL from a mathematicakl perspective : \nhttps:\/\/www.deeplearningbook.org\/ [](https:\/\/www.deeplearningbook.org\/)\n","3c14a93d":"**INTRODUCTION**\n\nThis is my first kernel on Kaggle, and I will be attempting to predict the test data using a comparison of Supervised Learning ML models.\n\nThis is largely for beginners who are keen on learning ML, and are interested to learn a methodical workflow that covers importation of libraires and data cleaning to actual modelling. \n\nSure, this is dataset done to depth on Kaggle, but if you are keen to see my approach, feel free to stay and give me an upvote.** I'll be explaining things in layman terms** so if that's what you are looking for, good for you :)","f2ed1fdb":"**As you can see, predictive value has certainly increased to 75 % ! **","f2cdc6ea":"It is apparent that 'Age' and 'Cabin' are missing alot of values. Let's keep this in mind while we proceed to do some Exploratory Data Analysis.\n\nThereafter, we will proceed onto Data Cleaning.","a50ebff3":"**Now we see that a K value of 7 yields a low error rate. Let's use 7 in our model. **","96ccfbbf":"**STEP 3**\n\nExploratory Data Analysis (EDA)\n\nFor those of you unsure with EDA, it is just a way to visualise your training data, and look at the relationships between different variables. \n\nThis is primarily done here once again using seaborn (ie. sns)","3f28162e":"**STEP 6b. ** : Train Test Split\n\nWhat this step does is split the training data given by kaggle into a training data set and a test data set internally. \n\nWe shall use this test data to gauge the predictive value of our model before we apply it to Kaggle's test data under the Titanic data set competition.\n\nThe model will then compare and find a relationship (based on logistic regression) between X_train (ie the training features) and y_train (ie the label).\n\nWe start by importing train_test_split from sklearn and then assigning examples in our current df dataframe to either 'train' or 'test' data sets for our model to run on. ","bf98780a":"**STEP 4**\n\nData Cleaning\n\nWe will attempt to impute values into the missing age values to run our model. We do this by first associating a relationship between age and passenger class.\n\nOur boxplot shows the median age values for each passenger class.\n\nNext, we create a function to impute these median age values into the missing slots as seen on our heatmap earlier.\n\nLastly, we apply this function to the age column of the training dataframe.","54f83dde":"**STEP 6d. ** : Import and run predictions on test data\n\nWe shall now use the trained model above to predict the y_test values from the X_test examples and features. \n\nWe will then compare it to the true y_test labels and see how accurate our model is.\n\nRemember, the goal of our model is one of classification - to predict which class of survival passengers in the test data will fall into."}}