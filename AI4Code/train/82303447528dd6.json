{"cell_type":{"a650b18d":"code","9bcfd819":"code","f61fe716":"code","813a38e3":"code","277b67cd":"code","33a9d0e2":"code","79239b77":"code","f525b374":"code","d05a0940":"code","6dd59f5f":"code","d47ef715":"code","8e75d914":"code","343d9e12":"markdown","402e3c78":"markdown","bdf13001":"markdown","94e650ae":"markdown","93728cce":"markdown","74827d55":"markdown","7e0c86c6":"markdown","683bf92c":"markdown","f36fc7ac":"markdown","2475c722":"markdown","35e4ef6f":"markdown","8ddb25b3":"markdown","bd3b71fa":"markdown","4a59c8fb":"markdown","092c1b63":"markdown","0c29cab1":"markdown","364a75a9":"markdown","b826c1ce":"markdown","515063b2":"markdown","4e2660bf":"markdown","1dbe6667":"markdown","78ebe494":"markdown","d534230b":"markdown","1ba0c7a3":"markdown","90c0be1d":"markdown","ba5b6c50":"markdown","8a7975f1":"markdown"},"source":{"a650b18d":"#Numpy is used so that we can deal with array's, which are necessary for any linear algebra\n# that takes place \"under-the-hood\" for any of these algorithms.\n\nimport numpy as np\n\n\n#Pandas is used so that we can create dataframes, which is particularly useful when\n# reading or writing from a CSV.\n\nimport pandas as pd\n\n\n#Matplotlib is used to generate graphs in just a few lines of code.\n\nimport matplotlib.pyplot as plt\n\n\n#Import the classes we need to test linear, ridge, and lasso to compare\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, LassoCV\n\n#Need these for selecting the best model\nfrom sklearn.model_selection import KFold, GridSearchCV\n\n\n#These will be our main evaluation metrics \nfrom sklearn.metrics import r2_score, mean_squared_error\n\n\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\n\n# Will use this to \"normalize\" our data.\nfrom sklearn.preprocessing import normalize\n\n","9bcfd819":"#read the data from csv\ndataset = pd.read_csv('..\/input\/50-startups\/50_Startups.csv')\n\n#take a look at our dataset.  head() gives the first 5 lines. \ndataset.head()","f61fe716":"#drop the column\ndataset = dataset.drop(columns = ['State'])\n\n#take a look again \ndataset.head()","813a38e3":"#set independent variable by using all rows, but just column 1.\nX = dataset.iloc[:, :-1].values\n\n#set the dependent variable using all rows but only the last column. \ny = dataset.iloc[:, -1].values\n\n#lets take a look at X right now.\nX[0:10]","277b67cd":"X = normalize(X, 'l2')\n\nX[0:10]","33a9d0e2":"#split the dataset.  Take 40% to be our test set. \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 0)\n","79239b77":"#this sets the object regressor to the class of LinearRegression from the Sklearn library.\nregressor = LinearRegression()\n\n#this fits the model to our training data.\nregressor.fit(X_train, y_train)","f525b374":"#Predict on our test set.\ny_pred = regressor.predict(X_test)","d05a0940":"#calculate the R^2 score\nscore = r2_score(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\n\n#print out our score properly formatted as a percent.\nprint(\"R^2 score:\", \"{:.4f}%\".format(score))\nprint(\"MSE\", round(mse,2))","6dd59f5f":"alphas = [-5, -1, 1e-4, 1e-3, 1e-2, 1, 5]\n\ndef test_alpha(a):\n    model_lasso = Lasso(alpha=a)\n    model_lasso.fit(X_train, y_train) \n    pred_test_lasso = model_lasso.predict(X_test)\n    new_score = r2_score(y_test, pred_test_lasso)\n    new_mse = mean_squared_error(y_test, pred_test_lasso)\n    print('ALPHA: {:.3f} R2 SCORE: {:.4f}% new_score, {:.1f}'.format(a, new_score, new_mse))\n    \n    \nfor alpha in alphas:\n    test_alpha(alpha)\n","d47ef715":"alphas = [-5, -1, 1e-4, 1e-3, 1e-2, 1, 5]\n\ndef test_alpha_ridge(a):\n    model_lasso = Ridge(alpha=a)\n    model_lasso.fit(X_train, y_train) \n    pred_test_lasso = model_lasso.predict(X_test)\n    new_score = r2_score(y_test, pred_test_lasso)\n    new_mse = mean_squared_error(y_test, pred_test_lasso)\n    print('ALPHA: {:.3f} R2 SCORE: {:.4f}% new_score, {:.1f}'.format(a, new_score, new_mse))\n    \n    \nfor alpha in alphas:\n    test_alpha_ridge(alpha)","8e75d914":"new_alphas = [1e-15,1e-10,1e-8,1e-4, 1e-3, 1e-2, 1]\n\nfor alpha in new_alphas:\n    test_alpha_ridge(alpha)","343d9e12":"Next we will need to load our data.\n\nI will use data on 50 start ups as I have for decision trees and random forests. ","402e3c78":"The penalty is added to our residual, and then the algorithm proceeds via the least-squares method.\n\n![image.png](attachment:image.png)","bdf13001":"We calculate the residuals as usual.\n\n![image.png](attachment:image.png)","94e650ae":"Overall, both regularization techniques help reduce overfitting, especially with small datasets or those with many variables.","93728cce":"The goal of regularization is to improve the overall fit by increasing **\u201cbias\u201d** to reduce **\u201cvariance\u201d**, by adding a penalty that scales with model complexity.\n\n![image.png](attachment:image.png)","74827d55":"Ok it's looking good.  Now we need to select the X variables and the Y variable (Independent and Dependent)","7e0c86c6":"Applying this to linear regression, we start with a line through our data.\n\n![image.png](attachment:image.png)","683bf92c":"Now we can use the model to predict fit on the test set. ","f36fc7ac":"This is also powerful with higher dimensional data, as the penalty is calculated using the coefficients of all predictive variables.\n\n![image.png](attachment:image.png)","2475c722":"Now to keep this simple, I am only going to look at the continous variables, so we need to drop the State Column.","35e4ef6f":"## Implementation\n\nIn this section I will implement the code in its **simplest verison** so that it is understandable if you are brand new to machine learning. \n\nBelow we will predict salary based on the current role someone is in, and then make some predictions on start ups.  I will also include side by side examples showing how it performs compared to decision trees. \n\nThe first step is to start with \"imports\".","8ddb25b3":"With Ridge Regression, the influence of unnecessary variables minimized, and with Lasso Regression their coefficients can actually drop to zero, removing them from the model all together.\n\n![image.png](attachment:image.png)","bd3b71fa":"Next we need to scale down our X variables in order for our \"alphas\" to have an impact later when we introduce LASSO and RIDGE.\n\nWe have two options, standardize and normalize.  Since we don't know the distrobution of our data, we will use normalize.","4a59c8fb":"With the dataset split, we can now load and fit the models.\n\nI will start with basic Linear Regression first.","092c1b63":"Here we see that Lasso only slightly improved the accuracy of the model in the base case scenario.  Specifically where the Alpha was a large negative value.  This means that the true distrubtion actually has a higher slope than our inital model predicted. \n\nNow lets move on to ridge.","0c29cab1":"Next, the penalty is calculated.  For Lasso, the penalty scales with the absolute value of the slope, and for Ridge it scales with the slope squared.\n\n![image.png](attachment:image.png)","364a75a9":"## Conceptual Overview\n\n\nLasso and Ridge regression, also known as L1 & L2 respectively, are **\u201cregularization\u201d** techniques.\n\n![image.png](attachment:image.png)","b826c1ce":"Here we see that Lasso and Ridge Regression in this case **DO NOT** significantly improve the overall fit of the model. \n\nThis is to be expected in some situations, but it is always great to check and verify when trying to fine tune your regression model. \n\nThis idea of regularization has other applications as well, which I will demonstrate in my next notebook on XGBoost! ","515063b2":"This is particularly useful when working with a small amount of training data.\n\n![image.png](attachment:image.png)","4e2660bf":"Finally, we can evaluate the quality of the fit. ","1dbe6667":"The result is a best-fit line with a smaller slope, that will hopefully fit our test data better.\n\n![image.png](attachment:image.png)","78ebe494":"In order to understand the effectiveness of Lasso and Ridge regression we will need a test set, so lets split our data into training and test sets.","d534230b":"Here we see that Ridge is much more sensitive to the scale of Alpha, so lets fine tune it using only positive values. ","1ba0c7a3":"> # Enough to be Dangeous: LASSO and RIDGE Regression\n\n> ### This is the 6th notebook of my **\"Enough to be Dangeous\"** notebook series\n\nSee the other notebooks here:\n\n[Simple Linear regression](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangeous-simple-linear-regression)\n\n[Multiple Linear Regression](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-multiple-linear-regression)\n\n[Polynomial Regression](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-polynomial-regression)\n\n[Decision Tree](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-decision-tree-regression)\n\n[Random Forest](https:\/\/www.kaggle.com\/thaddeussegura\/enough-to-be-dangerous-random-forest-regression)","90c0be1d":"We get an R^2 score of 83%, which is pretty good, but the mean squared error looks super high. \n\nI would normally visualize this, but because we have 4 dimensions in the data that won't be possible.\n\nSo let's move on and look at Lasso Regression and see if we can bring down the error. ","ba5b6c50":"> ## This notebook is separated into two parts:\n\n### **1) Conceptual Overview:**  I will introduce the topic in 200 words or less.\n\n### **2) Implementation:**  I will implement the algorithm in as few lines as possible.","8a7975f1":"With Lasso regression, we have a new metric, 'alpha' to play with.\n\nTo keep this simple, I will just use a \"for-loop\" to test our multiple alphas and show the accuracy. \n\nSo I will define a function to do all of this, and then pass in the alphas. "}}