{"cell_type":{"89673682":"code","3707fa21":"code","5af1f6c9":"code","fbd205ed":"code","ac882527":"code","4d6d25bb":"code","8fa05662":"code","0321a0bd":"code","d507ff1b":"code","184f527c":"code","359a3eaa":"code","0de591e8":"code","0aefdde4":"code","69960685":"code","432996f9":"code","b0e88cc0":"code","99a9ec70":"code","b67b04bd":"code","8a2a98c0":"code","454277ce":"code","0411ad43":"code","64bfe4ef":"code","94172475":"code","fd721a79":"code","32d700e5":"code","5fd8cdc9":"code","bfd8a867":"code","6e847252":"code","188a04bb":"code","c6d73fcf":"code","e96e3af5":"code","47250231":"code","39775a04":"code","9b426fea":"code","d16525c1":"code","9a2aff5b":"code","2f13b7b3":"code","fcfc148c":"code","3f93b718":"code","76b2bc53":"code","9253b0a4":"code","ff19654b":"code","654d3ef7":"code","b71fa3e9":"code","a1e97a31":"code","54335756":"code","07bd9828":"code","23910246":"code","71d4eb7c":"code","297a5409":"code","f271493c":"code","38f531f9":"code","727dc0e0":"code","e3317aae":"code","ccd496a8":"code","5ae06b4f":"code","38f4fad0":"code","d7b148dd":"code","9d3e635d":"code","ceb73d18":"code","95c3cf12":"code","d12691c5":"code","c6100d63":"code","92c83f21":"code","cd85e3ca":"code","af323d50":"code","deab86f4":"code","65dd9f57":"code","c4288460":"code","a568e429":"code","e4fecd48":"code","1f7ea7db":"code","d125c37a":"code","ca6ae882":"code","6fd728da":"code","ece7b0bc":"code","3e97d6ff":"markdown","0cfbfd6b":"markdown","40ba3bf1":"markdown","e4f4d058":"markdown","8e44a419":"markdown","82824d6e":"markdown","ff4e1d1f":"markdown","345f969a":"markdown","2b5d3a02":"markdown","33b575d9":"markdown","5a56cac6":"markdown","864482ca":"markdown","5f2b15b5":"markdown","576f86ed":"markdown","a75cedaa":"markdown","2b7408e0":"markdown","dc362ae8":"markdown","cfbf015f":"markdown","8f508dd4":"markdown","b6286a1e":"markdown","353c9ec4":"markdown","4b1f63b5":"markdown","fd91e0c7":"markdown","32c9142b":"markdown","72dae034":"markdown","d9b10235":"markdown","ad7b8807":"markdown","2fafb4b3":"markdown","a182010d":"markdown","9ab326a9":"markdown","15e88ada":"markdown","66928be6":"markdown","769eab9c":"markdown","74971462":"markdown","abff2563":"markdown","63082ef1":"markdown","1cfbc628":"markdown","336e7347":"markdown","1b31c063":"markdown","62d3cd8e":"markdown","d7262a89":"markdown","43f0b2d2":"markdown","b5753f46":"markdown","34723706":"markdown","c08be3ad":"markdown","b2c6a3c3":"markdown","f2c3704e":"markdown","25eeb49a":"markdown","0e553a8b":"markdown","4e43209e":"markdown","503f9053":"markdown","0be99678":"markdown","ddbc0dea":"markdown","6848e3e4":"markdown","4def41b2":"markdown","3ccc2359":"markdown","bc3ab2d5":"markdown","742ec2af":"markdown","ba591f20":"markdown","edff1416":"markdown","1b325036":"markdown","158f4e3b":"markdown","6b5ee50b":"markdown","2f0133bd":"markdown"},"source":{"89673682":"import numpy as np # linear algebra\nimport pandas as pd # data processing\n\n# libraries for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n# split into train and test, do grid search for hyperparameter optimization and\n# do cross validation with stratified data\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n\n# data scaling\nfrom sklearn.preprocessing import scale, MinMaxScaler\n# test data\nfrom sklearn.metrics import accuracy_score\n\n# models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport xgboost as xgb\n\n# libraries for ANN\nfrom tensorflow import keras\nimport tensorflow as tf\nfrom keras.wrappers.scikit_learn import KerasClassifier\n","3707fa21":"df = pd.read_csv('..\/input\/adult-census-income\/adult.csv', sep=\",\")\ndf.head()","5af1f6c9":"df.shape","fbd205ed":"# Income is the target variable. \ndf['income'].unique() # show unique values\n\n","ac882527":"df['income'].replace(['<=50K','>50K'],[0,1], inplace=True) # replace for 0 and 1\ndf['income'].value_counts() # show number of samples for each value\n","4d6d25bb":"# we can see the percentage of people that has >50k:\nnp.mean(df['income'])\n","8fa05662":"df['workclass'].value_counts() \n","0321a0bd":"# probability of belonging to the group with the highest income\nworkclass_income = df.groupby('workclass')['income'].mean() # there is correlation as spected\n\nplt.rcParams['axes.axisbelow'] = True # grid behind graphs bars\nplt.figure(figsize=(20, 8))\nplt.ylim(0,1) # values from 0 to 1 as there are probabilities\nplt.bar(workclass_income.index.astype(str), workclass_income,\n       color = 'SkyBlue' , edgecolor='black' )\nplt.ylabel('Probability', size=20)\nplt.xlabel('Workclass', size=20)\nplt.grid(axis='y')","d507ff1b":"df['education'].unique() \n","184f527c":"# probability of belonging to the group with the highest income\neducation_income = df.groupby('education')['income'].mean() # there is correlation as spected\n\nplt.figure(figsize=(20, 8))\nplt.ylim(0,1)\nplt.xticks(rotation=30) # rotate axis text\nplt.bar(education_income.index.astype(str), education_income,\n       color = 'SkyBlue', edgecolor='black' )\nplt.ylabel('Probability of earning >50k', size=20)\nplt.xlabel('Education', size=20)\nplt.grid(axis='y')\n","359a3eaa":"df['marital.status'].unique() \n","0de591e8":"# probability of belonging to the group with the highest income\nmarital_income = df.groupby('marital.status')['income'].mean()\n\nplt.figure(figsize=(20, 8))\nplt.ylim(0,1)\nplt.bar(marital_income.index.astype(str), marital_income,\n       color = 'SkyBlue', edgecolor='black' )\nplt.ylabel('Probability of earning >50k', size=20)\nplt.xlabel('Marital status', size=20)\nplt.grid(axis='y')","0aefdde4":"df['occupation'].value_counts() \n","69960685":"# Show null values in common\nwork_ocupation = df.loc[df['workclass'] == df['occupation'],'workclass']\nwork_ocupation.value_counts()\n","432996f9":"# probability of belonging to the group with the highest income\noccupation_income = df.groupby('occupation')['income'].mean()\n\nplt.figure(figsize=(25, 8))\nplt.ylim(0,1)\nplt.xticks(rotation=30) # rotate axis text\nplt.bar(occupation_income.index.astype(str), occupation_income,\n       color = 'SkyBlue', edgecolor='black' )\nplt.ylabel('Probability of earning >50k', size=20)\nplt.xlabel('Occupation', size=20)\nplt.grid(axis='y')","b0e88cc0":"df['relationship'].value_counts() \n","99a9ec70":"# probability of belonging to the group with the highest income\nrelationship_income = df.groupby('relationship')['income'].mean()\n\nplt.figure(figsize=(20, 8))\nplt.ylim(0,1)\nplt.bar(relationship_income.index.astype(str), relationship_income,\n       color = 'SkyBlue', edgecolor='black')\nplt.ylabel('Probability of earning >50k', size=20)\nplt.xlabel('Relationship', size=20)\nplt.grid(axis='y')","b67b04bd":"df['race'].value_counts()\n","8a2a98c0":"race_income = df.groupby('race')['income'].mean()\n\nplt.figure(figsize=(20, 8))\nplt.ylim(0,1)\nplt.bar(race_income.index.astype(str), race_income,\n       color = 'SkyBlue', edgecolor='black')\nplt.ylabel('Probability of earning >50k', size=20)\nplt.xlabel('Race', size=20)\nplt.grid(axis='y')","454277ce":"df['sex'].value_counts()\n","0411ad43":"sex_income = df.groupby('sex')['income'].mean()\n\nplt.figure(figsize=(20, 8))\nplt.ylim(0,1)\nplt.bar(sex_income.index.astype(str), sex_income,\n       color = 'SkyBlue', edgecolor='black')\nplt.ylabel('Probability of earning >50k', size=20)\nplt.xlabel('Sex', size=20)\nplt.grid(axis='y')","64bfe4ef":"df['native.country'].unique() \n","94172475":"# Show number of missing values\ndf.loc[df['native.country'] == '?', 'native.country'].count() \n","fd721a79":"# Show if missing values have something to do with occupation missing data\ndf.loc[df['native.country'] == 'occupation','occupation' ].count()","32d700e5":"# Show if missing values have something to do with workclass missing data\ndf.loc[df['native.country'] == 'workclass','workclass' ].count()","5fd8cdc9":"df.info() # Show continoues variables \n","bfd8a867":"# Group all continous variables \ndf_continous = df[['income', 'age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']]\n# Correlation matrix\nplt.figure(figsize=(15, 8))\nsns.heatmap(data=df_continous.corr(), annot=True, vmin=-1, vmax=1)\n","6e847252":"df['age'].unique()","188a04bb":"# plot histogram\nplt.figure(figsize=(20, 8))\nplt.hist(df['age'],density=True, bins=20, color = 'SkyBlue')\nplt.ylabel('Probability', size=20)\nplt.xlabel('Age', size=20)\nplt.grid(axis='y')","c6d73fcf":"# Show average age by income\ndf.groupby(\"income\")[\"age\"].mean() \n","e96e3af5":"\n# divide age into groups\nage_range = pd.cut(df['age'], bins = [20,30,40,50,60,70,80,90])\n\n# show probability of belonging to the group with the highest income\nage_income = df.groupby(age_range)['income'].mean()\n\n# barplot showing probability of belonging to the group with the highest income per age range\nplt.figure(figsize=(20, 8))\nplt.ylim(0,1)\nplt.bar(age_income.index.astype(str), age_income, color = 'SkyBlue',\n       edgecolor='black')\nplt.ylabel('Probability of earning >50k', size=20)\nplt.xlabel('Age range', size=20)\nplt.grid(axis='y')","47250231":"df.loc[df['fnlwgt'] == '?'] \n","39775a04":"df['education.num'].value_counts()","9b426fea":"df['education'].value_counts() # we can see that it has the same number of values","d16525c1":"df['capital.gain'].unique()\n","9a2aff5b":"df['capital.loss'].unique()\n","2f13b7b3":"df['hours.per.week'].unique()\n","fcfc148c":"# plot histogram\nplt.figure(figsize=(15, 8))\nplt.hist(df['hours.per.week'],density=True, bins=10,  color = 'SkyBlue')\nplt.ylabel('Probability', size=20)\nplt.xlabel('hours per week', size=20)\nplt.grid(axis='y')","3f93b718":"df = df.drop('fnlwgt', axis=1)\ndf = df.drop('education.num', axis=1)\ndf.shape","76b2bc53":"df = df.loc[ (df['workclass'] != '?') & (df['occupation'] != '?') & (df['native.country']!= '?')]\ndf.shape","9253b0a4":"# Split into dependend and independent variables\nX = df.drop('income', axis=1)\ny = df['income']","ff19654b":"# Split X into continous variables and categorical variables\n\nX_continous  = X[['age', 'capital.gain', 'capital.loss', 'hours.per.week']]\n\nX_categorical = X[['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race',\n                   'sex', 'native.country']]\n","654d3ef7":"# Get the dummies\nX_encoded = pd.get_dummies(X_categorical)\n# Concatenate both continous and encoded sets:\nX = pd.concat([X_continous, X_encoded],axis=1)\nX","b71fa3e9":"# Prepare the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 1\/3,\n                                                    stratify=y,random_state=10 )\n\n# MODEL\nlogit = LogisticRegression(max_iter=10000)\nlogit = logit.fit(X_train, y_train)\n\n# CROSS VALIDATION\ncv = StratifiedKFold(n_splits=3) # we make 3 splits\nval_logit = cross_val_score(logit, X_train, y_train, cv=cv).mean()\nval_logit # show validation set score","a1e97a31":"# PREDICTIONS\nlogit_predictions = logit.predict(X_test)\nacc_logit = accuracy_score(y_test,logit_predictions)\nacc_logit # show test set score\n","54335756":"# Prepare the data. We only use categorical independent variables \nX_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size= 1\/3, \n                                                    stratify=y, random_state=10 )\n\n# MODEL\ncnb = CategoricalNB()\ncnb = cnb.fit(X_train, y_train)\n\n# PREDICTIONS\ncnb_predictions = cnb.predict(X_test)\nacc_cnb = accuracy_score(y_test,cnb_predictions)\nacc_cnb # show test set score","07bd9828":"# Prepare the data. We only use continous independent variables \nX_train, X_test, y_train, y_test = train_test_split(X_continous, y, test_size= 1\/3,\n                                                    stratify=y, random_state=10)\n\n# MODEL\ngnb = GaussianNB()\ngnb = gnb.fit(X_train, y_train)\n\n# CROSS VALIDATION\ncv = StratifiedKFold(n_splits=3)\nval_cnb = cross_val_score(gnb, X_train, y_train, cv=cv).mean()\nval_cnb # validation set score\n\n","23910246":"# PREDICTIONS\ngnb_predictions = gnb.predict(X_test)\nacc_gnb = accuracy_score(y_test,gnb_predictions)\nacc_gnb # test set score","71d4eb7c":"# Prepare the data. We scale the data as this algorithm is distance-based\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 1\/3, \n                                                    stratify=y, random_state=10)\n# scale data in a range of (0,1)\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler = scaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n","297a5409":"# HYPERPARAMETERS OPTIMIZATION\n\n# set the hyperparameters we want to test\nparam_grid = {'n_neighbors' : [40, 60, 70]}\n\ncv = StratifiedKFold(n_splits=3)\n\noptimal_params = GridSearchCV(\n    estimator = KNeighborsClassifier(),\n    param_grid = param_grid,\n    scoring = 'accuracy',\n    verbose = 2,\n    cv = cv,\n)\n\n#>>> optimal_params.fit(X_train, y_train)\n\n#>>> optimal_params.best_params_\n# {'n_neighbors': 40}","f271493c":"# MODEL\n\nknn = KNeighborsClassifier(n_neighbors=40)\nknn = knn.fit(X_train, y_train)\n\n# CROSS VALIDATION\ncv = StratifiedKFold(n_splits=3)\nval_knn = cross_val_score(knn, X_train, y_train, cv=cv).mean()\nval_knn # validation score\n","38f531f9":"# PREDICTIONS\nknn_predictions = knn.predict(X_test)\nacc_knn = accuracy_score(y_test,knn_predictions)\nacc_knn # test score","727dc0e0":"# Prepare the data. We scale the data as this algorithm is distance-based\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 1\/3,\n                                                    stratify=y, random_state=10)\n# scale the data (mean=0 and sd=1)\nX_train = scale(X_train)\nX_test = scale(X_test)","e3317aae":"\n# HYPERPARAMETERS OPTIMIZATION\n\n# 1 ROUND\nparam_grid = {\n    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n}\n\ncv = StratifiedKFold(n_splits=3)\n\noptimal_params = GridSearchCV(\n    estimator = svm.SVC(),\n    param_grid = param_grid,\n    scoring = 'accuracy',\n    verbose = 2,\n    cv = cv\n)\n\n#>>> optimal_params.fit(X_train, y_train,)\n\n\n#>>> optimal_params.best_params_\n# {'kernel': 'linear'}\n\n# As linear kernels are the simplest ones the result is unexpected.\n# wW can see how better linear kernels are with respect to the rest.\n\n#>>> opt = optimal_params.cv_results_\n#>>> opt = pd.DataFrame.from_dict(opt)\n#>>> opt[['params', 'mean_test_score']]\n#                   params  mean_test_score\n# 0   {'kernel': 'linear'}         0.843346\n# 1     {'kernel': 'poly'}         0.818580\n# 2      {'kernel': 'rbf'}         0.839865\n# 3  {'kernel': 'sigmoid'}         0.830316\n\n\n# As we can tune parameters from the rbf kernel, we will try to improve\n# its performance\n\n\n# 2 ROUND\n\nparam_grid = {\n    'kernel': ['rbf'],\n    'C' : [1, 2, 3, 4, 5],\n}\n\ncv = StratifiedKFold(n_splits=3)\n\noptimal_params = GridSearchCV(\n    estimator = svm.SVC(),\n    param_grid = param_grid,\n    scoring = 'accuracy',\n    verbose = 2,\n    cv = cv\n)\n\n#>>> optimal_params.fit(X_train, y_train,)\n\n#>>> optimal_params.best_params_\n#{'C': 2, 'kernel': 'rbf'}\n\n#>>> optimal_params.best_score_\n# 0.8398648211769877\n\n# as we see rbf is worse than linear, so linear will be used\n","ccd496a8":"# MODEL\nsuppvm = svm.SVC(kernel='linear')\nsuppvm = suppvm.fit(X_train, y_train)\n\n# CROSS VALIDATION\ncv = StratifiedKFold(n_splits=3)\nval_suppvm = cross_val_score(suppvm, X_train, y_train, cv=cv).mean()\nval_suppvm # validation score","5ae06b4f":"# PREDICTIONS\nsuppvm_predictions = suppvm.predict(X_test)\nacc_suppvm = accuracy_score(y_test,suppvm_predictions)\nacc_suppvm # test score","38f4fad0":"# Prepare the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 1\/3, \n                                                    stratify=y, random_state=10)\n","d7b148dd":"# HYPERPARAMETERS OPTIMIZATION\nparam_grid = {\n'max_depth' : [2,4,6,7,8,9,10,11,12,16,20]\n}\n\ncv = StratifiedKFold(n_splits=3)\n\noptimal_params = GridSearchCV(\n    estimator = DecisionTreeClassifier(),\n    param_grid = param_grid,\n    scoring = 'accuracy',\n    verbose = 2,\n    cv = cv\n)\n\n#>>> optimal_params.fit(X_train, y_train,)\n\n#>>> optimal_params.best_params_\n# {'max_depth': 11}","9d3e635d":"# MODEL\ntree = DecisionTreeClassifier(max_depth=11)\ntree = tree.fit(X_train, y_train)\n\n# CROSS VALIDATION\ncv = StratifiedKFold(n_splits=3)\nval_tree = cross_val_score(tree, X_train, y_train, cv=cv).mean()\nval_tree # validation score","ceb73d18":"# PREDICTIONS\ntree_predictions = tree.predict(X_test)\nacc_tree = accuracy_score(y_test,tree_predictions)\nacc_tree # test score","95c3cf12":"# Prepare the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 1\/3,\n                                                    stratify=y, random_state=10)\n","d12691c5":"# HYPERPARAMETERS OPTIMIZATION\nparam_grid = {\n'max_depth' : [8,10,12,16,18,20],\n'n_estimators': [50,100,200],\n'max_samples': [1,0.8,0.6]\n}\n\ncv = StratifiedKFold(n_splits=3)\n\noptimal_params = GridSearchCV(\n    estimator = RandomForestClassifier(),\n    param_grid = param_grid,\n    scoring = 'accuracy',\n    verbose = 2,\n    cv = cv\n)\n\n#>>> optimal_params.fit(X_train, y_train,)\n\n#>>> optimal_params.best_params_\n#{'max_depth': 16, 'max_samples': 0.6, 'n_estimators': 200}\n","c6100d63":"# MODEL\nRforest = RandomForestClassifier(max_depth=16,max_samples=0.6, n_estimators=200)\nRforest = Rforest.fit(X_train, y_train)\n\n# CROSS VALIDATION\ncv = StratifiedKFold(n_splits=3)\nval_Rforest = cross_val_score(Rforest, X_train, y_train, cv=cv).mean()\nval_Rforest # validation score","92c83f21":"# PREDICTIONS\nRforest_predictions = Rforest.predict(X_test)\nacc_Rforest = accuracy_score(y_test,Rforest_predictions)\nacc_Rforest # test score","cd85e3ca":"# Prepare the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 1\/3,\n                                                    stratify=y, random_state=10)\n","af323d50":"# HYPERPARAMETER OPTIMIZATION\n\n# ROUND 1\n\nparam_grid = {\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.3, 0.1, 0.05],\n    'gamma': [0, 1, 10],\n    'reg_lambda': [0, 1, 10]\n}\n\n\ncv = StratifiedKFold(n_splits=3)\n\noptimal_params = GridSearchCV(\n    estimator=xgb.XGBClassifier(objective='binary:logistic', #for binary classification\n                                eval_metric=\"logloss\",\n                                use_label_encoder=False), #avoid warning (since we have done encoding)\n    param_grid=param_grid,\n    scoring='accuracy',\n    verbose=2,\n    cv = cv\n)\n\n#>>> optimal_params.fit(X_train, y_train,)\n\n#>>> optimal_params.best_params_\n#{'gamma': 0, 'learning_rate': 0.3, 'max_depth': 5, 'reg_lambda': 10}\n\n\n\n# ROUND 2\n\n\nparam_grid = {\n    'max_depth': [4, 5, 6],\n    'learning_rate': [0.3, 0.5],\n    'subsample': [1, 0.8, 0.6, 0.4],\n    'gamma' : [10, 50, 100]\n}\n\n\n\ncv = StratifiedKFold(n_splits=3)\n\noptimal_params = GridSearchCV(\n    estimator=xgb.XGBClassifier(objective='binary:logistic', #for binary classification\n                                eval_metric=\"logloss\",\n                                learning_rate= 0.1,\n                                reg_lambda=0,\n                                use_label_encoder=False), #avoid warning (since we have done encoding)\n    param_grid=param_grid,\n    scoring='accuracy',\n    verbose=2,\n    cv = cv\n)\n\n#>>> optimal_params.fit(X_train, y_train,)\n\n#>>> optimal_params.best_params_\n#{'learning_rate': 0.3, 'max_depth': 5, 'reg_lambda': 10, 'subsample': 1}\n\n\n","deab86f4":"# MODEL\nxgbm = xgb.XGBClassifier(eval_metric=\"logloss\",\n                        learning_rate= 0.3,\n                        reg_lambda=10,\n                        use_label_encoder=False, # as we have done encoding\n                        max_depth=5,\n                        subsample=1)\n\nxgbm = xgbm.fit(X_train, y_train)\n\n# CROSS VALIDATION\ncv = StratifiedKFold(n_splits=3)\nval_xgbm = cross_val_score(xgbm, X_train, y_train, cv=cv).mean()\nval_xgbm","65dd9f57":"# PREDICTIONS\nxgbm_predictions = xgbm.predict(X_test)\nacc_xgbm = accuracy_score(y_test,xgbm_predictions)\nacc_xgbm","c4288460":"# Save predictions with probabilities in order to later make the ensembling\nxgbm_predictions_prob = xgbm.predict_proba(X_test)\nxgbm_predictions_prob = xgbm_predictions_prob[:,1]","a568e429":"# Prepare the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 1\/3, \n                                                    stratify=y, random_state=10)\n# scale the data (mean=0, sd=1)\nX_train = scale(X_train)\nX_test = scale(X_test)\n","e4fecd48":"\n# HYPERPARAMETERS OPTIMIZATION\n\n# ROUND 1\n\n# first we need to define the model \ndef ANN_1(neurons=10, hidden_layers=0, dropout_rate=0, learn_rate= 0.1):\n    # model\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(neurons, input_shape = (X_train.shape[1], ), activation='relu'))\n    for i in range(hidden_layers):\n        # Add one hidden layer\n        model.add(keras.layers.Dense(neurons, activation='relu'))\n        model.add(keras.layers.Dropout(dropout_rate))\n    model.add(keras.layers.Dense(1, activation='sigmoid'))\n    # Compile model\n    optimizer = keras.optimizers.SGD(lr=learn_rate, momentum = 0)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n\n# we will do the grid search with KerasClassifier\nann = KerasClassifier(build_fn=ANN_1, batch_size=30)\n\n\nparam_grid = {\n    'neurons': [10, 30, 60, 100, 200],\n    'hidden_layers': [0, 1, 2],\n    'dropout_rate': [0, 0.1, 0.2, 0.4],\n    'epochs': [8,15],\n    'learn_rate': [0.1, 0.03]\n}\n\ncv = StratifiedKFold(n_splits=3)\n\noptimal_params = GridSearchCV(estimator=ann, param_grid=param_grid, verbose=2, cv=cv)\n\n#>>> optimal_params.fit(X_train, y_train,)\n\n#>>> optimal_params.best_params_\n# {'dropout_rate': 0.2, 'epochs': 15, 'hidden_layers': 1, 'learn_rate': 0.1, 'neurons': 10}\n\n\n\n# ROUND 2\n\ndef ANN_2(init_mode='uniform', activation='relu'):\n    # model\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(10,kernel_initializer=init_mode,\n                                 input_shape = (X_train.shape[1], ), activation=activation))\n    model.add(keras.layers.Dense(10, kernel_initializer=init_mode,activation=activation))\n    model.add(keras.layers.Dropout(0.2))\n    model.add(keras.layers.Dense(1,kernel_initializer=init_mode, activation='sigmoid'))\n    # Compile model\n    optimizer = keras.optimizers.SGD(lr=0.1, momentum = 0)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n\n\nann = KerasClassifier(build_fn=ANN_2, epochs= 15,  batch_size=30)\n\n\nparam_grid = {\n    'init_mode': ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal',\n                  'glorot_uniform', 'he_normal', 'he_uniform'],\n    'activation': ['softmax','relu', 'tanh', 'sigmoid']\n}\n\n\ncv = StratifiedKFold(n_splits=3)\n\n\noptimal_params = GridSearchCV(estimator=ann, param_grid=param_grid, verbose=2, cv=cv)\n\n#>>> optimal_params.fit(X_train, y_train)\n\n#>>> optimal_params.best_params_\n# {'activation': 'relu', 'init_mode': 'uniform'}\n\n","1f7ea7db":"# MODEL\n\ndef ANN_():\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(10,kernel_initializer='uniform',\n                                 input_shape = (X_train.shape[1], ), activation='relu'))\n    model.add(keras.layers.Dense(10, kernel_initializer='uniform',activation='relu'))\n    model.add(keras.layers.Dropout(0.2))\n    model.add(keras.layers.Dense(1,kernel_initializer='uniform', activation='sigmoid'))\n    # Compile model\n    optimizer = keras.optimizers.SGD(lr=0.1, momentum = 0)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n\n# we define a learning rate schedule in order to decrease the learning rate\n# as we epoch increases.\ndef scheduler(epoch, lr):\n  if epoch < 10:\n    return lr\n  if epoch < 15:\n\t  return 0.05\n  else:\n      return 0.01\n\n\n# Early stopping: stop the learning when it has 3 consecutive epoch without improvement\ncallback2 = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n# Learning rate schedule\ncallback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n\nann = KerasClassifier(build_fn=ANN_, epochs= 15,  batch_size=30, verbose=0)\n\n\n# CROSS VALIDATION\ncv = StratifiedKFold(n_splits=3)\nval_ann= cross_val_score(ann, X_train, y_train,\n                         cv=cv, fit_params={'callbacks': [callback,callback2]}).mean()\nval_ann # validation score","d125c37a":"# Neural Networks Ensembling\n# we will make 10 Neural Networks and then join its predictions by averaging. \n\n\nn_members = 10\nann_dict = {} # dictionary where we will store the predictions\n\nfor i in range(n_members):\n    ann = ANN_()\n    ann.fit(X_train, y_train, epochs=15, batch_size=30, \n            verbose=0, callbacks=[callback, callback2])\n    ann_predictions = ann.predict(X_test)\n    ann_predictions = ann_predictions.reshape(ann_predictions.shape[0], )\n    ann_dict[\"ann%s\" %i] = ann_predictions\n\n# create a pandas DataFrame from the dictionary\nann_dataframe = pd.DataFrame.from_dict(ann_dict)\n\nann_mean_prob = ann_dataframe.mean(axis=1) #averaging all the ANN predictions for each row\nann_mean = np.where(ann_mean_prob > 0.5, 1, 0) # transforem probabilities to a binary variable\nacc_ann = accuracy_score(y_test, ann_mean)\nacc_ann # test score","ca6ae882":"# Select the models by looking at the validation score\n\nnp.array([val_logit, val_cnb, val_knn, val_suppvm, val_tree, val_Rforest, val_xgbm, val_ann])","6fd728da":"# Create dataset with the best predictions\nbest_predictions = pd.DataFrame(data= {'ann':ann_mean_prob, \n                                       'xgb':xgbm_predictions_prob})\n\n# We give a higher weight to XGBoost\nensembling = best_predictions['ann']* 0.4 + best_predictions['xgb']*0.6\n\n# Probabilities to binary\nensembling_binary = np.where(ensembling > 0.5, 1, 0)\nacc_ensembling = accuracy_score(y_test, ensembling_binary)\nacc_ensembling # test score","ece7b0bc":"# make a dictionary with all the results\nresults = {'Logistic Regression': acc_logit, \n           'Categorical Naive Bayes': acc_cnb,\n           'Gaussian Naive Bayes': acc_gnb,\n           'K-Nearest Neighbors': acc_knn ,\n           'Support Vector Machines': acc_suppvm,\n           'Decision Trees':acc_tree ,\n           'Random Forest': acc_Rforest,\n           'XGBoost':acc_xgbm ,\n           'Artificial Neural Networks':acc_ann ,\n           'XGBoost-ANN Ensembling': acc_ensembling,         \n          }\n\nresults_dataframe = pd.DataFrame.from_dict(results, orient='index', \n                                           columns=['Accuracy'])\nresults_dataframe","3e97d6ff":"# **2. Cleaning data** <a class=\"anchor\" id=\"2\"><\/a>","0cfbfd6b":"Probabilities are what we would expect: it increases with education.","40ba3bf1":"## **Workclass**","e4f4d058":"We can see that there are missing data.","8e44a419":"We can see that white people and Asian-Pac-Islander have the higher probabilities.","82824d6e":"## **Marital.status**","ff4e1d1f":"## **Income**","345f969a":"# **4.5 Support Vector Machines** <a class=\"anchor\" id=\"4.5\"><\/a>","2b5d3a02":"# **4.3 Gaussian Na\u00efve Bayes** <a class=\"anchor\" id=\"4.3\"><\/a>","33b575d9":"There are not null values.","5a56cac6":"# **From Na\u00efve to XGBoost and ANN: Adult Census Income**\n\nIn this kernel, we will use the Adult Census Income dataset from UCI Machine Learning in order to predict if a person earns more than 50k per year or not. Therefore, this will be a binary classification problem where input data is both continous and categorical. In order to make predictions we will develop the following models:\n\n- Logistic Regression\n- Categorical Na\u00efve Bayes\n- Gaussian Na\u00efve Bayes\n- K-Nearest Neighbors\n- Support Vector Machines\n- Decision Trees\n- Random Forest\n- XGBoost\n- Artificial Neural Networks\n\n\nAs usual, first we will have a look at the data in order to understand the different variables, then we will clean it in order to use it for prediction purposes, and finally we will develop the different models. These models will be tested with cross validation in order to pick the best ones and use them to develop an ensemble model with the purpose of achieving more accuracy than each member of it. Then, all the models will be tested in a test set in order to determine the best one.\n","864482ca":"# **4.1 Logistic Regression** <a class=\"anchor\" id=\"4.1\"><\/a>","5f2b15b5":"## **Race**","576f86ed":"## **fnlwgt**","a75cedaa":"Probabilities are what we would expect.","2b7408e0":"# **4.8 XGBoost** <a class=\"anchor\" id=\"4.8\"><\/a>","dc362ae8":"## **Initial Set-Up**","cfbf015f":"We can see that we have both continous and categorical data. Now we are going to study each of the 15 variables in the dataset.","8f508dd4":"## **capital.loss**","b6286a1e":"As we see male have higher probabilities than female.","353c9ec4":"# **1.2 Categorical variables** <a class=\"anchor\" id=\"1.2\"><\/a>","4b1f63b5":"Now we can get the dummies from the categorical variables and concatenate both continous and categorical datasets.","fd91e0c7":"## **Sex**","32c9142b":"## **Occupation**","72dae034":"There are not null values.","d9b10235":"As we see missing data from native.country as nothing to do with missing data from occupation and workclass.  ","ad7b8807":"People with more than 50k are on average older.","2fafb4b3":"## **Relationship**","a182010d":"# **Table of Contents**\n\n- [1. Descriptive analysis](#1)\n    - [1.1 Target variable](#1.1)\n    - [1.2 Categorical variables](#1.2)\n    - [1.3 Continous variables](#1.3)\n        - [1.3.1 Correlation matrix](#1.3.1)\n- [2. Cleaning data](#2)\n    - [2.1 Drop useless variables](#2.1)\n    - [2.2 Deal with missing data](#2.2)\n- [3. Split data and get dummies](#3)\n- [4. Proposed models](#4)\n    - [4.1 Logistic Regression](#4.1)\n    - [4.2 Categorical Na\u00efve Bayes](#4.2)\n    - [4.3 Gaussian Na\u00efve Bayes](#4.3)\n    - [4.4 K-Nearest Neighbors](#4.4)\n    - [4.5 Support Vector Machines](#4.5)\n    - [4.6 Decision Trees](#4.6)\n    - [4.7 Random Forest](#4.7)\n    - [4.8 XGBoost](#4.8)\n    - [4.9 Artificial Neural Networks](#4.9)\n    - [4.10 Ensembling](#4.10)\n","9ab326a9":"## **Age**","15e88ada":"# **4.2 Categorical Na\u00efve Bayes** <a class=\"anchor\" id=\"4.2\"><\/a>","66928be6":"## **2.2 Deal with missing data** <a class=\"anchor\" id=\"2.2\"><\/a>\nAs we have a lot of data and the missing values is just a small part of the dataset, we drop rows with missing values.","769eab9c":"## **hours.per.week**","74971462":"Results match with what we would expect.","abff2563":"# **1.3 Continous variables** <a class=\"anchor\" id=\"1.3\"><\/a>","63082ef1":"# **1. Descriptive analysis** <a class=\"anchor\" id=\"1\"><\/a>\n\nIn this section we will have a first look at the data and try to understand each variable in the dataset. First of all we read the data:","1cfbc628":"# **4.7 Random Forest** <a class=\"anchor\" id=\"4.7\"><\/a>","336e7347":" ## **1.3.1 Correlation matrix** <a class=\"anchor\" id=\"1.3.1\"><\/a>","1b31c063":"Probabilities are what we would expect. Married people has more probability than the rest.","62d3cd8e":"There are 1843 missing values. It must be correlated with workclass","d7262a89":"# **5. Conclusion**\n\nWe can represent the different accuracy results:\n\n","43f0b2d2":"## **2.1 Drop useless variables** <a class=\"anchor\" id=\"2.1\"><\/a>\n\nWe have seen that fnlwgt variable has a really small correlation with the target variable so we can drop it. We can also drop education.num as if we don't do that it will be a multicollinearity problem with education.\n","b5753f46":"As we see XGBoost outperforms the rest of the models. It it worth mentioning that the validation score for the ANN is for just one ANN, so when doing the Neural Networks Ensemble we expect it to be higher. Thus, we can do the ensembling with XGBoost and ANN. As XGBoost seems to perform better, we will give it a higher weight.","34723706":"It doesn't have null values.","c08be3ad":"There are not null values.","b2c6a3c3":"As wee see there are 1836 missing values.","f2c3704e":"# **1.1 Target variable** <a class=\"anchor\" id=\"1.1\"><\/a>","25eeb49a":"## **Education**","0e553a8b":"# **4.4 K-Nearest Neighbors** <a class=\"anchor\" id=\"4.4\"><\/a>","4e43209e":"As we have seen, the XGBoost-ANN ensembling model has the best accuracy, but it is not remarkably better than the single XGBoost model. We can conclude that XGBoost model outperforms all the other models. ","503f9053":"# **3. Split data and get dummies** <a class=\"anchor\" id=\"3\"><\/a>\nFirst we will split the data into dependent and independent variables, and then we will split the dependent variables into continous variables and categorical variables.","0be99678":"As we see fnlwgt does not have a high correlation with income so we will drop it.","ddbc0dea":"# **4.10 Ensembling** <a class=\"anchor\" id=\"4.10\"><\/a>","6848e3e4":"# **4.6 Decision Trees** <a class=\"anchor\" id=\"4.6\"><\/a>","4def41b2":"As we see being married increases the probability of earning more than 50k.","3ccc2359":"We can see that we have an imbalanced target variable, so we will use stratification when splitting data into train and test sets and when doing cross-validation.","bc3ab2d5":"We substitute the values for ones and zeros and count the number of samples for each value.","742ec2af":"## **education.num**\nThis is an ordinal variable for education.","ba591f20":"# **4. Proposed models** <a class=\"anchor\" id=\"4\"><\/a>\n\nIn this section we will develop the predictive models. We will stratify the data and use a specific random state so all the models have the same target values.\nIt is worth mentioning that in most of the models (the complex ones) I have done hyperparameter optimization with GridSearchCV. As this process takes a lot of time, I have commented the lines where I fit the GridSearch and I comment the results that I had when running it. ","edff1416":"As we see there are 1836 null values in common.","1b325036":"## **native.country**","158f4e3b":"## **capital.gain**","6b5ee50b":"Probabilities are what we would expect.","2f0133bd":"# **4.9 Artifical Neural Networks** <a class=\"anchor\" id=\"4.9\"><\/a>"}}