{"cell_type":{"9891824e":"code","0620ef25":"code","869b6576":"code","433ae9cc":"code","f3d78e79":"code","b8082a36":"code","44bdb528":"code","c48293bd":"code","2d9ec224":"code","2f99d858":"code","27a999b9":"code","3753adf8":"code","53730d69":"code","bfaadbc0":"code","9a5f80e9":"code","853c7757":"code","c87b6fb3":"code","bf0aae36":"code","d39abddf":"code","54bf70a0":"code","948ce635":"code","59f8b2cf":"code","a1538c32":"code","bc6301dc":"code","083d6a8f":"code","8246b2f9":"code","d258302e":"code","1bc20da6":"code","a6d3dc12":"code","510e90a0":"code","8a4e596f":"code","3a044e6a":"code","de63dc8a":"code","4265e794":"code","a35d489f":"code","5b38676d":"code","40f3ae1e":"code","e4e1785e":"code","82c6fd86":"code","b14ba627":"code","99f3b981":"code","0d92398e":"code","6fec619e":"code","6b3c9756":"code","8fd67e2d":"code","738af612":"code","156edaa5":"code","839bc753":"code","ee2f006d":"code","8ada21ff":"code","ec4bc085":"code","5dddf8d1":"code","4ee3a8e7":"code","5ab0a27f":"code","703ec2a1":"code","93b95653":"code","d2e46101":"code","44298798":"code","c22396c5":"code","f0f4fbbf":"code","1bb2c196":"code","f2d97ef4":"code","04d11dd2":"code","0ed904e2":"code","eb782482":"code","be950ac7":"code","c043cbe8":"code","5b7d4e16":"code","9cb7b560":"code","729f8c19":"code","c2847206":"code","e3a5d734":"code","5915108f":"code","234396f5":"code","65cb0863":"code","00d41ca2":"code","d6b7a894":"code","1aaaee54":"code","adf17cff":"code","4c5451d6":"code","7ba8da78":"code","2797cbc3":"code","4a63ad6f":"code","3996d2a1":"code","8bc35de6":"code","c3f2337e":"code","05549e51":"code","383f364f":"markdown","89c26942":"markdown","df9f1e0d":"markdown","5a2418f9":"markdown","c37466e5":"markdown","07c4d503":"markdown","b032a313":"markdown","3960ee90":"markdown","a68c606c":"markdown","733a3aa9":"markdown","7a085529":"markdown","0f50cbf0":"markdown","9d4fc79b":"markdown","c016cfb6":"markdown","de34df34":"markdown","9d603a40":"markdown","162499cf":"markdown","7320571f":"markdown","d91a7988":"markdown","5dbfc3aa":"markdown","2ef86060":"markdown","8b8c4d02":"markdown","3e0fee70":"markdown","94c35c65":"markdown"},"source":{"9891824e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0620ef25":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.pipeline import make_pipeline\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor #Decision tree,regression model\nfrom sklearn.model_selection import cross_val_score #import cross validation\u2423,\u2192score package\nfrom sklearn.model_selection import GridSearchCV #import grid search cv\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error,mean_squared_log_error\n\nfrom sklearn.svm import SVR\n\n\nimport numpy as np\nimport pandas as pd\nimport datetime\n\nimport random\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfrom sklearn.kernel_ridge import KernelRidge\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet \nfrom sklearn.preprocessing import RobustScaler\n\nimport lightgbm as lgb\n","869b6576":"\ntrain=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\ntrain=train.dropna(axis=1)\ntrain.head()\ntrain.drop('Id',axis=1).corr()\nfrom seaborn import heatmap\nheatmap(train.drop('Id',axis=1).corr())\n\n\n\n\n","433ae9cc":"# path to file you will use for predictions\ntest_data_path = '..\/input\/house-prices-advanced-regression-techniques\/test.csv'\n# read test data file using pandas\ntest_data = pd.read_csv(test_data_path)\ntest_data\n","f3d78e79":"set(test_data.columns) - set(train.columns)","b8082a36":"set(train.columns)-set(test_data.columns)","44bdb528":"for col in set(test_data.columns)-set(train.columns):\n    test_data.drop(col,axis=1,inplace=True)","c48293bd":"train.head()","2d9ec224":"set(test_data.columns)-set(train.columns)","2f99d858":"res=train\nif True:\n    res['Street'] = res['Street'].fillna('missing')\n    res['LotShape'] = res['LotShape'].fillna('missing')\n    res['LandContour'] = res['LandContour'].fillna('missing')\n    res['CentralAir'] = res['CentralAir'].fillna('missing')\n    res['MSZoning'] = res['MSZoning'].fillna(res['MSZoning'].mode()[0])    \n    res['Utilities'] = res['Utilities'].fillna('missing')\n    res['Exterior1st'] = res['Exterior1st'].fillna(res['Exterior1st'].mode()[0])\n    res['Exterior2nd'] = res['Exterior2nd'].fillna(res['Exterior2nd'].mode()[0])    \n    res['KitchenQual'] = res['KitchenQual'].fillna(res['KitchenQual'].mode()[0])\n    res[\"Functional\"] = res[\"Functional\"].fillna(\"Typ\")\n    res['SaleType'] = res['SaleType'].fillna(res['SaleType'].mode()[0])\n    res['SaleCondition'] = res['SaleCondition'].fillna('missing')\n    \n\n\n\nres['TotalBsmtSF'] = res['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nres['2ndFlrSF'] = res['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\nres['GarageArea'] = res['GarageArea'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nres['GarageCars'] = res['GarageCars'].apply(lambda x: 0 if x <= 0.0 else x)\nres['BsmtFinSF1'] = res['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\ntrain=res","27a999b9":"res=test_data\nif True:\n    res['Street'] = res['Street'].fillna('missing')\n    res['LotShape'] = res['LotShape'].fillna('missing')\n    res['LandContour'] = res['LandContour'].fillna('missing')\n    res['CentralAir'] = res['CentralAir'].fillna('missing')\n    res['MSZoning'] = res['MSZoning'].fillna(res['MSZoning'].mode()[0])    \n    res['Utilities'] = res['Utilities'].fillna('missing')\n    res['Exterior1st'] = res['Exterior1st'].fillna(res['Exterior1st'].mode()[0])\n    res['Exterior2nd'] = res['Exterior2nd'].fillna(res['Exterior2nd'].mode()[0])    \n    res['KitchenQual'] = res['KitchenQual'].fillna(res['KitchenQual'].mode()[0])\n    res[\"Functional\"] = res[\"Functional\"].fillna(\"Typ\")\n    res['SaleType'] = res['SaleType'].fillna(res['SaleType'].mode()[0])\n    res['SaleCondition'] = res['SaleCondition'].fillna('missing')\n    \nres['TotalBsmtSF'] = res['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nres['2ndFlrSF'] = res['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\nres['GarageArea'] = res['GarageArea'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nres['GarageCars'] = res['GarageCars'].apply(lambda x: 0 if x <= 0.0 else x)\nres['BsmtFinSF1'] = res['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\ntest_data=res","3753adf8":"def addlogs(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n        res.columns.values[m] = l + '_log'\n        m += 1\n    return res\n","53730d69":"\nloglist = ['LotArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd']\n\ntest_data = addlogs(test_data, loglist)","bfaadbc0":"\nloglist = ['LotArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd']\ntrain = addlogs(train, loglist)","9a5f80e9":"train.drop(['Utilities', 'Street'], axis=1,inplace=True)\n\ntrain.drop(['YearRemodAdd', 'GarageArea', 'TotalBsmtSF', 'TotRmsAbvGrd', 'BsmtFinSF1'],axis=1,inplace=True)\n\n\ntest_data.drop(['Utilities', 'Street'], axis=1,inplace=True)\n\ntest_data.drop(['YearRemodAdd', 'GarageArea', 'TotalBsmtSF', 'TotRmsAbvGrd', 'BsmtFinSF1'],axis=1,inplace=True)","853c7757":"set(test_data.columns)-set(train.columns)","c87b6fb3":"set(train.columns)-set(test_data.columns)","bf0aae36":" train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n    #MSSubClass=The building class\ntrain['MSSubClass'] = train['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\ntrain['OverallCond'] = train['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\ntrain['YrSold'] = train['YrSold'].astype(str)\ntrain['MoSold'] = train['MoSold'].astype(str)","d39abddf":"test_data['MSSubClass'] = test_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\ntest_data['OverallCond'] = test_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\ntest_data['YrSold'] = test_data['YrSold'].astype(str)\ntest_data['MoSold'] = test_data['MoSold'].astype(str)","54bf70a0":"set(test_data.columns)-set(train.columns)","948ce635":"cats=train.dtypes[train.dtypes==object].index\ncats_test=test_data.dtypes[test_data.dtypes==object].index","59f8b2cf":"set(cats)-set(cats_test)","a1538c32":"set(cats_test)-set(cats)","bc6301dc":"def one_hot(df, cols):\n    \"\"\"\n    @param df pandas DataFrame\n    @param cols a list of columns to encode \n    @return a DataFrame with one-hot encoding\n    \"\"\"\n    for each in cols:\n        dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)\n        df = pd.concat([df, dummies], axis=1)\n    return df","083d6a8f":"train=one_hot(train,cats)\ntrain.head()","8246b2f9":"train.drop(cats,inplace=True,axis=1)\n","d258302e":"test_data=one_hot(test_data,cats)\ntest_data.head()","1bc20da6":"test_data.drop(cats,inplace=True,axis=1)","a6d3dc12":"train.head()","510e90a0":"set(train.columns)-set(test_data.columns)","8a4e596f":"set(test_data.columns)-set(train.columns)","3a044e6a":"test_data.MSSubClass_160=test_data.apply(lambda x: 1 if (bool(x.MSSubClass_150==1) or bool(x.MSSubClass_160==1)) else 0,axis=1)","de63dc8a":"test_data[['MSSubClass_150','MSSubClass_160']].head(-100)","4265e794":"test_data.drop('MSSubClass_150',inplace=True,axis=1)","a35d489f":"set(train.columns) - set(test_data.columns)","5b38676d":"for col in (set(train.columns) - set(test_data.columns)):\n    print(train[col].unique(),col,train[col].mode(),train[col].mean())","40f3ae1e":"for col in (set(train.columns) - set(test_data.columns)):\n    print(col)","e4e1785e":"cols=(set(train.columns) - set(test_data.columns) )- set(['SalePrice'])\ncols","82c6fd86":"for col in cols:\n    test_data[col]=test_data.MiscVal.apply(lambda x:np.uint8(0))\n    test_data[col]=test_data[col].astype('uint8',copy=True)\n    print(test_data[col].unique(),test_data[col].dtype,train[col].dtype)","b14ba627":"test_data.SalePrice","99f3b981":"test_data.Heating_Floor.unique()","0d92398e":"(set(train.columns) - set(test_data.columns) )","6fec619e":"(set(test_data.columns) - set(train.columns) )","6b3c9756":"## ","8fd67e2d":"X,y=train.drop(['Id','SalePrice'],axis=1),train.SalePrice\n#y = np.log(1+np.array(mydata['SalePrice']))\n#train_X,train_y,test_x,test_y=train_test_split(train.drop(['Id','SalePrice'],axis=1),train.SalePrice)\n#model=RandomForestRegressor()\n#model.fit(train_X,train_y)\nset(test_data.columns)-set(X.columns)\n","738af612":"X.head()","156edaa5":"print(X.isnull().any().sum())","839bc753":"X_train,X_val,y_train,y_val = train_test_split(X,y)\ny_train.min()","ee2f006d":"\nlr = LinearRegression()\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_val)\nlr_rmse_score = np.sqrt(mean_squared_error(y_pred,y_val))\nlr_r2_score = r2_score(y_pred,y_val)\nprint(\"Root Mean Squared Error :\",lr_rmse_score)\nprint(\"R2Score :\",lr_r2_score)\nprint(y_pred.min(),y_val.min())\nprint('mean_squared_log_error',mean_squared_log_error(abs(y_pred),y_val))\n\n\n\npredicted=lr.predict(test_data)\nprint(mean_squared_log_error(abs(predicted),submission.SalePrice))\n","8ada21ff":"for col in test_data.columns:\n    if(test_data[col].isnull().sum()!=0):\n        a=test_data[col].mode()[0]\n        print(test_data[col].isna().sum(),col,test_data[col].dtype,test_data[col].mode()[0],test_data[col].mean())\n        print(a)\n        test_data[col].fillna(value=a,inplace=True)\n        print(test_data[col].isna().sum(),col,test_data[col].dtype,test_data[col].mode()[0],test_data[col].mean())","ec4bc085":" dt = DecisionTreeRegressor()\ndt_model=dt.fit(X_train,y_train)\ny_pred_dtone=dt_model.predict(X_val)\n# calculate RMSE\nrms_dt = np.sqrt(mean_squared_error(y_pred_dtone,y_val))\nr2_dt = r2_score(y_val, y_pred_dtone)\nprint('RMSE of Decision Tree Regression:',rms_dt)\nprint('R-Squared value:',r2_dt)\nR2 = r2_score(y_val, y_pred)\nn = X_train.shape[0]\np = len(X_train.columns)\nAdj_r2 = 1-(1-R2)*(n-1)\/(n-p-1)\nprint('Adjusted R-Square is : ',Adj_r2)\nprint('mean_squared_log_error',mean_squared_log_error(y_val,y_pred_dtone))","5dddf8d1":"test_data_1=test_data.drop('Id',inplace=False,axis=1)\npredicted=dt_model.predict(test_data_1)","4ee3a8e7":"submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\ntest=test_data\ntrain1=train\n\n\n# drop missing values\nmissing = test.isnull().sum()\nmissing = missing[missing>0]\ntrain1.drop(missing.index, axis=1, inplace=True)\n#train1.drop(['Electrical'], axis=1, inplace=True)\n\ntest.dropna(axis=1, inplace=True)\n#test.drop(['Electrical'], axis=1, inplace=True)\n\nl_test = tqdm(range(0, len(test)), desc='Matching')\n\nfor i in l_test:\n    for j in range(0, len(train1)):\n        for k in range(1, len(test.columns)):\n            if test.iloc[i,k] == train1.iloc[j,k]:\n                continue\n            else:\n                break\n        else:\n            submission.iloc[i, 1] = train1.iloc[j, -1]\n            break\nl_test.close()\nsubmission","5ab0a27f":"submission","703ec2a1":"#predicted=(np.exp(predicted)-1)\nmean_squared_log_error(predicted,submission.SalePrice)","93b95653":"set(test_data.columns) - set(X_train.columns)","d2e46101":"set(X_train.columns) - set(test_data.columns)","44298798":"\n#{'colsample_bytree': 0.6, \n#'eval_metric': 'rmsle', 'gamma': 0.5, \n#'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 1100, 'nthread': 3, \n#'objective': 'reg:linear', 'subsample': 0.8}\nparameters={'colsample_bytree': 0.3,\n 'eval_metric': 'rmsle',\n 'gamma': 0.5, \n 'learning_rate': 0.2, \n 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 1100, 'nthread': 3, 'objective': 'reg:linear', 'subsample': 0.8}\n\n\nxgb_grid = xgb.XGBRegressor(\n            colsample_bytree=0.6, \n            eval_metric= 'rmsle', \n            gamma= 0.5, \n            learning_rate= 0.05,\n            max_depth= 3,\n            min_child_weight= 1,\n            n_estimators= 1100, nthread= 3, \n            objective= 'reg:linear',\n                             \n            reg_alpha= 1e-05, seed= 45, \n                            subsample= 0.8)\n#{'colsample_bytree': 0.4, 'eval_metric': 'rmsle', 'gamma': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 900, 'nthread': 3, 'objective': 'reg:squarederror', 'subsample': 0.6}\nxgb_grid.fit(X, y)\n","c22396c5":"\n\ny_pred_xgb=xgb_grid.predict(X_val)\n# calculate RMSE\nrms_dt = np.sqrt(mean_squared_error(y_pred_xgb,y_val))\nr2_dt = r2_score(y_val, y_pred_xgb)\nprint('RMSE of Decision Tree Regression:',rms_dt)\nprint('R-Squared value:',r2_dt)\nR2 = r2_score(y_val, y_pred_xgb)\nn = X_train.shape[0]\np = len(X_train.columns)\nAdj_r2 = 1-(1-R2)*(n-1)\/(n-p-1)\nprint('Adjusted R-Square is : ',Adj_r2)\nprint('mean_squared_log_error',mean_squared_log_error(y_val,y_pred_dtone))","f0f4fbbf":"test_data_1=test_data.drop('Id',inplace=False,axis=1)\n\ntest_data_1 = test_data_1[X.columns]\n\n\n#x = [features_test[i]]\n#if x[0][-1] == 0:\n #        x[0][-1] = 0.0000001\n#pred = int(xgb_regressor.predict(x))\n\npredicted=xgb_grid.predict(test_data_1)","1bb2c196":"#predicted=(np.exp(predicted)-1)\nmean_squared_log_error(predicted,submission.SalePrice)\n","f2d97ef4":"set(test_data_1.columns) - set(X_train.columns)","04d11dd2":"set(X_train.columns)-set(test_data_1.columns)","0ed904e2":"# Importing libraries\nrf_reg = RandomForestRegressor(n_estimators=1100,max_leaf_nodes=990,max_depth=1,min_samples_leaf=5,min_samples_split=3)\nrf_model = rf_reg.fit(X,y)\ny_pred_rf = rf_model.predict(X_val)\nrmse_rf = np.sqrt(mean_squared_error(y_pred_rf,y_val))\nr2_rf = r2_score(y_pred_rf,y_val)\nprint('RMSE of predicted in RF model:',rmse_rf)\nprint('R Sqaured in RF model :',r2_rf)\nprint('mean_squared_log_error',mean_squared_log_error(y_val,y_pred_rf))","eb782482":"test_data_1=test_data.drop('Id',inplace=False,axis=1)\n\ntest_data_1 = test_data_1[X.columns]\n\n\n#x = [features_test[i]]\n#if x[0][-1] == 0:\n #        x[0][-1] = 0.0000001\n#pred = int(xgb_regressor.predict(x))\n\npredicted=rf_model.predict(test_data_1)","be950ac7":"#predicted=(np.exp(predicted)-1)\nmean_squared_log_error(predicted,submission.SalePrice)\n","c043cbe8":"from mlxtend.regressor import StackingCVRegressor\n\nstack_gen = StackingCVRegressor(regressors=(xgb_grid,rf_model),\n                                meta_regressor=rf_model,\n                                use_features_in_secondary=True)\nstack_gen.fit(np.array(X), np.array(y))\n\n","5b7d4e16":"predicted=stack_gen.predict(np.array(test_data_1))","9cb7b560":"#predicted=(np.exp(predicted)-1)\nmean_squared_log_error(predicted,submission.SalePrice)","729f8c19":"lasso_model=Lasso(alpha = 0.0003, random_state=1, max_iter=50000)","c2847206":"lasso_model.fit(X,y)","e3a5d734":"y_pred_rf = lasso_model.predict(X_val)\nrmse_rf = np.sqrt(mean_squared_error(y_pred_rf,y_val))\nr2_rf = r2_score(y_pred_rf,y_val)\nprint('RMSE of predicted in RF model:',rmse_rf)\nprint('R Sqaured in RF model :',r2_rf)\nprint('mean_squared_log_error',mean_squared_log_error(y_val,y_pred_rf))","5915108f":"predicted=lasso_model.predict(test_data_1)","234396f5":"mean_squared_log_error(abs(predicted),abs(submission.SalePrice))","65cb0863":"myGBR = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.02,\n                                      max_depth=4, max_features='sqrt',\n                                      min_samples_leaf=15, min_samples_split=50,\n                                      loss='huber', random_state = 5) ","00d41ca2":"myGBR.fit(X,y)","d6b7a894":"y_pred_rf = myGBR.predict(X_val)\nrmse_rf = np.sqrt(mean_squared_error(y_pred_rf,y_val))\nr2_rf = r2_score(y_pred_rf,y_val)\nprint('RMSE of predicted in RF model:',rmse_rf)\nprint('R Sqaured in RF model :',r2_rf)\nprint('mean_squared_log_error',mean_squared_log_error(y_val,y_pred_rf))","1aaaee54":"predicted=myGBR.predict(test_data_1)","adf17cff":"mean_squared_log_error(predicted,submission.SalePrice)","4c5451d6":"myLGB = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=600,\n                              max_bin = 50, bagging_fraction = 0.6,\n                              bagging_freq = 5, feature_fraction = 0.25,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf = 6, min_sum_hessian_in_leaf = 11)\nmyLGB.fit(X,y)\n\ny_pred_rf = myLGB.predict(X_val)\nrmse_rf = np.sqrt(mean_squared_error(y_pred_rf,y_val))\nr2_rf = r2_score(y_pred_rf,y_val)\nprint('RMSE of predicted in RF model:',rmse_rf)\nprint('R Sqaured in RF model :',r2_rf)\nprint('mean_squared_log_error',mean_squared_log_error(y_val,y_pred_rf))\npredicted=myLGB.predict(test_data_1)\nprint(mean_squared_log_error(predicted,submission.SalePrice))","7ba8da78":" ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=4.0, l1_ratio=0.005, random_state=3))\nENet.fit(X,y)\n\ny_pred_rf = ENet.predict(X_val)\nrmse_rf = np.sqrt(mean_squared_error(y_pred_rf,y_val))\nr2_rf = r2_score(y_pred_rf,y_val)\nprint('RMSE of predicted in RF model:',rmse_rf)\nprint('R Sqaured in RF model :',r2_rf)\nprint('mean_squared_log_error',mean_squared_log_error(y_val,y_pred_rf))\npredicted=ENet.predict(test_data_1)\nprint(mean_squared_log_error(predicted,submission.SalePrice))","2797cbc3":"l2Regr = Ridge(alpha=9.0, fit_intercept = True)\n\n\nl2Regr.fit(X,y)\n\ny_pred_rf = l2Regr.predict(X_val)\nrmse_rf = np.sqrt(mean_squared_error(y_pred_rf,y_val))\nr2_rf = r2_score(y_pred_rf,y_val)\nprint('RMSE of predicted in RF model:',rmse_rf)\nprint('R Sqaured in RF model :',r2_rf)\nprint('mean_squared_log_error',mean_squared_log_error(y_val,y_pred_rf))\npredicted=l2Regr.predict(test_data_1)\nprint(mean_squared_log_error(predicted,submission.SalePrice))","4a63ad6f":"\nstack_gen = StackingCVRegressor(regressors=(ENet,rf_model),\n                                meta_regressor=ENet,\n                                use_features_in_secondary=True)\nstack_gen.fit(np.array(X), np.array(y))\npredicted=stack_gen.predict(test_data_1)\nprint(mean_squared_log_error(predicted,submission.SalePrice))\n","3996d2a1":"Id = test_data['Id']\nfin_score = pd.DataFrame({'SalePrice': ENet.predict(test_data_1)})\nfin_data = pd.concat([Id,fin_score],axis=1)","8bc35de6":"mean_squared_log_error(submission.SalePrice,fin_data.SalePrice)","c3f2337e":"fin_data.to_csv('House_Prices_submit.csv', sep=',', index = False)","05549e51":"from sklearn.model_selection import GridSearchCV\nimport multiprocessing\nn_jobs = multiprocessing.cpu_count()-1\n\n# Applying hyper parameter\nrf_params = {'n_estimators':range(100,150,10),'max_depth':range(30,60,10),'max_leaf_nodes':range(150,200,20)}\nrf_grid = GridSearchCV(rf_reg,rf_params,cv=10,verbose=10,n_jobs=4)\nrf_model_two = rf_grid.fit(X_train,y_train)\ny_pred_rf_two = rf_model_two.predict(X_val)\nrmse_rf_2 = np.sqrt(mean_squared_error(y_val,y_pred_rf_two))\nr2_rf_2 = r2_score(y_pred_rf_two,y_val)\nprint('RMSE using RF grid search method:',rmse_rf_2)\nprint('R Sqaured in RF model :',r2_rf_2)\nprint('mean_squared_log_error',mean_squared_log_error(y_pred_rf_two,y_val))","383f364f":"## Drop useless predictors","89c26942":"## Use ENET Finally gives around 0.04 rmse score here","df9f1e0d":"## XGBoost with leak data to check score","5a2418f9":"## Ridge with leak data to check score","c37466e5":"## ENET with leak data to check score","07c4d503":"## Import test and train data\n","b032a313":"## Linear regression with leak data to check score","3960ee90":"## gradienttboosting regression with leak data to check score","a68c606c":"## LGB with leak data to check score","733a3aa9":"Inspired fromm Aleksandrs Gehsbargs and Shaitender Singh and many others","7a085529":"## Imports","0f50cbf0":"## Random forest with parameter tuning with leak data to check score","9d4fc79b":"## Decision tree with leak data to check score","c016cfb6":"## lasso with leak data to check score","de34df34":"## This is some rough book from hyperparameter tuning and trying to check accuracy from leaked data","9d603a40":"## Some column in test are not in train :-( remove those","162499cf":"## Drop outliers and handle categorical data","7320571f":"## MSSubclass does not have value 150 in train data so to handle it we encoded it as 160, the closest available value in train data","d91a7988":"## Stack enet and rf","5dbfc3aa":"## Take logs of some of predictors (RESULTED IN LOW ACCURACY SO DROPPED THIS)","2ef86060":"## Fill missing data with 0 \/ constant \/ missing string","8b8c4d02":"## Train test split","3e0fee70":"## Stacking with leak data to check score","94c35c65":"## Fill the categories that did not have certain value in test data"}}