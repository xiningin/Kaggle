{"cell_type":{"70b2bcab":"code","f7fe7558":"code","dacdef2f":"code","c6144c6d":"code","a2985f70":"code","c67bd314":"code","feb663b5":"code","1bba42bf":"code","ace7fece":"code","f004f0c3":"code","36ffe643":"code","244735b0":"code","43568493":"code","763e74a7":"code","456b465e":"code","8cca9b7f":"code","da53b51e":"code","de37533f":"code","a0172295":"code","50c386c7":"code","6907992e":"code","e40e5dc9":"code","7f273afe":"code","3633eaa9":"code","e1cd578e":"code","6d52507b":"code","5b6aa7b9":"code","157c6912":"code","3d59458d":"code","a92dd082":"code","6e039be1":"code","9387bf3b":"code","97a2b10e":"code","bc7e32e7":"code","42bf8e68":"code","efa42e99":"code","92ea2091":"code","d2671ddb":"code","80edb501":"code","3cc10f14":"code","f51a6d7c":"code","e6f77503":"code","d589baad":"code","9496f829":"code","64516d46":"code","9ca7ce58":"code","23c72fe1":"markdown","d17a7128":"markdown","91611c8e":"markdown","d646ca67":"markdown","6151a921":"markdown","ecaa432d":"markdown","ca580813":"markdown","5b0d225d":"markdown","8cfe907a":"markdown","a4798ab1":"markdown","d8e7f121":"markdown","e20c0fe6":"markdown","8cd82a4c":"markdown","62bd94e7":"markdown","1d49002b":"markdown","2e1d7118":"markdown","bff0eb81":"markdown","41dddee5":"markdown","63c721fd":"markdown","1eeec60e":"markdown","6bb5485f":"markdown","25b855fd":"markdown","2411ce9d":"markdown","fcedf2c4":"markdown","caf57b69":"markdown","368ea2b3":"markdown","27a95e13":"markdown","955b2b3f":"markdown","ddfe4f23":"markdown","06afd2ab":"markdown","68a379c7":"markdown","1971a60c":"markdown","a49574e0":"markdown"},"source":{"70b2bcab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f7fe7558":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","dacdef2f":"raw_data = pd.read_csv(\"..\/input\/OLX_Car_Data_CSV.csv\", encoding = 'unicode_escape')\nraw_data.head()","c6144c6d":"raw_data.describe(include='all')","a2985f70":"raw_data.isnull().sum()","c67bd314":"## We won't need model. so let's drop it\nraw_data.drop(['Model'], inplace=True, axis=1)\nraw_data.head()","feb663b5":"data_no_mv = raw_data.dropna(axis=0)\ndata_no_mv.describe(include='all')","1bba42bf":"import seaborn as sns\nsns.set()\nsns.distplot(data_no_mv['Price'])","ace7fece":"q = data_no_mv[\"Price\"].quantile(0.99)\ndata1 = data_no_mv[data_no_mv['Price']<q]\ndata1.describe(include='all')","f004f0c3":"sns.distplot(data1['Price'])","36ffe643":"sns.distplot(data1['KMs Driven'])","244735b0":"q = data_no_mv[\"KMs Driven\"].quantile(0.96)\ndata2 = data1[(1<data1['KMs Driven']) & (data1['KMs Driven']<q)]\ndata2.describe(include='all')","43568493":"sns.distplot(data1['KMs Driven'])","763e74a7":"sns.distplot(data1['Year'])","456b465e":"q = data_no_mv[\"Year\"].quantile(0.01)\ndata3 = data2[q<data_no_mv['Year']]\ndata3.describe(include='all')","8cca9b7f":"sns.distplot(data1['Year'])","da53b51e":"data_cleaned = data3.reset_index(drop=True)","de37533f":"data_cleaned.describe(include='all')","a0172295":"f, (ax1, ax2) = plt.subplots(1,2, sharey=True, figsize=(15,3))\nax1.scatter(data_cleaned['Year'], data_cleaned['Price'])\nax1.set_title('Price vs Year')\nax2.scatter(data_cleaned['KMs Driven'], data_cleaned['Price'])\nax2.set_title('Price vs KMs Driven')\n","50c386c7":"sns.distplot(data_cleaned['Price'])","6907992e":"log_price = np.log(data_cleaned['Price'])\ndata_cleaned['log_price'] = log_price","e40e5dc9":"f, (ax1, ax2) = plt.subplots(1,2, sharey=True, figsize=(15,3))\nax1.scatter(data_cleaned['Year'], data_cleaned['log_price'])\nax1.set_title('log_price vs Year')\nax2.scatter(data_cleaned['KMs Driven'], data_cleaned['log_price'])\nax2.set_title('log_price vs KMs Driven')","7f273afe":"data_cleaned.drop(['Price'], axis=1, inplace=True)\ndata_cleaned.head()","3633eaa9":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvariables = data_cleaned[['KMs Driven', 'Year']]\nvif = pd.DataFrame()\nvif['VIF'] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\nvif['features'] = variables.columns","e1cd578e":"vif","6d52507b":"sns.scatterplot(data_cleaned['Year'], data_cleaned['KMs Driven'])","5b6aa7b9":"data_cleaned.columns.values","157c6912":"data_with_dummies = pd.get_dummies(data_cleaned, drop_first=True)","3d59458d":"data_with_dummies.head()","a92dd082":"data_with_dummies.columns.values","6e039be1":"# Just moving log_price to front\ncols = ['log_price', 'KMs Driven', 'Year', 'Brand_BMW', 'Brand_Changan',\n       'Brand_Chevrolet', 'Brand_Classic & Antiques', 'Brand_Daewoo',\n       'Brand_Daihatsu', 'Brand_FAW', 'Brand_Honda', 'Brand_Hyundai',\n       'Brand_KIA', 'Brand_Land Rover', 'Brand_Lexus', 'Brand_Mazda',\n       'Brand_Mercedes', 'Brand_Mitsubishi', 'Brand_Nissan',\n       'Brand_Other Brands', 'Brand_Porsche', 'Brand_Range Rover',\n       'Brand_Subaru', 'Brand_Suzuki', 'Brand_Toyota', 'Condition_Used',\n       'Fuel_Diesel', 'Fuel_Hybrid', 'Fuel_LPG', 'Fuel_Petrol',\n       'Registered City_Ali Masjid', 'Registered City_Askoley',\n       'Registered City_Attock', 'Registered City_Badin',\n       'Registered City_Bagh', 'Registered City_Bahawalnagar',\n       'Registered City_Bahawalpur', 'Registered City_Bela',\n       'Registered City_Bhimber', 'Registered City_Chilas',\n       'Registered City_Chiniot', 'Registered City_Chitral',\n       'Registered City_Dera Ghazi Khan',\n       'Registered City_Dera Ismail Khan', 'Registered City_Faisalabad',\n       'Registered City_Gujranwala', 'Registered City_Gujrat',\n       'Registered City_Haripur', 'Registered City_Hyderabad',\n       'Registered City_Islamabad', 'Registered City_Jhelum',\n       'Registered City_Kandhura', 'Registered City_Karachi',\n       'Registered City_Karak', 'Registered City_Kasur',\n       'Registered City_Khairpur', 'Registered City_Khanewal',\n       'Registered City_Khanpur', 'Registered City_Khaplu',\n       'Registered City_Khushab', 'Registered City_Kohat',\n       'Registered City_Lahore', 'Registered City_Larkana',\n       'Registered City_Lasbela', 'Registered City_Mandi Bahauddin',\n       'Registered City_Mardan', 'Registered City_Mirpur',\n       'Registered City_Multan', 'Registered City_Muzaffargarh',\n       'Registered City_Nawabshah', 'Registered City_Nowshera',\n       'Registered City_Okara', 'Registered City_Pakpattan',\n       'Registered City_Peshawar', 'Registered City_Quetta',\n       'Registered City_Rahimyar Khan', 'Registered City_Rawalpindi',\n       'Registered City_Sahiwal', 'Registered City_Sargodha',\n       'Registered City_Sheikh\u00fcpura', 'Registered City_Sialkot',\n       'Registered City_Sukkar', 'Registered City_Sukkur',\n       'Registered City_Tank', 'Registered City_Vehari',\n       'Registered City_Wah', 'Transaction Type_Installment\/Leasing']","9387bf3b":"data_preprocessed = data_with_dummies[cols]\ndata_preprocessed.head()","97a2b10e":"targets = data_preprocessed['log_price']\ninputs = data_preprocessed.drop([\"log_price\"], axis=1)","bc7e32e7":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\n\nclass CustomScaler(BaseEstimator, TransformerMixin):\n    def __init__(self, columns, copy=True, with_mean=True, with_std=True):\n        self.scaler = StandardScaler(copy, with_mean, with_std)\n        self.columns = columns\n        self.mean_ = None\n        self.std_ = None\n    \n    def fit(self, X, y=None):\n        self.scaler.fit(X[self.columns], y)\n        self.mean_ = np.mean(X[self.columns])\n        self.std_ = np.std(X[self.columns])\n        return self\n    \n    def transform(self, X, y=None, copy=None):\n        init_col_order = X.columns\n        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n        X_not_scaled = X.loc[:, ~X.columns.isin(self.columns)]\n        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]","42bf8e68":"columns_to_scale = ['KMs Driven', 'Year']","efa42e99":"scaler = CustomScaler(columns_to_scale)\nscaler.fit(inputs)\ninputs_scaled = scaler.transform(inputs)","92ea2091":"inputs_scaled.head(10)","d2671ddb":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(inputs, targets, random_state=42, test_size=.2)","80edb501":"from sklearn.linear_model import LinearRegression\nreg = LinearRegression()\nreg.fit(x_train, y_train)","3cc10f14":"y_hat = reg.predict(x_train)","f51a6d7c":"plt.scatter(y_train, y_hat)\nplt.xlabel('y_train')\nplt.ylabel('y_hat')","e6f77503":"sns.distplot(y_train-y_hat)\nplt.title('Residual PDF')","d589baad":"reg.score(x_train, y_train)","9496f829":"reg_summary = pd.DataFrame(inputs.columns.values, columns=['Feautures'])\nreg_summary['Weights'] = reg.coef_\nreg_summary","64516d46":"y_hat_test = reg.predict(x_test)","9ca7ce58":"plt.scatter(y_train, y_hat, alpha=.2)\nplt.plot([10,16], [10,16], 'r')\nplt.xlabel('y_train')\nplt.ylabel('y_hat')\nplt.xlim([9, 17])\nplt.ylim([9, 17])","23c72fe1":"## Declare target and input data","d17a7128":"## MultiColliniearity\nWe can check multicollinearity using VIF method of statsmodels. \nIt is logical to expect for years and KMs Driven will be correlated, the  newer the car the lower its km. ","91611c8e":"I assume you already know how to read above table. \nCount shows number of values in each column\nUnique shows number of uniques values \ntop is the most common value (generally works for categorical variables)\nfreq is the frequency of most common value\nmean, std, min and mmax doesn't need explanation\nthe rest oare quartile values\n\nNote: There are 303 different models. That is a lot of dummies (which we will talk about when we are dealing with categrical values) for a regression model\n\nLet's count the nbumber of missing values in each column","d646ca67":"## Create Dummy variables\nNominal (or categorical) variables cannot be processed by linear regression, so we need to turn them into something that our model can interpret. \nPandas has a very useful method called get_dummies, it will automatically detect categorical variables and creat dummies automatically. \nImportant note here, do not forget to set drop_first hyperparameter to True otherwise you would introduce multicollinearity to the regression\n","6151a921":"It doesn't look that bad. REsidual plots will give us some more information about our model","ecaa432d":"Price is not normally distributed and from there its relationship with other features is not linear. Log transformation is common way to deal with this type of issues. It is especially useful when facing exponential scatter plots like we do no. Let's take log of price and replot it","ca580813":"Now price looks more normally distributed. \n\nWe will do the same thing for KMs Driven and Year columns as well.","5b0d225d":"## Testing","8cfe907a":"They look much better now. Let's drop original price since we won't need anymore","a4798ab1":"## Cheking Linear Regression Assumption\nLinear Regression has 6 assumpitions: 1)Linearity, 2) No Endogeneity, 3) Normality 4) No autocorrelation 5) NO multicollinearity 6) Homoscedasticity\nwe will check whether continuous variables such as KMs Driven, Year are likely to cause us some problems. We don't need to worry about categorical regresssors though. ","d8e7f121":"## No Autocorrelation\nAuto correlation is common in time series data, there is no auto correlation in between data points, all the cars sold by different individuals. There is no reason for data to be dependent on each others.","e20c0fe6":"A vif value between 1 and 5 is considered ok. \nIt seems like there is no correlation at all, let's see the plot","8cd82a4c":"There are 303 different models. Let's drop it.\nAnd Drop missing values","62bd94e7":"## No Endogenity\nThere are test to check whether it is violated, or one can take residuals and find their correlation with each indepdent x. We will assume it is not vialoted because it is not.  ","1d49002b":"We lost around 2500 data points after deleting missing rows. We could replace some missing values with average values of the rows they are belong to but I believe we have enought data and I am not going to do that","2e1d7118":"## Rearrange the dataset a little","bff0eb81":"## Weights inpterpretation\nOK there are too many of them. LEt me say this\nPositive Continuous variable shows that as the value increases so do the log_price and Price respectively \nNegative Continuous variable shows that as the value increases, log_price and Price decrease\n\nPositive dummy shows that respective category is more expensive than benchmark (dropped value) in the case of Brand that is AUDI (get_dummies put things in alphabetical order and drop first one). For example BMW has a positive weigth that means BMW are more expensive than AUDIs\n\nNegative dummy shows that respective category is chepaer than benchmark (dropped value). For example Chevrolet has a negtive weigth that means AUDIs are more expensive than Chevrolet","41dddee5":"## Scale the data\nNormally we shouldn't scale dummies. For that reason instead of using StandardScaler from sklean I will creat a CustomScaler and scale only continuous variables.","63c721fd":"Residuals are distributed normally with mean of zero as expected. The result looks quite normal in the sense of normally distributed the mean seems to be 0.\nThe only possible issue is that there is a much longer tail on the negative side. Therefore there are certain observations for which y_train minus y_hat is much lower than the mean. This implies that those predictions tend to overestimate the targets. However, there are no such observations on the right side, we conclude that the predictions rarely underestimate the targets.\n\nNow let's see R-squared","1eeec60e":"That means our model explains 77% of the variability of the data. This is a relatively good result. \n\nLet's interpret coefficients and bias","6bb5485f":"## Create a regression","25b855fd":"By setting alpha to 0.2 we can see the where the concentration of data points. We see that most of the points are in indeed very close to 45 degree line. ","2411ce9d":"One of the assumption of Linear Regression is normality. Data should be normally distributed. In this section we will check pdf of each continuous variable and fix themf if distribution is skewed. \n\nLet's start with Price","fcedf2c4":"LEt's plot y_train y_hat graph, if we predictions are goo everything should be clustered around a 45 degree line because if the value of y_train is 10 we would want that our prediction was 10 as well. The further we go from 10 the worst our prdiction is","caf57b69":"## Exploring the PDFs","368ea2b3":"WE can spot pattern but they don't seem to be linear, we can use log tranformation to convert exponential data into linear one. Let's look at distribution of Price up again","27a95e13":"## Normality and Homoscadasticity\nNormality is assumed for big samles (Thanks Central Limit Theorem)\nThe zero mean of the distribution of errors is accomplished to the inclusion of the intercept to the regression\nHomoscadasticity generally holds. Check two plots above, scatter plots can be contained within two lines without diverging from a linear line that would fit the data. And the reason for that is because we already implemented log transformation which is a remedy for Homoscadasticity","955b2b3f":"# Preprocessing\n## Exploring the descriptive statistics of the variables","ddfe4f23":"## Let's start with Linearity","06afd2ab":"We would be looking for a normal distribution price however has an exponential one. This will surely be a problem for our regression.\nThis inforamtion could be gained from descriptive statistics as well. If you look at mean, min, and max values of Prce column above you can draw similar conclusion. There are some outliers in our dataset. We will remove them by using quantile method. I want to get the ninety ninth percentile and keep the data below the ninety ninth percentile. Pay attention we will lose some 200 data points after removing outliers in the price column. ","68a379c7":"# Regresssion Model","1971a60c":"## Train and test split","a49574e0":"# Conclusion\nOur model is trying to predict price of a car based on km, year, brand, registration and city. On average it is pretty decent in prediction the price. Clearly we are missing some information. It can be the model we removed at the beginning or an information we didn't initially have; for example we don't know if these cars have involved in accidents. Machine Learning algorithms require some subject matter expertise and months of labor. Here I just implemented most strightforward method. Feel free to imporove this model. "}}