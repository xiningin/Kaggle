{"cell_type":{"0287ec6b":"code","4e5da574":"code","578804f3":"code","1daff683":"code","0bcf9805":"code","6551397b":"code","f24801f9":"code","2468cf49":"code","8fa9f355":"code","a2d93bc7":"code","65a498c9":"code","7ed10f9f":"code","c0e951f4":"code","07fa013f":"code","b7128c6b":"code","57ad2b79":"code","17407bef":"code","761f0c1a":"code","72ca8731":"code","c263f05f":"code","5e77269c":"code","9d82b2b1":"code","d6ba53fa":"code","36c3f5f9":"code","c167e6fe":"code","b57eb191":"code","0d68490c":"code","ff759916":"code","f8456564":"code","d15ad1bc":"code","a11608b1":"code","17c6f4e4":"code","a60f1dad":"code","30fcaa24":"code","b7074f38":"code","7c86a0d3":"code","d96a7b1f":"code","e37c2add":"code","5d981d7a":"code","8d88e463":"code","5c4f2a75":"code","a9518b08":"code","087f5845":"markdown","f22bbc34":"markdown","4c325ad5":"markdown","b0d00ee5":"markdown","b6554ec6":"markdown","cda6fc8a":"markdown","351be3c7":"markdown","075a9b78":"markdown","46ff6be9":"markdown","5dadf16c":"markdown","ea489b39":"markdown","03125348":"markdown","1a1a27f9":"markdown","f40cded2":"markdown","4f4d3aa7":"markdown","ec805182":"markdown","b8b041a3":"markdown","13470d46":"markdown","855526ca":"markdown","155c146d":"markdown","791bfe97":"markdown","b1b17648":"markdown","726800bd":"markdown","22623923":"markdown","896fb7b8":"markdown","acf0942a":"markdown","90e5f532":"markdown","24ea4fbf":"markdown","0f4cf32f":"markdown","0da97938":"markdown","41861998":"markdown","cc215f4b":"markdown","2a774c3c":"markdown","09b70618":"markdown","5d72b8f3":"markdown","5b6cf795":"markdown"},"source":{"0287ec6b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4e5da574":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(color_codes=True)\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass color:\n   PURPLE = '\\033[95m'\n   CYAN = '\\033[96m'\n   DARKCYAN = '\\033[36m'\n   BLUE = '\\033[94m'\n   GREEN = '\\033[92m'\n   YELLOW = '\\033[93m'\n   RED = '\\033[91m'\n   BOLD = '\\033[1m'\n   UNDERLINE = '\\033[4m'\n   END = '\\033[0m'","578804f3":"ds = pd.read_csv('..\/input\/regression-with-neural-networking\/concrete_data.csv');\nds.info()\nds.shape","1daff683":"#Check for missing values\nprint(ds.isna().sum())\nprint(ds.isnull().sum())","0bcf9805":"#Little more statistical understanding\nds.describe().T","6551397b":"#Let's rename the columns for better handling\nds.rename(columns = {'Strength': 'strength', 'Cement': 'cement', 'Blast Furnace Slag': 'slag', 'Fly Ash': 'ash' ,'Water': 'water', 'Superplasticizer': 'superplastic', 'Coarse Aggregate': 'coarseagg' ,'Fine Aggregate': 'fineagg', 'Age': 'age' }, inplace=True)","f24801f9":"# Let's split the data before doing further analysis:\n# train test split 70:30\n#Remove the target varible\ntarget = ds[['strength']]\ndata_set_buffer = ds.drop('strength',axis=1)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data_set_buffer, target, test_size = 0.3, random_state = 123)","2468cf49":"data_set_buffer.tail()","8fa9f355":"sns.relplot(x=\"cement\", y=\"strength\",  data=ds);\nsns.relplot(x=\"slag\", y=\"strength\",  data=ds);\nsns.relplot(x=\"ash\", y=\"strength\",  data=ds);\nsns.relplot(x=\"water\", y=\"strength\",  data=ds);\nsns.relplot(x=\"superplastic\", y=\"strength\",  data=ds);\nsns.relplot(x=\"coarseagg\", y=\"strength\",  data=ds);\nsns.relplot(x=\"fineagg\", y=\"strength\",  data=ds);\nsns.relplot(x=\"age\", y=\"strength\",  data=ds);\n#Looks like superplastic and cement have +ve correlation with the strength rest all are not straight forward to infer.","a2d93bc7":"sns.relplot(x=\"cement\", y=\"strength\", alpha=.5, palette=\"muted\",\n            height=6, data=ds)\n#distplot\nsns.relplot(x=\"water\", y=\"strength\", alpha=.5, palette=\"muted\",\n            height=6, data=ds)\n\n#histogram\nsns.relplot(x=\"slag\", y=\"strength\", alpha=.5, palette=\"muted\",\n            height=6, data=ds)\n\nsns.relplot(x=\"ash\", y=\"strength\", alpha=.5, palette=\"muted\",\n            height=6, data=ds)\n","65a498c9":"# Let's analyse the strength against water and fly-ash level\n\n# create figure and axis objects with subplots()\nfig,ax = plt.subplots()\n# make a plot\nax.scatter(ds.strength,ds.ash,  color=\"red\", marker=\",\")\n# set x-axis label\nax.set_xlabel(\"Strength\",fontsize=14)\n# set y-axis label\nax.set_ylabel(\"Ash\",color=\"red\",fontsize=14)\n\n# twin object for two different y-axis on the sample plot\nax2=ax.twinx()\n# make a plot with different y-axis using second axis object\nax2.scatter(ds.strength,ds.water,color=\"blue\",marker=\"+\")\nax2.set_ylabel(\"Water\",color=\"blue\",fontsize=14)\nplt.show()\n# save the plot as a file\nsns.relplot(x=\"water\", y=\"ash\", alpha=.5, palette=\"muted\",\n            height=6, data=ds)","7ed10f9f":"from pandas.plotting import scatter_matrix\n\nscatter_matrix(ds.loc[:],figsize=(12, 12),diagonal=\"kde\")\nplt.tight_layout()\nplt.show()","c0e951f4":"ds.corr().T","07fa013f":"#Strength is the target column while rest are our input variable.\n#Let's do box plot to find the outliers if there any\nplt.figure(figsize=(35,15))\nsns.boxplot(data=ds)","b7128c6b":"import itertools\nplt.figure(figsize=(10,20))\nj=0;\nfor i, c in zip(ds.columns , list(ds.columns.values) ):\n    j=j+1\n    plt.subplot(3,3,j)\n    sns.swarmplot( y = ds[i]);\n    plt.title= c\n    plt.show","57ad2b79":"# Let's replace the outliers with median values\nimport numpy as np\noutliers =  []\nfrom scipy import stats\nzscore = ds.apply(stats.zscore)\nds_columns = ds.columns.values\nfor key, value in zscore.iteritems():\n    row=0\n    medianV = np.median(ds[[key]])\n    columnnum = ds.columns.get_loc(key)\n    for v in value:\n        if v>3 or v < -3 :\n            outliers.append(v)\n            ds.iloc[row,columnnum] = medianV\n            print (ds.iloc[row,columnnum] )\n        row = row + 1\n     \nprint(\"outlier size \", len(outliers))","17407bef":"from sklearn.tree import DecisionTreeRegressor\ndtr_model = DecisionTreeRegressor()\ndtr_model.fit(X_train , y_train)","761f0c1a":"print('The feature importances starting from hightest:')\nfor (i, j) in zip(X_train.columns , reversed(sorted(dtr_model.feature_importances_))): \n    print(i,j)","72ca8731":"print(dtr_model.score(X_train, y_train))\nprint(dtr_model.score(X_test, y_test))","c263f05f":"# Regularizing the Decision tree classifier to avoid over fitting situation described above\nreg_dtr_model = DecisionTreeRegressor( max_depth = 5,random_state=1,min_samples_leaf=6)\nreg_dtr_model.fit(X_train, y_train)","5e77269c":"print(reg_dtr_model.score(X_train, y_train))\nprint(reg_dtr_model.score(X_test, y_test))","9d82b2b1":"print('The feature importances:')\nfor (i, j) in zip(X_train.columns ,reversed(sorted(reg_dtr_model.feature_importances_))): \n    print(i,j)","d6ba53fa":"import matplotlib.pyplot as plt\n\n# Pie chart, where the slices will be ordered and plotted counter-clockwise:\nlabels = X_train.columns\nsizes = reg_dtr_model.feature_importances_\nfig1, ax1 = plt.subplots()\nwedges, texts, autotexts = ax1.pie(sizes, labels=labels, autopct='%1.01f%%',\n        shadow=True, startangle=90,pctdistance=1.2, labeldistance=1.5 )\nax1.axis('equal') \nax1.legend(wedges, labels,\n          title=\"Feature Importance:\",\n          loc=\"center left\",\n          bbox_to_anchor=(1, 0, 0.5, 1))\nplt.setp(autotexts, size=10, weight=\"bold\")\n\nax1.set_title(\"Feature Importance Pie\")","36c3f5f9":"#Let's confirm the correlation with correlation matrix\n#Find correlation\nplt.figure(figsize=(10,10))\ncorr =train_data_Set.corr()\nmatrix = np.triu(corr)\nax=sns.heatmap(corr, annot=True, mask=matrix,cmap= sns.diverging_palette(20, 220, n=200), linewidths=2,linecolor='white',square=True)\nplt.show()","c167e6fe":"#Let's drop the coarseagg column and use important features only to analyse further.\ndata_set_buffer_feature_imprtance = ds.drop('coarseagg',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(data_set_buffer_feature_imprtance, target, test_size = 0.3, random_state = 123)","b57eb191":"import statsmodels.api as sm\nlr_1 = sm.OLS(y_train, X_train).fit()","0d68490c":"y_pred = lr_1.predict(X_test)","ff759916":"from sklearn import metrics\nmetrics.explained_variance_score(y_test, y_pred)","f8456564":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n# Import Linear Regression machine learning library\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nnum_folds = 50\nseed = 7\n\nkfold = KFold(n_splits=num_folds, random_state=seed)\nmodel = LinearRegression()\nresults = cross_val_score(model, data_set_buffer, target, cv=kfold)\nprint(results)\nprint(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))\nprint(color.RED)\nprint(color.BOLD)\nprint('Accuracy Score')\nprint(color.BLUE)\nprint('Average: ', results.mean())\nprint('Standard deviation ',results.std())","d15ad1bc":"data_set_buffer_removed_outlier = data_set_buffer\nzscore = data_set_buffer_removed_outlier.apply(stats.zscore)\nds_columns = data_set_buffer_removed_outlier.columns.values\nfor key, value in zscore.iteritems():\n    row=0\n    medianV = np.median(data_set_buffer[[key]])\n    columnnum = data_set_buffer_removed_outlier.columns.get_loc(key)\n    for v in value:\n        if v>3 or v < -3 :\n            data_set_buffer_removed_outlier.iloc[row,columnnum] = medianV\n        row = row + 1\n\ntarget_removed_outlier = target\nzscore = target_removed_outlier.apply(stats.zscore)\nds_columns = target_removed_outlier.columns.values\nfor key, value in zscore.iteritems():\n    row=0\n    medianV = np.median(target[[key]])\n    columnnum = target_removed_outlier.columns.get_loc(key)\n    for v in value:\n        if v>3 or v < -3 :\n            target_removed_outlier.iloc[row,columnnum] = medianV\n        row = row + 1\n","a11608b1":"num_folds = 6\nseed = 7\n\nkfold = KFold(n_splits=num_folds, random_state=seed)\nmodel = LinearRegression()\nresults = cross_val_score(model, data_set_buffer_removed_outlier, target_removed_outlier, cv=kfold)\nprint(results)\nprint(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))\nprint(color.RED)\nprint(color.BOLD)\nprint('Accuracy Score after replacing outliers with median')\nprint(color.BLUE)\nprint('Average: ', results.mean())\nprint('Standard deviation ',results.std())","17c6f4e4":"ridge = Ridge(alpha=.3)\nridge.fit(X_train,y_train)\nprint (\"Ridge model:\", (ridge.coef_))","a60f1dad":"lasso = Lasso(alpha=0.1)\nlasso.fit(X_train,y_train)\nprint (\"Lasso model:\", (lasso.coef_))","30fcaa24":"print(lasso.score(X_train, y_train))\nprint(lasso.score(X_test, y_test))","b7074f38":"print(ridge.score(X_train, y_train))\nprint(ridge.score(X_test, y_test))","7c86a0d3":"from sklearn import preprocessing\nfrom sklearn.preprocessing import PolynomialFeatures\n\nX_scaled = preprocessing.scale(data_set_buffer_removed_outlier)\nX_scaled = pd.DataFrame(X_scaled, columns=data_set_buffer_removed_outlier.columns)  \n\ny_scaled = preprocessing.scale(y_train)\ny_scaled = pd.DataFrame(y_scaled, columns=target.columns)  \n\npoly = PolynomialFeatures(degree = 2, interaction_only=True)\nX_poly = poly.fit_transform(X_scaled)\nX_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(X_poly, target, test_size=0.30, random_state=1)\nX_train_poly.shape\n\nfrom sklearn.linear_model import LinearRegression\nregression_model = LinearRegression()\nregression_model.fit(X_train_poly, y_train_poly)\nprint(regression_model.coef_[0])\nridge = Ridge(alpha=.3)\nridge.fit(X_train_poly,y_train_poly)\nprint (\"Ridge model:\", (ridge.coef_))","d96a7b1f":"print(ridge.score(X_train_poly, y_train_poly))\nprint(ridge.score(X_test_poly, y_test_poly))\n","e37c2add":"lasso = Lasso(alpha=0.01)\nlasso.fit(X_train_poly,y_train_poly)\nprint (\"Lasso model:\", (lasso.coef_))\nprint(lasso.score(X_train_poly, y_train_poly))\nprint(lasso.score(X_test_poly, y_test_poly))","5d981d7a":"regression_model = LinearRegression()\nregression_model.fit(X_train, y_train)\ny_pred = regression_model.predict(X_test)\nfrom sklearn import metrics\nmetrics.explained_variance_score(y_test, y_pred)","8d88e463":"from sklearn.ensemble import RandomForestRegressor\nrandomf = RandomForestRegressor(n_estimators=10, max_depth = 3)\nrandomf.fit(X_train, y_train)\nrandomf.score(X_test, y_test)","5c4f2a75":"from sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor()\ngbr.fit(X_train, y_train)\ngbr.score(X_test, y_test)","a9518b08":"from sklearn.neighbors import KNeighborsRegressor\nknnr = KNeighborsRegressor(n_neighbors=10)\nknnr.fit(X_train, y_train)\nknnr.score(X_test, y_test)","087f5845":"### Observation:\nLooks like there are not much outliers but age,water, slag and superplastic surely have some outlier. If these belong to feature importance we have to take care of it.\nThere are data for slag and ash also looks quite right skewed as we have analysed earlier.","f22bbc34":"### Univariant Analysis: ","4c325ad5":"**Linear Regression is almost perfect! Let's find out with cross validation.**","b0d00ee5":"### Observation : \nwater has -ve correlation with superplastic and fineagg.\ncement and strength has +ve correlation.","b6554ec6":"**Let's apply paarameter tuning**","cda6fc8a":"Observation : Our accuracy seems sensitive to outliers here as we can see with 6 K fold cross validation.","351be3c7":"**OBSERVATION : As simple linear regression is providing high accuracy we DON'T NEED polynomial of higher order. We shall be fine with simple linear models. As we observed with higher order, we loose our predectibility power.**","075a9b78":"# Let us generate polynomial models reflecting the non-linear interaction between some dimensions","46ff6be9":"# Random Forest","5dadf16c":"*It looks like the strength of cement depends on various proportions of ingredient. For e.g. The strength remains same for various amoung of ash level. Same behaviour can be observed for Slag ans Super plasitc. But one thing is clear that the strength has linearly related with the main ingredient that is 'Cement'. One important thing like any chemical composition, there need to be a perfect balance of various reagents required for best strength to achieve.This paves the way to do multivariant analysis. For e.g. we might observe that, if we have more water, we might need more fly ash to strengthen the concrete. Let's find out this.*","ea489b39":"## Let us compare their scores","03125348":"### Let's address the presence outliers and missing values and perform necessary imputation ","1a1a27f9":"###  Feature Engineering , Model Building and Model Tuning","f40cded2":"### Observation : Cement,Age,Slag and Water Can  explain more than 96% of Strength. We can keep 4 variables and drop the rest. Just before doing let's find if there any strong correlation between other dependant variables.","4f4d3aa7":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.9em;color:#0e92ea\">EDA","ec805182":"Looks like yes. We might need more water for more ash.","b8b041a3":"UNIVARIANT ANALYSIS :\n\n1) The data set looks balanced. With 1030 number of rows and 8 columns to learn from is ok. Except the age column rest of the data looks well spread with uniform distribution.\n\n2) slag and ash  data looks highly spreadout with min zero, 1st and 2nd Quirtile almost zero whild max value ranged from 350 to 540.\n\n3) age data looks in days and max 365 and min 1.","13470d46":"# Create a regularized LASSO model and note the coefficients","855526ca":"### Linear Regression","155c146d":"# Create a regularized RIDGE model and note the coefficients","791bfe97":"### 1) With Linear Regression , Random Forest and Gradient Boosting Regressor we can achieve > 95% accuracy while KNeighbors Regressor has only 76% accuracy.\n### 2) This Proves our earlier assumption that the given dataset fits well for regressor algos which works well for continuous data rather than classification techniques.\n### 3) Our model accuracy improved significantly after replacing outliers with corresponding medians. i.e. the models we have used or infact most of the models are succeptible to  outliers present in feature importance input data.\n### 4) Polynomials of higher orders are not necessarily improve model accuracy or generalization. It depends the dataset. As we observed this data set the strength of concrete surely depends on various proportions of inputs e.g. amount of cement, amount of fly ash, age so linear equations or first degree equations works well. e.g. =>3x+6y+6z = 8s","b1b17648":"K fold cross validation without removing outlier and with 50 folds:","726800bd":"**Observation : Water has strong -ve relation with Superplastic and fineagg so just considering water and not taking Superplastic and fineagg will not be a good idea.\nSimilarly, ash has -ve co-relation with cement so we can't drop ash too.**","22623923":"### Observation :\nThere is no fixed ratio and proportion of input variables to achive the higher strength. \n\nWe can also see that, The parameters like water vs superplastic follows an L shaped curve. That shows we need to maintain the ratios of multiple ingredients to achieve the same strength. \n\nWe can  observe ash vs slag follows a linear relation. i.e. we have multi-colinearity relation among the input parameters which we must have to take care. \n\nExcept cement and Strength other variables have multiple Gaussian distributions So definitely they have outliers also. \n\nScaling : Looks fine as we can see the input parameters with  outliers and we have independent variables in same units.\n ","896fb7b8":"### KNeighborsRegressor ","acf0942a":"### Bi-variant Analysis: ","90e5f532":"### Observation : Our feature of imprtance would be : Cement,Age,Slag, Water, Superplastic,fineagg and ash","24ea4fbf":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.5em;color:#0e92ea\">\nLooks the given data set is quite clean there is no missing values and all data types are numbers.","0f4cf32f":"#### Looks like the Decision Tree Regression model is in over fitting state we have to tune our model.","0da97938":"<span style=\"font-family: Arial; font-weight:bold;font-size:1.9em;color:#0e92ea\">Conclusion","41861998":"Important Point : We have to employ only Regression technique and not classificiation ones because our output or the target is continious variable. Classification is ONLY be used for categorical varibles. As we have a data set of where target variable is continuous neiture, Regression would do good rather than classification Techniques such as Naive Bayes or KNN. Let's see\n\nBut our Kfold cross validation is only Accuracy 70%. That shows our model is succeptible to outliers and not generalizing well also.","cc215f4b":"### Observation : cement,age ,water , ash , slag are important feature of imprtance","2a774c3c":"### There are 10 values with zscore inbetween -3 and +3 so we got 10 outliers. ","09b70618":"## Multi-Variant Analysis:","5d72b8f3":"### Let's try a simple Linear Regression and test with cross validataion to arrive any conclusion.","5b6cf795":"### Gradient Boosting Regressor "}}