{"cell_type":{"9216be4f":"code","f386ae1e":"code","b8c0b075":"code","4500a90e":"code","ab09ed69":"code","434ee588":"code","53ad558f":"code","644dbc27":"code","6229e087":"code","54a3a723":"code","bf6467fa":"code","acdab2ac":"code","cafeb3c1":"code","912c2648":"code","92a64d3b":"markdown","ff6278fb":"markdown","15a520e4":"markdown"},"source":{"9216be4f":"#import libraries\nimport tensorflow as tf\nfrom keras.layers import Input, Lambda, Dense, Flatten\nfrom keras.models import Model\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.preprocessing import image\nfrom keras.models import Sequential\nfrom keras.models import load_model\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt","f386ae1e":"#resizing images to 224\nIMAGE_SIZE = [224, 224]","b8c0b075":"#assigning the train, validation directories of the dataset\ntrain_path = \"..\/input\/fruits\/fruits-360_dataset\/fruits-360\/Training\"\nvalidation_path = \"..\/input\/fruits\/fruits-360_dataset\/fruits-360\/Test\"","4500a90e":"vgg = VGG16(input_shape = IMAGE_SIZE + [3], weights = 'imagenet', include_top = False)","ab09ed69":"for layer in vgg.layers:\n    layer.trainable = False","434ee588":"folders = glob(train_path + '\/*')\nfolders","53ad558f":"flattened_vgg = Flatten()(vgg.output)","644dbc27":"prediction  =  Dense(len(folders), activation='softmax')(flattened_vgg )","6229e087":"model = Model(inputs = vgg.input, outputs = prediction)\n\n#model structure\nmodel.summary()","54a3a723":"model.compile(\n    loss = 'categorical_crossentropy',\n    optimizer = 'adam',\n    metrics = 'accuracy'\n)","bf6467fa":"#Using Data Augmentation technigues to build a more compact \nfrom keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True)\n\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntraining_set = train_datagen.flow_from_directory(train_path,\n                                                 target_size = (224, 224),\n                                                 batch_size = 32,\n                                                 class_mode = 'categorical')\n\ntest_set = test_datagen.flow_from_directory(validation_path,\n                                            target_size = (224, 224),\n                                            batch_size = 32,\n                                            class_mode = 'categorical')\n","acdab2ac":"# fit the model\nr = model.fit(\n  training_set,\n  validation_data=test_set,\n  epochs=5,\n  steps_per_epoch=10000,\n  validation_steps=3000\n)","cafeb3c1":"# loss\nplt.plot(r.history['loss'], label='train loss')\nplt.plot(r.history['val_loss'], label='val loss')\nplt.legend()\nplt.show()\nplt.savefig('LossVal_loss')\n\n# accuracies\nplt.plot(r.history['accuracy'], label='train acc')\nplt.plot(r.history['val_accuracy'], label='val acc')\nplt.legend()\nplt.show()\nplt.savefig('AccVal_acc')","912c2648":"model.save('fruits_model.h5')","92a64d3b":"Hello folks!!\nRecently i cam across the transfer learning methods which include VGG16, Inception, Resnet50, Xception.\nVGG16 is a convolution neural net (CNN ) architecture which was used to win ILSVR(Imagenet) competition in 2014. It is considered to be one of the excellent vision model architecture till date. The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is an annual computer vision competition. Each year, teams compete on two tasks. The first is to detect objects within an image coming from 200 classes, which is called object localization. The second is to classify images, each labeled with one of 1000 categories, which is called image classification.\n\nThe structure of the VGG is as follows:\n\n![vgg-structure.png](attachment:c811a6cc-adb4-43b1-ad4b-72d3ed714f7b.png)\n\nWithout further adieu, lets dive in to the code and implement the Fruits 360 dataset classification using VGG16. The complete documentation can be found at https:\/\/keras.io\/api\/applications\/vgg\/","ff6278fb":"Below we save the model naming it fruits_model","15a520e4":"The IMAGE_SIZE variable is usually size of [224,224] because the images trained in the VGG16 model are of that size. However we can use lower number in order to reduce the accuracy and increase the computation."}}