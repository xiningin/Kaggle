{"cell_type":{"d688290d":"code","aa643628":"code","fa04209e":"code","4454e19d":"code","0b538096":"code","042f8e72":"code","4e11ff35":"code","6461244c":"code","14ac670b":"code","7cb904ec":"code","e34f3e23":"code","623b0610":"code","1bd57d5c":"code","e1d3b2dc":"code","7289b606":"code","e2cf8aa1":"code","9d7f9a41":"code","f8c8e2b5":"code","8cca439f":"markdown","fa062419":"markdown","02fcfa23":"markdown","57814d22":"markdown","5d80b459":"markdown"},"source":{"d688290d":"!nvcc -V\n!gcc --version ","aa643628":"!pip install -U torch==1.7.1+cu110 torchvision==0.8.2+cu101 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html\n!pip install mmcv-full\n!rm -rf mmdetection\n!git clone https:\/\/github.com\/open-mmlab\/mmdetection\n%cd mmdetection\n!pip install -e .\n!pip install Pillow==7.0.0","fa04209e":"import torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())\n\nimport mmdet as mmdet\nprint(mmdet.__version__)\n\nfrom mmcv.ops import get_compiling_cuda_version, get_compiler_version\nprint(get_compiling_cuda_version())\nprint(get_compiler_version())\n\nimport os\nfrom mmdet.datasets import build_dataset\nfrom mmdet.models import build_detector\nfrom mmdet.apis import train_detector\nimport glob\nimport cv2\nimport shutil\nimport random\nimport os.path as osp\nimport json\nimport numpy as np\nimport pandas as pd\nimport mmcv\nfrom mmdet.apis import set_random_seed\nfrom sklearn.model_selection import train_test_split\nimport re\nimport xml.etree.ElementTree as ET\nfrom typing import Dict, List\nfrom mmdet.apis import inference_detector, init_detector, show_result_pyplot","4454e19d":"global_seed = 0\n\ndef set_seed(seed=global_seed):\n    \"\"\"Sets the random seeds.\"\"\"\n    set_random_seed(seed, deterministic=False)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed()","0b538096":"%cd ..","042f8e72":"%%writefile labels.txt \nboat","4e11ff35":"files_list = []\nfor file in os.listdir('..\/input\/ship-detection\/annotations'):\n    file_path = os.path.join('..\/input\/ship-detection\/annotations', file)\n    files_list.append(file_path)","6461244c":"train_files, val_files = train_test_split(files_list, train_size=0.8, random_state=0)\nval_files, test_files = train_test_split(val_files, train_size=0.5, random_state=0)","14ac670b":"with open('train.txt', 'x') as f:\n    for file in train_files:\n        f.write(file)\n        f.write('\\n')\n\nwith open('val.txt', 'x') as f:\n    for file in val_files:\n        f.write(file)\n        f.write('\\n')\n        \nwith open('test.txt', 'x') as f:\n    for file in test_files:\n        f.write(file)\n        f.write('\\n')","7cb904ec":"def get_label2id(labels_path: str) -> Dict[str, int]:\n    with open(labels_path, 'r') as f:\n        labels_str = f.read().split()\n    labels_ids = list(range(1, len(labels_str)+1))\n    return dict(zip(labels_str, labels_ids))\n\n\ndef get_annpaths(ann_dir_path: str = None,\n                 ann_ids_path: str = None,\n                 ext: str = '',\n                 annpaths_list_path: str = None) -> List[str]:\n    # If use annotation paths list\n    if annpaths_list_path is not None:\n        with open(annpaths_list_path, 'r') as f:\n            ann_paths = f.read().split()\n        return ann_paths\n\n    # If use annotaion ids list\n    ext_with_dot = '.' + ext if ext != '' else ''\n    with open(ann_ids_path, 'r') as f:\n        ann_ids = f.read().split()\n    ann_paths = [os.path.join(ann_dir_path, aid+ext_with_dot) for aid in ann_ids]\n    return ann_paths\n\n\ndef get_image_info(annotation_root, extract_num_from_imgid=True):\n    filename = annotation_root.findtext('filename')\n    filename = '..\/input\/ship-detection\/images\/' + filename\n    img_name = os.path.basename(filename)\n    img_id = os.path.splitext(img_name)[0]\n    if extract_num_from_imgid and isinstance(img_id, str):\n        img_id = int(re.findall(r'\\d+', img_id)[0])\n    \n\n#     size = annotation_root.find('size')\n#     width = int(size.findtext('width'))\n#     height = int(size.findtext('height'))\n#     print(filename)\n    height, width, depth = cv2.imread(filename).shape\n\n    image_info = {\n        'id': img_id,\n        'width': width,\n        'height': height,\n        'file_name': filename,\n    }\n    return image_info\n\n\ndef get_coco_annotation_from_obj(obj, label2id):\n    label = obj.findtext('name')\n#     assert label in label2id, f\"Error: {label} is not in label2id !\"\n    category_id = label2id[label]\n    bndbox = obj.find('bndbox')\n    xmin = int(float(bndbox.findtext('xmin')))\n    ymin = int(float(bndbox.findtext('ymin')))\n    xmax = int(float(bndbox.findtext('xmax')))\n    ymax = int(float(bndbox.findtext('ymax')))\n    assert xmax > xmin and ymax > ymin, f\"Box size error !: (xmin, ymin, xmax, ymax): {xmin, ymin, xmax, ymax}\"\n    o_width = xmax - xmin\n    o_height = ymax - ymin\n    ann = {\n        'category_id': category_id,\n        'segmentation': [],  # This script is not for segmentation\n        'area': o_width * o_height,\n        'bbox': [xmin, ymin, o_width, o_height],\n        'iscrowd': 0,\n    }\n    return ann\n\n\ndef convert_xmls_to_cocojson(annotation_paths: List[str],label2id: Dict[str, int], output_jsonpath: str, extract_num_from_imgid: bool = True):\n    output_json_dict = { \n        \"images\": [],\n        \"annotations\": [],\n        \"categories\": []\n    }\n    bnd_id = 1  # START_BOUNDING_BOX_ID, TODO input as args ?\n    print('Start converting !')\n    for a_path in annotation_paths:\n        # Read annotation xml\n        ann_tree = ET.parse(a_path)\n        ann_root = ann_tree.getroot()\n\n        img_info = get_image_info(annotation_root=ann_root,extract_num_from_imgid=extract_num_from_imgid)\n        img_id = img_info['id']\n        output_json_dict['images'].append(img_info)\n\n        for obj in ann_root.findall('object'):\n            ann = get_coco_annotation_from_obj(obj=obj, label2id=label2id)\n            annot = {'id': bnd_id, 'image_id': img_id,}\n            annot.update(ann)\n            output_json_dict['annotations'].append(annot)\n            bnd_id = bnd_id + 1\n\n    for label, label_id in label2id.items():\n        category_info = {'id': label_id, 'name': label, 'supercategory': 'none'}\n        output_json_dict['categories'].append(category_info)\n\n    with open(output_jsonpath, 'w') as f:\n        output_json = json.dumps(output_json_dict)\n        f.write(output_json)\n        \n        \ndef convert_to_coco(ann_path_list='\/kaggle\/working\/train.txt', labels='\/kaggle\/working\/labels.txt', output='\/kaggle\/working\/output.json'):\n    label2id = get_label2id(labels_path=labels)\n    ann_paths = get_annpaths(\n        annpaths_list_path=ann_path_list\n    )\n    convert_xmls_to_cocojson(\n        annotation_paths=ann_paths,\n        label2id=label2id,\n        output_jsonpath=output,\n    )","e34f3e23":"convert_to_coco()","623b0610":"convert_to_coco(ann_path_list='\/kaggle\/working\/val.txt', labels='\/kaggle\/working\/labels.txt', output='\/kaggle\/working\/val_output.json')","1bd57d5c":"convert_to_coco(ann_path_list='\/kaggle\/working\/test.txt', labels='\/kaggle\/working\/labels.txt', output='\/kaggle\/working\/test_output.json')","e1d3b2dc":"from mmcv import Config\ncfg = Config.fromfile('\/kaggle\/working\/mmdetection\/configs\/dcn\/cascade_rcnn_r101_fpn_dconv_c3-c5_1x_coco.py')","7289b606":"from mmdet.apis import set_random_seed\n\ncfg.dataset_type = 'CocoDataset'\ncfg.classes = '\/kaggle\/working\/labels.txt'\ncfg.data_root = '\/kaggle\/working'\n\nfor head in cfg.model.roi_head.bbox_head:\n    head.num_classes = 1\n\ncfg.data.test.type = 'CocoDataset'\ncfg.data.test.classes = 'labels.txt'\ncfg.data.test.data_root = '\/kaggle\/working'\ncfg.data.test.ann_file = 'test_output.json'\ncfg.data.test.img_prefix = ''\n\ncfg.data.train.type = 'CocoDataset'\ncfg.data.train.data_root = '\/kaggle\/working'\ncfg.data.train.ann_file = 'output.json'\ncfg.data.train.img_prefix = ''\ncfg.data.train.classes = 'labels.txt'\n\ncfg.data.val.type = 'CocoDataset'\ncfg.data.val.data_root = '\/kaggle\/working'\ncfg.data.val.ann_file = 'val_output.json'\ncfg.data.val.img_prefix = ''\ncfg.data.val.classes = 'labels.txt'\n\nalbu_train_transforms = [\n    dict(type='ShiftScaleRotate', shift_limit=0.0625,\n         scale_limit=0.15, rotate_limit=15, p=0.4),\n    dict(type='RandomBrightnessContrast', brightness_limit=0.2,\n         contrast_limit=0.2, p=0.5),\n    dict(type='IAAAffine', shear=(-10.0, 10.0), p=0.4),\n#     dict(type='MixUp', p=0.2, lambd=0.5),\n#     dict(type=\"Blur\", p=1.0, blur_limit=7),\n#     dict(type='CLAHE', p=0.5),\n#     dict(type='Equalize', mode='cv', p=0.4),\n    dict(\n        type=\"OneOf\",\n        transforms=[\n            dict(type=\"GaussianBlur\", p=1.0, blur_limit=7),\n            dict(type=\"MedianBlur\", p=1.0, blur_limit=7),\n        ],\n        p=0.4,\n    ),]\n\ncfg.train_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(type='Resize', img_scale=(512, 512), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(\n        type='Albu',\n        transforms=albu_train_transforms,\n        bbox_params=dict(\n        type='BboxParams',\n        format='coco',\n        label_fields=['gt_labels'],\n        min_visibility=0.0,\n        filter_lost_elements=True),\n        keymap=dict(img='image', gt_bboxes='bboxes'),\n        update_pad_shape=False,\n        skip_img_without_anno=True),\n    dict(\n        type='Normalize',\n        mean=[123.675, 116.28, 103.53],\n        std=[58.395, 57.12, 57.375],\n        to_rgb=True),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n]\n\ncfg.test_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(512, 512),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n#             dict(type='RandomFlip'),\n            dict(\n                type='Normalize',\n                mean=[123.675, 116.28, 103.53],\n                std=[58.395, 57.12, 57.375],\n                to_rgb=True),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img'])\n        ])\n]\n\ncfg.load_from = '..\/input\/dcn-checkpoint\/cascade_rcnn_r101_fpn_dconv_c3-c5_1x_coco_20200203-3b2f0594.pth'\n\ncfg.work_dir = '\/kaggle\/working\/model_output'\n\ncfg.optimizer.lr = 0.02 \/ 8\ncfg.lr_config = dict(\n    policy='CosineAnnealing', \n    by_epoch=False,\n    warmup='linear', \n    warmup_iters=500, \n    warmup_ratio=0.001,\n    min_lr=1e-07)\n\ncfg.data.samples_per_gpu = 4\ncfg.data.workers_per_gpu = 2\n\ncfg.evaluation.metric = 'bbox'\ncfg.evaluation.interval = 4\n\ncfg.checkpoint_config.interval = 12\ncfg.runner.max_epochs = 12\ncfg.log_config.interval = 50\n\ncfg.seed = 0\nset_random_seed(0, deterministic=False)\ncfg.gpu_ids = range(1)\n\nprint(f'Config:\\n{cfg.pretty_text}')","e2cf8aa1":"datasets = [build_dataset(cfg.data.train)]\nmodel = build_detector(\n cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\nmodel.CLASSES = datasets[0].CLASSES\n\nmmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\ntrain_detector(model, datasets, cfg, distributed=False, validate=True)","9d7f9a41":"test_ints = []\nfor file in test_files:\n    test_ints.append(os.path.basename(file).split('.')[0].split('t')[1])","f8c8e2b5":"model = init_detector(cfg, '\/kaggle\/working\/model_output\/epoch_12.pth')\nfor i in range(len(test_ints)):\n    img = mmcv.imread('..\/input\/ship-detection\/images\/boat' + test_ints[i] + '.png')\n    result = inference_detector(model, img)\n    show_result_pyplot(model, img, result)","8cca439f":"# **Install MMDetection and Import Libraries**","fa062419":"# **Visualized Inferences on Test Set**","02fcfa23":"# **Training Model**","57814d22":"# **Preprocess Data**","5d80b459":"# **Building Model**"}}