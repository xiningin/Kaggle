{"cell_type":{"afa8a99f":"code","c8b58eb4":"code","d44f10ed":"code","31bb4485":"code","2f6b86b7":"code","b4497035":"code","44da0818":"code","e856dd62":"code","b2592acd":"code","5b6d4c13":"code","a72332d5":"code","e720f724":"code","f8736e88":"code","e833d780":"code","1b3fbe67":"code","23291b7f":"code","a0356715":"code","6d3f96f2":"code","b35aa259":"code","f96e39d2":"code","513309fc":"code","94823280":"code","85b453f4":"code","cd0d01ea":"code","e6e3dbf7":"code","dd2ddc1e":"code","22d68ad1":"code","2aa83ece":"code","8de6cecf":"markdown","94948e63":"markdown","c09f2d13":"markdown","b6c0bc15":"markdown","692a68d7":"markdown","07982fad":"markdown","1b893c7d":"markdown","6d41df3a":"markdown","0964b632":"markdown","4aada790":"markdown","f8f4b167":"markdown","041a1e58":"markdown","5989b376":"markdown","38ca818d":"markdown","dfbb77ed":"markdown","d721d921":"markdown","ad4ae80c":"markdown","57869ff5":"markdown","9c4666aa":"markdown","ed3a07eb":"markdown","d2b1f6d8":"markdown","ec1ecd0f":"markdown","63689fce":"markdown","5d1596cb":"markdown","c8525884":"markdown","f2871d7d":"markdown","af8447f1":"markdown","ab677661":"markdown"},"source":{"afa8a99f":"# Data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Data viz\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split\n\n# Classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Regression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\n\n\nnp.warnings.filterwarnings('ignore')","c8b58eb4":"gam = pd.read_csv('..\/input\/arcade-game-stats\/GameStats.csv')\ndisplay(gam)\nprint(gam.info())\nprint(\"\\n\", gam.describe())\nprint(\"\\n\", gam.shape)\nsns.heatmap(gam.isnull())","d44f10ed":"date = gam.sort_values(by=['Date'])\nsns.scatterplot(x='Date', y='Score', data=date)\nplt.show()","31bb4485":"del gam['Date']","2f6b86b7":"gam['Level']","b4497035":"gam['Level'] = gam['Level'].str.lstrip('Level_')\ngam['Level'] = gam['Level'].astype(int)","44da0818":"gam['ElapsedTime'] = gam['ElapsedTime']*4","e856dd62":"gam.info()\n\n# separating numbers and booleans\nnumbers = gam.select_dtypes(['float64', 'int64']).columns\nbooleans = gam.select_dtypes('bool').columns","b2592acd":"gam[numbers].hist(figsize=(20,10), edgecolor='black', color='aliceblue', bins=10)\nplt.show()\n\ndef todf(column):\n    df = pd.DataFrame(pd.cut(gam[column], bins=10).value_counts()).reset_index()\n    df = df.rename(columns={'index': column + ' bins', column: column + ' count'})\n    df = df.sort_values(by=[column + ' bins']).reset_index()\n    return df\n\nlevel = todf('Level')\nnumblocks = todf('NumBlocks')\nElapsedTime = todf('ElapsedTime')\nscore = todf('Score')\naccuracy = todf('Accuracy')\n\ndisplay(pd.concat([level,numblocks, ElapsedTime, score, accuracy], axis=1))\ndisplay(gam[numbers].describe())","5b6d4c13":"sns.countplot('IsWin', data=gam[booleans], palette='Set3')\nplt.show()\n\nwin = pd.DataFrame(gam[booleans].value_counts()).reset_index().rename(columns={0:'Count'})\nwin['%'] = np.around(win['Count']\/win['Count'].sum() * 100, 2)\nwin","a72332d5":"gam.groupby('IsWin')[numbers].agg(['mean', 'median'])","e720f724":"fig, ax = plt.subplots(2,3, figsize=(20,10))\nsns.barplot(x='IsWin', y='Level', data=gam, ax=ax[0,0], palette='Set3', ci=None)\nsns.barplot(x='IsWin', y='NumBlocks', data=gam, ax=ax[0,1], palette='Set3', ci=None)\nsns.barplot(x='IsWin', y='ElapsedTime', data=gam, ax=ax[0,2], palette='Set3', ci=None)\nsns.barplot(x='IsWin', y='Score', data=gam, ax=ax[1,0], palette='Set3', ci=None)\nsns.barplot(x='IsWin', y='Accuracy', data=gam, ax=ax[1,1], palette='Set3', ci=None)\n\ndisplay(gam.groupby('IsWin')[numbers].agg(['mean', 'median']))\n\n\nplt.show()","f8736e88":"fig, ax = plt.subplots(figsize=(20,8))\n\n# Setting the score bins\ngam['Score bins'] = pd.cut(gam['Score'], bins=20)\n\nsns.countplot(x='Score bins', data=gam, hue='IsWin', ax=ax, palette='Set3')\norder = gam['Score bins'].value_counts().sort_index().keys()\nax.set_xticklabels(order, rotation=90)\n\n# Getting the win proportion\ngroup = gam.groupby(['IsWin', 'Score bins']).size().reset_index()\ngroupf = group[group['IsWin']==False]\ngroupt = group[group['IsWin']==True]\n\ndf = pd.merge(groupf, groupt, on='Score bins', suffixes=[' False', ' True'])\ndf['Win Proportion %'] = np.around((df['0 True'] \/ (df['0 False'] + df['0 True'])) * 100, 2)\ndf = df.drop(['IsWin False', 'IsWin True'], axis=1)\ndf['Score bins'] = df['Score bins'].astype(str)\n\nax.plot(df['Score bins'], df['Win Proportion %'], label = 'Win proportion')\nax.set_xticklabels(order, rotation=90)\nax.legend()\n\nplt.show()\n\ndisplay(gam.groupby(['IsWin', 'Score bins']).size())\ndisplay(df)","e833d780":"display(gam[gam['Score']<5355].describe())\ndisplay(gam[gam['Score']>5355].describe())","1b3fbe67":"sns.heatmap(gam[numbers].corr(), vmin=-1, vmax=1, cmap=sns.diverging_palette(20, 220, as_cmap=True), annot=True)\nplt.show()","23291b7f":"fig, ax = plt.subplots(1,2, figsize=(17,5))\n\nsns.regplot(x='NumBlocks', y='Score', data=gam, ax=ax[0], marker=',', color='teal')\nsns.regplot(x='Accuracy', y='Score', data=gam, ax=ax[1], marker=',', color='teal')","a0356715":"fig, ax = plt.subplots(figsize=(10,5))\n\nsns.regplot(x='NumBlocks', y='Score', data=gam[gam['NumBlocks']<100], ax=ax, color='teal')\nsns.jointplot(x='Accuracy', y='Score', data=gam[gam['Accuracy']>0.15], kind='hex', gridsize=20)","6d3f96f2":"features = ['Level', 'NumBlocks', 'ElapsedTime', 'Score', 'Accuracy','IsWin']\n\ngam_model = gam[features]\ngam_model_X = gam_model.iloc[:,:5].values\ngam_model_Y = gam_model.iloc[:,5].values\n\nX_train, X_test, y_train, y_test = train_test_split(gam_model_X, gam_model_Y, random_state=0)","b35aa259":"# knn\n\nclf = KNeighborsClassifier(n_neighbors=4)\nclf.fit(X_train, y_train)\nprint(\"training set score : {:.2f}\".format(clf.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(clf.score(X_test, y_test)))","f96e39d2":"# Logistic Regression\n\nlr = LogisticRegression().fit(X_train, y_train)\nprint(\"training set score : {:.2f}\".format(lr.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(lr.score(X_test, y_test)))","513309fc":"# Linear SVC\n\nsvc = LinearSVC(C=150).fit(X_train, y_train)\nprint(\"training set score : {:.2f}\".format(svc.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(svc.score(X_test, y_test)))","94823280":"# Decision Tree Classifier\n\ntree = DecisionTreeClassifier(max_depth=11, random_state=0).fit(X_train, y_train)\nprint(\"training set score : {:.2f}\".format(tree.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(tree.score(X_test, y_test)))\n\nprint(\"feature importances:\")\nfeature_importance = pd.DataFrame(gam_model.iloc[:,:5].keys(), tree.feature_importances_)\nprint(feature_importance.sort_index(ascending=False))","85b453f4":"features = ['Level', 'NumBlocks', 'ElapsedTime', 'Score', 'Accuracy']\ngam_model[features]\ngam_model_X = gam_model.loc[:,['Level', 'NumBlocks', 'ElapsedTime', 'Accuracy']].values\ngam_model_Y = gam_model.loc[:,['Score']].values\n\nX_train, X_test, y_train, y_test = train_test_split(gam_model_X, gam_model_Y, random_state=0)","cd0d01ea":"# KNN Regressor\n\nreg = KNeighborsRegressor(n_neighbors = 5).fit(X_train, y_train)\nprint(\"training set score : {:.2f}\".format(reg.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(reg.score(X_test, y_test)))","e6e3dbf7":"# Linear regression: least squares\n\nlr = LinearRegression().fit(X_train, y_train)\nprint(\"training set score : {:.2f}\".format(lr.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(lr.score(X_test, y_test)))","dd2ddc1e":"# Ridge\n\nridge = Ridge().fit(X_train, y_train)\nprint(\"training set score : {:.2f}\".format(ridge.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(ridge.score(X_test, y_test)))","22d68ad1":"# Lasso\n\nlasso = Lasso().fit(X_train, y_train)\n\nprint(\"training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\nprint(\"Number of features used:\", np.sum(lasso.coef_ != 0))","2aa83ece":"tree = DecisionTreeRegressor(max_depth=6, random_state=0).fit(X_train, y_train)\nprint(\"training set score : {:.2f}\".format(tree.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(tree.score(X_test, y_test)))\n\nprint(\"feature importances:\")\nfeature_importance = pd.DataFrame(['Level', 'NumBlocks', 'ElapsedTime', 'Accuracy'], tree.feature_importances_)\nprint(feature_importance.sort_index(ascending=False))","8de6cecf":"## 1.1 Date\nSince is a bot which is playing the game and it doesn't learn, i doubt that this field could be meaningfull. Let's graph time vs score to see if the bot gets better or not. If the field doesn't show to be meaningfull is going to be dropped","94948e63":"- from the table we can conclude that is a matter of distribution, having just 200 samples from 5355 and onwards, this is the influence of outliers.","c09f2d13":"## 1.4 ElapsedTime\nSince the game was played at 4x speed, we will multiply this value by 4 to make it more human understandable","b6c0bc15":"let's remove that \"Level_\" part and change the data type to integer","692a68d7":"- So we do see the relationship between the variables: the higher the accuracy and number of blocks, the higher the score.","07982fad":"# 1. Data loading and data cleaning","1b893c7d":"from the above tables and graph we can start getting some insights on how we can start cleaning the data.\n\nThe most important thing to remark right now is that we don't have any null values.\n\nLets go feature by feature to clean it.","6d41df3a":"- We see in the graph how the bot losses as the score increases\n- The proportion of wins gets bigger as the score increases.\n- Something happens from score 5355 till score 11305, where, even though the score keeps increasing, there are 0 wins. **We'll check that in section 3.1**","0964b632":"- We have balanced data! (fairly distributed) an excelent new for our analysis. This will make easier interpretation and model building","4aada790":"## 1.2 Level\nThe info function shows us that the \"Level\" feature is of type Object when it should be an ineger representing the level of difficulty","f8f4b167":"## 2.1 Numbers","041a1e58":"## 3.1 what happens from score 5355?","5989b376":"# 4. How do we get a high score?\nSo we know that we need a high score to win, but how can we get it?","38ca818d":"If we focus our attention on \"Score\", we can extract two relevant relationships: Score-NumBlocs and Score-Accuracy","dfbb77ed":"- The model that did the best work was KNN with a test accuracy of 0.99\n- Decision tree also did a great job  with a test accuracy of 0.98. It also set 'Score' and 'NumBlocks' as the most important features with 0.622123 and 0.211470 respectively. Score importance in the model reflects the conclusions of the previous analysis","d721d921":"# 5. Classification models: Can we predict wins?\n\nLet's try different models and choose the best one","ad4ae80c":"## 1.3 NumBlocks & IsWin\nBoth features have their correct data type, int and bool respectively","57869ff5":"**Level**:\n- The values are fairly distributed through the sample.\n- Mean and median are really close together, which means that the distributions is not skewed.\n- The distribution is fairly distributed so the interquantile range doesn't make any sense. In this case we can say that the distribution is best represented from the minimum value (378) to the maximum value (715)\n\n**NumBlock**:\n- The first five bins resemble a normal distribution, then all the next bins are empty until we find an outlier in the last bin with 400 samples.\n- The mean is a bit higher than the median, which is the result of the right outlier pulling the distirbution to the right. The effect, though, is not significant\n- Taking the outliers out, most of the values are found between 28 and 50 (IQR)\n\n**ElapsedTime**\n- Here we do have an outlier that distortionates significally the distribution, locating most of the values in the first bin (6809) leaving the middle bins empty until the last one where there are only 5 samples. **This five samples could be an error and are probably going to be dropped**\n- We see how big the mean is in relation to the median, which further reflect the effect of the outlier\n- Taking the outliers out, most of the values are located between 3.212832e+01 and 1.163324e+02\t(IQR)\n\n**Score**\n- The distribution is skewed to the right, reflected by the mean being bigger than the median. Still, the values are not far apart and we can keep the outliers\n- Taking extreme values out, most of the values are between 1150 and 3550\n\n**Accuracy**\n- Bit similar to ElapsedTime but the opposite, we have 16 samples in the first bin and the remaining in the last 3. The mean is lower than the median, which reflect this phenomenon. The so small scale of the values makes it hard to recognize the otliers with the mean and median, but stil is significant\n- Taking extreme values out, most of values are located between 0.325245 and 0.374960","9c4666aa":"# 2. Desctriptive analysis\nHaving all the correct data types and cleaned data we can begin with our descriptive analysis.","ed3a07eb":"As thought, the bot doesn't show any trend. Let's drop the 'Date' column","d2b1f6d8":"## Score & Accuracy\nBoth features have their correct data type, int and float respectively","ec1ecd0f":"- KNN did the best job in this section again with a test score of 0.93\n- Decision tree Regressor also did a good job with a test score of 0.91. The model gives elapsedtime and Numlocks as the most import features in the model","63689fce":"- We have here two interesting plots showing a proportion of 'True' being bigger than 'False': ElapsedTime and Score. But, if we recall section 2.1, we know how 'ElapsedTime is influenced by the outliers. Then we are going to set 'Score' as a the most determinant factor to Win, which makes sense in the context of gaming. Let's dig deeper here\n","5d1596cb":"Let's take outliers out and use density hexagons onaccuracy to better represent the relationship","c8525884":"# Arcade Game Stats Data Analysis \/ Blockbreaker\n\n## Intro \n\nBlockbreaker is a classic arcade game with the object of the game being for the player to break as many blocks as they can without causing the ball to fall.\n\n## About the data set\n\nThe author is in the process of creating a blockbreaker-like game, in which the jumping-off point is the \"Block Breaker\" section of the Udemy course, Complete C# Unity Developer 2D: Learn to Code Making Games\n\nAfter making lots of levels, the author needed to sort them by difficulty. How does one measure the difficulty of a level? A first-cut solution is\nto make an auto-play bot that is not perfect, and see how well the bot does on each level, using thousands of trials.\n\n## Purpose of the notebook\n\nThe purpose of this notebook is to analyse the data provided by the game and extract some insights about the functionality of the game. What makes the game hard? is there a factor that might help a player win? can we predict the outcome of the game based on the data? valuable insight can be useful to make the game a challenging one but yet a fun one to play.\n\n## Features of the dataset\n\n- Date: date and time the game was auto-played\n- Level: the name of the level (the 3-digit number is an estimate of the difficulty from a previous run, no longer valid after tweaking)\n- NumBlocks: how many blocks have to be broken to win the level\n- IsWin: True if autoplay broke all the blocks, False if the ball fell past the paddle.\n- ElapsedTime: Seconds until either won or lost (game is played at 4x speed, so multiply by 4 to get an estimate of how long a human might play it)\n- Score: total score when the game was won or lost\n- Accuracy: the autoplay is tuned with a randomly-chosen accuracy. Higher numbers are more likely to win.","f2871d7d":"## 2.2 Boolean","af8447f1":"# 6. Regression models: Can we predict score?\n","ab677661":"# 3. So what does it take to win?\n\nlet's find out"}}