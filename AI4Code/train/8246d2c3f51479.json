{"cell_type":{"f0a29f75":"code","73912890":"code","9760ea43":"code","8de5dd59":"code","271d88ec":"code","0ea655a2":"code","f0f40f4a":"code","c8734cba":"code","f94f87f6":"code","8b3d954d":"code","18d159d7":"code","c464dc39":"code","59395b46":"code","0b7924bf":"code","1915149e":"code","0f9c1793":"code","79c2d038":"code","d588eaab":"code","683766c3":"code","fab17997":"code","fbf586d1":"code","31429622":"code","74214e49":"code","9d39f0c5":"code","d6c9c29a":"markdown","7448569d":"markdown","6bb18645":"markdown","e36b9c07":"markdown","a8384a2e":"markdown","ebf608aa":"markdown","4f30eaa6":"markdown","cfbe8445":"markdown","19da6c6f":"markdown"},"source":{"f0a29f75":"import pandas as pd\nimport numpy as np\nimport scipy as sc\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\n\nnp.random.seed(2)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, mean_squared_error\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.applications import vgg16, inception_v3, resnet50\nfrom tensorflow.keras import backend\n\nsns.set(style='white', context='notebook', palette='deep')","73912890":"def add_one_to_one_correlation_line(ax, min_factor=1, max_factor=1, **plot_kwargs):\n    lim_min, lim_max = pd.DataFrame([ax.get_ylim(), ax.get_xlim()]).agg({0: 'min', 1: 'max'})\n    lim_min *= min_factor\n    lim_max *= max_factor\n    plot_kwargs_internal = dict(color='grey', ls='--')\n    plot_kwargs_internal.update(plot_kwargs)\n    ax.plot([lim_min, lim_max], [lim_min, lim_max], **plot_kwargs_internal)\n    ax.set_ylim([lim_min, lim_max])\n    ax.set_xlim([lim_min, lim_max])\n","9760ea43":"# Load the data\ndf = pd.read_csv(\"..\/input\/crowd-counting\/labels.csv\")","8de5dd59":"# Map each id to its appropriate file name\ndf['image_name'] = df['id'].map('seq_{:06d}.jpg'.format)","271d88ec":"df.describe()","0ea655a2":"df['count'].hist(bins=30);","f0f40f4a":"# Setup some constants\nsize = 224\nbatch_size = 64","c8734cba":"# ImageDataGenerator - with defined augmentaions\ndatagen = ImageDataGenerator(\n    rescale=1.\/255,  # Rescale the pixels to [0,1]. This seems to work well with pretrained models.\n    featurewise_center=False,  # set input mean to 0 over the dataset\n    samplewise_center=False,  # set each sample mean to 0\n    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n    samplewise_std_normalization=False,  # divide each input by its std\n    zca_whitening=False,  # apply ZCA whitening\n#     rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n#     zoom_range = 0.2, # Randomly zoom image \n#     width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n#     height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n    horizontal_flip=False,  # randomly flip images\n    vertical_flip=False,\n    validation_split=0.2,  # 20% of data randomly assigned to validation\n    \n    # This one is important:\n    preprocessing_function=resnet50.preprocess_input,  # Whenever working with a pretrained model, it is said to be essential to use its provided preprocess\n)","f94f87f6":"flow_params = dict(\n    dataframe=df,\n    directory='..\/input\/crowd-counting\/frames\/frames',\n    x_col=\"image_name\",\n    y_col=\"count\",\n    weight_col=None,\n    target_size=(size, size),\n    color_mode='rgb',\n    class_mode=\"raw\",\n    batch_size=batch_size,\n    shuffle=True,\n    seed=0,\n)\n\n# The dataset is split to training and validation sets at this point\ntrain_generator = datagen.flow_from_dataframe(\n    subset='training',\n    **flow_params    \n)\nvalid_generator = datagen.flow_from_dataframe(\n    subset='validation',\n    **flow_params\n)","8b3d954d":"batch = next(train_generator)\nfig, axes = plt.subplots(4, 4, figsize=(14, 14))\naxes = axes.flatten()\nfor i in range(16):\n    ax = axes[i]\n    ax.imshow(batch[0][i])\n    ax.axis('off')\n    ax.set_title(batch[1][i])\nplt.tight_layout()\nplt.show()","18d159d7":"base_model = resnet50.ResNet50(\n    weights='imagenet',  # Load the pretrained weights, trained on the ImageNet dataset.\n    include_top=False,  # We don't include the fully-connected layer at the top of the network - we need to modify the top.\n    input_shape=(size, size, 3),  # 224x224 was the original size ResNet was trained on, so I decided to use this.\n    pooling='avg',  # A global average pooling layer will be added after the last convolutional block.\n)","c464dc39":"# base_model.summary()","59395b46":"# Here we change the top (the last parts) of the network.\nx = base_model.output  # Since we used pooling='avg', the output is of the pooling layer\nx = Dense(1024, activation='relu')(x)  # We add a single fully-connected layer\npredictions = Dense(1, activation='linear')(x)  # This is the new output layer - notice only 1 output, this will correspond to the number of people in the image","0b7924bf":"model = Model(inputs=base_model.input, outputs=predictions)","1915149e":"k = -7\nfor layer in model.layers[:k]:\n    layer.trainable = False\nprint('Trainable:')\nfor layer in model.layers[k:]:\n    print(layer.name)\n    layer.trainable = True","0f9c1793":"model.summary()","79c2d038":"# Define the optimizer - this function will iteratively improve parameters in order to minimise the loss. \n# The Adam optimization algorithm is an extension to stochastic gradient descent, which is usually more effective and fast.\noptimizer = Adam(\n    # The most important parameter is the learning rate - controls the amount that the weights are updated during eache round of training.\n    learning_rate=0.001,\n    # Additional parameters to play with:\n#     beta_1=0.9,\n#     beta_2=0.999,\n#     epsilon=1e-07,\n)","d588eaab":"# Compile the model\nmodel.compile(\n    optimizer=optimizer, \n    loss=\"mean_squared_error\",  # This is a classic regression score - the lower the better\n    metrics=['mean_absolute_error', 'mean_squared_error']\n)","683766c3":"# Set a learning rate annealer - to have a decreasing learning rate during the training to reach efficiently the global minimum of the loss function. \n# The LR is decreased dynamically when the score is not improved. This keeps the advantage of the fast computation time with a high LR at the start.\nlearning_rate_reduction = ReduceLROnPlateau(\n    monitor='val_mean_squared_error',  # Track the score on the validation set\n    patience=3,  # Number of epochs in which no improvement is seen.\n    verbose=1, \n    factor=0.2,  # Factor by which the LR is multiplied.\n    min_lr=0.000001  # Don't go below this value for LR.\n)","fab17997":"# Fit the model\nhistory = model.fit_generator(\n    generator=train_generator,\n    epochs=50,  # 50 epochs seems to have reached the minimal loss for this setup\n    validation_data=valid_generator,\n    verbose=2, \n    callbacks=[learning_rate_reduction],\n)\nprint('\\nDone.')","fbf586d1":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(1, 1, figsize=(10, 5))\nax.plot(history.history['loss'], color='b', label=\"Training loss\")\nax.plot(history.history['val_loss'], color='r', label=\"Validation loss\",axes =ax)\nax.set_ylim(top=np.max(history.history['val_loss'])*1.2, bottom=0)\nlegend = ax.legend(loc='best', shadow=True)","31429622":"# Predict on entire validation set, to be able to review the predictions manually\nvalid_generator.reset()\nall_labels = []\nall_pred = []\nfor i in range(len(valid_generator)):\n    x = next(valid_generator)\n    pred_i = model.predict(x[0])[:,0]\n    labels_i = x[1]\n    all_labels.append(labels_i)\n    all_pred.append(pred_i)\n#     print(np.shape(pred_i), np.shape(labels_i))\n\ncat_labels = np.concatenate(all_labels)\ncat_pred = np.concatenate(all_pred)","74214e49":"df_predictions = pd.DataFrame({'True values': cat_labels, 'Predicted values': cat_pred})\nax = df_predictions.plot.scatter('True values', 'Predicted values', alpha=0.5, s=14, figsize=(9,9))\nax.grid(axis='both')\nadd_one_to_one_correlation_line(ax)\nax.set_title('Validation')\n\nplt.show()","9d39f0c5":"mse = mean_squared_error(*df_predictions.T.values)\npearson_r = sc.stats.pearsonr(*df_predictions.T.values)[0]\n\nprint(f'MSE: {mse:.1f}\\nPearson r: {pearson_r:.1f}')","d6c9c29a":"# 3. CNN\n## 3.1 Load and modify the pretrained model","7448569d":"# 1. Introduction","6bb18645":"## 2.3 Load image data\nWe use the defined ImageDataGenerator to read the images using the dataframe we read earlier.","e36b9c07":"## 3.2 Set the optimizer and annealer","a8384a2e":"## 2.2 Setup data generator with optional augmentation ","ebf608aa":"In order to avoid overfitting problem, we need to expand artificially our handwritten digit dataset. We can make your existing dataset even larger. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit.\n\nFor example, the number is not centered \nThe scale is not the same (some who write with big\/small numbers)\nThe image is rotated...\n\nApproaches that alter the training data in ways that change the array representation while keeping the label the same are known as data augmentation techniques. Some popular augmentations people use are grayscales, horizontal flips, vertical flips, random crops, color jitters, translations, rotations, and much more. \n\nThe approaches can help avoid overfitting, but it is not clear that we *want* to add this extra variance in this specific problem. You can play with the optional augmentations below and see how they affect the results.","4f30eaa6":"# 2. Data preparation\n## 2.1 Load and review data","cfbe8445":"# 4. Evaluate the model\n## 4.1 Training and validation curves","19da6c6f":"I used the Keras implementation of ResNet50 - a convolutional neural network that is 50 layers deep.\n\nIt was initially applied to image recognition tasks, i.e. classification, but can be modified to be used for regression problems, like this case."}}