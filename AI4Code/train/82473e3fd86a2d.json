{"cell_type":{"a2168c81":"code","5ecf2acc":"code","9ea7e3be":"code","0473999b":"code","aaa23534":"code","eddfa1f5":"code","707779a2":"code","85e19ca7":"code","58274476":"code","29a0c197":"code","cf5b906c":"code","644567b5":"code","eb7d8e62":"code","5db047d9":"code","22925a1d":"code","cfce34a5":"code","574d1cf0":"code","2ef86351":"code","e39d6d8f":"code","f870f6c3":"code","a2b35e27":"code","6d336958":"code","dbd562bf":"code","87cc4d73":"markdown","71d16172":"markdown","cc445b62":"markdown","411c2c76":"markdown","d5f7dc83":"markdown","d893bbd6":"markdown","0cc284e3":"markdown","e2325cf5":"markdown","ad75e310":"markdown","1b7ca018":"markdown","dc7ee72f":"markdown","87adee0d":"markdown","c14bb37b":"markdown","975f4c0f":"markdown","6dae325a":"markdown","e7d3f27d":"markdown","31fe9646":"markdown","783f0b1d":"markdown","149d04d6":"markdown","21025af5":"markdown","20813f6d":"markdown","7c2fa6b4":"markdown","ce197bc2":"markdown","5ecd7d9a":"markdown","850b7ab8":"markdown","622ac65b":"markdown"},"source":{"a2168c81":"import plotly as py\n#import chart_studio.plotly as py\nimport cufflinks as cf\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\ncf.go_offline()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import fetch_openml\nmnist = fetch_openml('mnist_784', version=1)\nmnist.keys()","5ecf2acc":"X, y = mnist['data'], mnist['target']\nX.shape","9ea7e3be":"X[0].reshape(28,28)","0473999b":"#adding a bias term to each digit\nX_with_bias = np.c_[np.ones([len(X), 1]), X]\n#normalizing\nX_with_bias = X_with_bias\/255","aaa23534":"len(X)","eddfa1f5":"def categorical_to_num(y):\n    nums = np.zeros(y.size)\n    for i in range(len(np.unique(y))):\n        nums[np.where(y==np.unique(y)[i])] = i\n    return nums.astype(int)","707779a2":"def to_one_hot(y):\n    shape = (y.size, int(np.max(y) + 1))\n    rows = np.arange(y.size)\n    one_hot = np.zeros(shape)\n    one_hot[rows, y] = 1.\n    return one_hot","85e19ca7":"X_train, X_val, y_train, y_val = train_test_split(X_with_bias, y, train_size = 0.8, random_state = 42)\nprint(\"X_train size:\", X_train.shape, '\\n',\n    \"X_val size:\", X_val.shape, '\\n',\n    \"y_train size:\", y_train.shape, '\\n',\n    \"y_val size:\", y_val.shape, '\\n')","58274476":"#converting y validation set to one hot\nnums = categorical_to_num(y_val)\ny_val_one_hot = to_one_hot(nums)\ny_val_one_hot","29a0c197":"#converting y training set to one hot\nnums = categorical_to_num(y_train)\ny_train_one_hot = to_one_hot(nums)\ny_train_one_hot","cf5b906c":"def SoftmaxScore(x, theta):\n    score = x@theta\n    return score","644567b5":"x=np.linspace(-10, 10, 100)\nz=1\/(1+np.exp(-x))\npx.line(x=x, y=z,\n       labels={'x':'X', 'y':'Sigmoid(X)'}, title='Sigmoid Function')","eb7d8e62":"def SoftmaxFunction(score):\n    exps = np.exp(score)\n    exp_sums = np.sum(exps, axis=1, keepdims=True)\n    return exps \/ exp_sums","5db047d9":"def Prediction(p_hat):\n    y_hat = np.argmax(p_hat, axis = 1)\n    return y_hat","22925a1d":"def xentropyloss(y, p_hat):\n    epsilon = 1e-6\n    #we will add epsilon (really small number) to p_hat \n    #before taking the log to avoid p_hat being 0\n    loss = -np.mean(np.sum(np.log(p_hat + epsilon) * y,axis=1))\n    return loss","cfce34a5":"def GradientStep(p_hat, theta, y, x, lr, m):\n    cost = p_hat - y\n    gradients = 1\/m * x.T.dot(cost)\n    theta = theta - lr * gradients\n    return theta\n    ","574d1cf0":"lr = 0.5\nn_inputs = X_train.shape[1]\nm = 10000 #batch size\nn_outputs = len(np.unique(y))\nepochs = 5001\nnp.random.seed(2042)\ntheta = np.random.randn(n_inputs, n_outputs)\ncharts = {}","2ef86351":"image_theta = theta[:,7][:784].reshape(28, 28)\nimage_theta = (image_theta\/image_theta.max()) * 255\n\nplt.imshow(image_theta, cmap = 'binary')\nplt.axis('off')\nplt.show()","e39d6d8f":"for epoch in range(epochs):\n    score = SoftmaxScore(X_train[:m], theta)\n    p_hat = SoftmaxFunction(score)\n    if epoch % 50 == 0:\n        loss = xentropyloss(y_train_one_hot[:m], p_hat)\n        print(epoch, ':', loss )\n        charts[str(epoch)] = loss\n    theta = GradientStep(p_hat, theta, y_train_one_hot[:m], X_train[:m], lr, m)","f870f6c3":"charts.pop('0')\nfig = px.line(charts.values(), title='Training Loss', labels={'x':'Epoch', 'y':'Loss'})\nfig.update_layout(xaxis_title='Epoch (Each One is 50)', yaxis_title='Loss')","a2b35e27":"score = SoftmaxScore(X_val, theta)\np_hat = SoftmaxFunction(score)\npred = Prediction(p_hat)\n\naccuracy = np.mean(pred == categorical_to_num(y_val))\naccuracy","6d336958":"image_theta = theta[:,7][:784].reshape(28, 28)\nimage_theta = (image_theta\/image_theta.max()) * 255\n\nplt.imshow(image_theta, cmap = 'binary')\nplt.axis('off')\nplt.show()","dbd562bf":"softmax_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10, max_iter = 5001)\nsoftmax_reg.fit(X_train[:10000], y_train[:10000])\nval_predictions = softmax_reg.predict(X_val)\nval_score = accuracy_score(y_val, val_predictions)\nval_score","87cc4d73":"Here we create a function to convert the labels in their number form to a one-hot encoding (puts a one in the column of a row the same length as the number of possible options). This will become needed later when calculating loss and gradients.","71d16172":"As the labels come in the form of strings, here I create a function to convert them to numbers based on their index in a list of unique values.","cc445b62":"# Cross Entropy Cost Function","411c2c76":"Here are the equations that we will need for building out the softmax regressor:","d5f7dc83":"Here's a visualization of the weight vector used of the digit 7. As you can see it begins as random noise.","d893bbd6":"# Putting It All Together","0cc284e3":"To prepare our data, I'm starting by adding a bias term of 1 to each digit. This bias term is similar to a y-intercept (b) in the equation for a line $y = mx + b$. It allows for predictions to be adjusted without regard to variables.\n\nTo keep the calculations from becoming too large and not being able to compute, I divide all X values by 255 to start, as 255 is the maximum possible value. This is called normalizing.","e2325cf5":"   The goal of this notebook is to explain and implement Gradient Descent for Softmax Regression (without using Scikit-Learn).\n\n   Logistic regression is using a probability between 0 and 1 (based on a sigmoid function) to identify which of two categories a probability falls into (eg. this is the number 5 or is not the number 5). \n   \n   Softmax regression is the same thing as logistic regression, but it uses more than two possible outcomes (or output classes), outputs a probability between 0 and 1 for each outcome, and then based on which of those outcomes has the highest probability (eg. \"I am 80% sure that I am looking at 4. I am going to say it is a 4.\") . This entails that the classes that we are classifying be exclusive (eg. the single digit number we are identifying is not both an 8 and a 0, it is one or the other).","ad75e310":"# Softmax Score","1b7ca018":"This cost function identifies how wrong a prediction was using the natural log of its probability, and multiplies by 1 (if that class was the correct one), then all of these distances get averaged together for a collective distance from the correct predictions.\n\n${J}(\\mathbf{\\Theta})$ represents the cost over the weight vector of the entire batch\n\n$m$ is the number of instances in the batch\n\n$i$ is one such instance\n\n$k$ is one possible class\n\n$K$ is a set of all of the classes\n\n$log$ is the natural log (again the base is $e$ because it simplifies our math, because the slope of the graph is constant)\n\n$y_k^{(i)}$ is either a 0 (if the handwritten digit was not of that class) or 1 (if it was part of that class)\n\nBonus: Logarithms are useful in this situation because they \"undo\" exponential equations (because they are the inverse). So because we got $\\hat{p}$ using normalizing exponentials, taking the natural log helps us to correlate gradients more closely with larger mistakes in prediction.\n\n\n$${J}(\\mathbf{\\Theta}) = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K}y_k^{(i)} log(\\hat{p}_k^i)$$\n\nHere's how we can implement it:","dc7ee72f":"Here's a plot of how training optimizes the loss function.","87adee0d":"# **Introduction**","c14bb37b":"As you can see in the shape of our data, X, there are 70,000 example handwritten digits, and each one is made up of 784 pixels (a 28 x 28 box). \n\nThe value for each pixel, as seen below for the first example, can range from 0 (white) to 255 (black) in grayscale. ","975f4c0f":"Now that we have all of our equations in place, we will put it all together!","6dae325a":"# Softmax Function","e7d3f27d":"# Softmax Regression Classifier Prediction","31fe9646":"We are going to work on the MNIST Data set as the classes are exclusive (they are digits 0-9 and they are only one digit at a time, not 5 and 3 both for example).","783f0b1d":"The $\\mathbf{\\sigma}$ function puts the scores we just got and puts them on a limited range of probabilities (between 0 and 1). This results in an s shaped graph (as shown above), that gets infinitely close to 0 on the left side, and infinitely close to 1 on the right side (being able to visualize these probabilities helps with intuition).\n\n$\\hat{p}$ Shows that we are aiming to define probabilities.\n\nThe $i$ in $\\mathbf{\\sigma} {(\\overrightarrow z)}_i$ lets you know that we are doing this sigmoid function for just one example (instance) of a handwritten digit.\n\n$\\overrightarrow z$ are the logits (the dot product of the weights $\\mathbf{\\theta}$ and instances $\\mathbf{x}$).\n\n$e^{zi}$ Is Euler's constant to the power of the score $z$ we just found for an individual class $i$ (one possible digit the instance could be). $e^{zj}$ Is the same idea, but we are taking the sum of all the possible digits it could be (0-9) so we use a different character $j$ to represent it.\n\nThis is called taking the exponent of a number. In this case that number is the logit we computed in the softmax score function.\n\nThe reason we use Euler's constant here is because it allows us to focus on the exponent's value itself (because in exponential equations the derivative of Euler's constant to any power is itself).\n\n$\\sum_{j=1}^{K}$ is the sum of the exponent of each logit ($e$ to the power of each logit over all classes for all handwritten digits.\n\nThis is referred to as normalizing the exponent of the score.\n\n$$\\hat{p} = \\mathbf{\\sigma} {(\\overrightarrow z)}_i = \\frac{e^{zi}}{{\\sum_{j=1}^{K} e^{zj}}}$$\n\nHere's how we will implement it (broken up so that it's not as computationally expensive):","149d04d6":"For the sake of comparing to a baseline. Notice that the score that is achievable with the pre-built Scikit-Learn softmax regressor is very close to our accuracy score.","21025af5":"# Cross Entropy Gradient Vector","20813f6d":"It's still pretty fuzzy, but you can make out the figure of a 7 slightly.","7c2fa6b4":"This is the derivative of the cost function that we just defined above. This will allow us to perform gradient descent in order to optimize the cost function (lower the amount of error). \n\n$\\mathbf{\\nabla}$ (nabla) Is the math symbol for a gradient.\n\n$\\sum_{i=1}^{m}$ Takes the sum of the difference between the ground truth and the probability for all instances in the batch.\n\nMultiplying by $\\frac{1}{m}$ will give us the average cost over the entire batch.\n\nBy multiplying by $\\mathbf{x}$ you effect each pixel represented in the handwritten digit.\n\n$$\\mathbf{\\nabla}_{\\theta_{k}} J(\\mathbf{\\Theta}) = \\frac{1}{m} \\sum_{i=1}^{m}(\\hat{p}_k^{(i)}-{y}_k^{(i)})\\mathbf{x}^{(i)} $$\n\nHere's how we can implement it (again, broken up so as to not be too computationally expensive):","ce197bc2":"**Imports**","5ecd7d9a":"We will split up the 70,000 example hand-written digits below into an 80%\/20% split between training and test data.","850b7ab8":"This equation is used for scoring probabilities. That means combining our prior or initial thought of the weight or importance that each pixel of a handwritten digit make it part of a certain class, with a new handwritten digit (\"based on what I think up front how much do I think that this new digit is a 4?\").\n\n$s_k$ is the score (logits) for class $k$. \n\n$\\mathbf{x}$ is the batch of handwritten digits you are supplying the model.\n\n$\\mathbf{\\theta}$ (theta) is the weight for the class ${k}$ that we are working with.\n\nThe $^T$ next to $\\mathbf{\\theta}$ shows that we are going to transpose it before taking the dot product of it and our handwritten digits $\\mathbf{x}$.\n\n\n\n$$s_k(\\mathbf{x}) = (\\mathbf{\\theta}^{(k)})^T\\mathbf{x}$$\n\nAs this is the dot product of $\\mathbf{\\theta}$ and $\\mathbf{x}$, we have to make sure that our dimensions match (the last dimension of the first matrix is the same as the first dimension of the second matrix). $\\mathbf{x}$ will be in the shape of (m, n_inputs) and $\\mathbf{\\theta}$ will be in the shape of (n_inputs, n_outputs).\n\nNotice how we implement it:","622ac65b":"This function states that the class with the highest probability (the digit that it is most likely to be) will be chosen as the prediction for a given handwritten digit.\n\n$\\hat{y}$ (or y hat) represents our final prediction of what handwritten an instance is.\n\n$$\\hat{y} = \\underset{k}{\\operatorname{argmax}} \\mathbf{\\sigma}(\\mathbf{s}(\\mathbf{x}))_k = \\underset{k}{\\operatorname{argmax}}s_k(\\mathbf{x}) = \\underset{k}{\\operatorname{argmax}} ((\\mathbf{\\theta}^{(k)})^T\\mathbf{x}) $$\n\nWe will implement it like this:"}}