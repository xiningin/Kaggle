{"cell_type":{"06af45d0":"code","2e2160c2":"code","e1c957ab":"code","b621dfc5":"code","8e23613a":"code","9daa6dda":"code","1f836140":"code","279acc25":"code","a6485d97":"code","ed9f98c0":"code","02269fbd":"code","c508a1d2":"code","bed6d850":"markdown","fcb8f37b":"markdown","0c5cc1dc":"markdown","c7fedbbc":"markdown","15aa30c3":"markdown","bf70260e":"markdown","8e95e11f":"markdown","43d6149c":"markdown","126c7d24":"markdown","af56a139":"markdown","6dc4b759":"markdown","aa9ae375":"markdown","ddf7cf31":"markdown","9b7ca2ac":"markdown","3a5a2bcb":"markdown","bd442f59":"markdown","73bc93e8":"markdown","74e221f9":"markdown"},"source":{"06af45d0":"# tensorflow\nimport tensorflow as tf\n\n# Keras modules\nimport keras\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import backend\n# from tensorflow.keras.applications import DenseNet --> Discarded due to too much memory usage\nfrom keras.applications import InceptionV3\nfrom keras.applications import InceptionResNetV2\nfrom keras.applications import MobileNet\nfrom keras.applications import ResNet50\nfrom keras.applications import VGG16\nfrom keras.applications import VGG19\nfrom keras.applications import Xception\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Conv2D\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.layers import Input\nfrom keras.layers import MaxPooling2D\nfrom keras.models import load_model\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.utils import Sequence\n\n# data processing modules\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n# metrics functions\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_recall_fscore_support\n\n# plotting\nimport matplotlib.pyplot as plt\n\n# image processing\nimport cv2\nfrom PIL import Image\nfrom imgaug import augmenters as iaa\n\n# python support libraries\nimport os\nimport datetime\nimport pickle\nfrom zipfile import ZipFile\nfrom collections import Iterable\nimport warnings\nimport itertools\nwarnings.filterwarnings(\"ignore\")","2e2160c2":"def get_classes(x):\n    \"\"\"\n    Transform the Target column from the train dataset into an array of classes found in the image\n    to be later used as a target variable\n    :param x: row value for column Target from apply function\n    :return: array of length 27 with 0 and 1\n    \"\"\"\n    return np.array([0 if str(i) not in str(x).split(' ') else 1 for i in range(28)])\n\n\ndef load_image(file_path, image_name, shape=(512, 512, 3)):\n    \"\"\"\n    Load image contained within a zip file\n    :param file_path:  (string) Path to image on disk\n    :param image_name: (string) Image name contained in zip file\n    :param shape:      (tuple) Shape of image to be outputed\n    :return:           (array) 3D of RGBY divided by 255\n    \"\"\"\n    # load images by channel\n    channel_list = list()\n    for c in ['red', 'green', 'blue', 'yellow']:\n        img = cv2.imread(file_path + '\/' + image_name + '_' + c + '.png', 0)\n        img = cv2.resize(img, dsize=(shape[0], shape[1]), interpolation=cv2.INTER_CUBIC)\n        img = np.array(img)\n        channel_list.append(img)\n    \n    # stack pixels of image\n    if shape[2] == 3:\n        image = np.stack((\n            channel_list[0]\/2 + channel_list[3]\/2, \n            channel_list[1]\/2 + channel_list[3]\/2, \n            channel_list[2]\n        ),-1)\n    else:\n        image = np.stack(channel_list, -1)\n\n    # normalize pixels range\n    image = np.divide(image, 255)\n\n    # return array with normalized colors\n    return image\n\n\ndef f1(y_true, y_pred):\n    \"\"\"\n    Calculate the f1 score given the true values and predictions\n    :param y_true: (array) true value array\n    :param y_pred: (array) predictions array\n    :return: (float) f1 score\n    \"\"\"\n    tp = backend.sum(backend.cast(y_true * y_pred, 'float'), axis=0)\n    fp = backend.sum(backend.cast((1 - y_true) * y_pred, 'float'), axis=0)\n    fn = backend.sum(backend.cast(y_true * (1 - y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + backend.epsilon())\n    r = tp \/ (tp + fn + backend.epsilon())\n\n    f1 = 2 * p * r \/ (p + r + backend.epsilon())\n    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n    return backend.mean(f1)\n\n\ndef focal_loss(gamma=2., alpha=.25):\n    \"\"\"\n    Function to use the focal loss function\n    source: https:\/\/github.com\/mkocabas\/focal-loss-keras\n    :param gamma: (float) gamma value\n    :param alpha: (float) alpha value\n    :return: (function) parameter adjusted focal loss function\n    \"\"\"\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        return -keras.backend.sum(alpha * keras.backend.pow(1. - pt_1, gamma) * keras.backend.log(pt_1))-keras.backend.sum((1-alpha) * keras.backend.pow( pt_0, gamma) * keras.backend.log(1. - pt_0))\n    return focal_loss_fixed\n\n\ndef get_weighted_loss(weights):\n    \"\"\"\n    Function to use the weighted loss function\n    source: https:\/\/stackoverflow.com\/questions\/48485870\/multi-label-classification-with-class-weights-in-keras\n    :param weights: (array) weights dictionary\n    :return: (function) parameter adjusted weighted loss function\n    \"\"\"\n    def weighted_loss(y_true, y_pred):\n        return keras.backend.mean((weights[:, 0] ** (1-y_true)) * (weights[:,1] ** (y_true)) * keras.backend.binary_crossentropy(y_true, y_pred), axis=-1)\n    return weighted_loss\n\n\ndef augment(image):\n    \"\"\"\n    Apply transformations to images\n    source: https:\/\/www.kaggle.com\/rejpalcz\/cnn-128x128x4-keras-from-scratch-lb-0-328\n    :param image:\n    :return:\n    \"\"\"\n    augment_img = iaa.Sequential([\n        iaa.OneOf([\n            # flip the image\n            iaa.Fliplr(0.5),\n            iaa.Flipud(0.5),\n\n            # random crops\n            iaa.Crop(percent=(0, 0.1)),\n\n            # Strengthen or weaken the contrast in each image\n            iaa.ContrastNormalization((0.75, 1.5)),\n\n            # Add gaussian noise\n            iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05 * 255), per_channel=0.5),\n\n            # Make some images brighter and some darker\n            iaa.Multiply((0.8, 1.2), per_channel=0.2),\n\n            # Apply affine transformations to each image.\n            # Scale\/zoom them, translate\/move them, rotate them and shear them.\n            iaa.Affine(\n                scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n                translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n                rotate=(-180, 180),\n                shear=(-8, 8)\n            )\n        ])], random_order=True)\n\n    image_aug = augment_img.augment_image(image)\n    return image_aug\n\n\ndef show_history(history):\n    fig, ax = plt.subplots(1, 3, figsize=(15,5))\n    ax[0].set_title('loss')\n    ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n    ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n    ax[1].set_title('f1')\n    ax[1].plot(history.epoch, history.history[\"f1\"], label=\"Train f1\")\n    ax[1].plot(history.epoch, history.history[\"val_f1\"], label=\"Validation f1\")\n    ax[2].set_title('acc')\n    ax[2].plot(history.epoch, history.history[\"acc\"], label=\"Train acc\")\n    ax[2].plot(history.epoch, history.history[\"val_acc\"], label=\"Validation acc\")\n    ax[0].legend()\n    ax[1].legend()\n    ax[2].legend()\n\n\ndef sequencial_model(input_shape, n_out):\n    # Initialising the CNN\n    model = Sequential()\n\n    # ##### LAYER 1\n    # model.add(BatchNormalization())\n    model.add(Conv2D(32, (3, 3), input_shape=input_shape, data_format='channels_last', activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.2))\n\n    # ##### LAYER 2\n    # model.add(BatchNormalization())\n    model.add(Conv2D(32, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.2))\n\n    # ##### FLATTENING\n    model.add(Flatten())\n\n    # ##### ANN\n    model.add(Dense(activation='relu', units=1024))\n    model.add(Dense(activation='relu', units=128))\n    model.add(Dense(activation='sigmoid', units=n_out))\n\n    return model\n\n\ndef vgg_model(input_shape, n_out):\n    model = VGG19(\n        include_top=False, weights='imagenet', input_shape=input_shape\n    )\n\n    input_tensor = Input(shape=input_shape)\n    bn = BatchNormalization()(input_tensor)\n    x = model(bn)\n    x = Conv2D(128, kernel_size=(1, 1), activation='relu')(x)\n    x = Flatten()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    output = Dense(n_out, activation='sigmoid')(x)\n    return Model(input_tensor, output)\n\n\ndef inception_res_net_model(input_shape, n_out):    \n    pretrain_model = InceptionResNetV2(\n        include_top=False, \n        weights='imagenet', \n        input_shape=input_shape\n    )    \n    \n    input_tensor = Input(shape=input_shape)\n    bn = BatchNormalization()(input_tensor)\n    x = pretrain_model(bn)\n    x = Conv2D(128, kernel_size=(1,1), activation='relu')(x)\n    x = Flatten()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    output = Dense(n_out, activation='sigmoid')(x)\n    model = Model(input_tensor, output)\n    \n    return model\n\n\ndef res_net_50(input_shape, n_out):    \n    pretrain_model = ResNet50(\n        include_top=False, \n        weights='imagenet', \n        input_shape=input_shape\n    )    \n    \n    input_tensor = Input(shape=input_shape)\n    bn = BatchNormalization()(input_tensor)\n    x = pretrain_model(bn)\n    x = Conv2D(128, kernel_size=(1, 1), activation='relu')(x)\n    x = Flatten()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    output = Dense(n_out, activation='sigmoid')(x)\n    model = Model(input_tensor, output)\n    \n    return model","e1c957ab":"class StratifiedMultiLabelKFold(object):\n    \"\"\"\n    Create a stratified k fold object capable of splitting the dataset in even parts according to \n    the multi labels contained within it\n    \"\"\"\n    def __init__(self, n_splits, shuffle=False, random_state=None):\n        \"\"\"\n        :param n_splits: (int) Number of folds. Must be at least 2\n        :param shuffle:  (bool) Whether to shuffle each stratification of the data before splitting into batches\n        :param random_state: (int, obj) If int, random_state is the seed used by the random number generator; \n                                        If RandomState instance, random_state is the random number generator; \n                                        If None, the random number generator is the RandomState instance used by np.random. \n                                        Used when shuffle == True\n        \"\"\"\n        assert type(n_splits) == int\n        assert n_splits >= 2\n        assert type(shuffle) == bool\n        assert random_state is None or type(random_state) == int or isinstance(random_state, np.random.RandomState)\n        \n        # save the variables\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = np.random.RandomState(random_state) if type(random_state) == int else random_state\n        \n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"\n        Get the amount of splits to be performed\n        :param X:      (obj) Always ignored, exists for compatibility\n        :param y:      (obj) Always ignored, exists for compatibility\n        :param groups: (obj) Always ignored, exists for compatibility\n        :return:       (int) number of splitting iterations in the cross-validator\n        \"\"\"\n        return self.n_splits\n    \n    def split(self, X, y, groups=None):\n        \"\"\"\n        Create a generator that return indexes of evenly split classes of multi label y\n        :param X:      (obj) Training data, where n_samples is the number of samples and n_features is the number of features\n        :param y:      (obj) The target variable for supervised learning problems. Stratification is done based on the y labels\n        :param groups: (obj) Always ignored, exists for compatibility\n        :yield: The training set indices for that split \/ The testing set indices for that split\n        \"\"\"\n        assert type(y) is np.ndarray\n        assert np.issubdtype(y.dtype, np.integer)\n        assert len(y.shape) == 2\n        assert y.shape[0] > self.n_splits\n        assert y.shape[1] > 1\n        \n        # create train data frame\n        df = pd.DataFrame(data=np.array([np.arange(y.shape[0]), np.empty(y.shape[0], dtype=object)]).T, columns=['X', 'Y'])\n        for i in range(y.shape[0]):\n            df.at[i, 'Y'] = y[i]\n        \n        # set the target labels\n        df['Target'] = df['Y'].apply(lambda x: ' '.join([str(i) for i in x if i == 1]))\n        \n        # do a value count of the target labels\n        vc = df['Target'].value_counts()\n        \n        # if shuffle\n        if self.shuffle:\n            for sp in range(self.n_splits):\n                # set the train and test lists\n                train = list()\n                test = list()\n\n                # for the higher represented classes\n                for target in vc[vc >= self.n_splits].index:\n                    # filter images\n                    images = df[df['Target'] == target]\n\n                    # shuffle the array \n                    indexes = images['X'].values\n                    if self.random_state is None:\n                        np.random.shuffle(indexes) \n                    else:\n                        self.random_state.shuffle(indexes)\n                        \n                    # select the first y_shape*(n_split-1)\/n_splits as train and the other y_shape\/n_splits as test\n                    i = int(np.round(images.shape[0]\/self.n_splits))\n                    i1 = indexes[:(self.n_splits - 1)*i]\n                    i2 = indexes[(self.n_splits - 1)*i:]\n                    if len(i1) > 0:\n                        train.append(i1)\n                    if len(i2) > 0:\n                        test.append(i2)\n\n                # filter the low represented classes\n                pool = df[df['Target'].isin(vc[vc < self.n_splits].index)]\n\n                # go through each class\n                for target in np.argsort(df['Y'].sum()):\n                    # filter pool\n                    images = pool[pool['Y'].apply(lambda x: x[target] == 1)]\n                    if images.shape[0] == 0:\n                        continue\n\n                    # shuffle the array \n                    indexes = images['X'].values\n                    if self.random_state is None:\n                        np.random.shuffle(indexes) \n                    else:\n                        self.random_state.shuffle(indexes)\n                        \n                    # select the first y_shape*(n_split-1)\/n_splits as train and the other y_shape\/n_splits as test\n                    i = int(np.round(images.shape[0]\/self.n_splits))\n                    i1 = indexes[:(self.n_splits - 1)*i]\n                    i2 = indexes[(self.n_splits - 1)*i:]\n                    if len(i1) > 0:\n                        train.append(i1)\n                    if len(i2) > 0:\n                        test.append(i2)\n\n                    # remove from pool\n                    pool = pool[~pool['X'].isin(images['X'])]\n\n                # stack the train and test indexes\n                train = np.concatenate(train)\n                test = np.concatenate(test)\n\n                # yield indexes\n                yield train, test\n        \n        # if not shuffle\n        else:\n            # set the train and test lists\n            dataset = [list() for i in range(self.n_splits)]\n            \n            # for the higher represented classes\n            for target in vc[vc > self.n_splits].index:\n                # filter images\n                images = df[df['Target'] == target]\n\n                # shuffle the array \n                indexes = images['X'].values\n                if self.random_state is None:\n                    np.random.shuffle(indexes) \n                else:\n                    self.random_state.shuffle(indexes)\n                \n                # calculate the size of indexes to split array\n                i = int(np.round(images.shape[0]\/self.n_splits))\n                \n                # go throw each split\n                for s in range(1, self.n_splits):\n                    # filter indexes by split size\n                    ds = indexes[(s - 1)*i:s*i]\n                    \n                    # If the array is not empty\n                    if len(ds) > 0:\n                        # add the indexes to the dataset part\n                        dataset[s - 1].append(ds)\n                \n                # for the final split add the rest of the dataset\n                ds = indexes[(self.n_splits - 1)*i:]\n                if len(ds) > 0:\n                    dataset[(self.n_splits - 1)].append(ds)\n                \n            # filter the low represented classes\n            pool = df[df['Target'].isin(vc[vc <= self.n_splits].index)]\n\n            # go through each class\n            for target in np.argsort(df['Y'].sum()):\n                # filter pool\n                images = pool[pool['Y'].apply(lambda x: x[target] == 1)]\n                if images.shape[0] == 0:\n                    continue\n\n                # shuffle the array \n                indexes = images['X'].values\n                if self.random_state is None:\n                    np.random.shuffle(indexes) \n                else:\n                    self.random_state.shuffle(indexes)\n                        \n                # calculate the size of indexes to split array\n                i = int(np.round(images.shape[0]\/self.n_splits))\n                \n                # go throw each split\n                for s in range(1, self.n_splits):\n                    # filter indexes by split size\n                    ds = indexes[(s - 1)*i:s*i]\n                    \n                    # If the array is not empty\n                    if len(ds) > 0:\n                        # add the indexes to the dataset part\n                        dataset[s - 1].append(ds)\n                \n                # for the final split add the rest of the dataset\n                ds = indexes[(self.n_splits - 1)*i:]\n                if len(ds) > 0:\n                    dataset[(self.n_splits - 1)].append(ds)\n\n                # remove from pool\n                pool = pool[~pool['X'].isin(images['X'])]\n            \n            # stack the datasets\n            f_dataset = [np.concatenate(d) for d in dataset]\n            \n            # go throw eac\n            for sp in range(self.n_splits):\n                # yield indexes\n                yield np.concatenate(f_dataset[:sp] + f_dataset[sp + 1:]), f_dataset[sp]\n\n    \nclass DataGenerator(keras.utils.Sequence):\n    \"\"\"\n    Extend the Sequence class from the keras.utils module to create a class\n    capable of loading the images from the zipfile in batches, resizing it and\n    selecting specific channels\n\n    source: https:\/\/stanford.edu\/~shervine\/blog\/keras-how-to-generate-data-on-the-fly\n    \"\"\"\n    def __init__(self, train_data, batch_size, file_path, shape, augment_flag=True):\n        assert isinstance(train_data, pd.DataFrame)\n        assert train_data.shape[0] > 0\n        assert batch_size > 0\n        assert (os.path.isfile(file_path) and '.zip' in file_path) or os.path.isdir(file_path)\n        assert isinstance(shape, Iterable)\n        assert len(shape) == 3\n        assert type(augment_flag) == bool\n\n        # saved arguments\n        self.train_data = train_data\n        self.batch_size = batch_size\n        self.file_path = file_path\n        self.shape = shape\n        self.augment_flag = augment_flag\n\n        # get the number of images in the train dataset\n        self.size = train_data.shape[0]\n\n        # get list of images\n        self.image_ids = train_data['Id'].values\n\n        # set a list of indexes to be extracted from the image\n        self.indexes = np.arange(len(self.image_ids))\n\n        # calculate the required number of batches\n        self.batches = int(np.floor(self.size \/ batch_size))\n\n    def __len__(self):\n        return self.batches\n\n    def __getitem__(self, index):\n        \"\"\"\n        Generate one batch of data\n        :param index: (int) index of the total batches size to be loaded\n        :return:\n        \"\"\"\n        # Generate indexes of the batch\n        indexes = self.indexes[index * self.batch_size: (index + 1) * self.batch_size]\n\n        # select images ids\n        batch_ids = [self.image_ids[k] for k in indexes]\n\n        # create array to hold images\n        batch_images = np.empty((self.batch_size, self.shape[0], self.shape[1], self.shape[2]))\n        batch_labels = np.zeros((self.batch_size, 28))\n\n        # load images into array\n        for i in range(self.batch_size):\n            # apply transformations to images based on augment flag\n            if self.augment_flag:\n                batch_images[i] = augment(self.__load_image(batch_ids[i]))\n            else:\n                batch_images[i] = self.__load_image(batch_ids[i])\n            batch_labels[i] = self.train_data.loc[self.train_data['Id'] == batch_ids[i], 'Classes'].values[0]\n\n        # return the images and batches\n        return batch_images, batch_labels\n\n    def __iter__(self):\n        \"\"\"\n        Create a generator that iterate over the Sequence\n        \"\"\"\n        for i in range(self.batches):\n            yield self[i]\n\n    def on_epoch_end(self):\n        \"\"\"\n        Updates indexes after each epoch\n        \"\"\"\n        self.indexes = np.arange(len(self.image_ids))\n        np.random.shuffle(self.indexes)\n\n    def __load_image(self, image_name):\n        \"\"\"\n        Load image contained within a zip file\n        :param image_name: (string) image name contained in zip file\n        :return:           (array) 3D of RGBY divided by 255\n        \"\"\"\n        if '.zip' in self.file_path:\n            # open the zipfile\n            with ZipFile(self.file_path) as z:\n                # load images by channel\n                channel_list = list()\n                for c in ['red', 'green', 'blue', 'yellow']:\n                    with z.open(image_name + '_' + c + '.png') as file:\n                        img = cv2.imread(file, 0)\n                        img = cv2.resize(img, dsize=(self.shape[0], self.shape[1]), interpolation=cv2.INTER_CUBIC)\n                        img = np.array(img)\n                        channel_list.append(img)\n\n        else:\n            # load images by channel\n            channel_list = list()\n            for c in ['red', 'green', 'blue', 'yellow']:\n                img = cv2.imread(self.file_path + '\/' + image_name + '_' + c + '.png', 0)\n                img = cv2.resize(img, dsize=(self.shape[0], self.shape[1]), interpolation=cv2.INTER_CUBIC)\n                img = np.array(img)\n                channel_list.append(img)\n        \n        # stack pixels of image\n        if self.shape[2] == 3:\n            image = np.stack((\n                channel_list[0]\/2 + channel_list[3]\/2, \n                channel_list[1]\/2 + channel_list[3]\/2, \n                channel_list[2]\n            ), -1)\n        else:\n            image = np.stack(channel_list, -1)\n        \n        # normalize pixels range\n        image = np.divide(image, 255)\n        \n        # return array with normalized colors\n        return image","b621dfc5":"# ##### GLOBAL SETTINGS ##### #\n# train set file path\nTRAIN_PATH = '\/kaggle\/input\/human-protein-atlas-image-classification\/train'\n# test file path\nTEST_PATH = '\/kaggle\/input\/human-protein-atlas-image-classification\/test'\n# ratio of total train set that is going to be used as test\nTEST_SIZE = 0.2\n# flag indicating if we should shuffle on K-Fold validation\nSHUFFLE = False\n# random state\nRANDOM_STATE = 42\n# model verbose flag\nVERBOSE = 1\n# check point verbose mode\nVERBOSE_CK = 2\n# test batch size\nTEST_BATCH = 256\n# loss function applied\nLOSS = focal_loss() # 'binary_crossentropy'\n# metrics mesured in model\nMETRICS = ['accuracy', f1]\n\n# ##### MODEL SPECIFIC ##### #\n# shape of image to go into model\nINPUT_SHAPE = (299, 299, 3)\n# model name\nMODEL_NAME = 'InceptionResNet'\n\n# ##### GRID ADJUSTABLE ##### #\n# size of train batch\nTRAIN_BATCH = 10\n# optimizer to be applied to model\nOPTIMIZER = Adam(1e-3)\n# total number of epochs\nEPOCHS = 15\n# number of steps per epoch\nSTEPS_PER_EPOCH = 100\n# total number of validation steps\nVALIDATION_STEPS = 50","8e23613a":"print('LOADING TRAIN CSV FILE')\ntrain_csv = pd.read_csv('\/kaggle\/input\/human-protein-atlas-image-classification\/train.csv')\n\nprint('OBTAINING CLASSES ARRAY')\ntrain_csv['Classes'] = train_csv['Target'].apply(get_classes)\n\nprint('ADJUSTING Y LABELS')\ny = MultiLabelBinarizer().fit_transform(train_csv['Target'].str.split(' '))","9daa6dda":"keras.backend.clear_session()\n\nprint('DEFINING CNN MODEL')\nif MODEL_NAME == 'Sequential':\n    model = sequencial_model(INPUT_SHAPE, 28)\nelif MODEL_NAME == 'InceptionResNet':\n    model = inception_res_net_model(INPUT_SHAPE, 28)\nelif MODEL_NAME == 'ResNet50':\n    model = res_net_50(INPUT_SHAPE, 28)\nelif MODEL_NAME == 'VGG19':\n    model = vgg_model(INPUT_SHAPE, 28)","1f836140":"# define parameters\ngrid = {\n    'epochs': [30, 50, 100],\n    'steps_per_epoch': [100, 150, 200],\n    'learning_rate': [0.001, 0.0005, 0.0001],\n    'loss': ['binary_crossentropy', 'focal_loss'],\n    'batch_size': [10, 30, 50],\n    'class_weight': [None, 0.5, 0.3]\n}\n\ndt = datetime.datetime.today().strftime('%Y%m%d%H%M%S')\nresults_path = '\/kaggle\/working\/' + 'GS_' + MODEL_NAME + dt + '.pkl'\n\n# Do train test split\nkf = StratifiedMultiLabelKFold(int(round(1\/TEST_SIZE)), shuffle=SHUFFLE, random_state=RANDOM_STATE)\nkf_gen = kf.split(None, y)\ntrain, test = next(kf_gen)\n\n# create combination of parameters\nparams = sorted(grid)\ncombinations = list(itertools.product(*(grid[key] for key in params)))\n\n# set results data frame\nresults = list()\nr = 0\n\n# for each combination\nfor comb in combinations:\n    # set combination dictionary\n    c = dict(zip(params, comb))\n    print('RUNNING FOR', c)\n    \n    print('    CREATING PARAMETERS DICTIONARY')\n    # create a dictionary of model parameters\n    model_params = {'metrics': METRICS}\n    fit_params = {'validation_steps': VALIDATION_STEPS, 'verbose': VERBOSE}\n    train_params = {'file_path': TRAIN_PATH, 'shape': INPUT_SHAPE, 'augment_flag': True}\n    \n    # for each parameter\n    for i in range(len(params)):\n        # if the parameter is one of the fit ones\n        if params[i] in ['epochs', 'steps_per_epoch']:\n            # add this parameter to the fit params dictionary\n            fit_params[params[i]] = comb[i]\n        \n        # if the parameter is one of the model ones\n        elif params[i] in ['loss']:\n            # add this parameter to the model params dictionary\n            if comb[i] == 'focal_loss':\n                model_params[params[i]] = focal_loss()\n            else:\n                model_params[params[i]] = comb[i]\n        \n        # if the parameter is one of the model ones\n        elif params[i] in ['learning_rate']:\n            # add this parameter to the model params dictionary\n            model_params['optimizer'] = Adam(comb[i])\n            \n        # if the parameter is one of the weighting ones\n        elif params[i] in ['class_weight']:\n            if comb[i] is None:\n                continue\n            weight = max(train_csv['Classes'].sum()) \/ train_csv['Classes'].sum()\n            weight = weight \/ weight.sum()\n            csort = np.argsort(weight)\n            csum = np.cumsum(weight[csort])\n            weight[csort[np.where(csum < 0.5)]] = weight[csort[np.where(csum < 0.5)]] * (1 - comb[i])\n            weight[csort[np.where(csum >= 0.5)]] = weight[csort[np.where(csum >= 0.5)]] * comb[i]\n            fit_params[params[i]] = 10000 * weight\n\n        # if the parameter is one of the trains parameter\n        elif params[i] in ['batch_size']:\n            # add this to the train param\n            TRAIN_BATCH = comb[i]\n    \n    print('    SETTING TRAIN AND TEST SETS')\n    \n    # create train and test generators\n    train_gen = DataGenerator(train_csv.iloc[train][['Id', 'Classes']], TRAIN_BATCH, TRAIN_PATH, INPUT_SHAPE)\n    test_gen = DataGenerator(train_csv.iloc[test][['Id', 'Classes']], TEST_BATCH, TRAIN_PATH, INPUT_SHAPE, augment_flag=False)\n    \n    print('    COMPILING MODEL')\n    # compile model\n    model.compile(**model_params)\n    \n    print('    CREATING MODEL CHECK POINT')\n    dt = datetime.datetime.today().strftime('%Y%m%d%H%M%S')\n    model_path = '\/kaggle\/working\/' + 'GS_' + MODEL_NAME + dt + '.model'\n    checkpoint = ModelCheckpoint(model_path, verbose=VERBOSE_CK, save_best_only=True)\n    \n    print('    FITTING MODEL')\n    # add generators to fit param\n    fit_params['generator'] = train_gen\n    fit_params['validation_data'] = test_gen\n    \n    # add checkpoint to fit param\n    fit_params['callbacks'] = [checkpoint]\n    \n    # fit model\n    hist = model.fit_generator(**fit_params)\n    \n    print('    RE-LOAD BEST MODEL')\n    model = load_model(\n        model_path, \n        custom_objects={'f1': f1, 'focal_loss_fixed': focal_loss()}\n    )\n\n    print('    PREDICTING ON TEST SET')\n    # create array of predictions\n    y_pred = np.array([model.predict(load_image(TRAIN_PATH, name, shape=INPUT_SHAPE)[np.newaxis])[0] for name in train_csv.iloc[test]['Id']])\n    \n    print('    EVALUATING MODEL')\n    results.append(list())\n\n    # add the combination\n    results.append(c)\n\n    # add predictions\n    results.append(pd.DataFrame(np.concatenate((train_csv.iloc[test]['Id'].values[:, None], y_pred), axis=1)))\n\n    # set the y test value\n    y_true = MultiLabelBinarizer().fit_transform(train_csv.iloc[test]['Target'].str.split(' '))\n\n    # apply the precision recall and f1 score\n    results.append(pd.DataFrame(np.stack(precision_recall_fscore_support(y_true, (y_pred > 0.2).astype(np.uint8)), -1)))\n\n    # add the accuracy\n    results.append(accuracy_score(y_true, (y_pred > 0.2).astype(np.uint8)))\n\n    r += 1\n\n    print('    EXPORTING PARTIAL RESULTS')\n    with open(results_path, 'wb') as f:\n        pickle.dump(results, f)","279acc25":"model_path = '\/kaggle\/working\/' + MODEL_NAME + dt + '.model'\nmodel = load_model(\n    model_path, \n    custom_objects={'f1': f1}\n)","a6485d97":"submit = pd.read_csv('\/kaggle\/input\/human-protein-atlas-image-classification\/sample_submission.csv')","ed9f98c0":"%%time\npredicted = []\nfor name in submit['Id'].values.flatten():\n    image = load_image(TEST_PATH, name, shape=INPUT_SHAPE)\n    score_predict = model.predict(image[np.newaxis])[0]\n    label_predict = np.arange(28)[score_predict >= 0.2]\n    str_predict_label = ' '.join(str(l) for l in label_predict)\n    predicted.append(str_predict_label)","02269fbd":"submit['Predicted'] = predicted","c508a1d2":"submit.to_csv('submission.csv', index=False)","bed6d850":"#### Load and adjust train dataset","fcb8f37b":"## Data processing","0c5cc1dc":"## Constants definition","c7fedbbc":"### Predict for test set","15aa30c3":"print('COMPILING MODEL')\nmodel.compile(optimizer=Adam(1e-3), loss=LOSS, metrics=METRICS)\nmodel.summary()\n\nprint('CREATING TRAN AND TEST GENERATORS')\nkf = StratifiedMultiLabelKFold(int(round(1\/TEST_SIZE)), shuffle=SHUFFLE, random_state=RANDOM_STATE)\nkf_gen = kf.split(None, y)\ntrain, test = next(kf_gen)\ntrain_gen = DataGenerator(train_csv.iloc[train][['Id', 'Classes']], TRAIN_BATCH, TRAIN_PATH, INPUT_SHAPE)\ntest_gen = DataGenerator(train_csv.iloc[test][['Id', 'Classes']], TEST_BATCH, TRAIN_PATH, INPUT_SHAPE, augment_flag=False)\nclass_weights = dict(zip(range(28), max(train_csv['Classes'].sum())\/train_csv['Classes'].sum()))\n\nprint('CREATING MODEL CHECK POINT')\ndt = datetime.datetime.today().strftime('%Y%m%d%H%M%S')\nmodel_path = '\/kaggle\/working\/' + MODEL_NAME + dt + '.model'\ncheckpoint = ModelCheckpoint(model_path, verbose=VERBOSE_CK, save_best_only=True)\n\nprint('FITTING MODEL')\nhist = model.fit_generator(\n    generator=train_gen, \n    validation_data=test_gen,\n    \n    epochs=EPOCHS,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_steps=VALIDATION_STEPS,\n    \n    class_weight=class_weights,\n    \n    verbose=VERBOSE,\n    callbacks=[checkpoint]\n)","bf70260e":"## Create submit","8e95e11f":"### Train especific model on pre selected parameters","43d6149c":" #### Check if split is even accross classes results","126c7d24":"## Train model","af56a139":"### Perform grid search","6dc4b759":"## Import necessary libraries","aa9ae375":"show_history(hist)","ddf7cf31":"### Load submit","9b7ca2ac":"## Functions definitions ","3a5a2bcb":"### Train baseline model","bd442f59":"d = {\n    'Base': train_csv['Classes'].sum(),\n    'Train': train_df['Classes'].sum(),\n    'Test': test_df['Classes'].sum()\n}\ndf = pd.DataFrame(d)\ndf['Train'] = df['Train']\/df['Base']\ndf['Test'] = df['Test']\/df['Base']","73bc93e8":"### Load best model","74e221f9":"## Classes definition"}}