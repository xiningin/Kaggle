{"cell_type":{"94d8143b":"code","e70ce17a":"code","78dd8758":"code","3364177b":"code","6366e175":"code","4338aac8":"code","c245bc4a":"code","07f020c5":"code","f3028bb4":"code","200cf723":"code","dc40dc53":"code","4f78c340":"code","2e01c0d3":"code","fcfc68dd":"code","ac09c92e":"code","37db8257":"code","82ade42d":"code","0c48478d":"code","7729f51e":"code","ff9da9f7":"code","a528c01d":"code","5bf517b6":"code","19c0188b":"markdown","193c5394":"markdown","3eef34ee":"markdown","2a7d5118":"markdown","5ca77949":"markdown","cc8c6524":"markdown","45484d39":"markdown","30d1d3da":"markdown","1bb17501":"markdown"},"source":{"94d8143b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e70ce17a":"# Data processing\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\n# import pandas_profiling as pp\n# import lux\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n\n# Machine Learning\nimport optuna\nfrom optuna.samplers import TPESampler\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold, cross_val_score, StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error,roc_curve,auc,accuracy_score,confusion_matrix,f1_score\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Dense,Dropout,BatchNormalization","78dd8758":"# load data\ninput_dir = Path('..\/input\/tabular-playground-series-aug-2021\/')\ntrain = pd.read_csv(input_dir \/ 'train.csv')\ntest = pd.read_csv(input_dir \/ 'test.csv')\nsubmission = pd.read_csv(input_dir \/ 'sample_submission.csv')","3364177b":"columns = [col for col in train.columns if col not in [\"id\", 'loss']]\nX=train[columns].values\ny=train['loss'].values\nX_test = test[columns].values","6366e175":"bins = len(np.unique(y, return_counts=False))\nfreq = train[\"loss\"].value_counts(normalize=True, sort=False)\ncum_sum = train[\"loss\"].value_counts(normalize=True, sort=False).cumsum()\n\nfig, ax = plt.subplots(2, 1, figsize=(10,10))\nsns.countplot(x=y, label='Train_loss', ax=ax[0])\nax[0].set_title(f'Distribution of the Loss, {bins} unique values', color = \"crimson\")\nax[0].set_xlabel('Loss value')\nax[0].legend()\n\n\n\nsns.ecdfplot(x=y, label='Train_loss', ax=ax[1])\nax[1].set_title(f'Cumulative sum per loss', color = \"crimson\")\nax[1].set_xlabel('Loss value')\nax[1].legend()\n\nfig.tight_layout(pad=3.0);","4338aac8":"%%time\nfig, ax = plt.subplots(10, 10,figsize=(24,24))\nfor i,col in enumerate(columns):\n    sns.kdeplot(train[columns[i]], legend=False, shade=True, ax = ax[i%10][i\/\/10])\n    ax[i%10][i\/\/10].set_title(f\"{train.columns[i]}\", fontsize=10, weight='bold',)\n    ax[i%10][i\/\/10].set_xlabel('')\n    ax[i%10][i\/\/10].set_ylabel('')\n    ax[i%10][i\/\/10].set_yticks([])\n#     plt.subplots_adjust(hspace=0.2)\n\nfig.tight_layout()","c245bc4a":"%%time\ndef cat_estimation(trial,data=X,target=y):\n    x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4,random_state=1)\n    params = {'iterations':trial.suggest_int(\"iterations\", 1000, 4000),\n              'od_wait':trial.suggest_int('od_wait', 300, 500),\n              'task_type':'GPU',\n              'eval_metric':'RMSE',\n              'learning_rate' : trial.suggest_uniform('learning_rate', 0.008,0.02),\n              'grow_policy': trial.suggest_categorical('grow_policy', ['Depthwise','SymmetricTree']),\n              'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.2 , 0.3),\n              'subsample': trial.suggest_uniform('subsample',0.5,1.0),\n              'random_strength': trial.suggest_uniform('random_strength',10,50),\n              'depth': trial.suggest_int('depth',3,15),\n              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n              'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',10,40),\n              'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bernoulli', 'Poisson']),\n               }\n    \n    \n    model = CatBoostRegressor(**params, random_state=1)\n    model.fit(x_train, y_train,eval_set=[(x_valid,y_valid)], verbose=False)\n    \n    preds = model.predict(x_valid)\n    rmse = mean_squared_error(y_valid, preds,squared=False)\n    \n    return rmse","07f020c5":"train_time = 1 * 30 * 60\nstudy_cat = optuna.create_study(direction='minimize', sampler=TPESampler(), study_name='CatRegressor')\nstudy_cat.optimize(cat_estimation, timeout=train_time)\n\nprint('Number of finished trials: ', len(study_cat.trials))\nprint('Best trial:')\ntrial_cat = study_cat.best_trial\n\nprint('\\tValue: {}'.format(trial_cat.value))\nprint('\\tParams: ')\nfor key, value in trial_cat.params.items():\n    print('\\t\\t{}: {}'.format(key, value))","f3028bb4":"#Now let's get a lightgbm\ndef lgb_estimation(trial,data=X,target=y):\n    x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4,random_state=1)\n    params = {\n        'reg_alpha' : trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'reg_lambda' : trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'num_leaves' : trial.suggest_int('num_leaves' , 40 , 70),\n        'learning_rate' : trial.suggest_uniform('learning_rate' , 0.008 , 0.04),\n        'max_depth' : trial.suggest_int('max_depth', 3 , 15),\n        'n_estimators' : trial.suggest_int('n_estimators', 400 , 4000),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 100),\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.5, 0.9, 0.05), \n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.15, 0.9, 0.05),\n        'min_child_samples' : trial.suggest_int('min_child_samples', 20, 65),\n        'metric' : 'rmse',\n        'subsample_freq' : 1,\n        'device_type' : 'gpu',}\n\n    model = LGBMRegressor(**params, random_state=1, n_jobs=-1)\n    model.fit(x_train, y_train,eval_set=[(x_valid,y_valid)], verbose=False)\n    \n    preds = model.predict(x_valid)\n    rmse = mean_squared_error(y_valid, preds,squared=False)\n    \n    return rmse\n\n#https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMRegressor.html","200cf723":"train_time = 1 * 30 * 60\nstudy_lgb = optuna.create_study(direction='minimize', sampler=TPESampler(), study_name='LGBRegressor')\nstudy_lgb.optimize(lgb_estimation, timeout=train_time)\n\nprint('Number of finished trials: ', len(study_lgb.trials))\nprint('Best trial:')\ntrial_lgb = study_lgb.best_trial\n\nprint('\\tValue: {}'.format(trial_lgb.value))\nprint('\\tParams: ')\nfor key, value in trial_lgb.params.items():\n    print('\\t\\t{}: {}'.format(key, value))","dc40dc53":"def xgb_estimation(trial,data=X,target=y):\n    \n    x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4,random_state=1)\n    param = {\n        'tweedie_variance_power': trial.suggest_discrete_uniform('tweedie_variance_power', 1.0, 2.0, 0.1),\n        'lambda': trial.suggest_loguniform('lambda', 1, 100),\n        'alpha': trial.suggest_loguniform('alpha', 1, 100),\n        'gamma': trial.suggest_loguniform('gamma', 1e-3, 1e4),\n        'n_estimators': trial.suggest_int('n_estimators', 400, 4000, 400),\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.2, 0.9, 0.05),\n        'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.2, 0.9, 0.05),\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.5, 0.9, 0.05),\n        'eta': trial.suggest_float('eta', 0.007,0.020),\n        'n_estimators': trial.suggest_int(\"n_estimators\",400,4000,400),\n        'max_depth': trial.suggest_int('max_depth', 3,15,1),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1, 1000),\n    }\n    model = xgb.XGBModel(\n        objective='reg:tweedie',\n        tree_method='gpu_hist',   #which tree to chose: https:\/\/xgboost.readthedocs.io\/en\/latest\/treemethod.html\n        predictor='gpu_predictor',\n        n_jobs=-1,\n        **param\n    ) \n    \n    model.fit(x_train, y_train,\n            eval_set=[(x_valid, y_valid)], eval_metric='rmse',\n            verbose=False)\n    \n    preds = model.predict(x_valid)\n    \n    rmse = mean_squared_error(y_valid, preds,squared=False)\n    \n    return rmse","4f78c340":"train_time = 1 * 30 * 60\nstudy_xgb = optuna.create_study(direction='minimize', sampler=TPESampler(), study_name='XGBRegressor')\nstudy_xgb.optimize(xgb_estimation, timeout=train_time)\n\nprint('Number of finished trials: ', len(study_xgb.trials))\nprint('Best trial:')\ntrial_xgb = study_xgb.best_trial\n\nprint('\\tValue: {}'.format(trial_xgb.value))\nprint('\\tParams: ')\nfor key, value in trial_xgb.params.items():\n    print('\\t\\t{}: {}'.format(key, value))","2e01c0d3":"# cat_params = {'iterations': 2352,\n#  'od_wait': 423,\n#  'learning_rate': 0.016226794416482,\n#  'grow_policy': 'Depthwise',\n#  'reg_lambda': 0.22443418652185604,\n#  'subsample': 0.6489507065409703,\n#  'random_strength': 33.31677512363275,\n#  'depth': 5,\n#  'min_data_in_leaf': 28,\n#  'leaf_estimation_iterations': 23,\n#  'bootstrap_type': 'Bernoulli',}\n\n# lgb_params = {'reg_alpha': 0.5127042132407919, 'reg_lambda': 5.357028975534249, \n#           'num_leaves': 50, 'learning_rate': 0.011363404060130413, \n#           'max_depth': 10, 'n_estimators': 3279, \n#           'min_child_weight': 0.07984529371109039,\n#           'subsample': 0.65, \n#           'colsample_bytree': 0.35, \n#           'min_child_samples': 43, 'subsample_freq' : 1}\n\n# xgb_params = {'tweedie_variance_power': 1.1,\n#  'max_depth': 6,\n#  'n_estimators': 3200,\n#  'eta': 0.011245712330378816,\n#  'subsample': 0.9,\n#  'min_child_weight': 318.8784492865065,\n#  'colsample_bytree': 0.45,\n#  'colsample_bylevel': 0.30000000000000004,\n#  'subsample': 0.9,\n#  'lambda': 30.619480207080088,\n#  'alpha': 42.00321964378956,\n#  'gamma': 0.24759763410326613,\n#  'tree_method':'gpu_hist'\n#              }    ","fcfc68dd":"cat_params = trial_cat.params\nlgb_params = trial_lgb.params\nxgb_params = trial_xgb.params\n\nxgb_params['objective'] = 'reg:tweedie'\nxgb_params['tree_method'] = 'gpu_hist'\nxgb_params['predictor'] = 'gpu_predictor'\n        \nn_splits = 10\n\ncat_preds = np.zeros((X_test.shape[0],))\noof_cat_preds = np.zeros((X.shape[0],))\nkf_cat_rmse = []\n\nlgb_preds = np.zeros((X_test.shape[0],))\noof_lgb_preds = np.zeros((X.shape[0],))\nkf_lgb_rmse = []\n\nxgb_preds = np.zeros((X_test.shape[0],))\noof_xgb_preds = np.zeros((X.shape[0],))\nkf_xgb_rmse = []\n\n\nfor fold, (train_idx, valid_idx) in enumerate(KFold(n_splits=n_splits, shuffle=True).split(X, y)):\n    # Fetch the train-validation indices.\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_valid, y_valid = X[valid_idx], y[valid_idx]\n    \n    # Create and fit the models using optuna hyperparameters\n    model_cat = CatBoostRegressor(**cat_params, task_type='GPU') #https:\/\/catboost.ai\/docs\/concepts\/python-reference_catboostregressor.html\n    model_cat.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n    \n    model_lgb = LGBMRegressor(**lgb_params, device='gpu') #https:\/\/catboost.ai\/docs\/concepts\/python-reference_catboostregressor.html\n    model_lgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False,early_stopping_rounds=200,)\n    \n    model_xgb = xgb.XGBModel(**xgb_params)\n    model_xgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_metric='rmse', verbose=False)\n    \n    # Validation predictions.\n    cat_pred = model_cat.predict(X_valid)\n    cat_rmse = mean_squared_error(y_valid, cat_pred,squared=False)\n    kf_cat_rmse.append(cat_rmse)\n    oof_cat_preds[valid_idx] += model_cat.predict(X_valid) \/ n_splits\n    \n    lgb_pred = model_lgb.predict(X_valid)\n    lgb_rmse = mean_squared_error(y_valid, lgb_pred,squared=False)\n    kf_lgb_rmse.append(lgb_rmse)\n    oof_lgb_preds[valid_idx] += model_lgb.predict(X_valid) \/ n_splits\n    \n    xgb_pred = model_xgb.predict(X_valid)\n    xgb_rmse = mean_squared_error(y_valid, xgb_pred,squared=False)\n    kf_xgb_rmse.append(xgb_rmse)\n    oof_xgb_preds[valid_idx] += model_xgb.predict(X_valid) \/ n_splits\n\n    #Pred on test set \n    cat_preds += model_cat.predict(X_test) \/ n_splits\n    lgb_preds += model_lgb.predict(X_test) \/ n_splits\n    xgb_preds += model_xgb.predict(X_test) \/ n_splits\n\nfor i in range(0,10):\n    print(f'Fold {i+1}\/{n_splits} CATBOOST RMSE: {kf_cat_rmse[i]:.4f}')\n\nfor i in range(0,10):\n    print(f'Fold {i+1}\/{n_splits} LGBM RMSE: {kf_lgb_rmse[i]:.4f}')\n    \nfor i in range(0,10):\n    print(f'Fold {i+1}\/{n_splits} XGB RMSE: {kf_xgb_rmse[i]:.4f}')","ac09c92e":"upper = 11\nmin_value = 1e6\nmae_plot = []\nfor i in range(0,11):\n    for j in range(0,upper-i):\n        pred_ensemble = ((0.1*i) * oof_lgb_preds) + ((0.1*j) * oof_cat_preds) +(0.1*(upper - 1 - i - j)) *  oof_xgb_preds\n        mae = mean_squared_error(y, pred_ensemble,squared=False)\n        if mae < min_value:\n            mae_plot.append(mae)\n            min_value = mae\n            weights = [i, j,upper - 1 - i - j ]\n        else:\n            mae_plot.append(mae)\n#             print(f'lgb_coeff : {i} cat_coeff: {j} lgb_coeff: {upper - 1 - i - j}')\nprint(f'Min_RMSE : {min_value} Best_Weights: {weights}')\nplt.plot(mae_plot)","37db8257":"#We will now build a simple Neural Network using Keras\ndef nn_model(h1, h2, h3, lr, dr):\n    #initiating the model\n    model = Sequential()\n    initializer = tf.keras.initializers.HeUniform()\n    model.add(Dense(input_dim=X.shape[1], units=h1, kernel_initializer=initializer , activation=\"elu\"))\n    model.add(BatchNormalization())\n    model.add(Dropout(dr))\n    model.add(Dense(units=h2, activation=\"elu\"))\n    model.add(BatchNormalization())\n    model.add(Dropout(dr))\n    model.add(Dense(units=h3, activation=\"elu\"))\n    model.add(BatchNormalization())\n    model.add(Dropout(dr))\n    model.add(Dense(units=1, activation='linear'))\n    \n    #compile the model\n    model.compile(optimizer = tf.optimizers.Adam(learning_rate = lr), loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    return model","82ade42d":"%%time\ndef nn_estimation(trial,data=X,target=y):\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4,random_state=1)\n    SS = StandardScaler().fit(X_train)\n    X_train = SS.transform(X_train)\n    X_valid = SS.transform(X_valid)\n    params = {'h1': trial.suggest_int('h1' ,3 , 100),\n              'h2': trial.suggest_int('h2' ,3 , 100),\n              'h3': trial.suggest_int('h3' ,3 , 100),\n              'lr' : trial.suggest_uniform('lr' , 0.002 , 0.1),\n              'dr' : trial.suggest_uniform('dr' ,0.01 , 0.2),\n               }\n        \n    \n    model = nn_model(**params)\n    model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs = 30, batch_size = 3000, verbose = 0)\n    \n    model_pred = model.predict(X_valid)\n    rmse = model.evaluate(X_valid, y_valid, verbose=0)\n    \n    return rmse[1]","0c48478d":"train_time = 1 * 30 * 60\nstudy_nn = optuna.create_study(direction='minimize', sampler=TPESampler(), study_name='DeepNN')\nstudy_nn.optimize(nn_estimation, timeout=train_time)\n\nprint('Number of finished trials: ', len(study_nn.trials))\nprint('Best trial:')\ntrial_nn = study_nn.best_trial\n\nprint('\\tValue: {}'.format(trial_nn.value))\nprint('\\tParams: ')\nfor key, value in trial_nn.params.items():\n    print('\\t\\t{}: {}'.format(key, value))","7729f51e":"n_splits = 10\n\nnn_params = trial_nn.params\n\nnn_preds = np.zeros((X_test.shape[0],))\noof_nn_preds = np.zeros((X.shape[0],))\nkf_nn_rmse = []\n\nSS = StandardScaler().fit(X)\nX_scaled = SS.transform(X)\nX_test_scaled = SS.transform(X_test)\n\nfor fold, (train_idx, valid_idx) in enumerate(KFold(n_splits=n_splits, shuffle=True).split(X_scaled, y)):\n    # Fetch the train-validation indices.\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_valid, y_valid = X[valid_idx], y[valid_idx]\n    \n    X_train = SS.transform(X_train)\n    X_valid = SS.transform(X_valid)\n    \n    # Create and fit the model using optuna hyperparameters\n    model_nn = nn_model(**nn_params)\n    model_nn.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=30, batch_size=3000, verbose=0)\n    \n    # Validation predictions.\n    nn_rmse = model_nn.evaluate(X_valid, y_valid, verbose=0)\n    kf_nn_rmse.append(nn_rmse[1])\n    oof_nn_preds[valid_idx] += model_nn.predict(X_valid).reshape(-1,) \/ n_splits\n\n    #Pred on test set \n    nn_preds += model_nn.predict(X_test_scaled).reshape(-1,) \/ n_splits\n\n\nfor i in range(0,10):\n    print(f'Fold {i+1}\/{n_splits} NN RMSE: {kf_nn_rmse[i]:.4f}')","ff9da9f7":"min_value = 1e6\nmae_plot_2 = []\nfor i in range(0,11):\n    pred_ensemble = (0.1 * i) * (weights[0] * oof_lgb_preds + weights[1] * oof_cat_preds + weights[2] * oof_xgb_preds) + (1 - (0.1 * i)) * oof_nn_preds\n    mae = mean_squared_error(y, pred_ensemble,squared=False)\n    if mae < min_value:\n        mae_plot_2.append(mae)\n        min_value = mae\n        weights_2 = [0.1*i, 1-(0.1*i)]\n    else:\n        mae_plot_2.append(mae)\n#             print(f'lgb_coeff : {i} cat_coeff: {j} lgb_coeff: {upper - 1 - i - j}')\nprint(f'Min_RMSE : {min_value} Best_Weights: {weights_2}')\nplt.plot(mae_plot_2)","a528c01d":"submission[\"loss\"] =  (weights_2[0] * (weights[0] * lgb_preds + weights[1] * cat_preds + weights[2] * xgb_preds))\/10 + (weights_2[1] * nn_preds)\nsubmission.to_csv('submission_final.csv', index=False)","5bf517b6":"submission[\"loss\"] =  (0.95 * (0.2 * lgb_preds + 0.6 * cat_preds + 0.2 * xgb_preds)) + (0.05 * nn_preds)\nsubmission.to_csv('submission_average_model.csv', index=False)","19c0188b":"Number of finished trials:  28\nBest trial:\n\tValue: 7.833826248634097\n\tParams: \n\t\talpha: 0.002844219172632396\n\t\tlambda: 0.04988448769361392\n\t\tnum_leaves: 58\n\t\tlearning_rate: 0.010295669290960142\n\t\tmax_depth: 8\n\t\tn_estimators: 3663\n\t\tmin_child_weight: 2.185521963310407\n\t\tsubsample: 0.75\n\t\tcolsample_bytree: 0.35\n\t\tmin_child_samples: 57","193c5394":"# XGB\/LBG\/CATBOOST - HYPERPARAMETERS TUNING","3eef34ee":"Number of finished trials:  47\nBest trial:\n\tValue: 7.835773902876162\n\tParams: \n\t\ttweedie_variance_power: 1.0\n\t\tlambda: 2.1531451037845706\n\t\talpha: 75.55781034374724\n\t\tgamma: 0.003783218732688592\n\t\tn_estimators: 3600\n\t\tcolsample_bytree: 0.8500000000000001\n\t\tcolsample_bylevel: 0.25\n\t\tsubsample: 0.8500000000000001\n\t\teta: 0.008050760753145963\n\t\tmax_depth: 7\n\t\tmin_child_weight: 27.35935907624855","2a7d5118":"# BASIC NEURAL NETWORK \n**(added at last minute to check how it worked with optuna)**","5ca77949":"Number of finished trials:  105\nBest trial:\n\tValue: 7.8848958015441895\n\tParams: \n\t\th1: 9\n\t\th2: 50\n\t\th3: 75\n\t\tlr: 0.005007293309139238\n\t\tdr: 0.19501276039972268","cc8c6524":"# EDA","45484d39":"Number of finished trials:  10\nBest trial:\n\tValue: 7.843146985719494\n\tParams: \n\t\titerations: 3084\n\t\tod_wait: 332\n\t\tlearning_rate: 0.019584817897930977\n\t\tgrow_policy: Depthwise\n\t\treg_lambda: 0.2587005604240873\n\t\tsubsample: 0.6264800857693638\n\t\trandom_strength: 46.10328452158566\n\t\tdepth: 3\n\t\tmin_data_in_leaf: 27\n\t\tleaf_estimation_iterations: 28\n\t\tbootstrap_type: Poisson","30d1d3da":"# SUBMISSION","1bb17501":"# XGB\/LBG\/CATBOOST - VALIDATION AND TEST SET"}}