{"cell_type":{"5c0a8c9c":"code","1c2c054d":"code","9d095b31":"code","f02b4fd0":"code","76d36321":"code","4e81cb7c":"code","290f4746":"code","61a65069":"code","b7e60537":"code","4cca4a45":"code","c4a663ec":"code","7a4b014f":"code","6de4578e":"code","cfa1a0d5":"code","4282978a":"code","1595eec4":"code","c7c0165f":"code","66285a85":"code","ccde0fe3":"code","04dcbdf5":"code","1cd26aa3":"code","f1eba698":"code","8c47c0e4":"code","bb1852a8":"code","267a1374":"code","a54400af":"code","e36c1e2f":"code","97ea3ac6":"code","e032579f":"code","6cc834fb":"code","171a6895":"code","368406f9":"code","7374d225":"code","88a34c97":"code","d5b0ce09":"code","d6bede55":"code","92555c24":"code","e3687c05":"code","49e16caf":"code","6fc724db":"code","c18b7c4c":"code","189bcd2b":"code","25e446c7":"markdown","3fe00c8b":"markdown","b53416e5":"markdown","283ae031":"markdown","3b714ab3":"markdown","65b06000":"markdown","b2627dba":"markdown","81aaea8f":"markdown","384061a5":"markdown","86176622":"markdown"},"source":{"5c0a8c9c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport datetime as dt\n\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import cut_tree","1c2c054d":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9d095b31":"# read the dataset\nretail_df = pd.read_csv(r\"\/kaggle\/input\/learning-data\/OnlineRetail.csv\", sep=\",\", encoding=\"ISO-8859-1\", header=0)\nretail_df.head()","f02b4fd0":"# basics of the df\nretail_df.info()","76d36321":"# missing values\nround(100*(retail_df.isnull().sum())\/len(retail_df), 2)","4e81cb7c":"# drop all rows having missing values\nretail_df = retail_df.dropna()\nretail_df.shape","290f4746":"retail_df.head()","61a65069":"# new column: amount \nretail_df['amount'] = retail_df['Quantity']*retail_df['UnitPrice']\nretail_df.head()","b7e60537":"# monetary\ngrouped_df = retail_df.groupby('CustomerID')['amount'].sum()\ngrouped_df = grouped_df.reset_index()\ngrouped_df.head()","4cca4a45":"# frequency\nfrequency = retail_df.groupby('CustomerID')['InvoiceNo'].count()\nfrequency = frequency.reset_index()\nfrequency.columns = ['CustomerID', 'frequency']\nfrequency.head()","c4a663ec":"# merge the two dfs\ngrouped_df = pd.merge(grouped_df, frequency, on='CustomerID', how='inner')\ngrouped_df.head()","7a4b014f":"retail_df.head()","6de4578e":"# recency\n# convert to datetime\nretail_df['InvoiceDate'] = pd.to_datetime(retail_df['InvoiceDate'], \n                                          format='%d-%m-%Y %H:%M')","cfa1a0d5":"retail_df.head()","4282978a":"# compute the max date\nmax_date = max(retail_df['InvoiceDate'])\nmax_date","1595eec4":"# compute the diff\nretail_df['diff'] = max_date - retail_df['InvoiceDate']\nretail_df.head()","c7c0165f":"# recency\nlast_purchase = retail_df.groupby('CustomerID')['diff'].min()\nlast_purchase = last_purchase.reset_index()\nlast_purchase.head()","66285a85":"# merge\ngrouped_df = pd.merge(grouped_df, last_purchase, on='CustomerID', how='inner')\ngrouped_df.columns = ['CustomerID', 'amount', 'frequency', 'recency']\ngrouped_df.head()","ccde0fe3":"# number of days only\ngrouped_df['recency'] = grouped_df['recency'].dt.days\ngrouped_df.head()","04dcbdf5":"# 1. outlier treatment\nfig=plt.figure(figsize=(20,8))\nplt.subplot(1,2,1)\nsns.violinplot(y=grouped_df['recency'])\nplt.subplot(1,2,2)\nsns.boxplot(y=grouped_df['recency'])\nplt.show()","1cd26aa3":"# two types of outliers:\n# - statistical\n# - domain specific","f1eba698":"# removing (statistical) outliers\nQ1 = grouped_df.amount.quantile(0.05)\nQ3 = grouped_df.amount.quantile(0.95)\nIQR = Q3 - Q1\ngrouped_df = grouped_df[(grouped_df.amount >= Q1 - 1.5*IQR) & (grouped_df.amount <= Q3 + 1.5*IQR)]\n\n# outlier treatment for recency\nQ1 = grouped_df.recency.quantile(0.05)\nQ3 = grouped_df.recency.quantile(0.95)\nIQR = Q3 - Q1\ngrouped_df = grouped_df[(grouped_df.recency >= Q1 - 1.5*IQR) & (grouped_df.recency <= Q3 + 1.5*IQR)]\n\n# outlier treatment for frequency\nQ1 = grouped_df.frequency.quantile(0.05)\nQ3 = grouped_df.frequency.quantile(0.95)\nIQR = Q3 - Q1\ngrouped_df = grouped_df[(grouped_df.frequency >= Q1 - 1.5*IQR) & (grouped_df.frequency <= Q3 + 1.5*IQR)]\n\n","8c47c0e4":"# 2. rescaling\nrfm_df = grouped_df[['amount', 'frequency', 'recency']]\n\n# instantiate\nscaler = StandardScaler()\n\n# fit_transform\nrfm_df_scaled = scaler.fit_transform(rfm_df)\nrfm_df_scaled.shape","bb1852a8":"rfm_df_scaled = pd.DataFrame(rfm_df_scaled)\nrfm_df_scaled.columns = ['amount', 'frequency', 'recency']\nrfm_df_scaled.head()","267a1374":"# k-means with some arbitrary k\nkmeans = KMeans(n_clusters=4, max_iter=50)\nkmeans.fit(rfm_df_scaled)","a54400af":"kmeans.labels_","e36c1e2f":"# help(KMeans)","97ea3ac6":"# elbow-curve\/SSD\nfig=plt.figure(figsize=(20,8))\nssd = []\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\nfor num_clusters in range_n_clusters:\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(rfm_df_scaled)\n    \n    ssd.append(kmeans.inertia_)\n    \n# plot the SSDs for each n_clusters\n# ssd\nplt.plot(ssd);","e032579f":"# silhouette analysis\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(rfm_df_scaled)\n    \n    cluster_labels = kmeans.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(rfm_df_scaled, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))\n    \n    ","6cc834fb":"# final model with k=3\nkmeans = KMeans(n_clusters=3, max_iter=50)\nkmeans.fit(rfm_df_scaled)","171a6895":"kmeans.labels_","368406f9":"# assign the label\ngrouped_df['cluster_id'] = kmeans.labels_\ngrouped_df.head()","7374d225":"# plot\nfig=plt.figure(figsize=(20,8))\nplt.subplot(1,2,1)\nsns.violinplot(x='cluster_id', y='amount', data=grouped_df)\nplt.subplot(1,2,2)\nsns.boxplot(x='cluster_id', y='amount', data=grouped_df)\nplt.show()","88a34c97":"rfm_df_scaled.head()","d5b0ce09":"grouped_df.head()","d6bede55":"# single linkage\nfig=plt.figure(figsize=(20,8))\nmergings = linkage(rfm_df_scaled, method=\"single\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","92555c24":"# complete linkage\nfig=plt.figure(figsize=(20,20))\nmergings = linkage(rfm_df_scaled, method=\"complete\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","e3687c05":"# 3 clusters\ncluster_labels = cut_tree(mergings, n_clusters=3).reshape(-1, )\ncluster_labels","49e16caf":"# assign cluster labels\ngrouped_df['cluster_labels'] = cluster_labels\ngrouped_df.head()","6fc724db":"# plots\nfig=plt.figure(figsize=(20,8))\nplt.subplot(1,2,1)\nsns.violinplot(x='cluster_labels', y='recency', data=grouped_df)\nplt.subplot(1,2,2)\nsns.boxplot(x='cluster_labels', y='recency', data=grouped_df)\nplt.show()","c18b7c4c":"# plots\nfig=plt.figure(figsize=(20,8))\nplt.subplot(1,2,1)\nsns.violinplot(x='cluster_labels', y='frequency', data=grouped_df)\nplt.subplot(1,2,2)\nsns.boxplot(x='cluster_labels', y='frequency', data=grouped_df)\nplt.show()","189bcd2b":"# plots\nfig=plt.figure(figsize=(20,8))\nplt.subplot(1,2,1)\nsns.violinplot(x='cluster_labels', y='amount', data=grouped_df)\nplt.subplot(1,2,2)\nsns.boxplot(x='cluster_labels', y='amount', data=grouped_df)\nplt.show();","25e446c7":"**Overview**<br>\n<a href=\"https:\/\/archive.ics.uci.edu\/ml\/datasets\/online+retail\">Online retail is a transnational data set<\/a> which contains all the transactions occurring between 01\/12\/2010 and 09\/12\/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n\nThe steps are broadly:\n1. Read and understand the data\n2. Clean the data\n3. Prepare the data for modelling\n4. Modelling\n5. Final analysis and record observation ","3fe00c8b":"## Finding the Optimal Number of Clusters\n\n### SSD","b53416e5":"# 2. Clean the data","283ae031":"# 4. Modelling","3b714ab3":"# 1. Read and visualise the data","65b06000":"## K-Means Clustering","b2627dba":"## Hierarchical Clustering","81aaea8f":"# 3. Prepare the data for modelling","384061a5":"- R (Recency): Number of days since last purchase\n- F (Frequency): Number of tracsactions\n- M (Monetary): Total amount of transactions (revenue contributed)","86176622":"### Silhouette Analysis\n\n$$\\text{silhouette score}=\\frac{p-q}{max(p,q)}$$\n\n$p$ is the mean distance to the points in the nearest cluster that the data point is not a part of\n\n$q$ is the mean intra-cluster distance to all the points in its own cluster.\n\n* The value of the silhouette score range lies between -1 to 1. \n\n* A score closer to 1 indicates that the data point is very similar to other data points in the cluster, \n\n* A score closer to -1 indicates that the data point is not similar to the data points in its cluster."}}