{"cell_type":{"bdcf7b41":"code","4a80dcd4":"code","cb32a28e":"code","2fddaae4":"code","bbc11c59":"code","4bc39de2":"code","ed0f5878":"code","331943b3":"code","59c23bb8":"code","7e088e03":"code","d903946c":"code","79c36d42":"markdown","9acbe641":"markdown","ad535d95":"markdown","652a7b40":"markdown","96ee215a":"markdown","165a7ae9":"markdown","235f3c54":"markdown","2e9a0407":"markdown"},"source":{"bdcf7b41":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB","4a80dcd4":"df = pd.read_csv(\"..\/input\/spam.csv\", encoding = \"latin-1\", usecols=[\"v1\", \"v2\"])\ndf.columns = [\"label\", \"message\"]","cb32a28e":"df.info()","2fddaae4":"df.head()","bbc11c59":"sns.set()\nsns.countplot(data = df, x= df[\"label\"]).set_title(\"Amount of spam and no-spam messages\", fontweight = \"bold\")\nplt.show()","4bc39de2":"non_spam_messages = df[df[\"label\"]==\"ham\"][\"message\"] #filters non-spam (ham) messages\nl = [message.split() for message in non_spam_messages] #creates lists of words from each message\nno_spam_words = list(itertools.chain.from_iterable(l)) #concatenates all lists of words into one\n\nspam_messages = df[df[\"label\"]==\"spam\"][\"message\"] \nk = [message.split() for message in spam_messages]\nspam_words = list(itertools.chain.from_iterable(k))\n\nnon_spam = Counter(no_spam_words) #creates dictionary with occurrences of each no_spam word\nspam = Counter(spam_words) #creates dictionary occurences of each spam word (creates dictionary)\n\nnon_spam = sorted(non_spam.items(), key=lambda kv: kv[1], reverse=True) #creates list sorted descending by value (number of occurences) \nspam = sorted(spam.items(), key=lambda kv: kv[1], reverse=True) \n\ntop15_spam = spam[:16]\ntop15_nonspam = non_spam[:16]\n\n#slice word and occurences so I can easily plot them\ntop_words_nonspam = [i[0] for i in top15_nonspam]  #(to,1530) [0]=to [1]=1530  \ntop_occurences_nonspam = [i[1] for i in top15_nonspam]\n\ntop_words_spam = [i[0] for i in top15_spam]       \ntop_occurences_spam = [i[1] for i in top15_spam]\n\nplt.bar(top_words_nonspam, top_occurences_nonspam, color = 'green')\nplt.title(\"Top 15 non spam words\")\nplt.xticks(rotation='vertical')\nplt.show()\n\nplt.bar(top_words_spam, top_occurences_spam, color = \"red\")\nplt.title(\"Top 15 spam words\")\nplt.xticks(rotation='vertical')\nplt.show()","ed0f5878":"X = df[\"message\"]\ny = df[\"label\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)\n\ncv = CountVectorizer(min_df = 0.01, max_features = 300, stop_words = 'english') #1% \/ 300 occurences \ncv.fit(X_train)\n\nX_train = cv.transform(X_train)\nX_test = cv.transform(X_test)","331943b3":"model = MultinomialNB()\nmodel.fit(X_train, y_train)\n\nprint(model.score(X_test, y_test))","59c23bb8":"y_test_pred = model.predict(X_test)\nconfusion_matrix(y_test, y_test_pred)","7e088e03":"def classifier(message):\n    \n    transformed = cv.transform([message])\n    prediction =  model.predict(transformed)\n    \n    if prediction == \"ham\":\n        return \"This message is no spam!\"\n    else:\n        return \"This message is spam!\"\n    \nclassifier(\"Free entry in 2 a wkly comp to win FA Cup fina...\")","d903946c":"# Print the first 10 features\nprint(len(cv.get_feature_names()))\n\n# Print the first 5 vectors of the tfidf training data\nprint(len(X_train.A[0]))","79c36d42":"### Text (Message) Analytics\n\n- Filter Top 15 and Bottom 15 words and the number of their occurences\n- Visualize Top 15 and Bottom 15 spam\/no spam words\n\n**Conclusion**\n\nTop 15 spam and no spam words have many similarities as both of them represent common words that are used in many messages, wether it being spam or no spam (ham). \n\nSame goes for Bottom 15 spam and no spam words. Both include uncommon words that occur only once and are thus not relevant for the classification model and should not be fitted as training data for the classifier to avoid overfitting.","9acbe641":"### Exploratory Data Analysis\n\n- general informations about data (volume, dtypes)\n- display first 5 entries\n- visualize distribution of spam and ham (no-spam)","ad535d95":"## Hyperparameter Tuning","652a7b40":"### Confusion-Matrix\n\nDue to the fact that the already classified data we trained our model with is not very balanced (way more no-spam messages), we don't know if the  R2 score is reliable. With the Confusion matrix, we can evaluate the accuracy of the spam classification more clearer and see exactly where our multinomial naive Bayes has its errors.\n\nFormat of Confusion-Matrix:\n\n| True ham | False spam\n| --- | --- |\n| False ham | True spam","96ee215a":"# Spam classification using multinomial naive bayes\n\n### Structure\n- Import libraries\n- Load and prepare the Dataset\n- Explore the Data\n- Text (Message) analytics\n- Transform data for model (CountVectorizer)\n- Classification\/Predictive Analytics (multinomial naive bayes)\n- R2-Score and Confusion matrix","165a7ae9":"### Transform data for Model (CountVectoriter)\n\n\n#### Use CountVectoriter to transform train and test date into a sparce matrix (based on train data)\n- Split data into Train and test data (75%\/25%)\n- Train CountVectorizer with the train data so it can learn to vocabular and count how often a word as been used. This is needes as input Value for the multinomial naive bayes because it can't handle raw test_messages.\n- Transform train and test date into a sparce matrix (rowsxcolumns) which creates a new column for every word in the messages and counts how often they have been used in each message.\n\nExample:\n\nMessage 1: random text text\n\nMessage 2: another random text \n\nAfter transformation: 2x4 sparce matrix (type = integer) with 4 Stored elements in Compressed Sparse Row format (one element for each unique word)\n\n                                      Displayed as an Array: array([[0, 1, 2], \n                                                                    [1, 1, 1]] \n                              \n                              Message 1: another = 0x, random = 1x, text = 2x. \n                              Message 2: another = 1x, random = 1x, text = 1x.\n                              \nThis means we will have thousands of columns, one for each word.  One of the strenghts of the (multinomial) naive bayes is that it can handle a lot of data. \n\n#### Filter non-relevant words\n\n- The CountVectoricer allows us to set a minimum and maximum occurence requirement, which allows us to filter out the non-relevant\/harmful words (most common\/uncommon) which I displayed in the Text (Message) Analytics. This will reduce the amount of columns we train our classifier with. We can use the parameters min_features and max_features to do this. To be sure all stop words are removed, I added the stop_words = 'english' parameter.","235f3c54":"### Import libraries","2e9a0407":"### Load Dataset\n- deleted unnecessary columns\n- renamed column names to clearer names (category, message)"}}