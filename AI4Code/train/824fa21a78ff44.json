{"cell_type":{"813d2b8c":"code","69128217":"code","95241fd1":"code","7cca3ff9":"code","318b4eac":"code","771585ad":"code","7c6df3c2":"code","0fd51088":"code","8d4173d6":"code","e22eb062":"code","dc20e64c":"code","0e78eccf":"code","70a942d5":"code","90ed76c6":"code","d8dfd7e9":"code","8e1d44d4":"code","75140540":"code","2848faaa":"code","aacbe614":"code","45d4a087":"code","c2501833":"code","957536fc":"code","28a88499":"code","622b0fd5":"code","b6bdb481":"code","e89c077a":"code","0e860b37":"code","672d675b":"markdown","d1cda6a4":"markdown","cc870d98":"markdown","dbf698fe":"markdown","185079a5":"markdown","9a52b680":"markdown","9f4c54f0":"markdown","795256c8":"markdown","7971dae9":"markdown","b5ac85cd":"markdown","7d881a6e":"markdown","03046311":"markdown","1da0e307":"markdown","d4083b4d":"markdown","3e6667c7":"markdown","04cfcc65":"markdown","712c9c5c":"markdown","1e06cabb":"markdown","fc72caee":"markdown","880e221f":"markdown","9b3a488a":"markdown","c1038b1c":"markdown","da929ab6":"markdown","8d3e24aa":"markdown","b50776e2":"markdown","9e1aa404":"markdown","c1f3adb5":"markdown","aabc82c6":"markdown","ba14b463":"markdown","6b50df80":"markdown","54e99eac":"markdown","82613405":"markdown","f0c32bb1":"markdown","8173f854":"markdown"},"source":{"813d2b8c":"\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport sklearn as sc\nimport os\nfrom bokeh import __version__ as bk_version\n\n\ndata = pd.read_csv('\/kaggle\/input\/french-motor-claims-datasets-fremtpl2freq\/freMTPL2freq.csv')\n\n\ndata['freq'] = data['ClaimNb']\/data['Exposure']\ndata['LogDensity'] = np.log(data['Density'])\n\nprint(data.describe)\n","69128217":"random_subset = data.sample(n=20000)\nsubset_summary = random_subset.describe()\nprint(subset_summary)","95241fd1":"def plotBarChart(data,x,ylimitFREQ):\n    EVY= data.groupby(x,as_index=False).agg({'Exposure': 'sum'})\n    plt.subplot(1, 2, 1)\n    plt.bar(EVY[x],EVY['Exposure'],align='center')\n    plt.xlabel(x)\n    plt.ylabel('Exposure')\n    plt.title(\"Exposure over \" + str(x))\n    plt.rcParams['figure.figsize'] = (10,6)\n    plt.xticks(rotation=50)\n    \n    Freq= data.groupby(x,as_index=False).agg({'freq': 'mean'})\n    plt.subplot(1, 2, 2)\n    plt.bar(Freq[x],Freq['freq'],align='center')\n    plt.xlabel(x)\n    plt.ylabel('Freq')\n    plt.title(\"Freq over \" + str(x))\n    plt.rcParams['figure.figsize'] = (10,6)\n    plt.ylim(top=ylimitFREQ)\n    plt.xticks(rotation=50)   \n    \n    plt.tight_layout()\nplt.show()\n\n\n","7cca3ff9":"%%time\n\nplotBarChart(data,'DrivAge',2)\n","318b4eac":"plotBarChart(data,'VehPower',2)","771585ad":"plotBarChart(data,'VehAge',2)","7c6df3c2":"plotBarChart(data,'BonusMalus',2)","0fd51088":"plotBarChart(data,'Density',2)","8d4173d6":"plotBarChart(data,'LogDensity',2)","e22eb062":"plotBarChart(data,'Area',2)","dc20e64c":"plotBarChart(data,'VehBrand',2)","0e78eccf":"plotBarChart(data,'VehGas',2)","70a942d5":"plotBarChart(data,'Region',2)\n","90ed76c6":"import geopandas as gpd\n\n\n\nsf = gpd.read_file('\/kaggle\/input\/france\/departements-version-simplifiee.geojson')\n","d8dfd7e9":"data.isnull().sum()","8e1d44d4":"data_new=pd.get_dummies(data,prefix=['Area','VehBrand','VehGas','Region'])\n\nprint(data_new.describe)\n","75140540":"# Import modules specific for this section\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\n# Proportions we want to split in (must sum to 1)\nsplit_props = pd.Series({\n    'train': 0.7,\n    'validation': 0.15,\n    'holdout': 0.15\n})\n\n\n# Split out training data\ndf_train,df_not_train = train_test_split(\n    data_new, test_size=(1 - split_props['train']), random_state=51, shuffle=True\n)\n# Split remaining data between validation and holdout\ndf_validation, df_holdout= train_test_split(\n    df_not_train, test_size=(split_props['holdout'] \/ (1 - split_props['train'])), random_state=13, shuffle=True\n)\n\n\ny_train = df_train.filter(['ClaimNb'])\n\n\nx_train = df_train.drop(columns=['ClaimNb','IDpol','freq','Exposure'])\nx_train_weight = df_train.filter(['Exposure'])\n\ntrain_dmatrix = xgb.DMatrix(data=x_train,label=y_train,weight=x_train_weight)\n\ny_valid = df_validation.filter(['ClaimNb'])\n\nx_valid = df_validation.drop(columns=['ClaimNb','IDpol','freq','Exposure'])\nx_valid_weight = df_validation.filter(['Exposure'])\n\nvalid_dmatrix = xgb.DMatrix(data=x_valid,label=y_valid,weight=x_valid_weight)\n\n\nprint(df_validation.describe)\nprint(x_valid.describe)\n\n","2848faaa":"import xgboost as xgb\n\nparams={'eval_metric':'poisson-nloglik',\"objective\": \"count:poisson\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n                'max_depth': 5,'min_child_weight':1, 'reg_alpha': 10}\n\nlog_exp= np.log(df_train.filter(['Exposure']))\n\ntrain_dmatrix.set_base_margin(log_exp)\n\n\nxgmodel = xgb.train(\n    params=params,\n    dtrain=train_dmatrix,\n    num_boost_round=999,\n    early_stopping_rounds=10,\n    evals=[(train_dmatrix, \"train\"),(valid_dmatrix,\"valid\")],\n    verbose_eval=False\n)\n\n\n    \nxgb.plot_tree(xgmodel,num_trees=0)\nplt.rcParams['figure.figsize'] = [10,10]\nplt.show()\n    \nxgb.plot_importance(xgmodel)\nplt.rcParams['figure.figsize'] = [10, 10]\nplt.show()\n\n","aacbe614":"from sklearn.metrics import mean_absolute_error\n\ny_train= df_train.iloc[:,1]\ny_valid = df_validation.iloc[:,1]\n# \"Learn\" the mean from the training data\nmean_train = np.mean(y_train)\n\n# Get predictions on the test set\nbaseline_predictions = np.ones(y_valid.shape)\n\n# Compute MAE\nmae_baseline = mean_absolute_error(y_valid, baseline_predictions)\n\nprint(\"Baseline MAE is {:.2f}\".format(mae_baseline))\n\nprint(\"Best Poisson Likelihood: {:.2f} with {} rounds\".format(\n                 xgmodel.best_score,\n                 xgmodel.best_iteration+1))\n","45d4a087":"params={'eval_metric':'poisson-nloglik',\"objective\": \"count:poisson\",'colsample_bytree':0.9 ,'learning_rate':0.1 ,\n                'max_depth':3 ,'min_child_weight':1, 'reg_alpha':5, 'subsample':0.9 }\n\nlog_exp= np.log(df_train.filter(['Exposure']))\n\ntrain_dmatrix.set_base_margin(log_exp)\n\n\nxgmodel = xgb.train(\n    params=params,\n    dtrain=train_dmatrix,\n    num_boost_round=65,\n    early_stopping_rounds=10,\n    evals=[(train_dmatrix, \"train\"),(valid_dmatrix,\"valid\")],\n    verbose_eval=False\n)\n\n\n    \nxgb.plot_tree(xgmodel,num_trees=0)\nplt.rcParams['figure.figsize'] = [10,10]\nplt.show()\n    \nxgb.plot_importance(xgmodel)\nplt.rcParams['figure.figsize'] = [10, 10]\nplt.show()\n\n","c2501833":"print(xgmodel)\npredictions_valid=xgmodel.predict(valid_dmatrix)\n\n","957536fc":"xg_reg = xgb.XGBRegressor(eval_metric='poisson-nloglik',objective= \"count:poisson\",colsample_bytree=0.9 ,learning_rate=0.1 ,\n                max_depth=3 ,min_child_weight=1, reg_alpha=5, subsample=0.9)\n\nxg_reg.fit(x_train,y_train)\n\nreg_pred = xg_reg.predict(x_valid)","28a88499":"\nimport matplotlib.pyplot as plt\nimport scikitplot as skplt\n\n# Reg_Pred\nregpredictions=pd.Series(reg_pred)\nregpredictions.rename(columns={\"0\":\"ClaimNb\"})\nregpredictions = regpredictions.reset_index()\ndel regpredictions['index']\n\n# Prediction\npredictions=pd.Series(predictions_valid)\npredictions.rename(columns={\"0\":\"ClaimNb\"})\npredictions = predictions.reset_index()\ndel predictions['index']\n\n\n#Actual\n\nactual_valid = df_validation.iloc[:,8]\nactual=pd.Series(actual_valid)\nactual = actual.reset_index()\ndel actual['index']\n\n#Exposure \nexposure = df_validation.iloc[:,2]\nexposure=pd.Series(exposure)\nexposure = exposure.reset_index()\ndel exposure['index']\n\nprint(regpredictions)\nprint(predictions)\nprint(actual)","622b0fd5":"nppred=np.array(predictions)\nnpactual=np.array(actual)\nnpexposure=np.array(exposure)\n\nnpregpred=np.array(regpredictions)\n\nresidual=npactual-nppred\nreg_residual = npactual-npregpred\n\n\noverall_res=np.average(residual, weights=npexposure)\noverall_reg_res=np.average(reg_residual, weights=npexposure)\n\nprint(overall_res)\nprint(overall_reg_res)","b6bdb481":"print(df_validation.describe())","e89c077a":"\nfiltered_pred_valid_set=df_validation.filter(['IDpol','ClaimNb','Exposure','freq'])\n\nfiltered_pred_valid_set = filtered_pred_valid_set.reset_index()\ndel filtered_pred_valid_set['index']\n\ndf_validation = df_validation.reset_index()\ndel df_validation['index']\n\npredicted_validation_set = df_validation.merge(predictions, left_index=True, right_index=True)\npredicted_validation_set.columns.values[-1] = 'pred_ClaimNb'\n\nfilt_pred_valid_set = filtered_pred_valid_set.merge(predictions, left_index=True, right_index=True)\nfilt_pred_valid_set.columns.values[-1] = 'pred_ClaimNb'\n\nfilt_regpred_valid_set = filtered_pred_valid_set.merge(regpredictions, left_index=True, right_index=True)\nfilt_regpred_valid_set.columns.values[-1] = 'pred_ClaimNb'\n\npredicted_reg_validation_set = df_validation.merge(regpredictions, left_index=True, right_index=True)\npredicted_reg_validation_set.columns.values[-1] = 'pred_ClaimNb'\n\npredicted_validation_set.to_pickle('xgb_pred_valid_set_new.gzip')\nfilt_pred_valid_set.to_pickle('xgb_filtered_pred_valid_set_new.gzip')\nfilt_regpred_valid_set.to_pickle('xgb_filt_reg_pred_valid_set_new.gzip')\npredicted_reg_validation_set.to_pickle('xgb_pred_reg_valid_set_new.gzip')\n\nx_train.to_pickle('xtrain.gzip')\n\nprint(filt_pred_valid_set.describe)\nprint(predicted_validation_set.describe)\n\nimport pickle\nfile_name = \"regressorxgbmodel.pkl\"\nfile_name2 = \"xgbmodel.pkl\"\n\n\n# save\npickle.dump(xg_reg, open(file_name, \"wb\"))\npickle.dump(xgmodel, open(file_name2, \"wb\"))\n","0e860b37":"print(filt_pred_valid_set.describe())","672d675b":"Exposure\n* median of 6\n* Exposure is concentrated from 1-15\n\nFrequency\n* Frequency high at Vehicle age 0\n* Also high for some very high Vehicle Ages, though EVY is minimal","d1cda6a4":"Exposure\n* large amount of our data has a default Bonus Malus of 50, and the rest has minimal exposure.\n\nFrequency\n* Frequency low for Bonus Malus of 50, where most EVY is concentrated\n* Very high at some high values, but with small EVY","cc870d98":"The standard deviation is very high for density, it seems that there are a large amount of different density. So we want to create a new variable which takes log(density):","dbf698fe":"Exposure\n* Median of 44.\n* Exposure largely concentrated from ~30 to ~60.\n\nFrequency \n* As expected, we see a rise in Frequency in young ages and old ages","185079a5":"# Data Exploration\nTrying to understand each of the explanatory variables (based on the subset that we have).\n\nThese graphs will give us a good visualisation of how Frequency and Exposure behave on these variables. Whilst we have a rough idea how it might look like, we are looking at France and not the UK hence we might find something else.\n\nFrequency capped at 2.\n","9a52b680":"### Density\n","9f4c54f0":"### Bonus Malus","795256c8":"### Residual","7971dae9":"### Final Model","b5ac85cd":"### XgBoost\n","7d881a6e":"Area looks clean and already grouped up as well.\n\nExposure\n* Good amount of EVY for every Area except for F\n\nFrequency\n* Trend of higher frequency from A to F. F hinted to be a risky area","03046311":"### Lift and Residual","1da0e307":"\n### Importing packages and dataset. \n\nAlso Creating Frequency variable (Claims\/Exposure)","d4083b4d":"### Creating a Function to create visualisation of Exposure and Claim Frequency over factors","3e6667c7":"XGB Regressor","04cfcc65":"### Vehicle Gas","712c9c5c":"### Saving the model and predicted validation dataset","1e06cabb":"### Driver Age","fc72caee":"### Vehicle Age","880e221f":"### Vehicle Brand","9b3a488a":"### Creating a subset","c1038b1c":"### Splitting Data and construction DMatrix","da929ab6":"Looking for a possible chance to merge Map data with the French Motor Claims Project, however it seems that the data is split into 'Department' areas,different to the French Claims data, where it is split into the 22 Regions (old Region mapping).","8d3e24aa":"### One hot encoding variables to make them suitable for XGBoost","b50776e2":"### Vehicle Power","9e1aa404":"Default Model","c1f3adb5":"### Region","aabc82c6":"### Checking if there are any missing values","ba14b463":"We have logDensity as a continous numeric variable. This is good to be an input for our model.","6b50df80":"# Models","54e99eac":"1. Check the data\n2. Split Train\/Validation\/Holdout Data\n3. Model \n4. Fitting\n5. Predict","82613405":"### Area","f0c32bb1":"\n#  **Introduction**\nFrench Motor Claims:\n\nA dataset including \n\n* Policy No: ID\n* 9 Explanatory Variables: Driver Age, Vehicle Age, Vehicle Power, Density, Bonus Malus, Area, Vehicle Brand, Vehicle Gas, Region\n* Weight: Exposure\n* Response Variable: Claims Number\n\nObjectives:\n1. Data Exploration \/ Pre-Process\n    Want to understand our explanatory variables to make suitable grouped and banded variables\n2. Testing Models\n    Using different techniques such as xgBoost, GLM etc\n3. Finding the best model\n    ","8173f854":"Exposure \n* largely concentrated around 4-7. With median of 6.\n\nFrequency\n* Does not give us a good idea of any trends or correlation"}}