{"cell_type":{"fde28e51":"code","6f08499d":"code","42b3b622":"code","0eb76d48":"code","e77fd4b0":"code","1a1df504":"code","b3eeb7ac":"code","48ac6a4e":"code","aa2a794f":"code","4bb80fc1":"code","3f79d99e":"code","4a20e96f":"code","b62b6950":"code","94a129b5":"code","cef74f19":"code","830a05a2":"code","78db7725":"code","1f0cf795":"markdown","3a0bbe95":"markdown","1b297836":"markdown","14cc7259":"markdown","3574dae2":"markdown","5c3a5f0e":"markdown","9db27711":"markdown","79f1f9ab":"markdown","4f2ef14e":"markdown","6f8fe746":"markdown","5820ba52":"markdown","d01ef720":"markdown","4fb81b5d":"markdown","33ccc779":"markdown","3ef6ab4a":"markdown","52bf1c5a":"markdown","5e22bafd":"markdown","f12b8747":"markdown","45c4a1a2":"markdown","c5708b12":"markdown","620705c9":"markdown"},"source":{"fde28e51":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6f08499d":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\n\n# for regression problems\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\n# for classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n# to split and standarize the datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# to evaluate regression models\nfrom sklearn.metrics import mean_squared_error\n\n# to evaluate classification models\nfrom sklearn.metrics import roc_auc_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","42b3b622":"# load the Titanic Dataset with a few variables for demonstration\n\ndata = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', usecols = ['Age', 'Fare','Survived'])\ndata.head()","0eb76d48":"# let's look at the percentage of NA\n\ndata.isnull().mean()","e77fd4b0":"# let's separate into training and testing set\n\nX_train, X_test, y_train, y_test = train_test_split(data, data.Survived, test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape","1a1df504":"# let's make a function to create 2 variables from Age:\n# one filling NA with median, and another one filling NA with zeroes\n\ndef impute_na(df, variable, median):\n    df[variable+'_median'] = df[variable].fillna(median)\n    df[variable+'_zero'] = df[variable].fillna(0)","b3eeb7ac":"median = X_train.Age.median()\nmedian","48ac6a4e":"impute_na(X_train, 'Age', median)\nX_train.head(15)","aa2a794f":"impute_na(X_test, 'Age', median)","4bb80fc1":"# we can see a change in the variance after imputation\n\nprint('Original Variance: ', X_train['Age'].std())\nprint('Variance after median imputation: ', X_train['Age_median'].std())","3f79d99e":"# we can see that the distribution has changed slightly with now more values accumulating towards the median\nfig = plt.figure()\nax = fig.add_subplot(111)\nX_train['Age'].plot(kind='kde', ax=ax)\nX_train.Age_median.plot(kind='kde', ax=ax, color='red')\nlines, labels = ax.get_legend_handles_labels()\nax.legend(lines, labels, loc='best')","4a20e96f":"# filling NA with zeroes creates a peak of population around 0, as expected\nfig = plt.figure()\nax = fig.add_subplot(111)\nX_train['Age'].plot(kind='kde', ax=ax)\nX_train.Age_zero.plot(kind='kde', ax=ax, color='red')\nlines, labels = ax.get_legend_handles_labels()\nax.legend(lines, labels, loc='best')","b62b6950":"# Let's compare the performance of Logistic Regression using Age filled with zeros or alternatively the median\n\n# model on NA imputed with zeroes\nlogit = LogisticRegression(random_state=44, C=1000) # c big to avoid regularization\nlogit.fit(X_train[['Age_zero', 'Fare']], y_train)\nprint('Train set zero imputation')\npred = logit.predict_proba(X_train[['Age_zero', 'Fare']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\nprint('Test set zero imputation')\npred = logit.predict_proba(X_test[['Age_zero', 'Fare']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))\nprint()\n\n# model on NA imputed with median\nlogit = LogisticRegression(random_state=44, C=1000) # c big to avoid regularization\nlogit.fit(X_train[['Age_median', 'Fare']], y_train)\nprint('Train set median imputation')\npred = logit.predict_proba(X_train[['Age_median', 'Fare']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\nprint('Test set median imputation')\npred = logit.predict_proba(X_test[['Age_median', 'Fare']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))","94a129b5":"print('Average total survival:', X_train.Survived.mean())","cef74f19":"print('Average real survival of children: ', X_train[X_train.Age<15].Survived.mean())\nprint('Average survival of children when using Age imputed with zeroes: ', X_train[X_train.Age_zero<15].Survived.mean())\nprint('Average survival of children when using Age imputed with median: ', X_train[X_train.Age_median<15].Survived.mean())","830a05a2":"# Let's compare the performance of SVM using Age filled with zeros or alternatively the median\n\nSVM_model = SVC(random_state=44, probability=True, max_iter=-1, kernel='linear',)\nSVM_model.fit(X_train[['Age_zero', 'Fare']], y_train)\nprint('Train set zero imputation')\npred = SVM_model.predict_proba(X_train[['Age_zero', 'Fare']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\nprint('Test set zero imputation')\npred = SVM_model.predict_proba(X_test[['Age_zero', 'Fare']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))\nprint()\nSVM_model = SVC(random_state=44, probability=True,  max_iter=-1, kernel='linear')\nSVM_model.fit(X_train[['Age_median', 'Fare']], y_train)\nprint('Train set median imputation')\npred = SVM_model.predict_proba(X_train[['Age_median', 'Fare']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\nprint('Test set median imputation')\npred = SVM_model.predict_proba(X_test[['Age_median', 'Fare']])\nprint('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))\nprint()","78db7725":"# Let's compare the performance of Random Forests using Age filled with zeros or alternatively the median\n\nrf = RandomForestClassifier(n_estimators=100, random_state=39, max_depth=3)\nrf.fit(X_train[['Age_zero', 'Fare']], y_train)\nprint('Train set zero imputation')\npred = rf.predict_proba(X_train[['Age_zero', 'Fare']])\nprint('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\nprint('Test set zero imputation')\npred = rf.predict_proba(X_test[['Age_zero', 'Fare']])\nprint('Random Forests zero imputation roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))\nprint()\nrf = RandomForestClassifier(n_estimators=100, random_state=39, max_depth=3)\nrf.fit(X_train[['Age_median', 'Fare']], y_train)\nprint('Train set median imputation')\npred = rf.predict_proba(X_train[['Age_median', 'Fare']])\nprint('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\nprint('Test set median imputation')\npred = rf.predict_proba(X_test[['Age_median', 'Fare']])\nprint('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))\nprint()","1f0cf795":"Filling NA with 0s also distorts the distribution of the original variable, generating an accumulation of values around 0. We will see in the next lecture a method of NA imputation that preserves variable distribution.","3a0bbe95":"**We see that median imputation leads to better performance of the logistic regression. Why?**","1b297836":"## Imputation important\nImputation should be done over the training set, and then propagated to the test set. This means that the mean\/median to be used to fill missing values both in train and test set, should be extracted from the train set only. And this is to avoid overfitting.","14cc7259":"## Final Note :\nReplacement of NA with mean\/median is widely used in the data science community and in various data science competitions. See for example the winning solution of the KDD 2009 cup: \"Winning the KDD Cup Orange Challenge with Ensemble Selection\" (http:\/\/www.mtome.com\/Publications\/CiML\/CiML-v3-book.pdf).\n\nTypically, mean\/median imputation is done together with adding a variable to capture those observations where the data was missing (see lecture \"Creating a new variable with the missing data\"), thus covering 2 angles: if the data was missing completely at random, this would be contemplated by the mean imputation, and if it wasn't this would be captured by the additional variable.\n\nIn addition, both methods are extremely straight forward to implement, and therefore are a top choice in data science competitions.\n<font color='red'><hr><\/font>\n<hr>","3574dae2":"**Random Forests, as well as SVMs, perform better with median imputation, compared with replacing with zeroes.**","5c3a5f0e":"Imputation is the act of replacing missing data with statistical estimates of the missing values. The goal of any imputation technique is to produce a complete dataset that can then be then used for machine learning.\n\nMean\/median imputation consists of replacing all occurrences of missing values (NA) within a variable by the mean (if the variable has a Gaussian distribution) or median (if the variable has a skewed distribution).\n\n\n","9db27711":"Children were more likely to survive the catastrophe (0.57 for children vs 0.38 for the entire Titanic). Thus, smaller values of Age are a good indicator of survival.\n\nWhen we replace NA with zeroes, we are masking the predictive power of Age. After zero imputation it looks like children did not have a greater chance of survival, and therefore the model loses predictive power.\n\nOn the other hand, replacing NA with the median, preserves the predictive power of the variable Age, as smaller Age values will favour survival.","79f1f9ab":"## Machine learning model performance on different imputation methods","4f2ef14e":"### <font color=\"red\">Support Vector Machine<\/font>","6f8fe746":"## Real Life Example : \n\n### Predicting Survival on the Titanic: understanding society behaviour and beliefs\n\nPerhaps one of the most infamous shipwrecks in history, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 people on board. Interestingly, by analysing the probability of survival based on few attributes like gender, age, and social status, we can make very accurate predictions on which passengers would survive. Some groups of people were more likely to survive than others, such as women, children, and the upper-class. Therefore, we can learn about the society priorities and privileges at the time.\n\n\nIn the following cells, I will demonstrate mean\/median imputation using the Titanic dataset.","5820ba52":"\nAs mentioned above, the median imputation distorts the original distribution of the variable Age. The transformed variable shows more values around the median value.","d01ef720":"### <font color='red' >That is all for this notebook. I hope you enjoyed this notebook, if you like it then please upvote \ud83d\ude0a\ud83d\ude0a.<\/font>","4fb81b5d":"When replacing NA with the mean or median, the variance of the variable will be distorted if the number of NA is big respect to the total number of observations (since the imputed values do not differ from the mean or from each other). Therefore leading to underestimation of the variance.\n\nIn addition, estimates of covariance and correlations with other variables in the dataset may also be affected. This is because we may be destroying intrinsic correlations since the mean\/median that now replace NA will not preserve the relation with the remaining variables.","33ccc779":"### <font color=\"red\">Random Forests<\/font>","3ef6ab4a":"## Advantages\n1. Easy to implement\n2. Fast way of obtaining complete datasets","52bf1c5a":"**For SVM as well, median imputation leads to better performance of the model, compared to replacing NA with zeroes.**","5e22bafd":"**Mean\/median imputation alters the variance of the original distribution of the variable**","f12b8747":"## Limitations\n\n1. Distortion of original variance\n2. Distortion of covariance with remaining variables within the datase","45c4a1a2":"### <font color=\"red\">Logistic Regression<\/font>","c5708b12":"# <font color='black'>Mean and median imputation<\/font>","620705c9":"## Assumptions\n\nMean\/median imputation has the assumption that the data are missing completely at random (MCAR). If this is the case, we can think of replacing the NA with the most frequent occurrence of the variable, which is the mean if the variable has a Gaussian distribution, or the median otherwise.\n\nThe rationale is to replace the population of missing values with the most frequent value, since this is the most likely occurrence."}}