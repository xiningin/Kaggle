{"cell_type":{"94c2dac8":"code","4ef8bbd0":"code","82d25850":"code","6dec3188":"code","286aa8cc":"code","7ab7b842":"code","79a8b472":"code","b7717323":"code","b354da16":"code","cad7d5c0":"code","c7d0fab5":"code","0b0344d9":"code","0a8e2720":"code","d1181ce5":"code","e0e3ef15":"code","122d5f14":"code","07f065e7":"code","97041e07":"code","10dd8902":"code","85f04a34":"code","3aa64d72":"code","2b543f3f":"code","f07a753c":"code","1fbea780":"code","1e88c65a":"code","c5bae86b":"code","b32f7580":"code","dcf663d9":"code","188a3fa5":"code","383f9bf8":"code","9bbfee5d":"code","6e9a6b04":"code","b9bb2edc":"markdown"},"source":{"94c2dac8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        break\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4ef8bbd0":"os.listdir('\/kaggle\/input\/petfinder-pawpularity-score')","82d25850":"IMG_PATH = '\/kaggle\/input\/petfinder-pawpularity-score\/train\/'","6dec3188":"df_train = pd.read_csv('\/kaggle\/input\/petfinder-pawpularity-score\/train.csv')","286aa8cc":"df_train.shape","7ab7b842":"df_train.head()","79a8b472":"import io\nimport shutil, random\nimport tensorflow as tf\nimport tensorflow_addons as tfa","b7717323":"from PIL import Image\nfrom pathlib import Path\nfrom tensorflow import keras\nfrom keras import backend as K\nfrom keras.utils import np_utils\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras import layers\nfrom keras.preprocessing import image\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator","b354da16":"weight_decay = 0.0001\nbatch_size = 64\nnum_epochs = 5\ndropout_rate = 0.2\nimage_size = 224  # We'll resize input images to this size.\npatch_size = 8  # Size of the patches to be extracted from the input images.\nnum_patches = (image_size \/\/ patch_size) ** 2  # Size of the data array.\nembedding_dim = 256  # Number of hidden units.\nnum_blocks = 4  # Number of blocks.\nnum_classes = 1","cad7d5c0":"image_data = []\nlabels = []\n\nfor idx in range(df_train.shape[0]):\n#     print(df_train['Id'][idx], df_train['Pawpularity'][idx])\n    img_path = IMG_PATH + str(df_train['Id'][idx]) + '.jpg'\n    try:\n        img = tf.keras.preprocessing.image.load_img(img_path, color_mode='rgb', target_size= (image_size, image_size))\n        img = np.array(img)\n        image_data.append(img)\n        labels.append(df_train['Pawpularity'][idx])\n    except:\n        ...\n    else:\n        ...","c7d0fab5":"print(len(image_data),len(labels))","0b0344d9":"combined = list(zip(image_data,labels))\nrandom.shuffle(combined)","0a8e2720":"X_train = np.array(image_data)\nY_train = np.array(labels)\nY_train = Y_train.reshape((Y_train.shape[0], 1))\nY_train = Y_train.astype(\"float32\")\nprint(X_train.shape)\nprint(Y_train.shape)","d1181ce5":"x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)","e0e3ef15":"y_train.dtype","122d5f14":"\"\"\"\n## Use data augmentation\n\"\"\"\n\ndata_augmentation = keras.Sequential(\n    [\n        layers.Normalization(),\n        layers.Resizing(image_size, image_size),\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomFlip(\"vertical\"),\n        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n    ],\n    name=\"data_augmentation\",\n)","07f065e7":"\"\"\"\n## Build a regression model\nWe implement a method that builds a regression model given the processing blocks.\n\"\"\"\n\n\ndef build_classifier(blocks, positional_encoding=False):\n    inputs = layers.Input(shape=(image_size, image_size, 3))\n    # Augment data.\n    augmented = data_augmentation(inputs)\n    # Create patches.\n    patches = Patches(patch_size, num_patches)(augmented)\n    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n    x = layers.Dense(units=embedding_dim)(patches)\n    if positional_encoding:\n        positions = tf.range(start=0, limit=num_patches, delta=1)\n        position_embedding = layers.Embedding(\n            input_dim=num_patches, output_dim=embedding_dim\n        )(positions)\n        x = x + position_embedding\n    # Process x using the module blocks.\n    x = blocks(x)\n    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor.\n    representation = layers.GlobalAveragePooling1D()(x)\n    # Apply dropout.\n    representation = layers.Dropout(rate=dropout_rate)(representation)\n    # Compute logits outputs.\n    logits = layers.Dense(num_classes, activation = 'linear')(representation)\n    # Create the Keras model.\n    return keras.Model(inputs=inputs, outputs=logits)","97041e07":"def root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true))) ","10dd8902":"\"\"\"\n## Define an experiment\nWe implement a utility function to compile, train, and evaluate a given model.\n\"\"\"\n\n\ndef run_experiment(model):\n    # Create Adam optimizer with weight decay.\n    optimizer = tfa.optimizers.AdamW(\n        learning_rate=learning_rate, weight_decay=weight_decay,\n    )\n    \n    # Compile the model.\n    model.compile(\n        optimizer = optimizer,\n        loss = root_mean_squared_error,\n        metrics=[\n            keras.metrics.RootMeanSquaredError()\n        ],\n    )\n    \n    # Fit the model.\n    history = model.fit(\n        x=x_train,\n        y=y_train,\n        batch_size=batch_size,\n        epochs=num_epochs,\n        validation_split=0.1,\n    )\n    \n    print(model.evaluate(x_test, y_test))\n\n    # Return history to plot learning curves.\n    return history\n    \n#     _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n#     print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n#     print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n    \n    ","85f04a34":"\"\"\"\n## Implement patch extraction as a layer\n\"\"\"\n\n\nclass Patches(layers.Layer):\n    def __init__(self, patch_size, num_patches):\n        super(Patches, self).__init__()\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=[1, self.patch_size, self.patch_size, 1],\n            strides=[1, self.patch_size, self.patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        patch_dims = patches.shape[-1]\n        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n        return patches","3aa64d72":"\"\"\"\n## The FNet model\nThe FNet uses a similar block to the Transformer block. However, FNet replaces the self-attention layer\nin the Transformer block with a parameter-free 2D Fourier transformation layer:\n1. One 1D Fourier Transform is applied along the patches.\n2. One 1D Fourier Transform is applied along the channels.\n\"\"\"\n\n\"\"\"\n### Implement the FNet module\n\"\"\"\n\nclass FNetLayer(layers.Layer):\n    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n        super(FNetLayer, self).__init__(*args, **kwargs)\n\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(units=embedding_dim),\n                tfa.layers.GELU(),\n                layers.Dropout(rate=dropout_rate),\n                layers.Dense(units=embedding_dim),\n            ]\n        )\n\n        self.normalize1 = layers.LayerNormalization(epsilon=1e-6)\n        self.normalize2 = layers.LayerNormalization(epsilon=1e-6)\n\n    def call(self, inputs):\n        # Apply fourier transformations.\n        x = tf.cast(\n            tf.signal.fft2d(tf.cast(inputs, dtype=tf.dtypes.complex64)),\n            dtype=tf.dtypes.float32,\n        )\n        # Add skip connection.\n        x = x + inputs\n        # Apply layer normalization.\n        x = self.normalize1(x)\n        # Apply Feedfowrad network.\n        x_ffn = self.ffn(x)\n        # Add skip connection.\n        x = x + x_ffn\n        # Apply layer normalization.\n        return self.normalize2(x)","2b543f3f":"\"\"\"\n### Build, train, and evaluate the FNet model\n\"\"\"\n\nfnet_blocks = keras.Sequential(\n    [FNetLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\n)\nlearning_rate = 0.001\nfnet_classifier = build_classifier(fnet_blocks, positional_encoding=True)\nhistory = run_experiment(fnet_classifier)","f07a753c":"df_test = pd.read_csv('\/kaggle\/input\/petfinder-pawpularity-score\/test.csv')","1fbea780":"df_test.head()","1e88c65a":"sample_sub = df_test = pd.read_csv('\/kaggle\/input\/petfinder-pawpularity-score\/sample_submission.csv')","c5bae86b":"sample_sub.head()","b32f7580":"TEST_IMG_PATH = '\/kaggle\/input\/petfinder-pawpularity-score\/test\/'","dcf663d9":"output = {\n    'Id' : [],\n    'Pawpularity' : []\n}\n\nfor idx in range(df_test.shape[0]):\n#     print(df_train['Id'][idx], df_train['Pawpularity'][idx])\n    img_path = TEST_IMG_PATH + str(df_test['Id'][idx]) + '.jpg'\n    try:\n        image = tf.keras.preprocessing.image.load_img(img_path, color_mode='rgb', target_size= (image_size, image_size))\n        image = np.array(image)\n        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n        preds = fnet_classifier.predict(image)\n        output['Id'].append(df_test['Id'][idx])\n        output['Pawpularity'].append(preds[0][0])\n#         print(preds[0][0])\n    except:\n        ...\n    else:\n        ...","188a3fa5":"sub = pd.DataFrame(output)","383f9bf8":"sub.shape","9bbfee5d":"sub.head()","6e9a6b04":"sub.to_csv('submission.csv', index = False)","b9bb2edc":"## Using FNET : [Paper_Link](https:\/\/arxiv.org\/abs\/2105.03824)"}}