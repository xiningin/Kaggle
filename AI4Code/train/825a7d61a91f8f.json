{"cell_type":{"b5d406ef":"code","47442bc7":"code","f31e6b66":"code","181f464e":"code","cdbdac0d":"code","9df27515":"code","725520ce":"code","cbf9e77b":"code","d51f281b":"code","e9c15e08":"code","3072efb1":"code","f7f5a28a":"code","f13aff3a":"code","61a99a21":"code","0bf97e21":"code","8cac8381":"code","862e5a09":"code","2d6bc2e4":"code","773394ff":"code","f232c84e":"markdown","f7a08ed3":"markdown","5082a826":"markdown","0ae7fb6d":"markdown","2718a411":"markdown","548a915f":"markdown","9e0d6a78":"markdown","e58c6423":"markdown","e03a7f0a":"markdown","686a1efc":"markdown","468c82b6":"markdown","baf2910b":"markdown","b78e4ee7":"markdown","01030dbe":"markdown","1b089d51":"markdown","8b74b3bb":"markdown","0e703008":"markdown"},"source":{"b5d406ef":"import pandas as pd\nimport numpy as np\nimport random\nfrom sklearn.metrics import mean_squared_error\nimport os\nimport matplotlib.pyplot as plt\nfrom scipy.sparse.linalg import svds\nfrom mpl_toolkits import mplot3d","47442bc7":"ratings_list = [i.strip().split(\"::\") for i in open('..\/input\/movielens-1m-dataset\/ratings.dat', 'r').readlines()]\nratings_df = pd.DataFrame(ratings_list, columns = ['UserID', 'MovieID', 'Rating', 'Timestamp'], dtype = int)\nratings_df['Rating']=ratings_df['Rating'].apply(pd.to_numeric)","f31e6b66":"ratings_df.head()","181f464e":"ratings_df.Rating.describe()","cdbdac0d":"print(\"#users: \"+ str(ratings_df.UserID.nunique()))\nprint(\"#movies: \"+ str(ratings_df.MovieID.nunique()))\nprint(\"#ratings %: \"+ str(ratings_df.shape[0]\/(ratings_df.UserID.nunique()*ratings_df.MovieID.nunique())*100))\n\n\np = ratings_df.groupby('Rating')['Rating'].agg(['count'])\n\nmovie_count = ratings_df[\"MovieID\"].nunique()\n\nuser_count = ratings_df['UserID'].nunique()\n\nrating_count = ratings_df.shape[0]\n\nax = p.plot(kind = 'barh', legend = False, figsize = (15,10))\nplt.title('{:,} Ratings for {:,} diffrent movies, given by {:,} users'.format(rating_count, movie_count, user_count), fontsize=20)\nplt.axis('off')\n\nfor i in range(1,6):\n    ax.text(p.iloc[i-1][0]\/4, i-1, 'Rating {}: {:.0f}%'.format(i, p.iloc[i-1][0]*100 \/ p.sum()[0]), color = 'white', weight = 'bold')","9df27515":"user_rating_count = ratings_df.groupby('UserID')['UserID'].agg(['count'])\ntop_raters = user_rating_count.nlargest(5,'count')\nworst_raters = user_rating_count.nsmallest(5,'count')\nprint(\"Top movie raters\")\nprint(top_raters)\nprint()\nprint(\"Worst movie raters\")\nprint(worst_raters)","725520ce":"R_df = ratings_df.pivot(index = 'UserID', columns ='MovieID', values = 'Rating').fillna(0)\n\nR = R_df.values\nuser_ratings_mean = np.mean(R, axis = 1)\nR_demeaned = R - user_ratings_mean.reshape(-1, 1)\n\nR_df.head()","cbf9e77b":"rated_indices = np.argwhere(R_demeaned>0)\nrandom.seed(123)\ntest_set = random.sample(list(rated_indices),500)\n\nprint(\"1st index is \"+str(test_set[0])+\" and its rating is \"+str(R_demeaned[test_set[0][0],test_set[0][1]]))\n\ntest_ratings = []\nfor i in range(len(test_set)):\n    test_ratings.append(R_demeaned[test_set[i][0],test_set[i][1]])\n    R_demeaned[test_set[i][0],test_set[i][1]]=0\n\nprint(\"1st index is \"+str(test_set[0])+\" and its rating is \"+str(R_demeaned[test_set[0][0],test_set[0][1]]))","d51f281b":"def printRMSEGraph(kArr, RMSEArr):\n    plt.plot(kArr, RMSEArr)\n    plt.xlabel(\"Num of latent features\")\n    plt.ylabel(\"RMSE\")\n    \ndef svdPrediction(kArr):\n    rmses = []\n    for k in kArr:\n        U, sigma, Vt = svds(R_demeaned, k = k)\n        sigma = np.diag(sigma)\n        predicted_ratings = np.dot(np.dot(U, sigma), Vt) + user_ratings_mean.reshape(-1, 1)\n        preds_df = pd.DataFrame(predicted_ratings, columns = R_df.columns)\n        pred_set = [predicted_ratings[x[0],x[1]] for x in test_set]\n\n        res_df = pd.DataFrame()\n        res_df[\"act\"] = test_ratings\n        res_df[\"pred\"] = pred_set\n\n        rmse = np.sqrt(mean_squared_error(res_df[\"act\"].values, res_df[\"pred\"].values))\n        rmses.append(rmse)\n        print(f\"k: {k}; RMSE: {rmse}\")\n    return rmses","e9c15e08":"kArr = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\nrmses = svdPrediction(kArr)\nprintRMSEGraph(kArr, rmses)","3072efb1":"kArr_2 = [40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90]\nrmses = svdPrediction(kArr_2)\nprintRMSEGraph(kArr_2, rmses)","f7f5a28a":"def svtPrediction(tau, delta, epsilon, max_iterations):\n    Y = np.zeros_like(R_demeaned)\n    mask = np.round(R_demeaned > 0).astype(int)\n    print(\"==========================================================\")\n    print(f\"Parameters: tau = {tau}, delta = {delta}\")\n\n    for i in range(max_iterations):\n        U, S, V = svds(Y, k=50)\n        S = np.maximum(S - tau, 0)\n        X = np.linalg.multi_dot([U, np.diag(S), V])\n        Y = Y + delta * mask * (R_demeaned - X)\n\n        error = np.linalg.norm(mask * (X - R_demeaned)) \/ np.linalg.norm(mask * R_demeaned)\n        print(f\"Iteration number: {i + 1}; Relative error: {error}\")\n            \n        if error < epsilon:\n            break\n            \n    pred_set = [X[x[0],x[1]] for x in test_set]    \n        \n    res_df = pd.DataFrame()\n    res_df[\"act\"] = test_ratings\n    res_df[\"pred\"] = pred_set\n    \n    differences = abs(res_df[\"act\"] - res_df[\"pred\"])\n    print(f\"The smallest prediction diffrence is {differences.min()} and the biggest is {differences.max()}\")\n\n    rmse = np.sqrt(mean_squared_error(res_df[\"act\"].values, res_df[\"pred\"].values))\n    print(f\"RMSE: {rmse}\")\n    \n    return rmse","f13aff3a":"taus = [0.5, 1, 1.5, 2]\ndeltas = [0.5, 1, 1.5, 2]\nresults = pd.DataFrame(columns=[\"delta\", \"tau\", \"rmse\"])\n\nfor delta in deltas:\n    for tau in taus:\n        rmse= svtPrediction(tau, delta, 0.01, 5)\n        results = results.append(pd.DataFrame([[delta, tau, rmse]], columns=[\"delta\", \"tau\", \"rmse\"]))","61a99a21":"def svtPredictionWithCap(tau, delta, epsilon, max_iterations):\n    Y = np.zeros_like(R_demeaned)\n    mask = np.round(R_demeaned > 0).astype(int)\n    print(\"==========================================================\")\n    print(f\"Parameters: tau = {tau}, delta = {delta}\")\n\n    for i in range(max_iterations):\n        U, S, V = svds(Y, k=50)\n        S = np.maximum(S - tau, 0)\n        X = np.linalg.multi_dot([U, np.diag(S), V])\n        Y = Y + delta * mask * (R_demeaned - X)\n\n        error = np.linalg.norm(mask * (X - R_demeaned)) \/ np.linalg.norm(mask * R_demeaned)\n        print(f\"Iteration number: {i + 1}; Relative error: {error}\")\n            \n        if error < epsilon:\n            break\n            \n    pred_set = [X[x[0],x[1]] for x in test_set]    \n        \n    res_df = pd.DataFrame()\n    res_df[\"act\"] = test_ratings\n    res_df[\"pred\"] = pred_set\n    res_df[\"pred\"] = np.where(res_df[\"pred\"] > 5, 5, res_df[\"pred\"])\n    res_df[\"pred\"] = np.where(res_df[\"pred\"] < 1, 1, res_df[\"pred\"])\n\n    \n    differences = abs(res_df[\"act\"] - res_df[\"pred\"])\n    print(f\"The smallest prediction diffrence is {differences.min()} and the biggest is {differences.max()}\")\n\n    rmse = np.sqrt(mean_squared_error(res_df[\"act\"].values, res_df[\"pred\"].values))\n    print(f\"RMSE: {rmse}\")\n    return rmse","0bf97e21":"taus = [0.5, 1, 1.5, 2]\ndeltas = [0.5, 1, 1.5, 2]\nresults = pd.DataFrame(columns=[\"delta\", \"tau\", \"rmse\"])\nbestResult = [0.0, 0.0, float(\"inf\")]\n\nfor delta in deltas:\n    for tau in taus:\n        rmse= svtPredictionWithCap(tau, delta, 0.01, 5)\n        if rmse < bestResult[2]:\n            bestResult = [delta, tau, rmse]\n        results = results.append(pd.DataFrame([[delta, tau, rmse]], columns=[\"delta\", \"tau\", \"rmse\"]))","8cac8381":"print(\"==========================================================\")\nprint(f\"Best result found for delta: {bestResult[0]} tau: {bestResult[1]} and the RMSE: {bestResult[2]}\")\nprint(\"==========================================================\")\nfig = plt.figure(figsize=(8, 6))\nax = plt.axes(projection='3d')\nax.scatter(results[\"delta\"], results[\"tau\"], results[\"rmse\"], cmap='viridis', linewidth=0.5)\nax.set_xlabel('delta')\nax.set_ylabel('tau')\nax.set_zlabel('RMSE')","862e5a09":"res = svtPredictionWithCap(bestResult[0], bestResult[1], 0.01, 20)","2d6bc2e4":"def svtPredictionWithCapAndDynamicDelta(tau, delta, epsilon, max_iterations):\n    Y = np.zeros_like(R_demeaned)\n    mask = np.round(R_demeaned > 0).astype(int)\n    lastError = float(\"inf\")\n    print(\"==========================================================\")\n    print(f\"Parameters: tau = {tau}, delta = {delta}\")\n\n    for i in range(max_iterations):\n        if i !=0 and i % 3 == 0:\n            delta = delta \/ 2\n            print(f\"Delta decrease to: {delta}\")\n            \n        U, S, V = svds(Y, k=50)\n        S = np.maximum(S - tau, 0)\n        X = np.linalg.multi_dot([U, np.diag(S), V])\n        Y = Y + delta * mask * (R_demeaned - X)\n\n        error = np.linalg.norm(mask * (X - R_demeaned)) \/ np.linalg.norm(mask * R_demeaned)\n        print(f\"Iteration number: {i + 1}; Relative error: {error}\")\n            \n        if error < epsilon or lastError < error:\n            break\n            \n        lastError = error\n            \n    pred_set = [X[x[0],x[1]] for x in test_set]    \n        \n    res_df = pd.DataFrame()\n    res_df[\"act\"] = test_ratings\n    res_df[\"pred\"] = pred_set\n    res_df[\"pred\"] = np.where(res_df[\"pred\"] > 5, 5, res_df[\"pred\"])\n    res_df[\"pred\"] = np.where(res_df[\"pred\"] < 1, 1, res_df[\"pred\"])\n\n    \n    differences = abs(res_df[\"act\"] - res_df[\"pred\"])\n    print(f\"The smallest prediction diffrence is {differences.min()} and the biggest is {differences.max()}\")\n\n    rmse = np.sqrt(mean_squared_error(res_df[\"act\"].values, res_df[\"pred\"].values))\n    print(f\"RMSE: {rmse}\")\n    return rmse","773394ff":"res = svtPredictionWithCapAndDynamicDelta(bestResult[0], bestResult[1], 0.01, 20)","f232c84e":"# Singular Value Decomposition","f7a08ed3":"Now lets try to run our best result parameters for many iterations to see what is the best RMSE we can get","5082a826":"Now we will pivot ratings_df to get userXmovie matrix format.\nWe will also de-mean the data (normalize by each users mean) and convert it from a dataframe to a numpy array.","0ae7fb6d":"# Matrix Completion using Singular Value Decomposition(SVD) and Singular Value Thresholding(SVT)","2718a411":"The problem we are facing is predicting movie ratings, we want to know how user will rate(like) movie he didn't saw. In this Jupyter notebook we will implemnt, analyze and compare diffrent alogrithems use for solving the matrix complition problem. We will use the movie lens dataset.","548a915f":"After exploring our dataset we can learn that we have 3706 users and 6040 movies, where we only have rating for around 1 million couples of (user,movie), which means that our users * ratings matrix will be sparse(we have less then 5% of the values). That means that we need to use matrix complition algorithems to fill the matrix and find predictions for the missing parts!\nWe can also learn that usually users gives movies ratings of 4,5 stars where the average rating is 3.5 and the median is 4.\nWe see that we have some users that ranked many movies(some ranked more than half of the movies), while there others that rank only 20 movies.","9e0d6a78":"create test data by sampling (and removing) 500 ratings from our matrix","e58c6423":"We can see that the Error can be bigger then 5 which means that we predict values not in the range of [1,5], lets try to cap the predictions and see if we can get better results.","e03a7f0a":"# Conclusions","686a1efc":"# Sample test data","468c82b6":"We can see how much the matrix is sparse, where most of the values in the matrix are zeros.","baf2910b":"So with SVD we getting RMSE of ~2.3.\n\nLets try to get better results using the SVT algorithm","b78e4ee7":"## Reading the data and show general statistics","01030dbe":"# Singular value thresholding","1b089d51":"We can see that even though we had more iterations we didn't go better results.\nLets try another improve to the algorithm:\n1. Stop the iterations when the norm relative error don't improve\n2. Decrease delta value on the fly","8b74b3bb":"In this note book we try to solve the matrix complition problem for sparse matrix.\nThe matrix is userXmovie patrix where each cell is the prediction user[x] gave to movie[y]. Most of the cells are empty(have 0) and our goal is to predict empty cells values.\nUsing this algorithm we can recomend to users each movies they should watch( by recommend them movies they will probably rank with high score).\nWe started with review the data and understand the data set we are facing,\nSecondly we start to solve the problem with SVD alogirthm using diffrent number of latent features. After simulating the diffrent runs of SVD we saw that the best result we get is around ~2.3 RMSE and we wanted to see if we can get better results using more complex algorithm.\nThe next algorithm we tryed was SVT, again we run it with diffrent parmeters and found out that we can get much better result, root mean square error of ~1.6 RMSE\n\nThis exercise of matrix compltion was really intresting and it feels like there is still a lot of room for improvment and algorithms to face this question in the futurs.\n\nFor future work it can be intersting to try and combine the matrix complition algorithms we tryed with other algorithm like clustring or k-nn(that base on features and not ratings) to see how much we can improve our results by using the aggregation of diffrent alogorithms.\n\n#### This notebook created by Ori Hershcovich","0e703008":"We can see that the best value is around 64 latent features so lets dig inside this area"}}