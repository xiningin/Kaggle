{"cell_type":{"71b7e0d9":"code","66eaaa2c":"code","b739bab7":"code","a1fa8c40":"code","0cf0ee0c":"code","c17af6b8":"code","4f92146c":"code","4df3519b":"code","9985801e":"code","82bba36a":"code","3581510c":"code","764acef1":"code","1835ff29":"code","564e1821":"code","7d642abd":"code","72aa746f":"code","9296f186":"code","2bb53cf4":"code","fc244b0b":"code","0c315c86":"code","81bf7f71":"code","c22193d2":"code","cb46598f":"code","66f19941":"code","81d3b0e9":"code","b057c4c1":"code","72f36e1f":"code","670e94af":"code","874542db":"markdown","1f0b6bb9":"markdown","accf9c24":"markdown","b1241f96":"markdown","ed542a7f":"markdown","59ed62e3":"markdown","fe730961":"markdown","4097b623":"markdown","cf81682d":"markdown","053b38fc":"markdown","e20f1ea5":"markdown","083b3396":"markdown","83a4225a":"markdown","e25f7066":"markdown","05387926":"markdown","d635d110":"markdown"},"source":{"71b7e0d9":"#Numpy, Pandas\nimport numpy as np\nimport pandas as pd\n\n#Train test split\nfrom sklearn.model_selection import train_test_split\n\n#Pipelines\nfrom sklearn.pipeline import Pipeline\n\n#Transformers\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\n\n#The models\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\n\n#Metrics & evaluation\nfrom sklearn.metrics import mean_squared_error\n\n#Visualisation libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","66eaaa2c":"#Setting up \ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","b739bab7":"#Opening the data description\nf = open(\"..\/input\/house-prices-advanced-regression-techniques\/data_description.txt\", \"r\")\nprint(f.read())","a1fa8c40":"train.head()","0cf0ee0c":"#Note only numeric features can be correlated to price (so only 38 correlations)!\n\ntrain.corrwith(train['SalePrice']).sort_values(ascending=False)","c17af6b8":"plt.figure(figsize=(10,5))\n\nsns.boxenplot(x=train['OverallQual'], y=train['SalePrice'], palette='magma')\nplt.title(\"OverallQual vs SalePrice\")","4f92146c":"g = sns.jointplot(data=train, x=train['GrLivArea'], y=train['SalePrice'], kind='reg')\n\ng.fig.suptitle(\"Above Ground Living Area vs Sale Price\", y=1.1)","4df3519b":"#Finding index of two outliers\n\ntrain.index[(train['GrLivArea']>4500) & (train['SalePrice']<250000)]","9985801e":"train = train.drop([523, 1298], axis=0)\n\n#Plotting new distribution\ng = sns.jointplot(data=train, x=train['GrLivArea'], y=train['SalePrice'], kind='reg')\n\ng.fig.suptitle(\"Above Ground Living Area vs Sale Price With Outliers Removed\", y=1.1)","82bba36a":"plt.figure(figsize=(20,6))\n\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","3581510c":"num_missing = train.isnull().sum()\n\npercent_missing = (num_missing\/len(train)).sort_values(ascending=False)\n\npercent_missing[percent_missing>0]\n\n#sns.barplot(missing_cols, percent_missing, data=train)","764acef1":"train.columns","1835ff29":"#Splitting up the train set into validation data; this will be used to determine optimal model hyperameters.\nX_train, X_valid, y_train, y_valid = train_test_split(train, train['SalePrice'], test_size=0.3, random_state=101)\n\n#X dataframes still contain the target feature. Let's remove it.\nX_train.drop('SalePrice', axis=1, inplace=True)\nX_valid.drop('SalePrice', axis=1, inplace=True)","564e1821":"#Dropping Index column\nX_train.drop('Id', axis=1, inplace=True)\nX_valid.drop('Id', axis=1, inplace=True)\n\n#Dropping 'leaky' columns\nX_train.drop(columns = ['MoSold', 'YrSold', 'SaleType', 'SaleCondition'], axis=1, inplace=True)\nX_valid.drop(columns = ['MoSold', 'YrSold', 'SaleType', 'SaleCondition'], axis=1, inplace=True)","7d642abd":"num_cols = [col for col in X_train.columns if train[col].dtype in ['int', 'float']]\ncat_cols = [col for col in X_train.columns if train[col].dtype == 'object']","72aa746f":"num_pipeline = Pipeline(steps = [\n    ('imputer', SimpleImputer()),\n    ('scaler', StandardScaler())\n])\n\ncat_pipeline = Pipeline(steps = [\n    ('cat_imputer', SimpleImputer(strategy='constant', fill_value='zero')),\n    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n])","9296f186":"#But how do we tell num_pipeline to only handle the numeric columns\/ cat_pipeline with categorical columns?\n\n#Use ColumnTransformer()\n\nct = ColumnTransformer(transformers=[\n    ('numeric', num_pipeline, num_cols),\n    ('categorical', cat_pipeline, cat_cols)\n])","2bb53cf4":"rf_model = RandomForestRegressor()\nlr_model = LinearRegression()\nxgb_model = XGBRegressor()\n\nmodels = [\n    rf_model,\n    lr_model,\n    xgb_model\n]","fc244b0b":"for model in models:\n    \n    full_pipeline = Pipeline(steps = [('ct', ct), ('model', model)])\n    \n    full_pipeline.fit(X_train, y_train)\n    \n    preds = full_pipeline.predict(X_valid)\n    \n    #Scoring\n    housing_mse = mean_squared_error(preds, y_valid)\n    housing_score = full_pipeline.score(X_valid, y_valid)\n    \n    #print('score of ' + str(model) + ':', housing_score)\n    \n    print('RMSE of ' + str(model) + ':', np.sqrt(housing_mse))","0c315c86":"grid_params = {\n    'xgb__min_child_weight' : [1, 5, 10],\n    'xgb__n_estimators' : [200, 400, 600, 800],\n    'xgb__max_depth' : [3,5,7,9],\n    'xgb__subsample' : [0.5, 0.7, 0.9],\n    'xgb__gamma' : [0.1, 0.5, 1, 1.5, 2],\n    'xgb__learning_rate' : [0.02, 0.04, 0.06, 0.08]\n}","81bf7f71":"xgb_model_2 = XGBRegressor()\nxgb_pipeline = Pipeline(steps = [('ct', ct), ('xgb', xgb_model_2)])\n\nregressor = GridSearchCV(xgb_pipeline, \n                  grid_params,\n                  cv = 3,\n                  n_jobs = -1,\n                  verbose = 5,\n                  scoring = 'neg_root_mean_squared_error'\n                  )","c22193d2":"final_model = regressor.fit(X_train, y_train)\n\nprint(final_model.best_params_)","cb46598f":"predictions = regressor.predict(X_valid)","66f19941":"print('RMSE of \"optimal\" model: ' + str(np.sqrt(mean_squared_error(y_valid, predictions))))","81d3b0e9":"X_test = test","b057c4c1":"xgb_model_test = XGBRegressor(gamma= 0.1, \n                                learning_rate= 0.06, \n                                max_depth= 3,\n                                min_child_weight= 1,\n                                n_estimators= 80,\n                                subsample= 0.7)\n\nxgb_test_pipeline = Pipeline(steps = [('ct', ct), ('xgb_model_test', xgb_model_test)])\n\n\nxgb_test_pipeline.fit(X_train, y_train)\n\nxgb_test_pipeline.predict(X_test)\n\ntest_predictions = xgb_test_pipeline.predict(X_test)","72f36e1f":"#Getting the id column in the output to match that of the sample_submission file\nsample_submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n\noutput = pd.DataFrame({'Id': sample_submission.Id,\n                       'SalePrice': test_predictions})\n\noutput.to_csv('house_prices', index=False)","670e94af":"output","874542db":"## Conclusions\n\nAll in all, I learned so much from this project. The approach here was to use as much of my data science knowledge\/ learn as many new techniques, as possible and not to necessarily over-optimise a model. That being said, the risk of data leakage is high in this dataset and by removing features which this is the cause of, I may have avoided overfitting my final model at the cost of what would seem to be a worse RMSE than many other models with high accuracy scores. \nThere are a few things I will do differently in the future:\n\n* It might be best to simply remove columns with a significant amount of missing data, rather than try to preserve the tiny amount of data that the model will end up using. \n\n* Normalise continuous features; most ML algorithms give more accurate results when fed normalised data.\n\n* Drop features will low\/no correlation to the target variable; this serves to reduce the overall number of columns a ML algorithm has to work with, thus giving it only relevant information.\n\n* Use RandomSearchCV for finding best hyperparameters quicker\n\n* When splitting data into train test and validation sets, use a stratified sampling technique such as StratifiedShuffleSplit.\n\n* I could have put many of the data processing steps into a single function to make processing of new data easier, should I ever need to.\n","1f0b6bb9":"Percentage of missing data per column:","accf9c24":"### Data Leakage\n\nLet's take a look again at all of our columns:","b1241f96":"There are a few columns which need further analysis to see if they will cause target leakage or train-test-split contamination:\n\nMoSold\/YrSold, SaleType, SaleCondition: what if we want to sell a house but haven't sold it yet? These features won't apply to us and are examples of target leakage. I will remove these columns in pre-processing.\n","ed542a7f":"### Importing Libraries","59ed62e3":"## Model Building With Pipelines\n\n* Find the best model out of a RandomForestRegressor, LinearRegression and a XGBRegressor model without hyperperameters using their RMSEs\n\n* Use GridSearchCV to find optimal hyperperameters for the best model\n\n* Evaluate model on the test set","fe730961":"**OverallQual** and **GrLivArea** seem to have a high correlation with SalePrice. From the Data Description these are:\n\n**OverallQual**: Rates the overall material and finish of the house from 1-10, 10 being \"Very Excellent\" and 1 being \"Very Poor\". \n\n**GrLivArea**: Above grade (ground) living area square feet.\n\nLet's look at these variables in some detail","4097b623":"## Preprocessing\n\n* Split data into training and validation sets\n* Drop 'leaky' features\n* Impute missing data\n* One-hot-encode categorical data and scale numerical data","cf81682d":"### Missing Values","053b38fc":"### GridSearchCV\n\nThe optimal parameters are:\n\n**gamma**: 0.1\n\n**learning_rate**: 0.06\n\n**max_depth**: 3\n\n**min_child_weight**: 1\n\n**n_estimators**: 80 \n\n**subsample**: 0.7","e20f1ea5":"There are two outliers in the bottom right; with a high living area and very low relative SalePrice. I'll delete these datapoints as they don't fit the general trend very well at all.","083b3396":"## Project Summary\n\nGoal: predict the SalePrice of a house based on its features.\n\nSub-goals:\n\n* practise EDA.\n\n* use ColumnTransformer to transform numeric and categorical data in one step.\n\n* use Pipelines to streamline data processing and model-building.\n\n* Evaluate multiple ML algorithms and use GridSearchCV to find the best model hyperparameters.","83a4225a":"print('RMSE of \"optimal\" model: ' + str(np.sqrt(mean_squared_error(y_valid, predictions))))","e25f7066":"Let's first see which numeric variables correlate the most with SalePrice","05387926":"## EDA\n\n* See which features correlate the most with target\n* Remove outliers\n","d635d110":"\nThe data_description file above can explain why some columns are missing so much data:\n\n* PoolQC: pool quality; many properties do not have a pool.\n* MiscFeature: extra features such as sheds, elevators, tennis courts, etc. \n* Alley: type of alley access to property; likely the missing data are unrecorded, properties with no alley access. \n* Fence: fence quality. NA means no fence.\n* FireplaceQU: fireplace quality; many properties do not have a fireplace.\n* Lotfrontage: linear feet of street connected to property; apartments may be exempt from this category, or simply missing data.\n\nMost of the missing values tend to be associated with categorical variblaes. NA values correspond with missing a feature (e.g. a pool or a fence). It is fair to say that these missing values are simply the result of the dataset maker's decision when categorising features and do not represent actual missing data. I will impute most of the missing values with the value 0.\n"}}