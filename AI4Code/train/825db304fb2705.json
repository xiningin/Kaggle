{"cell_type":{"34716dc8":"code","85363b16":"code","526e1e11":"code","b6f50016":"code","f438ab68":"code","4f5cdb68":"code","0550b16a":"code","6ce68e9c":"code","72b11567":"code","aad54fa2":"code","43a94515":"code","860747e8":"code","0596471a":"code","e1181cb8":"code","a1910609":"code","044fb204":"code","bd55e3d4":"code","b71644b6":"code","f8949fa7":"code","a1b3c3a2":"code","e88c45dd":"code","e0b4b0af":"markdown","403fbb10":"markdown","7e6a2014":"markdown","f1470398":"markdown"},"source":{"34716dc8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nprint('Setup complete.')","85363b16":"# Load the training dataset and look at the first few records.\ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nprint(train_df.shape)\ntrain_df.head()","526e1e11":"# Look at summary statistics for the training dataset.\ntrain_df.describe()","b6f50016":"train_df['Cabin'] = train_df['Cabin'].fillna('Unknown')\ntrain_df.groupby(['Cabin']).apply(lambda x: x['Survived'].sum()\/len(x))","f438ab68":"train_df['n_Cabins'] = train_df.apply(lambda row: len(row['Cabin'].split()), axis=1)\ntrain_df.groupby(['n_Cabins', 'Sex']).apply(lambda x: x['Survived'].sum()\/len(x))","4f5cdb68":"train_df['CabinPrefix'] = train_df.apply(lambda row: row['Cabin'][0], axis=1)\ntrain_df.groupby(['CabinPrefix', 'Sex']).apply(lambda x: x['Survived'].sum()\/len(x))","0550b16a":"def split_ticket(ticket):\n    # special case. a few Tickets are only LINE\n    if ticket == 'LINE':\n        return pd.Series(['LINE', 0])\n    \n    parts = ticket.split()  # split the ticket on whitespace\n    if len(parts) == 1:\n        return pd.Series([\"NO_PREFIX\", int(parts[0].strip(' .'))])\n    elif len(parts) == 2:\n        return pd.Series([parts[0].strip(' .'), int(parts[1].strip(' .'))])\n    else:\n        # special case. One Ticket has a prefix separated by a space.\n        return pd.Series([parts[0].strip(' .') + parts[2].strip(' .'), int(parts[2].strip(' .'))])\n\ntrain_df[['Ticket_Prefix', 'Ticket_NUM']] = train_df.apply(lambda row: split_ticket(row['Ticket']), axis=1)\ntrain_df.groupby(['Ticket_Prefix']).apply(lambda x: x['Survived'].sum()\/len(x))","6ce68e9c":"train_df['Title'] = train_df.apply(lambda row: row['Name'].split()[1], axis=1)\ntrain_df.groupby(['Title']).apply(lambda x: x['Survived'].sum()\/len(x))","72b11567":"from sklearn.preprocessing import LabelEncoder\n\n# Select feaures to base the model on.\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', \n            'Embarked', 'n_Cabins', 'CabinPrefix', 'Ticket_Prefix', 'Ticket_NUM', 'Title']\nX = train_df.loc[: ,features]\ny = train_df['Survived']\n\n# Scikit-learn doesn't like categorical features as strings, so we'll encode them as numbers.\nX['Sex'] = X['Sex'].map( {'male':1, 'female':0} )\n\n# Remember we had a couple of missing values in the Embarked column.\n# We'll fill those with 'Unknown' before we try to encode this column\nX['Embarked'] = X['Embarked'].fillna('Unknown')\n\n# Age also had several missing values. Fill those with the average age.\nX['Age'] = X['Age'].fillna(28.0)\n\nemb_encoder = LabelEncoder()\nemb_encoder.fit(X['Embarked'])\nX['Embarked'] = emb_encoder.transform(X['Embarked'])\n\ncab_encoder = LabelEncoder()\ncab_encoder.fit(X['CabinPrefix'])\nX['CabinPrefix'] = cab_encoder.transform(X['CabinPrefix'])\n\ntic_encoder = LabelEncoder()\ntic_encoder.fit(X['Ticket_Prefix'])\nX['Ticket_Prefix'] = tic_encoder.transform(X['Ticket_Prefix'])\n\ntitle_encoder = LabelEncoder()\ntitle_encoder.fit(X['Title'])\nX['Title'] = title_encoder.transform(X['Title'])\nX","aad54fa2":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = GradientBoostingClassifier(random_state=0)\n\nparam_dict = {\n    \"n_estimators\":[5,50,250,500],\n    \"max_depth\":[1,3,5,7,9],\n    \"learning_rate\":[0.01,0.1,1,10,100]\n}\n\ngrid = GridSearchCV(model, param_grid=param_dict, cv=3, verbose=2, n_jobs=4)\ngrid.fit(X, y)","43a94515":"grid.best_params_","860747e8":"grid.best_estimator_","0596471a":"grid.best_score_","e1181cb8":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nprint(test_df.shape)\ntest_df.head()","a1910609":"# Set aside the test PassengerId values for later.\npassenger_ids = test_df['PassengerId']\n\n# Add a fake value for Survived that we'll remove again later.\ntest_df['Survived'] = np.NaN\nprint(test_df.shape)\ntest_df.head()","044fb204":"combo_df = train_df.append(test_df)\nprint(combo_df.shape)","bd55e3d4":"# Add the engineered features to the combined dataset\ncombo_df['Cabin'] = combo_df['Cabin'].fillna('Unknown')\ncombo_df['n_Cabins'] = combo_df.apply(lambda row: len(row['Cabin'].split()), axis=1)\ncombo_df['CabinPrefix'] = combo_df.apply(lambda row: row['Cabin'][0], axis=1)\ncombo_df[['Ticket_Prefix', 'Ticket_NUM']] = combo_df.apply(lambda row: split_ticket(row['Ticket']), axis=1)\ncombo_df['Title'] = combo_df.apply(lambda row: row['Name'].split()[1], axis=1)\ncombo_df.describe()","b71644b6":"# Select feaures to base the model on.\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', \n            'Embarked', 'n_Cabins', 'CabinPrefix', 'Ticket_Prefix', 'Ticket_NUM', 'Title', 'Survived']\ncombo_df = combo_df[features]\n\n# Scikit-learn doesn't like categorical features as strings, so we'll encode them as numbers.\ncombo_df['Sex'] = combo_df['Sex'].map( {'male':1, 'female':0} )\n\n# Remember we had a couple of missing values in the Embarked column.\n# We'll fill those with 'Unknown' before we try to encode this column\ncombo_df['Embarked'] = combo_df['Embarked'].fillna('Unknown')\n\n# Age also had several missing values. Fill those with the median age.\ncombo_df['Age'] = combo_df['Age'].fillna(28.0)\n\n# There was one missing Fare. Fill it with the median fare.\ncombo_df['Fare'] = combo_df['Fare'].fillna(14.45)\n\nemb_encoder = LabelEncoder()\nemb_encoder.fit(combo_df['Embarked'])\ncombo_df['Embarked'] = emb_encoder.transform(combo_df['Embarked'])\n\ncab_encoder = LabelEncoder()\ncab_encoder.fit(combo_df['CabinPrefix'])\ncombo_df['CabinPrefix'] = cab_encoder.transform(combo_df['CabinPrefix'])\n\ntic_encoder = LabelEncoder()\ntic_encoder.fit(combo_df['Ticket_Prefix'])\ncombo_df['Ticket_Prefix'] = tic_encoder.transform(combo_df['Ticket_Prefix'])\n\ntitle_encoder = LabelEncoder()\ntitle_encoder.fit(combo_df['Title'])\ncombo_df['Title'] = title_encoder.transform(combo_df['Title'])\n\ncombo_df.head()","f8949fa7":"# Split the combined dataframe back into train and test data.\ntrain_df = combo_df[combo_df['Survived'].notna()]\nprint(train_df.shape)\ntrain_df.describe()","a1b3c3a2":"test_df = combo_df[combo_df['Survived'].isna()]\ntest_df = test_df.drop('Survived', axis=1)\nprint(test_df.shape)\ntest_df.describe()","e88c45dd":"# Split training features and target\ntrain_y = train_df['Survived']\ntrain_X = train_df.drop('Survived', axis=1)\n\n# create and fit the model\nmodel = GradientBoostingClassifier(learning_rate=0.01, max_depth=5, n_estimators=500,\n                           random_state=0)\nmodel.fit(train_X, train_y)\n\n# get predictions based on test data\ny_pred = model.predict(test_df)\n\nfinal_gb_predictions_df = pd.DataFrame({'PassengerId': passenger_ids, 'Survived': y_pred}, dtype=int)\nfinal_gb_predictions_df.to_csv('final_GB_predictions.csv', index=False)","e0b4b0af":"## Grid Search\n\nUse [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html) from scikit-learn to find the best parameters from an initial set. The un-tuned gradient boost model in the original notebook gave us 84.3% accuracy on the train\/validation split using the same features. That dropped all the way to 77.5% when the same model was used on the full training set to create a submission. Hopefully parameter tuning will improve upon that score.","403fbb10":"Data exploration and feature engineering based on [Titanic - Decision Tree \/ RF \/ GB](https:\/\/www.kaggle.com\/bcruise\/titanic-decision-tree-rf-gb). This notebook builds on that by showing how to do parameter tuning using grid search to get better results with a gradient boost model.","7e6a2014":"## Final submission","f1470398":"## Feature Engineering"}}