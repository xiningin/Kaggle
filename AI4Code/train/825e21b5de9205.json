{"cell_type":{"52d632cb":"code","4e924562":"code","3927bd2e":"code","04fd9e88":"code","4187ddd6":"code","5c7c110c":"code","05afde31":"code","8d313423":"code","b8a9f583":"code","b2b12434":"code","996848ad":"code","623d582c":"code","55e6ee8c":"code","6fd013e8":"code","c59e12ad":"code","33bba25d":"code","e76331a3":"code","df339ed1":"code","ef58ab91":"code","3eab01b5":"code","d679095e":"code","4513a3df":"code","110b06a3":"code","0bf4ce4c":"code","0a107bf5":"code","5c097dcb":"code","8d961461":"code","2de52c76":"code","d034cb4d":"code","9e5abc1c":"code","afc32ed9":"code","a516cc6f":"code","b30a09fb":"code","b81d6d35":"code","53ae6f83":"code","7fe83821":"code","8d73bb6e":"code","04b20c3f":"code","fcd1d45e":"code","3d537c8e":"code","7086d078":"code","264edcdc":"markdown","d66d242a":"markdown","267bd815":"markdown","02fe456b":"markdown","79a6ac62":"markdown","bdbfb6b3":"markdown","0c905f5a":"markdown","175477f6":"markdown","bb3c60ac":"markdown","d31c6700":"markdown","54460aed":"markdown","aa386b19":"markdown","5d0e9f73":"markdown","aebe09b8":"markdown"},"source":{"52d632cb":"# Load in our libraries","4e924562":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px","3927bd2e":"train = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")\n","04fd9e88":"train.head()","4187ddd6":"print(f'Number of rows: {train.shape[0]};  Number of columns: {train.shape[1]}; No of missing values: {sum(train.isna().sum())}')","5c7c110c":"train.info()","05afde31":"train.describe().T","8d313423":"test.head()","b8a9f583":"print(f'Number of rows: {test.shape[0]};  Number of columns: {test.shape[1]}; No of missing values: {sum(test.isna().sum())}')","b2b12434":"submission.head()","996848ad":"train.drop([\"Id\"] , axis = 1 , inplace = True)","623d582c":"fig, ax = plt.subplots(1, 1, figsize=(17, 8))\n\ntarget_count = train['Cover_Type'].value_counts().sort_index()\n\nax.bar(target_count.index, target_count, color=['#1520E6' if i%2==0 else '#93D1FF' for i in range(9)],\n       width=0.55, \n       edgecolor='black', \n       linewidth=0.7)\n\nax.margins(0.02, 0.05)\n\nfor i in range(1,8):\n    ax.annotate(f'{target_count[i]\/len(train)*100:.3}', xy=(i, target_count[i]+1000),\n                   va='center', ha='center',\n               )\n#Annotate the point xy with text text.\n\n#In the simplest form, the text is placed at xy.\n\nax.set_title('Cover_Type Distribution', weight='bold', fontsize=15)\nax.grid(axis='y', linestyle='-', alpha=0.4)\n\nfig.tight_layout()\nplt.show()","55e6ee8c":"target_count = train['Cover_Type'].value_counts().sort_index()\ntarget_count_df = pd.DataFrame(target_count)\n#pd.options.display.float_format = '{:,.2f}%'.format\ntarget_count_df['Cover_Type(%)'] = (target_count_df\/target_count.sum()*100)\ntarget_count_df.sort_values('Cover_Type(%)', ascending=False, inplace=True)\ndisplay(target_count_df)","6fd013e8":"train1=train.iloc[:400000,:]","c59e12ad":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train1.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","33bba25d":"train1=train.iloc[:400000,0:3]\ntrain1.head()","e76331a3":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train1.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","df339ed1":"train2=train.iloc[:400000,3:10]\ntrain2.head()","ef58ab91":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train2.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","3eab01b5":"train3=train.iloc[:400000,10:14]\ntrain3.head()","d679095e":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train3.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","4513a3df":"train4=train.iloc[:400000,14:40]\ntrain4.head()","110b06a3":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train4.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","0bf4ce4c":"#we have relation between Wilderness_Area3 and Wilderness_Area1\ntrain = train.drop(labels = \"Wilderness_Area3\"  ,axis = 1)","0a107bf5":"# so we have the relation between sol_typpe7 and soil_type15 with others colmuns \n\ntrain = train.drop(labels = [\"Soil_Type7\" , \"Soil_Type15\"] ,axis = 1)","5c097dcb":"from sklearn.preprocessing import StandardScaler , LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.utils import to_categorical","8d961461":"test = test.drop(labels = [\"Soil_Type7\" , \"Soil_Type15\",\"Wilderness_Area3\"] ,axis = 1)","2de52c76":"scaler = StandardScaler()\nTARGET = 'Cover_Type'\nFEATURES = [col for col in train.columns if col not in  TARGET]","d034cb4d":"for col in FEATURES:\n    train[col] = scaler.fit_transform(train[col].to_numpy().reshape(-1,1))\n    test[col] = scaler.transform(test[col].to_numpy().reshape(-1,1))\n    \nX = train[FEATURES].to_numpy().astype(np.float32)\ny = train[TARGET].to_numpy().astype(np.float32)\nX_test = test[FEATURES].to_numpy().astype(np.float32)\n\ndel train, test","9e5abc1c":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","afc32ed9":"#Decision Tree\n\nclf = DecisionTreeClassifier()\nscoring = 'accuracy'\nscore = cross_val_score(clf,  X, y, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","a516cc6f":"# decision tree Score\nround(np.mean(score)*100, 2)","b30a09fb":"clf.fit(X, y)","b81d6d35":"nn_preds = clf.predict(X_test )\nprint(nn_preds)","53ae6f83":"test.head()","7fe83821":"my_submission = pd.DataFrame({'Id': test.Id, 'Cover_Type': nn_preds})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission_DT.csv', index=False)\nmy_submission.head()","8d73bb6e":"#Random Forest\nran = RandomForestClassifier(n_estimators=13)\nscoring = 'accuracy'\nscore = cross_val_score(clf, X, y, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","04b20c3f":"# Random Forest Score\nround(np.mean(score)*100, 2)","fcd1d45e":"ran.fit(X, y)","3d537c8e":"nn_preds = ran.predict(X_test )\nprint(nn_preds)","7086d078":"my_submission = pd.DataFrame({'Id': test.Id, 'Cover_Type': nn_preds})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission_ran.csv', index=False)\nmy_submission.head()","264edcdc":"# Importing Classifier Modules","d66d242a":"#Cross Validation (K-fold)\n","267bd815":"There are a total of 7 Cover_Type. The top 2 distributions account for 80% of the total. All except the order of 2 and 1 are in increasing order.","02fe456b":"Submission The submission file is expected to have an id and loss columns.\nBelow is the first 5 rows of submission file:","79a6ac62":"because have 55 columns we can't check relation between the features so i decide to divide columns for 4 categories and each category has something common with others columns\n\n","bdbfb6b3":"Plots Pearson Correlation Heatmap\n\nlet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently as follows","0c905f5a":"# Remove id from train data  ","175477f6":"The dimension and number of missing values in the test dataset is as below:","bb3c60ac":"# Summarie and statistics","d31c6700":"# Infos","54460aed":"**Load and check data**","aa386b19":"Below is the first 5 rows of test dataset:","5d0e9f73":"because of big data we get only 400000 of train data","aebe09b8":"The dimension and number of missing values in the train dataset is as below:"}}