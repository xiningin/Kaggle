{"cell_type":{"23ad6aed":"code","de10b6e9":"code","cec021d7":"code","f034aa9c":"code","55b4b8b2":"code","a520e996":"code","bac4718f":"code","dee6fb21":"code","505576a6":"code","2be8a584":"code","5c2f512e":"code","b2956de7":"code","2705d339":"code","94713741":"code","407a2b29":"code","45220a01":"code","329dbde3":"code","42346d4e":"code","83619dfb":"code","2a3bc210":"code","6c84823c":"code","b3c37984":"code","db71abe8":"code","15253582":"code","91ae11e3":"markdown","7220092d":"markdown","c7658a5b":"markdown","15fa37ad":"markdown","c09b33b3":"markdown","df639ebc":"markdown","982e11ce":"markdown","f0a45de0":"markdown","0a67a3fa":"markdown"},"source":{"23ad6aed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","de10b6e9":"from tensorflow.python.client import device_lib\n\nprint(device_lib.list_local_devices())","cec021d7":"import numpy as np\nimport pandas as pd\nfrom collections import defaultdict\nimport re\n\n\nfrom bs4 import BeautifulSoup\n\nimport sys\nimport os\nimport keras\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\n\nfrom keras.layers import Embedding\nfrom keras import regularizers\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Dense, Input, Flatten\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding, Concatenate, Dropout, BatchNormalization\nfrom keras.models import Model\nfrom keras.layers.convolutional_recurrent import ConvLSTM2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\nfrom keras.layers import Dense, Embedding, LSTM, GRU\n\n\nMAX_SEQUENCE_LENGTH = 1000\nMAX_NB_WORDS = 200000\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.2","f034aa9c":"data = pd.read_csv('\/kaggle\/input\/ticnn-fake-real\/TICNN_fake_real.csv')\ndata.head()","55b4b8b2":"def clean_str(string):\n    \"\"\"\n    Cleaning of dataset\n    \"\"\"\n    string = re.sub(r\"\\\\\", \"\", string)    \n    string = re.sub(r\"\\'\", \"\", string)    \n    string = re.sub(r\"\\\"\", \"\", string)    \n    return string.strip().lower()\n","a520e996":"# Input Data preprocessing\n# data_train = pd.read_csv('.\/data\/TI CNN fake news dataset all_data.csv')\ndata['type'] = data['type'].replace('fake',1)\ndata['type'] = data['type'].replace('real',0)\nprint(data.columns)\nprint('What the raw input data looks like:')\nprint(data[0:5])\ntexts = []\nlabels = []\n\nfor i in range(data.text.shape[0]):\n    text1 = data.title[i]\n    text2 = data.text[i]\n    text = str(text1) +\"\"+ str(text2)\n    texts.append(text)\n    labels.append(data.type[i])\n    \ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","bac4718f":"# Pad input sequences\nfinal_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nlabels = to_categorical(np.asarray(labels),num_classes = 2)\nprint('Shape of data tensor:', final_data.shape)\nprint('Shape of label tensor:', labels.shape)","dee6fb21":"# Train test validation Split\nfrom sklearn.model_selection import train_test_split\n\nindices = np.arange(final_data.shape[0])\nnp.random.shuffle(indices)\nfinal_data = final_data[indices]\nlabels = labels[indices]\nx_train, x_test, y_train, y_test = train_test_split(final_data, labels, test_size=0.20, random_state=42)\nx_test, x_val, y_test, y_val = train_test_split(final_data, labels, test_size=0.50, random_state=42)\nprint('Size of train, validation, test:', len(y_train), len(y_val), len(y_test))\n\nprint('real & fake news in train,valt,test:')\nprint(y_train.sum(axis=0))\nprint(y_val.sum(axis=0))\nprint(y_test.sum(axis=0))","505576a6":"#Using Pre-trained word embeddings\nGLOVE_DIR = \"\/kaggle\/input\/glove6b100dtxt\/\" \nembeddings_index = {}\nf = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"utf8\")\nfor line in f:\n    values = line.split()\n    #print(values[1:])\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Total %s word vectors in Glove.' % len(embeddings_index))\n\nembedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n        \nembedding_layer = Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH)","2be8a584":"# Simple CNN model\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nl_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\nl_pool1 = MaxPooling1D(5)(l_cov1)\nl_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\nl_pool2 = MaxPooling1D(5)(l_cov2)\nl_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\nl_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n\nl_flat = Flatten()(l_pool3)\n\nl_dense = Dense(128, activation='relu')(l_flat)\nl_b = BatchNormalization()(l_dense)\npreds = Dense(2, activation='softmax')(l_b)\n\nmodel = Model(sequence_input, preds)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adadelta',\n              metrics=['acc'])\n\n\nprint(\"Fitting the simple convolutional neural network model\")\nmodel.summary()\n\n# simple early stopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\nhistory = model.fit(x_train, y_train, validation_data=(x_val, y_val),callbacks=[es], epochs=10, batch_size=128)","5c2f512e":"import matplotlib.pyplot as plt\n%matplotlib inline \n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","b2956de7":"score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","2705d339":"#convolutional approach 2\nconvs = []\nfilter_sizes = [3,4,5]\n\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\n\nfor fsz in filter_sizes:\n    l_conv = Conv1D(nb_filter=128,filter_length=fsz,activation='relu')(embedded_sequences)\n    l_pool = MaxPooling1D(5)(l_conv)\n    convs.append(l_pool)\n    \nl_concatenate = Concatenate(axis=-1)(convs)\nl_cov1= Conv1D(filters=128, kernel_size=5, activation='relu')(l_concatenate)\nl_pool1 = MaxPooling1D(5)(l_cov1)\nl_cov2 = Conv1D(filters=128, kernel_size=5, activation='relu')(l_pool1)\nl_pool2 = MaxPooling1D(30)(l_cov2)\nl_flat = Flatten()(l_pool2)\nl_dense = Dense(128, activation='relu')(l_flat)\npreds = Dense(2, activation='softmax')(l_dense)\n\nmodel2 = Model(sequence_input, preds)\nmodel2.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['acc'])\n\nprint(\"Fitting a more complex convolutional neural network model\")\nmodel2.summary()\n# simple early stopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\nhistory2 = model2.fit(x_train, y_train, validation_data=(x_val, y_val),callbacks=[es], epochs=10, batch_size=50)\nmodel2.save('model.h5')","94713741":"# list all data in history\nprint(history2.history.keys())\nimport matplotlib.pyplot as plt\n%matplotlib inline \n# summarize history for accuracy\nplt.plot(history2.history['acc'])\nplt.plot(history2.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history2.history['loss'])\nplt.plot(history2.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","407a2b29":"score = model2.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","45220a01":"# Recurrent CNN model\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nl_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\nl_pool1 = MaxPooling1D(5)(l_cov1)\nl_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\nl_pool2 = MaxPooling1D(5)(l_cov2)\nl_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\nl_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\nl_pool4 = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(l_pool3)\n# l_flat = Flatten()(l_pool4)\nl_b = BatchNormalization()(l_pool4)\nl_dense1 = Dense(128,kernel_regularizer=regularizers.l2(0.001), activation='relu')(l_b)\nl_dense2 = Dense(64, activation='relu')(l_dense1)\n\npreds = Dense(2, activation='softmax')(l_dense2)\n\nmodel3 = Model(sequence_input, preds)\nmodel3.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['acc'])\n\n\nprint(\"Fitting the simple convolutional neural network model\")\nmodel3.summary()\n\n# simple early stopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\nhistory3 = model3.fit(x_train, y_train, validation_data=(x_val, y_val), callbacks=[es],epochs=10, batch_size=128)\n","329dbde3":"# list all data in history\nprint(history3.history.keys())\nimport matplotlib.pyplot as plt\n%matplotlib inline \n# summarize history for accuracy\nplt.plot(history3.history['acc'])\nplt.plot(history3.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history3.history['loss'])\nplt.plot(history3.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","42346d4e":"score = model3.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","83619dfb":"# Test model 1\ntest_preds = model.predict(x_test)\ntest_preds = np.round(test_preds)\ncorrect_predictions = float(sum(test_preds == y_test)[0])\nprint(\"Correct predictions:\", correct_predictions)\nprint(\"Total number of test examples:\", len(y_test))\nprint(\"Accuracy of model1: \", correct_predictions\/float(len(y_test)))\n\n# Creating the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nx_pred = model.predict(x_test)\nx_pred = np.round(x_pred)\nx_pred = x_pred.argmax(1)\ny_test_s = y_test.argmax(1)\ncm = confusion_matrix(y_test_s, x_pred)\nplt.matshow(cm, cmap=plt.cm.binary, interpolation='nearest')\nplt.title('Confusion matrix - model1')\nplt.colorbar()\nplt.ylabel('expected label')\nplt.xlabel('predicted label')\nplt.show()","2a3bc210":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, test_preds, labels=[0,1]))","6c84823c":"#Test model 2\ntest_preds2 = model2.predict(x_test)\ntest_preds2 = np.round(test_preds2)\ncorrect_predictions = float(sum(test_preds2 == y_test)[0])\nprint(\"Correct predictions:\", correct_predictions)\nprint(\"Total number of test examples:\", len(y_test))\nprint(\"Accuracy of model2: \", correct_predictions\/float(len(y_test)))\n\n# Creating the Confusion Matrix\nx_pred = model2.predict(x_test)\nx_pred = np.round(x_pred)\nx_pred = x_pred.argmax(1)\ny_test_s = y_test.argmax(1)\ncm = confusion_matrix(y_test_s, x_pred)\nplt.matshow(cm, cmap=plt.cm.binary, interpolation='nearest',)\nplt.title('Confusion matrix - model2')\nplt.colorbar()\nplt.ylabel('expected label')\nplt.xlabel('predicted label')\nplt.show()","b3c37984":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, test_preds2, labels=[0,1]))","db71abe8":"#Test model 3\ntest_preds3 = model3.predict(x_test)\ntest_preds3 = np.round(test_preds3)\ncorrect_predictions = float(sum(test_preds3 == y_test)[0])\nprint(\"Correct predictions:\", correct_predictions)\nprint(\"Total number of test examples:\", len(y_test))\nprint(\"Accuracy of model3: \", correct_predictions\/float(len(y_test)))\n\n# Creating the Confusion Matrix\nx_pred = model3.predict(x_test)\nx_pred = np.round(x_pred)\nx_pred = x_pred.argmax(1)\ny_test_s = y_test.argmax(1)\ncm = confusion_matrix(y_test_s, x_pred)\nplt.matshow(cm, cmap=plt.cm.binary, interpolation='nearest',)\nplt.title('Confusion matrix - model3')\nplt.colorbar()\nplt.ylabel('expected label')\nplt.xlabel('predicted label')\nplt.show()","15253582":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, test_preds3, labels=[0,1]))","91ae11e3":"Our final approach is the recurrent convulutional neural network. We add a recurrent layer of LSTM in our archiecture. There are few reasons why we are adding a combination of these two models.\n\n*CNN:*\n* CNN takes a fixed size inputs and generates fixed-size outputs.\n\n* CNN is a type of feed-forward artificial neural network - are variations of multilayer perceptrons which are designed to use minimal amounts of preprocessing.\n\n* They train faster as compared to RNN.\n\n\n*RNN:*\n* RNN can handle arbitrary input\/output lengths.\n\n* RNN unlike feedforward neural networks - can use their internal memory to process arbitrary sequences of inputs.\n\n* Recurrent neural networks use time-series information. i.e. what I spoke last will impact what I will speak next.\n\n* RNNs are ideal for text and speech analysis.\n\nWe also changed the loss function and the optimizer. Since we consider this as a binary classification problem, we used a binary cross entropy loss and we used adam optimizer.","7220092d":"**Read the dataset which we created**","c7658a5b":"**Preprocess the dataset**","15fa37ad":"# Let us now test our model on the test data","c09b33b3":"**As we can see the performance of the model has descreased and even the train and the validation accuracy has descreased. From the graph we can see that there is a possibility this model might not be good when we test it on our Test dataset.**","df639ebc":"# First let us check a Simple CNN model","982e11ce":"**As we can see above the results are pretty good. This shows us why the CNN model's are used heavily these days in order to work on the text classification model. Let us now try to increase the complexity of the model with a modified kernal size. I have tried several kernal sizes out of which I found the below one which suits our use case**\n\n# Complex CNN model","f0a45de0":"**Create embedding layer**","0a67a3fa":"**From our final observation we can see that the Recurrent convolutional neural network model outperforms all the other models and it has the best generalization on the test data too. This model beats the current state of the art model which has accuracy of 100%. We also tested it on other dataset and the model performed very well with 95% accuracy on an average"}}