{"cell_type":{"cd6e093e":"code","cd153206":"code","f4470f89":"code","d3d7c3a9":"code","037fee65":"code","c20573da":"code","8b4c9e04":"code","3f9469a8":"code","806a0fc4":"code","0317acd5":"code","4e8171cd":"code","b2114295":"code","db6c6035":"code","b8bba781":"code","fbbd55a5":"code","b5f36164":"code","4473db39":"code","81ca5888":"code","f125a951":"code","b0e63163":"code","9ff326b8":"code","9ccedf37":"code","193c7b92":"code","4e4a020a":"code","3cb78883":"code","1f561f1f":"code","d25cb1bb":"code","0491af5f":"code","a11dbce0":"code","9714eeb5":"code","ad96d041":"code","865e5d4f":"code","62590798":"code","2259b4d1":"code","769c2bd2":"code","fc4b537f":"code","8b2a07e0":"code","7663a3fb":"code","32b3536c":"code","6900aa5d":"code","c41eea65":"code","b5547f8e":"code","813ea9dc":"code","e1d5e264":"code","d1c7b904":"code","d65319f7":"code","650c514e":"code","2bad37a0":"code","b22e37b8":"code","22d8da89":"code","f3219823":"code","ad3871ae":"code","7079b19c":"code","b4380ad4":"code","a7828122":"code","4a3d8baf":"code","537c082a":"code","cd68e5d8":"code","b78dda14":"code","4152b291":"code","2c735c84":"code","b1855a01":"code","15f84b8e":"code","051e7ee5":"code","2b467efe":"code","ec204248":"code","f3d905da":"code","7d2b2d37":"code","8dbede27":"markdown","ee0661b5":"markdown","93f62056":"markdown","f550c4b0":"markdown","86c99744":"markdown","4d0350cf":"markdown","9d38033c":"markdown","6dfa7a68":"markdown","a02adb8a":"markdown","81fed3cf":"markdown","fe21dbdd":"markdown","5f109f31":"markdown","65670732":"markdown","9cc4c7b2":"markdown","4904014f":"markdown","a4ba930c":"markdown","d528776c":"markdown","56ca3d8a":"markdown","eddf8920":"markdown","b33af0d8":"markdown","1fe278d3":"markdown","b7ec8e34":"markdown","ce07ad7d":"markdown","a59da478":"markdown","9f920c00":"markdown","d2b088dd":"markdown","ce5c6c63":"markdown","d3fce90d":"markdown","f6e92ff4":"markdown"},"source":{"cd6e093e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom collections import Counter","cd153206":"train= pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest =pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nss =pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")","f4470f89":"train.head()","d3d7c3a9":"test.head()","037fee65":"train.describe()","c20573da":"sns.catplot(x='Pclass', data=train,kind='count', hue='Sex',palette='rocket')","8b4c9e04":"sns.catplot(x = 'Sex',data=train, kind='count',palette='rocket')","3f9469a8":"sns.barplot(x='Pclass', y='Survived', data=train,palette='rocket')","806a0fc4":"sns.catplot(x ='Survived',data=train, kind='count',hue='Sex',palette='rocket')","0317acd5":"sns.catplot(x='Pclass', data=train,kind='count', hue='Sex',palette='rocket')","4e8171cd":"#Age\nsns.boxplot(x='Survived',y='Age',data=train,palette='winter')","b2114295":"sns.displot(train['Age'].dropna(),kde=False,color='darkred',bins=40)","db6c6035":"#Survival and dead distribution with respect to age\nplt.figure(figsize=(20, 30))\nsns.countplot(y = \"Age\",hue=\"Survived\", data=train)","b8bba781":"FacetGrid = sns.FacetGrid(train, row='Embarked', size=5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\nFacetGrid.add_legend()","fbbd55a5":"print(train[\"SibSp\"].value_counts())","b5f36164":"ax = sns.countplot(x = \"SibSp\",hue=\"Survived\", data=train,palette='rocket')","4473db39":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap=\"viridis\")","81ca5888":"total = train.isnull().sum().sort_values(ascending=False)\ntotal","f125a951":"train.head()","b0e63163":"train.drop(['Name','Ticket','Cabin','Fare'],axis=1,inplace=True)\ntest.drop(['Name','Ticket','Cabin','Fare'],axis=1,inplace=True)","9ff326b8":"train","9ccedf37":"sns.boxplot(x='Pclass',y='Age',data=train)","193c7b92":"sns.boxplot(x='Pclass',y='Age',data=test)","4e4a020a":"def impute_age_train(cols):\n    Age=cols[0]\n    Pclass=cols[1]\n    \n    if pd.isnull(Age):\n        if Pclass == 1:\n            return 37\n        elif Pclass == 2:\n            return 29\n        else :\n            return 24\n    else:\n        return Age","3cb78883":"def impute_age_test(cols):\n    Age=cols[0]\n    Pclass=cols[1]\n    \n    if pd.isnull(Age):\n        if Pclass == 1:\n            return 42\n        elif Pclass == 2:\n            return 27\n        else :\n            return 25\n    else:\n        return Age","1f561f1f":"train['Age']=train[['Age','Pclass']].apply(impute_age_train,axis=1)\ntest['Age']=test[['Age','Pclass']].apply(impute_age_test,axis=1)","d25cb1bb":"train['Embarked'].describe()","0491af5f":"common='S'\ntrain['Embarked'] = train['Embarked'].fillna(common)","a11dbce0":"train.isnull().sum()\n","9714eeb5":"Embarked = pd.get_dummies(train['Embarked'],drop_first=True)\nEmbarked_test = pd.get_dummies(test['Embarked'],drop_first=True)\nEmbarked.head()","ad96d041":"#Converting values\ntrain['Sex']=train['Sex'].map({'male':0,'female':1})\ntest['Sex']=test['Sex'].map({'male':0,'female':1})\ntrain.drop(['Embarked'],axis=1,inplace=True)\ntest.drop(['Embarked'],axis=1,inplace=True)","865e5d4f":"train","62590798":"test","2259b4d1":"train = pd.concat([train,Embarked],axis=1)\ntest = pd.concat([test,Embarked_test],axis=1)\ntrain.head()","769c2bd2":"train.loc[train['Age'] <= 16, 'Age'] = 0\ntrain.loc[(train['Age'] > 16) & (train['Age'] <= 32), 'Age'] = 1\ntrain.loc[(train['Age'] > 32) & (train['Age'] <= 48), 'Age'] = 2\ntrain.loc[(train['Age'] > 48) & (train['Age'] <= 64), 'Age'] = 3\ntrain.loc[ train['Age'] > 64, 'Age'] = 4","fc4b537f":"test.loc[test['Age'] <= 16, 'Age'] = 0\ntest.loc[(test['Age'] > 16) & (test['Age'] <= 32), 'Age'] = 1\ntest.loc[(test['Age'] > 32) & (test['Age'] <= 48), 'Age'] = 2\ntest.loc[(test['Age'] > 48) & (test['Age'] <= 64), 'Age'] = 3\ntest.loc[ test['Age'] > 64, 'Age'] = 4","8b2a07e0":"train= train.drop(\"PassengerId\", axis = 1)","7663a3fb":"test.head()","32b3536c":"from sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.model_selection import train_test_split\n","6900aa5d":"X_train = train.drop(\"Survived\", axis=1)\nY_train = train[\"Survived\"]\nX_test = test.drop(\"PassengerId\", axis = 1)\n","c41eea65":"KNN=KNeighborsClassifier()\nNAIVE=GaussianNB()\nSVM=SVC()\nDT=DecisionTreeClassifier()\nLR = LogisticRegression()\nRF = RandomForestClassifier()\nEnsemble = VotingClassifier( estimators= [('KNN',KNN),('NB',NAIVE),('SVM',SVM),('DT',DT),('LR',LR),('RF',RF)], voting = 'hard')","b5547f8e":"Ensemble.fit(X_train,Y_train)","813ea9dc":"from sklearn import metrics\nY_pred_rand = (Ensemble.predict(X_train) > 0.5).astype(int)\nprint('Precision : ', np.round(metrics.precision_score(Y_train, Y_pred_rand)*100,2))\nprint('Accuracy : ', np.round(metrics.accuracy_score(Y_train, Y_pred_rand)*100,2))\nprint('Recall : ', np.round(metrics.recall_score(Y_train, Y_pred_rand)*100,2))\nprint('F1 score : ', np.round(metrics.f1_score(Y_train, Y_pred_rand)*100,2))\nprint('AUC : ', np.round(metrics.roc_auc_score(Y_train, Y_pred_rand)*100,2))","e1d5e264":"catboost = CatBoostClassifier(silent=True)\ncatboost.fit(X_train, Y_train)\nY_pred = catboost.predict(X_test)\nacc_catboost = round(catboost.score(X_train, Y_train) * 100, 2);","d1c7b904":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","d65319f7":"svc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","650c514e":"knn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","2bad37a0":"gaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","b22e37b8":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","22d8da89":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","f3219823":"random_forest = RandomForestClassifier(n_estimators = 100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","ad3871ae":"\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","7079b19c":"sgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","b4380ad4":"models = pd.DataFrame({'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n                                 'Random Forest', 'Naive Bayes', 'Perceptron', 'Stochastic Gradient Decent', \n                                 'Linear SVC', 'Decision Tree', 'CatBoost'],\n                       'Score': [acc_svc, acc_knn, acc_log, acc_random_forest, acc_gaussian, acc_perceptron,\n                                 acc_sgd, acc_linear_svc, acc_decision_tree, acc_catboost]})\n\nmodels.sort_values(by = 'Score', ascending = False, ignore_index = True)","a7828122":"classifiers = []\nclassifiers.append(LogisticRegression())\nclassifiers.append(SVC())\nclassifiers.append(KNeighborsClassifier(n_neighbors = 5))\nclassifiers.append(GaussianNB())\nclassifiers.append(Perceptron())\nclassifiers.append(LinearSVC())\nclassifiers.append(SGDClassifier())\nclassifiers.append(DecisionTreeClassifier())\nclassifiers.append(RandomForestClassifier())\nclassifiers.append(CatBoostClassifier(silent=True))\n\nlen(classifiers)","4a3d8baf":"from sklearn.model_selection import cross_val_score","537c082a":"cv_results = []\nfor classifier in classifiers:\n    cv_results.append(cross_val_score(classifier, X_train, Y_train, scoring = 'accuracy',verbose=False))\n    ","cd68e5d8":"cv_mean = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_mean.append(cv_result.mean())\n    cv_std.append(cv_result.std())","b78dda14":"cv_res = pd.DataFrame({'Cross Validation Mean': cv_mean, 'Cross Validation Std': cv_std, 'Algorithm': ['Logistic Regression', 'Support Vector Machines', 'KNN', 'Gausian Naive Bayes', 'Perceptron', 'Linear SVC', 'Stochastic Gradient Descent', 'Decision Tree', 'Random Forest', 'CatBoost']})\ncv_res.sort_values(by = 'Cross Validation Mean', ascending = False, ignore_index = True)","4152b291":"sns.barplot('Cross Validation Mean', 'Algorithm', data = cv_res, order = cv_res.sort_values(by = 'Cross Validation Mean', ascending = False)['Algorithm'], palette = 'Set3', **{'xerr': cv_std})\nplt.ylabel('Algorithm')\nplt.title('Cross Validation Scores')","2c735c84":"param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf']}  \n  \ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 0) \n\ngrid.fit(X_train, Y_train)","b1855a01":"print(\"Best parameters: \", grid.best_params_) \nprint(\"Best estimator: \", grid.best_estimator_)","15f84b8e":"svc = SVC(C = 100, gamma = 0.01, kernel = 'rbf')\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","051e7ee5":"cross_val_score(svc, X_train, Y_train, scoring = 'accuracy', cv = 10).mean()","2b467efe":"ss.head()","ec204248":"submit = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': Y_pred})\nsubmit.head()","f3d905da":"submit.shape","7d2b2d37":"submit.to_csv(\"final_submission.csv\",index = False)","8dbede27":"<h1><center>Handling Missing values and imputing it<\/center><\/h1>","ee0661b5":"<h1><center>MODELLING<\/center><\/h1>","93f62056":"# Catogorical features","f550c4b0":"# Hyperparameter tuning","86c99744":"Despite the ship being male dominated.The survival rate of women is higher.As we can see that large number of women survived despite being minorities.","4d0350cf":"# Ensemble technique","9d38033c":"These are the following missing values.","6dfa7a68":"All the yellow lines that we see are missing values.We can see that majority of values are missing in cabin and age.","a02adb8a":"Filling up mean values for null.","81fed3cf":"**Relation ship between PCLASS and AGE**","fe21dbdd":"# Please upvote if you like this notebook.","5f109f31":"In class 3 males are more than 2x of women.","65670732":"S is common in Embarked.We will fill null values with it.","9cc4c7b2":"# k-fold Cross-validation","4904014f":"Thanks for it. ","a4ba930c":"Above,we can see that the age of passengers ranges between 0.4 to 80 years.And 38% of the people survived \nwhose records are mentioned in this training set.","d528776c":"In my case i got support vector machine with more accuracy.","56ca3d8a":"Age distrubution in titanic traning dataset","eddf8920":"**Svc has higher cv score**","b33af0d8":"Women on port Q and S has higher probability of surviving,Same cannot be said for the women on port C.The probability of men surviving is lower on port Q and S and it is higher in C.","1fe278d3":"The average age of people seems to almost same for survived and dead.however we can observe some outliers","b7ec8e34":"The number of male passengers are almost 2x of female passengers appox.","ce07ad7d":"From the above plot we can see that people with 0 and 1 Sibsp(Sibling or spouse) has greater chance of survival.Maybe because of sibsp with 0 or 1 were present in greater number.","a59da478":"# What's in the notebook?\n**1)EDA**\n\n**2)Detection of missing data and imputation.**\n\n**3)Feature enginnering.**\n\n**4)Coverting categorical features.**\n\n**5)Modelling.**\n\n**6)Ensemble.**\n\n**7)K-fold cv.**\n\n**8)Hyperparameter tuning.**","9f920c00":"# Types of features in titanic\n\n**Categorical**: is a collection of information that is divided into groups.\nEg. Embarked (C = Cherbourg; Q = Queenstown; S = Southampton)\n\n**Ordinal**: They are similar to categorical features but they have an order.\nEg. Pclass (1, 2, 3)\n\n**Binary**: A categrorical feature which has only 2 types of categories.Which is often represted as 0 and 1.\nEg: Sex (Male\/Female)\n\n**Continuous**: They can take up any value between the minimum and maximum values in a column.\nEg. Age, Fare\n\n**Count**: They represent the count of a variable.\nEg. SibSp, Parch","d2b088dd":"Here,we can see that person in class 1 has the higher chances of survival.Where as person in class 3 has lower chance.","ce5c6c63":"Class 3 males are more than 2x of women.","d3fce90d":"# What's new?\n**1. Removed executon logs to increase the readability.**","f6e92ff4":"<h1><center>Titanic EDA<\/center><\/h1>"}}