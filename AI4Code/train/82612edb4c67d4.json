{"cell_type":{"53d9a23c":"code","78d3a57a":"code","cbe7853d":"code","bc601099":"code","06f6761a":"code","7a12c606":"code","9c0a018b":"code","0d3fa41a":"markdown","d9f489b1":"markdown","0602a325":"markdown","6011ffae":"markdown"},"source":{"53d9a23c":"import numpy as np\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\n\n# clustering algorithms\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import davies_bouldin_score, calinski_harabaz_score","78d3a57a":"data = pd.read_csv('..\/input\/en.openfoodfacts.org.products.tsv', sep='\\t')","cbe7853d":"# get rid of all foods with nan or number as a product name - they are no fun :(\nrows_to_delete = []\nfor i, name in enumerate(data['product_name']):\n    if pd.isnull(name) or name.isdigit():\n        rows_to_delete.append(i)\ndata.drop(rows_to_delete, inplace=True)\n\ndata.set_index('product_name', inplace=True) # make the index be the food names","bc601099":"# minimizing the number of features we are dealing with in a convinient way:\n# get rid of categorical data\ndata = data.select_dtypes('float64')\n\nfor i in data:\n    if data[i].isnull().sum() > data.shape[0]*(4\/5): # get rid of features which are less than 4\/5 filled\n        data.drop([i], axis=1, inplace=True)\n    elif math.isnan(data[i].mean()):   # get rid of data with no mean\n        data.drop([i], axis=1, inplace=True)\n\ndata.describe()","06f6761a":"# impute the data\nmy_imputer = SimpleImputer()\nupdated_data = pd.DataFrame(data = my_imputer.fit_transform(data), index=data.index)\n\nupdated_data = (updated_data - np.mean(updated_data)) \/(updated_data.max() - updated_data.min())\n\nupdated_data.describe()","7a12c606":"cluster_count_candidates = np.arange(2, 25)\n\ndb_score_list = []\nch_score_list = []\nfor count in cluster_count_candidates:\n    clustering_model = KMeans(n_clusters=count).fit(updated_data)\n    db_score_list.append(davies_bouldin_score(updated_data, clustering_model.labels_))\n    ch_score_list.append(calinski_harabaz_score(updated_data, clustering_model.labels_))\n\nplt.figure(figsize=(18, 5))\n\nplt.subplot(1,2,1)\nplt.stem(cluster_count_candidates, db_score_list)\nplt.xlabel('Cluster Count')\nplt.ylabel('Davies Bouldin Score')\nplt.title('KMeans Model Evaluation (Lower is Better)')\n\nplt.subplot(1,2,2)\nplt.stem(cluster_count_candidates, ch_score_list)\nplt.xlabel('Cluster Count')\nplt.ylabel('Calinski Harabaz Score')\nplt.title('KMeans Model Evaluation (Higher is Better)')\n\nplt.show()","9c0a018b":"cluster_count = 4\nk_means_model = KMeans(n_clusters=cluster_count)\nclustered_data = k_means_model.fit_predict(updated_data)\n\n# seperate the foods indo cluster lists\nclusters = [[] for _ in range(cluster_count)]\nfor i, cluster in enumerate(clustered_data):\n    clusters[cluster].append(data.index[i])\n    \n# Pad all clusters with 0s to equal the length of the maximum cluser,\n# so that we can arrange them all into one dataframe.\nmax_count = 0\nfor cluster in clusters:\n    if len(cluster) > max_count:\n        max_count= len(cluster)\n\nfor i in range(cluster_count):\n    clusters[i] += [0]*(max_count - len(clusters[i]))\\\n    \n# upload results to output file\nresults = pd.DataFrame({'Cluster '+str(i+1): clusters[i] for i in range(cluster_count)})\nresults.to_csv('KMeans_results.csv')\nprint('Full results can be seen in the output file of this kernel.')\nresults.head(20)","0d3fa41a":"## Clustering Time\n\n### KMeans\n\nWe will find the optimal cluster count based on the Davies-Bouldin score and Calinski Haraz Score.\n","d9f489b1":"## Preprocess\n\nFirst let us clean up our data:","0602a325":"### Machine Learning at The Cooper Union\n\n### Unsupervised Learning Example - K-Means clustering\n\nShowing some clustering examples on a food dataset.\n\nWe will use a couple different clustering algorithms (KMeans and DBSCAN) and try and find the 'optimal' number of clusters using some evaluation metrics.","6011ffae":"Interestingly, when we first tried this with unnormalized data, we got very contradicting results from the two metrics, with Davies-Bouldin indicating that the less clusters the better and the Calinski Harabaz indicating that the more clusters the better.\n\nHowever after normalizing our data we are able to see results that more align with one another. As our final clustering count, lets use 4, which seems to strike a balance as the overall best when considering the two metrics."}}