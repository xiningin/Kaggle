{"cell_type":{"effa21e9":"code","16ecd965":"code","a1e5c026":"code","6353e939":"code","51fedd2e":"code","4db0b307":"code","77ed72d6":"code","66031f17":"code","a17f11bb":"code","80909b1f":"code","90ae79bb":"code","09b034a4":"code","1777d5a4":"markdown","ed476146":"markdown","aed7d862":"markdown","b7270a48":"markdown","5a137d6e":"markdown","ce1ebc7d":"markdown","5bab6d12":"markdown","a94b565f":"markdown","c88ee652":"markdown","afe763e2":"markdown","e1650339":"markdown","a73284c4":"markdown"},"source":{"effa21e9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold","16ecd965":"df = pd.read_csv('\/kaggle\/input\/mushroom-classification\/mushrooms.csv')\ndf.head()","a1e5c026":"pd.concat({'unique values': df.apply(pd.unique), 'number of unique values': df.nunique()}, axis=1)","6353e939":"df.drop('veil-type', axis=1, inplace=True)","51fedd2e":"# les is the dict of LabelEncoder objects created for each dataframe column\nles = {col: LabelEncoder() for col in df.columns}\n\nfor col in les:\n    df[col] = les[col].fit_transform(df[col])\n    \ndf.head()","4db0b307":"target = df['class'].values\ndata = df.drop('class', axis=1).values\nX_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=17, shuffle=False)","77ed72d6":"kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state=17)\n\ndef plot_feature_importance(tree_grid, n_cols=10):\n    f_imp = pd.DataFrame({'feature': list(df.drop('class', axis=1).columns), \n                          'importance': tree_grid.best_estimator_.feature_importances_}\n                        ).sort_values('importance', ascending = False).reset_index()\n    f_imp['importance_normalized'] = f_imp['importance'] \/ f_imp['importance'].sum()\n    \n    ax = plt.subplot()\n    ax.barh(list(reversed(list(f_imp.index[:n_cols]))), \n            f_imp['importance_normalized'].head(n_cols), \n            align = 'center', edgecolor = 'k')\n    ax.set_yticks(list(reversed(list(f_imp.index[:n_cols]))))\n    ax.set_yticklabels(f_imp['feature'].head(n_cols))\n    plt.show()\n\ndef train_by_gridsearch(train_set, test_set, clf, params, cv=kf, n_cols=10):\n    tree_grid = GridSearchCV(clf, params, cv=cv)\n    tree_grid.fit(train_set, test_set)\n    \n    plot_feature_importance(tree_grid, n_cols)\n    \n    return tree_grid.best_estimator_, tree_grid\n\ndef print_info(clf, greed):\n    train_score = accuracy_score(clf.predict(X_train), y_train)\n    test_score = accuracy_score(clf.predict(X_test), y_test)\n    best_params = greed.best_params_\n    print(f'Train Score = {train_score}')\n    print(f'Test Score = {test_score}')\n    print(f'Best Params:', best_params)","66031f17":"from sklearn.tree import DecisionTreeClassifier\ndtc_params = {'max_depth': list(range(1, 11)),\n              'min_samples_split': [2, 3, 4, 5],\n              'min_samples_leaf': [2, 3, 4, 5],\n              'max_leaf_nodes': [5, 10, 15, 20, 25, 30, 50],\n             }\ndtc, dtc_greed = train_by_gridsearch(X_train, y_train, clf=DecisionTreeClassifier(random_state=17), params=dtc_params)\nprint_info(dtc, dtc_greed)","a17f11bb":"from sklearn.ensemble import AdaBoostClassifier\nabc_params = {'n_estimators': [50, 100, 150], \n              'base_estimator': [\n                  DecisionTreeClassifier(max_depth=1), \n                  dtc\n              ]}\nabc, abc_greed = train_by_gridsearch(X_train, y_train, clf=AdaBoostClassifier(), params=abc_params)\n\nprint_info(abc, abc_greed)","80909b1f":"from sklearn.ensemble import RandomForestClassifier\nrfc_params = {'max_depth': list(range(2, 11)) + [None],\n              'n_estimators': [15, 25, 50, 75, 100],\n             }\nrfc, rfc_greed = train_by_gridsearch(X_train, y_train, clf=RandomForestClassifier(random_state=17), params=rfc_params)\n\nprint_info(rfc, rfc_greed)","90ae79bb":"from sklearn.ensemble import GradientBoostingClassifier\ngbc_params = {\n    'max_depth': list(range(2, 11)) + [None],\n    'n_estimators': [50, 75, 100, 150, 175, 200],\n}\ngbc, gbc_greed = train_by_gridsearch(X_train, y_train, clf=GradientBoostingClassifier(random_state=17), params=gbc_params)\n\nprint_info(gbc, gbc_greed)","09b034a4":"from sklearn.ensemble import ExtraTreesClassifier\netc_params = {'max_depth': list(range(2, 11)) + [None],\n             'max_leaf_nodes': [10, 15, 20, 30, 50],\n             }\netc, etc_greed = train_by_gridsearch(X_train, y_train, clf=ExtraTreesClassifier(random_state=17), params=etc_params, n_cols=15)\nprint_info(etc, etc_greed)","1777d5a4":"# Random Forest Classifier","ed476146":"# Gradient Boosting Classifier","aed7d862":"The 'veil-type' column has been removed because it only contains one value.","b7270a48":"# Training tools","5a137d6e":"# Extra Trees Classifier","ce1ebc7d":"# Split the data into train and test","5bab6d12":"# Loading and viewing data","a94b565f":"# Ada Boost Classifier","c88ee652":"# Features Encoding\nIn this notebook we are using sklearn's [Label Encoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html) to encode all catigorical features. Please note that using LabelEncoder on an input dataset is not a common way.","afe763e2":"Let's check number of unique values for each column:","e1650339":"# Decision Tree Classifier","a73284c4":"# Importing Libraries"}}