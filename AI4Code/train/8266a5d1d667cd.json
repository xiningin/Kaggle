{"cell_type":{"b34f99d8":"code","2198fcc3":"code","df66a1ec":"code","19d33fd4":"code","969a97e7":"code","394b4e9d":"code","481e4e0f":"code","3ac1d802":"code","c4703c42":"code","b4828f8e":"code","8d76880c":"code","59fe3938":"code","f6a0f9c6":"code","7f376af5":"code","1bc011f8":"code","a2c85ed1":"code","1029b521":"code","a7e336cf":"code","065145ab":"code","cb7906e7":"code","c7de3d60":"code","41264873":"code","961f309a":"code","e32e4ea0":"code","72e5e73d":"code","d2060695":"code","1523746d":"code","1a066fd5":"code","6e40ad03":"code","4fbecde2":"code","38069de1":"code","a7d2c94f":"code","b55fe238":"code","4e7cf0ec":"code","af5c8b04":"code","bbe2007f":"code","7d89d560":"code","cff821dc":"code","73e7859a":"code","b95ce117":"code","c8b5797a":"code","91c0238b":"markdown","1102b299":"markdown","c180ad83":"markdown","07902250":"markdown","a0d885d2":"markdown","f6efb5a2":"markdown","4b8af6c3":"markdown","4275c1ce":"markdown","7331cc3f":"markdown","5dd5c271":"markdown","23c63f74":"markdown","5436df71":"markdown","0d984f77":"markdown","d87f738c":"markdown","cd0fc4e8":"markdown","fcc51692":"markdown","20b33826":"markdown","1e239724":"markdown","41c42fba":"markdown","98a4ab6c":"markdown","09aebc97":"markdown","fd695000":"markdown","5d5243a4":"markdown","9657ba80":"markdown","15d62d40":"markdown","e508d167":"markdown","93221c5a":"markdown"},"source":{"b34f99d8":"!pip install seaborn -U","2198fcc3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","df66a1ec":"import numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom datetime import datetime\nimport warnings\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport umap\nfrom nltk.stem import WordNetLemmatizer\nimport time","19d33fd4":"nltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","969a97e7":"df = pd.read_csv(\"\/kaggle\/input\/askreddit-questions-and-answers\/reddit_questions.csv\", delimiter=\";\")\nfor i in df.sample(n=10)['text']:\n    print(i, \"\\n\")","394b4e9d":"questions = list(df['text'])\nquestions = [q.replace('\"\"', '\"') for q in questions]\ncounts = {}\nfor question in questions:\n    for word in word_tokenize(question.lower()):\n        if word.isalpha():\n            try:\n                counts[word] += 1\n            except KeyError:\n                counts[word] = 1\nsorted_count = {k: v for k, v in sorted(counts.items(), key=lambda item: item[1])[::-1]}","481e4e0f":"px.bar(x=list(sorted_count.keys())[:50], y=list(sorted_count.values())[:50], \n       title=\"50 most common words\", \n       labels={\"x\":\"word\", \"y\":\"count\"})","3ac1d802":"top_non_stop_words = []\ntop_non_stop_vals = []\nfor word in sorted_count.keys():\n    if word not in stopwords.words('english'):\n        top_non_stop_words.append(word)\n        top_non_stop_vals.append(sorted_count[word])","c4703c42":"px.bar(x=top_non_stop_words[:50], y=top_non_stop_vals[:50],\n       title=\"50 most common non stop-words\", \n       labels={\"x\":\"word\", \"y\":\"count\"})","b4828f8e":"keys = [\"who\", \"what\", \"where\", \"why\", \"when\", \"how\"]\nvals = [counts[k] for k in keys]\nvals, keys = zip(*(sorted(zip(vals, keys)))[::-1])","8d76880c":"px.bar(x=list(keys), y=list(vals),\n       title=\"word count\",\n       labels={\"x\":\"word\", \"y\":\"count\"})","59fe3938":"l = [len(q) for q in questions]\nsns.displot(l)\nplt.vlines(np.median(l), 0, 4500, colors='r')\nplt.title(\"question length\")\nplt.show()\nprint(\"median:\", np.median(l))","f6a0f9c6":"questions[np.argmax(l)]","7f376af5":"p = sns.histplot(df['timestamp'])\n\nxticklabels = []\nfor t in p.get_xticks():\n    dt = datetime.fromtimestamp(t)\n    xticklabels.append(f\"{dt.day}\/{dt.month}\/{dt.year}\")\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    p.set_xticklabels(xticklabels, rotation = 45, ha=\"right\")\nplt.title(\"questions posted over time\")\nplt.show()","1bc011f8":"def interest_over_time(terms, bins=100):\n    ts = []\n    for index, row in df.iterrows():\n        text = row['text'].lower()\n        tokens = word_tokenize(text)\n        for t in terms:\n            if t in tokens:\n                ts.append(row['timestamp'])\n    p = sns.histplot(ts, bins=bins)\n    \n    xticklabels = []\n    for t in p.get_xticks():\n        dt = datetime.fromtimestamp(t)\n        xticklabels.append(f\"{dt.day}\/{dt.month}\/{dt.year}\")\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n        p.set_xticklabels(xticklabels, rotation = 45, ha=\"right\")\n    \n    plt.title(f\"interest in {'\/'.join(terms)} over time\")\n    \n    plt.show()","a2c85ed1":"interest_over_time([\"coronavirus\", \"covid\", \"sars-cov-2\"])","1029b521":"for index, row in df.sort_values(\"timestamp\").iterrows():\n    t = row[\"text\"].lower()\n    if \"coronavirus\" in t or \"covid\" in t or \"cov-2\" in t:\n        break\nrow['text'], row['datetime']","a7e336cf":"interest_over_time([\"trump\"])","065145ab":"for index, row in df.sort_values(\"timestamp\").iterrows():\n    t = row[\"text\"].lower()\n    if \"trump\" in word_tokenize(t):\n        break\nrow['text']","cb7906e7":"for index, row in df.sort_values(\"timestamp\").iterrows():\n    t = row[\"text\"].lower()\n    if \"donald trump\" in t:\n        break\nrow['text'], row['datetime']","c7de3d60":"interest_over_time([\"biden\"])","41264873":"for index, row in df.sort_values(\"timestamp\").iterrows():\n    t = row[\"text\"].lower()\n    if \"biden\" in t:\n        break\nrow['text'], row['datetime']","961f309a":"sns.kdeplot(df['votes'], cut=0)\nplt.show()","e32e4ea0":"df['votes'].describe()","72e5e73d":"sample = df.sample(n=1000)\npx.scatter(sample, x=\"timestamp\", y=\"votes\", hover_name=\"text\", log_y=True, hover_data=['datetime'])","d2060695":"def to_date(timestamp):\n    dt = datetime.fromtimestamp(timestamp)\n    return f\"{dt.day}\/{dt.month}\/{dt.year}\"","1523746d":"df['x'] = pd.cut(df['timestamp'], bins=100)\npx.bar(x=[to_date(v) for v in df.groupby(\"x\").median()['timestamp'].values],\n          y=df.groupby(\"x\").mean()['votes'].values, title=\"Median upvotes per post over time\")","1a066fd5":"# when running in a kaggle kernel these plots might cause lag\n# or crashes when plotting the entire dateset, taking just a sample here\nsample = df.sample(frac=0.25)\nsamples_qs = list(sample['text'])","6e40ad03":"q_types = []\nw = ['who', 'what', 'where', 'why', 'how', 'when']\n\nfor q in samples_qs:\n    q = word_tokenize(q.lower())\n    f = False\n    for i in w:\n        if i in q:\n            f = True\n            q_types.append(i)\n            break\n    if not f:\n        q_types.append(\"other\")\n\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform([q.lower() for q in samples_qs])\nsvd = TruncatedSVD(n_components=2)\ny = svd.fit_transform(X)\n\npx.scatter(x=y[:, 0], y=y[:, 1],\n              hover_data=[samples_qs, sample['votes']],\n              color=q_types,\n              title=\"latent semantic analysis reduction of TF-IDF, coloured by question type\")","4fbecde2":"lemmatizer = WordNetLemmatizer()\nlemmatised = [lemmatizer.lemmatize(q.lower()) for q in samples_qs]\n\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(lemmatised)\nsvd = TruncatedSVD(n_components=100)\ny = svd.fit_transform(X)\n\nreducer = umap.UMAP()\nscaled = StandardScaler().fit_transform(y)\nembedding = reducer.fit_transform(scaled)\n\npx.scatter(x=embedding[:, 0], y=embedding[:, 1],\n              hover_data=[samples_qs, sample['votes']],\n              color=sample['timestamp'],\n              title=\"UMAP of lemmatised TF-IDF, coloured by votes\")","38069de1":"import tensorflow as tf\nimport tensorflow_hub as hub","a7d2c94f":"model = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\")","b55fe238":"pca = PCA()\ny = pca.fit_transform(model(samples_qs).numpy())\npx.scatter(x=y[:, 0], y=y[:, 1],\n              hover_data=[samples_qs, sample['votes']],\n              color=sample['timestamp'],\n              title=\"PCA of encoded, coloured by age\")","4e7cf0ec":"!pip install transformers -U","af5c8b04":"from transformers import TFGPT2LMHeadModel, GPT2Tokenizer","bbe2007f":"#download pretrained model\ntokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\ntokenizer.pad_token = tokenizer.eos_token \nmodel = TFGPT2LMHeadModel.from_pretrained(\"distilgpt2\", pad_token_id=tokenizer.eos_token_id)","7d89d560":"for prompt in [\"What is\", \"Who is\", \"Why do\", \"Where are\", \"How do\", \"When is\"]:\n\n    input_ids = tokenizer.encode(prompt, return_tensors='tf')\n\n    sample_outputs = model.generate(\n        input_ids,\n        do_sample=True, \n        max_length=50, \n        top_p=0.92, \n        top_k=0,\n        num_return_sequences=3\n    )\n\n    print(100 * '-' + \"\\nPrompt:\", prompt)\n    print(\"Output:\\n\" + 100 * '-')\n    for i, sample_output in enumerate(sample_outputs):\n        print(\"{}: {}\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))","cff821dc":"# only train on questions less than 150 chars long\nq2 = [q for q in questions if len(q) < 150]\n# encode using pretrained tokenizer\ntrain_encodings = tokenizer(q2, truncation=True, padding='max_length', max_length=150)#, return_tensors='tf')","73e7859a":"batch_size = 16\nepochs = 2\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices(\n    train_encodings['input_ids']\n    ).shuffle(1000)\\\n    .batch(batch_size, drop_remainder=True)\\\n    .prefetch(tf.data.experimental.AUTOTUNE)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)","b95ce117":"steps = len(train_dataset)\n\nfor e in range(epochs):\n    losses = []\n    start_time = time.time()\n    for i, inputs in train_dataset.enumerate():\n        i = i.numpy()\n        with tf.GradientTape() as tape:\n            loss_value = model(inputs, labels=inputs)['loss']\n\n        grad = tape.gradient(loss_value, model.trainable_variables)\n        optimizer.apply_gradients(zip(grad, model.trainable_variables))\n\n        losses.append(tf.reduce_mean(loss_value).numpy())\n\n        total_time = time.time() - start_time\n        time_per_step = total_time \/ (i+1)\n        eta = time_per_step * (steps - (i+1))\n        m, s = divmod(eta, 60)\n        h, m = divmod(m, 60)\n\n        print(f\"\\repoch {e+1}\/{epochs}, step {i+1}\/{steps}, loss:{np.mean(losses):.5f}, ETA:{int(h)}h{int(m)}m{int(s)}s\", end=\"\", flush=True)\n    print()","c8b5797a":"for prompt in [\"What is\", \"Who is\", \"Why do\", \"Where are\", \"How do\", \"When is\", \"Why does my\", \"What is your\", \"Where do\", \"If you could\"]:\n\n    input_ids = tokenizer.encode(prompt, return_tensors='tf')\n\n    sample_outputs = model.generate(\n        input_ids, \n        do_sample=True, \n        max_length=50, \n        top_p=0.92, \n        top_k=0,\n        num_return_sequences=3\n    )\n\n    print(100 * '-' + \"\\nPrompt:\", prompt)\n    print(\"Output:\\n\" + 100 * '-')\n    for i, sample_output in enumerate(sample_outputs):\n        print(\"{}: {}\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))","91c0238b":"It takes most people around 79 characters to ask a question, but some questions require a short prose to truly get their point across such as:","1102b299":"It seems that in early 2020, when people around the word were more bored and confused than ever before, there was a massive spike in questions posted to AskReddit. But this may be due to sampling biases.","c180ad83":"# Some light EDA\n\nReddit users are a seemingly curious bunch. The community known as [Ask Reddit](https:\/\/www.reddit.com\/r\/AskReddit\/) boasts over 30M members and even has its own [wikipedia entry](https:\/\/en.wikipedia.org\/wiki\/AskReddit), according to which:\n> The subreddit describes its focus as \"to ask and answer questions that elicit thought-provoking discussions\"\n\nHere we will explore a small portion of the questions posted to the AskReddit forum.\n\n## What is reddit asking?\n\ntake a sample of questions from the dataset","07902250":"Exploring the plot above it seems that newer posts tend to cluster towards the mean of the projected embedding. Obviously more thorough analysis is needed before any conclusions can be drawn, but it's fun to explore the interactive plot :)","a0d885d2":"Reddit sure likes talking about reddit.","f6efb5a2":"UMAP is another dimensionality reduction algorithm used for visualisation.\n\nHere, the sparse TF-IDF representation is reduced to a more manageable 100 dimensions before UMAP to improve the efficiency of UMAP. (this may still be quite slow)","4b8af6c3":"The votes are heavily skewed by just a few posts recieving tens of thousands of votes vs the median (50 percentile) votes of just 6. This is likely due to higher voted posts being shown to more users creating a self-reinforcing feedback.","4275c1ce":"## question length","7331cc3f":"# Asking questions with GPT-2\n\nI will be using the pretrained distilled gpt2 model from \ud83e\udd17 [Hugging Face](https:\/\/huggingface.co\/)","5dd5c271":"Exploring the plot above, there seem to be some clusters of semantically similar questions.","23c63f74":"# Here I will be finetuning GPT2 on the askreddit questions data","5436df71":"# Embeddings\n\nHere I am exploring different sentence embedding & visualisation strategies\n\n* Vectorise each sentence in TF-IDF form\n* Dimensionality reduction with LSA for visualisation\n\n\n* Vectorise each sentence in TF-IDF form\n* Dimensionality reduction with LSA and UMAP for visualisation\n\n\n* Vectorise each sentence with universal sentence encoder\n* Dimensionality reduction with PCA for visualisation","0d984f77":"## Interest in topics over time\n\n(anything measuring term fequency, especially over time, should be taken with a whole heap of salt as the dataset is most likely affected by sampling bias)","d87f738c":"It really seems like reddit is a place to ask almost anything. Can we find some patterns in the data?","cd0fc4e8":"Here is a sample of questions plotted against number of votes and dates posted to explore.\n\nThere is a weakly positive trend in the upper limit of votes with recency, but overall, newer questions seem to be clustering towards the lower end of the votes axis.","fcc51692":"## upvotes","20b33826":"We've already seen that the word 'what' is the second most common word in the whole dataset. How frequently do users ask \"what\" questions compared to other questions?","1e239724":"... clearly.","41c42fba":"It seems that with the increasing number to questions submitted from mid 2019 to early 2020, there was a corresponding decrease in the average votes per question. Are people just posting boring questions?","98a4ab6c":"Searching terms such as 'trump' can have misleading results as the word has multiple meanings.","09aebc97":"## Pretrained text generation\n\nsee https:\/\/huggingface.co\/blog\/how-to-generate for a very nice explanation of text generation methods","fd695000":"What are you in the 1% of? An oddly appropriate embedding for that question.\n\n\nExploring some of the data above, you can see that this has captured some of the semantics of the questions being asked, but it doesn't really give much interesting insight. A caveat of TF-IDF embedding seems to be that the length of the text is a significant factor.","5d5243a4":"## Fine tuning","9657ba80":"## Universal Sentence Encoder\n\nThe universal sentence encoder is a more sophisticated embedding algirithm from Google. The pretrained neural network is optimised for sentence encoding and outputs a single 512 dimension vector per input.","15d62d40":"## fine tuned text generation","e508d167":"Perhaps unsurprisingly, this looks very similar the most common words in the eglish language, except with words like \"what\", \"how\", \"why\" and of course \"reddit\" appearing more frequently than in common parlance. Does excluding stop words shed more light on what reddit users are asking?","93221c5a":"Reddit really is a place to ask anything."}}