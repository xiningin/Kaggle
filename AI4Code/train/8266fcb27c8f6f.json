{"cell_type":{"a81b2732":"code","f1bb211d":"code","64a646cd":"code","631cc4a4":"code","7603c11c":"code","c4d760df":"code","79eb47c3":"code","0460728f":"code","de8fd7cb":"code","cc5a83fd":"code","c7a62275":"code","0d08388b":"code","bd981845":"code","3a3499f1":"code","9ec8c405":"code","c63f3abc":"code","58618655":"code","935300e7":"code","53011aea":"code","bae2c477":"code","4c05a726":"code","7b098732":"code","f8d1499f":"code","4742d3d2":"code","d9f64751":"code","60266c35":"code","2a35577b":"code","3b2f6327":"code","8be73ee9":"code","16b96779":"code","10546916":"code","071ec256":"code","98189772":"markdown"},"source":{"a81b2732":"import numpy as np   # import numpy\nimport pandas as pd  # import pandas\nimport os\nimport gc   # for gabage collection\nimport seaborn as sns  # data visualization lib\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.sparse import csr_matrix, hstack\nimport operator\nfrom sklearn.model_selection import train_test_split,KFold,StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score,precision_recall_fscore_support,classification_report,confusion_matrix\nimport glob\nimport lightgbm as lgb # load lightGBM model\nimport pickle\n%matplotlib inline","f1bb211d":"DATA_PATH = '\/kaggle\/input\/homesite-quote-conversion'\nfile_name = os.path.join(DATA_PATH,r'train.csv.zip')\nfile_name","64a646cd":"df= pd.read_csv(file_name)\ndf.shape","631cc4a4":"#seperate the target \ny = df['QuoteConversion_Flag']\ny","7603c11c":"df.head()","c4d760df":"# find correlation between the features and drop one of two highly correlated ones.\ndef highly_corr_col(x):\n    corr_matrix = df[x].corr().abs()\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n    to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n    return to_drop","79eb47c3":"Field_col =[col for col in df if col.startswith('Field')]\nCoverageField_col = [col for col in df if col.startswith('CoverageField')]\nSalesField_col = [col for col in df if col.startswith('SalesField')]\npersonalField_col = [col for col in df if col.startswith('PersonalField')]\nPropertyField_col = [col for col in df if col.startswith('PropertyField')]\nGeographicField_col = [col for col in df if col.startswith('GeographicField')]\nField_col =[col for col in df if col.startswith('Field')]\nCoverageField_col = [col for col in df if col.startswith('CoverageField')]\nSalesField_col = [col for col in df if col.startswith('SalesField')]\npersonalField_col = [col for col in df if col.startswith('PersonalField')]\nPropertyField_col = [col for col in df if col.startswith('PropertyField')]\nGeographicField_col = [col for col in df if col.startswith('GeographicField')]","0460728f":"df.drop(highly_corr_col(Field_col), axis=1, inplace=True)\ndf.drop(highly_corr_col(CoverageField_col), axis=1, inplace=True)\ndf.drop(highly_corr_col(SalesField_col), axis=1, inplace=True)\ndf.drop(highly_corr_col(personalField_col), axis=1, inplace=True)\ndf.drop(highly_corr_col(PropertyField_col), axis=1, inplace=True)\ndf.drop(highly_corr_col(GeographicField_col), axis=1, inplace=True)","de8fd7cb":"df.head()","cc5a83fd":"# Convert str_type 'Date' into date_type\ndf['Date'] = pd.to_datetime(pd.Series(df['Original_Quote_Date']))\n\n# Drop 'Original_Quote_date'\ndf = df.drop('Original_Quote_Date', axis=1)\n\n# Extract year,month,weekday from 'Date'\ndf['Year'] = df['Date'].apply(lambda x: x.year)\ndf['Month'] = df['Date'].apply(lambda x: x.month)\ndf['weekday'] = df['Date'].apply(lambda x: x.weekday())\ndf['Quarter'] = df['Date'].apply(lambda x: x.quarter)\n\n# Drop 'Date' feature\ndf = df.drop('Date', axis=1)","c7a62275":"df.head()","0d08388b":"df['Month'].value_counts()","bd981845":"import seaborn as sns\nsns.distplot(df['Month'], kde=False)","3a3499f1":"import seaborn as sns\nsns.distplot(df[\"Year\"], kde=False)","9ec8c405":"import seaborn as sns\nsns.distplot(df[\"weekday\"], kde=False)","c63f3abc":"import seaborn as sns\nsns.distplot(df[\"Quarter\"], kde=False)","58618655":"# Let us organize above table and sort the table in terms of # of NAN in descending order\nnan_info = pd.DataFrame(df.isnull().sum()).reset_index()\nnan_info.columns = ['feature_name','nan_cnt']\nnan_info.sort_values(by = 'nan_cnt',ascending=False,inplace=True)\nnan_info['nan_percentage'] = nan_info['nan_cnt']\/len(df)\nnan_info","935300e7":"nan_info.head(10)","53011aea":"features = [f for f in df.columns.values if f not in ['QuoteConversion_Flag','QuoteNumber']] # you have to customize this according to your own needs\nprint(features)","bae2c477":"cols_with_missing = nan_info.loc[nan_info.nan_cnt>0].feature_name.values\nprint(cols_with_missing)","4c05a726":"for f in cols_with_missing:\n    print(f,':', df[f].dtype,' nan percentage:', nan_info.loc[nan_info.feature_name==f].nan_percentage.values[0])","7b098732":"def enc(x):\n    le=preprocessing.LabelEncoder()\n    le.fit(list(x.values))\n    x=le.transform(list(x.values))\n    return x","f8d1499f":"for ft in cols_with_missing:\n    if df[ft].dtype == 'object':\n        df[ft].fillna('unknown',inplace=True)\n    else:\n        df[ft].fillna(-1, inplace=True)\n    \n    print(enc(df[ft]))","4742d3d2":"#Convert all strings to equivalent numeric representations:\nfor f in df.columns:\n     if df[f].dtype=='object':\n            #print(df[f])\n            print(enc(df[f]))","d9f64751":"category_features = []\nf_cat = []\nthreshold = 70\nfor each in features:\n\n    if df[each].nunique() < threshold:\n        category_features.append(each)\nfor each in category_features:\n    df[each] = df[each].astype('category')\n    #df_cat.append(each)\n    #print(df[each])\n    print(enc(df[each]))\n    f_cat.append(each)","60266c35":"X = csr_matrix(pd.get_dummies(df[f_cat],drop_first=True,prefix=f_cat,sparse=True)).tocsr()\nX","2a35577b":"df.isnull().sum().sum()","3b2f6327":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nX = X.toarray()\ny = df['QuoteConversion_Flag'].values\nX.shape,len(y)\n #split 20% data as test data\nX_train,X_test,y_train,y_test = train_test_split(X,y,shuffle=True,random_state=2019,test_size=0.2)\n#print(train_X.shape,test_X.shape,len(train_y),len(test_y))\nX_train.shape,X_test.shape,len(y_train),len(y_test)","8be73ee9":"clf = RandomForestClassifier(n_estimators=500, max_depth=10, random_state=18, max_leaf_nodes=64, verbose=1,\n                                 n_jobs=4)\nscores_rfc = []\n# models1 = []\n# initialize KFold, we vcan use stratified KFold to keep the same imblance ratio for target\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=10)\nfor i, (train_idx, valid_idx) in enumerate(kf.split(X_train, y_train)):\n    print('...... training {}th fold \\n'.format(i + 1))\n    tr_x = X_train[train_idx]\n    tr_y = y_train[train_idx]\n\n    val_x = X_train[valid_idx]\n    val_y = y_train[valid_idx]\n    model = clf\n    model.fit(tr_x, tr_y)\n    # picking best model?\n    pred_val_y = model.predict(val_x)\n    # measuring model vs validation\n    score_rfc = roc_auc_score(val_y, pred_val_y)\n    scores_rfc.append(score_rfc)\n    print('current performance by auc:{}'.format(score_rfc))\n# auc_scores1.append(auc)\n# models1.append(model)\nbest_f1 = -np.inf\nbest_thred = 0\nv = [i * 0.01 for i in range(50)]\nfor thred in v:\n    preds = (pred_val_y > thred).astype(int)\n    f1 = f1_score(val_y, preds)\n    if f1 > best_f1:\n        best_f1 = f1\n        best_thred = thred\ny_pred_rfc = (pred_val_y > best_thred).astype(int)\nprint(confusion_matrix(val_y, y_pred_rfc))\nprint(f1_score(val_y, y_pred_rfc))\nprint('the average mean auc is:{}'.format(np.mean(scores_rfc)))","16b96779":"from sklearn.metrics import roc_auc_score  \nfrom sklearn.metrics import roc_curve\nmodel_lgb = lgb.LGBMClassifier(n_jobs=4, n_estimators=10000, boost_from_average='false', learning_rate=0.01,\n                                num_leaves=64, num_threads=4, max_depth=-1, tree_learner=\"serial\",\n                                feature_fraction=0.7, bagging_freq=5, bagging_fraction=0.7, min_data_in_leaf=100,\n                                silent=-1, verbose=-1, max_bin=255, bagging_seed=11, )\nauc_scores = []\nmodels = []\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=10)\nfor i, (train_idx, valid_idx) in enumerate(kf.split(X_train, y_train)):\n    print('...... training {}th fold \\n'.format(i + 1))\n    tr_x = X_train[train_idx]\n    tr_y = y_train[train_idx]\n\n    va_x = X_train[valid_idx]\n    va_y = y_train[valid_idx]\n    model = model_lgb  # you need to initialize your lgb model at each loop, otherwise it will overwrite\n    model.fit(tr_x, tr_y, eval_set=[(tr_x, tr_y), (va_x, va_y)], eval_metric='auc', verbose=500,\n                early_stopping_rounds=300)\n# calculate current auc after training the model\n    pred_va_y = model.predict_proba(va_x, num_iteration=model.best_iteration_)[:, 1]\n    auc = roc_auc_score(va_y, pred_va_y)\n    print('current best auc score is:{}'.format(auc))\n    auc_scores.append(auc)\n    models.append(model)\n\nbest_f1 = -np.inf\nbest_thred = 0\nv = [i * 0.01 for i in range(50)]\nfor thred in v:\n    preds = (pred_va_y > thred).astype(int)\n    f1 = f1_score(va_y, preds)\n    if f1 > best_f1:\n        best_f1 = f1\n        best_thred = thred\ny_pred_lgb = (pred_va_y > best_thred).astype(int)\nprint(confusion_matrix(va_y, y_pred_lgb))\nprint(f1_score(va_y, y_pred_lgb))\nprint('the average mean auc is:{}'.format(np.mean(auc_scores)))\nfpr, tpr, _ = roc_curve(va_y, pred_va_y)\n# plot model roc curve\nplt.plot(fpr, tpr, marker='.', label='LGB model')\n# axis labels\nplt.title('ROC AUC CURVE')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.savefig('LGB ROC_auc_curve.png')\nplt.show()\n# Test data\npred_test_1 = models[0].predict_proba(X_test, num_iteration=models[0].best_iteration_)[:, 1]\npred_test_2 = models[1].predict_proba(X_test, num_iteration=models[1].best_iteration_)[:, 1]\npred_test_3 = models[2].predict_proba(X_test, num_iteration=models[2].best_iteration_)[:, 1]\npred_test_4 = models[3].predict_proba(X_test, num_iteration=models[3].best_iteration_)[:, 1]\npred_test_5 = models[4].predict_proba(X_test, num_iteration=models[4].best_iteration_)[:, 1]\npred_test = (pred_test_1 + pred_test_2 + pred_test_3 + pred_test_4 + pred_test_5) \/ 5.0\nprint(pred_test)        ","10546916":"# Logging for Visual Comparison\n#log_cols=[\"Classifier\", \"AUC Score\", \"f1-Score\"]\n#log = pd.DataFrame(columns=log_cols)\n\n#for clf in classifiers:\n #   clf.fit(X_train, y_train)\n  #  name = clf.__class__.__name__\n    \n   # print(\"=\"*30)\n    #print(name)\n    \n    #print('****Results****')\n   # print('current best auc score is:{}'.format(auc))\n    #train_predictions = clf.predict_proba(X_test)\n    #ll = log_loss(y_test, train_predictions)\n    #print(\"Log Loss: {}\".format(ll))","071ec256":"#from sklearn.model_selection import cross_validate\n#from sklearn.ensemble import RandomForestClassifier\n#random_forest = RandomForestClassifier()\n\n#scoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\n#scores = cross_validate(random_forest, X_train, y_train, scoring=scoring, cv=20)\n\n#sorted(scores.keys())\n#forest_fit_time = scores['fit_time'].mean()\n#forest_score_time = scores['score_time'].mean()\n#forest_accuracy = scores['test_accuracy'].mean()\n#forest_precision = scores['test_precision_macro'].mean()\n#forest_recall = scores['test_recall_macro'].mean()\n#forest_f1 = scores['test_f1_weighted'].mean()\n#forest_roc = scores['test_roc_auc'].mean()","98189772":"Spliting the data into Training and test"}}