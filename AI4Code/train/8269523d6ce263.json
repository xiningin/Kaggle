{"cell_type":{"57142569":"code","b58a330f":"code","3b0a35b9":"code","227d4c55":"code","0e1d14fd":"code","06362939":"code","4ec98172":"code","76b801e9":"code","fa354fd5":"code","fd0cb360":"code","e56d63b8":"code","6cc17f1c":"code","05fc99f7":"code","3c7ce5d5":"code","a94792f8":"code","54b9ff82":"code","d4925d59":"code","2bfb03ea":"code","55d7b64c":"code","3d0bcd78":"code","4f3e95fe":"code","c3c56c1d":"code","d5600928":"code","0ae6e6ad":"markdown","f86d3b4a":"markdown","4d2c155a":"markdown","a8833219":"markdown","e3255d84":"markdown","38b0d731":"markdown","510b5a25":"markdown","3317d762":"markdown","d5276bce":"markdown","3b26bbd4":"markdown","cdf56068":"markdown","a5d81e98":"markdown","48ff158e":"markdown","5f4b1894":"markdown","b4b319f7":"markdown","4ec03588":"markdown","3d30a38a":"markdown","b3cd85af":"markdown","72ba1d07":"markdown","edc3f382":"markdown"},"source":{"57142569":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt\nimport random\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b58a330f":"# import gym library\n# for more information please visit \n# - https:\/\/gym.openai.com\/envs\/Taxi-v3\/\n# - https:\/\/github.com\/openai\/gym\/blob\/master\/gym\/envs\/toy_text\/taxi.py\n\nimport gym","3b0a35b9":"# create taxi environment\nenv = gym.make('Taxi-v2').env","227d4c55":"# taxi environment has been created, to test this:\nenv","0e1d14fd":"# to see this environment use: .render():\nenv.render()","06362939":"env.reset() # reset environment and return random initial state","4ec98172":"print('State space: ', env.observation_space) # will show us all possible states","76b801e9":"print('Action space: ', env.action_space) # will show us all possible actions","fa354fd5":"# taxi row, taxi columnn, passenger location, destination respectively\nstate = env.encode(3,1,2,3) # return state value from 500 state space options\nprint('state number: ',state)","fd0cb360":"# lets see this location, we expect that taxi is at location 3x1, \n# passenger is at 2 (namely at Y) and destination is 3 (namely B)\nenv.s = state\nenv.render()","e56d63b8":"env.P[331]","6cc17f1c":"env.reset() # reset first\n\ntime_step = 0\ntotal_reward = 0\nlist_visualize = []\n\n# this while loop is only one episopde\nwhile True:\n    \n    time_step +=1\n    \n    # choose action\n    action = env.action_space.sample() # take random sample from action space {0,1,2,3,4,5}\n    \n    # perform action and get reward\n    state, reward, done, _ = env.step(action) # here state = next_state\n    \n    # total reward\n    total_reward += reward\n    \n    # visualize\n    list_visualize.append({'frame': env, \n                       'state': state,\n                       'action': action,\n                       'reward': reward,\n                       'Total_Reward': total_reward\n                      })\n    # visualize all steps\n    if time_step %100 == 0:\n        env.render()\n    \n    if done: \n        break\n    ","05fc99f7":"print('number of iterations: ', time_step)\nprint('Total reward: ', total_reward)","3c7ce5d5":"# to see slowly how our taxi moves in the environment: \n'''\nimport time  \n\nfor c, value in enumerate(list_visualize):\n    print(value[\"frame\"].render())\n    print('Time_step: ', c + 1)\n    print('Action: ', value[\"action\"])\n    print('State: ', value[\"state\"])\n    print('Reward: ', value[\"reward\"])\n    print('Total_reward: ', value[\"Total_Reward\"])\n    time.sleep(1)\n'''","a94792f8":"# lets make 3 episodes (3 while loops)\n\nfor i in range(3):\n    \n    env.reset()\n    new_time_step = 0\n    \n    while True:\n    \n        new_time_step +=1\n\n        # choose action\n        action = env.action_space.sample() # take random sample from action space {0,1,2,3,4,5}\n\n        # perform action and get reward\n        state, reward, done, _ = env.step(action) # here state = next_state\n\n        # total reward\n        total_reward += reward\n\n        # visualize\n        list_visualize.append({'frame': env, \n                           'state': state,\n                           'action': action,\n                           'reward': reward,\n                           'Total_Reward': total_reward\n                          })\n\n        if done: \n            break\n    print('number of iterations: ', new_time_step)\n    print('Total reward: ', total_reward)\n    print('-'*40)","54b9ff82":"# Q learning template\n\nimport gym\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n\nenv = gym.make('Taxi-v2').env\n\n# Q table\n\nq_table = np.zeros([env.observation_space.n, env.action_space.n]) # zeros(states, actions) and use .n to make it integer\n\n# hyperparameters: alpha, gamma, epsilon\n\nalpha = 0.1\ngamma = 0.9\nepsilon = 0.1\n\n# plotting metrix\n\nreward_list = []\ndropouts_list = []\n\nepisode_number = 1000 # number of trainings \n\nfor i in range(1, episode_number):\n    \n    # initialize environment\n    \n    state = env.reset() # For each episode, reset our environment and it returns new starting state \n    \n    reward_count = 0\n    dropouts = 0\n    \n    while True:\n        \n        # exploit OR explore in order to choose action (using The Epsilon-Greedy Algorithm)\n        # epsilon = 0.1 means 10% explore and 90% exploit\n    \n        if random.uniform(0,1) < epsilon:\n            action = env.action_space.sample()\n        else:\n            action = np.argmax(q_table[state]) # let state = 4, so action = argument where \n                                               # q_table[4] has max value\n                                               # q_table is 500x5 matrix\n                                               # q_table[4] = [0, 12, 32, 2, 5]\n                                               # action = in third column corresponding to 32 let say south\n        \n        # action process and take reward \/ observation\n        next_state, reward, done, _ = env.step(action) # .step(action) performs action and returns 4 parameteres \n                                                       # which are the next state, reward, false or true, end probaility\n        \n        # Q learning funtion update\n        \n        # Q(s,a)\n        old_q_value = q_table[state,action]\n        \n        # max Q`(s`, a`)\n        next_q_max = np.max(q_table[next_state])\n        \n        # find new Q value using Q funtion\n        next_q_value = (1 - alpha) * old_q_value + alpha * (reward + gamma * next_q_max)\n        \n        # Q table update\n        q_table[state,action] = next_q_value\n        \n        # update state\n        state = next_state\n        \n        # find wrong drop-outs, no need for this actually, we will use it for visualization purposes \n        if reward == -10:\n            dropouts += 1\n        \n        # find total reward\n        reward_count += reward\n        \n        if done:\n            break\n    if i%10 == 0:\n        dropouts_list.append(dropouts)\n        reward_list.append(reward_count)\n        print('Episode: {}, reward: {}, wrong dropouts: {}'.format(i, reward_count, dropouts))","d4925d59":"fig, (axs1,axs2) = plt.subplots(1,2, figsize=(12, 6)) # create in 1 line 2 plots\n\naxs1.plot(reward_list)\naxs1.set_xlabel('episode*10')\naxs1.set_ylabel('reward')\naxs1.grid(True)\n\naxs2.plot(dropouts_list)\naxs2.set_xlabel('episode*10')\naxs2.set_ylabel('wrong dropouts')\naxs2.grid(True)\n\nplt.show()","2bfb03ea":"# now we have a good Q table that can help me\nq_table","55d7b64c":"env.render() # our environment right now","3d0bcd78":"at_this_state = env.encode(3,0,2,3) # taxi is at location 3x0, passenger is at location 2 and destination is 3\nenv.s = at_this_state\nenv.render()","4f3e95fe":"q_table[at_this_state]","c3c56c1d":"# let taxi be at 1x4, passenger in taxi (taxi will be green) (4), destination G (1) \nanother_state = env.encode(1,4,4,1)\nenv.s = another_state\nenv.render()","d5600928":"q_table[another_state]","0ae6e6ad":"Passenger locations:\n    - 0: R(ed)\n    - 1: G(reen)\n    - 2: Y(ellow)\n    - 3: B(lue)\n    - 4: in taxi\n    \nDestinations: \n    - 0: R(ed)\n    - 1: G(reen)\n    - 2: Y(ellow)\n    - 3: B(lue)\n    \n Rendering:\n    - blue: passenger\n    - magenta: destination\n    - yellow: empty taxi\n    - green: full taxi\n    - other letters (R, G, Y and B): locations for passengers and destinations\n    \nstate space is represented by:\n     - (taxi_row, taxi_col, passenger_location, destination)\n\nEnvironment is 5x5 matrix\n\nThere are 5x5x5x4 = 500 random states in total","f86d3b4a":"**1. Preliminary Work**","4d2c155a":"# these codes below are not related with this kernel\n\nfrom collections import deque\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\n\n# at this point, go to setting on the right menu to enable internet option ON\n!pip install --upgrade pip\n\n!pip install pygame","a8833219":"**Another example**","e3255d84":"Actions:\n    There are 6 discrete deterministic actions:\n    - 0: move south\n    - 1: move north\n    - 2: move east \n    - 3: move west \n    - 4: pickup passenger\n    - 5: dropoff passenger\n    \n","38b0d731":"**5. Visualization of wrong dropouts and rewards**","510b5a25":"**3. Let taxi to start its journey in the environment **","3317d762":"![](https:\/\/storage.googleapis.com\/lds-media\/images\/Reinforcement_Learning_Taxi_Env.width-1200.png)","d5276bce":"what kind of action is expected from the taxi at this moment?\n\nanswer: When look at the environment above, taxi is yellow which means it is empty. So, it should go to **south** first in order to pick-up the passeenger","3b26bbd4":"**4. Q LEARNING**","cdf56068":"We see that most probably taxi will take the correct action :)","a5d81e98":"- we see that max value of Q table at this state is at column 0\n- q_table[at_this_state, 0] should be performed, namely we anticipate to take the action 0\n\nlets remember actions again \n\nActions:\n    There are 6 discrete deterministic actions:\n    - 0: move south\n    - 1: move north\n    - 2: move east \n    - 3: move west \n    - 4: pickup passenger\n    - 5: dropoff passenger\n    ","48ff158e":"**6. Lets investigate Q table**","5f4b1894":"**lets remember again**\n\nPassenger locations:\n    - 0: R(ed)\n    - 1: G(reen)\n    - 2: Y(ellow)\n    - 3: B(lue)\n    - 4: in taxi\n    \nDestinations: \n    - 0: R(ed)\n    - 1: G(reen)\n    - 2: Y(ellow)\n    - 3: B(lue)","b4b319f7":"![](https:\/\/cdn-media-1.freecodecamp.org\/images\/TnN7ys7VGKoDszzv3WDnr5H8txOj3KKQ0G8o)","4ec03588":"When the taxi is yellow it means it is empty and when it becomes green it means that taxi has picked-up the passenger","3d30a38a":"**2. Explanations**","b3cd85af":"Q learning algorithm and function\n\nhttps:\/\/www.freecodecamp.org\/news\/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc\/","72ba1d07":"we anticipate that taxi will go towards **north**\n\nlets see what kind of action it will most probably take","edc3f382":"we see that max value is at column 1 (take action 1) meaning move north "}}