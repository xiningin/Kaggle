{"cell_type":{"f38978d0":"code","be297f22":"code","e90eb494":"code","dafddf27":"code","7ee463bf":"code","285d5ecf":"code","590b4582":"code","427ea49e":"code","f16d17fd":"code","fc28a441":"code","85616be5":"code","71dffc2a":"code","eee17c63":"markdown","b854fb69":"markdown","4fd2a1ec":"markdown","eac70bac":"markdown","3428d08d":"markdown","36768c5a":"markdown","2b714de5":"markdown"},"source":{"f38978d0":"#Import Libraries\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn import tree\nfrom sklearn.tree import _tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import classification_report\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n%matplotlib inline","be297f22":"# Read CSV into a DataFrame\ndf = pd.read_csv('\/kaggle\/input\/drugs-a-b-c-x-y-for-decision-trees\/drug200.csv')\n\n# Data has been cleaned and no need to perform data quality checks\ndf.shape ","e90eb494":"df.head(5)\n#print(df['Sex'].value_counts())\n#print(df['BP'].value_counts())\n#print(df['Cholesterol'].value_counts())","dafddf27":"my_cols = [\"Sex\", \"BP\", \"Cholesterol\"]\ndf1 = pd.get_dummies(data=df, columns=my_cols)","7ee463bf":"df1.head(5)","285d5ecf":"x_train, x_test, y_train, y_test = train_test_split(df1.drop(['Drug'], axis=1)\n                                                    , df1['Drug']\n                                                    , test_size = 0.33\n                                                    , random_state = 42)\nx_train.shape, x_test.shape","590b4582":"clf = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=42)\nclf.fit(x_train,y_train)\ny_pred = clf.predict(x_test)\n\nprint('Model accuracy score with criterion gini index: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\ny_pred_train = clf.predict(x_train)\ny_pred_train\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))\nprint(classification_report(y_train, y_pred_train))\n#print(confusion_matrix(y_train, y_pred_train))","427ea49e":"plt.barh(df1.drop(columns=['Drug']).columns, clf.feature_importances_)","f16d17fd":"def get_rules(tree, feature_names, class_names):\n    tree_ = tree.tree_\n    feature_name = [\n        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n        for i in tree_.feature\n    ]\n\n    paths = []\n    path = []\n    \n    def recurse(node, path, paths):\n        \n        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n            name = feature_name[node]\n            threshold = tree_.threshold[node]\n            p1, p2 = list(path), list(path)\n            p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n            recurse(tree_.children_left[node], p1, paths)\n            p2 += [f\"({name} > {np.round(threshold, 3)})\"]\n            recurse(tree_.children_right[node], p2, paths)\n        else:\n            path += [(tree_.value[node], tree_.n_node_samples[node])]\n            paths += [path]\n            \n    recurse(0, path, paths)\n\n    # sort by samples count\n    samples_count = [p[-1][1] for p in paths]\n    ii = list(np.argsort(samples_count))\n    paths = [paths[i] for i in reversed(ii)]\n    \n    rules = []\n    for path in paths:\n        rule = \"if \"\n        \n        for p in path[:-1]:\n            if rule != \"if \":\n                rule += \" and \"\n            rule += str(p)\n        rule += \" then \"\n        if class_names is None:\n            rule += \"response: \"+str(np.round(path[-1][0][0][0],3))\n        else:\n            classes = path[-1][0][0]\n            l = np.argmax(classes)\n            rule += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]\/np.sum(classes),2)}%)\"\n        rule += f\" | based on {path[-1][1]:,} samples\"\n        rules += [rule]\n        \n    return rules","fc28a441":"rules = get_rules(clf\n                  , list(df1.drop(columns=['Drug']).columns)\n                  , class_names=['drugA', 'drugB', 'drugC', 'drugX', 'drugY'])\nfor r in rules:\n    print(\"==========\")\n    print(r)","85616be5":"# Manually Validating\n\nprint(\"Drug A\")\nprint(df[(df['Na_to_K'] <= 14.829) & (df['BP'] == 'HIGH') & (df['Age'] <= 50)]['Drug'].value_counts())\n\nprint(\"\\nDrug B\")\nprint(df[(df['Na_to_K'] <= 14.829) & (df['BP'] == 'HIGH') & (df['Age'] > 50)]['Drug'].value_counts())\n\nprint(\"\\nDrug C\")\nprint(df[(df['Na_to_K'] <= 14.829) & (df['BP'] == 'LOW') & (df['Cholesterol'] == 'HIGH')]['Drug'].value_counts())\n\nprint(\"\\nDrug X\")\nprint(df[(df['Na_to_K'] <= 14.829) & (df['BP'] == 'LOW') & (df['Cholesterol'] != 'HIGH')]['Drug'].value_counts())\n\nprint(\"\\nDrug Y\")\nprint(df[df['Na_to_K'] > 14.829]['Drug'].value_counts())","71dffc2a":"fig = plt.figure(figsize=(25,20))\ntree.plot_tree(clf,\n               feature_names = list(df1.drop(columns=['Drug']).columns), \n               class_names=['drugA', 'drugB', 'drugC', 'drugX', 'drugY'],\n               filled = True)","eee17c63":"# Recommending which Drug to which Patient\n\n<i>Answer:<\/i><br>\n<u><b>Order of Drug Operation:<\/b><\/u>\n1. If `Na_to_K` greater than `14.829` recommend **Drug Y** (if less, go to step 2)\n2. If `Blood Pressure` is `HIGH` then (if not high, go to step 3)\n    -  `Age` is `50 or younger` then recommend **Drug A**\n    -  `Age` is `Older than 50` then recommend **Drug B**\n3. If `Cholesterol` is `HIGH` then **Drug C**, if not the recommend **Drug X**\n\n<br>\n\n**See how below!**","b854fb69":"## Features of Importance\n`Na_to_K` this feature being the strongest makes sense as it was found in a <a href=\"https:\/\/www.nih.gov\/news-events\/nih-research-matters\/sodium\/potassium-ratio-linked-cardiovascular-disease-risk#:~:text=A%20high%20sodium%2Fpotassium%20ratio,the%20incidence%20of%20cardiovascular%20disease.\">2009 study<\/a> that there is a strong correlation between Na and K when it comes to cardiovascular disease. \n\n`High Blood Pressure` this feature being strong makes sense as generally people with high blood pressure are at increased risks of heart disease (ex: Heart attacks). \n\n`Gender` appears to bear no significance when recommending a drug","4fd2a1ec":"# Rule Generation\nEffective rules for recommending a drug to a patient","eac70bac":"# Conclusion\n\n<u><b>Order of Drug Operation:<\/b><\/u>\n1. If `Na_to_K` greater than `14.829` recommend **Drug Y** (if less, go to step 2)\n2. If `Blood Pressure` is `HIGH` then (if not high, go to step 3)\n    -  `Age` is `50 or younger` then recommend **Drug A**\n    -  `Age` is `Older than 50` then recommend **Drug B**\n3. If `Cholesterol` is `HIGH` then **Drug C**, if not the recommend **Drug X**","3428d08d":"### Read CSV and probe data\nNo need for data quality checks & remediation as population of 200 and no null values","36768c5a":"## One-Hot Encode Sex, Blood Pressure, and Cholesterol\nThis will denote 1 or 0 for each value inside each column.","2b714de5":"## Decision Tree\n1.  Split the dataset into thirds: train & testing\n2.  Set the depth at 4 (At 4, we find 100% accuracy)\n    -  Usually 100% accuracy is frowned upon as there may be `overfitting`. However, due to the population being 200 and the quantity of categorical variables used in the final recommendation, I feel comfortable with the precision\/recall score spitting out perfection."}}