{"cell_type":{"db0401cd":"code","7c0e3c39":"code","e8da622c":"code","f08a0015":"code","30114d8e":"code","9bd27eff":"code","67469968":"code","67009146":"code","f5cae7ac":"code","aa9ebd1c":"code","2a845d35":"code","2e11f334":"code","95052b8c":"code","c161b3b9":"code","31dcd7dc":"code","68917d87":"markdown","520a202c":"markdown","6cb0815b":"markdown","9640965a":"markdown","47639d3f":"markdown","f44f7532":"markdown","3f8abb44":"markdown","dfcc5fef":"markdown","2e7f009c":"markdown","d73101ce":"markdown","03ae50f3":"markdown","53243dba":"markdown","f2be22bb":"markdown","83ac77db":"markdown","fff13841":"markdown","e072e56c":"markdown","5e3ff5f6":"markdown","72956fc6":"markdown","43016bf3":"markdown","87901f51":"markdown","a1db03b1":"markdown","8fa766c1":"markdown","46aa54be":"markdown","02383736":"markdown","eaa23714":"markdown","63d57aff":"markdown","50c5808e":"markdown"},"source":{"db0401cd":"!pip install hoggorm #not very popular, let's install\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib as plt\nimport seaborn as sns\nimport matplotlib.pyplot\nimport scipy.stats as scp\nfrom scipy.spatial import distance\nfrom scipy.spatial.distance import pdist, squareform\nimport hoggorm as hogg\nimport sklearn.preprocessing as skl\n\nVariables = ['video_id', 'views', 'likes', 'dislikes', 'comment_count']\n        \nus_yt = pd.read_csv('..\/input\/youtube-new\/USvideos.csv', usecols=Variables) #USA\nca_yt = pd.read_csv('..\/input\/youtube-new\/CAvideos.csv', usecols=Variables) #Canada\nde_yt = pd.read_csv('..\/input\/youtube-new\/DEvideos.csv', usecols=Variables) #Germany\nfr_yt = pd.read_csv('..\/input\/youtube-new\/FRvideos.csv', usecols=Variables) #France\ngb_yt = pd.read_csv('..\/input\/youtube-new\/GBvideos.csv', usecols=Variables) #Great Brittain\nin_yt = pd.read_csv('..\/input\/youtube-new\/INvideos.csv', usecols=Variables) #India\njp_yt = pd.read_csv('..\/input\/youtube-new\/JPvideos.csv', usecols=Variables) #Japan\nkr_yt = pd.read_csv('..\/input\/youtube-new\/KRvideos.csv', usecols=Variables) #South Korea\nmx_yt = pd.read_csv('..\/input\/youtube-new\/MXvideos.csv', usecols=Variables) #Mexico\nru_yt = pd.read_csv('..\/input\/youtube-new\/RUvideos.csv', usecols=Variables) #Russia","7c0e3c39":"N = (us_yt.dtypes == 'int64')\nNumeric = list(N[N].index)","e8da622c":"# Center the data before computation of RV coefficients\nus_yt_sc = skl.scale(us_yt[Numeric], axis=0, with_mean=True)\nca_yt_sc = skl.scale(ca_yt[Numeric], axis=0, with_mean=True)\nde_yt_sc = skl.scale(de_yt[Numeric], axis=0, with_mean=True)\nfr_yt_sc = skl.scale(fr_yt[Numeric], axis=0, with_mean=True)\ngb_yt_sc = skl.scale(gb_yt[Numeric], axis=0, with_mean=True)\nin_yt_sc = skl.scale(in_yt[Numeric], axis=0, with_mean=True)\njp_yt_sc = skl.scale(jp_yt[Numeric], axis=0, with_mean=True)\nkr_yt_sc = skl.scale(kr_yt[Numeric], axis=0, with_mean=True)\nmx_yt_sc = skl.scale(mx_yt[Numeric], axis=0, with_mean=True)\nru_yt_sc = skl.scale(ru_yt[Numeric], axis=0, with_mean=True)","f08a0015":"us_yt['Country'] = \"US\"\nca_yt['Country'] = \"CA\"\nde_yt['Country'] = \"DE\"\nfr_yt['Country'] = \"FR\"\ngb_yt['Country'] = \"GB\"\nin_yt['Country'] = \"IN\"\njp_yt['Country'] = \"JP\"\nkr_yt['Country'] = \"KR\"\nmx_yt['Country'] = \"MX\"\nru_yt['Country'] = \"RU\"\n \ndf = pd.concat([us_yt, ca_yt, de_yt,fr_yt,gb_yt,in_yt,jp_yt,kr_yt,mx_yt,ru_yt] )\ndf.reset_index\ndf.head()","30114d8e":"Missing_Percentage = (df.isnull().sum()).sum()\/np.product(df.shape)*100\nprint(\"The number of missing entries: \" + str(round(Missing_Percentage,8)) + \" %\")","9bd27eff":"PearsonCorr = df.corr(method=\"pearson\")\nmatplotlib.pyplot.figure(figsize=(10,10))\nsns.heatmap(PearsonCorr, vmax=.9, square=True)","67469968":"SpearmanCorr = df.corr(method=\"spearman\")\nmatplotlib.pyplot.figure(figsize=(10,10))\nsns.heatmap(SpearmanCorr, vmax=.9, square=True)","67009146":"DistanceCorr = pd.DataFrame([[0.00]*4,[0.00]*4,[0.00]*4,[0.00]*4], columns=['views','likes','dislikes','comment_count'], index=['views','likes','dislikes','comment_count'])\n\nfor i in range(0,4):\n    DistanceCorr.views[i] = -distance.correlation(df['views'],df[DistanceCorr.iloc[:, [i]].columns]) + 1 \n    DistanceCorr.likes[i] = -distance.correlation(df['likes'],df[DistanceCorr.iloc[:, [i]].columns]) + 1\n    DistanceCorr.dislikes[i] = -distance.correlation(df['dislikes'],df[DistanceCorr.iloc[:, [i]].columns]) + 1\n    DistanceCorr.comment_count[i] = -distance.correlation(df['comment_count'],df[DistanceCorr.iloc[:, [i]].columns]) + 1\n\nmatplotlib.pyplot.figure(figsize=(10,10))\nsns.heatmap(DistanceCorr, vmax=.9, square=True)","f5cae7ac":"PearsonCorr","aa9ebd1c":"SpearmanCorr","2a845d35":"DistanceCorr","2e11f334":"def distcorr(X, Y):\n    # Compute the distance correlation function\n    X = np.atleast_1d(X)\n    Y = np.atleast_1d(Y)\n    if np.prod(X.shape) == len(X):\n        X = X[:, None]\n    if np.prod(Y.shape) == len(Y):\n        Y = Y[:, None]\n    X = np.atleast_2d(X)\n    Y = np.atleast_2d(Y)\n    n = X.shape[0]\n    if Y.shape[0] != X.shape[0]:\n        raise ValueError('Number of samples must match')\n    a = squareform(pdist(X))\n    b = squareform(pdist(Y))\n    A = a - a.mean(axis=0)[None, :] - a.mean(axis=1)[:, None] + a.mean()\n    B = b - b.mean(axis=0)[None, :] - b.mean(axis=1)[:, None] + b.mean()\n    \n    dcov2_xy = (A * B).sum()\/float(n * n)\n    dcov2_xx = (A * A).sum()\/float(n * n)\n    dcov2_yy = (B * B).sum()\/float(n * n)\n    dcor = np.sqrt(dcov2_xy)\/np.sqrt(np.sqrt(dcov2_xx) * np.sqrt(dcov2_yy))\n    return dcor","95052b8c":"# Data sets have to have same length for this method\nShortest = min(ca_yt_sc.shape[0],de_yt_sc.shape[0],fr_yt_sc.shape[0],gb_yt_sc.shape[0],in_yt_sc.shape[0],\n                jp_yt_sc.shape[0],kr_yt_sc.shape[0],mx_yt_sc.shape[0],ru_yt_sc.shape[0],us_yt_sc.shape[0])\n\nUpper = 1000\n\nRV = pd.DataFrame(hogg.RVcoeff([ ca_yt_sc[0:Upper] , de_yt_sc[0:Upper], fr_yt_sc[0:Upper], gb_yt_sc[0:Upper], in_yt_sc[0:Upper], \n                    jp_yt_sc[0:Upper] , kr_yt_sc[0:Upper], mx_yt_sc[0:Upper], ru_yt_sc[0:Upper], us_yt_sc[0:Upper] ]),\n                 columns=['CA','DE','FR','GB','IN','JP','KR','MX','RU','US'])","c161b3b9":"matplotlib.pyplot.figure(figsize=(10,10))\nsns.heatmap(RV, vmax=.9, square=True)","31dcd7dc":"Check =  any(item in ca_yt.video_id for item in us_yt.video_id)\nCheck","68917d87":"After some checks it seems to work correctly.","520a202c":"Let's add the column with country origin and then merge all the data set.","6cb0815b":"# Data cleaning\nBasic libraries and data load:","9640965a":"No Canadian id's in American one. Similarly, we checked another ones. Hence, in this case method would not be very useful. However, for all other matrices with overlapping index the method will be fantastic global measure.","47639d3f":"**World data**\n\nStrong linear dependence, obviously between 'views' and 'likes', between 'likes' and 'comment_count' as well.","f44f7532":"**World data**\n\nAlright, let's apply this pretty measure to our data set.","3f8abb44":"We observe relevant differences between Perason and Spearman. It was expected cause linearity was not really good assumption for this case. However, in the last table I observe situation which introduces some confusion: namely, 'scipy' function seems to work exactly like Pearson' one, what is against any expectation (**Comments appreciated**). Because of it, I explored a bit GitHub and found good (but heavy regarding CPU & RAM) function ( [This guy is an author](https:\/\/gist.github.com\/satra\/aa3d19a12b74e9ab7941) ):","dfcc5fef":"# Pearson correlation coefficient\nFirst coefficient will be one of the most popular measures exisiting. It has couple of names, sometimes it is called Pearson' linear coefficient what better emphasizes what this measure actually does. To be precise, Pearson's measure gives us information not about correlation size, but how linear the dependence is. Namely, it provides us with information how strong is linear behaviour of dependence. ","2e7f009c":"$ r_{XY} = \\frac{cov({rg}_X,{rg}_Y)}{\\sigma_{rgX} \\times \\sigma_{rgY}}  $ ","d73101ce":"The Greek letter called rho correspond to the Perason correlation measure. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, the covariance is positive.","03ae50f3":"# Additional: RV coef.\nHere, we don't define really a new correlation measure per se, but it's generalization. RV coefficient is a multivariate generalization of the squared Pearson correlation coefficient. \n\nThis barely recognized measure has huge use in many statistical methods, like for example Principial component analysis (PCA) or powerful multivariate regression.\n\nThe formula is given as:\n\n$ RV(XY) = \\frac{COVV(X,Y)}{VAV(X) \\times VAV(Y)}  $ \n\nwhere denominator corresponds to scalar-valued covariance and denominator to scalar-valued variance. Alright, these two names sound a bit exotic but indeed they are just a generalization of concept which we know from Pearson coefficient.","53243dba":"**Mathematical formulas will be described in intuitive but not fully comprehensive theory-wise way.**","f2be22bb":"Alright, cleaning NA's will be not a challange here. We simply don't have empty entries.","83ac77db":"$\\rho_{XY} = \\frac{cov(X,Y)}{\\sigma_X \\times \\sigma_Y}  $ ","fff13841":"# Intro to diffrent dependence measures","e072e56c":"**Country-level data**","5e3ff5f6":"# Distance correlation\nMathematically:\n> distance correlation or distance covariance is a measure of dependence between two paired random vectors of arbitrary, not necessarily equal, dimension. The population distance correlation coefficient is zero if and only if the random vectors are independent. Thus, distance correlation measures both linear and nonlinear association between two random variables or random vectors. This is in contrast to Pearson's correlation, which can only detect linear association between two random variables.\n\nFor this one, we will omit auxiliary formulas as it invloves slightly more complicated algebra ( [Link](https:\/\/en.wikipedia.org\/wiki\/Distance_correlation) ). \n\nIntuitively:\n\nThis measure comapres values between vectors by use of so-called norm, so jsut a type of distance between them. And similarly to previous two measures, result can be depicted by such a simple equation:\n\n$ dCor_{XY} = \\frac{dcov(X,Y)}{dVar(X) \\times dVar(Y)}  $ \n\nwhere: nominator corresponds to distance covariance and denominator to distance variance. If somebody was interested, distance covariance equals to Brownian covariance. Brownian covariance is the measure of covariance of Brownian motion ( [Link](https:\/\/en.wikipedia.org\/wiki\/Brownian_motion) ), one of the most beautiful and interesting stochastic processes. Distance covariance and Brownian covariance are examples of energy distances ( [Link](https:\/\/en.wikipedia.org\/wiki\/Energy_distance)), which are another fascinating statistical distances describing the differences between distriubtions.","72956fc6":"One of the tasks for this data set is understanding the dependence between \"number of views and the number of likes by means of a visual representation\". We know that there is important not only to precisely reflects trends noticed for the given measure, but also to fully understand the measure and the limitations of it. This notebook focuses then on the measures comparison and analysis of their shortcomings and advantages.","43016bf3":"# Spearman correlation coefficient\nSecond measure is only slightly less popular than Perason's one. It is not much more complicated but overperfoms the first one on many fields. Alternative name of this measure is rank correlation coefficient, what comes from the fact that it measures statistical dependence between the rankings of two variables. \n\nIn other words, it assesses how well the relationship between two variables can be described using a monotonic function. And yes, in this sentence, we can see the clear shortcoming of it, we have to assume the monotonicity of the relation. However, how we can expect, this requirement is far easier to be met than linearity one.","87901f51":"**World data**\n\nLet's apply Spearman's tool in practice.","a1db03b1":"Simplifying: RV coefficient will work like Pearson coefficient, but instead of vectors, we can compare matrices.","8fa766c1":"# Perason vs Spearman vs Distance results","46aa54be":"Scale and center the data.","02383736":"Isn't it perfect possibility to check the differences between different countries? Let's do it!","eaa23714":"Having all three matrices, we would like to check what are the differences between them. Namely, to compare the performance of three measures on the same data set.","63d57aff":"So what do we see? No dependence between sets of records from different countries. Why? Because actually we compare completely random data sets containing 1000 records, hence the results are consistent with expectation. \n\nAnother check would be interested: to take 1000 records but containing the same videos from different countries and investigate the differences between countries' reactions. In other words, to check how for example Gangnam Style or Despacito is affecting different societies.","50c5808e":"To create $ rg_X$ from $X$, we just need to transform our variables into rankings. It is easy and fast. Then we can appreciate our improved measure."}}