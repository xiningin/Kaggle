{"cell_type":{"027da144":"code","f67b343b":"code","eeed12d9":"code","34672e99":"code","c9855bca":"code","f3a732b5":"code","2b4a4c96":"code","ad070a1c":"code","c3c7358a":"code","42963f53":"code","5a3aa4f3":"code","7742e38e":"code","befc3df0":"code","e9c5abbf":"code","5ce39eb4":"code","1f1f8e5e":"code","f19d2fad":"code","b50d707a":"code","1d67c010":"code","11c1283e":"markdown","a28855ab":"markdown"},"source":{"027da144":"!pip install pytorch-pretrained-bert","f67b343b":"!pip install spacy ftfy==4.4.3\n!python -m spacy download en","eeed12d9":"import pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook, tqdm\ntqdm.pandas('my bar!')\n\nimport warnings\nwarnings.filterwarnings('ignore')","34672e99":"import torch\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n\n# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\n# Load pre-trained model tokenizer (vocabulary)\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n\n# Tokenized input\ntext = \"[CLS] Who was Jim Henson ? Jim Henson was a puppeteer [SEP]\"\ntokenized_text = tokenizer.tokenize(text)\n\n# Convert token to vocabulary indices\nindexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n# Define sentence\nsegments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n# Convert inputs to PyTorch tensors\ntokens_tensor = torch.tensor([indexed_tokens])\nsegments_tensors = torch.tensor([segments_ids])","c9855bca":"tokenized_text","f3a732b5":"# Load pre-trained model (weights)\nmodel = BertModel.from_pretrained('bert-large-uncased')\nmodel.eval()\n\n# If you have a GPU, put everything on cuda\ntokens_tensor = tokens_tensor.to('cuda')\nsegments_tensors = segments_tensors.to('cuda')\nmodel.to('cuda')","2b4a4c96":"# Predict hidden states features for each layer\nwith torch.no_grad():\n    _, embedding_data = model(tokens_tensor, segments_tensors, output_all_encoded_layers=False)","ad070a1c":"embedding_data.cpu().numpy()[0][1]","c3c7358a":"%%time\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")","42963f53":"train_df.head()","5a3aa4f3":"MAX_SEQ_LENGTH = 220","7742e38e":"def convert_lines(example, max_seq_length, tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm_notebook(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n        # for testing the aactivity of the program\n#         if longer > 3:\n#             break\n    print(longer)\n    return np.array(all_tokens)","befc3df0":"tokenized_text = convert_lines(train_df['comment_text'].values, MAX_SEQ_LENGTH, tokenizer)","e9c5abbf":"tokenized_text.shape","5ce39eb4":"def get_BERT_embedding(model, sentence_index):\n    # If you have a GPU, put everything on cuda\n    tokens_tensor = torch.tensor([sentence_index])\n    tokens_tensor = tokens_tensor.to('cuda')\n    model.to('cuda')\n    # Predict hidden states features for each layer\n    with torch.no_grad():\n        _, embedding_data = model(tokens_tensor, output_all_encoded_layers=False)\n    return embedding_data","1f1f8e5e":"def get_BERT_embeddings(model, sentence_indexes):\n    embeddings = []\n    for i in tqdm_notebook(range(len(sentence_indexes))):        \n        embeddings.append(get_BERT_embedding(model, sentence_indexes[i])[0].cpu().numpy())\n    return embeddings","f19d2fad":"embeddings = get_BERT_embeddings(model, tokenized_text)\nembeddings","b50d707a":"sub = pd.DataFrame({'id' : train_df['id'],\n                    'target' : train_df['target'],\n                    'embedding' : embeddings})\nsub.to_csv('embedding', index=False)","1d67c010":"sub","11c1283e":"# Get Embedding\n\nIn this script, I use some models to generate embedding for \u2018Jigsaw Unintended Bias in Toxicity Classification\u2018 task.\n\n- BERT-uncased-1024\n\nThe embedding data will be used for train a model to classify the toxicity comment.","a28855ab":"The below code is modified from [the html page](https:\/\/www.ctolib.com\/huggingface-pytorch-pretrained-BERT.html)."}}