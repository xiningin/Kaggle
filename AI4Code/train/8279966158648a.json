{"cell_type":{"59554cba":"code","e6dc9d20":"code","ae0ea818":"code","0ff8ee29":"code","485332ff":"code","7f4e8681":"code","4b7d02ec":"code","193be678":"code","6e0fa90b":"code","28ed3447":"code","0919a2d2":"code","9bccfa0d":"code","ee6af623":"code","9b0ddb8d":"code","2e219723":"code","5fe626ea":"code","d4ab5059":"code","031bac40":"code","18336be4":"code","c1b2546c":"code","90d9baee":"code","356d75dd":"code","f4300c86":"code","105643ec":"code","79b9ab88":"code","547c4455":"code","6a0835e6":"code","c4cbb1ce":"code","eda4214d":"code","f5dd3809":"code","7f1c19d5":"code","bf0b77ff":"code","946c8c4c":"markdown","c10fe4f9":"markdown","9165e45c":"markdown","fea7da65":"markdown","59a77b8d":"markdown","a4312768":"markdown","97f35c3b":"markdown","62862257":"markdown","adfba365":"markdown","f941db9b":"markdown","44e79104":"markdown","f348beb1":"markdown","d65b003a":"markdown","f1317412":"markdown","931a0176":"markdown","56761ced":"markdown","984d75b0":"markdown","76b825af":"markdown","b2dbd6d3":"markdown","b0d75bd1":"markdown","71a3ee43":"markdown","0cf6619a":"markdown","b0110cdf":"markdown","6684ceb9":"markdown","6c8b85a0":"markdown","16e0089d":"markdown"},"source":{"59554cba":"# This Python                               3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra                     \nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e6dc9d20":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F","ae0ea818":"import pickle\nimport bz2\nimport base64\nfrom collections import deque","0ff8ee29":"import copy\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Action\n\nN_ROWS = 7\nN_COLS = 11\nN_CELL = N_ROWS * N_COLS\nmax_moves = 200\n\nACTIONS = [\n    Action.NORTH.name,\n    Action.WEST.name,\n    Action.SOUTH.name,\n    Action.EAST.name,\n]\n\nclass Env:\n    __slots__ = 'board', 'num_move', 'prev_opp_actions_opposite', 'geese', 'rewards', 'foods', 'agents', 'alive', 'b_state', 'is_game_end', 'labyrint'\n    \n    def __init__(self, agents, b_state={\n            'blank': 0, # Empty cell\n            'x': -1, # Head of opponent player\n            'h': 1, # Head of current player\n            'b': -2, # Body cell\n            'f': 2, # Food\n        }, verbose=False, labyrint=0):\n        self.agents = agents\n        self.init_game(b_state, labyrint, verbose)\n        pass\n    \n    def init_game(self, b_state, labyrint, verbose=False):\n        self.labyrint = labyrint\n        self.is_game_end = False\n        self.alive = np.ones(len(self.agents))\n        \n        # \u0441\u0442\u0440\u043e\u0438\u043c \u0434\u043e\u0441\u043a\u0443\n        self.b_state = b_state\n        \n        board = b_state['blank'] * np.zeros(N_CELL).astype(int).reshape((N_ROWS, N_COLS))\n        self.num_move = 0\n        \n        n = np.random.choice(N_CELL, replace=False, size=len(self.agents) + 2 + self.labyrint)\n        self.geese = [[n[0]], [n[1]], [n[2]], [n[3]]]\n        if labyrint:\n            self.foods = n[4:-self.labyrint]\n            laby_cells = n[-self.labyrint:]\n        else:\n            self.foods = n[4:]\n            laby_cells = []\n        for goose in self.geese:\n            if len(goose) <= 0:\n                continue\n            for i, cell in enumerate(goose):\n                row, col = self.row_col(cell)\n                board[row][col] = b_state['b']\n        # food\n        for food in self.foods:\n            food_row, food_col = self.row_col(food)\n            if board[food_row][food_col] != b_state['blank']:\n                raise Exception('Can\\'t initialize board: place for food is not empty')\n            board[food_row][food_col] = b_state['f']\n        # make labyrint\n        for cell in laby_cells:\n            laby_row, laby_col = self.row_col(cell)\n            if board[laby_row][laby_col] != b_state['blank']:\n                raise Exception('Can\\'t initialize board: place for labyrint is not empty')\n            board[laby_row][laby_col] = b_state['b']\n        self.board = board\n        self.prev_opp_actions_opposite = [-1, -1, -1, -1]\n        if verbose:\n            print('INIT BOARD')\n            self.print_board()\n    \n    def set_agents(self, agents):\n        if len(agents) != 4:\n            raise Exception('Num agents must be equal 4')\n        self.agents = agents\n    \n    def game_start(self, verbose=False):\n        self.prev_opp_actions_opposite = [-1, -1, -1, -1]\n        if self.is_game_end:\n            self.init_game(self.b_state, self.labyrint)\n        if verbose:\n            self.print_board()\n        while not self.is_game_end:\n            self.next_move(verbose=verbose)\n        if verbose:\n            print('Game end.')\n    \n    def next_move(self, verbose=False):\n        self.rewards = [0, 0, 0, 0]\n        if self.is_game_end:\n            return self.is_game_end\n        # Predict move\n        moves = [-1, -1, -1, -1]\n        for i, a in enumerate(self.agents):\n            if self.alive[i]:\n                state = self.get_state_num_player(i)\n                # print('STATE: \\n', state)\n                moves[i] = a.get_action(state)\n        if self.num_move >= 199:\n            self.is_game_end=True\n        # Here is move\n        deads, heads = self.move(moves, verbose=verbose)\n        # Check is game end\n        num_alive = 0\n        for a in self.alive:\n            if a:\n                num_alive += 1\n        if num_alive < 2:\n            self.is_game_end = True\n        # Rewards\n        for i, a in enumerate(self.agents):\n            if deads[i]:\n                r = self.rewards[i]\n                a.train(r, self.get_state_num_player(i, heads), is_game_end = True)\n                self.alive[i] = False\n            if self.alive[i]:\n                r = self.rewards[i]\n                a.train(r, self.get_state_num_player(i, heads), is_game_end = self.is_game_end)\n        # Move done\n        self.num_move += 1\n        if verbose:\n            print(self.rewards)\n            self.print_board()\n        return self.is_game_end\n    \n    def move(self, moves, verbose=False):\n        if verbose:\n            print('Moves: {}'.format(' '.join(map(str, moves))))\n        len_geese = len(self.geese)\n        b_state = self.b_state\n        deads = [False, False, False, False]\n        prev_pos = []\n        # \u0421\u0442\u0430\u0440\u044b\u0435 \u043f\u043e\u0437\u0438\u0446\u0438\u0438 \u0433\u043e\u043b\u043e\u0432\n        old_heads = [goose[0] if len(goose) > 0 else -1 for goose in self.geese]\n        if verbose:\n            print('Old heads positions: {}'.format(' '.join(map(str, old_heads))))\n        # \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c, \u043d\u0435 \u043f\u043e\u0445\u043e\u0434\u0438\u043b \u043b\u0438 \u0438\u0433\u0440\u043e\u043a \u043d\u0430\u0437\u0430\u0434\n        for i in range(len_geese):\n            if self.alive[i] and (moves[i] == self.prev_opp_actions_opposite[i]):\n                deads[i] = True\n                if verbose:\n                    print(\"Player {} move back: {}, last moves: {}\".format(i, moves, self.prev_opp_actions_opposite))\n                self.rewards[i] -= 0\n        # \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u043c \u043d\u043e\u0432\u044b\u0435 \u043f\u043e\u0437\u0438\u0446\u0438\u0438 - \u0443\u0434\u0430\u043b\u044f\u0435\u043c \u0445\u0432\u043e\u0441\u0442 \u0438\u0433\u0440\u043e\u043a\u0430\n        for i in range(len_geese):\n            if self.alive[i]:\n                cell = self.geese[i][-1]\n                row, col = self.row_col(cell)\n                self.board[row][col] = b_state['blank']\n                prev_pos.append(cell)\n                del self.geese[i][-1]\n            else:\n                prev_pos.append(0)\n        # \u041d\u043e\u0432\u044b\u0435 \u043f\u043e\u0437\u0438\u0446\u0438\u0438 \u0433\u043e\u043b\u043e\u0432\u044b, \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0447\u0442\u043e \u043e\u043d\u0438 \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u043d\u043e\u043c \u043c\u0435\u0441\u0442\u0435\n        new_moves = [-1, -1, -1, -1]\n        for i in range(len_geese):\n            if self.alive[i]:\n                if moves[i] == -1:\n                    deads[i] = True\n                    continue\n                new_cell = self.get_head_cell(moves[i], old_heads[i])\n                if new_cell == -1:\n                    deads[i] = True\n                    continue\n                new_moves[i] = new_cell\n        if verbose:\n            print('New heads positions: {}'.format(' '.join(map(str, new_moves))))\n        # \u0415\u0441\u043b\u0438 \u0434\u0432\u0435 \u0433\u043e\u043b\u043e\u0432\u044b \u043f\u043e\u0445\u043e\u0434\u0438\u043b\u0438 \u043d\u0430 \u043e\u0434\u043d\u0443 \u0438 \u0442\u0443 \u0436\u0435 \u043a\u043b\u0435\u0442\u043a\u0443\n        for i in range(len_geese):\n            if self.alive[i]:\n                for j in range(i + 1, len_geese):\n                    if self.alive[j] and new_moves[i] == new_moves[j]:\n                        deads[i] = True\n                        deads[j] = True\n                        break\n        \n        # \u0415\u0434\u0430\n        eaten = [False, False]\n        for i in range(len_geese):\n            if self.alive[i]:\n                row, col = self.row_col(new_moves[i])\n                if self.board[row][col] == b_state['f']:\n                    # \u0413\u0443\u0441\u044c \u0441\u044a\u0435\u043b \u0435\u0434\u0443\n                    self.rewards[i] += 1\n                    self.geese[i].append(prev_pos[i])\n                    # \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u043a\u043b\u0435\u0442\u043a\u0443 \u043d\u0430 \u0434\u043e\u0441\u043a\u0443\n                    row_body, col_body = self.row_col(prev_pos[i])\n                    self.board[row_body][col_body] = b_state['b']\n                    # \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u043a\u0430\u043a\u0443\u044e \u0435\u0434\u0443 \u0441\u044a\u0435\u043b\u0438\n                    if new_moves[i] == self.foods[0]:\n                        eaten[0] = True\n                    else:\n                        eaten[1] = True\n            elif self.board[row][col] == b_state['f']:\n                # \u0413\u0443\u0441\u044c \u0441\u044a\u0435\u043b \u0435\u0434\u0443\n                self.rewards[i] += 1\n                        \n        # Every 40 turns decrease len goose by 1\n        if self.num_move % 40 == 39:\n            if verbose:\n                print('Clear 1 cell from each goose')\n            for i in range(4):\n                if self.alive[i]:\n                    if verbose:\n                        print('Goose {} have len {}'.format(i, len(self.geese[i])))\n                    if len(self.geese[i]) > 0:\n                        cell = self.geese[i][-1]\n                        row, col = self.row_col(cell)\n                        self.board[row][col] = b_state['blank']\n                        del self.geese[i][-1]\n                    else:\n                        if verbose:\n                            print('Goose {} is dead'.format(i))\n                        deads[i] = True\n                        \n        # \u0421\u0442\u0430\u0432\u0438\u043c \u0433\u043e\u043b\u043e\u0432\u044b \u0433\u0443\u0441\u0435\u0439\n        for i, coord in enumerate(new_moves):\n            if self.alive[i]:\n                row, col = self.row_col(coord)\n                if self.board[row][col] == b_state['b']:\n                    deads[i] = True\n                    continue\n                self.board[row][col] = b_state['b']\n                self.geese[i].insert(0, coord)\n\n        # \u0423\u0431\u0438\u0440\u0430\u0435\u043c \u0443\u0431\u0438\u0442\u044b\u0445 \u0433\u0443\u0441\u0435\u0439\n        for i in range(len_geese):\n            is_dead = deads[i]\n            deads[i] = False\n            if is_dead and self.alive[i]:\n                self.alive[i] = False\n                deads[i] = True\n                # Punish for death\n                self.rewards[i] -= 10\n                for cell in self.geese[i]:\n                    row, col = self.row_col(cell)\n                    self.board[row][col] = b_state['blank']\n                self.geese[i] = []\n                \n        # \u0413\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u0435\u0434\u0443 \u0435\u0441\u043b\u0438 \u0441\u044a\u0435\u043b\u0438\n        if eaten[0]:\n            idx = np.random.randint(N_CELL)\n            row, col = self.row_col(idx)\n            while(self.board[row][col] != b_state['blank']):\n                idx = np.random.randint(N_CELL)\n                row, col = self.row_col(idx)\n            self.board[row][col] = b_state['f']\n            eaten[0] = False\n            self.foods[0] = idx\n        if eaten[1]:\n            idx = np.random.randint(N_CELL)\n            row, col = self.row_col(idx)\n            while(self.board[row][col] != b_state['blank']):\n                idx = np.random.randint(N_CELL)\n                row, col = self.row_col(idx)\n            self.board[row][col] = b_state['f']\n            eaten[1] = False\n            self.foods[1] = idx\n            \n        # \u0412\u044b\u0434\u0430\u0435\u043c \u043d\u0430\u0433\u0440\u0430\u0434\u0443 \u0437\u0430 \u0432\u044b\u0436\u0438\u0432\u0430\u0435\u043c\u043e\u0441\u0442\u044c\n        self.rewards = [r+1 if not deads[i] else r for i, r in enumerate(self.rewards)]\n        # \u0413\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0435 \u0445\u043e\u0434\u044b\n        self.prev_opp_actions_opposite = [self.get_opposite(move) for move in moves]\n        return deads, new_moves\n    \n    def row_col(self, num, *args, **kwargs):\n        row, col = int(num \/ 11), num % 11\n        return row, col\n    \n    def get_opposite(self, move):\n        if move not in ACTIONS:\n            return -1\n        if move == Action.NORTH.name:\n            return Action.SOUTH.name\n        if move == Action.SOUTH.name:\n            return Action.NORTH.name\n        if move == Action.EAST.name:\n            return Action.WEST.name\n        return Action.EAST.name\n    \n    def get_state(self):\n        return self.board\n    \n    def print_board(self):\n        print('Num move {}. Alive: {}'.format(self.num_move, ' '.join(map(str, self.alive))))\n        for row in self.board:\n            print(' '.join(map(str, row)))\n        print()\n        pass\n    \n    def get_state_num_player(self, num_player=0, heads=[]):\n        if len(heads) == 0:\n            heads = [h[0] if len(h) > 0 else -1 for h in self.geese]\n        state = self.get_state()\n        player_state = copy.deepcopy(state)\n        for i in range(len(self.agents)):\n            head = heads[i]\n            if self.alive[i]:\n                head_row, head_col = self.row_col(head)\n                if num_player == i:\n                    player_head = head\n                    player_state[head_row][head_col] = self.b_state['h']\n                else:\n                    player_state[head_row][head_col] = self.b_state['x']\n            elif num_player == i:\n                player_head = head\n        player_state = self.centroid_agent(player_state, player_head)\n        return player_state\n    \n    def get_head_cell(self, action, cell):\n        row, col = self.row_col(cell)\n        new_row, new_col = row, col\n        if action == Action.NORTH.name:\n            new_row -= 1\n        elif action == Action.SOUTH.name:\n            new_row += 1\n        elif action == Action.EAST.name:\n            new_col += 1\n        elif action == Action.WEST.name:\n            new_col -= 1\n        else:\n            return -1\n        new_row, new_col = new_row % N_ROWS, new_col % N_COLS\n        if self.board[new_row][new_col] == self.b_state['b']:\n            return -1\n        return self.get_num_cell(new_row, new_col)\n    \n    def centroid_agent(self, board, head):\n        head_row, head_col = self.row_col(head)\n        # X is [0, 11), center is 5\n        # Y is [0, 7), center is 3\n        dX = head_col - 5\n        dY = 3 - head_row\n        # \u0421\u0434\u0432\u0438\u0433 \u0441\u0442\u0440\u043e\u043a - dY\n        if dY != 0:\n            board = np.vstack((board[-dY:], board[:-dY]))\n        # \u0421\u0434\u0432\u0438\u0433 \u043a\u043e\u043b\u043e\u043d\u043e\u043a - dX\n        if dX != 0:\n            board = np.hstack((board[:, dX:], board[:, :dX]))\n        return board\n        \n    def get_num_cell(self, row, col):\n        return 11 * row + col","485332ff":"class DuelingDQN(nn.Module):\n    def __init__(self, ACTIONS, input_shape=(4, 4, 5)):\n        super(DuelingDQN, self).__init__()\n        self.ACTIONS = ACTIONS\n\n        n_neurons = np.prod(input_shape)\n        # Advantages A(s, a)\n        self.fc_a = nn.Sequential(\n        nn.Flatten(),\n        nn.Linear(n_neurons, int(n_neurons \/ 2)),\n        nn.ReLU(),\n        nn.Linear(int(n_neurons \/ 2), len(ACTIONS))\n        )\n        # Values of state V(s)\n        self.fc_v = nn.Sequential(\n        nn.Flatten(),\n        nn.Linear(n_neurons, int(n_neurons \/ 2)),\n        nn.ReLU(),\n        nn.Linear(int(n_neurons \/ 2), 1)\n        )\n\n    def forward(self, state):\n        val = self.fc_v(state)\n        adv = self.fc_a(state)\n        # Q(s, a) = V(s) + A(s, a)\n        # Subtract the mean for stability\n        return val + adv - adv.mean()","7f4e8681":"class TrainingAgent():\n    def __init__(self, training_model, target_model, loss=nn.MSELoss(), device=torch.device(\"cpu\"),\n                 batch_size=3, sync_time=250, buffer_size=100000,\n                 eps=1, eps_min=1e-3, eps_decay_time=500,\n                 gamma=0.5, learning_rate=3.1415e-4,\n                 reward_lookup_window=10, reward_threshold=500,\n                 ACTIONS=ACTIONS, prev_action=0, name='TrainingAgent',\n                is_train = True):\n        \n        self.is_train = is_train\n        self.prev_action = prev_action\n        self.training_model = training_model\n        self.training_model.device = device\n        self.target_model = target_model\n        self.target_model.device = device\n        # self.optimizer = torch.optim.Adam(training_model.parameters(), lr=learning_rate)\n        self.optimizer = torch.optim.Adam(training_model.parameters(), lr=learning_rate)\n        self.loss = loss\n        self.device = device\n        \n        self.state_buffer = deque(maxlen=buffer_size)\n        self.action_buffer = deque(maxlen=buffer_size)\n        self.next_state_buffer = deque(maxlen=buffer_size)\n        self.reward_buffer = deque(maxlen=buffer_size)\n        self.done_buffer = deque(maxlen=buffer_size) #is_game_end\n        \n        self.total_rewards = []\n        self.total_reward = 0\n        \n        self.batch_size = batch_size\n        self.sync_time = sync_time\n        self.eps = eps\n        self.eps_min = eps_min\n        self.eps_decay_time = eps_decay_time\n        self.gamma = gamma\n        self.reward_lookup_window = reward_lookup_window\n        self.reward_threshold = reward_threshold\n        self.ACTIONS = ACTIONS\n        self.name = name\n        self.frames = 0\n        self.best_mean_reward = 0\n        self.finished_training = False\n        self.input_size = None\n                \n    def get_action(self, state):\n        \"\"\"\n            Receive the state (np.ndarray), transform it to the tensor format,\n            choose an action and fill the buffers.\n        \"\"\"\n        state = self.edit_last_player_move(state, self.prev_action)\n        state_t = torch.FloatTensor(state).view(1, 1, *state.shape).to(self.device)\n        if self.input_size is None:\n            self.input_size = state_t.size()\n        if self.is_train and np.random.rand() < self.eps:\n            action = np.random.choice(range(len(self.ACTIONS)))\n            while action == self.get_opposite_idx_move(self.prev_action):\n                # Epsilon-greedy exploration\n                action = np.random.choice(range(len(self.ACTIONS)))\n        else:\n            # Choose the action with the best reward\n            ACTIONS_cur = self.training_model(state_t)\n            action = ACTIONS_cur.max(1)[1]\n        self.prev_action = action\n        # Fill the buffers and pass the action to the environment\n        self.state_buffer.append(state_t)\n        self.action_buffer.append(action)\n        return self.ACTIONS[action]\n    \n    def cut_buffers(self, window, shift=None):\n        if shift is None:\n            shift = int(window**0.5)\n        self.state_buffer = self.state_buffer[::shift][-window:]\n        self.action_buffer = self.action_buffer[::shift][-window:]\n        self.reward_buffer = self.reward_buffer[::shift][-window:]\n        self.next_state_buffer = self.next_state_buffer[::shift][-window:]\n        self.done_buffer = self.done_buffer[::shift][-window:]\n        \n    def train(self, reward, state, is_game_end=False):\n        \"\"\"\n            A training step. After passing the action to the environment, if the agent has survived it\n            receives a reward, next state and the marker indicating the end of the episode (is_game_end).\n            The agent adds this data to the buffers, calculates the Q-values and expected Q-values, \n        \"\"\"\n        state = self.edit_last_player_move(state, self.prev_action)\n        # Receive the response from the environment about action taken \n        self.next_state_buffer.append(torch.FloatTensor(state).view(1, 1, *state.shape))\n        self.reward_buffer.append(reward)\n        self.done_buffer.append(is_game_end)\n        \n        # Update the number of frames and the epsilon value\n        self.eps = max(self.eps_min, self.eps - self.frames * (self.eps-self.eps_min)\/self.eps_decay_time)\n        self.frames += 1\n        \n        # Sync if it's time to\n        if self.frames % self.sync_time:\n            self.target_model.load_state_dict(self.training_model.state_dict())\n        \n        # Add the rewards if the episode ended\n        self.total_reward += reward\n        if is_game_end:\n            self.prev_action = -1\n            self.total_rewards.append(self.total_reward)\n            self.total_reward = 0\n        \n        if self.frames > self.reward_lookup_window:\n            # If the current mean reward is better than the best mean reward, save the model\n            if np.mean(self.total_rewards[-self.reward_lookup_window:]) > self.best_mean_reward:\n                torch.save(self.training_model.state_dict(), self.name + '-best.dat')\n                self.best_mean_reward = np.mean(self.total_rewards[-self.reward_lookup_window:])\n            # If the current mean reward exceeds the threshold, stop training\n            if np.mean(self.total_rewards[-self.reward_lookup_window:]) > self.reward_threshold:\n                self.finished_training = True\n        \n        with torch.enable_grad():\n            # Begin training\n            self.optimizer.zero_grad()\n\n            # Choose a batch of (s, a, r, s', d)\n            # Use .cat() to concatenate the tensors in state buffers alongside the batch dimension\n            idx = np.random.choice(range(len(self.done_buffer)), size=min(len(self.done_buffer), self.batch_size))\n            states_v = torch.cat(list(self.state_buffer))[idx].type(torch.FloatTensor).to(self.device)\n            ACTIONS_v = torch.tensor(self.action_buffer).to(self.device)[idx]\n            next_states_v = torch.cat(list(self.next_state_buffer))[idx].type(torch.FloatTensor).to(self.device)\n            rewards_v = torch.tensor(self.reward_buffer).to(self.device)[idx]\n            done_mask = torch.ByteTensor(self.done_buffer).to(self.device)[idx]\n\n            if len(states_v.size()) == 3:\n                states_v = states_v.view(1, *states_v.size())\n                next_states_v = next_states_v.view(1, *next_states_v.size())\n            \n            # Collect the Q-values located at the position ACTIONS_v[i] (an integer) of vector (output of the model) a(states_v)[i]\n            # and reshape it into a shape of ACTIONS_v\n            # q_values = self.training_model(states_v).gather(1, ACTIONS_v.unsqueeze(-1)).squeeze(-1)\n            q_values = self.training_model(states_v).gather(1, ACTIONS_v.unsqueeze(-1)).squeeze(-1).to(self.device)\n            # Get the maximum Q-values of each next state from the target network, i.e.\n            # max_a' Q'(s', a')\n            # .max(i) returns a tuple of maximum values and their indices along i-th dimension\n            # If the transition is from the last transition of the episode, then the expected Q' value is R, not gamma*Q' + R\n            # To account for this, if the transition is the final transition of the episode, done_mask[i] is True,\n            # so we null the Q' value at that index. Then the expected value would be equal to gamma*0 + R = R.\n            target_q_values = self.target_model(next_states_v).to(self.device).max(1)[0]\n            target_q_values[done_mask] = 0\n            # Detach the target_q_values tensor so the target_model will not train\n            target_q_values = target_q_values.detach()\n\n            # Calculate the loss and make a step backwards through the training network\n            expected_q_values = self.gamma * target_q_values + rewards_v\n            L = self.loss(expected_q_values, q_values)\n            L.backward()\n            self.optimizer.step()\n    \n    def get_opposite_idx_move(self, idx):\n        # ['NORTH', 'WEST', 'SOUTH', 'EAST']\n        return (idx + 2) % 4\n    \n    def edit_last_player_move(self, board, idx_opposite):\n        # ['NORTH', 'WEST', 'SOUTH', 'EAST']\n        # CENTER BOARD [3, 5]\n        if idx_opposite == 0:\n            board[2][5] = -2\n        elif idx_opposite == 1:\n            board[3][4] = -2\n        elif idx_opposite == 2:\n            board[4][5] = -2\n        elif idx_opposite == 3:\n            board[3][6] = -2\n        board_rot = np.zeros((7, 5))\n        for i in range(7):\n            for j in range(5):\n                board_rot[i][j] = board[5 - j][2 + i]\n        small_boards = np.array([\n            board[:4, 3:8],\n            board_rot[:4],\n            board[3:, 3:8][::-1, ::-1],\n            board_rot[3:][::-1, ::-1],\n        ])\n        return small_boards","4b7d02ec":"def get_mean_reward(agent):\n    r = np.mean(agent.total_rewards[-agent.reward_lookup_window:])\n    return r if r is not np.nan else 'N\/A'","193be678":"def get_best_agent(rewards):\n    mean_rewards = {agent: np.mean(reward) for agent, reward in rewards.items()}\n    return sorted(mean_rewards.items(), key=lambda p: -p[1])[0]","6e0fa90b":"LOAD_FROM_CHECKPOINT = False","28ed3447":"if not LOAD_FROM_CHECKPOINT:\n    model = DuelingDQN(ACTIONS=ACTIONS)\n    generational_rewards = []\n    torch.save(model, 'gen_model_dueling_dqn.dat')\nelse:\n    with open('..\/input\/dump--\/dump.txt', 'r') as f:\n        state_dict_dump = f.read()\n        state_dict_dump = bytes(state_dict_dump, 'utf-8')\n    state_dict = pickle.loads(bz2.decompress(base64.b64decode(state_dict_dump)))\n    model = DuelingDQN(ACTIONS=ACTIONS)\n    model.load_state_dict(state_dict)\n    model.eval()\n    torch.save(model, 'gen_model_dueling_dqn.dat')\n    generational_rewards = []","0919a2d2":"class IntermediateAgent:\n    __slots__ = 'action'\n    \n    def __init__(self, action=lambda board: \"NORTH\"):\n        self.action = action\n        pass\n    \n    def get_action(self, state=[[]]):\n        return self.action(state)\n    \n    def train(self, reward=0, state=[[]], is_game_end=False):\n        pass","9bccfa0d":"def run_round_static(agents, static_agents, divide=2, games_per_match=10, labyrint=0):\n    rewards = {}\n    # \u041f\u0440\u043e\u0445\u043e\u0434\u0438\u043c \u0432\u0441\u0435 \u0438\u0433\u0440\u044b\n    for agent in agents:\n        for i in range(games_per_match):\n            env = Env([agent, *static_agents], labyrint=labyrint)\n            env.game_start(verbose=False)\n        rewards[agent] = get_mean_reward(agent)\n    half_items = sorted(rewards.items(), key=lambda p: -p[1])[:len(rewards) \/\/ divide]\n    print('Best reward: {}'.format(half_items[0]))\n    rewards = dict(half_items)\n    return rewards.keys()","ee6af623":"def run_competition_static(num_epoch=3, games_per_match=10, agent_class=TrainingAgent, \n                           static_agents_methods=[lambda state: \"NORTH\"] * 3,\n                           model_path='dueling_dqn_competition.dat', is_difference=False, state_agent=None,\n                           labyrint=0, **kwargs):\n    if state_agent:\n        agents = [state_agent]\n    else:\n        agents = [initialize_agent(model_path, is_difference=is_difference, **kwargs) for _ in range(2**num_epoch)]\n    bots = [\n        IntermediateAgent(method) for method in static_agents_methods\n    ]\n    for i in range(num_epoch):\n        print('EPOCH {}: '.format(i), end='')\n        if state_agent:\n            agents = run_round_static(agents, bots, games_per_match=games_per_match, divide=1, labyrint=labyrint)\n        else:\n            agents = run_round_static(agents, bots, games_per_match=games_per_match, labyrint=labyrint)\n    return list(agents)[0]","9b0ddb8d":"dir_ = 0\n\ndef set_dir_agent_simple_random():\n    global dir_\n    dir_ = np.random.randint(4)\n    return dir_\n\ndef agent_simple_random(state=0):\n    global dir_\n    return ACTIONS[dir_]","2e219723":"class SharpyAgent:\n    def __init__(self):\n        self.prev_opp_action = -1\n        self.ACTIONS = ACTIONS\n        pass\n        \n    def agent(self, state=0):\n        # -1 = \u0433\u043e\u043b\u043e\u0432\u0430 \u043f\u0440\u043e\u0442\u0438\u0432\u043d\u0438\u043a\u0430\n        # -2 = \u0442\u0435\u043b\u043e \u043b\u044e\u0431\u043e\u0433\u043e \u0438\u0433\u0440\u043e\u043a\u0430\n        # [3, 5]\n        # [ a x b\n        #   x 0 x\n        #   c x d]\n        idx = -1\n        # a\n        if state[2, 4] == -1:\n            if state[2, 5] >= 0 and self.prev_opp_action != 0:\n                if state[3, 4] >= 0 and self.prev_opp_action != 1:\n                    idx = np.random.randint(2)\n                else:\n                    idx = 0\n            elif state[3, 4] >= 0 and self.prev_opp_action != 1:\n                idx = 1\n        # b\n        if state[2, 6] == -1:\n            if state[2, 5] >= 0 and self.prev_opp_action != 0:\n                if state[3, 6] >= 0 and self.prev_opp_action != 3:\n                    idx = [0, 3][np.random.randint(2)]\n                else:\n                    idx = 0\n            elif state[3, 6] >= 0 and self.prev_opp_action != 3:\n                idx = 3\n        # c\n        if state[4, 4] == -1:\n            if state[4, 5] >= 0 and self.prev_opp_action != 2:\n                if state[3, 4] >= 0 and self.prev_opp_action != 1:\n                    idx = [2, 1][np.random.randint(2)]\n                else:\n                    idx = 2\n            elif state[3, 4] >= 0 and self.prev_opp_action != 1:\n                idx = 1\n        # d\n        if state[4, 6] == -1:\n            if state[4, 5] >= 0 and self.prev_opp_action != 2:\n                if state[3, 6] >= 0 and self.prev_opp_action != 3:\n                    idx = [2, 3][np.random.randint(2)]\n                else:\n                    idx = 2\n            elif state[3, 6] >= 0 and self.prev_opp_action != 3:\n                idx = 3\n        if idx == -1:\n            idxs = [state[2, 5], state[3, 4], state[4, 5], state[3, 6]]\n            idxs[self.prev_opp_action] = -100\n            idx = np.argmax(idxs)\n        # \u0417\u0430\u043f\u043e\u043c\u0438\u043d\u0430\u0435\u043c \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u0445\u043e\u0434\n        self.prev_opp_action = self.get_opposite_idx_move(idx)\n        return ACTIONS[idx]\n    \n    def get_opposite_idx_move(self, idx):\n        # ['NORTH', 'WEST', 'SOUTH', 'EAST']\n        return (idx + 2) % 4","5fe626ea":"class GreedyAgent:\n    def __init__(self):\n        self.prev_opp_action = -1\n        self.ACTIONS = ACTIONS\n        pass\n        \n    def agent(self, state=0):\n        # 2 = \u0435\u0434\u0430\n        # -1 = \u0433\u043e\u043b\u043e\u0432\u0430 \u043f\u0440\u043e\u0442\u0438\u0432\u043d\u0438\u043a\u0430\n        # -2 = \u0442\u0435\u043b\u043e \u043b\u044e\u0431\u043e\u0433\u043e \u0438\u0433\u0440\u043e\u043a\u0430\n        # [3, 5]\n        # [        1,5\n        #      2,4 2,5 2,6\n        #  3,3 3,4 3,5 3,6 3,7\n        #      4,4 4,5 4,6\n        #          5,5]\n        idxs = [state[2, 5], state[3, 4], state[4, 5], state[3, 6]]\n        # a\n        if state[2, 4] > 0:\n            if state[2, 5] >= 0 and state[1, 5] != -1 and self.prev_opp_action != 0:\n                if state[3, 4] >= 0 and state[3, 3] != -1  and self.prev_opp_action != 1:\n                    idxs[0] += 1\n                    idxs[1] += 1\n                else:\n                    idxs[0] += 1\n            elif state[3, 4] >= 0 and state[3, 3] != -1 and self.prev_opp_action != 1:\n                idxs[1] += 1\n        # b\n        if state[2, 6] > 0:\n            if state[2, 5] >= 0 and state[1, 5] != -1  and self.prev_opp_action != 0:\n                if state[3, 6] >= 0 and state[3, 7] != -1  and self.prev_opp_action != 3:\n                    idxs[0] += 1\n                    idxs[3] += 1\n                else:\n                    idxs[0] += 1\n            elif state[3, 6] >= 0 and state[3, 7] != -1  and self.prev_opp_action != 3:\n                idxs[3] += 1\n        # c\n        if state[4, 4] > 0:\n            if state[4, 5] >= 0 and state[5, 5] != -1  and self.prev_opp_action != 2:\n                if state[3, 4] >= 0 and state[3, 3] != -1  and self.prev_opp_action != 1:\n                    idxs[2] += 1\n                    idxs[1] += 1\n                else:\n                    idxs[2] += 1\n            elif state[3, 4] >= 0 and state[3, 3] != -1  and self.prev_opp_action != 1:\n                idxs[1] += 1\n        # d\n        if state[4, 6] > 0:\n            if state[4, 5] >= 0 and state[5, 5] != -1  and self.prev_opp_action != 2:\n                if state[3, 6] >= 0 and state[3, 7] != -1  and self.prev_opp_action != 3:\n                    idxs[2] += 1\n                    idxs[3] += 1\n                else:\n                    idxs[2] += 1\n            elif state[3, 6] >= 0 and state[3, 7] != -1  and self.prev_opp_action != 3:\n                idxs[3] += 1\n        if state[2, 5] < 0:\n            idxs[0] -= 20\n        if state[3, 4] < 0:\n            idxs[1] -= 20\n        if state[4, 5] < 0:\n            idxs[2] -= 20\n        if state[3, 6] < 0:\n            idxs[3] -= 20\n        if state[1, 5] == -1:\n            idxs[0] -= 10\n        if state[3, 3] == -1:\n            idxs[1] -= 10\n        if state[5, 5] == -1:\n            idxs[2] -= 10\n        if state[3, 7] == -1:\n            idxs[3] -= 10\n        idxs[self.prev_opp_action] = -100\n        idx = np.argmax(idxs)\n        # \u0417\u0430\u043f\u043e\u043c\u0438\u043d\u0430\u0435\u043c \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u0445\u043e\u0434\n        self.prev_opp_action = self.get_opposite_idx_move(idx)\n        return ACTIONS[idx]\n    \n    def get_opposite_idx_move(self, idx):\n        # ['NORTH', 'WEST', 'SOUTH', 'EAST']\n        return (idx + 2) % 4","d4ab5059":"def initialize_agent(model_path, agent_class=TrainingAgent, is_difference=False, **kwargs):\n    if is_difference:\n        training_model = DuelingDQN(ACTIONS=ACTIONS)\n        target_model = DuelingDQN(ACTIONS=ACTIONS)\n    else:\n        if isinstance(model_path, (list, np.ndarray)):\n            path = model_path[np.random.randint(len(model_path))]\n        else:\n            path = model_path\n        training_model = torch.load(path)\n        target_model = torch.load(path)\n    return agent_class(training_model=training_model, target_model=target_model, **kwargs)","031bac40":"def run_match(agents, num_games, labyrint=0):\n    \"\"\"\n        Takes a list of agents (# of agents is 4), plays num_games games.\n        Returns the best agent, other agents get disqualified.\n    \"\"\"\n    rewards = {agent: [] for agent in agents}\n    for game in range(num_games):\n        env = Env(agents, labyrint=labyrint)\n        env.game_start(verbose=False)\n        for agent in agents:\n            rewards[agent].append(get_mean_reward(agent))\n    return get_best_agent(rewards)\n\ndef run_round(agents, games_per_match, labyrint=0):\n    \"\"\"\n        Runs 1 round, taking 4 random agents each match. Returns the list of winners.\n    \"\"\"\n    winners = []\n    rewards = []\n    np.random.shuffle(agents)\n    i = 0\n    while len(agents) > 0:\n        \n        # Get competitors\n        competitors = agents[:4]\n        # Deallocate memory\n        del agents[:4]\n        # Run match\n        winner, reward = run_match(competitors, games_per_match, labyrint=labyrint)\n        # print('MATCH {} | WINNER: {} | REWARD {}'.format(i, winner, reward))\n        # Add to the winners\n        winners.append(winner)\n        rewards.append(reward)\n        i += 1\n    return winners, rewards\n\ndef run_competition(num_rounds=3, games_per_match=10, agent_class=TrainingAgent, model_path='dueling_dqn_competition.dat', is_difference=False,\n                    labyrint=0, **kwargs):\n    agents = [initialize_agent(model_path, is_difference=is_difference, **kwargs) for _ in range(4**num_rounds)]\n    for i in range(num_rounds):\n        agents, rewards = run_round(agents, games_per_match, labyrint=labyrint)\n        print('ROUND {} | # WINNERS: {} | MAX REWARD: {}'.format(i, len(agents), max(rewards)))\n    return agents[0]","18336be4":"device=torch.device(\"cpu\")","c1b2546c":"# \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u0438\u0445 \u043d\u0435 \u0445\u043e\u0434\u0438\u0442\u044c \u043d\u0430\u0437\u0430\u0434\n# \u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u0438\u0437 \u043d\u0438\u0445 \u0442\u043e\u0433\u043e, \u043a\u0442\u043e \u043d\u0435 \u0445\u043e\u0434\u0438\u0442\nmodel_path = 'gen_model_dueling_dqn.dat'\nfor i in range(1):\n    winner = run_competition(device=device, sync_time=1000, gamma=1, games_per_match = 10, \n                             model_path=model_path, num_rounds=4, is_difference=True)\n    torch.save(winner.training_model, model_path)\n    model = torch.load(model_path)","90d9baee":"model_load = ['agent_avoid_wall_v1.txt', 'agent_tango_v1.txt']\nbots_paths = []\nfor i, path_load in enumerate(model_load):\n    with open('..\/input\/dump--\/' + path_load, 'r') as f:\n        state_dict_dump = f.read()\n        state_dict_dump = bytes(state_dict_dump, 'utf-8')\n    state_dict = pickle.loads(bz2.decompress(base64.b64decode(state_dict_dump)))\n    model = DuelingDQN(ACTIONS=ACTIONS)\n    model.load_state_dict(state_dict)\n    model.eval()\n    new_path = str(i) + '.dat'\n    torch.save(model, new_path)\n    model = torch.load(new_path)\n    bots_paths.append(new_path)\nbots_paths","356d75dd":"agents_sharpy = [GreedyAgent(), GreedyAgent(), GreedyAgent()]\nmethods_agents_sharpy = [a.agent for a in agents_sharpy]\nfor i in range(10):\n    winner = run_competition_static(num_epoch=1, games_per_match=20, agent_class=TrainingAgent, batch_size=50,\n                           static_agents_methods=methods_agents_sharpy, model_path=model_path, state_agent=winner)\n    torch.save(winner.training_model, model_path)\n    model = torch.load(model_path)","f4300c86":"# \u041d\u0430 \u043b\u0443\u0447\u0448\u0435\u0439 \u0438\u0437 \u0441\u0442\u0430\u0440\u0442\u043e\u0432\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0443\u043b\u0443\u0447\u0448\u0430\u0435\u043c \u0435\u0435 (\u043e\u043d\u0430 \u043f\u0440\u043e\u0448\u043b\u0430 \u0438\u0441\u043f\u044b\u0442\u0430\u043d\u0438\u0435 \u0431\u043e\u0442\u0430\u043c\u0438)\nfor i in range(1):\n    winner = run_competition(device=device, batch_size=500, sync_time=1000, gamma=1, games_per_match = 100, model_path=model_path, num_rounds=1,\n                            labyrint=20)\n    torch.save(winner.training_model, model_path)\n    model = torch.load(model_path)","105643ec":"agents_sharpy = [SharpyAgent(), SharpyAgent(), GreedyAgent()]\nmethods_agents_sharpy = [a.agent for a in agents_sharpy]\nfor i in range(10):\n    winner = run_competition_static(num_epoch=1, games_per_match=20, agent_class=TrainingAgent, batch_size=250,\n                           static_agents_methods=methods_agents_sharpy, model_path=model_path, state_agent=winner)\n    torch.save(winner.training_model, model_path)\n    model = torch.load(model_path)","79b9ab88":"# \u041d\u0430 \u043b\u0443\u0447\u0448\u0435\u0439 \u0438\u0437 \u0441\u0442\u0430\u0440\u0442\u043e\u0432\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0443\u043b\u0443\u0447\u0448\u0430\u0435\u043c \u0435\u0435 (\u043e\u043d\u0430 \u043f\u0440\u043e\u0448\u043b\u0430 \u0438\u0441\u043f\u044b\u0442\u0430\u043d\u0438\u0435 \u0431\u043e\u0442\u0430\u043c\u0438)\nfor i in range(5):\n    winner = run_competition(device=device, batch_size=500, sync_time=1000, gamma=1, games_per_match = 20, model_path=model_path, num_rounds=1, labyrint=0)\n    torch.save(winner.training_model, model_path)\n    model = torch.load(model_path)","547c4455":"state_dump = base64.b64encode(bz2.compress(pickle.dumps(winner.training_model.state_dict())))\nwith open('WINNER_state_dict.txt', 'w') as fin:\n    fin.write(str(state_dump))","6a0835e6":"model = torch.load(model_path)\n\ntraining_model1 = torch.load(model_path)\ntarget_model1 = torch.load(model_path)\ntraining_model2 = torch.load(model_path)\ntarget_model2 = torch.load(model_path)\ntraining_model3 = torch.load(model_path)\ntarget_model3 = torch.load(model_path)\ntraining_model4 = torch.load(model_path)\ntarget_model4 = torch.load(model_path)\nagents = [\n    TrainingAgent(training_model=training_model1, target_model=target_model1, is_train = False),\n    TrainingAgent(training_model=training_model2, target_model=target_model2, is_train = False),\n    TrainingAgent(training_model=training_model3, target_model=target_model3, is_train = False),\n    TrainingAgent(training_model=training_model4, target_model=target_model4, is_train = False),\n    ]\n\nrewards = {agent: [] for agent in agents}\nenv = Env(agents, labyrint=10)\nprint(\"GAME START\")\nenv.game_start(verbose=True)\nprint(\"GAME END\")\nfor agent in agents:\n    rewards[agent].append(get_mean_reward(agent))","c4cbb1ce":"%%writefile submission.py\n\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport base64\nimport bz2\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport copy\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\n\nN_ROWS = 7\nN_COLS = 11\nN_CELL = N_ROWS * N_COLS\nmax_moves = 200\n\nACTIONS = [\n    Action.NORTH.name,\n    Action.WEST.name,\n    Action.SOUTH.name,\n    Action.EAST.name,\n]\n# \u0412\u0435\u0440\u0445 \u043b\u0435\u0432\u043e \u043d\u0438\u0437 \u043f\u0440\u0430\u0432\u043e\n\nclass DuelingDQN(nn.Module):\n    def __init__(self, actions, input_shape=(4, 4, 5)):\n        super(DuelingDQN, self).__init__()\n        self.actions = actions\n\n        n_neurons = np.prod(input_shape)\n        # Advantages A(s, a)\n        self.fc_a = nn.Sequential(\n        nn.Flatten(),\n        nn.Linear(n_neurons, int(n_neurons \/ 2)),\n        nn.ReLU(),\n        nn.Linear(int(n_neurons \/ 2), len(actions))\n        )\n        # Values of state V(s)\n        self.fc_v = nn.Sequential(\n        nn.Flatten(),\n        nn.Linear(n_neurons, int(n_neurons \/ 2)),\n        nn.ReLU(),\n        nn.Linear(int(n_neurons \/ 2), 1)\n        )\n\n    def forward(self, state):\n        val = self.fc_v(state)\n        adv = self.fc_a(state)\n        # Q(s, a) = V(s) + A(s, a)\n        # Subtract the mean for stability\n        return val + adv - adv.mean()\n\n\nclass Agent:\n    def __init__(self, model, actions):\n        self.model = model\n        self.actions = actions\n        self.prev_opp_action = None\n    \n    def get_action(self, obs_dict, config_dict):\n        \"\"\"\n            Receive the state (np.ndarray), transform it to the tensor format,\n            choose an action and fill the buffers.\n        \"\"\"\n        state = self.edit_last_player_move(self.process_state(obs_dict, config_dict), self.prev_opp_action)\n        state_t = torch.FloatTensor(state).view(1, 1, 4, 4, 5)\n        # Choose the action with the best reward\n        q_values = self.model(state_t)[0].detach().numpy()\n        if self.prev_opp_action:\n            q_values[self.prev_opp_action] = -np.inf\n        action_id = np.argmax(q_values)\n        while action_id == self.prev_opp_action:\n            action_id = (action_id + 1) % 4\n        self.prev_opp_action = self.get_opposite_idx_move(action_id)\n        return self.actions[action_id]\n    \n    def process_state(self, obs_dict, config_dict):\n        observation = Observation(obs_dict)\n        configuration = Configuration(config_dict)\n        player_index = observation.index\n        b_state={\n            'blank': 0, # Empty cell\n            'x': -1, # Head of opponent player\n            'h': 1, # Head of current player\n            'b': -2, # Body cell\n            'f': 2, # Food\n        }\n        # \u0441\u0442\u0440\u043e\u0438\u043c \u0434\u043e\u0441\u043a\u0443\n        board = b_state['blank'] * np.zeros(N_CELL).astype(int).reshape((N_ROWS, N_COLS))\n        # \u0433\u0443\u0441\u0438\n        geese = observation.geese\n        for goose in geese:\n            for i, cell in enumerate(goose):\n                row, col = row_col(cell, configuration.columns)\n                board[row][col] = b_state['b']\n        for i in range(len(geese)):\n            if(len(geese[i]) <= 0):\n                continue\n            head_row, head_col = row_col(geese[i][0], configuration.columns)\n            if player_index == i:\n                board[head_row][head_col] = b_state['h']\n            else:\n                board[head_row][head_col] = b_state['x']\n        # food\n        foods = observation.food\n        for food in foods:\n            food_row, food_col = row_col(food, configuration.columns)\n            board[food_row][food_col] = b_state['f']\n        board = self.centroid_agent(board, geese[player_index][0], configuration)\n        return board\n    \n    def get_opposite_idx_move(self, idx):\n        # ['NORTH', 'WEST', 'SOUTH', 'EAST']\n        return (idx + 2) % 4\n    \n    def edit_last_player_move(self, board, idx_opposite):\n        # ['NORTH', 'WEST', 'SOUTH', 'EAST']\n        # CENTER BOARD [3, 5]\n        # [ 2,4 2,5 2,6\n        #   3,4 3,5 3,6\n        #   4,4 4,5 4,6]\n        if idx_opposite == 0:\n            board[2][5] = -2\n        elif idx_opposite == 1:\n            board[3][4] = -2\n        elif idx_opposite == 2:\n            board[4][5] = -2\n        elif idx_opposite == 3:\n            board[3][6] = -2\n        board_rot = np.zeros((7, 5))\n        for i in range(7):\n            for j in range(5):\n                board_rot[i][j] = board[5 - j][2 + i]\n        small_boards = np.array([\n            board[:4, 3:8],\n            board_rot[:4],\n            board[3:, 3:8][::-1, ::-1],\n            board_rot[3:][::-1, ::-1],\n        ])\n        return small_boards\n    \n    def centroid_agent(self, board, head, configuration):\n        head_row, head_col = row_col(head, configuration.columns)\n        # X is [0, 11), center is 5\n        # Y is [0, 7), center is 3\n        dX = head_col - 5\n        dY = 3 - head_row\n        # \u0421\u0434\u0432\u0438\u0433 \u0441\u0442\u0440\u043e\u043a - dY\n        if dY != 0:\n            board = np.vstack((board[-dY:], board[:-dY]))\n        # \u0421\u0434\u0432\u0438\u0433 \u043a\u043e\u043b\u043e\u043d\u043e\u043a - dX\n        if dX != 0:\n            board = np.hstack((board[:, dX:], board[:, :dX]))\n        return board\n    \n    def agent(self, obs_dict, config_dict):\n        return self.get_action(obs_dict, config_dict)\n    \n    def __call__(self, obs_dict, config_dict, *args, **kwargs):\n        return self.get_action(obs_dict, config_dict)\n\nwith open('WINNER_state_dict.txt', 'r') as f:\n    state_dict_dump = f.read()[2:-1]\n    state_dict_dump = bytes(state_dict_dump, 'utf-8')\nstate_dict = pickle.loads(bz2.decompress(base64.b64decode(state_dict_dump)))\nmodel = DuelingDQN(actions=ACTIONS)\nmodel.load_state_dict(state_dict)\nmodel.eval()\nmyAgent = Agent(model=model, actions=ACTIONS)\n\ndef agent(obs_dict, config_dict):\n    return myAgent(obs_dict, config_dict)","eda4214d":"%run submission.py","f5dd3809":"%%writefile greedy_agent.py\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport base64\nimport bz2\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport copy\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\n\nN_ROWS = 7\nN_COLS = 11\nN_CELL = N_ROWS * N_COLS\nmax_moves = 200\n\n\n# \u0412\u0435\u0440\u0445 \u043b\u0435\u0432\u043e \u043d\u0438\u0437 \u043f\u0440\u0430\u0432\u043e\n\nclass GreedyAgent:\n    def __init__(self):\n        self.prev_opp_action = -1\n        self.ACTIONS = (\n            Action.NORTH.name,\n            Action.WEST.name,\n            Action.SOUTH.name,\n            Action.EAST.name,\n        )\n        pass\n        \n    def get_action(self, obs_dict, config_dict):\n        state = self.process_state(obs_dict, config_dict)\n        # 2 = \u0435\u0434\u0430\n        # -1 = \u0433\u043e\u043b\u043e\u0432\u0430 \u043f\u0440\u043e\u0442\u0438\u0432\u043d\u0438\u043a\u0430\n        # -2 = \u0442\u0435\u043b\u043e \u043b\u044e\u0431\u043e\u0433\u043e \u0438\u0433\u0440\u043e\u043a\u0430\n        # [3, 5]\n        # [        1,5\n        #      2,4 2,5 2,6\n        #  3,3 3,4 3,5 3,6 3,7\n        #      4,4 4,5 4,6\n        #          5,5]\n        idxs = [state[2, 5], state[3, 4], state[4, 5], state[3, 6]]\n        # a\n        if state[2, 4] > 0:\n            if state[2, 5] >= 0 and state[1, 5] != -1 and self.prev_opp_action != 0:\n                if state[3, 4] >= 0 and state[3, 3] != -1  and self.prev_opp_action != 1:\n                    idxs[0] += 1\n                    idxs[1] += 1\n                else:\n                    idxs[0] += 1\n            elif state[3, 4] >= 0 and state[3, 3] != -1 and self.prev_opp_action != 1:\n                idxs[1] += 1\n        # b\n        if state[2, 6] > 0:\n            if state[2, 5] >= 0 and state[1, 5] != -1  and self.prev_opp_action != 0:\n                if state[3, 6] >= 0 and state[3, 7] != -1  and self.prev_opp_action != 3:\n                    idxs[0] += 1\n                    idxs[3] += 1\n                else:\n                    idxs[0] += 1\n            elif state[3, 6] >= 0 and state[3, 7] != -1  and self.prev_opp_action != 3:\n                idxs[3] += 1\n        # c\n        if state[4, 4] > 0:\n            if state[4, 5] >= 0 and state[5, 5] != -1  and self.prev_opp_action != 2:\n                if state[3, 4] >= 0 and state[3, 3] != -1  and self.prev_opp_action != 1:\n                    idxs[2] += 1\n                    idxs[1] += 1\n                else:\n                    idxs[2] += 1\n            elif state[3, 4] >= 0 and state[3, 3] != -1  and self.prev_opp_action != 1:\n                idxs[1] += 1\n        # d\n        if state[4, 6] > 0:\n            if state[4, 5] >= 0 and state[5, 5] != -1  and self.prev_opp_action != 2:\n                if state[3, 6] >= 0 and state[3, 7] != -1  and self.prev_opp_action != 3:\n                    idxs[2] += 1\n                    idxs[3] += 1\n                else:\n                    idxs[2] += 1\n            elif state[3, 6] >= 0 and state[3, 7] != -1  and self.prev_opp_action != 3:\n                idxs[3] += 1\n        if state[2, 5] < 0:\n            idxs[0] -= 20\n        if state[3, 4] < 0:\n            idxs[1] -= 20\n        if state[4, 5] < 0:\n            idxs[2] -= 20\n        if state[3, 6] < 0:\n            idxs[3] -= 20\n        if state[1, 5] == -1:\n            idxs[0] -= 10\n        if state[3, 3] == -1:\n            idxs[1] -= 10\n        if state[5, 5] == -1:\n            idxs[2] -= 10\n        if state[3, 7] == -1:\n            idxs[3] -= 10\n        idxs[self.prev_opp_action] = -100\n        idx = np.argmax(idxs)\n        while idx == self.prev_opp_action:\n            idx = (idx + 1) % 4\n        # \u0417\u0430\u043f\u043e\u043c\u0438\u043d\u0430\u0435\u043c \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u0445\u043e\u0434\n        self.prev_opp_action = self.get_opposite_idx_move(idx)\n        return self.ACTIONS[idx]\n    \n    def get_opposite_idx_move(self, idx):\n        # ['NORTH', 'WEST', 'SOUTH', 'EAST']\n        return (idx + 2) % 4\n    \n    def process_state(self, obs_dict, config_dict):\n        observation = Observation(obs_dict)\n        configuration = Configuration(config_dict)\n        player_index = observation.index\n        b_state={\n            'blank': 0, # Empty cell\n            'x': -1, # Head of opponent player\n            'h': 1, # Head of current player\n            'b': -2, # Body cell\n            'f': 2, # Food\n        }\n        # \u0441\u0442\u0440\u043e\u0438\u043c \u0434\u043e\u0441\u043a\u0443\n        board = b_state['blank'] * np.zeros(N_CELL).astype(int).reshape((N_ROWS, N_COLS))\n        # \u0433\u0443\u0441\u0438\n        geese = observation.geese\n        for goose in geese:\n            for i, cell in enumerate(goose):\n                row, col = row_col(cell, configuration.columns)\n                board[row][col] = b_state['b']\n        for i in range(len(geese)):\n            if(len(geese[i]) <= 0):\n                continue\n            head_row, head_col = row_col(geese[i][0], configuration.columns)\n            if player_index == i:\n                board[head_row][head_col] = b_state['h']\n            else:\n                board[head_row][head_col] = b_state['x']\n        # food\n        foods = observation.food\n        for food in foods:\n            food_row, food_col = row_col(food, configuration.columns)\n            board[food_row][food_col] = b_state['f']\n        board = self.centroid_agent(board, geese[player_index][0], configuration)\n        return board\n    \n    def centroid_agent(self, board, head, configuration):\n        head_row, head_col = row_col(head, configuration.columns)\n        # X is [0, 11), center is 5\n        # Y is [0, 7), center is 3\n        dX = head_col - 5\n        dY = 3 - head_row\n        # \u0421\u0434\u0432\u0438\u0433 \u0441\u0442\u0440\u043e\u043a - dY\n        if dY != 0:\n            board = np.vstack((board[-dY:], board[:-dY]))\n        # \u0421\u0434\u0432\u0438\u0433 \u043a\u043e\u043b\u043e\u043d\u043e\u043a - dX\n        if dX != 0:\n            board = np.hstack((board[:, dX:], board[:, :dX]))\n        return board\n    \n    def __call__(self, obs_dict, config_dict, *args, **kwargs):\n        return self.get_action(obs_dict, config_dict)\n\nmyAgent = GreedyAgent()\n\ndef agent(obs_dict, config_dict):\n    return myAgent(obs_dict, config_dict)","7f1c19d5":"%run greedy_agent.py","bf0b77ff":"from kaggle_environments import make\nenv = make(\"hungry_geese\", debug=True)\n\nenv.reset()\n# Save subm\nenv.run(['submission.py', 'submission.py', 'submission.py', 'submission.py'])\nenv.render(mode=\"ipython\", width=800, height=700)","946c8c4c":"# PRODUCTION","c10fe4f9":"# \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435","9165e45c":"# Creating the agents","fea7da65":"<code>IntermediateAgent<\/code> \u043d\u0443\u0436\u0435\u043d \u0434\u043b\u044f \u043e\u0431\u0435\u0440\u0442\u043a\u0438 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 agent \u0432 \u043a\u043b\u0430\u0441\u0441","59a77b8d":"\u041a\u043b\u0430\u0441\u0441 Env \u043e\u0442\u0432\u0435\u0447\u0430\u0435\u0442 \u0437\u0430 \u043e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u0435.  \n\u0414\u0430\u043d\u043d\u044b\u0439 \u043a\u043b\u0430\u0441\u0441 \u0443\u043c\u0435\u0435\u0442:\n- \u0413\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0431\u0438\u0442\u0432\u0443 \u043c\u0435\u0436\u0434\u0443 4 \u0430\u0433\u0435\u043d\u0442\u0430\u043c\u0438\n- \u0421\u043e\u0437\u0434\u0430\u0432\u0430\u0442\u044c \u043f\u043e\u043b\u0435 \u0438 \u0432\u044b\u0441\u0442\u0430\u0432\u043b\u044f\u0442\u044c \u043d\u0430 \u043d\u0435\u043c \u0435\u0434\u0443 \u0438 \u0430\u0433\u0435\u043d\u0442\u043e\u0432 \u0432 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u043c \u043f\u043e\u0440\u044f\u0434\u043a\u0435\n- \u0412\u044b\u0441\u0442\u0430\u0432\u043b\u044f\u0442\u044c \u0441\u0442\u0435\u043d\u044b \u043b\u0430\u0431\u0438\u0440\u0438\u043d\u0442\u0430. <b>\u041f\u0440\u0438 \u0447\u0438\u0441\u043b\u0435 \u0441\u0442\u0435\u043d \u043b\u0430\u0431\u0438\u0440\u0438\u043d\u0442\u0430 \u0431\u043e\u043b\u044c\u0448\u0435\u043c 3, \u043d\u0435 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u0441\u0432\u044f\u0437\u043d\u043e\u0441\u0442\u044c<\/b>\n- \u0412\u044b\u0434\u0430\u0432\u0430\u0442\u044c \u0430\u0433\u0435\u043d\u0442\u0430\u043c \u043d\u0430\u0433\u0440\u0430\u0434\u0443 \u0432 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0438 \u0441 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u043c \u0441\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u043d\u044b\u043c \u0445\u043e\u0434\u043e\u043c","a4312768":"\u0418\u043c\u043f\u043e\u0440\u0442\u044b \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a","97f35c3b":"\u041a\u043e\u0434 submission.py \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043d\u0430 \u0441\u0435\u0440\u0432\u0435\u0440.\n\n<b>\u0412\u043d\u0438\u043c\u0430\u043d\u0438\u0435.<\/b> \u041f\u0435\u0440\u0435\u0434 \u043e\u0442\u043f\u0440\u0430\u0432\u043a\u043e\u0439 \u0432\u0440\u0443\u0447\u043d\u0443\u044e \u0432\u0441\u0442\u0430\u0432\u044c\u0442\u0435 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u043c\u043e\u0435 \u0444\u0430\u0439\u043b\u0430 <code>WINNER_state_dict.txt<\/code> \u0432 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0443\u044e <code>state_dict_dump<\/code>","62862257":"# Setting up the environment","adfba365":"## \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0441 \u043d\u0443\u043b\u044f (\u043f\u0440\u0438 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438)","f941db9b":"\u0411\u043e\u0442 \u0431\u0443\u0434\u0435\u0442 \u043f\u0440\u043e\u0445\u043e\u0434\u0438\u0442\u044c \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0443 \u043f\u0440\u043e\u0442\u0438\u0432 \u0441\u0442\u0430\u0442\u0438\u0447\u043d\u044b\u0445 \u0431\u043e\u0442\u043e\u0432","44e79104":"# \u041e\u0431\u044a\u044f\u0432\u043b\u0435\u043d\u0438\u0435 \u0431\u043e\u0442\u043e\u0432","f348beb1":"## SimpleRandomAgent","d65b003a":"\n# Training","f1317412":"\u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u0431\u043e\u0442\u0430 \u0434\u043b\u044f \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0439 \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438","931a0176":"The model is a basic DQN with two convolutional layers, a dense hidden layer and an output layer with 4 neurons and a softmax activation. Each neuron represents one of the four possible ACTIONS - `NORTH`, `WEST`, `SOUTH` and `EAST`.  \n\nThe input state must be a torch.LongTensor tensor of size `(batch_size, 1, H, W)`.","56761ced":"# Creating the model ","984d75b0":"## \u041e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0439 GreedyAgent (\u0441\u0442\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0431\u043e\u0442)","76b825af":"TrainingAgent is used to train the passed model.  \nAt each training step, TrainingAgent receives the current state of the environment (np.ndarray), makes a decision (according to the `ACTIONS`), passes the action back to the environment, receives the `reward`, `next_state` and the flag of episode completion, fills the buffers and trains the `training_model` according to the following algorithm:\n\n1. Sample batch_size frames `(s, a, r, s', d)` from the buffers.\n1. Calculate the predicted Q-values using the `training_model`.\n1. Calculate the expected Q-values employing the `target_model` according to Bellman optimality equation\n1. Compare the two Q-values using the loss function (specifically, MSE loss) and pass the gradients back to only the training model.\n1. Sync the training and target model every `sync_time` steps.\n1. Update the `epsilon` value.","b2dbd6d3":"## GreedyAgent","b0d75bd1":"## \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u043d\u043e\u0433\u043e \u0431\u043e\u0442\u0430","71a3ee43":"\u0417\u0430\u043f\u0443\u0441\u043a \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0438\u0433\u0440\u044b","0cf6619a":"## SharpyAgent","b0110cdf":"## \u0421\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u0435 \u0430\u0433\u0435\u043d\u0442\u043e\u0432 \u0434\u0440\u0443\u0433 \u0441 \u0434\u0440\u0443\u0433\u043e\u043c","6684ceb9":"\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u0432\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438","6c8b85a0":"# https:\/\/www.kaggle.com\/vyacheslavponomarev\/goosecompetitionnn","16e0089d":"# \u0421\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u0435 \u0441 \u0431\u043e\u0442\u0430\u043c\u0438"}}