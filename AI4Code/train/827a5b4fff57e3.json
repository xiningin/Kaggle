{"cell_type":{"01a0a0be":"code","533f569c":"code","18ecfb59":"code","74acb6cd":"code","b510aa26":"code","8d41d498":"code","8d798eaa":"code","2e20ad39":"code","ac0d1939":"code","e52c4c71":"code","05acb69e":"code","9e11057a":"code","3d9cce23":"code","e44a6899":"code","ff4e8956":"code","d38c7a13":"code","ffb3a6e5":"code","f2deb229":"code","d5abce07":"code","eaedcaba":"code","89db9529":"code","54e17c30":"code","ea35c990":"code","51966dc4":"code","d7fa921a":"code","0265afba":"code","30311aa0":"markdown","0399d3eb":"markdown","fceb5480":"markdown","99930692":"markdown","4ab38bb7":"markdown","56b2a08f":"markdown","88ef5a6d":"markdown","f31899f9":"markdown","51dd79e4":"markdown","c249f78f":"markdown","85223bf3":"markdown","f0d67802":"markdown","b5b7b47a":"markdown","4eb6b08b":"markdown","1427b8ef":"markdown","e1cc9926":"markdown","e2a109ac":"markdown","bf712baf":"markdown","eb230c1f":"markdown","64747bf2":"markdown","3ea50d5f":"markdown","d30e9bd5":"markdown","b4a7f395":"markdown","24b69eaf":"markdown"},"source":{"01a0a0be":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport copy\nimport itertools\nfrom sklearn.metrics import confusion_matrix\n","533f569c":"def MinMaxScaler(rawData):\n    minValue = min(rawData)\n    maxValue = max(rawData)\n    return (rawData\/(maxValue-minValue) - minValue\/(maxValue-minValue))","18ecfb59":"def kNNImputer(array):\n    output = copy.deepcopy(array)\n    nanIndices = np.where(np.isnan(array))[0]\n    array[np.isnan(array)] = 0\n    for i in range (len(nanIndices)):\n        if nanIndices[i] <= 1:\n            kNNMean = np.sum(array[0:nanIndices[i]+2])\/5\n            output[nanIndices[i]] = kNNMean\n        elif nanIndices[i] >= len(array)-2:\n            kNNMean = np.sum(array[nanIndices[i]-2:len(array)-1])\/5\n            output[nanIndices[i]] = kNNMean\n        else:\n            kNNMean = np.sum(array[nanIndices[i]-2:nanIndices[i]+2])\/5\n            output[nanIndices[i]] = kNNMean\n    return output\n    ","74acb6cd":"def sigmoid(Z):\n    return 1\/(1+np.exp(-Z))","b510aa26":"def gradientDescentLinReg(wInit,x,y,threshold,alpha):\n    isDiverged = False\n    noOfIterations = 1\n    percentageChangeOfCost = -100000\n    w = copy.deepcopy(wInit)\n    cost = []\n    while percentageChangeOfCost < threshold:\n        h = np.dot(x, w)\n        error = h - y\n        currentCost = np.sum(error ** 2) \/ (2 * m)\n        if noOfIterations > 1:\n            percentageChangeOfCost = 100 * (currentCost - cost[noOfIterations-1-1]) \/ currentCost\n        if percentageChangeOfCost > 0:\n            isDiverged = True\n            break\n        gradientVector = np.dot(x.T, error)\n        w -= (alpha \/ m) * gradientVector\n        cost.append(currentCost)\n        noOfIterations += 1\n    return w,noOfIterations,cost,isDiverged","8d41d498":"def gradientDescentLogReg(wInit,x,y,threshold,alpha):\n    isDiverged = False\n    noOfIterations = 1\n    percentageChangeOfCost = -100000\n    w = copy.deepcopy(wInit)\n    cost = []\n    while percentageChangeOfCost < threshold:\n        h = sigmoid(np.dot(x, w))\n        error = h - y\n        currentCost = -(np.dot(y.T,np.log(h))[0][0] + np.dot((1-y.T),np.log(1-h))[0][0]) \/ m\n        if noOfIterations > 1:\n            percentageChangeOfCost = 100 * (currentCost - cost[noOfIterations-1-1]) \/ currentCost\n        if percentageChangeOfCost > 0:\n            isDiverged = True\n            break\n        gradientVector = np.dot(x.T, error)\n        w -= (alpha \/ m) * gradientVector\n        cost.append(currentCost)\n        noOfIterations += 1\n    return w,noOfIterations,cost,isDiverged","8d798eaa":"def plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n    target_names: given classification classes such as [0, 1, 2]\n                  the class names, for example: ['high', 'medium', 'low']\n\n    title:        the text to display at the top of the matrix\n\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                  plt.get_cmap('jet') or plt.cm.Blues\n\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n\n    \"\"\"\n\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","2e20ad39":"trainData = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntestData  = pd.read_csv(\"..\/input\/titanic\/test.csv\")","ac0d1939":"sex = np.where(trainData['Sex'] == 'female',1,0)\npclass = np.array(trainData[\"Pclass\"])\nage = np.array(trainData['Age'])","e52c4c71":"ageImputed = kNNImputer(age)","05acb69e":"sexNormalized = MinMaxScaler(sex)\npclassNormalized = MinMaxScaler(pclass)\nageNormalized = MinMaxScaler(ageImputed)","9e11057a":"survived = np.array(trainData[\"Survived\"])","3d9cce23":"m = survived.shape[0]","e44a6899":"x = np.c_[np.ones((m,1)),sex,pclass,ageImputed]\ny = survived.reshape(-1,1)","ff4e8956":"stop = False\nalpha = 0.0001\nthreshold = -0.0001\nwInit = np.zeros((x.shape[1], 1))\ncostList = []","d38c7a13":"while (not stop):\n    print(alpha)\n    w,noOfIterations,cost,isDiverged = gradientDescentLinReg(wInit,x,y,threshold,alpha)\n    stop = isDiverged\n    if (not isDiverged):\n        currentCostListItem = [alpha, cost]\n        costList.append(currentCostListItem)\n        alpha *= 3\n        wFinalLinReg = w\n        print(noOfIterations,wFinalLinReg)\n","ffb3a6e5":"for index in range (len(costList)):\n    plt.plot(costList[index][1])","f2deb229":"linRegPred = np.dot(x,wFinalLinReg)\nlinRegPred","d5abce07":"stop = False\nalpha = 0.001\nthreshold = -0.0001\nwInit = np.zeros((x.shape[1], 1))\ncostList = []","eaedcaba":"while (not stop):\n    print(alpha)\n    w,noOfIterations,cost,isDiverged = gradientDescentLogReg(wInit,x,y,threshold,alpha)\n    stop = isDiverged\n    if (not isDiverged):\n        currentCostListItem = [alpha, cost]\n        costList.append(currentCostListItem)\n        alpha *= 3\n        wFinalLogReg = w\n        print(noOfIterations,wFinalLogReg)","89db9529":"for index in range (len(costList)):\n    plt.plot(costList[index][1])","54e17c30":"wFinalLogReg","ea35c990":"logRegPred = sigmoid(np.dot(x,wFinalLogReg))\nlogRegPredBinary = np.where(logRegPred >= 0.5, 1, 0)\npd.DataFrame.from_records(logRegPredBinary).to_csv('pred.csv',index=False)\n","51966dc4":"plot_confusion_matrix(cm           = confusion_matrix(y,logRegPredBinary), \n                      normalize    = False,\n                      target_names = ['dead', 'alive'],\n                      title        = \"Confusion Matrix\")","d7fa921a":"xx, yy = np.meshgrid(range(2), range(4))\nz = -(wFinalLogReg[1] * xx + wFinalLogReg[2] * yy + wFinalLogReg[0]) \/ wFinalLogReg[3]\n\nplt3d = plt.figure().gca(projection='3d')\nplt3d.plot_surface(xx, yy, z, alpha=0.2)\nplt3d.scatter(sex, pclass, ageImputed)","0265afba":"import plotly_express as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nlayout = go.Layout(\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0  \n    )\n    \n)\n\n\n\n\ntrace1 = go.Scatter3d(\n    x=sex.flatten(),\n    y=pclass.flatten(),\n    z=ageImputed.flatten(),\n    mode='markers',\n    marker=dict(\n        size=3,\n        color='rgb(255,0,0)',                # set color to an array\/list of desired values      \n    )\n)\n\ntrace2 = go.Surface(z=z, x=xx, y=yy)\n\nfig = go.Figure(data=[trace1,trace2], layout=layout)\n\nfig.show()\n","30311aa0":"### Initializing Variables","0399d3eb":"### Computing input x and y\n","fceb5480":"# Model Fitting","99930692":"### Applying Gradient Descent","4ab38bb7":"## Missing Values","56b2a08f":"# Importing Data","88ef5a6d":"### Confusion Matrix","f31899f9":"## Normalizing Input Data","51dd79e4":"## Applying Gradient Descent","c249f78f":"# Preprocessing Input and Output\n","85223bf3":"## Prediction","f0d67802":"## Cost Vs Varying Alpha","b5b7b47a":"### Cost Vs Varying Alpha","4eb6b08b":"## No of Data Points","1427b8ef":"## Visualizing the Plane of Decision Boundary","e1cc9926":"## Labels","e2a109ac":"## Prediction ","bf712baf":"## Linear Regression","eb230c1f":"## Inputs","64747bf2":"# Functions\n","3ea50d5f":"## Initializing Variables","d30e9bd5":"## Testing the Model","b4a7f395":"# Logistic Regression","24b69eaf":"# Importing Packages"}}