{"cell_type":{"35d857bc":"code","7140c40e":"code","eb8f3185":"code","b3fe7099":"code","09022348":"code","c2c0cd40":"code","681800cf":"code","da6da994":"code","90f681d7":"code","8e5c3524":"code","400ad50c":"code","c3f3c3cd":"code","b0243062":"code","5646a9cc":"code","b053e10e":"code","c8d230e1":"code","ecaf9c6f":"code","3a9cea5a":"code","504f7d48":"code","8bbb8952":"code","e12b3c6e":"code","17e7656c":"code","daf5e26e":"code","e2d38a80":"code","7936057d":"code","2fa5cd2e":"code","aa3dd8d9":"code","5f94a387":"code","9b19a636":"code","262929e7":"code","4461e1a3":"code","cb95e146":"code","14747ebb":"code","d230fee6":"code","4f29679d":"code","e3b3dd2d":"code","8e78b0f5":"code","b05a492d":"code","0d31c0c5":"code","2ed6fe62":"code","00800382":"code","60b202b9":"code","29c0a49e":"code","530eab0a":"code","d8aadb9f":"code","98f7ef14":"code","bcce9666":"code","efd5bcc4":"code","9c926367":"code","a4790b42":"code","225d4c9f":"code","15d8d23c":"code","f3431fc6":"code","d9dae90f":"code","0b1c7310":"code","97ea34a5":"markdown","dcce1a96":"markdown","1f963df9":"markdown","5cb5fae8":"markdown","48ae1a42":"markdown","de7e4caf":"markdown","3b806050":"markdown","77ed2ebb":"markdown","e1a6676b":"markdown","456a64ec":"markdown","1880fcba":"markdown","819b98df":"markdown","cca12263":"markdown","691c55ed":"markdown"},"source":{"35d857bc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imread\nimport cv2\nimport h5py\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.layers as layer\n\nimport os\n\nprint(\"Imports complete\")","7140c40e":"!mkdir \/tmp\/train_data","eb8f3185":"!cp -R \"..\/input\/covid19-radiography-database\/COVID-19_Radiography_Dataset\/COVID\" \"\/tmp\/train_data\"","b3fe7099":"!cp -R \"..\/input\/covid19-radiography-database\/COVID-19_Radiography_Dataset\/Viral Pneumonia\" \"\/tmp\/train_data\"","09022348":"dir = \"\/tmp\/train_data\"\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    dir, labels = \"inferred\", label_mode = \"int\", class_names = ['COVID','Viral Pneumonia'],\n    color_mode = \"rgb\", batch_size = 32, image_size = (224, 224), \n    shuffle = True, seed = 42, validation_split = 0.1, subset = \"training\", interpolation = \"bicubic\")\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    dir, labels = \"inferred\", label_mode = \"int\", class_names = ['COVID','Viral Pneumonia'],\n    color_mode = \"rgb\", batch_size = 32, image_size = (224, 224), \n    shuffle = True, seed = 42, validation_split = 0.1, subset = \"validation\", interpolation = \"bicubic\")","c2c0cd40":"class_names = train_ds.class_names\nprint(class_names)","681800cf":"train_ds = train_ds.map(lambda x, y : (x, tf.one_hot(y, depth = 2)))\nval_ds = val_ds.map(lambda x, y : (x, tf.one_hot(y,depth = 2)))","da6da994":"lr = 0.000001      # The Learning Rate","90f681d7":"def alex_model(input_shape):\n    \n    input_img = tf.keras.Input(shape=input_shape)\n    \n    A1 = layer.Conv2D(filters=96, kernel_size=11, strides =(4,4), activation='ReLU')(input_img)\n    P1 = layer.MaxPool2D(pool_size=(3,3), strides=(2,2), padding='valid')(A1)\n    \n    A2 = layer.Conv2D(filters=256, kernel_size=5, padding='same', activation='ReLU')(P1)\n    P2 = layer.MaxPool2D(pool_size=(3,3), strides=(2,2))(A2)\n    \n    A3 = layer.Conv2D(filters=384, kernel_size=3, padding='same', activation='ReLU')(P2)\n    \n    A4 = layer.Conv2D(filters=384, kernel_size=3, padding='same', activation='ReLU')(A3)\n    \n    A5 = layer.Conv2D(filters=256, kernel_size=3, padding='same', activation='ReLU')(A4)\n    P5 = layer.MaxPool2D(pool_size=(3,3), strides=(2,2))(A5)\n    \n    F = layer.Flatten()(P5)\n    \n    FC6 = layer.Dense(units=1024, activation='ReLU')(F)\n    \n    FC7 = layer.Dense(units=512, activation='ReLU')(FC6)\n    \n    outputs = layer.Dense(units = 2, activation = \"softmax\")(FC7)\n    \n    model = tf.keras.Model(inputs=input_img, outputs=outputs)\n    \n    return model","8e5c3524":"Alex_model = alex_model((224, 224, 3))\nAlex_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                   loss='binary_crossentropy',\n                   metrics=['accuracy', tfa.metrics.F1Score(num_classes=2, threshold=0.5), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\nAlex_model.summary()","400ad50c":"checkpoint_path = 'training\/conv\/Alex_cp.ckpt'\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\ncp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n                                                save_weights_only=False,\n                                                save_freq ='epoch',\n                                                verbose=1)\nestop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n                                         min_delta=0.0001,\n                                         patience=5,\n                                         mode=\"min\",\n                                         restore_best_weights=True)","c3f3c3cd":"historyA = Alex_model.fit(train_ds, \n                          epochs=100, \n                          validation_data=val_ds, \n                          callbacks = [cp_callback, estop])\nAlex_model.save('AlexNet_model.h5')","b0243062":"l = historyA.history.keys()\nmetrics = list(historyA.history.keys())\ndf = pd.DataFrame(historyA.history)\ndf.head()","5646a9cc":"def f1_mod(x):\n    return x[0]\n\ndef per_cent(x):\n    return x*100\n    \ndf['f1_score'] = df['f1_score'].apply(f1_mod)\ndf['val_f1_score'] = df['val_f1_score'].apply(f1_mod)\n\nfor i in df.columns:\n    df[i] = df[i].apply(per_cent)\n\n\ndf.head()","b053e10e":"for i in range(len(l)\/\/2):\n    tr = metrics[i]\n    val = \"val_\" + tr\n    df_pl= df[[tr,val]]\n    df_pl.rename(columns={tr:'Train',val:'Validation'},inplace=True)\n    df_pl.plot(title='Model '+tr,figsize=(12,8)).set(xlabel='Epoch',ylabel=tr)","c8d230e1":"def resnet_model(input_shape):\n    \n    input_img = tf.keras.Input(input_shape)\n    \n    base = tf.keras.applications.resnet50.ResNet50(input_shape = input_shape, weights = 'imagenet',\n                                                   include_top = False, input_tensor = input_img)\n    \n    base.trainable = False\n    \n    A0 = base.output\n    \n    A1 = layer.GlobalAveragePooling2D( )(A0)\n    N1 = layer.BatchNormalization()(A1)\n    N1 = layer.Dropout(0.1)(N1)\n    \n    A2 = layer.Dense(kernel_regularizer=tf.keras.regularizers.L2(0.0002),\n                     units=3072, activation='relu')(N1)\n    N2 = layer.BatchNormalization()(A2)\n    N2 = layer.Dropout(0.2)(N2)\n    \n    A3 = layer.Dense(kernel_regularizer=tf.keras.regularizers.L2(0.0002),\n                     units=512, activation='relu')(N2)\n    N3 = layer.BatchNormalization()(A3)\n    N3 = layer.Dropout(0.4)(N3)\n    \n    outputs = layer.Dense(units = 2, activation = 'softmax')(N3)\n    \n    model = tf.keras.Model(inputs = input_img, outputs = outputs)\n    \n    return model","ecaf9c6f":"checkpoint_path = 'training\/conv\/ResnetImgnet_cp.ckpt'\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\ncp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n                                                save_weights_only=False,\n                                                save_freq ='epoch',\n                                                verbose=1)\n\nestop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n                                         min_delta=0.0001,\n                                         patience=5,\n                                         mode=\"min\",\n                                         restore_best_weights=True)","3a9cea5a":"Resnet_model = resnet_model((224, 224, 3))\nResnet_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                     loss='binary_crossentropy',\n                     metrics=['accuracy', tfa.metrics.F1Score(num_classes=2, threshold=0.5), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\nResnet_model.summary()","504f7d48":"historyRI = Resnet_model.fit(train_ds, epochs=100, validation_data=val_ds, callbacks=[cp_callback, estop])\n\nResnet_model.save('ResNet_model.h5')","8bbb8952":"l = historyRI.history.keys()\nprint(l, len(l), type(l))\nmetrics = list(historyRI.history.keys())\n\ndf = pd.DataFrame(historyRI.history)","e12b3c6e":"def f1_mod(x):\n    return x[0]\n\ndef per_cent(x):\n    return x*100\n    \ndf['f1_score'] = df['f1_score'].apply(f1_mod)\ndf['val_f1_score'] = df['val_f1_score'].apply(f1_mod)\n\nfor i in df.columns:\n    df[i] = df[i].apply(per_cent)\n\ndf.head()","17e7656c":"for i in range(len(l)\/\/2):\n    tr = metrics[i]\n    val = \"val_\" + tr\n    df_pl= df[[tr,val]]\n    df_pl.rename(columns={tr:'train',val:'validation'},inplace=True)\n    df_pl.plot(title='Model '+tr,figsize=(12,8)).set(xlabel='Epoch',ylabel=tr)","daf5e26e":"def vgg_model(input_shape, weights='imagenet',transfer=True):\n    \n    input_img = tf.keras.Input(shape=input_shape)\n    \n    base = tf.keras.applications.VGG16(input_shape = input_shape, weights = weights,\n                                       include_top = False, input_tensor = input_img)\n    base.trainable = not(transfer)\n    \n    A0 = base.output\n    \n    A1 = layer.GlobalAveragePooling2D( )(A0)\n    N1 = layer.BatchNormalization()(A1)\n    N1 = layer.Dropout(0.1)(N1)\n    \n    A2 = layer.Dense(kernel_regularizer=tf.keras.regularizers.L2(0.0002),\n                     units = 256, activation = 'ReLU')(N1)\n    N2 = layer.BatchNormalization()(A2)\n    N2 = layer.Dropout(0.2)(N2)\n    \n    A3 = layer.Dense(kernel_regularizer=tf.keras.regularizers.L2(0.0002),\n                     units = 128, activation = 'ReLU')(A2)\n    N3 = layer.BatchNormalization()(A3)\n    N3 = layer.Dropout(0.4)(N3)\n    \n    outputs = layer.Dense(units = 2, activation = \"softmax\")(N3)\n    \n    model = tf.keras.Model(inputs = input_img, outputs = outputs)\n    \n    return model","e2d38a80":"vggnet_I = vgg_model(input_shape=(224,224,3), weights='imagenet',transfer=True)\n\nvggnet_I.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                 loss='binary_crossentropy',\n                 metrics=['accuracy', tfa.metrics.F1Score(num_classes=2, threshold=0.5), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\nvggnet_I.summary()","7936057d":"checkpoint_path = 'training\/conv\/VGG16Imgnet_cp.ckpt'\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\ncp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n                                                save_weights_only=False,\n                                                save_freq ='epoch',\n                                                verbose=1)\n\nestop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n                                         min_delta=0.0001,\n                                         patience=5,\n                                         mode=\"min\",\n                                         restore_best_weights=True)","2fa5cd2e":"historyVI = vggnet_I.fit(train_ds, \n                          epochs=100, \n                          validation_data=val_ds, \n                          callbacks = [cp_callback, estop])\nvggnet_I.save('VGG16_model.h5')","aa3dd8d9":"l = historyVI.history.keys()\nprint(l, len(l), type(l))\nmetrics = list(historyVI.history.keys())\n\ndf = pd.DataFrame(historyVI.history)","5f94a387":"def f1_mod(x):\n    return x[0]\n\ndef per_cent(x):\n    return x*100\n    \ndf['f1_score'] = df['f1_score'].apply(f1_mod)\ndf['val_f1_score'] = df['val_f1_score'].apply(f1_mod)\n\nfor i in df.columns:\n    df[i] = df[i].apply(per_cent)\n\n\ndf.head()","9b19a636":"for i in range(len(l)\/\/2):\n    tr = metrics[i]\n    val = \"val_\" + tr\n    df_pl= df[[tr,val]]\n    df_pl.rename(columns={tr:'train',val:'validation'},inplace=True)\n    df_pl.plot(title='Model '+tr,figsize=(12,8)).set(xlabel='Epoch',ylabel=tr)","262929e7":"from keras.models import Model\nfrom keras.layers import Input, Conv2D, MaxPooling2D, AveragePooling2D, Flatten, GlobalAveragePooling2D, Dense, Dropout\nfrom keras.layers.merge import concatenate","4461e1a3":"\ndef Inception_block(input_layer, f1, f2_conv1, f2_conv3, f3_conv1, f3_conv5, f4): \n  # Input: \n  # - f1: number of filters of the 1x1 convolutional layer in the first path\n  # - f2_conv1, f2_conv3 are number of filters corresponding to the 1x1 and 3x3 convolutional layers in the second path\n  # - f3_conv1, f3_conv5 are the number of filters corresponding to the 1x1 and 5x5  convolutional layer in the third path\n  # - f4: number of filters of the 1x1 convolutional layer in the fourth path\n\n  # 1st path:\n  path1 = Conv2D(filters=f1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n\n  # 2nd path\n  path2 = Conv2D(filters = f2_conv1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n  path2 = Conv2D(filters = f2_conv3, kernel_size = (3,3), padding = 'same', activation = 'relu')(path2)\n\n  # 3rd path\n  path3 = Conv2D(filters = f3_conv1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n  path3 = Conv2D(filters = f3_conv5, kernel_size = (5,5), padding = 'same', activation = 'relu')(path3)\n\n  # 4th path\n  path4 = MaxPooling2D((3,3), strides= (1,1), padding = 'same')(input_layer)\n  path4 = Conv2D(filters = f4, kernel_size = (1,1), padding = 'same', activation = 'relu')(path4)\n\n  output_layer = concatenate([path1, path2, path3, path4], axis = -1)\n\n  return output_layer","cb95e146":"\ndef GoogLeNet():\n  # input layer \n  input_layer = Input(shape = (224, 224, 3))\n  \n  # convolutional layer: filters = 64, kernel_size = (7,7), strides = 2\n  X = Conv2D(filters = 64, kernel_size = (7,7), strides = 2, padding = 'valid', activation = 'relu')(input_layer)\n\n  # max-pooling layer: pool_size = (3,3), strides = 2\n  X = MaxPooling2D(pool_size = (3,3), strides = 2)(X)\n\n  # convolutional layer: filters = 64, strides = 1\n  X = Conv2D(filters = 64, kernel_size = (1,1), strides = 1, padding = 'same', activation = 'relu')(X)\n\n  # convolutional layer: filters = 192, kernel_size = (3,3)\n  X = Conv2D(filters = 192, kernel_size = (3,3), padding = 'same', activation = 'relu')(X)\n\n  # max-pooling layer: pool_size = (3,3), strides = 2\n  X = MaxPooling2D(pool_size= (3,3), strides = 2)(X)\n\n  # 1st Inception block\n  X = Inception_block(X, f1 = 64, f2_conv1 = 96, f2_conv3 = 128, f3_conv1 = 16, f3_conv5 = 32, f4 = 32)\n\n  # 2nd Inception block\n  X = Inception_block(X, f1 = 128, f2_conv1 = 128, f2_conv3 = 192, f3_conv1 = 32, f3_conv5 = 96, f4 = 64)\n\n  # max-pooling layer: pool_size = (3,3), strides = 2\n  X = MaxPooling2D(pool_size= (3,3), strides = 2)(X)\n\n  # 3rd Inception block\n  X = Inception_block(X, f1 = 192, f2_conv1 = 96, f2_conv3 = 208, f3_conv1 = 16, f3_conv5 = 48, f4 = 64)\n\n  # Extra network 1:\n  X1 = AveragePooling2D(pool_size = (5,5), strides = 3)(X)\n  X1 = Conv2D(filters = 128, kernel_size = (1,1), padding = 'same', activation = 'relu')(X1)\n  X1 = Flatten()(X1)\n  X1 = Dense(1024, activation = 'relu')(X1)\n  X1 = Dropout(0.7)(X1)\n  X1 = Dense(5, activation = 'softmax')(X1)\n\n  \n  # 4th Inception block\n  X = Inception_block(X, f1 = 160, f2_conv1 = 112, f2_conv3 = 224, f3_conv1 = 24, f3_conv5 = 64, f4 = 64)\n\n  # 5th Inception block\n  X = Inception_block(X, f1 = 128, f2_conv1 = 128, f2_conv3 = 256, f3_conv1 = 24, f3_conv5 = 64, f4 = 64)\n\n  # 6th Inception block\n  X = Inception_block(X, f1 = 112, f2_conv1 = 144, f2_conv3 = 288, f3_conv1 = 32, f3_conv5 = 64, f4 = 64)\n\n  # Extra network 2:\n  X2 = AveragePooling2D(pool_size = (5,5), strides = 3)(X)\n  X2 = Conv2D(filters = 128, kernel_size = (1,1), padding = 'same', activation = 'relu')(X2)\n  X2 = Flatten()(X2)\n  X2 = Dense(1024, activation = 'relu')(X2)\n  X2 = Dropout(0.7)(X2)\n  X2 = Dense(1000, activation = 'softmax')(X2)\n  \n  \n  # 7th Inception block\n  X = Inception_block(X, f1 = 256, f2_conv1 = 160, f2_conv3 = 320, f3_conv1 = 32, \n                      f3_conv5 = 128, f4 = 128)\n\n  # max-pooling layer: pool_size = (3,3), strides = 2\n  X = MaxPooling2D(pool_size = (3,3), strides = 2)(X)\n\n  # 8th Inception block\n  X = Inception_block(X, f1 = 256, f2_conv1 = 160, f2_conv3 = 320, f3_conv1 = 32, f3_conv5 = 128, f4 = 128)\n\n  # 9th Inception block\n  X = Inception_block(X, f1 = 384, f2_conv1 = 192, f2_conv3 = 384, f3_conv1 = 48, f3_conv5 = 128, f4 = 128)\n\n  # Global Average pooling layer \n  X = GlobalAveragePooling2D(name = 'GAPL')(X)\n\n  # Dropoutlayer \n  X = Dropout(0.4)(X)\n\n  # output layer \n  #X = Dense(1000, activation = 'softmax')(X)\n  \n  # model\n  model = Model(input_layer, X, name = 'GoogLeNet')\n\n  return model","14747ebb":"def build_inception():\n    \n    base = GoogLeNet()\n    \n    A0 = base.output\n    \n    N1 = layer.BatchNormalization()(A0)\n    \n    A2 = layer.Dense(kernel_regularizer=tf.keras.regularizers.L2(0.0002),\n                     units=3072, activation='relu')(N1)\n    N2 = layer.BatchNormalization()(A2)\n    N2 = layer.Dropout(0.2)(N2)\n    \n    A3 = layer.Dense(kernel_regularizer=tf.keras.regularizers.L2(0.0002),\n                     units=512, activation='relu')(N2)\n    N3 = layer.BatchNormalization()(A3)\n    N3 = layer.Dropout(0.4)(N3)\n    \n    outputs = layer.Dense(units = 2, activation = \"softmax\")(N3)\n    \n    model = tf.keras.Model(inputs = base.input, outputs = outputs)\n    \n    return model    ","d230fee6":"Inc_model = build_inception()\nInc_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                  loss='binary_crossentropy',\n                  metrics=['accuracy', tfa.metrics.F1Score(num_classes=2, threshold=0.5), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\nInc_model.summary()","4f29679d":"checkpoint_path = 'training\/conv\/InceptionNet.ckpt'\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\ncp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n                                                save_weights_only=False,\n                                                save_freq ='epoch',\n                                                verbose=1)\n\nestop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n                                         min_delta=0.0001,\n                                         patience=5,\n                                         mode=\"min\",\n                                         restore_best_weights=True)","e3b3dd2d":"historyInc = Inc_model.fit(train_ds, \n                           epochs=100, \n                           validation_data=val_ds, \n                           callbacks = [cp_callback, estop])\nInc_model.save('InceptionNet_model.h5')","8e78b0f5":"l = historyInc.history.keys()\nprint(l, len(l), type(l))\nmetrics = list(historyInc.history.keys())\n\ndf = pd.DataFrame(historyInc.history)","b05a492d":"def f1_mod(x):\n    return x[0]\n\ndef per_cent(x):\n    return x*100\n    \ndf['f1_score'] = df['f1_score'].apply(f1_mod)\ndf['val_f1_score'] = df['val_f1_score'].apply(f1_mod)\n\nfor i in df.columns:\n    df[i] = df[i].apply(per_cent)\n\n\ndf.head()","0d31c0c5":"for i in range(len(l)\/\/2):\n    tr = metrics[i]\n    val = \"val_\" + tr\n    df_pl= df[[tr,val]]\n    df_pl.rename(columns={tr:'train',val:'validation'},inplace=True)\n    df_pl.plot(title='Model '+tr,figsize=(12,8)).set(xlabel='Epoch',ylabel=tr)","2ed6fe62":"def Mobilenet_model(input_shape, weights=None, transfer=False):\n    \n    input_img = tf.keras.Input(shape=input_shape)\n    \n    base = tf.keras.applications.MobileNetV2(input_shape = input_shape, weights = weights, \n                                             include_top = False, input_tensor = input_img)\n    base.trainable = not(transfer)\n    \n    A0 = base.output\n    \n    A1 = layer.GlobalAveragePooling2D( )(A0)\n    N1 = layer.BatchNormalization()(A1)\n    N1 = layer.Dropout(0.1)(N1)\n    \n    A2 = layer.Dense(kernel_regularizer=tf.keras.regularizers.L2(0.0002),\n                     units = 256, activation = 'ReLU')(N1)\n    N2 = layer.BatchNormalization()(A2)\n    N2 = layer.Dropout(0.2)(N2)\n    \n    A3 = layer.Dense(kernel_regularizer=tf.keras.regularizers.L2(0.0002),\n                     units = 128, activation = 'ReLU')(A2)\n    N3 = layer.BatchNormalization()(A3)\n    N3 = layer.Dropout(0.4)(N3)\n    \n    outputs = layer.Dense(units = 2, activation = \"softmax\")(N3)\n    \n    model = tf.keras.Model(inputs = input_img, outputs = outputs)\n    \n    return model","00800382":"mobile_I = Mobilenet_model(input_shape=(224,224,3), weights='imagenet',transfer=True)\n\nmobile_I.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                 loss='binary_crossentropy',\n                 metrics=['accuracy', tfa.metrics.F1Score(num_classes=2, threshold=0.5), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n                 \nmobile_I.summary()","60b202b9":"checkpoint_path = 'training\/conv\/Mobile_image_cp.ckpt'\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\ncp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n                                                save_weights_only=False,\n                                                save_freq ='epoch',\n                                                verbose=1)\n\nestop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n                                         min_delta=0.0001,\n                                         patience=5,\n                                         mode=\"min\",\n                                         restore_best_weights=True)","29c0a49e":"historyMi = mobile_I.fit(train_ds, \n                          epochs=100, \n                          validation_data=val_ds, \n                          callbacks = [cp_callback, estop])\nmobile_I.save('MobileNet_model.h5')","530eab0a":"l = historyMi.history.keys()\nprint(l, len(l), type(l))\nmetrics = list(historyMi.history.keys())\n\ndf = pd.DataFrame(historyMi.history)","d8aadb9f":"def f1_mod(x):\n    return x[0]\n\ndef per_cent(x):\n    return x*100\n    \ndf['f1_score'] = df['f1_score'].apply(f1_mod)\ndf['val_f1_score'] = df['val_f1_score'].apply(f1_mod)\n\nfor i in df.columns:\n    df[i] = df[i].apply(per_cent)\n\n\ndf.head()","98f7ef14":"for i in range(len(l)\/\/2):\n    tr = metrics[i]\n    val = \"val_\" + tr\n    df_pl= df[[tr,val]]\n    df_pl.rename(columns={tr:'train',val:'validation'},inplace=True)\n    df_pl.plot(title='Model '+tr,figsize=(12,8)).set(xlabel='Epoch',ylabel=tr)","bcce9666":"from tensorflow.keras import layers\nfrom tensorflow import keras\n\n\ndef get_model(img_size, num_classes):\n    inputs = keras.Input(shape=img_size + (3,))\n\n    ### [First half of the network: downsampling inputs] ###\n\n    # Entry block\n    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    previous_block_activation = x  # Set aside residual\n\n    # Blocks 1, 2, 3 are identical apart from the feature depth.\n    for filters in [64, 128, 256]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n\n        # Project residual\n        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n            previous_block_activation\n        )\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    ### [Second half of the network: upsampling inputs] ###\n\n    for filters in [256, 128, 64, 32]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.UpSampling2D(2)(x)\n\n        # Project residual\n        residual = layers.UpSampling2D(2)(previous_block_activation)\n        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    # Add a per-pixel classification layer\n    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n\n    # Define the model\n    model = keras.Model(inputs, outputs)\n    return model\n\n\n# Free up RAM in case the model definition cells were run multiple times\nkeras.backend.clear_session()\n\n# Build model\nimg_size = (224, 224)\nUNet_model = get_model(img_size, 2)\nUNet_model.summary()","efd5bcc4":"def Unet_classifier(base):\n    \n    A0 = base.output\n    \n    A1 = layer.GlobalAveragePooling2D( )(A0)\n    N1 = layer.BatchNormalization()(A1)\n    N1 = layer.Dropout(0.1)(N1)\n    \n    A2 = layer.Dense(kernel_regularizer=tf.keras.regularizers.L2(0.0002),\n                     units=3072, activation='relu')(N1)\n    N2 = layer.BatchNormalization()(A2)\n    N2 = layer.Dropout(0.2)(N2)\n    \n    A3 = layer.Dense(kernel_regularizer=tf.keras.regularizers.L2(0.0002),\n                     units=512, activation='relu')(N2)\n    N3 = layer.BatchNormalization()(A3)\n    N3 = layer.Dropout(0.4)(N3)\n    \n    outputs = layer.Dense(units = 2, activation = \"softmax\")(N3)\n    \n    model = tf.keras.Model(inputs = base.input, outputs = outputs)\n    \n    return model","9c926367":"Unet = Unet_classifier(UNet_model)\nUnet.summary()","a4790b42":"Unet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                 loss='binary_crossentropy',\n                 metrics=['accuracy', tfa.metrics.F1Score(num_classes=2, threshold=0.5), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])","225d4c9f":"checkpoint_path = 'training\/conv\/UNet1.ckpt'\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\ncp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n                                                save_weights_only=False,\n                                                save_freq ='epoch',\n                                                verbose=1)\n\nestop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n                                         min_delta=0.0001,\n                                         patience=5,\n                                         mode=\"min\",\n                                         restore_best_weights=True)","15d8d23c":"historyU = Unet.fit(train_ds, epochs=100, validation_data=val_ds, callbacks = [cp_callback, estop])\nUnet.save('UNet.h5')","f3431fc6":"l = historyU.history.keys()\nprint(l, len(l), type(l))\nmetrics = list(historyU.history.keys())\n\ndf = pd.DataFrame(historyU.history)","d9dae90f":"def f1_mod(x):\n    return x[0]\n\ndef per_cent(x):\n    return x*100\n    \ndf['f1_score'] = df['f1_score'].apply(f1_mod)\ndf['val_f1_score'] = df['val_f1_score'].apply(f1_mod)\n\nfor i in df.columns:\n    df[i] = df[i].apply(per_cent)\n\n\ndf.head()","0b1c7310":"for i in range(len(l)\/\/2):\n    tr = metrics[i]\n    val = \"val_\" + tr\n    df_pl= df[[tr,val]]\n    df_pl.rename(columns={tr:'train',val:'validation'},inplace=True)\n    df_pl.plot(title='Model '+tr,figsize=(12,8)).set(xlabel='Epoch',ylabel=tr)","97ea34a5":"## Transfer learning with weights pretrained on imagenet","dcce1a96":"# VGG16","1f963df9":"# ResNet","5cb5fae8":"## Transfer Learning with weights pretrained on *imagenet*","48ae1a42":"### One-Hot Encoding","de7e4caf":"# Imports\n## Importing Libraries","3b806050":"[Implementation of GoogLeNet on Keras](https:\/\/medium.com\/mlearning-ai\/implementation-of-googlenet-on-keras-d9873aeed83c)","77ed2ebb":"## Loading the dataset","e1a6676b":"# Inception\/GoogLeNet","456a64ec":"# AlexNet","1880fcba":"# U-Net","819b98df":"## Transfer Learning with weights pretrained on *imagenet*","cca12263":"# The Models","691c55ed":"# MobileNetV2"}}