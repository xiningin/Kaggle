{"cell_type":{"f1cf8372":"code","21490116":"code","d57f964b":"code","8a664ff9":"code","1556ac02":"code","1739aadd":"code","c5f4cf26":"code","c7f80d6f":"code","76327188":"code","7fb1b071":"code","63ea7ae1":"code","b3618b63":"code","0f55d6eb":"code","49a88bfc":"code","af66ab0f":"code","f089b1b8":"code","1a9489e9":"code","e42f4e9f":"code","b0ee5979":"code","21ac84f6":"code","d1a1a17f":"code","fbfa91fd":"code","4d269427":"code","c71af05a":"code","1c02c430":"code","aef92510":"code","7d686c59":"code","df87c386":"code","cedc32d2":"code","c69ac271":"code","7be965c4":"code","83dde2a2":"code","48ac1e67":"code","05d43f1f":"code","ade7abf9":"code","9ee7d14a":"code","eb038047":"code","4c815275":"code","9a5f9eb6":"code","c98f5c4b":"code","377248ed":"code","efe5e807":"code","24a23f63":"code","c75c4787":"code","ec4bcbb2":"code","c1706065":"code","ec46d9dc":"code","1ece8bcb":"code","6f3c678b":"markdown","7bd52c6d":"markdown","4cbdf2a6":"markdown","2f626169":"markdown","f78cbe9c":"markdown","6f266ead":"markdown","3b36ea39":"markdown","b5c8acb8":"markdown","eb0a2ab6":"markdown","a750df6d":"markdown","f7562c1f":"markdown"},"source":{"f1cf8372":"## We will start by importing all the necessary libraries\n\n## For Analysis\nimport numpy as np\nimport pandas as pd\n\n## For visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n## For prediction\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier \n\n\n","21490116":"## After all our libraries are imported, its time to import the dataset\n## We will use the pandas read_csv function to import the test.csv and the train.csv\n\ntrain_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')","d57f964b":"## As the standard norm, we will have a look at the first few rows\ntrain_df.head()\n## We will drop the PassengerId and Ticket columns since those two columns are not required for analysis\ntrain_df.drop(['PassengerId','Ticket'],axis = 1,inplace = True)\n## Viewing the dataset one more time\ntrain_df.head()\n## Embarked, Sex and Pclass are categorical that we will encode later.","8a664ff9":"train_df.info()\n## OBSERVATION\n## Cabin has 204 not null values\n## Age has 714 not null values\n## Embarked has 889 not null values\n## We will do data cleaning\/Imputation on these columns","1556ac02":"train_df.describe()\n## OBSERVATION\n## Out of 891 passengers on board:\n## 25% aged 20 or below,50% aged 28 or below and 75% aged 38 or below. We also had passenger\/s whose age was 80\n## 50% of passengers travelled with no siblings or spouce while 75% passengers travelled with no parents or children\n## 75% of the passengers bought tickets that cost 31 USD or less. However the ticket mean price is 32 USD, so 25% of the passengers\n## got expensive tickets","1739aadd":"## Drop the Cabin column\ntrain_df.drop(['Cabin'],axis = 1, inplace = True)\n## Drop the Name column\ntrain_df.drop(['Name'],axis = 1, inplace = True)\n## Drop the fare column since its correlated to Pclass\ntrain_df.drop(['Fare'], axis = 1, inplace = True)","c5f4cf26":"## Check passengder details for all those passengers whose age is missing\n## .loc[] -> Access a group of rows and columns by label(s) or a boolean array.\n## .loc[] is primarily label based, but may also be used with a boolean array.\n\ntrain_df.loc[train_df['Age'].isnull()]\n\n## So we will impute all the null values in Age with the median age\ntrain_df['Age'].fillna(train_df['Age'].median(), inplace = True)","c7f80d6f":"## Check passengder details for all those passengers whose Embarkment info is missing\ntrain_df.loc[train_df['Embarked'].isnull()]\n\n## So we will impute all the null values in Embarkment with the mode value\ntrain_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace = True)","76327188":"## Checking the data data once more after the data cleaning\ntrain_df.isnull().sum()\n","7fb1b071":"## So now our dataset looks as follows\ntrain_df.head(15)\n## Our Observations:\n## The following are categorial features - Pclass,Sex,SibSp,Parch,Embarked\n## The following features are continious - Age","63ea7ae1":"## Observations by Pclass\nprint('The total number of people survivied from each class\\n')\nprint(train_df[['Pclass','Survived']].groupby(['Pclass'], as_index = False).sum())\nprint('*'* 50)\nprint('The average number of people survivied from each class\\n')\nprint(train_df[['Pclass','Survived']].groupby(['Pclass'], as_index = False).mean())\n\n## So the column Pclass has a strong predictive power.","b3618b63":"sns.barplot(x=\"Pclass\", y=\"Survived\", data=train_df)","0f55d6eb":"## Observations by Sex\nprint('The total number of people survived according to gender\\n')\nprint(train_df[['Sex','Survived']].groupby(['Sex']).sum())\nprint('*'* 50)\nprint('The average number of people survivied according to gender\\n')\nprint(train_df[['Sex','Survived']].groupby(['Sex']).mean())\n\n## So the column Sex has a strong predictive power.","49a88bfc":"sns.barplot(x=\"Sex\", y=\"Survived\", data=train_df)","af66ab0f":"## Observations by SibSp\nprint('The total number of people survived according to the number of Siblings or Spouce they had\\n')\nprint(train_df[['SibSp','Survived']].groupby(['SibSp']).sum())\nprint('*'* 50)\nprint('The average number of people survivied according to the number of Siblings or Spouce they had\\n')\nprint(train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp']).mean())\n","f089b1b8":"sns.barplot(x=\"SibSp\", y=\"Survived\", data=train_df)","1a9489e9":"## Observations by Parch\nprint('The total number of people survived according to the number of Parents or children they had\\n')\nprint(train_df[['Parch','Survived']].groupby(['Parch']).sum())\nprint('*'* 50)\nprint('The average number of people survivied according to the number of Parents or children they had\\n')\nprint(train_df[[\"Parch\", \"Survived\"]].groupby(['Parch']).mean())","e42f4e9f":"sns.barplot(x=\"Parch\", y=\"Survived\", data=train_df)","b0ee5979":"sns.barplot(x=\"Embarked\", y=\"Survived\",hue=\"Sex\", data=train_df)\n","21ac84f6":"## Since Age is a continious numerical feature, a groupby will not make much sense.\n## Instead we will plot a barplot after binning\n\ntrain_df['age_by_decade'] = pd.cut(x=train_df['Age'], bins=[0,10, 20, 30, 40, 50, 60, 80], labels=['Infants','Teenagers','20s', '30s','40s','50s','Oldies'])","d1a1a17f":"## Adding a New column Relatives\n## train_df['Relatives'] = train_df['SibSp'] + train_df['Parch']","fbfa91fd":"train_df.head(15)","4d269427":"## Observations by Age groups\/bins\nprint('The total number of people survived according to the Age group\\n')\nprint(train_df[['age_by_decade','Survived']].groupby(['age_by_decade']).sum())\nprint('*'* 50)\nprint('The average number of people survivied according to the Age group\\n')\nprint(train_df[[\"age_by_decade\", \"Survived\"]].groupby(['age_by_decade']).mean())\n\n\n","c71af05a":"sns.barplot(x=\"age_by_decade\", y=\"Survived\", data=train_df)","1c02c430":"#map each Age value to a numerical value\nage_mapping = {'Infants': 1, 'Teenagers': 2, '20s': 3, '30s': 4, '40s': 5, '50s': 6, 'Oldies': 7}\ntrain_df['age_by_decade'] = train_df['age_by_decade'].map(age_mapping)\ntrain_df['age_by_decade'] = pd.to_numeric(train_df['age_by_decade'])\n\n\n#dropping the Age feature for now, might change\ntrain_df = train_df.drop(['Age'], axis = 1)\n## test = test.drop(['Age'], axis = 1)\n\ntrain_df.head()","aef92510":"#map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain_df['Sex'] = train_df['Sex'].map(sex_mapping)\n## test['Sex'] = test['Sex'].map(sex_mapping)\n\ntrain_df.head(15)","7d686c59":"LikelySurvived = pd.Series([])\nfor i in range(len(train_df)):\n    if  (train_df['Sex'][i]  == 1) | (train_df['age_by_decade'][i] == 1):\n        a = pd.Series([1])\n        LikelySurvived = LikelySurvived.append(a)\n    else:\n        a = pd.Series([0])\n        LikelySurvived = LikelySurvived.append(a)\n        \n\ntrain_df['LikelySurvived'] = LikelySurvived.values\n\n","df87c386":"LikelyDied = pd.Series([])\nfor i in range(len(train_df)):\n    if  ((train_df['Sex'][i]  == 0) &  (train_df['Pclass'][i]  == 3)) | (train_df['Sex'][i]  == 0  & (train_df['Embarked'][i]  == 'Q')) :\n        a = pd.Series([1])\n        LikelyDied = LikelyDied.append(a)\n    else:\n        a = pd.Series([0])\n        LikelyDied = LikelyDied.append(a)\n        \n\ntrain_df['LikelyDied'] = LikelyDied.values","cedc32d2":"#map each Embarked value to a numerical value\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ntrain_df['Embarked'] = train_df['Embarked'].map(embarked_mapping)\n## test['Embarked'] = test['Embarked'].map(embarked_mapping)\n\ntrain_df.head()","c69ac271":"## Checking the correlation between the features\npd.DataFrame(abs(train_df.corr()['Survived']).sort_values(ascending = False))","7be965c4":"## The correlation Heatmap\nsns.heatmap(train_df.corr(),linewidth = .5)","83dde2a2":"X = train_df.drop(['Survived'], axis=1)\ny = train_df[\"Survived\"]\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.22, random_state = 5)","48ac1e67":"## Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","05d43f1f":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)","ade7abf9":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC(max_iter=1200000)\nlinear_svc.fit(X_train, y_train)\ny_pred = linear_svc.predict(X_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)","9ee7d14a":"decisiontree = DecisionTreeClassifier()\ndecisiontree.fit(X_train, y_train)\ny_pred = decisiontree.predict(X_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)","eb038047":"## Random Forest\nrandomforest = RandomForestClassifier()\nrandomforest.fit(X_train, y_train)\ny_pred = randomforest.predict(X_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","4c815275":"# KNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)","9a5f9eb6":"## Stochaistic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, y_train)\ny_pred = sgd.predict(X_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)","c98f5c4b":"## Gradient Boosting\n\ngbk = GradientBoostingClassifier()\ngbk.fit(X_train, y_train)\ny_pred = gbk.predict(X_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","377248ed":"## XGBoost Classifier\nxgb = XGBClassifier(random_state=5,learning_rate=0.01)\nxgb.fit(X_train, y_train)\ny_pred = xgb.predict(X_val)\nacc_xgb = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_xgb)","efe5e807":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier','XGBoost'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk,acc_xgb]})\nmodels.sort_values(by='Score', ascending=False)","24a23f63":"test_df.head(15)","c75c4787":"## Dropping columns Cabin, Fare,Ticket,Name\ntest_df.drop(['Name','Ticket','Fare','Cabin'], axis = 1, inplace = True)\n","ec4bcbb2":"## Imputing missing values\ntest_df['Age'].fillna(test_df['Age'].median(), inplace = True)\ntest_df['Embarked'].fillna(test_df['Embarked'].mode()[0], inplace = True)","c1706065":"## Changing colums Sex,Age and Embarked\n\n## Sex\nsex_mapping = {\"male\": 0, \"female\": 1}\ntest_df['Sex'] = test_df['Sex'].map(sex_mapping)\n\n## Age\ntest_df['age_by_decade'] = pd.cut(x=test_df['Age'], bins=[0,10, 20, 30, 40, 50, 60, 80], labels=['Infants','Teenagers','20s', '30s','40s','50s','Oldies'])\nage_mapping = {'Infants': 1, 'Teenagers': 2, '20s': 3, '30s': 4, '40s': 5, '50s': 6, 'Oldies': 7}\ntest_df['age_by_decade'] = test_df['age_by_decade'].map(age_mapping)\ntest_df['age_by_decade'] = pd.to_numeric(train_df['age_by_decade'])\n\n\n#dropping the Age feature for now, might change\ntest_df = test_df.drop(['Age'], axis = 1)\n\n## Embarked\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ntest_df['Embarked'] = test_df['Embarked'].map(embarked_mapping)\n\n## test_df['Relatives'] = test_df['SibSp'] + test_df['Parch']\n\n## Adding column LikelySurvive\nLikelySurvived = pd.Series([])\nfor i in range(len(test_df)):\n    if  (test_df['Sex'][i]  == 1) | (test_df['age_by_decade'][i] == 1):\n        a = pd.Series([1])\n        LikelySurvived = LikelySurvived.append(a)\n    else:\n        a = pd.Series([0])\n        LikelySurvived = LikelySurvived.append(a)\n        \n\ntest_df['LikelySurvived'] = LikelySurvived.values\n\n## Likely Died\nLikelyDied = pd.Series([])\nfor i in range(len(test_df)):\n    if  (test_df['Sex'][i]  == 0  &  (test_df['Pclass'][i]  == 3)) | (train_df['Sex'][i]  == 0  & (train_df['Embarked'][i]  == 'Q')) :\n        a = pd.Series([1])\n        LikelyDied = LikelyDied.append(a)\n    else:\n        a = pd.Series([0])\n        LikelyDied = LikelyDied.append(a)\n        \n\ntest_df['LikelyDied'] = LikelyDied.values","ec46d9dc":"test_df.head(15)","1ece8bcb":"#set ids as PassengerId and predict survival \nids = test_df['PassengerId']\npredictions = svc.predict(test_df.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","6f3c678b":"# Prediction","7bd52c6d":"# We will be analysing the Titanic Data Set in this notebook. Any feedback will be highly appreciated.\n\nI have applied the following models on the cleansed data (cleaning is also done in detail) to compare the performance.\n\n* Logistic Regression\n* Support Vector Machines\n* Decision Tree Classifier\n* Random Forest Classifier\n* KNN or k-Nearest Neighbors\n* Stochastic Gradient Descent\n* Gradient Boosting Classifier\n* XGBoost","4cbdf2a6":"# Data Cleaning\/ Imputation\nWe had the followin observation on our data. So we will be doing data imputation on those columns:                                    \n* Cabin has 204 not null values\n* Age has 714 not null values\n* Embarked has 889 not null values\n","2f626169":"# Submission to Kaggle","f78cbe9c":"Now that we have a clean dataset, we will observe the number of people survived based on Pclass,Sex and Age,SibSp,Parch and Embarked","6f266ead":"Creating the submission File","3b36ea39":"We split the training data into 80% training and 20% validation set","b5c8acb8":"# Data Analysis and Visualization","eb0a2ab6":"# Final Data Imputation For Models\n* Column 'age_by_decade' to be changed to numerical\n* Column Age to be dropped\n* Column Sex to be changed to numerical\n","a750df6d":"We will be training and testing the following models:\n\n* Logistic Regression\n* Support Vector Machines\n* Decision Tree Classifier\n* Random Forest Classifier\n* KNN or k-Nearest Neighbors\n* Stochastic Gradient Descent\n* Gradient Boosting Classifier\n* XGBoost","f7562c1f":"Resources referred:\n\n[Titanic Survival Predictions (Beginner)](https:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner)"}}