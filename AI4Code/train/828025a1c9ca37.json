{"cell_type":{"b9367b04":"code","5d362b04":"code","0bb627d8":"code","7cdb2455":"code","c7c1767b":"code","51146b14":"code","77effe63":"code","2c4ef004":"code","b18774a5":"code","3b4cfc91":"code","ad984372":"code","71b800ac":"code","3e2e6ffb":"code","6742b3c7":"code","f2ad9a90":"code","ae273c36":"code","65331205":"code","faee13cb":"code","4b6951c2":"code","764be230":"code","ba6155ac":"code","b56d4ed3":"code","3265cb2c":"code","75a106e8":"code","e37ca38a":"code","01af12bf":"code","4bf675fb":"code","e50a561e":"code","98bf7eb7":"code","b8c715b2":"code","3ea2a2fa":"code","6711dca5":"code","4e364d3a":"code","f2484cad":"code","fa881c57":"code","13c9e10c":"code","749ab83e":"code","6358d37b":"code","bc8f6d14":"code","c8ef7b6f":"code","65fa7715":"code","68248408":"markdown","50436c2f":"markdown","40995416":"markdown","377c79d7":"markdown","96673c5b":"markdown","67abd204":"markdown","39414ef8":"markdown","07378947":"markdown","8bda24e4":"markdown","d99acc1a":"markdown","cf1cec4c":"markdown","d84c5c0b":"markdown","7df8d59f":"markdown","43c6531f":"markdown","f3bf7bef":"markdown","dfe6e9b0":"markdown","15ae2fb2":"markdown","16c2822c":"markdown","fb24394c":"markdown","ab0426ce":"markdown","37a2050b":"markdown","46daa9c8":"markdown","d7b9b2b2":"markdown","e6975e54":"markdown","f78b272b":"markdown","38b4f838":"markdown","80217f4c":"markdown","06689044":"markdown","3173192c":"markdown","4e6d441b":"markdown","132574c6":"markdown","067de708":"markdown","3198b20b":"markdown","6885f90c":"markdown","18c3cee4":"markdown","f1f56077":"markdown","6a5bdf0a":"markdown","7fa55f16":"markdown","a2f27b81":"markdown","af2a1c7b":"markdown","e1346016":"markdown","48b74b10":"markdown","8fbf4d44":"markdown","8a4926af":"markdown","fca89ed7":"markdown","ec8cec98":"markdown","7b05ea23":"markdown","a8f1cf7d":"markdown","2cc1808a":"markdown","f49a5143":"markdown","4332eb7e":"markdown"},"source":{"b9367b04":"# TensorFlow\nimport tensorflow as tf\nprint(\"Tensorflow version \" + tf.__version__)\n\n\n# Model\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\n\n\n# Plotting\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Matplotlib defaults\nplt.style.use('seaborn-whitegrid')\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\n\n\n# Data Preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\n\n\nfrom sklearn.metrics import mean_absolute_error\nimport numpy as np\nimport shap","5d362b04":"# Set Global Random Seed (to get stable results)\n\nfrom numpy.random import seed\nseed(42)\n\ntf.random.set_seed(42)","0bb627d8":"def split_data(ds):\n    ds_train, val_test_ds = train_test_split(ds, test_size=0.2, random_state=1)\n    ds_valid, ds_test = train_test_split(val_test_ds, test_size=0.5, random_state=1)\n    return ds_train, ds_valid, ds_test","7cdb2455":"def load_data(ds_train, ds_test):\n    \n    X_train = ds_train.drop('quality', axis=1)\n    X_train = pd.DataFrame(preprocessor.fit_transform(X_train))\n    X_train.columns = features_num + [\"type_red\", \"type_white\"]\n    \n    y_train = ds_train['quality']\n    \n    X_test = ds_test.drop('quality', axis=1)\n    X_test = pd.DataFrame(preprocessor.transform(X_test))\n    X_test.columns = features_num + [\"type_red\", \"type_white\"]\n    \n    y_test = ds_test['quality']\n    \n    return X_train, X_test, y_train, y_test","c7c1767b":"transformer_cat = make_pipeline(\n    #SimpleImputer(strategy=\"constant\", fill_value=\"NA\"), # Fortunately, no missing values\n    OneHotEncoder(handle_unknown='ignore'),\n)","51146b14":"transformer_num = make_pipeline(\n    #SimpleImputer(strategy=\"constant\"), # Fortunately, no missing values\n    MinMaxScaler(),\n)","77effe63":"# Get data directory\ndata_dir = '\/kaggle\/input\/wine-quality-data-set-red-white-wine\/wine-quality-white-and-red.csv'\n\n# Read data\nds = pd.read_csv(data_dir)\n\n# Create data splits\nds_train, ds_valid, ds_test = split_data(ds)","2c4ef004":"features_num = [\n    'fixed acidity', 'volatile acidity', 'citric acid',\n    'residual sugar', 'chlorides', 'free sulfur dioxide',\n    'total sulfur dioxide', 'density', 'pH', 'sulphates', \n    'alcohol'\n]\n\nfeatures_cat = [\"type\"]\n\n\npreprocessor = make_column_transformer(\n    (transformer_num, features_num),\n    (transformer_cat, features_cat),\n)","b18774a5":"# Execute load_data() for training\nX_train, X_valid, y_train, y_valid = load_data(ds_train, ds_valid)","3b4cfc91":"# Original Data\nds_orig = pd.read_csv(data_dir)\nds_orig.head()","ad984372":"# Processed Features (training set - in DataFrame form)\npd.DataFrame(X_train).head()","71b800ac":"# Training Configuration\nEPOCHS = 300\nBATCH_SIZE = 2 ** 8 #256","3e2e6ffb":"input_shape=[X_train.shape[1]]","6742b3c7":"# Define linear model\nmodel = keras.Sequential([\n    layers.Dense(1, input_shape=input_shape),\n])\n\n# Compile in the optimizer and loss function\nmodel.compile(\n    optimizer='adam',\n    loss='mse',\n    metrics=['mae']\n)\n\n# Fit model (and save training history)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    verbose=0, # suppress output since we'll plot the curves\n)\n\n# Convert the training history to a dataframe\nhistory_frame = pd.DataFrame(history.history)\n\n# Plot training history\nhistory_frame.loc[0:, ['loss', 'val_loss']].plot()\n\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_frame['val_loss'].min()));\nprint(\"Minimum Validation MAE (mean absolute error): {:0.4f}\".format(history_frame['val_mae'].min()));","f2ad9a90":"# Define model with two representation layers\nmodel = keras.Sequential([\n    layers.Dense(2 ** 4, activation='relu', input_shape=input_shape),\n    layers.Dense(2 ** 4, activation='relu'),\n    layers.Dense(1),\n])\n\n# Compile in the optimizer and loss function\nmodel.compile(\n    optimizer='adam',\n    loss='mse',\n    metrics=['mae']\n)\n\n# Fit model (and save training history)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    verbose=0, # suppress output since we'll plot the curves\n)\n\n# Convert the training history to a dataframe\nhistory_frame = pd.DataFrame(history.history)\n\n# Plot training history\nhistory_frame.loc[0:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[0:, ['mae', 'val_mae']].plot()\n\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_frame['val_loss'].min()));\nprint(\"Minimum Validation MAE (mean absolute error): {:0.4f}\".format(history_frame['val_mae'].min()));","ae273c36":"# Define model with one representation layer\nmodel = keras.Sequential([\n    layers.Dense(2 ** 4, activation='relu', input_shape=input_shape),\n    layers.Dense(1),\n])\n\n# Compile in the optimizer and loss function\nmodel.compile(\n    optimizer='adam',\n    loss='mse',\n    metrics=['mae']\n)\n\n# Fit model (and save training history)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    verbose=0, # suppress output since we'll plot the curves\n)\n\n# Convert the training history to a dataframe\nhistory_frame = pd.DataFrame(history.history)\n\n# Plot training history\nhistory_frame.loc[0:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[0:, ['mae', 'val_mae']].plot()\n\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_frame['val_loss'].min()));\nprint(\"Minimum Validation MAE (mean absolute error): {:0.4f}\".format(history_frame['val_mae'].min()));","65331205":"# Define model with three representation layers\nmodel = keras.Sequential([\n    layers.Dense(2 ** 4, activation='relu', input_shape=input_shape),\n    layers.Dense(2 ** 4, activation='relu'),\n    layers.Dense(2 ** 4, activation='relu'),\n    layers.Dense(1),\n])\n\n# Compile in the optimizer and loss function\nmodel.compile(\n    optimizer='adam',\n    loss='mse',\n    metrics=['mae']\n)\n\n# Fit model (and save training history)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    verbose=0, # suppress output since we'll plot the curves\n)\n\n# Convert the training history to a dataframe\nhistory_frame = pd.DataFrame(history.history)\n\n# Plot training history\nhistory_frame.loc[0:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[0:, ['mae', 'val_mae']].plot()\n\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_frame['val_loss'].min()));\nprint(\"Minimum Validation MAE (mean absolute error): {:0.4f}\".format(history_frame['val_mae'].min()));","faee13cb":"# Define model with three representation layers and 32 units per lays\nmodel = keras.Sequential([\n    layers.Dense(2 ** 5, activation='relu', input_shape=input_shape),\n    layers.Dense(2 ** 5, activation='relu'),\n    layers.Dense(2 ** 5, activation='relu'),\n    layers.Dense(1),\n])\n\n# Compile in the optimizer and loss function\nmodel.compile(\n    optimizer='adam',\n    loss='mse',\n    metrics=['mae']\n)\n\n# Fit model (and save training history)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    verbose=0, # suppress output since we'll plot the curves\n)\n\n# Convert the training history to a dataframe\nhistory_frame = pd.DataFrame(history.history)\n\n# Plot training history\nhistory_frame.loc[0:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[0:, ['mae', 'val_mae']].plot()\n\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_frame['val_loss'].min()));\nprint(\"Minimum Validation MAE (mean absolute error): {:0.4f}\".format(history_frame['val_mae'].min()));","4b6951c2":"# Define Baseline Model with three representation layers and 64 units per layer\nmodel = keras.Sequential([\n    layers.Dense(2 ** 6, activation='relu', input_shape=input_shape),\n    layers.Dense(2 ** 6, activation='relu'),\n    layers.Dense(2 ** 6, activation='relu'),\n    layers.Dense(1),\n])\n\n# Compile in the optimizer and loss function\nmodel.compile(\n    optimizer='adam',\n    loss='mse',\n    metrics=['mae']\n)\n\n# Fit model (and save training history)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    verbose=0, # suppress output since we'll plot the curves\n)\n\n# Convert the training history to a dataframe\nhistory_frame = pd.DataFrame(history.history)\n\n# Plot training history\nhistory_frame.loc[0:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[0:, ['mae', 'val_mae']].plot()\n\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_frame['val_loss'].min()));\nprint(\"Minimum Validation MAE (mean absolute error): {:0.4f}\".format(history_frame['val_mae'].min()));","764be230":"# Define new model (use previous baseline model as comparison)\nmodel = keras.Sequential([\n    layers.Dense(2 ** 10, activation='relu', input_shape=input_shape),\n    layers.Dense(2 ** 10, activation='relu'),\n    layers.Dense(2 ** 10, activation='relu'),\n    layers.Dense(2 ** 10, activation='relu'),\n    layers.Dense(2 ** 10, activation='relu'),\n    layers.Dense(1),\n])\n\n# Compile in the optimizer and loss function\nmodel.compile(\n    optimizer='adam',\n    loss='mse',\n    metrics=['mae']\n)\n\n# Fit model (and save training history)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    verbose=0, # suppress output since we'll plot the curves\n)\n\n# Convert the training history to a dataframe\nhistory_frame = pd.DataFrame(history.history)\n\n# Plot training history\nhistory_frame.loc[0:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[0:, ['mae', 'val_mae']].plot()\n\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_frame['val_loss'].min()));\nprint(\"Minimum Validation MAE (mean absolute error): {:0.4f}\".format(history_frame['val_mae'].min()));","ba6155ac":"# (Updated) Training Configuration\nEPOCHS = 3000","b56d4ed3":"#Defining callbacks\nearly_stopping = callbacks.EarlyStopping(\n    patience=50, # how many epochs to wait before stopping\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    restore_best_weights=True,\n)\n\nlr_schedule = callbacks.ReduceLROnPlateau(\n    patience=0,\n    factor=0.2,\n    min_lr=0.001,\n)","3265cb2c":"# Define model\n# Add callbacks\nmodel = keras.Sequential([\n    layers.Dense(2 ** 10, activation='relu', input_shape=input_shape),\n    layers.Dense(2 ** 10, activation='relu'),\n    layers.Dense(2 ** 10, activation='relu'),\n    layers.Dense(2 ** 10, activation='relu'),\n    layers.Dense(2 ** 10, activation='relu'),\n    layers.Dense(1),\n])\n\n# Compile in the optimizer and loss function\nmodel.compile(\n    optimizer='adam',\n    loss='mse',\n    metrics=['mae']\n)\n\n# Fit model (and save training history)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    verbose=0, # suppress output since we'll plot the curves\n)\n\n# Convert the training history to a dataframe\nhistory_frame = pd.DataFrame(history.history)\n\n# Plot training history\nhistory_frame.loc[0:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[0:, ['mae', 'val_mae']].plot()\n\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_frame['val_loss'].min()));\nprint(\"Minimum Validation MAE (mean absolute error): {:0.4f}\".format(history_frame['val_mae'].min()));","75a106e8":"# Define model\n# Add dropout layers\nmodel = keras.Sequential([\n    layers.Dense(2 ** 10, activation='relu', input_shape=input_shape),\n    layers.Dropout(0.2),\n    layers.Dense(2 ** 10, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(2 ** 10, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(2 ** 10, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(2 ** 10, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1),\n])\n\n\n# Compile in the optimizer and loss function\nmodel.compile(\n    optimizer='adam',\n    loss='mse',\n    metrics=['mae']\n)\n\n# Fit model (and save training history)\n# (Add callbacks)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=[early_stopping, lr_schedule],\n    verbose=0, # suppress output since we'll plot the curves\n)\n\n# Convert the training history to a dataframe\nhistory_frame = pd.DataFrame(history.history)\n\n# Plot training history\nhistory_frame.loc[0:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[0:, ['mae', 'val_mae']].plot()\n\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_frame['val_loss'].min()));\nprint(\"Minimum Validation MAE (mean absolute error): {:0.4f}\".format(history_frame['val_mae'].min()));","e37ca38a":"# Define model\n# Add batch normalization layers\nmodel = keras.Sequential([\n    layers.Dense(2 ** 10, activation='relu', input_shape=input_shape),\n    layers.Dropout(0.2),\n    layers.BatchNormalization(),\n    layers.Dense(2 ** 10, activation='relu'),\n    layers.Dropout(0.2),\n    layers.BatchNormalization(),\n    layers.Dense(2 ** 10, activation='relu'),\n    layers.Dropout(0.2),\n    layers.BatchNormalization(),\n    layers.Dense(2 ** 10, activation='relu'),\n    layers.Dropout(0.2),\n    layers.BatchNormalization(),\n    layers.Dense(2 ** 10, activation='relu'),\n    layers.Dropout(0.2),\n    layers.BatchNormalization(),\n    layers.Dense(1),\n])\n\n# Compile in the optimizer and loss function\nmodel.compile(\n    optimizer='adam',\n    loss='mse',\n    metrics=['mae']\n)\n\n# Fit model (and save training history)\n# (Add callbacks)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=[early_stopping, lr_schedule],\n    verbose=1, # suppress output since we'll plot the curves\n)\n\n# Convert the training history to a dataframe\nhistory_frame = pd.DataFrame(history.history)\n\n# Plot training history\nhistory_frame.loc[0:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[0:, ['mae', 'val_mae']].plot()\n\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_frame['val_loss'].min()));\nprint(\"Minimum Validation MAE (mean absolute error): {:0.4f}\".format(history_frame['val_mae'].min()));","01af12bf":"# Combine training set and validation set to train final model\nds_train = ds_train.append(ds_valid)\n\n# Execute load_data() for prediction\nX_train, X_test, y_train, y_test = load_data(ds_train, ds_test)","4bf675fb":"# Training configuration\nBATCH_SIZE = 2 ** 8\n\n# Model Configuration\nUNITS = 2 ** 10\nACTIVATION = 'relu'\nDROPOUT = 0.2","e50a561e":"# Build final model from scratch\n\ndef dense_block(units, activation, dropout_rate, l1=None, l2=None):\n    def make(inputs):\n        x = layers.Dense(units)(inputs)\n        x = layers.Activation(activation)(x)\n        x = layers.Dropout(dropout_rate)(x)\n        x = layers.BatchNormalization()(x)\n        return x\n    return make\n\n   \n# Model \ninputs = keras.Input(shape=(13,))\nx = dense_block(UNITS, ACTIVATION, DROPOUT)(inputs)\nx = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\nx = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\nx = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\nx = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\noutputs = layers.Dense(1)(x)\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n\n\n# Compile in the optimizer and loss function\nmodel.compile(\n    optimizer='adam',\n    loss='mse',\n    metrics=['mae']\n)\n\n\n# Fit model (and save training history)\nhistory = model.fit(\n    X_train, y_train,\n    batch_size=BATCH_SIZE,\n    epochs=200,\n    verbose=0,\n)\n\n\n# Making predictions from test set\npredictions = model.predict(X_test)\n\n\n# Evaluate\nmodel_score = mean_absolute_error(y_test, predictions)\nprint(\"Final model score (MAE):\", model_score)","98bf7eb7":"# Place data into DataFrame for readability\nX_test_frame = pd.DataFrame(X_test)\nX_test_frame.columns = ['fixed acidity', 'volatile acidity', 'citric acid',\n       'residual sugar', 'chlorides', 'free sulfur dioxide',\n       'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol',\n       'red', 'white']\n\nX_train_frame = pd.DataFrame(X_train)\nX_train_frame.columns = ['fixed acidity', 'volatile acidity', 'citric acid',\n       'residual sugar', 'chlorides', 'free sulfur dioxide',\n       'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol',\n       'red', 'white']","b8c715b2":"# Summarize the training set to accelerate analysis\nX_train_frame = shap.kmeans(X_train_frame.values, 25)","3ea2a2fa":"# Instantiate an explainer with the model predictions and training data (or training data summary)\nexplainer = shap.KernelExplainer(model.predict, X_train_frame)","6711dca5":"# Extract Shapley values from the explainer\n# Select test data representing red wine category\nshap_values = explainer.shap_values(X_test_frame[X_test_frame['red']==1][:400])","4e364d3a":"# Summarize the Shapley values in a plot\nplt.title('Feature impact on model output')\nshap.summary_plot(shap_values[0][:,:-2], X_test_frame[X_test_frame['red']==1][:400].iloc[:,:-2][:400])","f2484cad":"# Plot the SHAP values for one red wine sample\nINSTANCE_NUM = 99\n\nshap.initjs()\nshap.force_plot(explainer.expected_value[0], shap_values[0][INSTANCE_NUM,:-2], X_test_frame[X_test_frame['red']==1].iloc[INSTANCE_NUM,:-2])","fa881c57":"# Plot the SHAP values for multiple red wine samples\n\nINSTANCE_NUM = list(np.arange(100))\n\nshap.initjs()\nshap.force_plot(explainer.expected_value[0], shap_values[0][INSTANCE_NUM,:-2], X_test_frame[X_test_frame['red']==1].iloc[INSTANCE_NUM,:-2])","13c9e10c":"# Extract Shapley values from the explainer\n# Select test data representing white wine category\nshap_values = explainer.shap_values(X_test_frame[X_test_frame['white']==1][:400])","749ab83e":"# Summarize the Shapley values in a plot\nplt.title('Feature impact on model output')\nshap.summary_plot(shap_values[0][:,:-2], X_test_frame[X_test_frame['white']==1][:400].iloc[:,:-2][:400])","6358d37b":"# Plot the SHAP values for one white wine sample\nINSTANCE_NUM = 42\n\nshap.initjs()\nshap.force_plot(explainer.expected_value[0], shap_values[0][INSTANCE_NUM,:-2], X_test_frame[X_test_frame['white']==1].iloc[INSTANCE_NUM,:-2])","bc8f6d14":"# Plot the SHAP values for multiple white wine samples\n\nINSTANCE_NUM = list(np.arange(100))\n\nshap.initjs()\nshap.force_plot(explainer.expected_value[0], shap_values[0][INSTANCE_NUM,:-2], X_test_frame[X_test_frame['white']==1].iloc[INSTANCE_NUM,:-2])","c8ef7b6f":"# Save training dataset\nds_train.to_csv('train.csv', index=False)\nprint(\"Your training set is successfully saved!\")","65fa7715":"# Save Keras model\nmodel.save('my_model')\nprint(\"Your model is successfully saved!\")","68248408":"Now we'll increase the capacity even more (by adding another dense layer), but also add dropout layers to control overfitting. Specifically, we're adding a 20% dropout layer for after each dense layer (excluding output layer).","50436c2f":"The number of inputs a network has should be equal to the number of input feature columns in the data matrix. In this analysis, be sure not to include the target ('quality') here -- only the input features.","40995416":"## Load Data","377c79d7":"## Data Preprocessing","96673c5b":"A **dropout layer** can help correct overfitting. By randomly *dropping out* some fraction of a layer's input units every step of training, we're making it much harder for the network to learn spurious patterns in the training data (which can cause overfitting), and forces it to search for broad, general patterns instead, whose weight patterns tend to be more robust.","67abd204":"## Define Callbacks","39414ef8":"# Part 4 - Model Interpretation","07378947":"## Introduction","8bda24e4":"Curious to learn more? Want to take the model for a spin *on your own*? Go ahead and do it with this [web app](https:\/\/share.streamlit.io\/ruthgn\/wine-quality-prediction-app\/main\/wine-quality-ml-app.py)!","d99acc1a":"In the beginning of this notebook, we discussed how neural networks can automatically generate patterns for information processing and decision making. The model we've built tries to mimick the decision-making patterns of wine experts in assessing the quality of wine. Understanding how our model works, therefore, gives us the understanding to answer the question -- what makes a good quality wine *good*?","cf1cec4c":"## Red Wine","d84c5c0b":"So much about wine making remains elusive. Taste is very subjective, making it extremely challenging to predict exactly how consumers will react to a certain bottle of wine. Wine experts have created extensive forms of common language to talk about wine \u2014 *astringency*, *empyreumatic*, *minerality*, etc. \u2014 that can seem inaccesible and full of jargon to the average consumers. Nonetheless, in reality, consumers deeply appreciate being helped by these professionals as we often look for their advice. Wine guides and award labels often influence our decision to get a particular bottle of wine. Furthermore, given the increasing number of available wine references, it's even harder for consumers to take into account all possibilities. To face this human issue, a wine expert (e.g., wine merchant or a sommelier) often currates and recommends their top wines of the moment.\n\nBecause deep learning tries to imitate the workings of the human brain in processing information (and automatically generates patterns for decision making and other complicated tasks), it can be a tremendous tool to help us understand and mimick the decision-making process of the experts in this field. This notebook outlines the step-by-step process of creating a deep learning model using Tensorflow to predict the quality of wine (i.e., how well it's received and scored by *wine experts* after a blind taste test) based on its physicochemical measurements. Subsequently, we will discuss model explainability and insights using SHAP values.","7df8d59f":"To build our desired predictive model using neural networks, we require two important data preprocessing steps: variable encoding and scaling.","43c6531f":"Why is `input_shape` a Python list?\nThe data we'll use in this course will be tabular data, like in a Pandas dataframe. We'll have one input for each feature in the dataset. The features are arranged by column, so we'll always have `input_shape`=[num_columns]. The reason Keras uses a list here is to permit use of more complex datasets. Image data, for instance, might need three dimensions: [height, width, channels].","f3bf7bef":"### Encoding (Categorical Data)","dfe6e9b0":"_____\n\n\n\nHave questions or comments? Share them on the Kaggle notebook comments section!\n","15ae2fb2":"We're going to re-write our final model using Keras's *Functional API* (better flexibility than the `Sequential` method we've used previously for future modifications) and re-train it using the complete training data (`X_train` and `X_valid` combined) before making predictions on the test set.","16c2822c":"For neural networks, it's generally a good idea to put all of your data on a common scale, perhaps with something like scikit-learn's `StandardScaler` or `MinMaxScaler`. The reason is that the weights in a neural network are calculated in proportion to how large an activation the data produces. Features that tend to produce activations of very different sizes can make for unstable training behavior.","fb24394c":"In Keras, a **callback** is just a function you want run every so often while the network trains. We can simply stop the training whenever it seems the validation loss isn't decreasing anymore by using **early stopping** through a callback. When using early stopping, we can choose a large number of epochs -- more than we'll actually need.\n\nAdditionally, we've also defined a learning rate schedule. It's been found that gradually decreasing the learning rate over the course of training can improve performance (the weights \"settle in\" to a minimum). This schedule will multiply the learning rate by 0.2 if the validation loss didn't decrease after an epoch. (Note: A smaller learning rate means the network needs to see more minibatches before its weights converge to their best values.)","ab0426ce":"In the previous model, we used two representation layers before the final layer. We can try experimenting with using one or three representation layers, for example, and see how doing so impacts validation metrics. We can also change the number of units\/neurons in each layer and see how it affects the validation loss and metric","37a2050b":"Let's check out one particular instance:","46daa9c8":"## Evaluate Baseline","d7b9b2b2":"In predicting the quality of white wine, features with the highest contribution also include **alcohol (ABV)**, **free sulfur dioxide**, and **volatile acidity**. Higher amounts of alcohol (ABV) and free sulfur dioxide are preffered (and very important) qualities in white wine as they are associated with higher-scoring wines. On the other hand, higher volatile acidity is considered less favorable in white wine (and even more so in red wine).","e6975e54":"Let's add some more capacity to our network.","f78b272b":"We can also review SHAP values across the whole data set, or a slice of *n* instances as shown below:","38b4f838":"### Create Train, Validation, and Test Sets","80217f4c":"Although our training dataset comes from experts scoring wine samples in a blind taste test (it shouldn't really matter whether the color of the wine they're tasting is red or white), in practice, consumers are usually aware of the type of wine they're about to drink even before their first sip. We will first look at what physicochemical qualities make a good *red* wine, according to the experts.","06689044":"## Add Batch Normalization Layers","3173192c":"### Our Approach\n\nDeep learning models are often capable enough in detecting and focusing on the most valuable features even with very little guidance from the programmer--they are mostly used in solving high-dimensionality problems. On the other hand, this also means that they excel in cases where humans don't necessarily know **what feature representations are best for a given task**.\n\nFeature engineering and feature extraction are key (and time consuming) parts of most machine learning workflows. They are about transforming the training data and augmenting it with additional features to help machine learning algorithms perform more effectively. Deep learning allows us to take a slightly different approach. The feature engineering approach was the dominant approach in machine learning until deep learning techniques started demonstrating better recognition performance than the carefully crafted feature detectors. Deep learning shifts the burden of feature design to the underlying learning system. With deep learning, for example, it is possible start with raw data as features will be automatically created by the neural network when it learns.\n\nNote that this does not mean that data preprocessing, feature extraction, and feature engineering are totally irrelevant when one uses deep learning. In practice, most deep learning neural networks contain hard coded data processing, feature extraction, and feature engineering. They may require less of these as manual operations compared to other machine learning algorithms, but they may require (and could massively benefit from) some still. While deep learning has simplified feature engineering, the architectures of the machine learning models themselves have become increasingly complex. Most of the time, these model architectures are as specific to a given task as feature engineering used to be. In short, with deep learning, architecture engineering may just be the new feature engineering.","4e6d441b":"### Dataset","132574c6":"The gap between these curves is quite small and the validation loss never increases, so it's more likely that the network is underfitting than overfitting. Now we know it's worth experimenting with even more capacity to see if that's the case.","067de708":"## Optimize Model Architecture","3198b20b":"# Part 3 - Train Model and Evaluate Predictions","6885f90c":"Our ability to understand what is happening in a model depends on the interpretability of the particular model. There is often a tradeoff between a model\u2019s accuracy and its interpretability. Simple linear models can be straightforward and easy understand, as they directly expose the impacts of variables and coefficients. Non-linear models, including those derived by neural networks like the one the we've built, can be much more difficult to interpret. This is where SHAP (or SHapley Additive exPlanations) come to our rescue. SHAP is a Python library created by Scott Lundberg that can explain the output of many machine learning frameworks, including those that are more challenging to interpret because of their complexity and non-linearity, such as neural networks and gradient-boosted trees. \n\n[This blog post](https:\/\/towardsdatascience.com\/one-feature-attribution-method-to-supposedly-rule-them-all-shapley-values-f3e04534983d) gives you a good theoretical explanation for SHAP and is a joy to read.","18c3cee4":"Next, we'll look at what makes a good *white* wine. Is it really distinct from the qualities of a good bottle of red wine?","f1f56077":"Now the training loss continues to decrease but the gap between validation loss and training loss is starting grow bigger each time we increase the capacity of our model. This can indicate that the network may begin to overfit if we keep increasing the number of epoch to keep decreasing our loss. Overfitting is a real possibility since we are using a fairly large network -- perhaps we should create a callback to halt the training once the validation loss shows signs of increasing.","6a5bdf0a":"## Imports and Setup","7fa55f16":"Here's how we're going to split our data -- 80%, 10% and 10% for the train, validation and test sets, respectively.","a2f27b81":"## White Wine","af2a1c7b":"### Add Capacity","e1346016":"# Part 2 - Define Model","48b74b10":"In general, experts tend to identify wines with lower levels of volatile acidity (mostly from acetic acid), and higher amounts of alcohol and free sulfur dioxide as higher-quality wines. These are the physicochemical measurements winemakers should keep their eye on to really impress the experts (and win the market!). *Cheers!* \ud83c\udf77","8fbf4d44":"# BONUS: Part 5 - Deployment","8a4926af":"## Add Dropout Layers","fca89ed7":"Let's look at some relationships from the plot. Three features with the highest contribution are **volatile acidity**, **alcohol (ABV)**, and **free sulfur dioxide**. Volatile acidity is represented by predominantly red dots (indicating high feature values) on the left-side where there are negative SHAP values. This means that a higher amount of acetic acid in red wine is associated with less favorable wine. Meanwhile, low amounts of free sulfur dioxide are also contributing to negative SHAP values, meaning that low free sulfur dioxide levels in red wines are indicative of lower quality ones. The other major contributor--alcohol (ABV)--is associated with better quality wine in higher amounts.","ec8cec98":"Now that the data is ready, we want to define our neural network using the `Sequential` method. Deciding the architecture of your model should be an iterative process.\n\nLet's start with the simplest network -- a Keras implementation of a linear model. This model has low capacity. A model's **capacity** refers to the size and complexity of the patterns it is able to learn. For neural networks, this will largely be determined by how many neurons it has and how they are connected together. If it appears that your network is underfitting the data, you should try increasing its capacity. You can increase the capacity of a network either by making it wider (more units to existing layers) or by making it deeper (adding more layers). Wider networks have an easier time learning more linear relationships, while deeper networks prefer more nonlinear ones. Which is better just depends on the dataset.\n\nWe want to start simple and eventually use the validation loss as a guide for our experimentation. We've told Keras to feed the optimizer 256 rows of the training data at a time (the batch_size) and to do that 300 times all the way through the dataset (the epochs). With batch size (and learning rate, which we'll determine later), you have some control over:\n- How long it takes to train a model\n- How noisy the learning curves are\n- How small the loss becomes\n\nWe've collected these hyperparameters in the next cell to make experimentation easier:","7b05ea23":"(Complete project code and data available on [Github](https:\/\/github.com\/ruthgn\/Wine-Quality-Prediction-App)).","a8f1cf7d":"### Scaling (Numeric Data)","2cc1808a":"As explained earlier in the notebook, it's generally a good idea to put all of your data on a common scale with dealing with neural networks. Now, if it's good to normalize the data before it goes into the network, normalizing inside the network would be even better! A **batch normalization layer** performs a kind of coordinated rescaling of its inputs -- it looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also putting the data on a new scale with two trainable rescaling parameters. Batch normalization can be used at almost any point in a network.\n\nBatch normalization helps speed up optimization (though it can sometimes also help prediction performance) and models with batchnorm tend to need fewer epochs to complete training.","f49a5143":"The data set we're going to work with contains records related to red and white variants of the Portuguese *Vinho Verde* wine. It contains information from 1599 red wine samples and 4898 white wine samples. Input variables in the data set consist of the type of wine (either red or white wine) and 11 additional metrics from objective tests (e.g. acidity levels, PH values, ABV, etc.), while the target\/output variable is a numerical score based on sensory data\u2014median of at least 3 evaluations (blind test) made by wine experts. Each expert graded the wine quality between 0 (very bad) and 10 (very excellent). Due to privacy and logistic issues, there is no data about grape types, wine brand, and wine selling price.\n\nFor more details, visit the [dataset page](https:\/\/www.kaggle.com\/ruthgn\/wine-quality-data-set-red-white-wine).","4332eb7e":"# Part 1 - Preliminaries"}}