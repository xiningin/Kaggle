{"cell_type":{"5f7ccdd3":"code","503b3c7d":"code","422674df":"code","d11ff9d9":"code","01bcdcab":"code","d2ec3307":"code","621c565f":"code","617b1d0b":"code","b496b85f":"code","cf80c43e":"code","877d8447":"code","573add01":"code","bd2e2138":"code","28d6ccf9":"code","ca2d85a6":"code","83022ea8":"code","4c250938":"code","8cdd413b":"code","0f697349":"code","f95e5e61":"code","c9882f4e":"code","6762aec6":"code","2cecfad3":"code","1470f5d4":"code","067c727a":"code","bfbb0c2e":"code","d9711ac2":"code","db26cdfa":"code","6d4afbab":"code","dc9faf66":"code","f2b9dfbf":"code","c8c902b7":"code","89fc67db":"code","4b5ab6c2":"code","bfef9a07":"code","e89005df":"code","3620012e":"code","63867bd3":"code","48685341":"code","201ad2ad":"code","99934587":"markdown","a338ac40":"markdown","d13392e2":"markdown","25d81344":"markdown","210b17bc":"markdown","a3fb0ecf":"markdown","cce56c62":"markdown","4d249806":"markdown","abf356c2":"markdown","2db3cf47":"markdown","5f00e565":"markdown","ca7084ec":"markdown","7d8eb74f":"markdown","823d58a8":"markdown","ce47e3cc":"markdown","4b367b9d":"markdown","b9e057dc":"markdown","a97d6531":"markdown","f27f0f8b":"markdown","06113ce7":"markdown","7008d436":"markdown","58902494":"markdown","e4a1954a":"markdown","6e318617":"markdown","1342fff5":"markdown","3caffe36":"markdown","4e1f707f":"markdown","8277c22c":"markdown","6057f0f3":"markdown","8f5eb0fc":"markdown","11d2ccc1":"markdown","056c55b0":"markdown","a7a26e14":"markdown","94098411":"markdown","938f0110":"markdown","243fc964":"markdown","b2f93c6f":"markdown","5ee8d817":"markdown","0feb776d":"markdown","16bee40e":"markdown","43fb2546":"markdown","13c43fa7":"markdown","71307877":"markdown","5fc717ab":"markdown","d3ef9ccc":"markdown","b28a87b6":"markdown","78faeed1":"markdown","6eccece2":"markdown","e01faf94":"markdown","3d624192":"markdown","d0dba146":"markdown","18884a7b":"markdown","802aa37d":"markdown","872b25db":"markdown","32e9dd47":"markdown","0b44df3a":"markdown","9defb9ea":"markdown"},"source":{"5f7ccdd3":"import pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as mno\nfrom sklearn import linear_model\n%matplotlib inline\n\ndata = pd.read_csv('..\/input\/melbourne-housing-market\/Melbourne_housing_FULL.csv')\ndata_full=data","503b3c7d":"data.head()","422674df":"data.info()","d11ff9d9":"data.describe()","01bcdcab":"mno.matrix(data, figsize = (20, 6))","d2ec3307":"for i in (data.columns):\n    # count number of rows with missing values\n    n_miss = data[i].isnull().sum()\n    perc = n_miss \/ data.shape[0] * 100\n    \n    print('> {}, Missing: {}  total {} %' .format (i, n_miss, perc))","621c565f":"data.dropna(inplace=True)\ndata","617b1d0b":"#dropna function in Pandas removes all the rows with missing values\n\nmno.matrix(data, figsize = (20, 6))","b496b85f":"#Putting axis=1 removes the columns with missing values\ndata.dropna(inplace=True, axis=1)\nmno.matrix(data, figsize = (20, 6))","cf80c43e":"data_full['BuildingArea'].fillna(data_full['BuildingArea'].mean(), inplace=True)\ndata_full.head(5)","877d8447":"data = pd.read_csv('..\/input\/melbourne-housing-market\/Melbourne_housing_FULL.csv')\ndata.head()","573add01":"data.head()","bd2e2138":"data['Price']=data['Price'].fillna(1480000.0)\ndata.head()","28d6ccf9":"from sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\ndata_selected = data[['Rooms','Bedroom2', 'Bathroom', 'Car', 'Landsize','Price', 'Lattitude', 'Longtitude', 'Propertycount', 'BuildingArea']]\ndata_with_imputed_values = my_imputer.fit_transform(data_selected)\ndf=pd.DataFrame(data_with_imputed_values)\ndf.head()","ca2d85a6":"df.columns=data_selected.columns\ndf","83022ea8":"# make copy to avoid changing original data (when Imputing)\nnew_data = data_selected.copy()\n\n\n# Imputation\nmy_imputer = SimpleImputer()\nnew_data = pd.DataFrame(my_imputer.fit_transform(new_data))\nnew_data.columns = data_selected.columns","4c250938":"colormap = plt.cm.RdBu\nplt.figure(figsize=(32,10))\nplt.title('Correlation of Features', y=1.05, size=15)\nsns.heatmap(data_selected.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","8cdd413b":"df = pd.read_csv(\"..\/input\/diabetes\/diabetes.csv\")\ndf.head(3)","0f697349":"df.info()","f95e5e61":"mno.matrix(df, figsize = (20, 6))","c9882f4e":"df.describe()","6762aec6":"df.loc[df[\"Glucose\"] == 0.0, \"Glucose\"] = np.NAN\ndf.loc[df[\"BloodPressure\"] == 0.0, \"BloodPressure\"] = np.NAN\ndf.loc[df[\"SkinThickness\"] == 0.0, \"SkinThickness\"] = np.NAN\ndf.loc[df[\"Insulin\"] == 0.0, \"Insulin\"] = np.NAN\ndf.loc[df[\"BMI\"] == 0.0, \"BMI\"] = np.NAN\n\ndf.isnull().sum()[1:6]","2cecfad3":"mno.matrix(df, figsize = (20, 6))","1470f5d4":"missing_columns = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]","067c727a":"def random_imputation(df, feature):\n    number_missing = df[feature].isnull().sum()\n    observed_values = df.loc[df[feature].notnull(), feature]\n    df.loc[df[feature].isnull(), feature + '_imp'] = np.random.choice(observed_values, number_missing, replace = True)\n    return df","bfbb0c2e":"for feature in missing_columns:\n    df[feature + '_imp'] = df[feature]\n    df = random_imputation(df, feature)\n","d9711ac2":"deter_data = pd.DataFrame(columns = [\"Det\" + name for name in missing_columns])\n\nfor feature in missing_columns:\n        \n    deter_data[\"Det\" + feature] = df[feature + \"_imp\"]\n    parameters = list(set(df.columns) - set(missing_columns) - {feature + '_imp'})\n    \n    #Create a Linear Regression model to estimate the missing data\n    model = linear_model.LinearRegression()\n    model.fit(X = df[parameters], y = df[feature + '_imp'])\n    \n    #observe that I preserve the index of the missing data from the original dataframe\n    deter_data.loc[df[feature].isnull(), \"Det\" + feature] = model.predict(df[parameters])[df[feature].isnull()]","db26cdfa":"mno.matrix(deter_data, figsize = (20,5))","6d4afbab":"sns.set()\nfig, axes = plt.subplots(nrows = 2, ncols = 2)\nfig.set_size_inches(8, 8)\n\nfor index, variable in enumerate([\"Insulin\", \"SkinThickness\"]):\n    sns.distplot(df[variable].dropna(), kde = False, ax = axes[index, 0])\n    sns.distplot(deter_data[\"Det\" + variable], kde = False, ax = axes[index, 0], color = 'red')\n    sns.boxplot(data = pd.concat([df[variable], deter_data[\"Det\" + variable]], axis = 1),ax = axes[index, 1])\nplt.tight_layout()","dc9faf66":"pd.concat([df[[\"Insulin\", \"SkinThickness\"]], deter_data[[\"DetInsulin\", \"DetSkinThickness\"]]], axis = 1).describe().T","f2b9dfbf":"random_data = pd.DataFrame(columns = [\"Ran\" + name for name in missing_columns])\n\nfor feature in missing_columns:\n        \n    random_data[\"Ran\" + feature] = df[feature + '_imp']\n    parameters = list(set(df.columns) - set(missing_columns) - {feature + '_imp'})\n    \n    model = linear_model.LinearRegression()\n    model.fit(X = df[parameters], y = df[feature + '_imp'])\n    \n    #Standard Error of the regression estimates is equal to std() of the errors of each estimates\n    predict = model.predict(df[parameters])\n    std_error = (predict[df[feature].notnull()] - df.loc[df[feature].notnull(), feature + '_imp']).std()\n    \n    #observe that I preserve the index of the missing data from the original dataframe\n    random_predict = np.random.normal(size = df[feature].shape[0], \n                                      loc = predict, \n                                      scale = std_error)\n    random_data.loc[(df[feature].isnull()) & (random_predict > 0), \"Ran\" + feature] = random_predict[(df[feature].isnull()) & \n                                                                            (random_predict > 0)]","c8c902b7":"sns.set()\nfig, axes = plt.subplots(nrows = 2, ncols = 2)\nfig.set_size_inches(8, 8)\n\nfor index, variable in enumerate([\"Insulin\", \"SkinThickness\"]):\n    sns.distplot(df[variable].dropna(), kde = False, ax = axes[index, 0])\n    sns.distplot(random_data[\"Ran\" + variable], kde = False, ax = axes[index, 0], color = 'red')\n    axes[index, 0].set(xlabel = variable + \" \/ \" + variable + '_imp')\n    \n    sns.boxplot(data = pd.concat([df[variable], random_data[\"Ran\" + variable]], axis = 1),\n                ax = axes[index, 1])\n    \n    plt.tight_layout()\n","89fc67db":"pd.concat([df[[\"Insulin\", \"SkinThickness\"]], random_data[[\"RanInsulin\", \"RanSkinThickness\"]]], axis = 1).describe().T","4b5ab6c2":"from sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=5)\ndf = pd.DataFrame(imputer.fit_transform(df),columns = df.columns)","bfef9a07":"df.isna().any()","e89005df":"!pip install miceforest","3620012e":"import miceforest as mf\nfrom sklearn.datasets import load_iris\nimport pandas as pd\n\n# Load and format data\niris = pd.concat(load_iris(as_frame=True,return_X_y=True),axis=1)\niris.rename(columns = {'target':'species'}, inplace = True)\niris['species'] = iris['species'].astype('category')\niris","63867bd3":"\n\n# Introduce missing values\niris_amp = mf.ampute_data(iris,perc=0.25,random_state=1991)","48685341":"# Create kernels. \nkernel = mf.MultipleImputedKernel(\n  data=iris_amp,\n  save_all_iterations=True,\n  random_state=2000\n)\n# Run the MICE algorithm for 3 iterations on each of the datasets\nkernel.mice(5,verbose=True)","201ad2ad":"kernel.plot_correlations(wspace=0.4,hspace=0.5)","99934587":"\nMultiple Imputation by Chained Equations, also called \u201cfully conditional specification\u201d, is defined as such:\n\n![](https:\/\/miro.medium.com\/max\/609\/1*PGMV2MkOnIl7lV5hm7kDmg.png)\n\n\nThis process is repeated for the desired number of datasets. The method mentioned on line 8, mean matching, is used to produce imputations that behave more like the original data. This idea is explored in depth in Stef van Buuren\u2019s online book. A reproducible example of the effects on mean matching can also be found on the miceforest Github page.\nMultiple iterations are sometimes required for the imputations to converge. There are several things that affect how many iterations are required to achieve convergence such as the type of missing data, the information density in the dataset, and the model used to impute the data.\n\n**MI involves the following three basic steps**\n\n**1. Imputation:** The missing data are filled in with estimated values and a complete data set is created. This process of imputation is repeated m times and m datasets are created.\n\n**2. Analysis:** Each of the m complete data sets is then analysed using a statistical method of interest (e.g. linear regression).\n\n**3. Pooling:** The parameter estimates (e.g. coefficients and standard errors) obtained from each analysed data set are then averaged to get a single point estimate.","a338ac40":"# 2. KNN Imputer","d13392e2":"A major disadvantage in this method is that we reduce the inherent variability in the imputed variable. In other words, since we substitute the missing data with regression outputs, the predicted values lie along the regression hyperplane where the variable would have actually contained some noise\/bias.\n\nWe can visualize the above fact in a number of ways. First one is plotting histograms for both the incomplete data and the complete data in which we can observe that the plot of the completed data is taller and narrower when compared to that of the incomplete data. In other words, the complete data has a lesser standard deviation (thus lesser variability) than the incomplete data.\n\nAnother method would be plotting a boxplot in which we can observe that the IQ Range is pretty compressed for the complete data when compared to that in the incomplete data.","25d81344":"Consider the above diagram that represents the working of kNN. In this case, the oval area represents the neighboring points of the green squared data point. We use a measure of distance to identify the neighbors. For a detailed introduction to kNN and distance measure calculations, ","210b17bc":"**works only with Numerical features**","a3fb0ecf":"A simple df.info() is ran for a quick and abstract check for missing data in any of the variables. This lists the number of non-null values and the datatype of each variable.","cce56c62":"Both the approaches have their own advantages and disadvantages and you will have to analyze for your use case to decide what needs to be done. If we drop the rows our total number of data points to train our model will go down which can reduce the model performance. Do this only if you have large number of training examples and the rows with missing data are not very high in number. Dropping the column altogether will remove a feature from our model i.e the model predictions will be independent of the building area. Sometimes you can drop variables or columns if the data is missing for more than 60% observations but only if that variable is insignificant. In general dropping data is not a good approach in most cases since you loose a lot of potentially useful information. Lets look at a better approach for dealing with missing data.","4d249806":"None of the variables seem to have any missing value based on our above observation. But there's more to it than what meets the eye. df.describe() which gives the Five Number Summary would show that some variables have 0.0 as their minimum value which would be meaningless in their case. Plasma glucose concentration, Diastolic blood pressure, Triceps skinfold thickness, 2-Hour serum insulin and Body mass index cannot be zero.\n\nImagine BMI to be zero. That would be a disaster!\n\nOn the contrary, Pregnencies can be zero because either that person is a female who has not had a baby yet.","abf356c2":"**MCAR:** Missing Completely At Random. It is the highest level of randomness. This means that the missing values in any features are not dependent on any other features values. This is the desirable scenario in case of missing data.\n\n**MAR:** Missing At Random. This means that the missing values in any feature are dependent on the values of other features.\n\n**MNAR:** Missing Not At Random. Missing not at random data is a more serious issue and in this case, it might be wise to check the data gathering process further and try to understand why the information is missing. For instance, if most of the people in a survey did not answer a certain question, why did they do that? Was the question unclear?","2db3cf47":"![](https:\/\/lh5.googleusercontent.com\/nPFpe1oPKYB1xUwU4GGxCrAEpi3pNBDckj0Jza5cMFGkA-tjMZAWQzEtqK1DJXJt0ZuOFcCoVymyIUzVEyBl_8bWRhFWA9k7x3AHiMgFxjXYaHzkx7qIQR24u3_p8TJkw6IMBj8i)","5f00e565":"A slightly better approach towards handling missing data is Imputation. Imputation means to replace or fill the missing data with some value.\nImputation fills in the missing value with some number. The imputed value won't be exactly right in most cases, but it usually gives more accurate models than dropping the column entirely.\n","ca7084ec":"**Types of missing values**\nWe can classify the missing values in different types. Each type of missing value require slightly different handling. The main types are \u2014\n\n1. Missing completely at Random (MCAR)\n\n1. Missing at Random (MAR)\n\n1. Missing Not at Random (MNAR)\n\n","7d8eb74f":"We see that our variable of interest BuildingArea is correlated to Rooms, Bedroom2, Bathroom, Car, Landsize. We will use these variables to predict the missing values of BuilaingArea.","823d58a8":"![](https:\/\/lh3.googleusercontent.com\/XoAmDPvK6VdXeR1Tvbre0GGPrVhjt0lKfsctH_U_DO4nTLMEQzFe0Cavzs50kqHdvBr483UA5HxJEOptHxBYtyRY5FTON28Yj4Q70oCeh-4Opk5KXojX5BwqUAEJRn2xiHtLoRdU)","ce47e3cc":"# **1 Regression to impute missing data**\n\nWhen we have multiple variables with missing values, we can't just directly use Regression Imputation to impute one of them as the predictors contain missing data themselves. But then, how can we impute one variable without imputing another?\n\nWe can avoid this Catch-22 situation by initially imputing all the variables with missing values using some trivial methods like Simple Random Imputation (we impute the missing data with random observed values of the variable) which is later followed by Regression Imputation of each of the variables iteratively.\nit also has two type:\n\n**1. Deterministic Regression Imputation**\n\n**2. Stochastic Regression Imputation**","4b367b9d":"We then plot a correlation matrix to find out which variables are correlated to each other which we will use as independent predictor variables for predicting missing values.","b9e057dc":"KNN Imputer was first supported by Scikit-Learn in December 2019 when it released its version 0.22. This imputer utilizes the k-Nearest Neighbors method to replace the missing values in the datasets with the mean value from the parameter \u2018n_neighbors\u2019 nearest neighbors found in the training set. By default, it uses a Euclidean distance metric to impute the missing values.\nTo see this imputer in action, we will import it from Scikit-Learn\u2019s impute package -","a97d6531":"Each plot represents the correlation of imputed values between 2 of the datasets. Every combination of datasets is included in the graph. If the correlation between imputed values is low, it means we did not impute the values with much confidence. It looks like our models all pretty much agreed on the imputations for petal length and petal width. If we ran more iterations, we might be able to get better results for sepal length and sepal width as well.","f27f0f8b":"# **Why not impute with a common value?**\n\nThen, why not impute the missing data with the Measure of Central Tendency of the variable? That does sound like a safe approach (and also pretty easy to implement).\n\nApparently not.\n\nWhen we replace the missing data with some common value we might under(over)estimate it. In other words, we add some bias to our estimation. For example, a person who earns just enough to meet his daily needs might not be comfortable in mentioning his salary, and thus the value for the variable salary would be missing for such a person. However, if we impute it with the mean value of the variable, we are overestimating that person's salary and thus introducing bias in our analysis.","06113ce7":"do the random imputation for training features","7008d436":"An issue of concern about is that, Regression Imputation might not serve as the best method when a variable is missing majority of it's data, as in case of insulin. In these cases we have to use more powerful approaches as Maximum Likelihood Imputation and Multple Imputaton.\n\nNotes\n* Regression Imputation assumes that the data is Missing At Random, more about it can be found in the refereneces below.\n* For a better Regression model, we might have to follow different Data Transformation methods depending on our data.\n* Do observe that we have included Outcome as one of our predictors eventhough it is caused by the other variables under scrutiny.\n* This kernel does not describe the best method for many cases, rather it justs the demonstrates Regression Imputation as one of the methods.","58902494":"# **1.1. Deterministic Regression Imputation**\nIn Deterministic Regression Imputation, we replace the missing data with the values predicted in our regression model and repeat this process for each variable.","e4a1954a":"Thus, for feasibility of further analysis, we replace all these \"missing\" data with nan and calculate their number. As you can observe, there's enough evidence of significant missingness in those variables.","6e318617":"the choice of k to impute the missing values using the kNN algorithm can be a bone of contention. Furthermore, research suggests that it is imperative to test the model using cross-validation after performing imputation with different values of k. Although the imputation of missing values is a continuously evolving field of study, kNN act as a simple and effective strategy.","1342fff5":"lets define a function random_imputation that replaces the missing values with some random observed values of the variable. The method is repeated for all the variables containing missing values, after which they serve as parameters in the regression model to estimate other variable values.\n\nSimple Random Imputation is one of the crude methods since it ignores all the other available data and thus it's very rarely used. But it serves as a good starting point for regression imputation.","3caffe36":"Univariate methods used for missing value imputation are simplistic ways of estimating the value and may not provide an accurate picture always. For example, let us say we have variables related to the density of cars on road and levels of pollutants in the air and there are few observations that are missing for the level of pollutants, imputing the level of pollutants by mean\/median level of pollutants may not necessarily be an appropriate strategy.\n\nIn such scenarios, algorithms like k-Nearest Neighbors (kNN) can help to impute the values of missing data. Sociologists and community researchers suggest that human beings live in a community because neighbors generate a feeling of security and safety, attachment to community, and relationships that bring out a community identity through participation in various activities.\n\nA similar imputation methodology that works on data is k-Nearest Neighbours (kNN) that identifies the neighboring points through a measure of distance and the missing values can be estimated using completed values of neighboring observations.","4e1f707f":"Now we use the sklearn to KNN imputer which uses the equalidiean distance for imputation","8277c22c":"**Regression Methods**\n\nThe variables with missing values are treated as dependent variables and variables with complete cases are taken as predictors or independent variables. The independent variables are used to fit a linear equation for the observed values of the dependent variable. This equation is then used to predict values for the missing data points.\n\nThe disadvantage of this method is that the identified independent variables would have a high correlation with the dependent variable by virtue of selection. This would result in fitting the missing values a little too well and reducing the uncertainty about that value. Also, this assumes that relationship is linear which might not be the case in reality.\n\n**K-Nearest Neighbour Imputation (KNN)**\n\nThis method uses k-nearest neighbour algorithms to estimate and replace missing data. The k-neighbours are chosen using some distance measure and their average is used as an imputation estimate. This could be used for estimating both qualitative attributes (the most frequent value among the k nearest neighbours) and quantitative attributes (the mean of the k nearest neighbours).\n\nOne should try different values of k with different distance metrics to find the best match. The distance metric could be chosen based on the properties of the data. For example, Euclidean is a good distance measure to use if the input variables are similar in type (e.g. all measured widths and heights). Manhattan distance is a good measure to use if the input variables are not similar in type (such as age, gender, height, etc.).\n\nThe advantage of using KNN is that it is simple to implement. But it suffers from the curse of dimensionality. It works well for a small number of variables but becomes computationally inefficient when the number of variables is large.\n\n**Multiple Imputation**\n\nMultiple imputations is an iterative method in which multiple values are estimated for the missing data points using the distribution of the observed data. The advantage of this method is that it reflects the uncertainty around the true value and returns unbiased estimates.\n\n**MI involves the following three basic steps**\n1. Imputation: The missing data are filled in with estimated values and a complete data set is created. This process of imputation is repeated m times and m datasets are created.\n\n2. Analysis: Each of the m complete data sets is then analysed using a statistical method of interest (e.g. linear regression).\n\n3. Pooling: The parameter estimates (e.g. coefficients and standard errors) obtained from each analysed data set are then averaged to get a single point estimate.\n\n","6057f0f3":"You can already see that the BuildingArea column has the first entry as NaN which means the value is not available there. We can go ahead and drop all the rows which does not have the BuildingArea value or we can drop the BuildingArea column altogether.\n","8f5eb0fc":"# Handle Missing Value","11d2ccc1":"# **1. Constant value Imputation**","056c55b0":"The points with the shortest distance based on Euclidean distances are considered to be the nearest neighbors. For example, 1-nearest neighbor to Point A is point B. For point B, the 1-nearest neighbor is point C.\n\nIn the presence of missing coordinates, the Euclidean distance is calculated by ignoring the missing values and scaling up the weight of the non-missing coordinates.","a7a26e14":"![](https:\/\/lh3.googleusercontent.com\/kgxBFV7v73pLLDLPa3hAQwkQKseBwKPI_CA5ISMtPdh9lIOWzIy4Qop4BuZ_WkT0_qr106SVpuC60hw0mqs6_aRbsihrF0kxu_a5PJa77EBOT5uqmMhLcEJFKE7PSKIntpgerVfX)","94098411":"# An Extension To Imputation\n\nImputation is the standard approach, and it usually works well. However, imputed values may by systematically above or below their actual values (which weren't collected in the dataset). Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing. Here's how it might look:","938f0110":"Real-world data would certainly have missing values. This could be due to many reasons such as data entry errors or data collection problems. Irrespective of the reasons, it is important to handle missing data because any statistical results based on a dataset with non-random missing values could be biased. Also, many ML algorithms do not support data with missing values.\n\nAfter starting a machine learning or a data science project you begin your EDA or exploratory data analysis hoping to find interesting patterns and insights about the data before you go on to extract features and build your model. But it is very common to find a lot of values missing in your data. These missing values arise due to many factors not in your direct control. Sometimes due to the ways the data was captured. In some cases the values are not available at all for observation. Nevertheless you will need to handle those missing values before you move further. Lets look at the ways to do that. To be honest there isn\u2019t a single standard technique or a general solution to handle missing values but there are a few ways which you can use depending upon your use case to help you deal with missing values in your data. But before that lets see what are the types of missing data.","243fc964":"Pima Indians Diabetes dataset is used for our analysis. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. You can read more about the dataset in the Data section of this kernel.","b2f93c6f":"We have some better options\nImputaion by Prediction :\n1. Regression Method\n2. Clustering Method\n3. Multiple Imputer","5ee8d817":"What we have done is created 5 separate datasets with different imputed values. We can never be sure what the original data was, but if our different datasets all come up with similar imputed values, we can say that we are confident in our imputations. Let\u2019s take a look at the correlations of the imputed values between datasets:","0feb776d":"A better way of realizing this missingness is by visualizing the same using missingno package by drawing a nullity matrix. And as we can observe, SkinThickness and Insulin have a large amount of their data missing whose number is mentioned above. To keep our kernel short, I would consider only these variables for further visualizations.\n\nYou can learn more about missingno package in the link mentioned in References below","16bee40e":"There are lot of ways to impute the data.\n\n1. A constant value that belongs to the set of possible values of that variable, such as 0, distinct from all other values\n1. A mean, median or mode value for the column\n1. A value estimated by another predictive model\n1. Multiple Imputation","43fb2546":"# 3. Multiple Imputation (The MICE Algorithm)\n","13c43fa7":"**Distance calculation in the presence of missing values**\nLet\u2019s look at an example to understand this. Consider a pair of observations in a two-dimensional space (2,0), (2,2), (3,3). The  graphical representation of these points are shown below:\n![](https:\/\/lh4.googleusercontent.com\/iIecYWAJ08ZMkqbkTEYFOJvVwJvVk6kst80v2QQh0QIUMaphrNeEnlGwP1H8gupSnCM2X2YEMsnyecVstXhUkAHnOMH03zMdNmvSIcotz36ApnKa_SE3v9-BQnPRN_76uslOMH1i)","71307877":"**1. Deletion**\n\nIn this method, cases which have missing values for one or more features are deleted. If the cases having missing values are small in number, it is better to drop them. Though this is an easy approach, it might lead to a significant decrease in the sample size. Also, the data may not always be missing completely at random. This may lead to biased estimation of parameters.\n\n**2. Imputation**\n\nImputation is the process of substituting the missing data by some statistical methods. Imputation is useful in the sense that it preserves all cases by replacing missing data with an estimated value based on other available information. But imputation methods should be used carefully as most of them introduce a large amount of bias and reduce variance in the dataset.","5fc717ab":"# A Better Option: Imputation","d3ef9ccc":"**What to do with the missing values?** \n\nNow that we have identified the missing values in our data, next we should check the extent of the missing values to decide the further course of action.\n\nIgnore the missing values\n\nMissing data under 10% for an individual case or observation can generally be ignored, except when the missing data is a MAR or MNAR.\nThe number of complete cases i.e. observation with no missing data must be sufficient for the selected analysis technique if the incomplete cases are not considered.","b28a87b6":"For example, the Euclidean distances between two points (3, NA, 5) and (1, 0, 0) is:","78faeed1":"**Pros:**\n\nComplete removal of data with missing values results in robust and highly accurate model\n\nDeleting a particular row or a column with no specific information is better, since it does not have a high weightage\n\n**Cons:**\n\nLoss of information and data\n\nWorks poorly if the percentage of missing values is high (say 30%), compared to the whole dataset","6eccece2":"# 2. Replace with mean value","e01faf94":"The predicted values from the model are inserted into the original dataframe. It theoretically provides good estimates for missing values. However, there are several disadvantages of this model which tend to outweigh the advantages. The replaced values are completely determined by a model applied to other variables and they tend to fit together too well, in other words, they contain no error. One must also assume that there is a linear relationship between the variables used in the regression equation which may not be the case always.","3d624192":"**Example-**\nSuppose, you run out of stock of necessary food items in your house, and due to the lockdown none of the nearby stores are open. Therefore, you ask your neighbors for help and you will end up cooking whatever they supply to you. This is an example of imputation from a 1-nearest neighbor (taking the help of your closest neighbor).\n\nInstead, if you identify 3 neighbors from whom you ask for help and choose to combine the items supplied by 3 of your nearest neighbors, that is an example of imputation from 3-nearest neighbors. Similarly, missing values in datasets can be imputed with the help of values of observations from the k-Nearest Neighbours in your dataset. Neighboring points for a dataset are identified by certain distance metrics, generally euclidean distance.","d0dba146":"fill Nan with constanct value (here constant value is 1480000)","18884a7b":"**Techniques of dealing with missing data**\n\nThere are a few techniques which can help you deal with missing values in your data set \u2014\n1. Drop missing values\/columns\/rows\n1. Imputation","802aa37d":"Also you can find correlation matrix b\/w features so that you can predict & replace the value on the basis of corrilated features","872b25db":"When we introduce this Gaussian noise we may end up imputing some negative values for the missing data due to the spread of the distibution for a particular pair of mean and standard deviation. But, as per our discussion earlier, there might be some variable whose values can never be zero. For example, a negative value for Insulin concentrations would be meaningless.\n\nWe can avoid this situation by retaining the values introduced by simple random imputation which is discussed above. This apparently reduces the variability that we introduce, but it's something we have to deal with, especially in case of these variables whose values are restricted to ceratin parts of the real number line.","32e9dd47":"We can observe from the plots above that we have introduced some degree of variability into the variables and retained the native distribution as well.","0b44df3a":"![](https:\/\/lh5.googleusercontent.com\/Ifp1O-KCM1gG0aQXXJbT3RLtTNC1eICgKhFZ89p25jzvujerKmEcn9nFXWVls16oZ-aWxBu7k4iockd3ohWyFh_jga8589F6Ra8h2lLc959pBClCGdGPoZhx0kbYnOlD4cRKK5fM)","9defb9ea":"# **1.2. Stochastic Regression Imputation**\n\nTo add uncertainity back to the imputed variable values, we can add some normally distributed noise with a mean of zero and the variance equal to the standard error of regression estimates . This method is called as Random Imputation or Stochastic Regression Imputation\n\n"}}