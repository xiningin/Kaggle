{"cell_type":{"cf2279b7":"code","e09c2800":"code","f5216fc6":"code","4638f5a2":"code","f2ab693e":"code","a4f6e411":"code","006650d8":"code","5c943801":"code","50e47a09":"code","cbb2026f":"code","dfdf7fdd":"code","d2d80f86":"code","8e044677":"code","9682a6a0":"code","00c450a9":"code","f6bd1b7b":"code","86c67249":"code","c6f21f24":"code","0b55a1a5":"code","3403849d":"code","384d5f2b":"code","e9c8fd0a":"code","ce2eabec":"code","4a05430b":"code","9a3fd26d":"code","cb099045":"code","ec5078cd":"code","73ca1636":"code","7fb5af3c":"code","f5c0a88f":"code","b6cb7ebf":"code","5f68700f":"code","cf4d3550":"code","534f9905":"markdown","16c5ec59":"markdown","8320a825":"markdown","9f6599cd":"markdown","f22f2529":"markdown","a09ce2e9":"markdown","5391cda2":"markdown","ed7ec916":"markdown","46b4cf30":"markdown","d77ed583":"markdown","3ca95561":"markdown","9eb7c6e1":"markdown","dd641d89":"markdown","672efbbe":"markdown"},"source":{"cf2279b7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('fivethirtyeight')\n\nimport datetime\n\nimport gc\nimport re\nimport os\nfrom PIL import Image\nfrom collections import Counter\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\n\nstop = stopwords.words(\"english\")","e09c2800":"bulleting = os.listdir(\"..\/input\/data-science-for-good-city-of-los-angeles\/cityofla\/CityofLA\/Job Bulletins\")","f5216fc6":"#Regex compile\n\n#Salary\n#salary = re.compile(\"\\$(\\d+)\")\nsalary = re.compile(\"\\$(\\d{4,8})\")\n\n#Opening date of the position\nopendate=re.compile(r'(Open [D,d]ate:)(\\s+)(\\d+\\-\\d+-\\d\\d)')\n\n#Requirements of the position\nreq_one = re.compile(\"REQUIREMENTS?(.*?)NOTES\")\n\n#Duties of the job\nduty = re.compile(\"DUTIES?(.*)(REQ[A-Z])\")\n\n#End date of the application\nend_date_reg = re.compile(\"(JANUARY|FEBRUARY|MARCH|APRIL|MAY|JUNE|JULY|AUGUST|SEPTEMBER|OCTOBER|NOVEMBER|DECEMBER)\\s(\\d{1,2}\\s\\d{4})\")\n\n#Extracts the selection process steps (Test\/Essay\/Interviw etc.)\n\n#First part extracts the general area\ntest_area = re.compile(\"SELECTION(.*)(NO[A-Z])\")\n#Second part extracts keywords\ntests = re.compile(\"Test|Essay|Interview|Questionnaire\",re.IGNORECASE)\n\ngc.collect()","4638f5a2":"def data_extractor(num_files,df_one,df_two):\n\n    \"\"\"\n    Function for extracting data from text files and implementing them in a tabular format\n    \"\"\"\n\n    for number in range(0, num_files):\n        with open(\"..\/input\/data-science-for-good-city-of-los-angeles\/cityofla\/CityofLA\/Job Bulletins\/\" + bulleting[number],encoding = \"ISO-8859-1\") as text_data:\n\n            #Remove tabs,new lines and commas with blank space\n            data=text_data.read().replace('\\t','')\n            data= data.replace('\\n',' ')\n            data= data.replace(',','')\n\n            #Extract file names\n\n            try:\n                job_name = re.search(\"^(.*?)\\d{4}\",bulleting[number]).group(1)\n\n            except:\n                job_name = np.nan\n\n            #Salary extraction minimum and max offered\n            sal=re.findall(salary,data)\n            try:\n                minimum_salary = min(sal)\n                maximum_salary = max(sal)\n            except:\n                minimum_salary = np.nan\n                maximum_salary = np.nan\n\n            #Extracts the opening date of the position\n            try:\n                open_date=re.search(opendate,data).group(3)\n            except:\n                open_date = np.nan\n\n            #Extracts a large portion of the sting describing the req for the job\n\n            try:\n                requi=re.search(req_one,data)[1]\n            #Cleans part of the sting from Min Qual, Process and special char\n                requi = re.sub(\"MI[A-Z]*\\sQUAL[A-Z]{7,9}S?\",\"\",requi)\n                requi = re.sub(\"PRO[A-Z]*\",\"\",requi)\n                requi = re.sub(r\"[-()\\\"#\/@;:<>{}`+=~|]\", \"\", requi)\n            except:\n                requi=np.nan     \n            #Extracts a large portion desc the duties of the position\n            try:\n                duties=re.search(duty,data)[1]\n            except:\n                duties = np.nan\n            #End date of the application\n            try:\n                endd=re.search(end_date_reg,data)[0]\n            except:\n                endd = np.nan\n            #Extract area, find keywords,transform all of them to lowercase and drop duplicates\n            try:\n                string=re.findall(tests,re.search(test_area,data).group(0))\n                string=list(pd.Series(string).str.lower())\n                string=np.unique(string)\n            except:\n                string = np.nan\n\n            df_one = df_one.append({\"file_name\":bulleting[number],\"minimum_salary\":minimum_salary,\"maximum_salary\":maximum_salary,\"open_date\":open_date,\"requirements\":requi,\"job_duties\":duties,\"application_end_date\":endd,\"selection\":string,\"job_name\":job_name}, ignore_index=True)\n\n            df_two = df_two.append({\"file_name\":bulleting[number],\"text_data\":data},ignore_index=True)\n\n    return df_one, df_two","f2ab693e":"df = pd.DataFrame()\ndf_file = pd.DataFrame()\ndf, df_file = data_extractor(len(bulleting),df,df_file)\n\ngc.collect()","a4f6e411":"#Converting columns to datetime\n\ndf[\"application_end_date\"] = pd.to_datetime(df[\"application_end_date\"])\ndf[\"open_date\"] = pd.to_datetime(df[\"open_date\"])\n\ngc.collect()","006650d8":"###### Wordcloud ######\n\nall_jobs_text = \" \".join(df_file[\"text_data\"])\nimage_mask = np.array(Image.open(\"..\/input\/imgs-los\/images\/cityoflos14.jpg\"))\n\nword_cloud = WordCloud(background_color=\"black\",max_words=5000,max_font_size=30, mask=image_mask, random_state=3)\nword_cloud.generate(all_jobs_text)\n\nimage_colours = ImageColorGenerator(image_mask)\n\n\nax, fig = plt.subplots(figsize=(8,8))\nplt.imshow(word_cloud.recolor(color_func = image_colours),interpolation=\"bilinear\")\n\nplt.axis(\"off\")\n\ngc.collect()","5c943801":"#df.dropna(subset=[\"open_date\"],inplace=True)\ndf.set_index(df[\"open_date\"], inplace=True)\n\n#Set numeric types\n\ndf[\"maximum_salary\"] = pd.to_numeric(df[\"maximum_salary\"])\ndf[\"minimum_salary\"] = pd.to_numeric(df[\"minimum_salary\"])\n\ngc.collect()","50e47a09":"#Resample data\n\nmonthly_salaries = pd.DataFrame(df.resample(\"M\").size(),columns=[\"count\"])\nmonthly_salaries[\"max_mean\"] = df[\"maximum_salary\"].resample(\"M\").mean()\nmonthly_salaries[\"min_mean\"] = df[\"minimum_salary\"].resample(\"M\").mean()\n\ngc.collect()","cbb2026f":"#Maximum salaries over time\nax, fig = plt.subplots(figsize=(15,7))\n\nax = plt.axes()\nax.set_xlim(left = datetime.date(2014, 1, 1), right = datetime.date(2019, 12, 1))\n\nplt.plot(monthly_salaries[\"max_mean\"])\nplt.plot(monthly_salaries[\"max_mean\"].rolling(window=5).mean())\n\nplt.xticks(fontsize=14,alpha=0.75)\nplt.yticks(fontsize=14,alpha=0.75)\n\n\nax.text(x=datetime.date(2019, 1, 1), y=115000,s=\"Max salary\", weight=\"bold\",fontsize=\"18\",alpha=0.75,color=\"#008fd5\")\n\nax.text(x=datetime.date(2019, 1, 1), y=100000,s=\"Rolling max salary\", weight=\"bold\",fontsize=\"18\",alpha=0.75,color=\"#fc4f30\")\n\n#Title\nax.text(x=datetime.date(2013, 10, 1), y=163000,s=\"Average maximum salary\", weight=\"bold\",fontsize=\"24\",alpha=0.75)\nax.text(x=datetime.date(2013, 10, 1), y=158000,s=\"The movement of the average maximum salary over time from 2014 - 2019.\", fontsize=\"18\",alpha=0.75)\n\ngc.collect()","dfdf7fdd":"#Minimum salaries over time\nax, fig = plt.subplots(figsize=(15,7))\n\nax = plt.axes()\nax.set_xlim(left = datetime.date(2014, 1, 1), right = datetime.date(2019, 12, 1))\n\nplt.plot(monthly_salaries[\"min_mean\"])\nplt.plot(monthly_salaries[\"min_mean\"].rolling(window=5).mean())\n\nplt.xticks(fontsize=14,alpha=0.75)\nplt.yticks(fontsize=14,alpha=0.75)\n\n\nax.text(x=datetime.date(2019, 4, 1), y=75000,s=\"Min salary\", weight=\"bold\",fontsize=\"18\",alpha=0.75,color=\"#008fd5\")\n\nax.text(x=datetime.date(2019, 1, 1), y=95000,s=\"Rolling min salary\", weight=\"bold\",fontsize=\"18\",alpha=0.75,color=\"#fc4f30\")\n\n#Title\nax.text(x=datetime.date(2013, 10, 1), y=140000,s=\"Average minimum salary\", weight=\"bold\",fontsize=\"24\",alpha=0.75)\nax.text(x=datetime.date(2013, 10, 1), y=135000,s=\"The movement of the average minimum salary over time from 2014 - 2019.\", fontsize=\"18\",alpha=0.75)\n\ngc.collect()","d2d80f86":"#Number of opportunities per year\/month\nax, fig = plt.subplots(figsize=(10,5))\n\nax = plt.axes()\n#ax.set_xlim(left = datetime.date(2014, 1, 1), right = datetime.date(2019, 12, 1))\nplt.plot(monthly_salaries[\"count\"])\n\nplt.xticks(fontsize=14,alpha=0.75)\nplt.yticks(fontsize=14,alpha=0.75)\n\nax.text(x=datetime.date(1998, 4, 1), y=max(monthly_salaries[\"count\"]) + 2.5,s=\"Number of opportunities over time\", weight=\"bold\",fontsize=\"18\",alpha=0.75,color=\"black\")\nax.text(x=datetime.date(1998, 4, 1), y=max(monthly_salaries[\"count\"]) + 1.3,s=\"The most opporutities in this dataset are between 2016-2019\",fontsize=\"14\",alpha=0.75,color=\"black\")\n\ngc.collect()","8e044677":"ax, fig = plt.subplots(figsize=(10,5))\n\nend_date = df[\"application_end_date\"].value_counts().sort_index()\n\nplt.plot(end_date.index, end_date)\n\nplt.xticks(fontsize=14,alpha=0.75)\nplt.yticks(fontsize=14,alpha=0.75)\n\n#Set the upper limit to match the above plot\nplt.ylim(top = 25)\n\nax.text(x=0.05, y=1,s=\"Number of deadlines over time\", weight=\"bold\",fontsize=\"18\",alpha=0.75,color=\"black\")\nax.text(x=0.05, y=0.955,s=\"The most opporutities and deadlines in this dataset are between 2016-2019\",fontsize=\"14\",alpha=0.75,color=\"black\")\n\ndel end_date\n\ngc.collect()","9682a6a0":"df_sorted = df.sort_values(by=[\"maximum_salary\"],ascending=False)\ndf.reset_index(inplace=True,drop=True)\n\nax, fig = plt.subplots(figsize=(10,5))\nax = sns.barplot(x=df_sorted[\"maximum_salary\"][:10], y=df_sorted[\"job_name\"][:10],palette=\"PuBuGn_d\")\n\nplt.xticks(fontsize=12,alpha=0.75)\nplt.yticks(fontsize=12,alpha=0.75)\n\nplt.xlabel(\"Salary\",fontsize=15, alpha=0.75, weight=\"bold\")\nplt.ylabel(\"Job Name\",fontsize=15, alpha=0.75, weight=\"bold\")\n\nax.text(x=-120000, y=-1.4, s=\"Top 10 highest paid jobs in Los Angeles\", weight=\"bold\",fontsize=18,alpha=0.75,color=\"black\")\nax.text(x=-120000, y=-0.8, s=\"The most paid position is the Cheif Port Pilot with a maximum salary of above 300k.\",fontsize=14,alpha=0.75,color=\"black\")","00c450a9":"ax, fig = plt.subplots(figsize=(10,5))\n\nax = sns.barplot(x=df_sorted[\"maximum_salary\"][-10:], y=df_sorted[\"job_name\"][-10:],palette=\"PuBuGn_d\")\n\nplt.xticks(fontsize=12,alpha=0.75)\nplt.yticks(fontsize=12,alpha=0.75)\n\nplt.xlabel(\"Salary\",fontsize=15, alpha=0.75, weight=\"bold\")\nplt.ylabel(\"Job Name\",fontsize=15, alpha=0.75, weight=\"bold\")\n\nax.text(x=-18000, y=-1.4, s=\"Top 10 least paid jobs in Los Angeles\", weight=\"bold\",fontsize=18,alpha=0.75,color=\"black\")\nax.text(x=-18000, y=-0.8, s=\"The least paid job in the city of Los Angeles is Office Trainee with less than 40k.\",fontsize=14,alpha=0.75,color=\"black\")\n\n\n#AIRPORT POLICE SPEC - indicate that no salary was given\nax.annotate(\"No salary given\",xy=(1000,8), xytext=(10500,8),fontsize=15,weight=\"bold\", color=\"black\",alpha=0.75,arrowprops=dict(arrowstyle=\"->\",color=\"black\", linewidth=4))","f6bd1b7b":"def counter_selection(df):\n    selection_counter = Counter()\n\n    for value in df:\n        \n        selection_counter += Counter(value)\n\n    return selection_counter","86c67249":"counter = counter_selection(df[\"selection\"].dropna())\ndf_counter = pd.DataFrame.from_dict(counter,orient=\"index\", columns=[\"count\"]).reset_index()","c6f21f24":"ax, fig = plt.subplots(figsize=(10,5))\n\nax = sns.barplot(x = df_counter[\"count\"], y=df_counter[\"index\"],palette=\"PuBuGn_d\")\n\nplt.xticks(fontsize=12,alpha=0.75)\nplt.yticks(fontsize=12,alpha=0.75)\n\nplt.xlabel(\"Number of selection tasks\",fontsize=15, alpha=0.75, weight=\"bold\")\nplt.ylabel(\"Name of the task\",fontsize=15, alpha=0.75, weight=\"bold\")\n\nax.text(x=-80, y=-0.85, s=\"Number of tasks required in the selection process\", weight=\"bold\",fontsize=18,alpha=0.75,color=\"black\")\nax.text(x=-80, y=-0.6, s=\"Most jobs require an interview as a selection task, while the second most taskis a test.\",fontsize=14,alpha=0.75,color=\"black\")\n","0b55a1a5":"def data_preperation_wordc(df):\n        \n        \"\"\"\n        Cleans, tokinizes and joins the words into a string to be used for a word cloud\n\n        \"\"\"\n\n        description = \" \".join(df.dropna())\n        description = re.sub('[^a-zA-Z0-9 ]+', '', description)\n        word_tokens = word_tokenize(description)\n        words = [word for word in word_tokens if word not in stop]\n        description = \" \".join(words)\n\n        return description, words","3403849d":"description_requirements, words_requirements = data_preperation_wordc(df[\"requirements\"])\ndescription_duties, words_duties = data_preperation_wordc(df[\"job_duties\"])","384d5f2b":"###### Wordcloud ######\n\nimage_mask = np.array(Image.open(\"..\/input\/imgs-los\/images\/circle.jpg\"))\n\nword_cloud = WordCloud(background_color=\"white\",max_words=1000,max_font_size=80, mask=image_mask, random_state=3)\nword_cloud.generate(description_requirements)\n\nimage_colours = ImageColorGenerator(image_mask)\n\n\nax, fig = plt.subplots(figsize=(8,8))\nplt.imshow(word_cloud.recolor(color_func = image_colours),interpolation=\"bilinear\")\n\nplt.axis(\"off\")\n","e9c8fd0a":"###### Wordcloud ######\n\nimage_mask = np.array(Image.open(\"..\/input\/imgs-los\/images\/circle.jpg\"))\n\nword_cloud = WordCloud(background_color=\"white\",max_words=1000,max_font_size=80, mask=image_mask, random_state=3)\nword_cloud.generate(description_duties)\n\nimage_colours = ImageColorGenerator(image_mask)\n\n\nax, fig = plt.subplots(figsize=(8,8))\nplt.imshow(word_cloud.recolor(color_func = image_colours),interpolation=\"bilinear\")\n\nplt.axis(\"off\")\n","ce2eabec":"#### Job similiarity ####\n\nvectorized = TfidfVectorizer(stop_words=\"english\")\n\nTfid = vectorized.fit_transform(df_file[\"text_data\"])\n\nparwise_sim = Tfid*Tfid.T","4a05430b":"#Creating matrix\n\nsimiliarity = parwise_sim.toarray()\nnp.fill_diagonal(similiarity, np.nan) ","9a3fd26d":"def find_similar_jobs(input_string,text):\n\n    \"\"\"\n    Inputs a job and returns the most similar job in the dataset and its similiarity index\n    \"\"\"\n    \n    index_id = text[text == input_string].index[0]\n    \n    result_id = np.nanargmax(similiarity[index_id])\n    result = text[result_id]\n\n    return result, print(similiarity[index_id,result_id])","cb099045":"results = find_similar_jobs(df_file[\"text_data\"][1],df_file[\"text_data\"])","ec5078cd":"#https:\/\/en.wikipedia.org\/wiki\/Flesch%E2%80%93Kincaid_readability_tests\n\n###ASL###\ndef avrg_sentence_leng(df):\n\n    data_list = []\n    for row in df:\n    \n        try:\n            sentence = sent_tokenize(row)\n            words = word_tokenize(row)\n            lenght_of_sentence = len(words) \/ len(sentence)\n\n        except:\n            lenght_of_sentence = np.nan\n\n        data_list.append(lenght_of_sentence)\n\n    return data_list","73ca1636":"df[\"ASL_duties\"] = avrg_sentence_leng(df[\"job_duties\"].astype(str))\ndf[\"ASL_req\"] = avrg_sentence_leng(df[\"requirements\"].astype(str))","7fb5af3c":"def syllable_counter (data):\n\n    data_list = []\n    try:\n        for text in data:\n\n            counter = 0\n            text = word_tokenize(text)\n\n            for word in text:\n\n                word = word.lower()\n\n                for letter in [\"a\",\"e\",\"i\",\"o\",\"u\"]:\n                    counter += word.count(letter)\n                for ending in [\"es\",\"ed\",\"e\"]:\n                    if word.endswith(ending):\n                        counter -=1\n                    else:\n                        pass\n                if word.endswith(\"le\"):\n                    counter +=1\n            leng_text = len(text)\n            counter = counter \/ leng_text\n            data_list.append(counter)\n    except:\n        data_list.append(counter)\n\n    return data_list","f5c0a88f":"df[\"ASW_duties\"] = syllable_counter(df[\"job_duties\"].astype(str))\ndf[\"ASW_req\"] = syllable_counter(df[\"requirements\"].astype(str))","b6cb7ebf":"#Reading Ease score = 206.835 - (1.015 \u00d7 ASL) - (84.6 \u00d7 ASW)\n\ndf[\"Flesch_index_duties\"] = 206.835 - (1.015 * df[\"ASL_duties\"]) - (84.6 * df[\"ASW_duties\"])\ndf[\"Flesch_index_duties\"] = df[\"Flesch_index_duties\"].apply(lambda x: max(x,0))\n\ndf[\"Flesch_index_req\"] = 206.835 - (1.015 * df[\"ASL_req\"]) - (84.6 * df[\"ASW_req\"])\ndf[\"Flesch_index_req\"] = df[\"Flesch_index_req\"].apply(lambda x: max(x,0))","5f68700f":"ax, fig = plt.subplots(figsize=(10,5))\n\nx = sns.distplot(df[\"Flesch_index_req\"], color=\"black\")\n\nplt.xticks(fontsize=12,alpha=0.75)\nplt.yticks(fontsize=12,alpha=0.75)\n\nplt.xlabel(\"Flesch\u2013Kincaid grade level index\",fontsize=15, alpha=0.75, weight=\"bold\")\n\n\nx = np.arange(0,50)\ny = x*0+0.040\nplt.plot(x,y,color=\"red\",alpha=0.50)\nplt.fill_between(x, y, color=\"red\",alpha=0.50)\nax.text(x=0.24, y =0.5, s=\"Very Difficult\/Difficult\", weight=\"bold\", fontsize=12)\n\nx = np.arange(50,80)\ny = x*0+0.040\nplt.plot(x,y,color=\"yellow\",alpha=0.50)\nplt.fill_between(x, y, color=\"yellow\",alpha=0.50)\nax.text(x=0.50, y =0.5, s=\"Medium\", weight=\"bold\", fontsize=12)\n\n\nx=np.arange(80,100)\ny = x*0+0.040\nplt.plot(x,y,color=\"green\",alpha=0.50)\nplt.fill_between(x, y, color=\"green\",alpha=0.50)\nax.text(x=0.635, y =0.5, s=\"Easy\", weight=\"bold\", fontsize=12)\n\nax.text(x=0, y=0.95, s=\"Distribution of the Flesch-Kincaid readability index for requirements.\", weight=\"bold\",fontsize=18,alpha=0.75,color=\"black\")\nax.text(x=0, y=0.90, s=\"Most requirements are rated very difficult or difficult to read and require a university degree to fully understand.\",fontsize=14,alpha=0.75,color=\"black\")\n\ngc.collect()\n\ndel x,y","cf4d3550":"ax, fig = plt.subplots(figsize=(10,5))\n\nx = sns.distplot(df[\"Flesch_index_duties\"], kde_kws={'bw': 0.1}, color=\"black\")\n\nplt.xticks(fontsize=12,alpha=0.75)\nplt.yticks(fontsize=12,alpha=0.75)\n\nplt.xlabel(\"Flesch\u2013Kincaid grade level index\",fontsize=15, alpha=0.75, weight=\"bold\")\n\n\nx = np.arange(0,50)\ny = x*0+0.180\nplt.plot(x,y,color=\"red\",alpha=0.50)\nplt.fill_between(x, y, color=\"red\",alpha=0.50)\nax.text(x=0.195, y =0.5, s=\"Very Difficult\/Difficult\", weight=\"bold\", fontsize=12)\n\nx = np.arange(50,80)\ny = x*0+0.180\nplt.plot(x,y,color=\"yellow\",alpha=0.50)\nplt.fill_between(x, y, color=\"yellow\",alpha=0.50)\nax.text(x=0.52, y =0.5, s=\"Medium\", weight=\"bold\", fontsize=12)\n\n\nx=np.arange(80,100)\ny = x*0+0.180\nplt.plot(x,y,color=\"green\",alpha=0.50)\nplt.fill_between(x, y, color=\"green\",alpha=0.50)\nax.text(x=0.675, y =0.5, s=\"Easy\", weight=\"bold\", fontsize=12)\n\nax.text(x=0, y=0.95, s=\"Distribution of the Flesch-Kincaid readability index for duties.\", weight=\"bold\",fontsize=18,alpha=0.75,color=\"black\")\nax.text(x=0, y=0.90, s=\"Almost all duties are rated very difficult to read and require a university degree to fully understand.\",fontsize=14,alpha=0.75,color=\"black\")\n\ngc.collect()\n\ndel x,y","534f9905":"The following function extracts the data from the textual files.","16c5ec59":"The job postings dataset contains a lot fewer deadlines than job openings. This gap might indicate that there are no date deadlines were given. ","8320a825":"The minimum salary experiences the same stagnation as the maximum mean salary. ","9f6599cd":"Wish I was the Chief Port Pilot.","f22f2529":"# **Flesch-Kincaid readability**\n\nThe secondary goal is to create a Flesch-Kincaid readability test using the requirements and duties collected from the textual data. The test is used to determine how difficult a passage in English is to understand. In this case, we are going to use the Flesch\u2013Kincaid Grade Level.\n","a09ce2e9":"# **Conclusion**\n\nThere are still a lot of angels left unexplored in the data and there is potential to go into more details. Overall, I am content with the project as I wanted to do some very specific things with the data.\n\nThank you for readings this notebook.","5391cda2":"Most requirements and duty descriptions in the job postings are difficult to understand. Reading them requires a high level of education and practice. This difficulty is making the application process hard and is potentially limiting the number of applications. The descriptions should be adjusted and their difficulty lowered to encourage more people into applying. This change could improve the overall health of the application process.","ed7ec916":"# **Data Extraction**\n\nRegex or regular expression is a string of characters that defines a search pattern for text. It is almost as old as computer science and still a great tool in many programming languages (including Python).\n\nI am going to define the regex patterns below. Implementing them directly into the function makes it too complicated and hard to navigate. Additionally, I have written a small description over each of them for clarity.","46b4cf30":"The data from 2014 is too unreliable and limited for us to consider it when constructing a hypothesis.  From 2015 to 2017 the mean salaries experienced some growth. This growth stagnated from 2017 onward. An interesting angle we can approach here is to compare the salary increases to the inflation rates and cost of living and determine if they are sufficient. ","d77ed583":"#  **Job analysis**\n\nThis notebook segment will consist of a quick analysis of some key metrics in the data. We are going to look at the salaries, opening & closing dates and selection tasks for the job.","3ca95561":"# **Introduction**\n\nThe following notebook contains the job posting data given to us by the city of Los Angeles.  LA provided the data in the hope that analysing and improving it would promote diversity, reduce potential discrimination and make the job selection process more unbiased overall. Each job posting has an individual text file with slight variation between them. \n\nMy goals for this notebook are:\n\n    1. Create a function that can successfully extract the necessary data from the provided text files.\n    2. Create a similarity function that can match similar jobs once provided the input.\n    3. Analyse the readability index of the requirements and duties for each job.","9eb7c6e1":"# **TF-IDF similarity function** \n\nThe similarity function was created using a TfidfVectorizer from the sklearn library. This class converts the selected documents into a TF-IDF matrix. TF-IDF term frequency\u2013inverse document frequency is often used in numerical statistics to determine the importance of words. Its most frequent use is in search engines where it is used to score and rank documents based on the user's query. In this particular case, the function using TF-IDF will find the most similar job for the one selected.\n","dd641d89":"Testing out the extraction with a word cloud (because why not).","672efbbe":"# **Library\/Data Import** "}}