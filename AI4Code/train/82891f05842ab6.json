{"cell_type":{"767d7cc0":"code","3560f3dd":"code","0a21f0d5":"code","9896bd30":"code","58dd16e8":"code","128ea119":"code","a61e0b2a":"code","45f3d680":"code","ea0cd943":"code","be12d3ed":"code","39b18fcd":"code","a60f8564":"code","23b1fed8":"code","cdca775a":"code","fc374344":"code","daa9dee5":"code","24fe2f3e":"code","27ba5a34":"code","cfa7ffe1":"code","f27f5b3d":"code","71fff8fe":"code","226891ad":"code","dea9d0d5":"code","89b9de1e":"code","cac4e802":"code","d171048c":"code","a24412c0":"code","69066bef":"code","c8dcef8d":"code","bb80b9a7":"code","a7c1c148":"code","6d08a376":"code","b628ee88":"code","945eb98e":"code","84043889":"code","5069238b":"code","f8c0cc45":"code","1c582269":"code","65f72a3b":"code","b932782f":"code","75d409d5":"markdown","ea92c7b6":"markdown","0453960f":"markdown","2711fe24":"markdown","369517a8":"markdown","9df07217":"markdown","c2a88ae5":"markdown","cd677e8f":"markdown","82ecc9a0":"markdown","ef375062":"markdown","ec172feb":"markdown","aabcb351":"markdown"},"source":{"767d7cc0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3560f3dd":"import numpy as np #linear algebra\nimport pandas as pd #data processing\n\nimport matplotlib.pyplot as plt #data visualization\nimport seaborn as sns #data visualization\n\nimport warnings\nwarnings.filterwarnings(\"ignore\") #to ignore the warnings\n\n#for building we need few of these lib\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb","0a21f0d5":"Bcancer_df= pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")","9896bd30":"Bcancer_df.shape\n# using shape function we can get an idea of how many rows and column does the data set hold here there are 569 rows and 33 columns ","58dd16e8":"Bcancer_df.info()\n# info() function gives information like nos of col , nos of rows, dtype of each col, info of null value, column name etc  ","128ea119":"Bcancer_df.head(30)\n# head() function used to display the data set and when we pass some specific nos like in our case i have pass 30 it is \n# showing me top 30 observation in our data set and oppsite of head is tail() is gives 5 bottom values by default unless we pass some number ","a61e0b2a":"Bcancer_df.columns # it gives information about the nme of columns in the data set ","45f3d680":"Bcancer_df.drop(['Unnamed: 32','id'], axis = 1 , inplace=True)\nBcancer_df.columns\n# in this step we are dropping the least important features from our data set to reduce the dimension of the data set in order to restrict \n# the model from overfitting. ","ea0cd943":"Bcancer_df.diagnosis.replace({\"M\":1,\"B\":0},inplace=True)\nBcancer_df.diagnosis.unique()\n#  diagnosis col contains M = malignant (cancerous cell) B = benign (non cancerous cell) and we are laeling it with M = 1, B = 0 as it will be \n# easy for our model to work on 0 and 1 value rather than on string value ","be12d3ed":"# here we are interested in finding the correlation between our target variable ie diagnosis and independent features \ncorr = Bcancer_df.corr()\nplt.figure(figsize=(20,20))\nsns.heatmap(Bcancer_df.corr(), cmap='PiYG', annot = True)\nplt.title(\"CORRELATION MAP \", fontweight = \"bold\", fontsize=16)","39b18fcd":"corr[abs(corr['diagnosis']) > 0.59].index  \n# here as we can see that there are numbers of features which are correlated with our target variable ","a60f8564":"Bcancer_df.drop('diagnosis', axis=1).corrwith(Bcancer_df.diagnosis).plot(kind='bar', grid=True, figsize=(12, 10), title=\"Correlation between feature variable and target variable \",color=\"purple\")","23b1fed8":"x = Bcancer_df.drop(\"diagnosis\",axis = 1)\ny = Bcancer_df.diagnosis","cdca775a":"x","fc374344":"y","daa9dee5":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split","24fe2f3e":"Bcancer_df.head(10)","27ba5a34":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.33,random_state=42,shuffle=True, stratify=y)","cfa7ffe1":"print('x_train shape : ', x_train.shape )\nprint('y_train shape : ', y_train.shape )\nprint('x_test shape : ', x_test.shape )\nprint('y_test shape : ', y_test.shape )","f27f5b3d":"from sklearn.neighbors import KNeighborsClassifier\n\n\ntest_scores = []\ntrain_scores = []\nK=[]\nfor i in range(1,15):\n\n    knn = KNeighborsClassifier(i)\n    knn.fit(x_train,y_train)\n    \n    train_scores.append(knn.score(x_train,y_train))\n    test_scores.append(knn.score(x_test,y_test))\n    K.append(i)","71fff8fe":"results= { 'train_scores':train_scores,\n         'test_scores':test_scores,\n         'K':K}\nresults_df =  pd.DataFrame(results,\n        columns=['train_scores','test_scores', 'K'])\nresults_df","226891ad":"KNNModel = KNeighborsClassifier(13)\nKNNModel.fit(x_train,y_train)\n\ntrain_score = KNNModel.score(x_train,y_train)\ntest_score = KNNModel.score(x_test,y_test)\nprint( 'train_score ', train_score)\nprint( 'test_score ',test_score )","dea9d0d5":"y_pred = knn.predict(x_test)\n\n\nknn_cm= confusion_matrix(y_test,y_pred)\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","89b9de1e":"sns.heatmap(knn_cm,annot = True , fmt = 'g', vmin = 0 , cmap = 'BuGn',cbar = False )\nplt.xlabel(\"predicted\")\nplt.ylabel(\"actual\")\nplt.title(\"confusion matrix\")\nplt.show()","cac4e802":"from sklearn import metrics \nprint('accuracy : ',metrics.accuracy_score(y_test,y_pred) )","d171048c":"x2_train,x2_test,y2_train,y2_test=train_test_split(x,y,test_size=0.3,random_state=42,shuffle=True, stratify=y)","a24412c0":"print('x2_train shape : ', x2_train.shape )\nprint('y2_train shape : ', y2_train.shape )\nprint('x2_test shape : ', x2_test.shape )\nprint('y2_test shape : ', y2_test.shape )","69066bef":"from sklearn.ensemble import RandomForestClassifier\nrf= RandomForestClassifier(random_state =42)\nfrom pprint import pprint # used to look at the parameter already in use \nprint('parameters currently in use: \\n')\npprint(rf.get_params())","c8dcef8d":"from sklearn.model_selection import RandomizedSearchCV\nn_estimators= [int(x) for x in np.linspace(start = 100, stop = 2000, num =10)]\nmax_features = ['auto','sqrt']\nmax_depth = [int(x) for x in np.linspace(10,110,num=11)]\nmin_samples_split = [2,5,10]\nmin_samples_leaf = [1,2,4]\nbootstrap=[True,False]\n\nrandom_grid = {'n_estimators':n_estimators,\n              'max_features':max_features,\n              'max_depth':max_depth,\n              'min_samples_split':min_samples_split,\n               'min_samples_leaf':min_samples_leaf,\n               'bootstrap':bootstrap\n              }\npprint(random_grid)","bb80b9a7":"\nrf=RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf , param_distributions = random_grid , n_iter = 100 , cv = 3 , verbose = 2, random_state  = 42, n_jobs = -1)\n\nrf_random.fit(x2_train,y2_train)\nrf_random.best_params_\n               ","a7c1c148":"def evaluate(model,x2_test,y2_test):\n    y_pred = model.predict(x2_test)\n    error = abs(y_pred-y2_test)\n    mape = 100*np.mean(error\/y2_test)\n    accuracy = 100-mape\n    print ('model performance')\n    print('average error : {:.4f} '.format(np.mean(error)))\n    print('accuracy = {:.2f}%'.format(accuracy))\n    return accuracy\nbest_random = rf_random.best_estimator_\nrandom_accuracy = evaluate(best_random,x2_test,y2_test)\n","6d08a376":"x1_train,x1_test,y1_train,y1_test=train_test_split(x,y,test_size=0.3,random_state=101,shuffle=True, stratify=y)","b628ee88":"print('x1_train shape : ', x1_train.shape )\nprint('y1_train shape : ', y1_train.shape )\nprint('x1_test shape : ', x1_test.shape )\nprint('y1_test shape : ', y1_test.shape )","945eb98e":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.svm import SVC","84043889":"# without hyperparameter tunning\nmodel = SVC()\nmodel.fit(x1_train, y1_train)\n \n# print prediction results\npredictions = model.predict(x1_test)\nprint(classification_report(y1_test, predictions))","5069238b":"print('accuracy : ',metrics.accuracy_score(y1_test,predictions))","f8c0cc45":"from sklearn.model_selection import GridSearchCV\n \n# defining parameter range\nparam_grid = {'C': [0.1, 1, 10, 100, 1000],\n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel': ['rbf']}\n \ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n \n# fitting the model for grid search\ngrid.fit(x1_train, y1_train)","1c582269":"\n# print best parameter after tuning\nprint(grid.best_params_)\n \n# print how our model looks after hyper-parameter tuning\nprint(grid.best_estimator_)","65f72a3b":"grid_predictions = grid.predict(x1_test)\n \n# print classification report\nprint(classification_report(y1_test, grid_predictions))","b932782f":"print('accuracy : ',metrics.accuracy_score(y1_test,grid_predictions))","75d409d5":"# Hyperparameter tunning \nit is a technique using which we tends to find the best value for our hyperparameters in order to boost the performance of the model \n# hyperparameter \nhyparameters are the parameter which is not the part of actual programing but it is used in addition from outside we are trying to controll the algorithim that is called hyperparameter. \n# hyperparameter tunning can be achieve by 2 ways \n1. RandomSearchCV --> it is a techniques wherein we give range for various hyper parameters and later we make a grid and store all the values in the grid and later we define a function which iterate over and overdifferent values of the parameter and give best out of it and that can be used to build model which can boost the performance \n2. GridSearchCv --> it is the technique in which we try to build a grid wherein we assign certain values to each parameter and then using a function we iterrate over and over and evaluate which values is best for building the model.\n","ea92c7b6":"# support vector machine ","0453960f":"# Random forest with hygperparameter tunning ","2711fe24":"above plot suggest or give informations which feature variable is more correlated to our target variable ","369517a8":"using hyperparameter tunning i got best hyperparameter value using which i can get best model with accuracy \n1. n_estimators': 100,\n2. min_samples_split': 2,\n3. min_samples_leaf': 2,\n4. max_features': 'sqrt',\n5. max_depth': 80,\n6. bootstrap': False\n","9df07217":"# History \nBreast cancer has become a matter of  concern which needs proper mitigation steps as it is very painful and terrifying. If we take example of India alone in 2018 census says total 1,62,468 new caese were registered and 87,090 were reported dead. \nCancer is like a trauma and it is very difficult to cure whenit is detected at later stage. Moreover, chances of survival is extemely impossible when it is detected at stage 4. \nIn India around 50% of womens are suffering from satge 3-4 breast cancer. So, it is the need of hour that there should be some robust and accurate technique to address this matter of concern at very early stage so, that with proper medical treatment it can be cured and can able to save life. \nmachine learning is contributing alot in this regard by preparing model and detecting cancer at early stage. \n# problem statement\nto detect cancer cell in order to save life - to solve this problem we have used various classification algorithm like (KNN,Random Forest, Support Vector Machine)\n# KNN algorithm \nit is simplest supervised machine learning algorithm which is used for classification purpose. KNN works on the principle of similarity for example it store training data and when we feed new data to the model it check for the similarity and accordingly it classify the data set. It is non parametric and lazy learner because it does not learn from the training data rather it store the data. \n# How does KNN work \n1. first step is to decide the number of nearest neighbour\n2. then it will calculate euclidean between new data and existing data and as per number of nearest neighbour it select the x numbers of data from existing which are close to new data \n3. suppose from category A (existing data ) total 3 data set are close to new data point and from category B(existing data ) 5 data points are close enough to new data points then the new data is similary to cateogry B based on maxing votting concept \n# Bagging \nBagging is an ensemble technique which is use to reduce the variance of prediction by combining the results of various models together which are build on sub sample of orginial data set using Bootstrap sampling technique wherein sampling is done with replacement and each and evry data points has equal opportunity to get selected. \n# Random Forest - \nRandom Forest is an ensemble machine learning algorithm which is based on bagging technique and it is robust technique which can perfome both regression and classification task. It is robust in nature because it can do dimension reduction, can treat missing data, treat outlier futher it can performe all necessary steps required during model building. \n# How does it work \nSince it is based on ensemble technique it divide the data into small subsets and build weak tree models using those small subset further using max votting or averaging technique based on the type of problem if it is regression it will use averaging  and if it is classification it will use max votting technique and combine numbers of weak learners together to build one robust strong model. \n# support vector machine \nit is also a supervise machine learning algorithm which can perform both regression as well as classicicaion algorithm,but it is not based on ensemle technique rather it is based on statistical approach so it is bit different from logistic regression which is based on probability approach. \n# types of SVM \n1. linear --> in this using one straight line as hyperplane we can able to classify the observation into different groups \n2. non linear --> where we can't be able to classify observations using straight line instead we will required a kernel or functions which will lift the dimension of the observation and then in higher dimension we can easliy make decision boundary and seperate our target observationin respective groups \n# Support vector \nit is defined as the data point which is very close to the Hyperplane(decision boundary)\n# margin \nthe maximum width between negative and positive hyperplane is called margin.The more is the margin the better will be the model \n# Hyperplane\/decision boundary \nit is the best line which is drawn in between the data set in order to classify them into respective class.\n# hard margin \nwhen we are able to sepearte the data using straight line then there whatever margin we get is hard margin \n# soft margin \nwhen we are unable to seperate the data using straight line then in that case we use some functions or kernels to do the speration task and there whatever margin we get is soft margin \n# kernel \nit is a function (non linear) which which is used to convert lower dimension data set into higher dimension which allows us to make a decision boundary in between the data set and seperate the data set and it is called kernel\n# types of kernel \n1. polynomial\n2. sigmoid \n3. RBF(radial base function) which is most widely used because of its simple mathematics. It basically create non-linear combinations of features to lift there dimension to higher in order to make decision boundary easily to seperate the data set. \n4. annova \n5. bassel\n# How does SVM work \nit works on statistical approach wherein it first check whether data can be seperated by staright line or not once this is done it draw a hyperplane between the observation then it try to find the support vector and using that support vector it further draw 2 more hyperplane passing through the support vector and are parallel to optimum hyperplane. Then it claculate the width of margin an try to get maximum margin width. Then it make prediction\n","c2a88ae5":"# splitting the data set ","cd677e8f":"from confusion matrix we can conclude the model is able to predict benign as benign 117 times and wrongly predicting benign as malignant 12 times further model is correctly predicting malignant as malignant 58 time and wronglyby 1 times ","82ecc9a0":"from the above heatmap we are trying to find the correlation between our target variable and independent variable and we are taking only those features which are having correlation greater than .59 or 59%\n\n1. correlation between diagnosis and radius_mean is .73\n2. correlation between diagnosis and perimeter_mean is .74\n3. correlation between diagnosis and area_mean is .71\n4. correlation between diagnosis and compactness_mean is .6\n5. correlation between diagnosis and concavity_mean is .7\n6. correlation between diagnosis and concave point_mean is .78\n7. correlation between diagnosis and radius_worst is .78\n8. correlation between diagnosis and perimeter_worst is .78\n9. correlation between diagnosis and area_worst is .73\n10. correlation between diagnosis and concave point_worst is .79\nhere further more the problem of multicollinearity does exist as we can see from the correlation heat map or we can use VIF(variance influence factor and it is equal to (1-1\/R^2) and if VIF>10 then there exist multicollinearity and in our analysis we didnt touch that part but for building logistic or multilinear regression model we have to address this problem and it can be done by simply identifying which all the parameters are highly correlated or not of that importance that can be droped out our further wwe can use the concept of PCA principal component analysis in order to find best parameters that is required for the analysis in this way we can incounter the problem of multicollinearity between the variables ","ef375062":"# train test split","ec172feb":"in our k nearest neighbour model we have passed k value from 1 to 14 and we trained our model using different value of k as we can see above and futher we calculated the train and test score and accordingly we decided the value of k for our further analysis ","aabcb351":"# KNN (K nearest neigbhors)"}}