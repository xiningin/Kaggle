{"cell_type":{"2d2323ea":"code","69bbf4cd":"code","1bf67a70":"code","704eadba":"code","80b538a7":"code","9e0686ae":"code","831e1651":"code","1aa1fa07":"code","b562deea":"code","c2581ea2":"code","b64994fe":"code","12f10f59":"code","2003fb0c":"code","4e5f55e5":"code","2eb05706":"code","6c15186b":"code","9982189d":"code","5e9658d0":"code","cbf10da7":"code","f1a77efc":"code","4680f9b9":"code","4d1160ef":"code","1d9e5738":"code","ee2096f1":"code","ef0e1649":"code","ce78347c":"code","3ba29829":"code","180a76db":"code","7a16e587":"code","d70d3943":"code","cd5cda90":"code","71824a2a":"code","9ceb4682":"code","18831c01":"code","35c0f6f2":"code","64772c42":"code","b5e77d8b":"code","f1073535":"code","8c91b8a1":"code","3c5d227f":"code","c7c0c679":"code","15110458":"code","99643dd6":"code","c9949597":"code","a40c98b8":"code","183f48b2":"code","70c49125":"code","e69b36af":"code","659b3918":"code","12f66e47":"code","c1968711":"code","35c15889":"code","e8f882eb":"code","548fffb7":"code","5da7d2c2":"code","dea287d7":"code","52426f9c":"code","9c52d370":"code","16170b31":"code","c09e7856":"code","aec40191":"code","8bb0449c":"code","5d4b942b":"code","d9422662":"code","07e6a72e":"code","41dd1411":"code","2d851963":"code","6092f946":"code","51aa3780":"code","f08e9228":"code","446f5340":"code","b3ee6770":"code","39776fca":"markdown","7d2fd568":"markdown","85f11661":"markdown","f2c4dbb3":"markdown","ab76fa25":"markdown","0a6d4b8b":"markdown","766078fe":"markdown","f819ba01":"markdown","e24e9c9f":"markdown","0c89825c":"markdown","882d4c3d":"markdown","7b804c70":"markdown","59ec7bb1":"markdown","12b75687":"markdown","75248428":"markdown","9065b554":"markdown","001b6549":"markdown","09a7a996":"markdown"},"source":{"2d2323ea":"# import library\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\n\n# import data\ndata = pd.read_csv(\"..\/input\/linearregressiondataset3\/linear-regression-dataset.csv\")\nprint(data.info())\nprint(data.head())\n#print(data.describe())","69bbf4cd":"# plot data\nplt.scatter(data.deneyim,data.maas)\nplt.xlabel(\"deneyim\")\nplt.ylabel(\"maas\")\nplt.show()","1bf67a70":"#%% linear regression\n\n# sklearn library\nfrom sklearn.linear_model import LinearRegression\n# linear regression model\nlinear_reg = LinearRegression()\n\nx = data.deneyim.values.reshape(-1,1)\ny = data.maas.values.reshape(-1,1)\n\nlinear_reg.fit(x,y)\n\nprint('R sq: ', linear_reg.score(x, y))\nprint('Correlation: ', math.sqrt(linear_reg.score(x, y)))","704eadba":"#%% prediction\nimport numpy as np\n\nprint(\"Coefficient for X: \", linear_reg.coef_)\nprint(\"Intercept for X: \", linear_reg.intercept_)\nprint(\"Regression line is: y = \" + str(linear_reg.intercept_[0]) + \" + (x * \" + str(linear_reg.coef_[0][0]) + \")\")\n\n# maas = 1663 + 1138*deneyim \nmaas_yeni = 1663 + 1138*11\nprint(maas_yeni)\n\narray = np.array([11]).reshape(-1,1)\nprint(linear_reg.predict(array))","80b538a7":"# visualize line\narray = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]).reshape(-1,1)  # deneyim\n\nplt.scatter(x,y)\n#plt.show()\ny_head = linear_reg.predict(array)  # maas\nplt.plot(array, y_head,color = \"red\")\narray = np.array([100]).reshape(-1,1)\nlinear_reg.predict(array)","9e0686ae":"y_head = linear_reg.predict(x)  # maas\nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y,y_head))","831e1651":"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndata = pd.read_csv(\"..\/input\/multiplelinearregressiondataset\/multiple-linear-regression-dataset.csv\")\nprint(data.info())\nprint(data.head())\n#print(data.describe())","1aa1fa07":"x = data.iloc[:,[0,2]].values\ny = data.maas.values.reshape(-1,1)\n\nmultiple_linear_regression = LinearRegression()\nmultiple_linear_regression.fit(x,y)\n\nprint(\"b0: \",multiple_linear_regression.intercept_)\nprint(\"b1: \", multiple_linear_regression.coef_)\n\n#predict\nx_ = np.array([[10,35],[5,35]])\nmultiple_linear_regression.predict(x_)\n\ny_head = multiple_linear_regression.predict(x) \nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y,y_head))","b562deea":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\"..\/input\/polynomialregressioncsv\/polynomial-regression.csv\")\nprint(data.info())\nprint(data.head())\n#print(data.describe())","c2581ea2":"x = data.araba_fiyat.values.reshape(-1,1)\ny = data.araba_max_hiz.values.reshape(-1,1)\n\nplt.scatter(x,y)\nplt.xlabel(\"araba_max_hiz\")\nplt.ylabel(\"araba_fiyat\")\nplt.show()","b64994fe":"# polynomial regression =  y = b0 + b1*x +b2*x^2 + b3*x^3 + ... + bn*x^n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\npolynominal_regression = PolynomialFeatures(degree=4)\nx_polynomial = polynominal_regression.fit_transform(x,y)\n\n# %% fit\nlinear_regression = LinearRegression()\nlinear_regression.fit(x_polynomial,y)\n# %%\ny_head2 = linear_regression.predict(x_polynomial)\n\nplt.plot(x,y_head2,color= \"green\",label = \"poly\")\nplt.legend()\nplt.scatter(x,y)\nplt.xlabel(\"araba_max_hiz\")\nplt.ylabel(\"araba_fiyat\")\nplt.show()\n\nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y,y_head2))\n\n","12f10f59":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\"..\/input\/support-vector-regression\/maaslar.csv\")\nprint(data.info())\nprint(data.head())\n#print(data.describe())","2003fb0c":"x = data.iloc[:,1:2].values\ny = data.iloc[:,2:].values\n\nplt.scatter(x,y)\nplt.xlabel(\"araba_max_hiz\")\nplt.ylabel(\"araba_fiyat\")\nplt.show()","4e5f55e5":"#verilerin olceklenmesi\nfrom sklearn.preprocessing import StandardScaler\nsc1 = StandardScaler()\nx_olcekli = sc1.fit_transform(x)\nsc2 = StandardScaler()\ny_olcekli = sc2.fit_transform(y)\n\n#%% SVR\nfrom sklearn.svm import SVR\n\nsvr_reg = SVR(kernel = 'rbf')\nsvr_reg.fit(x_olcekli,y_olcekli)\n\ny_head = svr_reg.predict(x_olcekli)\n\n# visualize line\nplt.plot(x_olcekli,y_head,color= \"green\",label = \"SVR\")\nplt.legend()\nplt.scatter(x_olcekli,y_olcekli,color='red')\nplt.show()\n\nprint('R sq: ', svr_reg.score(x_olcekli, y_olcekli))","2eb05706":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/decisiontreeregressiondataset\/decision-tree-regression-dataset.csv\", header=None)\nprint(data.info())\nprint(data.head())\n#print(data.describe())\n","6c15186b":"x = data.iloc[:,[0]].values.reshape(-1,1)\ny = data.iloc[:,[1]].values.reshape(-1,1)","9982189d":"#%%  decision tree regression\nfrom sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(x,y)\n\nprint(tree_reg.predict(np.array([5.5]).reshape(-1,1)))","5e9658d0":"x_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\n#print(x)\ny_head = tree_reg.predict(x_)\n#print(y_head)\n\n# %% visualize\nplt.scatter(x,y,color=\"red\")\nplt.plot(x_,y_head,color = \"green\")\nplt.xlabel(\"tribun level\")\nplt.ylabel(\"ucret\")\nplt.show()\n\ny_head = tree_reg.predict(x)\n#from sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y,y_head))","cbf10da7":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/randomforestregressiondataset\/random-forest-regression-dataset.csv\", header=None)\nprint(data.info())\nprint(data.head())\n#print(data.describe())","f1a77efc":"x = data.iloc[:,0].values.reshape(-1,1)\ny = data.iloc[:,1].values.reshape(-1,1)","4680f9b9":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators = 100, random_state= 42) \nrf.fit(x,y)\n\nprint(\"7.8 seviyesinde fiyat\u0131n ne kadar oldu\u011fu: \",rf.predict(np.array([7.8]).reshape(-1,1)))\n\nx_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_head = rf.predict(x_)","4d1160ef":"# visualize\nplt.scatter(x,y,color=\"red\")\nplt.plot(x_,y_head,color=\"green\")\nplt.xlabel(\"tribun level\")\nplt.ylabel(\"ucret\")\nplt.show()","1d9e5738":"y_head = rf.predict(x)\nfrom sklearn.metrics import r2_score\nprint(\"r_score: \", r2_score(y,y_head))","ee2096f1":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/classification\/data.csv\")\n#print(data.info())\n#print(data.head())\n#print(data.describe())\n# %%\ndata.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\ndata.tail()\n# malignant = M  kotu huylu tumor\n# benign = B     iyi huylu tumor","ef0e1649":"# %%\nM = data[data.diagnosis == \"M\"]\nB = data[data.diagnosis == \"B\"]\n# scatter plot\nplt.scatter(M.radius_mean,M.texture_mean,color=\"red\",label=\"kotu\",alpha= 0.3)\nplt.scatter(B.radius_mean,B.texture_mean,color=\"green\",label=\"iyi\",alpha= 0.3)\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"texture_mean\")\nplt.legend()\nplt.show()","ce78347c":"# %%\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\ny = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"],axis=1)\n# %%\n# normalization \nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","3ba29829":"#%%\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=1)\n\n# %%\n# knn model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3) # n_neighbors = k\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\" {} nn score: {} \".format(3,knn.score(x_test,y_test)))","180a76db":"# %%\n# find k value\nscore_list = []\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","7a16e587":"# %%\n# knn model\nknn = KNeighborsClassifier(n_neighbors = 8) # n_neighbors = k\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\" {} nn score: {} \".format(3,knn.score(x_test,y_test)))","d70d3943":"#%% confusion matrix\ny_pred = knn.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n# %% cm visualization\nimport seaborn as sns\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","cd5cda90":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/classification\/data.csv\")\n#print(data.info())\n#print(data.head())\n#print(data.describe())\n# %%\ndata.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\ndata.tail()\n# malignant = M  kotu huylu tumor\n# benign = B     iyi huylu tumor","71824a2a":"# %%\nM = data[data.diagnosis == \"M\"]\nB = data[data.diagnosis == \"B\"]\n# scatter plot\nplt.scatter(M.radius_mean,M.texture_mean,color=\"red\",label=\"kotu\",alpha= 0.3)\nplt.scatter(B.radius_mean,B.texture_mean,color=\"green\",label=\"iyi\",alpha= 0.3)\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"texture_mean\")\nplt.legend()\nplt.show()","9ceb4682":"# %%\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\ny = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"],axis=1)\n# %%\n# normalization \nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","18831c01":"#%%\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=1)\n\n# %% SVM\nfrom sklearn.svm import SVC\n \nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train)\n# %% test\nprint(\"print accuracy of svm algo: \",svm.score(x_test,y_test))\n\n","35c0f6f2":"#%% confusion matrix\ny_pred = svm.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n# %% cm visualization\nimport seaborn as sns\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","64772c42":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/classification\/data.csv\")\n#print(data.info())\n#print(data.head())\n#print(data.describe())\n# %%\ndata.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\ndata.tail()\n# malignant = M  kotu huylu tumor\n# benign = B     iyi huylu tumor","b5e77d8b":"# %%\nM = data[data.diagnosis == \"M\"]\nB = data[data.diagnosis == \"B\"]\n# scatter plot\nplt.scatter(M.radius_mean,M.texture_mean,color=\"red\",label=\"kotu\",alpha= 0.3)\nplt.scatter(B.radius_mean,B.texture_mean,color=\"green\",label=\"iyi\",alpha= 0.3)\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"texture_mean\")\nplt.legend()\nplt.show()","f1073535":"# %%\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\ny = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"],axis=1)\n\n# %%\n# normalization \nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","8c91b8a1":"#%%\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=1)\n\n# %% Naive bayes \nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n\nnb.score(x_test,y_test)\n # %% test\nprint(\"print accuracy of naive bayes algo: \",nb.score(x_test,y_test))","3c5d227f":"#%% confusion matrix\ny_pred = nb.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n# %% cm visualization\nimport seaborn as sns\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","c7c0c679":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/classification\/data.csv\")\n#print(data.info())\n#print(data.head())\n#print(data.describe())\n# %%\ndata.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\ndata.tail()\n# malignant = M  kotu huylu tumor\n# benign = B     iyi huylu tumor","15110458":"# %%\nM = data[data.diagnosis == \"M\"]\nB = data[data.diagnosis == \"B\"]\n# scatter plot\nplt.scatter(M.radius_mean,M.texture_mean,color=\"red\",label=\"kotu\",alpha= 0.3)\nplt.scatter(B.radius_mean,B.texture_mean,color=\"green\",label=\"iyi\",alpha= 0.3)\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"texture_mean\")\nplt.legend()\nplt.show()","99643dd6":"# %%\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\ny = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"],axis=1)\n\n# %%\n# normalization \nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","c9949597":"# %% train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test,y_train, y_test = train_test_split(x,y,test_size = 0.15,random_state = 42)\n\n#%%\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\n\nprint(\"score: \", dt.score(x_test,y_test))","a40c98b8":"#%% confusion matrix\ny_pred = dt.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n# %% cm visualization\nimport seaborn as sns\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","183f48b2":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/classification\/data.csv\")\n#print(data.info())\n#print(data.head())\n#print(data.describe())\n# %%\ndata.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\ndata.tail()\n# malignant = M  kotu huylu tumor\n# benign = B     iyi huylu tumor","70c49125":"# %%\nM = data[data.diagnosis == \"M\"]\nB = data[data.diagnosis == \"B\"]\n# scatter plot\nplt.scatter(M.radius_mean,M.texture_mean,color=\"red\",label=\"kotu\",alpha= 0.3)\nplt.scatter(B.radius_mean,B.texture_mean,color=\"green\",label=\"iyi\",alpha= 0.3)\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"texture_mean\")\nplt.legend()\nplt.show()","e69b36af":"# %%\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\ny = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"],axis=1)\n\n# %%\n# normalization \nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","659b3918":"# %% train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test,y_train, y_test = train_test_split(x,y,test_size = 0.15,random_state = 42)\n\n#%%  random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 100,random_state = 1)\nrf.fit(x_train,y_train)\nprint(\"random forest algo result: \",rf.score(x_test,y_test))","12f66e47":"#%% confusion matrix\ny_pred = rf.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n# %% cm visualization\nimport seaborn as sns\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","c1968711":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# %% create dataset\n# class1\nx1 = np.random.normal(25,5,1000)\ny1 = np.random.normal(25,5,1000)\n\n# class2\nx2 = np.random.normal(55,5,1000)\ny2 = np.random.normal(60,5,1000)\n\n# class3\nx3 = np.random.normal(55,5,1000)\ny3 = np.random.normal(15,5,1000)\n\nx = np.concatenate((x1,x2,x3),axis = 0)\ny = np.concatenate((y1,y2,y3),axis = 0)\n\ndictionary = {\"x\":x,\"y\":y}\ndata = pd.DataFrame(dictionary)\n\nplt.scatter(x1,y1)\nplt.scatter(x2,y2)\nplt.scatter(x3,y3)\nplt.show()","35c15889":"# %% KMEANS\n\nfrom sklearn.cluster import KMeans\nwcss = []\n\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(data)\n    wcss.append(kmeans.inertia_)\n    \nplt.plot(range(1,15),wcss)\nplt.xlabel(\"number of k (cluster) value\")\nplt.ylabel(\"wcss\")\nplt.show()","e8f882eb":"#%% k = 3 icin modelim\nkmeans2 = KMeans(n_clusters=3)\nclusters = kmeans2.fit_predict(data)\n\ndata[\"label\"] = clusters\n\nplt.scatter(data.x[data.label == 0 ],data.y[data.label == 0],color = \"red\")\nplt.scatter(data.x[data.label == 1 ],data.y[data.label == 1],color = \"green\")\nplt.scatter(data.x[data.label == 2 ],data.y[data.label == 2],color = \"blue\")\nplt.scatter(kmeans2.cluster_centers_[:,0],kmeans2.cluster_centers_[:,1],color = \"yellow\")\nplt.show()","548fffb7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# %% create dataset\n# class1\nx1 = np.random.normal(25,5,100)\ny1 = np.random.normal(25,5,100)\n\n# class2\nx2 = np.random.normal(55,5,100)\ny2 = np.random.normal(60,5,100)\n\n# class3\nx3 = np.random.normal(55,5,100)\ny3 = np.random.normal(15,5,100)\n\nx = np.concatenate((x1,x2,x3),axis = 0)\ny = np.concatenate((y1,y2,y3),axis = 0)\n\ndictionary = {\"x\":x,\"y\":y}\n\ndata = pd.DataFrame(dictionary)\n\nplt.scatter(x1,y1,color=\"black\")\nplt.scatter(x2,y2,color=\"black\")\nplt.scatter(x3,y3,color=\"black\")\nplt.show()","5da7d2c2":"# %% dendogram\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\nmerg = linkage(data,method=\"ward\")\ndendrogram(merg,leaf_rotation = 90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidean distance\")\nplt.show()","dea287d7":"# %% HC\nfrom sklearn.cluster import AgglomerativeClustering\n\nhiyerartical_cluster = AgglomerativeClustering(n_clusters = 3,affinity= \"euclidean\",linkage = \"ward\")\ncluster = hiyerartical_cluster.fit_predict(data)\n\ndata[\"label\"] = cluster\n\nplt.scatter(data.x[data.label == 0 ],data.y[data.label == 0],color = \"red\")\nplt.scatter(data.x[data.label == 1 ],data.y[data.label == 1],color = \"green\")\nplt.scatter(data.x[data.label == 2 ],data.y[data.label == 2],color = \"blue\")\n#plt.scatter(data.x[data.label == 3 ],data.y[data.label == 3],color = \"black\")\nplt.show()","52426f9c":"import pandas as pd\n# %% import twitter data\ndata = pd.read_csv(\"..\/input\/natural-language-process-nlp\/gender-classifier.csv\",encoding = \"latin1\")\ndata = pd.concat([data.gender,data.description],axis=1)\ndata.dropna(axis = 0,inplace = True)\ndata.gender = [1 if each == \"female\" else 0 for each in data.gender]\nprint(data.info())\nprint(data.head())\n#print(data.describe())","9c52d370":"import nltk # natural language tool kit\n#nltk.download(\"stopwords\")      # corpus diye bir kalsore indiriliyor\nfrom nltk.corpus import stopwords  # sonra ben corpus klasorunden import ediyorum\nimport re\ndescription_list = []\nfor description in data.description:\n    description = re.sub(\"[^a-zA-Z]\",\" \",description) # regular expression RE mesela \"[^a-zA-Z]\"\n    description = description.lower()   # buyuk harftan kucuk harfe cevirme\n    description = nltk.word_tokenize(description)# split kullan\u0131rsak \"shouldn't \" gibi kelimeler \"should\" ve \"not\" diye ikiye ayr\u0131lmaz ama word_tokenize() kullanirsak ayrilir\n    description = [ word for word in description if not word in set(stopwords.words(\"english\"))] # greksiz kelimeleri cikar\n    lemma = nltk.WordNetLemmatizer() # lemmatazation loved => love   gitmeyecegim = > git\n    description = [ lemma.lemmatize(word) for word in description]\n    description = \" \".join(description)\n    description_list.append(description)\n#print(description_list)","16170b31":"# %% bag of words\nfrom sklearn.feature_extraction.text import CountVectorizer # bag of words yaratmak icin kullandigim metot\nmax_features = 5000\n\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words = \"english\")\nsparce_matrix = count_vectorizer.fit_transform(description_list).toarray()  # x\n\n#print(\"en sik kullanilan {} kelimeler: {}\".format(max_features,count_vectorizer.get_feature_names()))\n","c09e7856":"# %%\ny = data.iloc[:,0].values   # male or female classes\nx = sparce_matrix\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.1, random_state = 42)","aec40191":"# %% naive bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n#%% prediction\ny_pred = nb.predict(x_test)\nprint(\"accuracy: \",nb.score(y_pred.reshape(-1,1),y_test))","8bb0449c":"from sklearn.datasets import load_iris\nimport pandas as pd\n# %%\niris = load_iris()\n\nfeature_names = iris.feature_names\ny = iris.target\n\ndata = pd.DataFrame(iris.data,columns = feature_names)\ndata[\"sinif\"] = y\n\nx = iris.data\nprint(data.info())\nprint(data.head())\n#print(data.describe())","5d4b942b":"#%% PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2, whiten= True )  # whitten = normalize\npca.fit(x)\n\nx_pca = pca.transform(x)\n\nprint(\"variance ratio: \", pca.explained_variance_ratio_)\nprint(\"sum: \",sum(pca.explained_variance_ratio_))","d9422662":"#%% 2D\ndata[\"p1\"] = x_pca[:,0]\ndata[\"p2\"] = x_pca[:,1]\n\ncolor = [\"red\",\"green\",\"blue\"]\n\nimport matplotlib.pyplot as plt\nfor each in range(3):\n    plt.scatter(data.p1[data.sinif == each],data.p2[data.sinif == each],color = color[each],label = iris.target_names[each])\n    \nplt.legend()\nplt.xlabel(\"p1\")\nplt.ylabel(\"p2\")\nplt.show()","07e6a72e":"from sklearn.datasets import load_iris\nimport pandas as pd\nimport numpy as np\n#%%\niris = load_iris()\nx = iris.data\ny = iris.target\n\ndata = pd.DataFrame(iris.data,columns = feature_names)\ndata[\"sinif\"] = y\n\nprint(data.info())\nprint(data.head())\n#print(data.describe())\n\n# %% normalization\nx = (x-np.min(x))\/(np.max(x)-np.min(x))","41dd1411":"# %% train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3)\n\n# knn model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 13) # n_neighbors = k\n\n# %% K fold CV K = 10\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = knn, X = x_train, y= y_train, cv = 10)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n\nknn.fit(x_train,y_train)\nprint(\"test accuracy: \",knn.score(x_test,y_test))","2d851963":"#Model Selection  grid search cross validation for knn\nfrom sklearn.model_selection import GridSearchCV\n\ngrid = {\"n_neighbors\":np.arange(1,50)}\nknn= KNeighborsClassifier()\n\nknn_cv = GridSearchCV(knn, grid, cv = 10)  # GridSearchCV\nknn_cv.fit(x,y)\n\n#%% print hyperparameter KNN algoritmasindaki K degeri\nprint(\"tuned hyperparameter K: \",knn_cv.best_params_)\nprint(\"tuned parametreye gore en iyi accuracy (best score): \",knn_cv.best_score_)","6092f946":"#Model Selection Grid search CV with logistic regression\nx = x[:100,:]\ny = y[:100] \n\nfrom sklearn.linear_model import LogisticRegression\ngrid = {\"C\":np.logspace(-3,3,7),\"penalty\":[\"l1\",\"l2\"]}  # l1 = lasso ve l2 = ridge\n\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg,grid,cv = 10)\nlogreg_cv.fit(x,y)\n\nprint(\"tuned hyperparameters: (best parameters): \",logreg_cv.best_params_)\nprint(\"accuracy: \",logreg_cv.best_score_)","51aa3780":"import pandas as pd\nimport os\nprint(os.listdir(\"..\/input\/movielens-20m-dataset\/\"))\n# import movie data set and look at columns\nmovie = pd.read_csv(\"..\/input\/movielens-20m-dataset\/movie.csv\")\nprint(movie.columns)\nmovie = movie.loc[:,[\"movieId\",\"title\"]]\nmovie.head(10)","f08e9228":"# import rating data and look at columsn\nrating = pd.read_csv(\"..\/input\/movielens-20m-dataset\/rating.csv\")\nprint(rating.columns)\n# what we need is that user id, movie id and rating\nrating = rating.loc[:,[\"userId\",\"movieId\",\"rating\"]]\nrating.head(10)","446f5340":"# then merge movie and rating data\ndata = pd.merge(movie,rating)\n# now lets look at our data \ndata.head(10)\nprint(data.shape)\ndata = data.iloc[:1000000,:]\n# lets make a pivot table in order to make rows are users and columns are movies. And values are rating\npivot_table = data.pivot_table(index = [\"userId\"],columns = [\"title\"],values = \"rating\")\npivot_table.head(10)","b3ee6770":"movie_watched = pivot_table[\"Bad Boys (1995)\"]\nsimilarity_with_other_movies = pivot_table.corrwith(movie_watched)  # find correlation between \"Bad Boys (1995)\" and other movies\nsimilarity_with_other_movies = similarity_with_other_movies.sort_values(ascending=False)\nsimilarity_with_other_movies.head()","39776fca":"**Polynomial Linear Regression**","7d2fd568":"**Natural Language Process (NLP)**","85f11661":"**Decision Tree Regression**","f2c4dbb3":"\n[Kaan Can Y\u0131lmaz'\u0131n](https:\/\/www.udemy.com\/user\/kaan-can-yilmaz\/)\n* [Machine Learning ve Python: A'dan Z'ye Makine \u00d6\u011frenmesi](https:\/\/www.udemy.com\/machine-learning-ve-python-adan-zye-makine-ogrenmesi-4)\n\nkursundan \u00f6\u011frendiklerimi denedi\u011fim ve derledi\u011fim kernelimdir.\n\n**\u0130\u00e7indekiler:**\n* Linear Regression\n* Multiple Linear Regression\n* Polynomial Linear Regression\n* Support Vector Regression\n* Decision Tree Regression\n* Random Forest Regression\n* K-Nearest Neighbour (KNN) Classification\n* Support Vector Machine (SVM) Classification\n* Naive Bayes Classification\n* Decision Tree Classification\n* Random Forest Classification\n* K-Means Clustering\n* Hierarchical Clustering\n* Natural Language Process (NLP)\n* Principal Component Analysis (PCA)\n* Model Selection\n* Recommendation Systems","ab76fa25":"**Naive Bayes Classification**","0a6d4b8b":"**Support Vector Regression**","766078fe":"**Linear Regression**","f819ba01":"**Decision Tree Classification**","e24e9c9f":"**K-Means Clustering**","0c89825c":"**K-Nearest Neighbour (KNN) Classification**","882d4c3d":"**Hierarchical Clustering**","7b804c70":"**Recommendation Systems**","59ec7bb1":"**Model Selection**","12b75687":"**Random Forest Regression**","75248428":"**Principal Component Analysis (PCA)**","9065b554":"**Multiple Linear Regression**","001b6549":"**Random Forest Classification**","09a7a996":"**Support Vector Machine (SVM) Classification**"}}