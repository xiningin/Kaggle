{"cell_type":{"c0789975":"code","a7735230":"code","8183838a":"code","711b7890":"code","8400c504":"code","d9fef36e":"code","588db8b4":"code","c0de7fe3":"code","de9f428d":"code","09da234d":"code","b267ca46":"code","186a9d15":"code","817ea902":"code","356b5b54":"code","a7445de2":"code","aad4ef84":"code","97d65ca7":"code","980b8864":"code","49bc307d":"code","7cffb22a":"code","7ba6270f":"code","75cc6fb0":"code","5b988a5b":"code","f81f96b8":"code","b87f2c3a":"code","f369211e":"code","0ac2c7f0":"code","29152214":"code","6080f7e5":"code","72fbf4ee":"code","202abcf6":"code","8d6f8f65":"code","d543cbf5":"code","4819c1cf":"code","5e70a1e4":"code","e839c60b":"code","bb637f75":"code","34e96806":"code","8b3ec24c":"code","d2d36cae":"code","6f783801":"code","648bb2eb":"code","8684106f":"code","17fb3164":"code","83510c25":"code","e2e6400a":"markdown","503e4e67":"markdown","4e5425ff":"markdown","045c3bca":"markdown","e70955d3":"markdown","4e2ca78c":"markdown","f2d426b3":"markdown"},"source":{"c0789975":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a7735230":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","8183838a":"print(train.shape)\nprint(test.shape)","711b7890":"# Concatenate both the training and the testing data together to make it easy to perform operatiions on both\ndata = pd.concat([train,test],axis = 0)\nprint(data.shape)","8400c504":"data.info()","d9fef36e":"# finding percentage of missing values in each feature that exceeds 20%\nprint((data.isnull().sum()\/len(data)*100).round(2)[(data.isnull().sum()\/len(data)*100) >=20])","588db8b4":"# Features with missing values more than 30% can be dropped and will hardly play any significance in the modelling\n\nfeatures_to_drop = ['Alley','FireplaceQu','PoolQC','Fence','MiscFeature','Id']\ndata = data.drop(features_to_drop,axis = 1)","c0de7fe3":"data.shape\n# We have dropped 6 collumns","de9f428d":"# Create new features based on intuition\ndata['TotalArea'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF'] + data['GrLivArea'] +data['GarageArea']\n\ndata['Bathrooms'] = data['FullBath'] + data['HalfBath']*0.5 \n\ndata['Year average']= (data['YearRemodAdd'] + data['YearBuilt'])\/2","09da234d":"# Collumns having at least one Nan value\nprint(\"Columns having at least one NaN Value:\" + str((data.isnull().sum()\/len(data)*100).round(2)[(data.isnull().sum()\/len(data)*100) >0.0].count()))","b267ca46":"print(\"Columns having at least one NaN Value:\" + str((data.isnull().sum()\/len(data)*100).round(2)[(data.isnull().sum()\/len(data)*100) >0.0].sort_values(ascending = False)))","186a9d15":"# Deal with the NaN values now!\nnan_val = (data.isnull().sum()\/len(data)*100).round(2)[(data.isnull().sum()\/len(data)*100) >0.0].sort_values(ascending = False).index.to_list()\nnan_val","817ea902":"# Finding Data types of the features\ndata_nan_val = (data[nan_val].dtypes ==object)\ndata_nan_val = data_nan_val.astype('str')\ndata_nan_val","356b5b54":"obj_nan_val = data_nan_val[data_nan_val == 'True'].index.to_list()\nnum_nan_val = data_nan_val[data_nan_val == 'False'].index.to_list()\nprint(\"Object data type Features:\\n\")\nprint(obj_nan_val)\nprint(\"\\n\")\nprint(\"Numeric data type Features:\\n\")\nprint(num_nan_val)","a7445de2":"num_nan_val.remove('SalePrice')","aad4ef84":"# USing LAbel Encoder we will not transform the object type inputs into numerical form\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nfor i in range(data.shape[1]):\n    if data.iloc[:,i].dtypes == object:\n        le.fit(list(data.iloc[:,i].values))\n        data.iloc[:,i] = le.transform(list(data.iloc[:,i].values))\n# Confirm that the code is working properly\nprint(data['GarageCond'].unique())","97d65ca7":"# Fill the NaN values with the mode and the median for the object_NaN and the numeric_NaN values respectively\ndata[num_nan_val] = data[num_nan_val].fillna(data[num_nan_val].median())\n\nfor j in data[obj_nan_val]:\n    data[j] = data[j].fillna(data[j].mode())  ","980b8864":"# Split the data back into test data and train data\ntest_data = data[data.SalePrice.isnull() == True]\ntest_data = test_data.drop(\"SalePrice\",axis = 1)\ntrain_data = data.dropna(axis = 0)","49bc307d":"print(\"Shape of training data after data cleaning:\")\nprint(train_data.shape)\nprint(\"Shape of testing data after data cleaning:\")\nprint(test_data.shape)\n# We have dropped out 6 columns","7cffb22a":"print(train_data.SalePrice.describe().round(2))\nplt.figure(figsize = (7,7))\nsns.distplot(train_data.SalePrice,color = 'red')","7ba6270f":"# Log transform the target variable for handling the outliers\ntrain_data[\"SalePrice\"] = np.log(train_data['SalePrice'])\nplt.figure(figsize = (7,7))\nsns.distplot(train_data.SalePrice,color = 'yellow')","75cc6fb0":"train_data.describe()","5b988a5b":"train_data.hist(figsize = (35,30),color = 'red');","f81f96b8":"data_corr = train_data.corr()\ndata_corr = data_corr.SalePrice\ndata_corr = data_corr.drop('SalePrice')","b87f2c3a":"strong_corr_feat = data_corr[abs(data_corr) > 0.3].round(2).sort_values(ascending = False)  \n# abs() to only take the magnitude\nprint(strong_corr_feat)\nlen(strong_corr_feat)\n# 29 features that have an impact on the selling prices","f369211e":"Y_train = train_data['SalePrice']\nX_train = train_data.drop('SalePrice',axis = 1)\nX_test = test_data","0ac2c7f0":"# Feature Importance using RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators = 80)\nrf.fit(X_train,Y_train)\nranking = np.argsort(-rf.feature_importances_)\n# np.argsort returns indices of features that are of maximum importance\nf, ax = plt.subplots(figsize=(18, 12))\nsns.barplot(x=rf.feature_importances_[ranking], y=X_train.columns.values[ranking], orient='h')\nax.set_xlabel(\"Feature Importance\")\nplt.tight_layout()\nplt.show()","29152214":"X_train = X_train.iloc[:,ranking[:29]]\nX_test = X_test.iloc[:,ranking[:29]]","6080f7e5":"# When dealing with pandas dataframes, You cannot directly slice up by using indices. 'df.iloc' allows slicing by using indices","72fbf4ee":"# Now we are going to find out how each and every feature influences the SalePrice in it's own unique way!\nfig = plt.figure(figsize = (12,9))\nfor i in np.arange(29):\n    ax = fig.add_subplot(5,6,i + 1)\n    sns.regplot(x = X_train.iloc[:,i],y=Y_train)\nplt.tight_layout()\nplt.show()","202abcf6":"sns.regplot(x = X_train.iloc[:,0],y=Y_train)\nplt.show()","8d6f8f65":"# Remove the outliers\nX = X_train\nX['SalePrice'] = Y_train\nX = X.drop(X[(X['YearBuilt']<1900) & (10.75>X['SalePrice']) & (X['SalePrice']>13.0)].index)\nX = X.drop(X[(X['TotalArea']>10000)&(10.5<X['SalePrice'])].index)\nX = X.drop(X[(X['GrLivArea']>3500) & (10.5>X['SalePrice'])].index)\nX = X.drop(X[(X['TotalBsmtSF']>2900) & (10.5>X['SalePrice'])].index)\nX = X.drop(X[(200>X['GarageArea']) & (X['GarageArea']>1100) & (X['SalePrice']<10.7)].index)\nY_train = X['SalePrice']\nX_train = X.drop(['SalePrice'],axis = 1)","d543cbf5":"import xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\n\nprint(\"Parameter optimization\")\nxgb_model = xgb.XGBRegressor()\nreg_xgb = GridSearchCV(xgb_model,\n                   {'max_depth': [2,4,6],\n                    'n_estimators': [50,100,200]}, verbose=1)\nreg_xgb.fit(X_train, Y_train)\nprint(reg_xgb.best_score_)\nprint(reg_xgb.best_params_)","4819c1cf":"from sklearn.linear_model import Lasso\nimport sklearn.model_selection as ms\nparameters= {'alpha':[0.0001,0.0009,0.001,0.01,0.1,1,10],\n            'max_iter':[100,500,1000]}\n\n\nlasso = Lasso()\nlasso_model = ms.GridSearchCV(lasso, param_grid=parameters, scoring='neg_mean_squared_error', cv=10)\nlasso_model.fit(X_train,Y_train)\n\nprint('The best value of Alpha is: ',lasso_model.best_params_)\nprint(lasso_model.best_score_)","5e70a1e4":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\ndef create_model(optimizer='adam'):\n    model = Sequential()\n    model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n    model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal'))\n\n    model.compile(loss='mean_squared_error', optimizer=optimizer)\n    return model\n\nmodel = KerasRegressor(build_fn=create_model, verbose=0)\n# define the grid search parameters\noptimizer = ['SGD','Adam']\nbatch_size = [10, 30, 50]\nepochs = [10, 50, 100]\nparam_grid = dict(optimizer=optimizer, batch_size=batch_size, epochs=epochs)\nreg_dl = GridSearchCV(estimator=model, param_grid=param_grid)\nreg_dl.fit(X_train, Y_train)\n\nprint(reg_dl.best_score_)\nprint(reg_dl.best_params_)","e839c60b":"model = Sequential()\nmodel.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(16, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(1, kernel_initializer='normal'))\nmodel.compile(optimizer='Adam',loss='mean_squared_error')\nmodel.fit(X_train,Y_train,batch_size = 10,epochs=100)","bb637f75":"pred2 = np.exp(model.predict(X_test))\npred2 = pred2.reshape(-1,)\npred2.shape","34e96806":"# SVR\nfrom sklearn.svm import SVR\n\nreg_svr = GridSearchCV(SVR(kernel='rbf', gamma=0.1), cv=5,\n                   param_grid={\"C\": [1e0, 1e1, 1e2, 1e3],\n                               \"gamma\": np.logspace(-2, 2, 5)})\nreg_svr.fit(X_train, Y_train)\n\nprint(reg_svr.best_score_)\nprint(reg_svr.best_params_)","8b3ec24c":"# second feature matrix\nX_train2 = pd.DataFrame( {'XGB': reg_xgb.predict(X_train),\n     'NN': reg_dl.predict(X_train).ravel(),\n     'SVR': reg_svr.predict(X_train),\n    })\nX_train2.head()","d2d36cae":"# prediction using the test set\nfrom sklearn import linear_model\n\nreg = linear_model.LinearRegression()\nreg.fit(X_train2, Y_train)\n\nX_test2 = pd.DataFrame( {'XGB': reg_xgb.predict(X_test),\n     'DL': reg_dl.predict(X_test).ravel(),\n     'SVR': reg_svr.predict(X_test),\n    })\n\n# Don't forget to convert the prediction back to non-log scale\ny_pred = np.exp(reg.predict(X_test2))\n","6f783801":"lasso = Lasso(alpha = 0.0001,max_iter = 1000)\nlasso.fit(X_train,Y_train)\npred1= np.exp(lasso.predict(X_test))","648bb2eb":"xgb1 = xgb.XGBRegressor(max_depth = 2,n_estimators=200)\nxgb1.fit(X_train,Y_train)\ny_pred = np.exp(xgb1.predict(X_test))","8684106f":"y_pred.shape\n","17fb3164":"test_Id = test['Id']\n","83510c25":"submission = pd.DataFrame({'Id':test_Id,\"SalePrice\":pred2})\nsubmission.to_csv('houseprice1.csv',index = False)","e2e6400a":"Use the pandas correlation function to find the correlation between different features","503e4e67":"## Co-relation between features","4e5425ff":"strong_corr_features involve only the first 29 features","045c3bca":"It will be suitable to take only the strongly co-related features instead of using all the features for training.\nThere are 29 strongly co-related features ","e70955d3":"The above plot shows us how important the different features are to us","4e2ca78c":"### Data Cleaning","f2d426b3":"#### Handling the outliers"}}