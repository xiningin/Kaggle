{"cell_type":{"a9a6128c":"code","a7470463":"code","f5ea5f94":"code","c14fc02f":"code","794e5528":"code","c5700592":"code","d44df10b":"code","16f5b718":"code","ecf5eca5":"code","f10a9c99":"code","03905b2f":"code","66bbcb19":"code","412d936c":"code","b730c36e":"code","9f68729b":"code","721331c8":"code","fc8cb10e":"code","f6574625":"code","ff9b8454":"code","a48576b7":"code","0e5f62c5":"code","272b3550":"code","c378daee":"code","04ad2e50":"code","aa255db2":"code","e5d4e3ff":"markdown","3964dfe0":"markdown","51b0fe69":"markdown","99369267":"markdown","6b2e1da2":"markdown","57c27d13":"markdown","cf8004a1":"markdown","0e715947":"markdown","53750857":"markdown","d05560e1":"markdown","8a1b91d1":"markdown","39211388":"markdown","2445429d":"markdown","7750dd46":"markdown","3e3865d9":"markdown","e62e3bdc":"markdown","ee3e0d7a":"markdown","53d4016a":"markdown","8cbe8c5b":"markdown","18f8755d":"markdown","aa8b3ccc":"markdown","e344020e":"markdown","0a44b5a9":"markdown","039e1b35":"markdown","3d38a2ca":"markdown","4e718b5b":"markdown","9359ee23":"markdown","04b4bb53":"markdown","03794aa2":"markdown","f5c45490":"markdown"},"source":{"a9a6128c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport plotly as py\nimport plotly.graph_objs as go\nfrom sklearn.cluster import KMeans\nimport warnings\nimport os\nwarnings.filterwarnings(\"ignore\")\npy.offline.init_notebook_mode(connected = True)","a7470463":"df = pd.read_csv('..\/input\/wine-customer-segmentation\/Wine.csv',sep = \",\", header = 0)","f5ea5f94":"df.head()","c14fc02f":"df.tail()","794e5528":"df.info()","c5700592":"df.drop('Customer_Segment', axis = 1, inplace = True)","d44df10b":"df.describe()","16f5b718":"df.info()\nsns.set_palette(\"GnBu_d\")\nplt.title(\"Missingess Map\")\nplt.rcParams['figure.figsize'] = (8.0, 5.0)\nsns.heatmap(df.isnull(), cbar=False)","ecf5eca5":"sns.set_palette(\"GnBu_d\")\nsns.pairplot(df)","f10a9c99":"plt.rcParams['figure.figsize'] = (15.0, 15.0)\nplt.title(\"Correlation Plot\")\nsns.heatmap(df.corr())","03905b2f":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\ndf_numeric = add_constant(df)\nVIF_frame = pd.Series([variance_inflation_factor(df_numeric.values, i) \n               for i in range(df_numeric.shape[1])], \n              index=df_numeric.columns).to_frame()\n\nVIF_frame.drop('const', axis = 0, inplace = True) \nVIF_frame.rename(columns={VIF_frame.columns[0]: 'VIF'},inplace = True)\nVIF_frame[~VIF_frame.isin([np.nan, np.inf, -np.inf]).any(1)]","66bbcb19":"df.drop('Flavanoids', axis = 1, inplace = True)","412d936c":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf_copy = df.copy()\ndf_copy = pd.DataFrame(scaler.fit_transform(df), \n                                      index=df.index,\n                                      columns=df.columns)\ndf_copy","b730c36e":"X1 = df_copy.iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state = 823, algorithm='elkan') )\n    algorithm.fit(X1)\n    inertia.append(algorithm.inertia_)\nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('WCSS')\nplt.title('Elbow Method Diagram')\nplt.show()","9f68729b":"algorithm = (KMeans(n_clusters = 3 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 823, algorithm='elkan') )\nalgorithm.fit(X1)\nlabels = algorithm.labels_\ncentroids = algorithm.cluster_centers_\ncentroids","721331c8":"df['Cluster'] = labels\ndf_copy['Cluster'] = labels\ndf.head()","fc8cb10e":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(random_state = 823)\ndf_dv = df.copy()\ndf_dv.drop('Cluster', axis = 1, inplace = True)\nrfc.fit(df_dv,df['Cluster'])\nfeatures = df_dv.columns.tolist()\nfeature_value = rfc.feature_importances_\nd = {'Features' : features, 'Values' : feature_value}\nfi = pd.DataFrame(d).sort_values('Values', ascending = False).reset_index()\nfi\nplt.rcParams['figure.figsize'] = (20.0, 5.0)\nax = sns.barplot(x=fi['Features'], y = fi['Values'], data = fi, palette=\"Blues_d\")","f6574625":"sns.pairplot(df[['Proline','OD280','Color_Intensity','Alcohol','Hue','Total_Phenols','Cluster']],palette = 'colorblind',hue='Cluster');","ff9b8454":"fig, axs = plt.subplots(ncols=3,nrows=3, figsize = (15,15))\nsns.scatterplot(x=\"Alcohol\", y=\"Proline\", hue=\"Cluster\",\n                     palette = 'colorblind', data = df, legend = False, s = 200, ax=axs[0][0])\nsns.scatterplot(x=\"Ash\", y=\"Proline\", hue=\"Cluster\",\n                     palette = 'colorblind', data = df, legend = False, s = 200, ax=axs[0][1])\nsns.scatterplot(x=\"Magnesium\", y=\"Proanthocyanins\", hue=\"Cluster\",\n                     palette = 'colorblind', data = df, legend = False, s = 200,  ax=axs[0][2])\nsns.scatterplot(x=\"Proanthocyanins\", y=\"Color_Intensity\", hue=\"Cluster\",\n                     palette = 'colorblind', data = df, legend = False, s = 200,  ax=axs[1][0])\nsns.scatterplot(x=\"Proline\", y=\"OD280\", hue=\"Cluster\",\n                     palette = 'colorblind', data = df, legend = False, s = 200,  ax=axs[1][1])\nsns.scatterplot(x=\"Nonflavanoid_Phenols\", y=\"Proline\", hue=\"Cluster\",\n                     palette = 'colorblind', data = df, legend = False, s = 200, ax=axs[1][2])\nsns.scatterplot(x=\"Color_Intensity\", y=\"Hue\", hue=\"Cluster\",\n                     palette = 'colorblind', data = df, legend = False, s = 200,  ax=axs[2][0])\nsns.scatterplot(x=\"Color_Intensity\", y=\"Proline\", hue=\"Cluster\",\n                     palette = 'colorblind', data = df, legend = False, s = 200,  ax=axs[2][1])\nsns.scatterplot(x=\"Ash_Alcanity\", y=\"Proline\", hue=\"Cluster\",\n                     palette = 'colorblind', data = df, legend = False, s = 200, ax=axs[2][2])","a48576b7":"fig, axs = plt.subplots(ncols=4,nrows=2, figsize = (20,10))\nsns.boxplot(x=\"Cluster\", y=\"Proline\", palette = 'colorblind', data = df, ax=axs[0][0])\nsns.stripplot(x='Cluster',y='Proline', palette = 'colorblind', data=df, jitter=True, ax=axs[0][0])\n\nsns.boxplot(x=\"Cluster\", y=\"Color_Intensity\", palette = 'colorblind', data = df, ax=axs[0][1])\nsns.stripplot(x='Cluster',y='Color_Intensity', palette = 'colorblind', data=df, jitter=True, ax=axs[0][1])\n\nsns.boxplot(x=\"Cluster\", y=\"OD280\", palette = 'colorblind', data = df, ax=axs[0][2])\nsns.stripplot(x='Cluster',y='OD280', palette = 'colorblind', data=df, jitter=True, ax=axs[0][2])\n\nsns.boxplot(x=\"Cluster\", y=\"Alcohol\", palette = 'colorblind', data = df, ax=axs[0][3])\nsns.stripplot(x='Cluster',y='Alcohol', palette = 'colorblind', data=df, jitter=True, ax=axs[0][3])\n\nsns.violinplot(x=\"Cluster\", y=\"Hue\", palette = 'colorblind', data = df, ax=axs[1][0])\nsns.violinplot(x=\"Cluster\", y=\"Total_Phenols\", palette = 'colorblind', data = df, ax=axs[1][1])\nsns.violinplot(x=\"Cluster\", y=\"Malic_Acid\", palette = 'colorblind', data = df, ax=axs[1][2])\nsns.violinplot(x=\"Cluster\", y=\"Ash_Alcanity\", palette = 'colorblind', data = df, ax=axs[1][3])","0e5f62c5":"from pandas.plotting import parallel_coordinates\nparallel_coordinates(df_copy[['Proline','OD280','Alcohol','Color_Intensity','Total_Phenols','Cluster']], \"Cluster\",  colormap = 'Accent')\nplt.ioff()","272b3550":"df_grouped = df_copy.groupby('Cluster',as_index=False).mean()\ndf_grouped = df_grouped[['Proline','OD280','Alcohol','Color_Intensity','Total_Phenols','Hue','Cluster']]\ndf_grouped","c378daee":"# Libraries\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom math import pi\n \ncategories = ['Proline','OD280','Alcohol','Color_Intensity','Hue','Total_Phenols']\nN = len(categories)\n \nangles = [n \/ float(N) * 2 * pi for n in range(N)]\nangles += angles[:1]\n \nax = plt.subplot(111, polar=True)\n\nax.set_theta_offset(pi \/ 2)\nax.set_theta_direction(-1)\n \nplt.xticks(angles[:-1], categories)\n \nax.set_rlabel_position(0)\nplt.yticks([-2,-1,0,1,2], [\"-2\",\"0\",\"1\",\"2\"], color=\"grey\", size=7)\nplt.ylim(-1.8,1.8)\n\nvalues=df_grouped.loc[0].drop('Cluster').values.flatten().tolist()\nvalues += values[:1]\nax.plot(angles, values, linewidth=1, linestyle='solid', label=\"Cluster 0\")\nax.fill(angles, values, 'b', alpha=0.2, color = \"#3475B9\")\n \nvalues=df_grouped.loc[1].drop('Cluster').values.flatten().tolist()\nvalues += values[:1]\nax.plot(angles, values, linewidth=1, linestyle='solid', label=\"Cluster 1\")\nax.fill(angles, values, 'r', alpha=0.5, color = \"#BAAC43\")\n\nvalues=df_grouped.loc[2].drop('Cluster').values.flatten().tolist()\nvalues += values[:1]\nax.plot(angles, values, linewidth=1, linestyle='solid', label=\"Cluster 1\")\nax.fill(angles, values, 'r', alpha=0.2, color = \"#4EA976\")\n \n# Add legend\nplt.rcParams['figure.figsize'] = (10.0, 10.0)","04ad2e50":"data = go.Scatter3d(\n    x= df['Proline'],\n    y= df['OD280'],\n    z= df['Alcohol'],\n    mode='markers',\n     marker=dict(\n        color= df['Cluster'],\n        size= 18,\n        opacity=0.8,\n        colorscale = 'Geyser'\n     )\n)\ndata = [data]\nlayout = go.Layout(\n    title= 'Clusters',\n    scene = dict(\n            xaxis = dict(title  = 'Proline'),\n            yaxis = dict(title  = 'OD280'),\n            zaxis = dict(title  = 'Alcohol')\n        )\n)\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig)","aa255db2":"data = go.Scatter3d(\n    x= df['OD280'],\n    y= df['Color_Intensity'],\n    z= df['Alcohol'],\n    mode='markers',\n     marker=dict(\n        color= df['Cluster'],\n        size= 18,\n        opacity=0.8,\n        colorscale = 'Geyser'\n     )\n)\ndata = [data]\nlayout = go.Layout(\n    title= 'Clusters',\n    scene = dict(\n            xaxis = dict(title  = 'OD280'),\n            yaxis = dict(title  = 'Color_Intensity'),\n            zaxis = dict(title  = 'Alcohol')\n        )\n)\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig)","e5d4e3ff":"##### There are no missing values","3964dfe0":"##### Pairplots using the top 6 important features for the clusters","51b0fe69":"## K-Means Clustering Implementation","99369267":"##### Box Plots and Violin Plots","6b2e1da2":"##### Let's see if there are features that are highly correlated","57c27d13":"##### Flavanoids has a high VIF, we should definitely drop it.","cf8004a1":"##### Take a peek at the Dataset","0e715947":"##### Drop the Customer_Segment","53750857":"### This kernel is intended for beginners starting their journey in Data Science \/ Machine Learning. Consider this a beginner's guide (step by step guide that can serve as template) to K-Means Clustering in Python. This contains minimal explanations \/ concept discussions, but I've mentioned the sources so you can refer to it. If you find this kernel useful, please do upvote :)","d05560e1":"### Import the Data","8a1b91d1":"### Feature Scaling","39211388":"## Data Preprocessing","2445429d":"##### Now let's see which features are important to the clustering we just did. Let's utilize the RandomForest Feature Importance Plot","7750dd46":"##### We get the intuition that the best number of clusters is 3 because the Within-Cluster-Sum of Squared Errors (WCSS) starts to diminish at k = 3.","3e3865d9":"##### Let's confirm by looking at the Variance Inflation Factor","e62e3bdc":"##### Looks like Total_Phenlos and Flavanoids are correlated. We'll try to drop one of them to avoid multicollinearity issue in our clustering model","ee3e0d7a":"##### That's it! Hope you learned something from this notebook\n\n##### References:\n* [StatquestVid](https:\/\/www.youtube.com\/watch?v=4b5d3muPQmA)\n* [Plotly 3D Plots](https:\/\/plotly.com\/python\/3d-scatter-plots\/)\n\nPlease note that all that I've written here is based on my own interpretation and does not represent the view\/s of the instructor\/s or author\/s from the above reference\/s","53d4016a":"##### 3D Plots","8cbe8c5b":"##### Parallel Coordinates from pandas.plotting","18f8755d":"### Check for Missing Values","aa8b3ccc":"##### Radar Chart","e344020e":"### Data Exploration","0a44b5a9":"### Clusters Initialization and Finding k.\n##### Using all the features:","039e1b35":"##### Pairplots","3d38a2ca":"K-Means Clustering is simply a clustering technique. The means in k means basically refers to averaging the data, that is finding the centroid. It\u2019s quite simple, the main idea is to first randomly assign a k number of data points in your feature space, where k is the number of clusters, then assigning all the points to whatever centroid cluster it is closest to (based on Euclidean distance). After doing so, the centroids of the newly formed clusters are recalculated, then assigning again the points to whatever centroid cluster it is closest to. The process is repeated until there are no more changes in the clustering. I suggest visiting Josh Starmer's [video](https:\/\/www.youtube.com\/watch?v=4b5d3muPQmA) if you find my explanation vague.\n\n\n\n![image.png](attachment:image.png)\n\n[Photo Reference](https:\/\/cmdlinetips.com\/2019\/05\/k-means-clustering-in-python\/)\n\n\n\nSo how is the optimal k selected? Currently there are two popular methods that helps in choosing the optimal value for k. The first one is the elbow method. The idea is to calculate the Within-Cluster-Sum of Squared Errors (WCSS) for different values of k, and choose the k for which WCSS starts to diminish. In the plot of WSS-versus-k, this is visible as an elbow. This can be easily implemented in python using the sklearn library.\n\nThe silhouette method is another method that can be combined with the elbow method for validation of the clustering. The idea is that the silhouette value measures how similar a point is to its own cluster compared to the other clusters. A high value is desirable and indicates that the point is placed in the correct cluster. This can also be easily implemented in python by plotting a graph of silhouette score vs k.\n\nElbow method and silhouette method are not alternatives rather they are used together to find the optimal value of k confidently.\n\nI've specifically chosen this wine data, because of its attributes. K-Means Clustering only works for purely numerical data. If we use Kmeans + one hot encoding it will increase the size of the dataset extensively if the categorical attributes have a large number of categories. This will make the Kmeans computationally costly. Also, the cluster means will make no sense since the 0 and 1 are not the real values of the data. \n\nFor this notebook, what I'll try to do is to drop the customer segment column, and cluster the remaining data.\n\nNow, let's get started!","4e718b5b":"##### Now let's try to visualize the segmentation","9359ee23":"##### Inserting the cluster labels to the dataframe","04b4bb53":"##### It's never bad to scale features","03794aa2":"## Overview","f5c45490":"##### Some zoomed in biplots"}}