{"cell_type":{"4103449c":"code","ca742371":"code","5b625b93":"code","c8c95711":"code","3a35d360":"code","54534570":"code","ec4d77db":"code","79988a07":"code","5e582495":"code","28e8f456":"code","3cc26005":"code","0d878636":"code","53f8f4f6":"code","56856dd7":"code","12acb9e2":"code","45a583b7":"code","a0051ae3":"code","dbad6f08":"code","6a9d8ffe":"code","bed0c46a":"code","792d2643":"code","a54f40e3":"code","4ec752cd":"code","e49098fe":"code","f8dcf5a6":"code","1ea7464f":"code","9ba1008f":"code","2a2aff12":"code","2f5c635e":"code","2829537b":"code","06e09340":"code","898da6fc":"code","4e856689":"code","4d4a7a74":"markdown","687b0e1e":"markdown","5c15e510":"markdown","e2b37005":"markdown","94082891":"markdown","70eb473d":"markdown","5bfc31f2":"markdown","2bf5c316":"markdown","b8c9e8fe":"markdown","7128df27":"markdown","ee55e557":"markdown","d2a8799a":"markdown","c1cb5128":"markdown","5020d5a1":"markdown","a655ab4d":"markdown","b15be371":"markdown","f8d580cd":"markdown","6b2bb12b":"markdown","c0052872":"markdown","2e4f4106":"markdown","25012659":"markdown","915dbda1":"markdown","9f2a430d":"markdown","5c5f40a4":"markdown","b751cf71":"markdown","9ca7f42b":"markdown","8a6ee37d":"markdown","8519c099":"markdown","fd30d5bb":"markdown","d28c7b38":"markdown","ce5b0e4a":"markdown","99891688":"markdown","851df17c":"markdown","f99cbfcd":"markdown","7b0229a7":"markdown","41557c88":"markdown","994b6d59":"markdown","c97fe92d":"markdown","d3c61341":"markdown","8492fab1":"markdown","75b76513":"markdown"},"source":{"4103449c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ca742371":"iris=pd.read_csv('..\/input\/iris-flower-dataset\/IRIS.csv')","5b625b93":"iris.head()","c8c95711":"iris['species'].unique()","3a35d360":"iris.describe(include='all')","54534570":"iris.info()","ec4d77db":"iris.drop(columns=\"species\",inplace=False)","79988a07":"iris.isnull().sum()","5e582495":"import missingno as msno\nmsno.bar(iris,figsize=(7,6),color='yellow')\nplt.show()","28e8f456":"g=sns.relplot(x='sepal_length',y='sepal_width',data=iris,hue='species',style='species')\ng.fig.set_size_inches(8,5)\nplt.show()","3cc26005":"g=sns.relplot(x='petal_length',y='petal_width',data=iris,hue='species',style='species')\ng.fig.set_size_inches(10,5)\nplt.show()","0d878636":"sns.pairplot(iris,hue=\"species\")\nplt.show()","53f8f4f6":"plt.figure(figsize=(12,10))\nplt.subplot(2,2,1)\nsns.boxplot(x='species',y='petal_length',data=iris)\nplt.subplot(2,2,2)\nsns.boxplot(x='species',y='petal_width',data=iris)\nplt.subplot(2,2,3)\nsns.boxplot(x='species',y='sepal_length',data=iris)\nplt.subplot(2,2,4)\nsns.boxplot(x='species',y='sepal_width',data=iris)\nplt.show()","56856dd7":"plt.subplots(figsize=(10,7))\nsns.boxplot(data=iris).set_title(\"Distribution of Sepal_length, Sepal_width, petal_length and petal_width of 3 flowers\")\nplt.show()","12acb9e2":"plt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nsns.violinplot(x='species',y='petal_length',data=iris)\nplt.subplot(2,2,2)\nsns.violinplot(x='species',y='petal_width',data=iris)\nplt.subplot(2,2,3)\nsns.violinplot(x='species',y='sepal_length',data=iris)\nplt.subplot(2,2,4)\nsns.violinplot(x='species',y='sepal_width',data=iris)\nplt.show()","45a583b7":"plt.subplots(figsize=(10,7))\nsns.violinplot(data=iris)\nsns.swarmplot( data=iris)\nplt.show()","a0051ae3":"iris.plot.area(y=['sepal_length','sepal_width','petal_length','petal_width'],alpha=0.4,figsize=(12, 6));","dbad6f08":"iris.corr()","6a9d8ffe":"plt.subplots(figsize = (8,8))\nsns.heatmap(iris.corr(),annot=True,fmt=\"f\").set_title(\"Corelation of attributes (petal length,width and sepal length,width) among Iris species\")\nplt.show()","bed0c46a":"X=iris.iloc[:,0:4].values\ny=iris.iloc[:,4].values","792d2643":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)","a54f40e3":"\n#Metrics\nfrom sklearn.metrics import make_scorer, accuracy_score,precision_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score ,precision_score,recall_score,f1_score\n\n#Model Select\nfrom sklearn.model_selection import KFold,train_test_split,cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import  LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import linear_model\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","4ec752cd":"#Train and Test split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\n","e49098fe":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\nY_prediction = random_forest.predict(X_test)\naccuracy_rf=round(accuracy_score(y_test,Y_prediction)* 100, 2)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\n\n\ncm = confusion_matrix(y_test, Y_prediction)\naccuracy = accuracy_score(y_test,Y_prediction)\nprecision =precision_score(y_test, Y_prediction,average='micro')\nrecall =  recall_score(y_test, Y_prediction,average='micro')\nf1 = f1_score(y_test,Y_prediction,average='micro')\nprint('Confusion matrix for Random Forest\\n',cm)\nprint('accuracy_random_Forest : %.3f' %accuracy)\nprint('precision_random_Forest : %.3f' %precision)\nprint('recall_random_Forest : %.3f' %recall)\nprint('f1-score_random_Forest : %.3f' %f1)\n","f8dcf5a6":"logreg = LogisticRegression(solver= 'lbfgs',max_iter=400)\nlogreg.fit(X_train, y_train)\nY_pred = logreg.predict(X_test)\naccuracy_lr=round(accuracy_score(y_test,Y_pred)* 100, 2)\nacc_log = round(logreg.score(X_train, y_train) * 100, 2)\n\n\ncm = confusion_matrix(y_test, Y_pred,)\naccuracy = accuracy_score(y_test,Y_pred)\nprecision =precision_score(y_test, Y_pred,average='micro')\nrecall =  recall_score(y_test, Y_pred,average='micro')\nf1 = f1_score(y_test,Y_pred,average='micro')\nprint('Confusion matrix for Logistic Regression\\n',cm)\nprint('accuracy_Logistic Regression : %.3f' %accuracy)\nprint('precision_Logistic Regression : %.3f' %precision)\nprint('recall_Logistic Regression: %.3f' %recall)\nprint('f1-score_Logistic Regression : %.3f' %f1)","1ea7464f":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\nY_pred = knn.predict(X_test) \naccuracy_knn=round(accuracy_score(y_test,Y_pred)* 100, 2)\nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\n\ncm = confusion_matrix(y_test, Y_pred)\naccuracy = accuracy_score(y_test,Y_pred)\nprecision =precision_score(y_test, Y_pred,average='micro')\nrecall =  recall_score(y_test, Y_pred,average='micro')\nf1 = f1_score(y_test,Y_pred,average='micro')\nprint('Confusion matrix for KNN\\n',cm)\nprint('accuracy_KNN : %.3f' %accuracy)\nprint('precision_KNN : %.3f' %precision)\nprint('recall_KNN: %.3f' %recall)\nprint('f1-score_KNN : %.3f' %f1)","9ba1008f":"plt.subplots(figsize=(20,5))\na_index=list(range(1,50))\na=pd.Series()\nx=range(1,50)\n#x=[1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,50)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(X_train, y_train) \n    prediction=model.predict(X_test)\n    a=a.append(pd.Series(accuracy_score(y_test,prediction)))\nplt.plot(a_index, a,marker=\"*\")\nplt.xticks(x)\nplt.show()","2a2aff12":"gaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\nY_pred = gaussian.predict(X_test) \naccuracy_nb=round(accuracy_score(y_test,Y_pred)* 100, 2)\nacc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)\n\ncm = confusion_matrix(y_test, Y_pred)\naccuracy = accuracy_score(y_test,Y_pred)\nprecision =precision_score(y_test, Y_pred,average='micro')\nrecall =  recall_score(y_test, Y_pred,average='micro')\nf1 = f1_score(y_test,Y_pred,average='micro')\nprint('Confusion matrix for Naive Bayes\\n',cm)\nprint('accuracy_Naive Bayes: %.3f' %accuracy)\nprint('precision_Naive Bayes: %.3f' %precision)\nprint('recall_Naive Bayes: %.3f' %recall)\nprint('f1-score_Naive Bayes : %.3f' %f1)","2f5c635e":"linear_svc = LinearSVC(max_iter=4000)\nlinear_svc.fit(X_train, y_train)\nY_pred = linear_svc.predict(X_test)\naccuracy_svc=round(accuracy_score(y_test,Y_pred)* 100, 2)\nacc_linear_svc = round(linear_svc.score(X_train, y_train) * 100, 2)\n\ncm = confusion_matrix(y_test, Y_pred)\naccuracy = accuracy_score(y_test,Y_pred)\nprecision =precision_score(y_test, Y_pred,average='micro')\nrecall =  recall_score(y_test, Y_pred,average='micro')\nf1 = f1_score(y_test,Y_pred,average='micro')\nprint('Confusion matrix for SVC\\n',cm)\nprint('accuracy_SVC: %.3f' %accuracy)\nprint('precision_SVC: %.3f' %precision)\nprint('recall_SVC: %.3f' %recall)\nprint('f1-score_SVC : %.3f' %f1)","2829537b":"decision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, y_train)  \nY_pred = decision_tree.predict(X_test) \naccuracy_dt=round(accuracy_score(y_test,Y_pred)* 100, 2)\nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\n\ncm = confusion_matrix(y_test, Y_pred)\naccuracy = accuracy_score(y_test,Y_pred)\nprecision =precision_score(y_test, Y_pred,average='micro')\nrecall =  recall_score(y_test, Y_pred,average='micro')\nf1 = f1_score(y_test,Y_pred,average='micro')\nprint('Confusion matrix for DecisionTree\\n',cm)\nprint('accuracy_DecisionTree: %.3f' %accuracy)\nprint('precision_DecisionTree: %.3f' %precision)\nprint('recall_DecisionTree: %.3f' %recall)\nprint('f1-score_DecisionTree : %.3f' %f1)","06e09340":"from sklearn.tree import plot_tree\nplt.figure(figsize = (15,10))\nplot_tree(decision_tree.fit(X_train, y_train)  ,filled=True)\nplt.show()","898da6fc":"results = pd.DataFrame({\n    'Model': [ 'KNN', \n              'Logistic Regression', \n              'Random Forest',\n              'Naive Bayes',  \n              ' Support Vector Machine', \n              'Decision Tree'],\n    'Score': [ acc_knn,\n              acc_log, \n              acc_random_forest,\n              acc_gaussian,  \n              acc_linear_svc,\n              acc_decision_tree],\n    \"Accuracy_score\":[accuracy_knn,\n                      accuracy_lr,\n                      accuracy_rf,\n                      accuracy_nb,\n                      accuracy_svc,\n                      accuracy_dt\n                     ]})\nresult_df = results.sort_values(by='Accuracy_score', ascending=False)\nresult_df = result_df.reset_index(drop=True)\nresult_df.head(9)","4e856689":"plt.subplots(figsize=(12,8))\nax=sns.barplot(x='Model',y=\"Accuracy_score\",data=result_df)\nlabels = (result_df[\"Accuracy_score\"])\n# add result numbers on barchart\nfor i, v in enumerate(labels):\n    ax.text(i, v+1, str(v), horizontalalignment = 'center', size = 15, color = 'black')","4d4a7a74":"### 3.5 Linear Support Vector Machine:\nSupport Vector Machine\u201d (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well\n![](https:\/\/blog-c7ff.kxcdn.com\/blog\/wp-content\/uploads\/2017\/02\/Margin.png)","687b0e1e":"Now, when we train any algorithm, the number of features and their correlation plays an important role. If there are features and many of the features are highly correlated, then training an algorithm with all the featues will reduce the accuracy. Thus features selection should be done carefully. This dataset has less featues but still we will see the correlation.","5c15e510":"### Splitting The Data into Training And Testing Dataset\n![](https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2018\/08\/1-16.png)","e2b37005":"### The variables are:\n![](https:\/\/i.imgur.com\/PQqYGaW.png)\n* sepal_length: Sepal length, in centimeters, used as input.\n* sepal_width: Sepal width, in centimeters, used as input.\n* petal_length: Petal length, in centimeters, used as input.\n* petal_width: Petal width, in centimeters, used as input.\n* class: Iris Setosa, Versicolor, or Virginica, used as the target.","94082891":"from the graph we can see the scatter plot between the any two features and the distributions.\nfrom the distributions above peatl length is separating the iris setosa from remaining .\nfrom plot between petal length and petal width we can separate the flowers","70eb473d":"# Contents\n### Data Preprocessing\n* Include Libraries\n* Import DataSet\n* Handle Missing Value\n\n### Data Visualization\n* Scatterplot\n* Pairplot\n* Barplot\n* Violin\n* Areaplot\n* Correlation\n\nFeature Engineering\n\n### Machine learning Model\n* Logistic Regression\n* Random Forest Classifier\n* Naive Bayes\n* KNN\t\n* Decision Tree\t\n* Support Vector Machine\n","5bfc31f2":"## Which is the best Model ?","2bf5c316":"As we see best Model is given by Naive Bayes(100% Accuracy).","b8c9e8fe":"There is not any missing  value in this dataset","7128df27":"### If you have reached till here, So i hope you liked my Analysis.\n\nDon't forget to **upvote** if you like it!.\n\nI'm a beginner and any suggestion in the comment box is highly appreciated.\n\nIf you have any doubt reagrding any part of the notebook, feel free to comment your doubt in the comment box.\n\nThank you!!","ee55e557":"### Importing Iris data set","d2a8799a":"### 3.6 Decision Tree:\nA decision tree is a flowchart-like structure in which each internal node represents a test on a feature (e.g. whether a coin flip comes up heads or tails) , each leaf node represents a class label (decision taken after computing all features) and branches represent conjunctions of features that lead to those class labels. The paths from root to leaf represent classification rules.\n![](https:\/\/miro.medium.com\/max\/1000\/1*LMoJmXCsQlciGTEyoSN39g.jpeg)","c1cb5128":"## Label encoding\n![](https:\/\/miro.medium.com\/max\/772\/1*QQe-4476Oy3_dI1vhb3dDg.png)\nAs we can see labels are categorical. KNeighborsClassifier does not accept string labels. We need to use LabelEncoder to transform them into numbers. Iris-setosa correspond to 0, Iris-versicolor correspond to 1 and Iris-virginica correspond to 2.","5020d5a1":"Observation--->\n\nThe Sepal Width and Length are not correlated The Petal Width and Length are highly correlated\n\nWe will use all the features for training the algorithm and check the accuracy.\n","a655ab4d":"Above is the graph showing the accuracy for the KNN models using different values of n.","b15be371":"### Displaying data","f8d580cd":"### 3.3 K Nearest Neighbor:\nK-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique.\n\nK-NN algorithm assumes the similarity between the new case\/data and available cases and put the new case into the category that is most similar to the available categories.\n\nK-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.\n![](https:\/\/www.kdnuggets.com\/wp-content\/uploads\/rapidminer-knn-image1.jpg)","6b2bb12b":"# 3. Building Machine Learning Models","c0052872":"\n <center><h1 style=\"color:red\">Don't forget to upvote if you like it! :)<\/h1><\/center>","2e4f4106":"### Observations:\nThis was expected as we saw in the heatmap above that the correlation between the Sepal Width and Length was very low whereas the correlation between Petal Width and Length was very high.\nThus we have just implemented some of the common Machine Learning. Since the dataset is small with very few features.","25012659":"## 2.1 Scatterplot\nA scatter plot is a two-dimensional data visualization that uses dots to represent the values obtained for two different variables \u2014 one plotted along the x-axis and the other plotted along the y-axis\nwe can plot the scatter plot between any two features.\ni\u2019am taking an example of petal length and petal width.","915dbda1":"## Checking if there are any missing values\n![](https:\/\/blogs.worldbank.org\/sites\/default\/files\/opendata\/missing-data.jpg)","9f2a430d":"### 3.1 Random Forest:\nRandom forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model\u2019s prediction.\n\nA large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.\n![](https:\/\/miro.medium.com\/max\/1170\/1*58f1CZ8M4il0OZYg2oRN4w.png)","5c5f40a4":"Now we will train several Machine Learning models and compare their results. Note that because the dataset does not provide labels for their testing-set, we need to use the predictions on the training set to compare the algorithms with each other.","b751cf71":"This data set has three varities of Iris plant.","9ca7f42b":"## Dividing data into features and labels\n![](https:\/\/miro.medium.com\/max\/1002\/1*68H8EsCwfqJNxzYdPYtEDw.png)\nAs we can see dataset contain six columns: Id, SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm and Species. The actual features are described by columns 1-4. Last column contains labels of samples. Firstly we need to split data into two arrays: X (features) and y (labels).","8a6ee37d":"## 2.3 BoxPlot \n boxplot is a standardized way of displaying the distribution of data based on a five number summary (\u201cminimum\u201d, first quartile (Q1), median, third quartile (Q3), and \u201cmaximum\u201d). It can tell you about your outliers and what their values are. It can also tell you if your data is symmetrical, how tightly your data is grouped, and if and how your data is skewed.\n![](https:\/\/miro.medium.com\/proxy\/1*2c21SkzJMf3frPXPAR_gZA.png) ","8519c099":"## 2.5 Area Plot \nArea Plot gives us a visual representation of Various dimensions of Iris flower and their range in dataset.","fd30d5bb":"# About Iris dataset\n![](https:\/\/www.oreilly.com\/library\/view\/python-artificial-intelligence\/9781789539462\/assets\/462dc4fa-fd62-4539-8599-ac80a441382c.png)\nThe iris dataset contains the following data\n* 50 samples of 3 different species of iris (150 samples total)\n* Measurements: sepal length, sepal width, petal length, petal width\n* The format for the data: (sepal length, sepal width, petal length, petal width)","d28c7b38":"## 2.6 Correlation\n![](https:\/\/www.mathsisfun.com\/data\/images\/correlation-examples.svg)","ce5b0e4a":"### Let's check the accuracy for various values of n for K-Nearest nerighbours","99891688":"### 3.2 Logistic Regression:\nLogistic Regression is a Machine Learning algorithm which is used for the classification problems, it is a predictive analysis algorithm and based on the concept of probability.\n\nWe can call a Logistic Regression a Linear Regression model but the Logistic Regression uses a more complex cost function, this cost function can be defined as the \u2018Sigmoid function\u2019 or also known as the \u2018logistic function\u2019 instead of a linear function.\n![](https:\/\/miro.medium.com\/max\/570\/1*50TdLe6f_AW8wnBBkyLYgw.png)","851df17c":"# 2. Data Visualization","f99cbfcd":"As we can see above data distribution of data points in each class is equal so Iris is a balanced dataset as the number of data points for every class is 50.","7b0229a7":"## Removing the unneeded column","41557c88":"## 2.2 Pairplot\nPair Plots are a really simple (one-line-of-code simple!) way to visualize relationships between each variable. It produces a matrix of relationships between each variable in your data for an instant examination of our data.\npair plot gives scatter plot of different features.\npair plot for iris data set.","994b6d59":"## 2.4 Violin\nViolin Plot is a method to visualize the distribution of numerical data of different variables. It is similar to Box Plot but with a rotated plot on each side, giving more information about the density estimate on the y-axis.\nThe density is mirrored and flipped over and the resulting shape is filled in, creating an image resembling a violin. The advantage of a violin plot is that it can show nuances in the distribution that aren\u2019t perceptible in a boxplot. On the other hand, the boxplot more clearly shows the outliers in the data.","c97fe92d":"As we can see that the Petal Features are giving a better cluster division compared to the Sepal features. This is an indication that the Petals can help in better and accurate Predictions over the Sepal.","d3c61341":"### Importing pandas,numpy,matplotlib and Seaborn module ","8492fab1":"# 1. Data Preprocessing","75b76513":"### 3.4 Gaussian Naive Bayes:\nNaive Bayes is a classification algorithm for binary (two-class) and multi-class classification problems. The technique is easiest to understand when described using binary or categorical input values.\n\nIt is called naive Bayes or idiot Bayes because the calculation of the probabilities for each hypothesis are simplified to make their calculation tractable. Rather than attempting to calculate the values of each attribute value P(d1, d2, d3|h), they are assumed to be conditionally independent given the target value and calculated as P(d1|h) * P(d2|H) and so on.\n\nThis is a very strong assumption that is most unlikely in real data, i.e. that the attributes do not interact. Nevertheless, the approach performs surprisingly well on data where this assumption does not hold.\n![](https:\/\/miro.medium.com\/max\/1200\/0*qFuHAV7Vd09064q-.jpeg)"}}