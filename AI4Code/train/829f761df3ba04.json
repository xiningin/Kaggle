{"cell_type":{"c2984251":"code","6f55dea2":"code","9e8dbf18":"code","04153c46":"code","c8b4d64e":"code","e98ab359":"code","c444b79d":"code","c3ff35fe":"code","3e91d318":"code","8dfd0b03":"code","f5929abe":"code","65123f6a":"code","19965366":"code","783f1a97":"code","f6d0aabe":"code","2b7267da":"code","fce0816c":"code","4007cab7":"code","92b87bc2":"code","88aee718":"code","ff4ca064":"code","1527e102":"code","486dd2b8":"code","47f952e5":"code","6e609ae9":"code","41bd615c":"code","d4af074c":"code","bf8eab79":"code","c7ded4a3":"code","eb8bd9dc":"code","a35c66e4":"code","b1b9bef2":"code","9b401177":"code","36c50bfd":"code","335fcfee":"code","2e8b29ab":"code","7a321ade":"code","a6f8b8dc":"code","27742ba6":"code","a9df5377":"code","efad95d7":"code","5fc1a5f2":"code","741f2e8a":"code","4a794389":"code","10c48ae8":"code","f5b0078c":"code","fcdc6475":"code","1cca68e7":"code","e4a94a6d":"code","eaf20b13":"code","50e065e1":"code","d113fa99":"code","5466ccc7":"code","86280144":"code","fa98b971":"code","0a798a10":"markdown","9c2c2a3c":"markdown","5c7c44ee":"markdown","a02aa9b7":"markdown","80aa108b":"markdown","ac5cc305":"markdown","a71159cd":"markdown","cc52e8db":"markdown","6a6992a8":"markdown","ec47d11c":"markdown","1e86372f":"markdown","01e9d298":"markdown","62c419e0":"markdown","ca816fa9":"markdown","1c9ff3a9":"markdown","ed592e76":"markdown","0d322096":"markdown","eec8a849":"markdown","10c74fce":"markdown","bdd86b4d":"markdown","890d8c7a":"markdown","ee336a1b":"markdown","db4b2321":"markdown","23b5f824":"markdown","b46d37ec":"markdown","98dc5cfd":"markdown","d32226a4":"markdown","afef5f83":"markdown","a1512c39":"markdown","ec02e8a7":"markdown","707ded4b":"markdown","7ac553ee":"markdown","1276b94f":"markdown","77d4a5fc":"markdown","38823f6c":"markdown","f0d8dfc4":"markdown","dd1c720f":"markdown","500fa703":"markdown","41ef00a5":"markdown","68459181":"markdown","54ac68f5":"markdown","3e5bc135":"markdown","080ffed4":"markdown","0d5bf8d7":"markdown","d5682447":"markdown","b4023587":"markdown","0820e4af":"markdown","459be897":"markdown","c0d4b761":"markdown","a4558d91":"markdown"},"source":{"c2984251":"#from pandas import read_csv\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pandas.plotting import scatter_matrix\nfrom matplotlib import pyplot","6f55dea2":"df = pd.read_csv('..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv')\ndf.head()","9e8dbf18":"columns = df.columns\ncolumns, len(columns)","04153c46":"df.shape","c8b4d64e":"# checking data types\ndf.info()","e98ab359":"# checking # of NaN in each column\nif(df.isnull().values.any() == True):\n    for name in columns:\n        print('%s: %i' %(name, df[name].isnull().sum()) )","c444b79d":"df.drop(columns = ['id', 'host_id', 'host_name', 'last_review'], inplace = True)\ndf.head()","c3ff35fe":"df_1 = df[df['availability_365'] > 0]\ndf_1","3e91d318":"df_1['reviews_per_month'] = df['reviews_per_month'].fillna(0)\ndf_1","8dfd0b03":"df = df_1.copy()\ndf['name'] = df_1['name'].fillna('to') # we are using 'to' bc it is a stopword -> explained later","f5929abe":"# checking our NaN values again\ndf.isnull().values.any() ","65123f6a":"df.drop(columns = ['latitude', 'longitude'], inplace = True)\ndf.head()","19965366":"df['room_type'].unique()","783f1a97":"df['neighbourhood_group'].unique()","f6d0aabe":"df['neighbourhood'].unique()","2b7267da":"# taking columns with an object type only\nobject_cols = df.select_dtypes(include = [object])\n\n# dropping the name column bc we do not want to perform encoding on it\nobject_cols.drop(columns = ['name'], inplace = True)\nobject_cols.head()","fce0816c":"from sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nobject_cols = encoder.fit_transform(object_cols)","4007cab7":"# creating a function to remove stop words to decrease our runtime\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \ndef stop_words(text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(text)\n    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n    return filtered_sentence","92b87bc2":"from langdetect import detect\nfrom langdetect.lang_detect_exception import LangDetectException\n\ndescription = df['name'].astype(str)\n\nerrors = []\nnot_english = []\nenglish = 0\nother = 0\nfor title in description:\n    try:\n        #print(title)\n        if detect(title) == 'en':\n            english += 1\n        else:\n            not_english += [title]\n            other += 1\n    except LangDetectException:\n        other += 1\n        errors += [title]\nenglish, other","88aee718":"not_english","ff4ca064":"def grammar(string):\n    # add whatever else you think you might have to account for\n    result = str(string)\n    result = result.replace('\/', ' ')\n    result = result.replace('*', ' ')\n    result = result.replace('&', ' ')\n    result = result.replace('>', ' ')\n    result = result.replace('<', ' ')\n    result = result.replace('-', ' ')\n    result = result.replace('...', ' ')\n    result = result.replace('@', ' ')\n    result = result.replace('#', ' ')\n    result = result.replace('-', ' ')\n    result = result.replace('$', ' ')\n    result = result.replace('%', ' ')\n    result = result.replace('+', ' ')\n    result = result.replace('=', ' ')\n    \n    return result","1527e102":"import langid\ndesc = description.apply(grammar)\nlangid.set_languages(['en', 'es', 'zh'])\nnot_en = []\nnot_en_index = []\ni = 0\nfor title in desc:\n    if langid.classify(title)[0] != 'en':\n        not_en += [title]\n        not_en_index += [desc.index[i]]\n    i += 1\n    \nlen(not_en)","486dd2b8":"not_en","47f952e5":"for i in desc.index:\n    if desc[i] in not_en:\n        desc[i] = ''","6e609ae9":"description = desc.apply(stop_words)\ndescription ","41bd615c":"# creating a function to convert a list of strings to single string\ndef to_single_string(list_of_strings):\n    result = ''\n    for string in list_of_strings:\n        result += ' ' +string\n    return result\n\n# applying above function\ndescription = description.apply(to_single_string)\ndescription","d4af074c":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\nsentiment_analyzer = SentimentIntensityAnalyzer()\n\ndef sentiment_score(string):\n        result = sentiment_analyzer.polarity_scores(string)\n        return result","bf8eab79":"sentiment = description.apply(sentiment_score)\nsentiment","c7ded4a3":"# method 1\ndef compound_score(sent):\n    return sent.get('compound')\n\nsentiment_M1 = sentiment.apply(compound_score)\nsentiment_M1","eb8bd9dc":"# method 2\ndef polarity(sent):\n    compound = sent.get('compound')\n    if(compound >= 0.05):\n        return 1\n    elif(compound <= -0.05):\n        return -1\n    return 0\n\nsentiment_M2 = sentiment.apply(polarity)\nsentiment_M2","a35c66e4":"df.columns","b1b9bef2":"# creating temporary df\ntemporary = pd.DataFrame()\ntemporary['location'] = df['neighbourhood']\ntemporary['sent_M1'] = sentiment_M1.to_frame()\ntemporary['sent_M2'] = sentiment_M2.to_frame()\ntemporary['name'] = description.to_frame()\ntemporary","9b401177":"# removing rows that are in not_en, with not_en_index (which was obtained in the same block of code as not_en)\ntemporary.drop(index = not_en_index, inplace = True)\ntemporary","36c50bfd":"neighborhood_sent = temporary.groupby(['location']).mean()\nneighborhood_sent","335fcfee":"def polarity_range(score):\n    if(score >= 0.05):\n        return 1\n    elif(score <= -0.05):\n        return -1\n    return 0\n\n\n# obtaining our different sentiment scores\nnhood_sent_M1 = neighborhood_sent['sent_M1']\nnhood_sent_M2 = neighborhood_sent['sent_M2']\n\n# applying our function to sent_M2 to turn it into -1, 0 and +1 only\nnhood_sent_M2 = nhood_sent_M2.apply(polarity_range)","2e8b29ab":"nhood_sent_M1","7a321ade":"sent_m1 = sentiment_M1.copy()\nsent_m2 = sentiment_M2.copy()\n\nfor index in not_en_index:\n    sent_m1[index] = nhood_sent_M1[df['neighbourhood'][index]]\n    sent_m2[index] = nhood_sent_M2[df['neighbourhood'][index]]","a6f8b8dc":"df.head()","27742ba6":"df.drop(columns = ['number_of_reviews',\t'reviews_per_month'], inplace = True)\ndf.head()","a9df5377":"df.drop(columns = ['name', 'neighbourhood_group', 'neighbourhood', 'room_type'], inplace = True)\ndf.head()","efad95d7":"# creating a copy of df for later\ndf_copy = df.copy()\n#################################\ny = df['price']\ndf.drop(columns = ['price'], inplace = True)","5fc1a5f2":"df_sent_m1 = df.copy()\ndf_sent_m2 = df.copy()\n\ndf_sent_m1['sentiment'] = sent_m1\ndf_sent_m2['sentiment'] = sent_m2\n\n\n# adding onto our copy of df\ndf_copy['sent_m1'] = sent_m1\ndf_copy['sent_m2'] = sent_m2","741f2e8a":"corr_matrix_m1 = df_copy.corr()\ncorr_matrix_m1[\"price\"].sort_values(ascending = False)","4a794389":"object_cols","10c48ae8":"# we will change df_sent_m1 and _m2 into sparse matrices as this data type get processed better\nfrom scipy import sparse\n#from scipy.sparse import hstack\n\nX_m1 = sparse.hstack([object_cols, sparse.csr_matrix(df_sent_m1.to_numpy())])\nX_m2 = sparse.hstack([object_cols, sparse.csr_matrix(df_sent_m2.to_numpy())])","f5b0078c":"from sklearn.model_selection import train_test_split\nX_train_m1, X_test_m1, y_train_m1, y_test_m1 = train_test_split(X_m1, y, test_size = 0.2, random_state = 42)\nX_train_m2, X_test_m2, y_train_m2, y_test_m2 = train_test_split(X_m2, y, test_size = 0.2, random_state = 42)\n\ny_train_m1 = y_train_m1.tolist()\ny_train_m2 = y_train_m2.tolist()\ny_test_m1 = y_test_m1.tolist()\ny_test_m2 = y_test_m2.tolist()","fcdc6475":"temp_train_m1 = X_train_m1[:, X_train_m1.shape[1]-4:].toarray()\ntemp_train_m2 = X_train_m2[:, X_train_m2.shape[1]-4: X_test_m2.shape[1]-1].toarray()\n\ntemp_test_m1 = X_test_m1[:, X_test_m1.shape[1]-4:].toarray()\ntemp_test_m2 = X_test_m2[:, X_test_m2.shape[1]-4: X_test_m2.shape[1]-1].toarray()\n\nfrom sklearn.preprocessing import StandardScaler\nscaler_m1 = StandardScaler().fit(temp_train_m1)\nscaler_m2 = StandardScaler().fit(temp_train_m2)\n\ntemp_train_m1 = scaler_m1.transform(temp_train_m1)\ntemp_train_m2 = scaler_m2.transform(temp_train_m2)\n\ntemp_test_m1 = scaler_m1.transform(temp_test_m1)\ntemp_test_m2 = scaler_m2.transform(temp_test_m2)","1cca68e7":"X_train_m1[:,X_train_m1.shape[1]-4:] = sparse.csr_matrix(temp_train_m1)\nX_train_m2[:,X_train_m2.shape[1]-4: X_test_m2.shape[1]-1] = sparse.csr_matrix(temp_train_m2)\n\nX_test_m1[:,X_test_m1.shape[1]-4: ] = sparse.csr_matrix(temp_test_m1)\nX_test_m2[:,X_test_m2.shape[1]-4: X_test_m2.shape[1]-1] = sparse.csr_matrix(temp_test_m2)","e4a94a6d":"from sklearn.svm import SVR\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import regularizers\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold\nfrom keras.initializers import RandomNormal\n\n#### FOR NEURAL NETWORK ####\ninitializer = RandomNormal(mean=0., stddev=1.)\n\ndef neural_net():\n    model = Sequential()\n    model.add(Dense(int(2\/3 * X_test_m1.shape[1]), kernel_initializer = initializer, activation = 'relu', input_dim = X_test_m1.shape[1]))\n    model.add(Dense(int(4\/9 * X_test_m1.shape[1]), kernel_initializer = initializer, activation = 'relu'))\n    model.add(Dense(1, kernel_initializer = 'normal', activation = 'relu'))\n    model.compile(optimizer = 'SGD', loss = 'mse', metrics = ['mae'])\n    return model\n############################    \n\nmodels = []\nmodels += [['SVM', SVR(kernel = 'linear')]]\nmodels += [['Lasso', Lasso(alpha = 0.9, normalize = False, selection = 'cyclic')]]\nmodels += [['Ridge', Ridge(alpha = 0.9, normalize = False, solver = 'auto')]]\nmodels += [['Linear', LinearRegression(normalize = False)]]\nmodels += [['Random Forests', RandomForestClassifier(n_estimators = 100, max_features = X_test_m1.shape[1], random_state = 42, max_depth = 9)]]\n\n# for the k fold cross validation\nkfold = KFold(n_splits = 10, random_state = 1, shuffle = True)","eaf20b13":"from sklearn.model_selection import cross_val_score\n\nresult_m1 =[]\nnames = []\n\nfor name, model in models:\n    cv_score = -1 * cross_val_score(model, X_train_m1, y_train_m1, cv = kfold, scoring = 'neg_mean_absolute_error')\n    result_m1 +=[cv_score]\n    names += [name]\n    print('%s: %f (%f)' % (name,cv_score.mean(), cv_score.std()))","50e065e1":"from keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\nestimator = KerasRegressor(build_fn = neural_net, epochs = 1000, batch_size = 2000, verbose=0)\nresults = -1 * cross_val_score(estimator, X_train_m1, y_train_m1, cv = kfold, scoring = 'neg_mean_absolute_error')\nprint(\"Neural net: %f (%f) MSE\" % (results.mean(), results.std()))","d113fa99":"result_m2 =[]\nnames = []\n\nfor name, model in models:\n    cv_score = -1 * cross_val_score(model, X_train_m2, y_train_m2, cv = kfold, scoring = 'neg_mean_absolute_error')\n    result_m2 +=[cv_score]\n    names += [name]\n    print('%s: %f (%f)' % (name,cv_score.mean(), cv_score.std()))","5466ccc7":"estimator = KerasRegressor(build_fn = neural_net, epochs = 1000, batch_size = 2000, verbose=0)\nresults = -1 * cross_val_score(estimator, X_train_m2, y_train_m2, cv = kfold, scoring = 'neg_mean_absolute_error')\nprint(\"Neural net: %f (%f) MSE\" % (results.mean(), results.std()))","86280144":"model = SVR(kernel = 'linear').fit(X_train_m2, y_train_m2)\n\n# obtaining predictions\npredictions = model.predict(X_test_m2)","fa98b971":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nprint(\"Final mean squared error: %f      R^2 value: %f\" %(mean_absolute_error(y_test_m2, predictions), r2_score(y_test_m2, predictions)))","0a798a10":"Now that we obtained the sentiment scores for the english titles, we have to obtain the average in each neighborhood and apply that average as the sentiment values for the non-english titles.","9c2c2a3c":"So our final chosen model will be a linear SVM, using the 2nd method for sentiment","5c7c44ee":"We had 4 NaN values in the name category as well. We will simply fill these with blanks (i.e. '')","a02aa9b7":"Now splitting the data by using train\/test","80aa108b":"To use VADER, we have to put our names into a single string","ac5cc305":"Now we will turn these into empty string lists so when the sentiment analysis is performed, a score of 0 will be obtained.","a71159cd":"Now that we have prepared our train and test data, we can begin to create models","cc52e8db":"<h3> Exploring Data","6a6992a8":"We will not be able to use string types in our model, so we will have to encode room_type, neighbourhood_group and neighbourhood. \n\nIn addition, the name that the airbnb is titled is (once again in my opinion) an important factor that people consider when going through AirBnb's (no one wants to rent an airbnb called craphole). So we will perform natural language processing on this column and perform sentiment analysis to obtain some kind of score. Note that there are several flaws with this plan such as the fact that there is no dictionary available to map terms specific for real estate and property to a value. Also, given how small the strings are in the name column (character wise), there might be large variance in sentiment values.\n\n","ec47d11c":"<h3> Final Evaluation","1e86372f":"Not all of the descriptions of the airbnbs get detected as english even though they are in english, which might create problems for our sentiment analysis. This issue is most likely caused by langdetect having too many languages it is capable of detecting. Couple other factors is that people use slangs, emojis and mix languages together, making it a difficult task to pinpoint the exact language. ","01e9d298":"Number_of_reviews and reviews_per_month does not give us much help since we don't know if the reviews were positive or negative. Because it is so ambiguous, we can drop these 2 columns","62c419e0":"The models that we will try are:\n1. SVM\n2. Lasso Regression\n3. Ridge Regression\n4. Linear Regression\n5. Random Forests\n6. Neural network\n\nCross validation with stratified k fold will be used to determine the most appropiate model.","ca816fa9":"Performing cross validation","1c9ff3a9":"Now let's go through our name column one last time. Since our sentiment analysis will only be applied to english, we should check to see if all the titles are in english. We will be using langdetect for this.","ed592e76":"Let's take another look at our dataframe","0d322096":"Extracting our target label","eec8a849":"**NOTE**: ~17000 samples were removed","10c74fce":"While a couple of the names that were categorized as not english are still in english, we were still able to obtain a greater # of accurate samples.","bdd86b4d":"Evaluating our model","890d8c7a":"<h3> Preparing Data","ee336a1b":"VADER uses 4 main metrics to measure sentiment for words. Positive, negative, and neutral represent the proportion of text falling into these categories. The last metric is a sum of all the lexicon ratings that have been normalized between -1 (for most negative) to +1 (for most positive). A general rule is:\n1. *positive sentiment*: compound score >= 0.05\n2. *neutral sentiment*: -0.05 < compound score < 0.05\n3. *negative sentiment*: compound score <= -0.05\n\nSimply to experiment, I will try two different sentiment scoring methods:\n1. sentiment value = compound score\n2. sentiment value = {-1, 0 ,1} depending on polarity of the sentiment from the ranges above\n\n","db4b2321":"<h4> Sentiment Analysis","23b5f824":"<h4> Train\/Test","b46d37ec":"Now adding our features from before. Two dataframes are created to account for both methods used to calculate sentiment","98dc5cfd":"<h4> Method 1","d32226a4":"We have 16 total features and ~49000 samples. Not all of the features are useful and we are also unsure if all samples are valid (i.e. presence of NaN) so we will most likely have to perform feature engineering to obtain more useful data. Let's see if we can get any insights from the data before making any changes to it. <br>\n\nNote that majority of the features are self-explanatory, but just for clarification:\n1. **'calculated_host_listings_count'** indicates the total number of apartments and bedrooms referred to the same landlord (one landlord can have more than one property in NY) [1]\n2. **'availability_365'** indicates the # of days in an year that the airbnb is available. It is possible for this value to be 0 [2]\n\n[1]https:\/\/www.kaggle.com\/dgomonov\/new-york-city-airbnb-open-data\/discussion\/120300 <br>\n[2]https:\/\/www.kaggle.com\/dgomonov\/new-york-city-airbnb-open-data\/discussion\/111835","afef5f83":"Dropping off all of our features that are not used or have been transformed.","a1512c39":"A method around this is to limit the # of possible languages (using langid) and for entries that are not in english, we can remove those titles and give a sentiment score equivalent to the average of the respective neighborhood. From the world_atlas, the most common languages spoken in NYC are: English, Spanish\/Spanish Creole and Chinese [2]. To get more accurate results, we should remove any characters that are not used in proper english (i.e. hypens, slashes, asterisks etc.)\n\n[2] https:\/\/www.worldatlas.com\/articles\/how-many-languages-are-spoken-in-nyc.html","ec02e8a7":"It seems that a linear SVM has the lowest mean score, and interestingly the 2nd method of using strict values of +1, 0 or -1 for the sentiment resulted in marginally better results for all models.","707ded4b":"To obtain more accurate results, normalization will be applied to the appropiate columns","7ac553ee":"The task at hand is to predict Airbnb prices given certain metrics in NYC. \n\n**NOTE**: This project is done solely for personal practice. Please let me know where I can improve on or if there are any questions to my thinking. Thank you!","1276b94f":"<h4> Method 2","77d4a5fc":"Now changing NaN values in the reviews_per_month into 0 ","38823f6c":"We left some columns over that might potentially be useful. If not, we will simply drop them later. First, let's deal with the NaN and potentially invalid data.\n\nAs mentioned earlier, availability_365 can equal 0, meaning that the owner is never open for an year. It is unknown whether the host is just closed or if it's bad data, but we will remove samples where availability_365 = 0. There are no NaN values in this column so we won't need to handle that case. ","f0d8dfc4":"Now that we have all the required features, we can put our final dataset together and begin creating our models\n","dd1c720f":"Now removing stop words","500fa703":"Now let's combine everything we have ","41ef00a5":"As of this moment, I have no idea how to incorporate latitude and longitude into my model. I think it's a fair assumption to make that latitude and longitude are not normal metrics used when people are considering prices n such. In addition, these two columns are essentially incorporated in the neighbourhood_group and neighbourhood columns. ","68459181":"<h3> Encoding + Feature Engineering ","54ac68f5":"<h4> Encoding","3e5bc135":"Let's take a quick look at the correlation coefficient that price has with every other feature","080ffed4":"Now we will perform sentiment analysis on the name column. Remember that we added 'to' to the NaN in the name column.  VADER will be our chosen library for sentiment analysis. It is capable of picking up on use of capitalization, slangs and emjois to some extent, making it more accurate for informal writings such as this.","0d5bf8d7":"<h3> Obtaining Data","d5682447":"There are no copied values or invalid values (from a glance) so we will not have to perform any cleaning","b4023587":"Majority of the NaN are in the last_review and reviews_per_month feature. This makes sense because last_review is a date type and some error most likely occured during the data gathering portion. reviews_per_month are NaN because of a division by 0 error. \n\nAs of now, we can drop a couple of columns that (in my honest opinion) are not that important:\n1. id\n2. host_id\n3. host_name\n4. last_review\n\n","0820e4af":"<h3> Model Building","459be897":"Now subbing these averages back into our original Series of sentiment scores","c0d4b761":"This is a very high MSE and has a very poor fit to our data.  \n\n**NOTE**: To be quite honest, tweaking hyperparameters is something that I am still not entirely comfortable doing, so it will be skipped here for now. Will pick this notebook back up once I learn more. ","a4558d91":"So there is a very low correlation coefficient with price and our other features (reaching even negative values for our sentiment values. This can indicate one of 2 things:\n1. there is little to no relationship between price and the other features\n2. the relationship between the other features is nonlinear to price since correlation coefficient is only a measure of linear relationship"}}