{"cell_type":{"ed8cbd24":"code","789b010b":"code","0a387252":"code","9ee99a25":"code","f1ae22b1":"code","674cd697":"code","f61fed8d":"code","7b4fc8d1":"code","54521ac3":"code","95bd0734":"code","e311431b":"code","d2642972":"code","43cc132c":"code","3296b453":"code","e50118ea":"code","d23cca7c":"markdown","0a3f6ddd":"markdown","bf3d0dd8":"markdown","cead01a2":"markdown","21bc636e":"markdown","733f5719":"markdown","e4d2003f":"markdown","d11a13b4":"markdown","caf544db":"markdown","c285c579":"markdown","d08076da":"markdown","b2a9cfe6":"markdown","7eda988d":"markdown","6b33fe69":"markdown"},"source":{"ed8cbd24":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\ndata = pd.read_csv(\"..\/input\/weatherAUS.csv\")\n\n#First check the data..\n#print(data.info())\n#data[0:5]","789b010b":"data = data.drop(columns=['Location','Date','Evaporation','Sunshine','Cloud3pm','Cloud9am','RISK_MM'],axis=1)","0a387252":"data['RainToday'].replace({'No': 0, 'Yes': 1},inplace = True)\ndata['RainTomorrow'].replace({'No': 0, 'Yes': 1},inplace = True)\n\n#remove null values\ndata.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)\nprint(data.shape)\n\n#make columns from categorical values.We have to numarize this columns..\ncategorical_columns = ['WindGustDir', 'WindDir3pm', 'WindDir9am']\ndata = pd.get_dummies(data, columns=categorical_columns)\nprint(data.shape)","9ee99a25":"from sklearn import preprocessing\nfrom scipy.stats import zscore\n\nnumeric_cols = data.select_dtypes(include=[np.number]).columns\ndata[numeric_cols].apply(zscore)\n\n#Scaling data..\ndata = data.astype('float64')\nscaler = preprocessing.MinMaxScaler()\nscaler.fit(data)\ndata = pd.DataFrame(scaler.transform(data), index=data.index, columns=data.columns)\n\n#Lets divide our features..\nX = data.loc[:,data.columns!='RainTomorrow']\ny = data['RainTomorrow']","f1ae22b1":"from sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\n\nselector = SelectKBest(chi2, k=4)\nselector.fit_transform(X, y)\nprint(X.columns[selector.get_support(indices=True)])\n\n#Lets change our training data set with the best features calculated by feature selection..\n\nX = data[X.columns[selector.get_support(indices=True)]]\n\n#spliting data for train and test\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)","674cd697":"import matplotlib.pyplot as plt\nimport matplotlib.style as sty\n\nsty.use('ggplot')\nplt.figure(figsize=(3,3))\nplt.hist(y,bins=2,rwidth=0.8)\nplt.xticks([0.25,0.75],['No Rain','Rain'])\n\nprint(y.value_counts())\n","f61fed8d":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 20)\nknn.fit(x_train,y_train.values.ravel())\ny_pred_KNN = knn.predict(x_test)\nprint(\" {} nn score: {} \".format(20,knn.score(x_test,y_test)))","7b4fc8d1":"from sklearn.metrics import confusion_matrix\n\nconfMatKnn = confusion_matrix(y_test,y_pred_KNN)\n\nf, ax = plt.subplots(figsize = (3,3))\nsns.heatmap(confMatKnn,annot=True,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predictions\")\nplt.ylabel(\"Test Values\")\nplt.show()","54521ac3":"#This part can take time..\n#r = range(15,20)\n#for i in r:\n#    knn = KNeighborsClassifier(n_neighbors = i)\n#    knn.fit(x_train,y_train.values.ravel())\n#    #prediction = knn.predict(x_test)\n#    print(\" {} nn score: {} \".format(i,knn.score(x_test,y_test)))    ","95bd0734":"from sklearn.naive_bayes import GaussianNB\n\ngaussianNB = GaussianNB()\ngaussianNB.fit(x_train,y_train.values.ravel())\ny_pred_NB = gaussianNB.predict(x_test)\nprint(\"Gaussian NB score: \" , gaussianNB.score(x_test,y_test))","e311431b":"confMatNB = confusion_matrix(y_test,y_pred_NB)\n\nf, ax = plt.subplots(figsize = (3,3))\nsns.heatmap(confMatNB,annot=True,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predictions\")\nplt.ylabel(\"Test Values\")\nplt.show()","d2642972":"from sklearn.tree import DecisionTreeClassifier\n\ndtClassifier = DecisionTreeClassifier()\ndtClassifier.fit(x_train,y_train)\ny_pred_DT = dtClassifier.predict(x_test)\nprint(\"Decision Tree Classifier NB score: \" , dtClassifier.score(x_test,y_test))\n","43cc132c":"confMatDT = confusion_matrix(y_test,y_pred_DT)\n\nf, ax = plt.subplots(figsize = (3,3))\nsns.heatmap(confMatDT,annot=True,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predictions\")\nplt.ylabel(\"Test Values\")\nplt.show()","3296b453":"from sklearn.ensemble import RandomForestClassifier\n\nrfClassifier = RandomForestClassifier(n_estimators = 20,random_state = 1)\nrfClassifier.fit(x_train,y_train.values.ravel())\ny_pred_RF = rfClassifier.predict(x_test)\nprint(\"Random Forest Classifier NB score: \" , dtClassifier.score(x_test,y_test))","e50118ea":"confMatRF = confusion_matrix(y_test,y_pred_RF)\n\nf, ax = plt.subplots(figsize = (3,3))\nsns.heatmap(confMatRF,annot=True,fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predictions\")\nplt.ylabel(\"Test Values\")\nplt.show()","d23cca7c":"Visualize confusion matrix for Random Forest Classifier","0a3f6ddd":"Ok lets try with all these values..","bf3d0dd8":"Visualize confusion matrix for Decision Tree Classifier","cead01a2":"Lets deal with the null values.I dont need location for analysis.And i'll ignore 'Evaporation','Sunshine','Cloud3pm','Cloud9am' because of the null values..One more think i remove 'RISK_MM' because it has lots of 0..","21bc636e":"Decision Tree Classifier","733f5719":"Lets Check for the best n_neighbors for KNN","e4d2003f":"Show data distribution..","d11a13b4":"Lets Try With Naive Bayes..","caf544db":"Lets clear our dataset from inconvenient datas and normalize it","c285c579":"Ok lets select best features for KNN and split data to train and test","d08076da":"Visualize confusion matrix for KNN","b2a9cfe6":"Visualize confusion matrix for Naive Bayes Gaussian","7eda988d":"Replace categorical values with the numbers..And remove null valued rows..","6b33fe69":"Random Forest"}}